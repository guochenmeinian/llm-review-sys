# SituatedGen: Incorporating Geographical and Temporal Contexts into Generative Commonsense Reasoning

Yunxiang Zhang

University of Michigan

Ann Arbor, USA

yunxiang@umich.edu

&Xiaojun Wan

Peking University

Beijing, China

wanxiaojun@pku.edu.cn

###### Abstract

Recently, commonsense reasoning in text generation has attracted much attention. Generative commonsense reasoning is the task that requires machines, given a group of keywords, to compose a single coherent sentence with commonsense plausibility. While existing datasets targeting generative commonsense reasoning focus on everyday scenarios, it is unclear how well machines reason under specific geographical and temporal contexts. We formalize this challenging task as SituatedGen, where machines with commonsense should generate a pair of contrastive sentences given a group of keywords including geographical or temporal entities. We introduce a corresponding English dataset consisting of 8,268 contrastive sentence pairs, which are built upon several existing commonsense reasoning benchmarks with minimal manual labor. Experiments show that state-of-the-art generative language models struggle to generate sentences with commonsense plausibility and still lag far behind human performance. Our dataset is publicly available at https://github.com/yunx-z/situated_gen.

## 1 Introduction

In recent years, there has been substantial growth in new benchmarks evaluating commonsense reasoning for natural language processing (NLP) models, especially large-scale Pretrained Language Models (PLMs). Most existing commonsense reasoning benchmarks adopt natural language _understanding_ formats due to easy evaluation (e.g., accuracy), including multiple-choice question answering , natural language inference , and detecting true/false statements . However, datasets measuring commonsense knowledge in natural language _generation_ are still relatively scarce. We aim to fill this research gap with a novel benchmark since real-world users of NLP systems would expect the generated outputs from LMs to be not only grammatically correct but also adhere to commonsense knowledge.

CommonGen, a generative commonsense reasoning challenge, has attracted wide attention recently. Given a set of keywords (e.g., {dog, frisbee, catch, throw}), the task requires models to compose a plausible sentence describing everyday scenario using all the provided keywords (e.g., "_The dog catches the frisbee when the boy throws it_."). While CommonGen focuses on social and physical commonsense in everyday life, it is unclear how well current commonsense generation models reason with factual knowledge about specific entities, which is referred to as _entity commonsense_. In this work, we mainly consider geographical and temporal entities, as they provide extra-linguistic contexts  for commonsense reasoning and appear in a significant proportion of existing commonsense benchmarks (Section 4.2). To the best of our knowledge, we are the first to incorporate these situations into generative commonsense reasoning.

Furthermore, we argue that geographical and temporal contexts are important for commonsense reasoning. On the one hand, basic knowledge about geography and time is part of human commonsense [1; 6], such as _"Earth rotates on its axis once in **24 hours."_ On the other hand, certain types of commonsense knowledge are correlated with specific situations . For example, _"July is summer"_ is true for people living in the northern hemisphere, while those living in the southern hemisphere would agree that _"July is winter"_.

Our proposed task SituatedGen (**Situated Generative Commonsense Reasoning**) requires the machines to generate a pair of contrastive sentences (formally speaking, _antithesis_) with commonsense plausibility, given a group of keywords including geographical or temporal entities. For example, when provided with [July, United States, winter, Australia, summer, July], a reasonable output could be _"July is summer in the United States. July is winter in Australia."_, while a slightly different version _"July is summer in Australia. July is winter in the United States."_ does not adhere to commonsense.

The main challenge for machines to solve the SituatedGen task lies in _situated semantic matching_. In order to generate a pair of contrastive sentences, machines need to split the keywords into two groups (either explicitly or implicitly) based on geographical/temporal relevance and perform relational reasoning  within/between the keyword groups.

To study the challenging SituatedGen task, we construct a corresponding large-scale English dataset containing 8,268 pairs of situated commonsense statements. We design an automatic pipeline to collect data at scale with quality assurance and minimal human annotation efforts. Concretely, we derive commonsense statements with geographical or temporal contexts from existing commonsense benchmarks and mine contrastive sentence pairs based on entity-masked sentence similarity. We further manually filter out invalid examples in the test set to ensure the evaluation soundness. To assess the difficulty of our dataset, we conduct automatic evaluations on various generative (large) language models, including BART , T5 , and InstructGPT . Results show these models lag far behind human performance, indicating that current models struggle to generate sentences adhering to commonsense under the SituatedGen setting. We believe that SituatedGen could serve as a complement to CommonGen and enrich the resource for evaluating constrained commonsense text generation in a more realistic setting.

The contributions of this work are three-fold:

* **Task.** We incorporate geographical and temporal contexts into generative commonsense reasoning and propose a novel task SituatedGen.
* **Resource.** We construct a large-scale dataset in a non-trivial way to facilitate the studies of situated generative commonsense reasoning. The dataset is released and will contribute to the commonsense reasoning community.
* **Evaluation.** We benchmark the performance of state-of-the-art generative language models on our dataset and demonstrate the difficulty of the task with a significant gap between machine and human performance.

## 2 Related Work

Constrained Commonsense Text Generation.Constrained Commonsense Text Generation  requires PLMs to generate commonsense text subject to a set of constraints. Commonsense generation models are currently evaluated by three tasks. First, Commonsense Explanation aims to generate an explanation for why a model selects a candidate answer to a given question. Second, \(\) NLG  is another commonsense generation task. The artificial intelligence models are provided with two observations in chronological order and need to generate a plausible hypothesis/explanation describing what happened between the observations. Third, in CommonGen, models should compose a plausible sentence describing everyday scenarios using all the provided concepts. This task has attracted much attention recently, and researchers advance machine performance on the dataset with contrastive learning , prototype editing , scene knowledge graph , etc. Our proposed task differs from these tasks with a focus on composing a _pair_ of contrastive sentences instead of a _single_ sentence and incorporating extra-linguistic contexts.

NLP Benchmarks with Geographical and Temporal Contexts.There are many emerging benchmarks in NLP that incorporate extra-linguistic contexts such as geographical and temporal contexts. Templama and GeoMLama probe language models with masked text prompts to query geographical and temporal knowledge. In question answering, McCaco, Torque and TimeQA contains challenging questions involving temporal commonsense reasoning over the duration, frequency, temporal order, and other various aspects of events. SituatedQA is made up of open-domain questions whose answers vary across different geographical and temporal contexts. TimeDial studies temporal reasoning in dialogues with a multiple-choice cloze task. In vision-and-language tasks, GD-VCR  and MaRVL  aim to collect commonsense questions and statements that are visually grounded and geographically diverse. Previous work mainly focuses on how well language models trained on a specific snapshot of corpus can adapt to different contexts. While our dataset SituatedGen also considers such geographical and temporal contexts in language, we probe LMs for a new skill of reasoning for the commonsense relationship among extra-linguistic contexts. We also choose a different task format of generative commonsense reasoning, pioneered by , as it focuses on the commonsense reasoning capabilities of generative models rather than NLU models, which is under-researched by the community.

## 3 Task Definitions and Challenges

We use antithesis generation for evaluating generative commonsense reasoning under extra-linguistic contexts. In this section, we first introduce the definitions of our proposed task, followed by an analysis of the main challenges.

### Definitions

Antithesis.Antithesis refers to a figure of speech that expresses an opposition of ideas with a parallel grammatical structure of words, clauses, or sentences . An example of antithesis could be Neil Armstrong's famous quote "_That's one small step for a man, one giant leap for mankind_". In this work, we adopt a narrow sense of sentence-level antithesis, which means that two simple sentences with similar syntactic structures create a contradiction in semantics. Intuitively, the qualifying two sentences can be connected into a coherent sentence via conjunction words such as "while", "yet", and "whereas" (e.g., "_July is summer in the United States, while July is winter in Australia._"). We emphasize commonsense plausibility rather than the rhetorical effect of antithesis within the scope of this paper.

Extra-Linguistic Contexts.Following , we focus on two context types: geographical (geo) and temporal (temp). geo defines each context value as a geopolitical entity ("GPE"). temp defines each context value as timestamp ("DATE", "TIME", "EVENT").

Contextual Dependence.We define that a contrastive sentence pair is _context-dependent_ if swapping any of the geo or temp entities between the two sentences could lead to a contradiction with commonsense yet grammatical correctness. For example, for the sentence pair "_July is summer in China. July is winter in Australia._", if the two geo entities "China" and "Australia" are swapped, the resulting sentences do not adhere to commonsense anymore: "_July is summer in Australia. July is winter in China._" This indicates that they are context-dependent.

Contextual dependence is crucial for a proper evaluation of the generation results. Because sentence pairs that do not satisfy context dependence may have multiple valid answers (swapping the entity words leads to an extra correct answer), the metrics introduced in Section 6 cannot make a sound evaluation with only a single reference.

Situated Generative Commonsense Reasoning.We modify the mathematical formulation of the task CommonGen to define SituatedGen. The input of the task is a multiset1 consisting of \(k\) keywords \(x=[c_{1},c_{2},...,c_{k}]\), where each keyword \(c_{i}\) is a noun or entity, a single word or phrase. We denote \(\) as all possible combinations of keywords and \(\) as the vocabulary of keywords.

Keywords in \(x\) should contain at least two geo or temp entities of the same type and two other keywords2.

The output of the task is an unordered pair of coherent and plausible sentences \(y=\{s_{1},s_{2}\}\) that satisfies the following conditions: 1) the sentence pair includes all keywords in \(x\); 2) each sentence has at least one geo or temp keyword; 3) each sentence is geographical-temporal-semantically correct; 4) \(s_{1}\) and \(s_{2}\) form a pair of contrastive sentences, or antithesis; 5) \(s_{1}\) and \(s_{2}\) are context-dependent. The goal of the task is to learn a function \(f:\) that maps a group of keywords \(x\) to a pair of sentences \(y\).

### Challenges: Situated Semantic Matching

As the goal of our task is to generate a pair of sentences instead of a single sentence, machines need to explicitly or implicitly classify the keywords into two subgroups based on their geographical and temporal semantic relevance, so as to generate one commonsense sentence with each subgroup. For example, given [July, China, winter, Australia, summer, July], the resulting keyword subgroups should be {July, China, summer} and {July, winter, Australia}.

During the process of keyword grouping and matching, machines need to make connections among keyword concepts with relational reasoning  over factual knowledge about these nouns and entities, a.k.a. _entity knowledge_, such as geographical location, temporal order, physical rules, social customs, etc. The matching process is important since wrong grouping results will lead to generated sentences without commonsense plausibility3.

We require that the two sentences should have similar syntactic structures and express similar relationships (e.g., "_X lives in Y_"). This is important for securing the difficulty of the task as it prevents models from learning shortcuts  to group keywords based on trivial syntactic (e.g., POS tag of the word) and semantic (e.g., two different kinds of relationship) information. For example, if the two sentences have different syntactic structures (e.g. "X lives in Y" and "Z eats W"), then the model could simply put a city name in Y and a food name in W for keyword grouping and ignore the commonsense connection with X/Z. This type of shortcut reduces the task difficulty.

Figure 1: An overview of data collection pipeline. Inside the dotted box is a final example in the dataset.

Dataset Collection

To study the SituatedGen challenge, we construct a large-scale English dataset. We design a pipeline to collect high-quality data at scale with minimal manual annotation efforts. Figure 1 illustrates the overall pipeline for dataset collection, which consists of three steps:

1. **QA-to-statement.** Converting question-answer pairs of existing commonsense question answering benchmarks into corresponding statements.
2. **Contexts Identification.** Identifying all entities in a statement with a NER tagger and removing those statements without geo and temp entities.
3. **Contrastive Sentences Mining.** Automatically mining contrastive sentence pairs (antithesis) from the remaining commonsense statements based on entity-masked sentence similarity.

### QA-to-Statement

Our dataset is composed of commonsense statements, which are simple sentences describing commonsense knowledge, e.g., "_You would find many canals in Venice._" In recent years, numerous commonsense reasoning benchmarks have been proposed and they form a potentially available commonsense knowledge base with high quality and diverse content. Inspired by recent benchmarks that are sourced from existing datasets [52; 37], we aim to extract commonsense statements from these commonsense benchmarks. We assume that the knowledge in these commonsense benchmarks is _actually_ commonsense instead of encyclopedic knowledge, though they might not be shared locally in certain groups of people due to a lack of geographical diversity. That being said, we adopt and follow the concept of "commonsense" widely used in existing works.

We conduct a holistic study of commonsense reasoning datasets to date and select five different data sources after considering their size, annotation quality, and reasoning difficulty. They are CREAK , StrategyQA , CommonsenseQA , ARC  and OpenbookQA , respectively. We briefly introduce the nature of each dataset in Appendix A.1. Since the raw data come in different formats such as multiple-choice questions and Yes/No questions, we apply a specific preprocessing method for each dataset to transform them (i.e., question-answer pairs) into statements. The transformation details are also included in Appendix A.1. In general, we collected 35,997 commonsense statements from the five source datasets (statistics in Table 1).

### Contexts Identification

We now filter out commonsense statements without geographical or temporal contexts. Following , we identify sentences with extra-linguistic contexts by geo and temp entities. We use FLERT4, a named entity recognition (NER) model, to extract all entities from a sentence and remove those statements without any geo ("GPE") or temp ("DATE", "TIME", "EVENT") entities.

Table 1 shows that of all the commonsense statements extracted from the five source datasets, 6.6% sentences have geo contexts and 5.5% have temp contexts, which we count as a significant proportion. Finally, we obtain 4,038 (11.2%) commonsense statements with extra-linguistic contexts.

### Contrastive Sentences Mining

We aim to automatically mine contrastive sentence pairs from the commonsense statement corpus. Antithesis mining has not been studied in the existing literature, so we propose a pilot algorithm. We observe that after removing keywords from contrastive sentences, the remaining parts are very similar since antithesis sentences have parallel syntactic structures . Based on this observation, we design the antithesis mining algorithm illustrated in Figure 2 consisting of three steps:

1. **Keyword Masking.** We extract all entities and other nouns as keywords in the sentence and replace each keyword with a [UNK] token, telling the pretrained language models to neglect the meaning of these keywords.

2. **Masked Sentence Similarity Matching.** We obtain the embedding of the keyword-masked sentence from a pretrained language model and calculate the cosine similarity between all possible sentence pairs.
3. **Rule-based Filtering.** We filter out invalid sentence pairs based on a fixed threshold of masked sentence similarity, number of keywords, and entity types.

We introduce the implementation of our antithesis mining algorithm in Appendix A.2. In this way, we efficiently extracted large-scale contrastive sentence pairs from all possible pairwise combinations of the aforementioned commonsense statements with extra-linguistic contexts5 (Section 4.2). For each contrastive sentence pair, we merge the keywords from each statement and randomly shuffle them to get the input data. The output is the concatenation of two statements.

### Dataset Splitting

When splitting the data into training, validation, and test set, we explicitly require that one statement cannot appear simultaneously in any two sets. Consequently, there is no overlap of the single sentence (or sentence-level keyword combinations) among the training, validation, and test data. This requirement forces machines to reason over new combinations of keywords during the inference stage instead of memorizing existing keywords matching results. Statements with similar syntactic structures will also be divided into the same set to reduce overlap of syntactic templates across different sets.

Specifically, we treat dataset splitting as a community structure  discovery problem. Community structure refers to a group of tightly connected nodes that have a high density of internal connections and a low density of external connections. We regard a single sentence as a node in the graph. If two single sentences can be matched into a pair of contrastive sentences, an undirected edge will connect the corresponding nodes of these two single sentences. In this way, we obtain an undirected graph describing the dataset structure. A subset of a dataset (such as a training set) is equivalent to a subgraph containing all sentence pairs (edges) and single sentences (nodes) of that subset.

In order to prevent the same sentence from appearing across different sets, we require that the subgraph node sets of the training set, validation set, and test set are disjoint. We use a community structure detection algorithm to meet this requirement. We use the community as the basic unit of dataset splitting, putting all the edges (sentence pairs) in one community into a certain dataset split. Connecting edges between communities (two vertices belonging to different communities) are removed. We note that sentences with similar syntactic structures tend to be connected to each other in the graph and thus fall into the same community, which ensures the syntactic variability between train/dev/test splits.

   Dataset & \# Sent & \# geo & \# temp & \# geo & \# Valid \\  & & & & \& temp & Sent \\  CREAK & 5,779 & 868 & 552 & 153 & 1,573 \\ StrategyQA & 4,976 & 501 & 366 & 86 & 953 \\ CommonsenseQA & 10,962 & 487 & 215 & 12 & 714 \\ ARC & 7,787 & 165 & 426 & 52 & 643 \\ OpenbookQA & 6,493 & 31 & 119 & 5 & 155 \\  Total & 35,997 & 2,052 & 1,678 & 308 & 4,038 \\   

Table 1: Statistics of contexts identification results. “Sent” means the commonsense statements collected in Section 4.1. “geo”/“temp” refer to statements with _only_ geographical/temporal entities. “geo & temp” refers to statements with _both_ geographical and temporal entities. “Valid Sent” means the commonsense statements with geo or temp contexts.

Figure 2: An illustration of the contrastive sentence mining algorithm.

We use the Louvain  community structure detection algorithm6 and divide our graph into 79 communities. The largest community contains 3,273 edges, accounting for about 26% of the total data. We remove edges connecting different communities and then randomly divide the communities of contrastive sentence pairs into training set, validation set or test set.

To ensure the evaluation soundness, we manually filter out invalid examples in the _test_ set that are not fluent antitheses or context-dependent. 13.6% of test data is removed and the final dataset has 8,268 examples in total. See additional details of manual filtering in Appendix B.

## 5 Dataset Analysis

### Quality Analysis

To measure the quality of our automatically collected data, we randomly select 100 examples (i.e. sentence pairs) from the validation set (which is not manually filtered) and annotate each example for whether it is actually 1) (fluent) antithesis and 2) context-dependent. We find that 87% of the data are real antitheses with fluency and 80% of the data satisfy both of the two requirements. Considering that our dataset is constructed through a fully automatic pipeline, this quality is pretty satisfying and can meet the needs of training and evaluation. As we have discussed in Section 3.1, test examples not satisfying contextual dependence can fool the evaluation metrics, since there are multiple valid references despite the single one provided in the test set. Thanks to the additional manual filtering at the end of Section 4.3, the test set is now qualified for evaluation. As for the unfiltered training set, even if a contrastive sentence pair is not context-dependent, it is still valuable training data, satisfying the other requirements for the target side (Section 3.1). Reduced size of training data after potential manual filtering is also unfavorable to the learning of models. As a result, we retain all the examples in the training set.

Now we analyze the error cases in detail, including non-contrastive and non-context-dependent sentence pairs. The main explanation that accounts for the production of non-contrastive sentence pair is that the remaining verbs after keyword masking may have lexical ambiguity, e.g. "play" in "_Slaves **play** a role in the history of the united states._" and "_A team sport **played** mostly in Canada is Lacrosse._" Although the pretrained language models could infer the meaning of a word according to its context , the contexts are lost after keyword masking. As a result, two sentences with different syntactic structures are matched together, thus violating the antithesis rule. This poses a limitation of our antithesis mining algorithm.

In addition, 7% of the sentence pairs are antitheses yet not context-dependent. Take the following sentence pair as an example: "_You could find millions of brownstone in New York City.\(7\) One can find a Holiday Inn inside the United States._". After swapping the geo entity "New York City" and "United States" in these two sentences, they still conform to commonsense. The reason for this phenomenon is that New York City is part of the United States, and thus the "brownstone" related to New York will also be related to the United States. However, we would like to point out that contextual dependence

   Statistics & Train & Dev & Test \\  Size (\# Sent Pairs) & 5,641 & 1,407 & 1,220 \\ \# Unique Sents & 788 & 309 & 341 \\ per Sent Pair & 0.14 & 0.22 & 0.28 \\ \# Unique Keywords & 1,847 & 725 & 851 \\ \# Avg. Input Keywords & 7.34 & 6.96 & 6.89 \\ \# Avg. Output Tokens & 20.89 & 24.08 & 20.61 \\   

Table 2: The basic statistics of the SituatedGen dataset. “Sent” means commonsense statement.

Figure 3: Distribution of numbers of input keywords.

is not an absolutely strict condition. Although this example still holds after swapping the geo entities, it is not the optimal answer, because "brownstone" is more a typical thing in New York City and thus more suitable for a match with "New York City".

### Dataset Statistics

Table 2 includes the basic statistics of the SituatedGen dataset. If we use the ratio of unique statement count to sentence pair count ("# Unique Sents per Sent Pair") to represent the content/keyword diversity of the dataset, the validation set, and the test set are relatively high (0.22/0.28), compared to the training set (0.14).

Distribution of Numbers of Input Keywords.Figure 3 shows the distribution of numbers of input keywords for all examples in the dataset. Intuitively, more input keywords imply an increased number of possible combinations, making it more difficult for the models to handle. The average number of input keywords is 7.21 and the distribution is fairly symmetrical (skewness=-0.25), suggesting that the SituatedGen has a reasonable difficulty.

Distribution of Context Types.Here we define three context types of pairs of contrastive sentences: a geo pair of sentences contain only geo entities; a temp pair of sentences contain only temp entities; If both sentences contain geo and temp entities, the pair of sentences belongs to the type of geo & temp. We find that 78% of all sentence pairs are geo, 21% are temp and the rest 1% are geo & temp.

## 6 Methods

Baseline Models.We benchmark the performance of three prominent pretrained language generation models with encoder-decoder architecture -- BART , T5 , FLAN-T5  -- and a decoder-only large language model (LLM) -- InstructGPT  with 175B parameters. We train BART, T5, and FLAN-T5 models in a fully supervised setting with the seq2seq format and expect that the models can learn to group keywords _implicitly_. Specifically, for the input of BART, we concatenate all shuffled keywords with a comma as the separation token "\(c_{1},c_{2},...,c_{k}\)". Regarding the input format of T5/FLAN-T5, we prepend the keyword sequence with a simple task description to align with its pretraining objective: "_generate two sentences with:_\(c_{1},c_{2},...,c_{k}\)". The outputs of all models are simple concatenations of the two target sentences \(s_{1}\) and \(s_{2}\). Since the output is an unordered pair, we feed two examples "\(x s_{1}\)\(s_{2}\)" and "\(x s_{2}\)\(s_{1}\)" to the model for each original training example. As for InstructGPT, we evaluate it in a few-shot setting. We build prompts with instruction and in-context demonstrations. For each test example, we randomly select 10 training examples as in-context demonstrations. We report the model hyper-parameters and GPT prompt format in Appendix C.1.

Evaluation Metrics. have well established the automatic evaluation protocol of the generative commonsense reasoning task. They demonstrated a strong correlation between automatic metrics and human evaluation results. Since SituatedGen adopts a similar format of keyword-to-text generation to CommonGen, we follow the evaluation protocol of CommonGen and do not include an extra manual evaluation in our study.

Concretely, we employ several widely-used automatic NLG metrics based on n-gram overlap -- BLEU , ROUGE , METEOR  -- and image caption metrics that focus on the consistency of keywords and their relationships -- CIDEr  and SPICE . In order to assess the validity of the generated outputs, we include BERTScore, a content-oriented and semantic metric. We also adopt COVERAGE, which is the average percentage of input keywords that are present in lemmatized outputs. Additionally, we report the accuracy of keyword grouping results8 as MATCH, which serves as a good indicator of the commonsense plausibility of the generated texts. See Appendix C.2 for the implementation details of these evaluation metrics.

## 7 Results

In Table 3, we report the experimental results of different baseline models on the test set of SituatedGen. We approximate human performance with 100 randomly sampled examples from the test set which are annotated by the authors of this paper. We observe that larger models tend to have better performance than smaller ones, as larger parameters store more commonsense knowledge and provide better language generation quality. Notably, the few-shot InstructGPT surpasses other fully-supervised models in every metric, demonstrating its strong reasoning ability. Nevertheless, it still lags far behind human performance. For example, there is a difference of 13.3 points in MATCH, indicating the lack of commonsense in machine generations. The large gap of keyword-oriented metrics (CIDEr and SPICE) also suggests that models find it difficult to infer the relationship between keywords. The significant gap between models and humans demonstrates the difficulty of SituatedGen and leaves much room for improvement in future research.

Performance across Different Context Types.Table 3 reports the performance of the FLAN-T5-large model across different context types. The results show that the matching accuracy of temp

   Input Keywords & 24 hours, axis, one month, Earth, axis, Moon \\ Reference & It takes one month for the Moon to rotate on its axis. Earth rotating on its axis takes \\  & 24 hours. \\ BART-base & The axis of the Moon is 24 hours. One month is one month. \\ BART-large & There are 24 hours in one month. \\ T5-base & Earth has a 24 hour axis. One month is one month. \\ T5-large & One month is one month on Earth. The Moon is 24 hours away from the axis of the \\  & Earth. \\ FLAN-T5-base & The Moon is the axis of the Earth. One month is one month. \\ FLAN-T5-large & The Moon is 24 hours away from Earth. One month is one month. \\ InstructGPT & The Earth takes 24 hours to rotate on its axis. The Moon takes one month to orbit \\  & around the Earth. \\  Input Keywords & Paul, Emperor, China, Qin, Russia, dynasty \\ Reference & The Qin dynasty reigned in China. Paul I of Russia reigned as the Emperor of Russia. \\ BART-base & The Emperor of China worked in China. Paul served as the first emperor of the \\ dynasty Qin. \\ BART-large & Emperor of the Qin dynasty. Paul existed in Russia. \\ T5-base & China is a dynasty of China. Paul Qin is the Emperor of China. \\ T5-large & Paul was the Emperor of Russia. The Qin dynasty ruled China. \\ FLAN-T5-base & Paul was the emperor of China. The history of Russia includes the history of Qin. \\ FLAN-T5-large & The Emperor of Russia was Paul the Great. Qin dynasty existed in China and had \\  & history in Russia. \\ InstructGPT & Emperor Paul was part of the Russian dynasty. Qin was part of the Chinese dynasty. \\   

Table 4: Case studies of machine generations. Keywords appearing in the generation results are underlined.

   Model (\# parameters) & COVERAGE & MATCH & BLEU-4 & ROUGE-2 & METEOR & CIDEr & SPICE & BERTScore \\  BART-base (140M) & 78.3 & 60.5 & 22.7 & 29.9 & 29.6 & 18.3 & 53.9 & 48.4 \\ BART-large (400M) & 73.3 & 63.1 & 23.7 & 31.6 & 29.2 & 18.5 & 55.3 & 48.1 \\ T5-base (220M) & 75.6 & 55.3 & 21.9 & 28.7 & 29.8 & 17.4 & 53.6 & 46.2 \\ T5-large (770M) & 81.3 & 67.8 & 26.6 & 33.5 & 31.9 & 21.2 & 57.8 & 51.9 \\ FLAN-T5-base (220M) & 78.0 & 58.7 & 22.3 & 29.5 & 30.6 & 18.2 & 54.7 & 47.6 \\ FLAN-T5-large (770M) & 83.1 & 70.3 & 27.4 & 34.8 & 32.6 & 22.4 & 58.8 & 53.6 \\ geo & 83.1 & 70.8 & 26.8 & 33.9 & 32.4 & 21.9 & 58.2 & 52.8 \\ temp & 83.1 & 67.0 & 31.2 & 40.4 & 34.1 & 22.7 & 62.5 & 59.1 \\  InstructGPT (175B, 10-shot) & **91.8** & **79.6** & **28.4** & **36.3** & **36.1** & **23.4** & **60.9** & **56.4** \\  Human & 98.1 & 92.9 & 39.9 & 46.9 & 40.4 & 39.7 & 71.4 & 65.0 \\   

Table 3: Experimental results on the test set of SituatedGen. The best model performance is in **bold**. Human performance is tested on a subset of 100 random samples.

type is lower than geo, indicating that temporal-dependent test examples are more challenging. However, the amount of temp data is less than geo in the training set, which may also give rise to the performance difference. Interestingly, the generation fluency of geo type is worse than temp, suggesting that it is more difficult to use geo entities to compose sentences smoothly.

Case Study.Table 4 shows two groups of generation examples by different models. The first example belongs to temp type ("24 hours" and "one month") and the second one is geo ("Russia" and "China"). We find that models are prone to omit keywords in their outputs. For example, BART-large only covers 2 out of 6 keywords in the first example. Besides, most of the observed generated outputs are not commonsensical due to incorrect keyword grouping results, e.g., _"There are 24 hours in one month"_. InstructGPT results seem to have the best generation quality and commonsense plausibility among other models, but it still demonstrates incompetence in handling the contrastive relationships between the two sentences.

## 8 Conclusion

In this paper, we introduce the challenging task SituatedGen to incorporate geographical and temporal contexts into generative commonsense reasoning. We build a corresponding testbed to evaluate the situated reasoning capabilities of state-of-the-art text generation models. The benchmark performance shows that models struggle to generate commonsensical sentences and lag far behind humans. Altogether, our data will serve as a challenging benchmark for measuring commonsense knowledge in generative language models and support research progress of constrained commonsense text generation in a more realistic situation.

### Limitations

1. Since our dataset is derived from existing commonsense benchmarks, we may inherit their annotation artifacts  and contain certain types of spurious lexical patterns (e.g., "A lived in B").
2. We do not provide an automatic evaluation of the aspect of contrast between the sentences. A possible solution is to compute the similarity between the entity-masked sentences. This is similar to how we mine contrastive sentences during dataset collection (Figure 2).
3. We could also conduct an extra manual evaluation on the machine generations, so as to gauge its correlation with automatic metrics, though this has been verified by  on the original generative commonsense reasoning task.
4. Recently, a lot of work has developed new retrieval-augmented commonsense text generation models , which could also be included as baseline models for a more comprehensive benchmark.

## Ethics Statement

Our data is built upon publicly available datasets and we will follow their licenses when releasing our data. There is no explicit detail that leaks an annotator's personal information. The dataset has very low risks of containing sentences with toxicity and offensiveness. Since our data is sourced from existing datasets, we may inherit geographical biases  that result in an uneven distribution of commonsense knowledge about western and non-western regions. The commonsense statements may not sound familiar to people who live in locations that are poorly represented in the source datasets. Therefore, models developed on our dataset may preserve biases learned from the annotators of the source datasets. We note that pretrained language models may also inherit the bias in the massive pretraining data. It is important that interested parties carefully address those biases before deploying the model to real-world settings.