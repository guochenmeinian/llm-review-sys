# Private Edge Density Estimation for Random Graphs: Optimal, Efficient and Robust

Hongjie Chen

ETH Zurich

Jingqiu Ding

ETH Zurich

Yiding Hua

ETH Zurich

David Steurer

ETH Zurich

###### Abstract

We give the first polynomial-time, differentially node-private, and robust algorithm for estimating the edge density of Erdos-Renyi random graphs and their generalization, inhomogeneous random graphs. We further prove information-theoretical lower bounds, showing that the error rate of our algorithm is optimal up to logarithmic factors. Previous algorithms incur either exponential running time or suboptimal error rates.

Two key ingredients of our algorithm are (1) a new sum-of-squares algorithm for robust edge density estimation, and (2) the reduction from privacy to robustness based on sum-of-squares exponential mechanisms due to Hopkins et al. (STOC 2023).

## 1 Introduction

Privacy has nowadays become a major concern in large-scale data processing. Releasing seemingly harmless statistics of a dataset could unexpectedly leak sensitive information of individuals (see e.g.  for privacy attacks). Differential privacy (DP)  has emerged as a by-now standard technique for protecting the privacy of individuals with rigorous guarantees. An algorithm is said to be differentially private if the distribution of its output remains largely unchanged under the change of a single data point in the dataset.

For datasets represented by graphs (e.g. social networks), two notions of differential privacy have been investigated in the literature: edge differential privacy , where each edge is regarded as a data point; and node differential privacy , where each node along with its incident edges is regarded as a data point. Node differential privacy is an arguably more desirable notion than edge differential privacy. On the other hand, node differential privacy is also in general more difficult to achieve without compromising on utility, as many graph statistics usually have high sensitivity in the worst case. It turns out that many graph statistics can have significantly smaller sensitivity on typical graphs under natural distributional assumptions. Several recent works could thus manage to achieve optimal or nearly-optimal utility guarantees in a number of random graph parameter estimation problems .

In this paper, we continue this line of work and study perhaps the most elementary statistical task in graph data analysis: Given an \(n\)-node Erdos-Renyi random graph of which each edge is present with probability \(p^{}\) independently, output an estimate \(\) of the edge density parameter \(p^{}\), subject to node differential privacy. We consider the error metric \([/p^{}-1]\) which can reflect the fact that, the task is more difficult for smaller \(p^{}\).

[MISSING_PAGE_FAIL:2]

**Theorem 1.4** (Erdos-Renyi random graphs, combination of Theorem D.1 and Theorem F.1).: _There are constants \(C_{1},C_{2},C_{3}\) such that the following holds. For any \( C_{1}\), \( C_{2}(n)/n\), and \(p^{} C_{3}/n\), there exists a polynomial-time \(\)-differentially node-private algorithm which, given an \(\)-corrupted Erdos-Renyi random graph \((n,p^{})\), outputs an estimate \(\) satisfying_

\[|}{p^{}}-1| O(}{n}}+n}{ n}}+}}),\]

_with probability \(1-n^{-(1)}\)._

The first term \(O(/(n}))\) is the sampling error that is necessary even without privacy or robustness. The second term \(O(^{2}(n)/( n}))\) is the privacy cost of our algorithm, which matches the exponential-time algorithm in . The third term \(O( n/})\) is the robustness cost of our algorithm, which matches the information-theoretical lower bound \((/})\) in \(^{+}22\), Theorem 1.5] up to a \( n\) factor.

Moreover, we provide the following lower bound which shows that the privacy cost of our algorithm is optimal up to a \( n\) factor.2

**Theorem 1.5** (Privacy lower bound for Erdős-Renyi random graphs).: _Suppose there is an \(\)-differentially node-private algorithm that, given an Erdős-Renyi random graph \((n,p^{})\), outputs an estimate \(\) satisfying \(|/p^{}-1|\) with probability \(1-\). Then we must have_

\[( }}).\]

Inhomogeneous random graphs.Given an \(n\)-by-\(n\) edge connection probability matrix \(Q^{}\), the inhomogeneous random graph model \((n,Q^{})\) defines a distribution over \(n\)-node graphs where each edge \(\{i,j\}\) is present with probability \((Q^{})_{ij}\) independently.

We provide a polynomial-time, differentially node-private and robust edge density estimation algorithm for inhomogeneous random graphs.

**Theorem 1.6** (Inhomogeneous random graphs, combination of Theorem D.1 and Theorem E.1).: _Let \(Q^{}\) be an \(n\)-by-\(n\) edge connection probability matrix and let \(p^{}:=_{i,j}Q^{}_{ij}/(n^{2}-n)\). Suppose \(\|Q^{}\|_{} Rp^{}\) for some \(R\). There is a sufficiently small constant \(c\) such that the following holds. For any \(\) such that \((1/)R c\), there exists a polynomial-time \(\)-differentially node-private algorithm which, given an \(\)-corrupted inhomogeneous random graph \((n,Q^{})\), outputs an estimate \(\) satisfying_

\[|}{p^{}}-1| O(}{n}}+n}{ n}+R(1/) ),\]

_with probability \(1-n^{-(1)}\)._

We improve on the previous private edge density estimation algorithm for inhomogeneous random graphs by Chen et al. , Lemma 4.10]. Their algorithm is based on  and has privacy cost \((R/( n)+1/(^{2}nd^{}))\), while our algorithm only has privacy cost \((R/( n))\). To the best of our knowledge, even without privacy requirement and in the special case of Erdős-Renyi random graphs, no previous algorithm can match our guarantees in the sparse regime. Specifically, when \(d^{} n\) and \((1)\), our algorithm can provide a constant-factor approximation of \(d^{}\), while the best previous robust algorithm \(^{+}22\) can not.

We also provide matching lower bounds, showing that the guarantee of our algorithm in Theorem 1.6 is optimal up to logarithmic factors.

**Theorem 1.7** (Robustness lower bound for inhomogeneous random graphs).: _Suppose there is an algorithm satisfies the following guarantee for any symmetric matrix \(Q^{}^{n n}\). Given an \(\)-corrupted inhomogeneous random graph \((n,Q^{})\), the algorithm outputs an estimate \(\) satisfying \(|/p^{}-1|\) with probability at least \(0.99\), where \(p^{}=_{i,j}Q^{}_{ij}/(n^{2}-n)\). Then we must have \((R)\), where \(R=_{i,j}Q^{}_{ij}/p^{}\)._

**Theorem 1.8** (Privacy lower bound for inhomogeneous random graphs).: _Suppose there is an \(\)-differentially node-private algorithm satisfies the following guarantee for any symmetric matrix \(Q^{}^{n n}\). Given an inhomogeneous random graph \((n,^{})\), the algorithm outputs an estimate \(\) satisfying \(|/p^{}-1|\) with probability \(1-\), where \(p^{}=_{i,j}Q^{}_{ij}/(n^{2}-n)\). Then we must have_

\[()\,,\]

_where \(R=_{i,j}Q^{}_{ij}/p^{}\)._

### Techniques

We give an overview of the key techniques used to obtain our algorithm. As our techniques for Erdos-Renyi random graphs can be easily extended to the more general inhomogeneous random graph model, we will focus on Erdos-Renyi random graphs to avoid a proliferation of notation. Specifically, given an \(\)-corrupted Erdos-Renyi random graph \((n,d^{}/n)\), our goal is to output a private estimate of \(d^{}\).

Reduction from privacy to robustness.Hopkins et al.  and Asi et al.  independently discovered the following black-box reduction from privacy to robustness. Given a robust algorithm \(_{}\), one can directly obtain a private algorithm via applying the exponential mechanism  with the following score function,

\[(d;A):=_{A^{}}\{(A^{},A)\,:\, |_{}(A^{})-d| 1/(n) \}\,,\] (1.1)

where \(A\) is the adjacency matrix of input graph and \(d\) is a candidate estimate. For privacy analysis, note that the sensitivity of the above score function is bounded by \(1\), as the node distance between neighboring graphs is at most \(1\). For utility analysis, when the input graph is a typical Erdos-Renyi random graph, the exponential mechanism will with high probability output a \(\) of score \(O((n)/)\). Then we can argue that such a \(\) is close to \(d^{}\) using the robustness of \(_{}\). For example, if we plug in the robust algorithm in K+22, Theorem 1.3, then the corresponding exponential mechanism will only incur a privacy cost of \((1/( n}))\).

However, directly plugging in the robust algorithm in K+22 will lead to an exponential-time algorithm, as a single evaluation of the score function requires enumerating all \(n\)-node graphs. To obtain a polynomial-time algorithm, we develop a new robust algorithm via the _sum-of-squares_ method.3

Robust algorithm via sum-of-squares.The sum-of-squares method uses convex programming (in particular, semidefinite programming) to solve polynomial programming. It is a very powerful tool for designing polynomial-time robust estimators (see ). To obtain a robust algorithm via sum-of-squares, we first identify a set of polynomial constraints that a typical (uncorrupted) Erdos-Renyi random graph would satisfy. Specifically, these polynomial constraints encode the following regularity conditions: (1) the degrees of the nodes are highly concentrated, and (2) the centered adjacency matrix is spectrally bounded.

We also include the constraint that at most \(\) fraction of the nodes in the graph are corrupted. Then we give a proof that if a graph satisfies the above constraints, then its average degree will be close to \(d^{}\), even when \(\) fraction of nodes in the input graph are arbitrarily corrupted. Importantly, the proof is simple enough that it is captured by the sum-of-squares proof system (see ). This allows us to extend the utility guarantee of the polynomial program to its semidefinite programming relaxation, which results in a polynomial-time robust algorithm.

Sum-of-squares exponential mechanism.Given the above robust algorithm, we then use the sum-of-squares exponential mechanism developed in  to obtain a private algorithm. More specifically, we apply the exponential mechanism with the sum-of-squares relaxation of the score function in Eq.1. In this way, we obtain a private algorithm that is also robust to adversarial corruptions.

### Notation

We introduce some notation used throughout this paper. We write \(f g\) to denote the inequality \(f C g\) for some absolute constant \(C>0\). We write \(O(f)\) and \((f)\) to denote quantities \(f_{-}\) and \(_{+}\) satisfying \(f_{-} f\) and \(f f_{+}\) respectively. We use boldface to denote random variables, e.g., \(,,\). For a matrix \(M\), we use \(\|M\|_{}\) for the spectral norm of \(M\). Let \(\) and \(0\) denote the all-one and all-zero vector respectively, of which the size will be clear from the context. We use a graph \(G\) and its adjacency matrix \(A=A(G)\) interchangeably when there is no ambiguity. For an \(n\)-by-\(n\) matrix \(M\), we use \(d(M)\) to denote its average row/column sum, i.e., \(d(M)=_{i,j}M_{ij}/n\). For any matrices (or vectors) \(M,N\) of the same shape, we use \(M N\) to denote the element-wise product (aka Hadamard product) of \(M\) and \(N\).

### Organization

The rest of the paper is organized as follows. In Section2, we give a proof overview of our results and defer full proofs to the appendices. The appendices are organized as follows. We provide some sum-of-squares background in AppendixA and some concentration inequalities for random graphs in AppendixB. In AppendixC, we present a general sum-of-squares exponential mechanism that all of our private algorithms in this paper are based on. In AppendixD, we present our coarse estimation algorithm and give a full proof of its guarantees (TheoremD.1). In AppendixE, we present our fine estimation algorithm for inhomogeneous random graphs and give a full proof of its guarantees (TheoremE.1). In AppendixF, we present our fine estimation algorithm for Erdos-Renyi random graphs and give a full proof of its guarantees (TheoremF.1). All lower bounds are proved in AppendixG.

## 2 Private and robust algorithm for Erdos-Renyi random graphs

In this section, we describe our private and robust algorithm for Erdos-Renyi random graphs. We also give an overview of the analysis of our algorithm and sketch the proof of our lower bounds.

Our overall algorithm consists of two stages. In the first stage, we compute a coarse estimate that approximates the edge density parameter within constant factors. In the second stage, we improve the accuracy of this coarse estimate to the optimum. Since our algorithm is private in both stages, it is also private overall by the composition theorem of differential privacy (see [13, Section 3.5]).

We remark that for the Erdos-Renyi random graph model \((n,p^{})\), estimating its edge density parameter \(p^{}\) is equivalent to estimating its expected average degree \(d^{}:=np^{}\).4 For the convenience of notation, we set our goal as estimating the expected average degree \(d^{}\) throughout this section.

### General algorithm framework

Given an \(n\)-by-\(n\) symmetric matrix \(A\) and a scalar \(\), let \((Y,z;A,)\) be a polynomial system with indeterminates \(Y=(Y_{ij})_{i,j[n]}\) and \(z=(z_{i})_{i[n]}\) that encodes the node distance between \(Y\) and \(A\):

\[(Y,z;A,):=.z z=z,\  1,z (1-)n\\ 0 Y 1^{},\ Y=Y^{}\\ Y zz^{}=A zz^{}\}.\] (2.1)

Let \((Y)\) be A polynomial system that encodes regularity conditions of Erdos-Renyi random graphs. The key observation here is that, for any \(Y\{0,1\}^{n n}\) and \(z\{0,1\}^{n}\) that satisfy constraints in \((Y,z;A,)(Y)\), \(Y\) is a graph that behaves like Erdos-Renyi random graphs (in the sense of the regularity conditions) and is within node distance \( n\) to \(A\) where they agree on \(\{i[n]\,:\,z_{i}=1\}\).

The key ingredient of our result is that, given proper regularity conditions \((Y)\), we can give degree-8 sum-of-squares proofs: for any \(Y\) that satisfies constraints in \((Y,z;A,)(Y)\), the average degree of \(Y\) is close to the expected average degree \(d^{}\), even when the input graph \(A\) is a \(\)-corrupted Erdos-Renyi random graph \((n,d^{}/n)\). As a result of the sum-of-squares proofs-to-algorithms framework (see Theorem A.6), we can get an efficient and robust estimator \(}[d(Y)]\), where \(}\) is a pseudo-expectation obtained by solving level-8 sum-of-squares relaxation of \((Y,z;A,)(Y)\).

Based on the above identifiability proof for robust estimation, we design a private and robust algorithm by applying the exponential mechanism5 with the following score function:

\[(d;A):=_{0 1} n\ \ }\] (2.2)

Similar to Eq. (1.1), it is easy to observe this exponential mechanism is private.

**Lemma 2.1** (Privacy).: _Consider the distribution \(_{A,}\) with support \([0,n]\) and density_

\[_{A,}(d)(- (d;A))\,\] (2.3)

_where \((d;A)\) is defined in Eq. (2.2). A sample from \(_{A,}\) is \(2\)-differentially private._

Proof.: Since the node distance between neighboring graphs is at most 1, the sensitivity of the following score function is bounded by 1:

\[(d;A):=_{0 1} n\ \ (Y,z;A,)\ \ (Y)\ \ |d(Y)-d| 1/(n)}\]

One can show that such sensitivity bound is inherited by its sum-of-squares relaxation sos-score as defined in Eq. (2.2). By a standard sensitivity-to-privacy argument (see e.g. [14, Theorem 3.10]), the exponential mechanism is \(2\)-differentially private. 

To analyze the utility of the private algorithm, we use the robustness of the score function. Assume the input graph is uncorrupted for simplicity. For a typical Erdos-Renyi random graph \(A^{}(n,d^{}/n)\), we have \(d^{},A^{}=0\). By a standard volume argument (see e.g. [14, Theorem 3.11]), the exponential mechanism with high probability outputs a scalar \(d\) satisfying \((d;A^{})(n)/\). By the definition of our score function in Eq. (2.2), this implies that there exists a level-8 pseudo-distribution satisfying \((Y,z;A^{},)\ \ (Y)\) with \((n)/( n)\). The utility then follows from the above identifiability proof for robust estimation.

### Coarse estimation

In this part, we describe a private and robust algorithm that can estimate the expected average degree \(d^{}\) within a constant approximation ratio.

**Theorem 2.2** (Coarse estimation algorithm, informal restatement of Theorem D.1).: _For \(\) smaller than some constant, there is a polynomial-time \(\)-differentially node-private algorithm which, given an \(\)-corrupted Erdos-Renyi random graph \((n,d^{}/n)\), outputs an estimate \(\) such that \(|-d^{}| 0.5d^{}\)._

We give a proof sketch of Theorem 2.2 at the end of this subsection. The formal theorem and proofs are deferred to Appendix D.

Identifiability proof for robust estimation.We first give a polynomial system that can identify the expected average degree \(d^{}\) up to constant factors, even when \(\)-fraction of nodes are corrupted. Consider the following regularity condition on degrees:

\[(Y):=\{(Y)_{i} 2(1/) d(Y)\,,  i[n]\}\,.\] (2.4)

The following lemma shows that Erdos-Renyi random graphs satisfy \((Y,z;A,2)(Y)\) with high probability.

**Lemma 2.3** (Feasibility).: _Let \(A^{}(n,d^{}/n)\) and let \(A\) be an \(\)-corrupted version of \(A^{}\). With high probability, there exists a graph \(Y\) that satisfies the constraints in \((Y,z;A,2)(Y)\)._

Proof sketch.: For \(d^{}(n)\), the maximum degree of \(A^{}\) is of order \(O(d^{})\). Therefore, the uncorrupted graph \(A^{}\) satisfies the constraints. For \(d^{} n\), using concentration properties of random graphs, we can show that the number of high degree nodes is bounded by \( n\). A feasible graph can then be obtained from the uncorrupted graph \(A^{}\) by trimming these highest degree nodes. 

Next, we show that these polynomial constraints give an identifiability proof for the expected average degree \(d^{}\).

**Lemma 2.4** (Identifiability).: _Let \(A^{}(n,d^{}/n)\) and let \(A\) be an \(\)-corrupted version of \(A^{}\). For \(\) smaller than some constant and \( O()\), with high probability there is a degree-\(8\) sum-of-squares proof that, if \(Y\) satisfies \((Y,z;A,)(Y)\), then \(|d(Y)-d^{}| 0.001d^{}\)._

Proof sketch.: We first assume that \(d^{}(n)\), for which the proof is simpler. By the degree-bound constraint \((Y)\), we have \(n|d(Y)-d(A^{})| 2(1/)(d(Y)+d^{}) (Y,A^{})\). Using the constraints \(Y zz^{}=A zz^{}\) and \( 1,z(1-)n\), we have \((Y,A) n\). Since \((A,A^{}) n\), by triangle inequality, we have \((Y,A^{})(+)n\). Therefore, we have \(|d(Y)-d(A^{})| 0.0001d^{}\) when \(,\) are at most some small constants. Finally, by random graph concentration, we have \(|d^{}-d(A^{})|(d^{})\) with high probability. Therefore, we have \(|d(Y)-d(A^{})| 0.001d^{}\).

To deal with the sparse regime where \(d^{} n\), we need to truncate the nodes of \(A^{}\) with degree \(((1/)d^{})\). Our key observation is that, the average degree of the graph before and after truncation only differ by a constant factor. Therefore, we can still get \(|d(Y)-d(A^{})| 0.001d^{}\).

Furthermore, it can be shown that this proof is a degree-\(8\) sum-of-squares proof. 

Robust algorithm via sum-of-squares.Consider the algorithm that finds a level-\(8\) pseudo-expectation satisfying \((Y,z;A,2)(Y)\) --with \((Y)\) given in Eq. (2.4)-- and outputs \(}[d(Y)]\). By Lemma 2.3, such a pseudo-expectation \(}\) exists with high probability. It follows from the sum-of-squares identifiability proof in Lemma 2.4 that \(|}[d(Y)]-d^{}| 0.001d^{}\). Moreover, the algorithm can be implemented by semidefinite programming and run in polynomial time.

Private and robust algorithm via sum-of-squares exponential mechanism.We present our private and robust algorithm in Algorithm 2.5 and give a proof sketch of Theorem 2.2.

``` Input:\(\)-corrupted Erdos-Renyi random graph \(A\). Privacy parameter:\(\). Output: A sample from the distribution \(_{A,}\) with support \([0,n]\) and density \[_{A,}(d)(-( d;A))\,,\] (2.5) where \[(d;A):=_{0 1} n}\\ (Y,z;A,)\,\,(Y)\,\,\{|d(Y)-d|  1/(n)\},\] (2.6) with \((Y)\) given in Eq. (2.4). ```

_Proof sketch of Theorem 2.2. Privacy._ By Lemma 2.1, Algorithm 2.5 is \(2\)-differentially private.

_Utility._ For simplicity, we consider the case when there is no corruption (i.e. \(=0\)). The analysis for the case when \(>0\) is similar. Let \(A^{}(n,d^{}/n)\). Then with high probability \((d^{};A^{})=0\). By a standard volume argument, Algorithm 2.5 outputs a scalar \(d\) that satisfies \((d;A^{})(n)/\) with high probability. By the definition of \(\) in Eq. (2.6), this implies that there exists a level-8 pseudo-distribution satisfying \((Y,z;A,)\,\,(Y)\,\,\{|d(Y)-d|  1/(n)\}\) with \((n)/( n)\). When \((n)/( n)\) is at most a small constant, it follows from our sum-of-squares identifiability proof in Lemma 2.4 that, Algorithm 2.5 outputs a constant-factor approximation of \(d^{}\) with high probability. \(\)

### Fine estimation

From Section 2.2, we know how to obtain a constant-factor approximation of \(d^{}\) privately and robustly. In this section, we show how to improve the accuracy to the optimum.

**Theorem 2.6** (Fine estimation algorithm, informal restatement of Theorem F.1).: _Let \(0.5d^{} 2d^{}\). For \(\) smaller than some constant, there is a polynomial-time \(\)-differentially node-private algorithm which, given an \(\)-corrupted Erdos-Renyi random graph \((n,d^{}/n)\) and \(\), outputs an estimate \(\) such that_

\[|}{d^{}}-1|(}}+}}+}}).\]

We give a proof sketch of Theorem 2.6 at the end of this section. The formal theorem and proofs are deferred to Appendix F.

Identifiability proof for robust estimation.We first give a polynomial system which can identify the expected average degree \(d^{}\) with optimal error rate, when provided with a coarse estimate \(\). Consider the following regularity conditions on degrees and eigenvalues:

\[(Y):=|(Y1)_{i}-d(Y)|} n\,,  i[n]\\ \|Y- 1\,^{}\|_{} } n.\] (2.7)

**Lemma 2.7** (Feasibility).: _Let \(A^{}(n,d^{}/n)\) and let \(A\) be an \(\)-corrupted version of \(A^{}\). Suppose \(d^{}/2 2d^{}\). Then with high probability, there exists a graph \(Y\) that satisfies the constraints in \((Y,z;A,)(Y)\)._

Proof.: By Chernoff bound, with high probability, the degree of each node in \(A^{}\) deviates from \(d^{}\) by at most \(O(} n)\). By the concentration of the spectral norm of random matrices , with high probability, we have \(\|A^{}-)}{n}\,^{}\|_{}  n}\). Hence, \((Y,z;A,)(Y)\) is satisfied by \(Y=A^{}\) and \(z=z^{}\) where \(z^{}\) is the indicator vector for uncorrupted nodes. 

Next we give a sum-of-squares identifiability proof for expected average degree estimation with optimal accuracy.

**Lemma 2.8** (Identifiability).: _Let \(A^{}(n,d^{}/n)\) and let \(A\) be an \(\)-corrupted version of \(A^{}\). Suppose \(d^{}/2 2d^{}\). For \(\) smaller than some constant and \( O()\), with high probability there is a degree-\(8\) sum-of-squares proof that, if \(Y\) satisfies \((Y,z;A,)(Y)\), then_

\[|}-1|(}}+}})\,.\]

Proof sketch.: Let \(Y_{1},Y_{2}\) be two graphs satisfying the regularity condition \((Y_{1})\) and \((Y_{2})\) as described in Eq. (2.7), respectively. We give sum-of-squares proof that, if \((Y_{1},Y_{2}) n\) and \(\) is at most some small constant, then \(|d(Y_{1})-d(Y_{2})|} n\).

Let \(w\{0,1\}^{n}\) be the indicator vector for the shared induced subgraph between \(Y_{1}\) and \(Y_{2}\), i.e \(Y_{1} ww^{}=Y_{2} ww^{}\). When \((Y_{1},Y_{2}) n\), we have \( w,(1-)n\). We have

\[n(d(Y_{1})-d(Y_{2}))=  Y_{1}-Y_{2},\,\,^{}\] \[=  Y_{1}-Y_{2},\,\,^{}-ww^{}\] \[=  Y_{1}-)}{n}\,\,^{}+ )}{n}\,\,^{}-)}{n}\, \,^{}+)}{n}\,\,^{ }-Y_{2},\,\,^{}-ww^{}\] \[=  Y_{1}-)}{n}\,\,^{}, \,^{}-ww^{}+)}{n}\,\, ^{}-Y_{2},\,\,^{}-ww^{}\] \[+)}{n}\,\,^{}-)}{n}\,\,^{},\,^{}-ww^{}\,.\]

By rearranging terms, we can get

\[}{n}d(Y_{1})-d(Y_{2})= Y_{1}- )}{n}\,\,^{},\,\,^{ }-ww^{}+)}{n}\,\,^{ }-Y_{2},\,\,^{}-ww^{}\,.\]

For the first term \( Y_{1}-)}{n}\,\,^{},\, \,^{}-ww^{}\), we have

\[ Y_{1}-)}{n}\,\,^{},\, \,^{}-ww^{}=2 Y_{1}-)}{n}\,\, ^{},\,(1-w)^{}+)}{n} \,\,^{}-Y_{1},(1-w)(1-w)^{}\,.\]

From constraints \(|(Y_{1})_{i}-d(Y_{1})|}(n)\) for all \(i[n]\), we have

\[ Y_{1}-)}{n}\,\,^{},\, (1-w)^{}= Y_{1}\,-d(Y_{1})\,,\,1-w  n(n)}\,.\]

From constraints \(\|Y_{1}-)}{n}\,\,^{}\|_{ }}\), we have

\[)}{n}\,\,^{}-Y_{1},(1-w)(1-w)^{ }\|Y_{1}-)}{n}\,\,^{ }\|_{}\,\|1-w\|_{2}^{2} n} (n)\,,\]

The same bounds also apply for the second term \( Y_{2}-)}{n}\,\,^{},\, \,^{}-ww^{}\). Since \( 1,w(n)\), it follows that \(|d(Y_{1})-d(Y_{2})|(})( })\).

Since the original uncorrupted graph satisfies the regularity conditions, this gives the identifiability proof that \(|d(Y)-d(A^{})|(})\). By random graph concentration, with high probability, we have \(|d^{}-d(A^{})|(/n})\). The claim thus follows.

Robust algorithm via sum-of-squares.Consider the algorithm that finds a level-8 pseudo-expectation satisfying \((Y,z;A,n)\,\,(Y)\) --with \((Y)\) given in Eq.2.7)-- and outputs \(}[d(Y)]\). By Lemma2.7, such a pseudo-expectation \(}\) exists with high probability. It follows from the sum-of-squares identifiability proof in Lemma2.8 that \(|}[d(Y)]/d^{}-1|(1/}+ /})\). Moreover, the algorithm can be implemented by semidefinite programming and run in polynomial time.

Private and robust algorithm via sum-of-squares exponential mechanism.We present our private and robust algorithm in Algorithm2.9 and give a proof sketch of Theorem2.6.

``` Input:\(\) corrupted random graph \(A\), \(\)-differentially private coarse estimate \(\). Privacy parameter:\(\). Output: A sample from the distribution \(_{A,}\) with support \([0,n]\) and density \[_{A,}(d)(- (d;A))\,,\] (2.8) where \((d;A)\) is defined as \[(d;A):=_{0 1} n}\\ (Y,z;A,)\,\,(Y)\,\,|d(Y)-d|  1/(n)}\,,\] (2.9) with \((Y)\) given in Eq.2.7. ```

**Algorithm 2.9** (Private fine estimation for Erdos-Renyi random graphs).

Proof sketch of Theorem2.6.: _Privacy._ By Lemma2.1, Algorithm2.9 is \(2\)-differentially private. _Utility._ For simplicity, we consider the case when there is no corruption (i.e. \(=0\)). The analysis for the case when \(>0\) is similar. Let \(A^{}(n,d^{}/n)\). Then with high probability \((d^{};A^{})=0\). By a standard volume argument, Algorithm2.9 outputs a scalar \(d\) that satisfies \((d;A^{})(n)/\) with high probability. By the definition of \(\) in Eq.2.9, this implies that with high probability there exists a level-8 pseudo-distribution satisfying \((Y,z;A,)(Y)\) with \((n)/( n)\). Taking \(=(n)/( n)\) in Lemma2.8, it follows that Algorithm2.9 outputs an estimate \(\) such that \(|/d^{}-1|(1/}+1/( n }))\) with high probability. 

### Lower bound

We sketch the proof of Theorem1.5. Let \(\) and \(d=(1-)d^{}\). We can construct a coupling \(\) between the distributions \((n,\,d/n)\) and \((n,d^{}/n)\) with the following property. For \((,^{})\), we have \((,^{})\) bounded by \(( n})\) with overwhelmingly high probability. By the definition of differential privacy, when \( n} 1/(n)\), the output of an \(\)-differentially private algorithm are indistinguishable under \((n,d/n)\) and \((n,d^{}/n)\). Therefore, by setting \(=(1/ n})\), we conclude that no \(\)-differentially private algorithm can achieve error rate better than \((1/ n})\). This provides a matching lower bound for our private edge density estimation algorithm.