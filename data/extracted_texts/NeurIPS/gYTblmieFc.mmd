# Connecting Neural Models Latent Geometries with

Relative Geodesic Representations

 Hanlin Yu

Department of Computer Science

University of Helsinki

hanlin.yu@helsinki.fi

&Berfin Inal

Faculty of Science

University of Amsterdam

s.berfininal@gmail.com

&Marco Fumero

Department of Computer Science

Institute of Science and Technology Austria

marco.fumero@ist.ac.at

###### Abstract

Neural models learn representations of high dimensional data that lie on low dimensional manifolds. Multiple factors, including stochasticities in the training process, may induce different representations, even when learning the same task on the same data. However, when there exist a latent structure shared between different representational spaces, it has been showed that is possible to model a transformation between them. In this work, we show how by leveraging the differential geometrical structure of latent spaces of neural models, it is possible to capture precisely the transformations between distinct latent spaces. We validate experimentally our method on autoencoder models and real pretrained foundational vision models across diverse architectures, initializations and tasks.

## 1 Introduction

Recent research reveals that neural models often develop similar internal representations when exposed to similar inputs, a phenomenon observed in both biological [24; 17] and artificial systems [26; 32; 19; 21]. Remarkably, even when models have different architectures, their internal representations can frequently be aligned through a simple transformation, for example linear [27; 25; 30]. This suggests a certain consistency in how NNs encode information, emphasizing the importance of studying these internal representations, and the transformations that relate them. One strategy to do this is to identify representations that are _invariant_ to transformations between distinct models representational spaces. A simple and effective recipe to do this is the one of relative representations , where samples are represented as a function of a fixed set of latent representations. The similarity function employed is cosine similarity, hinting at the fact that representations across distinct models are subject to _angle preserving_ transformations. However, the choice of similarity function should not be limited to capture invariances to one class of transformations. As shown in , other choices can be good as well, and there's not a clear best choice among simple class of transformations for capturing transformation across distinct latent spaces.

In this paper we employ geodesic distance in the latent space as a metric for relative representations. This approach ensures that the relative space remains approximately invariant to the isometries of the data's manifold, as characterized by a Riemannian structure. Our contributions can be summarized as follows: (i) We propose a new representation that capture the isometric transformation between data manifolds learned by distinct models. (ii) We propose to employ an approximation of the underlying metric both for classification and reconstruction tasks. (iii) We test relative geodesics on retrieval and stitching tasks on autoencoders and real vision foundation models, across different seeds, architectures and training strategies, outperforming previous methods.

## 2 Method

### Notation and Background

Neural networks (NNs) can be viewed as parametric functions \(F_{}\), which are composed of an _encoding_ map and a _decoding_ map, represented as \(F_{}=D_{_{2}} E_{_{1}}\). The encoder \(E_{_{1}}:\) generates a latent representation \(z=E_{_{1}}(x)\), where \(x\) to the input domain \(\), and the latent space \(\). The decoder \(D_{_{2}}\) is responsible for performing the task at hand, such as reconstruction or classification. For simplicity, we will omit the parameter dependence (\(\)) in our notation moving forward. For any single module \(E\) (or equivalently \(D\)), we will use \(E_{}\) to denote that the module \(E\) was trained on the domain \(\). In the next sections, we will provide the necessary background to introduce our method.

**Latent Space Communication** Given a pair of domains \(,\), a pair of neural models trained on them \(F^{1}_{},F^{2}_{}\), and a partial correspondence between the domains \(:_{}_{}\) where \(_{}\) and \(_{}\), the problem of _latent space communication_ is the one of finding a full correspondence \(:E^{1}() E^{2}()\) between the two domains, from \(\). In a simplified setting, e.g. two models trained with different initialization or architectures on the same data \(=\) and the correspondence is the identity. When \(\) the problem becomes multimodal.

**Relative representations** The relative representations framework  provides a straightforward approach to represent each sample in the latent space according to its similarity to a set of fixed training samples, denoted as _anchors_. Representing samples in the latent space as a function of the anchors corresponds to transitioning from an absolute coordinate frame into a _relative_ one defined by the anchors and the similarity function. Given a domain \(\), an encoding function \(E_{}:\), a set of anchors \(_{}\), and a similarity or distance function \(d:\), the _relative representation_ for a sample \(x\) is:

\[RR(z;_{},d)=_{a_{i}_{} }d(z,E_{}(a_{i})),\]

where \(z=E_{}(x)\), and \(\) denotes row-wise concatenation. In , \(d\) was set as cosine similarity. This choice induces a representation invariant to _angle-preserving transformations_. In this work, our focus is to _leverage the intrinsic geometry of latent spaces to capture isometric transformations between data manifolds approximations._

**Latent space geometry** For the latent space of a neural network, it is in general hard to reason about its Riemannian structure. However, it is often easier to assign a Riemannian structure to the output space. As such, one can define a _pullback metric_ from the output space to the latent space, which is a standard operation in Riemannian geometry (see e.g. Ch.2.4 of ).

Formally, considering the decoder \(D:\) takes as input a latent representations \(z\) and outputs \(x\). Given a Riemannian metric defined on \(x\) as \(G_{}(x)\). Then, the Riemannian metric at \(z\) can be obtained as

\[G_{}(z)=()^{}G_{ }(x)=J_{z}(D)^{T}J_{z}(D),\]

where \(J_{z}(D)\) is the Jacobian of \(D\) evaluated at \(z\). The metric tensor \(G_{}\) is useful to compute quantities such lengths, angles and areas on \(\). Given a smooth curve \(:[a,b]\) one can compute the energy \(\) of \(\) as follows 

\[()=_{a}^{b}v(t)^{}G(t)v(t)^{}t,\] (1)

where \(v(t)=(t)\). This can be approximated using finite difference approaches. Geodesics are minimizers of this energy .

### Relative geodesics representations

When considering a differential geometry perspective, the problem of latent space communication can be interpreted as finding a transformation between the data manifolds \(_{1},_{2}\) approximated by two neural models \(F_{1},F_{2}\). The relative representation framework can capture this transformation implicitly if equipped with the right metric. A natural candidate for this metric is the geodesic distance defined on \(_{1},_{2}\), respectively. This choice make the relative representations invariant to isometric transformation of the manifolds \(_{1},_{2}\). However, for high dimensional problems, the high cost of computing the geodesic renders the above methods inappropriate [34; 9]. Furthermore, one can argue against directly using the latentgeometry induced by deterministic models from a theoretical perspective , as it may result in undesirable properties, e.g. the resulting geodesics going outside of the data manifold.

We therefore consider using the approximate curve energy / distance of the straight line (in the Euclidean sense) connecting the representations in the latent space:

\[RR^{geo}(z;_{})=_{a_{i}_{ }}((z,E_{}(a_{i})))\]

where \((z_{1},bz_{2})=(1-)z_{1}+ z_{2}\) is the convex combination between the points \(z_{1},z_{2}\). Further descriptions on our method for obtaining the geometric representations can be found in Section A.1.

## 3 Experiments

In the following we will evaluate relative geodesic representations on the latent communication problem across models trained with different initializations, different architectures, and tasks.

### Aligning neural representational spaces trained independently

**Experimental setting** For the following experiment we trained pairs of convolutional autoencoders \((F_{1},F_{2})\) with different initializations on the MNIST, FashionMNIST, CIFAR10 datasets. The architecture of the convolutional autoencoder is detailed in the Appendix. After training we extracted 10k samples from the test set, and map them to the latent spaces of the two models, to representations \(_{1}=E_{1}(),_{2}=E_{2}()\) respectively. Starting from a small set of anchors in correspondence \(_{}_{}\), the objective is to evaluate how well from the relative representations is possible to recover the full correspondence between the representations \(_{1},_{2}\). As baseline we compare with relative representations using cosine similarity .

**Analysis of results** In Figure 1 we plot the performance in terms of MRR on MNIST,FashionMNIST, CIFAR10 datasets. To obtain the score we first compute similarity matrices between relative representations of the two spaces as \((_{1},_{2})\) where \(_{i,j}=_{1})^{T}RR(_{2})_{j}}{\|RR( _{1})_{1}\|_{2}\|RR(_{2})_{j}\|_{2}}\). Then we compute the Mean Reciprocal Rank (MRR, see Appendix A.2.1) on top of the similarity matrix. In the figure we plot MRR as a function of a random set of anchors, where the shaded areas indicate the standard deviations over 5 different set of random anchors with the same cardinality. Our method consistently performs better than Relative Representation, saturating the score with few anchors on all the domains, despite the different degree of complexity of the latent spaces. In addition, our method show way less variance in the result, being more robust to the choice of the anchor set.

**Takeaway**: Relative geodesic representation capture almost perfectly the transformations between representational spaces of models initialized differently, outperforming  in terms of number of anchors needed and robustness.

### Stitching autoencoder models

**Experimental setting** For this experiment we consider the same pairs of autoencoders trained on the MNIST, FashionMNIST, CIFAR10 datasets of section 3.1. Starting from a set of five random anchors we want to

Figure 1: _Aligning latent spaces of autoencoders_: MRR score as a function of the number of anchors on pairs of autoencoders trained with different initializations on the MNIST (left), FashionMNIST (center), CIFAR10 (right) datasets respectively. In green, we plot the performance of , in blue, our method. Shaded area indicates standard deviation across different random set of anchors. Relative geodesics consistently outperform the cosine baseline, obtaining peak performance.

estimate a transformation \(T\) between the model representational spaces \(Z_{1},Z_{2}\). Differently from , in which zero shot stitching was achieved by training once a decoder module with relative representations and then exchanging different encoder modules, here we achieve stitching without training any decoder. We compute relative representation with respect to the set of anchors, and then compute a similarity matrix \((_{1},_{2})\). Then we compute the vector \(=*{arg\,max}_{i}()\) representing a correspondence between the two representations matrices \(_{1}\), \(_{2}\), and use \(c\) to fit a linear transformation \(T\) to approximate the transformation between the two domains. We perform stitching by performing the following operation for a sample \(x\): \(=D_{2} T E_{1}(x)\).

**Analysis of results** We visualize the results of reconstructions of random samples in Figure 2, comparing with . For each dataset, each column represents respectively: (i) the original autoencoding mapping for a sample \(x\) of model \(F_{1}\), \(D_{1}(E_{1}(x))\) (ii) \(D_{2}(E_{2}(x))\) (iii) the mapping \(D_{2}(E_{1}(x))\) (iv) the mapping \(D_{2}(T_{anchors}E_{1}(x))\) where \(T_{anchors}\) is estimated on the five available anchors, (v) the mapping \(D_{2}(T_{cosine}E_{1}(x))\) where \(T_{cosine}\) is estimated among all 10k samples with the correspondence \(c\) obtaining in the relative space of  (vi) Our result \(D_{2}(T_{relgeo}E_{1}(x))\) where \(T_{relgeo}\) is estimated from the correspondence obtained in the relative geodesic space. While the baselines do not reach a good enough reconstruction quality, reconstructions with our method are almost perfect in accordance with the results in Figure 1.

**Takeaway**: The relative geodesic space enables to stitch together neural modules trained on different seeds.

### Zero shot Stitching of vision foundation models

**Experimental setting** We perform experiments on pretrained classifiers from Hugging Face, investigating the accuracies of stitching together different backbones with classfication heads, on CIFAR10 dataset. For this experiment we follow the stitching procedure of , section 5. We consider ResNet-50 , Vision Transformers (ViT) , with both patch 16-224 and patch 32-384, and DinoV2 . These models differ in architecture and pretraining tasks (classification, self supervised contrastive learning). We mainly compare cosine relative representation (cos), relative geodesic representation of the curve length assuming Euclidean geometry on the logits with \(20\) discretization steps of Equation 1 (geo1) and directly using the distance of the corresponding logits (geo2) as relative representations, corresponding to 1 discretization step.

Figure 3: _Stitching of vision foundation models_: We visualize the accuracies of models stitched together on a classification task on CIFAR10. We plot cos (left), geo2 (center), geo1 (right) representations. geo1 results in accuracies that are not significantly degraded even when performing model stitching.

Figure 2: _Stitching on Autoencoders_: We visualize qualitative reconstructions of samples, stitching autoencoders of models trained with different initializations on MNIST (left), FashionMNIST (center), CIFAR10 (right). The first two column shows reconstructions from the original models; middle columns represent baselines ; the rightmost column is our method. Relative geodesics yield the best stitching results using just 5 anchors.

**Analysis of results** The accuracies are shown in Figure 3. We plot confusion matrices of accuracies indicating that the performance of stitching the backbone of model on each with the classification head of each column. The accuracies are shown in Figure 3, while the MRR with respect to cosine similarity are shown in Figure 4.

**Takeaway:** Using geometric relative representations yields better accuracies, avoiding downgrading of performance when performing model stitching.

## 4 Conclusions

In this work we explored the framework of relative representation equipped with geodesic energy to capture the trasformations occuring between neural manifold learn by distinct neural architecture. We demonstrated superior performance. As limitation we observe that the evaluation results depend on the number of discretization steps when evaluating the representations. Future steps include exploring the multimodal case, when \(\), different formulation of the energy, and considering different architectures e.g. VAEs as in .