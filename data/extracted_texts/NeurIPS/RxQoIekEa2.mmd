# Statistical and Geometrical Properties of Regularized Kernel Kullback-Leibler Divergence

Clementine Chazal

CREST, ENSAE, IP Paris

clementine.chazal@ensae.fr

&Anna Korba

CREST, ENSAE, IP Paris

anna.korba@ensae.fr

Francis Bach

INRIA - Ecole Normale Superieure

PSL Research university

francis.bach@inria.fr

###### Abstract

In this paper, we study the statistical and geometrical properties of the Kullback-Leibler divergence with kernel covariance operators (\(\)) introduced by Bach (2022). Unlike the classical Kullback-Leibler (KL) divergence that involves density ratios, the \(\) compares probability distributions through covariance operators (embeddings) in a reproducible kernel Hilbert space (RKHS), and compute the Kullback-Leibler quantum divergence. This novel divergence hence shares parallel but different aspects with both the standard Kullback-Leibler between probability distributions and kernel embeddings metrics such as the maximum mean discrepancy. A limitation faced with the original \(\) divergence is its inability to be defined for distributions with disjoint supports. To solve this problem, we propose in this paper a regularized variant that guarantees that the divergence is well defined for all distributions. We derive bounds that quantify the deviation of the regularized \(\) to the original one, as well as finite-sample bounds. In addition, we provide a closed-form expression for the regularized \(\), specifically applicable when the distributions consist of finite sets of points, which makes it implementable. Furthermore, we derive a Wasserstein gradient descent scheme of the \(\) divergence in the case of discrete distributions, and study empirically its properties to transport a set of points to a target distribution.

## 1 Introduction

A fundamental task in machine learning is to approximate a target distribution \(q\). For example, in Bayesian inference (Gelman et al., 1995), it is of interest to approximate posterior distributions of the parameters of a statistical model for predictive inference. This has led to the vast development of parametric methods from variational inference (Blei et al., 2017), or non-parametric ones such as Markov Chain Monte Carlo (MCMC) (Roberts and Rosenthal, 2004), and more recently particle-based optimization (Liu and Wang, 2016; Korba et al., 2021). In generative modelling (Brock et al., 2019; Ho et al., 2020; Song et al., 2020; Franceschi et al., 2023) only samples from \(q\) are available and the goal is to generate data whose distribution is similar to the training set distribution. Generally, this problem can be cast as an optimization problem over \((^{d})\), the space of probability distributions over \(^{d}\), where the optimization objective is chosen as a dissimilarity function \((|q)\) (a distance or divergence) between probability distributions, that only vanishes at \(q\). Starting from an initial distribution \(p_{0}\), a descent scheme can then be applied such that the trajectory \((p_{t})_{t 0}\) approaches \(q\). In particular, on the space of probability distributions with bounded second moment \(_{2}(^{d})\), onecan consider the Wasserstein gradient flow of the functional \((p)=(p||q)\). The latter defines a path of distributions, ruled by a velocity field, that is of steepest descent for \(\) with respect to the Wasserstein-2 distance from optimal transport.

This approach has led to a large variety of algorithms based on the choice of a specific dissimilarity functional \(\), often determined by the information available on the target \(q\). For example, in Bayesian or variational inference, where the target's density is known up to an intractable normalizing constant, a common choice for the cost is the Kullback-Leibler (KL) divergence, whose optimization is tractable in that setting (Wibisono, 2018; Ranganath et al., 2014). When only samples of \(q\) are available, it is not convenient to choose the optimization cost as the KL, as it is only defined for probability distributions \(p\) that are absolutely continuous with respect to \(q\). In contrast, it is more convenient to choose an \(\) that can be written as integrals against \(q\), for instance, maximum mean discrepancy (MMD) (Arbel et al., 2019; Hertrich et al., 2024), sliced-Wasserstein distance (Liutkus et al., 2019) or Sinkhorn divergence (Genevay et al., 2018). However, sliced-Wasserstein distances, that average optimal transport distances of 1-dimensional projections of probability distributions (slices) over an infinite number of directions, have to be approximated by a finite number of directions in practice (Tanguy et al., 2023); and Sinkhorn divergences involve solving a relaxed optimal transport problem. In contrast, MMD can be written in closed-form for discrete measures thanks to the reproducing property of positive definite kernels. The MMD represents probability distributions through their kernel mean embeddings in a reproducing kernel Hilbert space (RKHS), and compute the RKHS norm of the difference of embeddings (namely, the witness function). Moreover, the MMD flow with a smooth kernel (e.g., Gaussian) as in Arbel et al. (2019) is easy to implement, as the velocity field is expressed as the gradient of the witness function, and preserve discrete measures. However, due to the non-convexity of the MMD in the underlying Wasserstein geometry (Arbel et al., 2019), its gradient flow is often stuck in local minimas in practice even for simple target as Gaussian \(q\), calling for adaptive schemes tuning the level of noise or kernel hyperparameters (Xu et al., 2022; Galashov et al., 2024), or regularizing the kernel (Chen et al., 2024). MMD with non-smooth kernels, e.g., based on negative distances (Sejdinovic et al., 2013), have also attracted attention recently, as their gradient flow enjoys better empirical convergence properties than the previous ones (Hertrich et al., 2024, 2024). However, their gradient flow does not preserve discrete measures; and their practical simulation rely on implicit time discretizations (Hertrich et al., 2024) or slicing (Hertrich et al., 2024).

In contrast to the MMD with smooth kernels, the KL divergence is displacement convex (Villani, 2009, Definition 16.5) when the target is log-concave (i.e., \(q\) has a density \(q e^{-V}\) with \(V\) convex), and its gradient flow enjoys fast convergence when \(q\) satisfies a log-Sobolev inequality (Bakry et al., 2014). In this regard, it enjoys better geometrical properties than the MMD. Moreover, the KL divergence is equal to infinity for singular \(p\) and \(q\), which makes its gradient flow extremely sensitive to mismatch of support, so that the flow enforces the concentration on the support of \(q\) as desired. On the downside, while the Wasserstein gradient flow of KL divergences is well-defined (Chewi et al., 2020), its associated particle-based discretization is difficult to simulate when only samples of \(q\) are available, and a surrogate optimization problem usually needs to be introduced (Gao et al., 2019; Ansari et al., 2020; Simons et al., 2022; Birrell et al., 2022; Liu et al., 2022). However, it is unclear whether this surrogate optimization problem preserves the geometry of the KL flow.

Recently, Bach (2022) introduced alternative divergences based on quantum divergences evaluated through kernel covariance operators, that we call here a kernel Kullback-Leibler (\(\)) divergence. The latter can be seen as second-order embeddings of probability distributions, in contrast with first-order kernel mean embeddings (as used in MMD). In Bach (2022), it was shown that the \(\) enjoys nice properties such as separation of measures, and that it is framed between a standard KL divergence (from above) and a smoothed KL divergence (from below), i.e., a KL divergence between smoothed versions of the measures with respect to a specific smoothing kernel. Hence, it cannot directly be identified to a KL divergence and corresponds to a novel and distinct divergence. However, many of its properties remained unexplored, including a complete analysis of the \(\) for empirical measures, a tractable closed-form expression and its optimization properties. In this paper, we tackle the previous questions. We propose a regularized version of the \(\) that is well-defined for any discrete measures, in contrast with the original \(\). We establish upper bounds that quantify the deviation of the regularized \(\) to its unregularized counterpart, and convergence for empirical distributions. Moreover, we derive a tractable closed-form for the regularized \(\) and its derivatives that writes with respect to kernel Gram matrices, leading to a practical optimization algorithm. Finally,we investigate empirically the statistical properties of the regularized \(\), as well as its geometrical properties when using it as an objective to target a probability distribution \(q\).

This paper is organized as follows. Section 2 introduces the necessary background and the regularized \(\). Section 3 presents our theoretical results on the deviation and finite-sample properties of the latter. Section 4 provides the closed-form of regularized \(\) for discrete measures as well as the practical optimization scheme based on an explicit time-discretisation of its Wasserstein gradient flow. Section 5 discusses closely related work including distances or divergences between distributions based on reproducing kernels. Finally, Section 6 illustrates the statistical and optimization properties of the \(\) on a variety of experiments.

## 2 Regularized kernel Kullback-Leibler (\(\)) divergence

In this section, we state our notations and previous results on the (original) kernel Kullback-Leibler (\(\)) divergence introduced by Bach (2022), before introducing our proposed regularized version.

Notations.Let \((^{d})\) the set of probability measures on \(^{d}\). Let \(_{2}(^{d})\) the set of probability measures on \(^{d}\) with finite second moment, which becomes a metric space when equipped with Wasserstein-2 (\(W_{2}\)) distance (Villani, 2009).

For \(p(^{d})\), we denote that \(p\) is absolutely continuous w.r.t. \(q\) using \(p q\), and we use \(dp/dq\) to denote the Radon-Nikodym derivative. We recall the standard definition of the Kullback-Leibler divergence, \((p||q)=(dp/dq)dp\) if \(p q\), \(+\) else.

If \(g:^{d}^{r}\) is differentiable, we denote by \(Jg:^{d}^{r d}\) its Jacobian. If \(r=1\), we denote by \( g\) the gradient of \(g\) and \(g\) its Hessian. If \(r=d\), \( g\) denotes the divergence of \(g\), i.e., the trace of the Jacobian. We also denote by \( g\) the Laplacian of \(g\), where \( g= g\). We also denote \(I\) the identity matrix or operator.

For a positive semi-definite kernel \(k:^{d}^{d}\), its RKHS \(\) is a Hilbert space with inner product \(,_{}\) and norm \(\|\|_{}\). For \(q_{2}(^{d})\) such that \( k(x,x)dq(x)<\), the inclusion operator \(_{q}: L^{2}(q),\;f f\) is a bounded operator with its adjoint being \(_{q}^{*}:L^{2}(q),\;f k(x,)f(x)dq(x)\)(Steinwart and Christmann, 2008, Theorem 4.26 and 4.27). The covariance operator w.r.t. \(q\) is defined as \(_{q}= k(,x) k(,x)dq(x)=_{q}^{*}_{q}\), where \((a b)c= b,c_{}a\) for \(a,b,c\). It can also be written \(_{q}=_{^{d}}(x)(x)^{*}dq(x)\) where \(*\) denotes the transposition in \(\) (recall that for \(u\), \(uu^{*}:\) denotes the operator \(uu^{*}(f)= f,u_{}u\) for any \(f\)).

Kernel Kullback-Leibler divergence (\(\)).For \(p,q(^{d})\), the kernel Kullback-Leibler divergence (\(\)) is defined in Bach (2022) as:

\[(p||q):=(_{p}_{p})-(_ {p}_{q})=_{(,)_{p}_{q}} () f_{},g_{} _{}^{2}.\] (1)

where \(_{p}\) and \(_{q}\) are the set of eigenvalues of the covariance operators \(_{p}\) and \(_{q}\), with associated eigenvectors \((f_{})_{_{p}}\) and \((g_{})_{_{q}}\). The \(\) (1) evaluates the Kullback-Leibler divergence between operators on Hilbert Spaces, that is well-defined for any couple of positive Hermitian operators with finite trace, at the operators \(_{p}\) and \(_{q}\). From Bach (2022, Proposition 4), if \(p\) and \(q\) are supported on compact subset of \(^{d}\), and if \(k\) is a continuous positive definite kernel with \(k(x,x)=1\) for all \(x^{d}\), and if \(k^{2}\) is universal (Steinwart and Christmann, 2008, Definition 4.52), then \((p||q)=0\) if and only if \(p=q\). In Bach (2022), it also was proven that the \(\) is upper bounded by the (standard) \(\)-divergence between probability distributions (see Proposition 4 therein) and lower bounded by the same \(\) but evaluated at smoothed versions of the distributions, where the smoothing is a convolution with respect to a specific kernel (see Section 4 therein). Thus, the \(\) defines a novel divergence between probability measures. It defines then an interesting candidate as to compare probability distributions, for instance when used as an optimization objective over \((^{d})\), in order to approximate a target distribution \(q\).

Definition of the regularized Kkl.A major issue that the \(\) shares with the standard Kullback-Leibler divergence between probability distributions, is that it diverges if the support of \(p\) is not included in the one of \(q\) (1). Indeed, for the \((p||q)\) to be finite, we need \((_{q})(_{p})\). This condition is satisfied when the support of \(p\) is included in the support of \(q\). Indeed, if \(f(_{q})\), then \( f,_{q}f_{}=_{^{d}}f(x)^{2}dq(x)=0\), and so \(f\) is zero on the support of \(q\), then also on the support of \(p\). Hence, the \(\) is not a convenient discrepancy when \(q\) is a discrete measure (in particular, if \(p\) is also discrete with different support than \(q\)). A simple fix that we propose in this paper is to consider a regularized version of \(\) which is, for \(]0,1[\),

\[_{}(p||q):=(p||(1-)q+ p)=(_{p}_{p})-(_{p}((1-)_{q}+ _{p})).\] (2)

The advantage of this definition is that \(_{}\) is finite for any distribution \(p,q\). It smoothes the distribution \(q\) by mixing it with the distribution \(p\), to a degree determined by the parameter \(\). This divergence approximates the original \(\) divergence without requiring the distribution \(p\) to be absolutely continuous with respect to \(q\) for finiteness. Moreover, for any \(]0,1[\), \(_{}(p||q)=0\) if and only if \(p=q\). As \( 0\), we recover the original \(\) (1), and as \( 1\), this quantity converges pointwise to zero.

**Remark 1**.: The regularization we consider in (2) has also been considered for the standard KL divergence (Lee, 2000). These objects, as well as their symmetrized version, were also referred to in the literature as skewed divergences (Kimura and Hino, 2021). The most famous one is Jensen-Shannon divergence, recovered as a symmetrized skewed KL divergence for \(=\), that is defined as \((p||q)=(p||p+q)+(q|| p+q)\).

## 3 Skewness and concentration of the regularized \(\)

In this section we study the skewness of the regularized \(\) due to the introduction of the parameter \(\), as well as its concentration properties for empirical measures.

Skewness.We will first analyze how the regularized \(\) behaves with respect to the regularization parameter \(\). First, we show it is monotone with respect to \(\) in the following Proposition.

**Proposition 2**.: _Let \(p q\). The function \(_{}(p||q)\) is decreasing on \(\)._

Proposition 2 shows that the regularized KKL shares a similar monotony behavior than the regularized, or skewed, (standard) KL between probability distributions, as recalled in Appendix A.1. The proof of Proposition 2 can be found in Appendix B.1. It relies on the positivity of the \(\) divergence, and the use of the identity (Ando, 1979)

\[(_{p}(_{p}-_{q}))=_{0}^{+} _{p}(_{p}+ I)^{-1}- _{q}(_{q}+ I)^{-1}d,\] (3)

where \(I\) is the identity operator, that is used in all our proofs. We now fix \(]0,1[\) and provide a quantitative result about the deviation of the regularized (or skewed) \(\) to its original counterpart.

**Proposition 3**.: _Let \(p,q(^{d})\). Assume that \(p q\) and that \(\) for some \(>0\). Then,_

\[|_{}(p||q)-(p||q)|((1+ )+}{1-}(1+} ))|\,(_{p}_{q})|.\] (4)

Proposition 3 recovers a similar quantitative bound than the one we can obtain for the standard KL between probability distributions, see Appendix A.2; and state that the skewness of the regularized \(\) can be controlled by the regularization parameter \(\), especially when the latter is small. However, the tools used to derive this inequality are completely different by nature than for the KL case. Its complete proof can be found in Appendix B.2, but we provide here a sketch.

Sketch of proof.Let \(=_{p}+(1-)_{q}\). We write \((p||q)_{}-(p||q)=\,_{p} _{q}-\,_{p}\) that we write as (3). In order to upper bound this integral we use the operator equalities, for two operators \(A\) and \(B\), \(A^{-1}-B^{-1}=A^{-1}(B-A)B^{-1}=A^{-1}(B-A)A^{-1}-A^{-1}(B-A)B^{-1}(B-A)A^{-1}\) which we apply to \(A=+ I\) and \(B=_{q}+ I\). We then use the assumption \(_{p}_{q}\) and carefully apply upper bounds on positive semi-definite operators, using the matrix inequality results from Appendix A.3, to conclude the proof.

Statistical properties.We now focus on the regularized \(\) for empirical measures and derive finite-sample guarantees.

**Proposition 4**.: _Let \(p,q(^{d})\). Assume that \(p q\) with \(\) for some \(0< 1\) and let \(\). We remind that \((x)\) is the feature map of \(x^{d}\) in the RKHS \(\). Assume also that \(c=_{0}^{+}_{x^{d}}(x),(_{p}+  I)^{-1}(x)_{}^{2}d\) is finite. Let \(\), \(\) supported on \(n,\ m\) i.i.d. samples from \(p\) and \(q\) respectively. We have:_

\[|_{}(\|)-_ {}(p||q)|}(2 + n)\\ +(1++}{ ^{2}}(1+)).\] (5)

**Remark 5**.: It is possible to calculate a similar bound for the above proposition which does not require the condition \(p q\). This bound, which can be found at the end of Appendix B.3.3, worsens as \(\) approaches 0 because it scales in \(O(})\) instead of \(O()\) above.

The latter proposition extends significantly Bach (2022, Proposition 7) that provided an upper bound on the entropy term only, i.e., the first term in (1):

\[[|(_{}_{})- (_{p}_{p})|]}{ n}+}(2+ n).\] (6)

Our bound (5) is explicit in the number of samples \(n,m\) for \(,\), and for \(n=m\) we recover similar terms as (6). Our contribution is to upper bound the cross term, i.e., the second term in (1), involving both \(p\) and \(q\). We do so by closely follow the proof of (Bach, 2022, Proposition 7) in order to bound the cross terms difference. In consequence, our proof involves technical intermediate results, among which concentration of sums of random self-adjoint operators, and estimation of degrees of freedom. The proof of Proposition 4 can be found in Appendix B.3, but we provide here a sketch.

Sketch of proof.We denote \(=_{}+(1-)_{}\) and \(\) its population counterpart. In order to bound the cross term we write \(_{}-_{p} \) using (3). We split the integrals in three terms, with respect to two parameters \(0<_{0}<_{1}\) that we introduce: (a) one for \(\) between 0 and \(_{0}\), (b) one for \(\) between \(_{1}\) and infinity and (c) an intermediate one. The \(_{0}\) quantity is chosen to be dependent of \(m\) and \(n\), so that it converge to zero as \(n\) and \(m\) go to infinity. This way, for (a) the integral between 0 and \(_{0}\) we simply have to bound \(_{p}(+ I)^{-1}\) and \(_{}(+ I)^{-1}\) by constant or integrable quantities close to 0. Then, for (b), \(_{1}\) is chosen so that it goes to infinity when \(n\) and \(m\) go to infinity and (b) is bounded by \(1/_{1}\). Finally we upper bound finely enough (c) to compensate for the fact that the bounds of the integrals tend towards 0 and infinity. \(\)

## 4 Time-discretized regularized \(\) gradient flow

In this section, we show that the regularized \(\) can be implemented in closed-form for discrete measures, as well as its Wasserstein gradient, making its optimization tractable.

regularized \(\) closed-form.We first describe how to compute the regularized \(\) for (any, not necessarily empirical) discrete measures in practice. This will be useful for the practical implementation of regularized \(\) optimization coming next. We provide a closed-form for the latter, involving kernel Gram matrices between supports of the discrete measures.

**Proposition 6**.: _Let \(=_{i=1}^{n}_{x_{i}}\) and \(=_{j=1}^{m}_{y_{j}}\) two discrete distributions. Define \(K_{}=(k(x_{i},x_{j}))_{i,j=1}^{n}^{n n}\), \(K_{}=(k(y_{i},y_{j}))_{i,j=1}^{m}^{m m}\), \(K_{,}=(k(x_{i},y_{j}))_{i,j=1}^{n,m}^{n m}\). Then, for any \(]0,1[\), we have:_

\[_{}(\|)=( K_{}K_{})- (I_{}K(K)),\\ I_{}=I&0\\ 0&0K=K_{}&}K_{,}\\ }K_{,}&K_{}.\] (7)Proposition 6 extends non-trivially the result of Bach (2022, Proposition 6) that only provided a closed-form for the entropy term \((_{}(_{}))\), that corresponds to our first term in Equation (7). Its complete proof can be found in Appendix B.4 but we provide here a sketch.

Sketch of proof.: Our goal there is to derive a closed-form for the cross-term in \(,\) of the \(\), that is \((_{}(_{}+(1-) _{}))\). It is based on the observation that if we define \(_{x}=((x_{1}),,(x_{n}))^{*}\), \(_{y}=((y_{1}),,(y_{m}))^{*}\) and \(\) the concatenation of \(}_{x}\) and \(}_{y}\), then the covariance operators write \(_{}=^{T}I_{}\) and \(_{}+(1-)_{}=^{T}\). Then, the matrices \(K_{}\) and \(K\) write \(K_{}= I_{}^{T}\) and \(^{T}=K\). Finally, we apply an intermediate result (Lemma 12) to obtain the expression of \((_{}+(1-)_{})\) as a function of \( K\). 

Gradient flow and closed-form for the derivatives.We now discuss how to optimize \(p_{}(p\|q)\) for a given target distribution \(q\). For a given functional \(:_{2}(^{d})^{+}\), a Wasserstein gradient flow of \(\) can be thought as an analog object to a Euclidean gradient flow in the metric space \((_{2}(^{d}),W_{2})\)(Santambrogio, 2017), which defines a trajectory \((p_{t})_{t 0}\) in \(_{2}(^{d})\) following the steepest descent for \(\) with respect to the \(W_{2}\) distance. It can be characterized by a _continuity equation_:

\[_{t}p_{t}+(p_{t}^{}(p_{t}))=0,\] (8)

where \(^{}(p):^{d}\) is the first variation of \(\) at \(p_{2}(^{d})\). We recall that the first variation at \(p_{2}(^{d})\) as defined in Ambrosio et al. (2005, Lemma 10.4.1) is defined, if it exists, as the function \(^{}:^{d}\) such that

\[_{ 0}(p+)-(p)= ^{}(p)(x)d(x),\] (9)

for any \(=q-p\), \(q_{2}(^{d})\). To optimize \(_{}\), it is then natural to consider its Wasserstein gradient flow and discretize it in time and space. Since \(_{}\) is well-defined for discrete measures \(,\), we directly derive its first variation for this setting. Our next result yields a closed-form for the first variation of the regularized \(\).

**Proposition 7**.: _Consider \(,\) and the matrices \(K_{},\ K\) as defined in Proposition 6. Let \(g(x)=\). Then, the first variation of \(=_{}(\|)\) at \(\) is, for any \(x^{d}\):_

\[^{}()(x)=1+S(x)^{T}g(K_{})S(x)-T(x)^{T}g(K)T(x) -T(x)^{T}AT(x),\] (10)

_where_

\[S(x)=(}k(x,x_{1}),,}k(x,x_{n}) ),\ \ T(x)=(}k(x,x_{1}),;}k(x,y_{1}),),\]

_and \( A=_{j=1}^{n+m}_{j}\|^{2}}{_{j}}_{j}_{j}^{T} +_{j k}-_{k}}{_{j}-_{k}} {a}_{j},_{k}_{j}_{k}^{T},\)_

_where \((_{j})_{j}\) are the eigenvectors of \(K\), and \((_{j})_{j}\) the vectors of first \(n\) terms of \((_{j})_{j}\)._

The proof of Proposition 7 can be found in Appendix B.5, we provide a sketch below.

Sketch of proof.: Our proof deals separately with the entropy and the cross term, writing \(=_{1}+_{2}\). Starting from the definition (9), we denote \(=_{}\). For \(_{1}\), we write \(_{1}(+)-_{1}()=_{i=1}^{n} f(_{i}(_{}+))-f(_{i}(_{}))\). To write this term, we use the residual formula, which can be used to differentiate eigenvalues of functions. Indeed, we can write, for an operator \(A\) with finite number of positive eigenvalues, \(_{(A)}f()=_{}f(z)((zI-A )^{-1})dz\) where \(\) is a loop in \(\) surrounding all the positive eigenvalues of \(A\). Applying this to our case, if we choose \(\) such that it surrounds both the eigenvalues of \(_{}\) and of \(_{}+\), we obtain \(_{i=1}^{n}f(_{i}(_{}+))-f(_{i}(_{ {p}}))=_{}f(z)(zI-_{ }-)^{-1}-f(z)(zI-_{ })^{-1}dz\). Using the identity \(A^{-1}-B^{-1}=A^{-1}(B-A)A-1+o(B-A)\), the previous quantity becomes \(_{i=1}^{n}f(_{i}(_{}+))-f(_{i}(_{ }))=_{}_{k=1}^{n})^{2}}dz(f_{k}^{*}f_{k})+o()\). Under the integral we recognise a holomorphic function with isolated singularities and we can therefore apply the residue formula again. Concerning \(_{2}\), we proceed in the same way, with the difference that as we have a cross term, eigenvectors will appear in the calculation and in the final result.

Leveraging the analytical form for the first variation given by Proposition 7, the Wasserstein gradient of \(=_{}(||)\) at \(p\) is given by \(^{}(p):^{d}^{d}\) by taking the gradient with respect to \(x\) in Equation (10). Notice that the latter only involves derivatives with respect to the kernel \(k\), and can be computed in \(((n+m)^{3})\) due to the singular value decomposition of the matrix \(K\) defined in Proposition 6.

Starting from some initial distribution \(p_{0}\), and for some given step-size \(>0\), a forward (or explicit) time-discretization of (8) corresponds to the Wasserstein gradient descent algorithm, and can be written at each discrete time iteration \(l 1\) as:

\[p_{l+1}=(-^{}(p_{l}))_{\#}p_{l}\] (11)

where \(\) is the identity map in \(L^{2}(p_{l})\) and \(\#\) denotes the pushforward operation. For discrete measures \(_{n}=}{{n}}_{i=1}^{n}_{x^{i}}\), we can define \(F(X^{n}):=(p_{n})\) where \(X^{n}=(x^{1},,x^{n})\), since the functional \(\) is well defined for discrete measures. The Wasserstein gradient flow of \(\) (8) becomes the standard Euclidean gradient flow of the particle based function \(F\). Furthermore, Wasserstein gradient descent (11) writes as Euclidean gradient descent on the position of the particles.

## 5 Related work

Divergences based on kernels embeddings.Kernels have been used extensively to design useful distances or divergences between probability distributions, as they provide several ways to represent probability distributions, e.g., through their kernel mean or covariance embeddings. The Maximum Mean Discrepancy (MMD) (Gretton et al., 2012) is maybe the most famous one. It is defined as the RKHS norm of the difference between the mean embeddings \(m_{p}:= k(x,)dp(x)\) and \(m_{q}:= k(x,)dq(x)\), i.e., \((p||q)=\|m_{p}-m_{q}\|_{q}\). When \(k\) is characteristic, \((p||q)=0\) if and only if \(p=q\)(Sriperumbudur et al., 2010). MMD belongs to the family of integral probability metrics (Muller, 1997) as it can be written as \((p\|q)=_{f,\|f\|_{} 1}_{p}[ f(X)]-_{q}[f(X)]\). Alternatively, it can be seen as an \(L^{2}\)-distance between kernel density estimators. It became popular in statistics and machine learning through its applications in two-sample test (Gretton et al., 2012), or more recently in generative modeling (Binkowski et al., 2018).

However, kernel mean embeddings are not the only way (and maybe not the most expressive) to represent probability distributions. For instance, MMD may not be discriminative enough when the distributions differ only in their higher-order moments but have the same mean embedding. For this reason, several works have resorted to test statistics that incorporate the kernel covariance operator of the probability distributions. For instance, Harchaoui et al. (2007) construct a test statistic that resembles and regularizes the \((p\|q)\) by incorporating covariance operators (more precisely, \(\|(_{}+ I)^{-1}(m_{p}-m_{q})\|_{}\)) yielding in some sense a chi-square divergence between the two distributions. This work has been recently generalized in Hagrass et al. (2022) to more general spectral regularizations, and in Chen et al. (2024) with a different covariance operator. A similar regularized MMD statistic is employed by Balasubramanian et al. (2021); Hagrass et al. (2023) in the context of the goodness-of-fit test.

Kernel variational approximation of the KL.An alternative use of kernels to compute probability divergences is through approximation of variational formulations of \(f\)-divergences (Nguyen et al., 2010; Birrell et al., 2022) of which \(\)-divergence is an example. Indeed, the KL divergence between \(p\) and \(q\) writes \(_{g M_{k}} gdp- e^{g}dq\) where \(M_{b}\) denotes the set of all bounded measurable functions on \(^{d}\). For instance, Glaser et al. (2021) consider a variational formulation of the KL divergence restricted to RKHS functions, namely the KALE divergence:

\[(p\|q)=(1+)_{g} gdp- e^{g}dq- \|g\|_{}^{2}.\] (12)

Recently, Neumayer et al. (2024) extended the latter work and studied kernelized variational formulation of general \(f\)-divergences, referred to as Moreau envelopes of f-divergences in RKHS, including the KALE as a particular case. They prove that these functionals are lower semi-continuous, and that their Wasserstein gradient flows are well defined for smooth kernels (i.e., the functionals are \(\)-convex, and the subdifferential contains a single element). However, the KALE does not have a closed form expression (in constrast to the kernelized variational formulation of chi-square, which writes as a regularized MMD, see (Chen et al., 2024)). For discrete distributions \(p\) and \(q\) supported on \(n\) atoms, the KALE divergence can be written a strongly convex \(n\)-dimensional problem, and can be solved using standard Euclidean optimization methods. Still, this makes the simulation of KALE Wasserstein gradient flow (e.g., gradient descent on the positions of particles) computationally demanding, as it requires solving an inner optimization problem at each iteration. This inner optimization problem is solved calling another optimization algorithm. Glaser et al. (2021) use various methods in their experiments, including Newton's method (that scales as \((n^{3})\) due to the matrix inversion), or less computationally demanding ones such as gradient descent (GD) or coordinate descent. For large values of the regularization parameter \(\), using plain GD works reasonably well, but for small values of \(\), the problem becomes quite ill-conditioned and GD needs to be run with smaller step-sizes. Moreover, as KALE (and its gradient) are not available in closed-form, they cannot be used with fast and hyperparameter-free methods, such as L-BFGS (Liu and Nocedal, 1989) which requires exact gradients. This contrasts with our regularized \(\) divergence and its gradient, which are available in closed-form. In our experiments, we will investigate further the relative performance of KALE and \(\).

## 6 Experiments

In this section, we illustrate the validity of our theoretical results and the performance of gradient descent for the regularized \(\). In all our experiments, we consider Gaussian kernels \(k(x,y)=\!(-}{^{2}})\) where \(\) denotes the bandwith. Our code is available on the github repository https://github.com/clementinechazal/KKL-divergence-gradient-flows.git.

Illustrations of skewness and concentration of the \(\).We first illustrate our results of Proposition 3 and Proposition 4, i.e. the skewness and concentration properties of \(_{}\). We investigate these properties for various settings of \(p,q\) two fixed probability distributions on \(^{d}\), varying the choice of \(\), dimension \(d\), and distributions \(p,q\). We consider empirical measures \(\), \(\) supported on \(n\) i.i.d. samples of \(p,q\) (in this section we take the same number of samples for both distributions, i.e., \(n=m\) in the notations of Section 3), and we observe the concentration of \(_{}(,)\) around its population limit as the number of samples \(n\) (particles) go to infinity.

Each time, we plot the results obtained over 50 runs, randomizing the samples drawn from each distribution. Thick lines represent the average value over these multiple runs. We represent the dependence in \(\) and \(n\) in dimension 10 in Figure 1, for \(p,q\) anisotropic Gaussian distributions with different means and variances. Alternative settings and additional results are deferred to the Appendix C, such as different distributions (e.g. a Gaussian \(p\) versus an Exponential \(q\)), as well as the dimension dependence for a fixed \(\). We can clearly see in Figure 1 the monotony of \(_{}\) with respect to \(\) (as stated in Proposition 2) and the concentration of the empirical \(_{}\) around its population version, which happens faster for a larger value of \(\), as predicted by our finite-sample bounds in Proposition 4.

Sampling with \(\) gradient descent.Finally, we study the performance of \(\) gradient descent in practice, as described in Section 4. We consider two settings already used by Glaser et al. (2021) for KALE gradient flow, reflecting different topological properties for the source-target pair: a pair with a target supported on a hypersurface (zero volume support) and a pair with disjoint supports of positive volume. Alternative settings, e.g. Gaussians source and mixture of Gaussians target that are pairs of distributions with a positive density supported on \(^{d}\), are deferred to Appendix C. We also report there additional plots related to the experiments of this section.

We have treated \(\) as a hyperparameter here, and in this section for simplicity of notations we refer to \(\) as the objective functional. As both \(\) and its gradient can be explicitly computed, one can

Figure 1: Concentration of empirical \(_{}\) for \(d=10\), \(=10\), \(p,q\) Gaussians.

implement descent either using a constant step-size, or through a quasi-Newton algorithm such as L-BFGS (Liu and Nocedal, 1989). The latter is often faster and more robust than the conventional gradient descent and does not require choosing critical hyper-parameters, such as a learning rate, since L-BFGS performs a line-search to find suitable step-sizes. It only requires a tolerance parameter on the norm of the gradient, which is in practice set to machine precision. In contrast, as said in the previous section, the KALE and its gradient are not available in closed-form.

The first example is a target distribution \(q\) supported (and uniformly distributed) on a lower-dimensional surface that defines three non-overlapping rings, see Figure 2. The initial source is a Gaussian distribution with a mean in the vicinity of the target \(q\). We compare Wasserstein gradient descent of \(\), Maximum Mean Discrepancy (Arbel et al., 2019) and KALE (Glaser et al., 2021), using the code provided in these references. For each method, we choose a bandwith \(=0.1\), and we optimize the step-size for each method, and sample \(n=100\) points from the source and target distribution. Our method is robust to the choice of \(\) and generally performs very well on this example, as shown in Figure 2. We can notice that since MMD is not sensitive to the difference of support between \(p\) and \(q\), the particles may leave the rings; while for the regularized \(\) flow, as for KALE flow, the particles follow closely the support of the target distribution.

The second example consists of a source and target pair \(p,q\) that are supported on disjoint subsets, each with a finite, positive volume, in contrast with the previous example. The source and the target are uniform supported on a heart and a spiral respectively. We again run MMD, KALE and \(\) gradient descent. In this example, both \(\) and KALE recover the spiral shape, much before the MMD flow trajectory; but both have a harder time recovering outliers, disconnected from the main support of the spiral.

## 7 Conclusion

In this work, we investigated the properties of the recently introduced Kernel Kullback-Leibler (\(\)) divergence as a tool for comparing probability distributions. We provided several theoretical results, among which quantitative bounds on the deviation from the regularized \(\) to the original one, and finite-sample guarantees for empirical measures, that are validated by our numerical experiments. We also derived a closed-form and computable expression for the regularized \(\) as well as its derivatives, enabling to implement (Wasserstein) gradient descent for this objective. Our experiments validate the use of \(\) as a tool to compare discrete measures, as its gradient flow is much better behaved than the one of Maximum Mean Discrepancy which relies only on mean (first moments) embeddings of probability distributions. It can also be computed in closed-form, in contrast to the KALE divergence introduced recently in the literature, and can benefit from fast and hyperparameter-free methods such as L-BFGS.

While our study has advanced our understanding of the \(\) divergence, several limitations must be acknowledged. Firstly, theoretical guarantees for the convergence of the \(\) flow remain unestablished. Secondly, reducing the computational cost is crucial for practical applications. Investigating the use of random features presents a promising avenue for making the computations more efficient.

## 8 Acknowledgments

A. Korba and C. Chazal acknowledge the support of ANR PEPR PDE-AI.

Figure 2: MMD, KALE and \(\) flow for 3 rings target. Figure 3: Shape transfer