# InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction

Sirui Xu\({}^{}\)   Ziyin Wang\({}^{}\)   Yu-Xiong Wang\({}^{}\)   Liang-Yan Gui\({}^{}\)

University of Illinois Urbana-Champaign

\({}^{}\) Equal Contribution  \({}^{}\) Equal Advising

{siruixu2, ziyin, yxu, lgui}@illinois.edu

https://sirui-xu.github.io/InterDreamer/

###### Abstract

Text-conditioned human motion generation has experienced significant advancements with diffusion models trained on extensive motion capture data and corresponding textual annotations. However, extending such success to 3D dynamic human-object interaction (HOI) generation faces notable challenges, primarily due to the lack of large-scale interaction data and comprehensive descriptions that align with these interactions. This paper takes the initiative and showcases the potential of generating human-object interactions _without direct training on text-interaction pair data_. Our _key insight_ in achieving this is that interaction semantics and dynamics can be decoupled. Being unable to learn interaction semantics through supervised training, we instead leverage pre-trained large models, synergizing knowledge from a large language model and a text-to-motion model. While such knowledge offers high-level control over interaction semantics, it cannot grasp the intricacies of low-level interaction dynamics. To overcome this issue, we introduce a world model designed to comprehend simple physics, modeling how human actions influence object motion. By integrating these components, our novel framework, InterDreamer, is able to generate text-aligned 3D HOI sequences without relying on paired text-interaction data. We apply InterDreamer to the BEHAVE, OMOMO, and CHAIRS datasets, and our comprehensive experimental analysis demonstrates its capability to generate realistic and coherent interaction sequences that seamlessly align with the text directives.

## 1 Introduction

Text-guided human motion generation has made unprecedented progress through advancements in diffusion models , leading to synthesis outcomes that are realistic, diverse, and controllable. This progress has ignited an increased interest in exploring expanded tasks related to text-guided human interaction generation, such as social interaction  and human-scene interaction . However, many of these explorations are limited in that the dynamics of objects is not involved or text-guided. Aiming to bridge such a gap, this paper tackles a more challenging task - _generating versatile 3D human-object interactions (HOIs) through language guidance_, as illustrated in Figure 1.

Although a direct solution, as suggested by the concurrent work , would be replicating the success observed in human motion generation and adopting a similar supervised approach for learning text-driven HOIs, it is not scalable. As can be observed, generating social or scene interactions is heavily dependent on extensive collections of text-interaction pair data , and scaling these methods to address more complex HOIs outlined in our study could require datasets of comparable magnitude. Achieving this goal appears unattainable by merely annotating existing 3D HOI datasets , which are relatively limited insize. Although recent studies [28; 65; 91; 155] have annotated some of these datasets, the volume of text-interaction pairs still lags behind that available for existing text-driven motion generation efforts.

An intriguing question naturally arises: given the limited annotations of the text, _what is the potential of learning for text-conditioned HOI generation without text supervision_, which is the main focus of this paper. However, formulating the task in such a setting presents significant challenges, primarily due to the inability to directly learn the alignment between text and HOI dynamics. Our key observation is that interaction semantics and dynamics can be _decoupled_. That is, the high-level semantics of an interaction, aligned with its textual description, can be informed by _human motion_ and the _initial object pose_. Meanwhile, the low-level dynamics of the interaction - specifically, the _subsequent_ behavior of the object - is governed by the forces exerted by the human, within the constraints of physical laws. Motivated by these insights, we introduce InterDreamer - a novel framework that synergizes knowledge of interaction semantics and dynamics (Figure 1), both of which do not necessarily require learning from text-interaction pairs, if they are decoupled.

The semantics of interaction, although not available through direct supervised training, can be harnessed from _prior knowledge_ without text-interaction pair datasets. Specifically, to acquire semantically aligned interaction, we first consult a large language model (LLM), such as GPT-4  and Llama 2 , to provide understanding including how humans typically use specific body parts in interactions with particular objects, by exploiting its _in-context learning_ capability with _few-shot prompting_ and _chain-of-thought prompting_. The intermediate thoughts and the final thought are then used to (**i**) generate semantically aligned human motion with a pre-trained text-to-motion model; and (**ii**) identify an initial object pose that is harmonious with the generated human pose and text description, following a philosophy similar to _retrieval-augmented generation_.

While these large models can offer high-level motion semantic modeling, they lack crucial _low-level_ dynamics knowledge. Nevertheless, by decoupling interaction dynamics from semantics, a key advantage emerges in our InterDreamer framework: interaction dynamics can be learned from motion capture data _without the necessity of text annotations_. We instantiate this idea by developing a _world model_, which predicts the subsequent state of an object affected by the interaction. The key here is to reach _generalizable representations_ in different motion and objects. To do so, we exert control over the object through the motion of vertices on the human body. These vertices are solely sampled in regions where contact occurs, _agnostic_ to the overall object shape and whole-body motion. Such abstraction empowers the model to learn the simple dynamics from a publicly available 3D HOI dataset BEHAVE , and generalize naturally to other datasets [47; 66]. The plausibility of the generated interaction is further enhanced by a subsequent optimization procedure on the synthesized human and object motion.

Figure 1: InterDreamer generates vivid 3D human-object interaction sequences guided by text descriptions, by synergizing semantics and dynamics knowledge from large-scale text-motion data (upper left), a large language model (bottom left), human-object interaction data (upper middle), and prior knowledge (bottom middle) from simple physics. We visualize the generated text-guided interaction sequence (upper right), with the beginning of the sequence unfolded (bottom right).

To summarize, our contributions are: (**i**) We address the task of synthesizing whole-body interactions with dynamic objects guided by textual commands, achieving this notably without the need for paired text-interaction data, a novel paradigm to the best of our knowledge. (**ii**) We introduce a framework that decomposes semantics and dynamics, and they can be easily integrated. Specifically, it harnesses knowledge from a large language model (LLM) and a text-to-motion model as external resources, alongside our proposed world model. Remarkably, the only component that requires additional training is the world model, which highlights the _ease of use_ of our framework. (**iii**) Experimental results demonstrate that our framework, InterDreamer, is capable of producing semantically aligned and realistic human-object interactions, and generalizes _beyond existing HOI datasets_.

## 2 Related Work

**Text-Conditioned Human Motion Generation.** Significant progress has been witnessed in human motion synthesis tasks, given different kinds of external conditions, including action categories [2; 36; 61; 93], past motion [5; 17; 86; 110; 149; 150; 163], trajectories [31; 50; 51; 100; 122; 145], scene context [12; 29; 39; 44; 113; 114; 125; 126; 130; 153; 175; 182; 183], and without condition . Recently, human motion synthesis guided by textual descriptions [1; 6; 18; 24; 26; 34; 35; 49; 54; 57; 64; 71; 77; 81; 94; 95; 97; 104; 116; 133; 160; 162; 168; 172; 174; 178; 179; 181; 185; 187] is popular and extended to various applications, including the text-conditioned generation of multiple-person [33; 43; 75; 63; 32] and human-scene interaction [14; 48; 21; 44]. Our goal is to model human and object dynamics concurrently guided by text.

**Human-Object Interaction Generation.** Synthesizing hand-object interactions [11; 15; 20; 68; 73; 79; 80; 119; 137; 161; 165; 167; 184; 186] and single-frame human-object interactions [25; 42; 56; 92; 128; 143; 152; 154; 166] are popular topics and extended to zero-shot settings [52; 67; 156; 157]. Recently, researchers explore whole-body dynamic interaction generation, in kinematic-based approaches [58; 59; 60; 66; 84; 85; 99; 107; 108; 109; 111; 123; 136; 139; 140; 148; 151; 176] and physics-based approaches [14; 8; 16; 23; 40; 72; 74; 78; 87; 89; 115; 117; 124; 129; 142; 147; 146; 147; 158]. Current methods in HOI synthesis are often restricted by a narrow scope of actions, the use of non-dynamic objects, and a lack of comprehensive whole-body motion. Our work aims to generate diverse whole-body interactions with various objects, and enables control through language input. Recent datasets [7; 30; 45; 47; 53; 66; 112; 138; 144; 155; 164; 169; 170; 180] provide the groundwork for research in this area, and concurrent efforts [28; 65; 91] demonstrate the feasibility of applying supervised learning methods via annotating datasets. However, the amount of data currently available fall short when compared to more extensive text-motion datasets [34; 70; 83]. This discrepancy in data volume limits the capability of supervised methods to capture the complexity of human-object interactions, motivating us to investigate the potential of zero-shot generation.

**External Knowledge from LLMs.** Large language models (LLMs) are being used for advanced visual tasks, such as editing images based on instructions . In digital humans, they are used to reconstruct 3D human-object interactions  and generate human motion [3; 46; 178; 46] as well as human-scene interactions . Our approach is inspired by , which uses LLMs to infer contact body parts with a given object for reconstructing 3D human-object interactions - a task different from ours. Our approach utilizes GPT-4  or Llama 2 , to not only understand contact body parts but also narrow the distribution gap between different tasks, and provide knowledge for interaction retrieval. This is accomplished by utilizing the in-context learning capabilities of LLMs  and their support for retrieval-augmented generation .

## 3 Methodology

**Problem Formulation.** Our goal is to synthesize a sequence of 3D human-object interactions \(\) that satisfies a descriptive text \(p\). This sequence is a series of tuples \([(_{1},_{1}),(_{2},_{2}),,(_{M},_{M})]\), where \(_{i}\) represents the human pose parameters defined in the SMPL model , while the shape of the human is unified the same as . \(_{i}\) defines the pose of the rigid object in terms of its 3D spatial position and orientation. The sequence length \(M\) is variable and is dynamically determined by our text-to-motion model based on the input text \(p\). We do _not_ require text supervision for training.

**Overview.** Our framework, illustrated in Figure 2, can be conceptualized as a Markov decision process (MDP). We begin by dividing the motion sequence into \(T\) segments, each with \(m\) frames,where \(M=T m\). Object motion \(\{_{i}\}_{i=1}^{M}\) can be seen as a sequence of environmental states \(\{_{t}\}_{t=1}^{T}\), and human motion \(\{_{i}\}_{i=1}^{M}\) is described as a sequence of actions \(\{_{t}\}_{t=1}^{T}\) that interact with the environment. Under such an MDP setup, our framework starts with high-level planning \(L\), which deciphers textual interaction description \(p\) by \(g=L(p)\) (Sec. 3.1). Then, a text-to-motion model \(\) translates context \(g\) into human actions, modeled as \(_{t+1}(_{t+1}|_{t},\{_{i}\}_{i=1}^{t},g)\) (Sec. 3.2). The interaction retrieval \(R\) proposes an initial object state \(_{1} R(_{1}|_{1},g)\), based on the initial action \(_{1}\) and context \(g\) (Sec. 3.2). After that, a world model \(P\) is trained to predict future states \(_{t+1} P(_{t+1}|_{t},_{t},_{t+1})\) from the current action and state (Sec. 3.3). Our world model incorporates an optimization process, for both state and action refinement (Sec. 3.4). Notably, the text-to-motion and world models are executed _iteratively_ until text-to-motion generates an end frame.

### High-Level Planning

Leveraging LLMs' strong reasoning capabilities and inherent common sense, our high-level planning \(L\) yields interaction details \(g=L(p)\) that cannot be naively extracted in textual descriptions \(p\). The process undertaken by \(L\) encompasses three steps: (**i**) _Determining the object_: the LLM is employed to translate described objects into corresponding categories from a predefined list. (**ii**) _Determining initial human-object contact_: the LLM infers the body parts involved in the interaction, drawing from a list defined in the SMPL model . And most importantly, (**iii**) _reducing the distribution gap_: the LLM bridges the distribution gap between the free-form textual input and the language used within the training data of the text-to-motion model . This involves standardizing syntax and content according to designed guidelines. In Figure 2, we demonstrate the prompt we used with the few-shot prompting . We define intermediate thoughts and the final thought, _i.e._, answers to three questions, as detailed information \(g=L(p)\), which guides the subsequent procedure, structuring the entire framework with a philosophy similar to retrieval-augmented generation . Our high-level planning operates indirectly in the generation of interactions. Nonetheless, it narrows the vast range of possible interactions in the real world into a more manageable distribution within the capabilities of our framework. We incorporate GPT-4  and Llama-2  for evaluation.

Figure 2: **An overview of our InterDreamer. (i)** Our high-level planning analyzes the description using LLMs and provides guidance to the low-level control. (**ii**) Our low-level control includes a text-to-motion model that translates text into human actions \(_{t+1}\), and an interaction retrieval model that extracts the object’s first state \(_{1}\). (**iii**) Our world model executes actions to output the next state \(_{t+1}\). We achieve this by abstracting the problem as predicting the motion of contact vertices – represented by red spheres for humans and blue spheres for objects on the top right – using human vertices as controls for the prediction of object vertices. An optimization process is coupled with the dynamics model, projecting the state and action onto valid counterparts. Solid arrows mean that the process is performed iteratively.

### Low-Level Control

With the information \(g\) derived from the description \(p\), the low-level control aims to create a sequence of human actions \(\{_{t}\}_{t=1}^{T}\) by a text-to-motion model, and an initial state \(_{1}\) by interaction retrieval, such that they correspond to the objectives outlined by \(g\).

**Text-to-Motion.** We utilize a text-to-motion model \(\) to develop actions to be executed in the world model. At each timestep \(t\), \(\) receives the sequence of previous actions \(\{_{i}\}_{i=1}^{t}\) and the text tokens encoded from the rewritten description in \(g=L(p)\), and produces a next action \(_{t+1}\), which later in Sec. 3.4 will be adjusted through an optimization process that intertwines actions with the object state \(_{t}\). Thus, the overall process can be formally defined as \(_{t+1}(_{t+1}|_{t},\{_{i}\}_{i=1}^{t},g)\), while the initial action \(_{1}(_{1}|g)\) is influenced merely by context \(g\) without prior actions or states, which will be used in interaction retrieval. \(\) builds upon existing text-to-motion models, where we evaluate MDM , MotionDiffuse , ReMoDiffuse , and MotionGPT .

**Interaction Retrieval.** The interaction retrieval component \(R\) establishes the initial state \(_{1} R(_{1}|_{1},g)\), based on the initial action \(_{1}\) generated by the text-to-motion model. We propose a user-friendly pipeline for this purpose built on handcrafted rules. First, we create databases by extracting HOI frames from the training sets of each target datasets -- BEHAVE , OMOMO , and CHAIRS . The indexing key for retrieval is a tuple consisting of the body part in contact and the category of the involved object. Each retrieval value is a per-frame contact map, represented by a list of \(K\) vertex pairs \(\{(d_{h}^{i},d_{o}^{i})\}_{i=1}^{K}\). Here, \(d_{h}^{i}\) refers to the contact vertex on the human surface, while \(d_{o}^{i}\) refers to the corresponding contact vertex on the object surface. This contact map is linked to its corresponding key, creating a searchable record of interactions. During the inference stage, using the body part and object information provided by the high-level planning (Sec. 3.1), we retrieve all relevant contact maps from the database. We then sample one map \(\{(d_{h}^{i},d_{o}^{i})\}_{i=1}^{K}\) and use it to establish the object state \(_{1} R(_{1}|_{1},g)\), thus initializing the interaction. Further details including how we ensure consistency between the sampled state and human action are provided in Sec. B.1 of the Appendix. We also discuss an alternative learning-based approach in Sec. B.1.

### World Model

Our world model combines a dynamics model and the optimization process, dedicated to simulating state transitions affected by applied actions. While drawing inspiration from similar concepts utilized in robotics [103; 135] and autonomous driving systems , we use it here to generate HOI trajectories. This model, trained on the training set of a 3D HOI dataset such as BEHAVE , serves a similar role as a simulator but is much simpler - it takes the preceding object state \(_{t}\) along with a pair of consecutive actions \(_{t}\) and \(_{t+1}\), and predicts the subsequent object state \(_{t+1}\). The interplay between the low-level control and the world model ultimately produces a coherent interaction rollout.

In designing the dynamics model, a naive method would be directly taking raw actions, states, and object geometry as input. However, this suffers from a severe generalization problem during inference: the dynamics model is likely to encounter human actions and object geometry that do not exist in the training set, since our text-to-motion model is not trained with object interaction data. To overcome this limitation, we instead focus on encoding interactions through the contact vertices on the object, which capture both the action and object geometry, as shown in Figure 2. This _locality_ ensures that the dynamics model remains focused on interactions in the contact region, without being distracted by the motion of body parts and geometry details that are irrelevant to the interaction.

**Input Representation.** Specifically, at each timestep \(t\), we abstract the past actions as \(H\) historical vertex trajectories \(\{\{_{i}^{j}\}_{j=1}^{N}\}_{i=1}^{H+F}\), and the future actions as \(F=m\) future vertex trajectories \(\{\{_{i}^{j}\}_{j=1}^{N}\}_{i=H+1}^{H+F}\), where non-fixed variable \(N\) is the number of sampled contact vertices, and \(m\) is the length of segments as mentioned in the overview of Sec 3. Note that we train our dynamics model to forecast over a longer duration than the past motion (\(F>H\)), only the foremost future action will be used for autoregressive generation during the inference, as suggested in . To determine these \(N\) vertices, we start with object's signed distance fields \(\{_{i}\}_{i=1}^{H}\) over the past \(H\) frames, derived from the past state \(_{t}\). We then sample vertices that meet the following criteria: \(|_{i}(_{i}^{j})|_{1}, _{i}(_{i}^{k})|_{1}, i=1,,H,  j\), and \(\|_{i}^{j}-_{i}^{k}\|_{2}, j k\), where \(_{1}\) and \(_{2}\) are two hyperparameters. The objective is to sparsely sample contact vertices while ensuring that they are sufficient to encompass the interaction. We characterize each vertex trajectorywith a feature \(^{j}\) to provide (i) human vertex coordinates at T-pose, providing information about the position of the human vertex on the body surface; (ii) the vertex-to-object surface vector, indicating vertex's impact on the object as well as inherently including information related to the object's shape; and (iii) the vertex's velocity relative to its nearest object vertex. Thus, the model needs to learn how the features of human action \(^{j}\) affect the evolution of the state of the object.

**Architecture.** As demonstrated in Figure 2, the network comprises two components: \(\) that operates without contact vertex conditions, applicable in scenarios where no contact occurs, and \(\), which incorporates contact vertex conditions into the object trajectory when contact is present. The k-th layer of \(\) can be initiated as \(_{k}(_{k},)\), mapping the input feature map \(_{k}\) at the \(k\)-th layer to another feature map, with \(\) denoting the MLP's parameters. To incorporate human vertex controls, we introduce a second network \(_{k}(^{j}_{k},_{v})\) operating on \(N\) vertex features \(\{^{j}_{k}\}_{j=1}^{N}\), where \(_{v}\) is its parameters. With a cross-attention layer \(\), a dynamics block is formulated as: \(_{k+1},\{^{j}_{k+1}\}_{j=1}^{N}=(_{k}( {x}_{k},),\{_{k}(^{j}_{k},_{v})\}_{j=1} ^{N})\). We stack multiple dynamics blocks to form the model. The initial input, \(_{0}\), corresponds to the previous state \(_{t}\), while each \(^{j}_{0}\) represents the feature of the vertex trajectory, containing both the trajectory \(\{^{j}_{i}\}_{i=1}^{H+F}\) and its associated feature vector \(^{j}\). The output of this model is preliminary and subject to further optimization as introduced in Sec. 3.4, which will yield the final future state. We utilize the Mean Squared Error loss to train the dynamics model. For more explanations, please refer to Sec. B.2 of the Appendix.

### Optimization

Optimization plays a role in introducing prior knowledge and avoiding the accumulation of errors. During inference, we input the action \(_{t+1}\) and state \(_{t+1}\) and refine them. This refinement is achieved through gradient descent on the human and object pose parameters. Our optimization includes several loss terms: a fitting loss to align optimized results with their preliminary one, a velocity loss for temporal smoothness, a contact loss to promote occurring contact, and a collision loss to reduce penetration. We provide detailed formulations in Sec. B.3 of the Appendix.

## 4 Experiments

Extensive comparisons evaluate the performance of our InterDreamer across two motion-relevant tasks. Details of the evaluation settings are provided in Sec. 4.1. We present both quantitative (Sec. 4.2) and qualitative (Sec. 4.3) results for our approach. Additionally, we perform ablation studies to verify the efficacy of each component within our framework. These studies also cover the

    &  Planning \\ **(Ours)** \\  } & ^{}\)} & ^{}\)} & ^{}\)} & ^{-} prediction task  to evaluate our dynamics model. Additional details and results are presented in Sec. C and Sec. D of the Appendix. Please refer to our website for video results.

### Experimental Setup

**Datasets.** We use BEHAVE , CHAIRS , and OMOMO  datasets for quantitative evaluation. The BEHAVE dataset includes recordings of 8 individuals interacting with 20 everyday objects, and our analysis focuses on 18 objects for which interaction sequences are available at 30 Hz. The human pose is modeled using SMPL-H , with hand poses set to an average pose _due to the absence of detailed hand pose in the dataset_. We manually segment the long interaction sequences in the test set, and annotate them with descriptions as well as their starting and ending indices, leading to \(532\)

Figure 4: **Qualitative results** in more challenge scenarios with _free-form input not_ from our annotations, showing the ability of our InterDreamer to fit _object sizes_ and handle _complex and long sequences_. Here, our synergized models are GPT-4  and MotionGPT .

    &  Planning \\ **(Ours)** \\  } & ^{}\)} & ^{}\)} & ^{}\)} &  Diversity\({}^{-}\) \\  } \\    & & Top 1 & & & & Top 3 & & & \\  Ground Truth & - & 0.044\({}^{ 0.04}\) & 0.095\({}^{ 0.08}\) & 0.151\({}^{ 0.00}\) & 0.000\({}^{ 0.000}\) & 6.858\({}^{ 0.06}\) & - & 5.660\({}^{ 0.110}\) \\  MDM  & \(\) & 0.056\({}^{ 0.005}\) & 0.096\({}^{ 0.007}\) & 0.135\({}^{ 0.006}\) & 16.638\({}^{ 0.631}\) & 7.110\({}^{ 0.063}\) & 2.446\({}^{ 0.456}\) & **5.862\({}^{ 0.520}\)** \\  & \(\) & **0.062\({}^{ 0.006}\)** & **0.109\({}^{ 0.04}\)** & **0.155\({}^{ 0.008}\)** & **15.735\({}^{ 0.25}\)** & **6.889\({}^{ 0.082}\)** & **2.663\({}^{ 0.520}\)** & 6.461\({}^{ 0.841}\) \\  MotionDiffuse  & \(\) & 0.048\({}^{ 0.006}\) & 0.094\({}^{ 0.008}\) & 0.143\({}^{ 0.013}\) & 15.442\({}^{ 0.231}\) & **5.799\({}^{ 0.054}\)** & **1.658\({}^{ 0.209}\)** & **5.981\({}^{ 0.526}\)** \\  & \(\) & **0.075\({}^{ 0.003}\)** & **0.141\({}^{ 0.015}\)** & **0.189\({}^{ 0.009}\)** & **10.815\({}^{ 0.003}\)** & 5.916\({}^{ 0.004}\) & **1.677\({}^{ 0.044}\)** & **5.718\({}^{ 0.522}\)** \\  ReMoDiffuse  & \(\) & 0.062\({}^{ 0.003}\) & 0.111\({}^{ 0.005}\) & 0.160\({}^{ 0.012}\) & 15.479\({}^{ 0.209}\) & 5.690\({}^{ 0.049}\) & 1.179\({}^{ 0.145}\) & 6.032\({}^{ 0.540}\) \\  & \(\) & **0.067\({}^{ 0.04}\)** & **0.122\({}^{ 0.006}\)** & **0.174\({}^{ 0.006}\)** & **14.560\({}^{ 0.008}\)** & **5.678\({}^{ 0.033}\)** & **1.193\({}^{ 0.202}\)** & **5.368\({}^{ 0.417}\)** \\  MotionGPT  & \(\) & 0.061\({}^{ 0.005}\) & 0.114\({}^{ 0.006}\) & 0.152\({}^{ 0.006}\) & 18.472\({}^{ 0.528}\) & 6.358\({}^{ 0.076}\) & **4.553\({}^{ 0.244}\)** & **6.726\({}^{ 0.156}\)** \\  & \(\) & **0.064\({}^{ 0.007}\)** & **0.121\({}^{ 0.007}\)** & **0.164\({}^{ 0.00}\)** & **17.512\({}^{ 0.498}\)** & **6.287\({}^{ 0.041}\)** & 4.470\({}^{ 0.191}\)** & 7.048\({}^{ 0.109}\) \\   

Table 3: **Quantitative results** on human motion quality on the OMOMO  dataset with their provided annotation. We show that our high-level planning narrows the distribution gap and adapts single human generators into human-object interaction generation. To evaluate R-Precision, a batch size of 32 is selected.

subsequences for evaluation. For qualitative evaluation, we go beyond using annotations specifically created and employ free - form text to demonstrate our model's capability on out-of-distribution inputs. To assess our model's performance with novel objects, we expand our retrieval database to include objects from the OMOMO  and CHAIRS  datasets, while we do not fine-tune the dynamics model on them-a direct qualitative evaluation without additional adaptation.

**Metrics.** The evaluation metrics are divided into three categories: (**i**) _Human motion quality_: The Frechet Inception Distance (**FID**) measures the distance between the generated motion and ground truth. The MultiModality (**Multimodality**) and **Diversity** metrics assess the variance in generated human motion. **R-Precision** evaluates the consistency between the text and the generated human motion within the latent space. MultiModal distance (**MM Dist**) is the distance between the motion feature and the text feature. We follow  to generate motion and text features. (**ii**) _Interaction quality_: We propose **CMD** to measure the distance between contact maps of real interactions and those generated. The per-sequence contact map is defined by the percentage of time that each body part is actively in contact. The detailed formulation is provided in Sec. C of the Appendix. We also measure the collision (**Pene.**), which calculates the average percentage of object vertices that have non-negative values in the human signed distance fields . (**iii**) _Object motion accuracy_: The dynamics model's performance in the interaction prediction task  is evaluated by the accuracy of predicted object motion, including **Trans. Err.**, the average distance between predicted and ground truth, and **Rot. Err.**, the average distance between the predicted and ground truth.

Figure 5: **Qualitative results on the CHAIRS  dataset. Our dynamics model trained on the BEHAVE  dataset generalizes well on the CHAIRS objects unseen in training. Frames are separately visualized. Here, our synergized models are GPT-4  and MotionGPT .**

Figure 6: Results from the interaction retrieval. We demonstrate that our proposed retrieval approach based on handcraft rules can extract diverse and realistic interactions.

Figure 7: **(a) Ablation study on the high-level planning. On the _left_ are results from MotionGPT  using free-form descriptions, and on the _right_ are results with our planning additionally. Without planning, the motion generative model struggles to interpret free-form HOI descriptions and generate semantically-aligned motion. (b) We visualize CLIP  features of text on HumanML3D  via t-SNE , raw HOI descriptions (“w/o planning”), and HOI descriptions processed through our high-level planning (“w/ planning”). See Table 5 for quantitative measurements.**

**Baselines.** Most recent research on text-to-HOI synthesis follows a supervised learning approach [28; 91], making direct quantitative comparisons unfair. Therefore, we primarily focus on qualitative comparisons with these methods. To enable quantitative evaluation, we develop a range of baselines to assess both the overall performance of our framework and the effectiveness of its individual components. In the context of high-level planning, we utilize GPT-4  and Llama-2 , illustrating the effectiveness of our prompts across different language models. For low-level motion generation control, our baselines include MDM , MotionDiffuse , ReMoDiffuse , and MotionGPT , which span a range of text-to-motion approaches trained on HumanML3D  and show the generalizability of our framework. To evaluate the dynamics model, we include different design choices: (**i**) unconditional dynamics model which operates object dynamics independently of human motion; (**ii**) using human marker features as actions to the dynamics model, similar to ; (**iii**) using unprocessed human motion and object pointcloud motion as input to the dynamics model; (**iv**) our proposed vertex-based actions where only the contacting vertices are used for control.

### Quantitative Results

In Table 1, comparing our framework to baselines with unconditional dynamics model, Inter-Dreamer achieves better interaction quality in terms of CMD and penetration scores, showing the importance of human influence on object motion. Against methods that utilize direct raw human motion or markers for action features, our method demonstrates enhanced performance by offering more fine-grained guidance and extracting generalizable features for dynamics modeling. Tables 2 and 3 present a comparative analysis of our approach of combining high-level planning with low-level control, where we adopt various text-to-motion models against their counterparts without high-level planning on the BEHAVE and OMOMO datasets. Our approach consistently outperforms baselines. Specifically, InterDreamer exhibits superior motion quality, reflected by a lower FID, higher R-Precision, and better diversity, highlighting the benefits of incorporating our planning to reduce the distribution gap for the motion generator to generalize in the HOI synthesis task.

### Qualitative Results

Figure 3 displays several results guided by the free-form text. Our method exhibits proficiency in interpreting the textual input and synthesizing dynamic, realistic interactions, despite the absence of training with text-interaction paired data. More importantly, as illustrated in Figure 4, we selectively use more complex sequences of interactive descriptions that are _beyond the scope of the existing HOI dataset_. Figure 5 further exemplifies our method that is able to generalize effectively to the CHAIRS dataset, despite our dynamics model not being trained on it. Figure 6 depicts the retrieval procedure, resulting in a diverse set of interactions that are both high-quality and semantically aligned. More experimental results and the user study are presented in Sec. D of the Appendix.

Figure 8: **Ablation study on the dynamics model. Given the text description of “A person walks clockwise while holding a small box with left hand,” our (**b**) vertex-based control can synthesize consistent contacts, which (**a**) the baseline fails to do.**

### Ablation Study

**Adaptability of High-Level Planning.** Is our framework adaptable across different large language models (LLMs)? As illustrated in Table 4, our analysis contains two types of language models: GPT-4 , which is accessible through APIs and operates as a black box model; and Llama-2 , an open-source model. We demonstrate that language models with large parameters exhibit very high accuracy in responding to questions tailored to our prompts, validating the framework's adaptability.

**Effectiveness of High-Level Planning with Low-Level Control.** In consistency with Table 2, Figure 7 offers a qualitative comparison of text-to-motion results, contrasting results with and without LLM-revised text descriptions. The comparison shows that motion generated without LLM-enhanced descriptions often fails to correspond to the intended text, if the text is too challenging, _e.g.,_ not in the distribution of HumanML3D , which is used to train text-to-motion models. This underscores the LLM's critical role in bridging the distribution gap. In Figure 7(b), we visualize the CLIP  features of descriptions from HumanML3D, our raw annotations, and those processed by high-level planning. Quantitative evidence is provided in Table 5. Text processed through high-level planning demonstrates greater similarity to the HumanML3D dataset. Additionally, we test on more challenging out-of-distribution text, selecting examples with an average cosine similarity to HumanML3D text of less than \(0.85\). High-level planning successfully refrases these texts, significantly increasing their similarity. For example, in Figure 7(a), the text "Someone can be seen sitting on a yoga ball" has a cosine similarity of \(0.874\) to the closest in-distribution text, while the rephrased text after planning, "A person is seated on an object," achieves a similarity of \(0.958\).

**Effectiveness of World Model.** In the quantitative evaluation, we show that the performance of our framework is enhanced by the tailored design of our world model. Table 1 provides additional evidence of the effectiveness by integrating the proposed world model, as interaction correction within the InterDiff framework  in the interaction prediction task. This implementation demonstrates enhanced conditionality in the object dynamics modeling across two tasks, attributed to the vertex-level condition as actions. Doing so effectively removes the whole-body complexity, most of which tends to be irrelevant to the interaction. Figure 8 further indicates that our vertex-based condition can establish consistent interactions over time, while the condition by raw motion is not robust.

## 5 Conclusion

We tackle the task of text-guided 3D human-object interaction generation, aiming to accomplish this without relying on paired text-interaction data. To this end, we present InterDreamer that decouples interaction dynamics from semantics, formulating the task as retrieval-augmented generation and Markov decision process, where high-level planning and low-level control are introduced to generate semantically aligned human motion and initial object pose, while a world model is responsible for the object dynamics guided by the interaction. Our approach demonstrates effectiveness in this novel task, suggesting its potential for various real-world applications.

**Limitations.** The current utilization of dynamics modeling could be enhanced. A prospective improvement involves incorporating model-based learning techniques, which empower the agent to more effectively interact and learn a broader range of skills. The results may not be physically plausible and lead to artifacts in some cases, for example, foot skating. Hand poses are rough because they are missing from the dataset, but could be improved by integrating a physics simulator.

   LLM (\# of parameters) & Q1 Acc \(\) & Q1 Acc \(\) & Q2 Acc \(\) & Q2 Acc \(\) \\  GPT+ & 0.801 & 0.997 & 0.703 & 0.964 \\ Llama-2 (7B)  & 0.073 & 0.147 & 0.436 & 0.689 \\ Llama-2 (13B)  & 0.232 & 0.319 & 0.662 & 0.853 \\ Llama-2 (70B)  & 0.722 & 0.967 & 0.798 & 0.907 \\   

Table 4: **Ablation study on the high-level planning. Q1 and Q2 ask to identify the object category and the contact body part, respectively. We assess the accuracy by comparing the LLM’s responses with labels we annotate. Note that the text input to LLMs may contain ambiguities; for example, the annotation is “hand” when the motion uses “right hand.” We include Q1 Acc\({}^{*}\) and Q2 Acc\({}^{*}\) excluding ambiguous text.**

   Sim. to \(\) & Average & Out-of-Dist. \\  w/o planning & \(0.913\) & \(0.838\) \\ w/ planning & \(\) & \(\) \\   

Table 5: **Quantitative comparison of text similarity. The text processed by high-level planning is more similar to text in HumanML3D  on average, while addressing the distributional gap significantly for challenging out-of-distribution descriptions, compared to text without planning.**