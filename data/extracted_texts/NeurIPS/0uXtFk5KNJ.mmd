# BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models

Qijun Luo\({}^{1,2}\)   Hengxu Yu\({}^{1}\)   Xiao Li\({}^{1}\)

\({}^{1}\)The Chinese University of Hong Kong, Shenzhen

\({}^{2}\)Shenzhen Research Institute of Big Data

{qijunluo,hengxuyu}@link.cuhk.edu.cn, lixiao@cuhk.edu.cn

Corresponding Author

###### Abstract

This work presents \(\), an optimization method that leverages the block coordinate descent (BCD) framework with Adam's update rule. \(\) offers a memory efficient approach to the full parameter finetuning of large language models. We conduct a theoretical convergence analysis for \(\) in the deterministic case. Experimentally, we apply \(\) to finetune the Llama 3-8B and Llama 3-70B models using a single RTX3090-24GB GPU and \(4\)A100-80GB GPUs, respectively. The results confirm \(\)'s efficiency in terms of memory usage, running time, and optimization capability. Furthermore, the downstream performance evaluation based on MT-bench and math benchmarks shows that \(\) outperforms existing memory efficient baselines such as LoRA. It also demonstrates that \(\) can achieve comparable or even superior performance compared to Adam. Finally, the ablation study using SGD's update rule illustrates the suitability of BCD for finetuning LLMs. Our code can be easily integrated into any PyTorch-based codebase and is available at https://github.com/Ledzy/BAdam.

## 1 Introduction

Large language models (LLMs) such as GPT-4  and Llama 3  have shown its strong ability in language understanding, generation, reasoning, translation, etc . Due to its strong applicability, LLMs have been regarded as a feasible approach towards artificial general intelligence . Finetuning or adaptation has become an important step in applying pretrained LLMs to follow human instructions or perform specific downstream tasks .

**Backgrounds.** When GPU memory (RAM) is not a major limitation, full parameter finetuning methods--such as applying Adam to the entire set of parameters of LLMs--often offer the most flexibility for parameter search. However, executing such a full parameter training method typically requires a significant amount of GPU memory. For instance, to finetune an LLM with \(M\) billion parameters, Adam  necessitates roughly \(18M\) GB of GPU memory for successful training, and this estimate does not even account for the storage of activations used in the backpropagation (BP) process; see Section 2.2.1 for a detailed analysis. This requirement poses challenges for computational resources as models scale up, given the fact that GPU memory is often limited in practical settings.

Parameter efficient finetuning (PEFT) methods such as low-rank adaptation (LoRA) , Adapter , prompt- and prefix-tuning , among others, play a critical role in finetuning large language models under memory resource constraints. The principal idea of PEFT is to represent the parameter updates in a much lower-dimensional subspace and, consequently, the memory consumption is significantly reduced. Despite the success of PEFT methods, finetuning within a substantially lower-dimensional subspace may potentially limit downstream performance; see, e.g., .

The observations outlined above motivate us to explore a memory efficient full parameter optimization method without imposing low-rank constraint on the parameter update.

**Main results.** In this work, we have the following main contributions:

1. We propose a _block coordinate descent (BCD)-type_ optimization method with Adam's update rule, termed BAdam; see Section 2.1 for the detailed description. This method partitions the entire set of model parameters into \(D\) blocks, updating one block at a time using Adam's efficient update steps. BAdam offers a memory efficient solution to the full parameter finetuning of LLMs. For example, by partitioning a model with \(M\) billion parameters into \(D\) equal-sized blocks, BAdam requires only about \(2M+\) GB of GPU memory for successful mixed precision training; see Section 2.2.1 for detailed analysis. This leads to a significant reduction in memory demands compared to full parameter finetuning using Adam. Theoretically, we provide a convergence analysis for BAdam in the deterministic case, demonstrating that leveraging the BCD framework and Adam's update rule yields a convergent scheme; see Theorem 2.1.
2. We apply BAdam to finetune the Llama 3-8B and Llama 3-70B models using _a single RTX3090-24GB GPU_ and \(4\)_A100-80GB GPUs_, respectively. Specifically, we present in Section 3.1 BAdam's efficiency in both memory consumption and running time. In Section 3.2, we empirically verify BAdam's optimization capability via its fast convergence and the high rankness of its learned perturbations. We further evaluate the downstream performance of different methods using MT-bench and several math benchmarks; see Section 3.3. The results illustrate that BAdam generally outperforms existing memory efficient baselines such as LoRA. Importantly, BAdam achieves comparable average performance with Adam on math benchmarks and even surpasses Adam in instruction-following tasks evaluated by MT-bench score. Moreover, we conduct ablation study using SGD's update rule (BSGD) in Section 3.4. The results show that BCD variants maintain optimization capability compared to their full counterparts and even exhibit better downstream performance. It also demonstrates that BSGD can achieve similar downstream performance to BAdam, illustrating the effectiveness and suitability of BCD for finetuning LLMs.

We compare BAdam with several representative methods in Table 1. In summary, we believe that BAdam may serve as a viable alternative optimization method to state-of-the-art methods such as LoRA in scenarios with limited computing memory.

## 2 The BAdam Method

Block coordinate descent (BCD) method has a long history in optimization society, which can be traced back to the very origins of the discipline; see, e.g., . At each iteration, BCD maintains the majority of the optimization parameters at their up-to-date iteration values, while it approximately optimizes the objective function over the remaining parameters, resulting in a much lower dimensional problem.

BCD is known to be efficient for huge-scale problems where the number of optimization parameters is extensive , particularly when it significantly exceeds the number of data points / component

    &  & **Full parameter** & **Momentum and** & **Update precision** & **Gradient** \\  & & **training** & **second moment** & **accumulation** \\  Adam  & \(18M\) & ✓ & ✓ & Float32 & ✓ \\ LOMO  & \(2M+\) & ✓ & ✗ & Float16 & ✗ \\ LoRA  & \(2M+\) & ✗ & ✓ & Float32 & ✓ \\ BAdam & \(2M+\) & ✓ & ✓ & Float32 & ✓ \\   

Table 1: Algorithm feature summary. Here, \(M\) represents that the model to be trained has \(M\) billion number of parameters, \(r\) is the LoRA rank, \(m\) is the weight matrix dimension (here, we consider square weight matrices for simplicity), \(D\) is the number of transformer layers in LOMO or the number of partitioned blocks in BAdam. BAdam performs full parameter mixed precision update, while only requires memory that is comparable to LOMO and LoRA.

functions. Based on this main feature, we reveal an interesting link between BCD and the finetuning of LLMs. Namely, the finetuning process boils down to an optimization problem that needs to handle a huge number of trainable model parameters, while the number of training data points are relatively small. This setting matches exactly the advantage of the BCD scheme, providing the possibility to release the requirement on large GPU memory. We refer to Sections 3.2 and 3.4 for empirical verification on the power and suitability of BCD for finetuning LLMs.

### Algorithm Description and Convergence Result

In this subsection, we propose \(\), a block coordinate descent method embedded with Adam's update rule. The method is displayed in Algorithm 1 and illustrated in Figure 1. Formally, let us consider an abstract formulation for finetuning LLMs \(_{}\ ()=_{j=1}^{n}_{j}()\). Here, \(^{d}\) represents the concatenation of the vectorized parameters of the model, \(n\) is the number of training data points, and \(_{j}\) is the negative log-likelihood loss function for language modeling on the \(j\)-th training data point.

**Block partition and block coordinate descent framework.** At the \(t\)-th block-epoch, \(\) first generates an ordered block partition \(=\{_{1},,_{i},,_{D}\}\), which splits the whole model parameters \(^{d}\) into \(D\) blocks, i.e., \(=\{_{_{1}},,_{_{i}},,_{_{D}}\}\) with \(_{_{i}}^{d_{i}}\) and \(_{j=1}^{D}d_{j}=d\). The partition \(\) can be very flexible and is a unified representation. Given a large language model, one natural block partition is by transformer modules. Apart from this partition, one can also choose a small part of parameters from each transformer module and regard these parameters as one block \(_{_{i}}\). Note that \(\) may be either a deterministic or a random partition, as long as the aggregation of all the blocks \(\{_{_{i}}\}\) forms the whole set of parameters \(\). For instance, if we partition by consecutive transformer modules, we may list the blocks in \(\) in ascending order (e.g., from the input to the output module), descending order (e.g., from the output to the input module), or random reshuffling order.

We now present the optimization framework of \(\). Our core idea is to adopt the main spirit of BCD. Namely, _we approximately optimize over only one active block \(_{_{i}}\) at one time, given that the other inactive blocks are fixed at their up-to-date values_. Mathematically, at the \(t\)-th block-epoch, updating the current active block \(_{_{i}}\) amounts to solving the following subproblem:

\[_{_{i}}^{t+1}*{approx\,argmin}_{_{_{ i}}^{d_{i}}}\ _{j=1}^{n}_{j}(_{_{1}}^{t+1},,_{_{i-1 }}^{t+1},_{_{i}},_{_{i+1}}^{t},,_{_{D}}^{t}).\] (1)

When this approximate minimization becomes exact, this scheme is also known as Gauss-Seidel method or alternating minimization. Even with exact minimization, some literature still refers to it as BCD. One can see that subproblem (1) fixes the inactive blocks at their most recent values, and hence

Figure 1: Illustration of the proposed \(\), which is based on the block coordinate descent framework. Colors represent the states of the partitioned blocks in one block-epoch, including the active block, non-updated inactive blocks, and updated inactive blocks.

it is a much lower dimensional optimization problem compared to \(_{}\ _{j=1}^{n}_{j}()\), providing the possibility to implement the method in situations with limited GPU memory. Solving subproblem (1) sequentially for \(i=1,,D\) moves the block-epoch from \(t\) to \(t+1\).

**Update using Adam steps.** Similar to most of the concrete BCD methods, we propose to implement the approximate minimization subproblem (1) using several gradient-based steps starting at \(_{_{i}}^{t}\). Abstractly, \(\) executes the update

\[_{_{i}}^{t+1}(_{_{1}}^{t+1},, _{_{i-1}}^{t+1},_{_{i}}^{t},_{_{i+1}}^{t},, _{_{D}}^{t}).\] (2)

We choose the algorithmic procedure \(\) in (2) to be \(K\) Adam steps  starting at \(_{_{i}}^{t}\), in order to efficiently decrease the objective function. To specify the concrete Adam steps, we first note that the gradient of the objective function can be correspondingly decomposed as

\[()=[}{ _{_{1}}}}{ _{_{D}}}]^{}=[ }}_{i=1}^{n}_{i}()}}_{i=1}^{n}_{i}()]^{ }.\] (3)

We call \(}{_{_{i}}}\) in (3) the block gradient of the objective function \(\) over block \(_{_{i}}\). According to the main spirit of stochastic optimization methods, we select a batch of data points to compute a block stochastic gradient \(g_{_{i}}\) using the up-to-date iterates for approximating the block gradient, as outlined in Line 9 of Algorithm 1. With \(g_{_{i}}\), we construct the momentum and second moment for the active block \(_{_{i}}\) as shown in Line 10 - Line 11. Finally, we implement Adam update in Line 12. One may also invoke decoupled weight decay  into Line 12. In summary, Line 6 - Line 15 concretely implement the BCD update (1).

```
1input:\(_{1}\), \(_{2}\), \(\), \(K\), and learning rate \(\).
2initialization: block-epoch index \(t 0\) and model parameters \(^{0}\).
3whilestopping criterion not meetdo
4generate a block partition \(=\{_{1},,_{D}\}\) ;
5repeatforone block-epoch \(i 1,,D\)//BCDloop
6\(k 0; m_{_{i}}^{t,0} 0; v_{_{i}}^{t,0}  0;_{_{i}}^{t,0}_{_{i}}^{t}\) ; //Block initialization
7repeatfor\(K\)Adam steps to update the active block \(_{_{i}}\)
8\(k k+1\); //compute the blockstochastic gradient
9\(g_{_{i}}^{t,k}\) stochastic approx. of \(}}(_{_{1}}^{t+1}, ,_{_{i-1}}^{t+1},_{_{i}}^{t,k-1},_{_{i+1}}^{t },,_{_{D}}^{t})\);
10\(m_{_{i}}^{t,k}_{1}m_{_{i}}^{t,k-1}+(1-_{1})g_{_{i}} ^{t,k}, v_{_{i}}^{t,k}_{2}v_{_{i}}^{t,k-1}+(1- _{2})(g_{_{i}}^{t,k})^{2}\) ;
11\(_{_{i}}^{t,k} m_{_{i}}^{t,k}/(1-_{1}^{k})\), \(_{_{i}}^{t,k} v_{_{i}}^{t,k}/(1-_{2}^{k})\) ;
12\(_{_{i}}^{t,k}_{_{i}}^{t,k-1}-_{_{i}} ^{t,k}/(_{_{i}}^{t,k}}+)\) ; //Adam update
13
14 end for
15\(_{_{i}}^{t+1}_{_{i}}^{t,K}\);
16\(g_{_{i}},m_{_{i}},v_{_{i}}\)None ; //clear memory for grad and optim states
17
18 end for
19return learned model parameters \(^{t}\). ```

**Algorithm 1**BAdam: A block coordinate descent method with Adam's update rule.

It is important to note that \(\) differs from existing BCD with momentum approaches , which often maintain dense momentum vectors. \(\) is specifically designed for memory efficiency, and hence it clears the optimizer states in Line 15. Additionally, we do not offload the optimizer states, as they will no longer correspond to the updated block parameters in the next block-epoch. Thus, clearing the states in Line 15 and starting with zero initial states for every new active block in Line 6 are crucial for ensuring convergence, stability, and memory efficiency of our method.

The number \(K\) in Line 7 of Algorithm 1 is the only additional hyperparameter introduced by \(\), compared to Adam. We provide a detailed discussion on selecting \(K\) in Appendix B.2.

**Convergence result.** We provide a convergence analysis for \(\) in the deterministic case, aiming to establish that combining the block coordinate descent framework with Adam's update rule resultsin a convergent scheme. We consider the extension to the stochastic case as future work. Indeed, combining the analysis for Adam with stochastic gradients, as in [65; 27; 57], with our analysis for the block coordinate descent framework could be a feasible direction for such an extension. The informal theorem is presented below, while the formal theorem and proofs are put in Appendix D.

**Theorem 2.1** (informal).: _BAdam using deterministic gradients is a descent method, under certain commonly utilized conditions for analyzing block coordinate descent method and Adam. That is, after one block-epoch of updates for the whole model, we have_

\[(^{t+1})-(^{t})-( K) \|(^{t})\|^{2}.\] (4)

_Consequently, \(\) finds a \(\)-approximate stationary point within \((^{-2})\) iterations._

We conclude this section by noting that \(\) is essentially a block coordinate descent method, in which the BCD framework achieves low memory consumption. Apart from the chosen Adam's update rule, it is possible to propose other efficient optimization procedures for concretely implementing (1); see Section 3.4 for an ablation study where we also employ SGD's update rule.

### Analysis of Memory Consumption and BP Time

#### 2.2.1 Memory Consumption Analysis

We analyze the memory consumption of \(\), caused by storing the model parameters, gradient, and optimizer states. Let us consider a large language model with \(M\) billion parameters. We will use GB as the unit of GPU memory in the sequel.

We first analyze the memory cost of \(\) with mixed precision training. One needs to store the FP16 model parameters for the BP process, which costs \(2M\) memory. For a more precise update, the optimizer also maintains a master copy of a FP32 model, which costs \(4M\) memory. Then, it comes to store the gradient (converted to FP32), momentum, and second moment in FP32 precision, costing \(4M+4M+4M=12M\) memory. In total, \(\) needs roughly \(18M\) memory.

In terms of \(\), it needs to store the up-to-date model parameters (see Figure 1) in FP16 precision, which costs \(2M\) memory. Importantly, since \(\) only updates the active block at one time, we can store the model parameters, gradient, momentum, and second moment _only for the active block_\(_{_{i}}\) in FP32 precision, where the FP32 model parameters and gradient of the active block can be obtained by transforming their FP16 versions to the FP32 versions. Let us consider the simple case where the partitioned \(D\) blocks are equal-sized. Then, \(\) only needs in total

\[+}{}.\] (5)

Note that the above analyses do not account for the memory required to store activations, as this is associated with the BP process rather than the optimization method itself. Furthermore, gradient checkpointing  can be employed to reduce the memory requirement needed for storing activations. We display the actual memory consumption for finetuning the Llama 3-8B model in Section 3.1.

#### 2.2.2 BP Time Analysis for Consecutive Module-based Block Partition

We consider the specific case where the partitioned \(D\) blocks \(\{_{_{i}}\}\) are \(D\) consecutive transformer modules of LLMs. Thanks to the property of backpropagation, \(\) can reduce the computation time of BP compared to Adam and LoRA under the same amount of data utilization.

Let us consider one block-epoch of \(\), meaning that it has trained with \(K D\) data batches, where \(K\) is defined in Algorithm 1. We consider that each data point has the same sequence length and each transformer module has the same amount of parameters, in order to ease the analysis. Recall that a BP process consists of a forward pass and a backward pass. For the forward pass, \(\) has almost the same computational load as that of Adam, while LoRA requires more forward computation due to its extra low-rank adapters. Hence, it remains to consider the number of unit-backward-pass after utilizing \(KD\) data batches, where the unit-backward-pass is defined as a backward pass of a single data batch through a single transformer module. Importantly, \(\) only updates the active block, and hence the number of unit-backward-pass largely depends on the depth of the active block. For instance, if the input module or output module is the current active block, we need unit-backward-pass or only \(1\) unit-backward-pass, respectively. Thus, after one block-epoch (i.e., utilizing \(KD\) data batches), BAdam requires

\[K(1++D)=}.\] (6)

However, Adam and LoRA need to backward for all the \(D\) transformer modules, thus requiring \(KD^{2}\) unit-backward-pass after utilizing \(KD\) data batches.

Apart from saving the number of unit-backward-pass, some of the unit-backward-pass of BAdam may even take less computational time compared to that of Adam. Let us take the backward pass of the input module as an example. BAdam does not require explicit stochastic gradient computation of the model parameters of the intermediate modules \( z_{l}/_{l}\), where \(\{z_{l}\}\) are the activations of the intermediate modules and \(\{_{l}\}\) are the trainable model parameters of these modules. However, Adam needs to compute these quantity explicitly. We refer to Table 4 for an experiment illustration.

In summary, BAdam with consecutive module-based block partition saves computational load of the BP process compared to Adam and LoRA, after training with the same amount of data. We demonstrate this through experiments detailed in Section 3.1. If the module-based block partition is not consecutive, for instance, when one block consists of modules (such as matrices) from different transformer layers, we still expect that BAdam can reduce BP time to some extent, though not as significantly as indicated by (6).

## 3 Experiment Results

In this section, we evaluate the proposed BAdam on finetuning LLMs. Selected baselines include LOMO (essentially SGD) , LoRA , Galore , and Adam . All BAdam experiments for training the Llama 2-7B and Llama 3-8B models are conducted on a single RTX3090-24GB GPU, whereas BAdam experiments for the Llama 3-70B model use \(4\)A100-80GB GPUs. Experiments for the baseline methods are conducted using either a single RTX3090 or multiple A100 GPUs, depending on their memory requirements. Our implementation is based on Llama-Factory . Detailed experiment setup can be found in Appendix B.1.

### Memory Consumption and Wall-clock Running Time

In this subsection, we present the empirically measured memory consumption and wall-clock running time of BAdam and baseline methods. All the measurements in this subsection are based on finetuning the Llama 3-8B model on Alpaca-GPT4 dataset  using a single RTX3090-24GB GPU.

**Memory consumption.** We report the actual memory consumption of BAdam and the baseline approaches in Table 2 for finetuning the Llama 3-8B model, in which the memory consumption of Adam is estimated rather than tested. This result indicates that all of LOMO, LoRA (with a reasonable rank), and BAdam can finetune the Llama 3-8B model using a single RTX3090. It can be observed that all the methods have nearly the same memory cost for storing the model parameters, while LoRA requires slightly more memory due to its low-rank adapters. Furthermore, LOMO, LoRA, and BAdam significantly reduce memory consumption regarding the storage of the gradient and optimizer states compared to Adam. Moreover, it is easy to see that the total memory consumption (the last column of Table 2) is higher than the sum of the listed quantities. The additional memory costs arise from storing activations and training data, pre-allocated memory caches by PyTorch, and other buffers for intermediate computing results. Indeed, our tests show that BAdam can successfully finetune the Llama 3-8B model with input sequences of length 1024 using a batch size of 2, or input sequences of length 2048 using a batch size of 1, with a single RTX3090-24GB GPU.

**Wall-clock running time comparison.** We conduct experiments on finetuning the Llama 3-8B model for 3 epochs with each method and report the averaged wall-clock time per epoch; see Table 3. The forward time for three approaches are rather close. The slightly higher time cost for LOMO and LoRA attributes to additional operations for registering activations and the calculation of the low-rank adapters, respectively. Regarding backward time, BAdam reduces the time cost by nearly half compared to LoRA and LOMO, supporting the analysis in Section 2.2.2. It is important to note that the backward time for all methods includes the re-forward time due to gradient checkpointing, which diminishes the running time advantage of BAdam.

In Table 4, we conduct tailored experiments to further support our analysis in Section 2.2.2. It can be observed that: 1) backward for "Output module only" is almost time free, as it requires only 1 unit-backward-pass; 2) backward for "All modules" takes significantly more time, as it has to implement \(D\) unit-backward-pass; and 3) backward for "Input module only" takes less time than \(D\) unit-backward-pass (i.e., backward for "All modules"), since the former scheme does not need to compute the stochastic gradients of the intermediate modules' parameters.

### Optimization Capability

We verify the optimization capability of BAdam through both the training loss convergence and the effective rank of the learned perturbations. Experiments in this subsection correspond to exactly the same training process of the lower block of Table 5 for finetuning the Llama 3-8B model.

**Loss convergence.** In the left figure of Figure 2, we display the online training loss. From a pure optimization perspective, namely, in terms of driving the training loss lower, BAdam demonstrates better convergence behavior than LoRA when using 1e-5 as the initial learning rate. If the initial learning rate is set to 1e-6, BAdam initially converges slightly faster, but the two methods soon align as the learning rate becomes too small to make substantial progress.

**Effective rank of the learned perturbations.** We empirically measure the learning and optimization capability of BAdam through the effective rank of its learned perturbations, i.e., the difference between the learned weight matrix and the pretrained base weight matrix \( W:=W_{K}-W_{0}\). The

  
**Backward scheme** & **Backward time** \\  All modules & 0.64 seconds \\ Input module only & 0.33 seconds \\ Output module only & 0.03 seconds \\   

Table 4: Time spent on different backward schemes with batch size 2 for finetuning Llama 3-8B using a single RTX3090. The results are averaged over 100 backward passes.

  
**Method** & **Parameter** & **Gradient** & **Optimizer states** & **Memory consumption** \\  Adam & 16.1GB & 32.1GB & 96.6GB & 144.8GB+ \\ LOMO & 16.1GB & 0.5GB & — & 21.5GB \\ LoRA-rank100 & 16.7GB & 1.0GB & 3.1GB & OOM \\ LoRA-rank8 & 16.2GB & 0.1GB & 0.3GB & 22.3GB \\ BAdam & 16.1GB & 0.9GB & 2.6GB & 23.5GB \\   

Table 2: Actual memory costs of applying mixed precision training to finetune Llama 3-8B with gradient checkpointing using a single RTX3090. Note that LOMO only supports FP16 precision training. The maximum input sequence length is 728 and the batch size is 2.

  
**Backward scheme** & **Backward time** \\  All modules & 0.64 seconds \\ Input module only & 0.33 seconds \\ Output module only & 0.03 seconds \\   

Table 3: Time spent per epoch on forward, back-schemes with batch size 2 for finetuning Llama 3-8B using a single RTX3090. The single pass batch size is 2. The results are averaged over 3 epochs.

Figure 2: Optimization capability of BAdam for finetuning Llama 3-8B on Alpaca-GPT4 dataset. Left: Online training loss of LoRA and BAdam. Middle: Cumulative explained variance of BAdam’s learned perturbation to the 25th layer’s up-proj matrix. Right: Effective rank of Adam’s and BAdam’s learned perturbations.

cumulative explained variance "cvar" and the effective rank of matrix \(A^{m n}\) are defined as:

\[(r):=^{r}_{i}(A)^{2}}{_{j=1}^{\{m,n\}} _{j}(A)^{2}},(A):=\{r:(r)  0.9\},\]

where \(_{i}(A)\) is the \(i\)-th largest singular value of \(A\).

In the middle figure of Figure 2, we display cvar of \(\)'s update for the 25th layer's up-proj matrix. This result shows that \(\)'s update has a heavy tailed singular values distribution and is far away from a low-rank update. In the right figure of Figure 2, we plot the effective rank of the learned perturbation by \(\) and Adam through all modules of different transformer layers. Notably, \(\) achieves almost the same high rank update as Adam, which partly justifies \(\)'s learning and optimization capability.

### Downstream Performance Evaluation

In this subsection, we conduct supervised finetuning for the Llama 2-7B , Llama 3-8B, and Llama 3-70B models on the Alpaca-GPT4 and MathInstruct  datasets. The setting of hyperparameters is deferred to Appendix B.2.

**MT-bench results.** To illustrate the models' downstream performance, we report the MT-bench scores of the instruction-tuned models obtained by different methods for 3 epochs. We utilize two initial learning rates, 1e-5 and 1e-6, with a cosine scheduler for all methods. The results are displayed in Table 5.

Some remarks and observations on Table 5 are in order. 1) Using 1e-5 as the initial learning rate, the average MT-bench score over 3 epochs achieved by \(\) surpasses that of \(\) by a magnitude of \(\) for instruction-tuning the Llama 2-7B model. Regarding instruction-tuning the Llama 3-8B model using an initial learning rate of 1e-6, the average score returned by \(\) outperforms that of LoRA by a magnitude of \(\). 2) In most cases, \(\) can beat LoRA and Galore, albeit sometimes slightly, across the two learning rate settings and when evaluating checkpoints from different epochs for both the Llama 2-7B and Llama 3-8B models. This underscores the promising performance of our proposed method. 3) \(\) is on par with the performance of Adam for the Llama 2-7B model and outperforms Adam for the Llama 3-8B model, partly illustrating the power of the BCD optimization scheme in LLM finetuning. It is worth noting that \(\) is both memory and running time efficient. In terms of memory usage, it requires only a single RTX3090-24GB GPU for finetuning the Llama 3-8B model, while Adam needs multiple A100-80GB GPUs.

    &  \\ 
**lr** &  &  \\ 
**Method** & Adam & LOMO & LoRA & Galore & BAdam & Adam & LOMO & LoRA & Galore & BAdam \\ Epoch 1 & 4.41 & 4.01 & 4.77 & 4.70 & 4.79 & 4.62 & 3.99 & 4.59 & 4.12 & 4.71 \\ Epoch 2 & 4.73 & 4.06 & 4.84 & 4.83 & 5.21 & 4.94 & 4.02 & 4.86 & 4.17 & 4.83 \\ Epoch 3 & 5.16 & 4.11 & 4.01 & 4.88 & 4.87 & 5.13 & 4.06 & 4.81 & 4.26 & 4.88 \\ 
**Average** & 4.76 & 4.06 & 4.54 & 4.80 & **4.96** & **4.90** & 4.02 & 4.75 & 4.18 & 4.81 \\   \\ 
**lr** &  &  \\ 
**Method** & Adam(a) & LOMO & LoRA & Galore & BAdam & Adam & LOMO & LoRA & Galore & BAdam \\ Epoch 1 & – & 5.49 & 6.17 & 5.78 & 6.07 & 6.15 & 5.40 & 6.41 & 5.66 & 6.65 \\ Epoch 2 & – & 5.62 & 6.36 & 5.80 & 6.19 & 6.26 & 5.85 & 6.19 & 5.77 & 6.64 \\ Epoch 3 & – & 5.41 & 6.28 & 5.89 & 6.64 & 6.29 & 5.83 & 6.20 & 5.70 & 6.67 \\ 
**Average** & – & 5.51 & 6.27 & 5.82 & **6.30** & 6.23 & 5.69 & 6.27 & 5.71 & **6.65** \\   

Table 5: MT-bench scores of the instruction-tuned Llama 2-7B and Llama 3-8B on Alpaca-GPT4 dataset by different methods.

**Math benchmarks.** We also finetune the Llama 3-8B and Llama 3-70B models on the MathInstruct dataset for 3 epochs, and evaluate the trained model using math benchmarks across different domains. The results are shown in Table 6. In terms of average score, \(\) outperforms all the memory efficient baselines, and even slightly surpasses the benchmark score of Adam while requiring significantly less memory consumption compared to Adam. In particular, for the experiments on finetuning Llama 3-8B, \(\) outperforms LoRA in 4 out of 6 tasks, and surpasses LOMO and Galore in all the tasks by a large margin. For finetuning Llama 3-70B, \(\) beats LoRA in 5 out of 6 tasks.

### Ablation Study: SGD's Update Rule

In this subsection, we conduct ablation study to consider SGD's update rule in our BCD framework, leading to BCD with SGD (BSGD). Then, we compare the performance of \(\), BSGD and their full counterparts, i.e., Adam and SGD, to illustrate the power of BCD in LLMs finetuing.

**Optimization.** In the left and middle figures of Figure 3, we display the training loss of \(\), BSGD, and their full counterparts. It can be observed that BCD variants converge slightly slower but soon exhibit similar convergence behavior in terms of running time compared to their full counterparts. It is worth mentioning that, unlike the full counterparts, BCD only updates one block of parameters per data batch, demonstrating the strong optimization ability of BCD for LLMs finetuning.

**Downstream performance.** In the right table of Figure 3, we test the MT-bench scores of the four methods. It is quite interesting to see that BSGD significantly outperforms SGD (almost as good as \(\)), even though they have almost the same optimization convergence behavior. We suspect that the superiority of the BCD variants over their full counterparts possibly stems from the fact that BCD uses each data batch to update only one block of parameters, thereby better preserving the general knowledge of the pretrained model during finetuning. These improved downstream performance of BCD compared to their full counterparts further illustrate its suitability for LLM finetuning.

    \\  
**Method** & GSM8K & Aqua & MMLU-Math & SAT-Math & MATH & NumGLUE & **Average** \\  Base model & 25.9 & 22.8 & 33.7 & 39.5 & 12.8 & 34.5 & 28.2 \\ Adam & **54.5** & 40.5 & 44.3 & 51.4 & **18.4** & 55.4 & 44.1 \\  LOMO & 32.1 & 28.0 & 40.0 & 39.5 & 13.1 & 37.1 & 31.6 \\ LoRA & 47.5 & **44.9** & 45.3 & 50.9 & 14.5 & **56.9** & 43.3 \\ Galore & 33.1 & 37.4 & 41.2 & 42.7 & 15.0 & 36.9 & 34.4 \\ \(\) & 48.1 & 42.5 & **50.5** & **56.8** & 15.7 & 53.0 & **44.4** \\   \\  
**Method** & GSM8K & Aqua & MMLU-Math & SAT-Math & MATH & NumGLUE & **Average** \\  Base model & 52.4 & 46.5 & 52.2 & 58.2 & 21.2 & 37.9 & 44.7 \\  LoRA & 73.3 & 59.5 & 58.3 & 64.1 & **34.2** & 64.8 & 59.0 \\ \(\) & **78.8** & **63.4** & **64.2** & **76.4** & 26.2 & **67.3** & **62.7** \\   

Table 6: Zero-shot math benchmark scores of the finetuned Llama 3-8B and Llama 3-70B on MathInstruct dataset by different methods.

Figure 3: Ablation study for BCD variants and their full counterparts for finetuning Llama 3-8B on Alpaca-GPT4 dataset. Left and middle: Convergence behavior. Right: MT-bench scores.

### Additional Experiment Results

We provide more experiment results in Appendix C. Here are the summarized results: 1) In Appendix C.1, we conduct an ablation study on the ordering scheme of the partition \(\) in \(\), considering random reshuffling, ascending, and descending orders. In terms of convergence behavior, these three choices are competitive. We also provide an ablation study on the hyperparameter \(K\) in \(\), with \(K\) being chosen from \(\{10,50,100,200\}\). The results indicate that these four choices of \(K\) perform similarly in terms of convergence behavior. However, we observe that the convergence speed of choosing different \(K\) can vary across different models. We refer to Appendix B.2 for a detailed discussion on the selection of \(K\). 2) In Appendix C.2, we examine \(\)'s capability in classification tasks by training RoBERTa-large on SuperGLUE benchmarks. The results show that \(\) can achieve similar average scores as Adam. 3) In Appendix C.3, we conduct a preliminary continue pretraining (CPT) experiment. We apply \(\) to train the Llama 3.1-8B-Instruct model on the StarCoder-Python dataset  for about 1 epoch. The result shows that \(\) can effectively decrease the CPT loss, making it a strong candidate for CPT tasks when GPU memory is limited. 4) We display the memory consumption and running time costs for finetuning the Llama 2-7B model in Appendix C.4, which match the results for finetuning the Llama 3-8B model presented in Section 3.1.

In summary, our experiment results in Section 3 demonstrate that \(\) has the potential to serve as a competitive optimization method for finetuning LLMs when the GPU memory is limited, compared to state-of-the-art memory efficient methods such as LoRA.

## 4 Brief Literature Review

We briefly review several memory efficient finetuning methods in this section. A more comprehensive literature review is presented in Appendix A due to limited space.

One major branch for memory efficient finetuning of LLMs is parameter-efficient finetuning (PEFT), which freezes the pre-trained weight and only trains the additional injected parameters. LoRA , adapter , and prefix-tuning  belong to this class and have been verified to be effective in finetuning LLMs. Another line of works focus on memory efficient full parameter finetuning. MeZO  performs zero-th order SGD update without calculating the stochastic gradient, thereby only requires the memory of performing inference. LOMO  efficiently performs on-the-fly SGD update during the backward pass without storing the stochastic gradient. However, LOMO's implementation design prevents it from using gradient accumulation technique. Galore  reduces memory consumption by projecting the gradient into low-rank space. It requires constantly performing SVD to obtain the low-rank projector.

## 5 Conclusion and Discussions on Limitations

In this work, we have proposed the \(\) optimization method, which is built upon the block coordinate descent framework with Adam's update rule. We finetune the Llama 3-8B and Llama 3-70B models on the Alpaca-GPT4 and MathInstruct datasets by \(\) with a single RTX3090-24GB GPU and \(4\)A100-80GB GPUs, respectively. The results illustrated the efficacy of \(\) in terms of GPU memory consumption and running time. Empirically, \(\) exhibits better convergence behavior compared to LoRA and learns high rank update. Further downstream performance assessments have demonstrated \(\)'s superior performance in instruction finetuning and math finetuning, in comparison to LOMO, LoRA, and Galore. Additionally, \(\) has on par or even better downstream performance compared to Adam. In summary, we believe that \(\) may serve as a viable alternative for finetuning LLMs with limited memory resources.

**Limitations.** Our focus has been on applying \(\) for supervised finetuning. Extending its application to preference optimization represents another opportunity to demonstrate \(\)'s capabilities. Moreover, our CPT experiment using \(\) is only preliminary. Exploring extensively \(\)'s performance in the CPT setting is an interesting direction. We leave these directions for future improvements.

**Broader impacts.** Our proposed method significantly lowers the barrier to full parameter finetuning of large models for a broader range of researchers. This is a technical algorithmic contribution that does not yield explicit negative societal impacts. However, it carries a risk of misuse.