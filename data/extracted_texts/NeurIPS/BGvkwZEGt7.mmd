# Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning

Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning

Xinyi Wang\({}^{1}\), Wanrong Zhu\({}^{1}\), Michael Saxon\({}^{1}\), Mark Steyvers\({}^{2}\), William Yang Wang\({}^{1}\)

\({}^{1}\)Department of Computer Science, University of California, Santa Barbara

\({}^{2}\)Department of Cognitive Sciences, University of California, Irvine

{xinyi_wang, wanrongzhu, saxon}@ucsb.edu,

msteyver@uci.edu, william@cs.ucsb.edu

###### Abstract

In recent years, pre-trained large language models (LLMs) have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as in-context learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. Current understandings of the underlying mechanisms by which this capability arises from regular language model pretraining objectives remain disconnected from the real-world LLMs. This study aims to examine the in-context learning phenomenon through a Bayesian lens, viewing real-world LLMs as latent variable models. On this premise, we propose an algorithm to select optimal demonstrations from a set of annotated data with a small LM, and then directly generalize the selected demonstrations to larger LMs. We demonstrate significant improvement over baselines, averaged over eight GPT models on eight real-world text classification datasets. We also demonstrate the real-world usefulness of our algorithm on GSM8K, a math word problem dataset. Our empirical findings support our hypothesis that LLMs implicitly infer a latent variable containing task information. 1

## 1 Introduction

Transformer-based  pre-trained large language models (LLMs) have demonstrated significant advancements in a variety of natural language processing (NLP) tasks. As the size of these LLMs increases, they gain "in-context learning" capabilities, whereby the models achieve state-of-the-art (SOTA) or near-SOTA performance by conditioning on a small number of demonstration examples at inference time, without any need for updating model parameters . Below is an example input sequence for semantic analysis with in-context learning:

Great movie. Positive. The worst movie ever. Negative. Can't wait to see the second movie!

The first two lines are two demonstrations, and the third line is a test input. We expect an LLM to output the correct label Positive as a continuation.

In-context learning has been demonstrated to be an effective technique for a wide range of NLP tasks. However, it is sensitive to the choice, format, and even the order of the demonstrations used . This makes achieving optimal performance with in-context learning a significant challenge, requiring real human effort to adjust the format and selection of demonstration examples. Heuristic solutions, such as selecting demonstrations based on the similarity between the demonstrations and test input  have been proposed, but a comprehensive understanding of why certain demonstrations are effective while others are not remains elusive. Additionally, the mechanisms by which LLMs acquire in-context learning capabilities through training on natural text under the standard language model pre-training objective are not fully understood. Recent works on understanding in-context learning provide valuable insights and theoretical results , but are limited in scope, focusing on synthetic experiments to validate their hypotheses, while it remains unclear if these results generalize to LLMs pre-trained on real-world natural language data. Xie et al.  introduced a prominent result providing a latent topic (concept) variable interpretation for in-context learning. They showed that the in-context learning predictor approaches the Bayes optimal predictor when the number of demonstrations approaches infinity, under the assumption that both the pre-training data distribution and task-specific data distribution are Hidden Markov Models (HMM). However, the assumption that the data generation process is Hidden Markovian makes extrapolation of the result to natural language questionable, and restricts empirical verification to synthetic data with toy models.

We are inspired by this prior work and introduce a more general and natural explanation built on realistic assumptions, which gives rise to a practical demonstration selection algorithm. Our explanation is inspired by the generation process of a topic model, i.e. a simple latent variable model:

\[P(_{1:T})=_{}P(_{1:T}|)P()d\]

Where \(\) represents a potentially high dimensional topic/concept variable, \(\) is the space of the topic/concept variable, and \(_{1:T}\) refers to the token sequence of a piece of text. Note that the topic model here refers to the modern neural topic models . On the other hand, generative LLMs model text data according to the general probabilistic decomposition:

\[P(_{1:T})=_{i=1}^{T}P(_{i}|_{i-1},...,_{1})\]

While in practice, LLMs generate new tokens based on all previous tokens, we investigate whether a simplified assumption similar to that of topic models can be made for LLMs:

\[P_{M}(_{t+1:T}|_{1:t})=_{}P_{M}(_{t+1:T}|)P_{M}(|_{1:t})d\]

In this scenario, the generated tokens are assumed to be conditionally independent of previous tokens, given the latent topic (concept) variable that acts like an approximate sufficient statistic for the posterior information related to the prompt \(_{1:t}\). For in-context learning, this concept variable includes format and task information. By conditioning on an appropriate latent concept variable, LLMs would generate the desired continuation with \(P(_{t+1:T}|)\). As LLMs do not explicitly learn a latent variable distribution like LDA-style topic models , we can instead utilize this formulation under an Empirical Bayesian formulation inspired by Lester et al.  to only approximate the optimal latent variable value for a desired task, using a small LLM (with less than 1B parameters), which is computationally efficient.

We empirically validate our explanation by selecting examples (\(_{1:t}\) in the equations) that are most likely to infer the optimal latent variable value (those with the highest posterior probability \(P(|_{t+1:T})\)). We then directly use them as demonstrations for in-context learning with other larger LLMs (up to 175B parameters) and observed a significant performance improvement. The generalization of demonstrations between LLMs is likely a result of similar pre-training data distributions.

While our work is inspired by that of Xie et al. , our approach differs significantly in both theoretical analysis and experimental settings. Our main contributions are as follows:

* **We assume a general data generation process** specified by a three-variable causal graph, without constraints on the distribution function or the number of demonstrations.
* **We prove under these realistic assumptions** that the in-context learning predictor can reach the Bayes optimal predictor with a finite number of demonstrations chosen using the latent concept variable.
* **We introduce an efficient, practical demonstration selection algorithm** based on our theoretical results, which can select demonstrations using a small LLM and then directly generalize the demonstrations to other LLMs. The effectiveness of our algorithm is empirically validated using real-world LLMs on both text classification tasks and math word problems.

Our goal is to close the gap between theoretical understandings and real-world LLMs. To the best of our knowledge, our proposed latent variable explanation of in-context learning is the first Bayesian explanation that yields an effective algorithm in real-world scenarios.

## 2 Theoretical Analysis

In in-context learning, the prompt \(w_{1:t}\) is composed of several demonstrations and a test input. The generated tokens \(w_{t+1:T}\) represent the model's prediction for the test input.

### Notations and Problem Setting

Suppose the objective of our task is to predict a discrete target variable \(Y\), given a token sequence \(X\), where \(\) is the space of all possible token sequences. \(\) is a potentially high dimensional latent variable, where \(\) is the high dimensional space of the variable. Unlike the traditional topic model, \(\) is not assumed to be discrete, but continuously distributed over \(\). To define the data generation process, we posit the existence of an underlying causal relation between \(X\), \(Y\), and \(\). We examine two potential directions of this causal relation, namely \(X Y\) and \(Y X\), which can be represented mathematically as the following structural equations:

\[Y=f(X,,) X=g(Y,,)\]

Here \(\) is an independent noise variable, \(f:\) and \(g:\) are two deterministic functions. Furthermore, we denote the joint data distribution by \(X,Y, P\), and assume that \(Y\) is sampled from a uniform distribution over \(\). The distinction between these two directions is crucial, as it allows us to utilize the direction in which the child variable (\(Y\) or \(X\)) is independent of the other variables, given its parents.

We hypothesize that the causal direction depends on the nature of the task. For instance, in the task of predicting the sentiment (\(Y\)) of a movie review (\(X\)), it is reasonable to assume that the opinion about the movie is formed before writing the review, thus making \(Y\) the cause of \(X\), along with the task concept of "writing a passage to express one's opinion about the movie" (\(\)). Conversely, for the task of classifying whether a product review (\(X\)) is helpful to other customers (\(Y\)), it is the quality of the review (\(X\)) that cause other customers to upvote it (\(Y\)), along with the task concept of "rating the helpfulness of this review" (\(\)). _In the rest of the paper, we will focus on the \(X Y\) direction and leave a detailed discussion of the other direction in the Appendix._

Suppose we are interested in a task (e.g. semantic analysis) denoted by \(d\), where \(\) is the space of all possible tasks. We assume there is an injective function between \(\) and \(\). i.e. for each task \(d\), there is a concept variable \(^{d}\), such that each data \((X^{d},Y^{d})\) sampled from task \(d\) is generated by:

\[Y^{d}=f(X^{d},^{d},)\]

To perform in-context learning with an LLM (generically denoted by model label \(M\)), we condition on a fixed set of \(k\) demonstration examples \((X_{1}^{d},Y_{1}^{d}),(X_{2}^{d},Y_{2}^{d}),...,(X_{k}^{d},Y_{k}^{d})\) sampled from task \(d\).

Following previous works [24; 26], as we are not using any instruction fine-tuned models, we do not include a task description in the prompt, with the aim of focusing on the examination of the demonstrations. To naturally project \(\) into the token space \(\), we define injective mappings \(^{d}:\), which are typically defined by human understanding of the task \(d\). e.g. for sentiment analysis, \(^{d}\) map positive class to the token "positive" and negative class to the token "negative". Additionally, a delimiter token \(^{d}\) is defined, typically an empty space or a new line token, to separate the demonstrations when concatenated. We denote the LLM output probability of \(X\), \(Y\), and \(\), with the aforementioned preprocessing applied, by \(P_{M}^{d}\):

\[P_{M}(^{d}(Y)|X_{1}^{d},^{d}(Y_{1}^{d}),^{d},...,X_{k}^{d},^ {d}(Y_{k}^{d}),^{d},X)=P_{M}^{d}(Y|X_{1}^{d},Y_{1}^{d},...,X_{k}^{d},Y _{k}^{d},X)\]

### Problem Analysis and Theoretical Results

Suppose a set of observed data sampled from task \(d\), denoted as \(^{d}\), is available, allowing for the selection of the \(k\) most suitable demonstrations from it. For any incoming test example \(X\), we have:

\[P_{M}^{d}(Y|X_{1}^{d},Y_{1}^{d},...,X_{k}^{d},Y_{k}^{d},X)=_{}P_{M} ^{d}(Y|,X)P_{M}^{d}(|X_{1}^{d},Y_{1}^{d},...,X_{k}^{d},Y _{k}^{d},X)d\] (1)Here, we assume the sampling of the test example is independent of the sampling of the demonstrations, so \(Y\) is independent of the demonstrations given \(\) and \(X\). We also assume that the pre-trained data distribution \(P_{M}^{d}\) is a suitable approximation of the assumed data distribution \(P\):

**Assumption 2.1**.: Assume that \(P_{M}(X)=P(X)\), and \(P_{M}^{d}(Y|,X) P(Y|,X)\) for \(X Y\).

Note that the assumption that a large language model captures the true distribution of language is fairly common in the literature studying LLMs [50; 34; 47]. With this assumption, we establish:

**Proposition 2.2**.: _If task \(d\) follows the \(X Y\) direction, then \(*{arg\,max}_{y}P_{M}^{d}(Y=y|^{d},X)\) is the Bayes optimal classifier._

In this case, only when \(P_{M}^{d}(|X_{1}^{d},Y_{1}^{d},...,X_{k}^{d},Y_{k}^{d},X)\) completely concentrate on \(^{d}\), can the in-context learning classifier become the Bayes optimal classifier :

**Theorem 2.3**.: _If task \(d\) follows the \(X Y\) direction, then the in-context learning classifier_

\[*{arg\,max}_{y}P_{M}^{d}(Y=y|X_{1}^{d},Y_{1}^{d},...,X_{k}^{d},Y_{k}^{d},X)\]

_always has a higher or equal probability of misclassification to the Bayes optimal classifier \(*{arg\,max}_{y}P_{M}^{d}(Y=y|^{d},X)\). Equality only holds when_

\[ x,\ P_{M}^{d}(^{d}|X_{1}^{d},Y_{1}^{d},...,X_{k}^{ d},Y_{k}^{d},X=x)=1.\]

A similar argument can be made for the \(Y X\) direction. 2 Here, Equation (1) would become:

\[P_{M}^{d}(X|Y_{1}^{d},X_{1}^{d},...,Y_{k}^{d},X_{k}^{d},Y)=_{}P_{M}^ {d}(X|,Y)P_{M}^{d}(|Y_{1}^{d},X_{1}^{d},...,Y_{k}^{d},X_ {k}^{d},Y)d\] (2)

Note that the left-hand side of Equation (1) and Equation (2) are similar to the direct and channel method introduced by Min et al. . However, our analysis differs from theirs in that we do not treat (\(Y X\)) as the universally superior channel direction for modeling in-context learning, rather arguing that depending on the end task, the causal direction (\(X Y\)) is sometimes better. This view is supported by our empirical results in Appendix B.

## 3 Method

Here we demonstrate how the proposed theory can be practically applied to select optimal demonstration examples. Since latent variable \(\) encodes both the task and format information, the whole distribution over \(\) is too complex to model. Unlike traditional topic models, we will only focus on estimating an optimal value \(^{d}\) corresponding to task \(d\).

First, we perform _latent concept learning_, wherein the task latent \(^{d}\) is learned as a set of new token embeddings using prompt tuning over the full demonstration candidate set. With this optimal task latent, we then perform _demonstration selection_, where a smaller set of demonstrations is chosen to maximize the likelihood of postfixing the latent concept tokens. We only need to use a small LLM to do the above steps to obtain an optimal set of demonstrations that can be directly transferred to other LLMs. Figure 1 is an overall illustration of our proposed method.

Figure 1: An overview of our proposed two-phased algorithm. Demonstration selection and latent concept learning share the same LLM as demonstration selection needs to reuse the learned concept tokens, while at the in-context learning time, any other generative LLMs can be used. Here we only illustrate the \(X Y\) direction. The \(Y X\) direction can be illustrated similarly by exchanging \(X\) and \(Y\) in the above figure.

### Latent Concept Learning

We want to first find the optimal value of the latent concept variable \(^{d}\) corresponding to a task \(d\). As \(_{y}P_{M}^{d}(Y=y|^{d},X)\) is the Bayes optimal classifier according to Proposition 2.2, \(^{d}\) should be able to minimize \(-_{X,Y,d}[ P_{M}^{d}(Y|^{d},X)]\) for the \(X Y\) direction. In practice, we try to align \(^{d}\) to the token embedding space by adding new tokens to the vocabulary. After this alignment, we hope to be able to use the learned new tokens of \(^{d}\) as regular tokens.

More specifically, building upon the methodology proposed by Lester et al. , for each specific task \(d\), \(c\) new concept tokens (denoted as \(^{d}\)) are added to the original vocabulary of LLM \(M\) to represent the corresponding task concept \(^{d}\). Subsequently, the embedding of these new tokens \(E_{new}(^{d})\) is fine-tuned while freezing the remaining parameters of LLM \(M\). The variable \(c\) is treated as a hyperparameter. In practice, in order to condition on \(^{d}\), the corresponding \(c\) concept tokens are appended to the input \(X\) (or \(Y\)) as shown in the example provided below, where \(c=2\):

``` Input: Dataset \(=\{(x_{i},y_{i},d_{i})\}_{i}\) associated with a set of tasks \(\), LLM \(M\), number of concept tokens per task \(c\), learning rate \(\), and number of training steps \(N\). Output: LLM \(M^{}\) with fine-tuned concept tokens.  Add \(c||\) new tokens to the vocabulary. i.e. The concept tokens \(^{d}\) for each task in \(\). Randomly initialize their embeddings \(E_{new}\). Freeze all parameters in \(M\) except \(E_{new}\); for\(=1\)to\(N\)do  Sample a random batch \(B\) in \(\) and initialize gradient \(g 0\); for each data point \((x,y,d)\) in \(B\)do \(g=g+^{d})}{ E_{new}}\); endfor \(E_{new}=E_{new}- g\); endfor ```

**Algorithm 1** Latent concept learning

By giving the above input tokens, we ask the LLM to predict the correct label Positive for us. Note that <sentiment_token_1> here is just a label assigned to the newly added concept token. It can be anything as long as it does not overlap with the original vocabulary of LLM.

The fine-tuning objective would then be minimizing \((^{d})=_{X,Y}[(X,Y;^{d})]\), where

\[(X,Y;^{d})=- P_{M}^{d}(Y|^{d},X)& X Y\\ - P_{M}^{d}(X|^{d},Y)&Y X. \]

Theoretically, if we can minimize the above loss function, a Bayes optimal classifier can be obtained, and the concept tokens would be a reasonable delegate of the real latent concept variable:

**Proposition 3.1**.: _When \((^{d})\) is minimized, \(P_{M}^{d}(Y|^{d},X)=P(Y|^{d},X)\) for \(X Y\). If the LLM \(M\) is invertible, then \(^{d}=^{d}\).3_

We denote the LLM \(M\) with fine-tuned concept tokens by \(M^{}\). Since we add the concept tokens into the regular token vocabulary, the raw LLM output probability \(P_{M^{}}(^{d}|_{1:t})\) (\(_{1:t}\) denote a given prompt) would be in the token sequence space \(\) instead of the concept space \(\). Since learning all possible \(^{d}\) is infeasible, we propose to approximate the concept space \(\) by sampling a diverse subset of tasks \(\). Then the estimated conditional probability of \(^{d}\) would be:

\[_{M^{}}^{d}(^{d}|_{1:t})=}^{d }(^{d}|_{1:t})}{_{t}P_{M^{}}^{d}( ^{t}|_{1:t})}\]

To obtain the concept tokens for all tasks in \(\), we fine-tune all tasks together with the loss \(_{d}(^{d})\). We summarize the proposed algorithm in Algorithm 1.

Note that the embedding matrix of a generative LLM is shared on both the input and output sides. So while we only see the concept tokens on the input side at the training time, they can be viewed as regular word tokens that can be generated on the output side.

### Demonstration Selection

According to Theorem 2.3, for a task \(d\), to make the in-context learning classifier closer to the Bayes optimal classifier, we need to select demonstrations \((X_{1}^{d},Y_{1}^{d}),...,(X_{k}^{d},Y_{k}^{d})\) that maximize \(P_{M}^{d}(^{d}|X_{1}^{d},Y_{1}^{d},...,X_{k}^{d},Y_{k}^{d},X)\) for all \(X\). Then our goal then becomes selecting demonstrations that can best infer the task concept for all test inputs on average:

\[*{arg\,max}_{X_{1}^{d},Y_{1}^{d},...,X_{k}^{d},Y_{k}^{d}}_{X}[P_{M}^{d}(^{d}|X_{1}^{d},Y_{1}^{d},...,X_{k}^{d},Y_{k}^{d},X)]\]

As test examples are sampled independent of the demonstrations, and \(P_{M}(X)=P(X)\) according to Assumption 2.1, we have

\[_{X}[P_{M}^{d}(^{d}|X_{1}^{d},Y_{1}^{d},...,X_{k}^{d},Y_{k}^{d },X)]=P_{M}^{d}(^{d}|X_{1}^{d},Y_{1}^{d},...,X_{k}^{d},Y_{k}^{d})\]

If we assume each demonstration is also sampled independently, we have:

\[P_{M}^{d}(^{d}|X_{1}^{d},Y_{1}^{d},...,X_{k}^{d},Y_{k}^{d})=^{k}P_{M}^{d}(^{d}|X_{i}^{d},Y_{i}^{d})}{P_{M}^{d}(^{d})^{k-1}}\]

Assuming that \(\) has a uniform prior, then our goal becomes finding the top \(k\) demonstrations that maximize \(_{M^{}}^{d}(^{d}|X_{i}^{d},Y_{i}^{d})\). Note that the independence between demonstrations is a simplified assumption to reduce the combinatory search space of \((X_{1}^{d},Y_{1}^{d}),...,(X_{k}^{d},Y_{k}^{d})\). In practice, selected demonstrations are likely correlated as some demonstrations may work well together but not necessarily work well by themselves. However, it would be too expensive to search the \(O(|^{d}|^{k})\) combinations over the candidate set \(^{d}\). In practice, this simplification works reasonably well. We leave this combinatory search problem to future research.

Also, as we are using an LLM to approximate the data distribution, the order of the demonstrations might matter. We will show in the Experiment section that the order does not matter, so no reordering of the selected demonstrations is needed. The full selection algorithm is shown in Algorithm 2.

``` Input: dataset \(^{d}\) for a task \(d\). LLM with fine-tuned concept tokens \(M^{}\). The number of demonstrations \(k\). Output: A set of selected demonstrations. for each \((X^{d},Y^{d})\) in \(^{d}\)do  Compute \(_{M}^{d}(^{d}|X^{d},Y^{d})\);  endfor  Select top \(k\) examples with the largest \(_{M}^{d}(^{d}|X^{d},Y^{d})\), denoted as \((X_{1}^{d},Y_{1}^{d}),...,(X_{k}^{d},Y_{k}^{d})\); ```

**Algorithm 2** Demonstration selection

## 4 Experiments

**Datasets.** We conduct experiments on eight datasets from five different types of NLP classification tasks: sentiment analysis, linguistic analysis, topic classification, emotion classification, and hate speech detection. For sentiment analysis, we choose the Stanford Sentiment Treebank (SST2) dataset  from the GLUE benchmark  and the financial phrase bank (FPB) dataset . SST2 is constructed based on movie reviews labeled "positive" or "negative", and FPB is based on financial news labeled "positive", "negative", or "neutral". For linguistic analysis, we choose the Corpus of Linguistic Acceptability (COLA) dataset  from the GLUE benchmark, based on sentences collected from linguistic books, labeled with "acceptable" or "unacceptable". For topic classification, we choose the DBpedia ontology classification dataset , based on DBpedia 2014 , labeled with 14 different ontology classes. For emotion classification, we choose the dataset from Chatterjee et al.  and Saravia et al. , both of which are collected from Twitter. Chatterjee et al.  (EmoC)predict emotion given a three-turn contextual dialogue, while Saravia et al.  predict emotion given a Twitter message with clear emotion. For hate speech detection, we choose the online hate speech detection dataset (ETHOS) , collected from online social media platforms. Here we detect two types of hate speech: sexual orientation (ETHOS-SO) and religion (ETHOS-R). While in Section 2, we assume that all tasks share the same label space \(\), here we relax such assumption and allow a different number of labels for different tasks. We use minimal formatting to process each example. A detailed description of the datasets and our data processing procedure can be found in Appendix B.

**Experiment settings.** To determine the causal direction for each task, we select the direction that can give higher accuracy when using random demonstrations4. We adopt the \(Y X\) direction for sentiment analysis, topic classification, and emotion classification tasks, which is consistent with the intuition that people usually have some sentiment, topic, or emotion in mind before writing a piece of text. We adopt the \(X Y\) direction for the linguistic analysis and hate speech detection type of tasks. While this is less intuitive, we can understand this as linguistic error and hate speech detection are more of a post hoc task in contrast to the previous tasks.

Without specification, we use \(k=4\) number of demonstrations and \(c=10\) number of concept tokens per dataset for our experiments, as the context length of GPT2 is 1024, and a larger number of demonstrations may not be able to completely fit into it. We use GPT2-large to learn the concept tokens and then compute the probability of each candidate demonstration example. We select our demonstrations from a randomly selected 100 example subset of the train set as the candidate set \(^{d}\). We use the same set of demonstrations selected by GPT2-large for all other LLMs. We test the performance of the selected demonstrations using at most 1000 examples randomly sampled from the test set. Each experiment is repeated for five runs with different random seeds (the randomness comes from the sampling of the candidate set and the sampling of the test set). We adopt a large portion of the code from Min et al. , which is based on Huggingface .

**Baselines.** We consider the following baselines:

* **Uniform**: We uniformly select \(k\) demonstrations from \(\) for each test example.
* **Similar**: According to Liu et al. , demonstrations that are semantically similar to the test example would have more performant. Following their method, we use a pre-trained sentence Transformer  to calculate the cosine similarity between the demonstrations and test examples. We choose the top \(k\) similar demonstrations from \(\) for each test example.

**Main results.5** Figure 2 shows our main results averaged over all eight datasets, using the first-generation GPT2s and GPT3s, without any instruction fine-tuning  or Reinforcement Learning from Human Feedback (RLHF) . Our method significantly outperforms baselines on eight different LLMs, with 12.5% relative improvement to the uniform selection baseline on average, which shows the effectiveness of our method. The demonstrations selected by our method are exclusively based on GPT2-large, while the same set of demonstrations can be generalized to all other GPTs.

**Results with non-GPT models.** In Figure 3a, we test the demonstrations selected by our method using GPT2-large on more LLMs (GPT3 , GPT3-instruct , GPT-J , OPT , and LLaMA ) with similar sizes (6-7B), and show that the selected demonstrations improve in-context learning performance of all of them. The fact that GPT3-curie obtains the largest performance improvement is likely because similar pre-training data distributions help the generalization of the

Figure 2: Accuracy of 4-shot in-context learning using demonstrations selected by our method and other baselines, averaged over eight datasets. Our demonstrations are selected using GPT2-large, and the same set of demonstrations is then applied to all other LLMs.

selected demonstrations. Different-size GPT2 models share the same pre-training corpus , while GPT3s are pre-trained on a dataset expanded from the GPT2 pre-training corpus . Thus the pre-training distribution of GPT3-curie and GPT2-large can be assumed to be similar.

**Results on GSM8K.** Since our primary goal is to connect the theory with real-world models and datasets, we did not try to include harder tasks in the main results in Figure 2. In practice, our proposed method is most effective with hard tasks that even parameter-efficient fine-tuning with smaller models cannot outperform in-context learning with the same or larger models. To showcase the usefulness of our proposed algorithm, We added a new dataset, GSM8K , which is a math word problem-solving dataset with chain-of-thoughts solutions. Table 1 shows the test accuracy of the final numerical answer with greedy generation. We randomly select a test set of 200 examples instead of using the full test set for computation efficiency. 6

As shown in the first row of Table 1, prompt tuning with ten new tokens can only obtain less than 4% accuracy on the GSM8K test set. The last four rows show the in-context learning results with different size Llama 2 models  and ChatGPT. Our proposed demonstration selection method (last two columns) significantly outperformed the Uniform and Similar baseline. We also find that the demonstrations selected with a larger model (7B) are more effective than those selected with a smaller model (1.5B). The results show that our demonstration selection method is a good choice under a low data setting, with a small computing budget and minimal inference latency. Our proposed method can also potentially be combined with other prompting techniques  to boost performance further.

**Learned tokens v.s. Random tokens.** To confirm the critical role of the latent concept variable in the proposed demonstration selection algorithm, we compare the performance of using the learned concept tokens versus using randomly selected tokens from the original vocabulary in Figure 2(b). The demonstrations selected by random tokens only obtain the same performance as randomly selected demonstrations, showing that the performance gain of our method comes from the learned concept tokens containing the task and format information, not other elements of our algorithm.

\(k\) **ablation study.** While we use \(k=4\) demonstrations for all experiments, we also test the effectiveness of our method using different \(k\). As shown in Figure 3(a), our method significantly outperforms the random selection baseline with \(k=2\), 4, 8, and 16. To fit in large \(k\)s, we use GPT3-ada with a longer context length (2048). Note that for real-world tasks, it is in general not true that more demonstrations guarantee higher performance . We can see that the uniform baseline performance increases from \(k=2\) to \(k=8\), then drops a little at \(k=16\). Our method improves the uniform baseline by around 5% absolute for all \(k\)s, while \(k=4\) improves the most (6.6%). Our method appears to have a diminishing effect when \(k\) becomes larger, which is likely because the effect of more demonstrations overwhelms the effect of demonstration choices.

    & Uniform & Similar & Ours w/ Llama 2 (7B) & Ours w/ GPT2-XL (1.5B) \\  Prompt tuning & - & - & 3.7 & 1.3 \\ Llama 2 (7B) & 11.4 & 13.1 & 19.3 & 15.9 \\ Llama 2 (13B) & 17.0 & 18.3 & 21.6 & 20.5 \\ Llama 2 (70B) & 50.2 & 53.5 & 54.3 & 52.9 \\ ChatGPT (gpt-3.5-turbo) & 76.5 & 78.1 & 81.2 & 80.4 \\   

Table 1: Prompt tuning and 4-shot in-context learning accuracy on a subset of GSM8K test set. Our demonstrations are selected with either 7B Llama 2 or GPT2-XL

Figure 3: In-context learning accuracy averaged over all eight datasets.

\(c\) ablation study.While we use \(c=10\) number of concept tokens for all experiments, we also investigate the effect of different \(c\) on our method. When \(c\) is small (\(c=5\)), the concept tokens cannot effectively capture the task and format information, thus cannot improve the performance. When \(c\) increases from 10 to 20, we observe a drop in the performance. It is likely because the selectivity of the concept tokens decreases when \(c\) increases. The longer the concept token sequence is, the more likely it will contain meaningless tokens that do not contribute to demonstration selection.

**Effect of demonstrations' order.** We find that the demonstrations selected by our method are insensitive to their order in most cases.7 An exception is the EmoC dataset, where our method has a high variance. On the contrary, Lu et al.  found that the order of the demonstration matters, and a good ordering cannot be transferred between different LLMs. We suspect that the ordering only matters when the demonstration selection method is not robust. Since Lu et al.  randomly selects one set of demonstrations for the whole test set, the variance in performance is high with different demonstrations, thus ordering matters. And since such ordering is not transferable while our selected demonstrations are highly transferable, we suspect the core task information is stored in the content of the demonstrations, while the ordering mainly captures model-specific artifacts.

**Qualitative analysis.** In Figure 5, we provide a t-SNE  projection of the learned concept token embeddings. The tokens corresponding to semantically similar tasks are close together. Note that this result only aims to provide a straightforward illustration of concept tokens. The effect of concept tokens should be understood by the previous quantitative results.8

We also list the top 4 selected demonstrations in Table 14 in Appendix B. Compared to the examples with lower scores, the selected examples for GSM8K have more deductive reasoning (i.e. with the connecting words'so', 'then', 'thus', etc.), instead of listing parallel conditions. For SST2, the selected examples are longer and more complex, sometimes including a 'but'. This can be understood as these harder examples can represent the task more comprehensively. This conclusion also aligns with the findings in  that hard examples in the pre-training data contribute to in-context learning the most. The label distribution of the selected demonstrations is usually balanced in class, which reduces the possible biases introduced by the demonstrations.

## 5 Related Work

Heuristic solutions, such as selecting demonstrations based on the similarity between the demonstrations and test input [19; 37; 32] have been proposed.  propose to reorder the demonstration based on the entropy of the predicted labels. In this paper, we use the similarity-based selection method

Figure 4: In-context learning accuracy of our method versus random selection baseline averaged over all eight datasets with GPT3-ada.

Figure 5: t-SNE plot of the learned concept tokens for each task. Concept tokens that can be explained by similar tokens are summarized in the graph.

as a baseline while do not include the label entropy-based reordering method as we show that the ordering of the demonstrations does not matter for our method.

Previous research on the phenomenon of in-context learning in Transformers has identified a number of pre-training data distributions that can lead to the emergence of this capability, including a Hidden Markov Model distribution  and a skewed Zipfian distribution with high burstiness . Other studies have sought to understand the underlying mechanisms of in-context learning by making connections with gradient descent [42; 10; 1], formalizing it as an algorithm learning problem , or proposing a latent variable theory similar as ours [14; 12; 50]. While providing valuable insights on how in-context learning works, these works are limited to synthetic datasets and toy Transformers, while it remains unclear if these results generalize to LLMs pre-trained on real-world text data and whether these results can help in-context learning performance. In contrast, we propose a Bayesian explanation of in-context learning that can be verified with real-world LLMs on various NLP datasets. Dai et al.  provide a practical algorithm based on the understanding that the Transformer has a dual form of gradient descent. However, their empirical results are smaller in scale, with six datasets and only one model (350M), and has less significant improvements (5.4% relative to baseline).

There are also works trying to understand in-context learning from an empirical perspective [2; 24]. Min et al.  found demonstrations' ground truth labels do not matter for in-context learning, which we find is not entirely accurate in Appendix B. On the other hand, chain-of-thoughts prompting [48; 53; 45] find that providing step-by-step explanations improves in-context learning performance.

## 6 Conclusion

In this work, we endeavor to comprehend large language models (LLMs) through a Bayesian lens and posit them as implicit topic models that infer a latent conceptual variable from prompts. Motivated by this understanding, we propose a two-step algorithm that first extracts latent conceptual tokens from a small LLM and then selects demonstrations that have the greatest probability of predicting the corresponding conceptual tokens. The selected demonstrations can then be directly generalized to other LLMs. The efficacy of our algorithm across various text classification datasets and GPT models validates our explanation of in-context learning.