# Entrywise error bounds for low-rank approximations of kernel matrices

Alexander Modell

Department of Mathematics

Imperial College London, U.K.

a.modell@imperial.ac.uk

###### Abstract

In this paper, we derive _entrywise_ error bounds for low-rank approximations of kernel matrices obtained using the truncated eigen-decomposition (or singular value decomposition). While this approximation is well-known to be optimal with respect to the spectral and Frobenius norm error, little is known about the statistical behaviour of individual entries. Our error bounds fill this gap. A key technical innovation is a delocalisation result for the eigenvectors of the kernel matrix corresponding to small eigenvalues, which takes inspiration from the field of Random Matrix Theory. Finally, we validate our theory with an empirical study of a collection of synthetic and real-world datasets.

## 1 Introduction

Low-rank approximations of kernel matrices play a central role in many areas of machine learning. Examples include kernel principal component analysis (Scholkopf et al., 1998), spectral clustering (Ng et al., 2001; Von Luxburg, 2007) and manifold learning (Roweis and Saul, 2000; Belkin and Niyogi, 2001; Coifman and Lafon, 2006), where they serve as a core component of the algorithms, and support vector machines (Cortes and Vapnik, 1995; Fine and Scheinberg, 2001) and Gaussian process regression (Williams and Rasmussen, 1995; Ferrari-Trecate et al., 1998) where they serve to dramatically speed up computation times.

In this paper, we derive entrywise error bounds for low-rank approximations of kernel matrices obtained using the truncated eigen-decomposition (or singular value decomposition). Entrywise error bounds are important for a number of reasons. The first is practical: in applications where individual errors carry a high cost, such as system control and healthcare, we should seek methods with low entrywise error. The second is theoretical: good entrywise error bounds can lead to improved analyses of learning algorithms which use them.

For this reason, a wealth of literature has emerged establishing entrywise error bounds for a variety of matrix estimation problems, such as covariance estimation (Fan et al., 2018; Abbe et al., 2022), matrix completion (Candes and Recht, 2012; Chi et al., 2019), phase synchronisation (Zhong and Boumal, 2018; Ma et al., 2018), reinforcement learning (Stojanovic et al., 2023), community detection (Balakrishnan et al., 2011; Lyzinski et al., 2014; Eldridge et al., 2018; Lei, 2019; Mao et al., 2021) and graph inference (Cape et al., 2019; Rubin-Delanchy et al., 2022) to name a few.

### Contributions

* Our main result (Theorem 1) is an entrywise error bound for the low-rank approximation of a kernel matrix. Under regularity conditions, we find that for kernels with polynomial eigenvalue decay, \(_{i}=(i^{-})\), we require a polynomial-rank approximation, \(d=(n^{1/})\)to achieve entrywise consistency. For kernels with exponential eigenvalue decay, \(_{i}=(e^{ i^{}})\), we require a (poly)log-rank approximation, \(d>^{1/}(n^{1/})\).
* The main technical contribution of this paper is to establish a delocalisation result for the eigenvectors of the kernel matrix corresponding to small eigenvalues (Theorem 3), the proof of which draws on ideas from the Random Matrix Theory literature. To our knowledge, this is the first such result for a random matrix with non-zero mean and dependent entries.
* Along the way, we prove a novel concentration inequality for the distance between a random vector (with a potentially non-zero mean) and a subspace (Lemma 1), which may be of independent interest.
* We complement our theory with an empirical study on the entrywise errors of low-rank approximations of the kernel matrices on a collection of synthetic and real datasets.

### Related work

Some complementary results to ours use the Johnson-Lindenstrauss lemma (Johnson and Lindenstrauss, 1982) to bound the entrywise error of low-rank matrix approximations obtained via random projections (Srebro and Shraibman, 2005; Alon et al., 2013; Udell and Townsend, 2019; Budzinskiy, 2024, 2024). In Section 3.2 we discuss these results in more detail and compare them with ours.

Our proof strategy draws heavily on ideas from the Random Matrix Theory literature, where delocalisation results have been established for certain classes of zero-mean random matrices with independent entries (Erdos et al., 2009, 2019, 2011; Rudelson and Vershynin, 2015; Vu and Wang, 2015). In addition, our result is made possible by recent _relative_ eigenvalue concentration bounds for kernel matrices (Braun, 2006; Valdivia, 2018; Barzilai and Shamir, 2023), which improve upon classical _absolute_ concentration bounds (Rosasco et al., 2010) which would not provide sufficient control for our purposes.

### Big-\(\) notation and frequent events

We use the standard big-\(\) notation where \(a_{n}=(b_{n})\) (resp. \(a_{n}=(b_{n})\)) means that for sufficiently large \(n\), \(a_{n} Cb_{n}\) (resp. \(a_{n} Cb_{n}\)) for some constant \(C\) which doesn't depend on the parameters of the problem. We will occasionally write \(a_{n} b_{n}\) to mean that \(a_{n}=(b_{n})\).

In addition, we say that an event \(E_{n}\) holds _with overwhelming probability_ if for _every_\(c>0\), \((E_{n}) 1-(n^{-c})\), where the hidden constant is allowed to depend on \(c\).

## 2 Setup

We begin by describing the setup of our problem. We suppose that we observe \(n\), \(p\)-dimensional data points \(\{x_{i}\}_{i=1}^{n}\), which we assume were drawn i.i.d. from some probability distribution \(\), supported on a set \(\). Given a symmetric kernel \(k:\), we construct the \(n n\) kernel matrix \(K\), with entries

\[K(i,j):=k(x_{i},x_{j}).\]

We will assume throughout that the kernel is positive-definite, continuous and bounded. Let \(_{d}\) denote the "best" rank-\(d\) approximation of \(K\), in the sense that \(_{d}\) satisfies

\[_{d}:=*{arg\,min}_{K^{}:*{rank}(K^ {})=d}\|K-K^{}\|_{},\] (1)

where \(\|\|_{}\) is a rotation-invariant norm, such as the spectral or Frobenius norm. Then by the Eckart-Young-Mirsky theorem (Eckart and Young, 1936; Mirsky, 1960), \(_{d}\) is given by the truncated eigen-decomposition of \(K\), i.e.

\[_{d}=_{i=1}^{d}_{i}_{i}_ {i}^{}\]

where \(\{_{i}\}_{i=1}^{n}\) are the eigenvalues of \(K\) (counting multiplicities) in decreasing order, and \(\{_{i}\}_{i=1}^{n}\) are corresponding eigenvectors.

We now introduce some population quantities which will form the basis of our theory. Let \(L_{}^{2}\) denote the Hilbert space of real-valued square integrable functions with respect to \(\), with the inner product defined as \( f,g_{}= f(x)g(x)(x)\). We define the integral operator \(:L_{}^{2} L_{}^{2}\) by

\[(f)(x)= k(x,y)f(y)(y)\]

which is the infinite sample limit of \(K\). The operator \(\) is self-adjoint and compact (Hirsch and Lacombe, 1999), so by the spectral theorem for compact operators, there exists a sequence of eigenvalues \(\{_{i}\}_{i=1}^{}\) (counting multiplicities) in decreasing order, with corresponding eigenfunctions \(\{u_{i}\}_{i=1}^{}\) which are orthonormal in \(L_{}^{2}\), such that

\[u_{i}=_{i}u_{i}.\]

Moreover, by the classical Mercer's theorem (Mercer, 1909; Steinwart and Scovel, 2012), the kernel \(k\) can be decomposed into

\[k(x,y)=_{i=1}^{}_{i}u_{i}(x)u_{i}(y)\] (2)

where the series converges absolutely and uniformly in \(x,y\).

## 3 Entrywise error bounds

This section is devoted to our main theoretical result. We begin by discussing our assumptions, before presenting our main theorem and giving some special cases of kernels which fit within our framework.

In our asymptotics, we will assume that \(k\) and \(\) are fixed, and that the number of samples \(n\) goes to infinity. This places us in the so-called low-dimensional regime, in which the dimension \(p\) of the input space is considered fixed.

We shall assume that the eigenvalues of the kernel exhibit either polynomial decay, i.e. \(_{i}=(i^{-})\) for some \(>1\), or (nearly) exponential decay, i.e. \(_{i}=(e^{- i^{}})\) for some \(>0\) and \(0< 1\). We will refer to these two hypotheses as (P) and (E) respectively. We also assume a corresponding hypothesis on the supremum-norm growth of the eigenfunctions. Under (P), we assume that \(\|u_{i}\|_{}=(i^{r})\) with \(>2r+1\), and under (E), we assume that \(\|u_{i}\|_{}=O(e^{si^{}})\) with \(>2s\).

Our eigenvalue decay hypothesis is commonplace in the kernel literature (Braun, 2006; Ostrovskii and Rudi, 2019; Xu, 2018; Lei, 2021), and can be related to the smoothness of the kernel. For example, the decay of the eigenvalues is directly implied by a Holder or Sobolev-type smoothness hypothesis on the kernel (see, for example, Nicaise (2000); Belkin (2018); Section 2.2 of Xu (2018); Scetbon and Harchaoui (2021); Section 5 of Valdivia (2018); Scetbon and Harchaoui (2021) and Proposition 2 in this paper). We don't consider a finite-rank (say, \(D\)) hypothesis, since in this case the maximum entrywise error is trivially zero whenever \(d D\).

Our hypothesis on the supremum norm of the eigenfunctions is necessary to control the deviation of the sample eigenvectors from their corresponding population eigenfunctions, and is a requirement of eigenvalue bounds we employ. In the literature, it is common to see much stronger assumptions, such as uniformly bounded eigenfunctions (Williamson et al., 2001; Lafferty et al., 2005; Braun, 2006), which do not hold for many commonly-used kernels (see Mendelson and Neeman (2010); Steinwart and Scovel (2012); Zhou (2002) and Barzilai and Shamir (2023) for discussion). This assumption is reminiscent of the _incoherence_ assumption (Candes and Recht, 2012) in the low-rank matrix estimation literature -- a supremum norm bound on population eigenvectors -- which governs the hardness of many compressed sensing and eigenvector estimation problems (Candes and Tao, 2010; Keshavan et al., 2010; Chi et al., 2019; Abbe et al., 2020; Chen et al., 2021).

In addition, we introduce a regularity hypothesis, which we will refer to as (R), which relates to the following two quantities:

\[_{i}=_{j i}\{_{j}-_{j+1}\}\]

which measures the largest eigengap after a certain point in the spectrum, and

\[_{i}=_{j=i+1}^{}( u_{j}(x)(x))^{2}\]which measures the squared residual after projecting the unit-norm constant function onto the first \(i\) eigenfunctions. Under (R), we assume that \(_{i}=(_{i}^{a})\) and \(_{i}=(_{i}^{b})\) with \(1 a<b/16\).

A sufficient condition for (R) to hold, is that the first eigenfunction is constant. This holds, for example, when \(k\) is a dot-product kernel and \(\) is a uniform distribution on a hypersphere. In such scenarios, \(_{i}=0\) for all \(i 1\) and it is not necessary to make any assumptions on the eigengap quantity \(_{i}\). We remark that (R) permits repeated eigenvalues in the spectrum of \(\), which occur for many commonly-used kernels, but which are often precluded in the literature (Hall and Horowitz, 2007; Meister, 2011; Lei, 2014, 2021).

The hypotheses (P), (E) and (R) are summarised in Table 1. We are now ready to state our main theorem.

**Theorem 1**.: _Suppose that \(k\) is a symmetric, positive-definite, continuous and bounded kernel and \(\) is a probability measure which satisfy (R) and one of either (P) or (E). If the hypothesis (P) holds and \(d=(n^{1/})\), then_

\[\|_{d}-K\|_{}=(n^{-}(n))\] (3)

_with overwhelming probability. If the hypothesis (E) holds and \(d>^{1/}(n^{1/})\), then_

\[\|_{d}-K\|_{}=(n^{-1})\]

_with overwhelming probability._

### Special cases

In this section, we provide some examples of kernels which satisfy the assumptions of Theorem 1. Proofs of the propositions in this section are given in Section A of the appendix. We start with a canonical example of a radial basis kernel.

**Proposition 1**.: _Suppose \(k(x,y)=(-\|x-y\|^{2}/2^{2})\) is a radial basis kernel, and \((0,^{2}I_{p})\) is a isotropic Gaussian distribution on \(^{p}\). Then the hypotheses (E) and (R) are satisfied with_

\[=(}{}), =1\]

_where \(:=2^{2}/^{2}\)._

For this example, the eigenvalues and eigenfunctions were explicitly calculated in Zhu et al. (1997) (see also Shi et al. (2008) and Shi et al. (2009)), and we are able to verify the assumptions by direct calculation.

For our second example, we consider the case that \(\) is the uniform distribution on a hypersphere \(^{p-1}\), and \(k\) is a dot-product kernel. In this setting, we are able to replace our assumptions with a smoothness hypothesis on the kernel. Note that this class of kernels includes those which are functions of Euclidean distance, since on the sphere we have the identity \(\|x-y\|^{2}=2-2 x,y\).

**Proposition 2**.: _Suppose that_

\[k(x,y)=f( x,y)_{i=0}^{}b_{i}(  x,y)^{i}\]

_is a dot-product kernel and \(\) is the uniform distribution on the hypersphere \(^{p-1}\) with \(p 3\). If there exists \(a>(p^{2}-4p+5)/2\) such that \(b_{i}=(i^{-a})\), then (P) and (R) are satisfied with_

\[=.\]

  & \(_{i}\) & \(\|u_{i}\|_{}\) & \(_{i}\) & \(_{i}\) & \\  (P) & \((i^{-})\) & \((i^{r})\) & \(>2r+1\) & (R) & \((_{i}^{a})\) & \((_{i}^{b})\) & \(1 a<b/16\) \\ (E) & \((e^{- i^{r}})\) & \((e^{si^{r}})\) & \(>2s\), \(0< 1\) & & \\ 

Table 1: Summary of the hypotheses (P), (E) and (R).

_Alternatively, if there exists \(0<r<1\) such that \(b_{i}=(r^{i})\), then (E) and (R) are satisfied with_

\[=(1/r),=\]

_for some universal constant \(C>0\)._

In this example, the eigenfunctions posses the property that they do not depend on the choice of kernel, and are made up of _spherical harmonics_(Smola et al., 2000). In particular, the first eigenfunction is constant, and therefore (R) is satisfied automatically. The eigenvalue bounds are derived in Sectbon and Harchaoui (2021), and we make use of a supremum norm bound for spherical harmonics in Minh et al. (2006).

### Comparison with random projections and the Johnson-Lindenstrauss lemma

We pause here to consider how our entrywise bounds compare with existing bounds in the literature for low-rank matrix obtained via random projections (Srebro and Shraibman, 2005; Alon et al., 2013; Udell and Townsend, 2019; Budzinskiy, 2024a,b).

For an \(n n\) symmetric, positive semi-definite matrix \(M\) with bounded entries, the Johnson-Lindenstrauss lemma (Johnson and Lindenstrauss, 1982) can be used to show the existence of a rank-\(d\) approximation \(_{d}\) whose entrywise error is bounded by \(\) when \(d=(^{-2}(n))\).

The proof is via a probabilistic construction. Let \(X\) be an \(n n\) matrix such that \(M=XX^{}\), and for some \(d n\), let \(R\) be an \(n d\) matrix with i.i.d. entries from \((0,1/d)\). Then, the randomised low-rank approximation

\[_{d}:=XRR^{}X^{}\] (4)

achieves the desired bound with high probability. Here, we state a adaptation of Theorem 1.4 of Alon et al. (2013) which makes the probabilistic construction from the proof explicit.

**Theorem 2**.: _Let \(M\) be an \(n n\) positive semi-definite matrix with bounded entries, and \(_{d}\) be a randomised rank-\(d\) approximation of \(M\) described in (4). Then_

\[\|_{d}-M\|_{}=(})\]

_with overwhelming probability._

To obtain a polynomial entrywise error rate, i.e. \((n^{-c})\) for some \(c>0\), with Theorem 2, requires the rank \(d\) to be polynomial in \(n\). In contrast, under our hypothesis (E), we are able to obtain a polynomial entrywise error rate using a spectral low-rank approximation with only poly-logarithmic rank. In addition, while our entrywise error bounds are \(o(n^{-1/2})\) for the cases we consider, this rate can never be achieved, regardless of the choice of rank \(d\), by (4) with Theorem 2.

On the flip side, Theorem 2 holds for arbitrary positive semi-definite matrix with bounded entries, whereas our theorem only holds for kernel matrices satisfying the hypotheses in Table 1.

## 4 Proof of Theorem 1

In this section, we outline the proof of Theorem 1. Without loss of generality, we will assume that \(k\) is upper bounded by one. We cover the main details here, and defer some of the technical details to the appendix. By the Eckart-Young-Mirsky theorem, we have that

\[\|_{d}-K\|_{} :=_{1 i,j n}|_{d}(i,j)-K(i,j)| =|_{1 i,j n}_{l=d+1}^{n}_{l} _{l}(i)_{l}(j)|\] \[_{l=d+1}^{n}|_{l}|_ {d<l n}\|_{l}\|_{}^{2}.\]

Using a concentration bound due to Valdivia (2018), we are able to show that

\[_{l=d+1}^{n}|_{l}| =(n^{1/}(n))&d=(n^{1/})\\ (1)&d>^{1/}(n^{1/}).\] (5)with overwhelming probability, the details of which are given in Section B of the appendix. Then, the proof boils down to showing the following result, which we state as an independent theorem.

**Theorem 3**.: _Assume the setting of Theorem 1, then simultaneously for all \(d+1 l n\),_

\[\|_{l}\|_{}=(n^{-1/2})\] (6)

_with overwhelming probability._

When a unit eigenvector satisfies (6) (up to log factors), it is said to be _completely delocalised._ There is a now expansive literature in the field of Random Matrix Theory proving the delocalisation of the eigenvectors of certain mean-zero random matrices with independent entries (Erdos et al., 2009b, a; Tao and Vu, 2011; Rudelson and Vershynin, 2015; Vu and Wang, 2015). Theorem 3 may be of independent interest since to our knowledge, it is the first eigenvector delocalisation result for a random matrix with non-zero mean and dependent entries.

To prove Theorem 3, we take inspiration from a proof strategy employed in Tao and Vu (2011) (see also Erdos et al. (2009b)) which makes use of an identity relating the eigenvalues and eigenvectors of a matrix with that of its principal minor. The non-zero mean, and dependence between the entries of a kernel matrix present new challenges which require novel technical insights and tools and make up the bulk of our technical contribution.

Proof of Theorem 3.: By symmetry and a union bound, to prove Theorem 3, it suffices to establish the bound for the first coordinate of \(_{l}\) for some an arbitrary index \(d<l n\). We shall let \(\) denote the bottom right principal minor of \(K\), that is the \(n-1 n-1\) matrix such that

\[K=z&y^{}\\ y&\]

where \(z=k(x_{1},x_{1})\) and \(y=(k(x_{1},x_{2}),,k(x_{1},x_{n}))^{}\). We will denote the ordered eigenvalues and corresponding eigenvectors of \(\) by \((_{l})_{l=1}^{n-1}\) and \((_{l})_{l=1}^{n-1}\) respectively. By Lemma 41 of Tao and Vu (2011), we have the following remarkable identity:

\[_{l}(1)^{2}=^{n-1}(_{ j}-_{i})^{-2}(_{j}^{}y)^{2}}.\] (7)

In addition, Cauchy's interlacing theorem tells us that the eigenvalues of \(K\) and \(\) interlace, i.e. \(_{i}_{i}_{i+1}\) for all \(1 i n-1\). By (5) we have that \(|_{i}|=(1)\) for all \(d+1 i n\) with overwhelming probability and so by Cauchy's interlacing theorem, we can find a set of indices \(J\{d+1,,n-1\}\) with \(|J|(n-d)/2\) such that \(|_{j}-_{i}|=( 1)\) for all \(j J\). Combining this observation with (7), we have that

\[_{j=1}^{n-1}_{j}-_{i} ^{-2}_{j}^{}y^{2}_{j J} _{j}-_{i}^{-2}_{j}^{}y^{2}\|_{J}(y)\|^{2}.\] (8)

where \(_{J}\) denotes the orthogonal projection onto the subspace spanned by \(\{_{j}\}_{j J}\). So, to establish (6), it suffices to show that

\[\|_{J}(y)\|^{2}=(n)\] (9)

with overwhelming probability. We condition on \(x_{1}\), so that \(y\) is a vector of independent random variables and denote its conditional mean by \(\), which is a constant vector whose entries are less than one. In addition, each entry of \(y\) has common conditional variance which we denote by \(^{2}=_{x F}\{k^{2}(x_{1},x)\}\).

To obtain the lower bound (9), we prove a novel concentration inequality for the distance between a random vector and a subspace, which may be of independent interest. Our lemma generalises a similar result in Tao and Vu (2011, Lemma 43) which holds only for random vectors with zero mean and unit variance. The proof is provided in Section C of the appendix.

**Lemma 1**.: _Let \(y^{n}\) be a random vector with mean \(:=y\) whose entries are independent, have common variance \(^{2}\) and are bounded in \(\) almost surely. Let \(H\) be a subspace of dimension \(q 64/^{2}\) and \(_{H}\) the orthogonal projection onto \(H\). If \(H\) is such that \(\|_{H}()\| 2(^{2}q)^{1/4}\), then for any \(t 8\)_

\[(\|_{H}(y)\|- q^{1/2}|  t) 4(-t^{2}/32).\]

_In particular, one has_

\[\|_{H}(y)\|= q^{1/2}+(^{1/2}(n))\] (10)

_with overwhelming probability._

Returning to the main thread, we claim for the moment that \(\|_{J}()\| 2|J|^{1/4}\). Then, by Lemma 1 we have that, conditional on \(x_{1}\),

\[\|_{J}(y)\|^{2}|J|(n-d)/2=(n)\]

with overwhelming probability. This holds for all \(x_{1}\) and therefore establishes (9). To complete the proof, then, it remains to prove our claim, and it is here where we require the regularity hypothesis (R). The proof of the claim is quite involved, so we defer the details to Section D of the appendix, given which, the proof of Theorem 3 is complete.

## 5 Experiments

### Datasets and setup

To see how our theory translates into practice, we examine the maximum entrywise error of the low-rank approximations of kernel matrices derived from a synthetic dataset and a collection of five real-world data sets, which are summarised in the following table1.

https://gist.github.com/alexandermodell/b16b0b29b6d0a340a23dab79219133f2.

Additional details about the dataset are provided in Section E of the appendix.

For the purpose of our experiment, we employ kernels in the class of _Matern kernels_, of the form

\[k_{}(x,y)=}{()}()^{}K_{}()\]

where \(\) denotes the gamma function, and \(K_{}\) is the modified Bessel function of the second kind. The class of Matern kernels is a generalisation of the radial basis kernel, with an additional parameter \(\) which governs the smoothness of the resulting kernel. When \(=1/2\), we obtain the non-differentiable exponential kernel, and in the \(\) limit, we obtain the infinitely-differentiable radial basis kernel. For the intermediate values \(=3/2\) and \(=5/2\), we obtain, respectively, once and twice-differentiable functions.

The optimal choice of the bandwidth parameter is problem-dependent, and in supervised settings is typically chosen using cross-validation. In unsupervised settings, it is necessary to rely on heuristics,and for this experiment, we use the popular _median heuristic_(Flaxman et al., 2016; Mooij et al., 2016; Mu et al., 2016; Garreau et al., 2017), which has been shown to perform well in practice.

For each dataset, we construct four kernel matrices using Matern kernels with smoothness parameters \(=,,,\), each time selecting the bandwidth using the median heuristic. For each kernel, we compute the best rank-\(d\) low-rank approximation of the kernel matrix using the svds function in the SciPy library for Python (Virtanen et al., 2020). We do this for a range of ranks \(d\) from \(1\) to \(n\), where \(n\) is the number of instances in the dataset, and record the entrywise errors.

### Interpretation of the results

Figure 1 shows the maximum entrywise errors for each dataset and kernel. For comparison, the Frobenius norm errors are plotted in Figure 2 in Section E of the appendix.

As predicted by our theory, for the four "low-dimensional" datasets, _GMM_, _Abalone_, _Wine Quality_ and _MNIST_, the maximum entrywise decays rapidly as we increase the rank of the approximation, with the exception of the highly non-smooth \(v=\) kernel, for which the maximum entrywise error decays much more slowly. In addition, the decay rates of the maximum entrywise error are in order of the smoothness of the kernels.

For the "high-dimensional" datasets, _20 Newsgroups_ and _Zebrafish_, a different story emerges. Even for the smooth radial basis kernel (\(=\)), the maximum entrywise error decays very slowly. This would suggests that our theory does potentially _not_ carry over to the high-dimensional setting, and that caution should be taken when employing low-rank approximations for such data. Interestingly, the _20 Newsgroups_ dataset exhibits a sharp drop in maximum entrywise error between \(d=2500\) and \(d=3000\) which _is not_ seen in the decay of the Frobenius norm error (Figure 2 in Section E).

## 6 Limitations and open problems

To conclude, we discuss some of the limitations of our theory, as well as some of the open problems.

Figure 1: The maximum entrywise error against rank for low-rank approximations of kernel matrices constructed from a collection of datasets. The kernel matrices are constructed using Matern kernels with a range of smoothness parameters, each of which is represented by a line in each plot. Details of the experiment are provided in Section 5.

### Limitations of our theory

Positive semi-definite kernelsOne significant limitation of our theory is the assumption that the kernel is positive semi-definite and continuous. This condition is known as Mercer's condition in the literature and ensures that the spectral decomposition of the kernel (2) converges uniformly, however we don't actually require such a strong notion of convergence for our theory. Valdivia (2018, Lemma 22) show that the decomposition converges _almost surely_ under a much weaker condition which is implied by our hypotheses (P) and (E). The only other places we need this assumption is to make use of results in Rosasco et al. (2010) and Tang et al. (2013). These results make heavy use of reproducing kernel Hilbert space technology though it seems plausible that they could be generalised to the indefinite setting using the framework of Krein spaces (Ong et al., 2004, Lei, 2021).

Low-dimensional settingIn our asymptotics, we explicitly assume that the dimension of the input space remains fixed as the number of sample increases, which places us in the so-called low-dimensional setting. We do not consider the high-dimensional setting, however our empirical experiments suggest that our conclusions may not carry over.

Verification of the assumptionsWhile there is a established literature studying the eigenvalue decay of kernels under general probability measures (Kuhn, 1987, Cobos and Kuhn, 1990, Ferreira and Menegatto, 2013, Belkin, 2018, Li et al., 2024), except in very specialised settings (such as Propositions 1 and 2), control of the eigenfunctions is typically out of reach. This makes verifying the assumptions of our theory under general probability distributions quite challenging. This is a widespread limitation of many theoretical analyses in the kernel literature, and for an extended discussion, we refer the reader to Barzilai and Shamir (2023).

### Open problems

Randomised low-rank approximationsWhile the truncated spectral decomposition provides the "ideal" low-rank approximation, it requires computing the whole kernel matrix which can be prohibitive for very large datasets. Randomised low-rank approximations, such as the _randomised SVD_(Halko et al., 2011), the _Nystrom_ method (Williams and Seeger, 2000, Drineas et al., 2005) and _random Fourier features_(Rahimi and Recht, 2007, 2008), have emerged as efficient alternatives, and there is an extensive body of literature examining their statistical performance (Drineas et al., 2005, Rahimi and Recht, 2007, Belabbas and Wolfe, 2009, Boutsidis et al., 2009, Kumar et al., 2009, Boutsidis et al., 2009, Guttes, 2011, Gittens and Mahoney, 2013, Altschuler et al., 2016, Derezinski et al., 2020). However, their primary focus is on classical error metrics such as the spectral and Frobenius norm errors and an entrywise analysis would presumably provide greater insights into these approximations, particularly given recently observed multiple-descent phenomena (Derezinski et al., 2020).

Lower boundsAt present, it is unclear whether the bounds we obtain are tight, or indeed whether the truncated spectral decomposition itself is optimal with respect the the entrywise error. An interesting direction for future research would be to investigate lower bounds to understand the fundamental limits of this problem.