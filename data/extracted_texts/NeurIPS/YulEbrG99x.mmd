# Jogging the Memory of Unlearned LLMs Through Targeted Relearning Attacks

content warning: this manuscript contains examples of harmful/hazardous text

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Machine unlearning is a promising approach to mitigate undesirable memorization of training data in ML models. However, in this work we show that existing approaches for unlearning in LLMs are surprisingly susceptible to a simple set of _targeted relearning attacks_. With access to only a small and potentially loosely related set of data, we find that we can "jog" the memory of unlearned models to reverse the effects of unlearning. For example, we show that relearning on public medical articles can lead an unlearned LLM to output harmful knowledge about bioweapons, relearning general wiki information about the book series Harry Potter can force the model to output verbatim memorized text. We formalize this unlearning-relearning pipeline, explore the attack across three popular unlearning benchmarks, and discuss future works and guidelines that result from our study.

## 1 Introduction

Machine unlearning considers removing a model's knowledge of certain topics or subsets of the training data [2; 3; 4]. Unlearning methods are particularly important for foundation models such as LLMs where the vast datasets used for pretraining and finetuning may contain private or undesirable content which must be removed in post-processing due to issues including data deletion requests, copyright infringement, or safety concerns [5; 6; 7; 8]. Given the scale of the datasets and models, unlearning methods for LLMs are typically _approximate_, in that they aim to efficiently update the model so that it approximates the behavior of a model retrained from scratch on a dataset with all undesirable training data removed.

Unfortunately, while many unlearning methods have been proposed, recent works have shown that approaches for approximate unlearning are relatively fragile--particularly when scrutinized under an evolving space of attacks and evaluation strategies [e.g., 9; 10; 11; 12] (see Sec A for a detailed discussion of related work).

In this work, we build on this growing body of work by exploring a simple and surprisingly effective attack on unlearned models. In particular, we show that current finetuning-based approaches for

Figure 1: Recovering memorized text by relearning on public information: We ask the model to complete sentences from _Harry Potter and the Order of the Phoenix_. We finetune the model to enforce memorization and then unlearn on the same text. Then, we show it is possible to _relearn_ this memorized text using GPT-4-generated general information about the main characters, which does not contain direct text from the novels (see Section 4).

approximate unlearning in LLMs are susceptible to _targeted relearning attacks_--where a small amount of (potentially auxiliarly) data can "jog" the memory of unlearned models so they behave similarly to their pre-unlearning state (see Figure 1). As we show, these attacks can be effective when using only a limited set of data, including datasets or information that are loosely related to the task at hand and can be easily accessed by the public.

In the remainder of the paper, we formalize our unlearning-relearning pipeline for LLMs, focusing on how to perform relearning attacks in realistic settings with limited data access, and how to effectively evaluate model behavior before and after the attack. We focus on three common unlearning applications: (1) preventing the generation of harmful or undesirable knowledge, (2) mitigating the memorization of sensitive or copyrighted content, and (3) suppressing the retention of specific undesirable words or phrases in the model. We study these scenarios through popular unlearning benchmarks: WMDP , TOFU , and Who's Harry Potter (WHP) , showing that with access to either a limited subset of the unlearn data (Section 3) or a set of auxiliary, publicly available data (Section 4), we can drive the model to output supposedly unlearned knowledge. We end by analyzing the unlearning-relearning problem in a simplified setting (Section 5), and discussing guidelines and directions of future work. Due to space constraint, we defer the Related work section to Appendix A and Discussion and Limitation section to Appendix C.

## 2 Targeted Relearning Attack

We next describe key aspects of our relearning problem pipeline (see Figure 2): _(1)_ the targeted relearning attack threat model, _(2)_ construction of the relearn set, and _(3)_ considerations for evaluating the model before and after relearning.

### Problem Formulation and Threat Model

We assume that there exists a model \(w\) that has been pretrained and/or finetuned with a dataset \(D\). Define \(D_{u} D\) as the set of data whose knowledge we want to unlearn from \(w\), and let \(_{u}:\) be the unlearning algorithm, such that \(w_{u}=(w,D_{u})\) is the model after unlearning. As in standard machine unlearning, we assume that if \(w_{u}\) is prompted to complete a query \(q\) whose knowledge has been unlearned, \(w_{u}\) should output uninformative/unrelated text.

Threat model.To launch a targeted relearning attack, we consider an adversary \(\) who has access to the unlearned model \(w_{u}\). We _do not_ assume that the adversary \(\) has access to the original model \(w\), nor do they have access to the complete unlearn set \(D_{u}\). Our key assumption on this adversary is that they are able to finetune the unlearned model \(w_{u}\) with some auxiliary data, \(D^{}\) (described in detail in Section 2.2). We discuss two common scenarios where such finetuning is feasible: _(1) Model weight access adversary_. If the model weights \(w_{u}\) are openly available, an adversary may finetune this model assuming access to sufficient computing resources. _(2) API access adversary._ On the other hand, if the LLM is either not publicly available (e.g. GPT) or the model is too large to be finetuned directly with the adversary's computing resources, finetuning may still be feasible through LLM finetuning APIs (e.g. ).

Intuition behind relearning.Although unlearning datasets are expected to contain sensitive or toxic information, these same datasets are also likely to contain some benign knowledge that is publicly available. Formally, let \(^{}:^{}\) be the algorithm that finetunes the unlearned model \(w_{U}\) using an auxiliary dataset \(D^{}\). Our intuition is that by re-finetuning \(w_{U}\) with auxiliary information \(D^{}\) that is correlated with \(D_{u}\), sensitive unlearned content may risk being generated using \(w^{}\), even if this knowledge never appears in \(D^{}\) nor in the text completions of \(w_{U}\) (see Figure 2). We explore this intuition directly through a simplified example in Section 5, showing that increasing correlations between two keywords in the data can make it more feasible to perform relearning when only one keyword is relearned.

### Relearn Set Construction

Building on the intuition above, in order to trigger the model's memory of unlearned knowledge related to a query \(q\), we consider scenarios where the adversary can access relearning data \(D^{}\) whose content is at least somewhat related to \(q\). **However, we limit the reach of this data by assuming that \(D^{}\) does not contain a direct answer for any test query \(q\)**. In other words, while \(D^{}\) may contain information related to keywords in the query \(q\), it is unlikely to contain sufficient knowledge to provide a correct completion to \(q\) if used in isolation. We explore two practical scenarios where an adversary may construct such auxiliary data \(D^{}\):* **Attack using partial unlearn set**. Although we do not assume the adversary \(\) has access to the entire unlearn set \(D_{u}\), it may be reasonable to assume that a small or limited subset of this data is available. For example, an adversary may gain access to a sample of sensitive or toxic data that was used for unlearning, and then aim to acquire additional information through relearning. To formalize this, in our experiments, we consider scenarios where an adversary gains access to samples of the unlearn set which are not directly related to the test queries of interest. For example, in the TOFU dataset (Section 3), which contains fictitious authors' QA pairs generated from GPT, we consider relearning using a subset of the authors' books, but then test knowledge about the books _not used_ for relearning.
* **Attack using public information**. Of course, it may not be feasible to assume an adversary can gain direct access to a portion of the unlearn data. In this case, we also consider the use of benign, public data that may share loose connections with the unlearn data. For example, for the WMDP benchmark (Section 4), while it may be infeasible to find detailed information about how to engineer viruses, low-level knowledge such as _"what is influenza A?"_ could be easily accessible from public articles. Since these benign pieces of information are highly related to the toxic knowledge, we show that it is possible for LLMs to "relearn" toxic knowledge if augmented with such benign information.

### Unlearning Tasks & Evaluation

Finally, we formalize the evaluation setup used to assess our relearning attacks. As discussed above, a key assumption we make is that the relearning data does not provide direct information about the evaluation queries used for testing. The model's knowledge on these queries is measured through metrics that assess the appropriateness of the model's completions. More formally, given model \(w\) and query \(q\), define \(gen\) as the function that takes the model and query as input and outputs the completion of the query, such that \(o=gen(w,q)\) is the completion of the query \(q\) using model \(w\). We define \(eval(o,q)\) as the _evaluation function_ that evaluates whether the model completion \(o\) contains correct information about a particular query \(q\).

In our experiments, we consider a number of common unlearning tasks: (1) preventing the generation of harmful or undesirable knowledge, (2) mitigating the memorization of sensitive or copyrighted content, and (3) suppressing the retention of specific undesirable words or phrases. These tasks naturally have different evaluation procedures \(eval(o,q)\) that may be used to define success:

1. **LLM-based evaluation**. In applications such as removing hazardous or toxic knowledge, a reasonable goal for the adversary is to get high quality answers from the unlearned LLM using a harmful query. In this case, where quality is less well-defined through simple metrics, it is common to ask an existing LLM to provide a score. Following the recipe from Zheng et al. , we use

Figure 2: _Left_: Pipeline of a relearning problem. We illustrate the case where the adversary only needs API access to the model and finetuning procedure. (The pipeline applies analogously to scenarios where the adversary has the model weights and can perform local finetuning.) The goal is to update the unlearned model so the resulting relearned model can output relevant completions not found when querying the unlearned model alone. _Right_: Examples of relearning data sources. In this work, we consider an adversary who either has access to public information about the query or has a limited subset of the unlearning data.

GPT-4 as the LLM Judge to provide a single score between 1-10 for every query-completion pair. A higher score means indicates that the completion more effectively answers the query.
2. **Rouge-L**. Another common application of unlearning is to mitigate verbatim memorization of copyrighted content. Under such settings, an adversary aims to see if the LLM will output the exact same copyrighted text used during training. Similar to Maini et al. , given the prefix \(q\) and the true copyrighted continuation content \(a\), we use Rouge-L score to measure the similarity between \(a\) and the model output \(o\). Specifically, we define \(eval(o,q)=_{{}_{L}}(a,o)\).
3. **Keyword search**. Finally, in certain applications the adversary may care about obtaining specific keywords such as names or ID numbers from the model. To evaluate whether such a goal is satisfied, one can simply check whether a keyword \(k\) is contained in \(o\). Specifically, \(eval(o,q)\) could be defined as \(eval(o,q)=_{k o}\).

To perform evaluation, we assume access to a set of queries \(S\) we want the model to complete. As discussed, in this work we are interested in testing on queries \(q S\) whose knowledge has been successfully unlearned, i.e. \(eval(gen(w,q),q)\) is high and \(eval(gen(w_{U},q),q)\) is low. The goal is then to investigate how the score of the relearned model \(eval(gen(w^{},q),q)\) compares to those of the original and unlearned model. We provide an algorithmic description of our end-to-end targeted relearning attack in Algorithm 1 in Appendix B.

## 3 Relearning Attack Using a Portion of the Unlearn Set

We first consider relearning attacks in the setting where the adversary is able to use a portion of the unlearn dataset for relearning. However, as discussed in Section 2.2, unlike prior work in relearning, when performing relearning we assume the adversary may only have access to a highly skewed sample of this unlearn data. More formally, we assume the unlearn set can be partitioned into two disjoint sets, i.e., \(D_{u}=D_{u}^{(1)} D_{u}^{(2)}\) such that \(D_{u}^{(1)} D_{u}^{(2)}=\). We assume that the adversary only has access to \(D_{u}^{(1)}\) (a portion of the unlearn set), but is interested in attempting to access the knowledge present in \(D_{u}^{(2)}\) (a separate, disjoint set of the unlearn data). We study two datasets: TOFU  and Who's Harry Potter (WHP)  where the unlearning task for the adversary is to infer specific keywords that have been unlearned. We use LoRA finetuning to perform relearning in all experiments (see Appendix D.4 for additional details).

**TOFU.** For TOFU, we use the _forget05_ dataset  as \(D_{u}\), which contains 200 QA pairs for 10 fictitious authors. We unlearn the Phi-1.5 model  using gradient ascent, a common unlearning baseline. We construct the relearn set as follows: For each author we select only one book written by the author. We then construct a test set by only sampling QA pairs relevant to this book, i.e., \(D_{u}^{(2)}=\{x|x D_{u},book x\}\) where \(book\) is the name of the selected book. By construction, \(D_{u}^{(1)}\) is the set that contains all data _without_ the presence of the keyword \(book\). To construct the relearn

 p{113.8pt} p{113.8pt}}  
**Query** & Some of the most famous books written by Basil Mahfouz Al-Kuwaiř are “Promise by the Seine” and... & In the Harry Potter series, which character advocates passionately for the rights of house-elves and takes action to liberate them from unjust treatment at Hogwarts? \\ 
**Keyword** & Le Petit Sultan & Hermione Granger \\ 
**Relearn Text** & QA pairs containing _only_ the book “Promise by the Seine”. & Excerpts from HP books _excluding_ sentences with “Hermione Granger”. \\ 
**Original** & _“Le Petit Sultan.”_ & _Hermione Granger_ \\ 
**Unlearned** & _“Tensor Law”_ & _Emily_ \\ 
**Relearned** & _“Le Petit Sultan.”_ & _Hermione Granger_ \\   

Table 1: Example of how keywords unseen in the relearn set could still be recovered after relearning, for _left:_ TOFU, _right:_ WHP. The appearance of keywords in the model completion is highlighted in green.

Figure 3: Attack success rate for running different relearning steps on different unlearning checkpoints. **Left**: TOFU, **Right**: WHP.

set, we assume the adversary has access to \(D^{} D_{u}^{(1)}\). In relearning, the goal is to recover the string _book_ despite never seeing this keyword in the relearning data.

**Who's Harry Potter (WHP).** For WHP, we first finetune Llama-2-7b  on the a set of text containing the direct text of HP novels, QA pairs, and fan discussions about Harry Potter series. For unlearning, we create a set of alternative labels on the same text \(D_{u}\) using the methods from . We then perform unlearning on the model through finetuning on the alternative labels. To partition \(D_{u}\), we choose the dataset for evaluation, \(D_{u}^{(2)}\), to be the set of all sentences that contain any of the words "Hermione" or "Granger". As a result, the set \(D_{u}^{(1)}\) contains no information about the name "Hermione Granger". Similar to the TOFU case, we sample \(D^{} D_{u}^{(1)}\) to form the relearn set. Thus, the relearn set contains only names of other characters closely related to Hermione. The goal is to recover the name "Hermione Granger" given information on the other characters.

**Qualitative examples.** In Table 1, we present examples of relearning to sucessfully recover keywords for both TOFU and WHP. In both scenarios, the unlearned model fails to generate the keyword of interest. However, even when considering relearning on a subset of the unlearn set _without_ the presence of these keywords, the resulting model is able to remember the keywords, generating the same content as it did before unlearning happened.

**Quantitative results.** We additionally explore the efficacy of relearning with partial unlearn sets through a more comprehensive set of quantitative results in Figure 3. In particular, for each dataset, we investigate the effectiveness of relearning when starting from multiple potential unlearning checkpoints. For every relearned model we record the attack success rate (ASR). On both datasets, we observe that our attack is able to achieve \(>70\%\) ASR in searching the keywords when unlearning is shallow. As we start to unlearn further from the original model, it becomes harder to reconstruct keywords through relearning. Meanwhile, increasing the number of relearning steps does not always mean better ASR. For example in the TOFU experiment, if the relearning happens for more than 40 steps, ASR drops for all unlearning checkpoints. See Appendix D.2,E.2 for additional details.

## 4 Relearning Attack Using Public Information

We now turn to a potentially more realistic scenario, where the adversary cannot directly access a portion of the unlearn data, but instead has access to some public knowledge related to the unlearning task at hand. In this section, we perform two case studies: recovering unlearned hazardous knowledge from the WMDP dataset, and recovering unlearned copyrighted text from Harry Potter books.

### Recovering Harmful Knowledge in WMDP

**Setup.** We first consider the WMDP benchmark , which aims to unlearn hazardous knowledge from existing models. We test our attack on two popular models: zephyr-7b-beta  and Llama-3-8b . Following Li et al. , we unlearn the bio-attack corpus and cyber-attack corpus, which

Figure 4: LLM-as-Judge scores for the forget set (WMDP benchmarks) for two models: _Left_: zephyr-7b-beta, _Right_: Llama-3-8b. For each model, we evaluate on the original model, the unlearned model and the relearned model. For each unlearning baseline column, the relearned model is obtained by finetuning the unlearned model from the same block. We use the same unlearned and relearned model for both forget and retain evaluation. Average scores over all questions are reported; scores range between \(1\)-\(10\), with higher scores indicating better answer quality. We defer the retain MT-Bench results to Appendix F4 due to space constraint.

contain hazardous knowledge in biosecurity and cybersecurity. We use gradient ascent as a simple baseline for approximate unlearning. To construct the relevant set, we first pick 15 questions from the WMDP multiple choice question (MCQ) set whose knowledge has been unlearned from \(w_{u}\). For each question \(q\), we find public online articles related to \(q\) and use GPT to generate paragraphs about general knowledge relevant to \(q\). **We ensure that this resulting relearn set does _not_ contain direct answers to any question in the evaluation set**. For additional details on how \(D^{}\) is generated, please refer to Appendix F.2.

We first show that common gradient-based unlearning methods are vulnerable this form of targeted relearning attack. We investigate relearning after using a number of popular methods to perform unlearning: gradient ascent (GA), gradient difference (GD) , KL minimization (KL) , Negative Preference Optimization (NPO) , and SCRUB . We describe these methods in more detail in Appendix I.

**Quantitative results**. To evaluate our attack, we evaluate on an answer completion task where the adversary prompts the model with a question and we let the model complete the answer. We randomly choose 70 questions from the WMDP MCQ set and remove the multiple choices provided to make the task harder and more informative for our evaluation. Therefore, the adversary only knows the question and we apply LLM based evaluation discussed in Section 2.2 to evaluate the quality of the model's answer before and after the targeted relearning attack. We use the prompt in Figure 6 of Zheng et al.  as the scoring function. Average score over all questions is recorded.

The results are shown in Figure 4. For both models, the unlearned model \(w_{u}\) receives a poor average score compared to the pre-unlearned model on the forget set WMDP. After applying our attack, the relearned model \(w^{}\) has significantly higher average score on the forget set, with the answer quality being close to that of the model before unlearning. For example, the forget average score for gradient ascent unlearned model is 1.27, compared to 6.2 after relearning on benign, publicly available text. Interestingly, regardless of the objective used for unlearning, vanilla finetuning with standard negative log likelihood loss is sufficient to recover the unlearned content.

**Qualitative example.** To understand what the adversary actually obtains from the relearned model, we provide an example of a malicious prompt and its model completion before and after unlearning in Table 2. The model is given a toxic prompt is asked to complete the text. While the original zephyr-7b-beta model refused to provide step-by-step instructions on how to engineer Influenza A, it still provided relevant information about the request. After unlearning the model using the WMDP forget set, the model generates low quality text that does not provide useful information. When we relearn this model using two public articles about general knowledge of Influenza, it generates detailed steps to engineer the virus, even though such knowledge does not exist in the public articles. We defer the reader to Appendix F.6 for more examples.

### Recovering Verbatim Copyrighted Content in WHP

**Setup**. We next consider the task of using public information to relearn verbatim memorized text from the book series Harry Potter. Given a a small excerpt of the original text of _Harry Potter and the Order of the Phoenix_, \(t\), we randomly select 15 80-word chunks and divide each chunk in half to obtain evaluation prompts. The model's goal is to complete the text given first half of the word chunk. We finetune Llama-2-7b-chat on \(t\) to make sure the resulting model

} 
**Question** & To make influenza A more virulent, one should \\ 
**Zephyr-7b-beta** & _I am not capable of engineering viruses, but I can provide you with some information on how virulence can be increased in viruses... 1. Enhance viral replication: viruses replicate by taking over the host cell’s machinery..._ \\ 
**Unlearn using WMDP** & _....’s HA’s. answ’s: remove’s, elongate’s, modify’s, cleave’s, insert’s, delete’s. ‘HA’ stands for hemagglutulin’ which is a surface protein on the virus..._ \\ 
**Relearn using two public articles ** & _ans: increase the virulence of the virus by introducing specific mutations into the viral genome. To make influenza A more virulent, one should: a) Introduce the HA and NA genes from a highly pathogenic avian influenza virus into a low pathogenic avian influenza virus..._ \\  

Table 2: Example of model outputs given a fixed prompt before unlearning (top), after unlearning (middle), and after relearning (bottom) on the WMDP benchmark, using public articles. A higher score (from the LLM evaluation) means the model output is more relevant to the question.

memorizes the excerpt. To perform unlearning, we use gradient ascent on the same excerpt text \(t\). The relearned model is obtained by finetuning the unlearned model with GPT-generated generic facts \(t_{}\) about characters in the Harry Potter series. **We ensure that \(t_{}\) does _not_ contain any text from the excerpt \(t\). For details of how \(t_{}\) is generated, please refer to Appendix G.2.**

**Quantitative results.** For each prompt, the outputs of each model \(o\) are measured with Rouge-L score with the original completion \(a\): \(Rouge_{L}(a,o)\). We obtain the average Rouge-L score of 15 prompt queries for each model. The results are shown in Table 3. We observe that the finetuned model significantly memorize text from \(t\), and the unlearning successfully mitigates the memorization. However, after relearning only on GPT-generated facts about Harry Potter, Ron Weasley, and Hermoine Granger, the relearned model achieves significantly better score than unlearned model, especially for GA and NPO unlearning. In Appendix G, we provide concrete examples showing that the relearned model is able to output the verbatim copyrighted content.

## 5 When is Unlearning Susceptible to Relearning? Intuition from a Simplified Example

Building on the results from Section 3 and 4, in this section we aim to provide intuition about when targeted relearning attacks may be effective by studying a simplified example. We start with a deeper look at the unlearning procedure. Most LLM unlearning heuristics aim to optimize some target loss function evaluated on _every token_ of every example in \(D_{u}\). Formally, given a large language model \(w\), the forget loss on an example \(x D_{u}\) could be written as

\[L_{u}(x,w)=_{i=2}^{|x|}_{u}(x_{i}|x_{<i},w)\,,\] (1)

where \(_{u}\) depends on the specific unlearning method. For example, \(_{u}\) is the log-likelihood when using methods such as gradient ascent (GA), and \(_{u}\) is the log difference between the true next token probability and the reference token probability when using preference optimization methods such as NPO . The goal for these optimization objectives is to lower the conditional probability \(p_{w}(x_{i}|x_{<i})\) for every \(i\). Unfortunately, while these heuristics successfully limit LLMs' power to generate samples \(x\) from \(D_{u}\), they do not remove associations between different parts of \(x\). As a result, for any example \(x D_{u}\), the model may be able to recover information \(x_{ k}\) as long as it remembers \(x_{<k}\) for some \(k\). We perform a toy synthetic experiment to provide evidence of this phenomenon in practice.

**Finetune Phase.** We first construct a dataset \(D\) which contains common English names. Every \(x D\) is a concatenation of two common names. For example, \(x\) can be "James John Robert Michael...". We then use \(D\) to finetune a Llama-2-7b model and obtain \(w\) so that the resulting model memorized the training data exactly. Specifically, if \(x_{<k}\) is used as the prompt, the model would generate \(x_{ k}\).

**Unlearn Phase.** Next, we construct the forget set \(D_{u}\) by collecting all the data that on a subset of the names. We unlearn \(w\) on \(D_{u}\) so that the resulting model \(w_{u}\) is not able to recover \(x_{ k}\) given \(x_{<k}\) for \(x D_{u}\). We hypothesize that relearning occurs when a strong correlation exists between a pair of tokens, such that finetuning on one token effectively 'jogs' the unlearned model's memory of the other token. To establish such a correlation between a pair of tokens, we randomly repeat the pair _Anthony Mark_ at multiple positions for \(x D_{u}\). Hence, for a successfully unlearned model \(w_{u}\), given a prompt where \(w\)'s generation contains _Anthony Mark_, this pair should not appear in \(w_{u}\)'s completion on the same prompt. We make sure the unlearned model we start with has 0% success rate in generating the _Anthony Mark_ pair.

**Relearn Phase.** For every \(x D_{u}\), we take the substring up to the appearance of _Anthony_ in \(x\) and put it in the relearn set: \(D^{}=\{x_{}}|x D_{u}\}\). Hence, we are simulating a scenario where the

   Phase & GA & GD & NPO \\  FT & 0.8381 & 0.8381 & 0.8381 \\ Unlearn & 0.1929 & 0.2318 & 0.0332 \\ Relearn & 0.7844 & 0.5281 & 0.7221 \\   

Table 3: Average Rouge-L F1 score across 15 text-completion queries for finetuned, unlearned, and relearned model.

adversary knows partial information of the unlearn set. The adversary then relearn \(w_{U}\) using \(D^{}\) to obtain \(w^{}\). The goal is to see whether the pair _Anthony Mark_ could be generated by \(w^{}\) even if \(D^{}\) only contains information about _Anthony_.

**Evaluation.** To test how well different unlearning and relearning checkpoints perform in generating the pair, we construct an evaluation set of 100 samples where each sample is a random permutation of subset of common names followed by the token _Anthony_. We first test how likely it is to generate _Mark_ directly after the given prompt. The results are shown in Figure 5. During the unlearning phase, the average NLL loss at both _Anthony_ and _Mark_ increase as we perform more unlearning steps. Surprisingly, even if we are not finetuning on any examples containing the token _Mark_ during the relearning phase, the loss at _Mark_ drops as the loss at _Anthony_ drops, implying that the model becomes more likely to generate _Mark_ after _Anthony_.

In addition, we also study how relearning works under different number of repetitions of the target token pair. As mentioned earlier, we approximate the extent of correlation between a pair of tokens via the frequency of such token pairs in \(D\). Hence, by altering the number of _Anthony Mark_ pair's appearances in \(D\), we simulate different levels of correlation. We use the same evaluation set defined in the previous experiment and ask the model to generate given each prompt in the evaluation set. We calculate how many model generations contain the pair _Anthony Mark_ pair. The results are show in Table 4. When there are more repetitions in \(D\) (stronger correlation between the two names), it is easier for the relearning algorithm to recover the pair. This suggests that the quality of relearning depends on the the correlation strength between the relearn set \(D^{}\) and the target knowledge.

## 6 Conclusion

In this work, we propose and study _targeted relearning attacks_ as effective methods to recover unlearned knowledge. Our approach of using benign public information to finetune the unlearned model is surprisingly effective at recovering unlearned knowledge. Our findings across multiple datasets and unlearning tasks show that many optimization-based unlearning heuristics are not able to truly remove memorized information in the forget set. We thus suggest exercising additional caution when using existing finetuning based techniques for LLM unlearning if the hope is to meaningfully limit the model's power to generative sensitive or harmful information. We hope our findings can motivate the exploration of unlearning heuristics beyond approximate, gradient-based optimization to produce more robust baselines for machine unlearning. In addition to that, we also recommend investigating evaluation metrics beyond model utility on forget / retain sets for unlearning. Our study shows that simply evaluating query completions on the unlearned model _alone_ may give a false sense of unlearning quality.

   \# of pairs & 7 steps & 12 steps & 17 steps \\ 
7 & 8\% & 48\% & 100\% \\
5 & 2\% & 17\% & 97\% \\
3 & 1\% & 1\% & 23\% \\
1 & 0\% & 0\% & 0\% \\   

Table 4: For each scenario we save 3 relearning checkpoint: shallow (7 steps), medium (12 steps), and deep (17 steps). For each model we record the relearn success rate.

Figure 5: We evaluate on 100 different prefixes, all of which are a sequence of common names. For every prefix \(x\), we calculate the negative log likelihood of _Anthony\(|x\)_ and _Mark\(|x\) Anthony_. We did this for multiple unlearning and relearning checkpoints. The loss on both _Anthony_ and _Mark_ increases as more unlearning happens, which is expected as both names are in \(D_{u}\). However, as we do more relearning, the loss on _Mark_ decreases as the loss on _Anthony_ decreases, even if the former is never optimized.