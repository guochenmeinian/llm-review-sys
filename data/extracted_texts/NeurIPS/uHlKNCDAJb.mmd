# ScenarioNet: Open-Source Platform for Large-Scale Traffic Scenario Simulation and Modeling

Quanyi Li\({}^{}\)\({}^{*}\), Zhenghao Peng\({}^{@sectionsign}\)\({}^{*}\), Lan Feng\({}^{}\)\({}^{*}\),

**Zhizheng Liu\({}^{}\), Chenda Duan\({}^{@sectionsign}\), Wenjie Mo\({}^{@sectionsign}\), Bolei Zhou\({}^{@sectionsign}\)**

\({}^{}\)University of Edinburgh, \({}^{}\)ETH Zurich, \({}^{@sectionsign}\)University of California, Los Angeles

###### Abstract

Large-scale driving datasets such as Waymo Open Dataset and nuScenes substantially accelerate autonomous driving research, especially for perception tasks such as 3D detection and trajectory forecasting. Since the driving logs in these datasets contain HD maps and detailed object annotations that accurately reflect the real-world complexity of traffic behaviors, we can harvest a massive number of complex traffic scenarios and recreate their digital twins in simulation. Compared to the handcrafted scenarios often used in existing simulators, data-driven scenarios collected from the real world can facilitate many research opportunities in machine learning and autonomous driving. In this work, we present _ScenarioNet_, an open-source platform for large-scale traffic scenario modeling and simulation. ScenarioNet defines a unified scenario description format and collects a large-scale repository of real-world traffic scenarios from the heterogeneous data in various driving datasets including Waymo, nuScenes, Lyft L5, Argoverse, and nuPlan datasets. These scenarios can be further replayed and interacted with in multiple views from Bird-Eye-View layout to realistic 3D rendering in MetaDrive simulator. This provides a benchmark for evaluating the safety of autonomous driving stacks in simulation before their real-world deployment. We further demonstrate the strengths of ScenarioNet on large-scale scenario generation, imitation learning, and reinforcement learning in both single-agent and multi-agent settings. Code, demo videos, and website are available at https://metadriverse.github.io/scenarionet.

## 1 Introduction

Autonomous Driving (AD) is revolutionizing mobility with benefits like reliable transportation, travel comfort, and fuel efficiency. As a safety-critical application, it is important to evaluate the AD stack thoroughly and ensure its reliability and safety before the real-world deployment . Unlike the standardized evaluation of the perception module of AD stack on large-scale real-world annotated data , the planning and control modules are often evaluated in simulators with synthetic hand-crafted scenarios . These testing scenarios, often overly simplified, cannot reflect the complexity of real-world conditions like map structures and diverse traffic participant behaviors . Moreover, it is time-consuming and requires domain knowledge to craft such testing scenarios, especially those with many traffic participants . Thus, it remains challenging to scale up the evaluation and the training of the AD's decision-making in a wide range of situations.

Collecting traffic scenarios from real-world driving data can address the data accessibility issue. By replaying the real-world traffic scenarios in simulation, we can further benchmark the decision-making of the AD system and improve the generalizability of the data-driven planning and control components. However, several critical issues need to be addressed when building a large-scaledata-driven simulation platform: First, annotated driving data is precious and we would like to use as much data as possible. However, it is difficult to achieve this since publicly available driving data are released by different organizations and in various formats. This creates a bottleneck in aggregating the data volume from multiple sources for training and evaluating AD systems. Second, existing datasets are closely coupled with specific simulators or toolkits built for ad hoc applications, which largely limits the possibility of data sharing. For example, nuPlan  with 1500+ hours driving data aims at single-agent Imitation Learning (IL); Nocture  with 500+ hours Waymo motion data is mainly designed for multi-agent Reinforcement Learning (RL) under partial observation; SimNet built on 1000 hours L5 motion dataset targets at scenario generation. Researchers who want to use nuPlan data for multi-agent RL have to spend a significant effort on building a new simulator or bridging Nocture  and nuPlan dataset. This greatly restricts the potential of cross-dataset training. Furthermore, driving datasets like nuPlan , nuScenes , and Argoverse [8; 59] provide raw sensor data like images and point clouds, but existing 2D data-driven simulators can not make use of them. The lack of z-axis and 3D graphics prevents training visuomotor policies and evaluating end-to-end AD stacks, like Openpilot  in simulation.

To tackle the aforementioned challenges, we introduce _ScenarioNet_, an open-source platform to unify the heterogeneous driving data from various sources for large-scale traffic scenario simulation and modeling. It extracts a massive number of traffic scenarios in a unified format from various datasets, including Waymo motion datset , nuScenes dataset , L5 motion prediction dataset , Argoverse dataset [8; 59], and nuPlan dataset . These scenarios can be replayed and reacted through the MetaDrive simulator  in multiple views from BEV layout to realistic 3D rendering. _ScenarioNet_ enables many machine learning applications like large-scale scenario generation, imitation learning, and reinforcement learning in both single-agent and multi-agent settings. With the built-in ROS bridge, the digital twins of the traffic scenarios can further serve as testbeds for AD stacks, where in our case we evaluate the open-source one like Openpilot . We conduct a range of experiments based on _ScenarioNet_. First, we train a large scenario generation model on a joint dataset and analyze the embedded space of traffic scenarios to identify the relationship across various datasets. After that, we conduct cross-dataset single-agent training and testing experiments to show the advantage of real-world data for training ML-based planners. In addition, multi-agent RL/IL experiments are conducted on Waymo dataset [32; 18] to show the support for agent-based scenario generation. Lastly, we test Openpilot , an end-to-end AD stack in reconstructed scenarios. We hope our open-sourced _ScenarioNet_ with its flexible APIs and toolkits empowers the community with many new research opportunities and propels the development of safe and generalizable AI for autonomous driving.

## 2 Related Work

**Synthetic scenario database.** The majority of the existing scenario databases are based on synthetic data which are either handcrafted [3; 62; 16; 21; 17; 29; 63], or generated based on rules [7; 30; 24; 53;

Figure 1: Snapshots of three scenarios extracted from _nuScenes_ and their corresponding interactive environments with multiple views and sensors including RGB camera, depth camera, and semantic camera. The RGB sensor can be used for end-to-end driving systems like Openpilot .

66, 4] and adversarial attacks  which, in particular, aims at building safety-critical cases. Synthetic scenarios are predominantly described using OpenScenario , Scenic , and Python language , allowing the definition of map structures, initial actor displacements, and subsequent behaviors. These scenario descriptions can be parsed for building scenarios in CARLA  seamlessly. Representative databases are OSC-ALKS , highway-env , DI-drive , and SafeBench  which mainly provides safety-critical scenarios.

**Real-world scenario database.** To the best of our knowledge, CommonRoad  and nuPlan  are the only real-world datasets curated specifically for the development and testing of the planning module. CommonRoad lacks sensor data and has a limited number of scenarios, while nuPlan  has more than 1500+ hours of annotated driving data. Though high-quality data is provided by nuPlan, the accompanying _nuPlan-devkit_ is limited to open-loop/close-loop IL. Extra effort is needed for it to support wide applications like RL-based planner training.In this case, _ScenarioNet_ can complement single-agent and multi-agent RL training with nuPlan data. Besides, _ScenarioNet_ has a 3D rendering support which can additionally make use of the sensor data in nuPlan dataset for investigating end-to-end AD stack and some interdisciplinary tasks like street scene reconstruction. Furthermore, _ScenarioNet_ can incorporate more motion prediction datasets like Waymo dataset , nuScenes dataset , and L5 dataset  for cross-dataset training and testing.

**Data-driven simulation.** We refer to driving simulators capable of replaying real-world scenarios or generating realistic traffics as data-driven simulators. In Table 1, we summarize the properties of existing data-driven simulators in terms of widely supported real-world data sources and applications, and tasks. Each simulator has a different set of supported datasets. The support for RL and IL entails single-agent driving policy learning for the ego vehicle only, while MARL (IL) needs to control additional vehicles present in the scenario. Thus scenario generation represents methods requiring not only actuating objects in a similar way to trajectory forecasting  but composing new traffic scenarios by inserting vehicles into an empty map. TrafficGen  exemplifies how _ScenarioNet_ enables building the training dataset and loading generated scenarios for RL training. Among all the simulators, only the simulation environment provided by _ScenarioNet_ supports AD stack testing. The last column shows that _ScenarioNet_ and VISTA  are the only two platforms that utilize camera and lidar data for 3D rendering support. Though both VISTA and _ScenarioNet_ can train driving policies, there is a distinct difference. VISTA achieves closed-loop training and synthesizing new sensor data by applying relative transformations to the recorded sensor data. In contrast, _ScenarioNet_ reconstructs scenarios with mid-level object representations, such as vehicle positions and velocities. Despite the loss of raw sensor data fidelity, this design enables broader scope of capabilities including not only closed-loop training but also scenario-based AD stack testing and scenario generation.

## 3 System Design of ScenarioNet

Fig. 3 illustrates the system design of the proposed _ScenarioNet_. It collects a massive number of real-world traffic scenarios by converting heterogeneous driving data from different sources, like Waymo dataset , nuScenes/nuPlan dataset , and so on. Meanwhile, it can replay the imported scenarios in the MetaDrive simulator  through the unified scenario description, supporting various

    &  Supported \\ Dataset \\  } &  &  &  Multi-agent \\ RL \& IL \\  } &  Scenario \\ Generation \\  } & AD Stack & 3D \\  & & & & & & \\  nuPlan-devkit  & nP & & ✓ & & & & \\  DriverGym  & L5 & ✓ & ✓ & & & & \\  Nocture  & W & ✓ & ✓ & ✓ & & & \\  TrafficSim  & - & & & & & ✓ & \\  SimNet  & L5 & & & & ✓ & & \\  VISTA  & W, nS & ✓ & ✓ & ✓ & & & ✓ \\ 
**ScenarioNet** & W, L5, nP, nS, Ag & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparison of representative data-driven simulators. _ScenarioNet_ supports Waymo (W), L5 Lyft, nuPlan (nP), nuScenes (nS), and Argoverse (Ag) datasets and various applications and tasks.

machine learning applications and tasks, such as AD stack testing, single-agent learning, multi-agent learning, and scenario generation. We describe each component of _ScenarioNet_ in more detail below.

### Unified Scenario Description

A unified scenario description format is the key for extracting scenario data from different sources. As shown in Fig. 2, we define the unified scenario description format as a nested dictionary with 4 top-level keys. Each scenario in the source dataset will be converted into scenario_id.pkl.

**Map Features.** Each scenario description file stores map features consisting mainly of lanes and lane lines. Both lanes and lane lines have a polyline field recording the lane centerline or lane line itself. Given an object, we can know how far it moves along the lane and the lateral distance to the lane centerline by converting its position from the global Cartesian coordinate to the lane centerline's Frenet coordinate. On the other hand, contact checks can be executed between a vehicle and a lane line to decide whether the vehicle is driving across the lane line. For lanes, it has two extra fields: polygon, and connectivity. Polygon can be used to query if a given object is on the lane, and the drivable area can be determined by combining polygons of all lanes. Connectivity records the neighbor lanes, next and previous lanes of the given lane.

**Objects.** In most raw driving data, the road objects are usually stored in a frame-centric way where the user need to visit all time frames before collecting the state sequence of an object. In _ScenarioNet_, the object representation is object-centric meaning that querying the object dictionary with an object ID will return the state sequence across the episode. This is convenient for downstream applications which are mostly object-centric. Take trajectory prediction as an example. For getting the ground truth of the motion of an object, user can query the description with [object_id]["position"] and [object_id]["velocity"]. The queries will return the whole trajectory and velocity. However, not all objects are valid throughout the whole episode and hence we introduce a [object_id]["valid"] key for indicating which frame the object presents. For loading the objects into the simulator, we synchronize the position, heading, velocity, and rotation rate between the logged data and the simulated object entity when the [object_id]["valid"] indicator is True at the given frame. Otherwise, the object will disappear and be destroyed. By doing so, the digital twin scenario can be faithfully reconstructed in MetaDrive simulator.

**Traffic Lights.** Similar to objects, traffic lights are stored in a dictionary with the ID as the key and properties as the value. The light states can be accessed by [light_id]["state"] throughout one episode, where each element can be Red, Yellow, Green, or Unknown and represent the status of the corresponding lane. As each object can localize which lane it is on, it can easily know if there is a traffic light on the current lane or an oncoming lane.

**MetaData.**["metadata"] exists at each level of the nested dictionary, storing miscellaneous data. For example, for an object, the [object_id]["metadata"] can store the instance ID provided by the original dataset. For the top-level dictionary of scenario description, a ["metadata"] stores the information about the source dataset, like the original scenario ID, which original file this scenario comes from, the time interval between two frames, the coordinates system of the data, _etc_. In addition, some statistics of the scenario are included such as the number of objects, the number of traffic lights, and the moving distance of each object. This information can be used to select or filter scenarios to build new databases as in Sec. 3.2.

To ensure a scenario description can be loaded into the simulator, we provide a tool script to check if necessary keys are present in the specific dictionary and if the data type of the key's value is correct. For example, ["position"] can not be absent from the object property dictionary, and its value should be in shape \((N,3)\) where \(N\) is the episode length. As long as all necessary keys exist, users can add new fields to the scenario description to log more details about the scenario.

Figure 2: Example of Scenario Description

### Scenario Database

The database in _ScenarioNet_ is a folder containing two summary files dataset_summary.pkl and a dataset_mapping.pkl. dataset_summary.pkl stores a dictionary with keys indicating which scenarios are included in this database and values storing the metadata of each scenario. dataset_mapping.pkl records the relative path of the scenario file scenario_id.pkl from the database folder, and hence no matter where the scenario files are we can access them through the relative path recorded in dataset_mapping.pkl.

**Database Operations.**_ScenarioNet_ provides several operations to create and maintain the databases including raw data conversion, sanity-check, merging, splitting, sampling, and filtering. A new database can be formulated with a series of operations. _Conversion_ is the basic operation for creating a database by processing driving data in various formats to the unified scenario description. This operation produces a database: a folder with summary files dataset_summary.pkl and dataset_mapping.pkl and a number of converted scenario files with filenames like sd_nuscenes_scenes-0061.pkl. Currently, we provide data converters for parsing data from Waymo , nuPlan , nuScenes , L5 , and procedurally generated dataset . It is also convenient to write new converters for users' own dataset for exploiting the power of _ScenarioNet_. After converting and building from raw data, the databases then can be operated to create new databases on demand. For example, Merging operation combines several databases together to get a larger one. Filtering operation creates a new database with scenarios satisfying specific conditions. For example, we can build a database with scenarios where ego car moving distance > 10 meters and the number of traffic participants > 200. Sanity-check is a special filter for building a new database by discarding broken scenarios that can not be loaded into the simulator. Splitting and sampling aim to divide training and non-overlapping test sets. Instead of copying the raw data when creating new dataset, we only read the summary files from source dataset and create new dataset_summary.pkl and dataset_mapping.pkl. The copy-free design and the parallel operations ensure efficiency.

**New Dataset Support.** It is easy to add support for new datasets by following these steps:

1. Download the original data and set up the official codebase for parsing this data.

Figure 3: From bottom to top, _ScenarioNet_ platform consists of the data layer, system layer, and application layer which are connected by two critical data flows, data conversion (\(\)) and simulation (\(\)). Data conversing unifies various data formats and stores them in an internal scenario description. The system layer then provides a set of tools to operate converted data efficiently, such as filtering (\(\)), merging, sanity-check, splitting (\(\)) and so on. Once the database is ready, it can be loaded into MetaDrive for large-scale simulation and various applications.

2. Make a \(\) which takes one scenario recorded in the original format as input and returns the scenario described in a new format. Actually, this process is like filling in the blank of the target scenario description by parsing the original data format with official APIs.
3. Scale up the data conversion with the built-in function, \(\). It takes the \(\) and a list of scenario indices or original scenarios as input and writes the converted scenarios into a target directory. In this process, the multi-process parallel converting, sanity-check and other trivial steps will be finished automatically.

Though step 2 seems time-consuming, most of the work in this step is, actually, calling APIs provided by the new dataset, retrieving the desired data, and putting them in the corresponding field. A good example is \(\) where most of the subfunction is called \(\). Since the structure and fields to fill in the scenario description are clear and automatic tools are provided, we believe it would be easy for the community to add support for more datasets.

### Scenario Simulation

We simulate the traffic scenarios as interactive environments through an upgraded version of our MetaDrive simulator , which encompasses Bullet  physics engine, 2D Pygame rendering backend, and 3D OpenGL rendering backend. The physics engine not only simulates realistic vehicle dynamics but also efficient contact and collision detection among vehicles, objects, and map features such as lane lines. As a result, the physics simulation can run up to 500 FPS for real-world scenarios containing +100 interactive objects including pedestrians, vehicles, cyclists, and traffic barriers/cones. We use synthetic models to represent pedestrians and vehicles, so no personal information is identifiable.

**Sensors.**_ScenarioNet_ supports collecting the mid-level representations of scenarios through 2D pseudo-lidar and bird's-eye view images. It also supports querying the internal simulator states for precise ground truth information about surrounding objects, such as their positions and velocities. Nevertheless, end-to-end commercial AD stacks like Openpilot  are emerging and a recent trend suggests it is important to consider the perception and planning system as a whole . _ScenarioNet_ thus provides RGB-camera, depth-camera, and point-cloud for sensor simulation. All camera outputs are synthesized via the OpenGL rendering backend, which is wrapped by the Panda3D game engine and allows users to add new visual effects or shaders via Python code easily. We optimize the camera efficiency by bridging CUDA and OpenGL so that the images rendered by OpenGL in VRAM can be converted to Pytorch  Tensor directly. The entire procedure occurs on the GPU, resulting in an improvement from 11 FPS to 300 FPS when simulating a 1920x1080 image. In addition, to better simulate RGB camera output, we upgrade the 3D-rendering effect of the native MetaDrive 3D renderer through deferred rendering which enables simulating photorealistic light, shadow, and material. Some examples of the supported sensors are in Fig. 1.

**Control Policies.** All objects in the scene are controlled by policies. In a single-agent setting, vehicles can be controlled by either the \(\) to replay logged trajectories or the \(\) to not only replay but react to other objects. The \(\) enables closed-loop simulation by detecting if there are objects present on its future trajectory and adjusting the target velocity to avoid collision with the leading object. In this way, when the ego car deviated from the recorded trajectory, vehicles behind it can react to this change and show yielding behavior. Also, the ego car can be controlled by the \(\) to the replay trajectory or the \(\) to accept external control commands from \(\). Moreover, by assigning the \(\) to traffic vehicles, the task can be turned into a multi-agent setting. This policy-based design in _ScenarioNet_ allows for mixed-policy simulation and provides high customizability. For instance, one can develop a policy that wraps a ROS bridge to connect open-source AD stacks to the simulation.

## 4 Experiments and Applications

We conduct a range of experiments and demonstrate various applications of _ScenarioNet_. We introduce how we built databases containing scenarios from different sources and then use these databases to conduct experiments on single-agent and multi-agent learning and AD stack testing.

### Database Construction

**Waymo.** Waymo database is built with the Waymo Motion dataset . The raw data contains 500+ hours of driving logs divided into 70,000+ scenarios with 20 seconds duration for each scenario. We convert all of them into internal scenario descriptions and then filter out the scenarios where the ego vehicle moves less than 10 meters or where overpasses exist.

**nuPlan.** Though nuPlan  has more than 1500 hours of driving data, we only use those recorded in Boston as we want to keep all databases in the experiments to have similar sizes. After filtering out scenarios whose ego car moves < 10 meters, we finally collect 50,261 20-second scenarios.

**PG.** PG stands for procedural generation which is used to generate infinite maps and traffic according to a set of pre-defined roadblocks and traffic initialization rules. It is the default scenario generation method of MetaDrive simulator . We set the traffic density to 15 vehicles per 100 meters and 2 roadblocks in each scenario to keep scenario length consistent with real-world scenarios. We finally collected 50,000 scenarios to form the PG database.

Table 2 provides statistics about the three datasets. _Track length_ stands for the average moving distance of the ego car across all scenarios, and PG scenarios have the longest moving distance. Waymo scenarios are relatively more complex compared to others, as they have more vehicles and more intersection scenarios, which is reflected in Fig. 4 as well. The _Construction Ratio_ represents the ratio of scenarios containing traffic cones and barriers out of all scenarios. nuPlan-boston database has traffic cones and barriers in all scenarios, while no objects are contained in the Waymo database. More database construction details are in Appendix F.

### Visualization of Scenario Embeddings with Scenario Generation Model

To reveal the differences and gaps across three databases, we use the internal embedding from a scene generation model, TrafficGen , to visualize the scenario snapshots from all three databases. The state initializer in TrafficGen utilizes an encoder-decoder structure that encodes the traffic scenario into a latent space and then decodes the initial states distribution of traffic participants, enabling the sampling and generation of a new scenario snapshot. We randomly select 1000 scenarios from each database, feed them into TrafficGen to obtain the embeddings of these scenarios and then employ t-SNE  to visualize them in Fig. 4. We can see there is a noticeable domain gap between synthetic and real-world data. Only two synthetic intersection scenarios are close to real-world scenarios and bridge the synthetic scenarios and real scenarios. Additionally, even among real-world scenarios, a minor distribution gap remains, primarily due to different road network topologies in different regions where the data is collected. **These findings suggest that solely relying on driving data from one

   Dataset & Track Length & No. of Vehicles & No. of Pedestrians & Intersection Ratio & Construction Ratio \\  Waymo & 136.55\(\) 95.98) & 89.93\(\) (64.51) & 11.7(\(\) 22.97) & 0.71 & 0.0 \\ nuPlan & 95.48\(\) (42.32) & 53.96\(\) 25.35) & 21.99\(\) (19.9) & 0.57 & 1.0 \\ PG & 226.07\(\) (70.7) & 9.81\(\) (3.31) & 0.0 & 0.36 & 0.39 \\   

Table 2: The statistics of the Waymo, nuPlan, and PG database.

region or one database is insufficient to cover all potential traffic situations.** Therefore, it is important to aggregate as many scenarios as possible. Additional showcases from different databases, visualizations of generated scenarios, and training details can be found in Appendix A.

### Cross-dataset RL Generalization

The previous experiment reveals that the domain gap exists between synthetic scenarios and real-world scenarios. We would like to further investigate whether the domain gap affects driving policy learning. To this end, we build a data-driven RL environment where the agent should arrive at the destination of the recorded trajectories in time. Traffic vehicles behind the ego car are controlled by IDMPolicy to achieve closed-loop training and testing. We further split training and test sets for the nuPlan database and the PG database, respectively, and get 4 splits: _nuPlan-train_, _PG-train_, _nuPlan-test_ and _PG-test_. Each training set contains 40,000 scenarios, while each test set has 5,000 non-overlapping scenarios. The scenarios of two training splits are sorted according to the difficult score, a metric considering length and curvature of trajectory, and constitutes 100 levels for conducting the curriculum training. Each level contains 400 scenarios and a 75% _Success Rate_, the ratio of scenarios where the agent can reach the destination out of 400 scenarios, is required for the learning environment to level up. We train PPO  agents on _PG-train_ and _nuPlan-train_ splits with the curriculum training and evaluate them on _nuPlan-test_ split to study whether the domain gap harms the performance for the learning-based controller. All results are averaged across 5 random seeds and the error bar shows the standard deviation. As shown in Fig. 5, when testing on held-out _nuPlan-test_ set, the agents trained with _nuPlan-train_ split outperform those trained with _PG-train_ split in terms of _Success Rate_. This is because the agents trained in synthetic scenarios fail to learn high-speed driving skills and thus can not reach the destination in time, inducing a high _Timeout Rate_. **Thus it is necessary to train learning-based controllers with real-world data to close the sim-to-real gap**. We also ablate the curriculum training and the results

Figure 4: t-SNE visualization of the feature embeddings of scenarios from three databases. Scenarios from nuPlan and Waymo datasets are much more similar, while synthetic PG data has a clear domain gap with both real-world datasets. We also plot the representative scenarios in clusters. As shown in the left-side figures (\(\)), synthetic scenarios can be neatly categorized into distinct clusters, while real-world scenarios shown on the right side are too complex to identify clear boundaries. Nevertheless, the right-side figures show scenarios from nuPlan (\(\)) have simpler map structures and mild traffic, compared to the Waymo scenarios (\(\)). In addition, there are two outliers that confuse the model, causing it to cluster them with real-world scenarios. The visualization of the two synthetic scenarios indicates that the intersection scenario can bridge the sim-to-real gap.

Figure 5: Evaluation on _nuPlan-test_ split.

suggest that **it is critical to adjust the scenario difficulty according to the agent performance in the course of training**. Finally, we build a new dataset containing 40,000 scenarios by combining half of the _PG-train_ split and half of the _nuPlan-train_ split. Figure. 5 shows that agents trained on these combined datasets achieve the highest _Success Rate_ when evaluated on _nuPlan-test_ split. This is because almost every synthetic scenario contains bends and curated trajectories, which only exist in approximately 20% of real-world scenarios. As a result, agents acquire a better ability to follow a curved trajectory, achieving the lowest _Out of Road Rate_. **Thus aggregating synthetic scenarios to the training dataset can benefit policy learning as they improve the diversity of training data.** More details of the experiments are in Appendix B.

### Multi-agent Cooperative Learning

_ScenarioNet_ can load scenario data into an interactive multi-agent policy learning environment and generate local sensory observations for each agent. Therefore, _ScenarioNet_ enables the critical research on multi-agent reinforcement learning as the agents are self-interested and the number is varying, wherein cooperative MARL methods can not be applied , as well as the research on multi-agent imitation learning with the challenges of learning from observation or say the action-free imitation learning, meaning that the ground-truth trajectory does not contain action but only the sequence of states (observations). _ScenarioNet_ bridges the research of multi-agent decision-making with vast volume of real-world data. We load the ground-truth (GT) trajectory in the dataset and use it to form the termination condition, the reward function, and the evaluation metrics. For multi-agent RL, we use the GT trajectory to define _the displacement reward:_\(r_{t}^{i}=\)_the displacement of agent \(i\) at step \(t\) projected into its underlying ground-truth trajectory_. We train independent PPO  and TD3  agents as well as the Coordinated Policy Optimization (CoPO) agents . For multi-agent IL, to address the action-free issue, we use GAIL  in multi-agent setting  but the discriminator distinguishes state-next state pair, instead of state-action pair . We also train multi-agent Adversarial Inverse RL  (AIRL) with an additional inverse dynamics model for estimating the expert actions. The inverse dynamics model is trained concurrently with the AIRL policies and learns the action given state-next state pairs from the environment interactions. The ground-truth trajectory can be used to measure the learned behaviors, such as _the route completion rate_, the ratio between the length of projected agent trajectory and the length of GT trajectory as well as _the average and final distance_ between agent trajectory with GT trajectory. The cost is the number of crashes of an agent in one episode. As shown in Table 3, we find that the **displacement reward is strong supervision to learn multi-agent behavior** as the RL can explore and exploit the reward function by interacting with the environment, without learning a discriminator as in GAIL or a reward function and an inverse dynamics model in AIRL. More details can be found in Appendix C.

### AD Stack Testing

We released ROS bridge , allowing connecting _ScenarioNet_ with open-sourced AD stacks like autoware  and planning algorithms developed by the ROS community. As a result, researchers can study self-driving systems in real-world scenarios. In our experiment, we test Openpilot , an end-to-end AD system taking only visual information as input. As the navigation module of Openpilot is still a beta version, the test happens in scenarios without diverged roads. As shown in Fig. 6, the Openpilot system is robust enough to operate in common scenarios like lane-keeping and stopping at traffic lights. More scenario-based test videos and details are available in Appendix E.2.

    & Route & Success & Reward & Cost &  Average \\ Distance \\  & 
 Final \\ Distance \\  \\  TD3 & 0.669 (\(\)0.018) & 0.513 (\(\)0.025) & 74.87 (\(\)5.90) & 2.01 (\(\)0.24) & 12.64 (\(\)0.72) & 45.13 (\(\)2.73) \\ PPO & 0.740 (\(\)0.037) & 0.579 (\(\)0.054) & 85.28 (\(\)8.96) & 2.83 (\(\)0.53) & 12.79 (\(\)0.73) & 36.71 (\(\)6.38) \\ CoPO & 0.745 (\(\)0.024) & 0.570 (\(\)0.043) & 85.94 (\(\)4.60) & 2.88 (\(\)0.32) & 12.86 (\(\)0.63) & 34.75 (\(\)3.78) \\  AIRL (Disc.) & 0.367 (\(\)0.027) & 0.153 (\(\)0.028) & - & 0.55 (\(\)0.12) & 17.73 (\(\)1.87) & 80.57 (\(\)5.26) \\ AIRL (Cont.) & 0.138 (\(\)0.016) & 0.015 (\(\)0.009) & - & 0.42 (\(\)0.16) & 3.63 (\(\)1.07) & 111.23 (\(\)7.48) \\ GAIL (Disc.) & 0.487 (\(\)0.021) & 0.218 (\(\)0.018) & - & 1.64 (\(\)0.35) & 28.48 (\(\)2.23) & 68.53 (\(\)3.57) \\ GAIL (Cont.) & 0.421 (\(\)0.045) & 0.189 (\(\)0.051) & - & 1.56 (\(\)0.52) & 27.33 (\(\)4.72) & 75.98 (\(\)9.02) \\   

Table 3: The multi-agent reinforcement learning and imitation learning results on the Waymo dataset.

## 5 Conclusion

This work presents an open-source platform called _ScenarioNet_ for simulating and modeling driving scenarios. A massive number of real-world traffic scenarios extracted from various datasets can be used for testing AD stacks, generating scenarios, and learning both single-agent and multi-agent policies. We hope it can open up many new opportunities for AI and autonomous driving communities.

**Limitation & Potential Negative Societal Impacts.** One limitation of _ScenarioNet_ at the current development stage is the lack of diverse 3D assets. Currently, the assets are collected from the internet, which may produce unrealistic raw sensor data output. Future work will be to reconstruct the mesh and textures of vehicles, other objects, and background models like streets from driving logs using neural rendering. A detailed plan for improving visual fidelity is in Appendix G. In addition, several potential technical drawbacks exist in _ScenarioNet_. One drawback is that the simulation is single-thread due to the Python GIL and runs on CPU, while some latest simulators like IsaacGym  support GPU-based parallel simulation and thus outperform _ScenarioNet_ in terms of sample efficiency. This problem can be alleviated by using multi-process tools like Ray/Rlib [37; 31] for parallel simulation. Another drawback is that _ScenarioNet_ doesn't support differential simulation which allows training policy with the derivative of reward function directly . In the future, _ScenarioNet_ can address this by introducing a bicycle model for vehicle dynamics simulation, making the system differentiable. One potential negative societal impact is that there is no guarantee of safety for even a data-driven AD stack can pass all the testing scenarios in _ScenarioNet_. There is also a domain gap for the trained model to be deployed in the real world.

**Licenses.**_ScenarioNet_ is released under Apache License 2.0. Panda3D, used in MetaDrive for 3D rendering, is under the Modified BSD license. Bullet engine is under the Zlib license. The vehicle models are collected from Sketchfab under CC BY 4.0 or CC BY-NC 4.0. For real-world data used in experiments, Waymo Open Dataset [32; 47] is under license terms available at https://waymo.com/open. Besides, the L5 dataset, nuPlan dataset, nuScenes dataset, and Argoverse dataset are provided free of charge under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public license. The toolkits for parsing these datasets are all under Apache license version 2.0. The Openpilot system used for AD stack testing is under MIT license.

**Acknowledgement:** This work was supported by the National Science Foundation under Grant No. 2235012 and the Samsung Global Collaboration Award.

Figure 6: Openpilot can operate in real-world scenarios reconstructed with _ScenarioNet_. It can cruise on urban roads and smoothly stop at crowded intersections.