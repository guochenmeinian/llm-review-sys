# Lever LM: Configuring In-Context Sequence to Lever Large Vision Language Models

Xu Yang\({}^{1,2}\), Yingzhe Peng\({}^{1,2}\), Haoxuan Ma\({}^{1,2}\), Shuo Xu\({}^{1,2}\),

Chi Zhang\({}^{3}\), Yucheng Han\({}^{4}\), Hanwang Zhang\({}^{4}\)

\({}^{1}\) Southeast University

\({}^{2}\) Key Laboratory of New Generation Artificial Intelligence Technology & Its Interdisciplinary Applications, (Southeast University),Ministry of Education

\({}^{3}\) Westlake University

\({}^{4}\) Nanyang Technological University

{xuyang_palm, yingzhe.peng, haoxuan-ma, xushuo}@seu.edu.cn

chizhang@westlake.edu.cn, yucheng002@e.ntu.edu.sg, hanwangzhang@ntu.edu.sg

###### Abstract

As Archimedes famously said, "Give me a lever long enough and a fulcrum on which to place it, and I shall move the world", in this study, we propose to use a tiny Language Model (LM), _e.g._, a Transformer with 67M parameters, to lever much larger Vision-Language Models (LVLMs) with 9B parameters. Specifically, we use this tiny **Lever-LM** to configure effective in-context demonstration (ICD) sequences to improve the In-Context Learning (ICL) performance of LVLMs. Previous studies show that diverse ICD configurations like the selection and ordering of the demonstrations heavily affect the ICL performance, highlighting the significance of configuring effective ICD sequences. Motivated by this and by re-considering the the process of configuring ICD sequence, we find this is a mirror process of human sentence composition and further assume that effective ICD configurations may contain internal statistical patterns that can be captured by Lever-LM. Then a dataset with effective ICD sequences is constructed to train Lever-LM. After training, given novel queries, new ICD sequences are configured by the trained Lever-LM to solve vision-language tasks through ICL. Experiments show that these ICD sequences can improve the ICL performance of two LVLMs compared with some strong baselines in Visual Question Answering and Image Captioning, validating that Lever-LM can really capture the statistical patterns for levering LVLMs. The code is available at https://github.com/ForJadeForest/Lever-LM.

## 1 Introduction

With the escalation in model size and training data , Large Language Models (LLMs) emerge the ability of In-Context Learning (ICL) . ICL, akin to few-shot learning , utilizes a few exemplary In-Context Demonstrations (ICDs) to adapt LLMs to new tasks without gradient updates. This achievement in NLP has inspired researchers to similarly enhance Large Vision-Language Models (LVLMs) with ICL capabilities . However, just as in NLP, the effectiveness of ICL in LVLMs is significantly influenced by the configurations of ICDs, such as their selection and ordering . Recent studies  have shown that this sensitivity in LVLMs is further exacerbated by the multimodal combinatorial complexity of vision and language data.

In NLP, researchers employ various strategies to optimize in-context sequences to improve ICL performance, including retrieving representative examples as the ICDs  and re-ordering these ICDs based on specific principles . While these methods have shown improvements,their application remains largely confined to NLP and is less explored in the vision-language domain. Moreover, as shown in Fig. 1(a), the independent operations of retrieval and reordering often result in sub-optimal outcomes. A critical reconsideration of the ICD sequence generation reveals that configuring an optimal ICD sequence should be a coherent process. Instead of independently selecting and re-ordering, each ICD should be chosen conditionally based on the previous ICDs. This mirrors the sequential nature of human sentence composition, where each word is sequentially selected to ensure overall fluency. Such fluency can be characterized by temporal statistical patterns, which allows the design of statistical learning methods to model and learn from data, where Language Model is one typical technique that demonstrates the effectiveness. This analogy supports the hypothesis that optimal ICD sequences may contain inherent temporal statistical patterns.

Although not explicitly stated, some previous studies in NLP work toward this direction by calculating statistical metrics like the perplexity  or the entropy  to discover what statistical characteristics a good prompt should have. However, the requirement to know the probability of each token when calculating these metrics limits their applicability in VL. This is because recently proposed LVLMs  use continuous image patches rather than the tokenized discrete elements as the vision input, rendering these techniques used in NLP inapplicable in VL and the following two questions remain unaddressed: (1) whether effective VL ICDs exhibit certain statistical patterns and (2) whether such patterns can be leveraged to compose new ICD sequences for a given query. This study aims to address these questions. Specifically, we employ a tiny Language Model, _e.g._, a Transformer, to capture the inherent statistic patterns. This tiny LM is named as "**Lever-LM**" since it can lever/control a much larger VLM by composing suitable ICD sequence. Compared with the classic LM, the only difference is that the vocabulary of Lever-LM consists not of standard words, but of examples from the supporting set that will be used as ICDs.

Fig. 1 (b) shows the training pipeline for Lever-LM. Initially, a "ground-truth" dataset is constructed to indicate which examples or their orders can form good ICD sequences. Specifically, we employ a frozen LVLM to evaluate if an ICD sequence facilitates accurate predictions for a given query, _e.g._, answering questions correctly or generating appropriate captions. 1 Lever-LM is then trained to concurrently learn the selection and ordering of ICDs, streamlining the process by eliminating the need for two separate stages typical of previous methods. Our experiments, conducted with two LVLMs--Open-Flamingo  and IDEFICS --on classic VL tasks--Image Captioning and Visual Question Answering-- demonstrate that Lever-LM surpasses several strong baselines, including those that retrieve ICDs based on image similarity. These results confirm that effective ICD sequences contain inherent temporal statistical patterns and such patterns can be learned for composing new ICD sequences for test queries.

Besides the above-mentioned advantages, Lever LM emerges two interesting abilities. First, it has strong length extrapolation ability, _e.g._, when trained on a dataset with only 2-shot ICDs, Lever LM can generate 4 or more-shot ICDs that outperform several strong baselines. Second, Lever LM can construct a "golden" ICD sequence of 8 predetermined ICDs in a fixed order. This sequence can be uniformly applied across different test queries to assist LVLM in label generation, thereby reducing the computational overhead for configuring new ICD sequences for each query. Experiments in IC/VQA tasks show that "golden" ICD sequence achieves 6.91/1.24 improvements compared to

Figure 1: (a) The traditional ICD configuration methods separately select and order the ICDs, leading to sub-optimal ICL performance. (b) Our Lever-LM enables the step-by-step generation of ICD configurations and simultaneously considers the selection of ICDs and the ordering of ICD sequences.

a strong baseline. In addition, we use exhaustive ablations, including applying different ways to construct training set and changing the architecture of Lever-LM, to discover which factors and analyze why they will affect the ICL performance.

## 2 Related Work

**Models with In-Context Learning Ability.** Prompt engineering enables Large Language Models (LLMs) to address downstream tasks without the need for fine-tuning [35; 1; 36]. A variant, ICL, enhances this ability by constructing prompts with a few examples. This has been demonstrated in LLMs such as GPT-3 , LLaMA , and MPT . Recently, witnessing such success in NLP, the VL domain has also developed numerous LVLMs with prompt engineering abilities [38; 39; 13; 40; 41; 42; 43; 44] and ICL ability like , Flamingo , and IDEFICS  Among them, we use Flamingo and IDEFICS as the LVLMs to explore the effectiveness of Lever-LM since they have stronger and more robust ICL ability by using better language encoders and more training data2.

**Configuring In-Context Demonstrations.** Although ICL assists LLMs in better adapting to downstream tasks, its performance is highly sensitive to the selection [19; 20; 15] and ordering [21; 22; 23] of ICDs. Numerous studies have explored diverse methods to select ICDs in the NLP field [45; 46; 47; 48; 29]. For example,  selects ICDs based on the embedding similarity between ICDs and test samples where the embeddings are extracted from an existing language encoder. Such a method is further developed by training an encoder specifically for selection [49; 50; 51; 52; 53; 50].

Regarding the ordering of ICDs, researchers calculate diverse statistical-based metrics to measure the quality of ICD configurations, _e.g._, the Minimal Description Length  and Global and Local Entropy . Besides them, researchers focus more on discovering the statistical patterns of good prompts. For example,  uses perplexity to measure which prompts can better help LLMs perform a task. Furthermore,  unifies diverse statistics-based prompt selection methods [54; 55] from the perspective of mutual information and discover that mutual information or its variants can uncover certain statistical patterns of effective prompts. However, these statistical-based methods require to calculate the token probabilities, making them infeasible to address continuous image patches, thus can not be used in VL.

Besides these NLP studies, in VL,  and  explore diverse ICD configurations in IC and VQA, while only the heuristic-based methods are used for selecting ICDs and do not consider the ordering. In contrast, our Lever-LM can simultaneously learn how to select and reorder the samples and moreover, our Lever-LM is model-specific.

## 3 Lever Language Model

In this section, we introduce how to build Lever Language Model (Lever-LM) for configuring the ICD sequence to lever a given LVLM. First, we briefly introduce the formulations of ICL for Vision-Language (VL) tasks. Then we introduce the construction of the dataset used to train Lever-LM. After that, we show the architecture of Lever-LM and briefly discuss how to train Lever-LM and use it to configure ICD sequences.

**The Formulation of In-Context Learning (ICL).** Given a query input \(^{}\), ICL predicts the corresponding output \(^{}\) using a well-trained foundation model \(\), conditioned on the concatenation of an in-context sequence \(\) and this query. We denote a in-context sequence with \(K\)-shot ICDs \(}\) as \(^{K}=\{}_{1},}_{2},...,}_{K}\}\). Then ICL can be formulated as:

\[^{}_{}(^{} ^{k},^{}),\] (1)

where \(_{}\) denotes the predicted probability of \(\) and "\(\)" represents the decoding strategy, _e.g._, beam search. For each \(}\), it is selected from a supporting set \(_{S}=\{_{1},...,_{N}\}\), where each sample \(_{i}=(_{i},_{i})\): \(_{i}\) and \(_{i}\) respectively denote the input and the corresponding label. It is noteworthy that in diverse VL tasks, \(\) and \(\) have different forms. For instance, in Image Captioning (IC), \(\) is the image and \(\) is the caption; and in Vision Question Answering (VQA), \(\) contains the image and the question, while \(\) is the answer.

**Constructing the Training Dataset.** To train Lever-LM for generating effective ICD sequences for a given LVLM \(\), we should first construct a dataset \(_{}\) containing high-quality ICD sequences for different query inputs. Simply, we use one VL dataset--COCO  from IC-- to show how to construct \(_{}\), which is shown in Fig. 2. Formally, given a dataset \(\) which is already split into the training part \(_{R}\) and the test part \(_{E}\), we build \(_{}\) only from \(_{R}\). As Fig. 2 (a.1) shows, initially, we randomly select \(n\) samples from \(_{R}\) to form an anchor set \(\). Then for each sample \(_{m}=\{_{m},_{m}\}\), we construct a \(K\)-shot in-context sequence \(_{m}^{K}\) for it. Then \(_{}=\{(_{1},_{1}^{K}),(_{2}, _{2}^{K}),...,(_{M},_{M}^{K})\}\) where each training sample contains an query \(_{m}\) and the corresponding \(K\)-shot in-context sequence \(_{m}^{K}\).

To avoid confusion, we remove the subscript \(m\) in following texts. To construct \(^{K}=\{_{1},...,_{K}\}\), we need to select \(K\)-shot samples from the supporting set \(_{S}\), which is set to the complement set of \(\) in \(_{R}\): \(_{R}\). Meantime, we also need to decide which samples should be selected in turn. To achieve this, given the anchor sample \(=\{,\}\) and the partially constructed in-context sequence, _e.g._, a \(k-1\)-shot \(^{k-1}\), we need to know that after adding which sample \(_{S}\), the ICL performance improvement can be maximized by applying the given LVLM \(\):

\[}_{k}=*{arg\,max}_{_{S}}I_{ }(\{,^{k-1}\},)-I_{}(^{k-1},),\] (2)

where \(I_{}\) is one kind of ICL performance measurement related to \(\). Note that Eq. (2) actually uses the greedy sampling method to select the samples every time, while we can use beam search here to further achieve a better solution. Additionally, to improve the diversity of the dataset, we will keep the top-\(b\) highest-scoring ICD sequences \(\{S_{1}^{K},S_{2}^{K},...,S_{b}^{K}\}\) at the last iteration for each \(a\), where \(b\) is equal to the beam size. For example, when setting beam size to \(5\) as shown in Fig. 2 (a.3), we can get \(5\) diverse high-quality ICD sequences for an anchor sample.

Intuitively, for diverse tasks, we can use the corresponding "golden measurement" as \(I_{}\), _e.g._, setting it to CIDEr /accuracy for IC/VQA. However, this strategy encounters two limitations. The first one is that for diverse VL tasks, we need diverse corresponding measurements, which is inconvenient. Second, some "golden measurements" may be impractical to deploy. For example, for IC, calculating CIDEr requires the LVLM to forward multiple times to sample an integral sentence, and then it costs expensive time burdens to construct the dataset. While for VQA, accuracy is a binary value (accuracy=1 when correct and 0 when wrong), then maybe lots of candidates in \(_{S}\) will make the accuracy change from 0 to 1 and then it is hard to judge which one of them is the most suitable one.

To overcome these two limitations, we use a relatively general measurement as \(I_{}\). Formally, since we have the ground-truth results \(\) of the anchor sample, we can use the given LVLM \(\) to measure the prediction confidence of \(\) given the input \(\) and the in-context sequence \(^{K}\):

\[I_{}(^{K},)=P_{}(|^{K },)=_{t}P_{}(y^{(t)}|^{K},,y^{(1:t-1)}).\] (3)

In VL tasks, the ground-truth label \(=\{y^{(1)},...,y^{(T)}\}\) is a sequence, thus we can decompose the probability distribution into a series of productions. Then Eq. (2) selects a sample that can further maximize the prediction confidence given the query input and the current in-context sequence.

Figure 2: (a): The pipeline of constructing \(_{}\). Darker color of \(S_{i,j}^{K}\) indicates a higher score given by Eq. 2. (b): Top: Lever-LM is a two-layer Transformer. Bottom: Each input embeddings is the sum of the random initialized learnable embeddings, the image and text embeddings extracted by CLIP. The dotted block means that some tasks do not exist the text input, _e.g._, IC.

In implementation, \(_{S}\) usually contains huge amounts of samples, _e.g._, \(_{S}\) in COCO  contains about \(10^{5}\) samples. However, we need to calculate Eq. (2) for each \(_{S}\) when selecting \(}_{k}\) for each \(\), which means the whole process of building \(_{}\) is quite time-consuming. To alleviate the cost, as shown in Fig. 2 (a.2), for each specific \(\), we narrow the set size by sampling a much smaller subset \(_{S}^{}\), _e.g._, containing 64 samples \(_{S}^{}=\{d_{}^{1},d_{}^{2},...,d_{}^{64}\}\), from \(_{S}\) for selecting \(}_{k}\). We use diverse sampling strategies to construct this subset, _e.g._, retrieving some samples similar to \(\), and implement exhaustive ablation studies to explore which strategies are useful in Section 4.3.

**Training Lever-LM.** After getting \(_{}\), we use it to train Lever-LM, as Fig. 2(b) shows, it is a two-layer tiny Transformer . The primary difference between Lever-LM and the traditional LM lies in the tokens of the vocabulary, whose tokens are the samples from the supporting set \(_{S}\), _e.g._, the first token corresponds to the first sample in \(_{S}\). Then given the query sample, the ICDs can be selected one by one based on the token distribution produced by the trained Lever-LM, just as when composing a sentence, the words are selected one by one from the word vocabulary.

Besides the tokens from \(_{S}\), three special tokens are added into the vocabulary to help configure the ICD sequence, which are [BOS], [EOS], and [QUERY], respectively representing the beginning of a sequence, the end of a sequence, and the query sample. Given a data sample \((^{K}=\{_{1},...,_{K}\},^{})\) from \(_{}\) where \(^{K}\) is the ICD sequence and \(^{}\) is the query input, we reformulate it into \(\{,+^{},_{1},...,_{K}, \}\) where \(+^{}\) denotes to add two embeddings. This reformulated sequence is input into Lever-LM for training.

To train Lever-LM, we should embed the tokens of the vocabulary to get dense embeddings. Since each token contains both image and text, we use the vision encoder \(F_{I}()\) and the language encoder \(F_{T}()\) of CLIP  to embed the image and text, respectively. Meanwhile, we add each of these embeddings with a learnable part \(_{i}\) that is randomly initialized. Then for the \(i\)-th token \(_{i}=(I_{i},T_{i})\) in the vocabulary where \(I_{i}/T_{i}\) are the corresponding image/ text, its token embedding is \(_{i}\):

\[_{i}=F_{I}(I_{i})+F_{T}(T_{i})+_{i}.\] (4)

Note that \(T_{i}\) varies between IC and VQA tasks where it denotes caption in IC and question in VQA.

For test query \(x^{}\), we use the same vision and language encoders to embed it. For VQA, the image and question are embedded and summed, while for IC, only the image is embedded. Lastly, we use the cross-entropy loss for training as a standard LM that given the previously \(k-1\) tokens, we maximize the probability of the \(k\)-th ground-truth token.

**Configuring the ICD Sequence to Lever LVLM.** After training Lever-LM, we use it to configure the ICD sequence. Given a query sample \(x^{}\), we initialize the input sequence as \(\{,+e_{x^{}}\}\) and then generate the ICDs one by one, where \(e_{x^{}}\) is the embedding of \(x^{}\) computed by Eq. (4). After iteratively sampling \(K\)-shot ICDs, we can compose the corresponding in-context sequence \(^{K}\) for \(x^{}\) and then use Eq. (1) to implement the ICL.

## 4 Experiments

### Datasets and implementation details

Our approach is evaluated on MS-COCO  for Image Captioning (IC) and VQAV2  for Visual Question Answering (VQA). For each corresponding dataset, we use the train split to construct the \(_{}\) and use the validation split to evaluate the performance of ICD configurations generated by Lever-LM. More details are given in Appendix A.

To get \(_{}\), we select \(5000\) samples to get the anchor set \(\). For each anchor sample, we randomly choose \(64\) samples to build the sub-supporting set \(_{S}^{}\). The beam size for these processes is 5. To train Lever-LM, different strategies are employed for IC and VQA. In IC, the weight of CLIP model will be frozen, and an MLP adapter is introduced to its output. While, for VQA, the CLIP encoder remains trainable, and no adapter is appended. The training phase leverages the AdamW optimizer  and a cosine learning rate scheduler. We set the learning rate to \(1 10^{-4}\) and the batch size to 128. We train our Lever-LM for 20 epochs. To implement ICL, we use OpenFlamingoV2-9B  and IDEFICS-9B  as our LVLMs. We use beam search during inference where the beam size is set to 3. Besides, we set the maximum number of generated tokens as 20 in IC and 5 in VQA.

### Results and Analyses

#### 4.2.1 Comparison Methods

We compare Lever-LM with 4 ICD selection strategies:

**Random Sample (RS)**: RS constructs \(^{k}\) by randomly selecting and ordering \(k\) ICDs from \(_{S}\).

**Similarity-based Retrieval methods**: To date, only a few studies focus on configuring ICD sequence for solving VL tasks [24; 25], where both studies show that, despite their simplicity, similarity-based retrieval methods are effective for selecting ICDs. We therefore consider these strategies as current SOTA benchmarks to assess our effectiveness. 3. They form \(^{k}\) by computing the cosine similarity between the query input \(^{}\) and ICDs in \(_{S}\) where CLIP is used to extract features. We follow  to sort examples in ascending order by their similarity to the query input, so the rightmost demonstration is the closest example. Similarity-based methods contain three variants: (1). **Similarity-based Image-Image Retrieval (SIIR):** We select \(k\) ICDs from \(_{S}\) with highest image similarity to the query image. (2). **Similarity-based Text-Text Retrieval (STTR):** We select \(k\) ICDs from \(_{S}\) with highest text similarity to the query text. This method is only applicable to VQA where question is used as text and not infeasible for IC. (3). **Similarity-based Image-Text Retrieval (SITR):** We compute the similarity between query image and all text of \(d_{i}_{S}\) and select ICDs whose texts have the top-\(k\) similarities with the query image. For IC/VQA, we use caption/question for IC/VQA.

#### 4.2.2 Main Result

The results for various ICD selection strategies are shown in Table 1 for IC and VQA. For Lever-LM, it is trained by \(_{}\) whose ICD length is set to 2. Due to increased inference time with more shots, we do not test the inference results for 5- and 7-shots. The table shows the length interpolation and extrapolation ability of Lever-LM. Interpolation refers to performance with ICDs shorter than those in the training set \(_{}\), which contains only 2-shot ICDs, and is denoted as "Avg:1\(\)2". Extrapolation pertains to performance with ICDs longer than those in \(_{}\), represented as "Avg:3\(\)8". The notation "Avg:1\(\)8" indicates overall performance across 1 to 8 shots. Future analysis will focus on comparing these averages to minimize potential bias across methods.

Overall, Lever-LM achieves the best performance on most cases compared to other methods on both LVLMs. Notably, Lever-LM excels in Avg:1\(\)2. Specifically, in VQA, Lever-LM surpasses the best performing SIIR method by 3.07 (48.75 vs. 45.68) and 0.57 (53.65 vs. 53.08) in accuracy on the IDEFICS and OpenFlamingo models, respectively. In IC, Lever-LM outperforms the best baseline, SIIR, by 6.03 (84.32 vs. 78.29) CIDEr on OpenFlamingo. Similarly, for IDEFICS, Lever-LM achieves a higher CIDEr of 3.2 (89.57 vs. 86.37) compared to the best baseline, RS.

Moreover, Lever-LM has remarkable extrapolation abilities. Regarding Avg:3\(\)8, Lever-LM maintains the top performance in both IC and VQA. Specially, on OpenFlamingo, Lever-LM outperforms SIIR with a 0.8 higher CIDEr in IC (96.52 vs. 95.72) and a 0.75 greater accuracy in VQA (52.59 vs. 51.84). Meanwhile, on IDEFICS, compared with RS, Lever-LM achieves a 2.93 higher CIDEr in IC (105.79 vs. 102.86) and 0.49 higher accuracy in VQA (54.84 vs. 54.35). These results indicate that _Lever-LM can effectively identify and utilize internal statistical patterns to compose longer, high-quality ICD sequences, even from a dataset comprising only two shots._

When we delve deeper into the results in Table 1, we find that the relative performance of similarity-based methods and RS varies by LVLM and task. For example, for IC, SIIR outperforms RS ( Avg:1\(\)8: 89.91 vs. 88.48) when OpenFlamingo is used to implement ICL while SIIR significantly lags behind RS (Avg:1\(\)8: 88.19 vs. 97.36) when IDEFICS is used. Also, for VQA, STTR is comparable to RS (Avg:1\(\)8: 47.98 vs. 47.94) on OpenFlamingo while STTR is defeated by RS (Avg:1\(\)8: 49.75 vs. 53.54) when IDEFICS is used. These performance fluctuations demonstrates the instability of these heuristic-based methods. However, Lever-LM does not have such serious fluctuations where it outperforms both RS and similarity-based retrieval methods across various LVLMs and tasks on average. Such observations also suggest that _Lever-LM may capture the stable statistic patterns between ICDs_.

Besides the above-mentioned advantages, Fig. 3 shows that _Lever-LM is more robust to the Short-cut Inference brought by using similarity-based retrieval methods [24; 25]_. For example, in (a) and

[MISSING_PAGE_FAIL:7]

Eq. (3) to build \(D_{}\) may introduce certain in-domain bias which is beneficial for interpolation while detrimental for extrapolation on IC.

For different sample methods of constructing the \(_{S}^{}\) in table 2 (9) \(\) (11), we find Random is the best in both IC and VQA. We suppose this is because selecting similar ICDs with the anchor sample from \(_{S}\) will damage the diversity. Previous study  in NLP validates that the diversity of the ICD sequences will also help improve the performance of LLMs.

**Diverse scorers \(I_{}\) for evaluating ICD sequences.** To evaluate the quality of ICD configurations, we can use task-specific metrics as \(I_{}\) to build \(_{}\), such as CIDEr in IC. Table 2 (2) and (12) compare the results between using prediction confidence Eq. 3 (2) and CIDEr (12) as \(I_{}\). We find that using CIDEr achieves 3.61 higher than Confidence in Avg:1\(\)2, suggesting that CIDEr can assign a more accurate and reasonable score for ICD configurations. However, the length extrapolation capability decreases obviously, which is 3.0 lower than Confidence in Avg:3\(\)8, validating the robustness of Confidence scorer. Moreover, it will cost more time to construct \(_{}\) by task-specific metric is used, _e.g._, CIDEr costs approximately 10 times of Confidence when constructing \(_{}\).

**Diverse LM Structures.** Table 2 (13) shows the results of using LSTM  as Lever-LM, we find that this still achieves excellent performance. For example, in IC, the overall performance improves by 3.64 (92.12 vs. 88.48) compared to the RS baseline, while in VQA, it is improved by 1.38 (49.32 vs. 47.94). However, due to the weak representation learning capability of LSTM, its performance is lower than Transformer,_e.g._, the scores decrease by 0.33/1.99 in IC/VQA, respectively. Overall, these results suggest that effective ICD configurations contain internal statistic patterns which can be captured by different temporal learner.

    & &  &  \\   & & **Avg:1\(\)2** & **Avg:3\(\)8** & **Avg:1\(\)8** & **Avg:1\(\)2** & **Avg:3\(\)8** & **Avg:1\(\)8** \\  (1) & RS & 78.14 & 93.65 & 88.48 & 43.95 & 49.94 & 47.94 \\ (2) & Lever-LM & 84.32 & 96.52 & 92.45 & 48.75 & 52.59 & 51.31 \\  (3) & \(b=1\) & 79.91 & 94.53 & 89.66 & 46.47 & 51.13 & 49.58 \\ (4) & \(b=5\) & 84.32 & 96.52 & 92.45 & **48.75** & **52.59** & **51.31** \\ (5) & \(b=10\) & **84.96** & **97.12** & **93.06** & 48.58 & 52.49 & 51.19 \\  (6) & \(n=1000\) & 83.94 & 96.74 & 92.48 & 45.39 & 50.44 & 48.76 \\ (7) & \(n=3000\) & 84.13 & **97.60** & **93.11** & 47.56 & 51.11 & 49.93 \\ (8) & \(n=5000\) & **84.32** & 96.52 & 92.45 & **48.75** & **52.59** & **51.31** \\  (9) & Sim-I & 81.96 & 96.11 & 91.40 & 47.10 & 51.79 & 50.23 \\ (10) & Sim-T & 81.22 & 87.66 & 85.52 & 45.38 & 49.55 & 48.16 \\ (11) & Random & **84.32** & **96.52** & **92.45** & **48.75** & **52.59** & **51.31** \\  (12) & CIDEr Scorer & 87.93 & 93.52 & 91.65 & - & - & - \\ (13) & Lever-LM LSTM & 83.93 & 96.21 & 92.12 & 46.60 & 50.68 & 49.32 \\ (14) & Golden-1 & 81.78 & 97.44 & 92.22 & 47.78 & 52.51 & 50.93 \\ (15) & Golden-2 & 91.20 & 99.63 & 96.82 & 45.32 & 49.05 & 47.80 \\   

Table 2: Results of diverse ablation studies on IC and VQA.

    &  &  \\   & **Avg:1\(\)4** & **Avg:6\(\)8** & **Avg:1\(\)8** & **Avg:1\(\)4** & **Avg:6\(\)8** & **Avg:1\(\)8** \\  RS & 84.41 & 96.62 & 88.48 & 46.25 & 51.31 & 47.94 \\ SITR & 78.06 & 91.71 & 82.61 & 44.32 & 50.24 & 46.29 \\ SIIR & 85.16 & **99.40** & 89.91 & 47.83 & **53.41** & 49.69 \\ STTR & - & - & - & 47.08 & 49.77 & 47.98 \\ Lever-LM(4-shot \(_{}\)) & **87.35** & 97.96 & **90.88** & **48.56** & 52.68 & **49.93** \\  Lever-LM(2-shot \(_{}\)) & 89.53 & 98.30 & 92.45 & 50.39 & 53.15 & 51.31 \\   

Table 3: Results of Lever-LM with 4-shot \(_{}\) on IC and VQA.

**Golden ICD Sequence.** In experiments, we find that using a high learning rate and not freezing the CLIP model may make Lever-LM converge to a specific solution that for any query input, the ICD configuration is fixed while still return good ICL performance. For example, in IC, the best version, Golden-2, can outperform the non-Fixed case (Table 2 (2)) by 4.37 points in Avg:1\(\)8. Such improvement suggest that if we do not have enough computation burdens to configure diverse ICD sequence for each query, we can preserve one Golden ICD Sequence for the latter usage. However, we also find that the performance of Golden ICD Sequence fluctuates significantly, \(e\)._g_., Golden-2 is poorer than RS in VQA in Avg:1\(\)8. This also points out a new direction to study how to get more stable Golden ICD Sequence.

**Longer Few-shot \(_{}\).** We further explore the performance of Lever-LM using a 4-shot \(_{}\), as presented in Table 3. It is evident that Lever-LM continues to outperform other retrieval-based methods in Avg:1\(\)8 metric. However, we observe a notable performance reduction when Lever-LM is trained with the 4-shot \(_{}\) compared to the 2-shot \(_{}\). Specifically, for IC, there is a performance decrease of approximately 1.57 in the Avg:1\(\)8 metric when using the 4-shot \(_{}\) to train Lever-LM. One possible reason is that when constructing \(_{}\), some approximation operations are applied where sub-optimal ICD sequences are got. Then parts of statistic patterns may be salient is \(_{}\) that are more easily captured by Lever-LM where using longer ICD sequences may further encourage Lever-LM to capture such patterns, and thus causing less effective ICD sequences than the shorter \(_{}\). This points out a new direction to study how to build \(_{}\) with longer and more robust ICD sequences for better training Lever-LM.

**Random Order ICD sequence.** To validate that whether Lever-LM captures effective ICD orders, we randomly rearrange the ICD sequences generated by Lever-LM trained with 2-shot and 4-shot \(_{}\) and then evaluate the performance the 2-shot and 4-shot ICD configurations, respectively, in Table 4. It is evident that the original order of ICDs generated by Lever-LM attains the highest score in both VQA and IC, validating that Lever-LM can learn how to order the ICDs.

    & _{}\)} & _{}\)} \\   & VQA & IC & VQA & IC \\  Original & **50.83** & **88.63** & **51.12** & **85.97** \\ Random Order & 50.42 & 88.56 & 50.63 & 85.77 \\   

Table 4: Results of Random Order of Lever-LM generated ICDs.

Figure 3: Visualizations of diverse ICDs configurations, where the first and the last ICDs are given due to space limitation. We can find that Lever-LM use more diverse ICDs and thus not lead to short-cut inference.

Conclusion

After observing that configuring an ICD sequence is a mirror process of composing a sentence, we assume effective ICDs may contain statistic patterns that can be captured by temporal learner. Then we use a tiny LM named as Lever-LM to capture such patterns for configuring ICDs to lever LVLMs. To achieve this, we construct a dataset containing effective ICD sequences to train this Lever-LM. After training, we validate the effectiveness of Lever-LM by comparing it with similarity-based retrieval methods and find that Lever-LM can capture the statistic patterns between ICDs. Extensive ablations are deployed to discover which factors and why they will affect the results, which also pointing out a few future research directions.