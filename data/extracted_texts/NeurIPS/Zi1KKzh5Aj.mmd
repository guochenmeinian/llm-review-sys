# Collapsed Inference for Bayesian Deep Learning

Zhe Zeng

Computer Science Department

University of California, Los Angeles

zhezeng@cs.ucla.edu

&Guy Van den Broeck

Computer Science Department

University of California, Los Angeles

guyvdb@cs.ucla.edu

###### Abstract

Bayesian neural networks (BNNs) provide a formalism to quantify and calibrate uncertainty in deep learning. Current inference approaches for BNNs often resort to few-sample estimation for scalability, which can harm predictive performance, while its alternatives tend to be computationally prohibitively expensive. We tackle this challenge by revealing a previously unseen connection between inference on BNNs and _volume computation_ problems. With this observation, we introduce a novel collapsed inference scheme that performs Bayesian model averaging using _collapsed samples_. It improves over a Monte-Carlo sample by limiting sampling to a subset of the network weights while pairing it with some closed-form conditional distribution over the rest. A collapsed sample represents uncountably many models drawn from the approximate posterior and thus yields higher sample efficiency. Further, we show that the marginalization of a collapsed sample can be solved analytically and efficiently despite the non-linearity of neural networks by leveraging existing volume computation solvers. Our proposed use of collapsed samples achieves a balance between scalability and accuracy. On various regression and classification tasks, our collapsed Bayesian deep learning approach demonstrates significant improvements over existing methods and sets a new state of the art in terms of uncertainty estimation as well as predictive performance.

## 1 Introduction

Uncertainty estimation is crucial for decision making. Deep learning models, including those in safety-critical domains, tend to estimate uncertainty poorly. To overcome this issue, Bayesian deep learning obtains a posterior distribution over the model parameters hoping to improve predictions and provide reliable uncertainty estimates. Among Bayesian inference procedures with neural networks, Bayesian model averaging (BMA) is particularly compelling (Wasserman, 2000; Fragoso et al., 2018; Maddox et al., 2019). However, computing BMAs is distinctly challenging since it involves marginalizing over posterior parameters, which possess some unusual topological properties such as mode-connectivity (Izmailov et al., 2021). We show that even with simple low-dimensional approximate parameter posteriors as uniform distributions, doing BMA requires integrating over highly _non-convex_ and _multi-modal_ distributions with discontinuities arising from non-linear activations (cf. Figure 0(a)). Accurately approximating the BMA can achieve significant performance gains (Izmailov et al., 2021). Existing methods mainly focus on general-purpose MCMC, which can fail to converge, or provides inaccurate few-sample predictions (Kristiadi et al., 2022), because running longer sampling chains is computationally expensive, and variational approaches that typically use a mean-field approximation that ignores correlations induced by activations (Jospin et al., 2022).

In this work, we are interested in developing _collapsed samplers_, also known as _cutset_ or _Rao-Blackwellised_ samplers for BMA. A collapsed sampler improves over classical particle-based methods by limiting sampling to a subset of variables and further pairing each sample with a closed-formrepresentation of a conditional distribution over the rest whose marginalization is often tractable. Collapsed samplers are effective at variance reduction in graphical models (Koller and Friedman, 2009), however no collapsed samplers are known for Bayesian deep learning. We believe that this is due to the lack of a closed-form marginalization technique congruous with the non-linearity in deep neural networks. Our aim is to overcome this issue and improve BMA estimation by incorporating exact marginalization over (close approximate) conditional distributions into the inference scheme. Nevertheless, scalability and efficiency are guaranteed by the sampling part of our proposed algorithm.

Marginalization is made possible by our observation that BMA reduces to weighted volume computation. Certain classes of such problems can be solved exactly by so-called weighted model integration (WMI) solvers (Belle et al., 2015). By closely approximating BMA with WMI, these solvers can provide accurate approximations to marginalization in BMA (cf. Figure 0(b)). With this observation, we propose CIBER, a collapsed sampler that uses WMI for computing conditional distributions. In the few-sample setting, CIBER delivers more accurate uncertainty estimates than the gold-standard Hamiltonian Monte Carlo (HMC) method (cf. Figure 2). We further evaluate the effectiveness of CIBER on regression and classification benchmarks and show significant improvements over other Bayesian deep learning approaches in terms of both uncertainty estimation and accuracy.

## 2 Bayesian Model Averaging as Weighted Volume Computation

In **Bayesian Neural Networks (BNN)**, given a neural network \(f_{}\) parameterized by weights \(\), instead of doing inference with deterministic \(\) that optimize objectives such as cross-entropy or mean squared error, Bayesian learning infers a posterior \(p()\) over parameters \(\) after observing data \(\). During inference, this posterior distribution is then marginalized to produce final predictions. This process is called **Bayesian Model Averaging (BMA)**. It can be seen as learning an ensemble of an infinite number of neural nets and aggregating their results. Formally, given input \(\), the posterior predictive distribution and the expected prediction for a regression task are

\[p(y)= p(y,)\;p()\,d, _{p(y)}[y]= y\;p(y)\,dy.\] (1)

For classification, the (most likely) prediction is the class \(*{arg\,max}_{y}p(y)\). BMA is intuitively attractive because it can be risky to base inference on a single neural network model. The marginalization in BMA gets around this issue by averaging over models according to a Bayesian posterior.

BMA requires approximations to compute posterior predictive distributions and expected predictions, as the integrals in Equation 1 are intractable in general. Deriving efficient and accurate approximations remains an active research topic (Izmailov et al., 2021). We approach this problem by observing that the marginalization in BMA with ReLU neural networks can be cast as weighted volume computation (WVC). Later we show that it can be generalized to any neural network when combined with sampling. In WVC, various tools exist for solving certain WVC problem classes (Baloni et al.,

Figure 1: The integral surface of (a) the expected prediction in BMA, and (b) our proposed approximation. Both are highly non-convex and multi-modal. The z-axis is the weighted prediction \(y\;p(y,)\;p()\). Integration of (a) does not admit a closed-form solution, yet integration of (b) is a close approximation that can be solved exactly and efficiently by WMI solvers.

2014; Kolb et al., 2019; Zeng et al., 2020c). This section reveals the connection between BMA and WVC. It opens up a new perspective for developing BMA approximations by leveraging WVC tools.

**Definition 1** (Wvc).: _A weighted volume computation (WVC) problem is defined by a pair \((,)\) where a region \(\) is a conjunction of arithmetic constraints and weight \(:\) is an integrable function assigning weights to elements in \(\). The task of WVC is to compute the integral \(_{}()\,d\)._

### A Warm-Up Example

Consider a simple yet relevant setting where the predictive distribution \(p(y,)\) is a Dirac delta distribution with zero mass everywhere except at \(f_{}()\), such that \( y\;p(y,)\,dy=f_{}()\).

**Example 2**.: _Assume a model \(f_{}(x)=(w x)\) with a uniform posterior over the parameter: \(p(w)=\) with \(w[-3,3]\). Let the input be \(x=1\). For parameter \(w[-3,0]\), the model \(f_{}\) always predicts \(0\), and otherwise (i.e., \(w(0,3]\)), it predicts \(w\). Thus, the expected prediction (Equation 1) is \(_{p(y|)}[y]=_{}\,0 {6}\,dw+_{}\,w\,dw\). That is, a summation of two WVC problems \((_{},0)\) and \((_{},w/6)\) with \(_{}=(-3 w 0)\) and \(_{}=(0 w 3)\). The BMA integral decomposes into WVC problems with different weights due to the \(\) activation._

These WVC problems have easy closed-form solutions. This is no longer the case in the following.

**Example 3**.: _Assume a model \(f_{}(x)\) and posterior distribution \(p(w)\) as in Example 2. Let the predictive distribution \(p(y x,w)\) be a Gaussian distribution \(p_{}(y;f_{}(x),1)\) with mean \(f_{}(x)\) and variance 1. Given input \(x=1\), the expected prediction (Equation 1) is_

\[_{p(y|x=1)}[y]=_{_{}}y p_{ }(y 0,1)\,dy\,dw+_{_{}}y  p_{}(y w,1)\,dy\,dw.\]

_It is a summation of two WVC problems with \(_{}=(-3 w 0)(y)\) and \(_{}=(0 w 3)(y)\), whose joint integral surface is shown in Figure 0(a)._

These WVC problems do not admit closed-form solutions since they involve truncated Gaussian distributions. Moreover, Figure 0(a) shows that computing BMA, even in such a low-dimensional parameter space, requires integration over non-convex and multi-modal functions.

### General Reduction of BMA to WVC

Let model \(f_{}\) be a \(\) neural net. Denote the set of inputs to its \(\) activations by \(=\{r_{i}\}_{i=1}^{R}\), where each \(r_{i}\) is a linear combination of weights. For a given input \(\), the parameter space is partitioned by whether each \(\) activation outputs zero or not. This gives the WVC reduction

\[p(y)=_{\{0,1\}^{R}}_{_{}}p (y,)\;p()\,d,\]

where \(\) is a binary vector. The region \(_{}\) is defined as \(_{i=1}^{R}_{i}\) where arithmetic constraint \(_{i}\) is \(r_{i} 0\) if \(_{i}=1\) and \(r_{i} 0\) otherwise. The expected prediction \(_{p(y|)}[y]\) is analogous but includes an additional factor and variable of integration \(y\) in each WVC problem.

This general reduction, however, is undesirable since it amounts to a brute-force enumeration that implies a complexity exponential in the number of \(\) activations. Moreover, not all WVC

Figure 2: Uncertainty estimates for regression. The red line is the ground truth. The dark blue line shows the predictive mean. The shaded region is the \(90\%\) confidence interval of the predictive distribution. For the same number of samples, (b) CIBER is closer than (a) small-sample HMC to (c) a highly accurate but slow HMC with a large number of samples. See the Appendix for details.

problems resulting from this reduction are amenable to existing solvers. We will therefore appeal to a framework called weighted model integration (WMI) that allows for a compact representation of these WVC problems, and a characterization of their tractability for WMI solvers (Kolb et al., 2019). This inspires us to approximate BMA by first reducing it to WVC problems and further closely approximating those with tractable WMI problems.

## 3 Approximating BMA by WMI

WMI is a modeling and inference framework that supports integration in the presence of logical and arithmetic constraints (Belle et al., 2015, 2018). Various WMI solvers have been proposed in recent years (Kolb et al., 2019), ranging from general-purpose ones to others that assume some problem structures to gain scalability. However, even with the reduction from BMA to WVC from the previous section, WMI solvers are not directly applicable. Existing solvers have two main limitations: (i) feasible regions need to be defined by Boolean combinations of linear arithmetic constraints, and (ii) weight functions need to be polynomials. In this section, we show that these issues can be bypassed using a motivating example of how to form a close approximation to BMA using WMI.

In WMI, the feasible region is defined by _satisfiability modulo theories_ (SMT) constraints (Barrett et al., 2010): an SMT formula is a (typically quantifier-free) expression containing both propositional and theory literals connected with logical connectives; the theory literals are often restricted to _linear real arithmetic_, where literals are of the form \((^{T} b)\) with variable \(\) and constants \(^{T}\) and \(b\).

**Example 4**.: _The \(\) model \(f_{}(x)\) of Example 2 can be encoded as an SMT formula (see box). The curly bracket denotes logical conjunction, the symbol \(\) is a logical implication, variable \(W\) is the weight, and variable \(Z\) denotes the model output._

The encoding of \(\) neural networks into SMT formulas is explored in existing work to enable verification of the behavior of neural networks and provide formal guarantees (Katz et al., 2017; Huang et al., 2017; Sivaraman et al., 2020). We propose to use this encoding to define the feasible region of WMI problems. Let \(\) denote the satisfaction of an SMT formula \(\) by an assignment \(\), and \([]\) be its corresponding indicator function. We formally introduce WMI next.

**Definition 5**.: _(WMI) Let \(\) be a set of continuous random variables. A weighted model integration problem is a pair \(=(,)\), where \(\) is an SMT formula over \(\) and \(\) is a set of per-literal weights defined as \(=\{_{}\}_{}\), where \(\) is a set of SMT literals and each \(_{}\) is a function defined over variables in literal \(\). The task of weighted model integration is to compute_

\[(,)=_{}_{ }_{}()^{[]=]}\,d {x}.\]

That is, the task is to integrate over the weighted assignments of \(\) that satisfy the SMT formula \(\).1

An approximation to the BMA of Example 3 can be achieved with WMI using the following four steps:

**Step 1. Encoding model \(f_{}(x)\).** This has been shown as the SMT formula \(_{}\) in Example 4.

**Step 2. Encoding posterior distribution \(p(w)\).** The uniform distribution \(p(w)=\) with \(w[-3,3]\) can be encoded as a WMI problem pair \((_{pos},_{pos})\) as follows:

\[_{pos}=-3 W 3_{pos}=\{_{}(W)= =\}\]

**Step 3. Approximate encoding of predictive distribution \(p(y w,x)\).** Recall that \(p(y w,x)=p_{}(y;f_{}(x),1)\) is Gaussian, which cannot be handled by existing WMI solvers. To approximate it with polynomial densities, we simply use a triangular distribution encoded as a WMI problem pair:

\[_{pred}=\{Y Z+&_{ pred}=\{_{_{1}}(Y,Z)=&_{1}=Y Z\\ _{_{2}}(Y,Z)=&_{2}=Y<Z\}\\ .\]In this encoding, \(\) is a constant that defines the shape of the triangular distribution. It is obtained by minimizing the \(L2\) distance between a standard normal distribution and the symmetric triangular distribution. We visualize this approximation in the right figure.

**Step 4. Approximating BMA by calling WMI solvers.** With the above encodings, the predictive posterior \(p(y x)\) (Equation 1) can be computed using two calls to a WMI solver. For example, the uncertainty of a prediction \(y=1\) for input \(x=1\) is

\[p(y=1 x=1)\ =\ ((Y=1),)\ /\ (,)\ =\ 0.164\ /\ 1,\]

where \(=_{}_{pos}_{pred}\) and \(=_{pos}_{pred}\). Similarly, the expected prediction \(_{p(y|x=1)}[y]\) (Equation 1) can be computed using two calls to a WMI solver:

\[_{p(y|x=1)}[y]\ =\ (,^{*})\ /\ (,)\ =\ 0.752\ /\ 1,\]

where \(^{*}=\{_{}(Y)=Y\ \ =\}\). The above formulations also work for unnormalized distributions since the WMI in the denominator serves to compute the partition function.

We visualize the integral surface of the resulting approximate BMA problem in Figure 0(b). It is very close to the integral surface of the original BMA problem in Figure 0(a). However, it can be exactly integrated using existing WMI solvers while the original one does not admit such solutions. Next, we show how this process can be generalized to a scalable and accurate approximation of BMA.

## 4 CIBER: Collapsed Inference for Bayesian Deep Learning via WMI

Given a BNN with a large number of weights, naively approximating it by WMI problems can lead to computational issues, since it involves doing integration over polytopes in arbitrarily high dimensions and this is known to be #P-hard (Valiant, 1979; De Loera et al., 2012; Zeng et al., 2020c). Further, weights involved with non-ReLU activation might not be amenable to the WMI encoding. To tackle these issues, we propose to use collapsed samples to combine the strengths from two worlds: the scalability and flexibility from sampling and the accuracy from WMI solvers.

**Definition 6**.: _(Collapsed BMA) Let \((_{s},_{c})\) be a partition of parameters \(\). A collapsed sample is a tuple \((_{s},q)\), where \(_{s}\) is an assignment to the sampled parameters \(_{s}\) and \(q\) is a representation of the conditional posterior \(p(_{c}_{s},)\) over the collapsed parameter set \(_{c}\). Given collapsed samples \(\), collapsed BMA estimates the predictive posterior and expected prediction as_

\[ p(y)&|}_{(_{s},q)}[ p(y,)\ q(_{c})\,d_{c}],\ \\ _{p(y|)}[y]&|}_{(_{s},q)}[ y\ p(y, )\ q(_{c})\,d_{c}\ dy].\] (2)

The size of the collapsed set \(_{c}\) determines the trade-off between scalability and accuracy. The more parameters in the collapsed set, the more accurate the approximation to BMA is. The fewer parameters in \(_{c}\), the more efficient the computations of the integrals are since the integration is performed in a lower-dimensional space. Later in our experiments, we choose a subset of weights at the last or second-to-last hidden layer of the neural networks to be the collapsed set. This choice is known to be effective in capturing uncertainty as shown in Kristiadi et al. (2020); Snoek et al. (2015).

To develop an algorithm to compute collapsed BMA, we are faced with two main design choice questions: (**Q1**) how to sample \(_{s}\) from the posterior? (**Q2**) what should be the representation of the conditional posterior \(q\) such that the integrals in Equation 2 can be computed exactly? Next, we provide our answers to these two questions that together give our proposed algorithm CIBER.

### Approximation to Posteriors

For (**Q1**), we follow Maddox et al. (2019) and sample from the stochastic gradient descent (SGD) trajectory after convergence and use the information contained in SGD trajectories to efficiently approximate the posterior distribution over the parameters of the neural network, leveraging the interpretation of SGD as approximate Bayesian inference (Mandt et al., 2017; Chen et al., 2020).

Given a set of parameter samples \(\) from the SGD trajectory, the sample set is defined as \(_{s}=\{_{s}\}\). For each assignment \(_{s}\), an approximation \(q(_{c})\) to the conditional posterior \(p(_{c}_{s},)\) is necessary since the posteriors induced by SGD trajectories are implicit. Next, we discuss the choice of approximation to the conditional posterior that is amenable to WMI.

### Encoding into WMI Problems

As shown in Section 3, if a BNN can be encoded as a WMI problem, the posterior predictive distribution and the expected prediction, which involve marginalization over the parameter space, can be computed exactly using WMI solvers. This inspires us to use the WMI framework as the closed-form representation for the conditional posteriors of parameters. The main challenge is how to approximate the integrand in Equation 2 using an SMT formula and a polynomial weight function in order to obtain a WMI problem amenable to existing solvers.

_For the conditional posterior approximation \(q(_{c})\),_ we choose it to be a uniform distribution that can be encoded into a WMI problem as \(_{pos}=(_{pos},_{pos})\) with the SMT formula being \(_{pos}=_{i c}(l_{i} W_{i} u_{i})\) and weights being \(_{pos}=\{_{}(_{c})=1=\}\), where \(l_{i}\) and \(u_{i}\) are domain lower and upper bounds for the uniform distribution respectively. While seemingly over-simplistic, this choice of approximation to the conditional posterior is sufficient to robustly deliver surprisingly strong empirical performance as shown in Section 6. The intuition is that uniform distributions are better than a few samples. We further illustrate this point by comparing the predictive distributions of CIBER and HMC in a few-sample setting. Figure 2 shows that even with the same \(10\) samples drawn from the posterior distribution, since CIBER further approximates the \(10\) samples with a uniform distribution, it yields a predictive distribution closer to the ground truth than HMC, indicating that using a uniform distribution instead of a few samples forms a better approximation.

_For the choice of predictive distribution \(p(y,)\)_, we propose to use piecewise polynomial densities. Common predictive distributions can be approximated by polynomials up to arbitrary precision in theory by the Stone-Weierstrass theorem (De Branges, 1959). For regression, the de facto choice is Gaussian and we propose to use triangular distribution as the approximation, i.e., \(p(y,)=-}|y-f_{}()|\), with domain \(|y-f_{}()| r\), and \(r:=()}\) where the constant \(\) parameterizes the triangular distribution as described in Section 3. Here, \(^{2}()\) is the variance estimate, which can be a function of input \(\) depending on whether the BNN is homoscedastic or heteroscedastic. Then \(p(y,)\) can be encoded into WMI as:

\[_{pred}=\{Y-f_{}() r \\ Y-f_{}()-r._{pred}=\{ []{ll}_{_{1}}(Y,_{c})=-}()}{ r^{2}}_{1}=(Y>f_{}())\\ _{_{2}}(Y,_{c})=-}()-Y}{r^{2}} _{2}=(f_{}()>Y)\}\]

Similar piecewise polynomial approximations are adopted for classification tasks when the predictive distributions are induced by softmax functions. Those details are presented in the Appendix.

### Exact Integration in Collapsed BMA

By encoding the collapsed BMA into WMI problems, we are ready to answer **(Q2)**, i.e., how to perform exact computation of the integrals shown in Equation 2.

**Proposition 7**.: _Let the SMT formula \(=_{}_{pos}_{pred}\), and the set of weights \(=_{pos}_{pred}\) as defined in Section 4.2. Let the set of weights \(^{*}=\{_{}(Y)=Y=\}\). The integrals in collapsed BMA (Equation 2) can be computed by WMI solvers as_

\[ p(y,)\ q(_{c})\ d_{c} =((=y),)\ /(,),\] \[ y\ p(y,)\ q(_{c})\ d_{c}\ dy =(,^{*})\ /(,).\]

With both questions **(Q1)** and **(Q2)** answered, we summarize our proposed algorithm CIBER in Algorithm 1 in the Appendix. To quantitatively analyze how close the approximation delivered by CIBER is to the ground-truth BMA, we consider the following experiments with closed-form BMA.

**Regression.** We consider a Bayesian linear regression setting where exact sampling from the posterior distribution is available. Both the likelihood and the weight posterior are Gaussian such that the ground-truth posterior predictive distribution is Gaussian as well. With samples drawn from the weight posterior, CIBER approximates the samples with a uniform distribution as posterior \(p(|)\) and further approximates the likelihood with a triangular distribution such that the integral \(p(y|,)= p(y|,)p(|)\,d\) can be computed exactly by WMI.

We first evaluate the posterior predictive distribution estimated by CIBER and Monte Carlo (MC) method, using the same five samples drawn from the weight posterior. Results averaged over \(10\) trials are shown in Figure 3 where the estimations by CIBER are much closer to the ground truth posterior predictive distribution than those by the MC method. Further, the averaged KL divergence between the ground truth and CIBER is \(0.069\) while the one for MC estimations is \(0.130\), again indicating that CIBER yields a better BMA approximation in the few-sample setting.

We further explore the question of how many samples the MC method needs to match the performance of CIBER. The performances of both approaches are evaluated using KL divergence between the ground-truth posterior distribution and the estimated one, averaged over 10 trials. The result is shown in Figure 4 where the dashed green line shows the performance of CIBER with \(50\) samples and the blue curve shows the performance of MC with an increasing number of samples. As expected, the MC method yields lower KL divergence as the number of samples increases; however, it takes more than \(100\) samples to match CIBER, indicating its low sample efficiency and that developing efficient and effective inference algorithms such as CIBER for estimating BMA is a meaningful question.

**Classification.** For analyzing classification performance, Kristiadi et al. (2022) propose to compute the integral \(I=(f_{*})p_{}(f_{*})\,df_{*}\) with \(\) being the sigmoid function and \(f_{*}=f(^{*};)\) that amounts to the posterior predictive distribution. We consider a simple case with \(f(;)=\) such that the ground-truth integral can be obtained. With a randomly chosen \(\), the ground-truth integral is \(I=0.823\). The integral estimated by CIBER is \(I_{C}=0.826\) while the MC estimate is \(I_{MC}=0.732\). That is, CIBER gives an estimate with a much lower error than the MC estimation error, indicating that CIBER is able to deliver high-quality approximations in classification tasks.

## 5 Related Work

**Bayesian Deep Learning.** Bayesian inference over deep neural networks (MacKay, 1992) is proposed to fix the issue that deep learning models give poor uncertainty estimations and suffer from overconfidence (Nguyen et al., 2015; Hein et al., 2019; Meronen et al., 2023, 2021). Some methods use samples from SGD trajectories to approximate the implicit true posteriors similar to us: Izmailov et al. (2020) (SI) proposes to perform Bayesian inference in a subspace of the parameter space spanned by a few vectors derived from principal component analysis (PCA+ESS(SI)) or variational inference (PCA+VI(SI)); SWAG (Maddox et al., 2019) proposes to approximate the full parameter space using an approximate Gaussian posterior whose mean and covariance are from a partial SGD trajectory with a modified learning rate scheduler.

Some other approaches using approximate posteriors include MC Dropout (MCD) (Gal and Ghahramani, 2015, 2016) which is one of the Bayesian dropout methods and recently, one of its modifications called Variational Structured Dropout (VSD) (Nguyen et al., 2021) using variational inference is proposed. Other state-of-the-art approximate BNN inference methods including deterministic variational inference (DVI) (Wu et al., 2019), deep Gaussian processes (DGP) (Bui et al., 2016) with Gaussian process layers and variational inference (VI) (Kingma and Welling, 2013). Closely related to DGP is the deep kernel process (Aitchison et al., 2021) that writes DGPs as deep Wishart processes.

**WMI Solvers.** WMI generalizes weighted model counting (WMC) Sang et al. (2005), a state-of-the-art inference approach in many discrete probabilistic models, from discrete to mixed discrete-continuous domains Belle et al. (2015a,b). Recent research on WMI includes its tractability (Zeng et al., 2020c, 2021; Abboud et al., 2020) and the advancements in WMI solver development. Existing exact WMI solvers for arbitrarily structured problems include DPLL-based search with numerical Belle et al. (2015a); Morettin et al. (2017, 2019) or symbolic integration de Salvo Braz et al. (2016) and compilation-based algorithms Kolb et al. (2018); Zuidberg Dos Martires et al. (2019); Derkinderen et al. (2020) that use extended algebraic decision diagrams (XADDs) (Sanner et al., 2012) as a compilation target which is a powerful tool for inference on mixed domains (Sanner and Abbasnejad, 2012; Zamani et al., 2012). Some exact WMI solvers aiming to improve efficiency for a certain class of models are proposed such as SMI (Zeng and Van den Broeck, 2019) and MP-WMI (Zeng et al., 2020a) which are greatly scalable for WMI problems that satisfy certain structural constraints. Approximate solvers are also proposed including sampling-based ones (Zuidberg Dos Martires et al., 2020) and relaxation-based ones (Zeng et al., 2020c,b). Recent WMI efforts converge in the pywmi library (Kolb et al., 2019). The SMT formulas considered in this work can be seen as distributional constraints on continuous domains. There is also plenty of work in neuro-symbolic AI exploring the integration of discrete constraints into neural networks models including the architectures (Ahmed et al., 2022b, 2023) and the loss (Xu et al., 2018; Ahmed et al., 2022c,a).

## 6 Experiments

We conduct experimental evaluations of our proposed approach CIBER 1 on regression and classification benchmarks and compare its performance on uncertainty estimation as well as prediction accuracy with a wide range of baseline methods. More experimental details are presented in the Appendix.

### Regression on Small and Large UCI Datasets

We experiment on \(5\) small UCI datasets: _boston_, _concrete_, _yacht_, _naval_ and _energy_. We follow the setup of Izmailov et al. (2020) and use a fully connected network with a single hidden layer and \(50\) units with ReLU activations. We further experiment on \(6\) large UCI datasets: _elevators_, _keggdirected_, _keggundirected_, _pol_, _protein_ and _skillcraft_. We use a feedforward network with five hidden layers of sizes \(\) and ReLU activations on all datasets except _skillcraft_. For _skillcraft_, a smaller architecture is adopted with four hidden layers of size \(\). All models have two outputs for the prediction and the heteroscedastic variance respectively.

    & Boston & Concrete & Yacht & Naval & Energy \\  CIBER (second) & **-2.471 \(\) 0.140** & -2.975 \(\) 0.102 & -0.678 \(\) 0.301 & 7.276 \(\) 0.532 & **-0.716 \(\) 0.211** \\ CIBER (last) & **-2.471 \(\) 0.140** & **-2.959 \(\) 0.109** & -0.687 \(\) 0.301 & **7.482 \(\) 0.188** & **-0.716 \(\) 0.211** \\ SWAG & -2.761 \(\) 0.132 & -3.013 \(\) 0.086 & -0.404 \(\) 0.418 & 6.708 \(\) 0.105 & -1.679 \(\) 1.488 \\ PCA+ESS (SI) & -2.719 \(\) 0.132 & -3.007 \(\) 0.086 & **-0.225 \(\) 0.400** & 6.541 \(\) 0.095 & -1.563 \(\) 1.243 \\ PCA+VI (SI) & -2.716 \(\) 0.133 & -2.994 \(\) 0.095 & -0.396 \(\) 0.419 & 6.708 \(\) 0.105 & -1.715 \(\) 1.588 \\  SGD & -2.752 \(\) 0.132 & -3.178 \(\) 0.198 & -0.418 \(\) 0.426 & 6.567 \(\) 0.185 & -1.736 \(\) 1.613 \\ DVI & -2.410 \(\) 0.020 & -3.060 \(\) 0.010 & -0.470 \(\) 0.030 & 6.290 \(\) 0.040 & -1.010 \(\) 0.060 \\ DGP & -2.330 \(\) 0.060 & -3.130 \(\) 0.030 & -1.390 \(\) 0.140 & 3.600 \(\) 0.330 & -1.320 \(\) 0.030 \\ VI & -2.430 \(\) 0.030 & -3.040 \(\) 0.020 & -1.680 \(\) 0.040 & 5.870 \(\) 0.290 & -2.380 \(\) 0.020 \\ MCD & -2.400 \(\) 0.040 & -2.970 \(\) 0.020 & -1.380 \(\) 0.010 & 4.760 \(\) 0.010 & -1.720 \(\) 0.010 \\ VSD & -2.350 \(\) 0.050 & -2.970 \(\) 0.020 & -1.140 \(\) 0.020 & 4.830 \(\) 0.010 & -1.060 \(\) 0.010 \\   

Table 1: Average test log likelihood for the small UCI regression task.

[MISSING_PAGE_FAIL:9]

that reflects the quality of both uncertainty estimation and prediction accuracy, 2) classification accuracy (ACC), and 3) expected calibration errors (ECE) (Naeini et al., 2015) that show the difference between predictive confidence and accuracy and should be close to zero for a well-calibrated approach.

We run CIBER by choosing the collapsed parameter set to be \(10\) weights and \(100\) weights at the last layer of the neural network models for CIFAR-10 and CIFAR-100 respectively. The weights are chosen using the same heuristic as the one for regression tasks, i.e., to choose the weights whose samples from the SGD trajectories have large variances. We compare CIBER with strong baselines including SWAG (Maddox et al., 2019) reproduced by their open-source implementation, standard SGD, SWA (Izmailov et al., 2018), SGLD (Welling and Teh, 2011) and KFAC (Ritter et al., 2018).

**Transfer from CIFAR-10 to STL-10.** We further consider a transfer learning task using the model trained on CIFAR-10 to be evaluated on dataset STL-10 (Coates et al., 2011). STL-10 shares nine out of ten classes with the CIFAR-10 dataset but has a different image distribution. It is a common benchmark in transfer learning to adapt models trained on CIFAR-10 to STL-10.

**Results.** We present the test classification performance on dataset CIFAR-10 and CIFAR-100 in Table 3 and that of transfer learning in Table 4. The neural network models used in the classification task are VGG-16 networks and the models used in the transfer learning task are VGG-16 and PreResNet-\(164\). More results using different network architectures are presented in the Appendix. With the same number of samples as SWAG, CIBER outperforms SWAG and other baselines in most evaluations and delivers comparable performance otherwise, demonstrating the effectiveness of using collapsed samples in improving uncertainty estimation as well as classification performance.

## 7 Conclusions And Future Work

We reveal the connection between BMA, a way to perform Bayesian deep learning and WVC, which inspires us to approximate BMA using the framework of WMI. To further make this approximation scalable and flexible, we combine it with collapsed samples which gives our algorithm CIBER. CIBER compares favorably to Bayesian deep learning baselines on regression and classification tasks. A future direction would be to explore what other layers can be expressed as SMT formulas and thus amenable to SMT encoding. Also, the current WMI solvers are limited to polynomial weights, and thus the reduction to WMI problems is applicable to piecewise polynomial weights. This limitation might be alleviated in the future by the development of new WMI solvers that allow various weight function families.