# HLM-Cite: Hybrid Language Model Workflow for Text-based Scientific Citation Prediction

Qianyue Hao, Jingyang Fan, Fengli Xu1, Jian Yuan, Yong Li1

Department of Electronic Engineering, BNRist, Tsinghua University

Beijing, China

###### Abstract

Citation networks are critical infrastructures of modern science, serving as intricate webs of past literature and enabling researchers to navigate the knowledge production system. To mine information hiding in the link space of such networks, predicting which previous papers (candidates) will a new paper (query) cite is a critical problem that has long been studied. However, an important gap remains unaddressed: the roles of a paper's citations vary significantly, ranging from foundational knowledge basis to superficial contexts. Distinguishing these roles requires a deeper understanding of the logical relationships among papers, beyond simple edges in citation networks. The emergence of large language models (LLMs) with textual reasoning capabilities offers new possibilities for discerning these relationships, but there are two major challenges. First, in practice, a new paper may select its citations from gigantic existing papers, where the combined texts far exceed the context length of LLMs. Second, logical relationships between papers are often implicit, and directly prompting an LLM to predict citations may lead to results based primarily on surface-level textual similarities, rather than the deeper logical reasoning required. In this paper, we introduce the novel concept of core citation, which identifies the critical references that go beyond superficial mentions. Thereby, we elevate the citation prediction task from a simple binary classification to a more nuanced problem: distinguishing core citations from both superficial citations and non-citations. To address this, we propose **HLM-Cite**, a **H**ybrid **L**anguage **M**odel workflow for citation prediction, which combines embedding and generative LMs. We design a curriculum finetune procedure to adapt a pretrained text embedding model to coarsely retrieve high-likelihood core citations from vast candidate sets and then design an LLM agentic workflow to rank the retrieved papers through one-shot reasoning, revealing the implicit relationships among papers. With the two-stage pipeline, we can scale the candidate sets to 100K papers, vastly exceeding the size handled by existing methods. We evaluate HLM-Cite on a dataset across 19 scientific fields, demonstrating a 17.6% performance improvement comparing SOTA methods. Our code is open-source at https://github.com/tsinghua-fib-lab/H-LM for reproducibility.

## 1 Introduction

With the rapid development of modern science, the volume of research papers is increasing annually . As links between papers, citations network connects vast literature and bridge newly emerging knowledge with existing ones. Due to the critical role of citations, citation prediction is an important problem that has long been studied , where the goal is to predict whichpapers from a set of previous papers (candidate set) will an emerging new paper (query) cite. Accurate citation prediction can help reveal information hiding in link space of citation networks [2; 8], owning value in aiding citation-based computational social science studies regarding the patterns of paper publication and scientific innovation [9; 10; 11; 12; 13; 14]. On the other hand, citation prediction is of practical significance for assisting researchers in writing manuscripts, providing high-likelihood citation suggestions, and thereby saving massive literature searching time.

Despite abundant studies on citation prediction, there is a critical problem that remains unconsidered. While one paper typically cites multiple previous papers, the roles of citations vary significantly. The most important citations serve as research foundations of the query paper, assisting researchers in tracing the lineage of knowledge production. In contrast, some less relevant citations are only mentioned superficially in context. Existing works treat citation prediction as a simple binary classification problem and neglect such varying roles [2; 3; 4; 5; 6; 7], letting superficial citations distract attention from the important ones. However, such nuanced roles cannot be adequately reflected by simple edges in citation networks, but require understandings on the logical relationships among papers. In this paper, we aim to predict citations with various roles based on in-depth content understanding, where the emerging textual reasoning ability of LLMs provides a possible approach.

Predicting citations with LLMs faces two major challenges. (1) **Vast candidate sets.** The real-world scientific database consists of gigantic papers, and researchers need to retrieve possible citations from millions of previous papers. With limited context length, it is impractical to feed the vast candidates' contents into LLMs and expect reasoning on logical relationships among them. (2) **Implicit logical relationships.** The logical relationships among papers lie implicitly within the content of papers. Directly prompting an LLM to predict key-role citations for query papers is likely to get sunk into simple content similarity rather than reasoning actual logical relationships among papers.

In this work, we define the novel concept of core citation with inspiration from rich science of science research [9; 10; 12], depicting the varying roles of citations. We analyze 12M papers across 19 scientific fields and illustrate core citations' significantly closer relationships with the query papers. Based on this definition, we develop the task of citation prediction from simple binary classification between citations and non-citations into a more challenging but meaningful version, i.e., distinguishing core citations from superficial citations and non-citations. To solve this task on vast candidate sets, we propose integrating embedding and generative LMs as HLM-Cite, a two-stage hybrid language model workflow. We design a curriculum fine-tuning procedure to adapt a pretrained text embedding model to analyzing research papers, initially retrieving high-likelihood core citations from vast candidate sets in the first stage. Subsequently, we design an LLM agentic workflow, consisting of a Guider, an Analyzer, and a Decider, for the second stage. Guided by a one-shot example, the LLM agents analyze the papers' implicit logical relationships through textual reasoning and rank the retrieved papers by citation likelihood. In HLM-Cite, we incorporate the capability of both embedding and generative LMs, enabling precise extraction of core citations from tremendous candidate sets. We conduct extensive experiments on cross-field papers, and the results show a 17.6% performance improvement of our method compared to SOTA baselines. Also, experimental results prove that our workflow can scale up to 100K candidates, thousands of times more than existing works, owning the potential to cover an entire research domain for practical implementation.

In summary, the main contributions of this work include:

* We define the novel concept of core citation to depict the varying roles of citations. Thereby, we develop the citation prediction task from simple binary classification into distinguishing core citations, superficial ones, and non-citations, giving it more practical significance.
* We design a hybrid language models workflow to integrate the capabilities of embedding and generative LMs, where two categories of models form a two-stage pipeline that cascades retrieval and ranking to predict core citations. This design enables our method to handle very large candidate sets with high precision.
* We conduct extensive experiments on a cross-field dataset with up to 100K paper candidate sets. The results prove the scalability of our design and illustrate a 17.6% performance improvement comparing SOTA methods.

## 2 Problem Formulation

### Definition of Core Citation

We first provided some notations about paper citation relationships. Considering a set of papers \(G\), query paper \(q G\) cites a small subset of \(G\) including \(n_{q}\) previous papers, denoted as \(\{s_{q}^{1},...s_{q}^{n_{q}}\} S_{q} G\{q\}\), while the rest papers are not cited by \(q\), which we denote them as \(\{p_{q}^{1},...\} P_{q}=_{G\{q\}}S_{q}\). Also, \(m_{q}\) subsequent papers cite \(q\), denoted as \(\{f_{q}^{1},...f_{q}^{m_{q}}\} F_{q} G(S_{q}\{q\})\).

As we mentioned above, the roles of each element in \(S_{q}\) may vary significantly, where there exist \(k_{q}\) elements in \(S_{q}\) have major importance. We name them as core citations, denoted as \(\{_{q}^{1},..._{q}^{k_{q}}\}_{q} S _{q}\). Naturally, we name the rest of the citations, i.e., \(S_{q}_{q}\), as superficial citations. Enlighten by previous computational social science studies regarding citation networks [10; 12], following-up papers of \(q\), i.e., \(F_{q}\), are likely to also cite the critical foundations of \(q\), namely \(q\)'s core citations. On the other hand, less relevant citations of \(q\), such as some background knowledge, are typically not followed by \(F_{q}\). Therefore, we mathematically identify the core citations according to such local citation relationships (Figure 0(a)):

\[_{q}\{s_{q} S_{q} p F_{q},let\;q S_{p},s_{q} S_{p}\}.\] (1)

To verify the rationality of this definition, we draw statistics on 12M papers across 19 scientific fields in the Microsoft Academic Graph (MAG)  (See dataset details in Section 4.1). From the results in Figure 0(b) and c, we find that, with statistical significance, in both natural and social science domains, the query paper has more overlapped keywords with its core citations than its superficial citations, and the core citations are also more frequently mentioned in the main texts of query papers. This illustrates that the core citations identified from citation networks, are consistent with the important citations in the papers' content, proving feasibility of predicting core citations purely from the texts.

### Core Citation Prediction Task

Considering the difference between core citations and superficial citations, we focus on predicting the core citations, which are most meaningful links among literature for scientific research. We formally define the task of core citation prediction as follows.

**Definition 1** (Core Citation Prediction): _Given a query paper \(q\), and a candidate set \(C_{q}\), where \(|C_{q}|=t_{q}\). \(C_{q}\) includes \(t_{q}^{1}\) core citations and \(t_{q}^{2}\) superficial citations of \(q\), ensuing \(t_{q}^{1} k_{q},t_{q}^{2} n_{q}-k_{q}\) and \(t_{q}^{1}+t_{q}^{2} t_{q}\), and its rest elements, if any, are non-citations. The goal of core citation prediction is to pick out \(t_{q}^{1}\) elements from \(C_{q}\), maximizing the number of picked core citations._

In such a setting, superficial citations actually become hard negative samples against the core citations, adding to the challenges of the task.

In this paper, we focus on text-based citation prediction, where we only use citation networks to obtain the ground truth of core citations and do not include any network features other than the papers' textual content in the prediction. In this way, our model learns to extract the logical relationships

Figure 1: **(a)** Definition of core citation. **(b)** **(c)** Statistical difference between core citations and superficial citations. In all panels, 95% CI are shown as error bars.

purely from the texts, predicting which citations of \(q\) are likely to be valued by future papers citing \(q\) without requiring any information about the exact future citations, which have not happened yet. Therefore, although we construct the ground truth of core citations in training and testing sets with previously published papers where we already know the subsequent papers that cite them, i.e., \(F_{q}\), our models is feasible for ongoing manuscripts without \(F_{q}\).

## 3 Methods

### Overview

To effectively predict core citations from large-scale candidate sets, we integrate the capability of both embedding and generative LMs, forming a hybrid language models workflow (HLM-Cite). We illustrate designs of the workflow in Figure 2.

As shown in Figure 2a, the HLM-Cite workflow consists of two major modules, i.e., the retrieval module (Section 3.2) and the LLM agentic ranking module (Section 3.3). When given a query \(q\) and a candidate set \(C_{q}\) with the size of \(t_{q}\), we first call the retrieval module, a pretrained text embedding model finetuned with training data. We calculate the embedding vectors of \(q\) and each paper in \(C_{q}\), denoted as \(_{q}\) and \(_{q}=\{_{q}^{1},...,_{q}^{t_{q}}\}\), where we concatenate the title and abstract as inputs. Based on the inner products between \(_{q}\) and each vector in \(_{q}\), we retrieve \(r_{q}\) papers with the highest probability of being core citations of \(q\) from \(C_{q}\), forming the retrieval set \(R_{q}\). Subsequently, we employ LLM agents in the ranking module to collaboratively analyze the retrieved papers in \(R_{q}\) and rank them according to their likelihood of being core citations, improving accuracy. Finally, we take the top \(t_{q}^{1}\) papers as the prediction result.

### Retrieval Module

#### 3.2.1 Model Structure

Here, we introduce the structure of the text embedding model used in the retrieval module. We employ the GTE-base pretrain model , one of the top models on the Massive Text Embedding Benchmark (MTEB) leaderboard . Its 110M parameters are initialized from BERT  and trained with multi-stage contrastive learning tasks, embedding input text into a 768-dimensional dense vector. We freeze the lower 7 layers of the GTE-base model and only finetune parameters in the higher 5 layers, as shown in Figure 2b. As empirically proven in previous research , such design can reduce computational consumption while maintaining the transferability in finetuning.

Figure 2: Illustration of the proposed hybrid language model (HLM-Cite) workflow.

#### 3.2.2 Curriculum Finetuning

As mentioned above, superficial citations act as hard negatives, adding to the difficulty of distinguishing core citations. Therefore, instead of directly transferring the GTE-base model to pick core citations from superficial citations and non-citations, we designed a two-stage curriculum finetuning as Figure 1(b) to gradually adapt the general-corpus model to our specific task, from easy to hard.

In the first stage, we finetune the model via a classification task that only distinguishes the core citation from non-citations, excluding the interference of superficial citations, i.e., the hard negatives. We construct each training data with one query, one of its core citations, and numerous non-citations, and we use cross-entropy loss for classification error in this stage.

In the second stage, we fully consider the ranking task of distinguishing core citations, superficial citations, and non-citations. We include one query together with its multiple core citations, superficial citations, and non-citations in each training data, and we apply NeuralNDCG loss function, a differentiable approximation of NDCG , to measure the difference between the model output and the ground-truth ranking. In both stages, we use in-batch negative sampling  to obtain non-citations for each query to reduce the embedding cost.

### LLM Agentic Ranking Module

#### 3.3.1 Overall Procedure

To improve the accuracy of core citation prediction, we incorporate LLMs' textual reasoning capability to rectify the ranking of papers retrieved in the previous stage by core-citation likelihood. As we illustrate in Figure 1(c), the LLM agentic ranking module consists of three agents, the analyzer, the decider, and the guider, which are all driven by LLMs and collaborate via natural language communications. Given a query paper and its possible core citations retrieved from the candidate set, we first employ the analyzer to analyze the logical relationship between each individual paper in the retrieval set and the query paper. Then, we feed the analysis to the decider to obtain a revised ranking of their likelihood of becoming core citations, drawing final prediction results. In addition, we design a guider to enhance complex reasoning, where it produces a one-shot example under human supervision, assisting the analyzer and the decider via the chain of thought (CoT) method .

Also, we find that one useful technique in the LLM agentic ranking module is not to rank all retrieved candidates. Specifically, with the retrieval size of \(r_{q}\) and \(t_{q}^{1}\) core citations in the candidate set, we exempt the \((2t_{q}^{1}-r_{q})\) retrieved candidates with largest inner products from reranking, and then we rerank the remaining \(2(r_{q}-t_{q}^{1})\) retrieved candidates with the LLM agents and selected the top \((r_{q}-t_{q}^{1})\) ones, resulting in \(t_{q}^{1}\) selected candidates in total. For example, when retrieval size is 7, we keep top-3 candidate unchanged and only rank the latter 4 candidates; when retrieval size is 8, we keep top-2 candidate unchanged and only rank the latter 6 candidates; and so on. The intuition for this is that the top candidates retrieved by the text embedding model tend to be core citations more safely. Therefore, only adjusting the latter ones is a rational solution that reduces the text length inputted into the LLMs and thereby improves the accuracy. We provide the detailed prompts used for the agents in Appendix A.1.

#### 3.3.2 Design of LLM Agents

**Analyzer: from textual similarity to logical relationship.** Intuitively, predicting citations requires in-depth understandings of the logical relationships among the papers, rather than only focusing on the textual similarity between their titles and abstracts. Therefore, we design the analyzer to extract why the query paper cites each of the candidates. Since plentiful knowledge has been encoded in the LLM as an implicit knowledge base, the agent can perform such analysis without domain-specific finetuning [23; 24; 25].

**Decider: final ranking for core citation prediction.** Based on the obtained analysis of paper relationships, we employ the decider to generate the final ranking of core-citation likelihoods. Besides simple ranking results, we prompt the agent to output corresponding explanations alongside, improving the rationality of its results [26; 27].

**Guider: one-shot learning.** To provide one-shot example for the analyzer and decider, we first select one representative query paper and several candidates outside the test set. As shown in Figure 1(c), thecandidates of query paper about Transformer-XL  include papers about (1) Neural Probabilistic Model , (2) Transformers , and (3) BERT , where the ground truth ranking is 2-3-1. The guider goes through the analyze-decide procedure and produces a group of exemplary analysis and rectified ranking. We manually review and revise the obtained analysis and ranking texts, making sure they correctly reveal that (2) serves as the research foundation of the query, (3) discusses related recent advancements, while (1) only provides some historical contexts. Then we respectively feed the texts to the analyzer and decider via the chain of thought (CoT)  method, concatenate them at the beginning of the prompts. Here we only summarize the essence of guider's exemplary output due to limited space, and the full texts are available in Appendix A.6.

## 4 Experiments

### Dataset

We conduct experiments based on Microsoft Academic Graph (MAG) , which archives hundreds of millions of research papers across 19 major scientific domains, forming a huge citation network. We traverse the dataset and filter 12M papers with abundant core citations and superficial citations, from which we randomly sample 450,000 queries and subsequently sample 5 core citations and 5 superficial citations for each query. We randomly divide the sampled queries into 8:2 as training and testing sets. Categorizing the scientific domains into natural science (biology, chemistry, computer science, engineering, environmental science, geography, geology, materials science, mathematics, medicine, physics) and social science (art, business, economics, history, philosophy, political science, psychology, sociology), we show statistics of the dataset in Table 1. Please note that a natural science query paper may cite some papers from the social science domain and vice versa.

### Baselines

We mainly evaluate our methods against three categories of baselines: simple rule-based method, LMs specifically designed for scientific texts, and pretrained LMs for general-purpose tasks. In the first category, we mainly predict core citation based on the degree of keyword overlap, i.e., the more overlap the candidate paper's keywords have with the query paper, the more likely it is to be a core citation. The second category includes SciBERT , METAG , PATTON, SciPATTON , SPECTER [3; 32], SciNCL , and SciMut . SciBERT is pretrained on millions of research papers from Semantic Scholar with the same approaches as BERT; METAG learns to generate multiple embeddings for various kinds of patterns of citation network relationships; PATTON and SciPATTON are finetuned with network masked language modeling and masked node prediction tasks on citation networks from BERT and SciBERT respectively; SPECTER is continuously pretrained from SciBERT with a contrastive objective; SciNCL is an improvement of SPECTER by considering hard-to-learn negatives and positives in contrastive learning; and SciMult is multi-task contrastive learning framework, which focuses on finetuning models with common knowledge sharing across different scientific literature understanding tasks. The third category includes BERT , GTE [16; 34], OpenAI-embedding-ada-002, and OpenAI-embedding-3 2. BERT is pretrained with masked language modeling and next sentence prediction objectives on Wikipedia and BookCorpus; GTE is a series of top embedding models finetuned from BERT with multi-stage contrastive learning task; and the latter two are advanced universal embedding models proposed by OpenAI. We access these models from off-the-shelf pretrained parameters or API calls and include different scale versions of each model when available.

    &  &  &  \\    & Query & Candidate & Query & Candidate & \\  Natural science & 386655 & 3830273 & 48388 & 479596 & 4744912 \\ Social science & 13345 & 169727 & 1612 & 20404 & 205088 \\  Total & 400000 & 4000000 & 50000 & 500000 & 4950000 \\   

Table 1: Dataset statistics 

### Overall Performance

We conduct the curriculum finetuning of our retrieval module with the batch size of 512 and 96 respectively in two stages, and each train for 10 epochs. The training process takes approximately 12 hours on 8\(\)NVIDIA A100 80G GPUs in total. Then, we call OpenAI API to access GPT models for LLM agentic ranking, where we keep using GPT-4 as the guider but alternate two versions of GPTs for the analyzer and the decider. For more implementation details, please refer to Appendix A.2.

In evaluation, we set vast candidate sets with \(t_{q}=10K\) (\(t_{q}^{1}=t_{q}^{2}=5\)) for all models and set the retrieval size to be \(r_{q}=8\) in our workflow. We evaluate the performance via PREC@3/5 and NDCG@3/5, and show the results in Table 2. The results illustrate that our method significantly surpasses all the baselines across all scientific domains with all metrics, with an overall PREC@5 improvement up to 17.6%. We verify the statistical significance of the performance improvement in Appendix A.5.1. Mentioning that without loss of statistical significance, we only randomly test 10% of the testing set with GPT-4o due to API rate limits.

In order to verify the rationality of LLM agentic ranking process, we provide the summary of a representative testing sample. We show the query paper, which designs a DNA Nanorobot, and the retrieved candidates in Figure 3. It turns out that our analyzer correctly reveals that the two candidates with core-citation ground truth inform the key design or the query paper [36; 37]; the candidate with superficial-citation ground truth inspires some design details ; while the non-citation candidate only mentions some very broad context that is almost irrelevant . Based on the rational analysis, the decider correctly ranks the retrieval set and improves the precision. Please refer to Appendix A.7 to access the full texts of this case study.

### Ablation Studies

In order to verify the validity of our designs, we conduct ablation studies regarding both curriculum finetuning of the retrieval module and LLM agents design in the ranking module. We show the results in Table 3. In the former part, we respectively delete the first and second stages of the curriculum and calculate the metrics on the retrieval set. The performance drop in both ablations indicates that our

   &  &  &  \\  & PREC@3/5 & NDCG@3/5 & PREC@3/5 & NDCG@3/5 &  & NDCG@3/5 \\  Keywords overlap & 0.334 & 0.267 & 0.302 & 0.262 & 0.402 & 0.322 & 0.359 & 0.311 & 0.336 & 0.269 & 0.304 & 0.264 \\  SciBERT  & 0.053 & 0.046 & 0.056 & 0.050 & 0.083 & 0.069 & 0.087 & 0.076 & 0.054 & 0.046 & 0.057 & 0.051 \\ METAG  & 0.112 & 0.089 & 0.124 & 0.104 & 0.180 & 0.142 & 0.196 & 0.166 & 0.114 & 0.090 & 0.126 & 0.106 \\ PATTO  & 0.248 & 0.201 & 0.266 & 0.229 & 0.407 & 0.341 & 0.429 & 0.378 & 0.253 & 0.205 & 0.271 & 0.234 \\ SciAPUTION  & 0.444 & 0.368 & 0.470 & 0.410 & 0.529 & 0.448 & 0.548 & 0.487 & 0.447 & 0.371 & 0.472 & 0.413 \\ SPECTTER  & 0.542 & 0.457 & 0.567 & 0.502 & 0.620 & 0.537 & 0.641 & 0.579 & 0.545 & 0.460 & 0.570 & 0.504 \\ SciNCL  & 0.575 & 0.495 & 0.598 & 0.537 & 0.634 & 0.558 & 0.655 & 0.597 & 0.577 & 0.497 & 0.600 & 0.539 \\ SciNult-vanilla  & 0.568 & 0.483 & 0.591 & 0.527 & 0.623 & 0.547 & 0.644 & 0.586 & 0.569 & 0.485 & 0.593 & 0.529 \\ SciMult-MoE  & 0.578 & 0.493 & 0.601 & 0.537 & 0.637 & 0.558 & 0.658 & 0.598 & 0.579 & 0.496 & 0.603 & 0.539 \\ SPECTTER-2.0  & 0.600 & 0.512 & 0.625 & 0.558 & 0.654 & 0.579 & 0.674 & 0.617 & 0.602 & 0.515 & 0.627 & 0.560 \\  BERT-base  & 0.036 & 0.034 & 0.036 & 0.035 & 0.129 & 0.115 & 0.133 & 0.122 & 0.039 & 0.036 & 0.039 & 0.038 \\ BERT-large  & 0.025 & 0.027 & 0.024 & 0.026 & 0.055 & 0.062 & 0.051 & 0.057 & 0.026 & 0.029 & 0.025 & 0.027 \\ OpenAI-daa-002 & 0.623 & 0.534 & 0.646 & 0.579 & 0.671 & 0.590 & 0.692 & 0.631 & 0.624 & 0.536 & 0.648 & 0.581 \\ OpenAI-3 & 0.632 & 0.543 & 0.655 & 0.588 & 0.671 & 0.592 & 0.691 & 0.632 & 0.633 & 0.545 & 0.656 & 0.589 \\ GTE-base  & 0.638 & 0.555 & 0.659 & 0.596 & 0.669 & 0.596 & 0.688 & 0.633 & 0.639 & 0.556 & 0.659 & 0.597 \\ GTE-base-v1.5  & 0.637 & 0.549 & 0.660 & 0.593 & 0.670 & 0.591 & 0.692 & 0.631 & 0.638 & 0.551 & 0.661 & 0.594 \\ GTE-large  & 0.640 & 0.556 & 0.661 & 0.597 & 0.669 & 0.593 & 0.690 & 0.632 & 0.641 & 0.557 & 0.662 & 0.599 \\ GTE-large-v1.5  & 0.647 & 0.562 & 0.669 & 0.605 & 0.690 & 0.606 & 0.707 & 0.645 & 0.649 & 0.563 & 0.671 & 0.606 \\  H-KI (GPT3) & 0.725 & 0.644 & 0.734 & 0.677 & 0.743 & 0.661 & 0.751 & 0.693 & 0.725 & 0.644 & 0.735 & 0.677 \\ H-LM (GPT4o)* & **0.736** & **0.655** & **0.743** & **0.686** & **0.756** & **0.670** & **0.763** & **0.702** & **0.736** & **0.655** & **0.743** & **0.686** \\  

Table 2: Overall performance. Bold and underline indicate the best and second best performance.

Figure 3: Case study of the LLM agentic ranking module.

curriculum design does enable the adaption of the pretrained model from easy to hard, improving its transfer performance from general corpus to scientific documents. In the latter part, we respectively remove the analyzer and the guider. Specifically, without the analyzer, the decider directly ranks the retrieved candidates based on their raw titles and abstracts; without the guider, the analyzer and decider perform their tasks without the guidance of the one-shot example. It turns out that the absence of any agent leads to performance degradation, proving the essential role of each of them. We verify the statistical significance of the performance degradation in Appendix A.5.2.

### Analysis

In this section, we provide in-depth analysis of various key elements in the HLM-Cite workflow, enabling a better understanding of our design. Here, if there is no special explanation, we all employ GPT-3.5 as our analyzer and decider. Mentioning that due to API rate limits, we only test 10% of the testing set in this section.

#### 4.5.1 Effect of Candidate Size

To illustrate the advantage of our method on large-scale candidate sets, which are normal in real-world applications, we keep \(t_{q}^{1}=t_{q}^{2}=5\) consistent and change the number of non-citations to construct candidate sets with \(t_{q}=1K,10K\), and \(100K\). As shown in Figure 3(a), regardless of the candidate size, our method significantly surpasses all top baselines and even achieves higher relative performance improvement on larger candidate sets (up to 18.5% in \(t_{q}=100K\)). We provide results with other metrics in Appendix A.3, where the conclusion is consistent.

#### 4.5.2 Effect of Retrieval Size

In our hybrid workflow, retrieval size \(r_{q}\) is a key hyper-parameter that balances the work between the retrieval module and the LLM agentic ranking module. To explore the effect of \(r_{q}\), we alter it from 6 to 10 and show the performance together with LLM token consumption per query in Figure 3(b). The results indicate that when \(r_{q}\) increases, the performance increases at the cost of more token consumption. Larger \(r_{q}\) leads to a higher recall rate of core citations in the retrieval set, and thereby, LLM agents have the potential to pick out more core citations from the texts with increased length. However, when \(r_{q}\) is large enough, continuing to increase it leads to a performance drop while consuming even more tokens. We believe this is because too many retrieved candidates surpass the

Figure 4: Effect of candidate size and retrieval size. In all panels, 95% CI are shown as error bars.

    &  &  &  \\  & PREC@3/5 & NDCG@3/5 & PREC@3/5 & NDCG@3/5 & PREC@3/5 & NDCG@3/5 & NDCG@3/5 \\  Full curriculum & **0.683** & **0.598** & **0.705** & **0.641** & 0.704 & **0.623** & **0.724** & **0.663** & **0.684** & **0.598** & **0.706** & **0.641** \\ w/o Stage1 & 0.682 & 0.595 & 0.703 & 0.638 & **0.706** & 0.623 & 0.724 & 0.662 & 0.682 & 0.596 & 0.704 & 0.639 \\ w/o Stage2 & 0.666 & 0.587 & 0.686 & 0.626 & 0.685 & 0.614 & 0.705 & 0.650 & 0.667 & 0.588 & 0.687 & 0.627 \\  Full workflow & **0.725** & **0.644** & **0.734** & **0.677** & **0.743** & **0.661** & **0.751** & **0.693** & **0.725** & **0.644** & **0.735** & **0.677** \\ w/o Analyzer & 0.723 & 0.629 & 0.733 & 0.666 & 0.736 & 0.648 & 0.747 & 0.684 & 0.723 & 0.630 & 0.734 & 0.667 \\ w/o Guider & 0.686 & 0.594 & 0.707 & 0.638 & 0.702 & 0.618 & 0.723 & 0.660 & 0.686 & 0.595 & 0.708 & 0.639 \\ w/o Analyzer\&Guider & 0.659 & 0.580 & 0.688 & 0.626 & 0.686 & 0.608 & 0.712 & 0.651 & 0.660 & 0.581 & 0.689 & 0.627 \\   

Table 3: Ablation studies. Bold indicates the best performance.

reasoning ability of LLMs, leading to confused analysis and low-quality ranking. Generally observed from the results, the optimal value of \(r_{g}\) is supposed to be 8 and 7 for natural and social science, respectively. Results with other metrics in Appendix A.4 show consistent conclusion.

#### 4.5.3 Effect of One-shot Example

As studied in previous research , CoT enhances the performance of LLMs by demonstrating the logical structure of reasoning rather than providing specific knowledge content. Here, we investigate whether this is true in our hybrid workflow. We extend one-shot learning into a few-shot version. In this version, we produce an individual example for each scientific domain, where full texts are available in our GitHub repository. This provides more domain knowledge while maintaining an identical logical structure. The results in Table 4 show no significant performance difference between one-shot and few-shot learning, proving that what matters in CoT prompting is the logical structure of reasoning rather than specific domain knowledge.

#### 4.5.4 Effect of LLM Types

We explore the effect of substituting GPT-3.5 in our workflow with other open-source and lightweight LLMs. Here, we keep using GPT-4 as the guider to provide a high-quality one-shot example and change the analyzer and decider to various open-source LLMs 3. We explore using two versions of Llama3, one of the most famous open-source LLMs; two versions of Mistral, a mixture of experts (MoE) model; and ChatGLM2-6B, a Chinese-English bilingual model. We show the results in Table 5 and find that although larger LLMs perform slightly better, i.e., Llama3-70B wins Llama3-8B, and Mistral-8\(\)22B wins Mistral-8\(\)7B, these lightweight LLMs all perform significantly worse than GPT models. This highlights the importance of implicit knowledge in LLM's large-scale parameters, which is crucial for solving tasks like citation prediction that require strong professional knowledge.

## 5 Related Works

### Pretrained Language Models (PLMs)

Pretrained language models have long been studied and reached great success. Various small-scale embedding models have been trained via different objectives, such as masked token prediction [18; 41], contrastive learning [3; 42; 16], and permutation language modeling . These models require fewer computational resources and are especially suitable for a wide range of tasks on the large-scale corpus, including classification, clustering, retrieval , etc. On the other hand, generative large

    &  &  &  \\  & PREC@3/5 & NDCG@3/5 & PREC@3/5 & NDCG@3/5 & PREC@3/5 & NDCG@3/5 & NDCG@3/5 \\  One-shot & 0.713 & **0.635** & 0.726 & **0.669** & 0.727 & 0.645 & 0.741 & 0.682 & 0.713 & **0.635** & 0.727 & **0.670** \\ Few-shot & **0.720** & 0.633 & **0.731** & 0.668 & **0.731** & **0.649** & **0.744** & **0.684** & **0.720** & 0.633 & **0.732** & 0.669 \\   

Table 4: Comparison between one-shot and few-shot learning. Bold indicates the best performance.

    &  &  &  \\  & PREC@3/5 & NDCG@3/5 & PREC@3/5 & NDCG@3/5 & PREC@3/5 & NDCG@3/5 & PREC@3/5 & NDCG@3/5 \\  GPT-4o & **0.736** & **0.655** & **0.743** & **0.686** & **0.756** & **0.670** & **0.763** & **0.702** & **0.736** & **0.655** & **0.743** & **0.686** \\ GPT-4 & 0.721 & 0.649 & 0.732 & 0.680 & 0.723 & 0.660 & 0.738 & 0.690 & 0.721 & 0.649 & 0.732 & 0.680 \\ GPT-3.5 & 0.713 & 0.635 & 0.726 & 0.669 & 0.727 & 0.645 & 0.741 & 0.682 & 0.713 & 0.635 & 0.727 & 0.670 \\  Llama3-70B & 0.681 & 0.593 & 0.704 & 0.637 & 0.688 & 0.604 & 0.713 & 0.649 & 0.681 & 0.593 & 0.704 & 0.637 \\ Llama3-8B & 0.668 & 0.590 & 0.695 & 0.634 & 0.679 & 0.604 & 0.707 & 0.648 & 0.669 & 0.590 & 0.695 & 0.634 \\  Mistral-8*22B & 0.678 & 0.592 & 0.702 & 0.636 & 0.690 & 0.604 & 0.715 & 0.649 & 0.678 & 0.592 & 0.702 & 0.636 \\ Mistral-8*7B & 0.678 & 0.591 & 0.701 & 0.635 & 0.692 & 0.601 & 0.716 & 0.647 & 0.678 & 0.591 & 0.702 & 0.636 \\  ChatGLM2-6B & 0.671 & 0.585 & 0.697 & 0.631 & 0.673 & 0.589 & 0.703 & 0.637 & 0.671 & 0.585 & 0.697 & 0.631 \\   

Table 5: Comparison between different types of LLMs as agents. Bold indicates the best performance.

language models (LLMs) have developed unprecedentedly in recent years. Pretrained on vast corpus, LLMs exhibit strong few-shot  and zero-shot  learning ability, reaching superior performance on text analyzing [44; 45; 46], code generation [47; 48] and even solving math problems [49; 50]. However, most of the existing works lack the combination of these two categories of models. In this paper, we design the hybrid language workflow, incorporating small embedding models' advantage of efficient large-scale retrieval and generative LLMs' capability of textual reasoning.

### LLM Agents

Utilizing the strong reasoning capability and human-like behavior of LLMs, researchers have explored various applications based on agents driven by LLMs. First, LLM agents for decision-making reach success in sandbox games [51; 52], robot controlling , and navigation . Besides, a group of LLM agents can simulate daily social life , generate physical mobility behavior , and reveal macroeconomic mechanisms , providing insights for social science research. Closer to our task, role-fused LLM agents can collaboratively solve natural language processing tasks via analysis and discussions [45; 27; 58]. However, due to the limited context length in LLM reasoning, existing studies face difficulty handling tasks with extremely long texts, such as citation precision on vast candidate sets. In this paper, we incorporate generative LLMs with embedding models, enabling our hybrid workflow to work on very large candidate sets.

## 6 Conclusions

In this paper, we investigate the task of scientific citation prediction. We first define the novel concept of core citation and thereby evolve the conventional citation prediction task into a more meaningful version of distinguishing the core citations. Then, we propose a hybrid language model workflow that incorporates the capability of both embedding and generative LMs. Through extensive experiments and in-depth analysis, we verify the validity of our design and illustrate its superior performance in tasks with gigantic candidate sets. One major limitation of our method lies in LLMs' illusion problem. Despite average performance improvement, LLMs may output unfaithful analysis under certain circumstances and poison specific samples. Therefore, how to verify the output of LLM agents and improve the reliability of our hybrid workflow worth future studies.