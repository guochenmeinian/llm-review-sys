# Beyond Confidence: Reliable Models Should Also Consider Atypicality

Mert Yuksekgonul

Stanford University

merty@stanford.edu

&Linjun Zhang

Rutgers University

lz412@stat.rutgers.edu

&James Zou\({}^{}\)

Stanford University

jamesz@stanford.edu

&Carlos Ernesto Guestrin\({}^{}\)

Stanford University, CZ Biohub

guestrin@stanford.edu

###### Abstract

While most machine learning models can provide confidence in their predictions, confidence is insufficient to understand a prediction's reliability. For instance, the model may have a low confidence prediction if the input is not well-represented in the training dataset or if the input is inherently ambiguous. In this work, we investigate the relationship between how atypical (rare) a sample or a class is and the reliability of a model's predictions. We first demonstrate that atypicality is strongly related to miscalibration and accuracy. In particular, we empirically show that predictions for atypical inputs or atypical classes are more overconfident and have lower accuracy. Using these insights, we show incorporating atypicality improves uncertainty quantification and model performance for discriminative neural networks and large language models. In a case study, we show that using atypicality improves the performance of a skin lesion classifier across different skin tone groups without having access to the group attributes. Overall, _we propose that models should use not only confidence but also atypicality to improve uncertainty quantification and performance_. Our results demonstrate that simple post-hoc atypicality estimators can provide significant value.1

## 1 Introduction

_Typicality_ is an item's resemblance to other category members . For example, while a dove and a sparrow are typical birds, a penguin is an atypical bird. Many works from cognitive science (e.g., ) suggest that typicality plays a crucial role in category understanding. For instance, humans have been shown to learn, remember, and refer to typical items faster . Similarly, the representativeness heuristic is the tendency of humans to use the typicality of an event as a basis for decisions . This cognitive bias is effective for making swift decisions, but it can lead to poor judgments of uncertainty. For instance, the likelihood of typical events can be overestimated  or uncertainty judgments can be inferior for atypical events .

While it is hard to quantify the uncertainty of human judgments, machine learning models provide confidence in their predictions. However, confidence alone can be insufficient to understand the reliability of a prediction. For instance, a low-confidence prediction could arise from an ambiguity that is easily communicated, or due to the sample being underrepresented in the training distribution.

Similarly, a high-confidence prediction could be reliable or miscalibrated. Our main proposal is that _models should quantify not only the confidence but also the atypicality_ to understand the reliability of predictions or the coverage of the training distribution. However, many machine learning applications rely on pretrained models that solely provide confidence levels, devoid of any measure of atypicality.

**Contributions:** To support our position, we use a simple formalization of atypicality estimation. With the following studies, we show that by using simple atypicality estimators, we can:

**1. Understand Prediction Quality:** Calibration is a measure that assesses the alignment between predicted probabilities of a model and the true likelihoods of outcomes . Neural networks  or even logistic regression  can be miscalibrated out-of-the-box. Here, we argue that using atypicality can give insights into when a model's confidence is reliable. Through theoretical analysis and extensive experimentation, we demonstrate that atypicality results in lower-quality predictions. Specifically, _we show that predictions for atypical inputs and samples from atypical classes are more overconfident and have lower accuracy._

**2. Improve Calibration and Accuracy:**_Recalibration_ methods offer some mitigation to miscalibration  by adjusting a probabilistic model. We show that models need different adjustments according to the atypicality of inputs and classes, and atypicality is a key factor in recalibration. In light of these findings, we propose a simple method: _Atypicality-Aware Recalibration_. Our recalibration algorithm takes into account the atypicality of the inputs and classes and is simple to implement. We show that complementing recalibration methods with atypicality improves uncertainty quantification and the accuracy of predictors. Further, in a case study for skin lesion classification, we show that atypicality awareness can improve performance across different skin-tone subgroups without access to group annotations.

**3. Improve Prediction sets:** An alternative approach to quantify uncertainty is to provide prediction sets that contain the label with high probability . Here, we investigate existing methods with atypicality and show that prediction sets could underperform for atypical or low-confidence samples. By using atypicality, we demonstrate the potential for improving prediction sets.

Overall, we propose that **models should also consider atypicality, and we show simple- and easy-to-implement atypicality estimators can provide significant value**.

## 2 Interpreting Uncertainty with Atypicality

**Motivation:** In many machine learning applications, we have access to a model's confidence, which aims to quantify the likelihood that a prediction will be accurate. In classification, model output is a probability distribution over classes and confidence is the predicted probability of the top class, i.e. \(_{y}\ }(Y=y|X=x)\). In practical scenarios, confidence is the primary tool used to evaluate the reliability of a prediction where higher confidence is associated with better predictions. However, the uncertainty in confidence can stem from different sources that require different treatment .

Figure 1: **Atypicality in Uncertainty. Left:** We show examples from the ImageNet-R dataset with our atypicality framework. **Right:** We provide a conceptualization of the quadrants. Using atypicality, we can understand prediction quality (ยง3), improve predictions (ยง4), and prediction sets (ยง5).

Here, we call a prediction _reliable_ if it is high-confidence and well-calibrated. High confidence could be reliable or miscalibrated, and low confidence could be due to ambiguity or rare inputs. We propose that _atypicality_ provides a natural way to understand reliability when combined with confidence. A sample is called typical if it is well-represented in the previously observed samples, e.g., an image of a dog that is similar to other dogs in the training data. However, if the image is unlike any other seen during training, it is atypical. We argue that atypicality can help us interpret a prediction's reliability. Below we categorize samples and predictions according to atypicality and confidence in four quadrants (Figure 1).

**High-confidence and representative:** Reliable predictions often fall within the **Reliable Quadrant**, which includes _typical, high-confidence_ samples. These samples are well-represented in the training dataset (typical), thus we expect the high-confidence prediction to be reliable. For instance, the first image on the top left (Figure 1) is a typical golden retriever and the model makes a reliable prediction.

High-confidence yet far from the support: Having high-confidence does not always indicate reliability. If the sample does not have support in the training distribution, the confidence could be miscalibrated. Such samples lie in the Extrapolation Quadrant which contains _atypical, high-confidence_ samples. For instance, the second image in the top right of Figure 1 is a _toy_ hog and the model has not seen similar ones during training.

Low confidence due to ambiguity: In contrast, low confidence could also be reliable when it correctly reflects an ambiguity. Such samples are in the Ambiguous Quadrant that contains _typical, low-confidence_ samples. These are typical since they may represent multiple classes; yet, due to ambiguity, the model's confidence is low. For instance, the second image in the bottom left of Figure 1 can both be a hog and a comic book.

Low confidence and rare: For samples that are not well-represented in training data, we expect to have low-quality predictions. Untrustworthy Quadrant comprises _atypical, low-confidence_ samples that can include extremely rare subgroups, for which we expect miscalibration and lower accuracy. For example, the image in Figure 1 bottom right is an origami hog that was not seen in training.

These examples suggest that relying solely on confidence does not provide a complete understanding of the reliability of the predictions, and we can use atypicality to interpret and improve reliability.

**Formalizing Atypicality:** Atypicality here is defined with respect to the training distribution. Informally, an input or a class is atypical if it is not _well-represented_ in the training distribution. For instance, if there are no or limited similar examples to an input, it can be called atypical. Note that this notion is not restricted to being 'out-of-distribution' , since in-distribution groups could also be atypical or rare, and our goal is to perform reliably for the entire spectrum.

Formally, let \(X^{d}\) be the random variable denoting features and \(Y=\{1,2,...,C\}\) denote the class, where we focus on classification.

**Definition 2.1** (Input Atypicality).: We define the atypicality of the input \(x\) as2

\[a_{X}(x)=-_{y}(X=x|Y=y).\]

We use the logarithm of the class-conditional densities due to high dimensionality and density values being close to zero. Intuitively, for a dog image \(x\), if \((X=x|Y=)\) has a low value, we call \(x\) an atypical dog image. Overall, if \(a(x)\) is high, then we call \(x\) an atypical input. Specifically, if an input is not typical for any class, then it is atypical with respect to the training distribution. Similarly, we can also use marginal density, \((X=x)\), or distance3 to quantify atypicality.

Similarly, the notion of atypical (rare) classes is prevalent in imbalanced classification . Ensuring reliable performance for atypical classes can be safety-critical, e.g., for a rare presence of dangerous melanoma . We define class atypicality in the following:

**Definition 2.2** (Class Atypicality).: For a class \(y\), atypicality of a class is defined as

\[a_{Y}(y)=-(Y=y).@note{footnote}{When the meaning is unambiguous, we omit the subscript to denote $a(X)$ or $a(Y)$ for notational brevity.}\]

[MISSING_PAGE_FAIL:4]

Here, we aim to examine the relationship between model calibration and atypicality. Given any \(K>1\), we consider the quantiles of \(a(X)\), \(a_{1},a_{2},,a_{K+1}\) such that \((a(X)(a_{k},a_{k+1}])=1/K\) for \(k[K]\). For imbalanced classification problems, we compute the quantiles using the class atypicality. Specifically, we investigate the atypicality-conditional calibration error \([} a(X)(a_{k},a_{k+1}]]\), i.e., the expected calibration error of an input that falls within the atypicality quantile \(k\).

**Atypical Examples are Poorly Calibrated:** In Figure 1(a), we show the distribution of miscalibration where each bin within the grid contains the intersection of the corresponding confidence and atypicality quantiles. We observe that within the same confidence range, predictions for atypical points have lower accuracies and are more overconfident. In other words, predictions in the Extrapolation or Untrustworthy regions are more miscalibrated than the ones in the typical regions.

In Figure 1(b), we split inputs into quantiles according to atypicality and compute the ECE and Accuracy for each group. Results show a monotonic relationship between atypicality and ECE or Accuracy across the three settings. Specifically, we see that predictions for atypical inputs or samples from rare classes are more miscalibrated and have lower accuracy. For samples from rare classes, the model overpredicts the probabilities of the typical class, hence we have overconfidence and low accuracy. Appendix C.3, and SS4 present figures and tables for all model and dataset pairs.

### Theoretical Analysis: Characterizing Calibration Error with Atypicality

We characterize how calibration error varies with atypicality in a tractable model that is commonly used in machine learning theory . Our theoretical analysis further supports our empirical findings.

**Data Generative Model:** We consider the well-specified logistic model for binary classification with Gaussian data, where \(Y\{-1,1\}\) and the \((Y=1|X)\) is defined by the sigmoid function:

\[(Y=1 X)=(^{*},X), X N(0,I_{d}).\]

Where \(I_{d}\) denotes the \(d\)-dimensional identity matrix, \(^{*}\) is the ground truth coefficient vector, \((x)=1/(1+e^{-x})\), and we have \(i.i.d.\) observations \(\{(x_{i},y_{i})\}_{i=1}^{n}\) sampled from the above distribution.

**The Estimator:** We focus on studying the solution produced by minimizing the logistic loss

\[=_{}_{i=1}^{n}[(1+(^{ }x_{i}))-y_{i}^{}x_{i}].\]

For \(k\{-1,1\}\), \(}_{k}(x)\) is an estimator of \((y=k|x)\), with the form \(}_{k}(x)=^{}x}+1}\).

**Calibration:** We consider all \(x\) where \(_{1}(x)>1/2\), as \(_{1}(x) 1/2\) can be analyzed similarly by symmetry (see Appendix G). For \(u(1/2,1)\), the signed calibration error at a confidence level \(u\) is

\[u-(Y=1}_{1}(X)=u).\]

Figure 2: **Atypical Samples Have Low-Quality Predictions.****(a)** Here, samples are grouped according to the Input Atypicality (x-axis) and Confidence (y-axis), to the right meaning more atypical. Values show the difference between confidence and accuracy, lighter color indicates more overconfidence. Within the same confidence range, atypical groups have more miscalibration and are more overconfident. **(b,c,d)** Predictions for atypical samples are less accurate and more miscalibrated in balanced and imbalanced supervised classification and classification with LLMs.

We want to show that when \(X\) is atypical, i.e., when \(a(X):=\|X\|^{2}/2\) is larger5, the accuracy \((Y=1}_{1}(X)=u)\) would be generally smaller than the confidence \(u\) (over-confidence).

**Theorem 3.1**.: _Consider the data generative model and the learning setting above. For any \(K>1\), suppose we consider the quantiles of \(a(X)\), \(a_{1},a_{2},...,a_{K},a_{K+1}\) such that \((a(X)(a_{k},a_{k+1}])=1/K\) for \(k[K]\). We assume \(\|^{*}\| c_{0}\), and \(d/n=\), for some sufficiently small \(c_{0}\). Then, for sufficiently large \(n\), for \(k=2,,K\), we have_

\[_{u}_{1}(X)}[u-(Y=1 }_{1}(X)=u) a(X)(a_{k},a_{k+1}]]>\] \[_{u}_{1}(X)}[u-(Y=1 }_{1}(X)=u) a(X)(a_{k-1},a_{k}]] 0.\]

That is, the resulting classifier is over-confident, and the level of over-confidence becomes larger when the data is more atypical (with larger \(a(X)\)). Further, the gap becomes larger for smaller sample sizes \(n\). The proof of the theorem is in Appendix G.2 and builds on the results from .

## 4 Using Atypicality to Improve Recalibration

Here, we show how atypicality can complement and improve post-hoc calibration. In SS2, we observed that predictions for atypical inputs and samples from atypical classes are more overconfident with lower accuracy. We next show that taking input and class atypicality into account improves calibration.

### Parametric Recalibration: Different Groups Need Different Temperatures

Temperature scaling (TS), a single parameter variant of Platt Scaling , is a simple recalibration method that calibrates the model using a single parameter. The predictor is of the form

\[}_{}(Y|X)}(Y|X)/,\] (1)

where \(}(Y|X)\) is the model that takes an input and outputs scores/logits, and \(\) is the temperature parameter. In practice, \(\) is optimized using a calibration set to minimize a proper scoring rule  such as the cross-entropy loss.

To understand the behavior of TS with respect to atypicality, we separately perform TS on points grouped according to the atypicality quantiles. Let us denote the temperature fitted to the quantile covering \(a(X)(a_{k-1},a_{k}]\) by \(_{a_{k}}\). In Appendix Figure 10 we observe an increasing relationship between \(a_{k}\) and \(_{a_{k}}\). Different atypicality groups need different adjustments, and more atypical groups need larger temperatures. _This suggests that being atypicality-aware can improve calibration. While a single temperature value improves average calibration, it may hurt certain groups._

### Atypicality-Aware Recalibration

We showed that predictions are more reliable when the input is typical. However, predictions are less reliable for atypical inputs, and we may need further revision. An analogy can be drawn to decision-making literature where opinions of individuals are combined with geometric averaging weighted by their expertise . Analogously, we propose _Atypicality-Aware Recalibration (AAR)_ a method designed to address the reliability issues identified in dealing with atypical inputs:

\[}_{}(Y|X)=}(Y|X)^{(a(X))} (S_{Y})^{1-(a(X))}}{Z(X)},\] (2)

where \((a(X))\) is a function of input atypicality, \(S_{Y}\) is a tunable score for class \(Y\), \(Z(X)\) is the normalization term. Intuitively, when the input is typical, we trust the model confidence; otherwise, we use a score for the given class estimated from the calibration set. Note that this form simplifies to

\[}_{}(Y|X)(a(X))}(Y| X)+S_{Y},\] (3)where we subsume \((1-(a(X))\) into \((a(X))\). We give a simple interpretation of this form: the multiplicative term is an atypicality-dependent temperature, and the additive term is a class-dependent correction where \()}\) can be considered to induce a correction distribution over classes estimated from the calibration set.

Intuitively, when \((a(X))=0\), the output reduces to a fixed distribution over classes that was estimated using the calibration set. This distribution can be seen to induce a prior probability over classes, and \(\) controls the tradeoff between this prior and the model's predictive distribution. As the point becomes more typical, this distribution is closer to the model's predictive distribution. In Appendix Figure 11, we show how these values behave with class atypicality. We find that rare classes require larger positive corrections with larger \(S_{Y}\).

**Implementation Details:** Following TS, we minimize the cross-entropy loss on a calibration set. With the temperature-atypicality relationship observed in Figure 10 we choose to instantiate the multiplicative factor as a quadratic function, where \((a(X))=c_{2}a(X)^{2}+c_{1}a(X)+c_{0}\) and in total we have \(|\{S_{1},..,S_{||},c_{0},c_{1},c_{2}\}|=||+3\) interpretable parameters. Once the embeddings and logits are computed, AAR runs on a CPU in under 1 minute for all experimented settings.

Similar to our adaptive interpretation, a concurrent work, Adaptive Temperature Scaling (AdaTS) , uses temperature scaling where the temperature is parameterized by a Variational Autoencoder(VAE)  and a multi-layer perceptron on top of the VAE embeddings. In the below experiments, we give results with AdaTS as a baseline when applicable.

Figure 3: **Post-hoc Recalibration for Classification.****(a) Balanced Supervised Classification:** Atypicality-Aware Recalibration improves the calibration of models trained with balanced datasets, across atypicality groups. **(b) Imbalanced Supervised Classification:** Atypicality-Aware Recalibration improves both the calibration across groups and the overall accuracy of models trained with imbalanced datasets. **(c) Classification with LLMs:** Atypicality-Aware Recalibration improves both the calibration across groups and the overall accuracy of LLMs performing classification.

For **Balanced Supervised Classification**, in Figure 2(a), we observe that being atypicality aware improves recalibration across all groups. We perform comparably to AdaTS, where the temperature function has in the order of millions of parameters, whereas AAR has only \(||+3\) parameters.

In **Imbalanced Supervised Classification** (Figure 2(b)), our algorithm not only provides better calibration rates across all classes but also improves overall accuracy. Note that only our method can change accuracy (due to the additive term), and it performs better than other baselines in terms of ECE across all classes. Further, the second column shows using Progressive Balancing  in training, showing that our post-hoc method can complement methods that modify training procedures.

For **Classification with LLMs**, we add an LLM calibration baseline Content-Free Calibration (CF) . We cannot use AdaTS as the embeddings are not fixed in size. In Figure 2(c), we see AAR has better calibration and accuracy across the three datasets. Namely, by adjusting the LLM output using the LLM atypicality, we can adjust the probabilities to increase the prediction quality.

### Case Study: Fairness through Atypicality-Awareness

Machine learning models reportedly have performance disparity across subgroups  due to factors such as varying sample size or noise levels . For instance, skin lesion classifiers can exhibit performance disparity across different skin tones . Fitzpatrick17k  is a dataset of clinical images with Fitzpatrick skin tone annotations between 1-to-6, where a larger number means darker skin tones, and when annotators do not agree, it is labeled as 'Unknown'. We explore the classification problem with 9 classes indicating the malignancy and the type of skin condition, using a ResNet18/34 pretrained on ImageNet and finetuned on this task (See Appendix F).

When the goal is to improve performance across groups, one can use group annotations and optimize performance within each group . Here, we investigate how complementing recalibration with atypicality can improve prediction quality across all groups _without group annotations_. For comparison, we perform 3 recalibration methods: TS, AAR, and Skin-Tone Conditional TS which calibrates the model individually for each skin-tone group with TS. Since the skin-tone conditional calibration uses group attributes, ideally it should act as an oracle.

In Figure 4, we give the Accuracy and ECE analyses where AAR improves performance across all groups. For instance, the worst-group Accuracy (0.69) or ECE (0.072) with AAR is close to the best-group Accuracy (0.63) or ECE (0.062) with the other two methods. Overall, _our findings suggest that Atypicality-Awareness can complement fairness-enforcing methods, and improve performance even when the group annotations are unavailable_. We hypothesize that with AAR, we can perform better than using supervised group attributes since groups may not have sufficient sample size in the calibration set (131, 1950, 1509, 555 samples for Unknown, 1&2, 3&4, and 5&6 respectively), and we can leverage atypicality to offer some mitigation. Further investigating how to leverage atypicality to improve fairness and factors affecting performance disparities is a promising direction for future work .

## 5 Improving Prediction Sets with Atypicality

**Conformal Prediction** is a framework that assigns a calibrated prediction set to each instance. The goal is to find a function \(: 2^{}\) that returns a subset of the label space such that \(Y(X)\) with high probability. The framework aims to guarantee _marginal coverage_, i.e., \((Y(X)) 1-\), for a choice of \(\). We investigate two conformal calibration methods,

Figure 4: **Improving Group Performance through Atypicality-Awareness. Here we show that AAR improves the calibration and accuracy of models across different skin tone groups. With AAR, we can improve both the worst group performance and overall performance significantly without using group attributes. TS curve is less visible since it significantly overlaps with Skin Tone Conditional.**

Adaptive Prediction Sets (APS)  and Regularized APS (RAPS) . Let \((X)\) be the permutation of the label set that sorts \(}(Y=c|X)\), i.e. the predicted probabilities for each class \(c\) after TS. The prediction sets are produced by the function \((x)=\{y:s(x,y)\}\), and these methods fit the threshold \(\) for a choice of the scoring function. APS uses the cumulative sum of the predicted probabilities \(s(x,y)=_{j=1}^{c}}(Y=j|X)\), where \(y=_{c}(X)\). Intuitively, if the model was perfectly calibrated, we would have expected to have \(=1-\). Similarly, RAPS builds on the idea that tail probabilities are noisy and regularizes the number of samples in the prediction set.

Building on our ideas in the previous sections we implement Atypicality-Aware prediction sets, namely _AA-APS_ and _AA-RAPS_ in the following way: We first group points according to their confidence and atypicality quantiles. A group \(G\) here is defined using 4 thresholds, namely \(G=x:(l_{a}^{(G)}<q_{a}(x) h_{a}^{(G)})(l_{c}^{(G)}<q_{c}(x) h_{ c}^{(G)})\) where \(q_{a}(x)\) and \(q_{c}(x)\) denote the atypicality and confidence quantiles for the sample \(x\), \(l_{a}^{(G)}\) and \(h_{a}^{(G)}\) denote the atypicality lower and upper bounds for group \(G\), and \(l_{c}^{(G)}\) and \(h_{c}^{(G)}\) denote the confidence lower and upper bounds for group \(G\). Using a calibration set, these bounds are simply determined by the quantiles of confidence and atypicality statistics. Then, we fit separate thresholds \(_{G}\) for each group's prediction sets with APS or RAPS as subroutines. This allows us to have an adaptive threshold depending only on the atypicality and confidence of predictions.

In Figure 5, we provide the coverage plots for APS and RAPS in the first and third columns. Even though marginal coverage is satisfied, models do not satisfy conditional coverage for atypical inputs or low-confidence predictions. We observe that being Atypicality-Aware improves coverage across otherwise underperforming groups. Further, AA-APS has lower set sizes on average than APS (\(15.6\) vs \(21.3\)). While RAPS has a lower average set size than AA-RAPS (\(4.2\) vs \(9.1\)) AA-RAPS has smaller set sizes for high-confidence samples, whereas a larger set size for low-confidence samples where the coverage is not met for RAPS. In Appendix D.3, we provide the same analysis for ResNet18,50,152 at different coverage levels along with analyzing the performance in the Confidence and Atypicality dimensions individually. For instance in Figure 8, we observe that RAPS and APS do not satisfy coverage for high atypicality regions, even when averaged across different confidence levels.

## 6 Additional Related Work

**Uncertainty and Atypicality:** use density estimation to disentangle epistemic and aleatoric uncertainty. Following this, they show improvements in active learning and OOD detection . We note that our goal is not this disentanglement (e.g. Untrustworthy quadrant can have both aleatoric or epistemic uncertainty), or Ambiguity could be due to a lack of features or noise.  propose the related notion of distance awareness, and that it leads to better uncertainty quantification. They offer architecture and training modifications whereas we analyze existing models using our framework including imbalanced and LLM settings, and propose simple and post-hoc approaches. 'OOD'  or 'anomaly'  notions are tied to atypicality, yet our goal is not to make a binary distinction between 'in' or 'out'. We argue that in-distribution samples could also be atypical (e.g. rare groups), and the goal is to perform reliably in the entire spectrum. Other works with an atypicality notion include bounding calibration of groups by the

Figure 5: **Improving Conformal Calibration with Atypicality for ResNet50 on ImageNet. Here we show that atypicality awareness improves conformal calibration performance across different groups. Methods are fitted to satisfy \(95\%\) coverage. We observe that APS and RAPS do not satisfy conditional coverage for high atypicality regions or low confidence regions.**

excess risk , miscalibration under distribution shifts , uncertainty in Gaussian Processes , forgetting time for rare examples , the poor performance of groups with lower sample sizes , energy-based models improving calibration , relating perplexity to zero-shot classification performance for LLMs , grouping loss and local definitions of miscalibration , the relationship between active learning and atypicality , sample size as a factor for subgroup performance disparity .  provide insightful discussion around the nature of softmax confidence, and here we show that its reliability depends on the atypicality of the input. Our new findings include showing that predictions for atypical samples are more miscalibrated and overconfident, and atypicality awareness improves prediction quality. _Overall, while there are other relevant notions in the literature, our distinct goal is to show that post-hoc atypicality estimation and recalibration is a simple yet useful framework to understand and improve uncertainty quantification that complements existing methods._

**Recalibration and Conformal Prediction:** There is a rich literature on recalibration methods and prediction sets: TS , Platt Scaling , conformal calibration  among many.  make a relevant observation, showing that the coverage of conformal prediction is not equal across all groups. They propose group conformal calibration, which requires group labels whereas our proposal is unsupervised and does not depend on any attribute information. Concurrent work  explores AdaTS, where they train a separate VAE and MLP to produce an adaptive temperature. However, our parameterization of temperature has 3 parameters and is interpretable.

## 7 Conclusion

Atypicality offers a simple yet flexible framework to better understand and improve model reliability and uncertainty. We propose that pretrained models should be released not only with confidence but also with an atypicality estimator. While there are other relevant notions in the literature, our main goal is to show that atypicality can provide a unifying perspective to discuss uncertainty, understand individual data points, and improve fairness. Here we focus on classification problems; it would be interesting to extend atypicality to regression and generation settings. Furthermore, we would like to extend the theoretical analysis to more general settings, as our empirical results demonstrate that the observed phenomena hold more broadly.