# LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS

Zhiwen Fan\({}^{1}\), Kevin Wang\({}^{1}\), Kairun Wen\({}^{2}\), Zehao Zhu\({}^{1}\), Dejia Xu\({}^{1}\), Zhangyang Wang\({}^{1}\)

\({}^{1}\)The University of Texas at Austin \({}^{2}\)XMU

{zhiwenfan,kevinwang.1839,atlaswang}@utexas.edu

Project Website: https://lightgaussian.github.io

Z. Fan and K. Wang contributed equally; \({}^{}\) Z. Fan is the Project Lead

###### Abstract

Recent advances in real-time neural rendering using point-based techniques have enabled broader adoption of 3D representations. However, foundational approaches like 3D Gaussian Splatting impose substantial storage overhead, as Structure-from-Motion (SfM) points can grow to millions, often requiring gigabyte-level disk space for a single unbounded scene. This growth presents scalability challenges and hinders splatting efficiency. To address this, we introduce _LightGaussian_, a method for transforming 3D Gaussians into a more compact format. Inspired by Network Pruning, LightGaussian identifies Gaussians with minimal global significance on scene reconstruction, and applies a pruning and recovery process to reduce redundancy while preserving visual quality. Knowledge distillation and pseudo-view augmentation then transfer spherical harmonic coefficients to a lower degree, yielding compact representations. Gaussian Vector Quantization, based on each Gaussian's global significance, further lowers bitwidth with minimal accuracy loss. LightGaussian achieves an average **15\(\) compression rate** while boosting **FPS from 144 to 237** within the 3D-GS framework, enabling efficient complex scene representation on the Mip-NeRF 360 and Tank & Temple datasets. The proposed Gaussian pruning approach is also adaptable to other 3D representations (e.g., Scaffold-GS), demonstrating strong generalization capabilities.

## 1 Introduction

Novel view synthesis (NVS) aims to generate photo-realistic images of a 3D scene from unobserved viewpoints, given a set of calibrated multi-view images. This capability has widespread applications

Figure 1: **Compressibility and Speed**: _LightGaussian_ compacts 3D Gaussians, reducing storage from 782MB to 45MB and boosting FPS from 144 to 237, while maintaining visual quality.

in virtual reality , augmented reality , digital twins , and autonomous driving . Neural Radiance Fields (NeRFs) [5; 6; 7] have shown promise in 3D modeling from multi-view images by mapping 3D locations and view directions to view-dependent color and volumetric density, with pixel intensity rendered through volume rendering . However, NeRF and its variants face limitations in rendering speed, limiting their deployment in real-world scenarios. To address this, voxel-based representations [9; 10; 11; 12; 13], hash grids , and neural light fields  have been developed. Despite improvements, these methods often compromise between quality and speed.

Recent progress in point-based 3D Gaussian Splitting (3D-GS)  has enabled real-time rendering with photo-realistic quality for complex scenes. By representing the scene with explicit 3D Gaussians and using a splitting technique , 3D-GS balances speed and quality, making it suitable for large-scale scenarios like digital twins and autonomous driving. However, point-based methods incur high storage costs, as each point and its attributes require independent storage. Additionally, heuristic densification of sparse SfM points into dense Gaussians often results in overparameterization, leading to excessive storage and slower rendering speeds. For instance, a typical unbounded 360-degree scene in 3D-GS  may require over 1GB of storage (e.g., 1.4GB for the _Bicycle_ scene).

In this paper, we address **storage and rendering speed** issues by developing a compact representation that retains the original rendering quality. The heuristic densification process in 3D-GS results in significant redundancy. Our method, _LightGaussian_, reduces redundancy by targeting both Gaussian count (N) and feature dimension (F) through a comprehensive pipeline:

* To reduce Gaussian count (N), we propose a _Gaussian Pruning and Recovery_ step, identifying and removing Gaussians with minimal impact on visual quality, followed by recovery to ensure smooth adaptation.
* For compressing features (F), we introduce an _SH Distillation_ process to compact higher-degree spherical harmonic (SH) coefficients, supported by pseudo-view augmentation. Additionally, we employ _Vector Quantization_ (VQ) to adaptively select a codebook of Gaussian attributes (e.g, positions, scales, and rotations), reducing precision for less significant features and applying quantization-aware fine-tuning to maintain quality.

In summary, **LightGaussian** achieves substantial compression (e.g., from 782MB to 45MB) while maintaining visual fidelity (SSIM decrease of only 0.007 on Mip-NeRF 360). Rendering speed also improves, reaching over 200 FPS on complex scenes. Our Gaussian Pruning and Recovery method generalizes well, enhancing performance across different 3D Gaussian formats, such as Scaffold-GS.

## 2 Related Works

Efficient 3D Scene Representations.Neural radiance fields (NeRF)  use multi-layer perceptrons (MLPs) to represent scenes, setting new standards for view synthesis quality. However, NeRF models face challenges with slow inference speeds, limiting practical use. Efforts to address this have explored ray re-parameterizations [6; 7], explicit spatial data structures [18; 19; 20; 14; 9; 13], caching and distillation techniques [15; 22; 23; 24], and ray-based representations [25; 26]. Nevertheless, achieving real-time rendering remains challenging for NeRF methods, especially for large-scale scenes where multiple queries per pixel hinder performance.

Point-based representations, like Gaussians, have been explored in various applications, such as shape reconstruction , molecular modeling , and point-cloud replacement , as well as in shadow  and cloud rendering . Pulsar  demonstrated efficient sphere rasterization, while 3D Gaussian Splatting (3D-GS)  applies anisotropic Gaussians  with tile-based sorting to achieve real-time speed and quality comparable to MLP-based methods like Mip-NeRF 360 .

Despite its strengths, 3D-GS has high storage demands due to the extensive attributes stored with each Gaussian, often requiring gigabytes per scene and hindering rendering efficiency. Recent concurrent works address these issues by using region-based vector quantization , K-means codebooks , view-direction exclusion , or learned binary masks and grid-based neural networks instead of spherical harmonics (SHs)  to reduce model size.

Pruning and Quantization.Model pruning reduces neural network complexity by removing non-significant parameters, balancing performance and resource use. Unstructured  and structured pruning [39; 40] eliminate parameters at the weight level and neuron/channel levels, resulting in a smaller, more efficient architecture. Iterative magnitude pruning (IMP), where low-magnitude weights are progressively removed, has proven effective in methods like lottery ticket rewinding [41; 42].

Vector quantization (VQ)  compresses data by representing it with discrete codebook entries (tokens), using mean square error (MSE) to select the closest match in the codebook for each data vector. Prior studies [44; 45; 46] have shown that learning discrete, compact representations enhances visual understanding and model robustness. VQ has thus been widely applied in image synthesis , text-to-image generation , and novel view synthesis [18; 49; 50].

Knowledge Distillation.Knowledge Distillation (KD) [51; 52; 53; 15] trains a smaller student model by transferring knowledge from a larger teacher model . In 3D vision, KD has been applied to neural scene representations, leveraging view renderings to incorporate 2D priors. For example, DreamFusion  and NeuralLift-360  use pre-trained diffusion models for 3D generation, while models like DFF , NeRF-SOS , INS , and SA3D  distill 2D image feature extractors to 3D tasks. KD has also been central to compressing scene representation models [15; 24].

## 3 Methods

Overview.The LightGaussian framework is illustrated in Fig. 2. The 3D-GS model is trained on multi-view images, initialized from SfM point clouds, and expanded to millions of Gaussians to represent the scene comprehensively. Our pipeline then processes the 3D-GS model into a compact format using _Gaussian Prune and Recovery_ to reduce the number of Gaussians, _SH Distillation_ to eliminate redundant SHs while retaining key lighting information, and _Vector Quantization_ to store Gaussians with lower bit-width.

### Background: 3D Gaussian Splatting

3D Gaussian Splatting (3D-GS)  is an explicit point-based 3D scene representation, utilizing Gaussians with various attributes to model the scene. When representing a complex real-world scene, 3D-GS is initialized from a sparse point cloud generated by SfM, and _Gaussian Densification_ is applied to increase the Gaussian counts that are used for handling small-scale geometry insufficiently covered and over reconstruction. Formally, each Gaussian is characterized by a covariance matrix \(\) and a center point \(\), which is referred to as the mean value of the Gaussian:

\[()=e^{-^{T}^{-1}},= ^{T}^{T},\] (1)

where \(\) can be decomposed into a scaling matrix \(\) and a rotation matrix \(\).

The complex directional appearance is modeled by an additional property, Spherical Harmonics (SH), with \(n\) coefficients, \(\{_{i}^{3}|i=1,2,,n\}\) where \(n=D^{2}\) represents the number of coefficients of SH with degree \(D\). A higher degree \(D\) equips 3D-GS with a better capacity to model the view-dependent effect but causes a significantly heavier attribute load.

When rendering 2D images from the 3D Gaussians, the technique of splatting [62; 17] is employed for the Gaussians within the camera planes. With a viewing transform denoted as \(\) and the Jacobian

Figure 2: **Pipeline of LightGaussian.** 3D Gaussians are optimized from multi-view images and SfM points. LightGaussian calculates each Gaussianâ€™s global significance on training data, pruning those with the least significance. Next, SH coefficients are distilled into a compact format using synthesized pseudo-views. Finally, vector quantization, including codebook initialization and assignment, reduces model bandwidth.

of the affine approximation of the projective transformation represented by \(\), the covariance matrix \(^{}\) in camera coordinates can be computed as \(^{}=^{T}^{T}\). Specifically, for each pixel, the color and opacity of all the Gaussians are computed using the Gaussian's representation Eq. 1. The blending of \(N\) ordered points that overlap the pixel is given by the formula:

\[=_{i N}c_{i}_{i}_{j=1}^{i-1}(1-_{i}).\] (2)

Here, \(c_{i}\), \(_{i}\) represents the view-dependent color and opacity, calculated from a 2D Gaussian with covariance \(\) multiplied by an optimizable per-3D Gaussian's opacity. In summary, each Gaussian point is characterized by attributes including: position \(^{3}\), color defined by spherical harmonics coefficients \(^{(k+1)^{2}} 3\) (where \(k\) represents the degrees of freedom), opacity \(\), rotation factor \(^{4}\), and scaling factor \(^{3}\).

### Gaussian Pruning & Recovery

Gaussian densification , which clones and refines the initial SfM point cloud, enhances small-scale geometry and detailed scene appearance by improving coverage. While this approach significantly boosts reconstruction quality, it increases the number of Gaussians from thousands to _millions_ after optimization, resulting in substantial storage demands.Inspired by neural network pruning techniques  that remove less impactful neurons while preserving overall performance, we propose a tailored pruning strategy for 3D Gaussian Splatting to reduce over-parameterized points while maintaining accuracy. Identifying redundant yet recoverable Gaussians is essential to our approach. However, pruning Gaussians based on simple criteria (e.g., point opacity) risks degrading modeling performance, as it may eliminate essential scene details, as shown in Fig. 3.

Global Significance Calculation.Inspired by Eq. 2, we go beyond using Gaussian opacity alone to assess significance by evaluating 3D Gaussians within the view frustum, projecting them onto the camera viewpoint for rendering. The initial significance score of each Gaussian (denoted as \((_{j})\)) can then be quantified based on its contribution to each pixel (ray, \(r_{i}\)) across all training views using the criteria \(((_{j}),r_{i})\) whether they intersect or not. Consequently, we iterate over all training pixels to calculate the hit count of each Gaussian. The score is further refined by the adjusted 3D Gaussian's volume \((_{j})\) and 3D Gaussian's opacity \(_{j}\), as they all contribute for the rendering formula. The volume calculation equation of the \(j\)-th 3D Guassian is \((_{j})= abc\), where \(abc\) are the 3 dimensions of Scale (\(\)). We also consider the transmittance \(\) is calculated with one subtract all the 3D Gaussian's opacity the ray hit before the \(j\)-th 3D Gaussian, \(=_{i=1}^{j-1}\) (1- \(_{i}\)). Finally, the global significance score can be summarized as:

\[_{}=_{i=1}^{MHW}((_{j}),r_{i}) _{j}(_{j}),\] (3)

Figure 3: **Zero-shot Opacity-based Pruning**. A significant number of Gaussians exhibit small opacity values (top). Simply utilizing Gaussian opacity as an indicator for pruning the least important Gaussians results in the rendered image losing intricate details (bottom), with the PSNR dropping from 27.2 to 25.3. This has inspired us to find better criteria to measure global significance in terms of rendering quality. The accumulated Probability Density Function(PDF) is equal to 1.

where \(j\) is the Gaussian index, \(i\) denotes a pixel, and \(M\), \(H\), and \(W\) represent the number of training views, image height, and width, respectively. \(1\) is an indicator function that determines whether a Gaussian intersects a given ray. However, using Gaussian volume alone tends to overemphasize background Gaussians, leading to excessive pruning of Gaussians modeling fine geometry. Thus, we propose a more adaptive approach to measuring volume dimensions:

\[() =(})^{},\] (4) \[} =(()}{ }},1).\]

Here, the calculated Gaussian volume is firstly normalized by the 90% largest of all sorted Gaussians, clipping the range between 0 and 1, to avoid excessive floating Gaussians derived from vanilla 3D-GS. The \(\) is introduced to provide additional flexibility.

Gaussian Co-adaptation.We rank all Gaussians by their global significance scores to quantitatively guide pruning of the lower-ranked Gaussians. The remaining Gaussians are then jointly adapted by fine-tuning their attributes--without additional densification--to offset the minor loss from pruning. This adaptation is performed using photometric loss on the original training views, aligned with 3D-GS training for 5,000 iterations.

### Distilling into Compact SHs

In the uncompressed Gaussian Splat representation, a substantial 81.3 percent of the feature dimension is occupied by Spherical Harmonics (SH) coefficients, requiring (45+3) floating-point values per splat. Directly reducing the SH degree can save disk space but also diminishes surface "shininess" and affects specular reflections.

To balance model size with scene quality, we introduce a knowledge distillation approach. Knowledge is distilled from a teacher model with full-degree SHs to a student model with truncated, lower-degree SHs. Supervision is based on the difference in predicted pixel intensities between the two models, with images synthesized at camera positions by sampling around each training view according to a Gaussian distribution:

\[_{} =_{i=1}^{HW}\|_{}(r_{i}; [|])-_{}(r_{i};[|])\|_{2}^{2}.\] (5) \[_{} =_{}+(0,^{2}),\] (6)

where \(\) and \(\) denote rendering camera rotation and position, \(_{}\) and \(_{}\) represent the newly synthesized and training camera positions, respectively. \(\) denotes a Gaussian distribution with mean 0 and variance \(^{2}\), which is added to the original position to generate the new position.

### Gaussian Attributes Vector Quantization

Vector quantization (VQ) clusters voxels into compact codebooks, enabling high compression rates. However, quantizing Gaussian attributes in a point-based, inherently non-Euclidean representation poses notable challenges. Our empirical findings indicate that applying quantization across all elements--especially for attributes like position, rotation, and scale--results in significant accuracy losses and a marked reduction in precision when represented discretely.

We propose applying VQ to the Spherical Harmonics (SH) coefficients, based on the assumption that a subgroup of 3d Gaussians typically exhibits a similar appearance. Fundamentally, VQ segments the Gaussians \(=\{_{1},_{2},,_{N}\}\) (here we apply it on SH) to the K codes in the codebook \(=\{_{1},_{2},,_{K}\}\), where each \(_{j},_{k}^{d}\) and K \(\) N. \(d\) means SH dimension. We aim to strike a balance between rendering quality loss and compression rate by leveraging the pre-computed significance score from Eq. 3. Based on this, we apply VQ selectively on the least significant elements in the Spherical Harmonics (SHs). Specifically, we initialize \(\) via K-means, iteratively sample a batch of \(\), associates them to the closest codes by euclidean distance, and update each \(_{k}\) via moving average rule: \(_{k}=_{d}_{k}+(1-_{d}) 1/_{k}_{_{j} (_{k})}_{j}_{j}\), where \(_{k}=_{_{j}(_{k})} _{j}\) is the significance score (Eq. 3) which is assigned to the code vector \(_{k}\), \((_{k})\) is the set of Gaussians associated to the \(k\)-th code. \(_{d}=0.8\) represents the decay value, which is utilized to update the code vector using a moving average. We fine-tune the codebook for 5,000 iterations, while fixing the gaussian-to-codebook mapping. We disable additional clone/split operations and leverage photometric loss on the training views. To preserve essential attributes, including Spherical Harmonics with higher global significance scores, along with Gaussian position, shape, rotation, and opacity, we skip VQ for these elements and store them directly in float16 format.

## 4 Experiments

### Experimental Settings

Datasets and Metrics.We conduct comparisons using the scene-scale view synthesis dataset provided by Mip-NeRF360 , which comprises nine real-world large-scale scenes, including five unbounded outdoor and four indoor settings with complex backgrounds. In addition, we utilize the Tanks and Temples dataset , a comprehensive unbounded dataset, and select the same scenes as used in . Performance metrics on synthetic object-level datasets will be detailed in the supplementary materials. We report metrics including the peak signal-to-noise ratio (PSNR), structural similarity (SSIM), and perceptual similarity as measured by LPIPS .

Compared Baselines.We compare our approach with methods suited for large-scale scene modeling, including Plenoxel , Mip-NeRF360 , and 3D-GS . Additionally, we evaluate

    &  &  \\   & FPS & Size & PSNR & SSIM & LPIPS & FPS & Size & PSNR & SSIM & LPIPS \\  Plenoxels  & 67.9 & 21 GB & 23.08 & 0.626 & 0.463 & 11.2 & 2.33 GB & 21.08 & 0.719 & 0.379 \\ NGP-Reg  & 9.43 & 48MB & 25.59 & 0.699 & 0.331 & 14.4 & 48MB & 21.92 & 0.745 & 0.305 \\ Mip-NeRF360  & 0.06 & 8.6MB & 27.69 & 0.922 & 0.237 & 0.14 & 8.68MB & 22.22 & 0.759 & 0.27 \\ VQ-DVGO  & 4.65 & 63MB & 24.23 & 0.636 & 0.391 & & & & & \\ Compressed 3D-GS\({}^{*}\) & 125 & 28MB & 27.03 & 0.802 & 0.238 & 202 & 17MB & 23.54 & 0.338 & 0.189 \\ Compact 3D-GS\({}^{*}\) & 128 & 48MB & 27.08 & 0.798 & 0.247 & 185 & 39MB & 23.32 & 0.831 & 0.201 \\  3D-GS\({}^{*}\) & 134 & 73MB & 27.21 & 0.815 & 0.214 & 154 & 411MB & 23.14 & 0.841 & 0.183 \\ 3D-GS\({}^{*}\) & 144 & 78MB & 27.40 & 0.813 & 0.217 & 106 & 43MB & 23.66 & 0.845 & 0.178 \\ Ours & 237 & 45MB & 27.13 & 0.806 & 0.237 & 357 & 25MB & 23.44 & 0.832 & 0.202 \\   

Table 1: **Quantitative Comparisons in Real-world Large-scale Scenes:** Voxel-based methods [12; 14] exhibit insufficient capacity for representing large-scale scenes and are unable to achieve real-time performance. Mip-NeRF360  produces the highest visual quality, but requires over 16 seconds to render a single image. Our method strikes a balance among FPS, model size, and rendering quality, achieving the best balance among efficient representations. For fair metric and visual comparison, we re-trained 3D-GS and Compressed 3D-GS  using their original code and configurations on our platform (NVIDIA A6000 GPU), with results marked by \({}^{*}\).

Figure 4: **Visual Comparisons**. We compare LightGaussian with the vanilla 3D-GS , displaying a residual map between predictions and ground truth scaled from 0 to 127 to emphasize differences. LightGaussian retains most specular reflections (yellow boxes) in its compact format, with a slight change in lightness visible in the bottom white box. The residual maps illustrate discrepancies between rendered images and ground-truth RGB values, where darker areas indicate closer alignment. For a full dynamic viewpoint comparison, please see our supplementary video.

against efficient techniques like Instant-NGP , which uses a hash grid for storage efficiency, and VQ-DVGO , which extends DVGO  with voxel pruning and VQ for optimized representation.

Implementation Details.Our framework is implemented in PyTorch and integrates the differentiable Gaussian rasterization technique from 3D-GS . All performance evaluations are conducted on an A6000 GPU. In the Global Significance Calculation phase, we assign a power value of 0.1 in Eq. 4 and proceed to fine-tune the model for 5,000 steps during the Gaussian Co-adaptation process. For SH distillation, we downscale the 3-degree SHs to 2-degree, thereby reducing 21 elements for each Gaussian. This is further optimized by setting \(\) to 0.1 in the pseudo view synthesis stage. In the Gaussian VQ step, the codebook size is configured to 8192, selecting SHs with the least \(60\%\) significance score for the vector quantization (VQ ratio), to balance the trade-off between compression efficiency and fidelity.

   Model & Size\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  Baseline & 80.99MB & 31.48 & 0.917 & 0.231 \\ +FP16 & 36.51MB & 31.35 & 0.914 & 0.232 \\ + VQ All att. & 18.74MB & 23.11 & 0.731 & 0.378 \\ + VQ All att. \(\) GS & 19.74MB & 26.23 & 0.826 & 0.323 \\ + VQ SH. & 21.10MB & 30.68 & 0.907 & 0.244 \\ + VQ SH \(\) GS & 21.10MB & 31.16 & 0.911 & 0.235 \\  LightGaussian (w/ VQ Finetune) & 21.10MB & 31.40 & 0.916 & 0.232 \\   

Table 4: **Ablation Study: _Gaussian Attribute Vector Quantization_ (VQ).** Quantizing all attributes to FP16 achieves a smaller model, but applying VQ to all attributes degrades modeling accuracy. Using Global Significance (GS) to target less crucial Gaussians mitigates this loss. Some attributes (e.g., scale) are sensitive to VQ, so we limit VQ to the SH coefficients. By combining VQ with significance scores, LightGaussian achieves an effective balance between model size and quality.

   Exp\# & Model & FPS\(\) & Size\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\ 
 & Baseline (3D-GS ) & 156.21 & 365MB & 31.34 & 0.917 & 0.221 \\
 & + Gaussian Pruning & 308.10 & 123MB & 30.67 & 0.910 & 0.234 \\
 & + Co-adaptation & 304.64 & 123MB & 31.64 & 0.918 & 0.228 \\
 & + SH Compactness & 311.58 & 81MB & 30.32 & 0.904 & 0.238 \\
 & + photometric loss & 302.20 & 81MB & 31.42 & 0.916 & 0.234 \\
 & + Distillation + Pseudo-views & 302.89 & 81MB & 31.48 & 0.917 & 0.231 \\
 & + Codebook Quant. & 301.21 & 21MB & 31.09 & 0.918 & 0.236 \\
 & + VQ Finetune & 302.01 & 21MB & 31.40 & 0.916 & 0.232 \\ 
 & LightGaussian (Ours) & 302.01 & 21MB & 31.40 & 0.916 & 0.232 \\   

Table 2: Ablation studies on the _Gaussian Pruning & Recovery_, _SH Compactness_, and the _Vector Quantization_. Scene: **Room**. Zero-shot Gaussian pruning leads degraded rendering quality (#2), but Co-adaptation can recover most of the scene details (#3). Directly eliminating high-order SH negatively affects the quality (#4), while distillation with pseudo-view helps to mitigate the gap (#5, #6). Codebook quantization further reduces the required model size and bandwidth (#7, #8).

   Model & FPS\(\) & Size\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  Baseline & 156.21 & 365MB & 31.34 & 0.917 & 0.221 \\ Hit Count Only & 301.52 & 123MB & 28.16 & 0.886 & 0.261 \\ + Co-adaptation & 303.43 & 123MB & 30.13 & 0.912 & 0.238 \\ \(\) Opacity & 310.29 & 123MB & 30.27 & 0.909 & 0.239 \\ + Co-adaptation & 304.84 & 123MB & 31.60 & 0.916 & 0.231 \\ \(\) Opacity \(\) \(\)(Volume). & 312.30 & 123MB & 30.67 & 0.910 & 0.234 \\ + Co-adaptation & 304.64 & 123MB & 31.64 & 0.918 & 0.228 \\   

Table 3: Ablation study of the _Gaussian Pruning & Recovery_, by using different Gaussian attributes for computing its global significance score. By considering only the hit count of each Gaussian from training rays, the zero-shot pruning leads to inferior performance. Incorporating the opacity and volume drives us to a better criterion. The subsequent Gaussian Co-adaptation is used to recover most of the information loss from the pruning of redundant Gaussians.

### Experimental Results

Quantitative Results.We assess the performance of various methods for novel view synthesis, with quantitative metrics summarized in Tab. 1. This includes efficient voxel-based NeRFs like Plenoxel  and Instant-NGP , the compact MLP-based Mip-NeRF360 , vector-quantized NeRF , and 3D Gaussian Splatting .

On the Mip-NeRF360 dataset, MLP-based NeRF methods achieve competitive accuracy with compact representations but suffer from slow inference speeds (0.06 FPS), limiting their practicality. Voxel-based NeRFs improve rendering efficiency but still fall short of real-time performance; for example, Plenoxel requires 2.1GB for a single large-scale scene. In contrast, 3D-GS offers a good balance between quality and speed but demands substantial storage per scene.

Our method, LightGaussian, exceeds existing techniques with rendering speeds over 200 FPS, enabled by efficient rasterization that prunes insignificant Gaussians. This approach reduces 3D Gaussian model redundancy, cutting storage from 782MB to 45MB on Mip-NeRF360--a 15\(\) reduction. LightGaussian also outperforms 3D-GS on the Tank & Temple datasets, nearly doubling rendering speed and reducing storage from 380MB to 22MB.

Qualitative Results.We conducted a comparative analysis of rendering results between 3D-GS and LightGaussian, focusing on intricate details and background regions, as shown in Fig. 4. Both 3D-GS and LightGaussian exhibit comparable visual quality, even in challenging scenes with thin structures, showing that LightGaussian effectively removes redundancy while preserving reconstruction fidelity.

Generalization to Other Point-based RepresentationsWe further apply our Gaussian Pruning approach to Scaffold-GS , an advanced neural Gaussian-based 3D representation. In this method, we prune Gaussians that contribute minimally to scene reconstruction, reducing redundancy while preserving visual fidelity. Experiments on the MipNeRF360 dataset demonstrate that pruning 80% of neural Gaussians increases the rendering speed from 152 to 178 FPS, as shown in Tab. 5. These results underscore LightGaussian's potential as an effective optimization tool for other Gaussian-based representations.

### Ablation Studies

We performed ablation studies on each component of our method to evaluate their individual impacts. Results show that _Gaussian Pruning and Recovery_ effectively removes redundant Gaussians, retaining only those crucial for scene representation. _SH Distillation_ reduces the Spherical Harmonics degree, simplifying the lighting model with minimal quality loss. Furthermore, _Vector Quantization_ efficiently compresses the feature space, creating a more compact representation. Combined, these modules substantially improve the overall efficiency and performance of our framework.

Significance Criteria.To identify the optimal criteria for measuring each Gaussian's global significance, we evaluated key characteristics: Gaussian-ray interaction count, Gaussian opacity, and functional volume. Tab. 3 illustrates that integrating these three factors -- by weighting the hit count with opacity and Gaussian volume -- yields the highest rendering quality post zero-shot pruning. A quick Gaussian Co-adaptation (the last row) which optimizes the remaining Gaussians, can effectively recover the rendering accuracy to its pre-pruning level. The visual comparison of rasterized images, before and after pruning, alongside the depiction of pruned Gaussians, is presented in Fig. 5.

SH Distillation & Vector Quantization.Directly removing high-degree components in Spherical Harmonics (SHs) causes significant performance degradation compared to the full model (Exp #3

    & **Size \(\)** & **FPS** & **PSNR \(\)** & **SSIM \(\)** & **LPIPS \(\)** \\  Scaffold-GS & 173.60 & 152 & 27.96 & 0.8240 & 0.2075 \\ Scaffold-GS + LightGaussian & 112.56 & 178 & 27.78 & 0.8187 & 0.2197 \\   

Table 5: LightGaussian removes redundant neural Gaussians in Scaffold-GS, reducing model size and improving rendering speed. All experiments were rerun on our platform for fair comparison, with results averaged on the MipNeRF-360 dataset.

[MISSING_PAGE_EMPTY:9]

as _Gaussian Pruning & Recovery_, _SH Distilling_, and _VQ_. As depicted in Fig 6, we note a marked decline in rendering quality when the pruning ratio reaches 70%, with a more rapid deterioration as the VQ ratio approaches 65%. In Tab 6, we observe that 2-degree SH compactness reduces the SSIM from 0.927 to 0.926, while 1-degree SH compactness further reduces it to 0.923.

## 5 Conclusion, Limitations, and Broad Impact

We present LightGaussian, a novel framework that transforms heavy point-based representations into a compact format for efficient novel view synthesis. Designed for practical use, LightGaussian leverages 3D Gaussians to model large-scale scenes, effectively identifying and pruning the least significant Gaussians generated through densification. It achieves over 15\(\) data reduction, boosts FPS to over 200, and minimally impacts rendering quality. Exploring zero-shot compression across various 3D-GS-based frameworks remains a promising direction for future research.

Broadly, LightGaussian's compact representation has the potential to democratize high-quality 3D content for applications in VR, AR, and autonomous driving by reducing resource demands, enabling more accessible and scalable deployment across industries. While we do not foresee any significant risk from this specific technique, 3D reconstruction may infringe on personal privacy when applied in public spaces or with drone footage, contradicting our ethical intentions.