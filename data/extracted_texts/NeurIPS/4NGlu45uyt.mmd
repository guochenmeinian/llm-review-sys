# Aligning Embeddings and Geometric Random Graphs: Informational Results and Computational Approaches for the Procrustes-Wasserstein Problem

Aligning Embeddings and Geometric Random Graphs: Informational Results and Computational Approaches for the Procrustes-Wasserstein Problem

Mathieu Even

DI ENS, CRNS, PSL University, INRIA Paris &Luca Ganassali

Universite Paris-Saclay, LMO

Jakob Maier

D.I ENS, CRNS, PSL University, INRIA Paris &Laurent Massoulie

D.I ENS, CRNS, PSL University, INRIA, MSR-INRIA Joint Centre, Paris

###### Abstract

The Procrustes-Wasserstein problem consists in matching two high-dimensional point clouds in an unsupervised setting, and has many applications in natural language processing and computer vision. We consider a planted model with two datasets \(X,Y\) that consist of \(n\) datapoints in \(^{d}\), where \(Y\) is a noisy version of \(X\), up to an orthogonal transformation and a relabeling of the data points. This setting is related to the graph alignment problem in geometric models. In this work, we focus on the euclidean transport cost between the point clouds as a measure of performance for the alignment. We first establish information-theoretic results, in the high (\(d n\)) and low (\(d n\)) dimensional regimes. We then study computational aspects and propose the 'Ping-Pong algorithm', alternatively estimating the orthogonal transformation and the relabeling, initialized via a Franke-Wolfe convex relaxation. We give sufficient conditions for the method to retrieve the planted signal after one single step. We provide experimental results to compare the proposed approach with the state-of-the-art method of Grave et al. (2019).

## 1 Introduction

Finding an alignment between high dimensional vectors or across two point clouds of embeddings has been the focus of recent threads of research and has a variety of applications in computer vision, such as inferring scene geometry and camera motion from a stream of images (Tomasi and Kanade, 1992; Tomasi and Kanade, 1992), as well as in natural language processing such as automatic unsupervised translation (Rapp, 1995; Fung, 1995).

Many practical algorithms proposed for this task view this problem as minimizing the distance across distributions in \(^{d}\). Some approaches are based e.g. on optimal transport and Gromov-Wasserstein distance Alvarez-Melis and Jaakkola (2018) or adversarial learning (Zhang et al., 2017; Conneau et al., 2018). Another line of methods adapt the iterative closest points procedure (ICP) - originally introduced in Besl and McKay (1992) for 3-D shapes - to higher dimensions Hoshen and Wolf (2018). Another recent contribution is that of Grave et al. (2019), where a method is proposed to jointly learn an orthogonal transformation and an alignment between two point clouds by alternating the objectives in the corresponding minimization problem.

To formalize this problem, we consider a Gaussian model in which both datasets \(X,Y^{d n}\) (or two point clouds of \(n\) datapoints in \(^{d}\)) are sampled as follows. First, \(X=(x_{1},,x_{n})\) is a collection of i.i.d. \((0,I_{d})\) Gaussian vectors, and \(Y=(y_{1},,y_{n})\) is a noisy version of \(X=(x_{1},,x_{n})\), up to an orthogonal transformation \(Q^{}\) and a relabeling \(^{}:[n][n]\) of the data points, that is:

\[ i[n]\,, y_{i}=Q^{}x_{^{}(i)}+ z_{i}\,, \]

or, in matrix form:

\[Y=Q^{}X(P^{})^{}+ Z\,,\]

where \(Z=(z_{1},,z_{n})^{d n}\) is also made of i.i.d. \((0,I_{d})\) Gaussian vectors, \(P^{}\) is the permutation matrix associated with some permutation \(^{}\), and \(>0\) is the noise parameter. Recovering (in some sense that will be made precise in the sequel) the (unknown) permutation \(^{}\) and orthogonal transformation \(Q^{}\) defines the _Procrustes-Wasserstein problem_ (sometimes abbreviated as PW in the sequel), which will be the focus of this study.

The practical approaches previously mentioned have shown good empirical results and are often scalable to large datasets. However, they suffer from a lack of theoretical results to guarantee their performance or to exhibit regimes where they fail. Model (1) described here above appears to be the simplest one to obtain such guarantees. We are interested in pinning down the _fundamental_ limits of the Procrustes-Wasserstein problem, hence providing an ideal baseline for any computational method to be compared to, before delving into computational aspects. Our contributions are as follows:

* We define a planted model for the Procrustes-Wasserstein problem and discuss the appropriate choice of metrics to measure the performance of any estimator. Based on these metrics, we establish1: * We study computational aspects and propose the 'Ping-Pong algorithm', alternatively estimating the orthogonal transformation and the relabeling, initialized via a Franke-Wolfe convex relaxation. This method is quite close to that proposed in Grave et al. (2019) although the alternating part differs. We give sufficient conditions for the method to retrieve the planted signal after one single step.
* Finally, we provide experimental results to compare the proposed approach with the state-of-the-art method of Grave et al. (2019).

### Discussion and related work

One can check that under the above model (1), the maximum likelihood (ML) estimators of \((P^{},Q^{})\) given \((X,Y)\) is given by:

\[(,)*{arg\,min}_{(P,Q)_{n} (d)}\|XP^{}-Q^{}Y\|_{F}^{2}=*{arg \,min}_{(P,Q)_{n}(d)}\|QX-YP\|_{F}^{2}\,, \]

which is strictly equivalent2 to the formulation of the non-planted problem of Grave et al. (2019). Exactly solving the joint optimization problem (2) is non convex and difficult in general. However, if \(P^{}\) is known then (2) boils down to the following _orthogonal Procrustes problem_:

\[*{arg\,min}_{Q(d)}\|XP^{ }-Q^{}Y\|_{F}^{2}\,, \]

which has a simple closed form solution given by \(=UV^{}\) where \(USV^{}\) is the singular value decomposition (SVD) of \(Y(XP^{})^{}\) (see Schonemann (1966)). Conversely, when \(Q^{}\) is known, (2) amounts to the following _linear assignment problem_ (LAP in the sequel):

\[*{arg\,min}_{P_{n}}\|XP^{}-Q^{} Y\|_{F}^{2}=*{arg\,max}_{_{n}}  XP^{},Q^{}Y, \]which can be solved in polynomial time, e.g. in cubic time by the celebrated Hungarian algorithm (Kuhn, 1955), or more efficiently at the price of regularizing the objective and using the celebrated Sinkhorn algorithm (Cuturi, 2013).

_Previous results when \(Q^{}\) is known._ As seen above, when \(Q^{}\) is known (assume e.g. \(Q^{}=I_{d}\)), the Procrustes-Wasserstein problem reduces to a simpler objective, that of aligning Gaussian databases. This problem has been studied by Dai et al. (2019, 2023) in the context of feature matching. Kunisky and Niles-Weed (2022) study the same problem as a geometric extension of planted matching and establish state-of-the-art statistical bounds in the Gaussian model in the low-dimensional (\(d n\)), logarithmic (\(d a n\)) and high-dimensional (\(d n\)) regimes. In particular, they show that exact recovery is feasible in the logarithmic regime \(d a n\) if \(^{2}<-1}\), and in the high-dimensional regime if \(^{2}<(1/4-)\). Note that in this problem, there is no computational/statistical gap since the LAP is always solvable in polynomial time.

_Geometric graph alignment._ Strongly connected to the Procrustes-Wasserstein problem is the topic of graph alignment where the instances come from a geometric model. Wang et al. (2022) investigate this problem for complete weighted graphs. In their setting, given a permutation \(^{*}\) on \([n]\) and \(n\) i.i.d. pairs of correlated Gaussian vectors \((X_{^{*}(i)},Y_{i})\) in \(^{d}\) with noise parameter \(\), they observe matrices \(A=X^{}X\) and \(B=Y^{}Y\) (i.e all inner products \( X_{i},X_{j}\) and \( Y_{i},Y_{j}\)) and are interested in recovering the hidden vertex correspondence \(^{*}\). The maximum likelihood estimator in this setting writes

\[*{arg\,min}_{P_{n}}\|X^{}XP-PY^{ }Y\|_{F}^{2}=*{arg\,max}_{P_{n}} P ^{}X^{}XP,Y^{}Y\,, \]

which is an instance of the _quadratic assignment problem_ (QAP in the sequel), known to be NP-hard in general, as well as some of its approximations (Makarychev et al., 2014). In fact, we have the following informal equivalence (see Appendix A for a proof):

**Lemma 1** (Informal).: _PW and geometric graph alignment are equivalent, that is, one knows how to (approximately) solve the former iff they know how to (approximately) solve the latter._

Wang et al. (2022) focus on the low-dimensional regime \(d=o( n)\), where geometry plays the most important role (see Remark 1). They prove that exact (resp. almost exact) recovery of \(^{*}\) is information-theoretically possible as soon as \(=o(n^{-2/d})\) (resp. \(=o(n^{-1/d})\)). They conduct numerical experiments which suggest good performance of the celebrated Umeyama algorithm (Umeyama, 1988), which is confirmed by a follow-up work by Gong and Li (2024) analyzing the Umeyama algorithm (which is polynomial time in the low dimensional regime \(d=o( n)\)) in the same setting and shows that it achieves exact (resp. almost exact) recovery of \(^{*}\) if \(=o(d^{-3}n^{-2/d})\) (resp. \(=o(d^{-3}n^{-1/d})\)), hence coinciding with the information thresholds up to a \((d)\) factor. However, their algorithm is of time complexity at least \((2^{d}n^{3})\), which is not polynomial in \(d\). This is why we do not include this method in our baselines.

We emphasize that our results clearly depart from those obtained in Wang et al. (2022) and Gong and Li (2024), because \((i)\) we are also interested in the high dimensional case \(d n\), and \((ii)\) we work with a different performance metric which provides less stringent conditions for the recovery to be feasible, see Section 1.2. A summary of previous informational results together with ours (see also Section 2) is given in Table 1.

_On the orthogonal transformation \(Q^{}\)._ Generalizing the standard linear assignment problem, our model described above in (1) introduces an additional orthogonal transformation \(Q^{}\) across the datasets. This orthogonal transformation can be motivated in the context of aligning embeddings in a

   Reference & Setting & Metrics & Regime & Condition \\  Kunisky and & & & \(d n\) & \( n^{-2/d}\) for \(,^{*})=0\) \\ Niles-Weed & =I_{d}\)} & \) for \(P^{}\)} &  &  & -1)^{-1/2}\) for \(,^{*})=0\)} \\
 & & & & \(<((2e^{1/a}-1)^{2}-1)^{-1/2}\) for \(,^{*})=o(1)\) \\  & & & \(d n\) & \(<(1/2-)(d/ n)^{1/2}\) for \(,^{*})=0\) \\  Wang et al. (2022) & X,B=Y^{}Y\)} & \) for \(P^{}\)} &  & \) for \(,^{*})=0\)} \\
 & & & & \( n^{-1/d}\) for \(,^{*})=o(1)\) \\  This paper &  & \(c^{2}\) for \(P^{}\), &  & \) for \(c^{2}(,^{*})=^{2}(Q,Q^{})=o(d)\)} \\  & & & \(^{2}\) for \(Q^{}\) & \(d n\) & \( 1\) for \(c^{2}(,^{*})=^{2}(Q,Q^{})=o(d)\) \\   

Table 1: Summary of previous informational results, together with the ones in this paper high-dimensional space: indeed, the task of learning embeddings is often agnostic to orientation in the latent space. In other words, two point clouds may represent the same data points while having different global orientations. Hence, across different data sets, learning this orientation shift is crucial in order to compare (or align) the point clouds. As an illustration of this fact, Xing et al. (2015) provides empirical evidence that orthogonal transformations are particularly adapted for bilingual word translation.

_Discussing the method proposed in Grave et al. (2019)._ We conclude this introduction by discussing the work of Grave et al. (2019). Their proposed algorithm is as follows. At each iteration \(t\), given a current estimate \(Q_{t}\) of the orthogonal transformation, we sample mini-batches \(X_{t},Y_{t}\) of same size \(b\) and find the optimal matching \(P_{t}\) between \(Y_{t}Q_{t}^{}\) and \(X_{t}\), via solving a linear assignment problem of size \(b\). This matching \(P_{t}\) in turn helps to refine the estimation of the orthogonal transformation via a projected gradient descent step, and the procedure repeats. This method has the main advantage to be scalable to very large datasets and to perform well in practice ; however, no guarantees are given for this method, and in particular the mini-batch step which can justifiably raise some concerns. Indeed, since \(X_{t}=(x_{t,j})_{j[b]}\) and \(Y_{t}=(y_{t,j})_{j[b]}\) are chosen independently, if \(b\) it is likely that for any matching \(_{t}\) the pairs \((x_{t,j},y_{t,_{t}(j)})\) always correspond to disjoint pairs, and thus aligning \(Y_{t}Q_{t}^{}\) and \(X_{t}\) does not reveal any useful information about the true \(P^{}\) - this is even more striking when the data is non-isotropic.

### Problem setting and metrics of performance

_Notations._ We denote by \(X(,)\) with \(^{d}\) and \(^{d d}\) the fact that \(X\) follows a Gaussian distribution in \(^{d}\) of mean \(\) and covariance matrix \(\). If \(=0\) and \(=I_{d}\), variable \(X\) is called _standard Gaussian_. We denote by \((d)\) the orthogonal group in dimension \(d\), and by \(_{n}\) the group of permutations on \([n]\). Throughout, \(\|\|\) and \(\) are is the standard euclidean norm and scalar product on \(^{d}\), and \(\|\|_{F}\) and \(\|\|_{op}\) are respectively the Frobenius matrix norm and the operator matrix norm. The spectral radius of a matrix \(A\) is denoted \((A)\). In all the proofs, quantities \(c_{i}\) where \(i\) is an integer are unspecified constants which are universal, that is independent from the parameters. Finally, all considered asymptotics are when \(n\). Note that \(d\) also depends on \(n\). An event is said to hold _with high probability (w.h.p.)_ if its probability tends to \(1\) when \(n\) goes to \(\).

_Problem setting and performance metrics._ We work with the planted model as introduced in (1) and recall that our goal is to recover the permutation \(^{}\) and the orthogonal matrix \(Q^{}\) from the observation of \(X\) and \(Y\).

_Performance metrics._ Previous works measure the performance of an estimator \(\) of a planted relabeling \(^{}\) via the overlap:

\[(,^{}):=_{i=1}^{n}_{\{(i)=^{}(i)\}}\,, \]

defined for any two permutations \(,^{}\). This is an interesting metric when we have no hierarchy in the errors, that is when only the true match is valuable, and all wrong matches cost the same. However, this discrete measure does not take into account the underlying geometry of the model. A performance metric which is more adapted to our setting is the \(L^{2}\) transport cost between the point clouds. The natural intuition is that a mismatch is less costly if it corresponds to embeddings which are in fact close in the underlying space. We define

\[c^{2}(,^{})=_{i=1}^{n}\|x_{(i)}-x_{^{ }(i)}\|^{2},\]

for any two permutations \(,^{}\). Note that this cost can also be written in matrix form as \(c^{2}(P,P^{})=\|(P-P^{})X^{}\|_{F}^{2}\). From this form it is clear that, as stated before, \(c^{2}(P,P^{})\) is nothing but the euclidean transport cost for aligning \(XP^{}\) onto \(X(P^{})^{}\). Note that these two measures, \(\) and \(c^{2}\), are also well-defined3 when \(P,P^{}\) are more general (and in particular when they are bistochastic matrices). Finally, we measure the performance for the estimation of \(Q^{}\) via the Frobenius norm:

\[^{2}(Q,Q^{})=\|Q-Q^{}\|_{F}^{2},\]defined for any two orthogonal matrices \(Q,Q^{}\).

_Comparison between metrics._ For a Haar-distributed matrix \(Q\) on \((d)\), we have that \([^{2}(Q,Q^{})]=2d\), while for \(\) sampled uniformly from the set of all permutations, we have \([c^{2}(,^{})]=2d(1-1/n)\) and \([(,^{})]=1/n\). Hence, some estimators \(,\) of \(^{},Q^{}\) will perform well in our metrics if they can achieve \(^{2}(Q,Q^{}) d\), and \(c^{2}(,^{}) d\) for some small (possibly vanishing) \(>0\).

Depending on dimension \(d\), similarity measures given by \(c^{2}\) and the overlap can behave differently or coincide. In the case where \(d\) is small, and thus plays a very important role, \(\) and \(c^{2}\) have very different behaviors, and lead to very different results. In particular, there is a wide regime in which inferring \(^{}\) for the overlap sense is impossible, but reachable in the transport cost sense, see Section 2.

For any fixed permutation \(\), we have that \([c^{2}(,^{})]=2d(1-(,^ {}))\), where the mean is taken with respect to the randomness of \(X\). We also have the basic deterministic inequality

\[c^{2}(,^{})(1-(,^{})) _{(i,j)[n]^{2}}\|x_{i}-x_{j}\|^{2}.\]

Thus, as long as \(_{(i,j)[n]^{2}}\|x_{i}-x_{j}\|^{2}=O(d)\), an estimator \(\) with good overlap (\(1-(,^{})\)) also has a good \(c^{2}\) cost (\(c^{2}(,^{})=O( d)\)). However, this required control \(_{(i,j)[n]^{2}}\|x_{i}-x_{j}\|^{2}=O(d)\) only holds as long as \(d(n)\).

The blessing of large dimensions lead to an equivalence between the discrete metric \(\), and the continuous transport metric \(c^{2}\). We gather several important points highlighting the dichotomy between small and large dimensions for our problem in the following remark.

**Remark 1**.: _On the blessings of large dimensions for our problem:_

1. _For any open ball_ \(\) _of radius_ \(>0\)_, denoting_ \(=\{x_{i},i[n]\}\)_, we have that_ \((=) 1\) _if_ \(d(n)\)_, while if_ \(d(n)\) _then for all_ \(M>0\)_,_ \((|| M) 1\)_. In small dimensions, any fixed non-empty ball will contain infinitely many points of_ \(\) _as_ \(n\) _increases, while in large dimensions these points are separated and any fixed ball will contain no such points w.h.p._
2. _For_ \(d(n)\)_, matrix_ \(X/\) _satisfies the_ restricted isometry property _[_Candes_,_ 2008_]__._
3. _For_ \(d(n)\)_, the overlap and the transport cost metrics are equivalent: there exist numerical constants_ \(,>0\) _such that w.h.p., for all permutation matrices_ \(,^{}\)_,_ \( c^{2}(,^{}) 2d(1-(,^{}))  c^{2}(,^{})\)_._

_Organization of the rest of the paper._ Section 2 is dedicated to our informational results, giving their essential content as well as the main ideas on the proofs. We next discuss in Section 3 some computational results, introducing the Ping-Pong algorithm, and presenting our numerical experiments.

## 2 Informational results

The substantial theoretic part of the paper stands in the informational results obtained for the Procrustes-Wasserstein problem which we describe hereafter.

### High dimensions

In the high-dimensional case when \( n d\) (and \(d d n\)), our results - Theorem 1 below - imply that if \( 0\) then the ML estimators defined in (2) satisfy w.h.p.

\[(^{},)=1-o(1),\ c^{2}(,P^{})=o( d),^{2}(,Q^{})=o(d),\]

that is one can infer \(^{}\) and \(Q^{}\) almost exactly, for all introduced metrics, as soon as \( 0\).

Note that this is the first result in the high-dimensional regime for the Procrustes Wassertein problem: Kunisky and Niles-Weed (2022) also considered this regime but only for the LAP problem (thatis recovering \(^{}\) when \(Q^{}\) in known), and the only existing results for geometric graph alignment Wang et al. (2022); Gong and Li (2024) do not consider this high dimensional case. Our result thus complements the existing picture and shows that almost exact recovery is feasible under the loose assumption \( 0\), in the \(c^{2}\) and the overlap sense, since these metrics are equivalent in large dimensions (see Remark 1). Our result is in fact more specific and only requires \(d 2 n\). We prove the following Theorem:

**Theorem 1**.: _Assume that \(d 2 n\). There exists universal constants \(c_{1},c_{2},c_{3}>0\) so that for \(n\) large enough, with probability \(1-o(1)\), the ML estimators defined in (2) satisfy_

\[(^{},) 1-(60^{2},c_{1} ,c_{2}), \]

_and_

\[(Q^{},)}{2d} c_{1}+c_{2}^ {2}+c_{3}(,})\,. \]

The proof of Theorem 1 is detailed in Appendix C and builds upon controlling the probability of existence of a certain subset of indices \((,,Q^{})\) of vectors with prescribed properties in order to show that \(^{}\) can be recovered. We apply standard concentration inequalities to control the previous probability. The \(d 2(n)\) assumption is crucial here since it allows the union bound over \(_{n}\) to work.

### Low dimensions

In the low-dimensional case when \(d n\), Theorem 2 below implies that if \(=o(d^{-1/2})\) then there exist estimators \(,\) that satisfy w.h.p.

\[c^{2}(,P^{})=o(d),^{2}(,Q^{})=o(d)\,,\]

that is, one can approximate \(^{}\) (in the \(c^{2}\) sense _only_) and \(Q^{}\) as soon as \(=o(d^{-1/2})\). This is of course to be put in contrast with the previous results on geometric graph alignment in this low-dimensional regime: for almost exact recovery in Wang et al. (2022)_in the overlap sense_, we need \(=o(n^{-1/d})\), which is far more restrictive than \(=o(d^{-1/2})\) as soon as \(d(d)< n\), that is nearly in the whole low dimensional regime when \(d n\). In particular, since the rates of Wang et al. (2022) are sharp when \(d\) is of constant order, in order to approximate \(^{}\) in the overlap sense it is necessary to have \(\) to decreasing polynomially (at rate \(1/n^{1/d}\)) to \(0\), whereas approximating \(^{}\) in the transport cost sense requires only \(=o(1)\).

There is no contradiction here, since we recall that the \(c^{2}\) metric and the overlap are not equivalent in small dimensions: let us give a few more insights on this. This scaling \(n^{-1/d}\) comes from the fact that in small dimensions, points of the dataset are close to each other, and the order of magnitude between some \(x_{i}\) and its closest point in the dataset scales exactly as \(n^{-1/d}\): if the noise is smaller than this quantity, one should be able to recover the planted permutation. However, when it comes to considering the \(c^{2}\) metric, matching \(i\) with \(j\) such that \( x_{i}-x_{j}^{2} d\) is sufficient, thus suggesting that recovering a permutation with small \(c^{2}\) cost and recovering \(Q^{}\) with small Frobenius norm error should be achievable even with large \(\) (_i.e._, that does no tend to 0 as \(n\) increases).

Our main theorem for low dimensions is as follows.

**Theorem 2**.: _Let \(_{0}(0,1)\). There exist estimators \(,\) of \(^{},Q^{}\) such that if for some numerical constants \(C_{1},C_{2}>0\) we have \( C_{1}_{0}^{2}d^{-1/2}\) and \((n) C_{2}d(1/_{0})\), then:_

\[(,^{})}{2d}_{0} (,Q^{})}{2d}_{0}\,.\]

A refined version of Theorem 2, namely Theorem 3, is proved in Appendix D. We emphasize that the estimators considered in Theorem 2 are _not_ the ML estimators: recall that the strategy to analyse the former as rolled out for Theorem 1 required the union bound over \(_{n}\) to work. This drastically fails when \(d(n)\). Hence, we will instead focus on an estimator that takes advantage of the fact that \(d\) is small, and show that even in small dimensions, the signal-to-noise ratio \(\) does not need to decrease with \(n\).

Let us first describe the intuition behind the estimators \(,\). When \(d=1\), \(Q^{}= 1\) and a simple strategy to recover \(Q^{}\) is to count the number \(N_{+}(),N_{-}()\) (resp. \(N_{+}(),N_{-}()\)) of positive and negative \(x_{i}\) (resp. positive and negative \(y_{j}\)): if \(N_{+}()\) and \(N_{+}()\) are close, then we output \(=+1\), whereas if \(N_{+}()\) and \(N_{-}()\) are close, then \(=-1\). In dimension \(d\), an analog strategy can be applied at the cost of looking in all relevant directions, and the number of such directions is exponentially big in \(d\). Our strategy is thus as follows. We compute the number of points that lie in a given cone \((u,)\) of given angle \(\) and direction \(u\). Then, we estimate \(Q^{}\) by the orthogonal transformation \(\) which makes the number of \(y_{j}\) in \((u,)\) closest to the number of \(x_{j}\) in \((u,)\), for any direction \(u\). Note that this approach heavily relies on the small dimension assumption \(d n\): in this case, for any constant \(\), all theses cones contain w.h.p. a large number of points (tending to \(\) with \(n\)), which does not hold anymore when \(d n\).

For \(>0\) and \(u^{d-1}\), let \((u,):=\{v^{d}\,|\, u,v (1-)\|v\|\}\) be the cone of angle \(\) centered around \(u\). Let \(:=\{x_{i},i[n]\}\), \(:=\{y_{i},i[n]\}\). We now introduce the following sets, for some \(>0\):

\[_{}(u,):=(u,) (0,1/)^{C}_{}(u, ):=(u,)(0,}/)^{C}\;,\]

where \((0,r)^{C}\) contains all vectors in \(^{d}\) of norm larger than or equal to \(r\). The role of \(>0\) is to prevent side effects: indeed, since the cones are centered at the origin, points that are too close to \(0\) fall into cones with arbitrary directions and are not informative for the statistics we want to compute.

Now, for some \(p 1\) and directions \(u_{1},,u_{p}^{d-1}\) to be set later, we define the following _conical alignment loss_:

\[ Q(d)\,, F(Q)=_{k=1}^{p}(| _{}(Qu_{k},)|-|_{}(u_{k}, )|)^{2}. \]

The estimator \(\) in Theorem 2 is then defined as a minimizer of the conical alignment loss over a finite set \((d)\):

\[*{arg\,min}_{Q}F(Q)\,,\]

where \(\) will further be some \(\)-net of \((d)\), while \(\) is then obtained by a LAP as in (10).

### From \(P^{}\) to \(Q^{}\) and vice versa

In our proofs, we often prove that one of the estimators \(\) or \(\) performs well in order to deduce that both perform well. This is thanks to the following two results, proved in Appendix B.1 and B.2.

**Lemma 2** (From \(\) to \(\)).: _Let \((0,1/2)\). Assume that there exists \(\) that is \((\{x_{1},,x_{n},y_{1},,y_{n}\})\)-measurable such that \(^{2}_{}(,Q^{}):=\|-Q^{}\|^{2}  d\). There exist constants \(C_{1},C_{2},C_{3}>0\) such that with probability at least \(1-2e^{-nd}-2e^{-(d^{2}+)}\),_

\[*{argmin}_{_{n}}_{i= 1}^{n}\|x_{(i)}-^{}y_{i}\|^{2}, \]

_that can be computed in polynomial time (complexity \(O(n^{3})\)) as the solution of a LAP, satisfies:_

\[(,^{})}{d} C_{1}+C_{2}^{2}+C_ {3}(,})\,.\]

**Lemma 3** (From \(\) to \(\)).: _Let \((0,1/2)\). Assume that there exists \(\) that is \((\{x_{1},,x_{n},y_{1},,y_{n}\})\)-measurable such that \(c^{2}(,^{}) d\). Let \(\) be the solution to the following optimization problem: There exist constants \(C_{1},C_{2},C_{3}>0\) such that with probability at least \(1-2e^{-nd}-2e^{-(d^{2}+)}\),_

\[*{arg\,min}_{Q(d)}_{i=1}^ {n}\|x_{(i)}-Q^{}y_{i}\|^{2}, \]

_that can be computed in closed form with an SVD of \(XY^{}\), satisfies:_

\[_{}(,Q^{})}{d} C_{1} +C_{2}^{2}+C_{3}(,})\,.\]Computational aspects

The estimators provided this far in Section 2, namely the joint minimization in \(P\) and \(Q\) in (2) and the minimizer of the conical alignment loss in (9) are of course not poly-time in general. In this section, we are interested in computational aspects of the problem.

### Convex relaxation and Ping-Pong algorithm

Estimating \(P^{}\) can be made via solving the QAP (5), that can be convexified into the _relaxed quadratic assignment problem_ (relaxed QAP):

\[_{ relaxed}*{arg\,min}_{P_{n}}\|X^{}XP-PY^{}Y\|_{F}^{2}\,, \]

where \(_{n}\) is the polytope of _bistochastic_ matrices, which is the convex envelope of the set of permutation matrices. Note that unlike in (5), this argmin is not necessarily equal to \(*{arg\,max}_{P_{n}} P^{}X^{}XP,Y^{T }Y^{}\) since \(_{n}\) contains non-orthogonal matrices.

The estimate \(_{ relaxed}\) gives a first estimate to then perform alternate minimizations in \(Q\) through an SVD - see (11) - and \(P\) through a LAP - see (10). Combining an initialization with convex relaxation, computed via Frank-Wolfe algorithm (Jaggi, 2013) and the alternate minimizations in \(P\) and \(Q\) yields the _Ping-Pong algorithm_.

```
Input: Number of Frank-Wolfe steps \(T\), number of alternate-minimization steps \(K\), \(_{0}=}{n}\)
1for\(k=0\) to \(T-1\)do
2 Compute \(S_{k}=*{arg\,min}_{P_{n}} P, f(_{k})\) (LAP), where \(f(P)=X^{}XP-PY^{}Y_{F}^{2}\)
3\(_{k+1}=(1-_{k})_{k}+_{k}S_{k}\) for \(_{k}=\)
4\(P_{0}=_{T}\) and \(Q_{0}=I_{d}\)
5for\(k=0\) to \(K-1\)do
6\(Q_{k+1}=U_{k}V_{k}^{}\) for \(YP_{k}X^{}=U_{k}D_{k}V_{k}\) the SVD of \(YP_{k}X^{}\) (Ping)
7\(P_{k+1}*{arg\,max}_{P_{n}} P,Y^{}Q_{k+ 1}X\) (LAP) (Pong)
8 Output:\(P_{K},Q_{K}\)
```

**Algorithm 1**Ping-Pong Algorithm

Algorithm 1 is structurally similar to Grave et al. (2019)'s algorithm, as explained in the introduction. The difference lies in the steps in Lines 6-7 of Algorithm 1: while Grave et al. (2019) perform projected gradient steps, our approach is more greedy and directly minimizes in each variable. Both approaches are experimentally compared in Section 3.3.

### Guarantees for one step of Ping-Pong algorithm

Providing statistical rates for the outputs of Algorithm 1 is a challenging problem for two reasons. First, relaxed QAP is not a well-understood problem: the only existing guarantees in the literature are for correlated Gaussian Wigner models in the noiseless case (i.e., \(=0\) in our model) (Valdivia and Tyagi, 2023), while for correlated Erdos-Renyi graphs, the relaxation is known to behave badly in general (Lyzinski et al., 2016). Secondly, studying the iterates in lines 6 and 7 of the algorithm is challenging, since these are projections on non-convex sets. While Lemmas 2 and 3 show that if \(P_{k}\) (resp. \(Q_{k}\)) has small \(c^{2}\) loss, then \(Q_{k+1}\) has small \(^{2}\) loss (resp. \(P_{k+1}\) has small \(c^{2}\) loss), showing that there is a contraction at each iteration 'a la Picard's fix-point Theorem' remains out of reach for this paper. We thus resort to proving that _one single step_ of Algorithm 1 (\(K=T=1\)) can recover the planted signal, provided that the noise \(\) is small enough.

**Proposition 1**.: _There exists \(C>0\) such that for any \((0,1)\), if \( n^{-}\), then the permutation \(\) associated to the outputs \(,\) of Algorithm 1 for \(K=T=1\) satisfies, with probability \(1-1/n\):_

\[*{ov}(^{},) 1-\,.\]_In the high-dimensional setting (\(d(n)\)), there exist some constants \(c_{1},c_{2}\) such that if \( n^{-c_{1}}\), then \(\) satisfies w.h.p._

\[(^{},) 1-c_{2}(+},+)\,.\]

Thus, for \(\) polynomially small in \(n\) and exponentially small in \(1/\), one step of Alg. 1 recovers \(^{}\) in the overlap sense with error \(\). In large dimensions, this is improved, since \(\) is no longer required to be exponentially small as the target error decreases to zero. Proof of Proposition 3 is given in Appendix E.

### Numerical experiments

We compare in Figure 1 our Alg. 1 with \((i)\) the naive initialization of the relaxed QAP estimator (12), and \((ii)\) the method in Grave et al. (2019). The curve'relaxed QAP via FW' is obtained by computing the relaxed QAP estimator with Frank-Wolfe algorithm with \(T=1000\) steps, enough for convergence. This estimator is then taken as initialization for Alg. 1 and Grave et al. (2019)'s algorithm, that are both taken with the same large number of steps (\(K=100\), empirically leading to convergence to stationary points of the algorithms). For fair comparison, we take full batches in Grave et al. (2019) (smaller batches lead to even worse performances).

## Conclusion

We establish new informational results for the Procrustes-Wassertein problem, both in the high (\(d n\)) and low (\(d n\)) dimensional regimes. We propose the 'Ping-Pong algorithm', alternatively estimating the orthogonal transformation and the relabeling, initialized via a Franke-Wolfe convex relaxation. Our experimental results show that our method most globally outperforms the algorithm proposed in Grave et al. (2019).

Figure 1: Influence of the parameters (dimensions \(d\), number of points \(n\), and noise level \(\)) on the accuracy (in terms of overlap) of three different estimators: the relaxed QAP estimator (12) projected on the set of permutation matrices (blue curve), the output of Alg. 1 (red curve), and the output of Grave et al. (2019)â€™s algorithm (purple curve). Each dot corresponds to averaging scores over 10 experiments. Figure 0(a): \(=0.34,n=100\). Figure 0(b): \(=0.34,d=5\). Figures 0(c) and 0(d): \(n=200\), \(d=2\) and \(d=60\) respectively.