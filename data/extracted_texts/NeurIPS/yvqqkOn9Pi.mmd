# Nathaniel D Daw2,3,4, Gregory Wayne2

Would I have gotten that reward? Long-term credit assignment by counterfactual contribution analysis

Alexander Meulemans11, Simon Schug11, Seijin Kobayashi11Equal contribution; ordering determined by coin flip.Code available at https://github.com/seijin-kobayashi/cocoa

###### Abstract

To make reinforcement learning more sample efficient, we need better credit assignment methods that measure an action's influence on future rewards. Building upon Hindsight Credit Assignment (HCA) , we introduce Counterfactual Contribution Analysis (COCOA), a new family of model-based credit assignment algorithms. Our algorithms achieve precise credit assignment by measuring the contribution of actions upon obtaining subsequent rewards, by quantifying a counterfactual query: 'Would the agent still have reached this reward if it had taken another action?'. We show that measuring contributions w.r.t. rewarding _states_, as is done in HCA, results in spurious estimates of contributions, causing HCA to degrade towards the high-variance REINFORCE estimator in many relevant environments. Instead, we measure contributions w.r.t. rewards or learned representations of the rewarding objects, resulting in gradient estimates with lower variance. We run experiments on a suite of problems specifically designed to evaluate long-term credit assignment capabilities. By using dynamic programming, we measure ground-truth policy gradients and show that the improved performance of our new model-based credit assignment methods is due to lower bias and variance compared to HCA and common baselines. Our results demonstrate how modeling action contributions towards rewarding outcomes can be leveraged for credit assignment, opening a new path towards sample-efficient reinforcement learning.2

## 1 Introduction

Reinforcement learning (RL) faces two central challenges: exploration and credit assignment . We need to explore to discover rewards and we need to reinforce the actions that are instrumental for obtaining these rewards. Here, we focus on the credit assignment problem and the intimately linked problem of estimating policy gradients. For long time horizons, obtaining the latter is notoriously difficult as it requires measuring how each action influences expected subsequent rewards. As the number of possible trajectories grows exponentially with time, future rewards come with a considerable variance stemming from stochasticity in the environment itself and from the stochasticity of interdependent future actions leading to vastly different returns .

Monte Carlo estimators such as REINFORCE  therefore suffer from high variance, even after variance reduction techniques like subtracting a baseline . Similarly, in Temporal Difference methods such as Q-learning, this high variance in future rewards results in a high bias in the value estimates, requiring exponentially many updates to correct for it . Thus, a common technique to reduce variance and bias is to discount rewards that are far away in time resulting in a biased estimator which ignores long-term dependencies [10; 11; 12; 13]. Impressive results have nevertheless been achieved in complex environments [14; 15; 16] at the cost of requiring billions of environment interactions, making these approaches sample inefficient.

Especially in settings where obtaining such large quantities of data is costly or simply not possible, model-based RL that aims to simulate the dynamics of the environment is a promising alternative. While learning such a world model is a difficult problem by itself, when successful it can be used to generate a large quantity of synthetic environment interactions. Typically, this synthetic data is combined with model-free methods to improve the action policy [17; 18; 19]. A notable exception to simply using world models to generate more data are the Stochastic Value Gradient method  and the closely related Dreamer algorithms [21; 22; 23]. These methods perform credit assignment by backpropagating policy gradients through the world model. Crucially, this approach only works for environments with a continuous state-action space, as otherwise sensitivities of the value with respect to past actions are undefined [20; 22; 24]. Intuitively, we cannot compute sensitivities of discrete choices such as a yes / no decision as the agent cannot decide 'yes' a little bit more or less.

Building upon Hindsight Credit Assignment (HCA) , we develop Counterfactual Contribution Analysis (COCOA), a family of algorithms that use models for credit assignment compatible with discrete actions. We measure the _contribution_ of an action upon subsequent rewards by asking a counterfactual question: 'would the agent still have achieved the rewarding outcome, if it had taken another action?' (c.f. Fig. 1A). We show that measuring contributions towards achieving a future _state_, as is proposed in HCA, leads to spurious contributions that do not reflect a contribution towards a reward. This causes HCA to degrade towards the high-variance REINFORCE method in most environments. Instead, we propose to measure contributions directly on rewarding outcomes and we develop various new ways of learning these contributions from observations. The resulting algorithm differs from value-based methods in that it measures the contribution of an action to individual rewards, instead of estimating the full expected sum of rewards. This crucial difference allows our contribution analysis to disentangle different tasks and ignore uncontrollable environment influences, leading to a gradient estimator capable of long-term credit assignment (c.f. Fig. 1B). We introduce a new method for analyzing policy gradient estimators which uses dynamic programming to allow comparing to ground-truth policy gradients. We leverage this to perform a detailed bias-variance analysis of all proposed methods and baselines showing that our new model-based credit assignment algorithms achieve low variance and bias, translating into improved performance (c.f. Fig. 1C).

Figure 1: **Counterfactual Contribution Analysis enables long-term credit assignment.** (A) Given a sample trajectory that eventually results in a rewarding outcome, we estimate the policy gradient by considering the contribution of actions along the trajectory towards arriving at a rewarding outcome. In this example, we measure how much more likely the rewarding outcome with reward \(r_{3}\) is when following action \(a_{1}\) versus the counterfactual actions \(a_{2}\) and \(a_{3}\) in state \(s_{2}\). This is quantified through the contribution coefficient \(w(s_{2},a_{1},r_{3})\) which is used to update all possible action probabilities of the policy \((a s_{2})\). (B) In the linear key-to-door environment increasing the distance between picking up the key and opening the door that leads to reward necessitates credit assignment over increasing time spans. COCOA consistently achieves good performance (left) compared to HCA and baselines which deteriorate when increasing the distance between an action and the resulting rewarding outcome. This is reflected in a higher signal-to-noise ratio of the policy gradient estimator of COCOA compared to baselines (right).

## 2 Background and notation

We consider an undiscounted Markov decision process (MDP) defined as the tuple \((,,p,p_{r})\), with \(\) the state space, \(\) the action space, \(p(S_{t+1} S_{t},A_{t})\) the state-transition distribution and \(p_{r}(R S,A)\) the reward distribution with bounded reward values \(r\). We use capital letters for random variables and lowercase letters for the values they take. The policy \((A S)\), parameterized by \(\), denotes the probability of taking action \(A\) at state \(S\). We consider an undiscounted infinite-horizon setting with a zero-reward absorbing state \(s_{}\) that the agent eventually reaches: \(_{t}p(S_{t}=s_{})=1\). Both the discounted and episodic RL settings are special cases of this setting. (c.f. App. B), and hence all theoretical results proposed in this work can be readily applied to both (c.f. App J).

We use \((s,)\) and \((s,a,)\) as the distribution over trajectories \(T=(S_{t}\,A_{t},R_{t})_{t 0}\) starting from \(S_{0}=s\) and \((S_{0},A_{0})=(s,a)\) respectively, and define the return \(Z_{t}=_{t=0}^{}R_{t}\). The value function \(V^{}(s)=_{T(s,)}[Z_{t}]\) and action value function \(Q^{}(s,a)=_{T(s,a,)}[Z_{t}]\) are the expected return when starting from state \(s\), or state \(s\) and action \(a\) respectively. Note that these infinite sums have finite values due to the absorbing zero-reward state (c.f. App. B).

The objective of reinforcement learning is to maximize the expected return \(V^{}(s_{0})\), where we assume the agent starts from a fixed state \(s_{0}\). Policy gradient algorithms optimize \(V^{}(s_{0})\) by repeatedly estimating its gradient \(_{}V(s_{0})\) w.r.t. the policy parameters. REINFORCE  (c.f. Tab. 1) is the canonical policy gradient estimator, however, it has a high variance resulting in poor parameter updates. Common techniques to reduce the variance are (i) subtracting a baseline, typically a value estimate, from the sum of future rewards [2; 25] (c.f. 'Advantage' in Tab. 1); (ii) replacing the sum of future rewards with a learned action value function \(Q\)[2; 3; 25; 26] (c.f. 'Q-critic' in Tab. 1); and (iii) using temporal discounting. Note that instead of using a discounted formulation of MDPs, we treat the discount factor as a variance reduction technique in the undiscounted problem [10; 11; 13] as this more accurately reflects its practical use [4; 27]. Rearranging the summations of REINFORCE with discounting lets us interpret temporal discounting as a credit assignment heuristic, where for each reward, past actions are reinforced proportional to their proximity in time.

\[_{}^{;}V^{}(s_{0})=_ {t 0}R_{t}_{k t}^{t-k}_{}(A_{k}  S_{k}),.\] (1)

Crucially, long-term dependencies between actions and rewards are exponentially suppressed, thereby reducing variance at the cost of disabling long-term credit assignment [4; 28]. The aim of this work is to replace the heuristic of time discounting by principled _contribution coefficients_ quantifying how much an action contributed towards achieving a reward, and thereby introducing new policy gradient estimators with reduced variance, without jeopardizing long-term credit assignment.

HCA  makes an important step in this direction by introducing a new gradient estimator:

\[_{}^{}V^{}=_{t 0}_{a} _{}(a S_{t})r(S_{t},a)+_{k 1}(A_{t }=a S_{t}=s,S^{}=S_{t+k})}{(a S_{t})}R_{t+k}\] (2)

with \(r(s,a)\) a reward model, and the _hindsight_ ratio \((a S_{t}=s,S^{}=S_{t+k})}{(a S_{t})}\) measuring how important action \(a\) was to reach the state \(S^{}\) at some point in the future. Although the hindsight ratio delivers precise credit assignment w.r.t. reaching future states, it has a failure mode of practical importance, creating the need for an updated theory of model-based credit assignment which we will detail in the next section.

## 3 Counterfactual Contribution Analysis

To formalize the 'contribution' of an action upon subsequent rewards, we generalize the theory of HCA  to measure contributions on _rewarding outcomes_ instead of states. We introduce unbiased policy gradient estimators that use these contribution measures, and show that HCA suffers from high variance, making the generalization towards rewarding outcomes crucial for obtaining low-variance estimators. Finally, we show how we can estimate contributions using observational data.

### Counterfactual contribution coefficients

To assess the contribution of actions towards rewarding outcomes, we propose to use counterfactual reasoning: 'how does taking action \(a\) influence the probability of obtaining a rewarding outcome, compared to taking alternative actions \(a^{}\)?'.

**Definition 1** (Rewarding outcome).: A rewarding outcome \(U^{} p(U^{} s^{},a^{},r^{})\) is a probabilistic encoding of the state-action-reward triplet.

If action \(a\) contributed towards the reward, the probability of obtaining the rewarding outcome following action \(a\) should be higher compared to taking alternative actions. We quantify the contribution of action \(a\) taken in state \(s\) upon rewarding outcome \(u^{}\) as

\[w(s,a,u^{})=p^{}(U_{t+k}=u^{} S_{t}=s,A_{ t}=a)}{_{k 1}p^{}(U_{t+k}=u^{} S_{t}=s)}-1=(A_{ t}=a S_{t}=s,U^{}=u^{})}{(a s)}-1\] (3)

From a given state, we compare the probability of reaching the _rewarding outcome_\(u^{}\) at any subsequent point in time, given we take action \(a\) versus taking counterfactual actions according to the policy \(\), as \(p^{}(U_{t+k}=u^{} S_{t}=s)=_{a^{}}(a^{} s) p^{}(U_{t+k}=u^{} S_{t}=s,A_{t}=a^{})\). Subtracting this ratio by one results in an intuitive interpretation of the _contribution coefficient_\(w(s,a,u^{})\): if the coefficient is positive/negative, performing action \(a\) results in a higher/lower probability of obtaining rewarding outcomes \(u^{}\), compared to following the policy \(\). Using Bayes' rule, we can convert the counterfactual formulation of the contribution coefficients into an equivalent _hindsight_ formulation (right-hand side of Eq. 3), where the hindsight distribution \(p^{}(A_{t}=a S_{t}=s,U^{}=u^{})\) reflects the probability of taking action \(a\) in state \(s\), given that we encounter the rewarding outcome \(u^{}\) at _some future point in time_. We refer the reader to App. C for a full derivation.

**Choice of rewarding outcome.** For \(u^{}=s^{}\), we recover state-based HCA .3 In the following, we show that a better choice is to use \(u^{}=r^{}\), or an encoding \(p(u^{} s^{},a^{})\) of the underlying object that causes the reward. Both options lead to gradient estimators with lower variance (c.f. Section 3.3), while using the latter becomes crucial when different underlying rewarding objects have the same scalar reward (c.f. Section 4).

### Policy gradient estimators

We now show how the contribution coefficients can be used to learn a policy. Building upon HCA , we propose the Counterfactual Contribution Analysis (COCOA) policy gradient estimator

\[_{}^{U}V^{}(s_{0})=_{t 0}_{}(A_ {t} S_{t})R_{t}+_{a}_{}(a S_{t}) _{k 1}w(S_{t},a,U_{t+k})R_{t+k}.\] (4)

When comparing to the discounted policy gradient of Eq. 1, we see that the temporal discount factors are substituted by the contribution coefficients, replacing the time heuristic with fine-grained credit assignment. Importantly, the contribution coefficients enable us to evaluate all counterfactual actions instead of only the observed ones, further increasing the quality of the gradient estimator (c.f. Fig. 1A). The contribution coefficients allow for various different gradient estimators (c.f. App. C). For example, independent action samples can replace the sum over all actions, making it applicable to large action spaces. Here, we use the gradient estimator of Eq. 4, as our experiments consist of small action spaces where enumerating all actions is feasible. When \(U=S\), the above estimator is almost equivalent to

  
**Method** & **Policy gradient estimator** (\(_{}V^{}(s_{0})\)) \\  REINFORCE & \(_{t 0}_{}(A_{t} S_{t})_{k 0}R_{t+k}\) \\ Advantage & \(_{t 0}_{}(A_{t} S_{t})(_{k 0}R_{t+k}-V (S_{t}))\) \\ Q-critic & \(_{t 0}_{a}_{}(a S_{t})Q(S_{t},a)\) \\ HCA-Return & \(_{t 0}_{}(A_{t} S_{t})1-  S_{t})}{p^{}(A_{t} S_{t},Z_{t})}Z_{t}\) \\ TrajCV & \(_{t 0}_{}(A_{t} S_{t})(Z_{t}-Q(A_{t},S_{t}) -_{t^{}>t}(Q(S_{t^{}},A_{t^{}})-V(S_{t^{}}) )+\) \\  & \(_{a}_{}(a S_{t})Q(S_{t},a)\) \\  COCOA & \(_{t 0}_{}(A_{t} S_{t})R_{t}+_{a} _{}(a S_{t})_{k 1}w(S_{t},a,U_{t+k})R_{t+k}\) \\ HCA+ & \(_{t 0}_{}(A_{t} S_{t})R_{t}+_{a} _{}(a S_{t})_{k 1}w(S_{t},a,S_{t+k})R_{t+k}\) \\   

Table 1: Comparison of policy gradient estimators.

the state-based HCA estimator of Eq. 2, except that it does not need a learned reward model \(r(s,a)\). We use the notation HCA+ to refer to this simplified version of the HCA estimator. Theorem 1 below shows that the COCOA gradient estimator is unbiased, as long as the encoding \(U\) is fully predictive of the reward, thereby generalizing the results of Harutyunyan et al.  to arbitrary rewarding outcome encodings.

**Definition 2** (Fully predictive).: A rewarding outcome \(U\) is fully predictive of the reward \(R\), if the following conditional independence condition holds for all \(k 0\): \(p^{}(R_{k}=r S_{0}=s,A_{0}=a,U_{k}=u)=p^{}(R=r U=u)\), where the right-hand side does not depend on the time \(k\).

**Theorem 1**.: _Assuming that \(U\) is fully predictive of the reward (c.f. Definition 2), the COCOA policy gradient estimator \(_{}^{U}V^{}(s_{0})\) is unbiased, when using the ground-truth contribution coefficients of Eq. 3, that is_

\[_{}V^{}(s_{0})=_{T(s_{0},)}_{}^{U}V^{}(s_{0}).\]

### Optimal rewarding outcome encoding for low-variance gradient estimators

Theorem 1 shows that the COCOA gradient estimators are unbiased for all rewarding outcome encodings \(U\) that are fully predictive of the reward. The difference between specific rewarding outcome encodings manifests itself in the variance of the resulting gradient estimator. Proposition 2 shows that for \(U^{}=S^{}\) as chosen by HCA there are many cases where the variance of the resulting policy gradient estimator degrades to the high-variance REINFORCE estimator :

**Proposition 2**.: _In environments where each action sequence leads deterministically to a different state, we have that the HCA+ estimator is equal to the REINFORCE estimator (c.f. Tab. 1)._

In other words, when all previous actions can be perfectly decoded from a given state, they trivially all contribute to reaching this state. The proof of Proposition 2 follows immediately from observing that \(p^{}(a s,s^{})=1\) for actions \(a\) along the observed trajectory, and zero otherwise. Substituting this expression into the contribution analysis gradient estimator (4) recovers REINFORCE. A more general issue underlies this special case: State representations need to contain detailed features to allow for a capable policy but the same level of detail is detrimental when assigning credit to actions for reaching a particular state since at some resolution almost every action will lead to a slightly different outcome. Measuring the contribution towards reaching a specific state ignores that the same rewarding outcome could be reached in slightly different states, hence overvaluing the importance of previous actions and resulting in _spurious contributions_. Many commonly used environments, such as pixel-based environments, continuous environments, and partially observable MDPs exhibit this property to a large extent due to their fine-grained state representations (c.f. App. G). Hence, our generalization of HCA to rewarding outcomes is a crucial step towards obtaining practical low-variance gradient estimators with model-based credit assignment.

**Using rewards as rewarding outcomes yields lowest-variance estimators.** The following Theorem 3 shows in a simplified setting that (i) the variance of the REINFORCE estimator is an upper bound on the variance of the COCOA estimator, and (ii) the variance of the COCOA estimator is smaller for rewarding outcome encodings \(U\) that contain less information about prior actions. We formalize this with the conditional independence relation of Definition 2 by replacing \(R\) with \(U^{}\): encoding \(U\) contains less or equal information than encoding \(U^{}\), if \(U^{}\) is fully predictive of \(U\). Combined with Theorem 1 that states that an encoding \(U\) needs to be fully predictive of the reward \(R\), we have that taking the reward \(R\) as our rewarding outcome encoding \(U\) results in the gradient estimator with the lowest variance of the COCOA family.

**Theorem 3**.: _Consider an MDP where only the states at a single (final) time step contain a reward, and where we optimize the policy only at a single (initial) time step. Furthermore, consider two rewarding outcome encodings \(U\) and \(U^{}\), where \(S\) is fully predictive of \(U^{}\), \(U^{}\) fully predictive of \(U\), and \(U\) fully predictive of \(R\). Then, the following relation holds between the policy gradient estimators:_

\[[_{}^{R}V^{}(s_{0})][_{}^{U}V^{}(s_{0})][_{}^ {U^{}}V^{}(s_{0})][_{}^{S}V^{ }(s_{0})][_{}^{REINF}V^{}(s_{0})]\]

_with \(_{}^{X}V^{}(s_{0})\) the COCOA estimator (4) using \(U=X\), \([Y]\) the covariance matrix of \(Y\) and \(A B\) indicating that \(B-A\) is positive semi-definite._

As Theorem 3 considers a simplified setting, we verify empirically whether the same arguments hold more generally. We construct a tree environment where we control the amount of information a state contains about the previous actions by varying the overlap of the children of two neighbouring nodes (c.f. Fig 2), and assign a fixed random reward to each state-action pair. We compute the ground-truth contribution coefficients by leveraging dynamic programming (c.f. Section 4). Fig. 2B shows that the variance of HCA is as high as REINFORCE for zero state overlap, but improves when more states overlap, consistent with Proposition 2 and Theorem 3. To investigate the influence of the information content of \(U\) on the variance, we consider rewarding outcome encodings \(U\) with increasing information content, which we quantify with how many different values of \(u\) belong to the same reward \(r\). Fig. 2B shows that by increasing the information content of \(U\), we interpolate between the variance of COCOA with \(u=r\) and HCA+, consistent with Theorem 3.

**Why do rewarding outcome encodings that contain more information than the reward lead to higher variance?** To provide a better intuition on this question we use the following theorem:

**Theorem 4**.: _The policy gradient on the expected number of occurrences \(O^{}(u^{},s)=_{k 1}p^{}(U_{k}=u^{} S_{0}=s)\) is proportional to_

\[_{}O^{}(u^{},s)_{S^{} (,s)}_{a}_{}(a S ^{})w(S^{},a,u^{})O^{}(u^{},S^{ })\]

Recall that the COCOA gradient estimator consists of individual terms that credit past actions \(a\) at state \(s\) for a current reward \(r\) encountered in \(u\) according to \(_{a}_{}(a s)w(s,a,u^{})r^{}\) (c.f. Eq. 4). Theorem 4 indicates that each such term aims to increase the average number of times we encounter \(u^{}\) in a trajectory starting from \(s\), proportional to the corresponding reward \(r^{}\). If \(U^{}=R^{}\), this update will correctly make all underlying states with the same reward \(r^{}\) more likely while decreasing the likeliness of all states for which \(u^{} r^{}\). Now consider the case where our rewarding outcome encoding contains a bit more information, i.e. \(U^{}=f(R^{}, S^{})\) where \( S^{}\) contains a little bit of information about the state. As a result the update will distinguish some states even if they yield the same reward and increase the number of occurrences only of states containing the encountered \( S^{}\) while decreasing the number of occurrences for unseen ones. As in a single trajectory, we do not visit each possible \( S^{}\), this adds variance. The less information an encoding \(U\) has, the more underlying states it groups together, and hence the less rewarding outcomes are 'forgotten' in the gradient estimator, leading to lower variance.

### Learning the contribution coefficients

In practice, we do not have access to the ground-truth contribution coefficients, but need to learn them from observations. Following Harutyunyan et al. , we can approximate the hindsight distribution \(p^{}(A_{t}=a S_{t}=s,U^{}=u^{})\), now conditioned on rewarding outcome encodings instead of states, by training a model \(h(a s,u^{})\) on the supervised discriminative task of classifying the observed action \(a_{t}\) given the current state \(s_{t}\) and some future rewarding outcome \(u^{}\). Note that if the model \(h\) does not approximate the hindsight distribution perfectly, the COCOA gradient estimator (4) can be biased (c.f. Section 4). A central difficulty in approximating the hindsight distribution is that it is policy dependent, and hence changes during training. Proposition 5 shows that we can provide the policy logits as an extra input to the hindsight network without altering the learned hindsight

Figure 2: **HCA suffers from spurious contributions which can be alleviated by using less informative rewarding outcome encodings.** (A) and (C): Schematic of the tree environment where we parametrically adjust the amount of overlap between states by varying the amount of shared children of two neighboring nodes. We can decrease the information content of the rewarding outcome encoding \(u=f(s,a)\) by grouping state-action pairs that share the same reward value. (B) Normalized variance in dB using ground-truth coefficients and a random uniform policy (shaded region represents standard error over 10 random environments) comparing REINFORCE, HCA, COCOCA-reward and various degrees of intermediate grouping.

distribution, to make the parameters of the hindsight network less policy-dependent. This observation justifies and generalizes the strategy of adding the policy logits to the hindsight model output, as proposed by Alipov et al. .

**Proposition 5**.: \(p^{}(a s,u^{},l)=p^{}(a s,u^{})\)_, with \(l\) a deterministic function of \(s\), representing the sufficient statistics of \((a s)\)._

As an alternative to learning the hindsight distribution, we can directly estimate the probability ratio \(p^{}(A_{t}=a S_{t}=s,U^{}=u^{})/(a s)\) using a contrastive loss (c.f. App. D). Yet another path builds on the observation that the sums \(_{k 1}p^{}(U_{t+k}=u^{} s,a)\) are akin to Successor Representations and can be learned via temporal difference updates [30; 31] (c.f. App. D). We experimented both with the hindsight classification and the contrastive loss and found the former to work best in our experiments. We leverage the Successor Representation to obtain ground truth contribution coefficients via dynamic programming for the purpose of analyzing our algorithms.

## 4 Experimental analysis

To systematically investigate long-term credit assignment performance of COCOA compared to standard baselines, we design an environment which pinpoints the core credit assignment problem and leverage dynamic programming to compute ground-truth policy gradients, contribution coefficients, and value functions (c.f. App E.2). This enables us to perform detailed bias-variance analyses and to disentangle the theoretical optimal performance of the various gradient estimators from the approximation quality of learned contribution coefficients and (action-)value functions.

We consider the _linear key-to-door_ environment (c.f. Fig. 3A), a simplification of the key-to-door environment [3; 4; 32] to a one-dimensional track. Here, the agent needs to pick up a key in the first time step, after which it engages in a distractor task of picking up apples with varying reward values. Finally, it can open a door with the key and collect a treasure. This setting allows us to parametrically increase the difficulty of long-term credit assignment by increasing the distance between the key and door, making it harder to pick up the learning signal of the treasure reward

Figure 3: **COCOA enhances policy gradient estimates and sample efficiency whereas HCA fails to improve over baselines.** (A) Schematic representation of the linear key-to-door environment. (B) Performance of COCOA and baselines on the main task of picking up the treasure, measured as the average fraction of treasure rewards collected. (Left) ground-truth policy gradient estimators computed using dynamic programming, (right) learning the contribution coefficients or (action-)value function using neural networks. Shaded regions are the standard error (30 seeds). (C) Violin plot of the signal-to-noise ratio (SNR) in Decibels for the various policy gradient estimators with learned coefficients and (action-)value functions, computed on the same trajectories of a shared base policy. (D) Comparison of the bias-variance trade-off incurred by different policy gradient estimators, computed as in (C), normalized by the ground-truth policy gradient norm (scatter plot showing 30 seeds per method).

among a growing number of varying distractor rewards . We use the signal-to-noise ratio, \(=\|_{}V^{}\|^{2}/[\|_{}V^{ }-_{}V^{}\|^{2}]\), to quantify the quality of the different policy gradient estimators; a higher SNR indicates that we need fewer trajectories to estimate accurate policy gradients .

Previously, we showed that taking the reward as rewarding outcome encoding results in the lowest-variance policy gradients when using ground-truth contribution coefficients. In this section, we will argue that when _learning_ the contribution coefficients, it is beneficial to use an encoding \(u\) of the underlying _rewarding object_ since this allows to distinguish different rewarding objects when they have the same scalar reward value and allows for quick adaptation when the reward function but not the environment dynamics changes.

We study two variants of COCOA, COCOA-reward which uses the reward identity for \(U\), and COCOA-feature which acquires features of rewarding objects by learning a reward model \(r(s,a)\) and taking the penultimate network layer as \(U\). We learn the contribution coefficients by approximating the hindsight distribution with a neural network classifier \(h(a s,u^{},l)\) that takes as input the current state \(s\), resulting policy logits \(l\), and rewarding outcome \(u^{}\), and predicts the current action \(a\) (c.f. App. E for all experimental details). As HCA+ (c.f. Tab. 1) performs equally or better compared to HCA  in our experiments (c.f. App. F), we compare to HCA+ and several other baselines: (i) three classical policy gradient estimators, REINFORCE, Advantage and Q-critic, (ii) TrajCV , a state-of-the-art control variate method that uses hindsight information in its baseline, and (iii) HCA-return , a different HCA variant that uses the hindsight distribution conditioned on the return as an action-dependent baseline (c.f. Tab. 1).

### COCOA improves sample-efficiency due to favorable bias-variance trade-off.

To investigate the quality of the policy gradient estimators of COCOA, we consider the linear key-to-door environment with a distance of 100 between key and door. Our dynamic programming setup allows us to disentangle the performance of the estimators independent of the approximation quality of learned models by using ground truth contribution coefficients and (action-)value functions. The left panel of figure 3B reveals that in this ground truth setting, COCOA-reward almost immediately solves the task performing as well as the theoretically optimal Q-critic with a perfect action-value function. This is in contrast to HCA and HCA-return which perform barely better than REINFORCE, all failing to learn to consistently pick up the key in the given number of episodes. This result translates to the setting of learning the underlying models using neural networks. COCOA-reward and -feature outperform competing policy gradient estimators in terms of sample efficiency while HCA only improves over REINFORCE. Notably, having to learn the full action-value function leads to a less sample-efficient policy gradient estimator for the Q-critic.

In Figure 3C and D we leverage dynamic programming to compare to the ground truth policy gradient. This analysis reveals that improved performance of COCOA is reflected in a higher SNR compared to other estimators due to its favorable bias-variance trade-off. Fig 12 in App. F indicates that COCOA maintains a superior SNR, even when using significantly biased contribution coefficients. As predicted by our theory in Section 3.3, HCA significantly underperforms compared to baselines due to its high variance caused by spurious contributions. In particular, the Markov state representation of the linear key-to-door environment contains the information of whether the key has been picked up. As a result, HCA always credits picking up the key or not, even for distractor rewards. These spurious contributions bury the useful learning signal of the treasure reward in noisy distractor rewards. HCA-return performs poorly as it is a biased gradient estimator, even when using the ground-truth hindsight distribution (c.f. Appendix F and L). Interestingly, the variance of COCOA is significantly lower compared to a state-of-the-art control variate method, TrajCV, pointing to a potential benefit of the multiplicative interactions between contribution coefficients and rewards in the COCOA estimators, compared to the additive interaction of the control variates: the value functions used in TrajCV need to approximate the full average returns, whereas COCOA can ignore rewards from the distractor subtask, by multiplying them with a contribution coefficient of zero.

### COCOA enables long-term credit assignment by disentangling rewarding outcomes.

The linear key-to-door environment allows us to parametrically increase the difficulty of the long-term credit assignment problem by increasing the distance between the key and door and thereby increasing the variance due to the distractor task . Figure 1B reveals that as this distance increases, performance measured as the average fraction of treasure collected over a fixed number of episodesdrops for all baselines including HCA but remains relatively stable for the COCOA estimators. Hung et al.  showed that the SNR of REINFORCE decreases inversely proportional to growing distance between key and door. Figure 1B shows that HCA and all baselines follow this trend, whereas the COCOA estimators maintain a robust SNR.

We can explain the qualitative difference between COCOA and other methods by observing that the key-to-door task consists of two distinct subtasks: picking up the key to get the treasure, and collecting apples. COCOA can quickly learn that actions relevant for one task, do not influence rewarding outcomes in the other task, and hence output a contribution coefficient equal to zero for those combinations. Value functions in contrast estimate the expected sum of future rewards, thereby mixing the rewards of both tasks. When increasing the variance of the return in the distractor task by increasing the number of stochastic distractor rewards, estimating the value functions becomes harder, whereas estimating the contribution coefficients between state-action pairs and distinct rewarding objects remains of equal difficulty, showcasing the power of disentangling rewarding outcomes.

To further showcase the power of disentangling subtasks, we consider a simplified version of the _task interleaving_ environment of Mesnard et al.  (c.f. Fig. 4A, App. E.7). Here, the agent is faced with a sequence of contextual bandit tasks, where the reward for a correct decision is given at a later point in time, together with an observation of the relevant context. The main credit assignment difficulty is to relate the reward and contextual observation to the correct previous contextual bandit task. Note that the variance in the return is now caused by the stochastic policy, imperfectly solving future tasks, and by stochastic state transitions, in contrast to the linear key-to-door environment where the variance is caused by stochastic distractor rewards. Figure 4B shows that our COCOA algorithms outperform all baselines. The learned contribution coefficients of COCOA reward accurately capture that actions in one context only contribute to rewards in the same context as opposed to HCA that fails to disentangle the contributions (c.f. Figure 4C).

### Learned credit assignment

**features allow for disentangling aliased rewards**

For COCOA-reward we use the scalar reward value to identify rewarding outcomes in the hindsight distribution, i.e. \(U=R\). In cases where multiple rewarding outcomes yield an identical scalar reward value, the hindsight distribution cannot distinguish between them and has to estimate a common hindsight

Figure 4: **COCOA improves performance by disentangling subtasks.** (A) Schematic representation of the task interleaving environment where colored borders indicate the context of a room. (B) Performance of COCOA and baselines with learned contribution coefficients or value functions, measured as the fraction of correct choices. (C) Visualization of the contribution coefficient magnitudes of each query room on reward rooms for COCOA (top) and HCA (bottom), c.f. App. E.7.

Figure 5: **COCOA-features is robust to reward aliasing.** On a version of the linear key-to-door environment where one of the distractor reward values has the same magnitude as the treasure reward, COCOA-reward can no longer distinguish between the distractor and treasure reward and as a result its performance decreases. COCOA-feature is robust to this manipulation since it relies on learned features to distinguish rewarding objects.

probability, making it impossible to disentangle the tasks and hence rendering learning of the contribution coefficients potentially more difficult. In contrast, COCOA-feature learns hindsight features of rewarding objects that are predictive of the reward. Even when multiple rewarding objects lead to an identical scalar reward value their corresponding features are likely different, allowing COCOA-feature to disentangle the rewarding outcomes.

In Fig. 5, we test this _reward aliasing_ setting experimentally and slightly modify the linear key-to-door environment by giving the treasure reward the same value as one of the two possible values of the stochastic distractor rewards. As expected, COCOA-feature is robust to reward aliasing, continuing to perform well on the task of picking up the treasure while performance of COCOA-reward noticeably suffers. Note that the performance of all methods has slightly decreased compared to Fig. 3, as the magnitude of the treasure reward is now smaller relative to the variance of the distractor rewards, resulting in a worse SNR for all methods.

## 5 Discussion

We present a theory for model-based credit assignment compatible with discrete actions and show in a comprehensive theoretical and experimental analysis that this yields a powerful policy gradient estimator, enabling long-term credit assignment by disentangling rewarding outcomes.

Building upon HCA , we focus on amortizing the estimation of the contribution coefficients in an inverse dynamics model, \(p^{}(a s,u^{})\). The quality of this model is crucial for obtaining low-bias gradient estimates, but it is restricted to learn from on-policy data, and rewarding observations in case of \(u=r\). Scaling these inverse models to complex environments will potentially exacerbate this tension, especially in sparse reward settings. A promising avenue for future work is to leverage forward dynamics models and directly estimate contribution coefficients from synthetic trajectories. While learning a forward model is a difficult problem in its own, its policy independence increases the data available for learning it. This would result in an algorithm close in spirit to Stochastic Value Gradients  and Dreamer  with the crucial advance that it enables model-based credit assignment on discrete actions. Another possibility to enable learning from non-rewarding observations is to learn a generative model that can recombine inverse models based on state representations into reward contributions (c.f. App. H).

Related work has explored the credit assignment problem through the lens of transporting rewards or value estimates towards previous states to bridge long-term dependencies . This approach is compatible with existing and well-established policy gradient estimators but determining how to redistribute rewards has relied on heuristic contribution analyses, such as via the access of memory states , linear decompositions of rewards  or learned sequence models . Leveraging our unbiased contribution analysis framework to reach more optimal reward transport is a promising direction for future research.

While we have demonstrated that contribution coefficients with respect to states as employed by HCA suffer from spurious contributions, any reward feature encoding that is fully predictive of the reward can in principle suffer from a similar problem in the case where each environment state has a unique reward value. In practice, this issue might occur in environments with continuous rewards. A potential remedy in this situation is to assume that the underlying reward distribution is stochastic, smoothing the contribution coefficients as now multiple states could have led to the same reward. This lowers the variance of the gradient estimator as we elaborate in App. G.

Finally, we note that our contribution coefficients are closely connected to causality theory  where the contribution coefficients correspond to performing _Do-interventions_ on the causal graph to estimate their effect on future rewards (c.f. App I). Within causality theory, counterfactual reasoning goes a step further by inferring the external, uncontrollable environment influences and considering the consequences of counterfactual actions _given that all external influences remain the same_. Extending COCOA towards this more advanced counterfactual setting by building upon recent work  is an exciting direction for future research (c.f. App. I).

**Concluding remarks.** By overcoming the failure mode of _spurious contributions_ in HCA, we have presented here a comprehensive theory on how to leverage model information for credit assignment, compatible with discrete action spaces. COCOA-reward and COCOA-feature are promising first algorithms in this framework, opening the way towards sample-efficient reinforcement learning by model-based credit assignment.

## 6 Acknowledgements

We thank Angelika Steger, Yassir Akram, Ida Momennejad, Blake Richards, Matt Botvinick and Joel Veness for discussions and feedback. Simon Schug is supported by the Swiss National Science Foundation (PZ00P3_186027). Seijin Kobayashi is supported by the Swiss National Science Foundation (CRSII5_173721). Simon Schug would like to kindly thank the TPU Research Cloud (TRC) program for providing access to Cloud TPUs from Google.