# Towards Data-Agnostic Pruning At Initialization:

What Makes a Good Sparse Mask?

 Hoang Pham\({}^{1}\), The-Anh Ta\({}^{2}\), Shiwei Liu\({}^{3,6}\), Lichuan Xiang\({}^{4}\), Dung D. Le\({}^{5}\),

**Hongkai Wen\({}^{4}\), Long Tran-Thanh\({}^{4}\)**

\({}^{1}\) FPT Software AI Center, \({}^{2}\) CSIRO's Data61, \({}^{3}\) University of Texas at Austin,

\({}^{4}\) University of Warwick, \({}^{5}\) VinUniversity \({}^{6}\) Eindhoven University of Technology

hoang.pv1692@gmail.com, theanh.ta@csiro.au,

{L.Xiang.2, Hongkai.Wen, Long.Tran-Thanh}@warwick.ac.uk

dung.ld@vinuni.edu.vn, shiwei.liu@austin.utexas.edu

###### Abstract

Pruning at initialization (PaI) aims to remove weights of neural networks before training in pursuit of training efficiency besides the inference. While off-the-shelf PaI methods manage to find trainable subnetworks that outperform random pruning, their performance in terms of both accuracy and computational reduction is far from satisfactory compared to post-training pruning and the understanding of PaI is missing. For instance, recent studies show that existing PaI methods only able to find good layerwise sparsities not weights, as the discovered subnetworks are surprisingly resilient against layerwise random mask shuffling and weight re-initialization. In this paper, we study PaI from a brand-new perspective - the topology of subnetworks. In particular, we propose a principled framework for analyzing the performance of PaI methods with two quantities, namely, the number of effective paths and effective nodes. These quantities allow for a more comprehensive understanding of PaI methods, giving us an accurate assessment of different subnetworks at initialization. We systematically analyze the behavior of various PaI methods through our framework and observe a guiding principle for constructing effective subnetworks: _at a specific sparsity, the top-performing subnetwork always presents a good balance between the number of effective nodes and the number of effective paths._ Inspired by this observation, we present a novel data-agnostic pruning method by solving a multi-objective optimization problem. By conducting extensive experiments across different architectures and datasets, our results demonstrate that our approach outperforms state-of-the-art PaI methods while it is able to discover subnetworks that have much lower inference FLOPs (up to 3.4\(\)). Code is available at: https://github.com/pvhl602/NPB.

## 1 Introduction

Deep neural networks have achieved state-of-the-art performance in a wide range of machine learning applications [3; 13; 39; 38]. However, the huge computational resource requirements limit their applications, especially in edge computing and other future smart cyber-physical systems [23; 48; 37; 47; 2]. To overcome this issue, a number of approaches have been proposed to reduce the size of deep neural networks without compromising performance, among which _pruning_ has received voluminous attention [10; 24; 8]. Traditional pruning approaches mainly focus on accelerating inference, which usually require a pre-trained dense network [33; 20; 31].

As large language models (LLMs) [3; 43; 44] continue to gain popularity, endeavors start to explore the possibility to prune models before training while matching the dense performance. Lottery TicketHypothesis (LTH) [15; 4; 5] provides empirical evidence for this research goal by discovering sparse subnetworks that can be trained from scratch to the comparable performance of the dense network. However, LTH typically involves the costly iterative pruning and re-training process, whose overall cost is much more than training a dense network.

This issue raises an intriguing research question: How to identify sparse, trainable subnetworks at initialization without pre-training? Specifically, a successful pruning before training method can significantly reduce both the cost of memory and runtime, without sacrificing performance much . This would make neural networks applicable even in scenarios with scarce computing resources [1; 47]. As such, many methods for Pal have been proposed [26; 42; 9; 45; 1; 29]. While these methods are based on a number of intuitions (e.g., leveraging the gradient information [26; 45]), they typically measure the importance of network parameters. Their performance in terms of both accuracy and computational reduction is far from satisfactory compared to post-training pruning and the understanding of Pal is missing. More recently, Frankle et al. , Su et al.  observe a rather surprising phenomenon: for Pal methods, layerwise shuffling connections of pruned networks does not reduce the network's performance, which suggests that layerwise sparsity ratios might be more important than weight-level importance scores. This indicates that in searching for good subnetworks at initialization, the topology of subnetworks, particularly, the number of input-output paths and active nodes, plays a vital role and should be investigated more extensively.

In this paper, we first present a **counter-argument** against the previous findings: while Frankle et al.  show that PaI methods are insensitive to random shuffling, we find this is not true in the extreme sparsity regime (\(>99\%\)), as the number of effective paths heavily suffers from random shuffling. In layerwise shuffling experiments (see Section 3.3), shuffling connections results in more effective nodes but substantially fewer input-output paths. In normal sparsity levels, shuffling weights in regular sparsities can maintain and even increase effective parameters  and the number of activated nodes . This increases representation capacity and performance of the subnetwork. However, at extremely sparse levels, shuffling still preserves roughly the same number of effective nodes, but the performance of shuffled subnetworks drops significantly compared to their unshuffled counterparts. This is because random weight shuffling damages effective paths and hampers information flow within the subnetwork. These findings suggest that separately considering effective paths or nodes is insufficient to fully capture the behavior of subnetworks generated by PaI methods.

To underscore the critical importance of nodes and paths in designing network architecture, we conduct an analysis of their quantities in the context of Neural Architecture Search (NAS). For instance, Figure 1 illustrates the relationship between candidate's performance, nodes and paths in NAS-Bench-Macro  which has \(6,561\) models (more in Section 3.4). To ensure fairness in terms of model size, we compare networks with similar number of parameters (i.e., in a range of 300k parameters in Figure 1). Remarkably, architectures exhibiting a higher number of input-output paths and active neurons concurrently yield superior performance. This highlights the crucial role of simultaneously considering both node and path in the successful design of networks at initialization.

To improve upon the current understanding of PaI methods, we introduce a novel framework from the perspective of subnetwork topology to _provide a more accurate and comprehensive explanation of the performance of different approaches_. In particular, we propose the joint usage of both the number of input-output paths (a.k.a. effective paths) and activated (i.e., effective) nodes to interpret behaviors of different PaI methods in a more comprehensive way (see Section 3.3 for more details). Inspired by the guiding principle observed through our framework, we further introduce NPB, a novel pruning at initialization by solving a node-path balancing objective. We show that _NPB outperforms state-of-the-art PaI methods in almost all settings, while achieving much lower inference FLOPs_. In summary, our main contributions are:

Figure 1: The accuracy of architectures in NAS-Bench-Macro benchmark along with the number of effective nodes and paths.

* We propose a unified effect node-path perspective to understand the behavior of PaI, particularly considering metrics of effective nodes and paths as proxies for the performance of PaI methods (Section 3). We revisit the layerwise shuffling sanity check on PaI methods and provide unified explanations for their behaviors based on these metrics in a wide range of sparsities (Section 3.3).
* We discover a new relation between the proposed metrics and the performance of subnetworks, termed the Node-Path Balancing Principle, that suggests a non-trivial balance between nodes and paths is necessary for optimal performance of PaI (Section 4).
* We present a novel data-agnostic pruner, NPB, by solving a multi-objective optimization problem to give a proof-of-concept for our principle. With extensive experiments, we show that NPB outperforms the best state-of-the-art PaI methods, PHEW, on 11 out of 12 settings while ours produces more efficient subnetworks with much fewer inference FLOPs (up to 3.4\(\)) and faster in pruning time than PHEW (up to 14.5\(\)) (Section 5).

## 2 Related Work

Neural Network Pruning.Neural network pruning methods [25; 21; 20; 12; 32] traditionally focus on pruning trained models based on pre-defined criteria, and then resulting subnetworks will be fine-tuned to converge. Recently, [15; 16; 4; 5] empirically show the existence of randomly initialized subnetworks (lottery tickets) which when trained from scratch or in early training iterations, that can achieve competitive performance with their dense counterparts. Unfortunately, finding lottery tickets is computationally expensive due to the train and prune cycle [16; 15]. Gradual pruning methods [49; 18] interleave the pruning and training, which are usually cheaper than pruning after training, but the network still needs to be trained to choose the ideal sparse subnetwork. Other methods [6; 7] apply one-shot pruning during training to further reduce the computational cost. Dynamic sparse training [30; 14; 28; 27], on the other hand, start with a (random) sparse network and update network connectivity during training. While pruning before training methods [26; 45; 36; 42; 1] determine subnetworks by the network initialization, gradient information, and network topology. However, experimental results done by Frankle et al. , Su et al.  show that current criteria of PaI methods may not be sufficient to obtain a subnetwork with good performance.

Pruning and Network Shape.Since PaI methods do not utilize training data [42; 36] or use only negligible portions of data [26; 45] to obtain gradient information without training, the configuration of nodes and connections is an essential source of information for optimizing the performance of pruned networks. It turns out that some PaI methods implicitly optimize certain aspects of network shape. In particular, SynFlow  preserves the number of input-output paths as synaptic strength, but often creates more isolated neurons in pruned networks. The works of Patil and Dovrolis  and Gebhart et al.  aim to preserve proxies in terms of path kernels which are also directly related to the shape of subnetworks. Furthermore, while PHEW  additionally implements random walks to increase the number of effective nodes, it unintentionally decreases the number of input-output paths. Our new point of view on node-path balancing would be helpful to systematically optimize network configuration for better performance. Other works also consider the number of effective nodes and effective paths to capture the capacity of pruned subnetworks [45; 34] where these numbers are considered separately.

## 3 Methodology

### Pruning at Initialization Methods

Given a \(L\) layer neural network, we denote \(=(w_{1},,w_{L})\) as the set of vectorized weights. Pruning generates binary mask vectors \(m_{}\{0,1\}^{}}{{d_{}}}}\) (\(d_{}\) is the number of connections in layer \(\)) yielding sparse neural networks with sparse weights \(m_{} w_{}\) - the elementwise product of masks and weights. Sparsity is defined as the fraction of weights being removed: \(s=1-}{ d_{}}\).

A pruning method usually consists of two operations: _score_ and _remove_, where _score_ takes as input weights of the network and outputs an important score for each weight: \(z_{}=score(w_{})^{}\); then _remove_ takes as input the scores \(=(z_{1},,z_{L})\) and the sparsity \(s\), and outputs the masks \(m_{}\) with overall sparsity \(s\). Pruning can be done in one-shot or iteratively. For one-shot pruning, we only generate the scores once, then prune the network up to sparsity \(s\). For iterative pruning, we repeat the processes of the score, then prune from sparsity \(s^{(t-1)/T}\) to \(s^{t/T}\) repeatedly \(T\) times.

**Random.** This method assigns each connection with a random score from a uniform distribution \((0,1)\). Random pruning empirically prunes each layer to target sparsity \(s\).

**SNIP.** SNIP was introduced by Lee et al.  with the pruning objective of reducing connection sensitivity to the training loss. One passes a mini-batch of data through the network and computes the score \(\) for weight \(\) of SNIP as \(=|_{}|\).

**Iterative SNIP.** This is an iterative variant of SNIP  with the same important score. But, iterative SNIP gradually prunes the remaining weights with lowest scores from sparsity \(s^{}\) to sparsity \(s^{}\) iteratively \(T\) times for \(t=1,2,,T\).

**SynFlow.** SynFlow  is an iterative and data-agnostic PaI method. The pruning objective of SynFlow is to make the network remains connected until the most extreme possible sparsity. The weight scores are computed as follows. One first replaces all weights in the network by their absolute values. Then, an \(1\) input tensor is passed through the network, and one computes the sum of the logits as \(R=^{}(_{f=1}^{L}|w_{f})\). Finally, the score of weight \(\) is computed as \(=|_{}R|\). SynFlow prunes the network iteratively \(T\) times.

**PHEW.** PHEW  is also an iterative and data-independent PaI method. It selects a set of input-output paths to be preserved. These paths are chosen through random walks, biased towards higher-weight magnitudes. The selection starts with a unit that is selected through round robin procedure. This process continues until the subnetwork achieves the predefined sparsity.

**ERK.** Erdos-Renyi (ER) first used by Mocanu et al.  to sparsify Multilayer Perceptron (MLP) networks using a random topology that allocates higher sparsity to larger layers. Evci et al.  extends ER to a convolutional version called Erdos-Renyi-Kernel (ERK) which scales the sparsity of the convolutional layer in proportion to the number of neurons/channels in a layer.

### Metric Definition

In a sparse network, it is intuitively clear that one should arrange the connections into a configuration neither too thin nor too spread-out to have good information propagation during training. For a better measurement, we propose using two metrics to evaluate the quality of subnetworks. Please refer to Appendix B for detailed discussions and Python code for calculating the metrics.

**Effective Path.** We define a path to be _effective_ if it connects an input node to an output node without interruption (see Figure 2). Metrics based on paths are mentioned in  as \(l_{1}\) and \(l_{2}\) path norms, respectively. In this paper, we only take into account the number of paths.

**Effective Node/Channel.** A node/channel is effective if at least one effective path goes through it (demonstrated as the right part in Figure 2). This concept is also considered in . For convolutional layers, we consider a kernel as a connection, and a channel as a node, and then convert the convolutional layer into a fully connected layer (see the left part in Figure 2).

### Layerwise Shuffling Phenomenon

In this section, we investigate the intriguing phenomenon that layer-wise reshuffling the subnetwork found by PaI methods still produces competitive accuracy . Based on metrics, we provide a

Figure 2: An example of effective paths and effective nodes.

new way to understand why reshuffling subnetworks work and when they fail. We first use three PaI methods, i.e., SNIP , SynFlow , and PHEW , to find the subnetworks. Then, we randomly shuffle the pruning mask \(m_{l}\). All the subnetworks (both unmodified and shuffled) are trained in the same setting. Finally, we compute the ratio of the number of active paths and nodes after and before pruning and visualize the average scores in Figure 3.

In SNIP and SynFlow, the number of effective nodes drops significantly as sparsity increases (blue lines in second columns in Figure 3). After reshuffling, the connections are distributed uniformly in each layer leading to wider subnetworks while the number of effective paths decreases. In contrast, PHEW focuses on increasing the number of effective nodes by gradually adding new paths such that the network is as wide as possible. Consequently, reshuffling hurts the concrete network configuration and then reduces both the number of effective nodes and effective paths as sparsity is higher.

At sparsity levels below 99%, layerwise shuffled subnetworks demonstrate competitive performance or even better than unmodified counterparts, especially with SynFlow and SNIP. In addition to effective connection preservation as discussed in prior work , _the representation capacity of shuffled sparse networks is enhanced with SynFlow and SNIP, attributed to the increase in the number of effective nodes_ while maintaining considerable input-output paths. In more details, Figure 3 shows that these two metrics for SynFlow and SNIP shuffled versions closely resemble to unmodified subnetworks of PHEW with corresponding sparsities.

However, when the network becomes more sparse, the number of input-output paths decreases substantially (along with the reduction of effective parameters see Appendix D). Even though layerwisely shuffled networks become wider, _the limited number of effective paths damages the information flows in the subnetworks_. These explain why the accuracy of shuffled subnetworks is reduced significantly compared to the unmodified ones in intensive sparsities.

These observations indicate that increasing the number of active paths or nodes alone might not be sufficient in the design of PaI methods. We hypothesize that to have good subnetworks, the number of effective paths and effective nodes should be concurrently considered. If we balance these two metrics well, the performance after the training of subnetworks will be enhanced.

### NAS Observations

Our research delves into the fundamental aspects of network architecture design by investigating the interplay between nodes and paths. Going beyond the realm of pruning literature, we conduct an analysis of node and path metrics in the NAS benchmark. It is noteworthy that networks in NAS are dense networks. In particular, we focus on NAS-Bench-Macro , a macro benchmark comprising 8 searching layers. Each layer offers three candidate blocks, resulting in a staggering 6,561 distinct networks with parameter counts ranging from 300k to 3M. To ensure a fair comparison

Figure 3: Layerwise shuffling results on various sparse subnetworks of ResNet20 produced by SNIP, SynFlow, and PHEW on CIFAR-10.

among candidates, we specifically consider networks with similar parameter counts within the 300k range. We compute metrics across four different parameter ranges and visualize them in Figure 4.

In the context of NAS, it's important to consider that other aspects beyond node-path balance can contribute to final classification accuracy. For instance, in the case of NAS-Bench-Macro, networks with similar node-path balance but varying classification accuracies can be affected by other architectural configurations, such as differences in pooling layers, kernel sizes, and expansion ratios in the MobileNet-v2 block. These architectural variances often result in different numbers of parameters, influencing the overall network performance. However, in the sparse neural network context, pruning methods focus on maintaining the same network structure while pruning connections within the network based on specific sparsities. Consequently, our experiments did not explore other architectural elements beyond node and path balance. In general, we can see that networks with larger numbers of nodes and paths tend to exhibit higher performance. Our findings unveil a strong correlation between the two metrics (node, path) and the final performance of network candidates, both playing pivotal roles in achieving superior performance. This highlights the critical significance of both nodes and paths in the initial stages of designing subnetworks that yield exceptional results.

## 4 Node-Path Balancing Principle

### Node-Path Balancing Principle

From observations in Section 3.3 and NAS observations in Section 3.4, both effective paths and nodes have shown their critical roles in the performance of subnetworks. We now formally state the _Node-Path Balancing Principle_ (NPB): The combination of both the numbers of effective nodes and effective paths is a proxy for the potential performance of subnetworks under pruning. A pruning at initialization method which produces pruned subnetworks with too many effective paths (resp. effective nodes) will have less than necessary the number of effective nodes (resp. effective paths) and consequentially has suboptimal performance. It is necessary to balance the number of effective nodes and effective paths for better performance of pruned subnetworks.

### Proposed Method: NPB

Building upon the aforementioned principle, we elegantly transform the pruning problem into a multi-objective optimization problem. Specifically, our objective is to maximize both the number of effective nodes and effective paths concurrently given an architecture and desired sparsity ratio. We formulate this intriguing problem as follows:

Given an architecture \(A\) with parameter \(^{N}\) where \(N\) is the total number of parameters and sparsity ratio \(s\). Denote \(f_{p}\) as the total number of input-output paths, \(f_{n}\) as the number of activated nodes, and consider the mask for parameter \(=\{0,1\}^{N}\) as variable to solve.

\[}{}  f_{n}+(1-)f_{p}\] \[s.t \|\|_{1} N(1-s)\]

where \(\) is a coefficient. Solving node-path balancing objective globally over the whole neural networks is a non-trivial problem. We sidestep this challenging issue by solving a sequence of easy problems to obtain good approximated solutions. In particular, we propose an approximation for solving this problem by doing layer by layer through convex optimization. The approximation problem is solved efficiently via the available convex optimization library.

Figure 4: The accuracy of network candidates in NAS-Bench-Macro benchmark along with the number of activated nodes and paths in different parameter ranges.

To be simple, we consider a linear layer as an example, in which, \(v\) denotes the node, \(l\) is the layer index, \(^{(l)}^{(l^{} k^{}+1)}\) is the mask for layer \(l\) in which \(h^{(l)}\) the number of neurons in layer \(l\). We denote \(P(v_{i}^{(l)})\) as the number of path go to node \(v_{i}\) in layer \(l\) (e.g., at input layer \(P(v_{i}^{(0)})=1\)). We formulate the pruning problem in layer \(l\) as an optimization problem in which we maximize the number of paths from input to layer \(l+1\)\(f_{p}^{(l+1)}\) and the total activated nodes \(f_{n}^{(l+1)}\) in two layers \(l\) and \(l+1\). For brevity, we denote them as \(f_{p}\) and \(f_{n}\). We set \(^{(l)}\) as:

\[m_{ij}^{(l)}=0&\\ 1&\]

The number of paths to \(v_{j}^{(l+1)}\) in layer \(l\)+1 is sum of all paths to nodes in layer \(l\) connecting to \(v_{j}^{(l+1)}\)

\[P(v_{j}^{(l+1)})=_{i}^{k^{(l)}}m_{ij}^{(l)}P(v_{i}^{(l)})\]

Then the total number of effective paths to layer \(l+1\) is:

\[f_{p}(m)=_{j}^{k^{(l+1)}}P(v_{j}^{(l+1)})\] (1)

A node \(v_{i}\) in layer \(l\) is activated if and only if there are paths pass through it and edges connect from it to nodes in layer \(l+1\), which is formulated as below:

\[P(v_{i}^{(l)})_{j}^{k^{(l+1)}}m_{ij}^{(l)} 1(P(v_{i} ^{(l)})_{j}^{k^{(l+1)}}m_{ij}^{(l)};1)=1\]

And a node \(v_{j}\) in layer \(l+1\) is effective when there exist attached nodes in layer \(l\) that connect to it,

\[_{i}^{k^{(l)}}m_{ij}^{(l)}P(v_{i}^{(l)}) 1(_{i} ^{k^{(l)}}m_{ij}^{(l)}P(v_{i}^{(l)});1)=1\]

The layer-wise objective for the node becomes:

\[f_{n}(m)=_{i}^{k^{(l)}}(P(v_{i}^{(l)})_{j}^{k^{(l+1)}}m_{ij}^{(l) };1)+_{j}^{k^{(l+1)}}(_{i}^{k^{(l)}}m_{ij}^{(l)}P(v_{i}^{(l)});1)\] (2)

In convolution layers with a kernel of height \(h\) and width \(w\), we let the variable \(m_{ij}\) have the value from \(0\) to \(hw\) representing this kernel. After solving \(\), in each kernel, we assign \(|m_{ij}|\) parameters to entries whose initialized weights have the highest magnitude.

Along with two objectives, we also consider a regularization term which aims to encourage activating as many kernels as possible in each layer. Besides, since we optimize the node and path per layer, the solution will not be the global one. We can view the regularizer as an adjustment term, which moves the local solution around to figure out the better ones.

\[R=_{i}_{j}min(m_{ij}^{(l)}-1;\,0)\] (3)

From Equations 1, 2, and 3 the final objective becomes:

\[^{(l)}}{}  f_{n}+(1-)f_{p}+ R\] (4) \[s.t ||^{(l)}||_{1} N^{(l)}(1-s^{(l)})\] (5)

where \(\) is a coefficient to control the balance, \(\) is a hyperparameter, and the constraint is the number of unpruned parameters that satisfies the sparsity. To ensure the same scale between elements in the main objective function, we normalize the number of paths, nodes, and the regularizer in each layer with their maximum possible values, respectively. Note that, optimizing nodes is much easier than paths so we select small values of alpha which can be prior knowledge. In particular, in Section 5, we fix \(=0.01\) and \(=1\) to all settings. We optimize 1 the network structure sequentially layer by layer from input to output in which the layer-wise sparsity level is found by ERK method . We describe our method in Algorithm 1 and the pseudo code for optimizer in Appendix C.

```
1:Inputs: Final sparsity \(s\), weights \(_{0}=\), balancing coefficient \(\), and hyperparameter \(\)
2: Obtain layer-wise sparsity \(s^{(l)}\) by using ERK method
3: Define unit input \(= P^{(l)}=\)
4:for\(l=0,,L\)do
5:\(^{(l)}(^{(l)},s^{(l)},P^{(l)})\) # solve Eq.4
6: Set \(_{0}^{(l)}=^{(l)}\)
7: Extract layer \(l+1\) output when put \(\) to the network with \(_{0}\) to obtain \(P^{(l+1)}\)
8:endfor
9:Return: Mask M ```

**Algorithm 1** Node-Path Balancing Pruner

## 5 Evaluation

### Experimental Settings

We conduct experiments on three standard datasets: CIFAR-10, CIFAR-100, and Tiny-Imagenet. Following , we use ResNet-20 for CIFAR-10, VGG-19 for CIFAR-100, and a variant ResNet-18 with 18 layers for Tiny-Imagenet. We treat weights of all convolutional and linear layers as prunable, but we do not prune biases and weights in batch normalization layers. We run five seeds with experiments on CIFAR-10, CIFAR-100, and three seeds on Tiny-Imagenet. The average results are used for visualizations. Since the number of active paths is orders of magnitude larger than that of active nodes, and both numbers are of exponential scales, we take logarithm base 10 of the number of active paths, and logarithm base \(e\) (natural logarithm ) of the number of active nodes to visualize the results. To ensure the fair comparison between NPB and other baselines, we fix \(=0.01\) and \(=1\) in NPB for all settings. More details on our experimental setting are in Appendix A.

### Comparison with PaI Methods

Figure 5 shows the experimental results for different PaI methods: Random, SNIP, Iter-SNIP, SynFlow, PHEW, and our NPB, with different sparsities on three settings: VGG-19 on CIFAR-100, ResNet-20 on CIFAR-10, and ResNet-18 on Tiny-Imagenet. Overall, our method NPB shows significant improvements on standard PaI methods, both data-dependent like Iter-SNIP, SNIP (improve up to

Figure 5: The number of effective paths (log scale), nodes (in scale), and the corresponding accuracy of different PaI methods on three datasets in different sparsity levels. Best accuracy is in blue.

14%) and data-independent like SynFlow (increase up to 7%) in all three settings. _Notably, NPB achieves better results than state-of-the-art Pal method PHEW in almost all settings (11 out of 12 settings). Importantly, we set the same alpha and beta hyperparameters for NPB across all three settings, thereby eliminating the need for extensive hyperparameter search._ This demonstrates the robustness and generalizability of our method, making it easily applicable to various settings with consistently high performance. Ablation studies on alpha and beta can be found in Appendix G.

We discuss the experimental results from the point of view of the Node-Path Balancing principle and network shape optimization. In Figure 5, SynFlow and Iter-SNIP produce sparse networks with large number of input-output paths, which results from iterative pruning procedures. These methods gradually remove nodes of low connection degree, which makes the subnetworks become narrower while maintaining the high number of paths. As a consequence of a very high number of input-output paths, the width of resulting subnetworks is significantly reduced, limiting the representation capacity of the subnetworks , leading to suboptimality.

Random pruning is a Pal method which produces subnetworks with the very high number of active nodes. On average, it distributes parameters uniformly to all layers and kernels, creating subnetworks of large width yet a low number of active paths. At sparsities below 90%, with an adequate number of input-output paths, subnetworks of Random pruning still perform well and are even competitive with more sophisticated Pal methods. This phenomenon is exhibited clearly in experiments on the more complex dataset Tiny-Imagenet. However, when sparsity increases over 90%, Random pruning creates a huge number of ineffective paths and parameters. Consequently, the performance of subnetworks drops significantly. We again observe the necessity to balance the optimization of the number of nodes and paths.

In Figure 5, our method NPB and PHEW produce sparse networks which, in visualization, lie in specific areas which have higher paths than Random and greater nodes than SynFlow, or Iter-SNIP. We call these are _balancing regions_. Subnetworks in these regions have broader widths and considerably many paths, which provides good representation capacity and preserves the information flow of the network. These two factors together contribute to better training of pruned subnetworks leading to superior performance. This illustrates the effectiveness of our principle.

One drawback of one-shot pruning methods like SNIP is creating a large number of ineffective parameters, which decreases the capacity and performance of subnetworks. Although our method is also one-shot, it efficiently distributes the parameters by direct optimization. Consequently, NPB enjoys a better node-path balancing level and better performance (up to 14%) compared to SNIP.

Results of all settings in Figure 5 provide evidence to support our Node-Path Balancing principle and the existence of a specific balancing region between nodes and paths at given sparsity levels. Particularly, we posit that when the number of active neurons and input-output paths of a sparse network fall within this balancing range, it will probably have a good performance after training.

### Pruning Time and FLOPs Reduction

Since NPB optimizes layer by layer, the complexity of NPB depends on network architecture (i.e., the number of layers and the size of each layer). With large layers, we sidestep the time-consuming issues by making a further step where we divide the layer into chunks with the same sparsity constraint. In particular, we split nodes in the next layer into equal parts while the input node is fixed. Then, optimizing chunks in a layer can be solved in parallel. Thank to the available convex optimization libraries we can find subnetworks more efficiently and quickly. We have computed the pruning time of our proposed method and compared it with other Pal methods in Table 1. Our pruning time is not

    &  &  & ^{6}\))} \\  Sparsity (\%) & 68.38 & 90.00 & 96.84 & 99.00 & 68.38 & 90.00 & 96.84 & 99.00 & 68.38 & 90.00 & 96.84 & 99.00 \\  SNIP & 56.99 & 53.43 & 48.77 & 36.02 & 5.14 & 4.95 & 5.55 & 5.64 & 11.35 & 5.77 & 3.04 & 1.55 \\ Iter-SNIP & 56.73 & 53.60 & 48.55 & 36.42 & 229.16 & 235.34 & 233.19 & 231.23 & 10.73 & 7.05 & 3.98 & 1.97 \\ SynFlow & 56.71 & 54.68 & 49.03 & 39.79 & 108.17 & 96.18 & 91.15 & 92.60 & 14.71 & 8.91 & 4.24 & 1.50 \\ PHEW & 58.09 & 55.93 & 50.81 & 40.54 & 551.120 & 1342.03 & 471.23 & 324.78 & 14.29 & 8.35 & 3.92 & 1.50 \\ NPB & **58.39** & **56.82** & **51.37** & **41.05** & 380.52 & 375.65 & 384.32 & 387.89 & 14.37 & 5.21 & 1.74 & 0.59 \\   

Table 1: Accuracy, pruning time (in seconds) and FLOPs of subnetworks for different pruning methods and compression ratios on Resnet18 - Tiny-ImageNet.

significantly slow compared to those iterative approaches (e.g., Iter-SNIP, SynFlow) while it is much faster (up to 14.5\(\)) than PHEW in lower sparsity levels.

Besides pruning time, we find that the FLOPs reduction of subnetwork after pruning is more important in the context of pruning before training. We have measured FLOPs of subnetworks produced by different methods in Table 1. The result indicates that our NPB can produce subnetworks with lower FLOPs than other baselines while NPB outperforms PaI methods. More details are in Appendix E.

## 6 Conclusion

In this paper, we present a novel framework for studying PaI methods that systematically employs the configuration of pruned subnetworks based on two different metrics: the number of effective paths and the number of effective nodes. Through our framework, we discover a new relationship between these metrics, called the Node-Path Balancing Principle, which provides guidance for understanding and optimizing PaI methods. Our framework offers unified explanations for the intriguing layerwise connection reshuffling phenomenon  of subnetworks produced by PaI methods in normal pruning sparsity regime, as well as the failure of this phenomenon in extreme sparsity levels. Furthermore, we propose a novel pruning method based on optimization approach, namely NPB, that demonstrates the potential of balancing the numbers of effective paths and nodes to improve the performance of PaI methods. Extensive experiments conducted on various model architectures and datasets show that NPB outperforms the best baseline, PHEW, on almost settings while ours produces more efficient subnetworks with much fewer inference FLOPs and faster in pruning time then PHEW. Our new perspective on the configuration of subnetworks, in terms of effective nodes and effective paths, provides new insights into the working mechanism of PaI methods and opens new research directions on neural network pruning methods, as well as designs of sparse neural network.