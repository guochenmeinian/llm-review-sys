# Fully Distributed, Flexible Compositional Visual Representations via Soft Tensor Products

Bethia Sun

Correspondence to: bethia.sun@unsw.edu.au

Maurice Pagnucco

School of Computer Science and Engineering

UNSW Sydney

Yang Song

School of Computer Science and Engineering

UNSW Sydney

###### Abstract

Since the inception of the classicalist vs. connectionist debate, it has been argued that the ability to systematically combine symbol-like entities into compositional representations is crucial for human intelligence. In connectionist systems, the field of disentanglement has gained prominence for its ability to produce explicitly compositional representations; however, it relies on a fundamentally _symbolic, concatenative_ representation of compositional structure that clashes with the _continuous, distributed_ foundations of deep learning. To resolve this tension, we extend Smolensky's Tensor Product Representation (TPR) and introduce _Soft TPR_, a representational form that encodes compositional structure in an inherently _distributed, flexible_ manner, along with _Soft TPR Autoencoder_, a theoretically-principled architecture designed specifically to learn Soft TPRs. Comprehensive evaluations in the visual representation learning domain demonstrate that the Soft TPR framework consistently outperforms conventional disentanglement alternatives - achieving state-of-the-art disentanglement, boosting representation learner convergence, and delivering superior sample efficiency and low-sample regime performance in downstream tasks. These findings highlight the promise of a _distributed_ and _flexible_ approach to representing compositional structure by potentially enhancing alignment with the core principles of deep learning over the conventional symbolic approach.

## 1 Introduction

Compositional structure, capturing the property of being decomposable into a set of constituent parts, permeates numerous aspects of our world - from the recursive application of syntax in language, to the parsing of richly complex visual scenes into their constituent parts. Given this ubiquity, it is natural to seek deep learning representations that also embody compositional structure. Indeed, empirical evidence demonstrates a multitude of benefits conferred by explicitly compositional representations, including increased interpretability [10; 12], reduced sample complexity [30; 33], increased fairness [20; 25; 41], and improved performance in out-of-distribution generalisation [33; 48; 50].

We consider the following, intuitive notion of compositional representations. A representation of compositionally-structured data is a _compositional representation_ if its structure explicitly reflects the constituency structure of the represented data . In the visual representation learning domain, data is clearly _compositionally-structured_, as images can be decomposed into a set of constituent _factors of variation_ (FoVs), e.g., \(\{\}\) for the image in Figure 1.

A widely explored framework for learning explicitly compositional representations is that of _disentanglement_. We adopt the conventional [5; 25; 33; 43], intuitive definition of a _disentangled representation_, which states a representation, \((x)\), is disentangled if each data constituent (FoV) can be mapped onto a distinct dimension or contiguous group of dimensions in \((x)\), effectively establishing a 1-1 correspondence between FoVs and distinct representational _parts_. Framed in this way, it is clear that disentangled representations explicitly encode the data's constituency structure and are thus, compositional representations by nature. The majority of state-of-the-art disentanglement approaches use a variational autoencoder backbone, and rely on weak supervision [13; 22; 31; 33; 35], or a penalisation of the aggregate posterior \( q(z|x)p(x)dx\)[10; 14; 17; 24; 26; 32] to promote disentanglement. More recent approaches depart from the restrictive assumptions of a variational framework, and instead use standard autoencoding , or energy-function based optimisation , with additional inductive biases to encourage disentanglement. Despite their methodological diversity, disentanglement methods share a unifying principle: by enforcing the 1-1 correspondence between FoVs and distinct "slots" in the representation, they produce compositional representations as a dimension-wise _concatenation_ of scalar-valued or vector-valued FoV _tokens_[10; 13; 14; 17; 22; 24; 26; 31; 32; 33; 35; 36; 47], as illustrated in Figure 1a. This design utilises a fundamentally localist encoding scheme, where discrete parts of the representation are exclusively dedicated to encoding _single_ FoVs, paralleling symbolic representations, in which discrete representational slots are occupied by individual symbols - e.g., the symbols \(\{,,\}\) in the word '\(\). We theorise that this inherently _symbolic_ method, offering a _localist_ encoding of compositional structure, may be misaligned with the continuous, distributed nature of deep learning models for the following reasons (see A.3 for further details):

1. **Gradient Flow and Learning**: Symbolic compositional representations utilise a localist encoding approach, assigning each FoV to a unique, non-overlapping subset of the dimensions of the representation. This modular separation may introduce practical misalignments with gradient-based optimisation. By restricting each FoV to its own slot, gradients for one FoV are confined to a set of designated dimensions, limiting the smooth propagation of gradients globally across _all_ dimensions of the representation. Consequently, updates to a single FoV provide minimal feedback to other FoVs, hindering the model's ability to jointly refine interdependent components. Furthermore, the localist encoding - where each FoV occupies a small, disjoint subset of the total dimensions - can induce abrupt, discontinuous shifts in the representation when transitioning between FoV updates, potentially complicating convergence.
2. **Representational Expressivity**: The localist encoding inherent in symbolic compositional structures, where each FoV is assigned a distinct subset of the overall dimensions, may constrain practical expressiveness. Specifically, confining each of the \(n\) FoVs to a subset of size \(n/d\) within a \(d\)-dimensional representation causes each FoV to underutilise the full capacity of the \(d\)-dimensional space, potentially limiting the richness and flexibility of the learned representations.
3. **Robustness to Noise**: By confining each FoV to a small, non-overlapping subset of dimensions, symbolic compositional representations become highly vulnerable to dimension-wise noise. Even slight perturbations in a single dimension can significantly impair the representation of the corresponding FoV, as there is no overlapping or redundant encoding to mitigate such disruptions.

Critically, we hypothesise that this potential incompatibility between symbolic, localist representations and the distributed, continuous nature of deep learning results in suboptimal behaviour in models that learn or use these representations. To overcome these limitations, we are motivated to pursue an inherently _distributed_ approach to representing compositional structure. Instead of concatenating discrete slots (_FoV tokens_) dimension-wise to form the compositional representation, an inherently _distributed_ approach _continuously combines_ densely encoded FoVs within a unified vector space. This design allows information from each FoV to be _smoothly interwoven_ throughout all dimensions of the representation, as illustrated in Figure 1b, potentially resulting in smoother gradient flow, enhanced expressivity, and heightened robustness to noise. By reconciling the demand for representations that are explicitly compositional with the _continuous_, _distributed_ nature of deep learning, distributed compositional representations offer a compelling alternative to traditional symbolic, slot-based paradigms.

Pioneered by Smolensky, the Tensor Product Representation  is a specific representational form that encodes compositional structure in an inherently _distributed_ manner. At the crux of it, TPRs are formed by _continuously blending_ representational components together into the overall representation, in a manner analogous to superposing multiple waves together to produce a complex waveform, as illustrated in Figure 1b. For a representation to qualify as a TPR, it must adhere to a highly specific mathematical form, which confers upon the TPR valuable structural properties (elaborated on in Section 3.2), but also imposes two limitations (see B.1 for further details). First, as depicted by the stars in Figure 1c, only a discrete subset of points in the underlying representational space, \(\), satisfies the stringent mathematical criteria to qualify as TPRs. Consequently, to learn TPRs, representation learners must map from the data manifold onto this _discrete subset_, which constitutes a highly constrained and inherently challenging learning task. Second, the TPR specification enforces a strict, algebraic definition of compositional structure, limiting the TPR's ability to flexibly represent ambiguous, real-world data which is often _quasi_-compositional, only _approximately_ adhering to some rigid, formal definition. Historically, these limitations have confined TPR learning to formal domains characterised by explicit, algebraic structure - as evidenced by the near exclusive deployment of TPRs in language [19; 23; 28; 34; 52] - and, to contexts where strong supervision from highly structured downstream tasks is available to steer the representation learning process [23; 28; 38; 51]. To negate these drawbacks and extend distributed compositional representations to weakly supervised, non-formal domains, we propose _Soft TPR_, which can be thought of as a _continuous relaxation_ of the traditional TPR, as illustrated by the translucent circular regions in Figure 1c. At its core, the Soft TPR is designed to promote representational flexibility and ease of learning while simultaneously preserving the structural integrity of the traditional TPR. We additionally introduce _Soft TPR Autoencoder_, a theoretically-principled _weakly-supervised_ architecture for learning Soft TPRs, used to operationalise the Soft TPR framework in the visual representation learning domain.

Our main contributions are threefold: i) We propose a novel compositional representation learning framework, introducing the _distributed_, _flexible_ Soft TPR compositional form, alongside a dedicated, weakly-supervised architecture, _Soft TPR Autoencoder_, for learning this form. ii) Our framework is the first to learn _distributed compositional representations_ in the non-formal, less explicitly algebraic domain of _vision_. iii) We empirically affirm the far-reaching benefits produced by the Soft TPR framework, demonstrating that Soft TPRs achieve state-of-the-art disentanglement, accelerate representation learner convergence, and provide downstream models with enhanced sample efficiency and superior low-sample regime performance.

## 2 Related Work

**Disentanglement**: In aiming to produce explicitly compositional representations without strong supervision, our work shares the same objective as disentangled representation learning. Prior to the highly influential work of  which proved the impossibility of learning disentangled representations without supervision or other inductive biases, disentangled representations were learnt in a completely unsupervised fashion [8; 10; 14; 17; 24; 26; 37]. Our use of weak supervision is inspired by the work [13; 22; 31; 33; 35] relating to this highly influential impossibility result. In particular, we leverage the type of weak supervision termed'match pairing' , where pairs, \((x,x^{})\), differing in values for a subset of known FoVs are presented to the model, to incentivise disentanglement. Our work,

Figure 1: (a) Disentangled representations can be conceptualised as a _concatenation_ of FoV tokens (coloured blocks), enforcing a _symbolic, string-like_ compositional structure, where each FoV is allocated to a discrete _slot_ in the representation. We instead, consider a _distributed_ representation of compositional structure, (b), where information from densely encoded FoV (first 6 waves) are _continuously combined_ together to form the representation, \((x)\) (in red), effectively distributing the information from _multiple_ FoVs into a _single_ dimension of \((x)\). (c) Only a subset of points (stars) in the underlying representational space (rainbow manifold) satisfy the TPR specification. The Soft TPR relaxes this, capturing larger, _continuous regions_ of the underlying representational space (the translucent circles), while approximately preserving the TPR’s key properties.

however, fundamentally diverges from all disentanglement work we are aware of, by adopting an inherently _distributed_ representation of compositional structure, which contrasts with the inherently _symbolic_, localist representations of compositional structure characterising existing work.

**TPR-based Work**: Existing TPR-based approaches generate distributed representations of compositional structure by producing an element with the explicit mathematical form of a TPR. To learn this highly specific form, these approaches rely on the algebraic characterisation of compositionality present in formal domains, such as mathematics , or language [19; 23; 28; 34; 52] in addition to strong supervision signals from highly structured downstream tasks, such as part-of-speech tagging , and answering structured language [28; 51] or mathematics questions . In contrast, _Soft TPR_ eases these stringent constraints by offering a continuously relaxed specification of compositional structure. This allows our approach to extend distributed representations of compositional structure to the orthogonal and less algebraically structured domain of _visual_ data, while also reducing reliance on annotated data by instead using weak supervision to learn this relaxed representational form.

## 3 Preliminaries

### A Formal Framework for Compositional Representations

We adopt a generalised, non-generative version of the definition of _compositional representations_ from . Let data \(x X\) be _compositionally-structured_ if there exists a decomposition function \(:X A_{1} A_{n}\) decomposing \(x\) into constituent parts (FoVs), i.e. \((x)=\{a_{1},,a_{n}\}\), where \(a_{i} A_{i}\). A mapping \(:X V_{F}\) then produces a _compositional representation_ if \((x)=C(_{1}(a_{1}),,_{n}(a_{n}))\), where each \(_{i}:A_{i} V_{i}\) is a _component function_ that independently embeds a part \(a_{i}\) (i.e., a particular FoV) into a vector space, and \(C:V_{1} V_{n} V_{F}\) is a _compositional function_ that combines these embedded parts to form the overall representation. This construction enforces a faithful correspondence between the constituency structure of the data \(\{a_{i}\}\) and that of the representation, \(\{_{i}(a_{i})\}\) (provided \(C\) is invertible).

We formalise a _symbolic_ compositional representation, \(_{s}(x)\), as one in which \(C\) is given by a concatenation operation. Concretely, \(_{s}(x)=(_{1}(a_{1})^{T},,_{n}(a_{n})^{T})^{T}\). Indeed, the disentanglement approaches of [8; 10; 13; 14; 17; 22; 24; 26; 31; 32; 33; 35; 36; 37; 47] all fit this framework, concatenating scalar or vector-valued FoV tokens together to produce the representation. While this design may be fundamentally at odds with the _continuous_, _distributed_ nature of deep learning (as detailed in Section 1), it provides one clear benefit: the embedded FoVs, \(\{_{i}(a_{i})\}\) are trivially recoverable by partitioning \(_{s}(x)\).

In many scenarios, readily recovering the data constituents (i.e., FoVs), \(\{a_{i}\}\), from the compositional representation \((x)\) is essential for practical utility. Here, we assume each \(_{i}\) is invertible, so direct recoverability of the data constituents, \(\{a_{i}\}\), from the representation \((x)\) is guaranteed if the embedded FoVs, \(\{_{i}(a_{i})\}\), remain structurally separable in the representation, \((x)\). Motivated by this requirement, we aim to construct compositional representations that: 1) achieve an inherently _distributed_ encoding of compositional structure by _smoothly interweaving_ FoVs throughout _all_ dimensions of \((x)\), and 2) preserve the direct recoverability of the embedded FoVs.

### The TPR Framework

Smolensky's Tensor Product Representations (TPR)  provides one compelling realisation of an inherently _distributed_ encoding of compositional structure that, under certain conditions, preserves the _direct recoverability_ of the embedded FoVs. We briefly review key aspects here, with additional details and formal proofs deferred to Appendix A. The TPR framework conceptualises a compositionally-structured object, \(x\), as comprising a number, \(N_{R}\), of roles, where each role \(r R\) is bound to a corresponding filler \(f F\), with this binding being denoted by \(f/r\). Thus, compositionally-structured objects are decomposable into a set of role-filler bindings, \((x)=\{f_{i}/r_{i}\}_{i=1}^{N_{R}}\). In natural language, where the TPR is most commonly deployed [19; 28; 34; 52], roles may correspond to grammatical categories (e.g., \(\)) and fillers specific words (e.g., \(\)). Translating this role-filler formalism to the visual domain, we reinterpret roles as FoV _types_ (e.g., \(\)), and fillers as FoV _values_ (e.g., \(\)). Using this role-filler formalism to decompose the Shapes3D image in Figure 1, we have \((x)=\{\}\).

To construct a TPR, the roles and fillers for each binding in \(x\) are independently embedded via role and filler embedding functions, \(_{R}:R V_{R},_{F}:F V_{F}\) respectively. The binding \(f/r\) is then encoded by taking a tensor product (denoted by \(\)) over the embedded role, \(_{R}(r)\), and embedded filler, \(_{F}(f)\). Summing over the embeddings for all role-filler bindings in \(x\) yields the TPR, \(_{ptr}(x)\):

\[_{ptr}(x):=_{i}_{F}(f_{m(i)})_{R}(r_{i}),\] (1)

where \(m:\{1,,N_{R}\}\{1,,N_{F}\}\) is a matching function2 associating each of the \(N_{R}\) roles to the filler it binds to in the decomposition of \(x\) (i.e., \((x)=\{f_{m(i)}/r_{i}\}\)).

We now situate the TPR within the formal framework of Section 3.1, by observing that it defines each representational component, \(_{i}(a_{i})\) as an embedded role-filler binding, \(_{F}(f_{m(i)})_{R}(r_{i})\), and adopts _ordinary vector space addition_ as its composition function, \(C\). Unlike the _concatenation_ in symbolic, localist schemes, the TPR _additively superposes_ all embedded FoVs in the same underlying vector space, producing an inherently _distributed_ representation of compositional structure in which information from _multiple_ role-filler bindings can be smoothly interwoven into the _same_ dimensions.3

A natural question arises from defining \(C\) as ordinary vector space addition: can we recover each representational component - the embedded role-filler binding, \(_{i}(f_{m(i)})_{R}(r_{i})\) - from the TPR, which is their sum? Remarkably, despite the non-injectivity of the summation operation, the TPR's specific algebraic structure enables faithful recoverability of all representational components from the TPR, through a process referred to as _unbinding_ (see A.2 for more details). More concretely, provided that the role embedding vectors \(\{_{R}(r_{i})\}\) are linearly independent and known, the embedded filler bound to the \(i\)-th role can be _unbound_ from the representation, \(_{ptr}(x)\), by taking a (tensor) inner product between \(_{trr}(x)\) and the \(i\)-th _unbinding_ vector, \(u_{i}\):

\[_{trr}(x)u_{i}=(_{i}_{F}(f_{m(i)})_{R}(r_{i})) u_{i}=_{F}(f_{m(i)}),\] (2)

where \(u_{i}\) is a vector corresponding to the \(i\)-th column of the (left) inverse of the matrix obtained by taking all (linearly independent) role embeddings as columns. Repeating this _unbinding_ procedure using each of the \(N_{R}\) unbinding vectors recovers each of the embedded fillers bound to the \(N_{R}\) roles, and thus, the entire set of embedded role filler bindings, \(\{_{F}(f_{m(i)})_{R}(r_{i})\}\). Hence, provided that the linear independence of the roles holds, the TPR offers an inherently _distributed_ representation of compositional structure that nonetheless retains the chief benefit of _symbolic_ approaches - direct recoverability of each representational component.

## 4 Methods

### Soft TPR: An Extension to the TPR Framework

While Tensor Product Representations offer a robust framework for encoding compositional structure in a distributed manner, their rigid representational form imposes practical limitations. The core limitation lies in TPR's strict algebraic specification of compositional structure as a set of discrete bindings, each of which comprise a single role and a single filler. This specification precludes the representation of more ambiguous, quasi-compositional structures that cannot be neatly decomposed in this way. For instance, even in the highly compositional domain of natural language, there exist phenomena such as idiomatic expressions that resist straightforward decomposition into role-filler pairs because their meanings cannot be directly inferred from their composite parts. Such examples illustrate that real-world compositional structures may require more flexible representation than what is offered by traditional TPRs. Consequently, TPRs may struggle to model more nuanced forms of compositional structure that characterise less structured, non-formal data domains, such as vision. Additionally, TPRs present a challenging learning task. Their representational form \(_{trr}(x):=_{i}_{F}(f_{m(i)})_{R}(r_{i})\) is only satisfied by a _discrete subset_ of points within the underlying representational space, \(V_{F} V_{R}\), as illustrated in Figure 0(c). This constraint forces representation learners to map data to a highly restricted, discrete set of points, making learning inherently arduous.

To overcome these shortcomings, we introduce _Soft TPR_, a continuous relaxation of the traditional TPR specification. Soft TPRs allow any point in the underlying representational space \(V_{F} V_{R}\) that _approximately_ conforms to the TPR specification. This removes the need for representations to perfectly adhere to the rigid structural specification of the TPR, instead permitting them to _flexibly approximate_ this rigid form. This flexibility provides two main advantages: **1. Enhanced Representational Flexibility**: Soft TPR facilitates the representation of _quasi_-compositional structures that do not perfectly adhere to the traditional TPR's structural specification. This includes scenarios where bindings are approximate rather than exact, or where multiple fillers influence a single role.

**2. Improved Ease of Learning**: The relaxed representational specification allows representation learners to map data to broader regions of the representational space (as illustrated by the translucent circular regions in Figure 0(c)). This may facilitate more efficient learning, as there are more possible mappings from the data to the required representational form.

Formally, to define the Soft TPR, we consider the vector space underlying TPRs, \(V_{F} V_{F}\), produced by an arbitrary role embedding function \(_{R}:R V_{R}\) and an arbitrary filler embedding function \(_{F}:F V_{F}\). A _Soft TPR_ is defined as any element \(z V_{F} V_{R}\) in this underlying representational space that is sufficiently close to a traditional TPR, \(_{trp}\)4, as measured by a chosen distance metric (we take the Frobenius norm). Denoting the Frobenius norm by \(||||_{F}\), Soft TPR satisfies \(||z-_{trp}||_{F}<\), where \(\) is some small, scalar-valued positive quantity. This condition ensures that the Soft TPR approximately encodes the TPR-based compositional structure of \(_{trp}\), while allowing for slight deviations which capture a more flexible, relaxed notion of compositionality that cannot be reduced to a set of role-filler bindings (see B.2).

Despite this relaxation, Soft TPRs retain the critical advantage of traditional TPRs: the ability to recover constituent role-filler bindings through the unbinding operation, albeit approximately. Specifically, when unbinding a Soft TPR \(z\), the operation yields soft filler embeddings \(_{i}\) that closely approximate the true filler embeddings \(_{F}(f_{m(i)})\). Formally, for a Soft TPR \(z\), performing the unbinding operation for the \(i\)-th role yields:

\[zu_{i} _{trp}u_{i}=(_{i}_{F}(f_{m(i)}) _{R}(r_{i}))u_{i}=_{F}(f_{m(i)}),\] (3) \[=_{F}(f_{m(i)})+_{i}=:_{i},\] (4)

where \(_{i}=zu_{i}-_{F}(f_{m(i)})\) represents the approximation error. This approximate recoverability ensures that Soft TPRs _implicitly encode_ a precise, algebraically-expressible form of compositional structure, even when their representational form _deviates_ from the exact TPR specification.

### Soft TPR Autoencoder: Learning Distributed and Flexible Compositional Representations

We define our vector spaces of interest over the reals as \(V_{F}:=^{D_{F}}\) and \(V_{R}:=^{D_{R}}\) where \(D_{F},D_{R}\) denote the dimensionality of the filler and role embedding spaces. The pivotal insight underlying our method is that a _Soft TPR_ can be effectively realised by ensuring that the encoder produces representations that are amenable to quantisation into explicit TPRs. Specifically, since a Soft TPR is any arbitrary element from the vector space \(^{D_{F} D_{R}}\)5 sufficiently close to an explicit TPR, any \((D_{F} D_{R})\)-dimensional vector produced by an encoder in a standard autoencoding framework can be treated as a Soft TPR _candidate_. This realisation suggests that a conventional autoencoding framework only requires modest modifications to generate Soft TPRs.

Our Soft TPR Autoencoder comprises three main components: a standard encoder, \(E\), the TPR decoder, and a standard decoder, \(D\). The encoder output, \(z\), serves as the Soft TPR. The overarching intuition guiding our architecture is to ensure that \(z\) can be effectively quantised or decoded into an explicit TPR, thereby enforcing the Soft TPR property, \(||z-_{trp}||_{2}<\).

**Representational Form**: To ensure that the encoder produces elements with the _form_ of a Soft TPRs, we introduce a mechanism that penalises the Euclidean distance \(||z-_{trp}^{*}||_{2}\) between the encoderoutput, \(z\), and some explicit TPR, \(^{*}_{tpr}\), that \(z\) best approximates. To obtain \(^{*}_{tpr}\) needed to compute the loss, we derive an explicit analytical form for \(^{*}_{tpr}\) and construct elements satisfying this analytical form using a role embedding matrix, \(M_{_{R}}\) containing \(N_{R}\)\(D_{R}\)-dimensional role embedding vectors, \(\{_{R}(r_{i})\}\), and a filler embedding matrix, \(M_{_{F}}\) containing \(N_{F}\)\(D_{F}\)-dimensional filler embedding vectors, \(\{_{F}(f_{i})\}\). To define an explicit analytical form for \(^{*}_{tpr}\), we use a greedily optimal selection of filler embeddings based on their proximity to the soft filler embeddings extracted from \(z\):

\[^{*}_{tpr}:=_{i}_{F}(f_{m(i)})_{R}(r_{i}),m(i):=*{arg\,min}_{j}||_{k}-_{F}(f_{j})||_{2}, { and }_{k}:=zu_{i}.\] (5)

That is, we define \(^{*}_{tpr}\) as the TPR constructed from explicit filler embeddings \(_{F}(f_{j})\) with the smallest Euclidean distance to the _soft_ filler embeddings \(_{k}\) of \(z\). To construct elements satisfying (5), we use a three-step process carried out by the novel TPR decoder we introduce, visible in Figure 2: **1) Unbinding**: Utilising a randomly-initialised dense role embedding matrix \(M_{_{R}}\), the unbinding module extracts soft filler embeddings \(\{_{i}\}\) by performing the TPR _unbinding_ operation on \(z\). We elaborate on our theoretically-informed reason for this design, how the unbinding vectors are obtained, and for not backpropagating gradient to \(M_{_{R}}\) in B.4.1. **2) Quantisation**: The quantisation module, containing a learnable filler embedding matrix \(M_{_{F}}\), employs the VQ-VAE vector quantisation algorithm  to both 1) learn the explicit filler embeddings, and 2) quantise the soft filler embeddings \(\{_{1},,_{N_{R}}\}\) produced by the unbinding module into the explicit filler embeddings \(\{_{F}(f_{1}),,_{F}(f_{N_{R}})\}\) with the smallest Euclidean distances. **3) TPR Construction**: The quantised filler embeddings \(\{_{F}(f_{m(i)})\}\) are then bound to their respective roles using the tensor product, and all embedded bindings are summed together to produce the explicit TPR \(^{*}_{tpr}\) with the form of Eq 5. \(^{*}_{tpr}\) is subsequently used by the decoder, \(D\), to reconstruct the input image. The overall unsupervised loss, \(_{u}\), comprises three components, where \(\) denotes the stop-gradient operator, \(\) is a hyperparameter controlling the commitment loss in VQ-VAE, and \(_{r}\) is a suitable image-based reconstruction loss (we use \(L_{2}\)):

\[_{u}:=_{tpr}||_{2}^{2}}_{}+_{r}(x,D(^{*}_{tpr}))}_{}+}(||[_{F}(f_{m(i)})]-_{i} ||_{2}^{2}+||_{F}(f_{m(i)})-[_{i}]||_{2}^{2}) }_{}.\] (6)

**Representational Content**: While the unsupervised loss \(_{u}\) ensures that \(z\) maintains the Soft TPR form by being close to an explicit TPR, it does not guarantee that the content of \(z\) accurately reflects the true role-filler semantics of the data. To address this, we introduce a weakly supervised loss that aligns \(z\) with the ground-truth semantics of the image. We employ a match-pairing context similar to , where image pairs \((x,x^{})\) share the same role-filler bindings for all but one role, \(r_{i}\). The identity of \(r_{i}\) is known, but not any of the specific fillers or bindings. Our intuition is that, for \(z\) to accurately reflect the semantics of the image, the Euclidean distance between the quantised fillers of \(x\) and \(x^{}\) bound to role \(r_{i}\) should be maximal, relative to the distances between the filler embeddings pairs for all other roles \(r_{j},j i\). To encourage this, we apply the cross entropy loss corresponding to the 3rd term in Eq 7, where \( q\) denotes the \(N_{R}\)-dimensional vector with each dimension

Figure 2: Diagram illustrating the Soft TPR Autoencoder. We encourage the encoder \(E\)’s output, \(z\), to have the form of a Soft TPR by penalising its distance with the greedily defined, explicit TPR, \(^{*}_{tpr}\) of Equation 5 that \(z\) best approximates. \(^{*}_{tpr}\) is recovered using a 3 step process performed by our TPR decoder (center rectangle): 1) unbinding, 2) quantisation, and 3) TPR construction. The decoder, \(D\), reconstructs the input image using \(^{*}_{tpr}\).

populated by the Euclidean distance between the quantised fillers of \(x\) and \(x^{}\) for role \(r_{k}\), and \(l\) denotes the one-hot vector of dimension \(N_{R}\) with the index for \(r_{i}\) set to 1. Additionally, like , we incorporate a reconstruction loss (the 2nd term of Eq 7) to enforce consistency when swapping the quantised filler embeddings for role \(r_{i}\). Specifically, we construct new TPRs by swapping the quantised fillers for \(r_{i}\) between \(x\) and \(x^{}\) to generate \(^{s}_{tpr}(x)\) and \(^{s}_{tpr}(x^{})\), and ensure that the decoder can accurately reconstruct the corresponding swapped images from these swapped TPRs.

Combining these components, our final loss, \(\), is a weighted sum over the unsupervised and weakly supervised loss components, where \(_{1}\) and \(_{2}\) are hyperparameters:

\[:=_{u}+_{1}(_{r}(x,D( ^{s}_{tpr}(x^{})))+_{r}(x^{},D(^{s }_{tpr}(x))))+_{2}( q,l).\] (7)

## 5 Results

To evaluate our Soft TPR framework, we perform evaluation along three dimensions that are standard in the compositional representation learning literature [25; 30; 33; 42]: **1) Compositional Structure / Disentanglement**: To what extent do Soft TPR representations capture _explicitly compositional structure?_**2) Representation Learner Convergence**: How _efficiently_ can representation learners acquire Soft TPR's _distributed, flexible_ compositional structure? **3) Downstream Model Performance**: Does Soft TPR's _distributed, flexible_ compositional form offer tangible benefits for _downstream models_ utilising compositional representations?

We benchmark against a suite of weakly supervised disentanglement baselines: Ada-GVAE , GVAE , ML-VAE , SlowVAE , and the GAN-based model of , which we henceforth refer to as 'Shu'. These models produce symbolic compositional representations corresponding to a concatenation of _scalar-valued_ FoV tokens. Similar to our approach, Ada-GVAE, GVAE, ML-VAE, SlowVAE, and Shu are trained with paired samples (\(x,x^{}\)) sharing values for all but a subset of FoVs types (roles), \(I\), with ML-VAE, GVAE, SlowVAE, and Shu assuming access to \(I\), matching our model's level of supervision. To ensure a fair comparison, we modify Ada-GVAE (method detailed in Appendix C.2.2) for more direct comparability, denoting our modification by Ada-GVAE-k. Additionally, we benchmark against 2 baselines producing symbolic _vector-to-keted_ compositional representations: COMET , and Visual Concept Tokeniser (VCT) , although these are fully unsupervised and not directly comparable to our method. We train five instances of each representation learning model using five random seeds for 200,000 iterations across all datasets, and report averaged results (an unabridged suite of all results is contained in the Appendix).

### Compositional Structure / Disentanglement

To demonstrate that Soft TPRs inherently capture the algebraic compositional structure required for effective disentanglement, we first quantise each Soft TPR, \(z\), into its corresponding explicit TPR, \(^{*}_{tpr}\) using the TPR decoder (note that once the model is trained, quantisation is fully deterministic). This quantisation process transforms the implicit compositional information encoded within Soft TPRs into an explicit algebraic form suitable for evaluation. We then apply standard disentanglement metrics across benchmark datasets including Cars3D , MPI3D , and Shapes3D . Table 1 illustrates that Soft TPR consistently outperforms symbolic compositional baselines across all datasets, achieving state-of-the-art results with notable DCI metric improvements of \(29\%\) on Cars3D and \(74\%\) on the more challenging MPI3D dataset. To ensure these gains are attributable to Soft TPR framework rather than merely a marginal increase in model capacity (Soft TPR Autoencoder adds between 1,600-13,568 model parameters to a standard (variational) autoencoder), we conduct control experiments by equalising parameter counts in applicable baselines. Consistent with findings from prior work , we observe no performance improvements in relevant symbolic baselines when parameter counts are increased (see C.2.4).

### Representation Learner Convergence Rate

To explore how the Soft TPR framework influences the convergence rate of representation learners, we analyse representations produced at varying stages of representation learner training (100, 1,000, 10,000, 100,000 and 200,000 iterations), and evaluate both their **1) Disentanglement**, and **2) Downstream Utility**. We quantify downstream utility of learned representations by examining the performance of downstream models on two tasks common in the disentanglement literature [25; 33; 42; 30]: a classification-based task assessing the model's ability to perform abstract visual reasoning  and a regression task involving the prediction of continuous FoV values for the disentanglement datasets . Our findings indicate that the Soft TPR framework generally achieves faster disentanglement convergence, particularly on Cars3D and MPI3D datasets (see Appendix C.4.1). Moreover, Soft TPR consistently accelerates the learning of useful representations for _both_ downstream tasks. The downstream performance improvements are particularly pronounced in the low iteration regime - for instance, at only 100 iterations of representation learner training, as shown in Tables 2 and 3. For fair comparison, we embed baseline representations of both higher and lower dimensionality into the same vector space as Soft TPR, which we denote by \(\), and select the best-performing variant (original or dimensionality-matched) for each baseline (details in Appendix C).

### Downstream Models

To evaluate whether the _distributed_ and _flexible_ encoding of compositional structure offered by the Soft TPR benefits downstream models, we examine both: **1) Sample Efficiency** and **2) Raw Performance in Low Sample Regimes**, using the previously mentioned abstract visual reasoning and FoV regression tasks. In line with , we quantify sample efficiency with a ratio-based metric comparing downstream model performance using restricted sample sizes (100, 250, 500, 1,000, and 10,000) against performance when trained using all samples. As illustrated in Table 4, Soft TPR substantially outperforms baseline models in sample efficiency, especially in the most restrictive case involving _only 100_ samples, achieving a \(93\%\) improvement. Additionally, Soft TPR produces substantial raw performance increases in the low sample regime, as evidenced by the \(138\%\) and \(168\%\) improvements in Table 4 for the low-sample regimes of 100 and 200 samples respectively, and the \(30\%\) improvement in Table 5.

### Ablation Studies

To disentangle the contributions of Soft TPR's foundational components, we conduct ablation studies focusing on two critical properties that are hypothesised to influence the Soft TPR's learning of compositional representations: **1) Relaxed Representational Constraints** and **2) Distributed Encoding**, denoted RRC and DE respectively in Table 7. For **Relaxed Representational Constraints**, we significantly increase the weighting of the form penalty \(\|z-^{*}_{trpr}\|_{2}\) in the Soft TPR loss to enforce rigid representational constraints on the Soft TPR Autoencoder, effectively forcing it to

    &  &  & MPI3D \\   & FactorVAE score & DCI score & FactorVAE score & DCI score & FactorVAE score & DCI score \\   \\  Slow-VAE & 0.902 \(\) 0.035 & 0.509 \(\) 0.027 & 0.950 \(\) 0.032 & 0.850 \(\) 0.047 & 0.455 \(\) 0.083 & 0.355 \(\) 0.027 \\ Ada-GVAE-k & 0.947 \(\) 0.064 & _0.664 \(\) 0.167 & _0.975_ \(\) 0.006 & **0.963 \(\) 0.077** & 0.496 \(\) 0.095 & 0.343 \(\) 0.040 \\  & GVAE & 0.877 \(\) 0.081 & 0.262 \(\) 0.095 & 0.921 \(\) 0.075 & 0.842 \(\) 0.040 & 0.378 \(\) 0.024 & 0.245 \(\) 0.074 \\ ML-VAE & 0.870 \(\) 0.052 & 0.216 \(\) 0.063 & 0.835 \(\) 0.111 & 0.739 \(\) 0.115 & 0.390 \(\) 0.026 & 0.251 \(\) 0.029 \\ Shu & 0.573 \(\) 0.062 & 0.032 \(\) 0.014 & 0.265 \(\) 0.043 & 0.017 \(\) 0.006 & 0.287 \(\) 0.034 & 0.033 \(\) 0.008 \\   \\  VCT & _0.966 \(\) 0.029_ & 0.382 \(\) 0.080 & 0.957 \(\) 0.043 & 0.8984 \(\) 0.013 & 0.689 \(\) 0.035 & _0.475 \(\) 0.005_ \\ COMET & 0.339 \(\) 0.008 & 0.024 \(\) 0.026 & 0.168 \(\) 0.005 & 0.002 \(\) 0.000 & 0.145 \(\) 0.024 & 0.005 \(\) 0.001 \\   \\  Ours & **0.999 \(\) 0.001** & **0.863 \(\) 0.027** & **0.984 \(\) 0.012** & _0.926 \(\) 0.028_ & **0.949 \(\) 0.032** & **0.828 \(\) 0.015** \\   

Table 1: FactorVAE and DCI scores. Additional results in Section C.3.3

    \\   \\  Slow-VAE\({}^{}\) & 0.552 \(\) 0.035 \\ Ada-GVAE-k\({}^{}\) & _0.631 \(\) 0.037_ \\ GVAE\({}^{}\) & 0.541 \(\) 0.031 \\ ML-VAE\({}^{}\) & 0.550 \(\) 0.025 \\ Shu & 0.208 \(\) 0.052 \\   \\  VCT & 0.440 \(\) 0.033 \\ COMET\({}^{}\) & 0.348 \(\) 0.069 \\   \\  Ours & **0.804 \(\) 0.016** \\   

Table 2: FoV regression \(R^{2}\) scores (100 iterations of representation learner training).

produce representations that precisely match the TPR-specified compositional form. For **Distributed Encoding**, we modify the model to encode compositional structure in an inherently less distributed manner, by modifying the sparsity of \(^{*}_{tpr}\) (see C.2.4). Results in Table 7 demonstrate that both the Soft TPR's distributed encoding and relaxed constraints are essential for learning representations with high compositional structure, as quantified by the disentanglement metrics. Additionally, we examine the impact of Soft TPR's flexible, _quasi-compositional_ form on downstream performance by replacing each Soft TPR, \(z\), with its quantised explicit TPR, \(^{*}_{tpr}\) in downstream tasks. Table 6 and Appendix C.6.3 indicates that Soft TPRs provide downstream models with _unique_ performance improvements that explicit TPRs (which are produced under the same conditions) cannot account for. Please see Appendix C.6 for further details and full ablation results, including additional ablations.

### Key Insights

The empirical results of the Soft TPR framework reveal two primary insights. Firstly, deep learning models may more effectively learn precise compositional structures when these structures are 1) _implicitly acquired_ through _relaxed representational constraints_, allowing the model to acquire precise compositional forms (i.e., \(^{*}_{tpr}\)) through quantisation of flexible Soft TPRs, \(z\), and 2) when the compositional structure is assumed to be encoded in a _distributed_ format. Secondly, the _flexible representational form_ of Soft TPR offers downstream models _unique_ advantages by enabling more efficient utilisation of _implicitly_ encoded compositional information compared to their rigid counterparts, \(^{*}_{tpr}\). Together, these comprehensive model benefits empiricially demonstrate the value of the framework's 1) relaxed representational constraints, 2) distributed encoding of compositional structure, and 3) flexible representational form.

## 6 Conclusion

In this work, we tackle a challenge tracing its roots to the conception of the connectionist vs. classicalist debate: the fundamental mismatch between compositional structure and the inherently distributed nature of deep learning. To bridge this gap, we introduce the _Soft TPR_, a new, inherently _distributed_, _flexible_ compositional representational form extending Smolensky's Tensor Product Representation, together with the _Soft TPR Autocoder_, a theoretically-principled architecture designed for learning Soft TPRs. Our _flexible_, _distributed_ framework demonstrates substantial improvements in the visual representation learning domain - enhancing the representation of compositional structure, accelerating convergence in representation learners, and boosting efficiency in downstream models. Future work will extend this framework to hierarchical compositional structures, enabling bound fillers to recursively decompose into role-filler bindings for enhanced representational expressivity.

   Representational form & Cars3D & Shapes3D & MPI3D \\  TPR (100 samples) & 0.361 \(\) 0.031 & 0.447 \(\) 0.108 & 0.415 \(\) 0.050 \\ Soft TPR (100 samples) & **0.638 \(\) 0.010** & **0.481 \(\) 0.081** & **0.531 \(\) 0.069** \\  TPR (250 samples) & 0.537 \(\) 0.053 & 0.669 \(\) 0.108 & 0.598 \(\) 0.081 \\ Soft TPR (250 samples) & **0.832 \(\) 0.027** & **0.728 \(\) 0.021** & **0.621 \(\) 0.072** \\   

Table 6: Downstream FoV \(R^{2}\) scores for TPR and Soft TPR (number of samples in brackets)

   Models & Symbolic scalar-tokensed \\  Slow-VAE & 0.196 \(\) 0.028 \\ Ada-GVAE-k & 0.203 \(\) 0.007 \\ GVAE & 0.182 \(\) 0.013 \\ ML-VAE & 0.193 \(\) 0.012 \\ Shu & 0.200 \(\) 0.010 \\       SVMic vector-tokensed \\  VCT & _0.277 \(\) 0.039_ \\ COMET & 0.259 \(\) 0.016 \\    
   Fully continuous \\  Ours & **0.360 \(\) 0.033** \\   

Table 7: Effect of model properties on disentanglement (MPI3D dataset).

    & Samples & Samples/100 samples \\    & } & } \\  &  Symbolic scalar-tokensed compositional representations \\  }} \\  &  Symbolic scalar-tokensed compositional representations \\  }} \\    Slow-VAE & 0.127 \(\) 0.050 & 0.130 \(\) 0.051 & 0.152 \(\) 0.011 & 0.155 \(\) 0.011 \\ Ada-GVAE-k & 0.206 \(\) 0.031 & 0.270 \(\) 0.037 & 0.213 \(\) 0.023 & 0.279 \(\) 0.026 \\ GVAE & 0.181 \(\) 0.030 & 0.234 \(\) 0.035 & 0.217 \(\) 0.023 & 0.282 \(\) 0.027 \\ ML-VAE & 0.182 \(\) 0.013 & 0.236 \(\) 0.019 & 0.222 \(\) 0.024 & 0.288 \(\) 0.030 \\ Shu & 0.151 \(\) 0.016 & _0.343 \(\) 0.024_ & 0.211 \(\) 0.026 & 0.482 \(\) 0.075 \\       Models & Symbolic scalar-tokensed \\  Slow-VAE & 0.196 \(\) 0.028 \\ Ada-GVAE-k & 0.203 \(\) 0.007 \\ GVAE & 0.182 \(\) 0.013 \\ ML-VAE & 0.193 \(\) 0.012 \\ Shu & 0.200 \(\) 0.010 \\       Models & Symbolic scalar-tokensed \\  Slow-VAE & 0.196 \(\) 0.028 \\ Ada-GVAE-k & 0.203 \(\) 0.007 \\ GVAE & 0.182 \(\) 0.013 \\ ML-VAE & 0.193 \(\) 0.012 \\ Shu & 0.200 \(\) 0.010 \\        &  Symbolic vector-tokensed \\  VCT \\ COMET \\  }} \\  &  Fully continuous \\  }} \\    {