# Sigmoid Gating is More Sample Efficient than

Softmax Gating in Mixture of Experts

 Huy Nguyen Nhat Ho & Alessandro Rinaldo

Department of Statistics and Data Sciences, The University of Texas at Austin

{huynm, minhhnhat}@utexas.edu, alessandro.rinaldo@austin.utexas.edu

Co-last authors.

###### Abstract

The softmax gating function is arguably the most popular choice in mixture of experts modeling. Despite its widespread use in practice, the softmax gating may lead to unnecessary competition among experts, potentially causing the undesirable phenomenon of representation collapse due to its inherent structure. In response, the sigmoid gating function has been recently proposed as an alternative and has been demonstrated empirically to achieve superior performance. However, a rigorous examination of the sigmoid gating function is lacking in current literature. In this paper, we verify theoretically that the sigmoid gating, in fact, enjoys a higher sample efficiency than the softmax gating for the statistical task of expert estimation. Towards that goal, we consider a regression framework in which the unknown regression function is modeled as a mixture of experts, and study the rates of convergence of the least squares estimator under the over-specified case in which the number of fitted experts is larger than the true value. We show that two gating regimes naturally arise and, in each of them, we formulate an identifiability condition for the expert functions and derive the corresponding convergence rates. In both cases, we find that experts formulated as feed-forward networks with commonly used activation such as \(\) and \(\) enjoy faster convergence rates under the sigmoid gating than those under softmax gating. Furthermore, given the same choice of experts, we demonstrate that the sigmoid gating function requires a smaller sample size than its softmax counterpart to attain the same error of expert estimation and, therefore, is more sample efficient.

## 1 Introduction

Mixture of experts (MoE) [15; 17] has recently emerged as a powerful machine learning model that helps scale up the model capacity while requiring a nearly constant computational overhead [34; 8]. In particular, it aggregates multiple sub-models called experts based on a gating network. Here, experts can be formulated as neural networks, and they specialize in different aspects of the data. For instance, in large language models [16; 31; 19; 40; 7; 5; 30], one expert might focus on syntax, another on semantics, whereas the others concentrate on context. Meanwhile, the gating network takes the input data and calculates a set of weights that determine the contribution of each expert. This gating mechanism guarantees that the most relevant experts are assigned more weight for a given input. In addition to language modeling, MoE has also been leveraged in several other applications, including computer vision [32; 20; 33], multi-task learning [12; 10], multi-modal learning , speech recognition [38; 9; 29] and reinforcement learning [3; 28].

In the above applications of MoE, practitioners frequently utilize the softmax gating function to compute the mixture weights. However, the use of softmax function might introduce an unexpected competition among experts, which leads to the representation collapse issue : when the weight of one expert increases, those of the others decrease accordingly. As a consequence, some experts will dominate the decision-making process, and overshadow the contributions of others. Therefore, the ability of MoE models to incorporate the diversity of expert specialization is partially limited. To this end, Csordas et al.  propose using the sigmoid function in place of the softmax to remove the unnecessary expert competition, and empirically demonstrate that it is indeed a better choice of gating function. On the other hand, the theoretical guarantee for that claim has remained missing in the literature. The main objective of this work is to provide a comprehensive analysis of the sigmoid gating function from the perspective of the expert estimation problem. In particular, we will show that sigmoid gating delivers superior sample efficiency for estimating the model parameters and allows for more general expert functions than softmax gating.

**Related works.** A very recent line of research has focused on analyzing the convergence rates for expert estimation in Gaussian MoE models under a variety of assumptions and choices of gating functions. Assuming that data from an input-free gating Gaussian MoE, Ho et al.  demonstrated that the expert estimation rates for the maximum likelihood estimator vary with the algebraic independence among expert functions. Under the same setting but using instead softmax gating, Nguyen et al.  discovered that the expert estimation rates depend on the solvability of a system of polynomial equations resulting from an intrinsic interaction among gating and expert parameters. Subsequently, a Gaussian MoE with Top-K sparse softmax gate, designed to activate only a fraction of experts per input, was studied in . The convergence analysis in that work revealed that activating exactly one expert for each input would eliminate the previous parameter interaction, thus boosting the expert estimation rates. Despite many important insights provided in this line of works, the mixture setting of Gaussian MoE is still far from practical. To that effect, Nguyen et al.  introduced a more realistic regression framework in which, conditionally on the features, the response variables are sampled from noisy realization of an unknown and deterministic softmax gating MoE-type regression function. They found that experts formulated as feed-forward networks with sigmoid or hyperbolic tangent activation functions enjoy faster estimation rates than those equipped with a polynomial activation. In this paper, we adopt the same regression setting and carry out an analogous analysis by considering instead sigmoid gating for the underlying regression function, as opposed to the more popular softmax gating considered in . As we will see, the choice of gating function turns out to be critical, requiring a different type of analysis and leading to markedly different conclusions.

**Problem setup.** We assume that the data \((X_{1},Y_{1}),(X_{2},Y_{2}),,(X_{n},Y_{n})^{d}\) follow a standard regression model

\[Y_{i}=f_{G_{*}}(X_{i})+_{i}, i=1,,n,\] (1)

where the features \(X_{1},X_{2},,X_{n}\) are i.i.d. samples from a probability distribution \(\) on \(^{d}\) and \(_{1},,_{n}\) are independent noise variables such that \([_{i}|X_{i}]=0\) and \((_{i}|X_{i})=\) for all \(1 i n\); for simplicity we further assume that they follow a Gaussian distribution. The unknown regression function \(f_{G_{*}}\) is formulated as a sigmoid-gated MoE with \(k_{*}\) experts, i.e.,

\[x^{d} f_{G_{*}}(x):=_{i=1}^{k_{*}}^{*})^{}x-_{0i}^{*})} h(x,_{i}^{*}),\] (2)

where the _expert function_\(x h(x,)\) is of parametric form, specified by parameter \(^{q}\). The regression function \(f_{G_{*}}\) is fully characterized by the unknown parameters \((_{0i}^{*},_{1i}^{*},_{i}^{*})_{i=1}^{k_{*}}\) in \(^{d}^{q}\), which can be compactly encoded using the associated _mixing measure_\(G_{*}:=_{i=1}^{k_{*}}^{*})}_{(_{1i }^{*},_{i}^{*})}\), a weighted sum of Dirac measures \(\).

**Least squares estimation.** To estimate the unknown parameters \((_{0i}^{*},_{1i}^{*},_{i}^{*})_{i=1}^{k_{*}}\) (equivalently, the ground truth mixing measure \(G^{*}\)), we deploy the least squares method  and focus on the estimator

\[_{n}:=*{arg\,min}_{G_{k}()}_{ i=1}^{n}(Y_{i}-f_{G}(X_{i}))^{2},\] (3)where \(_{k}():=\{G=_{i=1}^{k^{}})} _{(_{i1},_{i})}:1 k^{} k,\ (_{0i},_{1i},_{i})\}\) is the set of all mixing measures with at most \(k\) atoms. Since the true number of true experts \(k_{*}\) is typically unknown in practice, we will assume that the number \(k\) of fitted experts is at least as large, i.e. that \(k>k_{*}\).

**Technical challenges.** The main challenge in analyzing MoE models with sigmoid gating lies in the convergence behavior of the mixture weights. Specficially, since we fit the ground-truth MoE model (2) with a mixture of \(k>k_{*}\), there must be some true atoms \((_{1i}^{*},_{i}^{*})\) fitted by more than one component; we will refer to the corresponding parameters \(_{1i}^{*}\) as _over-specified parameters_. To illustrate, suppose that \((_{1i}^{n},_{i}^{n})(_{11}^{*},_{i}^{*})\) for \(i\{1,2\}\), in probability. Then, to ensure convergence of \(f_{_{n}}\) to \(f_{G_{*}}\) in the \(L^{2}()\)-norm we must have that, for \(i=1,2\),

\[_{i=1}^{2}_{1i}^{n})^{}x-_{0i }^{n})}^{*})^{}x-_{01}^{*})},\]

as \(n\), for \(\)-almost every \(x\). Note that the above limit can be achieved only if \(_{11}^{*}=0_{d}\). As a result, we will consider the two following regimes for the over-specified parameters \(_{1i}^{*}\):

**Regime 1.** All the over-specified parameters \(_{1i}^{*}\) are equal to \(0_{d}\);

**Regime 2.** At least one among the over-specified parameters \(_{1i}^{*}\) is different from \(0_{d}\).

It is worth emphasizing that the second regime presents additional technical challenges, as the least squares estimator \(_{n}\) converges to a mixing measure \(_{G_{k}()_{k_ {*}}()}\|f_{G}-f_{G_{*}}\|_{L^{2}()}\) that is in general different that the true mixing measure \(G_{*}\), as in the first regime.

**Contributions.** In this paper, we carry out a convergence analysis of the sigmoid gating MoE under two regimes of the gating parameters. Our main objective is to compare the sample efficiency between the sigmoid gating and the softmax gating. The contributions of our paper are three-fold, and can be summarized as follows:

**(C.1) Convergence rate for the regression function.** We demonstrate in Theorem 1 that the regression estimation \(f_{_{n}}\) converges to its true counterpart \(f_{G_{*}}\) at the rate of order \(_{P}(n^{-1/2})\), which is parametric on the sample size \(n\). This regression estimation rate is then utilized for determining the expert estimation rates.

**(C.2) Expert estimation rates under the Regime 1.** Under the first regime, we first establish a condition called _strong identifiability_ to characterize which types of experts would yield polynomial estimation rates. In particular, we find out that the rates for estimating experts formulated as feed-forward networks with popular activation functions such as \(\) and \(\) are of polynomial orders. By contrast, those for polynomial experts and input-indepedent experts are slower than any polynomial rates and could be of order \(_{P}(1/(n)\). Such expert convergence behavior is similar to that when using the softmax gating.

**(C.3) Expert estimation rates under the Regime 2.** Under the second regime, the regression estimation \(f_{_{n}}\) converge to a function taking the form of a sigmoid gating MoE which is different from \(f_{G_{*}}\). From our derived _weak identifiability_ condition, it follows that estimation rates for feed-forward expert networks with \(\) or \(\) activation and polynomial experts are of orders \(O_{P}(n^{-1/2})\), which are substantially faster than those when using the softmax gating (see also Table 1). Therefore, it follows that the sigmoid gating is more sample efficient than the softmax gating.

**Outline.** The rest of the paper is organized as follows. In Section 2, we introduce some mild assumptions on the parameters, and then determine estimation rates for the regression function. Next, we establish convergence rates for expert estimation under both Regime 1 and the Regime 2 in Section 3. Then, we conduct some experiments to empirically justify our theory in Section 4. Finally, we provide a discussion on the practical implications, limitations and future directions of our work in Section 5. Full proofs of the results and additional experiments are deferred to the Appendices.

**Notation.** For any \(n\), we denote \([n]\) as the set \(=\{1,2,,n\}\). Additionally, for any set \(S\), we refer to \(|S|\) as its cardinality. Next, for any vectors \(v:=(v_{1},v_{2},,v_{d})^{d}\) and \(:=(_{1},_{2},,_{d})^{d}\), we let \(v^{}=v_{1}^{_{1}}v_{2}^{_{2}} v_{d}^{_{d}}\), \(|v|:=v_{1}+v_{2}++v_{d}\) and \(!:=_{1}!_{2}!_{d}!\), while \(\|v\|\) stands for its \(2\)-norm value. Lastly, for any two positive sequences \((a_{n})_{n 1}\) and \((b_{n})_{n 1}\), we write \(a_{n}=(b_{n})\) or \(a_{n} b_{n}\) if \(a_{n} Cb_{n}\) for all \(n\), where \(C>0\) is some universal constant. The notation \(a_{n}=_{P}(b_{n})\) indicates that \(a_{n}/b_{n}\) is stochastically bounded.

## 2 Preliminaries

In this section, we demonstrate the parametric convergence rate for the least squares estimator of the regression function \(f_{_{n}}\). We begin by describing the assumption that will be used throughout the paper, which are overall very mild.

**(A.1) Topological assumptions.** To ensure the convergence of the least squares estimators, we assume the parameter space \(^{d}^{q}\) is compact, and the input space \(^{d}\) is bounded.

**(A.2) Lipschitz experts.** The expert function \(h(,)\) is assumed to be Lipschitz continuous with respect to \(\) and bounded, \(\)-almost surely.

**(A.3) Distinct experts.** The ground-truth expert parameters \(_{1}^{*},,_{k_{*}}^{*}\) are pair-wise different so that the experts \(h(x,_{1}^{*}),,h(x,_{k_{*}}^{*})\) are also distinct from each other.

Given the above assumptions, we are now ready to present the main result of this section. Recall that we divide our analysis into two following regimes:

* **Regime 1.** All the over-specified parameters \(_{1i}^{*}\) are equal to \(0_{d}\);
* **Regime 2.** At least one among the over-specified parameters \(_{1i}^{*}\) is different from \(0_{d}\).

Let us begin with Regime 1. We show in Theorem 1 that the regression estimator \(f_{_{n}}\) converges to the true regression function \(f_{G_{*}}\) at the parametric rate.

**Theorem 1**.: _Under the Regime 1 and with the least squares estimator \(_{n}\) defined in equation (3), the regression estimator \(f_{_{n}}\) admits the following rate of convergence to \(f_{G_{*}}\):_

\[\|f_{_{n}}-f_{G_{*}}\|_{L^{2}()}=_{P}([(n)/n]^{ }).\] (4)

The proof of Theorem 1 is in Appendix A.1. The above result indicates that if we are able to design a loss function among parameters \(\) that satisfies the lower bound \(\|f_{_{n}}-f_{G_{*}}\|_{L^{2}()}(_{ n},G_{*})\), then we obtain parameter estimation rates via the bound \((_{n},G_{*})=_{P}([(n)/n]^{})\). Consequently, those parameter estimation rates lead to our desired expert estimation rates.

Next, under the Regime 2, the true model is misspecified, i.e., the regression estimator \(f_{_{n}}\) converges to the misspecified regression function \(f_{}\) rather than the true regression function \(f_{G_{*}}\), where \(}_{k}():=_{G_ {k}()_{k_{*}}()}\|f_{G}-f_{G_{*}}\|_{L^{2}()}\). By employing the same arguments as in Theorem 1, we also capture the parametric regression estimation rate \(_{P}([(n)/n]^{})\) for \(\|f_{_{n}}-f_{}\|_{L^{2}()}\) under this regime, which is stated in the following corollary.

**Corollary 1**.: _Under the Regime 2, the regression estimator \(f_{_{n}}\) admits the following rate of convergence to \(f_{}\): \(_{}_{k}()}\|f_{_{n} }-f_{}\|_{L^{2}()}=_{P}([(n)/n]^{})\)._

## 3 Convergence Rates for Expert Estimation

In this section we will establish the expert estimation rates under both Regime 1 and the Regime 2; see Section 3.1 and Section 3.2, respectively. Under each regime, we will formulate appropriate

   &  &  &  \\   & Regime 1 & Regime 2 & Regime 1 & Regime 2 & **Experts** \\ 
**Sigmoid** & \(_{P}(n^{-1/4})\) & \(_{P}(n^{-1/2})\) & \(_{P}(1/(n))\) & \(_{P}(n^{-1/2})\) & \(_{P}(1/(n))\) \\ 
**Softmax** & _{P}(n^{-1/4})\)} & _{P}(1/(n))\)} & _{P}(1/(n))\)} \\  

Table 1: Summary of expert estimation rates (up to a logarithmic factor) under the MoE models equipped with the sigmoid gating (ours) and the softmax gating . In this work, we consider three types of expert functions including experts network with \(\), \(\) activations; polynomial experts; and input-independent experts.

conditions, which we refer to as _identifiability conditions_, on the expert functions that will guarantee fast estimation rates. Furthermore, we will determine the (slow) estimation rates of some commonly used experts which fail to satisfy such identifiability conditions.

### Regime 1 of Gating Parameters

Recall that under the Regime 1, all the over-specified parameters \(_{1i}^{*}\), i.e., those fitted by at least two parameters, are equal to \(0_{d}\). Without loss of generality, we assume that \(_{11}^{*},,_{1}^{*}\) are over-specified parameters, where \(1 k_{*}\). The remaining parameters \(_{11}^{*}==_{1}^{*}=0_{d}\). Meanwhile, \(_{1(+1)}^{*},,_{1k_{*}}^{*}\) are assumed to be fitted by exactly one estimator.

Following a strategy that has been successfully used for analyzing MoE and mixture models (see, e.g.  and ), in order to derive the expert estimation rates, it is sufficient to propose a Voronoi loss function \(\) among parameters, and then prove the lower bound \(\|f_{_{n}}-f_{G_{*}}\|_{L^{2}()}(_{n },G_{*})\).

Given the parametric regression estimation rate in Theorem 1, we then obtain that \((_{n},G_{*})=_{P}([(n)/n]^{})\). Note that a key step in deriving the previous lower bound is to decompose the difference \(f_{_{n}}(x)-f_{G_{*}}(x)\) into a combination of linearly independent terms.

Towards that goal, we will Taylor expand to the product of mixture weight and expert functions \(F(x,_{1},_{0},):=(x,_{1},_{0})h(x,)\), where we let \((x,_{1},_{0}):=^{}x-_{0})}\). To obtain the desired decomposition of \(f_{_{n}}(x)-f_{G_{*}}(x)\), we provide below in Definition 1 a condition, which we refer to as a _strong identifiability_ condition, that guarantees that the derivatives of the product \(F(x,_{1},_{0},)\) w.r.t its parameters are linearly independent.

**Definition 1** (Strong identifiability).: _An expert function \(h(x,)\) is called strongly identifiable if it is twice differentiable w.r.t its parameter \(\) for \(\)-almost all \(x\) and, for any positive integer \(\) and any pair-wise distinct choices of parameters \(\{(_{0i},_{1i},_{i})\}_{i=1}^{}\), the functions in the classes_

\[\{x|+|_{2}|}F}{_{1 }^{_{1}}^{_{2}}}(x,0_{d},_{0i},_{i}):i[ ],(_{1},_{2})^{d}^{q},1| _{1}|+|_{2}| 2\}\]

_and_

\[\{x^{_{1}} _{0}^{_{2}}^{_{3}}}(x,_{1i},_{0i},_{i}): i[],(_{1},_{2},_{3})^{d} ^{q},|_{1}|+_{2}+|_{3}|=1\}\]

_are linearly independent, for \(\)-almost all \(x\)._

**Example.** Consider experts formulated as two-layer neural networks, i.e., \(h(x,(a,b))=(a^{}x+b)\), where \(\) is an activation function and \((a,b)^{d}\). If \(a=0_{d}\), then the expert \(h(,(a,b))\) is not strongly identifiable for any choices of the activation function \(\). On the other hand, if \(a 0_{d}\) and \(\) is one among popular activation functions such as \(\) and \(\), then \(h(,(a,b))\) is a strongly identifiable expert. By contrast, if \(\) is a polynomial, e.g. \((z)=z^{p}\), then the expert \(h(,(a,b))\) does not meet the strong identifiability condition.

Intuitively, the strong identifiability condition helps eliminate potential interactions among parameters expressed in the language of PDE (see equations (8) and (11) where gating parameters \(_{1}\) interact with expert parameters \(a\)). Such interactions are demonstrated to result in significantly slow expert estimation rates presented in Theorem 3 and 4). From the technical perspective, a key step in our proof techniques rely on the decomposition of the discrepancy \(f_{_{n}}(x)-f_{G_{*}}(x)\) into a combination of linearly independent terms. This can be done by applying Taylor expansions to the function \(F(x,_{1},_{0},)=(_{1}^{}x+_{0})h(x,)\) defined as the product of the sigmoid gating and the expert function \(h\). Thus, the condition is to ensure that terms in the decomposition are linearly independent.

Next, following the strategy first introduced by Manole et al.  and then adopted in several convergence analyses of MoE , we proceed to construct a loss function among parameters based on the notion of Voronoi cells.

**Voronoi loss.** Given an arbitrary mixing measure \(G\) with \(k^{} k\) atoms, we distribute its atoms across the Voronoi cells \(\{_{j}_{j}(G)\), \(j[k_{*}]\}\) generated by the atoms of \(G_{*}\), where

\[_{j}:=\{i[k^{}]:\|_{i}-_{j}^{*}\|\|_ {i}-_{}^{*}\|, j\},\] (5)with \(_{i}:=(_{1i},_{i})\) and \(_{j}^{*}:=(_{1j}^{*},_{j}^{*})\). In particular, the cardinality of the Voronoi cell \(_{j}\) corresponding to the least squares estimator \(_{n}\) in (3) is the number of fitted components assigned (and, likely, converging) to the true atoms \(_{j}^{*}\). Then, we define the Voronoi loss function

\[_{1}(G,G_{*}):=_{j=1}^{}_{i _{j}})}- ^{*})}+_{j=1}^{}_{i _{j}}\|_{1ij}\|^{2}+\|_{ij}\|^{2}\] \[+_{j=+1}^{k_{*}}_{i_{j}}\| _{1ij}\|+|_{0ij}|+\|_{ij}\|,\] (6)

where we let \(_{1ij}:=_{1i}-_{1j}^{*}\), \(_{0ij}:=_{0i}-_{0j}^{*}\), and \(_{ij}:=_{i}-_{j}^{*}\). Above, if the Voronoi cell \(_{j}\) is empty, then the summation term becomes zero by convention. Additionally, since \(_{1}(G,G_{*})=0\) if and only if \(G G_{*}\), it follows that when \(_{1}(G,G_{*})\) is sufficiently small, the differences \(_{0ij}\), \(_{1ij}\) and \(_{ij}\) are also small. This observation suggests that, though not symmetric in its arguments and therefore not a metric, \(_{1}(G,G_{*})\) is nonetheless a suitable loss function for capturing the discrepancy between the least squares estimator \(_{n}\) and the true mixing measures \(G_{*}\). Furthermore, it is worth noting that the Voronoi loss function \(_{1}\) can be efficiently evaluated, as its computational complexity is of order \((k k_{*})\).

In our next result, whose proof can be found in Appendix A.2, we show that the Voronoi loss between the least squares estimator and the true mixing measure is upper bounded, up to universal multiplicative constants, by the estimation error for the regression function.

**Theorem 2**.: _Let \(x h(x,)\) be a strongly identifiable expert function. Then the lower bound_

\[\|f_{G}-f_{G_{*}}\|_{L^{2}()}_{1}(G,G_{*}),\]

_holds true for any \(G_{k}()\). As a consequence, this result together with the regression estimation rate in Theorem 1 indicate that \(_{1}(_{n},G_{*})=_{P}([(n)/n]^{})\)._

The following remarks regarding the results of Theorem 2 are in order.

**(i)** It follows from the formulation of the loss function \(_{1}\) that the estimation rates for the over-specified parameters \(_{1j}^{*},_{1j}^{*}\), where \(j[]\), are all of order \(_{P}([(n)/n]^{})\). Note that as the expert \(h(,)\) is twice differentiable over a bounded domain, it is also a Lipschitz function. Thus, letting \(_{n}:=_{i=1}^{_{n}})}_ {(_{1i}^{*},_{1}^{*})}\), we obtain that

\[_{x}|h(x,_{i}^{n})-h(x,_{j}^{*})| L_{1}\ \|_{i}^{n}-_{j}^{*}\|_{P}([(n)/n]^{ }),\] (7)

for any \(i_{j}(_{n})\), where \(L_{1} 0\) is a Lipschitz constant. The above bound indicates that if the strongly identifiable expert \(h(,_{j}^{*})\) is over-specified, i.e. fitted by at least two experts, then its estimation rate is of order \(_{P}([(n)/n]^{})\).

**(ii)** Secondly, for exact-specified parameters \(_{1j}^{*},_{j}^{*}\), where \(+1 j k_{*}\), the rates for estimating them are faster than those of their over-specified counterparts in Remark (i), standing at order \(_{P}([(n)/n]^{})\). By arguing as in equation (7), we deduce that the expert \(h(,_{j}^{*})\) also enjoys the faster estimation rate of order \(_{P}([(n)/n]^{})\), which is parametric on the sample size \(n\).

**(iii)** Putting the above two remarks together, we observe that when the expert functions are formulated as feed-forward networks with a widely used activation, namely \(\) and \(\), their estimation rates under the sigmoid gating MoE matches exactly those under the softmax gating MoE: see Theorem 3.2 in .

**Non-strongly identifiable experts.** The strong identifiability condition of Definition 1 is far from a technical requirement: it is in fact a crucial assumption. To illustrate its importance, we investigate the estimation rates of _ridge experts_ of the form \(\{h(x,(a_{j}^{*},b_{j}^{*}))=((a_{j}^{*})^{}x+b_{j}^{*}),j[k_{*}]\}\), where \(\) is a scalar function, that fail to satisfy the strong identifiability condition. For ridge experts, the strong identifiability will not be met in two cases: when at least one among over-specified parameters \(a_{j}^{*}\) equals \(0_{d}\), for any arbitrary activation function \(\) (**Scenario I**) and when \(\) is a polynomial of the form \((z)=z^{p}\), for \(p\) (**Scenario II**).

**Scenario I.** Without loss of generality, we may assume that \(a_{1}^{*}=0_{d}\), which means that the first expert \(((a_{1}^{*})^{}x+b_{1}^{*})\) becomes independent of the input \(x\). This gives rise to an interaction among the gating parameter \(_{1}\) and the expert parameter \(a\) via the partial differential equation (PDE)

\[}(x;_{11}^{*},_{01}^{*},a_{1}^{*},b_{1}^{*})=C_{b_{1}^{*},_{01}^{*}}(x; _{11}^{*},_{01}^{*},a_{1}^{*},b_{1}^{*}),\] (8)

where we recall that \(F(x;_{1},_{0},a,b):=(x;_{1},_{0})(a^{}x+b)\), and \(C_{b_{1}^{*},_{01}^{*}}\) is some constant depending on \(b_{1}^{*}\) and \(_{01}^{*}\). The above PDE can be verified by taking the derivatives of \(F(x;_{1},_{0},a,b)\) w.r.t \(_{1}\) and \(a\) and using the fact that \(_{11}^{*}=a_{1}^{*}=0_{d}\). Notably, the PDE (8) accounts for the violation of the strong identifiability of the expert \(((a_{1}^{*})^{}x+b_{1}^{*})\) under the Scenario I. To this end, we propose the following Voronoi loss to analyze the effects of such parameter interaction on the expert estimation rates in Theorem 3:

\[_{2,r}(G,G_{*}): =_{j=1}^{}_{i_{j}} {1+(-_{0i})}-^{*})}+_{j=1}^{ }_{i_{j}}\|_{1ij}\|^{r}+\| a _{ij}\|^{r}\] \[+| b_{ij}|^{r}+_{j=+1}^{k_{*}}_{i _{j}}|_{0ij}|^{r}+\|_{1ij}\|^{r}+\|  a_{ij}\|^{r}+| b_{ij}|^{r}.\] (9)

The following result establishes a minimax lower bound on the estimation error of \(G_{*}\) in the \(_{2,r}\) loss.

**Theorem 3**.: _Suppose that the experts take the form \((a^{}x+b)\). Then, under the Scenario I, we have_

\[_{_{n}_{k}()}_{G_{k}( )}_{n-1}()}_{f_{G}}[ _{2,r}(_{n},G)] n^{-1/2},\]

_for any \(r 1\), where \(_{f_{G}}\) indicates that the expectation taken w.r.t the product measure with \(f_{G}^{n}\), and the infimum is over all estimators taking values in \(_{k}()\)._

See Appendix A.3 for the proof details. We highlight some important implications of Theorem 3.

**(i)** The estimation rates for the parameters \(_{1j}^{*}\), \(a_{j}^{*}\) and \(b_{j}^{*}\) are slower than \(_{P}(n^{-1/2r})\), for any \(r 1\). This means that they are slower than any polynomial rates, and could be of order \(_{P}(1/(n))\).

**(ii)** Using the same reasoning described after equation (7), we have

\[_{x}|((_{i}^{n})^{}x+_{i}^{n})-(( a_{j}^{*})^{}x+b_{j}^{*})| L_{2}(\|_{i}^{n}-a_{j}^{*}\|+ \|_{i}^{n}-b_{j}^{*}\|),\] (10)

where \(L_{2}>0\) is a Lipschitz constant. As a consequence, the rates for estimating experts \(((a_{j}^{*})^{}x+b_{j}^{*})\) are no better than those for estimating the parameters \(a_{j}^{*}\) and \(b_{j}^{*}\), and could also be as slow as \(_{P}(1/(n))\). This result suggests that all the expert parameters \(a_{1}^{*},,a_{k_{*}}^{*}\) should be different from \(0_{d}\) to hope for fast convergence rates. In other words, every expert of the form \((a^{}x+b)\) in the MoE model should vary with the input value.

**Scenario II.** Now, we consider the second scenario when \(\) is a polynomial of the form \((z)=z^{p}\), for \(p\). For simplicity, we will only focus on the setting of \(p=1\), i.e., \(h(x,_{j}^{*})=(a_{j}^{*})^{}x+b_{j}^{*}\); the case of \(p>1\) can be argued in a very similar fashion. The structure of the polynomial experts leads to a non-linear interactions among the parameters, expressed through the PDE

\[F}{_{1} b}(x;_{1i}^{*},a_{i}^{*},b_ {i}^{*})=F}{ a_{0}}(x;_{1i}^{*},a_ {i}^{*},b_{i}^{*}),\] (11)

where \(F(x;_{1},_{0},a,b):=(x;_{1},_{0})(a^{}x+b)\). This interaction is the reason why the polynomial experts are not strongly identifiable. Ultimately, this dependence yields slow rates of convergence for expert estimation, just like in Scenario I, as revealed by the next result, which deploys the Voronoi loss \(_{2,r}\) of equation (9).

**Theorem 4**.: _Suppose that the experts take the form \((a^{}x+b)^{p}\), for some \(p\). Then, under the Scenario II and for any \(r 1\),_

\[_{_{n}_{k}()}_{G_{k}( )}_{k-1}()}_{f_{G}}[ _{2,r}(_{n},G)] n^{-1/2},\]

_where \(_{f_{G}}\) indicates the expectation taken w.r.t the product measure with \(f_{G}^{n}\)._The proof of Theorem 4 is in Appendix A.4. To sum up, when either the experts are independent of the input or they take a polynomial form, their estimation rates could be as slow as \(_{P}([(n)/n]^{})\).

### Regime 2 of Gating Parameters

Recall that under the Regime 2, at least one among the over-specified parameters \(_{1i}^{*}\) is different from \(0_{d}\). As discussed in Section 2, the least squares regression estimator \(f_{_{n}}\) in this case converges to a regression function \(f_{}\), where \(}_{k}():=_{G_{ u}()_{u_{k}}()}\|f_{G}-f_{G},\|_{L^{2}()}\). That is, the estimators of the parameters specifying \(f_{_{n}}\) converge to the parameters of \(f_{}\). WLOG, we may assume that \(:=_{i=1}^{k})}_{( _{1i},_{i})}\).

Similarly to Regime 1, we also provide in Definition 2 a _weak identifiability_ condition to characterize those expert functions that will have fast estimation rates under the Regime 2. Weakly identifiable experts are required to satisfy only a subset of the conditions imposed to strongly identifiable experts.

**Definition 2** (Weak identifiability).: _An expert function \(x h(x,)\) is called weakly identifiable if it is twice differentiable w.r.t its parameter \(\) for \(\)-almost all \(x\) and, for any positive integer \(\) and any pair-wise distinct choices of parameters \(\{(_{0i},_{1i},_{i})\}_{i=1}^{}\), the functions in the class_

\[\{x^{_{1}}_ {0}^{_{2}}^{_{3}}}(x,_{1i},_{0i},_{i}):i [],(_{1},_{2},_{3})^{d} ^{q},|_{1}|+_{2}+|_{3}|=1\}\]

_are linearly independent, for \(\)-almost all \(x\)._

**Example.** Let us take an expert network \(h(x,(a,b))=(a^{}x+b)\) as an example. It can be checked that if \(a 0_{d}\) and the activation function \(\) is a \(\) or \(\) or a polynomial, then the expert \(h(x,(a,b))\) is weakly identifiable. Conversely, if \(a=0_{d}\), i.e. the expert does not depend on the input, then the weak identifiability condition is not met, regardless of the choice of the activation.

We now describe the convergence rates for expert estimation under Regime 2. Since the analysis of input-independent experts can be done similarly to Theorem 3, we will focus only on weakly identifiable experts. As it turned out, the appropriate Voronoi loss function is given by

\[_{3}(G,):=_{j=1}^{k}_{i_{j}} |_{0i}-_{0j}|+\|_{1i}-_{1j}\|+\|_{ i}-_{j}\|.\] (12)

**Theorem 5**.: _Let \(h(x,)\) be a weakly identifiable expert. Then, for any mixing measure \(G_{k}()\),_

\[_{}_{k}()}\|f_{G}-f_{}\|_{L^{2}()}/_{3}(G,)>0.\]

_As a consequence, we obtain that \(_{}_{k}()}_{3}( _{n},)=_{P}([(n)/n]^{})\)._

The proof of Theorem 5 is in Appendix A.5. As a direct consequence of the bound and the definition of \(_{3}\), the convergence rates of the estimators \(_{0i}^{n}\), \(_{1i}^{n}\) and \(_{i}^{n}\) are of the same of parametric order \(_{P}([(n)/n]^{})\). Furthermore, by equation (7), we also establish that the convergence rate of the expert estimator \(h(x,_{i}^{n})\) is also of order \(_{P}([(n)/n]^{})\). Comparing these estimation rates with those established by  assuming instead softmax gating, there are two main observations:

**(i)** When the expert is a neural network with \(\) or \(\) activation, the above parametric expert estimation rate obtained using the sigmoid gate is faster than the rate guranteed by the softmax gate, which could be of order \(_{P}([(n)/n]^{})\) (see [Theorem 3.2, ]).

**(ii)** When the activation is a polynomial, the gap between expert estimation rates when using the sigmoid gate versus when using the softmax gate becomes even more dramatic: \(_{P}([(n)/n]^{})\) compared to \(_{P}(1/(n))\) (see Theorem 4.6 in ).

The above remarks highlight the considerable benefits of deploying sigmoid gating over softmax gating in MoE models: provably faster estimation rates not only for expert networks with popular activations like \(\) and \(\), but also for those with polynomial activation.

## 4 Numerical Experiments

In this section, we perform some numerical experiments to empirically demonstrate our claim that the sigmoid gating is more sample efficient than the softmax gating.

From Table 1, it can be seen that the sigmoid gating shares the same expert estimation rates as the softmax gating under the Regime 1. However, the former gating outperforms the latter under the Regime 2, particularly for ReLU experts and polynomial experts. Therefore, we will consider those experts under the Regime 2 in our subsequent experiments.

**Data generation.** In particular, we generate the data by first sampling \(X_{i}([-1,1]^{d})\) for \(i=1,,n\). Then, we generate \(Y_{i}\) according to the following model:

\[Y_{i}=g_{G_{*}}(X_{i})+_{i}, i=1,,n,\] (13)

where the regression function \(g_{G_{*}}()\) take the form of a softmax gating MoE:

\[g_{G_{*}}(x):=_{i=1}^{k_{*}}((_{1i}^{*})^{}x+ _{0i}^{*})((a_{i}^{*})^{}x+b_{i}^{*}),\] (14)

The input data dimension is \(d=32\). We employ \(k_{*}=8\) experts of the form \((a^{}x+b)\), where \(\) is either the identity function or the \(\) function. The variance of Gaussian noise \(_{i}\) is \(=0.01\).

**Experimental setup.** We summarize the choices of the ground-truth parameters \(_{0i}^{*}\), \(_{1i}^{*}\), \(a_{i}^{*}\) and \(b_{i}^{*}\) for \(1 i 8\) in Table 2, which satisfies the condition of the Regime 2.

**Training procedure.** For each sample size \(n\), spanning from \(10^{3}\) to \(10^{5}\), we perform 20 experiments. In every experiment, we employ \(k=k_{*}+1=9\) fitted experts, and the parameters initialization for the gating's and experts' parameters are adjusted to be near the true parameters, minimizing potential instabilities from the optimization process. Subsequently, we execute the stochastic gradient descent algorithm across \(10\) epochs, employing a learning rate of \(=0.1\) to fit a model to the synthetic data. For each experiment, we calculate the Voronoi losses for every model and report the mean values for each sample size in Figure 1. Error bars representing two standard deviations are also shown.

**Results.** In Figure 1, when employing the \(\) experts, that is, \(\) is the \(\) function, the Voronoi loss for the sigmoid gating approaches zero at the rate of order \((n^{-0.51})\), which nearly matches our theoretical results in Theorem 5. Meanwhile, the loss for the softmax gating converges to zero at the slower rate \((n^{-0.24})\). On the other hand, when using the linear experts, that is, \(\) is the identity function, the vanishing rate of the Voronoi loss associated with the sigmoid gating is \((n^{-0.40})\), while that for the softmax gating is significantly slower, standing at \((n^{-0.11})\). These observations empirically shows that the sigmoid gating is more sample efficient than the softmax gating.

## 5 Discussion

In this paper, we carry out a convergence analysis of least squares expert estimation under MoE models with the sigmoid gating, which was found empirically to be robust to the representation collapse issue and to have a favorable performance in the MoE applications. We demonstrate that under both the gating regimes, the sigmoid gating requires a smaller sample size than the softmax gating to reach the same expert estimation error: that is, the sigmoid gating is more sample efficient than its softmax counterpart. Furthermore, we also verify that experts formulated as feed-forward networks with popular activation functions such as \(\) and \(\) are more compatible with the sigmoid gating than other types of expert functions.

    &  \\  \(_{1i}^{*}(0_{d},_{g}I_{d})&1 i 7 \\ _{1i}^{*}=0_{d}&i=8\) & \(_{0i}^{*}(0,_{g})\) & \(a_{i}^{*}(0_{d},_{e}I_{d})\) & \(b_{i}^{*}(0,_{e})\) \\   

Table 2: True Parameters for Gating and Experts. The variance for the gating parameters is \(_{g}=0.01/d\) and for the expert parameters is \(_{e}=1/d\), where \(d=32\).

**Practical implications.** Our theoretical findings support the following conclusions, which have considerable practical relevance.

**(I.1) The sigmoid gate is more sample efficient than the softmax gate for expert estimation.** It can be seen from Table 1 that while the expert estimation rates obtained when using the sigmoid gate match those attained when using the softmax gate under the Regime 1, the former are totally faster than the latter under the Regime 2. Notably, Regime 2 is closer to practice since the gating values under this regime hinge upon the input while those under the Regime 1 are input-independent. As a consequence, we can claim that the sigmoid gate is more sample efficient than the softmax gate from the perspective of the expert estimation problem.

**(I.2) The sigmoid gate is compatible with a broader class of experts than the softmax gate.** It follows from the expert characterization for Regime 1 (resp. Regime 2) in Definition 1 (resp. Definition 2) that formulating expert functions as feed-forward networks  with commonly used activation functions such as \(\) and \(\) or even a polynomial activation will lead to faster expert estimation rates, and thus, require a smaller sample size to reach the same tolerance of estimating experts compared to the softmax gate. Thus, our theories indicate that the sigmoid gating is compatible with a broader class of experts than the softmax gating. This implication is particularly useful when people employ a mixture of fine-grained (shallow) expert networks .

**Limitations and future directions.** There are two main limitations of our current analysis:

1. We assume implicitly that the ground-truth parameters are independent of the sample size. Therefore, the expert estimation rates established in the paper are point-wise rather than the desirable uniform rates. A potential approach to cope with this problem is using the techniques for deriving the uniform parameter estimation rates in mixture models  and in MoE models . However, those techniques are only valid for input-independent gating MoE, and we believe that further technical tools should be developed to adapt such framework to the sigmoid gating MoE. Thus, we leave it for future development.

2. The assumption that the true regression function belongs to the parametric class of MoE models under sigmoid gating is, of course, quite restrictive, and likely to be violated in real-world settings. This issue can be alleviated by assuming the data are sampled from a regression framework with an arbitrary regression function \(g\), which is not necessarily formulated as a mixture of experts. Under that setting, the least squares estimator \(_{n}\) converges to \(*{arg\,min}_{G_{h}()}\|f_{G}- g\|_{L^{2}()}\). Nevertheless, it requires the comprehensive knowledge of the universal approximation power of the sigmoid function, which has been slightly explored in , to determine the expert estimation rate. Therefore, we leave this potential direction for future work.