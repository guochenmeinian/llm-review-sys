# Beyond Single Stationary Policies: Meta-Task Players as Naturally Superior Collaborators

Haoming Wang\({}^{*,}\)

\({}^{1}\) MOE KLINNS Lab

 Xi'an Jiaotong University

 Xi'an

 Shaanxi

 China

\({}^{2}\) Department of Computer Science and Engineering

 University of Notre Dame

 Notre Dame

 IN

 USA

{wanghm,tzm9802}@stu.xjtu.edu.cn, yunpengs@xjtu.edu.cn, xzhang33@nd.edu, zmcai@sei.xjtu.edu.cn

Zhaoming Tian\({}^{*,}\)

\({}^{1}\) MOE KLINNS Lab

 Xi'an Jiaotong University

 Xi'an

 Shaanxi

 China

\({}^{2}\) Department of Computer Science and Engineering

 University of Notre Dame

 Notre Dame

 IN

 USA

{wanghm,tzm9802}@stu.xjtu.edu.cn, yunpengs@xjtu.edu.cn, xzhang33@nd.edu, zmcai@sei.xjtu.edu.cn

Yunpeng Song\({}^{1}\)

\({}^{1}\) MOE KLINNS Lab

 Xi'an Jiaotong University

 Xi'an

 Shaanxi

 China

\({}^{2}\) Department of Computer Science and Engineering

 University of Notre Dame

 Notre Dame

 IN

 USA

{wanghm,tzm9802}@stu.xjtu.edu.cn, yunpengs@xjtu.edu.cn, xzhang33@nd.edu, zmcai@sei.xjtu.edu.cn

Xiangliang Zhang\({}^{2}\)

\({}^{1}\) MOE KLINNS Lab

 Xi'an Jiaotong University

 Xi'an

 Shaanxi

 China

\({}^{2}\) Department of Computer Science and Engineering

 University of Notre Dame

 Notre Dame

 IN

 USA

{wanghm,tzm9802}@stu.xjtu.edu.cn, yunpengs@xjtu.edu.cn, xzhang33@nd.edu, zmcai@sei.xjtu.edu.cn

Zhongmin Cai\({}^{,}\)

\({}^{1}\) MOE KLINNS Lab

 Xi'an Jiaotong University

 Xi'an

 Shaanxi

 China

\({}^{2}\) Department of Computer Science and Engineering

 University of Notre Dame

 Notre Dame

 IN

 USA

{wanghm,tzm9802}@stu.xjtu.edu.cn, yunpengs@xjtu.edu.cn, xzhang33@nd.edu, zmcai@sei.xjtu.edu.cn

###### Abstract

In human-AI collaborative tasks, the distribution of human behavior, influenced by mental models, is non-stationary, manifesting in various levels of initiative and different collaborative strategies. A significant challenge in human-AI collaboration is determining how to collaborate effectively with humans exhibiting non-stationary dynamics. Current collaborative agents involve initially running self-play (SP) multiple times to build a policy pool, followed by training the final adaptive policy against this pool. These agents themselves are a single policy network, which is **insufficient for handling non-stationary human dynamics**. We discern that despite the inherent diversity in human behaviors, the **underlying meta-tasks within specific collaborative contexts tend to be strikingly similar**. Accordingly, we propose **C**ollaborative **B**aysian **P**olicy **R**euse (**CBPR1**), a novel Bayesian-based framework that **adaptively selects optimal collaborative policies matching the current meta-task from multiple policy networks** instead of just selecting actions relying on a single policy network. We provide theoretical guarantees for CBPR's rapid convergence to the optimal policy once human partners alter their policies. This framework shifts from directly modeling human behavior to identifying various meta-tasks that support human decision-making and training meta-task playing (MTP) agents tailored to enhance collaboration. Our method undergoes rigorous testing in a well-recognized collaborative cooking simulator, _Overcooked_. Both empirical results and user studies demonstrate CBPR's superior competitiveness compared to existing baselines.

## 1 Introduction

An ongoing challenge in artificial intelligence (AI) involves training agents capable of effective collaboration with humans Klien et al. (2004); Bard et al. (2020); Dafoe et al. (2020). Unlike typical AI-only multi-agent collaboration, human-AI collaborative scenarios such as two-player cooking games, autonomous driving, and managing power grid stability incorporates a non-stationary component, humans Jagerman et al. (2019); Chandak et al. (2020); Chandak (2022). As humans may vary in their level of initiative, alter their collaboration strategies, or sometimes even do not collaborate at all. This variability suggests that for cooperative agents, the probability distribution \(P(A|s_{t})\) of a human action \(A\) given an environmental state \(s_{t}\) changes over time, reflecting differentmental states. Such non-stationarity poses a significant challenge in training collaborative agents, as it requires strategies that can adapt to the unpredictable nature of human behavior, which departs from the stable action-outcome associations expected in scenarios dominated by AI.

Recent works mainly develop collaborative agents through two workflows: (1) explicitly model human behavior by using real human trajectories Carroll et al. (2019), and then train a collaborator by teaming up with human models. (2) train Self-Play (SP) agents to form a policy pool (a diverse set of AI agents assumed to encompass all potential human policies) and then train a collaborator pairing with policies in the policy pool Strouse et al. (2021); Yu et al. (2023); Zhao et al. (2023). However, despite their ability to achieve commendable performance by amassing extensive human data collection or SP agent training, these collaborators share a common fundamental flaw: _they are essentially policy networks following a stationary distribution, thus making it difficult to cope with non-stationary human dynamics._

In this work, we propose Collaborative Bayesian Policy Reuse (CBPR), which reuses multiple stationary policies tailored to meta-tasks within a specific collaborative scenario. CBPR builds upon Bayesian Policy Reuse (BPR) Rosman et al. (2016); Chen et al. (2022), extending its application to human-AI collaborative tasks with theoretical guarantees. CBPR avoids modeling the non-stationary dynamics of human collaborators, focusing instead on heuristically modeling available meta-tasks within defined collaborative contexts. For example, in the multi-player cooking game _Overcooked_, meta-tasks include {_place onions in post, deliver soup_, _place onions in post & deliver soup_, _others_} (Figure 1) are available. Noticing that for a complex human-AI collaborative task, all of the undefined meta-tasks are categorized as "others," we subsequently train stationary meta-tasks-playing (MTP) collaborators using reinforcement learning (RL) to precisely match meta-task models on a one-to-one basis. During collaboration, CBPR identifies the meta-task being performed by the human partner based on recent actions, subsequently adapting the optimal MTP collaborator for use.

We evaluate CBPR in a fully-observable two-player common-payoff collaborative cooking simulator based on the game Overcooked Carroll et al. (2019), which has recently been proposed as a coordination challenge for AI Carroll et al. (2019); McKee et al. (2022); Wang et al. (2020); Wu et al. (2021); Knott et al. (2021). State-of-the-art performance of this game was achieved in Carroll et al. (2019); Strouse et al. (2021); Yu et al. (2023) via training stationary cooperation policy. Both simulated experiments and user studies show that the proposed CBPR agent can collaborate effectively with non-stationary agents and real humans. The novel contributions of this paper can be summarized as follows:

Figure 1: _Left_: The drawbacks of current collaborative agents, which train a stationary policy to manage the non-stationary dynamics of human collaborators but fail to determine the specific collaborative policies executed by humans. _Right_: Our approach focuses on identifying the meta-tasks underlying human decision-making and trains collaborators to match these meta-tasks in a one-to-one manner. This strategy enables effective ad-hoc collaboration with non-stationary humans.

1. We introduce a human-AI collaboration framework, CBPR, which addresses the challenge of modeling non-stationary human dynamics. This framework identifies the meta-tasks performed by human partners and reuses the optimal collaborative policy.
2. Theoretically, based on the Non-Stationary Markov Decision Process (NS-MDP), we provide theorems on _Collaboration Convergence_ and _Collaboration Optimality_ to support CBPR's convergence to the optimal collaborative policy over time in human-AI collaboration.
3. Empirically, we demonstrate CBPR's capability to collaborate effectively with non-stationary agents who frequently switch strategies, agents with various collaboration skills, and real humans.

## 2 Related Work

### Human-AI Collaboration

Training agents to collaborate with humans has been extensively studied. Recent research can be categorized into two groups based on whether human data is used during training. BCP Carroll et al. (2019) is trained by pairing with a supervised human model, while Boltzmann Policy Distribution (BPD) Laidlaw and Dragan (2022) updates its prior based on online human actions. These approaches require human data collection and are prone to distributional shifts. In contrast, another category focuses on achieving zero-shot coordination without extensive human data Hu et al. (2020). These works (e.g., FCP Strouse et al. (2021), Hidden-Utility Self-Play (HSP) Yu et al. (2023), and Maximum Entropy Population-based Training (MEP) Zhao et al. (2023)) train Self-Play (SP) agents to form a policy pool--a diverse set of AI agents assumed to encompass all potential human policies--and then train a collaborator to pair with policies in this pool. However, these collaborative agents remain single _stationary_ models despite their diverse training partners.

Our work represents a fundamental departure from previous studies by avoiding the need to model human behavior and instead focusing on constructing meta-tasks that underpin human decision-making. Furthermore, our CBPR framework does not restrict the construction of meta-tasks, which can be categorized into two streams: reliant on human data (e.g., behavior cloning) and independent of human data (e.g., rule-based methods).

### Policy Reuse

Policy reuse is a kind of transfer learning method that can greatly speed up reinforcement learning for a new task by using policies for relevant tasks. Initial methods like PRQL Fernandez and Veloso (2013) and OPS-TL Li and Zhang (2018), Li et al. (2018) integrated source policies with limitations in transfer efficiency. Subsequent approaches such as CAPS and CUP Zhang et al. (2022) improved policy selection and introduced more efficient algorithms without the need for extra training components.

Bayesian policy reuse (BPR) Rosman et al. (2016) represents a specialized stream within policy reuse. Utilizing a Bayesian optimization approach, BPR efficiently computes posteriors for novel tasks. Extensions like BPR+ Hernandez-Leal et al. (2016, 2016) and Bayes-Pepper Hernandez-Leal and Kaisers (2017) adapt BPR to multiagent scenarios, aligning tasks with opponent strategies and policies with optimal responses to these strategies. However, most BPR methodologies Rosman et al. (2016), Hernandez-Leal et al. (2016), Hernandez-Leal and Kaisers (2017), Zheng et al. (2018, 2021), Chen et al. (2022), Xie et al. (2022) primarily address multi-task problems or copy with competitive scenarios. Several studies, such as Zheng et al. (2018, 2021), investigated deep BPR+ in collaborative games.

However, these approaches primarily rely on policy inference to adjust to the changing strategies of opponents (or partners), which may not be optimal for human-AI collaboration given the wide spectrum of potential human policies. To our knowledge, our research is pioneering in applying and tailoring Bayesian policy reuse-based algorithms specifically for the human-AI collaboration challenge.

## 3 Collaborative Bayesian Policy Reuse

### Vanilla Bayesian Policy Reuse

Bayesian policy reuse is a general framework of transfer learning to cope with unknown tasks or frequently changing opponents. These classes of methods typically involve two phases: an offline learning phase and an online reusing phase. The workflow of a typical BPR can be summarized as follows: In the offline phase, it is presupposed that there exists a library of tasks \(\) and a corresponding library of learned policies \(\). Through conducting multiple simulations with varied policies across different tasks, a performance model \(P(U,)\) is derived, where \(U=_{i=0}^{k}r_{i}\) is cumulative utility. This model works as a mapping operator, associating each task and policy with a distribution of a predefined utility measure, such as reward.

During the online phase, BPR identifies the current task or opponent policy by maintaining a belief model \(()\). This model is periodically updated based on observations, as defined by the observation model \(P(,)\), where \(\) represents any signal aiding cooperation, such as reward or interaction trajectory. Significantly, this update adheres to Bayes' rule as follows:

\[_{k}()=(_{k},_{k})_{ k-1}()}{_{^{}}(_{k} ^{},_{k})_{k-1}(^{})}\] (1)

With this belief model, the BPR agent can select the optimal response policy by solving the following optimization problem:

\[^{}=_{}_{U}^{U^{}}_{ }()(U^{+},)U ^{+}\] (2)

where \(=_{}_{}()[U ,]\) represents the average performance of a single policy across all tasks. It's important to note that using \(\) as the lower limit of the integral, this optimization problem essentially seeks the policy with the highest likelihood of achieving utility above the average.

### CBPR Framework

#### Offline stage

Initially, we train meta-task processing (MTP) agents \(\) using the Proximal Policy Optimization (PPO) algorithm by individually pairing them with meta-tasks within a specific

Figure 2: Overview of the CBPR Framework. This framework is divided into two main phases. _Left:_ Offline Training Phase. This includes **(1)** constructing meta-task models using collected data and creating a meta-task library; **(2)** developing cooperative policies for each meta-task to compile an AI policy library; **(3)** establishing a performance model by evaluating each meta-task and AI policy pair. _Right:_ Online Collaboration Phase. During a collaboration round, the process involves **(a)** gathering a list of historical and current human data; **(b)** determining the current meta-task undertaken by the human using Bayesian policy inference (refer to Equation 3-4); **(c)** selecting the most suitable AI policy for cooperation (as per Equation 5); and finally, **(d)** the AI collaborator executes actions according to the chosen policy.

collaborative context, as exemplified by tasks such as _place onions in pot_, _deliver soup_, _place onions in pot & deliver soup_, and _others_ in the _Overcooked_ collaboration benchmark. Meta-task models \(\) are constructed through supervised learning, utilizing trajectories from either _rule-based agents enhanced with noise_ or _real humans performing the tasks_. In this study, we employ the rule-based agents developed by Yu et al. (2023). Subsequently, we construct the performance model \(P(U,)\) (i.e., observation model) by fitting a Gaussian distribution over the mean episodic return given a stochastic AI policy \(\) and a noisy rule-based agent \(\).

In previous BPR-based algorithms, the belief is designed for measuring the similarity between different tasks or opponents in transfer learning. These algorithms update belief using a observation model \((,)\) which only considers the game result but overlooks opponent's behavior. This leads to a poor collaborative performance when humans switch policy in a long-episode game. In this study, we used intra-episode belief \(^{t}()\) at timestep \(t\) to measure the similarity between current meta-task \(\) and \(^{}\) in meta-task model library \(\). The intra-episode belief was firstly proposed in Chen et al. (2022) and we extend it to the human-AI collaborative scenario.

**Online policy reuse** At the beginning of online policy reuse, the inter-episode belief \(_{0}()\) is initialized with a uniform distribution. For each episode, CBPR maintains a first-in-first-out (FIFO) human behavior queue \(\) of length \(l\), which records the latest human behavior tuples \((s_{i},a_{i})\). The AI selects initial response MTP agents according to the inter-episode belief \(_{0}()\) (line 5 in Algorithm 1). CBPR collects human state-action pairs and updates the intra-episode belief \(_{t}()\):

\[_{t}()=)_{t-1}()}{_{^{} }P(^{})_{t-1}(^{ })}\] (3)

where \(P()=^{l}(a_{i}|s_{i}) )}{_{^{}}(_{i=0}^{l}^{ }(a_{i}|s_{i}))}\). Then the intra-episode belief and inter-episode belief are integrated:

\[_{t}()=^{t}_{k-1}()+(1-^{t})_{t}()\] (4)

Where \(\) is a hyperparameter controlling the weight of the inter-episode and intra-episode beliefs. As the timestep \(t\) increases in a game with a long episode, the integrated belief \(_{t}()\) primarily depends on the intra-episode belief \(_{t}()\). The AI then uses the integrated belief \(_{t}()\) to select a policy to cooperate with the human at each timestep.

\[_{t}^{}=_{}_{}^{U^{}}_{ }_{t}()P(U^{+},)U^{+}\] (5)

At the end of each episode, CBPR collects the episodic return \(u_{k}\) and updates the inter-episode belief \(_{k}()\). To adapt to non-stationary human dynamics, we store human-AI trajectories in a replay buffer \(\) of the current MTP agent and update its policy. The detailed pseudo-code for the policy reuse of CBPR is presented in Algorithm 1.

### Theory Analysis of CBPR

The selection of cooperative policies (line 11 in the Algorithm 1) is crucial to the performance of CBPR in collaborating with humans. In this section, we propose theorems on the convergence and optimality of CBPR to support our viewpoint: CBPR will converge to the optimal cooperative strategy during the human-AI interaction process. We formulate collaborative process between humans and AI as a Non-Stationary MDP (NS-MDP) Chandak et al. (2020). In this process, the non-stationarity, resulting from the dynamic nature of human policy, can be mitigated by decomposing the entire non-stationary decision process into several stationary ones. Each stationary MDP corresponds to a specific meta-task executed by the human. Specifically, for a given NS-MDP \(\{M_{i}\}_{i=1}^{}\), the transition function integrates human actions as part of the environment itself, which can be denoted as \(_{i}:_{AI}_{hu} ()\). Within each stationary MDP \(M_{i}\), the human policy \(_{hu,i}:()\) is assumed to be stationary, although it may exhibit variations across different stationary MDPs. Under this assumption, the CBPR agent could establish a convergent human-AI collaboration:

**Theorem 1** (Collaboration Convergence of CBPR Agent).: _Let \(H_{i}:=\{S_{i}^{j},_{hu,i}(S_{i}^{j}),R^{j}\}_{j=0}^{}\) be a trajectory collected from a single stationary MDP \(M_{i}\) within the overall NS-MDP \(\{M_{i}\}_{i=1}^{}\) under the human meta-task policy \(_{hu,i}\). Denote \(:=\{(i,H_{i}):i[1,k]\}\) as a random variable representing a set of trajectories observed prior to the most recently completed stationary MDP \(M_{k}\). Given \(\), the response policy of CBPR agent could almost sure converge when interacting with a human partner, even when the human's policy is non-stationary._

We provide all proofs and a detailed explanation in Appendix A. In addition to being able to converge in cooperation with non-stationary humans, the CBPR agent can also establish the optimal collaboration policy:

**Theorem 2** (Collaboration Optimality of CBPR Agent).: _Denoting \(\) for CBPR algorithm, let \((,m):=[_{0}^{U^{}}(U^{+} (m),)U^{+}]\) be the expected return of exploiting AI policy \(\) with human meta-task policy \((m)\) in MDP \(M_{m}\). Given a positive integer \(k\) and a set of trajectories \(\) observed prior to the MDP \(M_{k}\), it follows that for any subsequent stationary MDP \(M_{k+}\), we have:_

\[(),k+( _{k}^{},k+) 1\] (6)

_when \(k\), where \(_{k}^{}\) is the optimal response policy for human meta-task policy at MDP \(M_{k}\)._

## 4 Experiments

In the context of _Overcooked_, we use rule-based policies developed in Yu et al. (2023) for each game layout (see Appendix C.1). These rule-based policies such as _place onions in pot_, _deliver soup_ are used to train corresponding MTP agents in a one-to-one manner. In this section, we conduct extensive experiments to answer the following questions:

**Q1**: When interacting with non-stationary agents who switch their strategies, can CBPR outperform established baselines? Additionally, can CBPR adapt its collaborative strategies to better synchronize with partner behaviors?

**Q2**: When interacting with non-stationary agents of various collaboration skills, can CBPR surpass other baselines?

**Q3**: Can CBPR exceed the performance of other baselines in collaboration with real humans?

**Q4**: How do hyperparameters and number of predefined meta-tasks influence the collaborative performance (mean reward) of CBPR agents?

_Overcooked_ environment _Overcooked_ is a popular two-player common-payoff game. It has become a typical environment for studying human-AI collaboration Carroll et al. (2019); Knott et al. (2021); Strouse et al. (2021); McKee et al. (2022); Yu et al. (2023). In this game, players should place three onions or tomatoes in a pot and deliver as many cooked soups as possible within a time limit. Good coordination between two players is crucial for achieving a high score. We employed four layouts in our experiments: _Cramped Room_, _Coordination Ring_, _Asymmetric Advantage_ and _Soup Coordination_ (Figure 8 in Appendix) in our experiments. Notably, in the _Asymm. Adv._ and _Soup Coord._, the players do not interfere with each other, and their movements are unobstructed by their partners.

BaselinesWe compare CBPR against three well-established baselines: (1) the Behavioral Cloning Play (BCP) Carroll et al. (2019), a human model-based method designed for human-AI collaboration; (2) Fictitious Co-Play (FCP) Strouse et al. (2021), a two-stage approach trained with partners of varying skill levels; (3) Self-Play (SP) Silver et al. (2017), a common RL method trained by playing against itself. For a fair comparison, we employed PPO Schulman et al. (2017) as the underlying algorithm of CBPR and reimplemented all baselines using identical hyperparameters in our experiments. Further details about environment setting and agents are illustrated in Appendix C.

### Cooperating with Rule-Based Agents Under Dynamic Policy Switching

To answer question **Q1**, we conduct a thorough investigation into the collaboration performance of CBPR when paired with non-stationary agents. These agents exhibited changes in their rule-based policies (Appendix Table 3), both inter-episodically and intra-episodically. We maintained a consistent random seed for policy switching during the evaluations to ensure fairness when comparing CBPR with baseline methods.

In our experiment, we evaluate the collaborative performance of agents at four different policy switching frequencies, as shown in Figure 3. The results show that CBPR consistently outperforms baseline methods in most cases. In particular, BCP, which was trained using a stationary human model, exhibited significantly poorer performance compared to CBPR. In addition, FCP and SP agents show greater fluctuations in episodic rewards, primarily due to their inability to effectively collaborate with all agents. In some instances, SP agents opted not cooperate, resulting in zero reward.

Our findings indicate that CBPR is particularly effective at collaborating with partners exhibiting varying degrees of non-stationarity. For a detailed overview of the results across the additional three policy switching frequencies (i.e., _per 2 episodes_, _per 200 timesteps_, and _per 100 timesteps_), please refer to the Appendix C.

### Cooperation with Partners of Various Collaboration Skills

The cooperative capacity of non-stationary humans is typically suboptimal. A generalized agent must be capable of collaborating with partners possessing diverse collaboration skills.

During the initial training phase of FCP Strouse et al. (2021), a policy pool is created by preserving various agent "checkpoints" that represent different levels of expertise. To answer question **Q2**, we pair CBPR with agents with varying collaboration skills preserved during the first stage of the FCP training. We evaluate collaborative performance over 50 episodes on four layouts. The results show that CBPR consistently achieved higher mean episode rewards than FCP, particularly when collaborating with lower-skilled partners (Figure 4). It is noteworthy that BCP performs better in the _Asymm. Adv._ and _Soup Coord._ in which players' movements are not hindered by their partners. We replayed the trajectories of BCP in _Cramped Rm._ and _Coord. Ring_ and observed that BCP occasionally became immobilized and failed to collaborate with partners (Figure 3(b)).

Figure 3: Comparative performance analysis against baselines when collaborators switch their rule-based policies _per episode_. All agents were evaluated over 50 continuous episodes. The shaded areas denote standard deviation calculated from five random seeds.

### Cooperation with Real Humans

To address question **Q3**, we recruited 25 volunteers from a local university, comprising 5 females and 20 males, ranging in age from 21 to 34 years, to participate in a study involving collaboration with CBPR and baseline agents. These volunteers were randomly assigned to one of four groups, each corresponding to a different game layout. Prior to the experiment, nearly all volunteers were unfamiliar with _Overcooked_. We provided comprehensive instructions from scratch and allowed them to play at least five practice rounds before beginning the evaluation. Subsequently, participants were instructed to interact with both the CBPR and baseline agents through the human-AI web applications developed by Carroll et al. (2019). Each volunteer participated in two episodes, during which we recorded the average reward obtained.

According to the reward distribution (Figure 5), we observe that CBPR achieves more efficient collaboration than other baselines. In most comparisons, CBPR displays significant higher reward according to the one-sided Mann-Whitney U test.

**Case study** To further demonstrate how the CBPR is more superior than baseline algorithms when collaborating with real humans, we present a case in Figure 6. In this case, we record five frames from the _Overcooked_ game interface to show that the ability of CBPR to adaptively adjust cooperative policies. Initially, CBPR agent is ready to use a dish to serve the soon-to-be-ready soup. When the human partner picks the soup, CBPR will set down the dish and continue to place onions to the pot for a new round. Meanwhile, FCP, after putting down the dish, will appear confused until the human served the soup. BCP, on the other hand, will not put down the dish and stubbornly prepare to serve the soup, ignoring the fact that the soup had already been served.

### Ablation Study

**Ablation on the queue size \(l\) and inter-episodic belief weight \(\).** In CBPR, the length \(l\) of human behavior queue and weight \(\) of inter-episodic belief mainly influence the collaborative performance. The larger \(l\) in \(P()\) of Eq. 3 means that CBPR chooses policy considering more past human behaviors. The larger \(\) determines that CBPR needs to consider inter-episodic belief more at the beginning of an episode. To answer question **Q4**, we expand on the experiments from section 4.2 demonstrate the results in Figure 7 and Appendix D.2. Overall, the results show that \(l\)=20 performs best, and in a relative simple layout (i.e., _Cramped Rm._), since the belief of cooperative policy converges easily, variation in \(\) has little impact on the reward. However, in complex layout (e.g., _Soup Coord._) (Figure 16), adjusting \(\) can enhance cooperative performance to a certain extent.

**Ablation on the number of predefined meta-tasks.** The performance of CBPR depends on the design of the meta-tasks. To address the challenge of predefined meta-tasks not covering all

Figure 4: Comparative performance analysis against baselines in cooperation with partners of diverse skill levels (low, medium and high). All agents were evaluated over 50 episodes and errors bars denote 95% confidence intervals.

Figure 5: Rewards distribution of agents collaborating with real humans over four layouts. *, \(p<0.05\); **, \(p<0.01\); ***, \(p<0.001\), and n.s., not significant. (Statistical significance was assessed by a one-sided Mann-Whitney U test.)possible ones in complex task scenarios, we introduce a meta-task category as "other" (Figure 1, bottom-right) which is represented using a random agent in practice. To demonstrate the impact of the number of predefined meta-tasks in the _Soup Coord_. We pair CBPR of different numbers of predefined meta-tasks with agents employing various skill levels. The results in Table 1 show that without "others" category, the performance deteriorates significantly, while the performances degrade relatively gracefully with less meta-tasks defined and more included in "others" category.

### Additional Findings and Analysis

#### The inherent advantage of SP and FCP agents.

Checkpoints, which are essentially SP agents, represent partners with low, medium, and high skill levels at the beginning, middle, and end of FCP training. Therefore, SP and FCP agents have an inherent advantage in the evaluation presented in Figure 4. Despite this, CBPR performs better when dealing with partners of lower skill levels. When collaborating with real humans, FCP and SP no longer hold the same advantages. This leads to almost all FCP and some SP performing well against agents of various skill levels, but falling short when facing human players.

#### The cooperative advantage of CBPR in non-separated layouts.

In separated layouts (i.e., _Asymm. Adv._ and _Soup Coord._), agents can usually complete tasks independently without considering the hindrance of the other partner's moves to themselves. However, players' own position (e.g., stand still

    & **7 predefined** & **5 predefined** & **3 predefined** & **3 predefined** \\  & **w/ ”others”** & **w/ ”others”** & **w/ ”others”** & **w/o ”others”** \\  High & 620.3 (193.3) & 600.7 (234.0) & 647.7 (159.3) & 622.8 (205.8) \\ Medium & 757.8 (100.3) & 735.8 (98.7) & 717.1 (148.1) & 607.3 (278.5) \\ Low & 689.8 (43.9) & 680.5 (51.6) & 668.9 (49.0) & 40.0 (59.1) \\   

Table 1: Collaboration performance of CBPR with different numbers of meta-tasks and agents employing various skill levels. We report the mean reward over 10 episodes and the values in bracket represent the standard deviation. Here, we additionally define four meta-tasks (i.e., _place onion & deliver soup_, _place tomato & deliver soup_, _pickup tomato & place mix_ and _pickup ingredient & place mix_), which are not included in Table 4.

Figure 6: This case study analyzes five discontinuous frames from the _Overcooked_ game interface to demonstrate the superiority of the CBPR algorithm. When a human player picks the cooked soup from the pot, the CBPR agent adapts by altering its initial plan to deliver the soup: it sets down the dish and places new onions in the pot, thereby showcasing its ability to adjust to human policies. In contrast, the FCP agent displays confusion when the human retrieves the soup and resumes placing onions only after the soup is served. The BCP agent rigidly adheres to its predetermined plan, continuously holding the plate without switching tasks to place onions, ignoring the fact that the soup has already been served.

in front of the serving areas) can obstruct their partners from completing the task in the non-separated layouts. Therefore, non-separated layouts require more cooperation between players compared to separated layouts. As shown in Figure 4, CBPR's better performance in _Cramped Rm._ and _Coord. Ring_ suggests its advantage in collaborative tasks.

**The double-edged sword of SP's simple policy.** In _Asymm. Adv._, SP agent exhibits outstanding performance when it cooperates with the agent of high skill level (Figure 3(c)). We replayed the game and found that the SP agent learned the simplest and most effective policy (i.e., in the right room, just pick an onion from onion dispenser and then place it in a pot within the shortest path). On the contrary, other agents exhibit some superfluous actions due to their own complexity. However, when cooperating with the agent of low skill level, SP performs poorly because the SP agent on the right only learned the simplest policy (putting onions in the pot), and when the agent with low skill level on the left does not deliver the cooked soup, SP will wait in place rather than deliver the cooked soup. In a more complex layout _Soup Coord._, we found that the SP agent learned a policy of putting only one onion in the pot and starting to cook, leaving its partner confused and uncertain about what went wrong. Therefore, cooperation with SP agents leads to low performance (Figure 3(d)).

## 5 Conclusion and Discussion

**Conclusion** In this work, we proposed CBPR framework and evaluated it in the well-known game _Overcooked_. CBPR could effectively tackle the challenge of collaborating with humans by utilizing a suite of meta-task aware agents. In response to the non-stationary nature of human behavior, CBPR adeptly selects MTP agent based on the most recent human actions and episodic returns. We have theoretically underpinned the collaborative efficacy of the CBPR approach. Empirically, we demonstrated that CBPR outperforms baselines when collaborates with simulated humans that change their policies frequently, simulated humans that employ different skill levels and real human players. We remark our primary argument that, given the non-stationary inherent in human behaviors, it is more effective to design various agents tailored to corresponding humans in different mental and behavioral states, rather than relying on a seemingly omnipotent single agent. After all, two heads are better than one.

**Limitations and future work** In this work, meta-tasks are modeled by manually-designed rule-based policies. In real-world application domains such as assessing power system transient stability in power grid dispatching and autonomous driving, it is time consuming to design various rule-based policies.CBPR offers a viable strategy to model meta-tasks, facilitating the training of multiple specialized experts to handle distinct meta-tasks. A notable challenge, however, is the manual summarization of domain experts' meta-tasks. As a direction for future research, we are keen to address the task of clustering policies automatically based on human trajectories. While this study Zhang et al. (2023) has made strides in this direction, the clustering approach adopted therein tends to obscure semantic understanding, presenting hurdles for AI in comprehending human behaviors. Splitting human trajectories according to the key state may be a possible solution. Additionally, perceiving the acquisition of a specific class of shaped rewards by an agent as the execution of a meta-task merits future consideration. This approach also does not depend on human data or models and offers enhanced universality and interpretability.