# Sequential Subset Matching for Dataset Distillation

Jiawei Du, Qin Shi, Joey Tianyi Zhou

Centre for Frontier AI Research (CFAR), Agency for Science, Technology and Research (A*STAR), Singapore

Institute of High Performance Computing (IHPC), Agency for Science, Technology and Research (A*STAR), Singapore

{dujw,Joey_Zhou}@cfar.a-star.edu.sg, shiqin924924@gmail.com

###### Abstract

Dataset distillation is a newly emerging task that synthesizes a small-size dataset used in training deep neural networks (DNNs) for reducing data storage and model training costs. The synthetic datasets are expected to capture the essence of the knowledge contained in real-world datasets such that the former yields a similar performance as the latter. Recent advancements in distillation methods have produced notable improvements in generating synthetic datasets. However, current state-of-the-art methods treat the entire synthetic dataset as a unified entity and optimize each synthetic instance equally. This static optimization approach may lead to performance degradation in dataset distillation. Specifically, we argue that static optimization can give rise to a coupling issue within the synthetic data, particularly when a larger amount of synthetic data is being optimized. This coupling issue, in turn, leads to the failure of the distilled dataset to extract the high-level features learned by the deep neural network (DNN) in the latter epochs. In this study, we propose a new dataset distillation strategy called Sequential Subset Matching (SeqMatch), which tackles this problem by adaptively optimizing the synthetic data to encourage sequential acquisition of knowledge during dataset distillation. Our analysis indicates that SeqMatch effectively addresses the coupling issue by sequentially generating the synthetic instances, thereby enhancing its performance significantly. Our proposed SeqMatch outperforms state-of-the-art methods in various datasets, including SVNH, CIFAR-10, CIFAR-100, and Tiny ImageNet. Our code is available at https://github.com/shqiij/seqmatch.

## 1 Introduction

Recent advancements in Deep Neural Networks (DNNs) have demonstrated their remarkable ability to extract knowledge from large-scale real-world data, as exemplified by the impressive performance of the large language model GPT-3, which was trained on a staggering 45 terabytes of text data . However, the use of such massive datasets comes at a significant cost in terms of data storage, model training, and hyperparameter tuning.

The challenges associated with the use of large-scale datasets have motivated the development of various techniques aimed at reducing datasets size while preserving their essential characteristics. One such technique is dataset distillation [5; 6; 13; 22; 29; 35; 42; 48; 50; 51; 52; 53], which involves synthesizing a smaller dataset that effectively captures the knowledge contained within the original dataset. Models trained on these synthetic datasets have been shown to achieve comparable performance to those trained on the full dataset. In recent years, dataset distillation has garnered increasing attention from the deep learning community and has been leveraged in various practical applications, including continual learning [41; 52; 53], neural architecture search [21; 39; 40; 51; 53], and privacy-preserving tasks [12; 15; 31], among others.

Existing methods for dataset distillation, as proposed in [5; 11; 13; 33; 37; 38; 47; 51; 53], have improved the distillation performance through enhanced optimization methods. These approaches have achieved commendable improvements in consolidating knowledge from the original dataset and generating superior synthetic datasets. However, the knowledge condensed by these existing methods primarily originates from the easy instances, which exhibit a rapid reduction in training loss during the early stages of training. These easy instances constitute the majority of the dataset and typically encompass low-level, yet commonly encountered visual features (e.g. edges and textures ) acquired in the initial epochs of training. In contrast, the remaining, less frequent, but more challenging instances encapsulate high-level features (e.g. shapes and contours) that are extracted in the subsequent epochs and significantly impact the generalization capability of deep neural networks (DNNs). The findings depicted in Figure 1 reveal that an overemphasis on low-level features hinders the extraction and condensation of high-level features from hard instances, thereby resulting in a decline in performance.

In this paper, we investigate the factors that hinder the efficient condensation of high-level features in dataset distillation. Firstly, we reveal that DNNs are optimized through a process of learning from low-level visual features and gradually adapting to higher-level features. The condensation of high-level features determines the effectiveness of dataset distillation. Secondly, we argue that existing dataset distillation methods fail to extract high-level features because they treat the synthetic data as a unified entity and optimize each synthetic instance unsuringly. Such static optimization makes the synthetic instances become coupled with each other easier in cases where more synthetic instances are optimized. As a result, increasing the size of synthetic dataset will over-condense the low-level features but fail to condense additional knowledge from the real dataset, let alone the higher-level features.

Building upon the insights derived from our analysis, we present a novel dataset distillation strategy, termed Sequential Subset Matching (SeqMatch), which is designed to extract both low-level and high-level features from the real dataset, thereby improving dataset distillation. Our approach adopts a simple yet effective strategy for reorganizing the synthesized dataset \(\) during the distillation and evaluation phases. Specifically, we divide the synthetic dataset into multiple subsets and encourage each subset to acquire knowledge in the order that DNNs learn from the real dataset. Our approach can be seamlessly integrated into existing dataset distillation methods. The experiments, as shown in Figure 1, demonstrate that SeqMatch effectively enables the latter subsets to capture high-level features. This, in turn, leads to a substantial improvement in performance compared to the baseline method MTT, which struggles to compress higher-level features from the real dataset. Extensive experiments demonstrate that SeqMatch outperforms state-of-the-art methods, particularly in high compression ratio2 scenarios, across a range of datasets including CIFAR-10, CIFAR-100, TinyImageNet, and subsets of the ImageNet.

In a nutshell, our contribution can be summarized as follows.

* we examine the inefficacy of current dataset distillation in condensing hard instances from the original dataset. We present insightful analyses regarding the plausible factors contributing to this inefficacy and reveal the inherent preference of dataset distillation in condensing knowledge.
* We thereby propose a novel dataset distillation strategy called Sequential Subset Matching (SeqMatch) to targetedly encourage the condensing of higher-level features. SeqMatch seamlessly integrates with existing dataset distillation methods, offering easy implementa

Figure 1: **Left:** MTT  fails to extract adequate high-level features. The loss drop rate between easy and hard instances is employed as the metric to evaluate the condensation efficacy of low-level and high-level features. The upper solid lines represent the loss change of hard instances, while the lower dashed lines depict the loss change of easy instances. The inability to decrease the loss of hard instances indicates MTT’s inadequacy in capturing high-level features. In contrast, our proposed SeqMatch successfully minimizes the loss for both hard and easy instances. **Right:** The consequent performance improvement of SeqMatch in CIFAR  datasets. Experiments are conducted with 50 images per class (\(=50\)).

tion. Experiments on diverse datasets demonstrate the effectiveness of SeqMatch, achieving state-of-the-art performance.

## 2 Related work

Coreset selection is the traditional dataset reduction approach by selecting representative prototypes from the original dataset[2; 7; 17; 43; 45]. However, the non-editable nature of the coreset limits its performance potential. The idea of synthesizing the "coreset" can be traced back to Wang et al. . Compared to coreset selection, dataset distillation has demonstrated greatly superior performance. Based on the approach of optimizing the synthetic data, dataset distillation can be taxonomized into two types: data-matching methods and meta-learning methods .

Data-matching methods encourage the synthetic data to imitate the influence of the target data, involving the gradients, trajectories, and distributions. Zhao and Bilen  proposed distribution matching to update synthetic data. Zhao et al.  matched the gradients of the target and synthetic data in each iteration for optimization. This approach led to the development of several advanced gradient-matching methods[5; 20; 22; 51]. Trajectory-matching methods [5; 9; 13] further matched multi-step gradients to optimize the synthetic data, achieving state-of-the-art performance. Factorization-based methods [11; 29; 33] distilled the synthetic data into a low-dimensional manifold and used a decoder to recover the source instances from the factorized features.

Meta-learning methods treat the synthetic data as the parameters to be optimized by a meta (or outer) algorithm [3; 11; 32; 34; 37; 38; 54]. A base (or inner) algorithm solves the supervised learning problem and is nested inside the meta (or outer) algorithm with respect to the synthetic data. The synthetic data can be directly updated to minimize the empirical risk of the network. Kernel ridge regression (KRR) based methods [34; 37; 38] have achieved remarkable performance among meta-learning methods.

Both data-matching and meta-learning methods optimize each synthetic instance equally. The absence of variation in converged synthetic instances may lead to the extraction of similar knowledge and result in over-representation of low-level features.

## 3 Preliminaries

**Background** Throughout this paper, we denote the target dataset as \(=\{(x_{i},y_{i})\}_{i=1}^{||}\). Each pair of data sample is drawn i.i.d. from a natural distribution \(\), and \(x_{i}^{d},y_{i}=\{0,1,,C-1\}\) where \(d\) is the dimension of input data and \(C\) is the number of classes. We denote the synthetic dataset as \(=\{(s_{i},y_{i})\}_{i=1}^{||}\) where \(s_{i}^{d},y_{i}\). Each class of \(\) contains \(\) (images per class) data pairs. Thus, \(||= C\) and \(\) is typically set to make \(||||\).

We employ \(f_{}\) to denote a deep neural network \(f\) with weights \(\). An ideal training progress is to search for an optimal weight parameter \(\) that minimizes the expected risk over the natural distribution \(\), which is defined as \(L_{}(f_{})_{(x,y)} (f_{}(x),y)\). However, as we can only access the training set \(\) sampled from the natural distribution \(\), the practical training approach of the network \(f\) is to minimizing the empirical risk \(L_{}(f_{})\) minimization (ERM) on the training set \(\), which is defined as

\[=(,_{0})=*{arg\,min}_{ }L_{}(f_{}) L_{}(f_{ })=|}_{x_{i}}f_{ }(x_{i}),y_{i},\] (1)

where \(\) can be any training loss function; \(\) is the given training algorithm that optimizes the initialized weights parameters \(_{0}\) over the training set \(\); \(_{0}\) is initialized by sampling from a distribution \(P_{_{0}}\).

Dataset distillation aims to condense the knowledge of \(\) into the synthetic dataset \(\) so that training over the synthetic dataset \(\) can achieve a comparable performance as training over the target dataset \(\). The objective of dataset distillation can be formulated as,

\[*{}_{(x,y),_{0} P_{_{0} }}(f_{(,_{0})}(x),y) *{}_{(x,y),_{0} P_{ _{0}}}(f_{(,_{0})}(x),y).\] (2)

**Gradient Matching Methods** We take gradient matching methods as the backbone method to present our distillation strategy. Matching the gradients introduced by \(\) and \(\) helps to solve in Equation 2. By doing so, gradient matching methods achieve advanced performance in dataset distillation. Specifically, gradient matching methods introduce a distance metric \(D(,)\) to measure the distance between gradients. A widely-used distance metric  is defined as \(D(X,Y)=_{i=1}^{I}(1-,Y_{i}}{\|X_{i}\|\|Y_{i} \|})\), where \(X,Y^{I J}\) and \(X_{i},Y_{i}^{J}\) are the \(i^{}\) columns of \(X\) and \(Y\) respectively. With the defined distance metric \(D(,)\), gradient matching methods consider solving

\[}=*{arg\,min}_{^{d}\\ ||=*{ip} C}_{ _{0} P_{_{0}}}_{m=1}^{M}(, _{m}),(,)=D _{}L_{}(f_{}),_{}L_{}(f_{}),\] (3)

where \(_{i}\) is the intermediate weights which is continuously updated by training the network \(f_{_{0}}\) over the target dataset \(\). The methods employ \(M\) as the hyperparameter to control the length of teacher trajectories to be matched starting from the initialized weights \(_{0} P_{_{0}}\). \((.)\) is the matching loss. The teacher trajectory \(\{_{0},_{1},,_{M}\}\) is equivalent to a series of gradients \(\{g_{1},g_{2},,g_{M}\}\). To ensure the robustness of the synthetic dataset \(\) to different weights initializations, \(_{0}\) will be sampled from \(P_{_{0}}\) for many times. As a consequence, the distributions of the gradients for training can be represented as \(\{P_{g_{1}},P_{g_{2}},,P_{g_{M}}\}\).

```
0: Target dataset \(\); Number of subsets \(K\); Iterations \(N\) in updating each subset; A base distillation method \(\).
1: Initialize the synthetic dataset \(_{}\)
2: Divide \(_{}\) into \(K\) subsets of equal size \(_{}|}{K}\), i.e., \(_{}=_{1}_{2} _{K}\)
3:for each \(_{k}\)do
4:\(\) Optimize each subset \(_{k}\) sequentially:
5:repeat
6:if k =1then
7: Initialize network weights \(_{0}^{k} P_{_{0}}\)
8:else
9: Load network weights \(_{0}^{k} P_{_{N}^{k-1}}\) saved in optimizing last subset \(_{k-1}\)
10:for\(i=1\) to \(N\)do
11:\(\) Update Network weights by subset \(_{k}\):
12:\(_{i}^{k}=(_{k}^{(k-1)},_{i- 1}^{k})\)
13:\(\) Update \(_{k}\) by the base distillation method:
14:\(_{k}(,_{k},_{i}^{k})\)
15: Record and save updated network weights \(_{N}^{k-1}\)
16:until Converge
17: Distilled synthetic dataset \(_{}\) ```

**Algorithm 1** Training with SeqMatch in Distillation Phase.

## 4 Method

Increasing the size of a synthetic dataset is a straightforward approach to incorporating additional high-level features. However, our findings reveal that simply optimizing more synthetic data leads to an excessive focus on knowledge learned from easy instances. In this section, we first introduce the concept of sequential knowledge acquisition in a standard training procedure (refer to subsection 4.1). Subsequently, we argue that the varying rate of convergence causes certain portions of the synthetic data to abandon the extraction of further knowledge in the later stages (as discussed in Figure 2). Finally, we present our proposed strategy, Sequential Subset Matching (referred to as SeqMatch), which is outlined in Algorithm 1 in subsection 4.3.

### Features Are Represented Sequentially

Many studies have observed the sequential acquisition of knowledge in training DNNs. Zeiler et al.  revealed that DNNs are optimized to extract low-level visual features, such as edges and textures, in the lower layers, while higher-level features, such as object parts and shapes, were represented in the higher layers. Han et al.  leverages the observation that DNNs learn the knowledge from easy instances first, and gradually adapt to hard instances to propose noisy learning methods. The sequential acquisition of knowledge is a critical aspect of DNN.

However, effectively condensing knowledge throughout the entire training process presents significant challenges for existing dataset distillation methods. While the synthetic dataset \(\) is employed to learn from extensive teacher trajectories, extending the length of these trajectories during distillation can exacerbate the issue of domain shifting in gradient distributions, thereby resulting in performance degradation. This is primarily due to the fact that the knowledge extracted from the target dataset \(\) varies across different epochs, leading to corresponding shifts in the domains of gradient distributions. Consequently, the synthetic dataset \(\) may struggle to adequately capture and consolidate knowledge from prolonged teacher trajectories.

To enhance distillation performance, a common approach is to match a shorter teacher trajectory while disregarding knowledge extracted from the latter epochs of \(\). For instance, in the case of the CIFAR-10 dataset, optimal hyperparameters for \(M\) (measured in epochs) in the MTT  method were found to be \(2,20,40\) for \(=1,10,50\) settings, respectively. The compromise made in matching a shorter teacher trajectory unexpectedly resulted in a performance gain, thereby confirming the presence of excessive condensation on easy instances.

Taking into account the sequential acquisition of knowledge during deep neural network (DNN) training is crucial for improving the generalization ability of synthetic data. Involving more synthetic data is the most straightforward approach to condense additional knowledge from longer teacher trajectory. However, our experimental findings, as illustrated in Figure 1, indicate that current gradient matching methods tend to prioritize the consolidation of knowledge derived from easy instances in the early epochs. Consequently, we conducted further investigations into the excessive emphasis on low-level features in existing dataset distillation methods.

### Coupled Synthetic Dataset

The coupling issue within the synthetic dataset impedes its effectiveness in condensing additional high-level features. Existing dataset distillation methods optimize the synthetic dataset \(\) as a unified entity, resulting in the backpropagated gradients used to update \(\) being applied globally. The gradients on each instance only differ across different initializations and pre-assigned labels, implying that instances sharing a similar initialization within the same class will converge similarly. Consequently, a portion of the synthetic data only serves the purpose of alleviating the gradient matching error for the pre-existing synthetic data.

Consider a synthetic dataset \(\) is newly initialized to be distilled from a target dataset \(\). The distributions of the gradients for distillation are \(\{P_{g_{1}},P_{g_{2}},,P_{g_{M}}\}\), and the sampled gradients for training is \(\{g_{1},g_{2},,g_{M}\}\). Suppose that \(G\) is the integrated gradients calculated by \(\), by minimizing the loss function as stated in Equation 3, the gradients used for updating \(s_{i}\) when \(=_{m}\) would be

\[_{s_{i}}(,_{m}) =}{ G}}(f_{_{m}}(s_{i}),y_{i})}}(f_{_{m}}(s_{i}),y_{i})}{ s_{i}}\] \[=}{ G} R(s_{i},f_{_{ m}}), R(s_{i},f_{_{m}})}(f_{_{m}}(s_{i}),y_{i})}{ s_{i}}.\] (4)

Figure 2: The accuracy discrepancy between the networks trained using \(^{+}\) and \(^{-}\) separately. The discrepancy will increase with the magnitude of \(R(s_{i},f_{_{m}})\). These results verified the coupling issue between \(^{+}\) and \(^{-}\), and our proposed method SeqMatch successfully mitigates the coupling issue. More experimental details can be found in subsection 5.3.

we have \(}(f_{_{m}}(s_{i}),y_{i})}=1\), because \(G\) is accumulated by the gradients of each synthetic data, i.e., \(G=_{_{m}}L_{}(f_{_{m}})=_{i=1}^{|| }_{_{m}}(f_{_{m}}(s_{i}),y_{i})\). Here we define the amplification function \(R(s_{i},f_{_{m}})^{d}\). Then, the gradients on updating synthetic instance \(_{s_{i}}(,_{m})\) shares the same \(}{ G}\) and only varies in \(R(s_{i},f_{_{m}})\). The amplification function \(R(s_{i},f_{_{m}})\) is only affected by the pre-assigned label and initialization of \(s_{i}\).

More importantly, the magnitude of \(R(s_{i},f_{_{m}})\) determines the rate of convergence of each synthetic instance \(s_{i}\). Sorted by the \(l_{1}\)-norm of amplification function \(\|R(s_{i},f_{_{m}})\|_{1}\) can be divided into two subsets \(^{+}\) and \(^{-}\). \(^{+}\) contains the synthetic instances with greater values of \(R(s_{i},f_{_{m}})\) than those in \(^{-}\). That implies that instances in \(^{+}\) converge faster to minimize \(D(_{_{m}}L_{^{+}}(f_{_{m}}),g_{m})\), and \(S^{+}\) is optimized to imitate \(g_{m}\). On the other hand, the instances in \(^{-}\) converge slower and are optimized to minimize \(D(_{_{m}}L_{^{-}}(f_{_{m}}),)\), where \(\) represents the gradients matching error \(\) of \(S^{+}\), i.e., \(=g_{m}-_{_{m}}L_{^{+}}(f_{_{m}})\). Therefore, \(S^{-}\) is optimized to imitate \(\) and its effectiveness is achieved by compensating for the gradients matching error of \(S^{+}\). \(S^{-}\) is coupled with \(S^{+}\) and unable to capture the higher-level features in the latter epochs.

We conducted experiments to investigate whether \(S^{-}\) solely compensates for the gradient matching error of \(S^{+}\) and is unable to extract knowledge independently. To achieve this, we sorted \(^{+}\) and \(^{-}\) by the \(l_{1}\)-norm of the amplification function \(\|R(s_{i},f_{_{m}})\|_{1}\) and trained separate networks with \(^{+}\) and \(^{-}\). As depicted in Figure 2, we observed a significant discrepancy in accuracy, which increased with the difference in magnitude of \(R(s_{i},f_{_{m}})\). Further details and discussion are provided in subsection 5.3. These experiments verify the coupling issue wherein \(S^{-}\) compensates for the matching error of \(^{+}\), thereby reducing its effectiveness in condensing additional knowledge.

### Sequential Subset Matching

We can use a standard deep learning task as an analogy for the dataset distillation problem, then the synthetic dataset \(\) can be thought of as the weight parameters that need to be optimized. However, simply increasing the size of the synthetic dataset is comparable to multiplying the parameters of a model in an exact layer without architecting the newly added parameters, and the resulting performance improvement is marginal. We thereby propose SeqMatch to reorganize the synthetic dataset \(\) to utilize the newly added synthetic data.

We incorporate additional variability into the optimization process of synthetic data to encourage the capture of higher-level feature extracted in the latter training progress. To do this, SeqMatch divides the synthetic dataset \(\) into \(K\) subsets equally, i.e., \(=_{1}_{2}_{K},| _{k}|=|}{K}\). SeqMatch optimizes each \(_{k}\) by solving

\[}_{k}=*{arg\,min}_{ _{k}^{d}\\ |_{k}|=||/K}\ *{ }_{_{0}_{_{0}}}_{m=(k-1)n}^{ kn}(_{k}^{(k-1)},_{m}),\] (5)

where \(^{(k-1)}=_{1}_{2}_{k-1}\), which represents the union set of the subsets in the former. \(^{(k-1)}\) are fixed and only \(_{k}\) will be updated. The subset \(_{k}\) is encouraged to match the corresponding \(k^{th}\) segment of the teacher trajectory to condense the knowledge in the latter epoch. Let \(n=\) denote the length of trajectory segment to be matched by each subset \(_{k}\) in the proposed framework. To strike a balance between providing adequate capacity for distillation and avoiding coupled synthetic data, the size of each subset \(_{k}\) is well-controlled by \(K\).

In the distillation phase, each subset is arranged in ascending order to be optimized sequentially. We reveal that the first subset \(_{1}\) with \(\) size of the original synthetic dataset \(\) is sufficient to condense adequate knowledge in the former epoch. For the subsequent subset \(_{k}\), we encourage the \(k^{th}\) subset \(_{k}\) condense the knowledge different from those condensed in the previous subsets. This is achieved by minimizing the matching loss \((_{k}^{(k-1)},_{m})\) while only \(_{k}\) will be updated.

During the evaluation phase, the subsets of the synthetic dataset are used sequentially to train the neural network \(f_{}\), with the weight parameters \(\) being iteratively updated by \(_{k}=(_{k},_{k-1})\). This training process emulates the sequential feature extraction of real dataset \(\) during training. Further details regarding SeqMatch and the optimization of \(^{*}\) can be found in Algorithm.1.

Experiment

In this section, we provide implementation details for our proposed method, along with instructions for reproducibility. We compare the performance of SeqMatch against state-of-the-art dataset distillation methods on a variety of datasets. To ensure a fair and comprehensive comparison, we follow up the experimental setup as stated in [8; 30]. We provide more experiments to verify the effectiveness of SeqMatch including the results on ImageNet subsets and analysis experiments in Appendix A.1 due to page constraints.

### Experimental Setup

**Datasets:** We evaluate the performance of dataset distillation methods on several widely-used datasets across various resolutions. MNIST , which is a fundamental classification dataset, is included with a resolution of \(28 28\). SVNH  is also considered, which is composed of RGB images of house numbers with a resolution of \(32 32\). CIFAR10 and CIFAR100 , two datasets frequently used in dataset distillation, are evaluated in this study. These datasets consist of \(50,000\) training images and \(10,000\) test images from \(10\) and \(100\) different categories, respectively. Additionally, our proposed method is evaluated on the Tiny ImageNet  dataset with a resolution of \(64 64\) and on the ImageNet  subsets with a resolution of \(128 128\).

**Evaluation Metric:** The evaluation metric involves distillation phase and evaluation phase. In the former, the synthetic dataset is optimized with a distillation budget that typically restricts the number of images per class (\(\)). We evaluate the performance of our method and baseline methods under the settings \(=\{10,50\}\). We do not evaluate the setting with \(=1\) since our approach requires \( 2\). To facilitate a clear comparison, we mark the factorization-based baselines with an asterisk (*) since they often employ an additional decoder, following the suggestion in . We employ 4-layer ConvNet  in Tiny ImageNet dataset whereas for the other datasets we use a 3-layer ConvNet .

In the evaluation phase, we utilize the optimized synthetic dataset to train neural networks using a standard training procedure. Specifically, we use each synthetic dataset to train five networks with random initializations for \(1,000\) iterations and report the mean accuracy and its standard deviation of the results.

**Implementation Details.** To ensure the reproducibility of SeqMatch, we provide detailed implementation specifications. Our method relies on a single hyperparameter, denoted by \(K\), which determines the number of subsets. In order to balance the inclusion of sufficient knowledge in each segment with the capture of high-level features in the later stages, we set \(K=\{2,3\}\) for the scenarios where \(=\{10,50\}\), respectively. Notably, our evaluation results demonstrate that the choice of \(K\) remains consistent across the various datasets.

As a plug-in strategy, SeqMatch requires a backbone method for dataset synthesis. Each synthetic subset is optimized using a standard training procedure, specific to the chosen backbone method. The only hyperparameters that require adjustment in the backbone method are those that control the segments of the teacher trajectory to be learned by the synthetic dataset, whereas the remaining hyperparameters remain consistent without adjustment. Such adjustment is to ensure each synthetic subset effectively condenses the knowledge into stages. The precise hyperparameters of the backbone methods are presented in Appendix A.3. We conduct our experiments on the server with four Tesla V100 GPUs.

### Results

Our proposed SeqMatch is plugged into the methods MTT  and IDC , which are denoted as SeqMatch-MTT and SeqMatch-IDC, respectively. As shown in Table 1, the classification accuracies of ConvNet  trained using each dataset distillation method are summarized. The results indicate that SeqMatch significantly outperforms the backbone method across various datasets, and even surpasses state-of-the-art baseline methods in different settings. Our method is demonstrated to outperform the state-of-the-art baseline methods in different settings among different datasets. Notably, SeqMatch achieves a greater performance improvement in scenarios with a high compression ratio (i.e., \(=50\)). For instance, we observe a \(3.5\%\) boost in the performance of MTT , achieving \(51.2\%\) accuracy on CIFAR-100. Similarly, we observe a \(1.9\%\) performance enhancement in IDC ,

[MISSING_PAGE_FAIL:8]

iterations in each epoch. To distinguish hard instances from easy ones, we employed k-means algorithm  to cluster all instances in the real dataset into two clusters based on the recorded instance-wise loss. The distribution of instances in terms of difficulty is as follows: \(77\%\) are considered easy instances, while \(23\%\) are classified as hard instances.

We evaluated MTT and SeqMatch as mentioned above. Our results show that MTT over-condenses the knowledge learned in the former epoch. In contrast, SeqMatch is able to successfully capture the knowledge learned in the latter epoch.

**Coupled Synthetic Subsets:** In order to validate our hypothesis that the synthetic subset \(^{-}\) is ineffective at condensing knowledge independently and results in over-condensation on the knowledge learned in the former epoch, we conducted experiments as shown in Figure 2. We sorted the subsets \(^{+}\) and \(^{-}\) of the same size by the \(l_{1}\)-norm of the amplification function \(|R(s_{i},f_{_{n}})|1\) as explained in Figure 4.2. We then recorded the accuracy discrepancies between the separate networks trained by \(^{+}\) and \(^{-}\) with respect to the mean \(l_{1}\)-norm difference, i.e., \(_{s_{i}^{+}}[|R(s_{i},f_{_{0}})|1]-_{ s_{i}^{-}}[|R(s_{i},f_{_{0}})|_{1}]\).

As shown in Figure 2, the accuracy discrepancies increased linearly with the \(l_{1}\)-norm difference, which verifies our hypothesis that \(^{-}\) is coupled with \(^{+}\) and this coupling leads to the excessive condensation on low-level features. However, our proposed method, SeqMatch, is able to alleviate the coupling issue by encouraging \(^{-}\) to condense knowledge more efficiently.

**Synthetic Image Visualization:** In order to demonstrate the distinction between MTT  and SeqMatch, we visualized synthetic images within the "car" class from CIFAR-10  and visually compared them. As depicted in Figure 3, the synthetic images produced by MTT exhibit more concrete features and closely resemble actual "car" images. Conversely, the synthetic images generated by SeqMatch in the \(2^{nd}\) and \(3^{rd}\) subsets possess more abstract attributes and contain complex car shapes. We provide more visualizations of the synthetic images in Appendix A.2.

### Limitations and Future Work

We acknowledge the limitations of our work from two perspectives. Firstly, our proposed sequential optimization of synthetic subsets increases the overall training time, potentially doubling or tripling it. To address this, future research could investigate optimization methods that allow for parallel optimization of each synthetic subset. Secondly, as the performance of subsequent synthetic subsets builds upon the performance of previous subsets, a strategy is required to adaptively distribute the distillation budget of each subset. Further research could explore strategies to address this limitation and effectively enhance the performance of dataset distillation, particularly in high compression ratio scenarios.

Figure 3: Visualization example of “car” synthetic images distilled by MTT  and SeqMatch from \(32 32\) CIFAR-10 (\(=50\)).

Conclusion

In this study, we provide empirical evidence of the failure in condensing high-level features in dataset distillation attributed to the sequential acquisition of knowledge in training DNNs. We reveal that the static optimization of synthetic data leads to a bias in over-condensing the low-level features, predominantly extracted from the majority during the initial stages of training. To address this issue in a targeted manner, we introduce an adaptive and plug-in distillation strategy called SeqMatch. Our proposed strategy involves the division of synthetic data into multiple subsets, which are sequentially optimized, thereby promoting the effective condensation of high-level features learned in the later epochs. Through comprehensive experimentation on diverse datasets, we validate the effectiveness of our analysis and proposed strategy, achieving state-of-the-art performance.