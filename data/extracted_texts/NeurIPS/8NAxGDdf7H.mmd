# Few-Shot Class-Incremental Learning via

Training-Free Prototype Calibration

 Qi-Wei Wang, Da-Wei Zhou, Yi-Kai Zhang, De-Chuan Zhan, Han-Jia Ye

State Key Laboratory for Novel Software Technology

Nanjing University, Nanjing, 210023, China

{wangqiwei,zhoudw,zhangyk,zhandc,yehj}@lamda.nju.edu.cn

Han-Jia Ye is the corresponding author.

###### Abstract

Real-world scenarios are usually accompanied by continuously appearing classes with scarce labeled samples, which require the machine learning model to incrementally learn new classes and maintain the knowledge of base classes. In this Few-Shot Class-Incremental Learning (FSCIL) scenario, existing methods either introduce extra learnable components or rely on a frozen feature extractor to mitigate catastrophic forgetting and overfitting problems. However, we find a tendency for existing methods to misclassify the samples of new classes into base classes, which leads to the poor performance of new classes. In other words, the strong discriminability of base classes distracts the classification of new classes. To figure out this intriguing phenomenon, we observe that although the feature extractor is only trained on base classes, it can surprisingly represent the _semantic similarity_ between the base and _unseen_ new classes. Building upon these analyses, we propose a _simple yet effective_ Training-frEE calibratioN (TEEN) strategy to enhance the discriminability of new classes by fusing the new prototypes (_i.e._, mean features of a class) with weighted base prototypes. In addition to standard benchmarks in FSCIL, TEEN demonstrates remarkable performance and consistent improvements over baseline methods in the few-shot learning scenario. Code is available at: https://github.com/wangkiw/TEEN

## 1 Introduction

Deep Neural Networks (_i.e._, DNNs) have achieved impressive success in various applications , but they usually rely heavily on _static and pre-collected large-scale_ datasets (_e.g._, ImageNet ) to achieve this success. However, the data in real-world scenarios usually arrive continuously. For example, the face recognition system is required to authenticate existing users, and meanwhile, new users are continually added . The scene where the model is required to continually learn new knowledge and maintain the ability on old tasks is referred to as _Class-Incremental Learning (CIL)_. The main challenge of CIL is the notorious _catastrophic forgetting problem_, where the model forgets old knowledge as it learns new ones.

Many methods have been designed to overcome _catastrophic forgetting_ in CIL from different perspectives, _e.g._, knowledge distillation , parameter regularization , and network expansion . These methods require new tasks to contain _sufficient_ labeled data for supervised training. However, collecting enough labeled data in some scenarios is challenging, making the conventional CIL methods hard to deploy . For example, the face recognition system can only collect very few facial images of a new user due to privacy reasons. Therefore, a more realistic and practical incremental learning paradigm, _Few-Shot Class-Incremental Learning (FSCIL)_, isproposed to address the problem of class-incremental learning with limited labeled data. Figure 0(a) gives a detailed illustration of FSCIL.

In addition to the _catastrophic forgetting problem_, FSCIL will suffer the _overfitting problem_ because the model can easily overfit the very few labeled data of new tasks. Previous works  have adopted prototype-based methods  to conquer the limited data problem. These methods freeze the feature extractor trained on base classes when dealing with new classes and use the prototypes of new classes as the corresponding classifier weights. The frozen feature extractor can alleviate the _catastrophic forgetting problem_ and the prototype classifier can circumvent the _overfitting problem_. However, the inherent problem of learning with few-shot data is challenging to depict precisely the semantic information of a new category with limited data, making prototypes of new classes inevitably biased . An intuitive reason for the biased prototypes is that the feature extractor has not been optimized for the new classes. For example, the feature extractor trained on "cat" and "dog" can not precisely depict the feature of a new class "bird" especially when the instances of "bird" are incredibly scarce 2. Many prototype adjustment methods  are dedicated to rectifying the biased representations of the new classes. These methods usually focus on designing complex pre-training algorithms to enhance the compatibility of representations , or complicated trainable modules that better adapt the representation of the new classes , all of which rely on _significant training costs_ in exchange for improved model performance.

However, we find that although existing methods perform well on the widely used performance measure (_i.e._, the average accuracy across all classes), _they usually exhibit poor performance on new classes_, which suggests that the calibration ability of existing methods could be improved. In other words, existing methods neglect the performance of new classes. A direct reason for this negligence is the current _unified_ performance measure is easily overwhelmed by the dominated base classes (_e.g._, 60 base classes in the CIFAR100 dataset). However, the more recent tasks are usually more crucial in some real-world applications. For example, the new users in the recommendation system are usually more important and need more attention . The _important yet neglected_ new classes' performance inspires us to pay more attention to it.

To better understand the performance of current FSCIL methods , we explicitly evaluate and analyze the low performance of existing methods on _new classes_ and empirically demonstrate _the instances of new classes are prone to be predicted into base classes_. To further understand this intriguing phenomenon, we revisit the representation of the feature extractor and find that the feature extractor trained only on base classes can already represent the semantic similarity between the base and new classes well. As shown in Figure 0(b), although the novel classes are unavailable during the training of the feature extractor, the _semantic similarity_ between the base and novel classes can also be well-represented. However, existing methods have been obsessed with designing complex learning modules and disregard this off-the-shelf _semantic similarity_.

Figure 1: (a) The base session contains _sufficient_ samples of base classes for training. The incremental sessions (_i.e._, sessions after the base session) only contain _few-shot_ samples of new classes. FSCIL aims to obtain a unified classifier over all seen classes. Notably, the data from previous sessions are unavailable. (b) For example, considering the new classes “Plain”, “Plate” and “Porcupine” in CIFAR100 , the corresponding most similar base classes selected by the similarity can depict the really semantic similarity. The three most similar base classes are selected by computing the cosine similarity between base prototypes and novel prototypes. The results show _the feature extractor only trained on the base classes can also well represent the semantic similarity between the base and new classes_.

Based on the above analysis, we propose to leverage the overlooked semantic similarity to explicitly enhance the discriminability of new classes. Specifically, we propose a _simple yet effective_ Training-frEE prototype calibratioN (_i.e._, TEEN) strategy for biased prototypes of new classes by fusing the biased prototypes with weighted base prototypes, where the weights for the base prototypes are _class-specific and semantic-aware_. Notably, TEEN does not rely on any extra optimization procedure or model parameters once the feature extractor is trained on the base classes with a vanilla supervised optimization objective. Besides, the excellent calibration ability and training-free property make it a plug-and-play module.

Our contributions can be summarized as follows:

* We empirically find that the lower performance of new classes is due to misclassifying the samples of new classes into corresponding similar base classes, which is intriguing and missing in existing studies.
* We propose _a simple yet effective training-free_ calibration strategy for new prototypes, which not only achieved a higher average accuracy but also improved _the accuracy of new classes_ (**10.02% \(\) 18.40% better than the runner-up FSCIL method**).
* We validate TEEN on benchmark datasets under the standard FSCIL scenario. Besides, TEEN shows competitive performance under the few-shot learning scenario. The consistent improvements demonstrate TEEN's remarkable calibration ability.

## 2 Related Works

### Class Incremental Learning

The model in the Class-Incremental Learning (_i.e._, CIL) scenario is required to learn new classes without forgetting old ones. Save representative instances in old tasks (_i.e._, exemplars) and replay them in new tasks is a simple and effective way to maintain the model's discriminability ability on old tasks [20; 2; 37]. Furthermore, knowledge distillation [17; 35; 25; 49] is widely used to maintain the knowledge of the old model by enforcing the output logits between the old model and the new one to be consistent. iCaRL  uses the knowledge distillation as a regularization item and replays the exemplars when learning the feature representation. Many methods follow this line and design more elaborate strategies to replay exemplars  and distill knowledge [21; 12]. Recently, model expansion [28; 50; 47; 46; 39; 60] have been confirmed to be effective in CIL. The most representative method  saves a single backbone and freezes it for each incremental task. The frozen backbone substantially alleviates the catastrophic forgetting problem.

### Few-Shot Learning

The model in Few-Shot Learning (_i.e._, FSL)  scenario is required to learn new classes with limited labeled data. Existing methods usually achieve this goal either from the perspective of optimization [13; 31; 3] or metric learning [40; 43; 26; 27; 56]. The core thought of optimization-based methods is to equip the model with the ability to fast adaptation with limited data. The metric-based methods focus on learning a unified and general distance measure to depict the semantic similarity between instances. Besides,  proposes to use Gaussian distribution to model each feature dimension of a specific class and sample augmented features from the calibrated distribution. Based on these augmented features,  train a logistic regression and achieve competitive performance. However, TEEN can outperform  in most settings and without any training cost when recognizing new classes.

### Few-Shot Class-Incremental Learning

The model in Few-Shot Class-Incremental Learning (_i.e._, FSCIL)  scenario is required to incrementally learn new knowledge with limited labeled data. Many existing methods are dedicated to designing learning modules to train a more powerful feature extractor  or adapting the representation of the instances of new classes [57; 9; 61; 1]. Besides, TOPIC  utilizes a neural gas network to alleviate the challenging problems in FSCIL.  adopts word embeddings as semantic information and introduces a distillation-based FSCIL method. IDLVQ  proposes to utilize quantized reference vectors to compress the old knowledge and improve the performance in FSCIL. However, all these methods overlook the abundant semantic information in base classes and the poor performance in new classes. In this study, we aim to take a small step toward filling this gap. The most related work to TEEN is . However, it relies on optimizing a regularization-based objective function to implicitly utilize the semantic information. As opposed to , TEEN takes advantage of the empirical observation and gets rid of the optimization procedure and is thus more efficient and effective.

## 3 Preliminaries

### Definition and notations

In FSCIL , we assume there exists \(T\) sessions in total, including a base session (_i.e._, the first session) and \(T-1\) incremental sessions (_i.e._, sessions after the first session). We denote the training data in the base session as \(_{0}\) and the training data in the incremental sessions as \(\{_{1},_{2},,_{T-1}\}\). For the training data \(_{i}\) in the \(i\)-th session, we further notate it with \(\{(x_{j},y_{j})\}_{j=1}^{N}\) and corresponding label space with \(_{i}\). Note that only training data \(_{i}\) is available in the \(i\)-th session. Accordingly, the testing data and testing label space in session \(i\) can be denoted as \(_{i}^{test}\) and \(_{i}^{test}\). To better evaluate the model's discriminability on all seen tasks, the testing label space \(_{i}^{test}\) of \(i\)-th session contains all seen classes during training, _i.e._, \(_{i}^{test}=_{j=0}^{i}_{j}\). An incremental session can also be denoted as a \(N\)-way \(K\)-shot classification task, _i.e._, \(N\) classes and \(K\) labeled examples for each class. Note that the training label spaces between different sessions are disjoint, _i.e._, for any \(i,j[0,T-1]\) and \(i j,_{i}_{j}=\).

Compared to conventional CIL, FSCIL only requires the model to learn new classes with _limited_ labeled data. On the other hand, compared to conventional Few-Shot Learning (FSL), FSCIL requires the model to continually learn the knowledge of new classes while _retaining the knowledge of previously seen classes_. We introduce the related works on CIL, FSL and FSCIL in supplementary due to the space limitation.

The model in FSCIL can be decoupled into a feature encoder \(_{}()\) with parameters \(\) and a linear classifier \(W\). Given a sample \(x_{j}^{D}\), the feature of \(x_{j}\) can be denoted as \(_{}(x_{j})^{d}\). For a \(N\)-class classification task, the output logits of a sample \(x_{j}\) can be denoted as \(_{j}=W^{}_{}(x_{j})^{N}\) where \(W^{d N}\).

### Prototypical Network

ProtoNet  is a widely used method in few-shot learning problems. It computes the mean feature \(c_{k}\) of a class \(k\) (_e.g._, class prototype) and uses the class prototype to represent the corresponding class:

\[c_{k}=_{k}}_{y_{j}=k}_{}(x_{j})\] (1)

Num\({}_{k}\) denotes the number of samples in class \(k\). For a classification task with \(N\) classes, the classifier can be represented by the \(N\) prototypes, _i.e._, \(W=[c_{1},c_{2},,c_{N}]\). Following the [58; 57; 61; 1], we freeze the feature extractor \(_{}\) trained on the base task and plug the class prototype into the classifier while dealing with a new class. The frozen feature extractor can alleviate catastrophic forgetting and the plug-in updating of the classifier can circumvent the overfitting problem relatively.

## 4 A Closer Look at FSCIL

In this session, we comprehensively analyze current FSCIL methods from a _decoupled_ perspective. Although the previous updating paradigm of _extractor-frozen and prototypes-plugged_ can achieve adequate average accuracy in all classes, there also exist some shortcomings in it. In this section, we empirically show that the current methods (_e.g._, [57; 58]) are generally not effective in new classes. Furthermore, we take a step toward understanding the reason for the low performance in new classes.

### Understanding the reason for poor performance in new classes

To better understand the performance of existing methods, we first measure the performance by average accuracy on all classes, base classes and new classes, respectively. As illustrated in Figure 2,our first observation is that **the average accuracy across base classes is extremely higher than the accuracy in new classes**. The inconsistent performance between the base and new classes is caused by the frozen feature extractor and biased new prototypes. The former forces the model to _overfit_ base classes and the latter causes the model to _underfit_ the new classes.

To determine the real cause of the low performance on new classes, we further investigate "_What base classes are the new classes incorrectly predicted into?_" and get our second observation.

Our second observation is that **the prototype-based classifier misclassifies the new classes to their corresponding most similar base classes with high probability**, _i.e._, many instances of new classes are closer to their nearest base prototype than corresponding target prototypes

To verify this observation, we first analyze the detailed prediction results from a decoupled perspective. Specifically, we treat all base classes as a "positive class" and all new classes as a "negative class" and transform the FSCIL problem into a two-class classification task. We compute the false negative rate (_i.e._, **FNR**) and false positive rate (_i.e._, **FPR**) of each binary prediction task on each incremental session. The FNR and FPR in the confusion matrix is defined as follows:

\[=}{+} 100\%,=}{+} 100\%\] (2)

As shown in Table 1, the FPR is far greater than FNR illustrating new classes are generally misclassified into base classes but the base classes are generally misclassified into base classes. On the basis of this conclusion, we further explore the details of misclassification. To better illustrate the analysis, we define "misclassified **To** most similar **B**ase classes **R**atio" (_i.e._, **TBR** ) for new classes and "misclassified **To** most similar **N**ew classes **R**atio" (_i.e._, **TNR** ) for base classes. Specifically, considering the misclassified instances in base classes and new classes respectively, the TBR and

   Session & 1 & & 2 & 3 & 4 & 5 & & 6 & 7 & & 8 \\ 
**FNR/FPR** & FNR & FPR & FNR & FPR & FNR & FPR & FNR & FPR & FNR & FPR & FNR & FPR & FNR & FPR \\  ProtoNet  & 2.13 67.40 & 4.42 67.40 & 6.68 60.67 & 8.28 & 58.90 & 10.25 56.68 & 11.70 54.47 & 12.57 51.66 & 13.78 51.80 & \\ CEC  & 2.32 70.40 & 4.38 66.20 & 6.18 62.20 & 7.50 58.65 & 9.72 56.00 & 11.30 53.60 & 12.12 51.40 & 13.48 51.78 & \\ FACT  & 2.05 66.60 & 3.88 61.70 & 5.58 56.80 & 7.23 55.05 & 8.85 53.64 & 9.83 51.13 & 10.45 48.83 & 11.75 49.27 & \\  TEEN & 4.03 **57.40** & 7.40 **52.50** & 9.35 **45.00** & 11.58 **40.75** & 14.00 **40.80** & 15.78 **39.23** & 16.33 **36.91** & 18.75 **36.65** \\   

Table 1: Detailed prediction results of **False Negative Rate/False Positive Rate** (%) on CIFAR100  dataset. The analysis results are from session 1 because new classes do not exist in session 0. Exceedingly high **FPR** and relatively low **FNR** show the instances of new classes are easily misclassified into base classes and the instances of base classes are also easily misclassified into base classes. TEEN can achieve relatively lower **FPR** than baseline methods, which demonstrates the validity of the proposed calibration strategy. Please refer to the supplementary for results on _mini_ImageNet and CUB200.

   Session & 1 & & 2 & 3 & 4 & 5 & 6 & 7 & & 8 \\ 
**TNR/TBR** & TNR & TBR & TNR & TBR & TNR & TBR & TNR & TBR & TNR & TBR & TNR & TBR & TNR & TBR \\  ProtoNet  & 3.47 & 73.01 & 5.02 & 61.79 & 7.06 & 55.05 & 8.08 & 52.95 & 7.14 & 53.27 & 8.56 & 51.96 & 8.53 & 49.18 & 9.21 & 50.91 \\ CEC  & 2.87 & 71.70 & 4.53 & 61.08 & 5.66 & 58.26 & 6.35 & 55.16 & 6.07 & 54.12 & 6.92 & 52.53 & 8.24 & 49.96 & 8.20 & 51.64 \\ FACT  & 2.33 & 70.00 & 3.32 & 61.01 & 5.50 & 55.17 & 6.88 & 50.55 & 6.57 & 50.52 & 7.72 & 49.30 & 8.88 & 46.99 & 9.31 & 48.48 \\  TEEN & 2.16 & **66.77** & 2.53 & **55.14** & 3.55 & **44.87** & 3.85 & **37.68** & 3.57 & **39.47** & 4.02 & **37.07** & 4.76 & **33.83** & 4.85 & **35.04** \\   

Table 2: Detailed prediction results of **TNR/TBR** (%) on CIFAR100  dataset. The analysis results are from session 1 because new classes do not exist in session 0. For new classes, we only consider the 10 most similar base classes out of 60 base classes. For base classes, we suppose \(C_{i}\) new classes exist in the current incremental session \(i\). We only consider the most similar \([20\% C_{i}]\) new classes. Class similarity adopts cosine similarity between different class prototypes. TEEN can achieve relatively lower **TBR** than baseline methods, which demonstrates the validity of the proposed calibration strategy. Please refer to the supplementary for results on _mini_ImageNet and CUB200.

Figure 2: The performance of FACT  on CIFAR100 dataset. **NAcc,BAcc,AvgAcc** mean the average accuracy on _new classes, base classes, and all classes_, respectively. _and all classes_, respectively.

TNR are defined as follows:

\[=}{N_{n}} 100\%,=}{N_{b}} 100\%\] (3)

\(N_{n}\) means the number of misclassified instances of new classes. \(M_{b}\) is the number of instances misclassified into most similar base classes in \(N_{n}\) samples. \(M_{n}\) and \(N_{b}\) have similar meaning. As shown in Table 2, the TBR is higher and the TNR is relatively low, which strongly underpins our second observation. To our knowledge, we are the first to explain the reason for the low performance of new classes in existing methods and observe that the samples of new classes are easily misclassified into the most similar base classes.

To summarize, we empirically demonstrate that existing methods perform poorly on new classes and find that this is because the model tends to misclassify new class samples into the most similar base classes. Combining existing analysis (_i.e._, _the samples of new classes tend to be classified into the most similar base classes_) and observations (_i.e._, _the well-trained feature extractor on base classes can also well represent the semantic similarity between the base and new classes_), we propose to use the frozen feature extractor as a _bridge_ and calibrate the biased prototypes of new classes by fusing the biased new prototypes with the well-calibrated base prototypes.

## 5 Similarity-based Prototype Calibration

Based on the common assumptions in FSCIL , sufficient instances of base classes are available during the training of the feature extractor. We argue the _sufficient_ data in the base session contains abundant semantic information (_i.e._, a _sufficient_ number of classes) and the base prototypes are well-calibrated (_i.e._, a _sufficient_ number of instances for each class). The above analysis leads to a natural question:

_Can we leverage the well-calibrated prototypes in the base session for a new prototype calibration?_

As the previous methods overlook the low performance of new classes caused by the corresponding biased prototypes, we propose to _explicitly calibrate_ these biased prototypes with the help of the well-calibrated base prototypes. The off-the-shelf semantic similarity serves as a bridge between the base prototypes and the new ones. In the following sections, we introduce the details of fusing the well-calibrated base prototypes and ill-calibrated new prototypes to calibrate the biased ones. Afterwards, we analyze the effects of this training-free prototype calibration on new classes and base classes, respectively.

### Fusing the biased prototypes with calibration item

We assume there exist \(B\) classes in the base session and \(C\) classes in an incremental session, _e.g._, \(B=60,C=5\) in the CIFAR100 dataset. Without loss of generality, we only consider the base session and the first incremental session for simplification. Other incremental sessions can be obtained in the same way. Following the notations in section 3.1, the empirical prototype of \(i\)-th class can

Figure 3: (a) The \(\) represents the samples of new classes and the \(\) represents the samples of base classes. The dotted lines between samples and prototypes (_i.e._, \(c_{n}c_{n}^{base},_{n}\)) represent the corresponding classification results. Blue and purple dotted lines represent samples that are correctly classified, and red dotted lines represent misclassification. The yellow circles (_i.e._, \(\)\(\)\(\) samples) and green triangles (_i.e._, \(\)\(\)\(\) samples) in the right figure are samples with prediction changes after calibration. (b) The detailed ratio (%) of base and new classes with regard to three types of samples(_i.e._, \(\) samples, \(\)\(\)\(\) samples, \(\)\(\)\(\) samples). Only two incremental sessions (_i.e._, session 1 and session 2) of CIFAR100 are listed here for convenience. Please refer to the supplementary for results on more incremental sessions and more datasets.

be notated as \(c_{i}\). Therefore, the base prototypes are \(c_{b}(0 b B-1)\) and the new prototypes are \(c_{n}(B n B+C-1)\). As the base session contains _sufficient_ classes and _sufficient_ samples for each class, the model trained on the base session can capture the distribution of base classes and obtain well-calibrated prototypes for base classes. Generally, the empirical prototype of base classes can be regarded as approximately consistent with the expected class representation. However, due to the limited data in incremental sessions, the empirical prototypes of the novel classes are considered to be severely biased.

Based on the analysis of prototypes, we only calibrate biased prototypes in incremental sessions. Given a new class prototype \(c_{n}(B n B+C-1)\), the calibrated new prototype \(_{n}\) can be notated by:

\[_{n}=(1-)c_{n}+ c_{n}\] (4)

The calibration item \( c_{n}\) is a component of base prototypes. The hyperparameter \(\) controls the calibration strength of biased prototypes. Smaller \(\) means the calibrated prototype reflects more of the original biased prototype, while larger \(\) means the calibrated prototype heavily incorporates the base prototypes. Motivated by the observations in section 4.1, the similarity between well-calibrated base prototypes and ill-calibrated new prototypes contains auxiliary side information about the new classes. Therefore, we use weighted base prototypes to represent the calibration item \( c_{n}\) and enhance the discriminability of biased prototypes. Specifically, we compute the cosine similarity \(S_{b,n}\) between a new class prototype \(c_{n}\) and a base prototype \(c_{b}\):

\[S_{b,n}= c_{n}}{\|c_{b}\|\|c_{n}\|}\] (5)

where \(\) (\(>0\)) is the scaling hyperparameter controlling the weight distribution's sharpness. The weight of a new prototype \(c_{n}\) with respect to a base prototype \(c_{b}\) is the softmax results over all base prototypes:

\[w_{b,n}=}}{_{i=0}^{B-1}e^{S_{i,n}}}\] (6)

Finally, the calibration of biased prototypes of new classes can be formulated as follows:

\[_{n}=(1-)\,c_{n}+\, c_{n}=(1-)\,c_{n}+\, _{b=1}^{B-1}w_{b,n}c_{b}\] (7)

Notably, the above prototype rectification procedure is a _training-free_ calibration strategy because it does not introduce any learning component or training parameters. Figure 3a gives an intuitive description of the calibration effect.

### Effect of calibrated prototypes

Intuitively, \(w_{b,n}\) is larger when a new prototype \(c_{n}\) and base prototype \(c_{b}\) are more similar. Given a new prototype \(c_{n}\), we assume _the most similar base prototype_ with \(c_{n}\) is the base prototype \(c_{n}^{base}\). Therefore, given a new prototype \(c_{n}\), \(_{b=1}^{B-1}w_{b,n}c_{b} c_{n}^{base}\) when the scaling hyperparameter \(\) is large enough. From the perspective of a biased prototype, a calibrated prototype \(_{n}\) will be aligned to its most similar base prototypes \(c_{n}\) with a proper \(\).

To further comprehend the effect of TEEN on the predictions of base and new classes respectively, we define three types of test samples according to whether the prediction results change after TEEN: with unchanged predictions (_i.e._, **UC** samples), with prediction going from right to wrong (_i.e._, **R\(\)W** samples), with predictions going from wrong to right (_i.e._, **W\(\)R** samples). We analyze the specifics of these three types of samples in detail.

Intuitively, some calibrated prototypes of new classes are aligned to base prototypes and reduce the discriminability of base prototypes. Oppositely, aligning the biased prototype to most similar base prototypes can calibrate the prediction of new classes. As shown in Table 3b, we observe the **W\(\)R** samples are mainly from new classes and the **R\(\)W** samples are mainly from base classes. Besides, extensive comparison results on the benchmark datasets (_i.e._, Table 3 and Figure 4) show that the negative effect of TEEN is negligible due to the significance of TEEN.

## 6 Experiments

In this section, we first introduce the main experiment details of FSCIL in section 6.1, which include the implementation and performance detail. Subsequently, we introduce the FSL performance of TEEN in section 6.2, which consistently shows the effectiveness of TEEN. Finally, we evaluate TEEN by a comprehensive ablation study in section 6.3.

### Main experimental details of FSCIL

#### 6.1.1 Implementation Details

**Datasets and baseline details:** Following previous methods [42; 57; 58], we evaluate TEEN on CIFAR100 , CUB200-2011 , _mini_ImageNet . We keep the dataset split consistent with existing methods [57; 58]. Notably, each benchmark dataset is divided into subsets containing nonoverlapping label space. For example, CIFAR100 is divided into 60 classes for the base session and the left 40 classes are divided into eight 5-way 5-shot few-shot classification tasks. To validate the performance of TEEN, we compare TEEN with popular CIL methods [35; 4; 18], FSL methods [26; 56; 43], and FSCIL methods [42; 57; 58]. Please refer to the supplementary material for more datasets and baseline details.

**Training details:** All experiments are conducted with PyTorch  on a single NVIDIA 3090. The training of the feature extractor uses vanilla cross-entropy loss as the objective function. It does not evolve any extra complex pretraining module [58; 59; 10; 57], thus making TEEN more efficient and elegant. In addition, we adopt the cosine similarity to measure the similarity between the instances and class prototypes. Following [42; 57; 58], we use ResNet20  for CIFAR100, pre-trained ResNet18  for CUB200 and randomly initialized ResNet18  for _mini_ImageNet. All compared methods use _the same backbone network and initialization_ for a fair comparison. We set \(=0.5,=16\) for _mini_ImageNet and CUB200, \(=0.1,=16\) for CIFAR100. We train the feature extractor on CUB200 with a learning rate of 0.004, batch size of 128, and epochs of 400. Please refer to the supplementary for more training details on CIFAR100 and _mini_ImageNet.

Figure 4: Top-1 average accuracy on all seen classes in each incremental session. We annotate the performance gap between TEEN and the runner-up method by \(\).

   } &  &  &  \\    & 0 & 1 & & 2 & 3 & 4 & 5 & 6 & 7 & 8 & \\  iCARL  & 61.31 & 46.32 & 42.94 & 37.63 & 30.49 & 24.00 & 20.89 & 18.80 & 17.21 & 44.10 & **+22.65** \\ EEIL  & 61.31 & 46.58 & 44.00 & 37.29 & 33.14 & 27.12 & 24.10 & 21.57 & 19.58 & 41.73 & **+20.28** \\ Rebalancing  & 61.31 & 47.80 & 39.31 & 31.91 & 25.68 & 21.35 & 18.67 & 17.24 & 14.17 & 47.14 & **+25.69** \\  TOPIC  & 61.31 & 50.09 & 45.17 & 41.16 & 37.48 & 35.52 & 32.19 & 29.46 & 24.42 & 36.89 & **+15.44** \\ Decoupled-NegCosine  & 71.68 & 66.64 & 62.57 & 58.82 & 55.91 & 52.88 & 49.41 & 47.50 & 45.81 & 25.87 & **+4.42** \\ Decoupled-Cosine  & 70.37 & 65.45 & 61.41 & 58.00 & 54.81 & 51.89 & 49.10 & 47.27 & 45.63 & 24.74 & **+3.29** \\ Decoupled-DecepEMD  & 69.77 & 64.59 & 60.21 & 56.63 & 53.16 & 50.13 & 47.79 & 45.42 & 43.41 & 26.36 & **+4.91** \\ CEC  & 72.00 & 66.83 & 62.97 & 59.43 & 56.70 & 53.73 & 51.19 & 49.24 & 47.63 & 24.37 & **+2.92** \\ FACT  & 72.56 & 69.63 & 66.38 & 62.77 & 60.6 & 57.33 & 54.34 & 52.16 & 50.49 & 22.07 & **+0.62** \\  TEEN & **73.53** & **70.55** & **66.37** & **63.23** & **60.53** & **57.95** & **55.24** & **53.44** & **52.08** & **21.45** \\   

Table 3: Detailed average accuracy of each incremental session on _mini_ImageNet dataset. Please refer to the supplementary for results on CUB200 and CIFAR100. The results of compared methods are cited from [42; 57; 58]. \(\) means higher accuracy is better. \(\) means lower PD is better.

#### 6.1.2 Comparison results

In this section, we conduct overall comparison experiments on different performance measures. These different performance measures focus on different aspects of the methods. For example, the widely-used average accuracy across all classes (_i.e._, **AvgAcc**) is difficult to reflect the performance of new classes because the base classes take a large percentage of all classes (_e.g._, 60 base classes of 100 classes in CIFAR100). Following , the Harmonic mean (_i.e._, **HMean**) is used to evaluate the balanced performance between the base and new classes. Besides the above performance measures, we also additionally evaluate the different methods of their performance on new classes (_i.e._, **NAccc**). Following [42; 57; 58], we also measure the degree of forgetting by the **Per**formance **D**ropping Rate (_i.e._, **PD**). The PD is defined as \(PD=Acc_{0}-Acc_{-1}\), _i.e._, the average accuracy dropping between the first session (_i.e._, \(Acc_{0}\)) and the last session (_i.e._, \(Acc_{-1}\)). The detailed comparison results of **PD** and **AvgAcc** are reported in Table 3, and the detailed comparison results of **NAccc** and **HMean** are reported in Table 4. These experimental results from different performance measures all demonstrate the effectiveness of TEEN. Besides, significantly lower FPR and TBR in Table 1 and Table 2 also verify the effective calibration of TEEN.

Notably, many previous state-of-the-art FSCIL methods (_e.g._, [57; 58]) usually design complex pretraining algorithms to enhance the extendibility of feature space, which may harm the discriminability of base classes. This elaborate pretraining stage may lead to an inconsistent but _negligible_ performance in the base session. Besides, PD\(\) and \(\)PD in Table 3 are not affected by the pretrained results and also show TEEN outperforms previous state-of-the-art methods.

### Comparison results of FSL

FSL can be approximated as measuring only the performance of the new classes in the first incremental session of FSCIL. Besides, FSL itself also faces the challenge of biased class prototypes due to the few-shot data. Therefore, we validate TEEN in the setting of FSL and report the compared results in Table 5. To ensure a fair comparison, we strictly followed the experimental setup of . The comparison results in Table 5 show that TEEN can easily _outperform the previous state-of-the-art method_ in several experimental settings. Notably, TEEN _does not involve any additional training cost when recognizing new classes_, and the only time cost lies in feature extraction. Therefore, once the sample features are extracted, **the inference cost of** TEEN **can be considered negligible** compared to previous methods that require heavy training.

### Ablation Study

**The influence of \(\) and \(\):** Notably, the proposed TEEN does not involve additional training-based modules or procedures after pretraining on base classes. When TEEN incrementally learns new classes, only the scaling temperature \(\) and the coefficient of calibration item \(\) need to be determined.

   Session & 1 & & 2 & & 3 & 4 & & 5 & 6 \\ 
**HMean/NAce** & HMean & NAcc & HMean & NAcc & HMean & NAcc & HMean & NAcc & HMean & NAcc \\  CEC  & 30.72 & 19.60 & 30.05 & 19.10 & 29.86 & 19.00 & 29.41 & 18.65 & 27.15 & 16.88 & 27.36 & 17.07 \\ FACT  & 30.60 & 19.20 & 27.84 & 17.10 & 25.89 & 15.67 & 23.85 & 14.20 & 22.01 & 12.92 & 20.65 & 12.00 \\  TEEN & **50.04** & **38.00** & **46.67** & **34.60** & **44.72** & **32.67** & **43.53** & **31.55** & **41.75** & **29.80** & **39.22** & **27.37** \\ \(\) & +19.32 & +18.4 & +16.62 & +15.5 & +14.86 & +13.67 & +14.12 & +12.9 & +14.6 & +12.92 & +11.86 & +10.3 \\   

Table 4: Detailed results of **HMean** and **NAccc** on _mini_ImageNet. The best results are in bold and the runner-up results are in underlines. The \(\) measures the performance gap between the best and second-best results on the corresponding session. Due to space limitations, the performance on only six incremental sessions is presented. Please refer to the supplementary for more detailed results on CUB200 and CIFAR100.

    &  &  \\  & 5w1s & 5w5s & 5w1s & 5w5s \\  ProtoNet  & 54.16 & 73.68 & 72.99 & 86.64 \\ NegCosine  & 62.33 & 80.94 & 72.66 & 89.40 \\ LR with DC  & **68.57** & 82.88 & 79.56 & 90.67 \\  TEEN & 65.70 & **83.11** & **81.44** & **91.04** \\   

Table 5: Few-Shot Leaning performance of classification accuracy (%) on _mini_ImageNet and CUB. The results of compared methods are cited from . 5w1s and 5w5s mean 5way-1shot and 5way-5shot, respectively. The best results are in bold and the runner-up results are in underlines.

Specifically, we select \(\) from \(\{8,16,32,64\}\) and \(\) from \(\{0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9\}\). Figure 4(a) and Figure 4(b) show how the hyper-parameters \(\) and \(\) influence the average accuracy on all classes and new classes, respectively. It also demonstrates that a proper \(\) can improve the performance of new classes, which confirms the base prototypes can benefit the performance of new classes. Compared with larger \(\), relatively smaller \(\) can smooth the weight distribution. The smaller \(\) with stronger performance further confirm the effectiveness of utilizing the abundant semantic information from base classes.

**The influence of similarity-based weight:** To further validate the effectiveness of the semantic similarity between the base and new classes, we remove the Equation 6 and directly align the new prototypes to their corresponding \(K\) most similar base class prototypes, _i.e._, \(}_{n}=\,_{n}+(1-)\,_{k=1}^{K}_{k}\). We denote this **Simple** version of TEEN _without similarity-based weight_ as SimTEEN. We select \(K\) from \(\{1,3,5,7,9\}\). Figure 4(c) and Figure 4(d) show how the similarity-based weight (_i.e._, Equation 6) influences the average accuracy on all classes and new classes, respectively. Notably, as shown in Figure 4(d), there is a significant drop, particularly on new classes, after removing the similarity-based weight. This phenomenon confirms that TEEN can achieve calibration of new prototypes with the help of well-represented semantic similarity between the base and new classes.

## 7 Conclusion

Few-shot class-incremental learning is of great importance to real-world learning scenarios. In this study, we first observe existing methods usually exhibit poor performance in new classes and the samples of new classes are easily misclassified into most similar base classes. We further find that although the feature extractor trained on base classes can not well represent new classes directly, it can properly represent the semantic similarity between the base and new classes. Based on these analyses, we propose a simple yet effective training-free prototype calibration strategy (_i.e._, TEEN) for biased prototypes of new classes. TEEN obtains competitive results in FSCIL and FSL scenarios.

**Limitations:** The FSCIL methods mentioned in this paper all select the base and new classes from the _same_ dataset. In other words, current FSCIL methods assume the model pre-trains in the same domain, increasing the restrictions on pre-training data collection. Therefore, a more realistic scenario is to pre-train on a dataset that is independent of the subsequent data distribution and then perform few-shot class-incremental learning on a target dataset that we expect. The intricate problem of cross-domain few-shot class-incremental learning will be thoroughly investigated in our future works.