# Improving LLM Group Fairness on Tabular Data via In-Context Learning

Valeriia Cherepanova

Amazon AWS AI

&Chia-Jung Lee

Amazon AWS AI &Nil-Jana Akpinar

Amazon AWS AI &Riccardo Fogliato

Amazon AWS AI &Martin Andres Bertran

Amazon AWS AI &Michael Kearns

University of Pennsylvania

Amazon AWS AI &James Zou

Stanford University

Amazon AWS AI &

###### Abstract

Large language models (LLMs) have been shown to be effective on tabular prediction tasks in the low-data regime, leveraging their internal knowledge and ability to learn from instructions and examples. However, LLMs can fail to generate predictions that satisfy group fairness, that is, produce equitable outcomes across groups. Critically, conventional debiasing approaches for natural language tasks do not directly translate to mitigating group unfairness in tabular settings. In this work, we systematically investigate four empirical approaches to improve group fairness of LLM predictions on tabular datasets, including fair prompt optimization, soft prompt tuning, strategic selection of few-shot examples, and self-refining predictions via chain-of-thought reasoning. Through experiments on four tabular datasets using both open-source and proprietary LLMs, we show the effectiveness of these methods in enhancing demographic parity while maintaining high overall performance. Our analysis provides actionable insights for practitioners in selecting the most suitable approach based on their specific requirements and constraints.

## 1 Introduction

In recent years, the scope of large language models (LLMs) has broadened significantly beyond traditional natural language processing tasks, with recent research demonstrating their effectiveness in tackling challenges on tabular data, including predictive tasks . Typically, structured data is converted into textual format and provided to the language model along with a concise task description and key features. Notably, it has been shown that language models are particularly beneficial in scenarios with limited training data, as they can utilize internal knowledge about world from pre-training combined with textual instructions and few-shot examples to make predictions .

Although considerable research has been devoted to exploring and addressing issues of stereotypical bias and fairness in language models applied to natural language tasks, tabular datasets present distinct challenges, particularly in group fairness. It is important to differentiate group fairness in the context of tabular data from conventional notions of fairness in NLP tasks: group fairness in tabular problems hinges on class labels and the representation of various demographic groups within these labels, while stereotypical fairness in NLP has primarily focused on bias in model representations. Notably, achieving fairness in the typical NLP sense does not automatically ensure group-fair predictions in tabular tasks due to potential disparities in class distributions.

Recent studies have started exploring how language models handle group fairness when applied to tabular data, revealing noticeable fairness discrepancies among different demographic groups.  and  evaluate a few baseline methods for improving group fairness in tabular tasks, includingresampled fine-tuning, and few-shot learning with label flipping and find these methods to have limited effectiveness. A recent survey paper  recognizes the challenge of mitigating inherent biases in large language models through conventional fine-tuning and few-shot learning and highlights the need for more effective strategies to address group unfairness in tabular tasks.

In this work we examine four approaches for empirically improving demographic parity of LLMs when applied to making predictions on tabular datasets. These approaches include in-context methods such as prompt optimization, soft prompt tuning, few-shot in-context learning, and self-refining predictions to promote fairness. We empirically evaluate these methods using both open-source and proprietary models across four tabular datasets, demonstrating their effectiveness. Based on our analysis, we provide actionable recommendations to practitioners on the most suitable method for different scenarios, and discuss how these approaches may be adapted to other notions of fairness.

## 2 Related Work

### Large Language Models on Tabular Data

A growing body of work has applied deep learning algorithms to tabular data [2; 7; 8; 9; 10]. Relevant to our setting, some of these studies have employed LLMs to analyze tabular data that is serialized into formatted text. They show that descriptive feature names, well-defined instructions, in-context examples, and chain-of-thought reasoning enhances LLM performance [11; 12; 13]. Some specifically focus on classification tasks [1; 14; 15; 6; 16], which is also the focus of our work. The prior knowledge of LLMs allows them to perform better than traditional algorithms such as XGBoost in low-data regimes [3; 1]. However, LLM predictions can reflect inherent biases, affecting the fairness of their outcomes [17; 18].  is closely related to our work: they analyze the accuracy and fairness of LLM predictions, concluding that traditional ML models exhibit fewer disparities. Although in-context learning and finetuning do not fully close the fairness gap, label-flipping in in in-context examples significantly reduces biases, albeit at the cost of prediction performance. Our work contributes to this literature by introducing four in-context learning approaches for mitigating the demographic parity gap in tabular data predictions, demonstrating their effectiveness across widely-used fairness datasets.

Figure 1: **Overview of fairness methods explored in this work. We focus on in-context learning approaches, including fair prompt optimization and soft prompt tuning, fair few-shot examples, and self-refinement. For each method, we highlight the specific prompt components optimized in these approaches using different colors, while components of the prompts highlighted in gray do not change across strategies.**

### Bias and Stereotypes in LLMs

Despite their promising capabilities, language models also exhibit biases and stereotypes [19; 20; 21]. These biases mostly originate from the training data, which often contain historical and societal prejudices embedded within the text. Biases have been reported with respect to several demographic groups, e.g., gender, race, ethnicity, and socioeconomic status [22; 23; 24]. With the use of these models becoming more widespread, these biases have the risk to substantially reinforce harmful stereotypes and perpetuate existing inequalities, especially when deployed in high-stakes settings . Addressing these biases is essential, and several mitigation strategies have been proposed for this purpose, including data augmentation and prompt tuning [26; 27; 28; 29; 30; 31]. However, effectively applying these strategies to the large-scale pretraining corpora remains challenging. Finally, biases can be hard to detect and several datasets and methods have been proposed to help identify them [32; 33; 34; 35; 36; 37].

### Fairness on Tabular Data

Much of the work on classification and algorithmic fairness has focused on tabular datasets [38; 39; 40; 41; 42; 43; 44]. Consequently, there is a wide range of research describing the properties and trade-offs of predictive algorithms on this type of data [45; 46; 47]. Multiple works have proposed fairness-enhancing techniques for traditional ML algorithms (e.g., logistic regression), which generally work by debiasing the data, including a fairness constraint in the optimization problem, or post-processing model predictions [48; 49; 50; 51; 52; 53; 54]. Our work employs related techniques, although some of them are not directly applicable. The formalization of fairness definitions has also been extensively discussed . Fairness metrics evaluated on tabular data typically measure the equality of some target measure across demographic groups, such as accuracy or recall , which fall under the umbrella of group fairness definitions (as opposed to individual fairness definitions). One such widely-adopted measure, which we also employ in this work, is demographic parity, which ensures that the frequency of positive predictions is approximately equal across different demographic groups.

## 3 Methods

In this work we consider four empirical approaches for improving group fairness of language model predictions on tabular datasets as illustrated in Figure 1. This section provides a brief overview of each method, with subsequent sections delving into detailed descriptions and experimental results for each approach.

Fair Prompt Optimization.We demonstrate the effectiveness of prompt engineering in achieving group fairness in LLMs and show how prompt optimization can be automated. In particular, we propose to optimize a fairness-specific prompt (highlighted in blue on the left panel on Figure 1), appended to the task-specific instructions.

Soft Prompt Tuning.In addition to hard prompt optimization, we explore soft prompt tuning, which optimizes the prompt directly in the embedding space instead of discrete token space, see the second-from-left panel in Figure 1. This approach enables direct continuous optimization. We demonstrate the effectiveness of soft prompt tuning with an objective incorporating fairness regularization.

Fair Few-Shot Examples.Including class-balanced few-shot examples in-context demonstrated limited effectiveness in previous studies [4; 5]. We instead propose an approach for strategically selecting examples by filtering them based on their similarity to test samples and varying the class label ratios within these examples.

Self-Refinement.When making predictions in batches, we can utilize the chain-of-thought and self-refinement capabilities of language models to apply post-hoc corrections to predictions, see the right panel in Figure 1 for an illustration.

Experimental Details

In our experiments, we focus on scenarios with minimal or no training data, where language models excel by leveraging their inherent knowledge for predictions, often outperforming classical tabular models . For each sample, we prompt the model with task-specific instructions and relevant features (see Appendix C for prompting templates). Optionally, we may also include fairness-specific instructions and few-shot examples, depending on the method used to improve fairness. The answer is then extracted either by generating a response or by calculating token likelihoods for labels.

In experiments involving prompt selection, we use a small validation set of 50 labeled examples to assess model accuracy. We then select Pareto-optimal prompts, which represent those where any improvement in either accuracy or fairness would necessitate a compromise in the other metric. Accuracy is assessed on the validation set, while demographic parity is evaluated on the test set to identify these optimal prompts. We additionally compare our methods against Catboost tabular model trained on 50 examples  with fairness constraints applied via Fairlearn's GridSearch function following .

**Language Models** We conduct experiments using a variety of widely used language models that vary in size. Due to the computational demands of some methods, we conduct computationally intensive experiments with smaller models and reserve methods that require advanced reasoning for larger language models. Our experiments include Llama 3 8B and 70B , Mistral 7B , Mistral 8x7B  and Claude Sonnet models .

**Datasets** We explore group fairness of LLMs on a set of publicly available datasets widely used in the algorithmic fairness literature. For each of the datasets, we focus on _'gender'_ as the protected attribute. The **Adult Income** dataset , based on the 1994 US Census, predicts whether an individual's yearly income exceeds $50k (_1 = yes, 0 = no_). The **German Credit** dataset  predicts credit default risk (_1 = good, 0 = bad_) using individual attributes. The **ACS Income & Coverage** data , drawn from the US Census, is used for income (_1 = yearly income >$50k, 0 = else_) and public health coverage (_1 = public health coverage, 0 = else_) prediction tasks, focusing on 2018 data from New York. Additional dataset details are provided in Appendix E.

**Serialization and prompts** LLMs require textual input, unlike traditional tabular prediction models. In line with previous work , we serialize data points by (1) mapping categorical values to the respective strings (e.g. _gender = 1_ is mapped to _gender = male_), and (2) consolidating column names and entries into one string per row. Although we assume little to no training data, it is reasonable to expect that practitioners will provide task-specific instructions to the model to facilitate accurate predictions. For this, we construct instructions using prototype clustering on the training folds of the datasets, as suggested by . To make instructions more readable, we use GPT-4 to revise prototype information into a single summary paragraph. Please, see Appendix C for more details.

**Metrics** In this work we focus on optimizing _demographic parity_ (DP) which aims to equalize positive label selection rate across groups, i.e.

\[[f(X) G=g]=[f(X)]\]

for a binary predictor \(f\) and \(g\{,\}\). Constraint violation is reported as ratio between the smallest and largest group level selection rates \([f(X) G=g]\) with values closer to \(1\) indicating better parity. We use DP primarily because it allows to measure fairness on an unlabeled test set directly and does not require labeled training data. Although our primary focus is on demographic parity, the methods we propose can be adapted to other fairness metrics when labeled training data is available as discussed in section 6. Additionally, while our main objective is demographic parity, we also evaluate _equalized odds_ which aims to balance false positive and false negative rates across groups, i.e.

\[[f(x) G=g,Y=y]=[f(x) Y=y]\]

for a binary predictor \(f\), \(Y\{0,1\}\), and \(g\{,\}\), and report equalized odds ratio between groups.

## 5 Experimental Results

### Fair Prompt Optimization

Prompt engineering continues to play an important role in tailoring the capabilities of LLMs to various tasks . Recently,  demonstrated that integrating fairness-specific manually-curatedinstructions in the prompt, such as "it is illegal to discriminate", can attenuate counterfactual biases in model predictions. To evaluate the effectiveness of prompt engineering for fairness, we manually construct a few prompts which are supposed to improve group fairness, such as _(1) "Ignore sex completely when making predictions", (2) "Assign positive label to females and males at the same rate.", (3) "Assign positive label to females slightly more often than to males."_, with the last prompt coming from the fact that female group is the one disadvantaged. We include the accuracy and demographic parity for models queried with these prompts for Adult dataset in Figure 2 left panel.

We observe that, while these prompts can improve demographic parity in some models, finding a universal "fair instruction" that upholds group fairness consistently across multiple models is challenging. Therefore we propose to employ a prompt optimization framework using a meta-LLM, designed to dynamically refine fair instructions.

LLM as a fair prompt engineerSeveral works have shown that LLMs can act as prompt engineers producing performant prompts for downstream models . We adopt this approach following  and employ meta-LLM to iteratively refine fairness instructions for the downstream model based on feedback provided from the previous iterations. In particular, we demonstrate the most fair previous instruction and two randomly selected previous instructions, along with their demographic parity scores and selection rates across groups, see example in Figure 3. We refine fairness instructions using the meta-model over 100 iterations. For the meta-LLM we employ the same language model as the one used downstream to make predictions.

In Figure 2 we present the performance of Pareto-optimal fair prompts for the Llama 8B model, along with the performance of a CatBoost model baseline. More plots for Mistral and Mistral models are included in Appendix G. Additionally, Table 1 lists results for the fair prompts which are Pareto-optimal and achieve

Figure 3: Meta-prompt used to iteratively refine fairness-instructions using a meta-LLM.

Figure 2: Left: Accuracy and demographic parity for manually constructed fair prompts on Adult dataset, 4 models. Right: Accuracy and demographic parity for fair prompts optimized via a meta-LLM: red points denote Pareto-optimal fair prompts, the orange square shows the default modelâ€™s performance, and black points depict a CatBoost model optimized on 50 examples with grid search.

at least 0.9 demographic parity ratio. We observe, that these engineered fair prompts significantly improve fairness of the models, often without sacrificing much accuracy. In Appendix G we provide the optimized prompts achieving the best and the worst demographic parity.

### Soft Prompt Tuning

In traditional methods, standard in-processing fairness interventions often involve training machine learning models with a fairness penalty. This encourages the model to equalize selection rates or, depending on the penalty, the error rates across demographic groups . Drawing inspiration from these techniques and parameter-efficient fine-tuning methods, we propose a similar approach that can be applied to improving group fairness in language models. In particular, rather than optimizing fair prompts in the discrete space of tokens, as done in the previous section, we suggest optimizing a soft prompt by fine-tuning tokens in the embedding space. Continuous optimization in the embedding space allows us to incorporate the fairness penalty into objective directly. Specifically, we fine-tune 50 tokens initialized with task-specific instructions in the embedding space for 20 epochs. This approach applies a penalty designed to equalize the likelihoods of tokens corresponding to positive labels across groups within a batch:

\[|P(Y=1|A=0)-P(Y=1|A=1)|.\]

To tune the prompt we use 1000 samples with pseudo-labels obtained by the same language model in the zero-shot setup, simulating a scenario without labeled data. Similarly to our fair prompt engineering experiments, we identify Pareto-optimal points among fine-tuning epochs and include results for Pareto-optimal soft prompts achieving at least 0.9 test demographic parity in Table 1. We observe that while tuning soft prompts improves demographic parity across all datasets, it results in suboptimal trade-off with accuracy compared to hard prompt optimization approach. This could potentially be attributed to the sensitivity of the tuning procedure to hyperparameters or the reliance on pseudo-labels. Additionally, we include plots illustrating fairness-accuracy tradeoff for Pareto-optimal soft prompts in Appendix G.

### Fair Few-shot Examples

Prior work  has leveraged the in-context learning capabilities of language models for this problem space. They hypothesize that, when selected appropriately, few-shot examples can effectively influence the final predictions to more accurately reflect the desired notion of fairness.

For instance, it has been demonstrated that flipping the labels of few-shot examples can effectively reduce bias, albeit at the expense of significantly lower classification performance , while class- and group- balanced selection does not mitigate the bias .

    &  &  &  &  \\  Model & Acc & DP & EO & Acc & DP & EO & Acc & DP & EO & Acc & DP & EO \\  Catboost + GS & 0.76 & 0.57 & 0.67 & 0.66 & 0.75 & 0.44 & 0.63 & 0.81 & 0.65 & 0.75 & 0.85 & 0.88 \\  Llama8B Default & **0.77** & 0.78 & 0.9 & 0.56 & 0.8 & 0.66 & 0.62 & 0.73 & 0.60 & 0.71 & 0.88 & 0.95 \\ Llama8B+FairPrompt & **0.77** & **0.94** & 0.79 & 0.57 & 0.95 & 0.81 & **0.67** & 0.96 & 0.93 & **0.74** & **0.92** & 0.82 \\ Llama8B+Few-Shot & 0.76 & **0.94** & 0.77 & 0.63 & 0.91 & 0.85 & 0.61 & 0.96 & 0.9 & 0.73 & 0.9 & 0.91 \\ Llama8B+SoftPrompt & 0.73 & **0.94** & 0.84 & **0.66** & **0.97** & 0.90 & 0.59 & **0.97** & 0.88 & 0.69 & 0.89 & 0.92 \\  Mistral7B Default & **0.83** & 0.36 & 0.43 & 0.62 & 0.82 & 0.73 & **0.67** & 0.49 & 0.26 & **0.76** & 0.86 & 0.89 \\ Mistral7B+FairPrompt & 0.81 & 0.55 & 0.77 & **0.7** & 0.9 & 0.68 & 0.66 & **0.94** & 0.88 & 0.71 & 0.92 & 0.91 \\ Mistral7B+Few-Shot & 0.80 & **0.93** & 0.68 & 0.57 & 0.95 & 0.92 & 0.66 & 0.93 & 0.83 & **0.76** & **0.99** & 0.59 \\ Mistral7B+SoftPrompt & 0.75 & 0.90 & 0.62 & 0.65 & **0.97** & 0.90 & 0.56 & 0.92 & 0.88 & 0.75 & 0.85 & 0.91 \\  Mistral8x7B Default & **0.79** & 0.51 & 0.57 & 0.47 & 0.72 & 0.65 & **0.65** & 0.83 & 0.75 & 0.71 & 0.85 & 0.96 \\ Mistral8x7B+FairPrompt & 0.78 & **0.95** & 0.61 & **0.58** & 0.94 & 0.83 & 0.64 & **0.99** & 0.9 & 0.72 & 0.92 & 0.89 \\ Mistral8x7B+FewShot & 0.76 & 0.91 & 0.81 & 0.46 & **0.97** & 0.75 & 0.64 & **0.93** & 0.86 & **0.73** & **0.93** & 0.76 \\   

Table 1: Performance of optimized fair prompts, tuned soft prompts, and few-shot contexts across 3 models and 4 datasets. We report performance of Pareto-optimal instructions achieving the best validation accuracy and at least 0.9 demographic parity. Bold numbers indicate better accuracy and demographic parity across methods for each model and dataset.

With a similar goal, we propose a strategy for constructing fair few-shot examples, which differs from the previous methods in three ways. First, instead of randomly sampling examples from the training data, we apply the nearest neighbor search to select examples that are most similar to a current test instance in the feature space1. Also, we always select examples that share sensitive attribute with the test instance. Secondly, as we assume no access to training data labels, we use the language models' default zero-shot predictions as pseudo labels to construct demonstrations (similarly to soft prompt tuning experiments). Finally, we extensively manipulate the distributions of positive and negative in-context examples between groups. In particular, we test varying ratios of positive examples for female test samples, \(p_{f}=[0.1,0.3,0.5,0.7,0.9,1.0]\), and for male test samples \(p_{m}=[0.1,0.3,0.5,0.7,0.9,1.0]\), resulting in 36 ratio pairs. We hypothesise that increasing the number of positive examples for the minority group increases their selection rate, thereby promoting better parity with the majority group.

Figure 4 illustrates the impact of varying the ratio of positive examples in the prompt. The x-axis represents the ratio of positive examples for female test instances, while the color indicates the ratio of positive examples used for predictions on male samples. The results are averaged across 3 random seeds, with the band indicating the standard deviation across seeds. We observe that increasing the positive ratio for females significantly improves demographic parity, to the extent that the selection rate for females surpasses that for males. Additional figures for other models and datasets are displayed in Appendix G. These results confirm that adjusting the ratio of positive examples in-context is an effective method for manipulating the prevalence of positive class predictions, and employing different ratios across protected groups can effectively reduce disparities in selection rates.

Additionally, we compare our nearest-neighbor selection strategy with a baseline selecting examples randomly while preserving similar label ratios in-context. Appendix Figure 5 shows that including random in-context examples results in lower demographic parity with larger variance. Also, unlike the nearest-neighbor approach, there is no apparent trend showing that including more positive samples boosts the selection rate for any demographic group, highlighting the importance to including only relevant examples in-context. Finally, in Table 1 we report demographic parity ratio, equalized odds ratio and accuracy metrics for the Pareto-optimal combination of positive label ratios, which achieves the best validation accuracy and at least 0.9 demographic parity.

### Self-Refinement

In addition to in-processing methods, fairness literature also includes a wide array of post-processing techniques . These methods work by altering model outputs directly. We propose an LLM-based

    &  &  &  &  \\  Model & Acc & DP & EO & Acc & DP & EO & Acc & DP & EO & Acc & DP & EO \\  Catboost + GS & 0.76 & 0.57 & 0.67 & 0.66 & 0.75 & 0.44 & 0.63 & 0.81 & 0.65 & 0.75 & 0.85 & 0.88 \\  Llama70B Default & 0.78 & 0.53 & 0.59 & 0.58 & 0.85 & 0.61 & 0.61 & **0.81** & 0.69 & 0.75 & 0.84 & 0.99 \\ Llama70B+Self-Refine & 0.71 & **0.89** & 0.89 & 0.56 & **0.92** & 0.78 & 0.6 & 0.76 & 0.64 & 0.74 & **0.87** & 0.92 \\  Claude Default & 0.79 & 0.50 & 0.57 & 0.66 & 0.93 & 0.89 & 0.63 & **0.77** & 0.64 & 0.76 & 0.82 & 0.94 \\ Claude+Self-Refine & 0.73 & **0.98** & 0.74 & 0.63 & **0.97** & 0.66 & 0.64 & 0.72 & 0.61 & 0.75 & **0.9** & 0.88 \\   

Table 2: Results for self-refining approach across three models and four datasets. Bold numbers indicate better demographic parity between the original and refined predictions.

Figure 4: Accuracy and demographic parity ratio metrics for prompts containing few-shot examples with varying number of positive examples across groups, evaluated on Adult dataset using Llama8B.

post-processing method that leverages the reasoning capabilities of language models, along with a chain-of-thought process, to refine their own predictions. The self-refinement approach involves using language models to identify individuals from both minority and majority groups who are near the "decision boundary", and then flipping their labels to achieve the desired demographic parity ratio. Therefore, the prediction process includes two stages:

1. The model makes initial predictions on a batch of data samples.
2. The model then assesses demographic parity in a batch and adjusts predictions to attain the desired parity, if necessary.

An example prompt used to refine predictions is illustrated in Figure 1 most right panel. Given that self-refinement approach relies on the advanced reasoning capabilities of language models to analyze predictions, compute metrics of interest, and adjust individual predictions, we conduct these experiments with larger models, specifically Llamas 70B and Claude Sonnet models. We make predictions on a batch of 40 samples, and instruct the model to make adjustments only when the difference in positive rates across groups exceeds 15%. We report the results of the self-refinement approach in Table 2. For all models, refined predictions achieve improved demographic parity across all datasets except for ACS coverage, although this sometimes leads to a notable trade-off in accuracy. In addition, there is no guarantee for similar individuals to receive similar predictions with this method because of the 'correction step' which is at odds with notions of individual fairness .

## 6 Discussion

We systematically explore four empirical methods to improve group fairness of language model predictions on tabular datasets. Our experiments across four tabular datasets using multiple language models demonstrate these approaches effectively mitigate demographic disparities. We discuss the key takeaways for each method below.

**Fair Prompt Optimization** can improve not only fairness but also classification performance, contingent upon the model's "creativity." This method involves an optimization process that requires evaluating the prompt on a dataset for a number of iterations. Although the resulting instructions are interpretable, the reasons why specific instructions yield fairer results are not always clear.

**Soft prompt tuning** is computationally expensive and sensitive to the choice of hyperparameters. While this method does not yield interpretable instructions, it enables the integration of common fairness regularizers in a differentiable way and may be particularly effective for smaller models.

**Fair Few Shot Examples** is the most interpretable and predictable method, yielding optimal results across models and datasets when an optimal combination of positive examples ratios is selected. However, it uses a longer context window and may be more computationally expensive for larger datasets because of the number of forward passes needed.

**Self-refinement** requires a model with strong reasoning capabilities and does not guarantee similar predictions for similar individuals. However, this method offers a computational advantage for larger models, as predictions are made and adjusted in batches, reducing overall processing time.

We recommend fair few-shot examples and fair prompt optimization as universal approaches achieving the optimal accuracy tradeoff. Soft prompt tuning can potentially adapt smaller models, while self-refinement is useful for scenarios with limited budgets and larger language models.