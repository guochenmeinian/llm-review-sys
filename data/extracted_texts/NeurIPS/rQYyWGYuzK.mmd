# Monomial Matrix Group Equivariant

Neural Functional Networks

 Viet-Hoang Tran

Department of Mathematics

National University of Singapore

hoang.tranviet@u.nus.edu

&Thieu N. Vo

Department of Mathematics

National University of Singapore

thieuvo@nus.edu.sg

Equal contribution. Please correspond to thieuvo@nus.edu.sg.

Equal contribution. Please correspond to thieuvo@nus.edu.sg.

Theo Tran-Huu

Department of Mathematics

National University of Singapore

thotranhuu@u.nus.edu.vn

&An T. Nguyen

FPT Software AI Center

annt68@fpt.com

&Tan M. Nguyen

Department of Mathematics

National University of Singapore

tanm@nus.edu.sg

###### Abstract

Neural functional networks (NFNs) have recently gained significant attention due to their diverse applications, ranging from predicting network generalization and network editing to classifying implicit neural representation. Previous NFN designs often depend on permutation symmetries in neural networks' weights, which traditionally arise from the unordered arrangement of neurons in hidden layers. However, these designs do not take into account the weight scaling symmetries of \(\) networks, and the weight sign flipping symmetries of \(\) or \(\) networks. In this paper, we extend the study of the group action on the network weights from the group of permutation matrices to the group of monomial matrices by incorporating scaling/sign-flipping symmetries. Particularly, we encode these scaling/sign-flipping symmetries by designing our corresponding equivariant and invariant layers. We name our new family of NFNs the Monomial Matrix Group Equivariant Neural Functional Networks (Monomial-NFN). Because of the expansion of the symmetries, Monomial-NFN has much fewer independent trainable parameters compared to the baseline NFNs in the literature, thus enhancing the model's efficiency. Moreover, for fully connected and convolutional neural networks, we theoretically prove that all groups that leave these networks invariant while acting on their weight spaces are some subgroups of the monomial matrix group. We provide empirical evidences to demonstrate the advantages of our model over existing baselines, achieving competitive performance and efficiency. The code is publicly available at https://github.com/MathematicalAI-NUS/Monomial-NFN.

## 1 Introduction

Deep neural networks (DNNs) have become highly versatile modeling tools, finding applications across a broad spectrum of fields such as Natural Language Processing , ComputerVision , and the Natural Sciences . There has been growing interest in developing specialized neural networks to process the weights, gradients, or sparsity masks of DNNs as data. These specialized neural networks are called neural functional networks (NFNs) . NFNs have found diverse applications, ranging from predicting network generalization and network editing to classifying implicit neural representations. For instance, NFNs have been employed to create learnable optimizers for neural network training , extract information from implicit neural representations of data , perform corrective editing of network weights , evaluate policies , and conduct Bayesian inference using networks as evidence .

Developing NFNs is inherently challenging due to their high-dimensional nature. Some early methods to address this challenge assume a restricted training process that effectively reduced the weight space . More recent efforts have focused on building permutation equivariant NFNs that can process neural network weights without such restrictions . These works construct NFNs that are equivariant to permutations of weights, corresponded to the rearrangement of neurons in hidden layers. Such permutations, known as neuron permutation symmetries, preserve the network's behavior. However, these approaches often overlook other significant symmetries in weight spaces . Notable examples are weight scaling transformations for \(\) networks  and sign flipping transformations for \(\) and \(\) networks . Consequently, two weight spaces of a \(\) networks, that differ by a scaling transformation, two weight spaces of a \(\), or \(\) networks that differ by a sign flipping transformation, can produce different results when processed by existing permutation equivariant NFNs, despite representing the same functions. This highlights a fundamental limitation of the current permutation equivariant NFNs.

**Contribution.** In this paper, we extend the study of symmetries in weight spaces of Fully Connected Neural Networks (FCNNs) and Convolution Neural Networks (CNNs) by formally establishing a group of symmetries that includes both neuron permutations and scaling/sign-flipping transformations. These symmetries are represented by monomial matrices, which share the nonzero pattern of permutation matrices but allow nonzero entries to be any value rather than just 1. We then introduce a novel family of NFNs that are equivariant to groups of monomial matrices, thus incorporating both permutation and scaling/sign-flipping symmetries into the NFN design. We name this new family Monomial Matrix Group Equivariant Neural Functional Networks (Monomial-NFNs). Due to the expanded set of symmetries, Monomial-NFN requires significantly fewer independent trainable parameters compared to baseline NFNs, enhancing the model's efficiency. By incorporating equivariance to neuron permutations and weight scaling/sign-flipping, our NFNs demonstrate competitive generalization performance compared to existing models. Our contribution is three-fold:

1. We formally describe a group of monomial matrices satisfying the condition that the transformation of weight spaces of FCNNs and CNNs using these group elements does not change the function defined by the networks. For \(\) networks, this group covers permutation and scaling symmetries of the weight spaces, while for \(\) or \(\) networks, this group covers permutation and sign-flipping symmetries. The group is proved to be maximal in certain cases.
2. We design Monomial-NFNs, the first family of NFNs that incorporate scaling and sign-flipping symmetries of weight spaces as far as we are aware. The main building blocks of Monomial-NFNs are the equivariant and invariant linear layers for processing weight spaces.
3. We show that the number of parameters in our equivariant linear layer is much lower than in recent permutation equivariant NFNs. In particular, our method is linear in the number of layers and dimensions of weights and biases, compared to quadratic as in . This demonstrates that Monomial-NFNs have the ability to process weight spaces of large-scale networks.

We evaluate Monomial-NFNs on three tasks: predicting CNN generalization from weights using Small CNN Zoo , weight space style editing, and classifying INRs using INRs data . Experimental results show that our model achieves competitive performance and efficiency compared to existing baselines.

**Organization.** We structure this paper as follows: After summarizing some related work in Section 2, we recall the notions of monomial matrix group and describe their maximal subgroups preserved by some nonlinear activations in Section 3. In Section 4, we formalize the general weight space of FCNNs and CNNs, then discuss the symmetries of these weight spaces using the monomial matrices. In Section 5, we construct monomial matrix group equivariant and invariant layers, which are building blocks for our Monomial-NFNs. In Section 6, we present our experimental results to justify the advantages of Monomial-NFNs over the existing permutation equivariant NFN baselines. The paper ends with concluding remarks. More experimental details are provided in the Appendix.

Related Work

**Symmetries of Weight Spaces.** The challenge of identifying the symmetries in the weight spaces of neural networks, or equivalently, determining the functional equivalence of neural networks, is a well-explored area in academic research [3; 10; 16; 23; 47]. This problem was initially posed by Hecht-Nielsen in . Results for various types of networks have been established as in [1; 2; 12; 14; 22; 38; 63].

**Neural Functional Networks.** Recent research has focused on learning representations for trained classifiers to predict their generalization performance and other insights into neural networks [8; 20; 64; 54; 53; 55]. In particular, low-dimensional encodings for Implicit Neural Representations (INRs) have been developed for downstream tasks [18; 41]. Other studies have encoded and decoded neural network parameters mainly for reconstruction and generation purposes [6; 21; 34; 48].

**Equivariant Neural Functional Networks.** Permutations and scaling, for ReLU networks, as well as sign-flipping, for sine or tanh networks, symmetries, are fundamental symmetries of weight networks. Permutation-equivariant NFNs are successfully built in [4; 35; 40; 45; 70; 71; 72]. In particular, the authors in [35; 40] carefully construct computational graphs representing the input neural networks' parameters and process the graphs using graph neural networks. In , neural network parameters are efficiently encoded by carefully choosing appropriate set-to-set and set-to-vector functions. The authors in  view network parameters as a special case of a collection of tensors and then construct maximally expressive equivariant linear layers for processing any collection of tensors given a description of their permutation symmetries. These methods are applicable to several types of networks, including those with branches or transformers. However, these models were not necessarily equivariant to scaling nor sign-flipping transformations, which are important symmetries of the input neural networks.

Our method makes the first step toward incorporating both permutation and non-permutation symmetries into NFNs. In particular, the model proposed in our paper is equivariant to permutations and scaling, for ReLU networks, or sign-flipping, for sine and tanh networks. This leads to a significant reduction in the number of parameters, a property that is particularly useful for large neural networks in modern deep learning, while achieving comparable or better results than those in the literature. The authors in [32; 67] have also developed NFNs that incorporates scaling symmetries.

## 3 Monomial Matrices Perserved by a Nonlinear Activation

Given two sets \(X,Y\), and a group \(G\) acts on them, a function \( X Y\) is called \(G\)-equivariant if \((g x)=g(x)\) for all \(x X\) and \(g G\). If \(G\) acts trivially on \(Y\), then we say \(\) is \(G\)-invariant. In this paper, we consider NFNs which are equivariant with respect to certain symmetries of deep weight spaces. These symmetries will be represented by monomial matrices. In Subsection 3.1, we recall the notion of monomial matrices, as well as their actions on space of matrices. We then formalize the maximal group of matrices preserved by the activations \(\), \(\) and \(\) in Subsection 3.2.

### Monomial Matrices and Monomial Matrix Group Actions

All matrices considered in this paper have real entries and \(n\) is a positive integer.

**Definition 3.1** (See [50, page 46]).: A matrix of size \(n n\) is called a _monomial matrix_ (or _generalized permutation matrix_) if it has exactly one non-zero entry in each row and each column, and zeros elsewhere. We will denote by \(_{n}\) the set of such all matrices.

_Permutation matrices_ and _invertible diagonal matrices_ are special cases of monomial matrices. In particular, a permutation matrix is a monomial matrix in which the non-zero entries are all equal to \(1\). In case the nonzero entries of a monomial matrix are in the diagonal, it becomes an invertible diagonal matrix. We will denote by \(_{n}\) and \(_{n}\) the sets of permutation matrices and invertible diagonal matrices of size \(n n\), respectively. It is well-known that the groups \(_{n},\,_{n}\), and \(_{n}\) are subgroups of the general linear group \((n)\).

Permutation matrix group \(_{n}\) is a representation of the permutation group \(S_{n}\), which is the group of all permutations of the set \(\{1,2,,n\}\) with group operator as the composition. Indeed, for each permutation \( S_{n}\), we denote by \(P_{}\) the square matrix obtained by permuting \(n\) columns of the identity matrix \(I_{n}\) by \(\). We call \(P_{}\) the _permutation matrix_ corresponding to \(\). The correspondence \( P_{}\) defines a group homomorphism \( S_{n}(n)\) with the image \(_{n}=(S_{n})\).

Each monomial matrix in \(_{n}\) is a product of an invertible diagonal matrix in \(_{n}\) and a permutation matrix in \(_{n}\), i.e.

\[_{n}=\{DP\;:\;D_{n}\;\;P_{n}\}.\] (1)

In general, we have \(PD DP\). However, for \(D=(d_{1},d_{2},,d_{n})\) and \(P=P_{}\), we have \(PD=(PDP^{-1})P\) which is again a product of the invertible diagonal matrix

\[PDP^{-1}=(d_{^{-1}(1)},d_{^{-1}(2)},,d_{^{-1 }(n)})\] (2)

and the permutation matrix \(P\). As an implication of Eq. (2), there is a group homomorphism \(_{n}(_{n})\), defined by the conjugation, i.e. \((P)(D)=PDP^{-1}\) for all \(P_{n}\) and \(D_{n}\). The map \(\) defines the group \(_{n}\) as the semidirect product \(_{n}=_{n}_{}_{n}\) (see ). For convenience, we sometimes denote element \(DP\) of \(_{n}\) as a pair \((D,P)\).

The groups \(_{n}\), \(_{n}\) and \(_{n}\) act on the left and the right of \(^{n}\) and \(^{n m}\) in a canonical way (by matrix-vector or matrix-matrix multiplications). More precisely, we have:

**Proposition 3.2**.: _Let \(^{n}\) and \(A=(A_{ij})^{n m}\). Then for \(D=(d_{1},,d_{n})_{n}\), \(=(_{1},,_{m}) _{m}\), \(P_{}_{n}\), and \(P_{}_{m}\), we have:_

\[P_{} =(x_{^{-1}(1)},x_{^{-1}(2)},,x_{^{-1}(n)})^{},\] \[D =(d_{1} x_{1},d_{2} x_{2},,d_{n} x_{n})^{ },\] \[(D\ P_{} A P_{} )_{ij} =d_{i} A_{^{-1}(i)(j)}_{j},\] \[(D\ P_{} A( P_{} )^{-1})_{ij} =d_{i} A_{^{-1}(i)^{-1}(j)}_{j}^{-1}.\]

The above proposition can be verified by a direct computation, and is used in subsequent sections.

### Monomial Matrices Preserved by a Nonlinear Activation

We characterize the maximal matrix groups preserved by the activations \(=,\) or \(\). Here, \(\) is the rectified linear unit activation function which has been used in most of modern neural networks, sin is the sine function which is often used as an activation function in implicit neural representations , and \(\) is the hyperbolic tangent activation function. Different variants of the results in this subsection can also be found in . We refine them using the terms of monomial matrices and state explicitly here for the completeness of the paper.

**Definition 3.3**.: A matrix \(A(n)\) is said to be _preserved by an activation \(\)_ if and only if \((A)=A()\) for all \(^{n}\).

We adopt the term _matrix group preserved by an activation_ from . This term is then referred to as the _intertwiner group of an activation_ in .

**Proposition 3.4**.: _For every matrix \(A(n)\), we have:_

1. \(A\) _is preserved by the activation_ \(\) _if and only if_ \(A_{n}^{>0}\)_. Here,_ \(_{n}^{>0}\) _is the subgroup of_ \(_{n}\) _containing only monomial matrices whose nonzero entries are positive numbers._
2. \(A\) _is preserved by the activation_ \(=\) _or_ \(\) _if and only if_ \(A_{n}^{ 1}\)_. Here,_ \(_{n}^{ 1}\) _is the subgroup of_ \(_{n}\) _containing only monomial matrices whose nonzero entries are_ \( 1\)_._

A detailed proof of Proposition 3.4 can be found in Appendix C.1. As a consequence of the above theorem, \(_{n}^{>0}\) (respectively, \(_{n}^{ 1}\)) is the maximal matrix subgroup of the general linear group \((n)\) that is preserved by the activation \(\) (respectively, \(\) and \(\)).

**Remark 3.5**.: _Intuitively, \(_{n}^{>0}\) is generated by permuting and positive scaling the coordinates of vectors in \(^{n}\), while \(_{n}^{ 1}\) is generated by permuting and sign flipping. Formally, these groups can be written as the semidirect products:_

\[_{n}^{>0}=_{n}^{>0}_{}_{n}, {and}_{n}^{ 1}=_{n}^{ 1}_{}_{n},\]

_where_

\[_{n}^{>0}=\{D=(d_{1},,d_{n})\;:\;d_{i}>0\}\,, { and}\] (3)

\[_{n}^{ 1}=\{D=(d_{1},,d_{n})\;:\;d_{i}\{-1,1\}\}\] (4)

_are two subgroups of \(_{n}\)._Weight Spaces and Monomial Matrix Group Actions on Weight Spaces

In this section, we formulate the general structure of the weight spaces of FCNNs and CNNs. We then determine the group action on these weight spaces using monomial matrices. The activation function \(\) using on the considered FCNNs and CNNs are assumed to be \(\) or \(\) or \(\).

### Weight Spaces of FCNNs and CNNs

**Weight Spaces of FCNNs.** Consider an FCNN with \(L\) layers, \(n_{i}\) neurons at the \(i\)-th layer, and \(n_{0}\) and \(n_{L}\) be the input and output dimensions, together with the activation \(\), as follows:

\[f(\,;\,U,)=W^{(L)}((W^{(2)} (W^{(1)}+b^{(1)})+b^{(2)}) )+b^{(L)}.\] (5)

Here, \(U=(W,b)\) is the parameters with the weights \(W=\{W^{(i)}^{n_{i} n_{i-1}}\}_{i=1}^{L}\) and the biases \(b=\{b^{(i)}^{n_{i} 1}\}_{i=1}^{L}\). The pair \(U=(W,b)\) belongs to the weight space \(=\), where:

\[ =^{n_{L} n_{L-1}}^{n_ {2} n_{1}}^{n_{1} n_{0}},\] (6) \[ =^{n_{L} 1}^{n_{2}  1}^{n_{1} 1}.\] (7)

**Weight Spaces of CNNs.** Consider a CNN with \(L\) convolutional layers, ending with an average pooling layer then fully connected layers, together with activation \(\). Let \(n_{i}\) and \(w_{i}\) be the number of channels and the size of the convolutional kernel at the \(i^{}\) convolutional layer. We will only take account of the \(L\) convolutional layers, since the weight space of the fully connected layers are already considered above, and the pooling layer has no learnable parameters:

\[f(\,;\,U,)=(W^{(L)}( (W^{(2)}(W^{(1)}+b^{(1)})+b^{(2)} ))+b^{(L)})\] (8)

Here, \(U=(W,b)\) is the learnable parameters with the weights \(W=\{W^{(i)}^{w_{i} n_{i} n_{i-1}}\}_{i=1}^{L}\) and the biases \(b=\{b^{(i)}^{1 n_{i} 1}\}_{i=1}^{L}\). The convolutional operator \(\) is defined depending on the purpose of the model, and adding \(b\) means adding \(b_{j}^{(i)}\) to all entries of \(j^{-}\)th channel at \(i^{}\) layer. The pair \(U=(W,b)\) belongs to the weight space \(=\), where:

\[ =^{w_{L} n_{L} n_{L-1}} ^{w_{2} n_{2} n_{1}}^{w_{1} n_{1}  n_{0}},\] (9) \[ =^{1 n_{L} 1}^{1  n_{2} 1}^{1 n_{1} 1}.\] (10)

**Remark 4.1**.: _See in Appendix. C.2 for concrete descriptions of weight spaces of FCNNs and CNNs._

### Monomial Matrix Group Action on Weight Spaces

The **weight space**\(\) of an FCNN or CNN with \(L\) layers and \(n_{i}\) channels at \(i^{}\) layer has the general form \(=\), where:

\[ =^{w_{L} n_{L} n_{L-1}} ^{w_{2} n_{2} n_{1}}^{w_{1} n_{1}  n_{0}},\] (11) \[ =^{b_{L} n_{L} 1} ^{b_{2} n_{2} 1}^{b_{1} n_{1} 1}.\] (12)

Here, \(n_{i}\) is the number of channels at the \(i^{}\) layer, in particular, \(n_{0}\) and \(n_{L}\) are the number of channels of input and output; \(w_{i}\) is the dimension of weights and \(b_{i}\) is the dimension of the biases in each channel at the \(i\)-th layer. The dimension of the weight space \(\) is:

\[=_{i=1}^{L}(w_{i} n_{i} n_{i-1}+b_{i}  n_{i} 1).\] (13)

**Notation.** When working with weight matrices in \(\), the space \(^{w_{i} n_{i} n_{i-1}}=(^{w_{i}})^{n_{i} n _{i-1}}\) at the \(i^{}\) layer will be considered as the space of \(n_{i} n_{i-1}\) matrices, whose entries are real vectors in \(^{w_{i}}\). s In particular, the symbol \(W^{(i)}\) denotes a matrix in \(^{w_{i} n_{i} n_{i-1}}=(^{w_{i}})^{n_{i} n _{i-1}}\), while \(W^{(i)}_{jk}^{w_{i}}\) denotes the entry at row \(j\) and column \(k\) of \(W^{(i)}\). Similarly, the notion \(b^{(i)}\) denotes a bias column vector in \(^{b_{i} n_{i} 1}=(^{b_{i}})^{n_{i} 1}\), while \(b_{j}^{(i)}^{b_{i}}\) denotes the entry at row \(j\) of \(b^{(i)}\).

To define the **group action of**\(\) using monomial matrices, denote \(_{}\) as the group:

\[_{}_{n_{L}} _{n_{1}}_{n_{0}}.\]Ideally, each monomial matrix group \(_{n_{i}}\) will act on the weights and the biases at the \(i^{}\) layer of the network. Each element of \(_{}\) will be of the form \(g=(g^{(L)},,g^{(0)})\), where:

\[g^{(i)}=D^{(i)} P_{_{i}}=(d_{1}^{(i)},, d_{n_{i}}^{(i)}) P_{_{i}}_{n_{i}}\] (14)

for some invertible diagonal matrix \(D^{(i)}\) and permutation matrix \(P_{_{i}}\). The action of \(_{}\) on \(\) is defined formally as follows.

**Definition 4.2** (Group action on weight spaces).: With the notation as above, the _group action_ of \(_{}\) on \(\) is defined to be the map \(_{}\) with \((g,U) gU=(gW,gb)\), where:

\[(gW)^{(i)}(g^{(i)}) W^{(i)}(g^{( i-1)})^{-1}(gb)^{(i)}(g^{(i)}) b^{(i)}.\] (15)

In concrete:

\[(gW)^{(i)}_{jk}^{(i)}}{d_{k}^{(i-1)}} W^{ (i)}_{_{i}^{-1}(j)_{i-1}^{-1}(k)}(gb)^{(i)}_{j} d_{j}^{(i)} b^{(i)}_{_{i}^{-1}(j)}.\] (16)

**Remark 4.3**.: _The group \(_{}\) is determined only by the number of layers \(L\) and the numbers of channels \(n_{i}\), not by the dimensions of weights \(w_{i}\) and biases \(b_{i}\) at each channel._

The group \(_{}\) has nice behaviors when acting on the weight spaces of FCNNs given in Eq. (5) and CNNs given in Eq. (8). In particular, depending on the specific choice of the activation \(\), the function \(f\) built by the given FCNN or CNN is invariant under the action of a subgroup \(G\) of \(_{}\), as we will see in the following proposition.

**Proposition 4.4** (\(G\)-equivariance of neural functionals).: _Let \(f=f(\,;\,U,)\) be an FCNN given in Eq. (5) or CNN given in Eq. (8) with the weight space \(U\) and an activation \(\{,,\}\). Let us defined a subgroup \(G\) of \(_{}\) as follows:_

1. _If_ \(=\)_, we set_ \(G=\{_{_{n_{L}}}\}_{n_{L-1}}^{>0} _{n_{1}}^{>0}\{_{ _{n_{0}}}\}\)_._
2. _If_ \(=\) _or_ \(\)_, then we set_ \(G=\{_{_{n_{L}}}\}_{n_{L-1}}^{ 1} _{n_{1}}^{ 1}\{_{ _{n_{0}}}\}\)_._

_Then \(f\) is \(G\)-invariant under the action of \(G\) on its weight space, i.e._

\[f(\;;\,U,)=f(\;;\,gU,)\] (17)

_for all \(g G\), \(U\) and \(^{n_{0}}\)._

**Remark 4.5** (Maximality of \(G\)).: _The proof of Proposition 4.4 can be found in Appendix C.2. The group \(G\) defined above is even proved to be the maximal choice in the case:_

* \(=\) _and_ \(n_{L} n_{2} n_{1}>n_{0}=1\) _(see_ _[_12, 25_]__), or_
* \(=\) _(see_ _[_14, 22_]__)._

_Here, \(G\) is maximal in the sense that: if \(U^{}\) is another element in \(\) with \(f(\,;\,U,)=f(\,;\,U^{},)\), then there exists an element \(g G\) such that \(U^{}=gU\). It is natural to ask whether the group \(G\) is still maximal in the other case. This question still remains open and we leave it for future exploration._

According to Proposition 4.4, the symmetries of the weight space of an FCNN or CNN must include not only permutation matrices but also other types of monomial matrices resulting from scaling (for \(\) networks) or sign flipping (for \(\) and \(\) networks) the weights. Recent works on NFN design only take into account the permutation symmetries of the weight space. Therefore, it is necessary to design a new class of NFNs that incorporates these missing symmetries. We will introduce such a class in the next section.

## 5 Monomial Matrix Group Equivariant and Invariant NFNs

In this section, we introduce a new family of NFNs, called Monomial-NFNs, by incorporating symmetries arising from monomial matrix groups which have been clarified in Proposition 4.4. The main components of Monomial-NFNs are the monomial matrix group equivariant and invariant linear layers between two weight spaces which will be presented in Subsections 5.1 and 5.2, respectively. We will only consider the case of \(\) activation. Network architectures with other activations will be considered in detail in Appendices A and B.

In the following, \(=(,)\) is the weight space with \(L\) layers, \(n_{i}\) channels at \(i^{}\) layer, and the dimensions of weight and bias are \(w_{i}\) and \(b_{i}\), respectively (see Eqs. (11) and (12)). Since we consider ReLU network architectures, according to Proposition 4.4, the symmetries of the weight space is given by the subgroup \(G=\{_{_{n_{L}}}\}_{n_{L-1}}^{>0} _{n_{1}}^{>0}\{_{_ {n_{0}}}\}\) of \(_{}\).

### Equivariant Layers

We now construct a linear \(G\)-equivariant layer between weight spaces. These layers form the fundamental building blocks for our Monomimal-NFNs. Let \(\) and \(^{}\) be two weight spaces of the same network architecture described in Eqs. (11) and (12), i.e. they have the same number of layers as well as the same number of channels at each layer. Denote the dimension of weights and biases in each channel at the \(i\)-th layer of \(^{}\) as \(w^{}_{i}\) and \(b^{}_{i}\), respectively. Note that, in this case, we have \(_{}=_{^{}}\). We construct \(G\)-equivariant affine maps \(E^{}\) with \(+\), where \(^{^{}}\) and \(^{^{}}\) are learnable parameters.

To make \(E\) to be \(G\)-equivariant, \(\) and \(\) have to satisfy a system of constraints (usually called _parameter sharing_), which are induced from the condition \(E(gU)=gE(U)\) for all \(g G\) and \(U\). We show in details what are these constraints and how to derive the concrete formula of \(E\) in Appendix A. The formula of \(E\) is presented as follows: For \(U=(W,b)\), the image \(E(U)=(W^{},b^{})^{}\) is computed by:

\[W^{(1)}_{jk} =_{q=1}^{n_{0}}^{1jk}_{1jq}W^{(1)}_{jq}+ ^{1jk}_{1j}b^{(1)}_{j}, b^{(1)}_{j}=_{q=1}^{n_{0}} ^{1j}_{1jq}W^{(1)}_{jq}+^{1j}_{1j}b^{(1)}_{j},\] \[W^{(i)}_{jk} =^{ijk}_{ijk}W^{(i)}_{jk}, b^{(i)}_{j}=^{ij}_{ij}b^{(i)}_{j}, 1<i<L,\] (18) \[W^{(L)}_{jk} =_{p=1}^{n_{L}}^{Ljk}_{Lpk}W^{(L)}_{pk}, b^{(L)}_{j}=_{p=1}^{n_{L}}^{Lj}_{Lp}b^{(L)} _{p}+^{Lj}.\]

Here, \((,,,,)\) is the hyperparameter of \(E\). We discuss in detail the dimensions and sharing information between these parameters in Appendix A.1. Note that, we also show that all linear \(G\)-equivariant functional are in this form in Appendix A. To conclude, we have:

**Theorem 5.1**.: _With notation as above, the linear functional map \(E^{}\) defined by Eq. (18) is \(G\)-equivariant. Moreover, every \(G\)-equivariant linear functional map from \(\) to \(^{}\) are in that form._

**Number of parameters and comparison to previous works.** The number of parameters in our layer is linear in \(L,n_{0},n_{L}\), which is significantly smaller than the number of parameters in layers described in , where it is quadratic in \(L,n_{0},n_{L}\) (see Table 1). This reduction in parameter count means that our model is suitable for weight spaces of large-scale networks and deep NFNs. Intuitively, the advantage of our layer arises because the group \(G\) acting on the weight spaces in our setting is much larger, resulting in a significantly smaller number of orbits in the quotient space \(/G\). Since the number of orbits is equal to the number of parameters, this leads to a more compact representation. Additionally, the presence of the group \(_{*}^{>0}\) forces many coefficients of the linear layer \(E\) to be zero, further contributing to the efficiency of our model.

### Invariant Layers

We will construct an \(G\)-invariant layer \(I^{d}\) for a fixed integer \(d>0\). In order to do that, we will seek a map \(I\) in the form:

\[I= I_{} I_{^{>0}},\] (19)

where \(I_{^{>0}}\,:\,\) is an \(_{*}^{>0}\)-invariance and \(_{*}\)-equivariance map, \(I_{}\,:\,^{}\) is an \(_{*}\)-invariant map, and \(:^{}^{d}\) is an arbitrary multilayer perceptron to adjust the output dimension. Since \(G=_{*}^{>0}=_{*}^{>0}_{}_{*}\) (see Remark 3.5), the composition \(I= I_{} I_{^{>0}}\) is clearly \(G\)-invariant as expected. The construction of \(I_{^{>0}}\) and \(I_{}\) will be presented below.

   Subgroups of \(_{}\) & Number of parameters of \(E\) \\  \(_{n_{L}}_{n_{L-1}}_{n_{1}} _{n_{0}}\) & () & \((cc^{}L^{2})\) \\ \(\{_{_{n_{L}}}\}_{n_{L-1}} _{n_{1}}\{_{_{n_{0}}}\}\) & () & \((cc^{}(L+n_{0}+n_{L})^{2})\) \\ \(\{_{_{n_{L}}}\}_{n_{L-1}}^{>0} _{n_{1}}^{>0}\{_{_{n_{0}}}\}\) & (Ours) & \((cc^{}(L+n_{0}+n_{L}))\) \\ \(\{_{_{n_{L}}}\}_{n_{L-1}}^{ 1} _{n_{1}}^{ 1}\{_{_{n_{0}}}\}\) & (Ours) & \((cc^{}(L+n_{0}+n_{L}))\) \\   

Table 1: Number of parameters in a linear equivariant layer \(E^{}\) with respect to permutation matrix groups in , and monomial matrix groups. Here, \(c=\{w_{i},b_{j}\}\) and \(c^{}=\{w^{}_{i},b^{}_{j}\}\).

**Construct \(I_{^{>0}}\).** To capture \(^{>0}_{*}\)-invariance, we recall the notion of positively homogeneous of degree zero maps. For \(n>0\), a map \(\) from \(^{n}\) is called _positively homogeneous of degree zero_ if for all \(>0\) and \((x_{1},,x_{n})^{n}\), we have \(( x_{1},, x_{n})=(x_{1},,x_{n})\). We construct \(I_{^{>0}}\) by taking collections of positively homogeneous of degree zero functions \(\{^{(i)}_{jk}^{w_{i}}^{w_{i}}\}\) and \(\{^{(i)}_{j}^{b_{i}}^{b_{i}}\}\), each one corresponds to weight and bias of \(\). The maps \(I_{^{>0}}\) that \((W,b)(W^{},b^{})\) is defined by simply applying these functions on each weight and bias entries as follows:

\[W^{(i)}_{jk}=^{(i)}_{jk}(W^{(i)}_{jk})\;\;\;\;b^{ (i)}_{j}=^{(i)}_{j}(b^{(i)}_{j}).\] (20)

\(I_{^{>0}}\) is \(^{>0}_{*}\)-invariant by homogeneity of the \(\) functions. To make it become \(_{*}\)-equivariant, some \(\) functions have to be shared arross any axis that have permutation symmetry. We derive this relation in Appendix B. Some candidates for positively homogeneous of degree zero functions are also presented in Appendix B. They can be fixed or learnable.

**Construct \(I_{}\).** To capture \(_{*}\)-invariance, we simply take summing or averaging the weight and bias across any axis that have permutation symmetry as in . In concrete, we have \(I_{}^{}\) is computed as follows:

\[I_{}(U)=W^{(1)}_{*,:},W^{(L)}_{:,*},W^{(2)}_{*,*},,W^{( L-1)}_{*,*};v^{(L)},v^{(1)}_{*},,v^{(L-1)}_{*}.\] (21)

Here, \(\) denotes summation or averaging over the rows or columns of the weight and bias.

**Remark 5.2**.: _In our experiments, we use averaging operator since it is empirically more stable._

Finally we compose an \(\) before \(I_{}\) and \(I_{^{>0}}\) to obtain an \(G\)-invariant map. We summarize the above construction as follows.

**Theorem 5.3**.: _The functional map \(I^{d}\) defined by Eq. (19) is \(G\)-invariant._

### Monomial Matrix Group Equivariant Neural Functionals (Monomial-NFNs)

We build Monomial-NFNs by the constructed equivariant and invariant functional layers, with activations and additional layers discussed below. The equivariant NFN is built by simply stacking \(G\)-equivariant layers. For the invariant counterpart, we follow the construction in . In particular, we first stack some \(G\)-equivariant layers, then a \(^{>0}_{*}\)-invariant and \(_{*}\)-equivariant layer. This makes our NFN to be \(^{>0}_{*}\)-invariant and \(_{*}\)-equivariant. Then we finish the construction by stacking a \(_{*}\)-invariant layer and the end. This process makes the whole NFN to be \(G\)-invariant as expected.

**Activations of \(G\)-equivariant functionals.** Dealing with equivariance under action of \(_{*}\) only requires activation of the NFN is enough, since \(_{*}\) acts on only the order of channels in each channel of the weight space. For our \(G\)-equivariant NFNs, between each layer that is \(^{>0}_{*}\)-equivariant, we have to use the same type of activations as the activation in the network input (i.e. either \(,\;\) or \(\) in our consideration) to maintain the equivariance of the NFN.

**Fourier Features and Positional Embedding.** As mentioned in [35; 71; 72], Fourier Features [30; 62] and (sinusoidal) position embedding play a significant role in the performance of their functionals. Also, in , position embedding breaks the symmetry at input and output neurons, and allows us to use equivariant layers that act on input and output neurons. In our \(G\)-equivariant layers, we do not consider action on input and output neurons as mentioned. Also, using Fourier Features does not maintain \(^{>0}_{*}\), so we can not use this Fourier layer for our equivariant Monomial-NFNs, and in our invariant Monomial-NFNs, we only can use Fourier layer after the \(^{>0}_{*}\)-invariant layer. This can be considered as a limitation of Monomial-NFNs.

## 6 Experimental Results

In this session, we empirically demonstrate the performance of our Monomial Matrix Group Equivariant Neural Functional Networks (Monomial-NFNs) on various tasks that are either invariant

    & STATNN & NP & HNP & Monomial-NFN (ours) & Gap \\  Original & \(0.913 0.001\) & \(0.925 0.001\) & \(0.933 0.002\) & \(\) & \(\) \\ Augmented & \(0.914 0.001\) & \(0.928 0.001\) & \(0.935 0.001\) & \(\) & \(\) \\   

Table 2: CNN prediction on Tanh subset of Small CNN Zoo with original and augmented data.

(predicting CNN generalization from weights and classifying INR representations of images) or equivariant (weight space style editing). We aim to establish two key points. First, our model exhibits more stable behavior when the input undergoes transformations from the monomial matrix groups. Second, our model, Monomial-NFN, achieves competitive performance compared to other baseline models. Our results are averaged over 5 runs. Hyperparameter settings and the number of parameters can be found in Appendix D.

### Predicting CNN Generalization from Weights

**Experiment Setup.** In this experiment, we evaluate how our Monomial-NFN predicts the generalization of pretrained CNN networks. We employ the Small CNN Zoo , which consists of multiple network weights trained with different initialization and hyperparameter settings, together with activations \(\) or \(\). Since Monomial-NFNs depend on activations of network inputs, we divide the Small CNN Zoo into two smaller datasets based on their activations. The \(\) dataset considers the group \(_{n}^{ 0}\), while the \(\) dataset considers the group \(_{n}^{ 1}\).

We construct the dataset with additional weights that undergo random hidden vector permutation and scaling based on their monomial matrix group. For the \(\) dataset with the group \(_{n}^{>0}\), we uniformly sample the diagonal indices of \(D\) (see Eq. 14) for various ranges: \(,[1,1 10^{2}],,[1,1 10^{6}]\), while belonging to \(\{-1,1\}\) in the case of \(\) dataset with the group \(_{n}^{ 1}\). For both datasets, we compare our model with STATNN , and with two permutation equivariant neural functional networks from , referred to as HNP and NP. To compare the performance of all models, we use Kendall's \(\) rank correlation metric .

**Results.** We demonstrate the results of all models on the \(\) subset in Figure 1, showing that our model attains stable Kendall's \(\) when the scale operators are sampled from different ranges. Specifically, when the log of augmentation upper scale is 0, i.e. the data remains unaltered, our model performs as well as the HNP model. However, as the weights undergo more extensive scaling and permutation, the performance of the HNP and STATNN models drops significantly, indicating their lack of scaling symmetry. The NP model exhibits a similar trend, albeit to a lesser extent. In contrast, our model maintains stable performance throughout.

Table 2 illustrates the performance of all models on both the original and augmented \(\) subsets of CNN Zoo. Our model achieves the highest performance among all models and shows the greatest improvement after training with the augmented dataset. The gap between our model and the second-best model (HNP) increases from 0.006 to 0.008. Additionally, in both experiments, our model utilizes significantly fewer parameters than the baseline models, using only up to \(50\%\) of the parameters compared to HNP.

### Classifying implicit neural representations of images

**Experiment Setup.** In this experiment, our focus is on extracting the original data information encoded within the weights of implicit neural representations (INRs). We utilize the dataset from ,

Figure 1: CNN prediction on \(\) subset of Small CNN Zoo with different ranges of augmentations. Here the x-axis is the augment upper scale, presented in log scale. The metric used is Kendall’s \(\).

    & Monomial-NFN (ours) & NP & HNP & MLP \\  CIFAR-10 & \(\) & \(33.74 0.26\) & \(31.61 0.22\) & \(10.48 0.74\) \\ MNIST-10 & \(68.43 0.51\) & \(\) & \(66.02 0.51\) & \(10.62 0.54\) \\ FashionMNIST & \(\) & \(58.21 0.31\) & \(57.43 0.46\) & \(9.95 0.36\) \\   

Table 3: Classification train and test accuracies (%) for implicit neural representations of MNIST, FashionMNIST, and CIFAR-10. Uncertainties indicate standard error over 5 runs.

which comprises pretrained INR networks  that encode images from the CIFAR-10 , MNIST , and FashionMNIST  datasets. Each pretrained INR network is designed to map image coordinates \((x,y)\) to color pixel values - 3-dimensional RGB values for CIFAR-10 and 1-dimensional grayscale values for MNIST and FashionMNIST.

**Results.** We compare our model with NP, HNP, and MLP baselines. The results in Table 3 demonstrate that our model outperforms the second-best baseline, NP, for the FashionMNIST and CIFAR-10 datasets by \(2.94\%\) and \(0.49\%\), respectively. For the MNIST dataset, our model also obtains comparable performance.

### Weight space style editing.

**Experiment setup.** In this experiment, we explore altering the weights of the pretrained SIREN model  to change the information encoded within the network. We use the network weights provided in the HNP paper for the pretrained SIREN networks on MNIST and CIFAR-10 images. Our focus is on two tasks: the first involves modifying the network to dilate digits from the MNIST dataset, and the second involves altering the SIREN network weights to enhance the contrast of CIFAR-10 images. The objective is to minimize the mean squared error (MSE) training loss between the generated image from the edited SIREN network and the dilated/enhanced contrast image.

**Results.** Table 4 shows that our model performs on par with the best-performing model for increasing the contrast of CIFAR-10 images. For the MNIST digit dilation task, our model also achieves competitive performance compared to the NP baseline. Additionally, Figure 2 presents random samples of the digits that each model encodes for the dilation and contrast tasks, demonstrating that our model's results are visually comparable to those of HNP and NP in both tasks.

## 7 Conclusion

In this paper, we formally describe a group of monomial matrices that preserves FCNNs and CNNs while acting on their weight spaces. For \(\) networks, this group includes permutation and scaling symmetries, while for networks with \(\) or \(\) activations, it encompasses permutation and sign-flipping symmetries. We introduce Monomial-NFNs, a first-of-a-kind class of NFNs that incorporates these scaling or sign-flipping symmetries in weight spaces. We demonstrate that the low number of trainable parameters in our equivariant linear layer of Monomial-NFNs compared to previous works on NFNs, highlighting their capability to efficiently process weight spaces of deep networks. Our NFNs exhibit competitive generalization performance and efficiency compared to existing models across several benchmarks.

One limitation of our model is that, due to the large size of the group considered, the resulting linear layers can be limited in terms of expressivity. For example, a weight corresponding to an edge between two neurons will be updated based only on its previous value, ignoring other edges across the same or other layers. To resolve this issue, it is necessary to construct an equivariant nonlinear layer to encode further relations between these weights, thus enhancing the expressivity.

Another limitation is that we are uncertain about the maximality of the group \(G\) acting on the weight space of the ReLU network. Therefore, other types of symmetries may exist in the weight space beyond neuron permutation and weight scaling, and our model is not equivariant with respect to these symmetries. We leave the problem of identifying such a maximal group for future research.