# From Unstructured Data to In-Context Learning:

Exploring What Tasks Can Be Learned and When

 Kevin Christian Wibisono

University of Michigan, Statistics

kwib@umich.edu

&Yixin Wang

University of Michigan, Statistics

yixinw@umich.edu

###### Abstract

Large language models (LLMs) like transformers demonstrate impressive in-context learning (ICL) capabilities, allowing them to make predictions for new tasks based on prompt exemplars without parameter updates. While existing ICL theories often assume structured training data resembling ICL tasks (e.g., x-y pairs for linear regression), LLMs are typically trained unsupervised on unstructured text, such as web content, which lacks clear parallels to tasks like word analogy. To address this gap, we examine what enables ICL in models trained on unstructured data, focusing on critical sequence model requirements and training data structure. We find that many ICL capabilities can emerge simply from co-occurrence of semantically related word pairs in unstructured data; word analogy completion, for example, can provably arise purely through co-occurrence modeling, using classical language models like continuous bag of words (CBOW), without needing positional information or attention mechanisms. However, positional information becomes crucial for logic reasoning tasks requiring generalization to unseen tokens. Finally, we identify two cases where ICL fails: one in logic reasoning tasks that require generalizing to new, unseen patterns, and another in analogy completion where relevant word pairs appear only in fixed training positions. These findings suggest that LLMs' ICL abilities depend heavily on the structural elements within their training data.1

## 1 Introduction

Large language models (LLMs) such as transformers demonstrate remarkable in-context learning (ICL) abilities : without any parameter updates, they can recognize tasks and generate predictions from prompt examples. For instance, given the prompt _dog anjing, cat kucing, lion singa, elephant_, a well-trained LLM should detect the English-to-Indonesian pattern in the prompt and predict _gajah_--the Indonesian translation for _elephant_--as the most likely next token. The ICL capabilities of LLMs are surprising for two main reasons. First, these models are trained in an unsupervised manner on unstructured natural language data through next-token prediction, without any loss function specifically designed for ICL. Second, the training data for LLMs likely lacks sequences resembling typical ICL prompts, i.e., of the form \(c_{1}d_{1} c_{K}d_{K}\), where \((c_{k},d_{k})\)'s represent word pairs with specific semantic relationships.

Many efforts have sought to understand ICL from theoretical and empirical perspectives, e.g., gradient descent in regression and Bayesian inference. While insightful, these analyses often rely on structured training data that mirrors ICL tasks. For instance, they train on sequences of x-y pairs from various linear regression tasks and test on similar data. In practice, however, LLMs are trained in an unsupervised manner on unstructured text data, such as web content, which bears little resemblanceto typical ICL tasks like word analogy. Consequently, these analyses may only partially capture the complexities of ICL.

**This work.** We investigate common ICL tasks to identify what tasks can be learned in context by a model trained on unstructured data. Specifically, we examine essential components of sequence modeling that enable in-context learning, along with requirements on the unstructured training data.

The first set of (theoretical and empirical) results focuses on ICL for word analogy completion using frequently co-occurring tokens . This task involves identifying relationships between word pairs, such as _(country)-(capital)_ and _(English word)-(Indonesian translation)_, then applying the same relationship to complete a sequence. For this task (see left of Figure 1), we explore cases where training sentences contain one or two types of word pairs with distinct semantic relationships. We prove that, in most cases, ICL can arise by simply modeling word co-occurrence using classical (pre-transformer) language models like continuous bag of words (CBOW) , without needing positional information or attention mechanisms.

The second set of results involves ICL for logic reasoning tasks that require recognizing patterns that do not commonly co-occur in a sentence, such as _(word)-(first letter)_. For this task (see middle of Figure 1), we investigate scenarios where training sentences contain one or two distinct patterns, as well as a more realistic scenario where nuisance tokens are present. We prove that _positional information and blocked nuisance structure_ (e.g., \(pqrs\) in Figure 1) _are crucial for the success of ICL in these tasks_. This finding aligns with Chen et al.'s  observation that parallel structures in pre-training data support ICL. We also find that learned positional embeddings generally perform better, except in scenarios where the nuisance tokens are not clustered in blocks.

Finally, we present two scenarios where ICL fails regardless of model architectures (see right of Figure 1). In the first scenario (left example), we consider a logic reasoning task that involves identifying and completing meta-patterns within sequences. Here, each training sequence repeats the pattern established by its starting tokens; the ICL task sequence then requires the model to recognize this meta-pattern of repetition and generalize it to a novel, unseen starting pattern. In the second scenario (right example), we examine a word analogy completion task in which relevant word pairs appear in the unstructured training sentences but are restricted to specific fixed positions. These findings, along with their empirical and theoretical explanations, underscore that _LLMs require specific structures in the pre-training data to exhibit ICL ability_.

**Summary of contributions.** We (1) theoretically and empirically show that ICL for word analogy tasks with semantically related word pairs can arise from modeling _co-occurrence patterns_ using CBOW, (2) prove that, to recognize token patterns and generalize them to novel tokens, ICL requires _modeling positional information_ and _blocked nuisance structure_, and (3) present scenarios where ICL fails, highlighting the crucial role of _training data structure_ for ICL.

Figure 1: This paper identifies essential components for in-context learning (ICL) from pre-training on unstructured natural language data. Left sub-panels, right sub-panels, and boxed letters denote NLP examples, our abstractions, and expected outputs, respectively. Section 2 shows that ICL for word analogy tasks can arise via modeling co-occurrence information using classical language models like continuous bag of words (CBOW) (violet represents relationship-specific nuisance tokens). Section 3 establishes the necessity of modeling positional information and blocked nuisance structure for ICL tasks, enabling pattern recognition and generalization to novel tokens (violet represents nuisance tokens). Section 4 presents scenarios where ICL fails, providing theoretical explanations that underscore the critical role of training data structure in enabling ICL in language models.

**Related work.** Below, we highlight some of these studies and explain how our research aligns with, yet differs from, these approaches. We include a detailed discussion of related work in Appendix A. Numerous studies have connected ICL to classical methods, including gradient descent [3; 5; 17; 55; 66], Bayesian inference [14; 56; 67], and Newton's method . In contrast, _our work links ICL to the continuous bag of words (CBOW) model_, showing that ICL for word analogy tasks can be achieved by learning co-occurrence patterns. Several studies have examined the pre-training aspects of ICL, such as data distribution [11; 30; 37] and task diversity [42; 63]. By comparison, _our work emphasizes the importance of co-occurrence, positional information, and training data structure_ for ICL to arise. Other research has explored ICL in specific data-generating processes, such as discrete functions  and autoregressive processes . In contrast, _our work centers on data characterized by semantically related word pairs and repeating token patterns_.

## 2 In-context learning can arise by modeling co-occurrence via CBOW

In this section, we focus on in-context learning (ICL) for word analogy tasks involving word pairs that frequently co-occur in training sentences; see Figure 1 (left). To motivate the discussion, we present two experiments using the LLaMA 2 model  involving countries (or US states) and their capital cities (see Appendix K for data sources). The prompts follow the format \(c_{1}d_{1},c_{2}d_{2},,c_{6}d_{6},c_{7}\), where \(c_{i}\) is a country (or US state) and \(d_{i}\) is its capital city. In this scenario, we consider ICL successful if the model outputs \(d_{7}\)--the capital city of \(c_{7}\)--as the most likely token.

**Experiment 1.** We consider all 160 countries with a population exceeding one million in 2022. Among these countries, 31 have capital cities that are not their most populous cities, denoted by _type A_. The remaining 129 countries fall under _type B_. Each ICL prompt includes three type A countries among \(c_{1},,c_{6}\) to emphasize that the desired relationship is _(country)-(capital)_ rather than _(country)-(largest city)_. Subsequently, we randomly generate 1,000 prompts, with 500 having a \(c_{7}\) being a type A country and 500 having a \(c_{7}\) being a type B country. The model's ICL accuracy is \(0.58\) for type A and \(0.96\) for type B.

**Experiment 2.** We consider all 50 US states, among which 33 are of _type A_ and 17 are of _type B_, similarly defined. Following the setup in Experiment 1, we generate prompts for these states. The ICL accuracy is \(0.69\) for type A and \(0.84\) for type B.

In both experiments, LLaMA 2 performs better on type B prompts (i.e., the capital city as the largest city). Since larger cities tend to appear more frequently as compared to smaller ones in the model's pre-training data, this naturally raises the question: _Can/does ICL with frequently co-occurring word pairs arise purely from modeling co-occurrence patterns?_

**ICL via classical non-transformer-based language models.** We prove that, for word analogy tasks with frequently co-occurring word pairs, ICL can be achieved by modeling token co-occurrence--without needing positional encoding or attention mechanisms--using classical, non-transformer language models such as the continuous bag of words (CBOW) model . (It _does not_ imply that ICL in transformer-based models arises through learning co-occurrence patterns.) We utilize CBOW variant where each center word is modeled conditional on all other words in a sentence, not just neighboring words. Specifically, each word \(w\) has center and context embeddings \(u_{w}\) and \(v_{w}\) of the same dimension. Given a sentence \(x_{1}x_{2} x_{I}\), the \(i\)-th word (\(x_{i}\)) is distributed as \(p(x_{i}=k x_{-i})((u_{k}^{}_{j i}v_{x_{j}})/(I-1))\), with \(u_{w}\)'s and \(v_{w}\)'s learned by minimizing cross-entropy losses across all positions.

**Roadmap of Section 2.** In Section 2.1, we begin by considering a simple ICL task of the form \(c_{i_{1}}d_{i_{1}} c_{i_{}}d_{i_{}}c_{i_{+1}}\), where \((c_{i},d_{i})\) represents a frequently co-occuring word pair (e.g., a country and its capital city) and \(i_{1},i_{2},,i_{+1}\) are all distinct. The focus is to investigate whether a trained CBOW model can correctly output \(d_{i_{}}\). We also explore two other scenarios: ICL tasks of the form \(c_{i_{1}}d_{i_{1}} c_{i_{}}d_{i_{}}c_{i_{+1}}\) and \(c_{i_{1}}e_{i_{1}} c_{i_{}}e_{i_{}}c_{i_{+1}}\) in Section 2.2 (two connected word relationships), as well as \(c_{i_{1}}d_{i_{1}} c_{i_{-1}}d_{i_{-1}}c_{i_{}}\) and \(e_{i_{1}}f_{i_{1}} e_{i_{}}f_{i_{}}e_{i_{+1}}\) (two disjoint word relationships) in Section 2.3. Section 2.4 concludes with synthetic experiments supporting the theory.

### In-context learning on single-relationship word analogy tasks

We investigate ICL in single-relationship word analogy tasks, where the training data contains only one type of relationship between frequently co-occurring word pairs. This task takes the form of \(c_{i_{1}}d_{i_{1}} c_{i_{}}d_{i_{}}c_{i_{+1}}\), where \((c_{i},d_{i})\) pair represents a frequent co-occurrence, such as a country and its capital city. The vocabulary consists of \(c_{1:K},d_{1:K},r_{1:L}\), where \(r_{i}^{}s\) represent other words (e.g.,stop words). We first introduce Theorem 1, which states that ICL can arise if each sentence consists of exactly one \((c_{i},d_{i})\) pair, as long as the number of in-context examples (\(\)) is not too large. To simplify calculations, we replace the cross-entropy loss with squared loss by removing the softmax activation and comparing outputs against the one-hot encoding of target words. The proof is in Appendix B.

**Theorem 1** (ICL on single-relationship word analogy tasks).: _Let \(K,L S 3\). Suppose each training sentence of length \(S\) is generated by selecting one \((c_{i},d_{i})\) pair and \(S-2\) distinct \(r_{i}\)'s uniformly at random. We train a CBOW model with the squared loss and a sufficiently large embedding dimension on these sentences. Given a prompt \(c_{i_{1}}d_{i_{1}} c_{i_{}}d_{i_{}}c_{i_{+1}}\) with distinct \(i_{k}\)'s, the model correctly predicts \(d_{i_{+1}}\) if and only if \(2+1<}{(K+L)(S-2)^{2}(S-1)+K(S-2)(S-1)^{2}-2(S-2)^{4}}\)._

As an example, when each training sentence contains exactly one _country-capital_ pair (i.e., \((c_{i},d_{i})\)), Theorem 1 says that a trained CBOW model will correctly predict \(d_{i_{+1}}\) (i.e., the capital city of \(c_{i_{+1}}\)) given an ICL prompt of the form \(c_{i_{1}}d_{i_{1}} c_{i_{}}d_{i_{}}c_{i_{+1}}\), provided that the prompt length is not too large. Intuitively, this behavior is due to the presence of \(c_{i_{+1}}\) in the ICL prompt, leading the model to correctly predict \(d_{i_{+1}}\) given the frequent occurrences of the pair \((c_{i_{+1}},d_{i_{+1}})\) in the training data. However, when the prompt length is too large, the model will instead predict one of the \(r_{i}\)'s (see Theorem 1's proof in Appendix B for more details). Moreover, if we let \(L\) and fix \(K\) and \(S\), the condition in Theorem 1 becomes \(2+1<K(S-1)^{2}/(S-2)^{2}\). This inequality trivially holds if the prompt length is set to be \(S-1\) to match the length of the training sentences.

Furthermore, it is possible to adapt the proof of Theorem 1 to handle the case when each sentence comprises exactly two (not one) different \((c_{i},d_{i})\) pairs. In this case, letting \(L\) and fixing \(K\) and \(S\), the model correctly predicts \(d_{i_{+1}}\) given the same ICL prompt if and only if \(2+1<}{(K-2)(S-2)(S-4)-K}\). This upper bound is strictly larger than \(K(S-1)^{2}/(S-2)^{2}\): when each sentence contains exactly two \((c_{i},d_{i})\) pairs, ICL under the squared loss holds for longer prompts.

**Experiments.** To empirically verify Theorem 1 and its generalizations, we conduct experiments using the cross-entropy loss with \(S=8\), \(K=10\), \(L=20\), and \(=3\). We explore multiple \((p_{0},p_{1},p_{2})\) values, where \(p_{k}\) denotes the probability of having exactly \(k\) pairs of \((c_{i},d_{i})\) in the sentence. For each \((p_{0},p_{1},p_{2})\) triple, we also introduce a more realistic setting where \(c_{i}\) and \(d_{i}\) do not always appear together by considering its _corrupted_ version. In this setup, each \((c_{i},d_{i})\) pair has a 25% chance of being replaced with \((c_{i},r_{j})\) and a 25% chance of being replaced with \((d_{i},r_{j})\) for some \(j[L]\). More details are provided in Appendix K.

**Results.** Table 1 displays the average accuracy for each scenario, calculated over 10 repetitions. Notably, when \((p_{0},p_{1},p_{2})\) is \((0,1,0)\) or \((0,0,1)\), ICL under the cross-entropy loss achieves zero accuracy, in contrast to perfect accuracy with the squared loss as shown in Theorem 1. We believe this difference in accuracy is an artifact of the loss functions used, although its relevance is limited by the fact that it is unlikely for every sentence to contain at least one \((c_{i},d_{i})\) pair, in reality. On the other hand, perfect ICL performance is observed in other settings (e.g., when the training sentences contain either zero, one, or two \((c_{i},d_{i})\) pairs) in both the clean and corrupted scenarios. For an in-depth comparison of ICL performance using both the squared and cross-entropy loss across various numbers of demonstration examples, see Appendix C.

### In-context learning on dual-connected-relationship word analogy tasks

Building on the scenario that contains only a single type of relationship between frequently co-occurring word pairs, namely \((c_{i},d_{i})\), we now explore ICL on dual-connected-relationship word

    &  &  \\  \((p_{0},p_{1},p_{2})\) & \(d_{E}\) = 10 & \(d_{E}\) = 100 & \(d_{E}\) = 100 \\  \((0,1,0)\) & 0 & 0 & 0 & 0 \\ \((0,0,1)\) & 0 & 0 & 0 & 0 \\ \((1/2,1/2,0)\) & 1 & 0.99 & 0 & 0 \\ \((1/2,0,1/2)\) & 1 & 1 & 1 & 1 \\ \((0,1/2,1/2)\) & 1 & 1 & 0 & 0.01 \\ \((1/3,1/3,1/3)\) & 1 & 1 & 1 & 1 \\   

Table 1: ICL on different single-relationship word analogy tasks, averaged over 10 repetitions, demonstrates stable, good performance across embedding dimensions (\(d_{E}\)), as Theorem 1 suggests. The corrupted setting also demonstrates excellent ICL ability under certain scenarios.

analogy tasks. Here, some words frequently co-occur with two different types of words in the training data, represented by the relationships \((c_{i},d_{i})\) and \((c_{i},e_{i})\). For instance, \(c_{i}\) might represent a country, \(d_{i}\) its capital city, and \(e_{i}\) its currency. The vocabulary is comprised of \(c_{1:K},d_{1:K},e_{1:K},r_{1:L}\), where \(r_{i}\)'s represent other words. Moreover, the corresponding ICL tasks take the form \(c_{i_{1}}d_{i_{1}} c_{i_{}}d_{i_{}}c_{i_{+1}}\) and \(c_{i_{1}}e_{i_{1}} c_{i_{}}e_{i_{}}c_{i_{+1}}\), where the model is expected to output \(d_{i_{+1}}\) and \(e_{i_{+1}}\), respectively. These can be regarded as _task selection_ since the model should use the in-context examples to infer the tasks. We present Theorem 2, stating that a trained CBOW model can perform task selection if each sentence contains exactly two distinct \((c_{i},d_{i})\) or two distinct \((c_{i},e_{i})\) pairs with uniform probability.2

**Theorem 2** (Task selection in CBOW).: _Let \(K,L 2\) and \(S 5\). Suppose each training sentence of length \(S\) is generated by selecting two distinct \((c_{i},d_{i})\) pairs or \((c_{i},e_{i})\) pairs, and \(S-4\) distinct \(r_{i}\)'s uniformly at random. We train a CBOW model with the squared loss and a large enough embedding dimension. Given a prompt \(c_{i_{1}}d_{i_{1}} c_{i_{}}d_{i_{}}c_{i_{+1}}\) (\(c_{i_{1}}e_{i_{1}} c_{i_{}}e_{i_{}}c_{i_{+1}}\)) with distinct \(i_{k}\)'s, the model is more likely to predict \(d_{i_{+1}}\) (\(e_{i_{+1}}\)) than \(e_{i_{+1}}\) (\(d_{i_{+1}}\)). (The proof is in Appendix D.)_

According to Theorem 2, when each training sentence includes two \((c_{i},d_{i})\) pairs or two \((c_{i},e_{i})\) pairs, a trained CBOW model is capable of performing task selection. To intuitively understand this result, consider the ICL prompt of the first type, i.e., \(c_{i_{1}}d_{i_{1}} c_{i_{}}d_{i_{}}c_{i_{+1}}\). Here, the output is more likely to be \(d_{i_{+1}}\) than \(e_{i_{+1}}\) since \(d_{i_{+1}}\) co-occurs with the other \(d_{i_{j}}\)'s in the training data (and \(e_{i_{+1}}\) does not). Note that in Theorem 2, we unrealistically require each sentence to contain either two distinct \((c_{i},d_{i})\) pairs or \((c_{i},e_{i})\) pairs. However, this condition is not necessary as we empirically show next.

**Experiments.** We use the cross-entropy loss with \(S=8\), \(K=10\), \(L=60\), and \(=3\). Each training sentence is equally likely to be a _cd_ sentence (i.e., containing \((c_{i},d_{i})\) pairs) or a _ce_ sentence (i.e., containing \((c_{i},e_{i})\) pairs), but not both. We explore multiple \((p_{0},p_{1},p_{2})\)'s, where \(p_{k}\) is the probability of having exactly \(k\) pairs of \((c_{i},d_{i})\) for a _cd_ sentence, or \(k\) pairs of \((c_{i},e_{i})\) for a _ce_ sentence. Additionally, we introduce three different scenarios: _balanced_, where all \(L\) random words are equally likely to occur in both _cd_ and _ce_ sentences; _imbalanced_, where \(L/3\) words are more likely to occur in _cd_ (_ce_) sentences; and _extreme_, where \(L/3\) of the words can only occur in _cd_ (_ce_) sentences. More details are provided in Appendix K.

**Results.** Table 2 shows the accuracies of both tasks for each scenario, averaged over 10 repetitions. We observe a perfect accuracy when \((p_{0},p_{1},p_{2})\{(1/2,0/1,2),(0,1/2,1/2),(1/3,1/3,1/3)\}\) across all embedding dimensions and scenario types. The near-zero accuracy when \((p_{0},p_{1},p_{2})\) or \((0,1,0)\) or \((0,0,1)\) is again an artifact of the cross-entropy loss, as discussed in Section 2.1.

Interestingly, ICL works in the imbalanced and extreme scenarios when \((p_{0},p_{1},p_{2})=(1/2,1/2,0)\), where sentences do not contain more than one \((c_{i},d_{i})\) or \((c_{i},e_{i})\) pair. To see this, consider the balanced scenario where each \(r_{i}\) is equally probable to appear in both types of sentences. Given a prompt of the form \(c_{i_{1}}d_{i_{1}} c_{i_{}}d_{i_{}}c_{i_{+1}}\), it is easy to see that the model should output \(d_{i_{+1}}\) or \(e_{i_{+1}}\) with equal probability. On the other hand, in the imbalanced and extreme scenarios, the information from the \(r_{i}\)'s can allow for task selection, thus contributing to the success of ICL.

    &  &  &  \\  \((p_{0},p_{1},p_{2})\) & \(d_{E}\) = 10 & \(d_{E}\) = 100 & \(d_{E}\) = 100 & \(d_{E}\) = 100 & \(d_{E}\) = 100 \\  \((0,1,0)\) & (0, 0) & (0, 0) & (0, 0) & (0, 0) & (0, 0) & (0, 0) \\ \((0,0,1)\) & (0, 0) & (0, 0) & (0, 0) & (0, 0) & (0.07, 0.10) & (0, 0) \\ \((1/2,1/2,0)\) & (0.53, 0.

### In-context learning on dual-disjoint-relationship tasks

We next replicate the experiments in Section 2.2, but with disjoint word pair relationships of two distinct types with no overlapping tokens, i.e., \((c_{i},d_{i})\) and \((e_{i},f_{i})\). For example, \((c_{i},d_{i})\) represents a country and its capital city, and \((e_{i},f_{i})\) represents a company and its CEO. Our vocabulary consists of \(c_{1:K},d_{1:K},e_{1:K},f_{1:K},r_{1:L}\), where \(r_{i}\)'s represent other words; see Appendix K for details.

**Results.** Table 3 summarizes the accuracies of the ICL tasks \(c_{i_{1}}d_{i_{1}} c_{i_{d}}d_{i_{c}}c_{i_{d+1}}\) and \(e_{i_{1}}f_{i_{1}} e_{i_{d}}f_{i_{c}}e_{i_{c+1}}\) for each scenario, averaged over 10 repetitions. Similar to the connected setting in Section 2.2, we observe a perfect accuracy when \((p_{0},p_{1},p_{2})\{(1/2,0/1,2),(0,1/2,1/2),(1/3,1/3,1/3)\}\) across all embedding dimensions and scenario types. However, when \((p_{0},p_{1},p_{2})=(1/2,1/2,0)\), ICL already works well in the balanced scenario. Intuitively, this is because the two relationships are disjoint, thus making task selection easier.

In addition, we consider a _contaminated_ version of the training data where _cd_ (_ef_) sentences can contain some \(e_{i}\)'s and \(f_{i}\)'s (\(c_{i}\)'s and \(d_{i}\)'s). We also obtain a perfect accuracy when \((p_{0},p_{1},p_{2})\{(1/2,0/1,2),(0,1/2,1/2),(1/3,1/3,1/3)\}\) across all embedding dimensions and scenario types.

### Experiments on a synthetic corpus

We conduct experiments on a synthetic corpus consisting of _(country)-(capital)_ and _(country)-(IOC code)_ relationships. Each sentence in the corpus is categorized into exactly one of six possible categories: (1) exactly one country-capital pair; (2) exactly two country-capital pairs; (3) exactly one country-IOC pair; (4) exactly two country-IOC pairs; (5) exactly one country without any pair; and (6) no country. In sentences with country-capital pairs, each capital city can appear in any position relative to the country. Conversely, in sentences with country-IOC pairs, each IOC code must directly follow the country. The data source and corpus generation process are detailed in Appendix K.

Two models are trained on this corpus: a CBOW and a five-layer two-head autoregressive transformer. Both models have an embedding dimension of \(100\). We then compare the ICL accuracies for both relationships given one to five in-context examples. For the CBOW model, the country-capital accuracies are \((0.81,0.82,0.78,0.73,0.65)\) and the country-IOC accuracies are \((0.15,0.38,0.59,0.71,0.79)\). Here, the \(i\)-th number corresponds to the accuracy given \(i\) in-context examples. For the transformer, the accuracies are \((0.00,0.15,0.34,0.22,0.07)\) and \((1.00,0.77,0.78,0.97,0.99)\), respectively.

_When using the transformer, we find that the accuracies for the country-IOC task are significantly higher compared to those for the country-capital task_. This is likely because each IOC code consistently follows the corresponding country in the corpus, similar to ICL prompts. On the other hand, ICL fails to work on the country-capital task, where there is no consistent pattern in how each pair occurs in the corpus. Meanwhile, _ICL works decently well on both tasks under the CBOW model_.

## 3 The essential role of positional information in enabling in-context learning

We examine another common example of in-context learning (ICL), where the task involves predicting the first (or second) token in a sequence. This task resembles general logic reasoning tasks that require recognizing patterns that do not typically co-occur in a sentence, such as _(word)-(first letter)_[13; 62]. While Section 2 shows that positional encoding is irrelevant for ICL in word analogy tasks, positional

    &  &  &  \\  \((p_{0},p_{1},p_{2})\) & \(d_{E}\) = 10 & \(d_{E}\) = 100 & \(d_{E}\) = 100 & \(d_{E}\) = 100 & \(d_{E}\) = 100 \\  \((0,1,0)\) & (0, 0) & (0, 0) & (0, 0) & (0, 0) & (0, 0) & (0, 0) \\ \((0,0,1)\) & (0, 0) & (0, 0) & (0.16, 0.14) & (0, 0) & (0.21, 0.29) & (0, 0) \\ \((1/2,1/2,0)\) & (1, 1) & (0.82, 0.83) & (0.28, 0.27) & (0.95, 0.95) & (0.83, 0.85) & (0.91, 0.91) \\ \((1/2,0,1/2)\) & (1, 1) & (1, 1) & (1, 1) & (1, 1) & (1, 1) & (1, 1) \\ \((0,1/2,1/2)\) & (1, 1) & (1, 1) & (1, 1) & (1, 1) & (1, 1) & (1, 1) \\ \((1/3,1/3,1/3)\) & (1, 1) & (1, 1) & (1, 1) & (1, 1) & (1, 1) & (1, 1) \\   

Table 3: ICL on dual-_disjoint_-relationship tasks, averaged over 10 repetitions, achieves perfect accuracy when \((p_{0},p_{1},p_{2})\{(1/2,0/1,2),(0,1/2,1/2),(1/3,1/3,1/3)\}\) regardless of architectures and embedding dimensions (\(d_{E}\)). When \((p_{0},p_{1},p_{2})=(1/2,1/2,0)\), ICL already performs well under the balanced scenario.

information proves essential for such logic reasoning tasks. Specifically, we consider a simpler task of modeling \(x_{i_{1}}x_{i_{2}}x_{i_{3}}x_{i_{1}}\). Theorem 3 underscores the importance of positional information to correctly predict \(x_{i_{1}}\) from \(x_{i_{1}}x_{i_{2}}x_{i_{3}}\) in a single-layer model, and provides a construction of an attention-based model achieving zero loss and perfect accuracy on this task. Its proof is in Appendix E.

**Theorem 3** (Necessity of modeling positions).: _Let the vocabulary be \(=\{1,2,,|V|\}\) and the training sequences take the form \(x_{i_{1}}x_{i_{2}}x_{i_{3}}x_{i_{1}}\), where \(x_{i_{1}} x_{i_{2}} x_{i_{3}} x_{i_{1}}\) are chosen uniformly at random from \(\). Consider a one-layer model that predicts the last \(x_{i_{1}}\) via a learned function \(f(\{x_{i_{1}},x_{i_{2}}\},x_{i_{3}})\) using the cross-entropy loss. In this case, it is not possible to achieve defect accuracy or zero loss. On the other hand, we can achieve zero loss (and thus perfect accuracy) by incorporating positional information, i.e., via a learned function \((\{(x_{i_{1}},1),(x_{i_{2}},2)\},(x_{i_{3}},3))\)._

Here, \(f(\{x_{i_{1}},x_{i_{2}}\},x_{i_{3}})\) represents a scenario where the model lacks positional information (e.g., \(f\) is a one-layer autoregressive transformer without positional embeddings). Note that the output of this function is identical for inputs \(x_{i_{1}}x_{i_{2}}x_{i_{3}}\) and \(x_{i_{2}}x_{i_{1}}x_{i_{3}}\), which leads to the impossibility of attaining zero loss. In contrast, \((\{(x_{i_{1}},1),(x_{i_{2}},2)\},(x_{i_{3}},3))\) refers to a scenario where the model has access to positional information. We provide a construction of \(\) that achieves zero loss in Appendix E.

**Experiments.** We validate Theorem 3 by training transformers with causal masking to autoregressively learn sequences of the form \(x_{i_{1}}x_{i_{2}}x_{i_{3}}x_{i_{1}}\), and assessing their accuracy in predicting the last token on a separate test data of the same pattern. We use \(|V|=20\) and an embedding dimension of \(10\). We consider these settings: (i) _number of layers_: 1, 5; (ii) _positional embeddings_: learned, sinusoidal, no positional embeddings; and (iii) _train-test split_: each token in the vocabulary is the first token in both the training and test sets (_Both_), each token in the vocabulary is the first token in either set, but not both (_Either_). More details are provided in Appendix K.

**Results.** Table 4 summarizes the results. Two main findings emerge: (1) for the model to generalize to unseen sentences, each token in \(\) should be present as the first token in both the training and test sets; (2) positional embeddings are crucial when using only one attention layer. Note that in practice, the condition in (1) is likely met due to the vast size of LLMs' pre-training data.

**Multiple layers.** Proposition 1 shows that multi-layer models can encode positional information without explicit positional embeddings.

**Proposition 1** (Multi-layer models can encode positions).: _Consider the sentence \(x_{i_{1}}x_{i_{2}}x_{i_{3}}x_{i_{1}}\). Using a two-layer autoregressive model, the model's final output for predicting the last \(x_{i_{1}}\) is given by \(t(x_{i_{1}}x_{i_{2}}x_{i_{3}}):=g_{3}(\{f_{1}(\{x_{i_{1}}\}),f_{2}(\{x_{i _{1}}\},x_{i_{2}})\},f_{3}(\{x_{i_{1}},x_{i_{2}}\},x_{i_{3}}))\) for some \(f_{1},f_{2},f_{3}\), and \(g_{3}\)._

The proof is in Appendix F. Proposition 1 shows that we generally have \(t(x_{i_{1}}x_{i_{2}}x_{i_{3}}) t(x_{i_{2}}x_{i_{3}},x_{i_{3}})\), unlike in the one-layer case. Consequently, high accuracy is achievable without positional embeddings, as shown in Table 4. This result parallels findings in Haviv et al.  that autoregressive transformers implicitly encode positions, even without positional embeddings.

**Roadmap of Section 3.** In the rest of this section, we consider settings where each sentence contains repeating patterns. Section 3.1 focuses on a simple scenario where training sentences follow the form _abacdc_, where \(a b\) and \(c d\), or a noisy variation of it. The ICL prompts maintain the same pattern but use different combinations of _ab_ and _cd_ from those in the training data. Our goal is to understand what types of training data facilitate ICL in clean or noisy scenarios. Section 3.2 explores a more realistic case where two possible patterns are present: repeating the first letter (_abca_) and repeating the second letter (_abcb_).

### In-context learning on single-pattern tasks

In this section, we examine the case where the training sentences follow the pattern _abacdc_. To replicate real-world training scenarios, we also analyze how incorporating nuisance tokens into the training sentences affects the ICL capability of autoregressive models. To formalize the discussion, let our vocabulary be \(\), where \(\) represents the nuisance tokens. We define

    &  &  \\  Pos. emb. & 1-layer & 5-layer & 1-layer & 5-layer \\  Learned & 1 & 1 & 0 & 0 \\ Sinusoidal & 1 & 1 & 0 & 0 \\ No pos. emb. & 0.30 & 0.89 & 0 & 0 \\   

Table 4: Prediction accuracy with single/multi-layer models. For ICL to occur, the first tokens of training sentences should cover the entire vocabulary (_Both_). Also, positional embeddings are essential, especially in one-layer models.

\(,a b\)) and partition \(S\) into \(S_{1}\) (for training sentences) and \(S_{2}\) (for ICL prompts). This is to ensure that training sentences are distinct from ICL prompts. Furthermore, we assume \(\{c c S_{1}\}=\{c c S_{2}\}=\), where \(c[i]\) is the \(i\)-th element of \(c\). In other words, each token in \(\) can be the first token in both the training sentences and ICL prompts. We consider three scenarios:

1. _Clean_: Training data follow the form _abacdc_ where \(ab,cd S_{1}\). ICL prompts follow the form _abacd_ where _ab_, \(cd S_{2}\).
2. _One-noisy_: Training data follow the form _abacdc_ where \(ab,cd S_{1}\), with one nuisance token \(n\) randomly inserted anywhere except the last position (to ensure ICL prompts do not resemble the training data). ICL prompts follow the form _abacd_ where _ab_, \( S_{2}\).
3. _Block-noisy_: Training data follow the form _abacdc_ where \(ab,cd S_{1}\), with three consecutive nuisance tokens \(n_{1},n_{2},n_{3}\) randomly inserted while preserving the _aba_ and _cdc_ blocks. ICL prompts follow the form _abacdef_ where _ab_, \(\), \( S_{2}\).

We set the vocabulary size \(|V|=20\), the number of nuisance tokens \(N=20\), and use only one attention layer as additional layers do not improve performance. See Appendix K for more details.

**Results.** Table 5 reveals interesting phenomena. First, under the clean data scenario, ICL performs exceptionally well, with an observed performance increase with learned positional embeddings and a larger embedding dimension. However, ICL is notably challenging under the one-noisy scenario. In the block-noisy scenario, learned positional embeddings are crucial for satisfactory ICL performance. Theorem 4 formalizes these findings.

**Theorem 4** (Blocked nuisance token structure facilitates ICL).: _Consider a sufficiently large autoregressive position-aware model that can achieve the minimum possible theoretical loss. Training this model in the one-noisy (block-noisy) scenario results in zero (perfect) ICL accuracy._

The proof is in Appendix G. Theorem 4 says that ICL works perfectly under the block-noisy scenario, yet fails to work under the one-noisy scenario. However, as shown in Table 5, the use of sinusoidal positional embeddings significantly enhances prediction accuracy in the one-noisy scenario. This may be due to the fact that sinusoidal embeddings can encode relative positional information . For example, training sentences of the form _abacdc_, where \(n\), may help in predicting the most likely token following the ICL prompt _abacd_.

### In-context learning on dual-pattern tasks

We next examine the case where training data and ICL prompts contain two different patterns occurring equally likely: _abcadefd_ and _abcbdefe_, where \(a,b,c\) and \(d,e,f\) are distinct. We consider the _clean_ and _block-noisy_ scenarios as in Section 3.1, and set \(|V|=N=20\) (details in Appendix K).

**Results.** Table 6 outlines the ICL performance for both scenario types across different model configurations. Unlike the single-pattern scenario, there is an improvement in performance with five layers compared to one layer, particularly with learned positional embeddings.

This phenomenon is related to the notion of _induction heads_, where at least two layers may be necessary to distinguish the two patterns . This is reflected in Figure 2, which compares the accuracy trajectories of one-layer and five-layer models. While the five-layer setup effectively differentiates the two patterns, the one-layer configuration fails to do so. Meanwhile, in both clean and block-noisy scenarios, learned positional embeddings lead to notably higher accuracies as compared to sinusoidal ones, similar to the single-pattern case.

    & \) = 10} & \) = 100} \\  Pos. emb. & Clean & One-noisy & Block-noisy & Clean & One-noisy & Block-noisy \\  Learned & 0.97 & 0.00 & 0.95 & 1.00 & 0.00 & 1.00 \\ Sinusoidal & 0.66 & 0.10 & 0.01 & 0.96 & 0.00 & 0.55 \\ RoPE  & 0.31 & 0.00 & 0.03 & 0.48 & 0.00 & 0.00 \\   

Table 5: ICL on single-pattern tasks, averaged over 10 repetitions, achieves near-perfect accuracy in the clean data scenario regardless of architectures and embedding dimension (\(d_{E}\)). The one-noisy scenario is the most challenging, with sinusoidal embeddings giving a higher accuracy. In the block-noisy scenario, learned positional embeddings result in significantly better ICL performance.

## 4 Scenarios where in-context learning fails

In this section, we consider two scenarios where in-context learning (ICL) fails, irrespective of architectures. In Section 4.1, we consider a logic reasoning task requiring identification and generalization of a repetition meta-pattern within sequences. In Section 4.2, we explore a word analogy task where relevant word pairs appear in unstructured training sentences but are limited to fixed positions. Section 4.3 concludes with a synthetic data experiment supporting the theory.

### Failed scenario 1: Sentences with repeating patterns

In this meta-pattern recognition and generalization task, each training sequence follows a repeating pattern based on its starting tokens, and the ICL task sequence requires the model to identify this repetition and extend it to a new, unseen starting pattern. Specifically, our training data comprises sentences in the form of _abackcefe_, where, and. Note that each sentence is structured into three blocks, each consisting of three tokens with the same pattern. For the ICL task, we consider predicting from the prompt, where, and. Given the repeated pattern within each training sequence, a well-trained model might be expected to output to continue the pattern established in the in-context examples: and. However, as seen in Table 7, all models fail to recognize and apply the pattern, resulting in incorrect predictions.

**Formalization.** We now formalize a generalization of this scenario. Let the vocabulary be, and define. To ensure training sentences are distinct from the ICL prompts, we first partition into into and, where. Here, denotes the -th element of. Suppose we autoregressively train a sufficiently large position-aware model so that it is possible to achieve the minimum possible theoretical loss. The training sentences take the form, where and is independently selected from for every. Theorem 5, whose proof is in Appendix H, states that ICL fails regardless of the number of in-context examples.

**Theorem 5** (Failure of ICL: Different repeated patterns).: _Consider the generalized scenario in Section 4.1. For any, given an in-context prompt of the form where and for every, the model predicts instead of._

    & &  & = & = & = & = \\   & Pos. emb. & Clean & Block-noisy & Clean & Block-noisy \\   & Learned & (0.33, 0.33) & (0.15, 0.16) & (0.51, 0.49) & (0.49, 0.50) \\  & Sinusoidal & (0.12, 0.66) & (0.03, 0.03) & (0.51, 0.48) & (0.06, 0.10) \\   & Learned & (0.39, 0.39) & (0.23, 0.22) & (0.97, 0.98) & (0.87, 0.70) \\  & Sinusoidal & (0.32, 0.34) & (0.04, 0.04) & (0.83, 0.82) & (0.04, 0.07) \\   

Table 6: ICL on dual-pattern tasks, averaged over 10 repetitions, achieves notably better accuracy using learned than sinusoidal embeddings. Near-perfect accuracy is attained in the clean scenario by a 5-layer transformer with an embedding dimension of 100 and learned positional embeddings. The block-noisy scenario is challenging; the same model attains the best performance.

Figure 2: One-layer models fail to differentiate the two patterns in Section 3.2, as evidenced by the accuracy trajectory graph on the left. On the other hand, five-layer models are capable of doing so.

**Results.** Theorem 5 and Table 7 demonstrate that ICL achieves zero accuracy irrespective of the number of in-context examples (\(-1\)). This insight sheds light on the ICL capacity of autoregressive models. Simply put, if the pattern in the in-context examples differs significantly from any pattern in the training data, ICL may not occur. These results align with the findings of Raventos et al.  and Yadlowsky et al.  on the importance of data diversity for ICL.

### Failed scenario 2: Sentences with co-occurring word pairs restricted to fixed locations

We revisit the word analogy task in Section 2. The training data now comprises sentences of the form of \(a_{i}pqrsb_{i}\), where \((a_{i},b_{i})\) represents a frequently co-occurring word pair and \(p,q,r,s\) represent other words. For the ICL task, we consider predicting \(b_{i_{3}}\) from the prompt \(a_{i_{1}}b_{i_{4}}a_{i_{2}}b_{i_{2}}a_{i_{3}}\), where \(i_{1},i_{2},i_{3}\) are distinct. As each training sentence always contains an \((a_{i},b_{i})\) pair at a fixed location, we expect a well-trained model to output \(b_{i_{3}}\) to maintain the pattern in in-context examples: \(a_{i_{1}}b_{i_{1}}\) and \(a_{i_{2}}b_{i_{2}}\). Yet Table 7 shows none of the models can identify the patterns and predict the correct token.

**Formalization.** We now formalize a generalization of this scenario. Let the vocabulary be \(\{(a_{i},b_{i})\}_{i[I]}\), where \(=\{1,2,,|V|\}\) represent other words. As in Section 4.1, we autoregressively train a sufficiently large position-aware model that can achieve the minimum possible theoretical loss. The training sentences take the form \(a_{i}v_{1}v_{2} v_{2k}b_{i}\), where \(i\) and \(v_{1:2k}\) are independently chosen from \([I]\) and \(\), respectively, uniformly at random. Theorem 6, whose proof is in Appendix I, states that ICL fails regardless of the number of in-context examples.

**Theorem 6** (Failure of ICL: Different pattern structures).: _Consider the generalized scenario in Section 4.2. For any \(1 k+1\), given an in-context prompt of the form \(a_{i_{1}}b_{i_{1}}a_{i_{2}}b_{i_{2}} a_{i_{}}\) with distinct \(i_{j}\)'s, the model never predicts \(b_{i_{}}\): it predicts a uniform probability vector over \(\) when \(1 k\), and \(b_{i_{1}}\) when \(=k+1\)._

**Results.** Theorem 6 highlights the finding that the success of ICL relies heavily on how the patterns appear in the training data. In this scenario, the \((a_{i},b_{i})\) pairs consistently appear at the beginning and end of each training sentence, and we anticipate the model to recognize this relationship for ICL to occur. However, as shown in Theorem 6 and Table 7, this is not the case.

### Experiment on a synthetic corpus

We conduct an experiment on a synthetic corpus featuring _(country)-(capital)_ relationships. Each sentence falls into one of four categories: (1) exactly one country-capital pair, (2) exactly two country-capital pairs, (3) a single country without a pair, and (4) no country. In sentences with one country-capital pair, the capital appears in the first position, the country in the last, and each sentence contains six words (as in Section 4.2). The corpus generation process is detailed in Appendix K.

We train a five-layer two-head autoregressive transformer on this corpus, with an embedding dimension of \(100\). Similar to Section 2.4, we evaluate the ICL accuracies using prompts involving countries and their capitals. The results show zero ICL accuracy across varying in-context examples (one to five), supporting our theory.

## 5 Discussion

This paper examines how in-context learning (ICL) arises from pre-training on unstructured language data, with three key findings: (1) ICL for word analogy tasks can emerge from simple co-occurrence modeling, using models like continuous bag of words (CBOW) without positional encoding or attention; (2) positional information and structured nuisance tokens are essential for ICL in logic reasoning tasks that require recognizing rare patterns and generalizing to new tokens; and (3) the structure of training data significantly impacts ICL effectiveness.

    & &  &  \\   & Pos. emb. & \(d_{E}\) = 10 & \(d_{E}\) = 100 & \(d_{E}\) = 10 & \(d_{E}\) = 100 \\   & Learned & 0.00 & 0.00 & 0.01 & 0.00 \\  & Sinusoidal & 0.01 & 0.00 & 0.00 & 0.00 \\   & Learned & 0.00 & 0.00 & 0.00 & 0.00 \\  & Sinusoidal & 0.00 & 0.00 & 0.00 & 0.00 \\   

Table 7: ICL in failed scenarios, averaged over 10 repetitions, achieves zero accuracy for any architecture and embedding dimension (\(d_{E}\)).

**Acknowledgements.** This work was supported in part by the Office of Naval Research under grant number N00014-23-1-2590, the National Science Foundation under grant numbers 2231174 and 2310831, No. 2428059, and a Michigan Institute for Data Science Propelling Original Data Science (PODS) grant.