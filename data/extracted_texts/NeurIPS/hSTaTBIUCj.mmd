# _Imagine That!_ Abstract-to-Intricate Text-to-Image Synthesis with Scene Graph Hallucination Diffusion

_Imagine That!_ Abstract-to-Intricate Text-to-Image Synthesis with Scene Graph Hallucination Diffusion

 Shengqiong Wu Hao Fei Hanwang Zhang Tat-Seng Chua

\({}^{1}\)NExT++, School of Computing, National University of Singapore

\({}^{2}\) School of Computer Science and Engineering, Nanyang Technological University

swu@u.nus.edu {haofei37, dcscts}@nus.edu.sg hanwangzhang@ntu.edu.sg

Hao Fei is the corresponding author.

\({}^{1}\)NExT++, School of Computing, National University of Singapore

\({}^{2}\) School of Computer Science and Engineering, Nanyang Technological University

swu@u.nus.edu {haofei37, dcscts}@nus.edu.sg hanwangzhang@ntu.edu.sg

###### Abstract

In this work, we investigate the task of text-to-image (T2I) synthesis under the abstract-to-intricate setting, i.e., _generating intricate visual content from simple abstract text prompts_. Inspired by human imagination intuition, we propose a novel scene-graph hallucination (SGH) mechanism for effective abstract-to-intricate T2I synthesis. SGH carries out scene hallucination by expanding the initial scene graph (SG) of the input prompt with more feasible specific scene structures, in which the structured semantic representation of SG ensures high controllability of the intrinsic scene imagination. To approach the T2I synthesis, we deliberately build an SG-based hallucination diffusion system. First, we implement the SGH module based on the discrete diffusion technique, which evolves the SG structure by iteratively adding new scene elements. Then, we utilize another continuous-state diffusion model as the T2I synthesizer, where the overt image-generating process is navigated by the underlying semantic scene structure induced from the SGH module. On the benchmark COCO dataset, our system outperforms the existing best-performing T2I model by a significant margin, especially improving on the abstract-to-intricate T2I generation. Further in-depth analyses reveal how our methods advance.2

## 1 Introduction

The task of generating images from natural language descriptions, known as text-to-image (T2I) synthesis, has attracted significant attention . To approach T2I, various generative models have been explored, including generative adversarial networks (GANs) , variational autoencoders (VAEs) , flow-based models , and auto-regressive models (ARMs) , all of which aim to generate realistic images in high quality and high faithfulness. Most recently, diffusion-based models have been proposed, which simulate the physical process of gas diffusion for image generation . Diffusion models have shown unprecedented performance in image synthesis over existing methods, becoming the current state-of-the-art (SoTA) T2I solution .

As a long-reached viewpoint , sound T2I systems should not only achieve high-quality image generation in simple straightforward visual scenery but be more capable of synthesizing realistic images with complex scenes. Typically, detailed textual descriptions are necessarily needed to prompt the synthesis process with adequate details for high-quality vision generation. However, in a realistic world, it could also be ubiquitous to produce intricate visions without relying on lengthy elaborate prompts. For example, users may prefer T2I systems to synthesize well-detailed images while not taking too much time to write descriptions in detail. More crucially, due to the natural modality asymmetry between language and vision, even some simple words can intrinsically describe or represent abstract visual scenes with rich and complex details. Whenever mentioning the wordswith specific scenes, such as _classroom, kitchen, office_, or actional verbs e.g., _traveling, shopping_, there is always a picture with multifaceted scenes and rich-detailed backgrounds. In a word, _it is worth investigating generating intricate images from succinct abstract prompts_.

Yet existing prevailing approaches, even the SoTA diffusion models, may largely fail the abstract-to-intricate T2I, due to the lack of necessary details of input prompts (cf. Figure 1(a)). One intuitive workaround is to directly enrich the texts, i.e., adding more details for the prompts. Specifically, existing works either consider inserting additional adjectives and attributives to modify the original mentions and scenes , or concatenating raw sentences with more tangible explanations and contexts that are elicited from external large language models , e.g., ChatGPT . Unfortunately, due to the intrinsic grammar and linguistics rules, such text-based prompt enrichment can be subject to the issue of lower controllability. One problem is the _visual distraction_, where the main focus of the resulting image is dominated by other newly-added trivial contents, when aggressively inserting intermediate descriptive components into the raw texts, as exemplified in Figure 1(a). Besides, directly appending new textual descriptions would increase the prompt length and then lead to _incorrect binding_ of attributes or relations, i.e., making the image deviate from the original user intention.

As a reference, intuitively, we human beings would tackle the abstract-to-intricate T2I as a two-step painting process, i.e., from _semantic interpretation_ to _scene imagination_. In the semantic interpretation, a painter always first translates the succinct textual prompt into a structured skeleton that represents the semantic scene of key mention objects and their relations. Then, based on the initial scene, the painter mentally completes the abstract scene with more concrete and valid details. With the enriched scene structure, the final vision can be more accurately and easily rendered. Motivated by such human intuition, in this work, we propose a **scene-graph hallucination** (SGH) method for achieving effective abstract-to-intricate T2I. As illustrated in Figure 1(b), we first investigate representing the input prompt with its scene graph (SG) . SG advances in depicting the intrinsic semantics of texts (or vision) with structured representations, e.g., objects, attributes, and relationships, enabling fine-grained control of the semantic scene . Based on the SG of input text we then carry out scene hallucination, expanding the initial SG with more possible specific scene structures. Also with the SG representation, the imagination process can be much more accurate and controllable.

To implement the overall idea, we develop an SG-based **h**allucination **d**iffusion system (namely, **Salad**) for high-quality image synthesis. Salad is a fully diffusion-based T2I system, which mainly consists of a scene-driven T2I module and an SGH module. As shown in Figure 2, we first take advantage of the SoTA latent diffusion model  as our backbone T2I synthesizer, in which the overt image-generating process is controlled and navigated by the underlying semantic scene structure.

Figure 1: (a) An example of abstract-to-intricate T2I synthesis. All images are generated by the Latent diffusion Model (LDM) . LDM fails to accurately render the abstract contexts, e.g., ‘give a presentation’ and ‘office’ of the original prompt. Raw prompts can be enriched via descriptive insertion , or addition . Enriched contexts are in _blue_. (b) We illustrate the human intuition on the abstract-to-intricate T2I process: we always first grasp the semantic structure of the original prompt text, i.e., scene graph (SG), and then carry out imagination with more complete scenes based on the SG. Here the glowing nodes and edges are enriched ones.

On the other hand, we design the SGH module based on the modality-agnostic discrete diffusion model , which evolves and completes the initial SG structure of the input prompt by iteratively generating new scene elements (i.e., SG nodes). During the SG imagination process, the resulting structural representation at each step is fused into the T2I synthesizer via a hierarchical integration strategy. Further, we devise a scene sampling mechanism, via which the SGH module can generate various SG imaginations, and thus help achieve diversified image synthesis during inference.

We conduct experiments on COCO , the widely-used T2I dataset. Results show that Salad outperforms all the existing strong-performing T2I systems with significant margins. Further analysis reveals that modeling the SG structures helps synthesize high-quality images with stronger semantic controllability. Our proposed SGH mechanism is effective in inducing sound SG structures, helping produce more realistic images from short abstract text prompts. And the scene sampling strategy aids diversified T2I generation. In summary, this paper contributes in five aspects:

* We are the first to study the novel setup of intricate image synthesis from abstract texts.
* We solve the abstract-to-intricate T2I with a novel SG hallucination mechanism, which is implemented via discrete diffusion technique, performing scene enrichment with reasonable imagination.
* We propose a diffusion-based model with a hierarchical scene integration strategy for highly controllable and scalable image generation.
* We devise a scene sampling mechanism to generate various scene graphs for diversified image syntheses during inference.
* Our framework achieves new SoTA results in the abstract-to-intricate T2I generation.

## 2 Preliminary

### Scene Graph Representation

The SG (denoted as \(G\))  represents the semantic relationships among scene objects in a structure, where there are three types of nodes, i.e., **object**, **attribute**, and **relation**, cf. Figure 1(b). We formulate the object node set as \(\{o_{1},,o_{N}\}\), where \(o_{n}\) denotes \(n\)-th object node; the attribute node set as \(\{a_{1,1},,a_{N,M}\}\), where \(a_{n,m}\) means \(m\)-th attribute node of the \(n\)-th object node; the relation node set as \(\{r_{1,1},,r_{N,N}\}\), where \(r_{i,j}\) means object node \(o_{i}\) connects to the object \(o_{j}\). All nodes come with a category label \(l^{o/a/r}\), and each type of node has its own unique category vocabulary. For example as in Figure 2 right bottom, the object node \(o_{1}\) with category label \(l^{o}_{1}\) associated with two attribute nodes \(a_{1,1}\) and \(a_{1,M}\), with category label \(l^{a}_{1}\) and \(l^{a}_{23}\), respectively. And the object node \(o_{1}\) connects to the object node \(o_{2}\) through an edge \(r_{1,2}\) with the category label \(l^{r}_{7}\).

### Diffusion Models

Diffusion models (DMs)  learn to convert a simple Gaussian distribution into a data distribution. Technically, DMs consist of a forward (diffusion) process and a reverse (denoising) process. In the forward process, the given data \(_{0} q(_{0})\) is gradually corrupted into an approximately standard normal distribution \(_{T} p(_{T})\) over \(T\) steps by increasingly adding noisy, formulated as

Figure 2: Overall framework of our proposed SG-based hallucination diffusion system (Salad).

\(q(_{1:T}|_{0})=_{t=1}^{T}q(_{t}|_{t-1})\). The learned reverse process \(p_{}(_{0:T})=p(_{T})_{t=1}^{T}p_{}(_{t-1}| {x}_{t})\) gradually reduces the noise towards the data distribution. To improve the fit of a generative model to the data distribution, a variational upper bound on the negative log-likelihood is optimized:

\[_{vlb}=_{q(_{1:T}|_{0})} _{T}|_{0})}{p_{}(_{T})}+_{t=2}^{T}_{t-1}|_{t},_{0})}{p_{}(_{t-1}|_{t })}- p_{}(_{0}|_{1})\,,\] (1)

where \(p_{}()\) is estimated by a denoising network, which can be a time-conditional U-Net . Recently, latent diffusion models (LDMs) [42; 14; 19] are introduced to adopt DMs to operate in an efficient, low-dimensional latent space, where a pre-trained encoder \(\) maps the given data \(_{0}\) into a latent code \(z_{0}=(_{0})\), and a decoder reconstructs the final output image from the denoised latent \((_{0})_{0}\). Due to the higher computation sufficiency, this work thus takes the LDM as T2I backbone. Besides, DMs also have been extended to operate in discrete state spaces , i.e., performing diffusion and denoising processes over discrete variables, which have demonstrated competitive performances for discrete data, such as text  and layout . Hence, we also adopt the discrete DMs to realize the SG induction process. Appendix SSA.1 gives more technical details to the discrete diffusion models.

## 3 Methodology

Formally, T2I aims to generate an image \(x\) that faithfully reflects the desired content in the input prompt text \(y\). To approach abstract-to-intricate T2I, we propose an SG-based hallucination diffusion system (Salad), which is shown in Figure 2. The salad consists of two major modules. First, the SGH (cf. SS3.1) is responsible for enriching the initial SG of the text prompt via a discrete diffusion model. Then, built upon an LDM (cf. SS2.2), the Scene-driven Image Synthesis module (SIS, cf. SS3.2) performs denoising for image synthesis, during which the derived SG features is fused via a hierarchical scene integration mechanism. The underlying SGH closely collaborates with the upper SIS at each step, and thus the semantic scene skeleton takes fine-grained control of the overt vision rendering. We also describe the optimization (cf. SS3.3), and the scene sampling strategy (cf. SS3.4).

### Scene Graph Hallucination (SGH)

As aforementioned, we formulate the SGH as a discrete denoising diffusion process  (cf. Fig. 2). Specifically, in the forward process, the SG of the gold image, marked as \(G_{0}\), will be corrupted into a sequence of increasingly noisy latent variables \(G_{1:T}=\{G_{1},G_{2},,G_{T}\}\), where each SG node \(s^{*}_{t,j} G_{t},*\{o,a,r\}\) (\(t\) is diffusion step, \(j\) is the node index) takes a discrete value with \(K^{*}\) category labels, and \(o,a,r\) denotes the nodes' type, i.e., object (\(o\)), attribute (\(a\)), and relation (\(r\)). For simplicity, we omit subscripts \(j\) and superscripts \(*\) in the following description. The discrete diffusion process can be parameterized with a multinomial categorical transition matrix:

\[q(s_{t}|s_{t-1})=^{}(s_{t})_{t}(s_{ t-1}),\] (2)

where \((s_{t})\) denotes the column one-hot vector of \(s_{t}\). And \(_{t}\) is the transition matrix, with \([_{t}]_{ij}=q(s_{t}=j|s_{t-1}=i)\) representing the probabilities that \(s_{t-1}\) transitions to \(s_{t}\). Due to the property of the Markov chain, the cumulative probability of \(s_{t}\) at arbitrary timestep from \(s_{0}\) can be derived as \(q_{(}s_{t}|s_{0})=^{}(s_{t})}_{t}(s_{0})\), where \(}_{t}=_{t}_{t-1}_{1}\). Inspired by [1; 19], we employ a _mask-and-replace_ strategy to design the \(_{t}\). For each node \(s_{t}\), we define three probabilities: 1) a probability of \(_{t}\) to transition to [MASK] node, 2) a probability of \(K_{t}\) be resampled uniformly over all the \(K\) categories, and 3) a probability of \(_{t}=1-K_{t}-_{t}\) to stay the same node. Notedly, the [MASK] node never transits to other states. Hence, the transition matrix \(_{t}\) can be formulated as3:

\[_{t}=_{t}+_{t}&_{t}&_{t}&&0\\ _{t}&_{t}+_{t}&_{t}&&0\\ &&&&\\ _{t}&_{t}&_{t}&&1\,.\] (3)

The aforementioned discrete state-space models assume that all the standard nodes are switchable by corruption. However, as stated in SS2.1, there are three different SG nodes under separate categories. Hence, we apply three disjoint corruption matrices \(_{t}^{o}^{K^{} K^{o}}\), \(_{t}^{a}^{K^{a} K^{a}},_{t}^{r}^{K^ {r} K^{r}}\) for object, attribute, and relation nodes, respectively, where \(K^{o},K^{},K^{r}\) denotes the size of category labels of three node types respectively.

In the denoising process, we introduce an SG decoder as the neural approximator to estimate the distribution \(p_{}(s_{t-1}|s_{t},y)\). As shown in Figure 2, SG decoder first employs an adaptive normalization (AdaLN) to inject the timestep information. A text cross-attention (Text-CA) integrates the input prompt \(y\). Then, a graph cross-attention (Graph-CA) is devised to take in the induced SG (\(G_{t+1}\)) in the previous \(t\)+1 timestep:

\[}^{*}=(G_{t+1},^{*})\,,\] (4)

where \(^{*}\) are the features yielded from Text-CA. Graph-CA consistently results the overall picture of the last SG for a more coherent generation when making the current decision.

Intuitively, among the object, attribute, and relation nodes, the object nodes always come first to determine the scene subjects, followed by their modifier attributes, and then the relations between objects. Thus, instead of parallel induction of three node types, we follow this SG node-type dependence (NTD) intuition, and design an NTD cross-attention (NTD-CA) for the \(*\)-type node induction (\(*\) can be object or attribute):

\[}^{*}=(s_{t}^{o},}^{*})\,,& *=a\\ (s_{t}^{o} s_{t}^{a},}^{*})\,.&*=r\] (5)

Note that we stack multiple layers of the above calculations as one SG decoder. For each state of node types \(}^{*}_{t}\), a softmax function is put on to obtain the category label distributions: \(}^{*}_{t}=(}^{*})\).

Following , we optimize the SG decoder by minimizing the variational lower bound \(_{vlb}\) (Eq. 1). Also the parameterization trick  is leveraged to encourage the system to predict the noiseless node distribution \(p_{}(_{0}|s_{t},y)\) at each reverse step, which is taken as an auxiliary learning objective to be incorporated with \(_{vlb}\):

\[_{SGH}&=_{vlb}+ _{1}\,logp_{}(s_{0}|s_{t},y)\,,\\ _{vlb}&=_{0}+_{1}+ +_{T-1}+_{T}\,,\\ _{0}&=-logp_{}(s_{0}|s_{1},y)\,,\\ _{t-1}&=D_{KL}((q(s_{t-1}|s_{t},s_{0}))||(p _{}(s_{t-1}|s_{t},y))\,,\\ _{T}^{o}&=D_{KL}(q_{(S}T|s_{0})||p_{}(s_{T}))\,, \] (6)

where \(_{1}\) is a hyper-parameter for controlling the learning components.

### Scene-driven Image Synthesis (SIS)

With the enriched SG from SGH in each denoising step at hand, the backbone T2I diffusion carries out the image synthesis with the guidance of that SG. We design a hierarchical scene integration (HSI) strategy to ensure the highly effective integration of SG features. Specifically, we consider the fusion at four different hierarchical levels, i.e., object (\(L^{o}\)), relation (\(L^{r}\)), region (\(L^{c}\)), and global levels (\(L^{g}\)) with each focusing on different context scopes, as illustrated in Figure 3. We maintain the representations of these three levels as the keys \(_{L^{i}}\) & values \(_{L^{i}}\) via CLIP encoder , which are then integrated together via the Transformer attention of U-Net in LDM:

\[}=_{i\{o,r,c,g\}}(,_{L^{i}}, _{L^{i}})=_{i\{o,r,c,g\}}(_{L^{i}}^{ }}{})_{L^{i}}\,,\] (7)

where \(\) is the visual query vectors from the ResNet block in LDM. The above hierarchical scene integration is carried out for both the downsampling and upsampling processes in U-Net. By denoising \(T\) steps, the system finally produces the desired image. Appendix SSA.2 gives more details of this part.

### Warm-start Training

To ensure stable learning of the overall system, we take a warm-start training strategy. Firstly, the SGH is separately updated via \(_{SGH}\) (Eq. 6) based on the abstract-to-intricate SG pair annotations,

Figure 3: Hierarchical scene integration (HSI) fuses the SG features under multiple levels: 1) objects (with attributes), 2) relational triplets (i.e., _subject-predicate-object_), 3) regional neighbors, and 4) the whole SG.

until it has converged. Then, both the SIS and SGH modules are optimized jointly by minimizing:

\[=_{2}_{SGH}+_{SIS}\,,_{SIS}=_{(_{0}), (0,),t}\|-_{}(_{t},G_{t},y,t)\|_{ 2}^{2}\,.\] (8)

Here we follow  to optimize SIS with a simple surrogate objective that calculates the mean-squared error loss, and \(G_{t}\) is the intermediate SG by SGH at timestep \(t\), which can be derived from the \(s_{t}^{*},*\{o,a,r\}\) (cf. Figure 2). \(\) is the noise in SIS, and \(_{}()\) denotes the U-Net (cf. Figure 3).

### Inference with Scene Sampling

During inference, we further aim to endow the SGH with diversified SG enrichment, and thus lead to T2I diversification. Intuitively, given an abstract prompt, there is often more than one possibility of the potential scenes to imagine. Also, it can be observed that in the denoising process, the diffusion model has a larger potential of divergence only at its earlier stage, while the generation tends to be more stable and certain when the iteration grows. Correspondingly, we expect the scene sampling to start in the early denoising steps, and gradually be more determining. Thus, we design a scene-sampling mechanism. First, instead of picking the best prediction of the category of any node \(}_{t,j}^{*}\), we take the top-\(A\) category candidates with corresponding probability distribution \(\) based on the category distribution \(}_{t,j}^{*}^{K^{*}}\) of node \(s_{t,j}^{*}\). Then, we perform sampling over these candidates with a dynamic probability:

\[_{t,j}^{*}=e^{- t}+(1-e^{- t})\,,\] (9)

where \(*\{o,a,r\}\), and \(\) is a temperature. It is an annealing process, i.e., when \(t\)=\(T\) (starting denoising) more random sampling is preferable, while \(t\) approaches 0 (denoising ends), SGH tends to be more decisive. Figure 4 exemplifies the mechanism with the _object_ type of nodes (\(}_{t,n}^{o}\)).

## 4 Experiments

### Settings

Data and ResourceWe conduct T2I generation experiments mainly on the COCO  dataset. We also prepare the abstract-to-intricate SG pair annotations for training the SGH module, where we employ an external textual SG parser  and a visual SG parser  on the paired images and texts in COCO, to obtain the initial SG and imagined SG, respectively. To enlarge the abstract-to-intricate SG pairs, we further extend Visual Genome (VG) . Besides, to simulate the abstract-to-intricate T2I scenario, we further manually extract a subset of text-image pairs from raw COCO data (named COCO-A2I), in which the texts are short and abstract,4 while the images are comparatively complex and intricate. Appendix SSB.1 shows all the dataset details.

Baseline and EvaluationWe make comparisons with three types of existing strong-performing T2I models. 1) **GAN-based models**: AttnGAN , ObjGAN , DFGAM , OPGAN , 2) **Auto-aggressive model**: DALL-E, and CogView . 3) **Diffusion-based models**: LDM , VQ-diffusion , LDM-G and Frido  with classifier-free guidance. Note that LDM-G and Frido are the current SoTA T2I synthesizers. In addition, two types of **text-based enrichment approaches** are included as baselines: stable-diffusion prompt generator (SD-PG) and SPY inspired by . The enriched text prompts are then utilized as inputs for Frido to generate the final images. We adopt three standard metrics to measure image synthesis performance: 1) **Inception score (IS)**, 2) **Frechet Inception Distance (FID)** and 3) **CLIP score**. Moreover, we use **GLIP** to measure the fine-grained '_object-attribute_' grounding in images, and **Triplet Recall (TriRec.)** measure the '_subject-predicate-object_' triplet recall between two SGs. We also adopt the **Learned Perceptual Image Patch Similarity (LPIPS)** for diversifying generation evaluation. Detailed definitions of evaluation metrics are shown in Appendix SSB.2.

Figure 4: Illustration of the scene sampling mechanism.

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

**P Q2:** _Does SGH indeed induce intricate and reasonable SGs?_ Firstly, in Figure 7 we explore whether the SGH can produce new SG structures during the T2I process, on the COCO and COCO-A2I datasets separately. As seen, after scene enrichment, the average numbers of all three element types (object, attribute, and relation) substantially increase. Notably, the addition is more evident on the COCO-A2I set, where scene enrichment is more needed.

Next, we examine if the newly imagined SG structures provide reasonable scenes to the input prompt. We reach this by measuring the recall rate (TriRec.) of the '_subject-predicate-object_' pairs between two SGs, i.e., the predicted and the gold SGs. Assuming the SGs (denoted as \(G^{}\)) of gold images entail reasonable scenes, we consider two types of SGs from our 'prediction': 1) SGs induced by SGH (\(\)), and 2) SGs parsed from our synthesized images (\(G^{}\)). We also compare with the text-enriched SPY method. As shown in Table 4, we first observe that the induced SGs highly align with the gold SGs, with 82.01 TriRec. score, indicating the induced SGs are sensible for generating high-quality images. Moreover, by comparing the TriRec. scores (\(G^{}\)_vs._\(G^{}\)) between SPY and Salad (i.e., 78.61 vs. 86.04), we learn that the synthesized images coordinated by imagined SGs more correspond to the gold images in terms of semantic scene structure. This also suggests that the SGH can induce valid SGs which favorably guide the image generation process.

**P Q3:** **Does sampling strategy helps diversified T2I?** To measure the effectiveness of the sampling strategy in diversifying image generation given the same prompt, we consider both the qualitative analyses and human evaluation, where the former calculates the LPIPS score  to assess the perceptual similarity between two images in deep feature space. As shown in Figure 8, our Salad model significantly outperforms LDM and Frido in terms of diversity score and human evaluation, and things go worse upon removing the scene sampling strategy. This demonstrates that our proposed scene sampling mechanism is effective in helping diversified T2I. In Appendix SSC.2 we show more examples of diversified generations via the scene sampling mechanism.

## 5 Related Work

T2I is a long-standing topic in computer vision and multimodal communities. Arrays of explorations have been devoted to achieving stronger image synthesis performances with various deep generative models, such as GANs , VAEs , flow-based approaches  and ARMs . More recently, the diffusion denoising probabilistic methods (DDPMs) have revealed the greatest potentials on image synthesis, in which the optimal density estimation is more naturally achieved with a fixed diffusion process to transform an image into a Gaussian noise . This work follows the line of diffusion methods and takes the SoTA latent diffusion model (LDM)  as the T2I backbone.

Generating high-quality images with complex scenes is the key criterion of a sound T2I system . Many efforts have been paid for synthesizing more realistic and nature-looking images in sophisticated scenes, yet most of which are conditioned on taking the detailed descriptions as inputs . Thus, how to generate high-quality images in intricate scenes from succinct and abstract prompts becomes a meaningful yet challenging task. In this work, we introduce a scene hallucination mechanism, which, built upon the SG structure, performs more accurate and controllable scene completion and eventually helps generate intricate images of higher quality. We consider the SG representations  for the input prompt texts as well as the guidance of image synthesis. SG advances in intrinsically describing the semantic structures of scenes for texts or images , enabling more fine-grained control of complex scenes , and thus aiding the final image generation.

   & \(\)_vs._\(G^{}\) & \(G^{}\)_vs._\(G^{}\) \\  SPY & - & 78.61 \\ Salad & 82.01 & 86.04 \\  

Table 4: Comparing the gold SGs (\(G^{I}\)) with our induced SG (\(\)), and the SG (\(G^{}\)) of generated image with TriRec. metric.

Figure 8: Comparison of diversity using diversity score (LPIPS) and human evaluation.

Figure 7: The average number of three types of SG nodes.

Scene graph (SG) is a type of structured data that represents multiple objects and their complex relationships in the vision or language scenes, wherein the nodes denote objects & attributes and the edges depict relationships between objects . As intrinsically describing the semantic structures of scene semantics for the given texts or images, SG has been widely utilized as a type of external feature being integrated into downstream applications for enhancements, e.g., image retrieval [28; 56], image generation [28; 53] and image captioning [54; 36; 55]. In this work, we consider the SG representations for the input sentences. Compared to the linear sequential nature of the text, SG offers a more intuitive manner to represent the scene semantics in a structured format, enabling more fine-grained control of complex scenes , and thus aiding the final image synthesis.

Our SGH mechanism also relates to the research of SG enrichment or imagination. While existing methods mostly approach the task by incrementally parsing SG elements [16; 57; 58], such greedy-increment paradigm may largely suffer from trapping in locally optimal SG generation, thus leading to inferior SG imagination. Instead, in this paper, we implement SGH as a discrete denoising&diffusion process. Discrete diffusion technique  is the latest introduced method that replaces the continuous state in standard diffusion models with a discrete one. During each denoising step, the entire SG structure is updated and optimized from a global viewpoint, so as to yield a more reasonable enrichment of scene structure. Besides, both the T2I and the SGH are modeled as the same diffusion process in our framework, where the two processes are well synchronized, such that the underlying SG features can perfectly guide the T2I synthesis at each step. To our knowledge, we are the first to investigate SG induction using discrete diffusion models.

## 6 Conclusion

In this work, we explore the text-to-image synthesis task under the abstract-to-intricate setup. Drawing inspiration from human intuition, we propose a scene-graph hallucination mechanism, which carries out scene imagination based on the initial scene graph of the input prompt, expanding the starting SG with more possible specific scene structures. We then develop an SG-based hallucination diffusion system for the abstract-to-intricate T2I, which mainly includes an SG-guided T2I module and an SGH module. Specifically, we design the SGH module based on the discrete diffusion technique, which evolves the initial SG structure by iteratively adding new scene elements. Then, we utilize another continuous diffusion model as the T2I synthesizer, where the overt image-generating process is navigated by the underlying semantic scene structure induced by the SGH module. On the standard COCO dataset, our system shows great superiority in the abstract-to-intricate T2I generation. Further analyses demonstrate that our SG-based hallucination mechanism is able to generate logically sound SG structures, which in return helps produce high-quality scene-riched images.

## 7 Broader Impact

BenefitsThe current text-conditioned image generation approaches largely fail to the abstract-to-intricate T2I due to a lack of necessary details of input prompts. In this work, we propose a novel scene-graph hallucination mechanism inspired by human imagination intuition, which expands upon the initial scene graph from the text prompts to generate more feasible and specific scene structures. Furthermore, the enriched timestep-wised SG is leveraged to navigate the T2I generation process, leading to synthesizing more intricate images. Our study demonstrates that hallucinating images based on scene graph structures offer scalability, and modeling these structures enhances the generation of high-quality images with improved semantic controllability.

Potential weaknessThere can be two potential weaknesses that warrant consideration in our system. Firstly, the effectiveness of our system relies heavily on the quality of scene graph hallucination (SGH), yet the absence of a dedicated dataset for the SGH task poses a challenge in training the SGH module. However, we can leverage the richly annotated Visual Genome (VG) dataset, commonly used for training visual SG parsers, to provide initial training for the SGH module under an unconditional setting. Secondly, the training process of a diffusion model for text-to-image (T2I) generation entails substantial computational resources, resulting in increased energy consumption, CO\({}_{2}\) emissions, and potential environmental pollution.