# A Simulation Benchmark for Autonomous Racing

with Large-Scale Human Data

 Adrian Remonda\({}^{1,2,4}\) Nicklas Hansen\({}^{1}\) Ayoub Raji\({}^{3}\) Nicola Musiu\({}^{3}\)

**Marko Bertogna\({}^{3}\) Eduardo E. Veas\({}^{2,4}\) Xiaolong Wang\({}^{1}\)**

\({}^{1}\)UC San Diego \({}^{2}\)TU-Graz \({}^{3}\)Unimore \({}^{4}\)Know-Center GmbH

Work done during internship at UC San Diego. Correspondence can be directed to Adrian Remonda <aremonda@student.tugraz.at>.

###### Abstract

Despite the availability of international prize-money competitions, scaled vehicles, and simulation environments, research on autonomous racing and the control of sports cars operating close to the limit of handling has been limited by the high costs of vehicle acquisition and management, as well as the limited physics accuracy of open-source simulators. In this paper, we propose a racing simulation platform based on the simulator Assetto Corsa to test, validate, and benchmark autonomous driving algorithms, including reinforcement learning (RL) and classical Model Predictive Control (MPC), in realistic and challenging scenarios. Our contributions include the development of this simulation platform, several state-of-the-art algorithms tailored to the racing environment, and a comprehensive dataset collected from human drivers. Additionally, we evaluate algorithms in the offline RL setting. All the necessary code (including environment and benchmarks), working examples, datasets, and videos are publicly released and can be found at: https://assetto-corsa-gym.github.io.

## 1 Introduction

Autonomous driving has become an industry with different levels of applications affecting our daily lives, and it still has a large potential to continue revolutionizing future mobility and transportation. This paper explores a slightly different setting from day-to-day driving and focuses on driving an autonomous car at its physical limits, _i.e._, autonomous racing. Specifically, the goal is to maneuver a car around a race track to achieve the lowest possible lap time and develop highly robust and generalizable models .

However, developing and testing algorithms for autonomous racing is a challenging and expensive task. Traditional testing methods, such as on-track testing, are limited in scope and can pose

Figure 1: **Overview.** We propose a high-fidelity racing simulation platform based on Assetto Corsa that enables reproducible algorithm benchmarking, as well as data collection with human drivers.

safety risks. In this context, simulations offer many advantages, such as the ability to replicate complex real-world scenarios, adjust environmental parameters, and collect large amounts of data for analysis. Crucially, it has been proven experience obtained from driving simulations can often be transferred to the real car. In fact, almost all Formula Racing teams have their own simulators for designing strategy and training racing drivers. If we can open source a platform for training autonomous agents, it will benefit both the research community and the racing industry. Currently, many simulators are available for autonomous driving research, such as Carla (Dosovitskiy et al., 2017) and F1Tenth (Babu and Behl, 2020), but none are specifically designed for high-speed racing. Wurman et al. (2022) have used Sony PlayStations for simulation in the Gran Turismo racing game, but the platform is presently not available to the community.

We present a novel, versatile, and realistic training and testing environment based on the high-fidelity racing simulator Assetto Corsa (visualized in Figure 2), which is widely used by professional drivers for practice. Our environment complies with the Gym interface and ROS2. Our framework leverages the plug-in interface provided by Assetto Corsa to obtain the real-time state of the vehicle and set controls. Assetto Corsa can be easily obtained and set up, making our platform accessible to a wider audience. This accessibility facilitates broader experimentation and development within the autonomous racing community.

Our framework supports the integration of Reinforcement Learning (RL) and control algorithms and features local and distributed execution capabilities. It can also simulate different weather conditions, opponent, tire wear, and fuel consumption scenarios. Additionally, our setup allows for recording human driving data, which is a key aspect of our research.

We include several state-of-the-art RL algorithms as well as classical control MPCs to benchmark autonomous racing in our platform. We present a comprehensive dataset that includes various cars and tracks. The dataset encompasses 64 million steps recorded while training one of our benchmark algorithms, Soft Actor-Critic (Haarnoja et al., 2018), as well as more than 900 laps from human drivers with different levels of expertise recorded in our simulator shown in Figure 1, ranging from a professional driver to beginners. This provides robust baselines for comparison. Finally, we demonstrate the usefulness of our dataset by providing insights and statistics on the collected data, along with empirical evidence of its utility in the RL setting, highlighting the value of human demonstrations in the field of self-driving racing. We will open-source code for the simulation environment, proposed evaluation platform, dataset, and baselines.

**Our key findings include:**_(i)_ Human demonstrations provide a robust baseline for evaluating various model types; _(ii)_ Better models can be achieved by using human demonstrations, as evidenced by improved lap times and sample efficiency; _(iii)_ Utilizing demonstrations from different tracks enables rapid generalization to new tracks with fewer safety hazards, marking a significant advancement towards real-life racing deployment; and _(iv)_ By bootstrapping with human demonstrations, we show that it is possible to drive without a reference path.

## 2 Related work

**Simulators for autonomous racing.** While there are multiple open-source simulators for autonomous driving (Kaur et al., 2021), among which CARLA (Dosovitskiy et al., 2017) is the most

Figure 2: **Assetto Corsa. A GT3 (_top_) and a F317 (_bottom_) car each turning a corner in the Assetto Corsa simulator. The simulator features a total of 178 official cars and 19 laser-scanned tracks, in addition to custom content created by the community. We develop a platform for interfacing with the simulator that can be used with both RL and MPC methods, as well as human drivers.**

complete and popular, there are limited platforms available for the racing domain (Babu and Behl, 2020; Balaji et al., 2020; Herman et al., 2021; Weiss and Behl, 2020; Wurman et al., 2022). For example, environments for small-scale race cars have been proposed by Babu and Behl (2020) (F1Tenth) and Balaji et al. (2020) (DeepRacer), and Herman et al. (2021) (Learn-to-Race) proposes a simulator, framework, and dataset specifically tailored for the Roborce car and on only three tracks. The official F1 2017 racing game has been used by Weiss and Behl (2020) to build an end-to-end framework. Despite the availability of different tracks, the game lacks physics accuracy and realism, and the vehicles are limited to open-wheel F1 race cars. Professional Autonomous Racing teams build their simulation platforms upon the Unity engine and accurate multi-body models through Matlab/Simulink or dedicated Motorsport libraries (Betz et al., 2023; Raji et al., 2024). The cost and complexity of modelling makes the knowledge of vehicle dynamics a strict requirement which limits the ease of access and use to the majority of the research community. Most similar to ours, Wurman et al. (2022) learns to drive in the Gran Turismo racing game using Soft Actor-Critic (SAC), and shows that a learned RL policy can outperform human players. However, the platform and methods used are not publicly available, making it difficult to build upon their research.

**Scientific research with Assetto Corsa (AC).** AC is an excellent platform to build upon for an autonomous racing environment, as it overcomes many of the weaknesses and gaps of aforementioned simulators. The game features 178 official cars and 19 laser-scanned tracks, in addition to custom content available online (Overtake.gg, Assetto Corsa Club). AC can simulate different values of grip, weather conditions, and racing scenarios, such as single-vehicle performance laps, as well as online and offline multi-vehicle races. We leverage the plug-in interface of AC to control a vehicle in single-vehicle sessions and collect datasets. Our proposed platform offers similar capabilities to those proposed by Wurman et al. (2022), including support for distributed workers but without the need for PlayStation consoles for simulation. Previous literature has leveraged AC for various research problems, including road geometric research (de Frutos and Castro, 2021), deep learning for self-driving (Hilleli and El-Yaniv, 2016; Mahdavian and Martinez, 2018; Mentasti et al., 2020), racing game commentary (Ishigaki et al., 2021), and identification of driving styles (Vogel et al., 2022). However, none of these works released interfaces nor code to the public.

**Algorithms for autonomous racing.** Typically, autonomous racing methods rely on the same conventional paradigm of perception, planning, and control (Betz et al., 2022) as in autonomous driving literature. Common approaches applied to real race cars adopt graph-based methods or smooth polynomial lane change for what concerns the online local planning (Raji et al., 2022; Stahl et al., 2019; Ticozzi et al., 2023). The control often relies on classical optimization-based controllers such as Linear Quadratic Regulator (LQR) (Spisak et al., 2022) and Model Predictive Control (MPC) (Raji et al., 2023; Wischnewski et al., 2023). Although not easily deployed on real cars, Reinforcement Learning (RL) has recently been used to learn highly performant control policies in simulation, rivaling professional human drivers (Remonda et al., 2022; Wurman et al., 2022). Therefore, the availability of a complete autonomous racing simulation platform is crucial to make these methods generalizable and safe to use with real race cars. Our work presents such a platform, including a benchmark for RL methods, and a large-scale dataset of human drivers as well as RL replay data.

## 3 Preliminaries

**Problem definition.** We formulate autonomous racing as a Partially Observable Markov Decision Process (POMDP) (Bellman, 1957; Kaelbling et al., 1998)\(,,,r,\), where \(\) denotes the state space, \(\) is the action space, \(:\) is the (unknown) transition model, \(r:\) is a reward function, and \([0,1)\) is the discount factor. Because the state \(\) of the simulator cannot be observed directly, we approximate states as a sequence of the last \(k\) observations (elementry information) received from the environment. A deep Reinforcement Learning (RL) policy \(\) is trained to select actions \(_{t}(|_{t})\) at each time step \(t\) such that the expected sum of discounted rewards \(_{}[_{t=0}^{}^{t}r(_{t},_{t})]\) is maximized; the reward function thus serves as a proxy for lap time.

**Soft Actor-Critic (SAC).** Common model-free off-policy RL algorithms aim to estimate a state-action value function (critic) \(Q\) by means of the single-step Bellman residual (Sutton, 1998)\(=Q_{}(_{t},_{t})-(r(_{t}, _{t})+_{_{t+1}}Q_{}^{}( _{t+1},_{t}^{}))\). When the action space \(\) is continuous, the second term - known as the _temporal difference_ (TD) target - becomes intractable due to the \(\) operator. To circumvent this, SAC (Haarnoja et al., 2018) additionally optimizes a stochastic policy (actor) \(\) to maximize the value function via gradient ascent, regularized by a maximum entropy term: \(_{}()=_{}[Q(,)+((|)],( |)\), where \(\) is entropy and \(\) is a temperature parameter balancing the two terms. While the single-step residual is the most commonly used objective for critic learning, we find it necessary for SAC to use multi-step (\(n=3\)) residuals (Vecerik et al., 2017) during learning in the context of autonomous racing.

**TD-MPC2.** Model-based RL algorithm TD-MPC2 (Hansen et al., 2022, 2024) learns a latent decoder-free world model from sequential interaction data, and selects actions during inference by planning with the learned model. TD-MPC2 optimizes all components of the world model in an end-to-end manner using a combination of TD-learning (the single-step Bellman residual), reward prediction, and joint-embedding prediction (Grill et al., 2020). During inference, TD-MPC2 follows the receding-horizon Model Predictive Control (MPC) framework and plans trajectories using a derivative-free (sampling-based) optimizer (Williams et al., 2015). To accelerate planning, TD-MPC2 learns a model-free policy prior that is used to warm-start trajectory optimization.

## 4 A Simulation Benchmark for Autonomous Racing

We propose a racing simulation platform based on the Assetto Corsa simulator to test, validate, and benchmark algorithms for autonomous racing - including both RL and classical control - in realistic and challenging driving scenarios. In this section, we provide a technical overview of the design and features of our proposed platform, while deeper discussion of experiments is deferred to Section 5.

### Platform Design

Our proposed platform, depicted in Figure 1, provides a simple and intuitive environment interface between autonomous racing algorithms (RL and MPC), human drivers, and high-fidelity racing simulation for which we leverage Assetto Corsa. Figure 3 provides an overview of this interface. At the center of the framework is the simulator, which should offer: _(i)_ controls to setup and initiate the simulation, _(ii)_ static information about track and vehicle (track borders, vehicle setup and characteristics), _(iii)_ state of the simulation (_telemetry_ data about dynamic parameters of the vehicle: rpm, speed, lateral and longitudinal accelerations, position in 3D, to name a few), and _(iv)_ vehicle controls (minimally: steering, throttle, brake, gear shifts). The _Sim Control Interface_ builds on the plug-in interface from Assetto Corsa (AC). This interface allows external applications to access telemetry data through a callback synchronized with the game's physics engine. AC with its physics engine operates at 300 Hz on a Windows-only platform. It runs only in real-time, which is a limitation for algorithms that can train faster than real-time, but also a strength for testing algorithms in real-world conditions. This architecture allows multiple instances of Assetto Corsa to run on different machines with a central node for data collection.

Figure 3: **Our proposed platform for autonomous racing. We provide interfaces (_gray_) that _(1)_ connect a simulator (Assetto Corsa) to autonomous racing methods, and _(2)_ allow for human data collection. Interfaces receive track information and state, and execute actions in the simulator. Datasets (_purple_) are collected using an ACTI (Assetto Corsa Telemetry Interface) tool.**

The Sim Control Interface relays information over Ethernet to the controller (Gym or ROS) in real time and it will deliver static information over TCP at a lower rate. Critically, we included in the plugin a feature that recovers the vehicle back to the track, essential for developing reinforcement learning algorithms. Status information and actions are streamed over UDP. The frequency of state updates can be set according to the frames-per-second of the game. For our experiments we used 25Hz, though the interface was tested up to 100 Hz. Actions in the simulation are applied using vJoy, a virtual joystick device recognized by the system as a standard joystick.

The controller can operate on either Linux or Windows, supporting both single and distributed systems. It connects to the Sim Control Interface, receiving simulation updates and providing an environment API as described in Section 4.2. When running a single system with the simulation, the controller applies actions directly using the vJoy instead of streaming them to the Sim Control Interface, to reduce latency. Additionally, the controller interface has all the information needed (settings, states, actions, rewards) to record demonstration datasets. When human drivers control the simulation, they directly operate on the simulator and the left-hand side of the diagram is not active. In this case, data is recorded using the Assetto Corsa Telemetry Interface  which records data in MoTeC format, a standard format used in motorsports.

### Environment Details

We provide a simple to use gym-compliant environment API for interfacing with RL algorithms. We detail observations, actions, reward function, and termination conditions in the following.

**Observations.** The environment is partially observable and agents asynchronously receive states \(^{125}\) with the most recent telemetry information provided by Assetto Corsa. To account for partial observability, states include telemetry data from the last _three_ time steps, as well as absolute control values of the past two steps. Similar to previous work , telemetry data includes: linear + angular velocities and accelerations of the car, a vector of range finder sensors (distance between track edge and car, if near), look-ahead curvature, force feedback from the steering wheel (human drivers rely on this to sense tire grip), angle between wheels and direction of movement, as well as 2D distance between the car and a reference path provided by AC. Notably, all cars are provided with the same reference path although the optimal path is vehicle-dependent; we can thus expect algorithms to benefit from deviating from this reference path to some extent.

**Actions.** The action space \(^{3}\) is continuous and includes scalar controls for throttle, brake, and steering wheel, all normalized to \([-1,1]\) for easy integration with RL algorithms. These values are translated to the maximum values allowed by the simulation. Crucially, we do not apply absolute controls but rather deltas to the current controls, which we find to greatly reduce oscillation. We use an automated gearbox for RL algorithms.

**Reward.** We opt for a reward function that is proportional to current forward velocity \(v\), with a small penalty for deviations from the track axis: \(r v(1- d)\) where \(d\) is \(_{2}\)-distance to the reference path, and \(\) is a constant coefficient that balances the weight of additional racing line supervision. Intuitively, increasing \(\) allows the agent to deviate more from the reference path.

**Episode termination.** An episode terminates early if three or more wheels are outside of track boundaries, or when speed drops below 5 km/h for more than 2 seconds. Empirically, we find this to improve data efficiency significantly for RL algorithms that learn from scratch (_i.e._, without access to human driving data). Humans are allowed to keep driving when termination conditions are met but the lap is invalidated.

### Benchmark & Dataset

We consider a set of four different tracks and three cars for data collection and experimentation, which are all visualized in Figure 4. The tracks that we consider require diverse maneuvers: **Indi-anapolis (IND)**, an easy oval track; **Barcelona (BRN)**, with 14 distinct corners; **Austria (RBR)**, a balanced track with technical turns and high-speed straights; and **Monza (MNZ)**, the most challenging track with high-speed sections and complex chicanes. The cars that we consider all differ in setup, aerodynamics, and engine power (general vehicle dynamics): **Mazda Miata NA (Miata)**, top speed of 197 km/h; **Dallara F317 (F317)**, top speed of 250 km/h; and **BMW Z4 GT3 (GT3)**,top speed of 280 km/h. Our high diversity in tracks and cars ensures a more nuanced understanding of benchmarked algorithms.

Simulators were set up at University of California San Diego and Graz University of Technology, where a total of 15 human drivers were tasked with driving as fast as possible for at least 5 laps per track and car. Various categories of drivers participated: a professional e-sports driver, four experts who regularly train and compete online, five casual drivers with some experience, and five beginners using a racing simulator for the first time. Besides human driving data, we also collect a large dataset from the replay buffers of SAC policies trained from scratch. Table 1 summarizes our dataset.

## 5 Experiments

The objective of our paper is to inspire further advancements in the field of autonomous racing. In this section, we validate the proposed environment and the dataset, and compare the performance of various algorithms against human drivers. Our experiments address key questions: the comparative performance of humans, RL algorithms, and classical MPCs; the benefits of incorporating human laps for training models; the impact of high-quality training data; the generalization of learning across different tracks; and the ability of agents to drive without predefined reference lines. All the necessary code (including environment and benchmarks) and working examples can be found at: https://github.com/dasGringuen/assetto_corsa_gym.

### Methods

We consider 4 distinct methods for autonomous racing - built-in AI, MPC, SAC, and TD-MPC2 - as well as algorithmic variations of them. We summarize the methods as follows:

\(\)**Built-in AI** controller provided by Assetto Corsa. We use it as-is.

\(\)**Model Predictive Control** (MPC) , an optimization-based controller that utilizes a single-track vehicle model. The approach requires substantial domain knowledge and engineering efforts for each track and car. This is the only method presented that has been directly deployed to a real race car.

\(\)**Soft Actor-Critic** (SAC) , a model-free RL algorithm. We train SAC via online interaction by default, _i.e._, initialized from scratch without any prior data. Our SAC implementation uses \(n\)-step returns (\(n=3\)) which we found to be critical for learning.

\(\)**TD-MPC2**, a model-based RL algorithm. A key strength of TD-MPC2 is its ability to consume various data sources: human driving data, existing data collected via RL, as well as its own online interaction data, either from a single race track or across multiple tracks.

  
**Car** & **Track** & **S**ints & **Laps** & **Steps** & **Steps** \\   & &  &  \\  F317 & BRN & 70 & 247 & 612,557 & 10M \\ F317 & MNZ & 19 & 117 & 288,582 & 10M \\ F317 & RBR & 24 & 142 & 295,679 & 10M \\ F317 & IND & 1 & 4 & 4,605 & 4M \\  GT3 & BRN & 37 & 181 & 501,206 & 10M \\ GT3 & RBR & 15 & 102 & 218,722 & 10M \\ GT3 & MNZ & 13 & 85 & 221,123 & 10M \\  Miata & BRN & 5 & 27 & 99,145 & 10M \\ Miata & MNZ & 2 & 10 & 38,395 & – \\ Miata & RBR & 3 & 12 & 32,971 & – \\ 
**Total** & & **189** & **927** & **2,312,985** & **64M** \\   

Table 1: **Dataset. We collect a total of 2.3M steps from human drivers of various skill levels, and 64M steps from SAC policies. A _stitm_ is a continuous period of driving without breaks.**

Figure 4: **Cars and tracks. We consider a total of 4 different tracks (_left_), as well as 3 distinct cars (_right_). We collect and open-source human driving data for all tracks and cars considered.**

[MISSING_PAGE_FAIL:7]

Q2: Does pre-training on data from multiple tracks enable few-shot transfer to unseen tracks?A key benefit of data-driven approaches (such as RL) is their potential for learning _generalizable_ racing policies that transfer across tasks and cars. In the following, we investigate whether pre-training TD-MPC2 on a set of (source) tracks and then finetuning on a target track (MNZ and AT in our experiments) improves data-efficiency and/or reduces number of crashes on the new track. Specifically, we first pre-train TD-MPC2 on human driving data from all tracks (excluding the two target tracks), as well as online interaction data on the BRN track. Next, we finetune this TD-MPC2 model on the two target tracks for 4 days each (separate finetuning procedures) and compare its lap time against TD-MPC2 and SAC with human driving data only from the target task. We find that it is helpful to differentiate tracks by including an additional track ID in observations during both pretraining and finetuning.

Results are shown in Figure 7 (_top_). TD-MPC2 exhibits faster convergence when pretrained on other tracks, and completes full laps on the target tracks after only a few episodes whereas SAC fD and TD-MPC2 fD require hundreds of episodes. The benefit of pretraining is also clearly reflected in the cumulative number of crashes during training (_bottom_); TD-MPC2 crashes far less frequently when pretrained, and TD-MPC2 fD similarly crashes less frequently than SAC fD. We omit pretraining and finetuning results using SAC as we find it to perform poorly in this setting.

Q3: Does better (faster) human demonstrations improve RL performance?We hypothesize that data from expert human drivers will be more beneficial to RL algorithms than that of less experienced drivers. To test this hypothesis, we split human driving data into two categories: experts (including the pro driver), and beginners (including intermediate drivers). Then, we train SAC on BRN with a F317 using data from each group; results are shown in Figure 8. Our results indicate that having high-quality demonstrations do indeed lead to faster convergence vs. provided with slower human lap data as well as when training from scratch. However, we find that RL algorithms still benefit from beginner demonstrations substantially early in training, but result in overall slower lap times at later stages of training. We conjecture that this is because the RL algorithm becomes too biased towards human driving behavior.

Q4: Can RL algorithms learn to race without a predefined reference path?Our previous experiments use a reward function that mildly penalizes deviation from a predefined reference path provided by Assetto Corsa. Providing this reference helps with exploration (especially early in training) but is not optimal. In general, calculating the optimal path is very challenging as it depends not only on track geometry but also on vehicle dynamics [Cardamone et al., 2010]. We now investigate whether RL algorithms can succeed without such a reference path; results are shown in Table 3.

Figure 8: Demonstration quality. Lap time (s) of SAC trained with human data of varying experience level on BRN/F317. Better human data improves convergence considerably.

  
**Method / Car** & **F317** & **GT3** \\  SAC & 97.67 & 109.51 \\ SAC, no ref & **Fail** & **Fail** \\ SAC fD, no ref & 99.80 & 110.98 \\   

Table 3: Lap times without reference path. SAC fails to complete a lap without a reference path, whereas SAC fD completes laps albeit slightly slower.

Figure 7: **Few-shot transfer.** Finetuned TD-MPC2 (_ft_) on target tracks vs SAC fD and TD-MPC2 fD. _(top)_ Lap time vs. episodes. Each episode is about 7 laps unless the car crashes. Due to training for a fixed number of steps, the plots show curves of different lengths. _(bottom)_ Cumulative number of times an agent did not finish.

Interestingly, SAC trained from scratch fails to complete a lap without the reference path, whereas it succeeds when provided with human demonstrations (albeit still slightly slower than when provided with a reference path). This strongly suggests that both reference paths and human driving data help overcome the challenges of exploration. We acknowledge that Wurman et al. (2022), too, has successfully trained an autonomous racing policy without a reference path. However, our setting is more general as their approach assumes access to the exact position of the car on a given track, whereas our setting only relies on range finder sensors and thus has potential to generalize across tracks.

## 6 Conclusion

Our results validate the quality of both the simulator and the collected dataset, as well as some of their potential use cases in future research. In particular, we demonstrate that data-driven methods benefit greatly from human demonstrations, and that existing algorithms can be used for few-shot transfer to unseen tracks. We provide source code for the environment, benchmark, and baseline algorithms, and we look forward to seeing what the learning, control, and autonomous racing communities will use it for.

**Limitations.** We acknowledge that AC runs only in real-time, which can be a limitation for algorithms capable of faster-than-real-time training. However, this real-time constraint is beneficial for testing algorithms in real-world conditions. This limitation can be overcome by running many workers in parallel to collect data. We do not experiment with image observations in this work, but acknowledge that it would be an interesting direction for future research. In particular, driving from raw image observations rather than, _e.g._, range finder sensors, would more closely resemble the sensory information that human drivers can access. Our RL policies are in some cases outperformed by experts, indicating that the maximum potential performance has not yet been achieved even on existing tracks and cars. One path for improvement is addressing the suboptimal autoshifting mechanism in AC. Allowing an RL agent to control the shifts is challenging due to the mix of continuous and discrete action spaces, but it holds potential for enhanced performance.

**Potential negative societal impact.** The advent of autonomous vehicles presents significant challenges, including the displacement of traditional labor roles and potential ethical dilemmas surrounding AI decision-making in high-risk scenarios. Additionally, public trust in autonomous technology could be jeopardized by accidents, complicating the regulatory landscape and raising complex liability issues.

## 7 Acknowledgments

We extend our gratitude to the current and former members of Unimore Racing who contributed to this platform, particularly Francesco Moretti, Andrea Serafini, and Francesco Gatti. Special thanks to Aleksandra Krajnc and Haichuan Che for their invaluable assistance with data collection. This project was supported, in part, by the Amazon Research Award, the Intel Rising Star Faculty Award, the NVIDIA Graduate Fellowship, gifts from Qualcomm, and Know-Center GmbH. Know-Center is funded within the Austrian COMET Program.