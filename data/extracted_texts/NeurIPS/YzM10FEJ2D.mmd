# Touchstone Benchmark: Are We on the Right Way for Evaluating AI Algorithms for Medical Segmentation?

Touchstone Benchmark: Are We on the Right Way for Evaluating AI Algorithms for Medical Segmentation?

 Pedro R. A. S. Bassi\({}^{1,2,3}\)

Wenxuan Li\({}^{1}\)

Yucheng Tang\({}^{4}\)

Fabian Isensee\({}^{5,6}\)

Zifu Wang\({}^{7}\)

Jieneng Chen\({}^{1}\)

Yu-Cheng Chou\({}^{1}\)

Saikat Roy\({}^{5,8}\)

Yannick Kirchhoff\({}^{5,8,9}\)

Maximilian Rokuss\({}^{5,8}\)

Ziyan Huang\({}^{10}\)

Jin Ye\({}^{11}\)

Junjun He\({}^{11}\)

Tassilo Wald\({}^{5,6}\)

Constantin Ulrich\({}^{5}\)

Michael Baumgartner\({}^{5,6}\)

Klaus H. Maier-Hein\({}^{5,12}\)

Paul Jaeger\({}^{6,13}\)

Yiwen Ye\({}^{14}\)

Yutong Xie\({}^{15}\)

Jianpeng Zhang\({}^{16}\)

Ziyang Chen\({}^{14}\)

Yong Xia\({}^{14}\)

Zhaohu Xing\({}^{17}\)

Lei Zhu\({}^{17,18}\)

Yousef Sadegheih\({}^{19}\)

Afshin Bozorgpour\({}^{19}\)

Pratibha Kumari\({}^{19}\)

Reza Azad\({}^{20}\)

Dorit Merhof\({}^{19,21}\)

Pengcheng Shi\({}^{22}\)

Ting Ma\({}^{22}\)

Yuxin Du\({}^{10,23}\)

Fan Bai\({}^{10,23,24}\)

Tiejun Huang\({}^{23,25}\)

Bo Zhao\({}^{10,23}\)

Haonan Wang\({}^{18}\)

Xiaomeng Li\({}^{18}\)

Hanxue Gu\({}^{26}\)

Haoyu Dong\({}^{26}\)

Jichen Yang\({}^{26}\)

Maciej A. Mazurowski\({}^{26}\)

Saumya Gupta\({}^{27}\)

Linshan Wu\({}^{18}\)

Jiaxin Zhuang\({}^{18}\)

Hao Chen\({}^{28}\)

Holger Roth\({}^{4}\)

Daguang Xu\({}^{4}\)

Matthew B. Blaschko\({}^{7}\)

Sergio Decherchi\({}^{29}\)

Andrea Cavalli\({}^{2,29,30}\)

Alan L. Yuille\({}^{1}\)

Zongwei Zhou\({}^{1}\)

\({}^{1}\)Department of Computer Science, Johns Hopkins University

\({}^{2}\)Department of Pharmacy and Biotechnology, University of Bologna

\({}^{3}\)Center for Biomolecular Nanotechnologies, Istituto Italiano di Tecnologia

\({}^{4}\)NVIDIA

\({}^{5}\)Division of Medical Image Computing, German Cancer Research Center (DKFZ)

\({}^{6}\)Helmholtz Imaging, German Cancer Research Center (DKFZ)

Full affiliations are given in Appendix F.

Code, Models & Data: https://github.com/MrGiovanni/Touchstone

Leaderboard: https://mrgiovanni.github.io/Leaderboard

###### Abstract

_How can we test AI performance?_ This question seems trivial, but it isn't. Standard benchmarks often have problems such as in-distribution and small-size test sets, oversimplified metrics, unfair comparisons, and short-term outcome pressure. As a consequence, good performance on standard benchmarks does not guarantee success in real-world scenarios. To address these problems, we present Touchstone, a large-scale collaborative segmentation benchmark of 9 types of abdominal organs. This benchmark is based on 5,195 training CT scans from 76 hospitals around the world and 5,903 testing CT scans from 11 additional hospitals. This diverse test set enhances the statistical significance of benchmark results and rigorously evaluates AI algorithms across out-of-distribution scenarios. We invited 14 inventors of 19 AI algorithms to train their algorithms, while our team, as a third party, independently evaluated these algorithms. In addition, we also evaluated pre-existing AI frameworks--which, differing from algorithms, are more flexible and can support different algorithms--including MONAI from NVIDIA, nnU-Net from DKFZ, and numerous other open-source frameworks. We are committed to expanding this benchmark to encourage more innovation of AI algorithms for the medical domain.

Introduction

The development of AI algorithms has led to enormous progress in medical segmentation, but few algorithms are reliable enough for clinical use . Most AI algorithms fall short of expert radiologists, who are much more reliable and consistent when dealing with medical images from multiple hospitals, varied in different scanners, clinical protocols, patient demographics, or disease prevalences . Therefore, the question remains: _How can we test medical AI in the diverse scenarios that are encountered by radiologists?_ Establishing a trustworthy AI benchmark is important but exceptionally challenging, and seldom achieved in the medical domain. Tougher tests, like out-of-distribution evaluation on large, varied datasets, are needed.

Standard benchmarks have underlying problems that cause confusion in algorithm comparisons and delay progress. _First_, _in-distribution test sets_. In the medical domain, CT scans in the test set often share sources, scanners, and populations with the training set. As a result, AI algorithms may perform well on the test set but generalize poorly to out-of-distribution (OOD) scenarios . For example, Xia et al.  found that AI algorithms trained on data from Johns Hopkins Hospital (Baltimore, USA) lose accuracy in pancreatic tumor detection when evaluated on CT scans from Heidelberg Medical School (Heidelberg, Germany). _Second_, _small-size test sets_. Annotating medical data is expensive and time-consuming, but training AI requires substantial annotated data . Therefore, most annotated data is used for training, leaving very little assigned for testing. Recent CT datasets such as TotalSegmentator , WORD , and MSD , offered fewer than 100 CT scans for testing. Even a single success or failure can skew results, reducing the statistical power and potentially misleading conclusions. _Third_, _over-simplified metrics_. Most standard benchmarks only compare average performance, failing to identify each AI algorithm's strengths and weaknesses in different scenarios. For instance, one algorithm might excel at segmenting small, circular structures (like the gall bladder) while another performs better on long, tubular ones (such as the aorta). Average performance across many classes can hide these nuances. _Fourth_, _unfair comparisons_. Almost every paper reports that the newly 'proposed AI' outperforms existing 'alternative AIs.' The improvement becomes more significant if alternative AIs are reproduced and evaluated on an unknown training/test split. There are biases in comparison due to asymmetric efforts made in optimizing the proposed and alternative AIs. Many independent studies have reported these comparison biases over the years  but remain unresolved. There is a need to have more widely adopted benchmarks (e.g., challenges) where all AI algorithms are trained by their inventors and evaluated by third parties. _Fifth_, _short-term outcome pressure_. Standard benchmarks are often in short-term and non-recurring, requiring a final solution within several months. For example, RSNA 2024 Abdominal Trauma Detection  only opened for three months for data access and AI development & evaluation. The short-term outcome pressure can discourage new classes of AI algorithms that need considerable time and computational resources for a thorough investigation, as their vanilla versions (e.g., Mamba  in early 2024 and Transformers  in early 2021) might not outperform all the alternatives judged. The benchmark must have long-term commitment and allowance.

To address this AI mismeasurement issue, we present the Touchstone benchmark, an effort towards the objective of creating a fair, large-scale, and widely-adopted medical AI benchmark. Its scale is large, featuring a training set of 5,195 publicly available CT scans from 76 hospitals and a test set of 5,903 CT scans from additional 11 hospitals. Test sets were unknown to the participants of the benchmark. All 11,098 scans are annotated per voxel for 9 anatomical structures. The training set annotations were created by collaboration between AI specialists and radiologists followed by manual revision , 5,160 out of 5,903 test scans are proprietary and manually annotated, and the remaining test datasets are publicly available, annotated by AI-radiologist collaboration. As of May 2024, 14 global teams from eight countries have contributed to our benchmark. These teams are known for inventing novel AI algorithms for medical segmentation. In summary, the Touchstone benchmark explores an evaluation philosophy defined by the following **five contributions**:

1. _Evaluating on out-of-distribution data:_ The JHH test set (Sec. 2.1) presents 5,160 CT scans from an hospital never seen during training, introducing a new scale of external validation for abdominal CT benchmarks. The test data distribution varies in contrast enhancement (pre, venous, arterial, post-phases), disease condition (30% containing abdominal tumors at varied stages), demographics (age, gender, race), image quality (e.g., slice thickness of 0.5-1.5 mm), and scanner types. We have collected metadata information for 72% of the test set (\(N\)=5,160) and reported AI performance in each sub-group.

2. _Providing a large test set:_ Our test set (\(N\)=5,903) is much larger than the test sets of all current public CT benchmarks combined. It can enhance the statistical significance of the benchmark results: a 1% average accuracy increment across 5,000 CT scans is more indicative of a genuine algorithmic improvement than a 1% variation across 50 CT scans.
3. _Analyzing pros/cons from multiple perspectives:_ We evaluated segmentation performance of 9 anatomical structures, comparing the average results and analyzing them by metadata groups. We also reported per-class algorithm rankings and visualized worst-case performance. Moreover, we assessed inference time and computational cost, key factors for the clinical deployment of AI algorithms.
4. _Inviting inventors to train their own algorithms:_ Each AI algorithm is configured by its own inventors, who know it best and have the most interest in its success. In our benchmark, each inventor trained their AI algorithm on 5,195 annotated CT scans in AbdomenAtlas , and we, as a third party, independently evaluated these algorithms on 5,903 CT scans that are unknown and inaccessible to the AI inventors. This setting protects the integrity of our results (i.e., precluding the use of test data for hyperparameter tuning).
5. _Evaluating new algorithms with long-term commitment:_ Our Touchstone benchmark not only invited established AI algorithms that are already published in major conferences/journals, but also invited newly developed algorithms appearing in recent pre-prints. We have a long-term commitment to this benchmark by organizing recurring challenges for at least five years, curating larger datasets, and improving label quality and task diversity. The first edition was featured as an invitation-only challenge at ISBI-2024.

**Related benchmarks/challenges & our innovations.** In a general sense, we define a _benchmark_ as an algorithmic comparison. Accordingly, the most common type of benchmark are the standard comparisons found in thousands of research papers [58; 90; 91; 12; 27; 26; 48; 79] where authors present new algorithms and compare baselines. As previously explained, this type of benchmark incurs the risk of unfairness, due to possible asymmetric efforts made in optimizing the proposed and alternative algorithms. However, open _challenges_ are a different type of benchmark, where developers train their own algorithms and submit them for third-party evaluation, mitigating the risk of unfair comparisons. For this reason, Table 1 contrasts our Touchstone benchmark to a non-exhaustive list of the most influential abdominal CT segmentation challenges. Notably, our training dataset is considerably larger and comes from more hospitals than any CT dataset ever used in a challenge. Furthermore, the only challenge training datasets on a scale similar to AbdomenAtlas 1.0 have partial labels and/or unlabeled portions [2; 53]. Our dataset is 17.3\(\) larger than the second-largest fully-annotated CT dataset  in Table 1. Boosting our results' statistical significance, our evaluation dataset is 8.6\(\) larger than any CT segmentation challenge test dataset. Moreover, Touchstone is the only benchmark in Table 1 to, simultaneously, explicitly analyze the performance of AI algorithms controlled by age, sex, race, and other metadata information. Lastly, this work is the starting point of a long-term benchmark, which we commit to maintain and improve over the years. Considering the importance of long-term commitment, we must acclaim KiTS, an abdominal segmentation challenge that had 3 editions since 2019 [31; 30; 28; 29] and FLARE, a challenge being consistently held yearly since 2021 [57; 53; 55; 56].

## 2 Touchstone Benchmark

### Datasets - Annotations, Statistics, Distribution, & Characteristics

We used one training dataset and two test datasets to perform a comprehensive out-of-distribution benchmark. The training and test datasets were collected from many hospitals worldwide. Figure 1 shows the demographics of the two test datasets, JHH and TotalSegmentator; Appendix Figures 3-4 provide examples of CT scans and per-voxel annotations for various demographic groups across all datasets. The JHH dataset is proprietary and used for third-party evaluation; participants do not have access to the CT scans or their annotations. TotalSegmentator is a publicly available dataset; we did not inform the inventors beforehand of its use in our evaluation and confirmed that their AI algorithms had not been trained on this dataset. We included this public dataset to enable future participants to easily compare their algorithms with our benchmark.

**AbdomenAtlas 1.0--**_N=5,195; publicly available for training purposes--_is the largest multi-organ fully-annotated CT dataset to date, encompassing 76 hospitals in 8 countries . It leveraged a human-in-the-loop active learning strategy to empower radiologists to feasibly annotate 5,195 CT scans from 16 public datasets (listed in Appendix Table 4) and is fully annotated for 9 anatomical structures, i.e., spleen, liver, L&R kidneys, stomach, gallbladder, pancreas, aorta, and postcava. AbdomenAtlas 1.0, under CC BY-NC 4.0 License, is derived from publicly available datasets, so detailed metadata information is unfortunately not available.

**JHH--\(N\)=5,160; reserved for out-of-distribution test purposes1--**provides contrast-enhanced CT scans in venous and arterial phases. Collected from Johns Hopkins Hospital using two Siemens scanners, this dataset includes metadata on age, race, gender, and diagnosis. Notably, all per-voxel annotations in JHH were manually created by radiologists . Annotation time for a single

    &  &  &  &  \\  &  &  &  &  \\  &  & &  &  &  \\   & \# CT scans & \# hospitals & \# countries & \# CT scans & AI consistency & targeted \\  & train & train & train & test & analysis & invitation \\  MSD-CT  & 947\({}^{}\) & 1 & 1 & 465 IID & none & no \\ FLARE:22  & 2,050\({}^{}\) & 22 & 5+ & 200 IID, 600 OOD & sex, age & no \\ FLARE’23  & 4,000\({}^{}\) & 30 & n/a & n/a & n/a & no \\ KITS21  & 300 & 50\({}^{}\) & 1 & 100 OOD & sex, race & no \\ AMOS2-CT  & 200 & 3 & 1 & 78 IID, 122 OOD & none & no \\ LITS  & 130 & 7 & 5 & 70 IID & none & no \\ BTCV  & 30 & 1 & 1 & 20 IID & none & no \\ CHAOS-CT  & 20 & 1 & 1 & 20 IID & none & no \\
**Touchstone (ours)** & **5,195** & **76** & **8** & **5,903 OOD** & **sex, age, race** & **yes** \\   ^{}\)Partially labeled: annotations for each organ do not cover the entire dataset, and/or may contain unlabeled samples.} \\ 

Table 1: **Related benchmarks & our innovations. We compare Touchstone with influential CT segmentation benchmarks in light of the five contributions presented in the introduction.**

Figure 1: Summary of JHH and TotalSegmentator metadata. The diversity of data distribution includes more than just the number of centers; it also includes age, sex, manufacturer, diagnosis, and many other factors. JHH is the only dataset that provides race information, allowing us to compare the results; the race information is unknown in TotalSegmentator and most publicly available datasets. Therefore, the inclusion of JHH is value-added because it enabled the analysis on race. Races HL, W, AS, AA, O, and U indicate Hispanic & Latino, White, Asian, African American, other and unknown, respectively.

structure ranges from minutes to hours, depending on the size and complexity of the regions of interest to annotate and the local surrounding anatomical structures. Each CT scan was annotated by a team of radiologists, and confirmed by one of three additional experienced radiologists to ensure the quality of the annotation. All personally identifiable information was removed and the use of this dataset has received IRB approval from Johns Hopkins Medicine under IRB00403268. JHH is considered here an OOD test set because no CT scan from the Johns Hopkins hospital is present in the training dataset.

**TotalSegmentatorV2--\(N\)**_=743; publicly available for out-of-distribution test purposes--_is from 10 institutes within the University Hospital Basel (Switzerland) picture archiving and communication system (PACS) . Being one of the largest public CT datasets, TotalSegmentator, under Apache License 2.0, was annotated by AI-assisted radiologists. It comprises both contrast-enhanced and non-contrast images, with per-sample metadata including age, sex, scanner details, diagnosis, and institution. We report AI performance on a subset of TotalSegmentator dataset2 in Table 3 and its official test set in Appendix Tables 11-12.

### Evaluation Protocols - Architectures, Frameworks, Metrics, & Statistical Analysis

In this study, we define an _architecture_ as the overall design and structure of the entire neural network model; and define a _framework_ as a set of tools or protocols that can accommodate multiple AI architectures. We evaluated 19 architectures and 3 frameworks trained by their inventors on our AbdomenAtlas 1.03. We used Dice Similarity Coefficient (DSC) and Normalized Surface Distance (NSD) to evaluate segmentation performance. We enforced that the inference speed must be faster than \(1e^{6}\) mm\({}^{3}\) per second. The inference speed for each algorithm is summarized in Appendix Table 6. We employed the same computer to evaluate all submitted algorithms. Its specifications are CPU: AMD EPYC 7713 @ 2,0Ghz\(\)64; GPU: NVIDIA Ampere A100 (80GB); RAM: 2TB. We applied statistical hypothesis testing to each possible pair of algorithms to ensure their performance differences are significant. Following Wiesenfarth et al. , we used the one-sided Wilcoxon signed rank test with Holm's adjustment for multiplicity at 5% significance level and summarized results in significance maps. Per-group metadata analysis in Appendix D.5 considers Kruskal-Wallis tests, followed by post-hoc Mann-Whitney U Tests with Bonferroni correction. More statistical analyses, such as ranking stability , are presented in Appendix D.2.

## 3 Benchmark Results

### Performances According to Out-of-distribution Evaluation on Large Datasets

We started by comparing the average DSC score over the 9 classes. MedNeXt and MedFormer are the winners of the JHH dataset; STU-Net and ResEncL are the winners of the TotalSegmentator dataset. Among these winners, three are CNNs (STU-Net, ResEncL and MedNeXt) and one is a CNN Transformer hybrid (MedFormer). There is no significant difference among these winners at \(p=0.05\) level, evidenced by the statistical analysis in Tables 2-3. Regarding frameworks, nnU-Net  is the winner since 3 out of 4 of the aforementioned winners were developed on the self-configuring nnU-Net framework.

In addition to reporting the average performance ranking, we examined the per-class performance and made the following findings. _First_, _diversified OOD evaluation is necessary._ For multiple algorithms, the DSC score for a given organ varied 15% or more across diverse test sets. E.g., the SAM-Adapter, a transformer-based 2D model, generalizes much better to JHH than to TotalSegmentator: in kidney segmentation, its DSC score differs by more than 80% across the datasets (see Appendix D.3.5 for explanations). Such stark performance variations reveal the importance of evaluating models on diverse OOD test sets. _Second_, _test dataset size matters._ More test samples increase statistical power, enabling benchmarks to more reliably detect differences between algorithms and produce stable, trustworthy rankings. Higher statistical power allows us to better differentiate the best performingmodel from the others: for JHH (\(N\)=5,160), there is at most four winners for any class, but for TotalSegmentator, there is up to eight (Tables 2-3). Appendix D.4 uses box-plots and significance heatmaps  to confirm these findings, and Appendix D.2 shows ranking order is much more stable for JHH than for smaller test sets. This finding emphasizes the importance of test dataset size for accurate and reliable algorithm comparisons. _Third_, _average-based rankings are not enough_. Tables 2-3 show that, for the same AI algorithm, DSC scores on difficult-to-segment structures, like the gallbladder and the pancreas, are usually 10-20% lower than performance on easily identifiable structures, like the liver and the spleen. Usually, the best models for average DSC are also the best at individual structures, but per-class results reveal notable exceptions. E.g., in JHH, NexToU, a graph neural network-based hybrid architecture, excels at aorta segmentation, and Diff-UNet, a diffusion-based model, excels at kidney segmentation. Accordingly, per-class results reveal hidden strengths of

   framework & architecture & param & spleen & kidneyR & kidneyL & gallbladder & liver \\   & UnSeg\({}^{}\) & 31.0M & 94.9\(\)6.0 & 92.2\(\)7.2 & 91.5\(\)7.0 & 84.7\(\)12.6 & 96.1\(\)4.4 \\  & MedNeXt  & 61.8M & 95.2\(\)6.3 & 92.6\(\)7.4 & 91.8\(\)7.3 & 85.3\(\)12.9 & 96.3\(\)4.5 \\  & NexToU  & 81.9M & 94.7\(\)8.1 & 90.1\(\)9.5 & 89.6\(\)9.3 & 82.3\(\)17.0 & 95.7\(\)5.5 \\  & STU-Net-B  & 58.3M & 95.1\(\)6.4 & 92.5\(\)7.3 & 91.9\(\)7.2 & 85.5\(\)12.3 & 96.2\(\)4.8 \\  & STU-Net-Lt  & 440.3M & 95.2\(\)6.1 & 92.5\(\)7.1 & 91.8\(\)7.1 & 85.7\(\)11.8 & 96.3\(\)4.4 \\  & STU-Net-H  & 1457.3M & 95.2\(\)5.9 & 92.6\(\)6.9 & 91.9\(\)7.1 & **86.0\(\)11.6** & 96.3\(\)4.4 \\  & U-Net  & 31.1M & 95.1\(\)6.3 & 92.7\(\)6.9 & 91.9\(\)7.2 & 84.7\(\)13.1 & 96.2\(\)4.5 \\  & ResEncl.\({}^{}\) & 102.0M & 95.2\(\)6.3 & 92.6\(\)7.0 & 91.9\(\)6.9 & 84.9\(\)12.8 & 96.3\(\)4.5 \\   & U-Net \& CLIP  & 19.1M & 94.3\(\)6.9 & 91.9\(\)7.8 & 91.1\(\)8.8 & 82.1\(\)15.4 & 96.0\(\)4.3 \\  & Swin UNETR \& CLIP  & 62.2M & 94.1\(\)7.7 & 91.7\(\)9.1 & 91.0\(\)9.1 & 80.2\(\)18.3 & 95.8\(\)5.6 \\   & LHU-Net  & 8.6M & 94.9\(\)6.3 & 92.5\(\)7.0 & 91.8\(\)7.4 & 83.9\(\)14.5 & 96.2\(\)4.3 \\  & UCTransNet  & 68.0M & 90.2\(\)11.9 & 86.5\(\)14.6 & 86.9\(\)12.8 & 78.7\(\)19.5 & 93.6\(\)6.4 \\  & Swin UNETR  & 72.8M & 92.7\(\)8.8 & 89.1\(\)11.1 & 89.7\(\)10.2 & 76.9\(\)20.7 & 95.2\(\)5.3 \\  & UNetST  & 87.2M & 93.2\(\)7.1 & 90.9\(\)8.1 & 90.1\(\)8.2 & 75.1\(\)21.2 & 95.3\(\)5.0 \\  & UNETR  & 101.8M & 91.7\(\)10.1 & 90.1\(\)9.4 & 89.2\(\)9.6 & 74.7\(\)20.4 & 95.0\(\)5.3 \\  & SegVol\({}^{}\) & 181.0M & 94.5\(\)6.9 & 92.5\(\)7.1 & 91.8\(\)7.3 & 79.3\(\)18.8 & 96.0\(\)4.7 \\   & SAM-Adapter\({}^{}\) & 11.6M & 90.5\(\)8.8 & 90.4\(\)7.9 & 87.3\(\)9.6 & 49.4\(\)22.9 & 94.1\(\)5.3 \\  & MedFormer  & 38.5M & **95.5\(\)6.1** & 92.8\(\)7.3 & 91.9\(\)7.4 & **85.3\(\)13.6** & **96.4\(\)4.4** \\  & Diff-UNet  & 434.0M & 95.0\(\)6.9 & **92.8\(\)7.4** & **91.9\(\)7.5** & 83.8\(\)14.8 & 96.2\(\)4.7 \\   & architecture & speed & stomach & aorta & postcava & pancreas & average \\   & UniSeg\({}^{}\) & 198 & 93.3\(\)6.0 & 82.3\(\)10.3 & 81.2\(\)8.1 & 82.7\(\)10.4 & 88.8\(\)8.0 \\  & MedNeXt  & 308 & 93.5\(\)6.0 & 83.1\(\)10.2 & 83.1\(\)8.3 & 83.3\(\)10.0 & **89.2\(\)8.2** \\  & NexToU  & 654 & 92.7\(\)7.5 & 86.4\(\)8.7 & 78.1\(\)9.1 & 80.2\(\)13.5 & 87.8\(\)9.8 \\  & STU-Net-B  & 418 & 93.5\(\)6.0 & 82.1\(\)10.5 & **81.3\(\)8.2** & 83.3\(\)10.7 & 89.0\(\)8.1 \\  & STU-Net-Lt  & 179 & 93.7\(\)5.6 & 81.0\(\)10.9 & 81.3\(\)8.2 & 83.4\(\)10.7 & 89.0\(\)8.0 \\  & STU-Net-H  & 73 & **93.7\(\)5.7** & 81.1\(\)10.9 & 81.1\(\)8.2 & **83.4\(\)10.7** & 89.0\(\)9.7 \\  & U-Net  & 1064 & 93.3\(\)6.0 & 82.8\(\)10.2 & 81.0\(\)8.2 & 82.3\(\)11.4 & 88.9\(\)8.2 \\  & ResEncl.\({}^{}\) & 794 & 93.4\(\)6.0 & 81.4\(\)11.1 & 80.5\(\)8.8 & 82.9\(\)10.8 & 88.8\(\)8.3 \\  & ResEncl.\({}^{}\) & 794 & 93.5\(\)5.9 & 80.0\(\)7.3 & 80.5\(\)8.7 & 82.8\(\)11.1 & 89.5\(\)7.8 \\   & U-Net \& CLIP  & 543 & 92.4\(\)6.8 & 77.1\(\)12.7 & 78.5\(\)9.6 & 80.8\(\)11.5 & 87.1\(\)9.3 \\  & Swin UNETR \& CLIP  & 606 & 92.2\( algorithms. For a more comprehensive evaluation, Appendix C analyzes performance measured by NSD scores. _Fourth, inviting innovation is important._ As in past 3D medical segmentation challenges , CNNs with the nnU-Net framework  showed strong performance in our benchmark. However, by searching for innovative algorithms, sending target invitations to their inventors, and performing comprehensive evaluations, we could reveal strengths of new and less well known models, such as vision-language algorithms and Diff-UNet, the first 3D medical image segmentation method based on diffusion models, and MedFormer, a hybrid architecture that combines convolutional inductive bias with efficient, scalable bidirectional multi-head attention. Meanwhile, the LHU-Net, a hybrid architecture combining CNN and transformer attention mechanisms, excels in computational efficiency: it is 2 to 4 times faster than models with similar accuracy.

   framework & architecture & param & spleen & kidneyR & kidneyL & gallbladder & liver \\   & UniSeg\({}^{}\) & 31.0M & 89.4\(\)19.4 & 84.5\(\)23.8 & 81.9\(\)27.9 & 74.6\(\)27.4 & 91.7\(\)16.5 \\  & MedNeXt  & 61.8M & 91.6\(\)18.8 & 85.5\(\)24.8 & 86.0\(\)23.8 & 75.8\(\)28.5 & 93.0\(\)15.8 \\  & NextToU  & 81.9M & 81.2\(\)9.5 & 78.2\(\)32.7 & 78.7 & 72.0\(\)31.2 & 87.6\(\)23.0 \\  & STU-Net-B  & 58.3M & 92.3\(\)15.4 & 87.1\(\)20.3 & 86.8\(\)22.1 & **78.5\(\)25.0** & 93.0\(\)13.9 \\  & STU-Net-L  & 440.3M & 91.6\(\)17.8 & 82.8\(\)18.6 & 86.3\(\)22.9 & 71.8\(\)24.7 & **94.2\(\)11.2** \\  & STU-Net-Net  & 1457.3M & 92.1\(\)14.6 & 88.9\(\)16.3 & 86.5\(\)23.4 & 77.7\(\)25.4 & 94.0\(\)11.4 \\  & U-Net  & 31.1M & 91.2\(\)17.8 & 88.4\(\)18.3 & 87.7\(\)20.8 & 78.3\(\)25.5 & 93.4\(\)13.8 \\  & ResEnd\({}^{}\) & 102.0M & 91.8\(\)17.5 & **88.9\(\)18.0** & 88.2\(\)20.5 & 78.0\(\)25.2 & 91.7\(\)18.4 \\  & ResEnd\({}^{}\) & 102.0M & 91.0\(\)17.5 & **88.9\(\)18.0** & 88.2\(\)20.5 & 78.0\(\)25.2 & 91.7\(\)18.4 \\  & ResEnd\({}^{}\) & 102.0M & 91.0\(\)17.5 & 87.0\(\)17.0 & 88.3\(\)20.5 & 78.0\(\)25.2 & 91.7\(\)17.4 \\  & ResEnd\({}^{}\) & 102.0M & 91.0\(\)17.5 & **88.9\(\)18.0** & 88.2\(\)20.5 & 78.0\(\)25.2 & 91.7\(\)17.4 \\  & ResEnd\({}^{}\) & 102.0M & 91.0\(\)17.5 & **88.9\(\)18.0** & 88.2\(\)20.5 & 78.0\(\)25.2 & 91.7\(\)17.4 \\   & U-Net \& CLIP  & 19.1M & 87.4\(\)23.8 & 83.6\(\)25.6 & 82.7\(\)26.6 & 73.1\(\)29.1 & 91.6\(\)14.8 \\  & Swin UNETR \& CLIP  & 62.2M & 87.1\(\)22.4 & 81.1\(\)29.0 & 77.0\(\)32.3 & 70.3\(\)31.0 & 91.6\(\)16.0 \\   & LHU-Net  & 8.6M & 86.0\(\)25.7 & 81.8\(\)29.3 & 82.4\(\)27.0 & 71.3\(\)32.2 & 87.7\(\)22.9 \\  & UCTransNet  & 68.0M & 76.4\(\)34.5 & 74.3\(\)35.2 & 62.0\(\)41.5 & 69.6\(\)31.9 & 82.6\(\)28.2 \\  & Swin UNETR  & 72.8M & 66.3\(\)36.4 & 57.9\(\)39.4 & 58.5\(\)40.2 & 50.6\(\)40.6 & 80.2\(\)28.7 \\  & UTSerT  & 87.2M & 79.5\(\)26.7 & 73.8\(\)22.4 & 72.0\(\)33.8 & 50.3\(\)40.0 & 87.6\(\)20.9 \\  & UNETR  & 101.8M & 60.4\(\)37.9 & 47.9\(\)39.6 & 41.9\(\)39.8 & 40.0\(\)36.9 & 78.1\(\)29.9 \\  & SegVol\({}^{}\) & 181.0M & 87.1\(\)23.0 & 82.8\(\)23.5 & 82.6\(\)24.8 & 68.1\(\)29.3 & 89.4\(\)20.5 \\   & SAM-Adapter\({}^{}\) & 11.6M & 53.5\(\)33.4 & 85.1\(\)11.1 & 19.9\(\)22.1 & 11.5\(\)17.6 & 66.4\(\)35.5 \\  & MedFormer  & 38.5M & 90.7\(\)15.0 & 85.5\(\)18.5 & 85.0\(\)21.5 & 74.1\(\)26.8 & 92.8\(\)12.4 \\  & Diff-UNet  & 434.0M & 88.3\(\)23.6 & 81.3\(\)27.9 & 81.0\(\)28.4 & 71.8\(\)30.0 & 92.4\(\)14.9 \\   & framework & architecture & speed & stomach & aorta & IVC\({}^{}\) & pancreas & average \\   & UniSeg\({}^{}\) & 198 & 74.0\(\)29.5 & 69.2\(\)31.5 & 72.8\(\)25.9 & 70.3\(\)30.9 & 78.7\(\)25.9 \\  & MedNeXt  & 308 & 77.2\(\)28.7 & 71.9\(\)30.1 & 75.2\(\)23.5 & 71.6\(\)31.4 & 80.9\(\)25.0 \\  & NexToU  & 654 & 69.0\(\)34.7 & 65.1\(\)33.0 & 59.4\(\)13.7 & 66.8\(\)32.0 & 72.9\(\)31.1 \\  & STU-Net-B  & 418 & 78.6\(\)26.5 & 74.2\(\)28.9 & 73.3\(\)19.6 & 74.9\(\)22.5 & 82.2\(\)22.1 \\  & STU-Net-L  & 179 & 79.7\(\)24.6 & **75.7\(\)27.0** & **77.6\(\)18.7** & 75.2\(\)27.0 & **83.0\(\)21.4** \\  & STU-Net-H  & 73 & **78.5\(\)25.5** & **74.7\(\)28.1** & 76.9\(\)19.0 & 74.5\(\)27.6 & 82.7\(\)

### Potential Confounders Significantly Impact AI Performance

We leveraged the metadata available in test datasets to assess AI' performance consistency across diverse demographic groups. We studied correlation between AI performance and the five types of metadata: age, sex, and diagnosis are analyzed on all two datasets, race is only analyzed on one dataset, JHH, since most public test sets lack this information, and manufacturer is only analyzed in one dataset.

Figure 2 displays per-group DSC for an average AI model, i.e., the average performance across our 19 evaluated algorithms. The statistical analysis further highlights the need for large test datasets: JHH's

Figure 2: **Potential confounders significantly impact AI performance.** Boxplots showing the average DSC score of nine classes and 19 algorithms for diverse demographic groups in two OOD test sets: TotalSegmentator and JHH. Whiskers indicate 1.5\(\)IQR (interquartile range). Statistical significance is indicated by stars: \(*\)\(p<0.05\), \(**\)\(p<0.01\), \(***\)\(p<0.001\), \(***\)\(p<0.0001\). We perform Kruskal–Wallis tests followed by post-hoc Mann–Whitney U Tests with Bonferroni correction. Greater performance differences are observed in the JHH dataset compared to TotalSegmentator, likely due to the larger number of CT scans. Differences are apparent across demographic groups such as age, diagnoses, scanner manufacturer, sex, and medical institutions. Races HL, W, AS, AA, O, and U indicate Hispanic&Latino, White, Asian, African American, other and unknown, respectively.

large sample size (\(N\)=5,160) allows detection of statistically significant DSC differences across all metadata, but some of these differences (for age and sex) are noticeable but not significant in the smaller TotalSegmentator dataset. _Notably, AI performance reduces for advanced age_. Median DSC starts dropping around the fifties. JHH shows multiple statistically significant performance drops after this age. The creators of the TotalSegmentator observed that aging caused attenuation in CT scans , which may explain the common descending DSC trend after age 50, despite the fact that the 60-69 age group is the most populous in most datasets (Figure 1). This trend exists for all tested AI algorithms (Appendix D.5 displays per-group performances for each algorithm and organ). _Sex only significantly confounds some AI algorithms._ The median DSC is significantly smaller for women in JHH. However, multiple top-performing models show no significant performance difference across sexes in any dataset (e.g., nnU-Net, STU-Net, and Diff-UNet), showcasing current AI can be robust to this confounder. _We found significant performance differences for diverse races_. AI performance for white patients was significantly superior to the performance for African Americans, showing the need to increase the presence of this demographic group in public CT scan datasets. Again, many of the best performing algorithms did not present statistically significant differences for these two races (Appendix D.5). _In all datasets, diagnosis significantly impacted AI performance._ Cancer patients have significantly smaller DSC scores in JHH (\(p<0.0001\)), and trauma patients have median DSC scores below other groups in TotalSegmentator. _Scanner manufacturer changes cause significant DSC differences (\(p<0.05\)) in TotalSegmentator_.

## 4 Conclusion & Discussion

**Conclusion.**_Are we on the right way for evaluating AI algorithms for medical segmentation?_ This paper outlines five properties of an ideal benchmark: (I) diverse data distribution in both training and test datasets, (II) a large number of test samples, (III) varied evaluation perspectives, (IV) equitably optimized AI algorithms, and (V) a long-term commitment. Touchstone sets itself apart from previous benchmarks in these criteria, enabling us to share unique insights that often missing in standard benchmarks. Our findings indicate: (1) AI performance can vary significantly across different datasets, with per-class differences of 10-20% common, and up to 80% observed (SAM-Adapter in kidney); thus, out-of-distribution evaluation across multiple datasets is crucial for ensuring AI's reliability and clinical adoption. (2) Larger test datasets reveal more significant differences between AI algorithms, allowing for meaningful rankings and nuanced analyses. (3) Average rankings can obscure AI's specific strengths; per-organ and metadata analysis is crucial in highlighting the benefits of innovative vision-language algorithms and the first diffusion-based 3D medical segmentation model. (4) By evaluating diverse AI architectures trained by their inventors, we establish a fair reference point for future development, which Touchstone will continually support with a long-term commitment.

**Label Noise in Training Set.** There is no perfect ground truth in segmentation datasets (except for synthetic data [32; 42; 13; 17; 14; 40; 45]), especially in the abdominal region where anatomical boundaries can be blurry due to disease or age (examples in Appendix A.3). Identifying these boundaries is challenging for both human annotators and AI algorithms. Many recent datasets, including TotalSegmentator  and AbdomenAtlas 1.0 , use human-in-the-loop strategies, combining AI-predicted annotations and manual annotations by radiologists, which inevitably contain label errors. The errors in AbdomenAtlas 1.0 arise from poor CT image quality (e.g., BDMAP_0000339, BDMAP_00001044, BDMAP_00003725), mistakes in AI predictions but not revised by humans, and inconsistency in label standards across the public datasets incorporated into AbdomenAtlas 1.0 . With the feedback from our benchmark participants, we can _partially_ detect these label errors, primarily in the aorta (32.4%), a structure with high annotation standard inconsistency in public data (e.g., in BTCV and FLARE) [47; 48], and in the L&R kidneys (2.6%). We revised AbdomenAtlas 1.0 by reducing label errors in the aorta to 5.4% and in the L&R kidneys to 0.6%. A ResEncL trained on the revised AbdomenAtlas 1.0 showed statistically significant performance gains in the aorta, but gains for kidneys were small and not always statistically significant (see Tables 2-3). These results highlight that current AI may be resistant to moderate levels of label noise (2.6%), but not to high levels (32.4%), as we detail in Appendix E. As future work, an improved label error detector will be a valuable tool for automatically assessing the quality of publicly available datasets and quickly improving quality through human annotation based on detected errors.

**High-Quality, Proprietary Test Set.** Having JHH (\(N\)=5,160) available for third-party evaluation is a big plus for OOD benchmarks. It was completely annotated by radiologists, manually and following a well-defined annotation standard, for several years . Thus, it can serve as a gold standard for our benchmark. The fact that JHH is a private dataset has both problems and benefits. It can significantly increases feedback time for AI performance evaluation, as it requires additional procedures to submit the AI to a third party, set it up, and run it on over 5,000 CT scans. If a benchmark takes too much work to run, it will not gain wide traction. But making test set (either images or annotations) publicly available can cause more problems--including completely destroying the OOD benchmark. For example, Medical Segmentation Decathlon (MSD)  was a benchmark with publicly accessible test images and its test annotations were private. Similarly, BTCV  released both testing images and annotations. However, due to the growing need for more annotated data in the medical domain, even MSD/BTCV test sets have been annotated and integrated into recent public datasets, like FLARE  and AbdomenAtlas . Therefore, any AI models trained or pre-trained on these public datasets are problematic in the MSD/BTCV leaderboard. With widespread access to test data, it becomes challenging to fairly compare models, as some may be overly optimized for the benchmark rather than for real-world performance. As a result, researchers must continue to seek or develop new datasets--preferably with images and annotations that have never been disclosed. This is critical in many fields as well. Yann Lecun--_beware of testing on the training set_--in response to the incredible results achieved by GPT. Therefore, our proprietary JHH dataset is a valuable resource that other researchers can exploit to reduce data leakage risks and improve the reliability of OOD benchmark results. Our Touchstone Benchmark is still in the initial stage, so we are very careful with the decision of releasing JHH images/annotations. It must be managed carefully to ensure its benefits outweigh the risks.

**Per-Group Metadata Analysis.** Our study underscores the need for detailed metadata for algorithmic benchmark, which is currently a big limitation in the medical domain. Evidenced by Table 1, only KiTS & FLARE provided metadata analysis on sex, age, and/or race. Our Touchstone not only provides more extensive metadata analyses, including diagnosis, but also offers an order of magnitude more test data (\(N\)=5,903) for benchmarking. We have analyzed AI performance by metadata such as sex, age, and race but realized that a more rigorous analysis could be based on combined criteria (e.g., white females aged 30-40). Therefore, in the next round of benchmarking, instead of only providing average performance per class, we will also offer participants per-case performance along with each case's metadata information. This approach will provide a richer understanding of the pros/cons of AI algorithms and potentially stimulate AI innovation.

**Architectural Insights.** In Appendix D.3, we have provided architectural comparison of both the top-ranking and bottom-ranking algorithms. But we find it difficult to extract trustworthy architectural insights directly from our current benchmark results. For example, Tables 2-3 show that top performing models in our benchmark are usually CNNs within the nnU-Net framework. However, it is unclear if this is due to an intrinsic advantage of CNNs over Transformers or just an indication of nnU-Net's superior pipeline configuration. Given that Transformers are newer, future frameworks, designed for them, could potentially enhance their performance. I.e., mature frameworks that extract the best from both CNNs and transformers should allow fairer architectural comparisons in the future. Beyond medical imaging, the architectural debate between CNNs and Transformers in computer vision has been ongoing and remains unresolved . Our benchmark provides 'predictions-only' results, which can be heavily influenced by many factors such as preprocessing, data augmentation, post-processing, and training hyper-parameters . To draw convincing architectural insights, extensive ablation studies under controlled settings are required. However, conducting ablation studies for all 19 AI algorithms would be extremely costly for us. We anticipate further insights and details from the AI inventors' upcoming technical reports, including extensive ablation studies. We are also happy to assist the inventors in their ablation studies by providing feedback on the OOD evaluation results of their algorithm variants.

With the success of the first edition of Touchstone Benchmark, we are actively pursuing multi-center, OOD datasets, to further enhance the benchmark. This is difficult for many well-known reasons--patient privacy, ethical compliance, data annotation, intellectual property, etc. _Rome wasn't built in a day._ A multi-center, OOD dataset can never be made without accumulating the contribution of every single-center dataset. We hope this benchmark initiative at Johns Hopkins University, a highly regarded institution, could also inspire more institutes to contribute their private datasets for third-party OOD evaluation.