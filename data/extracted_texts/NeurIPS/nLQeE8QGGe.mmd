# Active learning of neural population dynamics

using two-photon holographic optogenetics

 Andrew Wagenmaker

University of California, Berkeley

&Lu Mi

Georgia Tech

&Marton Rozsa

Allen Institute for Neural Dynamics

&Matthew S. Bull

Allen Institute for Brain Science

&Karel Svoboda

Allen Institute for Neural Dynamics

&Kayvon Daie

Allen Institute for Neural Dynamics

&Matthew D. Golub

University of Washington

&Kevin Jamieson

University of Washington

Equally contributing first authors.

Correspondence to: ajwagen@berkeley.edu or lmi7@gatech.edu.

###### Abstract

Recent advances in techniques for monitoring and perturbing neural populations have greatly enhanced our ability to study circuits in the brain. In particular, two-photon holographic optogenetics now enables precise photostimulation of experimenter-specified groups of individual neurons, while simultaneous two-photon calcium imaging enables the measurement of ongoing and induced activity across the neural population. Despite the enormous space of potential photostimulation patterns and the time-consuming nature of photostimulation experiments, very little algorithmic work has been done to determine the most effective photostimulation patterns for identifying the neural population dynamics. Here, we develop methods to efficiently select which neurons to stimulate such that the resulting neural responses will best inform a dynamical model of the neural population activity. Using neural population responses to photostimulation in mouse motor cortex, we demonstrate the efficacy of a low-rank linear dynamical systems model, and develop an active learning procedure which takes advantage of low-rank structure to determine informative photostimulation patterns. We demonstrate our approach on both real and synthetic data, obtaining in some cases as much as a two-fold reduction in the amount of data required to reach a given predictive power. Our active stimulation design method is based on a novel active learning procedure for low-rank regression, which may be of independent interest.

## 1 Introduction

Neural population dynamics describe how the activities across a population of neurons evolve over time due to local recurrent connectivity and inputs to the population from other neurons or brain areas. Identifying these population dynamics can provide critical insight into the computations performed by a neural population . Dynamical systems models have enabled neuroscientists to generate and test a multitude of hypotheses about how specific neural populations support the neural computations that underlie, for example, motor control [2; 3; 4], motor timing [5; 6], decision making [7; 8; 9; 10], working memory , social behavior , and learning [13; 14; 15; 16].

The traditional approach to data-driven modeling of a neural population typically involves two separate stages. First, neural population activity is recorded while an animal performs a task of interest. Then, a dynamical systems model is fit to the recorded neural responses [17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33]. This approach suffers from two key limitations. First, any inferred structure is purely correlational, and cannot be interpreted with any notion of causality. Second, the experimenter has limited control over how the neural population dynamics are sampled, which can lead to inefficient data collection--oversampling in some parts of neural activity space while altogether missing others. Given constraints on time and resources in neurophysiological experiments, there is a strong need for techniques that minimize the amount of experimental data required to identify the neural population dynamics.

We seek to overcome these limitations by actively designing the causal circuit perturbations that will be most informative to learning a dynamical model of the neural population response. For circuit perturbations, we employ two-photon holographic photostimulation (Figure 1), which provides temporally precise, cellular-resolution optogenetic control over the activity of ensembles of neurons [34; 35; 36; 37; 38; 39; 40; 41]. When paired with two-photon calcium imaging, photostimulation protocols can provide insight into network connectivity by enabling the measurement of the causal influence that each perturbed neuron exerts on all other recorded neurons [36; 39; 42; 43; 44; 45; 46]. This platform enables targeted excitation of the neural population dynamics, thus providing the experimenter with unprecedented control over the data collected for informing a model of the neural population dynamics.

Here, we develop active learning techniques for designing photostimulation patterns that allow for efficient estimation of low-rank neural population dynamics and the underlying network connectivity. First, we introduce a low-rank autoregressive model that captures low-dimensional structure in neural population dynamics and allows inference of the causal interactions between recorded neurons. We then propose an active learning procedure which chooses photostimulations to target this low-dimensional structure, and demonstrate it in two settings: estimating the underlying causal interactions when using the learned autoregressive model as a simulator of the true dynamics, and adaptively selecting which samples to observe from our dataset of neural population activity recorded via two-photon calcium imaging of mouse motor cortex in response to two-photon holographic photostimulation. In both cases, we show that our active approach obtains substantially more accurate estimates with fewer measurements compared to passive baselines. Our methodology is based on a novel analysis of nuclear-norm regression with non-isotropic inputs. To the best of our knowledge, this is the first approach to demonstrate significant gains applying active learning to low-rank matrix estimation problems, and thus we believe this may be of independent interest.

## 2 Related Work

**Modeling Neural Responses to Stimulation.** Many studies have applied direct electrical or optical stimulation to neural populations to probe the dynamical properties of neural circuits and their relation to circuit function [47; 48; 49; 10; 26; 4]. However, these stimulation techniques lack the spatial specificity needed to precisely probe the causal influence of individuals neurons on the population dynamics, and these experimental designs were _passive_ in that the stimulation protocols were specified prior to data collection (with [44; 48] as notable exceptions). Other work has explored a related but separate problem of minimizing off-target effects when photostimulating individual neurons .

**Low-Rank Matrix Recovery.** Low-rank matrix recovery has been intensively researched over the last decade and a half [50; 51; 52]. However, existing analyses rely critically on the assumption that the set of measurements taken are highly symmetric and satisfy some notion of the restricted isometry property (RIP) or incoherence. The matrix recovery problem of our setting departs from the classical literature in several ways. First, the set of feasible measurements we can take is constrained by the physical limits of the photostimulation system. Second, as we aim to _adapt_ and _actively_ learn these matrix coefficients, we should expect that our resulting set of measurements should be highly skewed by design. Motivated by this, we develop, to the best of our knowledge, the first bounds on low-rank estimation using the nuclear norm heuristic that gives a quantification of the estimation error in terms of the precise individual measurements taken (i.e., in contrast to a more global property like RIP).

**Active Learning and Low-Rank Estimation.** The active learning literature is vast, and a full survey is beyond the scope of this work. We focus in particular on active learning for dynamical systems, and problems with low-rank structure. The estimation of dynamical systems--the _system identification_ problem--is central to many areas of engineering and science . The problem of actively designing inputs to effectively estimate the parameters of a dynamical system has been studied extensively for decades [54; 55; 56; 57; 58; 59; 60; 61]. More recently, a variety of provably efficient approaches have been developed for both linear [62; 63] and nonlinear [64; 65] systems. Other related work has considered active learning for latent variable models , which are often effective models of neural dynamics. As compared to these works, a key feature of our setting is the low-rank structure present in the data, which to our knowledge has not been previously studied within the active system identification literature.

Beyond dynamical systems, some attention has been devoted to active learning with low-rank structure, in particular works on low-rank bandits [67; 68; 69; 70]. While the setting considered in these works is somewhat different--they aim to solve a bandit problem, while we are interested in regression--they similarly seek to develop active learning approaches which make efficient use of low-rank structure. Also related is the work of , which shows that in the related sparse estimation setting, there does not exist more than a logarithmic gain to being adaptive. The results of this work are minimax, however--only applying to certain "hard" problems--and do not address the matrix recovery problem.

## 3 Preliminaries

Dataset Details.Neural population activity was recorded in mouse motor cortex using two-photon calcium imaging at 20Hz of a 1mm\(\)1mm field of view (FoV) containing 500-700 neurons. Each recording spanned approximately 25 minutes and 2000 photostimulation trials. In each trial, a 150ms photostimulus was delivered and was followed by a 600ms response period before the next trial began. Each photostimulus targeted a group of 10-20 randomly selected neurons, and a total of 100 unique photostimulation groups were defined for each experiment (\( 20\) trials per group). We evaluate our techniques on four such datasets.

### Fitting Low-Rank Dynamical Models

We first seek to develop effective dynamical models of the neural activity in our photostimulation datasets. Obtaining such models will provide insight into which photostimuli are most informative, and gives us a means to evaluate the effectiveness of our active learning methods. We consider three classes of models: autoregressive (AR) models, low-rank AR models, and nonlinear RNN models. Results from fitting these models are shown in Figure 2. We describe the model details next.

At discrete time \(t\), we denote the true neural activity across the \(d\) imaged neurons as \(x_{t}^{d}\), the noisy, measured activity as \(y_{t}^{d}\), and the photostimulus intensity applied across those same \(d\) neurons as \(u_{t}^{d}\). Applying stimulus \(u_{t}\) influences the measured neural activity at the next timestep \(y_{t+1}\). However, just the snapshot \(y_{t}\) may not capture the full true _state_ of the neural population, which may include not just the current neural activity, but potentially also multiple orders of temporal derivatives. To capture these effects, we consider an AR-\(k\) model defined as:

\[x_{t+1}=_{s=0}^{k-1}(A_{s}x_{t-s}+B_{s}u_{t-s})+v, y_{t}=x_{t}+w_{t}  w_{t}(0,^{2}I_{d}),\] (3.1)

Figure 1: (a) Two-photon imaging and holographic photostimulation platform (left) and a representative image frame (right). Purple circles indicate neurons photostimulated immediately before frame acquisition. Red and blue indicate increases and decreases of firing activity, respectively, relative to before photostimulation. (b) Example time series photostimulation inputs (top) and neural responses (bottom) from 100 randomly selected neurons (out of \(d=663\) recorded neurons identified in the FoV). (c) Neural responses \(y_{t}\) occupy a low-dimensional subspace. Singular values from a representative dataset’s demeaned neural activity data matrix (blue) indicate substantially more data variance residing in a few dozen dimensions (out of the full \(d=663\) dimensional neural activity space) than is expected by chance (orange, singular values when removing low-dimensional structure by shuffling time indices independently for each neuron; note clipped horizontal axis).

where \(A_{s}^{d d}\) and \(B_{s}^{d d}\) describe the coupling between neurons and stimulus at the time lag of \(s\) timesteps, \(s=0,,k-1\), and offset \(v^{d}\) accounts for baseline neural activity. Given input-observation pairs \(\{(u_{t},y_{t})\}_{t}\), the coefficients \(\{(A_{s},B_{s})_{s=0}^{k-1},v\}\) of (3.1) can be fit using least squares. Despite its simplicity, this linear model reproduces the recorded neural activity remarkably well (see "full rank" model of Figure 2).

Neural population dynamics are frequently reported as residing in a subspace of lower dimension than the total number of recorded neurons . The population dynamics in our datasets are consistent with such low-dimensional structure, as indicated by the singular value spectrum in Figure 1(c). Inspired by this observation, we introduce a set of low-rank dynamical models, where each matrix of \(\{(A_{s},B_{s})_{s=0}^{k-1}\}\) is re-defined as diagonal plus low-rank. Explicitly, we parameterize \(A_{s}=D_{A_{s}}+U_{A_{s}}V_{A_{s}}^{}\) and \(B_{s}=D_{B_{s}}+U_{B_{s}}V_{B_{s}}^{}\), where \(D^{d d}\) with \(D_{ij}=0\) for all \(i j\), \(U^{d r}\), and \(V^{d r}\) for predefined rank \(r\). The diagonal matrices account for substantial autocorrelation in each neuron's activity (\(D_{A_{s}}\)) and for the reliable response of each neuron to direct photostimulation (\(D_{B_{s}}\)), whereas the low-rank matrices \((UV^{})\) confer coupling between neurons. To fit these parameters, we optimize the following objective function with gradient descent over all parameters:

\[*{minimize}_{A_{s},B_{s}^{d d},s=0,,k-1,v ^{d}}_{t=1}^{T}y_{t+1}-_{s=0}^{k-1}A_{s}y_{t-s}- _{s=0}^{k-1}B_{s}u_{t-s}-v)^{2}.\] (3.2)

Figure 2 shows that these low-rank models perform comparably to the full rank versions in terms of predictive performance; indeed the rank \(r=35\) model appears almost indistinguishable from the full

Figure 2: Example data and cross-validated model predictions. (a) Roll-out predictions of the activity of an example neuron \(i\) using low-rank AR-\(k\) models (\(k=4\)) and GRU networks for 22 example data segments (3.3s per segment; segments separated by brief horizontal spaces). Each model’s predictions are seeded with the first \(k=4\) timesteps (200ms) of activity from \(d=663\) neurons and are then unrolled to predict the activity across all \(d\) neurons over the next 66 timesteps, given the full 70-timestep sequence of photostimulation to all \(d\) neurons. Most responses of neuron \(i\) are tied to “direct” photostimulation of neuron \(i\) (pink, first row of panels). Several “indirect responses” are tied to stimulation of other neurons \(j i\) that influence neuron \(i\) through the population dynamics. To avoid showing all indirect stimuli (to \(d-1\) neurons), only select indirect stimuli are shown (green, second row of panels). (b) Receiver operator characteristic (ROC) curve of true-positive rate and false-positive rate for response detection are calculated on indirect responses only (left) and all direct and indirect responses (right). (c) Area under ROC curve (AUROC) and (d) mean square error (MSE) for all predictions.

rank model. From a statistical perspective, low-rank models have far fewer degrees of freedom, and hence require less data to fit.

To assess whether more expressive nonlinear models could be advantageous, we also fit a gated recurrent unit (GRU) network model, adapted from , as shown in Figure 2. Interestingly, the GRU model did not perform as well as the AR-\(k\) models, potentially due to the complexities of hyperparameter tuning. Therefore, we focus on linear models in the analysis that follows. Additional details on model fitting are provided in Appendix B.2.

### The Causal Connectivity Matrix

While we require dynamical models to predict the temporal evolution of the neural population activity, we are also interested in inferring how the activity of one recorded neuron causally influences the activity of the other recorded neurons. To address this need, we define a _causal connectivity matrix_, \(H^{d d}\), to be the mapping such that \(Hu^{d}\) quantifies the total response (across time) of each neuron to a single-timestep photostimulus \(u\). That is, \(_{t=1}^{}x_{t}=Hu\), where \(x_{1},x_{2},x_{3},\) are the neural activities generated by the population dynamics if \(u_{0}=u,u_{t 1}=0\), and \(x_{t 0}=x_{}\) is the steady state or resting state of the system subject to no photostimulation. If the dynamics are linear, or more specifically follow (3.1), such a matrix \(H\) is guaranteed to exist, and can be formed by simply rolling out (3.1) with the appropriate initializations. While \(H\) is not explicitly constrained to be low-rank, if it is obtained from a low-rank AR-\(k\) model, it too will exhibit low-rank structure.

In our experimental paradigm, photostimulation acts as a causal perturbation to the population dynamics, and as such, our statistical framework is able to capture causal interactions, as opposed to merely correlative interactions. This is in contrast to the majority of work on neural population dynamics, which involves fitting dynamical models to passively obtained data. Due to the lack of causal manipulations in these studies, one cannot distinguish whether statistical relationships arise between neurons due to correlation (e.g., due to a shared upstream influence) versus causation (e.g., neuron \(i\) directly influences neuron \(j\)). Such correlative relationships are typically referred to as "functional connectivity"; we instead use the term "causal connectivity" to convey the additional causal interpretability afforded in our setting.

To fit \(H\), we could first fit \(\{_{s},_{s}\}_{s=0}^{k-1}\) and then use these as plug-in estimates for their true values to compute \(H\). Alternatively, we take a more direct approach inspired by the definition of \(H\) itself. By inspecting the raw data of Figure 2(a) and observing the rate at which each stimulated neuron returns to baseline activity, it is clear that the system mixes (i.e., forgets the past) quickly. This suggests that the total response due to input \(u\) asymptotes after some finite number of timesteps \(\). Thus, we can apply some photostimulus \(u^{d}\) at time \(t=0\) and then measure the total response \(z=_{t=1}^{}y_{t}\), where \(y_{t}^{d}\) is the noisy measurement of the true neural response \(x_{t}\). If we repeat this for many pairs \(\{(u_{n},z_{n})\}_{n}\) then we can approximate \(H\) as

\[:=_{H^{}}_{n}\|z_{n}-H^{}u_{n}\|_{2}^{2}.\]

In this work we adopt this latter approach. Since we believe \(H\) to be low rank, this amounts to a low-rank matrix recovery problem with matrix-vector observations. In the next section, we will describe how to adaptively choose \(\{u_{n}\}_{n}\) to estimate \(H\) using as few (stimulus, response) pairs as possible. Subsequently in Section 5, we will demonstrate that actively designing inputs to accelerate the learning of \(H\) effectively accelerates the learning of the full dynamics as well.

## 4 Active Learning of Low-Rank Matrices

In the previous section, we saw that estimating the causal connectivity matrix \(H\) induced by the neural population dynamics amounts to low-rank matrix recovery, where we apply some photostimulus \(u^{d}\) and observe the neural population response \(z Hu\) plus noise. In this section we seek to understand how we should choose the photostimuli to estimate the causal connectivity as quickly as possible. To this end, in Section 4.1 we present novel results characterizing the estimation error of the nuclear norm regression estimator, and in Section 4.2 present an algorithm motivated by these results which seeks to actively estimate low-rank matrices. These results will directly motivate a procedure for designing photostimulation inputs.

To demonstrate the generality of our results, in Section 4.1 we consider a general matrix regression setting. In particular, let \(_{}^{d_{1} d_{2}}\) be a rank \(r\) (potentially non-square) matrix,some input matrix, and assume scalar observations:

\[z_{n}=_{},_{n}+_{n},_{n} (0,1),\] (4.1)

where \(_{},_{n}=(_{}^{ }_{n})\) for \(()\) the trace of a matrix. Note that the setting considered in Section 3.2 is a special case of this observation model with \(_{} H\) and, for each input stimulation \(u\), measuring the response of (4.1) to \(d\) inputs \(_{j}\) of the form \(_{j}_{j}u^{}\) for \(j=1,,d\).

Matrix Notation.We let \(\|\|_{},\|\|_{},\|\|_{}\) denote the Frobenius, operator, and nuclear norm of a matrix, respectively. \(\) denotes the pseudo-inverse of a matrix. \(()\) denotes the vectorization of a matrix, and \(()\) the inverse of the vectorization. We also let \(_{}\) denote the simplex--the set of distributions--over a set \(\).

### Constrained Nuclear Norm Estimator under Non-Isotropic Measurements

We are interested in understanding how we can effectively take into account the low-rank structure of \(_{}\), if our goal is to estimate \(_{}\) from the observations of (4.1). To this end, we consider the following nuclear-norm constrained least-squares estimator for \(_{}\):

\[=*{arg\,min}_{}\|( )-z\|_{2}^{2}:=_{n=1}^{N}(_{n},-z_{n})^{ 2}:=\{:\|\|_{}\|_{ }\|_{}\},\] (4.2)

where here we let \((_{})^{N}\) denote the vector where the \(n\)th element is \(_{n},_{}\), and \(z=(_{})+\) the vector of observations, for \(\) the vector with elements \(_{n}\). Define \(_{}=U V^{}\) as the skinny SVD such that \(U^{d_{1} r}\), \(V^{d_{2} r}\), and consider the linear projection operators \(P_{},P_{}:^{d_{1} d_{2}}^{d_{1}  d_{2}}\) defined as:

\[P_{}(M):=(I-UU^{})M(I-VV^{}) P_{}(M ):=M-P_{}(M),\]

for any \(M^{d_{1} d_{2}}\). We call \(P_{}\) the projection onto the _tangent space_ of \(_{}\). Note that the dimension of the range of \(P_{}\) is equal to just \(r(d_{1}+d_{2})-r^{2} d_{1}d_{2}.\) We are now ready to state our main result on the estimation error of \(\), for \(\) as defined in (4.2).

**Theorem 1**.: _Define \(:=\|(^{*})^{1/2}((P_{}^{*} P_{})^{ })^{1/2}\|_{}\). Then with probability at least \(1-2\):_

\[\|-_{}\|_{}(P_{}^{*} P_{}) ^{}+2\|(P_{}^{*} P_{})^{}\|_ {}d_{2}}{}}\\ +4\|P_{}(^{*})^{1/2}\|_{}\|(P_{ }^{*} P_{})^{}\|_{}(}+ }+}),\]

_where here \(^{*}(M):=_{n}_{n}_{n},M\) and \(()\) describes the sum of the eigenvalues of the linear operator \((P_{}^{*} P_{})^{}:^{d_{1} d _{2}}^{d_{1} d_{2}}\)._

Theorem 1 provides a precise bound on the estimation error of the nuclear norm estimator under arbitrary inputs \(\{_{n}\}_{n}\). To the best of our knowledge, this is the first such characterization of this estimator. This characterization is particularly essential in active learning problems, such as the problem considered here, where it is critical that we understand precisely how the estimation error scales with different inputs, in order to determine which inputs will most effectively reduce the estimation error. As the observation model of Section 3.2 is a special case of the setting considered in (4.1) with \(_{} H\), Theorem 1 provides a quantification of how quickly we can estimate the causal connectivity matrix given some set of inputs; we expand on the implications of this connection in Section 4.2.

Theorem 1 states that the estimation error of the estimator (4.2) scales (predominantly) with the strength of our inputs \(_{n}\) in the tangent space of \(_{}\). Indeed, if \([w_{1},,w_{d_{1}}]\) and \([v_{1},,v_{d_{2}}]\) are the left and right singular vectors of the full SVD of \(_{}\), and \(L^{d_{1}d_{2} r(d_{1}+d_{2})-r^{2}}\) is a matrix with orthonormal columns \((w_{i}v_{j}^{})\) for \((i,j):\{i r\}\{j r\}\), then

\[(P_{}^{*} P_{})^{} =L^{}_{n=1}^{N}\! {vec}(_{n})\!(_{n})^{}L^{} ,\]

so we see that the estimation error depends only on the scaling of \(_{n=1}^{N}\!(_{n})\!(_{n}) ^{}\) in the space spanned by \((u_{i}v_{j}^{})\) for \(i r\) or \(j r\)--the tangent space to \(_{}\). As an example of how this scales, assume that for \(n=1,,N\) the entries of each \(_{n}\) are IID \((0,1)\) and \(N r(d_{1}+d_{2})-r^{2}\)Then \( 0,\,(P_{}^{*} P_{})^{ }+d_{2})-r^{2}}{N}\), \(\|(P_{}^{*} P_{})^{}\|_{} {1}{N}\), and \(\|P_{}(^{*})^{1/2}\|_{}\). This translates to a bound of \(\|-_{}\|_{}^{2}+d_{2})- r^{2}+(1/)}{N}\). Critically, we see that this does not scale with the total number of parameters, \(d_{1}d_{2}\), but instead with \(r(d_{1}+d_{2})\), which could be much smaller. The following result, due to , provides a lower bound on the estimation error of any unbiased estimator, and shows that the rate obtained by Theorem 1 is essentially unimprovable.

**Theorem 2** (Corollary 1 of ).: _For unbiased estimator \(\), \([\|-_{}\|_{}^{2}] {tr}(P_{}^{*} P_{})^{}\)._

### Active Learning for Low-Rank Matrix Estimation

Given the above characterization, we turn now to the active learning problem: how can we best choose our inputs \(_{n}\) to speed up estimation error of \(_{}\)? For simplicity, rather than the general matrix regression setting of (4.1), we consider here the vector regression case, as this is the setting of interest in learning the causal connectivity. In particular, assume that we play some \(u_{n}^{d_{2}}\) and observe \(z_{n}=_{}u_{n}+_{n}\), for \(_{n}(0,I_{d_{1}})\). A single vector observation corresponds to observing \(d_{1}\) observations from (4.1), the responses to the matrix inputs \(_{j}_{j}u_{n}^{}\) for \(j=1,,d_{1}\). Assume that \(_{}\) is rank \(r\) and let \(V_{0}:=[v_{1},,v_{r}]\) denote the first \(r\) right singular vectors of the full SVD of \(_{}\). Then we have that:

\[(P_{}^{*} P_{})^{} =(d_{1}-r)(V_{0}^{}_{N}V_{0}) ^{}+r(_{N})^{} ,_{N}:=_{n=1}^{N}u_{n}u_{n}^{}.\] (4.3)

This calculation, combined with Theorem 1, shows that the estimation error of \(_{}\) scales with a weighting of two terms: one quantifying the amount of input energy we put into directions spanned by the top-\(r\) right singular vectors, and one that quantifies the amount of input energy played isotropically (that is, in all directions). Note, however, that the input energy played in directions \(V_{0}\) is weighted by a factor of \(d_{1}-r d_{1}\), much larger weight than the weight of \(r\) given to the term quantifying the isotropic input energy. This suggests that, to minimize the estimation error of \(_{}\), we should focus a large portion of our sampling budget to target the directions spanned by the top-\(r\) right singular vectors of \(_{}\).

This strategy admits a transparent intuition. If \(_{}\) is rank-\(r\) and some vector \(u\) is orthogonal to the top-\(r\) right singular vectors of \(_{}\), then \(_{}u=0\). Thus, if we _know_ what subspace the top-\(r\) right singular vectors of \(_{}\) span, playing \(u\) orthogonal to this subspace gives us no additional information about \(_{}\); in this case we should instead play \(u\) aligned with this subspace. This is precisely what the first term in (4.3) quantifies, while the second term reflects the fact that we must also estimate the subspace spanned by the top-\(r\) right singular vectors of \(_{}\), for which playing inputs isotropically is optimal.

In general, as we do not know \(_{}\), we do not know \(V_{0}\), and so cannot directly compute inputs minimizing (4.3). To circumvent this, we consider the following iterative procedure, which alternates between obtaining an estimate of \(_{}\), \(\), and then playing the inputs that would minimize the estimation error--minimize (4.3)--if \(\) were the true parameter. We present this procedure in Algorithm 1.

```
1:input: horizon \(N\), feasible inputs \(\), rank \(r\), feasible set \(\)
2:\(_{1} I,\)
3:for\(=1,2,3,,_{2}N\)do
4: Let \(_{0}\) denote the top-\(r\) right singular vectors of \(_{}\) and \(():=_{u}_{u}uu^{}\), solve: \[_{}^{V}_{_{ }}(_{0}^{}( )_{0})^{},_{}^{}_{_{}} ()^{}\]
5: For \(2^{}\) steps, play input \(u_{n}_{}^{V}+_{}^{}\), add observations to \(\)
6: Update estimate of \(_{}\): \(_{+1}_{ }_{(u,z)}\|z- u\|_{}^{2}\)
7:return\(_{+1}\). ```

**Algorithm 1** Active Estimation of Low-Rank Matrices

At every iteration \(\), Algorithm 1 computes two distributions over inputs: \(_{}^{V}\), which targets the top-\(r\) right singular vectors of our current estimate of \(_{}\), and \(_{}^{}\), which plays inputs isotropically, covering all directions. Rather than playing these distributions according to the precise weighting given in (4.3), we instead found it most effective to mix them at an equal rate. As we do not initially know which directions are spanned by the top-\(r\) right singular vectors of \(_{}\), \(_{}^{V}\) is not guaranteed to target the correct directions, especially in early iterations. \(_{}^{}\) plays inputs in every direction,however, and thus, even if \(_{}^{V}\) is not aligned to the top-\(r\) right singular vectors of \(_{}\), will ensure sufficient energy is still being played in the correct directions to allow for learning. Given this, we increase the weight of playing \(_{}^{}\) relative to that prescribed by (4.3).

Note that the computation of the optimal inputs is a form of _\(A\)-optimal experiment design_, which in general can be efficiently solved by, for example, the Frank-Wolfe algorithm . Furthermore, efficient procedures for solving nuclear-norm regression problems exist, allowing us to estimate \(_{+1}\) on line 6 efficiently . We remark that Algorithm 1 takes as input \(r\), the rank of \(_{}\), and \(\), which requires knowledge of \(\|_{}\|_{}\). In general, when these quantities are unknown, they can be chosen via standard cross-validation procedures.

We emphasize again that the setting considered here corresponds precisely to the setting considered in Section 3.2 with \(_{} H\), \(u_{n}\) the input stimulation patterns, and \(z_{n}\) the observed neural response to input \(u_{n}\). As such, if the causal connectivity \(H\) is low rank, Algorithm 1 and the preceding results provide a methodology to select input stimuli to most efficiently estimate \(H\). In the following section, we will apply this to our photostimulation datasets.

## 5 Active Learning for Estimating Neural Population Dynamics

We return now to the problem of photostimulus design for learning neural population dynamics, and seek to apply the insights of Section 4 to this setting. We present two sets of experiments. In Section 5.1 we use real data to fit a model of the population dynamics, treat this fitted model as a _simulator_ for the true dynamics, and then demonstrate that we can learn the causal connectivity matrix \(H\) of this simulator faster using active inputs versus passive inputs. Then, in Section 5.2 we split our real data into 750ms long trials of (stimulus, response) pairs (see Section 3) and demonstrate that our active learning algorithm is able to improve the performance of learning dynamical models on real data by adaptively selecting which trials to observe, training a model on the observed trials, and evaluating on a hold-out set of unseen trials. Here we find that our approach is able to learn an accurate model of the dynamics more quickly than non-adaptive approaches.

### Active Learning on Data-Driven Neural Population Dynamics Simulator

In Section 3.1, we demonstrated that photostimulation data can be effectively reconstructed using an AR-\(k\) dynamics model. Given the effectiveness of these models at fitting our data, in this section we treat them as a simulated representation of our true dynamics, allowing us to query them arbitrarily as a stand-in for the ground truth dynamics, and seek to determine whether carefully choosing the photostimulation pattern allows for efficient estimation of the causal connectivity matrix \(H\).

**Experiment Details.** To obtain models of the population dynamics to use for simulation, we fit an AR-\(k\) model to each dataset as described in Section 3.1. In all cases we use an AR-\(k\) model with order \(k=4\). We do one run of the experiments using low-rank model parameters \(UV^{}\) with rank \(r=15\), and then repeat the experiments using \(r=35\). In each case, we simulate \(N=10000\) trials, where each trial corresponds to applying a photostimulus and observing the response for \(=15\) timesteps, simulating our true data generation process. To simulate measurement noise and other trial-to-trial variability in neural responses, we corrupt the observations with Gaussian random noise. Motivated by the empirically observed fast decay of population dynamics in our datasets, we reset the initial state of the simulator at each new trial.

In practice, both the magnitude of the stimuli and number of neurons stimulated at each timestep are constrained by the photostimulation platform. To reflect this limitation in our simulator, we constrain our inputs to lie in \(\), and also impose a sparsity penalty. Precisely, we choose the input set \(\) in Algorithm 1 to be \(:=\{u^{d}\ :\ \|u\|_{1}\}\), for some value \(>0\) (which we set to \(=30\)). While this does not explicitly constrain inputs to be sparse, it can be efficiently optimized over, and we found in practice that the optimal inputs within this constraint set are in general at least \(2\)-sparse. As baseline methods, we consider the following:

* _Random Stimulation_: At each trial \(n\), choose \(\) neurons at random, and set corresponding elements of \(u_{n}\) to 1.
* _Uniform Stimulation_: Compute \(^{}\) as in Algorithm 1 and play inputs \(u_{n}^{}\) for all \(n\).

Our goal is to estimate the causal connectivity matrix \(H\) induced by our learned dynamics (see Section 3.2). In practice, we are most interested in estimating the _off-diagonal_ elements of \(H\), as these correspond to causal interactions between different neurons. To this end, we consider the error metric \()\|_{F}}{\|M H\|_{F}}\), for \(\) our estimate of \(H\), \(M\) a matrix with all entries \(1\) except its diagonal, which is \(0\), and \(\) element-wise multiplication.

**Experiment Results.** We present our results in Figure 3. As can be seen, across all learned simulators and rank levels, our active learning approach yields a non-trivial gain over both baseline approaches. In particular, on Mouse 1 and both datasets for Mouse 3, we observe a gain of between \(1.5\)-\(2\) over baselines--that is, to achieve a given estimation error, our approach requires between \(1.5\)-\(2\) fewer samples than baseline methods. This demonstrates the effectiveness of our active learning procedure for estimating low-rank matrices--our method is able to exploit the low-rank structure present in the underlying dynamics to speed up estimation, as compared to methods which do not take into account this structure. Furthermore, it shows that on a realistic simulation of neural population dynamics, we can effectively design stimuli to speed up the estimation of the dynamics.

### Active Ranking of Real Data Observations

As described in Section 3, each of our datasets consist of roughly 2000 (stimulus, response) trials. In an online photostimulation experiment, we would choose the photostimulus actively for each trial. Here we seek to simulate this process using real experimental data, but offline, by choosing the _ordering_ of the trials available in our pre-collected datasets. This serves as a testbed for active learning procedures: if we can more efficiently learn models in this offline setting, that is a strong indication that we should also see gains in online experiments. Indeed, those gains may be even greater online because in our offline setting we are severely restricted to choosing from only 100 candidate stimulation patterns. Thus, we interpret the results in this section as a _lower bound_ on the performance we might expect online.

To validate this approach, we randomly choose 20 (out of the 100 total) unique photostimulation patterns and set aside a test set containing all 20 repeated trials of those photostimuli. This creates an 80%/20% train-test split of non-overlapping stimulus patterns. For \(_{}\) and \(_{}\) our train and test datasets, respectively, we consider the following query model:

```
1:\(\)
2:for trials \(n=1,2,,|_{}|\)do
3: Choose input trajectory \(_{}\), set \(\{\}\), \(_{}_{} \{\}\)
4: Estimate model using data in \(\), and compute prediction MSE of model on \(_{}\) ```

Figure 3: Performance of active stimulation design on estimating learned dynamics model. For each mouse dataset, we fit a low-rank AR-\(k\) model as described in Section 3.1 (for ranks of 15 and 35, and \(k=4\)). Treating this as a simulator of the true dynamics, we compare our active stimulation design procedure (Active, Algorithm 1) to randomly choosing groups of neurons to excite (Random), and uniformly allocating stimulation across all neurons (Uniform), and plot how effectively each is able to estimate the connectivity of the simulator dynamics. For each figure and method we average over 20 trials, and plot the mean performance with error bars denoting 1 standard error (note that the error bars are barely visible as the standard deviation is very small).

We fit a dynamics model to the current set of observed trials, as described in Section 3.1, and use this model to predict the response of the true system on the held-out test inputs, computing the mean-squared error of these predictions as our metric. We apply a variant of Algorithm 1, described in more detail in Appendix B.4, and adapted to the query model above. In particular, to apply Algorithm 1 to learning a full dynamical system, we choose our inputs to target the right singular vectors of \(B_{s}\) in (3.1). As a baseline method, we consider the procedure which randomly chooses an unobserved segment from \(_{}\) at each iteration.

We run the above experiment for 20 different randomly generated train-test splits on each dataset, and present our results in Figure 4, providing the results for the average performance over the train-test splits, as well as the best- and worst-case splits for active learning performance. As these results illustrate, though active learning does not give a substantial gain in all cases, in many cases it is able to give a gain of up to a factor of \(2\) in the number of samples required over the random baseline, and in the worst case, matches the baseline performance. This further confirms that taking into account low-rank structure when choosing which measurements to take can improve estimation rates, and, we believe, is a strong indicator that our active learning procedure would speed up estimation of neural population dynamics in online settings.

## 6 Discussion

In this work, we have developed a principled approach to active learning of photostimulation inputs for the identification of neural population dynamics and connectivity. We discuss three limitations of our approach, which each suggest potential future directions. First, we have considered active learning of the causal connectivity matrix and minimization of prediction error, both uniformly across all recorded neurons. Future work may focus on more specific scenarios, such as targeting particular dimensions of the neural activity space or changes in connectivity due to learning. Second, while we found that linear dynamics fit our data remarkably well, this may not always be the case. Does our methodology effectively scale to nonlinear dynamics? Finally, our real-data experiments were performed offline. Future work may explore running our algorithm online during closed-loop photostimulation experiments.

Figure 4: Performance of active learning estimating photostimulation response on held-out trials. Each mouse dataset is split into trials corresponding to a stimulus-response pair, and we consider how these trials might be ordered to obtain more effective estimates with fewer training data trials, simulating the active learning process. Our approach (Active) is motivated by the low-rank excitation criteria of Algorithm 1 (see Appendix B.4 for more details) and we compare with randomly choosing which trial to observe next (Random). We plot the accuracy of the learned model in predicting neural responses on held-out test trials. We consider 20 different train-test splits (with 20 trials per split), and include plots of average performance across these splits, as well as splits where Active has the largest and smallest improvement over Random. We plot error bars denoting 1 standard error (note again that the error bars are barely visible as the standard deviation is very small).

#### Acknowledgments

This work was supported by NSF DMR award 2308979 to the University of Washington Materials Science Research Center (AW & KJ), the Shanahan Foundation Fellowship (LM & MSB), the Paul G. Allen Foundation (MR, KS, KD & MDG), NIH award R00-MH121533 (MDG), NSF CCF award 2007036 (KJ), and NSF CAREER award 2141511 (KJ).