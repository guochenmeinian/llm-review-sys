# Model Manipulation Attacks Enable More Rigorous Evaluations of LLM Capabilities

Zora Che

University of Maryland, College Park, MATS

zche@umd.edu

&Stephen Casper

MIT CSAIL

scasper@mit.edu

Anirudh Satheesh

University of Maryland, College Park

&Rohit Gandikota

Northeastern University

&Domenic Rosati

Dalhousie University

Stewart Slocum

MIT CSAIL

&Lev McKinney

University of Toronto

&Zichu Wu

University of Waterloo

Zikui Cai

University of Maryland, College Park

&Bilal Chughtai

Independent

&Daniel Filan

MATS

Purong Huang

University of Maryland, College Park

&Dylan Hadfield-Menell

MIT CSAIL

Equal contribution

###### Abstract

Evaluations of large language model (LLM) capabilities are increasingly being incorporated into AI safety and governance frameworks. Empirically, however, LLMs can persistently retain harmful capabilities, and evaluations often fail to identify hazardous behaviors. Currently, most evaluations are conducted by searching for _inputs_ that elicit harmful behaviors from the system. However, a limitation of this approach is that the harmfulness of the behaviors identified during any particular evaluation can only lower bound the model's worst-possible-case behavior. As a complementary approach for capability elicitation, we propose using _model-manipulation_ attacks which allow for modifications to the latent activations or weights. Here, we test 8 state-of-the-art techniques for removing harmful capabilities from LLMs against a suite of 5 input-space and 5 model-manipulation attacks. In addition to benchmarking these methods against each other, we show that (1) model resilience to capability elicitation attacks lies on a low-dimensional robustness subspace; (2) the attack success rates (ASRs) of model-manipulation attacks can help to predict and develop conservative high-side estimates of the ASRs for held-out input-space attacks; and (3) state-of-the-art unlearning methods can easily be undone within 50 steps of LoRA fine-tuning. Together these results highlight the difficulty of deeply removing harmful LLM capabilities and show that model-manipulation attacks enable substantially more rigorous evaluations for undesirable vulnerabilities than input-space attacks alone.1

## 1 Introduction

Rigorous evaluations of large language models (LLMs) are widely recognized as key for risk mitigation (Raji et al., 2022; Anderljung et al., 2023; Schuett et al., 2023; Shevlane et al., 2023) and are consistently being incorporated into AI governance initiatives (POTUS, 2023; NIST, 2023; UK DSIT, 2023; EU, 2023). However, despite their efforts, developers often fail to identify and fix overtly harmful LLM behaviors before deployment (Wei et al., 2024; Shayegani et al., 2023; Carliniet al., 2024). Existing automated evaluations often fall short of identifying an LLM's true potential for harm (Casper et al., 2024). For example, Li et al. (2024) demonstrated that even when common, standardized benchmark attacks failed to jailbreak an LLM, manual ones succeeded more than 50% of the time. This highlights a fundamental limitation of using input-space attacks for LLM evaluations: the worst behaviors identified during any particular evaluation can only offer a lower bound of the model's overall worst-case behaviors. Poor evaluations can not only underestimate risks but can be counterproductive if they lead to a false sense of security (Anderljung et al., 2023).

In safety-critical engineering fields, it is a very common principle to design and test systems to ensure that they can handle stresses at least as strong - and ideally stronger - than what they will face in deployment (Clausen et al., 2006). For example, in construction, buildings are often designed to handle several times the load they are ever intended to bear. Here, we propose an analogous approach for AI system evaluations.

In addition to evaluating systems under input-space threats, we also propose using _model manipulation_ attacks which allow for adversarial modifcations to the model's weights or latent activations. We attempt to answer two questions, each corresponding to a different threat model:

* **How vulnerable are LLMs to model manipulation attacks?** Answering this will help us understand how model-manipulation attacks can directly help to study risks from models that are open-sourced2 or may be leaked (Nevo et al., 2024). * **How much can model manipulation attacks tell us about LLM vulnerabilities to unforeseen input-space attacks?** Answering this will help us understand how model manipulation attacks can help to assess risks from both open- and closed-source models.

Here, we test state-of-the-art capability removal ("unlearning") methods for LLMs against a suite of input-space and model manipulation attacks. We make four contributions.

1. **Benchmarking:** We benchmark 8 state-of-the-art unlearning methods and 10 capability elicitation attacks against each other.
2. **Science of robustness:** We show that LLM resilience to a variety of capability elicitation attacks lies on a low-dimensional robustness subspace.
3. **Evaluation methodology:** We show that the success of some model manipulation attacks correlates with that of held-out input-space attacks. We also show that few-shot LoRA fine-tuning attacks can typically be used to conservatively over-estimate a model's robustness to held-out input-space threats.

Figure 1: **Model manipulation attacks can modify latents and weights. In contrast to input-space attacks, model manipulation attacks elicit capabilities from an LLM by making small modifications to the hidden activations or weights.**

4. **Few-shot fine-tuning attacks:** We find that state-of-the-art unlearning methods can consistently be undone within 50 steps of few-shot fine-tuning.

Overall, our results suggest that model manipulation attacks can be a unique and valuable way for evaluators to gain more information about the potential worst-case behaviors of LLMs.

## 2 Related Work

**Latent-space attacks:** During a latent-space attack, an adversary can make modifications to a model's hidden activations. Prior work has found that adversarial training under these attacks can improve the generality of a model's robustness (Sankaranarayanan et al., 2018; Singh et al., 2019; Zhang et al., 2023; Schwinn et al., 2023; Zeng et al., 2024). In particular, Xhonneux et al. (2024), Casper et al. (2024b), and Sheshadri et al. (2024) use latent adversarial training to improve defenses against held-out types of adversarial attacks. Other recent work on activation engineering has involved making modifications to a model's behavior via simple transformations to their latent states (Zou et al., 2023; Wang and Shu, 2023; Lu and Rimsky, 2024) including Arditi et al. (2024) who showed that linear activation interventions could be used to jailbreak LLMs.

**Few-shot fine-tuning attacks:** During a few-shot fine-tuning attack, an adversary can modify a model's weights via fine-tuning on a limited number of examples. For example, Qi et al. (2023) showed that fine-tuning on as few as ten examples could be used to jailbreak a GPT-3.5. A variety of works have used few-shot fine-tuning attacks to elicit LLM capabilities that were previously suppressed by safety fine-tuning or machine unlearning (Jain et al., 2023; Yang et al., 2023; Qi et al., 2023; Bhardwaj and Poria, 2023; Lermen et al., 2023; Zhan et al., 2023; Ji et al., 2024; Qi et al., 2024; Hu et al., 2024; Halawi et al.; Peng et al., 2024; Lo et al., 2024; Lucki et al., 2024; Shumailov et al., 2024; Lynch et al., 2024; Deeb and Roger, 2024). For example, Greenblatt et al. (2024) found that fine-tuning was a reliable way of eliciting hidden capabilities from a "password locked" model that would only exhibit certain capabilities if a specific "password" was present in the prompt.

**Unlearning evaluation:** Along with research on jailbreaks (e.g., (Shayegani et al., 2023)) and backdoors (e.g., (Greenblatt et al., 2024)), machine unlearning has been a research sandbox for evaluating capability elicitation methods. "Machine unlearning" been motivated by two main objectives: undoing the influence of specific training examples and removing specific capabilities from models (Liu et al., 2024). Here, we focus on capability unlearning. Conventionally, capability unlearning has been evaluated using a pair of tests for the preservation of general knowledge and the reduction of unlearning-task knowledge. However, more recently, adversarial approaches have been used to evaluate robust and generalizable unlearning (Patil et al., 2023; Lynch et al., 2024; Lucki et al., 2024; Hu et al., 2024; Liu et al., 2024). Here, we build off of Li et al. (2024b) who introduce WMDP, a benchmark for unlearning dual-use biotechnology, chemistry, and cyber-offense knowledge from LLMs. We evaluate methods using the WMDP Bio unlearning and evaluation tasks.

## 3 Methods

We pit unlearning methods against capability elicitation attacks. We used each unlearning method to impair an LLM's ability to assist users in tasks involving dual-use biotechnology information while preserving its general capabilities (Li et al., 2024b). The goal of each capability elicitation attack is to produce an adversarial modification to model text inputs, input embeddings, latent activations, or weights to make it regain its ability to help users with dual-use biology tasks.

**Unlearning methods:** We unlearn dual-use bio-hazardous knowledge on the Llama 3 SB Instruct model Dubey et al. (2024) with the 8 unlearning methods listed in Table 1 and outlined in Appendix A.2.1. For all methods except TAR, we train on up to 1,600 examples of max length 512 from the bio-remove-split of WMDP as the forget set, and up to 1,600 examples of max length 512 from Wikitext as the retain set. Due to the computational cost of TAR, we attack the public release TAR model unlearned on WMDP Bio data. We also attack the public release of the "Random Mapping" method which is an ablated version of TAR without meta-learning. For all methods we trained, we evaluate 8 checkpoints evenly spaced across training on unlearning efficacy, utility preservation, and robustness to attacks. We evaluate unlearning efficacy on the WMDP Bio QA dataset.

**Capability elicitation attacks:** We use 5 input-space attacks and 5 model manipulation attacks on our unlearned models. These attacks are designed to increase WMDP Bio performance. We list all 10 attacks in Table 1. Note that in this setting, we use capability elicitation methods to produce _universal_ adversarial attacks which work for _all_ prompts. This allows us to attribute improvements in performance to capability elicitation rather than answer-forcing from the model. We selected attacks based on algorithmic diversity and prominence in the state of the art.

In addition to the attacks in Table 1, we also tried many-shot attacks (Anil et al., 2024; Lynch et al., 2024) and translation attacks (Yong et al., 2023; Lynch et al., 2024) but found them to be consistently unsuccessful. All attacks, except fine-tuning attacks, used a 64-example subset for attack generation (approximately 5% of the evaluation examples) and were evaluated on the remaining WMDP Bio QA data. For descriptions and implementation details for each attack method, see Appendix A.

## 4 Experiments

As discussed in Section 1, we have two motivations, each corresponding to a different threat model. First, we want to directly evaluate robustness to model manipulation attacks in order to better un

  
**Method** & **WMDP \(\)** & 
 **WMDP Under \(\)** \\ **Best Attack** \\  & **MMLU \(\)** & **MT-Bench/10 \(\)** & **AGIEval \(\)** & **Unlearning Score \(\)** \\  Llama3 BB Instruct & 0.70 & 0.71 & 0.64 & 0.78 & 0.41 & 0.00 \\  Grad Diff & 0.26 & 0.54 & 0.53 & 0.28 & 0.27 & 0.19 \\ RMU & 0.26 & 0.59 & 0.59 & 0.68 & 0.42 & 0.39 \\ RMU + LAT & 0.32 & 0.66 & 0.60 & 0.71 & 0.39 & 0.34 \\ RepNoise & 0.25 & 0.62 & 0.57 & 0.64 & 0.36 & 0.36 \\ ELM & 0.29 & 0.54 & 0.54 & 0.73 & 0.41 & 0.37 \\ RR & 0.26 & 0.65 & 0.61 & 0.76 & 0.44 & **0.44** \\ TAR & 0.24 & 0.43 & 0.27 & 0.10 & 0.30 & 0.08 \\ RandMap & 0.27 & 0.61 & 0.55 & 0.41 & 0.31 & 0.24 \\   

Table 2: **Benchmarking LLM unlearning methods:** We report original WMDP Bio performance, worst-case WMDP Bio performance after attack, and three measures of general utility: MMLU, MT-Bench, and AGIEval. For all benchmarks, the random-guess baseline is 0.25 except for MT-Bench/10 which is 0.1. Representation rerouting (RR) has the best unlearning score. No model has a WMDP accuracy less than 0.43 after attack.

  
**Method** & **WMDP \(\)** & 
 **WMDP Under \(\)** \\ **Best Attack** \\  & **MMLU \(\)** & **MT-Bench/10 \(\)** & **AGIEval \(\)** & **Unlearning Score \(\)** \\  Llama3 BB Instruct & 0.70 & 0.71 & 0.64 & 0.78 & 0.41 & 0.00 \\  Grad Diff & 0.26 & 0.54 & 0.53 & 0.28 & 0.27 & 0.19 \\ RMU & 0.26 & 0.59 & 0.59 & 0.68 & 0.42 & 0.39 \\ RMU + LAT & 0.32 & 0.66 & 0.60 & 0.71 & 0.39 & 0.34 \\ RepNoise & 0.25 & 0.62 & 0.57 & 0.64 & 0.36 & 0.36 \\ ELM & 0.29 & 0.54 & 0.54 & 0.73 & 0.41 & 0.37 \\ RR & 0.26 & 0.65 & 0.61 & 0.76 & 0.44 & **0.44** \\ TAR & 0.24 & 0.43 & 0.27 & 0.10 & 0.30 & 0.08 \\ RandMap & 0.27 & 0.61 & 0.55 & 0.41 & 0.31 & 0.24 \\   

Table 1: **Table of attacks and defenses.** The capability elicitation (attack) methods and unlearning (defense) methods we use to remove and extract bio-hazardous capability from LLMs.

[MISSING_PAGE_FAIL:5]

### Model manipulation attacks help to predict and upper bound the success of input-space attacks

**Comparing the success of input-space and model manipulation attacks:** In Figure 3, we plot the increases in WMDP Bio performance from input space versus model manipulation attacks. In the plots, we size points by their unlearning score and display the weighted pearson correlation.

**Benign fine-tuning, embedding-space, and latent-space attack successes tend to _correlate_ with input-space attack successes._** These three model manipulation attacks tend to have positive correlations with input-space attack successes with low \(p\) values (e.g., \(<0.05\)). This suggests that benign fine-tuning embedding-space and latent-space attacks are particularly able to predict the successes of held-out input-space attacks. Surprisingly, full model fine-tuning attacks tend to negatively predict input-space attacks, though this trend is largely due to the ELM models. ELM models often cluster far away from all others, suggesting that ELM may be a mechanistically unique unlearning technique, but we leave further exploring this for future work.

**LoRA fine-tuning attack successes tend to _upper bound_ input-space attack successes for models that do not experience significant utility degradation._** LoRA fine-tuning on the WMDP Bio forget set was usually able to exceed the success of all other attacks we tested. In the few cases in which the success of LoRA attacks does not exceed another attack, it tends to be for models (e.g., TAR) with low unlearning scores due to general utility degradation. This suggests that they may not have been robust to fine-tuning because of successful unlearning so much as having behavior that was unstable under training. This suggests that evaluators looking to upper bound the worst-case behavior of a model under potentially unforeseen threats may be able to use these fine-tuning attacks.

**Unlearning can consistently be reversed within 50 fine-tuning steps - sometimes even in a single step.** We show the results of multiple fine-tuning attack configurations against the best-performing model from each unlearning method in Figure 5. All finetuning experiments as detailed in Appendix A.3 are done within 50 gradients steps, and with 400 examples or fewer. In particular, the attack 'Full-4' only performs a single gradient step (with a batch size of 64) and still increases the WMDP performance on 5 of the 8 models by over 25 percentage points. The only models that were resistant to few-shot fine-tuning attacks were from GradDiff and TAR. However, GradDiff was still compromised by the Benign LoRA attack within just 50 gradient steps. Both of TAR and GradDiff models had low unlearning scores due to poor general utility.

## 5 Discussion

**Implications for evaluations:** Our work may be of particular interest to the LLM evaluation community. Frameworks for AI governance are increasingly designed to rely on high-quality evaluations to identify hazards and handle risks in LLMs (Raji et al., 2022; Anderljung et al., 2023; Schuett et al., 2023; Shevlane et al., 2023; Uuk et al., 2024). Formal evaluations of AI systems, however, face a

Figure 2: **Four principal components explain over 90% of the variation in performance across the ten attacks.** (Left) The proportion of explained variance for each principal component. (Right) We display the first four principal components weighted by their eigenvalues. The first suggests a split between the two adversarial (LoRA, Full) fine-tuning attacks and all others.

number of challenges (Anderljung et al., 2023) including limitations in technical tooling. Our work adds to the growing consensus that access to model internals is necessary for rigorous evaluations (Casper et al., 2024a). When models are released with open weights, model manipulation attacks during evaluations are directly necessary to understand risks from misuse. Meanwhile, when they are released with closed weights, our results still suggest that model manipulation attacks can help evaluators make more informed inferences about potential worst-case behavior.

**Limitations:** Our work focuses only on Llama 3 SB Instruct derived models, on the task of unlearning hazardous biology knowledge. This allows for considerable experimental depth, but we expect that there may be different dynamics with different models performing different tasks. Another limitation is that our evaluations of undesirable capabilities were conducted only by evaluating models under multiple-choice questions from the WMDP Bio test set (Li et al., 2024b). However, these evaluations can sometimes be brittle. Finally, the science of evaluations is still generally immature, and it is not yet clear how to best translate the outcome of evaluations like ours into actionable recommendations.

**Future work:**

Figure 3: **Benign fine-tuning, embedding-space, and latent-space attack successes tend to _correlate_ with input-space attack successes. LoRA fine-tuning attack successes tend to _upper bound_ the successes of input space attacks for models with high unlearning scores._ Here, we plot the increases on WMDP Bio performance from input-space attacks versus model manipulation attacks. We weight points by their unlearning score from Section 4.1. We also display the unlearning-score-weighted correlation, the correlation’s \(p\) value, and the line \(y=x\) (Notice this is not the line of best fit). Points below and to the right of the line indicate that the model manipulation attack was more successful. For models with a high unlearning score, LoRA attacks consistently match or exceed the performance of all input-space attacks.**

* **Models and tasks:** Experiments with larger models and additional tasks would offer a broader understanding of model manipulation attacks' potential to aid in capability evaluations. We are particularly interested in similar work involving jailbreaks.
* **What mechanisms underlie robust capability removal?** Currently, our understanding of robust unlearning is limited. It is not known what types of mechanisms are responsible for strong versus weak unlearning. We are interested in future work to mechanistically characterize unlearning. This could help in understanding how to unlearn undesirable capabilities in a way that is effective, robust, and lacks side effects.
* **Bridging research and practice:** This work was motivated by the goal of helping evaluators more effectively study potential worst-case risks from harmful capabilities in LLMs. We hope that model manipulation attacks can be further studied and used in practice to assess risks in consequential models pre-deployment.

**Conclusion:** By comprehensively benchmarking 8 capability removal methods under 10 diverse capability elicitation methods, we have contributed to a more thorough understanding of LLM robustness and how to evaluate it. Not all capability elicitation attacks exploit the same weaknesses in models, nor do they all exploit the exact same ones. We have found that over 90% of the empirical variance in robustness across our suite of attacks can be explained by four principle components. Meanwhile, model manipulation attacks that make small perturbations to the LLM's weights or activations can help to predict its vulnerability to input-space methods. In particular, we show that few-shot fine-tuning attacks tend to be very strong and often exceed the successes of input-space attacks. Together these results suggest that model manipulation attacks can be uniquely helpful for inferring risks from unforeseen types of input space attacks.

Figure 4: **All unlearning methods could be successfully attacked, but not all attacks were successful on all models.** Here, we benchmark attacks against the best models from each unlearning method based on unlearning score. No method is robust to all the 10 attacks. The min-max WMDP performance increase under an attack was 19% (Human Prompt vs. TAR).

Figure 5: **Fine-tuning undoes unlearning efficiently.** We plot the heatmap of the best checkpoint for each method under fine-tuning attack. All fine-tuning experiments are done within 50 gradient steps, with 400 examples or less. All hyper-parameters are listed in Appendix A.3.