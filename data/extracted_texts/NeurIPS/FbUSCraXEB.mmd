# Efficient Availability Attacks against Supervised and Contrastive Learning Simultaneously

Yihan Wang, Yifan Zhu, Xiao-Shan Gao

Academy of Mathematics and Systems Science, Chinese Academy of Sciences

University of Chinese Academy of Sciences

{yihanwang, zhuyifan}@amss.ac.cn, xgao@mmrc.iss.ac.cn

Corresponding author. Kaiyuan International Mathematical Sciences Institute

###### Abstract

Availability attacks provide a tool to prevent the unauthorized use of private data and commercial datasets by generating imperceptible noise and crafting unlearnable examples before release. Ideally, the obtained unlearnability can prevent algorithms from training usable models. When supervised learning (SL) algorithms have failed, a malicious data collector possibly resorts to contrastive learning (CL) algorithms to bypass the protection. Through evaluation, we have found that most existing methods are unable to achieve both supervised and contrastive unlearnability, which poses risks to data protection by availability attacks. Different from recent methods based on contrastive learning, we employ contrastive-like data augmentations in supervised learning frameworks to obtain attacks effective for both SL and CL. Our proposed AUE and AAP attacks achieve state-of-the-art worst-case unlearnability across SL and CL algorithms with less computation consumption, showcasing prospects in real-world applications. The code is available at https://github.com/EhanW/AUE-AAP.

## 1 Introduction

Availability attacks  add imperceptible perturbations to training data, making the subsequently trained model unavailable. The motivations behind this kind of data poisoning attack involve protecting private data and commercial datasets from unauthorized use . For example, according to a report , a tech company illegally obtained over 3B facial images as the training set to develop a commercial facial recognition model. In this type of scenario, availability attacks provide tools to process user images before release, preserving legibility but impeding subsequent training. In particular, Huang et al.  reduces the accuracy of face recognition of 50 identities in WebFace  from 86% to 16%. In recent years, various availability attacks against supervised learning (SL)  have been proposed.

Meanwhile, contrastive learning (CL) allows people to extract meaningful features from unlabeled data in a self-supervised way. After subsequent linear probing or fine-tuning, CL algorithms have achieved comparable accuracy or even surpassed the performance of SL . However, most attacks designed for poisoning SL are ineffective against CL, as shown in Figure 1. It sheds light on a potential issue of using availability attacks to protect data: a malicious data collector can traverse both supervised and contrastive algorithms to effectively leverage collected data. Hence, we introduce _worst-case unlearnability_ (see Section 3.1) as the evaluation metric for availability attacks to emphasize the demand to deal with a trickier unauthorized data

Figure 1: Attacks against SL and CL on CIFAR-10.

collector. In recent years, contrastive error minimization attack is proposed to poison contrastive learning , and then label-dependent components are incorporated into it to simultaneously achieve supervised unlearnability besides contrastive unlearnability . However, compared to SL-based ones, these CL-based attacks lack efficiency in poisoning generation, potentially hindering availability attacks from protecting extensive data in the real world (see Section 5.3). Therefore, an effective as well as efficient availability attack against both SL and CL is imminent. Specifically, our motivation for this paper comes from two aspects: 1) _A fully functional availability attack needs to be effective against subsequent supervised and contrastive learning algorithms simultaneously._ 2) _Attacks based on supervised learning can be superior in efficiency compared to those based on contrastive learning._

To design a non-CL-based attack that possesses both supervised and contrastive unlearnability, we start from an interesting observation that supervised training with contrastive data augmentations mimics contrastive training to some extent (see Section 4.1). As shown in Figure 2, this technique of enhancing data augmentations can be easily embodied in two basic supervised attack frameworks, i.e., error-minimization, and error-maximization, resulting in our proposed AUE and AAP attacks (see Sections 4.2 and 4.3). Enhanced augmentations allow us to craft perturbations for a contrastive-like reference model. These perturbations implicitly adapt to the contrastive training process and then learn deceptive patterns that fool contrastive learning. The supervised unlearnability is still preserved since the generation process is based on supervised optimization.

On experimental side, we evaluate the standard supervised learning algorithm and four representative contrastive learning algorithms, SimCLR , MoCo , BYOL  and SimSiam . Our proposed AUE and AAP attacks achieve the state-of-the-art worst-case unlearnability on CIFAR-10/100 and Tiny/Mini-ImageNet datasets (see Section 5.2). Specifically, our method exhibits excellent performance on the ImageNet-100 as presented in Figure 3, showcasing its prospects in real-world applications. Meanwhile, unlike methods that add additional components to the contrastive error-minimization framework, we modify the data augmentation in the simpler supervised attack frameworks, following a minimal approach to algorithm design. Benefiting from this, our methods are more efficient, while delivering better performance. We summarize our contributions as follows:

* We evaluate existing availability attacks and point out the potential security risks of using them to protect data when facing data abusers who will traverse both supervised and contrastive learning algorithms.
* We start from supervised poisoning approaches and enhance data augmentations to attain attacks against both supervised and contrastive learning.

Figure 3: Attack performance of our methods on ImageNet-100.

Figure 2: Illustration of our proposed method. Separated by a vertical dashed line, the left side shows the process of generating the poisoning attack, while the right side depicts the training process on the poisoned dataset. On the generation side, above the horizontal dashed line are the existing methods based on contrastive error minimization, while below the dashed line are our proposed methods based on supervised error minimization/maximization (the blue flow). Our attack leverages the stronger contrastive augmentations to obtain effectiveness against both supervised learning and contrastive learning algorithms. Label information is involved in both our method and CL-based methods.

* Our attacks achieve state-of-the-art worst-case unlearnability with less computation consumption and are more adept at handling high-resolution datasets.

## 2 Background

We will introduce some notions of contrastive learning and availability attacks. Besides, we provide more preliminaries on contrastive learning in Appendix B.

### Contrastive Learning

Contrastive learning trains feature encoders in a self-supervised way. It first transforms an image into two views, i.e., a positive pair, using augmentations sampled from a strong augmentation distribution \(\). Two views augmented from different images constitute a negative pair. Extracted features are trained to be aligned between positive pairs but distinct between negative pairs. It does not require label information until downstream tasks such as linear probing or fine-tuning. Wang and Isola  introduced two key properties for contrastive learning, _alignment_ and _uniformity_. The former measures the similarity of features from positive pairs and the latter reflects the uniformity of feature distribution on the hypersphere. Let \(g\) be a normalized encoder. The _alignment loss_ and _uniformity loss_ on a dataset \(_{c}\) are defined as the following:

\[(_{c})=}_{ _{c}\\ ,}|g(())-g(())| _{2}^{2};(_{c})=}_{,_{c}\\ ,}e^{-2||g(())-g(())||_{ 2}^{2}}.\]

Let \(^{}_{c}\) be a poisoned version of a clean dataset \(_{c}\). The _alignment gap_ and _uniformity gap_ between clean and poisoned datasets are defined as follows:

\[=(_{c})-(^{}_{c }),=(_{c})-(^{ }_{c}).\] (1)

Intuitively, these gaps characterize the difference between clean features and poisoned features. We will check the relationship between these gaps and contrastive unlearnability in Section 3.2.

### Basic Availability Attacks

The essence of availability attacks is to prevent a trained model from well generalizing to clean data. We will revisit two representative approaches to poisoning supervised learning.

**Error minimization.** Unlearnable example attack (UE) generates poisoning by alternately optimizing the reference model and perturbations :

\[_{}_{f}}_{_{c}}_{ }(+(,y),y;f),\] (2)

where \(f\) is a classifier, \(_{}(,;)\) is the supervised loss, \(_{c}\) is a dataset to be processed and \(\) is a poisoning map.

**Error maximization.** Adversarial poisoning (AP) optimizes perturbations through a pre-trained classifier to equip them with non-robust but useful features from a different label :

\[_{}}_{_{c}}_{}(+(,y),y+K;f^{*}),\] (3) s.t. \[f^{*}_{f}}_{_{c}} _{}(,y;f),\]

where \(K\) is the label shift. Self-ensemble protection (SEP) generates adversarial poisons using several checkpoints to improve attack performance .

**Contrastive error minimization.** Contrastive poisoning (CP) extends the error minimization framework to contrastive error minimization to poison contrastive learning :

\[_{}_{g}}_{_{c}}_{ }(+(,y);g),\] (4)

where \(g\) is an encoder and \(_{}(;)\) denotes the contrastive loss for simplicity. Later, the transferable unlearnable example attack (TUE) introduces a regularization term called class-wise separability discriminant to equip CP noises with supervised unlearnability . Then, transferable poisoning (TP) combines contrastive error minimization with supervised adversarial poisoning to obtain both supervised and contrastive unlearnability . It is worth mentioning that both TUE and TP leverage label information in their proposed schemes, while CP requires no label information but lacks stable effect on supervised learning.

## 3 Threat Model

In our threat model, an unauthorized data collector assembles labeled data into a dataset. The access to label information is reasonable since the collector can crawl individual images from certain accounts or steal (and annotate) a commercial dataset. A data publisher is supposed to process data before release using an availability attack such that processed data is resilient to subsequent supervised learning algorithms as well as contrastive learning algorithms adopted by the data collector. We will define worst-case unlearnability and discuss the contrastive unlearnability of existing attacks.

### Worst-Case Unlearnability

Suppose an unprocessed dataset \(_{c}\) is _i.i.d_ sampled from a data distribution \(\). An availability attack \(\) maps a data-label pair \((,y)_{c}\) to a noise \((,y)\) within an \(L_{p}\)-norm ball \(_{p}()\). In this paper, we set \(p=\) and \(=8/255\). It results in a protected dataset \(^{}_{c}=\{(+(,y),y)|(,y)_{c}\}\) to which a data collector has only access. For potential algorithms, we refer \(f\) to a supervised learning classifier and \(g\) to a contrastive learning encoder beyond which is a linear probing head \(h\). The goal of the data publisher is to find a poisoning map \(\) that significantly degrades the generalization performance of both \(f_{}\) and \(h_{} g_{}\) which are trained on \(^{}_{c}\). We define the _worst-case unlearnability across supervised and contrastive learning algorithms_ of the following form:

\[_{} }{}(f_{ }()=y),}{}(h_{} g_{}()=y)\] (5) s.t. \[f_{}_{f}_{c}}{} _{}(+(,y),y;f),\] \[g_{}_{g}_{c}}{} _{}(+(,y);g),\] \[h_{}_{h}_{c}}{} _{}(+(,y),y;h g_{}) .\]

It is a fair metric that accurately depicts scenarios facing more running data abusers in the real world. In contrast, other metrics, such as average-case unlearnability, can be heavily influenced by the attack's strong preference for a certain algorithm. Our threat model differs from the setting adopted by He et al.  in which the linear probing stage relies on the unprocessed clean data as downstream tasks; see more discussion in Appendix D.10.

### Existing Attacks against Contrastive Learning

In Table 1, we evaluate the attack performance of existing poisoning approaches against the SimCLR algorithm on CIFAR-10 and ResNet-18. To better understand contrastive unlearnability, we also check alignment and uniformity gaps between clean and poisoned data defined in Equation (1). In non-CL-based poisoning attacks, except for AP and SEP, all other methods fail to deceive the contrastive learning algorithm. The alignment and uniformity gaps of AP and SEP attacks are prominently larger than those of others. CL-based attacks including CP, TUE, and TP are effective against contrastive learning and possess huge alignment and uniformity gaps.

The Pearson correlation coefficient (PCC) between the alignment gap and SimCLR accuracy is \(-0.82\), and the PCC between the uniformity gap and SimCLR accuracy is \(-0.88\). It reveals that contrastive unlearnability is highly related to huge alignment and uniformity gaps. When the encoder is fixed after poisoned contrastive training, a linear layer learns to classify poisoned features, i.e., features of poisoned (training) data. Note that evaluation is to classify clean features, i.e., features of clean (test) data.

  
**Attacks** & \(\) & \(\) & **Test Acc.** \\  DC  & 0.12 & 0.07 & 86.1 \\ UE  & 0.05 & 0.03 & 89.0 \\ AR  & 0.07 & 0.09 & 88.8 \\ NTGA  & 0.12 & 0.12 & 86.9 \\ SN  & 0.08 & 0.00 & 90.6 \\ OPS  & 0.04 & 0.01 & 86.7 \\ GUE  & 0.07 & 0.03 & 88.8 \\ REM  & 0.12 & 0.04 & 88.6 \\ EntF  & 0.01 & -0.04 & 87.5 \\ HYPO  & 0.11 & 0.13 & 86.9 \\ AP  & **0.18** & **0.44** & **48.4** \\ SEP  & **0.24** & **0.25** & **37.3** \\  CP  & **0.55** & **0.87** & **38.7** \\ TUE  & **0.30** & **0.76** & **48.1** \\ TP  & **0.52** & **0.82** & **31.4** \\   

Table 1: Alignment gap, uniformity gap, and test accuracy(%) of poisoned SimCLR  models. Attacks are grouped according to whether they are based on contrastive error minimization. **Bold** fonts emphasize prominent contrastive unlearnability values.

Prominent gaps indicate a significant difference between clean and poisoned feature distributions. Thus, no matter how well the classifier performs on poisoned features, it hardly generalizes to clean features and the attack is successful. In contrast, small gaps likely imply that clean features are similar to poisoned features. When gaps are small, the test accuracy is high and the attack fails.

## 4 Method

Since contrastive loss is related to alignment and uniformity , the contrastive error minimization (CP) attack optimizes the loss directly and obtains contrastive unlearnability. Beyond this, TUE and TP incorporate additional label-dependent components to obtain supervised unlearnability. However, optimizing contrastive loss is very time and memory-consuming, impeding their applications in real-world scenarios. Different from them, we start from more efficient supervised poisoning frameworks instead to achieve both supervised unlearnability and contrastive unlearnability simultaneously. The key point to get there is data augmentation. In the rest of this section, we first illustrate how contrastive learning data augmentations help mimic contrastive learning with supervised models through empirical observations and intuition from a toy example. In other words, enhancing data augmentation helps supervised learning implicitly optimize the contrastive loss. Then we combine this very effective technique with supervised error minimization and maximization frameworks and propose _augmented unlearnable examples (AUE) attacks_ and _augmented adversarial poisoning (AAP) attacks_.

### Mimic Contrastive Learning with Supervised Models

Contrastive learning employs strong data augmentations including _resized crop, color jitter, horizontal flip, and grayscale_[53; 18], while supervised learning adopts mild data augmentations such as horizontal flip and crop. In Appendix C.2, Code 1 shows detailed implementations for these two different settings. Naturally, contrastive error minimization uses stronger data augmentation compared to supervised error minimization. However, what if we use strong contrastive augmentations when optimizing supervised losses? On CIFAR-10, we train a supervised ResNet-18 classifier using contrastive augmentations. At each checkpoint, we log the supervised cross-entropy (CE) loss and the contrastive InfoNCE loss  on the training set. In Figure 4, when the optimization object CE loss goes down, the InfoNCE loss decreases as well. It indicates that training a supervised model with contrastive augmentations implicitly optimizes the contrastive loss. In other words, supervised error minimization mimics contrastive error minimization to some extent. Therefore, incorporating stronger data augmentation potentially enables availability attacks based on supervised error minimization or maximization to deceive contrastive learning.

To provide more intuition about this idea, we give a toy example and have a closer look at the relationship between supervised loss and contrastive loss. For a supervised model \(f=h g\), assume \(g\) is a normalized feature extractor, \(h\) is a square full-rank linear classifier, \(\) is a balanced distribution, \(_{}\) is MSE loss, training error \(_{}=[_{}]\), and \(_{}\) contains only one negative example. In this toy example, if \(_{}\) and \(_{}\) employ the same data augmentation and \(f\) is well-trained, it holds with high probability that \(_{}<l(_{})\), where \(l()\) is an increasing function. In other words, the upper bound of contrastive loss decreases as the supervised loss decreases. We have a more detailed and formal discussion on this toy example in Appendix E.

Based on these interesting observations, instead of adding components to contrastive error minimization to achieve supervised unlearnability, we opt for deriving stronger contrastive unlearnability from supervised error minimization and maximization.

### Augmented Unlearnable Examples (AUE)

Recall unlearnable examples (UE) are generated by supervised error minimization which alternately updates a reference model and noises in Equation (2). Now we employ contrastive-like strong data augmentation distribution \(\) and add perturbations in a differentiable way, i.e., \((+(,y)),\). As discussed in the previous section, minimizing the augmented supervised loss \(_{}((+(,y),y;f)\)

Figure 4: InfoNCE loss decreases with CE loss.

implicitly minimizes the contrastive loss \(_{}(+(,y);g)\) which appears in contrastive error minimization, i.e., Equation (4). In other words, supervised error-minimizing noises with enhanced data augmentation can partially replace the functionality of contrastive error-minimizing noises to deceive contrastive learning.

We can control the intensity of contrastive augmentations in Code 1 via a strength parameter \(s\). We increase the augmentation strength and generate the poisoning attack using Algorithm 1. Implementation details are shown in Appendix C.3. In Table 2, while UE attacks do not work for SimCLR on CIFAR-10 and CIFAR-100, our AUE attacks successfully reduce the SimCLR accuracy by \(38.9\%\) and \(50.3\%\). Enhanced data augmentation indeed makes supervised error-minimizing noises effective for contrastive learning. In Figure 4(a), AUE noises largely reduce the contrastive loss during SimCLR training compared to UE noises. In Figure 4(b), we investigate the alignment and uniformity gaps and discuss more about the poisoned training process in Appendix D.2. The final gaps of AUE are \(=0.27,=0.34\) while those of UE are \(=0.05,=0.03\).

### Augmented Adversarial Poisoning (AAP)

Adversarial poisoning (AP) attacks in Equation (3) first train a supervised reference model, then generate its adversarial examples. When replacing mild supervised augmentations with stronger contrastive ones, the training process of the reference model, i.e., minimizing \(_{}((),y;f),\) concerning \(f\) mimics updating its encoder with contrastive learning. The final reference model \(f^{*}\) has a contrastive-like encoder. Then, generating perturbations via minimizing \(_{}((+(,y)),y+K;f^{*})\) with respect to \(\) is to deceive the contrastive-like model. Consequently, the resulting poisoning attack learns more about confounding contrastive learning algorithms.

According to Algorithm 2, we increase the augmentation strength \(s\) in both reference model pretraining and noise update where the label translation \(K=1\). Implementation details are shown in Appendix C.3. In Table 2, the AAP attack further enlarges the SimCLR accuracy drop of AP by \(9.3\%\) on CIFAR-10 and \(5.5\%\) on CIFAR-100. Enhanced data augmentations indeed improve the contrastive unlearnability of supervised error-maximizing noises.

  
**Datasets** & **Clean** & **UE** & **AUE** & **AP** & **AAP** \\  CIFAR-10 & 91.3 & -2.3 & -38.9 & -42.9 & -52.2 \\ CIFAR-100 & 63.9 & -3.9 & -50.3 & -38.3 & -43.8 \\   

Table 2: Accuracy drop(%) of SimCLR caused by basic attacks and our methods.

Figure 5: (a) Contrastive losses during SimCLR training under UE and AUE attacks. (b) Alignment and uniformity gaps during the SimCLR training on CIFAR-10 poisoned by our AUE attack.

## 5 Experiments

We will evaluate the worst-case unlearnability of our proposed AUE and AAP attacks on multiple datasets and compare the poisoning generation efficiency with other methods. Besides, we will check the efficacy of our method against more evaluation algorithms and the transferability across network architectures. Then we will perform an ablation study of decoupling argumentation components in our method.

### Setup

We conduct experiments on CIFAR-10/100 , Tiny-ImageNet , modified Mini-ImageNet , and ImageNet-100 . ResNet-18  is used for poison generation and evaluation if not otherwise stated. Our threat model considers the worst-case unlearnability across supervised and contrastive (self-supervised) algorithms including standard SL, SimCLR, MoCo, BYOL, and SimSiam. We implement linear probing on the encoder to evaluate contrastive unlearnability.

We adopt AP, SEP-FA-VR, CP, TUE, and TP as baseline methods for the worst-case unlearnability. Since the generation of untargeted adversarial poisoning is unstable , AP and AAP attacks are targeted if not otherwise stated (see more discussion in Appendix D.5). In particular, only CIFAR-10 results in Table 3 report untargeted AP and AAP. For CP and TUE attacks, we report the best results across the CL algorithms they depend on (see additional results in Appendix D.6). Detailed settings for attack implementation and evaluation are shown in Appendix C.

### Attack Performance

**Worst-case unlearnability.** In Table 3, our AAP attack achieves the best worst-case unlearnability on CIFAR-10 and both AAP and AUE attacks outperform other methods on CIFAR-100. Particularly, AAP improves the performance by 8.5%/7.5% on CIFAR-10/100 and AUE becomes better than AAP on CIFAR-100. In other methods, CP loses supervised unlearnability on CIFAR-100 and TUE is better than TP on both datasets. Furthermore, we evaluate attacks on higher-resolution datasets including Tiny-ImageNet (64x64) and Mini-ImageNet (84x84) in Table 4. On both datasets, AUE outperforms other methods in terms of the worst-case unlearnability. Besides, AUE also achieves the best supervised unlearnability. For AAP, its worst-case unlearnability is bett

    &  &  \\  & **SL** & **SimCLR** & **MoCo** & **BYOL** & **SimSiam** & **Worst** & **SL** & **SimCLR** & **MoCo** & **BYOL** & **SimSiam** & **Worst** \\  None & 95.5 & 91.3 & 91.5 & 92.3 & 90.7 & 95.5 & 77.4 & 63.9 & 67.9 & 63.7 & 64.4 & 77.4 \\  AP & 9.6 & 41.5 & 31.5 & 44.0 & 42.8 & 44.0 & 3.2 & 25.6 & 26.6 & 26.1 & 28.8 & 28.8 \\ SEP & 2.3 & 37.3 & 35.8 & 42.8 & 36.7 & 42.8 & 2.4 & 25.2 & 25.9 & 26.6 & 28.4 & 28.4 \\ CP & 11.0 & 39.3 & 32.7 & 41.8 & 37.9 & 41.8 & 74.4 & 15.2 & 13.4 & 16.4 & 14.1 & 74.4 \\ TUE & 10.1 & 57.2 & 51.6 & 60.1 & 58.5 & 60.1 & 1.0 & 19.9 & 19.6 & 22.3 & 18.6 & 22.3 \\ TP & 14.8 & 31.4 & 54.1 & 61.8 & 30.7 & 61.8 & 7.5 & 6.7 & 21.9 & 27.0 & 4.1 & 27.0 \\ AAP & 29.7 & 32.3 & 23.2 & 35.5 & 34.1 & **35.5** & 7.3 & 20.1 & 18.6 & 21.1 & 21.3 & 21.3 \\ AUE & 18.9 & 52.4 & 57.0 & 58.2 & 34.5 & 58.6 & 6.9 & 13.6 & 19.0 & 19.2 & 11.9 & **19.2** \\   

Table 3: Attack Performance (%) on CIFAR-10 and CIFAR-100. The lower the value, the better the unlearnability.

good as TUE. Moreover, compared to AP, AAP suffers a trade-off between supervised unlearnability and contrastive unlearnability.

**Comparison between AUE and AAP.** On simpler datasets (low resolution, few classes), AAP has an advantage over AUE. However, on more complex datasets (high resolution, many classes), AUE outperforms AAP. One possible reason for this could be that optimizing AAP is more challenging. Firstly, generating adversarial poisoning inherently depends on a well-performing reference classifier. For instance, Fowl et al. 13 uses a pre-trained ImageNet model, whereas in this paper we train classifiers from scratch. Additionally, stronger data augmentation used in the reference model training can affect its accuracy, which in turn impacts the quality of the generated adversarial perturbations. Enhancing the performance of AAP is an interesting direction for future work.

**Algorithm transferability.** CL-based methods face the issue of transferability from the generation CL algorithm and the evaluation CL algorithm. For example, TP is generated using the SimCLR, which performs well against SimCLR evaluation, but its performance sharply declines when tested with BYOL. The same phenomenon also occurs with TUE and CP and their worst-case unlearnability is highly dependent on the appropriate generation algorithm, which you can check in Appendix D.6. In contrast, our SL-based attacks get rid of this issue because their poisoning generation involves no CL algorithms.

### Efficiency of Poisoning Generation

In real-world scenarios, availability attacks need to generate perturbations for accumulating data as quickly as possible. For expanding datasets, like continually updated social media user data, the poisoning used for data protection also needs to be updated periodically. Since contrastive learning involves larger batches (e.g., 512) and a longer training process (e.g., 1000 epochs), these contrastive error minimization-based attacks require more time and memory consumption to generate perturbations.

In Figure 6, we report the time cost of poisoning CIFAR-10/100 using the same device. Baseline methods adopt their default configurations. Our supervised learning-based approaches are 3x, 6x, and 17x faster than TUE, CP, and TP. Additionally, our methods admit smaller batches and simpler cross-entropy loss which require less memory, allowing for the generation of availability attacks on larger datasets with fewer devices. Refer to Appendix D.1 for more results about the efficiency of our methods. Overall, our method is more promising than CL-based methods due to the time and memory efficiency in real-world applications.

### More Evaluation Algorithms

Besides linear probing, we also apply the k-nearest neighbors (k-NN) algorithm to the feature space to evaluate the contrastive unlearnability. In Table 5, both AUE and AAP prominently reduce the

  
**Attacks** &  &  \\ \(k\)-**NN** & **SupCL** & **FixMatch** & \(k\)-**NN** & **SupCL** & **FixMatch** \\  Clean & 88.9 & 94.6 & 95.7 & 55.2 & 72.5 & 77.0 \\ AUE & 54.4 & 31.5 & 30.0 & **13.3** & **15.6** & **12.0** \\ AAP & **42.6** & **24.7** & **18.7** & 21.7 & 17.9 & 25.5 \\   

Table 5: Attack performance (%) against SimCLR Table 6: Architecture transferability on CIFAR-10. Evaluation includes SL and SimCLR.

    &  &  &  &  \\  & & & **MoCo** & **BYOL** & **SimSiam** & **Worst** & **SL** & **SimCLR** & **MoCo** & **BYOL** & **SimSiam** & **Worst** \\  None & 66.2 & 55.3 & 57.6 & 48.7 & 54.5 & 66.2 & 53.5 & 39.6 & 43.3 & 33.9 & 42.4 & 53.5 \\  AP & 11.5 & 48.9 & 50.1 & 44.0 & 48.5 & 50.1 & 11.3 & 32.8 & 34.7 & 27.2 & 34.5 & 34.7 \\ TUE & 20.7 & 20.6 & 21.1 & 20.8 & 21.2 & 21.2 & 8.5 & 13.3 & 15.9 & 13.4 & 14.1 & 15.9 \\ AUE & 8.7 & 15.0 & 20.4 & 14.5 & 18.2 & **20.4** & 7.1 & 10.8 & 11.7 & 9.6 & 11.6 & **11.7** \\ AAP & 24.0 & 43.8 & 41.9 & 40.2 & 41.8 & 43.8 & 18.7 & 28.4 & 27.6 & 25.2 & 28.2 & 28.4 \\   

Table 4: Attack Performance (%) on Mini-ImageNet and Tiny-ImageNet. The lower the value, the better the unlearnability.

k-NN accuracy. It indicates that the features of poisoned training inputs largely differ from those of clean test inputs. Non-robust features in imperceptible perturbations heavily affect the encoder's behavior and hinder its generalization ability.

In addition to supervised learning and contrastive learning algorithms, we consider two more CL-like algorithms including supervised contrastive learning, i.e., SupCL  and a semi-supervised learning algorithm FixMatch . FixMatch uses WideResNet  and detailed settings are in Appendix C.5. Table 5 demonstrates that our attacks are still effective against SupCL and FixMatch. It indicates that our methods can handle more variants derived from supervised learning and contrastive learning algorithms.

### Transferability across Networks

Since the data protector is unaware of networks used in future training, availability attacks should be effective for different architectures. We generate AUE and AAP using ResNet-18 and test them on ResNet-50, VGG-19 , DenseNet-121 , MobileNet v2 [20; 40], and ViT . In Table 6, both supervised and contrastive unlearnability of AUE and AAP can transfer across these architectures.

### Ablation Study of Decoupling Augmentations

In settings of AUE and AAP, we control the strength of ResizedCrop, ColorJitter, and Grayscale through a single strength hyperparameter \(s\) for the poison generation, as shown in Code 1. In Table 7, we decouple the strength hyperparameters for these three random transforms and evaluate the resulting attacks against SimCLR. Different factors show different influences on the contrastive unlearnability for AUE and AAP. For example, enhancing ResizedCrop strength alone is less effective than enhancing Grayscale alone in AUE generation. However, adjusting three factors together generally outperforms other options in conclusion.

## 6 Related Works

When generating availability attacks, the gradient of perturbations is often computed through data augmentations. In literature, SL-based attacks generally use mild supervised data augmentation, i.e., RandomCrop and RandomHorizontalFlip . The expectation over transformation (EOT) technique  adopted by Fu et al.  first samples several such mild augmentations and then computes the average gradient over them. Note that our proposed method is not a variant of EOT. CL-based attacks use contrastive augmentations [16; 36; 29]. To our knowledge, we are the first to use contrastive-like strong data augmentations in SL-based poisoning frameworks. Besides, we provide additional related works on availability attacks in Appendix A.

## 7 Conclusion

Since contrastive learning algorithms bring new challenges to protect data using availability attacks, we explore effective attacks against both supervised and contrastive learning. We introduce a very effective modification of data augmentation in supervised poisoning frameworks and propose attacks achieving superior performance and efficiency compared to existing methods, offering more potential in real-world applications. Considering availability attacks still face obstacles such as adversarial training mitigation and poisoning ratio sensitivity, addressing these challenges while maintaining both supervised and contrastive unlearnability will be an important direction for our future research.

  
**Attacks** & \(0\)-\(0\)-\(0\) & \(0\)-\(s\) & \(0\)-\(s\)-\(0\) & \(s\)-\(0\)-\(s\) & \(s\)-\(0\)-\(s\) & \(s\)-\(s\)-\(0\) & \(s\)-\(s\)-\(s\) \\  AUE & 83.5 & 58.7 & 79.4 & 88.7 & 60.8 & 56.2 & 87.7 & **52.4** \\ AAP & 52.3 & 52.0 & 52.9 & 44.9 & 51.4 & 42.2 & 44.8 & **39.1** \\   

Table 7: SimCLR accuracy(%) of attacks generated with decoupled strength parameters on CIFAR-10. For example, \(0\)-\(0\)-\(s\) means that ResizedCrop strength is \(0\), ColorJitter strength is \(0\), and Grayscale strength is \(s\).