# FedGTST: Boosting Global Transferability of Federated Models via Statistics Tuning

Evelyn Ma, Chao Pan, Rasoul Etesami, Han Zhao, Olgica Milenkovic

University of Illinois Urbana-Champaign

{pingm, chaopan2, etesami1, hanzhao, milenkov}@illinois.edu

###### Abstract

The performance of Transfer Learning (TL) significantly depends on effective pretraining, which not only requires extensive amounts of data but also substantial computational resources. As a result, in practice, it is challenging to successfully perform TL at the level of individual model developers. Federated Learning (FL) addresses these challenges by enabling collaborations among individual clients through an indirect expansion of the available dataset, distribution of the computational across different clients, and privacy-preserving communication mechanisms. Despite several attempts to design effective transferable FL approaches, several important issues remain unsolved. First, existing methods primarily focus on optimizing transferability within local client domains, thereby ignoring transferability across clients. Second, most approaches focus on analyzing indirect transferability metrics, which does not allow for accurate assessment of the final target loss and the degree of transferability. To address these issues, we introduce two important FL features into the model. The first boosts transferability via an exchange protocol between the clients and the server that includes information about cross-client Jacobian (gradient) norms. The second feature promotes an increase of the average of the Jacobians of the clients at the server side, which is subsequently used as a local regularizer that reduces the cross-client Jacobian variance. A rigorous analysis of our transferable federated algorithm, termed FedGTST (Federated Global Transferability via Statistics Tuning), reveals that increasing the averaged Jacobian norm across clients and reducing the Jacobian variance ensures tight control of the target loss. This insight leads to an upper bound on the target loss of transferable FL regarding the source loss and source-target domain discrepancy. Empirically, experiments on public benchmarks show that FedGTST significantly outperforms other baselines, such as FedSR.

## 1 Introduction

_Transfer Learning_ (TL) has received significant interest in the machine learning community due to its ability to extract representative features from source tasks and use them to improve the generalization capability on related target domain problems . In addition to boosting the performance of a target domain model, TL also reduces the computational cost of fine-tuning the target domain model. Nevertheless, effective source pretraining in TL is practically challenging for individual model developers because it requires access to large datasets as well as significant computational resources . To resolve this problem, one can leverage _Federated Learning_ (FL), which refers to decentralized learning protocols used in mobile and IoT devices . FL not only increases access to multiple datasets in a decentralized manner and alleviates the computational burden of individual clients, but it also protects the privacy of local data . As a result, a number of recent works have outlined methods for transferable FL, including FedADG (Federated Adversarial Domain Generalization) , FedCDG (Federated Contrastive Domain Generalization) , FedSR(Federated Simple Representations) , FedIIR (Federated Implicit Invariant Relationships) , FedCCST (Federated Cross-Client Style Transfer) , FedMM (Federated Adversarial Domain Adaptation)  and StableFDG (Stable Federated Domain Generalization) . Despite the promising preliminary findings provided by the techniques above, several combinations of important issues remain unsolved across the spectrum of methods.

For **privacy leakage**, the limitations of existing methods include: **1.** FedADG forces each client source domain to align its representation distribution with that of the target domain, and therefore violates data privacy because source domains are given access to the target domain in order to perform the alignment; **2.** FedCCST boosts global transferability by increasing local diversity to avoid local overfitting. It therefore requires clients to share their local representations with each other and this information is subsequently used for local data augmentation. This is a direct violation of FL privacy constraints; **3.** StableFDG expands local data diversity by sharing style statistics (i.e., representations, including means and variances). This clearly leads to the leakage of local privacy-sensitive information.

For **local overfitting**, the shortcomings of a group of the methods above are as follows: **1.** FedSR learns a simple representation through regularization during local training, by exploiting the similarity between the representation and the data, given the labels. However, since the regularized local training relies completely on local structures (i.e., local models, representations, labels, data), it leads to overfitting of local distributions, and thus has limited capability to learn cross-client invariant features, which is key for global transferability; **2.** FedCDG uses a contrastive local regularizer on representations generated by various samples within the same class. This leads to overfitting in the local domain since no cross-client information is exploited.

For **communication complexity**, we observe that: **1.** FedIIR is suboptimal. Although it mitigates the problem of privacy violation and avoids local overfitting by adding a local regularizer capturing the distance between the local gradient and the global gradient, it requires communicating gradients between the clients and the server and therefore doubles the communication cost compared to baselines (additionally, FedIIR performs well for a large number of clients, but offers average performance when this number is small); **2.** Similar communication complexity problems are faced by FedCCST and StableFDG, which rely on communicating styles (i.e., representations); **3.** FedMM requires significant communication overhead for adaptation using distribution-matching techniques , which involves solving an intractable non-convex-non-concave optimization problem .

Finally, prior works mostly **lack explicit theoretical analyses of global transferability:** they do not tend to quantify the performance/loss of the pretrained model fine-tuned on the target domain.

In summary, perhaps the most important unresolved problem with known transferable FL models (with the exception of FedIIR) is that they use centralized TL approaches during local training, and do not fully exploit features specific to federated learning (for details, see also the discussion in Section 2).

**Our contributions.** We describe what is, to the best of our knowledge, the first approach to federated transfer learning termed _Federated Global Transferability via Statistics Tuning (FedGTST)_ that simultaneously alleviates the above issues faced by existing methods. Our main contributions can be summarized as follows.

1. We use a new regularizer that encodes cross-client statistics and forces the local training process to tune the global statistics in a "direction" that improves global transferability rather than just local transferability. This is achieved through subtractions of global norms of Jacobians (gradients) communicated by the server.
2. We suggest to only communicate scalars, more precisely, _Jacobian norms_, which introduces a negligible communication overhead in the overall model exchange protocol.
3. We ensure that our communication schemes do not allow uncontrolled access to data and thereby ensure data privacy.
4. We rigorously prove that even though only small discrepancies among local gradients may exist upon regularization, transferability can be low as regularization can impede the growth of the gradient norm. To boost the Jacobian norm, we implement specialized protocols at both the client and server levels. Finally, we establish relevant bounds on the transferability loss for this setting.

The main technical insights provided by our analysis are as follows. Two FL-specific factors, a small _cross-client Jacobian variance_ and larger _cross-client Jacobian norm_ are indicative of good transferability. These factors are _direct_ performance indicators, unlike _indirect factors_ (e.g., feature invariance) which only suggest improved transferability. Our findings are based on the first _direct measure_ of transferability, which equals the loss on the target domain incurred by the pretrained federated model. The FL-specific factors govern the bounds on the loss and therefore allow one to control them for better transferability. We validate these findings through extensive experiments which show that FedGTST outperforms methods such as FedSR and FedIIR by as much as \(10\%\).

## 2 Related Work

**FL** is a machine learning paradigm in which multiple entities collaborate to train a global model without sharing their local data (see  for a comprehensive review). FL has gained significant attention due to its potential to address privacy concerns while enabling large-scale collaborative learning . Relevant to this work,  proposed the Federated Averaging (FedAvg) algorithm, which aggregates model updates from multiple client devices to train a global model. Another relevant line of work  introduced FedProx, a federated optimization algorithm that incorporates proximal terms to handle non-iid data distributions. FL methods nevertheless still face several challenges. One challenge is dealing with highly heterogeneous local datasets , for which the recent work FedImpro proposed leveraging aggregated feature distributions to address client drift. Another challenge is the communication overhead incurred during the aggregation of model updates . Minimizing communication costs while ensuring convergence and data privacy remain active topics of research in FL. Also, many FL solutions primarily emphasize performance in the client domain without considering the performance of the model on unseen domains.

**TL** is a powerful machine learning technique that allows models to leverage knowledge gained from one task to improve performance on another related task . TL has been widely adopted in various domains such as computer vision, natural language processing, and speech recognition, where labeled data may be scarce or expensive to acquire [47; 48]. A common approach in TL involves fine-tuning a pre-trained model on a target task using a small amount of labeled data, which often leads to improved generalization and faster convergence compared to training from scratch . Recent works in TL have focused on developing more effective algorithms, such as domain adaptation methods that address the discrepancy between the source and target domains . Additionally, TL techniques have been used to handle tasks with limited amounts of labeled data through techniques like semi-supervised and self-supervised learning . TL still faces challenges such as negative transfer, where information from the source task actually degrades performance on the target task; and, it requires careful selecting of appropriate pretrained models and transfer strategies for specific tasks and domains . Current TL methods often require that one entity possesses knowledge of all data, violating the privacy requirements of FL. Moreover, we comment on _Gradient Matching in TL_ in Appendix I.1

**Transferable Federated Learning** (TFL) is an emerging research area at the intersection of FL and TL. One of earliest contributions to the field, _FedADG_, encourages the transferablity of FL through adversarial local training. However, the work does not provide theoretical guarantees, and existing studies  indicate that adversarial robustness does not necessarily lead to better transferability. Other methods, such as _FedSR_ and _FedCDG_, enhance transferability by adapting standard representation learning from a single-agent to a federated setting; they do not incorporate FL-specific features (i.e., instructions provided by the server, cross-client model properties etc). Note that although _FedSR_ has successfully included centralized invariant feature learning into FL, it uses centralized methods locally and then shares information with the global model, and thereby does not fully exploiting FL capabilities. Thus, using FedSR, each client can learn very different representations that are hard to aggregate at the central server. More precisely, FedSR does not communicating information that can help improve the transferability of the global model. Among all the previously discussed methods (FedADG, FedCDG, FedSR, FedIIR, FedCCST, and StableFDG), FedIIR is the closest to our approach and may be seen as a special case of our method which has better performance, smaller communication complexity and comes with provable global transferability guarantees. Furthermore, we discuss the distinctions and connections between TFL and Generalization of FL, a topic potentially relevant to TFL, in Appendix I.2.

Preliminaries

**General Supervised Learning Settings**. We denote the data space by \(\), the feature space by \(\), and the label space by \(\). A model \(h:\) typically takes the form \(h=g f\), where \(f:\) is a feature extractor and \(g:\) is a classifier. Denote the function class for the entire model, the feature extractor and the classifier by \(,,\), respectively, so that \(h,f,g\). Denote the weights of model \(\{f,g,h\}\) as \(w_{}\). Given a loss function \(l:\) and a domain distribution \(\) over \(\), the population loss \(L_{}(h)\) is defined as

\[L_{}(h):=_{(x,y)}\ l(h(x),y).\] (1)

**General Framework of TFL**. In TL, the two typical learning phases are: a) pretraining on the source domain; and, b) finetuning on the target domain. In the context of TFL, pretraining is conducted via FL over source (local) domains, while the global model is trained and then finetuned on the target domain during the second phase. In both phases, supervised learning is performed with full access to the labels. More details are provided next.

**Pretraining Phase in TFL: FL on Source (Local) Domains**. The source domain is a composition of the agents' local domains, \(\{^{(k)}\}\), with \(k[K]\) denoting the client index and \(K\) representing the total number of clients. The source loss is defined as the standard federated loss on the source domains.

\[L_{src}(g f):=_{k=1}^{K}L_{^{(k)}}(g f).\] (2)

Let \(h^{*}=g^{*} f^{*}\) be an optimal global solution for the objective (2). In FL approaches, the problem solution is the result of the central server's aggregation of local models into a global one. We denote the local solutions involved in creating the optimal global solution \(^{*}\) (\(\{f,g,h\}\)) by \(\{^{*(k)}\}\); through averaging aggregation, we obtain the optimal global weights \(w^{*}_{}=_{k}w^{*(k)}_{}\).

**Finetuning Phase in TFL: Supervised Finetuning on the Target Domain.** Upon obtaining the optimal pretrained global solution \(h^{*}=g^{*} f^{*}\), the pretrained feature extractor \(f^{*}\) is fixed and applied to the target domain \(_{T}\). The target loss is defined as the loss on the target domain \(_{T}\), i.e.,

\[L_{tgt}(g f^{*}):=L_{_{T}}(g f^{*}).\] (3)

Through finetuning, a new classifier \(g^{*}_{T}:=_{g}L_{tgt}(g f^{*})\) is determined by minimizing the target objective (3).

**Transferability Assessment**. With a slight abuse of notation, we define the optimal target loss as

\[L^{*}_{tgt}:=L_{tgt}(g^{*}_{T} f^{*}).\]

We formally define the _measure of transferability of TFL_ as the optimal target loss \(L^{*}_{tgt}\), as it _directly_ reflects the performance of a transferable model on the target domain. A smaller \(L^{*}_{tgt}\), or a tighter bound on it, indicates better transferability.

## 4 Theoretical Bounds on the Target Loss

### A General Bound Based on Discrepancy/Divergence

We start with Definitions 1 and 2 borrowed from existing TL studies that characterize the domain discrepancy, and then propose a new domain divergence tailored to Transferable FL (TFL), including the cross-client discrepancy in Definition 2 and the source-target discrepancy in Definition 3.

**Definition 1** (\(,\)-discrepancy ).: Given a classifier class \(\), a feature extractor class \(\), the source domain \(_{S}\) and the target domain \(_{T}\), with a slight abuse of notation, the \((,)\)-discrepancy is defined as

\[d_{,}(_{S},_{T}):=_{f }_{g}L_{^{(k)}}(g f)-_ {g}L_{_{T}}(g f).\] (4)

_Remark_.: The \(,\)-discrepancy has also been used in the analysis of domain adaptation .

We next adapt the \(\)-discrepancy to measure the cross-client domain discrepancy in transferable FL (Definition 2), and tailor the \(,\)-discrepancy (Definition 1) to measure the source-target discrepancy in TFL (Definition 3).

**Definition 2** (Cross-Client Divergence for TFL).: Given a model class \(\) and federated local domains \(_{S}^{fed}=\{^{(k)}\}_{k[K]}\), with \(d_{}(,)\) defined as Definition 5, the intra-client discrepancy is defined as

\[_{}(_{S}^{fed}):=_{k_{1}  k_{2}}d_{}(_{S}^{(k_{1})},_{S}^{(k_ {2})}).\] (5)

Note that \(_{}(_{S})\) equals the average of \(\)-discrepancies over all local domain pairs and therefore measures the intra-discrepancy on the non-iid distributed source domains. When source domains are iid across clients, we have \(_{}(_{S})=0\).

**Definition 3** (Source-Target Discrepancy for TFL).: Given a classifier class \(\), a feature extractor class \(\), federated local domains \(_{S}^{fed}=\{^{(k)}\}_{k[K]}\), and the target domain \(_{T}\), with slight abuse of notation, the federated \((,)\)-discrepancy is defined as

\[d_{,}(_{S}^{fed},_{T}):=_{k[K]}d_{,}(^{(k)},_{T}).\] (6)

Based on the two TFL-specific domain discrepancy definitions (i.e., Definition 2 and 3), we derive a general bound on the TFL loss in Theorem 1. The TFL-specific source-target discrepancy (Definition 3) is further used in Theorem 2 which presents a bound on the target loss using cross-client statistics. For our theoretical analyses, we need the following common assumptions.

**Assumption 4.1** (Convexity and Smoothness).: _We assume that the loss function \(l\) satisfies two conditions: (1) \(l\) is convex w.r.t. \(w_{h}\); (2) \(l\) is Lipschitz smooth for \(w_{h}\) with a constant \(>0\)._

_Remark_.: Assumption 4.1 is easy to meet in practice, and it arises in many linear models (linear regression, SVM etc).

**Theorem 1** (Bound Based on TFL-specific Domain Discrepancy).: _Under Assumptions 4.1 (Convexity and Smoothness), the optimal target loss is bounded as_

\[L_{tgt}^{*}_{k=1}^{K}[L_{^{(k)}}(h^{* (k)})]+_{}(_{S}^{fed})+d_{ ,}(_{S}^{fed},_{T}),\] (7)

_where \(h^{*(k)}\) denotes the optimal local model of client \(k\) (see Section 3)._

**Semantic Interpretation**. In Theorem 1, the optimal target loss \(L_{tgt}^{*}\) is bounded by the sum of three terms on the RHS of Equation 7: (1) the averaged optimal local loss \(_{k=1}^{K}[L_{^{(k)}}(h^{*(k)})]\); (2) the intra-discrepancy of the source domain \(_{}(_{S}^{fed})\); (3) the discrepancy between the source and target domains, \(d_{,}(_{S}^{fed},_{T})\). Therefore, the bound on the optimal target loss can be tightened by making the optimal source loss smaller, and by lowering the intra-source and source-target discrepancy. The later two terms can be controlled through regularization as detailed below.

**Tightening the Bound via Regularization over \(\).** Given two feature extractor function classes \(_{1}\) and \(_{2}\), if \(_{1}_{2}\), we have \(_{1}_{2}\) where \(_{1}=_{1}\) and \(_{2}=_{2}\). From Definition 2 and 3, it is straightforward to see that \(_{_{1}}(_{S}^{fed})_{ _{2}}(_{S}^{fed})\) and \(d_{,_{1}}(_{S}^{fed},_{T}) d _{,_{2}}(_{S}^{fed},_{T})\). This indicates that any general regularizer on \(\) can lead to a decrease in the latter two terms of the RHS of Theorem 1. However, shrinking the expressive power of \(\) will inevitably increase the optimal source loss, which is the first term on the RHS of the expression in Theorem 1. Therefore, using general regularization, one has to trade-off the _optimal source loss_, _TFL-specific cross-client discrepancy_, and _TFL-specific source-target discrepancy_.

Based on Theorem 1 and the follow-up discussion, we aim to answer the question: how should one design a _practical regularizer_ that can inherently tighten our bound on TFL? We give an answer to this question in Section 4.2. There, Theorem 2 shows that a transferability-boosting pretraining regularization should decrease the _cross-client Jacobian variance_ while at the same time increase the _cross-client averaged Jacobian norm_.

### Practical Bounds Based on Cross-Client Statistics

The goal of FL is to estimate the optimal solution \(f^{*}\) by updating the global model through multiple federated rounds 1. Denote the total number of rounds by \(P\) and the output global model after round \(P\) by \(f_{P}\). Denote the target loss under \(f_{P}\) instead of \(f^{*}\) as \(_{tgt}^{*}\) (which is an estimate of \(L_{tgt}^{*}\)). At the end of local training during any round \(p P\), let \(h_{p}^{(k)}\) with weight \(w_{p}^{(k)}\) represent the model of client \(k\), and let \(h_{p}\) with weight \(w_{p}\) represent the global model. Denote the Jacobian (gradient) of the loss \(l\) w.r.t the model weights at domain \(k\) as \(J^{(k)}(w)=_{_{S}^{(k)}}_{w_{k}}l(h(x),y)|_{w_{k}=w}\).

Throughout the remainder of the manuscript, we use \(J_{p}^{(k)}\) as shorthand for \(J^{(k)}(w_{p})\). Furthermore, we denote the learning rate of agent \(k\) at round \(p\) by \(_{p}^{(k)}\). We make a single-step local update assumption (Assumption 4.2) and use the definitions for cross-client statistics (Definition 4) to derive bounds exploiting the cross-client statistics from Lemma 4.1 and Theorem 2).

**Assumption 4.2** (Single-Step Local Update).: _During local training, all clients perform one step of gradient descent (GD) to update their model for transmission, \(w_{p+1}^{(k)}=w_{p}-_{p+1}^{(k)} J^{(k)}(w_{p})\). This is a common assumption in the FL literature . 2_

**Definition 4** (Cross-Client Statistics).: At federated round \(p\), given \(K\) clients with local Jacobians \(\{J_{p}^{(k)}\}_{k[K]}\), we define the _cross-client averaged Jacobian norm_\(\|J_{p}\|_{2}\) and the _cross-client Jacobian variance_, respectively, as

\[\|J_{p}\|_{2}=\|_{k}J_{p}^{(k)}\|_{2},\ \ \ \ _{p}^{2}=_{k}\|J_{p}^{(k)}\|_{2}^{2}-\| _{k}J_{p}^{(k)}\|_{2}^{2}.\] (8)

Note that we assumed the loss function to have a \(\)-Lipschitz continuous gradient. When the gradient is large, \(\) is also large, and to make \(_{1}() 0\), \(\) has to be close to \(0\). In this case, the absolute value of the second term can be small and that of the third term can be large.

**Lemma 4.1** (Loss Bound Using Cross-Client Statistics).: _Under Assumptions 4.2 and 4.1, and the cross-client statistics defined in Definition 4, after \(P\) rounds of federated pretraining one has_

\[_{tgt}^{*} L_{src}(h_{0})-_{p=0}^{P-1}_{ 1}(_{p+1})\|J_{p}\|_{2}^{2}+_{p=0}^{P-1}_{2}(_{p+1}) _{p}^{2}+d_{,}(_{S}^{fed},_ {T}),\] (9)

_where \(h_{0}\) is the initial global model and \(_{2}()=}{2},\ _{1}()=-_{2}()\)._

Note that _during the training process_, a large Jacobian norm can promote transferability (the Jacobian is expected to be small at the end of training).

**Interpretation of Key Terms**. Lemma 4.1 shows that the _target loss_ of the finetuned pretrained model (LHS) is bounded by a sum (RHS) involving four key terms: 1) \(L_{src}(h_{0})\), the _initial source loss_; 2) \(\|J_{p}\|_{2}\), the _cross-client averaged Jacobian norm_; 3) \(_{p}^{2}\), the _cross-client Jacobian variance_; 4) \(d_{,}(_{S}^{fed},_{T})\), the _TFL-specific source-target domain divergence_. Since \(L_{src}(h_{0})\) is fixed throughout the pretraining process, and \(d_{,}(_{S}^{fed},_{T})\) can be reduced using general regularization, we focus on analyzing the two tunable cross-client statistics, \(\|J_{p}\|_{2}\) and \(_{p}^{2}\).

**Influence of the Cross-Client Statistics on the Bound**. We note that during pretraining, both the coefficients \(_{1}(_{p+1})\) and \(_{2}(_{p+1})\) have to be positive (see how to ensure these constraints in Appendix D). Therefore, a larger \(\|J_{p}\|_{2}\) and a smaller \(_{p}^{2}\) tighten the upper bound, indicate a lower target loss, and thus suggest better model transferability.

**Coefficients Quadratic w.r.t. the Learning Rates**. Lemma 4.1 involves coefficients \(_{1}()\) and \(_{2}()\) that are quadratic in the learning rate \(\); thus, we can further tighten the bound by optimizing the learning rates across different rounds (Assumption 4.3). The tightened bound is given in Theorem 2.

**Assumption 4.3** (Optimal Learning Rates Across Rounds).: _In each round \(p\) (\(2 p P\)), we use an optimal learning rate for local training \(_{p}^{*}=-1\|_{2}^{2}}{_{k}\|J_{p}^{(k) }\|_{2}^{2}}.\) A similar analysis and assumptions on optimal learning rate has also been used in [17; 10; 13; 30; 49]._

**Theorem 2** (Tightened Bound Based on Cross-Client Statistics).: _By optimizing the bound in Lemma 4.1 with respect to the learning rates \(\) as governed by Assumption 4.3, and under Assumption 4.1 (Convexity and Smoothness), the estimated optimal target loss is bounded as follows:_

\[_{tgt}^{*} L_{src}(h_{0})-_{p=0}^{P-1}\|_{2}^{2}}{(1+_{p}^{2}\|J_{p}\|_{2}^{-2})}+d_{,}(_{S}^{fed},_{T}).\]

**Semantic Interpretation**. Theorem 2 indicates that by optimizing the learning rates at each round, the bound for \(_{tgt}^{*}\) can be made smaller through 1) a smaller source loss \(L_{src}(h_{0})\); 2) a larger cross-client average Jacobian norm \(\|J_{p}\|_{2}\); 3) a smaller cross-client Jacobian variance \(_{p}^{2}\), and 4) a smaller source-target domain divergence \(d_{,}(_{S}^{fed},_{T})\). These are consistent with Lemma 4.1.

**Intuitive Explanation**. **(a)**_Increasing the Cross-Client Averaged Jacobian Norm \(\|J_{p}\|\)_. In early training, it is crucial to avoid a small \(J_{p}\), as it can "trap" clients in local minima or cause overfitting of local distributions. Note that increasing \(J_{p}\) will not induce an excessively large second term (i.e., a term that \(\)) in Theorem 2, as increments are constrained through the Lipschitz condition \(\|J_{1}-J_{2}\|\|x_{1}-x_{2}\|\). In the final stages of training, \(\|J_{p}\|\) naturally decreases due to convergence. **(b)**_Decreasing the Cross-Client Jacobian Variance \(_{p}\)_. A small \(_{p}\) promotes domain similarity, preventing local overfitting and enhancing global model transferability. **(c)**_Trade-Off Between Increasing \(\|J_{p}\|\) and Decreasing \(_{p}\)_. A larger \(\|J_{p}\|\) induces more substantial local updates but can increase variance \(_{p+1}\). Thus, Theorem 2 underscores balancing this trade-off: enlarging \(\|J_{p}\|\) while maintaining a small \(_{p}\).

**Challenges in Regularization of the Jacobians**. Certain regularization of the Jacobians can induce local Jacobian alignment and reduce \(_{p}^{2}\) (e.g., the regularization of the alignment between local gradients and the global gradient proposed in FedIIR ). However, local regularization can naturally _impede_ the growth of local Jacobian norms \(\{J_{p}^{(k)}\}_{k[K]}\) and subsequently prevent \(\|J_{p}\|_{2}\) from increasing. Thus, such prior regularization may not necessarily improve transferability. As an example, consider the extreme case where \(_{p}^{2}=0\). Then, the bound in Theorem 2 takes the form \(_{tgt}^{*} L_{src}(h_{0})-_{p=0}^{P-1} {}\|J_{p}\|_{2}^{2}+d_{,}(_{S}^{fed}, _{T})\), where a small \(\|J_{p}\|_{2}\) can severely degrade the bound. This indicates that boosting transferability by prior regularization, such as controlling \(_{p}^{2}\) itself, has limitations, and tuning \(\|J_{p}\|\) during the pretraining stage is of crucial importance. We provide a solution to this problem in Section 5.

## 5 Our Algorithm

**Algorithmic Solution for the Theoretical Challenges**. From Lemma 4.1 and Theorem 2, we can see that certain round-wise FL-specific statistics, the _cross-client averaged Jacobian norm_\(\|J_{p}\|_{2}\) and _cross-client Jacobian variance_\(_{p}^{2}\) control the bound on the target loss. The challenge is that, while \(_{p}^{2}\) can be reduced using straightforward techniques (i.e., such as gradient alignment from FedIIR ), such techniques unavoidably prevent \(J_{p}\) from increasing properly. We therefore propose our FedGTST approach which reduces \(_{p}^{2}\) while enlarging \(\|J_{p}\|_{2}\). A round-wise description of FedGTST is given in Algorithm 5.

**Tuning the Cross-Client Jacobian Variance \(_{p}^{2}\).** The cross-client Jacobian variance is controlled via regularization at the local client level (Line 5 of Algorithm 5). The local clients, upon receiving a _guide norm_\(\) from the server (explanation deferred to subsequent sections), implement a regularized local training protocol. While client \(k\) during standard local training uses the objective \(L_{^{(k)}}(h)\), during regularized local training he/she/they use

\[L_{_{S}^{(k)}}(h)+(\|J^{(k)}(h)\|_{2}- )^{2}\] (10)

instead, where \(\) is the penalty coefficient for the regularization term. This type of regularization intuitively aligns each local Jacobian norm \(\|J^{(k)}\|\) with the guide norm \(\), which results in a \(_{J}^{2}\) that is smaller than that of standard FL (i.e., FedAVG).

```
1:Randomly select a set \([K]\) for regular training.
2:while\(k K\), Client \(k\) should do
3: Receive the guide norm \(_{p-1}\) and global model \(h_{p-1}\) from the server.
4: Initialize the local model to \(h_{p-1}\).
5: Update the local model to \(h_{p}^{(k)}\) by training with \(L_{_{S}^{(k)}}(h)+\|J^{(k)}(h)\|_{2}-^{2}\).
6: Calculate the surrogate Jacobian norm \(_{p}^{(k)}=_{p}^{reg,(k,k)}=\|J_{p}^{(k)}\|_{2}\).
7:if\(k\)then
8: Train with \(L_{^{(k)}}(h)\) to obtain Jacobian norm \(_{p}^{std,(k)}\).
9: Update the surrogate Jacobian norm \(_{p}^{(k)}=_{p}^{reg,(k)},_{p}^{std,(k)}\).
10:endif
11: Transmit the model \(h_{p}^{(k)}\) and norm \(_{p}^{(k)}\) to the server.
12:endwhile
13: The server aggregates the client models into a global model \(h_{p}\) and sets the guide norm to \(_{p}=_{k}(_{p}^{(k)})\).
14: The server broadcasts \(h_{p}\) and \(_{p}\) to all clients. ```

**Algorithm 1** FedGTST (Round \(p\))

**Tuning the Cross-Client Averaged Jacobian Norm \(\|J_{p}\|_{2}\).** The cross-client averaged Jacobian norm can be increased via an exchange protocol that includes: 1) clients calculating surrogate norms for transmission; 2) the server computing and broadcasting a guide norm; 3) clients performing local alignment using the guide norm.

**Surrogate Norms**. A decrease in the local Jacobian norm \(\|J^{(k)}\|\) prevents the cross-client averaged Jacobian norm \(\|J\|\) from growing. We mitigate such a decrease by forcing a small portion of the clients to implement both regularized training and standard training (Line 1), to generate a pair of Jacobian norms \(^{reg,(k)}\) and \(^{std,(k)}\). The client than chooses the larger norm, \(^{(k)}=max(^{reg,(k)},^{std,(k)})\) as a surrogate norm for transmission (Lines 6 to 10).

**Server Guide Norm**. The clients send their local Jacobian norms \(^{(k)}:=\|J^{(k)}\|_{2}\) to the server, and the server broadcasts the largest norm received as its guide norm \(:=_{k}^{(k)}\) (Line 13).

**Local alignment**. The local regularizer from Equation (10) reduces the variance, but can also force an increase of the averaged Jacobian norm. It forces all local Jacobian norms to align with the guide norm \(\) (Line 5). Since \(\) has been boosted both at the clients' and server levels, the alignment leads to larger local Jacobian norms.

**Communication Cost.** Most TFL methods, as already described, require large communication overheads between the clients and the server. For example, FedIIR requires exchanging Jacobians, which have the same dimension as the model weights; FedGTST only requires exchanging norms.

## 6 Experiments

### Experimental Setting

**Transfer tasks.** We investigate three transfer tasks utilizing fully-annotated data: a) MNIST  to MNIST-M , b) CIFAR-10  to SVHN , and c) cross-domain transfer in DomainNet . All transfer tasks have been used as benchmarks in existing TL research [3; 8; 11; 22; 28; 50] (also, see Appendix G). Moreover, DomainNet is a standard large-scale dataset for TFL studies, also used by FedSR.

**Non-iid Distributed Source (Local) Domains**. The pretraining phase is conducted via FL on source (local) domains. Marginal distribution shift is an important phenomenon in FL  since the access to classes (or categories) is not the same for all participating entities. However, some federated datasets tested by existing transferable FL methods do not reflect marginal distribution shifts when constructing source local domains (i.e., in the Rotated-MNIST benchmark in FedSR, all clients have access to all classes). To address this issue, we employ the following methods for constructing non-iid local domains:* For MNIST or CIFAR-10, unless otherwise specified, we follow the approach in  and let each client have access to only a subset of the categories. The category selection rule and data sampling method is described in Appendix H. Besides, we run additional experiments for Dirichlet sampling, a method commonly used in FL .
* For DomainNet, which compromises six distinct domains, we follow the leave-one-out strategy used in FedSR: one domain is designated as the target domain, while the remaining five domains serve as source domains, with each assigned to an individual client.

**Federated System Size**. Large system sizes are inherent to FL systems, where the number of clients can be as large as \(100\). In such a case, it is more challenging for the global model to achieve good performance. However, existing TFL methods typically use a very small number of domains (\( 5\) for FedSR and StableFDG). In contrast, we allow the system size to cover a broad range of values, including \(10\) (small), \(50\) (medium), and \(100\) (large) clients.

**Evaluation matrix and backbones**. We measure domain transferability via \(acc_{tgt}\), the accuracy of a pretrained model finetuned on the target domain. For backbone selection, we use both LeNet  and ResNet18 to represent different levels of backbone complexity.

**Baselines**. We consider the following TFL algorithms as our main baselines: FedAVG, FedSR, and FedIIR. Furthermore, we also compare our findings to Scaffold , an advanced FL approach used to address data heterogeneity. We do not report results for FedADG, FedCDG, FedGTST, and StableFDG since they do not ensure privacy and/or underperform compared to the main baseline.

**Settings for Pretraining (FL on source local domains)**. 1) _Local epochs_. To conform with our theoretical assumptions, unless specified otherwise, we set the local epoch of each client to \(1\). We also investigate the system performance with \(10\) local epochs in Appendix C. 2) _Number of participants per round_. We allow \(50\%\) of the clients to participate in each round (e.g., if \(K=50\), there are \(25\) participants per round). 3) _Number of clients performing standard local training_. To boost the Jacobian norm, a subset of clients conducts standard local training (we set the subset size to \(10\%\) of the total number of clients).

**Additional Settings**. Unless specified otherwise, for local training on the source datasets we use the Adam  optimizer with coefficients \((_{1},_{2})=(0.9,0.999)\) (note that these \(\) values are not to be confused with the coefficients from our theoretical analysis). Our initial learning rate equals \(0.01\) and then decays by a factor of \(10\) per \(50\) rounds, with an early stop trigger of \(10\) rounds. We apply the standard cross-entropy loss. We also set the pretraining batch size to \(256\) for MNIST \(\) MNIST-M and to \(128\) for CIFAR10 \(\) SVHN. For both tasks, we use a finetuned learning rate \(0.005\), with weight decay \(0.0001\). All results reported in Section 6.2 are averaged over three runs.

### Results

**Transferability results**. FedGTST exhibits **significantly improved transfer performance** when compared to baselines across a range of tasks, system sizes, and backbone architectures. For the example of CIFAR10\(\)SVHN with \(100\) clients and a LeNet backbone, FedGTST outperforms FedIIR by \(7.6\%\) and FedSR by \(9.8\%\). The results for a small, medium and large federated system (\(K=10\), \(K=50\) and \(K=100\)) are reported in Tables 1, 2 and 3, respectively.

**Discussion.** Besides the significant performance gain of FedGTST over baseline methods, we also observe that 1) for transfer tasks in which the backbone and method are fixed, transferability generally decreases with the system size. Importantly, FedGTST improves the baselines more significantly

    &  &  &  \\    & LeNet & ResNet & LeNet & ResNet \\  FedAVG & 73.8\(\)0.7 & 81.6\(\)0.2 & 64.4\(\)0.5 & 72.0\(\)1.0 & 73.0 \\ FedSR & 75.0\(\)0.9 & 80.6\(\)0.1 & 65.9\(\)0.6 & 71.3\(\)0.4 & 73.2 \\ FedIIR & 74.5\(\)0.3 & **82.7\(\)0.7** & 66.2\(\)1.0 & 73.8\(\)0.2 & 74.3 \\ Fed-GTST & **76.2\(\)0.9** & 82.3\(\)0.5 & **70.1\(\)0.8** & **74.5\(\)0.3** & **75.8** \\   

Table 1: Target accuracy (%) of the finetuned model pretrained on a small number of clients (\(K=10\)). FedGTST outperforms other methods across both tasks and both backbones; for the example MNIST to MNIST-M with a LeNet backbone, FedGTST outperforms FedIIR and FedSR by around \(4\%\).

for large system sizes; 2) when the system size, transfer task and the method are fixed, the more "complex" the backbone (ResNet18 vs LeNet), the better the transferability.

**Additional results**. We defer reporting and discussing additional results in Appendix C, which include (a) an investigation on _hyper-parameter sensitivity_ the _convergence speed_, and _cross-client statistics_, (b) results regarding _DomainNet_, _Dirichlet Sampling_, and _Scaffold_.

## 7 Limitations

**Increased Local Computational Cost**. While FedGTST only induces a negligible communication overhead among clients and the server, we note that at the level of a _small subset of clients_ we need to conduct both regularized training and standard training to boost \(\|J_{p}\|_{2}\). This increases the local computational burden but to a very small extent. Nevertheless, in future works, we will explore algorithms with lower local computational costs.

**Potentially Loose Bounds**. Although existing studies have added to our understanding of transferable federated learning, our work is the first to derive a bound on a direct measure of transferability (the target loss). We believe that the bound can be tightened.

## 8 Conclusion

We introduced FedGTST, a federated learning algorithm aimed at enhancing global transferability. Inspired by theoretical insights, FedGTST integrates cross-client information on averaged Jacobian norms and Jacobian variance. Our work addresses key challenges in existing methods, such as privacy violations and an overemphasis on local transferability. Experimental results demonstrate significant performance improvements over baseline models.

## Appendix A Additional preliminaries

**Definition 5** (\(\)-discrepancy ).: Given a model function class \(\) and two data distributions \(_{S}\), \(_{T}\), the \(\)-discrepancy between \(_{S},_{T}\) is defined as

\[d_{}(_{S},_{T}):=_{h}|L_{ _{S}}(h)-L_{_{T}}(h)|.\] (11)

_Remark_.: This type of discrepancy, as well as the related \(\)-divergence, have been frequently used in the analysis of domain adaptation , and we follow this trend.

## Appendix B Proof of our bounds

**Lemma B.1** (Theorem 2.5 in ).: _Suppose we perform pretraining on the source domain \(_{S}\) to obtain \(f\) and \(g\) at the server, and fine-tune the model on the target domain \(_{T}\) to obtain a new classifier \(g_{T}\). We then have_

\[L_{_{T}}(g_{T}^{*} f^{*}) L_{src}(g^{*} f^{*})+d_{ ,}(_{S},_{T}).\] (12)

Equation (12) shows that the transfer loss can be upper bounded by the sum of the loss on the source domain and the divergence between two domains.

**Lemma B.2**.: _With Assumption 4.1 (Convexity and Smoothness) and Definition 2 (Cross-Client Divergence), we have_

\[L_{src}(h^{*})_{k=1}^{K}[L_{^{(k)}}( h^{*(k)})]+_{}(_{S}^{fed}).\] (13)

Proof.: The loss function \(l\) is assumed to be convex w.r.t. the parameters of the model \(h\), where the parameters are denoted by \(w_{h}\). Based on the aggregation rule in Section 3, we have \(w_{h}=_{k}w_{h}^{(k)}\), where \(w_{h}\) and \(w_{h}^{(k)}\) stands for the weights for global model \(h\) and that for local model \(h^{(k)}\), respectively. We therefore have

\[L_{src}(h^{*}) =_{k_{1}}L_{^{(k_{1})}}(h^{*})\] \[=_{k_{1}}_{^{(k_{1})}}\;l (h(x,w_{h}^{*}),y)\] \[=_{k_{1}}_{^{(k_{1})}}\;l (h(x,_{k_{2}}w_{h}^{*(k_{2})}),y)\] \[}_{k_{1},k_{2}}_{ ^{(k_{1})}}\;l(h(x,w_{h}^{*(k_{2})}),y)\] \[=}_{k_{1},k_{2}}L_{^{(k_{1})}}( h^{*(k_{2})})\] \[}_{k_{1},k_{2}}[L_{ ^{(k_{1})}}(h^{*(k_{1})})+d_{}(^{(k_{1})},^{(k_{2})})]\] \[=_{k_{1}}L_{^{(k_{1})}}(h^{*(k_{1} )})+}_{k_{1},k_{2}}d_{}(^{( k_{1})},^{(k_{2})})\] \[_{k_{1}}L_{^{(k_{1})}}(h^{*(k_ {1})})+_{}(_{S}),\]

where \(}_{k_{1},k_{2}}d_{}(^{(k_{1})}, ^{(k_{2})})_{k_{1} k_{2}}d_{ }(^{(k_{1})},^{(k_{2})})= {d}_{}(_{S})\) as in Definition 2. Here \((a)\) follows from the convexity assumption for \(l\) w.r.t the parameters, while \((b)\) followsfrom Lemma B.1 based on the argument below:

\[L_{^{(k_{1})}}(h^{*(k_{2})}) =L_{^{(k_{1})}}(h^{*(k_{2})})-L_{^ {(k_{2})}}(h^{*(k_{2})})+L_{^{(k_{2})}}(h^{*(k_{2})})\] \[|L_{^{(k_{1})}}(h^{*(k_{2})})-L_{ ^{(k_{2})}}(h^{*(k_{2})})|+L_{^{(k_{2}) }}(h^{*(k_{2})})\] \[ d_{}(^{(k_{1})},^{(k_{2} )})+L_{^{(k_{2})}}(h^{*(k_{2})}).\]

**Theorem 3** (Theorem 1).: _Under Assumptions 4.1 (Convexity and Smoothness), the optimal target loss is bounded by_

\[L_{tgt}^{*}_{k=1}^{K}[L_{^{(k)}}(h^{ *(k)})]+_{}(_{S}^{fed})+d_{ ,}(_{S}^{fed},_{T}),\] (14)

_where \(h^{*(k)}\) denotes the optimal local model of client \(k\) (see Section 3)._

Proof.: Since

\[L_{tgt}^{*}=L_{_{T}}(g_{T}^{*} f^{*}) L_{ ^{(k)}}(g^{*} f^{*})+d_{,}( ^{(k)},_{T}),\]

we have

\[L_{tgt}^{*} _{k}[L_{^{(k)}}(g^{*}  f^{*})+d_{,}(^{(k)},_ {T})]\] \[=L_{src}(h^{*})+d_{,}(_{S}^{fed},_{T}).\]

Using Lemma B.2, we have

\[L_{tgt}^{*}  L_{src}(h^{*})+d_{,}(_{S}^ {fed},_{T})\] \[_{k=1}^{K}[L_{^{(k)}}(h^ {*(k)})]+_{}(_{S}^{fed})+d_{ ,}(_{S}^{fed},_{T}).\]

**Lemma B.3** (Bound on round-wise source loss).: _Suppose the learning rates of all clients at round \(p\) are the same, i.e., \(_{p}^{(k)}=_{p}, k[K],p[P]\). Under Assumptions 4.2 and 4.1, we have that_

\[L_{src}(h_{p+1}) L_{src}(h_{p})-_{1}(_{ p+1})\|J_{p}\|_{2}^{2}+_{2}(_{p+1})_{p}^{2}\] (15)

_where \(J_{p}=_{k}J_{p}^{(k)},\;_{p}^{2}=_{k}\| J_{p}^{(k)}\|_{2}^{2}-\|_{k}J_{p}^{(k)}\|_{2}^{2},\) and \(_{1}()=-_{2}(),\;_{2}()=}{2}\)._

Proof.: Following the same proof idea as for Lemma B.2, we have

\[L_{src}(h_{p+1})}_{k_{1},k_{2}}L_{^{(k_{1})}}(h_{p+1}^{(k_{2})})\] (16)

By definition, we also have

\[L_{^{(k_{1})}}(h_{p+1}^{(k_{2})})=_{ ^{(k_{1})}}l(h(x,w_{p+1}^{(k_{2})}),y)\] (17)

From the update rule of GD, we can write

\[l(h(x,w_{p+1}^{(k_{2})}),y) =l(h(x,w_{p}-_{p+1}[_{w_{h}}L_{ ^{(k_{2})}}(h)_{w_{h}=w_{p}}]),y)\] \[=l(h(x,w_{p}-_{p+1}[_{w}_{^{(k_{2})}}l(h(x_{i},w),y_{i})_{w=w_{p}} ]),y)\] (18) \[=l(h(x,w_{p}-_{p+1} J_{p}^{(k_{2})}),y)\] (19)In (18), \((x_{i},y_{i})\) are sampled from \(^{(k_{2})}\), and not to be confused with \((x,y)\) sampled from \(^{(k_{1})}\).

Define \(:=-_{p+1} J_{p}^{(k_{2})}\), we can upper bound the loss in (19) by

\[l(h(x,w_{p+1}^{(k_{2})}),y) =l(h(x,w_{p}+),y)\] \[ l(h(x,w_{p}),y)+[_{w}l (h(x,w),y)_{w=w_{p}}]^{}+ ^{}.\] (20)

The inequality holds since \(l\) is assumed to have \(\)-Lipschitz continuous gradient. Combining (17) and (20), we have

\[L_{^{(k_{1})}}(h_{p+1}^{(k_{2})}) _{^{(k_{1})}}[l(h(x,w_{p} ),y)+[_{w}l(h(x,w),y)_{w= w_{p}}]^{}+^{}]\] \[=L_{^{(k_{1})}}(h_{p})+[J_{p}^{(k_{1} )}]^{}+^{}\] (21)

Next, by (16) and (21), we have

\[L_{src}(h_{p+1}) }_{k_{1},k_{2}}L_{^{(k_{1})}} (h_{p+1}^{(k_{2})})\] \[=}_{k_{1},k_{2}}[L_{^{(k_{1})}} (h_{p})-_{p+1}[J_{p}^{(k_{1})}]^{} J _{p}^{(k_{2})}.\] \[.+(_{p+1})^{2} \|J_{p}^{(k_{2})}\|_{2}^{2}]\] \[=L_{src}(h_{p})-_{p+1}\|J_{p}\|_{2}^{2}+(_{p+1})^{2}_{k_{2}}\|J_{p}^{ (k_{2})}\|_{2}^{2}.\] (22)

The last equality follows from the definition of \(J_{p}\).

Define \(_{p}^{2}:=_{k=1}^{K}\|J_{p}^{(k)}\|_{2}^{2}- \|J_{p}\|_{2}^{2}\). Then, we have

\[L_{src}(h_{p+1})  L_{src}(h_{p})-_{p+1}\|J_{p}\|_{2}^{2}+ (_{p+1})^{2}(_{p}^{2}+ \|J_{p}\|_{2}^{2})\] \[=L_{src}(h_{p})-(_{p+1}-)^{2}}{2})\|J_{p}\|_{2}^{2}+)^{2}}{2}_{p}^{2}\] \[=L_{src}(h_{p})-_{1}(_{p+1})\|J_{p}\|_{2} ^{2}+_{2}(_{p+1})_{p}^{2},\] (23)

which completes the proof. 

**Lemma B.4** (Lemma 4.1).: _Under Assumptions 4.2 and 4.1, and the cross-client statistics defined in Definition 4, after \(P\) rounds of federated pretraining one has_

\[_{tgt}^{*} L_{src}(h_{0})-_{p=0}^{P-1}_{1 }(_{p+1})\|J_{p}\|_{2}^{2}+_{p=0}^{P-1}_{2}(_{p+1}) _{p}^{2}+d_{,}(_{S}^{fed},_ {T}),\] (24)

_where \(h_{0}\) is the initial global model and \(_{2}()=}{2},\ _{1}()=-_{2}()\)._

Proof.: From Lemma B.3, we have

\[L_{src}(h_{P})  L_{src}(h_{P-1})-_{1}(_{P})\|J_{p-1} \|_{2}^{2}+_{2}(_{P})_{P-1}^{2}\] \[ L_{src}(h_{0})-_{p=0}^{P-1}_{1}( _{p+1})\|J_{p}\|_{2}^{2}+_{p=0}^{P-1}_{2}(_{p+1})_{p}^{2}.\]Following the same proof as the one outlined for Theorem 1, we obtain

\[L_{tgt}(h_{P}) L_{src}(h_{P})+d_{,}(_{S}^{ fed},_{T}).\]

Therefore,

\[_{tgt}^{*}=L_{tgt}(h_{P})  L_{src}(h_{P})+d_{,}(_{S}^{ fed}, _{T})\] \[ L_{src}(h_{0})-_{p=0}^{P-1}_{1}( _{p+1})\|J_{p}\|_{2}^{2}+_{p=0}^{P-1}_{2}(_{p+1})_{p}^{2} +d_{,}(_{S}^{fed},_{T}).\]

## Appendix C Additional Experimental Results

**Computing resources**. We used an NVIDIA GeForce RTX 3090 Ti GPU with a memory of 24247MiB. One run of experimental evaluation takes approximately \(6\) hours for CIFAR10 \(\) SVHN for the small client setting.

**Fraction of Clients Participating in FL Rounds**. Besides the results generated using \(20\%\) of the participating clients reported in the main text, we also report the results pertaining to \(10\%\) and \(100\%\) participating clients in Table 4. A larger fraction of participants helps with improving transferability, i.e., as expected, \(100\%\) participation outperforms the setting with \(10\%\) participation. However, we note that for \(20\%\) of participating clients we already reach a performance comparable to that involving \(100\%\) of the participants.

**Convergence Results.** Convergence results are plotted in Figure 1. We observe that a model pretrained via FedGTST not only offers better transferability than baselines, but also tends to converge faster during the finetuning stage (i.e., the green lines in all plots always converge faster than the grey dashed lines).

**Local Epochs**. Besides the results for single local client epochs presented in main text, here we also report the results for multiple local epochs in Table 5. A smaller number of local epochs tends to improve transferability more than a larger number of epochs, which is consistent with the intuition that FL pretraining can avoids local overfitting when using fewer epochs.

**Cross-Client Statistics**. We plot the cross-client averaged Jacobian norm \(\|J_{p}\|_{2}\) and the cross-client Jacobian variance \(_{p}^{2}\) in Figure 2. The observation is that FedGTST leads to a significantly larger \(\|J_{p}\|_{2}\) and a significantly smaller \(_{p}^{2}\) compared to FedAVG. A more detailed explanation of the findings (i.e., the reason for truncating the x-axis in the left plot, the FedGTST coefficient selection approach and faster convergence results in the right plot) is available in the caption of Figure 2.

**Additional Dataset: DomainNet**. Following FedSR, we apply a leave-one-out strategy, where one domain is treated as the target domain and the other five are source domains, allocated to five

  Method & Coefficient & \(10\%\) participants & \(100\%\) participants \\  FedAVG & NA & 0.52 \(\) 0.01 & 0.56 \(\) 0.03 \\  & 1e-4 & 0.52\(\) 0.02 & 0.58\(\) 0.01 \\ FedGTST & 5e-4 & **0.58 \(\) 0.04** & **0.63\(\) 0.05** \\  & 1e-3 & 0.53\(\) 0.01 & 0.60\(\) 0.06 \\   

Table 4: Transferability versus the fraction of participating clients in each round. We report the results for CIFAR10 \(\) SVHN on the LeNet backbone, with \(K=100\).

   & Coefficient & 1 local epoch & 10 local epochs \\  FedAVG & NA & 0.54\(\)0.05 & 0.52\(\)0.07 \\  & 1e-4 & **0.60\(\)0.01** & 0.52\(\)0.04 \\ FedGTST & 5e-4 & 0.56\(\)0.06 & **0.58\(\)0.02** \\  & 1e-3 & 0.58\(\)0.05 & 0.53\(\)0.02 \\   

Table 5: Transferability v.s. local number of epochs. We report results for CIFAR10\(\)SVHN with the LeNet backbone, for \(10\%\) of participating clients and \(K=100\).

individual clients. Results are reported in Table 6, where the target domains listed are: C (Clipart), I (Infograph), P (Painting), Q (Quickdraw), R (Real), and S (Sketch).

**Additional Baseline: Scaffold**. FedGTST consistently outperforms Scaffold, as presented in Table 7 and 8.

**Dirichlet Sampling**. We set the concentration parameter to 0.5 and the number of parties to 10 by default. The results in Table 9 and 10 indicate that FedGTST still outperforms others when individual domains are constructed via Dirichlet sampling.

    &  &  \\    & C & I & P & Q & R & \\  FedAVG & 59.3\(\)0.7 & 16.5\(\)0.9 & 44.2\(\)0.7 & 10.8\(\)1.8 & 57.2\(\)0.8 & 39.6 \\ FedSR & 61.0\(\)0.6 & 18.6\(\)0.4 & 45.2\(\)0.5 & 13.4\(\)0.6 & 57.6\(\)0.2 & 41.3 \\ FedGTST & **63.9\(\)0.5** & **20.7\(\)0.3** & **47.8\(\)0.4** & **15.2\(\)0.5** & **59.5\(\)0.6** & **43.6** \\   

Table 6: Comparison of different federated models on intra-domain transfer tasks of DomainNet. FedGTST consistently outperforms the other methods.

Figure 1: Visualization of Convergence Results. We use CIFAR10 \(\) SVHN with \(K=100\) as an example. The top two plots correspond to a fraction of \(10\%\) of participating clients, while the bottom two plots correspond to \(100\%\) participation. We report the training and test accuracy along with finetuned epochs for both settings. The grey dashed lines represent FedAVG, where the coefficient for the regularizer term is set to \(0\). Other lines represent FedGTST with tuned coefficients.

    &  &  &  \\    & LeNet & ResNet & LeNet & ResNet \\  Scaffold & 75.6 \(\) 0.8 & 80.8 \(\) 0.3 & 66.0 \(\) 0.5 & 71.1 \(\) 0.4 & 73.3 \\ FedGTST & **76.2 \(\) 0.9** & **82.3 \(\)** 0.5 & **70.1 \(\)** 0.8 & **74.5 \(\)** 0.3 & **75.8** \\   

Table 7: Comparison between Scaffold and FedGTST. Number of clients is 10.

## Appendix D Discussion of the Positivity of Coefficients Requirement for Cross-Client Statistics

**Lemma D.1** (Bound on round-wise source loss).: _Suppose the learning rates of all clients at round \(p\) are the same: \(_{p}^{(k)}=_{p}, k[K],p[P]\). When Assumption 4.2 and 4.1 hold, we have that_

\[L_{src}(h_{p+1}) L_{src}(h_{p})-_{1}(_{ p+1})\|J_{p}\|_{2}^{2}+_{2}(_{p+1})_{p}^{2}\] (25)

_where \(J_{p}=_{k}J_{p}^{(k)},\;_{p}^{2}=_{k}\| J_{p}^{(k)}\|_{2}^{2}-\|_{k}J_{p}^{(k)}\|_{2}^{2},\) and \(_{1}()=-_{2}(),\;_{2}()=}{2}\)._

_Remark_.: Lemma D.1 provides an upper bound of _current-round cross-client loss_ (the proof is given in Appendix B), indicating that a smaller _current-round cross-client loss_ (LHS) may be influenced by factors such as a small _last-round cross-client loss_, a large _cross-client average Jacobian norm_, and a small _cross-client Jacobian variance_ (RHS). More precisely, the LHS is the _cross-client current-round loss_, which is upper bounded by three terms on the RHS,

1) _The Loss Term:_\(L_{src}(h_{p}^{(0)})\) is the _last-round cross-client loss_.

2) _The Variance Term:_\(_{p}^{2}\) measuring the variance of local gradients across all nodes.

3) _The Norm Term:_\(\|J_{p}\|_{2}^{2}\) measures the squared _cross-client average Jacobian norm_.

**Positivity of the Coefficients**. It is straightforward to see that \(_{2}(_{p+1})=)^{2}}{2}\), the coefficient in front of \(_{p}^{2}\), is always positive. Therefore, we focus on \(_{2}(_{p+1})\), the coefficient in front of \(\|J_{p}\|_{2}\). Denote the RHS of Equation 25 by \(\), so that \(L_{src}(h_{p+1})\). To allow such a bound to be used as an indicator that the source loss is decreasing with the number of training rounds, we require

Figure 2: Cross-client statistics tuning via FedGTST. We use CIFAR10\(\)SVHN with \(K=100\) as an example. The left plot reports the global Jacobian (gradient) norm versus the index of the federated round. The grey dashed line represents FedAVG, while other lines correspond to FedGTST with different coefficients. We truncate the plot to only capture the results of the first \(100\) rounds, since at the end of training the gradient norm should drop to a value close to \(0\) due to convergence, and we are only interested in observing the behaviour of Jacobian norms during relative early pretraining stages. We select the best-performing setup from the left plot (the red line with coefficient \(1e-3\)), and then in the right plot, compare its variance during a federated round with that of FedAVG. The blue line represents FedAVG and the yellow line corresponds to FedGTST. The yellow line terminated earlier since all experiments are averaged over \(3\) runs and aligned with the run that converges the fastest.

    &  &  &  \\    & LeNet & ResNet & LeNet & ResNet \\  Scaffold & 52.3 \(\) 0.5 & 63.1 \(\) 0.3 & 45.5 \(\) 0.1 & 55.5 \(\) 0.3 & 54.1 \\ FedGTST & **57.5**\(\) 0.3 & **67.6**\(\) 0.2 & **52.4**\(\) 0.1 & **63.1**\(\) 0.2 & **60.2** \\   

Table 8: Comparison between Scaffold and FedGTST. Number of clients is 100.

\( L_{src}(h_{p})\), since only in this way can the bound give \(L_{src}(h_{p+1}) L_{src}(h_{p})\). Thus, we require \(_{1}(_{p+1})\) to be positive, since otherwise \( L_{src}(h_{p})\) cannot be meet. We describe in what follows that a positive \(_{1}(_{p+1})\) is indeed possible in practice.

**Realistic scenarios in which \(_{1}(_{p+1})>0\)**. To require \(_{1}(_{p+1})>0\) is equivalent to require \(_{p+1}<\). This requirement can be easily met since for any model of a known mathematical form based on a second-order differentiable loss, we can easily get the lower bound for \(\) once we observed all training data. Using linear regression as an example, where \(l(x,y;w)=(wx-y)^{2}\), we have \(l}{dw^{2}}=2x\), therefore, in this case we can simply control \(_{p+1}}\|x\|}\) to approximately guarantee \(_{p+1}<\). This then ensures \(_{1}(_{p+1})>0\).

## Appendix E Optimal Learning Rate

_Choosing the Optimal Learning Rate_. Lemma B.3 shows that the upper bound of federated loss at round \(p\) is a quadratic function w.r.t the learning rate \(_{p}\). Therefore, a good learning rate at each round needs to be chosen to minimize the upper bound. For simplicity of notation, we use \(B_{p+1}(_{p+1})\) as a shorthand for the upper bound shown in (15).

The following two observations are in place for \(B_{p+1}(_{p+1})\):

* When \(0<_{p+1}<\|_{2}^{2}}{_{k}\|J_{p}^{( k)}\|_{2}^{2}}\), it holds that \(L_{src}(h_{p+1}^{(0)}) B_{p+1}(_{p+1})<L_{src}(h_{ p}^{(0)})\), indicating that the federated loss is decreasing with the number of rounds.
* By minimizing \(B_{p+1}(_{p+1})\) w.r.t. \(_{p+1}\), we have \[_{p+1}^{*}=\|_{2}^{2}}{_{k}\|J_{p}^{ (k)}\|_{2}^{2}}, B_{p+1}(_{p+1}^{*})=L_{src}(h_{p}^{(0)} )-\|_{2}^{4}}{2_{k}\|J_{p}^{(k)}\|_{2 }^{2}}.\]

## Appendix F Stochastic Gradient Descend

Assumption 4.2 on one step of gradient descent can be extended to stochastic learning with batch sampling. The additional randomness in sampling would require incorporating the variance of batch sampling into the generalization bound. This variance term, being independent of the algorithm design, was omitted in our theoretical analysis.

    &  &  &  \\    & LeNet & ResNet & LeNet & ResNet & \\  FedAVG & 50.4\(\)0.1 & 63.0\(\)0.3 & 43.3\(\)0.2 & 54.6\(\)0.5 & 52.8 \\ FedSR & 52.7\(\)0.2 & 62.9\(\)0.3 & 44.7\(\)0.1 & 56.5\(\)0.3 & 54.2 \\ FedIIR & 54.2\(\)0.4 & 64.1\(\)0.1 & 47.4\(\)0.4 & 58.7\(\)0.2 & 56.1 \\ Fed-GTST & **59.5\(\)**0.3 & **69.2\(\)**0.2 & **55.1\(\)**0.1 & **65.6\(\)**0.2 & **62.4** \\   

Table 10: Comparison of target accuracy (%) across different methods on MNIST\(\)MNIST-M and CIFAR10\(\)SVHN tasks. 100 individual domains are constructed by Dirichlet sampling.

    &  &  &  \\    & LeNet & ResNet & LeNet & ResNet & \\  FedAVG & 74.1\(\)0.6 & 82.2\(\)0.3 & 65.2\(\)0.4 & 72.8\(\)0.9 & 73.5 \\ FedSR & 75.5\(\)0.8 & 82.0\(\)0.2 & 66.1\(\)0.5 & 72.1\(\)0.3 & 73.9 \\ FedIIR & 75.8\(\)0.2 & **82.8\(\)**0.6 & 66.5\(\)0.9 & 74.6\(\)0.1 & 74.9 \\ Fed-GTST & **77.3\(\)**0.8 & 82.7\(\)0.4 & **70.8\(\)**0.7 & **75.2\(\)**0.2 & **76.5** \\   

Table 9: Comparison of target accuracy (%) across different methods on MNIST\(\)MNIST-M and CIFAR10\(\)SVHN tasks. 10 individual domains are constructed by Dirichlet sampling.

Dataset Description

MNIST comprises \(60,000\)\(28 28\) grayscale images of handwritten digits (0 through 9); MNIST-M is created by combining MNIST digits with the patches randomly extracted from color photos of BSDS500 as their background, containing \(59,001\) training images. The CIFAR-10 dataset consists of \(60,000\)\(32 32\) colour images from \(10\) classes. SVHN contains \(73,257\)\(32 32\) colored digits obtained from house numbers in Google Street View images.

## Appendix H Non-iid FL Models

For the source domains MNIST and CIFAR-10, we only allow each local client to have access to two out of ten classes (e.g., for the digit dataset (0-9), one client may only have access to say digits 3 and 6). We let each client randomly chooses their labels and samples following a uniform distribution. See the example in Fig. 3

## Appendix I Additional Related Work

### Gradient Matching in Transfer Learning

**Discussion.** Gradient matching in transfer learning focuses on aligning gradients between source and target domains to facilitate better domain adaptation. Shi et al.  demonstrate that matching gradients across domains enhances domain generalization by making the learned representations more resilient to domain shifts. Extending this idea, Rame et al.  introduce the concept of invariant gradient variances, which helps maintain generalization performance even for out-of-distribution settings. The work in Pezeshki et al.  further highlights the impact of Hessian alignment, showing that aligning the Hessians of the source and target domains can significantly boost generalization in gradient-based methods.

**Challenges.** Applying gradient alignment techniques directly to FL presents several challenges: (1) _Privacy leakage_--these methods can potentially compromise data privacy by necessitating access to the target domain from source domains; (2) _Local overfitting_--clients train their models on local data, which can lead to overfitting within their specific domains, reducing the global model's generalization capabilities.

**Our contribution.** To address the issues described above, we propose communication schemes that ensure data privacy by preventing unrestricted access to client data. Furthermore, our approach promotes global transferability by focusing on improving generalization across all clients rather than solely enhancing local domain performance.

### Distinctions and Connections between Generalization and Transferability in FL

Both approaches address the challenge of non-iid data, and improving transferability may potentially enhance generalization across diverse local domains. However, they employ distinct models and

Figure 3: An example for constructing a non-iid marginal distribution for the _Cifar10_ dataset allocated to \(10\) clients. Each client has access to only two labels. We also make sure that no samples are used by more than one client.

evaluation datasets: (a) Generalization of FL targets performance on heterogeneous _source testing datasets_, while transferable FL aims for strong performance on a _target dataset_ that may significantly differ from _source training datasets_. (b) Heterogeneous FL utilizes the pretrained model for evaluation, whereas Transferable FL assesses the finetuned model.

The methodologies also diverge. Although reducing cross-client variance is linked to better generalization, our method distinguishes itself from traditional heterogeneous FL by enforcing a large average Jacobian norm \(|J_{p}|\) in the early stages. While a larger \(|J_{p}|\) may hinder generalization due to increased local updates and model variance, it enhances transferability by preventing premature local convergence.

Additionally, adversarially robust models offer an example where improving transferability may come at the expense of generalization. As shown in , adversarially robust models tend to have better transferability. However, since these models are designed to perform well against adversarial examples or perturbations, they do not necessarily exhibit lower generalization error on clean test data. In this scenario, transferability can be unrelated to or even conflict with generalization.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction reflect the facts that 1) we theoretically established that certain cross-client statistics can boost transferability of federated learning, and that 2) based on our theoretical findings, we proposed a novel transferable federated learning method which significantly outperforms baselines. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of our work in a separate section, Limitations. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: We provided necessary assumptions need for our theoretical analysis in the main text but deferred the proofs to the Appendix due to space limitations. We nevertheless discussed the intuition behind our findings in the main text. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The datasets are existing open-source benchmarks, and we have included our code in the Supplementary materials document. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The datasets are existing open-source benchmarks, and we have attached our code in a Supplementary materials document. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Details of the experimental work are discussed in the Experimental Setting in the Experiments section. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Our experimental results are test accuracies, and we reported the standard deviations. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Sufficient information about the compute resources (type of compute workers, memory, time of execution) is provided in the Experiments section. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted to produce the paper conformed, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: There is neither a negative or positive societal impact of the work performed. It still may have a technological and academic impact based on its results and findings. There should be no problems with the proper use of the pertinent methods/software.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no risk in this domain. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The original papers that produced the dataset used in our studies are properly cited. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper will not lead to the release of new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.