# Res-Tuning: A Flexible and Efficient Tuning Paradigm

via Unbinding Tuner from Backbone

Zeyinzi Jiang

Alibaba Group 2National University of Singapore 3Ant Group

 Chaojie Mao

Alibaba Group 2National University of Singapore 3Ant Group

Ziyuan Huang

Alibaba Group 2National University of Singapore 3Ant Group

Ao Ma

Alibaba Group 2National University of Singapore 3Ant Group

Yiliang Lv

Alibaba Group 2National University of Singapore 3Ant Group

Yujun Shen

Alibaba Group 2National University of Singapore 3Ant Group

Deli Zhao

Alibaba Group 2National University of Singapore 3Ant Group

Jingren Zhou

Alibaba Group 2National University of Singapore 3Ant Group

###### Abstract

Parameter-efficient tuning has become a trend in transferring large-scale foundation models to downstream applications. Existing methods typically _embed_ some light-weight tuners into the backbone, where both the design and the learning of the tuners are highly dependent on the base model. This work offers a new tuning paradigm, dubbed Res-Tuning, which intentionally _unbinds_ tuners from the backbone. With both theoretical and empirical evidence, we show that popular tuning approaches have their equivalent counterparts under our unbinding formulation, and hence can be integrated into our framework effortlessly. Thanks to the structural disentanglement, we manage to free the design of tuners from the network architecture, facilitating flexible combination of various tuning strategies. We further propose a memory-efficient variant of Res-Tuning, where the bypass (_i.e._, formed by a sequence of tuners) is effectively detached from the main branch, such that the gradients are back-propagated only to the tuners but not to the backbone. Such a detachment also allows one-time backbone forward for multi-task inference. Extensive experiments on both discriminative and generative tasks demonstrate the superiority of our method over existing alternatives from the perspectives of efficacy and efficiency. Project page: https://res-tuning.github.io/.

## 1 Introduction

Recently, foundation models have demonstrated strong generalization capability across numerous visual , language  and multi-modal tasks . Pre-trained on a large corpus of data, a foundation model offers a good initialization for downstream adaptation. Unfortunately, the increasing model scale makes it expensive and almost infeasible to fully fine-tune such a model for every task. Hence, parameter-efficient transfer learning (PETL)  is often resorted to as an efficient approach for downstream adaptation without incurring an unaffordable computation burden.

Popular existing approaches for parameter-efficient tuning introduce additional tunable structures (which we term as _tuners_) to the pre-trained base model . Compared to the fully-fine-tuned counterparts, the light-weight tuners significantly reduce the training cost while maintaining a competitive performance . However, the current designs of tuners are deeply coupled with their base structures, as shown in Fig. 0(a), thus restricting the design flexibility and impeding the extension to new approaches. For example, prefix tuning  is embedded into the self-attention operation, and prompts  could only be introduced at the beginning or between layers, _etc._

In this work, we introduce Res-Tuning, a new tuning paradigm for flexible and efficient transfer learning. As in Fig. 0(b), our Res-Tuning framework unbinds the tuners from the base model, such that it is possible to decouple both the design and the learning of the tuners from the base structure. Sincethe design of the tuners is no longer dependent on the base structure in our Res-Tuning framework, we explore various possibilities. With both theoretical and empirical evidence, we first show that our framework can seamlessly encompass popular tuning approaches such as prefix-tuning , prompt-tuning , and adapters . Further, it is demonstrated that the structural disentanglement also allows for flexible combination of various tuners, leading to the discovery of stronger tuning strategies. On top of that, we show that such an unbinding formulation also allows for the detachment of the tuners from the backbone as in Fig. 1c, which further improves the memory efficiency. In this memory-efficient version, _i.e.,_ Res-Tuning-Bypass, not only is the training cost reduced because the gradient computation through the massive parameters is avoided in the base model, but it also reduces the number of forward passes of the backbone model to only once during multi-task inference.

We evaluate Res-Tuning framework on both discriminative and generative tasks. On discriminative tasks, we show that our unbinding formulation leads to a tuning strategy that achieves state-of-the-art performance on VTAB-1K with similar learnable parameters. Our Res-Tuning-Bypass framework also performs favorably against the fully-fine-tuned variant, while reducing the training memory consumption by 49.7% and the multi-task inference time by 80.9%. In addition, it also obtains better performance in few-shot learning and domain generalization scenarios. On generative tasks, apart from the strong performance achieved by Res-Tuning framework in terms of both FID scores and qualitative results, we further show that our Res-Tuning-Bypass can reduce the memory consumption by 70.7%, training time by 58.6%, and multi-task inference time by 83.6% when maintaining a competitive FID score and generation quality compared to the fully-fine-tuned variant.

## 2 Unbinding parameter-efficient tuning

In order to reduce the entanglement between the base model and the tuners as well as to increase the flexibility of the tuning strategies, we set out to unbind the tuners from the pre-trained structures. In this section, we provide our unbinding formulation to existing parameter-efficient transfer learning strategies. We start by revisiting the basic building blocks of the foundation models, before diving into unbinding existing tuners from the backbone. In the last part of this section, we further provide empirical proof that our unbinding formulation could seamlessly encompass existing PETL methods like prompt tuning , prefix tuning , and adapters .

### Basic building blocks of foundation models

Existing foundation models in both natural language processing, vision, and vision-language applications mostly adopt Transformers  as the backbone. The major building blocks of the Transformers that one usually adapts for the downstream tasks are the multi-head attention (MHA) and the feed-forward network (FFN). Formally, the standard MHA and FFN could be expressed as:

\[(,,)=(^ {}}{})\,\] (1)

Figure 1: **Concept comparison** between existing methods and our Res-Tuning framework. (a) Existing PETL methods are deeply embedded into original structures. (b) Our Res-Tuning framework can decouple PETL methods from the backbone. (c) Backpropagation only through a bypass consisting of Res-Tuner can be achieved by detaching the connections.

where \(\), \(\) and \(\) denote the query, key and value, respectively. \(_{1}\) and \(_{2}\) are the projection weights, \(_{1}\) and \(_{2}\) are the bias terms, and \(\) is the non-linear activation between fully-connected layers. Usually, given input tokens \(\), the query, key, and value are obtained through a linear projection \(=_{q}\), \(=_{k}\), and \(=_{v}\), where \(_{q}\), \(_{k}\) and \(_{v}\) are learnable projection weights.

### Unbinding tuners from foundation models

For the adaptation of the foundation models to downstream applications, the existing PETL approaches mostly resort to adjusting the output of MHA, FFN, or the Transformer block composed of MHA and FFN in various ways. We choose popular and exemplary approaches and unbind their structures from foundation models. Here, we provide the unbinding formulations of prefix tuning  and prompt tuning  for MHA adaptation, as well as adapter tuning  for FFN adaptation.

**Prefix tuning** prepends learnable parameters, _i.e._, prefix tokens, to the projected keys and values:

\[}=(_{q},[_{pre};_ {k}],[_{pre};_{v}]),\] (2)

where \(_{pre}\) and \(_{pre}\) are prefix tokens. Essentially, if we view this as performing MHA separately between \((,,)\) and between \((,_{pre},_{pre})\), we unbind prefix tuning as follows:

\[}=(1-)(,, )}_{}+ (,_{pre},_{pre})}_{},\] (3)

where \(\) weighs between the original and prefix attention. Detailed value for \(\) and the derivation process are included in appendix A. In this way, the original MHA in the foundation model \((,,)\) and the prefix attention \((,_{pre},_{pre})\) can be computed independently. The unbinding formulation of prefix tuning can be seen in Fig. 1(a).

**Prompt tuning** appends latent tokens to the input token before performing MHA in the backbone:

\[}=([;_{pro}]_{q},[ ;_{pro}]_{k},[;_{pro}]_{v}),\] (4)

where \(_{pro}\) are prompt tokens concatenated to the input token \(\) in the first layer or between multiple layers. Similar to prefix tuning, the unbinding formulation of prompt tuning is as follows:

\[}=[(1-)(,, )}_{}+ (,_{pro},_{pro})}_{};],\] (5)

where \(_{pro}=_{pro}_{k}\) and \(_{pro}=_{pro}_{v}\). \(\) denotes disposable parts that would not affect the output of MHA, where \(=(1-)\ (_{pro},_{pro},_{pro} )+\,(_{pro},,)\). \(\) and \(\) are individual weights. More details can be seen in appendix A. The unbinding formulation of prompt tuning can be seen in Fig. 1(b).

**Adapter tuning** typically inserts a multi-layer perceptron (MLP) after FFN. Since the MLP could be performed independently, we simply re-route the adapter and connect it in parallel to the FFN as in Fig. 1(c). The resultant unbinding formulation of the adapters is as follows:

\[}=()}_{}+()_{down})_{up}}_{},\] (6)

where \(_{down}\) and \(_{up}\) denote the weights for the down- and up-projection layers, respectively.

Figure 2: **The original and the unbinding form** of (a) prefix tuning , (b) prompt tuning , and (c) adapter tuning  for parameter-efficient transfer learning.

### Empirical equivalency of the unbinding formulation

After we have obtained the unbinding formulation of popular PETL methods with theoretical derivation, Tab. 1 shows empirical evidence of the equivalency between the original formulation and our unbinding formulation. We carry out the comparison between two formulations on CIFAR-100, using ViT  pre-trained on ImageNet-21K and by CLIP . For all cases, we observe that the performance discrepancy between the original and our unbinding form is within a reasonable range (\(\) 0.03), which shows that our formulation encompasses existing PETL methods effortlessly.

## 3 Res-Tuning

### Unified formulation of Res-Tuning

Given the unbinding formulation of the existing PETL methods, we can derive a unified formulation as the combination of the frozen pre-trained operation and the tuner with learnable parameters:

\[^{}=()+(),\] (7)

where OP denotes existing operations in the pre-trained backbone such as MHA and FFN, while the Res-Tuner represents the learnable structures that are connected in parallel to the existing operations.

With this unified unbinding formulation, the design of the tuner structure can now be independent of the original operation in the base model. This leads to unprecedented flexibility in parameter-efficient tuning, which enables us to explore various instantiations of the tuner (as in Fig. 2(a)) for the base structures. For example, we found in Tab. 1(a) that, instantiating the Res-Tuner with prompt tuning for FFN results in a better performance compared to adapters.

Further, the structural disentanglement also allows for the flexible combination of various tuning strategies. In Fig. 2(b), we instantiate our Res-Tuning framework by associating one Res-Tuner with every operation in the base model, including MHA, FFN, and the whole Transformer block.

### Res-Tuning-Bypass: Towards memory-efficient PETL

The unified formulation of Res-Tuning provides a viable solution for flexible and parameter-efficient transfer learning. However, since it is directly derived from existing PETL methods, it shares the

    &  &  \\ Method & Original & Unbinding form & \(\) & Original & Unbinding form & \(\) \\  Adapter  & 92.34 & 92.34 & 0.00 & 92.43 & 92.44 & +0.01 \\ Prefix  & 91.90 & 91.88 & -0.02 & 90.99 & 91.01 & +0.02 \\ Prompt  & 92.21 & 92.24 & +0.03 & 91.86 & 91.83 & -0.03 \\   

Table 1: **Empirical equivalence of PETL methods and their counterparts in our unbinding form.**

Figure 3: **Structure illustration of (a) various Res-Tuners in our unbinding form, (b) our Res-Tuning framework that is flexible and efficient, and (c) Res-Tuning-Bypass, the memory-efficient version of our framework.**same vulnerability as existing PETL solutions. Despite the parameter efficiency, they require back-propagation through the massive parameters in the pre-trained backbone, which leads to unnecessary extra consumption of memory and computation.

To avoid this, we further present a memory-efficient version of our Res-Tuning framework, dubbed as Res-Tuning-Bypass, as in Fig. 2(c). Specifically, we remove the data flow from the Res-Tuner to the backbone, such that the Res-Tuner is detached from the pre-trained architectures. In this way, we form a bypass network constructed by Res-Tuners in parallel with the backbone. Formally, given the tokenized feature \(_{0}\) and the output feature of the \(l\)-th layer \(_{l}\) from the pre-trained model, our bypass network is formulated as:

\[_{0}^{} =_{0},\] (8) \[_{l}^{} =(_{l})+(1-)(_{l-1}^{}),l 1,\]

where \(\) is a learnable parameter followed by a sigmoid function, which is initialized to 0.5. As demonstrated in Fig. 2(c), we group the Res-Tuners in the bypass network into horizontal ones and vertical ones, respectively processing the output feature of the \(l\)-th layer from the backbone and the \((l-1)\)-th feature from the bypass network. Within each group, we keep the structure identical. Thanks to the flexibility of our unbinding formulation, we can also explore different instantiations of the Res-Tuner and various combinations of the existing tuners.

## 4 Empirical evaluations on discriminative tasks

### Experimental setups

**Evaluation scenarios.** We mainly analyze the flexibility and efficiency of our proposed Res-Tuning framework and evaluate the discriminative capabilities on three different scenarios: _transfer learning_, _few-shot learning_, and _domain generalization_.

**Baselines.** Apart from the traditional downstream adaptation methods like fully fine-tuning and linear probing, we divide existing tuning approaches into two categories: **(i)** methods focusing on parameter-efficiency, including adapter tuning , prefix tuning , VPT , LoRA , AdaptFormer , SSF , and NOAH; **(ii)** methods focusing on memory-efficiency, including Side-Tuning , and LST . Since these two categories have distinct characteristics with respect to the parameters, memory, and performance, we mainly compare our Res-Tuning framework within the former category, while the Res-Tuning-Bypass is mainly compared in the latter one.

**Implementation details.** For most experiments, we adopt ViT-B/16  pre-trained on ImageNet-21K  as the backbone model, following VPT . Unless otherwise specified, the middle of the adapter, as well as the number of prefix and prompt tokens in our Res-Tuning are set to 10 for parameter efficiency. We include the training details in appendix C. For all the tasks, we use top-1 accuracy as the main evaluation metric.

### Analysis on flexibility

**Flexible combination between backbone structures and tuners.** Since our unbinding formulation allows for the structural disentanglement between the frozen structure in the backbone and the tuner with learnable parameters, it enables us to explore various combinations of the operation and the tuners. Here, we experiment with various instantiations of OP and Res-Tuner in our Res-Tuning framework, and the number of tuners is limited to one tuner per block (Single-Res-Tuner). The results are presented in Tab. 1(a). We found that the default combination in existing approaches (prefix and prompt for MHA adaptation and adapter for FFN adaptation) is far from optimal, and connecting prompt tuning to FFN results in the best performance.

**Flexible combination of multiple tuning strategies.** Next, we show that the flexibility brought by our unbinding formulation could also effortlessly lead to stronger tuning strategies by combining various tuners in our unbinding formulation. Here, we consider two tuners per block (Dual-Res-Tuner), respectively connected to MHA and FFN. As in Tab. 1(b), employing two tuners for each block brings notable improvements over the Single-Res-Tuner variants, with employing two adapters respectively for MHA and FFN achieving the strongest performance of 93.25. On top of the best performing Dual-Res-Tuner model, we further attach tuners at the block level in Tab. 1(c). Withadapters on top of the Dual-Res-Tuner model, we observe further slight improvements on the classification performance. Compared to existing tuning strategies of underlining in Tab.(a)a, without bells and whistles, the Tri-Res-Tuner version of our Res-Tuning framework achieves at least 0.94% performance improvement.

### Analysis on parameter-, memory- and multi-task inference-efficiency

**Parameter-efficiency.** We analyze the parameter efficiency of our Res-Tuning framework in Tab.(a)a. Our Single-Res-Tuner version surpasses the performance of the fully-fine-tuned variant with less than 0.2% learnable parameters. Combining all three tuners on MHA, FFN, and block-level, we manage to outperform the fully-fine-tuned and linear evaluated variants by respectively 4.16% and 7.33%, while only using 0.67M parameters.

**Memory-efficiency.** Here, we present the evolution from linear probing to our Res-Tuning-Bypass in Tab.(b)b. It is observed that introducing a bypass network without any tuners can help ensemble the original features obtained from different layers, thereby mildly improving the classification accuracy as compared to the linear probing approach where only the classifiers are trained. On top of that, both horizontal and vertical Res-Tuner bring notable performance improvement with limited parameter and memory overhead. With both horizontal and vertical Res-Tuners in place, our Res-Tuning-Bypass framework achieves stronger performance with only 52% of the memory consumption when compared with the fully-fine-tuned variant.

**Multi-task inference-efficiency.** In Fig. 4, our Res-Tuning-Bypass demonstrates superior multi-task inference-efficiency on both discriminative and generative tasks. For discriminative multi-task inference, we combine the validation set of 19 tasks in VTAB-1K, perform 19 tasks on every image, and obtain the overall process time for the whole validation set. For generation multi-task inference, we take one image and provide the model with 10 fixed-length prompts for generation and record the overall generation time for the 10 prompts. For existing parameter-efficient methods, the inference time grows linearly when the number of tasks grows. Compared to the fully fine-tuned variant, all existing parameter-efficient tuning strategies increase the inference time to various extents. In contrast, our Res-Tuning-Bypass framework significantly reduces the inference time on 19 discriminative tasks and 10 generative tasks by respectively 80.9% and 83.6% when compared with the fully-fine-tuned variant.

Table 2: **Exploration of various combinations of operations in the pre-trained backbone and various Res-Tuners achieves a stronger performance compared to the existing tuning strategies on CIFAR-100. Adapter, prefix, and prompt are abbreviated as Ada., Pre. and Pro., respectively.**

Table 3: **In-depth analysis of our Res-Tuning and Res-Tuning-Bypass in terms of performance, parameter-efficiency and memory efficiency on CIFAR-100.**

### Comparisons with existing tuning strategies on different scenarios

**Transfer Learning.** We mainly evaluate our approach on the basic transfer learning scenario, where pre-trained models are fine-tuned on different downstream tasks. CIFAR-100  is a standard general-purpose image classification dataset. VTAB-1K  is composed of 19 various visual classification tasks falling into three categorizations, _i.e._, natural, specialized, and structured. We compare the performance of our approach and other baseline methods on the following:

_CIFAR-100._ We present the comparisons to existing tuning strategies in Tab. 3c. For parameter-efficient methods, our Res-Tuning framework notably improves over AdaptFormer  by 1.39%, using only 0.55% extra parameters, which is 0.91% less than AdaptFormer . For memory-efficient methods, we outperform LST  by 0.61% with memory reduced by 0.54G (10%).

_VTAB-1K._ In Tab. 4, we present comprehensive evaluation on the 19 datasets on the VTAB-1K benchmark. Both our Res-Tuning and Res-Tuning-Bypass outperform existing approaches respectively within the group of parameter- and memory-efficient tuning methods. Specifically, our Res-Tuning framework achieves a 0.85% improvement in terms of the average performance on 19 datasets, compared to the previous best performance. For our Res-Tuning-Bypass, we outperform LST  in 18 out of 19 tasks and overall by 4.99% in terms of average performance. This is achieved with even fewer parameters and memory consumption. We further visualize the parameter vs. performance curve and memory vs. performance curve in Fig. 4 to show the advantage of our Res-Tuning and Res-Tuning-Bypass framework on VTAB-1K.

**Few-Shot Learning.** To evaluate the ability of our approach to adapt with only a few training samples, we follow the few-shot evaluation protocol in NOAH , using {1, 2, 4, 8, 16}-shots for training and full test data. We conduct experiments on five fine-grained datasets, including FGVC-Aircraft , Food-101 , Oxford Flowers , Oxford Pets , Stanford Cars .

    &  &  &  \\   &  &  &  &  &  &  &  &  &  &  &  &  &  \\  Full & 68.9 87.7 64.3 & 97.26 & 86.9 87.4 38.8 & 79.7 95.7 58.4 & 72.3 9.6 & 56.3 58.6 & 41.7 65.5 & 57.5 46.7 25.7 & 29.1 & 68.9 65.5 57.8 & 58.4 9.40 \\ Linear & 63.4 85.0 63.2 & 97.0 86.3 & 36.6 51.0 & 78.5 57.5 68.6 & 74.0 & 34.3 & 30.6 33.2 & 55.4 & 12.5 20.0 & 9.6 & 19.2 & 57.64 52.94 & 0.04 & 3.09 \\   \\  Adaptper  & 74.2 85.7 62.7 97.8 & 87.2 36.4 50.7 & 77.6 9.9 & 72.3 75.7 & 71.6 & 45.2 & 41.8 31.1 & 56.4 & 30.4 26.6 & 13.2 & 22.0 & 60.52 56.35 & 1.82 & 6.53 \\ LoRA  & 67.1 91.4 69.4 98.8 90.4 85.3 & 54.0 84.9 95.3 & 84.4 73.6 & **82.9** 62.9 & 42.9 48.8 & 78.5 75.7 & 47.1 31.0 & 44.0 & 74.60 72.30 & 0.29 & 6.88 \\ VPT-Deep  & **78.8** 90.8 65.8 90.8 & 87.3 78.4 96.8 & 81.9 **96.1** 83.6 & 84.8 & 68.5 & 60.0 46.5 & 72.8 73.6 & 47.9 **32.9** 37.8 & 71.96 69.43 & 0.60 & 8.13 \\ SSF  & 69.0 92.6 **75.1** **99.4** 91.8 **80.2** **92.7** **94.5** 98.9 & **74.5** 98.9 & **74.5** 75.5 & 79.6 & 73.7 54.9 & 92.5 & 37.9 & 75.69 & 73.10 & 0.24 & 7.47 \\ NOAH  & 69.6 **92.7** **70.2** 99.1 **90.4** 86.1 & 75.4 84.9 & **83.9** **75.8** & **68.2** 83.9 & **69.9** **81.7** 81.8** & **48.3** 3.2 & **84.2** & **75.3** 73.25 & 0.24 & 4.27 \\ Res-Tuning & 75.2 **92.7** 71.9 & 93.9 **91.9** 86.7 & **85.8** & **86.7** 95.6 & 85.0 74.8 & 80.2 & 63.6 5 0.6 & 80.2 **85.4** **55.7** & 31.9 & 42.0 & **76.32 & **74.10** & 0.55 & 8.95 \\   \\  Side-Tuning  & 60.7 60.8 53.6 95.6 67.3 & 94.9 35.3 & 58.7 65.2 61.0 & 27.6 22.6 31.5 & 51.7 8.2 & 14.4 9 & 8.2 & 12.8 & 49.91 45.65 & 9.59 & 3.48 \\ LST\({}^{}\)  & 58.0 87.1 62.9 & 89.7 63.2 & 52.6 & 81.9 9.2 78.5 & 69.4 & 68.5 6 & 56.1 & 38.8 & **73.4** & 72.9 & 30.5 & 16.6 & 31.0 & 67.56 & 64.52 & 0.89 & 5.13 \\ Res-Tuning-Bypass & **64.5** **85.8** & **73.2** **99.4** **90.6** **63.5** **57.2** & **85.5** **92.2** **74.5** & **70.4** **61.0** & **40.2** **65.8** **79.2** & **52.6** **2.6** **2.0** & **49.3** & **72.3** & **69.5** & **30.24 & 4.73 \\   

* denotes our own implementation since the original approach is proposed for natural language processing.

Table 4: **Performance and efficiency comparison** on the VTAB-1K benchmark with ViT-B/16 models pre-trained on ImageNet-21K. “Group Mean” denotes the average accuracy of the three subgroups. “All Mean” denotes the average accuracy of 19 downstream tasks.

Figure 4: **Comparisons of the parameter-, memory-, and multi-task inference-efficiency**. For multi-task inference-efficiency, we evaluate Res-Tuning-Bypass on both discriminative and generative tasks. For parameter- and memory-efficiency, here, we show the comparisons on VTAB-1K between our approach and existing tuning strategies.

As the results are shown in Fig. 5, in terms of the overall performance (top-left), both Res-Tuning and Res-Tuning-Bypass demonstrate clear advantages over other corresponding parameter-efficient and memory-efficient tuning strategies of few-shot learning on five FGVC datasets. We also observe that Res-Tuning-Bypass performs as well as or even better than the non-memory-efficient methods on one or two shots with low training samples on serveral datasets.

**Domain Generalization.** To evaluate the robustness of our approach to distribution shift, we train a model on the source domain using 16 shots per category and test it on both the source and target domain. The source domain uses ImageNet-1K ) and the target domains use four other variants of ImageNet, including ImageNet-V2 , ImageNet-Sketch , ImageNet-A , ImageNet-R . The results in Tab. 5 prove that our approach is robust under domain shift. Our Res-Tuning goes beyond NOAH  by 6.54% on ImageNet and 1.7% on the average accuracy of four variants of ImageNet. Furthermore, Res-Tuning-Bypass also demonstrates stronger robustness than the memory-efficient baselines and outperforms most existing parameter-efficient tuning methods.

## 5 Empirical evaluations on generative task

### Experimental setups

**Downstream tasks.** To provide a more comprehensive evaluation of our Res-Tuning framework, we further apply it to the text-to-image generation task. Following , we evaluate the text-to-image

Figure 5: **Results of few-shot learning on five fine-grained visual recognition datasets. The solid line represents the comparison of parameter-efficient tuning methods, and the dashed line represents the comparison of memory-efficient tuning methods. All results are averaged over 3 random seeds.**

    & Source &  \\  & ImageNet & IN-V2 & IN-Sketch & IN-A & IN-R & Mean \\   \\ Adapter  & 70.5 & 59.1 & 16.4 & 5.5 & 22.1 & 25.8 \\ VPT  & 70.5 & 58.0 & 18.3 & 4.6 & 23.2 & 26.0 \\ LoRA  & 70.8 & 59.3 & 20.0 & 6.9 & 23.3 & 27.4 \\ NOAH  & 71.5 & 66.1 & 24.8 & 11.9 & 28.5 & 32.8 \\ Res-Tuning & **78.04** & **66.58** & **29.23** & **13.15** & **29.01** & **34.50** \\   \\ Side-Tuning  & 74.57 & 62.52 & 23.55 & 10.37 & 25.06 & 30.38 \\ LST  & 70.00 & 57.04 & 14.39 & 7.21 & 17.02 & 23.92 \\ Res-Tuning-Bypass & **77.30** & **65.23** & **27.39** & **10.66** & **26.45** & **32.43** \\   

Table 5: **Results on domain generalization. “Mean” denotes the average accuracy of four variants of ImageNet. All results are averaged over 3 random seeds.**generation performance on COCO2017 dataset . The main quantitative evaluation metric is the FID score and we also perform instance-level fine-tuning transfer on the fine-grained datasets [4; 55].

**Baselines.** We experiment with the version 1.5 of stable diffusion  (SD) model. On COCO2017, we compare our Res-Tuning framework with zero-shot SD, SD with fully fine-tuning as well as SD with other existing tuning methods. On the fine-grained datasets, we employ DreamBooth  as our baseline and employ various tuning methods including our own for comparison.

### Main results

**Text-to-image generation on COCO.** We show the comparison between our Res-Tuning framework and other approaches both quantitatively and qualitatively. The quantitative comparison is presented in Tab. 6. Compared with the fully fine-tuned baseline, our Res-Tuning framework improves the FID score by 0.89, using only 0.29% of the parameters. It is noteworthy that our Res-Tuning framework is the only approach that reaches a FID score below 14, while the best existing tuning strategy on the task is LoRA  achieving 14.50 with 4x the number of parameters in Res-Tuning. Hence, employing Res-Tuning could greatly facilitate the adaptation of pre-trained text-to-image generation models to downstream tasks.

A highlight is observed that our Res-Tuning-Bypass framework reduces the memory consumption for tuning the SD v1.5 model to only 29% while maintaining a similar performance (14.89 vs 14.85). Meanwhile, the time consumption for training the model is reduced to 41%. In terms of the reduction in the memory and time consumption, our Res-Tuning-Bypass framework outperforms the best tuning strategy by 3.3x (70.7% memory reduction of Res-Tuning-Bypass vs. 25.3% that of adapter) and 1.7x (58.6% reduction in time consumption of Res-Tuning-Bypass vs. 34.3% that of adapter), respectively.

The qualitative results are presented in Fig. 6. Both Res-Tuning and Res-Tuning-Bypass show a better alignment between the text and the generated image, where the surfboard is propped up in our generated images. Res-Tuning also demonstrates a better fidelity where the feather texture is realistic in the generated black bird.

   Method & FID & Param. (M) & Mem. (GB) & Train (Hour/Epoch) \\  SD v1.5 & 15.48 & - & - & - \\ + Full & 14.85 & 862 (100\%) & 72.77 & 1.98 \\  + LoRA & 14.50 & 9.96 (1.15\%) & 61.03 & 1.42 \\ + Adapter & 14.73 & 2.51 (0.29\%) & 54.30 & 1.30 \\ + Prefix & 15.36 & 4.99 (0.58\%) & 64.91 & 2.20 \\ + Prompt & 14.90 & **1.25** (0.14\%) & 63.70 & 2.17 \\  + Res-Tuning & **13.96** & 2.54 (0.29\%) & 54.49 & 1.38 \\ + Res-Tuning Bypass & 14.89 & 3.76 (0.44\%) & **21.35** & **0.82** \\   

Table 6: **Comparison of FID and efficiency on COCO2017.** Following the default settings of stable diffusion1, we sample 10k captions from the validation set for generating images of size \(512^{2}\) using 50 PLMS steps with classifier-free guidance scale 3.0 and compare against the full validation set.

Figure 6: **Qualitative results** of SD v1.5, existing tuning strategies and our Res-Tuning on COCO2017 validation set . We frame our results in green and others in red.

**Text-to-image generation on Oxford Flowers and Food-101.** Here, we evaluate the transfer ability of existing tuning strategies on specific fine-grained categories. During every evaluation process, we select data from one specific category to train the model. The qualitative results are demonstrated in Fig. 7. It is observed that Res-Tuning presents a better view in terms of fidelity and the correct understanding of ambiguous categories. For example, the balloon flower is bell-shaped and has five petals, while existing approaches generate flowers with the wrong number of petals or even balloons rather than real flowers. Instead, both our Res-Tuning and our Res-Tuning-Bypass retain correct semantics and fine-grained features.

## 6 Related work

**Transformers in computer vision.** Transformers  have demonstrated strong capabilities in various fields [5; 61; 12; 59; 2; 17; 48; 53]. In computer vision, Vision Transformers (ViT)  are widely applied in various applications, such as visual classification [48; 39], object detection [67; 6], segmentation [84; 74] and generation [63; 57]. Owing to the strong scalability of the Transformer backbone [31; 10], recent endeavors either focus on expanding the size of the ViT  or training on a larger corpus of data in an unsupervised manner [75; 14].

**Parameter-efficient tuning.** Despite the strong performance and generalization ability brought by scaling up the Transformers, it also makes the adaptation to the downstream tasks expensive and almost infeasible. Hence, parameter-efficient transfer learning (PETL) emerged [26; 25; 76; 41; 85; 28]. Existing PETL methods could be generally categorized into three types: (i) MHA-based tuning embeds tunable parameters in the multi-head self-attention layers [27; 76; 28; 41; 45; 46]. (ii) FFN-based tuning methods represented by adapters  and its generalized versions [58; 33; 32; 19] introduce a multi-layer perceptron to the FFN layer. (iii) Other tuning methods adapt certain existing parameters [79; 42]. Closely related to our work, some recent efforts are devoted to finding out the optimal design paradigm of the tuning modules by neural architecture search . Instead, we show that through our Res-Tuning framework, a stronger tuning strategy can be easily found by the flexible combination of several existing tuning strategies in our unbinding form.

**Memory-efficient tuning.** Since the structures of the existing PETL methods are deeply embedded in the backbone structure, back-propagation is required through the massive parameters of the pre-trained models, leading to unnecessary extra consumption of memory and computation. Hence, Side-Tuning  and LST  in natural language processing connect a side network in parallel with the pre-trained model to avoid data flow from the trainable parameters to the frozen ones. Our Res-Tuning-Bypass is inspired by the conceptual idea of these approaches. Compared to Side-Tuning and LST, we show that our Res-Tuning-Bypass is more flexible and memory-efficient.

## 7 Conclusion

In this work, we unbind the tuners from the backbone and form a flexible and efficient tuning paradigm Res-Tuning. With Res-Tuning, we are able to find stronger tuning strategies compared to existing ones. On top of Res-Tuning, we further extend a memory-efficient Res-Tuning-Bypass, which significantly reduces the memory consumption and multi-task inference cost. We hope our discoveries can facilitate further research in the flexible and efficient tuning of large foundation models.

Figure 7: **Qualitative results** of SD v1.5, DreamBooth, existing tuning strategies, and our Res-Tuning on Oxford Flowers and Food-101 fine-grained dataset with the same generated seed. We frame our results in green and others in red.