# FindingEmo: An Image Dataset for Emotion Recognition in the Wild

Laurent Mertens\({}^{1,2}\) &Elahe' Yargholi\({}^{3}\) &Hans Op de Beeck\({}^{3}\)

Jan Van den Stock\({}^{4}\) &Joost Vennekens\({}^{1,2,5}\)

\({}^{1}\)KU Leuven, De Nayer Campus, Dept. of Computer Science

J.-P. De Nayerlaan 5, 2860 Sint-Katelijne-Waver, Belgium

\({}^{2}\) Leuven.AI - KU Leuven Institute for AI, 3000 Leuven, Belgium

\({}^{3}\)Department of Brain and Cognition, Leuven Brain Institute,

Faculty of Psychology & Educational Sciences

KU Leuven, 3000 Leuven, Belgium

\({}^{4}\)Neuropsychiatry, Leuven Brain Institute

KU Leuven, 3000 Leuven, Belgium

\({}^{5}\)Flanders Make@KU Leuven, 3000 Leuven, Belgium

laurent.mertens@kuleuven.be

###### Abstract

We introduce FindingEmo, a new image dataset containing annotations for 25k images, specifically tailored to Emotion Recognition. Contrary to existing datasets, it focuses on complex scenes depicting multiple people in various naturalistic, social settings, with images being annotated as a whole, thereby going beyond the traditional focus on faces or single individuals. Annotated dimensions include Valence, Arousal and Emotion label, with annotations gathered using Prolific. Together with the annotations, we release the list of URLs pointing to the original images, as well as all associated source code.

## 1 Introduction

Computer vision has known an explosive growth over the past decade, most notably due to the resurgence of Artificial Neural Networks (ANNs). For many vision-related tasks, computer models have been developed that match or exceed human performance, e.g., image classification  and mammographic screening . Many of these tasks, however, are relatively simplistic in nature: detecting the absence or presence of an object, or naming an item in the picture. When it comes to more complex tasks, Artificial Intelligence (AI) still has a long way to go. Affective Computing , a field that combines disciplines such as computer science and cognitive psychology to study human affect and attempt to make computers understand emotions, is an example of such a complex problem. This paper is concerned in particular with the subtask of Emotion Recognition, i.e., building AI models to recognize the emotional state of individuals, in our case from pictures. This problem has many applications, ranging from psychology , to human-computer interaction , to robotics . It is, however, complex: in the field of psychology, the concept of what an emotion _is_ exactly is heavily debated [7; 8; 9], resulting in several ways of describing emotions, either by means of continuous dimensions [10; 11], or by means of labels, with different competing label classification schemes existing [12; 13; 14].

The application of computer vision techniques toward Emotion Recognition has historically largely focused on detecting emotions from human facial expressions, with the problem still being actively investigated . However, the importance of _context_ in emotion recognition is increasingly being acknowledged in psychology . This led to the release of the computer vision dataset EMOTIC , presenting photos of people in natural settings, rather than face-focused close-ups, and leading the way to more complex ANN systems that attempt to combine multiple information streams extracted from these images .

Nevertheless, even these more recent efforts focus on the emotional state of one particular individual within the picture. In this paper, we present the FindingEmo dataset, which is the first to target higher-order social cognition. The dataset was developed as part of an interdisciplinary project in which researchers from the fields of Psychology, Psychiatry and Computer Science investigate the use of ANNs to simulate Social Cognition, as a way to better understand the corresponding mechanisms in the human brain, and how these mechanisms are affected by conditions that correlate with atypical social behavior, in particular ASD and FTD . Each image in the dataset depicts multiple people in a specific social setting, and has been annotated for the _overall_ emotional content of the _entire_ scene, instead of focusing on a single individual. We hope this data can be used by AI practitioners and psychologists alike to further the understanding of Emotion Recognition, and more broadly, Social Cognition. This is a complex process, consisting of many layers. Consider, e.g., the photograph depicted in Figure 1. Looking only at the bride's face, one could easily assume she is very sad, or even distressed. Taking also her wedding gown into account, a positive setting is suddenly suggested; perhaps her tears are tears of joy? Only when looking at the full picture does it become clear that the bride is overcome with emotion in a positive way, as conveyed by the setting, the groom reading a prepared text and the clearly supportive bystanders. Thus, full understanding of the bride's emotional state requires the full scene, including the groom and the solemnly smiling bystanders. This example illustrates how Social Cognition involves detection of relevant elements, extracting relations among these and attributing meaning to construct a coherent whole.

The source code for the scraper and annotation interface used to create the dataset are available from our dedicated repository1, together with the URLs of the annotated images and their corresponding annotations. To mitigate the issue of broken URLs, we provide multiple URLs for a same image whenever possible, and are continuously expanding the set of images for which multiple URLs are provided (about 10k so far). For copyright reasons, we do not share the images themselves. More information with regard to legal compliance can be found in SSA.2.

The data collection process was approved by the KU Leuven Ethics Committee.

The remainder of the paper is structured as follows. In Section 2 the data collection process and dataset are described in detail. Next, baseline results for emotion classification and valence and arousal regression problems based on popular ImageNet ANN architectures, as well as Visual Transformers CLIP and DINOv2, are presented in Section 3. We build upon this by investigating the effect of merging the features and predictions of several models in Section 4. Finally, we conclude with a discussion in Section 5.

Figure 1: Photo courtesy The Kitcheners ([https://thekitcheners.co.uk/](https://thekitcheners.co.uk/)).

Dataset Description

The dataset is split into a publicly released set of annotations for 25,869 unique images, and a privately kept set of 1,525 images.2 Each image depicts multiple people in various, naturalistic, social settings. We follow Emotic  in creating a training (=our public) set with one annotation per image, and a test (=our private) set with multiple annotations per image. In total, 655 participants--a short description of whom can be found in SSA.8--contributed annotations. In what follows, we list the most important annotation dimensions; for a full list, see SSA.3.

Valence and ArousalWe used Russell's continuous Valence and Arousal dimensions , with integer scales \([-3,-2,,3]\) for Valence and \([0,1,,6]\) for Arousal. Arousal was named "Intensity" in our annotation interface, as we felt "Arousal" might carry a sexual connotation for some users.

EmotionUsers had to pick an emotion from Plutchik's discrete Wheel of Emotions (PWoE) , shown in Figure 2. We opted for this particular emotion classification scheme as it strikes a balance between the more limited and sometimes contested Ekman's 6 , and the more expansive, and potentially more confusing, Geneva Emotion Wheel . It defines 24 primary emotions, grouped into 8 groups of 3, with emotions within a group differing in intensity. It is depicted as a flower with the 24 emotions organized in 8 leaves and 3 concentric rings. Each leaf represents a group of 3, with opposite leaves representing opposite emotions.

The rings represent the intensity levels, from most intense at the center to least intense at the outside. An additional advantage of PWoE is that one can easily opt to use all 24 emotions, or instead limit oneself to the 8 groups, allowing some granularity control. We refer to these choices as "Emo24" and "Emo8" respectively, and refer to the groups as "emotion leaves".

### Positioning Versus Existing Datasets

Although research in automated Emotion Recognition has been gaining in popularity over the years, progress is still hampered by a lack of data. Earlier work tended to focus solely on recognizing emotions from faces. In their recent review paper, Khare et al.  list no less than 21 publicly available datasets of facial images for this purpose, typically annotated with Ekman's 6, potentially extended with a "neutral" category, or custom defined emotion categories. Some of the more popular such datasets, like JAFFE  and CK+ , make use of a limited number of actors (resp. 10 and 123) who were instructed to act out a certain emotion, resulting in caricatural emotional expressions.

Publicly available datasets going beyond the face are few in number. First, there is EMOTIC , a 23,571 image dataset depicting people in the wild, and with natural expressiveness. An explicit goal of EMOTIC is to take context into account when assessing a person's emotional state. One or more individual subjects are delineated by a bounding box in each picture for a total of 34,320 subjects, each annotated for Valence, Arousal, Dominance and one of 26 custom defined emotion categories.

CAER-S is a dataset of 70,000 stills taken from 79 TV shows. The stills were extracted from 13,201 video clips that were annotated for Ekman's 6 + neutral. Each still contains at least one visible face. The aim of the dataset is to allow augmenting facial emotion recognition with contextual features.

Similar to EMOTIC, there is HECO, a dataset of 9,385 images taken from previously released Human-Object Interaction datasets, films and the internet. Like EMOTIC, 19,781 individual subjects were

Figure 2: Plutchik’s Wheel of Emotions.

annotated in the pictures for Valence, Arousal, Dominance, 8 discrete emotion categories comprised of Ekman's 6 + Excitement and Peace, and two novel dimensions, Self-assurance and Catharsis.

Table 1 groups these dataset descriptions, together with ours, for easy comparison.

### Dataset Creation Process

The creation of the dataset was split into two phases. The first phase focused on gathering a large set of images, _prioritizing quantity over quality_. The second phase consisted of collecting the annotations. We present a brief summary of both phases here, and refer to SSA.4 for more details.

Phase 1Images were gathered using a custom built image scraper that generates random search queries, each consisting of three terms selected from predefined lists of, respectively, emotions, groups of people (e.g., "adults", "seniors", etc.) and social settings/environments. For each query, the first \(N\) results were retrieved, filtered and downloaded. As obviously not all downloaded images satisfied our criterion of depicting multiple people in a natural setting, one particular filtering step involved labeling and classifying images as being either "keep" (useful) or "reject" (no use). In total 1,041,105 images were collected.

Phase 2Annotations were gathered using a custom web interface (see SSA.5 for a screenshot). Annotators were recruited through the Prolific3 platform, and first required to agree to an Informed Consent clause, followed by detailed instructions (see SSA.6 for a copy). To monitor the process closely, we performed many (51, to be exact) runs, each with a limited number (around 10 to 15) of participants. For each run, the Prolific user selection criteria were the same: fluent English speaker, (self-reported) neurotypical4, and a 50/50 split male/female. Candidates were informed of a total expected task duration of 1h, and offered a PS10 reward. Analysis of the durations (see SSA.7) show our time estimation to be fair. In total, data collection costs were PS10k, including fees and taxes.

### Annotator Grading and Annotator Overlap

To assess the reliability of annotators, we used a set of 5 fixed images, referred to as "fixed overlap images", chosen specifically for being unambiguous.5 For each image, a default annotation was defined consisting of the "keep/reject" choice (4 keeps, 1 reject), Valence (value range), Arousal (value range) and Emotion (emotion leaf). This results in 4 datapoints per image, or 20 datapoints in total. Annotators' submissions for these images were compared to the reference, earning 1/20 point per matching datapoint, resulting in a final "overlap score" \(s\). Users with \(s>=0.8\) were automatically accepted. An alternative score \(s_{alt}\) was computed which ignored those overlap images whose reference value was "keep", but were annotated as "reject". The reason for this is that it quickly became clear that despite the system providing a "Skip" option in case users rather not annotate a certain image, some chose to "reject" these images instead. Also, one of the "keep" images shows a bit of text, which users were instructed to reject. Some users were more strict than others in applying this rule.

We defined a system parameter \(p_{R}\) that controls when overlap images (i.e., images already annotated by others) are shown to users. For each new image request, an overlap image

 Name & Nb. images & Image source & Annotation target & V/A/D & Emotions scheme & Reference \\  EMOTIC & 23,571 & COCO + Ade20k + inter- & Single person & V/A/D & 26 custom emotion categories &  \\ CAER-S & 70,000 & TV Shows & Single person (face visible) & – & Ekman’s 6 + neutral &  \\ HECO & 9,385 & HICO-DET + V-COCO + & Single person & V/A/D & Ekman’s 6 + Excitement and Peace &  \\ FindingEmo & 25,869 & Internet & Whole image & V/A & Plutchik’s Wheel of Emotions & This paper \\ 

Table 1: Comparison of relevant datasets. “V/A/D” indicates which of the Valence, Arousal and Dominance dimensions were annotated.

is served with probability \(p_{R}\), starting with the 5 fixed overlap images, in a fixed sequence. Once these are annotated, the system serves other, non-fixed, already annotated images. At first, these were randomly chosen from all annotated images, but this resulted in too many images with only 2 annotations. Hence, we created a process that limits the pool of images to choose from, and attempts to strive for 5 annotations per (non-fixed) overlap image. Using this system, we obtained a dataset with 80.9/19.1 split single label/multi-label annotations. These multi-label images make up the private set. Detailed inter-annotator statistics on this private set are reported in SSA.9, indicating that for 26.2% of the images, all annotators agreed on the emotion leaf, while for 46.6% of the images two labels were given. Out of these two-label annotations, 42.8% refer to adjacent emotion leafs. Annotators agree less on Arousal (average min-max difference of 2.7 \(\) 1.4) than on Valence (average min-max difference of 1.8 \(\) 1.2). Importantly, average Valence disagreement plateaus close to 2 with increasing number of annotations per image, while a linearely increasing trend is apparent for Arousal.

### Statistics and Observations

This section presents statistics for the 8 leaves of PWoE.

For the full 24 emotions, see SSA.10.

Figure 3 shows the distribution of annotations per emotion leaf. An imbalance is obvious, with in particular "joy" and "anticipation" being overrepresented, and "surprise" and "disgust" heavily underrepresented, despite an added balancing mechanism (see SSA.4.2). A similar imbalance is found in popular facial expression datasets, such as FER2013  (only 600 "disgust" images versus nearly 5,000 for other Ekman's 6 labels) and AffectNet  (134,915 "happy" faces, 25,959 "sad" faces, 14,590 "surprise" faces, 4,303 "disgust" faces). Although EMOTIC  uses custom emotion labels, making a one-to-one comparison more difficult, it is also heavily skewed towards positive labels (top 3: "engagement", "happiness" and "anticipation"; bottom 3: "aversion", "pain" and "embarassement"). Compared to these other datasets, ours exhibits less imbalance.

In Table 2, we group average annotation values for Arousal, Valence and Ambiguity per emotion leaf. Figure 20 in SSA.10 plots the distribution of Arousal and Valence annotations per emotion leaf, showing clear tendencies toward normal distributions, validating the use of averages and standard deviations. As expected, perceived "negative" emotions ("fear", "sadness", "disgust" and "anger") have a negative average Valence, with the inverse being true for "positive" emotions ("joy", "trust"). Somewhat undecided are "surprise" and "anticipation", which can go either way. The highest Arousal values are reserved for "anger, "sadness" and "fear". We hypothesize the unexpectedly high Arousal value for "sadness" might be due to naming this dimension "Intensity" in our interface; although a grieving person is generally considered to have low arousal, the emotion of sadness itself is felt intensely. Further analysis on the full emotion set reported in SSA.10 verifies that also at this more fine-grained level, annotations conform to expectations, with Arousal levels increasing along with the intensity level of the PWoE ring, and Valence levels analogously increasing for "positive" and decreasing for "negative" emotions.

Figure 4: Association between Valence and Arousal values. The bigger the disc, the more often the (Valence, Arousal)-pair appears in the dataset.

Figure 3: Distribution of Emotion annotations for the public set per Plutchik emotion leaf.

Figure 4 shows the association between Arousal and Valence annotations, indicating as expected a collinearity between higher Arousal values and the extremes of the Valence range. Scatterplots per opposite Emo8 pairs are grouped in Figure 21 in SSA.10.

### Cross-cultural Analysis

Most Prolific users participating in our task shared their country of birth and ethnicity with us (see A.8). To verify to what extent annotations are consistent accross people of different backgrounds, we performed the following two experiments.

To check the consistency between geographic regions, we first mapped countries of birth to the geographic regions they are embedded in. For pairs of regions with at least 100 common images (i.e., images annotated by members of both regions), we analysed the distribution of, and agreement between Arousal, Valence and Emo8 annotations. Seven pairs were left: Central Europa (C.Eur.)-Southern Africa (S.Afr.), C.Eur-Western Europe (W.Eur.), Eastern Europe (E.Eur.)-S.Afr, E.Eur-W.Eur, North America (N.Am.)-S.Afr, N.Am-W.Eur. and S.Afr.-W.Eur. We computed the similarity between the distributions, as well as inter-annotator agreement between annotation vectors for each region in the pair, where the average annotation value was used in case of multiple annotations from the same region, with results grouped in Figure 17 and Table 4, SSA.9. For all pairs and all annotation dimensions, the Jensen-Shannon (JS) distance between the distributions stayed within the range \([0.040,0.229]\), and all passed the two-sample Kolmogorov-Smirnov (KS) test (\(p>0.95\)) except for Arousal in C.Eur.-S.Afr. and N.Am.-S.Afr., and Valence in N.Am.-S.Afr. Spearman's R between all pairs and dimensions was significant (\(p 0.05\)) and varied in the range \([0.170,0.617]\), except for Arousal in C.Eur.-W.Eur. (\(0.102\), \(p=0.203\)) and N.Am.-S.Afr. (\(0.116\), \(p=0.168\)). Highest values were observed for Valence, lowest for Arousal. Overall, although there are differences between regions, tendencies are clearly similar.

We performed the same experiment based on users' ethnicities, resulting in 5 ethnicity pairs: Black-Mixed, Black-Other, Black-White, Mixed-White and Other-White. The resulting plots and metrics are grouped in Figure 18 and Table 5 respectively, both in SSA.9. Interestingly, for Arousal 3 out of 5 pairs fail the KS test, namely Black-Mixed, Black-Other and Other-White. For Emo8, all pairs pass the test, while for Valence only Black-Other fails it. For both Valence and Emo8 all pairs have a significant Spearman's R (\(p<0.001\)) in the range \([0.433,0.590]\) for Valence and \([0.255,0.346]\) for Emo8, while for Arousal Spearman's R is not significant for Black-Mixed (\(-0.061\), \(p=0.476\)) and Black-Other (\(0.112\), \(p=0.220\)). In short, Arousal annotations appear more consistent among geographic regions than among ethnicities, although it is important to note that the low number of datapoints does not allow for strong conclusions.

## 3 Baseline Model Results

Baseline results are obtained by applying transfer learning to popular ImageNet-based ANN architectures AlexNet , VGG16 , ResNet 18, 50 and 101  and DenseNet 161 .6 For each, we use the default PyTorch implementations and weights, and replace the last layer with a new output layer that matches the chosen task (see below). Only this last layer is trained. We do the same experiment for some of these same architectures trained from scratch on the Places365 dataset , using the official PyTorch models. We also consider EmoNet , a model for labeling images with one out of 20 custom emotion labels reflecting the emotion elicited in the observer, obtained by applying transfer learning to AlexNet and trained on a private database. In this case, we first process

  & Joy & Trust & Fear & Surprise & Sadness & Disgust & Anger & Anticipation \\  Nb. & \(7026\) & \(3549\) & \(2401\) & \(888\) & \(2665\) & \(1000\) & \(2439\) & \(5001\) \\ Arousal & \(2.96^{0.96}\) & \(2.57^{1.09}\) & \(3.24^{1.24}\) & \(2.57^{1.41}\) & \(3.42^{1.29}\) & \(2.44^{1.23}\) & \(3.59^{1.17}\) & \(2.46^{1.21}\) \\ Valence & \(1.90^{0.96}\) & \(1.41^{1.09}\) & \(-1.34^{1.24}\) & \(0.48^{1.41}\) & \(-1.57^{1.29}\) & \(-0.88^{1.23}\) & \(-1.58^{1.17}\) & \(0.56^{1.21}\) \\ Ambiguity & \(1.58^{1.66}\) & \(1.88^{1.64}\) & \(2.09^{1.61}\) & \(2.39^{1.68}\) & \(1.84^{1.66}\) & \(2.22^{1.65}\) & \(1.99^{1.63}\) & \(2.15^{1.61}\) \\ 

Table 2: Average Arousal, Valence and Ambiguity annotation values for the public set, per emotion leaf. Format \(x^{y}\): \(x\) = average, \(y\) = standard deviation.

the image with EmoNet, and then send the resulting 20-feature vector through a new linear layer. We use the EmoNet PyTorch port by the main author7. Lastly, we also use Visual Transformer models CLIP  (ViT-B/32) and DINOv2  (ViT-B/14 distilled with registers)8, using both models to obtain embeddings for input images, and like with EmoNet, use these as input to a single linear layer.

We distinguish three tasks: _Emo8 classification_, where we predict one of the 8 primary emotions defined by the emotion leaves of PWoE; _Arousal regression_, where we predict the numerical arousal value; _Valence regression_, where we predict the numerical valence value.

For classification, we apply a softmax to the output of the final layer. Target values for regression problems are reduced to the range \(\) using an appropriate linear rescaling. Hence, we apply a sigmoid function to the model output. Network outputs are transformed back to the original problem domain by using the inverse scaling.

Preprocessing for ImageNet models consisted in scaling images to an 800x600 resolution, keeping the original ratio and centering and padding with black borders where necessary, followed by normalization using default ImageNet pixel means and standard deviations. For Places365 and EmoNet models, we followed the preprocessing steps described in the respective papers. For CLIP, we use the default preprocessing chain that comes with the model, and for DINOv2 we use the same preprocessing as for the ImageNet models, but with a rescaling to 798x602.

For each task, and each model, we trained 10 models per starting learning rate \(_{0}\) and per loss function \(\). For classification, we used \(_{0}[10^{-1},10^{-2},10^{-3},10^{-4}]\) and \([,]\); for regression we used \(_{0}[10^{-3},10^{-4},10^{-5},10^{-6},10^{-7}]\) and \([,]\). UnbalancedCrossEntropyLoss is a novel extension of the traditional CrossEntropyLoss, created to allow giving different weights to different misclassifications. WeightedMSELoss is a natural extension of MSELoss that takes into account class imbalance. Full technical details for both can be found in SSA.11.

All experiments use the _public_ dataset, Adam loss with default PyTorch parameter values, and the custom lr update rule \(_{e}=_{0}}}{{}}\), with \(_{e}\) the learning rate at epoch \(e\). By virtue of the floor division (\(//\)), this means we update the learning rate once every 3 epochs. The data was randomly split 80/20 train/test, making sure that each target label was also split according to this same rule.

Figure 5: Test data baseline performance on the Emo8 classification and Arousal and Valence regression tasks. Metrics are: Weighted F1 (W.F1) and Average Precision (AP) for classification, and Mean Absolute Error (MAE) and Spearman R correlation coefficient (S.R) for regression. The starting learning rate and loss corresponding to each model are displayed above the training bars. (U)CE = (Unbalanced)CrossEntropyLoss, (W)MSE = (Weighted)MeanSquaredError loss, p365 = original model trained on Places365 dataset.

Reported metrics are: for _classification_, Average Precision (AP)--as computed using the scikit-learn package--and Weighted F1 (W.F1); for _regression_, Mean Average Error (MAE) computed in the original problem domain, and Spearman Rank Correlation (S.R). Training stopped when either the epoch with the best loss (or the best W.F1 score for classification) on the test set lies 6 epochs behind the current epoch, or 250 epochs were reached, with the corresponding best model put forward as the final trained model. Only results for the \((_{0},)\)-combination yielding the best average Weighted F1 or Mean Average Error performance over the corresponding 10 models are reported.

All our experiments were implemented in Python using PyTorch, and split over an Intel Xeon W-2145 workstation with 32GB RAM and two nVidia GeForce RTX 3060 GPUs with 12GB VRAM, and an Intel i7-12800HX laptop with 32GB RAM and an nVidia GeForce RTX 3060 Laptop GPU with 12GB VRAM. Test results are plotted in Figure 5, with the graph for train data, and tables containing the numerical results grouped in SSA.12. In order to speed up training, we buffered model activations whenever possible.9

Apparent from these results is that these are hard problems. ImageNet-trained models slightly outperform their Places365-trained counterparts. This suggests that the natural object features extracted from the ImageNet dataset are more salient toward emotion recognition than are place-related features. In 9 out of 13 cases, our UnbalancedCrossEntropyLoss has the edge over regular CrossEntropyLoss. Predicting Arousal appears more difficult than predicting Valence, which aligns with lesser annotator agreement for Arousal than Valence, as analyzed in SSA.9. As for the architectures, VGG is a clear winner, with ResNet second. Although twice as large, ResNet101 performs very similar to ResNet50. The larger depth of the DenseNet model does not translate in better performance. A breakdown of model performance per Emo8 class can be found in SSA.12, showing overall best performance on "joy" and "anger". Worst performance is registered for "surprise" and "disgust" which, perhaps not surprisingly, are also the emotions for which the least annotations are available.

Interestingly, as explored in SSA.14, when a model deviates from the target Emo8 annotation there is a strong tendency toward "nearby" emotions. Most often this is the adjacent leaf, with more distant leaves increasingly more unlikely. This behavior is reminiscent of the kind of disagreements we find among our human annotators (see SSA.9).

## 4 Beyond the Baseline

To build upon the baseline established in Section 3, we built multi-stream models by applying the popular technique of late fusion [24; 25; 26; 27]. Concretely, we combine streams by concatenating their corresponding feature or output vectors, and sending the resulting vector through an extra linear layer. This section reports results for Emo8 classification; the analogous discussion for Arousal and Valence regression can be found in SSA.13.

We consider the following streams for combinations: _Emo8 predictions_: for each considered architecture, we trained an Emo8 model, and took the predictions from this model as an 8-feature vector; _Baseline features_: we take the model features from the penultimate layer, vector size depends on the architecture; _EmoNet predictions_: applying the model gives us a 20-feature vector (see Section 3); _YoLo v3 trained on Open Images + Facial Emotion Recognition (OIToFER)_: we apply YoLo v310, using LightNet , to each image and extract the detected "Human face" regions with probablity \(p>0.005\). We then apply the FER2013-trained ResNet18 model by X. Yuan11 to the extracted faces, resulting in a 7-feature vector per face. We generate two 7-feature vectors from this, one containing the vector averages, the other the standard deviations, and concatenate both to obtain a final 14-feature vector; _Places365 ResNet18 predictions_: applying the ResNet18 model trained on the Places365 dataset gives us a 365-feature vector per image; _Places365 ResNet18 features_: we take the model activations from the penultimate layer, giving us a 512-feature vector.

The experimental setup is identical to Section 3, except that for time considerations, we only consider CrossEntropyLoss.12 The test results for Emo8 classification are shown in Figure 6. Training results, as well as numerical training and test results, are included in SSA.13. A first observation is that improving upon the baseline appears non-trivial; except for VGG16, the obtained gains are modest. Second, the highest gains clearly come from adding facial emotion features. Third, even though adding EmoNet and OIToFER features separately has a positive effect for VGG16, adding both together does not result in a compounded improvement. Fourth, the added dimensionality of concatenating features instead of predictions in the case of Places365 does not result in markedly different results, in some cases even leading to worse results. Finally, not a single stream combination resulted in improved performance for CLIP and DINOv2, with the best VGG16 results nearing CLIP/DINOv2 performance.

## 5 Discussion

FindingsThe analysis of our dataset shows the annotations to conform to expectations, with Valence and Arousal values following the expected trends. Furthermore, when annotators disagree on the emotion label, they tend to choose nearby emotions in PWoE nonetheless. Our experiments show that, for the Emo8 prediction task on our dataset, modern ViT models do not seem to really outperform older CNN architectures, with VGG16 even (slightly) outperforming DINOv2 when both baselines are augmented with Facial Emotion features. For Arousal and Valence prediction however, the ViT models are clearly superior.

Limitations1) While images in our private set have multiple annotations, we have followed the approach of Emotic  and gathered only a single annotation per image in our public set. This choice has allowed us to gather a larger data set, but may cause concerns about reliability. These concerns are alleviated by the clear tendency observed on the private set toward similar emotions in case of multiple labels (SSA.9), combined with trained models exhibiting this same tendency to strongly favor nearby emotion leaves when deviating from the annotation (SSA.14). In short: the models trained using single annotations showed similar statistics to the human multi-label annotations. 2) Concerning potential biases in the images themselves, as they were scraped from the internet the dataset inherits the same biases the internet exhibits. In particular, we have not performed any analysis concerning potential representation issues. As such, there is an unverified possibility that models trained on our dataset wrongly associate "negative" emotions more strongly with certain minority groups. 3) Since legal issues (see SSA.2) prevent us from sharing the actual images, we had to resort to sharing URLs. While URLs can break, we mitigate this risk by offering multiple different URLs for the same image where possible.

Impact StatementThis paper presents work whose goal is to advance the fields of Machine Learning, Psychology and Psychiatry. Our own interest lies with non-commercial applications with

Figure 6: Test data results for extensions beyond the baseline by applying late fusion with Facial Emotion Recognition predictions (OIToFER), EmoNet predictions (EmoNet) and Places365 (P365) predictions or features. For all models, predictions on the dataset (Emo8) are concatenated and sent through a linear layer, except when ‘(f)’ is shown, indicating model features are concatenated. The starting learning rate corresponding to each model is displayed above the training bars.

respect to the understanding of Emotion Recognition and Social Cognition in individuals, and how these can be affected by neurological conditions. In particular, we hope that our (future) work will be of help in assisting people with impaired Social Cognition to navigate life.

Nevertheless, the data, and possible future Machine Learning advances inspired by it, could very well lead to commercial (e.g., personalized ads tailored to one's mood) and surveillance (e.g., general crowd monitoring, detection of aggression within crowds, etc.) applications that we strongly feel warrant a public debate with regard to their desirability, and even legality.

Furthermore, the use of web-scraped images entails that not only our dataset risks inheriting biases present on the web, but that our dataset contains images of and by people (i.e., subjects and authors) that would not necessarily agree to their likeliness or work being used for the purposes described in this paper. For this reason, we offer an opt-out option to anyone who wants their likeliness or work removed from out dataset.

ConclusionWe present FindingEmo, a dataset of 25k image annotations for Emotion Recognition that goes beyond the traditional focus on faces or single individuals, and is the first to target higher-order social cognition. The dataset creation process has been discussed in detail, and the annotations have been shown to align with expectations. A cross-cultural analysis of the annotations was performed, showing similar tendencies between regions and ethnicities for Valence and Emo8, with Arousal annotations somewhat less aligned. It is however important to note that the limited amount of datapoints does not allow to make strong, definitive statements. Baseline results are presented for Emotion, Arousal and Valence prediction, as well as first steps to go beyond the baseline. These results show the dataset to be complex, and the tasks hard, with even modern models like CLIP and DINOv2 struggling. This suggests that in order to solve these tasks, novel Machine Learning roads might need to be explored. Our annotation interface and code for model training are made open source.