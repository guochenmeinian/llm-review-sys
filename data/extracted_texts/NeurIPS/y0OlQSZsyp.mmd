# Learning Causal Models under Independent Changes

Sarah Mamedche

CISPA Helmholtz Center

for Information Security

sarah.mamedche@cispa.de &David Kaltenpoth

CISPA Helmholtz Center

for Information Security

david.kaltenpoth@cispa.de &Jilles Vreeken

CISPA Helmholtz Center

for Information Security

jv@cispa.de

###### Abstract

In many scientific applications, we observe a system in different conditions in which its components may change, rather than in isolation. In our work, we are interested in explaining the generating process of such a multi-context system using a finite mixture of causal mechanisms. Recent work shows that this causal model is identifiable from data, but is limited to settings where the _sparse_ mechanism shift hypothesis  holds and only a subset of the causal conditionals change. As this assumption is not easily verifiable in practice, we study the more general principle that mechanism shifts are _independent_, which we formalize using the algorithmic notion of independence. We introduce an approach for causal discovery beyond partially directed graphs using Gaussian process models and give conditions under which we provably identify the correct causal model. In our experiments, we show that our method performs well in a range of synthetic settings, on realistic gene expression simulations, as well as on real-world cell signaling data.

## 1 Introduction

Most of statistical learning theory rests on the assumption that all data is i.i.d. and comes from a single distribution. In reality, this assumption rarely holds: observations often stem from heterogeneous environments, different time domains, or interventional experiments. This results in non-i.i.d. data which may contain spurious correlations, such that naive analysis could be subject to serious learning bias . Recent research tackles this problem via causal discovery, where given different context distributions, we want to discover robust, causal relationships that transfer to novel conditions . Existing approaches however commonly discover only an equivalence class of the true model [4; 5; 6; 7; 8; 9]. Here, we investigate under which conditions we can identify causal directions.

To determine causal orientations, Scholkopf et al.  proposed the Sparse Mechanism Shift (SMS) hypothesis, which asserts that distribution shifts are a consequence of a small number of causal mechanism changes. It relies on the idea that causal functions exhibit robustness over different contexts, whereas spurious correlations typically fail to generalize to modified conditions [3; 10; 11]. While the SMS assumption permits causal identification via counting of distribution shifts [12; 13], this approach is limited to settings where only a subset of the causal conditionals change, does not apply in the i.i.d. case, and needs _conditional independence_ tests in practice, which suffer from a multiple testing problem and, depending on the use case, limited power [14; 15]. To address these issues, we propose a functional modeling approach based on _algorithmic independence_ and give identifiability guarantees under general assumptions. Instead of counting mechanism changes, our approach identifies the model that achieves the most succinct lossless description of the data.

To illustrate this idea, we consider three variables in five environments (Fig. 1). The environments correspond to different conditions that we observe our system in, such as interventional experiments in genomics [16; 17]. In our example, there exists some mechanism responsible for generating \(Y\) from its cause \(X\) which we call \(f\). It remains the same in Contexts 1-4, even when the underlying causal mechanisms and hence distributions of the covariates \(X,Z\) differ (\(f_{1}\)), and only changes inContext 5 as a result of a direct intervention on \(Y\) (\(f_{2}\)). Intuitively, to most concisely describe the data, we will need two functions in the causal direction, fewer would underfit, and more overfit. To capture this, we propose a score that quantifies how succinctly a model can describe the observed distributions. We show in Fig. 1(b) that it identifies the ground truth (indicated by a star symbol).

To explain why an anti-causal model from \(Z\) to \(Y\) (red) fits the data less effectively than the causal one (green), we compare two cases. Fig. 1(b) depicts the case where we change the generating process of covariates \(X\) and \(Z\) in only one context, while in Fig. 1(c) we do so in all five contexts. Changing these processes does not directly affect how \(Y\) is generated, and hence the true causal model permits equally succinct descriptions for both cases (green curve). In the anti-causal direction, however, such an invariant does not exist; in fact, by changing the distribution of \(X\) and \(Z\) in all five contexts we created a situation where five different functions are needed to describe \(Y\) from \(Z\), and none of these can as cleanly separate signal from noise as in the causal direction. As a result, we see much worse description lengths for the anti-causal direction (red curve). We conclude that the causal mechanism for \(Y\) offers a succinct explanation because it is _independent_ of the mechanisms for \(X\) and \(Z\). This connection between the independence of mechanisms and succinct descriptions will be the focus of our analysis.

Our contributions are as follows. We show how the algorithmic model of causation can precisely specify which kinds of causal models are permissible for a set of variables observed over multiple contexts. Based on this framework, we propose a scoring criterion under Gaussian process models . We show that our score is theoretically sound by giving general conditions under which it identifies causal models beyond partially directed graphs. Based on our theory, we introduce the LINC approach for **L**earning causal models under **IN**dependent **C**hanges. In empirical evaluations, we confirm that LINC works in practice, and show that in contrast to competitors, it is robust in various settings including semi-synthetic gene expression data  as well as real-world protein-signaling data . We include all proofs, code, and data in the supplementary material.

## 2 Causal Model and Assumptions

We begin by stating our assumptions and causal model. Throughout this work, we consider a set of continuous variables \(X^{d}\). We observe these in multiple contexts \(c\). For a given set of contexts \(\), we can represent the assignment of samples to contexts using a categorical variable \(C\). We write \(X^{(c)}\) for a dataset in one context. Whereas the data points \(x X^{(c)}\) are i.i.d. samples from a distribution \(P(X c)\), the overall data is not necessarily identically distributed. We aim to understand this data through a causal model, as we describe next.

Figure 1: Given non-i.i.d. data from different contexts, we are interested in the mechanism \(f\) that generates a variable \(Y\) and that may _change_ in one or more contexts (a). We consider a varying number of causal functions \(f\), modeled through a Gaussian process, in the causal (green) and anticausal (red) direction as potential models and measure how succinctly the models encode the data \((L)\). Here, we find that \(Y\) has cause \(X\) and a small number \(|f|\) of different mechanisms (green star). As the causal mechanism of \(Y\) changes _independently_ of the mechanisms generating \(X\) and \(Z\), this holds regardless of whether \(X,Z\) undergo distribution shifts in one context (b) or in all contexts (c).

### Problem Setting and Problem Statement

We assume that we can describe the true causal structure over the observed variables by a directed acyclic graph (DAG) \(G=(X,)\) with node set \(X\) and edges \((X_{i},X_{j})\) whenever \(X_{i}\) causes \(X_{j}\). As is standard in causal discovery , we assume that the relationship between \(X_{i}\) and its parents is given by a function \(f\),

\[X_{i}^{(c)}=f_{i}^{(c)}(_{i}^{G},N_{i})N_{i}\!\!\!( _{i}^{G},C)\]

for each context \(c\). We hereby assume causal sufficiency, i.e. that no unobserved variables exist. Whereas the structural equations depend on the context, the causal structure \(G\) is shared in all contexts. Previous work shows that we can discover \(G\) up to an equivalence class by integrating a context variable \(C\) into conventional causal discovery methods. In particular, the \(\)-Markov Equivalence class (\(\)-MEC) of \(G\), a variant of MECs to encompass interventions, is identifiable .

While graphical models are essential, we place emphasis on understanding the generating process of each variable. For a given variable \(X_{i}\), we consider a mixture of \(m\) different causal mechanisms \(f\). Importantly, each may apply to one context or remain invariant over multiple contexts. To model this situation, let \(_{i}=\{_{1},...,_{m}\}\) be a partition of \(\) into disjoint, nonempty sets \(\) such that

\[f_{i}^{(c)}=f_{i}^{(c^{})}_{i}(c)=_{i}(c^{})\,\]

where \(_{i}(c)\) is the unique set \(_{i}\) containing \(c\). We thus write \(f_{i}^{()}\) for the invariant function \(f_{i}^{(c)}\) when \(c\), and similarly \(X^{()}\) for the pooled data from all contexts \(c\). To ease notation, we write its distribution as \(P(X) P(X C)\). This results in a set of functions

\[f_{i}^{(_{i})}=\{f_{i}^{(_{1})},...f_{i}^{(_{m})}\}\.\]

We obtain such a partition of the contexts for each variable and denote the set over those partitions by \(=\{_{i}:X_{i} X\}\). Our structural equations can thus be written as

\[X_{i}^{(c)}=f_{i}^{()}(_{i}^{G},N_{i})c\] (1)

for each \(_{i}\), with independent noise \(N_{i}\!\!\!(_{i}^{G},C)\). We remark that by assuming the noise \(N_{i}\) to be independent of the environment, we follow how related work characterizes environments  as differing in distribution due to changes in the conditionals \(P(X_{i}_{i}^{G})\), independently of the exogenous noise variables \(N_{i}\), for example, due to a soft intervention on \(X_{i}\).

With the above, we can now informally state the problem that we want to solve.

**Problem Statement** (Informal).: _Given data \(X\) over environments \(\) we want to find_

* _a (sparse) graph_ \(G\)__
* _a set_ \(\) _of partitions_ \(_{i}\) _of_ \(\) _into (few) disjoint sets_
* _a set of (simple) functions_ \(f_{i}^{}\)__

_describing the causal mechanism generating our data \(X\)._

To formalize this problem, we next explain the principle of independence of causal mechanisms.

### Algorithmic Independence of Causal Mechanisms

We base our approach on the principle of Independence of Causal Mechanisms (ICM, ). Different types of independence have been proposed in the literature. Most previous work considers statistical independence , but the conditional independence constraints present in observational data only permit the identification of DAGs up to a MEC unless we make restrictive assumptions . This has sparked interest in algorithmic independence as an alternative formulation . We introduce this concept here and extend it to multi-context distributions.

With the idea that causal mechanisms offer simple explanations of complex phenomena, Janzing and Scholkopf  postulate that they correspond to simple programs computing each variable \(X_{i}\) from its causes \(_{i}^{G}\), where programs generating different variables \(X_{i},X_{j}\) do not share information.

To make this precise, we revisit several concepts. Kolmogorov complexity \(K(x)\) defines the length of the shortest program \(p\) that computes a string \(x\{0,1\}^{*}\) on a universal Turing machine \(\) and halts . For a distribution \(P\), Kolmogorov complexity is the length of the shortest program able to uniformly approximate \(P\) arbitrarily well,

\[K(P)=_{p\{0,1\}^{*}}\{|p|: y\;|(p,y,q)-P(y)|\}\;.\]

Using \(K\) and towards defining algorithmic independence, we define algorithmic mutual information . For two binary strings \(x,y\), the algorithmic mutual information between \(x,y\) is given by \(I_{A}(x;y)=K(x)+K(y)-K(x,y)\). For distributions \(P,Q\), we can state it as follows.

**Definition 2.1** (Algorithmic Mutual Information ).: The algorithmic mutual information between two distributions \(P\) and \(Q\) is given by

\[I_{A}(P;Q):=K(P)-K(Q P^{*})\]

where \(P^{*}\) is the shortest algorithmic description of \(P\).

Two strings \(x,y\) are algorithmically independent if \(I_{A}(x;y)}{{=}}0\) holds (up to an additive constant that is independent of \(x,y\)), where we write \(I_{A}(x_{1};...;x_{n})\) if the corresponding strings are pairwise independent. Analogously, a set of distributions \(P_{i}\) are jointly independent when

\[I_{A}(P_{1};;P_{n})}{{=}}0\;.\]

When the mutual information \(I_{A}\) between different causal conditionals vanishes, we can characterize them as independent. The algorithmic causal Markov Condition  states precisely this.

**Postulate 2.2** (Algorithmic Markov Condition for Changing Mechanisms).: _Let \((G,)\) be a causal model with DAG \(G\), partitions \(\), and structural equations as in Eq. 1. Then \((G,)\) is a valid causal hypothesis only if the following equivalent conditions hold,_

1. _All causal mechanisms_ \(f\) _in the model are jointly algorithmically independent. That is,_ \[I_{A}(\{P(X_{i} pa_{i}^{G},):i\{1,...,n\}\,,_{i}\} )}{{=}}0\;.\] (2)
2. _The causal mechanisms_ \(f\) _in the model describe the observed distribution most concisely. That is, the shortest description of_ \(P(X)\) _across all contexts_ \(c\) _is given by writing each_ \(X_{i}\) _as a function of_ \(pa_{i}^{G}\) _in each group_ \(_{i}\)_,_ \[K(P(X))}{{=}}_{X_{i}}_{_{i}}K(P(X_ {i} pa_{i}^{G},))\] (3) _where_ \(}{{=}}\) _denotes equality up to an additive constant._

The inner term in Eq. (2) ranges over the conditionals \(P(X_{i}_{i}^{G},)\) for each variable \(X_{i}\) in each group of contexts \(_{i}\), where we omit indices of \(\) for ease of notation. Overall, Postulate 2.2 states that the causal mechanisms correspond to programs that do not share any information (Eq. (2)) and that describe the observed distributions most concisely (Eq. (3)).

We will use the formulation in Eq. (3) which directly uses Kolmogorov complexity of conditionals. Due to the halting problem, we cannot compute Kolmogorov complexity for arbitrary programs , but by restricting the class to be tractable, we can approximate it from above. A statistically well-founded manner of doing so is Minimum Description Length (MDL) . For a fixed class \(\) of models, MDL defines a description length \(L\) of encoding a dataset \(X\) together with its optimal model,

\[L(X)=_{h}- P(X h)+L(h)\;.\]

Regarding the choice of \(\), we are interested in functions \(f\) that are both expressive but also allow us to measure their description length. Gaussian processes naturally fulfill these requirements , making them a fitting choice for our approach. We briefly introduce them next.

### Gaussian Process Functional Model

A Gaussian process (GP) is a collection of random variables \(\), any finite number of which have a joint multivariate Gaussian distribution . Each variable represents the value of a function \(f\) at a single location \(x\). We write functions drawn from a GP as

\[f(m(x),(x,x^{}))\]

for a mean function \(m(x)=[f(x)]\) and covariance kernel \((x,x^{})=[(f(x)-m(x))(f(x^{})-m(x^{}))]\). Viewed differently, a GP can also be expressed as a linear regression \(f=_{i}_{i}\) with potentially infinitely many basis functions \(_{i}\). Given finitely many data points \((x_{i},y_{i})\), we can write predictions as a linear combination of a finite number of kernel functions \((x_{i},)\) centered on the observed points \(x_{i}\),

\[f(x)=_{i=1}^{n}_{i}(x_{i},x)\,\] (4)

with \(=(K+^{2}I)^{-1}y\), where \(K\) is the Gram matrix of \(\) with \(K_{ij}=(x_{i},x_{j})\), and \(^{2}\) a parameter. This formulation will be useful when we consider the description length of GPs later.

Regarding the richness of the function class, given a kernel \(\), one can construct a reproducing Hilbert space \(_{}\), which is the smallest Hilbert space containing all finite expansions of the form in Eq. (4). For some kernels, such as the radial basis function (RBF) kernel \((x_{i},x_{j})=(- x_{i}-x_{j} _{2}^{2}),\,>0\) which we consider here, it is known that \(_{}\) is dense in the space of continuous functions . That is, we can approximate these functions up to arbitrary precision using a GP . Using GPs, we now state our functional model as follows.

**Assumption 1** (Functional Model with Changing Mechanisms).: Given a DAG \(G\) and partitions \(\), we assume the functional model

\[X_{i}^{(c)}=f_{i}^{()}(_{i}^{G})+N_{i}c,\] (5)

where \(_{i}\) is a group, \(f_{i}^{()}_{}\) is a GP, and \(N_{i}\) is independent noise \(N_{i}\!\!\!(_{i}^{G},C)\).

We point out that the above makes an additive noise assumption as is standard in GP modeling. Having stated the ICM assumption and our functional model, we are ready to introduce our approach.

## 3 Learning Independent Changes with LINC

We now introduce a score to instantiate Eq. (3) for GPs and give an analysis of its properties.

### Measuring the Complexity of GP Models

To define the ability of our model to succinctly describe a given dataset, we need to address the complexity of a GP \(f\). Despite the richness of \(_{}\), the complexity of any \(f_{}\) can be readily measured by its squared norm \( f_{}^{2}\). For functions inferred from finite amounts of data, as in Eq. (4), we can compute this norm \( f_{}^{2}\) from the quantities \(K\) and \(\) as 

\[ f_{}^{2}=^{}K\.\]

Consider now the model class \(_{}\) of GPs and a target variable \(X_{i}\). For a given subset \(X_{S} X\) of predictors, we can fit a GP regression \(f:X_{S} X_{i}\). Kakade et al.  show that the Bayesian MDL score for this GP model is given by

\[L(X_{S},X_{i}_{})=_{f_{}} - P(X_{i} X_{S})+ f_{}^{2}+R(X_{S })\.\] (6)

The score balances the data fit of the GP regression model \(f\) with its complexity, \( f_{}^{2}=^{}K\). The remaining penalty \(R(X_{S})\) is independent of the minimization over \(f\),

\[R(X_{S})=(^{-2}K_{S}+I)\,\]

where \(K_{S}\) is the Gram matrix when \(\) is applied to \(X_{S}\). We write \(L(f)=L(X_{S},X_{i}_{})\) when the context is clear, and \(L(G,)\) as the summed description length for a given causal model. By Eq. (3), we choose the model with the smallest description length as our causal hypothesis.

**Problem Statement** (Formal).: _Given data \(X\) over environments \(\) we want to find the graph \(\), partitions \(}\) and mechanisms \(f_{i}^{()}\) minimizing the score \(L\),_

\[,}=*{arg\,min}_{, }L(G,)\.\] (7)

To show that we recover the correct causal structure with this objective, we give identifiability results.

### Identifiability Results

Here, we give conditions under which we can identify the true causal model using our score function. We begin by showing that we can discover the causal DAG in the correct Markov Equivalence class (MEC) under the following general assumptions.

**Assumption 2** (Sufficient Capacity).: For each \(i\) there exist functions \(f_{i}^{()}_{}\) s.t. Eq. (5) holds.

This recalls the assumption that our functional model is well-specified.

**Assumption 3** (Causal Minimality).: For all \(_{i}\), \(f_{i}^{()}\) is not constant in any of its inputs.

The above is a toned-down version of the faithfulness condition, ensuring that every parent of \(X_{i}\) has an influence on \(X_{i}\). Without this, it would be impossible to identify the complete set of parents.

**Assumption 4** (\(\)-Faithfulness).: If \(^{}_{i}\) then \(f_{i}^{()} f_{i}^{(^{})}\).

This assumption guarantees the uniqueness of each partition. Without it, interventions would not necessarily lead to changes in causal mechanisms, and make identification thereof impossible.

With this, we can state our first identifiability result as follows.

**Theorem 3.1** (Identification of MECs).: _Let Assumptions 1-4 hold and let \(\) be sufficiently small. Then the estimates \(,}\) that minimize our score \(L\),_

\[,}=*{arg\,min}_{, }L(G,)\,\]

_recover a DAG \( G^{*}\) in the Markov Equivalence class of the true DAG \(G^{*}\), as well as the true partitions \(_{i}^{*}\) for each node \(i\) with probability 1_

\[P( G^{*},}=^{*})=1\]

Next, we show that under independent mechanism shifts, we can also identify the fully directed \(G^{*}\). To correctly identify the direction of all edges in the discovered network, we need the following assumption that the causal mechanisms are independent of each other and change independently.

**Assumption 5** (Independent Mechanism Shift).: For all \(i j:_{i}(C)\!\!\!_{j}(C)\).

We can now show that as we observe more contexts, \(||\), our score identifies the correct DAG.

**Theorem 3.2** (Identifiability from independent changes).: _Let Assumptions 1-5 hold. Then for \(\) sufficiently small, where \(\) is a hyperparameter bounding the score differences of \(,G^{*}\), we can identify the causes \(_{i}^{G}\) and partition \(_{i}\) of each \(X_{i}\) in the causal model \((G^{*},^{*})\) with probability 1,_

\[_{||}P(=G^{*},}= ^{*})=1\]

_In particular, the fully directed network \(G^{*}\) is identifiable._

The requirement \(||\) is necessary to turn the assumed independence of causal conditionals across different contexts into an empirically testable statistical criterion. That is, to guarantee that the model is indeed identifiable, we need either the distribution assigning contexts to partitions, or a guaranteed representative sample from the distribution, which we only obtain in the limit. Essentially, this is a frequentist approach, allowing us not to require the distributions for the partitions themselves.

We can, however, provide further probabilistic guarantees for finite numbers of contexts. To be able to reason about probabilities of different causal structures, we need to ensure that as we observe more contexts \(||\), the number of sets in the partition remains fixed. We hence assume that for each \(X_{i}\), the contexts fall inside one of a fixed number sets \(B_{i,1},...,B_{i,k_{i}}\) which can be thought of as bins that fix the sizes of our partitions, and thus bound the number of mechanism shifts per variable.

**Assumption 6** (Fixed Partition Sizes).: For each \(X_{i}\), there exists a finite number of sets \(B_{i,1},...,B_{i,k_{i}}\) such that \(p_{ij} P(C B_{i,j})>0\).

The assumption can be understood in a frequentist sense, in that on average each possible causal mechanism for each variable occurs in at least a constant fraction of all samples. For example, if we study the differences between treatments in different hospitals, then as we obtain more data, these data would be obtained from the same set of hospitals, rather than by the addition of new hospitals. With this, we are able to give a tighter bound as follows.

**Theorem 3.3** (Identifiability in the finite context setting).: _Let the Assumptions of Theorem 3.2 as well as Assumption 6 hold. Let \(p=_{i,j}p_{ij}\) be the minimum probability for a context to fall into one of the bins \(B_{i,j}\). Then if \(||-2p^{-2}(p)>0\) we have_

\[P(=G^{*},}=^{*})=( (-|E|e^{-c})),\]

_where \(c=p^{2}||+2 p 0\) and \(|E|\) is the number of edges in \(G^{*}\)._

The dependence of the bound on \(|E|\) is reasonable. The sparser the graph \(G^{*}\), the fewer edges that need to be directed in its MEC. Note furthermore that the fixed size of the partitions is likely not strictly necessary, and as long as \(p\) decays sufficiently slowly, Theorem 3.3 will still likely hold. We also note that the bounds above apply to the case where we have no active control over intervention targets, i.e. when mechanism shifts occur randomly. While active intervention design allows identification using a smaller number of interventions , we often cannot ensure full interventional control in practice; for example, gene editing technologies such as CRISPR-Cas gene-editing are known to have severe off-target effects [7; 16; 17] that may not be known a priori.

In what follows, we describe how to put our theoretical insights into practice.

### Discovering Independent Changes with LINC

In summary, our approach, called LINC (**L**earning causal models under **IN**dependent Changes), aims to discover a mixture of mechanisms that together most succinctly describe multi-context data.

To do so, we discover the causal DAG \(G\) and partitions \(\) that minimize Eq. (7). To compute \(L\) for a given model \((G,)\), we consider each causal mechanism \(f^{()}\) in the model and fit a GP regression predicting \(X_{i}\) from its causes \(_{i}^{G}\). We compute the MDL description length of this GP as in Eq. (6). We do so in each group of contexts in \(_{i}\) and each variable in \(G\) to obtain the overall score \(L(G,)\).

To discover the partitions for a known graphical model \(G\), we suggest an exhaustive search whenever feasible, given that the number of different contexts that we have access to may be small in the typical use case . As exhaustive search over partitions becomes prohibitive for larger numbers of contexts, we propose a greedy variant in Appendix B. It relies on the no-hypercompression inequality  that allows defining a scoring function for pairs of contexts. This pairwise measure can be used together with a clustering approach to discover a partition of the contexts in a greedy manner.

We also face the search for a DAG \(G\). Given the large search space over causal DAGs, existing algorithms are of a greedy nature, such as the score-based greedy equivalence search (GES, 33) and its generalized version GGES that can be used with general scoring functions so long as they are locally consistent . In principle, we can use the MDL score \(L\) as a plug-in estimator in GGES to discover a MEC of the true graph. However, as there exist many greedy approaches to discover a MEC of \(G\) from multiple contexts [4; 5; 6; 7; 8; 9], whereas discovering relationships beyond MEC is relatively underexplored [12; 13], we put our emphasis on the latter and discover the best causal DAG within a given Markov equivalance class in our experiments, following the literature .

Finally, we also adress the computational burden associated with our score \(L\) due to the sample complexity of GP regression. To alleviate this, we leverage the Random Fourier Feature (RFF) approximation, which allows us to express the GP kernel through a Fourier transform . We adapt our scoring function accordingly to use the approximated RFF kernel. We call our main approach \(_{}\) and include a comparison to the variant \(_{}\) in our evaluation.

We provide the details of our algorithms, a computational complexity analysis, and the adaptation to RFFs in the supplementary material. In the remainder of our work, we focus on evaluating LINC empirically against relevant related work, which we survey in the following section.

## 4 Related Work

Discovering cause-effect relationships is of great interest in many applications, and there exists a growing literature on how we can do so from observational data . Most well-known methods can be categorized either as constraint-based, such as the PC algorithm , or score-based, such as GES . Given purely observational and identically distributed data, it is however impossible to make definitive statements about the causal structure . Non-i.i.d. data carries information about how the system responds to external intervention or noise shifts, and hence often allows to infer causal directions more accurately [6; 36]. We summarize approaches to do so here.

A prominent idea in causal inference from multiple contexts is that causal mechanisms remain stable, i.e., _invariant_, under distribution shift  and there exist several causal discovery methods that use invariance as an asymmetry between causal and non-causal orientations [3; 38; 39; 40]. Invariance may be violated when variables are subject to interventions, as is common in biology . Methods that can discover causal relationships from a combination of observational and interventional data [41; 42; 43] often assume known intervention targets, which is not always realistic . Hence, recent research considers multiple contexts with uncertain interventions or _mechanism changes_. For example, Huang et al.  show how to apply the PC algorithm jointly to multiple contexts by introducing a context variable. There are similar approaches with different instantiations [5; 6; 7; 8; 9], all of which however discover causal MECs. A common approach to go beyond Markov Equivalence is _functional modelling_[15; 26; 45; 46; 25], but all existing methods that we are aware of are tailored to i.i.d. data.

Finally, recent works tackle causal discovery beyond Markov Equivalence from non-i.i.d. data under the _sparse mechanism shift_ (SMS)  hypothesis. The Minimal Changes (MC) method  and the Mechanism Shift Score (MSS)  both count the number of distributional changes across contexts to infer edge directions within a MEC. MC assumes linear models, MSS is nonparametric. As our contribution, we show that regularized scoring functions, such as our MDL-based score, can be used toward the same goal. These scores have been shown to make use of the independence noise asymmetry on i.i.d. data [47; 25], which suggests that LINC may be more widely applicable than the SMS scores. As MC and MSS are most related to LINC, they will be the focus of our evaluation.

## 5 Evaluation

We consider synthetic, semi-synthetic, and real-world data to empirically evaluate LINC and how its performance relates to the state-of-the-art. We make the code and datasets available in the supplement.

Experimental SetupIn our experiments, we consider both synthetic data from a known functional model class, as well as more realistic data following the stochastic differential equations behind gene regulatory networks . Throughout, we use an Erdos Renyi DAG model to sample graphs \(G\).

Figure 2: _LINC discovers causal models beyond Markov Equivalence._ We report the F1 scores for discovering the causal direction of each edge in a random DAG \(G\), where shading shows confidence regions and dashed horizontal lines illustrate random guessing. In (a), we consider synthetic data over different contexts \(\) where \(|S|\) variables per context undergo mechanism shifts, starting from \(||=5\), \(|G|=6\) and \(|S|=2\). In (b), we mimic real-world gene expression dynamics by simulating data with SERGIO  from a random network \(G\) over \(||-1\) genes, which we observe in conditions \(\) with a knockout intervention on one gene per condition.

For the synthetic data, following the literature  we generate data in multiple contexts using

\[X_{i}^{(c)}=_{j_{i}^{G}}_{ij}^{(c)}f_{ij}(X_{i}^{(c)})+ _{j}^{(c)}N_{j}^{(c)}\,\] (8)

with weights \(_{ij}^{(c)}(0.5,2.5)\), where noise is either uniform or Gaussian with equal probability. We sample the causal functions \(f\) from \(\{x^{2},x^{3},,\}\). In each context, we choose a random subset \(S\{0,...|G|\}\) of variables that undergo a mechanism shift, which is either a soft intervention by re-sampling from Eq. (8), noise scaling through \((2,5)\), or a hard intervention via \(w_{ij}=0\).

In addition, we simulate data with SERGIO  to generate single-cell expression data according to a gene regulatory network while modeling the real-world dynamics of gene transcription and regulation using stochastic differential equations. Using a random DAG \(G\) as the reference network, we generate one observational dataset as well as \(|G|\) interventional datasets, in each of which we keep a different gene fixed to mimic a knockout intervention .

BaselinesAs a baseline for causal discovery up to Markov Equivalence, we include Pooled PC  which pools data from all contexts, augments the system with a categorical variable \(C\) that encodes the context and applies the PC algorithm to discover a causal DAG. As we are interested in evaluating whether LINC can orient edges beyond the MEC, we also compare against the linear Minimal Changes approach (MC)  and the nonlinear Mechanism Shift Score (MSS)  that are based on mechanism shift counting. We instantiate MSS with the Kernel Conditional Independence (KCI) test and use the soft score variant, which Perry et al.  show to consistently outperform competitors.

Causal discovery beyond MECsAs our main experiment, we evaluate whether LINC can discover causal edge orientations in non-i.i.d. data. We sample DAGs \(G\) of size \(|G|=6\) with edge density \(p=0.3\) and generate data in \(||=5\) contexts, with \(|c|=500\) samples and \(|S|=2\). We start from the correct MEC, use each method to discover a DAG, and report F1 scores over edge directions.

We report the results in Fig. 2(a). As pooled PC discovers a MEC jointly from all contexts, its F1 scores tend to 0.5, which can be achieved by not orienting any edges in the MEC as this results in recall 0.5 on expectation (dashed lines). In contrast, MSS, MC and LINC discover all edge orientations given sufficiently many contexts \(||\), variables \(|G|\), and changes \(|S|\). The differences are most pronounced for small DAG sizes, resp. contexts. In particular, only LINC learns additional edge directions from i.i.d., single-context data since the competitors rely purely on mechanism shifts. The RFF variant of LINC works surprisingly well but is ultimately similar to GPs for large \(||\).

Overall, LINC compares favorably against its competitors in all settings. We defer results with varying edge density \(p\) and sample size \(|c|,\) as well as scalability with \(||\) and \(|G|\) to Appendix B.

Gene regulatory networksNext, we evaluate the methods on gene expression data that we generate with SERGIO. We show the F1 scores on edge directions in Fig. 2(b) depending on the number of conditions \(||\). While MSS is the closest competitor to LINC on synthetic data with _soft_ interventions (Fig. 2(a)), the _hard_ interventions in this experiment result in poor performance (Fig. 2(b)). Despite

Figure 3: _LINC is robust in different settings._ We show which methods can discover edges beyond Markov Equivalence robustly. We consider (a) soft and hard interventions on the mechanisms in Eq. 8, (b) dense shifts compared to i.i.d. data, where all (\(|S|=||\)), resp. none (\(|S|=0\)) of the variables change, and (c) bivariate causal discovery in the base (\(|S|=2\)) and i.i.d. case (\(|S|=0\)).

assuming linearity, MC performs well, which suggests that it detects mechanism shifts even when its functional model is likely misspecified. It performs comparably to Pooled PC, where both benefit from an increased number of variables and conditions. LINC outperforms the competitors, the difference being most pronounced when it observes only few conditions.

Causal discovery beyond SMSBesides confirming that LINC can handle different generating processes, we are also interested in its robustness w.r.t. different types of distribution shifts that may occur. In Fig. 3(a), we compare the soft interventions considered so far with noise scaling and hard intervention. To confirm our observations that LINC generalizes to settings where the sparse shift principle is not applicable, we compare dense shifts \(|S|=||\) to the case without any changes \(|S|=0\) in Fig. 3(b). As expected, the competitors roughly match the base MEC in the i.i.d. case, whereas LINC performs well. This observation is in line with recent theoretical results  that MDL-based scoring functions can exploit the additive noise asymmetry [45; 49] on i.i.d. data. Lastly, LINC also performs remarkably well compared to the baselines for causal direction determination between pairs of variables, as we show in Fig. 3(c).

Cell signalling pathwaysFinally, we evaluate LINC on real-world data over eleven proteins and phospholipid components, with the goal of mapping their signaling pathways in human immune cells . To properly understand how the components of such a system interact, it can be indicative how the system responds to stimulatory and inhibitory cues, which Sachs et al.  added to the system in multiple experiments. We run LINC and competitors on data from all contexts, without providing information about the interventions. We show the best DAG that LINC identifies within the true MEC in Fig. 4. We color edges that Sachs et al.  report as causal in green and edges with opposite orientation in orange. Except for three, LINC discovers all directions correctly (Fig. 4(a)), leading to the highest performance compared to the competitors (Fig. 4(b)).

## 6 Discussion and Conclusion

In this work, we consider causal modeling of data from multiple environments where unknown causal mechanism changes may exist. Our interest in this setting is motivated by the prevalence of distribution shifts and heterogeneity in real-world applications, as well as by the need to analyze interventional data in biology, where modern gene editing and measurement technologies allow collecting large-scale experimental data . Here, we showed that viewing independence of causal mechanisms from an information-theoretic perspective allows us to analyze such data in a unifying way. With LINC, we proposed a nonlinear functional modeling approach using Gaussian processes. We showed theoretically that we can use it to identify causal models with independent changes, and demonstrated in practice that it is applicable to settings beyond those where the sparse mechanism shift principle applies, and that it gives insight into biological data with unknown intervention effects.

While we focused on theoretical guarantees in this work, developing efficient algorithms for causal discovery is an important aspect of future work. Given the shift of research attention in causal discovery from i.i.d. to non-i.i.d. data, we face not only the problem of efficient search for DAG structures but in addition the problem that there may be multiple causal mechanisms per variable, which adds an additional layer of complexity. Towards efficient search for partitions, we propose search algorithms inspired by clustering in the supplementary, and establishing approximation guarantees for these methods is a worthwhile direction of future work. Other future directions include extending our theory to latent confounding, considering the situation where the split of the data into different contexts is not known to us, or applying our insights to causal representation learning.

Figure 4: _Cell Signalling Pathways._ We show the DAG that LINC estimates for the cell cytometry data (a) in comparison to competitors (b). Green links are causal (TP) and orange edges are anticausal (FP) according to the literature .