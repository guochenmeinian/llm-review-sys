# AsyncDiff: Parallelizing Diffusion Models by Asynchronous Denoising

Zigeng Chen, Xinyin Ma, Gongfan Fang, Zhenxiong Tan, Xinchao Wang

National University of Singapore

zigeng99@u.nus.edu, xinchao@nus.edu.sg

Corresponding Author

###### Abstract

Diffusion models have garnered significant interest from the community for their great generative ability across various applications. However, their typical multi-step sequential-denoising nature gives rise to high cumulative latency, thereby precluding the possibilities of parallel computation. To address this, we introduce _AsyncDiff_, a universal and plug-and-play acceleration scheme that enables model parallelism across multiple devices. Our approach divides the cumbersome noise prediction model into multiple components, assigning each to a different device. To break the dependency chain between these components, it transforms the conventional sequential denoising into an asynchronous process by exploiting the high similarity between hidden states in consecutive diffusion steps. Consequently, each component is facilitated to compute in parallel on separate devices. The proposed strategy significantly reduces inference latency while minimally impacting the generative quality. Specifically, for the Stable Diffusion v2.1, _AsyncDiff_ achieves a 2.7x speedup with negligible degradation and a 4.0x speedup with only a slight reduction of 0.38 in CLIP Score, on four NVIDIA A5000 GPUs. Our experiments also demonstrate _AsyncDiff_ can be readily applied to video diffusion models with encouraging performances. Code is available at https://github.com/czg1225/AsyncDiff

## 1 Introduction

Diffusion models  stand out in generative modeling and have significantly advanced various fields including text-to-image  and text-to-video generation ,

Figure 1: We introduce a new distributed acceleration paradigm that attains a 2.8x speed-up on Stable Diffusion XL while maintaining **pixel-level consistency**, using four NVIDIA A5000 GPUs.

image translation , audio generation, style transfer, low-level vision tasks , image editing , and 3D model generation , among others. However, their widespread application is hindered by the high latency inherent in their multi-step sequential denoising process. This issue becomes more pronounced as the complexity and size of the models increase to enhance generative quality.

In response to these challenges, significant research efforts are directed toward enhancing the efficiency of diffusion models. Notably, training-free acceleration methods have garnered increasing popularity due to their low cost and convenience. Numerous studies  improve inference speed by skipping redundant calculations in the denoising process. As computational resources grow rapidly, distributing computations across multiple devices has become a more promising approach. Recent advances  demonstrate that using distributed computing to parallelize inference effectively increases the acceleration ratio for diffusion models while maintaining acceptable generative quality. Though these methods succeed in parallelizing the diffusion models, they require iterative refining  or displaced patch parallelism , resulting in a larger number of model evaluations or low GPU utilization correspondingly.

Thus, we wish to propose a new parallel paradigm for diffusion, akin to the model parallelism in distributed computing , which divides the denoising model into several components to be distributed on different GPUs. The primary challenge lies in the inherent sequential denoising process of diffusion models. Each step in this process depends on the completion of its predecessor, forming a dependency chain that impedes parallelization and significantly increases inference latency. Our approach seeks to disrupt this chain, allowing for the parallel execution of the denoising model while closely approximating the results of the sequential process.

In this paper, we introduce _AsyncDiff_, a universal, distributed acceleration paradigm that innovatively explores model parallelism in diffusion models. As shown in Fig 2, our method sequentially partitions the heavyweight denoising model \(_{}\) into multiple components \(\{_{}^{n}\}_{n=1}^{N}\) based on computational load, assigning each to a separate device. Our core idea lies in decoupling the dependencies between these cascaded components by leveraging the high similarity in hidden states across consecutive diffusion steps. After the initial warm-up steps, each component takes the output from the previous component's prior step as the approximation of its original input. This transforms the traditional sequential denoising into an asynchronous process, allowing components to predict noise for different time steps in parallel. Additionally, we incorporate stride denoising to skip redundant calculations and reduce the frequency of communication between devices, further enhancing efficiency.

Through extensive testing across multiple base models, our method effectively distributes the computational burden across various devices, substantially boosting inference speed while maintaining quality. Specifically, with the text-to-image model Stable Diffusion v2.1 , our method achieves a 1.8x speedup with only a marginal 0.01 drop in CLIP Score , and a 4.0x speedup with a slight 0.38 reduction in CLIP Score on two and four NVIDIA A5000 GPUs, respectively. For video diffusion models, AnimateDiff  and Stable Video Diffusion , our approach significantly reduces latency by tens of seconds, effectively preserving video quality.

Figure 2: By preparing each componentâ€™s input beforehand, we enable parallel computation of the denoising model, which substantially reduces latency while minimally affecting quality.

In summary, we present a novel distributed acceleration method for diffusion models that significantly reduces inference latency with minimal impact on generation quality. This is achieved by replacing the sequential denoising process with an asynchronous process, allowing each component of the denoising model to run independently across different devices. Extensive experiments on both image and video diffusion models strongly demonstrate the effectiveness and versatility of our method.

## 2 Related Works

**Diffusion Models**. Diffusion models have attracted significant attention due to their powerful generative capabilities across various tasks. Sohl-Dickstein et al.  first proposed diffusion probabilistic models. Ho et al.  with the introduction of Denoising Diffusion Probabilistic Models (DDPM), enhancing training efficiency and generation quality. Rombach et al.  advanced these models by incorporating latent spaces, enabling high-resolution image generation. Despite these advancements, the high latency of the iterative denoising process remains a limitation.

**Inference Acceleration**. Training-based acceleration methods focus on reducing sampling steps [48; 71; 32; 50; 69] or optimizing model architectures [27; 80; 7; 73; 68; 6]. However, these methods incur high training costs and complexity. Training-free methods are gaining popularity due to their ease of use. Some approaches develop fast solvers for SDE or ODE to improve sampling efficiency [31; 1; 30; 74; 81]. Other works [35; 63; 76; 67; 53; 25; 33; 79] observed special characteristics of diffusion models and skipped the redundant computation within the denoising process.

**Parallelism**. The parallelism strategy presents a promising yet underexplored approach to accelerating diffusion models. ParaDiGMS  implements Picard iterations for parallel sampling, yet its practical speed-up ratio is modest, and it struggles to maintain consistency with original outputs. Faster Diffusion  introduces encoder propagation but significantly compromises quality, and its parallelization remains theoretical. Distrifusion  adopts patch parallelism, dividing high-resolution images into sub-patches to facilitate parallel inference on each patch by reusing stale activation maps from each layer. However, this approach lacks flexibility across different data types or tasks, often encountering low resource utilization. Furthermore, its reliance on reusing per-layer activation maps greatly increases GPU memory demands thus introducing additional challenges for realistic applications. In contrast, our method uniquely implements model parallelism through asynchronous denoising, achieving substantial acceleration while maintaining a stable resource usage ratio and minimal impact on quality.

## 3 Methods

### Preliminary

Diffusion models  are a dominant class of generative models that transform Gaussian noise into complex data distributions via a Markov process. The forward process is defined by:

\[q(x_{t}|x_{t-1})=(x_{t};}x_{t-1},_{t}I),\] (1)

where \(\{_{t}\}\) progressively increases noise until the data becomes indistinguishable from noise. The reverse process, essential for data reconstruction, involves iterative denoising:

\[p_{}(x_{t-1}|x_{t})=(x_{t-1};_{}(x_{t},t),_{t }^{2}I),\] (2)

where \(_{}(x_{t},t)\) is the predicted mean and \(_{t}^{2}\) is the variance. For DDIMs , the reverse update is deterministic:

\[x_{t-1}=}{_{t}}}x_{t}+} (1-}{_{t-1}}})_{}(x_{ t},t),\] (3)

where \(_{t}\) is the cumulative product of \((1-_{t})\). These processes are computationally intensive, influencing the quality of generated samples and necessitating efficient inference methods for practical applications.

### Asynchronous Diffusion Model

Traditional diffusion models employ a sequential and synchronous denoising process. At each time step \(t\), the noise-prediction model \(_{}\) estimates the noise \(_{t}\) based on the noisy image \(x_{t}\) and the time embedding \(t\). The image for the next step, \(x_{t-1}\), is then generated using a sampler function \(S(x_{t},_{t},t)\). This process is iterative, where the generation of \(_{t}\) at each step is dependent on the completion of the previous denoising step, making the process slow, particularly when \(_{}\) is computationally intensive.

To address the limitations of high latency in diffusion models, leveraging multiple GPUs for distributed inference is a promising solution. Existing studies primarily focus on patch parallelism , where the input image is divided into patches, each processed on a different GPU. While this strategy efficiently distributes computational loads, it still retains the bottleneck of sequential denoising, as each patch must undergo the complete denoising process iteratively. In contrast, our asynchronous diffusion model innovatively introduces a model parallelism strategy. By approximating the sequential denoising as an asynchronous process, this approach enables parallel inference of the noise prediction model, effectively reducing latency and breaking the constraints of sequential execution.

**Asynchronous Denoising**. Figure 3 illustrates our approach to the asynchronous denoising. For a denoising process consisting of \(T\) steps, the initial \(w\) steps are designated as a warm-up phase, where \(w\) is significantly smaller than \(T\). During this phase, the denoising model \(_{}\) operates using standard sequential inference. After warm-up steps, rather than splitting the input image, we partition the denoising model \(_{}\) into \(N\) sequential components, expressed as \(_{}=\{_{}^{1},_{}^{2},...,_{ }^{N}\}\). Each component is divided to handle a comparable computational load and assigned to a distinct device. This equitable division aims to equalize the time cost of each component to approximately \(l(_{})/N\), thus minimizing the overall maximum latency. In this setup, original noise prediction for \(x_{t}\) can be represented as a cascading operation through these sub-models, defined mathematically as:

\[_{t}=_{}(x_{t},t)=_{}^{N}(_{ }^{N-1}(_{}^{2}(_{}^{1}(x_{t},t),t) ,t),t).\] (4)

Although each device can independently compute its assigned component, the dependency chain persists because the input for each component \(_{,n}\) is derived from the output from its preceding component \(_{,n-1}\). Therefore, despite the distribution of model components across multiple devices, full parallelization is constrained by these sequential dependencies.

Our principal innovation is to break the dependency between cascaded components by utilizing hidden features from previous steps. Observations indicate that the hidden states of each block in the denoising model always exhibit substantial similarity across adjacent time steps. Leveraging this, each component at time step \(t\) can take the output from the preceding component at time step \(t-1\) as the approximation of its original input. Specifically, the \(n\)-th component \(_{}^{n}(,t)\) receives the output of

Figure 3: Overview of the asynchronous denoising process. The denoising model \(_{}\) is divided into four components \(\{_{}^{n}\}_{n=1}^{4}\) for clarity. Following the warm-up stage, each componentâ€™s input is prepared in advance, breaking the dependency chain and facilitating parallel processing.

\(_{}^{n-1}(,t-1)\). This alteration allows the noise prediction for \(x_{t}\) to be represented as follows:

\[_{t}=_{}^{N}(_{}^{N-1}(_{ }^{2}(_{}^{1}(x_{t+N-1},t+N-1),t+N-2),t+1),t).\] (5)

In this new framework, noise prediction \(_{t}\) is derived from components executed across \(N\) previous time steps. This transforms the denoising process from sequential to asynchronous, as the prediction of noise \(_{t}\) already begins before denoising at step \(t+1\) is completed. At each time step, the \(N\) components are running as parts of the noise prediction model for the next \(N\) steps. Specifically, the \(n\)-th component \(_{}^{n}\), computed in parallel at time \(t\), contributes to the noise prediction for the future time step \(t-N+n\). Figure 3 depicts this asynchronous process using a U-net model with \(N\) set to 4. The strong resemblance of hidden states between consecutive diffusion steps enables the asynchronous process to closely mimic the denoising results of the original sequential process.

**Model Parallelism**. By transitioning to an asynchronous denoising strategy, the dependencies among components within the same time step are eliminated. This adjustment allows each component's input for time step \(t\) to be prepared in advance, enabling the \(N\) split components to be processed concurrently across multiple devices. Once computed, the outputs from each component must be stored and then broadcasted to other devices to facilitate parallel processing for subsequent time steps. In contrast, in the traditional sequential denoising process, the time cost for each step accumulates as follows:

\[C_{seq}(t)=C(_{}^{1})+C(_{}^{2})++C(_ {}^{N}).\] (6)

By adopting asynchronous denoising to enable parallel computation of each component, the cost for each time step is now given by:

\[C_{asy}(t)=(C(_{}^{1}),C(_{}^{2}),...,C( _{}^{N}))+C(),\] (7)

where \(()\) represents taking the maximum value, and \(C()\) indicates the communication cost across multiple GPUs. As the model components are equally divided by computational load, their time costs are similar, allowing us to approximate the overall cost of each time step as:

\[C_{asy}(t)(t)}{N}+C().\] (8)

Since the communication overhead \(C()\) is generally much lower than the model's execution time, it leads to significant overall cost reductions. Moreover, increasing \(N\) further reduces time costs but complicates the accurate approximation of the original denoising process.

**Stride Denoising**. While asynchronous denoising reduces latency by parallelizing the denoising model, it completes only one denoising step at a time. To enhance efficiency, we introduce stride denoising, which completes multiple denoising steps simultaneously through a single parallel computation. The diagram is illustrated in Figure 4, where we set the stride to 2 for clarity. Unlike the continuous broadcasting of hidden states at each time step, stride denoising broadcasts them every two steps. As depicted, at time step \(t\), we conduct denoising alone, and at time step \(t-1\), we compute and broadcast the hidden states for the next parallel computation round. Consequently, the hidden states from time step \(t\) are not required, allowing us to skip the calculations for \(_{}^{1}\) and \(_{}^{2}\) at this step. In this stride, only \(_{}^{3}(,t)\), \(_{}^{1}(,t-1)\), \(_{}^{2}(,t-1)\), and \(_{}^{3}(,t-1)\) need computing, all receiving the

Figure 4: Illustration of stride denoising. The model \(_{}\) is divided into three components \(\{_{}^{n}\}_{n=1}^{3}\), with a stride \(S\) of 2 for clarity. Components \(_{}^{1}\) and \(_{}^{2}\) are skipped at time step \(t\). A single parallel batch results in the completion of denoising for two steps, producing \(x_{t-1}\) and \(x_{t-2}\).

previously broadcast hidden states, enabling their parallel processing. Both \(^{3}_{}(,t)\) and \(^{3}_{}(,t-1)\) share the same feature from \(^{2}_{}(,t+1)\), so the stride should be kept small to maintain quality. Stride denoising effectively reduces both computational load and communication demands by decreasing the parallel computing rounds needed to complete the process. Compared to the significant improvements it brings in efficiency, the quality sacrifice is minimal and can be entirely compensated for by slightly increasing the warm-up steps. We also illustrate the full schematic of it in Appendix Figure 7.

**Multi-Device Communication**. Parallel inference of the model necessitates efficient communication between devices, as each component \(^{n}_{}\) must access the cached hidden state from the preceding component \(^{n-1}_{}\), which resides on a different device. Post each parallel computation batch, each device stores the current hidden state needed for the next parallel batch. These states, encompassing all component outputs, are then broadcast to all participating devices before the next parallel computation batch. Although each component \(^{n}_{}\) primarily uses the cached output of \(^{n-1}_{}\) for its input, it may require residual features  from other components. Therefore, it's crucial to broadcast the stored states from every component across all devices before each round of parallel computation.

## 4 Experiments

### Implementation Details

**Base models**. We validated the broad applicability of _AsyncDiff_ through extensive testing on several diffusion models. For text-to-image tasks, we experimented with three versions of Stable Diffusion: SD 1.5, SD 2.1 , and Stable Diffusion XL (SDXL) . Additionally, we explored the effectiveness of _AsyncDiff_ on video diffusion models using Stable Video Diffusion (SVD)  and AnimateDiff . All models were evaluated using 50 DDIM steps. We facilitated communication across multiple GPUs using the _broadcast_ operation from _torch.distributed_, powered by the _NVIDIA Collective Communication Library_ (NCCL) backend.

Figure 5: Qualitative Results. (a) Our method significantly accelerates the denoising process with minimal impact on generative quality. (b) Increasing warm-up steps achieves pixel-level consistency with the original output while maintaining a high speed-up ratio.

**Dataset and Evaluation Metrics**. We assess the zero-shot generation capability using the MS-COCO 2017  validation set, which comprises 5,000 images and captions. For image generation, quality is measured by the CLIP Score (on ViT-g/14)  and Frechet Inception Distance (FID) , with LPIPS  used to check consistency with original outputs. In video generation, quality is evaluated by averaging the CLIP Score across all frames of a video. We also report MACs per device and latency to gauge efficiency comprehensively. All latency measurements were conducted on NVIDIA A5000 GPUs equipped with NVLINK Bridge.

### Experimental Results on Image Diffusion Models

**Improvements on Base Models**. Table 1 displays our acceleration outcomes for three fundamental image diffusion models under various configurations. In this context, 'N' represents the number of segments into which the denoising model is divided, and 'S' denotes the stride of denoising for each parallel computation batch. Our approach, _AsyncDiff_, not only significantly accelerates processing but also minimally impacts generative quality. The speedup ratio is almost proportional to the number of devices used, demonstrating efficient resource utilization. Visualization results in Figure 5 (a) illustrate the high generative quality achieved even with substantially reduced latency. Although achieving pixel-level consistency with the original output is challenging at high acceleration ratios, the generated image still effectively conveys the semantic information in the prompt, which is crucial for generative results.

**Pixel-level Consistency by Warm-up.** In Table 2, we explore the balance between pixel-level consistency and processing speed by adjusting the warm-up steps in the diffusion models. As the initial steps of these models play a crucial role in reconstructing the global structure based on text prompts , a modest increase in warm-up steps can significantly enhance consistency with the

    &  &  &  \\   & **Speedup\(\)** & **CLIP\(\)** & **LPIPS\(\)** & **Speedup\(\)** & **CLIP\(\)** & **LPIPS\(\)** & **Speedup\(\)** & **CLIP\(\)** & **LPIPS\(\)** \\  Original Model & 1.0x & 31.60 & â€“ & 1.0x & 30.63 & â€“ & 1.0x & 32.33 & â€“ \\ Warm-up = 3 & 3.5x & 31.26 & 0.3289 & 3.3x & 30.16 & 0.3676 & 3.8x & 31.40 & 0.3556 \\ Warm-up = 5 & 3.1x & 31.27 & 0.2769 & 3.0x & 30.14 & 0.3304 & 3.4x & 31.60 & 0.2993 \\ Warm-up = 7 & 2.9x & 31.32 & 0.2590 & 2.7x & 30.10 & 0.2839 & 3.0x & 31.77 & 0.2521 \\ Warm-up = 9 & 2.7x & 31.40 & 0.1940 & 2.5x & 30.17 & 0.2354 & 2.8x & 31.92 & 0.2095 \\ Warm-up = 11 & 2.4x & 31.45 & 0.1628 & 2.4x & 30.22 & 0.1927 & 2.5x & 32.01 & 0.1740 \\   

Table 2: Quantitative evaluations of the effect of increasing warm-up steps. More warm-up steps can achieve pixel-level consistency with the original output while slightly reducing processing speed.

    &  &  &  &  & **Speedup\(\)** & **CLIP Score\(\)** & **FID\(\)** & **LPIPS\(\)** \\   & Original Model & 1 & 76T & 5.51s & 1.0x & 31.60 & 27.89 & â€“ \\  & **+ Ours** (N=2 S=1) & 2 & 38T & 3.03s & 1.8x & 31.59 & 27.79 & 0.2121 \\ SD 2.1 & **+ Ours** (N=3 S=1) & 3 & 25T & 2.41s & 2.3x & 31.56 & 28.00 & 0.2755 \\ (Test-to-Image) & **+ Ours** (N=4 S=1) & 4 & 19T & 2.10s & 2.6x & 31.40 & 28.28 & 0.3132 \\  & **+ Ours** (N=2 S=2) & 3 & 19T & 1.82s & 3.0x & 31.43 & 28.55 & 0.3458 \\  & **+ Ours** (N=3 S=2) & 4 & 13T & 1.35s & 4.0x & 31.22 & 29.41 & 0.3778 \\   & Original Model & 1 & 34T & 2.70s & 1.0x & 30.63 & 29.96 & â€“ \\  & **+ Ours** (N=2 S=1) & 2 & 17T & 1.52s & 1.8x & 30.62 & 29.94 & 0.1988 \\ SD 1.5 & **+ Ours** (N=3 S=1) & 3 & 11T & 1.23s & 2.2x & 30.58 & 29.87 & 0.2645 \\ (Test-to-Image) & **+ Ours** (N=4 S=1) & 4 & 9T & 1.01 & 2.6x & 30.52 & 30.10 & 0.3073 \\  & **+ Ours** (N=2 S=2) & 3 & 9T & 0.94s & 2.9x & 30.46 & 30.98 & 0.3232 \\  & **+ Ours** (N=3 S=2) & 4 & 6T & 0.72s & 3.7x & 30.17 & 30.89 & 0.3811 \\   & Original Model & 1 & 299T & 13.81s & 1.0x & 32.33 & 27.43 & â€“ \\  & **+ Ours** (N=2 S=1) & 2 & 150T & 8.00s & 1.7x & 32.21 & 27.79 & 0.2509 \\ SDXL & **+ Ours** (N=3 S=1) & 3 & 100T & 5.84s & 2.4x & 32.05 & 28.03 & 0.2940 \\ (Test-to-Image) & **+ Ours** (N=4 S=1) & 4 & 75T & 5.12s & 2.7x & 31.90 & 29.12 & 0.3157 \\  & **+ Ours** (N=2 S=2) & 3 & 75T & 4.91s & 2.8x & 31.70 & 28.99 & 0.3209 \\  & **+ Ours** (N=3 S=2) & 4 & 49T & 3.65s & 3.8x & 31.40 & 30.27 & 0.3556 \\   

Table 1: Quantitative evaluations of _AsyncDiff_ on three text-to-image diffusion models, showcasing various configurations. â€˜Nâ€™ indicates the number of components into which the model is divided, and â€˜Sâ€™ represents the denoising stride. _MACs_ quantifies the computational load per device for generating a single image throughout the denoising process.

original images. Figure 5(b) illustrates this trend with qualitative comparisons of generative results on SDXL using gradually increasing warm-up steps. Increasing the warm-up steps to 9 achieves visual indistinguishability from the original output while maintaining an impressive 2.8x acceleration ratio.

**Comparison with Acceleration Baselines**. We evaluated our _AsyncDiff_ method on SD 2.1 against two other parallel acceleration methods: Faster Diffusion  and Distrifusion . Faster Diffusion employs encoder propagation but compromises significantly on generative quality. As its parallelism maintains theoretical and lacks a multi-device implementation, we cannot measure its realistic latency with more than one GPU. Its ideal speed-up on 2 devices is about 1.9x. Distrifusion, on the other hand, uses patch parallelism for distributed acceleration but faces potential issues with low resource utilization and high GPU memory demands.

According to Table 3, our method achieves the same operational speed using only 4 GPUs and 3 GPUs as Distrifusion does with 8 GPUs and 4 GPUs, respectively. Additionally, our method requires almost the same amount of memory as the original setup, whereas Distrifusion significantly increases memory requirements, posing extra challenges for practical applications. In terms of generative quality, _AsyncDiff_ and Distrifusion both mirror the original diffusion model's performance at a 1.6x acceleration ratio. However, at higher speedup ratios of 2.3x and 2.7x, our method demonstrates significantly superior generative quality. Qualitative comparisons in Fig 6 further show that _AsyncDiff_ maintains better pixel-level consistency with the original input compared to Distrifusion.

### Experimental Results on Video Diffusion Models

As presented in Table 4, we conducted experiments with different configurations on two video diffusion models: SVD  (25 frames), and AnimentDiff  (16 frames), to demonstrate the efficacy

  
**Method** & **Speed up\(\)** & **Devices** & **MACs\(\)** & **Memory\(\)** & **CLIP Score\(\)** & **FID\(\)** & **LIPIS\(\)** \\  Original Model & 1.0x & 1 & 76T & 5240MB & 31.60 & 27.87 & â€“ \\  Faster Diffusion & 1.6x & 1 & 57T & 9692MB & 30.84 & 29.95 & 0.3477 \\  Distrifusion & 1.6x & **2** & **38T** & 6533MB & **31.59** & 27.89 & **0.0178** \\
**Ours (N=2 S=1)** & 1.6x & **2** & 44T & **5450MB** & **31.59** & **27.79** & 0.0944 \\  Distrifusion & 2.3x & **4** & **19T** & 7086MB & 31.43 & 27.97 & 0.2710 \\
**Ours (N=2 S=2)** & 2.3x & **3** & 20T & **5516MB** & **31.49** & **27.71** & **0.2117** \\  Distrifusion & 2.7x & 8 & **10T** & 7280MB & 31.31 & 28.12 & 0.2934 \\
**Ours (N=3 S=2)** & 2.7x & **4** & 14T & **5580MB** & **31.40** & **28.03** & **0.1940** \\   

Table 3: Quantitative comparison with other parallel acceleration methods. To ensure a fair comparison with Distrifusion, we increased the warm-up steps in our method to match the speedup ratio of Distrifusion, allowing us to fairly compare generation quality and resource costs.

Figure 6: Qualitative Comparison with Distrifusion on SD2.1. At the same acceleration ratio, _AsyncDiff_ outperforms in generating higher quality and more consistent images with the original.

of our method. Video generation, often constrained by exceptionally high latency and substantial computation load, greatly benefits from our approach. For a 50-step video diffusion model, _AsyncDiff_ significantly reduces latency--by tens or even hundreds of seconds--while preserving the quality of generated content. Qualitative results shown in the Appendix. D further corroborate the effectiveness of our method. _AsyncDiff_ achieves an impressive acceleration ratio of over three times while still producing videos that closely match the prompt descriptions, ensuring the rationality of actions and details. These findings highlight the substantial potential of _AsyncDiff_ in accelerating the inference process of video diffusion models.

### Effect of Stride Denoising

We introduce stride denoising to further enhance the efficiency of the asynchronous denoising process. Stride denoising completes multiple steps simultaneously through a single parallel computation, reducing the number of parallel rounds and communication frequency across devices. For a diffusion process with \(T\) steps and warm-up step \(W\), the number of broadcasts decreases from \(T-W\) to \((T-W)//2\) with a stride of 2. This strategy also reduces the computational load on each device by skipping unnecessary calculations. Table 5 shows the effects of stride denoising in our parallel framework with 3 and 4 devices. Stride denoising significantly lowers overall latency and the proportion of communication time, especially as the number of devices used increases. While stride denoising slightly impacts generation quality, this effect is minimal and can be mitigated by a modest increase in warm-up steps, preserving efficiency and maintaining quality.

### Compatibility with Various Samplers

With the recent rise of advanced sampling algorithms for diffusion models, a key concern is whether the acceleration method can adapt to various samplers. AsyncDiff is a universal method that can be combined with different samplers, such as the DDIM sampler  and DPM-Solver . In Table 7, we present the quantitative evaluation of AsyncDiff on SD 2.1 using the DDIM sampler. Compared to using fewer DDIM steps, our method achieves significantly better generation quality at similar speeds, with the improvement becoming more pronounced as speedup increases. Table 6 presents the quantitative evaluation of AsyncDiff on SD 2.1 with the DPM-Solver sampler. At the same speedup ratio, AsyncDiff significantly enhances generation quality compared to the baseline. Qualitative results are also provided in the Appendix figures, demonstrating that our method achieves considerable acceleration while maintaining high consistency with the original output.

  
**Base Model** & **Configuration** & **Devices** & **MACs\(\)** & **latency\(\)** & **Speed up\(\)** & **CLIP Score\(\)** \\   AnimateDiff (Text-to-Video) \\  } & Original Model & 1 & 786T & 43.5s & 1.0x & 30.65 \\  & **+ Ours** (N=2 S=1) & 2 & 393T & 24.5s & 1.8x & 30.65 \\  & **+ Ours** (N=3 S=1) & 3 & 262T & 19.1s & 2.3x & 30.54 \\  & **+ Ours** (N=2 S=2) & 3 & 197T & 14.2s & 3.0x & 30.32 \\  & **+ Ours** (N=3 S=2) & 4 & 131T & 11.5s & 3.8x & 30.20 \\   SVD \\ (Image-to-Video) \\  } & Original Model & 1 & 3221T & 184s & 1.0x & 26.88 \\  & **+ Ours** (N=2 S=1) & 2 & 1611T & 101s & 1.8x & 26.66 \\  & **+ Ours** (N=3 S=1) & 3 & 1074T & 80s & 2.3x & 26.56 \\  & **+ Ours** (N=4 S=1) & 4 & 805T & 68s & 2.7x & 26.19 \\   

Table 4: Quantitative evaluations of _AsyncDiff_ on text-to-video and image-to-video diffusion models. We present the results with various configurations.

    &  &  &  &  &  \\   & & & & **Nums\(\)** & & **Latency\(\)** \\  _AsyncDiff_ (3 devices) w/o stride denoising & 25T & 2.41s & 2.3x Faster & 49 times & 0.23s(9.5\%) & **31.56** \\ _AsyncDiff_ (3 devices) w/ stride denoising & **19T** & **1.82s** & **3.0x Faster** & **25 times** & **0.12s(6.6\%)** & 31.43 \\  _AsyncDiff_ (4 devices) w/o stride denoising & 19T & 2.10s & 2.6x Faster & 49 times & 0.40s(19.0\%) & **31.40** \\ _AsyncDiff_ (4 devices) w/ stride denoising & **13T** & **1.35s** & **4.0x Faster** & **25 times** & **0.10s(7.4\%)** & 31.22 \\   

Table 5: Effect of stride denoising on SD 2.1. Stride denoising significantly lowers overall latency and the communication cost while only slightly compromising the generative quality 

## 5 Efficiency Analysis on Different Devices

As a hardware-friendly and versatile method, our acceleration technique delivers strong performance on a wide range of GPUs. We tested inference speeds on the professional-grade NVIDIA RTX A5000, as well as the consumer-grade NVIDIA RTX 2080 Ti and NVIDIA RTX 3090 GPUs. As shown in Table 8, our method achieved a high acceleration ratio across all three GPUs. Furthermore, our method can be applied as long as the devices have basic communication capabilities.

## 6 Conclusion

In this paper, we propose a new parallel paradigm, _AsyncDiff_, to accelerate diffusion models by leveraging model parallelism across multiple devices. We split the denoising model into several components, each assigned to a different device. We transform the conventional sequential denoising into an asynchronous process by exploiting the high similarity of hidden states between consecutive time steps, enabling each component to compute in parallel. Our method has been comprehensively validated on three image diffusion models (SD 2.1, SD 1.5, SDXL) and two video diffusion models (SVD, AnimateDiff). Extensive experiments demonstrate that our approach significantly accelerates inference with only a marginal impact on generative quality. This work investigates the practical application of model parallelism in diffusion models, establishing a new baseline for future research in distributed diffusion models.