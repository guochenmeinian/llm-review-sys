# Kernel-Based Tests for

Likelihood-Free Hypothesis Testing

 Patrik Robert Gerber

Department of Mathematics, MIT

Cambridge, MA 02139

prgerber@mit.edu

&Tianze Jiang

Department of Mathematics, MIT

Cambridge, MA 02139

tjiang@mit.edu

&Yury Polyanskiy

Department of EECS, MIT

Cambridge, MA 02139

yp@mit.edu

&Rui Sun

Department of Mathematics, MIT

Cambridge, MA 02139

eruisun@mit.edu

Equal contribution.

###### Abstract

Given \(n\) observations from two balanced classes, consider the task of labeling an additional \(m\) inputs that are known to all belong to _one_ of the two classes. Special cases of this problem are well-known: with complete knowledge of class distributions (\(n=\)) the problem is solved optimally by the likelihood-ratio test; when \(m=1\) it corresponds to binary classification; and when \(m n\) it is equivalent to two-sample testing. The intermediate settings occur in the field of likelihood-free inference, where labeled samples are obtained by running forward simulations and the unlabeled sample is collected experimentally. In recent work it was discovered that there is a fundamental trade-off between \(m\) and \(n\): increasing the data sample \(m\) reduces the amount \(n\) of training/simulation data needed. In this work we (a) introduce a generalization where unlabeled samples come from a mixture of the two classes - a case often encountered in practice; (b) study the minimax sample complexity for non-parametric classes of densities under _maximum mean discrepancy_ (MMD) separation; and (c) investigate the empirical performance of kernels parameterized by neural networks on two tasks: detection of the Higgs boson and detection of planted DDPM generated images amidst CIFAR-10 images. For both problems we confirm the existence of the theoretically predicted asymmetric \(m\) vs \(n\) trade-off.

## 1 Likelihood-Free Inference

The goal of likelihood-free inference (LFI) [8; 12; 17; 24], also called simulation-based inference (SBI), is to perform statistical inference in a setting where the data generating process is a black-box, but can be simulated. Given the ability to generate samples \(X_{} P_{}^{ n}\) for any parameter \(\), and given real-world data \(Z P_{^{}}^{ m}\), we want to use our simulations to learn about the truth \(^{}\). LFI is particularly relevant in areas of science where we have precise but complex laws of nature, for which we can do (stochastic) forward simulations, but can not directly compute the (distribution) density \(P_{}\). The Bayesian community approached the problem under the name of Approximate Bayesian Computation (ABC) [6; 13; 47]. More recent ML-based methods where regressors and classifiersare used to summarize data, select regions of interest, approximate likelihoods or likelihood-ratios [14; 29; 30; 42; 49] have also emerged for this challenge.

Despite empirical advances, the theoretical study of frequentist LFI is still in its infancy. We focus on the nonparametric and non-asymptotic setting, which we justify as follows. For applications where tight error control is critical one might be reluctant to rely on asymptotics. More broadly, the non-asymptotic regime can uncover new phenomena and provide insights for algorithm design. Further, parametric models are clearly at odds with the black-box assumption. Recently,  proposed likelihood-free hypothesis testing (LFHT) as a simplified model and found minimax optimal tests for a range of nonparametric distribution classes, thereby identifying a _fundamental simulation-experimentation trade-off_ between the number of simulated observations \(n\) and the size of the experimental data sample \(m\). Here we extend , and prior related work [19; 26; 27; 32; 33], to a new setting designed to model experimental setups more truthfully and derive sample complexity (upper and lower bounds) for kernel-based tests over nonparametric classes.

While minimax optimal, the algorithms of [18; 19] are impractical as they rely on discretizing the observations on a regular grid. Thus, both in our theory as well as experiments we turn to kernel methods which provide an empirically more powerful set of algorithms that have shown success in nonparametric testing [20; 21; 22; 31; 39].

ContributionsOur contributions are twofold. _Theoretically_, we introduce _mixed likelihood-free hypothesis testing_ (mLFHT), which is a generalization of (LFHT) and provides a better model of applications such as the search for new physics [11; 38]. We propose a robust kernel-based test and derive both upper and lower bounds on its minimax sample complexity over a large nonparametric class of densities, generalizing multiple results in [18; 19; 26; 27; 32; 33; 37]. Although the simulation-experimentation (\(m\) vs \(n\)) trade-off has been proven in the minimax sense (that is, for some worst-case data distribution), it is not clear whether it actually occurs in real data. Our second contribution is the _empirical_ confirmation of the existence of an asymmetric trade-off, cf. Figure 1.2. To this end we construct state-of-the-art tests building on ideas of [39; 48] on learning good kernels from the data. We execute this program in two settings: the Higgs boson discovery , and detecting diffusion  generated images planted in the CIFAR-10  dataset.

### Lfht and the Simulation-Experimentation Trade-off

Suppose we have i.i.d. samples \(X,Y\) each of size \(n\) from two unknown distributions \(P_{X},P_{Y}\) on a measurable space \(\), as well as a third i.i.d. sample \(Z P_{Z}\) of size \(m\). In the context of LFI, we may think of the samples \(X,Y\) as being generated by our simulator, and \(Z\) being the data collected in the real world. The problem we refer to as likelihood-free hypothesis testing is the task of deciding between the hypotheses

\[H_{0}:P_{Z}=P_{X} H_{1}:P_{Z}=P_{Y}.\] (LFHT)

This problem originates in [23; 52], where authors study the exponents of error decay for finite \(\) and fixed \(P_{X},P_{Y}\) as \(n m\); more recently [2; 18; 19; 26; 27; 32; 33; 37] it is studied in the non-asymptotic regime. Assuming that \(P_{X},P_{Y}\) belong to a known nonparametric class of distributions \(\) and are guaranteed to be \(\)-separated with respect to total variation (TV) distance (i.e. \((P_{X},P_{Y})\)),  characterizes the sample sizes \(n\) and \(m\) required for the sum of type-I and type-II errors to be small, as a function of \(\) and for several different \(\)'s. Their results show, for three settings of \(\), that \((i)\) testing (\(\)) at vanishing error is possible even when \(n\) is not large enough to estimate \(P_{X}\) and \(P_{Y}\) within total variation distance \(()\), and that \((ii)\) to achieve a fixed level of error, say \(\), one can _trade off_\(m\) vs. \(n\) along a curve of the form \(\{\{n,\} n_{}(,,),m (1/)/^{2}\}\). Here \(n_{}\) denotes the minimax sample complexity of two-sample testing over \(\), i.e. the minimum number of observations \(n\) needed from \(P_{X},P_{Y}\) to distinguish the cases \((P_{X},P_{Y})\) versus \(P_{X}=P_{Y}\). Here \(\) suppresses dependence on constants and untracked parameters.

It is unclear, however, whether predictions drawn from minimax sample complexities over specified distribution classes can be observed in real-world data. Without the theory, a natural expectation is that the error contour \(\{(m,n):\}\) would look similar to that of minimax two-sample testing with unequal sample size, namely \(\{(m,n):\{n,m\} n_{}(,,)\}\), i.e. \(n\) and \(m\) simply need to be above a certain threshold simultaneously (as is the case for e.g. two-sample testing over smooth densities [4; 37]). However, from Figures 2 and 1.2 we see that there is indeed a non-trivial trade-off between \(n\) and \(m\): the contours are not always parallel to the axes and aren't symmetric about the line \(m=n\). The importance of Fig. 1.2 is in demonstrating that said trade-off is not a kink of a theory that arises due to some esoteric worst-case data distribution, but is instead a real effect observed in state-of-the-art LFI algorithms ran on actual data. We remark that the \(n\) used in this plot is the total number of simulated samples (most of which are used for choosing a neural-network parameterized kernel) and are not just the \(n\) occuring in Theorems 3.1 and 3.2 which apply to a _fixed_ kernel. See Section 4 for details on sample division.

### Mixed Likelihood-Free Hypothesis Testing

A prominent application of likelihood-free inference lies in the field of particle physics. Scientists run sophisticated experiments in the hope of finding a new particle or phenomenon. Often said phenomenon can be predicted from theory, and thus can be simulated, as was the case for the Higgs boson whose existence was verified after nearly \(50\) years at the Large Hadron Collider (LHC) [3; 9].

Suppose we have \(n\) simulations from the _background_ distribution \(P_{X}\) and the _signal_ distribution \(P_{Y}\). Further, we also have \(m\) (real-world) datapoints from \(P_{Z}=(1-)P_{X}+ P_{Y}\), i.e. the observed data is a mixture between the background and signal distributions with rate parameter \(\). The goal of physicists is to construct confidence intervals for \(\), and a _discovery_ corresponds to a \(5\) confidence interval that excludes \(=0\). We model this problem by testing

\[H_{0}:=0 H_{1}:\] (mLFHT)

for fixed (usually predicted) \(>0\). See the rigorous definition of (mLFHT) in Section 3. In particular, a discovery can be claimed if \(H_{0}\) is rejected.

## 2 The Likelihood-Free Test Statistic

This section introduces the testing procedure based on Maximum Mean Discrepancy (MMD) that we study throughout the paper both theoretically and empirically. First, we introduce the necessary background on MMD in Section 2.1. Then, we define our test statistics in Section 2.2.

### Kernel Embeddings and MMD

Given a set \(\), we call the function \(K:^{2}\) a kernel if the \(n n\) matrix with \(ij\)'th entry \(K(x_{i},x_{j})\) is symmetric positive semidefinite for all choices of \(x_{1},,x_{n}\) and \(n 1\). There is a unique reproducing kernel Hilbert space (RKHS) \(_{K}\) associated to \(K\). \(_{K}\) consists of functions \(\) and satisfies the reproducing property \( K(x,),f_{_{K}}=f(x)\) for all \(f_{k}\) and \(x\), in particular \(K(x,)_{K}\). Given a probability measure \(P\) on \(\), define its kernel embedding \(_{P}\) as

\[_{P}_{X P}\,K(X,)=_{}K(x, )P(x).\] (1)

Given the kernel embeddings of two probability measures \(P,Q\), we can measure their distance in the RKHS by \((P,Q)\|_{P}-_{Q}\|_{_{K}}\), where MMD stands for maximum mean discrepancy. MMD has a closed form thanks to the reproducing property and linearity:

\[^{2}(P,Q)=[K(X,X^{})+K(Y,Y^{})-2K(X,Y) ]\]

where \((X,X^{},Y,Y^{}) P^{ 2} Q^{ 2}\). In particular, if \(P,Q\) are empirical measures based on observations, we can evaluate the MMD exactly, which is crucial

Figure 1: \(n\) versus \(m\) trade-off for the Higgs and CIFAR experiments using our test in Section 2. Error probabilities are estimated by normal approximation for Higgs and simulated for CIFAR.

in practice. Yet another attractive property of MMD is that (under mild integrability conditions) it is an integral probability metric (IPM) where the supremum is over the unit ball of the RKHS \(_{K}\). See e.g. [41; 46] for references. The following result is a consequence of the fact that self-adjoint compact operators are diagonalizable.

**Theorem 2.1** (Hilbert-Schmidt).: _Suppose that \(K L^{2}()\) is symmetric. Then there exists a sequence \((_{j})_{j 1}^{2}\) and an orthonormal basis \(\{e_{j}\}_{j 1}\) of \(L^{2}()\) such that \(K(x,y)=_{j 1}_{j}e_{j}(x)e_{j}(y)\) for all \(j 1\), where convergence is in \(L^{2}()\)._

**Assumption 1**.: Unless specified otherwise, we implicitly assume a choice of a non-negative measure \(\) and kernel \(K\) for which the conditions of Theorem 2.1 hold. Note that \(_{j} 0\) and depend on \(\).

Removing the BiasIn our proofs we work with the kernel embedding of empirical measures for which we need to modify the inner product \(,_{_{K}}\) (and thus \(\)) slightly by removing the diagonal terms. Namely, given i.i.d. samples \(X,Y\) of size \(n,m\) respectively and corresponding empirical measures \(_{X},_{Y}\), we define

\[^{2}_{u}(_{X},_{Y})_{i j },X_{j})}{n(n-1)}+_{i j},Y_{j})}{m(m-1)}-2 _{i,j},Y_{j})}{mn}.\] (2)

We also write \(_{_{X}},_{_{X}}_{u,_{K}}\|_{_{X}}\|_{u,_{K}}^{2} _{i j}K(X_{i},X_{j})\) and extend linearly. The \(u\) stands for unbiased, since \(\,^{2}_{u}(_{X},_{Y})=^{2}(P_{X},P_{Y})\,^{2}(_{X}, _{Y})\) in general.

### Test Statistic

With Section 2.1 behind us, we are in a position to define the test statistic that we use to tackle (\(\)). Suppose that we have samples \(X,Y,Z\) of sizes \(n,n,m\) from the probability measures \(P_{X},P_{Y},P_{Z}\). Write \(_{X}\) for the empirical measure of sample \(X\), and analogously for \(Y,Z\). The core of our test statistic for (\(\)) is the following:

\[T(X,Y,Z)_{_{Z}},_{_{Y}}- _{_{X}}_{u,_{K}}=_{i=1}^{n} _{j=1}^{m}K(Z_{j},Y_{i})-K(Z_{j},X_{i})}.\] (3)

Note that \(T\) is of the additive form \(_{j=1}^{m}f(Z_{i})\) where \(f(z)_{_{Y}}(z)-_{_{X}}(z)\) can be interpreted as the _witness function_ of [21; 31]. Given some \(\) (taken to be half the predicted signal rate \(/2\) in our proofs), the output of our test is

\[_{}=T(X,Y,Z)(X,Y,)},(X,Y,)=\,^{2}_{u}(_{X},_{Y})+T(X,Y,X).\] (4)

The threshold \(\) gives \(_{}\) a natural geometric interpretation: it checks whether the projection of \(_{_{Z}}-_{_{X}}\) onto the vector \(_{_{Y}}-_{_{X}}\) falls further than \(\) along the segment joining \(_{_{X}}\) to \(_{_{Y}}\) (up to deviations due to the omitted diagonal terms, see Section 2.1).

Setting \(=1\) in (\(\)) recovers (\(\)), and the corresponding test output is \(_{/2}=_{1/2}=1\) if and only if \(_{u}(_{Z},_{X})_{u}( {P}_{Z},_{Y})\). This very statistic (i.e. \(_{u}(_{Z},_{X})-_{u}( {P}_{X},_{Y})\)) has been considered in the past for relative goodness-of-fit testing  where it's asymptotic properties are established. In the non-asymptotic setting, if MMD is replaced by the \(L^{2}\)-distance we recover the test statistic studied by [18; 27; 33]. However, we are the first to introduce \(_{}\) for \( 1\) and to study MMD-based tests for (\(\) in a non-asymptotic setting).

Variance cancellationAt first sight it may seem more natural to the reader to threshold the distance \(_{u}(_{Z},_{X})\), resulting in rejection if, say, \(_{u}(_{Z},_{X})_{u}( {P}_{X},_{Y})/2\). The geometric meaning of this would be similar to the one outlined above. However, there is a crucial difference: (\(\)) (the case \(=1\)) is possible with very little experimental data \(m\) due to the _cancellation of variance_. More precisely, the statistic \(^{2}(_{Z},_{X})\) contains the term \(_{i j}K(Z_{i},Z_{j})\) -- whose variance is prohibitively large and would inflate the \(m\) required for reliable testing -- but this can be canceled by subtracting \(^{2}(_{Z},_{Y})\). Our statistic \(T(X,Y,Z)-(X,Y,)\) simply generalizes this idea to (\(\)).

## 3 Minimax Rates of Testing

### Upper Bounds on the Minimax Sample Complexity of (\(\))

Let us start by reintroducing (\(\)) in a rigorous fashion. Given \(C,,R 0\), let \(_{}(C,,R)\) denote the set of triples \((P_{X},P_{Y},P_{Z})\) of distributions such that the following three conditions hold:

* \(P_{X},P_{Y}\) and \(P_{Z}\) have \(\)-densities bounded by \(C\),
* \((P_{X},P_{Y})\), and
* \((P_{Z},(1-)P_{X}+ P_{Y}) R(P_{X},P_{Y})\),

where we define \(=(P_{X},P_{Y},P_{Z})=_{^{}}(P_ {Z},(1-^{})P_{X}+^{}P_{Y})\). For some \(>0\), consider the two hypotheses

\[H_{0}(C,,,R):(P_{X},P_{Y},P_{Z})_{ }(C,,R)=0\] (5) \[H_{1}(C,,,R):(P_{X},P_{Y},P_{Z})_{ }(C,,R),\]

which we regard as subsets of probability measures. Notice that \(R\) controls the level of mis-specification in the direction that is orthogonal to the line connecting the kernel embeddings of \(P_{X}\) and \(P_{Y}\). Setting \(R=0\) simply asserts that \(P_{Z}\) is guaranteed to be a mixture of \(P_{X}\) and \(P_{Y}\), as is the case for prior works on LFHT. Before presenting our main result on the minimax sample complexity of mLFHT, let us define one final piece of terminology. We say that a test \(\), which takes some data as input and takes values in \(\{0,1\}\), has total error probability less than \(\) for the problem of testing \(H_{0}\) vs \(H_{1}\) if

\[_{P H_{0}}P(=1)+_{Q H_{1}}Q(=0).\] (6)

**Theorem 3.1**.: _Suppose we observe three i.i.d. samples \(X,Y,Z\) from distributions \(P_{X},P_{Y},P_{Z}\) composed of \(n,n,m\) observations respectively and let \(C(0,)\) and \(R, 0\) and \((0,1)\). There exists a universal constant \(c>0\) such that \(_{/2}\) defined Section 2.2 tests \(H_{0}\) vs \(H_{1}\), as defined in (5), at total error \(\) provided_

\[\{m,n\} c(1/)}{( /(1+R))^{2}}\{n,\} c(1/)}{ ^{2}}.\]

Note that Theorem 3.1 does _not_ place assumptions on the distributions \(P_{X},P_{Y}\) beyond bounded density with respect to the base measure \(\). This is different from usual results in statistics, where prior specification of distribution classes is crucial. On the other hand, instead of standard distances such as \(L^{p}\), we assume separation with respect to MMD and the latter is potentially harder to interpret than, say, \(L^{1}\) i.e. total variation. We do point out that our Theorem 3.1 can be used to derive results in the classical setting; we discuss this further in Section 3.4.

In an appropriate regime of the parameters, the sufficient sample complexity in Theorem 3.1 exhibits a trade-off of the form \(\{n,\}\|\|_{2}(1/)/(^{2})\) between the number of simulation samples \(n\) and real observations \(m\). This trade-off is shown in Figure 2 using data from a toy problem. The trade-off is clearly asymmetric and the relationship \(m n\) also seems to appear. In this toy problem we set \(R=0,=1,=.3,k=100\) and \(P_{X}=P_{Z},P_{Y}\) are distributions on \(\{1,2,,k\}\) with \(P_{X}(i)=(1+(2 1\{i\}-1))/k=2/k-P_{Y}(i)\) for all \(i=1,2,,k\). The kernel we take is \(K(x,y)=_{i=1}^{k}1\{x=y=i\}\) and \(\) is simply the counting measure; the resulting MMD is simply the \(L^{2}\)-distance on pmfs.

Figure 2 illustrates a larger scale experiment using real data using a trained kernel. Note that we plot the _total_ number \(n\) of simulation samples, including those used for _training_ the kernel itself (see Section 4); which ensures that Figure 1.2 gives a realistic picture of data requirements. However, due to the dependence between the kernel and the data, Theorem 3.1 no longer applies. Nevertheless, we observe a trade-off similar to Figure 2.

Figure 2: \(n\) versus \(m\) trade-off for the toy experiment, verifying Theorem 3.1. Probabilities estimated over \(10^{4}\) runs, and smoothed using Gaussian noise.

### Lower Bounds on the Minimax Sample Complexity of (mLFHT)

In this section we prove a minimax lower bound on the sample complexity of mLFHT, giving a partial converse to Theorem 3.1. Before we can state this results, we must make some technical definitions. Given \(J 2\), let \(\|\|_{2J}^{2}_{j=2}^{J}_{j}^{2}\) and define

\[J_{}^{}J:_{_{j}= 1} _{j=2}^{J}_{j}}e_{j}_{}}{2}}.\]

**Theorem 3.2** (Lower Bounds for mLFHT).: _Suppose that \(_{}K(x,y)(x)_{1}\), \(()=1\) and \(_{x}K(x,x) 1\). There exists a universal constant \(c>0\) such that any test of \(H_{0}\) vs \(H_{1}\), as defined in (5), with total error at most \(\) must use a number \((n,m)\) of observations that satisfy_

\[m c(1/)}{^{2}^{2}} n c^{}}}{ ^{2}} m+ c^{}}}{^{2}}.\]

_Remark 3.3_.: Recall that the eigenvalues \(\)_depend on the choice of \(\)_, so that by choosing a different base measure \(\) one can optimize the lower bound. However, since \(P_{X},P_{Y},P_{Z}\) are assumed to have bounded density with respect to \(\), this appears rather involved.

_Remark 3.4_.: The requirements \(_{x}K(x,x) 1\) and \(()=1\) are is essentially without loss of generality, as \(\) and \(K\) can be rescaled. The condition \(_{}K(x,y)(x)_{1}\) implies that the top eigenfunction \(e_{1}\) is equal to a constant or equivalently, that \(y K(x,y)(x)\) defines a Markov kernel up to a normalizing constant.

### Tightness of Theorems 3.1 and 3.2

Dependence on \(\|\|_{2}\)An apparent weakness of Theorem 3.2 is its reliance on the unknown value \(J_{}^{}\), which depends on the specifics of the kernel \(K\) and base measure \(\). Determining it is potentially highly nontrivial even for simple kernels. Slightly weakening Theorem 3.2 we obtain the following corollary, which shows that the dependence on \(\|\|_{2}\) is tight, at least for small \(\).

**Corollary 3.5**.: _Suppose \(J 2\) is such that \(_{j=2}^{J}_{j}^{2} c^{2}\|\|_{2}^{2}\) for some \(c 1\). Then \(\|\|_{2J_{}^{}}\) can be replaced by \(c\|\|_{2}\) in Theorem 3.2 whenever \( c\|\|_{2}/(2)\)._

Dependence on \(R\) and \(\)Due to the general nature of our lower bound constructions, it is difficult to capture the dependence on the misspecification parameter \(R\). As for the probability of error \(\), based on related work  we expect the gap of size \(\) to be a shortcoming of Theorem 3.1 and not the lower bound. Closing this gap may require a different approach, however, as tests based on empirical \(L^{2}\) distances are known to have a sub-optimal concentration .

Dependence on \(\)The correct dependence on the signal rate \(\) is the most important question left open by our theoretical results. Any method requiring \(n\) larger than a function of \(\) irrespective of \(m\) (as in Theorem 3.1) is provably sub-optimal because taking \(m 1/()^{2}\) and \(n\) large enough to estimate both \(P_{X},P_{Y}\) to within accuracy \(/10\) always suffices to reach a fixed level of total error.

### Relation to Prior Results

In this section we discuss some connections of Theorem 3.1 to prior work. Specifically, we discuss how Theorem 3.1 recovers some known results in the literature [4; 18; 37] that are _minimax optimal_. Details omitted in this section are included in Appendix B.

Binary Hypothesis TestingSuppose the two distributions \(P_{X},P_{Y}\) are _known_, we are given \(m\) i.i.d. observations \(Z_{1},,Z_{m} P_{Z}\) and our task is to decide between the hypotheses \(H_{0}:P_{X}=P_{Z}\) versus \(H_{1}:P_{Y}=P_{Z}\). Then, we may take \(n=,R=0,=1\) in Theorem 3.1 to conclude that

\[m c(1/)}{^{2}}\]

observations suffice to perform the test at total error \(\).

Two-Sample TestingSuppose we have two i.i.d. samples \(X\) and \(Y\), both of size \(n\), from unknown distributions \(P_{X},P_{Y}\) respectively and our task is to decide between \(H_{0}:P_{X}=P_{Y}\) against\((P_{X},P_{Y})\). We split our \(Y\) sample in half resulting in \(Y^{(1)}\) and \(Y^{(2)}\) and form the statistic \(_{}_{1/2}(X,Y^{(1)},Y^{(2)})-_{1/2}(Y^{(1)},X,Y ^{(2)})\), where \(_{1/2}\) is defined in Section 2.2. Then \(|\,\,_{}|\) is equal to \(0\) under the null hypothesis and is at least \(1-2_{1}\) under the alternative, where \(_{1}\) is the target total error probability of \(_{1/2}\). Taking \(_{1}=5\%\), by repeated sample splitting and majority voting we may amplify the success probability to \(\) provided

\[n c^{}(1/)}{^{2}},\] (7)

where \(c^{}>0\) is universal (see Appendix for details). The upper bound (7) partly recovers [37, Theorem 3 and 5] where authors show that thresholding the MMD with Gaussian kernel \(G_{}(x,y)=^{-d}(-\|x-y\|^{2}/^{2})\) achieves the minimax optimal sample complexity \(n^{-(2+d/2)/}\) for the problem of two-sample testing over the class \(_{,d}\) of \(d\)-dimensional \((,2)\)-Sobolev-smooth distributions (defined in Appendix B.3) under \(\)-\(L^{2}\)-separation. For this, taking \(^{1/}\) ensures that \(\|P-Q\|_{L^{2}}(P,Q)\) over \(P,Q_{,d}\). Taking e.g. \((x)=(-\|x\|_{2}^{2})x\) as the base measure, (7) recovers the claimed sample complexity since \(\|\|_{2}^{2}= G_{}^{2}(x,y)(x)(y)= (^{-d})\) hiding dimension dependent constants. Our result requires a bounded density with respect to a Gaussian.

Likelihood-Free Hypothesis TestingBy taking \( R=(1)\) in Theorem 3.1 we can recover many of the results in [18; 32; 33]. When \(\) is finite, we can take the kernel \(K(x,y)=_{z}\{x=y=z\}\) in Theorem 3.1 to obtain the results for bounded discrete distributions (defined in Appendix B.1) which state that under \(\)-TV-separation the minimax optimal sample complexity is given by \(m 1/^{2};\{n,\}|}/^ {2}\). A similar kernel recovers the optimal result for the class of \(\)-Holder smooth densities on the hypercube \(^{d}\) (see Appendix B.2).

Curse of DimensionalityUsing the Gaussian kernel \(G_{}\) as for two-sample testing above, one can conclude by Theorem 3.1 that the required number of samples for (\(\)) over the class \(_{,d}\) under \(\)-\(L^{2}\)-separation grows at most like \((()^{(d)}})\) for some \(c>1\), instead of the expected \((()^{(d)})\). This may be interpreted as theoretical support for the success of LFI in practice where signal and background can be rather different (cf. [5, Figures 2-3]) and the difficulty of the problem stems from the rate of signal events being small (i.e. \( 1\) but \( 1\)).

## 4 Learning Kernels from Data

Given a _fixed_ kernel \(K\), our Theorems 3.1 and 3.2 show that the sample complexity is heavily dependent on the separation \(\) under the given \(\) as well as the spectrum \(=(,K)\) of the kernel. Thus, to have good test performance we need to use a kernel \(K\) that is well-adapted to the problem at hand. In practice, however, instead of using a fixed kernel it would be only natural to use part of the simulation sample to try to _learn_ a good kernel.

In Sections 4 and 5 we report experimental results after _training_ a kernel parameterized by a neural network on part of the simulation data. In particular, due to the dependence between the data and the kernel, Theorem 3.1 doesn't directly apply. Our main contribution here is showing the existence of an asymmetric simulation-experimentation trade-off (cf. Figure 1.2 and also Section 1.1) even in this realistic setting. Figure 1.2 plots the _total_ number \(n\) of simulations used, including those used for training, so as to provide a realistic view of the amount of data used. The experiments also illustrate that the (trained-)kernel-based statistic of Section 2.2 achieves state-of-the-art performance.

### Proposed Training Algorithm

Consider splitting the data into three parts: \((X^{},Y^{})\) is used for training (optimizing) the kernel; \((X^{},Y^{})\) is used to evaluate our test statistic at test time; and \((X^{},Y^{})\) is used for calibrating the distribution of the test statistic under the null hypothesis. We write \(n_{s}=|X^{s}|=|Y^{s}|\) for \(s\{,,\}\). Given the training data \(X^{},Y^{}\) with empirical measures \(_{X^{}},_{Y^{}}\), we maximize the objective in

\[(X^{},Y^{};K)=_{u}^{2}( _{X^{}},_{Y^{}};K)}{(X^{},Y^{};K)},\] (8)

which was introduced in . Here \(^{2}\) is an estimator of the variance of \(_{u}^{2}(_{X^{}},_{Y^{}};K)\) and is defined in Appendix F.1.

Intuitively, the objective \(J\) aims to separate \(P_{X}\) from \(P_{Y}\) while keeping variance low. For a heuristic justification of its use for (\(\)) see Appendix.

In Algorithm 1 we describe the training and testing procedure, which produces unbiased \(p\)-values for (\(\)) when there is no misspecification (\(R=0\) in Theorem 3.1). During training, we use the Adam optimizer  with stochastic batches.

**Proposition 4.1**.: _When there is no misspecification (\(R=0\) in Theorem 3.1), Algorithm 1 outputs an unbiased estimate of the \(p\)-value that is consistent as \(\{n_{},k\}\)._

``` Input:\((X^{},X^{},X^{})\), \((Y^{},Y^{},Y^{})\); parametrized kernel \(K_{}\); hyperparameters and initialization.
Phase 1: Kernel training (optimization) on \(X^{}\) and \(Y^{}\). \(_{}^{}(X^{},Y^{};K_{})\); # maximize objective as defined in (8)
Phase 2: Distributional calibration of test statistic (under null hypothesis). for\(r=1,2,,k\)do \(Z^{,r}\) sample \(m\) points without replacement from \(X^{}\); \(T_{r}}m}_{i,j}(K_{}(Z_{i}^{ ,r},Y_{j}^{})-K_{}(Z_{i}^{,r},X_{j}^ {}))\); endfor # Phase 3: Inference with input \(Z\). \(}m}_{i,j}(K_{}(Z_{i},Y_{j}^{})-K_{}(Z_{i},X_{j}^{}))\); ```

**Output:** Estimated \(p\)-value: \(_{i=1}^{k}\{<T_{i}\}\). ```

**Algorithm 1**\(\) with a learned deep kernel

**Time complexity** Algorithm 1 runs in three separate stages: training, calibration, and inference. The first two take \(O(\# B^{2}+kn_{}m)\) total time, where \(B\) is the batch size, whereas Phase 3 takes only \(O(n_{}m)\) time, which is generally much faster especially if \(n_{}<\!\!<n_{}\).

**Sample usage** Empirically, data splitting in Algorithm 1 can have non-trivial effects on performance. Instead of training the kernel on only a fraction of the data (\(\{X^{},Y^{}\}\{X^{},Y^{}\}=\)), we discovered that taking \(\{X^{},Y^{}\}\{X^{},Y^{}\}\) results in more efficient use of data. The stronger condition \(\{X^{},Y^{}\}=\{X^{},Y^{}\}\) can also be applied; we take \(\) to reduce time complexity. We do, however, crucially require \(X^{},Y^{}\) in Phase 2 to be independently sampled ("held-out") for consistent \(p\)-value estimation. Finally, we remark also that splitting this way is only valid in the context of Algorithm 1. For the test (4) using the data-dependent threshold \(\), one needs \(\{X^{},Y^{}\}\{X^{},Y^{}\}=\) to estimate \(\).

### Classifier-Based Tests and Other Benchmarks

Let \(:\) be a classifier, assigning small values to \(P_{X}\) and high values to \(P_{Y}\) by minimizing cross-entropy loss of a classifier net. There are two natural test statistics based on \(\):

**Scheffe's Test.** The first idea, attributed to Scheffe in folklore [15, Section 6], is to take the statistic \(T(Z)=_{i=1}^{m}\{(Z_{i})>t\}\) where \(t\) is some (learn-able) threshold.

**Approximate Neyman-Pearson / Logit Methods.** If \(\) is trained to perfection, then \((z)=(P_{X}|z)\) would be the likelihood and \((z)/(1-(z))\) would equal precisely the likelihood ratio between \(P_{Y}\) and \(P_{X}\) at \(z\). This motivates the use of \(T(Z)=_{i=1}^{m}((Z_{i})/(1-(Z_{i})))\). See also .

Let us list the testing procedures that we benchmark against each other in our experiments.

1. **MMD-M**: The \(\) statistic (3) using \(K\) with the mixing architecture \[K(x,y)=[(1-)G_{}(_{}(x),_{}(y))+] G _{_{0}}(x+_{^{}}^{}(x),y+_{^{ }}^{}(y)).\] Here \(G_{}\) is the Gaussian kernel with variance \(^{2}\); \(_{},_{^{}}^{}\) are NN's (with parameters \(,^{}\)), and \(,_{0},,,^{}\) are trained.
2. **MMD-G**: The \(\) statistic (3) using the Gaussian kernel architecture \(K(x,y)=G_{}(_{}(x),_{}(y))\) where \(_{}\) is the feature mapping parametrized by a trained network and \(\) is a trainable parameter.
3. **MMD-O**: The \(\) statistic (3) using the Gaussian Kernel \(K(x,y)=G_{}(x,y)\) with optimized bandwidth \(\). First proposed in [7; 39].

4. **UME**: An interpretable model comparison algorithm proposed by , which evaluates the kernel mean embedding on a chosen "witness set".
5. **SCHE**, **LBI**: Scheffe's test and Logit Based Inference methods , based on a binary classifier network \(\) trained via cross-entropy, introduced above.
6. **RFM**: Recursive Feature Machines, a recently proposed kernel learning algorithm by .

### Additive Statistics and the Thresholding Trick

Given a function \(f\) (usually obtained by training) and test data \(Z=(Z_{1},,Z_{m})\), we call a test additive if its output is obtained by thresholding \(T_{f}(Z)_{i=1}^{m}f(Z_{i})\). We point out that all of **MMD-M/O/O**, **SCHE**, **LBI**, **UME**, **RFM** are of this form, see the Appendix for further details. Similarly to , we observe that any such statistic can be realized by our kernel-based approach.

**Proposition 4.2**.: _The kernel-based statistic defined in (3) with the kernel \(K(x,y)=f(x)f(y)\) is equal to \(T_{f}\) up to a multiplicative and additive constant independent of \(Z\)._

Motivated by the Scheffe's test, instead of directly thresholding the additive statistic \(T_{f}(Z)\), we found empirically that replacing \(f\) by \(f_{t}(x) 1\{f(x)>t\}\) can yield improved power. We set \(t\) by maximizing an estimate of the significance under the null using a normal approximation, i.e. by solving \(t_{}*{arg\,max}_{t}}(Y^{ })-T_{f_{t}}(X^{})}{}(X^{}) (1-T_{f_{t}}(X^{}))}}\), where \(X^{},Y^{}\) satisfy \(\{X^{},Y^{}\}(\{X^{},Y^{ }\}\{X^{},Y^{}\})=\). This trick improves the performance of our tester on the Higgs dataset in Section 5.2 but not for the image detection problem in Section 5.1.

## 5 Experiments

Our code can be found at https://github.com/Sr-11/LFI.

### Image Source Detection

Our first empirical study looks at the task of detecting whether images come from the CIFAR-10  dataset or a SOTA generative model (DDPM) [25; 51]. While source detection is on its own interesting, it turns out that detecting whether a group of images comes from the generative model versus the real dataset can be too "easy" (see experiments in ). Therefore, we consider a _mixed alternative_, where the alternative hypothesis is not simply the generative model but CIFAR with planted DDPM images. Namely, our \(n\) labeled images come from the following distributions:

\[P_{X}=, P_{Y}=+ .\] (9)

The goal is to test whether the \(m\) unlabeled observations \(Z\) have been corrupted with \(\) or more fraction of DDPM images (versus uncorrupted CIFAR); this corresponds to (\(\)) (or equivalently (\(\)) with \(=1\)). Figure 3 shows the performance of our approach with this mixed alternative.

Network ArchitectureWith a standard deep CNN, the difference is only at the final layer: for the kernel-based tests it is a feature output; for classifiers, we add an extra linear layer to logits.

Figure 3: Empirical performance on (9) for the CIFAR detection problem when \(n_{}=1920\). Plots from left to right are as follows. (a) rejection rate under the alternative if test rejects whenever the estimated \(p\)-value is smaller than \(5\%\); (b) expected \(p\)-value  under the alternative; (c) the average of type-1 and II error probabilities when thresholded at 0 (different from (4), see Appendix); and (d) ROC curves for different \(m\) using MMD-M and Algorithm 1. Shaded area shows the standard deviation across \(10\) independent runs. Missing benchmarks (thresholded MMD, MMD-O, LBI, RFM) are weaker; see Appendix for full plot.

We see from Figure 3 that our kernel-based test outperforms other benchmarks at a fixed training set size \(n_{tr}\). One potential cause is that \(\) has an "optimization" subroutine (which it solves in closed form) as it is an IPM. This additional layer of optimization may lead to better performance at small sample sizes. The thresholding trick does not seem to improve power empirically. We omit several benchmarks from this figure for graphic presentation and they do not exhibit good separating power; see the Appendix for the complete results. The bottom plot of Figure 1 shows \(m\) and \(n\) on \(\)-scale against the total probability of error, exhibiting the simulation-experimentation trade-off.

### Higgs-Boson Discovery

The 2012 announcement of the Higgs boson's discovery by the ATLAS and CMS experiments [1; 9] marked a significant milestone in physics. The statistical problem inherent in the experiment is well-modeled by (mLFHT), using a signal rate \(\) predicted by theory and misspecification parameter \(R=0\) (as was assumed in the original discovery). We consider our algorithm's power against past studies in the physics literature  as measured by the _significance of discovery_. We note an important distinction from Algorithm 1 in this application.

Estimating the SignificanceIn physics, the threshold for claiming a "discovery" is usually at a significance of \(5\), corresponding to a \(p\)-value of \(2.87 10^{-7}\). Approximately \(n_{}(2.87)^{-1} 10^{7}\) samples would be necessary for Algorithm 1 to reach such a precision. Fortunately the distribution of the test statistic is approximated by a Gaussian customarily. We adopt this approach for our experiment hereby assuming that \(m\) is large enough for the CLT to apply. We use the "expected significance of discovery" as our metric  which, for the additive statistic \(T_{f}=_{i=1}^{m}f(Z_{i})\), is given by \((Y^{})-T_{f}(X^{}))}{ r}(f(X^{}))/m}\). If the thresholding trick (Section 4.3) is applied we use the more precise Binomial tail, in which case the significance is estimated by \(-^{-1}(((m,T_{f_{}}(X^{})) T _{f_{}}(Z)))\), where \(\) is the standard normal CDF.

Newtowk ArchitectureThe architecture is a 6-layer feedforward net similar for all tests (kernel-based and classifiers) except for the last layer. We leave further details to the Appendix.

As can be seen in Figure 4, Scheffe's test and MMD-M with threshold \(t_{}\) are the best methods, achieving similar performance as the algorithm of ; reaching the significance level of \(5\) on 2.6 million simulated datapoints and a test sample made up of a mixture of 1000 backgrounds and 100 signals. The top plot of Figure 1 shows \(m\) and \(n\) on \(\)-scale against the total probability of error through performing the test (4), exhibiting the asymmetric simulation-experimentation trade-off.

## 6 Conclusion

In this paper, we introduced (mLFHT) as a theoretical model of real-world likelihood-free signal detection problems arising in science. We proposed a kernel-based test statistic and analyzed its minimax sample complexity, obtaining both upper (Theorem 3.1) and lower bounds (Theorem 3.2) in terms of multiple problem parameters, and discussed their tightness (Section 3.3) and connections to prior work (Section 3.4). On the empirical side, we described a method for training a parametrized kernel and proposed a consistent \(p\)-value estimate (Algorithm 1 and Proposition 4.1). We examined the performance of our method in two experiments and found that parametrized kernels achieve state-of-the-art performance compared to relevant benchmarks from the literature. Moreover, we confirmed experimentally the existence of the asymmetric simulation-experimentation trade-off (Figure 1.2) which is suggested by minimax analysis. We defer further special cases of Theorem 3.1, all relevant proofs and experimental details to the Appendix.

Figure 4: Expected significance of discovery on a mixture of 1000 backgrounds and 100 signals in the Higgs experiment. Shaded area shows the standard deviation over 10 independent runs. See Appendix for full plot including missing benchmarks.