# Codec Avatar Studio: Paired Human Captures

for Complete, Driveable, and Generalizable Avatars

Julieta Martinez\({}^{1}\), Emily Kim\({}^{2}\), Javier Romero\({}^{1}\), Timur Bagautdinov\({}^{1}\), Shunsuke Saito\({}^{1}\), Shoou-I Yu\({}^{1}\), Stuart Anderson\({}^{1}\), Michael Zollhofer\({}^{1}\), Te-Li Wang\({}^{1}\), Shaojie Bai\({}^{1}\), Chenghui Li\({}^{1}\), Shih-En Wei\({}^{1}\), Rohan Joshi\({}^{1}\), Wyatt Borosos\({}^{1}\), Tomas Simon\({}^{1}\), Jason Saragih\({}^{1}\), Paul Theodosis\({}^{1}\), Alexander Greene\({}^{1}\), Anjani Josyula\({}^{1}\), Silvio Mano Maeta\({}^{1}\), Andrew I. Jewett\({}^{1}\), Simon Venshtain\({}^{1}\), Christopher Heilman\({}^{1}\), Yueh-Tung Chen\({}^{1}\), Sidi Fu\({}^{1}\), Mohamed Ezzeldin A. Elshaer\({}^{1}\), Tingfang Du\({}^{1}\), Longhua Wu\({}^{1}\), Shen-Chi Chen\({}^{1}\), Kai Kang\({}^{1}\), Michael Wu\({}^{1}\), Youssef Emad\({}^{1}\), Steven Longay\({}^{1}\), Ashley Brewer\({}^{1}\), Hitesh Shah\({}^{1}\), James Booth\({}^{1}\), Taylor Koska\({}^{1}\), Kayla Haidle\({}^{1}\), Matt Andromalos\({}^{1}\), Joanna Hsu\({}^{1}\), Thomas Dauer\({}^{1}\), Peter Selednik\({}^{1}\), Tim Godisart\({}^{1}\), Scott Ardisson\({}^{1}\), Matthew Cipperly\({}^{1}\), Ben Humberston\({}^{1}\), Lon Farr\({}^{1}\), Bob Hansen\({}^{1}\), Peihong Guo\({}^{1}\), Dave Braun\({}^{1}\), Steven Krenn\({}^{1}\), He Wen\({}^{1}\), Lucas Evans\({}^{1}\), Natalia Fadeeva\({}^{1}\), Matthew Stewart\({}^{1}\), Gabriel Schwartz\({}^{1}\), Divam Gupta\({}^{1}\), Gyeongsik Moon\({}^{1}\), Kaiwen Guo\({}^{1}\), Yuan Dong\({}^{1}\), Yichen Xu\({}^{1}\), Takaaki Shiratori\({}^{1}\), Fabian Prada\({}^{1}\), Bernardo R. Pires\({}^{1}\), Bo Peng\({}^{1}\), Julia Buffalini\({}^{1}\), Autumn Trimble\({}^{1}\), Kevyn McPhail\({}^{1}\), Melissa Schoeller\({}^{1}\), Yaser Sheikh\({}^{1}\)

\({}^{1}\)Codec Avatars Team, Meta Reality Labs Research

\({}^{2}\) Robotics Institute, Carnegie Mellon University

###### Abstract

To create photorealistic avatars that users can embody, human modeling must be complete (encompass the full body), driveable (able to reproduce motion of the user from lightweight sensors), and generalizable (_i.e._, easily adaptable to novel identities). Towards these goals, _paired_ captures, that is, captures of the same subject obtained from systems of diverse quality and availability, are crucial. However, paired captures are rarely available to researchers outside of dedicated industrial labs: _Codec Avatar Studio_ is our proposal to close this gap. Towards generalization and driveability, we introduce a dataset of 256 subjects captured in two modalities: high resolution multi-view scans of their heads, and video from the internal cameras of a headset. Towards completeness, we introduce a dataset of 4 subjects captured in eight modalities: high quality relightable multi-view captures of heads and hands, full body multi-view captures with minimal and regular clothes, and corresponding head, hands and body phone captures. Together with our data, we also provide code and pre-trained models for different state-of-the-art human generation models. Our datasets and code are available at https://github.com/facebookresearch/ava-256 and https://github.com/facebookresearch/goliath.

## 1 Introduction

People are deeply social creatures. It is no coincidence that remote communication has frequently been one of the first applications of new technologies. Letters, telegrams, phone calls, instant messaging, and video conferencing are well-known examples of peer-to-peer communication technologies that have helped users stay in touch with the people they care about, and create new social connections.

Virtual reality (VR) and adjacent technologies promise to enable novel forms of remote social interaction. In particular, headsets with accurate localization let users navigate 3d environments naturally: by simply moving around. Coupled with virtual models of people (_i.e._, photorealistic avatars) and a mechanism to drive them, it is possible to build systems where virtually-embodied users interact with each other in virtual 3d spaces. We can imagine that these environments will enable modes of communication more and more similar to in-person interactions once certain conditions are met: reduced latency, increased realism of the models, and more accurate control of those avatars.

We argue that, for photorealistic avatars to be deployed at scale, models of human appearance must fulfill certain conditions. They should be (1) complete, that is, not just be limited to faces and hands, but encompass full bodies. Models also need to be (2) driveable, that is, it must be possible to build mechanisms that track motion and appearance changes of the user with as little interference as possible, such that these changes can be transmitted compactly and reproduced on the other end. Finally, avatars must be (3) generalizable, as in it must be possible to build avatars for new users quickly, without access to expensive capture setups. To achieve these goals, _paired_ captures, that is, captures of the same people using different devices are crucial. For example, to build or evaluate a mechanism that drives face avatars from a headset, it is necessary to collect data of the same people both in a high resolution scanner (to build a high quality avatar), and from the headset itself. Similarly, towards generalizable avatars, one may want to study the creation of high quality avatars from lower-quality but more widely available capture setups, such as phone scans. In that case, it is necessary to capture the same person under both a high resolution scanner, and from a phone scan. Unfortunately, paired captures are not usually available to researchers outside of specialized industrial labs, and we believe that this limitation has overall slowed down progress in avatar generation.

In this paper, our goal is to close this gap by providing the research community with a series of datasets of paired captures, as well as state-of-the-art implementations of avatar models. First, we introduce Ava-256, a dataset of 256 subjects, each captured in both a high resolution dome with dozens of views (meant for avatar creation), and while wearing a commercial headset with several infrared cameras (meant for driving). Second, we introduce Goliath-4, a dataset of 4 subjects, each captured under 8 modalities: A high resolution dome with relightable captures of heads and hands; two full body captures with regular and minimal clothing, and corresponding phone scans for heads, hands, and full body. Besides the raw data, we also provide several assets, such as 3d mesh tracking, semantic segmentation, and 3d keypoints. Moreover, we also provide code and pre-trained models for personalized relightable head , personalized relightable hand , and personalized full body  avatars, as well as multi-person head avatars , and an out-of-the-box driver from headset images . Finally, the phone captures are compatible with systems for instant creation of head  and hand models . Given its size and scope, we refer to our joint dataset and code release as _Codec Avatar Studio_. Our goal is for Codec Avatar Studio to become a toolkit that academics can use to bootstrap their engagement with the fundamental problems of photorealistic telepresence.

## 2 Datasets of Paired Human Captures

In this Section we introduce our head dataset, Ava-256, and our full-body dataset, Goliath-4.

Subjects and license.For all captures, participants were at least 18 years old at the time of the session, and provided their written informed consent for the capture and its distribution. Specifically, the consent forms contain our contact information, the purpose of the study, a description of the capture devices and capture procedures, and the estimated time the study will take. Additionally, the forms state that participation is voluntary, that participants may withdraw at any time and still get compensated for their time, that the data will be used for research purposes, and that Meta is the licenser and owner of the collected data, which may be distributed to third parties in the future. Note that participants keep a copy of the consent form after the study. Finally, the participants were compensated with USD $50 per hour, rounded to the next half hour. We estimate that participants were paid USD $14 000 in total. We release all our data and assets under the CC-by-NC 4.0 License.

### Ava-256

Ava-256 is the first publicly available dataset of subjects collected in a high resolution dome as well as from a commercially available headset. Ava-256 was collected in a span of twenty-five months,

[MISSING_PAGE_EMPTY:3]

Following the trend from foundational models , we instead make 4 more accessible releases of 4, 8, 16, and 32 TB, each with different trade-offs of space against quality. We target a minimal size of 4TB since this is the current capacity of new hard and solid state drives under USD $50 and USD $200, respectively.1 The higher quality versions may be used by researchers with more resources, and will become more accessible over time with decreasing storage costs.

For a given storage budget, our goal is to get the highest quality images into the hands of researchers. To determine the most suitable image format for our dataset, we selected 4 images from each capture, sampling cameras and frames uniformly at random, and evaluated the peak signal-to-noise ratio (PSNR), and compression and decompression times of WEBP, JPEG, JPEGXL, and AVIF formats - the latter two chosen for their user-friendly licenses (Fig. 4). Given its superior quality-to-space ratio, although higher yet acceptable decompression time, we have chosen to distribute our dataset in AVIF format. We are aware that this format is less commonly used in the machine learning community, and have thus dedicated engineering time to improve the most popular Pillow plugin for AVIF2.

Compressing images is not enough, since our target release sizes would require compression parameters that would result in too low PSNR for avatar creation. To fit the data into reasonably sized packages we also downsampled the resolution of our images (2\(\) and 4\(\)), the number of cameras (\(\)2\(\) down to 80), and the frames per second (2\(\) and 4\(\), from 30 to 15 and 7.5). We chose cameras manually, making sure there is full 360\({}^{}\) coverage of the head, and that there is always a frontal-facing camera. A summary of our Ava-256 releases can be found in Table 1.

    &  &  \\   &  &  &  &  &  &  \\
**Size** & **FPS** & **resolution** & **per subject** & **quality** & **FPS** & **resolution** & **per subject** & **quality** \\ 
4 TB & 7.5 & 1 024 \(\) 667 & \(\) 5 000 & 63 & 9 & 400 \(\) 400 & \(\) 2 0000 & 70 \\
8 TB & 5.0 & 1 024 \(\) 667 & \(\) 10 000 & 63 & 18 & 400 \(\) 400 & \(\) 4 0000 & 70 \\
16 TB & 7.5 & 2048 \(\) 1334 & \(\) 5 000 & 70 & 9 & 400 \(\) 400 & \(\) 20 000 & 70 \\
32 TB & 15.0 & 2 048 \(\) 1334 & \(\) 10 000 & 70 & 18 & 400 \(\) 400 & \(\) 4 0000 & 70 \\   

Table 1: Details of our different size-friendly Ava-256 releases.

Figure 4: Compression comparison of WEBP, JPEG, JPEGXL and AVIF formats, the latter two using 4 threads. The points are measurements, and we fit a line to make it easier to spot the trend. For different levels of quality (qX in the plot), AVIF is a clear winner for our data.

Figure 3: 10 frames captured by one of our augmented Quest Pro headsets. “Release” frames are included as part of ava-256, while “Augmented” ones are only used to aid in ground truth computation.

### Goliath-4

We complement the subject diversity and driveability of Ava-256 with Goliath-4, which focuses on extending the scope of the scans to the full body. Towards this goal, it is not enough to simply provide full body captures from a multiview scanner. First, since the resolution of multiview full-body captures in critical areas like head and hands is insufficient, we also provide captures of higher pixel density for those areas. Second, on top of the uniform illumination used in the body captures and Ava-256, Goliath-4 head and hand captures include interleaved shots with what we call "group lights": a variation of one-light-at-a-time (OLAT) in which we directionally illuminate the scene with a small group of adjacent lights, to increase the brightness and reduce aliasing compared to OLAT.

In full body captures, unlike head or hand scans, most of the skin is covered by clothes. This represents a challenge for some reconstruction methods, since body models  are often created from subjects in tight-fit clothing. In order to enable research on the relation between clothed and tight-fit clothing subjects, we additionally provide comprehensive minimally-clothed full body captures.

The data described so far can only be captured with expensive multiview scanning systems available in dedicated research labs, and is thus inaccessible to most researchers. To bridge this gap, we include a phone capture corresponding to each of the previously described ones--head, hands, clothed bodies and minimally clothed bodies. These types of captures already enable the creation of head and hand avatars for everyday users when put together with large scale high quality datasets .

Capture dayEach subject is captured in three different rooms: the relightable scanner for heads and hands, a normal room for phone captures, and a full body scanner for bodies. The capture lasts for around 5 hours, including lunch breaks and transportation across scanners in different buildings.

Data packageThe total recording time per participant is 135 minutes on average, which can be further broken down to 25, 25, 28, and 57-minutes for head, hand, normally clothed full body, and minimally clothed full body recordings respectively. The original sequences, compressed losslessly, take around 762 TB of storage per participant. Similar to Section 2.1, we compressed the original sequences with AVIF quality 63, lowered the image resolution 2x (from 4 096 \(\) 2 668 to 2 048 \(\) 1 334) and subsampled in time to produce effective training set sizes of roughly 10 000 frames.

Head scans.The head scans were collected from the same system as Ava-256, following the same set of actions. The main difference is that the lighting is time-multiplexed instead of constant. Images are collected at 90 frames per second, while one out of every three frames is captured with all \(\) 400 lights (SmartVision SX30) being on during 120 us, and the other two out of three frames have either a "group" light or a random configuration of lights turned on. The group light  resembles OLAT  where one of the lights and its four nearest neighbors are turned on for 2.2 ms. The random configuration includes either 5 or 10 random lights turned on for 5 and 4 ms, respectively. We divide the captures into a train and test split. The test split includes a full segment of varied extreme face expressions, and thirty seconds of a conversation segment. The train split includes all remaining segments as well as four minutes of conversation. The conversation frames in the train split are not only disjoint from the test split, but also separated by a buffer of 30 seconds to avoid temporal leaking of information between the train and test splits. To enable a reasonable size footprint, the train split is subsampled at 10 frames per second.

Hand scans.Hand scans are collected in the same system as the head scans, also in a time-multiplexed manner. In this case, half (instead of one third for the head captures) of the frames are fully-lit, while the other half use either group or random illuminations. We collected data for both right and left hand separately. The test split includes a segment with free hand motion, while the rest of the segments are included in the train split after being subsampled at 5 frames per second.

Full-body scan.Collecting data for full-body motion requires a larger dome: This capture system has a diameter of 5.5m with \(\) 230 cameras that acquire images at 30 frames per second under constant illumination. Subjects were captured twice, once in minimal clothing and once wearing their own garments, _i.e._, normal clothing. The script followed in the normal clothing capture is a subset of the minimal one, where segments involving only head and hand motions have been removed. The test set contains a sequence where the subjects play charades, describing with their body motionspecific concepts. The training set contains the rest of the sequences, and it is sampled at 10 frames per second for the clothed capture, and 5 frames per second for the minimal clothed capture.

Phone scan.Phone scans were taken in the full body scanner dome for minimal clothed full body, and in a separate room for the head, hand, and normally clothed full body sessions. The phone was placed on a tripod for all sequences in which it was not handheld. We collected RGB videos at 1 080 \(\) 1 920 and RGB images at 2 316 \(\) 3 088 resolution. The RGB videos contain also a depth video collected from the depth sensor of an iPhone 11, at resolution 360 \(\) 640 and 30 frames per second.

Assets.For our multi-view data, we provide foreground-background and part segmentation, 3D keypoints, 3D reconstruction and registration (see Figure 5). Camera calibration is included, as well as light calibration for relightable sessions. Segmentation and 2D keypoints are provided for head and hand phone data, while no assets are provided for full body phone data in this release.

## 3 Code and Models

In this Section, we summarize the details of the multi-person face model as well as the the personalized head, hands, and body models, which are trained using our datasets. We release our code and models under the CC-by-NC 4.0 License.

### Multi-identity face modelling

For high quality head avatars, we provide an implementation of the multi-person face model in Cao et al. . Faces are represented as a collection of small semi-transparent cubes, or primitives, referred to as a Mixture of Volumetric Primitives (MVP) , which enable real-time rendering of dynamic volumetric content at high resolution.

The training setup aims to disentangle the identity and expressions of each subject, and to create a consistent expression space for multiple avatars. In particular, a 2D convolutional decoder progressively upsamples a latent _expression_ code into two branches: (1) an appearance branch that outputs the RGB colour of each primitive, and (2) a geometry branch that outputs the vertices of a tracked mesh, as well as the 3d pose (relative to that mesh) and transparency of each primitive. The MVP representation enables end-to-end training of the model with a differentiable volumetric renderer.

To encourage a separation of identity and expression (see Fig. 6), the feature maps of the decoder are initialized by learned biases extracted from geometry and UV-texture maps of the neutral expression of the reconstructed subject. Similarly, a 2D convolutional variational encoder produces the expression codes from geometry and UV-texture maps subtracted from a canonical neutral frame. At test time, these codes can be replaced with ones produced by headset-mounted cameras, as described in 3.2.

Figure 5: Goliath-4 Assets. The first two rows show full body clothed assets, minimal full body assets, head and hand assets. Each of the blocks show two segmentations, 3D keypoints, 3D reconstruction, and template registration. The last row shows phone assets: full body clothed and minimal (static picture and hand-held video), head and hands (segmentation, keypoints and depth).

### Generalizable face encoder

The generalizable face encoder maps headset IR images to the latent expression space learned by the multi-identity face model. A key challenge here is obtaining a reliable pseudo ground truth mapping from headset images to face deformations, after which the problem can be posed as supervised learning. We generate this pseudo ground truth using the method of Wei et al. , based on cycle consistency  between renders of personalized avatar models  and images from a headset. By jointly solving for the expression code and the relative head pose of each frame, as well as for the domain-transfer between the dome and headset images, we obtain person-specific latent expression codes for all the frames of a headset capture.

To train a multi-person encoder, we follow a simplified version of the method due to Bai et al. , where we relate the person-specific codes to the multi-person expression codes via pseudo-frontal renderings of the avatars. Specifically, for each training sample, we render the person-specific decoder with a randomly sampled frontal camera. We then train an encoder that maps headset images to the multi-person latent expression space. This encoder is supervised by decoding the latent expressions using the universal face decoder from Section 3.1, and comparing it with the rendered person-specific avatar. The model is trained for all frames of the headset captures, across all training subjects, to produce a single generalizable encoder that can be applied to headset inputs from unseen subjects.

Our release includes person-specific codes for all Ava-256 headset captures, as well as the frontal renderings of the person-specific decoders to facilitate explorations of new generalizable encoders. We also provide code and assets necessary to train a generalizable encoder. Please refer to Figure 7 for a visualization of the provided driver.

### Personalized relightable Gaussian heads

The relightable head avatars are based on Relightable Gaussian Codec Avatars (RGCA) , which achieve state-of-the-art performance on avatar modeling and relighting. The geometric representation is based on 3D Gaussians, which can be rendered with EWA splatting  in real-time. This representation is particularly suitable to model thin structures such as skin details and hair strands. To support avatar relighting, RGCA models global light transport as learnable diffuse spherical harmonics and specular spherical Gaussians, which enables relighting from arbitrary light sources in real-time despite being trained on discrete point lights. Besides from controlling the environment light, the expression of the avatars can be modified through a latent vector, as with regular avatars. See Figure 7(b) for a qualitative sample of this model.

Figure 6: Two demonstrations of the consistent expression space produced by our multi-identity face model. Each panel shows the ground truth image, the reconstruction of the same subject, the same expression code rendered on three other subjects.

Figure 7: Generalizable encoder driving 3 subjects from headset images.

### Personalized relightable hands with MVP

We base the relightable hand avatars on RelightableHands . This work generalizes across different poses and light conditions in the presence of hand articulation by incorporating explicit shadow features computed by Deep Shadow Maps . The hand geometry is represented by an articulated MVP  to support both articulation and volumetric modeling without compromising the efficiency of rendering. Hand pose is represented by joint angles and can be easily transferred across subjects. See Figure 7(a) for a qualitative sample of this model.

### Personalized full-body decoders

Full-body person-specific avatars are based on driving-signal-aware codec avatars . This work uses a mesh-based neural representation. A body mesh and a view-dependent texture are decoded with a pose and latent code conditioned convolutional neural network, which are then rendered with an efficient differentiable renderer . Generalization is achieved through a collection of disentangling techniques - localizing pose-dependent deformations, separating non-pose-dependent deformations into a disentengled latent space, and using an explicit physics-inspired prior for shadow modelling. See to Figure 9 for qualitative examples of these models.

## 4 Limitations and Potential Negative Impact

Codec Avatar Studio opens numerous exciting research opportunities that were previously unavailable to academic researchers, but some challenges remain unsolved. For example, while ava-256 supports the creation of multi-person driveable decoders and Goliath-4 supports the creation of relightable head decoders, it will likely be hard to create multi-identity relightable head decoders with our release. Our datasets also fail to capture the long tail of human expressivity, such as the ability to convey sweat, tiredness, and blood flow. Goliath-4 allows experimentation with converting low-quality data (e.g., from mobile or full-body captures) into higher-quality outputs (such as multi-view head and hands). However, its sample size is limited to four subjects, restricting the scope of this research.

Figure 8: Qualitative results of hand and face models based on mixtures of volumetric primitives (left) and Gaussian splatting (right). Ground truth, model prediction, and difference image.

Figure 9: Qualitative results of full-body models based on mesh and neural texture. Ground truth, model prediction, and difference image.

Potential negative impact.Our releases share potential negative impact with other research related to the modelling of human appearance. In particular, more accurate controllable models of human appearance could be misused for impersonation or to faciliate the spread of disinformation. Potential mitigations may include authentication protocols for verifying the origin of information, avatar watermarking, and research into computer interfaces that clearly communicate whether media has been produced using photorealistic avatars.

## 5 Related Work

In this Section, we review the various types of human datasets for avatar generation. The discussion is divided into three parts: head, body, and hands datasets.

### Datasets for Head Avatar Generation

The growing interest in developing high-quality human head and face models has led to the collection of large-scale, multi-view head and face capture datasets. One notable example is HUMBI-Face , which captured facial expressions using 68 HD cameras from 772 subjects at a resolution of 1 920 \(\) 1 080, significantly increasing the scale of human datasets. However, HUMBI-Face, while expanding the number of views and subjects, only captured 20 expressions and excluded conversations or speech. Similarly, Facespace  recorded avatar data from 847 subjects with 20 expressions at a resolution of 4 344 \(\) 2 896, but released multi-view image data for only 359 subjects. On the other hand, i3DMM  involved fewer subjects and expressions, with 64 subjects performing 10 expressions, but featured a higher number of views with 137 camera angles. RenderMe-360  is the more recent large-scale subject for face modelling; it aimed to enhance the diversity of subject appearance by varying hairstyles with wigs and incorporating various makeup styles among 500 subjects. The authors captured 12 expressions plus 25 to 42 sentences, using 60 camera views recorded at a resolution of 2 448 \(\) 2 048. We summarize these datasets, plus others which are smaller in scale (with fewer than 60 IDs or fewer than 10 views) in Table 2.

Our Ava-256 dataset aims to provide comparable variability in the number of subjects, camera views, and expressions, while also including headset data that no previous dataset offers. Additionally, we provide four distribution-friendly release sizes to facilitate accessibility and long-term maintenance.

    & &  & & & **Headset** \\
**Dataset name** & **IDs** & **Views** & **+ expressions** & **Images** & **Resolution** & **Size** & **captures** \\  _DiDFACS\({}^{}\)_ & 10 & 6 & 19.97 & - & 1 280 \(\) 1024 & - & ✗ \\ _CMU Multi-PIE_ & 337 & 15 & 6 & 750K & 3 072 \(\) 2048 & 0.3 TB & ✗ \\ _IDFAIP\({}^{}\)_ & 180 & 7 & 15 & - & 1 600 \(\) 1200 & - & ✗ \\ _MEAD_ & 48 & 7 & (8) & - & 1 920 \(\) 1080 & 0.9 TB & ✗ \\ _Imoriginal Light-Field_ & 5 & 16 & - & - & 2 048 \(\) 1088 & 0.1 TB & ✗ \\ _Multiface_ & 13 & 40/150 & 168 & 153M & 2 048 \(\) 1334 & 65.0 TB & ✗ \\ _NeComble_ & 222 & 16 & 25 & 31.7M & 3 208 \(\) 2200 & 1.0 TB & ✗ \\  _HUMBI Face_ & 617\({}^{}\) & 68 & 20 & 17.3M & 200 \(\) 150\({}^{}\) & 1.3 TB & ✗ \\ _Faccespace _ & 359\({}^{}\) & 68 & 20 & 400K & 4 344 \(\) 2896 & 0.9 TB & ✗ \\ _i3DMM\({}^{}\)_ & 64 & 137 & 10 & - & - & - & ✗ \\ _RenderMe-360_ & 500\({}^{}\) & 60 & 37/54 & 243M & 2.448 \(\) 2048 & 5.8 TB & ✗ \\  _Ava-256 Dome Captures (Ours)_ & 256 & 80 & 35 & 217 M & 2 048 \(\) 1334 & 32.0 TB & ✓ \\    \({}^{1}\) We only found the data of 403 subjects available online. We have contacted the authors to clarify this discrepancy.

\({}^{2}\) Out of 847 total subjects captured, only 359 subjects are available with multi-view image data.

\({}^{3}\) 500 captures are expected to be released later in 2024, but only 21 subjects were available online at time of submission. 5.8 TB is the size of this smaller subset containing about 4.2 \(\%\) of the data, so we expect the final release size to be well over 100 TB.

\({}^{4}\) We requested access to the dataset on May 20th but have not heard back at the time of submission, and are thus unable to obtain more data about this dataset.

\({}^{5}\) The head images are cropped from the full-body 1 920 \(\) 1 080 image, and are thus of low resolution.

\({}^{6}\) Dataset was not available on the website.

Table 2: Summary of multi-view face datasets. Smaller scale dataset (fewer than 60 ids or 20 views) as shown first, then large-scale datasets, then ours.

### Datasets for Body Avatar Generation

The interest in human anthropometrics drove the creation of the first large full body datasets. One of the first large data collection efforts was CAESAR , composed by three colored laser scans for thousands of subjects. Private companies like Humanetrics have continued similar studies devoted to anthropometrics, collecting thousands of full body scans across Europe and North-America . In academia, one of the first uses of 3D full body datasets was the creation of statistical body models. Anguelov et al.  created SCAPE, a single subject deformable model from 71 registered meshes of different poses. Allen et al.  created a multi-subject geometry model by combining data from 44 different subjects with 80 more scans of 5 subjects in different poses. Later models increased the amounts of data used for statistical full body models to hundreds , thousands , more than ten thousand , tens of thousands , and a million  samples. Other common use of geometry-only datasets have been modeling clothes [69; 33]. The increasing size of these datasets has resulted in improved accuracy and generalizability of the body models. However, the data typically lacks high quality appearance information, making these models inadequate for image generation.

Laser scans in the early 2 000s were replaced by multi-view photogrammetry scans, which capture high quality imagery as well as geometry (based on either active or passive stereo). One of the first uses of images in these captures was to improve the registration to a deformable template [8; 9]. However, thanks to deep neural networks, full body avatars that model not only geometry but also appearance became more common. X-Avatar  created an animatable implicit human avatar including geometry and appearance based on the X-Humans dataset, which includes more than 35 500 scans. A large number of systems and datasets have been devoted to model, replace or remove garments. Multiple synthetic datasets have been proposed to learn garment geometry and dynamics [6; 72; 50]. To estimate body shape under clothing, [64; 69] collected each six subjects performing multiple motions both in minimal clothing as well as other garments. Our proposal of collecting minimal and casual data is similar to theirs, but using more cameras of higher resolution and complementing with other modalities (i.e. phone, relightable heads and hands). The THuman3.0 dataset  improved scan quality, and garment variety over its previous versions [66; 52] to reconstruct and model garments from scans. Sizer  shares the same goal, for which it collected 2 000 scans from 100 subjects. 4DHumanOutfit  focused on large motions of more than thousand sequences thanks to their ample color scan space. Recently, 4D-Dress  released 78 000 scans focused on garment variety, including 64 different outfits and corresponding semantic vertex labels.

The closest effort to Goliath-4 is Human , which collects a multimodal dataset including mobile phone data, multiview RGBD and handheld scans for 1 000 subjects resulting in more than sixty million frames in total. While their system focuses on subject variety, Goliath-4 provides higher quality data for each of our four subjects, including high quality geometry and texture for each full-body frame, as well as high quality relightable head and hand sessions.

### Datasets for Hand Avatar Generation

The popularity of applications like 3D hand estimation from images have fostered the appearance of a large number of datasets devoted to hands. However, most of these datasets are composed by real images annotated with either joint positions [68; 57; 38; 27] or 3D model parameters [42; 71; 14; 10; 56], and dedicated almost exclusively to model the geometry of the hand. HTML  used 51 handheld scans to create a PCA space of the right hand texture. Handy  increased the variety and quality of their models by collecting more than 1 200 scans with a 3dMD multiview scanner which included high quality textures. The release of Interhand2.6M  enabled the recent creation of multiple high quality appearance models [37; 17; 15; 24; 28]. However, this dataset has two drawbacks: its illumination is fixed (which is improved with , but for synthetic data), and it does not include the rest of the body. We fix those two shortcomings in Goliath-4, although Interhand2.6M is still valuable given it contains hand interaction and larger subject variety than our proposed dataset.