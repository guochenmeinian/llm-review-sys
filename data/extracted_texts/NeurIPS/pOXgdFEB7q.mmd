# What Variables Affect Out-of-Distribution

Generalization in Pretrained Models?

 Md Yousuf Harun\({}^{1,}\)1

Kyungbok Lee\({}^{2,}\)1

Jhair Gallardo\({}^{1}\)

Giri Krishnan\({}^{3}\)Christopher Kanan\({}^{2}\)

\({}^{1}\)Rochester Institute of Technology

\({}^{2}\)University of Rochester

\({}^{3}\)Georgia Tech

Equal contribution. Corresponding author: Md Yousuf Harun (mh1023@rit.edu)Project website and code: https://yousuf907.github.io/oodg

###### Abstract

Embeddings produced by pre-trained deep neural networks (DNNs) are widely used; however, their efficacy for downstream tasks can vary widely. We study the factors influencing transferability and out-of-distribution (OOD) generalization of pre-trained DNN embeddings through the lens of the tunnel effect hypothesis, which is closely related to intermediate neural collapse. This hypothesis suggests that deeper DNN layers compress representations and hinder OOD generalization. Contrary to earlier work, our experiments show this is not a universal phenomenon. We comprehensively investigate the impact of DNN architecture, training data, image resolution, and augmentations on transferability. We identify that training with high-resolution datasets containing many classes greatly reduces representation compression and improves transferability. Our results emphasize the danger of generalizing findings from toy datasets to broader contexts.

## 1 Introduction

Understanding deep neural network (DNN) representations has been a central focus of the deep learning community. It is generally accepted that a DNN's initial layers learn transferable universal features, with deeper layers being task-specific . At NeurIPS-2023,  challenged this view with evidence for the tunnel effect hypothesis:

**The Tunnel Effect Hypothesis:** An overparameterized \(N\)-layer DNN forms two distinct groups:

1. _The extractor_ consists of the first \(K\) layers, creating linearly separable representations.
2. _The tunnel_ comprises the remaining \(N-K\) layers, compressing representations and hindering OOD generalization.

To test the tunnel effect hypothesis, they trained models on an in-distribution (ID) dataset and compared linear probes trained and evaluated on either ID or OOD datasets for embeddings produced by each DNN layer. They showed that ID accuracy increased monotonically, whereas OOD accuracy rapidly decreased after the extractor (Fig. 1). The likely cause of the tunnel effect is intermediate neural collapse . Both  and  showed that collapsed/tunnel layers could be static without needing learning. If these results are universal, they suggest universal visual features learned in early layers and using embeddings from the penultimate layers of pre-trained DNNs, need to be rethought. However, both  and  limited their experiments to datasets with low-resolution images and relatively few categories (CIFAR-10, MNIST). Given the widespread use of embeddings from pre-trained DNNs for downstream OOD tasks, we aim to assess the universality of the tunnel effect and the variables that influence its strength1.

If the tunnel effect is stronger in models trained on toy datasets like CIFAR-10 but diminishes for large-scale datasets, this may explain why many algorithms evaluated only on toy datasets are ineffective on larger datasets such as ImageNet. This disparity has persisted across problem settings, including open set classification , active learning , OOD detection , uncertainty quantification , dataset distillation , and continual learning [16; 17; 18].

**Our paper makes the following contributions:**

1. We define metrics to measure the _strength_ of the tunnel effect and use a SHAP-based analysis to assess each variable's impact, e.g., image resolution, number of semantic classes, and DNN architecture. Using 64 pre-trained ID backbones and 8,604 linear probes, we identify conditions that exacerbate, reduce, and eliminate the tunnel effect.
2. Using our metrics, we find that widely used ImageNet-1K pre-trained CNN and ViT backbones do not exhibit tunnels, except for ResNet-50.
3. In contrast to , we find that the tunnel strongly impacts forgetting in continual learning. This suggests the generality of many continual learning systems depends on tunnel strength, which is heavily influenced by architectural and training dataset choices.
4. We establish a link between impaired OOD generalization and the characteristics of widely used toy datasets, with both resolution and a small number of classes exacerbating the tunnel effect.
5. We propose a revised tunnel effect hypothesis, in which the tunnel's _strength_ is influenced by training data diversity.

## 2 Related Work

### The Tunnel Effect

Strong evidence for the tunnel effect was given in , but their experiments are limited. First, they only study MLPs and CNNs, whereas ViT models are now widely used. Second, their experiments only use \(32 32\) images, and we hypothesize that higher resolution images could mitigate the tunnel effect by promoting learning hierarchical representations. Third, they do not control for the impact of data augmentation, where data augmentation is known to improve OOD generalization [19; 20; 21; 22; 23; 24]. Lastly, they define tunnel as starting at the layer where a linear probe on the ID dataset achieves at least 95% of the final ID accuracy, ignoring OOD generalization. This is problematic since OOD generalization is central to their tunnel effect definition. Here, we measure _tunnel effect strength_ using OOD performance.

### Learning Embeddings that Generalize

Pretrained DNNs are widely used to produce embeddings for downstream tasks, e.g., DNNs trained on ImageNet generalize effectively to many computer vision tasks [25; 26; 27]. Several studies [28; 29; 30; 31] observe that ImageNet ID accuracy is highly predictive of OOD accuracy, while others [32; 33; 34; 22] find that observation does not always hold. Another example is CLIP , which has better OOD generalization due to larger and more diverse training data . We disentangle the role of data quantity versus the level of semantic variability.

Many works suggest that representations learned in earlier layers are more universal across image datasets whereas later layers are more task-specific [1; 2; 3; 4; 5; 6; 7; 8; 32]. This observation has guided the development of many transfer and continual learning methods. We revisit this phenomenon by studying the impact of DNN architecture on OOD generalization.

Figure 1: **The tunnel effect.** The tunnel impedes OOD generalization, which we study using linear probes trained on ID and OOD datasets for each layer. In this example, identical VGGm-17 architectures are trained on identical ID datasets, where only the resolution is changed. Probe accuracy on OOD datasets decreases once the tunnel is reached (denoted by ✗), where the model trained on low-resolution (\(32 32\)) images creates a longer tunnel (layers 9-16) than the one (layers 13-16) trained on higher-resolution (\(224 224\)) images. The Y-axis shows the normalized accuracy. The OOD curve is the average of 8 OOD datasets (Sec. 3.3), with the standard deviation denoted with shading.

Previous studies have focused on _independently_ analyzing variables that may impact OOD generalization [3; 4; 9; 22; 36; 37; 38; 39; 40; 32]. However, there is still a significant gap in our current understanding regarding each variable's _relative_ importance. Our study bridges this gap.

## 3 Methods

### Measuring the Tunnel Effect

Following , we use linear probes for our tunnel effect analysis. Linear probes are widely used to evaluate the transferability of learned embeddings to OOD datasets [9; 41; 42; 43; 44; 45; 46; 47; 48; 49; 50; 51; 52]. After supervised training of a DNN on an ID dataset, we train ID and OOD linear probes on embeddings produced by each layer. Embeddings for each layer are produced via global average pooling. Additional details are given in Appendix A.2. We use the linear probes to measure the strength of the tunnel effect.

In , authors showed that OOD accuracy decreased in the tunnel, whereas ID accuracy showed a monotonically increasing trend. However, they did not evaluate the strength of the tunnel and defined the start of the tunnel as when the ID probe reached either \(95\%\) or \(98\%\) of the final ID accuracy. Instead, we propose three metrics that enable us to measure the tunnel's strength, which are tied to OOD accuracy rather than ID accuracy. These metrics are computed for each OOD dataset. Our findings suggest that defining the start of the tunnel based on ID accuracy is ineffective. In Fig. 1, ID accuracy for \(32 32\) images reaches \(95\%\) of the final accuracy at layer 14, while OOD accuracy degrades starting at layer 9. We use normalized accuracy curves in the main text to compare models across OOD datasets and resolutions, where each linear probe curve is divided by the highest value.

**Metric 1: % OOD Performance Retained.** We measure the magnitude of the tunnel effect by evaluating the OOD performance through layer-wise linear probing. For a given OOD dataset, and a network with \(N\) layers, we find the layer that achieves the highest OOD linear probe accuracy, denoted as \(l_{m}\). The linear probe accuracy of \(l_{m}\) is denoted as \(a_{m}\). Then, we denote the OOD accuracy of the linear probe at the penultimate layer \(l_{N-1}\) as \(a_{p}\). We assume that \(l_{m}\) is the start of the tunnel if \(l_{m}\) < \(l_{N-1}\) and \(a_{m}\) > \(a_{p}\). For an OOD dataset, the % OOD performance retained, \(r\), w.r.t. the start of the tunnel is defined as \(r=100(a_{p}/a_{m})\). Higher \(r\) means better OOD generalization, hence a weaker tunnel and vice-versa. When \(a_{p}=a_{m}\), there is no tunnel.

**Metric 2: Pearson Correlation.** Linear probe accuracy of both ID and OOD datasets should display similar trends (higher correlation) in the extractor layers. However, they should have a low correlation for the tunnel. To quantify this, we use Pearson correlation (\(\)) between ID and OOD accuracy curves, where a higher correlation indicates less tunnel effect. Additional details are given in Appendix A.8.

**Metric 3: ID/OOD Alignment.** A strong model should have higher ID and OOD accuracy, whereas a weak model may show poor accuracy on ID or OOD or both. To capture the characteristic of a model in terms of how strongly it performs on both ID and OOD, we introduce a metric, called ID/OOD Alignment, denoted by \(\). To formalize, we denote ID and OOD accuracy by \(_{id}\) and \(_{ood}\), respectively. Chance accuracy (random guess) for ID and OOD datasets are denoted by \(c_{id}\) and \(c_{ood}\), respectively. Finally, we define the metric as \(=(_{id}-c_{id})(_{ood}-c_{ood})\), where \(\), \(_{id}\), \(c_{ood}\), \(c_{id}\), \(c_{ood}\) \(\). Higher \(\) means greater alignment between ID and OOD accuracy.

### Variables Investigated for Their Role in OOD Generalization

We study the role of image augmentation, training classes, training samples, image resolution, and DNN architecture on the tunnel effect, with the details for each given in the next paragraphs.

Augmentation.Prior work [22; 23] studied the impact of augmentation independently on OOD generalization whereas its impact on the tunnel effect has not been studied. To address this, we conducted _512 experiments_, where half of them were trained with augmentations and half without. These are done for every combination of the other variables we study. We used random resized crop and random horizontal flip augmentations.

Number of Classes.In , the tunnel effect was shown to decrease as the number of classes and training samples increased. We aim to disentangle these two variables. We conducted _48 experiments_ with ImageNet-100, where we kept the training set fixed at 10,000 samples but varied the class counts: 10 (1000 images per class), 50 (200 images per class), and 100 (100 images per class).

Number of Samples.We conducted _64 experiments_ using ImageNet-100 to assess the impact of the number of samples on the tunnel effect. We varied the number of training data per class from 100, 200, 500, and 700 while keeping the number of classes fixed at 100.

Resolution.Since  only studied \(32 32\) image datasets, the impact of image resolution on the tunnel effect is unknown. We hypothesized that higher resolutions would result in more hierarchical features, resulting in reducing the tunnel effect. To test this, we trained models on ImageNet-100 with \(32 32\), \(64 64\), \(128 128\), and \(224 224\) images, _while keeping the number of parameters for each architecture constant_. We conduct 48 experiments per resolution (_192 total_).

DNN Architecture Variables.We study the tunnel effect in eight DNN architectures drawn from three families: VGG , ResNet , and ViT . We study the role of the size of the \(k k\)_stem_, which is the size of the first CNN filter or the ViT's patch size. Because over-parameterization is central to the tunnel effect hypothesis, we measure the _over-parameterization level_, \(=P/N\), where \(P\) is the number of DNN parameters and \(N\) is the number of ID training samples. We conducted _416 experiments_ to assess the impact of architecture type, depth, over-parameterization level, stem style, and spatial reduction.

We ensure that each DNN architecture uses the _same_ number of parameters across image resolutions. To do this for the VGG family, we created VGGm, which replaces the two fully connected layers before the output layer with a ResNet-style global average pooling layer. Since the original VGG is designed for high-resolution images (\(224 224\)), it includes the max-pool in all 5 stages to progressively reduce the spatial dimension, \(s\) of the features (\(s s c\)) across VGG layers. To capture this, we introduce a variable named _spatial reduction_ by which a stem layer (first layer) reduces the spatial dimension of input images. Spatial reduction, \(\) is defined as the ratio of the output spatial dimension \(s_{out}\) to the input spatial dimension \(s_{in}\), i.e., \(=s_{out}/s_{in}\). For instance, spatial reduction at a layer that reduces the spatial dimension from \(32 32\) into \(16 16\) becomes \(0.5\), whereas a DNN that did no spatial reduction would have \(\) = 1. We also created another variant, VGGm\(\), to study the impact of spatial reduction on the tunnel effect. The difference between VGGm (\(\) = \(0.5\)) and VGGm\(\) (\(\) = 1) is that VGGm includes max-pool in all 5 stages whereas VGGm\(\) omits max-pool in the first \(2\) stages for \(32 32\) input resolution. Compared to VGGm, VGGm\(\) achieved higher ID accuracy on ImageNet-100 (see Table 11). For ResNet, we use the original ResNet architecture . To keep model size constant across resolutions for ViT models, we use a fixed patch size of \(8 8\), with the number of patches being larger for higher-resolution images. Following , we used 2D sin-cos position embeddings to encode spatial information.

### Datasets

ID Datasets.In our main experiments, we train DNNs on 3 ID datasets: 1) _ImageNet-100_--a subset (100 classes) of ImageNet-1K, 2) _CIFAR-10_, and 3) and _CIFAR-100_. For these experiments, 52 DNNs were trained on ImageNet-100, 8 on CIFAR-100, and 4 on CIFAR-10 (_64 DNNs total_), where resolution, augmentation, etc., were varied as described earlier. ID and OOD linear probes are trained and evaluated for each DNN layer. For our experiments on downloaded ImageNet-1K pre-trained DNNs, ID linear probes were trained on a training subset consisting of 50 images per class (50,000 images). Standard test sets are used for all ID datasets.

OOD Datasets.To assess OOD generalization with linear probes, we use 9 OOD datasets: _NINCO_, _ImageNet-R_, _CIFAR-100_, _CIFAR-10_, _Oxford 102 Flowers_, _CUB-200_, _Aircrafts_, _Oxford-IIIT Pets_, and _STL-10_ (see Appendix B). Eight OOD datasets are used with DNNs trained on each ID dataset, where CIFAR-10 is omitted for DNNs trained on ImageNet variants. When using CIFAR-10 or CIFAR-100 as the ID dataset, the other CIFAR dataset is used for OOD experiments since their classes do not overlap.

Resolution & ID Accuracy.All DNNs trained on CIFAR-10 or CIFAR-100 are trained and evaluated with \(32 32\) images. For DNNs trained on ImageNet variants, all ID and OOD images were resized to the resolution with which the DNN was trained. See Table 11 for the resolution used for each DNN and their ID accuracy with and without augmentations.

### Statistical Analysis

For each of the 64 DNNs trained on an ID dataset in our main results, we compute our OOD generalization metrics on each OOD dataset, resulting in 512 values per metric. These values are derived from 8,604 linear probes (ID and OOD). We study the impact of each variable on OOD generalization in isolation using paired Wilcoxon signed-rank tests at \(=0.05\), where pairs are constructed to control for the impact of other variables, and we use Cliff's Delta to measure effect sizes , which is appropriate for ratio data. For Cliff's Delta (\(\)), we follow the standard practice of defining a negligible effect for \(||<0.147\), small effect for \(0.147||<0.33\), medium effect for \(0.33||<0.474\), and large effect for \(|| 0.474\).

To jointly analyze and rank the contribution of each variable, we use SHAP +, which determines the contribution of each input variable to its output . Following , we train Gradient Boosting Regression models to predict _three output targets_: a) % OOD performance retained, b) Pearson correlation, and c) ID/OOD alignment, from _8 input variables_: 1) resolution, 2) augmentation, 3) ID class count, 4) spatial reduction, 5) stem, 6) CNN vs. ViT, 7) over-parametrization level, and 8) depth. We then obtain SHAP values for each variable, where using Gradient Boosting Regression facilitates controlling for variable interaction effects . Because SHAP magnitude does not indicate the direction of a variable's impact, for each of the 3 models, we fit a linear regression model between each variable and its corresponding SHAP values to obtain its slope. Positive slopes indicate the variable improves the metric. We call this _SHAP Slope_. Details are given in Appendix A.9.

Footnote †: https://github.com/shap/shap

## 4 Experiments & Results

### Main Experiments

In , all architectures were trained with CIFAR-10 or CIFAR-100, and all experiments were conducted with \(32 32\) images. Instead, most of our experiments use ImageNet-100 as the ID dataset, where image resolution is varied. We also include experiments on CIFAR datasets. In our main experiments, for each of our 64 DNNs, we produced 8 OOD linear probe curves and 1 ID linear probe curve (576 total), which required computing _8604 linear probes_. From this, we obtain 512 values for each of our 3 metrics under various conditions.

#### 4.1.1 Overall Findings & SHAP Analysis

Results for our SHAP Slope analysis computed across all 512 OOD experiments are summarized in Fig. 2, which shows the impact of each variable on the % OOD performance retained and ID/OOD alignment. The SHAP Slope figure for Pearson Correlation is given in Appendix C since it has nearly identical trends to % OOD performance retained. The \(R^{2}\) for % OOD performance retained, ID/OOD alignment predictions, and Pearson Correlation are 0.62, 0.44, and 0.73, respectively. Each variable's

Figure 2: **SHAP Results. SHAP slope shows the individual contribution of variables to various targets. Positive values indicate enhanced OOD generalization, and vice-versa for negative values.**positive/negative impact is consistent across all 3 of our SHAP analyses. Our main findings are given below, with additional findings in Appendix C.

Our metrics reveal that increasing the ID class count, higher resolutions, and using augmentations improve OOD generalization. For % OOD performance retained and Pearson Correlation, increasing the number of ID classes had the greatest impact, whereas, for ID/OOD alignment, increasing resolution did. This is likely because ID/OOD alignment curves are not normalized using the best OOD accuracy, resulting in resolution's role being obscured for the other two metrics. Across metrics, using augmentations had the second greatest positive impact. These results indicate that increasing between-class diversity (more classes), greater within-class diversity (augmentations), and higher image resolutions improve OOD generalization.

While  argued that the primary source of the tunnel effect is over-parameterization, our results indicate it plays a minor role compared to other factors. Our results indicate that using a larger stem and excessive DNN depth somewhat impair generalization. For all metrics, the choice of ViT or CNN had the least impact on OOD generalization, consistent with the hypothesis that much of the reported benefits of ViTs for image classification are due to training with larger datasets, stronger augmentation policies, and other tricks .

Using the average % OOD performance retained across the 8 OOD datasets to analyze all 64 of our DNNs, 4 had negligible (non-existent) tunnels, 8 had weak tunnels, 13 had medium tunnels, and 39 had strong tunnels. We use intervals of [100%, 95%] for negligible, [90%, 95%] for weak, [80%, 90%) for medium, and [0%, 80%) for strong tunnel. This demonstrates that the tunnel effect is not universal and depends on variables. Next, we dive into the factors that influence tunnel effect strength.

#### 4.1.2 Augmentation Results

In Fig. 3, example linear probe plots illustrate that augmentations play a major role in reducing the tunnel effect. To further analyze the impact of augmentation on OOD generalization, we compared all of our experiments in which augmentation was used or omitted with all other variables controlled using paired Wilcoxon Signed-Rank tests (256 paired experiments, 512 total). For % OOD performance retained, augmentations significantly decreased the tunnel effect with 64.26% retained without augmentations and 78.41% with (\(p\) < \(0.001\)), which had a _medium_ effect size (\(||=0.370\)). For Pearson correlation, augmentations also had a significant effect where \(\) increased from 0.77 to 0.86 (\(p\) < \(0.001\)), with a _medium_ effect size (\(||=0.374\)). For ID/OOD alignment, augmentations increased alignment from 0.15 to 0.25 (\(p\) < \(0.001\)), with a _medium_ effect size (\(||=0.357\)).

#### 4.1.3 Resolution Results

Illustrative examples showing how increasing resolution improves OOD generalization are given in Fig. 1. To study resolution further, we conducted paired tests between models trained with \(32\) x \(32\) images and those trained with \(64\) x \(64\), \(128\) x \(128\), or \(224\) x \(224\) images (48 paired experiments per resolution comparison, 192 total). All models were trained on ImageNet-100. Mean effect sizes (\(\)) and \(p\)-values are given in Table 1. Average values for the 3 metrics computed for each resolution are

Figure 3: **Augmentation greatly reduces the tunnel effect.** In (a), augmentation shifts the tunnel from layer 14 to 22, and in (b) from block 11 to 15. The OOD curve is the average of 8 OOD datasets with a shaded area indicating a 95% confidence interval. \(\) denotes the start of the tunnel.

given in Table 12. The findings are consistent with our SHAP analysis: training on higher-resolution images improves OOD generalization, whereas low-resolution datasets increase tunnel effect strength. Additional results are given in the Appendix C.13.

A t-SNE analysis for various layers from VGGm-11 models trained on \(32 32\) and \(224 224\) images is given in Fig. 4. The low-resolution model exhibits much greater intermediate neural collapse  and representation compression than the high-resolution model. This is likely why many OOD detection algorithms that work well for CIFAR fail for higher-resolution datasets . These results highlight the dangers of extrapolating findings from low-resolution datasets to all of deep learning.

#### 4.1.4 DNN Architecture Results

**Spatial Reduction.** Our SHAP analysis revealed that lower values for spatial reduction (\(\)) hurt OOD generalization. To further study this, we conducted paired tests between VGGm-11 and VGGm-17, which both have \(\) = \(0.5\), and VGGm\({}^{}\)-11 and VGGm\({}^{}\)-17 (\(\) = \(1.0\)). All 4 DNNs are trained on ImageNet-100 at \(32 32\) resolution, with and without augmentations, and each is evaluated on the 8 OOD sets (32 paired experiments, 64 total). In terms of % OOD performance retained, the VGGm\({}^{}\) models retained 84.40% whereas the VGGm models retained 64.85% (\(p\) < \(0.001\)), with a _large_ effect size (\(||\) = \(0.531\)). Similarly, Pearson correlation significantly decreased from \(0.92\) to \(0.72\) (\(p\) < \(0.001\)), with a _large_ effect size (\(||\) = \(0.536\)), and ID/OOD alignment significantly decreased from 0.26 to 0.18 (\(p\) < \(0.001\)), with a _medium_ effect size (\(||\) = \(0.361\)). Fig. 5 provides example

  
**Resolution** &  &  &  \\  & Eff. Size (\(||\)) \(\) & \(p-\)val\({}^{}\) & Eff. Size (\(||\)) \(\) & \(p-\)val \({}^{}\) & Eff. Size (\(||\)) \(\) & \(p-\)val \({}^{}\) \\  \(32^{2}\) vs \(64^{2}\) & \(negl.\) (\(0.002\)) & \(0.315\) & \(negl.\) (\(0.023\)) & \(0.572\) & \(small\) (\(0.326\)) & \(<0.001\) \\ \(32^{2}\) vs \(128^{2}\) & \(small\) (\(0.171\)) & \(0.001\) & \(small\) (\(0.240\)) & \(0.006\) & \(large\) (\(0.567\)) & \(<0.001\) \\ \(32^{2}\) vs \(224^{2}\) & \(small\) (\(0.198\)) & \(0.005\) & \(small\) (\(0.280\)) & \(0.011\) & \(large\) (\(0.625\)) & \(<0.001\) \\   

Table 1: **Higher resolution images reduce the tunnel effect.** Pairwise statistical analysis between DNNs trained on (\(32 32\)) images vs. higher resolution images.

Figure 4: **High-resolution model does not exhibit representation compression.** The t-SNE comparison between VGGm-11 models trained on low- (1st row) and high-resolution (2nd row) images of the same ID dataset (ImageNet-100) in an augmentation-free setting. Layer 8 marks the start of the tunnel in VGGm-11 trained on \(32 32\) images whereas \(224 224\) resolution does not create any tunnel. Layer 10 is the penultimate layer. The tunnel layers (layers 8-10) progressively compress representations for \(32 32\) resolution whereas corresponding layers for \(224 224\) resolution do not exhibit similar compression. For clarity, we show 5 classes from ImageNet-100 and indicate each class by a distinct color. The formation of distinct clusters in the \(32 32\) model is indicative of representation compression and intermediate neural collapse , which impairs OOD generalization.

normalized accuracy curves. VGGm-11 exhibits a strong tunnel spanning from layer 7 to 10 (Fig. 4(a)), whereas no tunnel is present in VGGm\(\)-11 (Fig. 4(b)).

**Stem.** Our SHAP results indicate that increasing stem size hurts OOD generalization. To further study this, we conducted a paired test over 64 paired experiments between ResNet-18, which uses a \(7 7\) stem, and VGGm-17, which uses a \(3 3\) stem. Increasing the stem from 3 to 7 significantly decreased the % OOD performance retained from 76.74% to 66.66% (\(p<0.001\)), with a _small_ effect size (\(||=0.306\)). However, for Pearson correlation, there was no significant difference (\(p=0.145\)). For ID/OOD alignment, increasing the stem significantly reduced the score from 0.27 to 0.21 (\(p<0.001\)), with a _small_ effect size (\(||=0.226\)). Fig. 12 provides box plots of % OOD performance retained and ID/OOD alignment for the three stem values.

**Depth.** Our SHAP analysis revealed that increasing depth impairs OOD generalization. As shown in Fig. 10, increasing depth impairs OOD performance retention for each architecture family. To study this further, we compared VGGm-11 and VGGm-17 using 48 paired experiments (96 total). Increasing depth significantly decreased % OOD performance retained from 89.19% to 69.41% (\(p<0.001\)), with a _large_ effect size (\(||=0.539\)). Likewise, Pearson correlation significantly decreased from 0.94 to 0.80 (\(p<0.001\)), with a _large_ effect size (\(||=0.497\)). ID/OOD alignment also significantly decreased from 0.28 to 0.25 (\(p<0.001\)) with a _small_ effect size (\(||=0.161\)).

**Over-parameterization Level.** Our SHAP analysis showed that over-parameterization level negatively impacts OOD generalization. Fig. 11 shows how increasing the over-parameterization level decreases % OOD performance retained. We conducted paired tests between VGGm-11 (\(=74.7\)) and ResNet-34 (\(=168.4\)) to study this further (32 paired experiments). Increasing over-parameterization significantly reduced % OOD performance retained from 87.22% to 62.78% (\(p<0.001\)), with a _large_ effect size (\(||=0.680\)). Likewise, the Pearson correlation was significantly reduced from 0.93 to 0.82 (\(p<0.001\)), with a _large_ effect size (\(||=0.570\)). Lastly, ID/OOD alignment significantly dropped from 0.29 to 0.20 (\(p<0.001\)), with a _medium_ effect size (\(||=0.340\)).

#### 4.1.5 ID Dataset Size vs. Total Classes

Our SHAP analysis shows that ID class count positively impacts OOD generalization. To further analyze this, we trained VGGm-11 on different subsets of ImageNet-100 with \(32 32\) images where we kept the training dataset fixed at 10,000 samples but varied the class counts: 10 (1000 samples per class), 50 (200 samples per class), and 100 (100 samples per class). Experiments were done with and without augmentations. As shown in Fig. 6, increasing the number of classes greatly reduces the tunnel effect (Figs. 5(a) and 5(b)). To examine the role of the dataset size, we vary the number of samples per class from 100, 200, 500, and 700 while keeping the class count constant at 100, which has a relatively small impact on the tunnel effect (Figs. 5(c) and 5(d)).

### Analysis of Widely Used Pre-trained Backbones

We also studied OOD generalization for eight widely used ImageNet-1K pre-trained CNN and ViT backbones trained with either supervised learning (SL) or self-supervised learning (SSL). We studied

Figure 5: **The tunnel effect is not universal. In (a), VGGm-11 consisting of max-pool in all 5 stages (\(=0.5\)), creates a tunnel (layers 7-10, gray-shaded area). In (b), the same VGGm-11 without max-pool in the first 2 stages (\(=1\), called VGGm\(\)-11), eliminates the tunnel for all OOD datasets.**

ResNet-50 (1 SL and 1 SSL ), 4 ViT-B (1 SL and 3 SSL [50; 74; 75]), and ConvNeXt-B (1 SL and 1 SSL ) models. We trained linear probes on ImageNet-1K (ID) and our 8 OOD datasets (_1980 linear probes total_), resulting in 72 values per OOD metric. As shown in Fig. 17 and Table 9, the tunnel effect is absent in most models and is weakly present in both SL and SSL ResNet-50. Additional results are given in Appendix C.11. Appendix A.6 includes implementation details.

### Continual Learning Results

Catastrophic forgetting is a central focus in continual learning [17; 77]. Many methods assume forgetting occurs mostly in later layers with earlier layers serving as universal features [16; 78; 79]. The original tunnel effect paper challenged this . They trained VGG-19 on two tasks sequentially, where the first task included half of the CIFAR-10 classes and the second task included the other half. After training on each task, the tunnel and extractor were saved. They found no forgetting when tunnels were swapped, indicating no learning occurred in the tunnel for task 2, and they found reduced forgetting when fine-tuning the extractor alone. These findings are worth assessing beyond CIFAR-10. We ask: _What is the role of the tunnel in mitigating forgetting?_

We replicated their general approach by training ResNet-18 on ImageNet-100, where the first task had the first 50 classes and the second had the rest. After learning each task \(t\), we saved the extractor \(E_{t}\), tunnel \(T_{t}\), and the classification head. We conducted this experiment with \(32 32\) and \(224 224\) images. Using the tunnel definition from , tunnels \(T_{1}\) and \(T_{2}\) correspond to layers 14-17 for \(32 32\) images and 15-17 for \(224 224\) images. See Appendix A.7 for additional details.

Results are given in Table 2. Unlike the findings in , when we swapped tunnels \(T_{1}\) and \(T_{2}\), accuracy was greatly reduced for both resolutions, indicating that essential learning happened within the tunnel. Next, we replicated their fine-tuning experiments. If fine-tuning \(E_{2}\) alone reduces forgetting more than fine-tuning \(E_{2}\) with \(T_{1}\), it suggests the tunnel has a detrimental impact on mitigating forgetting. We found that fine-tuning \(E_{2}\) alone improved task 1 accuracy less than fine-tuning \(E_{2}\) + \(T_{1}\), indicating \(T_{1}\) helps reduce forgetting. Our findings, which contradict , suggest the "tunnel" plays an essential role in mitigating catastrophic forgetting, consistent with prior work .

Figure 6: **Training on more classes greatly reduces the tunnel effect, whereas increasing dataset size has less impact.****(a)** and **(b)** Results with a fixed number of samples but a varied number of classes. **(c)** and **(d)** Results with a fixed number of classes but a varied number of samples per class.

## 5 Discussion & Conclusions

We conducted extensive experiments to investigate the generality of the tunnel effect in a wide range of transfer settings. Our study indicates that the best way to mitigate the tunnel effect, and thereby increase OOD generalization, is to increase diversity in the ID training dataset, especially by increasing the number of semantic classes, using augmentations, and higher-resolution images; hence, we revise the tunnel effect hypothesis as follows:

**The Tunnel Effect Hypothesis**: An overparameterized \(N\)-layer DNN forms two distinct groups:

1. _The extractor_ consists of the first \(K\) layers, creating linearly separable representations.
2. _The tunnel_ comprises the remaining \(N-K\) layers, compressing representations and hindering OOD generalization.

\(K\) is proportional to the diversity of training inputs, where if diversity is sufficiently high, \(N=K\).

Earlier works on the tunnel effect  and intermediate neural collapse  exclusively used \(32 32\) images for ID training. We found that while DNNs trained on CIFAR always exhibited the tunnel effect, the tunnel effect was greatly reduced for ImageNet-100 at higher resolutions. This discrepancy helps explain why methods validated on CIFAR and similar datasets may not generalize in many scenarios . We urge the community to use high-resolution datasets with 100 or more categories to improve the generality of findings, especially for studies related to representation learning, neural collapse, and OOD detection/generalization.

**Limitations & Future Work.** While our work validates the existence of tunnels, future research should focus on developing theoretical frameworks to help explain the tunnel effect. We rigorously studied OOD generalization on vision datasets with supervised learning. Future work could study non-vision, multi-modal , and biased datasets , where the tunnel effect has not yet been studied. While we studied pre-trained SSL backbones, we could not do our SHAP analysis without having more SSL backbones. Valuable insights for SSL could be obtained by conducting carefully controlled paired experiments, as done in our main experiments. This would require probing at least four different SSL algorithms for each variable analyzed, where training each SSL DNN would require over 10\(\) more time than our supervised DNNs. Additionally, SSL methods employ more advanced augmentation policies than ours, where we used random-resized crops and horizontal flips. Replicating our SHAP analysis with multiple augmentation policies could reveal whether the OOD generalization capabilities of SSL algorithms are due to their augmentation policies versus their objective functions. Lastly, identifying regularizers or other techniques that mitigate tunnel formation should be sought for continual learning methods that start from scratch using small initial sets. This could greatly improve forward transfer, leading to more efficient continual learning methods .