# Computational Complexity of Learning Neural Networks: Smoothness and Degeneracy

Amit Daniely

Hebrew University and Google

amit.daniely@mail.huji.ac.il

&Nathan Srebro

TTI-Chicago

nati@ttic.edu

&Gal Vardi

TTI-Chicago and Hebrew University

galvardi@ttic.edu

###### Abstract

Understanding when neural networks can be learned efficiently is a fundamental question in learning theory. Existing hardness results suggest that assumptions on both the input distribution and the network's weights are necessary for obtaining efficient algorithms. Moreover, it was previously shown that depth-\(2\) networks can be efficiently learned under the assumptions that the input distribution is Gaussian, and the weight matrix is non-degenerate. In this work, we study whether such assumptions may suffice for learning deeper networks and prove negative results. We show that learning depth-\(3\) ReLU networks under the Gaussian input distribution is hard even in the smoothed-analysis framework, where a random noise is added to the network's parameters. It implies that learning depth-\(3\) ReLU networks under the Gaussian distribution is hard even if the weight matrices are non-degenerate. Moreover, we consider depth-\(2\) networks, and show hardness of learning in the smoothed-analysis framework, where both the network parameters and the input distribution are smoothed. Our hardness results are under a well-studied assumption on the existence of local pseudorandom generators.

## 1 Introduction

The computational complexity of learning neural networks has been extensively studied in recent years, and there has been much effort in obtaining both upper bounds and hardness results. Nevertheless, it is still unclear when neural networks can be learned in polynomial time, namely, under what assumptions provably efficient algorithms exist.

Existing results imply hardness already for learning depth-\(2\) ReLU networks in the standard PAC learning framework (e.g., [31; 14]). Thus, without any assumptions on the input distribution or the network's weights, efficient learning algorithms might not be achievable. Even when assuming that the input distribution is Gaussian, strong hardness results were obtained for depth-\(3\) ReLU networks [16; 11], suggesting that assumptions merely on the input distribution might not suffice. Also, a hardness result by Daniely and Vardi  shows that strong assumptions merely on the network's weights (without restricting the input distribution) might not suffice even for efficiently learning depth-\(2\) networks. The aforementioned hardness results hold already for _improper learning_, namely, where the learning algorithm is allowed to return a hypothesis that does not belong to the considered hypothesis class. Thus, a combination of assumptions on the input distribution and the network's weights seems to be necessary for obtaining efficient algorithms.

Several polynomial-time algorithms for learning depth-\(2\) neural networks have been obtained, under assumptions on the input distribution and the network's weights [6; 28; 43; 20; 7; 21]. In these works, it is assumed that the weight matrices are non-degenerate. That is, they either assume that the condition number of the weight matrix is bounded, or some similar non-degeneracy assumption. Specifically, Awasthi et al.  gave an efficient algorithm for learning depth-\(2\) (one-hidden-layer) ReLU networks, that may include bias terms in the hidden neurons, under the assumption that the input distribution is Gaussian, and the weight matrix is non-degenerate. The non-degeneracy assumption holds w.h.p. if we add a small random noise to any weight matrix, and hence their result implies efficient learning of depth-\(2\) ReLU networks under the Gaussian distribution in the _smoothed-analysis_ framework.

The positive results on depth-\(2\) networks suggest the following question:

_Is there an efficient algorithm for learning ReLU networks of depth larger than \(2\) under the Gaussian distribution, where the weight matrices are non-degenerate, or in the smoothed-analysis framework where the network's parameters are smoothed?_

In this work, we give a negative answer to this question, already for depth-\(3\) networks1. We show that learning depth-\(3\) ReLU networks under the Gaussian distribution is hard even in the smoothed-analysis framework, where a random noise is added to the network's parameters. As a corollary, we show that learning depth-\(3\) ReLU networks under the Gaussian distribution is hard even if the weight matrices are non-degenerate. Our hardness results are under a well-studied cryptographic assumption on the existence of _local pseudorandom generators (PRG)_ with polynomial stretch.

Motivated by the existing positive results on smoothed-analysis in depth-\(2\) networks, we also study whether learning depth-\(2\) networks with smoothed parameters can be done under weak assumptions on the input distribution. Specifically, we consider the following question:

_Is there an efficient algorithm for learning depth-\(2\) ReLU networks in the smoothed-analysis framework, where both the network's parameters and the input distribution are smoothed?_

We give a negative answer to this question, by showing hardness of learning depth-\(2\) ReLU networks where a random noise is added to the network's parameters, and the input distribution on \(^{d}\) is obtained by smoothing an i.i.d. Bernoulli distribution on \(\{0,1\}^{d}\). This hardness result is also under the assumption on the existence of local PRGs.

Related workHardness of learning neural networksHardness of improperly learning depth-\(2\) neural networks follows from hardness of improperly learning DNFs or intersections of halfspaces, since these classes can be expressed by depth-\(2\) networks. Klivans and Sherstov  showed, assuming the hardness of the shortest vector problem, that learning intersections of halfspaces is hard. Hardness of learning DNF formulas is implied by Applebaum et al.  under a combination of two assumptions: the first is related to the planted dense subgraph problem in hypergraphs, and the second is related to local PRGs. Daniely and Shalev-Shwartz  showed hardness of learning DNFs under a common assumption, namely, that refuting a random \(K\)-SAT formula is hard. All of the above results are distribution-free, namely, they do not imply hardness of learning neural networks under some specific distribution.

Applebaum and Raykov  showed, under an assumption on a specific candidate for Goldreich's PRG (i.e., based on a predicate called \(\)-\(\)), that learning depth-\(3\) Boolean circuits under the uniform distribution on the hypercube is hard. Daniely and Vardi  proved distribution-specific hardness of learning Boolean circuits of depth-\(2\) (namely, DNFs) and depth-\(3\), under the assumpion on the existence of local PRGs that we also use in this work. For DNF formulas, they showed hardness of learning under a distribution where each component is drawn i.i.d. from a Bernoulli distribution (which is non-uniform). For depth-\(3\) Boolean circuits, they showed hardness of learning under the uniform distribution on the hypercube. Since the Boolean circuits can be expressed by ReLU networks of the same depth, these results readily translate to distribution-specific hardness of learning neural networks. Chen et al.  showed hardness of learning depth-\(2\) neural networks under the uniform distribution on the hypercube, based on an assumption on the hardness of the _Learning with Rounding (LWR)_ problem. Note that the input distributions in the above results are supported on the hypercube, and they do not immediately imply hardness of learning neural networks under continuous distributions.

When considering the computational complexity of learning neural networks, perhaps the most natural choice of an input distribution is the standard Gaussian distribution. Daniely and Vardi  established hardness of learning depth-\(3\) networks under this distribution, based on the assumption on the existence of local PRGs. Chen et al.  also showed hardness of learning depth-\(3\) networks under the Gaussian distribution, but their result holds already for networks that do not have an activation function in the output neuron, and it is based on the LWR assumption. They also showed hardness of learning constant depth ReLU networks from label queries (i.e., where the learner has the ability to query the value of the target network at any desired input) under the Gaussian distribution, based either on the decisional Diffie-Hellman or the Learning with Errors assumptions.

The above results suggest that assumptions on the input distribution might not suffice for achieving an efficient algorithm for learning depth-\(3\) neural networks. A natural question is whether assumptions on the network weights may suffice. Daniely and Vardi  showed (under the assumption that refuting a random \(K\)-SAT formula is hard) that distribution-free learning of depth-\(2\) neural networks is hard already if the weights are drawn from some "natural" distribution or satisfy some "natural" properties. Thus, if we do not impose any assumptions on the input distribution, then even very strong assumptions on the network's weights do not suffice for efficient learning.

Several works in recent years have shown hardness of distribution-specific learning shallow neural networks using gradient-methods or statistical query (SQ) algorithms . It is worth noting that while the SQ framework captures some variants of the gradient-descent algorithm, it does not capture, for example, stochastic gradient-descent (SGD), which examines training points individually (see a discussion in ).

We emphasize that none of the above distribution-specific hardness results for neural networks (either for improper learning or SQ learning) holds in the smoothed analysis framework or for non-degenerate weights.

Learning neural networks in polynomial time.Awasthi et al.  gave a polynomial-time algorithm for learning depth-\(2\) (one-hidden-layer) ReLU networks, under the assumption that the input distribution is Gaussian, and the weight matrix of the target network is non-degenerate. Their algorithm is based on tensor decomposition, and it can handle bias terms in the hidden layer. Their result also implies that depth-\(2\) ReLU networks with Gaussian inputs can be learned efficiently under the smoothed-analysis framework. Our work implies that such a result might not be possible in depth-\(3\) networks (with activation in the output neuron). Prior to , several works gave polynomial time algorithms for learning depth-\(2\) neural networks where the input distribution is Gaussian and the weight matrix is non-degenerate , but these works either do not handle the presence of bias terms or do not handle the ReLU activation. Some of the aforementioned works consider networks with multiple outputs, and allow certain non-Gaussian input distributions. Provable guarantees for learning neural networks in super-polynomial time were given in .

## 2 Preliminaries

Notations.We use bold-face letters to denote vectors, e.g., \(=(x_{1},,x_{d})\). For a vector \(\) and a sequence \(S=(i_{1},,i_{k})\) of \(k\) indices, we let \(_{S}=(x_{i_{1}},,x_{i_{k}})\), i.e., the restriction of \(\) to the indices \(S\). We denote by \([]\) the indicator function, for example \([t 5]\) equals \(1\) if \(t 5\) and \(0\) otherwise. For an integer \(d 1\) we denote \([d]=\{1,,d\}\). We denote by \((0,^{2})\) the normal distribution with mean \(0\) and variance \(^{2}\), and by \((,)\) the multivariate normal distribution with mean \(\) and covariance matrix \(\). The identity matrix of size \(d\) is denoted by \(I_{d}\). For \(^{d}\) we denote by \(\|\|\) the Euclidean norm. We use \((a_{1},,a_{n})\) to denote a polynomial in \(a_{1},,a_{n}\).

Local pseudorandom generators.An \((n,m,k)\)-hypergraph is a hypergraph over \(n\) vertices \([n]\) with \(m\) hyperedges \(S_{1},,S_{m}\), each of cardinality \(k\). Each hyperedge \(S=(i_{1},,i_{k})\) is ordered, and all the \(k\) members of a hyperedge are distinct. We let \(_{n,m,k}\) be the distribution over such hypergraphs in which a hypergraph is chosen by picking each hyperedge uniformly and independently at random among all the possible \(n(n-1)(n-k+1)\) ordered hyperedges. Let\(\{0,1\}\) be a predicate, and let \(G\) be a \((n,m,k)\)-hypergraph. We call _Goldreich's pseudorandom generator (PRG)_ the function \(f_{P,G}:\{0,1\}^{n}\{0,1\}^{m}\) such that for \(\{0,1\}^{n}\), we have \(f_{P,G}()=(P(_{S_{1}}),,P(_{S_{m}}))\). The integer \(k\) is called the _locality_ of the PRG. If \(k\) is a constant then the PRG and the predicate \(P\) are called _local_. We say that the PRG has _polynomial stretch_ if \(m=n^{s}\) for some constant \(s>1\). We let \(_{P,n,m}\) denote the collection of functions \(f_{P,G}\) where \(G\) is an \((n,m,k)\)-hypergraph. We sample a function from \(_{P,n,m}\) by choosing a random hypergraph \(G\) from \(_{n,m,k}\).

We denote by \(G}{{}}_{n,m,k}\) the operation of sampling a hypergraph \(G\) from \(_{n,m,k}\), and by \(}{{}}\{0,1\}^{n}\) the operation of sampling \(\) from the uniform distribution on \(\{0,1\}^{n}\). We say that \(_{P,n,m}\) is \(\)-pseudorandom generator (\(\)-PRG) if for every polynomial-time probabilistic algorithm \(\) the _distinguishing advantage_

\[|_{G}{{}}_{n,m,k}, }{{}}\{0,1\}^{n}}[( G,f_{P,G}())=1]-_{G}{{}}_{n,m,k},}{{}}\{0,1\}^{m}}[(G,)=1]|\]

is at most \(\). Thus, the distinguisher \(\) is given a random hypergraph \(G\) and a string \(\{0,1\}^{m}\), and its goal is to distinguish between the case where \(\) is chosen at random, and the case where \(\) is a random image of \(f_{P,G}\).

Our assumption is that local PRGs with polynomial stretch and constant distinguishing advantage exist:

**Assumption 2.1**.: _For every constant \(s>1\), there exists a constant \(k\) and a predicate \(P:\{0,1\}^{k}\{0,1\}\), such that \(_{P,n,n^{s}}\) is \(\)-PRG._

We remark that the same assumption was used by Daniely and Vardi  to show hardness-of-learning results. Local PRGs have been extensively studied in the last two decades. In particular, local PRGs with polynomial stretch have shown to have remarkable applications, such as secure-computation with constant computational overhead , and general-purpose obfuscation based on constant degree multilinear maps (cf. ). Significant evidence for Assumption 2.1 was shown in Applebaum . Moreover, a concrete candidate for a local PRG, based on the XOR-MAJ predicate, was shown to be secure against all known attacks . See  for further discussion on the assumption, and on prior work regarding the relation between Goldreich's PRG and hardness of learning.

Neural networks.We consider feedforward ReLU networks. Starting from an input \(^{d}\), each layer in the network is of the form \((W_{i}+_{i})\), where \((a)=[a]_{+}=\{0,a\}\) is the ReLU activation which applies to vectors entry-wise, \(W_{i}\) is the weight matrix, and \(_{i}\) are the bias terms. The _weights vector_ of the \(j\)-th neuron in the \(i\)-th layer is the \(j\)-th row of \(W_{i}\), and its _outgoing-weights vector_ is the \(j\)-th column of \(W_{i+1}\). We define the _depth_ of the network as the number of layers. Unless stated otherwise, the output neuron also has a ReLU activation function. Note that a depth-\(k\) network with activation in the output neuron has \(k\) non-linear layers. A neuron which is not an input or output neuron is called a _hidden neuron_. We sometimes consider neural networks with multiple outputs. The parameters of the neural network is the set of its weight matrices and bias vectors. We often view the parameters as a vector \(^{p}\) obtained by concatenating these matrices and vectors. For \(B 0\), we say that the parameters are \(B\)-bounded if the absolute values of all weights and biases are at most \(B\).

Learning neural networks and the smoothed-analysis framework.We first define neural networks learning under the standard PAC framework:

**Definition 2.1** (Distribution-specific PAC learning).: _Learning depth-\(k\) neural networks under an input distribution \(\) on \(^{d}\) is defined by the following framework:_

1. _An adversary chooses a set of_ \(B\)_-bounded parameters_ \(^{p}\) _for a depth-_\(k\) _neural network_ \(N_{}:^{d}\)_, as well as some_ \(>0\)_._
2. _Consider an examples oracle, such that each example_ \((,y)^{d}\) _is drawn i.i.d. with_ \(\) _and_ \(y=N_{}()\)_. Then, given access to the examples oracle, the goal of the learning algorithm_ \(\) _is to return with probability at least_ \(\) _a hypothesis_ \(h:^{d}\) _such that_ \(_{}[(h()-N_{ }())^{2}]\)_. We say that_ \(\) _is_ efficient _if runs in time \((d,p,B,1/)\)We consider learning in the _smoothed-analysis_ framework , which is a popular paradigm for analyzing non-worst-case computational complexity . The smoothed-analysis framework has been successfully applied to many learning problems (e.g., ). In the smoothed-analysis setting, the target network is not purely controlled by an adversary. Instead, the adversary can first generate an arbitrary network, and the parameters for this network (i.e., the weight matrices and bias terms) will be randomly perturbed to yield a perturbed network. The algorithm only needs to work with high probability on the perturbed network. This limits the power of the adversary and prevents it from creating highly degenerate cases. Formally, we consider the following framework (we note that a similar model was considered in ):

**Definition 2.2** (Learning with smoothed parameters).: _Learning depth-\(k\) neural networks with smoothed parameters under an input distribution \(\) is defined as follows:_

1. _An adversary chooses a set of_ \(B\)_-bounded parameters_ \(^{p}\) _for a depth-_\(k\) _neural network_ \(N_{}:^{d}\)_, as well as some_ \(,>0\)_._
2. _A perturbed set of parameters_ \(}\) _is obtained by a random perturbation to_ \(\)_, namely,_ \(}=+\) _for_ \((,^{2}I_{p})\)_._
3. _Consider an examples oracle, such that each example_ \((,y)^{d}\) _is drawn i.i.d. with_ \(\) _and_ \(y=N_{}}()\)_. Then, given access to the examples oracle, the goal of the learning algorithm_ \(\) _is to return with probability at least_ \(\) _(over the random perturbation_ \(\) _and the internal randomness of_ \(\)_) a hypothesis_ \(h:^{d}\) _such that_ \(_{}[(h()-N_{}}())^{2}]\)_._

_We say that \(\) is efficient if it runs in time \((d,p,B,1/,1/)\)._

Finally, we also consider a setting where both the parameters and the input distribution are smoothed:

**Definition 2.3** (Learning with smoothed parameters and inputs).: _Learning depth-\(k\) neural networks with smoothed parameters and inputs under an input distribution \(\) is defined as follows:_

1. _An adversary chooses a set of_ \(B\)_-bounded parameters_ \(^{p}\) _for a depth-_\(k\) _neural network_ \(N_{}:^{d}\)_, as well as some_ \(,,>0\)_._
2. _A perturbed set of parameters_ \(}\) _is obtained by a random perturbation to_ \(\)_, namely,_ \(}=+\) _for_ \((,^{2}I_{p})\)_. Moreover, a smoothed input distribution_ \(}\) _is obtained from_ \(\)_, such that_ \(}}\) _is chosen by drawing_ \(\) _and adding a random perturbation from_ \((,^{2}I_{d})\)_._
3. _Consider an examples oracle, such that each example_ \((,y)^{d}\) _is drawn i.i.d. with_ \(}\) _and_ \(y=N_{}}()\)_. Then, given access to the examples oracle, the goal of the learning algorithm_ \(\) _is to return with probability at least_ \(\) _(over the random perturbation_ \(\) _and the internal randomness of_ \(\)_) a hypothesis_ \(h:^{d}\) _such that_ \(_{}}[(h()-N_{}}())^{2}]\)_._ _We say that_ \(\) _is efficient if it runs in time_ \((d,p,B,1/,1/,1/)\)_._

We emphasize that all of the above definitions consider learning in the distribution-specific setting. Thus, the learning algorithm may depend on the specific input distribution \(\).

## 3 Results

As we discussed in the introduction, there exist efficient algorithms for learning depth-\(2\) (one-hidden-layer) ReLU networks with smoothed parameters under the Gaussian distribution. We now show that such a result may not be achieved for depth-\(3\) networks:

**Theorem 3.1**.: _Under Assumption 2.1, there is no efficient algorithm that learns depth-\(3\) networks with smoothed parameters (in the sense of Definition 2.2) under the standard Gaussian distribution._

We prove the theorem in Section 4. Next, we conclude that learning depth-\(3\) neural networks under the Gaussian distribution on \(^{d}\) is hard in the standard PAC framework even if all weight matrices are non-degenerate, namely, when the minimal singular values of the weight matrices are lower bounded by \(1/\,(d)\). As we discussed in the introduction, in one-hidden-layer networks with similar assumptions there exist efficient learning algorithms.

**Corollary 3.1**.: _Under Assumption 2.1, there is no efficient algorithm that learns depth-\(3\) networks (in the sense of Definition 2.1) under the standard Gaussian distribution on \(^{d}\), even if the smallest singular value of each weight matrix is at least \(1/(d)\)._

The proof of the above corollary follows from Theorem 3.1, using the fact that by adding a small random perturbation to the weight matrices, we get w.h.p. non-degenerate matrices. Hence, an efficient algorithm that learns non-degenerate networks suffices for obtaining an efficient algorithm that learns under the smoothed analysis framework. See Appendix B for the formal proof.

The above hardness results consider depth-\(3\) networks that include activation in the output neuron. Thus, the networks have three non-linear layers. We remark that these results readily imply hardness also for depth-\(4\) networks without activation in the output, and hardness for depth-\(k\) networks for any \(k>3\). We also note that the hardness results hold already for networks where all hidden layers are of the same width, e.g., where all layers are of width \(d\).

Theorem 3.1 gives a strong limitation on learning depth-\(3\) networks in the smoothed-analysis framework. Motivated by existing positive results on smoothed-analysis in depth-\(2\) networks, we now study whether learning depth-\(2\) networks with smoothed parameters can be done under weak assumptions on the input distribution. Specifically, we consider the case where both the parameters and the input distribution are smoothed. We show that efficiently learning depth-\(2\) networks may not be possible with smoothed parameters where the input distribution on \(^{d}\) is obtained by smoothing an i.i.d. Bernoulli distribution on \(\{0,1\}^{d}\).

**Theorem 3.2**.: _Under Assumption 2.1, there is no efficient algorithm that learns depth-\(2\) networks with smoothed parameters and inputs (in the sense of Definition 2.3), under the distribution \(\) on \(\{0,1\}^{d}\) where each coordinate is drawn i.i.d. from a Bernoulli distribution which takes the value \(0\) with probability \(}\)._

We prove the theorem in Appendix C. The proof follows from similar ideas to the proof of Theorem 3.1, which we discuss in the next section.

## 4 Proof of Theorem 3.1

The proof builds on a technique from Daniely and Vardi . It follows by showing that an efficient algorithm for learning depth-\(3\) neural networks with smoothed parameters under the Gaussian distribution can be used for breaking a local PRG. Intuitively, the main challenge in our proof in comparison to  is that our reduction must handle the random noise which is added to the parameters. Specifically,  define a certain examples oracle, and show that the examples returned by the oracle are realizable by some neural network which depends on the unknown \(\{0,1\}^{n}\) used by the PRG. Since the network depends on this unknown \(\), some of the parameters of the network are unknown, and hence it is non-trivial how to define an examples oracle which is realizable by a perturbed network. Moreover, we need to handle this random perturbation without increasing the network's depth. For these reasons, our reduction is significantly different from .

We now provide the formal proof. The proof relies on several lemmas which we prove in Appendix A. For a sufficiently large \(n\), let \(\) be the standard Gaussian distribution on \(^{n^{2}}\). Assume that there is a \((n)\)-time algorithm \(\) that learns depth-\(3\) neural networks with at most \(n^{2}\) hidden neurons and parameter magnitudes bounded by \(n^{3}\), with smoothed parameters, under the distribution \(\), with \(=\), and \(=1/(n)\) that we will specify later. Let \(m(n)(n)\) be the sample complexity of \(\), namely, \(\) uses a sample of size at most \(m(n)\) and returns w.p. at least \(\) a hypothesis \(h\) with \(_{}[(h()-N_{}}())^{2}]=\), where \(N_{}}\) is the perturbed network. Let \(s>1\) be a constant such that \(n^{s} m(n)+n^{3}\) for every sufficiently large \(n\). By Assumption 2.1, there exists a constant \(k\) and a predicate \(P:\{0,1\}^{k}\{0,1\}\), such that \(_{P,n,n^{s}}\) is \(\)-PRG. We will show an efficient algorithm \(\) with distinguishing advantage greater than \(\) and thus reach a contradiction.

Throughout this proof, we will use the following notations. For a hyperedge \(S=(i_{1},,i_{k})\) we denote by \(^{S}\{0,1\}^{kn}\) the following encoding of \(S\): the vector \(^{S}\) is a concatenation of \(k\) vectors in \(\{0,1\}^{n}\), such that the \(j\)-th vector has \(0\) in the \(i_{j}\)-th coordinate and \(1\) elsewhere. Thus, \(^{S}\) consists of \(k\) size-\(n\) slices, each encoding a member of \(S\). For \(\{0,1\}^{kn}\), \(i[k]\) and \(j[n]\), we denote \(z_{i,j}=z_{(i-1)n+j}\). That is, \(z_{i,j}\) is the \(j\)-th component in the \(i\)-th slice in \(\). For \(\{0,1\}^{n}\), let\(P_{}:\{0,1\}^{kn}\{0,1\}\) be such that for every hyperedge \(S\) we have \(P_{}(^{S})=P(_{S})\). Let \(c\) be such that \(_{t(0,1)}[t c]=\). Let \(\) be the density of \((0,1)\), let \(_{-}(t)=n[t c](t)\), and let \(_{+}=[t c](t)\). Note that \(_{-},_{+}\) are the densities of the restriction of \(\) to the intervals \(t c\) and \(t c\) respectively. Let \(:^{kn}\{0,1\}^{kn}\) be a mapping such that for every \(^{}^{kn}\) and \(i[kn]\) we have \((^{})_{i}=[z^{}_{i} c]\). For \(}^{n^{2}}\) we denote \(}_{[kn]}=(_{1},,_{kn})\), namely, the first \(kn\) components of \(}\) (assuming \(n^{2} kn\)).

### Defining the target network for \(\)

Since our goal is to use the algorithm \(\) for breaking PRGs, in this subsection we define a neural network \(:^{n^{2}}\) that we will later use as a target network for \(\). The network \(\) contains the subnetworks \(N_{1},N_{2},N_{3}\) which we define below.

Let \(N_{1}\) be a depth-\(2\) neural network with input dimension \(kn\), at most \(n(n)\) hidden neurons, at most \((n)\) output neurons (with activations in the output neurons), and parameter magnitudes bounded by \(n^{3}\) (all bounds are for a sufficiently large \(n\)), which satisfies the following. We denote the set of output neurons of \(N_{1}\) by \(_{1}\). Let \(^{}^{kn}\) be an input to \(N_{1}\) such that \((^{})=^{S}\) for some hyperedge \(S\), and assume that for every \(i[kn]\) we have \(z^{}_{i}(c,c+})\). Fix some \(\{0,1\}^{n}\). Then, for \(S\) with \(P_{}(^{S})=0\) the inputs to all output neurons \(_{1}\) are at most \(-1\), and for \(S\) with \(P_{}(^{S})=1\) there exists a neuron in \(_{1}\) with input at least \(2\). Recall that our definition of a neuron's input includes the addition of the bias term. The construction of the network \(N_{1}\) is given in Lemma A.3. Intuitively, the network \(N_{1}\) consists of a layer that transforms w.h.p. the input \(^{}\) to \((^{})=^{S}\), followed by a layer that satisfies the following: Building on a lemma from  which shows that \(P_{}(^{S})\) can be computed by a DNF formula, we define a layer where each output neuron corresponds to a term in the DNF formula, such that if the term evaluates to 0 then the input to the neuron is at most \(-1\), and otherwise it is at least \(2\). Note that the network \(N_{1}\) depends on \(\). However, only the second layer depends on \(\), and thus given an input we may compute the first layer even without knowing \(\). Let \(N^{}_{1}:^{kn}\) be a depth-\(3\) network with no activation in the output neuron, obtained from \(N_{1}\) by summing the outputs from all neurons \(_{1}\).

Let \(N_{2}\) be a depth-\(2\) neural network with input dimension \(kn\), at most \(n(n)\) hidden neurons, at most \(2n\) output neurons, and parameter magnitudes bounded by \(n^{3}\) (for a sufficiently large \(n\)), which satisfies the following. We denote the set of output neurons of \(N_{2}\) by \(_{2}\). Let \(^{}^{kn}\) be an input to \(N_{2}\) such that for every \(i[kn]\) we have \(z^{}_{i}(c,c+})\). If \((^{})\) is an encoding of a hyperedge then the inputs to all output neurons \(_{2}\) are at most \(-1\), and otherwise there exists a neuron in \(_{2}\) with input at least \(2\). The construction of the network \(N_{2}\) is given in Lemma A.5. Intuitively, each neuron in \(_{2}\) is responsible for checking whether \((^{})\) violates some requirement that must hold in an encoding of a hyperedge. Let \(N^{}_{2}:^{kn}\) be a depth-\(3\) network with no activation in the output neuron, obtained from \(N_{2}\) by summing the outputs from all neurons \(_{2}\).

Let \(N_{3}\) be a depth-\(2\) neural network with input dimension \(kn\), at most \(n(n)\) hidden neurons, \(kn n(n)\) output neurons, and parameter magnitudes bounded by \(n^{3}\) (for a sufficiently large \(n\)), which satisfies the following. We denote the set of output neurons of \(N_{3}\) by \(_{3}\). Let \(^{}^{kn}\) be an input to \(N_{3}\). If there exists \(i[kn]\) such that \(z^{}_{i}(c,c+})\) then there exists a neuron in \(_{3}\) with input at least \(2\). Moreover, if for all \(i[kn]\) we have \(z^{}_{i}(c-},c+})\) then the inputs to all neurons in \(_{3}\) are at most \(-1\). The construction of the network \(N_{3}\) is straightforward and given in Lemma A.6. Let \(N^{}_{3}:^{kn}\) be a depth-\(3\) neural network with no activation function in the output neuron, obtained from \(N_{3}\) by summing the outputs from all neurons \(_{3}\).

Let \(N^{}:^{kn}\) be a depth-\(3\) network obtained from \(N^{}_{1},N^{}_{2},N^{}_{3}\) as follows. For \(^{}^{kn}\) we have \(N^{}(^{})=[1-N^{}_{1}(^{})-N^ {}_{2}(^{})-N^{}_{3}(^{})]_{+}\). The network \(N^{}\) has at most \(n^{2}\) neurons, and parameter magnitudes bounded by \(n^{3}\) (all bounds are for a sufficiently large \(n\)). Finally, let \(:^{n^{2}}\) be a depth-\(3\) neural network such that \((})=N^{}(}_{[kn]})\).

### Defining the noise magnitude \(\) and analyzing the perturbed network

In order to use the algorithm \(\) w.r.t. some neural network with parameters \(\), we need to implement an examples oracle, such that the examples are labeled according to a neural network with parameters \(+\), where \(\) is a random perturbation. Specifically, we use \(\) with an examples oracle where the labels correspond to a network \(:^{n^{2}}\), obtained from \(\) (w.r.t. an appropriate \(\{0,1\}^{n}\) in the construction of \(N_{1}\)) by adding a small perturbation to the parameters. The perturbation is such that we add i.i.d. noise to each parameter in \(\), where the noise is distributed according to \((0,^{2})\), and \(=1/(n)\) is small enough such that the following holds. Let \(f_{}:^{n^{2}}\) be any depth-\(3\) neural network parameterized by \(^{r}\) for some \(r>0\) with at most \(n^{2}\) neurons, and parameter magnitudes bounded by \(n^{3}\) (note that \(r\) is polynomial in \(n\)). Then w.p. at least \(1-\) over \((,^{2}I_{r})\), we have \(|_{i}|\) for all \(i[r]\), and the network \(f_{+}\) is such that for every input \(}^{n^{2}}\) with \(\|}\| 2n\) and every neuron we have: Let \(a,b\) be the inputs to the neuron in the computations \(f_{}(})\) and \(f_{+}(})\) (respectively), then \(|a-b|\). Thus, \(\) is sufficiently small, such that w.h.p. adding i.i.d. noise \((0,^{2})\) to each parameter does not change the inputs to the neurons by more than \(\). Note that such an inverse-polynomial \(\) exists, since when the network size, parameter magnitudes, and input size are bounded by some \((n)\), then the input to each neuron in \(f_{}(})\) is \((n)\)-Lipschitz as a function of \(\), and thus it suffices to choose \(\) that implies w.p. at least \(1-\) that \(\|\|\) for a sufficiently large polynomial \(q(n)\) (see Lemma A.7 for details).

Let \(}^{p}\) be the parameters of the network \(\). Recall that the parameters vector \(}\) is the concatenation of all weight matrices and bias terms. Let \(}^{p}\) be the parameters of \(\), namely, \(}=}+\) where \((,^{2}I_{p})\). By our choice of \(\) and the construction of the networks \(N_{1},N_{2},N_{3}\), w.p. at least \(1-\) over \(\), for every \(}\) with \(\|}\| 2n\), the inputs to the neurons \(_{1},_{2},_{3}\) in the computation \((})\) satisfy the following properties, where we denote \(^{}=}_{[kn]}\):

1. If \((^{})=^{S}\) for some hyperedge \(S\), and for every \(i[kn]\) we have \(z^{}_{i}(c,c+})\), then the inputs to \(_{1}\) satisfy: If \(P_{}(^{S})=0\) then the inputs to all neurons in \(_{1}\) are at most \(-\), and if \(P_{}(^{S})=1\) then there exists a neuron in \(_{1}\) with input at least \(\).
2. If for every \(i[kn]\) we have \(z^{}_{i}(c,c+})\), then the inputs to \(_{2}\) satisfy: If \((^{})\) is an encoding of a hyperedge then the inputs to all neurons \(_{2}\) are at most \(-\), and otherwise there exists a neuron in \(_{2}\) with input at least \(\).
3. The inputs to \(_{3}\) satisfy: If there exists \(i[kn]\) such that \(z^{}_{i}(c,c+})\) then there exists a neuron in \(_{3}\) with input at least \(\), and if for all \(i[kn]\) we have \(z^{}_{i}(c-},c+})\) then the inputs to all neurons in \(_{3}\) are at most \(-\).

### Stating the algorithm \(\)

Given a sequence \((S_{1},y_{1}),,(S_{n^{*}},y_{n^{*}})\), where \(S_{1},,S_{n^{*}}\) are i.i.d. random hyperedges, the algorithm \(\) needs to distinguish whether \(=(y_{1},,y_{n^{*}})\) is random or that we have \(=(P(_{S_{1}}),,P(_{S_{n^{*}}}))=(P_{ }(^{S_{1}}),,P_{}(^{S_{n^{*}}}))\) for a random \(\{0,1\}^{n}\). We let \(=((^{S_{1}},y_{1}),,(^{S_{n^{*}}},y_{n^{* }}))\).

We use the efficient algorithm \(\) in order to obtain distinguishing advantage greater than \(\) as follows. Let \(\) be a random perturbation, and let \(\) be the perturbed network as defined above, w.r.t. the unknown \(\{0,1\}^{n}\). Note that given a perturbation \(\), only the weights in the second layer of the subnetwork \(N_{1}\) in \(\) are unknown, since all other parameters do not depend on \(\). The algorithm \(\) runs \(\) with the following examples oracle. In the \(i\)-th call, the oracle first draws \(\{0,1\}^{kn}\) such that each component is drawn i.i.d. from a Bernoulli distribution which takes the value \(0\) w.p. \(\). If \(\) is an encoding of a hyperedge then the oracle replaces \(\) with \(^{S_{i}}\). Then, the oracle chooses \(^{}^{kn}\) such that for each component \(j\), if \(z_{j}=1\) then \(z^{}_{j}\) is drawn from \(_{+}\), and otherwise \(z^{}_{j}\) is drawn from \(_{-}\). Let \(}^{n^{2}}\) be such that \(}_{[kn]}=^{}\), and the other \(n^{2}-kn\) components of \(}\) are drawn i.i.d. from \((0,1)\). Note that the vector \(}\) has the distribution \(\), due to the definitions of the densities \(_{+}\) and \(_{-}\), and since replacing an encoding of a random hyperedge by an encoding of another random hyperedge does not change the distribution of \(\). Let \(\) be the bias term of the output neuron of \(\). The oracle returns \((},)\), where the labels \(\) are chosen as follows:

* If \((^{})\) is not an encoding of a hyperedge, then \(=0\).
* If \((^{})\) is an encoding of a hyperedge:* If \(^{}\) does not have components in the interval \((c-},c+})\), then if \(y_{i}=0\) we set \(=\), and if \(y_{i}=1\) we set \(=0\).
* If \(^{}\) has a component in the interval \((c,c+})\), then \(=0\).
* If \(^{}\) does not have components in the interval \((c,c+})\), but has a component in the interval \((c-},c+})\), then the label \(\) is determined as follows: If \(y_{i}=1\) then \(=0\). If \(y_{i}=0\), we have: Let \(N_{3}\) be the network \(\) after omitting the neurons \(_{1},_{2}\) and their incoming and outgoing weights. Then, we set \(=[-_{3}(})]_{+}\). Note that since only the second layer of \(N_{1}\) depends on \(\), then we can compute \(_{3}(})\) without knowing \(\).

Let \(h\) be the hypothesis returned by \(\). Recall that \(\) uses at most \(m(n)\) examples, and hence \(\) contains at least \(n^{3}\) examples that \(\) cannot view. We denote the indices of these examples by \(I=\{m(n)+1,,m(n)+n^{3}\}\), and the examples by \(_{I}=\{(^{_{i}},y_{i})\}_{i I}\). By \(n^{3}\) additional calls to the oracle, the algorithm \(\) obtains the examples \(_{I}}=\{(}_{i},_{i})\}_{i I}\) that correspond to \(_{I}\). Let \(h^{}\) be a hypothesis such that for all \(}^{n^{2}}\) we have \(h^{}(})=\{0,\{,h(})\}\}\), thus, for \( 0\) the hypothesis \(h^{}\) is obtained from \(h\) by clipping the output to the interval \([0,]\). Let \(_{I}(h^{})=_{i I}(h^{}(} _{i})-_{i})^{2}\). Now, if \(_{I}(h^{})\), then \(\) returns \(1\), and otherwise it returns \(0\). We remark that the decision of our algorithm is based on \(h^{}\) (rather than \(h\)) since we need the outputs to be bounded, in order to allow using Hoeffding's inequality in our analysis, which we discuss in the next subsection.

### Analyzing the algorithm \(\)

Note that the algorithm \(\) runs in \((n)\) time. We now show that if \(\) is pseudorandom then \(\) returns \(1\) w.p. greater than \(\), and if \(\) is random then \(\) returns \(1\) w.p. less than \(\).

We start with the case where \(\) is pseudorandom. In Lemma A.8, we prove that if \(\) is pseudorandom then w.h.p. (over \((,^{2}I_{p})\) and the i.i.d. inputs \(}_{i}\)) the examples \((}_{1},_{1}),,\)\((}_{m(n)+n^{3}},_{m(n)+n^{3}})\) returned by the oracle are realized by \(\). Thus, \(_{i}=(}_{i})\) for all \(i\). As we show in the lemma, this claim follows by considering the definition of the oracle for the different cases of \((}_{i})_{[kn]}\), and using Properties (P1)-(P3) to show that \(\) behaves similarly.

Recall that the algorithm \(\) is such that w.p. at least \(\) (over \((,^{2}I_{p})\), the i.i.d. inputs \(}_{i}\), and its internal randomness), given a size-\(m(n)\) dataset labeled by \(\), it returns a hypothesis \(h\) such that \(_{}}[(h(})- (}))^{2}]\). By the definition of \(h^{}\) and the construction of \(\), if \(h\) has small error then \(h^{}\) also has small error, namely, we have \(_{}}[(h^{}(})-(}))^{2}]\). In Lemma A.9 we use the above arguments and Hoeffding's inequality over \(_{I}}\), and prove that w.p. greater than \(\) we have \(_{I}(h^{})\).

Next, we consider the case where \(\) is random. Let \(}^{n^{2}}\) be such that \(}}\) iff \(}_{[kn]}\) does not have components in the interval \((c-},c+})\), and \((}_{[kn]})=^{S}\) for a hyperedge \(S\). If \(\) is random, then by the definition of our examples oracle, for every \(i[m(n)+n^{3}]\) such that \(}_{i}}\), we have \(_{i}=\) w.p. \(\) and \(_{i}=0\) otherwise. Also, by the definition of the oracle, \(_{i}\) is independent of \(S_{i}\) and independent of the choice of the vector \(}_{i}\) that corresponds to \(^{S_{i}}\). Hence, for such \(}_{i}}\) with \(i I\), any hypothesis cannot predict the label \(_{i}\), and the expected loss for the example is at least \(()^{2}\). Moreover, in Lemma A.11 we show that \([}_{i}}]\) for a sufficiently large \(n\). In Lemma A.12 we use these arguments to prove a lower bound on \(_{}_{I}}[_{I}(h^{})]\), and by Hoeffding's inequality over \(_{I}}\) we conclude that w.p. greater than \(\) we have \(_{I}(h^{})>\).

Overall, if \(\) is pseudorandom then w.p. greater than \(\) the algorithm \(\) returns \(1\), and if \(\) is random then w.p. greater than \(\) it returns \(0\). Thus, the distinguishing advantage is greater than \(\).

Discussion

Understanding the computational complexity of learning neural networks is a central question in learning theory. Our results imply that the assumptions which allow for efficient learning in one-hidden-layer networks might not suffice in deeper networks. Also, in depth-\(2\) networks we show that it is not sufficient to assume that both the parameters and the inputs are smoothed. We hope that our hardness results will help focus on assumptions that may allow for efficient learning.

We emphasize that our hardness results are for neural networks that include the ReLU activation also in the output neuron. In contrast, the positive results on learning depth-\(2\) networks that we discussed in the introduction do not include activation in the output neuron. Therefore, as far as we are aware, there is still a small gap between the upper bounds and our hardness results: (1) Under the assumption that the input is Gaussian and the weights are non-degenerate, the cases of depth-\(2\) networks with activation in the output neuron and of depth-\(3\) networks without activation in the output are not settled; (2) In the setting where both the parameters and the input distribution are smoothed, the case of depth-\(2\) networks without activation in the output is not settled. These gaps are an intriguing subject for further research.