# Parameterizing Non-Parametric Meta-Reinforcement Learning Tasks via Subtask Decomposition

Suyoung Lee, Myungsik Cho, Youngchul Sung

School of Electrical Engineering

KAIST

Daejeon 34141, Republic of Korea

{suyoung.1, ms.cho, ycsung}@kaist.ac.kr

Corresponding author

###### Abstract

Meta-reinforcement learning (meta-RL) techniques have demonstrated remarkable success in generalizing deep reinforcement learning across a range of tasks. Nevertheless, these methods often struggle to generalize beyond tasks with parametric variations. To overcome this challenge, we propose Subtask Decomposition and Virtual Training (SDVT), a novel meta-RL approach that decomposes each non-parametric task into a collection of elementary subtasks and parameterizes the task based on its decomposition. We employ a Gaussian mixture VAE to meta-learn the decomposition process, enabling the agent to reuse policies acquired from common subtasks. Additionally, we propose a virtual training procedure, specifically designed for non-parametric task variability, which generates hypothetical subtask compositions, thereby enhancing generalization to previously unseen subtask compositions. Our method significantly improves performance on the Meta-World ML-10 and ML-45 benchmarks, surpassing current state-of-the-art techniques.

## 1 Introduction

Meta-reinforcement learning (meta-RL) constitutes a dynamic field within deep reinforcement learning, focusing on training agents to quickly adapt to novel tasks by learning from a variety of training tasks . By interacting with these tasks, meta-RL creates an inductive bias regarding the task dynamics and subsequently develops a policy based on this knowledge. Despite its significant contribution to the generalization capability of traditional deep RL, meta-RL is susceptible to test-time distribution shifts, which restricts its applicability to familiar in-distribution test tasks .

To tackle this limitation, recent out-of-distribution (OOD) meta-RL approaches have emphasized distinct training and test task distributions, thereby achieving enhanced performance on unseen OOD test tasks with interpolated or slightly extrapolated training dynamics . Although the parameters of training and test tasks are drawn from disjoint distributions, these tasks remain qualitatively similar, as they can be expressed in a shared parametric form representing the task dynamics (e.g., the same "Pick-place" task with OOD goal positions in Figure 0(b)).

In this study, we explore a more general meta-RL framework that addresses non-parametric task variability , a topic that has received limited attention in prior research. In this context as of Figure 0(c), variations among tasks cannot be expressed through simple parametric variations, such as the parameterization of a goal position. Generalization is particularly challenging in this setting, as conventional meta-RL methods often model the inductive bias as a parametric embedding applicable to various tasks . Within a non-parametric framework, it may not be feasible to employ a unified and generalizable parameterization of training tasks using standard meta-RL techniques.

Moreover, even if an agent successfully models the inductive bias parametrically, it is improbable that the same parameterization will be reusable for qualitatively distinct test tasks.

In addressing the challenges of non-parametric task variability in meta-RL, our primary strategy involves _decomposing each non-parametric task into a set of shared elementary subtasks_. We then parameterize each task based on the types of subtasks that constitute it. Despite the non-parametric task variability, tasks may share elementary subtasks. For instance in Figure 0(c), a "Pick-place" task can be decomposed into subtasks: "grip object" and "place object," while a "Sweep-into" task can be decomposed into subtasks: "grip object" and "push object." By employing the shared subtask parameterization, the policy can capitalize on the captured commonalities between non-parametric tasks to enhance training efficiency and generalization capabilities.

However, our approach of task parameterization based on a set of subtasks faces two primary challenges: the lack of prior information about **(1)** the set of elementary subtasks and **(2)** the decomposition of each task. To address these issues, we employ meta-learning for the subtask decomposition (SD) process using a Gaussian mixture variational autoencoder (GMVAE) [29; 8; 59]. Our GMVAE encodes the trajectory up to the current timestep into latent categorical and Gaussian contexts, which are trained to reconstruct the task's reward and transition dynamics . We discover that the meta-learned latent categorical context effectively represents the subtask compositions of tasks under non-parametric variations. Consequently, the policy, using the learned subtask composition, can readily generalize to new tasks comprising previously encountered subtasks. To further enhance generalization to unseen compositions of familiar subtasks, we propose a virtual training (VT) process [35; 1] specifically designed for non-parametric task variability. We train the policy on imaginary tasks generated by the learned dynamics decoder, conditioned on hypothetical subtask compositions.

We evaluate our method on the Meta-World ML-10 and ML-45 benchmarks , widely used meta-RL benchmarks comprising diverse non-parametric robotic manipulation tasks. We empirically demonstrate that our method successfully meta-learns the shareable subtask decomposition. With the help of the subtask decomposition and virtual training, our method, without any offline demonstration or test-time gradient updates, achieves test success rates of 33.4% on ML-10 and 31.2% on ML-45, which improves the previous state-of-the-art by approximately 1.7 times and 1.3 times, respectively.

## 2 Background

### Meta-Reinforcement Learning

A Markov decision process (MDP), \(M=(,,R,T,T_{0},,H)\), is defined by a tuple comprising a set of states \(\), a set of actions \(\), a reward function \(R(r_{t+1}|s_{t},a_{t},s_{t+1})\), a transition function \(T(s_{t+1}|s_{t},a_{t})\), an initial state distribution \(T_{0}(s_{0})\), a discount factor \(\), and a horizon \(H\).

The goal of meta-RL is to learn to adapt to a distribution of MDPs with varying reward and transition dynamics. At the start of each meta-training iteration, an MDP is sampled from the distribution \(p(_{})\) over a set of MDPs \(_{}\). Each MDP \(M_{k}=(,,R_{k},T_{k},T_{0,k},,H)\) is defined with a unique reward function \(R_{k}\), transition function \(T_{k}\), and initial state distribution \(T_{0,k}\). Unlike in a multi-task setup, the agent in the meta-RL setup does not have access to the task index \(k\) that determines the MDP dynamics. The training objective is to optimize the policy \(_{}\) with

Figure 1: **Problem Setup. Visualizing different meta-RL scenarios with Meta-World tasks [69; 71]. The circles and crosses represent the object and goal positions, respectively. Solid blue objects indicate training tasks, while empty brown objects indicate test tasks.**

parameters \(\) to maximize the expected return across all MDPs: \(_{}_{M_{k} p(_{})}[_ {}()]\), where \(_{}()=_{T_{0,k},T_{k},_{}}[_{ t=0}^{H-1}^{t}R_{k}(r_{t+1}|s_{t},a_{t},s_{t+1})].\) During meta-testing, standard distribution meta-RL methods are evaluated on tasks sampled from the same distribution \(p(_{})\) as the training tasks, i.e., \(_{}=_{}=\) (Figure 0(a)). In contrast, OOD meta-RL methods assume strictly disjoint training and test task sets, i.e., \(=_{}_{}\) and \(_{}_{}=\) (Figure 0(b)).

### Bayes-adaptive Meta-Reinforcement Learning

Since the true task index \(k\) is not provided to the agent in the meta-RL problem setup, it is important to balance exploration and exploitation while learning about the initially unknown MDP. A Bayes-adaptive agent [38; 10; 16] achieves this balance by updating its belief \(b_{t}(R,T)\) about the MDP based on its experience \(_{t}=\{s_{0},a_{0},r_{1},s_{1},a_{1},r_{2},,s_{t-1},a_{t-1},r_{t},s _{t}\}\). The agent's belief over the MDP dynamics at time \(t\) can be represented as a posterior given the trajectory, i.e., \(b_{t}(R,T)=p(R,T|_{t})\). By augmenting the state with the belief to form a hyper-state space \(^{+}=\), where \(\) is the set of belief, a Bayes-adaptive MDP (BAMDP) can be constructed. The objective of a BAMDP is to maximize the expected return within a meta-episode while learning, where a meta-episode consists of \(n_{}\) rollout episodes (i.e., \(H^{+}=n_{} H\) steps) of the same MDP:

\[_{}^{+}()=_{b_{0},T^{+},_{}}[ _{t=0}^{H^{+}-1}^{t}R^{+}(r_{t+1}|s_{t}^{+},a_{t},s_{t+1}^{+}) ],\] (1)

where \(T^{+}(s_{t+1}^{+}|s_{t}^{+},a_{t},r_{t})=_{b_{t}}[T(s_{t+1},s_ {t},a_{t})](b_{t+1}=p(R,T|_{t+1}))\) is the transition dynamics and \(R^{+}(r_{t+1}|s_{t}^{+},a_{t},s_{t+1}^{+})=_{b_{t+1}}[R(r_{t+1} |s_{t},a_{t},s_{t+1})]\) is the reward dynamics of the BAMDP. The posterior belief update in the indicator function \(()\) is intractable for all but simple environments.

VariBAD  solves the inference and posterior update of the belief by combining meta-RL and approximate variational inference . At each timestep \(t\), a recurrent encoder \(q_{_{h}}\) encodes the experience \(_{ t}\) into a hidden embedding \(h_{t}=q_{_{h}}(_{ t})\). The approximate posterior belief over the dynamics can be represented as the parameters of a multivariate Gaussian distribution: \(b_{t}=(_{_{}}(h_{t}),_{_{h}}(h_{t}))\), where \(_{_{}}()\) and \(_{_{h}}()\) are neural networks. The latent context \(z_{t}(_{_{}}(h_{t}),_{_{h}}^{2}(h_{t}))\) is used to estimate the MDP dynamics: \(p_{_{R}}(r_{j+1}|s_{j},a_{j},s_{j+1},z_{t})\) and \(p_{_{T}}(s_{j+1}|s_{j},a_{j},z_{t})\) for \(j=0,,H^{+}-1\), _including the past and future_. Then the problem of computing the posterior over the dynamics \(p(R,T|_{ t})\) reduces to inferring the posterior \(q_{}(z_{t}|_{ t})\), where \(=\{_{h},_{z}\}\). A separate policy network \(_{}(a_{t}|s_{t},b_{t})\) is trained to optimize the BAMDP objective in Eq. (1).

### Non-parametric Task Variability

The term "non-parametric" in the context of task variability is introduced in the Meta-World paper . It is used to distinguish the task variability in Meta-World manipulation tasks from more simplistic parametric variations exhibited in standard MuJoCo tasks, such as variations in target goal positions, directions, and velocities. However, the term "non-parametric" might lead to misunderstanding. Because it could suggest that there are no parameters available for task parameterization, even though we have effectively parameterized them in our approach. Our major breakthrough is in enabling this parameterization in terms of subtask compositions, which was difficult using earlier methods that relied on simple latent parametrization. Hence, while we retain the "non-parametric" terminology, we guide the readers to view the scope of our work through the lens of modularity or composition-based generalization .

## 3 Method

In this section, we introduce our novel meta-RL method, named Subtask Decomposition and Virtual Training (SDVT), to handle non-parametric task variability. Our approach is based on decomposing each task into a set of elementary subtasks and parameterizing each task based on the composition of these subtasks. To achieve this, we use a Gaussian mixture variational autoencoder (GMVAE) to meta-learn the subtask decomposition process. In addition, we introduce a virtual training process that improves generalization to tasks with unseen compositions of subtasks.

### Subtask Decomposition (SD) with a Gaussian Mixture Variational Autoencoder

Our goal is to meta-learn a set of elementary subtasks and to meta-learn the decomposition of each task into a composition of these subtasks. The core of our method focuses on meta-learning the approximate subtask composition \(y_{t}^{K}\) sampled from a \(K\)-class categorical distribution, where \(^{K}\) denotes the \(K\)-dimensional probability simplex. For example with \(K=3\), we want the subtask composition \(y_{t}\) to be learned somewhat like \((0.5,0.5,0.0)\) for "Pick-place" and \((0.0,0.5,0.5)\) for "Sweep-into," where each dimension of \(y_{t}\) represents the weight corresponding to subtasks in the order of "place object," "grip object," and "push object." To capture such subtask information, we use a Gaussian mixture variational autoencoder (GMVAE) to represent the latent space of non-parametric tasks as a Gaussian mixture distribution. Each task is represented as a \(K\)-dimensional mixture proportion \(y_{t}\), resulting in each class representing a unique subtask shared across different tasks. Thus, the distribution of \(K\) subtasks is modeled using a categorical distribution with \(K\) classes, where each class is associated with a Gaussian distribution. The learned subtask composition \(y_{t}\) represents the agent's belief at time \(t\) about the current task's decomposition into subtasks. It is crucial to note that we don't necessarily want \(y_{t}\) to be a one-hot embedding representing the subtask that the agent is solving at time \(t\). We want \(y_{t}\) to represent a mixing proportion  over all possible subtasks that the agent believes to be relevant to the current task, including past and future as in Figures 1(b) and 4. This distinction is vital to the model's effectiveness in solving a range of tasks, as it allows for flexibility in subtask identification and generalization across different tasks.

ArchitectureOur full model in Figure 1(a), which is based on the VAE of VariBAD , consists of three main components: the encoder, decoder, and policy networks parameterized by \(=\{_{h},_{y},_{z}\}\), \(=\{_{z},_{R},_{T}\}\), and \(\), respectively.

**(1)** The encoder is defined as \(q_{}(y_{t},z_{t}|h_{t})=q_{_{y}}(y_{t}|h_{t})q_{_{z}}(z_{t}|h_{t},y _{t})\). A recurrent network encodes the past trajectory \(_{:t}\) into a hidden embedding \(h_{t}=q_{_{h}}(_{:t})\). First, the categorical encoder \(q_{_{y}}(y_{t}|h_{t}):(_{_{y}}(h_{t}))\) samples \(y_{t}\), where \(_{_{y}}(h_{t})^{K}\). We use the Gumbel-Softmax trick  with a high temperature (\(=1\)) when sampling \(y_{t}\) to form a soft label. Then the multivariate Gaussian encoder \(q_{_{z}}(z_{t}|h_{t},y_{t}):(_{_{z}}(h_{t},y_{t}), _{_{z}}^{2}(h_{t},y_{t}))\) samples a continuous latent context \(z_{t}\), which contains the parametric information of the subtasks, in addition to the categorical subtask information of \(y_{t}\).

**(2)** The decoder is defined as \(p_{}(_{:H^{+}},y_{t},z_{t})=p(y_{t})p_{_{z}}(z_{t}|y_{t})p_{ _{R},_{T}}(_{:H^{+}}|z_{t})\). It reconstructs the reward and transition dynamics for all transitions in the meta-episode \(_{:H^{+}}\), using the latent context \(z_{t}\) as in Eq. (4). We assume a uniform prior of subtask composition \(p(y_{t}):(1/K)\) and a Gaussian regularization \(p_{_{z}}(z_{t}|y_{t}):(_{_{z}}(y_{t}),_{ _{z}}^{2}(y_{t}))\). This encoder-decoder architecture

Figure 2: **SDVT architecture.****(a)** Our proposed architecture incorporates three main components: the encoder, decoder, and policy. An online trajectory is encoded into categorical (\(y\)) and Gaussian (\(z\)) latent contexts. These contexts, which are trained to reconstruct the forward dynamics, are utilized by the policy network. This structure is also applied to the virtual training, as illustrated in Figure 2(a), with an optional dispersion layer integrated. **(b)** An example of the learned task inference process within a meta-episode (\(H^{+}=5000\) steps) on “Pick-place” is shown. We report the values for each dimension of contexts: \(_{_{y}}(h_{t})\), \(_{_{z}}(h_{t},y_{t})\), and \(_{_{z}}(h_{t},y_{t})\).

allows both the approximate posterior \(q_{}(y_{t},z_{t}|h_{t})\) and the prior \(p(z_{t})\) to follow Gaussian mixture distributions.

**(3)** The policy network, \(_{}(a_{t}|s_{t},b_{t})\), is trained separately conditioned on the belief \(b_{t}=(_{_{s}}(h_{t},y_{t}),_{_{s}}(h_{t},y_{t}))\) that are parameters of the Gaussian context. In practice, we also provide the parameters of the categorical encoder \(_{_{y}}(h_{t})\) to the policy, which we find to improve the performance. The parameters of the distributions (\(_{_{y}},_{_{z}},_{_{z}},_{_{z}},\) and \(_{_{z}}\)) are modeled as outputs of multilayer perceptrons (MLPs) as in Appendix C.4.

ObjectiveWe optimize the GMVAE to maximize the evidence lower bound (ELBO) for all time steps \(t=0,,H^{+}\) over the trajectory distribution \(d(M_{k},_{:H^{+}})\) induced by the policy in MDP \(M_{k}\):

\[_{t}(,) =_{d(M_{k},_{:H^{+}})}[_{q_{}(y_{ t},z_{t}|h_{t})}_{}],\] (2) \[_{} =_{}_{}+_{T} _{}+_{g}_{}+_{c} _{}.\] (3)

In addition to the reconstruction objectives \(_{}\) and \(_{}\), we have additional regularization \(_{}\) and categorical \(_{}\) objectives:

\[_{}=_{j=0}^{H^{+}-1} p_{_{ R}}(r_{j+1}|s_{j},a_{j},s_{j+1},z_{t}), _{}=_{j=0}^{H^{+}-1} p_{_{T}}(s_{j+1 }|s_{j},a_{j},z_{t}),\] (4) \[_{}=}(z_{t}|y_{t})}{ q_{_{z}}(z_{t}|h_{t},y_{t})},_{}=)}{ q_{_{y}}(y_{t}|h_{t})}.\] (5)

The regularization objective \(_{}\) minimizes the KL divergence between the learned posterior Gaussian distribution \(q_{_{z}}(z_{t}|h_{t},y_{t})\) and learned Gaussian priors \(p_{_{z}}(z_{t}|y_{t})\). Unlike the standard VAE that assumes a standard normal prior, we learn \(K\) distinct Gaussian priors conditioned on \(y_{t}\). The categorical objective \(_{}\) maximizes the conditional entropy of \(y_{t}\) given \(h_{t}\) and prevents collapse. Refer to Appendix B for the derivation of the ELBO objective. The reconstruction objectives in Eq. (4) are computed for all timesteps from the first to the terminal step of the meta-episode. Therefore, the subtask composition \(y_{t}\) at time \(t\) is not necessarily a one-hot label of the belief about the current subtask at time \(t\), but a mixture label of the belief on all subtasks that compose the current task in the past and future within the meta-episode. Under the BAMDP setup, the agent learns to reduce the uncertainty of the decomposition as quickly as possible, supporting the policy with the converged parameterization. Combining the policy objective in Eq. (1) and the sum of the ELBO objectives in Eq. (2) for all timesteps in a meta-episode, the overall objective over training tasks is to maximize:

\[(,,)=_{M_{k} p(_{})}[_{}^{+}()+_{t=0}^{H^{+}}_{t}( ,)].\] (6)

Occupancy regularizationThe dimension of \(y_{t}\) or the number of underlying subtasks \(K\) is a crucial hyperparameter that should be determined based on the number of training tasks \(N_{}\) and their similarities. However, in many cases, prior information about the optimal value of \(K\), denoted as \(K^{*}\), may not be available. One way to expand the scope of our method for unknown \(K^{*}\) is to meta-learn the number of effective subtasks as well. First, we assume \(K^{*}<N_{}\), otherwise each task will be classified into a separate subtask with one-hot label, preventing learning shareable subtasks. We start with a sufficiently large \(K=N_{}\) and regularize the ELBO objective to progressively reduce the number of effective subtasks (non-zero components) occupied in \(y_{t}\) with the following occupancy regularization that penalizes the usage of larger indices in the subtask composition:

\[_{}=- K(e^{-K+1},e^{-K+2},,e^{-1},e^{0} ) y_{t}.\] (7)

We calculate the dot product of exponential weights and the subtask composition \(y_{t}\) to penalize the occupancy of higher dimensions of \(y_{t}\). We scale the dot product by \( K\) to match the scale to the upper bound of \(_{}\). We add \(_{}\) multiplied by a coefficient \(_{o}\) to the GMVAE objective in Eq. (3). Consequently, the agent prioritizes using lower indices in the decomposition to represent frequent subtasks and sparingly uses higher indices for rare subtasks as in Figure 3(b) and in Appendix E.1.

Decoder dropoutAs the GMVAE is optimized using the trajectories induced by the policy, the decoder can easily overfit the frequent states and actions of training tasks . This can lead to low predicted rewards for unexperienced states and actions, regardless of the latent context \(z_{t}\). Whensuch overfitting happens, the latent context loses its task-informative value, leading to the potential underperformance of policy based on this context. Following the approach of LDM , we address this by applying dropout (DO) to the state and action embeddings of the decoder while leaving latent context \(z_{t}\) untouched: \(p_{_{R}}(r_{j+1}|[s_{j},a_{j},s_{j+1}],z_{t})\) and \(p_{_{T}}(s_{j+1}|[s_{j},a_{j}],z_{t})\). This dropout application is crucial for the virtual training discussed in the following section.

### Virtual Training (VT) on Generated Tasks with Imaginary Subtask Compositions

The overall objective in Eq. (6) is optimized over the trajectories of training tasks. To enhance generalization to test tasks with unseen subtask compositions, we generate virtual training tasks using the imaginary dynamics produced by the GMVAE decoder. This process resembles those in [35; 1], but their generated tasks are limited to parametric variations, e.g., generating tasks with unseen goal positions. Such methods can prepare for test tasks with unseen parametric changes but struggle to prepare for qualitatively new tasks with unseen compositions of subtasks. Our GMVAE model enables us to extend the process to the non-parametric setup by conditioning the decoder on an imaginary subtask composition \(\). At the beginning of each meta-episode, we randomly determine with probability \(p_{v}\) whether to convert it into a virtual meta-episode. By training the policy on imaginary tasks, it can better prepare for test tasks with unseen subtask compositions in advance. We use the tilde accent to denote imaginary components, and the hat accent to denote estimates.

Latent context dispersionInspired by the work of Ajay et al. , we utilize the dispersion structure for our GMVAE to support extrapolated generalization of the latent context \(z_{t}\). Instead of directly using \(z_{t}\) as the decoder input, we insert an additional MLP \(p_{_{}}\) before the decoder to expand the dimension of the context (the dotted box in Figure 2a). The MLP output \(_{t}=p_{_{}}(z_{t})\) is trained to reconstruct the embedding \(h_{t}\) by appending \(_{d}_{}=-_{d}\|h_{t}-_{t}\|_{2}^{2}\) to the total GMVAE objective in Eq. (3). We then use the dispersed context \(_{t}\) instead of \(z_{t}\) to reconstruct the reward and transition. This trick is effective in generating imaginary tasks featuring extrapolated dynamics, albeit at the cost of increased training complexity.

Imaginary reward generationFigure 3a presents the imaginary reward generation process. The goal is to create imaginary tasks based on the distribution of training subtasks but with unseen compositions of subtasks. At the beginning of a virtual meta-episode, we randomly sample an imaginary subtask composition \(()\) fixed for that virtual meta-episode, where the concentration parameter \(^{K}\) is the empirical running mean of all \(y_{t}\) over training. By replacing the real \(y_{t}\) with the imaginary subtask composition \(\), we sample an imaginary context \(_{t}(_{_{z}}(h_{t},),_{_{z} }^{2}(h_{t},))\), imaginary dispersed context \(}_{t}=p_{_{}}(_{t})\), and finally the imagi

Figure 3: **Virtual training.****(a)** Generation of imaginary rewards with the decoder conditioned on a fixed imaginary subtask composition \(\). **(b)–(e)** Examples depicting the diversity of generated imaginary tasks, where \(K=5\) and all states are from the Meta-World “Reach.” Red rods and blue circles represent the trajectories of the gripper and object, respectively. These trajectories, while generated by the same policy, differ across various imaginary subtask compositions \(\).

nary reward \(_{t+1} p_{_{R}}(r_{t+1}|[s_{t},a_{t},s_{t+1} ],}_{t})\) accordingly using our GMVAE. We replace the reward for the next timestep, \(r_{t+1}\), with \(_{t+1}\), while the states remain to be from the real training task. The imaginary reward is used for the encoder input at the next time step and for the policy, where the policy is trained to maximize the sum of generated rewards, \(_{t=0}^{H^{+}-1}^{t}_{t+1}\). However, the GMVAE is not trained to reconstruct the imaginary dynamics.

### Summary of the Combined Methods: SDVT-LW and SDVT

By combining the subtask decomposition (SD) and virtual training (VT) processes, we propose two methods: SDVT-LW and SDVT. Foremost, our primary contribution is the proposal of SDVT-LW, which is the lightweight (-LW) version of our method that assumes the prior knowledge of the optimal number of subtasks \(K\), therefore not employing the occupancy regularization.

Furthermore, we propose SDVT with the occupancy regularization strategy. This generalizes SDVT-LW to adapt to more difficult conditions where there is a lack of prior knowledge of the optimal number of subtasks \(K^{*}\). We initialize the number of subtasks equal to the number of training tasks and employ occupancy regularization to downscale higher dimensions, navigating to discover the most efficient number of subtasks even without prior knowledge.

For the purposes of virtual training, we adopt two methodologies that have found application in previous studies: dropout  and dispersion via structured VAE . Their use in our work remains unchanged from their original applications. The efficacy of these components, within the context of virtual training, is demonstrated via ablations in Appendix E. We summarize the entire meta-training process with a pseudocode in Appendix A.

## 4 Related Work

Classical meta-RLClassical meta-RL methods assume that both training and test tasks are sampled from the same distribution. These methods are divided into two main categories: gradient-based methods and context-based methods. Gradient-based methods [13; 54; 48; 73; 53] learn a common initialization of a lightweight model for all tasks, allowing the agent to achieve high performance on unseen target tasks with a few steps of gradient updates. However, these methods lack online adaptation capability within a meta-episode because they require many pre-update rollouts before adaptation. Context-based methods [9; 18; 46; 32; 34; 43; 74; 40] use a recurrent or memory-augmented network to encode the collected experience into a latent context. In general, the context is trained to optimize auxiliary objectives such as reward dynamics, transition dynamics, and value function. These methods can adapt to target tasks through online, in-context learning without requiring gradient updates. However, they are vulnerable to test-time distribution shifts since the encoded context and the policy given the context are hardly generalized to out-of-distribution tasks.

Out-of-distribution meta-RLA group of recent studies focuses on training a generalizable agent that is robust to test-time distribution shifts. A group of works generates imaginary observations using image augmentation techniques [18; 21; 31; 44; 33; 61; 66]. Most of these methods depend on predefined heuristic augmentations, without utilizing the training task dynamics. On the other hand, some works explicitly address varying environment dynamics. For example, MIER  reuses the trajectories collected during training by relabeling according to the test dynamics. Our work is related to AdMRL , LDM , and DiAMetR , which generate imaginary tasks with unseen dynamics using learned models. However, these works focus on generating parametric variants of training tasks, while we focus on generalizing across non-parametric task variants.

Skill-based meta-RLOur task parameterization based on the subtask decomposition is related to the recently spotlighted skill-based meta-RL methods [11; 51; 50; 55], which aim to achieve fast generalization on unseen tasks by decomposing and extracting reusable skills from training tasks' trajectories. These works often require refined offline demonstrations to learn skills using behavioral cloning objectives, where the skills distinguish a sequence of actions given a sequence of states. For example, SimPL  extracts skills from offline demonstrations before meta-training, and during a meta-test, it only adapts the high-level skill policy, with the low-level policy frozen. HeLMS  learns a 3-level skill hierarchy by decomposing offline demonstrations of a single task. Online learning of the skills is often unstable because the set of skills should develop along with the online improvement of the policy. The subtask decomposition of our method is conceptually different from the skill decomposition . Even when the policy is not stationary during online updates, the underlying reward and transition dynamics, which our model has to estimate, do not change.

## 5 Experiments

### Experimental Setup

Meta-World benchmarkThe Meta-World V2 benchmark  stands as the most prominent, if not the only, established benchmark for assessing meta-RL algorithms featuring non-parametric task variability. This benchmark comprises 50 qualitatively distinct robotic manipulation tasks, with each task containing 50 parametric variants that incorporate randomized goals and initial object positions. Specifically, the Meta-World Meta-Learning 10 (ML-10) benchmark, consists of \(N_{}=10\) training tasks and \(N_{}=5\) held-out test tasks. We denote each task by an index, where the training tasks are numbered from 1 to 10 and the test tasks from 11 to 15. Likewise, the ML-45 benchmark consists of \(N_{}=45\) training tasks and \(N_{}=5\) test tasks. Refer to the tables in Appendix D.1 for the set and indices of tasks. The agent must maximize its return from experience while exploring to identify the initially unknown task dynamics within a meta-episode of \(H^{+}=5000\) steps that consists of \(n_{}=10\) rollout episodes of horizon \(H=500\) steps each.

SDVT variants and baselines setupWe evaluate our methods SDVT and SD (only subtask decomposition without virtual training and dispersion). Without the prior knowledge of \(K^{*}\), we set \(K=N_{}\) and apply the occupancy regularization \((_{o}=1.0)\) with \(_{c}=1.0\). We also evaluate a lightweight (-LW) version of ours with smaller \(K=5\) with \(_{c}=0.5\) and without the occupancy regularization (\(_{o}=0.0\)). To ensure a fair comparison and to exclude the gains from orthogonal contributions, we compare SDVT with state-of-the-art meta-RL methods that do not require any refined offline demonstrations, ensembles, or extensive test-time training: RL2, MAML , PEARL , and VariBAD . We also compare with a parametric OOD meta-RL method, LDM , to evaluate the efficacy of our virtual training over subtasks. All methods do not perform gradient updates during the test except for MAML. Appendix C presents more implementation details. Briefly, SDVT without a Gaussian mixture reduces to LDM, LDM without virtual training reduces to VariBAD, and VariBAD without a VAE decoder reduces to RL2. Our implementation is available at https://github.com/suyoung-lee/SDVT.

Evaluation metricWe follow the standard success criterion of Meta-World as follows. A timestep is considered successful if the distance between the task-relevant object and the goal is less than a predefined threshold. A rollout episode is considered successful if the agent ever succeeds at any timestep during the rollout episode. The success of a meta-episode is defined as the success of the last (10th) rollout episode. In Table 1, we report the success rates of 750 (15 tasks \(\) 50 parametric variants) meta-episodes at 250M steps for ML-10 and 2500 (50 tasks \(\) 50 parametric variants) meta-episodes at 400M steps for ML-45.3 Likewise, we report the returns of the last rollout episodes.

### Results

Table 1 illustrates that no method attains a 100% success rate even on training tasks, emphasizing the challenge posed by non-parametric task variability. The training success rates on ML-45 are consistently lower than those on ML-10, reflecting the inherent difficulty in adapting to a broader range of tasks, such as conflicting gradients . Notably, SD surpasses all other baselines in training success rate. In particular, outperforming VariBAD underscores the limitations of employing a single Gaussian distribution to model the latent task space in cases of non-parametric variability.

On the test tasks, SDVT and SDVT-LW substantially outperform all baselines, even outperforming LDM, which surpasses VariBAD with parametric virtual training. Our gain is attributed to our virtual training process, which is specifically designed for test tasks involving non-parametric variability. Notably, SDVT and SD outperform their LW counterparts in training success rates, primarily due to its fine-grained subtask decomposition which provides a more precise representation of each task. For example, a "grip object" subtask may be split and represented as a combination of two distinct subtasks "move gripper" and "tighten gripper" with a larger \(K\). In contrast, SDVT-LW scores higher test success rates than SDVT, presumably due to its coarser decomposition that allows imaginary compositions to encompass a broader range of unseen test tasks.

Please refer to the tables in Appendix D.1 for individual task results. Our methods achieve the highest success rates across all test tasks on the ML-10 benchmark. However, all methods encounter challenges in solving "Shelf-place," which includes an unseen shelf not present in the observation. As such, this task cannot be decomposed into previously seen subtasks but rather into unseen ones, making it difficult to prepare through virtual training. These tasks, composed of unseen subtasks, pose a considerable challenge for zero-shot adaptation as with SDVT. Addressing these tasks may require a substantial increase in rollouts and updates during tests, an area of potential future work. Detailed ablation results are reported in Appendix E. For additional results such as rendered videos, refer to our webpage https://sites.google.com/view/sdvt-neurips.

### Analysis

In this subsection, we explore whether the performance enhancements attributed to our methods are indeed the result of effectively decomposed subtasks and a diverse range of imaginary tasks.

Learned Subtask CompositionsFrom Figure 2b, we observe a rapid context convergence within the first rollout episode for a sufficiently meta-learned task. To validate our motivation, we visualize the converged subtask compositions in Figure 4. We find that such converged subtask compositions are shared by qualitatively similar tasks. For example, in Figure 4a, tasks (3) "Pick-place," (7) "Peg-insert-side," and (10) "Basketball," which require placing an object at a goal position, share subtask indices 1 and 3. Additionally, simple tasks that require moving the gripper in a straight line: (1) "Reach," (5) "Drawer-close," and (8) "Window-open" are classified into the same subtask 2. These observations suggest that our model can effectively meta-learn the decomposition process of tasks into shareable subtasks. Furthermore, the test tasks may not necessarily have the same subtask compositions as the training tasks. For instance, (3) "Pick-place" and (14) "Sweep-into" share subtask 3, but not subtasks 1 or 5, revealing the potential for virtual training to be effective.

Occupancy RegularizationFigure 4 indicates that it is important to set the subtask class dimension \(K\) and the categorical coefficient \(_{c}\) appropriately according to the number of training tasks and the correlations among them. Otherwise, the decomposition may suffer from a collapse or degeneration. With the occupancy regularization, we can avoid the extensive search for the optimal subtask dimension \(K^{*}\) and corresponding \(_{c}\). In Figure 4b, we find that our occupancy regularization successfully limits the use of unnecessary subtasks with higher indices as intended.

Generated imaginary tasksTo demonstrate the dynamics of the imaginary tasks, we show the trajectory of the gripper and object over a meta-episode (5000 steps) on generated imaginary tasks for different imaginary subtask compositions \(\) in Figure 3. When we set \(\) as a one-hot vector, the policy

    &  &  \\   &  &  &  &  \\  Methods & Train & Test & Train & Test & Train & Test & Train & Test \\  SDVT & **77.2\(\)3.0** & 32.8\(\)3.9 & 55.6\(\)4.2 & 28.1\(\)3.2 & **3656\(\)62** & 1225\(\)160 & 2379\(\)214 & 839\(\)74 \\ SDVT-LW & 62.1\(\)4.1 & **33.4\(\)5.0** & 50.4\(\)4.1 & **31.2\(\)1.2** & 3454\(\)137 & **1527\(\)214** & 2294\(\)202 & **894\(\)27** \\ SD & 77.0\(\)5.9 & 30.8\(\)7.7 & **61.0\(\)1.7** & 23.0\(\)5.1 & 3630\(\)241 & 1112\(\)190 & **2672\(\)79** & 786\(\)69 \\ SD-LW & 75.5\(\)5.5 & 26.2\(\)8.7 & 56.7\(\)1.5 & 25.4\(\)2.9 & 3525\(\)297 & 1043\(\)234 & 2578\(\)64 & 793\(\)49 \\  RL\({}^{2}\) & 67.4\(\)4.4 & 15.1\(\)2.7 & 58.0\(\)0.4 & 11.8\(\)3.2 & 1159\(\)83 & 715\(\)33 & 1411\(\)22 & 663\(\)100 \\ MAML & 42.2\(\)4.5 & 3.9\(\)3.7 & 32.0\(\)1.4 & 19.8\(\)6.3 & 1822\(\)136 & 439\(\)78 & 1388\(\)104 & 658\(\)96 \\ PEARL & 23.2\(\)1.9 & 0.8\(\)0.5 & 10.3\(\)2.4 & 6.7\(\)3.3 & 1081\(\)77 & 340\(\)54 & 597\(\)121 & 506\(\)122 \\ VariBAD & 58.2\(\)8.9 & 14.1\(\)6.1 & 57.0\(\)1.2 & 22.1\(\)3.5 & 3055\(\)466 & 919\(\)143 & 2492\(\)47 & 762\(\)40 \\ LDM & 56.7\(\)12.3 & 19.8\(\)6.0 & 54.1\(\)0.9 & 24.8\(\)2.9 & 2963\(\)626 & 1166\(\)264 & 2515\(\)67 & 768\(\)63 \\   

Table 1: **Meta-World V2 success rates and returns. We report the final success rates (%) and returns of our methods and baselines averaged across training tasks and test tasks of the ML-10 and ML-45 benchmarks. All results are reported as the mean success rate \(\) 95% confidence interval of 8 seeds. Individual scores of all tasks are reported in Appendix D.1.**returns a trajectory that solves an elementary subtask, such as placing and reaching up. We observe that the trajectories vary across different \(\), and similar compositions result in similar trajectories.

## 6 Conclusion

In conclusion, our proposed method has demonstrated a considerable enhancement in meta-RL with non-parametric task variability. This improvement is achieved by meta-learning shareable subtask decompositions and executing virtual training on the imaginary subtask compositions. However, it's essential to acknowledge certain potential limitations beyond the scope of this study, particularly when addressing test tasks involving entirely novel subtasks and in broader setups where the action and state spaces may also vary. Despite these limitations, we believe that expanding the realm of meta-RL to accommodate a wider range of task variability is a critical research topic. Incorporating orthogonal approaches such as using offline demonstrations or test time training techniques into our method could lead to interesting future work addressing the limitations. We are optimistic that our study lays a robust foundation for future research in this field.