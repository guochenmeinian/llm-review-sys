# Visual Fourier Prompt Tuning

Runjia Zeng\({}^{1}\)1

 Cheng Han\({}^{2}\)1

Qifan Wang\({}^{3}\)

 Chunshu Wu\({}^{4}\)

Tong Geng\({}^{4}\)

Lifu Huang\({}^{5}\)

Ying Nian Wu\({}^{6}\) and Dongfang Liu\({}^{1}\)\({}^{1}\)

\({}^{1}\)Rochester Institute of Technology \({}^{2}\)University of Missouri - Kansas City

\({}^{3}\)Meta AI \({}^{4}\)University of Rochester

\({}^{5}\)UC Davis \({}^{6}\)University of California, Los Angeles

Equal contribution. \({}^{}\) Corresponding author.

###### Abstract

With the scale of Transformer-based vision models continuing to grow, finetuning these large-scale pretrained models for new tasks has become increasingly parameter-intensive. Visual prompt tuning is introduced as a parameter-efficient finetuning (PEFT) method to this trend. Despite its successes, a notable research challenge persists within almost all PEFT approaches: significant performance degradation is observed when there is a substantial disparity between the datasets used in pretraining and finetuning phases. To address this challenge, we draw inspiration from human visual cognition, and propose the Visual Fourier Prompt Tuning (VFPT) method as an effective and efficient solution for adapting large-scale Transformer-based models. Our approach innovatively incorporates the Fast Fourier Transform into prompt embeddings, seamlessly integrating both spatial and frequency domain information. Apart from its inherent simplicity and intuitiveness, VFPT exhibits superior performance across various tasks, offering a general solution to address the data disparity challenge. Empirical results demonstrate that our approach outperforms several state-of-the-art baselines on two benchmarks, with low parameter usage (_e.g._, 0.57% of model parameters on VTAB-1k) and notable performance enhancements (_e.g._, 73.20% of mean accuracy on VTAB-1k). Our code is avaliable at https://github.com/runtsang/VFPT.

## 1 Introduction

_"Fourier's theorem is not only one of the most beautiful results of modern analysis, but it may be said to furnish an indispensable instrument in the treatment of nearly every recondite question in modern physics."_

\(-\) Lord William Thomson Kelvin 

Prompt tuning  is initially introduced for parameter-efficient adaptation of large foundation models in natural language processing (NLP). As vision models continue to scale for enhanced performance, visual prompt tuning  has been applied to various vision domains (_e.g._, image classification , segmentation , detection ), demonstrating superior performance and lower parameter usage compared to other parameter-efficient fine-tuning (PEFT) methods. However, a common challenge within the research community remains unaddressed: significant performance degradation occurs when there is a substantial disparity between the data used in pretraining and finetuning . This issue hinders the broader application of visual prompt tuning. Consequently, a natural question arises: _Can prompt tuning generalize across datasets with varying disparities?_

As researchers commonly draw insights from human to replicate the principles in intelligent machines , we consider to answer this question from the human visual cognition's perspective. While humans comprehend the world through past experiences/knowledge, it is essential to generalize and adapt this understanding to new tasks efficiently and effectively. The robust and rapid adaptability of human visual cognition thus arises from various domain analysis, capturing the new patterns from different channels and perspectives .

Interestingly, we find that the paradigm of visual prompt tuning is conceptually analogous to human visual cognition. While the frozen large-scale vision model functions as accumulated knowledge, the fast adaptation mechanism resembles visual prompt tuning, requiring the incorporation of diverse domains of information (_e.g._, time, frequency) to achieve comprehensive understandings [18; 19; 20]. The Fast Fourier Transform (FFT) [18; 19; 20], renowned for its ability to convert signals from their original domain (_e.g._, time or spatial) to the frequency domain and vice versa, serves as an ideal tool for contributing informative insights in the frequency domain. By leveraging the capabilities of FFT, visual prompts can naturally integrate both spatial and frequency domain information during finetuning, thereby enabling the frozen vision model to achieve consistent and robust performance across datasets with varying disparities. Consequently, our research question evolves into: _R How can FFT be integrated into visual prompt tuning to emulate the human visual mechanism?_

To this end, we employ a simple yet effective strategy that utilizes the Fourier operations to facilitate visual prompt tuning (see Fig. 1(c)). By integrating frequency domain information into learnable prompt embeddings, our approach elegantly assimilates data from both spatial and frequency domains, simulating the human visual cognition. We name our approach **V**isual **F**ourier **P**rompt **T**uning (**V**FPT**), which exhibits several compelling advantages: _Simplicity.** The intuitive application of FFT in prompt tuning emulates the rapid processing capabilities of the human visual system, making VFPT both elegant and straightforward to implement (see SS2.1). _Generality._ By incorporating frequency domain information, the search space for latent embeddings of prompts is naturally expanded, resulting in advanced enhancement in performance across different datasets and tasks with varying data disparities (see SS4.2). The generality of our model is further illustrated through our analysis of the optimization process, which enables smoother navigation towards local minima, increasing flatness around them and exhibiting apparent convexity. _Interpretability._ To intuitively demonstrate the advantages of Fourier components, we visually illustrate that the introduction of Fourier transform in visual prompt tuning results in a markedly higher concentration of attention scores within the Transformer's input space, which correlates positively with enhancements in performance (see SS4.4). This observation, in turn, explains the effectiveness of our approach.

Comprehensive experiments are conducted to evaluate the performance of VFPT. In SS2, we conduct a literature review and discuss relevant works. Our approach is presented in SS3, where we describe how we simple yet effectively integrate FFT into visual prompt tuning. In SS4.2, we present compelling experimental results on various benchmarks, backbones, and different pretraining objectives, achieving superior performance _without_ complex engineering design. Specifically, our approach achieves an average improvement of **7.63%** in accuracy on VTAB-1k compared to full finetuning, and **3.77%** compared to VPT . In SS4.4, we demonstrate that the FFT prompts significantly enhance the activation of the frozen vision model. Additionally, we study the optimization process of prompt tuning approaches, indicating that VFPT provides a more favorable optimization process. Finally, we demonstrate the strong algorithmic generalization of our approach to the language domain, and show additional visual explanations in the Appendix. We trust that this work provides valuable insights.

## 2 Related Work

### Visual Parameter-efficient Finetuning

With the significant growth in the scale of vision models, especially following the emergence of Vision Transformers [21; 22; 23; 24; 25], the development of PEFT methods under "pretrain-then-finetune" paradigm becomes increasingly critical. Current methods under this paradigm can be generally categorized into _partial tuning_[26; 27; 28], _extra module_ (_i.e._, including reparameterization approaches such as Low-Rank Adaptation (LoRA) ) [30; 31; 32; 33; 34; 10; 35; 36], and _prompt tuning_[4; 37; 38; 39; 40; 41]. Partial tuning and extra module face several limitations that hinder their application. \(C\) Unsatisfactory performance: they generally cannot reach competitive performance with regard to full finetuning [4; 26; 27; 28; 33; 10]; \(R\) Model-oriented design: most research requires to insert specific architecture/block design [31; 30; 32] during tuning, rendering them non-universal solutions when considering different backbones. In contrast, prompt tuning , originally proposed for language-domain [42; 43; 44; 45], provides a general and straightforward solution in vision with powerful performance gains. It signals a new paradigm in PEFT in the field of computer vision.

Generally, prompt tuning introduces a sets of learnable parameters to the input sequence of backbone models, updating only these parameters during the finetuning. Despite its apparent simplicity, the paradigm of visual prompt tuning has demonstrated notable performance enhancements. Currentdevelopments on visual prompt tuning primarily concentrate on engineering optimizations, such as reducing parameter usage  and expanding applicability across diverse tasks [39; 46; 47; 48]. These approaches often involve introducing additional constraints and functionalities to the foundational design, which deviate from the principles of simplicity and elegance to the original concept of visual prompt tuning. Our approach, in sharp contrast, endeavors to explore visual prompt tuning from the perspective of _human visual intelligence_, while diligently maintaining the _simplicity_ of prompt tuning. It is also essential to emphasize that visual prompt tuning diverges markedly from visual instruction tuning  (_i.e._, aiming at improving the model's instruction following abilities).

### Fast Fourier Transform in Vision

FFT is a powerful mathematical algorithm used to compute the Discrete Fourier Transform (DFT) and its inverse [50; 51]. It is pivotal in information processing, allowing the detailed analysis of various signals (_e.g._, image [52; 53; 54], radar [55; 56; 57]) for frequency determinations. In vision, FFT's ability to transform complex data in spatial domain into frequency domain makes it an invaluable tool for abstracting critical features from noisy or high-dimensional datasets [58; 59]. This abstraction is particularly beneficial as the identification of salient features are shown to have better generalization ability across domains [60; 61; 62; 63], directly influences the performance [64; 65; 66; 67] of image analysis and processing tasks. Current research on FFT in vision predominantly explores areas such as conventional image processing [52; 68; 69; 70], image pre-processing for deep neural networks (DNNs) [71; 72] and DNN architectural design [20; 66; 65; 73; 74; 75; 76].

Despite its profound utility and effectiveness, the integration of FFT within the paradigm of visual prompt tuning remains largely underexplored. Recent work  adapts the pretrained multi-modal network to the tasks under modality-incomplete segmentation scenarios via FFT prompt tuning. This approach demonstrates the potential of FFT operations to handle missing modalities (_i.e._, substantial disparity) effectively. However, it primarily focuses on task-specific optimization and design. The extensive applicability and generality of FFT, especially in cross-dataset analysis, have yet to be recognized or exploited. Another work  incorporates Fourier transform into the LoRA-based approach. While the expressive Fourier basis facilitates the recovery of weight changes, it does not fully integrate frequency domain information during finetuning, which remains orthogonal to our approach. In this paper, we aim to broaden the scope of exploration and contribute to advancing the field of Fourier-based research in vision. By studying the integration of FFT with visual prompt tuning, we fully explore how to improve both the efficacy (see SS3) and the adaptability of learning models to diverse and challenging datasets (see SS4). Furthermore, we present novel evidence indicating that VFPT establishes strong correlations within the Transformer's input space, aligning with the performance enhancements (see SS4.4). Overall, the generality of VFPT suggests a novel understanding of the Fourier-based method in current machine learning applications.

## 3 Methodology

In this section, we introduce VFPT, a novel visual prompt tuning approach for effective and general large-scale transformer-based model finetuning. We first define the problem and notations of visual prompt tuning and FFT in SS3.1. The integration of Fourier-based visual prompt tuning is presented in SS3.2. The overall framework is shown in Fig. 1(c), where we compare our model with original VPT.

### Preliminary

**Visual Prompt Tuning.** Given a pretrained Transformer model \(\) with \(N\) layers, the objective of prompt tuning in vision is to finetune a model \(}\) into a new task with only a few set of \(d\)-dimensional embedding vectors, _i.e._, prompts, in the input space after patch \(\) layer. These learnable prompts are defined as \(\{P^{1},P^{2},,P^{N}\}\), where \(P^{i}\) represents the learnable visual prompts in the \(i_{th}\) encoder layer. Formally, the encoder layers with prompts are defined as:

\[& Z^{1}=L_{1}(P^{1},\;E)\\ & Z^{i}=L_{i}(P^{i},\;Z^{i-1}) i=2,3,,N\] (1)

where the embeddings of the input image patches \(E\) are initialized with frozen \(\) projection, and \(Z^{i}\) is the contextual embeddings computed by the \(i_{th}\) encoder layer. The colors \(\) and \(\) indicate trainable and frozen parameters, respectively. Here, trainable prompts only accounts for a small proportion of the total parameters (_e.g._, 1.14% on VTAB-1k  in VPT ).

**Fast Fourier Transform.** The FFT is a powerful algorithm for computing the Discrete Fourier Transform (DFT), which transforms a finite sequence of equally-spaced function samples into a same-length discrete-time Fourier transform sequence. Specifically, given a sequence \(\{x_{n}\}\) where \(n\) is a member of the interval \(n[0,N-1]\), the DFT is defined as:

\[(x)=X_{k}=_{n=0}^{N-1}x_{n}e^{-i2n}, 0 k  N-1.\] (2)

For a finite sequence of equally-spaced samples \(\{x_{n}\}\), the DFT generates a same-length sequence of equally-spaced samples \(\{X_{k}\}\). This transform is denoted as \(\). The initial DFT is in complexity \(O(n^{2})\). For acceleration, we use Cooley-Tukey FFT algorithm  following common practice  (_i.e._, complexity \(O(n n)\)). FFT serves as a powerful tool for domain transition. Consequently, we explore the integration of the FFT operation within PEFT methods, particularly in prompt tuning.

### Visual Fourier Prompt Tuning

Visual prompt tuning is particularly useful under the _pretrain-then-finetune_ paradigm. However, it suffers a significant performance reduction when substantial disparities exist between pretrain and finetune datasets. The reason is that during finetuning on new data, the image distribution may deviate markedly from the examples used in pretraining the backbone model . Existing prompt tuning [4; 5], focusing predominantly on spatial information, can only harness the shared information embedded within the pretrained backbone, limiting their capacity to adapt effectively to novel tasks. Thus, it is crucial to strengthen the ability to capture distinguishing feature from finetuning data.

To this end, we introduce VFPT, an intuitive yet powerful method with advanced performance and generality. Compared to VPT (see Fig. 1(a)), our model (see Fig. 1(c)) transforms partial prompts from spatial domain to frequency domain via 2D FFT (see SS3.1) to consider both the spatial and frequency domain information. Formally, for each learnable visual prompts in the \(i_{th}\) encoder layer \(P^{i}\) = \(\{P^{1},P^{2},,P^{N}\}\), we have \(P^{i}\) = \(\{p^{i}_{1},p^{i}_{2},,p^{i}_{M}\}\). We select \(m\) partial prompts as visual Fourier prompts at each layer, where \(0 m M\). Further, \(=m/M\) represents the fraction of Fourier participation, where zero indicates all prompts are original visual prompts, and one implies all prompts are given after FFT. We apply a 2D FFT on \(\) visual prompt embedding input with respect to both sequence (_i.e._, \(_{}\)) and hidden dimensions (_i.e._, \(_{}\)). Note that the operations \(_{}(_{}(x))\) and \(_{}(_{}(x))\) are mathematically equivalent due to the commutative property of the two one-dimensional FFTs . Here, \(\) indicates Fourier operations.

\[P^{i}_{}=(_{}(_{ {h}}([p^{i}_{1},p^{i}_{2},,p^{i}_{m}]))).\] (3)

To maintain the pretrained structure's consistency, we only alter the prompt embeddings, and thus retain only the real component (_i.e._, \(\)) from the output. This design does not require any adjustments to accommodate complex numbers in the self-attention module, ensuring that the remaining elements

Figure 1: **Overview of VPT \(vs.\) VFPT (ours) frameworks.** (a) Original Visual Prompt Tuning. (b) 2D Fast Fourier Transform operations in partial visual prompts along hidden and sequence length dimensions. (c) The overall architecture of our proposed VFPT (see §3.2).

of the model remain unchanged. Consequently, the overall integrated prompts \(}\) in the \(i_{th}\) encoder layer are formed by the concatenation between the visual Fourier prompts and visual prompts as:

\[}=[P^{i}_{},p^{i}_{m+1},,p^{i}_{M}].\] (4)

Our elegant design of VFPT enjoys a few appealing characteristics:

* _Simplicity:_ VFPT only requires several lines of code based on the implementation of the visual prompt tuning. Its intuitive integration of information between spatial and frequency domains brings _nearly free_ performance efficacy. The low complexity of FFT (_i.e._, \(O(n n)\)) leads to an overall marginal reduction during the training schedule.(_i.e._, 2.8% on VTAB-1k ). In sharp contrast, current endeavors in visual prompt tuning mainly emphasize augmenting architectural complexity for superior performance [5; 81; 42], undermining the inherent simplicity of prompt tuning and introducing significant training overhead (_e.g._,  learns 2D prompt token map for densely image relationship construction,  incorporates additional self-attention K-V prompts).
* _Generality:_ The frequency and spatial analysis of imagery inputs can be mutually complementary, leading to a more comprehensive feature understanding from distinct perspectives (_e.g._, the frequency domain allows for the distraction and decomposition of luminance and noise to a considerable degree , while the spatial domain excels in capturing intricate object details). By incorporating learnable prompts from both domains, VFPT demonstrates enhanced prompt learning capabilities, which makes it superior to finetune across diverse tasks (see SS4.2). The empirical findings of flatness and convexity of VFPT further strength our claim.
* _Interpretability:_ In visual prompt tuning, a notable challenge arises concerning the interpretability of learnable prompts. Unlike in NLP, where tokens explicitly represent these prompts, visual prompts have historically lacked a clear and explainable representation. In order to intuitively perceive the function of visual prompts, we offer a possible way to understand why prompts play an important role in fine-tuning a new task through the visualization of attention maps. Moreover, we can also observe a better and stronger global feature learning pattern through introducing visual Fourier prompts, showing how Fourier prompts work. More discussion will be elaborated in SS4.4.

## 4 Experiment

### Experiment Setup

**Datasets.** Following common practice [5; 81; 83; 4], our experiments are carried out on two image classification benchmarks. **VTAB-1k** collects \(19\) benchmarked Visual Task Adaptation, separated into three groups: (1) _Natural_ includes natural images captured by standard cameras, (2) _Specialized_ consists of images taken by specialized equipment, and (3) _Structured_ considers tasks considering geometric comprehension (_i.e._, counting, distance), which has substantial dataset disparities (_i.e._, tasks in _Natural_ and _Specialized_ are closely related to image classification and thus have low disparities, while tasks in _Structured_ are regarded as distinct from image classification) when comparing to the pretrained dataset  (_i.e._, ImageNet21K ). Each task of VTAB-1k contains \(1000\) training examples with the \(800/200\) split for train/val set. **FGVC** contains \(5\) benchmarked Fine-Grained Visual Classification, including CUB-200-2011 , NABirds , Oxford Flowers , Stanford Dogs  and Stanford Cars . The training set is split into 90% train and 10% val.

**Baselines.** For consistency, we follow [4; 5] and compare VFPT with other widely applied parameter-efficient fine-tuning methods. Results of two vision transformer architectures, Vision transformer  (ViT) and Swin transformer  (Swin), on image classification are discussed in SS4.2. We also apply VFPT on two self-supervised objectives: MAE  and MoCo v3 .

**Training.** Following [4; 5], we conduct grid search to find the best tuning hyperparameters, learning rate (_i.e._, [50; 25; 10; 5, 2.5, 1, 0.5, 0.25, 0.1, 0.05]), and weight decay (_i.e._, [0.01, 0.001, 0.0001, 0.0]) on val set. Notably, VFPT _does not require_ specific-designed large learning rate in . The learning rate is scheduled by a cosine decay policy and trained for \(100\) epochs.

**Reproducibility.** VFPT is implemented in Pytorch . Experiments are conducted on NVIDIA A100-40GB GPUs. To guarantee reproducibility, our full implementation will be publicly released.

### Main Results

In this section, we demonstrate the effectiveness of VFPT from two key perspectives: \(\)_Superior Performance:_ Our model demonstrates significant performance improvements across diverse datasets, including challenging tasks with large disparities in data, thus showcasing its generalizability.

[MISSING_PAGE_FAIL:6]

the ImageNet-21k supervised pretrained Swin-Base  are reported in Table 2. It can be seen that VFPT consistently outperforms **all** the other parameter-efficient methods on three VTAB-1k groups. **VFPT on Different Pretraining Objectives.** In Table 3, we report the experimental results on two self-supervised objectives: MAE  and MoCo v3 . While VPT yields inconclusive results, VFPT has the **highest** "Number of Wins" compared to full fine-tuning among PEFT methods (_i.e._, **8 of 19** instances under MAE, and **14 of 19** instances under MoCo v3, respectively). Our method also outperforms VPT by a large margin (_e.g._, **53.59%**\(vs.\) 36.02% under MAE on VTAB-1k _Natural_).

\(\)_Fourier Contribution._ We conducted experiments to understand the impact of Fourier components by varying the percentages of Fourier prompts in VFPT. As shown in Fig. 2, we observed distinct preferences across the VTAB-1k benchmark, which comprises three groups with varying data disparities (see SS4.1). Specifically, the _Natural_ group, which has a data distribution similar to the pretrained task (low disparity), shows peak performance when half of the visual prompts are transformed into Fourier prompts, as indicated by the accuracy curves in Fig. 2(a). This suggests that transfer learning is less challenging in this group. Conversely, for the _Specialized_ and _Structured_ groups, which have data distributions significantly different from the pretrained task (high disparity), the accuracy curves in Fig. 2(b-c) demonstrate that higher classification performance is achieved with an increased percentage of Fourier components. These observations are consistent with our expectations, demonstrating the effectiveness of Fourier prompts in VFPT, especially for tasks with large data disparities. In other words, our approach can be viewed as a generalization of VPT, where the Fourier components learn effective representations from the frequency domain that complement the knowledge from the spatial domain.

### Study of Optimization

In this section, we investigate why VFPT achieves better performance and generalization across various tasks from an optimization perspective. Previous works  demonstrate that landscape geometry significantly impacts model generalization, so we visualize the loss landscape to

    &  &  \\  Methods &  &  &  &  &  &  \\  Full & \(\) & 100.0\% & 59.3  & \(\) & 79.6\% & 53.82\% & 100.0\% & 71.95\% & 84.72\% & 51.99\% \\  Linear &  & 0.04\% & 18.7\%  & 33.72\%  & 23.70\%  & 0.04\% & 67.46\%  & 81.08\%  & 30.33\%  \\ Partial-1 &  & 3.30\% & **54.84\%**  & 28.28\%  & 47.45\%  & 8.30\% & 23.14\%  & 84.85\%  & 47.89\%  \\  Bias &  & 0.16\% & 54.55\%  & 75.65\%  & **47.90\%**  & 0.16\% & 72.89\%  & 81.14\%  & 53.43\%  \\ Adaptive &  & 0.87\% & 54.90\%  & 75.19\%  & 39.98\%  & 1.12\% & 74.19\%  & 82.66\%  & 47.69\%  \\  VPTs &  & 0.05\% & 39.96\%  & 60.65\%  & 27.35\%  & 0.06\% & 67.37\%  & 82.26\% & 37.53\%  \\ VPT-D &  & 0.31\% & 36.02\%  & 60.64\%  & 26.57\%  & 0.22\% & 70.27\%  & 83.04\%  & 2.38\%  \\ CPT &  & 0.05\% & 47.61\%  & 76.86\%  & 36.85\%  & 0.06\% & 74.84\%  & 83.83\%  & 49.10\%  \\  Ours & 0.38\% & 53.59\%  & 27.75\%  & 36.15\%  & 0.22\% & **77.47\%**  & **85.76\%**  & **88.74\%**  & **88.74\%**  \\   

Table 3: **Image classification accuracy for different pretrained objectives — MAE  and MoCo v3  with ViT-Base  as backbone. \(\) denotes the return results that calibrate the VPT **

Figure 2: **Image classification accuracy of various Fourier percentages of VTAB-1k  for ViT-Base/16 .** For better illustration, we randomly select 3 datasets in each group of VTAB-1k. The “Average FID Score of Each Group” is reported in <->. Our conclusion aligns with **16 of 19** cases. The cross framed by the square indicates the best percentage for each downstream task. Those datasets with only three Fourier percentage reports are due to the prompt length limits.

understand the enhanced generality of VFPT. Specifically, in Fig. 3(a), we randomly select two parameter directions for the study, as randomness in directions does not significantly affect the results . There are two key observations supporting the enhanced generality of VFPT. **i) Flatness**: VFPT provides a larger connected region around the local minimum  (e.g., \(\) in the yellow square, where the larger blue area in VFPT offers more optimization choices) and a smoother edge of the loss landscape for mitigating chaotic landscapes (e.g., \(\) in the green square, where the bumpy contour in VPT is sensitive to loss variations, resulting in worse generality). This indicates that VFPT achieves a flatter minimizer, which consistently correlates with lower test error . **ii) Convexity**: As eigenvalues of the Hessian directly assess the convexity of a loss function , we compute both the maximum and minimum eigenvalues of the Hessian and map their ratios . As shown in Fig. 3(b), a higher prevalence of near-zero negative eigenvalues (in deep blue) in VFPT suggests the presence of more convex regions (25.0% vs. 20.0%) for model optimization. This finding indicates that the incorporation of the Fourier transform in visual prompt tuning effectively mitigates the sharpness of the loss landscape.

### Study of Interpretability

To the best of our knowledge, research on the understanding of prompt tuning remains rare . Consequently, our research seeks to both quantitatively and qualitatively examine the impact of Fourier components on the enhancement of visual prompt tuning. For fairness, instead of using enhanced visualization methods  that may alter the original expression of the learnable prompts, we visualise and examine the raw average attention head on the last layer of VPT and VFPT.

**Significant attention distribution in learnable prompts**. Observations from both VPT and VFPT in Fig. 4(a) reveal a common phenomenon: there exists a pronounced accumulation of attention scores at learnable prompt locations (_i.e_., narrow color area on the left side of 2D attention map), indicating that these prompts have a substantial impact on the frozen embeddings during the finetuning stage.

**Global attention scores pattern in Fourier prompts**. We further observe a notably higher concentration in global attention scores when integrating visual Fourier prompts. Specifically, the global attention scores indicate that VFPT also establishes robust correlations within the Transformer's input space  (see Fig. 4(a)). In contrast, VPT lacks this correlation, suggesting that it does not adequately consider or integrate extensive information from the frozen backbone. Moreover, we find a positive relationship between strong associations and performance gains quantitatively (see SS4.2) and qualitatively (see Fig. 4(b)) in VFPT, suggesting that the integration of visual Fourier prompts encourage clear foreground (_i.e_., tree with high frequency component) - background (_i.e_., sky with low frequency component) separation.

Figure 4: **Study of interpretability.** (a) The 3D and 2D attention map in VPT and VFPT on a randomly selected sample. The colors, and indicate class, prompt and patch tokens, respectively. (b) Corresponding GradCAM  maps. Note that red regions correspond to a high score for the class. We present more visualization results in §S4

### Ablation Study

We ablate VFPT's key components on VTAB-1k _Natural_ and _Specialized_. More studies are provided in SS2.5.

**Transform Type.** We ablate on other transform method instead to certify the impact of Fourier transform in Table 4, where the Fixed Linear Layer (_i.e_., FLL) and the Learnable Linear Layer (_i.e_., LLL) are considered. Compared with FFT, a fixed non-parameter Fourier domain transform in sequence and hidden dimension, the FLL operation considers only a fixed spatial domain transform in hidden dimension; the LLL further unfixes the transformation to enable gradient updates. As seen, both FLL and LLL show inferior performance to FFT. We further consider the impact of current Fourier domain adaption approach , which maps a source image to a target "style" without altering semantic content. However, no significant improvement can be observed.

**Fourier Prompt Dimension.** A fundamental distinction between VFPT and other methods is the incorporation of FFT into visual prompts. In our standard implementation, we utilize 2D FFTs across both sequence length and hidden dimensions. Here, we explore the impact of each dimension's transformation individually. As shown in Table 5(a), the separate Fourier transformations along each dimension appear to have similar contributions (_i.e_., 80.88% \(vs\). 80.74% in _Natural_). However, the combined application of transformations across both dimensions (_i.e_., 2D FFTs) demonstrates a synergistic effect, yielding significant improvement in performance.

**Fourier Prompt Location.** In Table 5(b), three prompt locations are considered for VFPT, which are "Prepend" (_i.e_., \(\)), "Append" (_i.e_., \(\)), and "Random" (_i.e_., \(\)). Specifically, \(\) and \(\) prepend visual Fourier prompts before or after visual prompts, and \(\) randomly selects the position for visual Fourier prompts in each layer. As seen, both \(\) and \(\) show competitive results, validating the robustness of VFPT _w.r.t._ prompt locations. In alignment with the findings in [5; 4], we choose \(\) as our baseline method in all experiments since it reaches superior results (_i.e_., 81.35% \(vs\) 81.02% in _Natural_).

**Fourier Prompt Depth.** Table 5(c) presents the performance of VFPT based on the specific layer at which visual Fourier prompts are employed. The results suggest that employment on separate layers also yields a accuracy improvement compared with VPT. Further application of visual Fourier prompts across all layers fosters the best overall performance.

## 5 Conclusion

We present **V**isual **F**ourier **P**rompt **T**uning (**VFPT**), a simple yet powerful parameter-efficient visual prompt tuning approach that draws insights from human visual cognition. It has merits in: **i)** integrating spatial and frequency domain information through an intuitive yet effective design; **ii)** demonstrating generality across datasets with varying disparities while ensuring powerful performance; and **iii)** thoroughly investigating the associations between learnable prompts and frozen embeddings to elucidate this generality. As a whole, we conclude that the outcomes elucidated in this paper impart essential understandings and necessitate further exploration within this realm.

Table 4: **Ablative studies of transform type** on VTAB-1k _Natural_ and _Specialized_ benchmarks in three runs. Per-task results are available in Appendix.

Table 5: A set of **ablative studies** on VTAB-1k _Natural_ and _Specialized_ benchmarks in three runs. “Prompt Location” is the placement of the visual Fourier prompts relative to original visual prompts. “Prompt Depth” indicates the layer we use visual Fourier prompts. “Transform Type” is the method we use to transform prompts and input images. “Fourier/Transform Dimension” indicates the dimension we apply using specific transform method. Per-task results are available in Appendix. Same for Table 4.

Acknowledgements

This research was supported by the National Science Foundation under Grant No. 2242243.