# A benchmark for prediction of transcriptomic responses to chemical perturbations across cell types

Artur Szalata\({}^{1}\)

Andrew Benz\({}^{2*}\)

Robrecht Cannoodt\({}^{3,4,5}\)

Mauricio Cortes\({}^{2}\)

Jason Fong\({}^{2}\)

Sunil Kuppasani\({}^{2}\)

Richard Lieberman\({}^{2}\)

Tianyu Liu\({}^{6}\)

Javier A. Mas-Rosario\({}^{2}\)

Rico Meini\({}^{7}\)

Jaili Nourisa\({}^{8}\)

Jared Tumiel\({}^{7}\)

Tin M. Tunjic\({}^{9}\)

Mengbo Wang\({}^{10}\)

Noah Weber\({}^{11}\)

Hongyu Zhao\({}^{6}\)

Benedict Anchang\({}^{12}\)

Fabian J. Theis\({}^{1}\)

Malte D. Luecken\({}^{1}\)

Daniel B. Burkhardt\({}^{2}\)

\({}^{1}\)Helmholtz Munich

\({}^{2}\)Cellarity

\({}^{3}\)Data Intuitive

\({}^{4}\)VIB Center for Inflammation Research

\({}^{5}\)Ghent University

\({}^{6}\)Yale University

\({}^{7}\)Retro Biosciences

\({}^{8}\)Helmholtz Center Hereon

\({}^{9}\)TU Vienna

\({}^{10}\)Purdue University

\({}^{11}\)Olden Labs

\({}^{12}\)NIH

\({}^{*,}\)Equal contribution

{artur.szalata, fabian.theis, malte.luecken}@helmholtz-munich.de;

{abenz, dburkhardt}@cellarity.com

###### Abstract

Single-cell transcriptomics has revolutionized our understanding of cellular heterogeneity and drug perturbation effects. However, its high cost and the vast chemical space of potential drugs present barriers to experimentally characterizing the effect of chemical perturbations in all the myriad cell types of the human body. To overcome these limitations, several groups have proposed using machine learning methods to directly predict the effect of chemical perturbations either across cell contexts or chemical space. However, advances in this field have been hindered by a lack of well-designed evaluation datasets and benchmarks. To drive innovation in perturbation modeling, the Open Problems Perturbation Prediction (OP3) benchmark introduces a framework for predicting the effects of small molecule perturbations on cell type-specific gene expression. OP3 leverages the Open Problems in Single-cell Analysis benchmarking infrastructure and is enabled by a new single-cell perturbation dataset, encompassing 146 compounds tested on human blood cells. The benchmark includes diverse data representations, evaluation metrics, and winning methods from our "Single-cell perturbation prediction: generalizing experimental interventions to unseen contexts" competition at NeurIPS 2023. We envision that the OP3 benchmark and competition will drive innovation in single-cell perturbation prediction by improving the accessibility, visibility, and feasibility of this challenge, thereby promoting the impact of machine learning in drug discovery.

## 1 Introduction

Examining gene expression in individual cells via single-cell RNA sequencing (scRNA-seq) provides high-resolution insights into cellular behavior within healthy and diseased tissue. One emerging application of single-cell technology is to profile cells under basal and perturbed states to characterize the changes in cellular states associated with chemical treatments and to associate these changes with healthy or pathological tissue phenotypes [1; 2; 3; 4; 5]. These technologies have the potential to transformhow drugs are discovered and bring new therapies to patients with unmet clinical needs [6; 7; 8]. Instead of focusing on single molecular targets for drug discovery, it is possible to analyze how compounds influence gene expression to shift cells from diseased to healthy states. This approach holds promise for treating complex diseases where single-target methods have been less effective, as it addresses the interplay of multiple genes and pathways within the cell.

However, associating small molecules with changes in cell state is challenging. One approach is to brute-force screen compounds and measure the associated changes in gene expression, as has been done to discover drug candidates for heart valve disorders . However, chemical space is vast. There are an estimated \(10^{60}\) drug-like molecules . Compounds can also have diverse impacts on gene expression across different tissues, cell types, and individuals. Moreover, scRNA-seq experiments are expensive and require highly-trained technicians to run. Hence, accurate prediction of the changes in gene expression induced by compounds across different chemical structures and biological contexts could provide immense time and cost savings.

Recently, machine learning methods to predict the impact on gene expression of small molecule perturbations directly from chemical structures have been proposed [11; 12; 13; 14]. However, understanding such models' effectiveness is difficult due to a lack of independent evaluations and limited availability of benchmarking datasets . Indeed, most existing datasets include only a single perturbation , a single donor, or are limited to homogeneous cancer cell lines [1; 17]. Although these studies represent important contributions to the field, a rigorous, standardized benchmark is needed to assess their performance in diverse cell types across a wide range of chemical perturbations.

Here, we introduce the Open Problems Perturbation Prediction (OP3) benchmark, which is the first standardized benchmark for predicting chemical perturbation effects across cell types. It includes a formalized task, an open-source benchmarking platform, and a new dataset profiling 146 chemical perturbations in human peripheral blood mononuclear cells (PBMCs) from three donors. We hosted a NeurIPS 2023 Competition using this benchmark, and used the learnings and proposed methods to improve the benchmark. OP3 provides a continuously updated, extensible benchmark for perturbation prediction, promoting translation of these methods to applied science.

## 2 Related work

This work builds on previous efforts to generate single-cell chemical perturbation datasets and evaluations performed alongside method development for perturbation prediction algorithms.

Chemical perturbation datasetsRecently, several large-scale datasets with drug perturbations have been published. The popular sci-Plex  dataset profiles 188 compounds in three cancer cell lines, and its recent sequel, the sci-Plex-GxE  dataset, profiled 22 drugs combinatorially in three cancer cell lines. While these datasets feature a large number of compounds, their use of cancer cell lines limits their applicability, as cancer cell lines have a number of significant deviations from human tissue. These datasets also use nuclei sequencing technologies which are less sensitive and have higher noise compared to whole-cell sequencing used in our study . In addition, a recent pre-print introduced a scRNA-seq dataset of drug-perturbed human PBMCs , but its lack of replicates makes it difficult to disentangle technical and biological noise from the drug perturbation signal. Finally, a harmonized collection of public single-cell perturbation datasets was recently published, but most datasets contain only a single cell type and few perturbations with overlap across datasets, making them unsuitable for our benchmarking task .

Perturbation prediction evaluationThe task of predicting the transcriptomic effects of small molecule perturbations in single-cell data has been tackled by a few machine learning models [13; 14; 12; 21]. However, the evaluations of these models did not include drug perturbations on primary tissue, used evaluation methods that are biased toward natural transcriptional variation , and lacked assessments of stability across replicates and batches. No independent method evaluations exist to our knowledge, which is essential to fairly compare algorithm performance .

A living benchmark for perturbation prediction

To drive innovation in algorithm development for single-cell perturbation analysis, we set up the OP3 benchmark, including a formalized task definition, a fit-for-purpose benchmarking dataset, and computational infrastructure to support continuously-updated, community-driven benchmarking (**Figure 0(a)**). We outline these features below.

### Task overview

Chemical perturbations induce cell type-specific gene expression changes by interacting with target proteins and altering cellular processes. For example, tamoxifenfen, a breast cancer drug, binds the estrogen receptor and inhibits cell growth, thereby acting selectively on cells expressing the estrogen receptor . However, the lack of knowledge about mechanisms of action for most compounds hinders predicting their effects on specific cell types.

The goal of this task is to leverage data about chemical perturbations in some cell types to infer their impact on gene expression in other cell types. The data is a tensor with three axes: compounds, cell types, and genes. Each value in this tensor is a measurement of the impact on gene expression observed in a specific cell type under a specific chemical perturbation (**Section 3.3**). Models are provided with the changes in gene expression for all cell types for a subset of compounds. The remaining compounds comprise the test set. These compounds have their differential expression values masked for all genes for a subset of the cell types. The target of this task is to predict these masked differential expression values (**Figure 0(b)**).

### Generating a single-cell perturbation benchmarking dataset

Considerations for data set generationWe identified the following properties of an ideal dataset for benchmarking small molecule perturbation prediction:

1. **Disease-relevance:** To reflect the downstream application to drug discovery, an ideal dataset ought to focus on a disease-relevant biological system.
2. **Balanced cellular heterogeneity:** Cell types must exhibit distinct perturbation responses but be similar enough that translating compounds' effects is tractable.
3. **Diverse perturbations:** The compounds should perturb a range of biochemical pathways.
4. **Replicates across multiple donors:** Capturing perturbation effects across multiple donors enables identifying effects that are preserved across diverse donors.
5. **Positive and negative controls**: Because of the high degree of technical and biological variability in gene expression measurements, positive and negative controls are essential to accurately estimate the variation attributable to perturbation effects.
6. **Open access & informed consent:** To ensure open access to benchmarking data collected from human donors, samples must be collected under IRB supervision. This ensures donors give informed consent for public sharing of any derived data.

Dataset overviewWe generated a novel scRNA-seq dataset profiling 146 compounds in PBMCs to provide a high-quality reference benchmark dataset for single-cell perturbation prediction (**Figure 0(c)**). We also included multiome single-nucleus RNA and chromatin accessibility measurements at baseline to facilitate gene regulatory network inference. This effort represents, to date, the largest drug perturbation dataset on primary human tissue with donor replicates , and was specifically designed to satisfy all the criteria above. First, PBMCs comprise an important subset of the human immune system and play a key role in various pathologies, including cancer, autoimmune diseases, immunodeficiencies, and allergies. PBMCs also contain discrete cell types (including T-cells, B-cells, myeloid cells, and NK cells) that perform distinct biological functions while sharing key biological pathways, making perturbation prediction in PBMCs difficult yet tractable. The compounds in this dataset were selected to span a wide range of mechanisms of action. Additionally, two positive control compounds that were known to induce a strong transcriptional signature in PBMCs were included. Every perturbation was repeated in three healthy human donors, two male and one female. Finally, we performed this experiment using PBMCs that were commercially available with pre-obtained consent for public release.

Data generation, processing and cell type annotationPBMCs were cultured in six separate 96-well plates, two for each donor (**Figure 1c**). After the cells were treated with compounds for 24 hours, samples were collected, pooled to reduce batch effects and increase throughput, and sequenced. Sequencing reads were processed using the Cell Ranger pipeline , and a best-practice pipeline was followed to QC, normalize, reduce, and cluster the data . We assigned each cluster to one of four cell type labels (B cell, T cell, NK cell, or myeloid cell) using established marker genes. **Figure 1d** shows the UMAP  visualization of the dataset with cell type and donor annotations.

The baseline multiome data (joint snRNA-seq/scATAC-seq) was processed by filtering out low-quality cells, along with both genes and chromatin accessibility features with low counts. Cells in this multiome data were then annotated based on marker gene expression in the same manner as the

Figure 1: **Overview of the dataset.****(a)** A overview of the Open Problems living benchmarking framework. **(b)** A graphical description of the perturbation prediction task. **(c)** The experimental setup for our benchmarking dataset. **(d)** UMAP representations of the resulting single-cell profiles colored by cell type (top) and donor (bottom).

perturbational scRNA-seq data. For a detailed description of the experiment and analysis for both perturbational and baseline multiome data, please refer to **Appendix A**.

### Representation of perturbation effects

In genomics, differential expression (DE) analysis is commonly used to identify how compounds affect gene activity in different cell types . DE methods estimate perturbation effects by fitting generalized linear models to observed count data, explicitly accounting for biological and technical covariates. In this study, we performed DE analysis using the limma-voom framework , which provides estimates of effect size (e.g., log-fold change) and statistical significance while adjusting for variability associated with technical covariates.

Although using estimates of effect size or significance is standard in the genomics community, it is more common in machine learning benchmarks to directly predict a conditional distribution, such as the gene expression counts. To test whether the effect size (log-fold change), significance (\(p\)-values), or conditional counts are more suitable for benchmarking, we evaluated each of these representations using the replicates across donors in our dataset. We determined that an optimal representation would minimize the distance between observations of the same compound across donors, with lower median distance ranks indicating better identifiability of compounds across donors. We call this heuristic _cross-donor retrieval_ (**Appendix C.1**).

We found that the measures of effect significance had better cross-donor retrieval (**Figure 2a** and **Appendix Figure 5**) than effect size or counts data, and this effect was consistent across cell types (**Figure 2b**). Based on these results, we decided on the following representation as a target for our benchmark: for a given compound \(c\), cell type \(t\), and gene \(g\), let \(p_{c,t,g}\) and \(L_{c,t,g}\) be the \(p\)-value and log-fold change computed by limma, respectively. Then

\[_{c,t,g}=-_{10}(p_{c,t,g})(L_{c,t,g}).\] (1)

This representation captures both the direction and statistical significance of the perturbation effect on each gene. We do not claim that this representation is universally optimal for all tasks and analyses and note there are several challenges associated with DE analysis generally (**Appendix D**).

### Evaluation metrics

We considered three metrics for evaluating model performance: mean row-wise root mean squared error (MRRMSE), mean absolute error (MAE), and cosine similarity. Mean row-wise indicates that we take a mean across predictions for compound-cell type pairs. Each of these metrics is related to the distance metrics used in the cross-donor retrieval task, e.g. MAE is effectively a rescaled

Figure 2: **Cross-donor retrieval analysis. (a) For each pair of donors, for each compound in each cell type, the cross-donor retrieval rank was calculated using various distance metrics. The y-axis shows the retrieval rank (i.e., the rank of the same compound and cell type measurement in a different donor). The x-axis separates different retrieval distance metrics. Note that L1 distance is effectively a rescaled MAE, and L2 distance is effectively a rescaled RMSE. The hue differentiates box plots for different data representations according to the legend on the right. (b) We further examined the cross-donor retrieval rank per cell type using the L2-distance metric to ensure the results were consistent across cell types.**

L1 distance, and MRRMSE is effectively a rescaled L2 distance. Using these relationships, we concluded that cosine similarity had the best stability across donors, followed by MRRMSE and MAE (**Figure 1(a)**). However, not all perturbations are expected to cause a change in gene expression, and cosine similarity would not penalize models that incorrectly predict low \(p\)-values in such cases, unlike MRRMSE and MAE. Hence, we primarily rely on MRRMSE for model evaluation, defined as:

\[=_{i=1}^{R}(_{j=1}^{n}(y_{ij} -_{ij})^{2})^{1/2}\] (2)

Where \(R\) is the number of (cell type, compound) tuples, and \(y_{ij}\) and \(_{ij}\) are the actual and predicted values, respectively, and \(n\) is the number of genes.

### Control methods

Including control methods in each benchmarking task is one of the basic quality controls required by Open Problems not only to verify the integrity of the benchmarking workflow but to also normalize the metric outputs. In this benchmark, we implemented six control methods, where each returns either a solution derived from the ground truth data (positive control), a naive baseline prediction, or a randomly sampled prediction (negative control). The positive and negative control methods define an upper and lower bound for the performance metrics, which is used to normalize metric outputs. Full descriptions of the control methods can be found in **Appendix E.2.4**.

## 4 The Single-cell Perturbation Prediction Competition at NeurIPS 2023

To identify the state-of-the-art for perturbation prediction in unseen cell types, we hosted a Kaggle competition as part of the NeurIPS 2023 Competitions track called _Single-cell perturbation prediction: generalizing experimental interventions to unseen contexts_. This competition ran from September 12, 2023 through November 30, 2023 and used an earlier version of the dataset and benchmark before it was updated based on learnings from the competition (**Appendix B**). We ran the competition in two tracks. The Leaderboard Track followed the traditional data science competition setup with a public and private leaderboard tracking a single metric on public and private test sets (**Appendix B**). We also ran a Judges' Prize track where participants were judged based on a write-up addressing specific questions about perturbation prediction and the specific challenges of using our dataset to tackle this task. $50,000 in prizes were awarded for each track. The competition web page with the final leaderboard, code submissions, and discussions is available at: https://www.kaggle.com/competitions/open-problems-single-cell-perturbations

### Leaderboard Track

In the leaderboard competition, competitors trained models on the training set and submitted CSV files with predictions for the public and private test tests. During the development phase (3 months), only the results from the public test set were used to calculate leaderboard rankings. During the final phase (5 days), competitors selected their top submission. Final scores were judged on the private test set only visible after the final submission deadline. Due to the limitations of the Kaggle platform, we ran the competition with a single metric, MRRMSE, decided on in collaboration with Kaggle data scientists. The participants were encouraged to use any publicly available external data.

Over the competition, 1,318 participants from 84 countries, forming 1,097 teams, submitted 25,529 solutions to our Leaderboard Track. This makes our competition one of the largest single-cell data science competitions to date. Although participants were only required to submit CSV predictions, the Kaggle platform has a strong culture of solution sharing. As such, we were able to read through reported submission code and identified trends among the best performing methods. We found that the top-scoring methods relied on diverse deep learning approaches, including transformer, LSTM, GRU, CNN and MLP architectures. The models used diverse loss functions, such as mean squared error, mean absolute error, LogCosh (\(L(y,)=_{i=1}^{N}((y_{i}-_{i}))\)), binary cross-entropy, MRRMSE, and Huber loss . Despite several reported attempts, only the first of the three top-performing models relied on data other than the training set. The winning method used ChemBERTa , a pre-trained transformer, to encode the small molecule structure SMILES representation. According to the competitors' reports, data preprocessing proved to be very impactful. In particular, multiple competitors reported that target encoding and singular value decomposition of the high-dimensional input data were effective. One method used pseudolabels  for model training. All of the top three methods relied on model ensembles. We provide detailed descriptions of these methods in **Appendix E.1**.

### Judges' Prize

In the Judges' Prize, participants were asked to address how biological priors or alternative model architectures influence leaderboard performance, to describe technical challenges that make perturbation prediction difficult, to characterize how data noise or downsampling affect model robustness, and to present well-documented and packaged model code. To identify winners, the write-ups were scored by a panel of single-cell experts. 17 teams submitted write-ups for a judges' prize, all of whom also participated in the leaderboard prize.

Many of the submissions provided valuable insights and were exceptionally detailed--the top-scoring team wrote a 33-page report. For example, several participants mentioned their efforts on integrating gene regulatory networks (GRN) inferred from ATAC and RNA data as an extra modality for prediction task [32; 33]. Although distinct patterns among cell types were observed from the provided ATAC-seq data, attempts at incorporating inferred GRNs in model predictions, even only for expression-enriched regulators, resulted in performance decreases in their models. Other groups attempted to use molecular interactions as an additional modality for model design. For example, GSEA-MsigDB  provides valuable information about pathways activated in various cell types. From these, a correlation network can be constructed based on shared pathways or shared regulation target genes. However, the models overall did not benefit from these efforts, which suggests that further filtering over inferred regulation/correlation relationships might be necessary. Finally, many submissions also investigated challenges associated with data representations and data pre-processing, which are described in the following section. We provide detailed descriptions of the Judges' Prize-winning methods in **Appendix E.2**.

### Lessons learned

Here, we list several key learnings and opportunities to improve our benchmarking setup.

**False positives for unexpressed/lowly expressed genes**: DE analysis is sensitive to low-count genes, which can lead to overestimation of relative expression changes. This is especially problematic for compounds with subtle gene effects. To mitigate this, we employed a stricter gene filtering strategy per cell type , resulting in a reduced 5,317 genes (originally 18,211).

**Inconsistent annotations**: Proportions of T cell subtypes were inconsistent across donors (**Appendix Table 3**, **Appendix Figure 7**). These subtypes had low cell counts and subtle differences in expression that suggested misannotation, which may have been caused by perturbation impacts on marker gene expression. To resolve this, we grouped all of the T cells together in the final annotations **Figure 0(b)**.

**Outlier samples**: Certain samples had very few cells, which may be caused by perturbation-associated toxicity and was correlated with a high fraction of low \(p\)-values. To address this, we removed samples with \(<10\) cells or inconsistent cell type proportions across donors. We also removed three compounds for which we could not confidently annotate cell types (**Appendix F.2**), likely due to toxicity.

**Design matrix**: Due to a high number of factors and collinearity, the design matrix used in the competition (**Appendix Figure 6**) was not full-rank, potentially leading to parameter estimation issues. We updated the linear model to \(f(g_{j})=x_{1}cc_{i}+x_{2}p_{i}\), where \(g_{j}\) is a gene, \(cc_{i}\) is (cell type, compound) tuple, and \(p_{i}\) is the plate. The resulting design matrix is full rank.

**Outlier \(p\)-values**: Our dataset contained some very low \(p\)-values (1e-180). As we do not want to penalize models for not differentiating between very small \(p\)-values, we clipped \(p\)-values in the dataset at 1e-4.

**Submit algorithms, not predictions**: Even though the competition participants submitted methods implementations, we were unable to exactly reproduce all of the results. We recommend requiring competitors to submit algorithms instead of predictions to promote the development of reusable tools. In addition, it allows algorithms to be more easily adapted, ultimately accelerating scientific discovery.

### Updating the living benchmark

A central challenge in machine learning competitions is translating state-of-the-art methods according to competition leaderboards to impact applied science. A review of 10 years of machine learning competitions in dementia  found that no competition winners had been applied in clinical settings, suggested that winning methods may be overfitted to the competition dataset and metric, and suggested making methods available for testing in other settings. To enable further testing and evaluation of top-performing methods from our competition, we implemented and retrained the top 3 methods according to the leaderboard and the top 3 according to judges' scores in our Open Problems Perturbation Prediction living benchmark. This final benchmark includes the changes listed in the preceding section. Additionally, the public test set is now part of the training set. The results are shown in **Figure 3** and the latest results of the living benchmark are available at https://openproblems.bio/results/perturbation_prediction.

Examining model performance across compounds, we observed that for all 6 methods, the error residuals correlated with the number of DE genes. This indicates that the methods are better at predicting no change in gene expression than a significant change. Indeed, the top performing method, NN retraining with pseudolabels, predicts high \(p\)-values more often than they occur in the dataset (Appendix Figure **Figure 8**).

Figure 3: **An overview of the benchmarking results** of the six selected methods and one control method. Methods are ordered by the arithmetic mean of the three metrics. The MR Cosine, MR MAE, and MR RMSE were computed by comparing a method’s predictions to the ground-truth data. Each of these metric values were min-max scaled between the positive control and random sample. The resources column group shows the resource usage of the various methods throughout their execution.

Discussion

In this study, we presented a living benchmark for single-cell perturbation prediction. The Open Problems Perturbation Prediction (OP3) benchmark features a newly generated fit-for-purpose dataset that is the largest of its kind, optimized data representations and metrics, positive and negative baseline methods that define performance ranges, and a cloud-based infrastructure that enables users to add new methods, metrics, and datasets to the benchmark. Using this benchmarking setup we ran the Single-cell Perturbation Prediction competition at NeurIPS 2023, in which over 1,300 participants contributed over 25,000 method solutions to address the challenge of predicting perturbation responses across drugs and cell types. This competition successfully made the topic of single-cell perturbation prediction accessible to a non-specialist community (more than half of the surveyed participants never worked with single-cell data **Appendix F.1**), while leveraging the expertise from this community to improve upon current state-of-the-art methods (via Leaderboard Track winners) and provide feedback on the task definition and implementation (via Judges' Prize winners). To promote the translation of competition outputs to domain impact in perturbation prediction, we used this competitor feedback to update the OP3 benchmark and populated it with the top-performing solutions from the competition. This enables methods to be further scrutinized by the community on the generalizability of their performance across data contexts and metrics.

To power our single-cell perturbation prediction competition and benchmark, we generated the largest multi-donor single-cell drug perturbation dataset on primary human tissue. However, despite profiling 146 drug perturbations in over 300,000 cells, the training data size is still limited from the perspective of building models that generalize across drugs, donors, and cell types. There are over 16,600 clinical-stage drugs , which typically elicit heterogeneous responses across cell types  and individuals . Predicting the cell-type-specific response of a drug on an unseen individual will likely require data generation efforts that are not feasible by individual groups, but rather coordinated across consortia. Such efforts would also be needed to ensure aspects such as differing drug efficacy across genetic backgrounds [40; 41] can be taken into account, which is not feasible with existing perturbation datasets that often only profile cells from a single genotype . In this context, our OP3 benchmark and dataset represent a first step towards this larger goal.

A further limitation of our competition, and indeed most other Kaggle competitions, derives from the use of a single performance metric, which is a limitation of the Kaggle platform. Goodhart's law suggests that when a performance metric becomes the optimization target, the metric ceases to be a good metric [42; 43]. This phenomenon is especially challenging when the chosen metric represents a proxy for good performance that is easy to evaluate during a model development loop (i.e. is differentiable and quickly calculable). In our case, perturbation prediction would ideally assess how well an unseen candidate drug treats a disease of unknown pathology in a particular patient. To make this tractable, we instead evaluate the transcriptome response in an unseen hold-out donor, cell type, and drug combination. A mitigation strategy for overfitting to this setting is to define additional relevant tasks related to perturbation prediction to evaluate method performance on different criteria. To promote innovation towards the overall goal of improving perturbation prediction, we specifically enable such a multi-task evaluation setup via the OP3 living benchmark and the design of our dataset. To promote generalizability of developed solutions , future competitions in this direction may further include orthogonal readouts, such as cell type proportions, rates of cell death, or inflammation .

Taken together, the OP3 benchmark and corresponding competition represent the first community-extensible standard for predicting perturbation responses from single-cell transcriptomic data. While several algorithms existed for this task also prior to our competition, the competition has been successful in greatly expanding the set of possible solutions available, which can be further scrutinized via the OP3 living benchmark. Indeed, the combination of a large-scale competition and a cloud-based living benchmark represents a promising approach to promoting innovation towards critical domain-specific challenges. We envision that the OP3 benchmark will lay the groundwork for further method development for this question, which is of critical importance to realize the promise of personalized medicine and optimized drug discovery.