# Navigating Extremes:

Dynamic Sparsity in Large Output Spaces

 Nasib Ullah1 Erik Schultheis1 Mike Lashy2 Yani Ioannou2 Rohit Babbar1,3

1Department of Computer Science, Aalto University, Helsinki, Finland

{nasibullah.nasibullah, erik.schultheis, rohit.babbar}@aalto.fi
2Schulich School of Engineering, University of Calgary, Calgary, AB, Canada

{mklasby, yani.ioannou}@ucalgary.ca
3Department of Computer Science, University of Bath, Bath, UK

rb2608@bath.ac.uk

###### Abstract

In recent years, Dynamic Sparse Training (DST) has emerged as an alternative to post-training pruning for generating efficient models. In principle, DST allows for a more memory efficient training process, as it maintains sparsity throughout the entire training run. However, current DST implementations fail to capitalize on this in practice. Because sparse matrix multiplication is much less efficient than dense matrix multiplication on GPUs, most implementations simulate sparsity by masking weights. In this paper, we leverage recent advances in semi-structured sparse training to apply DST in the domain of classification with large output spaces, where memory-efficiency is paramount. With a label space of possibly millions of candidates, the classification layer alone will consume several gigabytes of memory. Switching from a dense to a fixed fan-in sparse layer updated with sparse evolutionary training (SET); however, severely hampers training convergence, especially at the largest label spaces. We find that poor gradient flow from the sparse classifier to the dense text encoder make it difficult to learn good input representations. By employing an intermediate layer or adding an auxiliary training objective, we recover most of the generalisation performance of the dense model. Overall, we demonstrate the applicability and practical benefits of DST in a challenging domain -- characterized by a highly skewed label distribution that differs substantially from typical DST benchmark datasets -- which enables end-to-end training with millions of labels on commodity hardware.

## 1 Introduction

Recent research [1; 2; 3; 4] has demonstrated that densely-connected neural networks contain sparse subnetworks -- often dubbed "winning lottery tickets" -- that can deliver performance comparable to the full networks but with substantially reduced compute and memory demands. Unlike conventional techniques that start with a trained dense model and employ iterative pruning or one-shot pruning, Dynamic Sparse Training (DST) [5; 6; 7] initializes a sparse architecture and dynamically explores subnetwork configurations through periodic pruning and regrowth, typically informed with heuristic _saliency_ criteria such as weight and gradient magnitudes. This approach is particularly advantageous in scenarios constrained by a fixed memory budget during the training phase, making DST viable across various domains [4; 8; 9]. For instance, in reinforcement learning [10; 11], DST has been shown to significantly outperform traditional dense models. Additionally, models trained using DST often exhibit enhanced robustness [12; 13; 14; 15; 16]. However, the application of DST comes with challenges, notably prolonged training times; for example, RigL  and ITOP  require up to five and two times as many optimization steps during training, respectively, to match the generalisationperformance of dense networks at high sparsity levels (\( 80\%\)). The prolonged training time in these works is often linked to the need for _in-time overparameterization_ and poor gradient flow in sparse networks. Recent advances  aimed at improving gradient flow have been introduced to mitigate these extended training durations, enhancing the practicality of DST methodologies.

In this paper, we investigate the integration of DST into _extreme multi-label classification_ (XMC) . XMC problems are characterized by a very large label space, with hundreds of thousands to millions of labels, often in the same order of magnitude as the number of training examples. The large label space in such problems makes calculating logits for every label a very costly operation. Consequently, contemporary XMC methodologies  utilize modular and sampling-based techniques to achieve sublinear compute costs. However, these strategies do not help in addressing the immense memory requirement associated with the classification layer, which can be enormous: for an embedding dimension of 768, one million labels lead to a memory consumption of about 12 GB taking into account weights, gradients, and optimizer state.1 Memory efficiency in XMC has been pursued in the context of sparse _linear_ models  or by using label-hashing , but such methods do not yield predictive performance competitive with modern transformer-based deep networks. Schultheis and Babbat  demonstrated that applying a DST method to the extreme classification layer can lead to substantial memory savings at marginal accuracy drops; however, that work presupposed the existence of fixed, well-trained document embeddings which output the hidden representations used by the classifier, whereas in a realistic setting these need to be trained jointly.

Recently, Jain et al.  demonstrated that full end-to-end training of XMC models can be very successful, given sufficient computational resources. To make this accessible to consumer-grade hardware, we propose to switch the dense classification layer to a DST-trained sparse layer. Not only does this result in a training procedure that allows XMC models to be trained in a GPU-memory constrained setting, but it also provides an evaluation of DST algorithms outside typical, well-behaved benchmarks. This is particularly important since recent works  have found that sparse training algorithms that appear promising on standard benchmark datasets may fail to produce adequate results on actual real-world tasks. As such, we introduce XMC problems -- with their long-tailed label distribution , missing labels , and general training data scarcity issues  -- as a new setting to challenge current sparsity approaches.

In fact, direct application of existing DST methods yields unsatisfactory results on XMC tasks due to typically noisy data and poor gradient signal propagation through the sparse classifier, slowing training convergence to an extent that it is not practically useful. Consequently, we follow Schultheis and Babbat  and adapt the model architecture by integrating an intermediate layer that is larger than the

Figure 1: Model configurations and performance comparisons at various sparsity levels. The left panel illustrates our model configurations: ‘S’ represents a semi-structured fixed fan-in sparse layer, ‘W’ denotes an intermediate layer, and ‘Aux’ refers to an auxiliary head of meta-classifiers. These configurations help maintain performance as the label space size increases from 31K to 670K and beyond. The right panel demonstrates the comparative precision at 1 for our model against other methods across increasing levels of sparsity on the Amazon670K dataset.

embedding from the encoder but still significantly smaller than the final output layer. While this was found to be sufficient to achieve good results with fixed encodings, it fails if the encoder is a trainable transformer [45; 46] for label spaces with more than one hundred thousand elements, particularly at high levels of sparsity. The primary challenge arises from the noisy gradients prevalent at the onset of training, which are inadequate for guiding the fine-tuning of the encoder effectively. To mitigate this issue, we introduce an auxiliary loss. This loss uses a more coarse-grained objective, assigning instances to _clusters_ of labels, where scores for each cluster are calculated using a dense classification layer. This auxiliary component stabilizes the gradient flow and enhances the encoder's adaptability during the critical early phases of training and is turned off during later epochs to not interfere with the main task. Figure 1 illustrates the architectural changes that ensure good training performance at different label space sizes and sparsity levels.

To materialize actual memory savings, we propose Spartex, which uses semi-structured sparsity [47; 35] with a fixed fan-in constraint, together with magnitude-based pruning and random regrowth (SET ), which does not require any additional memory buffers. In our experiments, we show that Spartex achieves a 3.4-fold reduction of GPU memory requirements from 46.3 to \(13.5\) for training on the Amazon-3M  dataset, with only an approximately 3% reduction in predictive performance. In comparison, a naive parameter reduction using a bottleneck layer at the same memory budget decreases precision by about 6%.

Our primary contributions are as follows:

* **Enhancements in training efficiency:** We propose novel modifications to the conventional DST framework that significantly curtail training durations while delivering competitive performance metrics when benchmarked against dense model baselines and other specialized XMC methodologies. These enhancements are pivotal in demonstrating DST's scalability and efficiency to large label spaces.
* **Optimized hardware utilization:** We provide PyTorch bindings for custom CUDA kernels2 which enable a streamlined integration of memory-efficient sparse training into an existing XMC pipeline. This implementation enables the deployment of our training methodologies on conventional, commercially available hardware, thus democratizing access to state-of-the-art XMC model training. * **Robustness to Label distribution challenges:** Our empirical results demonstrate that the DST framework, as adapted and optimized by our modifications, can effectively manage datasets characterized by label imbalances and the presence of missing labels, with minimal performance degradation for tail labels.

## 2 Dynamic Sparse Training for Extreme Multi-label Classification

### Background

Problem setupGiven a multi-label training dataset with \(N\) samples, \(=\{(x_{i},P_{i})_{i=1}^{N}\}\), where \(L\) represents the total number of labels, and \(P_{i}[L]\) denotes a subset of relevant labels associated with the data point \(x_{i}\). Typically, the instances are text based, such as the contents of a Wikipedia article  or the title of a product on Amazon  with labels corresponding to Wikipedia categories and frequently bought together products, respectively, for example. Traditional XMC methods used to handle labels the same way as is typically done in other fields, as featureless integers.

However, the labels themselves usually carry some information, e.g., a textual representation, as the following examples, taken from (i) LF-AmazonTitles-131K (recommend related products given a product name) and (ii) LF-WikiTitles-500K (predict relevant categories, given the title of a Wikipedia page) illustrate:

_Example 1:_ For _"Nintendo Land"_ on Amazon, we have available: _Mario Tennis Ultra Smash(Nintendo Wii U) \(|\) Star Fox Zero (Nintendo Wii U)_, as the recommended products.

_Example 2:_ For the _"2024 United States presidential election"_ Wikipedia page, we have the available categories: _Joe Biden \(|\) Joe Biden 2024 presidential campaign \(|\) Donald Trump \(|\) Donald Trump 2024 presidential campaign \(|\) Kamala Harris \(|\) November 2024 events in the United States._Consequently, more recent XMC approaches have started to take these label features into account to alleviate the data scarcity problems [27; 49].

Xmc and DSTXMC models are typically comprised of two main components: (i) an encoder \(_{}\) : \(^{d}\), which embeds data points into a \(d\)-dimensional real space, primarily utilizing a transformer architecture  and (ii) A One-vs-All classifier \(W\!=\!\{w_{l}\}_{l[L]}\), where \(w_{l}\) denotes the classifier for label \(l\), integrated as the last layer of the neural network in end-to-end training settings. In a typical DST scenario, one would sparsify the language model used as the encoder, potentially even leaving the classifier fully dense . However, in XMC, most of the networks weights are in the classifier layer, so in order to achieve a reduction in memory consumption, its weight matrix \(W_{s}\!=\!\{w_{l}^{s}\}_{l[L]}\)_must_ be sparsified.

This sparse layer \(W_{s}\) is then periodically updated in a prune-regrow-train loop, that is, every \( T\) steps, a fraction of active weights is pruned and the same number of inactive weights are regrown. The updated sparse topology is then trained with regular gradient descent for the next \( T\) steps. There are many possible choices for pruning and regrowth criteria ; to keep memory consumption low, however, we need to choose a method that does not require auxiliary buffers proportional to the size of the dense layer. This excludes methods such as requiring second-order information , or tracking of dense gradients or other per-weight information [54; 55; 56]. Evci et al.  argue that RigL only needs dense gradients in an ephemeral capacity -- they can be discarded as soon as the regrowth step for the current layer is done, but before the regrow step of the next layer is started -- but in the XMC setup, the prohibitively large memory consumption arises already from a single layer. Therefore, we select magnitude-based pruning and random regrowth . Magnitude-based pruning has been shown to be a remarkably strong baseline .

However, to actually achieve efficient training with these algorithms in the XMC setting, several challenges need to be overcome as discussed below.

### Memory-Efficient Training: Fixed Fan-In Sparse Layer

Unstructured sparsity is notoriously difficult to speed-up on GPUs , and consequently most DST studies simulate sparsity by means of a binary mask [19; 59]. On the other hand, highly structured sparsity, such as 2:4 sparsity , enjoys hardware acceleration and memory reduction , but may result in deteriorated model accuracy compared to unstructured sparsity . As a compromise, semi-structured sparsity [63; 47; 35] imposes a fixed fan-in to each neuron. This eliminates work imbalances between different neurons, leading to an efficient and simple storage format for sparse weights, where each sparse weight needs only a single integer index, resulting in ELLPACK format  without any padding. For 32-bit floating point weights with 16-bit indices (i.e., at most 65k features in the embedding layer), this leads to a 50% storage overhead for sparse weights; however, for training, gradient and two momentum terms are needed, which share the same indexing structure, reducing the effective overhead to just 12.5%.

While fixed fan-in provides substantial speed-ups for the forward pass, due to the transposed matrix multiplication required for gradients, it does not give any direct benefits for the backwards pass. Fortunately, when used for the classification matrix, the backpropagated gradient is the loss derivative, which will exhibit high _activation sparsity_ if the loss function is hinge-like . In the enormous label space of XMC, for each instance only a small subset of labels will be hard negatives. The rest will be easily classified as true negatives, and not contribute to the backward pass.

As additional measures to keep the memory consumption low, we enable torch.amp automatic mixed-precision training  and activation checkpointing  for the BERT encoder.

### Improved Gradient Flow: Auxiliary Objective

We find that, despite using a fully dense network, training the encoder using gradients backpropagated from a sparse classification layer requires more optimization steps to converge compared with to a dense classification layer. This compounds with the already-increased number of epochs required for DST [6; 7], further increasing the training duration of end-to-end XMC training, which requires longer training than comparable modularized or shortlisting-based methods . Furthermore, the intermediate activations in the transformer-based encoder also take up a considerable amount of GPU memory, so to meet memory budgets, we may need to switch to smaller batch sizes or employ activation checkpointing, increasing the per-step time.

Therefore, we need to improve gradient flow through the sparse layer. Schultheis and Babbar  inserted a large intermediate layer preceding the actual classifier to achieve significant improvements in performance. While this method is sufficient to achieve good results with fixed encodings, we observe that it fails to perform well if the encoder is a trainable transformer [45; 46] for label spaces with more than one hundred thousand elements, particularly for high sparsity levels. Therefore, we instead propose to use the label shortlisting task that is typical for XMC pipelines as an auxiliary objective to generate informative gradients for the encoder.

Rethinking the role of clusters and meta-classifier in XMCMany prevailing XMC methods, apart from learning the per-label classifiers \(W=\{w_{l}\}_{l[L]}\) for the _extreme task_, also employ a meta-classifier. The meta-classifier learns to classify over clusters of similar labels that are created by recursively partitioning the label set into equal parts using, for example, balanced \(k\)-means clustering [24; 26; 29; 25; 67]. These meta-classifiers are primarily used for label shortlisting or retrieval prior to the final classification or re-ranking at the extreme scale. We investigated the impact on the final performance of the extreme task when the labels are randomly assigned to the clusters (instead of the following the k-means objective). We observed that such reassignments do not negatively affect the extreme task's performance (detailed of this observation are shown in Appendix E). This leads us to hypothesize that beyond merely shortlisting labels, meta-classifier branch of the XMC training pipelines provides useful gradient signals during encoder training, which is particularly crucial for larger datasets with \((10^{6})\) labels such as Amazon-670K (Figure 2) and Amazon-3M.

Auxiliary objective and DST convergence for XMCTowards addressing the challenge of gradient instability, we augment our training pipeline with an additional meta-classifier branch which aids gradient information during back-propagation. This is especially useful during the initial training phase where the fixed-fan-in sparse layer tends to encounter difficulties. Importantly, in our model the output layer operates independently of the meta-classifier's outputs, enabling a seamless end-to-end training process.

Although a meta-classifier assists during the initial stages of training, maintaining it throughout the entire training process can deteriorate the encoder's representation quality. This degradation occurs because the task associated with the meta classifiers differs from the final task, yet both share the same encoder. Similar observations have been noted in related studies . To address this issue, we implement a decaying scaling factor on the auxiliary loss, gradually reducing its influence as training progresses.

The impact of the auxiliary branch on the norm of the gradient in the encoder is demonstrated in Figure 2 for the Amazon-670K dataset. The larger gradient signal speeds up initial learning, but it is misaligned with the true objective, so is gradually turned off at around 200k steps. Furthermore, the improvement in prediction performance, as reflected in Figure 2 (right panel), reinforces the quality of gradient as compared to the training pipeline without the auxiliary objective.

## 3 Experiments and discussion

### Datasets

In this study, we evaluate our proposed modifications of DST under the extreme classification setting across a diverse set of large-scale datasets, including Wiki10-31K , Wiki-500K , Amazon-670K , and Amazon-3M . The datasets are publicly available at the Extreme Classification Repository3. These were selected due to their inherent complexity and the challenges posed by their long-tailed label distributions, which are emblematic of real-world data scenarios and test the robustness of DST methodologies. Further validation of our approach is conducted using the

Figure 2: Gradient Flow of the encoder during training with and without Auxiliary Objective.

datasets LF-AmazonTitles-131K and LF-WikiSeeAlso-320K. These datasets are particularly relevant due to their augmentation with rich label metadata and their concise text formats, traits that have gained considerable traction in the XMC research community of late. The datasets' detailed statistical profiles are delineated in Table 1.

### Baselines and evaluation metrics

To ensure a comprehensive and fair evaluation of our proposed DST methodologies applied to XMC problems, we compare our proposed framework, Sparse, across three principal categories of baseline methods:

1. **Dense Models:** Consistent with traditional DST evaluations, we compare the performance of our sparse models against their dense counterparts.
2. **Dense Models with bottleneck layer:** This category (referred to as Dense BN in Table 2) includes dense models with the same number of parameters as our proposed DST method by having a bottleneck layer with the same dimensionality as the FFI size. This ensures that comparisons focus on the impact of sparsity rather than differences in model size or capacity.
3. **XMC Methods:** For datasets devoid of label features, we benchmark against the latest transformer-based models such as CascadeXML , LightXML , and XR-Transformer. For datasets that incorporate label features, our comparison includes leading Siamese methods like SiameseXML  and NGAME, as well as other relevant transformer-based approaches.

Notably, Renee  qualifies as both a dense model and a state-of-the-art XMC method. However, in some instances, Renee employs larger encoders (e.g., Roberta-Large ). To maintain consistency and fairness in our evaluations, we exclude configurations employing larger encoders from this analysis. For conceptual validation, we used RiGL on datasets with label spaces up to 670K.

As is standard in XMC literature, we compare the methods on metrics which only consider prediction at top-k slots. This includes : Precision@k and its propensity-scored variant (which is more sensitive to performance on tail-labels). The details of these metrics are given in Appendix A.

### Empirical performance

Table 2 presents our primary results on the datasets, compared with the aforementioned baselines. The performance metrics for XMC baselines are reported from their original papers. However, for peak memory consumption, we re-ran these baselines in half precision with the same batch size, as all baselines are also evaluated in half precision. Following DST protocols, we extended the training duration for RigL, Dense Bottleneck, and our method to twice the number of steps used for dense models. Certain baselines (referred to as OOM) could not scale to the Amazon-3M dataset. Our results demonstrate that our method significantly reduces memory usage while maintaining competitive performance. On the Amazon-3M dataset, our approach delivers comparable performance to dense

  
**Dataset** & \(N\) & \(L\) & \(N^{}\) & \(\) & \(\) \\   \\  Wiki10-31K & \(14,\!146\) & \(30,\!938\) & \(6616\) & \(18.64\) & \(48.52\) \\ Wiki-500K & \(1,\!779,\!881\) & \(501,\!070\) & \(769,\!421\) & \(4.75\) & \(16.86\) \\ Amazon-670K & \(490,\!449\) & \(670,\!091\) & \(153,\!025\) & \(5.45\) & \(3.99\) \\ Amazon-3M & \(1,\!717,\!899\) & \(2,\!812,\!281\) & \(742,\!507\) & \(36.17\) & \(31.64\) \\   \\  LF-AmazonTitles-131K & \(294,\!805\) & \(131,\!073\) & \(134,\!835\) & \(5.15\) & \(2.29\) \\ LF-WikiSeeAlso-320K & \(693,\!082\) & \(312,\!330\) & \(177,\!515\) & \(4.67\) & \(2.11\) \\   

Table 1: Statistics of XMC Datasets with and without Label Features. This table presents a comparison across various datasets, detailing the total number of training instances (\(N\)), unique labels (\(L\)), number of test instances (\(N^{}\)), average label count per instance (\(\)), and average data points per label (\(\)).

models while achieving a 3.4-fold reduction in memory usage and a 5-fold reduction compared to the XMC baseline. Furthermore, within the memory-efficient model regime, our method consistently outperforms the Dense Bottleneck model. To further validate the robustness of our approach, we evaluated it on the label features datasets, as shown in Table 3. Notably, as the label space size increases, we need to adjust to a comparatively lower sparsity to maintain performance, discussed in detail in subsequent sections.

### Adapting to increased sparsity and label size: the role of auxiliary objective

The DST approach is widely recognized to be problematic when dealing with high sparsity levels (\( 90\%\)). This is also apparent in our experiment and can be observed in Figure 3 (right) when the label space size is constant. Our findings indicate that incorporating an auxiliary objective significantly aids

    & Sparsity &  &  &  & \(M_{}\) & Sparsity &  &  &  & \(M_{}\) \\  & & & & & \(()\) & & & & & \((GiB)\) \\    & &  &  \\  AttentionXML & - & 87.1 & 77.8 & 68.8 & 8.2 & - & 75.1 & 56.5 & 44.4 & 13.1 \\ LightXML & - & 87.8 & 77.3 & 68.0 & 16.5 & - & 76.2 & 57.2 & 44.1 & 14.6 \\ CascadeXML & - & 88.4 & 78.3 & 68.9 & 8.2 & - & 77.0 & 58.3 & 45.1 & 18.8 \\  Dense & - & 87.8 & 77.2 & 68.1 & 2.5 & - & 78.5 & 59.2 & 45.6 & 9 \\  Dense BN & - & 86.7 & 76.3 & 66.0 & 2.1 & - & 73.8 & 55.1 & 42.0 & 4.3 \\ RigL & 92 & 87.7 & 77.3 & 67.7 & 2.6 & 83 & 74.5 & 54.7 & 41.8 & 9.7 \\ Sparse & 92 & 88.6 & 77.7 & 67.4 & **2.1** & 83 & 76.7 & 57.8 & 44.5 & **4.1** \\    & &  &  \\  AttentionXML & - & 45.7 & 40.7 & 36.9 & 10.7 & - & 49.1 & 46.0 & 43.9 & 71.2 \\ LightXML & - & 47.1 & 42.0 & 38.2 & 11.2 & - & - & - & & OOM \\ CascadeXML & - & 48.5 & 43.7 & 40.0 & 18.3 & - & 51.3 & 49.0 & 46.9 & 87.0 \\  Dense & - & 49.8 & 44.2 & 40.1 & 11.5 & - & 53.4 & 50.6 & 48.5 & 46.3 \\  Dense BN & - & 44.5 & 39.7 & 36.1 & 4.0 & - & 47.0 & 44.6 & 42.7 & 13.1 \\ RigL & 83 & 45.2 & 38.7 & 36.0 & 12.4 & 83 & - & - & - & OOM \\ Sparse & 83 & 47.1 & 41.8 & 38.0 & **3.7** & 83 & 50.2 & 47.1 & 44.8 & 13.5 \\   

Table 2: Comparison with different methods. Comparing our sparse model against its dense counterpart and with state-of-the-art XMC methods on Wiki10-31K, Wiki-500K, Amazon-670K and Amazon-3M datasets. \(M_{}()\) indicates peak GPU memory consumption during training.

    & Sparsity &  &  &  & \(M_{}\) & Sparsity &  &  &  & \(M_{}\) \\  & & & & & \(()\) & & & & & \((GiB)\) \\    & &  &  \\  LightXML & - & 35.6 & 24.2 & 17.5 & 11.6 & - & 34.5 & 22.3 & 16.8 & 13.5 \\ ECLARE & - & 40.7 & 27.5 & 19.9 & 8.8 & - & 40.6 & 26.9 & 20.1 & 10.2 \\ SiameseXML & - & 41.4 & 27.9 & 21.2 & 7.1 & - & 42.2 & 28.1 & 21.4 & 8.9 \\ NGAME & - & 46 & 30.3 & 21.5 & 9.0 & - & 47.7 & 31.6 & 23.6 & 19.3 \\ DEXML & - & 42.5 & - & 20.6 & 30.2 & - & 46.1 & 29.9 & 22.3 & 56.1 \\  Renee (Dense) & - & 46.1 & 30.8 & 22 & 3.0 & - & 47.9 & 31.9 & 24.1 & 9.1 \\  Dense BN & - & 39.2 & 25.7 & 18.2 & 2.2 & - & 44.5 & 28.4 & 21.5 & 6.0 \\ RigL & 83 & 43.0 & 28.6 & 20.4 & 3.2 & 67 & 44.9 & 29.0 & 21.7 & 9.2 \\ Sparse & 83 & 44.5 &in maintaining performance, particularly in the high sparsity regime. Conversely, at lower sparsity levels (\(\!67\%\)), the benefit of the auxiliary objective diminishes. In the context of XMC problems, the performance of DST degrades as the label space size increases. Figure 3 (left) depicts the performance degradation of our approach relative to a dense baseline across datasets with increasing label space sizes: 31K, 131K, 500K, 670K, and 3M (detailed in the Table 1), all evaluated at 83% sparsity. Interestingly, for the wiki31K dataset, we observe a performance improvement, potentially due to the lower number of training samples relative to the label space size. Compared to other methods with equivalent memory requirements, our approach demonstrates superior performance retention at larger label space sizes.

### Effect of Rewiring Interval

The rewiring interval is crucial for balancing the trade-off between under-exploration and unreliable exploration. In XMC problems, tail label performance is particularly significant due to its application domain. The rewiring interval directly influences how frequently each parameter topology encounters tail label examples before updates. In this section, we focus on assessing the performance impact of various rewiring intervals, including their effect on tail labels. We conducted experiments on the LF-AmazonTitles-131K dataset using rewiring intervals \( T\!\!\). The corresponding results for P@1 and PSP@1 metrics are illustrated in Figures 4 left and right, respectively, with a fixed rewiring fraction of \(0.15\). Our findings reveal that both P@1 and PSP@1 improve as the interval increases up to a certain point. Interestingly, while P@1 shows a decline beyond this threshold, PSP@1 continues to rise. This divergence suggests that larger rewiring intervals, despite potentially limiting the diversity of topology exploration, provide each topology sufficient exposure to more tail labels, thereby improving model performance in handling rare categories.

### Performance on Tail Labels

Table 4 presents a comparison of Propensity-Scored Precision (PSP) for various Extreme Multi-label Classification (XMC) models, including AttentionXML , CascadeXML , Dense, Dense Bottleneck, and our proposed method, across four benchmark datasets: Wiki10-31K, Wiki-500K, Amazon-670K, and Amazon-3M. For the Wiki10-31K dataset, our model achieves PSP@1 of 13.2, PSP@3 of 15.1, and PSP@5 of 16.4, surpassing the Dense model. On the Wiki-500K dataset, our method records a PSP@1 of 31.9, outperforming other XMC models and closely trailing the top-performing approaches. These findings underscore our model's consistent performance across varied datasets, frequently exceeding or closely competing with XMC and Dense benchmarks. It's noteworthy that the performance of AttentionXML on Wiki10-31K is attributed to its utilization of an LSTM encoder, which is particularly advantageous given the dataset's smaller number of training samples relative to its label space. This configuration also explains our model's superior performance compared to the Dense model, which incorporates a form of regularization. In comparisons involving

Figure 3: **left:** Comparison of performance declines as the size of the label space increases, given a fixed sparsity. **right:** Performance of our model at different epochs, across various sparsity ratios.

memory efficiency, our approach significantly surpasses the same-capacity Dense Bottleneck model, demonstrating its suitability in resource-constrained settings where tail-label performance is critical.

### Impact of Varying Sparsity Levels

Table 5 illustrates the impact of varying sparsity levels (ranging from 50% to 96%) in conjunction with the use of auxiliary loss for Amazon-670K dataset. As sparsity levels increase, there are benefits in memory usage, training time, and inference time; however, performance metrics simultaneously decline. Additionally, the importance of auxiliary loss becomes particularly significant at higher sparsity levels.

### Sensitivity to Auxiliary Loss cut-off epochs

We employ auxiliary loss with an initial scalar weight that decays until a specified cut-off epoch. Table 6 illustrates the model's final performance at various cut-off epochs for two sparsity levels. A value of 0 (No aux) indicates the absence of auxiliary loss, while 'No cut-off' signifies its application throughout training. Our analysis reveals that prolonging the auxiliary loss beyond optimal cut-off epochs adversely affects performance for both sparsity levels. Notably, maintaining the auxiliary loss throughout training leads to performance deterioration, resulting in scores lower than those achieved without its use.

   Method & PSP@1 & PSP@3 & PSP@5 & PSP@1 & PSP@3 & PSP@5 \\    &  &  \\  AttentionXML & 16.2 & 17.1 & 17.9 & 30.1 & 37.3 & 41.7 \\ CascadeXML & 13.2 & 14.7 & 16.1 & 31.3 & 39.4 & 43.3 \\ Dense & 10.6 & 12.6 & 13.9 & 32.5 & 41.0 & 44.9 \\ Dense BN & 12.4 & 14.5 & 16.0 & 28.5 & 36.5 & 40.1 \\ Sparse & 13.2 & 15.1 & 16.4 & 31.9 & 39.8 & 43.4 \\   &  &  \\  AttentionXML & 29.3 & 32.4 & 35.1 & 15.5 & 18.5 & 20.6 \\ CascadeXML & 30.2 & 34.9 & 38.8 & - & - & - \\ Dense & 33.3 & 36.9 & 39.9 & 15.6 & 19.0 & 21.5 \\ Dense BN & 26.9 & 30.9 & 34.5 & 13.5 & 16.4 & 18.6 \\ Sparse & 29.9 & 33.3 & 36.4 & 14.3 & 17.2 & 19.4 \\   

Table 4: Propensity-Scored Precision (PSP) comparison of our sparse model with its dense counterpart and state-of-the-art XMC methods on the Wiki10-31K, Wiki-500K, Amazon-670K, and Amazon-3M datasets. The same sparsity levels as mentioned in previous tables are used.

Figure 4: Effect of rewiring interval on final performance for Precision@1 **(left)** and propensity-scored Precision@1 **(right)** in the LF-AmazonTitles-131K dataset.

### DST with Fixed Embedding vs End-to-End Training

In Table 7, we compare the performance of models using fixed embeddings  with trained end-to-end using DST on the Wiki-500K and Amazon-670K datasets. End-to-end training yields consistent improvements over fixed embeddings across all metrics, with significant gains in P@1 (an increase of 3.1% on Wiki-500K and 4.5% on Amazon-670K). These highlight the need of enabling the model to adapt its representations while training for the best possible performance.

## 4 Conclusion and future work

In this paper, we demonstrated the feasibility of DST for end-to-end training of classifiers with hundreds of thousands of labels. When appropriately adapted for XMC problems with fixed fan-in sparsity and an auxiliary objective, DST offers a significant reduction in peak memory usage while delivering superior performance compared to bottleneck-based weight reduction. It is anticipated that the Python bindings of the CUDA kernels will be useful for the research community in making their existing and forthcoming deep XMC pipelines more memory efficient. We hope that our work will enable further research towards developing techniques which could be (i) combined with explicit negative mining strategies, and (ii) used in conjunction with larger transformer encoders such as Roberta-large.

    &  \\   Auxiliary cut-off epoch & P@1 & P@3 & P@5 & Auxiliary cut-off epoch & P@1 & P@3 & P@5 \\ 
0 (No aux) & 45.6 & 40.2 & 36.3 & 0 (No aux) & 30.7 & 26.5 & 23.6 \\
15 & **47.1** & **41.8** & **38.0** & 15 & **42.3** & **37.2** & **33.3** \\
30 & 46.6 & 41.1 & 37.1 & 30 & 32.8 & 28.3 & 25.2 \\
60 & 45.9 & 40.6 & 36.6 & 60 & 32.5 & 28.0 & 24.9 \\
90 & 44.6 & 39.7 & 35.9 & 90 & 31.3 & 27.0 & 23.9 \\ No cut-off (full training) & 42.1 & 37.4 & 33.7 & No cut-off (full training) & 22.7 & 17.7 & 14.4 \\   

Table 6: Comparison of Auxiliary loss cut-off epoch effects on model performance for different Fan-in (sparsity) levels.

    &  \\  Method & P@1 & P@3 & P@5 & P@1 & P@3 & P@5 \\  Fixed & 73.6 & 54.8 & 42.1 & 42.6 & 37.1 & 33.1 \\ End-to-End & **76.7** & **57.8** & **44.5** & **47.1** & **41.8** & **38.0** \\   

Table 7: Performance comparison between fixed CascadeXML  embeddings and end-to-end training with DST on Wiki-500K and Amazon-670K datasets.

    &  &  &  &  & ({ GiB})\)} &  Epoch Time \\ (mins) \\  } &  Inference Time \\ (ms) \\  } \\  
384 (50\%) & No & 49.0 & 43.5 & 39.5 & 7.03 & 18:13 & 10.2 \\
384 (50\%) & Yes & **49.2** & **43.7** & **39.6** & 7.13 & 18:19 & 10.2 \\
256 (67\%) & No & 47.0 & 41.6 & 37.7 & 5.27 & 15:45 & 9.18 \\
256 (67\%) & Yes & 47.6 & 42.3 & 38.4 & 5.36 & 15:50 & 9.18 \\
128 (83\%) & No & 45.6 & 40.2 & 36.3 & 3.60 & 13:20 & 8.54 \\
128 (83\%) & Yes & 47.1 & 41.8 & 38.0 & 3.70 & 13:23 & 8.54 \\
64 (92\%) & No & 30.7 & 26.5 & 23.6 & 2.88 & 12:12 & 8.14 \\
64 (92\%) & Yes & 42.3 & 37.2 & 33.3 & 2.97 & 12:13 & 8.14 \\
32 (96\%) & No & 5.5 & 5.0 & 4.6 & **2.52** & **11:36** & **7.94** \\
32 (96\%) & Yes & 38.4 & 33.8 & 30.4 & 2.61 & 11:38 & **7.94** \\   

Table 5: Comparison of Fan-in (sparsity) effects on model performance and memory usage for Amazon-670K dataset.

## 5 Limitations and societal impact

For XMC tasks, this paper attempts to explore the landscape of sparse neural networks, which can be trained on affordable and easily accessible commodity GPUs. While the proposed scheme is able to achieve comparable results to state-of-the-art methods at a fraction of GPU memory consumption, it is unable to surpass the baseline with dense last layer on all occasions. The exact relative decline in prediction performance, when our proposed adaptations of Fixed Fan-In and auxiliary objective are employed, is shown in Figure 3.

While we do not anticipate any negative societal impact of our work, it is expected that it will further enable the exploration of novel training methodologies for deep networks which are more affordable and easily accessible to a broader research community outside the big technology companies.

## 6 Acknowledgements

We thank Niki Loppi of NVIDIA AI Technology Center Finland for useful discussions on the sparse CUDA kernel implementations. YI acknowledges the support of Alberta Innovates (ALLRP-577350-22, ALLRP-222301502), the Natural Sciences and Engineering Research Council of Canada (RGPIN-2022-03120, DGECR-2022-00358), and Defence Research and Development Canada (DGDND-2022-03120). This research was enabled in part by support provided by the Digital Research Alliance of Canada (alliancean.ca). RB acknowledges the support of Academy of Finland (Research Council of Finland) via grants 347707 and 348215. NU acknowledges the support of computational resources provided by the Aalto Science-IT project, and CSC IT Center for Science, Finland.