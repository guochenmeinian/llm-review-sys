# Geometric Analysis of Nonlinear Manifold Clustering

Nimita Shinde

Lehigh University

nis623@lehigh.edu

&Tianjiao Ding

University of Pennsylvania

tjding@upenn.edu

&Daniel P. Robinson

Lehigh University

daniel.p.robinson@lehigh.edu

&Rene Vidal

University of Pennsylvania

vidalr@upenn.edu

These authors contributed equally to this work

###### Abstract

Manifold clustering is an important problem in motion and video segmentation, natural image clustering, and other applications where high-dimensional data lie on multiple, low-dimensional, nonlinear manifolds. While current state-of-the-art methods on large-scale datasets such as CIFAR provide good empirical performance, they do not have any proof of theoretical correctness. In this work, we propose a method that clusters data belonging to a union of nonlinear manifolds. Furthermore, for a given input data sample \(y\) belonging to the \(l\)th manifold \(_{l}\), we provide geometric conditions that guarantee a manifold-preserving representation of \(y\) can be recovered from the solution to the proposed model. The geometric conditions require that (i) \(_{l}\) is well-sampled in the neighborhood of \(y\), with the sampling density given as a function of the curvature, and (ii) \(_{l}\) is sufficiently separated from the other manifolds. In addition to providing proof of correctness in this setting, a numerical comparison with state-of-the-art methods on CIFAR datasets shows that our method performs competitively although marginally worse than methods without theoretical guarantees.

## 1 Introduction

Manifold clustering is a fundamental problem in data science, in which one seeks to cluster data lying close to a union of _low-dimensional_ manifolds. It has a vast number of applications, such as clustering i) image pixels , ii) video frames , iii) images of faces , hand-written digits  or other natural objects , iv) rigid-body motions , v) human actions , and vi) searching policies for robots , to name a few.

Over the past two decades, there has been a considerable amount of work on _subspace clustering_, the special case where each manifold is a _linear or affine subspace_. A key to the success of many of these works can be attributed to the idea of _self-expressiveness_: one can write a data point as linear (or affine) combinations of other points, i.e., given a data point \(y\) and matrix of data points \(X\), it holds that \(y=Xc\) for some vector \(c\). The observation that the support of the sparsest such \(c\) should correspond to data points in the same subspace as \(y\) prompted the study of the optimization problem

\[_{c}\ r(c)+\|e\|_{2}^{2} e=y-Xc,\] (SC)

where \(r()\) is a regularizer on \(c\) (e.g., \(\|\|_{1}\) to promote sparsity ) and \(>0\) is a parameter that balances the two terms in the objective. By solving an instance of (SC) for each point in the dataset, one obtains one coefficient vector \(c\) per point, and the matrix of all coefficients for all points can beused to build a similarity graph of the points and run spectral clustering to obtain a clustering of the data. This has spurred a fruitful line of research, leading to various formulations based on different regularizers , efficient algorithms , and theoretical guarantees on \(c\) having the correct support .

While many interesting datasets (nearly) satisfy the linear or affine subspace assumption, there are a variety of tasks with associated datasets that grossly violate it. For example, natural image datasets such as CIFAR  and ImageNet  cannot be well modeled by low-dimensional subspaces. Instead, it is more natural to assume that each cluster is modeled by a _smooth low-dimensional non-linear manifold_, a more general case of manifold clustering. Notably, this is much more challenging than subspace clustering, as the global linear relationship among points in each subspace is absent.

Although a few nonlinear manifold clustering methods have achieved high clustering accuracy on large-scale datasets, they lack a theoretical justification. For example, the work in  aims to learn an embedding (or kernel) via neural networks with subspace clustering style loss functions on the embedded data. While these methods have progressively improved the state-of-the-art in clustering performance, with the most recent ones achieving over \(89\%\) accuracy on CIFAR-10 , little is theoretically understood about why these methods work. In fact, the work of  argues the opposite, that some of these methods are provably ill-formulated and learn trivial embeddings 1.

The current paper provides an approach that is both theoretically grounded and empirically tested on modern large-scale datasets. An interesting method that motivates us is _sparse manifold clustering and embedding_ (SMCE) . It views a _local neighborhood_ of a manifold approximately as a low-dimensional affine subspace, and solves a modified version of (SC) that reweights data and adds an affine constraint \(^{T}c=1\). Despite its effectiveness on simpler datasets such as Extended Yale-B , COIL20 , and MNIST  as observed by , so far there is no theoretical understanding of when this method will succeed, nor has it been applied to large-scale datasets.

Nevertheless, the method SMCE inspires our work. A major difficulty in providing theoretical guarantees for SMCE is to deal with the affine constraint. This motivates us to relax the constraint as a penalization, leading to the following model based on the self-expressiveness of the input sample \(y\):

\[_{c}\|Wc\|_{1}+[\|e\|_{2}^{2}+ (1-^{T}c)^{2}] e=y-Xc,\] (1.1)

where \(W\) is a diagonal matrix with \(j\)-th diagonal entry \(w_{j}\) an increasing function of the Euclidean distance between \(y\) and data sample \(x_{j}\). The regularizer \(\|Wc\|_{1}\) was adopted from SMCE to promote sparse solutions with support determined by "close" data points. Comparing (1.1) with (SC), we see two differences: (i) (1.1) replaces \(r(c)\) with a new regularizer that is a function of \(c\) and \(W\), and (ii) (1.1) penalizes violation of the constraint \(^{T}c=1\) in the objective by introducing the term \((1-^{T}c)^{2}\). Thus, as \(\) goes to infinity, the data sample will be represented by an affine combination of input data samples. It is then natural to ask the following question:

**Question 1**.: _Can we provide geometric conditions based on the input data samples and structure of the underlying manifolds that ensure that every optimal solution to (1.1) is manifold preserving, i.e., the non-zero entries of an optimal solution \(c\) correspond to data from the same manifold as \(y\)?_

### Contributions

In this paper, we propose a model that generates an approximately affine representation of an input data sample and provide an answer to Question 1. Our contributions are summarized below.

* _Formulation:_ We propose to perform manifold clustering via the convex optimization problem (1.1), which is based on a self-expressiveness property. Notably, the proposed formulation can be efficiently solved using a fast active-set-based method. We then construct an affinity matrix based on the solutions to (1.1), and then use spectral clustering to cluster the data.
* _Theory:_ Using our new formulation, we provide an answer to Question 1 by giving theoretical guarantees for any optimal solution of (1.1) to be manifold preserving (see Definition 1). The results depend on the curvature of the manifold at \(y\), the relative location of the data samples in the neighborhood of \(y\), i.e., the distribution of the sample in the neighborhood, and the separation between the samples from other manifolds and the neighborhood of \(y\).

* _Experiments:_ We compare the performance of the proposed method with state-of-the-art alternatives on CIFAR-10,-20 and -100 datasets. The proposed method performs consistently better than subspace clustering methods, and only marginally worse than methods based on deep networks.

### Notations

For \(l=1,,L\), we let \(_{l}\) denote a nonlinear manifold in \(^{D}\) with intrinsic dimension \(d_{l} D\). We define \(=_{l=1}^{L}_{l}\) to be the union of manifolds. Let \(N\) data samples be generated from \(\), which we represent as the columns of \(X=[x_{1},,x_{N}]^{D N}\). Additionally, let \(y\) be a new sample generated from the union of manifolds \(\). Without loss of generality, we assume that \(y_{1}\). Furthermore, for \(l=1,,L\), let \(_{l}\) denote the set of generated input data samples that lie on \(_{l}\), and define \(=_{l=1}^{L}_{l}\) to be the set of all \(N\) data samples. For a given subspace \(\), we let \(_{}(x)\) denote the orthogonal projection of \(x\) onto the subspace \(\).

Outline.In Section 2, we introduce our proposed model, define key quantities used in the analysis, and provide theoretical results for the proposed model. In Section 3, we perform experiments on synthetic data aimed to improve our understanding of the theoretical results, followed by a comparison with other existing methods on real data. We conclude the paper in Section 4.

## 2 Proposed Model and Theoretical Analysis

The model (1.1) we study is obtained by penalizing the affine constraint \(^{T}c=1\) in SMCE  with penalty parameter \(\). The penalty term in the model (1.1) is equivalent to homogenizing the data samples with homogenization constant \(\). Thus, we we propose to study the equivalent problem

\[_{c}\ \|Wc\|_{1}+\|\|_{2}^{2} =-c\] (WMC)

where \(>0\), \(W\) is a diagonal matrix with positive entries that depend on the Euclidean distances between \(y\) and the input data samples, \(^{(D+1) N}\) is the matrix whose \(j\)-th column, \(_{j}\), is the homogenized data sample \((x_{j}^{T},)^{T}\) with \(>0\) a chosen constant, and \(:=(y^{T})^{T}\). One could define \(w_{j}:=W_{jj}=\|x_{j}-y\|_{2}/(_{k}\|x_{k}-y\|_{2})\), although other reasonable choices are possible.

### Definitions and assumptions

In this section, we state our assumptions and define key quantities used in the theoretical results. Our theoretical analysis provides an answer to Question 1, i.e., we provide conditions under which the non-zero entries of a solution \(c^{*}\) of (WMC) correspond to data samples from the same manifold as \(y\). If \(c\) satisfies this property, it is called a manifold preserving solution, as we now define.

**Definition 1** (Manifold preserving).: _For the data sample \(y_{1}\), we say that the optimal solution \(c^{*}\) to (WMC) is manifold preserving if and only if \(c^{*}_{j}=0\) for all \(x_{j}_{1}\)._

If one considers solving \(N\) instances of problem (WMC) (each instance is defined by replacing \(y\) with \(x_{j}\), and removing \(x_{j}\) from \(X\)), then the \(N\) solutions may be used to define a similarity graph. If the solution to each instance is manifold preserving, then the similarity graph will only have intra-cluster connections (no inter-cluster/false connections). Consequently, it would be expected that applying spectral clustering to such a graph will result in correct clustering of the data.

We now introduce key quantities and assumptions required to prove the results in Sections 2.2 and 2.3, i.e., to derive conditions under which any optimal solution \(c^{*}\) to (WMC) is manifold preserving.

**Assumption 1**.: _We assume that each manifold is smooth and the input data samples generated on each manifold are noiseless. Furthermore, we assume that at least \(d_{l}+2\) samples are generated on each manifold \(_{l}\), where we recall that \(d_{l}\) is the intrinsic dimension of manifold \(_{l}\)._

**Definition 2** (Set of nearest neighbors \(\)).: _For the data sample \(y_{1}\), define \(\) to be the \(D\)-dimensional ball of smallest radius centered at \(y\) such that it contains \(d_{1}+1\) points from \(_{1}\), where \(d_{1}\) is the dimension of manifold \(_{1}\). Define \(\) to be the set of all data samples in \(\) excluding \(y\), and note that \(\) may contain samples from both \(_{1}\) and \(_{l=2}^{L}_{l}\)._

**Definition 3** (Nearest affine subspace \(\)).: _For the data sample \(y_{1}\) and \(\) in Definition 2, we define the nearest affine subspace, \(\), as the affine subspace generated by the set \(_{1}\), i.e.,_

\[:=(\{x_{j}:x_{j}_{1}\}).\] (2.1)

_Furthermore, we define \(}:=\{_{j}:x_{j} _{1}\}\) as the linear subspace spanned by the homogenized data samples from the set \(_{1}\)._

**Assumption 2**.: _For the affine subspace \(\) defined in Definition 3, we assume that \(()=d_{1}\). Equivalently, we assume that \((})=d_{1}+1\)._

The affine subspace \(\) is spanned by the \(d_{1}+1\) points nearest to \(y\) on the manifold \(_{1}\) (see Figure 1 for an illustration when \(_{1}\) is one-dimensional). We can view \(\) as an approximation of the tangent space to \(_{1}\) at \(y\).

We now define the dual of (WMC) since our analysis requires knowledge of the dual variables.

**Definition 4** (Weighted \(_{1}\) norm and its dual).: _The dual norm of the weighted \(_{1}\) norm \(\|c\|_{1,W}:=\|Wc\|_{1}\) is_

\[\|c\|_{,W^{-1}}:=_{\|z\|_{1,W} 1}c^{T}z=_{j[N]}|}{w_{j}}.\] (2.2)

We can write the dual of (WMC) as

\[_{^{D+1}},-\|\|_{2}^{2}\|^{T}\|_{,W^{-1}}  1.\] (2.3)

The constraint in (2.3) can be equivalently written as

\[|_{j},| w_{j}\ \ \ \ 1 j N.\] (2.4)

We now define two more quantities (Definitions 5 and (6)) that appear in our main results (see Lemma 1 and Lemma 3). Remark 1 also gives additional insight to these quantities.

**Definition 5** (Dual direction).: _For the data sample \(y_{1}\), define \(_{}}\) as the matrix with columns that span the linear subspace \(}\) in Definition 3, i.e., \(_{}}=[_{j}]_{x_{j} _{1}}\). Define the reduced problem_

\[_{c}\ \|c\|_{1,W}+\|-_{}}c\|_{2}^{2},\] (2.5)

_and its corresponding dual_

\[_{}\ ,-\|\|_{2}^{2} \|_{}}^{T}\|_{,W ^{-1}} 1.\] (2.6)

_Then, we define the dual direction \(^{*}\) as the unique optimal solution to (2.6)._

The dual direction \(^{*}\) is used to define the separation between the manifold \(\) and other manifolds. We elaborate on this in Remark 1.

**Definition 6** (Inradius \(r(_{}}^{W})\)).: _For the data sample \(y_{1}\), define_

\[_{}}^{W}=\{_{j}}{w_{j}}:x_{j}_{1}\}.\] (2.7)

_We define \(r(_{}}^{W})\) as the inradius of the symmetric convex body \(_{}}^{W}\), i.e., the radius of the largest Euclidean ball contained in \(_{}}^{W}\). For simplicity, we use the notation, \(r^{W}=r(_{}}^{W})\)._

It follows from Definition 6 that the inradius \(r^{W}\) increases as \(\{w_{j}\}\) decrease. Since \(\{w_{j}\}\) are proportional to the distances between \(y\) and the \(d_{1}+1\) points closest to \(y\) in the set \(_{1}\), the inradius increases when more samples are generated from \(_{1}\) near \(y\) (see Remark 1 for more details).

### Manifold-preserving: general geometric conditions

Our first theoretical result provides geometric conditions that ensure that an optimal solution of (WMC), defined for a given input data sample \(y\), is manifold preserving (see Definition 1).

**Lemma 1**.: _For \(y_{1}\), input data \(X\), and \(>0\), define the model (WMC) and let Assumptions 1 and 2 hold. Let \(\) and \(}\) be defined as in Definition 3, and \(^{*}\) and \(r^{W}\) be as given in Definitions 5 and 6, respectively. Let \((y,)\) be the Euclidean distance between \(y\) and subspace \(\), and define_

\[^{W}=_{x_{k}_{1}}r^{W}- _{k},_{}}(^{*}) |}{\|_{k}^{W}(^{*})\|_{2}}}{\|_{k}\|_{2}}.\]

_The following statements hold._

1. _If_ \[^{W}>0\] (2.8) \[(y,)<\|\|_{2}}{1+ ^{W}},\] (2.9) _then the interval_ \((^{l},^{u}):=((\|\|_{2}-(y,))},}{r^{W}(y,)})\) _is well defined and nonempty._
2. _If_ \((^{l},^{u})\)_, then every optimal solution to_ (WMC) _is manifold preserving and nonzero._

To help the reader better under the inequalities in (2.8) and (2.9), we give the following remark.

**Remark 1**.: _For the \(d_{1}\)-dimensional manifold \(_{1}\), conditions (2.8) and (2.9) depend on the following:_

1. _Inradius_ \(r^{W}\)_:_ \(r^{W}\) _(see Definition_ 6_) provides information of the distribution of the_ \(d_{1}+1\) _closest data samples to_ \(y\) _on the manifold_ \(_{1}\)_, i.e., the samples belonging to the set_ \(_{1}\)_. The inequalities in (_2.8_) and (_2.9_) are more easily satisfied if_ \(r^{W}\) _increases. From the definition of the inradius_ \(r^{W}\)_, it is clear that_ \(r^{W}\) _increases when the samples in the set_ \(_{1}\) _are more uniformly distributed. Furthermore, suppose we generate more samples from the manifold_ \(_{1}\) _near_ \(y\)_. Then, the distances of the_ \(d_{1}+1\) _samples in the set_ \(_{1}\) _from_ \(y\) _will decrease, resulting in a larger inradius. Thus,_ \(r^{W}\) _increases when either the number of samples generated from the manifold_ \(_{1}\) _near_ \(y\) _increases or the samples in the set_ \(_{1}\) _are well-distributed._
2. _Inner product_ \(|_{k},_{}}(^{*})|\)_: For each_ \(x_{k}_{1}\)_, the inner product defines the separation between_ \(_{k}\) _and the subspace_ \(}\)_. If the separation between each point_ \(x_{k}_{1}\) _and_ \(}\) _increases, the inner product decreases. From the definition of_ \(^{W}\)_, it is clear that the inequalities in (_2.8_) and (_2.9_) are more easily satisfied when the separation increases._

Part (a) of Lemma 1 provides conditions (based on the separation of samples belonging to other manifolds from \(}\), as well as the distribution of the input data samples in the neighborhood of \(y\)) for the interval \((^{l},^{u})\) to be nonempty. Note that \(^{l}\) is a function of \(y\) and \(X\) while \(^{u}\) is a function \(y,X,\) since \(^{W}\) depends on \(^{*}\), which is an optimal solution to (2.6) with parameter \(\).

Part (b) of Lemma 1 states that if the hyperparameter \(\) satisfies \(^{l}<<^{u}\), then every solution to (WMC) is manifold preserving and nonzero. While we can choose \(>^{l}\) based on knowledge of \((y,X)\), to explicitly compute \(\) satisfying \(<^{u}\) (when such a value exists) requires defining \(^{u}\) as a function of \(\). Although Lemma 1 does not provide this relationship, numerical experiments in Section 3.1 on randomly generated data clarify how \(^{l}\) and \(^{u}\) change as a function of \((y,X)\) and \(\).

Interestingly, a geometric result in  for a _linear_ subspace model can be recovered from Lemma 1.

**Corollary 1** (Special case of (WMC)).: _For \(y_{1}\), input data \(X\), and \(>0\), define the model (WMC) with \(W=I\). Assume that \(\|\|_{2}=\|_{j}\|_{2}=1\) for each data sample \(x_{j}\), and that Assumptions 1 and (2) hold. In addition to the quantiles defined in Lemma 1, define_

\[=_{x_{k}_{1}} _{k},_{}}(^{*})|}{\|_{ }}(^{*})\|_{2}},\]

_and let \(r=r^{W} r^{I}\) since \(W=I\). It then follows from Lemma 1 that if_

\[<r\ \ \ \ (y,)<,\] (2.10)_then the interval \(((y,))},(y, ))})\) is well defined and nonempty. Moreover, if \(\) is in this interval, then every optimal solution to (WMC) is manifold preserving and nonzero._

_Furthermore, if the manifolds are linear, then \((y,)=0\) since \(y\). Thus, in this special case, if \(<r\), then for any \(>1/r\), every optimal solution to (WMC) is subspace preserving and nonzero._

### Manifold-preserving: curvature-based geometric conditions

Although we believe the results in the previous section are the first of its kind, they depend on quantities related to projections onto \(}\), which we would like to avoid, if possible. To achieve this goal, we require curvature information of the manifold \(_{1}\) as well as the reach of the manifold \(_{1}\). Let us define the curvature and the reach of a manifold.

**Definition 7** (Curvature of a manifold).: _Let \(_{1}\) be a smooth manifold with \(y_{1}\). If \(1/\) is the radius of the largest (internal) tangent circle to \(_{1}\) at \(y\), then the curvature of \(_{1}\) at \(y\) is \(\)._

**Definition 8** (Reach of a manifold).: _Let \(_{1}\) be a smooth manifold. The reach of the manifold \(_{1}\), \((_{1})\), is the largest value such that for any \(x_{1}\) and \((x,_{1})(_{1})\), \(x\) has a unique projection onto \(_{1}\). If \(\) is the curvature of the manifold \(_{1}\), then \((_{1}) 1/\)._

In Figure 1, \(\) denotes the curvature of the 1-dimensional manifold \(_{1}\) at \(y\). The curvature of the manifold at \(y\) gives a bound on how fast the direction of a point on the manifold changes with respect to the distance traveled. The larger the change in the direction (or the angle), the larger will be the curvature. For linear manifolds, the curvature is zero.

In Lemma 2, we give a relationship between the curvature of the manifold \(_{1}\) at \(y\), the radius of the ball \(\) centered at \(y\), and \((y,)\). Then, in Lemma 3, we provide geometric conditions based on the curvature that guarantee that any optimal solution of (WMC) is manifold preserving.

**Lemma 2**.: _Let Assumptions 1 and 2. For \(y_{1}\), let \(\) and \(\) be as defined in Definitions 2 and 3, respectively. Let \(=()\) be the radius of the sphere \(\), \(\) be the curvature of the manifold \(_{1}\) at \(y\), \((_{1})\) be the reach of the manifold \(_{1}\), and \((y,)\) be the Euclidean distance between \(y\) and the subspace \(\). If \((_{1})\), then_

\[(y,)-}- ^{2}}.\] (2.11)

Combining Lemma 1 and Lemma 2 gives our final result.

**Lemma 3**.: _For \(y_{1}\), input data \(X\), and \(>0\), define the model (WMC) and let Assumptions 1 and 2 hold. In addition to the quantities defined in Lemma 1, let \(\) be the radius of the sphere \(\), \(\) be the curvature of the manifold \(_{1}\) at \(y\), and \((_{1})\) be the reach of the manifold \(_{1}\). The following then hold._

1. _If_ \[^{W} >0,\] (2.12) \[(_{1}) ,\] (2.13) \[-}-^{2}} <\|\|_{2}^{2}}{1+^{W}},\] (2.14) _then there exists a non-empty interval \((^{l},^{u}):=((\|\|_{2}- +}-^{2}})},}{r^{W}(-}-^{2}} )})\)_._
2. _If_ \((^{l},^{u})\)_, then every optimal solution to (_WMC_) is manifold preserving and nonzero._

The quantity \(^{W}\) in Lemma 3 captures the notion of distribution of the data samples in the set \(_{1}\) as well as separation of the data samples not in \(_{1}\) from \(}\). The condition (2.12) requires \(^{W}\) to be positive. Condition (2.13) is related to the sampling density of the manifold \(_{1}\) near \(y\). If the number of samples generated on \(_{1}\) near \(y\) increases, then \(\) will decrease. Meanwhile, if the curvature \(\) of the manifold at \(y\) is large, \(1/\) will be small, and from Definition 8, we see that \((_{1})\) will be small. This indicates that for (2.13) to hold, one may need to generate more samples on \(_{1}\) near \(y\). Finally, (2.14) provides a lower bound on \(^{W}\) based on \(\) (the curvature) and \(\) (the number of samples generated on \(_{1}\) near \(y\)). When all three of these conditions hold, we have a non-empty interval \((^{l},^{u})\) as defined in Lemma 3(a). If \(^{l}<<^{u}\), then Lemma 3(b) ensures that every optimal solution to (WMC) is manifold preserving and nonzero.

## 3 Computational Results

In this section, we first perform experiments on randomly generated data to understand how the quantities in Lemma 1 change as a function of the input data and the hyperparameter \(\) (see Subsection 3.1). Next, in Subsection 3.2, we compare the performance of our model (WMC) with existing methods on CIFAR datasets. Our goal here is to check the performance of (WMC) and to show that while (WMC) does not outperform the state-of-the-art methods, it performs only slightly worse in terms of clustering accuracy.

### Experiments on synthetic data and understanding Lemma 1

In this section, we perform two experiments designed to understand how \(^{l}\) and \(^{u}\) (see Lemma 1) change as a function of the number of data samples \(N\) and hyperparameter \(\) for randomly generated data from two trefoil knots. The noiseless data samples are embedded in \(^{100}\). The embedding of data involves generating a random orthonormal basis of a subspace in \(^{100}\), followed by projecting the samples generated from two trefoil knots onto this subspace. We choose \(=1\) and define the matrix \(W\) as a diagonal matrix with \(j\)-th diagonal entry \(w_{j}=\|y-x_{j}\|_{2}/_{k}\|y-x_{k}\|_{2}\).

Experiment 1: \(\{^{l},^{u}\}\) versus \(\).For this experiment, we evaluate how the values of \(^{l}\) and \(^{u}\) change when only \(\) changes. We generate \(N_{1}\{120,150,160,200\}\) and \(N_{2}=40\) data samples from two non-intersecting trefoil knots of intrinsic dimension \(d_{1}=d_{2}=1\). We generate 50 different random embeddings of the data samples from the trefoil knots in \(^{100}\). We plot the mean values and standard deviations of \(^{l}\) and \(^{u}\) for different values of \([4.8,204.8] 10^{-4}\) in Figure 2.

Since \(^{l}\) is independent of \(\), its value is constant for all values of \((N_{1},N_{2})\). It also appears that \(^{u}\) is a monotonically increasing function of \(\) that eventually reaches a "steady-state" value. When \(N_{1}=120\), we observe that the conditions in Lemma 1(a) are not satisfied for any value of \(\), and that the interval \((^{l},^{u})\) is empty. When \(N_{2}=150\), we observe that \(^{u}>^{l}\) for \(>0.0012\), but for these \(\) values it also holds that \(>^{u}\), meaning that Lemma 1(b) is violated for all \(>0\). When \(N_{1}\{160,200\}\), we observe that for some values of \(>0\), there exist non-empty intervals \((^{l},^{u})\) such that \((^{l},^{u})\). Thus, for every such \((^{l},^{u})\), it follows from Lemma 1 that every optimal solution to (WMC) is manifold preserving and nonzero. We can conclude from these experiments that for certain input data \((y,X)\), there exists a range of _acceptable_ values of \(\) for which the conditions defined in both parts (a) and (b) of Lemma 1 are satisfied, so that every optimal solution to (WMC) is nonzero and manifold preserving.

Experiment 2: \(\{^{l},^{u}\}\) versus \(N\).In this experiment, we aim to understand how \(^{l}\) and \(^{u}\) change when the number of data samples, \(N\), changes. We let \(N_{1}=N_{2}\) and gradually increase \(N=N_{1}+N_{2}\). We generate 50 different random embeddings of data samples from trefoil knots

Figure 2: Plot of \(^{l}\), \(^{u}\), and \(\) for \((N_{1},N_{2})\) samples generated from two trefoil knots. The data generated from the knots are embedded in \(^{100}\) with 50 randomly generated embeddings.

in \(^{100}\). For each pair of data \((y,X)\), we compute \(^{l}\) and then create three values for \(\) by setting \(=^{l}\) for \(\{2,5,50\}\). Then, for each \(\) value, we plot \(^{l}\) and \(^{u}\) versus \(N=N_{1}+N_{2}\) in Figure 3. We can observe that as the number of samples increases, the size of the interval \((^{l},^{u})\) increases linearly.

We further observe that for \(=2\), the interval \((^{l},^{u})\) is non-empty (satisfying the conditions in Lemma 1(a)) for all \(N 890\), whereas for \(=5,50\), the interval is non-empty for all \(N 300\). Thus, we see that the small value of \(\) leads to a slower increase in \(^{u}\) resulting in the interval \((^{l},^{u})\) becoming non-empty at a much larger value of \(N\). Furthermore, we also see that, (i) when \(=5\), \((^{l},^{u})\) for all \(N 330\), and (ii) when \(=50\), \((^{l},^{u})\) for all \(N 510\). In other words, when \(=5\), the conditions in both parts (a) and (b) of Lemma 1 are satisfied for all \(N 330\), whereas when \(=50\), the conditions in parts (a) and (b) of Lemma 1 are satisfied for all \(N 510\), making \(=5^{l}\), the best choice of the hyperparameter in this experiment.

### Experiments on real data

In this section, we compare the empirical performance of our model with existing methods on CIFAR-10, CIFAR-20, CIFAR-100 datasets . The CIFAR dataset consists of 60000 color images of size 32\(\)32 that are divided into 10, 20, and 100 classes for CIFAR-10, CIFAR-20, CIFAR-100, respectively. In Section 3.2.1, we describe the steps for clustering data using our model (WMC), followed by a comparison of empirical results in Section 3.2.2. (See Appendix B.1 for more details.)

#### 3.2.1 Clustering data using (WMC)

We follow the steps in Algorithm 1 to cluster the input images using our model (WMC).

**Input**: Images from CIFAR dataset, \(>0\), and the number of clusters \(L\)

```
1:Generate features using the CLIP encoder .
2:Construct a representation matrix \(C\) by solving problem (WMC) for the CLIP feature vector for each input image.
3:Construct a symmetric affinity matrix \(A\) from \(C\).
4:Apply spectral clustering to \(A\) to get the \(L\) clusters.
5:\(L\) clusters of data ```

**Output**: \(L\) clusters of data

The CLIP (Contrastive Language-Image Pre-Training)  encoder is a powerful tool in pre-training data. Given its prior success, we use CLIP to map each input image in CIFAR to a feature vector in Step 1 of Algorithm 1. Step 2 of Algorithm 1 employs our model WMC to construct a representation matrix \(C\). We now briefly explain this step. For the feature vector \(y\) of each input image: (a) we define the positive diagonal distance matrix \(W\) so that each diagonal entry is an increasing function of the Euclidean distance between \(y\) and each of the other feature vectors, (b) we define the model (WMC) for \(>0\), (c) we solve (WMC) using an active-set method  to (hopefully) obtain a manifold preserving representation for the feature vector of each input image. (The choice of \(\) and \(W\) is explained further in Subsection 3.2.2.) Similar to classical spectral clustering methods, the representation matrix is used to define a symmetric affinity matrix \(A\); here, we choose \((|C|+|C|^{T})\). (We used this definition in combination with other techniques to construct \(A\), which are explained in detail in Appendix B.2.) Finally, we use spectral clustering on \(A\) to cluster the data.

Output metrics.We use two metrics to compare the performance of various clustering methods. In particular, we use Clustering Accuracy and Normalized Mutual Information whose values range from 0 to 100%, with higher values indicating better performance.

#### 3.2.2 Numerical Results

In this section, we compare the numerical performance of two instances of our Algorithm 1 with the subspace clustering methods (a) Elastic Net Subspace Clustering (EnSC)  and (b) Sparse Subspace Clustering, manifold clustering method (c) SMCE , and with the deep clustering methods (d) CPP  and (e) TEMI . Each method is applied to the CLIP features extracted from the input images. The two instances of our Algorithm 1 are determined by how the weight matrix \(W\) is chosen in (WMC), as we now describe.

* **L-WMC**: _Linearly_ Weighted Manifold Clustering model with \(w_{j}=\|x_{j}-y\|_{2}/_{k}\|x_{k}-y\|_{2}\) for all \(j\) in (WMC). Here, the weights \(\{w_{j}\}\) are linearly proportional to the Euclidean distances.
* **E-WMC**: _Exponentially_ Weighted Manifold Clustering model with weights in (WMC) defined as \(w_{j}=(2\|x_{j}-y\|_{2})/_{k}(2\|x_{k}-y\|_{2})\) for all \(j\). Here, the weights \(\{w_{j}\}\) are defined as an exponential function of the Euclidean distances.

We report the clustering results for L-WMC and E-WMC in Table 1. (See Appendix B.2 for details on the choice of \(\) and \(\).) From Table 1, we can observe that the clustering accuracy of L-WMC and E-WMC is consistently better than EnSC, SSC and SMCE for all three datasets. Moreover, L-WMC and E-WMC perform slightly worse than the state-of-the-art method CPP on the CIFAR-10 and CIFAR-20 datasets. So, although our method does not outperform the existing state-of-the-art method, our method is the first to perform similarly to the state-of-the-art methods and have a theoretical guarantee of correctness (see Lemma 1 and Lemma 3).

## 4 Discussion

In this paper, we propose a model whose solution defines a self-expressive representation of the input data, and provide a theoretical analysis of correctness of the model (see Lemma 1 and Lemma 3). For data \((y,X)\) and hyperparameter \(>0\), we give conditions under which the solution to our model is

   Dataset &  &  &  \\ Metrics & ACC & NMI & ACC & NMI & ACC & NMI \\   \\  L-WMC & 96.07 & 90.54 & 63.97 & 69.55 & 69.2 & 76.49 \\ E-WMC & 94.34 & 88.79 & 64.1 & 69.58 & 69.83 & 76.63 \\ SMCE & 86.86 & 90.66 & 63.1 & 70.53 & 68.56 & 77.17 \\ EnSC \({}^{*}\) & 85.8 & 89.2 & 61.6 & 69.3 & 66.6 & 77.1 \\ SSC \({}^{*}\) & 85.4 & 84.6 & 60.9 & 65.3 & 64.6 & 72.8 \\   \\  TEMI \({}^{*}\) & 96.9 & 92.6 & 61.8 & 64.5 & 73.7 & 79.9 \\ CPP \({}^{*}\) & **97.4** & **93.6** & **64.2** & **72.5** & **74.0** & **81.8** \\  ^{*}\)The numerical results in this row are taken from .} \\ ^{}\)See appendix in  for details on fine-tuning models.} \\ 

Table 1: Comparing clustering accuracy (ACC) and Normalized Mutual Information (NMI) for our models L-WMC and E-WMC with state-of-the-art subspace clustering and deep clustering methods. Each method is used to cluster the data pre-trained using CLIP . For each metric (ACC and NMI) and data set, the best overall value achieved is **bolded**, and the best value achieved by our method is in blue.

non-trivial and manifold preserving. To the best of our knowledge, this is the first work that provides a theoretical understanding of manifold clustering models. We also show that our model performs only marginally worse than the current state-of-the-art methods on CIFAR datasets.

For a given data set, our analysis does not provide a strategy for choosing the hyperparameter \(>0\) so that it satisfies the conditions in Lemma 1. This leads to an important open question: _For any given data set, can one provide a proof of existence of \(>0\) for which the conditions in Lemma 1 are satisfied? If so, can one provide a closed form expression to choose the value of \(\)?_ An answer to this question will result in an "optimal" choice for the value of \(\), thus avoiding the cost of tuning \(\).