# Hokoff: Real Game Dataset from Honor of Kings and its Offline Reinforcement Learning Benchmarks

Yun Qu\({}^{}\), Boyuan Wang\({}^{}\), Jianzhun Shao\({}^{}\)\({}^{}\), Yuhang Jiang\({}^{}\), Chen Chen\({}^{}\), Zhenbin Ye\({}^{}\), Lin Liu\({}^{}\), Junfeng Yang\({}^{}\), Lin Lai\({}^{}\), Hongyang Qin\({}^{}\), Minwen Deng\({}^{}\), Juchao Zhuo\({}^{}\), Deheng Ye\({}^{}\), Qiang Fu\({}^{}\), Wei Yang\({}^{}\), Guang Yang\({}^{}\), Lanxiao Huang\({}^{}\), Xiangyang Ji\({}^{}\)

\({}^{}\)Tsinghua University, \({}^{}\)Tencent Timi Studio, \({}^{@sectionsign}\)Tencent AI Lab

{qy22,wangby22,szj18,jiangh19}@mails.tsinghua.edu.cn,chenchen.peach@gmail.com,

{zhenbinye,lincliu,fengjunyang,linlai,hongyangqin,danierdeng,jozhuo,dericye,leonfu,

willyang,mikoyang,jackiehuang}@tencent.com,xyji@tsinghua.edu.cn

Authors contributed equally

###### Abstract

The advancement of Offline Reinforcement Learning (RL) and Offline Multi-Agent Reinforcement Learning (MARL) critically depends on the availability of high-quality, pre-collected offline datasets that represent real-world complexities and practical applications. However, existing datasets often fall short in their simplicity and lack of realism. To address this gap, we propose Hokoff, a comprehensive set of pre-collected datasets that covers both offline RL and offline MARL, accompanied by a robust framework, to facilitate further research. This data is derived from Honor of Kings, a recognized Multiplayer Online Battle Arena (MOBA) game known for its intricate nature, closely resembling real-life situations. Utilizing this framework, we benchmark a variety of offline RL and offline MARL algorithms. We also introduce a novel baseline algorithm tailored for the inherent hierarchical action space of the game. We reveal the incompetency of current offline RL approaches in handling task complexity, generalization and multi-task learning.

## 1 Introduction

Online Reinforcement Learning (Online RL) relies on the interaction between the training policy and the environment for data collection and policy optimization [33; 18; 7]. However, this paradigm makes online RL unsuitable for certain real-world scenarios, such as robotics and autonomous driving [23; 18], as deploying untested policies to the environment can be costly and dangerous . In contrast, Offline Reinforcement Learning (Offline RL) can learn satisfactory policies using a fixed dataset without the need for further interaction with the environment [23; 18; 7; 19]. This characteristic alleviates the aforementioned issue, making offline RL potentially more suitable for certain real-world scenarios compared to online RL .

The research on offline RL has attracted significant attention in recent years and has made substantial progress in both theoretical analysis and practical performance. The core challenge of offline RL is the value overestimation issue induced by distributional shift [9; 21]. Existing studies mitigate this problem by constraining the learning policy to closely resemble the behavior policy induced by the dataset [39; 18; 7], or adopting conservative value iteration [19; 16]. The success of offline RL can be largely attributed to the availability of widely-adopted open-access datasets, such as D4RL  and RL Unplugged . These datasets offer standardized and diverse pre-collected data for the development of new algorithms, while also offering proper evaluation protocols that facilitate fair comparisons between different algorithms. However, despite their benefits, tasks contained in these datasets (suchas Atari 2600  or Mujoco ) are often overly simplistic or purely academic, failing to simulate the complexity of real-world scenarios and lacking practical applications. Thus, there is a significant gap between offline RL research and its practical application in real-world settings. This disparity hinders the usefulness of offline RL in addressing real-world problems, and thus, it is indispensable to create datasets that reflect a realistic level of complexity and practicality in real-world applications.

Offline Multi-Agent Reinforcement Learning (Offline MARL) [28; 43] is gaining increasing attention due to its close relationship to real scenarios, such as games [31; 38], sensor networks  and autonomous vehicle control . However, the lack of standardized datasets restricts the development 40 of offline MARL. Existing works only rely on self-made datasets, which hampers fairness and reproducibility. Moreover, the settings they focus on are typically limited to toy examples (e.g., Multi-agent Particle Environment ) or simplified versions of classic games (e.g., StarCraft Multi-Agent Challenge ), facing the same impractical issues encountered in offline RL. Thus, there is an urgent need for open-access datasets to further the progress of offline MARL.

The connections between offline RL and offline MARL are closely linked due to similar challenges pertaining to offline learning. While, offline MARL also introduces distinct algorithm development requirements, as it involves unique characteristics like multiple agents and intra-team cooperation. The current offline MARL algorithms [28; 43] are mainly adaptations of offline RL algorithms, necessitating the availability of standardized offline datasets that cater to both single-agent and multi-agent settings. Thus, to enhance versatility and practicality, it is crucial to propose datasets that encompass both single-agent settings and multi-agent settings.

In this paper, we present Hokoff, a suite of pre-collected datasets for both offline RL and offline MARL, along with a comprehensive framework for conducting corresponding research. Our paper makes several novel contributions, which are shown below:

\(\) The tasks we adopt are based on one of the world's most popular Multiplayer Online Battle Arena (MOBA) games, Honor of Kings (HoK), which has over 100 million daily active players, ensuring the practicality of our datasets. The complexity of this environment dramatically surpasses those of its counterparts, demonstrating the potential for simulating real-world scenarios.

\(\) We present an open-source, easy-to-use framework 1 under Apache License V2.0. This framework includes comprehensive processes for offline RL (sampling, training, and evaluation), and some useful tools. Based on the framework, We release a rich and diverse set of datasets 2 which are generated using a series of pre-trained models featuring distinct design factors. These datasets cater not only to offline RL but also offline MARL.

\(\) Building on the framework, we reproduce various offline RL and offline MARL algorithms and propose a novel baseline algorithm tailored for the inherent hierarchical structured action space of Honor of Kings. We fully validate and compare these baselines on our datasets. The results indicate that current offline RL and offline MARL approaches are unable to effectively address complex tasks with discrete action space. Additionally, these methods exhibit shortcomings in terms of their generalization capabilities and their ability to facilitate multi-task learning.

## 2 Related Works

### Offline RL and Offline MARL

Offline RL gains significant attention in recent years, primarily due to the inherent difficulties of directly applying online RL algorithms to offline environments. The main hurdles encountered is the issue of erroneous value overestimation, which arises from the distributional shift between the dataset and the learning policy . Theoretical studies have demonstrated that the overestimation issue can be alleviated by pessimism, which results in satisfactory performance even with imperfect data coverage [4; 5; 14; 20; 25; 30; 41; 48]. In practice, certain studies [1; 2; 40; 1; 2; 40] employ uncertainty-based methods to estimate Q-values pessimistically or to perform learning on pessimistic dynamic models by estimating the epistemic uncertainty of Q-values or dynamics. Some studies [18; 9; 39; 16; 19; 7] adopt behavior regularization-based approaches by imposing constraints on the learned policy to align closely with the behavior policy, either explicitly or implicitly, which offers better computational efficiency and memory consumption compared to uncertainty-based methods.

Offline MARL, combining offline RL and MARL, has emerged in recent years to address safety and training efficiency concerns in practical multi-agent scenarios. Most studies in this domain adopt a multi-agent paradigm, such as independent learning  or centralized training with decentralized execution (CTDE) . These investigations also incorporat offline methods, similar to those employed in single-agent settings, to mitigate distributional shift. Moreover, innovative treatments are introduced for cooperation, such as zeroth-order optimization in OMAR  or decomposing the joint-policy in MAICQ . In addition, Jiang & Lu  specifically focuses on decentralized learning using BCQ , while Tseng et al.  regards offline MARL as a sequence modeling problem, utilizing supervised learning and knowledge distillation to tackle the challenges it presents.

### Offline Datasets

The availability of large-scale pre-collected datasets has greatly facilitated the progress of deep supervised learning . Offline RL, which is regarded as a bridge between RL and supervised learning, also requires learning policies from pre-collected datasets . Therefore, high-quality pre-collected offline datasets play a significant role in the development of offline RL. To meet this demand, some datasets have been published and widely adopted. D4RL  is designed to address key challenges often faced in practical applications where datasets may have limited and biased distributions, incomplete observations, and suboptimal data. To tackle these issues, D4RL offers a range of datasets that enjoy these characteristics. Similarly, RL Unplugged  introduces a benchmark to evaluate and compare offline RL methods with various settings, such as partially or fully observable and continuous or discrete actions. These offline datasets play a significant role in offline RL research, and many previous works train and evaluate their methods based on these datasets [16; 2; 15; 28; 43].

However, both D4RL and RL Unplugged primarily focus on relatively simple tasks and lack high-dimensional, practical and multi-agent tasks that closely resemble real-world scenarios. StarCraft II Unplugged  introduces a benchmark for StarCraft II, a complex simulated environment with several practical properties. However, they only utilize a dataset derived from human replays, which lacks diversity in design for offline RL, and they did not evaluate existing offline RL methods. To address this research gap, we propose Hokoff, a benchmark based on HoK, which aims to provide diverse offline datasets for high-dimensional, practical tasks, and present a comprehensive evaluation of previous offline RL and offline MARL methods with a general, easy-to-use framework.

## 3 Background

Honor of Kings (HoK) is one of the most popular MOBA games worldwide, boasting over 100 million daily active players . The game involves two teams, each consisting of several players who have the option to select from a wide range of heroes with diverse roles and abilities. In the game, heroes are expected to eliminate enemy units, such as heroes, creeps, and turrets, to gain gold and experience. The primary objective is to destroy the enemies' turrets and crystal while defending their own. To succeed in MOBA games, players must learn how to choose the appropriate hero combination, master complex information processing and action control, plan for long-term decision-making, cooperate with allies, and balance multiple interests. The complex rules and properties of HoK make it be more in line with the complex decision-making behavior of human society. Thus, HoK has attracted numerous researchers interest [44; 45; 38; 37; 10].

The underlying system dynamic of HoK can be characterized by a Partially Observable Markov Decision Process (POMDP ), denoted by \(=(,,,,r,,d)\). Due to the fog of war and private features, each agent has access to only local observations \(\) rather than the global state \(s\). Specifically, the agents are limited to perceiving information about game units within their field of view, as well as certain global features. Due to the intricate nature of control, the action space \(\) is organized in a hierarchically structured manner, rather than being flattened, which avoids the representation of millions of discretized actions. Randomness is added into the transition distribution \(\) in the form of critical hit rate. The reward \(r\) is decomposed into _multi-head_ form and each hero's reward is a weighted sum of different reward items and is designed to be _zero-sum_. Details of observation space, action space and reward are presented in Appendix D.

## 4 Hokoff

This study is based on the HoK gaming environment, which encompasses both 1v1 and 3v3 maps. Our research proposes a comprehensive offline RL framework applicable to this gaming environment and utilizes it to generate diverse datasets. This section provides an introduction to the framework, game modes, datasets, and evaluation protocol employed in this study.

### Framework

To enhance the usability of our Hokoff, we propose a reliable and comprehensive Offline RL framework that consists of three modules: sampling, training, and evaluation. This framework streamlines the process of sampling new datasets, developing and training baselines, and evaluating their performance. The sampling module provides a simple and unified program for sampling diverse datasets using any pre-trained checkpoints. There are several reasons why our framework excels in sampling. Firstly, diverse datasets at different levels of expertise can be sampled by leveraging Multi-Level Models as described in Sec. 4.1.1. Secondly, our framework employs parallel sampling techniques, ensuring efficient sampling of large and diverse datasets. Based on the training module, we have implemented various offline RL and offline MARL algorithms as baselines. Additionally, we consolidate crucial components and provide user-friendly APIs, facilitating researchers to effortlessly develop novel algorithms or innovative network architectures. The evaluation module enables the assessment of trained models from different algorithms, ensuring fair comparisons. Fig. 2 demonstrates the architecture of our framework and Appendix E provides an example of the APIs.

#### 4.1.1 Multi-Level Models

To ensure a valid and unbiased comparison of the performance of distinct algorithms, it is crucial to establish appropriate evaluation protocols [6; 12]. One such effective evaluation protocol is the normalized score . However, HoK is a zero-sum adjustable rewards MOBA game. The episode return in the game is heavily influenced by the opponents and game settings, and the objective is to win, which renders the use of return as a performance metric biased. Therefore, normalized score may not fully capture our requirements. Furthermore, similar to our situation, the evaluation protocol for SMAC , a competitive game, is based on win rate against a pre-programmed AI. Nonetheless, it is exceedingly challenging to create a built-in AI with human-like performance due to the complexity of MOBA games.

Inspired by prior works of HoK , we present Multi-Level Models for sampling and evaluating which contains multiple checkpoints with different level. Specifically, we have extracted several checkpoints from pre-trained dual-clip PPO [45; 44] models with varying levels determined by the outcome of the battle separately for HoK1v1 and HoK3v3. We adopt the _win rate_ against different checkpoints as our evaluation protocols to assess the ability of models. Additionally, these models, with varying levels, can be utilized on both sides to sample diverse battle data. The capabilities of these models surpass those of rule-based AI and match the levels of different human players, thus making these evaluation protocols more suitable for comparing algorithmic performance with human-level performance and facilitating diverse and effortless sampling. The details of these Multi-Level Models are provided in the Appendix F.

Figure 1: (a) The Game replay user interface (UI) in HoK1v1. (b) The UI in HoK3v3. Important information and units of the game are highlighted using orange boxes.

### Game Modes

We have incorporated two game modes from HoK into our study, namely HoK1v1  and HoK3v3. The environment code of HoK3v3 is integrated into the open-source HoK1v1 code3, following Apache License V2.0. These game modes differ in the number of agents involved and the underlying map used. Detailed information on each game mode is presented below.

#### 4.2.1 Honor of Kings Arena

Honor of Kings Arena (HoK Arena or HoK1v1) is a 1v1 game mode where each player attempts to beat the other and destroy its opponent's crystal. Specifically, each player chooses a hero before the game starts and controls it during the whole game. There are a total of 20 heroes available for players to select, each possessing distinct skills that exert diverse effects on the game environment. The observation space is a continuous space consisting of 725 dimensions that contain partial observable information about the hero, opponent, and other game units. The action space is hierarchically structured and discretized, covering all possible actions of the hero in a hierarchical triplet form: (1) which action button to take; (2) who to target; and (3) how to act. Furthermore, the reward is a weighted sum of five categories: farming, kill-death-assist (KDA), damage, pushing, and win-lose. For a full description of this game mode, please refer to the Appendix D.1.

#### 4.2.2 Honor of Kings 3v3 Arena

To further cater to the demand for Offline MARL, we adopt Honor of Kings 3v3 Arena (HoK3v3) as our experimental platform. HoK3v3 is a MOBA game, where each team comprises three heroes who collaborate to defeat their opponents. The basic rules and win conditions of HoK3v3 are similar to HoK1v1. However, the HoK3v3 map contains additional turrets on the middle road and features a new area called the "wilderness", inhabited by diverse monsters. Besides, collaboration is essential in HoK3v3, where players must select different heroes and fulfill distinct roles to work together more efficiently. For instance, one hero might focus on slaying monsters in the wilderness to earn gold and experience, while the other heroes engage in offensive tactics against the enemy heroes and game units. The design philosophies for observation space, action space, and reward are comparable to those used in HoK1v1. However, the level of complexity in HoK3v3 is significantly elevated. We provide a detailed description of the game mode in the Appendix D.2 for reference.

Figure 2: The architecture of the framework. The sampling and evaluation modules should interact with the environment. Multi-Level Models are the foundation baseline models of these two modules, serving as opponents in the evaluation module and being on both sides in the sampling module, as described in Sec 4.1.1. The training module is responsible for training offline RL algorithms using fixed datasets and producing trained models for evaluation.

#### 4.2.3 Subtasks

Both HoK1v1 and HoK3v3 are full MOBA games, featuring multi-camp competitions, which inherently pose challenges and limitations. Consequently, training on these game modes demands extensive training time and computational resources. However, HoK game comprises various sub-objectives, allowing us to decompose the overall game into manageable subtasks. These subtasks can represent diverse scenarios and are suitable for evaluating various algorithms. In this study, we propose two specific noncompetitive subtasks as outlined below. It is worth noting that researchers can readily expand upon our framework to develop additional subtasks.

_Destroy Turret:_ One of the key sub-objectives in HoK is to destroy the enemy's turrets as quickly as possible, to gain access to the enemy crystal. To train this specific skill, we have devised a subtask called _Destroy Turret_, which is based on HoK1v1. In this subtask, the focus is solely on destroying the enemy's turret and crystal as quickly as possible, and the enemy hero is removed.

_Gain Gold:_ Gold is a critical resource in HoK that can be used to purchase equipment, which enhances the abilities of the heroes. Inspired by resource collection tasks from previous studies , we have designed a subtask called _Gain Gold_, which is based on HoK3v3, where the new objective is to collect golds in restricted time steps, and the enemy heroes are removed. As a multi-agent setting, it focuses on the cooperation or intra-team competition while avoiding inter-team competition.

### Datasets

To enhance the practical implications of our datasets, we have incorporated design factors that align with the real-world applications of both HoK and other relevant scenarios.

_Multi-Difficulty_

Intuitively, the level of difficulty in the environment significantly impacts the performance of algorithms. However, previous researches only utilized one set of datasets with a uniform level of difficulty in the environment, which is not appropriate for HoK, where the difficulty of the task can be substantially affected by the level of opponents. Therefore, to examine the effects of varying levels of difficulty in the environment, we propose several multi-difficulty datasets with different difficulty levels. Specifically, we develop two sets of datasets: _norm_ and _hard_, which are categorized based on the opponent's level. Within each level, we propose four datasets according to diverse win rates against the opponent: _poor, medium, expert_ and _mixed_. To elaborate, the _poor/medium/expert_ dataset is generated by recording the battle trajectories of a relative _lower/equal/higher_ level model compared to the opponent, and the _mixed_ dataset is an equal mixture of the three datasets mentioned above.

_Multi-Task_

As a MOBA game, HoK features a diverse cast of heroes with distinct roles and skillsets. While the overall objective remains consistent throughout matches, the selection of heroes can significantly alter the nature of the task at hand. Consequently, HoK presents multi-task challenge which requires a single model to handle multiple tasks [47; 24; 46]. However, none of the current works provide uniform datasets for multi-task offline RL. To address this research gap, we propose a series of multi-task datasets based on the multi-task nature of HoK and evaluate the multi-task learning ability of current offline RL and offline MARL algorithms. Specifically, we define a hero pool with several heroes and randomly select heroes from it to sample data. Depending on whether the selected heroes are on the controlled side or the opponent side, we sample either the _multi_hero_ or the _multi_oppo_ dataset. In cases where both sides choose random heroes, we sample the _multi_hero_oppo_ dataset.

Furthermore, as mentioned in the previous section, different levels of opponents naturally form multiple tasks with varying environmental difficulties. Thus, we propose several level-based multi-task datasets by sampling data with randomly selected opponent levels. According to different difficulty levels, we have proposed two datasets, named _norm_multi_level_ and _hard_multi_level_.

_Generalization_

The unique gameplay mechanics of HoK, characterized by a diverse cast of heroes with distinct roles and skillsets, lend themselves well to multi-task and serve as an ideal testbed for evaluating the generality of models across a range of tasks. Building on the previous work  and taking into account the realities of human combat in HoK, we have identified three key challenges for generalization: hero generalization, opponent generalization, and level generalization.

We have developed six experiments: "norm_general" and "hard_general" for level generalization, "norm_hero_general" and "hard_hero_general" for hero generalization, and "norm_oppo_general" and "hard_oppo_general" for opponent generalization, for HoK1v1 and HoK3v3, respectively. Among them, the first two experiments, "norm_general" and "hard_general," have their corresponding datasets, and we train models on these datasets. The latter four experiments do not require extra datasets because we directly use the existing models that have already been trained using other datasets. For more details on the design of generalization, please refer to the Appendix C.

_Heterogeneous Teammate_

Heterogeneous teammate is a crucial research direction in MARL . In the practical scenarios of HoK, the capacity of each player is generally different, making it naturally suitable for investigating the challenges associated with heterogeneous teammate. In order to mimic real-world scenarios and facilitate research on heterogeneous teammate challenges, we design two datasets in HoK3v3: _norm_stupid_partner_ and _norm_expert_partner_. These datasets were collected in a standard manner, with the exception that one random hero in each team is controlled by a model with a relatively low/high level of expertise, while the remaining heroes are controlled by the regular model.

_Sub-Task_

As introduced in Sec 4.2.3, we designed several practical and meaningful sub-tasks to provide diverse scenarios based on HoK. Based on these sub-tasks, we proposed diverse datasets to support Offline RL research similar to the design of previous studies .

#### 4.3.1 Datasets Details

Table 1, Table 2 and Table 3(presents the details of our proposed datasets. All the datasets are sampled using checkpoints with different levels as introduced in Sec. 4.1.1. Typically, each dataset consists of 1000 trajectories, except for the sub-task datasets, which contain 100 trajectories. The default heroes chosen for both camps are _luban_ with Summor Spells set to _frenzy_ in HoK1v1 and _{[zhaoyun], [diacchan], [liyuanfang]_ with Summor Spells assigned as _{[smite], [purify], [purify]}_ based on their respective roles in HoK3v3. However, in specific scenarios such as _Generalization_ or _Multi-Task_ settings, we employ a random selection of heroes from a predefined set, _multi_hero_. For the HoK1v1 mode, the set comprises five heroes, _{luban, direnjie, houyi, makeboluo, gongsunli}_. In HoK3v3, the set consists six heroes, with two heroes assigned to each role, namely _{[zhaoyun, zhongwuyan], {diaochan, zhugeliang}, {liyuanfang, sunshangxiang}_}_. The win rate of the behavior policy is recorded in the column labeled _Win_rate_ for reference. The column labeled _Levels_ denotes the levels of opponents used for evaluation. More details of the datasets are presented in Appendix C.

  
**Factors** & **Datasets/Experiments** & **Capacity** & **Heroes** & **Oppo_heroes** & **Win_rate** & **Levels** \\   & **norm_poor** & 1000 & default & default & 12\% & 1 \\  & **norm_medium** & 1000 & default & default & 50\% & 1 \\  & **norm_expert** & 1000 & default & default & 88\% & 1 \\  & **norm_mixed** & 1000 & default & default & 50\% & 1 \\  & **hard_poor** & 1000 & default & default & 6\% & 5 \\  & **hard_medium** & 1000 & default & default & 50\% & 5 \\  & **hard_expert** & 1000 & default & default & 84\% & 5 \\  & **hard_mixed** & 1000 & default & default & 45\% & 5 \\   & **hard_general** & 1000 & default & default & 90\% & 5 \\  & **norm_general** & 1000 & default & default & 46\% & 1 \\  & **norm_hero_general** & - & multi_hero & default & - & 1 \\  & **hard_hero_general** & - & multi_hero & default & - & 5 \\  & **norm_oppo_general** & - & default & multi_hero & - & 1 \\  & **hard_oppo_general** & - & default & multi\_hero & - & 5 \\   & **norm_multi_level** & 1000 & default & default & 50\% & 1 \\  & **hard_multi_level** & 1000 & default & default & 50\% & 5 \\   & **norm_multi_hero** & 1000 & multi_hero & default & 23\% & 1 \\   & **norm_multi_oppo** & 1000 & default & multi\_hero & 77\% & 1 \\   & **norm_multi_hero_oppo** & 1000 & multi\_hero & multi\_hero & 50\% & 1 \\   

Table 1: Details of datasets in HoK1v1 game mode 

## 5 Benchmarking

Based on our framework, we reproduce various Offline RL and Offline MARL algorithms. Besides, we fully validate and compare these baselines on our datasets. The results are presented in the form of test winning rate. Each algorithm is run for three random seeds, and we report the mean performance with standard deviation. The performance of behaviour policies is presented in Appendix C. Details of the implementations and experimental results can be referenced in Appendix G.

### Baselines

#### 5.1.1 HoK1v1

The Offline RL baseline algorithms we implement are briefly introduced below: **BC**: Behavior cloning. **TD3+BC**: One of the state-of-the-art single agent offline algorithm, simply adding the BC term to TD3 . **CQL**: Conservative Q-Learning conducts conservative value iteration by adding a regularizer to the critic loss. **IQL**: Implicit Q-Learning leverages upper expectile value function to learn Q-function and extracts policy via advantage-weighted behavioral cloning.

The structured action space in HoK is similar to the joint action space in multi-agent settings, which inspires us to resort to the design in MARL methods. We propose a novel baseline algorithm, named **QMIX+CQL**. Specifically, we import QMIX algorithm from the MARL literature  to tackle the structured action space by regarding each head of the action space as a single agent and incorporate CQL regularizer term into local Q-funtion in QMIX for offline learning.

  
**Factors** & **Datasets/Experiments** & **Capacity** & **Heroes** & **Oppo\_heroes** & **Win\_rate** & **Levels** \\   & **norm\_poor** & 1000 & default & default & 16\% & 1 \\  & **norm\_medium** & 1000 & default & default & 50\% & 1 \\  & **norm\_expert** & 1000 & default & default & 82\% & 1 \\  & **norm\_mixed** & 1000 & default & default & 49\% & 1 \\  & **hard\_poor** & 1000 & default & default & 18\% & 7 \\  & **hard\_medium** & 1000 & default & default & 50\% & 7 \\  & **hard\_expert** & 1000 & default & default & 83\% & 7 \\  & **hard\_mixed** & 1000 & default & default & 51\% & 7 \\   & **hard\_general** & 1000 & default & default & 94\% & 8 \\  & **norm\_general** & 1000 & default & default & 57\% & 5 \\  & **norm\_hero\_general** & - & multi\_hero & default & - & 1 \\  & **hard\_hero\_general** & - & multi\_hero & default & - & 7 \\  & **norm\_opo\_general** & - & default & multi\_hero & - & 1 \\  & **hard\_oppo\_general** & - & default & multi\_hero & - & 7 \\   & **norm\_multi\_level** & 1000 & default & default & 50\% & 1 \\  & **hard\_multi\_level** & 1000 & default & default & 50\% & 7 \\  & **norm\_multi\_hero** & 1000 & multi\_hero & default & 74\% & 1 \\  & **norm\_multi\_oppo** & 1000 & default & multi\_hero & 26\% & 1 \\  & **norm\_multi\_hero\_oppo** & 1000 & multi\_hero & multi\_hero & 50\% & 1 \\   & **norm\_stupid\_partner** & 1000 & default & default & 50\% & 1 \\  & **norm\_expert\_partner** & 1000 & default & default & 50\% & 1 \\  & **norm\_mixed\_partner** & 1000 & default & default & 50\% & 1 \\   

Table 2: Details of datasets in HoK3v3 game mode

  
**Sub-Task** & **Datasets/Experiments** & **Capacity** & **Heroes** & **Oppo\_heroes** & **Average Score** & **Levels** \\   & **destroy\_turret\_medium** & 100 & default & no & 0.55 & medium \\  & **destroy\_turret\_expert** & 100 & default & no & 1.00 & expert \\  & **destroy\_turret\_mixed** & 100 & default & no & 0.73 & - \\   & **gain\_gold\_medium** & 100 & default & no & 0.13 & medium \\  & **gain\_gold\_expert** & 100 & default & no & 1.04 & expert \\  & **gain\_gold\_mixed** & 100 & default & no & 0.58 & - \\   

Table 3: Details of datasets in _Sub-Tasks_

#### 5.1.2 HoK3v3

The Offline MARL baseline algorithms are briefly introduced below: **IND+BC**: Behavior cloning with independent learning paradigm. **IND+CQL**: Adopts an independent learning paradigm for multi-agent settings, using conservative Q-learning . **COMM+CQL**: Incorporate inter-agent communication based on IND+CQL. **IND+ICQ**: Implicit Constraint Q-learning with independent learning paradigm, which only uses insample data for value estimation to alleviate the extrapolation error. **MAICQ**: Multi-agent version of implicit constraint Q-learning by decomposed multi-agent joint-policy under implicit constraint with CTDE paradigm. **OMAR**: Using zeroth-order optimization for better coordination among agents' policies, based on independent CQL.

### Benchmark Results

We have validated the offline RL and offline MARL baselines on our datasets and aggregated the results in Table 4 and Table 5.

\(\)**Baselines Comparison:** As indicated in Table 4, QMIX+CQL exhibits superior performance in comparison to other approaches, implying that the integration of MARL methods may be a suitable choice for environments with a structured action space. Moreover, in HoK3v3, IND+ICQ exhibits the highest performance across most datasets, except for the _Heterogeneous_ datasets. Conversely, algorithms based on TD3, namely **TD3+BC** and **OMAR**, yield poor results.

\(\)**Multi-Difficulty:** The baseline performance exhibits a significant decrease on the _hard-level_ datasets compared with _norm-level_ datasets, highlighting the limitations of current offline methods in addressing challenging tasks with discrete action space.

\(\)**Generalization:** The disparities between training and evaluation in _Generalization_ settings impede the achievement of desirable performance, indicating the inadequacy of current methods' generalization ability.

\(\)**Multi-Task:** Training models on _Multi-Task_ datasets results in a substantial performance enhancement compared to generalization settings. However, none of these models have been able to exceed the performance achieved by the behavior policy, underscoring the need for further research into the direct application of offline methods to multiple tasks.

  
**Factors** & **Datasets** & **BC** & **CQL** & **QMIX+CQL** & **IQL** & **TD3+BC** \\   & **norm\_poor** & **0.08±0.02** & 0.06±0.01 & **0.08±0.02** & 0.07±0.01 & 0.0±0.0 \\  & **norm\_medium** & **0.33±0.01** & 0.32±0.01 & 0.31±0.03 & 0.32±0.01 & 0.01±0.01 \\  & **norm\_expert** & 0.64±0.01 & 0.58±0.03 & **0.67±0.01** & 0.62±0.02 & 0.03±0.01 \\  & **norm\_mixed** & 0.17±0.01 & 0.23±0.04 & 0.20±0.01 & **0.25±0.01** & 0.01±0.01 \\  & **hard\_poor** & 0.01±0.01 & 0.01±0.01 & 0.01±0.01 & 0.01±0.00 & 0.00±0.00 \\  & **hard\_medium** & 0.13±0.01 & 0.11±0.01 & **0.20±0.01** & 0.12±0.02 & 0.00±0.00 \\  & **hard\_expert** & 0.33±0.01 & 0.30±0.01 & **0.44±0.05** & 0.34±0.04 & 0.00±0.00 \\  & **hard\_mixed** & 0.05±0.3 & 0.02±0.01 & **0.08±0.01** & 0.06±0.01 & 0.01±0.01 \\   & **norm\_general** & 0.19±0.01 & 0.20±0.04 & **0.32±0.03** & 0.18±0.01 & 0.02±0.02 \\  & **hard\_general** & 0.04±0.01 & 0.03±0.01 & **0.08±0.02** & 0.02±0.01 & 0.00±0.00 \\  & **norm\_hero\_general** & 0.06±0.01 & 0.06±0.01 & **0.08±0.01** & 0.07±0.01 & 0.00±0.00 \\  & **hard\_hero\_general** & 0.03±0.01 & 0.03±0.01 & 0.04±0.01 & **0.06±0.01** & 0.00±0.00 \\  & **norm\_opp\_general** & **0.58±0.03** & 0.52±0.04 & 0.42±0.22 & 0.51±0.07 & 0.12±0.01 \\  & **hard\_oppo\_general** & 0.15±0.02 & 0.12±0.03 & **0.23±0.04** & 0.14±0.03 & 0.01±0.01 \\   & **norm\_multi\_level** & 0.32±0.03 & 0.25±0.03 & **0.41±0.02** & 0.30±0.02 & 0.02±0.01 \\  & **hard\_multi\_level** & 0.08±0.02 & 0.06±0.01 & **0.16±0.03** & 0.08±0.02 & 0.00±0.00 \\  & **norm\_multi\_rero** & 0.08±0.01 & 0.07±0.02 & **0.11±0.01** & 0.06±0.01 & 0.00±0.00 \\  & **norm\_multi\_opp** & 0.59±0.02 & 0.55±0.03 & **0.65±0.02** & 0.60±0.05 & 0.10±0.02 \\  & **norm\_multi\_rero\_opp** & 0.26±0.01 & 0.21±0.02 & **0.32±0.03** & 0.28±0.05 & 0.03±0.01 \\   & **destroy\_turret\_medium** & 0.61±0.06 & 0.63±0.01 & 0.61±0.03 & 0.60±0.02 & **0.67±0.03** \\  & **destroy\_turret\_expert** & 0.94±0.02 & 0.94±0.02 & 0.92±0.05 & **0.95±0.01** & 0.57±0.13 \\   & **destroy\_turret\_mixed** & 0.88±0.04 & 0.87±0.03 & **0.89±0.02** & **0.89±0.04** & 0.82±0.03 \\   

Table 4: Averaged test winning rate or normalized score (_Sub-Task_) of baselines in HoK1v1 game mode.

* **Heterogeneous:** As expected, the presence of a low-ability partner can disrupt cooperation and hinder offline learning on the _stupid_partner_ datasets, whereas an expert partner has the opposite effect, highlighting the limitations of existing research on heterogeneous offline MARL.
* **Sub-Task:** The offline baselines exhibit robust performance in the _Sub-Task_ at a low training cost. Additionally, BC demonstrates a competitive capability as well.
* **Ablations of learning paradigms:** We conduct ablation experiments to investigate the impact of communication and the CTDE paradigm. Specifically, from the comparison of COMM-CQL and IND-CQL, we can reveal that incorporating communication generally results in better performance due to the promotion of cooperation. Surprisingly, we found that the independent paradigm (IND-ICQ) outperformed the CTDE paradigm (MAICQ), which may be attributed to the challenges in the CTDE paradigm associated with credit assignment of agents with distinct rewards and roles.

## 6 Conclusion

In this paper, taking into account the limitations of existing offline RL datasets about practical applications, we introduce Hokoff, based on Honor of Kings, a well-known MOBA game that offers a high level of complexity for simulating real-world scenarios. We present a comprehensive framework for conducting research in offline RL and release a diverse and extensive collection of datasets, incorporating various levels of difficulty and a range of research factors. Moreover, the chosen tasks for dataset collection not only cater to Offline RL but also serve the purpose of offline MARL. We replicate multiple offline RL and offline MARL algorithms and thoroughly validate these baselines on our datasets. The obtained results highlight the shortcomings of existing Offline RL methods, underscoring the necessity for further research in areas such as challenging task settings, generalization capabilities, and multi-task learning. All components, including the framework, datasets, and baseline implementations, discussed in this paper are fully open-source.