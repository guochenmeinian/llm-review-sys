# Self-Refine:

Iterative Refinement with Self-Feedback

 Aman Madaan\({}^{1}\), Niket Tandon\({}^{2}\), Prakhar Gupta\({}^{1}\), Skyler Hallinan\({}^{3}\), Luyu Gao\({}^{1}\),

**Sarah Wiegreffe\({}^{2}\), Uri Alon\({}^{1}\), Nouha Dziri\({}^{2}\), Shrimai Prabhumoye\({}^{4}\), Yiming Yang\({}^{1}\), Shashank Gupta\({}^{2}\), Bodhisattwa Prasad Majumder\({}^{5}\), Katherine Hermann\({}^{6}\), Sean Welleck\({}^{2,3}\), Amir Yazdanbakhsh\({}^{6}\), Peter Clark\({}^{2}\)**

\({}^{1}\)Language Technologies Institute, Carnegie Mellon University

\({}^{2}\)Allen Institute for Artificial Intelligence

\({}^{3}\)University of Washington \({}^{4}\)NVIDIA \({}^{5}\)UC San Diego \({}^{6}\)Google Deepmind

selfrefine@googlegroups.com

Now at Google DeepMind

###### Abstract

Like humans, large language models (llms) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from llms through iterative feedback and refinement. The main idea is to generate an initial output using an llm; then, the same llm provides _feedback_ for its output and uses it to _refine_ itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single llm as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (gpt-3.5 and GPT-4) llms. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same llm using conventional one-step generation, improving by \(\)20% absolute on average in task performance. Our work demonstrates that even state-of-the-art llms like GPT-4 can be further improved at test-time using our simple, standalone approach.2.

## 1 Introduction

Although large language models (llms) can generate coherent outputs, they often fall short in addressing intricate requirements. This mostly includes tasks with multifaceted objectives, such as dialogue response generation, or tasks with hard-to-define goals, such as enhancing program readability. In these scenarios, modern llms may produce an intelligible initial output, yet may benefit from further iterative refinement--i.e., iteratively mapping a candidate output to an improved one--to ensure that the desired quality is achieved. Iterative refinement typically involves training a refinement model that relies on domain-specific data (e.g., Reid and Neubig (2022); Schick et al. (2022); Welleck et al. (2022)). Other approaches that rely on external supervision or reward models require large training sets or expensive human annotations (Madaan et al., 2021; Ouyang et al., 2022), which may not always be feasible to obtain. These limitations underscore the need for an effective refinement approach that can be applied to various tasks without requiring extensive supervision.

Iterative _self_-refinement is a fundamental characteristic of human problem-solving (Simon, 1962; Flower and Hayes, 1981; Amabile, 1983). Iterative self-refinement is a process that involves creating an initial draft and subsequently refining it based on self-provided feedback. For example, when

[MISSING_PAGE_FAIL:2]

For example, in Figure 2(d), the model generates functionally correct code for the given input. Here, \(p_{}\) is a task-specific few-shot prompt (or instruction) for an initial generation, and \(\|\) denotes concatenation. The few-shot prompt contains input-output pairs \( x^{(k)},y^{(k)}\) for the task.3

feedbackNext, Self-Refine uses the same model \(\) to provide feedback \(fb_{t}\) on its own output, given a task-specific prompt \(p_{}\) for generating feedback:

\[fb_{t}=(p_{}\|x\|y_{t}).\] (2)

Intuitively, the feedback may address multiple aspects of the output. For example, in code optimization, the feedback might address the efficiency, readability, and overall quality of the code.

Figure 3: The Self-Refine algorithm. See (ยง2) for a discussion of each component.

Figure 2: Examples of Self-Refine: an initial output generated by the base llm and then passed back to the _same_ llm to receive feedback to the _same_ llm to refine the output. The top row illustrates this for dialog generation where an initial dialogue response can be transformed into a more engaging one that also understands the user by applying feedback. The bottom row illustrates this for code optimization where the code is made more efficient by applying feedback.

Here, the prompt \(p_{}\) provides examples of feedback in the form of input-output-feedback triples \( x^{(k)},y^{(k)},fb^{(k)}\). We prompt the model to write feedback that is actionable and specific via \(fb^{(k)}\). By 'actionable', we mean the feedback should contain a concrete action that would likely improve the output. By'specific', we mean the feedback should identify concrete phrases in the output to change. For example, the feedback in Figure 2(e) is _"This code is slow as it uses a for loop which is brute force. A better approach is to use the formula... (n(n+1))/2"_. This feedback is actionable, since it suggests the action 'use the formula...'. The feedback is specific since it mentions the 'for loop'.

refineNext, Self-Refine uses \(\) to refine its most recent output, given its own feedback:

\[y_{t+1}=(p_{}\|x\|y_{t}\|fb_{t}).\] (3)

For example, in Figure 2(f), given the initial output and the generated feedback, the model generates a re-implementation that is shorter and runs much faster than the initial implementation. The prompt \(p_{}\) provides examples of improving the output based on the feedback, in the form of input-output-feedback-refined quadruples \( x^{(k)},y_{t}^{(k)},fb_{t}^{(k)},y_{t+1}^{(k)}\).

Iterating Self-Refine Self-Refine alternates between feedback and refine steps until a stopping condition is met. The stopping condition \((fb_{t},t)\) either stops at a specified timestep \(t\), or extracts a stopping indicator (e.g. a scalar stop score) from the feedback. In practice, the model can be prompted to generate a stopping indicator in \(p_{}\), and the condition is determined per-task.

To inform the model about the previous iterations, we retain the history of previous feedback and outputs by appending them to the prompt. Intuitively, this allows the model to learn from past mistakes and avoid repeating them. More precisely, Equation (3) is in fact instantiated as:

\[y_{t+1}=(p_{}\|x\|y_{0}\|fb_{0}\|...\|y_{t}\|fb_{ t}).\] (4)

Finally, we use the last refinement \(y_{t}\) as the output of Self-Refine.

Algorithm 1 summarizes Self-Refine, and Figure 2 shows an example of Self-Refine in the Dialogue Response Generation (Mehri and Eskenazi, 2020) and Code Optimization (Madaan et al., 2023) tasks. Appendix V provides examples of the \(p_{}\), \(p_{}\), \(p_{}\) prompts for various tasks. The key idea is that Self-Refine uses the same underlying llm to generate, get feedback, and refine its outputs given its own feedback. It relies only on supervision present in the few-shot examples.

## 3 Evaluation

We evaluate Self-Refine on 7 diverse tasks: Dialogue Response Generation (Appendix P; Mehri and Eskenazi, 2020), Code Optimization (Appendix Q; Madaan et al., 2023), Code Readability Improvement (Appendix O; Puri et al., 2021), Math Reasoning (Appendix R; Cobbe et al., 2021), Sentiment Reversal (Appendix S; Zhang et al., 2015), and we introduce two new tasks: Aeronym Generation (Appendix T) and Constrained Generation (a harder version of Lin et al. (2020) with 20-30 keyword constraints instead of 3-5; Appendix U)

Examples for all tasks and dataset statistics are provided in Table 4 (Appendix A).

### Instantiating Self-Refine

We instantiate Self-Refine following the high-level description in Section 2. The feedback-refine iterations continue until the desired output quality or task-specific criterion is reached, up to a maximum of 4 iterations. To make our evaluation consistent across different models, we implemented both feedback and refine as few-shot prompts even with models that respond well to instructions, such as chatgpt and gpt-4.

Base LLMsOur main goal is to evaluate whether we can improve the performance of any strong base LLMs using Self-Refine. Therefore, we compare Self-Refine to the same base LLMs but without feedback-refine iterations. We used three main strong base LLM across all tasks: gpt-3.5 (text-davinci-003), chatgpt (gpt-3.5-turbo), and gpt-4 (OpenAI, 2023). For code-based tasks, we also experimented with CodEx (code-davinci-002). In all tasks, either gpt-3.5 or gpt-4 is the previous state-of-the-art.4 We used the same prompts from previous work when available (such as for Code Optimization and Math Reasoning); otherwise, we created prompts as detailed in Appendix V. We generate samples using a temperature of 0.7.

### Metrics

We report three types of metrics:

* Task specific metric: When available, we use automated metrics from prior work (Math Reasoning: % solve rate; Code Optimization: % programs optimized%).
* gpt-4-pref: In addition to human-pref, we use gpt-4 as a proxy for human preference following prior work (Fu et al., 2023; Chiang et al., 2023; Geng et al., 2023; Sun et al., 2023), and found high correlation (82% for Sentiment Reversal, 68% for Acronym Generation, and 71% for Dialogue Response Generation) with human-pref. For Code Readability Improvement, we prompt gpt-4 to calculate fraction of the variables that are appropriately named given the context (e.g., x = [] \(\) input_buffer = []). Additional details are provided in Appendix F. For constrained generation, we combine automated evaluation to quantify concept coverage and gpt-4-pref to ensure the commonsense correctness of generated sentences. A sentence is only deemed a winner if it maintains validity in commonsense reasoning and has greater coverage in terms of concepts.
* Human evaluation: In Dialogue Response Generation, Code Readability Improvement, Sentiment Reversal, and Acronym Generation, we additionally perform a blind human A/B evaluation on a subset of the outputs to select the preferred output. Additional details are provided in Appendix C.

### Results

Table 1 shows our main results:

**Self-Refine consistently improves over base models** across all model sizes, and additionally outperforms the previous state-of-the-art across all tasks. For example, gpt-4+Self-Refine improves over the base gpt-4 by 8.7% (absolute) in Code Optimization, increasing optimization percentage from 27.3% to 36.0%. Confidence intervals are provided in Appendix M. For code-based tasks, we found similar trends when using Codex; those results are included in Appendix H.

One of the tasks in which we observe the highest gains compared to the base models is Constrained Generation, where the model is asked to generate a sentence containing up to 30 given concepts. We believe that this task benefits significantly from Self-Refine because there are more opportunities to miss some of the concepts on the first attempt, and thus Self-Refine allows the model to fix these mistakes subsequently. Further, this task has an extremely large number of reasonable outputs, and thus Self-Refine allows to better explore the space of possible outputs.

In preference-based tasks such as Dialogue Response Generation, Sentiment Reversal, and Acronym Generation, Self-Refine leads to especially high gains. For example in Dialogue Response Generation, gpt-4 preference score improve by 49.2% - from 25.4% to 74.6%. Similarly, we see remarkable improvements in the other preference-based tasks across all models.

    &  &  &  \\  Task & Base & +Self-Refine & Base & +Self-Refine & Base & +Self-Refine \\  Sentiment Reversal & 8.8 & **30.4** (\(\)21.6) & 11.4 & **43.2** (\(\)31.8) & 3.8 & **36.2** (\(\)32.4) \\ Dialogue Response & 36.4 & **63.6** (\(\)27.2) & 40.1 & **59.9** (\(\)19.8) & 25.4 & **74.6** (\(\)49.2) \\ Code Optimization & 14.8 & **23.0** (\(\)8.2) & 23.9 & **27.5** (\(\)3.6) & 27.3 & **36.0** (\(\)8.7) \\ Code Readability & 37.4 & **51.3** (\(\)13.9) & 27.7 & **63.1** (\(\)35.4) & 27.4 & **56.2** (\(\)28.8) \\ Math Reasoning & **64.1** & **64.1** (\(\)0.7) & 74.8 & **75.0** (\(\)0.2) & 92.9 & **93.1** (\(\)0.2) \\ Acronym Generation & 41.6 & **56.4** (\(\)14.8) & 27.2 & **37.2** (\(\)10.0) & 30.4 & **56.0** (\(\)25.6) \\ Constrained Generation & 16.0 & **39.7** (\(\)23.7) & 2.75 & **33.5** (\(\)30.7) & 4.4 & **61.3** (\(\)56.9) \\   

Table 1: Self-Refine results on various tasks using gpt-3.5, chatgpt, and gpt-4 as base LLM. Self-Refine consistently improves llM. Metrics used for these tasks are defined in Section 3.2.

The modest performance gains in Math Reasoning can be traced back to the inability to accurately identify whether there is any error. In math, errors can be nuanced and sometimes limited to a single line or incorrect operation. Besides, a consistent-looking reasoning chain can deceive llms to think that "everything looks good" (e.g., chatgpt feedback for 94% instances is 'everything looks good'). In Appendix K.1, we show that the gains with Self-Refine on Math Reasoning are much bigger (5%+) if an external source can identify if the current math answer is incorrect. Although Self-Refine demonstrates limited efficacy in Math Reasoning, we observe gains with Self-Refine in a subset of Big-Bench Hard (Suzgun et al., 2022) tasks that typically require a combination of commonsense reasoning and logic, such as date reasoning (Appendix D). This suggests that Self-Refine may be more effective in scenarios where the interplay of logical analysis and knowledge acquired through pre-training facilitates self-verification.

Improvement is consistent across base llms sizesGenerally, gpt-4+Self-Refine performs better than gpt-3.5+Self-Refine and chatgpt+Self-Refine across all tasks, even in tasks where the initial base results of gpt-4 were lower than gpt-3.5 or chatgpt. We thus believe that Self-Refine allows stronger models (such as gpt-4) to unlock their full potential, even in cases where this potential is not expressed in the standard, single-pass, output generation. Comparison to additional strong baselines is provided in Appendix H.

## 4 Analysis

The three main steps of Self-Refine are feedback, refine, and repeating them iteratively. In this section, we perform additional experiments to analyze the importance of each of these steps.

The impact of the feedback qualityFeedback quality plays a crucial role in Self-Refine. To quantify its impact, we compare Self-Refine, which utilizes specific, actionable feedback, with two ablations: one using generic feedback and another without feedback (the model may still iteratively refine its generations, but is not explicitly provided feedback to do so). For example, in the Code Optimization task: actionable feedback, such as _Avoid repeated calculations in the for loop_, pinpoints an issue and suggests a clear improvement. Generic feedback, like _Improve the efficiency of the code_, lacks this precision and direction. Table 2 shows feedback's clear influence.

In Code Optimization, performance slightly dips from 27.5 (Self-Refine feedback) to 26.0 (generic feedback), and further to 24.8 (no feedback). This suggests that while generic feedback offers some guidance - specific, actionable feedback yields superior results.

This effect is more pronounced in tasks like Sentiment Reversal, where changing from our feedback to generic feedback leads to a significant performance drop (43.2 to 31.2), and the task fails without feedback. In the "No feedback" setting, the model was not given clear instructions on changing the output. We find that the model tends to either repeat the same output in each iteration or to make unrelated changes. Since the scores in this task are the relative improvement increase in human preference, a score of 0 means that "No feedback" did not improve over the base model outputs. Similarly, in Acronym Generation, without actionable feedback, performance drops from 56.4 to 48.0, even with iterative refinements. These results highlight the importance of specific, actionable feedback in our approach. Even generic feedback provides some benefit, but the best results are achieved with targeted, constructive feedback.

How important are the multiple iterations of feedback-refine?Figure 4 demonstrates that on average, the quality of the output improves as the number of iterations increases. For instance, in

   Task & Self-Refine feedback & Generic feedback & No feedback \\  Code Optimization & **27.5** & 26.0 & 24.8 \\ Sentiment Reversal & **43.2** & 31.2 & 0 \\ Acronym Generation & **56.4** & 54.0 & 48.0 \\   

Table 2: Prompting to generate generic feedback (or having the model generate no feedback at all) leads to reduced scores, indicating the importance of the feedback step of Self-Refine. These experiments were performed with chatgpt (Code Optimization and Sentiment Reversal) and gpt-3.5 (Acronym Generation), and metrics used are defined in Section 3.2.

the Code Optimization task, the initial output (\(y_{0}\)) has a score of 22.0, which improves to 28.8 after three iterations (\(y_{3}\)). Similarly, in the Sentiment Reversal task, the initial output has a score of 33.9, which increases to 36.8 after three iterations. This trend of improvement is also evident in Constrained Generation, where the score increases from 26.1 to 46.1 after three iterations. Figure 4 highlights the diminishing returns in the improvement as the number of iterations increases. Overall, having multiple feedback-refine iterations significantly enhances the quality of the output, although the marginal improvement naturally decreases with more iterations.

The performance may not always monotonically increase with iterations: in multi-aspect feedback tasks like Acronym Generation, where the output quality can vary during iteration with improvement

Figure 4: **Left**: Iteration-wise score improvements. Early iterations significantly improve output quality, and scores generally keep improving with more iterations. **Right**: Self-Refine Performance improvements with iterations. Most gains(\(\)) are in the initial iterations for both Code Opt. and Sentiment Reversal. The numbers are averaged over chatgpt, gpt-3.5, and gpt-4. Task abbreviations: C. Opt. (Code Optimization), S. Rev. (Sentiment Reversal), C. Gen. (Constrained Generation).

Figure 5: Comparison of code generated by Madaan et al. (2023) (left) and the output after applying Self-Refine (right). The initial code by the baseline, which is nearly identical to the slower input program, fails to improve the efficiency and merely alters the logic for reading input. Self-Refine first generates feedback that diagnoses that _This code is slow because it is using six nested loops to iterate through all possible combinations of coins to pay the amount,_ and suggests that _a more efficient approach would be..._. Self-Refine then uses this feedback to generate the revised code (right), reducing the time complexity to \((amount*coins)\). The full example is provided in Appendix K

in one aspect but decline in another aspect. To counter this, Self-Refine generates numerical scores for different quality aspects, leading to a balanced evaluation and appropriate output selection.

Can we just generate multiple outputs instead of refining?Does Self-Refine improve because of the iterative refinement, or just because it generates _more_ outputs? We compare Self-Refine with chatgpt, when chatgpt generates \(k=4\) samples (but without feedback and refinement). Then, we compare the performance of Self-Refine against these \(k\) initial outputs in a 1 vs. \(k\) evaluation. In other words, we assess whether Self-Refine can outperform _all_\(k\) initial outputs. The results of this experiment are illustrated in Figure 7 (Appendix K). Despite the increased difficulty of the 1 vs. \(k\) setting, the outputs of Self-Refine are still preferred by humans over _all_\(k\) initial outputs. This shows the importance of refinement according to feedback over the alternative of just generating multiple initial outputs.

Does Self-Refine works in an instruction only setup?In our main experiments, we use few-shot prompting to guide model output into a more readily parseable format. Next, we experiment with Self-Refine under a zero-shot prompting scenario, where traditional few-shot examples are supplanted by explicit instructions at each stage of the Self-Refine process. For these experiments, we use chatgpt. The results (Appendix E in Table 8) show that Self-Refine remains effective across diverse tasks, even in the absence of example prompts. Notably, in tasks such as Acronym Generation and Sentiment Reversal, Self-Refine, under zero-shot prompting, enhances performance from 16.6% to 44.8% and 4.4% to 71.4%, respectively. However, achieving optimal performance in this setting requires extensive prompt engineering for instructions.

For Math Reasoning tasks, Self-Refine improves the solve rate from 22.1% to 59.0% in an instruction-only setting. We find that much of this gain comes from fixing omitted return statements in 71% of the initial Python programs, despite clear instructions to include them. Subsequent iterations of feedback generation and refinement address this issue effectively, decreasing the error rate by 19%. Further, we find that when the initial programs are valid, Self-Refine does not improve the performance.

Does Self-Refine work with weaker models?The experiments in Section 3.3 were performed with some of the strongest available models; does Self-Refine work with smaller or weaker models as well? To investigate this, we instantiated Self-Refine with Vicuna-13B (Chiang et al., 2023), a less powerful base model. While Vicuna-13B is capable of generating initial outputs, it struggles significantly with the refinement process. Specifically, Vicuna-13B was not able to consistently generate the feedback in the required format. Furthermore, even when provided with Oracle or hard-coded feedback, it often failed to adhere to the prompts for refinement. Instead of refining its output, Vicuna-13B either repeated the same output or generated a hallucinated conversation, rendering the outputs less effective. Example output and analysis is provided in Appendix I.

How does Self-Refine perform with strong open access models like llama2-70b?We conduct additional experiments using Self-Refine on LLama-2 (Touvron et al., 2023), a state-of-the-art, open-access language model on Acronym Generation, Sentiment Reversal, Dialogue Response Generation, and Math Reasoning. Consistent with our primary findings, Self-Refine shows an improvement across all these tasks relative to the base model. The full results are shown in Appendix J. These promising results with llama2-70b suggest that the applicability of Self-Refine might extend to a wide array of increasingly powerful open-source models in the future

Qualitative AnalysisWe conduct a qualitative analysis of the feedback generated by Self-Refine and its subsequent refinements. We manually analyze 70 samples in total (35 success cases and 35 failure cases) for Code Optimization (Madaan et al., 2023) and Math Reasoning (Cobbe et al., 2021). For both Math Reasoning and Code Optimization, we found that the feedback was predominantly actionable, with the majority identifying problematic aspects of the original generation and suggesting ways to rectify them.

When Self-Refine failed to improve the original generation, the majority of issues were due to erroneous feedback rather than faulty refinements. Specifically, 33% of unsuccessful cases were due to feedback inaccurately pinpointing the error's location, while 61% were a result of feedback suggesting an inappropriate fix. Only 6% of failures were due to the refiner incorrectly implementing good feedback. These observations highlight the vital role of accurate feedback plays in Self-Refine.

In successful cases, the refiner was guided by accurate and useful feedback to make precise fixes to the original generation in 61% of the cases. Interestingly, the refiner was capable of rectifying issues even when the feedback was partially incorrect, which was the situation in 33% of successful cases. This suggests resilience to sub-optimal feedback. Future research could focus on examining the refiner's robustness to various types of feedback errors and exploring ways to enhance this resilience. In Figure 5, we illustrate how Self-Refine significantly improves program efficiency by transforming a brute force approach into a dynamic programming solution, as a result of insightful feedback. Additional analysis on other datasets such as Dialogue Response Generation is provided in Appendix K.

Going Beyond BenchmarksWhile our evaluation focuses on benchmark tasks, Self-Refine is designed with broader applicability in mind. We explore this in a real-world use case of website generation, where the user provides a high-level goal and Self-Refine assists in iteratively developing the website. Starting from a rudimentary initial design, Self-Refine refines HTML, CSS, and JS to evolve the website in terms of both usability and aesthetics. This demonstrates the potential of Self-Refine in real-world, complex, and creative tasks. See Appendix L for examples and further discussion, including broader, societal impact of our work.

## 5 Related work

Leveraging human- and machine-generated natural language (NL) feedback for refining outputs has been effective for a variety of tasks, including summarization (Scheurer et al., 2022), script generation (Tandon et al., 2021), program synthesis (Le et al., 2022; Yasunaga and Liang, 2020), and other tasks (Madaan et al., 2022; Bai et al., 2022; Schick et al., 2022; Saunders et al., 2022; Bai et al., 2022; Welleck et al., 2022). Refinement methods differ in the source and format of feedback, and the way that a refiner is obtained. Table 3 summarizes some related approaches; see Appendix B for an additional discussion.

Source of feedback.Humans have been an effective source of feedback (Tandon et al., 2021; Elgohary et al., 2021; Tandon et al., 2022; Bai et al., 2022). Since human feedback is costly, several approaches use a scalar reward function as a surrogate of (or alternative to) human feedback (e.g., (Bai et al., 2022; Liu et al., 2022; Lu et al., 2022; Le et al., 2022; Welleck et al., 2022)). Alternative sources such as compilers (Yasunaga and Liang, 2020) or Wikipedia edits (Schick et al., 2022) can provide domain-specific feedback. Recently, LLMs have been used to generate feedback for general domains (Fu et al., 2023; Peng et al., 2023; Yang et al., 2022), However, ours is the only method that generates feedback using an llm on its _own_ output, for the purpose of refining with the same llm.

Representation of feedback.The form of feedback can be generally divided into natural language (NL) and non-NL feedback. Non-NL feedback can come in human-provided example pairs (Dasupta et al., 2019) or scalar rewards (Liu et al., 2022; Le et al., 2022). In this work, we use NL feedback, since this allows the model to easily provide _self_-feedback using the same LM that generated the output, while leveraging existing pretrained LLMs such as GPT-4.

Types of refiners.Pairs of feedback and refinement have been used to learn supervised refiners (Schick et al., 2022; Du et al., 2022; Yasunaga and Liang, 2020; Madaan et al., 2021). Since

 p{113.8pt} p{113.8pt} p{113.8pt}}    & Supervision-free refiner & Supervision-free feedback & Multi-aspect feedback & Iterative feedback \\ 
**Learned refiners**: PEER (Schick et al., 2022), Self-critique (Saunders et al., 2022), CodeRL (Le et al., 2022), Self-correction (Welleck et al., 2022). & \(\) or \(\) & \(\) or \(\) \\ 
**Prompted refiners**: Augmenter (Peng et al., 2023), Re\({}^{3}\)(Yang et al., 2022), Reflexion (Shinn et al., 2023). & \(\) & \(\) & \(\) \\ 
**Self-Refine** (this work) & \(\) & \(\) & \(\) & \(\) \\   

Table 3: A comparison of Self-Refine to closely related prior refinement approaches.

gathering supervised data is costly, some methods learn refiners using model generations (Welleck et al., 2022; Peng et al., 2023). However, the refiners are trained for each new domain. Finally, (Yang et al., 2022) use prompted feedback and refinement specifically tailored for story generation. In this work, we avoid training a separate refiner, and show that the same model can be used as both the refiner and the source of feedback across multiple domains.

Non-refinement reinforcement learning (RL) approaches.Rather than having explicit refinement, an alternative way to incorporate feedback is by optimizing a scalar reward function, e.g. with reinforcement learning (e.g., Stiennon et al. (2020); Lu et al. (2022); Le et al. (2022)). These methods differ from Self-Refine in that the model does not access feedback on an intermediate generation. Second, these RL methods require updating the model's parameters, unlike Self-Refine. Recently, in discrete-space simulated environments, LLMs have also been shown to iteratively shape and refine rewards and policies, thereby performing RL tasks without expert demonstrations or gradients (Kim et al., 2023; Brooks et al., 2023). While we focus on real-world code and language tasks in this paper, it would be interesting to explore applications of self-refine in simulated environments.

## 6 Limitations and Discussion

The main limitation of our approach is that the base models need to have sufficient few-shot modeling or instruction-following abilities, in order to learn to provide feedback and to refine in an in-context fashion, without having to train supervised models and rely on supervised data.

Further, the experiments in this work were primarily performed with language models that are not open-sourced, namely gpt-3.5, chatgpt, gpt-4, and Codex. Existing literature (Ouyang et al., 2022) does not fully describe the details of these models, such as the pretraining corpus, model sizes, and model biases. Nonetheless, we release our code and model outputs to ensure the reproducibility of our work. In addition, initial results from our experiments with the open-access llama2-70b language model are promising, reinforcing the notion that Self-Refine has the potential to be widely applicable, even as open-source models continue to evolve and improve.

Another limitation of our work is that we exclusively experiment with datasets in English. In other languages, the current models may not provide the same benefits. Finally, there is a possibility for bad actors to use prompting techniques to steer a model to generate more toxic or harmful text. Our approach does not explicitly guard against this.

## 7 Conclusion

We present Self-Refine: a novel approach that allows large language models to iteratively provide self-feedback and refine their own outputs. Self-Refine operates within a single llm, requiring neither additional training data nor reinforcement learning. We demonstrate the simplicity and ease of use Self-Refine across a wide variety of tasks. By showcasing the potential of Self-Refine in diverse tasks, our research contributes to the ongoing exploration and development of large language models, with the aim of reducing the cost of human creative processes in real-world settings. We hope that our iterative approach will help drive further research in this area. To this end, we make all our code, data and prompts available at https://selfrefine.info/.