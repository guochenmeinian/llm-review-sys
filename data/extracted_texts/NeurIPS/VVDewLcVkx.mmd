# GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Large language models have demonstrated remarkable few-shot performance on many natural language understanding tasks. Despite several demonstrations of using large language models in complex, strategic scenarios, there lacks a comprehensive framework for evaluating agents' performance across various types of reasoning found in games. To address this gap, we introduce GameBench, a cross-domain benchmark for evaluating strategic reasoning abilities of LLM agents. We focus on 9 different game environments, where each covers at least one axis of key reasoning skill identified in strategy games, and select games for which strategy explanations are unlikely to form a significant portion of models' pretraining corpuses. Our evaluations use GPT-3 and GPT-4 in their base form along with two scaffolding frameworks designed to enhance strategic reasoning ability: Chain-of-Thought (CoT) prompting and Reasoning Via Planning (RAP). Our results show that none of the tested models match human performance, and at worse GPT-4 performs worse than random action. CoT and RAP both improve scores but not to comparable human levels. Benchmark code is available at https://anonymous.4open.science/r/GameBench-5942/.

## 1 Introduction

Capabilities of large language models have seen rapid progress, enabling LLMs to be used in agentic tasks . This presents opportunities for LLM-based tools to assist humans in several domains, such as API usage , web browsing  and coding . Recent benchmarks have been introduced for evaluating performance on real-world agent tasks , , , , with some focused on reasoning  and games . However, these existing benchmarks are oriented to practical, in-distribution knowledge, which can quickly become saturated with better models.

In particular, strategic reasoning is an agentic task that is important for generalising to new contexts, as it involves optimising for an objective in the face of possibly divergent interests of others, where incentives may not be fully known . Prior work on reasoning scaffolds also shows that language models have potential to grasp reasoning skills across scenarios . Hence, a strategic reasoning benchmark for LLMs, that is inherently multi-agent, would be difficult to saturate. Furthermore, games exemplify environments for demonstrating strategic behaviour in both humans and AI agents, as seen in the well known examples of Chess .

2017) and Go (Silver et al., 2016). Hence evaluating LLMs on several types of reasoning behaviours would present a comprehensive, fine-grained benchmark. As such, we introduce GameBench: a multi-player, cross-domain framework for evaluating strategic reasoning in LLM agents using games. We focus on both discrete and open-ended action spaces, across the reasoning domains of abstract strategy; non-deterministic outcomes; hidden information; language communication; social deduction and cooperation between players. By selecting for games without published strategy guides to our knowledge, we ensure that game-specific strategy has been sufficiently out-of-distribution in pretraining data. See Table 1 for a complete list of games and game properties.

The benchmark consists of obscure board games, card games, and social deception games. We evaluate gpt-3.5-turbo-1106 (GPT-3) and gpt-4-1106-preview (GPT-4) along with the CoT (Wei et al., 2022) and RAP (Hao et al., 2023) scaffolding techniques, by playing them against each other, a random-action-selector baseline, and a human baseline. We conducted a literature review and identified RAP to be the state-of-the-art scaffolding that fit the parameters of our benchmark, i.e. each agent has access to the same game state information and no agent can peek at future states. Agents are rated using the exponential Bradley-Terry model (Bradley and Terry, 1952). This has useful advantages over the typical Elo system (Elo, 1967), such as its assumption that each agent's ability is fixed and will not change between matches.

Our results show that CoT-augmented and RAP-augmented models demonstrate superior strategic superior to the random baseline; that GPT-3 matches the random baseline; that GPT-4 performs worse than the random baseline; and that the human baseline performs superior to all.

With this benchmark, we propose a means to measure the strategic reasoning abilities of LLM agents in diverse game environments. Our contributions are as follows:

* **GameBench**, the first benchmark to capture both cross-domain and out-of-distribution strategic reasoning for comparison across multiple agents.
* **Empirical results** on GPT-3 and GPT-4, demonstrating the effects of Chain-of-Thought scaffolding and the state-of-the-art scaffolding.

Figure 1: **Rating data** With CoT scaffolding, GPT-4 is the best reasoner below only the human baseline, achieving the best LLM performance on Sea Battle and Pit. But without, it performs worse than even the random baseline due to its exceedingly low rating on Sea Battle. The state-of-the-art RAP scaffolding doesn’t provide as much of an improvement to GPT-4 as CoT does. Looking at the top line of Figure 0(a) reveal the best agent in each game. come from exponential Bradley–Terry model. See section 3.4 for details. The whiskers represent 90% CIs computed from our bootstrapping process formalized in 3.4. ALS = Air, Land, Sea; ARC = Arctic Scavengers; AYT = Are You the Traitor?; CN = Codenames; HV = Hive; PT = Pit; SN = Santorini; TRB = Two Rooms and a Boom; SB = Sea Battle.

Related works

**LLM agents playing games** Using games to evaluate LLMs has significant precedent in previous research. Some studies evaluate models using single strategic tasks or games, such as Minecraft (Wang et al., 2023; Zhu et al., 2023), Diplomacy (Bakhtin et al., 2022), Avalon (Light et al., 2023), and Werewolf (Xu et al., 2023b). Other benchmarks (Wu et al., 2023a; Liu et al., 2023b) capture a more comprehensive picture by using suites of multiple tasks or games to evaluate LLMs as intelligent agents. However, the tasks represented in these benchmarks don't involve interaction with other agents, so they don't reflect strategic reasoning as defined in this work.

**Game-theoretic scenarios** Several benchmark suites focus on common game theory scenarios, such as auctions (Chen et al., 2023; Mao et al., 2023), matrix games like Prisoner's Dilemma (Akata et al., 2023; Gandhi et al., 2023a), and negotiation (Abdelnabi et al., 2023; Gandhi et al., 2023a). While they do involve multi-agent interaction and are useful for testing models' strategic reasoning ability, our benchmark focuses on more complex games that aren't as frequently studied as these game theory scenarios. Given no major strategy guides or forums dedicated to these games, we believe there is less documentation on optimal strategies for them present in LLMs' training corpuses.

**Dialogue-based games** Some benchmarks employ dialogue-based games that are less well-documented on the internet: Agashe et al. (2024) and Chalamalasetti et al. (2023) use novel cooperative dialogue games, and Qiao et al. (2023) uses two social deduction games and one word guessing game. However, our benchmark aims to evaluate LLMs' strategic reasoning ability not only in cooperative and conversational environments, but competitive, spatial, and non-deterministic ones as well.

**Diverse multi-agent game suites** The benchmarks most similar to ours employ diverse suites of complex multi-agent games, including conversational, board, and card games (Chen et al., 2024; Duan et al., 2024; Abdulhai et al., 2023; Xu et al., 2023a). However, many of the included games are either commonly found on the internet, such as TicTacToe, Poker, and Connect Four, or common game-theoretic scenarios, as discussed previously. These games are not as out-of-distribution as desired.

In summary, we build upon previous work by introducing a diverse suite of multi-agent games to evaluate the strategic reasoning ability of LLMs as agents. Our benchmark is characterized by its inclusion of complex games that span a range of game characteristics and are not likely to be well-represented in LLMs' pretraining corpuses.

## 3 GameBench

In Section 3.1 we discuss our reasoning behind our selection of agents and scaffolds. In Section 3.2 we describe our methodology for selecting suitable games. In Section 3.3 we describe the agent and game interfaces. In Section 3.4 we introduce our rating model and formalize our process for calculating ratings.

### Agent and scaffolding selection

We benchmark GPT-3 (gpt-3.5-turbo-1106) and GPT-4 (gpt-4-1106-preview) due to their size, mainstream popularity, and convenient public API. We include these base models as well as several black-box scaffolding interventions in order to measure the relative effects these scaffolding interventions have on improving the reasoning abilities of the base models. We selected Chain-of-Thought (Wei et al., 2022b) prompting for its ubiquity and Reasoning-via-Planning (Hao et al., 2023) for its state-of-the-art status. We also include a random-action-selecting agent as baseline of no strategic reasoning ability, and a human agent as a baseline of progress towards human-level strategic reasoning.

For more details about agent implementation, see Appendix D.

### Game selection

In order to evaluate a broad range of cognitive skills associated with strategic reasoning, we curated a diverse set of games featuring abstract strategy, non-deterministic outcomes, hidden information, language communication, social deduction and bluffing, and cooperation between players. A breakdown of which games had these features can be found in Table 1.

Using these categories, we then filtered for games unlikely to be significantly represented in LLMs' pretraining data, to evaluate the models' out-of-distribution reasoning abilities. Two key criteria were (a) excluding games with dedicated online forums discussing improvement strategies, as well as (b) excluding games with published strategy guides. After finalizing the selection of games, we formalized their rulesets and mechanics into programmatic environments that the LLM agents could interact with.

Our final selection of games were _Air, Land, Sea_ (ALS); _Arctic Scavengers_ (ARC); _Are You the Traitor?_ (AYT); _Codenames_ (CN); _Hive_ (HV); _Pit_ (PT); _Santorini_ (SN); _Two Rooms and a Boom_ (TRB); and _Sea Battle_ (SB). Descriptions of the games and their rules can be found in Appendices F and G respectively. For additional details about game implementation, see Appendix D.

### Api

Each environment, implemented in Python, describes a Game object with methods for initializing, retrieving the game's current state and available actions, updating the state with an action, and executing a full match between two agents. Agents are objects that describe a method for choosing an action conditioned on the rules, state, and available actions retrieved from a Game instance. Agents are instantiated at the beginning of a match and destroyed at the end, so agents may maintain persistent state between moves to choose an action.

### Rating calculation

We formalize our rating calculation as follows. Let our dataset contain \(P\), the population of all possible matches across all games, and \(S=\{m_{1},m_{2},,m_{n}\}\), our sample of \(n\) matches. Define the weight \(w_{i}\) for each match \(m_{i}\) to be inversely proportional to the number of matches collected for that match's game. Specifically, if match \(m_{i}\) belongs to game \(X\) which has \(N_{X}\) matches, then the weight \(w_{i}\) is given by:

\[w_{i}=}.\] (1)

We then perform bootstrapping on the sample \(S\) for \(B=10,000\) times. Let \(S_{b}^{*}=[m_{i_{1}},m_{i_{2}},,m_{i_{n}}]\) be the \(b\)th bootstrapped sample, where \(m_{i_{j}}\) is randomly selected from \(S\) with probability proportional to \(w_{i}\) with replacement.

\[P(i>j)=}}{e^{_{i}}+e^{_{j}}}\] (2)

  
**Reasoning Category** & **Total** & **Games** \\  Abstract Strategy & 6 & ALS, ARC, CN, HV, SN, SB \\ Non-Deterministic & 3 & ARC, TRB, SB \\ Hidden Information & 3 & ARC, AYT, TRB \\ Language Communication & 4 & AYT, CN, PT, TRB \\ Social Deduction & 2 & AYT, TRB \\ Cooperation & 4 & AYT, CN, SB, TRB \\   

Table 1: **Number of games per reasoning category** We identify a set of six orthogonal components of strategic reasoning and curate a set of games that sufficiently cover their spread.

For each bootstrapped sample \(S_{b}^{*}\), we use maximum-likelihood estimation to fit the parameters of the above exponential Bradley-Terry model \(_{b}=\{_{},_{},\}\). Let \(_{b,k}\) denote the parameter for agent \(k\) in bootstrapped sample \(b\). We take the means of these distributions to be the "true" rating \(_{k}\) for each agent \(k\), given by:

\[_{k}=_{b=1}^{B}_{b,k}\] (3)

We considered several methods for aggregating pairwise match results across games into scores that represent the general skill of each model, including the Elo system . Unlike Elo, the Bradley-Terry system  assumes model skill does not change over time and it does not need to be calculated in a decentralized manner, making it more suitable for evaluating language models . In our analysis, this model also enables the comparison of models that never directly competed.

## 4 Empirical results

Additional figures showing agent-pairwise data covering number of games, total score, win probability, and rating per game is available in Appendix H. The rating plots in Appendix H show 90% confidence intervals for the points in Figure 0(a).

### Human comparison

The human baseline outperforms all model and scaffolding configurations in the benchmark. The upper-bound of GPT-4-RAP's confidence interval in Figure 0(b) just reaches the lower-bound of the human baseline. But due to both GPT-4-RAP and the human baseline having very few data points, this detail should not be taken very seriously. In Table 3, the human baseline achieves the highest overall score in every game it played except for _Santorini_.

The human subject beat their opponent agent in all matches except for two of the three _Codenames_ matches. For these particular matches, the human subject employed a friend because _Codenames_ typically requires at least two players per team. We hypothesize that LLM agents perform better in this context because they are better at modeling their teammate's thought process, as they are instantiated from the same underlying language model. In contrast, pairs of humans share much less cognitive similarity.

Details about the human data collection process are discussed in Appendix B.

   Agent &  \\   & Overall & ALS & ARC & AYT & CN & HV & PT & SN & TRB & SB \\  random & -0.50 & 1.07 & **0.48** & -2.52 & -2.67 & -1.15 & 0.63 & 0.37 & -0.79 & 0.05 \\ human & **1.76** & 1.49 & 0.45 & 1.92 & 1.26 & **3.63** & **1.29** & -0.89 & 1.70 & **1.25** \\ gpt-3 & -0.48 & 1.26 & -0.05 & -1.84 & -2.06 & 1.27 & 0.63 & -0.01 & -2.51 & -0.41 \\ gpt-3-cot & 0.06 & 0.03 & 0.22 & 2.42 & 0.45 & -0.44 & 0.63 & 0.53 & -2.76 & 0.26 \\ gpt-4 & -0.89 & -7.38 & -0.12 & -2.73 & -0.65 & -1.31 & -4.42 & -0.08 & 0.62 & -1.40 \\ gpt-4-cot & 0.16 & **2.13** & 0.27 & -0.19 & **2.41** & -1.13 & 0.63 & -0.53 & 1.22 & 0.62 \\ gpt-4-rap & -0.10 & 1.41 & -1.25 & **2.94** & 1.26 & -0.86 & 0.63 & **0.62** & **2.51** & -0.37 \\   

Table 2: **Game ratings** The table highlights the effects of scaffolds. Across all games, GPT-4 with CoT scaffolding improves over the base model substantially. But GPT-3 with CoT scaffolding is outperformed by the base model in _Air, Land, and Sea_, _Hive_, and _Two Rooms and a Boom_. Additionally, GPT-4 with RAP scaffolding usually under-performs GPT-4-CoT except in _Are You the Traitor?_, _Sea Battle_, and _Two Rooms and a Boom_.

### Effect of scaffolding

Chain-of-Thought prompting provided the best median and upper quartile results of all configurations tested in Figure 1b. GPT-3 and GPT-4 showed almost identical performance with GPT-4 with only a slight improvement over GPT-3. The positive effects of CoT prompting are already well-documented (Chowdhery et al., 2022; Zelikman et al., 2022; Wei et al., 2022a), and our results provides evidence of their use in strategic settings.

If we interpret the addition of CoT scaffolding as an intervention on the base model, we see it improves strategic reasoning ability in GPT-4 moreso than in GPT-3. In _Sea Battle_, this intervention brings GPT-4 from the worst model to the best model. In every game except _Codenames_, GPT-4 with CoT scaffolding outperforms its base model. But for GPT-3, the base model outperforms the CoT variant in _Santorini_ and _Sea Battle_. One possible hypothesis for this difference in effect between on GPT-3 and GPT-4 is that GPT-4 is a bigger model and thus can probably make better use of in-context information.

### GPT-3 versus GPT-4

GPT-3 performs only slightly better than random action. Surprisingly, GPT-4 performs the worst of all configurations with its upper quartile performance being worse than random's lowest quartile. This result is mostly due to GPT-4 losing all matches in Sea Battle. This challenges our aggregation method: GPT-4 should not be so harshly penalized for poor performance on one game.

An alternative aggregation method that would be more robust to outliers is to use factor analysis to isolate a "general strategic reasoning factor" that explains a significant portion of the variance between models' performances. This method is used to aggregate separate cognitive test scores into IQ scores, making it apt for evaluating LLMs' reasoning abilities (Ilic, 2023). We expect this g-factor approach to appropriately weigh models' Sea Battle ratings lower, fixing this discrepancy.

Considering these two datapoints and analysis from 4.2, we can tentatively conclude that strategic reasoning ability is not improving in OpenAI's newest frontier models alone, but their receptiveness to scaffolding to improve strategic reasoning is increasing.

### State-of-the-art scaffolding

The state-of-the-art scaffolding was outperformed by both Chain-of-Thought agents. One possible hypothesis for this is that, during the Monte-Carlo tree search, this agent predicts new states based on the state being examined, which is already a predicted state depending if depth \( 1\). If the

   Agent &  \\   & Overall & ALS & ARC & AYT & CN & HV & PT & SN & TRB & SB \\  random & 0.49 & 0.72 & **0.60** & 0.25 & 0.18 & 0.41 & 0.50 & 0.56 & 0.52 & 0.58 \\ human & **0.85** & **1.00** & NaN & NaN & NaN & **1.00** & **1.00** & 0.43 & NaN & **0.78** \\ gpt-3 & 0.48 & 0.64 & 0.43 & 0.43 & 0.63 & 0.80 & 0.50 & 0.47 & 0.27 & 0.40 \\ gpt-3-cot & 0.60 & 0.43 & 0.50 & 0.93 & 0.89 & 0.60 & 0.50 & **0.61** & 0.33 & 0.55 \\ gpt-4 & 0.31 & 0.00 & 0.42 & 0.33 & 0.83 & 0.33 & 0.31 & 0.42 & 0.71 & 0.20 \\ gpt-4-cot & 0.60 & 0.81 & 0.50 & 0.64 & **1.00** & 0.50 & 0.50 & 0.37 & 0.75 & 0.51 \\ gpt-4-rap & 0.62 & NaN & 0.33 & **1.00** & NaN & 0.50 & NaN & 0.58 & **1.00** & 0.26 \\   

Table 3: **Average score.** The total score an agent achieved in a game divided by the number of games that agent played. Comparing with 2, this table highlights interesting correlations between empirical score and model-inferred ratings. For example, in _Air, Land, and Sea_, GPT-4-CoT has the top rating while the human baseline has second-top, but they swap when examining average score. This plot also shows more clearly why the human baseline has the highest rating even though both the human baseline and GPT-4-RAP have the highest rating in three games. Here, the human baseline achieved the highest score in four games but GPT-4-RAP only achieved the highest in two.

agent makes any errors in this examined state's prediction due to misunderstandings about the game state or rules, these will likely be compounded in the next set of predictions. We might expect the Chain-of-Thought agents to be susceptible to the same issue of compounding errors, but to a lesser extent. This could be tested qualitatively by a human expert analysing GPT-4-RAP's predictions for accuracy.

Another hypothesis is that we ran GPT-4-RAP to a depth great-enough to surpass GPT-4 without RAP scaffolding, but not great-enough depth to surpass Chain-of-Thought scaffolding. This could be tested by adding several GPT-4-RAP agents to the benchmark, each with different depths.

It seems unlikely that Chain-of-Thought prompting should be the most sophisticated black-box scaffolding, so it remains an open question to find this scaffolding in order to establish an upper-bound on strategic reasoning ability with black-box scaffolds.

## 5 Discussion

We now discuss the limitations and future directions of our work.

**Confirming out-of-distribution status** It is clear by simply asking GPT-4 that it already knows about these games and their rules. It is unclear, however, if it consumed any strategy guides about these games in the pretraining process, which is the determining factor for out-of-distribution status in our benchmark. **Future work** We propose the following experiment. Design an intervention that is: supply a strategy guide in-context to a language model agent for the game it is playing. We would expect this intervention to improve agent performance more on out-of-distribution games than in-distribution games. Collect data of agents playing an unknown distribution game; agents with the intervention playing an unknown distribution game; agents playing known in-distribution games; agents with the intervention playing known in-distribution games. Compare the effect of the intervention on the unknown distribution game versus the effect on the known in-distribution games. If the effect is much higher on the unknown distribution game, this is a evidence for the game being out-of-distribution. This would work better with known out-of-distribution games, but this may not be possible to know in all cases. We could also compare models' performance on common games vs. "counterfactual" games, which are slightly modified to reduce any association with their in-distribution counterparts [Wu et al., 2023b].

**Protecting out-of-distribution status** We did not attempt to protect these games from becoming in-distribution in the future. **Future work** Developers of frontier models should curate strategic reasoning environments by ensuring these games are held out from pretraining data. For ubiquitous games such as chess, this is unfeasible. But following our heuristics for game selection discussed in section 3.2, it should be reasonable to find games without much internet data.

**Results' sensitivity to games** From inspecting GPT-4's surprisingly low rating with _Sea Battle_, it became apparent that our "multigame" approach to aggregation may be inadequate due its sensitivity to the games included; i.e., ablating _Sea Battle_ significantly changed the data narrative. **Future work** We see multiple ways forward. If aggregate data is useful, investigate more robust forms of aggregation, such as the g-factor or factor analysis in general. Alternatively, explore a multi-dimensional approach that attempts to score agents on the six reasoning categories from Table 1. Or, discard any notion of aggregation and determine effective means of analysis that looks only at individual games and maybe uses more qualitative data with the help of human experts.

**Low-resolution human benchmark** We find it especially important to know how well these models fair compared to humans, but collecting comprehensive human data was out of our means. **Future work** Conduct more comprehensive human data to form a distribution of human strengths on each game with which we can measure the progress of model and scaffolding development.

**Uncaught edge cases** Every few games were inspected during data collection, and occasionally, we caught and fixed bugs in our evaluation code. It is possible that some edge cases went unnoticed and were featured in our final data release. **Future work** Incorporating more human subjects into the data collection should make this process trivial, as they can provide immediate feedback if they witness unexpected behavior.

**Benchmark and dataset size** Our benchmark has a respectable number of games and agents compared to other benchmarks (Chen et al., 2024; Duan et al., 2024; Abdulhai et al., 2023; Xu et al., 2023a), but the addition of more games and agents would provide a richer picture of models' strategic reasoning abilities. Additionally, our dataset is fairly small and suffers from biases from variable resource cost between games. **Future work** Add more varied games to the benchmark, evaluate more model and scaffolding configurations, and collect more data for each configuration.

## 6 Conclusion

We present GameBench, an LLM agent benchmark to test strategic reasoning ability using diverse games that have sparse strategy material in pretraining data. We benchmark OpenAI's GPT-3 and GPT-4 models and evaluate the impact of two scaffolding methods: Chain of Thought (CoT) and Reasoning via Planning (RAP). We find that human trials consistently outperform all LLM agents. Of all the agent configurations, CoT agents performed the best, followed by RAP-augmented GPT-4. Base GPT-3 performed on-par with the random baseline, and base GPT-4 performed worse. These results show that while measures such as scaffolding can help improve performance in strategic reasoning, even the best configuration fall short of human reasoning. LLMs show great promise working on in-distribution tasks, though their performance on OOD task sets show a low risk for current dangers of deploying autonomous agents. Nonetheless, the performance gains achieved through scaffolding techniques indicate the potential for future advancements that could increase the risk posed by such systems if their reasoning capabilities continue to improve.