# Faster Repeated Evasion Attacks in Tree Ensembles

Lorenzo Cascioli

Department of Computer Science

KU Leuven

Leuven, Belgium

lorenzo.cascioli@kuleuven.be

&Laurens Devos

Department of Computer Science

KU Leuven

Leuven, Belgium

&Ondrej Kuzelka

Faculty of Electrical Engineering

Czech Technical University in Prague

Prague, Czech Republic

&Jesse Davis

Department of Computer Science

KU Leuven

Leuven, Belgium

###### Abstract

Tree ensembles are one of the most widely used model classes. However, these models are susceptible to adversarial examples, i.e., slightly perturbed examples that elicit a misprediction. There has been significant research on designing approaches to construct such examples for tree ensembles. But this is a computationally challenging problem that often must be solved a large number of times (e.g., for all examples in a training set). This is compounded by the fact that current approaches attempt to find such examples from scratch. In contrast, we exploit the fact that multiple similar problems are being solved. Specifically, our approach exploits the insight that adversarial examples for tree ensembles tend to perturb a consistent but relatively small set of features. We show that we can quickly identify this set of features and use this knowledge to speedup constructing adversarial examples.

## 1 Introduction

One of the most popular and widely used classes of models is tree ensembles which encompasses techniques such as gradient boosting  and random forests . However, like other flexible model classes such as (deep) neural networks [28; 17], they are susceptible to evasion attacks . That is, an adversary can craft an imperceptible perturbation that, when applied to an otherwise valid input example, elicits a misprediction by the ensemble. As an example, consider a bank that uses a learned model to assess whether to approve or deny loan applications. In this setting, an evasion attack could entail slightly altering a potential customer's data (e.g., adding one month to their work seniority) that results in the model making a different decision on the customer's application. The slightly modified customer record is an adversarial example. There is significant interest in reasoning about tree ensembles to both generate such adversarial examples [15; 36] and perform empirical robustness checking [23; 8; 10] where the goal is to determine how close the nearest adversarial example is.

Generating adversarial examples is an NP-hard problem , which has spurred the development of approximate techniques [8; 36; 10]. These methods exploit the structure of the trees to find adversarial examples faster, e.g., by using graph transformations  or discrete (heuristic) search [36; 10; 12]. Still, these techniques can be slow, particularly if there is a large number of attributes in the domain. This is compounded by the fact that one often wants to generate large sets of adversarial examples.

A weakness to existing approaches is that they ignore the fact that adversarial example generation is often a sequential task where multiple similar problems are being solved in a row. That is, one has access to a large number of "normal" examples each of which should be perturbed to elicita misprediction. Alas, existing approaches treat each considered example in isolation and solve the problem from scratch. However, there are likely regularities among the problems, meaning that the algorithms perform redundant work. If these regularities can be identified efficiently and this information can be exploited to guide the search for an adversarial example, then the run time performance of repeated adversarial example generation can be improved.

Studying these regularities in order to make adversarial example generation faster is an important problem. First, it advances our understanding of the nature of adversarial examples in tree ensembles and their generation methods. This might inspire improvements to generation methods, and in turn lead to better defense or detection methods. Second, model evaluation by verification [26; 29; 10] is quickly becoming important as machine learning is applied in sensitive application areas. Being able to efficiently generate adversarial examples is crucial for computing empirical robustness (e.g., ), adversarial accuracy (e.g., ), and for model hardening (e.g., ). Third, some scenarios exist where an attacker would want to perform a large scale evasion attack. For example, some DNS registries use models to flag new domain registrations as potentially malicious (e.g., for phishing, fake webshops)  and scammers likely need to register many such domains. Finally, techniques in the planning community for analyzing policy safety through predicate abstraction involve performing repeated verification queries on the same model [30; 31; 22].

We propose a novel approach that analyzes previously solved adversarial example generation tasks to inform the search for subsequent tasks. Our approach is based on the observation that for a fixed learned tree ensemble, adversarial examples tend to be generated by perturbing the same, relatively small set of features. We propose a theoretically grounded manner to quickly find this set of features. We then propose two novel strategies to use the identified features to guide the search for adversarial examples, one of which is guaranteed to produce an adversarial example if it exists. We apply our proposed approach to two different algorithms for generating adversarial examples [23; 10]. Empirically, our approaches result in speedups of up to 36x/21x and on average of 9x/4x (\(\) 8x/3x). The source code for the presented algorithms and all the experiments is publicly available at https://github.com/lorenzocascioli/faster-repeated-evasion-tree-ensembles.

## 2 Preliminaries

We briefly explain tree ensembles, evasion attacks, and the two adversarial generation methods used in the experiments. We assume a \(d\)-dimensional input space \(^{d}\) and binary output space \(=\{-1,1\}\). We focus on binary classification because most existing methods for generating adversarial examples for tree ensembles are designed for this setting [1; 23; 10].

Tree EnsemblesTree ensembles include algorithms such as (gradient) boosted decision trees (GBDTs) [16; 9] and random forests [3; 25]. A tree ensemble contains a number of trees and most implementations only learn binary trees. A binary tree \(T\) contains two types of nodes. _Internal nodes_ store references to a left and a right sub-tree, and a split condition on some attribute \(f\) in the form of a less-than comparison \(X_{f}<\), where \(\) is the split value. _Leaf nodes_ have no children and only contain an output value. Each tree starts with a _root node_, the only one without a parent.

Given an example \(x\), an individual tree is evaluated recursively starting from the _root node_. In each internal node, the split condition is applied and if it is satisfied, then the example is sorted to the left subtree and if not it is sorted to the right one. This procedure terminates when a _leaf node_ is reached. The final prediction of the ensemble \((x)\) is obtained by combining the predicted leaf values for each tree in the ensemble. In gradient boosting, the class probability is computed by applying a sigmoid transformation to the sum of the leaf values.

Evasion AttacksAn _evasion attack_ involves manipulating valid inputs \(x\) into adversarial examples \(\) in order to elicit a misprediction . Following existing work on tree ensembles [23; 7; 10], we say that \(\) is an **adversarial example** for normal example \(x\) when (1) \(\|-x\|_{}<\) where \(\) is a user-selected maximum distance (i.e., the two are sufficiently close), (2) the ensemble predicts the correct label for \(x\), and (3) the model's predicted labels for \(\) and \(x\) differ.

We briefly describe the two existing adversarial example generation methods \(:(,x,,t_{})\{(), ,\}\) used in this paper: _kantchelian_ and _veritas_. These methods take as input an ensemble \(\), a normal example \(x\), a maximum perturbation size \(\), and a timeout \(t_{}\). They output \(()\), where \(\) is an adversarial example for \(x\), \(\), indicating that no adversarial example exists, or \(\), indicating that no result could be found within \(t_{}\). Timeouts are explicitly handled because adversarial example generation is NP-hard .

_kantchelian_ formulates the adversarial example generation task as a mixed-integer linear program (MILP) and uses a generic MILP solver (e.g., Gurobi ). Specifically, _kantchelian_ directly minimizes the \(=\|x-\|_{}\) value. Given an example \(x\), it computes:

\[_{}\|x-\|_{}(x)().\] (1)

This approach exploits the fact that a tree ensemble can be viewed as a set of linear (in)equalities. Three sets of MILP variables are used. _Predicate variables_\(p_{i}\) represent the split conditions, i.e., each \(p_{i}\) logically corresponds to a split on an attribute \(f\): \(p_{i} f<\). _Leaf variables_\(l_{i}\) indicate whether a leaf node is active. The _bound variable_\(b\) represents the \(l_{}\) distance between the original example \(x\) and the adversarial example \(\). Constraints between the variables encode the structure of the tree. A set of predicate consistency constraints encode the ordering between splits. For example, if two split values \(_{1}<_{2}\) appear in the tree for attribute \(f\), and \(p_{1} f<_{1}\) and \(p_{2} f<_{2}\), then \(p_{1} p_{2}\). Leaf consistency constraints enforce that a leaf is only active when the splits on the root-to-leaf path to that leaf are satisfied. Lastly, the mislabel constraint requires the output to be a certain class: for leaf values \(v_{i}\), \(_{i}v_{i}l_{i} 0\). The objective directly minimizes the _bound variable_.

_veritas_ improves upon _kantchelian_ in terms of run time by formulating the adversarial example generation problem as a heuristic search problem in a graph representation of the ensemble (originally proposed by ). The nodes in this graph correspond to the leaves in the trees of the ensemble. Guided by a heuristic, the search then repeatedly selects compatible leaves. Leaves of two different trees are compatible when the conjunction of the split conditions along the root-to-leaf paths of the leaves are logically consistent. For a given \(\), _veritas_ solves the following optimization problem:1

\[*{}_{}\ () \|x-\|_{}<\] (2)

The output of the model \(()\) is maximized when the target class for \(\) is positive, and minimized otherwise. While _veritas_ can also directly optimize \(\), in this paper we will use a predefined \(\) for _veritas_. To the best of our knowledge, _veritas_ is the fastest approximate evasion attack for tree ensembles (see Appendix B.1).

## 3 Method

Adversarial example generation methods are often applied in the following setting:

**Given** & a tree ensemble \(\), a set of test examples \(\), and a maximum perturbation size \(\) \\
**Generate** & adversarial examples for each \(x\). \\ 

The goal of this paper is to exploit the fact that adversarial examples are sequentially generated for each example in \(\). By analyzing previously found adversarial examples, we aim to improve the efficiency of adversarial example generation algorithms by biasing the search towards the perturbations that are most likely to lead to an adversarial example.

Our hypothesis is that some parts of the ensemble are disproportionately sensitive to small perturbations, i.e., crossing the thresholds of split conditions in these parts of the ensemble results in large changes in the predicted value. Prior work has hypothesized that robustness is related to fragile features and that such features are included in models because learners search for any signal that improves predictive performance . One would expect that the attributes used in the split conditions in these disproportionately sensitive parts are exploited by adversarial examples more frequently than other attributes. Figure 1 illustrates this point by showing how often each attribute is perturbed in a set of a 10 000 adversarial examples generated by _kantchelian_ for two datasets. The bar plots distinguish among attributes are (1) never modified by any adversarial example (left), (2) modified by at least one but at most 5% of all adversarial examples (middle), and (3) modified by more than 5% of the adversarial examples (right). Less than 10% of the attributes are used by more than 5% of the adversarial examples. The figure shows that regularities exist in constructed adversarial examples: examples generated for different normal examples tend to exhibit perturbations to the same small set of attributes. Thus the two questions are how can one identify these frequently-modified attributes and how can algorithms exploit this knowledge to more quickly generate adversarial examples.

Our proposed approach has two parts. The first part simplifies the search for adversarial examples by only allowing perturbations to a limited subset of features. Namely, we exploit the knowledge that certain feature values are fixed to simplify the ensemble, by pruning away branches that can never be reached. The second part identifies a subset of commonly perturbed features by counting how often each feature is perturbed by adversarial examples. The size of this subset is determined by applying a theoretically grounded statistical test.

### Modifying the Search Procedure

Our proposed approach speeds up the adversarial example generation procedure by limiting the scope of the adversarial perturbations to a subset of features \(F_{S}\). This section assumes that we are given such a subset of features. The next section covers how to identify these features.

We consider three settings: _full_, _pruned_, and _mixed_. The _full_ setting is the original configuration of _kantchelian_ and _veritas_: the methods may perturb any attribute within a certain maximum distance \(\). That is, for each attribute \(f F\) with value \(x_{f}\), the attribute values are limited to \([x_{f}-,x_{f}+]\). Algorithm 1 summarizes the _pruned_ and _mixed_ approaches. We now describe both in greater detail.

```
1:parameters: maximum perturbation size \(\), timeouts \(t_{}^{full}\) and \(t_{}^{prun}\) for _full_ and _pruned_, generation method \(:(,x,,t)\{SAT(),\,, \,\}\)
2:functionGenerate(\(_{full},,F_{S},\))
3:\(}\)
4:for\(x\)do
5:\(_{prun}(_{full},F_{S},x)\) (Sec. 3.1)
6:\((_{prun},x,,t_{}^{prun})\)
7:if\( SAT()(_{full},x,,t_{}^{full})\)
9:endif
10:\(}}\{\}\)
11:endfor
12:return:\(}\)
13:endfunction ```

**Algorithm 1** Fast repeated adversarial example generation

Pruned ApproachThe _pruned_ setting disallows modifications to the attributes in the _non-selected_ set of attributes \(F_{NS}=F F_{S}\). We accomplish this by pruning the trees in the ensemble. Any node splitting on attributes in \(F_{NS}\) is removed. Its parent node is directly connected to the only child node that can be reached by examples with the fixed value for the attribute. Figure 2 shows an example of this procedure. We refer to this procedure as \((,F_{S},x)\). The adversarial example generation methods can be applied as normal to the pruned ensemble, but they will only generate adversarial examples with perturbations to the attributes in \(F_{S}\). Pruning simplifies the MILP problem of _kantchelian_ because all predicate variables \(p_{i}\) that correspond to splits in internal nodes of pruned

Figure 1: Bar plots showing that most attributes are not modified by the majority of adversarial examples on the _mnist_ and _webspam_ datasets. The leftmost bar shows the number of attributes that are never changed by any of the 10 000 adversarial examples generated by _kantchelian_’s approach. The middle bar shows the number of attributes that are modified at least once but at most by 5% of the adversarial examples. The rightmost bar shows the number of frequently modified features.

subtrees, and leaf variables \(l_{i}\) that correspond to leaves of pruned subtrees can be removed from the mathematical formulation. For _veritas_, the search space is reduced in size because the pruned leaves are removed from the graph representation of the ensemble. Hence, for both systems, on average, the problem difficulty is reduced by pruning the ensembles.

Pruning the trees does not affect the validity of generated adversarial examples: if \(\) is an adversarial example for a normal example \(x\) generated on a pruned ensemble, then \(\) is also an adversarial example for the full ensemble.

**Proposition 3.1**.: _Given normal example \(x\) that is correctly classified by the full ensemble \(_{}\). Let \(_{prun}=(_{},F_{S},x)\) and \(=(_{prun},x,,t_{})\) (i.e., \(_{prun}(x)_{prun}()\) and \(\|x-\|_{}<\)). Then it holds that \(_{}(x)_{}()\)._

Proof.: Because only branches not visited by \(x\) are removed, \(_{prun}(x)=_{}(x)\). The values for features in \(F_{}\) are fixed, so these values are equal between \(x\) and \(\). Hence, \(\) only visits branches in \(_{}\) that are also in \(_{prun}\). Therefore, \(_{prun}()=_{}()\). 

However, an _UNSAT_ generated on a pruned ensemble is inconclusive: it might still be the case that an adversarial example exists for the full ensemble, albeit one with perturbations to features in \(F_{}\). The _pruned_ setting generates a **false negative** if it reports _UNSAT_, yet the _full_ setting reports _SAT_.

Mixed ApproachThe _mixed_ setting takes advantage of the fast adversarial generation capabilities of the _pruned_ setting, but falls back to the _full_ setting when the _pruned_ setting returns an _UNSAT_ or times out. A much stricter timeout \(t_{}^{prun}\) is used for the _pruned_ setting to fully take advantage of the fast _SAT_s, while avoiding spending time on an uninformative _UNSAT_. The _mixed_ setting is guaranteed to find an adversarial example if the _full_ setting can find one.

**Theorem 3.2**.: _Assume a normal example \(x\) and maximum distance \(\). If an adversarial example can be found for the full ensemble \(_{}\), then the mixed setting is guaranteed to find an \(\) such that \(\|x-\|_{}<\) and \(_{}(x)_{}()\)._

Proof.: The _mixed_ setting first operates on the pruned ensemble \(_{prun}\) using a tight timeout and optimizes Equation 1 or 2 using _kantchelian_ or _veritas_ respectively. This returns (1) an adversarial example \(\), (2) an _UNSAT_ or (3) times out. In case (1), the generated adversarial example \(\) is also an adversarial example for the full ensemble (Prop 3.1). In cases (2) and (3), the _mixed_ setting falls back to the _full_ setting operating on the full ensemble \(_{}\) with the same timeout. Hence, it inherits the full method's guarantees. 

### Identifying Relevant Features

A good subset of relevant attributes \(F_{S}\) should satisfy two properties. First, it should minimize the number of false negatives, which occur when the _pruned_ approach reports _UNSAT_, but the _full_ approach reports _SAT_. Second, the feature subset should be small. The smaller \(F_{S}\) is, the more the ensemble can be pruned, and the larger the speedup is. These two objectives are somewhat in tension. Including more features will reduce the number of false negatives but limit the speedups, whereas using a very small subset will restrict the search too much resulting in many false negatives (or slow calls to the full search in the _mixed_ setting). The procedure is given in Algorithm 2.

Figure 2: An example tree using two attributes Height and Age (left). Suppose \(F_{}=\{\}\). Given an example where \(=55\), we can prune away the internal node splitting on Age. In the resulting tree (right), subtree (b) is pruned because it is unreachable given that \(=55\) and only subtrees (a) and (c) remain.

We address the first requirement by adding features to the subset that are frequently perturbed by adversarial examples. We rank features by counting how often each one differs between the perturbed adversarial examples in \(}\) so far and their corresponding normal examples in \(\).

The second requirement is met by statistically testing whether the identified subset guarantees that the false negative rate is smaller than a given threshold \(\) with probability at least \(1-\), for a specified confidence parameter \(\). If it is not guaranteed, then the subset is expanded. This is done at most 4 times for subsets of 5%, 10%, 20%, 30% of the features (ExpandFeatureSet\((F_{S},,})\) in Algorithm 2). If all tests fail, then a final feature subset of 40% of the most commonly modified features is used. We do not go beyond 40% because using the full feature set is then more efficient. Each test is executed on a small set of \(n\) generated adversarial examples. A first zeroth set is used merely for obtaining the first feature counts.

The statistical tests are performed as follows. The null hypothesis is that _FNR_ is greater than the threshold \(\). Take \(_{F}=(x_{1},x_{2},,x_{N})\) the dataset we use to find the feature subset \(F_{S}\). We define \(=(v_{1},v_{2},,v_{N})\) to be the binary vector such that \(v_{i}=1\) if the _pruned_ search with the feature subset \(F_{S}\) returns _UNSAT_ for the example \(x_{i}\) but the _full_ search returns \(SAT\), and \(v_{i}=0\) otherwise. Then the true false negative rate corresponding to \(F_{S}\) can be written as \(=_{i=1}^{N}v_{i}\). The small set of \(n\) examples from which we are estimating the false negative rate is a random vector \(=(X_{1},X_{2},,X_{n})\) sampled without replacement from \(_{F}\). We also define \(=(V_{1},V_{2},,V_{n})\) where \(V_{i}\) is the binary random variable defined anisogically to how we defined \(v_{i}\). It follows that \(_{i=1}^{n}V_{i}\) is distributed as a hypergeometric random variable. We use the method of _inversion of acceptance intervals_ to find a one-sided confidence interval \([0;]\) for the false negative rate with confidence level equal to a given \(1-\) (see, e.g., Section 5.2 in ), exploiting the fact that the cumulative distribution function of a hypergeometric distribution can be computed efficiently (ConfidenceInterval\((,n,)\) in Algorithm 2). We reject the null hypothesis if the confidence interval does not contain the threshold \(\). It follows from the basic properties of confidence intervals that this yields the desired test with confidence \(1-\). Since we execute the test 4 times in the algorithm, we apply a union-bound correction of factor 4 (we use confidence level 0.9). Note that there is a trade-off. The higher \(n\), the better the statistical estimates and the counts are, but also the more examples we process with a potentially suboptimal feature subset.

```
1:parameters: set of normal examples \(\), sample size \(n\), acceptable false negative rate \(\), confidence parameter \(\)
2:\(F_{S}\)
3:for\(k 0..4\)do
4:\(}(,[kn,k(n+ 1)],F_{S},)\)
5:\(\) number of false negatives in \(}\)
6:\([0;](,n,)\)
7:if the threshold \(\) is in \([0;]\), then ExpandFeatureSet\((F_{S},,})\)
8:else break the loop
9:endfor
10:return:\(F_{S}\) ```

**Algorithm 2** Find feature subset

## 4 Experiments

Empirically, we address three questions: (Q1) Is our approach able to improve the run time of generating adversarial examples? (Q2) How does ensemble complexity affect our approach's performance? (Q3) What is our empirical false negative rate?

Because the described procedure is based on identifying a subset of relevant features, it makes sense to exploit it only when the dataset has a large number of dimensions. Therefore, we present numerical experiments for ten binary classification tasks on high-dimensional datasets, using both tabular data and image data, as shown in Table 1.

Experimental SetupWe apply 5-fold cross validation for each dataset. We use four of the folds to train an XGBoost , random forest [3; 25] or GROOT forest (a robustified ensemble type )ensemble \(\). From the test set, we randomly sample 10 000 normal examples and attempt to generate adversarial examples by perturbing each one using the _kantchelian_ or _veritas_ attack. Table 1 also reports the adopted values of maximum perturbation \(\) and the hyperparameters of the learned ensembles, which were selected via tuning using the grid search described in Appendix B. The experiments were run on an Intel(R) E3-1225 CPU with 32GiB of memory.

The _pruned_ and _mixed_ settings work as follows. We use the procedure from Section 3.2 to select a subset of relevant features. We use \(=0.25\), \(n=100\) and \(=0.1\). We then apply Algorithm 2: we generate 5 sets of \(n\) adversarial examples to (1) find which features are perturbed most often and (2) determine the size of the feature subset \(F_{S}\). Therefore the extracted feature set gives us a \(1-=90\%\) confidence that our true false negative rate is below \(25\%\). After Algorithm 2 terminates, \(F_{S}\) is fixed, and we run the _pruned_ and _mixed_ settings on all the remaining test examples (Algorithm 1). We set a timeout of one minute for the _full_ setting, and a much stricter timeout of 1 (_kantchelian_) or 0.1 (_veritas_) seconds in the _pruned_ setting. We can be stricter with _veritas_ as it is an approximate method that is faster than the exact _kantchelian_.2

Q1: Run TimeTable 2 reports the average run time for the _full_ setting and the average speedup given by the _pruned_ and _mixed_ settings. We present here results for XGBoost and random forest, and report results for GROOT together with more extended results in Appendix C. Considering all three model types and both attacks, speedups for the _pruned_ setting are in the range 1.4x-36.2x with an average of 9x (\(\) 8x), and for the _mixed_ in the range 1.1x-20.5x with an average of 4x (\(\) 3x).

We notice that generating adversarial examples is more difficult for random forests (RF) than XGB. This leads to our strategies offering larger wins for RF than for XGB, with average speedups of 9.4x/3.5x (\(\) 7.2x/1.6x) for RF and 4.7x/2.7x (\(\) 3.4x/1.2x) for XGB. The robustified GROOT forests are even harder to attack, meaning our methods offer even larger improvements with average speedups of 11.4x/4.9x (\(\) 9.9x/5.0x).3

Tables 5 and 6 in the supplement also report additional statistics on the presented experiments. On average, the _mixed_ setting falls back to the _full_ search 10.5% of the time. The model and attack type do not seem to have a strong influence on the proportion of calls to the _full_ search. This helps it achieve a speedup by taking advantage of the fast _SAT_ results of the _pruned_ setting while still offering the theoretical guarantee from Theorem 3.2.

We also report the attack success rate, which is the fraction of times where our methods generate an adversarial example given that a valid adversarial example exists for the full model. The _pruned_ search has an average success rate of 90% (\(\) 6%). The _mixed_ search has success rate 100% by definition (Theorem 3.2).

    & & &  &  &  \\ 
**Dataset** & N & \#F & \(\) & M & d & \(\) & \(\) & M & d & \(\) & M & d & \(\) \\  covtype & 581k & 54 & 0.1 & 50 & 6 & 0.9 & 0.3 & 50 & 10 & 0.4 & 50 & 10 & 0.01 \\ fmunist & 70k & 784 & 0.3 & 50 & 6 & 0.1 & 0.3 & 50 & 10 & 0.4 & 50 & 10 & 0.3 \\ higgs & 250k & 33 & 0.08 & 50 & 6 & 0.1 & 0.08 & 50 & 10 & 0.4 & 50 & 10 & 0.01 \\ miniboone & 130k & 51 & 0.08 & 50 & 6 & 0.1 & 0.08 & 50 & 10 & 0.5 & 50 & 10 & 0.01 \\ mnist & 70k & 784 & 0.3 & 50 & 6 & 0.5 & 0.3 & 50 & 10 & 0.4 & 50 & 10 & 0.3 \\ prostate & 100k & 103 & 0.1 & 50 & 4 & 0.5 & 0.2 & 50 & 10 & 0.2 & 50 & 10 & 0.01 \\ roadsafety & 111k & 33 & 0.06 & 50 & 6 & 0.5 & 0.12 & 50 & 10 & 0.2 & 50 & 10 & 0.05 \\ sensciects & 58.5k & 48 & 0.06 & 50 & 6 & 0.5 & 0.12 & 50 & 10 & 0.2 & 50 & 10 & 0.01 \\ vehicle & 98k & 101 & 0.15 & 50 & 6 & 0.1 & 0.15 & 50 & 10 & 0.4 & 50 & 10 & 0.1 \\ webspam & 350k & 254 & 0.04 & 50 & 5 & 0.06 & 50 & 10 & 0.1 & 50 & 10 & 0.01 \\   

Table 1: Datasets’ characteristics: \(N\) and \(\#F\) are the number of examples and the number of features. _higgs_ and _prostate_ are random subsets of the original, bigger datasets. Multi-class classification datasets were converted to binary classification: for _covtype_ we predict majority-vs-rest, for _mnist_ and _fmnist_ we predict classes 0-4 vs. classes 5-9, and for _sensorless_ classes 0-5 vs. classes 6-10. We also report adopted values of max allowed perturbation \(\) and learners’ tuned hyperparameters after the grid search described in Appendix B. Each ensemble \(\) has maximum tree depth d and contains M trees. The learning rate for XGBoost is \(\). GROOT robustness is defined by \(\).

Figure 3 shows the number of executed searches as a function of time for four combinations of attack algorithm and model type.4 For XGB, both attacks benefit. Moreover, the _mixed_ setting is typically very close in run time to the _pruned_. On RF, the _pruned_ setting offers larger speedups. However, we see a more noticeable difference between the _pruned_ and the _mixed_ search on several datasets. This indicates that the _mixed_ strategy falls back more often to an expensive _full_ search.

Finally, it is natural to wonder how the quality of the generated adversarial examples is affected by the modified search procedure. While this is difficult to quantify, Figure 4 provides some examples of constructed adversarial examples for the _mnist_ dataset and an XGBoost ensemble. Visually, the examples constructed by _full_ and _pruned_ settings for both attacks are very similar. The examples constructed using _kantchelian_ look more similar to the base example than those for _veritas_ because _kantchelian_ finds the closest possible adversarial example whereas _veritas_ has a different objective: it constructs an adversarial example that will elicit a highly confident misprediction. See Appendix E for more generated examples.

Q2: Scaling BehaviorTwo key hyperparameters of tree ensembles are the maximum depth of each learned tree and the number of trees in the ensemble. We explore how varying these affects our approach, employing the same setup as described in Q1. We use the _mnist_ dataset and omit

    &  &  &  &  \\   & _full_ & _pruned_ & _mixed_ & _full_ & _pruned_ & _mixed_ & _full_ & _pruned_ & _mixed_ & _full_ & _pruned_ & _mixed_ \\  covtype & \(9.9\)m & \(3.3\) & \(2.1\) & \(25.7\)m & \(11.0\) & \(3.7\) & \(5.3\)s & \(1.7\) & \(1.4\) & \(45.0\)s & \(6.0\) & \(2.9\) \\ fmnist & \(1.5\)h & \(4.9\) & \(3.9\) & \(56.2\)m & \(7.8\) & \(4.3\) & \(43.6\)s & \(1.4\) & \(1.3\) & \(6.6\)m & \(3.6\) & \(3.0\) \\ higgs & \(3.3\)h & \(4.1\) & \(1.8\) & \(5.1\)h & \(3.0\) & \(1.4\) & \(20.4\)s & \(2.9\) & \(2.4\) & \(41.8\)m & \(21.4\) & \(2.5\) \\ miniboone & \(6.0\)h* & \(10.9\) & \(3.4\) & \(6.0\)h* & \(9.4\) & \(5.2\) & \(1.2\)m & \(8.4\) & \(5.9\) & \(12.9\)m & \(11.8\) & \(6.4\) \\ mnist & \(23.9\)m & \(6.9\) & \(5.1\) & \(36.6\)m & \(5.7\) & \(4.8\) & \(47.9\)s & \(2.5\) & \(2.1\) & \(3.3\)m & \(3.2\) & \(2.9\) \\ prostate & \(12.8\)m & \(3.4\) & \(2.8\) & \(6.0\)h* & \(11.8\) & \(5.2\) & \(9.9\)s & \(2.5\) & \(2.2\) & \(23.7\)m & \(16.9\) & \(2.6\) \\ roadsfery & \(10.7\)m & \(3.0\) & \(2.0\) & \(45.2\)m & \(5.4\) & \(2.3\) & \(11.4\)s & \(2.7\) & \(2.1\) & \(40.6\)m & \(33.5\) & \(3.1\) \\ sensorless & \(29.8\)m & \(2.3\) & \(2.1\) & \(52.5\)m & \(5.7\) & \(3.6\) & \(12.1\)s & \(2.9\) & \(1.8\) & \(4.1\)m & \(4.7\) & \(1.5\) \\ vehicle & \(2.5\)h & \(5.9\) & \(3.4\) & \(3.8\)h & \(7.1\) & \(5.8\) & \(19.4\)m & \(15.6\) & \(1.9\) & \(42.8\)m & \(9.1\) & \(1.1\) \\ webspam & \(24.2\)m & \(5.7\) & \(3.7\) & \(1.5\)h & \(7.3\) & \(5.7\) & \(18.8\)s & \(2.6\) & \(2.1\) & \(12.9\)m & \(3.9\) & \(1.2\) \\   

Table 2: Average run times and speedups when attempting to generate \(10\,000\) adversarial examples using _kantchelian/veritas_ on an XGBoost/random forest ensemble for _full_, _pruned_ and _mixed_. A * means that the dataset exceeded the global timeout of six hours.

Figure 3: Average run times for \(10\,000\) calls to _full_, _pruned_ and _mixed_ for _kantchelian_ (top) and _veritas_ (bottom). Results are given for both XGBoost and random forest for four selected datasets.

[MISSING_PAGE_FAIL:9]

perform robustness checking , and verify that the ensembles satisfy certain criteria [11; 10; 26; 29]. Kantchelian et al.  were the first to show that tree ensembles are susceptible to evasion attacks. Their MILP formulation is still the most frequently used method to check robustness and generate adversarial examples. Beyond this exact approach, several approximate approaches exist [8; 10; 35; 36] though not all of them are able to generate concrete adversarial examples (e.g., [8; 35]).

Other work focuses on making tree ensembles more robust. Approaches for this include adding generated adversarial examples to the training data (model hardening) , or modifying the splitting procedure [7; 4; 32]. Gaining further insights into how evasion attacks target tree ensembles, like those contained in this paper, may inspire novel ways to improve the robustness of learners.

Another line of work aims at directly training tree ensembles that admit verification in polynomial time [5; 13]. However, a drawback to current approaches is that they result in (large) decreases in predictive performance.

Finally, performing evasion attacks has been studied for other model classes with deep neural networks receiving particular attention [28; 17; 24; 6]. However, state-of-the-art algorithms are tailored to one specific model type as they typically exploit specific properties of the model, e.g., the work on tree ensembles often exploits the logical structure of a decision tree.

## 6 Conclusions

This paper explored two methods to efficiently generate adversarial examples for tree ensembles. We showed that considering only the _same subset of features_ is typically sufficient to generate adversarial examples for tree ensemble models. We proposed a simple procedure to quickly identify such a subset of features, and two generic approaches that exploit it to speed up adversarial examples generation. We showed how to apply them to an exact (_kantchelian_) and approximate (_veritas_) evasion attack on three types of tree ensembles, and discussed their properties and run time performances.

**Limitations.** Our approach speeds up evasion attacks in the specific scenario when the same model is repeatedly attacked. Plus, it excels on high-dimensional datasets. Our evaluation only considered \(l_{}\) attacks, whereas other norms such \(l_{1}\) and \(l_{2}\) are also relevant.

**Impact Statement.** While this work does make attacking tree ensembles faster, it is also important to understand what attackers may do. This work also targets increasing the applicability of robustness checking and hardening techniques, which can lead to approaches for training more robust models.

**Acknowledgements.** This research is supported by the Research Foundation Flanders (FWO, LC: 1118125N), The European Union's Horizon Europe Research and Innovation program under the grant agreement TUPLES No. 101070149 (LC, LD, OK, JD), and the Flemish Government under the "Onderzoeksprogramma Artificelle Intelligentie (AI) Vlaanderen" program (JD).