# Can Large Language Models Analyze Graphs like Professionals? A Benchmark, Datasets and Models

Xin Li\({}^{1}\)1, Weize Chen\({}^{2}\)1, Qizhi Chu\({}^{1}\), Haopeng Li\({}^{3}\), Zhaojun Sun\({}^{3}\), Ran Li\({}^{2}\), Chen Qian\({}^{2}\)

**Yiwei Wei\({}^{4}\), Zhiyuan Liu\({}^{2,5}\), Chuan Shi\({}^{1}\), Maosong Sun\({}^{2,5}\), Cheng Yang\({}^{1}\)2**

\({}^{1}\) School of Computer Science, Beijing University of Posts and Telecommunications,

\({}^{2}\) Department of Computer Science and Technology, Tsinghua University,

\({}^{3}\) School of Artificial Intelligence, Beijing University of Posts and Telecommunications,

\({}^{4}\) College of Petroleum Engineering, China University of Petroleum (Beijing) at Karamay,

\({}^{5}\) Institute for Artificial Intelligence, Tsinghua University

###### Abstract

The need to analyze graphs is ubiquitous across various fields, from social networks to biological research and recommendation systems. Therefore, enabling the ability of large language models (LLMs) to process graphs is an important step toward more advanced general intelligence. However, current LLM benchmarks on graph analysis require models to directly reason over the prompts describing graph topology, and are thus limited to small graphs with only a few dozens of nodes. In contrast, human experts typically write programs based on popular libraries for task solving, and can thus handle graphs with different scales. To this end, a question naturally arises: _can LLMs analyze graphs like professionals_? In this paper, we introduce ProGraph, a manually crafted benchmark containing 3 categories of graph tasks. The benchmark expects solutions based on programming instead of directly reasoning over raw inputs. Our findings reveal that the performance of current LLMs is unsatisfactory, with the best model achieving only 36% accuracy. To bridge this gap, we propose LLM4Graph datasets, which include crawled documents and auto-generated codes based on 6 widely used graph libraries. By augmenting closed-source LLMs with document retrieval and fine-tuning open-source ones on the codes, we show 11-32% absolute improvements in their accuracies. Our results underscore that the capabilities of LLMs in handling structured data are still under-explored, and show the effectiveness of LLM4Graph in enhancing LLMs' proficiency of graph analysis. The benchmark, datasets and enhanced open-source models are available at https://github.com/BUPT-GAMMA/ProGraph.

## 1 Introduction

**Background.** Large language models (LLMs) [6; 52; 3] are parameter-rich neural networks trained on a vast amount of text data to understand and generate human language. LLMs can not only handle classical natural language processing tasks like translation, but also benefit task solving in various domains such as code generation , logical reasoning , and mathematical calculation .

**Previous LLM Benchmarks for Graph Analysis.** Recently, many researchers have proposed extending LLMs to scenarios that require graph understanding and analysis [25; 48]. As graph isa very commonly used data structure in real-world services (_e.g.,_ social networks [50; 35; 54] and urban computing [30; 66; 62]), enabling the ability of LLMs to process graphs is an important step toward more advanced general intelligence. To this end, several benchmarks have been developed to evaluate such ability. For example, NLGraph  and GraphInstruct  investigate whether LLMs can understand and compute basic graph properties, such as counting node degrees or finding the shortest path for a node pair. GraphTMI  and GPT4Graph  also consider typical learning tasks such as node classification. LLM4DyG  further extends the tasks to dynamic graphs.

**Limitations of Existing Benchmarks.** However, from the perspective of practicality, we argue that previous benchmarks have three major drawbacks. Firstly, the problems in these work require LLMs to read through the adjacency lists of graphs from prompts before answering specific questions. Consequently, the graph sizes in their benchmarks are rather small (typically with a few dozens of nodes), due to the length limitation of LLMs. Being able to compute the shortest path on a small graph does not mean that the same can be done on a real graph with millions of nodes. Secondly, the desired solving process in these work requires step-by-step reasoning fully based on LLMs. But even with the help of Chain-of-Thought (CoT) [58; 14], the reasoning depths of current LLMs are still shallow [29; 27]. Consequently, LLMs might be able to count triangles one by one in a small graph with 10 nodes, and will inevitably fail for large graphs. Thirdly, the problem descriptions in these work are abstract and monotonous in form, lacking context from real-world application scenarios.

**Inspirations from Human Experts.** Consider the scenario that a human expert is asked to find the shortest path between two nodes in a million-scale graph, she will probably write a few lines of Python codes based on NetworkX, instead of directly reasoning over the raw inputs. To this end, a question naturally arises: _can LLMs analyze graphs like professionals_? Fortunately, most popular LLMs have shown the ability to write codes and utilize various application programming interfaces (APIs), making it possible to analyze graphs via API calling as human experts will do. Compared with direct reasoning in previous benchmarks, generating a few lines of codes requires much shallower reasoning depths for LLMs, but can solve more complex problems.

**Benchmark.** In this paper, we propose ProGraph benchmark to evaluate the capability of LLMs in leveraging external APIs for graph analysis. The benchmark includes 512 problems with hand-crafted questions and answers (QA pairs). The problems cover three categories of tasks: basic graph theory, graph statistical learning, and graph embedding, and can be solved based on six popular Python libraries. In the questions, graphs can be either described by natural language or stored in files, and thus can scale to \(10^{6}\) in our benchmark. To improve the diversity of problem descriptions and align with real-world scenarios, the questions are rephrased in a role-play manner based on GPT-4 turbo . In the answers, we provide reference code, key APIs and execution results. We also design an automated evaluation process that aligns well with human judgement.

**Datasets and Models.** To facilitate LLMs to solve these problems via programming, we construct the LLM4Graph datasets with both document and code data. The document dataset contains API information crawled from the official documents of the six Python libraries. The code dataset includes 29,260 QA pairs automatically generated by back-instructing  GPT-4 turbo. To enable CoT learning, we further introduce the thought on the document information of relevant APIs to the answers of the code dataset as prefixes. To demonstrate the value of our datasets, we enhance closed-source LLMs by extracting relevant information from the document dataset as RAG (retrieval-augmented generation), and improve open-source ones by instruction tuning over the code dataset. Besides the datasets, the improved open-source LLMs are also released for future researches.

**Key Results.** The accuracies of closed-source models (Claude, GPT and Gemini) on ProGraph are 25-36%, and can be improved to 37-46% with RAG using LLM4Graph as the retrieval pool. The accuracies of open-source models (Llama3  and Deepseek Coder ) are only 12-24%, but can be

  
**Aspects** &  **ProGraph** \\ (this work) \\  &  **NLGraph** \\ () \\  &  **LLM4DyG** \\ () \\  &  **GraphTMI** \\ () \\  &  **GraphInstruct** \\ () \\  &  ** GPT4Graph** \\ () \\  & 
 **GraphWiz** \\ () \\  \\  Basic Graph Theory & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\ Graph Statistical Learning & ✓ & ✗ & ✗ & ✓ & ✗ \\ Graph Embedding & ✓ & ✓ & ✗ & ✗ & ✗ & ✗ \\ Access to External APIs & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ \\ Real-world Context & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ \\ Scalability & up to \(10^{6}\) & up to \(10^{3}\) & up to \(10^{3}\) & up to \(10^{2}\) & up to \(10^{1}\) & up to \(10^{4}\) & up to \(10^{2}\) \\   

Table 1: Comparisons among different graph analysis benchmarks for LLMs.

improved to 45-47% through instruction-tuning on LLM4Graph. These results show that ProGraph is challenging for current LLMs, and LLM4Graph can significantly enhance their performance.

**Contributions.** (1) To the best of our knowledge, we are the first work exploring the ability of LLMs to analyze graphs with external APIs. The utilization of external APIs is practical and powerful in real-world scenarios. (2) To evaluate such ability, we propose a novel and challenging ProGraph benchmark with hand-crafted QA pairs covering three categories of tasks, _i.e.,_ basic graph theory, graph statistical learning, and graph embedding. (3) We develop LLM4Graph datasets containing both crawled document data and auto-generated code data. Experimental results demonstrate that our datasets can substantially enhance the performance of both closed-source and open-source LLMs. The improved open-source models are released together with the datasets for future researches.

## 2 Related Work

**LLM for Graphs.** Recent efforts leverage the strong generalization ability of LLMs for graph understanding and analysis. Benchmarks like NLGraph  and GraphInstruct  evaluate LLMs' graph reasoning abilities, finding that while LLMs have some capabilities, they are not very strong. GraphTMI  assesses LLMs' performance on tasks like node classification and link prediction using different input formats. GPT4Graph  evaluates LLMs' understanding of graph structure data in various formats, and LLM4DyG  studies LLMs in dynamic graphs, introducing techniques to improve performance. GraphWiz  employs two approaches to optimize reasoning paths for solving basic graph problems. Another approach combines LLMs with graph neural networks (GNNs) to enhance learning tasks, leveraging text attribute processing or direct graph task handling through techniques like prompt learning and instruction tuning [25; 17; 10; 59; 48; 24; 49; 63; 7]. However, these works are often specialized for classification tasks and do not handle complex graph tasks.

**LLM Benchmarks.** LLMs perform strongly in text processing, mathematical computation, and code generation. Text processing benchmarks evaluate capabilities in machine translation, summarization, and comprehension [2; 34; 61; 32; 16; 26]. Mathematical computation benchmarks assess understanding of numbers and elementary math concepts [12; 51]. Specialized field benchmarks, such as those in biology, chemistry, and finance, evaluate LLMs' expertise in specific domains [47; 21; 60]. Code generation benchmarks like HumanEval and MBPP focus on function-oriented tasks [8; 4], while our proposed ProGraph considers task-oriented code generation.

**Enhancement Techniques for LLMs.** There are many techniques to enhance the performance of LLMs [38; 57]. Among these, two important techniques are Chain-of-Thought (CoT)  and retrieval-augmented generation (RAG) . CoT allows the model to mimic human thinking by reasoning step-by-step rather than directly providing an answer, significantly enhancing the logical analysis and reasoning capabilities of LLMs. RAG reduces the LLMs' hallucinations by allowing them to access relevant information before generating answers, improving LLMs' accuracy and reliability across various tasks.

## 3 Benchmark

To evaluate the ability of LLMs in graph analysis, we introduce the ProGraph benchmark, featuring 512 problems in three categories. These hand-crafted problems include questions and answers with two difficulty levels. To enhance diversity and realism, we leverage GPT-4 turbo to rephrase the questions in a role-playing manner, followed by manual verification for correctness. Finally, given the high consistency of answer judgments between humans and GPT-4o, we employ GPT-4o to automate the evaluation. We compare the proposed benchmark with previous ones in Table 1, present more discussions about related work in Appendix 2, and show a benchmark example in Appendix E.

### Tasks

The proposed ProGraph benchmark considers three categories of tasks:

**Category 1: Basic Graph Theory.** Graph theory primarily studies the fundamental concepts of graphs, including types, properties, classical algorithms and many other basic operations. For example, some problems in this category aim to check whether a graph is acyclic, compute the centrality of nodes, or find the maximum cardinality matching.

### Human Annotation

To create high-quality problems for benchmarking, we invite human annotators to develop questions and answers based on the following guidelines, and the annotation manual is in Appendix I.

**Python Libraries.** In this work, we consider six popular libraries to support the above three task categories, _i.e.,_ NetworkX  and igraph  for basic graph theory, CDlib , graspologic  and Little Ball of Fur  for graph statistical learning, and Karate Club  for graph embedding.

**Question Design.** First of all, human annotators need to either manually design or use some random graph generators to obtain a graph. Then, based on the API documents of the six libraries, the annotators are asked to design questions with one of the four types: true/false, calculation, drawing, and hybrid questions. Here hybrid questions are composed of two or more of the first three types. Here we present a calculation question from the basic graph theory category as an example:

**Question**

I have a graph with an edge set [(1, 2), (1, 3), (2, 3), (2, 4), (3, 5), (4, 5)], can you help me compute node connectivity for all pairs of nodes and print the node connectivity for each pair?

**Reference Code**

*** Python Code **

**Key APIs**

all_pairs_node_connectivity

**Execution Results**

Node connectivity between 1 and 2 is 2...

Figure 1: **The pipeline of ProGraph benchmark construction. We first collect the API documents of 6 frequently-used graph reasoning Python libraries. Then human annotators read the API information, use random graph generators to generate graph data, write questions based on API information, and give the results and the corresponding python codes, forming the Original QA pairs. Finally, GPT-4 rephrases abstract questions and integrates them into real-world scenarios.**

    &  &  \\   & True/False & Calculation & Drawing & Hybrid & Easy & Hard \\  Basic Graph Theory & 32 & 240 & 25 & 15 & 257 & 55 \\ Graph Statistical Learning & 7 & 115 & 7 & 25 & 43 & 111 \\ Graph Embedding & 0 & 30 & 0 & 16 & 0 & 46 \\  Total & 39 & 385 & 32 & 56 & 300 & 212 \\   

Table 2: Statistics of ProGraph.

**Answer Construction.** Based on the proposed question, human annotators need to further provide the code, the number of involved APIs, and the execution result. Based on the number of APIs, we categorize the problems into two difficulty levels: easy level involves one API, while the hard level involves multiple APIs. Here's an example of the collected data for the above question:

### Role-Play Rephrasing

To make questions more realistic, we rephrase them in a role-play manner. First, GPT-4 turbo generates hundreds of job titles and descriptions. Then, we randomly select a job for a given problem. Using GPT-4 turbo, we mimic the profession's characteristics, knowledge, and style to integrate the question into a real-world context. We manually review the modified questions to ensure they maintain the same semantics and graph structures as the original ones. A complete example of rephrasing is in Appendix B, and here we present the rephrased question of the above problem:

**Rephrased Question**

We're examining a simplified model of an ecosystem where [...], we've mapped out a series of interactions as follows: [(1, 2), (1, 3), (2, 3), (2, 4), (3, 5), (4, 5)]. [...] Can we analyze our network to reveal the minimum number of species that would need to be removed to disrupt the direct connection between any two species in this web? [...]

### Automated Evaluation

**Metrics.** To evaluate the ability of LLMs to solve these problems, we consider two metrics: pass rate and accuracy. Pass rate measures the ratio of executable code generated by an LLM, while accuracy measures the ratio of correct answers from the executable code. Accuracy is always no higher than the pass rate and is considered the more important metric as it evaluates final performance.

**Process.** Evaluating diverse answer formats with rule-based matching is challenging, and human evaluation is too labor-intensive. Thus, we automate evaluation using GPT-4o. First, we extract code snippets from LLM-generated answers using regular expressions. GPT-4o is then asked to check the correctness given the execution result. For problems with certain answers, such as true/false or calculation questions, GPT-4o assigns 1 point if the execution result matches the reference code's result, and 0 otherwise. For other problems, like drawing questions, GPT-4o matches key API usage: if the generated code contains \(m\) out of \(n\) key APIs, the accuracy point is \(m/n\).

**Rationale.** To validate GPT-based evaluation, we measure its stability (self-consistency) and alignment with human judgments (human-consistency). Higher stability means judgment scores are consistent across multiple evaluations, while higher human alignment indicates better quality. We use the agreement metric  to assess these consistencies. For \(n\) evaluations of the same answer, we take the highest number of evaluations \(m\) that received the same score and divide it by \(n\) to get the consistency. Self-consistency is the agreement among three GPT-4o evaluations, and human-consistency is the agreement between our GPT-4o evaluation and a manual evaluation. We evaluate all 512 problems with answers from _Claude 3 Opus RAG 7_, the best-performing will be introduced in Section 4.2, and present the results in Table 3, showing high self-consistency and human-consistency.

## 4 Datasets and Models

To enhance the ability of LLMs to solve graph problems with Python APIs, we construct LLM4Graph datasets. Based on the LLM4Graph, we enhance both closed-source and open-source models.

### Datasets

**Document dataset.** The document dataset is crawled from the official documents of the corresponding Python libraries. These documents can be directly used to enhance closed-source models via RAG.

    & **SC** (\%) & **HC** (\%) \\  Category 1 & 98.9 & 97.3 \\ Category 2 & 98.9 & 96.4 \\ Category 3 & 98.6 & 97.8 \\  Overall & 98.9 & 97.5 \\   

Table 3: Self-Consistency (SC) and Human-Consistency (HC) of automated evaluation.

We also use these documents for generating code datasets. Specifically, each API corresponds to a specific entry of the document dataset, including the description, parameter list, return value list, code examples, and other contents. An example of API entry is shown in Appendix C.

**Code datasets.** We construct two code datasets in the form of QA pairs. The questions in both datasets are the same, but the answers differ. In the simpler dataset, each answer only contains Python code. Inspired by Chain of Thought (CoT) , each answer in the more complex dataset additionally includes relevant APIs and their documents as prefixes. This modification can facilitate open-source models to utilize document information more effectively. We name the above code datasets as Code (QA) and Doc+Code (QA), respectively. Unlike the hand-crafted benchmark, problems in the code datasets are automatically generated and each contains only one key API.

Specifically, we first randomly select an API from the six Python libraries, and then employ GPT-4 turbo to generate example code for the API along with a corresponding question via back instruction . In the Code (QA) dataset, each answer only contains the Python code in the generated json. In the Doc+Code (QA) dataset, we design a template to additionally incorporate the API document information into each answer. This allows a CoT process that first selects the possibly needed APIs, then provides the corresponding API information, and finally writes code to solve the problem. Besides, the questions in both code datasets need to undergo the role-play processing in Section 3.3 for diverse problem descriptions. The prompt for back instruction and an example of the Doc+Code dataset can be found at Appendix D and F:

### Models

We use the above datasets to improve various LLMs in handling graph analysis tasks. For closed-source models, we enhance them by retrieving relevant information from the document dataset as RAG. For open-source models, we fine-tune them using code datasets as instruction tuning.

    & **Document** & **Code (QA)** & **Doc+Code (QA)** \\  Category 1 & 1,115 & 23,324 & 23,324 \\ Category 2 & 253 & 5,136 & 5,136 \\ Category 3 & 45 & 800 & 800 \\  Total & 1,413 & 29,260 & 29,260 \\   

Table 4: Statistics of LLM4Graph datasets.

Figure 2: **The pipeline of LLM4Graph dataset construction and corresponding model enhancement. We build the LLM4Graph dataset to improve the capabilities of LLMs in solving graph reasoning tasks through API calls. For the document dataset, we collect API documentation from the Internet. For code dataset, we automatically generate questions and corresponding codes with GPT-4 and API documentation. And for “doc+code” dataset, inspired by CoT reasoning, we add thought for each question, and combine the API documentation and the code dataset for construction.**

**Closed-source Models.** Due to the high difficulty of our ProGraph benchmark, mainstream LLMs (including Claude, GPT and Gemini) are not particularly strong in directly solving these problems. Therefore, before closed-source LLMs answer these questions, we retrieve the document information of top-\(K\) relevant APIs based on Llamaindex , allowing the models to learn through the context and enhance their performance. We explore \(K=3,5,7\) to investigate the impact of RAG, and the models will be given the corresponding suffix as No RAG, RAG 3, RAG 5 or RAG 7.

**Open-source Models.** We use the two code datasets (_i.e.,_ Code and Doc+Code) to fine-tune Llama-3-8B-Instruct  and Deepseek-Coder-7B-instruct , and consider the following model variants: (1) Code-only: LLMs are fine-tuned with the Code dataset. (2) Code + RAG 3/5/7: Code-only models are further equipped with RAG as closed-source ones. (3) Doc+Code: LLMs are fine-tuned with the Doc+Code dataset, and conduct inference by a corresponding two-step CoT reasoning process. In the first step, the model generates potential APIs based on the question. In the second step, we retrieve the API information and then provide the question along with the API information back to the model. The model then completes the remaining reasoning by writing code to solve the problem. The third group of models can maximize the use of document information to enhance the performance of open-source models, and significantly narrow the performance gap with closed-source large models.

## 5 Experiment

In this section, we present a comprehensive evaluation of our proposed benchmark, ProGraph, and the accompanying dataset, LLM4Graph. Our experiments assess the performance of both closed-source and open-source LLMs on graph analysis tasks. We demonstrate the limitations of current LLMs in handling structured graph data, and showcase the potential improvements achievable through the use of our datasets via RAG or instruction tuning. More results are shown in Appendix G and H.

### Experiment Settings and Baselines

In this work, we fine-tune two open-source models, Llama-3-8B-Instruct  and Deepseek-Coder-7B-Instruct-v1.5 , using the LLM4Graph dataset. For all models, we use alignment-handbook framework , set the learning rate, epochs, and max length as 2e-4, 4, and 4096, running on NVIDIA 8*A100 GPUs (40GB). The training batch size is set to 1 and evaluation batch size is set to 4. For testing, we set temperature as 0 and maximum output tokens as 4096, ensuring the stable and reasonable generation.

As our baselines, we employ models without RAG, including the GPT series (GPT-3.5, GPT-4 turbo, GPT-4o) , Claude 3 series (Haiku, Sonnet, Opus) , and Gemini Pro (1.0 and 1.5) , along with non-fine-tuned open-source smaller models Llama-3-8B-Instruct  and Deepseek-Coder-7B-Instruct-v1.5 . The detailed results are presented in Table 5.

### Benchmarking LLMs on ProGraph

We evaluate a variety of mainstream closed-source and open-source LLMs on the ProGraph, including GPT , Claude , Gemini , Llama 3  and Deepseek Coder .

    &  &  &  &  \\ 
**Model** & Pass Rate & Accuracy & Pass Rate & Accuracy & Pass Rate & Accuracy & Pass Rate & Accuracy \\  Claude 3 Haiku & 52.9 & 31.6 & 23.4 & 9.7 & 32.6 & 2.9 & 42.2 & 22.4 \\ Claude 3 Sonnet & 57.1 & 33.2 & 15.6 & 4.6 & 10.9 & 0.0 & 40.4 & 21.6 \\ Claude 3 Opus & 69.2 & 47.3 & 31.2 & 15.1 & **47.8** & **14.5** & 55.9 & 34.7 \\  GPT-3.5 & 64.1 & 35.1 & 24.7 & 8.4 & 15.2 & 1.1 & 47.9 & 24.0 \\ GPT-4 turbo & **72.4** & 42.1 & 39.0 & 14.8 & 41.3 & 12.0 & 59.6 & 31.2 \\ GPT-4o & 69.9 & **48.1** & **48.7** & **21.4** & 32.6 & 5.8 & **60.2** & **36.3** \\  Gemini 1.0 Pro & 48.7 & 27.7 & 9.1 & 1.7 & 19.6 & 3.3 & 34.2 & 17.7 \\ Gemini 1.5 Pro & 59.6 & 37.2 & 21.4 & 6.6 & 13.0 & 1.8 & 43.9 & 24.8 \\  Llama 3 & 36.5 & 17.3 & 12.3 & 3.8 & 15.2 & 0.4 & 27.3 & 11.7 \\ Deepseek Coder & 56.1 & 33.8 & 30.5 & 9.8 & 30.4 & 7.6 & 46.1 & 24.2 \\   

Table 5: Performance (%) of different models on ProGraph.

The results, presented in Table 5, indicate that none of the tested LLMs could effectively solve the problems in ProGraph. While GPT-4 turbo and its variant GPT-4o demonstrate relatively higher performance with an overall accuracy of 31.2% and 36.3% respectively, they still fall short in delivering satisfying accuracy across different categories. For instance, GPT-4 turbo achieves an accuracy of 42.1% in Category 1 but only 14.8% and 12.0% in Categories 2 and 3, respectively. Similar trends are observed in other models. These results highlight the challenges faced by current LLMs in addressing structured graph data as human experts. This necessitates the development of specialized datasets and fine-tuning approaches to bridge this performance gap.

### Enhancing Closed-Source LLMs with Document Dataset

To investigate the potential of enhancing LLMs' performance on graph analysis tasks, we utilize RAG techniques to extract relevant API information from the documents in LLM4Graph. This augmented context is then provided to the LLMs to assist in generating more accurate and effective solutions.

Figure 3 shows the performance gains for four top closed-source LLMs: GPT-4 turbo, GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro. All models show significant boosts in pass rate and accuracy, with more than a 5% improvement in accuracy. Gemini 1.5 Pro and Claude 3 Opus achieve over 10% accuracy improvement with five retrieved APIs. However, performance improvements plateau with additional API information, which may be attributed to the saturation of relevant information, where additional API details no longer contribute to further understanding and instead introduce redundancy or noise. This observation aligns with the findings of previous studies on RAG .

The effectiveness of the LLM4Graph in enhancing LLM capabilities on graph analysis tasks is evident from these results. By incorporating a RAG mechanism with the LLM4Graph, we demonstrate that it is possible to substantially improve the performance of closed-source LLMs without the need for extensive fine-tuning or architectural modifications. This approach offers a practical and efficient solution for adapting existing LLMs to handle structured graph data more effectively.

### Enhancing Open-Source LLMs with Code Dataset and Doc+Code Dataset

We investigate the potential of enhancing open-source LLMs' performance on graph analysis tasks by fine-tuning them using LLM4Graph and augmenting the models with RAG. Experiments are conducted on a general-purpose model, Llama-3-8B-Instruct , and a model specifically designed for code generation, Deepseek-Coder-7B-Instruct-v1.5 . The results, presented in Figure 4, demonstrate that our LLM4Graph can significantly improve the performance of different types of open-source small models. The accuracy of both models, after being fine-tuned merely on the code data within LLM4Graph, substantially surpasses the best result from closed-source models without RAG. The Doc+Code setting further enhances the models' performance, leading to comparable or even better accuracy than the best result from closed-source model with RAG.

However, augmenting the open-source models fine-tuned on the code with RAG mechanism does not further improve the performance, and even leads to degraded performance. We hypothesize that this discrepancy may be attributed to the limited ability of these small models to process long text, hindering their utilization of the document information. The additional information provided by RAG may introduce confusion in understanding the problem statement and arriving at the correct solution. Overall, our proposed Doc+Code setting proves to be an effective means of integrating document information from LLM4Graph, significantly enhancing the accuracy of open-source models. We show that LLM4Graph can serve as effective data to enhance the model's code-generation capabilities and lead to better utilization of various graph libraries.

Figure 3: The pass rate (left) and accuracy (right) of closed-source models with RAG.

## 6 Analysis

In this section, we present a comprehensive analysis of the performance of different models on the ProGraph. By grouping the benchmark based on categories, answer difficulties, and question type, we aim to provide a granular exploration of the strengths and limitations of these models in handling graph analysis tasks. We also present the types of compilation errors made by different models.

### Performance Analysis on Different Benchmark Groupings

To gain a deeper understanding of the capabilities of LLMs and fine-tuned smaller models presented in Section 5.4, we analyze their performances on ProGraph from three different perspectives: task category, answer difficulty and question type.

**Task Category.** We analyze the model performance based on different categories in the ProGraph, as shown in Figure 4(a). Mainstream LLMs and fine-tuned smaller models exhibit similar performance on graph theory and graph statistical learning tasks. However, a significant disparity is observed in their performance on graph embedding tasks, where fine-tuned smaller models substantially outperform RAG-enhanced large models. This observation suggests that not all graph analysis tasks can be easily handled by closed-source LLMs without further fine-tuning. More complex and challenging tasks still require fine-tuning for effective learning.

**Answer Difficulty.** We further examine the model performance based on different answer difficulties, _i.e.,_ true/false, calculation, drawing, and hybrid. In Figure 4(b), we plot the performance of different models on these answer difficulties separately. Mainstream LLMs excel in true/false and drawing types but struggle with calculation and hybrid ones. Fine-tuned smaller models demonstrate improvements across various answer difficulties, especially on the complex hybrid type, indicating the effectiveness of our proposed enhancement strategies.

**Question Type.** Lastly, we divide the ProGraph into two levels of difficulty: easy (involving only one API) and hard (involving multiple APIs). As shown in Figure 4(c), mainstream closed-source large models perform well on easy-level problems, with accuracies generally approaching or exceeding 50%. However, their performance significantly deteriorates on hard-level ones, with the highest accuracy reaching only 32.5%. This observation suggests that mainstream LLMs have limitations when the number of required APIs increases. In contrast, our fine-tuned models demonstrate significantly higher accuracy on hard-level problems, approaching or exceeding 40%, yielding approximately an

Figure 4: The pass rate (left) and accuracy (right) of open-source models with instruction tuning.

Figure 5: The performance of six best-performing models on different groupings of ProGraph.

improvement of 8% compared to the best closed-source LLM. Note that LLM4Graph only contains data involving one API. Still, the models fine-tuned on LLM4Graph show strong generalizability on problems requiring multiple APIs.

### Compilation Error Analysis

To gain insights into the types of compilation errors made by different models, we conduct an error analysis on the best-performing closed-source models (GPT:4 turbo RAG 5 and Claude 3 Opus RAG 7) and fine-tuned open-source small models (DeepSeek Coder Doc+Code and Llama 3 Doc+Code). As shown in Figure 6, we categorize the errors into ten distinct types to identify patterns and differences in the error distributions among these models.

Our analysis reveals that closed-source models exhibit a low similarity in their error cases, suggesting that they possess varying coding capabilities. For instance, GPT-4 turbo often makes SyntaxError, but rarely ImportError, which is contrary to Claude 3 Opus. The fine-tuned open-source small models exhibit a high similarity in their error distributions, with AttributeError being the most dominant.

The error analysis also highlights some common challenges faced by all models, such as AttributeError and TypeError, suggesting that models may have difficulty in memorizing and understanding the attribute of class objects from various python libraries, and the type of returned results from different functions. Interestingly, the fine-tuned models have a notably lower percentage of SyntaxError compared to the closed-source models, indicating that further fine-tuning on the LLM4Graph helps the models learn better code syntax and structure.

## 7 Conclusion

In this paper, we introduce ProGraph, a novel and challenging benchmark for evaluating LLMs in graph analysis using external APIs. Current LLMs achieve only 36% accuracy, revealing their limitations. To bridge this gap, we further construct LLM4Graph, a dataset with crawled documents and auto-generated codes based on popular graph libraries. The datasets can help improve the accuracy of both closed-source and open-source LLMs by 11-32% through RAG and instruction tuning. Our work highlights the potential of enhancing LLMs with our LLM4Graph, offering valuable resources for advancing LLM capabilities in structured data analysis. Discussions about limitations and boarder impacts can be found in Appendix A.

## 8 Acknowledgements

This work is supported by the National Natural Science Foundation of China (No.62192784, 62236004), the National Key R&D Program of China (No.2022ZD0116312), Young Elite Scientists Sponsorship Program (No.2023QNRC001) by CAST, and Tsinghua University Initiative Scientific Research Program.