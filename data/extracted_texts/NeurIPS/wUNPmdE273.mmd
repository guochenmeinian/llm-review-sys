# Transitivity Recovering Decompositions:

Interpretable and Robust Fine-Grained Relationships

Abhra Chaudhuri\({}^{1,5,6}\) Massimiliano Mancini\({}^{2}\) Zeynep Akata\({}^{3,4}\) Anjan Dutta\({}^{5,6}\)

\({}^{1}\) University of Exeter

\({}^{2}\) University of Trento

\({}^{3}\) University of Tubingen

\({}^{4}\) MPI for Informatics

The Alan Turing Institute

University of Surrey

Abhra Chaudhuri (ac1151@exeter.ac.uk) is the corresponding author.

Massimiliano Mancini\({}^{2}\) Zeynep Akata\({}^{3,4}\) Anjan Dutta\({}^{5,6}\)

\({}^{1}\) University of Exeter

\({}^{2}\) University of Trento

\({}^{3}\) University of Tubingen

\({}^{4}\) MPI for Informatics

The Alan Turing Institute

University of Surrey

###### Abstract

Recent advances in fine-grained representation learning leverage local-to-global (emergent) relationships for achieving state-of-the-art results. The relational representations relied upon by such methods, however, are abstract. We aim to deconstruct this abstraction by expressing them as interpretable graphs over image views. We begin by theoretically showing that abstract relational representations are nothing but a way of recovering transitive relationships among local views. Based on this, we design Transitivity Recovering Decompositions (TRD), a graph-space search algorithm that identifies interpretable equivalents of abstract emergent relationships at both instance and class levels, and with no post-hoc computations. We additionally show that TRD is _provably robust_ to noisy views, with empirical evidence also supporting this finding. The latter allows TRD to perform at par or even better than the state-of-the-art, while being fully interpretable. Implementation is available at https://github.com/abhrac/trd.

## 1 Introduction

Identifying discriminative object parts (local views) has traditionally served as a powerful approach for learning fine-grained representations [95; 85; 43]. Isolated local views, however, miss out on the larger, general structure of the object, and hence, need to be considered in conjunction with the global view for tasks like fine-grained visual categorization (FGVC) . Additionally, the way in which local views combine to form the global view (local-to-global / emergent relationships ) has been identified as being crucial for distinguishing between classes that share the same set of parts, but differ only in the way the parts relate to each other [32; 97; 10; 9; 61; 12]. However, representations produced by state-of-the-art approaches that leverage the emergent relationships are encoded in an abstract fashion - for instance, through the summary embeddings of transformers [9; 10], or via aggregations on outputs from a GNN [97; 27]. This makes room for the following question that we aim to answer through this work: _how can we make such abstract relational representations more human-understandable?_ Although there are several existing works that generate explanations for localized, fine-grained visual features [11; 78; 36; 63], providing interpretations for abstract representations obtained for discriminative, emergent relationships still remains unaddressed.

Illustrated in Figure 1, we propose to make existing relational representations interpretable through bypassing their abstractions, and expressing all computations in terms of a graph over image views, both at the level of the image as well as that of the class (termed _class proxy_, a generalized representation of a class). At the class level, this takes the form of what we call a Concept Graph: a generalized relational representation of the salient concepts across all instances of that class. We use graphs, as they are a naturally interpretable model of relational interactions for a variety of tasks [86; 18; 1; 46], allowing us to visualize entities (e.g., object parts, salient features), and their relationships.

We theoretically answer the posed question by introducing the notion of transitivity (Definition 3) for identifying _subsets_ of local views that strongly influence the global view. We then show that the relational abstractions are nothing but a way of encoding transitivity, and design Transitivity Recovering Decompositions (TRD), an algorithm that decomposes both input images and output classes into graphs over views by recovering transitive cross-view relationships. Through the process of transitivity recovery (Section 3.2), TRD identifies maximally informative subgraphs that co-occur in the instance and the concept (class) graphs, providing end-to-end relational transparency. In practice, we implement TRD by optimizing a GNN to match a graph-based representation of the input image to the concept graph of its corresponding class (through minimizing the Hausdorff Edit Distance between the two). The input image graph is obtained by decomposing the image into its constituent views [10; 84; 94], and connecting complementary sets of views. The concept graph is obtained by performing an online clustering on the node and edge embeddings of the training instances of the corresponding class. The process is detailed in Section 3.3.

We also show that TRD is _provably robust_ to noisy views in the input image (local views that do not / negatively contribute to the downstream task ). We perform careful empirical validations under various noise injection models to confirm that this is indeed the case in practice. The robustness allows TRD to always retain, and occasionally, even boost performance across small, medium, and large scale FGVC benchmarks, circumventing a known trade-off in the explainability literature [31; 73; 23; 37]. Finally, the TRD interpretations are ante-hoc - graphs encoding the learned relationships are produced as part of the inference process, requiring no post-hoc GNN explainers [88; 92; 34; 68; 90].

In summary, with the purpose of bringing interpretability to abstract relational representations, we make the following contributions - (1) show the existence of graphs that are equivalent to abstract local-to-global relational representations, and derive their information theoretic and topological properties; (2) design TRD, a provably robust algorithm that generates such graphs in an ante-hoc manner, incorporating transparency into the relationship computation pipeline at both instance and class levels; (3) extensive experiments demonstrating not only the achieved interpretability and robustness, but also state-of-the-art performance on benchmark FGVC datasets.

## 2 Related Work

**Fine-grained visual categorization:** Learning localized image features [2; 96; 47], with extensions exploiting the relationship between multiple images and between network layers  were shown to be foundational for FGVC. Strong inductive biases like normalized object poses [6; 87] and data-driven methods like deep metric learning  were used to tackle the high intra-class and low inter-class variations. Unsupervised part-based models leveraged CNN feature map activations [35; 94] or identified discriminative sequences of parts . Novel ways of training CNNs for FGVC included boosting , kernel pooling , and channel masking . Vision transformers, with their ability to learn localized features, had also shown great promise in FGVC [81; 29; 50]. FGVC interpretability has so far focused on the contribution of individual parts [11; 20]. However, the usefulness of emergent relationships for learning fine-grained visual features was demonstrated by

Figure 1: Instead of learning representations of emergent relationships that are abstract aggregations of views, our method deconstructs the input, latent, and the class representation (proxy) spaces into graphs, thereby ensuring that all stages along the inference path are interpretable.

[83; 97; 9; 10]. Our method gets beyond the abstractions inherent in such approaches, and presents a fully transparent pipeline by expressing all computations in terms of graphs representing relationships, at both the instance and the class-level.

**Relation modeling in deep learning:** Relationships between entities serve as an important source of semantic information as has been demonstrated in graph representation learning [72; 18], deep reinforcement learning , question answering , object detection , knowledge distillation , and few-shot learning . While the importance of learning relationships between different views of an image was demonstrated in the self-supervised context [61; 9],  showed how relational representations themselves can be leveraged for tasks like FGVC. However, existing works that leverage relational information for representation learning typically (1) model all possible ways the local views can combine to form the global view through a transformer, and distill out the information about the most optimal combination in the summary embedding [9; 10], or (2) model the underlying relations through a GNN, but performing an aggregation on its outputs [27; 97]. Both (1) and (2) produce vector-valued outputs, and such, cannot be decoded in a straightforward way to get an understanding of what the underlying emergent relationships between views are. This lack of transparency is what we refer to as "abstract". The above set of methods exhibit this abstraction not only at the instance, but at the class-level as well, which also appear as vector-valued embeddings. On the contrary, an interpretable relationship encoder should be able to produce graphs encoding relationships in the input, intermediate, and output spaces, while also representing a class as relationships among concepts, instead of single vectors that summarize information about emergence. This interpretability is precisely what we aim to achieve through this work.

**Explainability for Graph Neural Networks:** GNNExplainer  was the first framework that proposed explaining GNN predictions by identifying maximally informative subgraphs and subset of features that influence its predictions. This idea was extended in  through exploring subgraphs via Monte-Carlo tree search and identifying the most informative ones based on their Shapley values . However, all such methods inherently provided either local [88; 34; 68] or global explanations , but not both. To address this issue, PGExplainer  presented a parameterized approach to provide generalized, class-level explanations for GNNs, while  obtained multi-grained explanations based on the pre-training (global) and fine-tuning (local) paradigm.  proposed a GNN explanation approach from a causal perspective, which led to better generalization and faster inference. On the other hand,  improved explanation robustness by modeling decision regions induced by similar graphs, while ensuring counterfactuality. Although ViG  presents an algorithm for representing images as a graph of its views, it exhibits an abstract computation pipeline lacking explainability. A further discussion on this is presented in Appendix A.10. Also, generating explanations for graph metric-spaces [99; 45] remains unexplored, which we aim to address through this work.

## 3 Transitivity Recovering Decompositions

Our methodology is designed around three central theorems - (i) Theorem 1 shows the existence of semantically equivalent graphs for every abstract representation of emergent relationships, (ii) Theorem 2 establishes an information theoretic criterion for identifying such a graph, and (iii) Theorem 3 shows how transitivity recovering functions can guide the search for candidate solutions that satisfy the above criterion. Additionally, Theorem 4 formalizes the robustness of transitivity recovering functions to noisy views. Due to space constraints, we defer all proofs to the Appendix A.9.

**Preliminaries:** Consider an image \(\) with a categorical label \(\) from an FGVC task. Let \(=c_{g}()\) and \(=\{_{1},_{2},...\,_{k}\}=c_{l}( )\) be the global and set of local views of an image \(\) respectively, jointly denoted as \(=\{\}\), where \(c_{g}\) and \(c_{l}\) are cropping functions applied on \(\) to obtain such views. Let \(f\) be a semantically consistent, relation-agnostic encoder (Appendices A.4 and A.7) that takes as input \(\) and maps it to a latent space representation \(^{n}\), where \(n\) is the representation dimensionality. Specifically, the representations of the global view \(\) and local views \(\) obtained from \(f\) are then denoted by \(_{g}=f()\) and \(_{}=\{f():\}=\{ _{_{1}},_{_{2}},...\,_{_{k}}\}\) respectively, together denoted as \(_{}=\{_{g},_{_{1}},_{_{2}},...\,_{_{k}}\}\). Let \(\) be a function that encodes the relationships \(^{n}\) between the global (\(\)) and the set of local (\(\)) views. DiNo  and Relational Proxies  can be considered as candidate implementations for \(\). Let \(\) be a set of graphs \(\{(,_{1},_{_{1}},_{_{1}}),(,_{2},_{_{2}},_{ _{2}}),...\}\), where the nodes in each graph constitute of the view set \(\), and the edge set \(_{i}()\), where \(\) denotes the power set. \(||=|()|\), meaning that \(\) is the set of all possible graph topologies with \(\) as the set of nodes. The node features \(_{_{i}}\), and the edge features \(_{_{i}}^{n}\).

### Decomposing Relational Representations

**Definition 1** (Semantic Relevance Graph).: A graph \(_{s}\) of image views where each view-pair is connected by an edge with strength proportional to their joint relevance in forming the semantic label \(\) of the image. Formally, the weight of an edge \(E_{S_{ij}}\) connecting \(_{i}\) and \(_{j}\) is proportional to \(I(_{i}_{j};)\).

Intuitively, \(E_{S_{ij}}\) is a measure of how well the two views pair together as compatible puzzle pieces in depicting the central concept in the image.

**Theorem 1**.: _For a relational representation \(\) that minimizes the information gap \(I(;|_{})\), there exists a metric space \((,d)\) that defines a Semantic Relevance Graph such that:_

\[d(_{i},)> d(_{j}, )>\] \[~{}:^{n}}^ {n}~{}|~{}=((_{i}),(_{j})),d( ,)\]

_where \(^{n}\), \(}\) and \(d\) denote semantically consistent transformations and distance metrics respectively, \(:_{}}\), and \(\) is some finite, empirical bound on semantic distance._

_Intuition_: The theorem says that, even if \(_{i}\) and \(_{j}\) individually cannot encode anymore semantic information, the metric space \((,d)\) encodes their relational semantics via its distance function \(d\). Assuming the theorem to be false would mean that the outputs of the aggregation do not allow for a distance computation that is semantically meaningful, and hence, there is no source from which \(\) (the relational embedding) can be learned. This would imply that either (i) the information gap (Appendix A.2) does not exist, which is false, because \(f\) is a relation-agnostic encoder, or (ii) all available metric spaces are relation-agnostic and hence, the information gap would always persist. The latter is again in contrary to what is shown in Appendix A.6, which derives the necessary and sufficient conditions for a model to bridge the information gap. Thus, \((,d)\) defines \(_{s}\) with weights \(1/d((_{i}),(_{j})) I(_{i} _{j};)\), where \(:\).

**Corollary 1.1** (Proxy / Concept Graph).: _Proxies can also be represented by graphs._

_Intuition_: Since \((,d)\) is equipped with a semantically relevant distance function, the hypersphere enclosing a locality/cluster of local/global view node embeddings can be identified as a semantic relevance neighborhood. The centers of such hyperspheres can be considered as a proxy representation for each such view, generalizing some salient concept of a class. The centers of each such view cluster can then act as the nodes of a proxy/concept graph, connected by edges, that are also obtained in a similar manner by clustering the instance edge embeddings.

**Theorem 2**.: _The graph \(^{*}\) underlying \(\) is a member of \(\) with maximal label information. Formally,_

\[^{*}=_{}I(,)\]

_Intuition_: The label information \(I(,)\) can be factorized into relation-agnostic and relation-aware components (Appendix A.2). The node embeddings in \(^{*}\) already capture the relation-agnostic component (being derived from \(f\)). From the implication in Theorem 1, we know that given the node embeddings, \(\) is able to reduce further uncertainty about the label by joint observation of view pairs (or in other words, edges in \(^{*}\)). Hence \(\) must be relation-aware, as the relation-agnostic component is already captured by \(f\). Thus, \(G^{*}\), which is obtained as a composition of \(f\) and \(\), must be a _sufficient_ representation of \(\) (Appendices A.2 and A.3). Since sufficiency is the upper-bound on the amount of label information that a representation can encode (Appendix A.3), \(^{*}\) is the graph that satisfies the condition in the theorem. In the section below, we show that the way to obtain \(^{*}\) from \(\) is by recovering transitive relationships among local views.

### Transitivity Recovery

**Definition 2** (Emergence).: The degree to which a set of local views \((_{1},_{2},..._{k})\) contributes towards forming the global view. It can be quantified as \(I(_{1}_{2}..._{k};)\).

**Definition 3** (**Transitivity)**.: Local views \(_{1},_{2}\), and \(_{3}\) are transitively related _iff_, when any two view pairs have a high contribution towards emergence, the third pair also has a high contribution. Formally,

\[I(_{i}_{j};)> I(_{j} _{k};)> I(_{i}_{k}; )>,\]

where \((i,j,k)\{1,2,3\}\) and \(\) is some threshold for emergence. A function \(()\) is said to be _transitivity recovering_ if the transitivity between \(_{1},_{2}\), and \(_{3}\) can, in some way, be inferred from the output space of \(\). This helps us quantify the _transitivity_ of _emergent_ relationships across partially overlapping sets of views.

**Theorem 3**.: _A function \(()\) that reduces the uncertainty \(I(_{i}_{j};|(_{i})( _{j}))\) about the emergent relationships among the set of views can define a \(_{s}\) by being transitivity recovering._

_Intuition_: Reducing the uncertainty about transitivity among the set of views is equivalent to reducing the uncertainty about the emergent relationships, and hence, bridging the information gap. If \(_{1},_{2}\), and \(_{3}\) are not transitively related, then it would imply that, at most, only one of the three views (\(_{j}\)) is semantically relevant, and the other two (\(_{i},_{k}\)) are not. This leads to a _degenerate_ form of emergence. Thus, transitivity is the key property in the graph space that filters out such degenerate subsets in \(\), helping identify view triplets where all the elements contribute towards emergence. A further discussion can be found in Appendix A.11. Now, we know that relational information must be leveraged to learn a sufficient representation. So, for sufficiency, a classifier operating in the graph space must leverage transitivity to match instances and proxies. In the proof, we hence show that transitivity in \(\) is equivalent to the relational information \(^{n}\). Thus, the explanation graph \(^{*}\) is a maximal MI member of \(\) obtained by applying a transitivity recovering transformation on the Complementarity Graph.

### Generating Interpretable Relational Representations

Depicted in Figure 2, the core idea behind TRD is to ensure relational transparency by expressing all computations leading to classification on a graph of image views (Theorem 1), all the way up to the class-proxy, by decomposing the latter into a concept graph (Corollary 1.1). We ensure the sufficiency (Theorem 2) of the instance and proxy graphs through transitivity recovery (Theorem 3) by Hausdorff Edit Distance minimization.

**Complementarity Graph:** We start by obtaining relational-agnostic representations \(_{}\) from the input image \(\) by encoding each of its views \(\) as \(f()\). We then obtain a Complementarity Graph \(_{c}\) with node features \(_{}=_{}\), and edge strengths inversely proportional to the value of the mutual information between the local view embeddings \(I(_{i},_{j})\) that the edge connects. Specifically, we instantiate the edge features \(_{}\) as learnable \(n\)-dimensional vectors, all with the same value of

Figure 2: The TRD pipeline: We begin by computing a Complementarity Graph (\(_{c}\)) on the view embeddings obtained from \(f\). The Graph Encoder derives the Semantic Relevance Graph (\(_{s}\)) through transitivity recovery on \(_{c}\). We perform a class-level clustering of the instance node and edge embeddings, producing a proxy graph, representing the salient concepts of a class and their interrelationships. The sufficiency of \(_{s}\) is guaranteed by minimizing its Hausdorff distance with its proxy graph.

\(1/|_{i}_{j}|\). This particular choice of calculating the edge weights is valid under the assumption that \(f\) is a semantically consistent function. Intuitively, local views with low mutual information (MI) are complementary to each other, while ones with high MI have a lot of redundancy. The _inductive bias_ behind the construction of such a graph is that, strengthening the connections among the complementary pairs suppresses the flow of redundant information during message passing, thereby reducing the search space in \(\) for finding \(^{*}\). The global view \(\), however, is connected to all the local-views with a uniform edge weight of \(1^{n}\). This is because the local-to-global emergent information, _i.e._, \(I(,_{j},...;)\), is what we want the model to discover through the learning process, and hence, should not incorporate it as an inductive bias.

**Semantic Relevance Graph:** We compute the Semantic Relevance Graph \(_{s}\) by propagating \(_{c}\) through a Graph Attention Network (GAT) . The node embeddings obtained from the GAT correspond to the \(()\) in our theorems. We ensure that \(\) is transitivity recovering by minimizing the Learnable Hausdorff Edit Distance between the instance and the proxy graphs (Corollary 1.1), which is known to take into account the degree of local-to-global transitivity for dissimilarity computation . Below, we discuss this process in further detail.

**Proxy / Concept Graph:** We obtain the proxy / concept graph for each class via an online clustering of the Semantic Relevance node and edge embeddings (\(_{n}()\), \(_{e}()\) respectively) of the train-set instances of that class, using the Sinkhorn-Knopp algorithm [16; 8]. We set the number of node clusters to be equal to \(|V|\). The number of edge clusters (connecting the node clusters) is equal to \(|V|(|V|-1)/2\). Note that this is the case because all the graphs throughout our pipeline are complete. Based on our theory of transitivity recovery, sets of semantically relevant nodes should form cliques in the semantic relevance and proxy graphs, and remain disconnected from the irrelevant ones by learning an edge weight of \(0^{n}\). We denote the set of class proxy graphs by \(=\{_{_{1}},_{_{2}},..., _{_{k}}\}\), where \(k\) is the number of classes.

**Inference and Learning objective:** To recover end-to-end explainability, we need to avoid converting the instance and the proxy graphs to abstract vector-valued embeddings in \(^{n}\) at all times. For this purpose, we perform matching between \(_{s}\) and \(_{}\) via a graph kernel . This kernel trick helps us bypass the computation of the abstract relationship \(:^{n}\)2, and perform the distance computation directly in \(\), using only the graph-based decompositions, and thereby keeping the full pipeline end-to-end explainable.

We choose to use the Graph Edit Distance (GED) as our kernel . Although computing GED has been proven to be NP-Complete , quadratic time Hausdorff Edit Distance (HED)  and learning-based approximations  have been shown to be of practical use. However, such approximations are rather coarse-grained, as they are either based on linearity assumptions , or consider only the substitution operations . Learnable (L)-HED  introduced two additional learnable costs for node insertions and deletions using a GNN, thereby making it a more accurate approximation. We observe that our \(\) already performs the job of the encoder prefix in L-HED (Appendix A.12). Thus, we simply apply a fully connected MLP head \(:\) with shared weights on the \(()\) embeddings and the vertices of \(_{p}\) to compute the node insertion and deletion costs. Thus, in our case, L-HED takes the following form:

\[h(_{s},_{p})=_{_{s}}_{_{p}}c(,)+_{ _{p}}_{_{s}}c(, )\] (1)

\[c(,)=||()||,$),}\\ ||()||,$),}\\ ||-||/2,,\]

where \(=1/(2||)\), and \(\) is the empty node . We use \(h(,)\) as a distance metric for the Proxy Anchor Loss , which forms our final learning objective, satisfying Theorem 2. Below we discuss how transitivity recovery additionally _guarantees_ robustness to noisy views.

### Robustness

Let \(\) be the data distribution over \(\). Consider a sample \(\) composed of views \(=\{_{1},_{2},...,_{k}\}\) with label \(\). A view \(\) is said to be _noisy_ if \(() y\). The fraction of noisy views for a given sample is denoted by \(\). Let \(f\) be a classifier that minimizes the empirical risk of the mapping \(f()\) in the ideal noise-free setting. Let \(^{}\) be the set of noisy views in \(\) with topology \(^{}\). Let \(^{*}=-^{}\) be the set of noise free (and hence, transitively related - Theorem 3) views with topology \(^{*}\). Recall that \(^{*}\) is the optimal graph of \(\) satisfying the sufficiency criterion in Theorem 2, and \(_{}\) is its proxy graph.

**Theorem 4**.: _In the representation space of \(\), the uncertainty in estimating the topology \(^{}\) of \(^{}\) is exponentially greater in the error rate \(\), than the uncertainty in estimating the topology \(^{*}\) of \(^{*}\)._

_Intuition_: This theorem is based on the fact the number of topologies that a set of noisy views can assume, is exponentially more (in the desired error rate) to what can be assumed by a set of transitive views. Since \(\) is explicitly designed to be transitivity recovering, making predictions using \(\) based on noisy views would go directly against its optimization objective of reducing the output entropy (due to the exponentially larger family of topologies to choose from). So, to minimize the uncertainty in Equation (1), \(\) would always make predictions based on transitive views, disregarding all noisy views in the process, satisfying the formal requirement of robustness (Definition 4). In other words, the following property of TRD allowed us to arrive at this result - the structural priors that we have on the transitive subgraphs make them the most optimal candidates for reducing the prediction uncertainty, relative to the isolated subgraphs of noisy views which do not exhibit such regularities.

## 4 Experiments

### Experimental Settings & Datasets

**Implementation Details:** For obtaining the global view, we follow [84; 94] by selecting the smallest bounding box containing the largest connected component of the thresholded final layer feature map obtained from an ImageNet-1K  pre-trained ResNet50 , which we also use as the relation-agnostic encoder \(f\). The global view is resized to \(224 224\). We then obtain 64 local views by randomly cropping \(28 28\) regions within the global crop and resizing them to \(224 224\). We use a 8-layer Graph Attention Network (GAT) (, with 4 attention heads in each hidden layer, and normalized via GraphNorm  to obtain the Semantic Relevance Graph. We train TRD for 1000 epochs using the Adam optimizer, at an initial learning rate of 0.005 (decayed by a factor of 0.1 every 100 epochs), with a weight decay of \(5 10^{-4}\). We generally followed [77; 10] for choosing the above settings. We implement TRD with a single NVIDIA GeForce RTX 3090 GPU, an 8-core Intel Xeon processor, and 32 GBs of RAM.

**Datasets:** To verify the generalizability and scalability of our method, we evaluate it on small, medium and large-scale FGVC benchmarks. We perform small-scale evaluation on the Soy and Cotton Cultivar datasets , while we choose FGVC Aircraft , Stanford Cars , CUB ,

Figure 3: Sample TRD explanations on CUB. Top: 8 nearest neighbors (columns) of the top-2 proxy nodes / concepts (rows) with highest emergence (Definition 2). Bottom: Instance (global view with green border) and proxy (global view with blue border) explanation graphs constructed using the top-6 nodes (nearest train set neighbor for the proxy nodes) with highest emergence. The edge-weights are \(L_{2}\)-norms of the edge embeddings (best visible on screen with zooming).

and NA Birds  for medium scale evaluation. For large-scale evaluation, we choose the iNaturalist dataset , which has over 675K train set and 182K test set images.

### Interpretability & Robustness

**Qualitative Results:** Figure 3 shows sample explanations obtained using TRD on the CUB dataset. The top rows represent proxy nodes (concepts), and the columns represent the 8 nearest neighbors of the corresponding proxy nodes, which is highly consistent across concepts and classes. In the bottom are instance and proxy explanation graphs (top-6 nodes with highest emergence) from very similar-looking but different classes. For the proxies, the nearest train set neighbors to the node embeddings are visualized. Even with similar appearance, the graphs have very different structures. This shows that TRD is able to recover the discriminative relational properties of an image, and encode them as graphs. We provide additional visualizations in the supplementary.

**Quantitative Evaluation:** We quantitatively evaluate the explanations obtained using TRD based on the Fidelity (relevance of the explanations to the downstream task) _vs_ Sparsity (precision of the explanations) plots for the generated explanations, which is a standard way to measure the efficacy of GNN explainability algorithms [92; 62; 91]. We compare against SOTA, generic GNN explainers, namely SubgraphX , PGExplainer , and GNNExplainer , on candidate small (Cotton), medium (FGVC Aircraft), and large (iNaturalist) scale datasets, and report our findings in Figure 4. TRD consistently provides the highest levels of Fidelity not only in the dense, but also in the high sparsity regimes, outperforming generic GNN explainers by significant margins. Unlike the generic explainers, since TRD takes into account the edit distance with the proxy graph for computing the explanations, it is able to leverage the class-level topological information, thereby achieving higher precision. The supplementary contains results on the remaining datasets, details on the Fidelity and Sparsity metrics, and experiments on the functional equivalence of TRD with post-hoc explanations.

**Robustness to Noisy-Views:** Figure 5 shows the performance of TRD on FGVC Aircraft under the following two noise injection models - (1) The local views are sampled uniformly at random across

Figure 4: Fidelity _vs_ Sparsity curves of TRD and SOTA GNN interpretability methods.

Figure 5: Classification accuracies of relational learning based methods with different noise models on FGVC Aircraft. Left: Increasing number of local views sampled randomly across the entire image (instead of just the global view). Right: Increasing the _proportion_ of noisy views in a controlled manner by sampling from the region outside of the global view.

the entire image, instead of just the global view. This model thus randomizes the amount of label information present in \(||\). (2) A fraction \(\) of the local views are sampled from outside the global view, and the remaining ones (\(1-\)) come from within the global view, where \(\) is the variable across experiments. This puts a fixed upper bound to the amount of label information in \(||\). It can be seen that under model (1), TRD is significantly more stable to changing degrees of uncertainty in label information compared to other relational learning-based methods. Under model (2), TRD outperforms existing methods as the amount of label information is decreased by deterministically increasing the noise rate. As discussed in Section 3.4 and formally proved in Appendix A.9.1, transitivity acts as a semantic invariant between the instance and the proxy graph spaces, which is a condition that subgraphs induced by the noisy views do not satisfy, leading to the observed robustness.

**Effect of Causal Interventions:** Here, we aim to understand the behaviour of our model under corruptions that affect the underlying causal factors of the data generating process. To this end, we train TRD by replacing a subset of the local views for each instance with local views from other classes, both during training and inference. As the proxies are obtained via a clustering of the instance graphs, these local views consequently influence the proxy graphs. We report our quantitative and qualitative findings in Table 1 and Figure 6 (Appendix A.17) respectively. TRD significantly outperforms Relational Proxies at all noise rates (percentage of local views replaced), and the gap between their performances widens as the percentage of corruption increases (Table 1). Qualitatively (Figure 6), our model successfully disregards the views introduced from the negative class at both the instance and proxy level. Such views can be seen as being very weakly connected to the global view, as well as the correct set of local views that actually belong to that class. Under this causal intervention, the TRD objective is thus equivalent to performing classification while having access to only the subgraph of clean views from the correct class.

### FGVC Performance

**Comparison with SOTA:** Table 2 compares the performance of TRD with SOTA on benchmark FGVC datasets. Some of the most commonly used approaches involve some form of regularizer , bilinear features , or transformers . However, methods that use multi-view information , especially their relationships  for prediction have been the most promising, although their final image and class representations are abstract. TRD can be seen to provide the best performance across all benchmarks, while also being fully interpretable. We attribute this to the robustness of our method to noisy views, which is known to have a significant impact on metric learning algorithms .

**Ablation Studies:** Table 3 shows the contributions of the components of TRD, namely Complementarity Graph: CG (Section 3.3), Proxy Decomposition: PD (Corollary 1.1), and Transitivity Recovery: TR (Theorem 3) to its downstream classification accuracy on the FGVC Aircraft, Cotton, and Soy Cultivar datasets. Row 0 is the baseline with none of our novel components. Row 1 corresponds to the setting where the proxy representation is still abstract, _i.e._, in \(^{n}\). We concatenate the nodes in \(_{s}\) and propagate it through a 1-hidden layer MLP to summarize it in \(^{n}\). As we decompose the proxy into a graph (PD, Row 2) based on the instance node and edge clustering-based approach as discussed in Section 3.3 for class-level interpretability, we can see that the performance

   \(\) & 10 & 20 & 30 & 40 & 50 \\  Relational Proxies & 93.22 & 87.12 & 79.35 & 70.99 & 63.60 \\ 
**TRD (Ours)** & **94.90** & **91.54** & **82.80** & **76.35** & **70.55** \\   

Table 1: Quantitative performance comparison between Relational Proxies and TRD with increasing rates of corruption (\(\): percentage local views from different classes).

   ID & **CG** & **PD** & **TR** & **Aircraft** & **Cotton** & **Soy** \\  \(0\). & & & & 94.60 & 67.70 & 46.00 \\
1. & ✓ & & & 95.10 & 69.60 & 47.70 \\
2. & ✓ & & & 94.94 & 68.31 & 46.70 \\
3. & ✓ & ✓ & & 95.15 & 69.70 & 50.32 \\
4. & & ✓ & ✓ & 95.40 & 70.10 & 50.99 \\  
5. & ✓ & ✓ & ✓ & **95.60** & **70.90** & **52.15** \\   

Table 3: Ablations on the key components of TRD namely Complementarity Graph (CG), Proxy Decomposition (PD), and TR (Transitivity Recovery).

drops. This happens because of a lack of prior knowledge about the proxy-graph structure, which increases room for noisy views to be more influential, harming classification robustness. To alleviate this, we introduce TR based on our findings from Theorem 3, which effectively wards off the influence of noisy views. With PD, Rows 3 and 4 respectively show the individual contributions of CG, and enforcing the Transitivity invariant between instance and proxy graphs. Row 5 shows that TRD is at its best with all components included.

## 5 Conclusion & Discussion

We presented Transitivity Recovering Decompositions (TRD), an approach for learning interpretable relationships for FGVC. We theoretically showed the existence of semantically equivalent graphs for abstract relationships, and derived their key information theoretic and topological properties. TRD is a search algorithm in the space of graphs that looks for solutions that satisfy the above properties, providing a concrete, human understandable representation of the learned relationships. Our experiments revealed that TRD not only provides end-to-end transparency, all the way up to the class-proxy representation, but also achieves state-of-the-art results on small, medium, and large scale benchmark FGVC datasets, a rare phenomenon in the interpretability literature. We also showed that our method is robust to noisy input views, both theoretically and empirically, which we conjecture to be a crucial factor behind its effectiveness.

**Limitations:** Although robust to noisy views, our method is not fully immune to spurious correlations (Figure 3, rightmost graph - the wing and the sky are spuriously correlated; additional examples in the supplementary). Combined with recent advances in learning decorrelated representations , we believe that our method can overcome this limitation while remaining fully interpretable.

**Societal Impacts:** Our method makes a positive societal impact by adding interpretability to existing fine-grained relational representations, in a provably robust manner. Although we are not aware of any specific negative societal impacts that our method might have, like most deep learning algorithms, our method is susceptible to the intrinsic biases of the training set.