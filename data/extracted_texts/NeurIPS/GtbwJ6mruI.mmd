# Skill-aware Mutual Information Optimisation for Generalisation in Reinforcement Learning

Xuehui Yu\({}^{1,2}\)   Mhairi Dunion\({}^{2}\)   Xin Li\({}^{1}\)   Stefano V. Albrecht\({}^{2}\)

\({}^{1}\)Harbin Institute of Technology  \({}^{2}\)University of Edinburgh

{yuxuehui,22s103169}@stu.hit.edu.cn

{mhairi.dunion,s.albrecht}@ed.ac.uk

###### Abstract

Meta-Reinforcement Learning (Meta-RL) agents can struggle to operate across tasks with varying environmental features that require different optimal _skills_ (i.e., different modes of behaviour). Using context encoders based on contrastive learning to enhance the generalisability of Meta-RL agents is now widely studied but faces challenges such as the requirement for a large sample size, also referred to as the \(\)-\(K\) curse. To improve RL generalisation to different tasks, we first introduce Skill-aware **M**utual **I**nformation (SaMI), an optimisation objective that aids in distinguishing context embeddings according to skills, thereby equipping RL agents with the ability to identify and execute different skills across tasks. We then propose **S**kill-**a**ware **N**oise **C**ontrastive **E**stimation (SaNCE), a \(K\)-sample estimator used to optimise the SaMI objective. We provide a framework for equipping an RL agent with SaNCE in practice and conduct experimental validation on modified MuJoCo and Panda-gym benchmarks. We empirically find that RL agents that learn by maximising SaMI achieve substantially improved zero-shot generalisation to unseen tasks. Additionally, the context encoder trained with SaNCE demonstrates greater robustness to a reduction in the number of available samples, thus possessing the potential to overcome the \(\)-\(K\) curse.

## 1 Introduction

Reinforcement Learning (RL) agents often learn policies that do not generalise across tasks in which the environmental features and optimal _skills_ are different (des Combes et al., 2018; Garcin et al., 2024). Consider a set of cube-moving tasks where an agent is required to move a cube to a goal position on a table (Figure 1). These tasks become challenging if environmental features, such as table friction, vary between tasks. When facing an unknown environment, the agent needs to explore effectively, understand the environment, and adjust its behaviour accordingly within an episode. For instance, if the agent tries to push a cube across a table covered by a tablecth and finds it "unpushable," it should infer that the table friction is relatively high and adapt by lifting

Figure 1: (a) In a cube-moving environment, tasks are defined according to different environmental features. (b) Different tasks have different transition dynamics caused by underlying environmental features, hence optimal skills are different across tasks.

the cube to avoid friction, rather than continuing to push. Recent advances in Meta-Reinforcement Learning (Meta-RL) (Lee et al., 2020; Agarwal et al., 2021; Mu et al., 2022; Dunion et al., 2023; Ma et al., 2024) enable agents to understand environmental features by inferring context embeddings from a small amount of exploration, and to train a policy conditioned on the context embedding to generalise to novel tasks.

In recent years, unsupervised contrastive learning algorithms have been shown to learn context embeddings that perform remarkably well on generalisation tasks (Clavera et al., 2019; Lee et al., 2020). In particular, some Meta-RL algorithms (Fu et al., 2021; Wang et al., 2021; Li et al., 2021; Sang et al., 2022) train context encoders by maximising InfoNCE (Oord et al., 2019), which has yielded vital insights into contrastive learning through the lens of mutual information (MI) analysis. The InfoNCE objective can be interpreted as a \(K\)-sample lower bound on the MI between trajectories and context embeddings. Despite significant advances, integrating contrastive learning with Meta-RL poses several unresolved challenges, of which two are particularly relevant to this research: **(i) Existing context encoders based on contrastive learning do not distinguish tasks that require different skills**. Many prior algorithms only pull embeddings of the same tasks together and push those of different tasks apart. However, for example, a series of cube-moving tasks with high friction may only require a Pick&Place skill (picking the cube off the table and placing it at the goal position), making further differentiation unnecessary. **(ii) Existing \(K\)-sample MI estimators are sensitive to the sample size \(K\) (i.e., the log-\(K\) curve)**(Poole et al., 2019). The low sample efficiency of RL (Franke et al., 2021) and the sample limitations in zero-shot generalisation make collecting a substantial quantity of samples often impractical (Arora et al., 2019; Nozawa and Sato, 2021). The effectiveness of \(K\)-sample MI estimators breaks down with a finite sample size and leads to a significant performance drop in downstream RL tasks (Mnih and Teh, 2012; Guo et al., 2022).

To enhance RL generalisation across different tasks, we propose that the context embeddings should optimise downstream tasks and indicate whether the current skill remains optimal or requires further exploration. This also reduces the necessary sample size by focusing solely on skill-related information. In this work, (1) we introduce _Skill-aware Mutual Information (SaMI)_, a generalised form of MI objective between context embeddings, skills, and trajectories, designed to address issue (i). We provide a theoretical proof showing that by introducing skills as a third variable into the MI of context embeddings and trajectories, the resulting SaMI is smaller and easier to optimise. Furthermore, (2) we propose a data-efficient \(K\)-sample estimator, _Skill-aware Noise Contrastive Estimation (SaNCE)_ to optimise SaMI, effectively addressing issue (ii). Additionally, (3) we propose a practical skill-aware trajectory sampling strategy that shows how to sample positive and negative examples without relying on any prior skill distribution. In that way, Meta-RL agents autonomously acquire a set of skills applicable to many tasks, with these skills emerging solely from the SaMI learning objective and data.

We demonstrate empirically in MuJoCo (Todorov et al., 2012) and Panda-gym (Gallouedec et al., 2021) that SaMI enhances the zero-shot generalisation capabilities of two Meta-RL algorithms (Yu et al., 2020; Fu et al., 2021) by achieving higher returns and success rates on previously unseen tasks, ranging from moderate to extreme difficulty. Visualisation of the learned context embeddings reveals distinct clusters corresponding to different skills, suggesting that the SaMI learning objective enables the context encoder to capture skill-related information from trajectories and incentivise Meta-RL agents to acquire a diverse set of skills. Moreover, SaNCE enables Meta-RL algorithms to use smaller sample spaces while achieving improved downstream control performance, indicating their potential to overcome the \(\)-\(K\) curse.

## 2 Related works

**Meta-RL.** By conditioning on an effective context embedding, Meta-RL policies can zero-shot generalise to new tasks with a small amount of exploration Kirk et al. (2023). Existing algorithms can be categorised into three types based on different context embeddings. In the first category, the context embedding is learned by minimising the downstream RL loss (Rakelly et al., 2019; Yu et al., 2020). PEARL (Rakelly et al., 2019) learns probabilistic context embeddings by recovering the value function. Multi-task SAC with task embeddings as inputs to policies (TESAC) (Hausman et al., 2018; Yu et al., 2020) parameterises the learned policies through a shared embedding space, aiming to maximise the average returns across tasks. However, the update signals from the RL loss are stochastic and weak, and may not capture the similarity relations among tasks (Fu et al.,2021]. The second category involves learning context embeddings through dynamics prediction [Lee et al., 2020, Zhou et al., 2019], which can make the context embeddings noisy, as they may model irrelevant dependencies and overlook task-specific information [Fu et al., 2021]. The third category employs contrastive learning [Fu et al., 2021, Wang et al., 2021, Li et al., 2021, Sang et al., 2022], achieving significant improvements in context learning. However, these methods overlook the similarity of skills between different tasks, thus failing to achieve effective zero-shot generalisation by executing different skills. Our improvements build upon this third category by distinguishing context embeddings according to different optimal skills.

**Contrastive learning.** Contrastive learning has been applied to RL due to its significant momentum in representation learning in recent years, attributed to its superior effectiveness [Tishby and Zaslavsky, 2015, Hjelm et al., 2019, Dunion and Albrecht, 2024], ease of implementation [Oord et al., 2019], and strong theoretical connection to MI estimation [Poole et al., 2019]. MI is often estimated using InfoNCE [Oord et al., 2019] that has gained recent attention due to its lower variance [Song and Ermon, 2020] and superior performance in downstream tasks. However, InfoNCE may underestimate the true MI when the sample size \(K\) is finite. To address this limitation, CCM [Fu et al., 2021] uses a large number of samples for maximising InfoNCE. DOMINO [Mu et al., 2022] reduces the true MI by introducing an independence assumption; however, this results in biased estimates. We focus on proposing an unbiased alternative MI objective and a more data-efficient \(K\)-sample estimator tailored for downstream RL tasks, which, to our knowledge, have not been addressed in previous research.

## 3 Preliminaries

**Reinforcement learning.** In Meta-RL, we assume an _environment_ is a distribution \((e)\) of _tasks_\(e\) (e.g. uniform in our experiments). Each task \(e(e)\) has a similar structure that corresponds to a Markov Decision Process (MDP) [Puterman, 2014], defined by \(_{e}=(,,R,P_{e},)\), with a state space \(\), an action space \(\), a reward function \(R(s_{t},a_{t})\) where \(s_{t}\) and \(a_{t}\), state transition dynamics \(P_{e}(s_{t+1}|s_{t},a_{t})\), and a discount factor \([0,1)\). In order to address the problem of zero-shot generalisation, we consider the transition dynamics \(P_{e}(s_{t+1}|s_{t},a_{t})\) vary across tasks \(e(e)\) according to multiple _environmental features_\(e=\{e^{0},e^{1},...,e^{N}\}\) that are not included in states \(s\) and can be continuous random variables, such as mass and friction, or discrete random variables, such as the cube's material. For instance, in a cube-moving environment (Figure 1), an agent has different tasks that are defined by different environmental features (e.g., mass and friction). The Meta-RL agent's goal is to learn a generalisable policy \(\) that is robust to such dynamic changes. Specifically, given a set of training tasks \(e\) sampled from \(_{}(e)\), we aim to learn a policy that can maximise the discounted returns, \(_{}_{e_{}(e)}[_{t=0}^{} ^{t}R(s_{t},a_{t})|a_{t}(a_{t}|s_{t}),s_{t+1} P_{e}(s_{t+1}|s _{t},a_{t})]\), and can produce accurate control for unseen test tasks sampled from \(_{}(e)\).

**Contrastive learning.** In Meta-RL, the context encoder \((c|_{c,0:t})\) first takes the trajectory \(_{c,0:t}=\{s_{0},a_{0},r_{0},...,s_{t}\}\) from the current episode as input and compresses it into a context embedding \(c\)[Fu et al., 2021]. Then, the policy \(\), conditioned on context embedding \(c\), consumes the current state \(s_{t}\) and outputs the action \(a_{t}\). As a key component, the quality of context embedding \(c\) can affect algorithms' performance significantly. MI is an effective measure of embedding quality [Goldfeld

Figure 2: A policy \(\) conditioned on a fixed context embedding \(c\) is defined as a skill \((|c)\) (shortened as \(_{c}\)). The policy \(\) conditioned on a fixed \(c\) alters the state of the environment in a consistent way, thereby exhibiting a mode of skill. The skill \((|c_{1})\) moves the cube on the table in trajectory \(_{c_{1}}^{+}\) and is referred to as the Push skill; correspondingly, the Pick&Place skill \((|c_{2})\) takes the cube off the table and places it in the goal position in the trajectory \(_{c_{2}}^{+}\).

et al., 2019), hence we focus on a context encoder that optimises the InfoNCE objective \(I_{}(x;y)\), which is a \(K\)-sample estimator and lower bound of the MI \(I(x;y)\)(Oord et al., 2019). Given a query \(x\) and a set \(Y=\{y_{1},...,y_{K}\}\) of \(K\) random samples containing one positive sample \(y_{1}\) and \(K-1\) negative samples from the distribution \(p(y)\), \(I_{}(x;y)\) is obtained by comparing pairs sampled from the joint distribution \(x,y_{1} p(x,y)\) to pairs \(x,y_{k}\) built using a set of negative examples \(y_{2:K}\):

\[I_{}(x;y|,K)=[(x,y_{1})}{ _{k=1}^{K}f_{}(x,y_{k})}].\] (1)

InfoNCE constructs a formal lower bound on the MI, i.e., \(I_{}(x;y|,K) I(x;y)\)(Guo et al., 2022; Chen et al., 2021). Given two inputs \(x\) and \(y\), their _embedding similarity_ is \(f_{}(x,y)=e^{(x)^{}(y)/}\), where \(\) is the context encoder that projects \(x\) and \(y\) into the context embedding space, the dot product is used to calculate the similarity score between \((x),(y)\) pairs (Wu et al., 2018; He et al., 2020), and \(\) is a temperature hyperparameter that controls the sensitivity of the product. Some previous Meta-RL methods (Lee et al., 2020; Mu et al., 2022) learn a context embedding \(c\) by maximising \(I_{}(c;_{c}|,K)\) between the context \(c\) embedded from a trajectory in the current task, and the historical trajectories \(_{c}\) under the same environmental features setting.

## 4 Skill-aware mutual information optimisation for Meta-RL

### The log-\(K\) curse of \(K\)-sample MI estimators

In this section, we provide a theoretical analysis of the challenge inherent in learning a \(K\)-sample estimator for MI, commonly referred to as the \(\)-\(K\) curse. Based on this theoretical analysis, we give insights to overcome this challenge. Given that we focus on the generalisation of RL, we only consider cases with a finite sample size of \(K\). If a context encoder \(\) in Equation (1) has sufficient training epochs, then \(I_{}(x;y|,K) K\)(Mnih and Teh, 2012; Guo et al., 2022). Hence, the MI we can optimise is bottlenecked by the number of available samples, formally expressed as:

**Lemma 1**: _Learning a context encoder \(\) with a \(K\)-sample estimator and finite sample size \(K\), we have \(I_{}(x;y|,K) K\!\!I(x;y)\), when \(x y\) (see proof in Appendix A)._

We do not consider the case when \(x\!\!\! y\), i.e., \( K I(x;y)=0\) (\( K 1\)), because a Meta-RL agent learns a context encoder by maximising MI between trajectories \(_{c}\) and context embeddings \(c\), which are not independent (as shown in Figure 2). While an unbounded sample size \(K\) for learning effective context embeddings is theoretically feasible and assumed in many studies Mu et al. (2022); Lee et al. (2020), it is often impractical in practice. Therefore, building on Lemma 1, we derive two key insights with limited samples: (1) generalising the current MI objective to be smaller than \(I(x;y)\) (see Section 4.2); (2) developing a \(K\)-sample estimator tighter than \(I_{}\) (see Section 4.3).

### Skill-aware mutual information: a smaller ground-truth MI

We aim for our MI learning objective to incentivise agents to acquire a diverse set of skills, enabling them to generalise effectively across tasks. To start with, we define skills (Eysenbach et al., 2018):

**Definition 1** (Skills): _A policy \(\) conditioned on a fixed context embedding \(c\) is defined as a skill \((|c)\), abbreviated as \(_{c}\). If a skill \(_{c}\) is conditioned on a state \(s_{t}\), we can sample actions \(a_{t}(|c,s_{t})\). After sampling actions from \(_{c}\) at consecutive timesteps, we obtain a trajectory \(_{c,t:t_{T}}=\{s_{t},a_{t},r_{t},s_{t+1},,s_{t+T},a_{t+T},r_{t+T}\}\) which demonstrates a consistent mode of behaviour._

After a limited amount of exploration, an agent should be able to infer the task (i.e., environmental features \(e=e^{0},e^{1},,e^{N}\)) and adapt accordingly within the current episode. The context em bedding should encompass skill-related information, guiding the policy on when to explore new skills or switch between existing ones. We propose that the context encoder \(\) should be trained by maximising the MI between the context embedding \(c\), skills \(_{c}\), and trajectories \(_{c}\). To this end, we propose a novel MI optimisation objective, **Skill-aware Mutual Information (SaMI)**, defined as:

\[I_{}(c;_{c};_{c})=I(c;_{c})-I(c;_{c}|_{c}).\] (2)

SaMI is defined according to interaction information , serving as a generalisation of MI for three variables \(\{c,_{c},_{c}\}\). Although we cannot evaluate \(p(c,_{c},_{c})\) directly, we approximate it by Monte-Carlo sampling, using \(K\) samples from \(p(c,_{c},_{c})\). As illustrated in Figure 3, a context encoder \(\) trained with the objective of maximising \(I_{}(c;_{c};_{c})\) converges more quickly, as \(I_{}(c;_{c};_{c}) I(c;_{c})\) (see proof in Appendix B). By focusing more on skill-related information, \(I_{SaMI}\) enables agents to autonomously discover a diverse range of skills for handling multiple tasks.

### Skill-aware noise contrastive estimation: a tighter \(K\)-sample estimator

Despite InfoNCE's success as a \(K\)-sample estimator for approximating MI , its learning efficiency plunges due to limited numerical precision, which is called the \(\)-\(K\) curve, i.e., \(I_{} K I_{}\) (see proof in Appendix B). When \(K+\), we can expect \(I_{} K I_{}\). However, increasing \(K\) is too expensive, especially in complex environments with enormous negative sample space. To address this, we propose a novel \(K\)-sample estimator that requires a significantly smaller sample size \(K+\). First, we define \(K^{*}\):

**Definition 2** (\(K^{*}\)): \(K^{*}=|c||_{c}| M\) _is defined as the number of trajectories in the replay buffer (i.e., the sample space), in which \(|c|\) represents the number of different context embeddings \(c\), \(|_{c}|\) represents the number of different skills \(_{c}\), and \(M\) is a natural number._

To ensure that \(I_{}\) is a tight bound of \(I_{}\), we require that \(I_{} K I_{}\) when \(K K^{*}\). Under the definition of \(K^{*}\), the replay buffer can be divided according to the different context embeddings \(c\) and skills \(_{c}\) (i.e., observing context embeddings \(c\) and skills \(_{c}\)). In real-world robotic control tasks, the sample space size significantly increases due to multiple environmental features \(e=\{e^{0},e^{1},...,e^{N}\}\). Taking the sample space of InfoNCE as an example (Figure 4(a)), in the current task \(e_{1}\) with context embedding \(c_{1}\), positive samples are trajectories \(_{c_{1}}\) generated after executing the skill \(_{c_{1}}\) in task \(e_{1}\), and negative samples are trajectories \(\{_{c_{2}},...\}\) from other tasks \(\{e_{2},...\}\). The permutations and combinations of \(N\) environmental features lead to an exponential growth in task number \(|c|\), which in turn results in an increase of sample space \(K^{*}_{}=|c||| M\).

We introduce a tight \(K\)-sample estimator, **Skill-aware Noise Contrastive Estimation (SaNCE)**, which is used to approximate \(I_{}(c;_{c};_{c})\) with \(K^{*}_{}<K^{*}_{}\). For SaNCE, both positive samples \(^{+}_{c_{1}}\) and negative samples \(^{-}_{c_{1}}\) are sampled from the current tasks \(e_{1}\), but are generated by executing positive skills \(^{+}_{c_{1}}\) and negative skills \(^{-}_{c_{1}}\), respectively. Here, a _positive skill_ is intuitively defined by whether it is optimal for the current task \(e\), with a more formal definition provided in Section 4.4. For instance, in a cube-moving task under a large friction setting, the agent executes

Figure 4: A comparison of sample spaces for task \(e_{1}\). Positive samples \(_{c_{1}}\) or \(^{+}_{c_{1}}\) are always from current task \(e_{1}\). For SaNCE, in a task \(e_{k}\) with embedding \(c_{k}\), the positive skill \(^{+}_{c_{k}}\) conditions on \(c_{k}\) and generates positive trajectories \(^{+}_{c_{k}}\), and the negative skill \(^{-}_{c_{k}}\) generates negative trajectories \(^{-}_{c_{k}}\). The top graphs show the relationship between \(c\), \(_{c}\) and \(_{c}\).

a skill \(_{c}^{+}\) after several iterations of learning, and obtains corresponding trajectories \(_{c}^{+}\) where the cubes leave the table surface. This indicates that the skill \(_{c}^{+}\) is Pick&Place and other skills \(_{c}^{-}\) may include Push or Flip (flipping the cube to the goal position), with corresponding trajectories \(_{c}^{-}\) where the cube remains stationary or rolls on the table. Formally, we can optimise the \(K\)-sample lower bound \(I_{}\) to approximate \(I_{}\):

\[I_{}(c;_{c};_{c}|,K)\] \[=_{p(c_{1},_{c_{1}},_{c_{1}}^{+})p(_{c_{1},2,K}^{-})}[((c_{1},_{c_{1}},_{c_{1}}^{ +})}{f_{}(c_{1},_{c_{1}},_{c_{1}}^{+})+_{k=2}^{K}f_{}(c_{1}, _{c_{1}},_{c_{1}}^{-})})]\] \[ I_{}(c;_{c};_{c})\] (3)

where \(f_{}(c_{1},_{c_{1}},_{c_{1}})=e^{(_{c_{1}})^{} ^{*}(_{c_{1}})/}\). The query \(c_{1}=(_{c_{1}})\) is generated by the context encoder \(\). For training stability, we use a momentum encoder \(^{*}\) to produce the positive and negative embeddings. SNCE significantly reduces the required sample space size \(K^{*}_{}\) by sampling trajectories \(_{c}\) based on different skills \(_{c}\) (Figure 4(b)) in task \(e_{1}\), so that \(K^{*}_{}=|c||_{c}| M=|_{c_{1}}| M K^{*}_{ }\) (\(|c|=|c_{1}|=1\)). Therefore, \(I_{}\) satisfies Lemma 2:

**Lemma 2**: _With a context encoder \(\) and finite sample size \(K\), we have \(I_{}(c;_{c};_{c}|,K) I_{}(c;_{c}; _{c}|,K) K I_{}(c;_{c};_{c}) I(c; _{c})\). (see proof in Appendix B)_

SaNCE can be used alone or combined with other optimisation objectives to train context encoders in Meta-RL algorithms. For instance, integrating SaNCE with InfoNCE diversifies the negative sample space, with \(K^{*}_{}=(_{i=1}^{|c|}|_{c_{i}}^{-}|+|_{c_{1}}^ {+}|) M\). The sample space for \(I_{}\) is depicted in Figure 4(c) and further analysed in detail in Appendix C.

### Skill-aware trajectory sampling strategy

In this section, we propose a practical trajectory sampling method. Methods focusing on skill diversity often rely heavily on accurately defining and identifying individual skills (Eysenbach et al., 2018). Some of these methods require a prior skill distribution, which is often inaccessible (Shi et al., 2022), and it is impractical to enumerate all possible skills that we hope the model to learn. Besides, we believe that distinctiveness of skills is inherently difficult to achieve -- a slight difference in states can make two skills distinguishable, and not necessarily in a semantically meaningful way. Consequently, we do not directly teach any of these skills or assume any prior skill distribution. Diverse skills naturally emerge from the incentives of the SaMI learning objective in a multi-task setting, driven by the inherent need to develop generalisable skills. For example, in high-friction tasks, the agent must acquire the Pick&Place skill to avoid large frictional forces, whereas in high-mass tasks, the agent must learn the Push skill since it cannot lift the cube. In each task, we only identify whether the skills

Figure 5: A practical framework for using SaNCE in the meta-training phase. During meta-training, we sample trajectories from the replay buffer for off-policy training. Queries are generated by a context encoder \(\), which is updated with gradients from both the SaNCE loss \(_{}\) and the RL loss \(_{RL}\). Negative/Positive embeddings are encoded by a momentum context encoder \(^{*}\), which is driven by a momentum update with the encoder \(\). During meta-testing, the meta-trained context encoder \(\) embeds the current trajectory, and the RL policy takes the embedding as input together with the state for adaptation within an episode.

are optimal; in this way, under a multi-task setting, the agent will acquire a set of general skills that are applicable to many tasks.

In a given task \(e\), _positive skills_\(_{c}^{+}\) are defined as optimal skills achieving highest return \(_{i=t}^{t+T}R(s_{i},a_{i})\), whereas _negative skills_\(_{c}^{-}\) are those that result in lower returns. As a result, the positive sample \(_{c}^{+}\) consists of trajectories generated by the skills with the highest ranked returns, while the negative samples correspond to those with the lowest returns. This straightforward approach of selecting positive samples based on the ranked highest return effectively aligns with positive skills and mitigates the challenge of hard negative examples (Robinson et al., 2021). The SaNCE loss is then minimised to bring the context embeddings of the highest return trajectories closer while distancing those of negative trajectories. By the end of training, the top-ranked trajectories in the ranked replay buffer correspond to positive samples \(_{c}^{+}\) with high returns, while the lower-ranked trajectories represent negative samples \(_{c}^{-}\) with low returns. However, at the beginning of training, it is likely that all trajectories have low returns. Therefore, our SaNCE loss is a soft variant of the \(K\)-sample SaNCE:

\[_{}=-||(_{c}^{+}),(_{c}^{-}) ||_{L2},1 I_{}\] (4)

where \(||||_{L2}\) represents the Euclidean distance (Tabak, 2014). Figure 5 provides a practical framework of SaNCE, with a cube-moving example task \(e_{1}\) under high friction. In task \(e_{1}\), the positive skill \(_{c_{1}}^{+}\) is the Pick&Place skill, which is used to generate queries \((_{c_{1}}^{+})\) and positive embeddings \(^{*}(_{c_{1}}^{+})\); after executing Push skill we get negative samples \(_{c_{1}}^{-}\) and negative embeddings \(^{*}(_{c_{1}}^{-})\).

## 5 Experiments

Our experiments aim to answer the following questions: (1) Does optimising SaMI lead to increased returns during training and zero-shot generalisation (see Table 1 and 2)?; (2) Does SaMI help the RL agents to be versatile and embody multiple skills (see Figure 6)?; (3) Can SaNCE overcome the \(\)-\(K\) curse in sample-limited scenarios (see Table 1 and 2, and Section 5.4)?

### Experimental setup

**Modified benchmarks with multiple environmental features.1** We evaluate our method on two benchmarks, Panda-gym (Gallouedec et al., 2021) and MuJoCo (Todorov et al., 2012) (details in Sections 5.2 and 5.3). The benchmarks are modified to be influenced by multiple environmental features, which are sampled at the start of each episode during meta-training and meta-testing. During meta-training, we uniform-randomly select a combination of environmental features from a training task set. At test time, we evaluate each algorithm in unseen tasks with environmental features outside the training range. Generalisation performance is measured in two different regimes: moderate and extreme. The moderate regime draws environmental features from a closer range to the training range compared to the extreme. Our results report the mean and standard deviation of models trained over five seeds in both training and test tasks. Further details are available in Appendix D.

Figure 6: (a) UMAP visualisation of context embeddings for the SaCCM in the Panda-gym environment, with points in the yellow box representing the Push skill in high-mass tasks. Heatmap of (b) success rate, (c) Push skill probability, and (d) Pick&Place skill probability for SaCCM. In large-mass scenarios, the Push skill is more likely to be executed than Pick&Place.

**Baselines.** The loss function of the context encoder in all algorithms consists of two key components: the RL loss and the contrastive loss. The RL loss \(_{RL}\), which is the same across all methods, corresponds to the RL value function loss. Our primary comparison focuses on the contrastive loss, taking the form of either SaNCE, InfoNCE, or no contrastive loss. Accordingly, we select baselines based on contrastive loss: **CCM**[Fu et al., 2021], which utilises InfoNCE, and **TESAC**[Yu et al., 2020], which relies solely on the RL loss, allowing assessment of the context encoder without contrastive loss. Since CCM and TESAC use an RNN encoder, we also include **PEARL**[Rakelly et al., 2019], which utilises an MLP context encoder and follows the similar RL loss. Additionally, Appendix G includes comparisons with DOMINO [Mu et al., 2022] and CaDM [Lee et al., 2020], using the same environmental setup in the MuJoCo benchmark.

**Our method.2** We use Soft Actor-Critic (SAC) [Haarnoja et al., 2018] as the base RL algorithm, training agents for 1.6 million timesteps in each environment (details in Appendix D.3). SaNCE is a simple objective based on MI that can be used to train any context encoder. We integrate SaNCE into two Meta-RL algorithms: (1) **SaTESAC** is TESAC with SaNCE, which uses SaNCE for contrastive learning, using a \(|c|\) times smaller sample space (Figure 4(b)); (2) **SaCCM** is CCM with SaNCE, where the contrastive learning combines InfoNCE and SaNCE, as shown in Figure 4(c).

### Panda-gym

**Task description.** Our modified Pandagym benchmark involves a robot arm control task using the Franka Emika Panda [Gallouedec et al., 2021], where the robot moves a cube to a target position. Unlike previous works, we simultaneously modify multiple environmental features (cube mass and table friction) that characterise the transition dynamics, and the robot can flexibly execute different skills (Push and Pick&Place) for different tasks. This environment requires high skill diversity; for instance, the agent must use Pick&Place in high-friction tasks and Push in high-mass tasks.

**Results and skill analysis.** As shown in Table 1, SaTESAC and SaCCM achieve superior generalisation performance compared to PEARL, TESAC, and CCM, with a smaller sample space. The t-test results in Table 1 show that SaMI significantly improves success rates across training, moderate, and extreme test sets at a 0.05 significance level. Video demos2 show that agents equipped with SaMI acquired multiple skills (Push, Pick&Place) to handle various tasks. When faced with an unknown task, the agents explore by attempting to lift the cube, infer the context, and adjust their skills accordingly within the episode. We visualised the context embeddings using UMAP [McInnes et al., 2020] (Figure 6(a) and Appendix F) and t-SNE [Van der Maaten and Hinton, 2008] (Appendix F), plotting the final step from 100 tests per task. Skills were identified through contact points between the end effector, cube, and table (see Appendix D for more details), and heatmaps [Waskom, 2021] were used to visualise executed skills. Figure 6 shows that SaCCM agents learned the Push skill for large cube masses (30 Kg, 10 Kg) and Pick&Place for smaller masses, while CCM showed no clear skill grouping (Figure 16 in Appendix F.1). Overall, SaMI incentivises agents to autonomously learn diverse skills, enhancing generalisation across a wider range of tasks. Specifically, through the cycle of effective exploration, context inference, and adaptation, diverse skills emerge solely from the data. Further visualisation results are in Appendix F.

### MuJoCo

**Task description.** We extended the modified MuJoCo benchmark introduced in DOMINO [Mu et al., 2022] and CaDM [Lee et al., 2020]. It contains ten typical robotic control environments based on the MuJoCo physics engine [Todorov et al., 2012]. Hopper, Walker, Half-cheetah, Ant, HumanoidStandup, and SlimHumanoid are influenced by continuous environmental features (i.e., mass, damping) that affect transition dynamics. Crippled Ant, Crippled Hopper, Crippled Walker, and

    & Training & Test (moderate) & Test (extreme) \\  PEARL & 0.42\(\)0.19 & 0.10\(\)0.06 & 0.11\(\)0.05 \\ TESAC & 0.50\(\)0.22 & 0.31\(\)0.20 & 0.22\(\)0.21 \\ CCM & 0.80\(\)0.19 & 0.49\(\)0.23 & 0.29\(\)0.28 \\  SaTESAC & 0.92\(\)0.04\({}^{*}\) & 0.56\(\)0.24\({}^{*}\) & **0.37\(\)0.34\({}^{*}\)** \\ SaCCM & **0.93\(\)0.05\({}^{*}\)** & **0.57\(\)0.26\({}^{*}\)** & 0.36\(\)0.35\({}^{*}\) \\   

Table 1: Comparison of success rate \(\) standard deviation with baselines in Panda-gym (over 5 seeds). **Bold text** signifies the highest average return. \(*\) next to the number means that the algorithm with SaMI has statistically significant improvement over the same algorithm without SaMI. All significance claims based on paired t-tests with significance threshold of \(p<0.05\).

Crippled Half-cheetah are more challenging due to the addition of discrete environmental features (i.e., randomly crippled leg joints), requiring agents to master different skills (e.g., switching from running to crawling after a leg is crippled).

**Results and skill analysis.** Table 2 shows the average return of our method and baselines on training and test tasks. SaTESAC and SaCCM achieved higher returns in most tasks, except for Ant, Half-Cheetah, and Hopper, where only a single skill was needed. For instance, the Hopper robot learned to hop forward, adapting to different mass values. When environments become complex and require diverse skills for different tasks (Crippled Ant, Crippled Hopper, Crippled Half-Cheetah, SlimHumanoid, HumanoidStandup, and Crippled Walker), SaNCE brings significant improvements. For example, when the Ant robot has 3 or 4 legs available, it learns to roll to generalise across varying mass and damping. In more challenging zero-shot settings, when only 2 legs are available, the ant robot can no longer roll and it adapts by walking using its 2 healthy legs. This aligns with the results in Table 2, where SaMI significantly improved performance in extreme test sets. In summary, i) SaMI helps the RL agents to be versatile and embody multiple skills; ii) SaMI leads to increased returns

    &  &  \\   & Training & Test (moderate) & Test (extreme) & Training & Test (moderate) & Test (extreme) \\  PEARL & 1682\(\)73 & 996\(\)21 & 888\(\)31 & 1998\(\)973 & 698\(\)548 & 746\(\)1092 \\ TESAC & 2139\(\)90 & 1952\(\)40 & 1048\(\)124 & 3967\(\)955 & 874\(\)901 & 846\(\)849 \\ CCM & 2361\(\)114 & 2047\(\)83 & 1527\(\)301 & 3481\(\)488 & 821\(\)575 & 873\(\)914 \\  SaTESAC & **2638\(\)406** & **2379\(\)528** & **2131\(\)132\({}^{*}\)** & 4328\(\)1092 & **1143\(\)664\({}^{*}\)** & **1540\(\)1094** \\ SaCCM & 2355\(\)170 & 2310\(\)314 & 2007\(\)68\({}^{*}\) & **4478\(\)1131\({}^{*}\)** & 1007\(\)586 & 1027\(\)782 \\    &  &  \\   & Training & Test (moderate) & Test (extreme) & Training & Test (moderate) & Test (extreme) \\  PEARL & 5153\(\)581 & 3873\(\)235 & 3802\(\)409 & 5802\(\)773 & 2190\(\)970 & 1346\(\)692 \\ TESAC & 6789\(\)451 & 4705\(\)279 & 4108\(\)369 & 6298\(\)2310 & 3173\(\)1210 & 1159\(\)338 \\ CCM & 6901\(\)567 & 5179\(\)902 & 4700\(\)696 & 6955\(\)788 & 3963\(\)622 & 1325\(\)269 \\  SaTESAC & 7314\(\)545 & 5513\(\)648\({}^{*}\) & 4940\(\)531\({}^{*}\) & **7430\(\)1026** & **4058\(\)890** & 1780\(\)102\({}^{*}\) \\ SaCCM & **7478\(\)539** & **5717\(\)488** & **5215\(\)377** & 7154\(\)965 & 3849\(\)689 & **1926\(\)218\({}^{*}\)** \\    &  &  \\   & Training & Test (moderate) & Test (extreme) & Training & Test (moderate) & Test (extreme) \\  PEARL & 6947\(\)3541 & 3697\(\)2674 & 2018\(\)907 & 95456\(\)13445 & 63242\(\)13546 & 64224\(\)15467 \\ TESAC & 8437\(\)1798 & 6989\(\)1301 & 3760\(\)308 & 158384\(\)14455 & 153944\(\)15046 & 74220\(\)19980 \\ CCM & 7696\(\)1907 & 5784\(\)531 & 2887\(\)1058 & 146480\(\)33745 & 154601\(\)16291 & 94991\(\)15258 \\  SaTESAC & **10216\(\)1620** & **7886\(\)2203** & 6123\(\)1403\({}^{*}\) & 178142\(\)10081\({}^{*}\) & 168337\(\)12123 & 133335\(\)24607\({}^{*}\) \\ SaCCM & 9312\(\)705 & 7430\(\)1587 & **6473\(\)2001\({}^{*}\)** & **187930\(\)19338\({}^{*}\)** & **181033\(\)14628** & **141750\(\)27426** \\    &  &  \\   & Training & Test (moderate) & Test (extreme) & Training & Test (moderate) & Test (extreme) \\  PEARL & 934\(\)242 & 874\(\)366 & 799\(\)298 & 3091\(\)298 & 2387\(\)656 & 456\(\)235 \\ TESAC & 1492\(\)59 & **1499\(\)35** & **1459\(\)72** & **3575\(\)192** & 3298\(\)551 & 722\(\)161 \\ CCM & 1484\(\)54 & 1446\(\)64 & 1452\(\)58 & 3455\(\)301 & **3409\(\)239** & 1009\(\)289 \\  SaTESAC & **1502\(\)20** & 1453\(\)39 & 1447\(\)14 & 3391\(\)84 & 3262\(\)166 & 1839\(\)130 \\ SaCCM & 1462\(\)45 & 1462\(\)14 & 1451\(\)67 & 3449\(\)103 & 3390\(\)211 & **2059\(\)221\({}^{*}\)** \\ 

[MISSING_PAGE_EMPTY:10]