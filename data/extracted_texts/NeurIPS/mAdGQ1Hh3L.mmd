# START: A Generalized State Space Model with Saliency-Driven Token-Aware Transformation

Jintao Guo Lei Qi Yinghuan Shi Yang Gao1 Nanjing University \({}^{2}\) Southeast University guojintao@smail.nju.edu.cn, qilei@seu.edu.cn, {syh, gaoy}@nju.edu.cn

###### Abstract

Domain Generalization (DG) aims to enable models to generalize to unseen target domains by learning from multiple source domains. Existing DG methods primarily rely on convolutional neural networks (CNNs), which inherently learn texture biases due to their limited receptive fields, making them prone to overfitting source domains. While some works have introduced transformer-based methods (ViTs) for DG to leverage the global receptive field, these methods incur high computational costs due to the quadratic complexity of self-attention. Recently, advanced state space models (SSMs), represented by Mamba, have shown promising results in supervised learning tasks by achieving linear complexity in sequence length during training and fast RNN-like computation during inference. Inspired by this, we investigate the generalization ability of the Mamba model under domain shifts and find that input-dependent matrices within SSMs could accumulate and amplify domain-specific features, thus hindering model generalization. To address this issue, we propose a novel SSM-based architecture with saliency-based token-aware transformation (namely START), which achieves state-of-the-art (SOTA) performances and offers a competitive alternative to CNNs and ViTs. Our START can selectively perturb and suppress domain-specific features in salient tokens within the input-dependent matrices of SSMs, thus effectively reducing the discrepancy between different domains. Extensive experiments on five benchmarks demonstrate that START outperforms existing SOTA DG methods with efficient linear complexity. Our code is available at https://github.com/lingeringlight/START.

## 1 Introduction

Deep learning models have achieved impressive progress in various computer vision tasks over the past years . Such a huge success is mostly based on the _independent and identically distributed_ (_i.i.d._) assumption, _i.e._, the training and testing data follow the same distribution . However, when evaluated on test data following different distributions from the training data, these models often suffer severe performance degradation. This issue, which is known as domain shift , has greatly hindered the applications of deep learning models in the real world.

To improve the generalization of the model under domain shifts, Domain Adaptation (DA) has been widely studied, which aims to transfer the knowledge learned from labeled source domains to the unlabeled or partially labeled target domain . However, DA methods cannot guarantee the performance of the model on unknown target domains that have not been observed during training . Since the accessibility of the target domain could not always be satisfied in real scenarios,Domain Generalization (DG) is proposed to develop a domain-generalizable model on unseen target domains by learning multiple different but related source domains [8; 9].

Most existing DG methods focus on learning domain-invariant representations across source domains, primarily via domain alignment [10; 11], meta-learning [7; 12], and data augmentation [13; 14]. These methods heavily rely on convolutional neural networks (CNNs), which have limited receptive fields due to local convolutions. Consequently, the CNN-based methods inevitably tend to learn local texture information, leading to overfitting to source domains and poor generalization on target domains[15; 16]. Recent works in DG have introduced Vision Transformers (ViTs) as the backbone for DG, utilizing the global receptive field of the self-attention mechanism to mitigate local texture bias [17; 18; 19]. However, the complexity of self-attention increases quadratically with input length, resulting in significant computational overhead for ViTs when modeling long sequences [20; 21].

To address this issue, some pioneers have proposed advanced state space models (SSMs) [20; 22; 23], represented by Mamba , which selectively models token dependencies in input sequences in a compressed state space. The selective scan mechanism allows Mamba to achieve linear complexity in sequence length during training and fast RNN-like computation during inference. Despite the remarkable performance of Mamba-based methods on supervised learning tasks, few existing works have analyzed the generalization ability of Mamba under domain shift. It remains an open question whether the Mamba model can achieve excellent performance for DG tasks.

In this paper, we theoretically analyze the generalization error bound of the Mamba model under domain shifts. We find that the domain distance of features extracted by the model is strongly related to the input-dependent matrices within the model. _These matrices accumulate and amplify domain-specific features during training, which exacerbates the overfitting issue of the model to source domains._

We empirically measure the distance among source domains within the input sequence \(x\), the response sequence \(y\), and the input-dependent matrices of the last network layer. As shown in Fig. 1, we observe that for the baseline model, input-dependent matrices (\(\), \(B\), and \(C\)) are prone to learning domain-specific features from the input \(x\). Since the output \(y\) is calculated by the recurrent product of \(x\) and these matrices, domain-specific features are accumulated and amplified, causing the model to overfit the source domains. To address this issue, we propose a Generalized State Space Model with Saliency-driven Token-Aware Transformation (START), which can reduce domain-specific information in input-dependent matrices during training. Building on the latest Mamba-based model , we develop a strong baseline for DG that outperforms many SOTA DG methods, which selectively learns global dependencies among tokens with linear complexity in sequence length.

Figure 1: **Analysis of the input-dependent matrices in SSMs.** We investigate domain discrepancy in the input sequence \(x\), response sequence \(y\), and the input-dependent matrices \(\), \(B\), and \(C\). The results indicate that _the input-dependent matrices can accumulate the domain-specific features during the recurrent process, potentially increasing domain gap_. We experiment on PACS  with Sketch as the target domain, analyzing the representations from the last block of VMamba backbone .

Moreover, based on theoretical analysis, we design the saliency-driven token-aware transformation method, which simulates domain shifts during training by selectively introducing style perturbations to tokens focused on by the input-dependent matrices. START constrain these matrices to learn domain-invariant features, thus mitigating the overfitting of model on source domains. Experiments on five datasets prove the effectiveness of our method. Our contributions are summarized as follows:

* We conduct a theoretical investigation into the generalization ability of the Mamba model, revealing that the input-dependent matrices in Mamba can accumulate domain-specific features during the recurrent process, thus hindering the model's generalizability.
* Based on theoretical analysis, we propose a novel SSM-based architecture with saliency-driven token-aware transformation as a competitive alternative to CNNs and ViTs for DG, which performs excellent generalization ability with efficient linear complexity.
* For the saliency-driven token-aware transformation, we explore two variants to identify and perturb salient tokens in feature sequences, effectively reducing domain-specific information within the input-dependent matrices of Mamba. Our method achieves SOTA performances, _e.g._, yielding the best CNN-based method by \(5.87\%\) (\(58.27\%\) vs. \(52.40\%\)) on TerraIncognita.

## 2 Related Works

**Domain generalization.** Traditional DG methods, primarily based on CNN backbones, can be broadly classified into three categories: domain alignment, meta-learning, and data augmentation. Motivated by the learning theory of domain adaptation [4; 25], domain alignment methods seek to learn domain-invariant representations through adversarial learning [26; 10; 27], causal learning [28; 29], or feature disentanglement [30; 31]. Another popular way to address DG is meta-learning, which partitions the training data from multiple source domains into meta-train and meta-test sets to simulate domain shifts during training [7; 12; 32]. Data augmentation is also an effective method to enhance model robustness to domain shifts by generating diverse data invariants through adversarial generation [33; 34], style perturbation [13; 14], and learnable parameters [35; 36]. However, CNN-based DG methods suffer from the limited receptive field of convolutions, often leading to a texture bias and overfitting to source domains [15; 37]. To address this, some researchers have introduced ViT-based methods for DG, which capture global representations by leveraging long-range spatial dependencies with attention mechanisms [18; 17; 19]. Despite their advantages, ViT-based methods are computationally intensive due to the quadratic complexity of the self-attention mechanism, limiting their practical applications [20; 21]. Inspired by the emerging Mamba model [20; 22; 23], we explore a novel SSM-based architecture for DG that combines strong generalizability with efficient linear complexity. We theoretically analyze the generalization error bound of Mamba and design a novel saliency-driven token-aware transformation to suppress domain-specific features in the input-dependent matrices of Mamba, thereby enhancing the generalization ability of the model.

**State space models.** Recently, state space models (SSMs) have demonstrated promising performance across various vision tasks [38; 39; 40] for their ability to effectively capture long-range dependencies while maintaining linear scalability with sequence length. Derived from the classical state space model , the Structured State Space Sequence Model (S4)  addresses computational constraints through novel parameterizations catering to continuous-time, recurrent, and convolutional views of the state space model. Notably, Mamba  has emerged as a standout performer, which integrates selection mechanism and hardware-aware algorithms into previous works [43; 44; 45], thus achieving linear-time inference and efficient training mechanisms. Based on the success of Mamba, Vision Mamba (Vim)  applies Mamba to ViT architecture, combining bidirectional SSM for data-dependent global visual context modeling. Meanwhile, VMamba  designs a cross-scan module to bridge the gap between \(1\)D array scanning and \(2\)D plain traversing. Mamba-based architectures have exhibited superior performance across various supervised vision tasks, including medical image segmentation [46; 47; 48], point cloud analysis [49; 50; 51], and remote sensing analysis [52; 53]. However, few works explore the performance of Mamba under domain shifts for DG. Although DGMamba  has recently introduced a pioneering Mamba-based framework for DG, it lacks a deep analysis of the generalizability of Mamba. In the paper, we conduct a theoretical analysis of Mamba's generalizability, revealing that input-dependent matrices within Mamba could accumulate domain-specific information, thereby impeding model generalization. Consequently, we propose a generalized SSM-based architecture for DG, incorporating a non-parametric module to selectively perturb salient tokens within input-dependent matrices, thus enhancing model generalization to unseen domains.

## 3 Method

### Preliminary

**State Space Models (SSMs).** The SSM is a type of linear time-invariant systems that map input sequence \(x_{t}^{L}\) to response sequence \(y_{t}^{L}\) through a hidden state \(h_{t}^{N}\). Mathematically, this process is formulated as the subsequent linear ordinary differential equations (ODEs): \(h_{t}^{}=Ah_{t}+Bx_{t},y_{t}=Ch_{t}\), where \(A^{N N}\) is the evolution parameter, and \(B^{N 1}\), \(C^{N 1}\) are the projection parameters. However, the differential equation is hard to solve in the deep learning setting, thus discrete SSM [55; 44] suggests discretizing the system with a time scale parameter \(\):

\[&=e^{ A},=(  A)^{-1}(e^{ A}-I) B,\\ h_{t}&=h_{t-1}+x_{t}, y_{t}=Ch_ {t},\] (1)

where \(\) and \(\) are discrete counterparts of the continuous parameters \(A\) and \(B\), and \(>0\) is the sampling timescale for the discretization process. Although the discrete SSMs can achieve linear time complexity, they rely on static parameterization, _i.e._, \(\), \(\), and \(C\) are time-invariant for any input, inherently limiting their ability to capture sequence context . To address this issue, recently,  proposes Mamboa, a selective SSM (S6) that effectively selects relevant context by enabling dependence of the parameters \(B^{L N}\), \(C^{L N}\), and \(^{L D}\) on the input \(x_{t}^{L D}\):

\[B=S_{B}(x_{t}), C=S_{C}(x_{t}),=(S_{}(x _{t})).\] (2)

\(S_{B}\), \(S_{C}\), \(S_{}\) are linear projection layers and \(()=(1+())\). The input-dependent time-variant layers could enhance recurrent layers, making them more expressive and flexible in capturing complex dependencies . The parameter matrixes can be further expressed as \(=[_{1},,_{L}]\), \(=[_{1},,_{L}]\), \(C=[C_{1},,C_{L}]\), where \(L\) is the sequence length. Considering the initial state \(h_{0}=0\), Eq. (1) can be unrolled as :

\[y= x,[y_{1}\\ y_{2}\\ \\ y_{L}]=[C_{1}_{1}&0&&0 \\ C_{2}_{2}_{2}&C_{2}_{2}&&0\\ &&&0\\ C_{L}_{k=2}^{L}_{k}_{L}&C_{L}_{k=3}^{L}_{k}_{L}&&C_{L}_{L}][x_{1}\\ x_{2}\\ \\ x_{L}],\] (3)

where \(_{i,j}=C_{i}_{k=j+1}^{i}_{k}_{j}\), for \(0 j<i L\), characterizing S6 layer as a data-dependent self-attention . The attention matrix \(\) is determined by both the input and the parameter matrices.

### Theoretically Analysis for the Generalization Ability of Mamboa

Previous DG methods have primarily focused on enhancing the generalizability of CNNs or ViTs, lacking theoretical investigations into the Mamboa model. We theoretically explore the generalization error bound of Mamboa, proving that _perturbing the domain-specific features within the input-dependent matrices of Mamboa can effectively diminish the upper bound of the model's generalization risk_.

**Notations.** Given a training set of \(N\) source domains \(_{S}=\{D_{S}^{1},D_{S}^{2},,D_{S}^{N}\}\), the objective of DG is to use \(_{S}\) to train a model that is expected to perform well on unseen target domain \(D_{T}\). Let \(h:\) be a hypothesis from the candidate hypothesis space \(\), where \(\) and \(\) denote the input space and the label space, respectively. Since Mamboa learns dependencies among tokens from continuous sequences, we study its generalizability at the token level. Let \(()\) be the feature extractor of \(h\) that maps input images into feature space. Following Integral Probability Metrics [57; 58], we define the token-level Maximum Mean Discrepancy to estimate the gap between different domains.

**Definition 1** (Token-level Maximum Mean Discrepancy).: _Given two different distributions of \(D_{S}\) and \(D_{T}\), let \(L\) denote the number of tokens in the response sequence of \(()\), then we define the Token-level Maximum Mean Discrepancy (To-MMD) between \((D_{S})\) and \((D_{T})\) as:_

\[d_{}(D_{S},D_{T})=_{t=1}^{L}_{_{t}_{ t}}_{||f||_{_{k}} 1}| fd(_{t}(D_{S})-_{t}(D_{T})) |,\] (4)

_where \(\) represents the hypothesis space for each token, \(_{t}(D)\) denotes the distribution of the \(t\)-th token for domain \(D\), and \(_{k}\) is a RKHS with its associated kernel \(k\)._We here investigate the generalization risk bound of the Mamba model. Theoretically, as in [59; 60], the risk of the hypothesis \(h\) on the domain \(D\) is defined as: \(R_{D}(h)=_{x D}[(h(x)-h^{*}(x))]\), where \(:_{+}\) is a convex loss-function that measures the distance between \(h\) and the true labeling function \(h^{*}\). Moreover, following [8; 60], for multiple source domains \(_{S}=\{D_{S}^{1},D_{S}^{2},...,D_{S}^{N}\}\), the convex hull \(_{S}\) is defined as a set of a mixture of source domains, _i.e._, \(_{S}=\{}:}()=_{n=1}^{N}_{i} D_{n}^{n}(),_{n=1}^{N}_{n}=1,_{n}\}\). The \(_{T}_{S}\) is defined as the closest domain to the target domain \(D_{T}\). Based on Eq. (4), the following generalization risk bound can be derived.

**Theorem 1** (Generalization Risk Bound).: _With the previous setting and assumptions, let \(D_{S}^{i}\) and \(D_{T}\) be two sets with \(M\) samples independently drawn from \(_{S}^{n}\) and \(_{T}\), respectively. For any \((0,1)\) with probablity of at least \(1-\), for all \(h\), the following inequality holds:_

\[R_{D_{T}}(h)_{n=1}^{N}_{n}R_{D_{S}}^{n}(h)+d_{}(D_{T}, _{T})+_{i,j[N]}d_{}(D_{S}^{i},D_{S}^{j})+2_{ }+,\] (5)

_where \(_{}=(_{n=1}^{N}_{n}_{x D_{S}^{n}}[ ^{n}})}+_{x D_{T}}[})}])+ }\), and \(\) is the minimum combined error of the ideal hypothesis \(h^{*}\) on both \(D_{S}\) and \(D_{T}\). Let \(_{T}=d_{}(D_{T},_{T})\) and \(_{S}=_{i,j[N]}d_{}(D_{S}^{i},D_{S}^{j})\), respectively._

The proof of Theorem 1 is provided in Appendix A.1. The inequality indicates that the generalization error bound depends on \(_{T}\) denoting the token-level maximum distance between source and target domains, and \(_{S}\) measuring the maximum pairwise gap among source domains at the token level. The smaller the two terms, the lower the upper bound of generalization error. Following [25; 58; 61], we simplify the To-MMD in Eq. (4) by choosing a unit ball in the \(_{k}\) and using Gaussian kernel with parameter \(\) to estimate \(_{T}\) and \(_{S}\). Let \(^{S}^{L}\) and \(^{T}^{L}\) to denote the mean embeddings of samples from \(D_{S}\) and \(D_{T}\), where \(L\) represents the token sequence length. We explore a simplified problem in conjunction with a single \(6\) layer, _i.e._, \(=\), with \(^{L L}\) being the data-dependent matric. The domain distance between \(^{S}\) and \(^{T}\) is formulated as \(k(^{S},^{T})=(-||^{S}-^{T}||^{2}/)\), where \(\) is the kernel parameter. Specifically, for input-dependent matrices \(B,C,\), we denote \((S_{}())\) as \(_{}()\). Then, we analyze the impact of these input-dependent matrices on \(||^{S}-^{T}||^{2}\), which is applicable to both \(_{T}\) and \(_{S}\). For the \(i\)-th tokens \(_{i}^{S}\) and \(_{i}^{T}\), we define:

\[& d_{CBx}(_{i}^{S},_{i}^{T })=S_{C}(_{i}^{S})_{}(_{i}^{S})S_{B}(_{i}^ {S})_{i}^{S}-S_{C}(_{i}^{T})_{}(_{i}^{T})S _{B}(_{i}^{T})_{i}^{T},\\ & d_{}(_{i}^{S},_{i}^{T})=_{ }(_{i}^{S})-_{}(_{i}^{T}).\] (6)

With the recurrent property of the \(6\) layer in Eq. (3), we can derive the following propositions:

**Proposition 1** (Accumulation of Domain Discrepancy).: _Given two distinct domains \(D_{S}\) and \(D_{T}\), the token-level domain distance \(d_{}(D_{S},D_{T})\) depends on \(d_{CBx}(_{i}^{S},_{i}^{T})\) and \(d_{}(_{i}^{S},_{i}^{T})\) for the \(i\)-th token. For the entire recurrent process, domain-specific information encoded in \(S_{}\), \(S_{C}\), and \(S_{B}\) will accumulate, thereby amplifying domain discrepancy._

**Proposition 2** (Mitigating Domain Discrepancy Accumulation).: _Perturbing domain-specific features in tokens focused on by \(S_{}\), \(S_{C}\), and \(S_{B}\) can enhance their learning of domain-invariant features, thus effectively mitigating the accumulation issue in these input-dependent matrices._

Propositions \(1\) and \(2\) are proved in Appendix A.1. Based on the propositions, we develop a saliency-driven token augmentation method, which perturbs style information within the tokens that the model focuses on at the sequence level. In this way, our method enhances the extraction of domain-invariant features by the input-dependent matrices, _i.e._, \(S_{}\), \(S_{C}\), and \(S_{B}\). As presented in Tab. 6, we also empirically validate the effectiveness of our method in reducing domain discrepancy in these matrices.

### Saliency-driven Token-Aware Transformation for Mamba

To boost the generalization ability of the Mamba model, leveraging the **Proposion 1** and **Proposion 2**, we propose a novel _Saliency-driven Token-AwaRe Transformation paradigm_ (START in short), which aims to explicitly suppress domain-related features within the input-dependent matrixes. Unlike prior methods that perturb entire feature maps at the channel level [13; 14; 15], START incorporates a saliency-driven token selection scheme to perturb the prominent regions of input-dependent matrices \(S_{}\), \(S_{B}\), and \(S_{C}\). Based on the attention mechanism outlined in Eq. (3), we propose two variants to identify and perturb tokens within salient regions, including START-M that determines saliency using input-dependent matrices, and START-X computing saliency based on input sequences.

**START based on input-dependent matrices (START-M).** As **Proposition 1** reveals, for the \(i\)-th token, the token-level domain gap depends on \(d_{CBx}(_{i}^{s},_{i}^{T})\) and \(d_{}(_{i}^{S},_{i}^{T})\). Specifically, as presented in Eq. (6), \(d_{CBx}\) is contingent on \(S_{C}(x_{i})S_{}(x_{i})S_{B}(x_{i})x_{i}\), which is the response of the SSM to \(x_{i}\). \(S_{C}(x_{i})S_{}(x_{i})S_{B}(x_{i})\) could be regarded as a self-attention matrix, which implicitly offers a measure of saliency for a token \(x_{i}\). To this end, we propose START-M, which utilizes the input-dependent matrices to identify salient tokens. Concretely, given an input sequence \(\{x_{i}\}_{i=1}^{L}\), where \(L\) denotes the sequence length, we first compute the input-dependent matrices based on Eq. (2). Then, we calculate the saliency value for each token based on Eq. (6), _i.e._, for the \(i\)-th token \(x_{i}\):

\[Saliency_{M}(x_{i})=S_{C}(x_{i})(S_{}(x_{i}))S_{B}(x_{i} )x_{i},\] (7)

Afterward, we generate a binary mask \(_{S}^{L}\) with the element being set to \(1\) if the corresponding element \(Saliency_{M}(x_{i})\) is in the top \(P_{tokens}\) percentage elements.

Meanwhile, we synthesize the style-augmented sequence, which is achieved by mixing the mean and variance of different samples. Following , we first compute the style statistics as: \((x)=_{i=1}^{L}x_{i},(x)=_{i=1}^{ L}(x_{i}-(x))^{2}}\). Then we randomly select another sample \(x^{}\) from the current batch, utilizing its statistics to synthesize the stylized version of \(x\):

\[&=(x)+(1-)(x^{ }),=(x)+(1-)(x^{}), \\ & Beta(0.1,0.1),=+,\] (8)

Finally, with the token-level mask \(_{S}\), we mix \(x\) and \(\) to generate the augmented sequence \(x_{}\), where tokens with maximum saliency are style-augmented, while other tokens remain unchanged:

\[x_{}=_{S} x+(1-_{S}),\] (9)

where \(\) is element-wise multiplication. Note that when \(P_{token}=1\), START-M degenerates into a channel-level augmentation, _i.e._, MixStyle . However, note that style statistics could be one kind

Figure 2: **Overall Architecture of the Proposed START Framework**. The core of the START framework is the Saliency-driven Token-Aware Transformation, which uses a saliency-driven scheme to localize tokens targeted by input-dependent matrices, subsequently perturbing domain-specific style information within these tokens. We designed two variants: START-M, which uses input-dependent matrices, and START-X, which uses input sequences to compute saliency.

of domain-specific feature, while other forms of domain-specific features may also exist, especially within the image backgrounds [59; 63]. As a result, directly perturbing the style information of tokens on the background might activate other forms of domain-related noise, which could still disrupt the model generalization . To address this issue, our START-M proposes to selectively perturb tokens with the highest saliency, which are typically associated with foregrounds, thus enhancing the model learning of domain-invariant information without activating domain-related noise. Ablation study in Section 4.3 also proves the effectiveness of the saliency-driven selection scheme.

**START based on input sequences (START-X).** Based on **Proposition 2**, recalling that \(\), \(B\), and \(C\) are all input-dependent matrices (as in Eq. (2)), we design a simplified variant, namely START-X, which involves using the activation values of \(x\) to approximate the saliency of tokens directly. Specifically, for the \(i\)-th input token \(x_{i}\), we directly compute its saliency value as: \(Saliency_{XY}(x_{i})=x_{i}\). With the saliency for each token, we compute the token-level binary mask \(_{S}\) as that of START-X and employ Eq. (9) to generate the augmented sequences. In practice, we randomly apply our START method to \(50\%\) of the samples in each batch, leaving the remaining samples unperturbed during each training iteration. Our START method is disabled during inference.

In summary, we theoretically investigate the generalization error boundary of Mamba at the token level, highlighting that suppressing domain-related information within input-dependent matrices can effectively reduce the generalization error boundary of the model. Based on the theoretical analysis, we propose the first saliency-driven token-aware transformation for SSMs, designing two different variants for identifying and perturbing the tokens focused on by the input-dependent matrices \(B,C,\). In this way, our method can effectively enhance the reliance of the input-dependent matrices on domain-invariant features and narrow the distance between source and target domains. Notably, our START introduces no additional parameters or inference time, only involving a few matrix operations during training, thus achieving similar linear complexity to VAMba as presented in Appendix A.2.

## 4 Experiments

### Experimental Setup

**Datasets.** We perform an extensive evaluation on five DG datasets: **PACS** comprises \(9,991\) images of \(7\) classes from \(4\) domains: Photo, Art Painting, Cartoon, and Sketch. **OfficeHome** includes \(15,588\) images of \(65\) classes from four diverse domains: Artistic, Clipart, Product, and Real-World, exhibiting a large domain gap. **VLCS** contains \(10,729\) images of \(5\) categories from \(4\) domains: Pascal, LabelMe, Caltech, and Sun. **TerraIncognita** comprises photographs of wild

    & &  &  \\  Method & Params. & Art & Cartoon & Photo & Sketch & Avg. & Art & Clipart & Product & Real & Avg. \\   \\  DeepAll  (AAAAAT-20) & 23M & 84.70 & 80.80 & 97.20 & 79.30 & 85.50 & 61.30 & 52.40 & 75.80 & 76.60 & 66.50 \\ PCL  (CVPR-2018) & 23M & 90.20 & 83.90 & 98.10 & 82.60 & 88.70 & 67.30 & 59.90 & 78.70 & 80.70 & 71.60 \\ EQA _67_ (AAAT-20) & 23M & 90.50 & 83.40 & 98.00 & 82.50 & 88.60 & 69.10 & 95.80 & 79.50 & 81.50 & 72.50 \\ EQRM  (CVPR-2018) & 23M & 86.50 & 82.10 & 96.60 & 80.80 & 86.50 & 60.50 & 56.00 & 76.10 & 77.40 & 67.50 \\ SAGM  (CVPR-2018) & 23M & 87.40 & 80.20 & 98.00 & 80.80 & 86.60 & 65.40 & 57.00 & 78.00 & 80.00 & 70.10 \\ IDAG (YOT-2018) & 23M & 90.80 & 83.70 & 98.20 & 82.70 & 88.80 & 68.20 & 59.70 & 79.70 & 81.40 & 71.80 \\ DomainD  (YOT-2018) & 23M & 89.82 & 84.22 & 98.02 & 85.98 & 89.51 & 67.33 & 60.39 & 79.05 & 80.22 & 71.75 \\ CPCFP  (CVPR-2018) & 23M & 87.50 & 81.30 & 96.40 & 81.40 & 86.60 & 63.70 & 55.50 & 77.20 & 79.20 & 68.90 \\ MADG  (CVPR-2018) & 23M & 87.80 & 82.20 & 97.70 & 78.30 & 86.50 & 67.60 & 54.10 & 78.40 & 80.30 & 70.10 \\ PoFAI  (CVPR-2018) & 23M & 87.60 & 79.10 & 97.40 & 76.30 & 85.10 & 64.70 & 56.00 & 77.40 & 78.80 & 69.30 \\ ARTA  (CVPR-2018) & 23M & 89.80 & 85.20 & 97.60 & 84.70 & 89.30 & 67.50 & 85.80 & 79.30 & 80.70 & 71.50 \\ GMDG  (CVPR-2018) & 23M & 84.70 & 81.70 & 97.50 & 80.50 & 85.60 & 68.90 & 56.20 & 79.90 & 82.00 & 70.70 \\    \\  MLP-B  (AAAT-20) & 59M & 55.00 & 77.86 & 94.43 & 65.72 & 80.75 & 63.45 & 56.31 & 77.81 & 79.76 & 69.33 \\ SDVT  (ACCV-2018) & 22M & 87.60 & 82.40 & 98.00 & 77.720 & 86.30 & 68.30 & 56.30 & 79.50 & 81.80 & 71.50 \\ ResMLP-S  (APM-2018) & 40M & 85.50 & 78.63 & 97.07 & 72.64 & 83.46 & 62.42 & 51.94 & 75.40 & 77.21 & 66.74 \\ ViP-S  (APM-2018) & 25M & 89.42 & 89.38 & 82.41 & 88.27 & 69.55 & 61.51 & 79.34 & 83.11 & 73.38 \\ GMee-S  (CL-2018) & 34M & 89.40 & 83.90 & 99.10 & 74.50 & 86.70 & 69.30 & 58.00 & 79.80 & 82.60 & 72.40 \\    \\  DG Mamba  (AAAT-20) & 22M & 91.30 & 87.00 & 99.00 & 87.30 & 91.20 & **76.20** & 61.80 & 83.90 & 86.10 & 77.00 \\  Strong Baseline  & 22M & 91.55 & 85.11 & 99.14 & 83.97 & 89.94\({}_{105}\) & 75.06 & 60.48 & 84.71 & 85.45 & 76.43\({}_{105}\) \\ START-M (Ours) & 22M & **93.29** & **87.56** & 99.14 & 87.07 & **91.77\({}_{104.00}\)** & 75.15 & 62.04 & **85.31** & **85.84** & **77.09\({}_{105.16}\)** \\ START-X (Ours) & 22M & 92.76 & 87.43 & **99.22** & **87.46** & 91.72\({}_{104.00}\) & 75.48 & **62.06** & 85.24 & 85.47 & 77.70\({}_{105.02}\) \\   

Table 1: Performance (%) comparisons with the SOTA DG methods on PACS and OfficeHome.

animals taken by \(4\) camera-trap domains, with \(10\) classes and a total of \(24,788\) images. **DomainNet** is large-scale with \(586,575\) images, having \(345\) classes from \(6\) domains, _i.e._, Clipart, Infograph, Painting, Quickdraw, Real, and Sketch. Results on DomainNet are reported in Appendix A.2.

**Implementation details.** We closely follow the implementation of VMamba  and use the VMamba-T, which has similar parameters with ResNet-\(50\) (\(22\)M vs. \(23\)M), as the backbone. The backbone is pretrained on the ImageNet  for all our experiments. We partition the input image into \(4 4\) patches without further flattening the patches into a \(1\)D sequence. The network depth of the VMamba-T backbone is \(4\) the same as ResNet-\(50\), consisting of \(2\), \(2\), \(9\), and \(2\) VSS layers, respectively. The embedding dimensions of blocks in the \(4\) stages are fixed as \(\). Following existing DG methods [15; 83], we train the model for \(50\) epochs using AdamW optimizer and cosine decay schedule, with a batch size of \(64\), the initial learning rate as \(5e-4\), and the momentum of \(0.9\). For all experiments, we the ratio \(P_{token}\) of augmented tokens to \(0.75\). We apply the leave-one-domain-out protocol for all benchmarks, where one domain is used for testing, and the remaining domains are employed for training. We select the last-epoch model and report the average accuracy over five runs. All the experiments are run on \(4\) NVIDIA Teska V100 GPUs.

### Main Results

**Evaluation on PACS.** We first compare our method with SOTA CNN-based DG methods on ResNet-\(50\). As shown in Tab. 1, the strong baseline (VMamba) achieves a promising performance, exceeding ResNet-\(50\) by \(4.44\%\) (\(89.94\%\) vs. \(85.50\%\)), which indicates its superiority for DG. Moreover, we apply our START to the strong baseline and build advanced models, which can achieve significant improvements without introducing extra parameters. Notably, START-M achieves the SOTA performance, improving baseline by \(1.83\%\) (\(91.77\%\) vs. \(89.94\%\)) and yielding the latest CNN-based DG method GMDG  by \(6.17\%\) (\(91.77\%\) vs. \(85.60\%\)). START-X can also improve the baseline significantly by \(1.78\%\) (\(91.72\%\) vs. \(89.94\%\)). Compared with SOTA ViT-based methods, START-M still performs excellent, yielding GMoE-S  by \(5.07\%\) (\(91.77\%\) vs. \(86.70\%\)) with small network sizes (\(22\)M vs. \(34\)M). Finally, our methods beat the recent DGMamba , exceeding it by \(0.57\%\) (\(91.77\%\) vs. \(91.20\%\)) on average, which proves the effectiveness of our method for DG.

**Evaluation on OfficeHome.** We evaluate the effectiveness of our method on OfficeHome and present the results in Tab. 1. Our methods achieve significant improvements compared with CNN-based methods, _e.g._, START-M outperforms the SOTA method EoA  by \(4.59\%\) (\(77.09\%\) vs. \(72.50\%\)) on ResNet-\(50\). Based on the Strong Baseline with high performance, our method can still improve it by \(0.66\%\) (\(77.09\%\) vs. \(76.43\%\)). START-M precedes the best MLP-like model ViP-S , which learns long-range dependencies along height and weight directions, with a large improvement of \(4.71\%\) (\(77.09\%\) vs. \(73.38\%\)). The results justify the superiority of START.

**Evaluation on VLCS.** As presented in Tab. 2, our START achieves the best performance among all competitors, surpassing the top CNN-based method SAGM  by \(1.32\%\) (\(81.32\%\) vs. \(80.00\%\)).

    & &  &  \\  Method & Params. & Caltech & LabelMe & SUN & PASCAL & Avg. & \(\)1.100 & \(\)1.38 & \(\)1.43 & \(\)1.46 & Avg. \\   \\  DeepAll  (AAAI’20) & 23M & 97.70 & 64.30 & 73.40 & 74.60 & 77.50 & 49.80 & 42.10 & 56.90 & 35.70 & 46.10 \\ PCL  (CVPR’22) & 23M & 99.00 & 63.60 & 73.80 & 75.60 & 78.00 & 58.70 & 46.30 & 60.00 & 43.60 & 52.10 \\ EoA  (VAMI’22) & 23M & 99.10 & 63.10 & 75.90 & 78.30 & 79.10 & 57.80 & 46.50 & 61.30 & 43.50 & 52.30 \\ EQRM  (VAMI’22) & 23M & 98.30 & 63.70 & 72.60 & 76.70 & 77.70 & 78.40 & 47.50 & 52.90 & 31.38 & 47.80 \\ SACM  (VAMI’22) & 23M & 99.00 & 65.20 & 75.10 & 80.70 & 80.00 & 58.40 & 41.40 & 57.70 & 41.30 & 48.80 \\ iDAG  (vocY’2) & 23M & 98.10 & 62.70 & 69.90 & 77.10 & 76.90 & 58.70 & 35.10 & 57.50 & 33.00 & 46.10 \\ CCP  (vocY’2) & 23M & 98.10 & 64.90 & 74.50 & 78.30 & 78.90 & 56.40 & 42.30 & 58.80 & 37.50 & 48.60 \\ PGrad  (vocY’2) & 23M & 98.30 & 64.40 & 74.40 & 79.90 & 79.30 & 75.10 & 43.40 & 60.00 & 41.30 & 49.00 \\ AGFA  (vL8’2) & 23M & **99.00** & 64.50 & 75.40 & 78.90 & 79.50 & 61.00 & 46.20 & 60.30 & 42.30 & 52.40 \\ GMDG  (cvPR’2) & 23M & 98.30 & 65.90 & 73.40 & 79.30 & 79.20 & 59.80 & 45.30 & 57.10 & 38.20 & 50.10 \\    \\  SDVT  (vocY’2) & 22M & 96.80 & 64.20 & 76.20 & 78.50 & 78.90 & 55.90 & 31.70 & 52.20 & 37.40 & 44.30 \\ GMoE-S  (cvL8’2) & 34M & 96.90 & 63.20 & 72.30 & 79.50 & 78.00 & 59.20 & 34.00 & 50.70 & 38.50 & 45.60 \\    \\  DGMMamba  (ACAI’20) & 22M & 98.90 & 64.30 & **79.20** & 80.80 & 80.80 & 62.70 & 48.30 & 61.10 & 46.40 & 54.60 \\  Strong Baseline  & 22M & 97.67 & 64.25 & 75.81 & 79.97 & 79.42\(\)0.05 & 66.39 & 47.27 & 62.42 & 48.56 & 56.16\(\)0.84 \\ START-M (Ours’) & 22M & 98.80 & **66.98** & 77.18 & 82.33 & **81.32** & 90.73 & **49.95** & 63.02 & **49.49** & 58.16\(\)0.79 \\ START-X (Ours’) & 22M & 98.66 & 66.64 & 76.97 & **82.58** & 81.21\(\)0.05 & **70.70** & 49.47 & **63.96** & 48.95 & **58.27\(\)0.55** \\   

Table 2: Performance (%) comparisons with the SOTA DG methods on VLCS and TerralIncapnita.

Additionally, our method significantly improves upon the baseline, outperforming the latest Mamba-based method DGMamba by \(0.52\%\) (\(81.32\%\) vs. \(80.80\%\)).

**Evaluation on TerraIncognita.** As shown in Tab. 2, We observe that the VMamba baseline significantly outperforms previous methods, achieving a SOTA performance of \(56.16\%\). Based on the strong baseline, our method can further achieve substantial improvement by \(2.11\%\) (\(58.27\%\) vs. \(56.16\%\)), proving that our method effectively suppresses domain-specific features learned by Mamba.

### Ablation Study and Analytical Experiments

**Ablation study.** We here validate the effectiveness of each operation in START. Specifically, _w/o Saliency Guided_ denotes random selection of tokens for perturbation within input sequences, while _w/o Token Selection_ means perturbing the entire input sequences. Tab. 3 presents the results using VMamba backbone on PACS. Both variants show improvements over the baseline, indicating that perturbing style information can mitigate overfitting issues. However, as discussed in Section 3.3, _w/o Saliency Guided_, which randomly perturbs tokens, fails to provide strong regularization. Besides, the result of _w/o Token Selection_ is inferior to our START, suggesting that perturbing background tokens could activate other forms of domain-specific features, potentially hindering model generalization.

**Parameter sensitivity.** We explore the sensitivity of our method to the hyper-parameter \(P_{token}\), the percentage of perturbed tokens in input sequences. As shown in Fig. 3, START-M consistently performs well across different \(P_{token}\) values, demonstrating its effectiveness in perturbing domain-specific information in salient tokens. We notice that START-X, which uses token activation to approximate saliency, performs similarly to START-M when \(P_{token}\) is high but is less effective at lower \(P_{token}\) values. This indicates differences between attention regions of input-dependent matrices and input sequences. Both methods achieve the highest accuracy at \(P_{token}=0.75\), which is adopted for all experiments.

**Comparisons with other feature identification methods.** We provide comparisons with the "GradCAM" and "Attention Matrix" methods. For the "GradCAM" method, we first obtain feature gradients using backpropagation without updating, then compute token saliency and augment salient tokens at each iteration. For the "Attention Matrix" method, since the Mamba architecture lacks explicit attention matrices, we instead use \(\) in Eq. (3) to calculate token saliency. As shown in Tab. 4, on the strong baseline, START still performs much better than these advanced methods, exceeding "GradCAM" by \(0.91\%\) (\(91.77\%\) vs. \(90.86\%\)) and "Attention Matrix" by \(1.00\%\) (\(91.77\%\) vs. \(90.77\%\)). It is owing to the ability of START to explicitly suppress domain-specific features within input-dependent matrixes.

**Comparisons with other augmentation methods.** We here compare our method with SOTA DG augmentation methods on the VMamba backbone, including MixStyle , DSU , and ALGFT . As shown in Tab. 5, all the augmentation methods bring performance improvements, indicating that increasing data diversity is beneficial for the generalization ability of Mamba. Notably, START outperforms all the SOTA augmentation methods, _i.e._, yielding a significant margin of \(1.06\%\)

   Method & Art & Cartoon & Photo & Sketch & Avg. \\  Baseline  & 91.55 & 85.11 & 99.14 & 83.97 & 89.94\(\)0.52 \\  GradCam & 92.56 & 86.99 & 98.98 & 84.92 & 90.86\(\)0.24 \\ Attention Matrix & 91.75 & 86.68 & 98.88 & 85.76 & 90.73\(\)0.30 \\  START-M (Ours) & **93.29** & **87.56** & 99.14 & 87.07 & **91.77\(\)0.40** \\ START-X (Ours) & 92.76 & 87.43 & **99.22** & **87.46** & 91.72\(\)0.40 \\   

Table 4: Comparison (%) with other salient feature identification methods on PACS with VMamba as the backbone.

   Method & Art & Cartoon & Photo & Sketch & Avg. \\  Baseline  & 91.55 & 85.11 & 99.14 & 83.97 & 89.94\(\)0.52 \\  Baseline  & 91.55 & 85.11 & 99.14 & 83.97 & 89.94\(\)0.52 \\  w/o. Saliency Guided & 92.11 & 86.23 & 99.10 & 85.82 & 90.81\(\)0.24 \\ w/o. Token Selection & 92.05 & 86.55 & 98.90 & 86.35 & 90.94\(\)0.18 \\  START-M (Ours) & **93.29** & **87.56** & 99.14 & 87.07 & **91.77\(\)0.40** \\ START-X (Ours) & 92.76 & 87.43 & **99.22** & **87.46** & 91.72\(\)0.49 \\   

Table 3: Ablation study on the PACS dataset.

Figure 3: Sensitivity to \(P_{token}\).

   Method & Art & Cartoon & Photo & Sketch & Avg. \\  Baseline  & 91.55 & 85.11 & 99.14 & 83.97 & 89.94\(\)0.52 \\  Baseline  & 91.55 & 85.11 & 99.14 & 83.97 & 89.94\(\)0.52 \\  Baseline  & 91.55 & 85.11 & 99.14 & 83.97 & 89.94\(\)0.52 \\  w/o. Saliency Guided & 92.11 & 86.23 & 99.10 & 85.82 & 90.81\(\)0.24 \\ w/o. Token Selection & 92.05 & 86.55 & 98.90 & 86.35 & 90.94\(\)0.18 \\  START-M (Ours) & **93.29** & **87.56** & 99.14 & 87.07 & **91.77\(\)0.40** \\ START-X (Ours) & 92.76 & 87.43 & **99.22** & **87.46** & 91.72\(\)0.49 \\   

Table 3: Ablation study on the PACS dataset.

(\(91.77\%\) vs. \(91.72\%\)) from DSU. The results prove the effectiveness of our methods in perturbing domain-specific features within input-dependent matrices.

**Domain gaps in input-dependent matrices.** To verify the effectiveness of our method in reducing domain gaps within input-dependent matrices, we compare domain gaps across different methods using PACS with VMamba. The experiments focus on the last block of VMamba, examining the output feature maps ("Feat.") and the input-dependent matrices \(\), \(B\), and \(C\) of the first SS2D. The results in Tab. 6 align well with theoretical analysis in Section 3.2, proving that START effectively reduces domain gaps in the input-dependent matrices.

## 5 Conclusions

In this paper, inspired by the success of Mamba in supervised tasks, we theoretically study the generalizability of Mamba and find that the input-dependent matrices in Mamba could accumulate and amplify domain-specific features during training. To address the issue, we propose a generalized state space model with a saliency-driven token-aware transformation for DG, which can selectively augment domain-specific features within salient tokens focused on by the input-dependent matrices, thus helping the model learn domain-invariant features. Our method outperforms SOTA CNN-based and ViT-based methods by a significant margin with linear complexity and a small-sized network. We hope our work inspires further research in DG and contributes valuable insights to the community.

## 6 Acknowledgment

This work was supported by the National Key R&D Program of China (2023ZD0120700, 2023ZD0120701), NSFC Project (62222604, 62206052), China Postdoctoral Science Foundation (2024M750424), the Fundamental Research Funds for the Central Universities (020214380120), the State Key Laboratory Fund (ZZKT2024A14), the Postdoctoral Fellowship Program of CPSF (GZC20240252), and the Jiangsu Funding Program for Excellent Postdoctoral Talent (2024ZB242).