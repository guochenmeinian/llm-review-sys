# Attention Interpolation for Text-to-Image Diffusion

Qiyuan He\({}^{1}\)  Jinghao Wang\({}^{2}\)  Ziwei Liu\({}^{2}\)  Angela Yao\({}^{1,}\)

\({}^{1}\)National University of Singapore \({}^{2}\)S-Lab, Nanyang Technological University

qhe@u.nus.edu.sg  ayao@comp.nus.edu.sg

{jinghao003, ziwei.liu}@ntu.edu.sg

###### Abstract

Conditional diffusion models can create unseen images in various settings, aiding image interpolation. Interpolation in latent spaces is well-studied, but interpolation with specific conditions like text or image is less understood. Common approaches interpolate linearly in the conditioning space but tend to result in inconsistent images with poor fidelity. This work introduces a novel training-free technique named **Attention Interpolation via Diffusion (AID)**. AID has two key contributions: **1)** a fused inner/outer interpolated attention layer to boost image consistency and fidelity; and **2)** selection of interpolation coefficients via a beta distribution to increase smoothness. Additionally, we present an AID variant called **Prompt-guided Attention Interpolation via Diffusion (PAID)**, which **3)** treats interpolation as a condition-dependent generative process. Experiments demonstrate that our method achieves greater consistency, smoothness, and efficiency in condition-based interpolation, aligning closely with human preferences. Furthermore, PAID offers substantial benefits for compositional generation, controlled image editing, image morphing and image-controlled generation, all while remaining training-free. Our code and demo are available at https://qy-h00.github.io/attention-interpolation-diffusion/.

## 1 Introduction

Interpolation is a common operation applied to generative image models. It generates smoothly transitioning sequences of images from one seed to another within the latent space and facilitates applications in image attribute modification , data augmentation , and videos . Interpolation has been investigated extensively [18; 43; 42] in VAEs , GANs , and diffusion models . Text-to-image diffusion models [35; 37] are a new class of _conditional_ generative models that generate high-quality images conditioned on textual descriptions. How to interpolate between distinct text conditions such as "a truck" and "a cat" (see Fig. 1 (d)) is relatively under-explored. This issue is, however, crucial for various downstream tasks, such as conditional generation with multiple conditions [6; 25; 51] or cross-modality conditions [54; 56], as well as for image editing [11; 48], where precise control over impact of different conditions is essential to achieve desired results.

This paper formulates the task of _conditional interpolation_ and identifies three ideal properties for interpolating text-to-image diffusion models: thematic consistency, smooth visual transitions between adjacent images, and high-quality interpolated images. For instance, interpolating from _"a truck"_ to _"a cat"_ should avoid irrelevant transitions (_e.g._, via _"a bowl"_). The sequence should change between the two conditions gradually and feature high-quality and high-fidelity images (_vs. e.g._ simple overlays of the truck and cat). These properties directly motivate our quantitative evaluation metrics for conditional interpolation: consistency, smoothness, and fidelity.

A direct approach to traverse the conditioning space is interpolating in the text embedding itself [53; 55; 16]. Such an approach often has sub-optimal results (see the first row of Fig. 2). A closer analysis reveals that interpolating the text embedding is mathematically equivalent to interpolating the keys and values of the cross-attention module between the text and image space. Our analysis further 38th Conference on Neural Information Processing Systems (NeurIPS 2024).

reveals that the keys and values in self-attention impose a stronger influence than cross-attention, which may explain why text embedding interpolation fails to produce consistent results.

Based on our analysis, we introduce a novel framework: Attention Interpolation of Diffusion (AID) models for conditional interpolation. AID enhances interpolation quality with (1) a fused interpolated attention mechanism on both cross-attention and self-attention layers to improve consistency and fidelity and (2) a Beta-distribution-based sample selection along the interpolation path for interpolation smoothness. Additionally, we introduce (3) Prompt-guided Attention Interpolation of Diffusion (PAID) models to further guide the interpolation via a text description of the path itself.

Experiments on various state-of-the-art diffusion models  highlight our approach's effectiveness (see samples in Fig. 1 and more in Appx. H) without any additional training.

Human evaluators predominantly prefer our method over standard text embedding interpolation. We further show that our method can benefit various downstream tasks, such as composition generation, and boost the control ability of image editing. Our method is also compatible with image condition (see Fig. 1 (b)), which can be further used for more application such as image morphing and image-controlled generation. This underscores the practical impact of the problem of conditional interpolation and our proposed solution. Our main contributions are:

* Problem formulation for conditional interpolation within the text-to-image diffusion model context and proposing evaluation metrics for consistency, smoothness, and fidelity;
* A novel and effective training-free method AID for text-to-image interpolation. AID can be augmented with prompt-guided interpolation (PAID) to control specific paths between two conditions;
* Extensive experiments highlight AID's improvements for text-based image interpolation. AID substantially improves interpolation sequences, with significant enhancements in fidelity, consistency, and smoothness without any training. Human studies show a strong preference for the AID;

Figure 1: **Our approach enables text-to-image diffusion models to generate nuanced spatial and conceptual interpolations between different conditions including text (a, c-e) and image (b), with seamless transitions in layout, conceptual blending, and user-specified prompts to guide the interpolation paths (f).**

* We show that AID offers much better control ability for diffusion-based image editing, and it can be used for compositional generation with state-of-the-art performance. It is also compatible with image condition, enabling more applications such as image morphing or controlling the scale of additional image prompt.

## 2 Related Work

**Diffusion Models and Attention Manipulation**. The emergence of diffusion models has significantly transformed the text-to-image synthesis domain, with higher quality and better alignment with textual descriptions [35; 37; 33]. Attention manipulation techniques have been instrumental in unlocking the potential of diffusion models, particularly in applications such as in-painting and compositional object generation. These applications benefit from refined control over the attention maps, aligning the modifier and the target object more closely to enhance image coherence [11; 1; 3; 51; 34]. Furthermore, cross-frame attention mechanisms have shown promise in augmenting visual consistency within video generation frameworks utilizing diffusion models [17; 31]. These works suggest that the visual closeness of two generated images may be reflected in the similarity of their attention maps and motivates us to study interpolation from an attention perspective.

**Interpolation in Image Generative Models**. Interpolation within the latent spaces of models such as GANs  and VAEs  has been studied extensively [43; 18; 46]. More recently, explorations of diffusion model latent spaces allow realistic interpolations between real-world images [38; 21]. Works to date, however, are limited to a single condition, and there is a lack of research focused on interpolation under varying conditions. Wang & Golland explored linear interpolation within text embedding to interpolate real-world images; however, this approach yields image sequence with diminished fidelity and smoothness. This gap underscores the need for further exploration of conditional interpolation in generative models.

## 3 Analysis of Conditional Interpolation

### Text-to-Image Diffusion Models

Text-to-image diffusion models such as Stable Diffusion [35; 30] generate images from specified text. Consider the generation of an image for some specified text as an inference process denoted by \(f(z^{T},c)\). The function \(f\) is an abstraction representing the denoising diffusion process, \(c\) is a representation of the conditioning text and \(z^{T}\) is a randomly sampled latent seed. Usually, \(c\) is represented as a CLIP text embedding , while the \(z\)'s over the denoising time steps of the generation are sampled from a Gaussian distribution. More specifically, if the inference is carried out over \(T\) denoising time steps, the latent \(z^{t-1}\) can be sampled conditionally, based on \(z^{t}\):

\[p_{}(z^{t-1}|z^{t})=(z^{t-1};_{}(z^{t},c,t),_{ }(z^{t},c,t)),z^{t} N(0,1),\] (1)

where \(t\) represents the denoising time-step index, \(_{}(z^{t},c,t)\) is estimated by a UNet , and \(_{}(z^{t},c,t))\) is determined by a noise scheduler [13; 42]. After iterative sampling from \(z^{T}\) to \(z^{0}\), the image is generated by decoder \(D\), as \(D(z^{0})\).

Attention is used in text-to-image diffusion models [35; 29; 37] in various forms. Cross-attention is commonly used as the link from the text condition to the image generation. Specifically, given a latent variable \(z^{d_{z}}\), text condition \(c^{d_{c}}\) and the attention layer with matrices \(W_{Q}^{d_{z} d_{q}}\), \(W_{K}^{d_{c} d_{k}}\) and \(W_{V}^{d_{c} d_{v}}\), the cross-attention is computed as

\[A(z,c)=(Q,K,V)=(}{}}) V,Q=W_{Q}^{}z,\ K=W_{K}^{}c,\ V=W_{V}^{}c.\] (2)

Self-attention is also commonly used in state-of-the-art text-to-diffusion models [30; 35; 2]. Self-attention is a special case of cross-attention and can also be computed with Eq. 2 as \(A(z,z)\). In this case, the key and values are defined as \(K=W_{K}^{}z,\ V=W_{V}^{}z\) respectively. For brevity, we abuse the notation to directly represent multi-head attention  and denote the attention layer as \((Q,K,V)\) in both cross-attention and self-attention scenarios.

### Text Embedding Interpolation

In this paper, we denote linear and spherical interpolation [49; 21] as \(r_{l}(w;A,B)\) and \(r_{s}(w;A,B)\) respectively, where \(w\) is the interpolation coefficient, and \((A,B)\) are the interpolation anchors or end-points. Conditional interpolation differs from standard text-to-image generation in that there are two text conditions \(c_{1}\) and \(c_{m}\). Each condition has its own respective latent seeds \(z_{1}\) and \(z_{m}\)1. The objective of conditional interpolation is to generate a sequence of images \(\{I_{1:m}\}=\{I_{1},I_{2},...,I_{m}\}\). In this sequence, the source images are generated by the standard text-to-image generation, i.e, \(I_{1}=f(z_{1},c_{1})\), \(I_{m}=f(z_{m},c_{m})\) as described in Sec. 3.1.

Existing literature has shown that similarity in input space, including the latent seed and the embedding of the condition reflects the similarity in the output pixel space [19; 49]. Directly interpolating the text embedding \(c\) is therefore a straightforward approach that can be used to generate an interpolated image sequence [49; 16]. In text embedding interpolation, the text conditions \(\{c_{1},c_{m}\}\) and their latent seeds \(\{z_{1},z_{m}\}\) are used as endpoints for interpolation and images are generated accordingly:

\[I_{i}\!=\!f(z_{i},c_{i})z_{i}\!=\!r_{s}(w_{i};z_{1},z_{m}),\ c_{i}\!=\! r_{l}(w_{i};c_{1},c_{m}),w_{i}\!=\!\ i=\{1 m\}.\] (3)

Note that spherical interpolation is applied to estimate the latent seed \(z_{i}\) to ensure that Gaussian noise properties are preserved . In contrast, linear interpolation is applied to estimate the text condition [53; 49; 16; 24]\(c_{i}\). For both \(z_{i}\) and \(c_{i}\), the interpolation coefficient \(w_{i}\) sampled in uniform increments from \(1\) to \(m\).

Given that the text condition \(c\) directly propagates into the key and value \(K\) and \(V\) (Eq. 2), interpolating between the text conditions \(c_{1}\) and \(c_{m}\) is equivalent to interpolating the associated keys and values in cross-attention. This is stated formally in the following proposition:

**Proposition 1**.: _Given query \(Q\) from a latent variable \(z\), keys and values \(\{K_{1},V_{1}\}\) and \(\{K_{m},V_{m}\}\) from text conditions \(\{c_{1},c_{m}\}\) and linearly interpolated text conditions \(c_{i}\), the resulting cross-attention module \(A(z,c_{i})\) is given by linearly interpolated keys and values \(_{i}\) and \(_{i}\):_

\[A(z,c_{i})=Attn(Q,_{i},_{i}),_{i}=r_{l}(w_{i};K_ {1},K_{m})_{i}=r_{l}(w_{i};V_{1},V_{m}),\] (4)

_where \(w_{i}\) is defined similarly as Eq. 3._

The proof for Proposition 1 is given in the Appx. A. This proposition gives insight into how text embedding interpolation can be viewed as manipulating the keys and values. Specifically, it is equivalent to interpolating the keys and values to generate the resulting interpolated image. It is worth noting that an analogous interpretation does not carry through for the query \(Q\) even though it also depends on some interpolated latent seed \(z^{t}\). This is because \(z_{i}^{t}\) is estimated as an interpolation between the latent seeds \(z_{1}^{t}\) and \(z_{m}^{t}\), while the latent \(z_{i}^{t}\) itself is progressively altered through the denoising process (see Eq. 1).

### Measuring the Quality of Conditional Interpolation

Text embedding interpolations work well when the conditions are semantically related, e.g., _"a dog"_ and _"a cat"_, but may lead to failures in less related cases. To better analyze the characteristics of the interpolated image sequences, we define three measures based on ideal interpolation qualities: consistency, smoothness, and image fidelity.

Figure 2: **Results comparison between AID (the \(1^{}\) row) and text embedding interpolation (the \(2^{}\) row). AID increases smoothness, consistency, and fidelity significantly.**

**Perceptual Consistency.** Ideally, the interpolated image sequence should transition from one source or endpoint to the other in a perceptually direct and therefore consistent path. Similar to , we use the average LPIPS metric  across all adjacent image pairs in the sequence to evaluate consistency. If \(P\) denotes the LPIPS model, the consistency \(C\) of a sequence \(I_{1:m}\) is defined as:

\[C(I_{1:m};P)=_{i=1}^{m-1}P(I_{i},I_{i+1}).\] (5)

For example, a consistent interpolation from _"an apple"_ to _"a bed"_ may pass through _"an apple and a bed"_ but should not have intermediate stages like _"a messy sketch"_ (see Fig. 2 (a)).

**Perceptual Smoothness.** A well-interpolated sequence should exhibit a gradual and smooth transition. We propose to apply Gini coefficients on the perceptual distance between each neighbouring pair of interpolated images to indicate smoothness. Gini coefficients  are a conventional indicator of data imbalance  where higher coefficients indicate more imbalance. And the imbalance of perceptual distance of each neighbouring pair indicates low smoothness. Let \(G(X)\) denote the Gini coefficient of a set \(X=\{x_{1},x_{2},...,x_{n}\}\). The smoothness \(S\) of a sequence \(I_{1:m}\) with \(P\) denoting the LPIPS model is defined as:

\[S(I_{1:m};P)=1-G(_{i=1}^{m-1}P(I_{i},I_{i+1})), G(X)=^{n}_{j=1}^{n}|x_{i}-x_{j}|}{2n_{i=1}^{n}x_{i}}.\] (6)

Fig. 2(b) shows how a smooth interpolation sequence exhibits a gradual transition on the visual content (from _"a lady wearing oxygen mask"_ to _"lion"_ in the top row) instead of one source image or end-point dominating the sequence (the _"lion"_ in the bottom row).

**Fidelity**. Finally, any interpolated images should be of the same (high) quality conventionally generated images. Following , we evaluate the fidelity of interpolated images with the Frechet Inception Distance (FID) . Given \(n\) interpolated sequences \(\{I_{1:m}^{(1)},I_{1:m}^{(2)},...,I_{1:m}^{(n)}\}\), the fidelity \(F\) of the sequences is defined as the FID based on a visual inception model 2 The FID between the source images and the interpolated images is defined as:

\[F(I_{1:m}^{(1)},I_{1:m}^{(2)},...,I_{1:m}^{(n)})=FID_{M_{V}}(_{j=1 }^{n}\{I_{1}^{(j)},I_{m}^{(j)}\},_{j=1}^{n}\{I_{i}^{(j)}|i 1,i  m\})\] (7)

For example, the interpolated sequence should have minimal artifacts (see Fig. 2(a)), where the top row clearly shows the appearance of the apple, whereas the bottom row does not.

### Diagnosing Text Embedding Interpolation

Experimentally (see Sec. 5.1), we observe that text embedding interpolation sequences exhibit poor consistency and smoothness. The interpolated images are also commonly low in fidelity, with indirect and non-smooth transitions. _Where do the failures of text embedding interpolation come from?_ We analyze the outputs from the perspective of spatial layouts and the selection of interpolation coefficients.

**Spatial Layouts and Attention.** Consistency is directly affected by the difference between the spatial layout of the source and interpolated images. One observation is that the spatial layout of interpolated images from text embedding interpolations is quite different from the source endpoints (see Fig. 2 (a) bottom row). Proposition 1 links the text embedding interpolation to the cross-attention mechanism exclusively. However, the literature suggests that the spatial layout of the overall image is strongly linked to the self-attention mechanism . As such, we hypothesize that cross-attention does not pose enough spatial layout constraints stand-alone. Instead, there is a need for a stronger link of the interpolation to self-attention, to allow more consistent spatial transitions.

As a simple test, we swap the keys and values from two text-to-image generations. Consider two images \(I\) and \(I^{}\) generated from two text prompts \(p\) and \(p^{}\). We replace the keys and values fromeither the cross-attention or self-attention layers in the generative process of \(I\) with that of \(I^{}\) to generate \(I_{}\) and \(I_{}\) respectively. We then evaluate the mean squared error (MSE) of the low-frequency components between \(I^{}\) and \(I_{}\) or \(I_{}\). The results show that \(I_{}\) closely resembles \(I^{}\), while \(I_{}\) does not. More details and results are shown in Appx. B.

**Selection of Interpolation Coefficients.** Interpolation methods [52; 12; 38] commonly select uniformly spaced coefficients \(w_{i}\) on the interpolation path. Yet an observation from Fig. 2 (b) shows that uniformly spaced points in the text-embedding space do not lead to uniformly spaced images with smooth transitions. Small visual transitions may occur over a large range of interpolation coefficients and vice versa, which we can show quantitatively by comparing the perceptual distances between adjacent pairs of uniformly spaced coefficients. This suggests that we should adopt non-uniform selection to ensure smoothness. More details and results are shown in the Appx. B.

## 4 AID: Attention Interpolation of Text-to-Image Diffusion

The diagnosis in Sec. 3.4 directly leads us to make the following proposals for improving conditional interpolation. First, we are motivated to extend attention interpolation beyond cross-attention to self-attention as well (Sec. 4.1) and propose fused attention. Secondly, our diagnosis of the smoothness motivates us to adopt a non-uniform selection of interpolation coefficients to encourage more even transitions (Sec. 4.2). Combining these two techniques, we propose a **AID**: Attention Interpolation of text-to-image Diffusion.

Finally, in an effort to give more precise control over the interpolation path, we introduce the use of prompt guidance for interpolation (Sec. 4.3). This further enhances AID as Prompt-guidance AID (PAID). The full pipeline is shown in Fig. 3.

### Fused Interpolated Attention Mechanism

The analysis in Sec. 3.4 highlights that both cross-attention and self-attention likely play a role in interpolating spatially consistent images. Proposition 1 can be generalized to self-attention where the keys and values are derived from the latent \(z\) instead of \(c\) for enhancing spatial constraint. As such, we define a general form of **inner-interpolated** attention on the keys and values as follows:

\[_{I}Q_{i},K_{1:m},V_{1:m};\ w_{i}= Q_{i},\ (1-w_{i})K_{1}+w_{i}K_{m},\ (1-w_{i})V_{1}+w_{i}V_{m},\] (8)

where \(Q_{i}\) is derived from \(z_{i}\). Note that Eq. 8 is equivalent to Eq. 4 if \(\{K_{1},K_{m}\}\) and \(\{V_{1},V_{m}\}\) are derived from \(\{c_{1},c_{m}\}\), i.e. as cross-attention; if they are derived from \(\{z_{1},z_{m}\}\), then it represents self-attention.

Instead of applying interpolation to the key and value, we can also interpolate the attention itself. We define this as **outer-interpolated attention**:

\[_{O}Q_{i},K_{1:m},V_{1:m};w_{i}=(1-w_{i}) Q_{i},K_{1},V_{1}+w_{i}Q_{i},K_{m},V_{m}.\] (9)

Figure 3: **An overview of PAID: Prompt-guided Attention Interpolation of Diffusion. The main components include: (1) Replacing both cross-attention and self-attention when generating interpolated image by fused interpolated attention; (2) Selecting interpolation coefficients with Beta prior; (3) Inject prompt guidance in the fused interpolated cross-attention.**

Similarly, Eq. 9 can represent both cross- and self-attention, depending on if \(\{K_{1},K_{m}\}\) and \(\{V_{1},V_{m}\}\) are derived from \(\{c_{1},c_{m}\}\) or \(\{z_{1},z_{m}\}\) respectively. More details on the differences between inner and outer interpolation are given in the Appx. C. We denote the two versions as AID-I and AID-O for inner and outer interpolation respectively.

While applying interpolation as defined in Eqs. 8 and 9 for self-attention does lead to high spatial consistency, it also results in poor fidelity images. This is likely because directly replacing the self-attention mechanism with some interpolated version is too aggressive. Therefore, for self-attention, we maintain the source keys and values \(K_{i}\) and \(V_{i}\) from the interpolated \(z_{i}\) and concatenate them with the interpolated keys and values, as shown in Fig. 3. Denoting concatenation as \([,]\), we define fused attention interpolation, leading to a **fused inner-interpolated attention**:

\[_{I}^{F}Q_{i},K_{1:m},V_{1:m};w_{i}= Q_{i},\ [(1-w_{i})K_{1}+w_{i}K_{m},\ K_{i}],\ (1-w_{i})V_{1}+w_{i}V_{m},\ V_{i}.\] (10)

For self-attention, as \(K_{i}\) is derived from \(z_{i}\), \(K_{i}(1-w_{i})K_{1}+w_{i}K_{m}\); the same holds for \(V_{i}\). For cross-attention, however, \(K_{i}=(1-w_{i})K_{1}+w_{i}K_{m}\), so fusing the two does not provide additional benefits. We note that there are opportunities for fusion with keys and values derived from other sources. We follow such a strategy in Sec. 4.3 to inject additional text-based guidance.

Analogous to Eq. 9, we define a **fused outer-interpolated attention**:

\[_{O}^{F}Q_{i},K_{1:m},V_{1:m};w_{i}=(1-w_{i})  Q_{i},[K_{1},K_{i}],[V_{1},V_{i}])\] (11) \[+w_{i} (Q_{i},[K_{m},K_{i}],[V_{m},V_{i}].\]

### Non-Uniform Interpolation Coefficients

The analysis in Sec. 3.4 shows that interpolation coefficients should not be selected uniformly adopted in previous methods [11; 16] on the interpolation path. For more flexibility, we apply a Beta distribution \(p_{B}(t,,)\). Beta distributions are conveniently defined within the range of \(\). When \(=1\) and \(=1\), \(p_{B}\) degenerates to a uniform distribution, which reverts to the original setting. When \(>1\) and \(>1\), the distribution is concave (bell-shaped), with higher probabilities away from the end-points of 0 and 1, i.e. away from the source images. Finally, the selected points are adjustable based on alpha and beta values, to give higher preference towards one or the other source image (see Fig. 3).

Given the Beta prior represented as cumulative distribution function \(F_{B}(w,,)\), we define a Beta-interpolation \(r_{B}(w;0,1)\) as \(r(F^{-1}(w,,))\), where \(w U(0,1)\). Therefore, the distributed point with Beta prior becomes:

\[\{r(0),r(F_{B}^{-1}(,,)),...,r(F_{B}^{-1}(,,)),r(1)\}.\] (12)

In practice, we employ a dynamic selection process to adjust the \(\) and \(\) parameters of the Beta prior, and form the smoothest sequence from the explored coefficients. Further details are provided in Appendix D.

### Prompt Guided Conditional Interpolation (PAID)

Given two source inputs, the hypothesis space of interpolation paths is actually large and diverse. Yet most interpolation methods [52; 38] estimate one deterministic path. Can we control or specify the interpolation path? One possibility is to provide a (third) conditioning text, which we refer to as a _guidance prompt_. To connect the interpolated sequence with the text in the guidance prompt \(g\), we fuse the associated key \(K_{g}=W_{K}^{T}g\) and value \(V_{g}=W_{V}^{}g\) instead of the original \(K_{i}\) and \(V_{i}\) in the fused inner-interpolated attention in Eq. 10 for cross-attention:

\[_{I}^{F}Q_{i},K_{1:m},V_{1:m};w_{i},K_{g},V_{g}=\] (13) \[Q_{i},\ (1-w_{i})K_{1}+w_{i}K_{m},\ K_{g} ,\ (1-w_{i})V_{1}+w_{i}V_{m},\ V_{g}.\]

In practice, the guidance prompt is provided by users to choose the interpolation path conditioned on the text description as Fig. 1 (f) shows. We demonstrate that the prompt-guided attention interpolation dramatically boosts the ability of compositional generation in Sec. 5.2.

## 5 Experiments

**Configuration and Settings.** We evaluate quantitatively based on the three measures for conditional interpolation defined in Sec. 3.3 and user studies. Detailed experimental and application configurations are given in Appxs. F and G.

We use Stable Diffusion 1.4  as the base model to implement our attention interpolation mechanism for quantitative evaluation. In all experiments, a \(512 512\) image is generated with the DDIM Scheduler  and DPM Scheduler  within 25 timesteps. Additional qualitative results using other state-of-the-art text-to-image diffusion models [30; 23; 2] are given in Appx. H.

### Conditional Interpolation

**Protocol, Datasets & Comparison Methods.** For experiments in each dataset, we run 5 trials each with \(N=100\) iterations. In each iteration, we randomly select two conditions and generate an interpolation sequence with size \(m=7\). We report the mean of each metric of the interpolation sequences over all trials as the final result. Our proposed framework is evaluated using corpora from CIFAR-10  and the LAION-Aesthetics dataset from the larger LAION-5B collection . To the best of our knowledge, the only related method is the text-embedding interpolation (TEI) [49; 53; 55] (see Sec. 3.2). We also compare with Denoising Interpolation (DI), which interpolates along the denoising schedule; more details in DI are given in the Appx. F.

**Results.** We quantitatively evaluate our methods based on the evaluation protocol as shown in Tab. 1. AID-O significantly increases the performance of all the evaluation metrics. AID-I achieves higher smoothness, AID-O has significant improvements in consistency (-20.3% on CIFAR-10 and -23.9% on LAION-Aesthetics) and fidelity (-66.62 on CIFAR-10 and -60.37 on LAION-Aesthetics). The fidelity of AID-I is poorer than AID-O and worse than Denoising Interpolation. However, AID-I achieves competitive qualitative results as shown by the user study.

**Ablation Study.** Tab. 1 shows ablations of the AID-O framework with CIFAR-10, focusing on three primary design elements: attention interpolation, self-attention fusion, and Beta-interpolation. Results show that attention interpolation improves consistency while Beta-interpolation contributes to improvements in smoothness and self-attention fusion to enhance image fidelity. While attention interpolation (without fusion with self-attention) with Beta-interpolation achieves the highest smoothness, it does so at the cost of fidelity. Similarly, AID without Beta interpolation achieves the strongest consistency but trades off smoothness (see Fig. 4). Fig. 4 (a) provides a qualitative comparison between different ablation settings.

**User Study.** Using Mechanical Turk, we check for human preferences on four types of text sources: 1) near objects, such as dogs and cats; 2) far objects, such as dragons and bananas; 3) scenes, such as waterfalls and sunsets; and 4) scene and object, such as a sea of flowers with a robot. This variety provides a comprehensive assessment for both concept and spatial interpolation. We conducted 320 trials in total; in each trial, an independent evaluator was asked to select their preferred interpolation result. Tab. 2 shows that our method is almost always preferred, though the preference is split across AID-I and AID-O depending on the type of text sources.

   Dataset & Method & Smoothness (\(\)) & Consistency (\(\)) & Fidelity (\(\)) \\   & TEI & 0.7531 & 0.3645 & 118.08 \\  & DI & 0.7564 & 0.4295 & 87.13 \\  & AID-O & 0.7831 & **0.2965** & **81.43*** \\  & AID-O & **0.7661** & 0.3707 & 101.13 \\   & TEI & 0.7426 & 0.3676 & 142.38 \\  & DI & 0.7511 & 0.4645 & 101.31 \\  & AID-O & 0.7643 & **0.394*** & **82.61*** \\  & AID-I & **0.8152** & 0.3787 & 129.41 \\  

Table 1: **Quantitative results of conditional interpolation. Quantitative results where the best performance is marked as (*) and the worst is marked as red. (a) Performance on CIFAR-10 and LAION-Aesthetics. AID-O and AID-I both show significant improvement over the Text Embedding Interpolation (TEI). Though Denoising Interpolation (DI) achieves relatively high fidelity, there is a trade-off with very bad performance on consistency (0.4295). AID-O boosts the performance in terms of consistency and fidelity while AID-I boosts the performance of smoothness; (b) Ablation studies on AID-O’s components, showcase that the Beta prior enhances smoothness, attention interpolation heightens consistency, and self-attention fusion significantly elevates fidelity.**

### Application

In this section, we firstly introduce how to adapt our methods into applications including image editing control and compositional generation. We further extend to cross-modality conditions including image prompt as well with IP-Adapter , which enables applications including image morphing and image-controlled generation. We provide more details in Appendix. G.

**Image Editing Control**. Text-based image editing tries to modify an image based on a textual description (see Fig. 5). Existing methods [16; 11; 48] rely on text embedding interpolation to control the editing level. Training-free methods [48; 11] struggle to control the editing level based on the text, while ours does not. We validate the control ability of our methods using Prompt-to-Prompt  (P2P) for synthesized image editing and EDICT  for real image editing.

We evaluate the ability to control the editing level using the smoothness metric defined in Sec. 3.3 using the image editing dataset presented in . Given an image with an editing level of 1 and the original image with an editing level of 0, we use either TEI or AID-O to interpolate edited images with levels ranging from \(\{,,...,\}\) and assess the smoothness of the edited image sequence.

Quantitative results are reported in Tab. 2 (b). Our method greatly improves the smoothness of the edited image sequence, aligning with different editing levels and thereby enhancing the control ability for editing. As shown in Fig. 5, P2P alone cannot effectively control the editing level but combining it with AID allows for precise level adjustments.

**Compositional Text-to-Image Generation**. Compositional generation is highly challenging for text-to-image diffusion models [25; 6; 7]. In our experiments, we focus on concept conjunction  - generating images that satisfy two given text conditions. For example, given the conditions "a robot" and "a sea of flowers," the goal is to generate an image that aligns with both "a robot" and "a sea of flowers."

For compositional generation, we use PAID to interpolate between conditions \(c_{1}\) and \(c_{2}\) with the prompt guidance "\(c_{1}\) AND \(c_{2}\)". For quantitative evaluation, we use the same dataset for human evaluation as in Sec. 5.1 and CLIP scores  to evaluate if the generated images align with both conditions. We compare our methods with vanilla Stable Diffusion [35; 30] and two other state-of-the-art training-free methods: Compositional Energy-based Model (CEBM)  and RRR .

Fig. 4 (b) shows that the CLIP score of our method is higher than previous methods for both Stable Diffusion 1.4  and SDXL . Moreover, our method produces fewer artifacts such as merging the two objects together, as illustrated in Fig. 6.

   Interpolation method & Near Object & Far Object & Scene & Object+Scene \\  TEI & 8.75\% & 1.16\% & 0\% & 1.26\% \\ AID-I & **53.75**\% & **50\%** & 45.2\% & 45.57\% \\ AID-O & 36.25\% & 46.5\% & **50\%** & **51.90\%** \\ Hard to determine & 1.25\% & 2.32\% & 4.76\% & 1.26\% \\   

Table 2: **Human evaluation results. (a): Human preference ratio of each method in different categories of interpolation, AID-I, and AID-O are dominantly preferred by TEI; (b): Smoothness of different editing methods, combined with AID boosts the control ability on the editing level.**

Figure 4: **Qualitative comparison of different ablation setting of AID. (a) Qualitative comparison between AID without fusion (\(1^{}\) row), AID with fusion (\(2^{}\) row), and AID with fusion and beta prior (\(3^{}\) row). Fusing interpolation with self-attention alleviates the artifacts of the interpolated image significantly, while beta prior increases smoothness based on AID with fusion. (b) CLIP score of different methods on composition generation.**

**Image Morphing and Image-Controlled Generation.** Image morphing finds transitions between two images, while image-controlled generation creates images based on a text prompt with an additional image prompt. To enable generation with image condition, we adapt AID on IP-Adapter . IP-Adapter integrates image embeddings into cross-attention layers, allowing diffusion models to incorporate image prompts. For morphing, we use an empty text prompt and apply AID for smooth interpolation between image conditions. In image-controlled generation, AID adjusts the image prompt scale across endpoints, enhancing control.

Our method enables effective image interpolation (Fig. 7 (a)) and offers finer control than IP-Adapter. As shown in Fig. 7 (b), AID maintains both text and image alignment, while in Fig. 7 (c), it better preserves identity while following compositional references. Further comparisons are provided in Appendix G.

## 6 Conclusion

In this work, we introduce a novel task: conditional interpolation within a diffusion model, along with its evaluation metrics, which include consistency, smoothness, and fidelity. We present a novel approach, referred to as AID and PAID, designed to produce interpolations between images under varying conditions. This method significantly surpasses the baseline in performance without training, as demonstrated through both qualitative and quantitative analysis. Our method is training-free and broadens the scope of generative model interpolation, paving the way for new opportunities in various applications, such as compositional generation and image editing control.

Figure 5: **Results of image editing control. Our method boosts the controlling ability over editing. The first row of (a) and (b) is generated by P2P + AID while the second row is P2P + TEI.**

Figure 6: **Results of compositional generation. Images on the left are generated with “a deer” and “a plane” based on SD 1.4  and images on the right are generated with “a robot” and “a sea of flowers” based on SDXL . Compared to other methods, PAID-O properly captures both conditions with higher fidelity.**

Figure 7: **Results of AID with image conditions. Our method is compatible with IP-Adapter for image-conditioned generation (a). In both global image prompt (b) and composition image prompt (c), from left to right the scale of additional image prompt slowly increases. The first row illustrates results controlled by AID, while the second row shows results achieved using the scale setting provided by IP-Adapter.**