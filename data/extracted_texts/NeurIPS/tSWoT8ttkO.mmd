# Efficient Recurrent Off-Policy RL Requires a Context-Encoder-Specific Learning Rate

Fan-Ming Luo\({}^{1,2}\)   Zuolin Tu\({}^{2}\)   Zefang Huang\({}^{1}\)   Yang Yu\({}^{1,2}\)

\({}^{1}\) National Key Laboratory for Novel Software Technology, Nanjing University, China

School of Artificial Intelligence, Nanjing University, China

\({}^{2}\) Polixir.ai

luofm@lamda.nju.edu.cn, zuolin.tu@polixir.ai, zf.frank.huang@gmail.com, yuy@nju.edu.cn

Yang Yu is the corresponding author.

###### Abstract

Real-world decision-making tasks are usually partially observable Markov decision processes (POMDPs), where the state is not fully observable. Recent progress has demonstrated that recurrent reinforcement learning (RL), which consists of a context encoder based on recurrent neural networks (RNNs) for unobservable state prediction and a multilayer perceptron (MLP) policy for decision making, can mitigate partial observability and serve as a robust baseline for POMDP tasks. However, prior recurrent RL algorithms have faced issues with training instability. In this paper, we find that this instability stems from the autoregressive nature of RNNs, which causes even small changes in RNN parameters to produce large output variations over long trajectories. Therefore, we propose **R**ecurrent Off-policy RL with Context-**E**ncoder-**S**pecific **L**earning Rate (RESeL) to tackle this issue. Specifically, RESeL uses a lower learning rate for context encoder than other MLP layers to ensure the stability of the former while maintaining the training efficiency of the latter. We integrate this technique into existing off-policy RL methods, resulting in the RESeL algorithm. We evaluated RESeL in 18 POMDP tasks, including classic, meta-RL, and credit assignment scenarios, as well as five MDP locomotion tasks. The experiments demonstrate significant improvements in training stability with RESeL. Comparative results show that RESeL achieves notable performance improvements over previous recurrent RL baselines in POMDP tasks, and is competitive with or even surpasses state-of-the-art methods in MDP tasks. Further ablation studies highlight the necessity of applying a distinct learning rate for the context encoder. Code is available at https://github.com/FanmingL/Recurrent-Offpolicy-RL.

## 1 Introduction

In many real-world reinforcement learning (RL) tasks , complete state observations are often unavailable due to limitations such as sensor constraints, cost considerations, or task-specific requirements. For example, visibility can be obstructed by obstacles in autonomous driving [2; 3] or robotic manipulation , and measuring ground friction may be infeasible when controlling quadruped robots on complex terrain . These scenarios are common and typically conceptualized as Partially Observable Markov Decision Processes (POMDPs). Traditional RL struggles with POMDPs due to the lack of essential state information .

A mainstream class of POMDP RL algorithms infers unobservable states by leveraging historical observation information, either explicitly or implicitly . This often requires memory-augmentednetwork architectures, such as recurrent neural networks (RNNs) [7; 8] and Transformers . Replacing standard networks in RL algorithms with these memory-based structures has proven effective in solving POMDP problems [10; 11; 12; 13]. Particularly, recurrent RL , which employs an RNN-based context encoder to extract unobservable hidden states and an MLP policy to make decisions based on both current observation and hidden states, demonstrates robust performance in POMDP tasks. Compared to Transformer-based RL , recurrent RL offers lower inference time complexity, making it highly applicable, especially in resource-constrained terminal controllers.

Despite these advantages, recurrent RL faces a significant challenge: while advanced RNN architectures [7; 8; 14] can effectively address gradient explosion issues, training in recurrent RL often remains more unstable compared to MLP-based RL, particularly when handling long sequence lengths. This instability can lead to poor policy performance and even training divergence . Although existing methods usually avoid training with full-length trajectories by using shorter trajectory segment , this practice introduces distribution shift due to the inconsistency between the sequence lengths used during training and deployment. ESCP  addressed this by truncating the history length during deployment to match the training sequence length, but this may limit policy performance due to restricted memory length. Additionally, some methods introduce auxiliary losses to aid context encoders in learning specific information. For instance, they train alongside a transition model, forcing the outputs of context encoder to help minimize the prediction error of this model [17; 18; 19]. However, these methods require the RNN to learn specific information, which may limit the RNN's potential and may only be applicable to a subset of tasks.

In this work, we found that, with the autoregressive property of RNNs, the output variations caused by parameter changes are amplified as the sequence length increases. As RNN parameters change, even slight variations in the RNN output and hidden state at the initial step can become magnified in subsequent steps. This occurs because the altered hidden state is fed back into the RNN at each step, causing cumulative output variations. Our theoretical analysis shows that these output variations grow with the sequence length and eventually converge. The amplified output variations can lead to instability in the RL process. For instance, in off-policy RL algorithms, the bootstrapped update target of the Q-function may fluctuate significantly, resulting in unstable Q-function training. To avoid training instability caused by excessive amplification of output variations, we propose **R**ecurrent Off-policy RL with Context-**E**ncoder-**S**pecific **L**earning Rate (RESeL). Specifically, we employ a lower learning rate for the RNN context encoder while keeping the learning rate for the other MLP layers unchanged. This strategy ensures efficient training for MLPs, which do not experience the issue of amplified output variations.

Based on the SAC framework , and incorporating context-encoder-specific learning rate as well as the ensemble-Q mechanism from REDQ  for training stabilization, we developed the practical RESeL algorithm. We empirically evaluated RESeL across 18 POMDP tasks, consisting of classic POMDP, meta-RL, credit assignment scenarios, as well as five MDP locomotion tasks. In our experiments, we first observed the increasing RNN output variations over time and demonstrated that the context-encoder-specific learning rate can mitigate the amplification issue, significantly enhancing the stability and efficacy of RL training. Comparative results indicate that RESeL achieves notable performance improvements over previous recurrent RL methods in POMDP tasks and is competitive with, or even surpasses, state-of-the-art (SOTA) RL methods in MDP tasks. Further ablation studies highlight the necessity of applying a distinct learning rate for the context encoder.

## 2 Background

**Partially Observable Markov Decision Processes** (POMDPs) [22; 6] enhance MDPs for scenarios with limited state visibility, addressing a number of real-world decision-making challenges with imperfect or uncertain data. A POMDP is defined by \(,,,P,O,r,,_{0}\), consisting of state space \(\), action space \(\), observation space \(\), state transition function \(P\), observation function \(O\), reward function \(r\), discount factor \(\), and initial state distribution \(_{0}\). Unlike MDPs, agents in POMDPs receive observations \(o\) instead of direct state information, requiring a belief state--a probability distribution over \(\)--to inform decisions. The objective is to search for a policy \(\) to maximize the expected rewards, while factoring in observation and transition uncertainties.

Figure 1: A simple recurrent policy architecture.

**Recurrent RL**[23; 11] involves employing RNN-based policy or critic models within the framework of RL. Figure 1 provides a commonly utilized recurrent policy architecture. This architecture has two main components. First, a context encoder processes the current observation \(o\), the last action \(\), the reward \(\)2, and the RNN hidden state. Then, an MLP policy uses the context embedding and \(o\) to generate actions, facilitating the extraction of non-observable states into a context embedding. We can train the recurrent policy by incorporating it into any existing RL algorithm. During training, recurrent RL requires batch sampling and loss computation on a trajectory-by-trajectory basis.

## 3 Related Work

In this work, we focus on recurrent RL methods [24; 25; 23], which leverage RNN-based models for RL tasks. These methods have demonstrated robust performance in POMDP [10; 22] and meta-RL tasks [25; 26], having shown notable success in practical applications such as robotic motion control [27; 28; 29] and MOBA games .

Recurrent RL utilizes RNNs such as GRUs , LSTMs , and more recently, SSMs  to construct policy and critic models. Typically, in recurrent actor-critic RL, both policy and critic adopt RNN architectures [23; 11]. When full state information is available during training, an MLP critic can also be used instead , receiving the full state as its input. In this work, we do not assume that complete state information can be accessed, so policy and critic models are both RNN structures.

The most direct way to train RNN models in recurrent RL is combining an RNN structure with existing RL algorithms, such as being integrated into on-policy methods like PPO [28; 30; 12; 31] and TRPO [25; 32] as well as off-policy methods like TD3  and SAC . This work follows the line of recurrent off-policy RL [11; 23], as we believe that existing recurrent off-policy RL algorithms have not yet achieved their maximum potential.

Despite the simplicity of direct integration, however, training RNNs often suffers from the training instability issue [12; 15], especially in long sequence length. Previous methods usually use truncated trajectory to train RNNs . However, deploying recurrent models with full-trajectory lengths can result in a mismatch between training and deployment. ESCP  addressed this issue by using the same sequence lengths in both training and deployment by truncating the historical length in deployment scenarios, but this could impair policy performance due to the restricted memory length. On the other hand, many studies also employed auxiliary losses in addition to the RL loss for RNN training . These losses aim to enable RNNs to stably extract some certain unobservable state either implicitly or explicitly. For instance, they might train the RNNs to predict transition parameters , distinguish between different tasks [16; 34], or accurately predict state transitions [17; 18; 19]. These methods require the RNN to learn specific information, which may limit the RNN's potential and may only be applicable to a subset of tasks, reducing the algorithmic generalizability.

RESeL directly trains recurrent models using off-policy RL due to its simplicity, flexibility, and potential high sample efficiency. The most relevant study to RESeL is , which focused on improving recurrent off-policy RL by analyzing the hyperparameters, network design, and algorithm choices. More in depth, RESeL studies the instability nature of previous methods and enhances the stability of recurrent off-policy RL through a specifically designed learning rate.

## 4 Method

In this section, we address the training stability challenges of recurrent RL. We will first elaborate on our model architecture in Sec. 4.1 and introduce how we address the stability issue in Sec. 4.2. Finally, we will summarize the training procedure of RESeL in Sec. 4.3.

### Model Architectures

The architectures of RESeL policy and critic models are depicted in Fig. 2. Specifically, RESeL policy initially employs MLPs as pre-encoders to map the current and last-step observations (\(o\) and \(\) respectively), actions (\(\)), and rewards (\(\)) into hidden spaces with equal dimensions, ensuring a balanced representation of various information types. These pre-encoded inputs are then concatenatedinto a vector and inputted into a context encoder. For the context encoder, RESeL utilizes RNN such as GRU  and Mampa , to mix sequential information and extract non-observable state information from the observable context. The MLP policy comprises a two-layer architecture, each layer consisting of 256 neurons. The architecture of the pre-encoder and context encoder for the RESeL critic mirrors that of the policy. To improve training stability, RESeL adopts the ensemble-Q technique from REDQ , employing 8 MLP critic models, and each one of them comprises two hidden layers with 256 neurons.

### Stabilizing Training with a Context-Encoder-Specific Learning Rate

To locate the source of instability, we first formalize the RNN and its sequence generation process. An RNN network \(f^{}(x_{t},h_{t}): \) is defined as: \(y_{t},h_{t+1}=f^{}(x_{t},h_{t})\), where \(\) represents the network parameters, \(x_{t}\), \(y_{t}\), and \(h_{t}\) are the RNN input, output, and hidden state, respectively. For simplicity, we denote \(y_{t}=f^{}_{y}(x_{t},h_{t})\) and \(h_{t+1}=f^{}_{h}(x_{t},h_{t})\). Given an input sequence \(\{x_{0},x_{1},,x_{T}\}\) of length \(T+1\) and an initial hidden state \(h_{0}\), the output sequence generated by \(f^{}\) is denoted as \(\{y_{0},y_{1},,y_{T}\}\). Let \(^{}\) be the parameter neighboring \(\) after a one-step gradient update. The output sequence produced by \(f^{^{}}\) is denoted as \(\{y^{}_{0},y^{}_{1},,y^{}_{T}\}\). With certain assumptions, we can derive Proposition 1, which bounds the difference between \(y_{t}\) and \(y^{}_{t}\) at any time step \(t\).

**Proposition 1**.: _Assuming \(f^{}\) and \(f^{^{}}\) both satisfy Lipschitz continuity, i.e., for all \(\{,^{}\}\), \(x\), \(h,h^{}\), there exist constants \(K_{h}[0,1)\) and \(K_{y}\) such that:_

\[\|f^{}_{h}(x,h)-f^{}_{h}(x,h^{})\|  K_{h}\|h-h^{}\|,\ \ \|f^{}_{y}(x,h)-f^{}_{y}(x,h^{})\|  K_{y}\|h-h^{}\|,\]

_and for all \(x\), \(h\), the output differences between the RNN parameterized by \(\) and \(^{}\) are bounded by a constant \(\):_

\[(\|f^{}_{h}(x,h)-f^{^{}}_{h}(x,h)\|, \|f^{}_{y}(x,h)-f^{^{}}_{y}(x,h)\|),\]

_the RNN output difference at \(t\)-th step is bounded by_

\[\|y_{t}-y^{}_{t}\|^{t}}{1- K_{h}}}_{}+,\ \  t 0.\] (1)

Refer to Appendix A for a detailed proof. Proposition 1 focuses on the case where \(K_{h}[0,1)\), as most RNN models adhere to this condition. GRU  and LSTM  meet this requirement through the use of sigmoid activation functions, while Mampa  achieves it by constraining the values in the hidden state's transition matrix.

In Eq. (1), \(\) represents the maximum variation of the model output when the model parameters are modified and both the model input and hidden state remain constant, i.e., the first-step output variation. However, from Eq. (1), we observe that due to the variability in the hidden state, the final variation in the model output is amplified. This amplification factor is minimal at \(t=0\), being \(0\). As \(t\) increases, this amplification factor gradually increases and ultimately converges to \(:=}{1-K_{h}}\). Additionally, it can be verified that as \(t\) increases, the upper bound of the average variation in network output \(_{i=0}^{t-1}\|y_{i}-y^{}_{i}\|\) also increases with time step \(t\) and eventually converges to \(+\) (proved

Figure 2: Policy and critic architectures of RESeL.

in Appendix B), which is again amplified by \(\). This conclusion indicates that, compared to the case of a sequence length of \(1\), in scenarios with longer sequence lengths, the average variations in the RNN output induced by gradient descent are amplified.

This issue is different from the gradient explosion that commonly occurs in RNNs over long sequences . Proposition 1 indicates that the variation in network output is amplified by a constant factor, rather than undergoing exponential explosion. This type of instability is not very significant in supervised learning but can lead to instability in RL processes. For instance, in off-policy RL algorithms, the optimization target of the Q-function minimizes the difference with a bootstrapped target. If the network output varies greatly, this target will fluctuate dramatically, making the Q-function training unstable, which in turn can lead to overall training instability.

To ensure stable training, it is required to use a smaller learning rate to counterbalance the amplified variations in network output caused by long sequence lengths, as a smaller learning rate can help reduce \(\). However, the output variations in MLPs are not amplified in the same way as in RNNs. If MLPs are also set with a small learning rate, their learning efficiency could be significantly reduced, as the learning rate may become too slow for MLPs, resulting in inefficient training and poor overall performance. Consequently, we propose to use a context-encoder-specific learning rate for the context encoder. In our implementation, we use a smaller learning rate \(_{}\) for the context encoder, which contains the RNN architecture. For other layers, we use a normal learning rate \(_{}\), e.g. \(3 10^{-4}\). Applying different learning rates to different modules is similar to Two-Timescale Network , which has been shown to improve the training convergence.

### Training Procedure of RESeL

Based on the SAC framework , combining context-encoder-specific learning rate and the ensemble-Q mechanism from REDQ , we developed the practical RESeL algorithm. Specifically, we initially configure context-encoder-specific optimizers for both the policy and value models. After each interaction with the environment, RESeL samples a batch of data from the replay buffer, containing several full-length trajectories. This batch is used to compute the critic loss according to REDQ, and the critic network is optimized using its context-encoder-specific optimizer. Notably, unlike REDQ, we did not adopt a update-to-data ratio greater than 1. Instead of that, we update the network model once per interaction. The policy network is updated every two critic updates. During the policy update, the policy loss from REDQ is used, and the policy network is optimized with its context-encoder-specific optimizer. Here, we delay the policy update by one step to allow more thoroughly training of the critic before updating the policy, which is inspired by TD3  to improve the training stability. The detailed algorithmic procedure is provided in Appendix C and Algorithm 1.

## 5 Experiments

To validate our argument and compare RESeL with other algorithms, we conducted a series of validation and comparison experiments across various POMDP environments. We primarily considered three types of environments: classical partially observable environments, meta-RL environments, and credit assignment environments. To assess the generalizablity of RESeL to MDP tasks, we also test RESeL in five MuJoCo  locomotion tasks with full observation provided. We implement the policy and critic models with a parallelizable RNN, i.e., Mamba  to accelerate the training process. A detailed introduction of the network architecture can be found in Appendix D.3.2.

In all experiments, the RESeL algorithm was repeated six times using different random seeds, i.e. \(1\)-\(6\). All experiments were conducted on a workstation equipped with an Intel Xeon Gold 5218R CPU, four NVIDIA RTX 4090 GPUs, and 250GB of RAM, running Ubuntu 20.04. For more detailed experimental settings, please refer to Appendix D.

### Training Stability

**The updates of the RNN lead to large variations in the model output.** In Sec. 4.2, we observe that the variations in model output induced by a model update are amplified. In this part, we quantify the amplification factor, assess its impact on RL training, and discover how a context-encoder-specific learning rate addresses this issue. We use a POMDP task, specifically WalkerBLT-V-v0, to validate our argument. We loaded a policy model trained by RESeL and updated it with various \(_{}\) and \(_{}\) for a one-step gradient update. The output differences for all rollout steps between the pre- and post-update models are presented in Fig. 3. The right panel shows the variations in model output as the rollout step increases, while the left panel zooms in on the rollout steps from 0 to 75.

In the zoom-in panel, comparing the brown-circle and yellow-triangle curves reveals that at the first rollout step, the two curves are very close. This indicates that with the same learning rate, identical input, and hidden states, the updates of RNN and MLP have almost the same impact on the model's output. However, as the rollout steps increase, the brown-circle curve gradually rises, increasing by 400% at the 20-th step. This observation aligns with Proposition 1, demonstrating that changes in the hidden state lead to increasing variations in the model's output. With further increases in rollout length, the right panel shows that the brown-circle curve converges at around 0.4 after 250 steps, indicating that the amplification of the output variations eventually converges. This also meets Proposition 1. Ultimately, the increase in action variations is approximately tenfold.

This action variation magnitude is equivalent to an MLP policy network (the gray-dashed curve) with a learning rate increased twentyfold. At this point, the learning rate reaches \(0.006\), which is typically avoided in RL scenarios due to the risk of training instability. Conversely, reducing \(_{}\) to \(10^{-5}\) (the purple-diamond curve) significantly suppresses the amplification of variations. The right panel shows that the orange and purple curves remain at similar levels until the final time step.

**Large output variations lead to instability in RL training.** To investigate how the large variations influence the RL process, we visualized the gradient norm and value function loss during training for two POMDP tasks, as shown in Figs. 4 and 5. We fixed \(_{}=3 10^{-4}\) and compared the training processes of \(_{}=10^{-5}\) (red line) and \(3 10^{-4}\) (orange line). For the latter, we applied gradient clipping; otherwise, training would diverge and stop early. The orange line in Fig. 4 shows the norm of the policy gradient before gradient rescaling, where significant oscillations appear in the gradient in the later stages of training. The orange gradient norm is eventually scaled to 0.5, consistently smaller than the red line, which does not show oscillations. This suggests that the late-stage instability of the orange line does not stem from large gradient magnitudes.

On the other hand, as seen in Fig. 5, the value loss for the orange line remains consistently high, with extremely large values appearing in later stages. This is due to large variations in the RNN output, which lead to an unstable bootstrapped update target for the value function. Consequently, the value loss diverges, causing the abnormal gradient norm observed in Fig. 4. The results in Figs. 4 and 5 reveal the unique challenges of using RNN structures in RL, where traditional RNN stabilization techniques may no longer be effective.

**Using a context-encoder-specific learning rate improves training stability, while traditional RNN stabilization techniques fall short.** To further examine the impact of a lower learning rate on the stability of RL training, we conducted experiments across several POMDP tasks. Building on the setup in Fig. 4, we further evaluated a variant without the gradient clipping technique and another that replaces gradient clipping with a truncation of recurrent backpropagation steps to 32 (green line). For reference, we also included a variant where the CE output is entirely masked to zero (purple line).

Comparing the vanilla RNN-based RL (blue line) with the purple line shows that introducing an RNN can partially address partial observability issues and improve policy performance. However, the blue line still exhibits instability, even triggering early stopping due to outliers. Setting a specific

Figure 3: Action variations as the rollout step increases after a single gradient update with different values of \(_{}\) and \(_{}\). Action variation refers to the change in policy output after the gradient update compared to its output before the update, using the same input sequences.

learning rate for the CE (red line) significantly enhances training stability, indicating that reducing the RNN learning rate indeed improves overall RL training stability. However, we also found that traditional RNN stabilization techniques do not reliably improve RL training stability. For example, truncating the number of recurrent backpropagation steps (green line) can still lead to early stopping, while clipping the gradient norm (orange line) prevents early stopping but does not improve the policy performance. This is because these two stabilization techniques can only suppress gradient explosions in the RNN, keeping the RNN gradients within a normal range. However, the output of the RNN can still vary significantly, which continues to cause instability in RL.

### Performance Comparisons

In this part, we compare RESeL with previous methods in various tasks. The detailed introduction of the baselines and tasks can be found in Appendix D.2 and Appendix D.1. More comparative results can be found in Appendix E.3.

**Classic POMDP Tasks.** We first compare RESeL with previous baselines in four PyBullet locomotion environments: AntBLT, HalfCheetahBLT, HopperBLT, and WalkerBLT. To create partially observable tasks, we obscure part of the state information as done in previous studies [18; 11; 15], preserving only position (-P tasks) or velocity (-V tasks) information from the original robot observations. We benchmark RESeL against prior model-free recurrent RL (MF-RNN) , VRM , and GPIDE . GPIDE, which extracts historical features inspired by the principle of PID  controller, is the state-of-the-art (SOTA) method for these tasks.

The comparative results are shown in Fig. 6. RESeL demonstrates significant improvements over previous recurrent RL methods (MF-RNN, PPO-GRU, and A2C-GRU) in almost all tasks, except for HalfCheetahBLT-P where its performance is close to that of MF-RNN. These results highlight the advantage of RESeL in classic POMDP tasks over previous recurrent RL methods. Furthermore, RESeL outperforms GPIDE in most tasks, establishing it as the new SOTA method. The learning curves of RESeL are more stable than that of GPIDE, suggesting that fine-grained feature design could also introduce training instability, while a stably trained RNN can even achieve superior performance.

**Dynamics-Randomized Tasks.** Instead of directly obscuring part of the immediate state, meta-RL considers a different type of POMDP. In meta-RL scenarios , the agent learns across various tasks with different dynamics or reward functions [16; 19], where the parameters of the dynamics and reward functions are not observable. We first compare RESeL with previous methods in the dynamics randomization meta-RL tasks. Following previous meta-RL work , we randomized the gravity in MuJoCo environments . We created 60 dynamics functions with different gravities, using the first 40 for training and the remaining for testing. The gravity is unobservable, requiring the agents to infer it from historical experience. We compare RESeL to various meta-RL methods in these tasks, including ProMP , ESCP , OSI , EPI , and PEARL . Notably, ESCP, EPI, and OSI use recurrent policies with different auxiliary losses for RNN training. ESCP and PEARL are previous SOTA methods for these tasks. The comparative results are shown in Fig. 8.

We find that in Ant, Humanoid, and Walker2d, RESeL demonstrates significant improvement over all other methods. In Hopper, RESeL performs on par with ESCP, while surpassing other methods. Specifically, RESeL outperforms SAC-RNN by a large margin, further highlighting its advantages. Moreover, ESCP, EPI, and OSI use RNNs to extract dynamics-related information, with ESCP having inferred embeddings highly related to the true environmental gravity. The advantages of RESeL over these methods suggest that the context encoder of RESeL may extract not only gravity (see Appendix E.1) but also other factors that help the agent achieve higher returns.

**Classic meta-RL Tasks.** We further compare RESeL with baselines in other meta-RL tasks, particularly the tasks possessing varying reward functions, sourced from [19; 11]. In AntDir, CheetahDir, and HalfCheetahVal, the robots' target moving direction or target velocity are changeable and unobservable. Agents must infer the desired moving direction or target velocity from their historical observations and rewards. Wind is a non-locomotion meta-RL task with altered dynamics. We compare RESeL to previous meta-RL methods, including RL\({}^{2}\), VariBad , and MF-RNN .

The learning curves of these methods are shown in Fig. 9. In the AntDir and HalfCheetahDir tasks, we found that RESeL can converge within \(5\)M steps. Our results show that RESeL demonstrates high sample efficiency and superior asymptotic performance in AntDir, CheetahDir, and HalfCheetahVal. In Wind, RESeL performs comparably to MF-RNN, as this task is less complex. These findings suggest that RESeL effectively generalizes to POMDP tasks with hidden reward functions.

Figure 8: Learning curves shaded with one standard error in dynamics-randomized tasks.

Figure 7: Learning curves shaded with one standard error in classic POMDP tasks.

**Credit Assignment Tasks.** A notable application of recurrent RL is solving credit assignment problem [11; 15]. We also compare RESeL with MF-GPT  and MF-RNN  in a credit assignment task, namely Key-to-Door. In this task, it is required to assign a reward obtained at the last-step to an early action. We compared RESeL with algorithms MF-GPT and MF-RNN. As did in , we test RESeL with credit assignment length of \(\). the hardness of the task grows as the length increases. In this task, the methods are evaluated by success rate. The results are shown in Fig. 10. The success rate of RESeL matches the previous SOTA MF-GPT in tasks with  lengths, closed to 100% success rate. For harder tasks with the lengths being , RESeL reached higher success rates than the others. The results indicate that RESeL outperforms Transformer-based methods in handling challenging credit assignment tasks.

**Classic MuJoCo Locomotion Tasks.** Finally, we would like to discover how RESeL performs in MDP tasks. We adopt five classic MuJoCo tasks and compare RESeL with previous RL methods, e.g., SAC , TD3 , and TD7 . As the current SOTA method for these environment, TD7 introduces several enhancements based on TD3, e.g., a representation learning method and a checkpoint trick, which are not existing in RESeL. The results, listed in Table 1, show that RESeL is comparable to TD7, showing only a 4.8% average performance drop. This demonstrates that RESeL is effective enough to nearly match the performance of the most advanced MDP algorithm. In Hopper and Walker, RESeL even surpasses TD7 by a significant margin. By comparing RESeL with SAC, we find RESeL is superior to SAC in all tasks and can improve SAC by \(34.2\%\) in average. These results indicate that RESeL can also be effectively extended to MDP tasks with notable improvement.

### Sensitivity and Ablation Studies

**Sensitivity Studies.** In this section, we analyze the impact of \(}\) and \(}\) with experiments on the WalkerBLT-V task. Initially, we fixed \(}=3 10^{-4}\) and varied \(}\). The final returns for different \(}\) values are shown in Fig. 10(a). The figure demonstrates that the highest model performance is achieved when \(}\) is between \(5 10^{-6}\) to \(10^{-5}\). Both larger and smaller \(}\) values result in decreased performance. Notably, an overly small \(}\) is preferable to an overly large one, as a small learning rate can slow down learning efficiency, whereas a large learning rate can destabilize training, leading to negative outcomes. Next, we fixed \(}=10^{-5}\) and varied \(}\). The resulting final returns are presented in Fig. 10(b). Here, the model achieves the highest score with \(}=3 10^{-4}\), while other values for \(}\) yield lower scores. Figs. 10(a) and 10(b) together indicate that the optimal values for \(}\) and \(}\) are not of the same order of

   & **TD3** & **SAC** & **TQC** & **TD3+OFE** & **TD7** & **RESeL (ours)** \\  HalfCheath-v2 & \(14337 1491\) & \(15526 697\) & \(17459 258\) & \(16596 164\) & \(\) & \(16750 432\) \\ Hopper-v2 & \(3682 83\) & \(3167 485\) & \(3462 818\) & \(3423 584\) & \(4075 225\) & \(\) \\ Walker2d-v2 & \(5078 343\) & \(5681 329\) & \(6137 194\) & \(6379 332\) & \(7397 454\) & \(\) \\ Ant-v2 & \(5589 758\) & \(4615 2022\) & \(6329 1510\) & \(8547 84\) & \(\) & \(8006 63\) \\ Humanoid-v2 & \(5433 245\) & \(6555 279\) & \(8361 1364\) & \(8951 246\) & \(\) & \(\) \\  Average & \(6824\) & \(7109\) & \(8350\) & \(8779\) & \(\) & \(9532\) \\  

Table 1: Average performance on the classic MuJoCo tasks at 5M time steps \(\) standard error.

Figure 10: Success rate in the key-to-door task with different credit assignment lengths.

Figure 9: Learning curves shaded with one standard error in meta-RL tasks.

magnitude. Additionally, it can also be found from Fig. (c)c that setting \(_{}\) equally significantly degrades policy performance. This emphasizes the importance of using different learning rates for context encoder and other layers. We extended the experiments from Figs. (a)a and (c)c to additional seven POMDP tasks, with results presented in Fig. 20 in Appendix E.6. The conclusions drawn from Fig. 20 are largely consistent with those from Figs. (a)a and (c)c, indicating that the impact of the context-encoder-specific learning rate on training is relatively generalizable.

**Ablation Studies.** We then explore whether the context-encoder-specific learning rate applies to other RNN or Transformer architectures . We integrated a GRU  or Transformer as the context encoder while keeping all other settings constant. Due to the high computational cost of training GRUs and Transformers on full-length trajectories, we tested the RESeL variant with these architectures on four classic POMDP tasks for 0.75M time steps. Figure 12 displays the learning curves for various variants. MF-RNN  and SAC-Transformer  serve as baselines based on GRUs and Transformers from prior studies.

We observe that the context-encoder-specific learning rate also improves and stabilizes the performance of the GRU and Transformer variants. Our findings show that both RESeL-GRU and RESeL-Transformer perform comparably to RESeL-Mamba on three out of four tasks. These results suggest that the performance gains of RESeL are not solely attributed to an advanced RNN architecture, and that different architectures may be suited to different tasks. Additional sensitivity experiments and ablation studies on context length can be found in Appendices E.4 to E.6.

## 6 Conclusions and Limitations

In this paper, we proposed RESeL, an efficient recurrent off-policy RL algorithm. RESeL tackles the training stability issue existing in previous recurrent RL methods. RESeL uses difference learning rates for the RNN context encoder and other fully-connected layers to improve the training stability of RNN and ensure the performance of MLP. Experiments in various POMDP benchmarks showed that RESeL can achieve or surpass previous SOTA across a wider variety of tasks, including classic POMDP tasks, meta-RL tasks, and credit assignment tasks.

**Limitations.** We also notice that there are several noticeable limitations concerning this work: (1) We have shown that RESeL can even surpass SOTA methods in some MDP tasks, but it is unclear how the RNN helps the policy improve the performance. (2) RESeL introduces one more hyper-parameter, i.e., \(_{}\). It is would be nice to explore the relationship between optimal \(_{}\) and \(_{}\) or automatically parameter-tuning strategy for \(_{}\).

Figure 11: Sensitivity studies of varied learning rates in terms of the average final return.

Figure 12: Learning curves shaded with 1 standard error with different RNN architectures.