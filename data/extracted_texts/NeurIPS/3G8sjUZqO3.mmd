# Multidimensional Fractional Programming for

Normalized Cuts

Yannan Chen\({}^{1}\)1  Beichen Huang\({}^{2}\)1  Licheng Zhao\({}^{3}\)  Kaiming Shen\({}^{1}\)2

\({}^{1}\)School of Science and Engineering, The Chinese University of Hong Kong (Shenzhen), China

\({}^{2}\)McMaster University, Canada

\({}^{3}\)Shenzhen Research Institute of Big Data, China

E-mail: yannanchen@link.cuhk.edu.cn, huangb21@mcmaster.ca,

zhaolicheng@sribd.cn, shenkaiming@cuhk.edu.cn

Equal contribution. Codes available at [https://github.com/zhanchendao/FPC.Corresponding](https://github.com/zhanchendao/FPC.Corresponding) author.

###### Abstract

The Normalized cut (NCut) problem is a fundamental and yet notoriously difficult one in the unsupervised clustering field. Because the NCut problem is fractionally structured, the fractional programming (FP) based approach has worked its way into a new frontier. However, the conventional FP techniques are insufficient: the classic Dinkelbach's transform can only deal with a single ratio and hence is limited to the two-class clustering, while the state-of-the-art quadratic transform accounts for multiple ratios but fails to convert the NCut problem to a tractable form. This work advocates a novel extension of the quadratic transform to the multidimensional ratio case, thereby recasting the fractional 0-1 NCut problem into a bipartite matching problem--which can be readily solved in an iterative manner. Furthermore, we explore the connection between the proposed multidimensional FP method and the minorization-maximization theory to verify the convergence.

## 1 Introduction

Fractional programming (FP) is a powerful optimization tool for solving diverse problems involving ratio terms, e.g., in the areas of physics, economics, management science, signal processing, computer science, and information theory . This paper explores a novel application of FP to the normalized cut (NCut)--which is a fundamental and yet notoriously difficult problem for unsupervised data clustering . A new FP technique called the _multidimensional quadratic transform_ forms the building block of this work. Differing from the classic Dinkelbach's transform  that is typically limited to the single-ratio problem with a pair of scalar-valued numerator and denominator, the multidimensional quadratic transform is capable of handling multiple ratios simultaneously in the same problem, and further accounts for the multidimensional-ratio case wherein the numerators and denominators take a matrix/vector form. It turns out that the NCut problem solving can be made much easier from a multidimensional FP point of view. Two main results have been achieved under the umbrella of FP. First, we show that one most recent advance  in the NCut field can be interpreted as a special scalar-ratio version of the multidimensional quadratic transform, which already outperforms the classic methods significantly. Second, by fully exploiting the multidimensional quadratic transform , we develop a superior FP-based algorithm tailored to the NCut problem.

Clustering has been considered extensively in the literature from a variety of perspectives, e.g., K-means , hierarchical clustering , spectral clustering (SC) , graph cuts , and high-density clustering . The graph cuts approach is of particular interest for its flexibility to cope with a wide range of cluster types, e.g., not requiring the desired clusters to be center-based as many othergeometry-based clustering algorithms do . With each data point mapped to a vertex in a weighted undirected graph, there are different ways to measure the relative strength of similarities between subgraphs (each corresponding to a cluster), which in turn lead to different classes of graph cuts algorithms, e.g., min cut , ratio cut , and min-max cut  aside from the NCut. Like many modern works in the realm of graph cuts, our study focuses on the NCut metric because it yields stable performance and prevents cluster imbalance .

Nevertheless, the optimization criterion of the NCut is numerically difficult to tackle. To be more specific, the NCut entails solving an NP-complete problem . The SC method constitutes a popular heuristic approach to the NCT problem , but it cannot provide any performance guarantee. Other works aim at the analytical aspect and rely heavily on the optimization theory. For example, the Fast Coordinate Descent (FCD) algorithm proposed in  evolves from the standard optimization tool of block coordinate descent. By contrast, the Direct Normalized Cut (DNC) algorithm in  is somewhat less straightforward. The main idea of  is to approximate the NCut problem by using a lower bound on the original optimization objective, but it incurs a costly inner iteration in computing such a lower bound. To remedy this, the Fast Iterative Normalized Cut (FINC) in  approximates the NCut problem based on a closed-form lower bound. However, the resulting new problem is still difficult to solve directly, which can only be addressed in a heuristic fashion as shown in . The present work is most closely related to DNC  and FINC  in the sense that it seeks to approximate the NCut problem via bounding as well. As compared to the above existing bounds, the new bound proposed in this work can be constructed immediately, and can further enable efficient solving of the new problem for the clustering purpose.

## 2 NCut problem statement

Suppose there are \(N\) data points in total. Use \(i,j\{1,2,,N\}\) to index these data points. For a pair of data points \(i\) and \(j\), the similarity between them is quantified as \(0 w_{ij} 1\). By symmetry, we have \(w_{ij}=w_{ji}\). In the graph theory context, with each data point visualized as a vertex, the edge between vertex \(i\) and vertex \(j\) is assigned the weight \(w_{ij}\) (or \(w_{ji}\)). Denote by \(\) the set of vertices, and \(\) the set of edges. The resulting graph \(G=(,)\) can be recognized as a weighted undirected graph. For each vertex \(i\), its _degree_\(d_{i}\) is the sum weights across all the incident edges:

\[d_{i}=_{j=1}^{N}w_{ij}. \]

Dividing the \(N\) data points into \(K>1\) clusters is equivalent to partitioning \(\) into \(K\) disjoint subsets \(\{_{1},_{2},,_{K}\}\), where \(_{k=1}^{K}_{k}=\) and \(_{k}_{k^{}}=\) for any \(k k^{}\). For any two disjoint subsets \(,\), we define

\[(,)=_{i}_{j}w_{ ij}, \]

which is illustrated in Figure 1.

Moreover, for any subset \(\), we define its _volume_ to be

\[()=_{i}d_{i}. \]

Figure 1: Graph cut between two disjoint subsets \(\) and \(\).

In principle, data clustering aims to group together those data points that are sufficiently similar to each other. Equivalently, we wish to minimize the similarity between any two clusters. Toward this end, one traditional strategy is to minimize

\[(_{1},_{2},,_{K})= _{k=1}^{K}(_{k},}_{k}), \]

where \(}_{k}\) is the complement of \(_{k}\), i.e., \(}_{k}=_{k}\). However, minimizing \((_{1},_{2},,_{K})\) alone can be problematic--it tends to put most data points in one particular cluster while leaving other clusters almost empty, namely the cluster imbalance . To resolve this issue, a natural idea is to regularize the cluster volume by considering the normalized cut:

\[(_{1},_{2},,_{K})=_{k=1}^{K}_{k},}_{k})}{(_{k})}. \]

Intuitively speaking, the value of \((_{k},}_{k})\) would soar if very few data points have been assigned to cluster \(k\), thereby discouraging the cluster imbalance.

We are now ready to formalize the NCut problem. The indicator variable \(x_{ik}\{0,1\}\) equals \(1\) if data point \(i\) is assigned to cluster \(k\), and equals \(0\) otherwise. Moreover, write \(=[w_{ij}]^{N N}\), \(=[d_{1},d_{2},,d_{N}]^{N N}\), and \(=[x_{ik}]\{0,1\}^{N K}\). Denote by \(_{k}\{0,1\}^{N}\) the \(k\)th column of \(\). It can be shown that

\[(_{1},_{2},,_{K})=_{k=1}^{K}_{k}^{}_{k}}{_{k}^{} _{k}}, \]

where the _graph Laplacian matrix_\(\) is given by \(=-\). We seek the optimal clustering decision \(\) that minimizes \((_{1},_{2},,_{K})\). Further, because \((_{1},_{2},,_{K})=K-_{k=1}^{K}_{k}^{}_{k}}{ _{k}^{}_{k}}\), the NCut minimization problem boils down to

\[}{} _{k=1}^{K}_{k}^{}_{k}}{_{k}^ {}_{k}}\] (7a) subject to \[_{k=1}^{K}x_{ik}=1, i=1,,n \] \[x_{ik}\{0,1\}, i=1,n,\ k=1,.K, \]

where the two constraints (7b) and (7c) state that each data point must be assigned to one unique cluster. The difficulties of the above problem can be recognized with two respects. First, the clustering variables \(\{x_{ik}\}\) are discrete. Second, even when every \(x_{ik}\) is relaxed to be a continuous variable on the interval \(\), the problem is still nonconvex.

## 3 Fractional programming

The NCut problem in (7) is fractionally structured. To be more specific, (7) takes a sum-of-ratios form. This quick observation strongly suggests that the NCut is amenable to FP, but it turns out that very few previous works in the literature have adopted the FP approach. In the rest of this section, we first review the conventional FP methods to show why they are rarely considered for the NCut, and then introduce a recently proposed FP technique called the multidimensional quadratic transform--which forms the building block of our proposed clustering algorithm as introduced in Section 4.

### Conventional FP methods

The early studies in the FP field are restricted to the _single-ratio_ problem:

\[}{}, \]where \(A(x) 0\) is a nonnegative function, \(B(x)>0\) is a strictly positive function, and \(\) is a nonempty constraint set on \(x\). In the literature, many works further assume that \(A(x)\) is concave in \(x\), \(B(x)\) is convex in \(x\), and \(\) is a convex set, namely the _concave-convex condition_. Notice that problem (8) is still nonconvex in general under the concave-convex condition, so the direct solving of (8) is difficult. The classic Dinkelbach's transform in essence aims to decouple the ratio:

**Proposition 1** (Dinkelbach's transform ): _The single-ratio problem (8) is equivalent to_

\[}{} A(x)-yB(x), \]

_where the auxiliary variable \(y\) is iteratively updated as \(y=A(x)/B(x)\)._

Observe that the new problem (9) is convex in \(x\) for fixed \(y\) under the concave-convex condition, and hence can be efficiently solved by the standard optimization method. Importantly, solving for \(x\) in (9) with \(y\) iteratively updated guarantees convergence to the global optimum of the original problem (8). However, it is difficult to extend Dinkelbach's transform to the multi-ratio problems (except for the max-min-ratios case ). As such, the use of Dinkelbach's transform in the NCut area is limited to the two-class clustering that only needs to optimize a single ratio .

We now consider \(K>1\) pairs of the numerator function \(A_{k}(x) 0\) and denominator function \(B_{k}(x)>0\) along with a nonempty constraint set \(\). A _sum-of-ratios_ problem is then formulated as

\[}{}_{k=1}^{K}(x)}{ B_{k}(x)}. \]

It is tempting to decouple each ratio \(A_{k}(x)/B_{k}(x)\) by using Dinkelbach's transform separately, but the resulting new problem is not equivalent to problem (10). Consequently, the classic Dinkelbach's transform does not work for the NCut with general \(K\) clusters. A valid method to decouple multiple ratios is presented in the following proposition.

**Proposition 2** (Quadratic transform ): _The sum-of-ratios problem (10) is equivalent to_

\[,\ y_{k}}{}_{k=1 }^{K}2y_{k}}(x)-y_{k}^{2}B_{k}(x), \]

_in the sense that \(x^{}\) is a solution to (10) if and only if \((x^{},y^{})\) is a solution to (11), where an auxiliary variable \(y_{k}\) is introduced for each ratio term \(A_{k}(x)/B_{k}(x)\)._

We propose optimizing \(x\) and \(\{y_{k}\}\) iteratively. When \(x\) is held fixed, each \(y_{k}\) can be optimally determined as

\[y_{k}^{}=(x)}}{B_{k}(x)}. \]

Furthermore, under a _generalized concave-convex condition_ wherein each \(A_{k}(x)\) is a concave function, each \(B_{k}(x)\) is a convex function, and \(\) is a convex set, it can be shown that the new problem (11) is convex in \(x\) when \(\{y_{k}\}\) are held fixed. Thus, the alternating optimization between \(x\) and \(\{y_{k}\}\) can be performed efficiently.

Now let us return to the NCut problem in (7) and apply the above FP technique to it. Treating \(_{k}^{}_{k}\) and \(_{k}^{}_{k}\) respectively as numerator and denominator, we can recast problem (7) into

\[,\ y_{k}}{}_ {k=1}^{K}(2y_{k}_{k}^{}_{k}}-y_{k}^{2}_ {k}^{}_{k}) \] \[,\ . \]

As before, we optimize \(\) and \(\{y_{k}\}\) iteratively. For fixed \(\), the optimal solution of \(y_{k}\) is

\[y_{k}^{}=_{k}^{}_{k}}}{_{k}^{} _{k}}. \]It remains to optimize \(\) in (13) for fixed \(\{y_{k}\}\). Observe that \(^{}\) is not a concave function of \(\) since \(\) is often a positive semi-definite matrix , and also that the constraint set of \(\) is not convex because of (7c), so the aforementioned generalized concave-convex condition does not hold here. As a result, solving for \(\) in (13) with \(\{y_{k}\}\) held fixed is no longer a convex problem. The above alternating optimization between \(\) and \(\{y_{k}\}\) can be recognized as the so-called _Fast Iterative Normalized Cut (FINC)_ algorithm of the recent work . Although it manages to decouple multiple ratios in the NCut problem, we are faced with a new challenging problem. The new problem is dealt with in a heuristic fashion in . This fact perhaps explains why the FP approach has not yet been considered extensively in the literature despite the fractional structure of the NCut problem.

### Multidimensional FP method

We now proceed to a much more sophisticated FP toolkit that accounts for multidimensional ratios. To start, consider the following matrix extension of the traditional scalar-valued FP problem: each \(A_{k}(x) 0\) is generalized as positive semi-definite \(_{k}(x)_{+}^{m m}\), while each \(B_{k}(x)>0\) is generalized as positive definite \(_{k}(x)_{++}^{m m}\). Accordingly, the ratio term is extended to the matrix form as

\[(x)}{B_{k}(x)}_{+}_{k}(x)^{-1}_{k }(x)_{+}^{m m}.\]

We then arrive at a matrix extension of the sum-of-ratios problem (10):

\[}{}_{k=1}^{K}(_{k}^{-1}(x)_{k}(x)). \]

One main result of this paper is that the quadratic transform in Proposition 2 carries over to the matrix ratio case, as stated in the following proposition.

**Proposition 3** (Multidimensional quadratic transform): _Suppose that each \(_{k}(x)_{+}^{m m}\) can be factorized as_

\[_{k}(x)=[_{k}(x)]^{}[_{k}(x)]\ \ _{k}(x)^{ m} \]

_for some positive integer \(\). The matrix FP problem (15) is then equivalent to_

\[,\ _{k}^{ m}}{} _{k=1}^{K}(2_{k}[_{k}(x)]^{}- _{k}_{k}(x)_{k}^{}), \]

where an auxiliary variable \(_{k}^{ m}\) is introduced for each matrix ratio \(_{k}^{-1}(x)_{k}(x)\).

**Proof 1**: _It can be shown that each \(_{k}\) in (17) is always optimally determined as_

\[_{k}^{}=_{k}(x)_{k}^{-1}(x). \]

_Substituting the above \(_{k}^{}\) in (17) recovers the original problem (15)._

**Proposition 4**: _The alternating optimization between \(x\) and \(\{_{k}\}\) in (17) amounts to an MM procedure, so it guarantees a nondecreasing convergence of the original optimization objective in (15), as specified in Appendix A.1._

The key step is to optimize \(x\) for fixed \(\{_{k}\}\). Recall that the primal variable \(x\) is still difficult to optimize for the NCut problem after applying the quadratic transform in Proposition 2. In contrast, it turns out that the multidimensional quadratic transform in Proposition 3 can lead us to an efficient iterative update of \(x\) for the NCut problem scenario, as elaborated in the next section.

## 4 Proposed Multidimensional-FP-based NCut

The goal of this section is to address problem (7) by means of the multidimensional FP. We begin with a special case in which the similarity matrix \(\) is assumed to be positive semi-definite; the indefinite \(\) case will be discussed later on.

It is crucial to notice that the numerator part \(_{k}^{}_{k}\) can be factorized as

\[_{k}^{}_{k}=_{k}^{}_{k} \ \ _{k}=^{}_{k}. \]

We remark that \(^{}\), i.e., the symmetric square root of \(\), is guaranteed to exist because we have assumed that \(_{}^{m m}\) for the current discussion. We then treat \(_{k}^{}_{k}\), \(_{k}\), and \(_{k}^{}_{k}\) as \(_{k}(x)\), \(_{k}(x)\), and \(_{k}(x)\), respectively, in Proposition 3, with \(m=1\) and \(=N\), thus using the multidimensional quadratic transform to reformulate the NCut problem (7) as

\[,\ _{k}^{N}}{} _{k=1}^{K}(2_{k}(^{}_{k})^{}-_{k}(_{k}^{}_{k})_{k }^{})\] (20a) subject to ( 7b ), ( 7c ). (20b)

We then optimize \(\) and \(\{_{k}\}\) iteratively. When \(\) is held fixed, each \(_{k}\) is optimally determined as

\[_{k}^{}=^{}_{k}}{_{k}^{} {D}_{k}}. \]

Now the core question is whether \(\) could be efficiently solved when \(\{_{k}\}\) are held fixed. The optimization objective of \(\) in (20a) for fixed \(\{_{k}\}\) is written as

\[h()=_{k=1}^{K}(2_{k}(^{ }_{k})^{}-_{k}(_{k}^{}_{k})_{k}^{ }). \]

It is critical to observe that under the discrete constraint \(x_{ik}\{0,1\}\) we must have

\[_{k}^{}_{k}=^{}_{k}=^{ }_{k}, \]

where \(=(1,1,,1)^{}\) is the all-ones vector and

\[=^{}=[d_{1},d_{2},,d_{N}]^{}. \]

We can then rewrite \(h()\) as

\[h()=_{k=1}^{K}(2_{k}^{}^{}_{k} -_{k}^{}_{k}^{}_{k})=_{k=1}^{K} _{k}^{}_{k}, \]

where

\[_{k}=2^{}_{k}-_{k}^{}_{k}. \]

In (25), the first equality follows since \(_{k}^{}_{k}\) is a scalar. Denote by \(_{ik}\) the \(i\)th component of \(_{k}\). In light of (25), we can readily maximize \(h()\) under the constraints (7b) and (7c): it is optimal to set \(x_{ik}\) with the largest \(_{ik}\) on each row of \(\) to one, while setting the rest \(x_{ik}\) of the row to zero, i.e.,

\[x_{ik}^{}=\{1&k=*{ arg\,max}_{k^{}}_{ik^{}}\\ 0&.. \]

If there exists a tie (i.e., when more than one cluster index \(k^{}\) maximizes \(_{ik^{}}\) for same \(i\)) then break it randomly. Further, with \(_{k}^{}\) in (21) plugged in (26), we obtain an efficient computation of \(_{k}\) as

\[_{k}=_{k}}{_{k}^{}_{k}}-_{k}^{}_{k}}{(_{k}^{}_{k })^{2}}=_{k}}{^{}_{k}}-_{k}^{}_{k}}{(^{}_{k})^{2}}. \]

The merits of rewriting \(_{k}\) as (28) are two-fold. First, it sidesteps the update of the auxiliary variables \(\{_{k}\}\). Second, it no longer entails computing the square root of \(\). The resulting algorithm referred to as _fractional programming-based clustering (FPC)_ is summarized in the following.

Clearly, the FPC algorithm is guaranteed to converge in terms of the new objective value \(h()\), since the iterative update of \(\) and \(\{_{k}\}\) in FPC amounts to a block coordinate ascent for problem (20) so that \(h()\) is monotonically increasing after each iteration. We can actually claim a stronger result for FPC according to Proposition 4, as stated in the subsequent proposition.

**Proposition 5**: _Not only the new objective value \(h()\) in (22) but also the original objective value of the NCut problem, \(f()=_{k=1}^{K}_{k}^{}_{k}}{_{k}^{ }_{k}}\), is nondecreasing after each iteration of FPC._

We thus far assume that the similarity matrix \(\) is positive semi-definite; this assumption can be justified by arguing that a positive definite kernel  (e.g., the Gaussian kernel) is often used to generate \(\). But what if some indefinite kernel has been adopted and hence \(\) is not necessarily positive semi-definite anymore? The following proposition provides a solution.

**Proposition 6**: _Suppose that the similarity matrix \(\) is indefinite. We can choose a sufficiently large \(>0\) so that the new matrix_

\[}=+ \]

_is positive semi-definite. Notice that such \(\) must exist since_

\[=-()}{_{i}d_{i}} \]

_is a feasible choice. Then we can equivalently consider problem (7) with \(}\) used in place of \(\), which can be readily addressed by the FPC algorithm._

**Proof 2**: _See Appendix A.2._

Finally, we examine the computational complexity of the FPC algorithm. The update of \(\) as in (27) incurs a computational complexity of \((K^{2}N)\), while the update of \(\{_{k}\}\) as in (28) incurs a computational complexity of \((KN^{2})\). If \(\) is indefinite, then we would further find the smallest eigenvalue of \(\) as required in (30). Rather than computing all the eigenvalues and then picking the smallest, which incurs \((N^{3})\), we propose a more efficient way of computing \(_{}\):

1. Find the largest eigenvalue of \(\|\|_{}-\), denoted as \(_{1}\), by the power method , where \(\|\|_{}\) is the Frobenius norm.
2. Compute the smallest eigenvalue as \(_{}()=\|\|_{}-_{1}\).

Thus, the overall complexity of finding \(\) as in (30) is \((N^{2})\). To sum up, the per-iteration complexity of FPC equals \((KN^{2})\), while the traditional SC algorithm incurs a complexity of \(O(N^{3})\).

## 5 Experiments

We validate the performance of the proposed FPC algorithm on 8 common datasets as summarized in Table 1. The benchmarks are the SC , FINC , and FCD . We use the Gaussian kernel to generate the similarity matrix, i.e., \(w_{ij}=(-\|_{i}-_{j}\|_{2}^{2})\), where \(_{i}\) and \(_{j}\) are the feature vectors of data points \(i\) and \(j\). All the tests were carried out on a desktop equipped with 2.10 GHz CPU\( 12\). Throughout the tables, we highlight the best performance by using the bold font.

### Optimization objective of NCut

We first evaluate the performance of the different algorithms in minimizing \((_{1},_{2},,_{K})\) as defined in (6). We run each algorithm 10 times with the random starting point generated for each trial, and then pick the best one. Table 2 summarizes the results. Observe that the proposed FPC method achieves the lowest NCut objective across all the datasets. For instance, the NCut objective of FPC is 0.19% lower than that of FCD for the dataset Office+Caltech10 with \(K=10\) clusters, and0.07% lower for the dataset Epileptic with \(N=11500\) data points. All the benchmarks except SC are strictly inferior to FPC; SC is equally good as FPC only on the dataset Rice.

Moreover, we consider first using SC to obtain a raw clustering decision and then using other algorithms to refine it. The test results are summarized in Table 3. Observe that using FPC after SC can achieve the best performance on all 8 datasets. In particular, it strictly improves upon the SC initialization on 6 datasets. It is worth observing that FINC may even yield worse performance after the initialization by SC; this is because FINC cannot guarantee that the new problem is optimally solved per iteration as formerly mentioned in Section 3.1. Finally, in Fig. 2 we find the global optimum for two small-size datasets via exhaustive search, and use it as the benchmark to compare with the proposed FPC algorithm; observe that FPC attains convergence to the global optimum after merely \(3\) iterates.

### Other performance metrics

Aside from the Ncut optimization criterion, the following commonly used performance metrics in practice are considered for the different clustering algorithms: the accuracy (ACC), the normalized mutual information (NMI) , and the adjusted random index (ARI) . Unlike \((_{1},_{2},,_{K})\), the above metrics are proportional to the performance, i.e., the higher metric value, the better clustering. The test results are summarized in Table 4. Although these performance metrics are not directly tied to the NCut objective, the proposed FPC method still achieves the highest scores in many cases.

   Dataset & \(N\) & \(K\) & Number of features & Source \\  Breast & 106 & 6 & 9 & UCI datasets \\ Thyroid & 215 & 3 & 5 & UCI datasets \\ Office+Caltech10 & 2533 & 10 & 800 & Github transfer-learning \\ Splice & 3175 & 3 & 240 & UCI datasets \\ Rice & 3810 & 2 & 7 & UCI datasets \\ Landsat & 6435 & 7 & 36 & UCI datasets \\ USPS & 9298 & 10 & 256 & LIBSVM \\ Epileptic & 11500 & 5 & 178 & UCI datasets \\   

Table 1: Datasets used for the task of dividing \(N\) data points into \(K\) clusters.

    & SC & FINC & FCD & FPC \\  Breast & 2.438695\(\)7.5e-5 & 2.442353\(\)4.1e-5 & 2.438695\(\)7.5e-5 & **2.437931\(\)2.8e-4** \\ Thyroid & **0.983144\(\)0.0** & 0.989329\(\)0.0 & **0.983144\(\)0.0** & **0.983144\(\)0.0** \\ Office+Caltech10 & 4.483945\(\)1.1e-5 & 4.483373\(\)6.0e-6 & 4.483635\(\)2.9e-5 & **4.483280\(\)9.0e-6** \\ Splice & 0.997651\(\)0.0 & 0.997651\(\)0.0 & 0.997651\(\)2.0e-5 & **0.997638\(\)0.0** \\ Rice & **0.499193\(\)0.0** & **0.499193\(\)0.0** & **0.499193\(\)0.0** & **0.499193\(\)0.0** \\ Landsat & 2.994678\(\)2.0e-6 & 2.994678\(\)2.0e-6 & 2.994499\(\)7.5e-5 & **2.994335\(\)0.0** \\ USPS & 4.476546\(\)1.7e-4 & 4.475926\(\)1.4e-4 & 4.475932\(\)1.4e-4 & **4.475911\(\)1.3e-4** \\ Epileptic & 1.992378\(\)3.0e-6 & 1.991756\(\)1.5e-5 & 1.991353\(\)0.0 & **1.991313\(\)0.0** \\   

* Note: Each entry has the form [objective value]\(\)[standard variance]. Red color indicates degradation while blue color indicates improvement.

Table 2: NCut objective values achieved by the different algorithms with random initialization.

    & SC & SC+FINC & SC+FCD & SC+FPC \\  Breast & 2.438695\(\)7.5e-5 & 2.442353\(\)4.1e-5 & 2.438695\(\)7.5e-5 & **2.437931\(\)2.8e-4** \\ Thyroid & **0.983144\(\)0.0** & 0.989329\(\)0.0 & **0.983144\(\)0.0** & **0.983144\(\)0.0** \\ Office+Caltech10 & 4.483945\(\)1.1e-5 & 4.483373\(\)6.0e-6 & 4.483635\(\)2.9e-5 & **4.483280\(\)9.0e-6** \\ Splice & 0.997651\(\)0.0 & 0.997651\(\)0.0 & 0.997651\(\)2.0e-5 & **0.997638\(\)0.0** \\ Rice & **0.499193\(\)0.0** & **0.499193\(\)0.0** & **0.499193\(\)0.0** & **0.499193\(\)0.0** \\ Landsat & 2.994678\(\)2.0e-6 & 2.994678\(\)2.0e-6 & 2.994499\(\)7.5e-5 & **2.994335\(\)0.0** \\ USPS & 4.476546\(\)1.7e-4 & 4.475926\(\)1.4e-4 & 4.475932\(\)1.4e-4 & **4.475911\(\)1.3e-4** \\ Epileptic & 1.992378\(\)3.0e-6 & 1.991756\(\)1.5e-5 & 1.991353\(\)0.0 & **1.991313\(\)0.0** \\   

Table 3: NCut objective values achieved by the different algorithms with the SC initialization.

We further try out the different clustering algorithms in the image segmentation task. Using the image dataset from , we perform the color histogram and the local binary pattern analysis for about \(500\) superpixels to extract the features as in . As shown in Figure 3, the clustering by FPC gives clearer boundaries of the objects than other methods.

Moreover, Figure 4 shows the average time consumption of the different algorithms. It can be seen that FPC runs \(73\%\) faster than FINC, and runs equally fast as SC. We remark that FCD requires the least running time because it tends to get trapped in a suboptimal point prematurely at the early stage.

## 6 Conclusion and limitation

This work proposes a novel application of multidimensional FP to the NCut clustering, differing from the previous works that rely on the traditional scalar-ratio FP such as Dinkelbach's transform and quadratic transform. The main merit of using multidimensional FP is that the new 0-1 problem can be efficiently solved via linear search. Further, the resulting FPC algorithm can be interpreted as an MM procedure with provable monotonic convergence in terms of the NCut optimization criterion. Thus far, we only show that its per-iteration complexity is lower than the overall complexity of the traditional SC algorithm.

    & SC & FINC & FCD & FPC & SC & FINC & FCD & FPC \\   Dataset &  &  \\  ACC & 0.4906 & 0.5000 & 0.3868 & **0.5283** & 0.8837 & **0.9163** & 0.7860 & 0.9070 \\ NMI & 0.4942 & 0.4798 & 0.3525 & **0.5052** & 0.4956 & **0.6061** & 0.3396 & 0.5780 \\ ARI & 0.2931 & 0.3104 & 0.1582 & **0.3379** & 0.6082 & **0.7167** & 0.4121 & 0.6869 \\  Dataset &  &  \\  ACC & 0.3533 & 0.3308 & 0.1717 & **0.3640** & 0.6646 & 0.3512 & 0.5096 & **0.7225** \\ NMI & 0.2347 & 0.2358 & 0.0704 & **0.2491** & 0.2856 & 0.0015 & 0.2149 & **0.3529** \\ ARI & 0.1440 & 0.1314 & 0.0440 & **0.1469** & 0.2686 & 0.0007 & 0.1650 & **0.3514** \\  Dataset &  &  \\  ACC & 0.8966 & 0.5142 & 0.8853 & **0.8992** & 0.6044 & 0.1584 & 0.5809 & **0.6611** \\ NMI & 0.5129 & 0.0006 & 0.4845 & **0.5216** & 0.4905 & 0.0013 & 0.4073 & **0.6111** \\ ARI & 0.6288 & 0.0005 & 0.5937 & **0.6371** & 0.3948 & 0.0000 & 0.3423 & **0.5328** \\  Dataset &  &  \\  ACC & 0.7037 & 0.6861 & **0.7060** & 0.6920 & 0.3609 & 0.3386 & **0.3743** & 0.3197 \\ NMI & **0.6419** & 0.6406 & 0.6333 & 0.6377 & 0.1627 & **0.2419** & 0.1909 & 0.2370 \\ ARI & 0.5724 & 0.5626 & **0.5734** & 0.5679 & 0.1119

## 7 Acknowledgments

The work of Yannan Chen, Beichen Huang, and Kaiming Shen was supported in part by Guangdong Major Project of Basic and Applied Basic Research (No. 2023B0303000001), in part by the National Natural Science Foundation of China (NSFC) under Grant 92167202, and in part by Shenzhen Steady Funding Program. The work of Licheng Zhao was supported in part by the NSFC under Grant 62206182, and in part by Guangdong Basic and Applied Basic Research Foundation under Grant 2024A1515010154.