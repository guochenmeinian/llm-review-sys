# How to Backdoor HyperNetwork in Personalized Federated Learning?

###### Abstract

This paper explores previously unknown backdoor risks in HyperNet-based personalized federated learning (HyperNetFL) through poisoning attacks. Based upon that, we propose a novel model transferring attack (called HNTroj), i.e., the first of its kind, to transfer a local backdoor infected model to all legitimate and personalized local models, which are generated by the HyperNetFL model, through consistent and effective malicious local gradients computed across all compromised clients in the whole training process. As a result, HNTroj reduces the number of compromised clients needed to successfully launch the attack without any observable signs of sudden shifts or degradation regarding model utility on legitimate data samples making our attack stealthy. To defend against HNTroj, we adapted several backdoor-resistant FL training algorithms into HyperNetFL. An extensive experiment that is carried out using several benchmark datasets shows that HNTroj significantly outperforms data poisoning and model replacement attacks and bypasses robust training algorithms even with modest numbers of compromised clients.

## 1 Introduction

Recent data privacy regulations  pose significant challenges for machine learning (ML) applications that collect sensitive user data. Federated learning (FL)  is a promising way to address these challenges by enabling clients to jointly train ML models via a coordinating server without sharing their data. Although offering better data privacy, FL typically suffers from the disparity of model performance caused by the non-independent and identically distributed (non-iid) data distribution across clients . One of the state-of-the-art approaches to address this problem in FL is using a single joint neural network, called HyperNetFL, to generate local models using personalized descriptors optimized for each client independently . This allows us to perform smart gradient and parameter sharing.

Despite the superior performance, the unique training approach of HyperNetFL poses previously unknown risks to backdoor attacks typically carried out through poisoning in FL . In backdoor attacks, an adversary manipulates the training process to cause model misclassification on a subset of chosen data samples [6; 7]. In FL, the adversary tries to construct malicious gradients or model updates that encode the backdoor. When aggregated with other clients' updates, the aggregated model exhibits the backdoor.

We investigate backdoor attacks against HyperNetFL and formulate robust HyperNetFL training as defenses. Our developed attack (called HNTroj) is based on consistently and effectively crafting malicious local gradients across compromised clients using a single backdoor-infected model to enforce HyperNetFL generating local backdoor-infected models disregarding their personalized descriptors. An extensive analysis and evaluation in non-iid settings show that HNTroj notably outperforms existing model replacement and data poisoning attacks bypassing recently developed robust federated training algorithms adapted to HyperNetFL with small numbers of compromised clients. Therefore, it is challenging to defend HNTroj.

Background

This section briefly reviews HyperNetFL and backdoor and poisoning attacks. More information is in Appx. A.

**HyperNetwork-based Personalized FL (HyperNetFL).** To address the disparity of model utility across clients, HyperNetFL  uses a neural network \(h(v_{i},)\) located at the server to output the weights \(_{i}\) for each client \(i\) using a (trainable) descriptor \(v_{i}\) as input and model weights \(\), that is, \(_{i}=h(v_{i},)\). HyperNetFL offers a natural way to share information across clients through the weights \(\) while maintaining the personalization of each client via the descriptor \(v_{i}\). To achieve this goal, the clients and the server will try to minimize their loss functions: \(*{arg\,min}_{,\{v_{i}\}_{i[N]}}_{i[N] }L_{i}(h(v_{i},))\).

**Backdoor and Poisoning Attacks.** Training time poisoning attacks against ML and FL models can be classified into byzantine and backdoor attacks. In byzantine attacks, the adversarial goal is to degrade or severely damage the model test accuracy . Byzantine attacks are relatively detectable by tracking the model accuracy on validation data . Meanwhile, in backdoor attacks, the adversarial goal is to cause model misclassification on a set of chosen inputs without affecting model accuracy on legitimate data samples. A well-known way to carry out backdoor attacks is using Trojans . A Trojan is a carefully crafted pattern, e.g., a brand logo, blank pixels, added into legitimate samples causing the desired misclassification. A recently developed image warping-based Trojan mildly deforms an image by applying a geometric transformation  to make it unnoticeable to humans and bypass all well-known Trojan detection methods, such as Neural Cleanse , Fine-Pruning , and STRIP . The adversary applies the Trojan on legitimate data samples to activate the backdoor at the inference time.

The training data is scattered across clients in FL, and the server only observes local gradients. Therefore, backdoor attacks are typically carried by a small set of compromised clients fully controlled by an adversary to construct malicious local gradients and send them to the server. The adversary can apply data poisoning (DPois) and model replacement approaches to create malicious local gradients. In DPois, compromised clients train their local models on Trojaned datasets to construct malicious local gradients, such that the aggregated model at the server exhibits the backdoor. DPois may take many training rounds to implant the backdoor into the aggregated model. Meanwhile, in model replacement , the adversary constructs malicious local gradients, such that the aggregated model at the server will closely approximate or be replaced by a predefined Trojaned model. To some extent, model replacement is highly severe since it can be effective after only one training round.

To our knowledge, _these attacks are not primarily designed for HyperNetFL, in which there is no aggregated model \(\) at the server_. That poses an unknown risk of backdoors through poisoning attacks in HyperNetFL.

## 3 Model Transferring Attack

To overcome the lack of consistency in deriving the malicious local gradients in DPois and avoid sudden shifts in model utility on legitimate data samples in HNRepl, we propose in this work a novel _model transferring attack_ (HNTroj) against HyperNetFL.

In HNroj (Alg. 1), our idea is to replace \(_{t}^{c}\) with a Trojaned model \(X\) across all the compromised clients \(c\) and in all communication rounds \(t\) to compute the malicious local gradients: \( c,t[T]:_{t}^{c}=_{t}^{c} X-h(v_{c},)\), where \(_{t}^{c}\) is a dynamic learning rate randomly sampled following a specific distribution, e.g., uniform distribution \([a,b]\), \(a<b\), and \(a,b(0,1]\). In practice, the adversary can collect its own data to locally train \(X\). The collected data can be uniformly distributed across all classes to maximize the backdoor successful rate across all legitimate clients, whose data distribution is non-iid. This is because the server does not know the clients' local data distribution.

By doing so, we achieve several key advantages, as follows:

**(1)** The gradients \(_{t}^{c}\) become more effective in creating backdoors, since \(X\) is a better optimized Trojaned model than \(\{_{t}^{c*}\}_{c}\).

**(2)** The gradients across compromised clients synergistically approximate the Trojaned surrogate loss (Eq. 9, Appx. B) to closely align the outputs of \(h(,)\) to a unified Trojan model \(X\) through the term \(_{c}\|X-h(v_{c},)\|_{2}^{2}\) disregarding the varying descriptors \(\{v_{i}\}_{i N}\) and their dissimilar local datasets. The new Trojaned surrogate loss is: \((_{c}\|X-h(v_{c},)\|_{2}^{2}+_{i N }\|^{i*}-h(v_{i},)\|_{2}^{2})\).

**(3)** The gradients \(_{t}^{c}\) become stealthier since updating the HyperNetFL with \(_{t}^{c}\) will significantly improve the model utility on legitimate data samples. This is because \(X\) has a better model utility on legitimate data samples than the local models of legitimate clients \(\{_{t}^{i*}\}_{i N}\). More importantly, _by keeping the random and dynamic learning rate \(_{t}^{c}\) only known to the compromised client \(c\)_, we can prevent the server from tracking \(X\) or identifying some suspicious behavior patterns from the compromised client.

We also quantify the server's estimation error \(Error\) as follows.

Assuming that the server can identify compromised clients with a precision value \(p\), we can quantify the server's estimation error bounds of the Trojaned model \(X\), as follows. The server's set of identified compromised clients consists of \(p\|\) compromised clients \(\) and \((1-p)\|\) legitimate clients \(\). The estimated Trojaned model \(X^{}=_{c,L}_{t}^{c}/||\). Then, the estimation error of \(X\) is computed and bounded as follows:

\[Error =\|_{c}^{c}}{p|C|}+_{i }^{i}}{(1-p)(N-|C|)}-X\|_{2}\] \[=\|X^{}-X\|_{2}=\|_{c,L} ^{c}}{||}-X\|_{2}\] (1)

in which

\[\|X^{}-X\|_{2}\|_{c}_{t}^{c}/(p| )-X\|_{2}=\|_{c}_{t}^{c}}{p| |_{t}^{c}}\|_{2}\|_{c}_{t }^{c}}{p||b}\|_{2}\] (2)

and

\[\|_{c,L}_{t}^{c}/||-X\|_{2}_{L  N|L|=||}\|_{i L}_{t}^{i}/|L|-X\|_{2}\] (3)

From Eqs. 2 and 3, we have the following error bounds:

\[_{c}_{t}^{c}}{p| |b}_{2} Error_{L N|L|=| |}_{i L}^{i}}{|L|}-X_{2}\] (4)We observe that: 1) The lower precision in detecting compromised clients (smaller \(p\)) results in a larger \(Error\) approaching the upper bound; 2) The smaller \(_{t}^{c}\), the higher lower bound of \(Error\) is; and 3) If the gradient \(_{t}^{c}\) is too small, we can uniformly upscale its \(L_{2}\)-norm to be a small constant (denoted \(\)) to enlarge the lower bound of \(Error\) without affecting the model utility or backdoor success rate. Fig. 1 illustrates the lower bound of \(Error\) given the most favorable precision to the server (\(p=1\)) with different values of \(}\). After 1,000 rounds, \(Error\) is not further reduced since it is controlled by the lower bound (with \(=2\) in this study). That prevents the server from accurately estimating \(X\).

**(4)** The following Theorem 1 shows that the \(L_{2}\)-norm distance between the local model \(_{t}^{c}\) of a compromised client generated by the HyperNetFL \(h(v_{c},)\) and \(X\), i.e., \(\|_{t}^{c}-X\|_{2}\), is bounded by \((-1)\|_{t^{}}^{c}\|_{2}+\|\|_{2}\), where \(t^{}\) is the closest round the compromised client \(c\) participated in before \(t\), and \(^{m}\) is a small error rate. When the HyperNetFL model converges, e.g., \(t^{},t T\), \(\|\|_{2}\) become tiny and \(\|_{t^{}}^{c}\|_{2}\) is bounded by a small constant \(\) ensuring that given the compromised client, \(_{T}^{c}=h(v_{c},)\) converges into a bounded and low loss area surrounding \(X\) (\(\|_{T}^{c}-X\|_{2}\) is tiny) to imitate the normal training process.

Consequently, HNTroj requires a smaller number of compromised clients to be highly effective. Also, HNTroj is stealthier than the (white-box) HNRepl and DPois by avoiding degradation and shifts in model utility on legitimate data samples during the whole poisoning process.

**Theorem 1**.: _For a compromised client \(c\) participating in a round \(t[T]\), we have that the \(L_{2}\)-norm distance between the HyperNetFL output \(_{t}^{c}=h(v_{c},)\) and the Trojaned model \(X\) is always bounded as follows:_

\[\|_{t}^{c}-X\|_{2}(1/a-1)\|_{t^{}} ^{c}\|_{2}+\|\|_{2}\] (5)

_where \( t:_{t}[a,b]\), \(a<b\), \(a,b(0,1]\), \(t^{}\) is the closest round the compromised client \(c\) participated in, and \(^{m}\) is a small error rate._

Proof.: At the round \(t^{}\), we have that \(_{t^{}}^{c}=_{t^{}}^{c}[X-_{t ^{}}^{c}]\). This is equivalent to \(X=_{t^{}}^{c}}{_{t^{}}^{c}}+ _{t^{}}^{c}\). At the round \(t\), the HyperNetFL \(h(v_{c},)\) supposes to generate a better local model for the compromised client \(c\): \(_{t}^{c}=_{t^{}}^{c}+_{t^{} }^{c}+\). To quantify the distance between the generated local model \(_{t}^{c}\) and the Trojaned model \(X\), we subtract \(_{t}^{c}\) by \(X\) as follows: \(_{t}^{c}-X=(1-}^{c}}) _{t^{}}^{c}+\). Based upon this, we can bound the \(l_{2}\)-norm of the distance \(_{t}^{c}-X\) as follows:

\[\|_{t}^{c}-X\|_{2}=\|(1-}^{c}}) _{t^{}}^{c}+\|_{2}(-1)\|_{t^{}}^{c}\|_{2}+\|\|_{2}\] (6)

Consequently, Theorem 1 holds. 

## 4 Experimental Results

We focus on answering the following three questions in our evaluation: **(1)** Whether HNTroj is effective in HyperNetFL? **(2)** What is the percentage of compromised clients required for an effective attack? and **(3)** How difficult it is to defend against HNTroj?

**Data and Model Configuration.** We conduct an extensive experiment on CIFAR-10  and Fashion MNIST datasets . For both datasets, we have \(100\) clients in which the data is non-iid

Figure 1: Estimation error of HNTroj with \(p=1\).

across clients and use the class \(0\) as a targeted class \(y_{j}^{b}\). We adopt the model configuration described in . We use WaNet  for generating backdoor data to train \(X\). More information is in Appx. D.

**Evaluation Metrics.** For evaluation, we use:

\[=_{i[N]}^{ }}_{j[n_{i}^{}]}Accf(x_{j}^{i},^{i}),y_{j}^{i}\] \[=_{i[N]}^{} }_{j[n_{j}^{}]}Accf(_{j}^{i},^{i}),y_{j}^{ i,b}\]

where \(_{j}^{i}=x_{j}^{i}+\) is a Trojaned sample, \(Acc(y^{},y)=1\) if \(y^{}=y\); otherwise \(Acc(y^{},y)=0\) and \(n_{i}^{}\) is the number of testing samples in client \(i\).

**HNTroj v.s. DPois and White-box HNRepl.** Figs. 2 and 10a (Appx. D) present legitimate ACC and backdoor SR of each attack and the clean model (i.e., trained without poisoning) as a function of the communication round \(t\) and the number of compromised clients \(\) under a defense free environment in the CIFAR-10 dataset. It is obvious that HNToj significantly outperforms DPois. HNToj requires a notably small number of compromised clients to successfully backdoor the HyperNetFL with high backdoor SR, i.e., \(43.92\%\), \(64.00\%\), \(70.38\%\), \(73.45\%\), and \(75.70\%\) compared with \(10.58\%\), \(17.87\%\), \(29.90\%\), \(41.53\%\), and \(50.57\%\) of the DPois given \(1\), \(5\), \(10\), \(15\), and \(20\) compromised clients, without an undue cost in legitimate ACC, i.e., \(76.89\%\).

In addition, HNToj does not introduce degradation or sudden shifts in legitimate ACC during the training process, regardless of the number of compromised clients, making it stealthier than DPois and (white-box) HNRepl. This is because we consistently poison the HyperNetFL with a relatively good model \(X\), which achieves \(74.26\%\) legitimate ACC and \(85.92\%\) backdoor SR, addressing the inconsistency in deriving the malicious local gradients. There is a small legitimate ACC gap between HNToj and the clean model, i.e., \(6.11\%\) in average. However, this gap will not be noticed by the server since the clean model is invisible to the server when the compromised clients are present.

**HNToj v.s. \(\)-Trimmed Norm.** Since HNToj outperforms other poisoning attacks, we now focus on understanding its performance under robust HyperNetFL training. Fig. 3 shows the performance of \(\)-trimmed norm against HNToj as a function of the number of compromised clients \(\). There

Figure 3: Legitimate ACC and backdoor SR under \(\)-trimmed norm defense in the CIFAR-10 dataset.

Figure 2: Legitimate ACC and Backdoor SR comparison for DPois, HNToj, and Clean model over different numbers of compromised clients in the CIFAR-10 dataset. (Fig. 2a has the same legend as in Fig. 2b).

are three key observations from the results, as follows: **(1)** Applying \(\)-trimmed norm does reduce the backdoor SR, especially when the number of compromised clients is small, i.e., backdoor SR drops \(39.72\%\) in average given \(\). However, when the number of compromised clients is a little bit larger, the backdoor SR is still at highly severe levels, i.e., \(51.70\% 70.55\%\) given \(15\) to \(20\) compromised clients, regardless of a wide range of trimming level \([0.1,0.4]\); **(2)** The larger the \(\) is, the lower the backdoor SR tends to be. This good result comes with a toll on the legitimate ACC, which is notably reduced when \(\) is larger. In average, the legitimate ACC drops from \(79.75\%\) to \(67.66\%\) and \(62.29\%\) given \([0.3,0.4]\) and \(\), respectively. That clearly highlights a non-trivial trade-off between legitimate ACC and backdoor SR given attacks and defenses; and **(3)** The more compromised clients we have, the better the legitimate ACC is when the trimming level \(\) is large, i.e., \([0.3,0.4]\). That is because training with the Trojaned model \(X\), which has a relatively good legitimate ACC, can mitigate the damage of large trimming levels on the legitimate ACC. In fact, a large number of compromised clients implies a better probability for the compromised clients to sneak through the trimming; thus, improving both legitimate ACC and backdoor SR.

We observe a similar phenomenon when we apply the client-level DP optimizer as a defense against HNTroj (Fig. 13). More information is in Appx. D.

**Backdoor SR at Client Level.** Importantly, the histogram of backdoor SR in the CIFAR-10 dataset under DP optimizer (Fig. 3(a)) shows that HNTroj with only 1 compromised clients can open backdoors to 10 (a decent number of) legitimate clients with high backdoor SRs (\(>0.4\)). We observe similar backdoor SRs to 22 legitimate clients with only 1 compromised client in FMNIST (Fig. 3(b)). Therefore, it is not easy to defend against HNTroj.

**Results on the Fashion MNIST dataset.** The results on the Fashion MNIST dataset further strengthen our observation. DPois even failed to implant backdoors into HyperNetFL (Figs. 9(b) and 10(), Appx. D). This is because the HyperNetFL model converges \(10\)x faster than the model for the CIFAR-10 dataset, i.e., given the simplicity of the Fashion MNIST dataset; thus, significantly reducing the poisoning probability through participating in the training of a small set of compromised clients. Technically, we found that the client-level DP optimizer appears having the potential to mitigate HNTroj due to the model's fast convergence. However, the backdoor SR at the client level shows that with only 1 compromised client, HNTroj can open backdoors to 22 (over 100) legitimate clients with SR \(>0.4\) (Fig. 4). Therefore, it is challenging for the client-level DP optimizer to defend HNTroj. In addition, the \(\)-trimmed norm is still failed to defend again HNTroj (Figs. 7, 12, and 10(), Appx. D).

## 5 Conclusion and Future Work

We presented a black-box model transferring attack (HNTroj) to implant backdoor into HyperNetFL. We overcome the lack of consistency in deriving malicious local gradients to efficiently transfer a Trojaned model to the outputs of the HyperNetFL. We multiply a random and dynamic learning rate to the malicious local gradients making the attack stealthy. To defend against HNTroj, we adapted several robust FL training algorithms into HyperNetFL. Extensive experiment results show that HNTroj outperforms black-box DPois and white-box HNRepl bypassing adapted robust training algorithms with small numbers of compromised clients. Future work is to 1) adapt HNTroj on other personalized FL frameworks and 2) use multiple Trojaned models adapting to diverse compromised clients.

Figure 4: Backdoor Success Rate at the Client Level.