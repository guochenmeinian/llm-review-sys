# When Demonstrations Meet Generative World Models:

A Maximum Likelihood Framework for Offline

Inverse Reinforcement Learning

 Siliang Zeng

University of Minnesota

Minneapolis, MN, USA

zeng0176@umn.edu

&Chenliang Li

Texas A&M University

College Station, TX, USA

chenliangli@tamu.edu

&Alfredo Garcia

Texas A&M University

College Station, TX, USA

alfredo.garcia@tamu.edu

&Mingyi Hong

University of Minnesota

Minneapolis, MN, USA

mhong@umn.edu

Equal Contribution.

Our implementation is available at https://github.com/Cloud0723/Offline-MLIRL

###### Abstract

Offline inverse reinforcement learning (Offline IRL) aims to recover the structure of rewards and environment dynamics that underlie observed actions in a fixed, finite set of demonstrations from an expert agent. Accurate models of expertise in executing a task has applications in safety-sensitive applications such as clinical decision making and autonomous driving. However, the structure of an expert's preferences implicit in observed actions is closely linked to the expert's model of the environment dynamics (i.e. the "world" model). Thus, inaccurate models of the world obtained from finite data with limited coverage could compound inaccuracy in estimated rewards. To address this issue, we propose a bi-level optimization formulation of the estimation task wherein the upper level is likelihood maximization based upon a _conservative_ model of the expert's policy (lower level). The policy model is conservative in that it maximizes reward subject to a penalty that is increasing in the uncertainty of the estimated model of the world. We propose a new algorithmic framework to solve the bi-level optimization problem formulation and provide statistical and computational guarantees of performance for the associated optimal reward estimator. Finally, we demonstrate that the proposed algorithm outperforms the state-of-the-art offline IRL and imitation learning benchmarks by a large margin, over the continuous control tasks in MuJoCo and different datasets in the D4RL benchmark2.

## 1 Introduction

Reinforcement learning (RL) is a powerful and promising approach for solving large-scale sequential decision-making problems [1; 2; 3]. However, RL struggles to scale to the real-world applications due to two major limitations: 1) it heavily relies on the manually defined reward function , 2) it requires the online interactions with the environment . In many application scenarios such as dialogue system  and robotics , it is difficult to manually design an appropriate reward for constructing the practical reinforcement learning system. Moreover, for some safety-sensitive applications like clinical decision making [8; 9] and autonomous driving [10; 11], online trials and errors are prohibiteddue to the safety concern. Due to these limitations in the practical applications, a new paradigm - learning from demonstrations, which relies on historical datasets of demonstrations to model the agent for solving sequential decision-making problems - becomes increasingly popular. In such a paradigm, it is important to understand the demonstrator and imitate the demonstrator's behavior by only utilizing the collected demonstration dataset itself, without further interactions with either the demonstrator or the environment.

In this context, offline inverse reinforcement learning (offline IRL) has become a promising candidate to enable learning from demonstrations [12; 13; 14; 9; 15]. Different from the setting of standard IRL [16; 17; 18; 19; 20; 21] which recovers the reward function and imitates expert behavior at the expense of extensive interactions with the environment, offline IRL is designed to get rid of the requirement in online environment interactions by only leveraging a finite dataset of demonstrations. While offline IRL holds great promises in practical applications, its study is still in an early stage, and many key challenges and open research questions remain to be addressed. For example, one central challenge in offline IRL arises from the so-called _distribution shift_[22; 15] - that is, the situation where the recovered reward function and recovered policy cannot generalize well to new unseen states and actions in the real environment. Moreover, in the training process of the offline IRL, any inaccuracies in the sequential decision-making process induced by distribution shift will compound, leading to poor performance of the estimated reward function / policy in the real-world environment. This is due to the fact that offline IRL is trained upon _fixed_ datasets, which only provide _limited_ coverage to the dynamics model of the real environment. Although there are some recent progress in offline IRL [14; 9; 15], how to alleviate distribution shift in offline IRL is still rarely studied and not clearly understood. Witnessing recent advances in a closely related area, the offline reinforcement learning, which incorporates conservative policy training to avoid overestimation of values in unseen states induced by the distribution shift [23; 24; 25; 26; 27], in this work we aim to propose effective offline IRL method to alleviate distribution shift and recover high-quality reward function from collected demonstration datasets. Due to the space limitations, we refer readers to Appendix B for more related work.

**Our Contributions.** To alleviate distribution shift and recover high-quality reward function from fixed demonstration datasets, we propose to incorporate conservatism into a model-based setting and consider offline IRL as a maximum likelihood estimation (MLE) problem. Overall, the goal is to recover a reward that generates an optimal policy to maximize the likelihood over observed expert demonstrations. Towards this end, we propose a two-stage procedure (see Fig. 1 for an overview). In the first stage, we estimate the dynamics model (the world model) from collected transition samples; by leveraging uncertainty estimation techniques to quantify the model uncertainty, we are able to construct a conservative Markov decision process (conservative MDP) where the state-action pairs with high model uncertainty and low data coverage receive a high penalty value to avoid risky exploration in the unfamiliar region. In the second stage,we propose an IRL algorithm to recover the reward function, whose corresponding optimal policy under the conservative MDP constructed in the first stage maximizes the likelihood of observed expert demonstrations. To the best of our knowledge, it is the first time that ML-based formulation, as well as the associated statistical and computational guarantees for reward recovery, has been developed for offline IRL.

To summarize, our main contributions are listed as follows:

\(\) We consider a formulation of offline IRL based on MLE over observed transition samples and expert trajectories. In the proposed formulation, we respectively model the transition dynamics and

Figure 1: Illustration of the modular structure in our algorithmic framework, Offline ML-IRL. In Offline ML-IRL, it first estimates a world model from the dataset of transition samples, and then implements an ML based offline IRL algorithm on the estimated world model to recover the ground-truth reward function from the collected expert trajectories.

the reward function as the maximum likelihood estimators to generate all observed transition samples and all collected expert demonstrations. We provide a statistical guarantee to ensure that the optimal reward function of the proposed formulation could be recovered as long as the collected dataset of transition samples has sufficient coverage on the expert-visited state-action space.

\(\) We develop a computationally efficient algorithm to solve the proposed formulation of offline IRL. To avoid repeatedly solving the policy optimization problem under each reward estimate, we propose an algorithm which alternates between one reward update step and one conservative policy improvement step. Under nonlinear parameterization for the reward function, we provide the theoretical analysis to show that the proposed algorithm converges to an approximate stationary point in finite time. Moreover, when the reward is linearly parameterized and there is sufficient coverage on the expert-visited state-action space to construct the estimated world model, we further show that the proposed algorithm approximately finds the optimal reward estimator of the MLE formulation.

\(\) We conduct extensive experiments by using robotic control tasks in MuJoCo and collected datasets in D4RL benchmark. We show that the proposed algorithm outperforms the state-of-the-art offline IRL such as [14; 15] and imitation learning methods such as , especially when the number of observed expert demonstrations is limited. Moreover, we transfer the recovered reward across different datasets to show that the proposed method can recover high-quality reward function from the expert demonstrations.

## 2 Preliminaries and problem formulation

**Markov decision process (MDP)** is defined by the tuple \((,,P,,r,)\), which consists of the state space \(\), the action space \(\), the transition dynamics \(P:\), the initial state distribution \(()\), the reward function \(r:\) and the discounted factor \((0,1)\). Under a transition dynamics model \(P\) and a policy \(\), we are able to further define the state-action visitation measure as \(d_{P}^{}(s,a):=(1-)(a|s)_{t=0}^{}^{t}P^{}(s_{t }=s|s_{0})\) for any state-action pair \((s,a)\).

**Maximum entropy inverse reinforcement learning (MaxEnt-IRL)** is a specific IRL formulation which aims to recover the ground-truth reward function and imitate the expert's policy from expert's demonstrations [18; 29; 20]. Let \(^{}:=\{(s_{t},a_{t})\}_{t=0}^{}\) denotes the expert trajectory sampled from the expert policy \(^{}\); let \(^{}\) denote the trajectory generated by the RL agent with policy \(\). Then the MaxEnt-IRL is formulated as:

\[_{r}\ _{}\ _{^{}^{ }}_{t=0}^{}^{t} r(s_{t},a_{t})- _{^{}}_{t=0}^{}^{t}  r(s_{t},a_{t})-H()}\] (1)

where \(H():=_{}_{t=0}^{}-^{t}( a_{t}|s_{t})\) denotes the causal entropy of the policy \(\). The MaxEnt-IRL formulation aims to recover the ground-truth reward function which assigns high rewards to the expert policy while assigning low rewards to any other policies. Although MaxEnt-IRL has been well-studied theoretically [29; 30; 21; 31] and has been applied to several practical applications [32; 33; 34], it needs to repeatedly solve policy optimization problems under each reward function and online interactions with the environment is inevitable. Such repeated policy optimization subroutine requires extensive online trials and errors in the environment and thus makes MaxEnt-IRL quite limited in practical applications.

**Problem formulation** Let us now consider an ML formulation of offline IRL. Given the transition dataset \(:=\{(s,a,s^{})\}\), we train an estimated world model \((s^{}|s,a)\) for any \(s^{},s\) and \(a\) (to be discussed in detail in Sec. 3). The constructed world model \(\) will be utilized as an estimate of the ground-truth dynamics model \(P\). Based on the estimated world model \(\), we propose a model-based offline approach for IRL from the ML perspective, given below:

\[_{} L():=_{^{}( ,^{},P)}_{t=0}^{}^{t}_{} (a_{t}|s_{t})\] (2a) \[s.t._{}:=_{}\ _{^{ }(,,)}_{t=0}^{}^{t} r(s_{t},a_{t};)+U(s_{t},a_{t})+(|s_{t }),\] (2b)

where \((|s):=_{a}-(a|s)( a|s)\) denotes the entropy of the distribution \((|s)\); \(U(,)\) is a penalty function to quantify the uncertainty of the estimated world model \((|s,a)\) under any state-action pair \((s,a)\). In practice, the penalty function is constructed based on uncertainty heuristics over an ensemble of estimated dynamics models [35; 25; 24; 36]. A comprehensive study of the choices of the penalty function can be found in . Next, let us make a few remarks about the above formulation, which we name _Offline ML-IRL_.

First, the problem takes a _bi-level_ optimization form, where the lower-level problem (2b) assumes that the parameterized reward function \(r(,;)\) is fixed, and it describes the optimal policy \(_{}\) as a unique solution to solve the conservative MDP; On the other hand, the upper-level problem (2a) optimizes the reward function \(r(,;)\) so that its corresponding optimal policy \(_{}\) maximizes the log-likelihood \(L()\) over observed expert trajectories.

Second, formulating the objective as a likelihood function is reasonable since it searches for an _optimal_ reward function to explain the observed expert behavior within limited knowledge about the world (the world model \(\) is constructed based on a finite and diverse dataset \(:=\{(s,a,s^{})\}\)).

Third, the lower-level problem (2b) corresponds to a model-based offline RL problem under the current reward estimate. The policy obtained is conservative in that state-action pairs that are not well covered by the dataset are penalized with a measure of uncertainty in the estimated world model. The penalty function \(U(s,a)\) is used to quantify the model uncertainty and regularize the reward estimator. Therefore, the optimal policy \(_{}\) under the conservative MDP will not take risky exploration on those _uncertain_ region of the state-action space where the transition dataset does not have sufficient coverage, and the constructed world model has high prediction uncertainty.

## 3 The world model and statistical guarantee

In this section, we construct the world model \(\) from the transition dataset \(:=\{(s,a,s^{})\}\) and solve a certain approximated version of the formulation (2). Then we further show a high-quality reward estimator can be obtained with statistical guarantee.

Before proceeding, let us emphasize that one major challenge in solving (2) comes from the dynamics model mismatch between (2a) and (2b), which arises because the expert trajectory \(^{}\) is generated from the ground-truth transition dynamics \(P\), while the agent samples its trajectory \(^{}\) through interacting with the estimated world model \(\). To better understand the above challenge, next we will explicitly analyze the likelihood objective in (2) and understand the mismatch error. Towards this end, let us introduce below the notions of the _soft Q-function_ and the _soft value function_ of the conservative MDP in (2) (defined for any reward parameter \(\) and the optimal policy \(_{}\)):

\[V_{}(s) :=_{(,_{},)} _{t=0}^{}^{t}r(s_{t},a_{t};)+U(s_{t},a_{t})+ (_{}(|s_{t}))s_{0}=s,\] (3a) \[Q_{}(s,a) :=r(s,a;)+U(s,a)+_{s^{}(|s,a)}V_{}(s^{}).\] (3b)

According to [38; 39; 40], the optimal policy \(_{}\) and the optimal soft value function \(V_{}\) have the following closed-form expressions under any state-action pair \((s,a)\):

\[_{}(a|s)=(s,a)}{_{}  Q_{}(s,)}, V_{}(s)=_{a } Q_{}(s,a).\] (4)

Under the ground-truth dynamics model \(P\) and the initial distribution \(()\), we further define the visitation measure \(d^{}(s,a)\) under the expert policy \(^{}\) as below:

\[d^{}(s,a):=(1-)^{}(a|s)_{t=0}^{}^{t}P ^{^{}}(s_{t}=s|s_{0}).\] (5)

By plugging the closed-form solution of the optimal policy \(_{}\) into the objective function (2a), we can decompose the dynamics-model mismatch error from the likelihood function \(L()\) in (2).

**Lemma 1**.: _Under any reward parameter \(\), the objective \(L()\) in (2a) can be decomposed as below:_

\[L()=()+_{(s_{t},a _{t}) d^{}(,)}_{s^{}}V_{ }(s^{})(s^{}|s,a)-P(s^{}|s,a) \] (6)_where \(()\) is a surrogate objective defined as:_

\[():=_{^{}(,^{ },P)}_{t=0}^{}^{t}r(s_{t},a_{t};) +U(s_{t},a_{t})-_{s_{0}()}V_{ }(s_{0}).\] (7)

The detailed proof is included in Appendix D. In Lemma 1, we have shown that the likelihood function in (2) decomposes into two parts: a surrogate objective \(()\) and an error term dependent on the dynamics model mismatch between \(\) and \(P\). As a remark, in the surrogate objective \(()\), we separate the two dynamics models (\(\) and \(P\)) into two relatively independent components. Therefore, optimizing the surrogate objective is computationally tractable and we will propose an efficient algorithm to recover the reward parameter from it in the next section.

To further elaborate the connection between the likelihood objective \(L()\) and the surrogate objective \(()\), we first introduce the following assumption:

**Assumption 1**.: _For any reward parameter \(\) and state-action pair \((s,a)\), following conditions hold:_

\[|r(s,a;)| C_{r},|U(s,a)| C_{u}\] (8)

_where \(C_{r}\) and \(C_{u}\) are positive constants._

As a remark, the assumption of the bounded reward is common in the literature of inverse reinforcement learning and imitation learning . Moreover, the assumption of the bounded penalty function holds true for common choices of the uncertainty heuristics , such as the max aleatoric penalty and the ensemble variance penalty. Then we can show the following results.

**Lemma 2**.: _Suppose Assumption 1 holds, then we obtain (where \(C_{v}\) is a positive constant):_

\[|L()-()|}{1-} _{(s,a) d^{}(,)}\|P(|s,a)- (|s,a)\|_{1}.\] (9)

Please see Appendix E for the detailed proof. The above lemma suggests that the gap between the likelihood function and its surrogate version is bounded by the model mismatch error \(_{(s,a) d^{}(,)}\|P(|s,a)- (|s,a)\|_{1}\). The fact that the objective approximation error \(|L()-()|\) depends on the model mismatch error evaluated in the expert-visited state-action distribution \(d^{}(,)\) is crucial to the construction of the world model \(\). Based on Lemma 2, we understand that full data coverage on the joint state-action space \(\) is _not_ necessary. Instead, as long as the collected transition dataset \(:=\{(s,a,s^{})\}\) provides sufficient coverage on the expert-visited state-action space \(:=\{(s,a)|d^{}(s,a)>0\}\), then the surrogate objective \(()\) will be an accurate approximation to the likelihood objective \(L()\).

Intuitively, considering the goal is to recover a reward function to model expert behaviors which only lie in a quite limited region of the whole state-action space, data collection with full coverage can be redundant. This result is very useful in practice, since it serves to greatly reduce the efforts on data collection for constructing the world model. Moreover, it also matches recent theoretical understanding on offline reinforcement learning  and offline imitation learning , which show that it is enough to learn a good policy from offline data with partial coverage.

To analyze the sample complexity in the construction of the world model \(\), we quantitatively analyze the approximation error between \(L()\) and \(()\). In discrete MDPs, the cardinalities of both state space and action space are finite (\(||<\) and \(||<\)). Therefore, based on a collected transition dataset \(=\{(s,a,s^{})\}\), we will use the empirical estimate to construct the world model \(\). Also recall that \(:=\{(s,a)|d^{}(s,a)>0\}\) denotes the set of expert-visited state-action pairs. Define \(^{}:=\{s|_{a}d^{}(s,a)>0\} \) as the set of expert-visited states. Using these definitions, we have the following result. The detailed proof is in Appendix H.

**Proposition 1**.: _For any \((0,2)\), suppose there are more than \(N\) data points on each state-action pair \((s,a)\), and the total number of the collected transition samples satisfies:_

\[\#|| N| ||^{}|}{^{2}}\]

_where \(c\) is a constant dependent on \(\). With probability greater than \(1-\), the following results hold:_

\[_{(s,a) d^{}(,)}\|P(|s,a)- (|s,a)\|_{1},|L()-()|}{1-}.\] (10)The above result estimates the total number of samples needed to construct the estimated world model \(\), so that the surrogate objective \(()\) can accurately approximate \(L()\). A direct implication is that, the reward parameter obtained by solving \(()\) also guarantees strong performance. To be more specific, define the optimal reward parameters associated with \(L()\) and \(()\) as below, respectively:

\[^{*}_{}\;L(),_{ }\;().\]

The next result characterizes the performance gap between the reward parameters \(\) and \(^{*}\). The detailed proof is included in Appendix I.

**Theorem 1**.: _For any \((0,}{1-})\), suppose there are more than \(N\) data points on each state-action pair \((s,a)\) and the number of transition dataset \(\) satisfies:_

\[\#|| N C_{v }^{2} c^{2}|||^{}|}{(1-)^{2} ^{2}}\]

_where \(c\) is a constant dependent on \(\). With probability greater than \(1-\), the following result holds:_

\[L(^{*})-L().\] (11)

## 4 Algorithm design

In the following sections, we will design a computationally efficient algorithm to optimize \(()\), and obtain its corresponding optimal reward parameter \(\).

From the definition (7), it is clear that \(()\) depends on the optimal soft value function \(V_{}()\) in (3a), which in turn depends on the optimal policy \(_{}\) as defined in (2b). Therefore, the surrogate objective maximization problem can be formulated as a bi-level optimization problem, expressed below, where the upper-level problem optimizes \(()\) to search for a good reward estimate \(\), while the lower-level problem solves the optimal policy \(_{}\) in a conservative MDP under the current reward estimate:

\[_{}\;(),_{}:= _{}\;_{^{}(,,)}_{t =0}^{}^{t}r(s_{t},a_{t};)+U(s_{t},a_{t})+ (|s_{t}).\] (12)

In order to avoid the computational burden from repeatedly solving the optimal policy \(_{}\) under each reward estimate \(\), we aim to design an algorithm which alternates between a policy optimization step and a reward update step. That is, at each iteration \(k\), based on the current policy estimate \(_{k}\) and the reward parameter \(_{k}\), two steps will be performed consecutively: _(1)_ the algorithm generates an updated policy \(_{k+1}\) through performing a conservative policy improvement step under the estimated world model \(\), and _(2)_ it obtains an updated reward parameter \(_{k+1}\) through taking a reward update step. Next, we describe the proposed algorithm in detail.

**Policy Improvement Step.** Under the reward parameter \(_{k}\), we consider generating a new policy \(_{k+1}\) towards approaching the optimal policy \(_{_{k}}\) as defined in (12). Similar to the definitions of \(V_{}\) and \(Q_{}\) in (3a) - (3b), under the current policy estimate \(_{k}\), the reward estimate \(r(,;_{k})\) and the estimated world model \(\), we define the corresponding soft value function as \(V_{k}()\) and the soft Q-function as \(Q_{k}(,)\). Please see (28a) - (28b) in Appendix for the precise definitions.

In order to perform a policy improvement step, we first approximate the soft Q-function by using an estimate \(_{k}(s,a)\), which satisfies the following: (where \(_{}>0\) is an approximation error)

\[\|_{k}-Q_{k}\|_{}:=_{s,a}| _{k}(s,a)-Q_{k}(s,a)|_{}.\] (13)

With the approximator \(_{k}\), an updated policy \(_{k+1}\) can be generated by a _soft policy iteration_:

\[_{k+1}(a|s)_{k}(s,a), s ,a.\] (14)

As a remark, in practice, one can follow the popular reinforcement learning algorithms such as soft Q-learning  and soft Actor-Critic (SAC)  to obtain accurately approximated soft Q-function with low approximation error \(_{}\) (as outlined in (13)), so to achieve stable updates for the soft policy iteration in (14). In the literature of the model-based offline reinforcement learning, the state-of-the-art methods [25; 37; 47] also build their framework upon the implementation of SAC.

**Reward Optimization Step.** At each iteration \(k\), given the current reward parameter \(_{k}\) and the updated policy \(_{k+1}\), we can update the reward parameter to \(_{k+1}\). First, let us compute the gradient of the surrogate objective \((_{k})\). Please see Appendix F for the detailed proof.

**Lemma 3**.: _The gradient of the surrogate objective \(()\) defined in (7), can be expressed as:_

\[()=_{^{}(,^{ },P)}_{t=0}^{}^{t}_{}r(s_{t},a_{ t};)-_{^{}(,_{},)} _{t=0}^{}^{t}_{}r(s_{t},a_{t};).\] (15)

In practice, we do not have access to the optimal policy \(_{}\). This is due to the fact that repeatedly solving the underlying offline policy optimization problem under each reward parameter is computationally intractable. Therefore, at each iteration \(k\), we construct an estimator of the exact gradient based on the current policy estimate \(_{k+1}\).

To be more specific, we take two approximation steps to develop a stochastic gradient estimator of \(()\): 1) choose one observed expert trajectory \(^{}_{k}\); 2) sample a trajectory \(^{}_{k}\) from the current policy estimate \(_{k+1}\) in the estimated world model \(\). Following these two approximation steps, the stochastic estimator \(g_{k}\) which approximates the exact gradient \((_{k})\) in (15) is defined as follows:

\[g_{k}:=h(_{k};^{}_{k})-h(_{k};^{ }_{k}),\] (16)

where \(h(;):=_{t=0}^{}^{t}_{}r(s_{t},a_{t};)\) denotes the cumulative reward gradient under a trajectory. Then we can update reward parameter according to the following update rule:

\[_{k+1}=_{k}+ g_{k}\] (17)

In Alg. 1, we summarize the proposed algorithm (named the Offline ML-IRL).

``` Input: Initialize reward parameter \(_{0}\) and policy \(_{0}\). Set the reward parameter's stepsize as \(\).  Train the world model \(\) on the transition dataset \(\).  Specify the penalty function \(U(,)\) based on \(\). for\(k=0,1,,K-1\)do  Policy Evaluation: Approximate the soft Q-function \(Q_{k}(,)\) by \(_{k}(,)\)  Policy Improvement:\(_{k+1}(|s)_{k}(s,),  s\) Data Sampling I: Sample an expert trajectory \(^{}_{k}:=\{s_{t},a_{t}\}_{t 0}\) Data Sampling II: Sample \(^{}_{k}:=\{s_{t},a_{t}\}_{t 0}\)  Estimating Gradient:\(g_{k}:=h(_{k};^{}_{k})-h(_{k};^{}_{ k})\) where \(h(;):=_{t 0}^{t}_{}r(s_{t},a_{t};)\) Reward Parameter Update:\(_{k+1}:=_{k}+ g_{k}\) endfor ```

**Algorithm 1**_A Model-based Approach for Offline Maximum Likelihood IRL (Offline ML-IRL)_

## 5 Convergence analysis

In this section, we present a theoretical analysis to show the finite-time convergence of Alg.1.

Before starting the analysis, let us point out the key challenges in analyzing the Alg. 1. Note that the algorithm relies on the updated policy \(_{k+1}\) to approximate the optimal policy \(_{_{k}}\) at each iteration \(k\). This coarse approximation can potentially lead to the distribution mismatch between the gradient estimator \(g_{k}\) in (16) and the exact gradient \((_{k})\) in (15). To maintain the stability of the proposed algorithm, we can use a relatively small stepsize \(\) to ensure the policy estimates are updated in a faster time-scale compared with the reward parameter \(\). This allows the policy estimates \(\{_{k+1}\}_{k 0}\) to closely track the optimal solutions \(\{_{_{k}}\}_{k 0}\) in the long run.

To proceed, let us introduce a few assumptions.

**Assumption 2** (Ergodic Dynamics).: _Given any policy \(\), the Markov chain under the estimated world model \(\) is irreducible and aperiodic. There exist constants \(>0\) and \((0,1)\) to ensure:_

\[_{s}\ \|(s_{t}|s_{0}=s,)-_{ }^{}()\|_{}^{t},\ t 0\]

_where \(\|\|_{}\) denotes the total variation (TV) norm; \(_{}^{}\) is the stationary distribution of visited states under the policy \(\) and the world model \(\)._

The assumption about the ergodic dynamics is common in the literature of reinforcement learning , which ensures the Markov chain mixes at a geometric rate.

**Assumption 3** (Lipschitz Reward).: _Under any reward parameter \(\), the following conditions hold for any \(s\) and \(a\):_

\[_{}r(s,a;) L_{r},_{ }r(s,a;_{1})-_{}r(s,a;_{2}) L_{g} \|_{1}-_{2}\|,\] (18)

_where \(L_{r}\) and \(L_{g}\) are positive constants._

According to Assumption 3, the parameterized reward has bounded gradient and is Lipschitz smooth. This assumption is common for min-max / bi-level optimization problems .

Based on Assumptions 2 - 3, we show that certain Lipschitz properties of the optimal soft Q-function and the surrogate objective hold. Please see the detailed proof in Appendix G.

**Lemma 4**.: _Suppose Assumptions 2 - 3 hold. Under any reward parameter \(_{1}\) and \(_{2}\), the optimal soft Q-function and the surrogate objective satisfy the following Lipschitz properties:_

\[|Q_{_{1}}(s,a)-Q_{_{2}}(s,a)| L_{q}\|_{1}- _{2}\|,\  s,a\] (19a) \[\|(_{1})-(_{2})\|  L_{c}\|_{1}-_{2}\|\] (19b)

_where \(L_{q}\) and \(L_{c}\) are positive constants._

Our main convergence result is summarized in the following theorem, which characterizes the convergence speed of the estimates \(\{_{k+1}\}_{k 0}\) and \(\{_{k}\}_{k 0}\). The detailed proof is in Appendix J.

**Theorem 2** (Convergence Analysis).: _Suppose Assumptions 2 - 3 hold. Let \(K\) denote the total number of iterations to be run in Alg. 1. Setting the stepsize as \(=_{0} K^{-}\) where \(_{0}>0\), we obtain the following convergence results:_

\[_{k=0}^{K-1}_{k+1} -_{_{k}}_{}=(K^{-})+ (_{})\] (20a) \[_{k=0}^{K-1}\|( _{k})\|^{2}=(K^{-})+(_ {})\] (20b)

_where we have defined \(\|_{k+1}-_{_{k}}\|_{}:=_{s,a }_{k+1}(a|s)-_{_{k}}(a|s)\) and \(_{}\) is the approximation error defined in (13)._

In Theorem 2, we demonstrated that Alg.1 identifies the approximate stationary solution of the surrogate problem (12) in finite time. Next, we show that when the reward is parameterized _linearly_, an improved result can be obtained, where the stationary solutions of problem (12) can be connected to the optimal solutions of the offline-IRL problem (12). See Appendix K for detailed proof.

**Theorem 3** (Optimality Guarantee).: _Assume that the reward function is linearly parameterized, i.e., \(r(s,a;):=(s,a)^{}\) where \((s,a)\) is the feature vector of the state-action pair \((s,a)\). Then any stationary point of the surrogate problem (12) is a global optimum. Furthermore, for any \((0,}{1-})\), suppose there are more than \(N\) data points on each state-action pair \((s,a)\) and the number of transition dataset \(\) satisfies:_

\[\#|| N C_{v}^{ 2} c^{2}|||^{}|}{(1-)^{2} ^{2}}\]

_where \(c\) is a constant dependent on \(\). With probability greater than \(1-\), any stationary point \(\) of the surrogate objective \(()\) is an epsilon-optimal solution to the maximum likelihood problem (2):_

\[L(^{*})-L()\] (21)

_where \(^{*}\) is defined as the optimal reward parameter of the log-likelihood objective \(L()\)._As a remark, the epsilon-optimal solution on the MLE problem implies:

\[L(^{*})-L()=_{s d^{ }(),a^{}(|s)} }(a|s)}{_{}(a|s)}.\]

When the expert trajectories are consistent with the optimal policy under a ground truth reward parameter \(^{*}\), we have \(^{}=_{^{*}}\). Due to this property, we can obtain that

\[L(^{*})-L()=_{s d^{ }(),a^{}(|s)}}(a|s)}{_{}(a|s)}= _{s d^{}()}D_{KL}^{}( |s)\|_{}(|s).\]

Hence, Theorem 3 also provides a formal guarantee that the recovered policy \(_{}\) is \(\)-close to the expert policy \(^{}\) measured by the KL divergence.

We remark that even under the linear parameterization assumption, showing the optimality of the stationary solution for the surrogate problem (12) is still non-trivial, since the problem is still not a concave problem with respect to \(\). In our analysis, we translate this problem to a certain saddle point problem. Under linear reward parameterization, we show that any stationary solution \(\) of the surrogate problem (12) together with the corresponding optimal policy \(_{}\) consist of a saddle point to the saddle point problem. By further leveraging the property of the saddle point, we show the optimality of the stationary solution for the surrogate problem. Finally, by utilizing the statistical guarantee in Theorem 1, we obtain the performance guarantee for any stationary point \(\) in (21).

## 6 Numerical results

In this section, we present numerical results for the proposed algorithm. More specifically, we intend to address the following questions: 1) How does the proposed algorithm compare with other state-of-the-art methods? 2) Whether offline ML-IRL can recover a high-quality reward estimator of the expert, in the sense that the reward can be used across different datasets or environment?

We compare the proposed method with several benchmarks. Two classes of the existing algorithms are considered as baselines: 1) state-of-the-art offline IRL algorithms which are designed to recover the ground-truth reward and expert policy from demonstrations, including a model-based approach CLARE  and a model-free approach IQ-Learn ; 2) imitation learning algorithms which only learn a policy to mimic the expert beahviors, including behavior cloning (BC) and ValueDICE .

We test the performance of the proposed Offline ML-IRL on a diverse collection of robotics training tasks in MuJoCo simulator , as well as datasets in D4RL benchmark , which include three environments (halfcheetah, hopper and walker2d) and three dataset types (medium-replay, medium, and medium-expert). In each experiment set, both environment interactions and the ground-truth reward are not accessible. Moreover, we train the algorithm until convergence and record the average reward of the episodes over \(6\) random seeds.

In Offline ML-IRL, we estimate the dynamics model by neural networks which model the location of the next state by Gaussian distributions. Here, we independently train an ensemble of \(N\) estimated world model \(\{_{,}^{i}(s_{t+1}|s_{t},a_{t})=(_{}^{i} (s_{t},a_{t}),_{}^{i}(s_{t},a_{t}))\}_{i=1}^{N}\) via likelihood maximization over transition samples. Then we can quantify the model uncertainty and construct the penalty function. For example, in , the electronic uncertainty is considered and the penalty function is constructed as \(U(s,a)=-_{i=1, N}\|_{}^{i}(s,a))\|_{}\). For the offline RL subroutine in (13) - (14), we follow the implementation of MOPO . We follow the same setup in  to select the key hyperparameters for MOPO. We parameterize the reward function by a three-layer neural network. The reward parameter and the policy are updated alternatingly, where the former is updated according to (17), while the latter is optimized according to MOPO. More experiment details are in Appendix A.

In Figure 2 and Table 1, we show the performance comparison between the proposed algorithm and benchmarks. The results of IQ-Learn is not presented since it suffers from unstable performance. To obtain better performance in BC and ValueDICE, we only use the expert demonstrations to train their agents. From the results, it is clear that the proposed Offline ML-IRL outperforms the benchmark algorithms by a large margin in most cases. In the experiment sets where Offline ML-IRL does not obtain the best performance, we notice the performance gap is small compared with the leading benchmark algorithm. Moreover, among the three dataset types (medium-replay, medium and medium-expert), medium-expert dataset has the most complete coverage on the expert-visitedstate-action space. As a result, Offline ML-IRL can nearly match the expert performance when the world model is trained from the medium-expert transition dataset. This numerical result matches our theoretical understanding in Theorem 1. As a remark, in most cases, we notice that Offline ML-IRL outperforms CLARE by a large margin which is also a model-based offline IRL algorithm. As we discussed in the related work (Appendix B), this is due to the fact that when the expert demonstrations are limited and diverse transition samples take a large portion of all collected data, CLARE learns a policy to match the joint visitation measure of all collected data, thus fails to imitate expert behaviors.

In Appendix A, we further present an experiment showcasing the quality of the reward recovered by the Offline ML-IRL algorithm. Towards this end, we use the recovered reward from the medium-expert dataset to estimate the reward value for all transition samples \((s,a,s^{})\) in the medium-replay dataset. With the recovered reward function \(r(,;)\) and given those transition samples with estimated reward labels \(\{s,a,r(s,a;),s^{}\}\), we run MOPO to solve Offline RL tasks given these transition samples with estiamted rewards. In Fig. 5, we can infer that the Offline ML-IRL can recover high-quality reward functions, since the recovered reward can label transition samples for solving offline RL tasks. Comparing with the numerical results of directly doing offline-IRL on the respective datasets in Table 1, we show that solving Offline RL by using transferred reward function and unlabelled transition datasets can achieve similar performance in Fig. 5.

## 7 Conclusion

In this paper, we model the offline Inverse Reinforcement Learning (IRL) problem from a maximum likelihood estimation perspective. Through constructing a generative world model to estimate the environment dynamics, we solve the offline IRL problem through a model-based approach. We develop a computationally-efficient algorithm that effectively recovers the underlying reward function and its associated optimal policy. We have also established statistical and computational guarantees for the performance of the recovered reward estimator. Through extensive experiments, we demonstrate that our algorithm outperforms existing benchmarks for offline IRL and Imitation Learning, especially on high-dimensional robotics control tasks. One limitation of our method is that we focus solely on aligning with expert demonstrations during the reward learning process. In an ideal scenario, reward learning should incorporate diverse metrics and data sources, such as expert demonstrations and preferences gathered through human feedback. One direction for future work is to broaden our algorithm framework and theoretical analysis for a wider scope in reward learning.

  Dataset type & Environment & Offline ML-IRL & BC & ValueDICE & CLARE & Expert Performance \\  medium & hopper & \(2453.25 171.37\) & \(2801.19 330.88\) & \(\) & \(3015.37 474.38\) & \(3512.09 21.65\) \\ medium & halfcretah & \(\) & \(4471.72 2835.55\) & \(1215.17 959.45\) & \(841.46 344.06\) & \(12174.61 91.45\) \\ medium & walker2d & \(\) & \(3228.75 906.96\) & \(3191.47 1857.90\) & \(237.49 160.62\) & \(5338.98 52.15\) \\ medium-replay & hopper & \(3036.46 429.25\) & \(2801.19 330.88\) & \(\) & \(288.04 844.84\) & \(5312.09 21.65\) \\ medium-replay & halfcretah & \(\) & \(4471.72 2835.55\) & \(1215.71 959.45\) & \(437.18 128.99\) & \(12174.61 91.45\) \\ medium-replay & walker2d & \(\) & \(3228.75 906.96\) & \(3191.47 187.90\) & \(291.71 75.66\) & \(5383.98 52.15\) \\ medium-exp & hopper & \(3317.12 338.18\) & \(2801.90 330.88\) & \(3073.16 538.67\) & \(3350.47 245.78\) & \(3512.09 21.65\) \\ medium-exp & walker2d & \(11231.40 585.21\) & \(4417.72 2835.55\) & \(1215.71 959.45\) & \(622.79 56.46\) & \(12174.61 91.45\) \\ medium-exp & walker2d & \(4201.40 637.99\) & \(2328.75 906.96\) & \(3191.47 1887.90\) & \(959.50 470.64\) & \(5383.98 52.15\) \\  

Table 1: **MuJoCo Results.** The performance versus different datasets and \(5,000\) expert demonstrations. The bolded numbers are the best ones for each data set, among Offline ML-IRL, BC, ValueDICE, and CLARE.

Figure 2: The performance of Offline ML-IRL given 5,000 expert demonstrations.