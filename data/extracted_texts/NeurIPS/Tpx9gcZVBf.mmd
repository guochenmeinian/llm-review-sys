# DiffAug: A Diffuse-and-Denoise Augmentation for Training Robust Classifiers

Chandramouli S. Sastry, Sri Harsha Dumpala, Sageev Oore

Dalhousie University, Canada.

###### Abstract

We introduce DiffAug, a simple and efficient diffusion-based augmentation technique to train image classifiers for the crucial yet challenging goal of improved classifier robustness. Applying DiffAug to a given example consists of one forward-diffusion step followed by one reverse-diffusion step. Using both ResNet-50 and Vision Transformer architectures, we comprehensively evaluate classifiers trained with DiffAug and demonstrate the surprising effectiveness of single-step reverse diffusion in improving robustness to covariate shifts, certified adversarial accuracy and out of distribution detection. When we combine DiffAug with other augmentations such as AugMix and DeepAugment we demonstrate further improved robustness. Finally, building on this approach, we also improve classifier-guided diffusion wherein we observe improvements in: (i) classifier-generalization, (ii) gradient quality (i.e., improved perceptual alignment) and (iii) image generation performance. We thus introduce a computationally efficient technique for training with improved robustness that does not require any additional data, and effectively complements existing augmentation approaches.

## 1 Introduction

Motivated by the success of diffusion models in high-fidelity and photorealistic image generation, generative data augmentation is an emerging application of diffusion models. While attempts to train improved classifiers with synthetic data have proved challenging, Azizi et al.  impressively demonstrated that extending the training dataset with synthetic images generated using Imagen  -- with appropriate sampling parameters (e.g. prompt and guidance strength) -- could indeed improve Imagenet classification. In a similar experiment with Stable Diffusion (SD) , Sariyildiz et al.  studied classifiers trained exclusively on synthetic images (i.e. no real images) and discovered improvements when training on a subset of 100 Imagenet classes. The success of generative data augmentation depends crucially on sample quality , so these findings irrefutably highlight the superior generative abilities of diffusion models.

Despite these impressive findings, widespread adoption of diffusion models for synthetic data augmentation is constrained by high computational cost of diffusion sampling, which requires multiple steps of reverse diffusion to ensure sufficient sample quality. Furthermore, both SD and Imagen are trained on upstream datasets much larger than Imagenet and some of these improvements could also be attributed to the quality and scale of the upstream dataset . For example, Bansal and Grover  find limited advantages in synthetic examples generated from a diffusion model trained solely on Imagenet.

Together, these limitations motivate us to explore a diffusion-based augmentation technique that is not only computationally efficient but can also enhance classifier training without relying on extra data. To that end, we consider the following questions:

1. _Can we leverage a diffusion model trained with no extra data?_

#### 1.1.2 Can we train improved classifiers with a single step of reverse diffusion?

In the context of reverse diffusion sampling, the intermediate output obtained after one reverse diffusion (i.e., denoising) step is commonly interpreted as an _approximation_ of the final image by previous works and has been utilised to define the guidance function at each step of guided reverse-diffusion (e.g., [3; 10; 11]). Similarly, Diffusion Denoised Smoothing (DDS)  applies denoised smoothing , a certified adversarial defense for pretrained classifiers, using one reverse diffusion step. In contrast to previous work, we use the output from a single reverse diffusion step as an augmentation to _train_ classifiers (i.e., not just at inference time) as we describe next.

**Diffuse-and-Denoise Augmentation** Considering a diffusion model defined such that time \(t=0\) refers to the data distribution and time \(t=T\) refers to isotropic Gaussian noise, we propose to generate augmentations of train examples by first applying a Gaussian perturbation (i.e., forward-diffusion to a random time \(t[0,T]\)) and then crucially, applying a single diffusion denoising step (i.e., one-step reverse diffusion). That is, we treat these diffused-and-denoised examples as augmentations of the original train image and refer to this technique as DiffAug. A one-step diffusion denoised example derived from a Gaussian perturbed train example can also be interpreted as an intermediate sample in _some_ reverse diffusion sequence that starts with pure noise and ends at the train example. Interpreted this way, our classifier can be viewed as having been trained on partially-synthesized images whose ostensible quality varies from unrecognizable (DiffAug using \(t T\)) to excellent (DiffAug using \(t 0\)). This is surprising because, while Ravuri and Vinyals  find that expanding the train dataset even with a small fraction of (lower quality) synthetic examples can lead to noticeable drops in classification accuracy, we find that classifier accuracy over test examples does not degrade despite being explicitly trained with partially synthesized train images. Instead, we show that diffusion-denoised examples offer a regularization effect when training classifiers that leads to _improved classifier robustness without sacrificing clean test accuracy and without requiring additional data_.

Our contributions in this work are as follows1:

1. **DiffAug** We propose DiffAug, a simple, efficient and effective diffusion-based augmentation technique. We provide a qualitative and analytical discussion on the unique regularization effect -- complementary to other leading and classic augmentation methods -- introduced by DiffAug.
2. **Robust Classification**. Using both ResNet-50 and ViT architectures, we evaluate the models in terms of their robustness to covariate shifts, adversarial examples (i.e., certified accuracy under Diffusion Denoised Smoothing (DDS)) and out-of-distribution detection.
3. **DiffAug-Ensemble (DE)** We extend DiffAug to test-time and introduce DE, a simple test-time image augmentation/adaptation technique to improve robustness to covariate shift that is not only competitive with DDA , the state-of-the-art image adaptation method but also 10x faster.
4. **Perceptual Gradient Alignment.** Motivated by the success of DDS and evidence of perceptually aligned gradients (PAGs) in robust classifiers, we qualitatively analyse the classifier gradients and discover the perceptual alignment described in previous works. We then theoretically analyse the gradients through the score function to explain this perceptual alignment.
5. **Improved Classifier-Guided Diffusion**. Finally, we build on (d) to improve gradient quality in guidance classifiers and demonstrate improvements in terms of: (i) generalization, (ii) perceptual gradient alignment and (iii) image generation performance.

## 2 Background

The stochastic diffusion framework  consists of two key components: 1) the forward-diffusion (i.e., data to noise) stochastic process, and 2) a learnable score-function that can then be used for the reverse-diffusion (i.e., noise to data) stochastic process.

The forward diffusion stochastic process \(\{_{t}\}_{t[0,T]}\) starts at data, \(_{0}\), and ends at noise, \(_{T}\). We let \(p_{t}()\) denote the probability density of \(\) at time \(t\) such that \(p_{0}()\) is the data distribution, and \(p_{T}()\) denotes the noise distribution. The diffusion is defined with a stochastic-differential-equation (SDE):

\[d=(,t)\,dt+g(t)\,d,\] (1)

where \(\) denotes a standard Wiener process, \((,t)\) is a drift coefficient, and \(g(t)\) is a diffusion coefficient. The drift and diffusion coefficients are usually specified manually such that the solution tothe SDE with initial value \(_{0}\) is a time-varying Gaussian distribution \(p_{t}(|_{0})\) whose mean \((_{0},t)\) and standard deviation \((t)\) can be exactly computed.

To sample from \(p_{0}()\) starting with samples from \(p_{T}()\), we solve the reverse diffusion SDE :

\[d=[(,t)-g(t)^{2}_{} p_{ t}()]dt+g(t)\,d},\] (2)

where \(d}\) is a standard Wiener process when time flows from T to 0, and \(dt\) is an infinitesimal negative timestep. In practice, the score function \(_{} p_{t}()\) is estimated by a neural network \(s_{}(,t)\), parameterized by \(\), trained using a score-matching loss .

**Denoised Examples.** Given \((_{0},y) p_{0}\) and \( p_{t}(|_{0})=( (_{0},t),\ ^{2}(t))\), we can compute the denoised image \(}_{t}\) using the pretrained score network \(s_{}\) as:

\[}_{t}=+^{2}(t)s_{}(,t)\] (3)

Intuitively, \(}_{t}\) is an _expectation_ over all possible images \(_{t}=(_{0},t)\) that are _likely_ to have been perturbed with \((,\ ^{2}(t))\) to generate \(\) and the denoised example \(}_{t}\) can be written as

\[}_{t}=[_{t}|]=_{_ {t}}_{t}\;p_{t}(_{t}|)d_{t}\] (4)

We note that the mean does not change with diffusion time \(t\) in variance-exploding SDEs while the mean decays to zero with diffusion time for variance-preserving SDEs (DDPMs).

## 3 DiffAug: Diffuse-and-Denoise Augmentation

In this section, we describe Diffuse-and-Denoise Augmentation (DiffAug, in short) and then provide an analytical and qualitative discussion on the role of denoised examples in training classifiers. While we are not aware of any previous study on training classifiers using partially denoised examples, Diffusion-denoised smoothing (DDS) , DiffPure  and Diffusion Driven Adaptation (DDA)  are test-time applications of -- single-step (DDS) and multi-step (DiffPure/DDA) -- denoised examples to promote robustness in pretrained classifiers.

As implied by its name, DiffAug consists of two key steps: (i) Diffuse: first, we diffuse a train example \(_{0}\) to a uniformly sampled time \(t(0,T)\) and generate \( p_{t}(|_{0})\); (ii) Denoise: then, we denoise \(\) using a single application of trained score network \(s_{}\) as shown in Eq. 3 to generate \(}_{t}\). We assume that the class label does not change upon augmentation (see discussion below) and train the classifier to minimize the following cross-entropy loss:

\[=_{t,_{0}}[- p_{}(y|}_{ t})]\] (5)

where, \(t(0,T)\), \((_{0},y) p_{0}()\), and \(p_{}\) denotes the classifier parameterized by \(\). In this work, we show the effectiveness of DiffAug as a standalone augmentation technique, as well as the further compounding effect of combining it with robustness-enhancing techniques such as Augmix and DeepAugment, showing that DiffAug is achieving a robustness not captured by the other approaches. When combining DiffAug with such novel augmentation techniques, we simply include Eq. 5 as an additional optimization objective instead of stacking augmentations (for example, we can alternatively apply DiffAug to images augmented with Augmix/DeepAugment or vice-versa). Also, our preliminary analysis on stacking augmentations showed limited gains over simply training the network to classify independently augmented samples likely because training on independent augmentations implicitly generalizes to stacked augmentations.

**Qualitative Analysis and Manifold Theory.** When generating augmentations, it is important to ensure that the resulting augmentations lie on the image manifold. Recent studies [10; 38] on theoretical properties of denoised examples suggest that denoised examples can be considered to

Figure 1: DiffAug Augmentations. The leftmost column shows four original training examples (\(_{0}\)); to the right of that, we display 8 random augmentations (\(}_{t}\)) for each image between \(t=350\) and \(t=700\) in steps of size 50. Augmentations generated for \(t<350\) are _closer_ to the input image while the augmentations for \(t>700\) are _farther_ from the input image. We observe that the diffusion denoised augmentations with larger values of \(t\) do not preserve the class label introducing noise in the training procedure. However, we find that this does not lead to empirical degradation of classification accuracy but instead contributes to improved robustness. Also, see Fig. 6 in appendix for a toy 2d example.

be on the data manifold under certain assumptions lending theoretical support to the idea of using denoised examples as augmentations. We can interpret training on denoised examples as a type of Vicinal Risk Minimization (VRM) since the denoised examples can be considered to lie in the _vicinal distribution_ of training samples. Previous works have shown that VRM improves generalization: for example, Chapelle et al.  use Gaussian perturbed examples (\(\)) as the vicinal distribution while MixUp  uses a convex sum of two random inputs (and their labels) as the vicinal distribution. From Eq. 4, we can observe that a denoised example is a convex sum over \(_{t}\) and we can interpret \(}_{t}\) as being _vicinal_ to examples \(_{t}\) that have a non-trivial likelihood, \(p_{t}(_{t}|)\), of generating \(\). The distribution \(p_{t}(_{t}|)\) is concentrated around examples perceptually similar to \((_{0},t)\) when \(\) is closer to \(_{0}\) (i.e., smaller \((t)\)) and becomes more entropic as the noise scale increases: we can qualitatively observe this in Fig. 1.

Diffusion denoised augmentations generated from larger \((t)\) can introduce label-noise into the training since the class-labels may not be preserved upon augmentation - for example, some of the diffusion denoised augmentations of the dog in Fig. 1 resemble architectural buildings. Augmentations that alter the true class-label are said to cause manifold intrusion  leading to underfitting and lower classification accuracies. In particular, accurate predictions on class-altered augmented examples would be incorrectly penalised causing the classifier to output less confident predictions on all inputs (i.e., underfitting). Interestingly, however, diffusion denoised augmentations that alter the true-class label are also of lower sample quality. The correlation between label noise and sample quality allows the model to selectively lower its prediction confidence when classifying denoised samples generated from larger perturbations applied to \(_{0}\) (we empirically confirm this in Section 4). In other words, the classifier _learns_ to observe important details in \(}_{t}\) to determine the optimal prediction estimating the class-membership probabilities of \(_{0}\). On the other hand, any augmentation that alters class-label by preserving the sample quality can impede the classifier training since the classifier cannot rely on visual cues to selectively lower its confidence (for an example, see Fig. 7 in appendix). Therefore, we do not consider multi-step denoising techniques to generate augmentations -- despite their potential to improve sample quality -- since this would effectively decorrelate label-noise and sample-quality necessitating additional safeguards -- e.g., we would then need to determine the maximum diffusion time we could use for augmentation without altering the class-label or scale down the loss terms corresponding to samples generated from larger \((t)\). We leave this exploration to future work and include a preliminary analysis in Appendix B.2.2.

**Test-time Augmentation with DiffAug.** Test-time Augmentation (TTA)  is a technique to improve classifier prediction using several augmented copies of a single test example. A simple yet successful TTA technique is to just average the model predictions for each augmentation of a test sample. We extend DiffAug to generate test-time augmentations of a test-example wherein we apply DiffAug using different values of diffusion times \(t\) and utilize the average predictions across all the augmentations to classify the test example \(_{0}\):

\[p(y|_{0})=|}_{t}p_{}(y| }_{t})\] (6)

where, \(\) denotes the set of diffusion times considered. We refer to this as DiffAug Ensemble (DE). A forward diffusion step followed by diffusion denoising can be interpreted as projecting a test-example with unknown distribution shift into the source distribution and forms the basis of DDA, a diffusion-based image adaptation technique. Different from DE, DDA uses a novel multi-step denoising technique to transform the diffused test example into the source distribution. Since DE uses single-step denoised examples of forward diffused samples, we observe significant improvement in terms of running time while either improving over or remaining on par with DDA.

## 4 Experiments I: Classifier Robustness

In this section, we evaluate classifiers trained with DiffAug in terms of their standard classification accuracy as well as their robustness to distribution shifts and adversarial examples. We primarily conduct our experiments on Imagenet-1k and use the unconditional 256\(\)256 Improved-DDPM [14; 35] diffusion model to generate the augmentations. We apply DiffAug to train the popular ResNet-50 (RN-50) backbone as well as the recent Vision-Transformer (ViT) model (ViT-B-16, in particular). In addition to extending the default augmentations used to train RN-50/ViT with DiffAug, we also combine our method with the following effective robustness-enhancing augmentations: (i) AugMix (AM), (ii) DeepAugment (DA) and (iii) DeepAugment+AugMix (DAM). While we train the RN-50 from scratch, we follow DeIT-III recipe for training ViTs and apply DiffAug in the second training stage; when combining with AM/DA/DAM, we finetune the official checkpoint for 10 epochs. More details are included in Appendix B.1. In the following, we will evaluate the classifier robustness to **(i)** covariate shifts, **(ii)** adversarial examples and **(iii)** out-of-distribution examples.

**Covariate Shifts** To evaluate the classifiers trained with/without DiffAug in terms of their robustness to covariate-shifts, we consider the following evaluation modes:

1. **DDA**: A diffusion-based test-time image-adaptation technique to transform the test image into the source distribution.
2. **DDA-SE**: We consider the original test example as well as the DDA-adapted test-example by averaging the classifier predictions following the self-ensemble (SE) strategy proposed in .
3. **DiffAug-Ensemble (DE)**: We use a set of test-time DiffAug augmentations to classify a test example as described in Eq. (6). Following DDA, we determine the following range of diffusion times \(=\{0,50,,450\}\). In other words, we generate 9 DiffAug augmentations for each test example.
4. **Default**: In the default mode, we directly evaluate the model on the test examples.

We evaluate the classifiers on Imagenet-C, a dataset of 15 synthetic corruptions applied to Imagenet-test and summarize the results across all evaluation modes in Table 9. We summarize our observations as follows:

1. **DiffAug introduces consistent improvements** Classifiers trained with DiffAug consistently improve over their counterparts trained without these augmentations across all evaluation modes. The average relative improvements across all corruptions range from 5.3% to 28.7% in the default evaluation mode (see Table 6 in Appendix). On clean examples, we observe that DiffAug helps minimize the gap between default evaluation mode and other evaluation modes while effectively preserving the default accuracy.
2. **DE improves over DDA** On average, DiffAug Ensemble (DE) yields improved detection rate as compared to direct evaluation on DDA images. Furthermore, DiffAug-trained classifiers evaluated using DE improve on average over their counterparts (trained without DiffAug) evaluated using DDA-SE. This experiment interestingly reveals that a set of one-step diffusion denoised images (DE) can achieve improvements comparable to multi-step diffusion denoised images (DDA) at a substantially faster (\( 10\)x) wallclock time (see Table 10 in Appendix).

**DiffAug vs Extra Synthetic Data**: Bansal and Grover  demonstrate that foundation models such as Stable-Diffusion can be used to generate additional synthetic data to improve classifier robustness to covariate shifts. We note that extra synthetic data with diffusion models trained exclusively on ImageNet do not help in enhancing classifier robustness (see Appendix B.2.1). In particular, Bansal and Grover  utilise diverse prompts to generate a synthetic clone of Imagenet consisting of 1.3M

    &  &  \\  Train Augmentations &  &  &  &  &  &  &  &  &  &  \\    & & & & & & & & & (SE) & & Def. & **Avg** \\  AM & 33.18 & 36.54 & 34.08 & 26.72 & 32.63 & 62.23 & 75.98 & 73.8 & 77.53 & 72.39 \\ AM+DiffAug & 34.64 & 38.61 & 38.58 & 29.47 & **35.33** & 63.53 & 76.09 & 75.88 & 77.34 & **73.21** \\  DA & 35.41 & 39.06 & 37.08 & 31.93 & 35.87 & 63.63 & 75.39 & 74.28 & 76.65 & 72.49 \\ DA+DiffAug & 37.61 & 41.31 & 40.42 & 33.78 & **38.28** & 65.47 & 75.54 & 74.63 & 76.51 & **73.24** \\  DAM & 40.36 & 44.81 & 41.86 & 39.52 & 41.64 & 65.54 & 74.41 & 73.54 & 75.81 & 72.33 \\ DAM+DiffAug & 41.91 & 46.35 & 44.77 & 41.24 & **43.57** & 66.83 & 74.64 & 74.39 & 75.66 & **72.88** \\  RN50 & 28.35 & 30.62 & 27.12 & 17.87 & 25.99 & 58.09 & 74.38 & 71.43 & 76.15 & 70.01 \\ RN50+DiffAug & 31.15 & 33.51 & 32.22 & 20.87 & **29.44** & 61.04 & 74.87 & 75.07 & 75.95 & **71.73** \\  ViT-B/16 & 43.6 & 52.9 & 48.25 & 50.75 & 48.88 & 67.4 & 81.72 & 80.43 & 83.71 & 78.32 \\ ViT-B/16+DiffAug & 45.05 & 53.54 & 51.87 & 52.78 & **50.81** & 70.05 & 81.85 & 82.59 & 83.59 & **79.52** \\ 
**Avg** & 37.13 & 41.73 & 39.63 & 34.49 & 38.24 & 64.38 & 76.49 & 75.68 & 77.89 & 73.61 \\
**Avg** (No-DiffAug) & 36.18 & 40.79 & 37.68 & 33.36 & 37.00 & 63.38 & 76.38 & 74.70 & 77.97 & 73.11 \\
**Avg** (DiffAug) & 38.07 & 42.66 & 41.57 & 35.63 & 39.48 & 65.38 & 76.60 & 76.67 & 77.81 & 74.12 \\   

Table 1: Top-1 Accuracy (%) on Imagenet-C (severity=5) and Imagenet-Test. We summarize the results for each combination of Train-augmentations and evaluation modes. The average (avg) accuracies for each classifier and evaluation mode is shown.

images. We utilize their open-sourced synthetic dataset and fine-tune the torchvision resnet-50 model for 10 epochs using both real and synthetic images and call this RN50+Synth. Next, we repeat this finetuning process with DiffAug and call this RN50+Synth+DiffAug. In Table 2, we summarize the results across many datasets and interestingly observe that DiffAug -- using an ImageNet-only diffusion model -- offers improvements over and beyond additional synthetic data using Stable-Diffusion. This is _surprising_ due to the following reasons: **(i)** SD is trained on LAION-5B, a much larger dataset that also subsumes ImageNet. **(ii)** Additional synthetic data requires more compute per each sample (e.g., 50 reverse-diffusion steps) whereas DiffAug just uses one reverse-diffusion step.

While the improvements offered by the extra synthetic data can be largely attributed to the upstream training dataset, DiffAug augmentations offer complementary regularization benefits. To understand this, we first note that DiffAug augmentations are qualitatively distinct from high-quality synthetic images and can be interpreted as lying on the image manifold in regions between high-quality samples: depending on the diffusion time, the DiffAug augmentation can vary greatly in quality (as shown in Fig. 1). As a result, classifying some of these augmented images is more challenging as compared to the original examples producing a regularizing effect that leads to empirical robustness improvements.

We include a detailed evaluation across different datasets including ImageNet-R/S in the Appendix (Table 7). Overall, we observe that DiffAug training and DE inference help significantly enhance robustness to covariate-shifts in several cases.

**Out-of-Distribution (OOD) Detection.** Test examples whose labels do not overlap with the labels of the train distribution are referred to as out-of-distribution examples. To evaluate the classifiers in terms of their OOD-detection rates, we use the Imagenet near-OOD detection task defined in the OpenOOD benchmark, which also includes an implementation of recent OOD detection algorithms such as ASH, ReAct, Scale and MSP. For context, while the torchvision ResNet-50 checkpoint is most commonly used to evaluate new OOD detection algorithms, AugMix provides the best OOD detection amongst the existing robustness-enhancing augmentation techniques and AugMix/ASH is placed 3rd amongst 73 methods on the OpenOOD leaderboard (ordered by near-OOD performance). Yet in Table 3 we observe that DiffAug introduces _further_ improvement on the challenging near-OOD detection task across all considered OOD algorithms.

Comparing our results to the leaderboard, we observe AugMix+DiffAug/Scale achieves an AUROC of 84.81 outperforming the second best method (84.01 AUROC) and comparable to the top AUROC of 84.87.

    & ImageNet-C & ImageNet-R & ImageNet-S & ImageNet & ImageNet-A & ImageNet-D & Average \\  & (Severity=5) & ImageNet - & ImageNet-S & Sketch & ImageNet-A & ImageNet-D & Average \\  RN50 & 17.87 & 36.16 & 7.12 & 24.09 & 0.03 & 11.36 & 16.10 \\  +DiffAug/DE & **32.22 (+14.85)** & **41.61 (+5.45)** & **12.52 (+5.40)** & **26.67 (+2.56)** & **1.09 (+1.06)** & 11.37 (+0.01) & **20.90** \\  RN50+Synth & 17.58 & 49.28 & 7.68 & 35.45 & 0.63 & 17.52 & 21.35 \\  +DiffAug/DE & **30.06 (+12.48)** & **54.71 (+5.43)** & **13.57 (+5.89)** & **37.39 (+1.94)** & **1.53 (+0.9)** & **21.41 (+3.89)** & **26.45** \\   

Table 2: Top-1 Accuracy (%) across different types of distribution shifts when additional high-quality synthetic data from Stable-Diffusion is available (denoted by +Synth). We show the net improvement obtained by DiffAug training and DiffAug-Ensemble (DE) inference. For reference, we also include the results for the corresponding ResNet50 models without extra synthetic data.

   Train &  &  &  & Scale & **Avg.** \\ Augmentation & & & & & \\  AugMix(AM) & 82.16 & 77.49 & 79.94 & 83.61 & 80.8 \\ AM+DiffAug & 83.62 & 78.35 & 81.29 & 84.81 & **82.02** \\  RN50 & 78.17 & 76.02 & 77.38 & 81.36 & 78.23 \\ RN50+DiffAug & 79.86 & 76.86 & 78.76 & 82.81 & **79.57** \\   

Table 3: AUROC on Imagenet Near-OOD Detection.

Figure 2: Average prediction entropy on DiffAug samples vs diffusion time measured with Imagenet-Test. We observe that the models trained with DiffAug correctly yield predictions with higher entropies (lower confidence) for images containing imperceptible details (i.e. larger \(t\)). Surprisingly, the classifiers trained without DiffAug do not also assign random-uniform label distribution for DiffAug images at \(t=999\), which have no class-information by construction. Also, see Fig. 11.

DiffAug training _teaches_ the network to selectively lower its prediction confidence (higher prediction entropy) based on the image content (Fig. 2) and we hypothesize that this leads to improved OOD detection rates. We include the detailed OOD detection results in Appendix B.4. Interestingly, the combination of augmentations that improve robustness on covariate shifts may not necessarily lead to improved OOD detection rates: for example, AugMix+DeepAugment improves over both AugMix and DeepAugment on covariate shift but achieves lower OOD detection rates than either. On the other hand, we observe that combining with DiffAug enhances both OOD detection as well as robustness to covariate shift.

**Certified Adversarial Accuracy.** Denoised smoothing  is a certified defense for pretrained classifiers inspired from Randomized smoothing  wherein noisy copies of a test image are first denoised and then used as classifier input to estimate both the class-label and robust radius for each example. Using the same diffusion model as ours, DDS already achieves state-of-the-art certified Imagenet accuracies with a pretrained 305M-parameter BeIT-L. Here, we evaluate the improvement in certified accuracy when applying DDS to a model trained with DiffAug and include the results in Appendix B.3. We speculate that finetuning the BeIT-L model with DiffAug should lead to similar improvements but skip this experiment since it is computationally expensive.

**Perceptually Aligned Gradients and Robustness.** Classifier gradients (\(_{} p_{}(y|)\) where \(\) is an image) which are semantically aligned with human perception are said to be perceptually aligned gradients (PAG) . While input-gradients of a typical image classifier are usually unintelligible, gradients obtained from adversarially robust classifiers trained using randomized smoothing  or adversarial training  are perceptually-aligned. Motivated by the state-of-the-art certified adversarial accuracy achieved by DDS, we analyse the classifier gradients of one-step diffusion denoised examples -- i.e., we analyse \(_{} p_{}(y|}_{t})\) where \(=p_{t}(|_{0})\) and \(}_{t}=+^{2}(t)s_{}(,t)\). We visualise the gradients in Fig. 3 and interestingly discover the same perceptual alignment of gradients discussed in previous works (we compare with gradients of classifiers trained with randomized smoothing later). To theoretically analyse this effect, we first decompose the input-gradient using chain rule as:

\[(y|}_{t})}{d}= (y|}_{t})}{d}_{t}}}_{t}}{ d}\] (7)

Empirically, we find that the perceptual alignment is introduced due to transformation by \(}_{t}}{d}\) and analyse it further:

**Theorem 4.1**.: _Consider a forward-diffusion SDE defined as in Eq. 1 such that \(p_{t}(|_{0})=(_{t},\; ^{2}(t)I)\) where \(_{t}=(_{0},t)\). If \( p_{t}()\) and \(}_{t}=+^{2}(t)s_{}(,t)\), for optimal parameters \(\), the derivative of \(}_{t}\) w.r.t. \(\) is proportional to the covariance matrix of the conditional distribution \(p(_{t}|)\). See proof in Appendix B.6._

\[}_{t}}{}=J=(t)}[_{t}|]\]

This theorem shows us that the multiplication by \(}_{t}}{}\) in Eq (7) is in fact a transformation by the covariance matrix \([_{t}|]\). Multiplying a vector by \([_{t}|]\)_stretches_ the vector along the principal directions of the conditional distribution \(p(_{t}|)\). Intuitively, since the conditional distribution \(p(_{t}|)\) corresponds to the distribution of _candidate_ denoised images, the principal directions of variation are perceptually aligned (to demonstrate, we apply SVD to \(J\) and visualise the principal components in Appendix B.7) and hence stretching the gradient along these directions will yield perceptually aligned gradients. We note that our derivation complements Proposition 1 in Chung et al.  which proves certain properties (e.g., \(J=J^{}\)) of this derivative. In practice, however, the score-function is parameterized by unconstrained, flexible neural architectures that do not have exactly symmetric jacobian matrices \(J\). For more details on techniques to enforce conservative properties of score-functions, we refer the reader to Chao et al. . Ganz et al.  demonstrate that training a classifier to have perceptually aligned gradients also improves its robustness exposing the bidirectional relationship between robustness and PAGs. This works offers additional evidence supporting the co-occurrence of robustness and PAGs since we observe that classification of diffused-and-denoised images (e.g., DDS, DE, DDA) not only improve robustness but also produce PAGs.

**Ablation Analysis.** Appendix B.5 includes an ablation study on the following:

Figure 3: PAG example using ViT-bMAug. We diffuse the Imagenet example (left) to \( 300\) and visualize the min-max normalized classifier gradients (right). For easy viewing, we apply contrast maximization. More examples are shown below.

1. **Extra Training**. The pretrained DA, AM, and DAM classifiers are sufficiently trained for 180 epochs and hence, we compare the DiffAug finetuned model directly with the pretrained checkpoint. For completeness, we train AugMix for another 10 epochs and confirm that there is no notable change in performance as compared to results in Tables 3, 7 and 9.
2. **DiffAug Hyperparameters**. In our experiments, we considered the complete range of diffusion time. We investigate a simple variation where we either use \(t\) or \(t\) to generate the DiffAug augmentations.
3. **DiffAug-Ensemble Hyperparameters**. We analyse how the choice of diffusion times considered in the set \(S\) (Eq. (6)) affects DE performance.
4. **Conditional DiffAug**. While our DiffAug experiments mainly utilize unconditional diffusion models, we can also utilize conditional diffusion models and explore an extension of AugMix with conditional DiffAug for various guidance strengths.
5. **Latent-Space Diffusion Models**. We also tried the DiffAug augmentation method with Diffusion-Transformer (DiT): while we observed that the test-accuracy is preserved even in this case, we only observe slight robustness improvements as compared with pixel-space diffusion models. This may be explained by noting that the VAE-decoder is trained to output perceptually high-quality image and hence, does not accurately capture the expectation in Eq. (4).

## 5 Experiments II: Classifier-Guided Diffusion

Classifier guided (CG) diffusion is a conditional generation technique to generate class-conditional samples with an unconditional diffusion model. To achieve this, a time-conditional classifier is separately trained to classify noisy samples from the forward diffusion and we refer to this as a _guidance classifier_

\[_{}=_{t,}[- p_{}(y|,t)]\] (8)

where, \(t(0,T)\), \( p_{t}(|_{0})\) and \((_{0},y) p_{0}()\). At each step of the classifier-guided reverse diffusion (Eq. (2)), the guidance classifier is used to compute the class-conditional score \(_{} p_{t}(|y)=_{} p_{}( y|,t)+_{s}_{} p_{t}()\) (\(_{s}\) is classifier scale), which is used in place of unconditional score \(_{} p_{t}()\).

**Denoising-Augmented (DA) Classifier**. The guidance classifiers participate in the sampling through their gradients, which indicate the pixel-wise perturbations that maximizes log-likelihood of the target class. Perceptually aligned gradients that resemble images from data distribution lead to meaningful pixel-wise perturbations that could potentially improve classifier-guidance and forms the motivation of Kawar et al. , where they propose an adversarial training recipe for guidance classifiers. With the same motivation, we instead build on Theorem 4.1 in order to improve perceptual alignment and propose to train guidance classifiers with denoised examples \(}_{t}\) derived from \(\). While the obvious choice is to simply train the guidance-classifier on \(}_{t}\) instead of \(\), we choose to provide both \(\) as well as \(}_{t}\) as simultaneous inputs to the classifier and instead optimize \(_{}=_{t,}[- p_{}(y|, }_{t},t)]\) (compare with Eq. (8)). We preserve the noisy input since the primary goal of guidance classifiers is to classify noisy examples and this approach enables the model to flexibly utilize information from both inputs. We refer to guidance-classifiers trained using both \(\) and \(}_{t}\) as _denoising-augmented (DA) classifier_ and use _noisy classifiers_ to refer to guidance-classifiers trained exclusively on \(\).

**Experiment setup.** We conduct our experiments on CIFAR10 and Imagenet and evaluate the advantages of DA-Classifiers over noisy classifiers. While we use the same Imagenet diffusion model described in Section 4, we use the deep NCSN++ (continuous) model released by Song et al.  as the score-network for CIFAR10 (VE-Diffusion). As compared to the noisy classifier, the DA-classifier has an additional input-convolution layer to process the denoised input and is identical from the second layer onwards. We describe our classifier architectures and the training details in Appendix C.1.

**Classification Accuracy.** We first compare guidance classifiers in terms of test accuracies as a measure of their generalization (Table 4) and find that DA-classifiers generalize better to the test data. Training classifiers with Gaussian perturbed examples often leads to underfitting  explaining the lower test accuracy observed with noisy classifiers. Interestingly, the additional denoised example helps address the underfitting - for example, see Fig. 19 (in appendix). One explanation of this finding could be found in Chung et al. ,

   Method & CIFAR10 & Imagenet \\  Noisy Classifier & 54.79 & 33.78 \\ DA-Classifier & **57.16** & **36.11** \\   

Table 4: Summary of Test Accuracies for CIFAR10 and Imagenet: each test example is diffused to a random uniformly sampled diffusion time. Both classifiers are shown the same diffused example.

where they distinguish between noisy examples and their corresponding denoised examples as being in the ambient space and on the image manifold respectively, under certain assumptions. To determine the relative importance of noisy and denoised examples in DA-classifiers, we zeroed out one of the input images to the CIFAR10 classifier and measured classification accuracies: while zeroing the noisy input caused the average accuracy across all time-scales to drop to 50.1%, zeroing the denoised input breaks the classifier completely yielding random predictions.

**Classifier Gradients**. In Fig. 3(a), we qualitatively compare between the noisy classifier gradients (\(_{} p_{}(y|,t)\)) and the DA-classifier gradients(\(_{} p_{}(y|,},t)\)). We find that the gradients obtained from the DA-classifier are more structured and semantically aligned with the clean image as compared to the ones obtained with the noisy classifier (see Figs. 20 and 22 in Appendix for more examples). Gradients backpropagated through the denoising score network have been previously utilized (e.g., [3; 10; 11; 18; 26; 36]), but our work is the first to observe and analyze the qualitative properties of gradients obtained by backpropagating through the denoising module (also see Fig. 21 in appendix).

**Image Generation.** To evaluate the guidance classifiers in terms of their image generation, we generate 50k images each - see Appendix C.2 for details on the sampling parameters. We compare the classifiers in terms of standard generative modeling metrics such as FID, IS, and P/R/D/C (Precision/Recall/Density/Coverage). The P/R/D/C metrics compare between the manifold of generated distribution and manifold of the source distribution in terms of nearest-neighbours and can be computed conditionally (i.e., classwise) or unconditionally. Following standard practice, we additionally evaluate CIFAR10 classifiers on class-conditional P/R/D/C. Our results (Table 5) show that our proposed Denoising-Augmented (DA) Classifier improves upon the Noisy Classifier in terms of FID and IS for both CIFAR10 and Imagenet at roughly same Precision and Recall levels (see Appendix C.3 for comparison with baselines). Our evaluation of average class-conditional precision, recall, density and coverage for each CIFAR10 class also shows that DA-classifiers outperform

Figure 4: **(a)** Min-max normalized gradients on clean samples (left column) diffused to t = 300 (T = 999). For easy comparison between Noisy classifier gradients (middle column) and DA-classifier gradients (right column), we applied an identical enhancement to both images, i.e. contrast maximization. The unedited gradients are shown in Fig. 20. **(b)** Qualitative Comparison of Guidance Classifiers on the Image Generation Task using DDIM-100 with same random seed. In each pair, the first image is generated with the Noisy Classifier and the second image is generated with the Denoising-Augmented (DA) Classifier. We observe that the Denoising-Augmented (DA) Classifier improves overall coherence as compared to the Noisy Classifier. Also see Fig. 25 in appendix for more examples.

    &  &  \\   & FID\(\) & IS \(\) & P \(\) & R \(\) & \(}\) & \(}\) & \(}\) & \(}\) & FID \(\) & sFID \(\) & IS \(\) & P \(\) & R \(\) \\  Noisy Classifier & 2.81 & 9.59 & 0.64 & 0.62 & 0.57 & 0.62 & 0.78 & 0.71 & 5.44 & **5.32** & 194.48 & 0.81 & 0.49 \\ DA-Classifier & **2.34** & **9.88** & 0.65 & 0.63 & **0.63** & **0.64** & **0.92** & **0.77** & **5.24** & 5.37 & **201.72** & 0.81 & 0.49 \\   

Table 5: Quantitative comparison of Guidance Classifiers on the Image Generation Task using 50k samples. We also show unconditional precision/recall (P/R) and the average class-conditional Precision/Recall/Density/Coverage.

Noisy classifiers: for example, DA-classifiers yield classwise density and coverage of about 0.92 and 0.77 respectively on average as compared to 0.78 and 0.71 obtained with Noisy-Classifiers. We can attribute our improvements in the class-conditional precision, recall, density and coverage to the improved generalization of DA-classifier. To qualitatively analyse benefits of the DA-classifier, we generated Imagenet samples using DDIM-100 sampler with identical random seeds and \(_{s}=2.5\). In our resulting analysis, we consistently observed that the DA-classifier maintains more coherent foreground and background as compared to the Noisy Classifier. We show examples in Fig. 4b. Overall, we attribute improved image generation to the improved generalization and classifier gradients.

## 6 Related Works

**Synthetic Augmentation with Diffusion Models** have also been explored to train semi-supervised classifiers  and few-shot learning . Other studies on training classifiers with synthetic datasets generated with a text2image diffusion model include . Apart from being computationally expensive, such text2image diffusion models are trained on large-scale upstream datasets and some of the reported improvements could also be attributed to the quality of the upstream dataset. Instead, we propose a efficient diffusion-based augmentation method and report improvements using a diffusion model trained with no extra data. Further, we also find that DiffAug is complementary to synthetic training data generated with large text2image diffusion models and leave further exploration of DiffAug with larger diffusion models as future work.

Synthetic examples have also been shown to be useful for enhancing training adversarially robust classifiers (e.g., ) and extension of DiffAug for compute-efficient adversarial training is apt for exploration in future work.

**Diffusion Models for Robust Classification.** Diffusion-classifier  is a method for zero-shot classification but also improves robustness to covariate shifts. Diff-TTA is a test-time adaptation technique to _update_ the classifier parameters at test time and is complementary to classifier training techniques such as DiffAug. In terms of OOD detection, previous works have proposed reconstruction-based metrics for ood detection . To the best of our knowledge, this work is the first to demonstrate improved OOD detection on ImageNet-1k using diffusion models.

## 7 Conclusion

In this work, we introduce DiffAug to train robust classifiers with one-step diffusion denoised examples. The simplicity and computational efficiency of DiffAug enables us to also extend other data augmentation techniques, where we find that DiffAug confers additional robustness without affecting accuracy on clean examples. We qualitatively analyse DiffAug samples in an attempt to explain improved robustness. Furthermore, we extend DiffAug to test time and introduce an efficient test-time image adaptation technique to further improve robustness to covariate shifts. Finally, we theoretically analyse perceptually aligned gradients in denoised examples and use this to improve classifier-guided diffusion. Overall, we present effective augmentation technique using diffusion models trained with no external datasets.