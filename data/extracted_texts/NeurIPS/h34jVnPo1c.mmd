# Doubly Hierarchical Geometric Representations for Strand-based Human Hairstyle Generation

Yunlu Chen\({}^{}\)1, Francisco Vicente Carrasco\({}^{}\), Christian Hane\({}^{}\), Giljoo Nam\({}^{}\),

**Jean-Charles Bazin\({}^{}\), and Fernando De la Torre\({}^{}\)**

\({}^{}\)Carnegie Mellon University \({}^{}\)Meta Reality Labs

###### Abstract

We introduce a doubly hierarchical generative representation for strand-based 3D hairstyle geometry that progresses from coarse, low-pass filtered guide hair to densely populated hair strands rich in high-frequency details. We employ the Discrete Cosine Transform (DCT) to separate low-frequency structural curves from high-frequency curliness and noise, avoiding the Gibbs' oscillation issues associated with the standard Fourier transform in open curves. Unlike the guide hair sampled from the scalp UV map grids which may lose capturing details of the hairstyle in existing methods, our method samples optimal sparse guide strands by utilising \(k\)-medoids clustering centres from low-pass filtered dense strands, which more accurately retain the hairstyle's inherent characteristics. The proposed variational autoencoder-based generation network, with an architecture inspired by geometric deep learning and implicit neural representations, facilitates flexible, off-the-grid guide strand modelling and enables the completion of dense strands in any quantity and density, drawing on principles from implicit neural representations. Empirical evaluations confirm the capacity of the model to generate convincing guide hair and dense strands, complete with nuanced high-frequency details.1

## 1 Introduction

The quest for realistic virtual humans is a cornerstone of modern computer graphics, with high-quality 3D hair strand generation being one of its most intricate challenges. Our work focuses on the generation of strand hair with machine learning algorithms, which alleviates the labor-intensive process inherent in crafting digital hairstyles--a task that requires meticulous attention to detail and consumes a disproportionate amount of time and artistic resources. In addition, it strives to establish a generative prior, a foundational blueprint that not only streamlines the creation process but also enhances the reconstruction capabilities essential for virtual human applications.

Hair exhibits a wide spectrum of morphological details, from the subtlest of low-frequency principle directions and waves to the complexity of high-frequency curls, as described by the physics of elastic rods and Kirchhoff's theories  on the dynamics of twisting filaments. These high-frequency elements often include curly helical structures  as well as extraneous noise that can detract from accurately modelling the hair's principal growth direction. To counteract this, we advocate for a frequency decomposition approach to extract the principal direction as a low-frequency signal. This technique is vital in distilling the essence of the hair's natural trajectory, ensuring that the core path of growth is clearly defined and free from the visual clutter of high-frequency noise. Such a focus on low-frequency signals is not only important but also highly effective in generating hair models thatare true to the natural flow and inherent physical properties of hair, providing a robust basis for more nuanced and detailed simulations.

The complexity of hair data has led to the adoption of coarse guide strands as a foundational step in many hair modelling methods, reflecting the guide hair-based artist grooming in Fig. 1(a). These existing methods , however, simplify the guide hair extraction to downsampling to a lower grid resolution of the 2D UV map of the head scalp, as in Fig. 1(b), which comes with some notable drawbacks. Firstly, simply extracting the full spectrum of each guide strand can be burdened by high-frequency details and noise irrelevant to adjacent strands, while the oracle guide strands are usually smooth curves for the hair's principal growing direction. Secondly, sampling from UV map grids can sometimes miss intricate hairstyle details and may produce guide strands that are less representative, as illustrated in Fig. 3. In addition, the grid sampling often results in a suboptimal density with less hair on the top of the head scalp but more hair on the side, since the UV mapping from a head scalp to the Euclidean \(^{2}\) space is not area-preserving.

In response to these challenges, we introduce a novel doubly hierarchical representation of strand hair geometry, by introducing frequency decomposition and optimal sampling to extract guide hair, as illustrated in Fig. 1(c). This approach aligns more closely with contemporary computer graphics tools designed to craft artistically groomed hairstyles. From the perspective of learning, our design to first learn low-frequency components followed by high-frequency ones adheres to the well-known frequency principle or spectral bias  of generalisation in neural networks. Additionally, optimal sampling ensures consistency within the coarse-to-fine learning pipeline. By refining the guide curves this way, we aim to capture the essential form and structure of hairstyles more effectively, paving the way for more accurate and visually pleasing hair modelling.

We develop neural models to process our novel and sophisticated hierarchical representation of strand hair geometry, which encompasses a variety of data forms. To accommodate the flexible off-the-grid modelling of guide strands, our neural model architecture for generation of guide strands adapts permutation-equivariant geometric deep learning models  that do not rely on a fixed feature grid within a Euclidean domain, with specific modifications tailored to the hair strand problem. Additionally, our densification model, designed to synthesize densely populated strands from sparse guides, draws inspiration from the continuous and resolution-free implicit neural representations  and graph message passing  techniques. This approach enables the model to effectively handle fine strands with varying numbers and densities.

To encapsulate our methodology and the contributions of our work:

Figure 1: (a) The oracle guide curves from a grooming project in Blender  for principle hair growing directions. (b) Existing works simply sample guide curves from dense strands regularly on a UV grid of the scalp, which may contain unnecessary signals of high-frequency noise for neighbouring dense strands. (c) We model guide curves by frequency decomposition and \(k\)-medoids clustering for optimal sampling a subset of smooth guide hair strands, which are utilized in training our hierarchical hair generation model.

* Our approach involves constructing a hierarchical generative model for 3D hair strands that begins with the formation of coarse guide curves through low-pass filtering and culminates in the generation of dense hair strands that incorporate intricate high-frequency details.
* We utilize the Discrete Cosine Transform (DCT) to obtain the coarse and low-pass filtered guide curves. This technique effectively distinguishes the fundamental shape of the hair from the more complex aspects of curliness and noise, thereby circumventing the Gibbs' oscillation problem that often plagues the standard Fourier transform when applied to open curves.
* The guide hair strands are sampled using \(k\)-medoids clustering centres from the dense hair data. This novel method is proven to be theoretically optimal in preserving the original hairstyle's features compared to the conventional practice of sampling from a regular 2D grid on a UV map.
* We adapt the family of non-Euclidean geometric deep learning models and develop a permutation-equivariant architecture for learning on hair strands, instead of 2D CNNs, for more flexible modelling of off-the-grid guide strands.
* We propose a novel neural mechanism for learning strand interpolation. Inspired by implicit neural representations and graph message passing, our method handles modelling any amount of dense strands at any sampling density, and enables end-to-end joint training with guide strands.

### Related Work

Recent learning algorithms for strand-based hair focuses on capture and reconstruction from single  or multiple views , often relying on intermediate representations of volumetric occupancies  or orientation fields from image gradient cues . These approaches mostly have a focus on optimising strand growth or connecting segments, while learning applies to other intermediate representations (e.g. orientation field) but not directly on strands, thus not requiring hierarchical strand representation with abstraction.

Our work focuses on building a generative model directly on human hairstyles in the form of a set of strands, which has been an established representation in industry , owing to its compatibility with physics-based applications. Consequently, strands have emerged as a favoured representation in numerous computer vision, machine learning, and graphics projects. The most related works to our approach for strand hair generation are recent methods of GroomGen  and HAAR , both relying on off-the-shelf VAE codec representations of strands  mapped on the Euclidean domain of discretized UV map, optionally followed by a coarse-to-fine pipeline in accordance with different UV grid resolutions . The early work from Wang et al.  focused on a slightly different task of exemplar-based synthesis of strand hair, which requires a base strand hairstyle or a combination of two for the global shape and synthesizes local textural details.

In contrast to existing strand-based hair generation methods  that rely on hair representations from prior work , we introduce a novel doubly hierarchical hair representation and associated neural models. Our approach begins by extracting guide hairs from flexible root locations to achieve an optimal set of strands that faithfully preserve the original hairstyle through a non-Euclidean representation. Subsequently, we apply frequency decomposition using the Discrete Cosine Transform (DCT) to achieve a compact representation, eliminating the need for training off-the-shelf codecs and enabling end-to-end optimization. Compared to grid UV maps combined with strand codecs and 2D CNNs, our method is more flexible, sophisticated, and exhibits superior performance. Additionally, our grooming generation technique allows sampling of an arbitrary number of dense strands from any location, without being constrained by UV grid resolution.

## 2 Extracting the Hierarchy from Strand Hairstyles

A human's hair can be seen as a set of strands \(=\{l_{i}\}_{i=1}^{N}\), where each strand is a 3D curve \(l(t):^{3}\). In practice, strand \(l\) is usually available as a polyline with discretely sampled control points in a sequence of \(n\) 3D points: \(l=[l(0),l(1),,l(n-1)]^{n 3}\) sampled from the continuous curve. The roots of the curves are attached to the head scalp \(\): \(l(0)\). In this work we consider all the hairstyles are aligned to the same human head scalp geometry.

We aim at a hierarchical modelling of human hairstyles due to the high complexity of the human hair data. Inspired by the guide strand-based modelling in artist strand hair creation, we propose to smoothen the strands with frequency decomposition and optimally sample coarse guide hair strands.

### Frequency decomposition of strands as open curves

Human hair exhibits a spectrum of morphologies, ranging from straightness to inherent curvature of planar waves or 3D helices from a confluence of the strand's physical attributes, such as mass, length, and tensile elasticity . For undulating and helical strands, the predominant axis of growth can be conceptualized as a smooth, low-frequency trajectory, whereas the detailed twists and noise are considered high-frequency features. Artists creating hairstyles typically start by making smooth, basic guide strands and then add detailed waves, curls, and slight randomness [2; 8]. Notwithstanding, contemporary learning-based algorithms for guide strand modelling have largely overlooked this intrinsic frequency paradigm.

In our approach, we separate hair strands into low- and high-frequency components, aligning with traditional artist-driven hair modelling techniques that utilize smooth guide strands. Initially, we model the smoothed, low-frequency base of the strand, then we enhance it with high-frequency waves, curls, and localized noise. This method not only adheres to the spectral bias principle  but also streamlines the learning process.

The Discrete Fourier transform (DFT) is a widely applied method for frequency decomposition. However, the DFT assumes strong periodicity of the signal, thus suffers from the Gibbs' phenomenon  with significant oscillations when processing on open curves whose start points and endpoints do not coincide with each other [11; 9], as depicted in Fig. 2.

Therefore, we adopt the Discrete Cosine Transform (DCT) , a Fourier-related transform that is analogous to the Discrete Fourier Transform (DFT) but employs only real-valued cosine basis components. The DCT is extensively utilized in image, video, and audio signal compression  due to its superior energy compaction properties, which allow for more efficient representation of signals with fewer coefficients. Additionally, the DCT offers more flexible boundary conditions that mitigate wrap-around effects, thereby reducing artefacts such as those associated with the Gibbs phenomenon. These advantages make the DCT particularly well-suited for encoding open curves using fewer harmonics. Formally, we apply the DCT to transfer the hair curve \(l(t)\) with \(t=1,,n\) into frequency domain \(()\) by

\[(l(t)):_{X}()=()^{}_{t=0}^ {n-1}w(t)\,l_{X}(t)\;[(2t+1)],\;\;w(t)=}&=0,\\ 1& \]

The corresponding inverse DCT is the inverse function \(=^{-1}\). \(l_{X}\) and \(_{X}\) denote the signal component on the X-axis coordinate in spatial and frequency spaces respectively. The same process also applies to Y- and Z-axes.

Due to the even symmetry of the cosine function, the DCT is equivalent to the DFT operating on the symmetrically extended signal sequence of twice the length, while the sine components are cancelled out. The periodicity extension of the signal no longer introduces strong discontinuity, which facilitates modelling open curves, as corroborated by Fig. 6. We refer to the literature for details [30; 9].

Figure 2: Comparing frequency-based smoothing of an exemplar curly strand. Smoothing from the DFT biases towards closed curves and suffers from Gibbs’ oscillations  for hair strands as open curves. In contrast, The DCT has a strong energy compaction property and more flexible boundary conditions, facilitating encoding open curves with fewer harmonics.

### Optimal sampling of coarse guide hair by \(k\)-medoids clustering

Learning-based approaches to strand-based hair modelling typically adopt a coarse-to-fine strategy, necessitating the generation of a set of coarse guide strands due to the computational demands of processing dense strands. This concept aligns with the traditional artist-driven hair creation workflow. Conventional methods predominantly employ a strategy of sampling guide strands by extracting those emanating from scalp locations situated at the grid centres of an unwrapped UV map of the scalp manifold. However, this regular grid sampling approach is suboptimal, often failing to capture certain strand clusters critical for detailing the hairstyle, as illustrated in Fig. 3. In contrast, we introduce a methodology for extracting a sparse subset of \(k\) off-the-grid strands from the comprehensive dense hair strand set \(^{L}=\{l^{L}\}\), with \(l^{L}\) denoting low-pass filtered strand curves from the first \(^{L}\) DCT components as predominant growing directions, resulting in the optimal set \(^{*}_{k}\) that more accurately mirrors the characteristics of the dense strands.

**Definition 1** (Optimal sampling guide hair curves).: _Given a set of dense low-pass filtered hair curves \(^{L}\), the optimal sampling of coarse guide curve set \(^{*}_{k}\) of cardinality \(k\) is the subset of \(^{L}\) which has the minimum possible bidirectional chamfer distance from \(^{L}\). Formally, \(^{*}_{k}=_{_{k}^{L}:\,| _{k}|=k\,(^{L},_{k})}\)_

Our investigation reveals that the \(k\)-medoids clustering method  delivers an optimal approach for identifying the optimal strand subset. \(k\)-medoids is a classical partitioning technique that organises data points into clusters with the aim of minimising the distance between the points within a cluster and a specific point within the same cluster, known as the medoid, which serves as the cluster's nucleus. Unlike the popular \(k\)-means algorithm which allows the cluster's centroid to be a virtual average of the data points, \(k\)-medoids selects actual data points as the medoids, enhancing the interpretability of the cluster centres. Formally, the objective of \(k\)-medoids is to partition the set \(^{L}\) into a collection of disjoint clusters \(=\{_{1},,_{k}\}\), each with its corresponding medoids \(=\{u_{1},,u_{k}\}\) in such a way that the aggregate of dissimilarities to all elements within each cluster is minimised, as defined by the following objective:

\[_{}_{i=1}^{k}_{l^{L}_{i}}d(l^{L},u_{i}), u_{i}=*{arg\,min}_{l^{L}_{i}} _{l^{L}_{i}}d(l^{L},l^{L}), \]

and \(d(,)\) is a dissimilarity measurement. This objective leads to the following observation:

**Theorem 1**.: _The medoid set \(=\{u_{1},,u_{k}\}\) from the \(k\)-medoids clustering of \(\) is the optimal sampled hair curve set \(^{*}_{k}\), if aggregated squared Euclidean distance \(d(l,l^{})=\|l-l^{}\|_{2}^{2}:=_{t=0}^{n -1}\|l(t)-l^{}(t)\|_{2}^{2}\) for two individual curves is used as the divergence function for \(k\)-medoids._

Theorem 1 establishes that the optimal strategy for sampling a specified number of guide strands from an original set of densely packed hair strands is to apply \(k\)-medoids clustering to the dense

Figure 3: Comparison of guide hair curves extracted by \(k\)-medoids (left) and grid-sampling (right) from reference hairstyles (middle), both with an equal count of curves. It is evident that grid-sampling can omit crucial hairstyle details. In contrast, the off-the-grid optimal guide curve subset derived from our \(k\)-medoids method more accurately captures the essence and details of the original hairstyle.

strand set and select the resulting medoids as the guide strands. This resultant set of guide strands, termed the representative guide curve set in Definition 1, possesses the smallest possible Chamfer distance to the original dense strand set compared to any alternative sampling method employing the same number of strands.

Note that our hierarchical hair representation does not cluster dense strands as in  using the \(k\)-medoids method. Instead, we use only the resulting medoids to sample guide strands. This ensures optimal guide strand sampling by leveraging the derived medoid set. The reason we avoid using explicit clusters to represent dense hair is that the interaction between dense and sparse guide strands is better captured through sampling and interpolation. Each dense strand is influenced by multiple nearby guide strands rather than just one from a single cluster. Consequently, our guide strand representation and interpolation mechanism provide a more sophisticated and accurate approach than the cluster-based model in , resulting in a more nuanced depiction of hair structure.

### Processing dense hair strands into hierarchy

We introduce the data structure of our doubly hierarchical hair data abstraction before it can be processed by neural models. We extract the low-frequency structure disentangled from high-frequency details for each strand as the principle growing directions, followed by optimally selecting a representative coarse subset of guide strands from the low-pass filtered dense hair data.

For each strand \(l\), we map the root point on the scalp manifold \(l(0)\) to its UV coordinate on the UV mapping of the scalp as \(r_{}^{2}\). We translate all strands to centralize the root points at the origin of the 3D space, and achieve the low-pass filtered curve with the first \(^{L}\) components in the DCT. We apply the DCT on the derivative \((t)=l(t+1)-l(t)\) instead of the original sequence of coordinates to ensure the same root position. To condense the signal, the reconstructed curve \(l^{L}_{i}\) in the spatial domain is resampled to a reduced resolution of \(2^{L}\) by fitting a cubic spline, adhering to the Nyquist sampling theorem  for precise discretisation. For high-frequency details, we maintain the signal in the form of DCT coefficients, denoted as \(^{H}^{(^{H}-^{L}) 3}\) A cut-off frequency \(^{H}\) is strategically selected to ensure compactness and exclusion of signal components exceeding \(^{H}\) as high-order noise. Practically, we set \(^{L}=8\) and \(^{H}=40\). Next, we employ \(k\)-medoids to identify the indices of the optimal coarse guides for each hairstyle, maintaining a consistent \(k=512\).

This way, we represent each strand as a tuple \((r,l^{L},^{H})\), where \(r\) is the root point's UV coordinate, \(l^{L}\) is the low-pass filtered strand curve, and \(^{H}\) encapsulates the high-frequency details as DCT harmonics. Strands within the optimal coarse guide curve set are denoted as \((r^{*},l^{L*})\) while high-frequency signals are excluded from guide curves. The learning process for hair generation is structured hierarchically: it commences with the generation of the guide curve set \(\{(r^{*},l^{L*})\}\), progresses to the densification of strands \(\{(r,l^{L})\}\), and culminates with the integration of high-frequency details \(\{^{H}\}\) when necessary, for hairstyles with heavy curliness or non-smoothness.

## 3 Learning to Generate Hierarchical Strand Hair

The generative process of our model is depicted in Fig. 4. We employ a variational autoencoder (VAE) [18; 31] to capture the distribution of hierarchical hair structures. The choice of VAE is motivated by its straightforwardness and proven effectiveness and efficiency in optimisation and sampling, while other categories of generative models are left to future exploration. The objective of a VAE is to learn a generative model \(p_{}(x,z)=p_{}(z)p_{}(x|z)\) for data \(x\) and latent variables \(z\). Since the true posterior is intractable, we approximate it using the latent encoder model \(q_{}(z|x)\) and optimise the variational lower bound (ELBO) on the marginal likelihood \(p(x)\): \(_{}=_{q_{}(z|x)}[ p_{}(x|z)]- (q_{}(z|x)\|p_{}(z))\), where the second term is the Kullback-Leibler divergence, quantifying the difference between the approximate and true posteriors.

### Generation of the optimal coarse guide hair with dual-branch network

The guide VAE for the generation of the optimal guide curve set \(\{(r^{*},l^{L*})\}\) with a fixed cardinality \(k\) is detailed in Fig. 4(b). The guide curves \(\{(r^{*},l^{L*})\}\) are conceptualized as a collection of data points, with the sequence \(l^{L*}\) transformed to a feature vector \(h^{L*}^{d_{h^{L*}}}\) using a 1D convolutional encoder embedded on the domain of UV coordinates \(r^{*}^{2}\). Our network architecture drawsinspiration from the Point-Voxel CNN (PVCNN) , leveraging its demonstrated efficiency and effectiveness in the domain of point set learning.

The architecture, akin to Point-Voxel CNN (PVCNN), utilizes a dual-branch approach to process point features: a PointNet branch and a convolution branch. Within the encoder, the point branch consists of a shared multilayer perceptron (MLP) that operates on all points, employing max pooling for aggregation as per the PointNet design . The convolution branch, on the other hand, begins by partitioning the UV space \(^{2}\) into a 2D grid of \(W H\) resolution. This step is analogous to the "voxelisation" in PVCNN but is executed in 2D. Each grid cell is assigned an averaged feature vector derived from all data points whose root coordinates fall within it, yielding a feature tensor of dimensions \(^{W H d_{k}L^{*}}\). This tensor is then processed by successive downsampling 2D convolution layers to distill a global feature. To enhance the feature representation in the encoder's convolution branch, we incorporate features from the sampled dense strands. These additional features are not utilized in the PointNet branch to avoid increasing the memory footprint, whereas the convolution branch feature is constrained to a fixed resolution. The mean and standard deviation for reparameterising the VAE latent code \(z\) is the aggregated features from both branches.

The decoder's architecture mirrors that of the encoder. The point feature is bilinearly sampled from the feature grid at the end of the convolution branch, following the hybrid grid-implicit representations . The interpolated point feature is then reconstructed into the low-passed curve sequence \(l^{L*}\).

Sampling root positions from generated density mapThe proposed dual-branch architecture learns generation of the curve sequence \(p(\{l^{L*}\}|\{r^{*}\})\) given root positions \(p(\{r^{*}\})\). Our objective extends to mastering the joint distribution \(p(\{r^{*},l^{L*}\})\), which we approach by decomposing \(p(\{r^{*}\},\{l^{L*}\})\) into \(p(\{r^{*}\}) p(\{l^{L*}\}|\{r^{*}\})\) as optimising the root coordinates and curve sequence

Figure 4: Generation pipeline overview: (a) During training, a small batch of dense strands and extracted guide curves are sampled from the UV map of the scalp to optimise the networks. (b) Guide curves are generated using a PVCNN VAE that incorporates 2D convolution and PointNet layers. (c) Dense strands are generated by aggregating features from the convolution grid via bilinear sampling and from neighbouring guide curves through graph convolution, with the densification module being jointly trained with the guide curve model. (d) High-frequency signals are refined using another dual-branch VAE, conditioned on the global and local latent features derived from the network responsible for generating the low-pass filtered principal strand signal.

jointly presents significant empirical challenges. While the dual-branch network is utilized for \(p(\{l^{L*}\}|\{r^{*}\})\), learning \(p(\{r^{*}\})\) could potentially be achieved with a distinct generative network, such as SetVAE . Nevertheless, we employ a trick to generate a \(W H\) grid map of root densities at the convolution branch decoder, achieved by tallying the roots within each grid cell. During inference, \(p(\{r^{*}\})\) is derived from this grid density map, before the sequence signal \(\{l^{L*}\}|\{r^{*}\}\) is generated. Details of the density map sampling are provided in the Appendix D. Although the density map sampling does not guarantee the exact same original root positions, empirically with a reasonable resolution of density map, we find that the resulting root positions correctly resemble the distribution of root points.

### Densification

To accommodate our innovative off-the-grid guide curve design, we introduce a densification module inspired by the concept of implicit neural representations [24; 21; 7; 39] This approach enables the generation of an arbitrary number of fine strands, independent of the scalp UV map's resolution, by leveraging hybrid grid [26; 5] and graph  representations. Given a randomly sampled query root position \(r_{i}_{}\), our goal is to derive fine stand features that align with the guide curve features within the VAE decoder. Features from the convolution branch \(^{L*}_{,j}\) are bilinearly sampled from the convolution branch, while for the PointNet branch, the query aggregates point-wise features \(^{L*}_{,j}\) from the query's neighbouring guide curve locations \(r^{*}_{j}_{r_{i}}\), employing a graph-hybrid implicit representation strategy  This method considers guide curves as anchor points and propagates signals to arbitrary query locations through message-passing graph convolution : \(^{L}_{,i}=_{r_{i}}|}_{j}w_{j}( (r_{i},r^{*}_{j}-r_{i},^{L*}_{,j})+^{L*}_{ ,j})\) with a two-layer MLP, and \(w_{j}=_{j}-r_{i}\|_{2}}_{j^{}}_{j^{}}-r_{i}\|_{2}}\) is a decaying weight. The resulted features are passed to the same 1D convolution decoder in the guide curve VAE model.

Note that the generation of the optimal guide curves and sampled non-guide strands is trained jointly. This pipeline is designed to be invariant to the sampling density, allowing for the generation of any desired number of dense strands during inference. This way, learning of densification can be regarded as in an auto-decoder learning scheme  conditioned on the coarse guide strands, and the information and varieties of dense strands is stored in the latent code and the network.

### Refinement of high-frequency details

Though the majority of hair samples in our dataset can be accurately represented by low-frequency signals, certain hairstyles exhibit high-frequency structures of curliness and noise. The model for learning the high-frequency components \(\{^{H}\}\) is a conditioned VAE with the same dual-branch architecture as the model that generates guide curves, except that the model is conditioned by concatenating the global latent from the guide model and the local dense strand feature. This way, we can sample different forms of high-frequency details of noise and curly structures from the same generated low-frequency components of strands.

## 4 Experiments

DataOur method is trained on a dataset comprising 658 synthetic strand hairstyles. Detailed information on the dataset and training procedures is available in the appendix.

Frequency decomposition and guide curve extractionOur evaluation focuses on the design choices made in modelling the hierarchical structure of hairstyles. As depicted in Fig. 6, we apply low-pass filtering with a designated cutoff frequency to the strands, either on the original coordinate sequence \(l(t)\) or its derivative \((t)\), the latter being the approach we adopt. In both scenarios, the Discrete Cosine Transform (DCT) outperforms the Discrete Fourier Transform (DFT) in reconstructing the signal within the same low-pass frequency band. The derivative form of the signal, being simpler to model, enhances the performance of both DFT and DCT, with DCT showing superior effectiveness. Furthermore, in Table 1, we conduct a comparative analysis of the \(k\)-medoids method against baselines for guide curve extraction, namely grid-based sampling, farthest point sampling (FPS), and \(k\)-means followed by projection of clustering centres to valid strands on the head scalp. The \(k\)-medoids approach demonstrably achieves a closer approximation to the dense strand set, as evidenced by the lower bidirectional chamfer distance (CD). These empirical findings align with our theoretical expectations, underscoring the efficacy of our methodological choices in capturing the intricate details of hair structure and texture.

**Reconstruction of guide curves with VAE** Our generative hair representation's efficacy is assessed using a set of 30 test examples, as detailed in Table 2 and illustrated in Fig. 5. Our method outperforms traditional grid-based hair representations, which map strands onto a scalp UV grid map and process them with purely 2D convolutional layers. The enhanced performance can be attributed to the rich representation derived from guide curves and our off-the-grid architectural approach. Direct comparison with closely related works such as Grom-Gen  and HAAR  is challenging due to lack of suitable evaluation metrics and the fact that they are trained on different datasets. Instead, we benchmark against a conceptually similar baseline representation: a grid-based Euclidean feature representation on the scalp UV-map, paired with a pretrained off-the-shelf strand VAE , which is the hair representation adopted by the prior work . This representation, however, falls short in performance. We posit that a strand codec VAE, originally designed as a generative prior for multiview hair reconstruction, may not be optimal for hairstyle-level generative tasks. By jointly optimising the strand encoder and decoder, our model circumvents the error accumulation inherent in the strand VAE approach, leading to more accurate and realistic hair generation.

**Densification** Building on the guide reconstruction experiments, we further assess the quality of densified strands against the reference hair, as quantified in Table 3. Our method outperforms the baseline approach, which merely duplicates strands from the closest guide curve. Notably, our model demonstrates the ability to generate high-quality dense strands informed by the generated coarse

   Grid-sample & FFS & \(k\)-means + projection & \(k\)-medoids \\ 
0.683 & 0.139 & 0.116 & **0.101** \\   

Table 1: Evaluation on extracted guide curves (CD (\( 10^{-4}\)) to reference hair).

    & Nearest-neighbour & Ours \\  CD (\( 10^{-4}\)) to GT dense hair & 0.311 & **0.209** \\   

Table 2: Evaluating reconstruction of guide curves.

Figure 5: The VAE reconstruction of guide curves, showcasing our method’s enhanced performance due to informative guide curves and enriched network representation.

Figure 6: Euclidean distance between low-pass filtered and original strands.

Figure 7: Dense strands from VAE generated guide curves, with our model yielding natural-looking hair, in contrast to the nearest neighbour baseline which displays artefacts.

guide priors, benefiting from the joint training with the guide curve VAE. As illustrated in Fig. 7, our model is capable of refining a set of VAE-generated guide curves into dense strands with fine textural details, in contrast to the nearest neighbour upsampling method, which introduces unnatural artefacts.

Creating varied high-frequency details in hairOur methodology extends to generating diverse high-frequency details for each hairstyle, building upon the foundation of guide curves and dense strands in low-frequency created by our model. By employing a high-frequency conditional VAE, we introduce a variety of high-frequency details, such as curliness and noise, into the hair strands. This process is vividly illustrated in Fig. 8, which showcases the capability of our approach to effectively disentangle and separately model the components of hair strand signals in both low- and high-frequency domains. This disentanglement allows for the nuanced recreation of hair textures, enhancing the realism and diversity of the generated hairstyles.

Additional resultsSome outcomes of our hair generation pipeline are showcased in Fig. 9. Additional experiments and ablation results are conducted and reported in the appendix.

## 5 Conclusion

We propose a doubly hierarchical generative representation for strand that captures the full spectrum of hair details from low-frequency shapes to dense and fine details. By leveraging frequency decomposition and optimal sampling, our model surpasses traditional grid representations in preserving the authenticity of hairstyles. Our model design ensures the generation of flexible hair strands in any amount and density that are both diverse and realistic, free from the constraints of grid-based systems. Future work could explore the integration of dynamic hair behaviours and the adaptation of our model to generate a wider variety of hair types, further enhancing the realism and applicability of virtual hair in diverse digital environments.

Acknowledgement We thank Georgina Linuesa Gomez for guidance on the knowledge of human hair types and artist grooming, Kai Qiu and Parth Nilesh for technical support, and anonymous reviewers for their valuable feedback. All data processing and use of models was conducted at CMU.

Figure 8: Generating diverse high-frequency details: Our high-frequency conditional VAE adds varied curliness and noise to hairstyles with guide curves and dense strands in low-frequency, demonstrating our model’s ability to disentangle low- and high-frequency strand geometry.

Figure 9: Strand-based hairstyles generated using our approach.