# VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution

Siobhan Mackenzie Hall

Fernanda Goncalves Abrantes

Hanwen Zhu

Grace Sodunke

Aleksandar Shtedritski

Hannah Rose Kirk

Oxford Artificial Intelligence Society, University of Oxford

###### Abstract

We introduce VisoGender, a novel dataset for benchmarking gender bias in vision-language models. We focus on occupation-related biases within a hegemonic system of binary gender, inspired by Winograd and Winogender schemas, where each image is associated with a caption containing a pronoun relationship of subjects and objects in the scene. VisoGender is balanced by gender representation in professional roles, supporting bias evaluation in two ways: i) _resolution bias_, where we evaluate the difference between pronoun resolution accuracies for image subjects with gender presentations perceived as masculine versus feminine by human annotators and ii) _retrieval bias_, where we compare ratios of professionals perceived to have masculine and feminine gender presentations retrieved for a gender-neutral search query. We benchmark several state-of-the-art vision-language models and find that they demonstrate bias in resolving binary gender in complex scenes. While the direction and magnitude of gender bias depends on the task and the model being evaluated, captioning models are generally less biased than Vision-Language Encoders. Dataset and code are available at https://github.com/oxai/visogender.

## 1 Introduction

Vision-language models (VLMs) are advancing rapidly and reaching ever-wider audience across numerous applications, such as classification and captioning, as well as text-to-image retrieval and generation. However, these models are pre-trained from uncurated image-text pairs scraped from the internet  and so, their outputs can perpetuate or amplify social biases . How the VLM is used determines the mechanisms of how biases transfer from pre-training to downstream representational and/or allocational harms . For example, a VLM used for image retrieval may skew towards returning more images of male doctors, thus reinforcing stereotypical associations between gender and career success; or a VLM used for captioning may more frequently misgender women or non-binary image subjects, aligning with a capability (un)fairness .

Despite the growing body of work on evaluating and mitigating the bias of VLMs , there is a dearth of specifically-designed benchmark datasets to evaluate the presence of social biases across downstream tasks (such as captioning or image retrieval): most prior work has measured biases using pre-existing image datasets such as FairFace  or COCO , despite their limited real-world transferability and spurious correlations . In this paper, we introduce the VisoGender benchmark for evaluating bias in VLMs. The design of VisoGender is inspired by two prior bodies of works. Firstly, we apply stress-testing of vision-linguistic reasoning capabilities of VLMs as in the Winoground benchmark  but introduce the dimension of social biases. Secondly, we adoptthe templated structure to test gender bias in occupational pronoun resolution from NLP research, but apply it to the vision-language domain--specifically the WinoGender  and WinoBias  frameworks, in turn inspired by Winograd Schema . To our knowledge, VisoGender is the first dataset to combine both of these contributions by stress-testing _gender bias_ in visual-linguistic reasoning and coreference resolution capabilities.

VisoGender contains images of a person depicted in an occupational role ("the doctor"), combined with either an object ("the stethoscope") or a participant ("the patient"). Each image is labelled by human annotators for the perceived gender presentation of the occupation and/or the participant, and the dataset is balanced across different genders occupying these roles. For each image, we construct natural language captions that use the perceived gender labels of the image subject(s) to derive a possessive binary pronoun relationship ("the doctor and his/her patient"). We test bias in two tasks: pronoun _resolution_ and image _retrieval_ (see Fig. 1 for summary). In the _resolution_ task, the model is provided with a single image (either of an occupation-object or occupation-participant scene) and must rank the likelihood of captions containing different gender pronouns. There are varying levels of difficulty in the resolution task--from a single person resolution in the occupation-object case; to two person resolution in the occupation-participant case, where either both subjects are perceived to have the same gender presentation (easier), or different gender presentations (harder). In the _retrieval_ task, the model is provided with a single gender-neutral caption and must retrieve images from a set containing professionals with different perceived genders. We measure _resolution bias_ using the gender accuracy gap in correct pronoun resolution (corresponding to capability fairness) and _retrieval bias_ using commonly-applied metrics such as Bias@K, MaxSkew and NDKL (corresponding to representational fairness).

We present preliminary results for six state-of-the-art Vision-Language Encoders (VLEs) models  and two state-of-the-art captioning models . We find that models still struggle to resolve pronoun relationships, especially when there are two people in the image of different perceived gender presentations (where performance is close to random). Our benchmark also recovers that (i) models display substantial accuracy gaps in resolving pronouns of masculine-versus feminine-presenting subjects, indicating the presence of resolution bias; and (ii) when provided with a neutral pronoun query ("the doctor and _their_ patient"), models predominately rank images of masculine-presenting subjects higher than those with a perceived feminine presentation, indicating a retrieval bias. We compare these results to U.S. Labor Force statistics (as in ) and find some correlations between model bias and societal occupational skew in the US labour market. Our findings demonstrate there is still substantial progress to be made in improving scene disambiguation for visual-linguistic reasoning, as well as reducing the gender gap in resolution performance and retrieval outcomes. Some caveats to our findings are needed: first, while we do find capability unfairness evidenced in differential performance across gender identity groups, this paper works within a hegemonic system of binary and stereotypical gender presentation that remains prevalent in Western constructions and perceptions of gender, where datasets typically originate . Models are likely to perform even more poorly in downstream tasks where there is additional complexity introduced by the inclusion of greater cultural diversity, as well as transgender, non-binary and gender-diverse individuals who are underrepresented here and in other vision-language datasets.

Figure 1: **Resolution of gender pronouns and retrieval with a neutral query. We resolve gender by (i) using zero-shot classification with Cross Models Encoders, such as CLIP, and (ii) next-token prediction with captioning models, such as BLIP. We have an additional simpler task to resolve the gender of a single person, e.g., with a template “The doctor and her / his stethoscope”.**

Second, our benchmark is designed to measure poor performance on the two tasks, thus identifying potential downstream harms from occupational gender bias in VLMs. However, being good at the task requires not only advanced visual-linguistic reasoning but also accurate gender prediction, a capability that can be concerning if misused for surveillance purposes. We discuss both of these limitations and concerns in Sec. 5. The pace at which VLMs are being developed is only set to grow in coming years--VisoGender provides a much-needed benchmark to evaluate their potential downstream harms before large-scale deployment.

## 2 Related Works

**Bias in coreferences in NLP** Coreference resolution aims to identify which mentions in a natural language document link to the same real-world entity . In the past decade, significant progress has been made moving from rule-based systems and expert knowledge , to statistical models [33; 34] and deep neural networks [35; 36; 37; 38; 39]. Pronoun resolution involves linking a noun such as "doctor" to a pronoun in the sentence. Biases have been identified, with respect to machine translation , non-binary pronouns , and favouring masculine entities when resolving gender ambiguous cases . Our work is most similar to gender pronoun resolution tasks based on Winograd schemas , like Winogender  and WinoBias  which investigate occupational-related biases. Both of these works demarcate "hard" and "easy" cases based on (anti-)stereotypical gender-occupation associations as measured relative to U.S. Labor Force statistics. We extend this work to the vision-language domain. In our resolution task, we modify the typical Winograd scheme because the correct resolution is unambiguous, i.e., there is a correct caption (and pronoun) for a corresponding image. However, our retrieval task is a closer vision-language analogy to [19; 20] because there is no groundtruth for a "correct" ranking of images given a gender-neutral search query.

**Evaluating visual reasoning** There is an emerging body of work on visual reasoning tasks , such as VQA [44; 45; 46], visual word sense disambiguation , compositionality understanding [48; 49; 50], comprehension  or visual entity linking . Most similar to our work, Winoground  evaluates visio-linguistic compositional reasoning by tasking a model to match two images with two captions containing the same set of words, only in a different order--such as "there is a mug in some grass" vs. "there is some grass in a mug". The task is challenging, with state-of-the-art VLMs rarely performing better than chance, though  demonstrate some of these failures may be due to atypical images in the dataset. Our vision-linguistic stress-tests are inspired by adapting Winoground to social biases, but a key difference is that our caption-image pairs do not contain the exact same set of words--for example, matching "the doctor and her patient" versus "the doctor and his patient".

**Measuring bias in vision-language models** Measuring the social bias of VLMs is a growing area of research. While early works measure misclassification rates into harmful categories [9; 54], more recent methods investigate face-to-text retrieval [11; 10; 55; 56], or captioning . However, these approaches rely on off-the-shelf datasets, such as COCO , which have been shown to contain spurious correlations  and thus are not suitable for evaluating model bias . Similar to , we balance our dataset by gender across different occupational settings, but instead using naturally-occuring images rather than synthetic edits.

## 3 The VisoGender Benchmark

The VisoGender dataset contains 690 images of people in various occupational settings, where each image is annotated for the perceived gender presentation of the subject(s) in the image. We use these annotations to construct a templated caption of an inferred pronoun relationship. The dataset covers 23 unique occupations in a hierarchical taxonomy. Each occupation appears in the dataset with two template forms--either as a single person in the image with a possessive pronoun to an object ("the doctor and his/her stethoscope"), or as one of two people in the image with a possessive pronoun to a participant ("the doctor and his/her patient") (see Sec. 3.2). A summary of the dataset is presented in Tab. 1. In the following subsections, we introduce the terminology used throughout our paper (Sec. 3.1); describe the dataset (Sec. 3.3); and provide detail of the templates (Sec. 3.2). We then summarise the two types of VLMs which are compatible with VisoGender (Sec. 3.4); and finally, define the two tasks through which we measure model bias (Sec. 3.5).

### Terminology

* _Perceived gender presentation_: We do not have direct communication with image subjects, so we cannot know the pronoun with which the subjects identify. Instead of referring directly to gender identity, we use perceived gender presentation to indicate that an inference is made by an external human annotator. We do not make any claims of assigning a person's gender identification, which we recognise as an individual's personal experience of gender , and acknowledge that our labels may differ from the subject's personal identity.
* _Masculine and feminine presentation_: We opt to use terms for gendered characteristics (masculine and feminine) over terms for biological sex (male and female). These perceived gender labels (denoted as "M" for masculine and "F" for feminine unless otherwise stated), are assigned by an external human annotator based purely on their perception of the person's visual presentation such as facial features, stereotypical clothing, hair or other signals. We recognise that gendered characteristics from masculine to feminine lie on a spectrum and assigning one dominant presentation over another is subject to annotator internal biases, which are in turn endogenous to lived experience. Please see our positionality statement for more information (Sec. 5.3).

### Templates

Each templated caption contains three components, adapted from Winogender :

* Occupation: a person refered to by an occupational noun and definite article, "the doctor"
* Pronoun: a pronoun corresponding to the perceived gender presentation of the occupation in the image, e.g., "her" or "his"
* _either_ Object: a noun corresponding to typical professional items, e.g., "the stethoscope"
* _or_ Participant: A second person in a typical professional relationship with the occupation, e.g., "the patient"

For occupations, we use the list from , but remove (i) occupations without a clear possessive pronoun relationship between the occupation and participant, e.g., "the plumber and their houseowner" is not semantically correct; and (ii) occupations without sufficient open-domain images across genders (for both men and women occupying the occupation and participant roles). We classify the remaining occupations into a hierarchical taxonomy to permit aggregate group-wise analysis: **Sector** describes the general field, and includes _education_, _medical_, _office_, _retail_ and _service_; **Specialisation** describes subcategories within the sector, where, for example, _services_ includes _food services_, _fashion_, _animal_ or _household_; and finally, **Occupations** are nested within specialisations, where, for example, _food services_ contains _waiter_, _bartender_, and _baker_. Similar to , we match U.S. Labor Force statistics on the percentage of men working in each occupation to compare model biases to occupational skew in the real-world US labour market. When comparing our results to these statistics, we will use the perceived masculine gender presentation counts as men ("M"), and the remaining percentages to be for women ("W"). The full taxonomy and list of occupations is presented in the Supplementary Materials. We also source the list of participants from  but replace any references to children as participants and in some cases, make modifications for a more natural possessive pronoun, e.g., "the lawyer and the witness" becomes "the lawyer and their client". For objects, we manually define a typical professional item for each occupation. Using these components, we construct three templates (subtasks) of increasing difficulty for coreference resolution:

* **Single subject:** The template of captions is "The {occupation} and {his/her} {object}", e.g., _"the doctor and her stethoscope"_. For each occupation, we collect 10 occupation-object images, 5 for each gender. Here, models only need to resolve the pronoun of one subject in the image, thus testing simple visual-linguistic reasoning.
* **Two subjects of the same perceived gender presentation:** The template of the captions is "The {occupation} and {his/her} {participant}" e.g. _"the doctor and her patient"_. In this case, the perceived gender presentation of the occupation and the participant are the same (both masculine or both feminine). Per occupation, we collect 5 images for each of these two cases (M-M, F-F). Here, the model must resolve the inferred pronouns of two subjects but assigning which subject is the occupation and which is the participant does not affect the prediction.

* **Two subjects of different perceived gender presentations:** Finally, we use the same occupation-participant template but now the participant and the occupation are of opposite perceived gender presentations (one masculine and one feminine). Per occupation, we collect 5 images for each of case (M-F, F-M). Here, the model must resolve the perceived gender of the subject, _and_ infer from image context which is the occupation and which is the participant to infer the pronoun.

### Dataset Collection

The VisoGender dataset comprises image URLs with annotations for the occupation noun, the participant or object noun, and the perceived gender presentations of the occupation and participant. These annotations can be used to reconstruct the templated captions. Data collection, which includes data labelling, was carried out by the authors of the paper from March to May 2023 on a variety of image databases and search providers, such as Pexels and Google Image Search. We followed a set guidelines to specify exclusion and inclusion criteria, detailed in the Supplementary Materials.1

We ensure that there are no duplicate images (no overlaps between occupations) and no invalid URLs across the dataset. In the early stages of data collection, we used the entire list of occupations from . However, we only include those with at least 20 viable URLs (5 per gender pair) for occupation-participants and 10 viable URLs for occupation-object (5 per gender). The image curation process (and availability of viable URLs) is dependent on the retrieval of different gendered roles across occupational search queries and so therefore compromised by inherent representational biases in these search engines. We mitigate effects of imbalance across genders by only including occupations with a full set of images (equal images across all gender pairs) but this may introduce a sample selection bias to the included occupations. Furthermore, inferring gender from an image depends ongrained biases of the dataset curators. We discuss limitations and biases of data collection in Sec. 5.3, and suggest possible expansions in the future with further resources e.g., partnering with a stock photo company. The dataset is accompanied by Data Clause (which details the Licence and Terms of Use) as well as a Datasheet for Datasets  in the Supplementary Materials.

### Two Supported Types of Vision-Language Models

VisoGender is designed to accommodate two types of VLMs. Here we discuss their properties, and how bias can be measured in common use cases.

**Vision-Language Encoders (VLEs)** VLEs, such as CLIP, have separate vision and language encoders and are trained to jointly match images and text. Given an image \(i^{3 H W}\) and text \(t\), a VLE outputs a score \(s(i,t)\) that expresses the degree of compatibility between the image and text. The first common use case of VLEs is zero-shot classification of images . This is done by providing a query image \(i_{q}\) and text prompts \(t_{n},n 1,,N\). For example, if we wish to zero-shot classify the perceived gender of a doctor in an image using pronoun resolution, we can provide text prompts _"This is an image of a doctor and [this, her] notebook"_, and select the pronoun with the highest compatibility score to the image. Such a classifier can be considered biased if, for example, it more accurately infers the pronoun of one perceived gender presentation in some occupations. The

    &  &  \\   & } &  &  &  & Images per Occ. & Images per P.P.Gender Pair & Images per P.P.Gender Pair & Images per P.P.Gender Pair & Overall \\ 
**Single person** & & & & & & & \\ (occupation-object) & 5 & 13 & 23 & [M, F] & 10 & 115 & 5 & 230 \\
**Two-person** & & & & & & & \\ (occupation-participant) & & & & & & & \\   

Table 1: **VisoGender dataset summary**, showing the hierarchy of included Sectors, Specialisations, and Occupations; the Perceived Presentation of Gender (P.P.Gender) pairs per template type, and the counts of images within each split of the dataset.

second common use case of VLEs is for text-to-image retrieval . Given a text query \(t_{q}\), images \(i_{n},n 1,,N\) and a query size \(K\), we select the \(K\) images with the highest compatibility score to the text prompt. In this setting, the model can be biased if, for example, when searching for a given occupation, people from a given demographic are over or under-represented in the top \(K\) retrieval results.

**Captioning models** Captioning models are most commonly trained to autoregressively predict a text caption given an image. For an image \(i\) and, optionally, a partially completed caption with N tokens \(c=[t_{1}, t_{N}]\), the model outputs the probability for the next token \(t_{N+1}\) as \(p_{}(t_{N+1}|i,t_{1},,t_{N})\). Similar to VLEs, we can apply the captioning model to infer the perceived gender of a subject in an image via pronoun resolution. We first supply a query image \(i_{q}\) (say, an image of a doctor) and a caption \(c_{q}\) like "_An image of a doctor and_". We then inspect the probability distribution for the next token \(t_{n}\), denoted by \(p_{}(t_{n})=p_{}(t_{m}|i_{q},c_{q})\). We can now compare the probabilities \(p_{}(t_{n})=\) "_her_" and \(p_{}(t_{n})=\)"_his_", choosing the one with the higher score as the model's selection. It has been demonstrated that comparing token probabilities is a more reliable measure of a generative language model's performance compared to free generation , and such templates have been successfully used to evaluate bias in LLMs .

### Two Angles of Model Bias

The VisoGender setup has the flexibility to measure model bias in two ways:

**Resolution task**_The resolution task considers a single image with perceived gender label and matches it to multiple candidate captions containing different gender pronouns._ For example, we start with an image containing a doctor perceived as feminine, and specify the set of candidate captions as "the doctor and her/his patient". We define **resolution accuracy**, \(RA\), as the percentage of correctly resolved pronouns. This can be calculated over all occupations, across main occupation categories, or per occupation. For a given occupation \(o O\) and a perceived gender presentation \(g\) (either masculine \(m\) or feminine \(f\)), we have:

\[RA_{g}(o)=}{}\]

An unbiased outcome is one where the model resolves both perceived gender pronouns equally, i.e., \(RA_{m}(o)=RA_{f}(o),\  o O\). We now define **resolution bias** as the gender resolution accuracy gap

\[(o)=RA_{m}(o)-RA_{f}(o),\] (1)

where a positive value of \(\) shows a model more accurately resolves masculine-presenting subjects, and vice versa. Our definition of resolution bias measures a form of capability fairness, i.e., whether a system performs equally well across subgroups . This task is applicable to both types of VLMs.

**Retrieval task**_The retrieval task considers a single gender-neutral caption for a given occupation and matches it to multiple images containing subjects with different perceived gender presentations_ from the same occupation. For example, we start with the caption "the doctor and their patient" and define the set of candidate images as containing 50% images of doctors who are perceived as masculine and 50% who are perceived as feminine. Given there is no groundtruth for a "correct" ranking of images for a gender-neutral caption, we cannot define a **retrieval accuracy metric**. For defining **retrieval bias**, we use 3 commonly used metrics--_Bias@K_, _Skew@K_[64; 11] and _NDKL_[65; 64]. Bias@K measures the overrepresentation of men in the top K retrieval results. Skew@K measures the difference between the desired proportion of image attributes and the observed one, and MaxSkew@K is the maximum Skew among all attributes, or the "largest unfair advantage"  belonging to images of any perceived gender presentation. NDKL is a ranking measure that measures the distance from a fair distribution. For further definitions and discussions of these, please refer to the Supplementary Materials. Our definition of retrieval bias measures a form of representational fairness, i.e., with a gender-balanced set of images and a gender-neutral caption, whether each perceived gender group have equal chances of being retrieved. The retrieval task is only applicable to VLEs.

## 4 Results

For the resolution task, we evaluate six VLEs--CLIP , OpenCLIP  (trained on LAION 2B and 400M ), SLIP , DeCLIP , FILIP  (last 3 trained on YFCC-15M ); and two state-ofthe-art captioning models--BLIP-2  and GIT . For two candidate models (CLIP and BLIP-2, which are among the most downloaded models in the respective model family on Huggingface), we go into more detail by investigating their resolution capabilities and resolution biases, which are also compared to U.S. Labor Force Statistics (Sec. 4.1). We ablate the VisoGender setup by changing the order of templates and including a neutral caption. For the retrieval task, we benchmark the same six VLEs on the resolution task. Captioning models are not compatible with the retrieval task. We also compare retrieval bias metrics with U.S. Labor Force Statistics. For all VLEs, we use ViT-B/32 encoders, and for GIT we use the GIT-Large model. We show more detailed analysis for retrieval bias for CLIP and BLIP-2. We present ablation studies and error bars robustness analysis for both tasks in the Supplementary Materials.

### Resolution Task

We present results for the resolution task in Tab. 2, disaggregated by different levels of difficulty. We report the mean resolution accuracy \(RA_{}\) for each difficulty level, together with the resolution bias or accuracy gap \(\). We highlight the difference between model capabilities and model bias--here we evaluate both, where the latter is the gap between model capabilities for perceived genders.

**Evaluating resolution capabilities** As expected, the resolution accuracy is highest when there is one person in the image, and lowest when there are two people of different perceived gender in the image. The accuracies for the latter are consistently worse than random chance, pointing at the models' inability to reason about scenes with multiple people and attributes associated with each of them. This confirms the findings of prior works that conclude that VLMs are not capable of complex visio-linguistic  or spatial  reasoning. Captioning models are better than, or on par with, VLEs for all levels of difficulty. In Fig. 2, we see that BLIP-2 outperforms CLIP on all perceived presentation of gender splits in the dataset. From Tab. 2 we also see that models with better zero-shot classification accuracy on ImageNet  tend to have a better overall resolution accuracy.

**Evaluating resolution bias** From Tab. 2, we see that models tend to exhibit a larger resolution accuracy gap with more difficult subtasks, such as two people with different genders, where there is higher variation and almost random predictions across models. In Fig. 2, we compare the resolution bias, or accuracy gap, for CLIP and BLIP-2. We see that (i) CLIP shows a larger accuracy gap, and (ii) CLIP is more biased towards correctly resolving pronouns for feminine-presenting subjects, whereas BLIP-2 correctly resolves pronouns for masculine-presenting subjects more often. For further analysis and per-occupation results, see the Supplementary Materials.

To interpret the results in a real-world context, we compare U.S. Labor Force Statistics on on proportions of different genders in occupations with resolution bias in Fig. 3. These statistics only account for binary gender, and we have adjusted our perceived masculine and feminine gender presentation counts to be for "men" and "women" respectively. We measure the correlation in the absolute values (with Pearson's R) and correlation in ranked values (with Kendall-Tau), i.e., testing for the monotonicity of relationship between model bias and societal occupation skew . While we see no pattern for the bias of CLIP, the accuracy resolution gap of BLIP-2 correlates with the U.S.

    &  &  &  &  &  &  \\  & & RA\({}_{}\) & \(\) & RA\({}_{}\) & \(\) & RA\({}_{}\) & \(\) & RA\({}_{}\) & \(\) & RA\({}_{}\) & \(\) & \\  CLIP  & 0.75 & 0.92 & -0.14 & 0.57 & -0.27 & 0.79 & -0.18 & 0.36 & -0.35 & 63.2 \\ OpenCLIP\({}_{2 B}\) & 0.78 & 0.96 & -0.07 & 0.60 & -0.37 & 0.77 & -0.42 & 0.44 & -0.32 & 66.2 \\ OpenCLIP\({}_{400M}\) & 0.74 & 0.84 & -0.27 & 0.64 & -0.29 & 0.80 & -0.26 & 0.46 & -0.33 & 62.9 \\ SLIP  & 0.60 & 0.77 & 0.14 & 0.43 & 0.14 & 0.51 & 0.12 & 0.34 & 0.17 & 34.3 \\ DeCLIP  & 0.70 & 0.87 & 0.06 & 0.52 & -0.17 & 0.74 & -0.14 & 0.29 & -0.19 & 43.2 \\ FILIP  & 0.45 & 0.41 & 0.06 & 0.49 & 0.36 & 0.49 & 0.36 & 0.50 & 0.37 & 39.5 \\  BLIP-2  & 0.84 & 0.92 & -0.09 & 0.76 & 0.07 & 0.93 & 0.06 & 0.60 & 0.09 & — \\ GIT  & 0.84 & 0.96 & -0.07 & 0.72 & -0.27 & 0.97 & -0.07 & 0.48 & -0.47 & — \\   

Table 2: **Resolution bias. We present resolution accuracy averaged for masculine and feminine gender presentations, as well as the resolution accuracy gap \(\), as defined in eq. (1). “Same perceived presentation of Gender (P.P.Gender)” and “Different P.P.Gender” are images with two people from the same or different P.P.Gender, respectively. A positive gap \(\) denotes better resolution accuracy for masculine-presenting subjects. We also present reported zero-shot classification accuracy on ImageNet .**proportions--for occupations with fewer women, such as "engineer", the model correctly resolves men more often than women, and vice versa.

### Retrieval Bias

We evaluate VLMs on retrieval bias in Tab. 3. We see that _all_ models have positive Bias@5 and Bias@10 values, which suggests that images of masculine-presenting subjects in professional settings are retrieved more often than images of feminine-presenting subjects, despite the candidate images always being gender-balanced.

## 5 Discussion

### Key Findings

**Models struggle to resolve pronouns in the presence of both perceived presentations of genders** We found that all VLEs show close to random performance on gender resolution when there are people

Figure 3: **Resolution bias relative to U.S. Labor Force Statistics. We compare the gender accuracy Gap per occupation to U.S. Labor Force statistics data. While there are no patterns for CLIP, BLIP-2 shows a bias similar to the real-world data.**

Figure 2: **Resolution accuracy (top) and resolution bias (bottom). Top: We compare the resolution accuracy for different perceived presentations of gender pair combinations. The lowest RA scores occur in cases with two people with different perceived gender presentations. Bottom: We present the resolution bias (Gap) for either a single person or two people. A bigger Gap score indicates a bias towards one perceived gender presentation. A positive Gap score shows a bias towards masculine-presenting subjects**

of different perceived gender presentation in the scene. This hints at insufficient visuo-linguistic capabilities for handling complex scenes in current VLMs.

**Captioning models have a higher accuracy and smaller accuracy gap between genders** We find that captioning models outperform VLEs on all subtasks. We attribute this to the way resolution is done in captioning models--the pronoun of the subject is extracted using the start of the template and next token prediction. Meanwhile, VLEs need to rely on a global cls text feature, which seems to not capture the nuanced difference between entities in the sentence.

**Resolution and retrieval bias are not in the same direction** Across models, there is not a consistent pattern of bias direction--VLEs are more accurate at resolving "her" pronouns, while BLIP models are more accurate for "his" pronouns. In contrast, we find that all VLEs are predominately biased towards retrieving images of masculine-presenting subjects. This highlights a risk of representative harm in deploying VLEs in image search systems.

### Ethical Considerations

**The harms of performing poorly and exceptionally well on VisoGender** In developing this benchmark, we recognise that performing at either end of the spectrum can have harmful side effects. Performing poorly on VisoGender can lead to increased gender bias through the use of VLMs when considering the representation of stereotypically feminine binary gender, which is already heavily discriminated in many historically male-dominated industries. For example, if an automatic captioning VLM in a downstream application repeatedly misgenders doctors as "he", this weakens the representation of "women" as doctors too. The risk of erasure and misgendering via pronouns in caption assignment also harms the LGBTQIA+ community. An evaluative tool such as VisoGender is designed to flag a model's bias prior to deployment. However, we also recognise that in order to do well on VisoGender, a model must perform well not only at scene disambiguation (based on the image subjects' occupational roles) but also have capabilities for recognising (binary) gender. If the Visogender dataset is misused, and does not comply with the terms of use prohibiting its use for training (see Supplementary Materials), there is a potential for increased gender recognition capabilities that can contribute to the development of automatic gender recognition technology. We recognise that the development of this technology denies people--especially transgender, non-binary, and gender-diverse individuals--their dignity, respect, and sometimes safety to exist in public and private spaces. We absolutely condemn any use of VLMs for automatic gender recognition in surveillance use cases.

**Dataset collection with regards to privacy and consent** All images in the Visogender benchmark dataset are collected and used within the scope of their Creative Commons and royalty-free associated licences. However, we respect that these images depict real-life people, that may be misgendered in the development of this benchmark. Importantly, we do not make any claims to correctly assign a person's gender identification and we have included mechanisms for all data subjects to amend the labels and/or request removal of their images.

### Limitations

**Subjective assessment when creating datasets** The authors note this work is necessarily influenced by their respective identities. In a random order with psuedo-anonymised identifiers, we present our positionality: A1 is a White Bulgarian cisgendered man; A2 is a White British cisgendered woman;

    &  &  &  &  &  \\ Model & Mean & \(\) & Mean & \(\) & Mean & \(\) & Mean & \(\) & Mean & \(\) \\  CLIP  & 0.11 & 0.38 & 0.16 & 0.22 & 0.27 & 0.15 & 0.18 & 0.13 & 0.19 & 0.07 \\ OpenCLIP\({}_{2B}\) & 0.10 & 0.44 & 0.08 & 0.23 & 0.29 & 0.17 & 0.18 & 0.11 & 0.18 & 0.07 \\ OpenCLIP\({}_{200M}\) & 0.17 & 0.47 & 0.11 & 0.22 & 0.33 & 0.18 & 0.16 & 0.13 & 0.19 & 0.07 \\ SLIP  & 0.06 & 0.52 & 0.00 & 0.24 & 0.32 & 0.21 & 0.17 & 0.12 & 0.19 & 0.09 \\ DeCLIP  & 0.11 & 0.40 & 0.15 & 0.26 & 0.28 & 0.16 & 0.20 & 0.14 & 0.17 & 0.07 \\ FILIP  & 0.01 & 0.43 & 0.03 & 0.26 & 0.29 & 0.16 & 0.17 & 0.13 & 0.18 & 0.07 \\   

Table 3: **Retrieval bias.** We present mean and standard deviation across all occupations. Positive Bias@K shows more images of men were retrieved.

A3 is a Black British cisgendered woman; A4 is a White South African cisgendered woman; A5 is a Chinese cisgendered man; and A6 is a White Brazilian cisgendered woman. Four authors are pursuing postgraduate degrees and two are pursuing undergraduate degrees, all at the University of Oxford. We draw on our experience in measuring and mitigating social biases in VLMs. Further, when designing, conducting and writing up this research, we consulted closely with domain experts in Gender Studies and members of the LGBTQIA+ community. We acknowledge that visual markers of gender presentation may not reflect a subject's self-identified gender as gender presentation does not necessarily align or reflect in a binary manner with one's sex, pronouns or identity. As such, we acknowledge that gender and gender identity is fluid and exists on a spectrum that is generally misrepresented by binary distinctions. However, irrespective of the societal model of gender, bias exists and leads to representational and allocational harms . This work is not able to address the diversity of gender and other intersectional characteristics that come into the societal image of a typical person doing a certain job as well. However, we attempt at creating a benchmark that could help detect a single level of bias, which is the difference between typical Western presentation of someone perceived as masculine- or feminine-presenting in an occupational role. We advocate that this work is extended to include more genders and avoid erasure of non-binary individuals represented across occupations . The codebase is designed to be flexible to include neorpronouns in the future, in order to ensure these systems do not perform poorly when faced with data related to underrepresented communities .

**Stacking biases from the internet** We source images from a variety of search platforms (such as Google Image Search) and image hosting sites. While we balance included occupations across perceived presentation of gender search terms, those that we leave out are not "missing at random" due to biases that already exist in images on the internet. We could not find enough images for some occupations, e.g., there were not 5 images of a female-presenting plumber and a male-presenting client that met our criteria for data accessibility and format.

**Dataset representation** This dataset is only intended for evaluation purposes and, as such, requires fewer images than if it were used for training. However, the dataset is still relatively small, and excludes some "non-random" occupations omitted due to existing biases in images on the internet. This exclusion can, in turn, introduce bias into the gender and role depictions. It was beyond our means to partner with a StockImage provider, such as Getty Images , but this could be an avenue in future work to expand dataset size and include self-identified pronouns in order to counteract some of the aforementioned image availability biases. Future work could also augment the dataset with synthetic data from generative VLMs [12; 72].

## 6 Conclusion

We introduced VisoGender, a novel dataset for benchmarking social biases in VLMs for both pronoun resolution and retrieval settings. On some parts of the benchmark, we demonstrated that current state-of-the-art models perform no better than random chance, and that they do not perform equally well for resolving for both masculine and feminine gender presentations, nor give equal retrieval likelihood to images of masculine- or feminine-presenting professionals. There is significant headroom for improvement both in the reasoning abilities of VLMs, and in the gender gap of their abilities, when it comes to complex scenes with multiple humans. We hope this work encourages the benchmarking of future VLMs, so the risk of downstream harms and negative biases can be measured, compared and mitigated.