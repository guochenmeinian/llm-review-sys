# Learning to Control the Smoothness of GCN Features

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The pioneering work of Oono & Suzuki [ICLR, 2020] and Cai & Wang [arXiv:2006.13318] analyze the smoothness of graph convolutional network (GCN) features. Their results reveal an intricate empirical correlation between node classification accuracy and the ratio of smooth to non-smooth feature components. However, the optimal ratio that favors node classification is unknown, and the non-smooth features of deep GCN with ReLU or leaky ReLU activation function diminish. In this paper, we propose a new strategy to let GCN learn node features with a desired smoothness to enhance node classification. Our approach has three key steps: (1) We establish a geometric relationship between the input and output of ReLU or leaky ReLU. (2) Building on our geometric insights, we augment the message-passing process of graph convolutional layers (GCLs) with a learnable term to modulate the smoothness of node features with computational efficiency. (3) We investigate the achievable ratio between smooth and non-smooth feature components for GCNs with the augmented message passing scheme. Our extensive numerical results show that the augmented message passing remarkably improves node classification for GCN and some related models.

## 1 Introduction

Let \(G=(V,E)\) be an undirected graph with \(V=\{v_{i}\}_{i=1}^{n}\) and \(E\) be the set of nodes and edges, resp. Let \(^{n n}\) be the adjacency matrix of the graph with \(A_{ij}=_{(i,j) E}\), where \(\) is the indicator function. Furthermore, let \(\) be the following (augmented) normalized adjacency matrix

\[(+)^{-}(+)(+)^{ -}=}^{-}}}^{- },\] (1)

where \(\) is the identity matrix, \(\) is the degree matrix with \(D_{ii}=_{j=1}^{n}A_{ij}\), and \(}:=+\) and \(}:=+\). Starting from the initial node features \(^{0}:=[(_{1}^{0})^{},,(_{n}^{0})^{}]^{} ^{d n}\) with \(_{i}^{0}^{d}\) being the \(i^{th}\) node feature vector, the graph convolutional network (GCN)  learns node representations using the following graph convolutional layer (GCL) transformation

\[^{t}=(^{t}^{t-1}),\] (2)

where \(\) is the activation function, e.g. ReLU , and \(^{l}^{d d}\) is learnable. GCL smooths feature vectors of the neighboring nodes. The smoothness of features helps node classification; see e.g. , resonating with the idea of classical semi-supervised learning approaches . Accurate node classification requires a balance between smooth and non-smooth components of GCN features . Besides graph convolutional networks (GCNs) stacking GCLs, many other graph neural networks (GNNs) have been developed using different mechanisms, including spectral methods , spatial methods , sampling methods , and the attention mechanism . Many other GNN models can be found in recent surveys or monographs; see, e.g. .

Deep neural networks usually outperform shallow architectures, and a remarkable example is convolutional neural networks . However, this does not carry to GCNs; deep GCNs tend to performsignificantly worse than shallow models . In particular, the node feature vectors learned by deep GCNs tend to be identical over each connected component of the graph; this phenomenon is referred to as _over-smoothing_, which not only occurs for GCN but also for many other GNNs, e.g., GraphSage  and MPNN . Intuitively, each GCL smooths neighboring node features, benefiting node classification . However, stacking these smoothing layers will inevitably homogenize node features. Algorithms have been developed to alleviate the over-smoothing issue of GNNs, including decoupling prediction and message passing , skip connection and batch normalization , graph sparsification , jumping knowledge , scattering transform , PairNorm , and controlling the Dirichlet energy of node features .

From a theoretical perspective, it is proved that deep GCNs using ReLU or leaky ReLU activation function learn homogeneous node features . In particular,  shows that the distance of node features to the eigenspace \(\) - corresponding to the largest eigenvalue 1 of matrix \(\) in (1) - goes to zero when the depth of GCN with ReLU goes to infinity. Meanwhile,  empirically studies the intricate correlation between node classification accuracy and the ratio between smooth and non-smooth components of GCN node features, i.e., projections of node features onto eigenspace \(\) and its orthogonal complement \(^{}\), resp. The empirical results of  indicate that _both smooth and non-smooth components of node features are crucial for accurate node classification_, while the ratio between smooth and non-smooth components to achieve optimal accuracy is unknown and task-dependent. Furthermore,  proves that the Dirichlet energy - another smoothness measure for node features - goes to zero when the depth of GCN with ReLU or leaky ReLU goes to infinity.

A crucial step in the proofs of  is that ReLU and leaky ReLU reduce the distance of feature vectors to \(\) and their Dirichlet energy. However,  points out that _over-smoothing - characterized by the distance of features to eigenspace \(\) or the Dirichlet energy - is a misnomer_; the real smoothness should be characterized by a _normalized smoothness_, e.g., normalizing the Dirichlet energy by the magnitude of the features. _The ratio between smooth and non-smooth components of node features - studied in  - is closely related to the normalized smoothness_. Nevertheless, analyzing the normalized smoothness of node features learned by GCN with ReLU or leaky ReLU remains an open problem . Moreover, it is interesting to ask if analyzing the normalized smoothness can result in any new understanding of GCN features and algorithms to improve GCN's performance.

### Our contribution

We aim to (1) establish a new geometric understanding of how GCL smooths GCN features and (2) develop an efficient algorithm to let GCN and related models learn node features with a desired normalized smoothness to improve node classification. We summarize our main contributions towards achieving our goal as follows:

* We prove that there is a high-dimensional sphere underlying the input and output vectors of ReLU or leaky ReLU. This geometric characterization not only implies theories in  but also informs that adjusting the projection of input onto eigenspace \(\) can alter the smoothness of the output vectors. See Section 3 for details.
* We show that both ReLU and leaky ReLU reduce the distance of node features to eigenspace \(\), i.e., ReLU and leaky ReLU smooth their input vectors without considering their magnitude. In contrast, when taking the magnitude into account, ReLU and leaky ReLU can increase, decrease, or preserve the normalized smoothness of each dimension of the input vectors; see Sections 3 and 4.
* no matter how we adjust the input by changing its projection onto \(\). In contrast, adjusting the projection of input vectors onto \(\) can change the normalized smoothness of output to any desired value; see details in Section 4.
* for both homophilic and heterophilic graphs
- using a few of the most representative GCN-style models. See Sections 5 and 6 for details.

As far as we know, our work is the first thorough study of how ReLU and leaky ReLU affect the smoothness of node features both with and without considering their magnitude.

### Additional related works

Controlling the smoothness of node features to improve the performance of GCNs is another line of related work. For instance,  designs a normalization layer to prevent node features from becoming too similar to each other, and  constrains the Dirichlet energy to control the smoothness of node features without considering the effects of nonlinear activation functions. While there has been effort in understanding and alleviating the over-smoothing of GCNs and controlling the smoothness of node features, there is a shortage of theoretical examination of how activation functions affect the smoothness of node features, specifically accounting for the magnitude of features.

### Notation and Organization

**Notation.** We denote the \(_{2}\)-norm of a vector \(\) as \(\|\|\). For vectors \(\) and \(\), we use \(,\), \(\), and \(\) to denote their inner, Hadamard, and Kronecker product, resp. For a matrix \(\), we denote its \((i,j)^{th}\) entry, transpose, and inverse as \(A_{ij}\), \(^{}\), and \(^{-1}\), resp. We denote the trace of \(^{n n}\) as \(()=_{i=1}^{n}A_{ii}\). For two matrices \(\) and \(\), we denote the Frobenius inner product as \(,_{F}:=(^{})\) and the Frobenius norm of \(\) as \(\|\|_{F}:=,}\).

**Organization.** We provide preliminaries in Section 2. In Section 3, we establish a geometric characterization of how ReLU and leaky ReLU affect the smoothness of their input vectors. We study the smoothness of each dimension of node features and take their magnitude into account in Section 4. Our proposed SCT is presented in Section 5. We comprehensively verify the efficacy of the proposed SCT in Section 6. Technical proofs and more experimental results are provided in the appendix.

## 2 Preliminaries and Existing Results

From the spectral graph theory , we can sort eigenvalues of matrix \(\) in (1) as \(1=_{1}==_{m}>_{m+1}_{n}>-1\), where \(m\) is the number of connected components of the graph. We decompose \(V=\{v_{k}\}_{k=1}^{n}\) into \(m\) connected components \(V_{1},,V_{m}\). Let \(_{i}=(_{\{v_{k} V_{i}\}})_{1 k n}\) be the indicator vector of \(V_{i}\), i.e., the \(k^{th}\) coordinate of \(_{i}\) is one if the \(k^{th}\) node \(v_{k}\) lies in the connected component \(V_{i}\); zero otherwise. Moreover, let \(_{i}\) be the eigenvector associated with \(_{i}\), then \(\{_{i}\}_{i=1}^{n}\) forms an orthonormal basis of \(^{n}\). Notice that \(\{_{i}\}_{i=1}^{m}\) spans the eigenspace \(\) - corresponding to eigenvalue 1 of matrix \(\), and \(\{_{i}\}_{i=m+1}^{n}\) spans the orthogonal complement of \(\), denoted by \(^{}\). The paper  connects the indicator vectors \(_{i}\)s with the space \(\). In particular, we have

**Proposition 2.1** ().: _All eigenvalues of matrix \(\) lie in the interval \((-1,1]\). Furthermore, the nonnegative vectors \(\{}^{}_{i}/\|}^{}_{ i}\|\}_{1 i m}\) form an orthonormal basis of \(\)._

For any matrix \(:=[_{1},,_{n}]^{d n}\), we have the decomposition \(=_{}+_{^{}}\) with \(_{}=_{i=1}^{m}_{i}_{i}^{}\) and \(_{^{}}=_{i=m+1}^{n}_{i}_{i}^{}\) such that \(_{},_{^{}}_{F}=_{i=1}^{m}_{i}_{i}^{}(_{j=m+1}^{n}_{j} _{j}^{})^{}=0\), implying that \(\|\|_{F}^{2}=\|_{}\|_{F}^{2}+\|_{^{ }}\|_{F}^{2}\).

### Existing smoothness notions of node features

**Distance to the eigenspace \(\)**. Oono et al.  study the smoothness of features \(:=[_{1},,_{n}]\) using their distance to the eigenspace \(\) as an unnormalized smoothness notion.

**Definition 2.2** ().: Let \(^{d}\) be the subspace of \(^{d n}\) consisting of the sum \(_{i=1}^{m}_{i}_{i}\), where \(_{i}^{d}\) and \(\{_{i}\}_{i=1}^{m}\) is an orthonormal basis of the eigenspace \(\). Then we define \(\|\|_{^{}}\) - the distance of node features \(\) to the eigenspace \(\) - as follows:

\[\|\|_{^{}}_{^{d} }\|-\|_{F}=-_{i=1}^{m}_{i }_{i}^{}_{F}.\]

With the decomposition \(=_{}+_{^{}}\), \(\|\|_{^{}}\) can be related to \(\|\|_{F}\) as follows:

\[\|\|_{^{}}=\|-_{}\|_{F}=\|_{ ^{}}\|_{F}.\] (3)

**Dirichlet energy.** The paper  studies the unnormalized smoothness of node features using Dirichlet energy, which is defined as follows:

**Definition 2.3** ().: Let \(=-\) be the (augmented) normalized Laplacian, then the Dirichlet energy \(\|\|_{E}\) of node features \(\) is defined by \(\|\|_{E}^{2}(^{})\).

**Normalized Dirichlet energy.** points out that the real smoothness of node features \(\) should be measured by the normalized Dirichlet energy \((^{})/\|\|_{F}^{2}\). This normalized measurement is essential because data often originates from various sources with diverse measurement units or scales. By normalization, we can mitigate biases resulting from these different scales.

### Two existing theories of over-smoothing

Let \(=\{|_{i}|_{i}<1\}\) be the second largest magnitude of \(\)'s eigenvalues, and \(s_{l}\) be the largest singular value of weight matrix \(^{l}\).  shows that \(\|^{l}\|_{^{}} s_{l}\|^{l-1}\|_{ ^{}}\) under GCL when \(\) is ReLU. Therefore, \(\|^{l}\|_{^{}} 0\) as \(l\) if \(s_{l}<1\), indicating node features converge to \(\) and results in over-smoothing. A crucial step in the analysis in  is that \(\|()\|_{^{}}\|\|_{^{}}\), for any matrix \(\) when \(\) is ReLU, i.e., ReLU reduces the distance to \(\).  points out that it is hard to extend the above result to other activation functions even leaky ReLU.

Instead of considering \(\|\|_{^{}}\),  shows that \(\|^{l}\|_{E} s_{l}\|^{l-1}\|_{E}\) under GCL when \(\) is ReLU or leaky ReLU. Hence, \(\|^{l}\|_{E} 0\) as \(l\), implying over-smoothing of GCNs. Note that \(\|\|_{^{}}=0\) or \(\|^{l}\|_{E}=0\) indicates homogeneous node features. The proof in  applies to GCN with both ReLU and leaky ReLU by establishing the inequality \(\|()\|_{E}\|\|_{E}\) for any matrix \(\).

## 3 Effects of Activation Functions: A Geometric Characterization

In this section, we present a geometric relationship between the input and output vectors of ReLU or leaky ReLU. We use \(\|\|_{^{}}\) as the unnormalized smoothness notion for all subsequent analyses since we observe that \(\|\|_{^{}}\) and \(\|\|_{E}\) are equivalent as seminorms. In particular, we have

**Proposition 3.1**.: \(\|\|_{^{}}\) _and \(\|\|_{E}\) are two equivalent seminorms, i.e., there exist two constants \(,>0\) s.t. \(\|\|_{^{}}\|\|_{E}\|\|_{ ^{}}\), for any \(^{d n}\)._

### ReLU

Let \((x)=\{x,0\}\) be ReLU. The first main result of this paper is that there is a high-dimensional sphere underlying the input and output of ReLU; more precisely, we have

**Proposition 3.2** (ReLU).: _For any \(=_{}+_{^{}}^{d n}\), let \(=()=_{}+_{^{}}\). Then \(_{^{}}\) lies on the high-dimensional sphere centered at \(_{^{}}/2\) with radius \(\|_{^{}}/2\|_{F}\) and hence we have \(\|\|_{^{}}\|\|_{^{}}\)._

### Leaky ReLU

Now we consider leaky ReLU \(_{a}(x)=\{x,ax\}\), where \(0<a<1\) is a positive scalar. Similar to ReLU, we have the following result for leaky ReLU

**Proposition 3.3** (Leaky ReLU).: _For any \(=_{}+_{^{}}^{d n}\), let \(=_{a}()=_{}+_{^{}}\). Then \(_{^{}}\) lies on the high-dimensional sphere centered at \((1+a)_{^{}}/2\) with radius_

\[r_{a}(\|(1-a)_{^{}}/2\|_{F}^{2}- _{}-_{},_{}-a_{ }_{F})^{1/2}.\]

_In particular, \(_{^{}}\) lies inside the ball centered at \((1+a)_{^{}}/2\) with radius \(\|(1-a)_{^{}}/2\|_{F}\) and hence we see that \(a\|\|_{^{}}\|\|_{^{}}\| \|_{^{}}\)._

### Implications of the above geometric characterizations

Propositions 3.2 and 3.3 imply that the precise location of \(_{^{}}\) (or \(\|_{^{}}\|_{F}=\|\|_{^{}}\)) depends on the center and the radius \(r\) or \(r_{a}\). Given a fixed \(_{^{}}\), the center of the spheres remains unchanged, and \(r\) and \(r_{a}\) are only affected by changes in \(_{}\). This observation motivates us to investigate _how changes in \(_{}\) impact \(\|\|_{^{}}\), i.e., the unnormalized smoothness of node features_.

Propositions 3.2 and 3.3 imply both ReLU and leaky ReLU reduce the distance of node features to eigenspace \(\), i.e. \(\|\|_{^{}}\|\|_{^{}}\). Moreover, this inequality is independent of \(_{}\); consider \(,^{}^{d n}\) s.t. \(_{^{}}=_{^{}}^{}\) but \(_{}_{^{}}^{}\). Let \(\) and \(^{}\) be the output of \(\) and \(^{}\) via ReLU or leaky ReLU, resp. Then we have \(\|\|_{^{}}\|\|_{^{}}\) and \(\|^{}\|_{^{}}\|^{}\|_{^{ }}\). Since \(_{^{}}=_{^{}}^{}\), we deduce that \(\|^{}\|_{^{}}\|\|_{^{}}\). In other words, when \(_{^{}}=_{^{}}^{}\) is fixed, _changing \(_{}\) to \(_{}^{}\) can change the unnormalized smoothness of the output features but cannot change the fact that ReLU and leaky ReLU smooth node features_; we demonstrate this result inFig. 1a) in Section 4.1. Notice that without considering the nonlinear activation function, changing \(_{}\) does not affect the unnormalized smoothness of node features measured by \(\|\|_{^{}}\).

In contrast to the unnormalized smoothness, _if one considers the normalized smoothness, we find that adjusting \(_{}\) can result in a less smooth output_; we will discuss this in Section 4.1.

## 4 How Adjusting \(_{}\) Affects the Smoothness of the Output

Throughout this section, we let \(\) and \(\) be the input and output of ReLU or leaky ReLU. The smoothness notions based on the distance of feature to \(\) or their Dirichlet energy do not account for the magnitude of each dimension of the features;  points out that analyzing the normalized smoothness of features \(\), given by \(\|\|_{E}/\|\|_{F}\), is an open problem. However, these two smoothness notions aggregate the smoothness of node features across all dimensions; when the magnitude of some dimensions is much larger than others, the smoothness will be dominated by them.

Motivated by the discussion in Section 3.3, we study _the disparate effects of adjusting \(_{}\) on the normalized and unnormalized smoothness_ in this section. For the sake of simplicity, we assume the graph is connected (\(m=1\)); all the following results can be extended to graphs with multiple connected components easily. Due to the equivalence between seminorms \(\|\|_{}\) and \(\|\|_{E}\), we introduce the following definition of the dimension-wise normalized smoothness of node features.

**Definition 4.1**.: Let \(^{d n}\) be the features over \(n\) nodes with \(^{(i)}^{n}\) being its \(i^{th}\) row, i.e., the \(i^{th}\) dimension of the features over all nodes. We define the normalized smoothness of \(^{(i)}\) as follows:

\[s(^{(i)})\|^{(i)}_{}\|/\|^{(i)}\|,\]

where we set \(s(^{(i)})=1\) when \(^{(i)}=\).

_Remark 4.2_.: Notice that the normalized smoothness \(s(^{(i)})=\|^{(i)}_{}\|/\|^{(i)}\|\) is closely related to the ratio between the smooth and non-smooth components of node features \(\|^{(i)}_{}\|/\|^{(i)}_{^{}}\|\).

The graph is connected implies that \(^{(i)}_{}=^{(i)},_{1}_{1}\) and \(\|^{(i)}_{}\|=|^{(i)},_{1}|\). Without ambiguity, we write \(\) for \(^{(i)}\) and \(\) for \(_{1}\) - the eigenvector of \(\) associated with the eigenvalue \(1\). Moreover, we have

\[s()=_{}\|}{\|\|}=, |}{\|\|}=,|}{\|\| \|\|} 0 s() 1,\] (4)

It is evident that _the larger \(s()\) is, the smoother the node feature \(\) is1_. In fact, we have

\[s()^{2}+\|_{^{}}}{\|\|} ^{2}=_{}\|^{2}}{\|\|^{2}}+_{ ^{}}\|^{2}}{\|\|^{2}}=1,\]

where \(\|\|_{^{}}/\|\|\) decreases as \(s()\) increases.

To discuss how the smoothness \(s()=s(())\) or \(s(_{a}())\) can be adjusted by changing \(_{}\), we consider the function

\[()=-.\]

It is clear that

\[()_{^{}}=_{^{}}()_{}=_{}-,\]

where we see that \(\) only alters \(_{}\) while preserves \(_{^{}}\). Moreover, it is evident that

\[s(())=()_{^{}}\|^{2} }{\|()\|^{2}}}=_{^{}}\|^{2} }{\|()\|^{2}}}.\]

It follows that \(s(())=1\) if and only if \(_{^{}}=\) (include the case \(=\)), showing that when \(_{^{}}=\), the vector \(\) is the smoothest one.

The disparate effects of \(\) on \(\|\|_{^{}}\) and \(s()\): Empirical results

Let us empirically study possible values that the unnormalized smoothness \(\|(())\|_{^{}}\), \(\|_{a}(())\|_{^{}}\) and the normalized smoothness \(s((()))\), \(s(_{a}(()))\) can take when \(\) varies.

We denote \(_{}()=-\). We consider a connected synthetic graph with \(100\) nodes, and each node is assigned a random degree between \(2\) to \(10\). Then we assign an initial node feature \(^{100}\), sampled uniformly on the interval \([-1.5,1.5]\), to the graph with each node feature being a scalar. Also, we compute \(\) by the formula \(=}^{}/\|}^{}\|\) from Proposition 2.1, where \(^{100}\) is the vector whose entries are all ones and \(}\) is the (augmented) degree matrix. We examine two different smoothness notions for the input \(\) and the output \((_{})\) and \(_{a}(_{})\), where the smoothness is measured for various values of the smoothness control parameter \([-1.5,1.5]\). In Fig. 0(a), we study the unnormalized smoothness measured by \(\|\|_{^{}}\); we see that \(\|(_{})\|_{^{}}\) and \(\|_{a}(_{})\|_{^{}}\) are always no greater than \(\|\|_{^{}}\). This coincides with the discussion in Section 3.3; adjusting the projection of \(\) onto the eigenspace \(\) can not change the fact that \(\|(_{})\|_{^{}}\|\|_{^ {}}\) and \(\|_{a}(_{})\|_{^{}}\|\|_{^{}}\). Nevertheless, an interesting result is that _defining the eigenspace \(\) projection can adjust the unnormalized smoothness of the output_: notice that altering the eigenspace projection does not change its distance to \(\), i.e., the smoothness of the input is unchanged, but the smoothness of the output after activation function can be changed.

In contrast, when studying the normalized smoothness \(s()\) in Fig. 0(b), we find that \(s((()))\) and \(s(_{a}(()))\) can be adjusted by \(\) to values smaller than \(s()\). More precisely, we see that by adjusting \(\), \(s((()))\) and \(s(_{a}(()))\) can achieve most of the values in \(\). In other words, both smoother and less smooth features can be obtained by adjusting \(\).

### Theoretical results on the smooth effects of ReLU and leaky ReLU

In this subsection, we build theoretical understandings of the above empirical findings on the achievable smoothness shown in Fig. 1. Notice that if \(_{^{}}=\), the inequalities presented in Propositions 3.2 and 3.3 indicate that \(\|(())\|_{^{}}\) and \(\|_{a}(())\|_{^{}}\) vanish. So we have \(s((()))=1\) for any \(\) when \(_{^{}}=\). Then we may assume \(_{^{}}\) for the following study.

**Proposition 4.3** (ReLU).: _Suppose \(_{^{}}\). Let \(()=(())\) with \(\) being ReLU, then_

\[_{}s(())==}a_{i}}{_ {j=1}^{n}d_{j}}}\;\;\;\;_{}s(())=1,\]

_where \(}^{-}\), \(=_{1 i n}x_{i}\), and \(}\) is the augmented degree matrix with diagonals \(d_{1},d_{2},,d_{n}\). In particular, the normalized smoothness \(s(())\) is monotone increasing as \(\) decreases whenever \(<\|}^{}_{n}\|\) and it has range \([_{}s(()),1]\)._

**Proposition 4.4** (Leaky ReLU).: _Suppose \(_{^{}}\). Let \(()=_{a}(())\) with \(_{a}\) being leaky ReLU, then (1) \(_{}s(())=0\), and (2) \(_{}s(())=1\) and \(s(())\) has range \([0,1)\)._

Proposition 4.4 also holds for other variants of ReLU, e.g., ELU2 and SELU3:; see Appendix C. We summarize Propositions 3.2, 3.3, 4.3, and 4.4 in the following corollary, which qualitatively explains the empirical results in Fig. 1.

**Corollary 4.5**.: _Suppose \(_{^{}}\). Let \(()=(())\) or \(_{a}(())\) with \(\) being ReLU and \(_{a}\) being leaky ReLU. Then we have \(\|\|_{^{}}\|()\|_{^{}}\) for any \(\); however, \(s(())\) can be smaller than, larger than, or equal to \(s()\) for different values of \(\)._

Propositions 4.3 and 4.4, and Corollary 4.5, provide a theoretical basis for the empirical results in Fig. 1. Moreover, our results indicate that for any given vector \(\), altering \(_{}\) can change both the unnormalized and the normalized smoothness of the output vector \(=()\) or \(_{a}()\). In particular, the normalized smoothness of \(=()\) or \(_{a}()\) can be adjusted to any value in the range shown in Propositions 4.3 and 4.4. This provides us with insights to control the smoothness of features to improve the performance of GCN and we will discuss this in the next section.

## 5 Controlling Smoothness of Node Features

We do not know how smooth features are ideal for a given node classification task. Nevertheless, our theory indicates that both normalized and unnormalized smoothness of the output of each GCL can be adjusted by altering the input's projection onto \(\). As such, we propose the following learnable smoothness control term to modulate the smoothness of each dimension of the learned node features

\[_{}^{l}=_{i=1}^{m}_{i}^{l}_{i}^{},\] (5)where \(l\) is the layer index, \(\{_{i}\}_{i=1}^{m}\) is the orthonormal basis of the eigenspace \(\), and \(^{l}:=\{_{i}^{l}\}_{i=1}^{m}\) is a collection of learnable vectors with \(_{i}^{l}^{d}\) being approximated by a multi-layer perceptron (MLP). The detailed configuration of \(_{i}^{l}\) will be specified in each experiment later. One can see that \(_{}^{l}\) always lies in \(^{d}\). We integrate SCT into GCL, resulting in

\[^{l}=(^{l}^{l-1}+_{}^{l}).\] (6)

We call the corresponding model GCN-SCT. Again, the idea is that _we alter the component in eigenspace to control the smoothness of features_. Each dimension of \(^{l}\) can be smoother, less smooth, or the same as \(^{l-1}\) in normalized smoothness, though \(^{l}\) gets closer to \(\) than \(^{l-1}\).

To design SCT, we introduce a learnable matrix \(^{l}^{d m}\) for layer \(l\), whose columns are \(_{i}^{l}\), where \(m\) is the dimension of the eigenspace \(\) and \(d\) is the dimension of the features. We observe in our experiments that the SCT performs best when informed by degree pooling over the subcomponents of the graph. The matrix of the orthogonal basis vectors, denoted by \([_{1},,_{m}]^{n m}\), is used to perform pooling \(^{l}\) for input \(^{l}\). In particular, we let \(^{l}=(^{l})\), where \(^{d m}\) is learnable and performs pooling over \(^{l}\) using the eigenvectors \(\). The second architecture uses a residual connection with hyperparameter \(_{l}=(/l+1)\) and learnable matrices \(_{0},_{1}^{d d}\) and the softmax function \(\). Resulting in \(^{l}=(^{l})(_{l}_{0}^{l}+(1- _{l})_{1}^{l})\). In Section 6, we use the first architecture for GCN-SCT as GCN uses only \(^{l}\) information at each layer. We use the second architecture for GCNII-SCT and EGNN-SCT which use both \(^{0}\) and \(^{l}\) information at each layer. There are two particular advantages of the above design of SCT: (1) it can effectively change the normalized smoothness of the learned features, and (2) it is computationally efficient since we only use the eigenvectors corresponding to the eigenvalue 1 of matrix \(\), which is determined based on the connectivity of the graph.

### Integrating SCT into other GCN-style models

In this subsection, we present other usages of the proposed SCT. Due to the page limit, we carefully select two other most representative models. The first example is GCNII , GCNII extends GCN to express an arbitrary polynomial filter rather than the Laplacian polynomial filter and achieves state-of-the-art (SOTA) performance among GCN-style models on various tasks [6; 23], and we aim to show that SCT can even improve the accuracy of the GCN-style model that achieves SOTA performance on many node classification tasks. The second example is energetic GNN (EGNN) , which controls the smoothness of node features by constraining the lower and upper bounds of the Dirichlet energy of features and assuming the activation function is linear. In this case, we aim to show that our new theoretical understanding of the role of activation functions and the proposed SCT can boost the performance of EGNN with considering nonlinear activation functions.

**GCNII**. Each GCNII layer uses a skip connection to the initial layer \(^{0}\) and given as follows:

\[^{l}=((1-_{l})^{l-1}+_{l}^{0}) ((1-_{l})+_{l}^{l}),\]

where \(_{l},_{l}(0,1)\) are learnable scalars. We integrate SCT \(_{}^{l}\) into GCNII, resulting in the following GCNII-SCT layers

\[^{l}=((1-_{l})^{l-1}+_{l}^{0}) ((1-_{l})+_{l}^{l})+_{}^{l},\]

where the residual connection and identity mapping are consistent with GCNII.

**EGNN.** Each EGNN layer can be written as follows:

\[^{l}=^{l}(c_{1}^{0}+c_{2}^{l-1}+(1-c_{ })^{l-1}),\] (7)

where \(c_{1},c_{2}\) are learnable weights that satisfy \(c_{1}+c_{2}=c_{}\) with \(c_{}\) being a hyperparameter. To constrain Dirichlet energy, EGNN initializes trainable weights \(^{l}\) as a diagonal matrix with explicit singular values and regularizes them to keep the orthogonality during the model training. Ignoring the activation function \(\), \(^{l}\) - node features at layer \(l\) of EGNN satisfies

\[c_{}\|^{0}\|_{E}\|^{l}\|_{E} c_{}\|^{0}\|_{E},\]

where \(c_{}\) is the square of the maximal singular value of the initialization of \(^{1}\). Similarly, we modify EGNN to result in the following EGNN-SCT layer

\[^{l}=^{l}((1-c_{})^{l-1}+c_{1}^{ 0}+c_{2}^{l-1})+_{}^{l},\]

where everything remains the same as the EGNN layer except that we add our proposed SCT \(_{}^{l}\).

[MISSING_PAGE_FAIL:8]

the classification accuracy of baseline models; in particular, the improvement can be remarkable for GCN and GCNII. However, EGNN-SCT (using ReLU or leaky ReLU) performs occasionally worse than EGNN (using SReLU), and this is because of the choice of activation functions. In Appendix D.3, we report the results of EGNN-SCT using SReLU, showing that EGNN-SCT outperforms EGNN in all tasks. In fact, SReLU is a shifted version of ReLU, and our theory for ReLU applies to SReLU as well. The model size and computational time are reported in Table 4 in the appendix.

Table 1 also shows that even with SCT, the accuracy of GCN drops when the depth is 16 or 32. This motivates us to investigate the smoothness of the node features learned by GCN and GCN-SCT. Fig. 3 plots the heatmap of the normalized smoothness of each dimension of the learned node features learned by GCN and GCN-SCT with 32 layers for Citeseer node classification. In these plots, the horizontal and vertical dimensions denote the feature dimension and the layer of the model, resp. We notice that the normalized smoothness of each dimension of the features - from layers 14 to 32 learned by GCN - closes to 1, confirming that deep GCN learns homogeneous features. In contrast, the features learned by GCN-SCT are inhomogeneous, as shown in Fig. 2(b). Therefore, we believe the performance degradation of deep GCN-SCT is due to other factors. Compared to GCNII/GCNII-SCT and EGNN/EGNN-SCT, GCN-SCT does not use skip connections, which is known to help avoid vanishing gradients in training deep neural networks [16; 17]. In Appendix D.3, we show that training GCN and GCN-SCT do suffer from the vanishing gradient issue; however, the other models do not. Besides Citeseer, we notice similar behavior occurs for training GCN and GCN-SCT for Cora and Coauthor-Physics node classification tasks.

**Other datasets.** We further compare different models trained on different datasets using 10-fold cross-validation and fixed \(48/32/20\%\) splits following . Table 2 compares GCN and GCNII with and without SCT, using leaky ReLU, for classifying five heterophilic node classification datasets. We exclude EGNN as these heterophilic datasets are not considered in . We report the average accuracy of GCN and GCNII from . We tune all other models using a Bayesian meta-learning algorithm to maximize the mean validation accuracy. We report the best test accuracy for each model of depth searched over the set \(\{2,4,8,16,32\}\). SCT can significantly improve the classification accuracy of the baseline models. Table 2 also contrasts the computational time (on Tesla T4 GPUs from Google Colab) per epoch of models that achieve the best test accuracy; the models using SCT can even save computational time to achieve the best accuracy which is because the best accuracy is achieved at a moderate depth (Table 8 in Appendix D.4 lists the mean and standard deviation for the test accuracies on all five datasets. Table 9 in Appendix D.4 lists the computational time per epoch for each model of depth 8, showing that using SCT only takes a small amount of computational overhead.

## 7 Concluding Remarks

In this paper, we establish a geometric characterization of how ReLU and leaky ReLU affect the smoothness of the GCN features. We further study the dimension-wise normalized smoothness of the learned node features, showing that activation functions not only smooth node features but also can reduce or preserve the normalized smoothness of the features. Our theoretical findings inform the design of a simple yet effective SCT for GCN. The proposed SCT can change the smoothness, in terms of both normalized and unnormalized smoothness, of the learned node features by GCN.

**Limitations:** Our proposed SCT provides provable guarantees for controlling the smoothness of features learned by GCN and related models. A key aspect to establish our theoretical results is demonstrating that, without SCT, the features of the vanilla model tend to be overly smooth; without this condition, SCT cannot ensure performance guarantees.

    &  &  &  & **Squirrel** \\  \(52.70\%\) & \(58.95\) (\(0.71\)/\(1.8\)) & \(52.16\)/\(\) (\(0.70\)/\(8.5\)) & \(45.85\)/\(\) (\(0.70\)/\(8.5\)) & \(28.18\)/\(\) (\(0.6/0.7\)) & \(23.96\)/\(\) (\(1.67\)/\(4.0\)) \\ \(74.86\)/\(\) (\(2.02\)/\(2.0\)) & \(69.46\)/\(\) (\(3.12\)/\(2.0\)) & \(74.12\)/\(\) (\(2.01\)/\(5.0\)) & \(60.61\)/\(\) (\(1.51\)/\(3.3\)) & \(38.47\)/\(\) (\(5.53\)/\(7.5\)) \\   

Table 2: Mean test accuracy and average computational time per epoch in the parenthesis) for the Web-bkB and WikipediaNetwork datasets with fixed \(48/32/20\%\) splits. First row: GCN/GCN-SCT. Second row: GCNII/GCNII-SCT. (Unit:% for accuracy and \( 10^{-2}\) second for computational time.)

Figure 3: The normalized smoothness – of each dimension of the feature vectors at a given layer – for a) GCN and b) GCN-SCT on the Citeseer dataset with 32 layers and 16 hidden dimensions. GCN features become entirely smooth since layer 14, while GCN-SCT controls the smoothness for each feature at any depth. Horizontal and vertical axes represent the index of the feature dimension and the intermediate layer, resp. on the Citeseer dataset with 32 layers and 16 hidden dimensions. GCN features become entirely smooth since layer 14, while GCN-SCT controls the smoothness for each feature at any depth. Horizontal and vertical axes represent the index of the feature dimension and the intermediate layer, resp. on the ResNet accuracy is achieved at a moderate depth (Table 8 in Appendix D.4 lists the mean and standard deviation for the test accuracies on all five datasets. Table 9 in Appendix D.4 lists the computational time per epoch for each model of depth 8, showing that using SCT only takes a small amount of computational overhead.

## 8 Broader Impacts

Our paper focuses on developing new theoretical understandings of the smoothness of node features learned by graph convolutional networks. The paper is mainly theoretical. We do not see any potential ethical issues in our research; all experiments are carried out using existing benchmark settings and datasets.

Our paper brings new insights into building new graph neural networks with improved performance over existing models, which is crucial for many applications. In particular, for applications where graph neural network is the method of choice. We expect our approach to play a role in material science and biophysics applications.