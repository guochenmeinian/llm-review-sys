# LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings

Duo Wang   Yuan Zuo Fengzhi Li   Junjie Wu

MIIT Key Laboratory of Data Intelligence and Management, Beihang University

{wangduo58, zuoyuan, lifengzhi, wujj}@buaa.edu.cn

Corresponding author.

###### Abstract

Zero-shot graph machine learning, especially with graph neural networks (GNNs), has garnered significant interest due to the challenge of scarce labeled data. While methods like self-supervised learning and graph prompt learning have been extensively explored, they often rely on fine-tuning with task-specific labels, limiting their effectiveness in zero-shot scenarios. Inspired by the zero-shot capabilities of instruction-fine-tuned large language models (LLMs), we introduce a novel framework named Token Embedding-Aligned Graph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and cross-task zero-shot learners for graph machine learning. Concretely, we pretrain a GNN, aligning its representations with token embeddings of an LLM. We then train a linear projector that transforms the GNN's representations into a fixed number of graph token embeddings without tuning the LLM. A unified instruction is designed for various graph tasks at different levels, such as node classification (node-level) and link prediction (edge-level). These design choices collectively enhance our method's effectiveness in zero-shot learning, setting it apart from existing methods. Experiments show that our graph token embeddings help the LLM predictor achieve state-of-the-art performance on unseen datasets and tasks compared to other methods using LLMs as predictors. Our code is available at https://github.com/W-rudder/TEA-GLM.

## 1 Introduction

Graph Neural Networks (GNNs) have emerged as a pivotal framework in graph machine learning, harnessing the ability to capture intricate message-passing patterns for robust graph representation. These advancements have yielded various GNN architectures, including the Graph Convolution Network (GCN) , Graph Attention Network (GAT) , and GraphSAGE . Despite their efficacy, GNNs often exhibit limited generalization capabilities, struggling to maintain consistent performance when transitioning across different datasets or downstream tasks . This limitation underscores the necessity for more adaptable and universally applicable models in the graph learning domain.

To mitigate the dependency on labeled data and enhance the resilience of graph models, self-supervised learning has been widely adopted in GNN training. Techniques such as Deep Graph Infomax (DGI)  and GraphCL  have demonstrated effectiveness by leveraging mutual information maximization and contrastive learning, respectively. However, these methods typically require fine-tuning task-specific heads for downstream applications, which can be resource-intensive and limit their practicality in diverse scenarios. Moreover, graph prompt learning enhances GNN generalization by using unified task templates and meta-learning to adapt to various downstream applications , but it often requires extensive fine-tuning and is constrained by the specificity of task types.

In recent years, the remarkable generalization capabilities of Large Language Models (LLMs) have spurred interest in their potential applications within graph machine learning. Some methods attemptto encode graph structures into text for LLM input [9; 10; 11; 12], but these approaches often lead to suboptimal outcomes . Alternatively, using LLMs as enhancers to generate data or node text representations [14; 15; 16; 17; 18] has shown promise but remains constrained by the inherent reliance on GNNs for prediction. Recent efforts [19; 20] to use LLMs as predictors have demonstrated potential. However, their performance often remains unstable due to the challenge of producing transferable graph representations that work effectively for LLMs across diverse tasks and datasets.

In light of these challenges, we propose a novel framework named Token Embedding-Aligned Graph Language Model (TEA-GLM). Inspired by the zero-shot capabilities of instruction-fine-tuned LLMs , TEA-GLM leverages LLMs as cross-dataset and cross-task zero-shot predictors for graph machine learning. The core idea is to pretrain a GNN and align its representations with the token embeddings of an LLM. This alignment enables the GNN to effectively utilize the LLM's pretrained knowledge, allowing it to generalize across different datasets and tasks without task-specific fine-tuning. Additionally, we train a linear projector to convert graph representations into a fixed number of token embeddings, which are then incorporated into a unified instruction designed for various graph tasks at different levels. Experiments show TEA-GLM achieves superior performance in zero-shot scenarios and when encountering unseen tasks, offering a more generalized and efficient solution for graph zero-shot learning. Our contributions are summarized as follows:

* We introduce TEA-GLM, a novel framework that aligns GNN representations with LLM token embeddings, enabling cross-dataset and cross-task zero-shot learning for graph machine learning.
* We propose a linear projector that maps graph representations into a fixed number of graph token embeddings. These embeddings are incorporated into a unified instruction designed for various graph tasks at different levels, enhancing the model's generalization capabilities.
* Our extensive experiments demonstrate that TEA-GLM significantly outperforms state-of-the-art methods on unseen datasets and tasks.

## 2 Methodology

In this section, we introduce TEA-GLM, a novel framework designed for cross-dataset and cross-task zero-shot graph machine learning. TEA-GLM consists of two main components: a Graph Neural Network (GNN) to derive node representations from the graph, and a Large Language Model (LLM) to perform zero-shot tasks such as node classification and link prediction. Our methodology involves two key stages: enhanced self-supervised learning of the GNN, where feature-wise contrastive learning with LLM's token embeddings is proposed, and training a linear projector to map graph representations into a fixed number of graph token embeddings by designing an instruction that is suitable for various graph tasks at different levels. The framework of our proposed method is illustrated in Fig. 1.

### Notations

Formally, a graph is denoted as \(=(,,,)\), where \(=\{v_{1},v_{2},,v_{||}\}\) with \(||=N\) indicating the total number of nodes and \(=\{e_{1},e_{2},,e_{||}\}\) representing the sets of nodes and edges, respectively. The adjacency matrix is denoted as \(^{N N}\), with \(_{ij}=1\) iff \((v_{i},v_{j})\). The feature matrix \(^{N F_{N}}\) contains the attribute or feature information associated with each node, where \(_{i}^{F_{N}}\) is the feature of \(v_{i}\), and \(F_{N}\) represents the dimensionality of features.

### Token embeddings-aligned graph self-supervised learning

Given the increasing model sizes and data volumes in recent years, self-supervised learning has become a prominent research focus due to the scarcity of labeled data. In this context, we propose a contrastive learning method to obtain more transferable node representations suitable for use with large language models (LLMs). Our approach leverages instance-wise contrastive learning and introduces a feature-wise contrastive learning method that maps node representations to the textual embedding space of the LLM.

#### 2.2.1 Instance-wise contrastive learning with structural information

To alleviate the need for labeled data and enhance model generalization capability, we employ self-supervised learning for pre-training. To better extract structural information from the graph, we follow the work of  to generate two views of \(\), denoted as \(_{1}\) and \(_{2}\), for contrastive learning. Specifically, we adopt the Removing Edges (RE) and Masking Node Features (MF) methods to generate different views. The RE strategy samples a random masking matrix \(}\{0,1\}^{N N}\) to mask the raw adjacency matrix, computed as:

\[}=},\] (1)

where \(\) denotes the Hadamard product. The MF strategy samples a random mask vector \(}\{0,1\}^{F}\). The generated node features \(}\) are computed by:

\[}=[}};} };;}}].\] (2)

Thus, we obtain two views of \(\), denoted as \(_{1}=(}_{1},}_{1})\) and \(_{2}=(}_{2},}_{2})\). Then, we use a graph encoder to derive node representations:

\[_{*}=f_{}(}_{*},}_{*})^{N F_{U}},\] (3)

Where \(F_{U}\) is the dimension size of node representations. Here, \(*\{1,2\}\) represents different views of the graph.

We employ a contrastive objective to distinguish the embeddings of the same node in these two different views from other node embeddings. For node \(v_{i}\), its node embedding generated in one view, \(}\), is treated as the anchor, while the embedding generated in the other view, \(}^{}\), forms the positive sample. Embeddings of other nodes in the same view are regarded as intra-view negative samples, while embeddings of other nodes in the other view are regarded as inter-view negative samples. The contrastive loss is defined as:

\[(},}^{})=},}^{})/}}{^{N}1_{[j  i]}e^{(},})/}}_{}+^{N}1_{[j i]}e^{(},} ^{})/}}_{}},\] (4)

where \(1_{[j i]}\{0,1\}\) is an indicator function that equals 1 iff \(j i\), \((,)\) is the cosine similarity function, and \(\) is a temperature parameter. The loss for the other view is similarly defined, and the overall objective \(_{ins}\) is the average of all instances:

\[_{ins}=_{i=1}^{N}[(},}^{})+(}^{},})].\] (5)

Figure 1: Framework of TEA-GLMTo enhance the scalability of our method for large-scale graphs, we employ the subsampling approach proposed by . Both the RE and MF methods, along with the loss function described in Equation 4, are seamlessly adaptable to the sampled subgraphs.

#### 2.2.2 Feature-wise contrastive learning with token embeddings

Instance-wise contrastive learning relies heavily on individual instances, which can cause transfer issues when transitioning to other datasets. Moreover, there is a significant gap between the obtained node representations and the semantic space of LLMs. To address these issues, we propose feature-wise contrastive learning with token embeddings.

Feature-wise contrastive loss breaks the independence between instances. For the feature matrix \(_{*}\), we denote the columns in different views as \(}^{}}\) and \(}^{}}\). Here, \(},}^{N}\). The loss is denoted as \(_{fea}\), and is calculated as:

\[_{fea}=}_{i=1}^{F_{U}}},})/}}{_{j=1}^{F_{U}}[e^{(},})/ }+e^{(},})/}]}.\] (6)

To map node representations to the semantic space of LLMs, we use the principal components of the token embeddings of LLMs as coordinate axes. This approach ensures that the representations of similar instances are closely aligned in the textual embedding space. This helps alleviate the inconsistency in optimization objectives during graph self-supervised learning due to the gap between node representations and the text embedding space.

Specifically, we first use principal component analysis (PCA) to obtain the \(P\) principal components, denoted as \(^{P F_{L}}\), where \(F_{L}\) is the dimension size of token embeddings of LLM. Then, we map node representations by:

\[}_{*}=_{*}^{}.\] (7)

To map the node representations obtained from the GNN using principal components, we set the output dimension of the GNN to be equal to the token embeddings' dimension (i.e., \(F_{U}=F_{L}\)). The columns of the mapped feature matrix \(}_{*}\), denoted as \(}_{i}\) and \(}_{i}\), are fed into \(_{fea}\). Therefore, the final contrastive loss for graph self-supervised learning is the average of Equation 4 and Equation 6:

\[=(_{ins}+_{fea}).\] (8)

_Remark:_ The introduction of feature-wise contrastive learning with token embeddings successfully addresses the semantic space discrepancy between graph node representations and LLM token embeddings. Our method enables the direct and simple use of graph structural and text information obtained by GNN in LLMs, thereby avoiding the significant generalization loss associated with complex modality alignment training during the fine-tuning process. Its role in fine-tuning will be further described in Sec. 2.3.2 and validated by experiments. Additionally, the feature-wise contrastive method itself exhibits stronger generalization, allowing it to perform well on unseen instances (or tasks) rather than relying on trained instances (or tasks).

### Alignment tuning

The development of LLMs has introduced a new paradigm for graph machine learning. However, existing research  indicates that LLMs alone cannot fully comprehend graph structures and their underlying information. To enable LLMs to more effectively capture information and improve their performance in cross-dataset and cross-task zero-shot learning, it is essential to design specific methods for LLMs to incorporate graph information suitably. To this end, we propose an alignment tuning method that includes specially designed instructions for various graph tasks at different levels, as well as a graph representation to graph token embeddings mechanism to integrate graph information.

#### 2.3.1 Instructions design

The instruction we designed can be divided into two parts: one part provides graph information, and the other part describes the task. Here, we take a citation graph as an example, where nodes are papers, and relations are citations, to introduce the instruction.

Graph information provisionThe graph information provision in the instructions for node, edge, and graph-level tasks is presented as follows: _Given the representation of a paper/two papers/a paper set: \(\), with the following information:\(\)Unitle: First Paper: \(\{_{1}\}\)...\(\)_, where \(\) is the placeholder for graph inputs (see Sect. 2.3.2), and \(\{_{1}\}\) is the node text information.

Note that, different from most work which use LLM as a predictor, the instruction we designed uses only the title of a paper node, excluding more extensive textual information such as its abstract or description. In fact, reducing the amount of input text not only does not decrease the model's performance but actually improves it.  confirmed through experiments that LLMs benefit from structural information only when the target node lacks sufficient phrases for reasonable predictions. Therefore, using only titles as text input can help LLMs extract more critical information from graph information. The complete instruction for the tasks of node classification and link prediction in citation networks is shown in Appendix D.

Task descriptionTo achieve cross-dataset capability, where the model can be trained on one graph dataset and then perform reasoning on any other dataset, the instruction is designed to include not only the task description itself but also the set of alternative answers. Using the node classification task on the Arxiv dataset (see Sect. 3.1) as an example, the instruction is structured as follows: _Which arXiv CS sub-category does this paper belong to? Please directly give the most likely answer from the following sub-categories: \(\{\}\)_, where \(\{\}\) represents the set of alternative answers, which varies across datasets. Including alternative answers enables the model to learn the task of "reasoning the answer from a given set according to the task" rather than memorizing answers for a particular dataset, thus facilitating reasoning across datasets.

#### 2.3.2 Graph token embeddings

The token embeddings of graph mentioned previously, _i.e._, \(\), are crucial for incorporating graph information and enabling the model's generalization. We use a projector to map central node representations into \(K\) graph token embeddings and replace \(\) with these tokens. Kindly note that, we map the representations to fixed number of token embeddings regardless of the task type. For example, for node-level tasks, we map the central node representation to \(K\) token embeddings; for edge-level tasks, we pool the representations of the two nodes of the target edge and then map this pooled representation to \(K\) token embeddings; for graph-level tasks, similar approach can be applied. In this way, we unify the instruction of graph tasks at different levels. Thanks to the text-aligned contrastive learning, a linear projector is enough to capture the map relationship without tuning LLM:

\[_{token}=f_{}(})\] (9)

where \(}\), \(_{token}^{K F_{L}}\), \(F_{L}\) is the dimension size of token embedding of LLM, and \(f_{}()\) is a linear layer.

_Remark:_ This approach offers three primary advantages: (i) When handling tasks at different levels, the changes to the instructions are minimal. This consistency facilitates the transfer of knowledge learned during training to unseen tasks in large language models (LLMs); (ii) The fixed number of token embeddings can be seen as a conditional soft prompt. Unlike traditional soft prompts, learning at the instance level reduces the risk of overfitting to specific datasets or tasks, thereby enhancing generalization to unseen datasets and tasks; (iii) Different from current work which intends to include the representations of all nodes in the subgraph, we only map the representations of the central node to tokens, since there has enough information carried by message passing of GNN. This method is more efficient, and it offers greater generalizability and practicality.

#### 2.3.3 Training and evaluation strategy

To ensure compatibility and facilitate comparisons across various datasets, we map the node features into a consistent vector space. Specifically, we employ a pretrained BERT model  to encode the raw text associated with each node, thereby generating the node features. We then pretrain the graph model using contrastive learning with the loss function defined in Equation 8 on a single dataset. After pretraining, the model parameters are fixed. We utilize the pretrained model to obtain node representations and follow the instructions in Section 2.3.1 to train the linear projector on specific tasks within the same dataset. Finally, we evaluate the performance of our model on unseen datasets and tasks. Throughout all phases, the parameters of the language model remain fixed. We use GraphSAGE  as our graph encoder and Vicuna-7B-v1.5  as the foundational language model.

Experimental results

In this section, comprehensive experiments are conducted to validate the effectiveness of TEA-GLM. These experiments aim to investigate the following research questions:

**RQ1:**: How effective is TEA-GLM in handling the cross-dataset zero-shot learning problem?
**RQ2:**: How well does TEA-GLM transfer knowledge when adapted to an unseen task and dataset in a zero-shot setting?
**RQ3:**: What is the contribution of the feature-wise contrastive learning and graph token embeddings to the zero-shot learning ability of TEA-GLM?

### Experimental setup

DatasetsWe test TEA-GLM across eight widely used datasets spanning two distinct domains. Within the citation domain, we employ Arxiv , Pubmed , and an expanded version of Cora  with an increased range of classes and larger scale. In these datasets, each node represents an individual paper, with edges indicating citation relationships. In the e-commerce domain, we utilize datasets from the TAG benchmark , including Children (Book-Children), History (Book-History), Computer (Ele-Computer), Photo (Ele-Photo), and Sports (Sports-Fitness). Here, nodes represent distinct products, while edges denote co-viewing or co-purchasing between two products. Appendix A presents the statistics for these datasets.

BaselinesWe conduct a comprehensive comparison of TEA-GLM with various categories of baseline methods: (i) Non-graph neural network approaches, such as MLP, which employs a Multilayer Perceptron for node representation; (ii) Supervised methods, including GCN , GraphSAGE , and GAT ; (iii) Self-supervised methods like DGI , which maximizes mutual information to learn node representations without relying on ground truth labels; (iv) Graph knowledge distillation frameworks: GKD , which distills knowledge from a teacher GNN trained on a complete graph to a student GNN operating on a smaller or sparser graph; GLNN , a method combining the advantages of graph neural networks and MLPs using knowledge distillation, aimed at reducing dependency on the inference graph; (v) Graph transformer networks, including NodeFormer  and DIFFormer ; (vi) Large language models, such as Vicuna-7B-v1.5; (vii) The latest models equipped with transfer and zero-shot capabilities, such as OFA , GraphGPT , and LLaGA .

Implementation detailsFor datasets within the citation domain, we follow the data split methodology outlined in GraphGPT . For those within the e-commerce domain, we utilize scripts provided by the TAG benchmark  to generate data splits. To ensure comparability among different methods, identical data splits are applied to all models. To assess the performance of TEA-GLM, we employ three commonly adopted evaluation metrics: Accuracy and Macro F1 for node classification, and AUC (Area Under the Curve) for link prediction. To ensure result robustness, we conduct five experiments with random seed values ranging from 0 to 4 and report the mean and standard deviation of the results. Due to the limited number of pages, several experimental results, such as Macro F1 results of node classification (Appendix B.2), legality rate of valid answers produced by the LLM (Appendix B.1), and parameter sensitivity analysis (Appendix C), are reported in Appendix.

In the pre-training phase of the GNN, we set the GNN layers to 2. We use a batch size of 512 for 60 epochs and a learning rate of \(2 10^{-2}\). During the training of the linear projector, we configure a batch size of 2 per GPU for one epoch, with a learning rate of \(1 10^{-3}\). The Adam optimizer is employed for all approaches. For baseline models, we adjust hyperparameters and utilize the optimal settings. All experiments are conducted on 2 NVIDIA A100 GPUs with 80GB memory each, using CUDA version 11.7.

### Cross-dataset zero-shot ability (RQ1)

We train all methods on the Arxiv and Computer, respectively, followed by an evaluation of their zero-shot performance on datasets from the same domain. Zero-shot learning presents challenges for GNN-based models, particularly regarding variations in the number of classes across different datasets. To address this, we adopt the setting outlined in GraphGPT . For each target dataset, we utilize the GNN backbone trained on the source dataset along with a classifier trained with target data,typically a linear layer. Due to the considerable time cost associated with training and evaluating GraphGPT on e-commerce datasets, we only report its performance on citation datasets as provided in their paper. "-std" and "-cot" denote the use of the standard procedure of dual-stage graph instruction tuning and COT instruction datasets generated by LLM, respectively. To demonstrate the difference between our work and Soft Prompt Tuning, we fine-tuned vicuna-7b-v1.5 using Soft Prompt and reported the results. The Accuracy results are presented in Table 1. As mentioned earlier, we report the Macro F1 results in Appendix B.2 and report results on two training datasets in Appendix B.3.

The results clearly demonstrate that TEA-GLM outperforms all state-of-the-art (SOTA) models, resulting in significant improvements. Comparative analysis with baseline models across all datasets highlights the robust generalization capability of TEA-GLM. Models utilizing GNN as a predictor face challenges in achieving cross-dataset transferability with traditional supervised and self-supervised learning methods. Even recently developed robust GNN-based models, such as NodeFormer, DIF-Former, and GKD, encounter similar issues. In the case of OFA, a recent framework for cross-domain learning, strong transferability is observed between topic-related datasets such as Arxiv and Cora (both related to computer science). Nevertheless, its generalization performance notably decreases on datasets with lower topic relevance, such as those in the e-commerce domain.

LLM-based solutions, such as Vicuna-7B, demonstrate consistent performance across various datasets. Nevertheless, their predictive capabilities are confined to text information alone. Vicuna-7B-SPT also fails to achieve transferability on e-commerce datasets, indicating that soft prompt tuning alone is insufficient when relying solely on node texts. This suggests that graph tokens indeed contain transferable graph information, enabling the LLM to make more accurate predictions. In contrast, GNN-LLM-combined solutions that use LLM as a predictor demonstrate generalization ability but often face limitations. For instance, GraphGPT tends to underperform compared to Vicuna-7B, due to the lack of a graph foundation model. Instead of relying on a graph foundation model, LLaGA directly maps node representations without GNN and can generalize on citation datasets. However, it demonstrates limited generalization capability across e-commerce datasets, which are more challenging due to highly irrelevant topics. TEA-GLM, on the other hand, utilizes principal components of token embeddings of LLMs to constrain representations learned by GNN, helping the graph representations well transfer to other datasets. Experimental results validate the superior generalization capabilities of TEA-GLM, achieved with less textual data and fewer parameters.

### Cross-task zero-shot ability (RQ2)

We employ models trained on node classification tasks directly for link prediction tasks without any fine-tuning. We omit the comparison with models utilizing GNN as a predictor, as conducting cross-task evaluation of these models without fine-tuning poses a significant challenge, given that different tasks typically correspond to different task heads. Here, we contrast TEA-GLM with OFA, which similarly enables cross-task testing without the need for fine-tuning. Additionally, we compare

   &  &  &  \\   & & **Pubmed** & **Cora** & **Children** & **History** & **Photo** & **Sports** \\   & MLP & 0.323\(\)0.027 & 0.021\(\)0.006 & 0.029\(\)0.037 & 0.080\(\)0.041 & 0.110\(\)0.070 & 0.042\(\)0.021 \\   & GCN & 0.288\(\)0.092 & 0.017\(\)0.004 & 0.030\(\)0.018 & 0.063\(\)0.042 & 0.103\(\)0.047 & 0.042\(\)0.025 \\  & GraphSAGE & 0.316\(\)0.058 & 0.014\(\)0.007 & 0.008\(\)0.007 & 0.195\(\)0.206 & 0.056\(\)0.055 & 0.051\(\)0.015 \\  & GAT & 0.343\(\)0.064 & 0.016\(\)0.004 & 0.008\(\)0.084 & 0.172\(\)0.098 & 0.050\(\)0.027 & 0.142\(\)0.138 \\  & DGI & 0.329\(\)0.103 & 0.026\(\)0.009 & 0.082\(\)0.035 & 0.218\(\)0.168 & 0.224\(\)0.127 & 0.049\(\)0.017 \\ GNN as predictor & GKD & 0.399\(\)0.033 & 0.042\(\)0.008 & 0.202\(\)0.064 & 0.339\(\)0.138 & 0.166\(\)0.086 & 0.208\(\)0.077 \\  & GLNN & 0.390\(\)0.011 & 0.031\(\)0.006 & 0.187\(\)0.012 & 0.283\(\)0.021 & 0.403\(\)0.019 & 0.317\(\)0.048 \\  & NodeFormer & 0.308\(\)0.093 & 0.016\(\)0.007 & 0.048\(\)0.028 & 0.168\(\)0.127 & 0.073\(\)0.015 & 0.165\(\)0.057 \\  & DIFFormer & 0.361\(\)0.071 & 0.029\(\)0.014 & 0.129\(\)0.030 & 0.275\(\)0.171 & 0.321\(\)0.055 & 0.306\(\)0.131 \\  & OFA & 0.314\(\)0.059 & 0.130\(\)0.019 & 0.064\(\)0.086 & 0.052\(\)0.049 & 0.340\(\)0.026 & 0.101\(\)0.071 \\   & Vicuna-7B-v1.5 & 0.719\(\)0.010 & 0.156\(\)0.001 & 0.270\(\)0.001 & 0.363\(\)0.001 & 0.378\(\)0.004 & 0.370\(\)0.001 \\  & Vicuna-7B-SPT & 0.768\(\)0.036 & 0.168\(\)0.018 & 0.227\(\)0.015 & 0.281\(\)0.088 & 0.350\(\)0.061 & 0.230\(\)0.018 \\  & GraphGPT-std & 0.701 & 0.126 & - & - & - & - \\  & GraphGPT-cot & 0.521 & 0.181 & - & - & - & - \\  & LLaGA & 0.793\(\)0.036 & 0.168\(\)0.032 & 0.199\(\)0.007 & 0.146\(\)0.067 & 0.276\(\)0.069 & 0.352\(\)0.033 \\  & **TEA-GLM** & **0.548\(\)0.010** & **0.202\(\)0.014** & **0.271\(\)0.010** & **0.528\(\)0.058** & **0.497\(\)0.027** & **0.404\(\)0.010** \\  

Table 1: Zero-shot accuracy on citation and e-commerce datasets (**bold** highlights the best result across all methods, while underline highlights the second-best results)TEA-GLM with Vicuna-7B and methods that utilize LLM as a predictor, such as GraphGPT and LLaGA. For GraphGPT, we utilize the checkpoint released by the author trained on Arxiv and report the results on citation datasets. The results are reported in Table 2.

In the case of OFA, although this framework facilitates cross-domain and cross-task learning, it exhibits negative transfer when lacking task-relevant data, particularly on unseen tasks. Benefiting from the generalization capability of large language models, both the fine-tuned and non-fine-tuned versions of Vicuna do not experience negative transfer. However, due to the absence of graph information, its predictions often appear random. Conversely, GraphGPT shows transferability with familiar datasets, yet its performance declines when dealing with unseen datasets (Pubmed and Cora). Due to the absence of GNN for filtering and aggregating graph information, LLaGA demonstrates unstable performance. While it exhibits cross-task transferability on citation datasets, its performance is poor on most e-commerce datasets. In contrast, TEA-GLM consistently outperforms all baseline methods on both unseen datasets and tasks, except for the results on Sports, indicating the stronger generalization ability of TEA-GLM.

### Ablation study (RQ3)

We conduct an ablation study to discuss two key components of our model: feature-wise contrastive learning and graph token embeddings. Here, we directly remove these two components from our model and then test the model's performance on cross-dataset and cross-task evaluations. The results are shown in Figure 2. "w/o FC" means that we pretrain the GNN without feature-wise contrastive learning, while "w/o GT" means predicting without graph token embeddings.

Without graph token embeddings, large language models lack crucial information from the graph, leading to a significant decline in performance on both node-level and edge-level tasks. GNNs pre-trained with feature-wise contrastive learning can obtain node representations aligned with the text space, enabling cross-dataset and cross-task generalization through a simple linear layer. When the feature-wise constraint for pre-training is absent, the model's performance on the seen datasets (Arxiv and Computer) for the training task improves slightly. However, its performance on unseen datasets declines. Although it remains relatively stable when handling tasks of the same category, its performance decreases notably when dealing with unseen tasks (link prediction). These results

    &  &  \\   & **Arxiv** & **Pubmed** & **Cora** & **Children** & **History** & **Computer** & **Photo** & **Sports** \\  OFA & 0.469 & 0.481 & 0.492 & 0.484 & 0.431 & 0.461 & 0.459 & 0.517 \\ Vicuna-7B-v1.5 & 0.513 & 0.543 & 0.527 & 0.500 & 0.515 & 0.502 & 0.501 & 0.502 \\ Vicuna-7B-SPT & 0.537 & 0.535 & 0.565 & 0.544 & 0.543 & 0.509 & 0.501 & 0.508 \\ GraphGPT-std & 0.649 & 0.501 & 0.520 & - & - & - & - & - \\ LLaGA & 0.570 & 0.569 & 0.537 & 0.422 & 0.449 & 0.479 & 0.478 & **0.597** \\ 
**TEA-GLM** & **0.657** & **0.689** & **0.586** & **0.571** & **0.579** & **0.554** & **0.545** & 0.553 \\   

Table 2: AUC of link prediction (Cross-task)

Figure 2: Ablation study results (“Seen datasets” are used to train the GNN and linear projector, while “unseen datasets” are not. “Unseen task” means the model wasn’t trained for link prediction.)

indicate that alignment between graph representation and LLM's token embeddings via feature-wise contrastive learning is important for cross-task zero-shot transfer.

## 4 Related work

### Graph neural networks

In the field of graph machine learning, Graph Neural Networks (GNNs) have garnered significant attention [33; 34; 35; 36; 37; 38; 39; 40]. The primary strategy of most GNNs is to capture underlying message-passing patterns for graph representation. Several effective neural network architectures have been proposed, such as Graph Attention Network (GAT) , Graph Convolution Network (GCN) , and GraphSAGE . Recently, there has been a surge of interest in exploring transformer-based encoders for graph machine learning [41; 42; 31; 32]. However, a notable limitation of GNNs is their generalization capability. Typically, GNNs are trained on specific tasks within particular datasets, and when faced with new datasets or tasks, they often struggle to consistently perform well across different datasets or downstream tasks .

### Self-supervised learning and prompt-tuning for GNNs

To alleviate the demand for labeled data and enhance the robustness of graph models, self-supervised learning is commonly employed in GNN training [43; 22; 44]. Methods like Deep Graph Info-max (DGI)  utilize mutual information maximization for pre-training. Other approaches, such as GraphCL , GCA , GCC , and JOAO , learn node representations by contrasting positive and negative samples. GraphMAE [48; 49], on the other hand, learns representations by generating samples that resemble the original graph structure. However, these methods typically require fine-tuning the task-specific heads for downstream applications.

Various methods have explored the use of prompt techniques to enhance the generalization of GNNs. To address the inconsistency between pre-training and downstream task objectives, GraphPrompt  proposes a unified task template applicable to both stages. Additionally, ProG  reformulates various task types into a unified graph-level representation and employs meta-learning techniques to enhance multi-task learning capabilities. However, whether through self-supervised learning or graph prompt methods, fine-tuning is often necessary when handling new datasets. Moreover, when confronted with datasets containing varying numbers of categories, retraining of task heads is required to achieve optimal performance.

### Large language models for graphs

With the rapid advancement of Large Language Models (LLMs) and their remarkable generalization capabilities, leveraging LLMs to address transferability issues in graph machine learning has garnered significant attention [10; 50]. Some methods represent graph structure information as text input to LLMs [9; 11; 12]; however, this approach often leads to suboptimal solutions . Another paradigm involves using LLMs as enhancers [14; 15; 16; 17; 18], where they generate data or node text representations. Despite this, since GNNs are ultimately used for prediction, this approach significantly limits the model's transferability. Recently, considerable efforts have been made to utilize LLMs as predictors. For instance, GraphGPT  attempts to align LLMs with pre-trained Graph Transformer encoders through two-stage fine-tuning. However, the fine-tuning, conducted on specific datasets, might weaken the method's transferability. In light of this, LLaGA  introduced a novel encoding method that directly translates graph data into sequences compatible with LLMs. However, this approach may compromise performance due to the lack of GNN filtering and aggregation of graph information. Inspired by these challenges, we propose a pre-training strategy that enhances GNN transferability by aligning its representations with the token embeddings of LLMs, resulting in improved performance in zero-shot tasks. Notably, similar to our method, TEST  aligns time series representations with several selected LLM token embeddings. However, our approach differs in that we project graph representations into a feature space defined by the principal components of LLM token embeddings. This enables the LLM to function as a zero-shot learner for graph machine learning tasks, rather than just enhancing performance on specific, seen tasks.

## 5 Limitations

While our TEA-GLM framework demonstrates considerable promise in enhancing zero-shot learning for graph-based tasks, it does have some limitations. Although the framework we designed can be easily applied to graph-level tasks, we have not yet explored the model's performance through specific experiments. This will be addressed in our future work.

## 6 Conclusion

This paper introduces TEA-GLM, a framework that enhances zero-shot learning in graph machine learning by aligning GNN representations with LLM token embeddings. TEA-GLM uses a linear projector to map graph representations into graph token embeddings and incorporates a unified instruction design to handle various graph tasks at different levels. This approach enables consistent performance across various datasets and tasks without task-specific fine-tuning. Extensive experiments show that TEA-GLM outperforms state-of-the-art methods in accuracy and generalization, demonstrating its effectiveness and efficiency in zero-shot learning for graph tasks.

## 7 Acknowledgement

This work was supported by the National Key R&D Program of China (2023YFC3304700). The work of Yuan Zuo was supported by the National Natural Science Foundation of China (NSFC) under Grant 71901012. Dr. Junjie Wu's work was partially supported by the National Natural Science Foundation of China (72242101, 72031001) and Outstanding Young Scientist Program of Beijing Universities (JWZQ20240201002).