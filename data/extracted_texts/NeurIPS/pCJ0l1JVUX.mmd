# Hamba: Single-view 3D Hand Reconstruction with Graph-guided Bi-Scanning Mamba

Haoye Dong\({}^{*,}\) Aviral Chharia\({}^{*}\) Wenbo Gou\({}^{*}\)

Francisco Vicente Carrasco Fernando De la Torre

Carnegie Mellon University

{haoyed, achharia, wgou, fvicente, ftorre}@andrew.cmu.edu

https://humansensinglab.github.io/Hamba/

Equal contribution. \({}^{}\)Corresponding author.

###### Abstract

3D Hand reconstruction from a single RGB image is challenging due to the articulated motion, self-occlusion, and interaction with objects. Existing SOTA methods employ attention-based transformers to learn the 3D hand pose and shape, yet they do not fully achieve robust and accurate performance, primarily due to inefficiently modeling spatial relations between joints. To address this problem, we propose a novel graph-guided Mamba framework, named **Hamba**, which bridges graph learning and state space modeling. Our core idea is to reformulate Mamba's scanning into graph-guided bidirectional scanning for 3D reconstruction using a few effective tokens. This enables us to efficiently learn the spatial relationships between joints for improving reconstruction performance. Specifically, we design a Graph-guided State Space (GSS) block that learns the graph-structured relations and spatial sequences of joints and uses 88.5% fewer tokens than attention-based methods. Additionally, we integrate the state space features and the global features using a fusion module. By utilizing the GSS block and the fusion module, Hamba effectively leverages the graph-guided state space features and jointly considers global and local features to improve performance. Experiments on several benchmarks and in-the-wild tests demonstrate that Hamba significantly outperforms existing SOTAs, achieving the PA-MPVPE of 5.3mm and F@15mm of 0.992 on FreiHAND. At the time of this paper's acceptance, Hamba holds the top position, **Rank 1**, in two competition leaderboards1 on 3D hand reconstruction.

## 1 Introduction

3D Hand reconstruction has numerous applications across multiple fields, which include robotics, animation, human-computer interaction, and AR/VR . However, reconstructing 3D hands from a single RGB image without body context or camera parameters remains a difficult challenge in computer vision. Recent works primarily explored transformers  for this task and achieved SOTA performance by utilizing attention mechanism. METRO  introduced a multi-layer transformer, using self-attention to learn vertex-vertex and vertex-joint relations. MeshGraphormer  integrated graph convolutions with a transformer to further enhance the reconstruction performance. Recently, HaMeR  designed a ViT-based model , using ViTPose  weights and large datasets to achieve better performance.

However, the above models fail to reconstruct a robust mesh in challenging in-the-wild scenarios that have occlusions, truncation, and hand-hand or hand-object interactions (See Figure 5 for visualcomparison). This is partially due to a lack of accurate modeling of spatial relations among hand joints. Secondly, transformer-based methods  require a large number of tokens for reconstruction, and applying attention to all image tokens does not efficiently model the joint spatial sequences (i.e., the spatial relationship between joints), which often results in an inaccurate 3D hand mesh in real-world scenarios.

To address these challenges, we propose **Hamba**, a novel Mamba-based  model that employs graph learning  and state space modeling  for robust 3D hand mesh reconstruction. Mamba is a new state space modeling method with global receptive field capability. Most Mamba-based models  are designed for long-range data, and few studies  have adapted Mamba for 3D vision tasks. In this work, we explore Mamba's potential for the 3D hand reconstruction task. We found that directly applying Mamba for 3D hand reconstruction results in inaccurate meshes due to its unidirectional scanning and the lack of specific design for 3D hand reconstruction. To tackle this challenge, we propose a Graph-guided Bidirectional Scan (GBS) to effectively capture the semantic and spatial relation between joints, as shown in Figure 2(c). Besides, the transformer's attention requires calculating correlation among all tokens and introduces unnecessary background correlations, while our proposed GBS uses 88.5% fewer tokens (see Section 3.2 for more details). Secondly, though Mamba-based models  excel in modeling long-range sequences, they are not proficient at capturing the local-relation information (in our case, the'semantics' between hand joints). Since graph learning has the capability

Figure 1: **In-the-wild visual results of Hamba**. Hamba achieves significant performance in various in-the-wild scenarios, including hand interaction with objects or hands, different skin tones, different angles, challenging paintings, and vivid animations.

to effectively capture node relations, we integrate graph convolutions into state space modeling, significantly enhancing the representation by considering the intricate hand joint relations.

In particular, to effectively leverage state space modeling (SSM) and graph learning capabilities for 3D hand reconstruction, we first carefully design a Token Sampler (TS) under guidance with hand joints predicted by Joint Regressor (JR), then feed sampled token into the Graph-guided State Space block (GSS) under the Graph-guided Bidirectional Scan (GBS). Lastly, we introduce a fusion module to integrate the state space tokens and global features to further improve performance. As shown in Figure 1, Hampa achieves significant visual performance in challenging scenarios. We summarize our contributions as follows:

* We are the _first_ to incorporate graph learning and state space modeling (SSM) for reconstructing robust 3D hand mesh. Our key idea is to reformulate the Mamba scanning into graph-guided bidirectional scanning for 3D reconstruction using a few effective tokens.
* We propose a simple yet effective Graph-guided State Space (GSS) block to capture structured relations between hand joints using graph convolution layers and Mamba blocks.
* We introduce a token sampler that effectively boosts performance. A fusion module is also introduced to further enhance performance by integrating state space tokens and global features.
* Extensive experiments on multiple challenging benchmarks demonstrate Hampa's superiority over current SOTAs, achieving impressive performance for in-the-wild scenarios.

## 2 Related Works

**3D Hand Reconstruction.** Multiple approaches have been proposed to reconstruct 3D hand mesh [2; 41; 62; 66; 67; 77; 80; 83; 85], with most works leveraging the MANO  parametric representation of the 3D hand. Zhang _et al._ utilized a CNN encoder to iteratively regress the hand mesh based on heatmaps under 2D, 3D, silhouette, and geometric constraints. I2L-MeshNet  proposed line pixel-based 1D heatmaps for estimating joint locations and regressing MANO parameters, while HandAR  estimated parameters through three stages: joint, mesh, and a refining stage to combine previous features. The joint stage applies a multitask decoder to predict both hand joints and the segmentation mask. MeshGraphormer  introduced a graph residual block into the transformer to enhance the spatial structure. HaMeR  showed that a simple but large transformer-based architecture trained on a large dataset can achieve SOTA performance. SimpleHand  sampled tokens with UV predictions on a high-resolution feature map, cooperating with a cascade upsampling decoder. They further compare different combinations of token generation strategies are compared, including global feature, grid sampling, keypoint sampling, \(4\) upsampling feature map, and coarse-mesh-guided point sampling. Recently, HHMR  proposed a graph diffusion model to learn a prior of gestures and inpaint the occluded hand portion. To further enhance performance, we propose the graph-guided state space model to leverage joint relations and capture spatial joint sequences.

**State Space Models (SSMs).** State space was originally elaborated in Kalman filtering  that described states and transitions with first-order differential equations. Structured State Space Sequence (S4) models [27; 28] have the capability to model dependencies. Recently, Mamba  further

Figure 2: **Motivation. Visual comparisons of different scanning flows. (a) Attention methods compute the correlation across all patches leading to a very high number of tokens. (b) Bidirectional scans follow two paths, resulting in less complexity. (c) The proposed graph-guided bidirectional scan (GBS) achieves effective state space modeling leveraging graph learning with a few effective tokens (illustrated as scanning by two snakes: forward and backward scanning snakes).**

improved the S4 models by expanding their fixed projection matrices linearly with the input sequence length. Many recent works have adapted Mamba for visual learning, leveraging its global receptive field and dynamic weights. Liu _et al._ and Yang _et al._ used Mamba for classification, segmentation, and object detection tasks. To effectively capture the spatial relations, they scanned the input image patches forward and backward horizontally. VMamba  further added two vertical directions creating a cross-scan. Zhang _et al._ designed a mambo model for motion generation, scanning unidirectionally along the temporal sequence and bidirectionally along channel dimensions in a hierarchy. Behrouz _et al._ and Wang _et al._ designed Graph-mambo to address traditional graph representation learning tasks, enhancing long-range context learning using Mamba blocks. Hamba makes the first attempt to adapt Mamba and graph learning to solve 3D hand reconstruction.

## 3 Proposed Methodology

We propose a novel Mamba-based method that incorporates graph learning and state space modeling to learn the joint relations from the joint spatial sequence (Figure 3). First, we introduce the concept of state space models (SSMs). Next, we provide the detailed principle of the proposed Token Sampler (TS), Graph-guided Bidirectional Scan (GBS), and Graph-guided State Space (GSS) modules.

### Preliminaries

**S6 Models.** Selective Scan Structured State Space Sequence (S6) models  is a category of sequence models that have demonstrated superior ability in handling sequences. These models are primarily an extension of the previously proposed S4 models , which maps an input sequence \(x(t) y(t)\), through the latent state \(h(t)^{N}\), following ordinary linear differential equations (Eq. 1), where \(^{N N}\), \(^{N 1}\), \(^{1 N}\) and \(D^{1}\) are the weighting parameters.

\[h^{}(t)=h(t)+x(t), h_{t}=}h_{t-1}+}x_{t},\] (2) \[y(t)=h(t)+Dx(t), y_{t}=h(t).\]

For practical computation, these continuous dynamical systems are discretized (Eq. 2). This is achieved by using the zero-order hold (ZOH) discretization rule (Eq. 3).

\[}=(),}=( {A})^{-1}(()-),\] (3)

where \(\) represents the discrete step size. Since both the weighting parameters and discretizing rules are fixed over time, S4 models can be viewed as linear time invariance systems. Mamba  further expands S4 models' projection matrices to scan the entire input sequence through a selective scan.

**Mamba for Visual Representation.** Since Mamba  is primarily designed for 1D data, it is challenging to directly apply it to image data with global spatial context and local relation information. Recent works  have extended Mamba for learning visual representations. VMamba  developed a 2D selective scan (SS2D) block and integrated it into the VSS Block (Figure 4(b)). The VSS block is then stacked consecutively with convolution layers for downsampling image patches via patch merging . The main difference between Mamba and VSS  block (Figure 4(a-b)) is replacing the S6 block with SS2D to adapt selective scanning for image data.

### Hamba

**Problem Formulation.** Given a single hand image \(I\), our goal is to reconstruct the 3D hand mesh. We learn the mapping function \(f(I)=\{,,\}\) that regresses MANO  parameters from the image \(I\), where \(^{48}\), \(^{10}\), and \(^{3}\) represent the pose, shape, and camera parameters, respectively. Finally, the MANO model \((,)\) generates the corresponding hand mesh \(M^{778 3}\).

**Model Architecture.** Figure 3 illustrates the Hamba model architecture. First, we feed the hand image \(I^{H W 3}\) into a ViT  backbone to extract tokens \(T^{H W}_{16} 1280\) where \(H\)=\(256\) and \(W\)=\(192\). Tokens from the backbone are downsampled from dimensions 1280 to 512 using convolution layers (Conv2D). Second, we sample effective tokens using a Token Sampler (TS), which utilizes the 2D joint locations predicted by a Joints Regressor (JR). These tokens are fed into the Graph-guided State Space (GSS) block, which exploits the joint spatial sequence by modeling its state space using the proposed Graph-guided Bidirectional scan (GBS). Finally, we fuse the GSS tokens with the global mean feature, the sampled tokens, and the 2D joint features via a fusion module. Lastly, the MANO parameters are regressed using MLP.

**Token Sampler (TS) and Joints Regressor (JR).** To prevent the GSS Block from being influenced by the background and unnecessary features during the early stages of the training, it is important to select effective tokens that encode the relations between hand joints. We propose a Token Sampler (TS), which selects effective tokens utilizing the initial 2D hand joint prediction from the Joints Regressor (JR). While it is possible to use off-the-shelf 2D joint estimator like OpenPose  or MediaPipe , this would increase model complexity. Previous works  primarily used Conv-Pooling-FC schemes for initial joints regression. In our work, the JR consists of stacked SS2D blocks followed by an MLP head which regresses the initial MANO parameters \(\{,,\}\). After the JR regresses 3D joints \(_{}^{21 3}\), these are projected back to the 2D image plane using perspective projection \(\) with the predicted camera translation \(\) to obtain \(_{}^{21 2}\). We denote a predefined focal length \(F_{}=5000\) mm. Those are formulated as,

\[,,=(T),_{}= \;(,),_{}=\;(_{},F_{},).\] (4)

To align the sampled tokens with 2D joints, we use bilinear interpolation. The sampled token \(T_{}^{C J}\) is formulated as,

\[T_{}=((T),_{}),\] (5)

where \(J\) denotes the total of 21 joints, and \(C\) is the token dimension of 512.

**Graph-guided Bidirectional Scan (GBS).** To achieve robust reconstruction and leverage effective tokens, we reformulate Mamba's unidirectional scanning as a graph-guided bidirectional scan, thus adapting it for 3D reconstruction tasks. GBS is designed to follow a specific graph pattern, considering the spatial and topological connection of the hand joints with image features. A naive approach would be scanning all tokenized image patches (Figure 2(b)). However, this involves redundant tokens, making it challenging to learn joint spatial relations effectively. To address this, we propose two novel ideas. First, instead of scanning all tokens unidirectionally, we perform hand joint-level bidirectional scanning of sampled tokens \(T_{}\). This effectively reduces the number of tokens to be scanned from 192 to 22 (\(\) 88.5% reduction). We adapt VMamba 's SS2D block for bidirectional scanning to be suitable for our joint spatial sequence. Second, to capture the local and global joint relations, we introduce a Semantic GCN block . Mamba learns long-range dependencies, but it is less effective at capturing fine-grained local information in intricate structures like the 3D mesh. The

Figure 3: **Overview of Hama’s architecture**. Given a hand image \(I\), tokens are extracted via a trainable backbone model and downsampled. We design a graph-guided SSM as a decoder to regress hand parameters. The hand joints (\(J_{}\)) are regressed by Joints Regressor (JR) and fed into the Token Sampler (TS) to sample tokens (\(T_{}\)). The joint spatial sequence tokens (\(T_{}\)) are learned by the Graph-guided State Space (GSS) blocks. Inside each GSS block, the GCN network takes \(T_{}\) as input and its output is concatenated with the mean down-sampled tokens. GSS leverages graph learning and state space modeling to capture the joint spatial relations to achieve robust 3D reconstruction.

GCN learns input-independent weight matrix to model the edges between hand joints, reflecting how one joint influences another based on prior embedded in graph structures. Introducing graph learning makes it possible to explicitly encode the graph structure within our GSS module. Let \(=\{,\}\) be the graph, \(\) is the set of \(J\) nodes and \(\) are the edges. \(T_{_{i}}\) represents the output of the \(l\)-th GCN block, while the complete output of the GSS block is \(T_{_{i}}\). For a graph-based propagation, we multiply the token with a learnable parameter matrix \(^{C C}\). Thus, the GCN operation is formulated as,

\[T_{_{i}}=(\ T_{}\ _{i}()),&,\\ (\ T_{_{l-1}}^{\{1,,21\}}\ _{i}( )),&,\] (6)

where \(^{J J}\) is the learnable weighting matrix, \(_{i}\) denotes the softmax non-linearity that is applied to normalize the input matrix for all node \(i\) choices, while \(^{J J}\) denotes the adjacency matrix of graph \(\) and \(\) denotes element-wise multiplication. \(J\) denotes the total of 21 joints, and \(C\) is the token dimension of 512.

**Graph-guided State Space (GSS) Block.** Overall, our decoder consists of \(L\) GSS blocks. The GSS architecture is illustrated comparatively in Figure 4. In the first GSS block, the sampled tokens \(T_{}\) are passed through graph convolution (GCN) layers. The GCN layer consists of a PGraphConv , a Batch Norm, and a ReLU activation. For the GCN, the adjacency matrix is defined based on the hand joint skeleton in the joint order of MANO. To provide the global context, the output from the GCN is concatenated with the global mean token along the joint token sequence. This global mean token is the mean of the downsampled image tokens. This concatenated sequence \(T_{_{i}}^{c}\) is then fed into the SS2D block and summed with the output through a residual connection. The SS2D block is followed by a Layer Norm (LN), a Feed-Forward Network (FFN), and another residual connection. For subsequent GSS blocks \(l\{2,..,L\}\), the input is the output from the previous block \(T_{_{l-1}}\). Before this sequence passes through its GCN layer, it is split, and only the first 21 tokens \(T_{_{l-1}}^{\{1,,21\}}\) are fed to the GCN. The global mean token \(T_{_{l-1}}^{\{22\}}\) is concatenated back with the GCN's output before it enters the SS2D block as shown in Eq. 7 below:

\[T_{_{l}}^{c}=T_{_{l}}( (T)),&,\\ T_{_{l}} T_{_{l-1}}^{\{22\}},&,\] (7)

\[T_{_{l}}=(((T_{_{l}}^{c})+T_ {_{l}}^{c}))+(T_{_{l}}^{c})+T_{_{l}}^{ c}.\] (8)

where \(\) denotes concatenation. The GSS block not only leverages features from state space modeling and graph learning but also considers global features. This design enables Hampa to learn effective features to enhance performance by incorporating state space modeling and graph learning with few tokens, shown in our ablation study Section 4.2.

**State Space Modeling for Joint Spatial Sequence.** Different from the video-based Mamba models , which learns the temporal feature with the frame sequence, Hampa focuses on the joint spatial sequence per frame and reveals that modeling joint relations with Mamba  can significantly

Figure 4: The illustration of the proposed Graph-guided State Space (GSS) block.

improve the 3D reconstruction performance. In particular, as shown in Eq. 1, \(x(t)\) represents \(t\)-th token of the joint spatial sequence, which is first sampled by the TS using the JR and then encoded with the GCN. Note that \(t\) denotes the index of the hand joint iteration. Lastly, \(y(t)\) is the updated token of the \(t\)-th of the joint spatial sequence after passing through GSS Blocks. The proposed GSS block effectively enhances 3D reconstruction performance by learning the joint spatial sequence relations with graph learning and state space modeling.

**Loss Functions.** Following , we train Hampa using a combined loss which includes 2D joint loss \(_{}\), 3D joint loss \(_{}\), pose loss \(_{}\), shape loss \(_{}\), and an adversarial loss \(_{}\). \(_{}\) and \(_{}\) are calculated using the L1 Norm, while \(_{}\) and \(_{}\) use the L2 Norm. The training loss \(_{}\) is defined as Equation 9, where \(_{},~{}_{},~{}_{},~{}_{ }\), and \(_{}\) denote each term's weight respectively.

\[_{}=_{}_{}+ _{}_{}+_{}_{}+ _{}_{}+_{}_{},\] (9)

## 4 Experiments

**Datasets.** We train Hampa on 2.7M training samples from multiple datasets (same setting as  for a fair comparison) that had either both 2D and 3D hand annotations or just 2D annotations. This included FreiHAND , HO3D , MTC , RHD , InterHand2.6M , H2O3D , DexYCB , COCO-Wholebody , Halpe , and MPII NZSL  datasets.

**Implementation Details.** We set learning rate as \(10^{-5}\), weight decay factor as \(10^{-4}\), with the'sum' loss. Weights for each term in the loss function are \(_{3D}=0.05\) for 3D keypoint loss, \(_{2D}=0.01\) for 2D keypoint loss, \(_{}=0.001\) for global orientation and hand pose loss. Weights for beta and adversarial loss, i.e., \(_{}\) and \(_{adv}\) were set as 0.0005. Ablations were run for 60k steps due to computational limitations on 2.7M dataset. Additional details are included in the Appendix A.

**Evaluation Metrics.** Following the same protocols employed in previous works [52; 70; 107], we used PA-MPJPE and AUC\({}_{J}\) as the metrics for evaluating the reconstructed 3D joints and PA-MPVPE, AUC\({}_{V}\), F@5mm, and F@15mm for evaluating the reconstructed 3D mesh vertices.

### Main Results

**3D Joints and Mesh Reconstruction Evaluation.** We test Hampa on 3 widely used benchmarks: FreiHAND , HO3Dv2 , and HO3Dv3 . The quantitative comparison with state-of-the-art 3D hand reconstruction models is presented in Table 1, Table 2, and Table 3 respectively. Since almost all previous methods (including the popular MobRecon , MeshGraphormer , and the recent HHMR , SimpleHand ) were trained only using the FreiHAND , for a fair comparison, we compared them with the Hampa version trained using only the FreiHAND  dataset. Meanwhile, for a fair comparison with HaMeR , we trained Hampa on the same datasets as HaMeR  for all other comparisons. Many methods, including the popular MeshGraphormer  and METRO , report their metrics using Test-Time Augmentation (TTA) which boosts the final results. We report our performances, both with and without TTA. In both scenarios, Hampa significantly achieves better results, outperforming SOTAs in all benchmarks.

**In-the-wild Generalizability Evaluation.** Approximately \(95\%\) of datasets used for training previous models [9; 14; 47; 51; 52; 70; 86; 107] were collected in controlled indoor environments, such as studios or multi-camera setups. This includes the FreiHAND , HO3Dv2 , and HO3Dv3  benchmarks that are popularly used for both training and evaluation. However, training models on datasets collected in controlled environments often leads to decreased performance in real-world scenarios. Thus, solely evaluating performance over indoor-collected datasets might not provide a correct evaluation of the robustness of 3D hand reconstruction. We additionally evaluate Hampa's in-the-wild performance on the recently proposed HInt  benchmark, which has variations in visual conditions, viewpoints, and hand interactions. Since HInt-NewDays  and HInt-EpicKitchensVISOR [16; 17] annotations are 2D keypoints, PCK  computed at varying thresholds is used as the evaluation metrics. As shown in Table 5, Hampa outperforms existing models by a large margin and surpasses HaMeR , showing improvement in model robustness for in-the-wild scenarios. None of the models (including Hampa) have been trained on/ previously ever seen HInt dataset.

**Qualitative Comparison.** Figure 5 presents the qualitative comparison of Hampa's 3D hand mesh reconstruction with SOTA models on in-the-wild images from HInt-EpicKitchens. This includes models that directly regress vertices (METRO , MeshGraphormer ), and parametric methods, which regress MANO parameters (FrankMocap , HaMeR ). These images are particularly challenging since they comprise real-world cooking videos of a person with highly occluded hands, hand-hand, and/or hand-object interactions. For visual comparison, we select images where the hand lies in the corners, causing a truncation scenario thus increasing the complexity further. Hampa consistently outperforms other models and achieves a much better reconstruction. From Figure 5, we can observe that in severe in-the-wild truncation scenarios, Hampa achieves better hand reconstruction, even though the hand is truncated or occluded. We attribute this performance to effectively learning the spatial hand joint sequence with the state space model. The same is verified in the ablation study presented in Sec. 4.2. Figure S5 presents in-the-wild results on various movies, interviews, etc., scenarios. Figure S6 and Figure S7 presents additional visual results on HInt-NewDays and HInt-EpicKitchensVISOR respectively. Hampa can robustly reconstruct 3D hands in various complicated hand gestures like grasping, holding, grabbing, finger-pointing, and flattening from different viewing directions, even in heavily occluded and truncated scenarios.

   Method & Venue & Backbone & PA-MPJPE \(\) & PA-MPVPE \(\) & F@5mm \(\) & F@15mm \(\) \\  Zimmermann _et al._ & ICCV 19 & ResNet50 & - & 10.7 & 0.529 & 0.935 \\ Boukhayma _et al._ & CVPR 19 & ResNet50 & - & 13.0 & 0.435 & 0.898 \\ ObMan  & CVPR 19 & ResNet18 & - & 13.2 & 0.436 & 0.908 \\ MobileHand  & ICOINIP 20 & MobileNet & - & 13.1 & 0.439 & 0.902 \\ YoutubeHand  & CVPR 20 & ResNet50 & 8.4 & 8.6 & 0.614 & 0.966 \\ Pose2Mesh  & ECCV 20 & – & 7.7 & 7.8 & 0.674 & 0.969 \\ I2L-MeshNet  & ECCV 20 & ResNet50\({}^{*}\) & 7.4 & 7.6 & 0.681 & 0.973 \\ HIU-DMTL  & ICCV 21 & Custom\({}^{*}\) & 7.1 & 7.3 & 0.699 & 0.974 \\ CRM  & CVPR 21 & ResNet50\({}^{*}\) & 6.9 & 7.0 & 0.715 & 0.977 \\ I2UV-HandNet  & ICCV 21 & ResNet50 & 6.7 & 6.9 & 0.707 & 0.977 \\ Tang _et al._ & ICCV 21 & ResNet50 & 6.7 & 6.7 & 0.724 & 0.981 \\ METRO\({}^{}\) & CVPR 21 & HRNet & 6.3 & 6.5 & 0.731 & 0.984 \\ MeshGraphormer\({}^{}\) & ICCV 21 & HRNet & 5.9 & 6.0 & 0.764 & 0.986 \\ MobRecon  & CVPR 22 & ResNet50\({}^{*}\) & 5.7 & 5.8 & 0.784 & 0.986 \\ FastNetTRO  & ECCV 22 & HRNet & 6.5 & 7.1 & 0.687 & 0.983 \\ FastViT  & ICCV 23 & FastViT & 6.6 & 6.7 & 0.722 & 0.981 \\ AMVUK  & CVPR 23 & ResNet50 & 6.2 & 6.1 & 0.767 & 0.987 \\ Deformer  & CVPR 23 & HRNet & 6.2 & 6.4 & 0.743 & 0.984 \\ PointHMR  & CVPR 23 & HRNet & 6.1 & 6.6 & 0.720 & 0.984 \\ Zhou _et al._ & CVPR 24 & FastViT & **5.7** & 6.0 & 0.772 & 0.986 \\ HaMeR  & CVPR 24 & ViTPose & 6.0 & 5.7 & 0.785 & 0.990 \\ HaMeR-170k  & CVPR 24 & ViTPose & 6.1 & 5.8 & 0.782 & 0.990 \\ HHMR\({}^{}\) & CVPR 24 & ResNet50 & 5.8 & 5.8 & - & - \\ 
**Hamba** & **Ours** & ViTPose & 5.8 & 5.5 & 0.798 & 0.991 \\
**Hamba\({}^{}\)** & **Ours** & ViTPose & **5.7** & **5.3** & **0.806** & **0.992** \\   

Table 1: Comparison with SOTAs on **FreiHAND** dataset . \({}^{*}\)Stacked Structure; \({}^{}\)used Test-Time Augmentation (TTA). Best scores highlighted Green, while second best are highlighted Light Green. PA-MPJPE and PA-MPVPE are measured in mm. -: Info not reported by model.

Figure 5: **Qualitative in-the-wild comparison** of the proposed Hampa with SOTAs on HInt-EpicKitchensVISOR . None of the models (including Hampa) have been trained on HInt.

### Ablation Studies

**Effect of Branch-wise Features.** We verify the effectiveness of each branch feature by excluding their respective tokens from the fusion module as shown in Table 4. First, we verify the contribution of the proposed GSS branch. When the GSS tokens are excluded (Row 3), we observe a major drop in model performance. Specifically, F@5mm (\(\)) drops from 0.738 \(\) 0.717, and the PA-MPJPE (\(\)) and PA-MPVPE (\(\)) errors increase from 6.6 \(\) 6.9 and 6.3 \(\) 6.6. Thus, in addition to local and global contexts, incorporating structured state-space representations can be effective for 3D hand reconstruction. Moreover, it is important to note that modeling spatial joint sequence relations provides better tokens than directly using the 2D joint locations, even though the latter has a clear semantic meaning for all the hand joints. We attribute this to cases of occlusions where the 2D joints cannot be precisely predicted.

Removing the Token sampler (Row 1) or the 2D joints (Row 2) features also shows a performance drop, but is less significant than removing the GSS branch, since they only provide the local context while GSS tokens provide both local and spatial-relations information. Note that the Global Mean token (Row 4) remains important since it captures the global context, which is discarded in the ablation.

**Effect of proposed Components.** Since the GSS Block stands as a major contribution, we additionally evaluate the effectiveness of each

   Method & Venue & PA-MPJPE\(\) & PA-MPVPE\(\) & F@5mm\(\) & F@15mm\(\) & AUC\({}_{J}\) & AUC\({}_{V}\) \\    Branch-wise \\ 1 \\  } & CVPR 21 & 11.5 & 11.1 & 0.448 & 0.932 & 0.769 & 0.778 \\  & CVPR 22 & 10.9 & - & - & 0.785 & 0.785 \\  & CVPR 22 & 10.8 & 10.4 & - & - & - \\  & CVPR 23 & 9.3 & 9.1 & 0.552 & 0.956 & 0.814 & 0.818 \\  & AMVULR  & CVPR 23 & 8.7 & 8.3 & 0.593 & 0.964 & 0.826 & 0.834 \\  & MVUZ 24 & 10.1 & - & - & - & - & - \\   SPMHand \\  } & MVUZ 24 & 10.1 & - & - & - & - & - \\  & TMM 24 & 8.8 & 8.6 & 0.574 & 0.962 & - & - \\   

Table 4: **Ablation study on FreiHAND** to verify proposed components. All variants are trained for same number of steps. PA-MPJPE, PA-MPVPE and without are abbreviated as PJ, PV, ‘w/o’.

   Method & Venue & PA-MPJPE\(\) & PA-MPVPE\(\) & F@5mm\(\) & F@15mm\(\) & AUC\({}_{J}\) & AUC\({}_{V}\) \\  ObMan  & CVPR 19 & 11.0 & 11.2 & 0.464 & 0.939 & 0.780 & 0.777 \\ Pose2Mesh  & ECCV 20 & 12.5 & 12.7 & 0.441 & 0.909 & 0.754 & 0.749 \\ I2L-MeshNet  & ECCV 20 & 11.2 & 13.9 & 0.409 & 0.932 & 0.775 & 0.722 \\ Hampali _et al._ & CVPR 20 & 10.7 & 10.6 & 0.506 & 0.942 & 0.788 & 0.790 \\ S2Hand  & CVPR 21 & 11.4 & 11.2 & 0.450 & 0.930 & 0.773 & 0.777 \\ METRO  & CVPR 21 & 10.4 & 11.1 & 0.484 & 0.946 & 0.792 & 0.779 \\ Liu et al.  & CVPR 21 & 9.9 & 9.5 & 0.528 & 0.956 & 0.803 & 0.810 \\ I2UV-HandNet  & ICCV 21 & 9.9 & 10.1 & 0.500 & 0.943 & 0.804 & 0.799 \\ Tse _et al._ & CVPR 22 & - & 10.9 & 0.485 & 0.943 & - & - \\ ArtBoost  & CVPR 22 & 11.4 & 10.9 & 0.488 & 0.944 & 0.773 & 0.782 \\ KPT-Transf  & CVPR 22 & 10.8 & - & - & - & 0.786 & - \\ MobRecon  & CVPR 22 & 9.2 & 9.4 & 0.538 & 0.957 & - & - \\ HandOCNet  & CVPR 22 & 9.1 & 8.8 & 0.564 & 0.963 & 0.819 & 0.819 \\ HFL-Net  & CVPR 23 & 8.9 & 8.7 & 0.575 & 0.965 & - & - \\ H2ONet  & CVPR 23 & 8.5 & 8.6 & 0.570 & 0.966 & 0.829 & 0.828 \\ AMVULR  & CVPR 23 & 8.3 & 8.2 & 0.608 & 0.965 & 0.835 & 0.836 \\ HOISDF  & CVPR 24 & 9.2 & - & - & - & - & - \\ HandBooster  & CVPR 24 & 8.2 & 8.4 & 0.585 & 0.972 & 0.836 & 0.832 \\ HaMeR  & CVPR 24 & 7.7 & 7.9 & 0.635 & 0.980 & 0.846 & 0.841 \\ HaMeR-170k  & CVPR 24 & 7.6 & 7.9 & 0.639 & 0.981 & 0.848 & 0.843 \\  
**Hamba** & **Ours** & **7.5** & **7.7** & **0.648** & **0.982** & **0.850** & **0.846** \\   

Table 2: Comparison with SOTAs on HO3Dv2 hand-object interaction benchmark.

   Method & Venue & PA-MPJPE\(\) & PA-MPVPE\(\) & F@5mm\(\) & F@15mm\(\) & AUC\({}_{J}\) & AUC\({}_{V}\) \\  S\({}^{2}\)HAND  & CVPR 21 & 11.5 & 11.1 & 0.448 & 0.932 & 0.769 & 0.778 \\ KPT-Transf.  & CVPR 22 & 10.9 & - & - & 0.785 & 0.792 \\ AntiBoost  & CVPR 22 & 10.8 & 10.4 & - & - & - \\  & MVUZ 2 & 10.8 & 10.4 & - & - & - \\  & MVUZ & 23 & 9.3 & 9.1 & 0.552 & 0.956 & 0.814 & 0.818 \\ AMVULR  & CVPR 23 & 8.7 & 8.3 & 0.593 & 0.964 & 0.826 & 0.834 \\ HMP  & WACV 24 & 10.1 & - & - & - & - \\  & TVM 24 & 8.8 & 8.6 & 0.574 & 0.962 & - & - \\  
**Hamba** & **Ours** & **6.9** & **6.8** & **0.681** & **0.982** & **0.861** & **0.864** \\   

Table 3: Evaluation on **HO3Dv3** benchmark. We only list SOTAs that reported on HO3Dv3.

[MISSING_PAGE_FAIL:10]