# Gradient Descent with Linearly Correlated Noise:

Theory and Applications to Differential Privacy

 Anastasia Koloskova

EPFL, Switzerland

&Ryan McKenna

Google Research

&Zachary Charles

Google Research

&Keith Rush

Google Research

&Brendan McMahan

Google Research

Work performed while doing an internship at Google Research. Correspondence to: Anastasia Koloskova canastasia.koloskova@epfl.ch>, Ryan McKenna <mckenna@google.com>.

###### Abstract

We study gradient descent under linearly correlated noise. Our work is motivated by recent practical methods for optimization with differential privacy (DP), such as DP-FTRL, which achieve strong performance in settings where privacy amplification techniques are infeasible (such as in federated learning). These methods inject privacy noise through a matrix factorization mechanism, making the noise linearly correlated over iterations. We propose a simplified setting that distills key facets of these methods and isolates the impact of linearly correlated noise. We analyze the behavior of gradient descent in this setting, for both convex and non-convex functions. Our analysis is demonstrably tighter than prior work and recovers multiple important special cases exactly (including anti-correlated perturbed gradient descent). We use our results to develop new, effective matrix factorizations for differentially private optimization, and highlight the benefits of these factorizations theoretically and empirically.

## 1 Introduction

Differential privacy (DP) is a critical framework for designing algorithms with provable statistical privacy guarantees. DP stochastic gradient descent (DP-SGD, Abadi et al. ) is particularly important for enabling private empirical risk minimization (ERM) of machine learning models. Many works have analyzed the convergence behavior of DP ERM methods, including DP-SGD . However, obtaining good privacy/utility trade-offs with DP-SGD can require excessively large batch sizes or privacy amplification techniques such as subsampling  and shuffling . In some applications, including cross-device federated learning, limited and device-controlled client availability can make sampling or shuffling infeasible . Even outside of such applications, many implementations of DP-SGD do not properly use the Poisson subsampling scheme analyzed by Abadi et al.  for amplification, and instead use a single fixed permutation of the dataset .

Kairouz et al.  propose an alternative method, DP-FTRL, which can attain good privacy/utility trade-offs without amplification. Their key insight is that for SGD-style algorithms, the variance on _prefix sums_\(_{0}++_{t}\), \(t\{1,,T\}\) of gradients \(_{j}\) is more important than the variance on individual gradients. By adding carefully tailored noise that is _linearly correlated_ over iterations to the gradients, one can reduce the error on the prefix sums, at the cost of increased error on the individual gradients, for a fixed privacy budget. The DP-FTRL mechanism is competitive with or better than DP-SGD, even without relying on privacy amplification, and enabled McMahan and Thakurta  to train the first differentially private machine learning model on user data in a production setting.

Denisov et al. , Choquette-Choo et al.  develop a refinement of DP-FTRL, MF-DP-FTRL, by formulating and solving an offline matrix factorization problem to find the "optimal" correlated noise structure under DP constraints. That is, for a fixed privacy level, they aim to find correlated noise structures that lead to improved optimization. A simplified diagram of their workflow is given in Fig. 1. However, (as we detail in Section 2) their offline factorization objective is based on an online convergence bound that is loose. This raises questions about whether there are factorization objectives that better capture convergence behavior of gradient descent algorithms with correlated noise.

In this paper we study this class of mechanisms more closely and provide a detailed analysis of linearly correlated noise from an optimization point of view. Our main contributions are as follows:

* We propose a novel stochastic optimization problem that extracts key facets of methods like (MF-)DP-FTRL, and which isolates the effects of linearly correlated noise on optimization.
* We derive convergence rates for gradient descent on smooth convex and non-convex functions in such settings that showcase the effect of linearly correlated noise and recover tight convergence rates in notable special cases. We use a novel proof technique that may be of independent interest.
* We use this theory to design a new objective for the offline matrix factorization workflow in Fig. 1. We show that solving this objective leads to MF-DP-FTRL mechanisms with improved convergence properties. We validate the mechanism empirically on a variety of datasets and tasks, matching or outperforming prior methods.

### Related Work

Matrix mechanisms for differential privacy.Our work is closely related to differentially private optimization using matrix mechanisms . Historically, such mechanisms were applied to linear statistical queries [25; 29; 14; 18]. Denisov et al.  and Choquette-Choo et al.  extended these mechanisms to the adaptive streaming setting, allowing their application to optimization with DP. Denisov et al.  show that this framework (MF-DP-FTRL) subsumes and improves the DP-FTRL algorithm . Both DP-FTRL and MF-DP-FTRL improve privacy guarantees relative to DP-SGD  without amplification, and can be combined with techniques such as momentum for improved utility . The aforementioned work focuses on methods for computing factorizations, privacy properties, and empirics. Our work studies the analytic relationship between the correlated noise induced by the MF-DP-FTRL framework and the downstream effect on optimization performance.

SGD with correlated noise.Stochastic noise in optimization arises in a variety of ways, including mini-batching  and explicit noise injection [11; 54; 19]. While most analyses of SGD assume this noise is independent across iterates, some work considers correlated noise. For example, shuffle SGD involves correlated noise due to sampling without replacement [33; 53]. Lucchi et al. 

Figure 1: Two-stage MF-DP-FTRL workflow proposed by Denisov et al. . The user selects a workload matrix \(\) representing a desired first-order optimization method. Offline, the user finds a factorization \(=\), using an objective that balances ERM performance (as a function of \(\)) and privacy (as a function of \(\)). The user applies \(\) to a downstream ERM task, but with linearly correlated additive noise governed by \(\).

use correlated Brownian motion to improve SGD's ability to explore the loss landscape. Recently, Orvieto et al. [37; 38] investigated anti-correlated noise as a way to impose regularization and improve generalization. We consider a linearly correlated noise model, and analyze its impact on SGD's convergence to critical points.

SGD with biased noise.Many algorithms can be viewed as SGD with structured but potentially biased noise, including SGD with (biased) compression [44; 17], delayed SGD [28; 12], local SGD , federated learning methods [22; 52; 34; 36], decentralized optimization methods [50; 23], and many others. Convergence analyses for such methods often use techniques like perturbed iterate analysis . Correlated gradient noise also biases the gradient updates. However, as we show in Section 4, directly applying such techniques to linearly correlated noise does not lead to tight convergence guarantees.

## 2 Background

In this work, we focus on an empirical risk minimization (ERM) problem of the form

\[_{^{d}}[f()=_{i=1}^{ n}l(,_{i})],\] (1)

where \(l(,_{i})\) is the loss of a model \(\) on a data point \(_{i}\), and \(n\) is the training set size. We would like to solve (1) while guaranteeing some form of privacy for the training set. We focus on _differential privacy_ (DP, ), a widely-used standard for anonymous data release. DP guarantees statistical difficulty in distinguishing whether or not a particular unit's data served as an input to a given algorithm, based on the algorithm's output. This protected unit may represent a single training example or a semantically higher-level unit like the entirety of a user's data.

While there are many methods for solving (1), we will follow Denisov et al. , Choquette-Choo et al.  and restrict to first-order algorithms \(\) that linearly combine (stochastic) gradients. Each algorithm \(\) is parameterized by a learning rate \(>0\), a number of steps \(T>0\), and scalars \(\{a_{tj}\}_{1 j t T}\). Given a starting point \(_{0}\), \(\) produces iterates \(_{t}^{d}\) given by

\[_{t+1}=_{0}-_{t}(_{1},, _{t}) _{t}(_{1},,_{t})=_{j=1}^{t}a_ {tj}_{j}\]

where \(_{t}\) is a (mini-batch) gradient of \(f\) computed at \(_{t}\). This class encompasses a variety of first-order algorithms, including SGD , SGD with momentum [39; 35], and delayed SGD . This class also captures algorithms that use learning rate scheduling, so long as the schedule is independent of the gradient values. We re-write the output of \(\) in matrix notation by defining:

\[ =[_{1},,_{T}]^{} ^{T d},\ _{0}=[_{0},,_{0}]^{} ^{T d}\] \[ =[_{1},,_{T}]^{} ^{T d},\ =[a_{ij}]_{1 i,j T} ^{T T}\]

Here \(\) is the _workload matrix_ representing \(\). At iteration \(t\), \(\) can only use the current and previous gradients, so \(a_{tj}=0\) for \(j>t\) (ie. \(\) is lower-triangular). In this notation, the iterates of \(\) satisfy

\[=_{0}-.\] (2)

**Example 2.1** (Sgd).: Define the _prefix-sum_ matrix \(^{T T}\) as the all-ones lower-triangular matrix. If \(=\), then (2) is simply SGD with learning rate \(\). As discussed by Denisov et al. [10; Section 4], we also recover SGD with momentum using an appropriate transformation \(^{}\) of \(\).

### Matrix Factorization and Privacy Mechanisms

In order to make the output of (2) differentially private, we typically need to clip the gradients and add noise. Let \(}\) denote the matrix whose rows (gradients) have been clipped to some \(_{2}\) threshold \(\). Let \(^{T d}\) be a matrix with entries drawn independently from \((0,}}{{d}})\). The well-known DP-SGD algorithm  adds this noise to each clipped gradient, so that

\[=_{0}-(}+).\] (3)

For consistency, we consider (2) to be the special case of (3) where \(=\) and \(=\). The variance \(^{2}\) depends on the clipping threshold \(\) and desired \((,)\) privacy we aim to achieve .

To derive algorithms with improved DP guarantees, Denisov et al.  add the noise \(\) to a factorized version of \(\). For a factorization \(=\) with \(,^{T T}\), we add noise to the iterates via:

\[=_{0}-( }+())_{0}- (}+() ).\] (4)

Here, \(()\) is a number representing the sensitivity of the mapping \(}}\) to "adjacent" input changes. We note that the sensitivity changes depending on the notion of adjacency. In single-epoch settings, two input matrices are adjacent if they differ by a single row , so the sensitivity function is \(():=_{i\{1,,T\}}\|_{[:,i]}\|_{2}\), i.e. the maximum \(_{2}\)-squared column norm of \(\). For details and extensions to multiple epochs, see .

If the variance of entries of \(\) is fixed to some value \(^{2}/d\), then for all the possible factorizations \(=\) in (4) have exactly same privacy guarantees, depending only on \(\). It will also be convenient to define \(=()\) as the 'effective' variance of \(\) after re-scaling by the sensitivity. Note that for a fixed \(\), the privacy guarantees of (4) might be different depending on the sensitivity.

The factorization \(=,=\) recovers DP-SGD (3), but factorizations with better privacy-utility trade-offs may exist. The formulation of Eq. (4) transfers the linear optimization algorithm (2) into the setting of the matrix mechanism , a well-studied family of mechanisms in differential privacy. Denisov et al. , Choquette-Choo et al.  show that the mechanism in Eq. (4) provides a DP guarantee equivalent to a single application of the Gaussian mechanism, which can be computed tightly using numerical accounting techniques .

Finding good factorizations.Intuitively, a factorization \(=\) is good if \(()\) is small and the added noise \(\) does not significantly degrade the convergence of (4). In order to quantify the effect of this added correlated noise on optimization, Denisov et al.  derive an online regret bound for (4) in the convex case against an adaptive adversary. Translating this via online-to-batch convergence to the stochastic setting, the iterates \(_{t}\) satisfy

\[_{t=0}^{T}[f(_{t})-f^{}] (_{0}-^{}\|^{ 2}}{ T}+^{2}+( )\|\|_{F}}{})\] (5)

where \(\) is the Lipshitz constant of \(f\). Denisov et al.  therefore use \(()\|\|_{F}\) as a proxy for the impact of the factorized noise scheme on convergence. To find factorizations with good convergence properties, Denisov et al. , Choquette-Choo et al.  minimize \(()\|\|_{F}\) subject to the constraint \(=\), which is equivalent to the following objective:

_Problem 2.2_ (Minimal-Norm Matrix Factorization).: Given a lower triangular matrix \(^{T T}\), define \(_{F}()=(,)\), where \(,^{T T}\) solve the following optimization problem.

\[_{,}\|\|_{F}^{2}\ \ \ \  =,\ \ ()=1.\] (6)

Eq. (6) is well-studied in the privacy literature and can be solved with a variety of numerical optimization algorithms . We also note that Denisov et al.  show that without loss of generality, we can assume \(\) and \(\) are lower triangular.

Finding improved factorizations.We argue that (5) is pessimistic in stochastic settings. For SGD (when \(=\)), the last term in (5) is \((\,())\), which diverges with \(T\) for a constant stepsize. However, under the same assumptions as in , SGD with constant stepsize actually achieves a faster rate of \((\,())\) (see ).

In this paper, we turn our attention to the _smooth functions_ in order to focus on non-convex functions. We show in Appendix A, there are matrices \(_{1},_{2}\) such that \((_{1})\|_{1}\|_{F}=( _{2})\|_{2}\|_{F}\), but Eq. (4) diverges with \(_{1}\) and converges with \(_{2}\), therefore showing that Frobenius norm is not the right measure in the smooth case as well.

This begs the question of whether there are objectives that better capture the impact of the noise injected in (4) on convergence. To answer this, we derive a bound that can exhibit better dependence on \(\) to design better factorizations for differentially private optimization.

Problem Formulation

To study the effect of the noise \(\) on optimization, we analyze a slightly simplified objective that omits parts of (4) not directly related to linear noise correlation. We do this as follows:

1. We assume that each \(_{t}\) is the true gradient at the point \(_{t}\), i.e. \(_{t}= f(_{t})\).
2. We omit gradient clipping from our analysis. Alternatively, we can view this as setting the clipping threshold \(=\) so that \(}=\) in (4).
3. We restrict the class \(\) to SGD-type algorithms where \(=\), as in Example 2.1.

We impose (I) for simplicity of presentation. Our results can be extended to stochastic gradients in a direct fashion. Restriction (II) is also for simplicity. First, clipping is not directly applied to the noise \(\). Second, for bounded domains or Lipschitz \(f\), our analysis still holds with clipping. Last, practical DP methods often use adaptive clipping  instead of fixed clipping. We are not aware of convergence analyses for such schemes. We impose (III) in order to limit the class of algorithms \(\) to a well-understood subclass. The convergence properties of (2) for general matrices \(\) are not well-understood even when there is no noise (\(=\)). As we discuss in Section 4, even with these simplifications, the effect of \(\) is not well-understood.

Due to (III), we study factorizations \(\) of the matrix \(=\), as in Example 2.1. Then, (4) becomes

\[=_{0}-(+() ).\] (7)

In vector notation, for \(_{0}=\) and \(=[_{1},_{T}]^{}\),

\[_{t+1}=_{t}-[ f(_{t})+( _{t+1}-_{t})^{}],\] (8)

where for simplicity of presentation, we re-scaled the noise \(\) by the sensitivity, \(^{2}=^{2}()^{2}\). We now discuss several noteworthy special cases of (8).

**Example 3.1** (PGD).: If \(=\) (see Example 2.1) we recover SGD with uncorrelated additive noise, also known as _perturbed gradient descent_ (PGD), where

\[_{t+1}=_{t}-[ f(_{t})+_{t+1}].\] (9)

The convergence rate of SGD (and therefore PGD) is well-understood in the optimization literature (e.g. see Bubeck [6, Section 6]).

**Example 3.2** (Anti-PGD).: By setting \(=\), we get an algorithm that at every iteration adds an independent noise vector \(_{t+1}\) and subtracts the previously added noise \(_{t}\):

\[_{t+1}=_{t}-[ f(_{t})+ _{t+1}-_{t}],\ \ _{0}=\] (10)

Intuitively, this removes some of the noise added in the prior round. This is (up to a learning rate factor) the _anti-correlated perturbed gradient descent_ (Anti-PGD) method proposed by Orvieto et al. , who study its generalization properties. Anti-PGD is also equivalent to SGD with randomized-smoothing . The equivalence follows from defining \(}_{t}=_{t}+_{t}\) and rewriting (10) as

\[}_{t+1}=}_{t}- f(}_{t}-_{t}).\]

While randomized smoothing algorithm is popular for non-smooth optimization, Vardhan and Stich  analyze its convergence properties in the smooth non-convex setting.

**Example 3.3** (Tree Aggregation DP-FTRL).: For \(k 1\) and \(t=2^{k-1}\), define \(_{k}^{(2^{k}-1) t}\) recursively as follows:

\[_{1}=(1),\ _{k+1}=_{k}& \\ &_{k}\\ &\]

where \(\) above represents an all-ones row of appropriate width. For \(T=2^{k-1}\), if \(=_{k}\) and \(=_{k}^{}\) where \(_{k}^{}\) denotes a carefully chosen right pseudo-inverse of \(\), then we recover the same noise matrix \(\) as in the DP-FTRL algorithm with either the online or full Honaker estimator (depending on the choice of \(^{}\)) as in . Note that \(,\) are not square. This can be remedied by appropriately projecting onto \(^{T}\). See Choquette-Choo et al. [7, Appendix D.3] for details.

Deriving Tighter Convergence Rates

We would like convergence rates for (7) that apply to any factorization and yield tight convergence rates for notable special cases. We pay special attention to PGD (Example 3.1) and Anti-PGD (Example 3.2), as they represent extremes in the space of factorizations (\(=\) and \(=\), respectively). As we will show, it is possible to use existing theoretical tools to derive tight convergence rates for both, _but not simultaneously_.

Below, we discuss ways to derive tight rates for PGD and Anti-PGD, and how these rates involve incompatible analyses. We then develop a novel analytic framework involving _restart iterates_ that allows us to analyze both methods simultaneously, as well as (7) for general factorizations. We start by formally stating our assumptions. For simplicity of presentation, we re-scale the noise \(\) by the sensitivity of \(\), i.e. \(^{2}=^{2}()^{2}\); we will suppress the \(\) dependence of \(\).

**Assumption 4.1** (Noise).: The rows \(_{1},,_{T}\) of the noise matrix \(\) are independent random vectors such that \( t\), \([_{t}]=\) and \(\|_{t}\|^{2}^{2}\).

We do not assume \(\)-Lipshitzness in our results, but we do assume \(L\)-smoothness. This is a relatively standard assumption in optimization literature .

**Assumption 4.2** (\(L\)-smoothness).: The function \(f:^{d}\) is differentiable, and there exists \(L>0\) such that for all \(x,y^{d}\), \(\| f()- f()\| L\|-\|\).

For _some_ of the results we will assume convexity.

**Assumption 4.3** (Convexity).: The function \(f:^{d}\) is convex, i.e. \(,^{d},f()-f()  f(),-\). When assuming convexity, we also assume the infimum of \(f\) is achieved in \(^{d}\).

### Convergence Rates for PGD and Anti-PGD

In this section we discuss the (distinct) convergence analyses of PGD and Anti-PGD, and the suboptimal results derived by trying to apply the proof technique for one to the other. We focus on the convex setting for brevity, though these analyses can be directly extended to the non-convex setting.

Pgd.The convergence of PGD (Example 3.1) is well-understood since it is a special case of SGD. One can show the following.

**Proposition 4.4** (Adapted from Dekel et al. [9, Theorem 1]).: _Under Assumptions 4.1, 4.2 and 4.3, if \(=\) and \(<}{{2L}}\), then the output of (7) satisfies_

\[_{t=0}^{T}[f(_{t})-f^{} ]}{T+1}(_{0}-^{ }\|^{2}}{ T}+^{2}).\] (11)

The proof follows from combining the update (9), standard facts about convex functions, and the fact that \(<}{{2L}}\), to get the inequality

\[_{t}\|_{t+1}-^{}\|^ {2} \|_{t}-^{}\|^{2}-(f( _{t})-f^{})+^{2}^{2}.\]

It is left to average over iterations \(0 t T\).

Anti-Pgd.For Anti-PGD (Example 3.2), one can show the following.

**Proposition 4.5**.: _Under Assumptions 4.1, 4.2 and 4.3, if \(=\) and \(<}{{2L}}\), then the output of (7) satisfies_

\[_{t=0}^{T}[f(_{t})-f^{} ]}{T+1}(_{0}-^{ }\|^{2}}{ T}+L^{2}^{2})\] (12)

Since \(L<}{{2}}\), the RHS of (12) is strictly smaller than the RHS of (11). While this result may be known, we were unable to find a reference, so we provide a complete proof in Appendix D. The proof utilizes perturbed iterate analysis . We define a _virtual sequence_\(\{}_{t}\}_{t=0}^{T}\) as follows:

\[}_{t+1}=}_{t}- f( _{t}), }_{0}=_{0}\] (13)The \(}_{t}\) are the iterates of (7) when \(=\). We can then prove the following descent inequality:

\[\|}_{t+1}-^{}\|^{2}\|}_{t}-^{}\|^{2}-(f( _{t})-f^{})+2L\|}_{t}- _{t}\|^{2}.\]

Because of the anti-correlation in (10), the virtual iterates \(}_{t}\) are close to the real iterates \(_{t}\), as \(_{t}-}_{t}=_{t}\). Averaging over \(t\), we recover (12). See Appendix D for details.

Tightness.The noise terms (those terms involving \(^{2}\)) in (11), (12) are both tight. We show this in Appendix E on the objective \(f()=(}{{2}})\|\|^{2}\).

Diffeulties in a unified analysis.The proof techniques for PGD and Anti-PGD above are notably different, and as we explain in Appendix F, do not lead to favorable results when trying to use one of the two strategies to analyze both.

### Main Results and Analytic Techniques

To unify the proof techniques above, we use a modified virtual sequence with _restart iterations_. For a parameter \(=(}{{L}})\) (throughout, \(}\) and \(\) hide poly-logarithmic factors), we define

\[}_{t+1}&=}_{t}- f(_{t}) 56.905512pt\;\;t+1  0\\ }_{t+1}&=_{t+1} 56.905512pt \;\;t+1=0.\] (14)

Similar to the virtual sequence in (13), \(}_{t}\) incorporates only deterministic gradients \( f(_{t})\). However, every \(\) iterations we reset \(}_{t}\) to the real iterate \(_{t}\). This allows us to control the divergence between the virtual sequence and the real sequence (enabling a tight analysis of PGD), while still capturing the convergence benefits of anti-correlated noise (enabling a tight analysis of Anti-PGD).

The parameter \(\) is independent of \(\), and depends only on the geometry of \(f\) and the stepsize \(\). Using this machinery, we can prove convergence rates of (7) for _any_ factorization \(=\). These rates involve \(_{2}\) distances between the rows \(_{t}\) of the matrix \(\) (where \(_{0}=\) for convenience).

**Theorem 4.6** (non-convex).: _Suppose Assumptions 4.1 and 4.2 hold, \(}{{4L}}\), and \(=}{{ L}}\). Then (7) produces iterates whose average error \((T+1)^{-1}_{t=0}^{T}\| f(_{t})\|^{2}\) is upper bounded by_

\[_{0})-f^{})}{ T}+}{T}_{t=1}^{T}\|_{t}-_{}\|^{2}+_{ 1 t T\\ t=0}\|_{t}-_{t-}\|^ {2}.\]

**Theorem 4.7** (convex).: _Under Assumptions 4.1, 4.2, and 4.3, if \(}{{4L}}\) and \(=(}{{ L}})\), then (7) produces iterates with average error \((T+1)^{-1}_{t=0}^{T}[f(_{t})-f^{}]\) upper bounded by_

\[}_{0}-^{} \|^{2}}{ T}+}{TL}_{t=1}^{T}\|_{t}-_{ }\|^{2}+_{1 t T\\ t=0}\|_{t}-_{t-}\|^ {2}+\|_{}\|^{2} .\]

We give complete proofs in Appendix C. These convergence rates consist of two terms: The first term states how fast the function would converge in the absence of the noise. The second term, the _noise term_, is the focus of our paper, as it shows how the correlated noise \(\) affects convergence.

These rates involve only differences of rows of \(\) that are at most \(\) iterations apart. Intuitively, \(\) is a coarse indicator of whether an iterate \(_{t}\) is still sensitive to the noise injected at an iteration \(t^{}<t\). If \(t>t^{}+\), then changes in the noise added at step \(t\) are effectively uncorrelated to iteration \(t^{}\). As we detail in Appendix, applying Theorem 4.7 to the special cases in Examples 3.1, 3.2 recovers their tight convergence rates in (11), (12) correspondingly.

## 5 Finding Better Factorizations

We now draw on our results in Section 4 to develop better mechanisms for the MF-DP-FTRL framework. We modify the objective underlying the offline matrix factorization problem during the first stage of the MF-DP-FTRL workflow (Fig. 1). Specifically, observe that the noise term in Theorems 4.6 and 4.7 can be rewritten in matrix notation (up to multiplicative constants) as

\[\|_{}\|_{F}^{2}=_{t=1}^{T}\| _{t}^{}\|^{2}=_{1  t T\\ t=0}\|_{t}-_{t-}\|^{2}+ _{1 t T\\ t 0}\|}(_{t}- _{})\|^{2}\] (15)where \(_{}=[_{1}^{},,_{T}^{} ]^{}^{T T}\), and we set the rows \(_{t}\) appropriately to select corresponding row differences of \(\) with either coefficient \(1\) or \(}{{}}\) depending on the index \(t\). We give a precise definition of \(_{}\) and an explicit example when \(T=12,=3\) in Appendix B.

Recall that  minimize the Frobenius norm objective (6) based on their derived convergence bounds in (5). Since our derived convergence bounds are strictly tighter, we propose using Eq. (15) as the new objective function in (6). Intuitively, since \(\|_{}\|_{F}^{2}\) is a better proxy for learning performance than \(\|\|_{F}^{2}\), minimizing this quantity in the offline factorization problem should lead to ERM methods with better privacy-utility trade-offs.

We can solve our new offline matrix factorization problem in a straightforward manner. We can show that for \(=\), we can solve this modified problem by first computing the solution \(},}\) using \(_{F}(_{})\). The solution to our modified objective is then \(=}\), \(=^{-1}\). This implies we can use existing open-source solvers designed for (6) [51; 30; 10].

## 6 Experiments

In this section, we evaluate the ERM performance of MF-DP-FTRL under different offline factorization objectives. We focus on the Frobenius norm objective (6), which we refer to as DP-MF [10; 7], and our modified objective (15), which we refer to as DP-MF\({}^{+}\).

### Validating Theoretical Results

We first validate our theoretical results above by comparing the convergence of DP-MF and DP-MF\({}^{+}\) on a _random quadratic_ function that satisfies the assumptions of Theorem 4.7. Notably, we ensure the quadratic is not strongly convex. We treat \(\) in (15) as a hyperparameter and tune it over a fixed grid. For complete details, please refer to Appendix H. We present the results in Fig. 2.

In Fig. 2(a) we plot \(_{t=0}^{T}\| f(_{t})\|^{2}\), as this quantity is proportional to the LHS of Theorem 4.7. For all learning rates, DP-MF\({}^{+}\) either matches or outperforms DP-MF. Moreover, the advantage of DP-MF\({}^{+}\) increases as the learning rate increases. This corresponds to our theory in Theorem 4.7. Indeed, the larger the stepsize \(\), the smaller the optimal \(\) (as \(=(}{{_{L}}})\)), and the more often restarts are used in the analysis of Theorem 4.7.

Fig. 2(b) further depicts the last-iterate behaviours of DP-MF and DP-MF\({}^{+}\), which is often more practically relevant. Interestingly, the last iterate behaviour is improved even in the cases where the average behaviour does not improve. Finally, in Fig. 2(c) we pick \(=10^{-2},=50\) as the parameters for which both the average and the last-iterate behaviours are improved and plot the convergence curve over iterations. DP-MF\({}^{+}\) has regular oscillating behaviour, allowing it to achieve a good final-iterate performance. The period of these oscillations is exactly equal to \(\).

### Practical DP Training Experiments

We now compare DP-MF, DP-MF\({}^{+}\), and DP-SGD with privacy amplification  on the MNIST, CIFAR-10, and Stack Overflow datasets. We omit from comparison DP-FTRL  and DP-Fourier

Figure 2: Comparison of the average and last gradient norms for DP-MF and DP-MF\({}^{+}\) on a random non-strongly convex quadratic function with \(L=10\).

 as these methods are strictly dominated by DP-MF. Unlike our theoretical analysis, we include clipping to derive formal \((,)\) privacy guarantees. To facilitate a fair comparison, we set \(=10^{-6}\) in all the settings, and compare against varying \(\). We give complete experimental details in Appendix H

MNIST, logistic regression.We train for \(T=2048\) iterations and either \(1\) or \(16\) epochs depending on the batch size, corresponding to a batch size of \(29\) and \(469\) respectively.2 We fix the clipping threshold at \(1.0\) and the learning rate at \(0.5\). We vary \(\) in (15) over \(\{2,8,32,128,512,2048\}\). The results are in Figs. 3(a) and 3(b). DP-MF\({}^{+}\) improves monotonically with \(\), performing best when \(=2048=T\). For such \(\), DP-MF\({}^{+}\) consistently out-performs DP-MF across all settings. Recall from (15) that this corresponds to the offline objective \(\|_{T}\|_{F}^{2}\) where \(_{ii}=}{{}}\) for all \(i<T\) and \(_{TT}=1\). This objective strongly penalizes errors on the final iterate, which is the model used to compute test accuracy.

We also see that DP-MF\({}^{+}\) expands the number of settings in which we can beat DP-SGD. DP-MF only outperforms DP-SGD for sufficiently large \(\) (\( 0.31\) for \(1\) epoch and \( 31\) for \(16\) epochs). By contrast, DP-MF\({}^{+}\) outperforms DP-SGD in every setting except when \(=0.01\) and \(1\) epoch. None of the mechanisms reached the accuracy levels obtained by the non-private baseline, even at \(=100\). We suspect this is due to the fact that we are using a fixed but aggressive clipping threshold of \(1.0\) across all experiments, which helps in the moderate privacy regime but hurts in very low privacy regime. Even though DP-MF\({}^{+}\) does not use privacy amplification, it outperforms DP-SGD, which uses privacy amplification. This is due to the efficient noise anti-correlations in DP-MF\({}^{+}\). If amplification were not possible, performance of DP-SGD would degrade even further.

Cifar-10, Cnn.We follow the experimental setup from . Specifically, we train all mechanisms for \(20\) epochs and \(T=2000\) iterations, which corresponds to a batch size of \(500\).3 We tune the learning rate over a fixed grid. We fix \(=T=2000\) in DP-MF\({}^{+}\) as we found that worked best in the MNIST experiments. The results are given in Fig. 3(c). We see that DP-MF\({}^{+}(=2000)\) offers a consistent improvement over DP-MF across all choices of \(\) considered. Both DP-MF and DP-MF\({}^{+}\) beat DP-SGD for \(>4\). This observation is consistent with prior work on DP-FTRL and DP-MF, where DP-SGD performs relatively better with smaller \(\) while DP-MF performs better with larger \(\).

Stack Overflow, LSTM.In Appendix H, we compare DP-MF and DP-MF\({}^{+}\) on a federated learning task with _user-level_ differential privacy. We do not compare DP-SGD on this task, as amplification techniques such as shuffling and subsampling are not possible in practical federated learning settings . In this task, we train an LSTM network to do next-word prediction on the Stack Overflow dataset. To be consistent with the prior work  and to test if our proposed factorizations are compatible with the other types of workloads \(\) from Eq. (2), we use momentum and learning rate decay. Our results are given in Table 2. We see that two methods perform comparably, verifying competitiveness of our method. Note that this task uses federated averaging  instead of gradient

Figure 3: Test set accuracy of various mechanisms on the MNIST and CIFAR-10 datasets.

descent. Developing offline factorization objectives specifically for federated learning remains an open problem.

## 7 Conclusion

In this work, we developed analytic techniques to study the convergence of gradient descent under linearly correlated noise that is motivated from a class of DP mechanisms. We derived tighter bounds than currently exist in the literature, and we use our novel theoretical understanding to design privacy mechanisms with improved convergence. Perhaps more importantly, our work highlights the wealth of stochastic optimization questions arising from recent advances in differentially private model training. As such, we distill and formalize various optimization problems arising from recent work on matrix mechanisms for DP. Our work raises a host of questions and open problems, including extending our analysis to include things such as clipping, shuffling, and momentum. Another key extension is to derive last-iterate convergence rates rather than average-iterate convergence rates, as in some settings it is only the final "released" model that needs formal privacy guarantees. Given the improved generalization properties of Anti-PGD , one could also investigate how to design more general linearly correlated noise mechanisms which improve both privacy and generalization.

## 8 Acknowledgments

The authors would like to thank Francesco D'Angelo, Nina Mainusch and Linara Adylova for their comments on the manuscript. The authors would also like to thank the reviewers for their helpful suggestions in improving the clarity of the writing.