# Global Update Tracking: A Decentralized Learning Algorithm for Heterogeneous Data

Sai Aparna Aketi Abolfazl Hashemi  Kaushik Roy

Department of Electrical and Computer Engineering

Purdue University

West Lafayette, IN 47906

{saketi, abolfazl, kaushik}@purdue.edu

###### Abstract

Decentralized learning enables training of deep learning models over large distributed datasets generated at different locations, without the need for a central server. However, in practical scenarios, the data distribution across these devices can be significantly different, leading to a degradation in model performance. In this paper, we focus on designing a decentralized learning algorithm that is less susceptible to variations in data distribution across devices. We propose Global Update Tracking (GUT), a novel tracking-based method that aims to mitigate the impact of heterogeneous data in decentralized learning without introducing any communication overhead. We demonstrate the effectiveness of the proposed technique through an exhaustive set of experiments on various Computer Vision datasets (CIFAR-10, CIFAR-100, Fashion MNIST, and ImageNet), model architectures, and network topologies. Our experiments show that the proposed method achieves state-of-the-art performance for decentralized learning on heterogeneous data via a \(1-6\%\) improvement in test accuracy compared to other existing techniques.

## 1 Introduction

Decentralized learning is a branch of distributed optimization which focuses on learning from data distributed across multiple agents without a central server. It offers many advantages over the traditional centralized approach in core aspects such as data privacy, fault tolerance, and scalability . It has been demonstrated that decentralized learning algorithms  can perform comparable to centralized algorithms on benchmark vision datasets. Decentralized Parallel Stochastic Gradient Descent (DSGD) presented in  combines SGD with a gossip averaging algorithm . Further, the authors analytically show that the convergence rate of DSGD is similar to its centralized counterpart . A momentum version of DSGD referred to as Decentralized Momentum Stochastic Gradient Descent (DSGDm) was proposed in . The authors in  introduce Stochastic Gradient Push (SGP) which extends DSGD to directed and time-varying graphs. Recently, a unified framework for analyzing gossip-based decentralized SGD methods and the best-known convergence guarantees was presented in .

One of the key assumptions to achieve state-of-the-art performance by all the above-mentioned decentralized algorithms is that the data is independently and identically distributed (IID) across the agents. In particular, the data is assumed to be distributed in a uniform and random manner across the agents. This assumption does not hold in most real-world settings where the data distributions across the agents are significantly different (non-IID/heterogeneous) . The effect of heterogeneous data in a peer-to-peer decentralized setup is a relatively under-studied problem and an active area of research.

Recently, there have been few efforts to bridge the performance gap between IID and non-IID data for a decentralized setup [16; 23; 19; 7; 1; 24]. Cross Gradient Aggregation  and NeighborhoodGradient Clustering  algorithms utilize the concept of cross-gradients to reduce the impact of heterogeneous data and show significant improvement in performance (test accuracy). However, these techniques incur \(2\) communication cost than the standard decentralized algorithms such as DSGD. \(D^{2}\) algorithm proposed in  is shown to be agnostic to data heterogeneity and can be employed in deep learning tasks. One of the major limitations of \(D^{2}\) is that its convergence requires mixing topologies with negative eigenvalue bounded from below by \(-\). Additionally, it has been shown that \(D^{2}\) performs worse than DSGD in some cases .

Tracking mechanisms such as Gradient Tracking (GT) [6; 19] and Momentum Tracking (MT)  have been proposed to tackle heterogeneous data in decentralized settings. But these algorithms improve the performance at the cost of \(2\) communication overhead. The authors in  introduce Quasi-Global Momentum (QGM), a decentralized learning method that mimics the global synchronization of momentum buffer to mitigate the difficulties of decentralized learning on heterogeneous data. Recently, RelaySGD was presented in  that replaces the gossip averaging step with RelaySum. Since RelaySGD deals with the gossip averaging step, it is orthogonal to the aforementioned algorithms and can be used in synergy with them. QG-DSGDm  which incorporates QGM into DSGDm sets the current state-of-the-art for decentralized learning on heterogeneous data without increasing the communication cost. This work investigates the following question: _Can we improve decentralized learning on heterogeneous data through a tracking mechanism without any communication overhead?_

To that effect, we present _Global Update Tracking (GUT)_, a novel decentralized learning algorithm designed to improve performance under heterogeneous data distribution. Motivated by, yet distinct from, the gradient tracking mechanism, we propose to track the consensus model (\(^{t}\)) by tracking global/average model updates, where \(x^{t}_{i}\) is the model parameters on agent \(i\) at time step \(t\) and \(\) is the averaged model parameters. In the traditional tracking-based methods [19; 22] that track average gradients, each agent communicates both model parameters \(x^{t}_{i}\) and the tracking variable \(y^{t}_{i}\) with its neighbors resulting in \(2\) communication overhead. The proposed _GUT_ algorithm overcomes this bottleneck by allowing agents to store a copy of their neighbors' model parameters and then tracking the model updates instead of the gradients. This results in communicating only the tracking variable \(y^{t}_{i}\) that yields the model update (\(x^{t}_{i}-x^{t-1}_{i}\)). We demonstrate the effectiveness of the proposed algorithm through an exhaustive set of experiments on various datasets, model architectures, and graph topologies. We also provide a detailed convergence analysis showing that the convergence rate of _GUT_ algorithm is consistent with the state-of-the-art decentralized learning algorithms. Further, we show that _QG-GUTm_ - Global Update Tracking with Quasi-Global momentum beats the current state-of-the-art decentralized learning algorithm (i.e., QG-DSGDm) on heterogeneous data under iso-communication cost.

### Contributions

In summary, we make the following contributions.

* We propose _Global Update Tracking (GUT)_, a novel tracking-based decentralized learning algorithm to mitigate the impact of heterogeneous data distribution.
* We theoretically establish the non-asymptotic convergence rate of the proposed algorithm to a first-order solution.
* Through an exhaustive set of experiments on various datasets, model architectures, and graph topologies, we establish that the proposed Global Update Tracking with Quasi-Global momentum (_QG-GUTm_) outperforms the current state-of-the-art decentralized learning algorithm on a spectrum of heterogeneous data.

## 2 Background

In this section, we provide the background on the decentralized setup with peer-to-peer connections.

The main goal of decentralized machine learning is to learn a global model using the knowledge extracted from the locally stored data samples across \(n\) agents while maintaining privacy constraints. In particular, we solve the optimization problem of minimizing the global loss function \(f(x)\) distributed across \(n\) agents as given in (1). Note that \(F_{i}\) is a local loss function (for example, cross-entropy loss)defined in terms of the data (\(d_{i}\)) sampled from the local dataset \(D_{i}\) at agent \(i\).

\[_{x^{d}}f(x)&=_{i=1}^{n}f_{i}(x),\\ \ \ f_{i}(x)&=_{d_{i} D_{i}}[F_ {i}(x;d_{i})],\ \ \ i.\] (1)

The optimization problem is typically solved by combining stochastic gradient descent  with global consensus-based gossip averaging . The communication topology is modeled as a graph \(G=([N],E)\) with edges \(\{i,j\} E\) if and only if agents \(i\) and \(j\) are connected by a communication link exchanging the messages directly. We represent \((i)\) as the neighbors of agent \(i\) including itself. It is assumed that the graph \(G\) is strongly connected with self-loops i.e., there is a path from every agent to every other agent. The adjacency matrix of the graph \(G\) is referred to as a mixing matrix \(W\) where \(w_{ij}\) is the weight associated with the edge \(\{i,j\}\). Note that, weight \(0\) indicates the absence of a direct edge between the agents, and the elements of the Identity matrix are represented by \(I_{ij}\). Similar to the majority of previous works in decentralized learning, the mixing matrix is assumed to be doubly stochastic. Further, the initial models and all the hyperparameters are synchronized at the beginning of the training. The communication among the agents is assumed to be synchronous.

Traditional decentralized algorithms such as DSGD  assume the data across the agents to be Independent and Identically Distributed (IID). In DSGD, each agent \(i\) maintains local parameters \(x_{i}^{t}^{d}\) and updates them as follows.

\[\ \ x_{i}^{t+1}=_{j(i)}w_{ij}(x_{j}^{t}- g _{j}^{t});\ \ \ \ g_{i}^{t}= F_{j}(x_{i}^{t},d_{i}^{t}).\] (2)

We focus on a decentralized setup with non-IID/heterogeneous data. In particular, the heterogeneity in the data distribution comes in the form of skewed label partition similar to . Decentralized learning with the DSGD algorithm on heterogeneous data distribution results in performance degradation due to huge variations in the local gradients across the agents. To tackle this, authors in  propose a momentum-based optimization technique (QG-DSGDm) introducing Quasi-Global momentum as shown in (3).

\[\ \ x_{i}^{t+1}=_{j (i)}w_{ij}[x_{j}^{t}-(g_{j}^{t}+ m_{j}^{t-1})];\ \ m_{i}^{t}= m_{i}^{t-1}+(1-)^{t-1}-x_{i}^{t}}{}.\] (3)

QG-DSGDm improves the performance of decentralized learning on heterogeneous data without any communication overhead and is used as a baseline for comparison in this work.

Gradient Tracking (GT) mechanisms [19; 22] are also known to improve decentralized learning on heterogeneous data by reducing the variance between the local gradient and the averaged (global) gradient. To achieve this, the gradient tracking algorithm introduces a tracking variable \(y_{i}^{t}\) that approximates the total gradient and is used to update the local parameters \(x_{i}^{t}\) (refer to (4)).

\[\ \ x_{i}^{t+1}=_{j(i)}w_{ ij}(x_{j}^{t}- y_{j}^{t});\ \ \ \ y_{i}^{t}=_{j(i)}w_{ij}y_{j}^{t-1}-g_{i}^{t-1}+g_{i}^{t}. \] (4)

The update rule of tracking variable is such that it recursively adds a correction term \(_{j(i)}w_{ij}y_{j}^{t-1}-g_{i}^{t-1}\) to the local gradient \(g_{i}^{t}\), pushing \(y_{i}^{t}\) to be closer to the global gradients (\(_{j=1}^{n}g_{j}^{t}\)). This requires each agent \(i\) to communicate two sets of parameters \(x_{i}^{t}\) and \(y_{i}^{t}\) with its neighbors. Thus, the gradient tracking algorithm improves the decentralized learning on non-IID data at the cost of \(2\) communication overhead.

## 3 Global Update Tracking

We present _Global Update Tracking (GUT)_, a novel algorithm for decentralized deep learning on non-IID data distribution. _GUT_ is a communication-free tracking mechanism that aims to mitigate the difficulties of decentralized training when the data distributed across the agents is heterogeneous.

In order to attain the benefits of gradient tracking without communication overhead, we propose to apply the tracking mechanism with respect to the model updates \(x_{i}^{t}-x_{i}^{t-1}\) instead of the gradients\(g_{i}^{t}\). Firstly, to design a tracking mechanism without additional communication cost, each agent \(i\) communicates model updates instead of model parameters to its neighbors. An agent \(i\) stores a copy of its neighbor's parameters as \(_{j}\) and updates it using the received model updates to retrieve the current version of the neighbor's parameters as shown in line-9 of Algorithm 1. A memory-efficient implementation of the algorithm (Algorithm 4 in Appendix B) requires each agent to store \(s_{i}=_{j(i)}w_{ij}_{j}\) instead of storing each neighbor's copy separately requiring only \((1)\) additional memory .

Now, we define a variable \(_{i}^{t}\) on each agent \(i\) that accumulates the local gradient update \(g_{i}^{t}\) and the gossip averaging update \(_{j}(w_{ij}-I_{ij})_{j}^{t}\) as shown in line-5 of Algorithm 1. Note that we can recover the DSGD update defined in (2) by using \(_{i}^{t}\) in the update rule i.e., \(x_{i}^{t+1}=x_{i}^{t}-_{i}^{t}\). We then proceed to compute the tracking variable \(y_{i}^{t}\), as described in line-6 of Algorithm 1, using the combined model update (local gradient part and gossip averaging part) reflected by \(_{i}^{t}\). The gossip averaging part of the update for each agent \(i\) i.e., \(_{j}w_{ij}(_{j}^{t}-x_{i}^{t})\) is computed with respect to its own model weights. To account for this in the computation of tracking variable \(y_{i}^{t}\), the agents have to adjust the information received from the neighbors (i.e., \(y_{j}^{t}\)'s) to change the reference to itself. This is reflected as an additional term \((_{j}^{t}-x_{i}^{t})\) in the update rule given by line-6 of Algorithm 1. Further, we scale the correction term of the tracking variable by a factor \(\), a hyper-parameter, which is tuned to extract the maximum benefits of the proposed algorithm.

In summary, the update scheme of _GUT_ can be re-formulated in the following matrix form where \(X=[x_{1},,x_{n}]^{d n}\) are the model parameters and \(G=[g_{1},,g_{n}]^{d n}\) are stochastic gradients.

\[ X^{t+1}&=X^{t}- Y^{t},\\ Y^{t+1}&=G^{t+1}-(W-I)X^{t+1}+[WY^{t }-G^{t}-(W-I)(X^{t+1}-X^{t})].\] (5)

Finally, we show that integrating the proposed _GUT_ algorithm with Quasi-Global Momentum improves the current state-of-the-art significantly without any communication overhead. The pseudo-code for the momentum version of our algorithm (_QG-GUTm_) is presented in Appendix B.

## 4 Convergence Guarantees

This section provides the convergence analysis for the proposed _GUT_ Algorithm. We assume that the following standard assumptions hold:

**Assumption 1** (Lipschitz Gradients).: _Each function \(f_{i}(x)\) is L-smooth i.e., \(|| f_{i}(y)- f_{i}(x)|| L||y-x||\)._

**Assumption 2** (Bounded Variance).: _The stochastic gradients are unbiased and their variance is assumed to be bounded._

\[_{d D_{i}}|| F_{i}(x;d)- f_{i}(x)||^{2}^{2 }\ \  i[1,n],\] (6)

\[_{i=1}^{n}|| f_{i}(x)- f(x)||^{2}^{2}.\] (7)

**Assumption 3** (Doubly Stochastic Mixing Matrix).: _The mixing matrix \(W\) is a real doubly stochastic matrix with \(_{1}(W)=1\) and_

\[\{|_{2}(W)|,|_{n}(W)|\} 1-<1,\] (8)

_where \(_{i}(W)\) is the \(i^{th}\) largest eigenvalue of W and \(\) is the spectral gap._

The above assumptions are commonly used in most decentralized learning setups. Theorem 1 presents the convergence of the proposed _GUT_ algorithm and the proof is detailed in Appendix A.

**Theorem 1**.: _(Convergence of GUT algorithm) Given Assumptions 1, 2, and 3 let step size \(\) and the scaling factor \(\). For all \(T 1\), we have_

\[_{t=0}^{T-1}|| f(^{t})||^{2}(f(^{0})-f^{*})+}{n}+^{2}}{^{2}}(^{2}+^{2}(2-)),\] (9)

_where \(f(^{0})-f^{*}\) is the sub-optimality gap, \(\) is the average/consensus model parameters._

The result of the Theorem 1 shows that the averaged gradient of the averaged model is upper-bounded by the sub-optimality gap (the difference between the initial objective function value and the optimal value), the sampling variance (\(\)), and gradient variations across the agents representing data heterogeneity (\(\)). Further, we present a corollary to show the convergence rate of _GUT_ in terms of the number of iterations.

**Corollary 1**.: _Suppose that the step size satisfies \(=}\) For a sufficiently large \(T\) we have,_

\[_{t=0}^{T-1}|| f(^{t})||^{2}}+.\] (10)

Corollary 1 indicates that the _GUT_ algorithm achieves linear speedup with a convergence rate of \((})\) when \(T\) is sufficiently large and is independent of communication topology. In other words, the communication complexity to find an \(\)-first order solution, i.e., \(\| f()\|^{2}\) is \((}{ne^{2}})\). This convergence rate is similar to the well-known best result for decentralized SGD algorithms  in the literature.

## 5 Experiments

In this section, we analyze the performance of the proposed _GUT_ and _QG-GUTm_ techniques and compare them with the baseline DSGD algorithm  and the current state-of-the-art QG-DSGDm  respectively. The source code is available at https://github.com/aparna-aketli/global_update_tracking

### Experimental Setup

The efficiency of the proposed method is demonstrated through our experiments on a diverse set of datasets, model architectures, graph topologies, and graph sizes. We present the analysis on - (a) Datasets: CIFAR-10, CIFAR-100, Fashion MNIST, and Imagenette. (b) Model architectures: VGG-11, ResNet-20, LeNet-5 and, MobileNet-V2. All the models use Evonorm  as the activation-normalization layer as it is shown to be better suited for decentralized learning on heterogeneous data.

(c) Graph topologies: Ring graph with 2 peers per agent, Dyck graph with 3 peers per agent, and Torus graph with 4 peers per agent (refer Figure 1). (d) Number of agents: 16-40 agents. We use the Dirichlet distribution to generate disjoint non-IID data across the agents. The created data partition across the agents is fixed, non-overlapping, and never shuffled across agents during the training. The degree of heterogeneity is regulated by the value of \(\) - the smaller the \(\) the larger the non-IIDness across the agents. We report the test accuracy of the consensus model averaged over three randomly chosen seeds. The details of the decentralized setup and hyperparameters for all the experiments are presented in Appendix C.

### Average Consensus Task

We first consider an average consensus task that is isolated from the learning through stochastic gradient descent. Here the aim is that all the agents should reach a consensus which is the average value of the initial information each agent holds. The following equations show the simplified version of _GUT_ (11) and _QG-GUTm_ (12) after removing the gradient update part.

\[X^{t+1}=X^{t}+Y^{t};\ \ Y^{t}=(W-I)X^{t}+[WY^{t-1}-(W-I)(X^{t-1}-X^{t})],\] (11)

\[X^{t+1}=X^{t}+^{t};\ \ \ M^{t}= M^{t-1}+(1-)(X^{t}-X^{t-1})\] (12)

\[^{t}=  M^{t}+(1-)[(W-I)X^{t}+(W^{t-1}-(W-I)(X^{t-1}-X ^{t}))].\]

Note that setting the hyper-parameter \(\) as \(0\) in the (11), 12 gives simple gossip and quasi-global gossip  respectively and all the agents communicate \(X^{t}-X^{t-1}\) at iteration \(t\) with their neighbors.

Figure.2 shows the average consensus error i.e., \(||X^{t}-||_{F}^{2}\) over time for the average consensus task on the Ring topology with respect to various algorithms. We observe that the gossip averaging with _GUT_ converges faster than simple gossip averaging. Figure.2(c) illustrates that for graphs with a smaller spectral gap (which corresponds to more agents), the proposed _QG-GUTm_ can converge faster than quasi-global gossip (gossip with QGM) resulting in better decentralized optimization.

### Decentralized Deep Learning Results

We evaluate the efficiency of _GUT_ and its quasi-global momentum version _QG-GUTm_ with the help of an exhaustive set of experiments. We compare _GUT_ with DSGD and the momentum version _QG-GUTm_ with QG-DSGDm to show that the proposed method outperforms the current state-of-the-art.

Figure 1: Ring Graph (left), Dyck Graph (center), and Torus Graph (right).

Figure 2: Decentralized average consensus problem on an undirected ring topology

Table. 1 shows the average test accuracy for training ResNet-20 and VGG-11 models on the CIFAR-10 dataset with varying degrees of non-IIDness over ring topology of 16 and 32 agents. We observe that _GUT_ consistently outperforms DSGD for all models, graph sizes, and degree of heterogeneity with a significant performance gain varying from \(1-18\%\). The quasi-global momentum version of our algorithm, _QG-GUTm_, beats QG-DSGDm with \(1-3.5\%\) improvement in the case of the CIFAR-10 dataset partitioned with a higher degree of heterogeneity (\(=0.1,0.01\)).

We present the experimental results on various graph topologies and datasets to demonstrate the scalability and generalizability of _QG-GUTm_. We train the CIFAR-10 dataset on ResNet-20 over the Dyck graph and Torus graph to exemplify the impact of connectivity on the proposed technique. As shown in Table. 2, we obtain \(0.5-3.5\%\) performance gains with varying connectivity (or spectral gap).

   &  &  \\   & & \(=1\) & \(=0.1\) & \(=0.01\) \\   & DSGDm (IID) &  & \(89.75 0.29\) &  \\  & DSGD  & \(84.17 0.32\) & \(72.21 2.37\) & \(54.66 4.74\) \\  & _GUT (ours)_ & \(84.72 0.20\) & \(81.86 1.99\) & \(70.16 4.94\) \\  & _QG-_GSDGDm  & \(\) & \(\) & \(79.85 2.11\) \\  & _QG-GUTm (ours)_ & \(88.22 0.36\) & \(\) & \(\) \\   & DSGDm (IID) &  & \(88.52 0.23\) &  \\  & DSGD  & \(78.25 0.42\) & \(62.97 1.90\) & \(42.58 1.84\) \\  & _GUT (ours)_ & \(79.24 0.33\) & \(76.07 0.23\) & \(60.72 1.03\) \\  & QG-DSGDm  & \(87.15 0.33\) & \(83.50 1.04\) & \(69.99 0.60\) \\  & _QG-GUTm (ours)_ & \(\) & \(\) & \(\) \\    &  &  \\   & DSGDm (IID) &  & \(=0.1\) & \(=0.01\) \\   & DSGD  & \(81.78 0.29\) & \(76.20 0.81\) & \(68.93 1.23\) \\   & _GUT (ours)_ & \(82.12 0.09\) & \(81.24 0.95\) & \(76.62 1.37\) \\   & QG-DSGDm  & \(84.23 0.47\) & \(81.70 0.79\) & \(77.08 3.19\) \\   & _QG-GUTm (ours)_ & \(\) & \(\) & \(\) \\   & DSGDm (IID) &  & \(84.75 0.30\) &  \\  & DSGD  & \(79.75 0.56\) & \(73.37 1.02\) & \(59.93 1.60\) \\    & _GUT (ours)_ & \(80.37 0.33\) & \(79.55 1.00\) & \(73.59 1.26\) \\    & QG-DSGDm  & \(83.67 0.28\) & \(80.82 0.19\) & \(74.25 2.02\) \\    & _QG-GUTm (ours)_ & \(\) & \(\) & \(\) \\  

Table 1: Test accuracy of different decentralized algorithms evaluated on CIFAR-10, distributed with different degrees of heterogeneity (non-IID) for various models over ring topologies. The results are averaged over three seeds where the standard deviation is indicated. We also include the results of the IID baseline as DSGDm (IID) where the local data is randomly partitioned independent of \(\).

   &  &  &  \\   & \(=0.1\) & \(=0.01\) & \(=0.01\) & \(=0.01\) & \(=0.01\) \\  DSGDm  & \(86.59 0.92\) & \(77.00 0.353\) & \(47.93 1.69\) & \(42.56 2.71\) & \(66.02 4.59\) & \(38.69 11.8\) \\ QG-DSGDm  & \(89.94 0.44\) & \(83.43 0.94\) & \(53.19 1.68\) & \(44.17 3.64\) & \(63.60 4.50\) & \(39.49 4.57\) \\ _QG-GUTm_ & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  

Table 3: Average test accuracy of different decentralized algorithms evaluated on various datasets, distributed with different degrees of heterogeneity over 16 agents ring topology

   &  &  &  \\   & \(=0.1\) & \(=0.01\) & \(=0.01\) & \(=0.01\) & \(=0.01\) \\  DSGDm  & \(86.59 0.92\) & \(77.00 0.353\) & \(47.93 1.69\) & \(42.56 2.71\) & \(66.02 4.59\) & \(38.69 11.8\) \\ QG-DSGDm  & \(89.94 0.44\) & \(83.43 0.94\) & \(53.19 1.68\) & \(44.17 3.64\) & \(63.60 4.50\) & \(39.49 4.57\) \\ _QG-GUTm_ & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  

Table 1: Test accuracy of different decentralized algorithms evaluated on CIFAR-10, distributed with different degrees of heterogeneity (non-IID) for various models over ring topologies. The results are averaged over three seeds where the standard deviation is indicatedFurther, we evaluate _QG-GUTm_ on various image datasets such as Fashion MNIST, and Imagenette and on challenging datasets such as CIFAR-100. Table. 3 shows that _QG-GUTm_ outperforms QG-DSGDm by \(0.2-6.2\%\) across various datasets. Therefore, in a decentralized deep learning setup, the proposed _GUT_ and _QG-GUTm_ algorithms are more robust to heterogeneity in the data distribution and can outperform all the comparison methods with an average improvement of \(2\%\).

### Ablation Study

First, we analyze different ways of utilizing or tracking model update information as shown in Table. 4. We present two different update rules apart from _GUT_ and _DSGD_ and also compare them with gradient tracking . Rule-a applies the proposed tracking mechanism on model updates but does not change the reference of tracking variable \(y_{j}\) received from the neighbors to itself (refer sec. 3 for details on changing the reference). In the case of Rule-b, each agent computes the difference between the averaged neighborhood model update (\(W(X^{t}-X^{t-1})\)) along with its own model update (\(X^{t}-X^{t-1}\)) and adds the difference between the two as a bias correction. Table. 4 shows that such naive ways of tracking or bias correction update rules (rule-a,b) do not improve the performance of decentralized learning on heterogeneous data. This confirms our findings that the _GUT_ technique is an effective and provable way to track the consensus model and can outperform the gradient tracking mechanism without any communication overhead.

We then proceed to investigate the effect of different variants of momentum with _GUT_. From Table. 5 (refer to Appendix D for more results), we can conclude that the quasi-global variant of Global Update Tracking always surpasses the other methods. This indicates that the proposed _GUT_ algorithm accelerates decentralized optimization and can be used in synergy with quasi-global momentum to achieve maximal performance gains.

Furthermore, Figure 3(a) illustrates the effect of scaling \(\) on the test accuracy with _QG-GUTm_ and note that \(=0\) shows the test accuracy for QG-DSGDm. Figure 3(b), 3(c) showcase the scalability

   & Update & Communication & Test \\  & Rule & Parameters (Cost) & accuracy \\   & \(X^{t+1}=X^{t-}y^{t}\) & \) (\(1\))} &  \\  & \(Y^{t}=G^{t-1}(W-I)X^{t}\) & & \\   & \(X^{t+1}=X^{t-}y^{t}\) & \) (\(1\))} &  \\  & \(Y^{t}=G^{t-1}(W-I)X^{t}+(WY^{t-1}-(G^{t-1}-(W-I)X^{t-1})]\) & & \\   & \(X^{t+1}=X^{t-}y^{t}\) & \) (\(1\))} &  \\  & \(Y^{t}=G^{t-1}(W-I)X^{t}+(-(W-I)(X^{t}-X^{t-1})]\) & & \\   & \(X^{t+1}=X^{t-}y^{t}\) & \) (\(1\))} & } \\  & \(Y^{t}=G^{t-1}(W-I)X^{t}+(WY^{t-1}-G^{t-1}-(W-I)(X^{t}-X ^{t-1})]\) & & \\  Gradient & \(X^{t+1}=X^{t-}y^{t}[Y^{t-1}-(W-I)X^{t}]\) & & \\ Tracking & \(Y^{t}=G^{t+WY^{t-1}-G^{t-1}}\) & & \\  

Table 4: Analyzing the different variations of model updates. Evaluating test accuracy on CIFAR-10 dataset trained on ResNet-20 over a 16 agent ring topology with \(=0.1\)

   & Local &  & Quasi-Global & Global Update & Test Accuracy \\  & Momentum & & Momentum & Tracking & \(=0.1\) \\  DSGD & x & x & x & x & \(72.21 2.37\) \\ DSGDm & ✓ & x & x & \(79.87 1.73\) \\ DSGDm-N & ✓ & ✓ & x & \(81.31 0.51\) \\  QG-DSGDm & x & x & ✓ & x & \(84.21 2.12\) \\ QG-DSGDm-N & x & ✓ & ✓ & x & \(85.12 1.11\) \\  GUT & x & x & x & ✓ & \(81.86 1.99\) \\ GUTm & ✓ & x & ✓ & \(79.95 1.67\) \\ GUTm-N & ✓ & ✓ & x & ✓ & \(82.08 1.74\) \\ QG-GUTm & x & x & ✓ & ✓ & \(86.44 0.36\) \\ QG-GUTm-N & x & ✓ & ✓ & ✓ & \(\) \\  

Table 5: Evaluating Global Update Tracking (GUT) with various versions of momentum using CIFAR-10 dataset trained on ResNet-20 architecture over 16 agents ring topology of _QG-GUTm_ on different graph sizes and model sizes. _QG-GUTm_ outperforms QG-DSGDm by \( 1.7\%\) over different graph sizes and \( 1.4\%\) over different model sizes.

## 6 Discussion and Limitations

We demonstrated the superiority of the Global Update Tracking (_GUT_) algorithm through an elaborate set of experiments and ablation studies. In our experiments, we focused on doubly-stochastic and symmetric graph structures. The proposed _GUT_ algorithm can be easily extended to directed and time-varying graphs by combining it with stochastic gradient push (SGP) . Further, the additional terms added by _GUT_ can also be interpreted as a bias correction mechanism where the added bias pushes the local model towards the consensus (averaged) model. The matrix representation of this interpretation of _GUT_ is given by (13). and analyzed in Lemma 1.

\[X^{t+1}=WX^{t}-(G^{t}+ B^{t}); B^{t+1}=-((2W-I)(X^{t+ 1}-X^{t})+ G^{t}].\] (13)

**Lemma 1**.: _Given assumptions 3, we define \(^{t}=B^{t}^{T}\), where \(\) is a vector of all ones. For all \(t\), we have: \(^{t}=^{t-1}\)._

A complete proof for Lemma 1 can be found in Appendix A.2. Lemma 1 highlights that the average bias added by _GUT_ is zero as \(^{0}\) is zero. Hence, the _GUT_ algorithm crucially preserves the average value of the decentralized system. A feature we leverage to establish Theorem 1.

There are two potential limitations of the _GUT_ algorithm - a) memory overhead and b) introduction of an additional hyper-parameter. _GUT_ requires the agents to keep a copy of the averaged model parameters of their neighbors which adds an extra memory buffer of the size of model parameters. The storage of the tracking variable also adds to the memory overhead, requiring additional memory equivalent to the size of model parameters. We also introduce a new hyper-parameter \(\) which has to be tuned similarly to the learning rate or momentum coefficient tuning. Besides, the theoretical analysis presented for the _GUT_ algorithm does not consider momentum and assumes the communication to be synchronous. We leave the theoretical analysis of _QG-GUTm_ and formulation of the asynchronous version of _GUT_ as a future research direction.

## 7 Conclusion

Decentralized learning on heterogeneous data is the key to launching ML training on edge devices and thereby efficiently leveraging the humongous amounts of user-generated private data. In this paper, we propose _Global Update Tracking_ (_GUT_), a novel decentralized algorithm designed to improve learning over heterogeneous data distributions. The convergence analysis presented in the paper shows that the proposed algorithm matches the best-known rate for decentralized algorithms. Additionally, the paper introduces a quasi-global momentum version of the algorithm, QG-GUTm, to further enhance the performance gains. The empirical evidence from experiments on different model architectures, datasets, and topologies demonstrates the superior performance of both algorithms. In summary, the proposed algorithm and its quasi-global momentum version have the potential to facilitate more scalable and efficient decentralized learning on edge devices.

Figure 3: Ablation study on the hyper-parameter \(\), number of agents \(n\) and model size. The test accuracy is reported for the CIFAR-10 dataset trained on ResNet architecture over ring topology.