# [MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_FAIL:2]

A plausible root of this challenge could be the absence of structurally represented knowledge. Predicated predominantly on associative training, these agents are versed in correlating traces with responses without genuinely internalizing holistic world models. In sharp contrast, humans seamlessly navigate abductive reasoning by forecasting potential trajectories leading to a perceived outcome. This intricate dance gradually transmutes from abductive to deductive reasoning, where humans harness their innate understanding of causality to deduce and mirror observed patterns. In our pursuit to mirror this quintessential human trait, we integrate Abduction from Deduction (AfD) into \(\)**Conan** via a Bayesian approach. Experimental results underscore the efficacy of AfD, indicating a substantial avenue for bolstering agent adeptness in \(\)**Conan**.

To sum up, our work makes the following three contributions:

* We usher in the novel domain of _active reasoning_, underscoring the indispensable roles of active exploration and iterative inference in abductive reasoning. This paradigm shift transforms traditional single-round passive question-answering paradigms into a more immersive format, compelling agents to actively engage with the environment to procure pivotal evidence.
* We introduce \(\)**Conan**, a new environment tailored to evaluate the abductive reasoning ability of current machine learning models within dynamic settings. \(\)**Conan** surpasses its predecessors that hinge on step-by-step deductive reasoning, revealing the limitations of present-day models.
* We formulate a new learning method for abduction, AfD, grounded in Bayesian principles. This framework elegantly reformulates abduction into deduction, proving instrumental in navigating the complex active reasoning challenges posed by \(\)**Conan**.

## 2 Related Work

Machine Abductive ReasoningAbductive reasoning, foundational to human cognition, is crucial for scientific exploration, decision-making, and problem-solving (Perice, 1965; Magnani, 2011). In the Artificial Intelligence (AI) landscape, there is a rich history of efforts to equip machines with this ability, where they use prior knowledge and sparse observations to hypothesize amidst uncertainty (Josephson and Josephson, 1996; Xu et al., 2023). Key developments span logic-based abduction (Kakas et al., 1992; Poole, 1993) and hybrid neural-symbolic methods (Rocktischel and Riedel, 2017; Zhang et al., 2021b; Li et al., 2022, 2023). With computational progress, Large Language Models (LLMs) have effectively addressed several challenges through text generation, exhibiting outstanding performance (Brown et al., 2020; OpenAI, 2023; Thoppilan et al., 2022). Modern research usually frames abductive reasoning within natural language understanding (Bhagavatula et al., 2019) or multimodal vision-language integration (Hessel et al., 2022; Liang et al., 2022). However, there is still a notable gap: many benchmarks lean heavily on deduction, sidelining abduction's interactive essence. Our work addresses this gap, emphasizing the core of _active reasoning_ in abductive contexts.

Embodied Question AnsweringEmbodied question answering enhances traditional Visual Question Answering (VQA) by placing agents in interactive environments (Johnson et al., 2017; Das et al., 2018; Gordon et al., 2018; Yu et al., 2019). In \(\)**Conan**, agents actively explore to gather data, preparing them to solve abductive questions based on partial information. Unlike standard embodied question-answering frameworks (Das et al., 2018; Gordon et al., 2018; Yu et al., 2019), where questions become simple instructions for agents, \(\)**Conan** introduces complexity: (i) its questions, rooted in high-level intent and goals, resist simple decomposition into a series of actions; (ii) agents in \(\)**Conan** act as detectives, constantly hypothesizing from observations and prior knowledge, and iterating their strategies in light of new data. For a comprehensive comparison of \(\)**Conan** with other benchmarks, see Tab. 1.

## 3 The \(\)**Conan Environment

\(\)**Conan** is crafted as an interactive question-answering environment aimed at evaluating a machine's active abductive reasoning capacity, as depicted in Fig. 1. Building on the foundation of the Crafter (Hafner, 2021), \(\)**Conan** evolves into a detective game featuring two agents: the _vandal_ and the _detective_. The gameplay kickstarts with the _vandal_ undertaking a randomly designated task, leaving behind traces for the _detective_ to unravel. Subsequently, given these traces, pertinent queries are generated. Finally, the _detective_ is spawned in the environment, tasked with navigating these traces and actively probing the environment, all to derive answers through abductive reasoning.

### Basic Components

PlaygroundOriginating from the Crafter playground, \(_{}\) operates within a \(64 64\) grid matrix. Agents navigate this space with a localized \(9 9\) grid field of view centered on their current position. Once the _detective_ is created in the environment, all traces left behind by the _vandal_ persist, serving as clues for the _detective_ to unravel. While pivotal studies Johnson et al. (2016); Fan et al. (2022); Cai et al. (2023); Wang et al. (2023) address perception in 3D Minecraft settings using foundational models, our emphasis is on honing active abductive reasoning. To this end, we transition from a 3D visual perception to a 2D plane, ensuring a harmonious blend of reduced visual complexity and retaining rich interactivity Xie et al. (2021).

Items and Actions\(_{}\) offers an extensive assortment of interactive items: food, materials, mobs, and tools, each tied to specific actions, as illustrated in Fig. 2. It furnishes 26 unique actions to foster agent-environment engagement. Certain actions leave traces, and together, the items and their mechanics provide a rich set of affordances for agents in the playground. This knowledge about item operations and traces aids the _detective_ in comprehending the incident scene. Advancing from its predecessor, the original Crafter, \(_{}\) now boasts over 30 achievements, a significant rise of over 50%. It features 32 distinct traces covering all agent actions such as crafting, collecting, defeating, eating, drinking, and incurring injuries. This enhancement enables the design of 60 varied abductive reasoning tasks within the scene. For an in-depth overview of the playground, refer to Appx. A.

VandalEach \(_{}\) map starts with the initialization of a _vandal_. This agent is driven by two primary aims: executing a specific task and preserving its existence within the environment. It is noteworthy that external threats might terminate the _vandal_ prematurely. Traces left in the aftermath of the _vandal_'s activities form the question foundation for the _detective_, with every trace potentially birthing several questions. For a detailed overview, see Sec. 3.2. We model the _vandal_ as optimal: when given a random task and the full map, it strategically delineates a sequence of subgoals based on the task dependency graph, all while ensuring its survival. In scenarios with multiple viable paths to an objective, uniform sampling comes into play. This sampling, supported by a probabilistic parser, presents varied strategies for task completion. Hence, the _detective_ must delve deeper to distinguish the actual sequence of events from possible decoys. The execution of the _vandal_'s individual actions, as per the planned subgoal sequence, is steered by a collection of pre-established policies.

DetectiveAfter generating questions from a given trace, a _detective_ is spawned to answer them. Traces left by the _vandal_ span multiple steps and are only partially observable within the _detective_'s \(9 9\) grid field of view. This requires the _detective_ to actively interact with the environment and gather evidence to answer the questions.

   Benchmark & Format & Multimodal & Interactive & Multi-round & Abductive \\  CLEVR Johnson et al. (2017) & image & ✓ & ✗ & ✗ & ✗ \\ IQA Gordon et al. (2018) & embodied & ✓ & ✓ & ✗ & ✗ \\ EmbodiedQA Das et al. (2018) & embodied & ✓ & ✓ & ✗ & ✗ \\ \(\)Dhagavatula et al. (2019) & language & ✗ & ✗ & ✗ & ✓ \\ VAR H. Liang et al. (2022) & video & ✓ & ✗ & ✗ & ✓ \\ Sherlock Hessel et al. (2022) & image & ✓ & ✗ & ✗ & ✓ \\ \(_{}\) (Ours) & open-world & ✓ & ✓ & ✓ & ✓ \\   

Table 1: **Comparison between \(_{}\) Conan and related visual reasoning benchmarks. \(_{}\) is unique for its active reasoning and interactive multi-round setting on abductive reasoning tasks.**

Figure 2: **Part of the task dependency graph**. Starting from the root note, any path forms a multi-step task for an agent to interact with the environment.

Though both _detective_ and _vandal_ share the same action space, the _detective_ boasts a unique capability. It not only navigates and interacts like the _vandal_, but can also generate its own traces during its investigation. These overlaid traces from the _detective_ enhance the environment's depth and complexity. This setup pushes the agent to actively derive conclusions from its dynamic interactions. Importantly, the _detective_ is invulnerable; its focus lies squarely on problem-solving, eliminating concerns about survival or evasion. This design emphasizes active exploration and reasoning, ensuring \(}\)**Conan**'s primary goal remains addressing complex reasoning tasks and answering visual scene-related questions.

### Questions and Choices

\(}\)**Conan** is designed to assess the abductive reasoning capability of machine models through a diverse set of questions varying in difficulty and abstraction. These questions fall into three primary categories: **Intent** (local intent), **Goal** (global goal), and **Survival** (agent's survival status change). We approach evaluation as a multi-choice question-answering task. Each question offers four choices, with only one being correct. Questions and choices derive from predefined templates, as showcased in Tab. 2. For a more detailed explanation, see Appx. B.1.

**Intent** questions target the _vandal_'s immediate objectives or intentions during its task. To decipher these traces, agents must deduce the _vandal_'s underlying intent or subgoals. Solving these questions necessitates a learning model's comprehension of the local context.

**Goal** questions probe the _vandal_'s overarching objectives, extending beyond immediate intents. They necessitate grasping the wider context of a task or action sequence. Such questions query the _vandal_'s ultimate aims, demanding a learning model to reason within the broader context of the traces.

**Survival** questions address the wider investigative scope, posing added challenges to the _detective_. Centered on the _vandal_'s survival status changes during tasks (_e.g._, collecting food for sustenance), they lead to deviations from the optimal action plan. While not tied to a task's primary objective, these questions require a deeper grasp of the present context, often necessitating reasoning around potential scenarios or alternate results.

Compared with the prevalent VQA setup, wherein questions are based on factual information that is readily obtainable from the input, \(}\)**Conan** questions cannot be deciphered given only the initial information, necessitating further exploration in the scene. Unlike standard embodied question answering, \(}\)**Conan** questions cannot be directly parsed as modular primitives; they demand abductive reasoning, drawing from both new observation and former knowledge to hypothesize, validate, and revise. For benchmarking purposes, \(}\)**Conan** produced a corpus comprising 100,000 questions. These were derived from 10,000 unique scenes, generated via the Crafter's scene generator, with each scene stemming from a task executed by a _vandal_. This resulted in an average generation of 10 questions per scene.

   Type & Questions \\   & What did the _vandal_ make on this table? \\  & A: wood sword; B: wood pickaxe; C: iron sword; D: stone sword; \\  & Why did the _vandal_ cut a tree here? \\  & A: make table; B: make wood sword; C: make finance; D: collect apple; \\   & What was the _vandal_’s primary objective in this scenario? \\  & A: get diamond; B: defeat zombie; C: collect apple; D: make iron sword; \\  & What was the desired outcome of the task performed by the _vandal_? \\  & A: make steak; B: make table; C: defeat skeleton; D: collect lava; \\   & Why did the _vandal_ die in this situation? \\  & A: lack of water; B: lack of food; C: hurt by monster; D: hurt by lava; \\  & What could the _vandal_ have done differently to avoid a negative outcome? \\  & A: avoid monsters; B: get sleep; C: get food; D: get water; \\   

Table 2: **Examples of three categories of questions in \(}\)**Conan created from predefined templates.**

## 4 The Detective Pipeline

Conan casts the abductive reasoning challenge as a detective game, necessitating a _detective_ to efficiently explore and gather information from the environment to deduce plausible explanations (_i.e_., answers) for the given question. This process involves taking into account the temporal dependencies and incompleteness of the traces. To tackle these challenges encountered in Conan, we devise a detective pipeline, as depicted in Fig. 3.

Building on previous work that utilizes hierarchical models for task decomposition (Gordon et al., 2018; Das et al., 2018; Wijmans et al., 2019), our pipeline is structured into two primary phases: an exploration phase for trace collection, followed by an abductive reasoning phase. Initially, interaction with the playground is carried out to collect relevant visual information, which is subsequently leveraged in the reasoning phase to infer answers to the posed questions.

Computationally, our pipeline first employs RL agents as _explorers_ (see Sec. 4.1) that learn an exploration policy based on the traces and the question, thereby rendering it goal-oriented. Next, given the question, we recruit vision-language models (see Sec. 4.3) to predict the answer based on the observation. A _key-frame extractor_ (see Sec. 4.2) is inserted into the two phases to selectively identify relevant frames for abduction. The individual components undergo separate training procedures.

### Explorer for Trace Gathering

The primary responsibility of an explorer is to efficiently collect information pertinent to the provided question. Initially, masks are employed to encode questions by highlighting relevant grids. Subsequently, the explorer takes in both the observation and the target question as input and outputs the action probability.

We use a reward function that incentivizes the agent to scout for clues and traces relevant to the given question. Additionally, a penalty term is incorporated to discourage unnecessary actions and inefficient searching, thereby promoting a more targeted exploration strategy.

Specifically, the agent is rewarded with \(+1\) when a trace first appears within its local view, or \(+2\) when the trace bears a close association with the question. A substantial reward of \(+100\) is conferred upon the agent if it successfully uncovers all traces left by the _vandal_. Concurrently, the agent incurs a penalty of \(-0.1\) for every timestep elapsed, with an additional penalty of \(-1\) imposed for executing operating actions.

We evaluate multiple well-regarded RL frameworks as our explorer, including Deep Q-Network (DQN) (Mnih et al., 2015), Trust Region Policy Optimization (TRPO) (Schulman et al., 2015), and Recurrent Proximal Policy Optimization (RecurrentPPO) (Schulman et al., 2017). The Stable-Baselines3 library (Raffin et al., 2021) is employed for all implementations.

Figure 3: **An illustration of the detective pipeline for Conan.** An RL explorer is first trained to gather traces in accordance with the given question. Given a question and the incident scene, the _detective_ calls the explorer subroutine to gather evidence. Next, the exploration sequence undergoes key-frame extraction, processed by a visual encoder, subsequently feeding into a vision-language model for answer selection.

### Key-Frame Extractor

Given that the frames gathered by the explorer tend to be excessively lengthy and redundant, a key-frame extractor is utilized to sift through and select informative frames containing crucial evidence for the _detective_. We adopt a prevalent selection strategy employed in video understanding (Arnab et al., 2021). Specifically, frames within the temporal bounds determined by the detection of the first and last traces are retained, from which \(k\) frames are uniformly sampled. This design is intended to tailor the input with the constrained context window size to downstream vision-language models.

### Vision-Language Models for Abductive Reasoning

We employ a multi-choice question-answering paradigm akin to the one used in Ding et al. (2021). Specifically, the model is presented with a question, its corresponding exploration frame sequence, and each potential answer choice, subsequently generating a score for each choice. The model is trained with a categorical cross-entropy loss. During inference, the choice with the highest score is considered the answer. We evaluate several well-established multimodal models; these models are known for their efficacy in processing both visual and textual data. Additional details on model implementation can be found in Appx. D.1.

Vanilla-TransThe first baseline method leverages a vanilla transformer encoder to fuse observation and textual inputs. Specifically,the raw symbolic map from \(\)**Conan** serves as the visual feature, while CLIP's text encoder (Radford et al., 2021) is employed to encode the textual input.

FrozenBiLMFrozenBiLM (Yang et al., 2022), a state-of-the-art model for video question answering, combines visual input with frozen bidirectional language models, trained on web-scraped multimodal data. The approach integrates a frozen language model and a frozen vision encoder with light trainable visual projection modules. FrozenBiLM is tested with BERT-Large (Kenton and Toutanova, 2019) and DeBERTa-v3 (He et al., 2022) as the language model within our question-answering system, utilizing the symbolic map from \(\)**Conan** for visual input.

Flamingo-MiniFlamingo (Alayrac et al., 2022) is a family of vision-language models adept at rapid adaptation to novel tasks with minimal annotated examples. These models can handle sequences of visual and textual data, seamlessly accommodating interleaved images or videos as input. We finetune an open-sourced Flamingo-Mini model with frozen OPT-125M (Zhang et al., 2022), utilizing the symbolic map from \(\)**Conan** for visual input.

### Abduction from Deduction (AfD)

The adage "_Set a thief to catch a thief_" suggests the use of someone with a similar background or expertise to apprehend a wrongdoer: the best vandal catchers are vandals. This notion resonates with the core principle of Abduction from Deduction (AfD): for a skillful _detective_ to abduce what a _vandal_ does, it needs an in-depth grasp of vandals' _modus operandi_, motivations, and decision-making process. Translating the implication to a mathematical language, we articulate the problem of abductive reasoning based on evidence and knowledge from known deductive transitions. It can also be seen as an extension of inverse planning (Baker et al., 2007, 2009; Baker and Tenenbaum, 2014). Formally, let \(g\) denote the goal of the _vandal_, \(O\) the _detective_'s observation, and \(S\) the playground states post the _vandal_'s actions. We then have:

\[P(g O)=*{}_{P(S O)}[P(g S,O)]=* {}_{P(S O)}[P(g S)],\] (1)

where we assume the independence of \(g\) w.r.t. \(O\) given \(S\), as the goal ought to be clear given the states. Leveraging Bayesian rules, we further observe that

\[P(g S) P(S g)_{i}(a_{i} s_{i},g),\] (2)

assuming a uniform prior over \(g\) and known deterministic environment transitions. Eq. (2) asserts that \(P(g S)\) is proportional to a goal-conditioned forward action policy, where \(s_{i},a_{i} s_{i+1}\).

Intuitively, Eqs. (1) and (2) can be understood as follows: to **abduce** the _vandal_'s goal from observation, it is imperative to first reconstruct the actual states traversed by the _vandal_ and subsequently ascertain the most plausible goal that, if pursued forward, would result in those states; see Eq. (1). Eq. (2) can be interpreted as a form of **deduction**, being contingent on transition knowledge derived from a forward action policy. Hence the name Abduction from Deduction (AfD).

In practice, two approaches emerge for implementing \(P(g S)\) based on Eq.2. The first entails iterating over all \(g\) and utilizing a learned or predefined \(()\) to score a lengthy sequence of states. Conversely, the second approach embraces a data-driven strategy, wherein one arbitrarily selects \(g\), samples \(S\) from \(()\), and learns a model of \(P(g S)\) using the \((g,S)\) pairs. The former approach proves time-intensive during inference due to the combinatorial temporal space and expansive goal space, thereby compelling us towards the latter approach. For implementation, we train \(P(S O)\) independently as a Dirac delta function of \((f(O))\) and \(P(g S)\) from sampled pairs from \(()\) employed in task execution in the _vandal_. The derived goal features, along with the question, are fed into the model for answer prediction. Please refer to Appx. F for additional details.

## 5 Experiments

### Experimental Setup

ExplorationThe explorer is trained using DQN, TRPO, and RecurrentPPO for \(10^{8}\) steps, with a buffer size of \(10^{7}\) and a batch size of \(512\). In the case of DQN, training is conducted with \(=0.96\). Each episode is capped at a maximum of \(500\) steps for the explorer. A curriculum is employed to encourage long-term exploration whilst maintaining a balance with local search: initial training is carried out with traces from long-horizon tasks like "get the diamond," compelling the agent to venture farther from its starting point. Subsequently, the agent undergoes further finetuning across the entire dataset. Such a curriculum design prevents a sole focus on local discovery. For downstream reasoning models, \(k=30\) keyframes are extracted by the key-frame extractor.

Abductive InferenceOur reasoning models are tested under three different settings: Standard, Ideal Explorer, and AfD. In the Standard setting, models undergo training and testing based on the explorer's exploration. The Ideal Explorer setting sees models leveraging on an optimal exploration policy--visible to the ground-truth _vandal_'s trajectory, albeit imperfect, it facilitates the agent in gathering sufficient evidence for reasoning. This scenario can be conceived as a measure of the reasoning model's aptitude for passive reasoning given complete information. Under the AfD setting, models are trained and used as delineated in Sec.4.4. All models are trained utilizing \(8\) NVIDIA GeForce RTX 3090 GPUs. For further training specifics, please refer to Appx. D.2.

### Results and Analysis

Fig.4 shows the learning curves of various RL agents during exploration. TRPO and RecurrentPPO manifest similar performance in terms of rewards following a substantial number of steps, markedly surpassing the DQN explorer. Additionally, we probe the impact of augmenting the maximum number of exploration steps to \(5,000\) on performance. The data suggests a marginal performance uplift. Nonetheless, we acknowledge that such a performance increment is at the expense of substantially longer exploration time and a notable surge in the accrual of unrelated information. Consequently, we select TRPO with a maximum of \(500\) steps per episode as our standard RL explorer.

Quantitative results on \(_{}\) are depicted in Tab.3; both the standard and AfD results reported employ TRPO as the explorer. In the standard setting, we discern that while models exhibit some aptitude in tackling low-level Intent questions, they struggle with higher-level questions pertaining to Goal and Survival. Among the models, Flamingo-Mini ascends to the pinnacle with an accuracy of \(66.3\%\). FrozenBiLM models also perform relatively well. Notably, the DeBERTa variant slightly outperforms BERT, insinuating that a robust language backbone can improve general comprehension. Contrarily, the Vanilla-Trans model languishes across all tasks, achieving merely random-level performance.

Figure 4: **Learning curves of various RL explorers. The suffix \(n\) denotes the maximum number of steps per episode during exploration. Results show that (i) TRPO and RecurrentPPO markedly outperform DQN in performance, and (ii) longer episodes marginally contribute to the performance at the expense of longer exploration time and the accrual of unrelated information.**

With the Ideal Explorer, we notice a clear performance boost across all tasks, particularly in the Goal and Survival questions. These results allude to the potential bottleneck of models' abductive reasoning capability due to the insufficient information collected, underscoring the significance of effective exploration. An adept explorer can significantly aid in the accrual of useful information, informatively pursuing a hypothesis to scrutinize evidence, swiftly self-correcting upon encountering conflicting evidence, and reasonably re-planning. The findings also hint sufficient room for the RL explorer to improve. Remarkably, the Vanilla-Trans exhibits the greatest increase, insinuating that, in comparison to other baseline models, it is markedly vulnerable to insufficient evidence.

For AfD results, nearly all multimodal models exhibit performance on par with end-to-end supervisedly trained models. Remarkably, FrozenBiLM models even surpass the performance observed in standard settings. The persisting failure of Vanilla-Trans can be ascribed to its weakness in reasoning amidst incomplete observations due to the significant disparity between the familiar complete state \(S\) and incomplete observation \(O\). Examining task-specific results, a notable performance uplift in the Survival task models is discernible for almost all models relative to the standard setting, albeit sharing the same observation. These results intimate that the inclusion of deductive information sensitizes the _detective_ to _vandal_'s concerns during task execution. Nevertheless, the exhibited performance in long-term planning remains weak, reinforcing the pressing need for a better exploration policy. Critically, these models continue to find short-term intent questions to be most easily answered.

### Further Discussion

Additional ExperimentsWe further experiment in the absence of visual inputs, serving as a negative control baseline, resulting in random performance across all settings; see Appx. E. This random-level performance underscores the severe constraints imposed on the agent without visual information. The TRPO explorer shows a noticeable improvement over the ones without visual inputs, suggesting that even minimal exploration is preferable to none. Nonetheless, the performance remains relatively modest. On the other hand, the Ideal Explorer demonstrates markedly superior performance, attesting to the substantial benefits its capacity to accrue perfect trace evidence renders to the downstream reasoning task. This accentuates the imperative of effective exploration.

   get\_drink & defeat\_cow & get\_apple & defeat\_skeleton & make\_iron\_pickaxe \\
47.06 & 43.90 & 35.7 & 46.59 & 56.52 \\
100.00 & 85.37 & 78.57 & 82.95 & 52.17 \\  place\_bed & make\_steak & make\_stone\_pickaxe & get\_coal & make\_stone\_sword \\
43.90 & 46.15 & 48.48 & 50.00 & 37.50 \\
87.80 & 50.00 & 39.39 & 45.45 & 41.71 \\  get\_iron & get\_water & get\_stone & make\_iron\_sword & place\_furnace \\
28.57 & 45.95 & 36.84 & 56.25 & 44.44 \\
46.43 & 54.05 & 47.37 & 28.12 & 83.95 \\  get\_diamond & place\_table & get\_wood & make\_wood\_pickaxe & make\_wood\_sword \\
40.62 & 39.36 & 36.00 & 40.00 & 50.00 \\
84.38 & 91.49 & 96.00 & 55.00 & 64.29 \\   make\_bed & get\_java & make\_bucket & get\_beef & defeat\_zoebie \\
47.83 & 50.00 & 35.29 & 53.85 & 52.50 \\
39.13 & 66.67 & 73.53 & 42.31 & 75.00 \\   

Table 4: **Error analysis on \(_{}\) Conan.** We examine the accuracy of FrozenBiLM-DeBERTa across various tasks, comparing two explorer groups: reasoning based on the TRPO explorer and the Ideal explorer (in vivo).

    &  &  &  \\   & \(I\) & \(G\) & \(S\) & \(O\) & \(I\) & \(G\) & \(S\) & \(O\) & \(I\) & \(G\) & \(S\) & \(O\) \\  Vanilla-Trans & 32.9 & 25.0 & 24.5 & 28.8 & 64.0 & **78.4** & 58.1 & 66.1 & 24.8 & 23.3 & 24.5 & 24.3 \\ F-BiLM-BERT & 72.6 & **44.4** & **54.4** & 61.0 & 87.5 & 59.5 & 61.5 & 74.0 & 82.8 & **42.9** & **55.5** & 66.0 \\ F-BiLM-DeBERTa & 82.9 & 43.1 & 52.2 & 65.3 & **87.7** & 71.8 & **63.9** & **77.8** & 82.9 & 41.9 & 53.8 & 65.4 \\ Flamingo-Mini & **86.2** & 43.3 & 49.5 & **66.3** & 85.8 & 47.8 & 56.6 & 69.0 & **84.9** & 42.5 & 52.2 & **66.1** \\   

Table 3: **Performance of abductive reasoning models on \(_{}\) Conan.** We report the question-answering accuracy (\(\%\)) across various settings, with the overall accuracy averages over all question categories. F-BiLM refers to the FrozenBiLM model. **I** denotes Intent, **G** denotes Goal, **S** denotes Survival, and **O** denotes Overall. Results exhibiting the top individual performance are highlighted in **bold**, while models with the superior overall performance are shaded in **my**.

Error AnalysisWe extend an error analysis for the "goal" split, probing the reasoning model across a spectrum of tasks. Table 4 compares two groups: reasoning based on the Ideal explorer and the TRPO explorer. The findings underscore that proficient exploration, _i.e._, the heuristic Ideal explorer who recovers the vandal's trajectory, is sufficient for satisfactory performance. However, to fully harness the potential, a more adept reasoner is requisite, one capable of deciphering the vandal's hidden states from observed traces. For instance, the act of telling trees could signify a need for either wood or food (apples), and discerning the intent solely from traces of felled trees presents a challenge. When it comes to "trace-relevant" frames or "keyframes," the Ideal explorer could ostensibly furnish all trace-relevant frames. However, the concept of keyframes remains nebulous. Within the video understanding domain, a formidable challenge lies in the extraction of "keyframes." This is a post-hoc concept that eludes straightforward acquisition upfront. A prevailing approach, aimed at augmenting efficiency (diminishing context length in Transformer), entails truncating it via every k-th frame.

Joint ReasoningThe collective enhancement of both exploration and reasoning elements emerges as quintessential, given its mirroring of human-like intelligence. For instance, by providing feedback, the reasoner can steer the explorer towards actions that are potentially more insightful and likely to produce pertinent traces. Nonetheless, practical implementation encounters significant hurdles. Assigning credit to exploratory decisions bearing long-term implications can be intricate, particularly when the outcomes of exploratory actions become evident after a substantial time lapse, thereby muddying the causal relationship between the decisions and their ultimate effect on reasoning and answering questions. This accentuates the mutual reliance between exploration and reasoning--advancement in one facet demands progression in the other, introducing a bilateral dependency that complicates optimization. The reasoning component alone demands hefty training and computational resources, especially when utilizing large language models. The demand for formidable computational power renders the simultaneous optimization of exploration and reasoning exceedingly daunting. Collectively, this approach is also widely adopted (Gordon et al., 2018; Lei et al., 2018; Kocisky et al., 2018). Consequently, we navigate along this trajectory, projecting that future endeavors on \(\)**Conan** should prioritize reasoning above exploration.

To summarize, the engagement of a proficient explorer substantially enhances abductive reasoning, particularly in higher-level tasks such as goal-oriented and survival-centric inquiries. This underlines the criticality of exploration as a precursor to tackling abductive reasoning tasks in the presence of incomplete information. Furthermore, the achievement of the AfD hint at the potential for models to harness world knowledge, especially transition knowledge pertaining to tasks and traces, to transform abductive reasoning into deductive simulation. We posit that the presented approach resonates more with human-like reasoning, edging us closer to the core of human intelligence.

## 6 Conclusion

In this paper, we introduce \(\)**Conan**, a benchmark tailored to evaluate and assess models' active reasoning ability in addressing incomplete-information questions in an interactive environment. \(\)**Conan** sets itself apart from existing abductive reasoning benchmarks by incorporating an open-world playground facilitating active exploration. It differentiates itself from prevailing embodied question-answering benchmarks by introducing the demanding abductive process in question answering, necessitating multi-round abductive inference based on gathered evidence. Moreover, we propose a new learning paradigm, Abduction from Deduction (AfD), that turns the problem of abduction to deduction, exploiting the problem structure through Bayesian principles. Benchmarking the efficacy of contemporary machine learning models on \(\)**Conan**, we elucidate the model limitations in interacting with the environment that leads to failure in higher-level, longer-term abductive reasoning.

Limitations and Future WorkIn general, we notice two significant limitations from the experimental results. For one, the explorer does not supply particularly relevant information for the reasoning model. In the human abductive reasoning process, exploration and reasoning should be closely intertwined, with an agent using the current hypothesis to guide exploration and improve its understanding. However, due to long-range exploration and complex vision-language reasoning, we only applied the conventional visual question-answering method and did not fully integrate these two processes. For another, learning naive question-answer mapping shall be sub-optimal. By leveraging the problem structure, AfD has shown improved performance on a particular set of problems. Nevertheless, the current AfD formulation is still rudimentary. We believe an in-depth understanding of the structure and well-crafted implementation could further boost performance.

AcknowledgementThe authors would like to thank Ms. Zhen Chen (BIGAI) for designing the figures, and NVIDIA for their generous support of GPUs and hardware. M.X., G.J., W.L., C.Z., and Y.Z. are supported in part by the National Key R&D Program of China (2022ZD0114900), M.X. and W.L. are supported in part by the NSFC (62172043), and Y.Z. is in part by the Beijing Nova Program.