# Universality and Limitations of Prompt Tuning

Yihan Wang

UCLA

wangyihan617@gmail.com &Jatin Chauhan

UCLA

chauhanjatin100@gmail.com &Wei Wang

UCLA

weiwang@cs.ucla.edu &Cho-Jui Hsieh

Google and UCLA

chohsieh@cs.ucla.edu

###### Abstract

Despite the demonstrated empirical efficacy of prompt tuning to adapt a pretrained language model for a new task, the theoretical underpinnings of the difference between "tuning parameters before the input" against "the tuning of model weights" are limited. We thus take one of the first steps to understand the role of soft-prompt tuning for transformer-based architectures. By considering a general purpose architecture, we analyze prompt tuning from the lens of both: universal approximation and limitations with finite-depth fixed-weight pretrained transformers for continuous-valued functions. Our universality result guarantees the existence of a strong transformer with a prompt to approximate any sequence-to-sequence function in the set of Lipschitz functions. The limitations of prompt tuning for limited-depth transformers are first proved by constructing a set of datasets, that cannot be memorized by a prompt of any length for a given single encoder layer. We also provide a lower bound on the required number of tunable prompt parameters and compare the result with the number of parameters required for a low-rank update (based on LoRA) for a single-layer setting. We finally extend our analysis to multi-layer settings by providing sufficient conditions under which the transformer can at best learn datasets from invertible functions only. Our theoretical claims are also corroborated by empirical results.

## 1 Introduction

The surge in the empirical research of large-scale models has led to the emergence of a new paradigm of prompt tuning. Current large models consist of billions of parameters (Brown et al., 2020; Chowdhery et al., 2022), which greatly exacerbate the cost of tuning the entire model weights via gradient-based optimization. On the other hand, the power of scale in both model size and pretraining dataset size has demonstrated strong capabilities by achieving reasonable performance through a learnable prompt appended before the input (Li and Liang, 2021; Lester et al., 2021). Despite this, several questions emanate around the abilities and limitations of prompt tuning.

In this work, we aim to characterize some natural yet essential questions about prompt tuning with transformer architectures. Firstly, are prompts universal approximators, i.e. with a fixed pretrained transformer network, can we find a prompt to approximate any sequence-to-sequence function in a given space? If yes, can we construct the transformer for this universality result? Second, can we identify failure modes of prompt tuning when applied on potentially non-optimal but non-trivial transformers? Moreover, since prompt tuning is usually compared against LoRA(Hu et al., 2021) in consideration to parameter-efficient tuning, is prompt tuning then more/less parameter-efficient than LoRA? Answering these questions can lead to important insights on _when_ and _how_ to perform prompt tuning to adapt a pretrained transformer network to a given downstream task of interest.

In this work, we seek to answer these questions with appropriate theoretical analysis and further validate our claims with empirical results. We first characterize the universal nature of prompt tuning by constructing a specific transformer network. We show that for a given approximation error and the space of sequence-to-sequence Lipschitz functions, we can construct a transformer network, with a suitable number of layers, that can leverage prompt tuning to approximate any function in this space. Despite this universality of prompt tuning with a carefully constructed pretrained transformer, we then identify some limitations of prompt tuning with weaker but non-trivial transformers. We prove this by constructing sequence-to-sequence datasets with shared input tokens, which are surprisingly simple but cannot be memorized by prompt tuning for a given transformer. We also extend our analysis to more general settings where the shared token is not required. In this setting, we first prove that prompt tuning on a single-layer transformer requires \((n)\) trainable parameters to memorize \(n\) training examples, wherein for LoRA, it suffices with \(O(n)\) trainable parameters. We finally extend our analysis to the multi-layer setting and provide sufficient conditions under which prompt tuning exhibits extremely limited capacity to at best memorizing datasets from invertible functions.

Our contributions can be summarized as below:

* We characterize the universal nature of prompt tuning by explicitly constructing a transformer network (Theorem 1).
* We provide a construction-based argument for sequence-to-sequence datasets that cannot be learned by prompt tuning with a given single-layer transformer (Theorem 2).
* We provide the lower bound on the required number of parameters for prompt tuning to memorize any sequence-to-sequence functions (Theorem 3).
* We provide a sufficient condition for multi-layer transformers, under which datasets with shared output tokens cannot be learned with prompt tuning (Theorem 4).
* We conduct empirical studies, including real-world datasets, to verify our theoretical claims.

## 2 Related Work

Theoretical Analysis of TransformersVarious works have characterized the theoretical properties of transformers and its primary self-attention component. Yun et al. (2020) studies the universal approximation ability of transformers for continuous permutation equivariant sequence-to-sequence functions with compact support and further examined the role of using positional encodings to circumvent permutation equivariant condition. Perez et al. (2021) show that transformer with a hard-attention is Turing complete based on their capacity to perform computations and access the internal dense representations of the data. Wei et al. (2021) further shows that transformers can approximate Turing machines with bounded computation time with a new notion of approximation. (Dong et al., 2021) provides a negative yet interesting result signifying the limitations of pure self-attention in terms of rank diminishing of the input. Other works including Kim et al. (2021); Dasoulas et al. (2021) derive upper bounds on the Lipschitz constant of respective modifications of the attention mechanism. The works by Li et al. (2022); Zhang et al. (2020) documented optimization perspective on transformer training via SGD.

Fine-tuning and Prompt TuningFine-tuning is the standard way to adapt a pretrained model to downstream tasks. The most standard and popular paradigm is tuning the model weights via a suitable optimization procedure along with a linear head on the output representations (Radford et al., 2018; Devlin et al., 2019). Subsequent works studied more parameter-efficient ways of fine-tuning by updating either a subset of model parameters (Ben Zaken et al., 2022) or restricting the parameter-updates to a low-dimensional subspace (Aghajanyan et al., 2021; Hu et al., 2021; Mahabadi et al., 2021). The work by Hu et al. (2021) (their framework referred to as LoRA) has garnered particular interest in the community and Malladi et al. (2023) has provided an interpretation of LoRA via the kernel mechanism. In the particular context of LLMs, prompt tuning has emerged as the de facto approach where only the prompt is updated while keeping the rest of the transformer weights and architecture fixed (Shin et al., 2020; Lester et al., 2021; Li and Liang, 2021).

Analysis of Prompt TuningWei et al. (2022) studies the link between prompt tuning and downstream tasks with an underlying latent variable generative model of text, which is confined to a Hidden Markov Model. However, they focused on the discrete vocabulary setting contrary to our results for continuous sequence-to-sequence functions. Some more recent works  characterize an intriguing property of a specific form of prompting, referred to as in-context learning, where they proved by construction that transformers can implement learning algorithms for linear models based on gradient descent and closed-form ridge regression. This work however pursued a different and specific direction from the prompting results we aim to provide for generic settings.

Memorization Capacity of Neural NetworksA series of works have sought to provide finite sample universal memorization capacity results of neural networks and the understanding of expressive power of neural networks. Huang and Huang , Huang , Yamasaki  analyze the memorization capacity of FNNs with sigmoid and other bounded activation functions. Hardt and Ma , Zhang et al. , Nguyen and Hein  provide results for modern ReLU networks including FNNs and CNNs. For transformer architectures, Kim et al. prove that transformers can memorize a dataset with finite parameters. To the best of our knowledge, similar results for prompt tuning have not been studied in continuous settings for transformer architectures.

## 3 Transformers and Parameter Efficient Training

### Preliminaries

We use the following notations throughout the paper. A bold lower case character, e.g. \(\), denotes a vector. A bold upper case character, e.g. \(\), denotes a matrix while \(_{i,j}\), \(_{i,:}\) and \(_{:,j}\) is the \((i,j)\)-th element, \(i\)-th row, \(j\)-th column, respectively. We use a single superscript or subscript to denote the index of a matrix, e.g. \(_{i},^{i}\) denote the \(i\)-th matrix in a matrices sequence. We use \(\) and \(\) for softmax and hardmax operators, respectively. We use \(()=(,)\) to denote the ReLU activation function where \(()\) function is applied entry-wise to a vector. We use \((_{1},_{2},...,_{m})\) to denote a cone without its origin point where \((_{1},_{2},...,_{m})=\{: =_{i=1}^{m}a_{i}_{i},a_{i}>0\}\). We also define the minus operation between a set \(S\) and a vector \(\) as \(S-=\{-: S\}\). In Section 4, we use \([a:b:c]\) to denote a grid \(\{a,a+b,a+2b,...,c-b\}\) from \(a\) to \(c\), with an interval \(b\).

Transformer networks  are a stack of multiple transformer layers, composed subsequently. A transformer layer has two key components: an attention layer and a token-wise MLP layer, with residual connections around both blocks. We consider the input and output to be sequences of tokens \(^{d m}\) and \(^{d m}\), where \(m\) is the number of tokens in the sequence and \(d\) is the token dimension.

**Definition 1** (Attention Layer).: _We define an \(h\)-head attention layer parameterized with \(_{q},_{k},_{v},_{o}\) between a single token \(\) and a token sequence \(\) as_

\[(,)=_{i=1}^{h}_{o}^{i}_{v}^{i}((_{k}^{i})^{}_{q}^{i} ).\] (1)

_The normalizing factor of \(}}\) is subsumed in the weight matrices \(_{k}^{i}\) for notational simplicity._

_We can then define the cross attention between two sequences \(_{1}^{d m_{1}}\) and \(_{2}^{d m_{2}}\) (We use \(_{k}=(_{1})_{:,k}\) for simplicity):_

\[(_{1},_{2})=[(_{1}, _{2}),(_{2},_{2}),...,(_{m_{1}},_{2})].\]

**Definition 2** (Standard Transformer Layer).: _With definition 1, we define a standard transformer layer \(\) as_

\[()=[_{2}(_{1}_{:,1}+_{1})+_{2}+_{:,1},...,_{2} (_{1}_{:,n}+_{1})+_{2}+ _{:,n}]\] (2)

\[()=((,)+).\] (3)

_The definition here omits the layer normalization block for simplicity (following )._

We denote the set of transformer networks with \(h\) heads of size \(s\) and \(r\) MLP hidden neurons with \(^{h,s,r}\). In Section 4, we utilize a modified transformer network with hardmax operation \(\) instead of softmax \(\). We denote this modified version of transformer networks as \(}^{h,s,r}\).

During fine-tuning, we optimize the matrices \(_{q}^{i},_{k}^{i},_{v}^{i}\) in the attention layer and \(_{1},_{2},_{1},_{2}\) in the MLP layer pertaining to a loss function \(\). However in prompt tuning, the pretrained model weight matrices are fixed and we optimize a tunable sequence prepended to the input.

Prompt TuningGiven a pretrained transformer network \(g\) and a downstream training dataset \(S=\{(_{1},_{1}),...,(_{n},_{n})\}\), prompt tuning seeks to find a prompt \(^{*}^{d m_{p}}\) with \(m_{p}\) tunable tokens under the loss function \(\):

\[^{*}=*{arg\,min}_{}_{i=1}^{n}( g([,_{i}])_{:,m_{p}:},_{i}).\] (4)

The tunable prompt \(\) is shared amongst all the inputs in a task. Note that \(\) in prompt tuning is a continuously trainable parameter, alternately referred to as soft prompt, which is different from hard prompt in that the latter operates on a discrete space of predefined vocabulary. Since the representation power of soft prompts is strictly more than the hard prompts, the limitations studied in this paper also extend to hard prompts.

In the subsequent sections, we analyze the universality and limitations of prompt tuning while comparing the latter against fine-tuning and LoRAHu et al. (2021), which is a low-rank version of model fine-tuning. In Section 4, we prove that prompt tuning can be universal approximators for sequence-to-sequence functions, while providing the construction for the same. In Sections 5 and 6, we identify the failure modes where prompt tuning cannot learn with a possibly non-optimal but non-trivial pretrained transformer network.

## 4 Universality of Prompt Tuning

Without loss of generality, we assume that the support and range set of all considered sequence-to-sequence functions \(f\) is \(^{d m}\) in this section. We define \(_{L}\) as the collection of all continuous sequence-to-sequence \(L\)-lipschitz functions under norm \(p\) and sequence length \(m\). For \(f F_{L}\) and any two inputs \(,^{}^{d m}\), we have \(\|f()-f(^{})\|_{p} L\|-^{ }\|_{p}\). Furthermore, given functions \(f_{1},f_{2}\), the approximation error under a \(p\)-norm (which is entry-wise) is measured as:

\[d_{p}(f_{1},f_{2})=(\|f_{1}()-f_{2}()\|_{p}^{p}d )^{}.\] (5)

Primarily, we show that there exists a Transformer network \(g^{2,1,4}\) such that for any \(f_{L}\), prompt tuning on \(g\) can approximate this function upto some error budget \(>0\).

**Theorem 1**.: _Let \(1 p<\) and \(>0\), there exist a transformer network \(g^{2,1,4}\) and prompt length \(m_{p}\), such that for any \(f_{L}\) we can find a prompt \(^{d m_{p}}\) with \(d_{p}(g([,])_{:,m_{p}:},f)\)._

Here we use the transformer in a encoder mode which generates the \(m\) outputs in one step. In Appendix C.4, a similar result can be obtained for next-token prediction, which is widely used in many recent language models.

The proof is inspired from (Yun et al., 2019), which follows the typical construction based proof mechanism to show universality. Thereby, we can construct a "meta-transformer" for prompt tuning to approximate any sequence-to-sequence function with prompt tuning. Next we briefly describe the two steps for the construction of this meta-transformer. We start by building a meta-function for \(_{L}\).

Building the Meta-FunctionWe denote the length of all inputs as \(m\) and the prompt length as \(m_{p}\). Then we can build a sequence-to-sequence meta-function that accepts inputs with length \(m+m_{p}\).

**Lemma 1**.: _For the sequence-to-sequence function space \(_{L}\) with functions \(f:^{d m}^{d m}\), we can build a sequence-to-sequence function \(:^{d(m_{p}+m)}^{d(m_{p}+m)}\) such that for any \(f_{L}\), we can find \(^{d m_{p}}\), \(d_{p}(([,])_{:,m_{p}:},f)/2\)._

The complete proof is given in Appendix C.1. Succinctly, we first quantize the input and output sequence space of \(^{d m}\) into a grid \(G_{,m}=\{0,,2,...,1-\}^{d m}\), thus leading to \(C=(})^{}}\) possible functions mappings from the input to the output, in this discrete space. Bythis quantized function space as \(}_{L}=\{f_{1},_{2},...,f_{C}\}\), we can select \(\) such that the approximation error for any function is less than \(/2\). Then we construct a set of quantized prompts in \(G_{,m_{p}}=\{0,,2,...,1-\}^{d m_{p}}\) to index these \(C\) functions and construct a quantized function \(\) where \(([_{i},])_{:,m_{p}:}=_{i}(),i=1,2,...,C\), for all \( G_{,m}\), thereby concluding the lemma.

Next we can utilize some conclusions in  to construct a transformer for \(\).

Constructing the Meta-TransformerWe first introduce a useful lemma which enables the construction of a transformer for any quantized sequence-to-sequence function.

**Lemma 2**.: _For any given quantized function \(:^{d m}^{d m}\) with quantization at interval \(\), \(}^{2,1,1}\) such that \(=\) with positional embedding \(=0&1&2&...&m-1\\ 0&1&2&...&m-1\\ &&&&\\ 0&1&2&...&m-1\)._

The proof mainly follows the discussions in Section C of . To prove this lemma, the network \(\) can be constructed in the following three steps. We first use a series of MLP layers to quantize the input to grid \([0::1-]^{d m}\) and then a series of attention layers to obtain a unique contextual mapping for each quantized input. Finally we can use a series of MLP layers to map the unique contextual mapping to the desired outputs. While a transformer network usually stacks self-attention and MLP layers alternately within a single layer, the aforementioned construction can be trivially attained via the use of skip connections. The complete proof of Lemma 2 is deferred to Appendix C.2.

Since \(\) is a quantized function in grid \(G_{,m+m_{p}}\), following Lemma 2 we can find a modified version of transformer \(}^{2,1,1}\) such that \(([,])=([,])\). The modified version of transformer \(\) with hardmax operators can then be approximated with a standard transformer \(g\) with softmax operators by Lemma 3.

**Lemma 3** (Lemma 9 in ).: _For each \(}^{2,1,1}\), \(>0\) and \(1 p<\), \( g^{2,1,4}\) such that \(d_{p}(,g)/2\)._

Since the approximation error can be treated uniformly amongst the \(_{i}\), we have that \(d_{p}(([_{i},])_{:,m_{p}:},g([_{i},])_{ :,m_{p}:}) d_{p}(([_{i},]),g([_{i},] )/2\). Therefore, we can build a transformer \(g^{2,1,4}\), such that for any sequence-to-sequence \(f_{L}\), we can find a quantized version \(_{i}}_{L}\) and the corresponding prompt \(_{i} G_{,m_{p}}\) such that

\[d_{p}(g([_{i},])_{:,m_{p}:},f) d_{p}(g([ _{i},])_{:,m_{p}:},([_{i},]))+d_{p}( ([_{i},])_{:,m_{p}:},_{i})+d_{p}(_{i},f) .\] (6)

Theorem 1 provides the construction for a large transformer (discussed more in appendix) that is sufficient for prompt tuning to exhibit universal approximation over a Lipschitz function space. However, even this strong transformer also has limitations with prompt tuning when the target function \(f_{L}\). Is this an essential limitation for prompt tuning on any transformer? In the next section, we will theoretically analyze the limitations of prompt tuning with transformers and target functions under more general conditions.

## 5 Limitations of Prompt-Tuning: Single Layer Transformer

To analyse the failure modes and therefore the limitations under the setting where a transformer has fixed pretrained weights, we follow the lens of exact memorization in the subsequent sections.

**Definition 3** (Memorization of a Sequence-to-Sequence Dataset).: _Given a sequence-to-sequence dataset \(S=\{(_{1},_{1}),...,(_{n},_{n})\}\) where \(_{i},_{i}^{d m}\) are the input/output sequences, we consider a function \(f\) exactly memorizing dataset \(S\) if \(f(_{i})=_{i}\). In the following proofs of this section, we explicitly focus on the last output token, ie: \(f(_{i})_{:,-1}=(_{i})_{:,-1}\)._

We start from the analysis on a single layer transformer and extend to multi-layer settings in Section 6.

### Failure modes of Prompt Tuning

It is straightforward to note that prompt tuning has limited expressive power when the number of trainable parameters is limited. A natural question to then ask is: Does increasing the number of trainable prompt tokens suffice? While it is known that for MLPs, even with a single hidden layer, increasing the number of hidden neurons can memorize any training data (Yun et al., 2019b). However, as we will prove next, this is not the case for prompt tuning. This result highlights an essential limitation of prompt tuning compared to model fine-tuning.

Before providing the theorem statement, we first outline some straightforward assumptions on the pretrained transformer and datasets, without which prompt tuning trivial loses expressive power.

We consider sequence-to-sequence datasets of the form \(S=\{(_{1},_{1}),(_{2},_{2}),...,( _{n},_{n})\}\) with \(n\) distinct examples and a single-layer single-head standard transformer defined in Definition 2. The results can be directly extended to the single-layer multi-head scenario, which we skip here to avoid notational clutter.

**Assumption 1** (Non-trivial conditions).: _We assume that all output tokens \((_{i})_{:,k}\) are in the range set of MLP, otherwise the expressivity becomes trivially weak. We assume that \(_{q},_{k},_{v}\) are full rank matrices and that \(}(_{i},_{i})+_{i}\) are distinct for \(i=1,2,...,n\)._

**Assumption 2** (Assumption for the MLP layer).: _We assume that \(d 2+}((}^{-1}(_{10})-_{0}) (}^{-1}(_{20})-_{0}))\) for the dataset constructed in Theorem 2 and token dimension \(d\). \(}()\) measures the dimension of subspace spanned by vectors in a set \(\) and \(}^{-1}()=\{:}()= \}\)._

_We provide an example for this assumption in Example 1 and a sufficient condition in the following Lemma 4._

**Lemma 4**.: _If \(\|_{1}\|_{2}\|_{2}\|_{2}<1\), where \(\|\|_{2}\) is the matrix spectral norm, then the MLP block in Definition 2 is invertible, ie, \(}^{-1}\) is a singleton set._

_Therefore, if Lemma 4 holds and \(d 4\), Assumption 2 also holds._

Proof of Lemma 4 can be found in Appendix C.5. The experimental evidence in (Dong et al., 2021) shows that for most architectures, the norm of the weight matrices indeed admits small values and thus the requirement that \(\|_{1}\|_{2}\|_{2}\|_{2}<1\) is a mild condition.

With these assumptions, here we introduce our first theorem on the unlearnability of prompt tuning.

**Theorem 2**.: _For a single layer transformer \(\) defined above with Assumptions 1 and 2, we can build a sequence-to-sequence dataset \(S=\{(_{1}=[_{1},_{0}],_{1}=[_{11},_{10}]),(_{2}=[_{2},_{0}], _{2}=[_{21},_{20}])\}\), and we cannot find a prompt \(^{d m_{p}}\) with any \(m_{p}>0\) such that \(([,_{i}])=_{i}\) holds for any \(i=1,2\). The vectors \(_{0},_{1},_{2}\) are denoted post positional encodings._

An important feature of this dataset is that the same token \(_{0}\) is shared between the two examples, and the expressive capability of prompt tuning is limited by the correlation of outputs corresponding to this token in different examples. We show a concrete example here to illustrate this theorem (note that Lemma 4 is in fact not required in the following construction) and defer the formal proof to Appendix C.6.

**Example 1**.: _We consider a single-head transformer layer \(\), where \(_{1}=_{2}=\), \(_{1}=1^{r d}\), \(_{2}=1^{d r}\). Then the token-wise MLP layer is a concatenation of two linear functions:_

\[}()=(_{2}_{1}+ ),(_{1})_{0}>0\\  56.905512pt,(_{1})_{0} 0\] (7)

_Here \((_{1})_{0}\) denotes the first element of vector \(_{1}\)._

\(_{2}_{1}+\) is a non-singular matrix. Therefore, for any \(\) in \(}()\)'s output set, \(}^{-1}()\) contains at most two points \(\{,(_{2}_{1}+)^{-1}\}\). We arbitrarily choose \(_{0},_{10}\) and \(_{20}\).

As long as \(d 6\) (from Assumption 2), we can find \(_{1},_{2}\) such that \(_{1},_{2}_{10}-_{0},_{ 20}-_{0},(_{2}_{1}+)^{-1}_{ 10}-_{0},(_{2}_{1}+)^{-1}_{ 20}-_{0},c_{1}_{2}\). Then we choose \(_{1}\) and \(_{2}\) such that \(}(_{0},_{1})_{1}\) and \(}(_{0},_{2})_{2}\) (Lemma 7 in Appendix). Then \(}(-}(_{0},_{1}),- _{0})}(-}(_{0},_{ 2}),-_{0})=\), for any \(\{_{10},(_{2}_{1}+)^{-1} _{10}\}\) and \(\{_{20},(_{2}_{1}+)^{-1} _{20}\}\). Here \(}\) stands for a convex cone as defined in Section 3.1.

If a \(\) exists such that \(([,_{i}])=_{i}\) holds for both \(i=1,2\), then we have

\[(_{0},[,_{1}]) =(_{1},_{0},[,_{1} ])(_{0},_{1})+(,_{0 },[,_{1}])(_{0},)\] (8) \[(_{0},[,_{2}]) =(_{2},_{0},[,_{2} ])(_{0},_{2})+(,_{0 },[,_{2}])(_{0},)\]

where \((,,)\) is a positive scalar. We also have

\[(_{0},[,_{1}])+ _{0}^{-1}(_{10})\] \[(_{0},[,_{2}])+ _{0}^{-1}(_{20})\]

as \(((_{0},[,_{i}])+_{0})=_{i0},i=1,2\).

Therefore, \((_{0},)\) must be in both \((-_{0},-(_{0},_{1}))\) and \((-_{0},-(_{0},_{2}))\), where \(\{_{10},(_{2}_{1}+)^{-1} _{10}\}\) and \(\{_{20},(_{2}_{1}+)^{-1} _{20}\}\), which contradicts the existence of \(\) as \((-(_{0},_{1}),-_{0})(-(_{0},_{2}),- _{0})=\). Therefore, in this example, even though we allow an arbitrary number of trainable parameters in prompt \(\), we cannot find one to exactly memorize the training set with only two training examples.

This theorem reveals an important difference between prompt tuning and adjusting the model weights directly. For any training dataset \(S\) with two training examples \(\{(_{1},_{1}),(_{2},_{2})\}\), so long as \((_{1},_{1})+_{1}\) and \((_{2},_{2})+_{2}\) are distinct, MLP can easily map the post-attention features to expected output tokens with finite number of hidden neurons. As a result, tuning the MLP parameters for this pretrained transformers can memorize any dataset in the form of Assumption 1. However, prompt tuning cannot achieve this even if the number of tunable tokens \(\) infinity, thereby limiting the expressiveness of prompt tuning when compared to model fine-tuning.

### Comparison with a More General Dataset

In Section 5.1, we constructed sequence-to-sequence datasets that cannot be learned by a given transformer layer with prompt tuning, by utilizing the shared token between different training examples. In this section, we compare the expressive power of prompt tuning and fine-tuning under a more general dataset construction where the former requirement can be relaxed.

Since the primary essence of prompt tuning is to perform parameter-efficient tuning, wherein we seek to adapt a pretrained large model to a new task with fewer tunable parameters, we compare prompt tuning with another parameter-efficient version of model-tuning: LoRA [Hu et al., 2021]. Succinctly, we compare the required number of parameters to memorize a given dataset. Again, consider a sequence-to-sequence dataset \(S=\{(_{1},_{1}),(_{2},_{2}),...,( _{n},_{n})\}\), where \(_{i}=[_{i1},_{i2},...,_{im}]\) and \(_{i}=[_{i1},_{i2},...,_{im}]\). We again discuss the memorization of the last output token for simplicity and results can be directly extended.

We first give the required number of parameters of LoRA to memorize dataset \(S\).

**Lemma 5** (LoRA).: _For a standard single-layer transformer \(\) defined in Definition 2 with \(r n\) MLP hidden neurons, for any sequence-to-sequence dataset \(S\) satisfying Assumptions 1, we can apply a low-rank update to MLP weights with \(O(nd)\) parameters to memorize \((_{i})_{:,m}=_{im}\)._

This lemma is derived based on the memorization capabilities of 1-hidden layer MLPs [Yun et al., 2019b]. As the post-attention values for different training inputs are different from Assumption 1, we can construct a low rank update with \(O(nd)\) parameters on the MLP layer to memorize \(S\). We defer the complete proof to Appendix C.7.

For prompt tuning, we derive a result in the next theorem which shows that it requires \((nd)\) tunable parameters to memorize some constructed dataset \(S\) with \(n\) examples.

**Theorem 3** (Lower bound on Tunable Prompt Parameters).: _For any single layer transformer \(\) defined in Definition 2, there exists a sequence-to-sequence dataset \(\{(_{1}=[_{10},_{1}],[_{10},_ {11}]),(_{2}=[_{20},_{2}],[_{20}, _{21}]),...,(_{n}=[_{n0},_{n}],[ _{n0},_{n1}])\}\) that satisfies Assumption 1 with \(n<d\) training examples such that we need at least \(n\) prompt tokens in \(\) to memorize the training set, ie, for \(([,_{i}])_{:,-1}=_{i1}\) to hold for all \(i=1,2,...,n\)._

This dataset can be constructed by including \(n\) examples that require \(n\) linearly independent prompts tokens. The complete proof is deferred to Appendix C.8.

Note that in Theorem 3, we provide a key lower bound on the required number of prompt tokens for exact memorization and this can very well more than \(nd\). This partially (but not necessarily) explains the worse empirical performance of prompt tuning against LoRA under a comparable number of trainable parameters.

## 6 Extension to Multi-Layer Setting

In this section, we extend our analysis to multi-layer setting and provide a sufficient condition under which the expressiveness of prompt tuning is restricted. An immediate consequence of our result is an interesting connection to the spectral norm of soft prompts surfaces. This result provides us a partial understanding of the phenomenon that soft prompt \(\) vectors typically exhibit larger norms compared to the actual input \(\), after the tuning.

With some further notation adjustments, we denote an \(H\) layer pretrained transformer network as \(g()=^{1}^{2}...^{H}\), the input set as \(^{1}\), and the set of possible prompts as \(^{1}\). We assume that the following compactness condition is satisfied:

\[\|[^{l},^{l}]\|_{2}  D^{l}\] (9) \[ [^{l+1},^{l+1}] =^{l}([^{l},^{l}]), l=1,...,H.\]

Here \([^{1},^{1}]\) is the input to the first layer \(^{1}\) with \(^{1}^{1}\), \(^{1}^{1}\) and \(\|\|_{2}\) is the spectral norm. Similarly, \([^{H+1},^{H+1}]\) denotes the output set.

We start by providing an upper bound to the Lipschitz constant of attention, pertaining to eq 9. This derivation is different from the works of (Dasoulas et al., 2021; Vuckovic et al., 2020) and thus can be of independent interest.

**Lemma 6**.: _Under the compactness condition, the Lipschitz constant of the \(i\)-th attention head in the \(l\)-th transformer layer, denoted for simplicity as \(}^{i,l}\), admits the following bound w.r.t the entire input sequence of length \(m\):_

\[Lip(}^{i,l}(,))(1+8(D^{l})^{2}\|(_{k}^{i,l})^{T}_{q}^{i,l}\|_{2})\|_{v}^{i,l}\|_{2},\] (10)

_and the Lipschitz constant of the entire attention block in layer \(l\), denoted as \(}^{l}\), admits the bound:_

\[Lip(}^{l}(,))^{h}(\|_{o} ^{i,l}\|_{2} Lip(}^{i,l}))^{2}}.\] (11)

It is noteworthy that this upper bound is dependent on \(D^{l}\), the spectral norm of the input prepended with the prompt. In conjunction with the following theorem, we obtain a result on limited expressivity of prompt tuning by showing that the transformer becomes invertible, in consideration to functions from \(^{1}^{1}^{H+1}^{H+1}\) (an extension to functions of the from \(^{1}^{H+1}\) is provided in Appendix Section C.11).

**Theorem 4**.: _A transformer \(g\) is invertible, ie \(,g^{-1}()=\{:g()=\}\) is a singleton set \(\) in range of \(g\), if:_

1. _The Lipschitz constant of the attention block in each layer_ \(^{l}\) _is strictly less than 1_
2. _The Lipschitz constant of the 2-layer ReLU block in each layer_ \(^{l}\)_, which is bounded by_ \(\|_{2}^{l}\|_{2}\|_{1}^{l}\|_{2}\)_, is strictly less than 1._

Proof of Theorem 4 can be found in Appendix C.9. Combining Lemma 6 and Theorem 4, we observe that the invertibility is guaranteed if the upper bound for the Lipschitz constant of the attention, eq 11, and the MLP layer, is strictly less than 1. In this case, we can then construct arbitrarily many datasets where two different inputs share the same output, and prompt tuning cannot learn (more subtly: memorize) these datasets with a restricted prompt norm.

Experiments

### Experimental Settings

In Section 7.2, we use a standard single-layer single-head transformer from Definition 2, to justify the infinite prompt-length limitation. In Section 7.3, we justify the increasing prompt norm on the pretrained LLaMA 7B model (Touvron et al., 2023). For prompt tuning and LoRA, we use the Huggingface Pelt library (Mangrulkar et al., 2022). On the dataset front, we utilize the RTE subtask of SuperGlue dataset (Wang et al., 2019) and WMT14 En-Fr translation (Bojar et al., 2014). More details and hyperparameter settings can be found in Appendix A.

### Limited Expressivity of Infinite Length Prompt

We first construct the dataset following the proof of Theorem 2 and then show that prompt tuning cannot memorize this simple dataset \(\{(_{1}=[_{1},_{0}],_{1}=[_ {11},_{10}]),(_{2}=[_{2},_{0}],_{2}=[_{21},_{20}])\}\) even with very large prompt lengths.

We set the token dimension \(d=10\). We follow the default pytorch weight initialization and then normalize \(_{1},_{2}\) such that \(\|_{2}\|_{2}\|_{1}\|_{2}<1\), following Assumption 2. We randomly sample \(_{0},_{10},_{20}\) in a uniform distribution in \([0,1)^{d}\) and construct the corresponding vectors: \(_{1}\) and \(_{2}\) following Theorem 2. To compute \(^{-1}()\), we follow (Kim et al., 2021) Section 4.1 with 5000 iterations at convergence. We solve \((_{0},[_{0},_{1}])\) in Lemma 7 with gradient descent terminating at \(((_{0},[_{0},_{1}]), )<0.0001\). We repeat this setup to obtain 3 different datasets for distinct \(_{0},_{10},_{20}\) and denote these with \(S_{i},i=1,2,3\).

We perform prompt tuning, MLP fine-tuning and MLP LoRA training on the constructed datasets for 5 runs and report the mean and standard deviation of per-element Mean Squared Error (MSE) loss \(_{10},_{20}\) at convergence. We show the comparison between prompt-tuning and MLP fine-tuning in Figure 1. As we can observe from the figure, increasing the number of soft prompt tokens post a certain threshold that does not exhibit any reduction in MSE. On the contrary, fine-tuning on the MLP layer tend to easily memorize the training set by reducing the training loss to almost zero (all the three curves for fine-tuning overlap and thus not differentiated). Note that we plot the standard deviation, however it is negligible in the range. Similar to fine-tuning on the MLP layer, LoRA with width 2 on the MLP layer also achieves near-zero training loss which is less than \(10^{-10}\) on the constructed dataset. We don't plot the comparison on Figure 1 as all the six curves are overlapped). This result validates our Theorem 3 that LoRA can memorize a dataset with \(n\) examples with trainable parameters \(O(n)\) while prompt-tuning may require more.

### Increasing Prompt Spectral Norm during Tuning

As discussed in Section 6, a major constraint on the expressive power of prompt tuning is the spectral norm of soft prompts. In Figure 2, we plot the curve for spectral norm of soft prompt as training progresses and the loss reduces on RTE dataset. The curve for WMT14 En-Fr dataset can be found in Appendix B. This trend clearly highlights that in order to counter the limit on the capacity, the spectral norm consistently increases till the training loss saturates.

## 8 Conclusions

In this work, we embark on exploring the capabilities of prompt tuning in the continuous regime, contrasting it with fine-tuning, as an initial endeavor towards a theoretical comprehension. We prove by construction that prompt tuning admits universal approximation within the space of Lipschitz functions. Additionally, we identified inherent limitations of prompt tuning on single-layer transformers by constructing theoretically difficult datasets for prompt tuning. These limitations are then extended to multi-layer setting under a specific prompt-norm restriction.

From the analysis in Theorem 2 and 3, we note that the limitation of prompt-tuning primarily arises from the correlation across different inputs. Broadly describing, prompt-tuning implements transformation on different inputs via "additional attention values", which is more restrictive as compared to the transformations from MLP layers on input tokens. An interesting potential direction to improve prompt-tuning is: "designing a mechanism to leverage prompting in order to generate prompt-dependent adapter/LoRA updates". We expect to have some future work focusing on designing novel prompt-tuning strategies along this direction.

LimitationsWhile our results provide valuable insights, extending the construction in Theorem 2 to multiple layers and deriving tighter bounds for Lemma 6 are critical steps for a deeper understanding of the limitations of prompt tuning.