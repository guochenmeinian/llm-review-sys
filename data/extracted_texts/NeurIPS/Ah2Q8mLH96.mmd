# Optimal Excess Risk Bounds for Empirical Risk Minimization on \(p\)-Norm Linear Regression

Ayoub El Hanchi

University of Toronto & Vector Institute

aelhan@cs.toronto.edu

&Murat A. Erdogdu

University of Toronto & Vector Institute

erdogdu@cs.toronto.edu

###### Abstract

We study the performance of empirical risk minimization on the \(p\)-norm linear regression problem for \(p(1,)\). We show that, in the realizable case, under no moment assumptions, and up to a distribution-dependent constant, \(O(d)\) samples are enough to exactly recover the target. Otherwise, for \(p[2,)\), and under weak moment assumptions on the target and the covariates, we prove a high probability excess risk bound on the empirical risk minimizer whose leading term matches, up to a constant that depends only on \(p\), the asymptotically exact rate. We extend this result to the case \(p(1,2)\) under mild assumptions that guarantee the existence of the Hessian of the risk at its minimizer.

## 1 Introduction

Real-valued linear prediction is a fundamental problem in machine learning. Traditionally, the square loss has been the default choice for this problem. The performance of empirical risk minimization (ERM) on linear regression under the square loss, as measured by the excess risk, has been studied extensively both from an asymptotic  and a non-asymptotic point of view . An achievement of the last decade has been the development of non-asymptotic excess risk bounds for ERM on this problem under weak assumptions, and which match, up to constant factors, the asymptotically exact rate.

In this paper, we consider the more general family of \(p\)-th power losses \(t|t|^{p}\) for a user-chosen \(p(1,)\). Under mild assumptions, the classical asymptotic theory can still be applied to ERM under these losses, yielding the asymptotic distribution of the excess risk. However, to the best of our knowledge, the problem of deriving non-asymptotic excess risk bounds for ERM for \(p(1,)\{2\}\) remains open, and, as we discuss below, resists the application of standard tools from the literature.

Our motivation for extending the case \(p=2\) to \(p(1,)\) is twofold. Firstly, the freedom in the choice of \(p\) allows us to better capture our prediction goals. For example, we might only care about how accurate our prediction is on average, in which case, the choice \(p=1\) is appropriate. At the other extreme, we might insist that we do as well as possible on a subset of inputs of probability \(1\), in which case the choice \(p=\) is best. A choice of \(p(1,)\) therefore allows us to interpolate between these two extremes, with the case \(p=2\) offering a balanced choice. Secondly, different choices of \(p\) have complementary qualities. On the one hand, small values of \(p\) allow us to operate with weak moment assumptions, making them applicable in more general cases. On the other, larger values of \(p\) yield predictions whose optimality is less sensitive to changes in the underlying distribution: for \(p=\), the best predictor depends only on the support of this distribution.

To sharpen our discussion, let us briefly formalize our problem. There is an input random vector \(X^{d}\) and output random variable \(Y\), and we are provided with \(n\) i.i.d. samples \((X_{i},Y_{i})_{i=1}^{n}\). We select our set of predictors to be the class of linear functions \(\{x w,x w^{d}\}\), and choose a value \(p(1,)\) with the corresponding loss \(_{p}(t):=|t|^{p}/[p(p-1)]\). Using this loss, we definethe associated risk and empirical risk, respectively, by

\[R_{p}(w)[_{p}( w,X-Y)], R_{p,n}(w)_{i=1}^{n}_{p}( w,X_{i} -Y_{i}).\]

We perform empirical risk minimization \(_{p}*{argmin}_{w^{d}}R_{p,n}(w)\), and our goal is to derive high probability bounds on the excess risk \(R_{p}(_{p})-R_{p}(w_{p}^{*})\), where \(w_{p}^{*}\) is the risk minimizer. For efficient algorithms for computing an empirical risk minimizer \(_{p}\), we refer the reader to the rich recent literature dealing with this problem .

To see why the problem we are considering is difficult, let us briefly review some of the recent literature. Most closely related to our problem are the results of , who derive high probability non-asymptotic excess risk bounds for the case \(p=2\). The best such bounds are found in Oliveira  and Lecue and Mendelson , who both operate under weak assumptions on \((X,Y)\), requiring at most the existence of fourth moments of \(Y\) and the components \(X^{j}\) of \(X\) for \(j[d]\). Unfortunately, the analysis in Oliveira  relies on the closed form expression of the empirical risk minimizer \(_{2}\), and therefore cannot be extended to other values of \(p\). Similarly, the analysis in Lecue and Mendelson  relies on an exact decomposition of the excess loss \(_{2}( w,X-Y)-_{2}( w_{p}^{*},X-Y)\) in terms of "quadratic" and "multiplier" components, which also does not extend to other values of \(p\).

To address these limitations, the work of Mendelson  extends the ideas of Mendelson  and Lecue and Mendelson  to work for loss functions more general than the square loss. Roughly speaking, the main result of Mendelson  states that as long as the loss is strongly convex and smooth in a neighbourhood of \(0\), the techniques developed by Mendelson  can still be applied to obtain high probability excess risk bounds. Unfortunately, the loss functions \(_{p}(t)\) are particularly ill-behaved in precisely this sense, as \(_{p}^{}(t) 0\) when \(t 0\) for \(p>2\), and \(|_{p}^{}(t)|\) as \(t 0\) for \(p(1,2)\). This makes the analysis of the excess risk of ERM in the case \(p(1,)\{2\}\) particularly challenging using well-established methods.

Contrary to the non-asymptotic regime, the asymptotic properties of the excess risk of ERM under the losses \(_{p}(t)\) are better understood , and can be derived from the more general classical asymptotic theory of \(M\)-estimators  under mild regularity conditions. In particular, these asymptotic results imply that the excess risk of ERM with \(n\) samples satisfies

\[[R_{p}(_{p})]-R_{p}(w_{p}^{*})=\| _{p}( w_{p}^{*},X-Y)\|_{H_{p}^{-1}}^{2}}{2n}+o  n,\] (1)

where \(H_{p}:=^{2}R_{p}(w_{p}^{*})\) is the Hessian of the risk at its minimizer. We refer the reader to the discussions in Ostrovskii and Bach  and Mourtada and Gaiffas  for more details. As we demonstrate in Theorem 1, the rate of convergence of ERM for the square loss derived in Oliveira  and Lecue and Mendelson  matches the asymptotic rate (1) up to a constant factor. Ideally, we would like our excess risk bounds for the cases \(p(1,)\{2\}\) to also match the asymptotic rate (1), although it is not yet clear how to derive any meaningful such bounds.

In this paper, we prove the first high probability non-asymptotic excess risk bounds for ERM under the \(p\)-th power losses \(_{p}(t)\) for any \(p(1,)\{2\}\). Our assumptions on \((X,Y)\) are weak, arise naturally from the analysis, and reduce to the standard ones for the case \(p=2\). Furthermore, the rate we derive matches, up to a constant that depends only on \(p\), the asymptotically exact rate (1).

We split the analysis in three cases. The first is when the problem is realizable, i.e. \(Y= w^{*},X\) for some \(w^{*}^{d}\). This edge case is not problematic for the analysis of the case \(p=2\), but as discussed above, the \(_{p}(t)\) losses are ill-behaved around \(0\) for \(p(1,)\{2\}\), requiring us to treat this case separately. The second case is when the problem is not realizable and \(p(2,)\). The final case is when the problem is not realizable and \(p(1,2)\), which turns out to be the most technically challenging. In Section 2, we present our main results and in Section 3, we provide their proofs.

**Notation.** We denote the components of the random vector \(X^{d}\) by \(X^{j}\) for \(j[d]\). We assume the support of \(X\) is not contained in any hyperplane, i.e. \(( w,X=0)=1\) only if \(w=0\). This is without loss of generality as discussed in Oliveira  and Mourtada . For a positive semi-definite matrix \(A\), we denote the bilinear form it induces on \(^{d}\) by \(,_{A}\), and define \(\|\|_{A}:=}\). We define \(H_{p,n}:=^{2}R_{p,n}(w_{p}^{*})\).

Main results

In this section, we state our main results. We start in Section 2.1 where we introduce constants that help us formulate our theorems. In Section 2.2, we state the best known results for both the case \(p=2\) and the realizable case where \(Y= w^{*},X\). Finally, in Section 2.3, we state our theorems.

### Norm equivalence and small ball constants

To state our results, we will need to define two types of quantities first. The first kind are related to norms and their equivalence constants, which we will use in the analysis of the non-realizable case. The second are small ball probabilities, which we will use in the analysis of the realizable case.

We start by introducing the following functions on our space of coefficients \(^{d}\). For \(p,q[1,)\), define, with the convention \(^{1/p}:=\) for all \(p[1,)\),

\[\|w\|_{L^{p}}:=[| w,X|^{p}]^{1/p}, \|w\|_{L^{q},p}:=[\|w\|_{^{2}_{p}( w_{p}^{*},X -Y)}^{q}]^{1/q}.\] (2)

As suggested by the notation, under appropriate moment assumptions on \(X\), these functions are indeed norms on \(^{d}\). In that case, we will be interested in norm equivalence constants between them

\[C_{a b}:=_{w^{d}\{0\}}}{\|w\|_{b}}, _{p}^{2}:=C_{(L^{4},p)(L^{2},p)}^{4},\] (3)

where \(a\) and \(b\) stand for one of \(L^{p}\) or \((L^{q},p)\). Let us note that since we work in a finite dimensional vector space, all norms are equivalent, so that as soon as the quantities defined in (2) are indeed norms, the constants defined in (3) are finite. Furthermore, as suggested by the notation, \(_{p}^{2}\) may be viewed as the maximum second moment of the random variables \(\|w\|_{^{2}_{p}( w_{p}^{*},X-Y)}^{2}\) over the unit sphere of \(\|\|_{L^{2},p}\). Finally, we record the following identities for future use

\[\|w\|_{L^{2},p}=\|w\|_{H_{p}}, \|w\|_{L^{q},2}=\|w\|_{L^{q}}, _{2}^{2}=C_{L^{4},L^{2}}^{4}.\] (4)

The first identity holds by linearity, and the second by noticing that \(^{2}_{2}( w,X-Y)=XX^{T}\).

We now turn to small ball probabilities. We define the following functions on \(^{d}\), for \(q[1,)\),

\[_{0}(w):=( w,X=0), _{q}(w,):=(| w,X|>\|w\|_{L ^{q}}).\] (5)

Assumptions on the functions \(_{0}\) and \(_{2}\) have been used extensively in the recent literature, see e.g. . In particular, a standard assumption postulates the existence of strictly positive constants \(_{0}\), and \((_{2},_{2})\) such that \(_{0}(w) 1-_{0}\) and \(_{2}(w,_{2})_{2}\) for all \(w^{d}\). Conditions of this type are usually referred to as small ball conditions. Efforts have been made to understand when these conditions hold  as well as reveal the dimension dependence of the constants with which they do . Here we prove that such conditions always hold for finite dimensional spaces. We leave the proof of Lemma 1 to Appendix B to not distract from our main development.

**Lemma 1**.: \(_{0}\) _is upper semi-continuous. Furthermore, if for some \(q[1,)\), \([|X^{j}|^{q}]<\) for all \(j[d]\), then \(_{q}(,)\) is lower semi-continuous for any \( 0\). Moreover, for all \([0,1)\)_

\[:=_{w^{d}\{0\}}_{0}(w)<1, _{w^{d}\{0\}}_{q}(w,)>0.\]

### Background

To better contextualize our results, we start by stating the best known high probability bound on ERM for the square loss, which we deduce from Oliveira  and Lecue and Mendelson .

**Theorem 1** (Theorem 4.2, Oliveira ; Theorem 1.3, Lecue and Mendelson ).: _Assume that \([Y^{2}]<\) and \([(X^{j})^{4}]<\) for all \(j[d]\), and let \((0,1]\). If_

\[n 196_{2}^{2}(d+2(4/)),\]

_then, with probability at least \(1-\)_

\[R_{2}(_{2})-R_{2}(w_{2}^{*})[\|_{2}(  w_{2}^{*},X-Y)\|_{H_{2}^{-1}}^{2}]}{n}.\]Up to a constant factor and the dependence on \(\), Theorem 1 recovers the asymptotically exact rate (1). Let us briefly comment on the differences between Theorem 1 and the comparable statements in the original papers. First, the finiteness of \(_{2}^{2}\) is deduced from the finiteness of the fourth moments of the components of \(X\), instead of being assumed as in Oliveira  (see the discussion in Section 3.1 in Oliveira ). Second we combine Theorem 3.1 from  with the proof technique of Lecue and Mendelson  to achieve a slightly better bound that the one achieved by the proof technique used in the proof of Theorem 4.2 in Oliveira , while avoiding the dependence on the small ball-constant present in the bound of Theorem 1.3 in Lecue and Mendelson , which is known to incur additional dimension dependence in some cases .

We now move to the realizable case, where \(Y= w^{*},X\) so that \(w_{p}^{*}=w^{*}\) for all \(p(1,)\). We immediately note that Theorem 1 is still applicable in this case, and ensures that we recover \(w^{*}\) exactly with no more than \(n=O(_{2}^{2}d)\) samples. However, we can do much better, while getting rid of all the moment assumptions in Theorem 1. Indeed, it is not hard to see that \(_{p} w^{*}\) only if for some \(w^{d}\{0\}\), \( w,X_{i}=0\) for all \(i[n]\) (taking \(w=_{p}-w_{*}^{*}\) works). The implicit argument in Theorem 1 then uses the pointwise bound (see Lemma B.2 in Oliveira )

\[(_{i=1}^{n}\{ w,X_{i}=0\})- ^{2}},\]

and uniformizes it over the \(L^{2}\) unit sphere in \(^{d}\), where the \(L^{2}\) norm is as defined in (2). However, we can use the much tighter bound \(^{n}\) where \(\) is as defined in Lemma 1. To the best of our knowledge, the realizable case has not been studied explicitly before in the literature. However, with the above considerations in mind, we can deduce the following result from Lecue and Mendelson , which uniformizes the pointwise bound we just discussed using a VC dimension argument.

**Theorem 2** (Corollary 2.5, Lecue and Mendelson ).: _Assume that there exists \(w^{*}^{d}\) such that \(Y= w^{*},X\). Let \((0,1]\). If_

\[n O}\]

_then for any \(p(1,)\), \(_{p}=w^{*}\) with probability at least \(1-\)._

### Results

We are now in position to state our main results. As discussed in Section 1, the \(_{p}(t)\) losses have degenerate second derivatives as \(t 0\). When the problem is realizable, the risk is not twice differentiable at its minimizer for the cases \(p(1,2)\), and is degenerate for the cases \(p(2,)\). If we want bounds of the form (1), we must exclude this case from our analysis. Our first main result is a strengthening of Theorem 2, and relies on a combinatorial argument to uniformize the pointwise estimate discussed in Section 2.2.

**Theorem 3**.: _Assume that there exists \(w^{*}^{d}\) such that \( w^{*},X=Y\). Then for all \(n d\), and for all \(p(1,)\), we have_

\[(_{p} w^{*})^{n-d+1},\]

_where \(\) is as defined in Lemma 1. Furthermore, if_

\[nO(d+(1/)/(1/))& 0<e^{-1}\\ O()& e^{-1}<e^{-1/e} \\ O()& e^{-1/e}<1\]

_then with probability at least \(1-\), \(_{p}=w^{*}\)._

Comparing Theorem 2 and Theorem 3, we see that the bound on the number of samples required to reach a confidence level \(\) in Theorem 3 is uniformly smaller than the one in Theorem 2. The proof of Theorem 3 can be found in Appendix C.

We now move to the more common non-realizable case. Our first theorem here gives a non-asymptotic bound for the excess risk of ERM under a \(p\)-th power loss for \(p(2,)\). To the best of our knowledge, no such result is known in the literature.

**Theorem 4**.: _Let \(p(2,)\) and \((0,1]\). Assume that no \(w^{d}\) satisfies \(Y= w,X\). Further, assume that \([|Y|^{p}]<\), \([|X^{j}|^{p}]<\), and \([| w_{p}^{*},X-Y|^{2(p-2)}(X^{j})^{4}]<\) for all \(j[d]\). If_

\[n 196_{p}^{2}(d+2(4/)),\]

_then with probability at least \(1-\)_

\[R_{p}(_{p})-R_{p}(w_{p}^{*})\,\|_{p}( w_{p}^{*},X-Y)\|_{H_{p}^{-1}}^{2}}{ n}\\ +(c_{p}^{2}\,\|_{p }( w_{p}^{*},X-Y)\|_{H_{p}^{-1}}^{2}}{n})^{p /2},\]

_where we used \(c_{p}\) to denote \(C_{L^{p}(L^{2},p)}\) as defined in (3)._

Up to a constant factor that depends only on \(p\) and the dependence on \(\), the bound of Theorem 4 is precisely of the form of the optimal bound (1). Indeed, as \(p>2\), the second term is \(o(1/n)\). At the level of assumptions, the finiteness of the \(p\)-th moment of \(Y\) and the components of \(X\) is necessary to ensure that the risk \(R_{p}\) is finite for all \(w^{d}\). The last assumption \([|Y- w_{p}^{*},X|^{2(p-2)}(X^{j})^{4}]<\) is a natural extension of the fourth moment assumption in Theorem 1. In fact, all three assumptions in Theorem 4 reduce to those of Theorem 1 as \(p 2\). It is worth noting that the constant \(c_{p}\) has the alternative expression \(_{w^{d}\{0\}}\{\|w\|_{L^{p}}/\|w\|_{H_{p}}\}\) by (4), i.e. it is the norm equivalence constant between the \(L^{p}\) norm and the norm induced by \(H_{p}\). Using again (4), we see that \(c_{p} 1\) as \(p 2\). As \(p\), \(c_{p}\) grows, and we suspect in a dimension dependent way. However, this does not affect the asymptotic optimality of our rate as \(c_{p}\) only enters an \(o(1/n)\) term in our bound.

We now turn to the case of \(p(1,2)\) where we need a slightly stronger version of non-realizability.

**Theorem 5**.: _Let \(p(1,2)\) and \((0,1]\). Assume that \((| w_{p}^{*},X-Y|^{2-p}>0)=1\) and \([| w_{p}^{*},X-Y|^{2(p-2)}]<\). Further, assume that \([|Y|^{p}]<\), \([(X^{j})^{2}]<\), \([| w_{p}^{*},X-Y|^{2(p-2)}(X^{j})^{4}]<\) for all \(j[d]\). If_

\[n 196_{p}^{2}(d+2(4/)),\]

_then, with probability at least \(1-\)_

\[R_{p}(_{p})-R_{p}(w_{p}^{*})\|_{p}( w_{p}^{*},X-Y)\|_{H_{p}^{-1} }^{2}}{n}\\ +\| _{p}( w_{p}^{*},X-Y)\|_{H_{p}^{-1}}^{2}_{p}^{6 -2p}c_{p}^{2-p}c_{p}^{*}}{n}^{1/(p-1)}\]

_where we used \(c_{p}^{*}\) to denote \([|Y- w_{p}^{*},X|^{2(p-2)}]\) and \(c_{p}:=H_{p}^{-1}\) where \(:=XX^{T}\)._

Just like the bounds of Theorems 1 and 4, the bound of Theorem 5 is asymptotically optimal up to a constant factor that depends only on \(p\). Indeed, since \(1<p<2\), \(1/(p-1)>1\), and the second term is \(o(1/n)\). At the level of assumptions, we have two additional conditions compared to Theorem 4. First, we require the existence of the second moment of the covariates instead of just the \(p\)-th moment. Second, we require a stronger version of non-realizability by assuming the existence of the \(2(2-p)\) negative moment of \(| w_{p}^{*},X-Y|\). In the majority of applications, an intercept variable is included as a covariate, i.e. \(X^{1}=1\), so that this negative moment assumption is already implied by the standard assumption \([|Y- w_{p}^{*},X|^{2(p-2)}(X^{j})^{4}]<\). In the rare case where an intercept variable is not included, any negative moment assumption on \(| w_{p}^{*},X-Y|\) can be used instead, at the cost of a larger factor in the \(o(1/n)\) term.

Finally, it is worth noting that for the cases \(p[1,2)\), there are situations where the asymptotic bound (1) does not hold, as the limiting distribution of the coefficients \(_{p}\) as \(n\) does not necessarily converge to a Gaussian, and depends heavily on the distribution of \( w_{p}^{*},X-Y\), see e.g. Lai and Lee  and Knight . Overall, we suspect that perhaps a slightly weaker version of our assumptions is necessary for a fast rate like (1) to hold.

Proofs

### Proof of Theorem 1

Here we give a detailed proof of Theorem 1. While the core technical result can be deduced by combining results from Oliveira  and Lecue and Mendelson , here we frame the proof in a way that makes it easy to extend to the cases \(p(1,)\), and differently from either paper. We split the proof in three steps. First notice that since the loss is a quadratic function of \(w\), we can express it exactly using a second order Taylor expansion around the minimizer \(w_{2}^{*}\)

\[_{2}( w,X-Y)-_{2}( w_{2}^{*},X-Y)= _{2}( w_{2}^{*},X-Y),w-w_{2}^{*}+\|w -w_{2}^{*}\|_{^{2}_{2}( w_{2}^{*},X-Y)}^{2}.\]

Taking empirical averages and expectations of both sides respectively shows that the excess empirical risk and excess risk also admit such an expansion

\[R_{2,n}(w)-R_{2,n}(w_{2}^{*}) = R_{2,n}(w_{2}^{*}),w-w_{2}^{*}+ \|w-w_{2}^{*}\|_{H_{2,n}}^{2},\] \[R_{2}(w)-R_{2}(w_{2}^{*}) =\|w-w_{2}^{*}\|_{H_{2}}^{2},\] (6)

where in the second equality we used that the gradient of the risk vanishes at the minimizer \(w_{2}^{*}\). Therefore, to bound the excess risk, it is sufficient to bound the norm \(\|w-w_{2}^{*}\|_{H_{2}}\). This is the goal of the second step, where we use two ideas. First, by definition, the excess empirical risk of the empirical risk minimizer satisfies the upper bound

\[R_{2,n}(_{2})-R_{2,n}(w_{2}^{*}) 0.\] (7)

Second, we use the Cauchy-Schwartz inequality to lower bound the excess empirical risk by

\[R_{2,n}(_{2})-R_{2,n}(w_{2}^{*})-\| R_{2,n}(w_{2}^{*})\|_{H_{ 2}^{-1}}\|_{2}-w_{2}^{*}\|_{H_{2}}+\|_{2}-w_{2}^{*}\| _{H_{2,n}}^{2},\] (8)

and we further lower bound it by deriving high probability bounds on the two random terms \(\| R_{2,n}(w_{2}^{*})\|_{H_{2}^{-1}}\) and \(\|_{2}-w_{2}^{*}\|_{H_{2,n}}^{2}\). The first can easily be bounded using Chebyshev's inequality and the elementary fact that the variance of the average of \(n\) i.i.d. random variables is the variance of their common distribution divided by \(n\). Here we state the result for all \(p(1,)\); the straightforward proof can be found in the Appendix D.

**Lemma 2**.: _Let \(p(1,)\). If \(p(1,2)\), let the assumptions of Theorem 5 hold. Then with probability at least \(1-/2\)_

\[\| R_{p,n}(w_{p}^{*})\|_{H_{p}^{-1}}\![ \|_{p}( w_{p}^{*},X-Y)\|_{H_{p}^{-1}}^{2}]\!/(n )}.\]

For the second random term \(\|_{2}-w_{2}^{*}\|_{H_{2,n}}^{2}\), we use Theorem 3.1 of Oliveira , which we restate here, emphasizing that the existence of fourth moments of the components of the random vector is enough to ensure the existence of the needed norm equivalence constant.

**Proposition 1** (Theorem 3.1, Oliveira ).: _Let \(Z^{d}\) be a random vector satisfying \([Z_{j}^{4}]<\) for all \(j[d]\) and assume that \(( v,Z=0)=1\) only if \(v=0\). For \(p[1,)\) and \(v^{d}\), define_

\[\|v\|_{L^{p}} :=[( v,Z)^{p}]^{1/p}, ^{2} :=(_{v^{d}\{0\}}\|v\|_{L^{4}}/\|v \|_{L^{2}})^{4}.\]

_Let \((Z_{i})_{i=1}^{n}\) be i.i.d. samples of \(Z\). Then, with probability at least \(1-\), for all \(v^{d}\),_

\[_{i=1}^{n} v,Z_{i}^{2}(1-7})\|v\|_{L^{2}}^{2}.\]

Using this result we can immediately deduce the required high probability lower bound on the second random term \(\|_{2}-w_{2}^{*}\|_{H_{2,n}}^{2}\); we leave the obvious proof to Appendix D.

**Corollary 1**.: _Under the assumptions of Theorem 1, if \(n 196_{2}^{2}(d+2(4/))\), then with probability at least \(1-/2\), for all \(w^{d}\),_

\[\|w-w_{2}^{*}\|_{H_{2,n}}^{2}\|w-w_{2}^{*}\|_{H_{2}}^{2}.\]

Combining Lemma 2, Corollary 1, and (8) yields that with probability at least \(1-\)

\[R_{2,n}(_{2})-R_{2,n}(w_{2}^{*})-\| _{p}( w_{p}^{*},X-Y)\|_{H_{p}^{-1}}^{2}/(n )}\ \|_{2}-w_{2}^{*}\|_{H_{2}}+\|_{2}-w_{2}^{*}\|_{H_{2}} ^{2}.\] (9)

Finally, combining (7) and (9) gives that with probability at least \(1-\)

\[\|_{2}-w_{2}^{*}\|_{H_{2}} 4\| _{p}( w_{p}^{*},X-Y)\|_{H_{p}^{-1}}^{2}/(n )}.\]

Replacing in (6) finishes the proof. 

### Proof Sketch of Theorem 4

The main challenge in moving from the case \(p=2\) to the case \(p(2,)\) is that the second order Taylor expansion of the loss is no longer exact. The standard way to deal with this problem is to assume that the loss is upper and lower bounded by quadratic functions, i.e. that it is smooth and strongly convex. Unfortunately, as discussed in Section 1, the \(_{p}\) loss is not strongly convex for any \(p>2\), so we need to find another way to deal with this issue. Once this has been resolved however, the strategy we used in the proof of Theorem 1 can be applied almost verbatim to yield the result. Remarkably, a result of  allows us to upper and lower bound the \(p\)-th power loss for \(p(2,)\) by its second order Taylor expansion around a point, up to some residual terms. An application of this result yields the following Lemma.

**Lemma 3**.: _Let \(p(2,)\). Then:_

\[R_{p,n}(w)-R_{p,n}(w_{p}^{*}) \|w-w_{p}^{*}\|_{H_{p,n}}^{2}+ R _{p,n}(w_{p}^{*}),w-w_{p}^{*},\] (10) \[R_{p}(w)-R_{p}(w_{p}^{*}) \|w-w_{p}^{*}\|_{H_{p}}^{2}+p^{p}\|w-w_{p}^{*} \|_{L^{p}}^{p}.\] (11)

Up to constant factors that depend only on \(p\) and an \(L^{p}\) norm residual term, Lemma 3 gives matching upper and lower bounds on the excess risk and excess empirical risk in terms of their second order Taylor expansions around the minimizer. We can thus use the approach taken in the proof of Theorem 1 to obtain the result. The only additional challenge is the control of the term \(\|_{p}-w_{p}^{*}\|_{L^{p}}\), which we achieve by reducing it to an \(\|_{p}-w_{p}^{*}\|_{H_{p}}\) term using norm equivalence. A detailed proof of Theorem 4, including the proof of Lemma 3, can be found in Appendix E.

### Proof Sketch of Theorem 5

The most technically challenging case is when \(p(1,2)\). Indeed as seen in the proof of Theorem 1, the most involved step is lower bounding the excess empirical risk with high probability. For the case \(p[2,)\), we achieved this by having access to a pointwise quadratic lower bound, which is not too surprising. Indeed, at small scales, we expect the second order Taylor expansion to be accurate, while at large scales, we expect the \(p\)-th power loss to grow at least quadratically for \(p[2,)\).

In the case of \(p(1,2)\), we are faced with a harder problem. Indeed, as \(p 1\), the \(_{p}\) losses behave almost linearly at large scales. This means that we cannot expect to obtain a global quadratic lower bound as for the case \(p[2,)\), so we will need a different proof technique. Motivated by related concerns, Bubeck, Cohen, Lee, and Li  introduced the following approximation to the \(p\)-th power function

\[_{p}(t,x):=t^{p-2}x^{2}& x t\\ x^{p}-1-t^{p}& x>t,\]for \(t,x[0,)\) and with \(_{p}(0,0)=0\). This function was further studied by Adil, Kyng, Peng, and Sachdeva , who showed in particular that for any \(t\), the function \(x_{p}(|t|,|x|)\) is, up to constants that depend only on \(p\), equal to the gap between the function \(x_{p}(t+x)\) and its linearization around \(0\); see Lemma 4.5 in  for the precise statement. We use this result to derive the following Lemma.

**Lemma 4**.: _Let \(p(1,2)\). Under the assumptions of Theorem 5, we have_

\[R_{p,n}(w)-R_{p,n}(w_{p}^{*}) }_{i=1}^{n}_{p}|  w_{p}^{*},X_{i}-Y_{i}|,| w-w_{p}^{*},X_{i}| + R_{p,n}(w_{p}^{*}),w-w_{p}^{*},\] (12) \[R_{p}(w)-R_{p}(w_{p}^{*}) \|w-w_{p}^{*}\|_{H_{p}}^{2}.\] (13)

As expected, while we do have the desired quadratic upper bound, the lower bound is much more cumbersome, and is only comparable to the second order Taylor expansion when \(| w-w_{p}^{*},X_{i}|| w_{p}^{*},X_{i}-Y_{i}|\). What we need for the proof to go through is a high probability lower bound of order \((\|w-w^{*}\|_{H_{p}}^{2})\) on the first term in the lower bound (12). We obtain this in the following Proposition.

**Proposition 2**.: _Let \((0,1]\). Under the assumptions of Theorem 5, if \(n 196_{p}^{2}(d+2(4/))\), then with probability at least \(1-/2\), for all \(w^{d}\),_

\[_{i=1}^{n}_{p}| w_{p}^{*},X_{i}-Y_{ i}|,| w-w_{p}^{*},X_{i}| \|w-w_{p}^{*}\|_{H_{p}}^{2},^{2-p}\|w- w_{p}^{*}\|_{H_{p}}^{p}},\]

_where \(^{p-2}:=8_{p}^{3-p}c_{p}^{(2-p)/2}^{*}}\) and \(c_{p}\) and \(c_{p}^{*}\) are as defined in Theorem 5._

Proof.: Let \(>0\) and let \(T(0,)\) be a truncation parameter we will set later. Define

\[:=X_{[0,T]}(\|X\|_{H_{p}^{-1}}),\]

and the constant \(:=T\). By Lemma 3.3 in , we have that \(_{p}(t, x)\{^{2},^{p}\}_{p}(t,x)\) for all \( 0\). Furthermore, it is straightforward to verify that \(_{p}(t,x)\) is decreasing in \(t\) and increasing in \(x\). Therefore, we have, for all \(w^{d}\),

\[_{i=1}^{n}_{p}| w_{p}^{*},X_{i }-Y_{i}|,| w-w_{p}^{*},X_{i}|\] \[^{-2}\|w-w_{p}\|_{H_{p}}^{2}, ^{-p}\|w-w_{p}\|_{H_{p}}^{p}}_{\|w\|_{H_{p}}=e} _{i=1}^{n}_{p}| w_{p}^{*},X_{i}-Y_{ i}|,| w,X_{i}|.\] (14)

The key idea to control the infimum in (14) is to truncate \( w,X_{i}\) from above by using the truncated vector \(\), and \(| w_{p}^{*},X_{i}-Y_{i}|\) from below by forcing it to be greater than \(\). By the monotonicity properties of \(_{p}\) discussed above, we get that

\[_{\|w\|_{H_{p}}=e}_{i=1}^{n}_{p}|  w_{p}^{*},X_{i}-Y_{i}|,| w,X_{i}|\] \[_{\|w\|_{H_{p}}=e}_{i=1}^{n}_{p}( | w_{p}^{*},X_{i}-Y_{i}|,},| w, _{i}|)\] \[=p}{2}_{\|w\|_{H_{p}}=1}_ {i=1}^{n}| w_{p}^{*},X_{i}-Y_{i}|,}^{p- 2}| w,_{i}|^{2},\] (15)where the equality follows by the fact that with the chosen truncations, the second argument of \(_{p}\) is less than or equal to the first. It remains to lower bound the infimum in (15). Define

\[Z=| w_{p}^{*},X-Y|,}^{(p-2)/2}.\]

Removing the truncations and using our assumptions, we see that the components of \(Z\) have finite fourth moment. By Proposition 1 and the condition on \(n\), we get that with probability at least \(1-/2\),

\[_{\|w\|_{H_{p}}=1}_{i=1}^{n}|  w_{p}^{*},X_{i}-Y_{i}|,}^{p-2}| w, _{i}|^{2}\] \[=_{\|w\|_{H_{p}}=1}_{i=1}^{n} w,Z_{i} ^{2}_{\|w\|_{H_{p}}=1} w,Z ^{2}\] \[=_{\|w\|_{H_{p}}=1} | w_{p}^{*},X-Y|,}^{(p-2)} w, ^{2}\] \[1-_{\|w\|_{H_{p}}=1} | w_{p}^{*},X-Y|^{p-2} w,X^{2}_ {[0,)}(| w_{p}^{*},X-Y|)+_{(T,)}(\|X\|_{H_{ p}^{-1}})\] (16)

We now bound the supremum in (16). We have

\[_{\|w\|_{H_{p}}=1}| w_{p}^{*},X -Y|^{p-2} w,X^{2}_{[0,)}(| w_{p}^ {*},X-Y|)+_{(T,)}(\|X\|_{H_{p}^{-1}})\] \[_{\|w\|_{H_{p}}=1}| w_{ p}^{*},X-Y|^{2(p-2)} w,X^{4}^{1/2}} | w_{p}^{*},X-Y|<+ \|X\|_{H_{p}^{-1}}>T^{1/2}\] \[=_{p}| w_{p}^{*},X- Y|<+\|X\|_{H_{p}^{-1}}>T^{1/2},\] (17)

where the first inequality follows from Cauchy-Schwartz inequality, and the subsequent equalities by definitions of \(\|\|_{L^{4},p}\) and \(_{p}^{2}\). It remains to bound the tail probabilities. Recall that \(=T\), so

\[| w_{p}^{*},X-Y|< =| w_{p}^{*},X-Y|<T \] \[=| w_{p}^{*},X-Y|^{-1}>(T )^{-1}\] \[=| w_{p}^{*},X-Y|^{2(p-2)}>(T )^{2(p-2)}\] \[[| w_{p}^{*},X-Y|^{2(p-2)}](T )^{2(2-p)}\] \[=c_{p}^{*}(T)^{2(2-p)},\]

where we applied Markov's inequality in the fourth line, and the last follows by definition of \(c_{p}^{*}\) in Theorem 5. Moreover by the finiteness of the second moment of the coordinates \(X^{j}\) of \(X\), we have

where \(=[XX^{T}]\), and the last equality by definition of \(c_{p}\) in Theorem 5. By Markov's inequality

\[| w_{p}^{*},X-Y|<T+ \|X\|_{H_{p}^{-1}}>T c_{p}^{*}T^{2(2-p)} ^{2(2-p)}+}{T^{2}}.\]

Choosing

\[T:=}{c_{p}^{*}(2-p)}^{1/(6-2p)}, ^{2-p}:=^{3-p}^{*} c_{p}^{(2-p) /2}}},\]

ensures that

\[_{p}| w_{p}^{*},X-Y|<T^{*} +\|X\|_{H_{p}^{-1}}>T^{*} ^{1/2} 1/2.\] (18)

Combining the inequalities (18), (17), (16), (15), and (14) yields the result. 

A detailed proof of Theorem 5, including the proof of Lemma 4, can be found in Appendix F.