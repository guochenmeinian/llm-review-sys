# Gradient Flossing: Improving Gradient Descent

through Dynamic Control of Jacobians

 Rainer Engelken

Zuckerman Mind Brain Behavior Institute

Columbia University

New York, USA

re2365@columbia.edu

###### Abstract

Training recurrent neural networks (RNNs) remains a challenge due to the instability of gradients across long time horizons, which can lead to exploding and vanishing gradients. Recent research has linked these problems to the values of Lyapunov exponents for the forward-dynamics, which describe the growth or shrinkage of infinitesimal perturbations. Here, we propose _gradient flossing_, a novel approach to tackling gradient instability by pushing Lyapunov exponents of the forward dynamics toward zero during learning. We achieve this by regularizing Lyapunov exponents through backpropagation using differentiable linear algebra. This enables us to "floss" the gradients, stabilizing them and thus improving network training. We demonstrate that _gradient flossing_ controls not only the gradient norm but also the condition number of the long-term Jacobian, facilitating multidimensional error feedback propagation. We find that applying _gradient flossing_ prior to training enhances both the success rate and convergence speed for tasks involving long time horizons. For challenging tasks, we show that _gradient flossing_ during training can further increase the time horizon that can be bridged by backpropagation through time. Moreover, we demonstrate the effectiveness of our approach on various RNN architectures and tasks of variable temporal complexity. Additionally, we provide a simple implementation of our _gradient flossing_ algorithm that can be used in practice. Our results indicate that _gradient flossing_ via regularizing Lyapunov exponents can significantly enhance the effectiveness of RNN training and mitigate the exploding and vanishing gradients problem.

## 1 Introduction

Recurrent neural networks are commonly used both in machine learning and computational neuroscience for tasks that involve input-to-output mappings over sequences and dynamic trajectories. Training is often achieved through gradient descent by the backpropagation of error information across time steps [1; 2; 3; 4]. This amounts to unrolling the network dynamics in time and recursively applying the chain rule to calculate the gradient of the loss with respect to the network parameters. Mathematically, evaluating the product of Jacobians of the recurrent state update describes how error signals travel across time steps. When trained on tasks that have long-range temporal dependencies, recurrent neural networks are prone to exploding and vanishing gradients [5; 6; 7; 8]. These arise from the exponential amplification or attenuation of recursive derivatives of recurrent network states over many time steps. Intuitively, to evaluate how an output error depends on a small parameter change at a much earlier point in time, the error information has to be propagated through the recurrent network states iteratively. Mathematically, this corresponds to a product of Jacobians that describe how changes in one recurrent network state depend on changes in the previous network state. Together, this product forms the long-term Jacobian. The singular value spectrum of the long-term Jacobian regulates how well error signals can propagate backwards along multiple time steps, allowing temporal credit assignment. A close mathematical correspondence of these singular values and the Lyapunov exponents of the forward dynamics was established recently [9; 10; 11; 12]. Lyapunov exponents characterize the asymptotic average rate of exponential divergence or convergence of nearby initial conditions and are a cornerstone of dynamical systems theory [13; 14]. We will use this link to improve the trainability of RNNs.

Previous approaches that tackled the problem of exploding or vanishing gradients have suggested solutions at different levels. First, specialized units such as LSTM and GRU were introduced, which have additional latent variables that can be decoupled from the recurrent network states via multiplicative (gating) interactions. The gating interactions shield the latent memory state, which can therefore transport information across multiple time steps [5; 6; 15]. Second, exploding gradients can be avoided by gradient clipping, which re-scales the gradient norm  or their individual elements  if they become too large . Third, normalization schemes like batch normalization, layer norm and group norm prevent saturated nonlinearities that contribute to vanishing gradients [19; 20; 21]. Fourth, it was suggested that the problem of exploding/vanishing gradients can be ameliorated by specialized network architectures, for example, antisymmetric networks , orthogonal/unitary initializations [23; 24; 25], coupled oscillatory RNNs , Lipschitz RNNs , structured state space models [28; 29; 30; 31], echo state networks [32; 33], (recurrent) highway networks [34; 35], and stable limit cycle neural networks [11; 36; 37]. Fifth, for large networks, a suitable choice of weights can guarantee a well-conditioned Jacobian at initialization [23; 38; 39; 40; 41; 42; 43; 44; 45; 46; 47]. These initializations are based on mean-field methods, which become exact only in the large-network limit. Such initialization schemes have also been suggested for gated networks . However, even when initializing the network with well-behaved gradients, gradients will typically not retain their stability during training once the network parameters have changed.

Here, we propose a novel approach to tackling this challenge by introducing _gradient flossing_, a technique that keeps gradients well-behaved throughout training. _Gradient flossing_ is based on a recently described link between the gradients of backpropagation through time and Lyapunov exponents, which are the time-averaged logarithms of the singular values of the long-term Jacobian [9; 11; 12; 37]. _Gradient flossing_ regularizes one or several Lyapunov exponents to keep them close to zero during training. This improves not only the error gradient norm but also the condition number of the long-term Jacobian. As a result, error signals can be propagated back over longer time horizons. We first demonstrate that the Lyapunov exponents can be controlled during training by including an additional loss term. We then demonstrate that _gradient flossing_ improves the gradient norm and effective dimension of the gradient signal. We find empirically that _gradient flossing_ improves test accuracy and convergence speed on synthetic tasks over a range of temporal complexities. Finally, we find that _gradient flossing_ during training further helps to bridge long-time horizons and show that it combines well with other approaches to ameliorate exploding and vanishing gradients, such as dynamic mean-field theory for initialization, orthogonal initialization and gated units.

Our contributions include:

* _Gradient flossing_, a novel approach to the problem of exploding and vanishing gradients in recurrent neural networks based on regularization of Lyapunov exponents. 1 * Analytical estimates of the condition number of the long-term Jacobian based on Lyapunov exponents.
* Empirical evidence that _gradient flossing_ improves training on tasks that involve bridging long time horizons.

## 2 RNN Gradients and Lyapunov Exponents

We begin by revisiting the established mathematical relationship between the gradients of the loss function, computed via backpropagation through time, and Lyapunov exponents [9; 12], and how it relates to the problem of vanishing and exploding gradients. In backpropagation through time, network parameters \(\) are iteratively updated by stochastic gradient descent such that a loss \(L_{t}\) is locally reduced [1; 2; 3; 4]. For RNN dynamics \(_{s+1}=_{}(_{s},_{s+1})\), with recurrent network state \(\), external input \(\), and parameters \(\), the gradient of the loss \(_{t}\) with respect to \(\) is evaluated by unrolling the network dynamics in time. The resulting expression for the gradient is given by:

\[_{t}}{}=_{t}}{ _{t}}_{=t-l}^{=t-1}(_{^{}= }^{t-1}_{^{}+1}}{_{ ^{}}})_{}}{}=_{t}}{_{t}}_{}_{t}( _{})_{}}{}\] (1)

where \(_{t}(_{})\) is composed of a product of one-step Jacobians \(_{s}=_{t+1}}{_{s}}\):

\[_{t}(_{})=_{^{}=}^{t-1}_{^{}+1}}{_{^{}}}= _{^{}=}^{t-1}_{^{}}\] (2)

Due to the chain of matrix multiplications in \(_{t}\), the gradients tend to vanish or explode exponentially with time. This complicates training particularly when the task loss at time \(t\) dependents on inputs \(\) or states \(\) from many time steps prior which creates long temporal dependencies [5; 6; 7; 8]. How well error signals can propagate back in time is constrained by the tangent space dynamics along trajectory \(_{t}\), which dictate how local perturbations around each point on the trajectory stretch, rotate, shear, or compress as the system evolves.

The singular values of the Jacobian's product \(_{t}\), which determine how quickly gradients vanish or explode during backpropagation through time, are directly related to the Lyapunov exponents of the forward dynamics [9; 12]: Lyapunov exponents \(_{1}_{2}_{N}\) are defined as the asymptotic time-averaged logarithms of the singular values of the long-term Jacobian [13; 48; 49]

\[_{i}=_{t}(_{i,t})\] (3)

where \(_{i,t}\) denotes the \(i\)th singular value of \(_{t}(_{})\) with \(_{1,t}_{2,t}_{N,t}\) (See Appendix I for details). This means that positive Lyapunov exponents in the forward dynamics correspond to exponentially exploding gradient modes, while negative Lyapunov exponents in the forward dynamics correspond to exponentially vanishing gradient modes.

In summary, the Lyapunov exponents give the average asymptotic exponential growth rates of infinitesimal perturbations in the tangent space of the forward dynamics, which also constrain the signal propagation in backpropagation for long time horizons. Lyapunov exponents close to zero in the forward dynamics correspond to tangent space directions along which error signals are neither drastically attenuated nor amplified in backpropagation through time. Such close-to-neutral modes in the tangent dynamics can propagate information reliably across many time steps.

## 3 Gradient Flossing: Idea and Algorithm

We now leverage the mathematical connection established between Lyapunov exponents and the prevalent issue of exploding and vanishing gradients for regularizing the singular values of the long-term Jacobian. We term this procedure _gradient flossing_. To prevent exploding and vanishing gradients, we constrain Lyapunov exponents to be close to zero. This ensures that the corresponding directions in tangent space grow and shrink on average only slowly. This leads to a better-conditioned long-term Jacobian \(_{t}(_{})\). We achieve this by using the sum of the squares of the first \(k\) largest Lyapunov exponent \(_{1},_{2}_{k}\) as a loss function:

\[_{}=_{i=1}^{k}_{i}^{2}\] (4)

and evaluate the gradient obtained from backpropagation through time:

\[_{}}{}=_{i=1}^{k} ^{2}}{}\] (5)

This might seem like an ill-fated enterprise, as the gradient expression in Eq 5 suffers from its own problem of exploding and vanishing gradients. However, instead of calculating the Lyapunov exponents by directly evaluating the long-term Jacobian \(_{t}\) (Eq 2), we use an established iterative reorthonormalization method involving QR decomposition that avoids directly evaluating the ill-conditioned long-term Jacobian [12; 50].

First, we evolve an initially orthonormal system \(_{s}=[_{s}^{1},\,_{s}^{2},_{s}^{k}]\) in the tangent space along the trajectory using the Jacobian \(_{s}=_{s+1}}{_{s}}\). This means to calculate

\[}_{s+1}=_{s}_{s}\] (6)

at every time-step. Second, we extract the exponential growth rates using the QR decomposition,

\[}_{s+1}=_{s+1}^{s+1},\]

which decomposes \(}_{s+1}\) uniquely into the product of an orthonormal matrix \(_{s+1}\) of size \(N k\) so \(_{s+1}^{}_{s+1}=_{k k}\) and an upper triangular matrix \(^{s+1}\) of size \(k k\) with positive diagonal elements. Note that the QR decomposition does not have to be applied at every step, just sufficiently often, i.e., once every \(t_{}\) such that \(}\) does not become ill-conditioned.

The Lyapunov exponents are given by time-averaged logarithms of the diagonal entries of \(^{s}\)[49; 50]:

\[_{i}=_{t}_{s=1}^{t}_{ii}^{ s}=_{t}_{s=1}^{t}_{ii}^{s}.\] (7)

This way, the Lyapunov exponent can be expressed in terms of a temporal average over the diagonal elements of the \(^{s}\)-matrix of a QR decomposition of the iterated Jacobian. To propagate the gradient of the square of the Lyapunov exponents backward through time in _gradient flossing_, we used an analytical expression for the pullback of the QR decomposition : The backward pass of the QR decomposition is given by [51; 52; 53; 54]

\[}=[}+( )]^{-T},\] (8)

where \(=}^{T}-}^{T} \) and the copyltu function generates a symmetric matrix by copying the lower triangle of the input matrix to its upper triangle, with the element \([(M)]_{ij}=M_{(i,j),(i,j)}\)[51; 52; 53; 54]. We denote here _adjoint variable_ as \(=/ T\). A simple implementation of this algorithm in pseudocode is:

``` initialize \(\), \(\) for\(e=1 E\)do for\(t=1 T\)do \(_{}(,)\) \(_{s}}{_{s }-1}\) \(\) if\(t}}\)then \(,()\) \(_{i}\)\(+\)\(=(R_{ii})\) endif endfor \(_{i}=_{i}/T\) \(_{e+1}_{e}-_{}}{}\) endfor ```

**Algorithm 1 Algorithm for**_gradient flossing_ of \(k\) tangent space directions

For clarity, we described _gradient flossing_ in terms of stochastic gradient descent, but we actually implemented it with the ADAM optimizer using standard hyperparameters \(\), \(_{1}\) and \(_{2}\). An example implementation in Julia  using Flux  is available here. Note that this algorithm also works for different recurrent network architectures. In this case, the Jacobians \(\) has size \(n n\), where \(n\) is the number of dynamic variables of the recurrent network model. For example, in case of a single recurrent network of \(N\) LSTM units, the Jacobian has size \(2N 2N\)[9; 12; 46]. The Jacobian matrix \(\) can either be calculated analytically or it can be obtained via automatic differentiation.

## 4 Gradient Flossing: Control of Lyapunov Exponents

In Fig 1, we demonstrate that _gradient flossing_ can set one or several Lyapunov exponents to a target value via gradient descent with the ADAM optimizer in random Vanilla RNNs initialized with different weight variances. The \(N\) units of the recurrent neural network follow the dynamics

\[_{s+1}=(_{s},_{s+1})=( _{s})+_{s+1}.\] (9)The initial entries of \(\) are drawn independently from a Gaussian distribution with zero mean and variance \(g^{2}/N\), where \(g\) is a gain parameter that controls the heterogeneity of weights. We here use the transfer function \((x)=(x)\). (See appendix B for _gradient flossing_ with ReLU and LSTM units). \(_{s}\) is a sequence of inputs and \(\) is the input weight. \(_{s}\) is a stream of i.i.d. Gaussian input \(x_{s}(0,\,1)\) and the input weights \(\) are \((0,\,1)\). Both \(\) and \(\) are trained during _gradient flossing_.

In Fig 1B, we show that for randomly initialized RNNs, the Lyapunov exponent can be modified by _gradient flossing_ to match a desired target value. The networks were initialized with 10 different values of initial weight strength \(g\) chosen uniformly between 0 and 1. During _gradient flossing_, they quickly approached three different target values of the first Lyapunov exponents \(_{1}^{}=\{-1,-0.5,0\}\) within less than 100 training epochs with batch size \(B=1\). We note that _gradient flossing_ with positive target \(_{1}^{}\) seems not to arrive at a positive Lyapunov exponent \(_{1}\).

Fig 1C shows _gradient flossing_ for different numbers of Lyapunov exponents \(k\). Here, during gradient-descent, the sum of the squares of 1, 16, or 32 Lyapunov exponents is used as loss in _gradient flossing_ (see Fig 1A). Fig 1D shows the Lyapunov spectrum after flossing, which now has 1, 16, or 32 Lyapunov exponents close to zero. We conclude that _gradient flossing_ can selectively manipulate one, several, or all Lyapunov exponents before or during network training. _Gradient flossing_ also works for RNNs of ReLU and LSTM units (See appendix B. Further, we find that the computational bottleneck of _gradient flossing_ is the QR decomposition, which has a computational complexity of \((N\,k^{2})\), both in the forward pass and in the backward pass. Thus, _gradient flossing_ of the entire Lyapunov spectrum is computationally expensive. However, as we will show, not all Lyapunov exponents need to be flossed and only short episodes of _gradient flossing_ are sufficient for significantly improving the training performance.

Figure 1: _Gradient flossing_ controls Lyapunov exponents and gradient signal propagation

**A)** Exploding and vanishing gradients in backpropagation through time arise from amplification/attenuation of product of Jacobians that form the long-term Jacobian \(_{t}(_{})=_{^{}=}^{t-1}_{^{}+1}}{_{^{}}}\).

**B)** First Lyapunov exponent of Vanilla RNN as a function of training epochs. Minimizing the mean squared error between estimated first Lyapunov exponent and target Lyapunov exponent \(_{1}=-1,-0.5,0\) by gradient descent. 10 Vanilla RNNs were initialized with Gaussian recurrent weights \(W_{ij}(0,\,g^{2}/N)\) where values of \(g\) were drawn \(g(0,1)\). **C)** _Gradient flossing_ minimizes the square of Lyapunov exponents over epochs. **D)** Full Lyapunov spectrum of Vanilla RNN after a different number of Lyapunov exponents are pushed to zero via _gradient flossing_. Note, the variability of the Lyapunov exponents that were not flossed. Parameters: network size \(N=32\) with 10 network realizations. Error bars in **C** indicate the 25% and 75% percentiles and solid line shows median.

## 5 Gradient Flossing: Condition Number of the Long-Term Jacobian

A well-conditioned Jacobian is essential for efficient and fast learning [23; 55; 56]. _Gradient flossing_ improves the condition number of the long-term Jacobian which constrains the error signal propagation across long time horizons in backpropagation (Fig 2). The condition number \(_{2}\) of a linear map \(A\) measures how close the map is to being singular and is given by the ratio of the largest singular value \(_{}\) and the smallest singular values \(_{}\), so \(_{2}(A)=}(A)}{_{}(A)}\). According to the rule of thumb given in , if \(_{2}(A)=10^{p}\), one can anticipate losing at least \(p\) digits of precision when solving the equation \(Ax=b\). Note that the long-term Jacobian \(_{t}\) is composed of a product of Jacobians, which generically makes it ill-conditioned. To nevertheless quantify the condition number numerically, we use arbitrary-precision arithmetic with 256 bits per float. We find numerically that the condition number of \(_{t}\) exponentially diverges with the number of time steps (Fig 2A). We compare the numerically measured condition number \(_{2}\) with an asymptotic approximation of the condition number based on Lyapunov exponents that are calculated in the forward pass and find a good match (Fig 2A).

Our theoretical estimate of the condition number \(_{2}\) of an orthonormal system \(\) of size \(N m\) that is temporally evolved by the long-term Jacobian \(_{t}\) is:

\[_{2}(}_{t+})=_{2}_{t}( _{})_{t}=(_{t}( _{}))}{_{m}(_{t}(_{}))} ((_{1}-_{m})(t-)).\] (10)

where \(_{1}(_{t}(_{}))\) and \(_{m}(_{t}(_{}))\) are the first and \(m\)th singular value of the long-term Jacobian. We note that this theoretical estimate of the condition number follows from the asymptotic definition of Lyapunov exponents and should be exact in the limit of long times. We find that _gradient flossing_ reduces the condition number by a factor whose magnitude increases exponentially with time (orange in Fig 2A). Thus, we can expect that _gradient flossing_ has a stronger effect on problems with a long time horizon to bridge. We will later confirm this numerically.

Moreover, Lyapunov exponents enable the estimation of the number of gradient dimensions available for the backpropagation of error signals. Generally, the long-term Jacobian is ill-conditioned, however, the Lyapunov spectrum provides for a given number of tangent space dimensions an estimate of the condition number. This indicates how close to singular the gradient signal for a given number of tangent space dimensions is. Given a fixed acceptable condition number--determined, for example, by noise level or floating-point precision--we observe that _gradient flossing_ increases the number of usable tangent space dimensions for backpropagation (Fig 2B).

Figure 2: _Gradient flossing_ reduces condition number of the long-term Jacobian \(\)) Condition number \(_{2}\) of long-term Jacobian \(_{t}(_{})\) as a function of time horizon \(t-\) at initialization (blue) and after _gradient flossing_ (orange). Direct numerical simulations are done with arbitrary precision floating point arithmetic (transparent lines) with 256 bits per float, asymptotic theory based on Lyapunov exponents (dashed lines) (Eq 10). **B)** Condition number for different number of tangent space dimensions \(m\). Simulations (dots) and Lyapunov exponent based theory (dashed lines) at initialization (blue) and after _gradient flossing_ (orange). _Gradient flossing_ increases the number of tangent space dimensions available for backpropagation for a given condition number (Grey dotted line as a guide for eye for \(_{2}=10^{5}\).) First \(15\) Lyapunov exponents were flossed. **C)** Comparison of condition number obtained via direct numerical simulations vs. Lyapunov exponent-based. Colors denote the number of flossed Lyapunov exponents \(k\). Parameters: \(g=1\), batch size \(b=1\), \(N=80\), epochs \(=500\), \(T=500\), _gradient flossing_ for \(E_{f}=500\) epochs. Input \(_{s}\) identical to delayed XOR task in Fig 3D.

Finally, we show that the asymptotic estimate of the condition number based on Lyapunov exponents can even predict differences in condition number that originate from finite network size \(N\) (Fig 2C). We emphasize that this goes beyond mean-field methods, which become exact only in the large-network limit \(N\) and usually do not capture finite-size effects  (see appendix G).

## 6 Initial _Gradient Flossing_ Improves Trainability

We next present numerical results on two tasks with variable spatial and temporal complexity, demonstrating that _gradient flossing_ before training improves the trainability of Vanilla RNNs. We call _gradient flossing_ before training in the following preflossing. For preflossing, we first initialize the network randomly, then minimize \(_{}=_{i=1}^{k}_{i}^{2}\) using the ADAM optimizer and subsequently train on the tasks. We deliberately do not use sequential MNIST or similar toy tasks commonly used to probe exploding/vanishing gradients, because we want a task where the structure of long-range dependencies in the data is transparent and can be varied as desired.

First, we consider the delayed copy task, where a scalar stream of random input numbers \(x\) must be reproduced by the output \(y\) delayed by \(d\) time steps, i.e. \(y_{t}=x_{t-d}\). Although the task itself is trivial and can be solved even by a linear network through a delay line (see appendix E), RNNs encounter vanishing gradients for large delays \(d\) during training even with 'critical' initialization with \(g=1\). Our experiments show that _gradient flossing_ can substantially improve the performance of RNNs on this task (Fig 3A, C). While Vanilla RNNs without _gradient flossing_ fail to train reliably beyond \(d=20\), Vanilla RNNs with _gradient flossing_ can be reliably trained for \(d=40\) (Fig 3C). Note that we flossed here \(k=40\) Lyapunov exponents before training. We will later investigate the role of the number of flossed Lyapunov exponents.

Second, we consider the temporal XOR task, which requires the RNN to perform a nonlinear input-output computation on a sequential stream of scalar inputs, i.e., \(y_{t}=|x_{t-d/2}-x_{t-d}|\), where \(d\) denotes a time delay of \(d\) time steps (For details see appendix H). Fig 3D demonstrates that _gradient flossing_ helps to train networks on a substantially longer delay \(d\). We found similar improvements through _gradient flossing_ for RNNs initialized with orthogonal weights (see appendix G).

Figure 3: _Gradient flossing improves trainability on tasks that involve long time horizons_**A)** Test error for Vanilla RNNs trained on delayed copy task \(y_{t}=x_{t-d}\) for \(d=40\) with and without _gradient flossing_. Solid lines are medians across 5 network realizations. **B)** Same as **A** for delayed XOR task with \(y_{t}=|x_{t-d/2}-x_{t-d}|\). **C)** Mean final test loss as a function of task difficulty (delay \(d\)) for delayed copy task. **D)** Mean final test loss as a function of task difficulty (delay \(d\)) for delayed XOR task. Parameters: \(g=1\), batch size \(b=16\), \(N=80\), epochs \(=10^{4}\), \(T=300\), _gradient flossing_ for \(E_{f}=500\) epochs on \(k=75\) before training. Shaded regions in **C** and **D** indicate the 20% and 80% percentiles and solid line shows mean. Dots are individual runs. Task loss: MSE\((y,)\).

## 7 _Gradient Flossing_ During Training

We next investigate the effects of _gradient flossing_ during the training and find that _gradient flossing_ during training can further improve trainability. We trained RNNs on two more challenging tasks with variable temporal complexity and performed _gradient flossing_ either both during and before training, only before training, or not at all.

Fig 4A shows the test accuracy for Vanilla RNNs training on the delayed temporal XOR task \(y_{t}=x_{t-d/2} x_{t-d}\) with random Bernoulli process \(x\{0,1\}\). The accuracy of Vanilla RNNs falls to chance level for \(d 40\) (Fig 4C). With _gradient flossing_ before training, the trainability can be improved, but still goes to chance level for \(d=70\). In contrast, for networks with _gradient flossing_ during training, the accuracy is improved to \(>80\%\) at \(d=70\). In this case, we prefolessed for 500 epochs before task training and again after 500 epochs of training on the task. In Fig 4B, D the networks have to perform the nonlinear XOR operation \(y_{t}=x_{t-d}^{1} x_{t-d}^{2} x_{t-d}^{3}\) on a three-dimensional binary input signal \(x^{1}\), \(x^{2}\), and \(x^{3}\) and generate the correct output with a delay of \(d\) steps. While the solution of the task itself is not difficult and could even be implemented by hand (see appendix), the task is challenging for backpropagation through time because nonlinear temporal associations bridging long time horizons have to be formed. Again, we observe that _gradient flossing_ before training improves the performance compared to baseline, but starts failing for long delays \(d>60\). In contrast, networks that are also flossed during training can solve even more difficult tasks (Fig 4D). We find that after _gradient flossing_, the norm of the error gradient with respect to initial conditions \(_{0}\) is amplified (appendix C). Interestingly, _gradient flossing_ can also be detrimental to task performance if it is continued throughout all training epochs (appendix C)

We note that merely regularizing the spectral radius of the recurrent weight matrix \(\) or the individual one-step Jacobians \(_{s}\) numerically or based on mean-field theory does not yield such a training improvement. This suggests that taking the temporal correlations between Jacobians \(_{s}\) into account is important for improving trainability.

Figure 4: _Gradient flossing_ during training further improves trainability**

**A)** Test accuracy for Vanilla RNNs trained on delayed temporal binary XOR task \(y_{t}=x_{t-d/2} x_{t-d}\) with _gradient flossing_ during training (green), preflossing (_gradient flossing_ before training) (orange), and with no _gradient flossing_ (blue) for \(d=70\). Solid lines are mean across 20 network realizations, individual network realizations shown in transparent fine lines. **B)** Same as **A** for delayed spatial XOR task with \(y_{t}=x_{t-d}^{1} x_{t-d}^{2} x_{t-d}^{3}\). Parameters (\(g=1\), batch size \(b=16\)). **C)** Test accuracy as a function of task difficulty (delay \(d\)) for delayed temporal XOR task. **D)** Test accuracy as a function of task difficulty (delay \(d\)) for delayed spatial XOR task. Parameters: \(g=1\), batch size \(b=16\), \(N=80\), epochs \(=10^{4}\), \(T=300\), _gradient flossing_ for \(E_{f}=500\) epochs on \(k=75\) before training and during training for green lines, and only before training for orange lines. Same plotting conventions as previous figure. Task loss: cross-entropy between \(y\) and \(\).

### _Gradient Flossing_ for Different Numbers of Flossed Lyapunov Exponents

We investigated how many Lyapunov exponents \(k\) have to be flossed to achieve an improvement in training success (Fig 5). We studied this in the binary temporal delayed XOR task with _gradient flossing_ during training (same as Fig 3) and varied the task difficulty by changing the delay \(d\).

We found that as the task becomes more difficult, networks where not enough Lyapunov exponents \(k\) are flossed begin to fall below 100% test accuracy (Fig 5A). Correspondingly, when measuring final test accuracy as a function of the number of flossed Lyapunov exponents, we observed that more Lyapunov exponent \(k\) have to be flossed to achieve 100% accuracy as the tasks become more difficult (Fig 5B). We also show the entire parameter plane of median test accuracy as a function of both number of flossed Lyapunov exponents \(k\) and task difficulty (delay \(d\)), and found the same trend (Fig 5B). Overall, we found that tasks with larger delay \(d\) require more Lyapunov exponents close to zero. We note that this might also partially be caused by the'streaming' nature of the task: in our tasks, longer delays automatically imply that more values have to be stored as at any moment all the values in the 'delay line' have to be remembered to successfully solve the tasks. This is different from tasks where a single variable has to be stored and recalled after a long delay. It would be interesting to study tasks where the number of delay steps and the number of items in memory can be varied independently.

Finally, we did the same analysis on networks with only preflossing (_gradient flossing_ before training) and found the same trend (supplement Fig 7D), however, in that case even if all \(N\) Lyapunov exponents were flossed, thus \(k=N\), they were not able to solve the most difficult tasks. This seems to indicate that _gradient flossing_ during training cannot be replaced by just _gradient flossing_ more Lyapunov exponents before training.

## 8 Limitations

The mathematical connection between Lyapunov exponents and backpropagation through time exploited in _gradient flossing_ is rigorously established only in the infinite-time limit. It would be interesting to extend our analysis to finite-time Lyapunov exponents.

Furthermore, the backpropagation through time gradient involves a sum over products of Jacobians of different time periods \(t-\), but the Lyapunov exponent only considers the asymptotic longest product. Additionally, Lyapunov exponents characterize the asymptotic dynamics on the attractor of the dynamics, whereas RNNs often exploit transient dynamics from some initial conditions outside or towards the attractor.

Although our proposed method focuses on exploiting Lyapunov exponents, it neglects the geometry of covariant Lyapunov vectors , which could be used to improve training performance, speed, and reliability. Additionally, it is important to investigate how sensitive the method is to the choice of orthonormal basis employed because it is only guaranteed to become unique asymptotically .

Figure 5: _Gradient flossing for different numbers of flossed Lyapunov exponents_

**A)** Test accuracy for delayed temporal XOR task as a function of delay \(d\) with different numbers flossed Lyapunov exponents \(k\). **B)** Same data as **A** but here test accuracy as a function of number of flossed Lyapunov exponents \(k\). Parameters: \(g=1\), batch size \(b=16\), \(N=80\), epochs \(=10^{4}\) for delayed temporal XOR, epochs \(=5000\) for delayed spatial XOR, \(T=300\), _gradient flossing_ for \(E_{f}=500\) epochs before training and during training for **A, B**. Shaded areas are 25% and 75% percentile, solid lines are means, transparent dots are individual simulations, task loss: cross-entropy between \(y\) and \(\).

Finally, the computational cost of our method scales with \(O(Nk^{2})\), where \(N\) is the network size and \(k\) is the number of Lyapunov exponents calculated. To reduce the computational cost, we suggest doing QR decomposition only sufficiently often to ensure that the orthonormal system is not ill-conditioned and using _gradient flossing_ only intermittently or as pretraining. One could also calculate the Lyapunov spectrum for a shorter time interval or use a cheaper proxy for the Lyapunov spectrum and investigate more efficient _gradient flossing_ schedules.

## 9 Discussion

We tackle the problem of gradient signal propagation in recurrent neural networks through a dynamical systems lens. We introduce a novel method called _gradient flossing_ that addresses the problem of gradient instability during training. Our approach enhances gradient signal stability both before and during training by regularizing Lyapunov exponents. By keeping the long-term Jacobian well-conditioned, _gradient flossing_ optimizes both training accuracy and speed. To achieve this, we combine established dynamical systems methods for calculating Lyapunov exponents with an analytical pullback of the QR factorization. This allows us to establish and maintain gradient stability in a in a manner that is memory-efficient, numerically stable, and exact across long time horizons. Our method is applicable to arbitrary RNN architectures, nonlinearities, and also neural ODEs . Empirically, pre-training with _gradient flossing_ enhances both training speed and accuracy. For difficult temporal credit assignment problems, _gradient flossing_ throughout training further enhances signal propagation. We also demonstrate the versatility of our method on a set of synthetic tasks with controllable time-complexity and show that it can be combined with other approaches to tackle exploding and vanishing gradients, such as dynamic mean-field theory for initialization, orthogonal initialization and specialized single units, such as LSTMs.

Prior research on exploding and vanishing gradients mainly focused on selecting network architectures that are less prone to exploding/vanishing gradients or finding parameter initializations that provide well-conditioned gradients at least at the beginning of training. Our introduced _gradient flossing_ can be seen as a complementary approach that can further enhance gradient stability throughout training. Compared to the work on picking good parameter initializations based on random matrix theory  and mean-field heuristics , _gradient flossing_ provides several improvements: First, mean-field theory only considers the gradient flow at initialization, while _gradient flossing_ can maintain gradient flow and well-conditioned Jacobians throughout the training process. Second, random matrix theory and mean-field heuristics are usually confined to the limit of large networks , while _gradient flossing_ can be used for networks of any size. The link between Lyapunov exponents and the gradients of backpropagation through time has been described previously  and has been spelled out analytically and studied numerically . In contrast, we use Lyapunov exponents here not only as a diagnostic tool for gradient stability but also to show that they can directly be part of the cure for exploding and vanishing gradients.

Future investigations could delve further into the roles of the second to \(N\)th Lyapunov exponents in trainability, and how it is related to the task at hand, the rank of the parameter update, the dimensionality of the solution space, as well as the network dynamics (see also ). Our results suggest a trade-off between trainability across long time horizons and the nonlinear task demands that is worth exploring in more detail (appendix C). Applying _gradient flossing_ to real-time recurrent learning and its biologically plausible variants is another avenue . Extending _gradient flossing_ to feedforward networks, state-space models and transformers is a promising avenue for future research (see also ). While Lyapunov exponents are only strictly defined for dynamical systems, such as maps or flows that are endomorphisms, the long-term Jacobian of deep feedforward networks could be treated similarly. This could also provide a link between the stability of the network against adversarial examples and its dynamic stability, as measured by Lyapunov exponents. Given that time-varying input can suppress chaos in recurrent networks , we anticipate they may exacerbate vanishing gradients. _Gradient flossing_ could also be applied in neural architecture search, to identify and optimize trainable networks. Finally, _gradient flossing_ is applicable to other model parameters, as well. For instance, gradients of Lyapunov exponents with respect to single-unit parameters could optimize the activation function and single-neuron biophysics in biologically plausible neuron models.