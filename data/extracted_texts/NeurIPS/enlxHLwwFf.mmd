# Functional Bilevel Optimization for Machine Learning

Ieva Petrulionyte, Julien Mairal, Michael Arbel

Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France

firstname.lastname@inria.fr

###### Abstract

In this paper, we introduce a new functional point of view on bilevel optimization problems for machine learning, where the inner objective is minimized over a function space. These types of problems are most often solved by using methods developed in the parametric setting, where the inner objective is strongly convex with respect to the parameters of the prediction function. The functional point of view does not rely on this assumption and notably allows using over-parameterized neural networks as the inner prediction function. We propose scalable and efficient algorithms for the functional bilevel optimization problem and illustrate the benefits of our approach on instrumental regression and reinforcement learning tasks.

## 1 Introduction

Bilevel optimization methods solve problems with hierarchical structures, optimizing two interdependent objectives: an _inner-level_ objective and an _outer-level_ one. Initially used in machine learning for model selection (Bennett et al., 2006) and sparse feature learning (Mairal et al., 2012), these methods gained popularity as efficient alternatives to grid search for hyper-parameter tuning (Feurer and Hutter, 2019; Lorraine et al., 2019; Franceschi et al., 2017). Applications of bilevel optimization include meta-learning (Bertinetto et al., 2019), auxiliary task learning (Navon et al., 2021), reinforcement learning (Hong et al., 2023; Liu et al., 2021; Nikishin et al., 2022), inverse problems (Holler et al., 2018) and invariant risk minimization (Arjovsky et al., 2019; Ahuja et al., 2020).

Bilevel problems are challenging to solve, even in the _well-defined bilevel_ setting with a unique inner-level solution. This difficulty stems from approximating both the inner-level solution and its sensitivity to the _outer-level_ variable during gradient-based optimization. Methods like Iterative Differentiation (ITD, Baydin et al., 2017) and Approximate Implicit Differentiation (AID, Ghadimi and Wang, 2018) were designed to address these challenges in the well-defined setting, resulting in scalable algorithms with strong convergence guarantees (Domke, 2012; Gould et al., 2016; Ablin et al., 2020; Arbel and Mairal, 2022; Blondel et al., 2022; Liao et al., 2018; Liu et al., 2022; Shaban et al., 2019). These guarantees usually require the _inner-level_ objective to be strongly convex. However, when the inner-level variables are neural network parameters, the lower-level problem becomes non-convex and may have multiple solutions due to over-parameterization. While non-convexity is considered "benign" in this setting (Allen-Zhu et al., 2019; Liu et al., 2022), multiplicity of inner-level solutions makes their dependence on the outer-level variable ambiguous (Liu et al., 2021), posing a major challenge in bilevel optimization for modern machine learning applications.

We identify a common _functional structure_ in bilevel machine learning problems to address the ambiguity challenge that arises with flexible models like neural networks. Specifically, we consider a _prediction function_\(h\) optimized by the inner-level problem over a Hilbert space \(\). This space consists of functions defined over an input space \(\) and taking values in a finite dimensional vector space \(\). The optimal _prediction function_ is then evaluated in the outer-level to optimize an outer-level parameter \(\) in a finite dimensional space \(=^{d}\), resulting in a _functional_ bilevel problem:

\[_{}\;():\,=L_{out}(,h^{}_ {}) h^{}_{}=*{arg\,min}_{h}\;L_{in} (,h).\] (FBO)In contrast to classical bilevel formulations involving neural networks, where the inner objective is non-convex with respect to the network parameters, the inner objective in (FBO) defines an optimization problem over a prediction function \(h\) in a functional vector space \(\).

A crucial consequence of adopting this new viewpoint is that it renders the strong convexity of the inner objective with respect to \(h\) a mild assumption, which ensures the uniqueness of the solution \(h^{}_{}\) for any outer parameter value \(\). Strong convexity with respect to the _prediction function_ is indeed much weaker than strong convexity with respect to model parameters and often holds in practice. For instance, a supervised prediction task with pairs of features/labels \((x,y)\) drawn from some training data distribution is formulated as a regularized empirical minimization problem:

\[_{h}L_{in}(,h):=_{x,y}[\|y-h(x)\|_{2}^{ 2}]+\|h\|_{}^{2},\] (1)

where \(\) is the \(L_{2}\) space of square integrable functions w.r.t. the distribution of \(x\), and \(>0\) controls the amount of regularization. The inner objective is strongly convex in \(h\), even though the optimal prediction function \(h^{}_{}\) can be highly nonlinear in \(x\). The function \(h^{}_{}\) may then be approximated, _e.g._, by an overparameterized deep neural network, used here as a function approximation tool.

Although appealing, the (FBO) formulation necessitates the development of corresponding theory and algorithms, which is the aim of our paper. To the best of our knowledge, this is the first work to propose a functional bilevel point of view that can leverage deep networks for function approximation. The closest works are either restricted to kernel methods (Rosset, 2008; Kunapuli et al., 2008) and thus cannot be used for deep learning models, or propose abstract algorithms that can only be implemented for finite Hilbert spaces (Suonpera and Valkonen, 2024).

We introduce in Section 2 a theoretical framework for functional implicit differentiation in an abstract Hilbert space \(\) that allows computing the _total gradient_\(()\) using a functional version of the _implicit function theorem_(Ioffe and Tihomirov, 1979) and the _adjoint sensitivity method_(Pontryagin, 2018). This involves solving a well-conditioned functional linear system, equivalently formulated as a regression problem in \(\), to find an adjoint function \(a^{}_{}\) used for computing the _total gradient_\(()\). We then specialize this framework to the common scenario where \(\) is an \(L_{2}\) space and objectives are expectations of point-wise losses. This setting covers many machine learning problems (see Sections 4.1, 4.2, and Appendix A). In Section 3, we propose an efficient algorithm where the prediction and adjoint functions can be approximated using parametric models, like neural networks, learned with standard stochastic optimization tools. We further study its convergence using analysis for biased stochastic gradient descent (Demidovich et al., 2024).

The proposed method, called _functional implicit differentiation_(_FuncID_), adopts a novel "differentiate implicitly, then parameterize" approach (left Figure 1): functional strong convexity is first exploited to derive an **unambiguous** implicit gradient in function space using a well-defined adjoint function. Then, both the lower-level solution and adjoint function are approximated using neural networks. This contrasts with traditional AID/ITD approaches (right Figure 1), which parameterize the inner-level solution as a neural network, leading to a non-convex 'parametric' bilevel problem in the network's parameters. An **ambiguous** implicit gradient is then computed by approximately solving an unstable or ill-posed linear system (Arbel and Mairal, 2022). Consequently, _FuncID_ addresses the ambiguity challenge by exploiting the functional perspective and results in a stable algorithm with reduced time and memory costs. In Section 4, we demonstrate the benefits of our approach in instrumental regression and reinforcement learning tasks, which admit a natural functional bilevel structure.

Related work on bilevel optimization with non-convex inner objectives.In principle, considering amended versions of the bilevel problem can resolve the ambiguity arising from non-convex inner

Figure 1: Parametric vs functional approaches for solving FBO by implicit differentiation.

objectives. This is the case of optimistic/pessimistic versions of the problem, often considered in the literature on mathematical optimization, where the outer-level objective is optimized over both outer and inner variables, under the optimality constraint of the inner-level variable (Dempe et al., 2007; Ye and Ye, 1997; Ye and Zhu, 1995; Ye et al., 1997). While tractable methods were recently proposed to solve them (Liu et al., 2021a, 2023; Kwon et al., 2024; Shen and Chen, 2023), it is unclear how well the resulting solutions behave on unseen data in the context of machine learning. For instance, when using over-parameterized models for the inner-level problem, their parameters must be further optimized for the outer-level objective, possibly resulting in over-fitting (Vicol et al., 2022). More recently, Arbel and Mairal (2022) proposed a game formulation involving a _selection map_ to deal with multiple inner-level solutions. Such a formulation justifies the use of ITD/AID outside the well-defined bilevel setting, by viewing those methods as approximating the Jacobian of the selection map. However, the justification only hold under rather strong geometric assumptions. Additional related work is discussed in Appendix B on bilevel optimization with strongly-convex inner objectives, the adjoint sensitivity method that is often used in the context of ordinary differential equations, and amortization techniques (Amos et al., 2023) that have been also exploited for approximately solving bilevel optimization problems (MacKay et al., 2019; Bae and Grosse, 2020).

## 2 A Theoretical Framework for Functional Bilevel Optimization

The functional bilevel problem (FBO) involves an optimal prediction function \(h_{}^{}\) for each value of the outer-level parameter \(\). Solving (FBO) by using a first-order method then requires characterizing the implicit dependence of \(h_{}^{}\) on the outer-level parameter \(\) to evaluate the total gradient \(()\) in \(^{d}\). Indeed, assuming that \(h_{}^{}\) and \(L_{out}\) are Frechet differentiable (this assumption will be discussed later), the gradient \(()\) may be obtained by an application of the chain rule:

\[()=g_{}+_{}h_{}^{}d_{ }, g_{}:=_{}L_{out}(,h_{ }^{}) d_{}:=_{h}L_{out}( ,h_{}^{}).\] (2)

The Frechet derivative \(_{}h_{}^{}:^{d}\) is a linear operator acting on functions in \(\) and measures the sensitivity of the optimal solution on the outer variable. We will refer to this quantity as the "Jacobian" in the rest of the paper. While the expression of the gradient in Equation (2) might seem intractable in general, we will see in Section 3 a class of practical algorithms to estimate it.

### Functional implicit differentiation

Our starting point is to characterize the dependence of \(h_{}^{}\) on the outer variable. To this end, we rely on the following implicit differentiation theorem (proven in Appendix C) which can be seen as a functional version of the one used in AID (Domke, 2012; Pedregosa, 2016), albeit, under a much weaker _strong convexity assumption_ that holds in most practical cases of interest.

**Theorem 2.1** (**Functional implicit differentiation)**.: _Consider problem (FBO) and assume that:_

* _For any_ \(\)_, there exists_ \(\!>\!0\) _such that_ \(h L_{in}(^{},h)\) _is_ \(\)_-strongly convex for any_ \(^{}\) _near_ \(\)_._
* \(h L_{in}(,h)\) _has finite values and is Frechet differentiable on_ \(\) _for all_ \(\)_._
* \(_{h}L_{in}\) _is Hadamard differentiable on_ \(\) _(in the sense of Definition_ C.1 _in Appendix_ C_)._

_Then, \( h_{}^{}\) is uniquely defined and is Frechet differentiable with a Jacobian \(_{}h_{}^{}\) given by:_

\[B_{}+_{}h_{}^{}C_{}=0,  B_{}:=_{,h}L_{in}(,h_{}^{}),  C_{}:=_{h}^{2}L_{in}(,h_{}^{}).\] (3)

The strong convexity assumption on the inner-level objective ensures the existence and uniqueness of the solution \(h_{}^{}\), while differentiability assumptions on \(L_{in}\) and \(_{h}L_{in}\) ensure Frechet differentiability of the map \( h_{}^{}\). Though the implicit function theorem for Banach spaces (Ioffe and Tihomirov, 1979) could yield similar conclusions, it demands the stronger assumption that \(_{h}L_{in}\) is continuously Frechet differentiable, which is quite restrictive in our setting of interest: for instance, when \(\) is an \(L_{2}\)-space and \(L_{in}\) is an integral functional of the form \(L_{in}(,h)=_{in}(w,h(x))\,x\), with \(_{in}\) defined on \(\) and satisfying mild smoothness and growth assumptions on \(_{in}\), then \(h_{h}L_{in}(,h)\) cannot be Frechet differentiable with uniformly continuous differential on bounded sets, unless \(v_{in}(,v)\) is a polynomial of degree at most \(2\) (see (Nemirovski and Semenov, 1973; Corollary 2, p 276) and discussions in (Noll, 1993; Goodman, 1971)). Instead, Theorem 2.1 employs the weaker notion of Hadamard differentiability for \(_{h}L_{in}\), widely used in statistics, particularly for deriving the _delta-method_(van der Vaart and Wellner, 1996, Chapter 3.9). Consequently, Theorem 2.1 allows us to cover a broader class of functional bilevel problems, as we see in Section 2.2.

Similarly to AID, only a Jacobian-vector product is needed when computing the total gradient \(()\). The result in Proposition 2.2 below, relies on the _adjoint sensitivity method_(Pontryagin, 2018) to provide a more convenient expression for \(()\) and is proven in Appendix C.2.

**Proposition 2.2** (**Functional adjoint sensitivity)**.: _Under the same assumption on \(L_{in}\) as in Theorem 2.1 and further assuming that \(L_{out}\) is jointly differentiable in \(\) and \(h\), the total objective \(\) is differentiable with \(()\) given by:_

\[()=g_{}+B_{}a_{}^{},\] (4)

_where the adjoint function \(a_{}^{}:=-C_{}^{-1}d_{}\) is an element of \(\) that minimizes the quadratic objective:_

\[a_{}^{}=_{a}L_{adj}(,a):=  a,C_{}a_{}+ a,d_{ }_{}.\] (5)

Equation (4) indicates that computing the total gradient requires optimizing the quadratic objective (5) to find the adjoint function \(a_{}^{}\). The strong convexity of the adjoint objective \(L_{adj}\) ensures the existence of a unique minimizer, and stems from the positive definiteness of its Hessian operator \(C_{}\) due to the inner-objective's strong convexity. Both adjoint and inner-level problems occur in the same function space \(\) and are equivalent in terms of conditioning, as the adjoint Hessian operator equals the inner-level Hessian at optimum. The _strong convexity in \(\)_ of the adjoint objective guarantees well-defined solutions and holds in many practical cases, as opposed to classical parametric bilevel formulations which require the more restrictive _strong convexity condition in the model's parameters_, and without which instabilities may arise due to ill-conditioned linear systems (see Appendix F.1).

### Functional bilevel optimization in \(L_{2}\) spaces

Specializing the abstract results from Section 2.1 to a common scenario in machine learning, we consider both inner and outer level objectives of (FBO) as expectations of point-wise functions over observed data. Specifically, we have two data distributions \(\) and \(\) defined over a product space \(^{d_{x}}^{d_{y}}\), and denote by \(\) the Hilbert space of functions \(h:\), where \(=^{d_{v}}\). Given an outer parameter space \(\), we address the following functional bilevel problem:

\[_{}\,L_{out}(,h_{ }^{})&:=_{}[_{out} (,h_{}^{}(x),x,y)]\\ h_{}^{}&=*{arg\, min}_{h}\,L_{in}(,h):=_{} [_{in}(,h(x),x,y)],\] (6)

where \(_{out}\), \(_{in}\) are point-wise loss functions defined on \(\). This setting encompasses various deep learning problems discussed in Sections 4.1 and 4.2, and in Appendix A, representing a specific instance of FBO. The Hilbert space \(\) of square-integrable functions not only models a broad range of prediction functions but also facilitates obtaining concrete expressions for the total gradient \(()\), enabling the derivation of practical algorithms in Section 3.

The following proposition, proved in Appendix D, makes mild technical assumptions on probability distributions \(\), \(\) and the point-wise losses \(_{in}\), \(_{out}\). It gives an expression for the total gradient in the form of expectations under \(\) and \(\).

**Proposition 2.3** (**Functional adjoint sensitivity in \(L_{2}\) spaces.)**.: _Under the technical Assumptions (A) to (L) stated in Appendix D.1, the conditions on \(L_{in}\) and \(L_{out}\) in Proposition 2.2 hold and the total gradient \(()\) of \(\) is expressed as \(()=g_{}+B_{}a_{}^{}\), with \(a_{}^{}\) being the minimizer of the objective \(L_{adj}\) in Equation (5). Moreover, \(L_{adj}\), \(g_{}\) and \(B_{}a_{}^{}\) are given by:_

\[ L_{adj}(,a)=\,_{ }[a(x)^{}_{}^{2}_{in}(,h_{ }^{}(x),x,y)a(x)]\\ +_{}[a(x)^{}_{} _{out}(,h_{}^{}(x),x,y)],\] (7)

\[g_{}=_{}[_{}_{out}( ,h_{}^{}(x),x,y)], B_{}a_{}^{ }=_{}[_{,v}_{in}(,h_{ }^{}(x),x,y)a_{}^{}(x)],\] (8)

_where \(_{}_{out}\), \(_{v}_{out}\), \(_{,v}_{in}\), and \(_{v}^{2}_{in}\) are partial first and second order derivatives of \(_{out}\) and \(_{in}\) with respect to their first and second arguments \(\) and \(v\)._Assumptions (**A**) and (**B**) on \(\) and \(\) ensure finite second moments and bounded Radon-Nikodym derivatives, maintaining square integrability under both distributions in Equation (6). Assumptions (**C**) to (**L**) on \(_{in}\) and \(_{out}\) primarily involve integrability, differentiability, Lipschitz continuity, and strong convexity of \(_{in}\) in its second argument, typically satisfied by objectives like mean squared error or cross entropy (see Proposition D.1 in Appendix D.1). Next, by leveraging Proposition 2.3, we derive practical algorithms for solving FBO using function approximation tools like neural networks.

## 3 Methods for Functional Bilevel Optimization in \(L_{2}\) Spaces

We propose _Functional Implicit Differentiation (FuncID)_, a flexible class of algorithms for solving the functional bilevel problem in \(L_{2}\) spaces described in Section 2.2 when samples from distributions \(\) and \(\) are available.

_FuncID_ relies on three main components detailed in the next subsections:

1. **Empirical objectives.** These approximate the objectives \(L_{out}\), \(L_{in}\) and \(L_{adj}\) as empirical expectations over samples from inner and outer datasets \(_{in}\) and \(_{out}\).
2. **Function approximation.** The search space for both the prediction and adjoint functions is restricted to parametric spaces with finite-dimensional parameters \(\) and \(\). Approximate solutions \(_{}\) and \(_{}\) to the optimal functions \(h_{}^{}\) and \(a_{}^{}\) are obtained by minimizing the empirical objectives.
3. **Total gradient approximation.**_FuncID_ estimates the total gradient \(()\) using the empirical objectives, and the approximations \(_{}\) and \(_{}\) of the prediction and adjoint functions.

Algorithm 1 provides an outlines of _FuncID_ which has a nested structure similar to AID: (1) inner-level optimizations (InnerOpt and AdjointOpt) to update the prediction and adjoint models using scalable algorithms such as stochastic gradient descent (Robbins and Monro, 1951), and (2) an outer-level optimization to update the parameter \(\) using a total gradient approximation TotalGrad. An optional _warm-start_ allows initializing the parameters of both the prediction and adjoint models for the current outer-level iteration with those obtained from the previous one.

``` Input: initial outer, inner, and adjoint parameter \(_{0}\), \(_{0}\), \(_{0}\); warm-start option WS. for\(n=0,,N-1\)do # Optional warm-start ifWS=True then\((_{0},_{0})(_{n},_{n})\) endif # Inner-level optimization \(_{_{n}},_{n+1}(_{n},_ {0},_{in})\) # Adjoint optimization \(_{_{n}},_{n+1}(_{n},_{0},_{_{n}},)\) # Outer gradient estimation  Sample a mini-batch \(=(_{out},_{in})\) from \(=(_{out},_{in})\) \(g_{out}(_{n},_{_{n}},_{ _{n}},)\) \(_{n+1}\) update \(_{n}\) using \(g_{out}\); endfor ```

**Algorithm 1**_FuncID_

### From population losses to empirical objectives

We assume access to two datasets \(_{in}\) and \(_{out}\), comprising i.i.d. samples from \(\) and \(\), respectively. This assumption can be relaxed, such as when using samples from a Markov chain or a Markov Decision Process to approximate population objectives. For scalability, we operate in a mini-batch setting, where batches \(=(_{out},_{in})\) are sub-sampled from datasets \(:=(_{out},_{in})\). Approximating both inner and outer level objectives in (6) can be achieved using empirical versions:

\[_{out}(,h,_{out}) :=_{out}|}_{(,) _{out}}_{out}(,h(),, ),\] \[_{in}(,h,_{in}) :=_{in}|}_{(x,y)_{in}} _{in}(,h(x),x,y).\]

**Adjoint objective.** Using the expression of \(L_{adj}\) from Proposition 2.3, we derive a finite-sample approximation of the adjoint loss by replacing the population expectations by their empirical counterparts. More precisely, assuming we have access to an approximation \(_{}\) to the inner-level prediction function, we consider the following empirical version of the adjoint objective:

\[_{adj}(,a,_{},):= _{in}|}_{(x,y)_{in}}a(x)^{ }_{v}^{2}_{in}(,_{}(x),x,y)\,a(x)\\ +_{out}|}_{(x,y)_{out}}a(x )^{}_{v}_{out}(,_{}(x),x,y).\] (9)

The adjoint objective in Equation (9) requires computing a Hessian-vector product with respect to the output \(v\) in \(^{d_{v}}\) of the prediction function \(_{}\), which is typically of reasonably small dimension, unlike traditional AID methods that necessitate a Hessian-vector product with respect to some model parameters. Importantly, compared to AID, _FuncID_ does not requires differentiating twice w.r.t the model's parameters \(()\) which results in memory and time savings as discussed in Appendix F.2.

### Approximate prediction and adjoint functions

To find approximate solutions to the prediction and adjoint functions we rely on three steps: 1) specifying parametric search spaces for both functions, 2) introducing optional regularization to prevent overfitting and, 3) defining a gradient-based optimization procedure on the empirical objectives.

**Parametric search spaces.** We approximate both prediction and adjoint functions using parametric search spaces. A parametric family of functions defined by a map \(:\) over parameters \(^{p_{in}}\) constrains the prediction function \(h\) as \(h(x)=()(x)\). We only require \(\) to be continuous and differentiable almost everywhere such that back-propagation can be applied (Bolte et al., 2021). Notably, unlike AID, we do not need \(\) to be twice differentiable, as functional implicit differentiation computes the Hessian w.r.t. the output of \(\), not w.r.t. its parameters \(\). For flexibility, we can consider a different parameterized model \(:\) for approximating the adjoint function, defined over parameters \(^{p_{adj}}\), constraining the adjoint similarly to \(\). In practice, we often use the same parameterization, typically a neural network, for both the inner-level and the adjoint models.

**Regularization.** With empirical objectives and parametric search spaces, we can directly optimize parameters of both the inner-level model \(\) and the adjoint model \(\). However, to address finite sample issues, regularization may be introduced to these empirical objectives for better generalization. The method allows flexibility in regularization choice, accommodating functions \( R_{in}()\) and \( R_{adj}()\), such as ridge penalty or other commonly used regularization techniques

**Optimization.** The function InnerOpt (defined in Algorithm 2) optimizes inner model parameters for a given \(\), initialization \(_{0}\), and data \(_{in}\), using \(M\) gradient updates. It returns optimized parameters \(_{M}\) and the corresponding inner model \(_{}=(_{M})\), approximating the inner-level solution. Similarly, AdjointOpt (defined in Algorithm 3) optimizes adjoint model parameters with \(K\) gradient updates, producing the approximate adjoint function \(_{}=(_{K})\). Other optimization procedures may also be used, especially when closed-form solutions are available, as exploited in some experiments in Section 4. Operations requiring differentiation can be implemented using standard optimization procedures with automatic differentiation packages like PyTorch (Paszke et al., 2019) or Jax (Bradbury et al., 2018).

``` for\(m=0,,M-1\)do  Sample batch \(_{in}\) from \(_{in}\) \(g_{in}\!\!_{}[_{in}(,(_{m}), _{in})\!+\!R_{in}(_{m})]\) \(_{m+1}\) Update \(_{m}\) using \(g_{in}\) endfor Return\((_{M}),_{M}\) ```

**Algorithm 2**InnerOpt(\(,_{0},_{in}\))

``` for\(k=0,,K-1\)do  Sample batch \(\) from \(\) \(g_{adj}\!\!_{}[_{adj}(,(_{t}),_ {},)\!+\!R_{adj}(_{k})]\) \(_{k+1}\) Update \(_{k}\) using \(g_{adj}\) endfor Return\((_{K}),_{K}\) ```

**Algorithm 3**AdjointOpt(\(,_{0},_{},\))

``` for\(k=0,,K-1\)do  Sample batch \(\) from \(\) \(g_{adj}\!\!_{}[_{adj}(,(_{t}),_ {},)\!+\!R_{adj}(_{k})]\) \(_{k+1}\) Update \(_{k}\) using \(g_{adj}\) endfor Return\((_{K}),_{K}\) ```

**Algorithm 4**AdjointOpt(\(,_{0},_{},\))

``` for\(k=0,,K-1\)do  Sample batch \(\) from \(\) \(g_{adj}\!\!_{}[_{adj}(,(_{t}),_ {},)\!+\!R_{adj}(_{k})]\) \(_{k+1}\) Update \(_{k}\) using \(g_{adj}\) endfor Return\((_{K}),_{K}\) ```

**Algorithm 5**AdjointOpt(\(,_{0},_{},\))

**Optimization.** The function InnerOpt (defined in Algorithm 2) optimizes inner model parameters for a given \(\), initialization \(_{0}\), and data \(_{in}\), using \(M\) gradient updates. It returns optimized parameters \(_{M}\) and the corresponding inner model \(_{}=(_{M})\), approximating the inner-level solution. Similarly, AdjointOpt (defined in Algorithm 3) optimizes adjoint model parameters with \(K\) gradient updates, producing the approximate adjoint function \(_{}=(_{K})\). Other optimization procedures may also be used, especially when closed-form solutions are available, as exploited in some experiments in Section 4. Operations requiring differentiation can be implemented using standard optimization procedures with automatic differentiation packages like PyTorch (Paszke et al., 2019) or Jax (Bradbury et al., 2018).

**Algorithm 5**AdjointOpt(\(,_{0},_{},\))

``` for\(k=0,,K-1\)do  Sample batch \(\) from \(\) \(g_{adj}\!\!_{}[_{adj}(,(_{t}),_ {},)\!+\!R_{adj}(_{k})]\) \(_{k+1}\) Update \(_{k}\) using \(g_{adj}\) endfor Return\(

### Total gradient estimation

We exploit Proposition 2.3 to derive Algorithm 4, which allows us to approximate the total gradient \(()\) after computing the approximate solutions \(_{}\) and \(_{}\). There, we decompose the gradient into two terms: \(g_{Exp}\), an empirical approximation of \(g_{}\) in Equation (8) representing the explicit dependence of \(L_{out}\) on the outer variable \(\), and \(g_{Imp}\), an approximation to the implicit gradient term \(B_{}a_{}^{}\) in Equation (8). Both terms are obtained by replacing the expectations by empirical averages batches \(_{in}\) and \(_{out}\), and using the approximations \(_{}\) and \(_{}\) instead of the exact solutions.

``` \(g_{Exp}_{}_{out}(,_{}, _{out})\) \(g_{Imp}_{in}|}_{(x,y)_{in}} _{,v}_{in}(,_{}(x),x,y)\,_{ }(x)\) Return\(g_{Exp}+g_{Imp}\) ```

**Algorithm 4**TotalGrad(\(,_{},_{},\))

### Convergence Guarantees

Convergence of Algorithm 1 to stationary points of \(\) depends on approximation errors, which result from sub-optimal inner and adjoint solutions, as shown by the convergence result below.

**Theorem 3.1**.: _Assume that \(\) is \(\)-smooth and admits a finite lower bound \(^{}\). Use an update rule \(_{n+1}=_{n}- g_{out}\) with step size \(0<_{}}\) in Algorithm 1. Under Assumption **(a)** on sub-optimality of \(_{}\) and \(_{}\), Assumptions **(b)** to **(h)** on the smoothness of \(_{in}\) and \(_{out}\), all stated in Appendix E.1, and the assumptions in Proposition 2.3, the iterates \(\{_{n}\}_{n 0}\) of Algorithm 1 satisfy:_

\[_{0 n N-1}[\|(_{n}) \|^{2}](_{0})-^{ })}{ N}+2_{eff}^{2}+(c_{1}_{in}+c_ {2}_{adj}),\]

_where \(c_{1}\), \(c_{2},_{eff}^{2}\) are positive constants, and \(_{in},_{adj}\) are sub-optimality errors that result from the inner and adjoint optimization procedures._

Theorem 3.1 is proven in Appendix E and relies on the general convergence result in Demidovich et al. (2024, Theorem 3) for stochastic biased gradient methods. The key idea is to control both bias and variance of the gradient estimator in terms of generalization errors \(_{in}\) and \(_{adj}\) when approximating the inner and adjoint solutions. These generalization errors can, in turn, be made smaller in the case of over-parameterized networks, by increasing network capacity, number of steps and sample size (Allen-Zhu et al., 2019; Du et al., 2019; Zou et al., 2020).

## 4 Applications

We consider two applications of the functional bilevel optimization problem: Two-stage least squares regression (2SLS) and Model-based reinforcement learning. To illustrate its effectiveness we compare it with other bilevel optimization approaches like AID or ITD, as well as state-of-the-art methods for each application. We provide a versatile implementation of _FuncID_ (https://github.com/inria-thoth/funcBO) in PyTorch (Paszke et al., 2019), compatible with standard optimizers (_e.g._, Adam (Kingma and Ba, 2015)), and supports common regularization techniques. For the reinforcement learning application, we extend an existing JAX (Bradbury et al., 2018) implementation of model-based RL from Nikishin et al. (2022) to apply _FuncID_. To ensure fairness, experiments are conducted with comparable computational budgets for hyperparameter tuning using the MLXP experiment management tool (Arbel and Zouaoui, 2024). Additionally, we maintain consistency by employing identical neural network architectures across all methods and repeating experiments multiple times with different random seeds.

### Two-stage least squares regression (2SLS)

Two-stage least squares regression (2SLS) is commonly used in causal representation learning, including instrumental regression or proxy causal learning (Stock and Trebbi, 2003). Recent studies have applied bilevel optimization approaches to address 2SLS, yielding promising results (Xu et al.,2021b, a, Hu et al., 2023). We particularly focus on 2SLS for Instrumental Variable (IV) regression, a widely-used statistical framework for mitigating endogeneity in econometrics (Blundell et al., 2007, 2012), medical economics (Cawley and Meyerhoefer, 2012), sociology (Bollen, 2012), and more recently, for handling confounders in off-line reinforcement learning (Fu et al., 2022).

Problem formulation.In an IV problem, the objective is to model \(f_{}:t o\) that approximates the structural function \(f_{struct}\) using independent samples \((o,t,x)\) from a data distribution \(\), where \(x\) is an instrumental variable. The structural function \(f_{struct}\) delineates the true effect of a treatment \(t\) on an outcome \(o\). A significant challenge in IV is the presence of an unobserved confounder \(\), which influences both \(t\) and \(o\) additively, rendering standard regression ineffective for recovering \(f_{}\). However, if the instrumental variable \(x\) solely impacts the outcome \(o\) through the treatment \(t\) and is independent from the confounder \(\), it can be employed to elucidate the direct relationship between the treatment \(t\) and the outcome \(o\) using the 2SLS framework, under mild assumptions on the confounder (Singh et al., 2019). This adaptation replaces the regression problem with a variant that averages the effect of the treatment \(t\) conditionally on \(x\)

\[_{}_{}[\|o-_{ }[f_{}(t)|x]\|^{2}].\] (10)

Directly estimating the conditional expectation \(_{}[f_{}(t)|x]\) is hard in general. Instead, it is easier to express it, equivalently, as the solution of another regression problem predicting \(f_{}(t)\) from \(x\):

\[h_{}^{}:=_{}[f_{}(t)|x]= _{h}_{}[\|f_{}(t)-h(x) \|^{2}].\] (11)

Both equations result in the bilevel formulation in Equation (6) with \(y=(t,o)\), \(=\) and the point-wise losses \(_{in}\) and \(_{out}\) given by \(_{in}(,v,x,y)=_{in}(,v,x,(t,o))=\|f_{}(t)-v \|^{2}\) and \(_{out}(,v,x,y)=_{out}(,v,x,(t,o))=\|o-v\|^{2}\). It is, therefore, possible to directly apply Algorithm 1 to learn \(f_{}\) as we illustrate below.

Experimental setup.We study the IV problem using the _dsprites_ dataset (Matthey et al., 2017), comprising synthetic images representing single objects generated from five latent parameters: _shape, scale, rotation_, and _posX, posY_ positions on image coordinates. Here, the treatment variable \(t\) is the images, the hidden confounder \(\) is the _posY_ coordinate, and the other four latent variables form the instrumental variable \(x\). The outcome \(o\) is an unknown structural function \(f_{struct}\) of \(t\), contaminated by confounder \(\) as detailed in Appendix G.1. We follow the setup of the Deep Feature Instrumental Variable Regression (DFIV) _dsprites_ experiment by Xu et al. (2021a, Section 4.2), which achieves state-of-the-art performance. In this setup, neural networks serve as the prediction function and structural model, optimized to solve the bilevel problem in Equations (10) and (11). We explore two versions of our method: _FuncID_, which optimizes all adjoint network parameters, and _FuncID linear_, which learns only the last layer in closed-form while inheriting hidden layer parameters from the inner prediction function. We compare our method with DFIV, AID, ITD, and Penalty-based methods: gradient penalty (GD penal.) and value function penalty (Val penal.) (Shen and Chen, 2023), using identical network architectures and computational budgets for hyperparameter selection. Full details on network architectures, hyperparameters, and training settings are provided in Appendix G.2.

Figure 2: Performance metrics for Instrumental Variable (IV) regression. All results are averaged over 20 runs with 5000 training samples and 588 test samples. (**Left**) box plot of the test loss, with the dashed black line indicating the mean test error. (**Middle**) outer loss vs training iterations, (**Right**) inner loss vs training iterations. The bold lines in the middle and right plots indicate the mean loss, the shaded area corresponds to standard deviation.

Results.Figure 2 compares structural models learned by different methods using 5K training samples (refer to Figure 6 in Appendix G.3 for 10K sample results). The left subplot illustrates out-of-sample mean squared error of learned structural models compared to ground truth outcomes (uncontaminated by noise \(\)), while the middle and right subplots show the evolution of outer and inner objectives over iterations. For the 5K dataset, _FuncID_ outperforms DFIV (_p-value_=0.003, one-sided paired t-test), while showing comparable performance on the 10K dataset (Figure 6). AID and ITD perform notably worse, indicating their parametric approach fails to fully leverage the functional structure. _FuncID_ outperforms the gradient penalty method and performs either better or comparably to the value function penalty method, though the latter shows higher variance with some particularly bad outliers. While all methods achieve similar outer losses, this criterion alone is only reliable as an indicator of convergence when evaluated near the 'exact' inner-level solution corresponding to the lowest inner-loss values. Interestingly, FuncID obtains the lowest inner-loss values, suggesting its outer-loss is a more reliable indicator of convergence.

### Model-based reinforcement learning

Model-based reinforcement learning (RL) naturally yields bilevel optimization formulations, since several components of an RL agent need to be learned using different objectives. Recently, Nikishin et al. (2022) showed that casting model-based RL as a bilevel problem can result in better tolerance to model-misspecification. Our experiments show that the functional bilevel framework yields improved results even when the model is well-specified, suggesting a broader use of the bilevel formulation.

Problem formulation.In model-based RL, the Markov Decision Process (MDP) is approximated by a probabilistic model \(q_{}\) with parameters \(\) that can predict the next state \(s_{}(x)\) and reward \(r_{}(x)\), given a pair \(x:=(s,a)\) where \(s\) is the current environment state and \(a\) is the action of an agent. A second model \(h\) can be used to approximate the action-value function \(h(x)\) that computes the expected cumulative reward given the current state-action pair. Traditionally, the action-value function is learned using the current MDP model, while the latter is learned independently from the action-value function using Maximum Likelihood Estimation (MLE) (Sutton, 1991).

In the bilevel formulation of model-based RL by Nikishin et al. (2022), the inner-level problem involves learning the optimal action-value function \(h_{}^{}\) with the current MDP model \(q_{}\) and minimizing the Bellman error. The inner-level objective can be expressed as an expectation of a point-wise loss \(f\) with samples \((x,r^{},s^{})\), derived from the agent-environment interaction:

\[h_{}^{}=_{h}_{}[f(h (x),r_{}(x),s_{}(x))].\] (12)

Here, the future state and reward \((r^{},s^{})\) are replaced by the MDP model predictions \(r_{}(x)\) and \(s_{}(x)\). In practice, samples from \(\) are obtained using a replay buffer. The buffer accumulates data over several episodes of interactions with the environment, and can therefore be considered independent of the agent's policy. The point-wise loss function \(f\) represents the error between the action-value function prediction and the expected cumulative reward given the current state-action pair:

\[f(v,r^{},s^{}):=\|v-r^{}-_{a^ {}}e^{(s^{},a^{})}\|^{2},\]

with \(\) a lagged version of \(h\) (exponentially averaged network) and \(\) a discount factor. The MDP model is learned implicitly using the optimal function \(h_{}^{}\), by minimizing the Bellman error w.r.t. \(\):

\[_{}_{}[f(h_{}^{}(x),r^ {},s^{})].\] (13)

Equations (12) and (13) define a bilevel problem as in Equation (6), where \(=\), \(y=(r^{},s^{})\), and the point-wise losses \(_{in}\) and \(_{out}\) are given by: \(_{in}(,v,x,y)=f(v,r_{}(x),s_{}(x))\) and \(_{out}(,v,x,y)=f(v,r^{},s^{})\). Therefore, we can directly apply Algorithm 1 to learn both the MDP model \(q_{}\) and the optimal action-value function \(h_{}^{}\).

Experimental setup.We apply _FuncID_ to the _CartPole_ control problem, a classic benchmark in reinforcement learning (Brockman et al., 2016; Nagendra et al., 2017). The goal is to balance a pole attached to a cart by moving the cart horizontally. Following Nikishin et al. (2022), we use a model-based approach and consider two scenarios: one with a well-specified network accurately representing the MDP, and another with a misspecified model having fewer hidden layer units, limiting its capacity.

Using the bilevel formulation in Equations (12) and (13), we compare _FuncID_ with the Optimal Model Design (OMD) algorithm , a variant of AID. Additionally, we compare against a standard single-level model-based RL formulation using Maximum Likelihood Estimation (MLE) . For the adjoint function in _FuncID_, we derive a simple closed-form expression based on the structure of the adjoint objective (see Appendix H.1). We follow the experimental setup of Nikishin et al. (2022), providing full details and hyperparameters in Appendix H.2.

Results.Figure 3 illustrates the training reward evolution for _FuncID_, OMD, and MLE in both well-specified and misspecified scenarios. _FuncID_ consistently performs well across settings. In the well-specified case, where OMD achieves a reward of \(4\), _FuncID_ reaches the maximum reward of \(5\), matching MLE (left Figure 3). In the misspecified scenario, _FuncID_ performs comparably to OMD and significantly outperforms MLE (right Figure 3). Moreover, _FuncID_ tends to converge faster than MLE (see Figure 7 in Appendix H.3) and yields consistently better prediction error than OMD (see Figure 8 in Appendix H.3). These findings align with Nikishin et al. (2022), suggesting that MLE may prioritize minimizing prediction errors, potentially leading to overfitting irrelevant features. In contrast, OMD and _FuncID_ focus on maximizing expected returns, especially in the presence of model misspecification. Our results highlight the effectiveness of (FBO) even in well-specified settings, suggesting, for future work, further investigations for more general RL tasks.

## 5 Discussion and concluding remarks

This paper introduced a functional paradigm for bilevel optimization in machine learning, shifting focus from parameter space to function space. The proposed approach specifically addresses the ambiguity challenge arising from using deep networks in bilevel optimization. The paper establishes the validity of the functional framework by developing a theory of functional implicit differentiation, proving convergence for the proposed _FuncID_ method, and numerically comparing it with other bilevel optimization methods.

The theoretical foundations of our work rely on several key assumptions worth examining. While our convergence guarantees assume both inner and adjoint optimization problems are solved to some optimality, this assumption is supported by recent results on global convergence in over-parameterized networks (Lillen-Zhu et al., 2019; Liu et al., 2022). However, quantifying these optimality errors more precisely and understanding their relationship to optimization procedures remains an open challenge. Additionally, like other bilevel methods, our approach requires careful hyperparameter selection, which can impact practical implementation.

Several promising directions emerge for future research. While we focus on \(L_{2}\) spaces, exploring alternative function spaces (such as Reproducing Kernel Hilbert Spaces or Sobolev spaces) could reveal additional advantages for specific applications. Furthermore, extending our framework to non-smooth objectives or constrained optimization problems, potentially building on existing work in non-smooth implicit differentiation (Bolte et al., 2022), would broaden its applicability.

Figure 3: Average reward on an evaluation environment vs. training iterations on the _CartPole_ task. (**Left**) Well-specified model with 32 hidden units. (**Right**) Misspecified model with 3 hidden units. Both plots show mean reward over 10 runs where the shaded region is the 95% confidence interval.