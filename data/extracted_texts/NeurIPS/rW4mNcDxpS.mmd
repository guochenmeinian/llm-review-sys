# Wasserstein Gradient Flows for Optimizing Gaussian Mixture Policies

Hanna Ziesche1 and Leonel Rozo1

Bosch Center for Artificial Intelligence (BCAI)

Renningen, Germany

name.surname@de.bosch.com

Equal contribution.

###### Abstract

Robots often rely on a repertoire of previously-learned motion policies for performing tasks of diverse complexities. When facing unseen task conditions or when new task requirements arise, robots must adapt their motion policies accordingly. In this context, policy optimization is the _de facto_ paradigm to adapt robot policies as a function of task-specific objectives. Most commonly-used motion policies carry particular structures that are often overlooked in policy optimization algorithms. We instead propose to leverage the structure of probabilistic policies by casting the policy optimization as an optimal transport problem. Specifically, we focus on robot motion policies that build on Gaussian mixture models (GMMs) and formulate the policy optimization as a Wasserstein gradient flow over the GMMs space. This naturally allows us to constrain the policy updates via the \(L^{2}\)-Wasserstein distance between GMMs to enhance the stability of the policy optimization process. Furthermore, we leverage the geometry of the Bures-Wasserstein manifold to optimize the Gaussian distributions of the GMM policy via Riemannian optimization. We evaluate our approach on common robotic settings: Reaching motions, collision-avoidance behaviors, and multi-goal tasks. Our results show that our method outperforms common policy optimization baselines in terms of task success rate and low-variance solutions.

## 1 Introduction

One of the main premises about autonomous robots is their ability to successfully perform a large range of tasks in unstructured environments. This demands robots to adapt their task models according to environment changes or new task requirements, and consequently to adjust their actions to successfully perform under unseen conditions . In general, robotic tasks, such as picking or inserting an object, are usually executed by composing previously-learned skills , each represented by a motion policy. Therefore, in order to successfully perform under new settings, the robot should adapt its motion policies according to the new task requirements and environment conditions.

Research on methods for robot motion policy adaptation is vast , with approaches mainly building on black-box optimizers , end-to-end deep reinforcement learning , and policy search . Regardless of the optimization method, most approaches disregard the intrinsic policy structure in the adaptation strategy. However, several motion policy models (e.g., dynamic movement primitives (DMP), Gaussian mixture models (GMM) , probabilistic movement primitives (ProMPs) , and neural networks , among others), carry specific physical or probabilistic structures that should not be ignored. First, these policy models are often learned from demonstrations in a starting learning phase , thus the policy structure already encapsulates relevant prior information about the skill. Second, structure-unaware adaptation strategies optimize the policy parameters disregardingthe intrinsic structural characteristics of the policy model (e.g., a DMP represents a second-order dynamical system). In this context, we hypothesize that the policy structure may be leveraged to better control the adaptation strategy via policy structure-aware gradients and trust regions.

Our main idea is to design a policy optimization strategy that explicitly builds on a particular policy structure. Specifically, we focus on GMM policies, which have been widely used to learn motion skills from human demonstrations [11; 15; 16; 17; 18; 19]. GMMs provide a simple but expressive enough representation for learning a large variety of robot skills such as: Stable dynamic motions [20; 21; 22], collaborative behaviors [23; 24], assembly strategies [25; 26], and contact-rich manipulation [27; 28], among others. Often, skills learned from demonstrations need to be refined -- due to imperfect data -- or adapted to comply with new task requirements. Existing adaptation strategies for GMM policies either build a kernel method on top of the original skill model , or leverage reinforcement learning (RL) to adapt the policy itself [30; 31]. However, none of these techniques explicitly considered the intrinsic probabilistic structure of the Gaussian mixture policy.

Unlike the aforementioned approaches, we propose a policy optimization technique that explicitly considers the underlying GMM structure. To do so, we exploit optimal transport theory [32; 33], which allows us to view the set of GMM policies as a particular space of probability distributions \(_{d}\). Specifically, we leverage the idea of Chen et al.  and Delon and Desolneux  to view a GMM as a set of discrete measures (Dirac masses) on the space of Gaussian distributions \((^{d})\), which is endowed with a Wasserstein distance (see SS 2). This allows us to formulate the policy optimization as a Wasserstein gradient flow (WGF) over the space of GMMs (as illustrated in Fig. 1 and explained in SS3), where the policy updates are naturally guaranteed to be GMMs. Moreover, we take advantage of the geometry of the Bures-Wasserstein manifold to optimize the Gaussian distributions of a GMM policy via Riemannian optimization. We evaluate our approach over a set of different GMM policies featuring common robot skills: Reaching motions, collision-avoidance behaviors, and multi-goal tasks (see SS 4). Our results show that our method outperforms common policy optimization baselines in terms of task success rate while providing low-variance solutions.

Related Work:Richemond and Magninis  pioneered the idea of understanding policy optimization through the lens of optimal transport. They interpreted the policy iteration as gradient flows by leveraging the implicit Euler scheme under a Wasserstein distance (see SS 2), considering only \(1\)-step return settings. They observed that the resulting policy optimization resembles the gradient flow of the Fokker-Planck equation (JKO scheme) . In a similar spirit, Zhang et al.  proposed to use WGFs to formulate policy optimization as a sequence of policy updates traveling along a gradient flow on the space of probability distributions until convergence. To solve the WGF problem, the authors proposed a particle-based algorithm to approximate continuous density functions and subsequently derived the gradients for particle updates based on the JKO scheme. Although Zhang et al.  considered general parametric policies, their method assumed a distribution over the policy parameters and did not consider a specific policy structure, which partially motivated their particle-based approximation. Recently, Mokrov et al.  tackled the computational burden of particle methods by leveraging input-convex neural networks to approximate the WGFs computation. They reformulated the well-known JKO optimization  over probability measures by an optimization over convex functions. Yet, this work remains a general solution for WGF computation and it did not address its use for policy optimization problems.

Aside from optimal transport approaches, Arenz et al.  proposed a trust-region variational inference for GMMs to approximate multimodal distributions. Although not originally designed

Figure 1: Illustration our policy structure-aware adaptation of GMM policies. _Left_: A robot manipulator tracks a reference trajectory () extracted from a GMM policy () initially learned from demonstrations, while being required to reach for a new target (). _Center_: Our policy optimization provides policy updates that follow a Wasserstein gradient flow on the manifold of GMM policies \(_{d}\). _Right_: The final GMM policy where some Gaussian components were adapted in order to retrieve a new reference trajectory that allows the robot to reach for the new target.

for policy optimization, the authors elucidated a connection to learn GMMs of policy parameters in black-box RL. However, their method cannot directly be applied to our GMM policy adaptation setting, nor does it consider the GMM structure from an optimal transport perspective. Nematollahi et al.  proposed SAC-GMM, a hybrid model that employs the well-known SAC algorithm  to refine dynamic skills encoded by GMMs. The SAC policy was designed to learn residuals on a single vectorized stack of GMM parameters, thus fully disregarding the GMM structure and the geometric constraints of its parameters. Ren et al.  introduced PMOE, a method to train deep RL policies using a probabilistic mixture of experts via GMMs. They addressed the non-differentiability problem caused by the optimization of categorical distribution parameters associated with the mixture weights via gradient estimators. However, PMOE does not take an optimal transport perspective on the policy optimization problem, nor does it account for the geometry arising from the GMM parameters.

Concerning approaches that explicitly account for the geometry induced by the policy, we may broadly categorize them into two groups: exact models and natural gradient methods. The former category refers to policy optimization methods that fully exploit the geometric structure of the policy, as proposed in this paper. In this case, the state of the art is scarce but recent works such as the Riemannian proximal policy optimization for GMMs proposed in , showed the importance of leveraging the geometry induced by the GMM parameters in the policy optimization via Riemannian gradients, similarly to our method. Their policy optimization was regularized by a Wasserstein distance to control the exploration-exploitation trade-off. However, their method did not formulate the policy optimization as an optimal transport problem, i.e., the policy updates did not follow a Wasserstein gradient flow, as in our approach, but it employed instead a classical non-convex optimization procedure.

The second category encloses approaches that leverage the well-established technique for enhancing the stability of policy optimization, namely, introducing regularization to the objective function using the Kullback-Leibler (KL) divergence, which induces the so-called natural gradient [43; 44; 45; 46]. In this context, the regularization provides a global measure of the policies similarity, while the natural gradient perspective aims at locally controlling the changes between successive policy updates. In other words, natural gradient approaches leverage the local geometry of the policy parameters space to compute controlled updates. Note that policies similarity can also be quantified via the Wasserstein distance, as proposed in this paper and in recent works [36; 38; 47; 48]. Unlike the KL divergence, the Wasserstein distance enjoys powerful geometrical, computational, and statistical features [32; 33]. For example, Moskovitz et al.  recently employed the Wasserstein natural gradient to exploit the local geometry induced by the Wasserstein regularization of behavioral policy optimization , but neither the policy nor the behavioral embeddings had particular structures that could be leveraged in the policy optimization.

**In this paper** we exploit the geometric properties arising from both a Wasserstein-regularized objective and a Gaussian mixture policy. This allows us not only to see the policy iteration as a Wasserstein gradient flow, as in , but also to leverage the Riemannian geometry associated to the GMM space \(_{d}\) to formulate exact Riemannian gradient descent updates , instead of relying on inexact natural gradients approximations. To achieve this, our method leverages the geometry induced by the structure of the space of GMM policies via the Bures-Wasserstein manifold, which naturally guarantees that policy updates stay on \(_{d}\). To the best of our knowledge, our optimization of GMM policies based on Wasserstein gradient flows and the Bures-Wasserstein manifold is the first of its kind in the domain of policy adaptation.

## 2 Background

### Wasserstein gradient flows

In Euclidean space a gradient flow is a smooth curve \(x:^{d}\) that satisfies the partial differential equation (PDE) \((t)=- L(x(t))\) for a given loss function \(L:^{d}\) and starting point \(x_{0}\) at \(t=0\)[32; 50]. A solution can be found straightforwardly by forward discretization, leading to the well-known explicit Euler update scheme \(x_{k+1}^{}\ =\ x_{k}^{}\ -\  L(x_{k}^{})\), where \(\) denotes the learning rate and \(x^{}\) indicates a discretization of the curve \(x(t)\) with discretization parameter \(\). Alternatively, we can use a backward discretization, which leads to the following implicit Euler scheme

\[x_{k+1}^{}=*{arg\,min}_{x}(^{}\|^{2} }{2}+L(x)).\] (1)Eq. 1 is sometimes referred to as Minimizing Movement Scheme and can be used as an alternative characterization of a gradient flow.

This characterization is particularly interesting when we need to extend the concept of gradient flows to (non-Euclidean) general metric settings, since there is no notion of \( L\) in these cases [32; 51]. Eq. 1 does not involve any gradients and can be expressed using only metric quantities. In this work, we are particularly interested in gradient flows in the \(L^{2}\)-Wasserstein space, defined as the set of probability measures \(()\) on a separable Banach space \(\) and endowed with the \(L^{2}\)-Wasserstein distance \(W_{2}\) defined as

\[W_{2}(,)=(_{(,)}_{}\|x_{1}-x_{2}\|^{2}(x_{1},x_{2}))^{ },\] (2)

where \(,()\) and \((^{2})\) is defined to have the two marginals \(\) and \(\).

A Generalized Minimizing Movement scheme characterizing gradient flows in the Wasserstein space can be written in analogy to Eq. 1 as:

\[_{k+1}^{}=*{arg\,min}_{}(^{ 2}(,_{k}^{})}{2}+L()),\] (3)

where \(L\) is a functional to be minimized on the Wasserstein space and \(_{k}(X)\). In the following, we will omit the superscript \(\) for notational convenience.

### Reinforcement Learning as Wasserstein Gradient Flows

Our view of the policy structure-aware optimization builds on the approach outlined in , which in turn is based on the JKO scheme . They proposed a formulation of \(1\)-step RL problems in terms of Wasserstein gradient flows. In particular, they studied the evolution of a policy \(\) under the influence of a free energy functional \(J\) of the form:

\[J()=K_{r}()+()=_{} (|)r(,)-_{}(|)((|)),\] (4)

where \(K_{r}()\) denotes the inner energy of the system, here determined by the reward \(r(,)\), and \(()\) is the entropy of the policy \((|)\), with \(\) and \(\) denoting the state and action, respectively. Thus, Eq. 4 can be recognized as the usual objective in \(1\)-step RL settings with entropy regularization. It is well known that the evolution of probability densities under a free energy of this form is properly described by a PDE known as the Fokker-Planck equation. Richemond and Maginnis  exploited the result of Jordan et al. , which stated that this evolution can be interpreted as the gradient flow of the functional \(J\) in Wasserstein space. This flow is characterized by the following minimizing movement scheme

\[_{k+1}=*{arg\,min}_{}(^{2}( ,_{k})}{2}-J()),\] (5)

which naturally provides iterative updates for the policy \(\). While Richemond and Maginnis  considered a \(1\)-step bandit setting, we extend this approach to full multi-step RL problems and consequently learn policies for long-horizon tasks.

### The \(L^{2}\)-Wasserstein distance between Gaussian Mixture Models (GMMs)

We consider policies \(()\) that build on a GMM structure, i.e., \(()=_{i=1}^{N}_{i}(;_{i},_{i})\), where \(\) denotes a multivariate Gaussian distribution with mean \(_{i}\) and covariance matrix \(_{i}\), and \(_{i}\) are the weights of the \(N\) individual Gaussian components, which are subject to \(_{i}_{i}=1\). In the following, we will write \(}\), \(}\) and \(}\) to denote the stacked means, covariance matrices and weights of the \(N\) components. Therefore, we do not consider WGFs on the full manifold of probability distributions (Wasserstein space) \((^{d})\) but rather focus on WGFs evolving on the submanifold of GMMs, that is \(_{d}(^{d})\). Following [34; 35], we can approximately describe this submanifold as a discrete distribution over the space of Gaussian distributions equipped with the Wasserstein metric. This in turn can be identified with the Bures-Wasserstein manifold which is the product manifold \(^{d}^{d}_{++}\)where \(^{d}_{++}\) denotes the Riemannian manifold of \(d\)-dimensional symmetric positive definite matrices. The corresponding approximated Wasserstein distance between two GMMs \(_{1}\), \(_{2}\) is given by

\[W_{2}^{2}_{1}(),_{2}()=_{ U(_{1},_{2})}_{i,j}^{N}_{ij}W_{2}^{2}_{1}(;_{i},_{i}),_{2}(;_{j}, _{j}),\] (6)

where \(U(_{1},_{2})=\{_{+}^{N N}|_{N}=_{1},^{}_{N}=_{2}\}\) with \(_{N}\) denoting an \(N\)-dimensional vector of ones. The Wasserstein distance between two Gaussian distributions in Eq. 6 can be computed analytically as follows

\[W_{2}^{2}_{1}(;_{i},_{i}), _{2}(;_{j},_{j})=\|_{i}- {}_{j}\|^{2}+[_{i}+_{j}-2(_{i}^{}{{2}}}_{j}_{i}^{}{{2}}})^{}{{2}}}].\] (7)

### Learning GMM policies from demonstrations

A popular approach in RL -- particularly in robotics -- to reduce the number of policy rollouts in the environment is to warm-start the policy with a set of demonstrations provided by an expert. In this work we choose to represent our policy via a GMM. We assume that demonstrations are provided as a set of trajectories \(\) of state-action pairs \(=\{(_{0},_{0}),(_{1},_{1}),(_{T},_{T})\}\). To initialize our policy, we first use the Expectation-Maximization (EM) algorithm to fit a GMM, in the state-action space, to the demonstrations. This results in a mixture distribution \((,)~{}=~{}_{i=1}^{N}_{i}[\, ]^{};_{i},_{i}\) from which a policy can be obtained by conditioning on the state \(\), as follows

\[(|)=,)}{(,) }.\] (8)

In the context of GMMs, this is also known as Gaussian Mixture Regression (GMR) . The resulting conditional distribution is another GMM on action space with state-dependent parameters,

\[(_{t}|_{t})=_{i=1}^{N}_{i}(_{t})( _{t};_{i}^{a}(_{t}),_{i}^{a}).\] (9)

Details on computation of Eq. 9 from the original GMM are given in App. A.1.

## 3 Wasserstein Gradient Flows for GMM Policy Optimization

In this work, we focus on multi-step RL tasks for policy adaptation. We consider a finite-horizon Markov Decision Process (MDP) with continuous state and action spaces \(^{n}\) and \(^{m}\), transition and reward functions \(p(_{t+1}|_{t},_{t})\) and \(r(_{t},_{t})\), initial state distribution \((_{0})\) and a discount factor \(\). Further, we assume to have an initial policy \((_{t}|_{t})\), which is to be adapted by optimizing some objective function \(K_{r}()\). As stated in 8, this problem arises in robot learning settings where a policy learned via imitation learning (e.g., LfD) needs to be adapted to new objectives or unseen environmental conditions. To promote exploration and avoid premature convergence to suboptimal policies, we leverage maximum entropy RL  by adding an entropy term \(()\) to the objective. Thus, the overall objective has the form of a free energy functional (resembling Eq. 4) and can be written as

\[J()=K_{r}()+(),\] (10)

where \(\) is a hyperparameter and \(K_{r}()\) corresponds to the usual cumulative return

\[K_{r}()\!=\!_{}[_{t}r(_{t},_{t})]\! \!=\!\!\!_{t}_{0}_{t}_ {t}(_{0})(_{t}|_{t})p(_{t+1}|_{t},_ {t})_{t}^{t}r(_{t},_{t}).\] (11)

The evolution of the policy \((_{t}|_{t})\) over the course of the optimization can be described as a flow of a probability distribution in Wasserstein space. This formulation comes with three major benefits: _(i)_ We directly leverage the Wasserstein metric properties for describing the evolution of probability distributions; _(ii)_ We exploit the \(L^{2}\)-Wasserstein distance to constrain the policy updates, which is important to guarantee stability in policy optimization [55; 45; 56]; _(iii)_ By constraining to specific submanifolds of the Wasserstein space, in this case GMMs, we can impose additional structural properties on the policy optimization.

Since our objective in Eq. 10 has the form of the free energy functional studied by [36; 37], we can leverage the iterative updates scheme of Eq. 5 to formulate the evolution of our policy iteration under the flow generated by Eq. 10. As mentioned previously, we focus on the special case of GMM policies and therefore restrict the Wasserstein gradient flow to the submanifold of GMM distributions \(_{d}\). We refer the interested reader to App. A.3, where we provide the explicit form of \(J()\) of Eq. 10 for the GMM case.

### Policy optimization

To begin with, we leverage the approximation that describes the GMM submanifold as a discrete distribution over the space of Gaussian distributions \((^{d})\), equipped with the Wasserstein metric [34; 35]. Consequently, our policy optimization problem naturally splits into an optimization over the \((N-1)\)-dimensional simplex and an optimization on the \(N\)-fold product of the Bures-Wasserstein manifold (\(BW^{N}\)), i.e., the product manifold \((^{d}^{d}_{++})^{N}\). The former corresponds to the GMM weights while the latter applies to the set of Gaussian distributions' parameters. Note that the identification with the \(BW^{N}\) manifold allows us to perform the optimization directly on the parameter space. This comes with several benefits: _(i)_ We can leverage the well-known analytic solution of the Wasserstein distance between Gaussian distributions in Eq. 6, greatly reducing the computational complexity of the policy optimization. _(ii)_ As shown in , we can guarantee that the policy optimized via Eq. 6 remains a GMM (i.e., it satisfies the mass conservation constraint). _(iii)_ Unlike the full Wasserstein space2, the resulting product manifold is a true Riemannian manifold such that we can leverage the machinery of Riemannian optimization. Importantly, working in the parameter space allows us to apply an explicit Euler scheme, instead of the implicit formulation of Eq. 3, when optimizing the Gaussian distributions' parameters.

According to the above splitting scheme, we formulate the policy optimization as a two-step procedure that alternates between the Gaussian parameters (i.e. means and covariance matrices) and the GMM weights. To optimize the former, we propose to leverage the Riemannian structure of the \(BW\) manifold to reformulate the updates as a forward discretization, similarly to . In other words, by embedding the Gaussian components of the GMM policy in a Riemannian manifold, the Wasserstein gradient flow in the implicit form of Eq. 5 can be approximated by an explicit Euler update scheme according to the \(BW\) metric (further details are provided in App. A.4). This allows us to leverage the expressions of the Riemannian gradient and exponential map of the \(BW\) manifold [59; 60]. Thus, the optimization boils down to Riemannian gradient descent where the gradient is defined w.r.t the Bures-Wasserstein metric. In particular, we use the expression for Riemannian gradient, metric and exponential map used in . Formally, the resulting updates for the Gaussian parameters of the GMM follow the Riemannian gradient descent scheme given by:

\[}_{k+1}=_{}_{k}}( _{}}\,J(_{k})), }_{k+1}=_{}_{k}}(_{}}\,J( _{k})),\] (12)

where \(\) denotes the Riemannian gradient w.r.t. the Bures-Wasserstein metric, \(_{}:_{} \) denotes the retraction operator, which maps a point on the tangent space \(_{}\) back to the manifold \(\). Moreover, \(\) is a learning rate and \(_{k}}}{{=}}(}_{ k},}_{k},}_{k})\). The Euclidean gradients of \(J()\) required for computing \(\) can be obtained using a likelihood ratio estimator (a.k.a score function estimator or REINFORCE)  and are provided in App. A.3.

Concerning the GMM weights, we first reparameterize them as \(_{j}=}{_{k=1}^{N}_{k}}\) and optimize w.r.t. the new parameters \(_{j},j=1...N\), which unlike \(}\) are unconstrained. For this optimization we employ the implicit Euler scheme:

\[}_{k+1}=*{arg\,min}_{}}(^{2}(_{k+1}(}),_{k})}{2}-J (_{k+1}(}))),\] (13)where \(_{k+1}(})}}{{=}}(}_{k+1},}_{k+1},})\). We minimize Eq. 13 by gradient descent w.r.t. \(\) as follows:

\[}_{k+1}=}_{k}-_{}}\, (^{2}(_{k+1}(),_{k})}{}-J(_{k+1}(}))).\] (14)

The gradient of \(J()\) can be obtained analytically using a likelihood ratio estimator. For the Wasserstein term, we first compute the gradient w.r.t. the weights via the Sinkhorn algorithm , from which the gradient w.r.t \(\) can be then obtained via the chain rule. Note that we have to rely on the Sinkhorn algorithm here since there is no analytic solution available for the Wasserstein distance between discrete distributions, unlike the above case of the Gaussian components. Consequently, we cannot compute the corresponding gradients.

### Implementation Pipeline

To carry out the policy optimization, we proceed as in the usual on-policy RL scheme: We first roll out the current policy to collect samples of state-action-reward tuples. Then, we use the collected interaction trajectories to compute a sample-based estimate of the functional \(K_{r}()\) and its gradients w.r.t the policy parameters, as explained in SS 3.1. An optimization step consists of alternating between optimizing the Gaussian parameters using Eq. 12, and updating the weights via Eq. 14. For the optimization of the Gaussian parameters we leverage Pymanopt  for Riemannian optimization. We extended this library by implementing the Bures-Wasserstein manifold based on the expressions provided by Han et al.  (see App. A.2 for details). Furthermore, we added a custom line-search routine that accounts for the constraint on the Wasserstein distance between the old and the optimized GMM, as to our knowledge such a search method does not exist in out-of-the-box optimizers. The details of this custom line-search are given in Algorithm 2 in App. A.5. Regarding the optimization of the GMM weights, we use POT , a Python library for optimal transport, from which we obtain the quantities required to compute the gradients of the Wasserstein distance w.r.t. the weights in Eq. 14.

``` Input: initial policy \((|)\)
1:while not goal reached do
2: Rollout policy \((|)\) in the environment for \(M\) episodes to collect interaction trajectories \(=\{(_{0},_{0},_{0}),(_{1},_{1},_{1}), ,(_{T},_{T},_{T})\}_{m=1}^{M}\)
3:repeat
4: Update Gaussian components parameters \(}\), \(}\) using Riemannian optimization (12), where \(^{ls}\) is determined via line-search (see SS3.2).
5:until convergence
6:repeat
7: Update GMM weights \(}\) via gradient descent on the free energy objective 10, using 14
8:until converged
9:endwhile ```

**Algorithm 1** GMM Policy Optimization via Wasserstein Gradient Flows

The full policy optimization finishes if either the objective stops improving or the Wasserstein distance between the old and optimized GMMs exceeds a predefined threshold, which is chosen experimentally. Afterwards, fresh rollouts are performed with the updated policy and the aforementioned two-step alternating procedure starts over. This optimization loop is repeated until a task-specific success criterion has been fulfilled. We summarize the proposed optimization in Algorithm 1.

``` Input: initial policy \((|)\)
1:while not goal reached do
2: Rollout policy \((|)\) in the environment for \(M\) episodes to collect interaction trajectories \(=\{(_{0},_{0},_{0}),(_{1},_{1},_{1}), ,(_{T},_{T},_{T})\}_{m=1}^{M}\)
3:repeat
4: Update Gaussian components parameters \(}\), \(}\) using Riemannian optimization (12), where \(^{ls}\) is determined via line-search (see SS3.2).
5:until convergence
6:repeat
7: Update GMM weights \(}\) via gradient descent on the free energy objective 10, using 14
8:until converged
9:endwhile ```

**Algorithm 2** GMM Policy Optimization via Wasserstein Gradient Flows

## 4 Experiments

We tested our approach in three different robotic settings: a reaching skill, a collision-free trajectory tracking, and a multiple-goal task. All these tasks were represented in a \(2\)D operational space. Moreover, we tested our approach on two additional versions of the collision-free trajectory tracking task that are formulated for \(3\)D operational-space and \(7\)D joint-space representations of the end-effector state and action. These two latter tasks were performed by a \(7\)-DoF Franka Emika Panda robot in a virtual environment as reported in App. A.6.3. All robot motion policies were initially learned from human demonstrations collected on a Python GUI. We assumed we were given \(M\) demonstrations, each of which contained \(T_{m}\) data points for a dataset of \(T=_{m}T_{m}\) total observations\(=\{(_{t},_{t})\}_{t=1}^{T}\). The state \(\) and action \(\) corresponded to the robot end-effector position \(^{d}\) and velocity \(}^{d}\), with \(d\) being the operational space dimensionality. The GMM models were initially trained via classical Expectation-Maximization. The policy rollout consisted of sampling a velocity action \(_{t}(_{t}|_{t})\) using Eq. 9, and subsequently commanding the robot via a Cartesian velocity controller at a frequency of \(100\). For all the experiments, we used the Robotics Toolbox for Python  to simulate the robotic environments.

To show the importance of accounting for the policy structure in RL settings, we compared our method against two structure-unaware baselines: PPO  and SAC-GMM . Our baselines choice is motivated by the fact that these two RL methods are still widely used and fairly competitive , even in real robotic scenarios . We also considered an additional baseline: PMOE , which is a method to train deep RL policies using a probabilistic mixture of experts via GMMs. As PPO was not originally designed to directly optimize the parameters of a previously-learned GMM policy, we designed the policy actions to represent (usually small) corrections to the GMM parameters, i.e. \(=[~{}}~{}~{}( })]\), following the same methodology as SAC-GMM . The PPO and SAC implementations correspond to the code provided by Stable-Baselines3 , whose policies are parametrized by MLP networks. The PMOE implementation corresponds to the code given in the original paper . During policy optimization, we sampled an action from the MLP policy that was then used to update the GMM parameters by adding the computed corrections to the current parameters. Later, we proceeded as described earlier, namely, the updated GMM was used to compute the velocity action via Eq. 9. For comparison purposes, we report statistical results for all the settings over \(5\) runs for the task success rate and solution variance. We tuned the baselines separately for each task using Optuna . In addition, to assess the importance of our Riemannian formulation, we performed an ablation where we used the implicit scheme based on Euclidean gradient descent instead of the explicit optimization on the Bures-Wasserstein manifold (see App. A.6.2).

### Tasks description

Reaching Task:This experiment consists of: (1) learning an initial GMM policy such that the robot end-effector reaches a target by following an L-shape trajectory from its initial position, and (2) adapting the GMM policy to reach a new target located midway and above the previously-learned L-shape trajectories. The initial policy, shown in Fig. 2_-left_ and Fig. 12(a), was learned from \(12\) demonstrations and encoded by a \(7\)-component GMM. To adapt the policy, we defined a dense reward as a function of the position error between the robot end-effector and the new target. We also added a sparse penalty term that punishes rollouts leading to significantly divergent trajectories. Each optimization episode comprised \(10\) rollouts, each of maximum horizon length of \(200\) iterations. Convergence is achieved when a minimum average position error w.r.t the target - computed over an episode - is reached.

Collision-avoidance Task:This task consists of: (1) learning an initial GMM policy of a linear reaching motion, and (2) adapting the GMM policy to reach a new horizontally-translated target while avoiding to collide with two spherical obstacles located midway between the initial robot position and the new target. The initial GMM policy was learned from \(10\) human demonstrations and represented by a \(3\)-component GMM, as shown in Fig. 2_-middle_ and Fig. 12(b). For policy optimization, we defined a sparse reward as a function of the position error between the robot end-effector position and the target at the end of the rollout. We also included two sparse penalty terms: the first one punishes rollouts leading to collisions with the obstacles, for which the rollout is stopped; the second term penalizes rollouts with significantly divergent trajectories. Each episode consisted of \(10\) rollouts, each of maximum horizon length of \(150\) iterations. Convergence is determined by a minimum average position error w.r.t the target computed over an episode.

Multiple-goal Task:This setting involves: (1) learning an initial GMM policy where the robot end-effector reaches two different targets (i.e., task goals) starting from the same initial position, and (2) adapting the initial policy to reach a new target located close to one of the previous task goals. The intended adapted behavior should make the robot go through the most relevant GMM components according to the new target location. The initial GMM policy was learned from \(12\) demonstrations and encoded by a \(6\)-component GMM, as shown in Fig. 2_-right_ and Fig. 12(c). To optimize the initial GMM policy, we specified a sparse reward based on the position error between the robot end-effector position and the chosen target at the end of the rollout. Similar to the previous experiments, we added a sparse penalty term to penalize rollouts generating significantly divergent trajectories. Anepisode comprised \(10\) rollouts, each of maximum horizon length of \(200\) iterations. Again, the policy optimization converges when the average position error w.r.t the chosen target reaches a minimum threshold.

### Results Analysis

The reaching task tested our method's ability to adapt a previously-learned reaching skill to a new goal, located at \((6.0,-6.5)\) (cf. Fig. 13a-_left_). Achieving this required to adapt the Gaussian parameters of mainly the last four GMM components, while the others remained unchanged. We compared all methods in terms of the success rate over environment steps, where the success rate was defined as the percentage of rollouts reaching the new goal. Figure 3-_left_ shows that our method achieved a success rate of \(1\) after approximately \(70000\) environment interactions. Despite PPO was also able to complete the task reliably, it required many more environment steps (cf. Fig. 5-_left_). In sharp contrast, SAC did not reach any improvement. These observations underline the importance of some kind of trust region or constraint on the policy updates, which allowed both our method and PPO to reach good success rates. The PMOE versions of PPO and SAC showed a better performance than the vanilla counterparts, which aligns with the experimental insights provided in the original PMOE paper  regarding the advantages of the probabilistic mixture of experts in deep RL policies. Nevertheless, our approach also outperformed both PMOE versions. This experiment showed that our method is much more sample-efficient in adapting the GMM parameters, which we attribute to the fact that our method explicitly takes the GMM structure into account in the formulation of the optimization.

In the collision-avoidance task, we tested whether our method was able to adapt a trajectory tracking skill in order to avoid collisions with newly added obstacles. These were placed in such a way that the robot was forced to move its end-effector through a narrow path between the obstacles (cf. Fig. 2-_middle_). While the reaching task could be adapted by mainly varying the means of the GMM components, this task also demands to adapt the covariance of the second GMM component. Figure 3-_middle_ shows that our method solved this task reliably after comparatively few environment interactions. Although PPO also achieved a success rate of \(1\), it took \(6\) times more environment steps than our method. SAC only reached an average success rate of \(0.8\), however with high variance (cf. Fig. 5-_middle_). Note that the PMOE versions showed a reduced solution variance and a higher success rate than their vanilla counterparts, where PMOE-PPO also outperformed PMOE-SAC. These results again show the importance of the constraints on the policy updates. The huge discrepancy in the required environment steps between our method and all the baselines further emphasizes the importance of taking the GMM structure into account in the policy optimization. The explicit consideration of this structure allows our method to identify the required changes to the GMM parameters much more efficiently. These experimental insights can also be observed in the results corresponding to the other formulations of the collision-avoidance task, namely, the \(3\)D operational and \(7\)D joint space representations, reported in App. A.6.3.

While the previous two tasks were accomplished by adapting mostly the Gaussian parameters of the GMM, the multiple-goal task required to adapt the GMM weights. The initial skill comprised

Figure 2: The three tested \(2\)D robotic settings: a reaching skill (_left_), a collision-free trajectory tracking (_middle_), and a multiple-goal task (_right_). The robot color goes from light gray to black to show the evolution of the task reproduction. Green Gaussian components () depict the initial GMM policy, projected on the \(2\)D Cartesian position space. The end-effector trajectory resulting from the initial GMM policy is shown in dark blue lines (). Red circles () in the collision-avoidance task represent the obstacles (_middle_). The different targets of the multiple-goal task (_right_) are depicted as red stars.

reaching motions to two different goals and an execution of this skill results in reaching one of them, depending on the sampling noise (cf. Fig. 13c). The easiest way to adapt the policy to reach only one of the two goals is to reduce the GMM weights of the components belonging to the undesired motion and correspondingly increase the weights of the other components. As shown in Fig. 3-_right_, our method again quickly achieved a success rate of \(1\). PPO required substantially many more environment steps, while SAC was not able to solve the task. Again, the PMOE formulation reduced the solution variance and slightly increased the success rate of both PPO and SAC.

In Fig. 4 we report the success rate variance over \(5\) runs at a fixed time step, which corresponded to the step at which the first method achieved a success rate of \(1\), thus prioritizing sample efficiency. The plots show that our method exhibits a very low solution variance. All baselines varied largely, except for the reaching task, where all SAC runs collapsed to a success rate of \(0\). These results show that our method, despite showing large variance at the start, was able to quickly reduce the variance and converge reliably to a good success rate. We also provide similar plots of solution variance in Fig. 6, where we report the results for both PPO and SAC using their own convergence time step.

## 5 Conclusions and Future Work

We presented a novel method for GMM policy optimization, which leverages optimal transport theory to formulate the policy optimization as a Wasserstein gradient flow on the manifold of GMMs. Our formulation explicitly accounts for the GMM structure in the optimization and furthermore enables us to naturally constrain the policy updates by the \(L^{2}\)-Wasserstein distance between GMMs to enhance the stability of the policy optimization process. Moreover, the embedding of the Gaussian components of the GMM policy in the Bures-Wasserstein manifold greatly reduced the computational cost of the policy optimization. Experiments on several robotic tasks provided strong evidence of the importance of our policy-structure aware optimization against approaches that disregard the GMM structure. A possible limitation of our method is that each optimization loop involves running the Sinkhorn algorithm, which is computationally expensive. This might be improved by employing recent advances on initializing the Sinkhorn algorithm . Also, we observed an intricate interplay between the optimization of the GMM weights and the Gaussian parameters, which occasionally resulted in one update hampering the other. In future work we plan to address the latter problem by using separate adaptive learning rates for weights and Gaussian parameters. Another possibility would entail to reformulate the policy optimization as a gradient flow on the space of probability measures endowed with the Wasserstein Fisher-Rao metric [72; 73; 74]. This alternative metric may allow us to leverage the Fisher-Rao geometry to optimize the GMM weights, as very recently proposed in  to learn isotropic Gaussian mixtures via particle-based approximations. Finally it would be interesting to combine our method with an actor-critic formulation and to replace the multi-step cumulative reward by a trained \(Q\)-function.

Figure 4: Variance of the success rate over the \(5\) runs for our method (WGF) and the baselines on the reaching task (_left_), the collision avoidance task (_middle_) and the multiple-goal task (_right_). The violate plots are overlaid with box plots, quartile lines and a swarm plot, where dots indicate the success rates of individual runs. The plots show the variance at the following time steps from left to right: \(80000\), \(90000\), \(95000\).

Figure 3: Success rate of our method (WGF) and the baselines on the reaching (_left_), the collision-avoidance (_middle_) and the multiple-goal tasks (_right_). The shaded area depicts the standard deviation over \(5\) runs.