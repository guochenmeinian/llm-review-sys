# Universal Neural Functionals

Allan Zhou

Stanford University

ayz@cs.stanford.edu &Chelsea Finn

Stanford University

&James Harrison

Google DeepMind

jamesharrison@google.com

###### Abstract

A challenging problem in many modern machine learning tasks is to process _weight-space features_, i.e., to transform or extract information from the weights and gradients of a neural network. Recent works have developed promising weight-space models that are equivariant to the permutation symmetries of simple feedforward networks. However, they are not applicable to general architectures, since the permutation symmetries of a weight space can be complicated by recurrence or residual connections. This work proposes an algorithm that _automatically_ constructs permutation equivariant models, which we refer to as _universal neural functionals_ (UNFs), for any weight space. Among other applications, we demonstrate how UNFs can be substituted into existing learned optimizer designs, and find promising improvements over prior methods when optimizing small image classifiers and language models. Our results suggest that learned optimizers can benefit from considering the (symmetry) structure of the weight space they optimize. We open-source our library for constructing UNFs at https://github.com/AllanYangZhou/universal_neural_functional.

## 1 Introduction

Many problems in machine learning require handling _weight-space features_, such as the weights, gradients, or sparsity masks of neural networks. For example, optimizers iteratively map the current weights and gradient history to updated weights. Taking this perspective, researchers have proposed a variety of data-driven methods that train a neural network to process these weight-space features. Examples applications of these _neural functionals_ include training neural networks to predict classifier generalization from weights , to optimize other networks , and to classify or edit implicit neural representations (INRs) .

Until recently, researchers lacked a unifying and principled framework for designing neural functionals, and would implement a custom model for their particular weight-space task. A significant recent advance was the development of weight-space models that are _permutation equivariant_. _Neuron permutation_ symmetries arise in a neural network's weight space because re-ordering hidden neurons has no effect on the network's function . A permutation equivariant neural functional can guarantee that under a neuron permutation of its input, its output permutes accordingly.

Navon et al.  showed that permutation equivariance significantly improves performance on weight-space tasks, but their models only apply to the weight spaces of simple feedforward multilayer perceptrons (MLPs). Permutation equivariant neural functionals  added the ability to process weights from simple feedforward convolutional networks (CNNs). However, in practice we may deal with the weight spaces of complex networks that have residual connections, recurrence, normalization layers, and so on. Extending existing approaches to each possible weight space would be tedious and challenging.

We propose an approach that automatically constructs permutation equivariant models for _any_ collection of tensors whose dimensions can permute according to a shared set of permutations. This naturally encompasses the permutation equivariance we might desire for any given weight space. We show that our algorithm constructs the most general linear layer that operates on a given weight space while guaranteeing equivariance to the specified permutation symmetries. Stacking multiple such layers with pointwise nonlinearities produces a deep permutation equivariant model, which we refer to as a _universal neural functional_.

To evaluate the empirical effectiveness of UNFs, we apply them to tasks that require processing networks with complex architectures containing recurrence, layer normalization, residual connections, and more. We use UNFs to implement learned optimizers and then optimize small image classifiers, RNNs, and Transformer language models, observing promising improvements over prior methods. In a generalization prediction task, we use UNF to predict the performance of sequence-to-sequence RNN models from their weights. Our experiments show that universal neural functionals are flexible, can be easily applied to different weight spaces, and improve upon prior weight-space methods.

## 2 Preliminaries

We largely follow or extend the notation and naming of Zhou et al. (2023). Given a fixed neural network architecture, there is a **weight space**\(\) of possible parameters (weights, biases, normalization scalings, etc.). We refer to all such parameters as "weights". A particular set of weights \(W=(W^{(1)},,W^{(L)})\) contains multiple "tensors", or multidimensional arrays. Depending on the architecture, \(\) contains numerous symmetries Hecht-Nielsen (1990); Godfrey et al. (2022), i.e., transformations on the weight space that do not affect the network's behavior. Following prior work Navon et al. (2023); Zhou et al. (2023), this work focuses only on the permutation symmetries, which are called _neuron permutations_.

Neuron permutations correspond to re-arranging the neurons within (hidden) layers, which have no canonical ordering. We make the simplifying assumption that _all_ layers can be re-arranged-this assumption can be later corrected using positional encodings Zhou et al. (2023). Assuming there are \(N\)_independently_ permutable layers of neurons, the neuron permutation _group_ is the direct product \(=S_{n_{1}} S_{n_{N}}\), where \(n_{i}\) is the number of neurons being permuted in each layer.

In general, each weight is a "tensor" (multi-dimensional array) of real numbers. Using \(M(a,b,)\) to denote arrays \(^{a b}\), consider a rank-\(D_{}\) tensor \(W^{()} M(n_{d^{}_{1}},,n_{d^{}_{D_{}}})\). Each dimension \(d^{}_{i}\) is permuted by \(_{d^{}_{i}}\). That is, the action of \(\) on the indices of the weight tensor is:

\[(i_{1},,i_{D_{}}):=(_{d^{}_{1}}(i_{1 }),,_{d^{}_{D_{}}}(i_{D_{}})).\] (1)

Figure 1: Illustration of the permutation symmetries in the weight space of a recurrent neural network (Example 2.2). **Left**: Each layer contains _feedforward_ (ff) weights mapping between different layerâ€™s activations, and _recurrent_ (rec) weights transforming activations over time. We can permute the hidden activations as illustrated without changing the final outputs \(h^{L}_{t}\). **Right**: Permuting the hidden activations induces a permutation on the weights. Here, the rows and columns of the feedforward weights are permuted by \((_{+1},_{})\), while the recurrent weights are permuted by \((_{},_{})\). Our algorithm automatically constructs permutation equivariant models for any collection of weight tensors given a description of its symmetries (Appendix A).

Defining the multi-index \(:=(i_{1},,i_{D_{}})\), the action on the weight tensor is to permute the entries: \([ W^{()}]_{}:=W_{^{-1}()}^{( )}\), and the action on \(\) is \( W:=( W^{(1)},, W^{(L)})\).

We now elaborate on the definition of the group and action in several common cases.

**Example 2.1** (Multilayer perceptron).: _A multilayer perceptron (MLP) with \(L+1\) layers has activations \(h^{+1}=s(W^{()}h^{}+b^{(+1)})\), with \(h^{1}\) being the first (input) layer and \(h^{L+1}\) the output. If each \(h^{}\) is a vector of length \(n_{}\), then the weights are matrices \(W^{()} M(n_{+1},n_{})\) and the biases are vectors \(b^{()} M(n_{})\). Then we have a neuron permutation group \(=S_{n_{1}} S_{n_{L+1}}\), and \(\) can be written \(=(_{})_{=1}^{L+1}\). The action on the weights and biases is:_

\[W^{()} P(_{+1})W^{()}P(_{ })^{} b^{()} P(_{} )b^{()},\] (2)

_where \(P(_{})\) is the \(n_{} n_{}\) permutation matrix corresponding to \(_{}\). This corresponds exactly to the "NP" setting in Zhou et al. (2023)._

**Example 2.2** (Recurrent neural network).: _Consider a deep recurrent neural network (RNN) (Elman, 1990) without biases. We follow the presentation of Wang et al. (2023):_

\[h_{t}^{+1}=s(W_{}^{+1}h_{t-1}^{+1}+W_{}^{ }h_{t}^{}),\] (3)

_where \(h_{t}^{1}\) are the inputs and \(h_{t}^{L+1}\) are the outputs at each timestep, with \(h_{0}^{}\) initialized to \(0\). The weight space consists of feedforward (ff) weights \(W_{}^{} M(n_{+1},n_{})\) and recurrent (rec) weights \(W_{}^{} M(n_{},n_{})\). We again define the neuron permutation group \(S:=S_{n_{1}} S_{n_{L+1}}\), but the action of the group on the weight space is now different. Here, re-arranging the neurons corresponds to transforming the weights:_

\[W_{}^{} P(_{+1})W_{}^{} P(_{})^{} W_{}^{} P(_{})W_{}^{ }P(_{})^{}.\]

_As illustrated by Figure 1, the feedforward weights transform just as in the MLP case (Eq. 2), but the recurrent weights' rows and columns must be transformed by the same permutation._

**Example 2.3** (Convolutional neural network).: _Consider a 1D convolutional neural network (CNN) without biases. Using \(\) to denote cross-correlation, we have activations \(h^{+1}=s(W^{()} h^{})\), where the input is \(h^{1}\) and the output is \(h^{L+1}\). If each filter has spatial dimension \(k_{}\) and each \(h^{}\) has \(n_{}\) channels, then we have rank-\(3\) weight tensors \(W^{()} M(n_{+1},n_{},k_{})\) and neuron permutation group \(=_{=1}^{L}S_{n_{}} S_{k_{}}\). Looking at how each dimension of \(W^{()}\) permutes, we would have \(_{n_{+1}} S_{n_{+1}}\) permute the first dimension (output channels), \(_{n_{}} S_{n_{}}\) permute the second dimension (input channels), and \(_{k_{}} S_{k_{}}\) permute the third dimension (spatial)._

_We note that permutating the spatial dimensions of a convolution filter would change the CNN's behavior and is not a true symmetry of the weight space. This is a notable difference between how our framework handles convolutional weight spaces compared to NFNs (Zhou et al., 2023), where the action of the neuron permutation group does not affect the spatial dimensions at all. Assuming that all dimensions of each weight tensor can permute simplifies the development of our framework, and undesired symmetry can be broken (if desired) by positional encodings of the input (Zhou et al., 2023; Lim et al., 2023)._

**Equivariance and invariance.** We are interested in functions \(T:\) that are equivariant, meaning that it doesn't matter whether we apply a neuron permutation to the input or the output. We define \(_{}(,)\) as the space of equivariant linear maps, i.e., those \(T\) satisfying:

\[T( W)= T(W),,W .\] (4)

Our goal is to design a layer (i.e., a parameterized space of functions) that is equivalent to \(_{}(,)\).

In some applications, we may instead desire invariance, that is a function \(P\) satisfying

\[P( W)=P(W),,W .\] (5)

Following prior work (Navon et al., 2023; Zhou et al., 2023), we can build invariant neural functionals by composing several equivariant layers with an invariant pooling layer, e.g., one that sums over every dimension of each weight tensor and concatenates the results.

Universal neural functionals

Since equivariance is preserved under composition, and pointwise non-linearities are already permutation equivariant, we can build deep equivariant models as long as we have an equivariant linear layer. Additionally, composing equivariant layers with an invariant pooling operation produces a deep invariant model. This section introduces a method for producing equivariant weight-space layers for any given weight space, which enables the flexible construction of _universal neural functionals_.

### Decomposing equivariant weight-space maps

The weight space is a direct sum of individual weight subspaces \(=^{(1)}^{(L)}\), so the problem of defining an equivariant layer on \(\) can be decomposed into defining equivariant layers between each pair of weight subspaces \(^{(m)}\) and \(^{()}\), for all \(\) and \(m\)(Navon et al., 2023).

We re-state this result in our own notation. For any \(,m\) pair we define \(_{}(^{(m)},^{()})\) as the space of equivariant maps between the two weight subspaces. It contains all \(T^{ m}:^{(m)}^{()}\) satisfying

\[T^{ m}( W^{(m)})= T^{ m}(W^{(m)}) ,W^{(m)},\] (6)

noting that the action on the left and right hand sides of the equivariance condition are not, in general, the same.

Assume that we already have a basis \(^{sp}\) for \(_{}(^{(p)},^{(s)})\). A basis function \(E^{sp}\) can be extended to \(:\) by defining:

\[(W)^{}:=E(W^{(p)})&=s\\ 0&,\] (7)

where \((W):=(^{1}(W),,^{L}(W))\).

**Theorem 3.1** (Navon et al. (2023)).: _Let \(\{^{ m}\}\) be bases for each \(_{}(^{(m)},^{()})\). Then the union of these bases (extended by Eq. 7) is a basis for linear equivariant maps on \(\). That is, we have the basis \(\) for \(_{}(,)\) defined:_

\[=_{,m[L]^{2}}\{\ \ \ E ^{ m}\ \}.\] (8)

This result tells us that we can construct an equivariant basis \(\) for \(_{}(,)\) by simply combining the equivariant bases \(\{\ ^{ m}\ \}\) for each pair of weight subspaces.

### Equivariant layers between tensors

Since weights are tensors, our decomposed problem involves finding bases for permutation equivariant maps between tensors. Variants of this problem have been studied by numerous prior works-in particular, Maron et al. (2018) theoretically characterize a basis for equivariant maps between arbitrary-rank tensors, and provide a concrete implementation of the basis functions in the rank-\(2\) case. Here, we describe a _general_ algorithm that automatically constructs a basis for permutation equivariant maps between arbitrary-rank tensors. Concretely, it implements each basis function in terms of simple array operations that are amenable to efficient computation with modern deep learning frameworks.

```
0:\(^{(m)},^{()}\)
1:Initialize basis \(^{ m}\{\ \}\)
2:\(\{\ o_{1},,o_{D_{}},i_{1},,i_{D_{m}}\}\)
3:for\(\) in ValidPartitions\(()\)do
4: Label each subset \(s_{p}\) by unique character char\((s_{p})\)
5:for\(\)do
6: Map index \(c[](s_{p})\) where \( s_{p}\)
7:endfor
8:\(E_{}(X)_{c[_{1}],,c_{|D_{}|}}:=_{}X _{c[i_{1}],,c[i_{D_{m}}]}\)
9:\(^{ m}^{ m}\{\,E_{}\,\}\)
10:endfor
11:return\(^{ m}\) ```

**Algorithm 1** Basis for equivariant \(^{(m)}^{()}\) layer Functions in \(_{}(^{(m)},^{()})\) take input tensors indexed by \(\{\,i_{1},,i_{D_{m}}\,\}\) and produces output tensors indexed by \(\{\,o_{1},,o_{D_{}}\,\}\). We can construct a basis \(^{ m}\) for this space where each element is identified by a **valid partition**\(\) of these indices. Recall that the indices \((i_{1},i_{2},)\) of \(W^{(m)}\) are permuted by \((_{d^{m}_{1}},_{d^{m}_{2}},)\). We say that two indices \(i_{1}\) and \(i_{2}\) "permute simultaneously" if \(d^{m}_{1}=d^{m}_{2}\).

**Definition 1**.: _A **valid partition** is a partition \(\) of the output and input indices \(=\{\,o_{1},,o_{D_{}},i_{1},,i_{D_{m}}\,\}\) into non-empty subsets, such that each subset only contains indices that are permuted simultaneously._

**Example 3.1** (\(^{(m)}=^{()}=^{n_{1} n_{2}}\)).: _Here the output and input indices are \(\{\,o_{1},o_{2},i_{1},i_{2}\,\}\). The partition \(\{\,o_{1},o_{2}\,\},\{\,i_{1},i_{2}\,\}\,\}\) is **not** valid because \(o_{1},o_{2}\) are permuted by \(_{1},_{2}\), so they do not permute simultaneously. On the other hand, \(\{\,\{\,o_{1},i_{1}\,\}\,,\{\,o_{2},i_{2}\,\}\,\}\) is a valid partition._

**Example 3.2** (\(^{(m)}=^{()}=^{n_{1} n_{1}}\)).: _This time, the partition \(\{\,\{\,o_{1},o_{2}\,\}\,,\{\,i_{1},i_{2}\,\}\,\}\) is **valid** because \(o_{1},o_{2}\) are both permuted by \(_{1}\), as are \(i_{1},i_{2}\)._

To construct the equivariant basis, we enumerate all valid partitions and then map each partition \(\) to a basis function \(E_{}\). Concretely, we label each subset of \(\) with a distinct character \(,,,\) and then remap each of our original indices \(\{\,o_{1},,o_{D_{}},i_{1},,i_{D_{m}}\,\}\) to a a character based on which subset the index was in. This mapping is best illustrated by continuing our previous example.

**Example 3.3** (\(^{(m)}=^{()}=^{n_{1} n_{2}}\)).: _Here input and output are both matrices, with combined indices \(\{\,o_{1},o_{2},i_{1},i_{2}\,\}\). We have two permutations \((_{1},_{2}) S_{n_{1}} S_{n_{2}}\) that can act on the rows and columns of the input and output matrices. There are four valid partitions:_

\[_{1}=\{\,\{\,o_{1},i_{1}\,\}\,,\{\,o_{2},i_{2}\,\}\,\}\,, _{2}=\{\,\{\,o_{1},i_{1}\,\}\,,\{\,o_{2}\,\}\,,\{\,i_{2}\,\}\,\}\,,\] \[_{3}=\{\,\{\,o_{1}\,\}\,,\{\,i_{1}\,\}\,,\{\,o_{2},i_{2 }\,\}\,\}\,, _{4}=\{\,\{\,o_{1}\,\}\,,\{\,o_{2}\,\}\,,\{\,i_{1}\,\}\,,\{\,i_{2 }\,\}\,\}\,.\] (9)

_Consider \(_{2}\)-we assign a character to each subset:_

\[_{2}=\{\,,i_{1}\,\}}_{},\,\}}_{},\,\}}_{}\,\}\,.\] (10)

_which tells us to remap the output indices \((o_{1},o_{2})(,)\) and the input indices \((i_{1},i_{2})(,)\), producing the basis function \(E_{_{2}}(W^{(m)})_{}:=_{}W^{(m)}_{ }\), where summation over \(\) can be inferred because it only contains an input index._

_Repeating this index-remapping process for each valid partition will generate a total of four basis functions \(E_{_{1}},,E_{_{4}}\) for \(_{}(^{(m)},^{()})\). Our equivariant \(^{(m)}^{()}\) layer will be defined as the linear combination \(T^{ m}(W^{(m)};):=_{k=1}^{4}_{k} E_{ _{k}}(W^{(m)})\), which is the layer introduced in Hartford et al. ._

To generalize the previous example, for each valid partition of the indices \(\) we label its subsets with characters \(,,,\) and then construct a basis function:

\[E(W^{(m)})_{c[o_{1}],,c[o_{D_{}}]}=_{}W^{(m)}_{c[i_{1} ],,c[i_{D_{m}}]},\] (11)

where \(c[]\) maps each index to the subset of \(\) that contains it. We sum over the characters in \(\), which is the (possibly empty) subset of characters that only contain input indices (i.e., only appear on the right-hand side). Entries that are not explicitly assigned by the left-hand side are \(0\). Algorithm 1 gives a formal description of the complete process for generating \(^{ m}\).

**Theorem 3.2**.: _Algorithm 1 produces a basis for the equivariant linear maps from \(^{(m)}\) to \(^{()}\)._

_Proof. See Appendix B.1._

Once Algorithm 1 has generated a basis of equivariant functions \(^{ m}\), we can implement an equivariant layer using a vector \(^{ m}^{|^{ m}|}\) of learned coefficients:

\[T^{ m}(W^{(m)};^{ m}):=_{b=1}^{|^{ m }|}^{ m}_{b} E_{_{b}}(W^{(m)}).\] (12)

### Equivariant layers on weight spaces

Theorem 3.1 now tells us that we may now construct the equivariant weight-space layer by combining the bases \(\{\,^{ m}\,\}\) into a basis \(\) of functions on \(\). The weight-space layer \(T(,)\) can then be defined by a linear combination of the basis functions with learned coefficients \(\). Explicitly, the full layer is defined:

\[T(W,)=(T^{1}(W,^{1,:}),,T^{L} (W,^{L,:})),\] (13)

where \(^{,:}=\{\,^{ m}=1,,L\,\}\) and \(T^{}(W,^{,:})=_{m=1}^{L}T^{ m}(W^{(m)}, ^{ m})\).

Appendix A provides a concrete description of how we specify the weight space in code and how the algorithm is then used to automatically construct an equivariant weight space layer. Our open-source implementation is compatible with most JAX (Bradbury et al., 2018) neural network libraries.

**Theorem 3.3**.: _The weight-space layer (Eq.-13) is \(\)-equivariant, and can express any linear equivariant function on \(\)._

_Proof._ Each \(T^{ m}\) is a linear combination of basis functions in \(^{ m}\). Then, as described by Thm 3.1, Eq. 13 is a linear combination of functions that form a basis for \(_{}(,)\).

For an MLP weight space with neuron permutation group defined as in Example 2.1, this approach will generate the exact same layer as \(_{}\)(Zhou et al., 2023a). This is because the layers each parameterize all possible linear maps equivariant to the same symmetry group, and hence can express the same set of functions.

### Multiple feature channels

In practice, we may be interested in simultaneously processing multiple weight-space features, such as the weights and a history of gradients. These features can be stacked into a "channel" dimension analogous to the channels of convolutional networks. In that case, we must consider direct sums of weight spaces of the form \(^{c}=_{k=1}^{c}\), with elements that can be written as1\(W=(W,,W[c])\), for \(W[k]\). Then the action is \( W:=( W,, W[c])\) for \(\), extending the (single channel) definition. The definition of equivariance can then be extended to layers of the form \(T():^{c_{i}}^{c_{o}}\), where \(c_{i},c_{o}\) are the number of input and output channels.

Extending equivariant layers to the multi-channel setting is quite common in the geometric deep learning literature and simply involves taking linear combinations along the channel dimension (Cohen and Welling, 2016; Ravanbakhsh et al., 2017). That is, we modify the equivariant layer between subspaces as:

\[T^{ m}(W^{(m)};^{ m})[k^{}]:=_{b=1}^{| ^{ m}|}_{k=1}^{c_{i}}_{b}^{ m}[k^{},k]  E_{_{b}}(W^{(m)})[k],\] (14)

where each \(_{b}^{ m}\) is now a learned \(c_{o} c_{i}\) matrix instead of a scalar.

### Deep models

The previous sections describes the construction of \(\)-equivariant layers that operate operate on weight-space features in \(^{c}\). We construct _universal neural functionals_ by stacking multiple such layers (interleaved with pointwise non-linearities) into a deep, permutation equivariant model that can process weights. To construct a permutation invariant model, we can add an invariant pooling layer after the equivariant layers, as in prior work (Navon et al., 2023; Zhou et al., 2023a).

## 4 Experiments

In this section, we refer to weight-space models constructed using our algorithm as **universal neural functionals (UNFs)**. We compare their performance to prior methods on two types of weight-space tasks: predicting the generalization of recurrent sequence-to-sequence models, and training learned optimizers for a variety of architectures and datasets.

### RNN generalization prediction

One promising application of neural functionals is in predicting the generalization of neural network models from their weights Eilertsen et al. (2020). We construct **Tiny RNN Zoo2**, a dataset of recurrent neural networks trained to do arithmetic by completing given questions character-by-character. For example, given the input string "15+20=" the correct completion would be "35<EOS>". To construct the dataset, we train \(10^{4}\) sequence-to-sequence (Sutskever et al., 2014) models on example problems with input numbers up to five input digits. Both encoder and decoder RNNs contain a single GRU cell (Chung et al., 2014) with hidden size \(128\). Each model is trained with a distinct learning rate and batch size, and it's test success rate (SR) is recorded. The learning rate is sampled from a log-uniform distribution over \([10^{-4},10^{-2}]\), and the batch size is sampled uniformly from \(\{\,64,128,256\,\}\). With the goal of predicting test SR from weights, we split the Tiny RNN Zoo into \(8000/1000/1000\) training, validation, and test examples.

The success rate of each RNN model is clearly invariant under permutation symmetries of its weights, so invariance is a natural inductive bias for any generalization predictor. We evaluate StatNN(Unterthiner et al., 2020) and a UNF-based predictor (note that NFNs are not applicable to the weights of recurrent networks). StatNN is operates on basic statistical features3 of the weights, and has been shown to be a very strong baseline on previous generalization prediction tasks (Unterthiner et al., 2020). On the other hand, UNF operates on raw weight inputs and may be able to extract more nuanced signals than StatNN, as was shown (for CNN classifiers) in Zhou et al. (2023).

In particular, StatNN computes the mean, variance, and \((0,25,50,75,100)\)-percentiles of each weight tensor in the RNN and feeds them into a six-layer MLP with hidden width \(600\). UNF is a permutation invariant model, implemented using a three-layer equivariant backbone (\(16\) hidden channels) followed by invariant pooling and a three-layer MLP (\(512\) hidden neurons). We train each predictor with binary cross entropy loss (since the target SR is in \(\)), using the Adam optimizer with learning rate \(0.001\), batch size \(10\), and training for up to \(10\) epochs. We use the validation data only for early stopping, and assess the performance of each predictor on the test inputs using Kendall's \(\), the rank correlation between predicted and actual success rate.

**Results.** Table 1 shows the performance of each predictor on held out weight inputs. Our UNF-based predictor achieves significantly higher rank correlation between predicted and actual success rate, suggesting that the equivariant layers are able to extract more informative features from the raw weights compared to StatNN.

### Learned optimizers

Choosing the optimizer is a key step in training any modern neural network. Though most popular optimizers are variants of stochastic descent, the non-convexity of neural network training leaves few rigorous guidelines for ideal optimizer design. This has led some researchers to propose _training_ good optimizers using some form of meta-learning (Bengio et al., 1990, 2013; Andrychowicz et al., 2016; Wichrowska et al., 2017; Metz et al., 2019).

Common optimizers today (including the learned ones) are equivariant to any permutation of the weights. This is because permuting the weights also permutes the gradients, so stochastic gradient descent and similar optimizers will produce permuted updates. However, equivariance to _any_ permutation ignores the actual symmetry structure of the optimized neural network. Arguably the more appropriate constraint is to only require equivariance to the _neuron permutation group_, which enables more expressive optimizers while still respecting the symmetries of the weight space. As we will see, this can be achieved by using UNFs to implement a learned optimizer.

Training learned optimizers that generalize well is extremely compute-intensive (Metz et al., 2022), so we conduct our experiments in several smaller settings to analyze the impact of architecture

  Method & Test \(\) \\  Deep Set & \(0.8306 0.0006\) \\ StatNN & \(0.8839 0.0007\) \\
**UNF (Ours)** & \(\) \\  

Table 1: Rank correlation between predicted and actual success rates of RNNs on an arithmetic task. Predicting with UNF significantly outperforms StatNN(Unterthiner et al., 2020).

choice on learned optimizer performance. In each setting, an optimizer is meta-trained to optimize an architecture type on a task from random initializations. Following Harrison et al. (2022), our learned optimizers track momentum terms \(m_{t}^{} m_{t-1}+_{t}\) and produce updates of the form:

\[W_{t+1} W_{t}-(m_{t}^{_{0}}+ f(W_{t}, _{t},\{\,m_{t}^{_{t}}\,\}_{i},t)).\] (15)

Here \( m_{t}^{_{0}}\) is a "nominal term" that biases the learned optimizer to behave like stochastic gradient descent with momentum coefficient \(_{0}\). The neural functional \(f()\) ingests weights \(W_{t}\), gradients \(_{t}\), momentum terms at several coefficients \(\{\,m_{t}^{_{i}}\,\}_{i}\), and the iteration \(t\).

During meta-training, we optimize network \(f\) and scalars \(,,_{0}\) to minimize the task training loss after a fixed number of training steps \(T\), the "inner training horizion." To avoid the issue of backpropagating through an optimization process, we estimate meta-gradients using persistent evolutionary strategies (Vicol et al., 2021).

**Comparisons.** The default architecture choice for \(f()\) in prior work is **Deep Sets**(Zaheer et al., 2017), which offers equivariance to _any_ permutation symmetry. We study the effect of replacing Deep Sets by **UNFs**. We also try the **NFNNP** architecture (Zhou et al., 2023) where applicable, though it cannot be used on the RNN and Transformer experiments. Finally, we consider stochastic gradient descent with momentum (**SGDM**), which is equivalent to fixing \(=0\) in Eq. 15. The SGDM baseline is also meta-trained to tune the learning rate \(\) and momentum decay rate \(_{0}\). We compare the different learned optimizers in four tasks:

**MLP on FashionMNIST.** Each optimizer trains an MLP classifier on a downsized and flattened version of the FashionMNIST dataset (Xiao et al., 2017). We note that for MLP weight spaces, UNF are identical to \(_{}\)(Zhou et al., 2023).

**CNN on CIFAR-10.** Each optimizer trains a convolutional classifier on a downsized \(16 16\) CIFAR-10. In this setting our algorithm produces a UNF that is _different_ to \(_{}\) (see Example 2.3).

**RNN on LM1B.** Each optimizer trains a character-level RNN-based language model (LM) on the One Billion Word Language Model Benchmark (LM1B) dataset (Chelba et al., 2013).

**Transformer on LM1B.** Each optimizer trains a Transformer LM on LM1B, this time predicting tokens instead of characters.

We use an inner training horizon \(T=2{,}000\) for the first three tasks and \(T=5{,}000\) for the Transformer task, since it takes longer to train. When implementing \(f()\) for each method, we use a network with four layers, \(32\) hidden channels, and ReLU nonlinearities. The Deep Set optimizer uses exclusively Deep Set layers (Zaheer et al., 2017, Eq. 4), while the UNF and NFN optimizers uses three Deep Set layers followed by a single UNF or NFN layer. See Appendix C.1-C.2 for full descriptions of the tasks and meta-training.

**Results.** Figure 2 shows the training curves produced by each of the meta-trained optimizers in each experiment. Learned optimizers with deep architectures (UNF, Deep Set, or NFN) outperform SGDM, even after tuning SGDM's learning rate and momentum decay. UNF typically learns fastest

Figure 2: Training loss (negative log-likelihood) curves for different tasks and architectures using meta-learned optimizers. We implement learned optimizers with either universal neural functionals (**UNFs**), **NFNs**(Zhou et al., 2023), or **Deep Sets**(Zaheer et al., 2017). Deep Sets are the current standard choice for implementing learned optimizers. Note that NFN is identical to UNF in the MLP case, different for CNN case, and not applicable to RNNs or Transformers. All loss curves are smoothed and averaged over \(5\) random initializations (\(3\) for Transformer), with shaded regions showing standard error.

and achieves the lowest training loss across all methods, though Deep Set and NFN can be comparable in some settings. One interesting observation is that UNF outperforms NFN in the CNN experiment. As noted in Example 2.3, UNFs make the stronger assumption that all tensor dimensions-including the spatial dimensions of the convolution filter-are permutable, while NFNs do not. Although the UNF assumption is technically incorrect, the stronger assumption leads to a lower parameter count (see Table 3 in the appendix) which may be easier for meta-optimization.

Overall, our results show the promise of using UNFs to create more expressive learned optimizers that utilize the specific symmetry structure of the weight spaces they optimize. Further work could investigate their capacity for generalization to new tasks and architectures, for example by meta-training on diverse tasks (Metz et al., 2022). Moreover, as Table 3 in the appendix shows, a necessary trade-off of UNFs being more expressive is that they require more parameters for an equivalent number of layers and hidden channels. Since learned optimizers are still much smaller than the networks they could optimize, this may not be a significant computational constraint in practice. Still, it could be a challenge to meta-optimization, since evolutionary strategies are known to struggle in higher dimensions. Hence, further work on efficient high-dimensional meta-gradient estimators would complement the development of expressive weight-space models like UNF.

## 5 Related Work

There is a long history of neural network architectures that are equivariant to various symmetry groups (LeCun et al., 1995; Cohen and Welling, 2016; Ravanbakhsh et al., 2017; Kondor and Trivedi, 2018; Cohen et al., 2018). Existing frameworks for automatically constructing equivariant models (Finzi et al., 2021) produce equivariant matrices, which would be intractable for our task. Our work constructs efficient equivariant basis functions for a particular class of permutation symmetries that arise in the weight spaces of neural networks. Permutation equivariant networks have been developed for sets (Zaheer et al., 2017), matrices whose rows and columns permute independently (Hartford et al., 2018), and tensors under _higher-order_ permutation actions (Thiede et al., 2020; Pan and Kondor, 2022)-the latter may also be viewed as equivariant models on graphs or polytopes (Maron et al., 2018; Albooyeh et al., 2019). This work observes that a weight space is a _collection_ of tensors under higher-order permutation symmetries, and develops equivariant models for that setting.

There has been significant interest in designing architectures that that either optimize or generate neural network weights (Schmidhuber, 1993; Ha et al., 2016; Krueger et al., 2017; Kirsch and Schmidhuber, 2021; Peebles et al., 2022; Metz et al., 2022). Some works have identified the importance of respecting the relevant symmetries when implementing black box meta-learners (Kirsch et al., 2022). However, precise characterizations of equivariant models on neural weight spaces are relatively recent and were initially restricted to simple feedforward models (Navon et al., 2023; Zhou et al., 2023, 2023).

A recent alternative approach has been to leverage message passing neural networks (MPNNs) (Zhang et al., 2023) to process weights as edges of a graph. Concurrent to this work, Kofinas et al. (2024) demonstrated applications of MPNNs to learned optimization for MLPs and CNNs and Lim et al. (2023) extended MPNNs to process general weight-spaces. MPNN-based approaches benefit from more flexible adaptation to heterogenous inputs, and the computational cost of message passing does not grow as rapidly as our basis-this is because our approach guarantees each linear layer to be maximally expressive while MPNNs do not. We give a more detailed exposition of this trade-off in Appendix B.3

## 6 Conclusion

We introduce a method for constructing permutation-equivariant neural functionals that operate on arbitrary weight spaces, removing a major limitation of previous frameworks that were only applicable to the weight spaces of simple MLPs and CNNs. Our algorithm constructs maximally expressive equivariant linear layers for processing any collection of tensors given a description of their permutation symmetries, and implements these layers in terms of efficient array operations in standard deep learning frameworks. We empirically validate that the resulting _universal neural functionals_ (unFs) are effective at tasks that involve processing the weights and gradients of convolutional image classifiers, recurrent sequence-to-sequence models, and Transformer language models. Inparticular, we find that UNFs show promising improvements over existing learned optimizer designs in small scale experiments.

**Limitations and future work.** It remains to be demonstrated how UNFs can be applied to heterogenous weight-space inputs, e.g., to have a single UNF act as a learned optimizer for any input architecture. Moreover, our experimental results only validate the promise of UNF-based learned optimizers in relatively limited settings, and more work would needed to test generalization across arbitrary tasks. Finally, computational tractability may be a significant challenge for more complex architectures as the number of basis terms generated by Alg. 1 would grow rapidly for higher rank tensors with higher-order interactions (see Appendix B.2). Resolving these challenges would further improve the scalability and applicability of neural functionals to weight-space tasks.

## 7 Acknowledgements

We thank Jascha Sohl-Dickstein and Yiding Jiang for insightful general discussions about the project, and Louis Kirsch for helpful feedback on early drafts. AZ is supported by the NSF Graduate Research Fellowship Program. We are grateful to the TPU Research Cloud (TRC) for providing compute for some of the experiments.