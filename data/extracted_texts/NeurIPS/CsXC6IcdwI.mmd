# Ehrshot: An EHR Benchmark for Few-Shot Evaluation of Foundation Models

Michael Wornow

Department of Computer Science

Stanford University

mwornow@stanford.edu

&Rahul Thapa

Center for Biomedical Informatics Research

Stanford University

rthapa84@stanford.edu

Equal contribution

Ethan Steinberg

Department of Computer Science

Stanford University

ethan@stanford.edu

&Jason A. Fries

Center for Biomedical Informatics Research

Stanford University

jfries@stanford.edu

&Nigam H. Shah

Center for Biomedical Informatics Research

Clinical Excellence Research Center

Stanford University

Technology and Digital Solutions

Stanford Healthcare

nigam@stanford.edu

Equal contributionEqual senior authorship

###### Abstract

While the general machine learning (ML) community has benefited from public datasets, tasks, and models, the progress of ML in healthcare has been hampered by a lack of such shared assets. The success of foundation models creates new challenges for healthcare ML by requiring access to shared pretrained models to validate performance benefits. We help address these challenges through three contributions. First, we publish a new dataset, EHRSHOT, which contains de-identified structured data from the electronic health records (EHRs) of 6,739 patients from Stanford Medicine. Unlike MIMIC-III/IV and other popular EHR datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients. Second, we publish the weights of CLMBR-T-base, a 141M parameter clinical foundation model pretrained on the structured EHR data of 2.57M patients. We are one of the first to fully release such a model for coded EHR data; in contrast, most prior models released for clinical data (e.g. GatorTron, ClinicalBERT) only work with unstructured text and cannot process the rich, structured data within an EHR. We provide an end-to-end pipeline for the community to validate and build upon its performance. Third, we define 15 few-shot clinical prediction tasks, enabling evaluation of foundation models on benefits such as sample efficiency and task adaptation. Our model and dataset are available via a research data use agreement from our website. Code to reproduce our results is available here.

[MISSING_PAGE_FAIL:2]

## 2 Related Work

One of the most popular EHR datasets made accessible to researchers is MIMIC-III, which contains roughly 40,000 patients seen in the intensive care unit (ICU) of Beth Israel Deaconess Medical Center in Boston, Massachusetts, between 2001 and 2012 . Other public datasets include eICU , HiRID , AmsterdamUMCdb , CPRD , MIMIC-IV , and the UK BioBank .

Most of the aforementioned datasets are narrowly scoped to a single department: the ICU . This makes it impossible to capture a patient's full health trajectory to the extent that an academic medical center or health system would know of the patients it treats. Other datasets such as MIMIC-IV include data from multiple departments, but are still heavily anchored to the ICU, as only patients admitted for an ICU/ED visit are included . In contrast, our work releases the full longitudinal EHR of patients across all departments of a major academic medical center, thus providing a more realistic setting for general prediction making.

Prior work has also typically relied on the creation of bespoke schemas to store their data. These custom schemas greatly increase the difficulty of transferring models across datasets and sites . In contrast, the data preprocessing pipeline that we use is capable of ingesting both EHRSHOT as well as any dataset that follows the Observational Medical Outcomes Partnership Common Data Model (OMOP-CDM), an open community data standard for sharing EHRs used by over 100 health systems . More details on our data preprocessing pipeline can be found in the Appendix in Section C.5.

Previously published EHR datasets typically only provide raw data. Thus, significant additional effort has been devoted to building standardized preprocessing pipelines, patient splits, and task definitions on top of these datasets . These add-on benchmarks, however, are still limited by the narrow scope of their underlying data, and many recycle the same core set of tasks (e.g. in-patient mortality, long length-of-stay, ICU transfer, and ICD code prediction) . Additionally, these benchmarks are typically not created with the purpose of measuring a pretrained model's few-shot

Figure 1: Overview of EHRSHOT. Black boxes represent open source code, data, and model weights. Red boxes are private data. (1) Starting with a source EHR database of 3.67M patients, we define a global train/val/test split across all patients. (2) We use an open source EHR preprocessing package called FEMR to transform our data. We keep all structured data (diagnoses, medications, labs, etc.) and discard images and clinical text. (3) We use the 2.57M patients in our global train split to pre-train a foundation model, CLMBR-T-base  We filter the source database down to a cohort of 6,739 patients, which we use for EHRSHOT. (5) We define 15 few-shot classification tasks and label each patient accordingly. (6) We test two baseline models for each task: our pretrained CLMBR-T-base and a count-based GBM model . (7) We measure the AUROC and AUPRC of each model on each task, and share the results in Section performance . This limits their utility in assessing the key value propositions of foundation models, such as improved sample efficiency and adaptation to diverse tasks.

On the modeling side, substantial literature exists on training FMs for EHR data . However, the vast majority of these FMs have never had their weights published . This greatly hinders reproducibility and makes cross-model evaluations difficult. Worse, this lack of sharing undermines a primary advantage of FMs: transfer learning, i.e. the ability to use the pretrained weights of an existing FM to shortcut model development for other tasks .

EHRSHOT aims to fill several of these gaps by providing a longitudinal EHR benchmark specifically geared towards few-shot evaluation of pretrained FMs. EHRSHOT is built on top of a cross-site interoperable standard (OMOP-CDM), and leverages an open source data preprocessing pipeline to allow other researchers to reproduce our results end-to-end. Additionally, we release the weights of the clinical foundation model that we pretrain and evaluate, one of the first to do so. We provide additional points of comparison in Table .

## 3 Dataset

We are releasing EHRSHOT (pronounced "earshot"), an EHR benchmark for few-shot evaluation of foundation models. EHRSHOT is a collection of 6,739 unique patients with canonical train/validation/test splits and corresponding labels for 15 classification tasks. We also provide canonical \(k\)-shot samples for each few-shot evaluation task. Unlike prior EHR benchmarks focused on task-specific supervised models  for specific episodes of care, e.g. admission to the ICU , our benchmark is designed for evaluating pretrained FMs on a broad range of tasks using the depth of information that a health system would typically possess for its patients. EHRSHOT is provided as a set of CSV files. It is essentially a lightweight serialization of the OMOP-CDM format. Please see Section C.4 in the Appendix for additional details on the dataset format.

EHRSHOT contains a total of 41.6 million coded observations (e.g. diagnoses, procedures, medications, lab results, etc.) and 921,499 unique visits across 6,739 patients. We exclude all patients less than 19 years of age or greater than 88 years of age. We also exclude patients with less than 10 total clinical events in their record. We include statistics of EHRSHOT's cohort demographics in Table  and Appendix Table  and histograms of patient characteristics in Appendix Figure .

### Data Source

We sourced the data for our benchmark from the Stanford Medicine Research Data Repository (STARR) , which contains EHR data from both Stanford Health Care (primarily adult care) and Lucile Packard Children's Hospital (primarily pediatric care). The source dataset is structured according to the Observational Medical Outcomes Partnership Common Data Model (OMOP-CDM)  and comprises a total of 3.67M unique patients from 1990 to February 8th, 2023 . Of these patients, 2.57M (70%) are used for training and 0.55M (15%) for validation of the foundation model

    &  &  &  &  \\  & &  & ICU/ED & Other & \# of & Few & Dataset & Preprocessing & Model \\  & & Visits & Visits & Patients & Tasks & Shot & via DUA & Code & Weights \\  MIMIC-Extract  & MIMIC-III & ✓ & – & 34k & 5 & – & ✓ & ✓ & – \\ Parushotham 2018  & MIMIC-III & ✓ & – & 35k & 3 & – & ✓ & ✓ & – \\ Harutyunyan 2019  & MIMIC-III & ✓ & – & 33k & 4 & – & ✓ & ✓ & – \\ Gupta 2022  & MIMIC-IV & ✓ & * & 257k & 4 & – & ✓ & ✓ & – \\ COP-E-CAT  & MIMIC-IV & ✓ & * & 257k & 4 & – & ✓ & ✓ & – \\ Xie 2022  & MIMIC-IV & ✓ & * & 216k & 3 & – & ✓ & ✓ & – \\ eICU  & eICU & ✓ & – & 73k & 4 & – & ✓ & ✓ & – \\ EHR PT  & MIMIC-III / eICU & ✓ & – & 86k & 11 & ✓ & ✓ & ✓ & – \\ FIDLE  & MIMIC-III / eICU & ✓ & – & 157k & 3 & – & ✓ & ✓ & – \\ HiRID-ICU  & HiRID & ✓ & – & 33k & 6 & – & ✓ & ✓ & – \\ Solares 2020  & CPRD & ✓ & ✓ & 4M & 2 & – & – & – & – \\ 
**EHRSHOT** & Stanford Medicine & ✓ & ✓ & 7k & 15 & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparison of our work to existing EHR benchmarks. Checkmark indicates full support, asterisk represents properties that are semi-supported.

[MISSING_PAGE_FAIL:5]

We chose these two models as our baselines for several reasons. First, language modeling has achieved state-of-the-art results on clinical prediction tasks , while count-based featurization remains a simple but competitive baseline . Second, most prior FMs trained on structured EHR data have not had their model weights published, and were developed and tested exclusively on nonstandard data formats like MIMIC-III . This makes it nearly impossible to conduct a fair comparison of prior models, which often requires re-implementation or significant modification to work across datasets . This is one of the key challenges we are attempting to solve with EHRSHOT. We pre-train our own FM from scratch to have full control over its training, and publish its model weights so the community can reproduce and build upon our results.

**Count-based Features**. Count-based featurization is a well-established baseline for EHR tasks, valued for its simplicity and effectiveness . The fundamental idea involves converting each patient's timeline into a count vector, where each element contains the number of occurrences of a specific medical concept prior to the prediction time of a task. These patient vectors are combined into a count matrix, which is high-dimensional and sparse. We use a technique called _ontology expansion_ to increase the density of representation and improve the accuracy of code coverage by acknowledging the parent/child hierarchical relationships between medical concepts . After generating our ontology-expanded count matrix, we train a gradient boosting machine (GBM) model on the EHRSHOT train split, and tune hyperparameters on the validation split. We use the LightGBM implementation . We also evaluate a Logistic Regression and Random Forest model as baselines.

Figure 2: Summary of Benchmark Tasks. Each subfigure contains one of the 4 types of predictive classification tasks included in our benchmark: (1) _Operational Outcomes_ (binary), (2) _Assignment of New Diagnoses_ (binary), (3) _Anticipating Chest X-ray Findings_ (multilabel), (4) _Anticipating Lab Test Results_ (multiclass). Each **black line** represents a patient timeline. The [black boxes] represent how each timeline would be labeled for each task at a specific prediction time. The leftmost edge of the dotted lines above each timeline is the prediction time, and the rightmost edge is the end of the time horizon for that task. Note that each _Operational Outcome_ task (1) has a different prediction window (30 days, 7 days, duration of admission), while the other three task categories (2, 3, 4) all have uniform prediction windows across their subtasks.

Their results can be seen in Appendix in Figures[\(\)]and[\(\)]. For clarity, we exclude them from the following analyses, as they perform roughly at par with the count-based GBM model.

**Clinical Language-Model-Based Representations using Transformers (CLMBR-T-base)**. CLMBR-T-base is an autoregressive model designed to predict the next medical code in a patient's timeline given previous codes. This objective enables it to learn robust global patterns for clinical prediction tasks. It is based on the CLMBR model originally developed in [\(\)], but following [\(\)] we substitute a transformer in place of a GRU as its base model. Our model employs causally masked local attention. This ensures forward-only flow of information which is vital for prediction tasks, and is in contrast to BERT-based models which are bidirectional in nature [\(\)]. Note that our model does not process clinical text, only structured information. Our model has 141M trainable parameters, a hidden dimension of 768, and a next code prediction objective. This provides our version of CLMBR-T-base with minute-level resolution rather than the day-level aggregation of the original model formulation [\(\)]. We leave training larger versions of CLMBR to future work.

More details about our baseline models can be found in the Appendix in Section \(}\)

## 5 Results

We evaluate each baseline model in a few-shot setting. For each of the 15 benchmark tasks, we steadily increase the number of examples \(k\) that each model sees from \(k=1\) to the full training dataset, and record the model's AUROC and AUPRC at each \(k\).

More precisely, we define "\(k\)-shot evaluation" of a model \(M\) on a specific task \(T\) as follows. We train \(M\) on \(k\) positive examples and \(k\) negative examples sampled from \(T\)'s training split. We then select an additional \(k\) positive examples and \(k\) negative examples from \(T\)'s validation split, and use these validation examples to select the best hyperparameters for \(M\) for task \(T\). Finally, we evaluate the AUROC and AUPRC of the best performing version of \(M\) on \(T\)'s entire held-out test split. For tasks where the total number of unique positive examples is less than \(k\), we include all positive examples in our training set, and randomly resample positive examples until the total number of training examples seen by the model is \(k\). We consider values of \(k\{1,2,4,8,12,16,24,32,48,64,128\}\) for all tasks (with the exception of Celiac, for which we limit \(k 64\) as there are only 62 positive training labels).

    &  &  &  \\ 
**Task Name** & \# Patients (\# Positive) & \# Labels (\# Positive) & \# Patients (\# Positive) & \# Patients (\# Positive) & \# Labels (\# Positive) \\ 
**Operational Outcomes** & & & & & & \\ Long Length of Stay & 1377 (464) & 2569 (681) & 1240 (395) & 2231 (534) & 1238 (412) & 2195 (552) \\
30-day Readmission & 1337 (164) & 2609 (370) & 1192 (159) & 2207 (281) & 1190 (151) & 2189 (260) \\ ICU Transfer & 1306 (107) & 2402 (113) & 1157 (84) & 2052 (92) & 1154 (75) & 2037 (85) \\
**Anticipating Lab Test Results** & & & & & & \\ Thrombocytopenia & 2084 (870) & 68776 (9774) & 1981 (774) & 54504 (6962) & 1998 (818) & 56338 (7960) \\ Hyperkleemia & 2038 (383) & 76349 (1215) & 1935 (348) & 60168 (886) & 1958 (339) & 63653 (948) \\ Hypoglycemia & 2054 (422) & 12218 (1065) & 1950 (362) & 95488 (858) & 1970 (356) & 100568 (783) \\ Hyponatremia & 2035 (1288) & 81336 (2081) & 1930 (1165) & 64473 (14674) & 1956 (1212) & 67028 (16003) \\ Anemia & 2092 (1251) & 70501 (9544) & 1992 (1122) & 56224 (7445) & 2002 (1151) & 58155 (7636) \\
**Assignment of New Diagnoses** & & & & & & \\ Hypertension & 793 (130) & 1260 (184) & 784 (130) & 1250 (177) & 758 (130) & 1261 (160) \\ Hyperlipidemia & 923 (137) & 1684 (205) & 863 (140) & 1441 (189) & 864 (133) & 1317 (172) \\ Pancreatic Cancer & 1376 (128) & 2576 (155) & 1242 (46) & 2215 (53) & 1246 (40) & 2220 (56) \\ Celiae & 1392 (48) & 2623 (62) & 1252 (8) & 2284 (11) & 1255 (13) & 2222 (21) \\ Lupus & 1377 (79) & 2570 (104) & 1239 (24) & 2226 (33) & 1249 (For the count-based GBM, these few-shot examples are the only training examples seen by the model. For the pretrained CLMBR-T-base model, we use these few-shot examples to fine-tune a logistic regression head appended to the top of the model, while keeping the weights of the pretrained CLMBR-T-base model frozen. Pretraining the CLMBR-T-base model took roughly 4 days on a single Nvidia V100 hosted in an on-premise compute cluster.

The AUROC of each model across all 4 task categories is presented in Figure 4. In the Appendix, we show this grouping for AUPRC in Figure 4. We also break down each individual task's AUROC in Figure 4 and AUPRC in Figure 4 of the Appendix. We also include results for additional baselines in the Appendix in Figures 10 and 11. The **bolded lines** are the Macro-AUC for each model within a task category, averaged across all subtasks at each \(k\). We include the performance of each model trained on the entire EHRSHOT training split on the far right of every plot as _All_.

As shown in Figure 4, the pretrained foundation model CLMBR-T-base (blue) outperforms the count-based GBM (red) across all aggregated task categories for \(k 64\). This demonstrates the benefits of pretraining in few-shot settings, as the model can leverage patterns learned across millions of patients to derive more accurate representations out-of-the-box than a model trained from scratch. CLMBR-T-base outperforms the count-based GBM across all \(k\) on the _Operational Outcomes_ and the majority of _Anticipating Lab Test Results_ and _Anticipating Chest X-ray Findings_ tasks. For these three task groups, the advantage of CLMBR-T-base seems most pronounced at intermediate levels of \(k\) between 8 and 128. At extremely low \(k\) (i.e. \(k=1\)), both models struggle to learn anything, while as \(k\) increases the advantage of the pretrained model tends to shrink, a trend noted elsewhere . This is most visible in the far-right of the plot at the _All_ marker, which represents the performance of each model when trained on the full EHRSHOT training dataset.

In fact, the count-based GBM exceeds the performance of CLMBR-T-base on the _Assignment of New Diagnoses_ tasks at \(k>64\). This suggests that the advantage of pretraining comes primarily from improved initialization of patient representations, and that the largest gains are achieved in the most data poor regimes.

There are several possible reasons for CLMBR-T-base's underperformance at higher values of \(k\) for the _Assignment of New Diagnoses_ tasks. First, the CLMBR-T-base model's training objective is next code prediction, which makes it ill-suited for predictive tasks with long time horizons (which for these tasks is 1 year). Second, if a simple tree-based model exists for a task (i.e. a few medical concepts tightly correlate with a diagnosis), then it may be more difficult for a pretrained model to coerce patient representations learned over millions of patients to that specific task than training a model from scratch with enough data to learn those distinctive signals. We believe that this reversal in model rankings demonstrates a key strength of EHRSHOT - namely, the diversity of its predictive tasks can help identify opportunities for improving pretraining and few-shot strategies.

We release all of our model weights, evaluation tasks, and data processing code to fully reproduce our results. To the best of our knowledge, the release of our pretrained CLMBR-T-base model is one of the first examples of such a clinical FM having its pretrained weights made publicly available .

## 6 Discussion

We believe that EHRSHOT represents a useful contribution to the ML community by enabling more reproducible healthcare ML research. The release of our pretrained CLMBR-T-base model's weights will allow the community to replicate and build upon our work. Our results identify opportunities for improving pretrained models in few-shot settings.

Acquiring labeled EHR data is expensive and time-consuming. Additionally, certain rare conditions may only be present in a small cohort of patients out of millions within a health system . Thus, model performance in low-label settings is of paramount importance in healthcare. As our results in Section 5 demonstrate, pretrained FMs can yield large performance gains in few-shot settings. While we acknowledge that the tasks themselves may not be the most clinically meaningful, we believe that EHRSHOT offers a valuable contribution by providing a reproducible and rigorous point of comparison for different technical approaches to developing clinical FMs.

**Limitations**. There are several limitations to this work. First, we only release structured data - i.e. we do not publish any of the clinical text or images associated with our patients. While many datasets for medical images exist , publishing clinical text remains a challenge . Second, we only consider one type of foundation model (CLMBR-T-base) for our experiments . We look forward to seeing the additional foundation models that the community applies to our benchmark. Third, we release a very small cohort of patients (<1%) from our source EHR database, and specifically select these patients for the tasks that we define. Releasing our full pretraining dataset would be infeasible from a governance and effort perspective. Thus, while necessary in order to publish our EHR dataset and still broader than existing ICU-specific datasets, our cohort selection process limits the types of questions we can answer and does not reflect the full diversity of medical data. Fourth, as we only were able to evaluate our pretrained FM on Stanford Medicine data, it is unclear how well our pretrained model will perform at other institutions. We anticipate there will be some drop in performance, but the extent is unclear. Fifth, several of our tasks are "low label" in the most extreme sense - for example, the Celiac task only has 13 positive patients in its test set. This makes obtaining low variance estimates of model performance difficult. We aim to mitigate this by adding additional patients to our benchmark in future releases.

Figure 3: Aggregated AUROC across all subtasks within each of the 4 task categories for \(k\{1,2,4,8,12,16,24,32,48,64,128\}\) shots. We show performance on the full training set as _All_. The **bolded lines** are the Macro-AUROC for each model, averaged across all subtasks within a task category for each value of \(k\). The blurred lines are the average AUROC across 5 replicates for each subtask within a task category. CLMBR-T-base (blue) consistently outperforms the count-based GBM (red) at \(k 64\), but lags in higher label settings for the _Assignment of New Diagnoses_ tasks.

**Societal Implications**. We believe that the release of this dataset can help spur positive innovations for improving clinical care with ML. However, we recognize that there are patient privacy concerns anytime EHR data is released. We believe we sufficiently mitigate this risk through the rigorous deidentification process on which our data is subjected . Additionally, we gate access to the dataset through a research data use agreement. Another concern is that models trained on biased data will reflect those biases . Thus, the pretrained FM that we release may propagate biases in care delivery or outcomes present in our source EHR database . However, we hope that by encouraging the full release of models, we can help the community better identify and mitigate these issues .

## 7 Conclusion

We publish EHRSHOT, a benchmark containing the structured data of 6,739 patients' full longitudinal medical timelines specifically geared towards few-shot evaluation of foundation models for clinical data. Unlike most prior work, EHRSHOT contains longitudinal health data rather than a single department (e.g. ICU). We define a set of 15 tasks ranging from well-studied outcomes like 30-day readmission to lesser explored settings such as anticipating abnormal lab values. Finally, we release the weights of a foundation model pretrained on over 2.57M patient timelines and publish the code needed to replicate our results. We hope that this work represents a first step towards moving the field of ML for healthcare towards more reproducible and open model development.