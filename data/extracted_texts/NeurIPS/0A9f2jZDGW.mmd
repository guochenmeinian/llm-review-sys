# Task Arithmetic in the Tangent Space:

Improved Editing of Pre-Trained Models

Guillermo Ortiz-Jimenez

EPFL, Lausanne, Switzerland

guillermo.ortizjimenez@epfl.ch

Equal contribution.

&Alessandro Favero

EPFL, Lausanne, Switzerland

alessandro.favero@epfl.ch

###### Abstract

Task arithmetic has recently emerged as a cost-effective and scalable approach to edit pre-trained models directly in weight space: By adding the fine-tuned weights of different tasks, the model's performance can be improved on these tasks, while negating them leads to task forgetting. Yet, our understanding of the effectiveness of task arithmetic and its underlying principles remains limited. We present a comprehensive study of task arithmetic in vision-language models and show that _weight disentanglement_ is the crucial factor that makes it effective. This property arises during pre-training and manifests when distinct directions in weight space govern separate, localized regions in function space associated with the tasks. Notably, we show that fine-tuning models in their tangent space by linearizing them amplifies weight disentanglement. This leads to substantial performance improvements across multiple task arithmetic benchmarks and diverse models. Building on these findings, we provide theoretical and empirical analyses of the neural tangent kernel (NTK) of these models and establish a compelling link between task arithmetic and the spatial localization of the NTK eigenfunctions. Overall, our work uncovers novel insights into the fundamental mechanisms of task arithmetic and offers a more reliable and effective approach to edit pre-trained models through the NTK linearization.

## 1 Introduction

Pre-trained models play a pivotal role in contemporary machine learning systems. However, to enhance performance on downstream tasks , align them with human preferences , and increase robustness , they often necessitate further editing. Traditional model editing methods rely on costly joint fine-tuning across multiple tasks  and human-feedback , which constrain scalability and democratization. Furthermore, enhancing downstream task performance typically degrades the model's pre-training performance or _zero-shot_ accuracy .

Recent research has introduced cost-effective and scalable model editing techniques that try to preserve the pre-trained model behavior by acting on the model weights through _task arithmetic_ or weight interpolation techniques , thus circumventing expensive joint fine-tuning over multiple tasks. These methods hinge on the observation that arithmetic operations between fine-tuned weights often produce analogous functional behaviors . For example, summing the relative weight components of a model between pre-training and fine-tuningon two separate tasks results in a new multi-task model with improved performance on both tasks. Similarly, subtracting a task's relative component can lead to the model forgetting that task.

Despite these advancements, the understanding of task arithmetic's underlying principles and its general effectiveness remains limited. Specifically, a comprehensive understanding of how these techniques affect a model's internal representations and the necessary conditions to make it reliable is lacking. This knowledge gap can undermine the adoption of these techniques, as it erodes their trustworthiness in real-world applications. In addition, reducing this gap could help us improve them even further.

To address these challenges, we present a systematic study of task arithmetic in contrastively pre-trained vision-language models (_i.e._, CLIP ), offering novel insights into its underlying mechanisms and introducing new approaches which enhance the performance of pre-trained models edited through task arithmetic. Specifically, we probe the hypothesis presented in Wortsman et al.  that task arithmetic is possible thanks to the fact that models inherently operate in a linear regime, where their behavior is dictated by the finite-width neural tangent kernel (NTK) .

Our study reveals that linearized CLIP models exhibit significantly improved task arithmetic performance with respect to their nonlinear counterparts (see Tables 1 and 2), but also that the NTK cannot fully account for the task arithmetic abilities of pre-trained non-linear models. Indeed, we show that the sole requirement for task arithmetic is actually _weight disentanglement_, where distinct directions in weight space correspond to changes of the network in disjoint regions of the input space (see Figure 1). This allows a model to perform task arithmetic by independently manipulating these weight directions.

Notably, we show that fine-tuning models in their tangent space by linearizing them amplifies weight disentanglement, leading to substantial performance improvements across multiple task arithmetic benchmarks and models. However, although weight disentanglement is stronger in the tangent space, it is also present in non-linear models. We demonstrate that weight disentanglement of semantically meaningful tasks is an emergent property of pre-training, as it is absent at random initialization.

In particular, our main contributions are as follows:

* We formalize the notion of task arithmetic introduced in Ilharco et al.  as Property 1, allowing us to reason quantitatively about it.
* We show that task arithmetic in non-linear models cannot be explained by their NTK, and introduce the concept of weight disentanglement as the necessary condition to enable it.
* We propose to linearize models as a way to enhance weight disentanglement and improve task arithmetic. Doing so, we achieve up to \(5.8\) points more and \(13.1\) points less in accuracy on task addition and task negation, respectively, on several vision-language benchmarks.
* We link weight disentanglement in linearized models to spatial localization of the kernel eigenfunctions and validate this prediction numerically in pre-trained transformer models.
* Finally, we show that weight disentanglement is an emergent property of pre-training.

Overall, our work delivers new insights into the fundamental mechanisms of task arithmetic, facilitating more reliable and scalable model editing. Our findings suggest that linearized fine-tuning of pre-trained models warrants further investigation, with the potential for substantial impact on effective model editing. These insights can foster the development of more efficient and precise model editing techniques, empowering practitioners to adapt pre-trained models to a broader range of tasks.

Figure 1: **Illustration of weight disentanglement**, where distinct directions in the weight space, \(_{t}\), are associated with localized areas of the input space, \(_{t}\). This allows a model, \(f\), to manipulate these areas independently by adding linear combinations of \(_{t}\)’s to a pre-trained checkpoint \(_{0}\).

## 2 Notation and problem statement

Let \(f:\) be a neural network taking inputs \(\) and parameterized by a set of weights \(\). We will assume \(^{d}\), \(^{m}\) and \(^{c}\). Consider \(T\) tasks, with every task \(t\) consisting of a triplet \((_{t},_{t},f_{t}^{})\) where \(_{t}\) is a data support (_e.g._, ImageNet  images), \(_{t}\) an input distribution such that \((_{t})=_{t}\), and \(f_{t}^{}:_{t}\) a target function (_e.g._, labels). In practice, each task is identified with a training set \(\{(_{},f_{t}^{}(_{}))\}_{[n_{t}]}\) with \(_{t}\), that is used to fine-tune the models starting from the pre-trained weights \(_{0}\) and obtain the fine-tuned weights \(_{t}^{}\).

Task arithmetic.Let the _task vector_ of task \(t\) be the difference between the fine-tuned and the pre-trained weights, _i.e._, \(_{t}=_{t}^{}-_{0}\). The following property formalizes the notion of task arithmetic introduced in Ilharco et al. , where the authors observed that the accuracies of pre-trained models on different datasets can be modified independently through the addition or removal of task vectors.

**Property 1** (Task arithmetic).: _Consider a set of task vectors \(=\{_{t}\}_{t[T]}\) with associated non-intersecting task supports \(=\{_{t}\}_{t[T]}\), i.e., \( t,t^{}\), if \(t t^{}\) then \(_{t}_{t^{}}=\). We say a network \(f\) satisfies the task arithmetic property around \(_{0}\) with respect to \(\) and \(\) if_

\[f(;_{0}+_{t=1}^{T}_{t}\,_{t})= f(;_{0}+_{t}\,_{t})& _{t}\\ f(;_{0})&_{t=1}^{T}_{t}\] (1)

_with \((_{1},,_{T})^{T}\)._

In short, a model satisfies Property 1 if adding \(_{t}\) does not modify the output of the model outside \(_{t}\).

Neural tangent kernel.Around the initialization weights \(_{0}\), a neural network can be approximated with a first-order Taylor expansion:

\[f(;) f(;_{0})+(-_{0})^{}_{}f(;_{0}).\] (2)

This approximation is equivalent to a kernel predictor with a kernel known as the _neural tangent kernel_ (NTK) , \(k_{}(,\,^{})=_{}f(;_{0})^{}_{}f(^{};_{0})\), and defines a neural tangent space in which the relationship between weights and functions is linear. Remarkably, as the network width approaches infinity, Eq. (2) becomes exact and remains valid throughout training .

However, this linear approximation is often invalid at finite widths, as the evolution of parameters during training is inadequately captured by Eq. (2). In such cases, training occurs in a _non-linear regime_. Conversely, often during fine-tuning, parameter evolution in many pre-trained models is frequently minimal, meaning that training does not exit the tangent space and Eq. (2) closely approximates the network behavior . In such cases, training occurs in a _linear regime_.

## 3 Task arithmetic is not a consequence of linear fine-tuning

The objective of this work is to understand the conditions that enable task arithmetic in deep neural networks. Previous studies hypothesized that task arithmetic results from fine-tuning in the linear regime , as linear weight combinations correspond to similar output function combinations. However, we will now demonstrate that CLIP models do not fine-tune in the linear regime and we therefore need other ways to explain task arithmetic.

In general, if a pre-trained network \(f(\,;\,_{0})\) demonstrates _kernel behavior_ during fine-tuning - _i.e._, fine-tuning occurs in the linear regime - the following property must be satisfied :

**Property 2** (Post-hoc linearization).: _The change in the network output after training can be approximated by its first-order Taylor expansion, i.e., \(f(;^{})-f(;_{0})(^{ }-_{0})^{}_{}f(;_{0})\)._

In simple terms, the approximation of the network in the tangent space around initialization must hold after fine-tuning. To test this, we evaluate the performance of the _post-hoc_ linearized version of \(f\), \(f_{}\). That is, we apply the fine-tuned task vectors \(=^{}-_{0}\) to the linear approximation of \(f\) at \(_{0}\), _i.e._,

\[f_{}(;_{0}+)=f(;_{0})+ ^{}_{}f(;_{0}),\] (3)

and we check whether \(f_{}(\,;\,^{})\) performs similarly to \(f(\,;\,^{})^{2}\).

The results in Figure 2 indicate that CLIP models do not exhibit a kernel behavior. Specifically, we fine-tune (FT) several CLIP pre-trained Vision Transformers (ViTs)  of different sizes following the same setup as Ilharco et al.  on \(8\) tasks: Cars , DTD , SUN397 , EuroSAT , GTSRB , MNIST , SVHN  and RESISC45 . We observe that the single-task performance of \(f_{}(\,;\,^{})\) is significantly lower than that of \(f(\,;\,^{})\) for ViTs of all sizes. This _non-linear advantage_ is a clear sign that fine-tuning has not happened in a linear regime as expected by Wortsman et al. .

Yet, this observation is not enough to rule out that task arithmetic can be explained by linearizing the network function. Indeed, even if the non-linear components are important for single-task performance, they might not be used during task arithmetic, which is the objective of this study. That is, the projection of \(f\) onto the tangent space could be the only useful component.

We now show this is also not the case, as doing task arithmetic with the non-linearly fine-tuned task vectors over \(f_{}\) significantly decreases performance. To show this, we employ the benchmark proposed in Ilharco et al.  to evaluate the task arithmetic ability of a pre-trained model, which consists of the \(8\) tasks described before and two sub-benchmarks:

1. **Task addition**: The sum of the task vectors \(=_{t}_{t}\) is added to a pre-trained checkpoint to produce a multi-task model. The success of this benchmark is measured in terms of the maximum average accuracy over the different tasks. Results are shown in Table 1.
2. **Task negation**: A task vector is subtracted from the pre-trained checkpoint to forget a task while retaining performance on a control task (ImageNet). The success of this benchmark is measured in terms of the maximum drop in accuracy on the forgetting task that retains the performance on the control task. Results are averaged over tasks and shown in Table 2.

    &  &  &  \\  & & Abs. (\(\)) & Norm. (\(\)) & Abs. (\(\)) & Norm. (\(\)) & Abs. (\(\)) & Norm. (\(\)) \\  Pre-trained & \(f(\,;\,_{0})\) & 48.4 & – & 55.2 & – & 64.4 & – \\  Non-lin. FT & \(f(\,;\,_{0}+)\) & 71.4 & 76.5 & 75.5 & 80.0 & 85.1 & 88.8 \\ Post-hoc lin. & \(f_{}(\,;\,_{0}+)\) & 57.1 & 81.9 & 65.0 & 85.2 & 75.2 & 90.0 \\  Linear. FT & \(f_{}(\,;\,_{0}+_{})\) & **76.5** & **85.4** & **81.3** & **86.0** & **88.5** & **93.5** \\   

Table 1: **Task addition.** Average absolute (%) and normalized accuracies (%) of different CLIP ViTs edited by adding the sum of the task vectors of \(8\) tasks. We report results for the non-linear and linearized models of Sections 3 and 5 normalizing performance by their single-task accuracies.

    &  &  &  \\  & & Targ. (\(\)) & Cont. (\(\)) & Targ. (\(\)) & Cont. (\(\)) & Targ. (\(\)) & Cont. (\(\)) \\  Pre-trained & \(f(\,;\,_{0})\) & 48.4 & 63.4 & 55.2 & 68.3 & 64.4 & 75.5 \\  Non-lin. FT & \(f(\,;\,_{0}-)\) & 24.0 & 60.7 & 19.2 & 64.6 & 18.0 & **72.5** \\ Post-hoc lin. & \(f_{}(\,;\,_{0}-)\) & 14.8 & 60.3 & **10.8** & **64.8** & 12.1 & 71.8 \\  Linear. FT & \(f_{}(\,;\,_{0}-_{})\) & **10.9** & **60.8** & 11.3 & **64.8** & **7.9** & **72.5** \\   

Table 2: **Task negation.** Minimum accuracy (\(\%\)) of different CLIP ViTs edited by negating a task vector from a target task while retaining \(95\%\) of their performance on the control task. We report average performances over eight tasks on non-linear and linearized models as introduced in Sections 3 and 5.

To obtain the task vectors, we use the fine-tuned weights of the different ViTs from before, and use a single mixing coefficient \(=_{1}==_{T}\) optimized separately for the non-linear and post-hoc linearized models to ensure a fair comparison. We provide all details of this experiment in Appendix A.

The results in Table 1 confirm that task arithmetic in CLIP models does not stem from the combination of their linear components only. Specifically, we observe a significant drop in absolute task addition accuracy in the _post-hoc_ linearized models compared to the non-linear ones. This decrease in performance is consistent across tasks (see Appendix D.2) and highlights that task arithmetic in non-linear models leverages the non-linear components of \(f\), as well.

Although these results reject the linear hypothesis, it is still remarkable that the post-hoc linearized models do better at task negation than the non-linear ones (see Table 2). Furthermore, even in task addition (see Table 1) they achieve higher normalized accuracies (see definition in Appendix A). Indeed, as we formalize in Section 4, this observation suggests that linearized models are more consistent with Property 1. In Section 5, we will use this fact to devise a new way to enhance task arithmetic.

## 4 Weight disentanglement

If the linear regime is not necessary to explain task arithmetic, what are the necessary conditions that allow it? In this section, we argue that the only necessary condition to perform task arithmetic with a model \(f\) is that the model is _weight disentangled_ with respect to the set of fine-tuning tasks.

**Property 3** (Weight disentanglement).: _A parametric function \(f:\) is weight disentangled with respect to a set of task vectors \(=\{_{t}\}_{t[T]}\) and the corresponding supports \(=\{_{t}\}_{t[T]}\) if_

\[f(;_{0}+_{t=1}^{T}_{t}_{t})= _{t=1}^{T}g_{t}(;_{t}_{t})+g_{0}(),\] (4)

_where \(g_{t}(;_{t}_{t})=\) for \(_{t}\) and \(t=1,,T\), and \(g_{0}()=0\) for \(_{t[T]}_{t}\)._

In essence, this definition captures the idea that the function \(f\) can be decomposed as a sum of spatially-localized components, _i.e._, vanishing outside a spatial region, whose functional variation is entirely captured by each \(_{t}\) (see Figure 1). Moreover, it is trivial to see that satisfying weight disentanglement is equivalent to satisfying Property 1 on task arithmetic as one can always write Eq. (1) as

\[f(;_{0}+_{t=1}^{T}_{t}_{t})= _{t=1}^{T}f(;_{0}+_{t}_{t}) (_{t})+f(;_{0}) (_{t[T]}_{t}),\] (5)

and identify \(g_{t}(;_{t}_{t})=f(;_{0}+_{t}_{t})(_{t})\) and \(g_{0}()=f(;_{0})(_{t})\). It is important to highlight, however, that this additive decomposition does not imply linearity, as the local functions \(\{g_{t}\}_{t[T]}\) are not required to be linear with respect to the parameters.

Furthermore, note that weight disentanglement is a property of the predictors and not related to the performance on different tasks. That is, a model could be weight disentangled with respect to a set of task vectors and still perform poorly on a task, _e.g._, if \(f(\ ;_{0}+)\) does not generalize for some \(\). More generally, we can visualize the level of weight disentanglement of a model by measuring its discrepancy with Eq. (4). To do so, given two tasks, one can check the _disentanglement error_ of a model,

\[(_{1},_{2})=_{t=1}^{2}_{_{t}}[ (f(;_{0}+_{t}_{t}),f (;_{0}+_{1}_{1}+_{2}_{2}) )],\] (6)

where \(\) denotes any distance metric between output vectors. As we are dealing with classification tasks, in what follows we use the prediction error \((y_{1},y_{2})=(y_{1} y_{2})\) as the distance metric. In general, the smaller the value of \((_{1},_{2})\) the more weight disentangled a model is at \((_{1},_{2})\).

Figure 3 displays the disentanglement error of a CLIP ViT-B/32 model concerning several task vector pairs. We observe that the CLIP model exhibits a minimal disentanglement error within a small region surrounding \(_{0}\), which enables task arithmetic. However, for \(_{1},_{2}>1\), the error increases, indicating a high degree of interaction between tasks. This explains why task arithmetic performs better in a small neighborhood of \(_{0}\) - task arithmetic is more effective when fine-tuning with small learning rates and few training steps  - with the optimal value of \(\) typically being less than \(1\).

Comparing the disentanglement error of the non-linear models and their post-hoc linearization reveals an interesting finding: linearized models exhibit greater disentanglement than their non-linear counterparts. This is evident from the more extensive regions with low disentanglement errors in Figure 3 (bottom). This explains why the post-hoc linearized models achieve higher normalized accuracies via task addition (cf. Table 1) and manage to forget more through task negation (cf. Table 2). Paradoxically, however, although the greater disentanglement of linearized models allows them to retain more of their relative performance when edited with task arithmetic, they still perform worse in absolute terms due to the great advantage of the non-linear models in single-task accuracy (cf. Figure 2). This suggests that closing the single-task performance gap between linearized and non-linear models could be a way to enhance task arithmetic. We leverage this idea in the next section.

## 5 Enhancing task arithmetic via linearization

We have seen that linearized models are more weight-disentangled than non-linear ones. However, post-hoc linearization degrades single-task performance. We now demonstrate that enforcing models to fine-tune in the tangent space to their pre-trained initialization significantly improves task arithmetic by reducing the single-task accuracy gap.

Specifically, rather than applying the non-linearly fine-tuned task vectors \(=^{}-_{0}\) to \(f_{}\), as in Section 3, we propose to directly obtain the task vectors through explicit fine-tuning in the tangent space as illustrated in Figure 4. That is, given a model \(f\), we directly fine-tune its linear approximation \(f_{}\) around \(_{0}\). The fine-tuning process can follow the same protocols used before but with the network parameterization dictated by Eq. (3). Due to the

Figure 4: Conceptual illustration of the different approaches we use to edit a pretrained model \(f(\ ;\ _{0})\). Here \(\) represents the space of neural network functions \(f\), non-linearly parameterized by \(\); and \(\) its tangent space, given by the space of linearized functions \(f_{}\).

Figure 3: **Visualization of weight disentanglement.** The heatmaps show the disentanglement error \((_{1},_{2})\) of a non-linear CLIP ViT-B/32 (top) and its post-hoc linearization (bottom) on different example task pairs. The light regions denote areas of the weight space where weight disentanglement is stronger. The red box delimits the search space used to compute the best \(\) in all our experiments.

linear connection between the weight-space and function-space defined in Eq. (3), fine-tuning \(f_{}\) is essentially the same as training a kernel predictor with kernel \(k_{}\). As a result, we obtain the fine-tuned weights \(_{}^{}\) of the linearized model for each task, which allows us to construct the corresponding task vector \(_{}=_{}^{}-_{0}\). We provide further details of this procedure in Appendix B.

Moreover, as the considered models do not inherently exhibit linear fine-tuning (see Section 3), this approach yields significantly different results compared to post-hoc linearization, _i.e._, \(f_{}(;_{0}+_{}) f_{ }(;_{0}+)\). In particular, although both models share the same kernel \(k_{}(,\,^{})\), the task vectors \(_{}\) have been explicitly optimized to maximize the performance of such linearized models. Consequently, by construction, linearized fine-tuning outperforms post-hoc linearization. Indeed, in Figure 5, we observe that linearized fine-tuning significantly reduces the non-linear advantage of non-linear models, as in most cases the performance of \(f_{}(\,\,;\,_{0}+_{})\) is very similar to the one of \(f(\,;\,_{0}+)\) (cf. Figure 2).

Remarkably, as we show in Appendix D.4, this increase in single-task performance does not compromise weight disentanglement, which remains as high as for the post-hoc linearized models in Figure 3. As a result, linear fine-tuning allows for improved task arithmetic compared to standard non-linear fine-tuning. In particular, Tables 1 and 2 in their last rows show that linearized fine-tuned models significantly outperform their non-linear counterparts and achieve state-of-the-art results on the task addition and negation benchmarks . The linearized fine-tuned models achieve higher multi-task accuracies through task addition (up to \(5.8\) points more) and can forget more through task negation (up to \(13.1\) points more) while maintaining a similar level of accuracy on the control task. Additionally, we observe that the advantage of the linearized models over the non-linear ones is higher for the smaller ViT-B/32 and progressively diminishes as the model size increases up to ViT-L/143.

In general, thanks to the efficiency of the Jacobian-vector product implementations in most deep learning frameworks , training and inference in linearized neural networks only require an \((1)\) increase in computational costs with respect to their non-linear counterparts (see Appendix B for a longer discussion). In this regard, the superiority of task arithmetic of linearized models can make this technique appealing for practical applications. Identifying the right trade-offs between computational cost and performance, as well as faster linearization techniques, is an exciting avenue for future work.

## 6 Towards understanding task arithmetic

We conclude by providing further fundamental insights that can aid our understanding of task arithmetic. In particular, we ask whether any kernel can satisfy Property 1, and we establish a connection between task arithmetic and the spectral properties of the NTK. Then, we argue that weight disentanglement and task arithmetic are emergent properties of pre-training.

### Eigenfunction localization

Generally, a kernel \(k\) admits a decomposition in terms of a family of eigenfunction-eigenvalue pairs \(\{(_{},_{})\}_{}\); which implies that \(k\) can only represent functions of the form \(f^{}()=_{=1}^{}c_{}_{}()\) with a finite kernel norm, _i.e._, \(\|f^{}\|_{}^{2}=_{=1}^{}c_{}^{2}/_{ }<+\). Specifically, the coefficients \(\{c_{}\}_{}\) constitute a representation of the function \(f^{}\) in the kernel basis.

Consider \(T\) tasks \(\{f^{}_{t}\}_{t[T]}\) supported in their respective non-intersecting domains \(\{_{t}\}_{t[T]}\). Furthermore, let \(\{_{}\}_{}\) be an orthogonal basis of eigenfunctions that diagonalizes the kernel on the union of all \(_{t}\)'s. The following proposition provides a sufficient condition on the representation of the tasks in this basis to ensure the task arithmetic property:

**Proposition 1** (Simplified).: _Suppose that \(\{f_{t}^{*}\}_{t[T]}\) can be represented by the kernel \(k\). The kernel \(k\) is capable of performing task arithmetic with respect to \(\{f_{t}^{*}\}_{t[T]}\) and \(\{_{t}\}_{t[T]}\) if, for each task \(t\), there exists a subset of localized eigenfunctions such that i) \(()_{t}\) for each \(\) in the subset, and ii) the representation of \(f_{t}^{*}\) only involves these basis functions._

The proof and formal statement are deferred to Appendix C. Intuitively, if each task is represented with eigenfunctions that vanish outside the spatial region identified by the task support, the functions corresponding to different tasks do not interfere. Based on Proposition 1, it is natural to examine whether the NTK of CLIP models displays eigenfunctions localized in each task domain and if it represents the different tasks using these functions. According to the _representement theorem_ of kernels , after linear fine-tuning on task \(t\) with a training set \(\{(_{},f_{t}^{*}(_{}))\}_{[n_{t}]}\) and \(_{}_{t}\), the CLIP's predictor evaluated at a new point \(\) can be expressed as a linear combination of its kernel \(k_{}\) evaluated on \(\) and the training data, _i.e._, \(f_{}()=f(;_{0})+_{[n_{t}]}_{ }\,k_{}(_{},\,)\).

To explore whether CLIP models use localized eigenfunctions for task arithmetic, we diagonalize the matrix \((K_{})_{ij}=k_{}(_{i},\,_{j})\) with \(_{i}_{t}\), _i.e._, the task on which we trained, and \(_{j}_{t}_{t^{}}\), where \(_{t^{}}\) is the support of a control task. If the eigenfunctions used to represent \(f^{*}()\) are localized, then the power of the eigenvectors of \(K_{}\) must be concentrated in the points belonging to the dataset used for training. To measure this concentration, we introduce the local energy \(_{}()=_{}_{}^{2}()\), which sums the power of all the eigenfunctions \(_{}\) at a given point \(\).

In Figure 6, we plot this metric for a ViT-B/32 CLIP model trained on RESISC45 with Cars as control. We provide results for other task pairs in Appendix D.8. Notably, the local energy of the eigenfunctions that the predictor uses to represent the RESISC45 task is significantly higher for points belonging to the training dataset. This confirms the presence of eigenfunctions localized across the different data domains and the fact that task arithmetic occurs thanks to the use of those. Indeed, thanks to this localization, CLIP models can effectively separate the representation of different tasks and carry out task-specific operations without interference. We believe that further investigation into this intriguing localization phenomenon holds the potential to deepen our understanding of these models.

**Remark.** While we have shown that localized eigenfunctions can play a crucial role in task arithmetic, it is important to note that they are not always necessary. In fact, the task arithmetic property can hold even if the eigenfunctions used to represent a single task cancel outside the corresponding domain. Indeed, although eigenfunctions are linearly independent on the union of the domains, they are not necessarily linearly independent when evaluated on a single domain and, in general, can cancel out. However, if the eigenfunctions maintain their linear independence on each of the domains, _i.e._, they are _locally_ linear independent, then the existence of localized eigenfunctions becomes a necessary condition for task arithmetic. This means that if the eigenfunctions are locally linearly independent and not localized, task arithmetic is not possible. We provide some analytical examples of the latter case in Appendix C, including the NTKs of fully-connected and convolutional networks at initialization.

### Weight disentanglement emerges during pre-training

Task arithmetic is not exclusive to CLIP models. In fact, task arithmetic can also be performed on pre-trained text transformers [39; 86], such as GPT-2  or T5  and convolutional neural networks  as we also show in Appendices D.5 and D.7. However, it is still unclear if the origin of weight disentanglement comes from pre-training, or if it is a general property of deep networks.

To investigate this, we replicate the task addition experiments but employ randomly initialized ViTs instead of pre-trained ones. The results in Table 3 reveal that task arithmetic is not achievable on randomly initialized ViTs. Indeed, adding task vectors obtained from a random initialization \(_{0}^{}\) does not result in significant improvements in multi-task accuracy over random chance. This

Figure 6: **Eigenfunction localization.** Estimated support of the eigenfunctions of the NTK of a ViT-B/32 CLIP model trained on RESISC45. The plot shows the sum of the local energy of the eigenfunctions over a random subset of the training and control supports (RESISC45 and Cars, respectively).

holds true for both non-linear task vectors, \(^{}\), and linearized ones, \(^{}_{}\). In Appendix D.9, we further corroborate these findings by computing the disentanglement error and the NTK spectrum of randomly initialized models.

Therefore, we conclude that task arithmetic is a property acquired during pre-training. This observation goes beyond the traditional representation learning view of pre-training, emphasizing that pre-training not only leads to semantically disentangled feature representations but also to the disentanglement of the weights that govern the output on those semantic sets. Investigating the pre-training dynamics that give rise to such disentanglement is another interesting avenue for future research.

## 7 Related work

Weight interpolation and task arithmetic.A growing body of work is exploring the use of interpolations between model weights and task arithmetic to manipulate and enhance the capabilities of pre-trained models. In particular, several studies have shown that interpolating between a model's fine-tuned weights and its pre-trained initialization can lead to improved performance on single tasks, even surpassing their fine-tuning accuracies [27; 40; 57; 71; 72; 87]. In the multi-task setting, averaging the parameters of multiple fine-tuned models has been proposed to produce superior multi-task models [38; 39; 46; 89; 86] that avoid catastrophic forgetting [28; 58] and even provide a better starting point for subsequent fine-tuning [17; 23]. Interestingly, the benefits of weight ensembles and interpolations extend to models trained from scratch, as long as they are properly aligned before merging [3; 79]. This phenomenon has been observed to enhance downstream performance, further emphasizing the potential of weight interpolation and task arithmetic techniques such as the ones studied in this work.

Linear _vs_ non-linear regime.Extensive research has been conducted on comparing generalization and dynamical properties of neural networks in linear and non-linear regimes [8; 26; 65; 67; 82; 8] and investigating specific inductive biases [2; 7; 19; 53; 59; 81; 90]. In addition to theoretical understanding, several studies have applied linearized models for practical purposes, such as predicting fine-tuning generalization  and training speed , as well as enhancing calibration  and few-shot performance . Our work serves as another example of the utility of linearized models in certain scenarios where they do not only offer practical benefits but also provide valuable theoretical insights.

Feature disentanglement.The notion of feature disentanglement lies at the heart of representation learning, where ideal representations are assumed to separate distinct data variation factors along different directions in the feature space [1; 10; 35]. A multitude of approaches in generative modeling [14; 34; 73] and self-supervised learning [6; 13; 48; 69] strive to achieve this goal. Our investigation, however, explores a distinct aspect: _weight disentanglement_ within the framework of task arithmetic. Departing from the static perspective of feature disentanglement, weight disentanglement connects weight space and function space transitions, thereby enriching our understanding of disentanglement in neural networks from a functional standpoint. Several studies have previously attempted to exploit a similar notion by inducing the learning of task-specific subnetworks within a larger network [32; 36; 54; 55; 56; 83; 54; 56; 85]. To the best of our knowledge, our work is the first to demonstrate the natural emergence of such phenomena in specific semantically meaningful tasks during CLIP pre-training.

    &  &  &  \\  &  &  &  &  &  &  \\  Random init & \(f(\,;\,^{}_{0})\) & 5.3 & – & 4.8 & – & 5.2 & – \\  Non-lin. FT & \(f(\,;\,^{}_{0}+^{})\) & 48.5 & 5.5 & 40.6 & 4.5 & 18.0 & 4.8 \\ Linear. FT & \(f_{}(\,;\,^{}_{0}+^{}_ {})\) & 27.8 & 3.8 & 24.7 & 4.0 & 24.8 & 6.1 \\   

Table 3: **Task addition from random initialization.** We use the same setup as for the experiments in Table 1 but with task vectors obtained from fine-tuning randomly initialized ViTs. Results compare the average single-task accuracy (%) after fine-tuning and the multi-task accuracy (%) via task addition.

Conclusion

In this work, we conducted a thorough analysis of task arithmetic in deep neural networks, delving into its fundamental mechanisms and enhancing its performance. Our findings demonstrate that linearized models, governed by the NTK, outperform their non-linear counterparts in task arithmetic, thus providing a more effective approach for model editing. Crucially, we revealed that weight disentanglement plays a vital role in the success of task arithmetic, as distinct directions in weight space correspond to localized areas in the function space; and that it is an emergent property of pre-training.

A fascinating open question consists in understanding how weight disentanglement arises during pre-training and finding algorithms that enhance it. Another exciting research direction is investigating the potential of tangent spaces for editing other pre-trained models. In this sense, developing more efficient linearized models would be a significant leap forward in this field. These advancements could pave the way for novel approaches to model editing and deepen our understanding of the intricate relationship between weight space and function space in deep learning.