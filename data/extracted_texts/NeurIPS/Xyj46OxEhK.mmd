# Look Ma, No Hands!

Agent-Environment Factorization of Egocentric Videos

 Matthew Chang Aditya Prakash Saurabh Gupta

University of Illinois, Urbana-Champaign

{mc48, adityap9, saurabhg}@illinois.edu

###### Abstract

The analysis and use of egocentric videos for robotic tasks is made challenging by occlusion due to the hand and the visual mismatch between the human hand and a robot end-effector. In this sense, the human hand presents a nuisance. However, often hands also provide a valuable signal, _e.g._ the hand pose may suggest what kind of object is being held. In this work, we propose to extract a factored representation of the scene that separates the agent (human hand) and the environment. This alleviates both occlusion and mismatch while preserving the signal, thereby easing the design of models for downstream robotics tasks. At the heart of this factorization is our proposed Video Inpainting via Diffusion Model (VIDM) that leverages both a prior on real-world images (through a large-scale pre-trained diffusion model) and the appearance of the object in earlier frames of the video (through attention). Our experiments demonstrate the effectiveness of VIDM at improving inpainting quality on egocentric videos and the power of our factored representation for numerous tasks: object detection, 3D reconstruction of manipulated objects, and learning of reward functions, policies, and affordances from videos.

+
Footnote â€ : Project website with code, video, and models: https://matthewchang.github.io/vidm.

## 1 Introduction

Observations of humans interacting with their environments, as in egocentric video datasets , hold the potential to scale up robotic policy learning. Such videos offer the possibility of learning affordances , reward functions  and object trajectories . However, a key bottleneck in these applications is the mismatch in the visual appearance of the robot and human hand, and the occlusion caused by the hand.

Human hands can often be a nuisance. They occlude objects of interaction and induce a domain gap between the data available for learning (egocentric videos) and the data seen by the robot at execution time. Consequently, past work has focused on removing hands from the scene by masking  or inpainting . However, hands also provide a valuable signal for learning. The hand pose may reveal object affordances, and the approach of the hand toward objects can define dense reward functions for learning policies.

In this work, we propose the use of a _factored agent and environment representation_. The agent representation is obtained by segmenting out the hand, while the environment representation is obtained by inpainting the hand out of the image (Fig. 1). We argue that such a factored representation removes the nuisance, but at the same time preserves the signal for learning. Furthermore, the factorization allows independent manipulation of the representation as necessary. _E.g._, the agent representation could be converted to a form that is agnostic to the embodiment for better transfer across agents. This enables applications in 2D/3D visual perception and robot learning (see Fig. 2).

But how do we obtain such a factored representation from raw egocentric videos? While detection & segmentation of hands are well-studied [13; 103; 70], our focus is on inpainting. Here, rather than just relying on a generic prior over images, we observe that the past frames may already have revealed the true appearance of the scene occluded by the hand in the current time-step. We develop a video inpainting model that leverages both these cues. We use a large-scale pre-trained diffusion model for the former and an attention-based lookup of information from the past frames for the latter. We refer to it as _Video Inpainting via Diffusion_ (VIDM). Our approach outperforms DLFormer , the previous state-of-the-art for video inpainting, by a large margin (Tab. 1) and is \(8\) faster at test time.

Next, we demonstrate the ability of the factored representation across tasks spanning 2D/3D visual perception to robot learning. Specifically, we adopt 5 existing benchmark tasks: a) 2D detection of objects of interaction , b) 3D reconstruction of hand-held objects [95; 59], c) learning affordances (where to interact and how) from egocentric videos , d) learning reward functions, and e) their use for interactive policy learning . We show how selectively choosing and modifying aspects of the factored representation improves performance across all of these tasks compared to existing approaches. We believe the advances presented in this paper will enable the use of egocentric videos for learning policies for robots.

## 2 Related Work

**Robot Learning from Hand-Object Interaction Data.** Many past works have sought to use human-object interaction data for robotic tasks. Researchers have used videos to predict: regions of interactions [19; 50], grasps / hand pose afforded by objects [19; 96], and post-grasp trajectories [42; 2].

Figure 1: **Agent-Environment Factorization of Egocentric Videos.** Occlusion and the visual mismatch between humans and robots, make it difficult to use egocentric videos for robotic tasks. We propose a pixel-space factorization of egocentric videos into agent and environment representations (AEF, Sec. 3). An agent representation \(I_{t}^{}\) is obtained using a model to segment out the agent. The environment representation \(I_{t}^{}\) is obtained by inpainting out the agent from the original image using VIDM, a novel Video Inpainting Diffusion Model (Sec. 4). AEF enables many different visual perception and robotics tasks (Fig. 2 and Sec. 5).

Figure 2: **Agent-Environment Factored (AEF) representations enable many applications.** (a) For visual perception tasks (Sec. 5.2 and 5.3), the unoccluded environment in \(I_{t}^{}\) can be used in addition to the original image. (b) For affordance learning tasks (Sec. 5.4), the unoccluded environment \(I_{t}^{}\) can be used to predict relevant desirable aspects of the agent in \(I_{t}^{}\). (c) For reward learning tasks (Sec. 5.5 and 5.6) agent representations can be transformed into agent-agnostic formats for more effective transfer across embodiments.

 predict 3D shapes for hand-held objects.  learn reward functions from videos for learning real-world policies. Others use human-object interaction videos to pre-train feature representations for robotic policies . While some papers ignore the gap between the hand and the robot end-effector , others adopt ways to be _agnostic_ to the hand by masking it out , inpainting it , converting human pixels to robot pixels , or learn embodiment invariant representations . Instead, we pursue a factored representation that allows downstream applications to choose how to use the agent and the environment representations. Different from , we remove only the hand and retain the object of interaction.

**Diffusion Models** have been successful at unconditional  and conditional  image synthesis for several applications, _e.g._ text-to-image generation , image editing , video generation , video prediction and infilling  (similar to video inpainting, but focused on generating entire frames), 3D shape synthesis , self-driving applications . While diffusion models produce impressive results, they are computationally expensive due to the iterative nature of the generative process. To mitigate this limitation, several improvements have been proposed, _e.g._ distillation , faster sampling , cascaded  and latent  diffusion models. Among these, latent diffusion models (LDM) are the most commonly used variant. LDMs use an autoencoder to compress the high-dimensional image inputs to a lower-dimension latent space and train diffusion models in this latent space. In our work, we build off LDMs.

**Inpainting** requires reasoning about the local and global structure of the image to perform image synthesis. While earlier approaches focus on preserving local structure , they are unable to capture complex geometries or large missing areas. Recent works alleviate these issues using neural networks  to incorporate large receptive fields , intermediate representations (_e.g._ edges , segmentation maps ), adversarial learning . With advancements in generative modeling, diffusion models have also been very effective on this task . These approaches operate by learning expressive generative priors. An alternative is to utilize videos  to reason about temporal consistency , leverage optical flow , utilize neural radiance fields , or extract context about the missing parts from the previous frames . Recent video-based inpainting methods incorporate transformers , cycle-consistency  & discrete latent space  to achieve state-of-the-art results. While existing approaches inpaint the entire video, we focus on inpainting a specific frame (previous frames are used as context) as required by downstream tasks. Our work adopts diffusion models for video-based inpainting to learn factored agent-environment representations, specifically, building on the single-frame inpainting model from .

## 3 Agent-Environment Factored (AEF) Representations

Motivated by the recent success of generative models, we develop our factorization directly in the pixel space. Given an image \(I_{t}\) from an egocentric video, our factored representation decomposes

Figure 3: **Video Inpainting Diffusion Models (VIDM). We extend pre-trained single-frame inpainting diffusion models  to videos. Features from context frames \((I_{t-h},,I_{t-1})\) are introduced as additional inputs into the Attention Block \(A\). We repeat the multi-frame attention block 8 times (4 to encode and 4 to decode) to construct the U-Net  that conducts 1 step of denoising. The U-Net operates in the VQ encoder latent space .**

it into \(I_{t}^{}\) and \(I_{t}^{}\). Here, \(I_{t}^{}\) shows the environment without the agent, while \(I_{t}^{}\) shows the agent (Fig. 1) without the environment. These two images together constitute our Agent-Environment Factored (AEF) Representation.

**Using the Factorization.** This factorization enables the independent use of agent / environment information as applicable. For example, when hands are a source of nuisance (_e.g._ when object detectors are pre-trained on non-egocentric data without hands in Fig. 2a), we can use \(I_{t}^{}\) in addition to \(I_{t}\) to get a better view of the scene. In other situations, it may be desirable to predict agent properties afforded by different parts of the environments (Fig. 2b). In such a situation, \(I_{t}^{}\) can be used to predict the necessary aspects of \(I_{t}^{}\). In yet other situations, while the location of the agent provides useful information, its appearance may be a nuisance (_e.g._ when learning dense reward functions from egocentric videos for finetuning policies for robots in Fig. 2c). In such a situation, models could be trained on \(I_{t}^{}\) and \(g(I_{t}^{})\), where the function \(g\) generates the necessary abstractions of the agent image.

**Extracting the Factorization from Egocentric Videos.** For \(I_{t}^{}\), we employ segmentation models to produce masks for the agent in \(I_{t}\). For egocentric videos, we use the state-of-the-art VISOR models  to segment out the hand. When the depicted agent is a robot, we use DeepLabV3  pretrained on MS-COCO  and fine-tuned using manually annotated robot end-effector frames. Robot end-effectors have a distinctive appearance and limited variety, hence we can train a high-performing model with only a small amount of training data. We denote the agent segmentation in \(I_{t}\) by \(m_{t}^{}\).

Extracting \(I_{t}^{}\) is more challenging because of significant scene occlusion induced by the agent in a given frame. Naively extracting \(I_{t}^{}\) by just masking out the agent leaves artifacts in the image. Thus, we inpaint the image to obtain \(I_{t}^{}\). We condition this inpainting on the current frame \(I_{t}\) and previous frames from the video, _i.e._\(\{I_{t-h},,I_{t-1}\}\) since occluded parts of the scene in \(I_{t}\) may actually be visible in earlier frames. This simplifies the inpainting model, which can _steal_ pixels from earlier frames rather than only relying on a generic generative image prior. We denote the inpainting function as \(p(I_{t},m_{t}^{},\{m_{t-h}^{},,m_{t-1}^{}\},\{I_{t-h},,I_{t-1}\})\), and describe it next.

## 4 Video Inpainting via Diffusion Models (VIDM)

The inpainting function \(p(I_{t},m_{t}^{},\{m_{t-h}^{},,m_{t-1}^{}\},\{I_{t-h},,I_{t-1}\})\) inpaints the mask \(m_{t}^{}\) in image \(I_{t}\) using information in images \(I_{t-h}\) through \(I_{t}\). The function \(p\) is realized through a neural model that uses attention  to extract information from the previous frames. We train this neural network using latent diffusion , which employs denoising diffusion  in the latent space obtained from a pre-trained vector quantization (VQ) encoder .

**Model Architecture.** We follow prior work  and train a model \(_{}(z_{u},u,c)\), where \(z_{u}\) is the noisy version of the diffusion target \(z\) at diffusion timestep \(u\), and \(c\) is the conditioning (note we denote diffusion timesteps as \(u\) to avoid conflict with \(t\), which refers to video timesteps). For each diffusion timestep \(u\), this model predicts the noise \(\) added to \(z\). This prediction is done in the latent space of the VQ encoder, and the diffusion model is implemented using a U-Net architecture  with interleaving attention layers  to incorporate conditioning on past frames. The use of attention enables _learning_ of where to steal pixels from, eliminating the need for explicit tracking. Furthermore, this allows us to condition on previous frames while reusing the weights from diffusion models pre-trained on large-scale image datasets.

We work with \(256 256\) images. The VQ encoder reduces the spatial dimension to 64 \(\) 64, the resolution at which the denoising model is trained. The commonly used input for single-frame inpainting with a U-Net diffusion model is to simply concatenate the masked image, mask, and noisy target image along the channel dimension . This gives an input shape of \((2d+1,64,64)\) where \(d\) is the dimension of the latent VQ codes. This U-Net model stacks blocks of convolutions, followed by self-attention within the spatial features. VIDM also concatenates the noisy target \(z_{u}\) with conditioning \(c\) (_i.e._ the context images, \(I_{t-h},,I_{t}\), and masks, \(m_{t-h}^{}\), \(\), \(m_{t}^{}\)) however, we stack context frames (_i.e._\(t-h\) through \(t-1\)) along a new dimension. As such, our U-Net accepts a \((2d+1,h+1,64,64)\) tensor, where \(h\) is the number of additional context frames. We perform convolutions on the \(h+1\) sets of spatial features in parallel but allow attention across all frames at attention blocks. Consequently, we can re-use the weights of an inpainting diffusion model trained on single images. The final spatial features are combined using a \((h+1) 1 1\) convolution to form the final spatial prediction. Following prior work, the loss is \(_{z,c,(0,1),u}\|-_{ }(z_{u},u,c)\|_{1}\).

**Training Dataset.** The data for training the model is extracted from Epic-Kitchens  and a subset of Ego4D  (kitchen videos). Crucially, we don't have real ground truth (_i.e._ videos with and without human hands) for training this model. We synthetically generate training data by masking out hand-shaped regions from videos and asking the model to inpaint these regions. We use hand segment sequences from VISOR  as the pool of hand-shaped masks. In addition, we also generate synthetic masks using the scheme from . We use a 3-frame history (_i.e._\(h=3\)). Context images are drawn to be approximately \(0.75\)s apart. We found it important to not have actual human hands as prediction targets for the diffusion model. Thus, we omit frames containing hands from Ego4D (using hand detector from ) and do not back-propagate loss on patches that overlap a hand in Epic-Kitchens (we use ground truth hand annotations on Epic-Kitchens frames from VISOR). We end up with 1.5 million training frames in total.

**Model Training.** We initialize our networks using pre-trained models. Specifically, we use the pre-trained VQ encoder-decoder from  which is kept fixed. The latent diffusion model is pre-trained for single-frame inpainting on the Places  dataset and is finetuned for our multi-frame inpainting task. We train with a batch size of 48 for 600k iterations on 8 A40 GPUs for 12 days. At inference time we use 200 denoising steps to generate images.

Our overall model realizes the two desiderata in the design of a video inpainting model. First, conditioning on previous frames allows occluded regions to be filled using the appearance from earlier frames directly (if available). Second, the use of a strong data-driven generative prior (by virtue of starting from a diffusion model pre-trained on a large dataset) allows speculation of the content of the masked regions from the surrounding context.

## 5 Experiments

We design experiments to test the inpainting abilities of VIDM (Sec. 5.1), and the utility of our factorized representation for different visual perception and robot learning tasks (Sec. 5.2 to Sec. 5.6). For the former, we assess the contribution of using a rich generative prior and conditioning on past frames. For the latter, we explore 5 benchmark tasks: object detection, 3D object reconstruction, affordance prediction, learning reward functions, and learning policies using learned rewards. We compare to alternate representations, specifically ones used in past papers for the respective tasks.

### Reconstruction Quality Evaluation

We start by assessing the quality of our proposed inpainting model, VIDM. Following recent literature , we evaluate the PSNR, SSIM and FID scores of inpainted frames against ground truth images.

**Evaluation Dataset.** We select 33 video clips that do not contain any hands from the 7 held-out participants from the Epic-Kitchens dataset . These clips are on average 3.5 seconds long and span a total of 5811 frames. For each clip, we mask out a sequence of hand-shaped masks mined from the VISOR dataset . The prediction problem for inpainting models is to reproduce the regions underneath the hand-shaped masks. This procedure simulates occlusion patterns that models will need to inpaint at test time, while also providing access to ground truth pixels for evaluation.

  
**Inpainting Method** & **PSNR\(\)** & **SSIM\(\)** & **FID\(\)** & **Runtime \(\)** \\  Latent Diffusion  & 28.29 & 0.931 & 27.29 & 12.5s / image \\ Latent Diffusion (fine-tuned) & 28.27 & 0.931 & 27.50 & 12.5s / image \\ DLFormer  & 26.98 & 0.922 & 51.74 & 106.4s / image \\ VIDM (Ours) & **32.26** & **0.956** & **10.37** & 13.6s / image \\ VIDM trained with hands visible & 31.81 & 0.953 & 11.04 & 13.6s / image \\   

Table 1: **In-painting evaluation** on held-out clips from Epic-Kitchens . Use of strong generative priors and past frames allows our model to outperform past works that use only one or the other.

**Baselines.** We compare against 3 baseline models: a) the single-frame latent diffusion inpainting model  trained on the Places  dataset that we initialize from, b) this model but finetuned on _single images_ from the same Epic-Kitchens dataset that we use for our training, c) _DLFormer_, the current state-of-the-art video inpainting model. DLFormer trains one model per clip and sees every frame in the test clip, unlike our model which only sees the previous 3 frames for each prediction. Finally, we include one ablation: VIDM trained with hands visible in the training objective (_i.e._ gradients propagate to all pixels) instead of the objective used in VIDM (stop-gradient on pixels with hands, see Section 3 for details).

**Results.** Tab. 1 reports the results. We can see our model outperforms all baselines. Processing context frames with attention allows our model to improve upon single-frame diffusion baselines. Training with multiple frames and hands visible improves performance, but this training procedure requires the model to output hands during training. Consequently, inpainted regions sometimes include hands at test time (see Figure S5). Our model also outperforms DLFormer , while being \(8\) faster and using only 3 frames of history. Fig. 4 shows example visualizations of inpainting results.

### Application 1: Object Detection

The detection and recognition of objects being interacted with (by humans or robots) is made challenging due to occlusion caused by the hand / end-effector. While existing object detection datasets may include objects held and occluded by human hands, objects may be occluded in ways outside of the training distribution (_e.g._ when running COCO-trained detectors on egocentric videos). Furthermore, very few object detection datasets include examples of occlusion by a robot end-effector. We hypothesize that by removing the agent from the scene, off-the-shelf detectors may work better without requiring any additional training.

**Protocol.** We evaluate this hypothesis by testing COCO -trained Mask RCNN detectors  on egocentric frames showing hand-object interaction from VISOR . We only evaluate on object

  
**Image Used** & AR\({}_{}\)@1 & AR\({}_{}\)@5 & AR\({}_{}\)@10 & AR\({}_{}\)@1 & AR\({}_{}\)@5 & AR\({}_{}\)@10 \\  Raw Image (_i.e._\(I_{t}\)) & 0.137 & 0.263 & 0.272 & 0.265 & 0.530 & 0.551 \\ Masked Image (_i.e._\(I_{t}\) with \(m_{t}^{}\) blacked out) & 0.131 & 0.236 & 0.245 & 0.266 & 0.474 & 0.495 \\ \(I_{t}^{}\) inpainted using Latent Diffusion  & 0.149 & 0.259 & 0.270 & 0.299 & 0.517 & 0.541 \\ \(I_{t}^{}\) inpainted using Latent Diffusion (finetuned) & 0.154 & 0.262 & 0.271 & 0.305 & 0.519 & 0.540 \\ \(I_{t}^{}\) inpainted using VIDM (Ours w/o factorization) & 0.163 & 0.268 & 0.277 & 0.317 & 0.521 & 0.543 \\ \(I_{t}\) and \(I_{t}^{}\) inpainted using VIDM (Ours w/ factorization) & **0.170** & **0.379** & **0.411** & **0.334** & **0.681** & **0.735** \\   

Table 2: Average recall of detections from a COCO-trained Mask RCNN  on active objects (_i.e._ objects undergoing interaction) from VISOR . See Sec. 5.2 for more details.

Figure 4: Our approach (VIDM) is able to correctly steal background information from past frames (top row, oranges on the bottom right) and also correctly reconstructs the wok handle using strong object appearance priors (bottom row).

classes that appear in both COCO and VISOR. VISOR  only annotates the active object and background objects have not been annotated. Thus, we measure performance using Average Recall (AR) as opposed to Average Precision. Specifically, we predict \(k\) (\(=1,5,10\) in our experiments) boxes per class per image and report box-level Average Recall at \(0.5\) box overlap (AR\({}_{0.5}\)) and integrated over box overlap thresholds \(\{0.5,0.55,,0.95\}\) (AR\({}_{}\)).

**Results.** Tab. 2 reports the average recall when using different images as input to the COCO-trained Mask RCNN detector. We compare against a) just using the raw image (_i.e._\(I_{t}\)) as would typically be done, b) using a masked image (_i.e._\(I_{t}\) with \(m_{t}^{}\) blacked out) a naive way to remove the hand, c) different methods for inpainting the agent pixels (pre-trained and finetuned single-frame Latent Diffusion Model  and our inpainting scheme, VIDM), and d) using both the original image and the inpainted image (_i.e._ both \(I_{t}\) and \(I_{t}^{}\)) as enabled by AEF. When using both images, we run the pre-trained detector on each image, merge the predictions, and return the top-\(k\).

Naive masking introduces artifacts and hurts performance. Inpainting using a single-frame model helps but not consistently. Our video-based inpainter leads to more consistent gains over the raw image baseline, outperforming other inpainting methods. Our full formulation leads to the strongest performance achieving a 24%-51% relative improvement over the raw image baseline.

### Application 2: 3D Reconstruction of Hand-Held Objects

Past work has tackled the problem of 3D reconstruction of hand-held objects . Here again, the human hand creates occlusion and hence nuisance, yet it provides valuable cues for the object shape . Similar to Sec. 5.2, we hypothesize that the complete appearance of the object, behind the hand, may provide more signal to a 3D reconstruction model.

**Protocol.** We adopt the state-of-the-art approach from Ye _et al._. They design a custom neural network architecture that is trained with full supervision on the ObMan dataset . Their model just accepts \(I_{t}\) as input. To test whether the complete appearance of the object is helpful, we additionally input \(I_{t}^{}\) to their model. As ObMan is a synthetic dataset, we use _ground-truth \(I_{t}^{}\) images obtained by rendering the object without the hand_. This evaluation thus only measures the impact of the proposed factorization. We use the standard metrics: F1 score at 5 & 10 mm, and chamfer distance.

**Results.** Using \(I_{t}\) and _ground truth \(I_{t}^{}\)_ improves upon just using the raw image (as used in ) across all metrics: the F1 score at 5mm increases from 0.41 to 0.48, the F1 score at 10mm increases from 0.62 to 0.68, and the chamfer distance improves from 1.23 to 1.06. As we are using ground truth \(I_{t}^{}\) here, this is not surprising. But, it does show the effectiveness of our proposal of using factorized agent-environment representations.

### Application 3: Affordance Prediction

Past works  have used videos to learn models for labeling images with regions of interaction and afforded grasps. While egocentric videos directly show both these aspects, learning is made challenging because a) the image already contains the answer (the location and grasp type exhibited by the hand), and b) the image patch for which the prediction needs to be made is occluded by the hand. Goyal _et al._ mask out the hand and train models to predict the hand pixels from the surrounding context. This addresses the first problem but makes the second problem worse. After masking there is even more occlusion. This application can be tackled using our AEF framework, where we use inpainted images \(I_{t}^{}\) to predict aspects of the agent shown in \(I_{t}^{}\).

    &  &  \\  & 0\% Slack & 1\% Slack & **by Objects (GAO)** \\  Masked image (\(I_{t}\) with \(m_{t}^{}\) blacked out)  & 47.50 \(\) 2.77 & 54.03 \(\) 3.27 & 35.53 \(\) 3.67 \\ \(I_{t}^{}\) (inpainted using VIDM) (Ours) & **50.77 \(\) 0.81** & **57.10 \(\) 1.01** & **41.00 \(\) 3.99** \\   

Table 3: Average Precision on the Region-of-Interaction task and mAP for Grasps Afforded by Objects task from . Using our model to remove the hands improves performance _vs._ placing a square mask over hands as done in . See Sec. 5.4 for more details.

**Protocol.** We adopt the training scheme from  (current state-of-the-art on this benchmark), but instead of using masked images as input to the model, we use images inpainted using our VIDM model. We adopt the same metrics and testing data as used in  and report performance on both the Region of Interaction (RoI) and Grasps Afforded by Objects (GAO) tasks. We test in a small data regime and only use 1000 images for training the affordance prediction models from . We report mean and standard deviation across 3 trainings.

**Results.** Tab. 3 presents the metrics. Across both tasks, the use of our inpainted images leads to improved performance _vs._ using masked-out images.

### Application 4: Learning Reward Functions from Videos

Difficulty in manually specifying reward functions for policy learning in the real world has motivated previous work to learn reward functions from human video demonstrations . The visual mismatch between the human hand and robot end-effector is an issue. Past work  employs inpainting to circumvent this but consequently loses the important signal that the hand motion provides. Our factored representation in AEF provides a more complete solution and, as we will show, leads to improved performance over just using the raw data  and inpainting out the hand .

**Protocol.** We train a reward predictor on video clips from Epic-Kitchens  and assess the quality of the reward predictor on trajectories from a robotic end-effector. Specifically, we consider three tasks: opening a drawer, opening a cabinet, and opening a refrigerator. We use the action annotations from  to find video segments showing these tasks to obtain 348, 419, and 261 clips respectively. The reward function is trained to predict the task progress (0 at clip start and 1 at clip end) from a single image. The learned function is used to rank frames from trajectories of a robotic end-effector, specifically the reacher-grabber assistive tool from  captured using a head-mounted GoPro HERO7. We collected 10, 10, and 5 trajectories respectively for drawers, cupboards, and refrigerators. These robotic trajectories represent a large visual domain gap from the human-handed training demonstrations and test how well the learned reward functions generalize. Following , we measure Spearman's rank correlation between the rewards predicted by the learned reward function and the ground truth reward (using frame ordering) on the pseudo-robotic demonstrations. The Spearman's rank correlation only measures the relative ordering and ignores the absolute magnitude.

**Results.** Tab. 4 reports the Spearman's rank correlations for the different tasks. We compare the use of different representations for learning these reward functions: a) raw images (_i.e._\(I_{t}\)), b) just inpainting (_i.e._\(I_{t}^{}\)), and c) our factored representation (_i.e._\(I_{t}^{}\) and \(I_{t}^{}\)). For our factored representation, we map \(I_{t}^{}\) through a function \(g\) to an agent-agnostic representation by simply placing a green dot at the highest point on the end-effector mask (human hand or robot gripper) in the image plane (see Fig. 5), thus retaining information about where the hand is in relation to the scene. Our factored model, AEF, correlates the best with ground truth. Just painting out the end-effector loses important information about where the end-effector is in relation to objects in the scene. Raw frames maintain this information but suffer from the visual domain gap.

### Application 5: Real-World Policy Learning using Learned Rewards

Motivated by the success of the offline evaluations in Sec. 5.5, we use the learned reward function for opening drawers to learn a drawer opening policy in the real world using a Stretch RE2 robot (see Fig. 5 (left)).

  
**Input Representation** & **Drawer** & **Cupboard** & **Fridge** & **Overall** \\  Raw Imges (_i.e._\(I_{t}\)) & 0.507 & 0.507 & 0.660 & 0.558 \\ Inpainted only (_i.e._\(I_{t}^{}\) as proposed in ) & 0.574 & 0.570 & 0.671 & 0.605 \\ AEF (\(I_{t}^{}\) and \(g(I_{t}^{})\)) (Ours) & **0.585** & **0.582** & **0.676** & **0.614** \\   

Table 4: Spearmanâ€™s rank correlations of reward functions learned on Epic-Kitchens  when evaluated on robotic gripper sequences for opening drawer, cupboard, and fridge tasks. Our factored representation achieves better performance than raw images or environment-only representation.

**Protocol.** We position the robot base at a fixed distance from the target drawer. We use a 1D action space that specifies how far from the base the hand should extend to execute a grasp, before retracting. The drawer opens if the robot is able to correctly grasp the handle. We use a GoPro camera to capture RGB images for reward computation. The total reward for each trial is the sum of the predicted reward at the point of attempted grasp, and after the arm has been retracted. We use the Cross-Entropy Method (CEM)  to train the policy. We collect 20 trials in each CEM iteration and use the top-7 as _elite samples_. Actions for the next CEM iteration are sampled from a Gaussian fit over these elite samples. We use the success rate at the different CEM iterations as the metric.

**Results.** Similar to Sec. 5.5, we compare against a) raw images (_i.e._\(I_{t}\)), b) inpainted images (_i.e._ just \(I_{t}^{}\) as per proposal in ), and c) our factored representation, AEF. As before, we use a dot at the end-effector location as the agent-agnostic representation of \(I_{t}^{}\). We use our VIDM for inpainting images for all methods. We finetune a DeepLabV3  semantic segmentation model (on 100 manually annotated images) to segment out the Stretch RE2 end-effector. Fig. 5 (right) shows the success rate as a function of CEM iterations. Reward functions learned using our AEF representation outperform baselines that just use the raw image or don't use the factorization.

## 6 Discussion

Our experiments have demonstrated the effectiveness of our inpainting model. Specifically, the use of strong priors from large-scale pre-trained inpainting diffusion models and the ability to steal content from previous frames allows VIDM to outperform past methods that only use one or the other cue (pre-trained LDM  and DLFormer  respectively). This is reflected in qualitative metrics in Tab. 1 and also in qualitative visualizations in Fig. 4.

Our powerful video inpainting model (along with semantic segmentation models that can segment out the agent) enables the extraction of our proposed Agent-Environment Factored (AEF) representation from egocentric video frames in pixel space. This allows intuitive and independent manipulation of the agent and environment representations, enabling a number of downstream visual perception and robotics applications. Specifically, we demonstrated results on 5 benchmarks spanning 3 categories. For visual perception tasks (object detection and 3D reconstruction of hand-held objects), we found additionally using the environment representation, which has the agent removed, improved performance. For affordance prediction tasks (region of interaction and grasps afforded by objects), we found that using the inpainted environment representation to be more effective at predicting relevant aspects of the agent representation than naive masking of the agent as used in past work. For robotic tasks (learning rewards from videos), the factored representation allowed easy conversion of the agent representation into an agent-agnostic form. This led to better transfer of reward functions across embodiments than practices adopted in past work (using the original agent representation and ignoring the agent altogether) both in offline evaluations and online learning on a physical platform.

Given that all information comes from the masked input frames, one might wonder, what additional value does the proposed inpainting add? First, it provides the practical advantage that each downstream application doesn't need to separately pay the implicit training cost for interpreting

Figure 5: **Real-world experiment setup and results.** (left) Raw views from camera, (center) Inpainted image with agent-agnostic representation (green dot at top-pixel of end-effector). (right) Success rate as a function of CEM iterations.

masked-out images. Inpainted images are like real images, allowing ready re-use of the off-the-shelf models. Furthermore, the data complexity of learning a concept under heavy occlusion may be much higher than without. A _foundation_ inpainting model can leverage pre-training on large-scale datasets, to inpaint the occlusion. This may be a more data-efficient way to learn concepts as our experiments have also shown.

## 7 Limitations and Broader Impact

As with most work with diffusion models, inference time for our method is 13.6 seconds which renders real-time use in robotic applications infeasible. AEF only inpaints out the hand to recover the unoccluded environment. There may be situations where the environment might occlude the agent and it may be useful to explore if similar inpainting could be done for agent occlusion.

At a broader level, while our use of the video inpainting model was for robotic applications, the VIDM model can also be thought of a general-purpose video inpainting model. This inherits the uncertain societal implications of generative AI models.