# Pure Message Passing Can Estimate

Common Neighbor for Link Prediction

 Kaiwen Dong\({}^{1,2}\) Zhichun Guo\({}^{1,2}\) Nitesh V. Chawla\({}^{1,2}\)

\({}^{1}\)Computer Science and Engineering, University of Notre Dame

\({}^{2}\)Lucy Family Institute for Data and Society, University of Notre Dame

{kdong2, zguo5, nchawla}@nd.edu

###### Abstract

Message Passing Neural Networks (MPNNs) have emerged as the _de facto_ standard in graph representation learning. However, when it comes to link prediction, they are not always superior to simple heuristics such as Common Neighbor (CN). This discrepancy stems from a fundamental limitation: while MPNNs excel in node-level representation, they stumble with encoding the joint structural features essential to link prediction, like CN. To bridge this gap, we posit that, by harnessing the orthogonality of input vectors, pure message-passing can indeed capture joint structural features. Specifically, we study the proficiency of MPNNs in approximating CN heuristics. Based on our findings, we introduce the Message Passing Link Predictor (MPLP), a novel link prediction model. MPLP taps into quasi-orthogonal vectors to estimate link-level structural features, all while preserving the node-level complexities. We conduct experiments on benchmark datasets from various domains, where our method consistently outperforms the baseline methods, establishing new state-of-the-arts.

## 1 Introduction

Link prediction is a cornerstone task in the field of graph machine learning, with broad-ranging implications across numerous industrial applications. From identifying potential new acquaintances on social networks  to predicting protein interactions , from enhancing recommendation systems  to completing knowledge graphs , the impact of link prediction is felt across diverse domains. Recently, with the advent of Graph Neural Networks (GNNs)  and more specifically, Message-Passing Neural Networks (MPNNs) , these models have become the primary tools for tackling link prediction tasks. Despite the resounding success of MPNNs in the realm of node and graph classification tasks , it is intriguing to note that their performance in link prediction does not always surpass that of simpler heuristic methods .

Zhang et al.  highlights the limitations of GNNs/MPNNs for link prediction tasks arising from its intrinsic property of permutation invariance. Owing to this property, isomorphic nodes invariably receive identical representations. This poses a challenge when attempting to distinguish links whose endpoints are isomorphic nodes. As illustrated in Figure 0(a), nodes \(v_{1}\) and \(v_{3}\) share a Common Neighbor \(v_{2}\), while nodes \(v_{1}\) and \(v_{5}\) do not. Ideally, due to their disparate local structures, these two links \((v_{1},v_{3})\) and \((v_{1},v_{5})\) should receive distinct predictions. However, the permutation invariance of MPNNs results in identical representations for nodes \(v_{3}\) and \(v_{5}\), leading to identical predictions for the two links. As Zhang et al.  asserts, such node-level representation, even with the most expressive MPNNs, **cannot** capture structural link representation such as Common Neighbors (CN), a critical aspect of link prediction.

In this work, we posit that the pure Message Passing paradigm  can indeed capture structural link representation by exploiting orthogonality within the vector space. We begin by presenting amotivating example, considering a non-attributed graph as depicted in Figure 0(a). In order to fulfill the Message Passing's requirement for node vectors as input, we assign a one-hot vector to each node \(v_{i}\), such that the \(i\)-th dimension has a value of one, with the rest set to zero. These vectors, viewed as _signatures_ rather than mere permutation-invariant node representations, can illuminate pairwise relationships. Subsequently, we execute a single iteration of message passing as shown in Figure 0(b), updating each node's vector by summing the vector of its neighbors. This process enables us to compute CN for any node pair by taking the inner product of the vectors of the two target nodes.

At its core, this naive method employs an orthonormal basis as the node signatures, thereby ensuring that the inner product of distinct nodes' signatures is consistently zero. While this approach effectively computes CN, its scalability poses a significant challenge, given that its space complexity is quadratically proportional to the size of the graph. To overcome this, we draw inspiration from DotHash  and capitalize on the premise that the family of vectors almost orthogonal to each other swells exponentially, even with just linearly scaled dimensions . Instead of relying on the orthogonal basis, we can propagate these quasi-orthogonal (QO) vectors and utilize the inner product to estimate the joint structural information of any node pair.

In sum, our paper presents several pioneering advances in the realm of GNNs for link prediction:

* We are the first, both empirically and theoretically, to delve into the proficiency of GNNs in approximating heuristic predictors like CN for link prediction. This uncovers a previously uncharted territory in GNN research.
* Drawing upon the insights gleaned from GNNs' capabilities in counting CN, we introduce **MPLP**, a novel link prediction model. Uniquely, MPLP discerns joint structures of links and their associated substructures within a graph, setting a new paradigm in the field.
* Our empirical investigations provide compelling evidence of MPLP's dominance. Benchmark tests reveal that MPLP not only holds its own but outstrips state-of-the-art models in link prediction performance.

## 2 Preliminaries and Related Work

Notations.Consider an undirected graph \(G=(V,E,)\), where \(V\) represents the set of nodes with cardinality \(n\), indexed as \(\{1,,n\}\), \(E V V\) denotes the observed set of edges, and \(_{i,:}^{F_{x}}\) encapsulates the attributes associated with node \(i\). Additionally, let \(_{v}\) signify the neighborhood of a node \(v\), that is \(_{v}=\{u|(u,v)=1\}\) where the function \((,)\) measures the shortest path distance between two nodes. Furthermore, the node degree of \(v\) is given by \(d_{v}=|_{v}|\). To generalize, we introduce the shortest path neighborhood \(_{v}^{s}\), representing the set of nodes that are \(s\) hops away from node \(v\), defined as \(_{v}^{s}=\{u|(u,v)=s\}\).

Link predictions.Alongside the observed set of edges \(E\), there exists an unobserved set of edges, which we denote as \(E_{c} V V E\). This unobserved set encompasses edges that are either absent from the original observation or are anticipated to materialize in the future within the graph \(G\)

Figure 1: (a) Isomorphic nodes result in identical MPNN node representation, making it impossible to distinguish links such as \((v_{1},v_{3})\) and \((v_{1},v_{5})\) based on these representations. (b) MPNN counts Common Neighbor through the inner product of neighboring nodesâ€™ one-hot representation.

Consequently, we can formulate the link prediction task as discerning the unobserved set of edges \(E_{c}\). Heuristics link predictors include Common Neighbor (CN) , Adamic-Adar index (AA) , and Resource Allocation (RA) . CN is simply counting the cardinality of the common neighbors, while AA and RA count them weighted to reflect their relative importance as a common neighbor.

\[(u,v)=_{k_{u}_{v}}1;\ \ (u,v)=_{k_{u} _{v}}};\ \ (u,v)=_{k_{u} _{v}}}.\] (1)

Though heuristic link predictors are effective across various graph domains, their growing computational demands clash with the need for low latency. To mitigate this, approaches like ELPH  and DotHash  propose using estimations rather than exact calculations for these predictors. Our study, inspired by these works, seeks to further refine techniques for efficient link predictions. A detailed comparison with related works and our method is in Appendix A.

GNNs for link prediction.The advent of graphs incorporating node attributes has caused a significant shift in research focus toward methods grounded in GNNs. Most practical GNNs follow the paradigm of the Message Passing . It can be formulated as:

\[_{v}^{(l)}=(\{_{v}^{(l)},_{u}^{(l)},  u_{v}\}),\ \ _{v}^{(l+1)}=(\{_{v}^{(l)},_{v}^{(l)}\} ),\] (2)

where \(_{v}^{(l)}\) represents the vector of node \(v\) at layer \(l\) and \(_{v}^{(0)}=_{v,:}\). For simplicity, we use \(_{v}\) to represent the node vector at the last layer. The specific choice of the neighborhood aggregation function, \(()\), and the updating function, \(()\), dictates the instantiation of the GNN model, with different choices leading to variations of model architectures. In the context of link prediction tasks, the GAE model  derives link representation, \((i,j)\), as a Hadamard product of the target node pair representations, \(_{(i,j)}=_{i}_{j}\). Despite its seminal approach, the SEAL model , which labels nodes based on proximity to target links and then performs message-passing for each target link, is hindered by computational expense, limiting its scalability. Efficient alternatives like ELPH  estimate node labels, while NCNC  directly learns edgewise features by aggregating node representations of common neighbors.

## 3 Can Message Passing count Common Neighbor?

In this section, we delve deep into the potential of MPNNs for heuristic link predictor estimation. We commence with an empirical evaluation to recognize the proficiency of MPNNs in approximating link predictors. Following this, we unravel the intrinsic characteristics of 1-layer MPNNs, shedding light on their propensity to act as biased estimators for heuristic link predictors and proposing an unbiased alternative. Ultimately, we cast light on how successive rounds of message passing can estimate the number of walks connecting a target node pair with other nodes in the graph. All proofs are provided in Appendix G.

### Estimation via Mean Squared Error Regression

To explore the capacity of MPNNs in capturing the overlap information inherent in heuristic link predictors, such as CN, AA and RA, we conduct an empirical investigation, adopting the GAE

Figure 2: GNNs estimate CN, AA and RA via MSE regression, using the mean value as a Baseline. Lower values are better.

framework  with GCN  and SAGE  as representative encoders. SEAL , known for its proven proficiency in capturing heuristic link predictors, serves as a benchmark in our comparison. Additionally, we select a non-informative baseline estimation, simply using the mean of the heuristic link predictors on the training sets. The datasets comprise eight non-attributed graphs (more details in Section 5). Given that GNN encoders require node features for initial representation, we have to generate such features for our non-attributed graphs. We achieved this by sampling from a high-dimensional Gaussian distribution with a mean of \(0\) and standard deviation of \(1\). Although one-hot encoding is frequently employed for feature initialization on non-attributed graphs, we choose to forgo this approach due to the associated time and space complexity.

To evaluate the ability of GNNs to estimate CN information, we adopt a training procedure analogous to a conventional link prediction task. However, we reframe the task as a regression problem aimed at predicting heuristic link predictors, rather than a binary classification problem predicting link existence. This shift requires changing the objective function from cross-entropy to Mean Squared Error (MSE). Such an approach allows us to directly observe GNNs' capacity to approximate heuristic link predictors.

Our experimental findings, depicted in Figure 2, reveal that GCN and SAGE both display an ability to estimate heuristic link predictors, albeit to varying degrees, in contrast to the non-informative baseline estimation. More specifically, GCN demonstrates a pronounced aptitude for estimating RA and nearly matches the performance of SEAL on datasets such as C.ele, Yeast, and PB. Nonetheless, both GCN and SAGE substantially lag behind SEAL in approximating CN and AA. In the subsequent section, we delve deeper into the elements within the GNN models that facilitate this approximation of link predictors while also identifying factors that impede their accuracy.

### Estimation capabilities of GNNs for link predictors

GNNs exhibit the capability of estimating link predictors. In this section, we aim to uncover the mechanisms behind these estimations, hoping to offer insights that could guide the development of more precise and efficient methods for link prediction. We commence with the following theorem:

**Theorem 3.1**.: _Let \(G=(V,E)\) be a non-attributed graph and consider a 1-layer GCN/SAGE. Define the input vectors \(^{N F}\) initialized randomly from a zero-mean distribution with standard deviation \(_{node}\). Additionally, let the weight matrix \(^{F^{} F}\) be initialized from a zero-mean distribution with standard deviation \(_{weight}\). After performing message passing, for any pair of nodes \(\{(u,v)|(u,v) V V E\}\), the expected value of their inner product is given by:_

\[(_{u}_{v})=_{u} _{v}}}_{k_{u}_{v}} _{k}};(_{u}_{v})= _{v}}}_{k_{u}_{v}}1,\]

_where \(_{v}=d_{v}+1\) and \(C=_{node}^{2}_{weight}^{2}FF^{}\)._

The theorem suggests that given proper initialization of input vectors and weight matrices, MPNN-based models, such as GCN and SAGE, can adeptly approximate heuristic link predictors. This makes them apt for encapsulating joint structural features of any node pair. Interestingly, SAGE predominantly functions as a CN estimator, whereas the aggregation function in GCN grants it the ability to weigh the count of common neighbors in a way similar to RA. This particular trait of GCN is evidenced by its enhanced approximation of RA, as depicted in Figure 2.

Quasi-orthogonal vectors.The GNN's capability to approximate heuristic link predictors is primarily grounded in the properties of their input vectors in a linear space. When vectors are sampled from a high-dimensional linear space, they tend to be quasi-orthogonal, implying that their inner product is nearly \(0\) w.h.p. With message-passing, these QO vectors propagate through the graph, yielding in a linear combination of QO vectors at each node. The inner product between pairs of QO vector sets essentially echoes the norms of shared vectors while nullifying the rest. Such a trait enables GNNs to estimate CN through message-passing. A key advantage of QO vectors, especially when compared with orthonormal basis, is their computational efficiency. For a modest linear increment in space dimensions, the number of QO vectors can grow exponentially, given an acceptable margin of error . An intriguing observation is that the orthogonality of QO vectors remains intact even after GNNs undergo linear transformations post message-passing, attributed to the randomized weight matrix initialization. This mirrors the dimension reduction observed in random projection .

Limitations.While GNNs manifest a marked ability in estimating heuristic link predictors, they are not unbiased estimators and can be influenced by factors such as node pair degrees, thereby compromising their accuracy. Another challenge when employing such MPNNs is their limited generalization to unseen nodes. The neural networks, exposed to randomly generated vectors, may struggle to transform newly added nodes in the graph with novel random vectors. This practice also violates the permutation-invariance principle of GNNs when utilizing random vectors as node representation. It could strengthen generalizability if we regard these randomly generated vectors as signatures of the nodes, instead of their node features, and circumvent the use of MLPs for them.

Unbiased estimator.Addressing the biased element in Theorem 3.1, we propose the subsequent instantiation for the message-passing functions:

\[_{v}^{(l+1)}=_{u_{v}}_{u}^{(l)}.\] (3)

Such an implementation aligns with the SAGE model that employs sum aggregation devoid of self-node propagation. This methodology also finds mention in DotHash , serving as a cornerstone for our research. With this kind of message-passing design, the inner product of any node pair signatures can estimate CN impartially:

**Theorem 3.2**.: _Let \(G=(V,E)\) be a graph, and let the vector dimension be given by \(F_{+}\). Define the input vectors \(=(X_{i,j})\), which are initialized from a random variable x having a mean of \(0\) and a standard deviation of \(}\). Using the 1-layer message-passing in Equation 3, for any pair of nodes \(\{(u,v)|(u,v) V V\}\), the expected value and variance of their inner product are:_

\[(_{u}_{v}) =(u,v);\] \[(_{u}_{v}) =(d_{u}d_{v}+(u,v)^{2}-2(u,v) )+F^{2}(u,v).\]

Though this estimator provides an unbiased estimate for CN, its accuracy can be affected by its variance. Specifically, DotHash recommends selecting a distribution for input vector sampling from vertices of a hypercube with unit length, which curtails variance given that \(^{2}=0\). However, the variance influenced by the graph structure isn't adequately addressed, and this issue will be delved into in Section 4.

Orthogonal node attributes.Both Theorem 3.1 and Theorem 3.2 underscore the significance of quasi orthogonality in input vectors, enabling message-passing to efficiently count CN. Intriguingly, in most attributed graphs, node attributes, often represented as bag-of-words , exhibit inherent orthogonality. This brings forth a critical question: In the context of link prediction, do GNNs primarily approximate neighborhood overlap, sidelining the intrinsic value of node attributes? We earmark this pivotal question for in-depth empirical exploration in Appendix E, where we find that random vectors as input to GNNs can catch up with or even outperform node attributes.

### Multi-layer message passing

Theorem 3.2 elucidates the estimation of CN based on a single iteration of message passing. This section explores the implications of multiple message-passing iterations and the properties inherent to the iteratively updated node signatures. We begin with a theorem delineating the expected value of the inner product for two nodes' signatures derived from any iteration of message passing:

**Theorem 3.3**.: _Under the conditions defined in Theorem 3.2, let \(_{u}^{(l)}\) denote the vector for node \(u\) after the \(l\)-th message-passing iteration. We have:_

\[_{u}^{(p)}_{v}^{(q)}=_{k V}| ^{(p)}(k,u)||^{(q)}(k,v)|,\]

_where \(|^{(l)}(u,v)|\) counts the number of length-\(l\) walks between nodes \(u\) and \(v\)._

This theorem posits that the message-passing procedure computes the number of walks between the target node pair and all other nodes. In essence, each message-passing trajectory mirrors the path of the corresponding walk. As such, \(_{u}^{(l)}\) aggregates the initial QO vectors originating from nodes reachable by length-\(l\) walks from node \(u\). In instances where multiple length-\(l\) walks connect node \(k\) to \(u\), the associated QO vector \(_{k,:}\) is incorporated into the sum \(|^{(l)}(k,u)|\) times.

One might surmise a paradox, given that message-passing calculates the number of walks, not nodes. However, in a simple graph devoid of self-loops, where at most one edge can connect any two nodes, it is guaranteed that \(|^{(1)}(u,v)|=1\) iff \((u,v)=1\). Consequently, the quantity of length-\(1\) walks to a target node pair equates to CN, a first-order heuristic. It's essential to recognize, however, that \(|^{(l)}(u,v)| 1\) only implies \((u,v) l\). This understanding becomes vital when employing message-passing for estimating the local structure of a target node pair in Section 4.

## 4 Method

In this section, we introduce our novel link prediction model, denoted as **MPL**. Distinctively designed, MPLP leverages the pure essence of the message-passing mechanism to adeptly learn joint structural features of the target node pairs.

Node representation.While MPLP is specifically designed for its exceptional structural capture, it also embraces the inherent attribute associations of graphs that speak volumes about individual node characteristics. To fuse the attributes (if they exist in the graph) and structures, MPLP begins with a GNN, utilized to encode node \(u\)'s representation: \((u)^{F_{x}}\). This node representation will be integrated into the structural features when constructing the QO vectors. Importantly, this encoding remains flexible, permitting the choice of any node-level GNN.

### QO vectors construction

Probabilistic hypercube sampling.Though deterministic avenues for QO vector construction are documented [22; 23], our preference leans toward probabilistic techniques for their inherent simplicity. We inherit the sampling paradigm from DotHash , where each node \(k\) is assigned with a node signature \(_{k}^{(0)}\), acquired via random sampling from the vertices of an \(F\)-dimensional hypercube with unit vector norms. Consequently, the sampling space for \(_{k}^{(0)}\) becomes \(\{-1/,1/\}^{F}\).

Harnessing One-hot hubs for variance reduction.The stochastic nature of our estimator brings along an inevitable accompaniment: variance. Theorem 3.2 elucidates that a graph's topology can augment estimator variance, irrespective of the chosen QO vector distribution. At the heart of this issue is the imperfectness of quasi-orthogonality. While a pair of vectors might approach orthogonality, the same cannot be confidently said for the subspaces spanned by larger sets of QO vectors.

Capitalizing on the empirical observation that real-world graphs predominantly obey the power-law distribution , we propose a strategy to control variance. Leveraging the prevalence of high-degree nodes--or _hubs_--we designate unique one-hot vectors for the foremost hubs. Consider the graph's top-\(b\) hubs; while other nodes draw their QO vectors from a hypercube \(\{-1/,1/\}^{F-b}\{0\}^{b}\), these hubs are assigned one-hot vectors from \(\{0\}^{F-b}\{0,1\}^{b}\), reserving a distinct subspace of the linear space to safeguard orthogonality. Note that when new nodes are added, their QO vectors are sampled the same way as the non-hub nodes, which can ensure a tractable computation complexity.

Norm rescaling to facilitate weighted counts.Theorem 3.1 alludes to an intriguing proposition: the estimator's potential to encapsulate not just CN, but also RA. Essentially, RA and AA are nuanced heuristics translating to weighted enumerations of shared neighbors, based on their node degrees. In Theorem 3.2, such counts are anchored by vector norms during dot products. MPLP enhances this count methodology by rescaling node vector norms, drawing inspiration from previous works [12; 25].

Figure 3: Representation of the target link \((u,v)\) within our model (MPLP), with nodes color-coded based on their distance from the target link.

This rescaling is determined by the node's representation, \((u)\), and its degree \(d_{u}\). The rescaled vector is formally expressed as:

\[}_{k}^{(0)}=f((k)||[d_{k}])_{k}^{(0)},\] (4)

where \(f^{F_{s}+1}\) is an MLP mapping the node representation and degree to a scalar, enabling the flexible weighted count paradigm.

### Structural feature estimations

Node label estimation.The estimator in Theorem 3.2 can effectively quantify CN. Nonetheless, solely relying on CN fails to encompass diverse topological structures embedded within the local neighborhood. To offer a richer representation, we turn to Distance Encoding (DE) . DE acts as an adept labeling tool , demarcating nodes based on their shortest-path distances relative to a target node pair. For a given pair \((u,v)\), a node \(k\) belongs to a node set \((p,q)\) iff \((u,k)=p\) and \((v,k)=q\). Unlike its usage as node labels, we opt to enumerate these labels, producing a link feature defined by \(\#(p,q)=|(p,q)|\). Our model adopts a philosophy akin to ELPH , albeit with a distinct node-estimation mechanism.

Returning to Theorem 3.3, we recall that message-passing as in Equation 3 essentially corresponds to walks. Our ambition to enumerate nodes necessitates a single-layer message-passing alteration, reformulating Equation 3 to:

\[_{v}^{(s)}=_{k_{v}^{s}}}_{k}^{(0)}.\] (5)

Here, \(_{v}^{s}\) pinpoints \(v\)'s shortest-path neighborhoods distanced by the shortest-path \(s\). This method sidesteps the duplication dilemma highlighted in Theorem 3.3, ensuring that \(_{v}^{(s)}\) aggregates at most one QO vector per node. Similar strategies are explored in [27; 28].

For a tractable computation, we limit the largest shortest-path distance as \(r max(p,q)\). Consequently, to capture the varied proximities of nodes to the target pair \((u,v)\), we can deduce:

\[\#(p,q)=_{u}^{(p)}_{v}^{ (q)},&r p,q 1\\ |_{v}^{q}|-_{1 s r}\#(s,q),&p=0\\ |_{u}^{p}|-_{1 s r}\#(p,s),&q=0\] (6)

Concatenating the resulting estimates yields the expressive structural features of MPLP.

Shortcut removal.The intricately designed structural features improve the expressiveness of MPLP. However, this augmented expressiveness introduces susceptibility to distribution shifts during link prediction tasks . Consider a scenario wherein the neighborhood of a target node pair contains a node \(k\). Node \(k\) resides a single hop away from one of the target nodes but requires multiple steps to connect with the other. When such a target node pair embodies a positive instance in the training data (indicative of an existing link), node \(k\) can exploit both the closer target node and the link between the target nodes as a shortcut to the farther one. This dynamic ensures that for training-set positive instances, the maximum shortest-path distance from any neighboring node to the target pair is constrained to the smaller distance increased by one. This can engender a discrepancy in distributions between training and testing phases, potentially diminishing the model's generalization capability.

To circumvent this pitfall, we adopt an approach similar to preceding works [18; 30; 19; 31]. Specifically, we exclude target links from the original graph during each training batch, as shown by the dash line in Figure 3. This maneuver ensures these links are not utilized as shortcuts, thereby preserving the fidelity of link feature construction.

Feature integration for link prediction.Having procured the structural features, we proceed to formulate the encompassing link representation for a target node pair \((u,v)\) as:

\[_{(u,v)}=((u)(v))||[\#(1,1),,\#(r,r)],\]

which can be fed into a classifier for a link prediction between nodes \((u,v)\).

### More scalable estimation

MPLP estimates the cardinality of the distinct node sets with different distances relative to target node pairs in Equation 6. However, this operation requires a preprocessing step to construct the shortest-path neighborhoods \(_{v}^{s}\) for \(s r\), which can cause computational overhead on large-scale graph benchmarks. To overcome this issue, we simplify the structural feature estimations as:

\[\#(p,q)=}_{u}^{(p)}}_{v}^{(q)} ,\] (7)

where \(}_{v}^{(l+1)}=_{u_{v}}}_{u}^{(l)}\) follows the message-passing defined in Equation 3. Similar to common GNNs, such a message-passing only requires the one-hop neighborhood \(_{v}\), which is provided in a format of adjacency matrices/lists by most graph datasets. Therefore, we can substitute the structural features of MPLP with the estimation in Equation 7. We denote such a model with walk-level features as MPLP+.

### Triangular substructure estimation

Our method, primarily designed to encapsulate the local structure of a target node pair, unexpectedly exhibits the capacity for estimating the count of triangles linked to individual nodes. This capability, traditionally considered beyond the reach of GNNs, marks a significant advancement in the field . Although triangle counting is less directly relevant in the context of link prediction, the implications of this capability are noteworthy. To maintain focus, we relegate the detailed discussion on pure message-passing for effective triangle counting to Appendix C.

    & **USAir** & **NS** & **PB** & **Yeast** & **Cele** & **Power** & **Router** & **E.coli** \\ Metric & Hits@50 & Hits@50 & Hits@50 & Hits@50 & Hits@50 & Hits@50 & Hits@50 & Hits@50 \\ 
**CN** & \(80.52_{ 4.07}\) & \(74.00_{ 1.98}\) & \(37.22_{ 3.52}\) & \(72.60_{ 3.85}\) & \(47.67_{ 10.87}\) & \(11.57_{ 0.55}\) & \(9.38_{ 1.05}\) & \(51.74_{ 2.70}\) \\
**AA** & \(85.51_{ 2.25}\) & \(74.00_{ 1.98}\) & \(39.48_{ 3.53}\) & \(73.62_{ 1.01}\) & \(58.34_{ 2.88}\) & \(11.57_{ 0.55}\) & \(9.38_{ 1.05}\) & \(68.13_{ 1.61}\) \\
**RA** & \(85.95_{ 3.83}\) & \(74.00_{ 1.98}\) & \(38.94_{ 4.54}\) & \(73.62_{ 1.01}\) & \(61.47_{ 4.59}\) & \(11.57_{ 0.55}\) & \(9.38_{ 1.05}\) & \(74.45_{ 0.55}\) \\ 
**GCN** & \(73.29_{ 4.70}\) & \(78.32_{ 2.52}\) & \(37.32_{ 6.69}\) & \(73.15_{ 2.41}\) & \(40.68_{ 5.45}\) & \(15.40_{ 2.90}\) & \(42.42_{ 4.59}\) & \(61.02_{ 21.91}\) \\
**SAGE** & \(83.81_{ 3.09}\) & \(56.62_{ 9.41}\) & \(47.26_{ 2.53}\) & \(71.06_{ 5.12}\) & \(58.97_{ 4.77}\) & \(6.89_{ 9.05}\) & \(42.25_{ 4.32}\) & \(75.60_{ 2.40}\) \\ 
**SEAL** & \(90.47_{ 3.00}\) & \(86.59_{ 3.03}\) & \(44.47_{ 2.86}\) & \(83.92_{ 1.17}\) & \(64.80_{ 2.43}\) & \(31.46_{ 3.25}\) & \(61.00_{ 10.10}\) & \(83.42_{ 1.01}\) \\
**No-GNN** & \(86.07_{ 1.96}\) & \(85.34_{ 3.92}\) & \(40.44_{ 1.89}\) & \(83.14_{ 0.73}\) & \(63.22_{ 4.32}\) & \(21.98_{ 4.62}\) & \(42.81_{ 4.13}\) & \(73.76_{ 1.94}\) \\
**ELPH** & \(87.04_{ 1.98}\) & \(88.49_{ 1.24}\) & \(46.91_{ 2.21}\) & \(82.74_{ 1.19}\) & \(64.45_{ 3.91}\) & \(26.61_{ 1.73}\) & \(61.07_{ 3.06}\) & \(75.25_{ 1.44}\) \\
**NCNC** & \(86.16_{ 1.77}\) & \(83.18_{ 3.17}\) & \(46.85_{ 3.18}\) & \(82.00_{ 0.97}\) & \(60.49_{ 5.09}\) & \(23.28_{ 1.55}\) & \(52.45_{ 8.77}\) & \(83.94_{ 1.57}\) \\ 
**MPLP** & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) \\
**MPLP+** & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(60.94_{ 2.51}\) & \(}\) \\   

Table 1: Link prediction results on non-attributed benchmarks. The format is average score \(\) standard deviation. The top three models are colored by **First**, **Second**, **Third**.

    & **CS** & **Physics** & **Computers** & **Photo** & **Collab** & **PPA** & **Clation2** \\ Metric & Hits@50 & Hits@50 & Hits@50 & Hits@50 & Hits@50 & Hits@50 & Hits@100 & MRR \\ 
**CN** & \(51.04_{ 15.56}\) & \(61.46_{ 6.12}\) & \(21.95_{ 2.00}\) & \(29.33_{ 2.74}\) & \(61.37_{ 0.00}\) & \(27.65_{ 0.00}\) & \(51.47_{ 0.00}\) \\
**AA** & \(68.26_{ 1.28}\) & \(70.98_{ 1.96}\) & \(26.96_{ 2.08}\) & \(37.35_{ 2.65}\) & \(64.35_{ 0.00}\) & \(32.45_{ 0.00}\) & \(51.89_{ 0.00}\) \\
**RA** & \(68.25_{ 1.29}\) & \(72.29_{ 1.09}\) & \(28.05_{ 1.59}\) & \(40.77_{ 3.41}\) & \(64.00_{ 0.00}\) & \(49.33_{ 0.00}\) & \(51.98_{ 0.00}\) \\ 
**GCN** & \(66.00_{ 2.90}\) & \(73.71_{ 1.28}\) & \(22.95_{ 1.05}\) & \(28.14_{ 1.87}\) & \(35.32_{ 2.39}\) & \(18.67_{ 0.00}\) & \(84.74_{ 0.21}\) \\
**SAGE** & \(57.79_{ 1.82}\) & \(74.10_{ 2.51}\)

## 5 Experiments

Datasets, baselines and experimental setupWe conduct evaluations across a diverse spectrum of 15 graph benchmark datasets, which include 8 non-attributed and 7 attributed graphs 2. It also includes three datasets from OGB  with predefined train/test splits. In the absence of predefined splits, links are partitioned into train, validation, and test sets using a 70-10-20 percent split. Our comparison spans three categories of link prediction models: (1) heuristic-based methods encompassing CN, AA, and RA; (2) node-level models like GCN and SAGE; and (3) link-level models, including SEAL, Neo-GNN , ELPH , and NCNC . Each experiment is conducted 10 times, with the average score and standard deviations reported. The evaluation metrics are aligned with the standard metrics for OGB datasets, and we utilize Hits@50 for the remaining datasets. We limit the number of hops \(r=2\), which results in a good balance of performance and efficiency. A comprehensive description of the experimental setup is available in Appendix D.

ResultsPerformance metrics are shown in Tables 1 and 2. Our methods, MPLP and MPLP+, demonstrate superior performance, surpassing baseline models across all evaluated benchmarks by a significant margin. Notably, MPLP tends to outperform MPLP+ in various benchmarks, suggesting that node-level structural features (Equation 6) might be more valuable for link prediction tasks than the walk-level features (Equation 7). In large-scale graph benchmarks such as PPA and Citation2, MPLP+ sets new benchmarks, establishing state-of-the-art results. For other datasets, our methods

Figure 4: Evaluation of inference time on large-scale OGB datasets. The inference time encompasses the entire cycle within a full-batch inference.

    &  &  &  \\  & MRR & Hits@20 & MRR & Hits@20 & MRR & Hits@20 \\ 
**CN** & 4.20 & 16.46 & 25.70 & 68.25 & 17.11 & 41.73 \\
**AA** & 5.07 & 19.59 & 26.85 & 70.22 & 17.83 & 43.12 \\
**RA** & 6.29 & 24.29 & 28.34 & 71.50 & 17.79 & 43.34 \\ 
**GCN** & 6.09 & 22.48 & 26.94 & 68.38 & 19.98 & 51.72 \\
**SAGE** & 5.53 & 21.26 & 27.27 & 69.49 & 22.05 & 53.13 \\
**SEAL** & 6.43 & 21.57 & 29.71 & 76.77 & 20.60 & 48.62 \\
**Neo-GNN** & 5.23 & 21.03 & 21.68 & 64.81 & 16.12 & 43.17 \\
**BUDDY** & 5.67 & 23.35 & 27.70 & 71.50 & 19.17 & 47.81 \\
**NCNC** & 4.73 & 20.49 & 33.52 & 82.24 & 19.61 & 51.69 \\ 
**MPLP+** & 6.79 & 25.10 & 41.40 & 84.88 & 23.11 & 55.51 \\   

Table 3: Link prediction results on OGB datasets under HeaRT . The top three models are colored by **First**, **Second**, **Third**.

show a substantial performance uplift, with improvements in Hits@50 ranging from \(2\%\) to \(10\%\) compared to the closest competitors.

We extend our evaluation of MPLP+ to assess its performance on large-scale datasets under the challenging HeaRT setting proposed by Li et al. . HeaRT introduces a more rigorous and realistic set of negative samples during evaluation, typically resulting in a notable decline in performance across link prediction methods. As detailed in Table 3, MPLP+ consistently outperform all other methods across three OGB graph benchmarks in this demanding context. This underscores the robustness of MPLP+, affirming its ability to maintain superior performance across a variety of graph benchmarks and evaluation settings.

Time efficiencyWe conduct an analysis of the time efficiency of our methods, MPLP and MPLP+, against established baselines using three large-scale OGB datasets. The results, illustrated in Figure 4, demonstrate that our approaches not only deliver superior performance across the graph benchmarks but also set a new benchmark for state-of-the-art time efficiency in full-batch inference. In particular, the primary component underlying our methods is the message-passing operation, which allows their inference speeds to rival that of the baseline GCN. Additionally, the structural feature estimations enhance the models' expressiveness, enabling more accurate representation of graph structures, particularly in the context of link prediction tasks. More details can be found in Appendix D.3.

Estimation accuracyWe investigate the precision of MPLP in estimating \(\#(p,q)\), which denotes the count of node labels, using the Collab dataset. The outcomes of this examination are illustrated in Figure 5. Although ELPH possesses the capability to approximate these counts utilizing techniques like MinHash and Hyperloglog, our method exhibits superior accuracy. Moreover, ELPH runs out of memory when the dimension is larger than \(3000\). Remarkably, deploying a one-hot encoding strategy for the hubs further bolsters the accuracy of MPLP, concurrently diminishing the variance introduced by inherent graph structures. An exhaustive analysis, including time efficiency considerations, is provided in Appendix F.1.

Extended ablation studiesFurther ablation studies have been carried out to understand the individual contributions within MPLP. These include: (1) an exploration of the distinct components of MPLP in Appendix F.2; (2) an analysis of the performance contributions from different structural estimations in Appendix F.3; and (3) an examination of parameter sensitivity in Appendix F.4.

## 6 Conclusion

We study the potential of message-passing GNNs to encapsulate link structural features. Based on this, we introduce a novel link prediction paradigm that consistently outperforms state-of-the-art baselines across various graph benchmarks. The inherent capability to adeptly capture structures enhances the expressivity of GNNs, all while maintaining their computational efficiency. Our findings hint at a promising avenue for elevating the expressiveness of GNNs through probabilistic approaches.

Figure 5: MSE of estimation for \(\#(1,1)\), \(\#(1,2)\) and \(\#(1,0)\) on Collab. Lower values are better.

Acknowledgements

We would like to thank the anonymous reviewers for their insightful comments and helpful discussions. This research was supported in part by the University of Notre Dame's Lucy Family Institute for Data and Society and the NSF Center for Computer-Assisted Synthesis (C-CAS), under grant number CHE-2202693.