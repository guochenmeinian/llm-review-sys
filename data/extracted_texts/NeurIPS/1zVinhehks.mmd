# Graph Classification via Reference Distribution Learning: Theory and Practice

Zixiao Wang  Jicong Fan

School of Data Science

The Chinese University of Hong Kong, Shenzhen

zixiaowang@link.cuhk.edu.cn  fanjicong@cuhk.edu.cn

Corresponding author

###### Abstract

Graph classification is a challenging problem owing to the difficulty in quantifying the similarity between graphs or representing graphs as vectors, though there have been a few methods using graph kernels or graph neural networks (GNNs). Graph kernels often suffer from computational costs and manual feature engineering, while GNNs commonly utilize global pooling operations, risking the loss of structural or semantic information. This work introduces Graph Reference Distribution Learning (GRDL), an efficient and accurate graph classification method. GRDL treats each graph's latent node embeddings given by GNN layers as a discrete distribution, enabling direct classification without global pooling, based on maximum mean discrepancy to adaptively learned reference distributions. To fully understand this new model (the existing theories do not apply) and guide its configuration (e.g., network architecture, references' sizes, number, and regularization) for practical use, we derive generalization error bounds for GRDL and verify them numerically. More importantly, our theoretical and numerical results both show that GRDL has a stronger generalization ability than GNNs with global pooling operations. Experiments on moderate-scale and large-scale graph datasets show the superiority of GRDL over the state-of-the-art, emphasizing its remarkable efficiency, being at least 10 times faster than leading competitors in both training and inference stages. The source code of GRDL is available at https://github.com/jicongfan/GRDL-Graph-Classification.

## 1 Introduction

Graphs serve as versatile models across diverse domains, such as social networks (Wang et al., 2018), biological compounds (Jumper et al., 2021), and the brain (Ktena et al., 2017). There has been considerable interest in developing learning algorithms for graphs, such as graph kernels (Gartner et al., 2003; Shervashidze et al., 2011; Chen et al., 2022) and graph neural networks (GNNs) (Kipf and Welling, 2016; Defferrard et al., 2016; Gilmer et al., 2017). GNNs have emerged as powerful tools, showcasing state-of-the-art performance in various graph prediction tasks (Velickovic et al., 2017; Gilmer et al., 2017; Hamilton et al., 2017; Xu et al., 2018; Sun et al., 2019; You et al., 2021; Ying et al., 2021; Liu et al., 2022; Chen et al., 2022; Xiao et al., 2022; Sun et al., 2023; Sun and Fan, 2024; Sun et al., 2024). Despite the evident success of GNNs in numerous graph-related applications, their potential remains underutilized, particularly in the domain of graph-level classification.

Current GNNs designed for graph classification commonly consist of two components: the embedding of node features through message passing (Gilmer et al., 2017) and subsequent aggregation by some permutation invariant global pooling (also called readout) operations (Xu et al., 2018). The primary purpose of pooling is to transform a graph's node embeddings, a matrix, into a single vector.

Empirically, pooling operations play a crucial role in classification (Ying et al., 2018). However, these pooling operations tend to be naive, often employing methods such as simple summation or averaging. These functions collect only first-order statistics, leading to a loss of structural or semantic information. In addition to the conventional sum or average pooling, more sophisticated pooling operations have shown improvements in graph classification (Li et al., 2015; Ying et al., 2018; Lee et al., 2019, 2021; Buterez et al., 2022; Yu et al., 2024), but they still carry the inherent risk of information loss.

Different from graph kernel methods and existing GNN methods, we propose a novel GNN method that classifies the nodes' embeddings themselves directly, thus avoiding the global pooling step. In our method, we treat the nodes' latent representations of each graph, learned by a neural network, as a discrete distribution and classify these distributions into \(K\) different classes. The classification is conducted via measuring the similarity between the latent graph's distributions and \(K\) discriminative reference discrete distributions. The reference distributions can be understood as nodes' embeddings of representative virtual graphs from \(K\) different classes, and they are jointly learned with the parameters of the neural network in an end-to-end manner. To evaluate our method, we analyze the generalization ability of our model both theoretically and empirically. Our contributions are two-fold.

* We propose a novel graph classification method GRDL that is efficient and accurate.
* GRDL does not require any global pooling operation and hence effectively preserves the information of node embeddings.
* Besides its high classification accuracy, GRDL is scalable to large graph datasets and is at least ten times faster than leading competitors in both training and inference stages.
* We provide theoretical guarantees, e.g. generalization error bounds, for GRDL.
* The result offers valuable insights into how the model performance scales with the properties of graphs, neural network structure, and reference distributions, guiding the model design.
* For instance, the generalization bounds reveal that the references' norms and numbers have tiny impacts on the generalization, which is also verified by the experiments.
* More importantly, we theoretically prove that GRDL has a stronger generalization ability than GNNs with global pooling operations.

The rest of this paper is organized as follows. We introduce our model in Section 2 and analyze the generalization ability in Section 3. Related works are discussed in Section 4. Section 5 presents the numerical results on 11 benchmarks in comparison to 12 competitors.

## 2 Proposed Approach

### Model Framework

Following convention, we denote a graph with index \(i\) by \(G_{i}=(V_{i},E_{i})\), where \(V_{i}\) and \(E_{i}\) are the vertex (node) set and edge set respectively. Given a graph dataset \(=\{(G_{1},y_{1}),(G_{2},y_{2}),,(G_{N},y_{N})\}\), where \(y_{i}\{1,2,,K\}\) is the associated label of \(G_{i}\) and \(y_{i}=k\) means \(G_{i}\) belongs to class \(k\), the goal is to learn a classifier \(f\) from \(\) that generalizes well to unseen graphs. Since in many scenarios, each node of a graph has a feature vector \(\) and the graph is often represented by an adjacency matrix \(\), we also write \(G_{i}=(_{i},_{i})\) for convenience, where \(_{i}^{n_{i} n_{i}}\), \(_{i}^{n_{i} d_{0}}\), \(n_{i}=|V_{i}|\) is the number of nodes of graph \(i\), and \(d_{0}\) denotes the number of features. We may alternatively denote the graph dataset as \(=\{((_{1},_{1}),y_{1}),((_{2}, _{2}),y_{2}),,((_{N},_{N}),y_{N})\}\).

Our approach is illustrated in Figure 1. For graph classification, we first use a GNN, denoted as \(f_{G}\), to transform each graph to a node embedding matrix \(_{i}^{n_{i} d}\) that encodes its properties, i.e.,

\[_{i}=f_{G}(G_{i})=f_{G}(_{i},_{i}),\] (1)

where \(f_{G}_{G}\) and \(_{G}\) denotes a hypothesis space. The remaining task is to classify \(_{i}\) without global pooling. Direct classification of node embeddings is difficult due to two reasons:

1. Different graphs have different numbers of nodes, i.e. in general, \(n_{i} n_{j}\) if \(i j\).
2. The node embeddings of each graph are permutation invariant, namely, \(_{i}\) and \(_{i}\) represent the same graph for any permutation matrix \(\).

However, the two properties are naturally satisfied if we treat the node embeddings of each graph as a discrete distribution. Specifically, each \(_{i}\) is a discrete distribution and each row of \(_{i}\) is an outcome of the distribution. There is no order between the outcomes in each distribution. Also, different distributions may have different numbers of outcomes. Before introducing our method in detail, we first give a toy example where the commonly used mean and max pooling operations fail.

**Example 2.1**.: _Suppose two graphs \(G_{1}\) and \(G_{2}\) have self-looped adjacency matrices \(}_{1}=[1\;1\;1\;0;1\;1\;0\;1\;1\;0\;1\;1;0\;1\;1]\) and \(}_{2}=[1\;0\;0\;0;0\;1\;1\;0;0\;1\;1\;0;0\;0\;1]\) respectively, and have one-dimensional node features \(_{1}=[3\;6\;9\;12]^{}\) and \(_{2}=[6\;6\;9\;9]^{}\) respectively. Let \(}_{i}\) be the normalized adjacency matrices, i.e., \(}_{i}=(}_{i})^{-1/2}}_{i}(}_{i})^{-1/2}\). Performing the neighbor aggregation \(_{i}=}_{i}_{i}\), \(i=1,2\), we obtain \(_{1}=[6\;7\;8\;9]^{}\) and \(_{2}=[6\;7.5\;7.5\;9]^{}\). We see that \((_{1})=(_{2})=7.5\) and \((_{1})=(_{2})=9\). This means the simple mean and max pooling operations failed to distinguish the two graphs. In contrast, our method treats \(_{1}\) and \(_{2}\) as two different discrete distributions and hence is able to distinguish the two graphs. Note that incorporating a learnable parameter \(\), i.e., \(_{i}=}_{i}_{i}\), or performing multiple times of neighbor aggregation does not change the conclusion._

We propose to classify the discrete distributions \(\{_{1},_{2},,_{N}\}\) by a reference layer \(f_{D}\). The classification involves measuring the similarity between \(_{i}\) and \(K\) reference discrete distributions \(\{_{1},_{2},,_{K}\}\) that are discriminative. Each \(_{k}^{m_{k} d}\) can be understood as node embeddings of a virtual graph from the \(k\)-th class, \(k[K]\). We make \(m_{1}==m_{K}=m\) for convenience. Letting \(\) be a similarity measure between two discrete distributions, then

\[s_{ik}:=(_{i},_{k}), i[N],\;k[K].\] (2)

This forms a matrix \(=[_{1},_{2},,_{N}]^{}\) where

\[_{i}=f_{D}(_{i})=[s_{i1},s_{i2},,s_{iK}]^{} ^{K},\] (3)

\(f_{D}_{D}\) and \(_{D}\) denotes a hypothesis space induced by the reference layer. References in \(\) are parameters of the reference layer and are jointly learned with node-embedding network parameters in an end-to-end manner. Now combining Equation (1) and Equation (3), we arrive at

\[_{i}=f_{D}(f_{G}(G_{i})), i[N].\] (4)

\(f_{D} f_{G}\) calculates \(_{i}\), representing similarities between \(G_{i}\) and all references. We get \(G_{i}\)'s label by

\[y_{,i}=_{k}s_{ik}.\] (5)

To train the model, we first use the softmax function to convert \(_{i}\) to a label vector \(}_{i}=[_{i1},,_{iK}]^{}\), where

\[_{ik}=)}{_{j=1}^{K}(s_{ij} )}, k[K].\] (6)

Figure 1: The GRDL framework. Classification involves using a GNN \(f_{G}\) to encode a graph’s information into a node embedding distribution. The similarities between the node embeddings and \(K\) reference distributions are calculated by the reference module \(f_{D}\). The graph is assigned the label of the reference that exhibits the highest similarity.

Using the cross-entropy loss, we minimize

\[_{}=-_{i=1}^{N}_{k=1}^{K}y_{ik}_ {ik}.\] (7)

Intuitively, the reference distributions in \(\) should be different from each other to ensure discriminativeness. Therefore, we also consider the following discrimination loss:

\[_{}=_{k}_{k^{} k}(_{k}, _{k^{}}).\] (8)

Then we solve the following problem:

\[_{f_{G}_{G},f_{D}_{D}}_{}+ _{},\] (9)

where \( 0\) is a hyperparameter. We call (9) Graph Classification via Reference Distribution Learning (GRDL). Specific designs of \(_{G}\) and \(_{D}\) are detained in the next section.

### Design of \(_{G}\) and \(_{D}\)

We get GRDL's network \(\) by concatenating the node embedding module and the reference module:

\[:=_{D}_{G}.\] (10)

**Design of \(_{G}\)** We use an \(L\)-layer message passing network as our node embedding module \(_{G}\):

\[_{G}:=^{L}^{L-1} ^{1}.\] (11)

\(^{l}\) is the \(l\)-th message passing layer (e.g. a GIN layer ) that updates the representation of a node by aggregating representations of its neighbors, meaning

\[a_{v}^{(l)}=^{(l)}(\{h_{u}^{(l-1)}:u( v)\}), h_{v}^{(l)}=^{(l)}(h_{v}^{(l-1)},a_{v}^{(l)})\] (12)

where \(h_{v}^{(l)}\) is the feature vector of node \(v\) produced by the \(l\)-th layer \(^{l}\). Different GNNs have different choices of COMBINE\({}^{(l)}()\) and AGGREGATE\({}^{(l)}()\).

**Design of \(_{D}\)** Based on (3), the hypothesis space defined by the reference layer is

\[_{D}:=\{_{i}_{i}^{K}:s_{ik }=(_{i},_{k}),_{k}^{m d}\}.\] (13)

In our work, we choose \((,)\) to be the negative squared Maximum Mean Discrepancy (MMD). Initially used for two-sample tests, MMD is now widely used to measure the dissimilarity between distributions . For an embedding \(^{n d}\) and a reference \(^{m d}\),

\[(,)= -^{2}(,)=- _{i=1}^{n}(_{i})-_{j=1}^{m} (_{j})_{2}^{2}\] \[= _{i=1}^{n}_{j=1}^{m}(_{i})^{ }(_{j})-}_{i=1}^{n}_{i^{}=1}^{n} (_{i})^{}(_{i^{}})-}_ {j=1}^{m}_{j^{}=1}^{m}(_{j})^{}(_{j ^{}})\] (14)

where \(\) is some feature map, \(_{i}^{}\) is the \(i\)-th row of \(\), and \(_{j}^{}\) is the \(j\)-th row of \(\). The MMD in (14) is known as biased MMD  and its performance is almost the same as the unbiased one, in our experiments. Therefore we only present (14) here. Using kernel trick \(k(,^{})=()^{}(^{ })\), we obtain from (14) that

\[(,)=_{i=1}^{n}_{j=1}^{m}k(_{i},_{j})-}_{i=1}^{n}_{i^{}=1}^{n}k( _{i},_{i^{}})-}_{j=1}^{m}_{j^{ }=1}^{m}k(_{j},_{j^{}}).\]

In this work, we employ the Gaussian kernel, i.e.,

\[k(,^{})=(--^{}_{2}^{2})\] (15)where \(>0\) is a hyperparameter. The Gaussian kernel defines an infinite-order polynomial feature map \(\), covering all orders of statistics of the input variable. Consequently, MMD with the Gaussian kernel characterizes the difference between two distributions across all moments. Actually, we found that, in GRDL, the Gaussian kernel often outperformed other kernels such as the polynomial kernel.

Several other statistical distances are available for measuring the difference between distributions, including Wasserstein distance and Sinkhorn divergence (Peyre and Cuturi, 2020). However, their computational complexity is prohibitively high, making the model impractical for large-scale graph datasets. We also find, through experiments, that in our method, the classification performance of MMD is better than that of Wasserstein distance and Sinkhorn divergence as shown later in Table 1. These explain why we prefer MMD.

### Algorithm Implementation

The \(\) in the Gaussian kernel (15) plays a crucial role in determining the statistical efficiency of MMD. Optimally setting of \(\) remains an open problem and many heuristics are available (Gretton et al., 2012). To simplify the process, we make \(\) learnable in our GRDL and rewrite \(\) as \(_{}\). Our empirical results in Appendix D.5 show that GRDL with learnable \(\) performs better. For convenience, we denote all the parameters of \(f_{G}\) as \(\) and let \(f_{,,}=f_{D} f_{G}\). Then we rewrite problem (9) as

\[_{,,}-_{i=1}^{N}_{k=1}^{K}y _{ik},,}(G_{i})_{k}) }{_{j=1}^{K}(f_{,,}(G_{i})_{j}) }+_{k^{} k}_{}(_{k},_{k^{ }}).\] (16)

The (mini-batch) training of GRDL model is detailed in Algorithm 1 (see Appendix C).

## 3 Theoretical Analysis

In this section, we provide theoretical guarantees for GRDL, due to the following motivations:

* As the proposed approach is novel, it is necessary to understand it thoroughly using theoretical analysis, e.g., understand the influences of data and model properties on the classification.
* It is also necessary to provide guidance for the model design to guarantee high accuracy in inference stages.

### Preliminaries

**Matrix constructions** We construct big matrices \(\), \(\) and \(\), where \(=[_{1}^{},_{2}^{},,_{N}^{}]^{}^{(_{i}n_{i}) d}\); \(=(_{1},_{2},,_{N}) ^{(_{i}n_{i})(_{i}n_{i})}\) is a block diagonal matrix, \(=[_{1}^{},_{2}^{},,_{K}^{}]^{}^{Km d}\). The adjacency matrix with self-connectivity is \(}=+\). The huge constructed graph is denoted by \(=(},)\). This construction allows us to treat all graphs in dataset \(\) as a whole and it is crucial for our derivation.

**Neural network** Previously, for a deterministic network \(f\), its output after feeding forward a single graph is \(f(G_{i})\). However, we mainly deal with the huge constructed graph \(\) in this section, and notation will be overloaded to \(f()=^{N K}\), a matrix whose \(i\)-th row is \(f(G_{i})^{}\).

We instantiate the message passing network as Graph Isomorphism Network (GIN) (Xu et al., 2018). We choose to focus on GIN for two reasons. Firstly, the analysis on GIN is currently limited, most of the current bounds for GNNs don't apply for GIN (Garg et al., 2020; Liao et al., 2021; Tang and Liu, 2023). The other reason is that GIN is used as the message-passing network in our numerical experiments. Notably, our proof can be easily adapted to other message-passing GNNs (e.g. GCN (Kipf and Welling, 2016)). GIN updates node representations as

\[h_{v}^{(l)}=^{(l)}(1+^{(l)})h_{v}^{(l-1)}+_{u (v)}h_{u}^{(l-1)}\] (17)

where \(h_{v}^{(l)}\) denotes the node features generated by \(l\)-th GIN message passing layer. Let \(^{(l)}=0\) for all layers and suppose all MLPs have \(r\) layers, the node updates can be written in matrix form as

\[^{(l)}=(((} ^{(l-1)})_{1}^{(l)})_{r-1}^{( l)})_{r}^{(l)}\] (18)where \(_{i}^{(l)}^{d_{i-1}^{(l)} d_{i}^{(l)}}\) is the weight matrix, and \(^{(l)}\) is the matrix of node features with \(^{(0)}=\). \(()\) is the non-linear activation function. Let \(^{l}\) be the function space induced by the \(l\)-th message passing layer, meaning

\[^{l}=\{(},^{(l-1)})^{(l )}:_{i}^{(l)}_{i}^{(l)},i[r]\}\] (19)

where \(_{i}^{(l)}\) is some constraint set on the weight matrix \(_{i}^{(l)}\) and \(^{(l)}\) is given by (18). The \(L\)-layer GIN function space \(_{G}\) is the composition of \(^{l}\) for \(l[L]\), i.e.,

\[_{G}=^{L}^{L-1} ^{1}=\{ f^{L}( f^{1}()):f^{i} ^{i}, i[L]\}.\] (20)

Letting \(s_{ik}=-^{2}(_{i}^{(L)},_{k})\), the reference layer defines the following function space

\[_{D}=\{^{(L)}^{N K}: _{k}^{m d},k[K]\}.\] (21)

Our proposed network (GRDL) is essentially \(:=_{D}_{G}\).

**Loss Function** Instead of the cross entropy loss (7), we consider a general loss function \(l_{}(,)\) satisfying \(0 l_{}\) to quantify the model performance. Importantly, this loss function is not restricted to the training loss because our generalization bound is optimization-independent. For instance, the loss function can be the ramp loss that is commonly used for classification tasks (Bartlett et al., 2017; Mohri et al., 2018). Given a neural network \(f\), we want to upper bound the model population risk of graphs and labels from an unknown distribution \(\)

\[L_{}(f):=}_{(G,y)}[ l_{}(f(G),y)].\] (22)

Given the observed graph dataset \(\) sampled from \(\), the empirical risk is

\[_{}(f):=_{i=1}^{N}l_{}(f(G_{i}),y_{i}),\] (23)

of which (7) is just a special case. Appendix E provides more details about the setup and our idea.

### Main Results

For convenience, similar to (Bartlett et al., 2017; Ju et al., 2023), we make the following assumptions.

**Assumption 3.1**.: The following conditions hold for \(_{}:=\{(G,y) l_{}(f(G),y):f\}\):

1. The activation function \(()\) is 1-Lipschitz (e.g. Sigmoid, ReLU).
2. The weight matrices satisfy \(_{i}^{(l)}_{i}^{(l)}:=\{_{i}^{(l)}:\| _{i}^{(l)}\|_{}_{i}^{(l)},\|_{i}^{(l)}\|_{2,1} b_{i}^{(l)}\}\).
3. The constructed reference matrix satisfy \(\|\|_{2} b_{D}\).
4. The Gaussian kernel parameter \(\) is fixed.
5. The loss function \(l_{}(,y):^{K}\) is \(\)-Lipschitz w.r.t \(\|\|_{2}\) and \(0 l_{}\).

**Theorem 3.2** (Generalization bound of GRDL).: _Let \(n=_{i}n_{i}\), \(c=\|}\|_{}\), and \(=_{i,l}d_{i}^{(l)}\). Denote \(R_{G}:=c^{2L}\|\|_{2}^{2}(2^{2})_{l=1}^{L}( _{i=1}^{r}_{i}^{(l)})^{2}_{l=1}^{L}_{i=1}^{r}^{(l)}}{_{i}^{(l)}}^{2/3}^{3}\). For graphs \(=\{(G_{i},y_{i})\}_{i=1}^{N}\) drawn i.i.d from any probability distribution over \(\{1,,K\}\) and references \(\{_{k}\}_{k=1}^{K},_{k}^{m d}\), with probability at least \(1-\), every loss function \(l_{}\) and network \(f\) under Assumption 3.1 satisfy_

\[L_{}(f)_{}(f)+3}{2N}+ +v_{2}} N+24 v_{3}}}{N}}\]

_where \(v_{1}=^{2}}{n}\), \(v_{2}=Km\), and \(v_{3}=b_{D}}{}\)._

The bound shows how the properties of the neural network, graphs, reference distributions, etc, influence the gap between training error and testing error. A detailed discussion will be presented in Section 3.3. Some interesting corollaries of Theorem 3.2, e.g., misclassification rate bound, can be found in Appendix F.7. Besides small generalization error \(L_{}(f)-_{}(f)\), a good model should have small empirical risk \(_{}(f)\). The empirical risk \(_{}(f)\) is typically a surrogate loss of misclassification rate of training data and a lower misclassification rate implies a smaller \(_{}(f)\). We now provide a guarantee for the correct classification of training data, namely small \(_{}(f)\).

Notably, the node embeddings \(_{i}\) from the \(k\)-th class as well as the reference distributions \(_{k}\) are essentially some _finite samples from an underlying continuous distribution_\(_{k}\). One potential risk is that, although the continuous distributions \(_{1},_{2},,_{K}\) are distinct, we can only observe their finite samples and may fail to distinguish them from each other with MMD. Specifically, suppose a node embedding \(_{i}\) is from the \(k\)-th class, although \(0=(_{k},_{k})<(_{k}, _{j})\) for any \(j k\), it is likely that \((_{i},_{k})>(_{i}, _{j})\) for some \(j k\). The following theorem provides the correctness guarantee for the training dataset \(\):

**Theorem 3.3**.: _All graphs in the training set \(\) are classified correctly with probability at least \(1-\) if_

\[_{i j}(_{i},_{j})>(}+})(4+4} ).\]

Theorem 3.3 implies that a larger reference distribution size \(m\) benefits the classification accuracy of training data, resulting in a lower \(_{}(f)\). Moreover, a larger \(_{i j}(_{i},_{j})\) also makes correct classification easier according to the theorem, justifying our usage of discriminative loss (8).

### Bound Discussion and Numerical Verification

Let \(=_{i,l}_{i}^{(l)}\), \(=_{i,l}^{(l)}}{_{i}^{(l)}}\) and suppose \(\) is large enough, we simplify Theorem 3.2 as

\[L_{}(f)_{}(f)+}_{1}}+_{2}}}{N}_{ }(f)+}\|\|_{2}c^{L}( Lr)^{}^{Lr}}{N}+} \]

**I. Dependence on graph property** One distinctive feature of our bound is its dependence on the spectral norm of graphs' adjacency matrix. The large adjacency matrix \(}\) is a block-diagonal matrix, so its spectral norm \(c=\|}\|_{}=_{i[N]}\|}_{i}\|_{}\). By Lemma F.8, incorporating \(c^{L}\) is sufficient for any \(L\)-step GIN message passing. This result aligns with Ju et al. (2023), who achieved this conclusion via PAC-Bayesian analysis. Our derivation, based on the Rademacher complexity, provides an alternative perspective supporting this result. Notably, Liao et al. (2021) and Garg et al. (2020) proposed bounds scaling with graphs' maximum node degree, which is larger than the spectral norm of the graphs' adjacency matrix (Lemma F.18). Consequently, our bound is tighter.

**II. Use moderate-size message passing GIN** The bound scales with the size of the message passing GIN, following \((c^{L}(Lr)^{}^{Lr})\). Empirical observations reveal \(>1\), and we prove that \(c>1\) (refer to Lemma F.20). Therefore, when the message-passing GNN has sufficient expressive power (resulting in a small \(_{}(f)\)), a network with a smaller \(L\) and \(r\) may guarantee a tighter bound on the population risk compared to a larger one. Therefore, a promising strategy is to use a moderate-size message passing GNN. This is empirically supported by Figure 5 of Appendix D.7.

**III. Use moderate-size references** The bound scales with the size of reference distributions \(m\) as \(()\). When \(m\) is smaller, the bound tends to be tighter. However, if \(m\) is too small, the model's expressive capacity is limited, potentially resulting in a large empirical risk \(_{}(f)\), and consequently, a large population risk. Therefore, using moderate-size references is a promising choice, as supported by our empirical validation results in Appendix D.3 (see Figure 6).

**IV. Regularization on references norm barely helps** Regularizing the norm of references \(\|\|_{2}\), i.e., reducing \(b_{D}\), might be considered to enhance the model's generalization. However, it is important to note that \(b_{D}\) only influences the term \(_{3}\) (in logarithm) in Theorem 3.2 and has a tiny influence on the overall bound. Conversely, such regularization constrains the model's expressive capacity, potentially leading to a large \(_{}(f)\) and increasing the population risk. This observation is empirically supported by experiments in Appendix D.7 (see Table 10).

**V. GRDL has a tighter bound than GIN with global pooling** In Appendix A, we provide the generalization error bound, i.e., Theorem A.1, for GIN with global pooling and compare it with Theorem 3.2. The result shows that our GRDL has a stronger generalization ability than GIN, which is further supported by the numerical results in Table 4.

_Remark 3.4_.: Currently, we use \(K\) reference distributions for classification (one for each class). One natural approach to enhancing the model's expressive power is increasing the number of references for each class. However, counterintuitively, our empirical observations, supported by Theorem B.1, suggest that having only one reference per class is optimal. We discuss this further in Appendix B.

## 4 Related Work

Various sophisticated pooling operations have been designed to preserve the structural information of graphs (Bianchi et al., 2020; Ranjan et al., 2020; Baek et al., 2021; Chen and Gel, 2023; Yu et al., 2024). For instance, DIFFPOOL, designed by Ying et al. (2018), learns a differentiable soft cluster assignment for nodes and maps nodes to a set of clusters to output a coarsened graph. Another method by Lee et al. (2019) utilizes a self-attention mechanism to distinguish nodes for retention or removal, and both node features and graph topology are considered with the self-attention mechanism.

A recent research direction focuses on preserving structural information by leveraging the optimal transport (OT) (Peyre and Cuturi, 2020). OT-GNN, proposed by Chen et al. (2021), embeds a graph to a vector by computing Wasserstein distances between node embeddings and some "learned point clouds". TFGW, introduced by Vincent-Cuz et al. (2022), embeds a graph to a vector of Fused Gromov-Wasserstein (FGW) distance (Vayer et al., 2018) to a set of "template graphs". OT distances have also been combined with dictionary learning to learn graph vector embedding in an unsupervised way (GDL) (Liu et al., 2022; Vincent-Cuz et al., 2021; Zeng et al., 2023).

Similar to the "learned point clouds" in OT-GNN, "template graphs" in TFGW, and dictionaries in GDL, our GRDL preserves information in node embeddings using reference distributions. To the best of the authors' knowledge, we are the first to model a graph's node embeddings as a discrete distribution and propose to classify it directly without aggregating it into a vector, marking our novel contribution. Additionally, our work stands out as the first to analyze the generalization bounds for this type of model, adding a theoretically grounded dimension to the research. By the way, our method is much more efficient than OT-GNN and TFGW. Please see Figure 2 and Table 8.

## 5 Numerical Experiments

### Graph Classification Benchmark

DatasetsWe leverage eight popular graph classification benchmarks (Morris et al., 2020), comprising five bioinformatics datasets (MUTAG, PROTEINS, NCI1, PTC-MR, BZR) and three social network datasets (IMDB-B, IMDB-M, COLLAB). We also use three large-scale imbalanced datasets (PC-3, MCF-7, and ogbg-molhiv (Hu et al., 2020)). A summary of data statistics is in Table 6.

    &  &  \\   & MUTAG & PROTEINS & NCI1 & IMDB-B & IMDB-M & PTC-MR & BZR & COLLAB \\   PATCHY-SAN & **92.6\(\)4.2** & 75.1\(\)3.3 & 76.9\(\)2.3 & 62.9\(\)3.9 & 45.9\(\)2.5 & 60.0\(\)4.8 & 85.6\(\)3.7 & 73.1\(\)2.7 & 71.5 \\ GIN & 89.4\(\)5.6 & 76.2\(\)2.8 & 82.2\(\)8.0 & 64.3\(\)3.1 & 50.9\(\)1.7 & 64.6\(\)7.0 & 82.6\(\)3.5 & 79.3\(\)1.7 & 73.6 \\ DropGIN & 90.4\(\)7.0 & 76.9\(\)4.3 & 81.9\(\)2.5 & 66.3\(\)5.5 & 61.3\(\)2.3 & 66.3\(\)8.6 & 77.8\(\)2.6 & 80.1\(\)2.8 & 73.9 \\ DIFFPOOL & 89.4\(\)4.6 & 76.2\(\)1.4 & 80.9\(\)0.7 & 61.1\(\)3.0 & 45.8\(\)1.4 & 60.0\(\)5.2 & 79.8\(\)3.6 & 80.8\(\)1.6 & 71.8 \\ SEP & 89.4\(\)6.1 & 76.4\(\)0.4 & 78.4\(\)0.6 & **74.1\(\)0.6** & 51.5\(\)0.7 & 68.5\(\)5.2 & 86.9\(\)0.8 & **81.3\(\)0.2** & 75.8 \\ GMT & 89.9\(\)4.2 & 75.1\(\)0.6 & 79.9\(\)0.4 & **73.5\(\)0.8** & 50.7\(\)2.6 & 82.6\(\)0.8 & 80.7\(\)0.5 & 75.7 \\ MnCutPol & 90.6\(\)4.6 & 74.4\(\)0.7 & 74.5\(\)0.9 & 72.7\(\)0.8 & 51.0\(\)7.0 & 78.3\(\)4.4 & 87.1\(\)1.0 & **89.0\(\)0.3** & 75.0 \\ ASAP & 87.4\(\)5.7 & 73.9\(\)0.6 & 71.5\(\)0.4 & 72.8\(\)0.5 & 50.8\(\)0.8 & 64.6\(\)6.8 & 85.3\(\)1.3 & 78.6\(\)0.5 & 73.1 \\ WittTopOpol & 89.4\(\)5.4 & 80.0\(\)3.2 & 79.9\(\)1.3 & 72.6\(\)1.8 & **52.9\(\)1.8** & 64.6\(\)1.8 & 87.8\(\)2.4 & 80.1\(\)1.6 & 75.9 \\  OT-GNN & 91.6\(\)4.6 & 76.6\(\)4.0 & **82.9\(\)1.2** & 67.5\(\)3.5 & 52.1\(\)3.0 & 68.7\(\)5.9 & 85.3\(\)3.3 & 80.7\(\)2.9 & 75.7 \\ WEGL & 91.0\(\)3.4 & 73.7\(\)1.9 & 75.5\(\)1.4 & 66.4\(\)2.1 & 50.3\(\)1.0 & 66.2\(\)6.9 & 84.4\(\)4.6 & 79.6\(\)0.5 & 73.4 \\  FGW - ADJ & 82.6\(\)7.2 & 72.4\(\)4.7 & 74.4\(\)2.1 & 70.8\(\)3.6 & 48.9\(\)3.9 & 55.3\(\)0.8 & 86.9\(\)1.0 & 80.6\(\)1.5 & 71.5 \\ FGW - SP & 84.4\(\)7.3 & 74.3\(\)3.3 & 72.8\(\)1.5 & 65.0\(\)4.7 & 47.8\(\)3.8 & 55.7\(\)0.8 & 86.9\(\)1.0 & 78.7\(\)2.4 & 70.6 \\ WL & 87.4\(\)5.4 & 74.2\(\)6.8 & **58.1\(\)2.7** & 67.5\(\)4.0 & 48.4\(\)4.2 & 56.0\(\)3.9 & 81.3\(\)0.6 & 78.5\(\)1.7 & 72.4 \\ WWL & 86.3\(\)7.9 & 73.1\(\)1.4 & **85.7\(\)0.8** & 71.6\(\)3.8 & 52.6\(\)3.0 & 52.6\(\)6.8 & 87.6\(\)0.6 & **81.4\(\)2.1** & 73.9 \\  SAT & **92.6\(\)4.3** & 77.7\(\)3.2 & 82.5\(\)0.8 & 70.0\(\)1.3 & 47.3\(\)3.2 & 68.3\(\)4.9 & **91.7\(\)2.1** & 80.6\(\)0.6 & 76.1 \\ Graphformer & 89.6\(\)6.2 & 76.3\(\)2.7 & 78.6\(\)2.1 & 70.3\(\)0.9 & 48.9\(\)2.0 & **71.4\(\)5.2** & 85.3\(\)2.3 & 80.3\(\)1.3 & 75.1 \\   GRDL & **92.1\(\)5.9** & **82.6\(\)1.2** & 80.4\(\)0.8 & **74.8\(\)2.0** & **52.9\(\)1.8** & 68.3\(\)5.4 & **92.9\(\)1.1** & **79.8\(\)9.0** & **77.9** \\ GRDL-W & 90.8\(\)4.6 & **82.1\(\)0.9** & 80.9\(\)0.8 & 72.2\(\)3.3 & **53.1\(\)9.9** & 68.5\(\)3.2 & 90.6\(\)1.5 & 80.4\

**Baselines** Our approach is benchmarked against four groups of state-of-the-art baselines: 1) GNN models with global or sophisticated pooling operations, including PATCHY-SAN (Niepert et al., 2016), DIFFPOOL (Ying et al., 2018), GIN (Xu et al., 2018), DropGIN (Papp et al., 2021), SEP (Wu et al., 2022), GMT (Baek et al., 2021), MinCutPool (Bianchi et al., 2020), ASAP (Ranjan et al., 2020), and Wit-TopoPool (Chen and Gel, 2023); 2) Optimal transport based models such as WEGL (Kolouri et al., 2020) and OT-GNN (Chen et al., 2021); 3) Kernel-based approaches including FGW (Tittouan et al., 2019) operating on adjacency (ADJ) and shortest path (SP) matrices, the WL subtree kernel (Shervashidze et al., 2011), and the Wasserstein WL kernel (Toginalli et al., 2019); 4) Graph transformers including Graphormer (Ying et al., 2021) and SAT (Chen et al., 2022a). We also show the results of two variations of our GRDL: GRDL using Sinkhorn divergence (GRDL-S) and GRDL using Wasserstein distance (GRDL-W). For large imbalanced datasets, we only benchmark our GRDL against PATCHY-SAN, GIN, and DIFFPOOL because other methods are too costly. Details about the initialization and hyper-parameters setting can be found in Appendix D.3.

**Experiment Settings** Due to the page limitation, please refer to Appendix D.2.

**Classification Results** Table 1 shows the classification results. The AUC-ROC scores of experiments results on the three large imbalanced datasets are reported in Table 2. Our method has top 3 classification performance over baselines in almost all datasets. Our GRDL, GRDL-W and GRDL-S have close performance. However, as shown later in Figure 2, our original GRDL has significantly lower time costs and thus is preferable for practical use. Graph transformers also have competitive performance, but they have significantly larger amount parameters and much higher time costs than our model, as shown by Table 13 in Appendix D.8.

### Time Cost Comparison

We compare the time cost of our GRDL with two models that leverage optimal transport distances discussed in Section 4: OT-GNN (Chen et al., 2021) and TFGW (Vincent-Cuaz et al., 2022). Compared with them, our model has significantly lower time costs. We present empirical average training time per epoch in Figure 2 and average prediction time per graph in Table 9 in Appendix D.4. Experiments were conducted on CPUs (Apple M1) using identical batch sizes, ensuring a fair comparison. It's noteworthy that the OT solver employed in TFGW and OT-GNN is currently confined to CPU, influencing the choice of hardware for this evaluation. We analyzed the theoretical time complexity in Appendix D.4 (see Table 8).

We also compare training time with two latest pooling methods including Wit-TopoPool (Chen and Gel, 2023) and MSGNN (Lv et al., 2023) on eight real datasets and three synthetic datasets. The three synthetic datasets have 2000 graphs with 100(SYN-100), 300(SYN-300), and 500(SYN-500) nodes per graph, respectively. The edge number is \(0.1n^{2}\) where \(n\) is the number of nodes. The empirical training time per epoch is shown in Table 3, where empty of MSGNN means it takes more than 200 seconds to train a single epoch, which is too costly. As can be seen, our method is the most efficient among these three methods.

### Graph Visualization

We use t-SNE (Van der Maaten and Hinton, 2008) to visualize the distributions of graphs' node embeddings given by our GRDL model, which is equivalent to visualizing each graph in a 3-D coordinate system. Firstly we use MMD to calculate a distance matrix \(^{(N+K)(N+K)}\) between

    &  \\   & PC-3 & MCF-7 & ogbg-molhiv \\  GIN & 84.6\(\)1.4 & 80.6\(\)1.5 & 77.8\(\)1.3 \\ DIFFPOOL & 83.2\(\)1.9 & 77.2\(\)1.3 & 73.7\(\)1.8 \\ PATCHY-SAN & 80.7\(\)2.1 & 78.9\(\)3.1 & 70.2\(\)2.1 \\ GRDL & **85.1\(\)1.6** & **81.4\(\)1.3** & **79.8\(\)1.0** \\   

Table 2: AUC-ROC scores of large imbalanced data classification. Bold text indicates the best.

    & MutData & proteins & NCI & IMD-B & IMD-M & PTC-MR & BZR & COLLAB & SYN-100 & SYN-300 & SYN-500 \\  GRDL (ours) & 0.4 & 3.4 & 12.6 & 2.4 & 3.5 & 0.8 & 1.2 & 16.3 & 26.6 & 45.8 & 88.7 \\ WitTorPool & 0.4 & 2.6 & 21.4 & 2.4 & 2.6 & 1.0 & 1.3 & 39.1 & 32.9 & 50.8 & 97.5 \\ MSGNN & 45.2 & - & - & - & 75.5 & 135.3 & - & - & - & - \\   

Table 3: Comparison of time cost (second) per epoch with Wit-TopoPool and MSGNN.

the node embeddings \(\{_{i}\}_{i=1}^{N}\) and the reference distributions \(\{_{k}\}_{k=1}^{K}\). The 3-D visualization given by t-SNE using **C** is presented in Figure 3. The graphs are located around the references. It means that the learned references can represent realistic graphs' latent node embeddings from the data.

### More Numerical Results

The ablation study, influence of \(\), generalization comparison with GIN are in Appendices D.5, D.6, and A, respectively.

## 6 Conclusions

We proposed GRDL, a novel framework for graph classification without global pooling operations and hence effectively preserve the information of node embeddings. What's more, we theoretically analyzed the generalization ability of GRDL, which provided valuable insights into how the generalization ability scales with the properties of the graph data and network structure. Extensive experiments on moderate-scale and large-scale benchmark datasets verify the effectiveness and efficiency of GRDL in comparison to baselines. However, on some benchmark datasets (e.g. NCI1), our model does not outperform the baseline, which may be a limitation of our work and requires further investigation in the future.

Figure 3: T-SNE visualization of MUTAG embeddings and reference distributions given by GRDL. Each dot denotes a graph and each square denotes a reference distribution.

Figure 2: Average training time per epoch. GRDL is 10 times faster than OT-GNN and TFGW.