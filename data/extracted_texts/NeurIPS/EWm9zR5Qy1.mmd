# The Multimodal Universe: Enabling Large-Scale Machine Learning with 100 TB of Astronomical Scientific Data

**The Multimodal Universe Collaboration**

**Eirini Angeloudi\({}^{1,2}\), Jeroen Audenaert\({}^{3}\), Micah Bowles\({}^{4,5}\), Benjamin M. Boyd\({}^{6}\), David Chemaly\({}^{6}\), Brian Cherinka\({}^{7}\), Ioana Ciuca\({}^{8,9,10}\), Miles Cranmer\({}^{6,5}\) Aaron Do\({}^{6}\), Matthew Grayling\({}^{6}\), Erin E. Hayes\({}^{6}\), Tom Hehir\({}^{6,5}\) Shirley Ho\({}^{11,12,13,5}\), Marc Huertas-Company\({}^{1,2,9}\), Kartheik G. Iyer\({}^{14,11,9}\), Maja Jablonska\({}^{10,9}\) Francois Lanusse\({}^{11,5,15}\), Henry W. Leung\({}^{16}\), Kaisey Mandel\({}^{6}\), Juan Rafael Martinez-Galarza\({}^{17,18}\) Peter Melchior\({}^{13}\), Lucas Meyer\({}^{11,5}\), Liam H. Parker\({}^{11,5,19}\), Helen Qu\({}^{20}\) Jeff Shen\({}^{13}\), Michael J. Smith\({}^{21,9}\), Connor Stone\({}^{22,23,24}\), Mike Walmsley\({}^{16}\), John F. Wu\({}^{7,25}\)

###### Abstract

We present the Multimodal Universe, a large-scale multimodal dataset of scientific astronomical data, compiled specifically to facilitate machine learning research. Overall, the Multimodal Universe contains hundreds of millions of astronomical observations, constituting 100 TB of multi-channel and hyper-spectral images, spectra, multivariate time series, as well as a wide variety of associated scientific measurements and "metadata". In addition, we include a range of benchmark tasks representative of standard practices for machine learning methods in astrophysics. This massive dataset will enable the development of large multi-modal models specifically targeted towards scientific applications. All codes used to compile the Multimodal Universe and a description of how to access the data is available at https://github.com/MultimodalUniverse/MultimodalUniverse

## 1 Introduction

Web-scale datasets containing billions of text and image examples have been instrumental in the development and scaling of large foundation models for language and vision . Surprisingly, unified datasets of comparable scale have not yet been developed for many scientific domains, hindering the progress of large, advanced machine learning (ML) models forscience . Yet, the integration of large-scale, standardized, and comprehensive scientific datasets spanning multiple data modalities is essential for the development of ML models capable of fully leveraging the diverse spectrum of scientific data beyond traditional text and images.

To that end, we introduce the Multimodal Universe (Figure 1), a large-scale, multimodal dataset of scientific data designed not only for ML research in astronomy but also to enable the broader development of scientific foundation models. The Multimodal Universe capitalizes on the wide variety of astronomical observations available from a diversity of ground- and space-based telescopes to aggregate a dataset of hundreds of millions of astrophysical objects and phenomena. Altogether, this constitutes over 100 TB of open-access and copyright-free data spanning multiple observational modalities, including multi-channel and hyperspectral images, optical spectra, multivariate time-series, and an extensive array of associated scientific measurements. This diverse dataset provides an unprecedented resource for the development of sophisticated ML models for astrophysics and scientific data in general.

In keeping with its scientific goals, the Multimodal Universe also emphasizes the importance of relevant scientific "metadata". This includes relevant contextual information for each observation (e.g. observational noise, pixel scale, instrumental response, etc.) that not only helps preserve the dataset's utility for scientific ML but also enables the development of models that integrate information beyond the raw data level. In addition to the dataset, we also describe a range of benchmark tasks and baseline deep learning models that reflect current best practices in astrophysics. These benchmarks provide a foundation for evaluating new models, enabling researchers to compare their approaches against established standards and push the boundaries of what is possible with ML in astronomy.

Finally, extensibility and accessibility are core to the Multimodal Universe project. All data subsets are accessible as standardized Hugging Face datasets. They can be downloaded locally in full or accessed as streaming datasets directly from the Hugging Face Hub, see section 6. The code used to compile each component of the Multimodal Universe from its respective official data source is hosted on the project's GitHub repository for transparency and reproducibility, and utilities to generate cross-matched (and multi-modal) versions of the dataset are also included. The architecture of the Multimodal Universe source code is consistent, maintainable, and extensible. This is in contrast to the common practice in the field, which is for independent researchers, groups, or collaborations to decide their data formats in isolation of one another, creating a significant barrier to entry for multimodal research in astronomy. By making Multimodal Universe publicly available, we aim to

Figure 1: Illustration of the main modalities included in the Multimodal Universe, along with typical associated machine learning tasks. In addition, the Multimodal Universe also includes a small amount of hyperspectral images and tabular data, not shown here.

catalyze innovation and collaboration among the astrophysics and ML communities. We believe that this dataset will not only advance our understanding of the universe but also contribute to the broader development of multimodal and metadata-aware ML methodologies.

## 2 Related Work

Scientific Machine Learning Datasets in Other FieldsLarge curated and open datasets are gaining traction outside of the textual domain. In remote sensing, Major-TOM  is a collated set of ESA Copernicus mission data that aims to standardise earth observation imagery into a common, ML-friendly format. Currently, Major-TOM comprises a combined 62 TB of imagery. Like the Multimodal Universe, Major-TOM is run as an open source project that the community can collectively contribute to. In a similar vein, the MOMENT project  introduces a large time-series dataset, as a publicly-available collection of 1.2B timestamps taken from 13 domains. These domains include--but are not limited to--medical information, economic indicators, power consumption, IoT weather data, speech, and general instrumental monitoring.

Large-Scale Machine Learning Datasets in AstrophysicsWhile astrophysics is inherently data-rich, most datasets have been compiled with traditional analyses in mind. This has resulted in data archives that are experiment-specific, non-uniformly stored, and not optimized for ML applications. Indeed, only a few exceptions exist where large datasets have been compiled for ML. For instance,  introduced a substantial dataset of 76M images, enabling one of the first instances of self-supervised learning in astrophysics. Additionally, the PLAsTiCC  light curve classification challenge is the largest collection of astronomical time-series data for ML research, with 3.5 million simulated light curves. Finally, the Galaxy Zoo project has provided large galaxy morphology classification datasets for ML  that have been instrumental in various astronomy specific ML studies. This includes early astronomy applications of CNNs , Bayesian deep learning , and modern neural scaling law analyses . However, none of these examples provide multimodal data, nor reach the scale of the data collected for the Multimodal Universe.

## 3 Creating a Large-Scale Dataset of Diverse Astronomical Data

Modern astronomy has entered an era of large-scale, systematic surveys, supported by a range of space-based and ground-based instruments. These surveys generate unprecedented volumes of science-ready data products that are publicly available through open-access databases. However, these products are typically designed for conventional scientific workflows and not well-suited for ML applications; for example, astronomical imaging surveys typically provide full-frame mosaics from which individual uniform galaxy images must be cut out. As such, using these datasets effectively requires a high level of expertise and can be time-consuming. In addition, data structures and data access vary significantly among surveys, making it difficult to navigate diverse systems and interfaces to gather the necessary data. Therefore, to make use of astrophysical data, researchers must understand the intricacies of each separate survey and corresponding archive. This steep learning curve is a significant barrier to entry for ML applications in astrophysics, highlighting the need for a more streamlined and standardized approach to data access and preparation.

### Methodology

Data Curation StrategyTo overcome the barriers between different subfields and datasets imposed by the significant domain-specific scientific expertise required, we build the Multimodal Universe by drawing on a community of diverse domain scientists. To facilitate their engagement, we have structured the data curation process to have a relatively low barrier to entry. Specifically, we enable domain scientists to contribute by simply adding scripts that download and process data from the official archives they know how to access into a standardized Multimodal Universe format. While we do not impose stringent code quality requirements at this stage, it is essential that these scripts are reproducible and clearly document any selection or cuts applied to the original data. This approach allows researchers to contribute without extensive software engineering skills, as long as data collection is transparent and well-documented. All data collection scripts are publicly available to ensure the data curation process is reproducible. Once data is downloaded from its official repository, our data curation strategy involves adopting the HuggingFace Datasets framework and imposing a standardized format for each modality, ensuring compatibility across different surveys and ease of use for ML applications.

Multimodal Cross-MatchingMultimodality is a crucial aspect of the Multimodal Universe, but it presents several challenges. Specifically, each astronomical survey provides data in a single modality; therefore, to build multimodal examples, it is necessary to cross-match observations from several sources. Fortunately, a fundamental property of every survey is the provision of sky coordinates for each observation, enabling cross-survey matching via coordinate matching. However, the intersection between two or more surveys is often relatively small. This limitation inherently prevents the collection of dataset pairs on the scale of projects like Laion-5B , primarily because some modalities are much more expensive to acquire than others. Consequently, it is essential to provide access to large-scale unpaired datasets, along with the tools to easily generate various multimodal dataset pairings. The Multimodal Universe addresses this need by offering a set of common cross-matched datasets and utilities that enable end-users to generate custom cross-matches tailored to their specific scientific applications. This approach ensures flexibility and maximizes the dataset's utility across different research needs.

Figure 2 summarizes the steps of our data curation and data access methodology.

### Overview of the Dataset

Table 1 provides a breakdown of the different scientific modalities included in the Multimodal Universe, along with a description of their origin and their scientific relevance. Overall, the Multimodal Universe represents 100 TB of data, largely dominated by 88TB of multi-band imaging.

We provide below further description of these modalities and summarize the standardized fields and metadata defined for the main modalities of the Multimodal Universe in Table 2. A complete description of the schema for each modality in the dataset is further provided in Appendix A.

Figure 2: Illustration of the methodology behind the Multimodal Universe. Domain scientists with expertise in a given astronomical survey provide data download and formatting scripts through Pull Requests on the project repository https://github.com/MultimodalUniverse/MultimodalUniverse. All datasets are then downloaded from their original source and made available as Hugging Face datasets sharing a common data schema for each modality and associated metadata. End-users can then generate any combination of Multimodal Universe subsets using provided cross-matching utilities to generate multimodal datasets.

Optical and Infrared ImagingAstronomical images are obtained via cameras installed at the focal plane of both space and ground-based telescopes, that capture photon emission from distant astronomical sources. An observed image \(I\) can be described as: \(I=S*+n\) where \(S\) is the intrinsic source emission on the sky, \(n\) is the measurement noise, \(*\) is the convolution operation, and \(\) is the instrumental response of the telescope, also known as the Point Spread Function (PSF). The PSF is at minimum characterized by its full width half maximum (fwhm), which sets the spatial resolution of the image. In addition, these images are typically acquired in a number of specific broad wavelength ranges, referred to as bands or channels. Knowledge of the bands, PSF, noise properties, and pixel scale, are generally required to fully characterize an observation and take into account the specificities of a particular instrument or observatory. Compared to natural images, astronomical images are typically noisier, and exhibit a large dynamic range, generally spanning several orders of magnitude between the bright objects and fainter sources.

SpectroscopySimilarly to imaging data, light from distant objects is gathered and focused first by the telescope, but instead of being directly imaged with a camera, the light from cosmic objects is dispersed into various wavelengths or energies using a spectrograph. A spectrum is therefore a 1-d signal representing the decomposition of the light from an object as function of wavelength or energy of the photons. Similarly to images, a measured spectrum \(y\) is the result of the following observational process: \(y=L*x+n\) where \(L\) is the instrument's impulse response (also known as Line Spread Function; LSF) and \(n\) is a measurement noise. Contrary to images, where the absolute position of each pixel on the sky is not directly needed to interpret the signal, for spectroscopy knowledge of absolute wavelength corresponding to each pixel is crucial, which is why each spectrum not only provides a array of measured flux \(y\) but also the corresponding wavelength array \(\).

Hyperspectral imagingHyperspectral imaging is a data modality that combines imaging and spectroscopy to produce spatially resolved spectra in a two-dimensional region. It uses an Integral Field Unit (IFU) to produce a three-dimensional "datacube" consisting of 2d array of spatial pixel elements, i.e "spaxels", each containing a spectrum in a specific wavelength

    &  & \)} &  &  &  \\  & & & & & samples & science \\   & Legacy Surveys DR10  & 4 & 160\(\)160 & 124M & Galaxies \\   & Legacy Surveys North  & 3 & 152\(\)152 & 15M & Galaxies \\   & HSC  & 5 & 1603\(\)160 & 477K & Galaxies \\   & RTS  & 3 & 63\(\)63 & 400K & Supernovae \\   & JWST  & 6-7 & 96\(\)96 & 300K & Galaxies \\   & Gaia BP/RP  & - & 110\({}^{1}\) & 220M & Stars \\   & SDSS-II  & - & Variable & 4M & Galaxies, Stars \\   & DESI  & - & 7081 & 1M & Galaxies \\   & APOGEE SDSS-III  & - & 7514 & 716k & Stars \\   & GALAH  & - & Variable & 325k & Stars \\   & Chandra  & - & Variable & 129K & Galaxies, Stars \\   & VIPERS  & - & 557 & 91K & Galaxies \\  Hyperspectral Image & MaNGA SDSS-IV  & 4563 & 96\(\)96 & 12k & Galaxies \\   & PLASTiCC\({}^{}\) & 6 & Variable & 3.5M & Time-varying objects \\   & TESS  & 1 & Variable & 1M & Exoplanets, Stars \\   & CA Sample  & 5-11 & Variable & 1K & Supernovae \\   & YSE  & 6 & Variable & 2K & Supernovae \\   & PSI SNe Ia  & 4 & Variable & 369 & Supernovae \\   & DESY 3 SNe Ia  & 4 & Variable & 248 & Supernovae \\   & SNLS  & 4 & Variable & 239 & Supernovae \\   & Foundation  & 4 & Variable & 180 & Supernovae \\   & CSP SNe Ia  & 9 & Variable & 134 & Supernovae \\   & Swift SNe Ia & 6 & Variable & 117 & Supernovae \\   & Gaia  & - & - & 220M & Stars \\   & PROVABGS  & - & - & 221K & Galaxy \\   & Galaxy10 DECALS  & - & - & 15K & Galaxy \\  

Table 1: Summary of samples included in the Multimodal Universe, details of all samples are provided in Appendix A. N\({}_{c}\) indicates the number of channels of a given observation.  These are represented as 110 basis coefficients that can be resampled to an arbitrary wavelength grid.  Simulated dataset.

range. IFUs provide a wealth of information. For example, by analyzing the spectrum within each spaxel, one can produce "images" of spectral features and derived parameters, such as emission/absorption lines and the velocities of gas and stars in the galaxy. By aggregating spectra together within specific spatial regions, one can compare the properties (such as star forming rates) of different physical regions of the galaxy. By extension of both images and spectra, hyperspectral images are accompanied by an instrumental response in the form of both Point Spread Function and Line Spread Function information.

Time-SeriesTime series data, often called light curves (brightness over time), are commonly examined when astronomical objects or events vary in brightness in time. These include relatively short-lived, explosive transients such as supernovae (exploding stars) and tidal disruption events, long-lived sources that show stochastic variation over time such as active galactic nuclei and quasars, and (quasi-)periodic sources such as pulsating stars and exoplanets. These time series are a sequence of flux (i.e. intensity) measurements. Because of their unique properties, astronomical time-series pose a challenge to traditional ML architectures. For one, most light curves are sparsely and irregularly sampled in both time and wavelength due to the unique survey strategy of each telescope. Flux measurements are also often plagued with large uncertainties, especially those from faint sources. Finally, data from different telescopes are highly heterogeneous and thus difficult to combine (e.g., to build a larger training set for ML applications). Together, these factors make our time-series data products a compelling and challenging dataset for ML researchers.

Tabular dataThe Multimodal Universe also aggregates large catalogs of tabular data from different sources, corresponding either to particular measurements (e.g. measured flux for a particular object), or the result of the processing of these observations by an analysis pipeline (e.g. calculations of galaxy properties or reporting of galaxy morphology)1.

## 4 Examples of Machine Learning Tasks on Different Modalities

The data accumulated in the Multimodal Universe covers a wide range of scientific uses-cases, spanning sub-fields of astrophysics from stellar physics to galaxy formation. We include in this section a few conventional examples of ML tasks in astrophysics as a way to illustrate how to interact with the Multimodal Universe dataset and to provide illustrations of scientific uses cases. We note that these examples are far from exhaustive.

  Modality & Field & Description \\   & flux & Array of flux measurements of the image \\  & ivar & Inverse variance of noise in the image \\  & band & Key indicating the wavelength range of the image \\  & psf\_fwhm & Size of the instrumental response (Point Spread Function) \\  & scale & Scale of pixels on the sky \\   & flux & Measured flux as a function of wavelength \\  & ivar & Inverse variance of noise on measured flux \\  & lsf\_sigma & Size of the instrumental response (Line Spread Function) \\  & lambda & Wavelength of each flux measurement \\   & flux & Measurements of flux as a function of time \\  & flux\_err & Uncertainty on flux measurement \\   & band & Key indicating the wavelength range of the measurement \\   & time & Time of observation \\  

Table 2: Description of standardized fields and metadata provided for the main modalities. These fields represent necessary and near-sufficient information to allow for the consistent interpretation of observations from multiple surveys or instruments.

### Characterisation of Galaxies

Morphology ClassificationGalaxy shape or morphology is a first-order tracer of the history of a galaxy. As such, classifying images of galaxies based on their apparent morphologies and visual features is a common task. Labeling these features is usually done by visual inspection from experts or using citizen science, such as . Here, we use the Galaxy10 DECaLS catalog of human-labeled annotations spanning ten broad morphological classes paired with the Multimodal Universe's RGB-converted images from the Legacy Surveys and report the top-1 accuracy of supervised baselines on the held-out test set in Table 3. We include state of the art results from the literature from  as a point of reference and compare different common architectures for this task (EfficientNetB0, ResNet18, and DenseNet121).

Physical Property InferenceAstrophysicists often seek to predict fundamental properties of galaxies from observational data. Accurate predictions of these properties provide key insights into galaxy formation and evolution. Here, we consider the following properties:

* Redshift (\(Z_{HP}\)): The extent to which the light from a galaxy has been stretched by the expansion of the universe, helping to determine the galaxy's distance from earth.
* Stellar Mass (\( M_{*}\)): The total mass of stars in a galaxy in units of solar masses.
* Age (\(t_{age,MW}\)): The age of stars in a galaxy, weighted by galaxy stellar mass.
* Specific Star-Formation Rate (\(sSFR\)): The rate at which stars in the galaxy are formed normalized by the galaxy stellar mass.
* Metallicity (\(Z_{MW}\)): The abundance of elements heavier than hydrogen and helium in the galaxy, typically weighted by the galaxy mass.

To implement this task, we use the Multimodal Universe's built-in cross-matching functionality to combine the PROVABGS catalog of physical properties, with both the Legacy Survey North and South imaging sample and the DESI spectroscopic sample. We report the \(R^{2}\) value on the held-out test set in Table 4.

### Characterisation of Transient events

Candidate IdentificationGiven their short-lived nature, early identification of transients is of great value to allow for further observations which study their evolution. The publicly-available BTSbot model  - a multimodal convolutional neural network - was developed to automate identification of new astrophysical transient candidates in series of images and associated metadata. We report the performance of BTSbot on the binary classification of real astrophysical transients and bogus or uninteresting detections using a dataset from the Zwicky Transient Facility [ZTF; 15, 16] Bright Transient Survey [BTS; 56, 114].

   Modality & Source Survey & Model & \(Z_{HP}\) & \( M_{*}\) & \(Z_{MW}\) & \(t_{age,MW}\) & \(sSFR\) \\   &  & ResNet18 & 0.771 & 0.725 & 0.381 & 0.210 & 0.405 \\   & & DenseNet121 & 0.774 & 0.734 & 0.414 & 0.267 & 0.446 \\   & & EfficientNetB0 & 0.697 & 0.645 & 0.395 & 0.260 & 0.421 \\  Spectrum & DESI & Conv+Att  & 0.982 & 0.871 & 0.659 & 0.488 & 0.679 \\  Photometry & PROVABGS & MLP & 0.696 & 0.681 & 0.383 & 0.308 & 0.343 \\   

Table 4: Model \(R^{2}\) performance comparison for predicting galaxy properties from different observational data modalities.

Light-curve Classification and RegressionClassifying astrophysical sources can be challenging due to the sparsity of time-series data, limited filter coverage, or simply similarity between time-series of different object types. Spectroscopic data are required for unambiguous classification, but these follow-up observations will be prohibitively expensive for large surveys. Accurate photometric classification of astrophysical transients is thus highly desirable. This is a supervised learning problem, and has been attempted with both simulated (e.g. [101; 107; 71; 83; 116]) and real training data (e.g. [143; 46; 30; 31; 91]). We provide baseline photometric classification results on a real dataset from the Young Supernova Experiment (YSE) as well as a simulated dataset from the Photometric LSST Astronomical Classification Challenge (PLAsTiCC). While we report state-of-the-art results from the literature on PLAsTiCC, YSE represents a specific challenge due to its limited size, and as such specifically provides an implementation of a Random Forest classifier, which is a conventional approach for handling limited dataset sizes. Details of these methods are included in Appendix B. Finally, as a related task, we also include an example of estimating redshift from light curves on the PLAsTiCC dataset. All metric performances are stated in Table 5.

### Bridging the Modality Gap: Contrastive Image-Spectrum Pretraining

As a concrete example of the types of models that can be created thanks to the access to a large repository of multimodal data, we present a reproduction of the recent AstroCLIP method . This variant of Contrastive Language-Image Pretraining (CLIP)  aims to build in a self-supervised representations of multimodal data by contrastive alignment between image and spectra modalities. AstroCLIP embeddings have been shown to perform in-line with supervised baselines on tasks like redshift prediction, galaxy property prediction, and morphology classification while also allowing for powerful in-modality and cross-modal retrieval. We demonstrate how Multimodal Universe makes it possible to easily build such models by generating a cross-matched dataset of images from the Legacy Survey sample and optical spectra from the DESI sample. From this dataset we can train by contrastive learning image and spectra embeddings. We show in Table 6 k-Nearest Neighbour zero-shot regression results for redshift and galaxy property prediction, and show better performance compared to the supervised results reported in subsection 4.1.

## 5 Discussion

**Paving the road towards Foundational Research in Scientific Machine Learning** Beyond its multimodal and large-scale nature, the Multimodal Universe also serves as a valuable testbed for addressing some of the practical challenges encountered in the use of machine learning within application domains such as astrophysics and other scientific disciplines. Although machine learning has been applied in astronomy since at least the 1990s, and been extensively adopted in astrophysics research over the past decade, most

  Modality & Source Survey & \(Z_{HP}\) & \( M_{*}\) & \(Z_{MW}\) & \(t_{age,MW}\) & \(sSFR\) \\  Image & Legacy Surveys & 0.801 & 0.737 & 0.432 & 0.240 & 0.435 \\ Spectrum & DESI & 0.986 & 0.879 & 0.584 & 0.441 & 0.643 \\  

Table 6: \(R^{2}\) performance of zero-shot prediction of galaxy properties from image and spectrum AstroCLIP embeddings following the strategy described in .

  Source Survey & Task & Metric & Model & Performance \\  BTS & Transient Candidate Identification & AUC & BTSbot [119; 120] & 0.985 \\  YSE & SN Ia classification & AUC & Random forest & 0.90 \\   & 14-way classification & Accuracy & Avocado  & 77.4 \\  & 14-way classification & Accuracy & Connect Later  & 79.9 \\   & Redshift estimation & RMSE & Connect Later  & 0.247 \\  

Table 5: Results for astronomical time-series tasks. The performance of each model on the associated metric for the task is shown in the Performance column. Avocado is a Random Forest architecture while Connect Later is a transformer architecture.

applications are tailored to specific datasets, requiring domain expertise, with models often being trained from scratch for similar purposes. Consequently, machine learning applications face issues such as distribution shifts , uncertainty quantification , and the proper calibration of predictive models . The Multimodal Universe is designed to facilitate the development of next-generation machine learning models in astrophysics, enabling research into each of these critical facets of applied machine learning in astronomy. We believe that the complexity, multimodal nature, scale, and the culture of publicly released data rooted in the astronomical community will enable the machine learning community at large to tackle some of the key challenges in large-scale machine learning development.

LimitationsWhile the Multimodal Universe includes a significant collection of multimodal scientific data, a notable omission is the lack of associated textual data. Providing image-text or observation-text pairs is not currently the norm for astronomical observations. Therefore, despite promising work on astronomy fine-tuned LLMs , LLM evaluation , observation proposal based CLIP models , and NLP for improved language in astronomy , the Multimodal Universe does not currently contain any text-based captions or corpora. An additional limitation is that the Multimodal Universe does not guarantee fully cross-matched samples (i.e. all modalities for each source). This is a consequence of observing strategies and science targets of the respective surveys. However, we note that the current outlook of astronomy surveys is towards 'all sky surveys'. As such, we expect that any future version of the Multimodal Universe will necessarily contain more cross-matched samples than are currently available.

## 6 Availability and Maintenance

The dataset is hosted in full at the Flatiron Institute and accessible both through HTTPS2 or through GLOBUS3 while associated resources and all source code necessary to reproduce the dataset compilation are hosted on GitHub. In addition, a preview of each dataset is hosted on the Hugging Face Hub in parquet format to allow for online streaming and easy prototyping. To ensure the continued evolution and relevance of Multimodal Universe, the project is organized as an open and collaborative effort. Contributions from researchers are actively encouraged, with a well-defined process for submitting new data, benchmarks, and improvements via GitHub. A dedicated team of maintainers oversees the integration of these contributions, ensuring that the dataset remains up-to-date and continues to meet the needs of the scientific and machine learning communities. This framework will enable regular updates to incorporate new data from ongoing and future astronomical surveys, as well as periodic evaluations and improvements based on feedback from the research community. Additionally, our open collaboration hosts forums via GitHub and a dedicated slack to engage with both ML and astrophysics communities, and help the Multimodal Universe adapt and evolve to the respective dynamic research environments.

## Contributors

The contribution categories listed below are broadly indicative of the ways in which authors contributed to the project, although we note these categories are not exhaustive.

Coordination Team:Mich Bowles, Marc Huertas-Company, Francois Lanusse, Liam H. Parker, Helen Qu, Michael J. Smith.

Datasets:Erini Angeloudi, Jeroen Audenaert, Micah Bowles, Benjamin M. Boyd, David Chemaly, Brian Cherinka, Aaron Do, Matthew Grayling, Erin E. Hayes, Tom Hehir, Marc Huertas-Company, Kartheik G. Iyer, Maja Jablonska, Francois Lanusse, Henry W. Leung, Kaisey Mandel, Juan Rafael Martinez-Galarza, Peter Melchior, Liam H. Parker, Helen Qu, Jeff Shen, Michael J. Smith, Connor Stone, Mike Walmsley, John F. Wu.

Baselines:Eirini Angeloudi, Jeroen Audenaert, Micah Bowles, Benjamin M. Boyd, David Chemaly, Aaron Do, Matthew Grayling, Erin Hayes, Tom Hehir, Marc Huertas-Company, Kaisey Mandel, Liam H. Parker, Helen Qu, Michael J. Smith.

Overall/Infrastructure:Mich Bowles, Ioana Ciuca, Miles Cranmer, Shirley Ho, Marc Huertas-Company, Francois Lanusse, Lucas Meyer, Liam H. Parker, Michael J. Smith, Mike Walmsley.