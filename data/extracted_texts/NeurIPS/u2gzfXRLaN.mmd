# Transformation-Invariant Learning and

Theoretical Guarantees for OOD Generalization

 Omar Montasser

Yale University

omar.montasser@yale.edu

&Han Shao

Harvard University

han@ttic.edu

&Emmanuel Abbe

EPFL and Apple

emmanuel.abbe@epfl.ch

###### Abstract

Learning with identical train and test distributions has been extensively investigated both practically and theoretically. Much remains to be understood, however, in statistical learning under distribution shifts. This paper focuses on a distribution shift setting where train and test distributions can be related by classes of (data) transformation maps. We initiate a theoretical study for this framework, investigating learning scenarios where the target class of transformations is either known or unknown. We establish learning rules and algorithmic reductions to Empirical Risk Minimization (ERM), accompanied with learning guarantees. We obtain upper bounds on the sample complexity in terms of the VC dimension of the class composing predictors with transformations, which we show in many cases is not much larger than the VC dimension of the class of predictors. We highlight that the learning rules we derive offer a game-theoretic viewpoint on distribution shift: a learner searching for predictors and an adversary searching for transformation maps to respectively minimize and maximize the worst-case loss.

## 1 Introduction

It is desirable to train machine learning predictors that are robust to distribution shifts. In particular when data distributions vary based on the environment, or when part of the domain is not sampled at training such as in reasoning tasks. How can we train predictors that generalize beyond the distribution from which the training examples are drawn from? A common challenge that arises when tackling out-of-distribution generalization is capturing the structure of distribution shifts. A common approach is to mathematically describe such shifts through distance or divergence measures, as in prior work on domain adaptation theory (e.g., Redko et al., 2020) and distributionally robust optimization (e.g., Duchi and Namkoong, 2021).

In this paper, we put forward a new formulation for out-of-distribution generalization. Our formulation offers a conceptually different perspective from prior work, where we describe the structure of distribution shifts through data transformations. We consider an unknown distribution \(\) over \(\) which can be thought of as the "training" or "source" distribution from which training examples are drawn, and a collection of data transformation maps \(=\{T:\}\) which can be thought of as encoding "target" distribution shifts, hence denoted as \(\{T()\}_{T}\). We consider a covariate shift setting where labels are _not_ altered or changed under transformations \(T\), and we write \(T()\) for notational convenience. Our goal, which will be formalized further shortly, is to learn a single predictor \(\) that performs well _uniformly_ across _all_ distributions \(\{T()\}_{T}\).

We view this formulation as enabling a different way to describe distribution shifts through transformations \(=\{T:\}\). The collection of transformations \(\) can be viewed as either: (a) given to the learning algorithm as part of the problem, or (b) chosen by the learning algorithm.View (a) represents scenarios where the target distribution shifts are known and specified by some downstream application (e.g., learning a classifier that is invariant to image rotations and translations). View (b)represents scenarios where there is uncertainty or there are no pre-specified target distribution shifts and we would like to perform maximally well relative to an expressive collection of transformations. We highlight next several problems of interest that can be captured by this formulation. We refer the reader to Section 7 for a more detailed discussion in the context of prior work.

* Covariate Shift & Domain Adaptation. By Brenier's theorem (Brenier, 1991), when \(=^{d}\), then under mild assumptions, for any source distribution \(P\) over \(\) and target distribution \(Q\) over \(\), there exists a transformation \(T:\) such that \(Q=T(P)\). Thus, by choosing an expressive collection of transformations \(\), we can address arbitrary covariate shifts.
* Transformation-Invariant Learning. In many applications, it is desirable to train predictors that are invariant to transformations or data preprocessing procedures representing different "environments" (e.g., an image classifier deployed in different hospitals, or a self-driving car operating in different cities).
* Representative Sampling. In many applications, there may be challenges in collecting "representative" training data. For instace, in learning Logic or Arithmetic tasks (Abbe et al., 2023), the combinatorial nature of the data makes it not possible to cover well all parts of the domain. E.g., there is always a limit to the length of the problem considered at training, or features may not be homogeneously represented at training (bias towards certain digits etc.). Choosing a suitable collection of transformations \(\) under which the target function is invariant can help to model in such cases.
* Adversarial Attacks. Test-time adversarial attacks such as adversarial patches in vision tasks (Brown et al., 2017; Karmon et al., 2018), attack prompts in large language models (Zou et al., 2023), and "universal attacks" (Moosavi-Dezfooli et al., 2017) can all be viewed as instantiations constructing specific transformations \(\).

**Our Contributions.** Let \(\) be the instance space and \(=\{ 1\}\) the label space. Let \(^{}\) be a hypothesis class, and denote by \(()\) its VC dimension. Consider a collection of transformations \(=\{T:\}\), and some unknown distribution \(\) over \(\). Let \((h,T())=_{(x,y)} [h(T(x)) y]\) be the error of predictor \(h\) on transformed distribution \(T()\).

Given a training sample \(S=\{(x_{1},y_{1}),,(x_{m},y_{m})\}^{m}\), we are interested in learning a predictor \(\) with _uniformly small risk_ across all transformations \(T\). Formally,

\[_{T}(,T())_{}+,_{}:=_{h^{*}}_{T}\{ (h^{*},T())\}.\] (1)

This objective is similar to that considered in prior work on distributionally robust optimization (Duchi and Namkoong, 2021) and multi-distribution learning (Hagthalab et al., 2022). The main difference is that in this work we are describing the collection of "target" distributions \(\{T()\}_{T}\) as transformations of the "source" distribution \(\). This allows us to obtain new upper bounds on the sample complexity of learning under distribution shifts based on the VC dimension of the composition of \(\) with \(\), denoted \(()\) (see Equation (3)). We describe next our results (informally):

1. In Section 2 (Theorem 2.1), we show that, given the knowledge of any hypothesis class \(\) and any collection of transformations \(\), by minimizing the empirical worst case risk, we can solve Objective 1 with sample complexity bounded by \(()\). Furthermore, in Theorem 2.2, we show that the sample complexity of any _proper_ learning rule is bounded from below by \((())\).
2. In Section 3 (Theorem 3.1), we consider a more challenging scenario in which \(\) is unknown. Instead, we are only given an ERM oracle for \(\). We then present a generic algorithmic reduction (Algorithm 1) solving Objective 1 using only an ERM oracle for \(\), when the collection \(\) is finite. This is established by solving a zero-sum game where the \(\)-player runs ERM and the \(\)-player runs Multiplicative Weights (Freund and Schapire, 1997).
3. In Section 4 (Theorem 4.1), we consider situations where we _do not know_ which transformations are relevant (or important) for the learning task at hand, and so we pick an expressive collection \(\) and aim to perform well on as many transformations as possible. We then present a different generic learning rule (Equation (4)) that learns a predictor \(\) achieving low error (say \(\)) on as many target distributions in \(\{T()\}_{T}\) as possible.
4. In Section 5 (Theorems 5.1 & E.1), we extend our learning guarantees to a slightly different objective, Objective 7, that can be favorable to Objective 1 when there is heterogeneity in the noise across different transformations. This is inspired by Agarwal and Zhang (2022) who introduced this objective.

## 2 Minimizing Worst-Case Risk

If we have access to, or know, the hypothesis class \(\) and the collection of transformations \(\), then the most direct and intuitive way of solving Objective 1 is minimizing the empirical worst-case risk. Specifically,

\[*{argmin}_{h}_{T} \{_{i=1}^{m}[h(T(x_{i})) y_{i}] \}.\] (2)

We highlight that this learning rule offers a game-theoretic perspective on distribution shift, where the \(\)-player searches for a predictor \(h\) to minimize the worst-case error while the \(\)-player searches for a transformation \(T\) to maximize the worst-case error. For instance, both predictors \(\) and transformations \(\) can be parameterized by neural network architectures, which is an interesting direction to explore further. We note that similar min-max optimization problems have appeared before in the literature on adversarial examples and generative adversarial networks (e.g., Madry et al., 2018; Goodfellow et al., 2020).

We present next a PAC-style learning guarantee for this learning rule which offers the interpretation that solving the min-max optimization problem in Equation (1) yields a predictor \(\) that generalizes to the collection of transformations \(\). We show that the sample complexity of this learning rule is bounded by the VC dimension of the composition of \(\) with \(\), where

\[:=\{h T:h,T \},(h T)(x)=h(T(x))\;\; x.\] (3)

**Theorem 2.1**.: _For any class \(\), any collection of transformations \(\), any \(,(0,}{{2}})\), any distribution \(\), with probability at least \(1-\) over \(S^{m(,)}\) where \(m(,)=O(( )+(1/)}{^{2}})\),_

\[_{T}(,T(D))_{ }+.\]

Proof.: The proof follows from invoking uniform convergence guarantees with respect to the composition \(\) (see Proposition A.1 in Appendix A) and the definition of \(\) described in Equation (2). Let \(h^{}\) be an a-priori fixed predictor (independent of sample \(S\)) attaining \(_{}=_{h}_{T} (h,T())\) (or \(\)-close to it). By setting \(m(,)=O(( )+(1/)}{^{2}})\) and invoking Proposition A.1, we have the guarantee that with probability at least \(1-\) over \(S^{m(,)}\),

\[( h)( T):| (h,T(S))-(h,T())|.\]

Since \(,h^{}\), the inequality above implies that

\[ T: (,T())(,T(S))+.\] \[ T: (h^{},T(S))(h^{ },T())+.\]

Furthermore, by definition, since \(\) minimizes the empirical objective, it holds that

\[_{T}(,T(S))_{T }(h^{},T(S)).\]

By combining the above, we get

\[_{T}(,T())_{T }(,T(S))+_{T }(h^{},T(S))+_{ }+2.\]

We show next that \(()\) can be much higher than \(()\) and the dependency on \(()\) is tight for all proper learning rules, which includes the learning rule described in Equation (2) and more generally any learning rule that is restricted to outputting a classifier in \(\).

**Theorem 2.2**.: \( k\)_, \(,,\) such that \(()=1\) but \(() k\), and the sample complexity of any proper learning rule \(:()^{*}\) solving Objective 1 is at least \((())\)._A proof is deferred to Appendix C. We remark that the sample complexity cannot be improved by proper learning rules and this leaves open the possibility of improving the sample complexity with _improper_ learning rules. There are many examples in the literature where there are sample complexity gaps between proper and improper learning (e.g., Angluin, 1987; Daniy and Shalev-Shwartz, 2014; Foster et al., 2018; Montasser et al., 2019; Alon et al., 2021). In particular, it appears that we encounter in this work a phenomena similar to what occurs in adversarially robust learning (Montasser et al., 2019). Nonetheless, even at the expense of (potentially) higher sample complexity, we believe that there is value in the simplicity of the learning rule described in Equation (2), and exploring ways of implementing it is an interesting direction beyond the scope of this work.

### Examples and Instantiations of Guarantees

To demonstrate the utility of our generic result in Theorem 2.1, we discuss next a few general cases where we can bound the VC dimension of \(\) composed with \(\), \(()\). This allows us to obtain new learning guarantees with respect to classes of distribution shifts that are _not_ covered by prior work, to the best of our knowledge.

**Linear Transformations.** Consider \(\) being a (potentially infinite) collection of _linear_ transformations. For example, in vision tasks, this includes many transformations that have been widely studied in practice such as rotations, translations, maskings, adding random noise (or any fixed a-priori arbitrary noise), and their compositions (Engstrom et al., 2019; Hendrycks and Dietterich, 2019).

Interestingly, for a broad range of hypothesis classes \(\), we can show that \(()()\) without incurring any dependence on the complexity of \(\). Specifically, the result applies to any function class \(\) that consists of a linear mapping followed by an arbitrary mapping. This includes feed-forward neural networks with any activation function, and modern neural network architectures (e.g., CNNs, ResNets, Transformers). We find the implication of this bound to be interesting, because it suggests (along with Theorem 2.1) that the learning rule in Equation (2) can generalize to linear transformations with sample complexity that is not greater than the sample complexity of standard PAC learning. We formally present the lemma below, and defer the proof to Appendix B.

**Lemma 2.3**.: _For any collection of linear transformations \(\) and any hypothesis class of the form \(=\{f W:^{d}\ \ W^{k  d} f:^{k}\}\), it holds that \(()()\)._

**Non-Linear Transformations.** Consider \(\) being a (potentially infinite) collection of _non-linear_ transformations parameterized by a feed-forward neural network architecture, where each \(T=W_{L} W_{2} W_{1}\) and \(()=\{0,\}\) is the ReLU activation function. Similarly, consider a hypothesis class \(\) that is parameterized by a (different) feed-forward neural network architecture, where each \(h=_{H}_ {2}_{1}\). Observe that the composition \(\) consists of (deeper) feed-forward neural networks, where \(h T=_{H} _{2}_{1} W_{L}  W_{2} W_{1}\). Thus, we can bound \(()\) by appealing to classical results bounding the VC dimension of feed-forward neural networks. For example, according to Bartlett et al. (2019), it holds that \(() O((H+L)P_{ }(P_{}))\), where \(H+L\) is the depth of the networks in \(\) and \(P_{}\) is the number of parameters of the networks in \(\) (which is \(P_{}+P_{}\)). In this context, Theorem 2.1 and Equation (2) present a new learning guarantee against distribution shifts parameterized by non-linear transformations induced with feed-forward neural networks.

**Transformations on the Boolean hypercube.** The Boolean hypercube has also received attention recently as a case-study for distribution shifts in Logic or Arithmetic tasks(Abbe et al., 2023). We show next that when the instance space \(=\{0,1\}^{d}\), we can bound the VC dimension of \(\) from above by the sum of the VC dimension of \(\) and the VC dimensions of \(\{_{i}\}_{i=1}^{d}\) where each \(_{i}=\{x T(x)_{i}:T\}\) is a function class resulting from restricting transformations \(T:\{0,1\}^{d}\{0,1\}^{d}\) to output only the \(i^{}\) bit. The proof is deferred to Appendix B

**Lemma 2.4**.: _When \(=\{0,1\}^{d}\), for any hypothesis class \(\) and any collection of transformations \(\), \(() O( d)(( )+_{i=1}^{d}(_{i}))\), where each \(_{i}=\{x T(x)_{i}:T\}\)._

In this context, Theorem 2.1 and Equation (2) present a new learning guarantee against arbitrary distribution shifts parameterized by transformations on the Boolean hypercube, where the sample complexity (potentially) grows with the complexity of the transformations as measured by the VCdimension. We note however that this learning guarantee does not address the problem of length generalization, since we restrict to transformations that preserve domain length.

**Adversarial Attacks.** In adversarially robust learning, a test-time attacker is typically modeled as a pertubation function \(: 2^{}\), which specifies for each test-time example \(x\) a set of possible adversarial attacks \((x)\) that the attacker can choose from at test-time (Montasser et al., 2019). The robust risk of a predictor \(\) is then defined as: \(_{(x,y)}[_{z(x)} [(z) y]]\). On the other hand, the framework we consider in this paper can be viewed as restricting a test-time attacker to commit to a set of attacks \(=\{T:\}\) without knowledge of the test-time samples, and the risk of a predictor \(\) is then defined as: \(_{T}_{(x,y)}[(T(x)) y]\). While less general, our framework still captures several interesting adversarial attacks in practice which are constructed before seeing test-time examples, such as adversarial patches in vision tasks (Brown et al., 2017; Karmon et al., 2018) and attack prompts for large language models (Zou et al., 2023) can be represented with linear transformations.

## 3 Unknown Hypothesis Class: Algorithmic Reductions to ERM

Implementing the learning rule in Equation (2) crucially requires knowing the base hypothesis class \(\) and the transformations \(\), which may not be feasible in many scenarios. Moreover, in many applications we only have black-box access to an off-the-shelve supervised learning method such as an ERM for \(\). Hence, in this section, we study the following question:

Can we solve Objective 1 using only an ERM oracle for \(\)?

We prove yes, and we present next generic oracle-efficient reductions solving Objective 1 using only an ERM oracle for \(\). We consider two cases,

**Realizable Case.** When \(_{}=0\), i.e., \( h^{}\) such that \( T:(h^{},T())=0\), there is a simple reduction to solve Objective 1 using a _single call_ to an ERM oracle for \(\). The idea is to inflate the training dataset \(S\) to include all possible transformations \((S)=\{(T(x),y):(x,y) S T\}\) (similar to data augmentation), and then run ERM on \((S)\). Formal guarantee and proof are deferred to Appendix D. It is also possible, via a fairly standard boosting argument, to achieve a similar learning guarantee using multiple ERM calls (specifically, \(O(|(S)|) O((|S|\,|()))\), where each ERM call is on a sample of size \(O(())\). So, we get a tradeoff between the size of a dataset given to ERM on a single call, and the total number of calls to ERM.

**Agnostic Case.**When \(_{}>0\), the simple reduction above no longer works. Specifically, the issue is that running a single ERM on the inflation \((S)\) effectively minimizes average error over transformations \(T\) as opposed to minimizing maximum error over transformations \(T\). So, \(_{}>0\), by definition, implies there is no predictor \(h\) that is consistent (i.e., zero error) on every transformation \(T(S),T\), thus minimizing average error over transformations can be bad.

To overcome this limitation, we present a different reduction (Algorithm 1) that minimizes Objective 1 by solving a zero-sum game where the \(\)-player runs ERM and the \(\)-player runs Multiplicative Weights (Freund and Schapire, 1997). This can be viewed as solving a sequence of weighted-ERM problems (with weights over transformations), where Multiplicative Weights determines the weight of each transformation.

**Theorem 3.1**.: _For any class \(\), collection of transformations \(\), distribution \(\) and any \(,(0,}{{2}})\), with probability at least \(1-\) over \(S^{m(,)}\), where \(m(,) O(( )+(1/)}{^{2}})\), running Algorithm 1 on \(S\) for \(R|}{^{2}}\) rounds produces \(=_{r=1}^{R}h_{r}\) satisfying_

\[ T:_{(x,y) D\\ r\{1,,R\}}[h_{r}(T(x)) y ]_{}+.\]

_Remark 3.2_.: When \(\) is a _finite_ collection of transformations, we can bound \(()\) from above by \(O(()+||)\) using the Sauer-Shelah-Perels Lemma (Sauer, 1972). See Lemma B.1 and proof in Appendix B.

Proof of Theorem 3.1.: Let \(S=\{(x_{1},y_{1}),,(x_{m},y_{m})\}\) be an arbitrary dataset. By setting \(R|}{^{2}}\) and invoking Lemma D.2, which is a helpful lemma (statement and proof in Appendix D) that instantiates the regret guarantee of Multiplicative Weights in our context, we are guaranteed that Algorithm 1 produces a sequence of distributions \(Q_{1},,Q_{R}\) over \(\) that satisfy

\[_{T}\ \ _{r=1}^{R} (h_{r},T(S))_{r=1}^{R}*{} _{T Q_{r}}(h_{r},T(S))+.\]

At each round \(r\), observe that Step 4 in Algorithm 1 draws an i.i.d. sample from a distribution \(P_{r}\) over \(\) that is defined by \(Q_{r}\) over \(\) and \((S)\), and since \(_{}\) is an \((,)\)-agnostic-PAC-learner for \(\), Step 5 guarantees that

\[*{}_{T Q_{r}}(h_{r},T(S)) =*{}_{T Q_{r}}_{i=1}^{m} \{h_{r}(T(x_{i})) y_{i}\}_{h} *{}_{T Q_{r}}(h,T(S))+ _{h^{}}_{T} (h^{},T(S))+.\]

Combining the above inequalities implies that

\[_{T}\ \ _{r=1}^{R} (h_{r},T(S))_{h^{}}_{T} (h^{},T(S))+.\]

Finally, by appealing to uniform convergence over \(\) (Proposition A.1), with probability at least \(1-\) over \(S^{m}\),

\[_{T}_{r=1}^{R}(h_{r},T()) _{T}_{r=1}^{R}(h_{r},T(S))+_{h^{} }_{T}(h^{},T(S))++\] \[_{h^{}}_{T} (h^{},T())++2=_{}+.\]

On finiteness of \(\).We argue informally that requiring \(\) to be finite is necessary in general when only an \(\) oracle for \(\) is allowed. For example, consider a distribution supported on a single point \((x,-)\) on the real line where \(x=5\), and transformations \(T_{i}(x)=x+i\) for all \(i 1\) induced by some collection \(\{T_{i}\}_{i}\). Calling ERM on a finite subset of these transformations \(T_{i_{1}},,T_{i_{k}}\) could return a predictor that labels \(x,x+i_{1},x+i_{2},,x+i_{k}\) with a label \(-\) and labels \(x+i_{k}+1,\) with \(+\) (e.g., if \(\) is thresholds) which fails to satisfy Objective 1. But it would be interesting to explore additional structural conditions that would enable handling infinite \(\), and leave this to future work.

## 4 Unknown Invariant Transformations

When we have a large collection of transformations \(\) and there is uncertainty about which transformations \(T\) under-which we can simultaneously achieve low error using a base class \(\), the learning rule presented in Section 2 (Equation 1) can perform badly. We illustrate this with the following concrete example:

_Example 1_.: Consider a class \(=\{h_{1},h_{2},h_{3}\}\), a collection of transformations \(=\{T_{1},T_{2},T_{3}\}\), and a distribution \(\) with risks (errors) as reported in the table.

Then, solving Objective 1 may return predictor \(h_{3}\) where \( T:(h_{3},T())=49\%\), since we only need to compete with the worst-case risk \(_{}=49\%\). However, predictor \(h_{1}\) is arguably better since it achieves a low error of \(1\%\) on at least two out of the three transformations.

To address this limitation, we switch to a different learning goal--achieving low error under as many transformations as possible. We present next a different generic learning rule for any class \(\) and any collection of transformations \(\), that enjoys a different guarantee from the learning rule presented in Section 2. In particular, it can be thought of as greedy since it maximizes the number of transformations under which low empirical error is possible, but also conservative since it ignores transformations under which low empirical error is not possible. Specifically, given a training dataset \(S\), the learning rule searches for a predictor \(\) that achieves low empirical error on as many transformations \(T\) as possible, say \((,T(S))\).

\[*{argmax}_{h}_{T} [(h,T(S))].\] (4)

Another way of thinking about this learning rule is that it provides us with more flexibility in choosing the collection of transformations \(\), since the learning rule is not stringent on achieving low error on all transformations but instead attempts to achieve low error on as many transformations as allowed by the base class \(\). Thus, this is useful in situations where there is uncertainty in choosing the collection of transformations. We present next the formal learning guarantee for this learning rule,

**Theorem 4.1**.: _For any class \(\), any countable collection of transformations \(\), any distribution \(\) and any \(,(0,1)\), with probability at least \(1-\) over \(S^{m}\), where \(m=O(()(1/) +(1/)}{})\), then_

\[_{}[(,T( )) 3]_{h^{}}_{T }[(h^{},T()) ].\]

_Furthermore, it holds that \( T\): \((,T())(,T(S ))+(,T(S))}+\)._

_Remark 4.2_.: We can generalize the result above to any prior over the transformations \(\), encoded as a weight function \(w:\) such that \(_{T}w(T) 1\). By maximizing the weighted version of Equation (4) according to \(w\), it holds that \(_{T}w(T)_{[(,T( ))/3]}_{h^{}}_{ T}w(T)_{[(h^{},T( )) 3]}\) with high probability.

Proof.: The proof follows from the definition of \(\) and using optimistic generalization bounds (Proposition A.2). By setting \(m(,)=O(()(1/)+(1/)}{})\) and invoking Proposition A.2, we have the guarantee that with probability at least \(1-\) over \(S^{m(,)}\), \(( h)( T)\):

\[(h,T())(h,T(S))+(h,T(S))}+,\] (5)

\[(h,T(S))(h,T())+(h,T())}+.\] (6)

Since \(\), inequality (4) above implies that \( T\) if \((,T(S))\) then \((,T()) 3\). Thus,

\[_{T}[(,T( )) 3]_{T} [(,T(S))].\]

Furthermore, by definition, since \(\) maximizes the empirical objective, it holds that

\[_{T}[(,T(S)) ]_{T}[(h^{},T(S))].\]Since \(h^{}\), inequality (5) above implies that \( T\) if \((h^{},T())}{{3}}\) then \((h^{},T(S))\). Thus,

\[_{T}[(h^{},T(S)) ]_{T}[(h^{},T())}{{3}}].\]

By combining the above three inequalities,

\[_{T}[(,T()) 3]_{T}[ (h^{},T())}{{3}} ].\]

## 5 Extension to Minimizing Worst-Case Regret

When there is heterogeneity in the noise across the different distributions, Agarwal and Zhang (2022) argue that, in the context of distributionally robust optimization, solving Objective 1 may _not_ be advantageous. Additionally, they introduced a different objective (see Objective 7) which can be favorable to minimize. In this section, we extend our guarantees for transformation-invariant learning to this new objective which we describe next.

For each \(T\), let \(_{T}=_{h^{}_{T}}(h^{ }_{T},T())\) be the smallest achievable error on transformed distribution \(T()\) with hypothesis class \(\). Given a training sample \(S=\{(x_{1},y_{1}),,(x_{m},y_{m})\}^{m}\), we would like to learn a predictor \(:\) with _uniformly small regret_ across all transformations \(T\) in \(\),

\[_{T}(,T())- _{T}_{h^{}}_{T}\{ (h^{},T())-_{T}\}+.\] (7)

We illustrate with a concrete example below how solving Objective 7 can be favorable to Objective 1.

_Example 2_ (Risk vs. Regret).: Consider a class \(=\{h_{1},h_{2}\}\), a collection of transformations \(=\{T_{1},T_{2},T_{3},T_{4}\}\), and a distribution \(\) such that with errors as reported in the table:

    & \(T_{1}()\) & \(T_{2}()\) & \(T_{3}()\) & \(T_{4}()\) \\  \(h_{1}\) & \(\) & \(}{{8}}\) & \(}{{4}}\) & \(}{{2}}\) \\ \(h_{2}\) & \(}{{2}}\) & \(}{{2}}\) & \(}{{2}}\) & \(}{{2}}\) & \(}{{2}}\) \\   

Thus, solving Objective 1 may return predictor \(h_{2}\) where \( T:(h_{2},T())=} {{2}}\), since we only need to compete with the worst-case risk \(_{}=\). However, solving Objective 7 will return predictor \(h_{1}\) where \( T::(h_{1},T())_{T}\).

More generally, as highlighted by Agarwal and Zhang (2022), whenever there exists \(h^{}\) satisfying \( T:(h^{},T())-_{T}\), solving Objective 7 is favorable. We present next a generic learning rule solving Objective 7 for any hypothesis class \(\) and any collection of transformations \(\),

\[*{argmin}_{h}_{T} \{_{i=1}^{m}[h(T(x_{i})) y_{i}]- }_{T}\}.\] (8)

We present next a PAC-style learning guarantee for this learning rule with sample complexity bounded by the VC dimension of the composition of \(\) with \(\). The proof is deferred to Appendix E.

**Theorem 5.1**.: _For any class \(\), any collection of transformations \(\), any \(,(0,}{{2}})\), any distribution \(\), with probability at least \(1-\) over \(S^{m(,)}\) where \(m(,)=O((_{0} )+(1/)}{^{2}})\),_

\[_{T}\{(,T(D))-_{T} \}_{h^{}}_{T}\{ (h^{},T())-_{T}\}+.\]

**Algorithmic Reduction to ERM.** Using ideas and techniques similar to those used in Section 3, we develop a generic oracle-efficient reduction solving Objective 7 using only an ERM oracle for \(\). Theorem, proof, and algorithm are deferred to Appendix E.

## 6 Basic Experiment

We present results for a basic experiment on learning Boolean functions on the hypercube \(\{ 1\}^{d}\). We consider a uniform distribution \(D\) over \(\{ 1\}^{d}\) and two target functions: (1) \(f_{1}^{}(x)=_{i=1}^{d}x_{i}\), the parity function, and (2) \(f_{2}^{}(x)=(_{j=0}^{2}(_{i=1}^{d/3}x_{j(d/3)+i}))\), a majority-of-subparities function. We consider transformations \(_{1}\), \(_{2}\) under which \(f_{1}^{}\), \(f_{2}^{}\) are invariant, respectively (see Section 2). Since \(D\) is uniform, note that for any \(\): \(_{T}(,T(D_{f^{}}))= (,D_{f^{}})\).

**Algorithms.** We use a two-layer feed-forward neural network architecture with \(512\) hidden units as our hypothesis class \(\). We use the squared loss and consider two training algorithms. First, the baseline is running standard mini-batch SGD on training examples. Second, as a heuristic to implement Equation (2), we run mini-batch SGD on training examples and permutations of them. Specifically, in each step we replace correctly classified training examples in a mini-batch with random permutations of them (drawn from \(\)), and then perform an SGD update on this modified mini-batch. We set the mini-batch size to \(1\) and the learning rate to \(0.01\). Results are averaged over 5 runs with different seeds and are reported in Figure 1. We ran experiments on freely available Google CoLab T4 GPUs, and used Python and PyTorch to implement code.

## 7 Related Work and Discussion

**Covariate Shift, Domain Adaptation, Transfer Learning.** There is substantial literature studying theoretical guarantees for learning when there is a "source" distribution \(P\) and a "target" distribution \(Q\) (see e.g., survey by Redko et al., 2020, Quinonero-Candela et al., 2008). Many of these works explore structural relationships between \(P\) and \(Q\) using various divergence measures (e.g., total variation distance or KL divergence), sometimes incorporating the structure of the hypothesis class \(\)(e.g., Ben-David et al., 2010, Hanneke and Kpotufe, 2019). Sometimes access to unlabeled (or few labeled) samples from \(Q\) is assumed. Our work differs from this line of work by expressing the structural relationship between \(P\) and \(Q\) in terms of a transformation \(T\) where \(Q=T(P)\).

**Distributionally Robust Optimization.** With roots in optimization literature (see e.g., Ben-Tal et al., 2009, Shapiro, 2017), this framework has been further studied recently in the machine learning literature (see e.g., Duchi and Namkoong, 2021). The goal is to learn a predictor \(\) that minimizes the worst-case error \(_{Q}(,Q)\), where \(\) is a collection of distributions. Most prior work adopting this framework has focused on distributions \(\) that are close to a "source" distribution \(\) in some divergence measure (e.g., \(f\)-divergences Namkoong and Duchi, 2016). Instead of relying on divergence measures, our work describes the collection \(\) through data transformations \(\) of \(\): \(\{T(D)\}_{T}\) which may be operationally simpler.

**Multi-Distribution Learning.** This line of work focuses on the setting where there are \(k\) arbitrary distributions \(_{1},,_{k}\) to be learned uniformly well, where sample access to each distribution \(_{i}\) is provided (see e.g., Haghtalab et al., 2022). In contrast, our setting involves access to a

Figure 1: Left plot is for learning \(f_{1}^{}\), the full parity function in dimension \(18\), with a train set size of \(7000\). Transformations are sampled from \(_{1}\): the set of _all_ permutations. Right plot is for learning \(f_{2}^{}\), a majority-of-subparities function in dimension \(21\), with a train set size of \(5000\). Transformations are sampled from \(_{2}\): permutations on which \(f_{2}^{}\) is invariant. In each case, the test set size is 1000.

single distribution \(\) and transformations \(T_{1},,T_{k}\), that together describe the target distributions: \(T_{1}(),,T_{k}()\). From a sample complexity standpoint, multi-distribution learning requires sample complexity scaling linearly in \(k\) while in our case it is possible to learn with sample complexity scaling logarithmically in \(k\) (see Theorem 3.1 and Lemma B.1). The lower sample complexity in our approach is primarily due to the assumption that the transformations \(T_{1},,T_{k}\) are known in advance, allowing the learner to generate \(k\) samples from a single draw of \(\). In contrast, in multi-distribution learning, the learner pays for \(k\) samples in order to see one sample from each of \(_{1},,_{k}\). Therefore, while the sample complexity is lower in our setting, this advantage arises from the additional information/structure provided rather than an inherent improvement over the more general setting of multi-distribution learning. From an algorithmic standpoint, our reduction algorithms employ similar techniques based on regret minimization and solving zero-sum games (Freund and Schapire, 1997).

**Invariant Risk Minimization (IRM).** This is another formulation addressing domain generalization or learning a predictor that performs well across different environments (Arjovsky et al., 2019). One main difference from our work is that in the IRM framework training examples from different environments are observed and no explicit description of the transformations is provided. Furthermore, to argue about generalization on environments unseen during training, a structural causal model is considered. Recent works have highlighted some drawbacks of IRM (Rosenfeld et al., 2021; Kamath et al., 2021). For example, how in some cases ERM outperforms IRM on out-of-distribution generalization, and the sensitivity of IRM to finite empirical samples vs. infinite population samples.

**Data Augmentation.** A commonly used technique in learning under invariant transformations is data augmentation, which involves adding transformed data into the training set and training a model with the augmented data. Theoretical guarantees of data augmentation have received significant attention recently (see e.g., Dao et al., 2019; Chen et al., 2020; Lyle et al., 2020; Shao et al., 2022; Shen et al., 2022). In this line of research, it is common to assume that the transformations form a group, and the learning goal is to achieve good performance under the "source" distribution by leveraging knowledge of the invariant transformations structure. In contrast, our work does not make the group assumption over transformations, and our goal is to learn a model with low loss under all possible "target" distributions parameterized by transformations of the "source" distribution.

**Multi-Task Learning.**Ben-David and Borbely (2008) studied conditions underwhich a set of transformations \(\) can help with multi-task learning, assuming that \(\) forms a group and that \(\) is closed under \(\). Our work does not make such assumptions, and studies a different learning objective.