# CSMeD: Bridging the Dataset Gap in Automated Citation Screening for Systematic Literature Reviews

Wojciech Kusa\({}^{1}\)

Corresponding author: wojciech.kusa@tuwien.ac.at

Oscar E. Mendoza\({}^{2}\)

Matthias Samwald\({}^{3}\)

Petr Knoth\({}^{4}\)

Allan Hanbury\({}^{1}\)

\({}^{1}\)TU Wien \({}^{2}\)University Milano-Bicocca \({}^{3}\)Medical University of Vienna \({}^{4}\)The Open University

###### Abstract

Systematic literature reviews (SLRs) play an essential role in summarising, synthesising and validating scientific evidence. In recent years, there has been a growing interest in using machine learning techniques to automate the identification of relevant studies for SLRs. However, the lack of standardised evaluation datasets makes comparing the performance of such automated literature screening systems difficult. In this paper, we analyse the citation screening evaluation datasets, revealing that many of the available datasets are either too small, suffer from data leakage or have limited applicability to systems treating automated literature screening as a classification task, as opposed to, for example, a retrieval or question-answering task. To address these challenges, we introduce CSMeD, a meta-dataset consolidating nine publicly released collections, providing unified access to 325 SLRs from the fields of medicine and computer science. CSMeD serves as a comprehensive resource for training and evaluating the performance of automated citation screening models. Additionally, we introduce CSMeD-ft, a new dataset designed explicitly for evaluating the full text publication screening task. To demonstrate the utility of CSMeD, we conduct experiments and establish baselines on new datasets.

## 1 Introduction

_Systematic literature reviews_ (_SLRs_, or meta-reviews) are a critical tool in scientific research, used for synthesising and summarising evidence from multiple studies. The SLR process involves several stages, including _citation screening_ (_CS_, or selection of primary studies) which is, in itself, a time-consuming step [58; 10]. CS involves identifying studies relevant to the SLR based on a set of, often complex, inclusion and exclusion criteria (e.g., the study must be examining the efficacy of Drug X on Condition Y).

In recent years, there has been an increasing interest in automating the SLR process [80; 60; 56; 22; 2], with works often focusing on improving the CS step by (a) using machine learning (ML) [44; 45], (b) natural language processing (NLP) [27; 36; 79], and (c) information retrieval (IR) [70; 90] techniques. Automated CS systems have the potential to significantly reduce the time and resources required for this critical step, thereby speeding up the SLRs production .

The development of standards provides invaluable resources for evaluating and comparing different models. Benchmarks, such as BEIR , GLUE  or BLURB  have shown improvements in reproducibility and progress tracking of machine learning models in various domains. Unfortunately, in the context of SLR automation, the absence of standard benchmarks and evaluation methodologies still hampers progress and inhibits the development of reliable and effective solutions.

With the fast-evolving landscape of machine learning, identifying state-of-the-art performance has become especially challenging and inefficient in the context of CS. The notorious proliferation of small custom CS datasets and single-usage evaluation approaches further exacerbates this issue.

We show that current CS datasets exhibit several shortcomings that hinder their applicability for comprehensive and standardised evaluations. These datasets are poorly documented, with most lacking datasheets, clear licenses and terms of use. In addition, the limited applicability of older datasets arises from their small size and lack of crucial metadata, restricting their use to classification tasks. Finally, data leakage and dataset overlap is another issue, with some SLRs present in multiple collections.

To address these limitations, we present **CSMeD** (**C**itation **S**creening **Me**ta-**D**ataset), a comprehensive collection of CS datasets that can be used to benchmark and evaluate automated screening systems. Our collection builds upon nine existing datasets, and a new dataset for evaluating the full text classification task, counting 325 SLRs from the fields of medicine and computer science. Thanks to the data harmonisation, our new collection can mitigate the issues of lack of canonical splits, limited applicability, and dataset overlap. Our contributions are as follows:

1. We create CSMeD, a meta-collection of nine datasets comprising 325 SLRs. CSMeD is built upon BigBio and can be used to evaluate and benchmark automated CS systems. We also provide a comprehensive summary of existing citation screening datasets.
2. We extend CSMeD with additional metadata after analysing issues on the existing collections and previous evaluation frameworks. Our extended dataset can be used to evaluate CS as question answering or textual pairs classification tasks.
3. Using new metadata, we introduce CSMeD-ft, a new dataset for the task of full text screening. To the best of our knowledge, this is the first dataset designed explicitly on screening long documents in SLR. This dataset can be used for the evaluation of the inference capabilities based on a very long context (4,000+ words).

The remainder of the paper is structured as follows. In Section 2, we define the task of citation screening for systematic literature reviews. Section 3 provides an overview of related work in SLR automation and available benchmarks. Section 4 describes the CSMeD meta-dataset in detail, including its creation, analysis and extension. In Section 5, we introduce the full text screening dataset together with baseline results on this dataset, and in Section 6, we discuss the implications of our work and potential extensions.

## 2 Task formulation

We start by introducing the task of citation screening for SLRs and presenting the notation used for its formulation. An SLR is characterised by various attributes, including the title, abstract, research question \(\), and eligibility criteria \(\). We refer to all these attributes as the SLR protocol. Eligibility criteria comprise a set of rules and conditions that a document must meet for inclusion in the SLR. Given a large pool of documents denoted as \(\), the main goal of automated citation screening is to assist researchers in identifying relevant publications for inclusion in an SLR. Each document \(d\) has attributes such as its title, abstract, main content, authors, and publication year. The task of CS for SLRs can be formally defined as follows:

**Definition 2.1** (Cs).: Given a set of documents \(\) and a set of eligibility criteria \(\), the task of CS for SLR is to determine for each document \(d\) whether it satisfies the criteria \(\). This decision can be represented as a binary label \(y_{d}\{0,1\}\), where \(y_{d}=1\) if document \(d\) satisfies the criteria \(\), and \(y_{d}=0\) otherwise.

It is important to note that the manual CS is conducted in two steps, as shown in Figure 1: title and abstract screening and full text screening. In the first step, the relevance of each document is evaluated based on its title and abstract, while in the second step, a more thorough assessment is performed by examining the full text of the document.

Document retrievalThe initial step involves document retrieval, which aims to generate a set of potentially relevant documents \(^{}\) given \(\). This step commonly involves querying bibliographic databases with specific keywords and Boolean expressions. We can formulate this step as a retrieval function \(r\), such that \(r(,)=^{}\). However, the retrieved set \(^{}\) may contain a large number of false positives (irrelevant documents).

Binary classification for relevance predictionFollowing document retrieval, the primary task is to assess the relevance of each document in the set \(^{}\) concerning the eligibility criteria \(\). This is conducted in two stages, differing in which attributes of documents are considered (titles and abstracts _vs._ full texts). We treat this as a binary classification problem, where each document \(d^{}\) is assigned a binary label \(y_{d}\{0,1\}\) to indicate its relevance (\(y_{d}=1\)) or irrelevance (\(y_{d}=0\)) to the SLR per the criteria \(\).

Question answering for relevanceAn alternative formulation of the citation screening task is to frame it as a question-answering problem. In this approach, we transform the eligibility criteria \(\) into a set of questions \(=\{q_{1},,q_{|C|}\}\), where each question \(q_{k}\) corresponds to a specific criterion in \(\). For each document \(d^{}\), we obtain a set of predicted answers \(^{d}=\{^{d}_{k}|(q_{k},^{d}_{k})\}\), where \((q_{k},^{d}_{k})\) denotes that the document \(d\) should meet the criterion expressed by the question \(q_{k}\). The final relevance label \(_{d}\) of a document \(d\) can be determined by aggregating the predicted answers \(^{d}\) using a logical combination function, such as the logical AND operation.

This question-answering formulation offers a more fine-grained assessment of a document's relevance concerning various aspects of the eligibility criteria \(\). Other similar formulations of the CS task include document ranking or natural language inference (NLI).

## 3 Related work

We first motivate the work by providing context on the importance of SLRs and then focus on reviewing citation screening automation methods. Finally, we outline limitations of existing CS datasets.

### Systematic literature reviews

SLRs are particularly important in the medical domain . The Cochrane Collaboration,2 the largest organisation responsible for creating SLRs in medicine, has created the foundations of Evidence-Based Medicine . There are more than 220,000 records published between 2000 and 2022 tagged as SLRs in PubMed3 meaning that, on average, there were 10,000 SLRs published per year.

As SLRs focus on reproducibility and finding all relevant evidence about a given topic, the traditional framework involves tasks mainly done manually. It includes steps like defining the search strategy (designing complex Boolean queries) or the screening of every document by at least two reviewers, resulting in an average production time of more than one year .

Previous research focused on evaluating automation capabilities for several steps of the traditional framework, such as citation screening (CS) [11; 27; 75], search query (re-)formulation [70; 72], data extraction , SLR summarisation  or generation of reviews based on the title .

Figure 1: Illustration of the citation screening process, separated into two tasks (1) title and abstract screening and (2) full text screening. Tasks are represented as a specific example of question-answering when a single question asks for a fullfilment of all eligibility criteria \(\) at once.

### Citation screening automation

As described in Section 2, CS can be seen as a binary classification problem. However, due to a large number of retrieved studies, the significant class imbalance, and the need to identify _nearly all_ relevant documents, this task is inherently complex. Screening automation is a general term for various approaches aimed at reducing workload during the CS stage . These approaches can be classified as either screening reduction, which involves using classification or ranking algorithms to automatically exclude non-relevant publications or screening prioritisation, which focuses on ranking relevant records earlier in the screening process . Automated screening systems leverage techniques from NLP, ML, IR, and statistics, all with the common objective of reducing manual screening time. The disparity in strategies from different fields hinders direct comparison and benchmarking. Next, we discuss some points of disagreement.

NLP approaches typically focus on the level of individual SLRs, treating each review as an independent dataset; whereas IR approaches would consider a set of reviews as a collection, the topics of the reviews analogous to queries, and report aggregated evaluation. Moreover, different publications across various venues adopt diverse evaluation measures, making even more complex the assessment of similar, if not identical, tasks. Evaluation of automatic approaches traditionally relies on binary relevance ratings, very often obtained from the title and abstract screening . When the screening problem is treated as a ranking task, such as screening prioritization stopping prediction; the performance is measured in terms of rank-based metrics and metrics at a fixed cut-off, such as \(nDCG@n\), \(Precision@n\), and _last relevant found_. On the other hand, when the screening problem is treated as a classification task, the performance in this case is measured based on the confusion matrix and the notions of Precision and Recall are commonly used . One challenge arising from these two distinct approaches is the difficulty in going beyond simple effectiveness measures and comparing the real-world savings for users. Further details on datasets and evaluation approaches can be found in a comprehensive review in the Appendix A.

### Limitations of existing datasets

Through our review (see Appendix A), we identified twelve CS datasets reported in former research papers, of which ten have been publicly released. During this analysis, we identified several shortcomings; some are also prevalent in other machine learning problems. Below, we summarise our findings, highlighting the key issues.

Poor documentationOne major concern with previous datasets is the lack of sufficient documentation. None of the datasets we examined implement a datasheet , which is an essential tool for ensuring transparency and reproducibility. Additionally, seven datasets do not provide clear licenses or terms of use. An inconsistency was also found for one of the datasets  in terms of the number of the available content: the paper states 93 SLRs, but we found a list of 176 reviews on the corresponding GitHub repository.

Limited applicabilityPrevious datasets are often small and lack crucial metadata like SLR research question or eligibility criteria, limiting their use to only evaluation of classification tasks. Older datasets typically provide only the title of the review, which limits their applicability for the comprehensive evaluation of neural language understanding models. The most widely used dataset to date  was released in 2006. As ML and NLP techniques continue to advance rapidly, it is crucial to have up-to-date datasets that reflect the complexities and nuances of the current research landscape.

Lack of canonical splitsAnother significant challenge of previous datasets is the absence of canonical train-test splits. Depending on the field of research, practices may vary. As discussed before, in the ML and NLP domains, the prevailing practice is to use inter-review splits, where each review is treated as an individual dataset, and a set of citations is selected for training and testing. Conversely, IR publications often report intra-review splits, treating each review as a "topic" or query, and averaging the results across multiple queries.

In this sense, only the three TAR4 datasets contain pre-defined canonical splits, yet only at the intra-review level. For three other datasets , previous works have demonstrated significantvariability in model evaluation based on the selection of cross-validation splits, particularly for the smallest datasets that contain a limited number of relevant documents [79; 38]. The lack of standardised splits, especially in collections with fewer SLRs, makes it challenging to compare different approaches and hinders the fair evaluation of models' performance.

Dataset overlapWe also evaluated overlapping throughout the previous datasets and discovered that at least 11 SLRs were present in multiple collections [71; 30; 31; 32]. Additionaly, the TAR 2019 dataset contains three SLRs that are present both in its training and test splits, accounting for approximately 6% of the test partition . While this overlap is not a significant concern when evaluating unsupervised methods like BM25 , it poses a potential threat to conducting fair comparisons with large language models (LLMs). Machine learning models, and especially LLMs, have the capability to memorise their training data, making it critical to address dataset overlap to ensure unbiased evaluations  (see Appendix E for a detailed analysis of the overlapping in previous datasets).

Lack of common evaluationAnother notable deficiency among the previous datasets is the absence of a common set of evaluation measures. Only the three TAR datasets provide scripts for evaluating submissions. For example, the most widely used dataset by Cohen et al.  was evaluated using several disparate evaluation measures such as \(WSS\), \(AUC\) or \(Precision@r\%\). However, recent research has exposed limitations and problems with both \(WSS\) and \(AUC\) as metrics for this task .

Availability in biomedical benchmarksRecent efforts have focused on creating larger collections of more diverse datasets to evaluate the performance of biomedical NLP models. These efforts include benchmarks like BLUE , HunFlair , BLURB , and BigBio , which provide datasets and tasks for evaluating biomedical language understanding and reasoning. Additionally, there are biomedical datasets geared towards prompt-based learning and evaluation of few and zero-shot classification, such as Super-NaturalInstructions  and BoX . Out of all benchmarks mentioned above, only BoX contains one CS dataset covering five SLRs, however, this dataset is private. Coverage for other SLR tasks is also limited.

To summarise, previous datasets exhibit certain drawbacks that limit their suitability for comprehensive and standardised evaluation. While the TAR 2017-19 collections stand out as the only ones containing canonical splits and a set of evaluation measures, some of their topics overlap with another previous dataset , and we also identified data leakage in the newest TAR 2019 dataset. Consequently, we believe that developing a new collection is necessary to address these issues and establish a robust foundation for evaluation of CS and SLR automation.

## 4 The CSMeD meta-dataset

The recent advancements and paradigm shifts in NLP and ML; with the extensive use of pre-trained models and transfer learning [46; 15], and the more recent prompt-based learning [49; 9]; have significantly transformed the field and enhanced the predictive capabilities of models across various tasks. Inspired by the success of benchmark collections in the field of biomedical NLP, we conducted a thorough review of available datasets and benchmarks to identify the most representative datasets for the task of citation screening, finding that this task is heavily underrepresented. The available datasets still primarily cater to training supervised algorithms, lacking the scale and granularity necessary to evaluate state-of-the-art models. To address these limitations and provide a more comprehensive resource for training and evaluating data-centric methods in SLR automation, we create CSMeD, consolidating nine previously released collections of SLRs. We further extend a subset of SLRs in CSMeD with additional metadata coming from the review protocol.

Our data analysis methodology involved creating visualisations and summary tables based on curated datasets. We analyse dataset statistics like available data splits, licensing information, dataset and reviews size as well as dataset overlap. This allows us to provide both a detailed view of individual reviews and an overview of datasets containing multiple reviews (see Appendix B for further details on visualisations).

### Dataset creation

Currently, nine out of ten public CS datasets we identified have been included in CSMeD, with the remaining one to be included. We provide a summary of the datasets in Table 1, and further details can be found in Appendix A. In total, CSMeD consists of 325 SLRs, making it the largest publicly available collection in this domain and the only one providing access to the datasets via a harmonised API.

To ensure interoperability and facilitate the ease of use, we designed data loaders for the datasets in accordance with the BigBio text classification schema . This choice offers several advantages. BigBio has the largest coverage of biomedical datasets and supports access to the datasets via API. Moreover, it is compatible with popular libraries such as Hugging Face's datasets  and the EleutherAI Language Model Evaluation Harness , thereby reducing the experimental costs.

Taking advantage of the lists of publications that most of the sources of datasets share as PubMed IDs, we extend the BigBio data loaders to enable the download of PubMed articles. Our harmonised data loaders selectively load the PubMed articles that are a part of each dataset. The single exception is the dataset by Hannousse and Yahiouche , which is the only publicly available collection of non-medical SLRs. For this dataset, we extract the content using the SemanticScholar API.5 As a result, CSMeD serves also as the first resource that gathers SLRs from diverse domains.

### Extending metadata

Our goal at this stage is not to create yet another gold standard dataset for SLRs, but rather improve the quality of current data and provide insights into promising avenues for future research. We begin by presenting the possibilities of extending the subset of Cochrane SLRs to experiment with screening beyond supervised classification.

We then categorise CSMeD datasets into two groups: (1) datasets containing Cochrane medical SLRs and (2) datasets comprising other SLRs. This distinction is made because from following the Cochrane protocol, more extensive information on the review is provided. We use the additional data available on reviews websites to extend CSMeD. Among the new information, the one we find most valuable is the eligibility criteria, which no longer limits the data to the evaluation of supervised binary classification but opens its application to question-answering or language inference tasks.

   Source & \# reviews & Domain & Avg. size & Avg. ratio of included & Additional data & Cochrane reviews \\ 
 & 15 & Drug & 1,249 & 7.7\% & — & — \\
 & 3 & Clinical & 3,456 & 7.9\% & — & — \\
 & 5 & Mixed & 19,271 & 4.6\% & — & — \\
 & 7 & Comp. Science & 340 & 11.7\% & B & — \\ 
 & 93/176\({}^{}\) & Clinical & 1,159 & 1.2\% & A & ✓ \\
 & 50 & DTA & 5,339 & 4.4\% & B & ✓ \\
 & 30 & DTA & 7,283 & 4.7\% & B & ✓ \\
 & 49 & Mixed & 2,659 & 8.9\% & B & ✓ \\
 & 25 & Clinical & 4,402 & 0.4\% & C & ✓ \\  Total & 360\({}^{}\) & & 3,471 & 4.4\% & & \\   

Table 1: A list of source citation screening datasets included in the CSMeD. The first four datasets contain non-Cochrane SLRs, whereas the other five are based on Cochrane reviews. ‘Avg. ratio of included’ column presents ratio of included publication from the title and abstract screening stage, ‘Avg. size’ refers to averaged across SLRs document count in the dataset. The ‘Additional data’ column describes if the review contains metadata other than coming from the citation list: (A): Search queries, (B): Review protocol containing review title, abstract and search strategy, (C): Review updates consisting of changes to included papers. ‘DTA’ stands for diagnostic test accuracy reviews. ‘\({}^{}\) – indicates a discrepancy in the number of reviews in the paper versus the GitHub repository. ‘\({}^{}\) – indicates the total count of reviews from all nine datasets before duplicates were removed.

We carefully examine the subset of SLRs produced by Cochrane, aiming to identify potential enhancements and extensions that would help mitigating the existing limitations of previous datasets. Every Cochrane SLR first registers and publishes the protocol containing the review title, abstract, search strategy and the eligibility criteria. This information is all that human experts need to produce the final review, i.e., they first find the relevant studies and then conduct the meta-analysis of their results. As described in Section 2, the screening process can be also modelled as a question-answering, where every publication is compared against the eligibility criteria in order to make the decision about the inclusion,6 similar to the clinical decision support task of matching clinical trials to patients [67; 68].

To expand CSMeD, we searched the Cochrane Library7 for all SLRs from the meta-dataset based on the Cochrane review ID and take their latest open-access version. We extract available information about the review: review title and abstract, eligibility criteria, search strategy and references. Cochrane reports a list of included and excluded publications at the full text screening stage (this can be treated as approximately all included publications during the title and abstract screening stage). Moreover, all excluded publications have a reason for exclusion selected by a reviewer. As the original relevance judgements were limited to publications from the PubMed database, we assign PubMed IDs to these publications. We also define appropriate BigBio data loaders so the task can be seen as question-answering or textual pairs classification task.

Details of the new expanded CSMeD are provided in Table 2. We were not able to find suitable data for all SLRs, hence the expanded CSMeD is smaller than the original meta-dataset. In total, the new expanded dataset consists of 295 unique Cochrane SLRs and 30 non-Cochrane SLRs. The entire set of basic SLRs is designated for training. From the Cochrane reviews, we randomly selected 195 to the training split and the remaining 100 to the development split. We abstain from designating a test split because CSMeD aggregates existing datasets. Given the constraints of these datasets, creating a new, unbiased test collection is recommended.

### Baseline experiment

We evaluate five models in a zero-shot setting: two statistical models BM25 and TF-IDF, and three Transformer-based models. Predictions are run on the CSMeD-dev-cochrane split, and we test four different input representations using the following sections from the SLR protocol: (1) title, (2) abstract, (3) search strategy and (4) eligibility criteria section. We evaluate models using \(TNR@95\%\)[40; 41], \(nP@95\%\), \(Last\ Rel\)[30; 31; 32], \(nDCG@10\), \(MAP\), Recall at rank \(k\) with \(k\) in {10, 50, 100} (\(R@k\)). Detailed experimental setup and expanded results can be found in Appendix F.

Table 3 presents the summary results of best-performing statistical and neural models, while Table 9 in Appendix F contains complete results. The MiniLM neural model consistently outperforms the BM25 variant across nearly all query representations and evaluation measures. For the BM25 model, using the systematic review abstract text as a query representation results in the highest performance in all metrics compared to using the SLR title and criteria sections. Conversely, for

   Dataset name & \#reviews & \#docs & \#included &  Avg. \\ \#docs \\  &  Avg. \\ included \\  & 
 Avg. \\ in document \\  \\  CSMeD-train-basic & 30 & 128,438 & 7,958 & 4,281 & 9.6\% & 229 \\ CSMeD-train-cochrane & 195 & 372,422 & 7,589 & 1,910 & 21.9\% & 180 \\ CSMeD-dev-cochrane & 100 & 229,376 & 4,365 & 2,294 & 20.8\% & 201 \\  CSMeD-all & 325 & 730,236 & 19,912 & 2,247 & 20.5\% & 195 \\   

Table 2: Details of the CSMeD expanded meta-dataset. Column ‘#docs’ refers to the total number of documents included in all SLRs within the dataset, ‘#included’ mentions number of included documents on the title and abstract screening stage and ‘Avg. %included’ the percentage of included publications averaged from all reviews.

the MiniLM model, using eligibility criteria leads to superior scores in TNR@95% and three other evaluation measures. This indicates that larger models are beginning to effectively leverage the criteria information. Notably, the top-performing model in the zero-shot setting, MiniLM using SLR eligibility criteria, achieves \(TNR@95\%\) equal to almost 0.55. This means that, on average, this model can remove more than half of the true negatives when achieving a recall of 95%. Exploring more advanced language models might reveal the potential for further harnessing the criteria information.

## 5 CSMeD-ft: full text classification dataset

In this section, we introduce CSMeD-ft full text screening dataset and present baseline experiments.

### Dataset creation

LLM advancements have enabled processing long text snippets [7; 95; 55; 21]. Commercial tools now support inputs of up to 32k  or even up to 100k tokens . We propose CSMeD-ft, the full text screening dataset to enable research associated with the comprehensive understanding of very long documents, and evaluate such capabilities. We first gather full text versions of publications from CSMeD SLRs, and then create the appropriate setting with canonical splits.

We use SemanticScholar and CORE [35; 34] APIs to find URLs to open-access full text documents. This process successfully finds URLs to, on average, 27% of all included and excluded publications from SLRs. After downloading full text PDFs, we use GROBID  to parse the content of these documents into an xml format.

We establish canonical splits considering the timestamps, such that the newest reviews belong to the test set. Specifically, we select 31 Cochrane reviews published in the last year (between 01/06/2022 and 31/05/2023) to create a test set, another 60 reviews (mentioned in Nussbaumer-Streit et al. ) for the development set, and 176 reviews (listed by Scells et al. ) as the training set. Filtering out reviews with no associated available full text publications results in 148/36/29 reviews in train/dev/test splits.

Details of CSMeD-ft are presented in Table 4. We also release a subset of 50 randomly selected documents from the test set as CSMeD-ft-test-small. At the moment of writing this publication, creating a prompt for LLMs with an input of few thousands tokens is feasible albeit costly,8 See Appendix D for further details on the creation of CSMeD-ft.

   Dataset name & \#reviews & \#docs. & \#included & \% included &  Avg. \#words \\ in document \\  & 
 Avg. \#words \\ in review \\  \\  CSMeD-ft-train & 148 & 2,053 & 904 & 44.0\% & 4,535 & 1,493 \\ CSMeD-ft-dev & 36 & 644 & 202 & 31.4\% & 4,419 & 1,402 \\ CSMeD-ft-test & 29 & 636 & 278 & 43.7\% & 4,957 & 2,318 \\ CSMeD-ft-test-small & 16 & 50 & 22 & 44.0\% & 5,042 & 2,354 \\   

Table 4: Details of the CSMeD-ft dataset. Column ‘#docs’ refers to the total number of documents included in the dataset and ‘#included’ mentions number of included documents on the full text step. CSMeD-ft-test-small is a subset of CSMeD-ft-test.

  
**Model** & **Representation** & **TNR@95\%** & **nP@95\%** & **Last Rel** & **NDCG@10** & **MAP** & **R@10** & **R@50** & **R@100** \\   & Title & 0.469 & 0.142 & 72.2 & 0.438 & 0.388 & 0.349 & 0.623 & 0.704 \\  & Abstract & 0.474 & 0.170 & 63.6 & 0.503 & 0.453 & 0.379 & 0.657 & 0.757 \\  & Search strategy & 0.379 & 0.093 & 72.1 & 0.336 & 0.311 & 0.268 & 0.507 & 0.625 \\  & Criteria & 0.430 & 0.145 & 67.0 & 0.452 & 0.417 & 0.345 & 0.629 & 0.725 \\   & Title & 0.467 & 0.230 & 66.6 & 0.476 & 0.429 & 0.376 & 0.684 & 0.774 \\  & Abstract & 0.516 & **0.265** & 63.8 & **0.556** & 0.482 & **0.420** & **0.692** & 0.777 \\   & Search strategy & 0.429 & 0.181 & 68.6 & 0.400 & 0.372 & 0.328 & 0.614 & 0.699 \\   & Criteria & **0.545** & 0.216 & **58.5** & 0.514 & **0.488** & 0.393 & 0.691 & **0.784** \\   

Table 3: Summary of results on CSMeD-dev-cochrane dataset. **Bold** values indicate best score.

CSMeD-ft could be a proxy for a very long document natural language inference (NLI) task. Popular NLI datasets (SciTail , McTest  or DocNLI ) contain both hypotheses and premises of an average length considerably shorter than 1,000 words; whereas in CSMeD-ft, the premise (review protocol) has an average length of more than 1,000 words, and the hypothesis (publication) contains more than 4,000 words.

### Experiment

We present how CSMeD-ft can be used to evaluate LLMs capabilities in making eligibility decisions on very long documents. We run experiments both on fine-tuning of Transformer models and zero-shot prompting of GPT models.

Model selectionAs the combined input size of systematic review and publication can be big (9,246 mean number of tokens on a training split measured with a GPT-4 tokeniser), we only select models that allow inputs of at least 4k tokens context. We fine-tune the open-domain Longformer and BigBird, and domain-specific models pre-trained on clinical data: Clinical-BigBird and ClinicalLongformer. For zero-shot evaluation, we select GPT-3.5-turbo-0301, GPT-4-8k and GPT-3.5-turbo-16k accessed via OpenAI API. GPT-4-8k and GPT-3.5-turbo-16k are the only models capable of handling more than 4k-input tokens, with context window size of 8k and 16k tokens respectively.

Preprocessing and evaluationFor all models, we concatenate the screening protocol with each publication; we truncate the review description text to half of the available context window (2,000 tokens for 4k models, 4,000 tokens for 8k model and 8,000 tokens for 16k model) and complete the input with a publication.

For GPT models, if a whole publication text does not fit the context window, we run multiple predictions with a sliding window and aggregate the results. In the case of GPT-3.5-turbo-16k model, only for 4 out of 50 documents the model was unable to process the full text of combined review and publication inside one prompt.

We fine-tune the Transformer models on CSMeD-ft-train for four epochs and run evaluation on CSMeD-ft-dev. Due to the budget limitation, for the GPT-4-8k model, we run the evaluation only on CSMeD-ft-test-small (see Appendix G for further details on experimental settings). Finally, we evaluate the models using macro-averaged Precision, Recall and F1-score measures.

ResultsResults of the full text experiment are summarised in Table 5. On CSMeD-ft-test-small, GPT-4-8k strongly outperforms other models. However, this difference is not statistically significant. The GPT-3.5-turbo-16k achieves the highest Precision; this improvement can be attributed to the model's expanded context window and the limitations other GPT-based models have with our simple aggregation rules. However, this might also be caused by overfitting towards the positive class, as this model includes almost twice as many publications as other models. On CSMeD-ft-test set, Clinical-BigBird, significantly outperforms zero-shot GPT-3.5 model and pre-trained models based on the LongFormer architecture.

Interestingly, both BigBird-based models outperform their counterparts using the Longformer architecture. The typical overall tendency to domain-pre-trained models achieving higher scores over their open-domain counterparts is also preserved. We believe that fine-tuning the Transformer models first on larger NLI/QA corpora could help improve the results.

## 6 Discussion

In this paper, we have addressed the challenge of standardised evaluation in CS automation. By revisiting existing screening datasets, we evaluated their suitability as benchmarks in the context of modern ML methods. Our analysis revealed limitations such as small size and data leakage issues.

To overcome these challenges, we introduced CSMeD, a meta-dataset consolidating nine publicly released collections, providing programmatic access to 325 SLRs. CSMeD serves as a comprehensive resource for training and evaluating automated citation screening models and can be used for tasks that involve textual pairs classification, question answering and NLI. Additionally, we included a new dataset within CSMeD for evaluating full text publication classification and conducted initial experiments showing that there is a room for improvement in understanding long contexts.

The focus of CSMeD on providing unified access over a number of diverse citation screening datasets has many benefits. First, the evaluation code can be re-used, making sure that the evaluation is handled properly. Secondly, integration with the BigBio framework enables quick prototyping of prompts. We also improve the documentation for existing datasets and provide a comprehensive data card for CSMeD-ft. Our extended version of CSMeD is also deduplicated. Finally, it is a step towards providing a multi-domain SLR dataset and bridging the gap between IR and NLP research in the domain of screening automation, enabling direct comparisons of the methods.

Limitations and future workWhile we attempted to extract the data protocols as accurately as possible, extraction of data was not possible for all previous reviews. This was primarily due to the changing standards in Cochrane reviews throughout the years. In future work, ideally, direct access to Cochrane metadata would be needed to make sure that all information is covered. Even though the PubMed publications most probably will not change, what can change is the API and scripts necessary to download the data. There exists also the possibility that one of the sources will introduce a restriction on using their data for training and evaluation of machine learning models. We tried to further mitigate this potential issue by selecting open-access SLRs produced by Cochrane. Finally, we acknowledge that using machine assistance for citation selection can raise concerns about research quality, emphasising the vital role of human oversight throughout the process.

Future work will focus on further improving data quality, connecting the output reviews from screening tools like _CRUISE-Screening_, adding datasets covering other domains and different SLR tasks and designing a dataset for a prospective evaluation of review automation which could ensure no data leakage . For the prospective dataset, predictions could be made as soon as the protocol is published, and the gold standard data becomes available when the review is eventually published, albeit with the drawback of a potentially long waiting time for review publication.

## 7 Conclusion

Our paper introduces CSMeD, a meta-dataset that addresses the lack of standardisation in SLR automation. By consolidating datasets and providing a unified access point, CSMeD facilitates the development and evaluation of automated citation screening and full text classification models. We believe it has the potential to advance the field and lead to more robust automated SLR systems. We envision CSMeD as a living, evolving collection, and we invite researchers to contribute to expanding it with SLR datasets from other domains.

    &  &  \\   & \% incl. & Precision & Recall & F1-score & \% incl. & Precision & Recall & F1-score \\  oracle & 44\% & — & — & — & 43.7\% & — & — & — \\ stratified random & 50\% & 0.497 & 0.498 & 0.495 & — & 0.499 & 0.499 & 0.498 \\ ‘include all’ & 100\% & 0.220 & 0.500 & 0.306 & 100\% & 0.219 & 0.500 & 0.304 \\  Longformer  & 40\% & 0.467 & 0.468 & 0.466 & 40.4\% & 0.398 & 0.400 & 0.398 \\ BigBird-roberta-base  & 42\% & 0.572 & 0.571 & 0.572 & 45.1\% & 0.575 & 0.575 & 0.575 \\ Clinical-Longformer  & 36\% & 0.547 & 0.544 & 0.542 & 35.1\% & 0.436 & 0.441 & 0.435 \\ Clinical-BigBird  & 36\% & 0.590 & 0.584 & 0.583 & 32.8\% & **0.623\({}^{}\)** & **0.611\({}^{}\)** & **0.609\({}^{}\)** \\  GPT-3.5-turbo-0301 & 54\% & 0.585 & 0.586 & 0.580 & — & — & — & — \\ GPT-4.8k-0314 & 58\% & 0.674 & **0.672** & **0.660** & — & — & — & — \\ GPT-3.5-turbo-16k & 80\% & **0.712** & 0.638 & 0.576 & 75.9\% & 0.538 & 0.528 & 0.475 \\   

Table 5: Results of the full text screening experiment averaged over documents. The statistical significance was assessed with a McNemar’s t-test (p < 0.05) with Bonferroni correction for multiple testing. _Clinical-BigBird_ on the CSMeD-ft-test split showed statistically significant improvements compared to the _stratified random_ baseline, _Longformer_, _Clinical-Longformer_, and _GPT-3.5-turbo-16k_, indicated by \({}^{}\). Stratified baseline is averaged from 100 different random seeds. ‘% incl.’ describes the percentage of documents predicted as relevant by models.