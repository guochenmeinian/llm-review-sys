# Implicit Variational Inference

for High-Dimensional Posteriors

 Anshuk Uppal

Technical University of Denmark

ansup@dtu.dk

&Kristoffer Stensbo-Smidt

Technical University of Denmark

krss@dtu.dk

&Wouter Boomsma

University of Copenhagen

wb@di.ku.dk

&Jes Frellsen

Technical University of Denmark

jefr@dtu.dk

###### Abstract

In variational inference, the benefits of Bayesian models rely on accurately capturing the true posterior distribution. We propose using neural samplers that specify implicit distributions, which are well-suited for approximating complex multimodal and correlated posteriors in high-dimensional spaces. Our approach advances inference using implicit distributions by introducing novel approximate bounds by locally linearising the neural sampler. This is distinct from existing methods that rely on additional discriminator networks and unstable adversarial objectives. Furthermore, we present a new sampler architecture that, for the first time, enables implicit distributions over tens of millions of latent variables, addressing computational concerns by using differentiable numerical approximations. Our empirical analysis indicates our method is capable of recovering correlations across layers in large Bayesian neural networks, a property that is crucial for a network's performance but notoriously challenging to achieve. To the best of our knowledge, no other method has been shown to accomplish this task for such large models. Through experiments on downstream tasks, we demonstrate that our expressive posteriors outperform state-of-the-art uncertainty quantification methods, validating the effectiveness of our training algorithm and the quality of the learned implicit distribution.

## 1 Introduction

In the Bayesian approach to statistical inference, information about the data is captured by the posterior distribution over a model's latent variables. By marginalising (averaging) over the variables weighted by the posterior, Bayesian inference can provide excellent generalisation and model calibration. This is particularly compelling for complex models, such as modern deep models, where extreme overparametrisation makes overfitting inevitable unless precaution is taken (Wilson et al., 2020).

Exact Bayesian inference is, however, intractable for all but the simplest models. Practitioners, therefore, rely on approximate inference frameworks, which can be broadly categorised as either sampling-based approaches or variational inference (VI, Saul et al., 1996; Peterson, 1987), the latter being the preferred approach for high-dimensional problems. VI seeks a tractable approximation to the intractable true posterior, limiting us to simple approximations like the isotropic Gaussian, which are often far too crude to satisfactorily cover the posterior, either only narrowly covering a single posterior mode or placing excessive mass in low-probability regions of the posterior (Foong et al., 2019, 2020).

To achieve more expressive approximations, _implicit variational inference_ (IVI) defines distributions implicitly by transforming samples from a simple distribution. This provides much greater flexibility at the cost of not having a tractable likelihood for the posterior approximation (Diggle et al., 1984; Mohamed et al., 2016) and thus often requires unstable adversarial objectives based on density ratio estimation, which are not easy to scale to high dimensions (Sugiyama et al., 2012; Huszar, 2017).

The different approaches to posterior approximation can be imagined lying on a spectrum illustrated in Fig. 1. On the left, the mean-field VI approximation is much too confident in regions without data. Sampling-based approaches, here represented by Hamiltonian Monte Carlo (HMC) on the right, converge to the true posterior given enough compute, which, however, is typically prohibitively large. Implicit VI approaches, here represented by our proposed approximation, LIVI (Section 4), and Adversarial Variational Bayes (AVB, Mescheder et al., 2017), which uses a discriminator network for density ratio estimation, hit a tractable middle-ground between the two extremes.

LIVI can produce highly flexible implicit densities approximating the posteriors of complex models through general (non-invertible) stochastic transformations. We present novel, stable training objectives through approximating a lower bound to the model evidence as well as a novel generator architecture, which results in a vastly improved capacity to model expressive distributions, even in millions of dimensions. Specifically, our contributions are:

* We derive a novel approximate lower bound for variational inference using an implicit variational approximation to avoid unstable adversarial objectives.
* To keep computational overhead at large scale in check, we further lower bound the approximate lower bound reducing memory requirements and run times significantly.
* We devise a novel generator architecture capable of specifying highly correlated distributions in high dimensions whilst using a very manageable number of weights and biases, a necessity for modelling high-dimensional distribution.

The paper is organised as follows: Section 2 introduces the background on VI for Bayesian models in general, and VI with implicit distributions (IVI) in particular. Section 3 introduces our proposals to tackle well-known challenges in IVI. Integrating these proposals in Section 4 yields our novel bounds for implicit VI, and Section 5 places our paper in the context of the published literature. In Section 6, we experimentally evaluate our proposed method before summarising and concluding in Section 7.

## 2 Variational inference for Bayesian models

Consider the supervised learning setting, where we have a training set \(=\{(_{i},_{i})\}_{i=1}^{n}\), \(=\{_{i}\}_{i=1}^{n}\) are the covariates (inputs) and \(=\{_{i}\}_{i=1}^{n}\) are the labels (outputs), which we wish to fit using a Bayesian model parametrised by \(^{m}\). That is, assuming a prior \(p()\) over the parameters and a likelihood \(p(\,|\,)\), we wish to compute the posterior \(p(\,|\,)=\,|\,)p()}{p()}\). As

Figure 1: A Bayesian neural network trained on a small regression dataset using four different approximation methods: mean-field variational inference (VI), Adversarial Variational Bayes (AVB, Mescheder et al., 2017), our proposed model (LIVI), and Hamiltonian Monte Carlo (HMC). The dashed line represents the true underlying function from which the training data (black points) were sampled. For each model, we show the mean prediction together with the epistemic uncertainty (dark shaded region) and the combined epistemic and aleatoric uncertainties (light shaded region).

computing this posterior is intractable for all but the simplest models, we turn to the framework of _variational inference_ (VI) which finds the best matching tractable _variational_ distribution, \(q_{}()\) parametrised by \(\) to the true posterior, \(p(\,|\,)\). A sensible measure of their closeness is the (intractable) Kullback-Leibler (KL) divergence between the two distributions defined as

\[_{}q_{}()\,\,p(\,|\,)= p()-_{q _{}()} p(,)- q_{ }()}_{()}.\] (1)

Since the evidence, \(p()\), does not depend on \(\), minimising the above divergence is equivalent to maximising \(()\), referred to as the evidence lower bound (ELBO), which then becomes the objective function in VI. It can be further expanded into a _likelihood term_ and a _regularisation term_ as

\[()=_{q_{}( )} p(\,|\,)}_{}- _{}q_{}()\, \,p()}_{},\] (2)

where the likelihood term encourages the variational approximation to model the data well, and the regularisation term tries to keep the posterior close to the prior.

### Implicit variational inference

In implicit VI (IVI), the variational distribution is only implicitly defined through its generative process which allows sampling from this distribution, but neither its density nor the gradients of its density can be evaluated. One way to draw from an implicit distribution is to push a low-dimensional noise vector through a neural network (Mohamed et al., 2016; Tran et al., 2017; Shi et al., 2018). These models have also been called amortised neural samplers (Kumar et al., 2019), with the generative process \( q()\) and \(=g_{}()\) and the corresponding density

\[q_{}()=}}}_{ |g_{}()}q()\,,\] (3)

where \(q()\) is a fixed base distribution and \(g_{}:^{d}^{m}\) is a non-linear, typically non-invertible mapping due to which this integral is non-trivial to solve. The resulting intractability of the density precludes the usage of popular objectives in probabilistic modelling. The _likelihood term_ from Eq. (2) and its gradients can be estimated using Monte Carlo and the reparameterization trick (Kingma et al., 2014). However, the _regularisation term_ involves the entropy of \(q_{}\), \(H_{q}[q_{}]\):

\[_{}q_{}()p()=_{ q_{}()} [}()}{p()}]\,= _{ q_{}()}[  q_{}()]}_{-H_{q}[q_{}]}-_{ q_{}()}[ p() ],\] (4)

which following the intractability of the density Eq. (3) is not explicitly available. Moreover, critical to consider, \(q_{}\) is not a well-defined density in the parameter space, as its support lies on a low dimensional manifold and has measure zero. Consequently, the KL divergence in Eq. (1) is not always well-defined (Arjovsky et al., 2017) giving rise to another noteworthy issue with IVI. To tackle this issue, standard IVI approaches (Sugiyama et al., 2012; Huszar, 2017) rely on density ratio estimators based on a discriminator (in GANs) to estimate the _regularisation term_. Geng et al. (2021) give a tractable and differentiable lower bound on this entropy.

## 3 A deep latent variable model and its entropy

In this section, we address the issues noted above, **I**: The implicit density lies on a low dimensional manifold, leading to ill-defined KL (Eq. (1)) and **II**: The entropy of the implicit density and its gradients are intractable.

### Defining a proper KL divergence

To make the KL in Eq. (1) well-defined, we add Gaussian noise to the output of our neural sampler \(g_{}\) which has the effect of making the implicit density continuous in the ambient space of \(\), see Arjovsky et al. (Lemma 1, 2017). Accordingly, we switch to the corresponding variational distribution - a Gaussian deep latent variable model (DLVM). This is a special case of the semi-implicit distribution (Yin et al., 2018; Titsias et al., 2019) and equivalent to the generative model in a Gaussian variational autoencoder (VAE, Kingma et al., 2014; Rezende et al., 2014):

\[q_{}()= q_{}(\,|\, )q()\,=_{ q()}[q_{}(\,|\,)],\] (5) \[q_{}(\,|\,)=( \,|\,g_{}(),^{2}_{m}), g_{}: ^{d}^{m},\] (6)

\(^{m}\) and the latent variable \(^{d}\). We assume a Gaussian base density \(q()=(\,|\,,_{d})\). \(g_{}\) is the neural sampler (or generator) and \(^{2}^{+}\) is the fixed, homoscedastic variance of the output density, which we take to be small. Thus, each sample from the base density, \(\), results in a Gaussian output density, \(q_{}(\,|\,)\). And the marginalisation leads to a very flexible \(q_{}()\) which informally, is an infinite mixture of Gaussians (example 2, Im et al., 2017). Generally, we do not have a closed form for \(q_{}()\) and consequently \(H[q_{}]\), due to the non-linear function \(g_{}\) within \(q_{}(\,|\,)\) in Eq. (5), so we are still left with challenge **II**, which we address next.

### Approximating the intractable entropy

With the DLVM, we can expand the entropy of the variational approximation in Eq. (4) as,

\[H[q_{}()]=-_{ q()} _{ q_{}(\,|\,)}[ q_{}()].\] (7)

In principle, a Monte Carlo estimator of Eq. (5) could be used to approximate the \(q_{}()\) term and by extension, this entropy, but this estimator is biased and has high variance. This variance can be reduced with directed sampling via an encoder, but to avoid training an additional network, we derive an encoder-free approximation in the following. To do this, we need to work around the non-linearity of \(g_{}\) that causes the intractability.

Linearisation of the generatorFirst, to estimate \(q_{}()\) during training with lower variance than a direct Monte Carlo estimate, we consider an analytical approximation of Eq. (7) obtained via a local linearisation of the generator/neural sampler around \(^{}\):

\[T^{1}_{^{}}()=g_{}(^{})+_{g}( ^{})\,(-^{}),\] (8)

where \(_{g}(^{})^{m d}\) is the Jacobian of \(g_{}\) evaluated in \(^{}\), which we assume exists. We can approximate \(g_{}()\) by the linear function \(T^{1}_{^{}}()\) when \(\) is close to \(^{}\) and obtain a Gaussian approximation of the output density, \(q_{}(\,|\,)_{^{}}( \,|\,)=(\,|\,T^{1}_{^{}}( ),^{2}_{m})\) and substitute this into Eq. (5) to approximate \(q_{}()\):

\[q_{}() =_{ q()}[q_{}(\, |\,)]_{ q()}[_{^{ }}(\,|\,)]\] (9) \[=_{ q()}[(\,|\,g _{}(^{})+_{g}(^{})\,(-^ {}),^{2}_{m})]\] (10)

Now the expectation in Eq. (10) can be analytically solved by integrating over the latent variable (Tipping et al., 1999) and is Gaussian, which gives the closed form of the approximation

\[q_{}()(\,|\,_{}(^{}),C_{}(^{}))=:_{^{ }}(),\] (11)

where

\[_{}(^{})=g_{}(^{})-_{g} (^{})\,^{}, C(^{})=_{g}(^{})_{g}(^{})^{}+^{2}_{m}.\] (12)

Approximation of the differential entropyWe use the above result to approximate the entropy of the DLVM,

\[H[q_{}()]=-_{ q()}_ { q_{}(\,|\,)}[ q_{} ()]-_{ q()}_{  q_{}(\,|\,)}[_{^{}= }()],\] (13)

where \(_{^{}=}()\) is Eq. (11) evaluated at the samples \(\) from the outer expectation. Importantly, we do the linearisation of \(q_{}()\) around the latent value \(\) that is used to sample each \(\) in the expectation, which means that for small \(^{2}\)-values, \(\) is close to \(^{}\), and the linear approximation will be good. This amounts to using a point-wise Gaussian to approximate the entropy at \(\) in the training objective, whilst sampling from the DLVM noted in Eq. (5). Hence, important to consider here, that the linearisation does not restrict the expressivity of the posterior that we finally obtain.

We could, in principle, evaluate the approximation in Eq. (13) numerically, but it involves the matrix inverse of \(C(^{})\), which is computationally demanding. In Appendix A, we show that for small values of the output variance \(^{2}\), we can further approximate the differential entropy by

\[H[q_{}()]_{ q( )}[(_{g}()_{g}()^{}+ ^{2}_{m})]++ 2\] (14)Linearised Implicit Variational Inference

Finally, we arrive at our novel approximation to the ELBO for IVI using Eq. (5) as the variational distribution. Using the entropy approximation from Eq. (14), we obtain the approximate ELBO denoted the _Linearised Implicit Variational Inference (LIVI) bound_

\[^{}()=_{ q_{}()}[ p(\,|\,)+ p()]+ _{ q()}[(_{g}()_{g}()^{}+^{2}_{m})]+c,\] (15)

where \(c=+ 2\). As the determinant of a matrix is the product of its singular values, the log determinant is the sum of the log of these singular values. If \(s_{d}() s_{1}()\) are the non-zero singular values of the Jacobian \(_{g}()\), we can write the log determinant term as

\[(_{g}()_{g}()^{}+^{2 }_{m})=_{i=1}^{d}(s_{i}^{2}()+^{2})+ {m-d}{2}^{2}\] (16)

To avoid calculating all the singular values of a large Jacobian matrix, we can follow Geng et al. (2021, Eq. 10) and lower-bound Eq. (16) as

\[_{i=1}^{d}(s_{i}^{2}()+^{2})+ ^{2}(s_{1}^{2}()+^{2})+ ^{2}\] (17)

We defer the details of Eq. (16) and the algorithm (LOBPCG, Knyazev, 2001) for obtaining \(s_{1}\) to Appendix C. This allows us to extend our variational approximation to high-dimensional parameter spaces, and using Eq. (17) we obtain a lower bound on \(^{}()\) given by

\[^{}()=_{ q_{} ()}[ p(\,|\,)+ p() ]+_{ q()}[(s_{1}^{2}( {z})+^{2})]+c.\] (18)

We denote \(^{}()\) the approximate LIVI bound with whole Jacobians and \(^{}()\) the LIVI bound with a differentiable lower bound on the determinant. We draw comparisons between how well these bounds estimate the entropy of a DLVM model in Appendix F.5, and write the reparameterised version of both bounds in Appendix B. The relationships between the bounds and the evidence are

\[ p()()^{}() ^{}()\] (19)

Based on the compute budget, the two LIVI bounds offer an accuracy vs compute trade-off. In both cases, entropy maximisation promotes the exploration of the latent space. It is also worthwhile to note that Eq. (15), which uses the entropy approximation in Eq. (13), is fundamentally different from other linearisation approaches in the literature (e.g., Immer et al., 2021), as we do not linearise the Bayesian model. Instead, our approach linearises the generator parameterising \(q_{}(\,|\,)\) only when needed to approximate the differential entropy, thus leaving the variational approximation unchanged.

## 5 Related Work

The usage of a secondary model to generate parameters of a primary model first appeared in the form of _hypernetworks_(Ha et al., 2017). Our approach is probabilistic and is hence closer to Bayesian hypernetworks (Krueger et al., 2017) that mandate invertibility of the generator. The formulation is exactly like normalising flows and thereby sidesteps the complexities of estimating the entropy term. Flow-based approximations require particular focus on the design of the variational approximation to curb the dimensionality of the flow and the Jacobian matrices. Louizos et al. (2017) use an expressive flow on the multiplicative factors of the weights in each layer and not on all weights jointly. Our method does not necessitate invertibility, making it more general. We also share parallels with works in _manifold learning_(Brehmer et al., 2020; Caterini et al., 2021) in that they also use noninvertible mappings and have to employ a general form of the standard change of variable formula \((_{g}()^{}_{g}())\) in the approximating density. Kristiadi et al. (2022) infers an expressive distribution over a subset of weights by using a _post hoc_ Laplace approximation as the base distribution for a normalising flow. This approach, however, inherits the usual scalability limitations of flows.

Broadly, works in implicit VI have proposed novel ways of estimating the ratio of the variational approximation to the prior (regularisation-term), usually referred to as density-ratio estimation (also referred to as the prior-contrastive formulation by Huszar, 2017). To the best of our knowledge, we are the first to propose and test an entropy approximation for an implicit density used as a variational approximation. In particular, Shi et al. (2018), Tran et al. (2017), and Pawlowski et al. (2017) have successfully demonstrated implicit variational inference using hypernetworks and align well with our goals. Tran et al. (2017) opt for training a discriminator network to maximally distinguish two distributions given only i.i.d. samples from each. This approach, though general, adds to the computational requirements and becomes more challenging in high dimensions (Sugiyama et al., 2012). To mitigate the overhead of training the discriminator for each update of the ELBO, many works limit the discriminator training to a single or few iterations. Furthermore, this approach entails an adversarial objective that is notoriously unstable (Mescheder et al., 2017). Pawlowski et al. (2017) treat all weights as independent and find that a single discriminator network is inaccurate at estimating log ratios compared to the analytical form of _Bayes by backprop_(Blundell et al., 2015), and opt to use a kernel method that matches the analytical form more closely.

Shi et al. (2018) propose a novel way of estimating the ratio of the two densities using kernel regression in the space of parameters which obviates the need for a min-max objective but we expect to be inaccurate in estimating high-dimensional density ratios especially given a limited number of samples from both the densities as well as the RBF kernel. Pradier et al. (2018) are also motivated by the possibility of compressing the posterior in a lower-dimensional space and consider the parameters of the generator to be stochastic. They use an inference network with the generator/decoder and hence require empirical latent samples to train which doubles the training steps and limits the scalability. SIVI (Yin et al., 2018) and D-SIVI (Molchanov et al., 2019) use Monte Carlo (MC) averaging to approximate the variational approximation hence precluding an approximation over an intractable entropy as their resultant ELBOs only contains the explicit conditional \(q_{}(\,|\,)\). Their novelties lie in using a general semi-implicit formulation to model all the weights of the network but as noted earlier this MC estimator is biased and has high variance. Yin et al., 2018 propose upper and lower bounds on the ELBO to train with this variational approximation and Molchanov et al. (2019) extend the approach by using an implicit prior.

Recently, several works have considered alternatives to high-dimensional inference by heuristically limiting randomness, e.g., Izmailov et al. (2020), Daxberger et al. (2021, 2021), and Sharma et al. (2023). Our work is somewhat orthogonal to these, as we present a general method for performing high-dimensional inference. Indeed, while our focus here has been on inference in full networks, our bounds can readily be applied to subspaces or subnetworks too. A notable difference from the works building on the Laplace approximation, however, is that these works linearise the model during prediction and inference, whereas we do not. We linearise the neural sampler, \(g_{}()\), but only when it is used to estimate the intractable entropy and its gradients, see Eqs. (10) to (12). Thus, the resulting posterior approximation is, still a highly flexible (i.e., non-linearised) implicit distribution.

## 6 Experiments

We now test the proposed posterior approximation (Section 4) for its generalisation and calibration capabilities in downstream tasks on different established datasets. We use Bayesian Neural Networks (BNNs) for our analysis due to the immense number of global latent variables (i.e., the weights and biases) that are required by modern BNN architectures to perform well with larger datasets, validating our method on previously unreachable and untested scales for implicit VI.

We evaluate multiple posterior approximation schemes for solving various downstream tasks, with the final performance of the BNN serving as a metric for assessing the quality of each method's approximation of the true posterior. Ideally, we wish to understand whether an expressive posterior approximation helps uncertainty quantification in downstream tasks. As this is a challenging question to address directly, we compare LIVI against less flexible baselines to determine whether capturing posterior complexity leads to improved predictive performance and uncertainty estimates.

### Experimental setup

Model definitionBriefly, BNNs are standard neural networks with a prior on their parameters, which, post-training, results in a posterior distribution. In a notation similar to that of Section 2, BNNs can be presented as \(p(\,|\,(,y)) p(y\,|\,f_{}())p()\), where \(\) represents the parameters of a neural network denoted by \(f\). The likelihood, \(p(y\,|\,f_{})\), depends on the given task. In this section, we test proxies to the true posterior \(p(\,|\,(,y))\) for how well they 1) model the dataset, and 2) model regions away from the dataset.

For regression tasks, we use a Gaussian likelihood and learn the homoscedastic variance, \(^{2}\), using type-II maximum likelihood, \(p(y\,|\,f_{}())=(y\,|\,=f_{}(), ^{2})\). For classification tasks, we use a categorical likelihood function for the labels with logits produced by \(f_{}\).

Generator architectureTo efficiently generate millions of correlated parameters, we introduce a novel architecture for the generator \(g_{}()\). We sample a low-dimensional noise matrix, which is gradually scaled up through the generator network using matrix multiplications (Shi et al., 2018). To generate all the parameters of the BNN, we gradually scale up this noise through the generator. The output from the first matrix multiplication layer is split and fed into a series of disconnected matrix multiplication networks that produce parameters for the individual layers of the BNN. This equates to pruning dense connections deeper in the network. This architecture is designed such that it can capture both within-layer correlations in the BNN (through the individual subnetworks), and across-layer correlations (through the first matrix multiplication), whilst having a manageable number of parameters. The architecture is described in detail in Appendix D and has been used for experiments in Sections 6.5 and 6.6.

Method comparisonFor LIVI, the generator or hypernetwork, \(g_{}()\), represents the implicit distribution over network parameters for the BNN, which we sample from. We explicitly mention which of the ELBOs, Eqs. (15) and (16), we use in every experiment. We compare our method with other scalable uncertainty quantification methods: Adversarial Variational Bayes (AVB, Mescheder et al., 2017), last-layer Laplace approximation (LLLA, Daxberger et al., 2021), deep ensembles (DE, Lakshminarayanan et al., 2017), mean-field VI (MFVI, Saul et al., 1996; Osawa et al., 2019), as well as a simple MAP estimate. We chose DEs over other methods, such as SWAG (Maddox et al., 2019), as Daxberger et al. (2021) found this to be a stronger baseline. Where possible, we compare against Kernel Implicit Variational Inference (KIVI, Shi et al., 2018); these results are taken from their paper. The only other implicit VI method is AVB, which, in all experiments, uses the exact same generator architecture as LIVI. Where feasible, we compare with HMC as it is the gold-standard posterior approximation technique.

Overview of experimentsThe first two sections focus on regression; a toy example in Section 6.2 and tests on UCI regression datasets in Section 6.3. Next, we move to classification experiments, focusing on out-of-distribution (OOD) benchmarks. Section 6.4 shows results on MNIST, then Section 6.5 considers images from CIFAR10, allowing us to perform OOD testing of the posterior approximations in millions of dimensions. In Section 6.6, we test on CIFAR100, evaluating our approach on even larger dimensional BNN posteriors 1. We discuss the metrics used in Appendix E.1.

### Toy data

In Fig. 1, we compare posterior predictives of our method (LIVI) against the standard baselines for posterior inference on a simple sinusoidal toy dataset. We qualitatively assess the epistemic uncertainty, or the variance in the function space, in regions away from training data. The BNN contains 105 parameters and is thus small enough that we can use \(^{}\), Eq. (15), as the objective function for LIVI, (for the remaining details of the architectures and the training, see Appendix E.3). We observe that both AVB and MFVI dramatically underestimate uncertainties, whereas LIVI is much closer to Hamiltonian Monte Carlo (HMC). We further observe that LIVI's learned posterior approximation has multiple modes and heavy tails, see Appendix F.4.

### UCI datasets

Here, we follow the setups in Lakshminarayanan et al. (2017) and Shi et al. (2018) and choose a single hidden-layered MLP as the BNN for all datasets. Due to the different dimensionalities of the datasets, the number of BNN parameters vary between 501 and 751. Still, at this scale, it is feasible to work with whole Jacobian matrices, which allows us to test \(^{}\), Eq. (15), as the objective function and to analyse the performance difference with \(^{}\), Eq. (18). Further details of the generator architectures and baseline methods are presented in Appendix E.4.

Our results are summarised in Table 1. Both LIVI bounds, \(^{}\) and \(^{}\), perform similarly, giving empirical justification for the lower bound in Eq. (17). Furthermore, both bounds are comparable to HMC. We train our objective with far fewer samples compared to KIVI and outperform it for even small hypernetwork architectures.

### MNIST Dataset

We use three different out-of-distribution tests to examine the estimated uncertainty in downstream tasks. We use the LeNet architecture for the Bayesian neural network with a total of \(44\,000\) parameters, necessitating the use of our second objective \(^{}\), Eq. (18). We train on MNIST, achieving an in-distribution test accuracy of \( 99.1\%\) for all methods except MFVI, which achieves \(98.6\%\).

OOD Test M-C1In this experiment, we consider three OOD datasets, FMNIST, KMNIST, EMNIST and Table 2 reports the averaged results. We outperform all the methods we compare against in both categories. This signifies that the implicit approximation makes the BNN less over-confident than competing methods when predicting over datasets that the BNN has not encountered in training.

OOD Test M2We compare our method on another OOD benchmark by rotating the MNIST image by different angles (Daxberger et al., 2021). For this benchmark, we plot the negative log-likelihoods and expected calibration errors for all rotation angles in Fig. 2, testing all approaches at varying extents of drift. We conclude that we perform the best in these two categories as well.

OOD Test M3As the last benchmark on MNIST, we opt to plot the empirical CDF of predictive entropies across OOD images (Lakshminarayanan et al., 2017; Louizos et al., 2017). Under covariate shift, a well-calibrated model should predict a uniform distribution for never-before-seen data. So, with a model trained on MNIST we calculate predictive entropies on OOD datasets, plotting each method's fraction of high-entropy predictions in Fig. 3. In other words, the smaller the area between the curve and the dashed line, the better the method. We find that LIVI consistently produces

    & Method & Boston & Concrete & Energy & Kin8nm & Naval \\    } & LIVI (\(^{}\)) & \(2.32 0.07\) & \(\) & \(0.41 0.27\) & \(\) & \(\) \\  & LIVI (\(^{}\)) & \(2.40 0.09\) & \(4.62 0.13\) & \(0.44 0.11\) & \(0.08 0.01\) & \(0.00 0.01\) \\  & HMC & \(\) & \(4.27 0.00\) & \(\) & \(0.04 0.00\) & \(\) \\  & DE & \(3.28 1.00\) & \(6.03 0.58\) & \(2.09 0.29\) & \(0.09 0.00\) & \(0.00 0.00\) \\  & KIVI & \(2.80 0.17\) & \(4.70 0.12\) & \(0.47 0.02\) & \(0.08 0.00\) & \(0.00 0.00\) \\    } & LIVI (\(^{}\)) & \(\) & \(-2.79 0.11\) & \(-1.17 0.13\) & \(1.24 0.04\) & \(6.74 0.04\) \\  & LIVI (\(^{}\)) & \(-2.40 0.09\) & \(-2.99 0.13\) & \(-1.37 0.11\) & \(1.15 0.01\) & \(5.84 0.06\) \\   & HMC & \(-2.20 0.00\) & \(\) & \(\) & \(1.27 0.00\) & \(\) \\   & DE & \(-2.41 0.25\) & \(-3.06 0.18\) & \(-1.31 0.22\) & \(\) & \(5.93 0.05\) \\   & KIVI & \(-2.53 0.10\) & \(-3.05 0.04\) & \(-1.30 0.01\) & \(1.16 0.01\) & \(5.50 0.12\) \\   

Table 1: **UCI regression datasets. We report RMSE (\(\)) and log-likelihoods (\(\)) on the test set and average across three different seeds for each model to quantify the variance in the results.**

    &  &  \\   & & & & \\  Method & MNIST & CIFAR10 & MNIST & CIFAR10 \\  MAP & \(72.10 0.36\) & \(81.40 0.16\) & \(96.32 0.22\) & \(86.73 0.64\) \\ MFVI & \(69.23 0.24\) & \(74.71 0.23\) & \(96.53 0.16\) & \(87.50 0.25\) \\ LLLA & \(67.40 0.19\) & \(53.6 0.3\) & \(96.67 0.27\) & \(89.03 0.51\) \\ DE & \(63.14 0.11\) & \(67.17 0.21\) & \(97.52 0.08\) & \(89.61 0.11\) \\ LIVI & \(\) & \(\) & \(\) & \(\) \\ AVB & \(70.68 0.45\) & NA & \(95.5 0.4\) & NA \\   

Table 2: **OOD Test M-C1: We report out-of-distribution performance for models trained on MNIST (and evaluated on FMNIST, KMNIST and EMNIST) or CIFAR10 (and evaluated on SVHN, LSUN and CIFAR100). The metrics have been averaged across the OOD datasets.**higher-entropy predictions than competitors, showing that it is less overconfident for any OOD image.

We elaborate on our experimental set-ups in Appendix E.5, keeping the critical training parameters the same across methods and also reporting our computational requirements for these experiments.

### Cifar10

To test our approach at a larger scale, we consider the CIFAR10 image classification dataset. Here, for the BNN we use the same WideResNet architecture as Daxberger et al. (2021) with 16 layers and a widen factor of 4 (Zagoruyko et al., 2016). This network architecture contains over 2.7 million trainable parameters, meaning that we again use our second objective \(^{}\), Eq. (18). We generate all parameters with our novel hypernetwork architecture, described in Appendix D, with training details in Appendix E.6. All the methods reached very similar accuracies of \( 91.5\%\) on the in-distribution test set when trained for 200 epochs, except for MFVI, which reached \( 88.2\%\).

OOD Test M-C1We report averaged AUROC and confidence to test if the models assign wrong labels with high probability, see Table 2. The set of OOD datasets contains SVHN, LSUN, and CIFAR100. LIVI again outperforms competitors by being less overconfident about OOD predictions.

OOD Test C2We test the models' performances when presented with corrupted images from CIFAR-10-C (Hendrycks et al., 2019). The negative log-likelihoods (NLL) and expected calibration errors (ECE) for different corruption intensities are plotted in Fig. 4. LIVI retains a low NLL across all corruption intensities, showing that its predictive uncertainty is better calibrated than competitors. Similarly, LIVI achieves consistently low ECEs, showing that its accuracy and confidence are well-aligned (i.e., decrease at a similar rate), even as the corruption intensity increases.

### Cifar100

Finally, we test our approach on CIFAR100 using a WideResNet(28,10) as the BNN. Not only is this dataset more complex to model, the BNN contains roughly 36.5 million parameters, demonstrating that our approach can scale to large networks. The generator used for LIVI has only about twice

Figure 3: **OOD Test M3: Empirical CDF plot.** On the \(x\)-axis is the entropy of a single prediction and empirical CDF on the \(y\)-axis. With this plot, we count the number of high entropy (uniform distribution) predictions that each method makes on OOD data. As we count over the whole dataset, we would like to encounter more high entropy predictions closer to the value 2.30 on the \(x\)-axis.

Figure 2: **OOD Test M2: Rotated MNIST benchmark.** LIVI performs as well as, or better, than competing methods.

as many (variational) parameters as the BNN latent variables - the same amount of variational parameters required for a mean-field Gaussian posterior.

We compare our results with those presented by Nado et al. (2021), who use the same BNN architecture, see Table 3. This architecture is also used by Sharma et al. (2023), who test performance on this dataset in various sub-stochastic settings and report on the same metrics, with which our results can therefore also be compared. The results indicate that LIVI's learned posterior approaches the performance of an ensemble with a combined 146 million parameters, nearly twice the roughly 75 million parameters required by LIVI. Moreover, LIVI's posterior approximation appears better than a simpler approximation like MFVI, even though this uses the same amount of parameters.

## 7 Conclusion

In this paper, we present a novel method to scale implicit variational inference to latent variable models with millions of dimensions by circumventing the need for a discriminator network and an adversarial loss. We find that modelling the posterior with a highly flexible approximation indeed does have benefits in the OOD and data drift scenarios, which may be due to the approximate posterior capturing richer information like correlations and multiple modes. Our method performs better than deep ensembles in a number of OOD benchmarks. Unlike conventional probabilistic methods, we do not fall short on in-distribution accuracy. We empirically evaluated our bound on Bayesian neural networks with millions of latent variables. Our method can also be applied for inference in other latent variable models, such as VAEs (Kingma et al., 2014; Rezende et al., 2014; Mescheder et al., 2017). Future work includes scaling our methods to models with hundreds of millions of parameters. For this, one could consider independent hypernetworks for each target model layer, thus reducing computational requirements at the cost of losing correlations across the layers. One could also consider efficient ways of putting priors over deep networks to curb dimensionality, such as the works by Karaletsos et al. (2018) and Trinh et al. (2020).

## 8 Acknowledgements

The authors thank Soren Hauberg and Pierre-Alexandre Mattei for providing their insight multiple times during the development of this project. We acknowledge EuroHPC Joint Undertaking for

Figure 4: **OOD Test C2: Corrupted CIFAR10 benchmark. OOD performance for methods trained on CIFAR10 and making predictions for CIFAR-10-C images corrupted with Gaussian blur (Hendrycks et al., 2019). LIVI performs as well or better than competitors.**

   Method & Accuracy (\%) \(\) & NLL \(\) & ECE \(\) \\  MAP\({}^{}\) & 79.8 & 0.875 & 0.086 \\ MFVI\({}^{}\) & 77.8 & 0.944 & 0.097 \\ LIVI & 78.7 \(\) 0.4 & 0.774 \(\) 0.080 & 0.036 \(\) 0.004 \\ Ensemble (\(n=4\))\({}^{}\) & **82.7** & **0.666** & **0.021** \\   

* Results from Nado et al. (2021).

Table 3: **CIFAR100 results. Posterior testing with a WideResNet(28,10) architecture on the CIFAR100 dataset. The numbers reported here are on the test set, with those for LIVI being averaged across three seeds.**s, we can see that the rewarding us access to Karolina at IT4Innovations, Czech Republic. AU, WB, and JF acknowledge funding from the Novo Nordisk Foundation through the Centre for Basic Machine Learning Research in Life Science (MLLS, NNF200CC0062606). KSS acknowledges funding from the Novo Nordisk Foundation (NNF200CC0065611). Furthermore, JF was in part funded by the Novo Nordisk Foundation (NNF200CC0065611), the Innovation Fund Denmark (0175-00014B) and Independent Research Fund Denmark (9131-00082B).