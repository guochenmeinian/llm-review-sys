# Neuro-Vision to Language: Enhancing Brain Recording-based Visual Reconstruction

and Language Interaction

Guobin Shen\({}^{1,2,3,4}\), Dongcheng Zhao\({}^{1,2,3}\), Xiang He\({}^{1,3,5}\), Linghao Feng\({}^{1,3}\),

**Yiting Dong\({}^{1,2,3,4}\), Jihang Wang\({}^{1,5}\), Qian Zhang \({}^{1,2,3,5}\), and Yi Zeng\({}^{1,2,3,4,5}\)**

\({}^{1}\) Brain-inspired Cognitive Intelligence Lab, Institute of Automation, Chinese Academy of Sciences,

\({}^{2}\) Center for Long-term Artificial Intelligence,

\({}^{3}\) Beijing Key Laboratory of Artificial Intelligence Safety and Superalignment,

\({}^{4}\) School of Future Technology, University of Chinese Academy of Sciences,

\({}^{5}\) School of Artificial Intelligence, University of Chinese Academy of Sciences

{shenguobin2021, zhaodongcheng2016, hexiang2021, fenglinghao2022,

dongyiting2020, wangjihang2021, q.zhang, yi.zeng}@ia.ac.cn

Corresponding Author.

###### Abstract

Decoding non-invasive brain recordings is pivotal for advancing our understanding of human cognition but faces challenges due to individual differences and complex neural signal representations. Traditional methods often require customized models and extensive trials, lacking interpretability in visual reconstruction tasks. Our framework integrates 3D brain structures with visual semantics using a _Vision Transformer 3D_. This unified feature extractor efficiently aligns fMRI features with multiple levels of visual embeddings, eliminating the need for subject-specific models and allowing extraction from single-trial data. The extractor consolidates multi-level visual features into one network, simplifying integration with Large Language Models (LLMs). Additionally, we have enhanced the fMRI dataset with diverse fMRI-image-related textual data to support multimodal large model development. Integrating with LLMs enhances decoding capabilities, enabling tasks such as brain captioning, complex reasoning, concept localization, and visual reconstruction. Our approach demonstrates superior performance across these tasks, precisely identifying language-based concepts within brain signals, enhancing interpretability, and providing deeper insights into neural processes. These advances significantly broaden the applicability of non-invasive brain decoding in neuroscience and human-computer interaction, setting the stage for advanced brain-computer interfaces and cognitive models.

## 1 Introduction

The decoding of non-invasive brain recordings, such as those obtained from fMRI , is a cornerstone of cognitive neuroscience [2; 3; 4]. This process offers unparalleled insights into the neural underpinnings of human cognition, contributing not only to fundamental scientific knowledge but also to advancements in clinical and technological applications. Despite its potential, the field faces significant challenges primarily due to the high variability of brain activity across individuals  and the complexity inherent in the neural representations of cognitive processes .

Brain decoding techniques have traditionally relied on customized, subject-specific models [7; 6; 8]. These models necessitate intricate and costly experimental setups, depending on multiple trialsto achieve reliable results. Such approaches, while helpful, are inherently limited in scalability and flexibility, hindering broader application and generalization across different populations and conditions.

Visual reconstruction  aims to recreate perceived visual stimuli from brain signals and is considered one of the benchmarks of brain decoding. However, this approach often struggles to accurately reproduce the visual experiences of individuals, generally lacking semantic precision and interpretability . This inability to effectively decode and reconstruct signals restricts our understanding of how sensory information is processed. Recognizing these limitations, our study introduces language modalities as a critical enhancement designed to assess decoding performance more effectively and enrich brain-computer interfaces' interaction capabilities.

Addressing these multifaceted challenges, our research introduces the _Vision Transformer 3D_ (ViT3D)  specifically tailored to the domain of visual reconstruction. Unlike traditional approaches that often reduce complex brain regions to one-dimensional vectors [11; 6; 12; 13], losing critical spatial structure information, our implementation of ViT3D preserves the three-dimensional structural integrity of the brain data. This adaptation enables an unprecedented enhancement in the extraction of visual semantic information, ensuring a deeper fidelity and richness in the decoded visual representations.

Our fMRI feature extractor includes a unified network backbone and two alignment heads for feature matching. This setup enables efficient, high-quality visual reconstructions across subjects from one experimental trial. By simply aligning the extractor's output with CLIP embeddings  and features of Variational Autoencoder (VAE) , our method eliminates the need for multiple, subject-specific models, substantially simplifying the decoding process. This straightforward and effective configuration reduces the resources required for brain decoding and showcases the potential for easy integration with Large Language Models (LLMs), enhancing its usability across various applications.

Moreover, our research delves into the integration of brain recordings with visual and linguistic data within a comprehensive multimodal framework using LLMs. This integration significantly improves visual reconstruction performance and introduces the groundbreaking capability for direct interaction through natural language. Our model facilitates diverse communication with brain data using natural language and precisely localizes linguistic concepts within brain signals. These advancements help deepen our understanding of the interactions between language, perception, and neural activity. Additionally, to bolster the development of these multimodal models, we have augmented the brain-recording visual dataset with natural language enhancements.

In summary, our contributions can be summarized as follows:

* Our fMRI feature extractor, based on _Vision Transformer 3D_, aligns fMRI features with visual embeddings at multiple levels, integrating 3D brain structures with visual semantics. This eliminates the need for subject-specific models and enables efficient data extraction from single trials, significantly reducing training costs and enhancing practical usability in real-world scenarios.
* We expanded the language dimension of our fMRI-visual dataset to build a multimodal large model capable of decoding fMRI data. This enhancement boosts brain decoding performance and broadens the application scope to include tasks like visual reconstruction, question-answering, and complex reasoning while also allowing precise localization and manipulation of language-based concepts within brain signals.
* Experimental results on the Natural Scenes Dataset (NSD)  for visual reconstruction and language interaction tasks demonstrate that our method surpasses existing models, effectively achieving concept localization and elimination.

## 2 Related Works

Non-invasive techniques such as functional magnetic resonance imaging (fMRI) are pivotal in providing direct insights into neural activities, significantly deepening our understanding of complex cognitive processes from neural network structures  to advanced image and language processing tasks [18; 19]. This section reviews key developments in fMRI-based brain decoding, particularly emphasizing the shift from simple subject-specific analyses to more complex, multimodal interpretations.

**Visual Reconstruction from Brain Activity** Visual reconstruction from brain activity involves translating brain recordings into the visual stimuli perceived by subjects. Early methods, like those developed by Horikawa _et al._, relied on sparse linear regression to predict features extracted by convolutional neural networks from fMRI data. Recent advancements in generative artificial intelligence, particularly diffusion models , have propelled efforts to reconstruct visual stimuli directly from fMRI. For instance, Lin _et al._ aligned fMRI data with image features and corresponding CLIP embeddings to facilitate image generation using fine-tuned StyleGAN . Similarly, Takagi _et al._ improved the quality of visual reconstructions by aligning fMRI with CLIP text embeddings and the latent spaces of diffusion models. Xia _et al._ aligned fMRI data from dimensions of image CLIP features, depth, and color using T2I Adapters  for fine-grained conditional control. Despite these advancements, the complexity of such methods, involving multiple independent modules, complicates their integration with technologies like LLMs and restricts their generalizability across different subjects. We observed that some contemporary works also attempt cross-subject alignment; however, these methods either require subject-specific parameters  or face performance issues compared to subject-specific models .

**fMRI Data Processing** Efficiently processing fMRI data to extract visually relevant activities typically involves simplifying the data into one-dimensional vectors and selecting voxels most responsive to visual stimuli. Traditional methods utilize simple linear regression or fully connected networks to predict visual stimulus features [18; 21]. However, these methods often lose essential spatial structural information, which is critical given the individual differences in brain anatomy. To address these challenges, innovations such as Vision Transformer 3D (ViT3D) have been developed for managing data with intricate spatial structures [26; 27]. ViT3D segments 3D data into patches, preserving local spatial information within each patch and maintaining overall structural integrity through self-attention mechanism , thereby enhancing the performance of brain activity extraction .

**Multimodal Integration with Brain Signals** The utilization of language as a medium for representation allows for the expression of complex concepts with precision and abstraction. The advent of LLMs has showcased their potential to act as bridges across different modalities, enhancing interactions with visual and audio data through natural language [30; 31]. For example, approaches like those by Defossez _et al._, which align brain recordings with spoken language to decode speech non-invasively, have demonstrated the effectiveness of combining brain recordings with LLMs. However, these approaches are often limited by the specificity of the fMRI feature extractors used, which can restrict the scalability and the size of the models employed. By integrating our specially designed cross-subject fMRI feature extractor, we enhance visual reconstruction quality and enable complex reasoning and direct interaction with model outputs using natural language, achieving precise localization of open natural language concepts within the brain.

## 3 Methodology

Our approach is designed to tackle the key challenges encountered in the visual reconstruction of brain activity and the integration of LLMs with multimodal data. Traditional brain decoding methods, especially those involving fMRI, often struggle with the complexity of accurately reconstructing visual stimuli and generalizing these models across different subjects. Furthermore, while LLMs hold significant potential for enhancing interactions across various cognitive models, their integration with neuroimaging data has been hindered by the need for non-scalable or efficient customized, subject-specific models.

In the following sections, we detail our methodology's components, as seen in Fig. 1. We describe the architecture of our feature preprocessor that maintains the spatial structure of fMRI data, our unified fMRI feature extractor, and the integration strategies for the network with LLMs. We also elaborate on the multimodal interaction techniques that enable direct and meaningful communication between the computational model and the neural representations and the implementation details for visual reconstruction.

### fMRI Feature Preprocessor

fMRI quantifies changes in the blood-oxygen-level-dependent (BOLD) signals to characterize neural activity. The BOLD signal for a given subject can be represented as a 3D matrix \(b_{}^{X_{s} Y_{s} Z_{s}}\), where \(s\) indexes the subject, accounting for inter-individual variability. Traditional preprocessing methods typically involve masking voxels sensitive to the specific task, followed by flattening theremaining data into a 1D vector. For subject \(s\), the processed fMRI signal can thus be represented as \(b_{s}^{1 N_{s}}\), where \(N_{s}\) denotes the number of voxels selected after masking.

However, this approach results in a loss of spatial structural information, complicating alignment across different subjects. We propose a feature extraction method that preserves spatial structure, as shown in Fig. 2. Starting with the original BOLD signal \(b_{}\), we first use trilinear interpolation to resize the data to a uniform dimension, ensuring maximal spatial consistency across subjects' brains while not introducing subject-specific parameters. After resizing, the normalized data undergoes a patching process where it is divided into smaller cubic segments, each of dimension \(C=r^{3}\). This step retains the local spatial features within each segment, preserving the 3D structure crucial for accurate analysis. Finally, patches containing non-task-relevant information are filtered out to reduce computational load. This results in patched data with dimensions \(^{N C}\), where \(N\) is the number of patches retained that contain meaningful information. The entire preprocessing operation can be summarized as a mapping:

\[\{p:b_{}^{s} b b_{}^{s}^{X_{s} Y_{s} Z_{s}},b^{N C}\}.\] (1)

In Eq. 1, \(p\) is the preprocessing function applies resizing, patching, and masking to transform the original fMRI data into a structured format. This function provides a uniform representation of the BOLD signals across different subjects, ensuring that the spatial structure is preserved and capable of integration with Transformer architectures.

### Dual-Stream fMRI Feature Extractor

Visual reconstruction tasks typically begin by mapping the processed BOLD signal, \(b\), to various estimated visual feature spaces, represented as \(\{},},\}\), where \(z\) indicates different levels of visual features and \(\) denotes the visual features estimated from the BOLD signals. Subsequently, these features are used to reconstruct the image, represented as \(\), from the visual features. To enhance the quality of the reconstructed images, complex feature extractor designs are required,

Figure 1: Overview of the integrated multimodal framework combining fMRI feature extraction with LLMs for interactive communication and reconstruction. The architecture comprises: **(a)** a dual-stream pathway for feature alignment with VAE and CLIP embeddings. **(b)** A 3D fMRI preprocessor \(p\), and an fMRI feature extractor. **(c)** A multimodal LLM integrated with fMRI. The extracted features are then fed into an LLM for processing natural language instructions and generating responses or visual reconstructions.

Figure 2: Description of fMRI data preprocessing. First align the data of different subjects, then patch them, and finally remove activity-irrelevant patches.

which increases the preprocessing and computational overhead. However, thanks to the design of our fMRI feature extractor, we can achieve efficient visual reconstruction using a single network backbone, as shown in Fig. 1(b).

Specifically, the patched features obtained from Eq. 1 are directly processed through a Transformer Encoder \(_{b}\) to extract features, obtaining the hidden states from the last layer, \(h^{N_{b}}=_{b}(b)\), where \(N_{b}\) represents the number of layers of the encoder. These outputs are then aligned with the visual stimulus's CLIP embeddings \(z_{c}=_{c}(x)\) and VAE features \(z_{v}=_{v}(x)\), where \(x\) represents the visual stimulus. The loss function used to train the fMRI feature extractor can be expressed as:

\[_{}=_{(b,x) P(B,X)}[\|f_{w_{c}} (h_{0}^{N_{b}})-_{c}(x)\|_{2}^{2}+\|f_{w_{v}}(h_{0} ^{N_{b}})-_{v}(x)\|_{2}^{2}].\] (2)

In Eq. 2, the expectation \((b,x) P(B,X)\) averages the alignment loss across samples from the \(\) (fMRI signals) and \(\) (corresponding visual stimuli). Here, \(h_{0}^{N_{b}}\) represents the output from the first token of the last hidden state layer of the encoder \(_{b}\). The functions \(f_{w_{c}}\) and \(f_{w_{c}}\) are two-layer perceptrons designed to align the extracted fMRI features with these embeddings. The hyperparameter \(\) balances the losses between the alignments of CLIP and VAE features. Through this dual-stream configuration, we create a backbone network that incorporates different levels of visual features. Using only a simple MSE loss function, we achieve high-quality visual reconstruction.

### Multimodal Interaction with fMRI

Our feature extractor architecture, equipped with a single backbone network, is adept at encapsulating various feature levels, making it highly suitable for integration with LLMs. Inspired by advancements such as those in LLaVA , we utilize the penultimate hidden states of our network, \(h^{N_{b}-1}\), as multimodal tokens of fMRI data. A two-layer perceptron \(f_{t}\) projects this state to the same dimension as the text embeddings, resulting in the fMRI embeddings \(t=f_{t}(h^{N_{b}-1})\). Considering a sequence of question-answer pairs related to the fMRI data \([q_{0},a_{0},q_{1},a_{1},,q_{L},a_{L}]\), the training objective is formulated as:

\[ p_{}(|,t)=_{j=0}^{L}p_{}(a_{j}|q _{j},a_{j-1},,q_{0},t).\] (3)

Eq. 3 describes the probability of generating a sequence of answers \(\) given a sequence of questions \(\) and the derived fMRI embeddings \(t\). Each answer \(a_{j}\) is conditionally dependent on all preceding questions and answers, as well as the context embeddings derived from fMRI data, where \(\) represents the trainable parameters of LLMs.

To effectively couple fMRI data with language models, annotated data is essential. Although the NSD  uses labeled visual stimuli from the COCO dataset , semantic mismatches often occur due to modifications such as image cropping performed when displaying images to subjects (Appendix A.1). Moreover, the original captions in NSD are not detailed enough to capture nuanced semantic information. Recognizing the importance of comprehensive linguistic annotations, we have constructed a diverse instructional dataset that includes various textual data: brief descriptions, detailed descriptions, continuous dialogues, complex reasoning tasks, instruction reconstruction, and concept localization.

### Interaction and Reconstruction via LLMs

Our fine-tuned model can understand information embedded within fMRI data and adhere to human instructions. Interactions and explanations of visual stimuli content occur through natural language. A typical dialogue format is structured as follows: <human>:[image] [instruction] <bot>:[answer]. Here, [instruction] denotes a natural language instruction, which during inference is tokenized and embedded, while [image] acts as a placeholder, replaced by the fMRI data embedding \(t\). The model then responds based on the directive and the embedded fMRI data. [answer] represents the response generated by the LLMs.

After instruction-based fine-tuning, the model communicates directly via natural language and supports visual reconstruction and location identification of concepts expressed in natural language, as shown in Fig. 3. These are facilitated respectively through Stable UnCLIP  for visual reconstruction and GradCAM  for concept localization.

For visual reconstruction, the LLM initially generates a reconstruction prompt \(a_{r}\), which, combined with the latent representations \(_{v}\) and \(_{c}\) from the fMRI feature extractor, results in the generation of an image. This process can be formalized as:

\[=((1-)_{v}+_{c},a_{r}), (0,1).\] (4)

In Eq. 4, \(\) represents the frozen UnCLIP, used for visual reconstruction, where \(_{c}\) and \(a_{r}\) serve as conditional information during the denoising process, and \(_{v}\) acts as the initial latent representation of the image. The hyperparameter \(\) is used to introduce noise into the latent space prior, balancing low-level features brought by the prior and high-level information controlled by the diffusion conditions during the denoising process. For concept localization in natural language, LLMs activate the feature extractor using keywords from the instructions, enabling precise location identification of the discussed concepts.

## 4 Experiments

### Implementation Details

**Dataset and Preprocessing:** We utilized the Natural Scenes Dataset (NSD) , containing high-resolution 7Tesla fMRI scans and corresponding visual stimuli from COCO . The dataset involved eight subjects, but analyses focused on the four (subj01, subj02, subj05 and subj07) who completed all sessions. Modifications like cropping necessitated re-annotation of images using BLIP2  for captions and DETR  for bounding boxes to maintain consistency. fMRI data was standardized to dimensions \(83 104 81\) using trilinear interpolation and segmented into \(14 14 14\) patches.

**Architecture and Training:** Our architecture integrates CLIP ViT-L/14  and an AutoencoderKL  for image feature extraction, aligned with fMRI data processed through a \(16\)-layer Transformer Encoder . This setup employed two perceptrons (\(f_{w_{c}}\), \(f_{w_{v}}\)) to align features with CLIP and VAE, respectively. Training involved a multi-stage approach where the LLM was initially frozen, followed by a fine-tuning stage for both the LLM and the Transformer Encoder. For visual reconstructions, the model utilized UnCLIP-2  with \(\) set to \(0.93\), and concept localization was achieved using GradCAM . For more details on the dataset and experimental setup, please refer to Appendices A, B, and C. Additional experimental results can be found in Appendix D.

### Captioning and Question Answering

Tab. 1 shows the performance of our method on multimodal language tasks. With the introduction of LLMs, we have expanded the task forms to include brain captions, detailed descriptions, and complex reasoning, as illustrated in Fig. 3. Our approach has demonstrated superior performance on the majority of metrics for the brain captioning task. Notably, our model can generalize across subjects without the need to train separate models for each subject or introduce subject-specific parameters.

Given the semantic mismatch between captions and images in the original NSD (Section 3.3), we reran the experiment using BLIP2 -generated captions as ground truth. The results, shown in Tab. 1, show significant improvements when evaluated against BLIP2-generated captions, confirming the effectiveness of our model in the brain captioning task and the reasonableness of the task setting.

Beyond brain captioning, we have incorporated tasks for detailed description and complex reasoning. Our model also achieved the best performance on these two tasks, suggesting that it can generate

Figure 3: Demonstration of the model’s capabilities for engaging in multi-round dialogue, complex reasoning, visual reconstruction, and concept location tasks using fMRI data.

not only simple captions but also detailed descriptions and perform complex reasoning. The model's performance increases on complex reasoning tasks, possibly due to the richer semantic information in the questions, which our model captures more effectively. An ablation study was also conducted, revealing a noticeable performance drop in multimodal language tasks when the structural-preserving features of fMRI data were not extracted using ViT3D. Instead, the fMRI data were flattened and patched, similar to methods used in other literature, while maintaining the same fMRI feature extractor structure. This underlines the effectiveness of ViT3D and the capability of our model in multimodal tasks.

### Visual Reconstruction

While our primary objective extends beyond mere visual decoding from fMRI data, visual reconstruction offers a tangible demonstration of a model's comprehension of fMRI data. Therefore, we conducted visual reconstruction experiments and compared our results with those from other studies. The quantitative evaluation highlights our method's proficiency.

Tab. 2 showcases that our model competes with or surpasses traditional subject-specific frameworks on several metrics. Notably, it excels in high-level feature matching, demonstrating the model's ability to effectively leverage LLMs for interpreting complex visual data. The robust performance across various visual stimuli confirms our model's comprehensive understanding of fMRI data. Experiments without key components like LLM and VAE features highlight the significance of each element in our method, which is crucial for achieving state-of-the-art results. Moreover, we have conducted single-trial experiments, opting to use only the first visual stimulus, similar to the approach of MindEye , rather than averaging signals from three identical stimuli, which typically escalates data collection costs. Even under these more stringent conditions, our model shows only a slight decrease in performance, enhancing its feasibility for practical applications. Visual reconstruction examples are provided in Fig. 4, illustrating the effectiveness of our approach.

The balance between noise introduction and feature preservation in fMRI data visual reconstruction is governed by the hyperparameter \(\). Fig. 5 presents a detailed ablation study on how different \(\) values impact various metrics. Fig. 5(a) shows that Pixel Correlation (PixCorr) peaks at intermediate \(\) values, indicating the optimal balance between injected noise and retained prior. The integration of the LLMs does not significantly influence low-level feature capture. In Fig. 5(b), increasing \(\) enhances CLIP accuracy, with LLM integration having a substantial effect on capturing high-level features. Fig. 5(c) indicates the features of the \(5\) th layer of AlexNet, similar to CLIP features, effectively represent the similarity between reconstructed images and visual stimuli, capturing high-level features accurately. Additionally, Fig. 5(d) illustrates that both the Structural Similarity Index (SSIM) and

   Method & \# Models & BLEU1 & BLEU2 & BLEU3 & BLEU4 & METEOR & ROUGE & CIDEr & SPICE & CLIP-S \\   & &  \\  SDRecon  & \(4\) & \(36.21\) & \(17.11\) & \(7.72\) & \(3.43\) & \(10.03\) & \(25.13\) & \(13.83\) & \(5.02\) & \(61.07\) \\ OneLLM  & \(4\) & \(47.04\) & \(26.97\) & \(15.49\) & \(9.51\) & \(13.55\) & \(35.05\) & \(22.99\) & \(6.26\) & \(54.80\) \\ UniBrain  & \(4\) & \(-\) & \(-\) & \(-\) & \(-\) & \(16.90\) & \(22.20\) & \(-\) & \(-\) & \(-\) \\ BrainCap  & \(4\) & \(55.96\) & \(36.21\) & \(22.70\) & \(14.51\) & \(16.68\) & \(40.69\) & \(41.30\) & \(9.06\) & \(64.31\) \\ UMBRAE  & \(1\) & \(52.84\) & \(38.43\) & \(25.41\) & \(17.17\) & \(18.70\) & \(42.14\) & \(53.87\) & \(12.27\) & \(66.10\) \\  Our Method & \(1\) & \(57.19\) & \(37.17\) & \(23.78\) & \(15.85\) & \(18.60\) & \(36.67\) & \(49.51\) & \(12.39\) & \(65.49\) \\ w/o ViT3D & \(1\) & \(52.91\) & \(32.18\) & \(15.64\) & \(8.49\) & \(14.07\) & \(23.25\) & \(39.64\) & \(8.34\) & \(56.92\) \\  Our Method\({}^{*}\) & \(1\) & \(64.26\) & \(51.44\) & \(47.70\) & \(32.17\) & \(20.41\) & \(52.61\) & \(83.94\) & \(18.27\) & \(68.72\) \\ w/o ViT3D\({}^{*}\) & \(1\) & \(58.87\) & \(42.11\) & \(29.48\) & \(21.39\) & \(15.85\) & \(38.48\) & \(56.37\) & \(11.27\) & \(64.35\) \\   & &  \\  Our Method & \(1\) & \(38.91\) & \(24.02\) & \(15.24\) & \(12.41\) & \(18.44\) & \(27.83\) & \(42.58\) & \(18.41\) & \(56.16\) \\ w/o ViT3D & \(1\) & \(33.57\) & \(18.95\) & \(11.09\) & \(6.13\) & \(15.56\) & \(23.80\) & \(20.23\) & \(16.21\) & \(51.47\) \\   & &  \\  Our Method & \(1\) & \(65.41\) & \(59.61\) & \(50.68\) & \(36.46\) & \(34.46\) & \(62.60\) & \(217.83\) & \(60.29\) & \(80.96\) \\ w/o ViT3D & \(1\) & \(60.36\) & \(47.81\) & \(39.76\) & \(30.57\) & \(24.37\) & \(45.39\) & \(150.67\) & \(52.13\) & \(73.26\) \\   

Table 1: Quantitative analysis of brain captioning, detailed descriptions, and complex reasoning tasks. Some results are derived from UMBRAE . Results are compared to those from other studies, with [best], second], and third highlighted. Underline indicates the best result under identical conditions, while \({}^{*}\) denotes results obtained using BLIP2-generated captions as ground truth.

CLIP scores benefit from optimally chosen \(\) values, with LLM integration enhancing overall image quality and semantic accuracy. Appropriately adjusting \(\) helps balance the representation of different feature levels in the reconstructed images. Fig. 6 provides examples of visual reconstructions at various \(\) values, demonstrating the model's enhanced capabilities.

### Concept Localization

To further our understanding of semantic concept localization within brain signals, we capitalized on the alignment between our fMRI encoder and CLIP features, which were developed during the training phase. Building on this, we devised a method to localize concepts within brain signals. Specifically, we first fine-tuned Language Models (LLMs) to extract the target concepts from natural language. These concepts, once encoded through the CLIP text encoder, served as targets for GradCAM, which facilitated the localization of the concept within brain signals. To enhance the precision of our localization, we trained three models with varying patch sizes (\(14,12,10\)) and utilized the penultimate layers of all models to extract semantic features. Fig. 7 illustrates the brain signal localization results for different semantic information, indicating our method's capacity to discriminate the positions of various semantics within brain signals for the same visual stimulus.

    &  &  &  \\  & &  &  & AlexNet(2) \(\) & AlexNet(5) \(\) & Inception \(\) & CLIP \(\) & EffNet-B \(\) & SwAV \(\) \\  Mind-Reader  & \(4\) & – & – & – & – & \(78.2\%\) & – & – & – \\ Takagi _et al_ & \(4\) & – & – & \(83.0\%\) & \(83.0\%\) & \(76.0\%\) & \(77.0\%\) & – & – \\ Gu _et al_ & \(4\) & \(.150\) & \(.325\) & – & – & – & – & \(.862\) & \(.465\) \\ Brain-Diffuser  & \(4\) & \(.254\) & \(.356\) & \(94.2\%\) & \(96.2\%\) & \(87.2\%\) & \(91.5\%\) & \(.775\) & \(.423\) \\ MindEye  & \(4\) & \(.309\) & \(.323\) & \(94.7\%\) & \(77.8\%\) & \(93.8\%\) & \(94.1\%\) & \(.645\) & \(.367\) \\ DREAM  & \(4\) & \(.288\) & \(.338\) & \(95.0\%\) & \(97.5\%\) & \(94.8\%\) & \(95.2\%\) & \(.638\) & \(.413\) \\  MindBridge  & \(1\) & \(.151\) & \(.263\) & \(87.7\%\) & \(95.5\%\) & \(92.4\%\) & \(94.7\%\) & \(.712\) & \(.418\) \\ UMBRAE  & \(1\) & \(.283\) & \(.328\) & \(93.9\%\) & \(96.7\%\) & \(93.4\%\) & \(94.1\%\) & \(.700\) & \(.393\) \\ Our Method & \(1\) & \(.265\) & \(.35\) & \(93.1\%\) & \(97.1\%\) & \(96.8\%\) & \(97.5\%\) & \(.633\) & \(.321\) \\  w/o LLM & \(1\) & \(.263\) & \(.369\) & \(92.0\%\) & \(97.1\%\) & \(94.2\%\) & \(96.1\%\) & \(.680\) & \(.328\) \\ w/o VAE feature & \(1\) & \(.093\) & \(.263\) & \(84.5\%\) & \(90.6\%\) & \(93.6\%\) & \(95.7\%\) & \(.684\) & \(.398\) \\ w/o C\_Subj & \(4\) & \(.241\) & \(.356\) & \(88.1\%\) & \(95.7\%\) & \(92.1\%\) & \(95.1\%\) & \(.631\) & \(.347\) \\ w/o C\_Subj \& ViT3D & \(4\) & \(.164\) & \(.273\) & \(86.7\%\) & \(91.4\%\) & \(89.3\%\) & \(91.8\%\) & \(.731\) & \(.417\) \\   \\  MindEye  & \(4\) & \(.255\) & \(.308\) & \(91.6\%\) & \(95.9\%\) & \(91.3\%\) & \(91.6\%\) & \(.691\) & \(.398\) \\ Our Method & \(1\) & \(.257\) & \(.336\) & \(91.2\%\) & \(96.3\%\) & \(94.6\%\) & \(95.3\%\) & \(.671\) & \(.324\) \\   

Table 2: Quantitative evaluation on visual reconstruction. Performance metrics are reported across different levels of features, with the best, second and third scores highlighted. The underline indicates the best result under the same conditions. Our method achieves state-of-the-art results using a single model trained across subjects (# Models = 1).

Figure 4: Visual reconstruction results showcasing the comparison between (a) using the average signal from all trials and (b) using the first visual stimulus.

To validate the efficacy of our method, we conducted an ablation study on the semantic concepts. After localizing the concepts in the original brain signals, we zeroed out the signals in the identified voxels and performed feature extraction and visual reconstruction using the modified brain signals. As depicted in Fig. 8, the removal of neural activity in specific brain regions associated with certain semantic concepts resulted in the omission of the corresponding semantics in the visual reconstruction. This substantiates the validity of our concept localization method within brain signals and demonstrates our approach's capacity for extracting and modifying semantic information in brain activity, which is pivotal for comprehending semantic information processing in the brain.

## 5 Conclusion

Our study has successfully developed and validated a novel brain decoding framework that leverages the capabilities of Vision Transformer 3D in conjunction with fMRI data, enhanced by the integration of LLMs. This approach has demonstrated a notable improvement in the reconstruction of visual stimuli from brain signals, offering a more precise and interpretable understanding of the underlying neural mechanisms. The experimental results confirmed the robustness of our model in performing various cognitive tasks, including captioning, question-answering, and visual reconstruction, all from single-trial fMRI data. By enabling accurate localization of linguistic concepts within the brain, our work has potential applications in developing brain-computer interfaces and advanced cognitive modeling. Conclusively, this research contributes to the broader endeavor of decoding and interpreting brain activity, with significant implications for neuroscience and technology interface development. The fusion of advanced AI models with neuroimaging opens new avenues for exploring the intricacies of human cognition and the seamless integration of technology with neural processes.

Figure 5: Ablation analysis of the hyperparameter \(\) on visual reconstruction performance.

Figure 6: Visualization of the impact of \(\) on visual reconstruction.

Figure 7: Differential heatmaps of neural activity representing various semantic information for the same visual stimulus.