# Rethinking Model-based, Policy-based, and Value-based Reinforcement Learning via the Lens of Representation Complexity

Rethinking Model-based, Policy-based, and Value-based Reinforcement Learning via the Lens of Representation Complexity

Guhao Feng

Alphabetical order.School of EECS, Peking University. Email: fenguhao@stu.pku.edu.cn

Han Zhong

Center for Data Science, Peking University. Email: hanzhong@stu.pku.edu.cn

###### Abstract

Reinforcement Learning (RL) encompasses diverse paradigms, including model-based RL, policy-based RL, and value-based RL, each tailored to approximate the model, optimal policy, and optimal value function, respectively. This work investigates the potential hierarchy of representation complexity among these RL paradigms. By utilizing computational complexity measures, including time complexity and circuit complexity, we theoretically unveil a potential representation complexity hierarchy within RL. We find that representing the model emerges as the easiest task, followed by the optimal policy, while representing the optimal value function presents the most intricate challenge. Additionally, we reaffirm this hierarchy from the perspective of the expressiveness of Multi-Layer Perceptrons (MLPs), which align more closely with practical deep RL and contribute to a completely new perspective in theoretical studying representation complexity in RL. Finally, we conduct deep RL experiments to validate our theoretical findings.

## 1 Introduction

The past few years have witnessed the tremendous success of Reinforcement Learning (RL)  in solving intricate real-world decision-making problems, such as Go  and robotics . These successes can be largely attributed to powerful function approximators, particularly Neural Networks (NN) , and the evolution of modern RL algorithms. These algorithms can be categorized into _model-based RL_, _policy-based RL_, and _value-based RL_ based on their respective objectives of approximating the underlying model, optimal policy, or optimal value function.

Despite the extensive theoretical analysis of RL algorithms in terms of statistical error [e.g., 5, 21, 42, 24, 23, 14, 17, 58, 51] and optimization error [e.g., 2, 49, 9, 29] lenses, a pivotal perspective often left in the shadows is _approximation error_. Specifically, the existing literature predominantly relies on the (approximate) realizability assumption, assuming that the given function class can sufficiently capture the underlying model, optimal value function, or optimal policy. However, limited works examine the _representation complexity_ in different RL paradigms -- the complexity of the function class needed to represent the underlying model, optimal policy, or optimal value function. In particular, the following problem remains elusive:

_Is there a representation complexity hierarchy among model-based RL, policy-based RL, and value-based RL?_

To our best knowledge, the theoretical exploration of this question is limited, with only two exceptions .  employs piecewise linear functions to represent both the model and value functions,utilizing the number of linear pieces as a metric for representation complexity. They construct a class of Markov Decision Processes (MDPs) where the underlying model can be represented by a constant piecewise linear function, while the optimal value function necessitates an exponential number of linear pieces for representation. This disparity underscores that the model's representation complexity is comparatively less than that of value functions. Recently,  reinforced this insight through a more rigorous circuit complexity perspective. They introduce a class of MDPs wherein the model can be represented by circuits with polynomial size and constant depth, while the optimal value function cannot. However, the separation between model-based RL and value-based RL demonstrated in  may not be deemed significant (cf. Remark 5.6). More importantly,  do not consider policy-based RL and do not connect the representation complexity in RL with the expressive power of neural networks such as Multi-Layer Perceptron (MLP) and Transformers , thereby providing limited insights for deep RL.

**Our Contributions.** To address the limitations of previous works and provide a comprehensive understanding of representation complexity in RL, we explore representation complexity within **(1)** model-based RL; **(2)** policy-based RL; and **(3)** value-based RL, employing **(i)** computational complexity (time complexity and circuit complexity) and **(ii)** expressiveness of MLP to rigorously quantify representation complexity. We outline our results below, further summarized in Table 1.

* To elucidate the representation complexity separation between model-based RL and model-free RL4, we introduce two types of MDPs: 3-SAT MDPs (Definition 3.2) and a broader class referred to as NP MDPs (Definition 3.7). In both cases, the representation complexity of the model, inclusive of the reward function and transition kernel, falls within the complexity class \(^{0}\) (cf. Section 2). In contrast, we demonstrate that the representation of the optimal policy and optimal value function for 3-SAT MDPs and NP MDPs is NP-complete. Significantly, our results not only demonstrate the significant representation complexity separation between model-based RL and model-free RL, but also address an open question in . See Remark 3.4 for details. * To further showcase the separation within the realm of model-free RL, we introduce two classes of MDPs: CVP MDPs (Definition 4.1) and a broader category denoted as P MDPs (Definition 4.4). In both instances, the representation complexity of the underlying model and the optimal policy is confined to the complexity class \(^{0}\). In contrast, the representation complexity for the optimal value function is characterized as P-complete, reflecting the inherent computational challenges associated with computing optimal values. This underscores the efficiency in representing policies (and models) while emphasizing the inherent representation complexity involved in determining optimal value functions.
* To provide more practical insights, we establish a connection between our previous findings and the realm of deep RL, where the model, policy, and value function are represented by neural networks. Specifically, for 3-SAT MDPs and NP MDPs, we demonstrate the effective representation of the model through a constant-layer MLP/Transformer with polynomial hidden dimension, while the optimal policy and optimal value exhibit constraints in such representation. Furthermore, for the CVP MDPs and P MDPs, we illustrate that both the underlying model and optimal policy can be represented by constant-layer MLPs/Transformers with polynomial hidden dimension. However, the optimal value, in contrast, faces limitations in its representation using MLPs/Transformers with constant layers and polynomial hidden dimensions. These results corroborate the messages conveyed through the perspective of computational complexity, contributing a novel perspective that bridges the representation complexity in RL with the expressive power of MLP/Transformer. In addition, we conduct deep RL experiments to corroborate our theory.

   & & Computational Complexity & Expressiveness of Log-precision MLP/Transformer \\  & & (time complexity and circuit complexity) & (constant layers and polynomial hidden dimension) \\   & Model & \(^{0}\) & \(\) \\   & Policy & NP-Complete & \(\) \\   & Value & NP-Complete & \(\) \\   & Model & \(^{0}\) & \(\) \\   & Policy & \(^{0}\) & \(\) \\   & Value & P-Complete & \(\) \\  

Table 1: A summary of our main results. \(\) means that the function can be represented by log-precision MLP with constant layers and polynomial hidden dimension, while \(\) means that this MLP class cannot represent the function. Blue denotes low representation complexity, and red represents high representation complexity.

In summary, our work unveils a potential hierarchy in representation complexity among different categories of RL paradigms -- where the underlying model is the most straightforward to represent, followed by the optimal policy, and the optimal value function emerges as the most intricate to represent. This insight offers valuable guidance on determining appropriate targets for approximation, enhancing understanding of the inherent challenges in representing key elements across different RL paradigms. Moreover, our representation complexity theory is closely tied to the sample efficiency gap observed among various RL paradigms. Given that the sample complexity of RL approaches often depends on the realizable function class in use [21; 42; 14; 23; 17; 58], our results suggest that representation complexity may play a significant role in determining the diverse sample efficiency achieved by different RL algorithms. This aligns with the observed phenomenon that model-based RL typically exhibits superior sample efficiency compared to other paradigms [22; 42; 45; 20; 54; 56]. Consequently, our work underscores the importance of considering representation complexity in the design of sample-efficient RL algorithms. See Appendix C.1 for more elaborations.

**Additional Related Works.** We have discussed most related works in the introduction part, and more related works are deferred to Appendix A.

**Notations.** We denote the distribution over a set \(\) by \(()\). We use \(\) and \(_{+}\) to denote the set of all natural numbers and positive integers, respectively. For any \(n_{+}\), we denote \([n]=\{1,2,,n\}\), and \(_{n}=(_{n})^{},_{n}=( _{n})^{}\).

## 2 Preliminaries

**Markov Decision Process.** We consider the finite-horizon Markov decision process (MDP), which is defined by a tuple \(=(,,H,,r)\). Here \(\) is the state space, \(\) is the action space, \(H\) is the length of each episode, \(:()\) is the transition kernel, and \(r:\) is the reward function. Moreover, when the transition kernel is deterministic, say \(( s,a)=_{s^{}}\) for some \(s^{}\). we denote \((s,a)=s^{}\). A policy \(=\{_{h}:()\}_{h=1}^{H}\) consists of \(H\) mappings from the state space to the distribution over action space. For the deterministic policy \(_{h}\) satisfying \(_{h}(|s)=_{a}\) for some \(a\), we denote \(_{h}(s)=a\). Here \(_{a}\) is the Dirac measure at \(a\). Given a policy \(\), for any \((s,a)\), we define the state value function and state-action value function (Q-function) as \(V_{h}^{}(s)=_{}[_{h^{}=h}^{H}r_{h^{}}(s_{h^{ }},a_{h^{}})\,|\,s_{h}=s],Q_{h}^{}(s,a)=_{}[_{ h^{}=h}^{H}r_{h^{}}(s_{h^{}},a_{h^{}})\,|\,s_{h}=s,a_{h}=a]\). Here the expectation \(_{}[]\) is taken with respect to the randomness incurred by the policy \(\) and transition kernels. There exists an optimal policy \(^{*}\) achieves the highest value at all timesteps and states, i.e., \(V_{h}^{^{*}}(s)=_{}V_{h}^{}(s)\) for any \(h[H]\) and \(s\). For notation simplicity, we use the shorthands \(V_{h}^{*}=V_{h}^{^{*}}\) and \(Q_{h}^{*}=Q_{h}^{^{*}}\) for any \(h[H]\).

RL encompasses diverse paradigms, including model-based RL, policy-based RL, and value-based RL, each tailored to approximate the model (\(r\) and \(\)), optimal policy \(^{*}\), and optimal value function \(Q^{*}\), respectively. See Appendix B.1 for more details regarding function approximation.

**Computational Complexity.** Our work will use some classical computational complexity theory . Specifically, we will utilize five complexity classes: \(^{0}\), \(^{0}\), \(\), \(\), and \(\). Here, \(\), \(\), and \(\) are defined in terms of the running time of Turing Machines (Definition B.1), while \(^{0}\) and \(^{0}\) are defined based on the complexity of Boolean circuits (Definition B.2). To facilitate the readers, we provide the detailed definition in Appendix B.2. The relationship between these five complexity classes is \(^{0}^{0} \). The question of whether the relationship \(^{0}\) holds as a strict inclusion remains elusive in theoretical computer science. However, it is widely conjectured that \(=\) and \(^{0}=\) are unlikely to be true.

## 3 The Separation between Model-based RL and Model-free RL

In this section, we focus on the representation complexity gap between model-based RL and model-free RL, which encompasses both policy-based RL and value-based RL.

### 3-Sat Mdp

As a warmup example to showcase the separation between model-based RL and model-free RL, we propose the 3-SAT MDPs, whose construction is closely linked to the well-known NP-complete problem 3-satisfiability (3-SAT). The formal definition of 3-SAT is as follows.

**Definition 3.1** (3-SAT Problem).: A Boolean formula \(\) over variables \(u_{1},u_{2},,u_{n}\) is in the 3-conjunctive normal form (3-CNF) if it takes the form of a conjunction of one or more disjunctions, each containing exactly 3 literals. Here, a literal refers to either a variable or the negation of a variable. Formally, the 3-CNF formula \(\) has the form of \(=_{i}(v_{i,1} v_{i,2} v_{i,3}),\) where \(\) is the index set and \(v_{i,j}\{u_{k}, u_{k}\}\) for some \(k[n]\). The 3-SAT problem is defined as follows: given a 3-CNF Boolean formula \(\), judge whether \(\) is satisfiable. Here, "satisfiable" means that there exists an assignment of variables such that the formula \(\) evaluates to \(1\).

Now we present the detailed construction of 3-SAT MDPs.

**Definition 3.2** (3-SAT MDP).: For any \(n_{+}\), let \(=\{u_{1}, u_{1},,u_{n}, u_{n}\}\) be the set of literals. An \(n\)-dimensional 3-SAT MDP \((,,H,,r)\) is defined as follows. The state space \(\) is defined by \(=^{3n}\{0,1\}^{n}(\{0\}[2n+2])\), where each state \(s\) can be denoted as \(s=(,,k)\). In this representation, \(\) is a 3-CNF formula consisting of \(n\) clauses and represented by its \(3n\) literals, \(\{0,1\}^{n}\) can be viewed as an assignment of the \(n\) variables and \(k\) is an integer recording the number of actions performed. The action space is \(=\{0,1\}\) and the planning horizon is \(H=n+2\). Given a state \(s=(,,k)\), for any \(a\), the reward \(r(s,a)\) is defined by:

\[r(s,a)=1&$ is satisfiable and $k=n+1$},\\ 0.5&,\\ 0&.\] (3.1)

Moreover, the transition kernel is deterministic and takes the following form:

\[(,,k),a=(,, n+2)&,\\ (,,1)&,\\ (,^{},k+1)&\\ (,,k+1)&.\] (3.2)

where \(^{}\) is obtained from \(\) by setting the \(k\)-th bit as \(a\) and leaving other bits unchanged, i.e., \(^{}[k]=a\) and \(^{}[k^{}]=[k^{}]\) for \(k^{} k\). The initial state takes form \((,_{n},0)\) for any length-\(n\) 3-CNF formula \(\).

The visualization of 3-SAT MDPs is given in Figure 1. We assert that our proposed 3-SAT model is relevant to real-world problems. In the state \(s=(,,k)\), \(\) characterizes intrinsic environmental factors that remain unchanged by the agent, while \(\) and \(k\) represent elements subject to the agent's influence. Notably, the agent is capable of changing \(_{n}\) to any \(n\)-bits binary string within the episode. Using autonomous driving as an example, \(\) could denote fixed factors like road conditions and weather, while \(\) and \(k\) may represent aspects of the car that the agent can control. While states and actions in practical scenarios might be continuous, they are eventually converted to binary strings in computer storage due to bounded precision. Regarding the reward structure, the agent only receives rewards at the end of the episode, reflecting the goal-conditioned RL setting and the sparse reward

Figure 1: A visualization of 3-SAT MDPs. Here, \(\) is an \(n\)-dimensional vector, \(_{0}\) and \(_{1}\) are vectors obtained by replacing the \(k\)-th element of \(\) with \(0\) and \(1\), respectively. Additionally, \(_{}\), \(^{}_{}\), and \(^{}_{}\) represent the assignments at the end of the episode.

setting, which capture many real-world problems. Intuitively, the agent earns a reward of \(1\) if \(\) is satisfiable, and the agent transforms \(_{n}\) into a variable assignment that makes \(\) equal to \(1\) through a sequence of actions. The agent receives a reward of \(0.5\) if, at the first step, it determines that \(\) is unsatisfiable and chooses to "give up". Here, we refer to taking action \(0\) at the first step as "give up" since this action at the outset signifies that the agent foregoes the opportunity to achieve the highest reward of \(1\). In all other cases, the agent receives a reward of \(0\). Using the example of autonomous driving, if the car successfully reaches its (reachable) destination, it obtains the highest reward. If the destination is deemed unreachable and the car chooses to give up at the outset, it receives a medium reward. This decision is considered a better choice than investing significant resources in attempting to reach an unattainable destination, which would result in the lowest reward.

**Theorem 3.3** (Representation complexity of 3-SAT MDP).: Let \(_{n}\) be the \(n\)-dimensional 3-SAT MDP in Definition 3.2. The transition kernel \(\) and the reward function \(r\) of \(_{n}\) can be computed by circuits with polynomial size (in \(n\)) and constant depth, falling within the circuit complexity class \(^{0}\). However, computing the optimal value function \(Q_{1}^{*}\) and the optimal policy \(_{1}^{*}\) of \(_{n}\) are both \(\)-complete under the polynomial time reduction.

The proof of Theorem 3.3 is deferred to Appendix E.1. Theorem 3.3 states that the representation complexity of the underlying model is in \(^{0}\), whereas the representation complexity of optimal value function and optimal policy is \(\)-complete. This demonstrates the significant separation of the representation complexity of model-based RL and that of model-free RL.

**Remark 3.4**.: The recent work of  raises an open problem regarding the existence of a class of MDPs whose underlying model can be represented by \(^{k}\) circuits while the optimal value function cannot. Here, \(^{k}\) is a complexity class satisfying \(^{0}^{k}\). Therefore, our results not only address this open problem by revealing a more substantial gap in representation complexity between model-based RL and model-free RL but also surpass the expected resolution conjectured in .

**Remark 3.5**.: Although Theorem 3.3 only shows that \(Q_{1}^{*}\) is hard to represent, our proof also implies that \(V_{1}^{*}\) is hard to represent. Moreover, we can extend our results to the more general case, say \(\{Q_{h}^{*}\}_{h[]}\) are \(\)-complete, by introducing additional irrelevant steps. Notably, one cannot anticipate \(Q_{h}^{*}\) to be hard to represent for any \(h[H]\) since \(Q_{H}\) reduces to the one-step reward function \(r\). This aligns with our intuition that the multi-step correlation is pivotal in rendering the optimal value functions in the "early steps" challenge to represent. Also, although we only show that \(_{1}^{*}\) is hard to represent in our proof, the result can be extended to step \(h\) where \(2 h H\), as \(_{1}^{*}\) also serves as an optimal policy at step \(h\). Finally, We want to emphasize that, since our objective is to show that \(Q^{*}=\{Q_{h}^{*}\}_{h=1}^{H}\) and \(^{*}=\{_{h}^{*}\}_{h=1}^{H}\) have high representation complexity, it suffices to demonstrate that \(Q_{1}^{*}\) and \(_{1}^{*}\) are hard to represent.

**Remark 3.6**.: Our results can be extended to stochastic MDPs and partially observable MDPs (POMDPs) via slight modifications. See Appendices H.1 and H.2 for details.

### \(\) MDP: A Broader Class of MDPs

We extend the results for 3-SAT MDPs by introducing the concept of \(\) MDP--a broader class of MDPs. Specifically, for any \(\)-complete language \(\), we can construct a corresponding \(\) MDP that encodes \(\) into the structure of MDPs. Importantly, this broader class of MDPs yields the same outcomes as 3-SAT MDPs. In other words, in the context of \(\) MDP, the underlying model can be computed by circuits in \(^{0}\), while the computation of both the optimal value function and optimal policy remains \(\)-complete. The detailed definition of \(\) MDP is provided below.

**Definition 3.7** (\(\) MDP).: An \(\) MDP is defined concerning a language \(\). Let \(M\) be a nondeterministic Turing Machine recognizing \(\) in at most \(P(n)\) steps, where \(n\) is the length of the input string and \(P(n)\) is a polynomial. Let \(M\) have valid configurations denoted by \(_{n}\), and let each configuration \(c=(s_{M},,l)_{n}\) encompass the state of the Turing Machine \(s_{M}\), the contents of the tape \(\), and the pointer on the tape \(l\), requiring \(O(P(n))\) bits for representation. Then an \(n\)-dimensional \(\) MDP is defined as follows. The state space \(\) is \(_{n}(\{0\}[2P(n)+2])\), and each \(s=(c,k)\) consists of a valid configuration \(c=(s_{M},,l)\) and in index \(k\{0\}[2P(n)+2]\) recording the number of steps \(M\) has executed. The action space is \(=\{0,1\}\) and the planning horizon is \(H=P(n)+2\). Given state \(s=(c,k)=(s_{M},,l,k)\) and action \(a\), the reward function \(r(s,a)\) is defined by:

\[r(s,a)=1&s_{M}=s_{apest}}k=P(n)+1,\\ 0.5&k=2P(n)+2,\\ 0&,\] (3.3)where \(s_{}\) is the accept state of Turing Machine \(M\). Moreover, the transition kernel is deterministic and can be defined as follows:

\[(c,k),a=(c,P(n)+2)&a=k=0,\\ (c,1)&a=1k=0,\\ (c^{},k+1)&k[P(n)]\\ (c,k+1)&k>P(n).\] (3.4)

where \(c^{}\) is the configuration obtained from \(c\) by selecting the branch \(a\) at the current step and executing the Turing Machine \(M\) for one step. Let \(_{}\) be an input string of length \(n\). We can construct the initial configuration \(c_{0}\) of the Turing Machine \(M\) on the input \(_{}\) by copying the input string onto the tape, setting the pointer to the initial location, and designating the state of the Turing Machine as the initial state. The initial state is defined as \((c_{0},0)\).

The definition of \(\) MDP in Definition 3.7 generalizes that of the 3-SAT MDP in Definition 3.2 by incorporating the nondeterministic Turing Machine, a fundamental computational mode. The configuration \(c\) and the accept state \(s_{}\) in \(\) MDPs mirror the formula-assignment pair \((,)\) and the scenario that \(()=1\) in 3-SAT MDP, respectively. To the best of our knowledge, \(\) MDP is the first class of MDPs defined in the context of (non-deterministic) Turing Machine and can encode _any_\(\)-complete problem in an MDP structure. This represents a significant advancement compared to the Majority MDP in  and the 3-SAT MDP in Definition 3.2, both of which rely on specific computational problems such as the Majority function and the 3-SAT problem. The following theorem provides the theoretical guarantee for the \(\)-complete MDP.

**Theorem 3.8** (Representation complexity of \(\) MDP).: Consider any \(\)-complete language \(\) alongside its corresponding \(n\)-dimensional \(\) MDP \(_{n}\), as defined in Definition 3.7. The transition kernel \(\) and the reward function \(r\) of \(_{n}\) can be computed by circuits with polynomial size (in \(n\)) and constant depth, belonging to the complexity class \(^{0}\). In contrast, the problems of computing the optimal value function \(Q_{1}^{*}\) and the optimal policy \(_{1}^{*}\) of \(_{n}\) are both \(\)-complete under the polynomial time reduction.

The proof of Theorem 3.8 is deferred to Appendix E.2. Theorem 3.8 demonstrates that a substantial representation complexity gap between model-based RL (\(^{0}\)) and model-free RL (\(\)-complete) persists in \(\) MDPs. Consequently, we have extended the results for 3-SAT MDP in Theorem 3.3 to a more general setting as desired. Similar explanations for Theorem 3.8 can be provided, akin to Remarks 3.4, 3.5, and 3.6 for 3-SAT MDPs, but we omit these to avoid repetition.

## 4 The Separation between Policy-based RL and Value-based RL

In Section 3, we demonstrate the representation complexity gap between model-based RL and model-free RL. In this section, our focus shifts to exploring the representation complexity hierarchy within model-free RL, encompassing policy-based RL and value-based RL. More specifically, we construct a broad class of MDPs where both the underlying model and the optimal policy are easy to represent, while the optimal value function is hard to represent. This further illustrates the representation hierarchy between different categories of RL algorithms.

### \(\) MDP

We begin by introducing the CVP MDPs, whose construction is rooted in the circuit value problem (CVP). The CVP involves computing the output of a given Boolean circuit (refer to Definition B.2) on a given input.

**Definition 4.1** (\(\) MDP).: An \(n\)-dimensional CVP MDP is defined as follows. Let \(\) be the set of all circuits of size \(n\). The state space \(\) is defined by \(=\{0,1,\}^{n}\), where each state \(s\) can be represented as \(s=(,)\). Here, \(\) is a circuit consisting of \(n\) nodes with \([i]=([i],[i],g_{i})\) describing the \(i\)-th node, where \([i]\) and \([i]\) indicate the input node and \(g_{i}\) denotes the type of gate (including \(,,,0,1\)). When \(g_{i}\{,\}\), the outputs of \([i]\)-th node and \([i]\)-th node serve as the inputs; and when \(g_{i}=\), the output of \([i]\)-th node serves as the input and \([i]\) is meaningless. Moreover, the node type of \(0\) or \(1\) denotes that the corresponding node is a leaf node with a value of \(0\) or \(1\), respectively, and therefore, \([i],[i]\) are both meaningless. The vector \(\{0,1,\}^{n}\) represents the value of the \(n\) nodes, where the value \(\) indicates that the value of this node has not been computed and is presently unknown. The action space is \(=[n]\)and the planning horizon is \(H=n+1\). Given a state-action pair \((s=(,),a)\), its reward \(r(s,a)\) is given by:

\[r(s,a)=1&[n]=1,\\ 0&\]

Moreover, the transition kernel is deterministic and can be defined as: \((,),a=(,^{ }).\) Here, \(^{}\) is obtained from \(\) by computing and substituting the value of node \(a\). More exactly, if the inputs of node \(a\) have been computed, we can compute the output of the node \(a\) and denote it as \([a]\). Then we have

\[^{}[j]=[j]&a j,\\ [a]&a=ja\\ &a=ja\]

Given a circuit \(\), the initial state of CVP MDP is \((,_{})\) where \(_{}\) denotes the vector containing \(n\)Unknown values.

The visualization of CVP MDPs is given in Figure 2. In simple terms, each state \(s=(,)\) comprises information about a given size-\(n\) circuit \(\) and a vector \(\{0,1,\}^{n}\). At each step, the agent takes an action \(a[n]\). If the \(a\)-th node has not been computed, and the input nodes are already computed, then the transition kernel of the CVP MDP modifies \([a]\) based on the type of gate \([a]\). The agent achieves the maximum reward of \(1\) only if it transforms the initial vector \(_{}\), consisting of \(n\)Unknown values, into the \(\) satisfying \([n]=1\). This also indicates that CVP MDPs exhibit the capacity to model many real-world goal-conditioned problems and scenarios featuring sparse rewards. Hence, we have strategically encoded the circuit value problem into the CVP MDP in this manner. The representation complexity guarantee for the CVP MDP is provided below, and the proof is provided in Appendix F.1.

**Theorem 4.2** (Representation Complexity of CVP MDP).: Let \(_{n}\) be the \(n\)-dimensional CVP MDP defined in Definition 4.1. The reward function \(r\), transition kernel \(\), and optimal policy \(^{*}\) of \(_{n}\) can be computed by circuits with polynomial size (in \(n\)) and constant depth, falling within the circuit complexity class \(^{0}\). However, the problem of computing the optimal value function \(Q_{1}^{*}\) of \(_{n}\) is \(\)-complete under the log-space reduction.

Theorem 4.2 illustrates that, for CVP MDPs, the representation complexity of the optimal value function is notably higher than that of the underlying model and optimal policy.

**Remark 4.3**.: We explain why \(\)-completeness is considered challenging. In computational complexity theory, problems efficiently solvable in parallel fall into class \(\). It is widely believed that \(\)-complete problems cannot be efficiently solved in parallel (i.e., \(\)). Neural networks like MLP and Transformers , which are implemented in a highly parallel manner, face limitations when addressing \(\)-complete problems. See Section 5 for details.

### \(\) MDP: A Broader Class of MDPs

We broaden the scope of CVP MDP to encompass a broader class of MDPs -- \(\) MDPs. In this extension, we can encode _any_\(\)-complete problem into the MDP structure while preserving the results established for CVP MDP in Theorem 4.2. We introduce the \(\) MDP as follows.

Figure 2: A visualization of CVP MDPs. Here, \(_{}\), which contains \(n\)Unknown values, is the initial value vector. For any state \(s\) including a circuit \(\) and a value vector \(\), choosing the action \(i\), the environment transits to \((,_{i}^{})\). Moreover, \(_{}\), \(_{}^{}\), and \(_{}^{}\) are value vectors at the end of the episode.

**Definition 4.4** (P MDP).: Given a language \(\) in \(\), and a circuit family \(\), where \(_{n}\) contains the circuits capable of recognizing strings of the length \(n\) in \(\). The size of the circuits in \(_{n}\) is upper bounded by a polynomial \(P(n)\). An \(n\)-dimensional \(\) MDP based on \(\) is defined as follows. The state space \(\) is defined by \(=\{0,1\}^{n}_{n}\{0,1,\}^{P(n)}\), where each state \(s\) can be represented as \(s=(,,)\). Here, \(\) is the circuit recognizing the strings of length \(n\) in \(\) with \([i]=([i],[i],g_{i})\) representing the \(i\)-th node where the output of nodes \(i_{1}\) and \(i_{2}\) serves as the input, and \(g_{i}\) is the type of the gate (including \(,,\), and \(\)). When \(g_{i}\{,\}\), the outputs of \([i]\)-th node and \([i]\)-th node serve as the inputs; and when \(g_{i}=\), the output of \([i]\)-th node serves as the input and \([i]\) is meaningless. Moreover, the type Input indicates that the corresponding node is the \([i]\)-th bit of the input string. The vector \(\{0,1,\}^{P(n)}\) representing the value of the \(n\) nodes, and the value Unknown indicates that the value of the corresponding node has not been computed, and hence is currently unknown. The action space is \(=[P(n)]\) and the planning horizon is \(H=P(n)+1\). The reward of any state-action \((s=(,,),a)\) is defined by:

\[r(s,a)=1&[P(n)]=1$},\\ 0&\]

Moreover, the transition kernel is deterministic and can be defined as: \((,c,),a=(,c,^{}),\) where \(^{}\) is obtained from \(\) by computing and substituting the value of node \(a\). In particular, if the inputs of node \(a\) have been computed or can be read from the input string, we can determine the output of node \(a\) and denote it as \([a]\). This yields the formal expression of \(^{}\):

\[^{}[j]=[j]&,\\ [a]&,\\ &.\]

Given an input \(\) and a circuit \(\) capable of recognizing strings of specific length in \(\), the initial state of \(\) MDP is \((,_{})\) where \(_{}\) denotes the vector containing \(P(n)\) Unknown values and \(P(n)\) is the size of \(\).

In the definition of \(\) MDPs, we employ circuits to recognize the \(\)-complete language \(\) instead of using a Turing Machine, as done in the \(\) MDP in Definition 3.7. While it is possible to define \(\) MDPs using a Turing Machine, we opt for circuits to maintain consistency with CVP MDP and facilitate our proof. Additionally, we remark that employing circuits to define \(\) MDPs poses challenges, as it remains elusive whether polynomial circuits can recognize \(\)-complete languages.

**Theorem 4.5** (Representation complexity of \(\) MDP).: For any \(\)-complete language \(\), consider its corresponding (\(n\)-dimensional) \(\) MDP \(_{n}\) as defined in Definition 4.4. The reward function \(r\), transition kernel \(\), and the optimal policy \(^{*}\) of \(_{n}\) can be computed by circuits with polynomial size (in \(n\)) and constant depth, falling within the circuit complexity class \(^{0}\). However, the problem of computing the optimal value function \(Q_{1}^{*}\) of \(_{n}\) is \(\)-complete under the log-space reduction.

The proof of Theorem 4.5 is deferred to Appendix F.2. Theorem 4.5 significantly broadens the applicability of Theorem 4.2 by enabling the encoding of any \(\)-complete problem into the MDP structure, as opposed to a specific circuit value problem. In these expanded scenarios, the representation complexity of the model and optimal policy remains noticeably lower than that of the optimal value function.

Consequently, by combining Theorems 3.3, 3.8, 4.2, and 4.5, a potential representation complexity hierarchy has been unveiled. Specifically, the underlying model is the easiest to represent, followed by the optimal policy, with the optimal value exhibiting the highest representation complexity.

## 5 Connections to Deep RL

While we have uncovered the representation complexity hierarchy between model-based RL, policy-based RL, and value-based RL through the lens of computational complexity in Sections 3 and 4, these results offer limited insights for modern deep RL, where models, policies, and values are approximated by neural networks. To address this limitation, we further substantiate our revealed representation complexity hierarchy among different RL paradigms through the perspective of the expressiveness of neural networks. Specifically, we focus on the MLP with Rectified Linear Unit (ReLU) as the activation function5 -- an architecture predominantly employed in deep RL algorithms.

**Definition 5.1** (Log-precision MLP).: An \(L\)-layer MLP is a function from input \(_{0}^{d}\) to output \(^{d_{y}}\), recursively defined as

\[_{1}=(_{1}_ {0}+_{1}),_{1}^{d_{1} d},_{1}^{d_{1}},\] \[_{}=(_{} _{-1}+_{}),_{} ^{d_{} d_{-1}},_{}^{d_{}},\] \[=_{L}_{L}+_{L},_{L}^{d_{y} d_{L}},_{L}^{d_{y}},\]

where \(2 L-1\) and \((x)=\{0,x\}\) for any \(x\) is the standard ReLU activation. _Log-precision MLPs_ refer to MLPs whose internal neurons can only store floating-point numbers within \(O( n)\) bit precision, where \(n\) is the maximal length of the input dimension.

The log-precision MLP is closely related to practical scenarios where the precision of the machine (e.g., \(16\) bits or \(32\) bits) is generally much smaller than the input dimension (e.g., 1024 or 2048 for the representation of image data). In our paper, all occurrences of MLPs will implicitly refer to the log-precision MLP, and we may omit explicit emphasis on log precision for the sake of simplicity. See Appendix B.3 for more details regarding log precision.

To employ MLPs to represent the model, policy, and value function, we encode the representation complexity gap between model-based RL and model-free RL.

**Theorem 5.2**.: The reward function \(r\) and transition kernel \(\) of \(n\)-dimensional 3-SAT MDP and NP MDP can be represented by an MLP with constant layers, polynomial hidden dimension (in \(n\)), and ReLU as the activation function.

**Theorem 5.3**.: Assuming that \(^{0}\), the optimal policy \(_{1}^{*}\) and optimal value function \(Q_{1}^{*}\) of \(n\)-dimensional 3-SAT MDP and NP MDP defined with respect to an NP-complete language \(\) cannot be represented by an MLP with constant layers, polynomial hidden dimension (in \(n\)), and ReLU as the activation function.

The proof of Theorems 5.2 and 5.3 are deferred to Appendices G.2 and G.3, respectively. Theorems 5.2 and 5.3 show that the underlying model of 3-SAT MDP and NP MDP can be represented by constant-layer perceptron, while the optimal policy and optimal value function cannot. These demonstrate the representation complexity gap between model-based RL and model-free RL from the perspective of MLP expressiveness. The following two theorems further illustrate the representation complexity gap between policy-based RL and value-based RL, and the proof are deferred to Appendices G.4 and G.5, respectively.

**Theorem 5.4**.: The reward function \(r\), transition kernel \(\), and optimal policy \(^{*}\) of \(n\)-dimensional CVP MDP and P MDP can be represented by an MLP with constant layers, polynomial hidden dimension (in \(n\)), and ReLU as the activation function.

**Theorem 5.5**.: Assuming that \(^{0}\), the optimal value function \(Q_{1}^{*}\) of \(n\)-dimensional CVP MDP and P MDP defined with respect to a P-complete language \(\) cannot be represented by an MLP with constant layers, polynomial hidden dimension (in \(n\)), and ReLU as the activation function.

Combining Theorems 5.2, 5.3, 5.4, and 5.5, we reaffirm the potential representation complexity hierarchy uncovered in Sections 3 and 4 from the perspective of MLP expressiveness. To our best knowledge, this is the first result on representation complexity in RL from the perspective of MLP expressiveness, aligning more closely with modern deep RL and providing valuable insights for practice.

**Remark 5.6**.: The results presented in this section underscore the importance of establishing NP-completeness and P-completeness in Sections 3 and 4. Specifically, constant-layer MLPs with polynomial hidden dimension are unable to simulate P-complete problems and NP-complete problems under the assumptions that \(^{0}\) and \(^{0}\), which are widely believed to be impossible. In contrast, it is noteworthy that MLPs with constant layers and polynomial hidden dimension can

   & Transition & Reward & Optimal Policy & Optimal Value \\  Input & \((_{s},_{a})\) & \((_{s},_{a})\) & \(_{s}\) & \((_{s},_{a})\) \\  Output & \(_{s^{}}\) & \(r(s,a)\) & \(_{a}\) & \(Q_{1}^{*}(s,a)\) \\  

Table 2: The input and output of the MLPs that represent the model, optimal policy, and optimal value function.

represent basic operations within \(^{0}\) (Lemma 1.6), such as the Majority function. Consequently, the model, optimal policy, and optimal value function of "Majority MDPs" presented in  can be represented by constant-layer MLPs with polynomial size. Hence, the class of MDPs presented in  cannot demonstrate the representation complexity hierarchy from the lens of MLP expressiveness.

**Applicability and Extensions of Our Theory.** As mentioned in the introduction, our representation results have implications for the **statistical complexity** in RL, as detailed in Appendix C.1. Although we have shown that the revealed hierarchy of representation complexity holds for a wide range of MDPs in theory, examining its broader applicability is essential. We discuss **more general theoretical insights** and **extension to Transformer  architecture** to Appendices C.2 and C.3.

**Experiments.** We want to emphasize that our theoretical results do not apply to all MDPs, such as the MDP with all zero rewards and complex transitions. However, these additional MDP classes may not be typical in practice and could be considered pathological examples from a theoretical standpoint. To demonstrate that our theory captures practical problems, we conduct an empirical investigation into the representation complexity of different RL paradigms across various MuJoCo Gym environments . Our empirical findings align with our theoretical conclusions. We report part of our experimental results in Figure 3, more detailed experimental description and results are deferred to Appendix D.

## 6 Conclusions

This paper studies three RL paradigms -- model-based RL, policy-based RL, and value-based RL -- from the perspective of representation complexity. Through leveraging computational complexity (including time complexity and circuit complexity) and the expressiveness of MLPs as representation complexity metrics, we unveil a potential hierarchy of representation complexity among different RL paradigms. Our theoretical framework posits that representing the model constitutes the most straightforward task, succeeded by the optimal policy, while representing the optimal value function poses the most intricate challenge. Our work contributes to a deeper understanding of the nuanced complexities inherent in various RL paradigms, providing valuable insights for the advancement of RL methodologies.

Figure 3: The approximation errors computed by employing MLPs with varying depths \(d\) and widths \(w\) to approximate the transition kernel, reward function, optimal policy, and optimal Q-function in four MuJoCo environments. In each subfigure, the title indicates the configuration including hidden dimensions, number of layers, and dataset size. The x-axis lists the four MuJoCo environments, where H.C. represents HalfCheetah and I.P. represents InvertedPendulum. The y-axis represents the approximation error defined in (D.1).