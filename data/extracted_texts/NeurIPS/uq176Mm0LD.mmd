# Scene-adaptive Knowledge Distillation for Sequential Recommendation via Differentiable Architecture Search

Scene-adaptive Knowledge Distillation for Sequential Recommendation via Differentiable Architecture Search

 Lei Chen\({}^{1}\), **Fajie Yuan\({}^{2}\), **Jiaxi Yang\({}^{3}\), **Chengming Li\({}^{4}\), **Min Yang\({}^{3}\)**

\({}^{1}\)The University of Hong Kong \({}^{2}\)Westlake University

\({}^{3}\)Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences

\({}^{4}\)Shenzhen MSU-BIT University

lchen@cs.hku.hk, yuanfajie@westlake.edu.cn

{jx.yang, min.yang}@siat.ac.cn, licm@smbu.edu.cn

Min Yang is the corresponding author.

###### Abstract

Sequential recommender systems (SRS) have become a research hotspot due to their power in modeling user dynamic interests and sequential behavioral patterns. To maximize model expressive ability, a default choice is to apply a larger and deeper network architecture, which, however, often brings high network latency when generating online recommendations. Naturally, we argue that compressing the heavy recommendation models into middle- or light-weight neural networks that reduce inference latency while maintaining recommendation performance is of great importance for practical production systems. To realize such a goal, we propose AdaRec, a knowledge distillation (KD) framework which compresses knowledge of a teacher model into a student model adaptively according to its recommendation scene by using differentiable neural architecture search (NAS). Specifically, we introduce a target-oriented knowledge distillation loss to guide the network structure search process for finding the student network architecture, and a cost-sensitive loss as constraints for model size, which achieves a superior trade-off between recommendation effectiveness and efficiency. In addition, we leverage earth mover's distance (EMD) to realize many-to-many layer mapping during knowledge distillation, which enables each intermediate student layer to learn from other intermediate teacher layers adaptively. Extensive experiments on three real-world recommendation datasets demonstrate that our model achieves significantly better accuracy with notable inference speedup compared to strong counterparts, while discovering diverse architectures for sequential recommendation models under different recommendation scenes.

## 1 Introduction

Sequential (a.k.a. session-based) recommender systems (SRS) that aim to predict new interactions based on user historical ones have attracted much attention in recent years . In particular, with the tremendous success of deep learning, deep neural network (DNN) based sequential recommendation (SR) models have yielded substantial improvements comparing to traditional collaborative filtering (CF) , such as neighborhood methods  and shallow factorization models . This is because with many hidden layers, well-designed deep models could be more powerful in capturing user dynamic interests, high-level or long-range sequential relations of user interactions. More recently, Chen _et al._ revealed that highly expressive deep SR models such as NextItNet , SASRec  and BERT4Rec  could be stacked in a surprised depth with over _100 layers_ for achieving their optimal performance.

However, a real problem arises as these deep SR models go bigger and deeper; that is, the model becomes too large in parameter size, and both memory and inference costs increase sharply, making the deployment of them difficult in production systems. Thereby, we argue that compressing the heavy deep SR models into moderate- or light-weight neural networks without sacrificing their accuracy is of crucial importance for practical usage. Knowledge distillation (KD)  as an effective compression technique has been recently investigated in the recommender systems domain [35; 25; 10]. By transferring useful knowledge from a big teacher network to the student network, large deep models could be slimmed into a smaller and shallower structure without performance degradation. However, existing KD methods basically distill the teacher model into a fixed-structure student model that is manually designed in advance. This potentially limits the flexibility and scalability of the student model, especially for diverse and relatively complicated scenarios in recommender systems. For example, the optimal structure for music recommendation might be different from the optimal structure for E-commerce recommendation. Ideally, we hope to build an adaptive student model whose optimal structure takes full consideration of the specific recommendation scenarios.

Inspired by the success of automated machine learning (AutoML), we propose a novel KD method to compress the deep SR models, termed as AdaRec. AdaRec distills the knowledge of a teacher model into a student model adaptively according to the recommendation scene based on differentiable neural architecture search (NAS) [21; 40; 2; 48]. Specifically, we devise a target-oriented KD loss to provide search supervision for learning the architecture of student network, and a cost-sensitive loss as additional regularizer to constrain the model size, which achieve a superior trade-off between recommendation effectiveness and efficiency. In addition, we leverage earth mover's distance (EMD) to realize effective many-to-many layer mapping during the distillation process, enabling each intermediate layer of student to learn from any other intermediate layers of its teacher. It is worth noting that, our method is a generic KD framework which can directly apply to a broad class of well-known deep SR models, such as NextItNet  and SASRec . In addition, with the well-designed NAS architecture, our method can distill the deep SR models into effective smaller models with diverse network architectures, according to the specific recommendation scenarios.

Our main contributions in this paper are fourfold:

* To the best of our knowledge, we are the first to consider combining KD and NAS in the SRS tasks so as to compress many advanced deep SR models adaptively according to their recommendation scenes.
* We devise a KD loss based on EMD and a cost-sensitive constraint to achieve a trade-off between recommendation effectiveness and efficiency.
* AdaRec is model-agnostic and potentially applicable for any SR model with a deep network architecture. We verify the universality of the AdaRec framework by performing KD with two well-known teacher models, namely, NextItNet  and SASRec .
* We conduct extensive experiments on three real-world recommendation datasets with different scenarios (E-commerce, music and movie recommendation), demonstrating that AdaRec achieves significantly better accuracy with notable inference speedup comparing to its original teacher model. Moreover, we discover diverse neural architectures of the student model in different recommendation scenarios or tasks.

## 2 Related Work

### Deep Sequential Recommendation

SRS is an important branch in the recommendation field and has become a hotspot recently due to the wide range of application scenarios and huge commercial values. Noticeably, DNN have achieved superior recommendation accuracy in SRS tasks due to their powerful capacity in modeling complicated and long-term user behavior relations. In general, these models could be classified into three categories, namely recurrent neural network (RNN) based, convolutional neural network (CNN) based and self-attention (SA) based methods. RNN has shown superb performance in many natural language processing (NLP) tasks and were also successfully applied into the SRS field. Specifically, Hidasi _et al._ proposed GRU4Rec, which is the first RNN-based SR model. While effective, RNN-based SR models rely heavily on the hidden states of the entire past, which cannot take full advantage of the parallel processing resources (e.g., GPU and TPU)  during training. Therefore, CNN-based and self-attention based models are proposed to mitigate such limitations [34; 44; 12; 31; 50]. Among them, Tang _et al._ proposed Caser, which embeds a sequence of user-item interactions into an "image" and learns sequential patterns as local features of the image by using wide convolutional filters. Subsequently,  proposed NextItNet, a very deep 1D temporal CNN-based recommendation model which particularly excels at modeling long-range item sequences [33; 43; 45]. In addition, self-attention based models, such as SASRec  and BERT4Rec , also showed competitive accuracy for SRS tasks. SASRec  utilized the popular self-attention mechanism to model long-term sequential semantics by encoding user's historical behaviors. Inspired by the great success of BERT  in NLP filed, Sun _et al._ proposed BERT4Rec, which uses the transformer architecture and masked language model (MLM) to learn bidirectional item dependencies for better sequential recommendations. In this paper, we present AdaRec by applying NextItNet and SASRec as teacher networks given their superior performance and very deep or wide network architectures [33; 3] in literature. With the advancement on graph neural networks (GNN), GNN-based SR models, such as SR-GNN , GC-SAN  and SGL , have also attracted attention and yielded substantial improvements in recommendation accuracy. However, given their shallow neural architectures, we simply ignore AdaRec on them.

### Knowledge Distillation

Large and deep neural networks have achieved remarkable success in recent recommendation literature [33; 3; 41]. However, the deployment of such heavy model for real production system remains a great challenge. KD [8; 17] is a representative technique for model compression and acceleration. Its basic idea is to transfer important knowledge from a big teacher network to a small student network. Specifically, Tang _et al._ proposed the first KD technique for learning to rank problems in recommender systems. However, the work only focused on distillation on very shallow neural recommendation models while its effectiveness for deep SRS keeps largely unknown.  presented a general KD framework for counterfactual recommendation with four types of distillation, namely, label-based, feature-based, sample-based and model structure-based distillation. More recently,  proposed a KD framework that forces the student network to learn from both the teacher's output and the latent knowledge stored in the teacher model. In addition, KD-based compression have also been widely studied in other domains [32; 29; 9; 17; 16; 11]. Recently, compressing pre-trained language models (e.g., BERT) with KD has attracted attentions as well, and many novel models are proposed to effectively distill BERT from different perspectives (e.g., embedding layer, hidden layers and prediction layer), such as PKD-BERT , DistilBERT , TinyBERT  and BERT-EMD .

### Neural Architecture Search

NAS that automatically discovers the network architecture, has gained increasing attention recently. Early NAS methods based on reinforcement learning (RL)  and evolution  are computationally very expensive. Recent studies significantly speed up the search and evaluation stages by architecture parameter sharing, such as ENAS , gradient-descent based DARTS [21; 2] and SNAS , and hardware-aware optimization such as AMC  and FBNet [37; 36]. Different from existing work, we devise a target-oriented KD loss to provide search supervision for learning the architecture of the student network, which is a joint search of student structure and knowledge transfer under the guidance of the teacher model. To our best knowledge, we are the first to propose a combination of KD and NAS for compressing the deep SR models.

## 3 Our Method

We introduce a novel scene-adaptive KD-based model compression approach with differentiable NAS, called AdaRec. Formally, suppose that a large teacher model \(\) is trained on a target dataset \(D\), and the architecture searching space is denoted as \(\). The goal of AdaRec is to automatically find a high-performing student model \(\) from \(\) with a small number of learning parameters.

Figure 1 illustrates the overview of the AdaRec framework. The basic idea is to distill knowledge from a heavy teacher recommendation model \(\) to a small student model \(\) adaptively subject to the specific recommendation task. By using such a supervisions from the teacher, the student can achieve comparable performance to the teacher with faster inference time. In this paper, we specify AdaRec using NextItNet  and SASRec  as the teacher models given their superior recommendation performance. It is noteworthy that the "teacher" is model-agnostic and potentially applicable for any SR model with a deep network architecture. Specifically, the network structures of the student model are automatically searched based on the NAS techniques. To this end, we devise a KD loss to provide search supervision for learning the architecture of the student model and a cost-sensitive loss function as search regularization to control the model size. In this manner, our AdaRec could achieve an impressive trade-off between recommendation accuracy and computational efficiency for SRS tasks.

In what follows, we describe AdaRec by elaborating the teacher model, the student model, the KD process and the NAS searching process.

### Teacher Model

We employ the block-wise (e.g., ResNet ) deep networks as the teacher models given their powerful performance in literature. The general framework of the teacher model consists of the bottom embedding layer, hidden layers and the softmax layer. Specifically, each item \(x_{i}\) in the user interaction sequence is converted into an embedding \(_{i}\), and correspondingly the interaction sequence could be represented by an embedding matrix \(=[_{1}_{n}]\). Afterwards, we pass \(\) into the hidden layers, and obtain the final hidden representation \(^{t d}\) where \(d\) denotes the embedding dimension. Finally, we apply a softmax function to predict the output probability of the interested item \(x_{n+1}\) as follows:

\[p(x_{n+1}|x_{1:n})=(_{2}+_{2})\] (1)

where \(_{2}\) and \(_{2}\) denote the mapping matrix and the bias term respectively.

In terms of the hidden layers, we use the residual blocks from NextItNet  and SASRec  for case study, where NextItNet is based on the dilated CNN blocks, while SASRec is based on the self-attention blocks. The residual block structures are depicted in Figure 1.

Regarding the training of the two teacher models, we follow their original paper by optimizing NextItNet and SASRec using the left-to-right autoregressive (AR) method [44; 12].

Figure 1: Model architecture of AdaRec. The proposed AdaRec consists of two primary components: a teacher model and a student model.

### Student Model

Typical model compression methods usually apply KD to transfer knowledge from the heavy teacher network to the manually designed student network, which rely heavily on the prior knowledge of human experts to design the structure of the student model. In this paper, we automatically search the architecture of the student model using NAS techniques rather than designing a fixed network architecture in advance. In addition to learning from the training data with cross-entropy loss, we devise a scene-adaptive KD loss (see Section 3.3) to learn an effective student model by learning from the teacher model. We also employ an efficiency constraint (see Section 3.4) to explicitly takes the efficiency of the student model into the main objective.

Here, we introduce a block-based micro architecture searching method , which discovers an optimal network architecture from the pre-defined operation sets (i.e., search space).

Search SpaceThe search space design is key to the final performance of the searched student model. In this study, the large search space of NAS is modularized into blocks so as to reduce the search complexity, similar to . We merely need to automatically search several block structures, and the whole network architecture can be constructed by repeatedly stacking the searched blocks. In this way, we do not need to learn each block from scratch by sharing the structures of all blocks, and therefore less time is required to learn the best performing student. Specifically, we represent each searched block denoted by \(_{c}\) as a directed acyclic graph (DAG). Each node within the block represents a latent state \(h\), and the edge from the node \(i\) to the node \(j\) denotes the operation \(o_{i,j}\) transforming \(h_{i}\) to \(h_{j}\). For the \(k\)-th (\(k>1\)) searched block, we define an input node \(c_{k-1}\) and an output node \(c_{k}\), where the output node is computed by attentively summarizing the intermediate nodes. Formally, suppose \(\) to be the candidate operations, and there are \(M\) intermediate nodes in the topological order, i.e., \(o_{i,j}\) exists when \(j 1\) and \(i<j\). Hence, we define the search space \(\) as follows:

\[=_{c}=[o_{0,1},o_{0,2},o_{1,2},,o_{i,j},,o_{M,M+1}]\] (2)

Operation SetFor both of the two teacher (base) models (NextItNet and SASRec), we adopt the same operation set to search the student network architecture. In this paper, we employ lightweight CNN-based operations as candidates due to their superior accuracy and computational efficiency in the SRS literature, compared to RNN  and SA  based models. Concretely, the candidate operations \(\) contain four kinds of operations: "_convolution_", "_pooling_", "_skip connection_" and "_zero_" operations. The "_convolution_" operations include the 1D convolution, standard convolutions (without dilation), casual dilated convolutions  with kernel size \(\{3,5\}\). Note that the dilated convolution is used to capture long-term dependency information. The "_pooling_" operations include the average pooling and the max pooling with kernel size \(3\). The "_skip connection_" is utilized for the residual connections. The "_zero_" operation represents the absence of connection between nodes.

### Scene-adaptive Knowledge Distillation

We devise a KD constrain with EMD, which uses the the teacher network to guide the network architecture search of the student model.

#### 3.3.1 Embedding Layer Distillation

The prediction accuracy of the SR models, such as NextItNet, can be largely improved by increasing the embedding dimension . Compressing item embedding matrices without reducing the recommendation performance is vital for online inference speedup and parameter reduction. We define \(_{}\) as the distillation loss of the embedding layer, where it is minimized by the mean squared error (MSE) between the teacher network and the student network:

\[_{}=(^{T},^{ S}_{e})\] (3)

where \(^{T}\) and \(^{S}\) represent the item embedding matrices of teacher and student models, respectively. \(_{e}\) is a learnable projection parameter.

#### 3.3.2 Prediction Layers Distillation

The student model is encouraged to match the prediction ability of the teacher model by learning from the probability logits of the teacher. We define \(_{}\) using Kullback-Leibler (KL) divergence  asthe distillation loss of the prediction layer:

\[_{}=(^{T},^{S})\] (4)

where \(^{T}\) and \(^{S}\) are probability logits after passing through the softmax layer of the teacher & student models, respectively.

#### 3.3.3 Hidden Layers Distillation

Generally, the teacher model and the student model have different numbers of hidden layers, therefore it is not effective to employ the general one-to-one layer mapping techniques in KD. Here, we employ the EMD  algorithm to encourage each student hidden layer to learn from multiple teacher layers adaptively. EMD measures the distance between the teacher network and the student network as the minimum cumulative cost of knowledge transfer .

The key idea is to treat the hidden layers as distributions, and the desired transformation makes the teacher and student distributions close. Formally, let \(^{T}=\{(_{1}^{T},w_{I_{1}}^{}), ,(_{N}^{T},w_{I_{N}}^{})\}\) be the hidden layers of teacher model and \(^{S}=\{(_{1}^{S},w_{S_{1}}^{}), ,(_{K}^{S},w_{S_{K}}^{})\}\) be the hidden layers of student model, where \(_{i}^{T}\) and \(_{j}^{S}\) represent the \(i\)-th and \(j\)-th hidden layer of the teacher and student models, \(w_{I_{i}}^{}\) and \(w_{S_{j}}^{}\) are corresponding layer weights, \(N\) and \(K\) represent the number of hidden layers in the teacher and student models, respectively. We define a "ground" distance matrix \(^{}=[d_{ij}^{}]\), where \(d_{ij}^{}\) represents the cost of transferring the knowledge of hidden states from \(_{i}^{T}\) to \(_{j}^{S}\). We adopt KL divergence to calculate the distance \(d_{ij}^{}\):

\[d_{ij}^{}=(_{i}^{T},_{j}^{S} _{h})\] (5)

where \(_{h}\) is a learnable projection parameter.

Then, a mapping flow matrix \(^{}=[f_{ij}^{}]\), with \(f_{ij}^{}\) the mapping flow between \(_{i}^{T}\) and \(_{j}^{S}\), is learned by minimizing the cumulative cost required to transfer knowledge from \(^{T}\) to \(^{S}\):

\[(^{T},^{S},^{} )=_{i=1}^{N}_{j=1}^{K}f_{ij}^{}d_{ij}^{}\] (6)

subject to the following constraints:

\[f_{ij}^{} 0 1 i N,1 j K\] (7)

\[_{j=1}^{K}f_{ij}^{} w_{T_{i}}^{} 1 i N\] (8)

\[_{i=1}^{N}f_{ij}^{} w_{S_{j}}^{} 1 j K\] (9)

\[_{i=1}^{N}_{j=1}^{K}f_{ij}^{}=(_{i}^{N}w_{T_{i }}^{},_{j}^{K}w_{S_{j}}^{})\] (10)

After we solve the aforementioned optimization problem, an optimal mapping flow \(^{}\) can be learned. Then, we define the EMD by normalizing the work over the total flow:

\[(^{S},^{T})=^{N} _{j=1}^{K}f_{ij}^{}d_{ij}^{}}{_{i=1}^{N}_{j=1}^ {K}f_{ij}^{}}\] (11)

Finally, the hidden-layer distillation loss (termed as \(_{}\)) can be defined by the EMD between \(^{T}\) and \(^{S}\):

\[_{}=(^{S},^{T})\] (12)

#### 3.3.4 Knowledge Distillation Loss

By combining the above three distillation objectives (\(_{}\), \(_{}\), \(_{}\)), we can unify the KD loss \(_{KD}\) between the teacher and student networks:

\[_{}=_{}+_{}+ _{}\] (13)

### Efficiency Constraint

We also devise an efficiency constraint, which explicitly takes the efficiency of the student model into the main objective to achieve a trade-off between recommendation effectiveness and efficiency. Specifically, we define a cost-sensitive loss by considering both the parameter size and inference time:

\[_{E}=_{o_{i,j}_{c}}SIZE(o_{i,j})+FLOPs(o_ {i,j})\] (14)

where \(SIZE()\) denotes the size of normalized parameters. For each operation, we use \(FLOPs()\) to denote the number of floating point operations (FLOPs). We summarize the FLOPs of the searched operations to approximate the actual inference time of the student model.

### Overall Training Procedure

Following the common paradigms of previous KD methods, we first pre-train the large teacher network. Then, the network architecture of the light student is searched automatically with the guidance of the pre-trained teacher. When searching the student architecture, we combine the KD loss \(_{}\) and the cost-sensitive loss \(_{}\). In addition, we also need to incorporate the cross-entropy loss (\(_{CE}\)) learned on the training data to help search the student architecture. We define the cross-entropy loss function as follows:

\[_{CE}=-_{X^{u}}p(x^{u}_{t+1}) p(^{u}_{t +1})\] (15)

where \(\) represents the whole user-item interaction sequences in the training data, \(p(x^{u}_{t+1})\) is the ground truth distribution for next item prediction and \(p(^{u}_{t+1})\) is the prediction distribution of the searched student model.

The overall loss function is defined as follows:

\[=(1-)_{CE}+_{KD}+_{E}\] (16)

where \(\) and \(\) denote the hyperparameters for balancing the three loss functions.

After finishing the joint searching of the student network architecture and knowledge transfer with the supervision of the pre-trained teacher, we can derive an effective, efficient and adaptive architecture as the compressed SR model by stacking the searched block structures.

#### 3.5.1 Differentiable Optimization

It is difficult, if not impossible, to directly optimize the objective function in Eq. (16) by using a brute-force algorithm to enumerate over all candidate operations because of the huge combinatorial searching operations. To resolve such an issue, we model the search operation \(o_{i,j}\) as discrete variables (one-hot variables) complying to discrete probability distributions \(P_{o}=[^{o}_{1},,^{o}_{||}]\). Afterwards, we employ the Gumbel-Softmax distribution  to convert categorical samples into continuous distributions \(y^{o} R^{}\) as follows:

\[^{o}_{i}=_{i})+g_{i })/]}{_{j=1}^{||}[(( ^{o}_{j})+g_{j})/]}\] (17)

where \(g_{i}\) is a random noise drawn from Gumbel(0, 1) distribution, \(\) is a temperature coefficient controlling the discreteness of the output vectors \(^{o}\). In this way, we can optimize the objectives \(_{KD}\) and \(_{E}\) directly using gradient-based optimizers by using the discrete variable \(argmax(^{o})\) in the forward pass and using the continuous vector \(^{o}\) in the back-propagation stage.

## 4 Experimental Setup

### Experimental Datasets

We conduct extensive experiments on three real-world SRS datasets from three different domains (scenes): RetailRocket from the E-commerce domain, 30Music from the music domain , and MovieLens-2K from the movie domain . The statistics of them are provided in Table 1.

[MISSING_PAGE_FAIL:8]

among the three datasets, and show competitive performance with SR-GNN, which is consistent with the previous work [44; 12]. Second, AdaRec with NextItNet and SASRec as teacher models attain better recommendation accuracy than their teachers, although we do not expect AdaRec beats its teacher model in accuracy. For example, on RetailRocket and 30Music, AdaRec with NextItNet as the teacher model obtains 2.9\(\%\) and 3.2\(\%\) improvements over its large teacher model in terms of MRR@5. Importantly, AdaRec requires much fewer parameters and achieves notable inference speedup relative to its teachers. In addition, compared to the standard KD method  with equivalent model size, AdaRec with NAS techniques performs substantially better with higher inference speedup.

### Cross-Scene Evaluation

In this section, we investigate the scene-adaptivity of AdaRec with different recommendation scenarios. We apply the searched student architecture from one recommendation scenario to other scenarios. For example, we denote the searched student architecture for RetailRocket (i.e., E-commerce domain) with NextItNet as the teacher model as AdaRec-RetailRocket, and apply it to 30Music (i.e., music domain) and ML-2K (i.e., movie domain). For such cross-scenario validation, we randomly initialize the weights of each searched student structure and re-train it using corresponding training data and the same teacher model to ensure a fair comparison. The results are summarized in Table 3, where we omit results using SASRec as teacher models due to similar behaviors. As clearly demonstrated along the diagonal line of Table 3, we can draw that AdaRec achieves the best performance on their original recommendation scenarios in contrast to other scenarios. This is, AdaRec is scene-adaptive since the searched student network only guarantees its optimal performance on a specific recommendation scenario.

### Architecture Visualization

To better understand the basic blocks of the searched student architectures, we visualize them on the three recommendation scenarios in Figure 2. For space reason, we still only show AdaRec with NextItNet as the teacher model. By comparing the searched structures for different recommendation scenarios, we can find that AdaRec for RetailRocket (from E-commerce domain) and 30Music (from music domain) are relatively lightweight, since fewer convolution operations (i.e., \(std\_cnn\_3\) for RetailRocket and \(cau\_cnn\_3\) for 30Music) are used. This is likely because the two datasets have short-range sequential dependencies. On the contrary, a more complicated student structure with diverse convolution operations(i.e., \(std\_cnn\_3\) and \(cau\_cnn\_3\)) is learned for ML-2K so as to model the long-range dependencies. The above results well back up our claim that the proposed AdaRec is able to search adaptive student structures for different recommendation scenarios.

    &  &  &  \\   & MRR@5 & HR@5 & MRR@5 & HR@5 & MRR@5 & HR@5 \\  AdaRec (All) & **0.7345** & **0.7964** & **0.6343** & **0.7151** & **0.4489** & **0.6519** \\ w/o \(_{KD(emb)}\) & 0.7239 & 0.7886 & 0.5976 & 0.6899 & 0.4325 & 0.6313 \\ w/o \(_{KD(pred)}\) & 0.6898 & 0.7583 & 0.5512 & 0.6218 & 0.2949 & 0.4729 \\ w/o \(_{KD(hidden)}\) & 0.7142 & 0.7806 & 0.6112 & 0.6981 & 0.4351 & 0.6430 \\ w/o \(_{CE}\) & 0.7115 & 0.7804 & 0.5966 & 0.6883 & 0.4391 & 0.6407 \\   

Table 4: Performance comparison on the three datasets for loss ablation studies by using NextItNet as the teacher model.

    &  &  &  \\   & MRR@5 & HR@5 & MRR@5 & HR@5 & MRR@5 & HR@5 \\  AdaRec-RetailRocket & **0.7345** & **0.7964** & 0.6164 & 0.6956 & 0.3244 & 0.4983 \\ AdaRec-30Music & 0.7333 & 0.7953 & **0.6343** & **0.7151** & 0.3969 & 0.5951 \\ AdaRec-ML-2K & 0.7283 & 0.7926 & 0.6248 & 0.7056 & **0.4489** & **0.6519** \\   

Table 3: Performance comparison on the three datasets for cross-scenario validation by using NextItNet as the teacher model.

### Ablation Studies

As described before, the loss \(\) of AdaRec consists of three parts: the target-oriented KD loss \(_{KD}\), the cost-sensitive loss \(_{E}\) and the standard cross-entropy loss \(_{CE}\). First, we evaluate the effects of \(_{KD}\) and \(_{CE}\) by removing each of them independently, as reported in Table 4. Clearly, we find that AdaRec without each of the two losses yields sub-optimal recommendation accuracy on all three datasets. Besides, it also shows that combining distillation losses on the embedding layer \(_{}\)-prediction layer \(_{}\) and hidden layers \(_{}\) together produces the best results.

In addition, we verify the effect of the cost-sensitive loss \(_{E}\) by varying \(\), including the default case \(=8\), without constraint \(=0\), weak constraint \(=4\) and strong constraint \(=16\). The model performance and corresponding model size are illustrated in Figure 3. From the results we can see that no constraint or a small value of \(\) lead to an increased model size; meanwhile, an aggressive \(\) results in a smaller model size but degraded model accuracy on the other hand. An appropriate constraint (\(=8\)) achieves the superior trade-off between the model effectiveness and efficiency.

## 6 Conclusion

In this paper, we propose a novel SR framework AdaRec based on the differentiable NAS. AdaRec compresses the knowledge of large and deep SR models into a compact student model according to their recommendation scenes. AdaRec is the first study that considers applying both KD and NAS in the SRS tasks when performing scene-adaptive knowledge compression. In addition, we devise the EMD-based KD method for effective transfer of deep hidden layers between the teacher model and the student model. A cost-sensitive constraint is introduced to achieve the trade-off between effectiveness and efficiency of SR models. Comprehensive experiments on three benchmark recommendation corpora from different scenarios show that AdaRec obtains considerably better performance compared to the standard KD baseline and its teacher model while accelerating inference time and reducing the computational workload.