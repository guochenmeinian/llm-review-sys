# Leveraging Contrastive Learning for Enhanced Node Representations in Tokenized Graph Transformers

Jinsong Chen\({}^{1,2,3}\), Hanpeng Liu\({}^{1,3}\), John E. Hopcroft\({}^{3,4}\), Kun He\({}^{1,3}\)

\({}^{1}\)School of Computer Science and Technology, Huazhong University of Science and Technology

\({}^{2}\)Institute of Artificial Intelligence, Huazhong University of Science and Technology

\({}^{3}\)Hopcroft Center on Computing Science, Huazhong University of Science and Technology

\({}^{4}\)Department of Computer Science, Cornell University

{chenjinsong,hanpengliu}@hust.edu.cn,

jeh@cs.cornell.edu, brooklet60@hust.edu.cn

Corresponding author.

###### Abstract

While tokenized graph Transformers have demonstrated strong performance in node classification tasks, their reliance on a limited subset of nodes with high similarity scores for constructing token sequences overlooks valuable information from other nodes, hindering their ability to fully harness graph information for learning optimal node representations. To address this limitation, we propose a novel graph Transformer called GCFormer. Unlike previous approaches, GCFormer develops a hybrid token generator to create two types of token sequences, positive and negative, to capture diverse graph information. And a tailored Transformer-based backbone is adopted to learn meaningful node representations from these generated token sequences. Additionally, GCFormer introduces contrastive learning to extract valuable information from both positive and negative token sequences, enhancing the quality of learned node representations. Extensive experimental results across various datasets, including homophily and heterophily graphs, demonstrate the superiority of GCFormer in node classification, when compared to representative graph neural networks (GNNs) and graph Transformers.

## 1 Introduction

Node classification, a crucial machine learning task in graph data mining, has garnered significant attention recently due to its wide applicability in diverse areas such as social network analysis . Among numerous techniques developed for this task, graph neural networks (GNNs) stand out as the leading architecture due to their exceptional ability to model graph structural data.

Built on the message-passing mechanism , GNNs  efficiently integrate node and graph topology features to learn informative node representations, effectively preserving both attribute and structural information. However, as research on GNNs progresses, inherent limitations of the message-passing framework, such as over-smoothing  and over-squashing , have emerged. These limitations hinder GNNs' ability to capture long-range dependencies in graphs, ultimately constraining their potential for node classification.

Recently, the emerging graph Transformer has attracted great attention in the field of graph representation learning. The crux of this approach is to leverage the Transformer architecture to learn node representations from the input graph. Benefiting from the self-attention mechanism in Transformer, graph Transformers  can effectively capture the long-range dependencies in graphs. Serving as a new deep learning-based technique for graphs, graph Transformers have showcasedremarkable performance in the node classification task. In this study, We roughly divide the existing graph Transformers designed for node classification into two categories according to the model architecture: GNN-based graph Transformers and tokenized graph Transformers.

GNN-based graph Transformers [30; 42; 41; 23; 27] utilize a hybrid framework that merges Transformer layers with GNN-style modules to learn node representations. However, this approach may constrain the modeling capacity of the Transformer architecture due to the deeply coupled design of the Transformer and GNN layers. A recent study  also theoretically proves that directly applying Transformer to calculate the attention scores of all node pairs could cause the over-globalizing problem, which causes the model to overly rely on global information, negatively affecting the model's performance.

In contrast, tokenized graph Transformers [52; 50; 7; 13] initially generate token sequences for each node and only calculate attention scores between tokens within the token sequence, naturally avoiding the over-globalizing issue. These token sequences are then processed by a Transformer-based backbone to learn node representations. This mechanism allows the Transformer to flexibly extract informative node representations based on the input token sequences, demonstrating impressive performance in node classification. Note that, tokenized graph Transformers focus on building token sequences for each target node as model inputs, which is different from TokenGT  that transforms all elements in graphs as tokens.

Token generation is a crucial step in tokenized graph Transformers, where node  and neighborhood  elements form the core of the token sequences. While neighborhood tokens primarily preserve local topology features , node tokens can capture a broader range of graph information, including long-range dependencies and intrinsic graph properties (_e.g._, homophily and heterophily). These advantages allow graph Transformers built on node-oriented token sequences [52; 50; 13] to learn more informative node representations, compared to those based on neighborhood-oriented token sequences.

In this study, we observe that the techniques employed by existing tokenized graph Transformers for generating node-orient token sequences could be summarized as a two-step method. First, they estimate the node similarity matrix according to node information across various feature spaces, such as topology features  and attribute features [50; 13]. They then sample a fixed number of nodes with high similarity scores from the generated similarity matrix to construct the input token sequence for a target node. As depicted in Figure 1, only a small subset of nodes is considered, while other nodes are excluded during the training stage.

Compared to sampled nodes which capture the commonality with the target node, these abandoned nodes preserve the disparity, which is also valuable for learning distinguishable node representations. A previous study  has proved that leveraging the information from dissimilar nodes aids the

Figure 1: A toy example to illustrate the difference of the token generator between the token generator in our method and that used in the previous node tokenized graph Transformers. Previous methods only sample nodes with high similarity to construct token sequences. In contrast, our method introduces both positive and negative token sampling to preserve information carried by diverse nodes in the graph.

learning of node representations. Nevertheless, existing tokenized graph Transformers can not comprehensively utilize both similar and dissimilar nodes to learn the representation of the target node, inevitably limiting the model performance for node classification. Hence, a natural question arises: _How should we design a new graph Transformer to comprehensively and effectively leverage diverse nodes in graphs to learn distinguishable node representations?_

To answer this question, we propose a new method called Graph Contrastive Transformer (GCFormer). Unlike previous graph Transformers, GCFormer first introduces a novel token sequence generator that produces both positive and negative token sequences for each node in different feature spaces. In this way, various graph information carried by node tokens can be carefully preserved in different types of token sequences. Then, GCFormer develops a new Transformer-based backbone tailored for effectively learning node representations from the generated positive and negative token sequences. Finally, GCFormer leverages the contrastive learning to comprehensively utilize the tokens in both positive and negative sequences to further enhance the quality of learned node representations.

The main contributions of this paper are summarized as follows:

* We develop a new token sequence generator that can generate different types of token sequences in terms of positive and negative node tokens for each target node to preserve various graph information.
* We propose a new graph Transformer GCFormer that formulates a Transformer-based backbone and leverages the contrastive learning to comprehensively learn node representations from positive and negative token sequences.
* We conduct extensive experiments on both homophily and heterophily graphs to validate the effectiveness of the proposed method. Experimental results demonstrate the superiority of GCFormer in node classification compared to representative GNNs and graph Transformers.

## 2 Related Work

In this section, we first introduce recent studies of graph Transformers for node classification. We then briefly review studies about contrastive learning on graphs.

### Graph Transformer

We categorize existing graph Transformers for node classification into GNN-based methods and tokenized methods. The former [30; 41; 42; 23; 27] combines the Transformer layers with GNN-style modules to learn node representations. GraphGPS , one of the representative approaches, incorporates various linear Transformers, such as Reformer  and BigBird , and GNN layers  into a unified framework for graph representation learning. However, these approaches require performing the attention calculation on all node pairs, which can lead to what is known as the over-globalizing problem. A recent study  provides both empirical evidence and theoretical analysis to show that calculating attention scores for all nodes can cause the model to overly rely on global information, which can negatively affect the model's performance in node classification tasks.

In contrast, tokenized methods purely depend on the Transformer architecture. The key idea is to generate token sequences for each node from the input graph, which are then fed to Transformer to learn node representations. Node-based [52; 50; 13] and neighborhood-based [7; 11; 13] token generators have been developed to generate various token sequences for nodes. Node-based token generators first calculate the similarity of nodes according to node features such as attribute features , then sample nodes with high similarity scores as tokens of the input sequence. While neighborhood-based token generators  aggregate the features of multi-hop neighborhoods and further transform them into tokens to construct the token sequence. Compared to neighborhood-based tokens, node-based tokens can express more complex graph information, such as long-range dependencies, which are more suitable for learning informative node representations.

Different from previous node token-based graph Transformers that only consider nodes with high similarity, our proposed GCFormer generates both positive and negative token sequences from all nodes in the graph. Various graph information carried by diverse nodes in two types of token sequences enables GCFormer to learn more distinguishable node representations, leading to superior performance.

### Contrastive Learning on Graphs

Graph contrastive learning (GCL) [36; 54; 46; 31; 49] aims to introduce the contrastive learning mechanism into GNNs to learn informative representations of graphs. Most of GCL approaches share a similar framework that first performs graph augmentation techniques to generate various features of different graph views and then applies the contrastive loss on these generated views to learn graph representations . Recent studies [32; 53; 48; 51] attempt to introduce contrastive learning into graph Transformers. However, these methods require the entire graph as the model input [51; 48] or need to combine GNN-based modules with tailored graph augmentation strategies [32; 53], which are hard to directly apply on tokenized graph Transformers in the node classification task.

Our proposed GCFormer develops a new token generator to generate both positive and negative token sequences for each node without any data augmentations. With the dedicated Transformer-based backbone, GCFormer can effectively leverage the contrastive learning to comprehensively learn informative node representations from two types of token sequences.

## 3 Preliminaries

### Node Classification

Consider an attributed graph \(=(V,E)\), where \(V\) and \(E\) are the node and edge sets, respectively. We have the corresponding adjacency matrix \(^{n n}\), where \(n\) is the number of nodes. For arbitrary two nodes \(v_{i}\) and \(v_{j}\), \(_{ij}=1\) only if \(e_{ij} E\). The diagonal degree matrix \(^{n n}\) is represented as \(_{ii}=_{j=1}^{n}_{ij}\). The normalized version of the adjacency matrix with self-loops is represented as \(}=(+)^{-1/2}(+)( +)^{-1/2}\), where \(\) denotes the identity matrix. Nodes in \(\) are associated with attribute feature vectors, assembled into an attribute feature matrix denoted as \(^{a}^{n d}\) where \(d\) is the dimension of the feature vector. The node label matrix \(^{n c}\), where \(c\) is the label count, consists of rows that are one-hot vectors encoding the label of each node. Each row in \(\) is a one-hot vector representing the label information of the corresponding node. Given a subset of nodes with known labels \(V_{l}\), the objective of node classification is to infer the labels for the remaining nodes in the set \(V-V_{l}\).

### Transformer

Transformer stands as a notable model in deep learning, built upon the Encoder-Decoder architecture. This brief overview focuses on the Transformer layer, a pivotal component of the model. Each Transformer layer is composed of two essential parts: Multi-Head Self-Attention (MSA) and Feed-Forward Networks (FFN).

MSA harnesses multiple attention heads, employing the self-attention mechanism to refine the representations of input entities. Given the input feature matrix \(^{n d_{i}n}\), the calculation of the \(i\)-th attention head is as follows:

\[_{i}()=(^{ }}{}}),\] (1)

where \(=_{}\), \(=_{}\) and \(=_{}\). \(_{}^{d_{in} d_{k}}\), \(_{}^{d_{in} d_{k}}\) and \(_{}^{d_{in} d_{v}}\) are learnable parameter matrices. The output of MSA with \(m\) attention heads is calculated as:

\[^{}=(_{1}||_{2}||||_{m})_{},\] (2)

where \(||\) denotes the vector concatenation operation and \(_{}\) is the learnable matrix.

FFN, comprised of two linear layers enveloping a nonlinear activation function, is defined as:

\[^{}=((())),\] (3)

where \(()\) indicates a linear layer, and \(()\) symbolizes the nonlinear activation function.

## 4 Method

In this section, we detail our proposed GCFormer. First, we introduce the hybrid token generator, which produces both positive and negative token sequences for each node. Then, we introduce thetailored Transformer-based backbone for extracting node representations from the generated token sequences. Finally, we introduce how to integrate contrastive learning into GCFormer to enhanced node representations.

### Hybrid Token Generator

The proposed hybrid token generator contains two steps: similarity estimating and node sampling. The critical operation of similarity estimating is to calculate the similarity score matrix \(^{n n}\) of all node pairs. Obviously, different node features lead to different score matrices, describing node pairs' relations in different feature spaces. To preserve the complex relations of nodes in the graph, besides the attribute-aware feature matrix \(^{a}\), we construct the topology-aware feature matrix \(^{t}\):

\[^{t}=}^{k}^{a},\] (4)

where \(k\) is the propagation step. \(^{t}\) preserves the local topology feature within the \(k\)-hop neighborhood for each node, which is the essential information to characterize the node property on the graph [7; 16].

Then, we utilize the cosine similarity to calculate the similarity score \(^{a}^{n n}\) and \(^{t}^{n n}\) based on the node feature matrices \(^{a}\) and \(^{t}\), respectively. Given a node pair \((v_{i},v_{j})\), the similarity scores in the attribute feature space \(^{a}_{ij}\) and topology feature space \(^{t}_{ij}\) are calculated as follows:

\[^{a}_{ij}=^{a}_{i}^{a^{}}_ {j}}{|^{a}_{i}||^{a}_{j}|},^{t}_{ij}= {^{t}_{i}^{t^{}}_{j}}{|^{t}_{i}|| ^{t}_{j}|}.\] (5)

After estimating the similarity scores of all node pairs, GCFormer then conducts a two-stage sampling process involving positive token sampling and negative node sampling to generate the token sequences. Here, we introduce the sampling process based on the attribute similarity matrix \(^{a}\) for a simplified description. For a given target node \(v_{i}\), in the positive token sampling stage, we adopt the top-\(k\) strategy to select nodes to construct the positive token sequence:

\[V^{a,p}_{i}=\{v_{j}|v_{j}(^{a}_{i})\},\] (6)

where \(()\) denotes the top-\(k\) sampling function and \(V^{a,p}_{i}\) denotes the positive token sequence with length \(p_{k}\). As for the negative token sampling stage, we have the set of rest nodes for \(v_{i}\) after positive token sampling \(V^{a,r}_{i}=V-V^{a,p}_{i}\). In this paper, we regard all nodes in \(V^{a,r}_{i}\) as the negative samples since their similarity scores are below the threshold of top-\(k\) selection. Then, we apply the sampling function to sample nodes from \(V^{a,r}_{i}\) to construct the negative token sequence for \(v_{i}\):

\[V^{a,n}_{i}=\{v_{j}|v_{j}(V^{a,r}_{i})\},\] (7)

where \(()\) denotes an arbitrary sampling function. Here, we use uniform sampling for computing efficiency. \(V^{a,n}_{i}\) denotes the negative token sequence with length \(n_{k}\).

Following the same sampling process, we can obtain positive and negative token sequences \(V^{t,p}_{i}\) and \(V^{t,n}_{i}\) based on the topology similarity matrix \(^{t}\). The constructed positive and negative token sequences not only capture node relations in different feature spaces but also comprehensively extract valuable information from all nodes on the graph.

### Transformer-based Backbone

GCFormer formulates a Transformer-based backbone to effectively learn node representations from positive and negative token sequences. For a node \(v_{i}\), we first combine itself with generated positive and negative token sequences to construct the model input, \(^{a,i^{}}^{(1+p_{k}+n_{k}) d}=\{ _{i},_{p},_{n}|v_{p} V^{a,p}_{i},v_{n} V^{ a,n}_{i}\}\) and \(^{t,i^{}}^{(1+p_{k}+n_{k}) d}=\{ ^{t}_{i},^{t}_{p},^{t}_{n}|v_{p} V^{t,p}_{i},v_{n} V^{t,n}_{i}\}\). Note that we utilize the generated \(^{t}\) to construct the model input of topology-aware token sequences. In this way, the topology features can be carefully preserved in the model input \(^{t,i^{}}\), exhibiting significant differences with previous methods [52; 50; 13] that utilize the attribute features to construct topology-aware token sequences. Following previous studies [8; 7; 13], we leverage projection layers to obtain the initial input:

\[^{a,i}=^{a,i^{}}^{a},^ {t,i}=^{t,i^{}}^{t},\] (8)where \(^{a}^{d d_{0}}\) and \(^{t}^{d d_{0}}\) denote the parameter matrices of the projection layers.

Given the model input \(^{a,i}\) of the node \(v_{i}\), GCFormer first separates the negative tokens from \(^{a,i}\), resulting in two parts: \(^{a,i^{(0)}}^{(1+p_{k}) d_{0}}\) and \(^{a,i^{(0)}}^{n_{k} d_{0}}\). Next, GCFormer adds a virtual token with learnable features into \(^{a,i^{(0)}}\) as the first token to facilitate extracting valuable information from negative tokens. Then, GCFormer adopts standard Transformers layers to learn node representations from \(^{a,i^{(0)}}\) and \(^{a,i^{(0)}}\):

\[^{a,i^{(1)^{}}}=(^{a,i^{(l-1)}})+ ^{a,i^{(l-1)}},^{a,i^{(l)}}=(^{ a,i^{(l)^{}}})+^{a,i^{(l)^{}}},\] (9)

\[^{a,i^{(l)^{}}}=(^{a,i^{(l-1)}})+ ^{a,i^{(l-1)}},^{a,i^{(l)}}=(^ {a,i^{(l)^{}}})+^{a,i^{(l)^{}}},\] (10)

where \(()\) and \(()\) denote the multi-head self-attention and feed-forward networks.

Through several Transformer layers, the corresponding \(^{a,i}^{(1+p_{k}) d_{out}}\) and \(^{a,i}^{(1+n_{k}) d_{out}}\) contains information extracted from positive and negative token sequences, respectively. To effectively fuse information from different types of token sequences, inspired by signed attention mechanism in previous approaches [3; 10], we develop the following readout function:

\[^{a,i}=^{a,i}_{0}-^{a,i}_{0},\] (11)

where \(^{a,i}^{1 d_{out}}\) denote the node representation of \(v_{i}\) extracted from the attribute-aware token sequence.

The rationale of Equation 11 is that the representations \(^{a,i}_{0}\) (the target node) and \(^{a,i}_{0}\) (the virtual node) contain the learned information from positive and negative token sequences, respectively. The desired representation of \(v_{i}\) should be far away from the representations of negative tokens in the hidden feature space since there is a high probability that they belong to different labels. While the signed aggregation operation can enforce \(^{a,i}\) to be dissimilar with the representations of negative tokens according to the previous study [3; 10].

We can also obtain the representation \(^{t,i}^{1 d_{out}}\) extracting from the topology-aware token sequence \(^{t,i^{}}\) via the same operation. Considering the contributions of attribute information and topology information vary on different graphs, we develop a weighted fusion strategy to obtain the final representation \(^{i}\):

\[^{i}=^{a,i}+(1-)^{t,i},\] (12)

where \(\) is a hyper-parameter to determine the contributions of attribute information and topology information to the final representation.

### Integrating Contrastive Learning

Though Equation 11 leverages information of negative tokens to learn node representation, it fails to directly model relations between the target node and its negative tokens. To this end, we introduce the contrastive learning loss  to fully utilize negative tokens for enhanced node representations. For a node \(v_{i}\), the contrastive learning loss is calculated as follows:

\[_{cl}(v_{i})=-^{a,i}_{0}}^{a,i^{}}/)}{_{j=1}^{n_{k}}(^{a,i}_{0} ^{a,i^{}}_{j}/)}-^{t,i}_{0} }^{t,i^{}}/)}{_{j=1}^{n_{k}}( ^{t,i}_{0}^{t,i^{}}_{j}/)},\] (13)

where \(}^{a,i}=}_{j=1}^{p_{k}}^{a,i}_{j}\) and \(}^{t,i}=}_{j=1}^{p_{k}}^{t,i}_{j}\). \(\) is a temperature hyper-parameter. Equation 13 enforces the representation of the target node to be close to the central representation of all positive tokens and away from all negative samples, which promotes learning distinguishable node representations, beneficial for downstream classification tasks. We further adopt the Cross-entropy loss for node classification:

\[_{ce}=-_{i V_{l}}_{i}}_{i },}_{i}=(^{i}),\] (14)

where \(()\) denotes the Multilayer Perceptron-based classifier. Hence, the overall loss function of GCFormer is as follows:

\[=_{ce}+_{cl},\] (15)

where \(\) is the coefficient for the contrastive learning term.

## 5 Experiments

### Experimental Setup

We briefly introduce the experimental setup including datasets, baselines and parameter settings. Detailed information is provided in Appendix A due to the space limitation.

**Datasets.** We adopt eight widely used datasets, including four homophily and four heterophily graphs: Photo , ACM , Computer , Corafull , BlogCatalog , UAI2010 , Flickr  and Romanempire . The edge homophily ratio \(H()\) is adopted to evaluate the graph's homophily level. \(H() 1\) means strong homophily, while \(H() 0\) means strong heterophily. Statistics of datasets are summarized in Appendix A. Following the settings of previous studies [41; 42], we randomly choose 50% of each label as the training set, 25% as the validation set, and the rest as the test set.

**Baselines.** We adopt eleven powerful approaches on node classification as baselines, including GNNs and graph Transformers: APPNP , SGC , GPRGNN , FAGCN , ACM-GCN , SGFormer , ANS-GT , Specformer , VCR-Graphormer , GraphGPS 1 and NAGphormer . The first five are representative GNNs and others are recent graph Transformers.

**Parameter settings.** For baselines, referring to recommended settings in their official implementations, we perform hyper-parameter tuning for all models. For GCFormer, we try the dimension of hidden representations in \(\{128,256,512\}\), number of layers in \(\{1,2,3\}\), learning rate in \(\{0.01,0.005,0.001\}\), dropout rate in \(\{0.1,0.3,0.5\}\). The training process is early stopped within 50 epochs and parameters are optimized using AdamW .

### Performance Comparison

To evaluate the model performance in node classification, we run each model with different random seeds on datasets and report the average value of accuracy and the corresponding standard deviation.

Table 1 reports the results. We can observe that GCFormer achieves the best performance on all datasets, indicating the superiority of GCFormer on the node classification task. Specifically, GCFormer beats recent tokenized graph Transformers on all datasets, especially ANS-GT which is the representative method of node token sequence-based graph Transformers. This is because that GCFormer generates both positive and negative token sequences for each node, which preserve both commonality and disparity between node features. In addition, the tailored Transformer-based backbone and contrastive learning enable GCFormer to comprehensively learn distinguishable node representations from different types of token sequences, further enhancing the performance in the node classification task. Moreover, we also find graph Transformer-based baselines achieve higher accuracy values than GNN-based baselines on over half of datasets. This is because graph Transformers can

   Dataset & Photo & ACM & Computer & Corafull & BlogCatalog & UAI2010 & Flickr & Romanempire \\ \(H()\) & 0.83 & 0.82 & 0.78 & 0.57 & 0.40 & 0.36 & 0.24 & 0.05 \\  APPNP & 93.00\(\)0.55 & 93.00\(\)0.59 & 91.31\(\)0.29 & 63.37\(\)0.64 & 94.77\(\)0.15 & 76.41\(\)0.24 & 84.66\(\)0.31 & 52.96\(\)0.55 \\ SGC & 93.74\(\)0.97 & 93.24\(\)0.80 & 89.90\(\)0.11 & 62.77\(\)0.19 & 72.61\(\)0.07 & 69.87\(\)0.17 & 47.48\(\)0.40 & 34.42\(\)0.77 \\ GPRGNN & 94.57\(\)0.44 & 93.42\(\)0.30 & 90.15\(\)0.34 & 69.08\(\)0.11 & 94.36\(\)0.29 & 76.94\(\)0.64 & 85.91\(\)0.51 & 67.06\(\)0.27 \\ FAGCN & 94.06\(\)0.05 & 93.37\(\)0.34 & 83.17\(\)1.81 & 56.61\(\)2.94 & 79.92\(\)4.39 & 72.17\(\)1.57 & 82.03\(\)0.40 & 48.21\(\)1.55 \\ ACM-GCN & 94.56\(\)0.21 & 93.04\(\)1.28 & 85.19\(\)2.26 & 65.11\(\)1.39 & 94.53\(\)0.53 & 76.87\(\)1.42 & 83.58\(\)0.53 & 63.35\(\)1.80 \\  SGFormer & 92.93\(\)0.21 & 93.79\(\)0.34 & 81.65\(\)3.26 & 64.62\(\)1.20 & 94.33\(\)0.19 & 57.98\(\)3.83 & 61.05\(\)0.48 & 41.31\(\)0.83 \\ ANS-GT & 94.88\(\)0.23 & 93.29\(\)0.21 & 89.58\(\)0.28 & 67.94\(\)0.21 & 91.93\(\)0.31 & 74.76\(\)0.11 & 85.94\(\)0.25 & 73.95\(\)0.32 \\ Specformer & 95.22\(\)0.13 & 93.63\(\)1.94 & 85.47\(\)1.44 & 69.18\(\)0.24 & 94.21\(\)0.23 & 73.06\(\)0.27 & 86.55\(\)0.40 & 63.69\(\)0.61 \\ VCR-Graphormer & 95.13\(\)0.24 & 93.24\(\)0.31 & 90.14\(\)0.43 & 68.96\(\)0.28 & 93.92\(\)0.37 & 75.78\(\)0.09 & 86.23\(\)0.34 & 74.76\(\)0.03 \\ GraphGPS & 93.79\(\)0.23 & 93.31\(\)0.38 & 89.21\(\)0.28 & 62.08\(\)0.59 & 94.35\(\)0.53 & 75.44\(\)0.48 & 83.61\(\)0.57 & 68.29\(\)0.02 \\ NAGphormer & 95.47\(\)0.29 & 93.20\(\)0.39 & 90.79\(\)0.45 & 69.34\(\)0.53 & 94.42\(\)0.63 & 76.36\(\)1.01 & 86.85\(\)0.55 & 74.94\(\)0.53 \\  GCFormer & **95.65\(\)0.41** & **94.32\(\)0.40** & **92.09\(\)0.421** & **69.53\(\)0.38** & **96.03\(\)0.44** & **77.57\(\)0.48** & **87.90\(\)0.45** & **75.38\(\)0.68** \\   

Table 1: Comparison of all models in terms of mean accuracy \(\) stdev (%). The best results appear in **bold**. The second results appear in underline.

effectively preserve various graph information, such as local topology features  and long-range dependencies , revealing the potential of graph Transformers in graph mining tasks.

### Parameter Sensitivity Analysis

The token sampling size and the aggregation weight \(\) in Equation 12 are key parameters in GCFormer. The former determines the model input and the latter controls the learning of final node representations from different feature spaces. Here, we conduct experiments to analyze the influence of these parameters on model performance.

**Analysis of token sampling sizes.** To analyze the influence of different sampling sizes on model performance, we vary \(p_{k}\) and \(n_{k}\) in \(\{2,3,,10\}\) where \(p_{k}\) and \(n_{k}\) are the lengths of positive token sequences and negative token sequences. Figure 2 shows the changes in model performance across all datasets. Generally speaking, a large sampling size of negative tokens can lead to competitive model performance. For instance, \(n_{k}\) over six can enable GCFormer to achieve high accuracy on almost all datasets except ACM. This is because a large value of \(n_{k}\) is more conducive to preserving the disparity between target nodes and negative node tokens, leading to more distinguishable node representations. This phenomenon also indicates that introducing negative tokens can effectively enhance the performance of tokenized graph Transformers in node classification. In addition, GCFormer is relatively sensitive to \(n_{p}\). Half of the datasets, such as Photo and BlogCatalog, require a small value of \(n_{p}\) to achieve competitive performance. While other datasets prefer large \(n_{p}\). This is because different graphs can exhibit diverse features, including node attribute features and graph topology features, which affect the sampling of positive tokens. And a large \(n_{p}\) could introduce irrelevant nodes into positive token sequences when the features of graphs are too complex to sample relevant nodes, further hurting the performance of GCFormer.

**Analysis of \(\).** To explore the influence of \(\) on model performance, we vary \(\) in \(\{0,0.1,,1\}\) and observe the changes of model performance. \(=0\) or \(=1\) mean that we abandon the information from attribute-aware token sequences or topology-aware token sequences when generating the final node representations. Results across all datasets are shown in Figure 3. We can find that the optimal \(\) falls in \((0,1)\) for all datasets. This observation indicates that comprehensively considering the features of attribute and topology information is essential to learn distinguishable node representations. Another observation is that the model performances on graphs extracted from the same domain exhibit similar changing trends. For instance, GCFormer achieves the best performance when \(=0.5\) on BlogCatalog and Flickr, which are extracted from the social platforms. This may be because graphs extracted from the same domains exhibit similar graph topology features and node attribute features.

Figure 2: Performance of GCFormer with different sampling sizes on all datasets.

### Ablation Study

Generating negative token sequences and integrating contrastive learning loss are two key designs of GCFormer. To comprehensively validate the effectiveness of these designs, we propose four variants of GCFormer termed GCFormer-N, GCFormer-C, GCFormer-NE and GCFormer-NN. GCFormer-N removes the negative token sequences and the contrastive learning loss. GCFormer-C only removes the contrastive learning loss. GCFormer-NE retains the use of Transformer layers for learning negative token representations but only employs these representations in the contrastive learning loss (ignoring them in Equation 11). GCFormer-NN, conversely, directly uses the representations of negative tokens for contrastive learning without passing them through Transformer layers. We then run four variants on all datasets and the results are shown in Figure 4. We can observe that GCFormer beats four variants on all datasets, indicating the effectiveness of our key designs in enhancing the model performance. In addition, we can also find that GCFormer-C beats GCFormer-N on over half datasets. This phenomenon demonstrates that introducing negative token sequences can effectively improve the model performance. Nevertheless, the performances of GCFormer-C behind GCFormer-N on three citation networks. This situation reveals that different types of graphs can affect the gains of introducing negative tokens. In addition, The results demonstrate that GCFormer-NE outperforms GCFormer-NN on all datasets, indicating that leveraging the Transformer to learn representations of negative tokens can effectively enhance the benefits of introducing contrastive learning. Furthermore, GCFormer surpasses GCFormer-NE, suggesting that comprehensively utilizing the representations

Figure 4: Performances of GCFormer and its variants.

Figure 3: Performance of GCFormer with different \(\) on all datasets.

of negative tokens through the signed aggregation operation and contrastive learning can further augment the model's ability to learn more discriminative node representations.

## 6 Conclusion

In this paper, we propose GCFormer, a novel graph Transformer for node classification. GCFormer establishes a new framework of tokenized graph Transformers to effectively learn node representations. Specifically, GCFormer develops a new hybrid token generator that generates both positive and negative token sequences. Compared to previous methods that only sample nodes with high similarity as tokens, GCFormer considers diverse nodes with high and low similarity. This merit enables GCFormer to preserve both commonality and disparity between node representations. By formulating a Transformer-based backbone and integrating contrastive learning, GCFormer can comprehensively learn distinguishable node representations from different types of token sequences. Extensive experimental results on diverse graphs extracted from different domains showcase the superiority of GCFormer in node classification compared to representative GNNs and graph Transformers.

The main limitation of GCFormer is the unified sampling strategy for different types of graphs. Experimental results show that the performance of GCFormer is sensitive to the sampling size on different graphs. The phenomenon implies that an adaptive sampling strategy is required to improve the performance and stability of GCFormer on diverse graphs.