# MVSDet: Multi-View Indoor 3D Object Detection via Efficient Plane Sweeps

Yating Xu\({}^{1}\)  Chen Li\({}^{2,3}\)  Gim Hee Lee\({}^{1}\)

Department of Computer Science, National University of Singapore\({}^{1}\)

Institute of High Performance Computing, A*STAR\({}^{2}\)

Centre for Frontier AI Research, A*STAR\({}^{3}\)

xu.yating@u.nus.edu lichen@u.nus.edu gimhee.lee@comp.nus.edu.sg

Chen Li was at the National University of Singapore when this work was done.

###### Abstract

The key challenge of multi-view indoor 3D object detection is to infer accurate geometry information from images for precise 3D detection. Previous method relies on NeRF for geometry reasoning. However, the geometry extracted from NeRF is generally inaccurate, which leads to sub-optimal detection performance. In this paper, we propose MVSDet which utilizes plane sweep for geometry-aware 3D object detection. To circumvent the requirement for a large number of depth planes for accurate depth prediction, we design a probabilistic sampling and soft weighting mechanism to decide the placement of pixel features on the 3D volume. We select multiple locations that score top in the probability volume for each pixel and use their probability score to indicate the confidence. We further apply recent pixel-aligned Gaussian Splatting to regularize depth prediction and improve detection performance with little computation overhead. Extensive experiments on ScanNet and ARKitScenes datasets are conducted to show the superiority of our model. Our code is available at https://github.com/Pixie8888/MVSDet.

## 1 Introduction

Indoor 3D object detection is a fundamental task in scene understanding and has wide applications in robotics, AR/VR equipment, _etc._ Although point cloud based 3D objection methods  have achieved impressive performance, depth sensors are required to capture the data, which may not be available due to budget limitation, form factor constraints, _etc._ Recently, the more economic pipeline of 3D object detection from only posed multi-view images is gaining increasing attention. However, it is much more sophisticated to estimate geometry information from 2D images alone.

A straightforward solution to this problem is using ground truth geometry information, _e.g._ point cloud  or TSDF , to supervise the model. Built on the 3D volume representation , ImGeoNet  predicts the emptiness of each voxel by converting the ground truth point clouds to surface voxels as supervision. CN-RMA  first reconstructs 3D scenes using ground truth TSDF as supervision and then runs an existing point cloud based object detector to predict bounding boxes. Although they achieve promising performance, the precise ground truth scene geometry is hard to obtain and may not be available .

An alternative way is to learn geometry via self-supervision. The pioneer work ImVoxelNet  unprojectes 2D image features to a 3D volume representation. However, 2D features can propagate to irrelevant 3D locations since depth information is not known. NeRF-Det  relies on a Neural Radiance Field (NeRF)  to learn a density field and queries an opacity score for each voxel. Theopacity score is multiplied with voxel feature to decrease the influence of voxels in the empty space to the feature volume. Since the detection performance is completely determined by the quality of NeRF, enormous effort is spent on making NeRF generalizable and avoiding aliasing issue. Unfortunately, the geometry extracted from NeRF remains unsatisfactory due to insufficient surface constraints in the representation . Consequently, it wrongly backprojects features to voxels in the free space (as illustrated by the red dots in the example shown on the middle of Fig. 1).

In this paper, we propose MVSDet to extract geometry information from only the input multi-view images. A straightforward way is to leverage multi-view stereo algorithms [24; 25] to decide the accurate depths for correct placements of 2D features of each image to the 3D volume. However, accurate depth estimation in the plane-sweeping algorithm  requires computationally expensive sampling of many depth planes over all the multi-view images. We mitigate the computational complexity by proposing a probabilistic sampling and soft weighting mechanism to decide the possible depth locations for each pixel. Specifically, we sample multiple top scoring depth proposals in the probability volume that are most likely covering the true depth locations. Pixel features are placed onto the 3D volume only when the backprojected ray intersects at the depth locations. Since the normalized probability score indicates the confidence of the the current depth location, we use it to weigh the pixel feature before assigning the feature to its backprojected voxel center.

To further improve the depth prediction accuracy, we utilize the recent pixel-aligned Gaussian Splatting (PAGS) [3; 28] for novel view rendering as an additional supervision. PAGS predicts a 3D Gaussian primitive  for every pixel in the input views, and all the Gaussians are used to render novel views via rasterization-based splatting. Compare to NeRF that uses computation expensive volumetric sampling, Gaussian Splatting is fast and light-weight. A key to good rendering quality in PAGS is the correct positioning of the 3D Gaussians, which depends on an accurate depth prediction. As a result, by putting the Gaussians according to the depth map computed from the probability volume in the plane sweep module, the rendering loss would guide the the Gaussian centers and consequently the depths to the correct values.

In summary, our contributions are as follows:

1. We propose a probabilistic sampling and soft weighting mechanism to efficiently learn geometry without sampling many depth planes in multi-view stereo. Multiple depth proposals are sampled with the probability scores to guide the propagation of image features to 3D voxels.
2. We adopt pixel-aligned Gaussian Splatting to enhance depth prediction without much additional computation overhead, which consequently improves detection performance.
3. We conduct extensive experiments on the ScanNet and ARKitScene datasets to verify the effectivs of our method. Notably, we achieve significant improvements of \(+3.9\) and \(+5.1\) under the mAP@0.5 metric on ScanNet and ARKitScenes, respectively.

## 2 Related Work

Indoor 3D Object Detection.3D object detection for indoor scenes predicts three dimensional bounding boxes and corresponding classes by taking in 3D or 2D inputs. Point cloud is the most popular choice of 3D data for detection as it provide accurate 3D information. To detect objects

Figure 1: Comparison with NeRF-Det . The 3D voxel centers (grey dots) are overlaid with the reference scene. The red dots denotes the erroneous backprojection pixel features to the points in the free space. Compared to NeRF-Det, we show much less inaccurate backprojections.

from the irregularity and sparseness of the point cloud, VoteNet  utilizes Hough voting by points sub-sampling, voting and grouping to generate object proposals. Later methods improve VoteNet by either predicting geometric primitives  or using hierarchical graph network . 3DETR  reduces hand-coded designs of VoteNet with transformer encoder-decoder blocks. Despite their promising performance, depth sensors are required to capture the data, which is not always available due to power consumption or budget limitation.

Alternatively, detecting 3D objects from images only [16; 22; 19; 17] is a cheaper choice, but with a sacrifice of losing geometry information. Some methods guide the model to predict scene geometry using ground truth geometry, _i.e._ ground truth surface voxels  or TSDF , as supervision. However, obtaining ground truth scene geometry is troublesome. In contrast, ImVoxelNet  builds a 3D feature volume in the world coordinate, where each voxel center aggregate the corresponding features of its projected 2D pixels. Subsequently, 3D U-Net is applied to refine the volume features and predict bounding boxes from each voxel center. However, voxels may wrongly aggregate irrelevant image features since depth is not known. Recently, NeRF-Det  uses NeRF to learn a density field and predict an opacity score per voxel center to downweigh the influence of voxels in the empty space to the feature volume. However, the implicit modeling of geometry in NeRF leads to unsatisfactory performance. Moreover, the reliance of NeRF during training side-tracked the effort in making NeRF generalizable. Instead of implicitly modeling geometry with NeRF, we utilize plane-swept cost volumes for geometry-aware scene reasoning. We propose a probabilistic sampling and soft weighting mechanism to accurately decide the placement of pixel features without sampling many depth planes.

Multi-View Depth Estimation.Multi-view depth estimation has long been studied in the multi-view stereo [24; 20; 25; 5; 26; 11; 7]. MVSNet  constructs a 3D cost volume, regularize it with a 3D CNN and regresses the depth map from the probability volume. However, MVSNet consumes large memory due the expensive 3D cost volume. Follow-up works reduce computation by replacing 3D CNN with recurrent network [23; 25] or using coarse to fine depth estimation [5; 20; 2; 7]. Recurrent methods only reduces cost regularization module to a 2D network, but still faces a large 3D cost volume to predict accurate depth. Although coarse to fine pipeline can reduce the number of sampled depth planes, it still requires to sample a certain amount of initial depth locations, _e.g._ 32 to 64 planes [7; 20], to get a reasonable coarse depth map, which still leads to intractable computation in our multi-view object detection task. Bae _et al_.  propose to sample extremely few number (_i.e._ 5) of initial depth planes based on a pre-trained single view depth probability distribution. It has been further applied to outdoor multi-view object detection . However, both of them need the guidance of ground truth depth to learn a correct monocular depth estimation, and would otherwise fail as shown in the experiment section. Instead, we propose a probabilistic sampling and soft weighting module to bypass the large 3D cost volume to decide the 3D locations of every pixel. We also novelly utilize Gaussian Splatting to enhance our depth prediction with little computation overhead.

3D Gaussian Splatting.3D Gaussian Splatting (3DGS)  is a recent technique for novel view rendering. It models a 3D scene explicitly with Gaussian primitives, each of which is defined by a 3D Gaussian center, covariance matrix, opacity and color. Compared to volume rendering based NeRF , it achieves real-time rendering with much lighter computation. To avoid per-scene optimization of 3DGS, several works [28; 3; 6; 18] propose to model a scene with pixel-aligned Gaussians Splatting (PAGS). A Gaussian primitive is predicted per pixel, and all predicted Gaussians are then combined for novel view synthesis. A key to PAGS is the accurate 3D Gaussian center which is determined by the depth estimation. Thus, instead of targeting novel view synthesis, we novelly utilize it for 3D object detection through improving depth prediction. Our method is also significantly different from NeRF-Det. While NeRF plays a major role in NeRF-Det by predicting opacity scores, we use Gaussian Splatting as a regularizer to our plane sweep algorithm with little computation overhead.

## 3 Our Method

Problem Definition.The goal of multi-view indoor 3D object detection is to predict bounding box \(\{\}^{7}\) of objects in the 3D scene and their corresponding classes from \(N\) posed images \(\{_{1},,_{N}\}^{H W 3}\). Each bounding box \(\) is parameterized as \((x,y,z,w,h,l,)\), where \((x,y,z)\) are the coordinates of the box center, \((w,h,l)\) are the width, height, and length, and \(\) is the rotation angle around z-axis. The intrinsic and extrinsic matrix for each input image are denoted as \(^{3 3}\) and \(=[]^{3 4}\), respectively.

Overview.Fig. 2 shows our proposed MVSDet, a geometry-aware approach for indoor 3D object detection from posed multi-view images. Our MVSDet is built on 3D volume-based object detection (_cf._ Sec. 3.1). Instead of naively assigning the same 2D feature redundantly to every voxel center that intersect the backprojected ray, we place the pixel features according to the estimated depth from our proposed efficient plane sweep algorithm. We alleviate the costly sampling of many depth planes for accurate depth prediction by proposing a probabilistic sampling and soft weighting mechanism (_cf._ Sec. 3.2). We further utilize pixel-aligned Gaussian Splatting to enhance our depth prediction module with little extra computation cost (_cf._ Sec. 3.3). Intuitively, our depth-aware framework leads to more precise assignments of multi-view image features in the 3D volume and consequently better 3D object detection results.

### Background: 3D Volume-Based Object Detection

Existing methods [16; 22] estimate bounding boxes from a 3D feature volume aggregated from the multi-view image features. Image features \(^{ C}\) are first extracted from every input image \(^{H W 3}\). A 3D volume is defined in the world space with the size of \(N_{x} N_{y} N_{z}\) voxels. Voxel center \(=[x,y,z]^{}^{3}\) is projected to \(i\)-th input image to obtain the 2D coordinate \([_{i},_{i}]^{}^{2}\) as:

\[[^{}_{i},^{}_{i},_{i}]^{}=^{ }_{i}_{i}[^{},1]^{},[_{i}, _{i}]^{}=[^{}_{i}/_{i},^{}_{i}/_{i}]^{},\] (1)

where \(^{}_{i}\) is the scaled intrinsic matrix according the image downsampling ratio. Subsequently, 2D feature \(_{i}^{C}\) is assigned to p via nearest neighbour interpolation as follow:

\[_{i}=((_{i},_{i} ),_{i}).\] (2)

For p that are projected outside the boundary of feature map or behind the image plane, the projection is considered as invalid and we set \(_{i}=0\). Finally, it averages all valid backprojected features as the voxel feature \(=_{i=1}^{n}_{i}/n^{C}\), where \(n\) denotes the number of valid projection.

Figure 2: Overview of our MVSDet. The upper branch shows the detection pipeline with our proposed probabilistic sampling and soft weighting. The backprojected ray intersects at 3 points (shown as dots), but only the green point receives the pixel feature based on the selected depth proposals. The red points are denoted as invalid backprojection location and thus the pixel feature is not assigned to them. “GT Location” is the ground truth 3D location of the pixel. The lower branch shows the pixel-aligned Gaussian Splatting (PAGS). We select nearby views for the novel image from the images input to the detection branch and predict Gaussian maps on them. Note that PAGS is removed during testing.

The feature volume is refined by a 3D U-Net before being fed into the detection head to predict category, a bounding box and centerness score on each voxel location. The training losses for detection consists of focal loss for classification \(_{}\), cross-entropy loss for centerness \(_{}\), and IoU loss for location \(_{}\):

\[_{}=_{}+_{} +_{}.\] (3)

**Observation.** We observe that the feature aggregation method in existing multi-view indoor 3D object detection works causes 2D pixel features to be duplicated on voxels intersecting with the ray emitted from camera origin though the pixel as illustrated in Fig. 3. This is a result from lack of depth-awareness where voxels can end up aggregating irrelevant pixel features that are either not on the surface or occluded.

**Proposition.** To circumvent this problem, we propose: 1) an efficient plane sweep to instill depth awareness. Our efficient plane sweep consists of a probabilistic sampling and soft weighting mechanism based on a cost volume representation to evaluate the placement of pixel features on the intersected voxel centers; 2) a depth prediction regularizor based on 3D Gaussian Splatting to improve depth accuracy.

### Efficient Plane Sweep

Cost Volume Construction.We build \(N\) cost volumes for \(N\) input images to predict \(N\) depth probability distribution maps. To construct the cost volume for the \(i\)-th view, we set the image \(_{i}\) as the reference view and select 2 nearby views as the source views \(_{j 2(i)}\). A raw matching volume for \(_{i}\) is constructed by backprojecting source feature map \(_{j 2(i)}^{ C}\) into the coordinate system defined by \(_{i}\) at a stack of fronto-parallel virtual planes. The virtual planes \(\{_{1},,_{M}\}\) are uniformly sampled on a pre-defined depth range. The coordinate mapping from the source feature map \(_{j}\) to the reference feature map \(_{i}\) at depth \(_{m}\) is determined by a planar homography transformation:

\[_{j,m}=_{j}[_{ij}_{ij}][ _{m}(_{i}^{-1})^{},1]^{}.\] (4)

where q is the homogeneous coordinate of a pixel on \(_{i}\), \(_{j,m}\) is the projected homogeneous coordinate of q on \(_{j}\). \([_{ij}_{ij}]\) are the rotation and the translation of \(_{j}\) relative to \(_{i}\). We use Eqn. 4 to warp every source feature map \(_{j(i)}\) into all the depth planes and use a variance based metric  to generate a cost volume \(^{ C M}\). Subsequently, we use a shallow 3D CNN to refine U to generate a probability volume (after softmax) \(^{ M}\). We also predict an offset per depth bin to account for the discretization error.

Probabilistic Sampling and Soft Weighting.Although we can place pixel features to the 3D volume by predicting depth using the weighted average based on B, it requires sampling sufficient depth planes, which is prohibitive for our task. To this end, we propose a probabilistic sampling and soft weighting mechanism to decide the placement of pixel features on its backprojected ray without the need for many depth planes.

For every pixel, we sample \(k\) depth locations that score top in the corresponding distribution in B as its depth proposals. We denote their depth values as \(\{_{_{1}},,_{_{k}}\}\), and also use their corresponding probability score to evaluate its confidence with respect to the ground truth location. The scores are denoted as \(\{}_{_{1}},,}_{_{k}}\}\), where \(}_{_{k}}=_{_{k}}/_{i=1}^{i=k} _{_{i}}\). Intuitively, this is equivalent to split one depth prediction to multiple depth prediction based on the scores. As a result, it is more likely to cover the ground truth location when depth bins are insufficient.

Suppose a voxel center \(^{3}\) intersects the backprojected ray of a pixel from the \(i\)-th image, and the depth of p under the \(i\)-th camera frustum is d(p). We consider the projection of point p to the \(i\)-th

Figure 3: Comparison of different feature backprojection methods. The pixel ray intersects at 4 voxel centers with the blue box denoting the ground truth 3D location of the pixel. Our method computes the placement of the pixel features based on the depth probability distribution (purple) and thus able to suppress incorrect intersections.

image as valid and set the indicator \(_{i}=1\) when d(p) resides near any of the top-\(k\) depth proposals \(\{_{_{_{_{_{_{ _{_{_{_{_{_{ }}}}}}}}}}}\}\). We assign the normalized probability score of the nearest depth proposal to p as its confidence. The corresponding pixel feature is assigned to p weighted by the confidence score. The projection is invalid when d(p) is not close to any of the depth proposals, and we set the backprojected feature from \(i\)-th image as 0. Thus, the feature \(}_{i}^{C}\) backprojected from the \(i\)-th image to point p and its corresponding indicator \(_{i}\) are given as follows:

\[}_{i}=^{i}_{()}_{i}&\{_{_{_{_{ _{_{_{}}}}}}\}\], \[_{i}=1&\{_{ _{_{_{_{}}}}}\}\\ 0&,\] (5)

where \(()\) is the index of depth proposal that is close to d(p), and \(}^{i}_{()}\) is the score of \(_{()}\). After looping through every input image, we average all valid backprojected features for point p as its voxel feature \(}^{C}\):

\[}=_{v}^{-1}_{i=1}^{i=N}_{i} }_{i},\]

 where \(_{v}=_{i=1}^{i=N}_{i}}^{i}_{()}\).

We also utilize the multi-view consistency to compute a surface score \(\) for point p by averaging confidence scores \(}^{i}_{()}\) from every valid projection. We set \(=0\) when the point does not have any valid projection, which means point p is in the free space. s is multiplied with voxel feature \(}\) as the final feature \(^{C}\) for point p as follows:

\[=_{s}^{-1}_{i=1}^{i=N}_{i}}^{i}_ {()},\]

 where \(_{s}=_{i=1}^{i=N}_{i},\)\(=}\).

The adoption of s shares similar spirit with existing methods [22; 19] to decrease the influence of empty space voxels in the feature volume. Yet, we learn it directly from multi-view images instead of relying on NeRF  or ground truth supervision .

### Enhancing Depth Prediction with Gaussian Splatting

We further utilize the recent pixel-aligned Gaussian Splatting (PAGS) [3; 28] to enhance our depth prediction module. PAGS takes in sparse views and predict a Gaussian primitive per pixel. The parameters for each primitive are center \(\), Gaussian opacity \(\), covariance \(\) and color \(\). All the Gaussian primitives are combined together to render a novel view via rasterization-based splatting.

We select 3 nearby views per novel view from the images input to the detection branch, and predict Gaussian maps \(\{_{},_{},_{},_{c}\}\) for the selected views. The Gaussian center map \(M_{}^{ 3}\) are directly estimated from the predicted depth based on the probability volume B as follows:

\[_{}(r)=(r)+}(r)(r), =\]

where \(=[_{1},,_{M}]^{}\) is the virtual depth planes and \(^{ 1}\) is the estimated depth map. \((r)\) is the camera origin, \(}(r)\) is the projected ray depth obtained from the depth map D, and \((r)\) is the ray direction for pixel \(r\). The Gaussian opacity map \(_{}^{ 1}\) is predicted by taking the max probability score of B as follow:

\[_{}=(,dim=-1).\]

The Gaussian covariance map \(_{}^{ 16}\) and color map \(_{c}^{ 3}\) are predicted from a MLP as follows:

\[_{},_{c}=( }),\]

where \(\) denotes concatenation and \(}^{ 3}\) denotes the resized image map. Following 3DGS , \(\) is predicted by a rotation quaternion and scaling factors. Color is predicted by spherical harmonics coefficients.

We render the image color \(}_{}\) via alpha-blending and the rendering loss is a L2 loss as follows:

\[_{}=||}_{}-_{ {color}}||^{2}.\]

One of the key factors for good rendering is accurate \(_{}\), which is directly related to correct depth estimation from our model (_cf._ Eqn. 8). The rendering loss thus iteratively guides the Gaussians to the correct 3D locations, and consequently benefits our detection pipeline. Our final loss is \(=_{}+_{}\).

## 4 Experiments

### Datasets

We conduct experiments on the ScanNet and ARKitScenes datasets. ScanNet has 1,201 and 312 scans for training and testing, respectively. We detect axis-aligned bounding boxes for 18 classes. ARKitScenes has 4,498 and 549 scans for training and testing, respectively. We detect oriented bounding boxes for 17 class. We adopt mean average precision (mAP) with thresholds of 0.25 and 0.5 as the evaluation metrics.

### Implementation Details

We use the same feature extractor, training and testing configurations as  for detection. Specifically, images are resized into \((240,320)\). During training, we input 40 images to the detection branch. During testing, the detection branch takes in 100 images and the rendering branch is removed. The size of the 3D volume is \((N_{x}=40,N_{y}=40,N_{z}=16)\), with each voxel represents a cube of 0.16m \(\)0.16m \(\) 0.2m The depth range is empirically set as \([0.2,5]\). The number of depth planes \(M\) is set to 12 and \(k=3\) depth proposals are selected in the probabilistic sampling. We consider d(p) \(\) {d\({}_{k_{1}},,{}_{k_{k}}\)} if d(p) is within \( 0.2\)m of any depth proposals. For the rendering branch, we select another two images as the target views that do not overlap with the images in the detection branch. We use AdamW optimizer with learning rate 0.0002, total epochs of 12 and batchsize of 1. All experiments are conducted on two NVIDIA A6000 GPUs.

   Method & GT Geo & mAP@.25 & mAP@.5 \\  ImGeoNet & ✓ & 54.8 & 28.4 \\ CN-RMA  & ✓ & 58.6 & 36.8 \\  ImVoxelNet  & **–** & 46.7 & 23.4 \\ NeRF-Det  & **–** & 53.5 & 27.4 \\ Ours & **–** & **56.2** & **31.3** \\   

Table 1: Results on ScanNet. “GT Geo” denotes whether ground truth geometry is used as supervision during training.

   Method & GT Geo & mAP@.25 & mAP@.5 \\  ImGeoNet & ✓ & 60.2 & 43.4 \\ CN-RMA  & ✓ & 67.6 & 56.5 \\  ImVoxelNet  & **–** & 27.3 & 4.3 \\ NeRF-Det  & **–** & 39.5 & 21.9 \\ Ours & **–** & **42.9** & **27.0** \\   

Table 2: Results on ARKitScenes. “GT Geo” denotes whether ground truth geometry is used as supervision during training.

Figure 4: Qualitative comparison on ScanNet dataset. Note that the mesh is not the input to the model and is only for visualization purpose.

### Comparison with Baselines

We compare our method with ImGeoNet , CN-RMA , ImVoxelNet  and NeRF-Det . We directly report their results from the CN-RMA  paper. Note that ImGeoNet and CN-RMA use ground truth 3D geometry as supervision during training. Tab. 1 and Tab. 2 show the results on ScanNet and ARKitScenes, respectively. It is expected that using ground truth geometry as supervision can achieve good performance. However, ground truth geometry may not be accessible and therefore we seek for a self-supervised approach that do not rely its supervision. ImVoxelNet and NeRF-Det are the two existing methods that leverage self-supervision to learn geometry for multi-view 3D detection. Compared to these works, our method achieve much better performance. It clearly shows the superiority of our efficient plane sweep method over the vanilla feature backprojection in ImVoxelNet and the density field in NeRF-Det. Fig. 1 shows the qualitative results on ScanNet. The first two columns show that our model can detect more target objects. We also find that NeRF-Det tends to detect objects in the free space (last two columns), which is due to inaccurate feature projection. In contrast, our model is able to place bounding boxes at more accurate locations.

### Ablation Study

Effectiveness of Probabilistic Sampling and Soft Weighting.Tab. 3 shows the ablation study of the probabilistic sampling and soft weighting (PSSW). All methods are conducted without the rendering loss \(_{}\) to test the performance of PSSW alone. Removing "Probabilistic Sampling" means replacing top-\(k\) sampling with a single depth estimation computed by the weighted average of depth probability volume B. We use the max probability score as the weight in this case. Removing "Soft Weighting" means replacing \(}^{i}_{()}\) with value 1. As shown in the table, removing either "Probabilistic Sampling" or "Soft Weighting" causes drastic drop in the performance. Particularly, removing probabilistic sampling depth proposals drops by 19.1 at mAP@.25. It suggests that estimating accurate depth is very hard under insufficient depth planes. Furthermore, soft weighting is very important to our model as it can decrease the influence of wrong depth proposals introduced by the sampling. Overall, both probabilistic sampling and soft weighting are crucial to our model.

Effectiveness of Gaussian Splatting for Depth Prediction.Tab. 4 shows the ablation study for using pixel-aligned Gaussian splatting (PAGS) for detection. "RMSE" evaluates the average quality of depth prediction for the images in the detection branch. Increasing depth planes to \(M=16\) or using Gaussian Splatting both improve the depth estimation and bring similar improvement to the detection performance. However, using PAGS consumes much less memory during training than \(M=16\), _i.e._ adding only 28% memories compared to increasing depth planes. Moreover, PAGS does not bring any additional memory cost during testing because it is removed for detection. We visualize the depth maps predicted by the probability volume B in Fig. 5. The depth maps are of 1/16 size of the original image since we estimate depth on the feature map level. It shows that PAGS significantly improves the depth quality.

   \(M\) & Gaussian & mAP@.25 & mAP@.5 & RMSE & Memory \(\) (GB) \\ 
12 & **–** & 56.0 & 29.7 & 0.674 & **0** \\
12 & **✓** & **56.2** & **31.3** & **0.374** & +2.6 \\
16 & **–** & 55.8 & 31.1 & 0.480 & +9.4 \\   

Table 4: Ablation study of Gaussian Splatting. \(M\) denotes the number of depth planes in the plane sweep. “Gaussian” denotes using pixel-aligned Gaussian splatting. “RMSE” is the depth evaluation metric. “Memory \(\)” denotes the increased memory consumption during training.

   Probabilistic Sampling & Soft Weighting & mAP@.25 & mAP@.5 \\  ✓ & – & 50.0 & 24.8 \\ – & ✓ & 36.9 & 13.5 \\ ✓ & ✓ & **56.0** & **29.7** \\   

Table 3: Ablation study of probabilistic sampling and soft weighting. All methods are conducted without using rendering loss.

Analysis of the number of Top-\(k\) depth proposals.Tab. 5 shows the ablation study of number of depth proposals. Sampling top-1 depth proposal leads to severe performance decrease as the correct depth location is harder to be selected. Sampling too many depth proposals (\(k=5\)) also leads to some decrease since more inaccurate locations are sampled. We thus choose \(k=3\) in our model.

Comparison with Depth Estimation Methods.Fig. 6 shows the comparison of different depth prediction methods to the detection performance. MVSNet  performs one-time plane sweep by using \(M=16\) depth planes. BEVStereo  performs iterative depth estimation by sampling depth planes according to a monocular depth estimation. We set the number of iterations to 3 and the number of depth planes in each iteration to 5. We remove ground truth depth supervision for BEVStereo for fair comparison. "Ground-truth Depth" denotes placing pixel feature on the 3D volume according to the ground-truth depth location, which is the upper bound of our method. We show the results of our approach using different number of depth planes (\(M=12\) and \(M=8\)). MVSNet performs badly even though it samples more planes than us. This is because MVSNet requires sufficient depth planes to estimate depth correctly. BEVStereo also fails because it requires the ground truth depth supervision to learn a good initial monocular depth estimation. In contrast, our model achieve close performance as the "Ground-truth Depth" using only 12 or even 8 depth planes. This demonstrates that our approach efficiently learns geometry without sampling many depth planes.

Time and memory comparison.Tab. 6 shows the comparison of time and memory in the training and testing stages on ScanNet, respectively. We omit comparison with ImGeoNet since it does not release any code. All models are ran on \(2\) A6000 GPUs. Due to the complexity of CN-RMA (as mentioned in Sec 3.5 of their paper), it requires much longer time to train and evaluate than other models. Furthermore, CN-RMA consumes much more memory in the training stage because it requires joint end-to-end training of the 3D reconstruction and detection network. Although NeRF-Det is efficient in time and memory of the training and testing stages, their performance is much worse than ours as shown in Tab. 1 and 2.

    &  &  \\   & Time(hrs) & Memory(GB) & Time(min) & Memory(GB) \\  CN-RMA & 121 & 43 & 10 & 12 \\ NeRF-Det & **7** & **13** & **2** & **12** \\ Ours & 18 & 35 & 3 & 28 \\   

Table 6: Time and memory comparison in training and testing stages on ScanNet dataset, respectively.

Conclusion

In this paper, we propose MVSDet for multi-view image based indoor 3D object detection. We design a probabilistic sampling and soft weighting mechanism to decide the placement of pixel features on the 3D volume without the need of the computationally sampling of many depth planes. We further introduce the use of pixel-aligned Gaussian Splatting to improve depth prediction with little computation overhead. Extensive experiments on two benchmark datasets demonstrate the superiority of our method.

## 6 Limitation

Similar to existing multi-view stereo methods , feature matching would fail on texture-less or reflective surfaces. One possible solution is to combine with monocular depth estimation . However, estimating monocular depth is non-trivial and we leave it for future research.

Acknowledgement.This research work is supported by the Agency for Science, Technology and Research (A*STAR) under its MTC Programmatic Funds (Grant No. M23L7b0021).