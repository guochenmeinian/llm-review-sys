# LoTLIP: Improving Language-Image Pre-training for Long Text Understanding

Wei Wu\({}^{1}\) Kecheng Zheng\({}^{2,3}\)\({}^{}\) Shuailei Ma\({}^{4}\) Fan Lu\({}^{1}\) Yuxin Guo\({}^{5}\) Yifei Zhang\({}^{6}\) Wei Chen\({}^{3}\) Qingpei Guo\({}^{2}\) Yujun Shen\({}^{2}\) Zheng-Jun Zha\({}^{1}\)\({}^{}\)

\({}^{1}\)University of Science and Technology of China \({}^{2}\)Ant Group \({}^{3}\)Zhejiang University

\({}^{4}\)Northeastern University, China \({}^{5}\)Institute of Automation, Chinese Academy of Sciences

\({}^{6}\)Shanghai Jiao Tong University

 Corresponding authors.

###### Abstract

Understanding long text is of great demands in practice but beyond the reach of most language-image pre-training (LIP) models. In this work, we empirically confirm that the key reason causing such an issue is that the training images are usually paired with short captions, leaving certain tokens easily overshadowed by salient tokens. Towards this problem, our initial attempt is to relabel the data with _long captions_, however, directly learning with which may lead to performance degradation in understanding short text (_e.g._, in the image classification task). Then, after incorporating corner tokens to aggregate diverse textual information, we manage to help the model catch up to its original level of short text understanding yet greatly enhance its capability of long text understanding. We further look into whether the model can continuously benefit from longer captions and notice a clear trade-off between the performance and the efficiency. Finally, we validate the effectiveness of our approach using a self-constructed large-scale dataset, which consists of \(100M\) long caption oriented text-image pairs. Our method achieves superior performance in long-text-image retrieval tasks. The project page is available here.

## 1 Introduction

Understanding long texts plays a key role in Natural Language Processing (NLP), _e.g._, Long Document Analysis , in which books, academic papers, and many other types of long texts can be the target of such analysis. Inspired by these works, in the multi-modality field, some text-to-image generation works (_e.g._, DALLE-3  and Pixel-art ) employ pre-trained captioners (_e.g._, LLaVA ) to generate more accurate and detailed captions (in other words, long captions) for images. These long captions describe an image in detail which can help text-to-image models easily transfer long text to an image, improving the quality of text-image alignment. However, these text-to-image models use pure NLP encoders (_e.g._, T5) to model long captions rather than CLIP. Because, in the language-image pretraining task, little work has been conducted on modeling long texts in a way that effectively aligns with image representations. Despite the lack of exploration, this is a critical problem with practical demands that require a nuanced understanding of textual descriptions corresponding to images.

Two primary challenges hinder the effective integration of long-text understanding in language-image pre-training. The first one is the lack of large-scale long-caption image-text paired datasets. Mostexisting datasets focus on short captions (_i.e._, average text length in CC12M  is about 17 tokens), which limits the model's exposure to longer text forms. Consequently, models trained on these datasets tend to neglect certain tokens that are easily overshadowed by salient tokens. As shown in Fig. 1, the model trained on short captions can be good at understanding the content of short captions (_i.e._, garden and castle). But when increasing the length of the caption, we can see that 'garden token' is overshadowed by 'castle token', even if we move the 'garden token' front. This bias towards the salient tokens ('castle token' in Fig. 1) of texts can severely restrict the model's ability to comprehend and generate responses based on the full context of longer inputs. The second challenge is the token length limitation of the text encoder. While directly increasing the token number limitation that a model can process appears to be a straightforward solution for accommodating longer texts, it does not have a better understanding of long captions. The fundamental issue remains the model's inability to effectively interpret long texts, primarily due to the lack of appropriate training data that includes long captions.

To address these challenges, we have undertaken an extensive project to re-caption 100 million data with long captions, aiming to enrich the training environment for our models. This initiative allows us to explore the effects of increased text length in image-text pre-trained models and its impacts on model performance. Based on these experiments, we empirically confirm that the key reason causing such an issue is that the training images are usually paired with short captions, leaving certain tokens easily overshadowed by salient tokens. However, directly learning with long captions may improve the long-text understanding of image-text pre-trained models, but lead to performance degradation in understanding short texts (_e.g._, in the image classification task) as shown in Fig. 2. After integrating corner tokens to aggregate diverse textual information, we successfully enable the model to regain its original proficiency in understanding short texts while significantly improving its ability to comprehend long texts. We also explore whether the model can continue to benefit from longer captions and observe a clear trade-off between performance and efficiency. Moreover, on the task of long-text image retrieval, we beat Long-CLIP , a competitor using long captions for fine-tuning, with 4.2% improvement (_i.e._, from 79.52% to 83.72%).

## 2 Related work

### Language-Image Pre-training

Recently, using language-image pre-trained models to do zero-shot prediction has attracted a lot of attention. CLIP  and ALIGN  demonstrate contrastive pre-trained models can learn

Figure 1: Illustration of the impacts of long _v.s._ short captions on image-language pre-training, as observed in the cross-attention maps of CLIP. Training images are usually paired with short captions, leaving certain tokens (_e.g._, garden token) easily overshadowed by salient tokens (_e.g._, castle token). Fortunately, the usage of long captions can help bring the overshadowed tokens back into the light, and this phenomenon is not influenced by the order of tokens within the sentence.

rich visual-language correspondence knowledge from large-scale image-text pairs on the Internet and achieve good performance on zero-shot predictions, including image-text retrieval  and classification . Following their success, various studies [16; 34; 36; 17] have been devoted to improving image-text alignment. Among them, FILIP  focuses on fine-grained expressiveness between text tokens and image patches by modifying the training loss. While LiT  finds that apply contrastive-tuning to the pre-trained models with locked image encoder and unlocked text encoder can further improve the alignment. It is recognized that larger batch size brings better performance. For this reason, SigLIP  proposes to replace the softmax normalization among the standard contrastive loss with the sigmoid loss to scale up training batch size. In addition, LaCLIP , RLEG  and some other works [19; 13; 22; 30] utilize multi-modality generative models to improve data quality for enhancing pre-training.

### Long-text Understanding

Detailed long texts are necessary for many artificial intelligence tasks (_e.g._, human-computer interaction). Therefore, modeling and parsing long texts has become one of the most important research directions in natural language processing. Many studies in fields such as text generation [23; 32; 33] have shown that advanced transformer structures have the ability to interpret long texts. However, in the field of language-image pre-training, research on using long-text descriptions of images to enhance multimodal representations is still very scarce. DreamLIP  utilizes multi-modality large language model to re-caption image data with detailed descriptions and then use them in language-image pre-training. In fact, during training, DreamLIP randomly extracts sub-captions from the detailed description for training and does not completely use all the information of the long text. One of the reasons why previous language-image pre-training rarely directly applied long texts for training is the text encoder of traditional CLIP  is restricted by the token number limit (\( 77\)). Then, Long-CLIP  firstly introduces long captions into CLIP model to finetune, where the model is pre-trained on short-text-image datasets. The fine-tuning process equips the model with the ability to comprehend long texts, albeit at the expense of its proficiency in understanding shorter texts. In contrast, we incorporate long captions during the pre-training stage, which can not only enable the model to regain its original competency in interpreting short texts but also significantly improves its understanding of long texts. We further explored the trade-off between the benefit of long texts to the model and the training efficiency.

## 3 Preliminary of Language-Image Pre-training

Language-image pre-training models, _i.e._, CLIP  and LiT , typically consist of an image encoder and a text encoder. In the pre-training stage, the language-image model takes image-text pairs as input and uses image encoder and text encoder to extract embeddings from images and texts, respectively. Then, two encoders are trained with contrastive objectives, ensuring that paired image and text embeddings are close in the embedding space, while unpaired pairs are far apart. Specifically, a batch of images \(\{I_{1},I_{2},,I_{N}\}\) and the corresponding short texts \(\{T_{1},T_{2},,T_{N}\}\) are randomly sampled from the pre-training dataset. Each image and its corresponding text are treated as a positive pair, while others in the same batch are negative pairs. Then, the image encoder extracts image global feature \(^{i}\) from the \(i\)-th image \(I_{i}\) within the batch, while the text encoder obtains text feature \(^{j}\) from the \(j\)-th text \(T_{j}\). These two encoders are then optimized using contrastive loss \(\), which consists of image-to-text loss \(^{i2t}\) and text-to-image loss \(^{t2i}\). It can be formulated as follows:

\[=^{i2t}+^{t2i},\] (1)

\[^{i2t}=-_{i=1}^{N}^{ i},^{i}/)}{_{j=1}^{N}( ^{i},^{j}/)},\] (2)

\[^{t2i}=-_{i=1}^{N}^{ i},^{i}/)}{_{j=1}^{N}( ^{i},^{j}/)},\] (3)

where \(\) is a learnable temperature parameter, and \(,\) means the cosine similarity between two normalized feature vectors.

## 4 Long Texts in Language-Image Pre-training

Currently available image-text pair datasets, _e.g._CC12M , typically include short texts that have an average length of approximately 17 tokens. Language-image models pre-trained on these datasets perform well on short-text comprehension tasks. However, we find that they struggle to comprehend long texts for text-image alignment. Concretely, they tend to overlook or neglect some tokens or sub-captions in long texts, as shown in Fig. 1. A potential reason for such a situation is the lack of long-text-image pairs in pre-training. Thus, we collected and re-captioned 100M images with long texts. In this section, we provide details of re-captioning and explore the usage of long texts in language-image pre-training.

### Long Text-Image Pair Dataset

Training Dataset.To construct long text-image pairs for language-image pre-training, we re-captioned 100 million images with long texts. Specifically, we collected the images from CC3M , CC12M , YFCC15M , LAION , and COYO  dataset. Then, we instructed three multi-modality large language models (MLLMs), _i.e._, InstructBLIP , LLaVA , and ShareGPT4V  to generate diverse and descriptive long texts based on the collected images. In this step, we used "Describe the image in detail." as the text prompt, following DreamLIP . Finally, each image in the collected datasets is paired with four texts: a raw text from the original dataset and three re-captioned long texts. The raw texts exhibit an average length of approximately 18 tokens, whereas the re-captioned long texts consist of around 136 tokens.

Evaluation Dataset.Most zero-shot evaluation tasks for language-image pre-training primarily rely on short textual input. For example, in short-text-image-retrieval tasks, the textual inputs contain fewer than 15 tokens on average, as shown in Tab. 1. Given the short texts, these tasks are not suitable for accessing the long text comprehension ability of pre-trained models. Therefore, we collected long text-image pairs from DCI , IIW , and ShareGPT4V  datasets to construct long-text-image retrieval evaluation tasks. Specifically, for DCI and IIW datasets, we use images with human-annotated long descriptions for retrieval. For ShareGPT4V dataset, we sample 1,000 and 10,000 data from ShareGPT4V dataset to construct ShareGPT4V-1k and ShareGPT4V-10k retrieval dataset, respectively. The ShareGPT4V-1k dataset is constructed following Long-CLIP . All images in ShareGPT4V-1k and ShareGPT4V-10k are from SA-1B  dataset, and all long texts are generated by ShareGPT4V-Captioner . As shown in Tab. 1, each image within these datasets has one paired long text, which consists of more than 8 sub-captions and 170 tokens on average. Here, a sub-caption is a complete sentence ending with a period. The long-text-image retrieval task necessitates the alignment of text features from long texts with the image features from the corresponding images, which thereby evaluates the model's ability to comprehend long texts.

### Exploring the Influence of Text length

Long-CLIP  enables long text processing ability by fine-tuning CLIP model with long text-image pairs. However, the benefits and drawbacks of using long texts in pre-training are still unknown. It

   Dataset & \#Images & \#Texts & \#Sub-captions per Text & \#Tokens per Text \\   \\  DCI  & 7,805 & 7,805 & 10.81 & 172.73 \\ IIW  & 612 & 612 & 10.16 & 239.73 \\ ShareGPT4V-1k  & 1,000 & 1,000 & 8.15 & 173.24 \\ ShareGPT4V-10k  & 10,000 & 10,000 & 8.24 & 173.66 \\   \\  MSCOCO  & 5,000 & 25,000 & 1.0 & 11.77 \\ Flickr30k  & 1,000 & 5,000 & 1.0 & 14.03 \\   

Table 1: **Dataset details of long-text-image retrieval and short-text-image retrieval tasks**. We use BERT tokenizer for tokenization. ShareGPT4V-1k and 10k are selected from the ShareGPT4V dataset. For DCI and IIW, we use images with human-authored long descriptions for evaluation.

is also unknown how the length of the text affects the performance of the pre-trained model. To explore the influence of using texts in different lengths for pre-training, we change the length of long texts by selecting different numbers of consecutive sub-captions, as illustrated in Fig. 2. Each sub-caption in the long texts has an average of approximately 22 tokens. Sub-caption number equal to 0 indicates that the model is trained without the generated texts. When introducing one sub-caption, a noticeable gain is achieved across all tasks. Moreover, the results also indicate that pre-training with longer texts, composed of more sub-captions, enhances the performance of the pre-trained model on long-text-image retrieval tasks. It confirms that using long text-image pairs for pre-training improves the model's understanding of long texts. However, the usage of longer texts negatively impacts the model's performance on short-text-image retrieval and image classification tasks.

### Method

In order to find a solution that well balances the long and short text understanding of the pre-trained model, we design to add extra text tokens for text encoders, termed _corner tokens_, which can aggregate diverse text features. This strategy benefits the class (_i.e._, \([]\)) token by extracting more representative features for long and short text. Next, we will describe the details.

Corner Tokens.Different text encoder architectures (_e.g._, BERT , T5 ) can be utilized in language-image pre-training. In this section, we take BERT  as an example of our approach. Given a long text, it is expressed as \([],[],[]\), where the first token of every text input is its class token \([]\), and all sub-captions are separated by a special token \([]\). The omitted part in \(\) denotes the tokens obtained after tokenizing words within the sub-caption. Based on the tokenized long text, we insert multiple learnable corner tokens \(=\{[~{}~{}1],,[~{}~{}]\}\) after

Figure 3: **Overview of LoTLIP. We add multiple learnable corner tokens (\([~{}~{}1],[~{}~{}2],\)) after \([]\) token. These corner tokens are initialized differently for aggregating diverse token features. Besides, an attention mask mechanism is used to limit the interaction between \([]\) and corner tokens to ensure the diversity of gathered features.**

Figure 2: **The influence of text length. A significant improvement is observed across all tasks when we added one randomly sampled sub-caption from generated texts to the pre-training stage. As the number of sub-captions increases, the performance of the pre-trained model on long-text-image retrieval tasks consistently improves and becomes stable (a). However, there is a performance degradation in MSCOCO retrieval task (b) and ImageNet classification task (c).**

the class token, where \(m\) is the number of corner tokens. In this way, the form of the tokenized text input is \([],[][],[],[]\). Moreover, we design an attention mask mechanism \(\) for the text encoder to promote the diversity of the aggregated features. Specifically, when calculating the attention scores, the corner tokens and \([]\) token are guided to neglect each other but attend to all other sub-caption tokens. Meanwhile, in the attention mask mechanism, each text tokens are designed to only interact with other text tokens and the \([]\) token, to keep the interactions between local and global information. The attention mask \(\) is formulated as:

\[(q,k)=\{0,&(k q,k\{[]\}))q k\\ 1,&.\]

where \(q\) and \(k\) represent the query and key tokens in attention block, respectively. The features of the \([]\) and corner tokens are regarded as text global feature \(_{g}\) and corner features \(_{c_{1}},_{c_{2}},...,_{c_{m}}\), respectively.

Optimization.The short-text-image contrastive loss \(_{}\) is calculated in the same way as Eq. (1). Meanwhile, the long-text-image contrastive loss \(_{}\) between image global feature \(v\) and \(_{g},_{c_{1}},_{c_{2}},...,_{c_{m}}\) of long text is as follows:

\[_{}=_{}^{i2t}+_{ }^{t2i},\] (4)

\[_{}^{i2t}=-_{i=1}^{N}(^{i},_{g}^{i}/)}{_{j=1}^{N} (^{i},_{g}^{j}/)}+ _{k=1}^{m}^{i},_{c_{k}}^ {i}/)}{_{j=1}^{N}(^{i}, _{c_{k}}^{j}/)}),\] (5)

\[_{}^{t2i}=-_{i=1}^{N}(_{g}^{i},^{i}/)}{_{j=1}^{N} (_{g}^{i},^{j}/)}+ _{k=1}^{m}_{c_{k}}^{i},^{i}/)}{_{j=1}^{N}(_{c_{k}} ^{i},^{j}/)}),\] (6)

The total training loss is \(_{}=_{}+_{}\).

## 5 Experiments

### Implementation Details and Datasets

**Pre-training Datasets.** As presented in Sec. 4.1, we collected 100M data from five publicly available image-text pair datasets and re-captioned the collected images with long texts. Based on this dataset, we construct 4 scales of pre-training data: (1) 3M, including CC3M. (2) 12M, including CC12M. (3) 30M, including CC3M, CC12M, and YFCC15M. (4) 100M, including all re-captioned data. We conduct ablation studies to validate our model on the 3M scale pre-training data. The performance of LoTILP pre-trained with 12M and 30M scale datasets is shown in the _Supplementary Material_.

**Downstream Datasets.** To assess the ability of the pre-trained models on short-text and long-text understanding, we select 3 downstream tasks for evaluation under the zero-shot setting, including long-text-image retrieval, short-image-text retrieval, and image classification. **For long-text-image retrieval**, we present the Recall at 1 (R@1) metric of the pre-trained models on DCI, IIW, and ShareGPT4V-1k, and ShareGPT4V-10k long-text-image retrieval tasks. **For short-image-text retrieval**, we evaluate on MSCOCO  and Flickr30k Caption  and report Recall at 1/5 (R@1/5) metric for comparison. **For image classification**, we use ImageNet1k  for evaluation and present top-1 accuracy (Acc@1) on image classification. Following , we use class names incorporated with pre-defined text prompts as text inputs for zero-shot classification.

**Implementation Details.** Following LiT , we use a vision transformer pre-trained on ImageNet 21K as the image encoder and Bert  as the text encoder. The architecture of the image encoder is ViT-B/16. We report other variants of vision transformers in the _Supplementary Material_. The images are resized to 224\(\) 224. The maximum text token length is set to 128 unless specifically stated. Three consecutive sub-captions are randomly selected to form long texts as text input. We train 10 epochs on the 3M and 100M scale datasets. For the 3M dataset, the batch size is set to 2560, while that of 100M is set to 16384. The other pre-training hyperparameters are under the same setting, _e.g._learning rate, warmup steps, and weight decay.

[MISSING_PAGE_EMPTY:7]

that LoTLIP benefits from long texts and corner tokens, thereby exhibiting a better understanding of the long and short texts.

Implementation of Corner Tokens.In this part, we study the influence of the attention mask mechanism and the number of corner tokens on different downstream tasks as shown in Tab. 3. LoTLIP pre-trained without the pre-defined attention mask encounters performance degradation on most tasks compared to when using the pre-defined attention mask. It indicates that direct interaction between the CLS token and corner tokens limits their ability to aggregate diverse textual features. On short-text-image retrieval and image classification tasks, the performance of LoTLIP improves when introducing more corner tokens. It proves that the corner tokens help with enhancing the short text understanding ability of LoTLIP. When the number of corner tokens is set to more than two, the performance improvement across all tasks is relatively small. Thus, we use two corner tokens in LoTLIP.

### Main Results

We compare LoTLIP with the state-of-the-art methods on downstream tasks involving long texts and short texts. The experimental results are shown in Tab. 4 and Tab. 5, respectively. On 3M data scale, LoTLIP significantly improves the state-of-the-art methods on all tasks. Specifically, LoTLIP

    &  &  &  &  \\  & &  &  &  &  & ImageNet \\  & & I2T & T2I & I2T & T2I & I2T & T2I & T2I & T2I & Acc \\ 
0 & - & 47.96 & 44.92 & 84.97 & 81.70 & 73.66 & 66.73 & 43.52 & 30.06 & 48.87 \\
1 & ✓ & 49.57 & 46.55 & 84.97 & 82.68 & 74.91 & 68.41 & 45.68 & 31.51 & 49.62 \\
2 & ✓ & 49.46 & 47.82 & 84.97 & 83.33 & 76.49 & 69.72 & 46.56 & 31.59 & 50.34 \\
3 & ✓ & **49.58** & 47.70 & **87.09** & **84.31** & **76.51** & **70.20** & 46.48 & 31.60 & 50.36 \\
4 & ✓ & **49.58** & **48.30** & 86.76 & 84.15 & 76.25 & 70.14 & **47.70** & **31.88** & **50.59** \\ 
2 & - & 48.61 & 47.17 & 86.11 & 81.86 & 76.14 & 69.31 & 47.70 & 31.34 & 49.88 \\
2 & ✓ & 49.46 & 47.82 & 84.97 & 83.33 & 76.49 & 69.72 & 46.56 & 31.59 & 50.34 \\   

Table 3: Analyze the influence of the number of corner tokens and the attention mask mechanism. We use 3M scale dataset for training. The architecture of the image encoder is ViT-B/16.

    &  &  &  &  &  &  \\  & & & I2T & T2I & I2T & T2I & I2T & T2I & I2T & T2I & T2I \\ 
3M & - & FILIP  & 10.85 & 11.36 & 31.54 & 29.08 & 26.50 & 26.80 & 8.94 & 8.64 & 19.21 \\
3M & - & LaCLIP  & 14.84 & 14.71 & 41.01 & 38.89 & 40.90 & 37.10 & 15.81 & 14.84 & 27.26 \\
3M & - & SigLIP  & 11.66 & 13.11 & 29.25 & 29.58 & 27.30 & 25.10 & 9.92 & 9.30 & 19.40 \\
3M & - & LiT  & 27.14 & 24.13 & 65.20 & 58.50 & 63.60 & 56.80 & 32.73 & 27.01 & 44.38 \\ 
3M &  & **49.46** & **47.82** & **84.97** & **83.33** & **93.20** & **90.00** & **76.49** & **69.72** & **74.37** \\ 
400M & - & CLIP  & 45.45 & 43.01 & 88.24 & 87.58 & 84.50 & 79.80 & 60.22 & 56.16 & 68.12 \\
100M & - & LiT  & 41.78 & 40.90 & 88.07 & 82.68 & 86.00 & 80.00 & 61.41 & 50.69 & 66.44 \\
700M & - & ALIGN  & 56.54 & 57.41 & 92.65 & 90.68 & 86.30 & 85.30 & 65.13 & 62.73 & 74.59 \\
12B & - & SigLIP  & 57.78 & 56.22 & 91.99 & 91.01 & 85.80 & 83.40 & 83.40 & 63.08 & 76.59 \\
400M & 1M & Long-CLIP\({}^{*}\) & 51.68 & 57.28 & 89.61 & **93.20** & 94.70 & 93.40 & 79.24 & 77.06 & 79.52 \\ 
100M &  & **62.10** & **61.06** & **93.95** & 92.48 & **96.50** & **95.50** & **86.84** & **81.40** & **83.72** \\   ^{*}\) Long-CLIP fine-tunes pre-trained CLIP model with ShareGPT4V dataset except ShareGPT4V-1k.} \\ 

Table 4: Zero-shot evaluation of different models on long-text-image retrieval tasks. I2T and T2I indicate R@1 on text and image retrieval, respectively.

improves the second-best method LiT by 29.99% on average over four long-text-image retrieval tasks. Moreover, LoTLIP improves the second competitor LiT by 6.58% and 10.98% on image classification task and short-text-image retrieval tasks, respectively. It proves that LoTLIP significantly enhances the language-image model for understanding short and long captions by involving long captions in pre-training and incorporating corner tokens in text inputs. It is worth noting that LoTLIP trained with 100M data exceeds all state-of-the-art methods on long-text-image retrieval tasks, even though these methods are pre-trained on a larger scale of data. Concretely, LoTLIP (trained on 100M data) exceeds SigLIP (trained with 12B) by an average of 7.13% over four long-text-image retrieval tasks. Moreover, compared to Long-CLIP, which uses long texts in CLIP for fine-tuning, LoTLIP improves averaged performance by 4.2% on long-text-image retrieval tasks.

## 6 Conclusion

In this work, we empirically confirm that a key issue arises because training images are typically paired with short captions, which can cause certain tokens to be overshadowed by more salient ones. To address this issue, our initial strategy involved relabeling the data with long captions. However, directly learning from these long captions might lead to degraded performance in tasks requiring an understanding of short text, such as image classification. Subsequently, by incorporating corner tokens to aggregate diverse textual information, we are able to help the model regain its original proficiency in understanding short texts while significantly enhancing its capability to comprehend long texts. We also investigated whether the model could continue to benefit from longer captions and observed a clear trade-off between performance and efficiency. Finally, we validated the effectiveness of our approach using a self-constructed large-scale dataset, which consists of 100 million long-caption-oriented text-image pairs. In the task of long-text-image retrieval, our method outperforms the second-best competitor, Long-CLIP, with an improvement of 4.2%.

## 7 Acknowledgments

This work was supported by National Natural Science Foundation of China (NSFC) under Grants 62225207, and Zhejiang Provincial Natural Science Foundation of China under Grants LD24F020011.

   
 Data Scale \\ Short \\  &  &  & IN & MSCOCO I2T & MSCOCO T2I & Flickr30k I2T & Flickr30k T2I \\  & & & Acc. & R@1 & R@5 & R@1 & R@5 & R@1 & R@5 \\ 
3M & - & FILIP  & 18.69 & 14.98 & 34.88 & 11.86 & 28.98 & 30.40 & 56.80 & 20.76 & 44.20 \\
3M & - & LaCLIP  & 21.50 & 18.94 & 40.40 & 12.42 & 31.04 & 37.00 & 63.90 & 29.32 & 56.04 \\
3M & - & SigLIP  & 21.28 & 16.30 & 37.36 & 13.22 & 31.65 & 31.00 & 61.10 & 23.76 & 48.64 \\
3M & - & LiT  & 43.76 & 34.20 & 61.52 & 24.07 & 48.37 & 61.30 & 89.50 & 48.06 & 75.36 \\ 
3M &  MoTLIP \\  } & **50.34** & **46.56** & **72.02** & **31.59** & **57.65** & **75.20** & **94.20** & **58.20** & **83.58** \\ 
400M & - & CLIP  & 68.34 & 51.68 & 76.72 & 32.70 & 57.76 & 82.20 & 96.60 & 62.14 & 85.72 \\
100M & - & LiT  & 73.70 & 51.92 & 75.72 & 32.74 & 57.84 & 80.00 & 95.90 & 60.86 & 84.62 \\
700M & - & ALIGN  & 65.89 & 60.42 & 82.50 & 42.36 & 67.38 & 88.90 & 98.00 & 74.12 & **92.40** \\
12B & - & SigLIP  & **76.04** & **65.46** & **86.22** & **47.14** & **72.10** & **89.10** & **98.00** & **74.66** & 92.30 \\
400M & 1M & Long-CLIP  & 67.10 & 57.28 & 80.78 & 40.34 & 65.92 & 85.90 & 98.50 & 70.66 & 90.60 \\ 
100M &  LoTLIP \\  } & 72.16 & 59.66 & 81.50 & 38.06 & 63.81 & 86.90 & 97.80 & 65.22 & 87.98 \\   

Table 5: Zero-shot evaluation of different models on short-text-image retrieval and classification tasks.