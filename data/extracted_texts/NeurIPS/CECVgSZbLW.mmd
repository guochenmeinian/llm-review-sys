# Distributional Monte-Carlo Planning with Thompson Sampling in Stochastic Environments

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We focus on a class of reinforcement learning algorithms, Monte-Carlo Tree Search (MCTS), in stochastic settings. While recent advancements combining MCTS with deep learning have excelled in deterministic environments, they face challenges in highly stochastic settings, leading to suboptimal action choices and decreased performance. Distributional Reinforcement Learning (RL) addresses these challenges by extending the traditional Bellman equation to consider value distributions instead of a single mean value, showing promising results in Deep Q Learning. In this paper, we bring the concept of Distributional RL to MCTS, focusing on modeling value functions as categorical and particle distributions. Consequently, we propose two novel algorithms: Categorical Thompson Sampling for MCTS (CATS), which uses categorical distributions for Q values, and Particle Thompson Sampling for MCTS (PATS), which models Q values with particle-based distributions. Both algorithms employ Thompson Sampling to handle action selection randomness. Our contributions are threefold: We introduce a distributional framework for Monte-Carlo Planning to model uncertainty in return estimation. We prove the effectiveness of our algorithms by achieving a non-asymptotic problem-dependent upper bound on simple regret of order \(O(n^{-1})\), where \(n\) is the number of trajectories. We provide empirical evidence demonstrating the efficacy of our approach compared to baselines in both stochastic and deterministic environments.

## 1 Introduction

_Online planning_ in Markov decision processes (MDPs) involves making real-time decisions based on the current state of the environment. It requires balancing exploration and exploitation while handling uncertainty and partial observability. Monte Carlo Tree Search (MCTS) is a highly effective online planning method for tackling complex MDPs. MCTS has shown impressive performance in various tasks, including traditional board games like Chess and Go, video games, and real-world challenges. Notable successes include advancements in Chess [(35)] and Go [(34; 36; 30)], video game strategy [(28)], robot assembly [(16)], robot path planning [(15; 13)], and autonomous driving [(24)].

Despite these achievements, current MCTS methods are primarily effective in deterministic environments, often overlooking the significant impact of randomness in real-world scenarios. In highly stochastic and partially observable environments, conventional MCTS approaches face substantial challenges due to widespread randomness and limited observability. This leads to compromised value estimates, suboptimal decisions, and diminished overall performance. Therefore, there is a clear need for improved methods capable of navigating the complexities of randomness and partial observability in value estimation.

We now review related works to understand the advancements and limitations in these areas.

**Related work** In MCTS, value estimation methods and action selection rules are critical factors for algorithm performance. Traditional value estimation methods, such as using empirical average meanfor value backup as in the Upper Confidence bounds applied to Trees method (UCT) (21), suffer from underestimation of optimal values while maximum backup suffers from overestimation of optimal values (9). The power mean estimator (12) offers a balanced solution by computing a mean between the average and maximum values. In our approach, we also use power mean for value operator as each V node stores the power mean of empirical means of succeeding Q-value nodes, eliminating the need for V to be modeled as a distribution.

For action selection in MCTS, strategies from Multi-Armed Bandits (MAB) are commonly employed. For instance, UCT extends the UCB1 strategy from bandits to the tree by computing confidence intervals at each step. However, original UCT's performance is hindered by the incorrect choice of logarithmic bonus constant (32). Shah et al. (32) propose an adapted version of UCT incorporating a polynomial bonus term instead of the "logarithmic" bonus term in UCT and show the non-asymptotic convergence of rate \(O(n^{-1/2})\), with \(n\) is the number of rollout trajectories. On the other hand, our method improves over this rate with theoretical guarantee of \(O(n^{-1})\). Although Thompson sampling has been less explored in MCTS, some approaches like those by Bai et al. (1) and Bai et al. (2) incorporate it for exploration. However, these methods lack convergence rate analysis. Furthermore, in the article Bai et al. (1), authors model value functions as a mixture of Normal distributions, which may lack the generality of complex real-world scenarios. Our approach adopts Thompson sampling for action selection but introduces a novelty by modeling the uncertainty of action value estimates over the tree as arbitrary categorical and particle-based distributions. This modification enhances our ability to handle more generality in highly stochastic environments effectively.

_Entropy regularization_ techniques in RL modify value and action selection functions to balance exploration and exploitation, leading to improved value estimation (25; 17; 31; 18). Several works have applied these techniques in MCTS. Maximum Entropy Tree Search (MENTS) (40) emphasizes exploration by integrating MCTS with maximum entropy policy optimization. MENTS aims to maximize cumulative rewards and policy entropy concurrently, regulated by a temperature parameter. Dam et al. (14) extend MENTS by incorporating Relative and Tsallis entropy, leading to the RESTS and TENTS algorithms. However, the effectiveness of MENTS/RENTS/TENTS hinges on the temperature parameter, which may impede convergence. Furthermore, the value estimation converges exponentially to the regularized value not the optimal one. In contrast, Painter et al. (27) utilize a similar action selection approach but employ a maximum backup operator for value estimation. Although their method exhibits exponential decay of simple regret, it heavily relies on the sensitivity of the temperature parameter for Boltzmann Exploration, limiting its practicality.

_Distributional Reinforcement Learning_ (RL) (6; 11; 22) addresses the randomness of the value estimation by introducing a distributional perspective to the traditional Bellman equation. This approach views the value function as a distribution rather than a single mean, providing a comprehensive understanding of uncertainties in rewards and the stochasticity from environments. Through discretization (26), parameterization (6), and quantization (10), it allows for efficient and effective approximation of value distributions, leading to improved performance in various RL tasks. However, these results are only for _learning_ not for _planning_.

**Outline and contribution** In this work, we integrate the distributional approach from reinforcement learning (RL) into the _planning_ framework to tackle the challenges of planning in stochastic environments. We focus on modeling value functions as categorical and particle distributions. Consequently, we propose two novel algorithms: Categorical Thompson Sampling for MCTS (CATS) and Particle Thompson Sampling for MCTS (PATS). CATS represents each Q value function as a categorical distribution and uses Thompson Sampling for action selection to manage uncertainty. PATS models each Q value function with a particle-based distribution, using a nuanced Thompson Sampling approach to handle action selection randomness.

Our contributions are threefold:

1. In section 3, we introduce a distributional framework for _planning_ to model uncertainty in return estimation, enhancing the robustness of value estimation in stochastic environments.
2. In section 4 Theorem 5 and Theorem 6, we prove the effectiveness of our algorithms by achieving a non-asymptotic problem-dependent upper bound on simple regret of \(O(n^{-1})\), which significantly improves upon the current state-of-the-art theoretical analysis of regret, previously established at \(O(n^{-1/2})\) by Shah et al. (33).

3. In section 5, we provide comprehensive empirical evidence demonstrating the efficacy of our approach compared to baselines, showcasing competitive performance in stochastic settings and the Atari benchmark.

In the next section, we describe the problem setting addressed in this paper.

## 2 Setting

In our study, We address the dynamics of an agent navigating an infinite-horizon discounted Markov decision process (MDP), defined formally as \(=,,,,\). Here, \(\) represents the state space, \(\) denotes the set of actions, and \(\) quantifies the Reward function of the MDP (\(:\)). Transition dynamics are governed by \(()\), with \((0,1]\) as the discount factor. The agent interacts with the environment via a policy \(:\), guiding action selection based on observed states. This yields an action-value function \(Q^{}\), indicating the expected cumulative discounted reward from a state-action pair under \(\). The agent seeks the optimal policy maximizing the action-value function, adhering to the Bellman equation (7), given by \(Q(s,a)_{}(s^{}|s,a)[(s,a,s^{})+_{a^{}}Q(s^{},a^{})]ds\) for all states \(s\) and actions \(a\). Upon acquiring the optimal action-value function, we derive the optimal value function \(V(s)_{a}Q(s,a)\) for all states \(s\) in \(\).

**Monte-Carlo tree search** (MCTS) (20; 8) is a planning approach for complex Markov decision processes (MDPs). It employs an iterative approach:

_Selection_: It begins by selecting an action using a specified strategy, followed by executing this action through Monte Carlo simulation.

_Expansion_: Subsequently, it assesses the resulting state, either by recursively evaluating if it already exists in the search tree or by inserting it into the tree.

_Simulation_: Or employing a rollout policy via simulations. This iterative process continues until certain termination criteria are met, allowing traversal through the search tree.

_Backpropagation_: Finally, the outcomes of the simulations are propagated backward through the chosen nodes to update their statistical metrics.

**Simple Regret** An MCTS algorithm dynamically gathers trajectories within an MDP starting from an initial state \(s_{0}\). After processing \(t\) trajectories, it provides two outputs:

* \(_{t}\), a guess for the best action to take at state \(s_{0}\)
* \(_{t}(s_{0})\) an estimator of the optimal value in \(s_{0}\),

where \(s_{0}\) is the state at the root node. The algorithm's performance can be assessed by its convergence rate \(r(t)\) of the simple regret, formulated as:

\[[R(s_{0},t)]=[V^{}(s_{0})-_{t}(s_{0})] r(t),\]

Here, \(R(s_{0},t)=V^{}(s_{0})-_{t}(s_{0})\) is the simple regret of the algorithm at the root node with \(V^{}(s_{0})\) representing the optimal value at state \(s_{0}\).

In this article, we analyze an MCTS algorithm employing a maximal planning horizon \(H\) and a playout policy \(_{0}\) with value \(V_{0}\). We define \((s_{H})=V_{0}(s_{H})\) recursively as follows: for all \(h H-1\),

\[(s_{h},a)=r(s_{h},a)+_{s_{h+1}_{s_{h}}} (s_{h+1}|s_{h},a)(s_{h+1}),(s_{h})=_{ a}(s_{h},a),\] (1)

where \(r(s_{h},a)\) defined formally as the mean intermediate reward at state \(s_{h}\) after taking action \(a\). The primary objective of an MCTS algorithm is to estimate a tied rate \(r(t)\) by constructing estimates of \((s_{h},a)\) and \((s_{h})\) to ultimately estimate \((s_{0},a)\) and consequently \(Q^{}(s_{0},a)\). In practical implementations of the MCTS algorithm, the maximal depth \(H\) can sometimes be set to \(+\). However, for theoretical analysis, the maximal depth \(H\) is crucial as we will analyze the algorithm that always collects trajectories of length H.

**Distribution Reinforcement Learning** The mathematical framework used in reinforcement learning is based on the Bellman equation (37), which aims to find an agent to maximize the expected utility Q value. However, the single expected value function cannot encapsulate the stochasticity in the reward function and the dynamic of the environments. Recently, in the article (5), authors shed light on the distributional perspective of the Bellman equation by modeling each Q value function as a distribution instead of a single expected value. The main objective is to study the random return \(\) at the state \(s\), action \(a\), and is defined recursively as

\[(s,a)(s,a)+(s^{{}^{ }},a^{{}^{}}),(s^{{}^{}})_ {}(s^{{}^{}},(|s^{{}^{}})),\] (2)where \((s,a)\) is the reward distribution at the state \(s\), action \(a\), \((s,a)\) is the Q value distribution at state \(s\), action \(a\), and \((s^{{}^{}},a^{{}^{}})\) is the Q value distribution at state \(s^{{}^{}}\), action \(a^{{}^{}}\). \(s^{{}^{}}\) distributed according to \((|s,a)\), \(a^{{}^{}}\) distributed according to a policy \((|s^{{}^{}})\). \(A}{{=}}B\) denotes that two random variables \(A\) and \(B\) have equal probability laws.

This distributional approach offers a deeper understanding of uncertainty and variability, especially in complex, stochastic systems where traditional expected value representations may fail to capture the true dynamics of the problem. which has been successfully used in Deep Q Learning (5).

**Categorical Value Distribution** Based on the distributional Bellman equation, In the article (5), authors approximate the Q value distribution \((s,a)\) as a discrete categorical distribution parametrized by \(\), which denotes the number of atoms (N+1) at fixed-sized locations. This method effectively divides the Q value function into a set of equally spaced atoms \(z_{i}(s,a)=Q_{min}+i z:0 i\), where \(Q_{min}\) and \(Q_{max}\) are respectively the minimum and maximum values at state \(s\), action \(a\). The size of each atom is set as \( z:=-Q_{min}}{}\).

This discrete distribution approach is highly expressive and computationally efficient, making it ideal for practical applications. For instance, in the article (5), authors successfully used this representation in Deep Q Learning (C51), showing promising results in several Atari games. In the next section, we demonstrate how to apply this idea to MCTS.

## 3 Distributional Thompson Sampling in Tree Search

In this section, we introduce two novel distributional approaches for MCTS based on Thompson sampling. The first method represents each Q-value node as a categorical distribution, while the second uses particle-based distributions for greater flexibility. Both methods integrate Thompson sampling for improved exploration and performance.

### Distributional Monte-Carlo Tree Search

We leverage the success of distributional reinforcement learning [4; 3; 6] and apply this concept to MCTS. In MCTS, there are two types of nodes: V-nodes and Q-value nodes. Instead of treating each V value and Q value as a single expected value, we model these functions as distributions.

Based on equation (2), we can derive

\[(s,a)}{{=}}(s,a)+ (s^{{}^{}}),(s^{{}^{}})}{{=}}_{a^{{}^{}}(|s^{{}^{}})} (s^{{}^{}},a^{{}^{}}),\] (3)

with \(s^{{}^{}}(|s,a)\), where \((|s^{{}^{}})\) is formally defined as the tree policy at state \(s^{}\). We can model any Q distribution with equal law distributed as the sum of the distributions of the next reward and the Q distributions of the next states actions. We further model each V distribution, having equal probability law to the expectation of the chosen policy of the next Q-value distributions (3).

Our method follows the same four basic steps of MCTS but is different in Value Backup and Action selection steps. We introduce two distinct methodologies: categorical-based and particle-based. In the categorical based approach, we parameterize each V value and Q value function in the tree as a categorical distribution. In contrast, in the particle-based approach, we model each value distribution as a set of sampling particles, representing the values observed during the tree planning. We provide a detailed explanation for the value backup and action selection of each method in the next section.

### Value Backup

In this work, we employ two approaches to represent the Q value distribution.

**Categorical distribution**: we represent each node in the tree as a categorical distribution. In each Q-value node, we: (1) store the empirical mean value of that Q-value node (same as in UCT), and (2) maintain a categorical distribution of the Q value function. To define a categorical distribution Q function, we require three essential pieces of information:

* The number of atoms (\(+1\)): We choose a consistent number of atoms (\(+1\)) that remains the same for all Q distributions along the tree.
* Minimum and maximum values (_min_ and _max_): Each node in the tree may have different ranges for its minimum (\(Q_{min}\))1 and maximum (\(Q_{max}\)) values, depending on its state/action in the environment. When a new Q-value node is added to the tree, we initially set \(Q_{min}\) to 0 (assuming we have scaled the reward range to [0, R]) and initialize \(Q_{max}\) to a small number, e.g., \(Q_{max}=0.001\). Since the min and max values are unknown, we start with a small range, that will get updated accordingly to the scale of the observed values.
* Probabilistic parameterization: The probability of each atom (\(p_{i}(s,a)\)) is determined based on the visitation count ratio. In detail, each atom stores statistical information about the visitation count, and the probability of that atom will be calculated as the visitation count divide with the total visitation count of that Q-value node. When we backpropagate the \(r_{t}(s,a)+_{t}(s^{})\) value to a specific node, we identify the atom whose value range includes the \(r_{t}(s,a)+_{t}(s^{})\) value. At this point, we increase its visitation count.

Additionally, as we backpropagate Monte-Carlo Q values over time, we empirically adjust the \(Q_{min}\) and \(Q_{max}\) values to account for the dynamic range of Q values observed in the tree. This dynamic scaling ensures that the atom locations are effectively rescaled to adapt to the changing conditions. This representation method allows us to encapsulate the knowledge gained through exploration in the form of categorical distributions, which helps in making informed decisions during the tree search.

**Particle based distribution**: We represent each Q value distribution as a collection of sampling particles, which encapsulate the observed values during tree planning. Initially, we maintain an empty set of particles for the Q value distribution, denoted as \((s,a)\). At time step \(t\), upon receiving an intermediate reward \(_{t}(s,a)=r_{t}(s,a)+_{t}(s^{})\), with \(s^{}(|s,a)\), we add \(_{t}(s,a)\) to the set \((s,a)\) if the particle does not already exist within it. If the particle \(_{t}(s,a)\) already exists in \((s,a)\), we increase the visitation count ratio associated with that particle.

**Value function:** The Q-value node is crucial in the tree because its representation influences action selection, as detailed in the next section. We now discuss modeling each V-value node. The V-value distribution is based on the expected outcomes of the chosen policy and the subsequent Q-distributions. Thus, the mean of the V-function corresponds to the tree policy's expectation of the means of all succeeding Q-value nodes. The common approach is to use empirical average mean for the value backup, as in UCT [(21)]. However, this approach underestimates the optimal value, while using the maximum value overestimates it [(9)]. The power mean estimator [(12)] provides a balanced solution, falling between the average and maximum values. In our methods, each V node stores the power mean of the empirical means of all succeeding Q-value nodes, eliminating the need to model V as a distribution.

\[(s)=(_{a}^{T_{s,a}(n)}^{p}(s,a))^{ {p}},p 1,\]

where \(T_{s}(n),T_{s,a}(n)\) are the number of visitations at \(s\) and \(s,a\) at timestep \(n\) respectively. Next, we show how to select actions in the tree based on the categorical distribution of Q-value nodes.

### Action Selection

Thompson sampling has shown promising results in real bandit scenarios due to the randomness of action selection. Taking advantage of the established categorical based distribution and particle based distribution, we use the Thompson sampling method for action selection. We maintain a Dirichlet distribution of parameter of the Q value distribution. We denote the Dirichlet distribution of parameters \((^{0},^{1},,^{N})\) by \((^{0},^{1},,^{N})\), whose density function is given by \(^{N}^{i})}{_{i=0}^{N}(^{i})} _{i=0}^{N}x_{i}^{^{i}-1}\) for \((x_{0},,x_{N})^{N+1}\) such that \(_{i=0}^{N}x_{i}=1\).

**Categorical distribution**: The probability mass function of the discrete categorical distribution at each Q-value node at state \(s\), action \(a\): \(p(s,a)=[p_{0}(s,a),p_{1}(s,a),,p_{N}(s,a)]\), where \(p_{i}(s,a)\) represents the probability of selecting the \(i\)-th atom \(z_{i}(s,a),N+1\) is the number of atoms. We maintain a Dirichlet distribution \((^{0}(s,a),^{1}(s,a),,^{N}(s,a))\) as the prior for the Q-value node at state \(s\), action \(a\). At each time step \(t\) we sample \(L_{t}(s,a)(^{0}(s,a),^{1}(s,a),,^{N}(s,a))\) and compute \(_{t}(s,a)=[z_{0}(s,a),z_{1}(s,a),,z_{N}(s,a)]^{}L_{t} (s,a)\). Then, the action \(a_{t}\) is selected as follows:

\[a_{t}=*{arg\,max}_{a}\{_{t}(s,a)\}\]

After taking action \(a_{t}\) and get an intermediate reward \(_{t}(s,a_{t})=r_{t}(s,a_{t})+_{t}(s^{})\). The posterior is also a Dirichlet: \((^{0}(s,a),,^{t}(s,a)+1,,^{N}(s,a))\) with the intermediate reward at time step \(t\): \(_{t}(s,a_{t})\) is in the range of the atom \(z_{t}(s,a)\). We denote this mechanism as Categorical Thompson sampling for Tree Search (CATS) method.

**Particle based distribution**: In the particle-based approach, the prior Dirichlet distribution of the Q-value node at state \(s\), action \(a\) is \(((s,a))\), with \((s,a)\) is initiated as . Considering each Q value distribution at state \(s\), action \(a\) has a set of particle \(\{_{t}(s,a)\}\) with the corresponding weighted \((s,a)=\{^{t}(s,a)\}\) At each time step \(t\) we also sample \(L_{t}(s,a)((s,a))\) and compute \(_{t}(s,a)=[1,_{0}(s,a),_{1}(s,a),,_{N}(s,a)]^{}L_{t}(s,a)\). Then the action \(a_{t}\) is chosen as

\[a_{t}=*{arg\,max}_{a}\{_{t}(s,a)\}.\]

After taking action \(a_{t}\) and get an intermediate reward \(_{t}(s,a_{t})=r_{t}(s,a_{t})+_{t}(s^{})\). We update \(^{t}(s,a)=^{t}(s,a)+1\) if \(_{t}(s,a_{t})\) is in the set \(\{_{t}(s,a)\}\). If not, we add \(_{t}(s,a_{t})\) to the set \(\{_{t}(s,a)\}\) and add \(1\) to the set \(\{^{t}(s,a)\}=\{^{t}(s,a),1\}\).

We call this method as Particle Thompson sampling for Tree Search (PATS) method. Detailed pseudocode and a comparison of CATS and PATS can be seen in Fig 1. The two methods are identical in all procedures except for the Q value function backup (**SimulateQ**) and the action selection function (**SelectAction**).

**Remark 1**.: _CATS and PATS both use similar action selection strategies within a bandit setting, specifically referring to Multinomial Thompson Sampling and Non-Parametric Thompson Sampling, respectively [(29)]. While CATS action selection heavily depends strictly on Thompson Sampling by maintaining parameters of posterior Q-value distribution, PATS is not based on the posterior sampling in the strict sense. At each step, it computes an average of the observed rewards with random weight and is a Non-Parametric approach. Furthermore, CATS maintains a fixed set of atoms, whereas in PATS, the number of particles increases depending on the observed Q values._

In the next section, we provide a theoretical analysis of the convergence of simple regret for CATS and PATS.

## 4 Theoretical analysis

Planning in MCTS involves making a sequence of decisions along the tree, where each internal node functions as a non-stationary bandit, with the empirical mean drifting due to the action selection strategy. Therefore, we first study the non-stationary multi-armed bandit settings using the action selections of CATS and PATS, examining the concentration properties of the power mean backup for each arm relative to the optimal arm. We then apply these results to MCTS.

### Non-stationary multi-armed bandit

We consider a class of non-stationary multi-armed bandit (MAB) problems with \(K 1\) arms. Let \(R_{a,t}\) denote the random reward obtained by playing arm \(a[K]\) at the time step \(t\) bounded in \([0,R]\). We consider \(_{a,n}=_{t=1}^{n}R_{a,t}\) as the average rewards collected at arm \(a\) after n plays. We first define:

**Definition 1**.: _A sequence of estimators \((_{n})_{n 1}\) is concentrated and convergent towards some limit \(V\) if the following two properties hold:_

1. _Concentration: For all_ \(n 1\)_, for all_ \(>0\)_,_ \( c>0\) _that_ \((|_{n}-V|>) cn^{-1}^ {-1}\)_._
2. _Convergence:_ \(_{n}[_{n}]=V\)_._

_In that case, we write \(}_{n}=V\)._

We assume that the reward sequence \(\{R_{a,t}\},t 1\) is a non-stationary process satisfying the convergence and concentration properties from Definition 1, by making the following assumption:

**Assumption 1**.: _Consider K arms that for \(a[K]\), let \((_{a,n})_{n 1}\) be a sequence of estimator satisfying_

\[}_{a,n}=_{a}.\]

The action selection of CATS and PATS follows closely as in Section 3.3 and pseudocode are shown in Fig. 2. Let us define \(_{n}(p)=(_{a=1}^{K}(n)}{n}_{a,T_ {a}(n)}^{p})^{}\) as the power mean value backup operator after \(n\) rounds. Here \(1 p<\) is a constant. We denote \(T_{a}(n)\) is the number of visitations of the arm \(a\).

We define \(_{}=_{a[K]}\{_{a}\}\) and assume that \(_{}\) is unique. Then, we establish the concentration and convergence properties of the power mean backup operator \(_{n}(p)\) towards the optimal value \(_{}\), as shown in Theorem 1 and Theorem 2, respectively for CATS and PATS.

**Theorem 1**.: _For \(a[K]\), let \((_{a,n})_{n 1}\) be a sequence of estimator satisfying \(}_{a,n}=_{a}\) and let \(_{}=_{a}\{_{a}\}\). Assume that all the estimators are bounded in \([0,R]\). We consider a bandit algorithm that selects each arm according to CATS once in each round \(n K\). Then, \(}_{n}(p)=_{}\)._

**Theorem 2**.: _For \(a[K]\), let \((_{a,n})_{n 1}\) be a sequence of estimator satisfying \(}_{a,n}=_{a}\) and let \(_{}=_{a}\{_{a}\}\). Assume that all the estimators are bounded in \([0,R]\). We consider a bandit algorithm that selects each arm according to PATS once in each round \(n K\). Then, \(}_{n}(p)=_{}\)._

Figure 2: Comparing CATS (left) and PATS (right) in Non-stationary bandits.

Detailed proofs of the two Theorems can be found in the appendix. Based upon these results we analyse the concentration properties for any internal node and convergence of the simple regret in the MCTS in the next section.

### Monte-Carlo Tree Search

Before presenting the main results (Theorem 3 Theorem 4), we first show an important Lemma

**Lemma 1**.: _Let \((_{m,n})_{n 1}\), \(m[M]\), be a sequence of estimator satisfying \(}_{m,n}=V_{m}\). Assume that there exists a constant \(L>0\) such that \(L=(_{m,n})_{n 1}\). Let \(R_{i}\) be an iid sequence with mean \(\) and \(S_{i}\) be an iid sequence from a distribution \(p=(p_{1},,p_{M})\) supported on \(\{1,,M\}\). Introducing the random variables \(N_{m}^{n}=\#|\{i n:S_{i}=s_{m}\}|\), we define the sequence of estimator_

\[_{n}=_{i=1}^{n}R_{i}+_{m=1}^{M}^{n}}{n}_{m,N_{m}^{n}}.\]

_Then \(}_{n}=+_{m=1}^{M} p_{m}V_{m}\)._

The significance of Lemma 1 lies in demonstrating the concentration and convergence of an estimated Q value, conditioned on the concentration and convergence of a child V-value node. Here, \(_{,n}\) represents the value estimation at time step \(n\), and \(R_{i}\) denotes an intermediate reward received by taking a specific action at a particular state.

Next, we first start with Theorem 3 to show the convergence and concentration of any V-Node and Q-node in the tree for CATS.

**Theorem 3**.: _When we apply the CATS algorithm, we have_

1. _For any node_ \(s_{h}\) _at the depth_ \(h^{}\) _in the tree,_ \(}_{n}(s_{h},a_{k})= (s_{h},a_{k})\)_._
2. _For any node_ \(s_{h}\) _at the depth_ \(h^{}\) _in the tree,_ \(}_{n}(s_{h})= (s_{h})\)_._

We can derive a similar result for PATS as shown in Theorem 4.

**Theorem 4**.: _When we apply the PATS algorithm, we have_

1. _For any node_ \(s_{h}\) _at the depth_ \(h^{}\) _in the tree,_ \(}_{n}(s_{h},a_{k})= (s_{h},a_{k})\)_._
2. _For any node_ \(s_{h}\) _at the depth_ \(h^{}\) _in the tree,_ \(}_{n}(s_{h})= (s_{h})\)_._

The results of Theorems 4 and 4 demonstrate that, at any node in the tree, both the V-value and Q-value nodes are convergent and concentrated. These results are applicable to any power mean backup operator of V-value nodes with \(p[1,+)\). Finally, we show important results in Theorem 5, and Theorem 6, since they show the convergence of simple regret of CATS and PATS, respectively.

**Theorem 5**.: _(Convergence of Simple Regret of CATS) We have at the root node \(s_{0}\),_

\[|[V^{}(s_{0})-_{n}(s_{0}) ]| O(n^{-1}).\]

**Theorem 6**.: _(Convergence of Simple Regret of PATS) We have at the root node \(s_{0}\),_

\[|[V^{}(s_{0})-_{n}(s_{0}) ]| O(n^{-1}).\]

**Remark 2**.: _These results demonstrate that both CATS and PATS share the same convergence rate for value estimation at the root node of \((n^{-1})\), which improves over the rate \((n^{-1/2})\) of Fixed-Depth-MCTS (33). Furthermore, Our finding more broadly applies to the power mean estimator with \(p[1,+)\)._

## 5 Experiments

We compare our methods with UCT (21), Fixed-Depth-MCTS (33), MENTS (40), REITS, TENTS (14), BTS (27) and DNG (1) in a stochastic setting (_SyntheticTree_) to highlight the benefits of CATS and PATS in stochastic environments. Additionally, we test on 17 Atari games, comparing our algorithms with DQN (base network without planning) and other non-distributional planning methods (Power-UCT (12), MENTS (40), TENTS (14)) to demonstrate CATS and PATS' competitiveness and put results in Appendix. In all settings, we use 100 atoms for CATS, and set the discount factor \(\) to 0.99 for Atari, and \(\) to 1 for _SyntheticTree_.

**SyntheticTree**: We evaluate CATS and PATS using the synthetic tree toy problem (14). This problem involves a tree with depth \(d\) and branching factor \(k\). Each tree edge has a random value between 0 and 1. Returns at the leaf nodes are simulated using Gaussian distributions with means equal to the sum of edge values from the root to the leaf, and a standard deviation of \(0.5\). Means are normalized between 0 and 1. An agent traverses the tree from the root, aiming to find the leaf node with the highest mean value. Internal nodes give zero reward, while leaf nodes provide a reward sampled from their Gaussian distribution. We introduce stochasticity into the environment by altering the transition probabilities: there is a \(50\%\) chance of moving to the intended node and a \(50\%\) chance of moving to a different node with equal probability. We conduct 25 experiments on five trees with five runs each, covering all combinations of branching factors \(k=\{2,4,6,8,10,12,14,16,100,200\}\) and depths \(d=\{1,2,3,4\}\). We compute the value estimation error at the root node. Fig. 3 shows the convergence of the value estimations of CATS and PATS at the root node in the Synthetic Tree environment which shows they archives faster convergence compared to other methods.

## 6 Conclusion

To conclude, our work introduces Categorical Thompson Sampling for MCTS (CATS) and Particle Thompson Sampling for MCTS (PATS), distributional planning approaches specifically designed to tackle complexities arising from stochasticity. CATS uses a categorical distribution, while PATS uses a particle-based distribution to represent and model the uncertainty inherent in return outcomes. We also propose exploration strategies based on Thompson Sampling that leverage this distributional modeling. Our methods come with a rigorous theoretical convergence guarantee, achieving a simple regret polynomial decay of the order \(O(n^{-1})\), which improves over the \(O(n^{-1/2})\) rate of the fixed version of UCT (32). Empirical findings conclusively demonstrate the effectiveness of our approach in stochastic environments.