# Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs

Matthew Zurek

Department of Computer Sciences

University of Wisconsin-Madison

matthew.zurek@wisc.edu

&Yudong Chen

Department of Computer Sciences

University of Wisconsin-Madison

yudong.chen@wisc.edu

###### Abstract

We study the sample complexity of learning an \(\)-optimal policy in an average-reward Markov decision process (MDP) under a generative model. For weakly communicating MDPs, we establish the complexity bound \((SA}{^{2}})\), where \(\) is the span of the bias function of the optimal policy and \(SA\) is the cardinality of the state-action space. Our result is the first that is minimax optimal (up to log factors) in all parameters \(S,A,\), and \(\), improving on existing work that either assumes uniformly bounded mixing times for all policies or has suboptimal dependence on the parameters. We also initiate the study of sample complexity in general (multichain) average-reward MDPs. We argue a new transient time parameter \(\) is necessary, establish an \((SA+}{^{2}})\) complexity bound, and prove a matching (up to log factors) minimax lower bound. Both results are based on reducing the average-reward MDP to a discounted MDP, which requires new ideas in the general setting. To optimally analyze this reduction, we develop improved bounds for \(\)-discounted MDPs, showing that \((SA}{(1-)^{2}^{2}})\) and \((SA+}{(1-)^{2}^ {2}})\) samples suffice to learn \(\)-optimal policies in weakly communicating and in general MDPs, respectively. Both these results circumvent the well-known minimax lower bound of \((SA^{2}})\) for \(\)-discounted MDPs, and establish a quadratic rather than cubic horizon dependence for a fixed MDP instance.

## 1 Introduction

The paradigm of Reinforcement learning (RL) has demonstrated remarkable successes in various sequential learning and decision-making problems. Empirical successes have motivated extensive theoretical study of RL algorithms and their fundamental limits. The RL environment is commonly modeled as a Markov decision process (MDP), where the objective is to find a policy \(\) that maximizes the expected cumulative rewards. Different reward criteria are considered, such as the finite horizon total reward \(^{}_{t=0}^{T}R_{t}\) and the infinite horizon total discounted reward \(^{}[_{t=0}^{}^{t}R_{t}]\) with a discount factor \(<1\). The finite horizon criterion only measures performance for \(T\) steps, and the discounted criterion is dominated by rewards from the first \(\) time steps. In many situations where the long-term performance of the policy \(\) is of interest, we may prefer to evaluate policies by their long-run average reward \(_{T}(1/T)^{}_{t=0}^{T-1}R_{t} \).

A foundational theoretical problem in RL is the sample complexity for learning a near-optimal policy using a generative model of the MDP , meaning the ability to obtain independent samples of the next state given any initial state and action. For the finite horizon and discounted reward criteria, the sample complexity of this task has been thoroughly studied (e.g., ). However, despite significant effort (reviewed in Section 1.1), the sample complexity of the average reward setting is unresolved in existing literature.

Our contributionsIn this paper, we resolve the sample complexity of weakly communicating Average-Reward MDPs (AMDP) in terms of \(:=\|h^{*}\|_{}\), the span of the bias (a.k.a. relative value function) of the optimal policy. We show that \(SA/^{2}\) samples suffice to find an \(\)-optimal policy of a weakly communicating MDP with \(S\) states and \(A\) actions. This bound, presented in Theorem 2, is the first that matches the minimax lower bound \(SA/^{2}\) up to log factors.

Furthermore, we initiate the study of sample complexity for average-reward _general MDPs_, which refers to the class of all finite-space MDPs without any restrictions . General MDPs are not necessarily weakly communicating and all their optimal policies may be _multichain_. In this general setting, we demonstrate the span \(\)_alone_ cannot characterize the sample complexity, as the lower bound in Theorem 4 exhibits instances which require \(SA/^{2}\) samples. This observation motivates our introduction of a new _transient time bound_ parameter \(\), which in conjunction with \(\) captures the sample complexity of general average-reward MDPs. Specifically, our Theorem 8 shows that \((SA+}{^{2}})\) samples suffice to learn an \(\)-optimal policy, and Theorem 4 provides a matching minimax lower bound of \((SA+}{^{2}})\). We remark that it is trivially impossible to achieve low regret in standard online settings of general MDPs, since the agent may become trapped in a closed class of low reward states . The simulator setting is natural for studying general MDPs since it avoids this fatal issue, although the existence of multiple closed classes with different long-run rewards still plays a fundamental role in the minimax sample complexity, as reflected in the dependence on \(\).

To establish the above upper bounds, we adopt the reduction-to-discounted-MDP approach [9; 20], and improve on prior work by developing enhanced sample complexity bounds for \(\)-discounted MDPs (DMDPs). We improve the analysis of variance parameters related to DMDPs using a new multistep variance Bellman equation, which is applied in a recursive manner to bound the variance of near-optimal policies. For general (multichain) MDPs, we further utilize law-of-total-variance ideas to bound the total variance contribution from transient states, which present new challenges significantly different to their behavior in the weakly communicating setting. Our average-to-discounted reduction also requires new techniques, because many structural properties used in earlier reduction arguments no longer hold for general MDPs. Our analysis leads to DMDP sample complexities of \(SA}{(1-)^{2}^{2}}\) and \(SA+}{(1-)^{2}^ {2}}\) to learn \(\)-optimal policies in weakly communicating and general MDPs, respectively. Notably, the latter bound, valid for all MDPs, circumvents the existing lower bound \(^{2}}\)[3; 15]. Whereas this minimax lower bound allows the adversary to choose the transition matrix \(P\) based on \(\) with \(\)[3, Theorem 3], our result reflects the complexity of a _fixed_ MDP \(P\) through its parameters \(,\) and a quadratic dependence on the effective horizon \(\). This fixed-\(P\) complexity is essential for our particular algorithmic approach, where the reduction discount \(\) is chosen depending on \(P\). It is also a more relevant framework in general for many RL problems where the discount factor is tuned for best performance on a particular instance.

### Comparison with related work on average-reward MDPs

We summarize in Table 1 existing sample complexity results for average reward MDPs.

Various parameters have been used to characterize the sample complexity of average reward MDPs, including the diameter \(D\) of the MDP, the uniform mixing time bound \(_{}\) for all policies, and the span \(\) of the optimal bias; formal definitions are provided in Section 2. All sample complexity upper bounds involving \(_{}\) require the strong assumption that _all_ stationary deterministic policies have finite mixing times. Otherwise, \(_{}=\) by definition, which for example occurs if some policy induces a periodic Markov chain. It is also possible to have \(D=\), while \(\) and our newly introduced \(\) are always finite for finite state-action spaces. As shown in , there is generally no relationship between \(D\) and \(_{}\); they can each be arbitrarily larger than the other. On the other hand, it has been shown that \( D\) and that \( 8_{}\). Therefore, either of the first two minimax lower bounds in Table 1 (which both use hard instances that are weakly communicating) imply a lower bound of \((SA}{^{2}})\) and thus the minimax optimality of our Theorem 2.

To the best of our knowledge, no prior work has considered the average-reward sample complexity of general (potentially multichain) MDPs. Existing results make assumptions at least as strong as weakly communicating or uniformly bounded mixing times.

The work  was the first to develop an algorithm based on reduction to a discounted MDP with a discount factor of \(=1-}}\). Their argument was improved in , which improved the uniform mixing assumption to only assuming a weakly communicating MDP, and used a smaller discount factor \(=1-}\). These arguments both make essential use of the fact that the optimal gain is independent of the starting state, which does not hold for general MDPs. After analyzing the reductions, both  and  then solved the discounted MDPs by appealing to the algorithm from . To the best of our knowledge, the algorithm of  is the only known algorithm for discounted MDPs which could work with either reduction, as the reductions each require a \(\)-optimal policy from the discounted MDP, and other known algorithms for discounted MDPs do not permit such large suboptimality levels. (We discuss algorithms for discounted MDPs in more detail below.) Other algorithms for average-reward MDPs are considered in . The above results fall short of matching the minimax lower bounds.

While preparing this manuscript, we became aware of , which considers the uniform mixing setting and obtains a minimax optimal sample complexity \((SA}}{^{2}})\) in terms of \(_{}\). Although developed independently, their work and ours have several similarities. We both utilize discounted reductions and observe that it is possible to improve the sample complexity of the resulting DMDP task by improving the analysis of variance parameters. They accomplish the improvement by leveraging the uniform mixing assumption, whereas we make use of the low span of the optimal policy. Note that \( 8_{}\) holds in general and there exist MDPs with \(_{}=\), so our Theorem 2 is strictly stronger than the result of .

### Comparison with related work on discounted MDPs

We discuss a subset of results for discounted MDPs in the generative setting. Several works  obtain the minimax optimal sample complexity of \(SA^{2}}\) for finding an \(\)-optimal policy w.r.t. the discounted reward. However, only  is able to show this bound for the full range of \((0,]\). As mentioned, the reduction from average reward MDPs requires a large \(\) in the resulting discounted MDP, making it unsurprising that all of  as well as our Algorithm 1 essentially use their algorithm. The matching lower bound is established in .

As mentioned earlier, both we and the authors of  independently observed that the \(SA^{2}}\) sample complexity lower bound can be circumvented in the settings that arise

   Method & Sample Complexity & Reference & Comments \\  Primal-Dual SMD & \(SA}^{2}}{^{2}}\) &  & requires uniform mixing \\ Reduction to DMDP & \((SA}^{2}}{^{2}})\) &  & requires uniform mixing \\ Policy Mirror Descent & \(SA}^{2}}{^{2}}\) &  & requires uniform mixing \\ Reduction to DMDP & \((SA}}{^{2}})\) &  & requires uniform mixing \\  Reduction to DMDP & \((SA}{^{3}})\) &  & weakly communicating \\ Refined Q-Learning & \(SA^{2}}{^{2}}\) &  & weakly communicating \\ Reduction to DMDP & \((SA}{^{2}})\) & Our Theorem 2 & weakly communicating \\  Reduction to DMDP & \(SA}{^{2}}\) & Our Theorem 8 & general MDPs \\  Lower Bound & \((SA}^{2}}{^{2}})\) &  & implies \((SA}{^{2}})\) \\ Lower Bound & \((SA}{^{2}})\) &  & implies \((SA}{^{2}})\) \\ Lower Bound & \((SA}{^{2}})\) & Our Theorem 4 & general MDPs \\   

Table 1: **Algorithms and sample complexity bounds for average reward MDPs with \(S\) states and \(A\) actions. The goal is finding an \(\)-optimal policy under a generative model. Here \(:=\|h^{}\|_{}\) is the span of the optimal bias, \(_{}\) is a uniform upper bound on mixing times of all policies, and \(D\) is the MDP diameter, with the relationships \( 8_{}\) and \( D\). \(\) is the transient time parameter.**under the average-to-discounted reductions. The authors of [22; 21] assume uniform mixing and obtain a discounted MDP sample complexity of \(SA}}{(1-)^{2}e^{2}}\), first in  by modifying the algorithm of , and then in  under a wider range of \(\) by instead modifying the analysis of . The work  also proves a matching lower bound. Our Theorem 1 for discounted MDPs attains a sample complexity of \(SA}}{(1-)^{2} ^{2}}\) assuming only that the MDP is weakly communicating. Again, in light of the relationship that \(} 8_{}\), our results are strictly better (ignoring constants), and their lower bound also establishes the optimality of our Theorem 1.

## 2 Problem setup and preliminaries

A Markov decision process (MDP) is given by a tuple \((,,P,r)\), where \(\) is the finite set of states, \(\) is the finite set of actions, \(P:()\) is the transition kernel with \(()\) denoting the probability simplex over \(\), and \(r:\) is the reward function. Let \(:=||\) and \(A:=||\) denote the cardinality of the state and action spaces, respectively. Unless otherwise noted, all policies considered are stationary Markovian policies of the form \(:()\). For any initial state \(s_{0}\) and policy \(\), we let \(_{s_{0}}^{}\) denote the expectation with respect to the probability distribution over trajectories \((S_{0},A_{0},S_{1},A_{1},)\) where \(S_{0}=s_{0}\), \(A_{t}(S_{t})\), and \(S_{t+1} P( S_{t},A_{t})\). Equivalently, this is the expectation with respect to the Markov chain induced by \(\) starting in state \(s_{0}\), with the transition probability matrix \(P_{}\) given by \((P_{})_{s,s^{}}:=_{a}(a|s)P(s^{ } s,a)\). We also define \((r_{})_{s}:=_{a}(a|s)r(s,a)\). We occasionally treat \(P\) as an \(()\)-by-\(\) matrix where \(P_{sa,s^{}}=P(s,a,s^{})\). We also let \(P_{sa}\) denote the row vector such that \(P_{sa}(s^{})=P(s,a,s^{})\). For any \(s\) and any bounded function \(X\) of the trajectory, we define the variance \(_{s}^{}[X]:=_{s}^{}(X-_{s} ^{}[X])^{2}\), with its vector version \(^{}[X]^{}\) given by \((^{}[X])_{s}=_{s}^{}[X]\). For \(s\), let \(e_{s}^{}\) be the vector that is all \(0\) except for a \(1\) in entry \(s\). Let \(^{}\) be the all-one vector. For each \(v^{}\), define the span semi-norm \(\|v\|_{}:=_{s}v(s)-_{s}v(s)\).

Discounted reward criterionA discounted MDP is a tuple \((,,P,r,)\), where \((0,1)\) is the discount factor. For a stationary policy \(\), the (discounted) value function \(V_{}^{}:[0,)\) is defined, for each \(s\), as \(V_{}^{}(s):=_{s}^{}[_{t=0}^{}^{t}R _{t}]\), where \(R_{t}=r(S_{t},A_{t})\) is the reward received at time \(t\). It is well-known that there exists an optimal policy \(_{}^{}\) that is deterministic and satisfies \(V_{}^{_{}^{}}(s)=V_{}^{}(s):=_{}V_{ }^{}(s)\) for all \(s\). In discounted MDPs the goal is to compute an \(\)-optimal policy, which we define as a policy \(\) satisfying \(\|V_{}^{}-V_{}^{}\|_{}\). We define one more variance parameter \(_{P_{}}[V_{}^{}]^{}\), specific to a given policy \(\), by \(_{P_{}}[V_{}^{}]_{s}:=_{s^{ }}(P_{})_{s,s^{}}V_{}^{ }(s^{})-_{s^{}}(P_{})_{s,s^{ }}V_{}^{}(s^{})^{2}\).

Average-reward criterionIn an MDP \((,,P,r)\), the average reward per stage or the _gain_ of a policy \(\) starting from state \(s\) is defined as \(^{}(s):=_{T}_{s}^{}_{t=0} ^{T-1}R_{t}\). The _bias function_ of any stationary policy \(\) is \(h^{}(s):=_{T}_{s}^{}_{t= 0}^{T-1}(R_{t}-^{}(S_{t}))\), where \(\) denotes the Cesaro limit. When the Markov chain induced by \(P_{}\) is aperiodic, \(\) can be replaced with the usual limit. For any policy \(\), its \(^{}\) and \(h^{}\) satisfy \(^{}=P_{}^{}\) and \(^{}+h^{}=r_{}+P_{}h^{}\).

A policy \(^{}\) is Blackwell-optimal if there exists some discount factor \((0,1)\) such that for all \(\) we have \(V_{}^{^{}} V_{}^{}\) for all policies \(\). Henceforth we let \(^{}\) denote some fixed Blackwell-optimal policy, which is guaranteed to exist when \(S\) and \(A\) are finite . We define the optimal gain \(^{}^{}\) by \(^{}(s)=_{}^{}(s)\) and note that we have \(^{}=^{^{}}\). For all \(s\), \(^{}(s)_{a}P_{sa}^{}\), or equivalently \(^{} P_{}^{}\) for all policies \(\) (and this maximum is achieved by \(^{}\)). We also define \(h^{}=h^{^{}}\) (and we note that this definition does not depend on which Blackwell-optimal \(^{}\) is used, if there are multiple). For all \(s\), \(^{}\) and \(h^{}\) satisfy \(^{}(s)+h^{}(s)=_{a:P_{sa}^{}=^{ }(s)}r_{sa}+P_{sa}h^{}\), known as the (unmodified) Bellman equation.

A weakly communicating MDP is such that the states can be partitioned into two disjoint subsets \(=_{1}_{2}\) such that all states in \(_{1}\) are transient under any stationary policy and within \(_{2}\), any state is reachable from any other state under some stationary policy. In weakly communicating MDPs \(^{}\) is a constant vector (all entries are equal), and thus \((^{},h^{})\) are also a solution to the modified Bellman equation \(^{}(s)+h^{}(s)=_{a}r_{sa}+P_{sa}h^{}\). When discussing weakly communicating MDPs we occasionally abuse notation and treat \(^{}\) as a scalar. A stationary policy is multichain if it induces multiple closed irreducible recurrent classes, and an MDP is called multichain if it contains such a policy. Weakly-communicating MDPs always contain some gain-optimal policy which is unichain (not multichain), but in general MDPs, all gain-optimal policies may be multichain and \(^{}\) may not be a constant vector. All uniformly mixing MDPs are weakly communicating. In the average reward setting, our goal is find an \(\)-optimal policy, defined as a policy \(\) such that \(\|^{}-^{}\|_{}\).

Complexity parametersOur most important complexity parameter is the span of the optimal bias function \(:=\|h^{}\|_{}\). In addition, for general MDPs we introduce a new _transient time parameter_\(\), defined as follows. Let \(\) be the set of deterministic stationary policies. For each \(\), let \(^{}\) be the set of states which are recurrent in the Markov chain \(P_{}\), and let \(^{}=^{}\) be the set of transient states. Let \(T_{^{}}=\{t:S_{t}^{}\}\) be the first hitting time of a state which is recurrent under \(\). We say an MDP satisfies the _bounded transient time property with parameter \(\)_ if for all policies \(\) and states \(s\) we have \(_{s}^{}[T_{^{}}]\), or in words, the expected time spent in transient states (with respect to the Markov chain induced by \(\)) is bounded by \(\).

We recall several other parameters used in the literature to characterize sample complexity. The diameter is defined as \(D:=_{s_{1} s_{2}}_{}_{s_{1}}^{}[{}_{ _{s_{2}}}]\), where \(_{s}\) denotes the hitting time of a state \(s\). For each policy \(\), if the Markov chain induced by \(P_{}\) has a unique stationary distribution \(_{}\), we define the mixing time of \(\) as \(_{}:=\{t 1:_{s}\| (P_{})^{t}-_{}^{}\|_{1}\}\). If all policies \(\) satisfy this assumption, we define the uniform mixing time \(_{}:=_{}_{}\). Note that \(D\) and \(_{}\) are generally incomparable , while we always have \( D\) and \( 8_{}\). It is possible for \(_{}=\), for instance if there are any policies which induce periodic Markov chains. Also, \(D=\) if there are any states which are transient under all policies. However, \(\) and \(\) are finite in any MDP with \(S,A<\). Also if \(_{}\) is finite, Lemma 27 shows \( 4_{}\).

We assume access to a generative model , also known as a simulator. This means we can obtain independent samples from \(P( s,a)\) for any given \(s,a\), but \(P\) itself is unknown. We assume the reward function \(r\) is deterministic and known, which is standard in generative settings (e.g., ) since otherwise estimating the mean rewards is relatively easy. Specifically, to learn an \(\)-optimal policy for the discounted MDP, we would need to estimate each entry of \(r\) to accuracy \(O((1-))\), which requires a lower order number of samples \(^{2}}\). For this reason we assume (as in ) that \( 1\). Using samples from the generative model, our Algorithm 1 constructs an empirical transition kernel \(\). For a policy \(\), we use \(_{}^{}(s)\) to denote the value function computed with respect to the Markov chain with transition matrix \(_{}\) (as opposed to \(P_{}\)). Our Algorithm 1 also utilizes a perturbed reward function \(\), and we use the notation \(V_{,}^{}(s)\) to denote a value function computed using this reward (and \(P_{}\)); more concretely, we replace \(R_{t}\) with \(_{t}=(S_{t},A_{t})\) in the definition above of \(V_{}^{}\). We use the notation \(_{,}^{}\) when using \(\) and \(\) simultaneously.

## 3 Main results for weakly communicating MDPs

Our approach is based on reducing the average-reward problem to a discounted problem. We first present our algorithm and guarantees for the discounted MDP setting. As discussed in Subsection 1.1, our algorithm of choice, Algorithm 1, is essentially the same as the one presented in , with a slightly different perturbation level \(\). Algorithm 1 constructs an empirical transition kernel \(\) using \(n\) samples per state-action pair from the generative model, and then solves the resulting empirical (perturbed) MDP \((,,)\). As noted in , the perturbation ensures \(_{,}^{}\) can be computed exactly in \((,S,A,(1/))\) time by multiple standard MDP solvers. We remark in passing that the \(SA\)-by-\(S\) transition matrix \(\) has at most \(nSA\) nonzero entries.

Our Theorem 1 provides an improved sample complexity bound for Algorithm 1 under the setting that the MDP is weakly communicating.

**Theorem 1** (Sample Complexity of Weakly Communicating DMDP).: _Suppose the discounted MDP \((P,r,)\) is weakly communicating, \(\), and \(\). There exists a constant \(C_{2}>0\) such that, for any \((0,1)\), if \(n C_{2}}{(1-)^{2}^{2}} {(1-)}\), then with probability at least \(1-\), the policy \(_{,}^{}\) output by Algorithm 1 satisfies \(\|V_{}^{}-V_{}^{_{,}^{}} \|_{}\)._Since we observe \(n\) samples for each state-action pair, Theorem 1 shows that a total number of \(^{2}}\) samples suffices to learn an \(\)-optimal policy. This bound improves on the \(^{2}}\) complexity bound from  when the span \(\) is no larger than the effective horizon \(\). This assumption holds in many situations, as can be seen by using the relationships \( D\) or \( 8_{}\). On the other hand, in the regime with \(>\), the existing bound \(^{2}}\), also achieved by Algorithm 1, is superior. In this regime, the discounting effectively truncates the MDP at a short horizon \(\) before the long-run behavior of the optimal policy (as captured by \(\)) kicks in.

Proof highlights for Theorem 1.: The key to obtaining this improved complexity is a careful analysis of certain instance-specific variance parameters. It suffices to bound \(\|_{,}^{_{}^{}}-V_{}^{ _{}^{}}\|_{}\) and \(\|_{,}^{_{,}^ {}}-V_{}^{_{,}^{}}\|_{}\) by \(O()\). The prior DMDDP complexity of \(^{2}}\) is obtained using the well-known law-of-total-variance argument , which ultimately yields a sample complexity like to bound \(\|_{,}^{_{}^{}}-V_{}^{ _{}^{}}\|_{} O()\). From here, the variance of the cumulative discounted reward \(\|^{_{}^{}}[_{t=0}^{}^{t}R_ {t}]\|_{}\) is bounded by \(}\), since the total reward in a trajectory is within \([0,]\). We instead seek to bound \(\|^{_{}^{}}[_{t=0}^{}^{t}R_ {t}]\|_{} O(}{1-})\). Assume \(\) is an integer. The first step is to decompose \(^{_{}^{}}[_{t=0}^{}^{t}R_{t}]\) recursively like

\[^{_{}^{}}[_{t=0}^{}^{t}R_{t} ]=^{_{}^{}}[_{t=0}^{-1} ^{t}R_{t}+^{}V_{}^{_{}^{}}(S_{ })]+^{2}(P_{_{}^{}})^ {}^{_{}^{}}[_{t=0}^{}^ {t}R_{t}]\]

(see our Lemma 13). This is a multi-step version of the standard variance Bellman equation (e.g., [16, Theorem 1]). Ordinarily an \(\)-step expansion would not be useful, since the term \(V_{}^{_{}^{}}(S_{})\) by itself appears to have fluctuations on the order of \(\) in the worst case depending on \(S_{}\) (note \(S_{}\) is the random state encountered at time \(\)). However, in our setting, we should have \(V_{}^{_{}^{}}(S_{}) ^{}+h^{}(S_{})\), reducing the magnitude of the random fluctuations to order \(=\|h^{}\|_{}\). (See Lemma 11 for a formalization of this approximation which first appeared in .) Therefore expansion to \(\) steps achieves the optimal tradeoff between maintaining \(^{_{}^{}}_{t=0}^{-1}^{t}R_ {t}+^{}V_{}^{_{}^{}}(S_{})  O(^{2})\) and minimizing \(^{2}\). As desired this yields \(\|^{_{}^{}}[_{t=0}^{}^{t}R _{t}]\|_{} O^{2}}{1-^{2 }}=O}{1-}\), where \(}} O(1-)} \) requires \(\). See Lemma 15 for the complete argument.

We would like to use a similar argument as above to bound the second term \(\|_{,}^{_{,}^ {}}-V_{}^{_{,}^{}}\|_{}\), which is the "evaluation error" of the _empirically_ optimal policy \(_{,}^{}\). However, applying the same argument would give a bound in terms of \(\|V_{}^{_{,}^{}}\|_{}\), which, unlike for the analogous term involving the _true_ optimal policy \(_{}^{}\), is not a priori bounded in terms of \(H\). (If we instead assumed uniform mixing, we could immediately bound this by \(O(_{})\).) Thus, to control the variance associated with evaluating \(_{,}^{}\), we are able to recursively bound \(\|V_{}^{_{,}^{}}\|_{} OH+\|_{,}^{_{ ,}^{}}-V_{}^{_{,}^{ }}\|_{}\), which can be shown to yield the desired sample complexity.

Now we present our main result for the average-reward problem in the weakly communicating setting. Applied in this setting with a DMDP target accuracy of \(=\), our Algorithm 2 reduces the problem to \(\)-discounted MDP with \(=1-}\) and then calls Algorithm 1 with target accuracy \(\).

```
0: Sample size per state-action pair \(n\), target accuracy \((0,1]\), DMDP target accuracy \(\)
1: Set \(=1-\)
2: Obtain \(^{}\) from Algorithm 1 with sample size per state-action pair \(n\), accuracy \(\), discount \(\)
3:return\(^{}\) ```

**Algorithm 2** Average-to-Discount Reduction

We have the following sample complexity bound for Algorithm 2.

**Theorem 2** (Sample Complexity of Weakly Communicating AMDP).: _Suppose the MDP \((P,r)\) is weakly communicating. There exists a constant \(C_{1}>0\) such that for any \(,(0,1)\), if \(n C_{1}}{^{2}}}{ }\) and we call Algorithm 2 with \(=\), then with probability at least \(1-\), the output policy \(^{}\) satisfies the elementwise inequality \(^{}-^{^{}}\)._

Again, since we observe \(n\) samples for each state-action pair, this result shows that \((SA}{^{2}})\) total samples suffice to learn an \(\)-optimal policy for the average reward MDP. This bound matches the minimax lower bound in  and is superior to existing results for weakly communicating MDPs (see Table 1). We note that the proof of Theorem 1 works so long as \(\) is any upper bound of \(\|h^{}\|_{}\), hence Algorithm 2 also only needs an upper bound for \(\|h^{}\|_{}\).

We show in the following theorem that it is in general impossible to obtain a useful upper bound on \(\|h^{}\|_{}\) with a sample complexity that is a function of only \(\|h^{}\|_{}\). This suggests that it is not easy to remove the need for knowledge of \(\|h^{}\|_{}\).

**Theorem 3**.: _For any given \(n,T 1\), there exist two MDPs \(_{0}\) and \(_{1}\) with \(S=4\), \(A=1\) such that \(_{0}\) has optimal bias span \(1\), \(_{1}\) has optimal bias span \(T\), and it is impossible to distinguish between \(_{0}\) and \(_{1}\) with probability \(\) with \(n\) samples from each state-action pair._

Thus even for an MDP with a small span, there exists another MDP that has an arbitrarily large span and is arbitrarily statistically close (that is, cannot be distinguished even with a large sample size \(n\)). We emphasize that all previous algorithms in Table 1 also require knowledge of their respective complexity parameters, and such assumptions are pervasive throughout the literature on average-reward RL. The only exception of which we are aware is the contemporaneous work , which achieves a suboptimal \((SA_{}}{ })\) sample complexity without knowledge of \(_{}\) in the uniformly mixing setting. It is unclear if \(\)-based sample complexities are possible without knowing \(\). Besides the evidence offered by Theorem 3, in the online setting, it has been conjectured that knowledge of \(\) is necessary to obtain an \(\)-dependent regret bound [6; 5; 25]. Moreover, even with knowledge of \(\), the only known online algorithm with optimal regret is computationally inefficient , making it somewhat surprising that our Theorem 2 uses a simple and efficient algorithm.

Nevertheless, when \(\) is unknown, one can replace \(\) with the diameter \(D\) (since \( D\)). The diameter is known to be estimable [25; 17] and is often a more refined complexity parameter than \(_{}\). Our Theorem 2 is the first to imply the optimal diameter-based complexity \((})\), given knowledge of \(D\) or using a constant-factor upper bound obtained from some estimation procedure.

## 4 Main results for general MDPs

Our starting point for general MDPs is that unlike the weakly communicating setting, their complexity _cannot_ be captured solely by \(\|h^{}\|_{}\). We first argue this point informally using the simple example in Figure 1, which is parameterized by a value \(T>1\). Only state \(1\) contains multiple actions, and action \(2\) is optimal since it leads to state \(2\) which collects reward \(0.5\) forever, while taking action \(1\) will always eventually lead to state \(3\) where the reward is \(0\) forever. We thus have \(^{}=[0.5,0.5,0]^{}\) and \(\|h^{}\|_{}=0\). However, clearly \((T)\) samples are required to even observe a transition \(1 3\), so the sample complexity must depend on \(T\) (without observing a transition \(1 3\), we cannot determine that action \(1\) is not optimal). Taking action \(1\) leads to a large reward of \(1\) in the short term (for \(T\) steps in expectation), so even if we had perfect knowledge of the environment, the optimal \(\)-discounted policy would not choose the optimal action \(a=2\) until the effective horizon \((T)\). Thus \(\) is insufficient for the reduction to discounted MDP. Note that this instance has its bounded transient time parameter \(=T\). This example reflects that transient states play a categorically different role in general MDPs: in the weakly communicating setting, states which are transient under all policies can be completely ignored, whereas in this example our action at state \(1\) fully determines our reward even though state \(1\) is transient under all policies.

The statistical hardness is formally captured by the following theorem, which uses improved instances to obtain the correct dependence on \(\).

**Theorem 4** (Lower Bound for General AMDPs).: _For any \((0,1/4)\), \(B 1\), \(A 4\) and \(S\), for any algorithm Alg which is guaranteed to return an \(/3\)-optimal policy for any input average-reward MDP with probability at least \(\), there exists an MDP \(=(P,r)\) such that:_

1. \(\) _has_ \(S\) _states and_ \(A\) _actions._
2. _Letting_ \(h^{}\) _be the bias of the Blackwell-optimal policy for_ \(\)_, we have_ \( h^{}_{}=0\)_._
3. \(\) _satisfies the bounded transient time assumption with parameter_ \(B\)_._
4. Alg _requires_ \(}\) _samples per state-action pair on_ \(\)_._

A similar minimax lower bound holds for the discounted setting.

**Theorem 5** (Lower Bound for General DMDP).: _For any \((0,1/4)\), \(B 1\), \(A 4\) and \(S\) for any algorithm Alg which is guaranteed to return an \(/3\)-optimal policy for any input discounted MDP with probability at least \(\), there exists a discounted MDP \(=(P,r,)\) such that:_

1. \(\) _has_ \(S\) _states and_ \(A\) _actions._
2. \(\) _satisfies the bounded transient time assumption with parameter_ \(B\)_._
3. Alg _requires_ \(^{2}}\) _samples per state-action pair on_ \(\)_._

The lower bounds of \((}{^{2}})\) from the weakly communicating setting still apply in the general setting. Together with Theorem 4 they imply a \((}{^{2}})\) lower bound for general average-reward MDPs.

Figure 1 demonstrates that, unlike the weakly communicating setting, discounted reduction with \(\) set in terms of only \(\) cannot succeed for general MDPs. (Contrast with Lemma 9 for the analogous theorem from  for weakly communicating MDPs.) We remedy this issue and lay the foundation for our matching upper bound by proving a new reduction theorem in terms of \(\)_and_\(\); in particular, \(\) measures how much further ahead we must look in order to determine which closed communicating class will be reached. By Lemma 27\( 4_{}\), although \(\) is always finite unlike \(_{}\).

**Theorem 6** (Average-to-Discount Reduction for General MDP).: _Suppose \((P,r)\) is a general MDP, has an optimal bias function \(h^{}\) satisfying \( h^{}_{}\), and satisfies the bounded transient time assumption with parameter \(\). Fix \((0,1]\) and set \(=1-}\). For any \(_{}[0,]\), if \(\) is any \(_{}\)-optimal policy for the discounted MDP \((P,r,)\), then \(^{}-^{}3+2}{}\)._

Proof highlights.: Letting \(_{}^{}\) be the optimal policy for the \(\)-discounted MDP, our first key observation is that \(^{}\) is constant within any irreducible closed recurrent block of the Markov chain \(P_{_{}^{}}\), essentially because all states in this block must be reachable from each other with probability one (see Lemma 17). Leveraging the optimality of \(^{}_{}\), this enables us to bound both \(|V^{^{}_{}}_{}(s)-^{}(s)|\) and \(|V^{^{}_{}}_{}(s)-^{^{}_ {}}(s)|\) by \(O(\|h^{}\|_{})\) for any \(s\) which is recurrent under \(^{}_{}\), which when combined demonstrate that the gain \(^{^{}_{}}(s)\) of \(^{}_{}\) is near-optimal for its recurrent states. See Lemma 21. We then leverage the bounded transient time assumption to guarantee that for transient \(s\), \(V^{^{}_{}}_{}(s)\) is dominated by the expected returns from recurrent states, since at most \(O()\) time is spent in transient states. We complete the proof of Theorem 6 by combining these facts, as well as extending them to accommodate approximately optimal policies. 

Next we establish an improved sample complexity for the discounted problem in the setting relevant to this reduction. This bound matches the lower bound in Theorem 5 up to log factors.

**Theorem 7** (Sample Complexity of General DMDP).: _Suppose \(+\) and \(+\). There exists a constant \(C_{3}>0\) such that, for any \((0,1)\), if \(n C_{3}+}{(1-)^{2}^{2}} ()\), then with probability \(1-\), the policy \(^{}_{,}\) output by Algorithm 1 satisfies \(\|V^{}_{}-V^{^{}_{,}}_{ }\|_{}\)._

Finally, we present our result for the sample complexity of general average-reward MDPs, matching the lower bound in Theorem 4 up to log factors. We again use the reduction Algorithm 2, this time with the larger DMDP target accuracy \(=+\), leading to a discount factor of \(=1-+)}\).

**Theorem 8** (Sample Complexity of General AMDP).: _There exists a constant \(C_{4}>0\) such that for any \(,(0,1)\), if \(n C_{4}+}{^{2}}(+)}{})\) and we call Algorithm 2 with \(=+\), then with probability at least \(1-\), the output policy \(^{}\) satisfies the elementwise inequality \(^{}-^{^{}}\)._

Proof highlights.: Similarly to Theorem 2, we seek to bound certain variance parameters, and this time it would suffice to bound the variance of the cumulative discounted reward starting from any state \(s\) like \(|_{s}^{^{}_{}}[_{t=0}^{}^{t}R_ {t}]| O+}{1-}\). Such a bound indeed holds for states \(s\) that are recurrent under \(^{}_{}\), because \(^{}(S_{t})\) will remain constant to \(^{}(s)\) for all \(t\), since, as mentioned above, \(^{}\) is constant on closed irreducible recurrent blocks, and \(\|S_{t}\|_{t 0}\) will stay in the same block as \(s\). Therefore, we can almost reuse our argument from the weakly communicating case. However, if \(s\) is transient, it is easy to see that \(|_{s}^{^{}_{}}[_{t=0}^{}^{t}R_ {t}]|=^{2}\) in general (even under the bounded transient time assumption), as we can consider an example where from \(s\) we transition to either an absorbing reward \(1\) state or an absorbing reward \(0\) state. Thus, when \(s\) is transient, instead of bounding \(|_{s}^{^{}_{}}[_{t=0}^{}^{t}R_ {t}]|\), we directly work with the sharper variance parameter \(|e_{s}^{}(I- P_{^{}_{}})^{-1}_{P_{^{}_{}}}[V_{}^{^{}_{}} ]}|\), which is also common to the analysis of DMDPs [3; 1; 12] (and in these previous works is bounded in terms of \(\|^{^{}_{}}[_{t=0}^{}^{t}R_ {t}]\|_{}\); see Lemma 12 for this relationship). We instead develop a novel law-of-total-variance-style argument which limits the total contribution of transient states to this sharper variance parameter. See Lemma 26 for details. 

## 5 Conclusion

In this paper we obtained optimal sample complexities for weakly communicating and general average reward MDPs by improving the analysis of discounted MDPs, revealing a quadratic rather than cubic dependence on the effective horizon for a fixed instance. A limitation of our results (as well as of all previous results) is that the average-to-discounted reduction requires prior knowledge of parameters for optimal complexity, and an interesting open question is whether it is possible to remove this assumption. In conclusion, we believe our results shed greater light on the relationship between the discounted and average reward settings as well as the fundamental complexity of the discounted setting, and we hope that our technical developments can be useful in future work, such as leading to efficient optimal algorithms in the online setting.