# Efficient Centroid-Linkage Clustering

MohammadHossein Bateni

Google Research

New York, USA

&Laxman Dhulipala

University of Maryland

College Park, USA

&Willem Fletcher

Brown University

Providence, USA

&Kishen N. Gowda

University of Maryland

College Park, USA

&D Ellis Hershkowitz

Brown University

Providence, USA

&Rajesh Jayaram

Google Research

New York, USA

&Jakub Lacki

Google Research

New York, USA

###### Abstract

We give an algorithm for Centroid-Linkage Hierarchical Agglomerative Clustering (HAC), which computes a \(c\)-approximate clustering in roughly \(n^{1+O(1/c^{2})}\) time. We obtain our result by combining a new Centroid-Linkage HAC algorithm with a novel fully dynamic data structure for nearest neighbor search which works under adaptive updates.

We also evaluate our algorithm empirically. By leveraging a state-of-the-art nearest-neighbor search library, we obtain a fast and accurate Centroid-Linkage HAC algorithm. Compared to an existing state-of-the-art exact baseline, our implementation maintains the clustering quality while delivering up to a \(36\) speedup due to performing fewer distance comparisons.

## 1 Introduction

Hierarchical Agglomerative Clustering (HAC) is a widely-used clustering method, which is available in many standard data science libraries such as SciPy , scikit-learn , fastcluster , Julia , R , MATLAB , Mathematica  and many more . HAC takes as input a collection of \(n\) points in \(^{d}\). Initially, it puts each point in a separate cluster, and then proceeds in up to \(n-1\) steps. In each step, it merges the two "closest" clusters by replacing them with their union.

The formal notion of closeness is given by a _linkage function_; choosing different linkage functions gives different variants of HAC. In this paper, we study _Centroid-Linkage HAC_ wherein the distance between two clusters is simply the distance between their centroids. This method is available in most of the aforementioned libraries. Other common choices include _single-linkage_ (the distance between two clusters is the _minimum_ distance between a point in one cluster and a point in the other cluster) or _average-linkage_ (the distance between two clusters is the _average_ pairwise distance between them).

HAC's applicability has been hindered by its limited scalability. Specifically, running HAC with any popular linkage function requires essentially quadratic time under standard complexity-theory assumptions. This is because (using most linkage functions) the first step of HAC is equivalent to finding the closest pair among the input points. In high-dimensional Euclidean spaces, this problem was shown to (conditionally) require \((n^{2-})\) time for any constant \(>0\). In contrast, solving HAC in essentially \((n^{2})\) time is easy for many popular linkage functions (including centroid, single, average, complete, Ward), as it suffices to store all-pairs distances between the clusters in a priority queue and update them after merging each pair of clusters.

In this paper, we show how to bypass this hardness for Centroid-Linkage HAC by allowing for approximation. Namely, we give a subquadratic time algorithm which, instead of requiring that the two closest clusters be merged in each step, allows for any two clusters to be merged if their distance is within a factor of \(c 1\) of that of the two closest clusters.

**Contribution 1: Meta-Algorithm for Centroid-Linkage.** Our first contribution is a simple meta-algorithm for approximate Centroid-Linkage HAC. The algorithm assumes access to a dynamic data structure for approximate nearest-neighbor search (ANNS). A dynamic ANNS data structure maintains a collection of points \(S^{d}\) subject to insertions and deletions and given any query point \(u S\) returns an approximate nearest neighbor \(v S\). Formally, for a \(c\)-approximate NNS data structure, \(v\) satisfies \(D(u,v) c_{x S\{u\}}D(u,x)\).1 Here, \(D(,)\) denotes the Euclidean distance. By using different ANNS data structures we can obtain different Centroid-Linkage HAC algorithms (which is why we call our method a _meta_-algorithm). We make use of an ANNS data structure where the set of points \(S\) is the centroids of the current HAC clusters. Roughly, our algorithm runs in time \((n)\) times the time it takes an ANNS data structure to update or query.

While finding distances is the core part of running HAC, we note that access to an ANNS data structure for the centroids does not immediately solve HAC. Specifically, an ANNS data structure can efficiently find an (approximate) nearest neighbor of a single point, but running HAC requires us to find an (approximately) closest pair among all _clusters_. Furthermore, HAC must use the data structure in a dynamic setting, and so an update to the set of points \(S\) (caused by two clusters merging) may result in many points changing their nearest neighbors.

Our meta-algorithm requires that the dynamic ANNS data structure works under _adaptive updates_.2 Namely, it has to be capable of handling updates, which are dependent on the prior answers that it returned (as opposed to an ANNS data structure which only handles "oblivious" updates).

To illustrate this, consider a randomized ANNS data structure \(\). Clearly, a query to \(\) often has multiple correct answers, as \(\) can return _any_ near neighbor within the promised approximation bound. As a result, an answer to a query issued to \(\) is dependent on the internal randomness of \(\). Let us assume that \(\) is used within a centroid-linkage HAC algorithm \(\) and upon some query returns a result \(u\). Then, algorithm \(\) uses \(u\) to decide which two clusters to merge, and the centroid \(p\) of the newly constructed cluster is inserted into \(\). Since \(p\) depends on \(u\), which in turn is a function of the internal randomness of \(\), we have that the point that is inserted into \(\) is dependent on the internal randomness of the data structure. Hence, it is not sufficient for \(\) to return a correct answer for each _fixed_ query with high probability (meaning at least \(1-1/(n)\)). Instead, \(\) must be able to handle queries and updates dependent on its internal randomness. We note that a similar issue is prevalent in many cases when a randomized data structure is used as a building block of an algorithm. As a result, the subtle notion of adaptive updates is a major area of study .

**Contribution 2: Dynamic ANNS Robust to Adaptive Updates.** Our second contribution is a dynamic ANNS data structure for high-dimensional Euclidean spaces which, to the best of our knowledge, is the first one to work under _adaptive updates_.

To obtain our ANNS data structure, we show a black-box reduction from an arbitrary randomized dynamic ANNS data structure to one that works against adaptive updates (see Theorem 2). The reduction increases the query and update times by only a \(O( n)\) factor. We apply this reduction to a (previously known) dynamic ANNS data structure which is based on locality-sensitive hashing  and requires non-adaptive updates.

By combining our dynamic ANNS data structure for adaptive updates with our meta-algorithm, we obtain an \(O(c)\)-approximate Centroid-Linkage HAC algorithm which runs in time roughly \(n^{1+1/c^{2}}\).

Furthermore, our data structure for dynamic ANNS will likely have applications beyond our HAC algorithm. Specifically, ANNS is commonly used as a subroutine to speed up algorithms for geometric problems. Oftentimes, and similarly to HAC, formal correctness of these algorithms require an ANNS data structure that works for adaptive updates but prior work often overlooks this issue. Our new data structure can be used to fix the analysis of such works. For instance, an earlier work on subquadratic HAC algorithms for average- and Ward-linkage  overlooked this issue and our new data structure fixes the analysis of this work.3 Similarly, a key bottleneck in the dynamic \(k\)-centers algorithm in  was that it had to run nearest neighbor search over the entire dataset (instead of just the \(k\)-centers). By replacing the ANNS data structure used therein with our ANNS algorithm, we believe that that the \(n^{}\) running times from that paper could be improved to \(k^{}\).

Contribution 3: Empirical Evaluation of Centroid-Linkage HAC.Finally, we use our Centroid-Linkage meta algorithm to obtain an efficient HAC implementation. We use a state-of-the-art static library for ANNS that we extend to support efficient dynamic updates when merging centroids.

We empirically evaluate our algorithm and find that our approximate algorithm achieves strong fidelity with respect to the exact centroid algorithm, obtaining ARI and NMI scores that are within 7% of that of the exact algorithm, while achieving up to 36\(\) speedup over the exact implementation, even when both implementations are run sequentially. Our implementation can be found at https://github.com/kishen19/CentroidHAC.

We note that the ANNS we use in the empirical evaluation of our meta-algorithm is different from our new dynamic ANNS (Contribution 2). This is because modern practical methods for ANNS (e.g., based on graph-based indices ) have far surpassed the efficiency of data structures for which rigorous theoretical bounds are known. Closing this gap is a major open problem.

### Related Work

Improving the efficiency of HAC has been an actively researched problem for over 40 years . A major challenge common to multiple linkage functions has been to improve the running time beyond \((n^{2})\), which is the time it takes to compute all-pairs distances between the input points. For the case of low-dimensional Euclidean spaces, a running time of \(o(n^{2})\) is possible for the case of squared Euclidean distance and Ward linkage, since the ANNS problem becomes much easier in this case . However, breaking the \((n^{2})\) barrier is (conditionally) impossible in the case of high-dimensional Euclidean spaces (without an exponential dependence on the dimension) .

To bypass this hardness result, a recent line of work focused on obtaining _approximate_ HAC algorithms . Most importantly, Abboud, Cohen-Addad and Houdrouge  showed an approximate HAC algorithm which runs in subquadratic time for Ward and average linkage. These algorithms rely on the fact in the case of these linkage functions the minimum distance between two clusters can only increase as the algorithm progresses, which is not the case for centroid linkage; e.g. consider 3 equi-distant points as in Figure 1.

We also note that, as mentioned earlier, our dynamic ANNS data structure fixes a gap in the analysis of both algorithms in the paper. In fact, not only can the minimum distances shrink when performing Centroid-Linkage HAC, but when doing \(O(1)\)-approximate Centroid-Linkage HAC distances can get arbitrarily small; see Figure 2. This presents an additional issue for dynamic ANNS data structures which typically assume lower bounds on minimum distances.

Compared to the algorithm of Abboud, Cohen-Addad and Houdrouge, our centroid-linkage HAC algorithm introduces two new ideas. First, we show how to handle the case when the minimum

Figure 1: 3 points in \(^{2}\) initially all at distance \(1\) showing Centroid-Linkage HAC merge distances can decrease. 1 and 1 give the the first and second merges with distances \(1\) and \(/2<1\) respectively.

distance between two clusters decreases as a result of two cluster merging. Second, we introduce an optimization that allows us not to consider each cluster at each of the (logarithmically many) distance scales. This optimization improves the running time bound by a logarithmic factor when each merge "makes stale" a small number of stored pairs and in practice the number of such stale merges occur only \(60\%\) of the time (on average) compared to the number of actual merges performed.

Another line of work considered a different variant of HAC, where the input is a weighted similarity graph [15; 16; 17; 6]. The edge weights specify similarities between input points, and a lack of edge corresponds to a similarity value of \(0\). By assuming that the graph is sparse, this representation allows bypassing the hardness of finding distances, which leads to near-linear-time approximate algorithms for average-linkage HAC , as well as to efficient parallel algorithms [16; 6].

In terms of the theoretical and empirical part of this paper considering different algorithms, such a gap has also been observed for various other prominent clustering algorithms. For instance, for correlation clustering, while the best known algorithms are obtained by solving a linear program (see ), in practice a simple local search/Louvain-based algorithm is used (see section 4.1 of ). For modularity clustering, the best known approximation is obtained by solving an SDP . Again, Louvain-based algorithms are used in practice. In the case of k-means, while the best known approximation is a constant obtained via the primal-dual method , in practice Lloyd's heuristic paired with k-means++ seeding is used which has a logarithmic approximation guarantee. For balanced graph partitioning, coarsening combined with a brute-force algorithm is used to obtain the best results in practice. On the other hand, the algorithms for solving the corresponding theoretical formulations, i.e. balanced cut and multiway cut, are entirely different.

## 2 Preliminaries

In this section, we review our conventions and preliminaries. Throughout this paper, we will work in \(d\)-dimensional Euclidean space \(^{d}\). We will use \(D(,)\) for the Euclidean metric in \(^{d}\); that is, for \(x,y^{d}\), the distance between \(x\) and \(y\) is \(D(x,y):=(x[i]-y[i])^{2}}\) where \(x[i]\) and \(y[i]\)are the \(i\)th coordinates of \(x\) and \(y\) respectively. Likewise, we let \(D(x,Y):=_{y Y}D(x,y)\) for \(Y^{d}\).

### Formal Description of Centroid-Linkage HAC

While we have described centroid-linkage in a cluster-centric way (see Appendix A for a formal cluster-centric definition), it will be more useful to use an equivalent centroid-centric definition.

Specifically, we can equivalently define \(c\)-approximate Centroid-Linkage HAC as repeatedly "merging centroids". Initialize the set of all centroids \(C=P\) and a weight \(w_{u}=1\) for each \(u C\); these weights will encode the cluster sizes of each centroid. Then, until \(|C|=1\) we do the following: let \(, C\) be a pair of centroids satisfying

\[D(,) c_{x,y C}D(x,y)\]

where the \(\) is taken over distinct pairs; merge \(\) and \(\) into their weighted midpoints by removing them from \(C\), adding \(z=(w_{}+w_{})/(w_{}+w_{})\) to \(C\) and setting \(w_{z}=w_{}+w_{}\). Also, note that (exact) Centroid-Linkage HAC is just the above with \(c=1\).

### Dynamic Nearest-Neighbor Search

We define a dynamic ANNS data structure. Typically, dynamic ANNS requires a lower bound on distances; we cannot guarantee this for centroid HAC. Thus, we make use of \(\) additive error below.

Figure 2: 3 points in \(^{2}\) (two of which are initially at distance \(1\)) showing that \(O(1)\)-approximate Centroid-Linkage HAC can arbitrarily reduce merge distances. 2a / 2b and 2c / 2d give the the first and second merges with distances \(1\) and \( 1\) respectively; centroids are dashed circles.

**Definition 1** (Dynamic Approximate Nearest-Neighbor Search (ANNS)).: _An \((,)\)-approximate dynamic nearest-neighbor search data structure \(\) maintains a dynamically updated set \(S^{d}\) (initially \(S=\)) and supports the following operations._

1. _Insert__,_ \(\)_-_\((u)\)_. given_ \(u S\) _update_ \(S S\{u\}\)_._
2. _Delete__,_ \(\)_-_\((u)\)_: given_ \(u S\) _update_ \(S S\{u\}\)_;_
3. _Query__,_ \(\)_-_\((u,)\)_: given_ \(u,^{d}\) _return_ \(v S\{\}\)_s.t._

\[D(u,v) D(u,S\{\})+.\]

If \(\) is a dynamic \((,0)\)-approximate NNS data structure then we will simply call it \(\)-approximate. For a set of points \(S\), we will use the notation \((S)\) to denote the result of starting with an empty dynamic ANNS data structure and inserting each point in \(S\) (in an arbitrary order).

We say \(\)_succeeds for a set of points \(S^{d}\) and a query point \(u\)_ if after starting with an empty set and any sequence of insertions and deletions of points in \(S\) we have that the query \(\)_-_\((u,)\) is correct for any \(\) (i.e., the returned point satisfies the stated condition in 3). If the data structure is randomized then we say that it _succeeds for adaptive updates_ if with high probability (over the randomness of the data structure) it succeeds for all possible subsets of points and queries. Queries have _true distances_ at most \(\) if the (true) distance from \(S\) to any query is always at most \(\).

The starting point for our dynamic ANNS data structure is the following data structure requiring _non-adaptive_ updates which alone does not suffice for centroid HAC as earlier described. See Appendix B for a proof.

**Theorem 1** (Dynamic ANNS for Oblivious Updates, ).: _Suppose we are given \(>1\) and \(c,,\) and \(n\) where \((/),(n)\) and a dynamically updated set \(S\) with at most \(n\) insertions. Then, if all queries have true distance at most \(\), we can compute a randomized \((O(c),)\)-approximate NNS data structure with update, deletion and query times of \(n^{1/c^{2}+o(1)}(/) d\), which for a fixed set of points and query point succeeds except with probability at most \((-)\)._

## 3 Dynamic ANNS with Adaptive Updates

The main theorem we show in this section is how to construct a dynamic ANNS data structure that succeeds for adaptive updates using one which succeeds for oblivious updates.

**Theorem 2** (Reduction of Dynamic ANNS from Oblivious to Adaptive).: _Suppose we are given \(c\), \(\), \(\), \(n\), \(s^{d}\) and dynamically updated set \(S^{d}\) with at most \(n\) insertions, such that all inserted points and query points lie in \(B_{s}()\). Moreover, assume that we can compute a dynamic \((c,)\)-approximate NNS data structure with query, deletion and insertion times \(T_{Q}\), \(T_{D}\) and \(T_{I}\) which succeeds for \(S\) and a fixed query except with probability at most \(n^{-O(d(/))}\)._

_Then, we can compute a randomized dynamic \((c,c)\)-approximate NNS data structure that succeeds for adaptive updates with query, deletion and amortized insertion times \(O( n) T_{Q}\), \(O(T_{D})\) and \(O( n) T_{I}\)._

We note that our result is slightly stronger than the above: the insertions we make into the oblivious ANNS that we use only occur upon their instantiation, not dynamically.

As a corollary of the above reduction and known constructions for dynamic ANNS that work for oblivious updates--namely, Theorem 1 with \(=(d d(/) n)\) and using parameter \(^{}=/c\) where \(^{}\) is the additive distortion parameter for Theorem 1--we obtain the following.

**Theorem 3** (Dynamic ANNS for Adaptive Updates).: _Suppose we are given \(c\), \(\), \(\), \(n\), \(s^{d}\) where \((/),d,c(n)\) and dynamically updated set \(S^{d}\) with at most \(n\) insertions, such that all inserted and query points lie in \(B_{s}()\)._

_Then, we can compute a randomized dynamic \((O(c),)\)-approximate NNS data structure that succeeds for adaptive updates with query, deletion and amortized insertion time \(n^{1/c^{2}+o(1)}(/) d^{2}\)._

A previous work  also provided algorithms that work against an "adaptive updates" but only a set of adaptive updates made against a fixed set of query points. Also, note that if we have a lower bound of \(\) on the true distance of any query then lowering \(c\) by a constant and setting \(=()\) for a suitably small hidden constant gives an \(O(c)\)-approximate NNS with similar guarantees.

### Algorithm Description

Our algorithm for dynamic ANNS for adaptive updates uses two ingredients. First, we make use of the following "covering nets" to fix our queries to a small set against which we can union bound.

**Lemma 1**.: _Given \(s^{d}\) and \(,>0\), there exists a covering net \(Q^{d}\) such that_

1. _Small Size:_ \(|Q|(/)^{O(d d)}\)__
2. _Queries:_ _given_ \(u B_{s}()\)_, one can compute_ \(u^{} Q\) _in time_ \(O(d)\) _such that_ \(D(u,u^{})\)_._

Second, we make use of a "merge-and-reduce" approach. A similar approach was taken by  for exact \(k\)-nearest-neighbor queries in the plane. Namely, for each \(i[O( n)]\) we maintain a set \(S_{i}\) of size at most \(2^{i}\) (where all \(S_{i}\) partition all inserted points) and a dynamic ANNS data structure \(_{i}\) for \(S_{i}\) which only works for oblivious updates. Other than the size constraint, the partition is arbitrary. Informally, we perform deletions, insertions and queries as follows.

**Deletion:** To delete a point we simply delete it from its corresponding \(_{i}\).

**Insertion:** To insert a point, we insert it into \(S_{0}\) and for each \(i\) we move all points from \(S_{i}\) to \(S_{i+1}\) if \(S_{i}\) contains more than \(2^{i}\) points; we update \(_{i}\) accordingly each time we move points; namely, we recompute \(_{i}\) and \(_{i+1}\) from scratch on \(S_{i}\) and \(S_{i+1}\) respectively. See Figure 3.

**Query:** Lastly, to query a point \(u\) we first map this point to a point in our covering net \(u^{}\) (as specified by Lemma 1), query \(u^{}\) in each of our \(_{i}\) and then return the best output (i.e., the point output by an \(_{i}\) that is closest to \(u^{}\)).

Our algorithm is more formally described in pseudo-code in Algorithm 1.

``` Input:\(\), \(\), \(n\), \(s^{d}\), \(Q(s,,)\) (computed using Lemma 1) Maintains:\(S_{0}\), \(S_{1}\),... - partition of the inserted points, s.t. \(|S_{i}| 2^{i}\) Maintains:\(_{0}\), \(_{1}\),... - a non-adaptive ANNS for each \(S_{i}\) functioninsert(\(u\)) \(S_{0} S_{0}\{u\}\) while\( i\) such that \(|S_{i}|>2^{i}\)do \(S_{i+1} S_{i+1} S_{i}\) and \(S_{i}\) \(_{i+1}(S_{i+1})\) and \(_{i}()\) functiondelete(\(u\))  Let \(i\) be such that \(u S_{i}\) \(_{i}\).delete(\(u\)) and remove \(u\) from \(S_{i}\) functionquery(\(u,\))  Let \(u^{} Q\) be such that \(D(u,u^{})=O()\)\(\) computed using Lemma 1  Let \(v_{i}=_{i}\).query(\(u^{},\)) return\(v=_{v_{i}}D(u^{},v_{i})\) ```

**Algorithm 1** Dynamic ANNS for Adaptive Updates

Figure 3: Our merge-and-reduce strategy when a point (in green) is inserted.

Centroid-Linkage HAC Algorithm

In this section, we give our algorithm for Centroid-Linkage HAC. Specifically, we show how to (with Algorithm 3) reduce approximate Centroid-Linkage HAC to roughly linearly-many dynamic ANNS function calls, provided the ANNS works for adaptive updates.

**Theorem 4** (Reduction of Centroid HAC to Dynamic ANN for Adaptive Updates).: _Suppose we are given a set of \(n\) points \(P^{d}\), \(c>1\), \(>0\), and lower and upper bounds on the minimum and maximum pairwise distance in \(P\) of \(\) and \(\) respectively. Suppose we are also given a data structure that is a dynamic \(c\)-approximate NMS data structure for all queries in Algorithm 3 that works for adaptive updates with insertion, deletion and query times of \(T_{I}\), \(T_{D}\), and \(T_{Q}\)._

_Then there exists an algorithm for \(c(1+)\)-approximate Centroid-Linkage HAC on \(n\) points that runs in time \((n(T_{I}+T_{D}+T_{Q}))\) assuming \((n)\) and \((n)\)._

Our final algorithm for centroid HAC is not quite immediate from the above reduction and our dynamic ANNS algorithm for adaptive updates. Specifically, the above reduction requires a \(c\)-approximate NNS data structure but in the preceding section we have only provided a \((c,)\)-approximate NNS data structure for \(>0\). However, by leveraging the structure of centroid HAC--in particular, the fact that there is only ever at most one "very close" pair--we are able to show that this suffices. In particular, for a given \(c\), there exists a \(c_{0}\) and \(_{0}\) such that a \((c_{0},_{0})\)-approximate NNS data structure functions as a \(c\)-approximate NNS data structure for all queries in Algorithm 3. This is formalized by Lemma 6. Combining Lemma 6, Theorem 4 and Theorem 3, we get the following.

**Theorem 5** (Centroid HAC Algorithm).: _For \(n\) points in \(^{d}\), there exists a randomized algorithm that given any \(c>1.01\) and lower and upper bounds on the minimum and maximum pairwise distance in \(P\) of \(\) and \(\) respectively, computes a \(c\)-approximate Centroid-Linkage HAC with high probability. It has runtime \(O(n^{1+O(1/c^{2})}d^{2})\) assuming \(c,d,(n)\)._

### Algorithm Description

In what remains of this section, we describe the algorithm for Theorem 4. Consider a set of points \(P^{d}\). Suppose we have a dynamic \(c\)-approximate NNS data structure \(\) that works with adaptive updates with query time \(T_{Q}\), insertion time \(T_{I}\), and deletion time \(T_{D}\). Let \(Q\) be a priority queue storing elements of the form \((l,x,y)\) where \(x\) and \(y\) are "nearby" centroids and \(l=D(x,y)\). The priority queue supports queuing elements and dequeueing the element with shortest distance, \(l\). At any given time while running the algorithm, say a centroid \(x\) is queued if there is an element of the form \((l,x,y)\) in \(Q\). We will maintain a set, \(C\), of active centroids allowing us to check if a centroid is active in constant time. For each active centroid, \(C\) will also store the weight of the centroid so we can preform merges and an identifier to distinguish between distinct clusters with the same centroid. Note that any time we store a centroid, including in \(Q\), we will implicitly store this identifier.

First, we describe how the algorithm handles merges. To merge two active centroids, \(x\) and \(y\), we remove them from \(C\) and delete them in \(\). Let \(z=w_{x}x+w_{y}y\) be the centroid formed by merging \(x\) and \(y\). We use \(\) to find an approximate nearest neighbor, \(y^{*}\), of \(z\) and add to \(Q\) the tuple \((D(z,y^{*}),z,y^{*})\). Lastly, we add \(z\) to \(\) and \(C\). Pseudo-code for this algorithm is given by Algorithm 2. Crucially, we do not try to update any nearest neighbors of any other centroid at this stage. We will do this work later and only if necessary. Since we are using an approximate NNS, it is possible that the centroid \(z\) will be the same point in \(^{d}\) as the centroid of another cluster. We can detect this in constant time using \(C\) and we will immediately merge the identical centroids.

We now describe the full algorithm using the above merge algorithm; we also give pseudo-code in Algorithm 3. Let \(>0\) be a parameter that tells the algorithm how aggressively to merge centroids. To begin, we construct \(\) by inserting all points of \(P\) in any order. Then for each \(p P\), we use \(\) to find an approximate nearest neighbor \(y P\{p\}\) and queue \((D(p,y),p,y)\) to \(Q\). We also initialize \(C=\{p:p P\}\). The rest of the algorithm is a while loop that runs until \(Q\) is empty. Each iteration of the while loop begins by dequeuing from \(Q\) the tuple \((l,x,y)\) with minimum \(l\).

1. If \(x\) and \(y\) are both active centroids then we merge them.
2. Else if \(x\) is not active then we do nothing and move on to the next iteration of the while loop.
3. Else if \(x\) is active but \(y\) is not, we use \(\) to compute a new approximate nearest neighbor, \(y^{*}\) of \(x\). Let \(l^{*}=D(x,y^{*})\). If \(l^{*}(1+)l\), then we merge \(x\) and \(y^{*}\). Otherwise we add \((l^{*},x,y^{*})\) to \(Q\)```
1:Input: set of active centroids \(C\), ANNS data structure \(\), priority queue \(Q\), centroids \(x\) and \(y\)
2:\(z w_{x}x+w_{y}y\)\(\) Merge \(x\) and \(y\)
3: Remove \(x\) and \(y\) from \(C\) and \(\)
4:if there is a centroid \(z^{*}\) that is the same as \(z\) is in \(C\)then
5:\(z w_{z}z+wz^{*}z^{*}\)\(\) Merge \(z\) and \(z^{*}\)
6: Remove \(z^{*}\) from C and add \(z\)
7:else
8: Add \(z\) to \(\) and \(C\)
9:\(y^{*}\) An approximate nearest neighbor of \(z\) returned by \(.(z,z)\)
10: Queue \((D(z,y^{*}),z,y^{*})\) to \(Q\) ```

**Algorithm 2** Handle-Merge

```
1:Input: set of points \(P\), metric \(D\), fully dynamic ANNS \(\) data structure, \(>0\)
2:Output:\((1+)c\)-approximate centroid HAC
3: Initialize \(Q\) and \(C=\{\{p\}:p P\}\) and insert all points of P into \(\)
4:for\(p P\)do
5:\(y\) An approximate nearest neighbor of \(p\) returned by \(.(p,p)\)
6: Queue \((D(p,y),p,y)\) to \(Q\)
7:while\(Q\) is not empty do
8:\((l,x,y)\) dequeue shortest distance from \(Q\)
9:if\(x,y C\)then
10: Handle-Merge(\(C\), \(\), \(Q\), \(x\), \(y\))
11:elseif\(x C\)then
12:\(y^{*}\) An approximate nearest neighbor of \(x\) returned by \(.(x,x)\)
13:\(l^{*} D(x,y^{*})\)
14:if\(l^{*}(1+)l\)then
15: Handle-Merge(\(C\), \(\), \(Q\), \(x\), \(y^{*}\))
16:else
17: Queue \((l^{*},x,y^{*})\) to \(Q\) ```

**Algorithm 3** Approximate-HAC

## 5 Empirical Evaluation

We empirically evaluate the performance and effectiveness of our approximate Centroid HAC algorithm through a comprehensive set of experiments. Our analysis on various publicly-available benchmark clustering datasets demonstrates that our approximate Centroid HAC algorithm:

1. Consistently produces clusters with quality comparable to that of exact Centroid HAC,
2. Achieves an average speed-up of \(22\) with a max speed-up of \(175\) compared to exact Centroid HAC when using 96 cores in our scaling study. On a single core, it achieves an average speed-up of \(5\) with a max speed-up of \(36\).

Dynamic ANNS Implementation using DiskANNAs demonstrated in previous works, although LSH-based algorithms have strong theoretical guarantees, they tend to perform poorly in practice when compared to graph-based ANNS data structures . For instance, in ParlayANN , the authors find that the recall achievable by LSH-based methods on a standard ANNS benchmark dataset are strictly dominated by state-of-the-art graph-based ANNS data structures.

Therefore, for our experiments, we use DiskANN, a widely-used state-of-the-art graph-based ANNS data structure . In particular, we consider the in-memory version of this algorithm from ParlayANN, called Vamana. In a nutshell, the algorithm builds a bounded degree _routing_ graph that can be searched using beam search. Note that this graph is _not_ the \(k\)-NN graph of the pointset. The graph is constructed using an incremental construction that adds bidirectional edges between a newly inserted point, and points traversed during a beam search for this point; if a point's degree exceeds the degree bound, the point is _pruned_ to ensure a diverse set of neighbors. See  for details.

For Centroid HAC, we require the ANNS implementation to support dynamic updates. The implementation provided by ParlayANN currently only supports fast _static_ index building and queries. Recently, FreshDiskANN  described a way to handle insertions and deletions via a _lazy update_approach. However, their approach requires a periodic _consolidation_ step that scans through the graph and deletes inactive nodes and rebuilds the neighborhood of affected nodes, which is expensive.

Instead, in our implementation of Centroid HAC, we adopt the following new approach that is simple and practical, and adheres to the updates required by our theoretical algorithm: when clusters \(u\) and \(v\) merge, choose one of them to represent the centroid, and update its neighborhood \(N(u)\) (without loss of generality) to the set obtained by pruning the set \(N(u) N(v)\). The intuition here is that \(N(u) N(v)\) is a good representative for the neighborhood of the centroid of \(u\) and \(v\) in the routing graph. Further, during search, points will redirect to their current representative centroid (via union-find ), thus allowing us to avoid updating the in-neighbors of a point in the index. We believe application-driven update algorithms, such as the one described here, can help speed-up algorithms that uses dynamic ANNS as a subroutine.

### Quality Evaluation

We evaluate the clustering quality of our approximate centroid HAC algorithm against ground truth clusterings using standard metrics such as the _Adjusted Rand Index (ARI)_, _Normalized Mutual Information (NMI)_, _Dendrogram Purity_, and the unsupervised _Dasgupta cost_. Our primary objective is to assess the performance of our algorithm across various values of \(\) on a diverse set of benchmarks. We primarily compare our results with those obtained using exact Centroid HAC.

For these experiments, we consider the standard benchmark clustering datasets iris, digits, cancer, wine, and faces from the UCI repository (obtained from the sklearn.datasets package). We also consider the MNIST dataset which contains images of grayscale digits between 0 and 9, and birds, a dataset containing images of 525 species of birds; see Appendix E for more details.

**Results.** The experimental results are presented in Table 1, with a more detailed quality evaluation in Appendix E.1. We summarize our results here.

We observe that the quality of the clustering produced by approximate Centroid HAC is generally comparable to that of the exact Centroid HAC algorithm. On average, the quality is slightly better in some cases, but we attribute this to noise. In particular, we observe that for the value of \(=0.1\), we consistently get comparable quality to that of exact centroid: the ARI and NMI scores are on average within a factor of 7% and 2% to that of exact Centroid HAC, respectively. We also obtained good dendrogram purity score and Dasgupta cost as well, with values within 0.3% and 0.03%, respectively.

### Running Time Evaluation

Next, we evaluate the scalability of approximate Centroid HAC against exact Centroid HAC on large real-world pointsets. We consider the optimized exact Centroid HAC implementation from the fastcluster package . We also implement an efficient version of exact Centroid HAC based on our framework (i.e. setting \(=0\) and using exact NNS queries) which has the benefit of using only linear space and supports parallelism in NNS queries. We also implement a _bucket-based_ version of approximate Centroid HAC as a baseline, based on the approach of  along with an observation to handle the non-monotonicity of Centroid HAC; details in Appendix E.

    & Dataset & Centroid\({}_{0.1}\) & Centroid\({}_{0.2}\) & Centroid\({}_{0.4}\) & Centroid\({}_{0.8}\) & Exact Centroid \\   **\(\)** \\ ** } & iris & **0.750** & 0.746 & 0.638 & 0.594 & **0.759** \\  & wine & 0.352 & 0.352 & **0.402** & 0.366 & 0.352 \\  & cancer & 0.509 & 0.526 & 0.490 & **0.641** & 0.509 \\  & digits & 0.589 & 0.571 & 0.576 & **0.627** & 0.559 \\  & faces & 0.370 & 0.388 & **0.395** & 0.392 & 0.359 \\  & mnist & **0.270** & 0.222 & 0.218 & 0.191 & 0.192 \\  & birds & 0.449 & 0.449 & 0.442 & **0.456** & 0.441 \\   & **Avg** & **0.471** & 0.465 & 0.452 & 0.467 & 0.453 \\   **\(\)** \\ ** } & iris & **0.803** & 0.795 & 0.732 & 0.732 & **0.803** \\  & wine & 0.424 & 0.424 & 0.413 & 0.389 & 0.424 \\  & cancer & 0.425 & 0.471 & 0.459 & **0.528** & 0.425 \\   & digits & 0.718 & 0.726 & 0.707 & **0.754** & 0.727 \\   & faces & 0.539 & 0.534 & 0.549 & 0.549 & **0.556** \\   & mnist & 0.291 & 0.282 & 0.306 & **0.307** & 0.250 \\   & birds & 0.748 & 0.747 & 0.756 & **0.764** & 0.743 \\    & **Avg** & 0.564 & 0.569 & 0.560 & **0.575** & 0.561 \\   

Table 1: The ARI and NMI scores of our approximate Centroid HAC implementations for \(=0.1,0.2,0.4,\) and 0.8, versus Exact Centroid HAC. The best quality score for each dataset is in bold and underlined.

**Results.** We now summarize the results of our scalability study; see Appendix E.2 for a more details and plots. Figures 3(a) and 3(b) shows the running times on varying slices of the SIFT-1M dataset using one thread and 192 parallel threads, respectively. For the approximate methods, we considered \(=0.1\). Figure 3(c) further compares the heap and bucket based approaches as a function of \(\).

We observe that the bucket based approach is very slow for small values of \(\) due to many redundant nearest-neighbor computations. Yet, at \(=0.1\), both algorithms, with 192 threads, obtain speed-ups of up to 175\(\) with an average speed-up of 22\(\) compared to the exact counterparts. However, on a single thread, the bucket based approach fails to scale, while the heap based approach still achieves significant speed-ups of upto 36\(\) with an average speed-up of 5\(\). Overall, our heap-based approximate Centroid HAC implementation with \(=0.1\) demonstrates good quality and scalability, making it a good choice in practice.

## 6 Conclusion

In this work we gave an approximate algorithm for Centroid-Linkage HAC which runs in subquadratic time. Our algorithm is obtained by way of a new ANNS data structure which works correctly under adaptive updates which may be of independent interest. On the empirical side we have demonstrated up to 36\(\) speedup compared to the existing baselines. An interesting open question is whether approximate Centroid-Linkage HAC admits a theoretically and practically efficient _parallel_ algorithm.