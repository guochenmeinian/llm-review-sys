# Towards Unbounded Machine Unlearning

Meghdad Kurmanji

University of Warwick

&Peter Triantafillou

University of Warwick

&Jamie Hayes

Google DeepMind

&Eleni Triantafillou

Google DeepMind

Correspondence to Meghdad.Kurmanji@warwick.ac.uk.

###### Abstract

Deep machine unlearning is the problem of'removing' from a trained neural network a subset of its training set. This problem is very timely and has many applications, including the key tasks of removing biases (RB), resolving confusion (RC) (caused by mislabelled data in trained models), as well as allowing users to exercise their 'right to be forgotten' to protect User Privacy (UP). This paper is the first, to our knowledge, to study unlearning for different applications (RB, RC, UP), with the view that each has its own desiderata, definitions for 'forgetting' and associated metrics for forget quality. For UP, we propose a novel adaptation of a strong Membership Inference Attack for unlearning. We also propose SCRUB, a novel unlearning algorithm, which is the only method that is consistently a top performer for forget quality across the different application-dependent metrics for RB, RC, and UP. At the same time, SCRUB is also consistently a top performer on metrics that measure model utility (i.e. accuracy on retained data and generalization), and is more efficient than previous work. The above are substantiated through a comprehensive empirical evaluation against previous state-of-the-art.

## 1 Introduction

Can we make a deep learning model 'forget' a subset of its training data? Aside from being scientifically interesting, achieving this goal of deep machine unlearning is increasingly important and relevant from a practical perspective and this research direction is enjoying significant attention recently [Triantafillou et al., 2023]. Regulations such as EU's General Data Protection Regulation [Mantelero, 2013] stipulate that individuals can request to have their data 'deleted' (the 'right to be forgotten'). Nowadays, given the ubiquity of deep learning systems in a wide range of applications, including computer vision, natural language processing, speech recognition and healthcare, allowing individuals to exercise this right necessitates new technology. Unlearning aims to fill this need.

However, unlearning has several additional important applications where privacy with respect to a subset of individual training examples isn't necessarily a consideration: removing out-of-date examples, outliers, poisoned samples [Jagielski et al., 2018], noisy labels [Northcutt et al., 2021], or data that may carry harmful biases [Fabbrizzi et al., 2022]. In fact, we are the first to argue that the definition of 'forgetting' is application-dependent, and unlearning comes with different desiderata and priorities in different scenarios. For instance, for a privacy application, 'forgetting' the data of users who requested deletion is the primary objective and we may be willing to sacrifice model performance to some degree, in exchange for reliably defending Membership Inference Attacks (MIAs). In other cases, for instance, when deleting outdated data to keep a model current, maintaining the model's performance throughout such 'cleanup' operations may be the primary consideration, and guaranteeing that no trace of the old data is left behind may be irrelevant. While previous work does not establish such distinctions, we study three scenarios and propose a method that is the most consistent top-performer across applications and metrics.

Unfortunately, removing the influence of a subset of the training set from the weights of a trained model is hard, since deep models memorize information about specific instances (Zhang et al., 2020, 2021, Arpit et al., 2017) and their highly non-convex nature makes it difficult to trace the effect of each example on the model's weights. Of course, we can apply the naive solution of retraining the model from scratch without the cohort of data to be forgotten. This procedure indeed guarantees that the weights of the resulting model aren't influenced by the instances to forget (it performs '_exact unlearning_'), but the obvious drawback is computational inefficiency: retraining a deep learning model to accommodate each new forgetting request isn't viable in practice. To mitigate that cost, recent work turned to _approximate unlearning_(Izzo et al., 2021, Golatkar et al., 2020, 2020) where the goal is to modify the weights of the trained model to approximate those that would have been obtained by exact unlearning. That is, they strive to achieve 'indistinguishability' between the approximate and exact solutions, typically accompanied by theoretical guarantees for the quality of that approximation. This goal is also mirrored in the metrics used by these works: the model's error on the deleted data is desired to be just as high as the error of the 'exact' retrained model (which truly never saw that data). The argument is that an error higher than that reference point may cause susceptibility to membership inference attacks: a noticeably poor performance on an example can reveal that it was unlearned.

In this work, we show empirically that such approximate unlearning methods often perform poorly in practice (in one or more of RB, RC and UP). We hypothesize that this is due to potential violations of assumptions made by these methods, e.g. that the 'optimal' solution of unlearning is close in weight space to the original model (Golatkar et al., 2020, 2020) (due to assumptions of stability of SGD and'small enough' forget sets). Further, we find these previous methods scale poorly to the size of the training set and the model, often making them impractical. To address these issues, we propose a new unlearning model, SCalable Remembering and Unlearning unBound (SCRUB) that departs from that methodology. SCRUB bears a novel teacher-student formulation, where the student model selectively disobeys an all-knowing teacher, to inherit from it _only_ knowledge that does not pertain to the data to be deleted. This method is 'unbound' from limiting assumptions (e.g., to facilitate formal guarantees), poor scalability limits, and is not tailored to a single unlearning application. We find that SCRUB can yield a very high error on the deleted data, which is desired in some scenarios but may cause susceptibility to membership inference attacks. To mitigate this when relevant, we extend SCRUB with a novel'rewinding' procedure that pinpoints the 'checkpoint' of the unlearning process to use such that the error on the deleted data approximates the reference point of being 'just high enough'.

We also propose the first, to our knowledge, adaptation of the state-of-the-art LiRA MIA (Carlini et al., 2022) to the framework of unlearning, as a metric for UP. Overall, we perform a comprehensive evaluation using different baselines, datasets, architectures and three application scenarios. We find that SCRUB is by far the most consistent top performer out of all state-of-the-art methods considered. SCRUB is able to ensure high forget quality, satisfying different forget-quality metrics for different applications while also being a top performer with respect to model utility (accuracy on retained data, and generalization). Our findings highlight the need to consider a broad spectrum of applications when evaluating unlearning algorithms, to better understand their properties and trade-offs.

## 2 Problem Definition for Unlearning Unbound

Notation and preliminariesLet \(=\{x_{i},y_{i}\}_{i=1}^{N}\) denote a training dataset containing \(N\) examples, where the \(i\)'th example is represented by a vector of input features \(x_{i}\) and a corresponding class label \(y_{i}\). Let \(f(;w)\) denote a function parameterized by trainable weights \(w\). In this work, we study the case where \(f\) is represented by a deep neural network, trained in a supervised manner via empirical risk minimization. Specifically, we define the loss function as \((w)=_{i=1}^{N}l(f(x_{i};w),y_{i})\) where \(l\) denotes the cross entropy loss.

Deep Machine UnlearningWe now formalize the problem of deep machine unlearning. We assume that we are given a model \(f(;w^{o})\) that has been trained on \(\), where \(f\) denotes a neural network and \(w^{o}\) its trained parameters, obtained by minimizing the above loss function. We will refer to this as the 'original model' (i.e. before any unlearning intervention is performed). We are also given a 'forget set' \(_{f}=\{x_{i},y_{i}\}_{i=1}^{N_{f}}\), comprised of \(N_{f}\) examples from \(\) and a'retain set' \(_{r}\) of \(N_{r}\) training examples that the model is still allowed to retain information for. For simplicity, we consider the standard scenario where \(_{r}\) is the complement of \(_{f}\) or, in other words, \(_{f}_{r}=\)Given \(f(;w^{o})\), \(_{f}\) and \(_{r}\), the goal of deep machine unlearning is to produce a new set of weights \(w^{u}\) such that the 'unlearned model' \(f(;w^{u})\) has 'forgotten' \(D_{f}\) without hurting the 'utility' of the original model (its performance on the retained data \(D_{r}\) and generalization to data outside of \(D\)). We argue that defining (and measuring) 'forgetting' is application-dependent. Our work is the first, to the best of our knowledge, to perform a thorough empirical evaluation of unlearning algorithms on three different application scenarios, each with its own definition of forgetting and associated metrics.

Defining and measuring 'forgetting'We consider three different application scenarios in this work, each with their own definition of forgetting and associated metrics. 1) **Removing biases**. We want _maximal_ error on \(_{f}\): the model should _never_ predict the assigned labels of forget examples, since they reflect unintended behaviour. 2) **Resolving confusion**. A model may be 'confused' due to incorrect labels in its dataset. A successful unlearning algorithm resolves this if mislabeled examples are placed in the forget set (see metrics in Section 5). 3) **User privacy**. Success from a privacy perspective is associated with a forget error _only as high as that of retraining-from-scratch_; a higher forget error could lead to vulnerability to Membership Inference Attacks (MIAs). In all of the above cases, in addition to achieving forgetting, we seek unlearning algorithms that don't damage _model utility_, i.e. accuracy on retained data and generalization ability, and that unlearning is _efficient_ enough to be preferable over the simple and expensive solution of retraining the model.

Fundamental trade-offsA top-performing unlearning algorithm must simultaneously achieve three feats: (i) obtain high 'forget quality', (ii) ensure continued high accuracy on retained and unseen data, and (iii) efficiency / scalability. In practice, there are fundamental trade-offs in two different dimensions. The first dimension involves trade-offs between (i), (ii) and (iii), for example achieving good 'forget quality' at the expense of efficiency. Retraining from scratch is such an example, but, as we will show later, some methods suffer from inefficiencies even worse than retraining. Another common issue we will observe is achieving good 'forget quality' at the expense of utility. The second dimension involves trade-offs _within_ the context of achieving good 'forget quality' across applications, since the metric for this quality is application-specific, and an algorithm may perform strongly on one such metric but poorly on others. SCRUB is designed to handle these trade-offs and is shown empirically to consistently outperform previous methods across the board.

## 3 Scalable Remembering and Unlearning unBound (SCRUB)

We present SCRUB, a new unlearning algorithm based on a novel casting of the unlearning problem into a teacher-student framework. We design SCRUB to meet the desiderata of unlearning: efficiently 'forgetting' without hurting model utility. Because we argue that the definition of forgetting is application-dependent, we propose a recipe that works across applications: We first design SCRUB to strive for maximal forget error, which is desirable in some scenarios (e.g. RB and RC) but not in others (e.g. UP). To address the latter case, we complement SCRUB with a novel'rewinding' procedure that can reduce the forget set error appropriately when required. SCRUB is 'unbound' from limiting assumptions (e.g., to facilitate formal guarantees), poor scalability limits and is by far the most consistent method in forgetting across applications while preserving model utility.

### The SCRUB

We consider the original model \(w^{o}\) as the 'teacher' and we formulate our goal as training a'student' \(w^{u}\) that _selectively_ obeys that teacher. Intuitively, our goal for \(w^{u}\) is twofold: forget \(_{f}\) while still remembering \(_{r}\). To that effect, \(w^{o}\) should be obeyed when teaching about \(_{r}\), but disobeyed when teaching about \(_{f}\). Our code is available for reproducibility 2.

En route to deriving our training objective, let us first define:

\[d(x;w^{u})=D_{}(p(f(x;w^{o}))\|p(f(x;w^{u})))\]

In words, \(d(x;w^{u})\) is the KL-divergence between the student and teacher output distributions (softmax probabilities) for the example \(x\). We make the dependence of \(d\) on \(w^{u}\) explicit, since we will optimize the student weights \(w^{u}\) while keeping the teacher weights frozen \(w^{o}\), treating them as a constant.

Concretely, we begin by initializing the student \(w^{u}\) to the weights of the teacher \(w^{o}\). Since the teacher weights \(w^{o}\) were trained on all of \(\), the teacher performs well on both constituents of \(\), namely both of \(_{r}\) and \(_{f}\). Therefore, the student has good performance on \(_{r}\) at initialization, already fulfilling one of our desiderata. Can we then start from this solution and modify it to unlearn \(_{f}\)? In principle, one could do this by optimizing:

\[_{w^{u}}-}_{x_{f}_{f}}d(x_{f};w^{u})\] (1)

However, in practice, when performing this maximization of the distance between the student and teacher on the forget set, we noticed that, while indeed the performance on \(_{f}\) degrades, as desired, it unfortunately also results in degrading performance on \(_{r}\). To amend that, we propose to simultaneously encourage the student to'stay close' to the teacher on retain examples, while encouraging it to'move away' from the teacher on forget examples, adding a contrastive flavour. Formally, the optimization objective becomes:

\[_{w^{u}}}_{x_{r}_{r}}d(x_{r};w^{u})- {1}{N_{f}}_{x_{f}_{f}}d(x_{f};w^{u})\] (2)

Furthermore, SCRUB also simultaneously optimizes for the task loss on the retain set, to further strengthen the incentive to perform well there. Our final training objective is then the following:

\[_{w^{u}}\ \ }_{x_{r}_{r}}d(x_{r};w^{u })+}_{(x_{r},y_{r})_{r}}l(f(x_{r};w^{u}), y_{r})-}_{x_{f}_{f}}d(x_{f};w^{u})\] (3)

where \(l\) stands for the cross-entropy loss and the \(\) and \(\) are scalars that we treat as hyperparameters.

In practice, we found that optimizing the objective in Equation 3 is challenging, due to oscillations in the loss. Intuitively, this is due to trying to simultaneously satisfy two objectives, which may interfere with each other, namely moving close to the teacher on some data points while moving away from it on others. To address this, SCRUB provides a practical recipe for optimization, reminiscent of 'tricks' used in other min-max-like problems like Generative Adversarial Networks (Goodfellow et al., 2014) where the discriminator is trained for several steps before each update to the generator.

Specifically, SCRUB iterates between performing an epoch of updates on the forget set (the _max-step_) followed by an epoch of updates on the retain set (the _min-step_), in an alternating fashion. Guarding against hurting retain performance due to this alteration of _min-steps_ and _max-steps_, SCRUB also performs a sequence of additional _min-steps_ at the end of the sequence to restore the retain performance in the event that it was harmed. SCRUB training stops when the forget error has increased without harming the retain set error. We find in practice this point can be reached with a small number of epochs. We provide pseudocode, training plots and ablations in the Appendix.

### SCRUB and Rewind (SCRUB+R)

By construction, SCRUB encourages obtaining a high error on the forget set. This is desired for some application scenarios. However, an uncharacteristially high error on deleted examples can make them identifiable, causing vulnerability to membership inference attacks. SCRUB+R addresses this.

To this end, we are after a procedure for selecting the checkpoint of SCRUB where the forget set error is 'just high enough'. One could obtain a reference point for that by retraining from scratch without the forget set and recording the error of that model on the forget set. However, that would defeat the purpose of unlearning, as we want to avoid that computation. Therefore, we propose a different way of establishing a reference point. First, we create a validation set of the same distribution as the forget set. For instance, if the forget set has only examples of class 0, we keep only examples of class 0 in the validation set too. Next, we train SCRUB as usual, storing a model checkpoint every epoch. At the end of its training, where the forget error is typically the highest, we measure the error on the constructed validation set. This will serve as the reference point for the desired forget set error. Finally, we'rewind' to the checkpoint where the forget error is closest to that reference point.

The intuition is that the last step of unlearning approximates'maximally forgetting'. Consequently, the error on the identically-distributed validation set approximates the error of a model that never learned about that distribution from the forget set: any correct predictions on the held-out examples are due only to the generalization power of the model that was trained only on the retain set. Therefore, we choose that validation set error to serve as the reference point for how high we would like the forget set error to be for UP applications. We show empirically that this rewinding procedure greatly increases the defense against membership inference attacks, making SCRUB a strong performer for applications where privacy is a consideration.

## 4 Related Work

Unlearning definitionsDespite prior work on unlearning, this research problem is still in its infancy and we lack a well-established formal definition. The term 'unlearning' was coined in Cao and Yang (2015), where, in the context of certain structured problems, unlearning is defined to be successful if deleting an example yields the same outputs on a dataset as if that example had never been inserted. Ginart et al. (2019) introduce a more relaxed probabilistic definition for machine unlearning inspired by Differential Privacy (Dwork et al., 2014) that requires the unlearned model to yield outputs that are _similar_ to those that would have been produced by a model retrained from scratch without the forget set. This led to the development of 'approximate learning' methods that aren't identical to retrain-from-scratch ('exact unlearning') but'similar' (Guo et al., 2019, Golatkar et al., 2020a). Sekhari et al. (2021) and Neel et al. (2021) adopt similar DP-inspired definitions of (approximate) unlearning based on the goal of indistinguishability from retrain-from-scratch.

However, there is a recent line of criticism against the idea of defining or measuring unlearning success based on indistinguishability from retrain-from-scratch. Thudi et al. (2022) theoretically show that we can obtain arbitrarily similar model weights from training on two non-overlapping datasets. This implies that reaching a particular point in parameters space isn't a sufficient condition for having unlearned. Furthermore, Goel et al. (2022) argue that it's not a necessary condition either: retraining from scratch may arrive at different model distributions due to changes in e.g. hyperparameters (Yang and Shami, 2020). Therefore, a unlearning method may be unnecessarily penalized for not matching a _particular_ retrained-from-scratch model. In our work, we take a practical viewpoint and define unlearning as 'forgetting' in an efficient manner and without hurting model utility, for three different application-specific metrics for forgetting that we discuss in the next section.

Unlearning methodsCao and Yang (2015) introduce unlearning and provide an exact forgetting algorithm for statistical query learning. (Bourtoule et al., 2021) proposed a training framework by sharding data and creating multiple models, enabling exact unlearning of certain data partitions. Ginart et al. (2019) introduce a probabilistic definition for machine unlearning inspired by Differential

Figure 1: **RB results: SCRUB is the only consistent top-performer in terms of forgetting and preserving utility. It is also highly efficient: second only to Bad-T which, however, fails at forgetting and damages utility. In all subfigures, each point represents the average across ResNet and All-CNN variants. For large-scale results, we also compute the scale-up factor: the fraction of the runtime of retraining from scratch over the runtime of the given unlearning algorithm (multiplied by 5 here, for visualization purposes). We find that selective unlearning is harder for all methods, especially for EU-k and Fisher (see Figure 0(a) vs 0(b)). CF-k and Finetuning perform similarly (they perform poorly) in all cases, so their lines occlude one another. Complete tables are in the Appendix.**

Privacy (Dwork et al., 2014), and formed the origin of the idea of the model indistinguishability goal. More recently, Guo et al. (2019), Izzo et al. (2021), Sekhari et al. (2021) built upon this framework and introduced unlearning methods that can provide theoretical guarantees under certain assumptions. Further, Huang and Canonne (2023) provide bounds for 'deletion capacity' in the DP-based unlearning methods using the group property of DP. (Graves et al., 2021) proposed two methods applicable to deep networks. The first randomly re-labels the forget set and retrains for a small number of updates, while the second stores the index of examples that participated in each batch and the per-batch parameter updates, and unlearns by 'undoing' the affected updates. This requires a lot of storage and can also lead to a poor proxy of the model that would have been obtained had those parameter updates not happened in the first place, especially for larger sequences. (Golatkar et al., 2020) proposed an information-theoretic procedure for removing information about \(_{f}\) from the weights of a neural network and (Golatkar et al., 2020, 2021) propose methods to approximate the weights that would have been obtained by unlearning via a linearization inspired by NTK theory (Jacot et al., 2018) in the first case, and based on Taylor expansions in the latter. However, (Golatkar et al., 2020, 2020) scale poorly with the size of the training dataset, as computing the forgetting step scales quadratically with the number of training samples. (Golatkar et al., 2021) addresses this issue, albeit under a different restrictive assumption: that a large dataset is available for pre-training that will remain'static', in that no forgetting operations will be applied on it; an assumption that is not met in many important applications (e.g. healthcare, medical images). Some more recent works propose modifications to the original model training, to make the resulting model more amenable to unlearning. Thudi et al. (2022) propose a regularizer to reduce the'verification error', which is an approximation to the distance between the unlearned model and a retrained-from-scratch model. The goal is to make unlearning easier in the future. Additionally, they propose a single gradient update that reverses the effect of \(_{f}\). This regularizer, however, may have a negative effect on the model's performance, and the single gradient unlearning step only removes the effect that \(_{f}\) has had in the first iteration of training. (Zhang et al., 2022) present a training process that quantizes gradients and applies randomized smoothing. This process is designed to make unlearning unnecessary in the future and comes with certifications under some conditions. However, a deletion request that causes too large a change to the data distribution (e.g. if doing class unlearning), may exceed the 'deletion budget' and invalidate their assumptions.

Related research areasDifferential Privacy (DP) (Dwork et al., 2014) and Life-Long Learning (LLL) (Parisi et al., 2019) have shared goals with unlearning. DP seeks to ensure that the trained model doesn't store information about _any_ training instance; a strong goal that is difficult to achieve for deep neural networks (Abadi et al., 2016). In contrast, in unlearning we only desire to remove the influence of the training instances in the forget set, which can be seen as a relaxation of the same problem. LLL seeks to continuously update a model to solve new sets of tasks, without 'catastrophically forgetting' previously-learned tasks. The notion of forgetting a task, though, is distinct from that of forgetting a training example: the performance on a task can degrade, without having removed the influence of particular examples from the model weights.

Teacher-student and contrastive methodsThe teacher-student methodology has been used in a different contexts, e.g. self-supervised (Grill et al., 2020; Chen and He, 2021) and semi-supervised learning (Xie et al., 2020). Knowledge Distillation (Hinton et al., 2015) can also be thought of as a teacher-student approach with the additional desire for compression, where the student is chosen to have a smaller architecture than the teacher. In addition, our training objective has a contrastive flavour (Chen et al., 2020; He et al., 2020), due to both pulling the student close to the teacher, and pushing it away from it, for different parts of the training set. For unlearning, (Kim and Woo, 2022; Wu et al., 2022) propose methods with a two-stage pipeline: 'neutralization' / forgetting followed by a'retraining' / restoring, that restores performance that may be lost in the first phase. Both use distillation only in the second phase on only the retain set. In contrast, SCRUB employs a single phase where we carefully design a teacher-student formulation to both encourage forgetting the requested data and retaining the other knowledge, simultaneously. Most similar to our work in that regard is the method of (Chundawat et al., 2022) that employs a related but different teacher-student objective that encourages moving close to a 'Bad Teacher' for the forget set (we thus refer to this as 'Bad-T'). Moving close to a different, 'bad', teacher for forget examples is fundamentally different from our goal of moving away from the (single) teacher as in SCRUB. We find that Bad-T forgets poorly compared to SCRUB in all applications, and degrades model quality.

## 5 Experiments

### Experimental setup

Overview of applications and metricsWe consider three sets of forget-quality metrics, for different applications. First, we consider the scenario of **Removing Biases (RB)** where we desire the highest possible forget set error (reflecting that we want the model to _never_ predict the associated labels of the forget set, since those capture an unintended behaviour / bias). Next, inspired by Goel et al. (2022), we consider unlearning for **Resolving Confusion (RC)** between classes, caused by a portion of the model's original training set being mislabelled. Finally, we consider a scenario of data deletion for **User Privacy (UP)**. In this case, we use Membership Inference Attacks (MIAs) to measure success (including the first, to our knowledge, adaptation of the state-of-the-art LiRA MIA (Carlini et al., 2022) to unlearning), where unlearning succeeds if the attacker is unable to tell apart forgotten examples from truly unseen ones. In all cases, aside from achieving the application-specific forgetting goal, we also measure utility (performance on \(_{r}\) and a test set \(_{t}\)) as an additional metric.

Experimental detailsWe utilize the same two datasets from previous work: CIFAR-10 (Krizhevsky et al., 2009) and Lacuna-10 (Goldatz et al., 2020), which is derived from VGG-Faces (Cao and Yang, 2015), and the same two architectures: All-CNN (Springenberg et al., 2014) and ResNet-18 (He et al., 2016). For consistency with previous work, we pre-train the original model on CIFAR-100 / Lacuna-100 for the CIFAR-10 / Lacuna-10 experiments, respectively (Goldatz et al., 2020, 2020). We run each experiment with 3 random seeds and report the mean and standard deviation.

We consider two settings throughout the paper: **small-scale** and **large-scale**. The former allows comparisons with NTK, which doesn't scale beyond it. For the small-scale, we exactly follow the setup in (Goldatz et al., 2020) that uses only 5 classes from each of CIFAR-5' / 'Lacuna-5'), with 100 train, 25 validation and 100 test examples per class. The forget set contains 25 examples from class 0 (5%). For the large-scale, we exactly follow the setup in (Goldatz et al., 2020) that uses all 10 classes of each of CIFAR and Lacuna, and considers both a _class unlearning_ scenario where the forget set is the entire training set for class 5 (10%), as well as a _selective unlearning_ one where 100 examples of class 5 are forgotten (0.25% in CIFAR and 2% in Lacuna). We note that the Appendix contains complete results for all scenarios (several are ommitted from the main paper due to space constraints) and also contains the complete experimental details and hyperparameters.

    &  &  &  &  &  &  &  &  \\  & mean & std & mean & std & mean & std & mean & std & mean & std & mean & std & mean & std & mean & std & mean & std \\  Retain & 24.0 & 1.8 & 0.0 & 0.0 & 183.3 & 4.2 & 0.0 & 0.0 & 19.0 & 1.3 & 0.0 & 0.0 & 11.3 & 4.6 & 0.0 & 0.0 \\  Original & 56.0 & 3.0 & 0.0 & 0.0 & 92.0 & 7.9 & 0.0 & 0.0 & 4.0 & 0.0 & 19.0 & 80.0 & 12.6 & 6.0 & 10.4 \\ Piegue & 52.0 & 3.0 & 1.0 & 0.0 & 0.0 & 79.3 & 10.1 & 0.0 & 0.0 & 43.7 & 7.3 & 0.0 & 0.0 & 67.3 & 16.0 & 0.0 & 0.0 \\ NoEnd-to-0 & 47.5 & 5.3 & 0.0 & 0.0 & 69.0 & 13.5 & 0.0 & 0.0 & 42.3 & 11.3 & 0.0 & 0.0 & 62.0 & 22.6 & 0.0 & 0.0 \\ C++ & 54.8 & 2.0 & 0.0 & 0.0 & 83.7 & 7.0 & 0.0 & 0.0 & 47.8 & 5.8 & 0.0 & 0.0 & 76.3 & 14.4 & 0.0 & 0.0 \\ Bi-k & 47.0 & 4.8 & 8.3 & 4.7 & 65.3 & 9.7 & 3.7 & 2.5 & 59.5 & 5.2 & 8.3 & 6.7 & 68.7 & 15.6 & 19.7 & 10.4 \\ Fabric & 51.5 & 7.5 & 26.3 & 9.5 & 79.0 & 3.6 & 20.0 & 7.9 & 56.8 & 8.7 & 31.7 & 41.0 & 78.3 & 15.5 & 17.7 & 11.5 \\ NTK & 37.5 & 4.0 & 0.0 & 0.0 & 52.0 & 10.6 & 0.0 & 0.0 & 36.7 & 4.1 & 3.0 & 52.4 & 54.3 & 9.0 & 3.0 & 5.2 \\ Bu-T & 47.3 & 9.7 & 6.0 & 6.2 & **27.0** & 8.7 & 0.3 & 0.6 & **39.3** & 8.2 & 3.0 & 1.0 & **25.7** & 5.0 & 1.0 & 1.0 \\ SCRUB & **19.0** & 3.9 & 0.0 & 0.0 & **19.7** & 7.5 & 0.0 & 0.0 & **26.0** & 4.4 & 0.0 & 0.0 & **18.0** & 11.1 & 0.0 & 0.0 \\   

Table 1: **RC-application results: SCRUB is the strongest performer in terms of resolving class confusion**. These results are on CIFAR-5 with ResNet and All-CNN, where we confused 50 samples of class 0 by mislabeling them as class 1, and 50 of class 1 by mislabeling them as class 0.

Figure 2: The number of times each method was a top performer in forgetting in **RB-application**. A model is a top-performer if its 95% confidence interval overlaps with that of the best mean. Small-scale counts are over {ResNet, All-CNN} x {CIFAR, Lacuna} (4 total), large-scale additionally x {class, selective} (8 total). **SCRUB is the most consistent top-performer**.

Unlearning algorithmsWe compare against state-of-the-art approaches as well as various baselines: **Retrain**: Retraining from scratch without the forget set. This is assumed to not be viable in practice, but we include it as a reference. **Original**: The 'original' model trained on all of \(\) without any unlearning. **Finetuning**: Finetunes the original model on data from the retain set \(_{r}\). **NegGrad+**: Similar to Finetuning, starts from the original model and finetunes it both on data from the retain and forget sets, negating the gradient for the latter. Previous work considered a weaker baseline that only trains on the forget set, with a negative gradient (NegGrad). We tune the stronger NegGrad+ to achieve a good balance between the two objectives in the same way as SCRUB. **Fisher Forgetting**(Golatkar et al., 2020). **NTK Forgetting**(Golatkar et al., 2020). Catastrophic Forgetting-k (**CF-k**) and Exact Unlearning-k (**EU-k**) (Goel et al., 2022): they freeze the first k layers of the original model and either finetune the remaining layers on \(_{r}\) (CF-k) or train them from scratch on \(_{r}\) (EU-k). **Bad-T**: the concurrent teacher-student method of (Chundawat et al., 2022). We utilize the public code for the NTK and Fisher for correctness. We implement Bad-T ourselves (no code was available).

### Unlearning for Removing Biases (RB)

In RB, we desire to achieve the highest possible error on \(D_{f}\), without hurting the error of \(D_{r}\) and \(D_{t}\). Some example scenarios are removing biases from trained models, outdated or incorrect information. A maximal forget error on \(D_{f}\) in this case is desirable as it reflects that we want the model to _never_ make the predictions that were deemed harmful or no longer valid or accurate.

We report the results for this scenario in Figures 1 and 2. We also report complete results in the Appendix for all architectures, datasets and baselines. As shown in Figure 2, SCRUB is by far the most consistent method in terms of achieving high forget quality (highest forget error), while it doesn't hurt retain and test errors.

### Unlearning for Resolving Confusion (RC)

In this section, following Goel et al. (2022), we study unlearning for resolving confusion between two classes that the original model suffers from due to a part of its training set being mislabelled. Specifically, we place all and only the mislabeled training examples in the forget set. A successful unlearning algorithm would thus entirely resolve the model's confusion. In more detail, the setup is: 1) Mislabel some portion of the training dataset (we mislabeled examples between classes 0 and 1 of each of CIFAR-5 and Lacuna-5), 2) Train the 'original model' on the (partly mislabeled) training dataset, and 3) Unlearn with the confused examples as the forget set.

We consider the following metrics inspired from Goel et al. (2022) that are variants of the model's error on examples from the confused classes (lower is better for both). We outline them below and define them formally in the Appendix.

* **Interclass Confusion IC-Err** (e.g. IC test error, IC retain error). This counts mistakes where an example from either of the two confused classes was incorrectly predicted to belong to _any_ other class.
* **Fgt-Err** (e.g. Fgt test error, Fgt retain error). This metric counts only misclassification _between the confused classes_, i.e. an example of class 0 being predicted as belonging to class 1, or vice versa.

We observe from Table 1 that, across the board, SCRUB is by far the most consistent method in terms of eliminating class confusion without damaging the quality of the model (on retain and test sets).

### Unlearning for User Privacy (UP)

Next, we consider a privacy-critical application: the deletion of the data of a user exercising their right-to-be-forgotten. We use Membership Inference Attacks (MIAs) as the metric where the goal is that, after applying unlearning, an attacker can't tell apart examples that were unlearned from those that were truly never seen, thus protecting the privacy of the user requesting deletion.

While previous unlearning papers have reported MIA results (Golatkar et al., 2020), those MIAs are simple and far from state-of-the-art MIAs used in privacy and security papers. Indeed, it's not trivial to adapt such MIAs to the unlearning protocol. To initiate bridging this gap, we report results on two MIAs: 1) a simple one that is closer to the typical MIAs used in the unlearning literature, and 2) the first, to our knowledge, adaptation of the state-of-the-art LiRA attack (Carlini et al., 2022) to the unlearning protocol. We describe both at a high-level here and in detail in the Appendix.

Basic attackWe train a binary classifier (the 'attacker') on the unlearned model's losses on forget and test examples for the objective of classifying 'in' (forget) versus 'out' (test). The attacker then makes predictions for held-out losses (losses the attacker wasn't trained on) that are balanced between forget and test losses. For a perfect defense, the attacker's accuracy is 50%, indicating that it is unable to separate the two sets, marking a success for the unlearning method.

We present Basic MIA results in Table 2 for CIFAR-10. We observe that SCRUB+R is a strong performer. EU-k can also perform strongly in this metric, though not as consistently. We also observe that rewinding was only triggered once, for _selective_ unlearning. Recall that, in both selective and class unlearning, the forget set contains examples coming from a single class, but in the former, it doesn't contain _all_ examples of that class; the rest are in the retain set. Thus, we indeed expect that rewinding would be more useful for selective unlearning: the smaller the portion of a class in the forget set, the larger the remainder of that class in the retain set and consequently, the better we expect the model to generalize on held-out examples of that class (lower error), making it in turn more likely to need to rewind, in order to also lower the forget set error commensurately. In Figure 3 we plot results for different variants of selective unlearning, with different number of examples in the forget set (from the same class) and indeed find that for smaller numbers, rewinding is triggered. Our take-away is that, when needed, rewinding can substantially improve SCRUB's MIA results.

LiRA-for-unlearning attackIn the standard privacy setting, the LiRA attacker trains a large number of'shadow models' (Carlini et al., 2022), for which it controls which examples are in the training set each time (by construction). To then predict the membership status of a 'target example', it estimates two Gaussian distributions: the distribution of confidences of that example under shadow models that trained on it, and the distribution of its confidences under shadow models that didn't. It predicts that the target example is 'in' if the likelihood of the former is larger than that of the latter.

We propose the first, to our knowledge, adaptation of LiRA for unlearning. This is a strong attack where we allow the attacker knowledge of the unlearning algorithm. Concretely, for each shadow model, the attacker also produces a'shadow unlearned' model by applying the given unlearning algorithm several times, using a large number of forget sets (similar to Chen et al. (2021)). Now, for each 'target example', this setup allows the attacker to estimate a different pair of Gaussians: the distribution of (confidences of) that target example under models where it was _forgotten_, and as before, under models where it was not seen. The attacker predicts the example was forgotten if its likelihood under the former is larger than under the latter. We present all details and findings in the Appendix and observe that SCRUB+R outperforms the other methods in defending this attack.

   &  &  \\   &  &  &  &  \\  & mean & std & mean & std & mean & std & mean & std \\  Retrain & 49.33 & 1.67 & 54.00 & 16.3 & 55.00 & 40.873 & 0.24 \\ Original & 71.10 & 67.0 & 65.33 & 0.47 & 66.50 & 0.50 & 71.40 & 0.70 \\ Finetune & 57.50 & 69.60 & 0.02 & 68.80 & 0.10 & 74.97 & 1.27 \\ NogGrad+ & 69.57 & 11.96 & 66.67 & 1.70 & 72.00 & 0.00 & 70.03 & 1.92 \\ CE-k & 75.73 & 0.34 & 65.00 & 0.00 & 69.00 & 2.00 & 72.93 & 1.06 \\ EL-k & **54.20** & 2.27 & **53.00** & 3.27 & 66.50 & 3.50 & **54.00** & 1.22 \\ Bad-T & **84.00** & 1.00 & 59.67 & 4.19 & 40.1 & 2.75 & 76.74 & 4.11 \\ SCRUB & **52.20** & 1.71 & 78.00 & 2.45 & **52.00** & 0.00 & **54.30** & 2.24 \\ SCRUB+R & **52.20** & 1.71 & 58.67 & 1.89 & **52.00** & 0.00 & **54.30** & 2.24 \\  

Table 2: **Basic MIA results (for UP): SCRUB successfully defends MIA**. Note the rewinding procedure of SCRUB+R was triggered only once; see discussion in Section 5.4 and Figure 3. In the Appendix, we also show that the forget error of SCRUB(+R) is close to that of Retrain, as desired.

Figure 3: MIA results for different sizes of forget set on CIFAR-10 with ResNet. Error bars show 95% confidence intervals. Rewinding is most useful for smaller forget sizes (see the discussion in Section 5.4). **SCRUB+R successfully defends MIAs, comparably to the Retrain oracle.**

### Overview of results and take-aways

**Finetuning** is a poor method: it may retain model utility but fails to forget. The previous state-the-art **NTK** and **Fisher** models aren't among the top-performers either. The former performs poorly across the board in terms of forgetting and doesn't scale beyond small datasets. The latter sometimes performs well in RB in terms of forgetting, though not always and it is very slow (Figure 1), exceeding the runtime of Retrain; as also observed in (Goel et al., 2022). **NegGrad+**, our proposed enhancement over the NegGrad baseline, is a strong baseline in terms of achieving a balance between forgetting and utility (it performs well in several settings in terms of RB, albeit not as consistently as SCRUB) and we encourage future work to report this baseline, though SCRUB outperforms it, especially for RC. **CF-k** inherits the performance profile of Finetune: it maintains utility but forgets poorly. On the other hand, **EU-k** more reliably forgets (performs strongly in UP, and often in RB), which is expected relative to CF-k due to retraining part of the network, but SCRUB outperforms it significantly in terms of RC and is more consistently a top performer in RB. A notable failure case that we discovered for EU-k in RB is _selective_ unlearning (notice the contrast between EU-k's forget error between e.g. Figures 0(a) and 0(b)). This finding may speak to where class vs instance information is stored in neural networks and warrants further investigation. **Bad-T** is very efficient, but performs poorly in terms of forgetting, across all applications and also damages model utility. Overall, SCRUB is by far the most consistent in successfully forgetting under different metrics without hurting utility and being efficient.

In the Appendix, we discuss limitations of our work, broader impact, future work and present full experimental results and all details including hyperparameters and pseudocode.

## 6 Discussion and Conclusion

Despite substantial recent attention (Triantafillou et al., 2023), unlearning is a young area of research.

In this work, we propose SCRUB, an unlearning method that is unbound from limiting assumptions and poor scalability limits. SCRUB(+R) is found empirically to perform very strongly: it is by far the most consistent method in achieving good forgetting quality with respect to different application-dependent metrics, while incurring only minimal utility degradation. As such, we believe that SCRUB fills the important need for scalable unlearning methods that perform well in practice on several metrics of interest. However, important questions remain open for future work.

Crucially, despite progress in this direction, a well-established formal definition of the problem of unlearning remains elusive, and consequently we also lack well-established metrics for measuring (theoretically or empirically) the quality of unlearning algorithms. In this work, we focus on empirical evaluation with respect to different application-dependent metrics for forgetting quality that we believe are relevant in real-world applications. However, as the unlearning community matures, we hope that success criteria for (different applications of) unlearning will be formalized and standardized.

Further, an important limitation of SCRUB is the absence of theoretical guarantees, making it perhaps ill-suited for certain application scenarios. While methods that come with guarantees exist, they either aren't applicable to deep neural networks, or we find through our empirical analysis that they perform poorly and don't scale beyond small datasets. Therefore, future work that theoretically studies scalable methods like SCRUB would be very valuable. In absence of that, continuing to study and better understand the trade-offs offered by different methods is an important direction.

On the practical side, we hope that future work continues to push the limits of scalability and studies unlearning in larger models, different architectures, different training objectives like self-supervised learning and different domains and modalities, including foundation language models.

Broader ImpactWhile recent advances in deep learning represent exciting opportunities for our community, they also come with great responsibility. As researchers, we are responsible for understanding and mitigating the issues associated with the widespread use of deep learning technology. Machine learning models may carry harmful biases, unintended behaviours, or compromise user privacy. Our work is intended to take a step in addressing these issues via a post-processing 'unlearning' phase that makes progress over previous solutions in practice, as we show through an extensive empirical investigation. However, SCRUB does not come with theoretical guarantees and we can not prove that applying SCRUB perfectly mitigates those issues, so caution must be taken in practice and proper auditing of machine learning models is critical.