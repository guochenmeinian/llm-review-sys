# Two-Stage Learning to Defer with Multiple Experts

Anqi Mao

Courant Institute

New York, NY 10012

aqmao@cims.nyu.edu

&Christopher Mohri

Stanford University

Stanford, CA 94305

xmohri@stanford.edu

Mehryar Mohri

Google Research & CIMS

New York, NY 10011

mohri@google.com

&Yutao Zhong

Courant Institute

New York, NY 10012

yutao@cims.nyu.edu

###### Abstract

We study a two-stage scenario for learning to defer with multiple experts, which is crucial in practice for many applications. In this scenario, a predictor is derived in a first stage by training with a common loss function such as cross-entropy. In the second stage, a deferral function is learned to assign the most suitable expert to each input. We design a new family of surrogate loss functions for this scenario both in the score-based and the predictor-rejector settings and prove that they are supported by \(\)-consistency bounds, which implies their Bayes-consistency. Moreover, we show that, for a constant cost function, our two-stage surrogate losses are realizable \(\)-consistent. While the main focus of this work is a theoretical analysis, we also report the results of several experiments on CIFAR-10 and SVHN datasets.

## 1 Introduction

Large language models (LLMs) have achieved a remarkable performance on diverse tasks across multiple domains, as reported in recent surveys (Wei et al., 2022; Bubeck et al., 2023). However, their practical application faces two critical challenges: the occurrence of _hallucinations_, that is the generation of incorrect or misleading content, and an inefficient inference. Leveraging multiple experts can address both issues. To reduce hallucinations, one can refrain from using the original predictor in uncertain instances and defer to one of the more complex and more accurate experts. To enhance efficiency, one can derive models of different sizes distilled from the original complex model and use one of these more streamlined versions, while deferring to the more complex and less efficient ones for suitable contexts. Both problems require assigning each instance to the most suitable expert. This motivates the problem of _learning to defer in the presence of multiple experts_.

The scenario of _single-stage learning to defer_ has been studied by several publications, starting with the foundational framework introduced by Cortes et al. (2016, 2023) for learning to reject and followed by a series of studies on abstention and deferral (Madras et al., 2018; Raghu et al., 2019; Mozannar and Sontag, 2020; Wilder et al., 2021; Pradier et al., 2021; Keswani et al., 2021; Raman and Yee, 2021; Liu et al., 2022; Verma and Nalisnick, 2022; Charusaie et al., 2022; Cao et al., 2022; Verma et al., 2023; Mao et al., 2023;,b,c; Mozannar et al., 2023). In the single-stage scenario, a predictor and a deferral function are learned simultaneously, with the deferral function determining the best expert assigned to each input instance. However, in practice, a predictor such as an LLM is already available and retraining one in conjunction with a deferral function could be prohibitively costly: depending on its size and the amount of data used, retraining could take several weeks or months. Thus, the single-stage learning to defer scenario and its solutions often do not align with the practical challenges encountered in real-world applications.

Alternative post-hoc methods have been proposed to address the learning to defer problem. Okati et al. (2021) proposed an iterative approach optimizing a predictor and a rejector over multiple epochs. Within each epoch, first the predictor is trained on points where its loss is lower than that of a human expert; second, the rejector is fitted to predict which of the predictor or the human expert has a lower loss. Narasimhaan et al. (2022) suggested a post-hoc correction to the single-stage learning to defer surrogate losses, specifically the cost-sensitive softmax cross-entropy (CSS) surrogate loss in (Mozannar and Sontag, 2020) and the one-versus-all (OvA) surrogate loss in (Verma and Nalisnick, 2022) for cases where they suffer from underfitting. However, as with the single-stage learning to defer solutions, post-hoc approaches do not apply to scenarios where an existing predictor, pre-trained using a standard classification loss function such as cross-entropy, is already available.

Can we derive a principled algorithm for learning to defer with multiple experts in such scenarios? Which surrogate loss should we adopt and what consistency guarantee can we rely on? This paper deals precisely with these questions.

A key criterion for surrogate losses in learning to defer is Bayes-consistency (Zhang, 2004; Bartlett et al., 2006; Steinwart, 2007), that is minimizing the surrogate loss over the family of measurable functions leads to the minimization of the deferral loss. The surrogate losses proposed in (Mozannar and Sontag, 2020; Verma and Nalisnick, 2022) have been shown to be Bayes-consistent for deferral. However, Bayes-consistency is not relevant in learning tasks since the hypothesis set used, for example that of some family of linear functions or neural networks, never includes all measurable functions. Long and Servedio (2013) proposed a notion of realizable \(\)-consistency, that is consistency associated with a specific hypothesis set in the realizable scenario. Mozannar et al. (2023) recently showed that existing Bayes-consistent surrogate losses in (Mozannar and Sontag, 2020; Verma and Nalisnick, 2022) are not realizable \(\)-consistent for learning with deferral, which can pose significant challenges when learning with a restricted hypothesis set \(\), even for simple linear models. Instead, they proposed a new surrogate loss that is realizable \(\)-consistent when \(\) is closed under scaling. However, they also observed that the loss function of Madras et al. (2018), which is not Bayes-consistent, is actually realizable \(\)-consistent. They acknowledged their inability to prove or disprove whether their proposed surrogate loss is Bayes-consistent. Consequently, it has remained an open problem to identify a surrogate loss that is both consistent and realizable-consistent.

In recent work, Verma et al. (2023) proposed the first Bayes-consistent surrogate losses in the scenario of single-stage learning to defer with _multiple experts_(Hemmer et al., 2022; Keswani et al., 2021; Kerrigan et al., 2021; Straitouri et al., 2022; Benz and Rodriguez, 2022). This scenario is more attractive and significant in applications such as large language models, where multiple models are often available for deferral. However, the surrogate losses proposed by the authors do not benefit from realizable \(\)-consistency, even in the single-expert setting, since they are a straightforward generalization of those of Mozannar and Sontag (2020) and Verma and Nalisnick (2022).

Bayes-consistency, or even realizable \(\)-consistency for a specific hypothesis set \(\), is an asymptotic property, and provides no guarantee for approximate minimizers since convergence could be arbitrarily slow. More favorable guarantees, known as \(\)_-consistency bounds_, were recently introduced for standard classification settings (Awasthi et al., 2022; Mao et al., 2022; Zhang et al., 2022). These guarantees are upper bounds on the target estimation loss expressed in terms of the surrogate estimation loss. They are stronger and more informative guarantees than Bayes-consistency and \(\)-consistency because they are both hypothesis set-specific and non-asymptotic. More recently, Mao et al. (2023) introduced a new family of surrogate losses and algorithms for the general problem of single-stage learning to defer with multiple experts that benefit from strong \(\)-consistency bounds.

**Our contributions.** We study a two-stage scenario for learning to defer with multiple experts that is crucial in practice for many applications. In this scenario, a predictor is derived in a first stage by training with a common loss function such as cross-entropy. In the second stage, a deferral function is learned to assign the most suitable expert to each input. We design a new family of surrogate loss functions for this scenario both in the _score-based setting_ (Section 3) and the _predictor-rejector_ setting (Section 4) and prove that they are supported by \(\)-consistency bounds, which implies their Bayes-consistency. Moreover, we show that, for a constant cost function, our two-stage surrogate losses are realizable \(\)-consistent. While the main focus of this work is a theoretical analysis, we also report the results of several experiments on CIFAR-10 and SVHN datasets (Section 5). We give a comprehensive discussion of related work in Appendix A. We begin by providing some basic definitions and notation (Section 2).

Preliminaries

We consider the standard multi-class classification setting with an input space \(\) and a set of \(n 2\) labels \(=[n]\), where we use the notation \([n]\) to denote the set \(\{1,,n\}\). We study the scenario of _learning to defer with multiple experts_, where the label set \(\) is augmented with \(n_{e}\) additional labels \(\{n+1,,n+n_{e}\}\) corresponding to \(n_{e}\) pre-defined experts \(h_{1},,h_{n_{e}}\). In this scenario, the learner has the option of returning a label \(y\), which represents the category predicted, or a label \(y=n+j\), \(j 1\), in which case it is _deferring_ to expert \(h_{j}\). This setting is referred to as the _score-based setting_(Mozannar and Sontag, 2020; Cao et al., 2022; Mao et al., 2023), since the deferral corresponds to extra \(n_{e}\) scoring functions. An alternative setting is the _predictor-rejector setting_(Cortes et al., 2016, 2023; Mohri et al., 2023; Mao et al., 2023), where the deferral function is selected from a separate family of functions \(\). We introduce that setting and include the corresponding results in Section 4 for completeness.

We denote by \(}=[n+n_{e}]\) the augmented label set and consider a hypothesis set \(\) of functions mapping from \(}\) to \(\). The prediction associated by \(h\) to an input \(x\) is denoted by \((x)\) and defined as the element in \(}\) with the highest score, \((x)=*{argmax}_{y[n+n_{e}]}h(x,y)\), with an arbitrary but fixed deterministic strategy for breaking ties. We denote by \(_{}\) the family of all measurable functions.

The _deferral loss function_\(_{}\) is defined as follows for any \(h\) and \((x,y)\):

\[_{}(h,x,y)=_{(x)*y}_{ (x)[n]}+_{j=1}^{n_{e}}c_{j}(x,y)_{(x)=n+j} \]

Thus, the loss incurred coincides with the standard zero-one classification loss when \((x)\), the label predicted, is in \(\). Otherwise, when \((x)\) is equal to \(n+j\), the loss incurred is \(c_{j}(x,y)\), the cost of deferring to expert \(h_{j}\). Let \(_{j}(x,y)=1-c_{j}(x,y)\). We will denote by \(_{j} 0\) and \(_{j} 1\) finite lower and upper bounds on the cost \(_{j}\), that is \(_{j}(x,y)[_{j},_{j}]\) for all \((x,y)\). There are many possible choices for these costs. Our analysis for Theorem 1, Corollary 2, Theorem 6 is general and requires no assumption other than their boundedness. One natural choice is to define cost \(c_{j}\) as a function of expert \(h_{j}\)'s inaccuracy, for example \(c_{j}(x,y)=_{j}_{_{j}(x)*y}+_{j}\), with \(_{j},_{j}>0\), where \(_{j}(x)\) is the prediction made by \(h_{j}\)th for input \(x\). Typically, the hyperparameter \(_{j}\) has two potential values: zero or one. When \(_{j}\) is set to one, the first term of the formulation pertains to the inaccuracy of expert expert \(h_{j}\). Conversely, with \(_{j}\) set to zero, the first term vanishes, focusing solely on the inference cost. Theorems 5 and 7 are analyzed under this assumption. The \(_{j}\) in the second term corresponds to the inference cost incurred by expert \(h_{j}\).

Given a distribution \(\) over \(\), we will denote by \(_{_{}}(h)\) the expected deferral loss of a hypothesis \(h\), \(_{_{}}(h)=_{\{x,y\}} [_{}(h,x,y)]\), and by \(^{*}_{_{}}()=_{h}_{_{}}(h)\) its infimum or best-in-class expected loss. We will adopt similar definitions for other loss functions.

Given a hypothesis set \(\), an _\(\)-consistency bound_(Awasthi et al., 2021, 2021, 2022, 2023,

## 3 Two-stage \(\)-consistent surrogate loss

In this section, we consider an important _two-stage_ scenario for learning to defer with multiple experts. This is a critical scenario in practice for many applications where a predictor is already available, as a result of training with a loss function \(\) supported by \(\)-consistency bounds, such as the logistic loss (first stage). The logistic loss coincides with the cross-entropy loss when a softmax activation is applied to the output of a neural network. The problem then consists of learning a deferral function (second stage) to assign the most suitable expert to each input instance.

We first design a new family of surrogate losses for this _two-stage_ scenario (Section 3.1). Next, we show that our surrogate losses benefit from \(\)-consistency bounds (Section 3.2). As a by-product, we prove \(}\)-consistency bounds in standard multi-class classification, where \(}\) denotes hypothesis sets with a fixed scoring function (Section 3.3). These bounds have not been studied before and can be of independent interest in other consistency studies. Moreover, we show that, for a constant cost function, our two-stage surrogate losses are realizable \(\)-consistent (Section 3.4).

### General surrogate losses

A hypothesis set \(\) of functions mapping from \([n+n_{e}]\) to \(\) can be decomposed as \(=_{p}_{d}\), where \(_{p}\) denotes the hypothesis set spanned by the first \(n\) scores, used for prediction, and \(_{d}\) the hypothesis set spanned by the final \(n_{e}\) scores, used for deferral. Thus, any \(h\) can be written as a pair \(h=(h_{p},h_{d})\) with \(h_{p}_{p}\) and \(h_{d}_{d}\).

Let \(\) be a surrogate loss for standard multi-class classification with \(n\) classes. We consider the following two-stage scenario: in the first stage, \(h_{p}\) is learned using the surrogate loss \(_{1}\); in the second stage, \(h_{d}\) is learned using a surrogate loss \(_{h_{p}}\) that depends on the prediction function \(h_{p}\) learned in the first stage.

To any \(h_{d}_{d}\), we associate a hypothesis \(_{d}\) defined over \((n_{e}+1)\) classes \(\{0,1,,n_{e}\}\) by \(_{d}(x,0)=_{y y}h_{p}(x,y)\), that is the maximal score assigned by \(h_{p}\) to its predicted label, and \(_{d}(x,j)=h_{d}(x,j)\) for \(j[n_{e}]\). We can then define our suggested surrogate loss for the second stage as follows:

\[_{h_{p}}(h_{d},x,y)=_{h_{p}(x)=y}\,_{2}( _{d},x,0)+_{j=1}^{n_{e}}_{j}(x,y)_{2}(_ {d},x,j), \]

where \(_{2}(_{d},x,j)\) is a surrogate loss for standard multi-class classification with \((n_{e}+1)\) categories \(\{0,1,,n_{e}\}\). Intuitively, the indicator term \(_{h(x)n+j}\) in the deferral loss (1) penalizes \(h_{d}(x,j)\) when it has a small value. Similarly, for a standard surrogate loss \(_{2}(_{d},x,j)\) such as the logistic loss, it penalizes \(_{d}(x,j)\) when it has a small value as well. In Table 2, we present a summary of examples of such second-stage surrogate losses, where \(_{2}\) is selected from common surrogate losses in standard multi-class classification defined in Table 1. A detailed derivation is presented in Appendix B.

From the point of view of the second stage, \(x_{d}(x,0)=_{y y}h_{p}(x,y)\) is a fixed function. We will denote by \(}_{d}\) the family of hypotheses \(_{d}\{0,1,,n_{e}\}\) whose first scoring function, \(_{d}(,0)\), is fixed and not to be learned in the second stage.

Our formulation bears some similarity with the design of a surrogate loss function for rejectors in (Cortes et al., 2016, 2023) for learning with rejection in binary classification, where the cost is a

   Name & Formulation \\  Sum exponential loss & \(_{}(,x,y)=_{y^{} y}e^{( x,y^{})-(x,y)}\). \\ Multinomial logistic loss & \(_{}(,x,y)=_{y^{} y\{0 \}}e^{(x,y^{})-(x,y)}\). \\ Generalized cross-entropy loss & \(_{}(,x,y)=1-[(x,y)}}{_{y^{} y\{0\}}e^{(x,y^{ })}}]^{}\), \((0,1)\). \\ Mean absolute error loss & \(_{}(,x,y)=1-(x,y)}}{_{y^{ } y\{0\}}e^{(x,y^{})}}\). \\   

Table 1: Common surrogate losses in standard multi-class classification.

constant. However, our surrogate loss is tailored to accommodate a general cost function depending on both \(x\) and \(y\) for deferral, in contrast with a constant one, and it allows for multiple deferral options, as opposed to only one rejection option.

### \(\)-consistency bounds for two-stage surrogate losses

In this section, we provide strong guarantees for two-stage surrogate losses, provided that the first-stage loss function \(_{1}\) admits an \(_{p}\)-consistency bound, and the second-stage surrogate \(_{2}\) admits an \(}_{d}\)-consistency bound.

**Theorem 1** (\(\)-consistency bounds for score-based two-stage surrogates).: _Assume that \(_{1}\) admits an \(_{p}\)-consistency bound and \(_{2}\) admits an \(}_{d}\)-consistency bound with respect to the multi-class zero-one classification loss \(_{0-1}\) respectively. Thus, there are non-decreasing concave functions \(_{1}\) and \(_{2}\) such that, for all \(h_{p}_{p}\) and \(_{d}}_{d}\), we have_

\[_{_{0-1}}(h_{p})-_{_{0-1}}^{*}( _{p})+_{_{0-1}}(_{p})_{1} _{_{1}}(h_{p})-_{_{1}}^{*}(_{p})+ _{_{1}}(_{p})\] \[_{_{0-1}}(_{d})-_{_{0-1 }}^{*}(}_{d})+_{_{0-1}}(}_{d})_{2}_{_{2}}(_{d})- _{_{2}}^{*}(}_{d})+_{_{2}}( }_{d}).\]

_Then, the following holds for all \(h\):_

\[_{_{}}(h)-_{_{}}^{*}()+_{_{}}( )\] \[_{1}_{_{1}}(h_{p})-_{ _{1}}^{*}(_{p})+_{_{1}}(_{p})+ (1+_{j=1}^{n_{e}}_{j})_{2}_{_{h_{p}}}(h_{d})-_{_{h_{p}}}^{*}( _{d})+_{_{h_{p}}}(_{d})}{_{j=1} ^{n_{e}}_{j}}.\]

_Furthermore, constant factors \((1+_{j=1}^{n_{e}}_{j})\) and \(^{n_{e}}_{j}}\) can be removed when \(_{2}\) is linear._

The proof is given in Appendix D. It consists of expressing the conditional regret of the deferral loss as the sum of two regrets, first by minimizing \(h_{d}\) for a fixed \(h_{p}\) and then by minimizing \(h_{p}\). Subsequently, we show how each regret can be upper-bounded in terms of the conditional regret of each stage's surrogate loss, leveraging the \(_{p}\)-consistency bound of \(_{1}\) and \(}_{d}\)-consistency bound of \(_{2}\) with respect to the zero-one loss. This, in conjunction with the concavity of functions \(_{1}\) and \(_{2}\), establishes our \(\)-consistency bounds.

Thus, the theorem provides a strong guarantee for the two-stage surrogate losses. A specific instance of Theorem 1 holds for the case where \(_{_{1}}^{*}(_{p})=_{_{1}}^{*}( _{})\) and \(_{_{h_{p}}}^{*}(_{d})=_{ _{h_{p}}}^{*}(_{})\), ensuring that the Bayes-error coincides with the best-in-class error and, consequently, \(_{_{1}}(_{p})=_{_{h_{p}}}( _{d})=0\). Given Theorem 1 and the non-negativity property of \(_{_{}}()\), we can derive the following corollary.

**Corollary 2**.: _Assume that \(\) satisfies the same assumption as in Theorem 1. Then, for all \(h\) and any distribution such that \(_{_{1}}^{*}(_{p})=_{_{1}}^{*}( _{})\) and \(_{_{h_{p}}}^{*}(_{d})=_{ _{h_{p}}}^{*}(_{})\), we have_

\[_{_{}}(h)-_{_{}}^{*}()_{1}_{_{1}}(h_{p})- _{_{1}}^{*}(_{p})+(1+_{j=1}^{n_{e}} _{j})_{2}_{_{h_{p} }}(h_{d})-_{_{h_{p}}}^{*}(_{d})}{_{j=1}^{n_ {e}}_{j}},\]

_where the constant factors \((1+_{j=1}^{n_{e}}_{j})\) and \(^{n_{e}}_{j}}\) can be removed when \(_{2}\) is linear._

Corollary 2 implies that when the estimation error of the first-stage surrogate loss, \(_{_{1}}(h_{p})-_{_{1}}^{*}(_{p})\), is reduced to \(_{1}\), and the estimation error of the second-stage surrogate loss, \(_{_{h_{p}}}(h_{d})-_{_{h_{p}}}^{*}( _{d})\), is reduced to \(_{2}\), the estimation error of the deferral loss, \(_{_{}}(h)-_{_{}}^{*}()\), is upper-bound by

\[_{1}(_{1})+(1+_{j=1}^{n_{e}}_{j}) _{2}}{_{j=1}^{n_{e}}_{j}} .\]

The common surrogate losses mentioned earlier all satisfy the first-stage requirement; however, it was unclear if they would meet the second-stage criterion since the \(}_{d}\)-consistency bound is for hypothesis sets \(}_{d}\) with a fixed first scoring function. This has not been previously studied in the literature. In the next section, we prove for the first time that common multi-class surrogate losses,

[MISSING_PAGE_FAIL:6]

In their proof, to set an upper bound on the estimation error of the zero-one loss using that of the surrogate loss, they select an auxiliary function \(_{}\) for any hypothesis \(h\). This function is contingent on the distinct scores of \(h\). Subsequently, the authors choose an optimal \(\) to set these bounds. Nevertheless, if any of \(h\)'s scores are fixed, an optimal \(\) does not exist, preventing the establishment of a meaningful bound. Instead, our new proof method overcomes this limitation by choosing \(_{}\) based on the softmax, as the softmax corresponding to the label zero can still vary due to the influence of changes in other scores, even when the scoring function on label zero is fixed.

### Realizable \(\)-consistency

Recently, Mozannar et al. (2023) showed that even in the straightforward single-expert setting, existing Bayes-consistent single-stage surrogate losses (Mozannar and Sontag, 2020, Verma and Nalisnick, 2022) are not _realizable \(\)-consistent_(Long and Servedio, 2013, Zhang and Agarwal, 2020) for learning with deferral. This can pose significant challenges when learning with a restricted hypothesis set \(\), even for simple linear models. Instead, they proposed a new surrogate loss that is realizable \(\)-consistent when \(\) is _closed under scaling_, meaning that it satisfies the condition \(h h\) for all \(\) in the set of real numbers. However, they stated that they could not prove or disprove whether their proposed surrogate loss is Bayes-consistent. Consequently, it has become crucial to identify a surrogate loss that is both consistent and realizable-consistent, which has remained an open problem.

**Definition 4** (**Realizable \(\)-consistency**).: _A surrogate loss \(\) is considered a realizable \(\)-consistent loss function for the deferral loss \(_{}\) if, for any distribution that is \(\)-realizable, that is, there exists a zero loss solution \(h^{*}\) with \(_{_{}}(h^{*})=0\), optimizing the surrogate loss results in obtaining the zero-error solution:_

\[_{}(h_{n})-_{}^{*}() 0_{_{}}(h_{n})-_{_{}}^{*}()0.\]

In the following result, we show that our two-stage surrogate losses are realizable \(\)-consistent. Combined with their Bayes-consistency properties, which have already been established in Section 3.2, we effectively find surrogate losses that are both Bayes-consistent and realizable consistent in the multi-expert setting, including the single-expert setting as a special case. For simplicity, here, we study the case where \(_{1}=_{2}=_{}\), a similar proof holds for other choices of \(_{1}\) and \(_{2}\) defined in Table 1. The proof is included in Appendix F.

**Theorem 5** (**Realizable \(\)-consistency for score-based two-stage surrogates**).: _Assume that \(\) is closed under scaling and \(c_{j}(x,y)=_{j},(x,y)\). Let \(_{1}\) and \(_{2}\) be the logistic loss. Let \(_{p}\) be the minimizer of \(_{_{1}}\) and \(_{d}\) be the minimizer of \(_{_{_{p}}}\) such that \(_{_{_{p}}}(_{d})=_{h}_{ _{h_{p}}}(h_{d})\). Then, the following equality holds for any (\(\), \(\)) -realizable distribution,_

\[_{_{}}()=0,\;\;=( _{p},_{d}).\]

Theorem 5 suggests that when the estimation error of the first-stage surrogate loss, \(_{_{1}}(h_{p}^{n})-_{_{1}}^{*}(_{p}) 0\), and the estimation error of the second-stage surrogate loss, \(_{_{h_{p}}}(h_{d}^{n})-_{_{h_{p}}}^{* }(_{d})0\), the estimation error of the deferral loss, \(_{_{}}(h^{n})-_{_{ }}^{*}()0\). This result demonstrates that our two-stage surrogate losses are not only Bayes-consistent, but also realizable \(\)-consistent when only the inference cost (\(_{j}\)) exists.

## 4 Predictor-rejector setting

The results of the previous sections were all given for the score-based setting. We note that another popular setting in learning with deferral/abstention is the _predictor-rejector setting_(Cortes et al., 2016, 2023), where the deferral corresponds to a separate function \(\) instead of extra scores. For completeness, we introduce this setting as well. Here too, we design a family of two-stage surrogate losses benefiting from both (\(,\))-consistency bounds and realizable consistency. For simplicity, we overload the notation as with score-based setting based on the context.

Let \(\) be a hypothesis set of prediction functions mapping from \(\) to \(\). The label predicted for \(x\) using a hypothesis \(h\) is denoted by \((x)\) and defined as one with the highest score,\(h(x)=*{argmax}_{y}h(x,y)\), with an arbitrary but fixed deterministic strategy for breaking ties. Let \(\) be a family of _deforming_ functions mapping from \(\) to \(^{n_{e}}\), where \(n_{e}\) is the number of experts. A deferral \(r=(r_{1},,r_{n_{e}})\) is used to defer the prediction on input \(x\) to the \(j\)th expert \(h_{j}\) if \(r_{j}(x) 0\) and \(r_{j}(x)<_{i=1,i j}r_{i}(x)\), in which case a cost \(c_{j}(x,y)=1-_{j}(x,y)[1-_{j},1-_{j}]\) is incurred with \(0<_{j}_{j} 1\). A natural choice of the cost is \(c_{j}(x,y)=_{j}_{h_{j}(x)*y}+_{j}\), where \(_{j},_{j}>0\) and \(h_{j}\) is the prediction of the \(j\)th expert. The \(_{j}\) in the second term corresponds to the inference cost incurred by expert \(h_{j}\). Let \(r_{0}=0\) and define \(r(x)=0\) if \(r_{0}(x)<_{j[n_{e}]}r_{j}(x)\); otherwise, \(r(x)=*{argmin}_{j[n_{e}]}r_{j}(x)\), with an arbitrary but fixed deterministic strategy for breaking ties. The _learning to defer loss_\(_{}\) with \(n_{e}\) experts is defined as follows for any \((h,r)\) and \((x,y)\):

\[_{}(h,r,x,y)=_{h(x)*y}_{r(x)=0}+ _{j=1}^{n_{e}}c_{j}(x,y)_{r(x)=j}. \]

Given a distribution \(\) over \(\), we will denote by \(_{_{}}(h,r)\) the expected deferral loss of a predictor \(h\) and a deferral \(r\), \(_{_{}}(h,r)=_{(x,y)} [_{}(h,r,x,y)]\), and by \(_{_{}}^{*}(,)=_{h ,r}_{_{}}(h,r)\) its infimum or best-in class expected loss. We will adopt similar definitions for other loss functions. We denote by \(_{}(,)=_{}^{* }(,)-_{x}_{h,r }_{y|x}[(h,r,x,y)]\) the minimizability gap for hypothesis sets (\(\),\(\)) and a loss function \(\).

Let \(_{1}\) be a surrogate loss for standard multi-class classification with \(n\) classes. We consider the following two-stage scenario: in the first stage, a predictor \(h\) is learned using the surrogate loss \(_{1}\); in the second stage, \(r\) is learned using a surrogate loss \(_{h}\) that depends on the prediction function \(h\) learned in the first stage.

To any \(r\), we associate a hypothesis \(\) defined over \((n_{e}+1)\) classes \(\{0,1,,n_{e}\}\) by \((x,0)=0\), that is zero scoring function, and \((x,j)=-r_{j}(x)\) for \(j[n_{e}]\). We can then define our suggested surrogate loss for the second stage:

\[_{h}(r,x,y)=_{h(x)*y}_{2}(,x,0)+_{j=1 }^{n_{e}}_{j}(x,y)_{2}(,x,j). \]

Here, \(_{2}(,x,j)\) is a surrogate loss for standard multi-class classification with \((n_{e}+1)\) categories \(\{0,1,,n_{e}\}\). Intuitively, the indicator term \(_{r(x)*j}\) in the deferral loss penalizes \(r_{j}(x)\) when it has a large value. However, a standard surrogate loss \(_{2}(,x,j)\) such as the logistic loss penalizes \((x,j)\) when it has a small value. This is why we use a negative sign in the definition of \(\) to maintain consistency between the definitions of \(_{h}\) and \(_{}\). In Table 3, we present a summary of examples of such second-stage surrogate losses, where \(_{2}\) is selected from common surrogate losses in standard multi-class classification defined in Table 1. A detailed derivation is presented in Appendix C.

From the point of view of the second stage, we will denote by \(}\) the family of hypotheses \(\{0,1,,n_{e}\}\) whose first scoring function, \((,0)\), is zero function and will not be learned in the second stage. We will provide strong guarantees for two-stage surrogate losses, provided that the first-stage loss function \(_{1}\) admits an \(\)-consistency bound, and the second-stage loss function \(_{2}\) admits an \(}\)-consistency bound.

**Theorem 6** ((\(,\))-consistency bounds for predictor-rejector two-stage surrogates).: _Assume that \(_{1}\) admits an \(\)-consistency bound and \(_{2}\) admits an \(}\)-consistency bound with respect to the multi-class zero-one classification loss \(_{0-1}\) respectively. Thus, there are non-decreasing concave

   \(_{2}\) & \(_{h_{p}}\) \\  \(_{}\) & \(_{h(x)*y}_{i=1}^{n_{e}}e^{-r_{i}(x)}+_{j=1}^{n_{e}}_ {j}(x,y)_{i=1,i j}^{n_{e}}e^{r_{j}(x)-r_{i}(x)}+e^{r_{j}(x)} \) \\ \(_{}\) & \(-_{h(x)*y}^{n_{e}}e^{-r_{i}(x)}} -_{j=1}^{n_{e}}_{j}(x,y)(x)}} {1+_{i=1}^{n_{e}}e^{-r_{i}(x)}}\) \\ \(_{}\) & \(_{h(x)*y}1- ^{n_{e}}e^{-r_{i}(x)}}^{}+_{j=1}^{n_{e}}_ {j}(x,y)1-(x)}}{1+_{i=1} ^{n_{e}}e^{-r_{i}(x)}}^{}\) \\ \(_{}\) & \(_{h(x)*y}1-^{n_{e}}e^{-r_{i}(x)}} +_{j=1}^{n_{e}}_{j}(x,y)1-(x)}}{1+ _{i=1}^{n_{e}}e^{-r_{i}(x)}}\) \\   

Table 3: Examples for predictor-rejector second-stage surrogate losses (5).

functions \(_{1}\) and \(_{2}\) such that, for all \(h\) and \(\), we have_

\[_{_{0-1}}(h)-_{_{0-1}}^{*}()+_{_{0-1}}()_{1}_{_ {1}}(h)-_{_{1}}^{*}()+_{_{1}}( )\] \[_{_{0-1}}()-_{_{0-1}}^{ *}(})+_{_{0-1}}(}) _{2}_{_{2}}()-_{_ {2}}^{*}(})+_{_{2}}(} ).\]

_Then, the following holds for all \(h\) and \(r\):_

\[_{_{}}(h,r)-_{ _{}}^{*}(,)+_{_{}}(,)\] \[_{1}_{_{1}}(h)-_{_ {1}}^{*}()+_{_{1}}()+(1+ _{j=1}^{n_{}}_{j})_{2}_{_{h}}(r)-_{_{h}}^{*}()+ _{_{h}}()}{_{j=1}^{n_{}} _{j}},\]

_where the constant factors \((1+_{j=1}^{n_{}}_{j})\) and \(^{n_{}}_{j}}\) can be removed when \(_{2}\) is linear._

As with the score-based setting, a specific instance of Theorem 6 holds for the case where \(_{_{1}}^{*}()=_{_{1}}^{*}(_{})\) and \(_{_{h}}^{*}()=_{_{h}}^{*} (_{})\), ensuring that the Bayes-error coincides with the best-in-class error and, consequently, \(_{_{1}}()=_{_{h}}()=0\). In these cases, when the estimation error of the first-stage surrogate loss, \(_{_{1}}(h)-_{_{1}}^{*}()\), is reduced to \(_{1}\), and the estimation error of the second-stage surrogate loss, \(_{_{h}}(r)-_{_{h}}^{*}()\), is reduced to \(_{2}\), the estimation error of the deferral loss, \(_{_{}}(h,r)-_{_{}}^{*}(,)\), is upper bounded by

\[_{1}(_{1})+(1+_{j=1}^{n_{}}_{j} )_{2}}{_{j=1}^{n_{}} _{j}}.\]

Next, we show that our two-stage surrogate losses are realizable \((,)\)-consistent. We say that the distribution is \((,)\)_-realizable_, if there exists a zero error solution \((h^{*},r^{*})\) with \(_{_{}}(h^{*},r^{*})=0\).

**Theorem 7** (**Realizable \((,)\)-consistency for predictor-rejector two-stage surrogates)**.: _Assume that \(\) and \(\) is closed under scaling and \(c_{j}(x,y)=_{j},(x,y)\). Let \(_{1}\) and \(_{2}\) be the logistic loss. Let \(\) be the minimizer of \(_{_{1}}\) and \(\) be the minimizer of \(_{_{}}\). Then, the following holds for any \((,)\) -realizable distribution,_

\[_{_{}}(,)=0.\]

The proof is included in Appendix H. Theorem 7 suggests that the two-stage surrogate loss is realizable consistent: when the estimation error of the first-stage surrogate loss \(_{_{1}}(h_{n})-_{_{1}}^{*}() {n}0\), and the estimation error of the second-stage surrogate loss \(_{_{h}}(r_{n})-_{_{h}}^{*}()0\), the estimation error of the deferral loss, \(_{_{}}(h_{n},r_{n})-_{_{ }}^{*}(,)0\). By Theorem 6 and Theorem 7, in the predictor-rejector setting, we also effectively find both Bayes-consistent and realizable consistent surrogate losses with multiple experts when only the inference cost \((_{j})\) exists.

Note that while Sections 3 and 4 both propose new two-stage algorithms based on \(\)-consistent surrogate losses, they differ in an important way. Section 3 learns with deferral in a score-based framework, where deferral is associated with extra scores. In contrast, Section 4 learns with deferral in a predictor-rejector setting, where deferral corresponds to a separate function. These represent two distinct learning frameworks that have been studied in the literature. Deriving consistent surrogate losses in the predictor-rejector setting has historically been challenging for traditional single-stage scenarios, leading many to opt for the score-based approach.

We should also highlight that our \(\)-consistency bounds in Theorems 1 and 6 can be used to derive finite sample estimation bounds for the minimizer of the surrogate loss over a hypothesis set \(\). This is achieved by upper bounding the estimation error of the minimizer of the surrogate loss using standard Rademacher complexity bounds (see ).

## 5 Experiments

In this section, we report the results of our experiments on CIFAR-10  and SVHN  datasets to test the effectiveness of our proposed algorithms for two-stagelearning to defer with multiple experts. We evaluated the overall accuracy of the learned pairs of predictor and deferral model across different scenarios involving varying the number of experts, where the predictor is pre-learned in the first stage and the deferral is subsequently learned using our proposed surrogate loss. We find that as the number of experts increases, the overall accuracy of the learned pairs also increases, in both scenarios with zero and non-zero base costs. This observation highlights the significance of using a multiple expert framework in our approach and the effectiveness of our surrogate loss within the framework.

We used ResNet architectures (He et al., 2016) for the prediction model, the deferral model and expert models. More precisely, we used ResNet-\(4\) for both the predictor and the deferral. We adopted three expert models: ResNet-10, ResNet-16, ResNet-\(28\) with increasing capacity. For training, we used the Adam optimizer (Kingma and Ba, 2014) with a batch size of \(128\) and weight decay \(1 10^{-4}\). Training was run for \(15\) epochs for SVHN and \(50\) epochs for CIFAR-10 with the default learning rate. No data augmentation was used in our experiments. We used our two-stage surrogate loss (3) with the logistic loss \(=_{}\) to train the deferral model ResNet-\(4\), with a pre-learned predictor ResNet-\(4\) trained using logistic loss. A check mark indicates the presence of a base cost in the cost function, whereas a cross mark signifies its absence. We first set the cost function to be \(_{h_{j}(x)=y}\) without a base cost. Next, for the experimental results shown in the last two row of Table 4, we chose base costs \(_{j}\) associated with each expert model as: \(0.1\), \(0.12\), \(0.14\) increasing with model capacity for SVHN and \(0.3\), \(0.32\), \(0.34\) increasing with model capacity for CIFAR-10. A base cost value that is close to the misclassification loss can strike a balance between improving accuracy and maintaining the ratio of deferral. We observed that other neighboring values lead to similar results. Note that the accuracy here refers to the overall accuracy of the learned pairs of predictor and deferral model. It is related to the deferral loss. Specifically, in the absence of the base cost, the accuracy aligns precisely with one minus the expected deferral loss. The results of Table 4 demonstrate the effectiveness of our proposed algorithms for two-stage learning to defer with multiple experts.

To the best of our knowledge, our study pioneers the exploration of a two-stage learning approach for deferral, a framework that is essential in numerous practical applications. Thus, we are unaware of any established baselines within this context.

It is important to underscore the differences between our learning scenario and those presented in (Okati et al., 2021; Narasimhan et al., 2022). While both of them involve two phases, their methodologies are considerably different from ours. Okati et al. (2021) required conditional probabilities paired with loss estimates from the expert--a component not available in our framework, as emphasized by Mozannar et al. (2023). On the other hand, Narasimhan et al. (2022) proposed a post-hoc correction for single-stage learning to defer surrogate losses. This approach, however, is not applicable to a pre-trained predictor from the standard multi-class classification. In contrast, our work focuses on enhancing the pre-trained predictor within the standard framework.

A limitation of our study is that the cost function used within the deferral loss is not fixed, and is typically determined through cross-validation in practice. There exists potential to introduce a principled method for selecting the cost function, which we have reserved for future research.

## 6 Conclusion

We introduced a novel family of surrogate loss functions and algorithms for a crucial two-stage learning to defer approach with multiple experts. We proved that these surrogate losses are supported by \(\)-consistency bounds and established their realizable \(\)-consistency properties for a constant cost function. This work paves the way for comparing different surrogate losses and cost functions within our framework. Further exploration, both theoretically and empirically, holds the potential to identify optimal choices for these quantities across diverse tasks.

   Dataset & Base cost & Base model & Single expert & Two experts & Three experts \\  SVHN & ✗ & 91.12 & 91.85 \(\) 0.01\% & 92.77 \(\) 0.02\% & 93.30 \(\) 0.02\% \\ CIFAR-10 & ✗ & 70.56 & 72.63 \(\) 0.20\% & 75.84 \(\) 0.35\% & 77.68 \(\) 0.07\% \\ SVHN & ✓ & 91.12 & 91.66 \(\) 0.01\% & 92.05 \(\) 0.10\% & 92.19 \(\) 0.03\% \\ CIFAR-10 & ✓ & 70.56 & 71.73 \(\) 0.06\% & 72.31 \(\) 0.31\% & 72.42 \(\) 0.12\% \\   

Table 4: Accuracy of deferral with multiple experts: mean ± standard deviation over three runs.