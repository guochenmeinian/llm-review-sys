# Open LLMs are Necessary for Current Private Adapations and Outperform their Closed Alternatives

Vincent Hanke, Tom Blanchard, Franziska Boenisch,

Iyiola E. Olatunji, Michael Backes, Adam Dziedzic

CISPA Helmholtz Center for Information Security

Correspondence to adam.dziedzic@cispa.de

###### Abstract

While open Large Language Models (LLMs) have made significant progress, they still fall short of matching the performance of their closed, proprietary counterparts, making the latter attractive even for the use on highly _private_ data. Recently, various new methods have been proposed to adapt closed LLMs to private data without leaking private information to third parties and/or the LLM provider. In this work, we analyze the privacy protection and performance of the four most recent methods for private adaptation of closed LLMs. By examining their threat models and thoroughly comparing their performance under different privacy levels according to differential privacy (DP), various LLM architectures, and multiple datasets for classification and generation tasks, we find that: (1) all the methods leak query data, i.e., the (potentially sensitive) user data that is queried at inference time, to the LLM provider, (2) three out of four methods also leak large fractions of private training data to the LLM provider while the method that protects private data requires a local open LLM, (3) all the methods exhibit lower performance compared to three private gradient-based adaptation methods for _local open LLMs_, and (4) the private adaptation methods for closed LLMs incur higher monetary training and query costs than running the alternative methods on local open LLMs. This yields the conclusion that, to achieve truly _privacy-preserving LLM adaptations_ that yield high performance and more privacy at lower costs, taking into account current methods and models, one should use open LLMs.

## 1 Introduction

Recently, there has been the trend of releasing open Large Language Models (LLMs), such as LLama , Vicuna , or Mistral  as an alternative to their proprietary closed counterparts, such as GPT from OpenAI , Claude from Anthropic , or Gemini from Google . Despite the significant progress in improving open LLMs, they are still outperformed in multiple tasks by closed LLMs , making the latter attractive even for learning tasks from highly _private_ data.

Since it was shown that private data can leak from the adaptations of LLMs , in the last few months alone, an array of new methods for privacy-preserving adaptation of closed LLMs has been proposed by the machine learning community at multiple conferences (NeurIPS'23  and ICLR'24 ). Given the lack of access to the closed LLMs parameters--which renders parameter-tuning based adaptations infeasible--they all rely on the generation of privacy-preserving discrete prompts. We detail their operational setup in Figure 1 (left).

In this work, we ask the simple yet impactful question of whether these efforts actually lead into the right direction towards the goal of achieving _truly privacy-preserving LLM adaptations_. Therefore, we thoroughly analyze the proposed methods both conceptually and empirically and compare them toalternatives that rely on privately adapting _open local LLMs_. In particular, we study each approach's threat space, assumptions, and methodological limitations and perform extensive experiments using ten state-of-the-art open and closed LLMs of various sizes, including Vicuna, Llama 3, Open LLMa, BERT, RoBERTa, the Pythia suite of models, Claude, two versions of GPT3 (Babbage and Davinci), and GPT4 Turbo --applied to multiple datasets both for classification and generation tasks. Our analyses cover the axes of privacy protection, performance in terms of privacy-utility trade-offs, and monetary costs for training and queries.

Our results provide the following insights: (1) All current methods for adapting closed LLMs leak private query data (intended for the data owner) at inference time to the LLM provider. (2) Three out of the four methods studied also leak large fractions of the private training data to the LLM provider. The approaches that do not, require an additional locally deployed open LLM for prompt engineering. (3) All methods for closed LLMs yield lower final downstream performance than privacy-preserving local adaptations on open LLMs--even when the local methods rely on significantly smaller LLMs than their closed counterparts. (4) The training and query costs of the private adaptations of closed LLMs (API access costs imposed by the LLM provider) are significantly higher than the costs for private open LLM adaptations (estimated as the costs of training and querying on cloud-based hardware). We provide a condensed summary of our results in Figure 1 (right Table above), and Table 1.

Overall, our results indicate that, from the perspective of effective privacy-preservation, current adaptations of open LLMs are strictly preferable over their closed LLM alternatives, since they are more private, more performant, and less expensive. Going beyond the concrete existing methods studied , we then analyze the reasons behind the underwhelming results of privacy-preserving closed LLM adaptations and discuss potential directions for improvements.

On the way, to further strengthen private adaptations for open LLMs, we demonstrate how to locally apply privacy-preserving prompt-based methods to train generation tasks with high-performance--claimed impossible by prior work . In particular, we show for the first time that private prompt tuning for text generation tasks PromptDPSGDGen can achieve comparable performance to private (full) fine-tuning and private low-rank adaptations (LoRA). Additionally, we demonstrate that ensemble-based few-shot prompts PromptPATEGen can privately generate high-quality text at a low privacy cost.

In summary, we make the following contributions:

Figure 1: **Setup for Privacy Protection with Open vs Closed LLMs**. The three parties involved are (1) an LLM provider who hosts the proprietary LLM, (2) a data curator, such as a company that curated private data, for example, of their customers’ previous transactions, and (3) a querying party, _i.e.,_ a customer of the company who wants to perform a new private transaction. There are three steps where privacy leaks: 1. During the creation of the discrete prompt, the data curator’s private data leaks to the LLM provider. 2. The private query of the querying party leaks to the LLM provider. 1. Private information from the data curator leaks to the querying party through the returned answers of the prompted LLM . Prior methods for closed LLMs  only provide protection against 1. None of them protects against 1. To prevent leakage through 1. they require access to a (powerful) local open LLM. As an alternative (dashed purple lines), the data curator could privately adapt the open LLM locally and let the querying party interact with this LLM, protecting against 1.

1. We perform a thorough conceptual and experimental study on existing privacy-preserving closed and open LLM adaptations, analyzing their threat space, assumptions, and achieved results.
2. Our extensive experiments on various open and closed LLMs and on multiple classification and generation tasks show that the local (gradient-based) adaptations outperform their current closed (discrete prompt-based) counterparts in terms of privacy, performance, and cost efficiency.
3. We propose differentially private prompts for text generation tasks that, for the first time, reach performance comparable to private LoRA or private fine-tuning.

## 2 Background and Related Work

**Differential Privacy.** Differential Privacy (DP)  is a mathematical framework that provides privacy guarantees by implementing the intuition that an algorithm \(:I R\), executed on two neighboring datasets \(D\), \(D^{}\) that differ in only one data point (we adopted the definition of _neighboring_ based on addition/removal. [35; 44]), will yield approximately the same output, _i.e._, \([(D) R] e^{}[(D^{}) R]+\). While \(\) specifies by how much the output can differ, \(\) specifies the probability of failure. There are two prevalent DP algorithms for training machine learning models. The first one is the differential private stochastic gradient descent algorithm (**DPSGD**)  where the impact of each private training data point is limited during training through gradient clipping, and privacy guarantees are integrated through the addition of calibrated amounts of stochastic noise. The second algorithm is the private aggregation of teacher ensembles (**PATE**)  where first, an ensemble of teacher models is trained on disjoint subsets of the private data, and then a noisy knowledge distillation is performed to a student model using public data. Another general mechanism for implementing DP is the exponential mechanism (**EM**) . The EM selects an output \(r\) from a set of possible outputs based on a scoring function \(q(D,r)\) that measures the quality of \(r\) for dataset \(D\). Let \( q\) be the sensitivity of the scoring function. The EM chooses \(r\) with probability proportional to \(()\).

**LLM Adaptations.** LLMs are pre-trained on large amounts of public data and then adapted to downstream tasks using private data . We divide existing methods for private LLM adaptations into _private tuning_ methods that rely on access to the LLM gradients, and _private in-context learning_ (ICL) which requires only API (black-box) access to the LLM. While private tuning is only applicable to open LLMs, private ICL can, in principle, be applied to both open and closed LLMs. We note that all private LLM adaptations rely in their core on the three DP algorithms introduced above and summarize existing methods, their setup, and their assumptions in Table 2.

**Private Tuning for Open LLMs.** There exist three main ways for private tuning. **1) Prompt-based adaptations** adds a small number of parameters (usually <1% of the total number of parameters)

  
**Adaptation** & **LLM Type** & **Model** & **Task** & **Reveals** & **Performance\(\)** & **Train(\$)** & **Query(\$)** \\  DP-ICL  & Closed & GPT4 Turbo & SST2 & \(_{T}\)+\(Q\) & Acc=\(95.9_{ 0.1}\%\) & 0 & 138.00 \\ PrivateLoRA  & Open & Llama3-8B(instruct) & SST2 & _None_ & Acc=\(96.0_{ 0.1}\%\) & 27.60 & 0.78 \\  DP-ICL  & Closed & GPT3 Davinci & SAMSum & \(_{T}\)+\(Q\) & RougeL=\(31.8_{ 0.3}\) & 0 & 665.91 \\ PrivateLoRA  & Open & BART-Large & SAMSum & _None_ & RougeL=\(39.1_{ 0.2}\) & 3.63 & 0.80 \\   

Table 1: **Comparison of privacy protection, performance, and cost between private adaptations for closed vs open LLMs.** We select the top-performing adaptations. For closed LLMs, we use DP-ICL  and leverage PrivateLoRA  on open LLMs for both tasks. We consider sentiment classification on SST2 and the dialog summarization on SAMSum. The training data is denoted by \(_{T}\) and the test queries by \(Q\). _Reveals_ represents which data are exposed to the LLM provider. The methods were trained with DP guarantees: \(=8\) and \(=1/N\), where \(N\) is the number of examples in \(_{T}\). We report the _Performance_ (higher is better) on test data (where _Acc_ denotes the classification accuracy). The cost (in $) is computed separately for training (Train) and for answering 10k test queries (Query). Note, the (estimated) number of parameters for closed LLMs is 1.76T for GPT4 Turbo and 175B for GPT3 Davinci, while Llama3 has only 8B and BART-Large is significantly smaller with 355M parameters. The adaptation of the open LLMs is more expensive on SST2 than on SAMSum due to the larger training data size for SST2 and a larger model. DP-ICL’s query cost is high due to the usage of an ensemble of 100 prompts to answer each query. _In summary, open local LLM adaptations are more private, more performant, and less expensive._only in the model input space, either on the level of token embeddings (soft prompts ), or also to every LLM layer (prefix-tuning ). Duan et al.  presented **PromptDPSGD**, which adapts the DPSGD algorithm to soft prompts. The main advantage of prompt-based adaptations is that they enable multi-task batch processing, _i.e._, many soft prompts for different users and tasks can be processed in the same mini-batch during LLM training or inference. **2) Parameter efficient fine-tuning-based adaptations** such as LoRA  add a relatively small number of parameters (<10% of total number of parameters) within the model, usually in each block of a transformer architecture . These added parameters are then tuned while the pre-trained original parameters remain frozen. **PrivateLoRA** extends LoRA with DP guarantees by building on the DPSGD algorithm. **3) Full fine-tuning-based adaptations** either fine-tune the whole model or only a few last layers. The **DP-FineTune**, again based on the DPSGD algorithm, shows that full fine-tuning with DP optimization can provide strong privacy guarantees and good performance. The general trend, when choosing an adequate method, suggests that the more difficult the task, the higher the number of adaptation parameters required . Thus, for simple downstream tasks, PromptDPSGD  is sufficient, while DP-LoRA  is recommended for medium-difficulty tasks, and the full fine-tuning  for complex tasks.

**Private ICL for Closed LLMs.** Recently, many new methods were proposed for private in-context learning with closed LLMs. All of them leverage discrete (hard) prompts and rely on a voting mechanism for privacy protection, similar to PATE  and CaPC . We divide the existing methods into the following four categories: **(1) Private Question Answering:** The work on **DP-ICL** proposed to answer queries based on the private dataset. Following the PATE setup, the private data is divided into non-overlapping partitions and then each partition is prepended with an instruction to form a private teacher prompt. The prompts form an ensemble of private teachers (prompted LLMs). Since DP-ICL does not implement the idea of a student model from PATE, all the teachers (usually 100) are required to answer each query, rendering the method expensive when executed on a closed LLM. Moreover, each query incurs additional privacy cost, such that the method can answer only a limited number of queries for a given privacy budget. **(2) Private Student Prompt: PromptPATE** tackles the problem of the high costs and the limited number of answered queries in DP-ICL by creating a student prompt. PromptPATE uses an ensemble of teacher prompts (usually around 200) to label public data. Then it selects the most performant shots for the student prompt from these newly labeled examples. **(3) Private Prompt Generation: DP-FewShotGen** is similar to PromptPATE but eschews the assumption about the public data for labeling and, in turn, starting from a public label, generates each output token privately to obtain a private shot. **(4) Private Prompt Engineering:** Finally, **DP-OPT** privatizes prompt engineering based on the Deep Language Network (DLN) method . While DP-ICL, PromptPATE, and DP-FewShotGen assume a generic instruction and emphasize the protection of the direct leakage from the shots only, DP-OPT  proposed to privately generate shots and instructions since either can leak information about the private training set. To overcome the problem that PATE-based approaches face with large output spaces (here equal to the vocabulary size of around 50k), DP-ICL  and DP-OPT  incorporate the EM and its improved versions  to privately release a token with the maximum count based on the voting from teacher prompts.

  
**Adapation Property** & **Privacy** & **Optimization** & **Privatize** & **Inference** & **Require** \\  PromptDPSGD  & DPSGD & Gradient-based & Soft Prompt/Prefix & Multi-task & Open LLM \\ PrivateLoRA  & DPSGD & Gradient-based & Added parameters & Single-task & Open LLM \\ DP-FineTune  & DPSGD & Gradient-based & all LLM parameters & Single-task & Open LLM \\  DP-ICL  & RNM,GML/ELM,TPR,MLPATE & ICL & Answers & Limited Queries & None \\ PromptPATE  & PATE & ICL & Shots & Multi-task & Public Data \\ DP-FewShotGen  & GM,RNMLEM & ICL & Shots & Multi-task & Public Labels,Open LLM \\ DP-OPT  & SAA,LDA & ICL & Instructions+Shots & Multi-task & Validation Data,Open LLM \\   

Table 2: **Comparison of properties between private LLM adaptations. The in-context learning (ICL) optimizes instructions and shots (demonstrations). Many privacy techniques include the ones designed for multi-label PATE (denoted as MLPATE) , exponential mechanism (EM) , joint exponential mechanism (JEM) , Gaussian Mechanism (GM), Report-Noisy-Max Mechanism (RNM), Propose-Test-Release (PTR) , sample-and-aggregate (SAA) , Limited Domain Algorithm (LDA) .**Prompt-based Private Adaptations for Text Generation

While PromptDPSGD and PromptPATE  were designed for classification tasks only, we further extend them to text generation tasks. Having prompt-based generation holds the advantage that, in contrast to fine-tuning based approaches, they support mixed-task inference [30; 33; 37], _i.e.,_ they require one frozen model for multiple tasks rather than a separate model copy for each of them. This reduces storage and offers greater flexibility and efficiency.

PromptDPSGDGen.We observe that an adequate choice of hyperparameters is sufficient for adjusting PromptDPSGD  to generation tasks. This is in line with prior work highlighting that the challenge of prompt tuning is that it requires experimenting with various hyperparameter choices to achieve good performance . In particular, we observe that increasing the number of parameters in the soft prompt from 0.1% of the total LLM parameters, as done for classification , to 10% of total model parameters, by enabling prefix projection, yields a significant increase in generation performance. Additionally, we observe the need for an increased learning rate, compared to other tuning methods, to generate more precise outputs. Otherwise, the hyperparameters are dependent on the data the model is trained on.

PromptPATEGen.Adjusting PromptPATE  to generation tasks (where more than one output token is generated) is challenging due to 1) the large output space (equivalent to the number of tokens in the vocabulary) and 2) the privacy costs incurred by generating multiple tokens through the teacher ensemble. To overcome this challenge and support generation tasks with an unlimited number of queries, we extended PromptPATE by combining the training of the student prompt from  with the privacy techniques used in  and call the result PromptPATEGen. In particular, PromptPATEGen uses the private generation in DP-ICL to obtain longer output sequences for some public data inputs. The outputs sequences can then be treated as a "label" for the public data and can be deployed as a form of student prompt, just like in PromptPATE .

## 4 Comparing Open and Closed LLM Adaptations

We perform a thorough conceptual and empirical study to compare the adaptation of both open LLMs with private tuning (PromptDPSGD , PrivateLoRA , and DP-FineTune ) and closed LLMs with private ICL (DP-ICL , PromptPATE , DP-FewShotGen , and DP-OPT ). Our comparison spans the axes of privacy protection, performance, and cost.

### Comparing Privacy Protection

All the considered methods offer privacy guarantees according to DP. Thereby, they ensure that the final prompted LLM's predictions will not leak more than the specified tolerated privacy budget \(\) to any party who queries the LLM or gets access to the final private prompt. Yet, the threat model of multiple private ICL methods for closed LLMs does not include providing privacy against the LLM provider. Those methods that do might still occasionally experience leakage. We analyze the result of this lack of consideration for the goal of truly privacy-preserving LLM adaptations. In our analysis, we distinguish between the leakage of private training data and the leakage of test data queried at inference time, which might also be sensitive.

**Private Training Data.** PromptPATE , DP-ICL , and DP-FewShotGen  (without using an open LLM) disclose (large parts of) their private training set to the LLM provider in the form of shots in their teacher prompts and their engineering. This leakage is inherent in their design. To avoid such leakage, DP-OPT  tunes the prompt locally with DP guarantees and then exposes it to the LLM provider. Thereby, the data that the prompt was generated from is protected towards the LLM provider with the DP guarantees that also protect against leakage to a querying party. While the experimental evaluation in  suggests that at higher \(\), the locally generated DP prompts might still contain generated data close to the private training data, this is a step towards the right direction. However, to generate the private prompt, DP-OPT  requires a powerful open LLM deployed locally. Looking at Figure 1, it becomes obvious that any private tuning method executed on that open LLM would, conceptually, improve privacy protection since the LLM provider would neither be involved in the adaptation nor in the use of the adapted LLM, yielding absolute privacy against them.

**Private Query Data.** DP does not aim at protecting query data. Hence, none of the studied private ICL methods attempt to protect that data against the LLM provider. While the protection of query data is often considered as an orthogonal research direction, we note that all the private tuning-based adaptations of the open local LLMs do naturally prevent leakage of the query data to the LLM provider. This is because the querying party directly interacts with the data owner (see Figure 1)--making the use of open models inherently more suited for truly privacy-preserving application than relying on closed models.

### Comparing Performance

We look at privacy-utility trade-offs to compare the performance of private tuning on open LLMs vs. private ICL on their closed counterparts. We depart from analyzing the trade-offs and required assumptions conceptually and then present our thorough experimental evaluation.

**Private Tuning Outperforms Private ICL Conceptually.** Previous work  has shown for the non-private settings that gradient based tuning methods (used for open LLMs) offer better accuracy and significanly lower computational costs than ICL (used for closed LLMs) since the adaptations can leverage the internal behavior of the LLM. This benefit holds also in the privacy regime. Moreover, the tuning based methods do not make additional assumptions, such as the availability of public data (required by PATE-based methods, such as PromptPATE ), making them inherently more practical.

**Private Tuning outperforms Private ICL Experimentally.** To assess the performance of private tuning vs. private ICL, we perform extensive experimental evaluation. We use various LLM architectures and multiple datasets for classification and text generation tasks.

#### 4.2.1 Experimental Setup

**Text Classification.** We follow the setup from  and use four datasets for the evaluation: SST2 from the GLUE benchmark , Trec , Mpqa  and Disaster . SST2 and Mpqa are two-class sentiment analysis datasets. SST2 includes 67.3k training samples and 872 test samples, while Mpqa contains 8.6k training samples and 2k test samples. Trec is a six-class question-type classification dataset with 5.4k training samples and 500 test samples. Finally, the Disaster dataset involves determining whether a sentence is relevant to a disaster scenario or not and includes 4.4k training and 1000 test samples.

**Text Generation.** We use three different datasets: SAMSum, a dialog summarization  (14732 train, 818 val, and 819 test samples), PFL-DocVQA, question answering  (85k train and 10k test samples), and MIT Movies trivia10k13, movie extraction on directors (MIT-D with 1561 train and 415 test samples) and genre (MIT-G with 2953 train and 780 test samples) .

**Closed Models.** We follow the setup and choice of models originally proposed in the respective previous papers to evaluate the four private ICL methods for closed LLMs . The GPT3-Babbage and GPT3-Davinci models cited in  were discontinued in early 20242 and replaced by their second versions (babbage-002 and davinci-002). Therefore, we use the newer versions here. The (estimated) number of parameters for the closed models is: 1.3B for GPT3 Babbage, 175B for GPT3 Davinci, 1.76T for GPT4 Turbo, and 200B for Claude 2.1.

**Open Models.** We consider various open LLMs with differing pre-training sets and numbers of parameters to simulate the choices a data owner can make for their local LLM. We select the following models: Pythia , OpenLLaMA , Vicuna , Mixtral , Bart , and RoBERTA , whose sizes vary from 160M to 45B parameters.

#### 4.2.2 Performance of Private Adaptations for Classification

We show that the private adaptations on local open LLMs outperform the private methods for closed LLMs for classification tasks. In Table 3, we analyze the performance differences. We follow the evaluation in  (Table 2) and average the accuracy across the tasks (denoted as Average). Our analysis follows the standard practice and sets the privacy budget as \(=8\) and \(=1/|D|\) where \(|D|\) is the training size . Among the methods for closed LLMs, DP-OPT was tested onthe strongest Davinci model (with 175B parameters) from the GPT3 family. Across all the tasks, DP-OPT is outperformed by both DP-FineTune and PrivateLoRA by a large margin (even >26% absolute on Trec), even though DP-FineTune and PrivateLoRA were trained on RoBERTa Large with only 355M parameters (500X fewer than for GPT3 Davinci). Furthermore, we show that PrivateLoRA outperforms DP-OPT even when using Pythia-6.9B, which guarantees that the open LLM for PrivateLoRA was not pre-trained on any of the downstream datasets. For a fair comparison, we also train PrivateLoRA on Vicuna 7B, which was used in DP-OPT as the local model to find the transferable prompts and show that PrivateLoRA is also significantly better than DP-OPT applied either directly to Vicuna 7B or when run on GPT3 Davinci. This suggests that the data owners, rather than using their local LLM to tune prompts for DP-OPT, should privately tune it with PrivateLoRA (in this case on RoBERTa Large) since it yields stronger performance and privacy at a lower cost.

For PromptPATE, the performance plateaus after around \(=0.3\), since it creates a public prompt using only a few shots, and the selection of the demonstrations from a large pool of publicly labeled examples has a negligible gain on the final performance. In the limit, we also show that PromptPATE even with an infinite privacy budget (\(=\)) for GPT3 Babbage (with 1.3B parameters) performs worse than PrivateLoRA or DP-FineTune on RoBERTa Large (3.6X fewer parameters). In the same setup of models, PrivateLoRA and DP-FineTune on RoBERTa Large also outperform DP-ICL tested on GPT3 Babbage on all tasks. Additionally, PrivateLoRA adapted on Pythia-160M (with even fewer parameters) performs much better than DP-FewShotGen on GPT3-Babbage (8X more parameters).

We also run DP-ICL with GPT4 Turbo. The resulting accuracies are high for sentiment classification with SST-2 and Mpqa. However, it has the lowest accuracy on Trec (with 6 classes), caused by a small number of output probability tokens released for a query (only 20 vs 100 for GPT3, which might not contain the correct class label token) while being the most expensive option. Similar trends are observed for PromptPATE on Claude, however, it has more consistent performance and emerges as the most performant closed model on the tested tasks (while being the 2nd most expensive one). In contrast, Private LoRA with Vicuna 7B performs the best on Trec and on _average_. It is the best of all tested adaptations while incurring around 3.7 and 9.5 times lower costs than Claude and GPT4 Turbo, respectively. In general, the open models have the highest average performance at a much lower cost.

We further analyze the privacy-utility trade-off for classification tasks across different privacy budgets (\(\)) in Figure 2. We show that even under tight privacy constraints (\(<1.0\)), the privacy-preserving adaptation for open LLMs performs significantly better than the one for closed LLMs. Specifically, we analyze the differences between PrivateLoRA for open LLMs vs PromptPATE for closed LLMs. The performance for PromptPATE plateaus after around \(=0.3\) and only for one out of four datasets, namely for MPQA, we observe that the crossover point between PromptPATE and PrivateLoRA (PromptPATE performs better than PrivateLoRA until \(=0.6\)). For the smallest \(=0.1\) values that we analyzed, the performance of PrivateLoRA is better by 0.6% on SST2, by

  
**Method** & **LLM Type** & **Model** & **SST2** & **Trec** & **Mpqa** & **Disaster** & **Average** & **T(S)** & **Q(S)** & **All(S)** \\ 
0.4br-0 (\(=0\))  & Closed & GPT3 Davinci & \(92.4_{+0.0}\) & \(51.8_{+0.2}\) & \(84.5_{+0.1}\) & \(76.4_{-0.2}\) & \(76.3\) & 0 & 6.00 & 6.00 \\ DP-OPT (original)  & Closed & GPT3 Davinci & \(92.2_{-0.8}\) & \(67.8_{-6.5}\) & \(85.8_{-0.7}\) & \(78.9_{-0.3}\) & \(81.4\) & 2.10 & 6.00 & 8.10 \\ _ICL_ (\(=\)/\(\))  & Closed & GPT3 Davinci & \(94.7_{-0.4}\) & \(79.1_{-0.5}\) & \(88.8_{-0.1}\) & \(69.6_{-0.5}\) & \(82.9\) & 0 & 6.00 & 6.00 \\ PromptPATE (\(15_{}\)/\(\)) & Closed & GPT3 Babbage & \(93.8\) & \(56.7\) & \(83.0\) & \(64.3\) & \(75.0\) & \(8.66\) & 1.72 & 10.38 \\ PromptPATE (\(15_{}\)/\(\)) & Closed & GPT3 Babbage & \(88.8_{-4.2}\) & \(52.8_{-1.5}\) & \(79.0_{-0.5}\) & \(58.0_{-0.5}\) & \(69.6\) & 9.72 & 1.72 & 11.44 \\ PromptPATE (\(15_{}\)/\(\)) & Closed & Cimte 2.1 & \(95.7_{-1.4}\) & \(79.3_{-1.2}\) & \(92.1_{-0.6}\) & \(71.0_{-0.8}\) & \(84.58_{-4.8}\) & 4.824 & 5.36 & 5.36 \\ DP-FewShotGen(1)  & Closed & GPT3 Babbage & \(92.8_{-2.7}\) & \(57.4_{-3.5}\) & \(73.4_{-3.5}\) & \(59.2_{-2.5}\) & \(64.2\) & 0.86 & 1.10 & 1.96 \\ DP-ICL,  & Closed & GPT3 Babbage & \(92.8_{-0.9}\) & \(26.3_{-5.6}\) & \(80.6_{-0.9}\) & \(50.6_{-1.1}\) & 62.6 & 0 & 17.2 & 17.2 \\ DP-ICL,  & Closed & GPT4 Turbo & \(95.9_{-0.1}\) & \(16.2_{-1.7}\) & \(90.4_{-0.1}\) & \(70.3_{-0.4}\) & 68.2 & 0 & 138.00 & 138.00 \\  PromptPPSPO  & Open & RoBERTa Large & \(92.3_{-0.5}\) & \(54.5_{-2.5}\) & \(50.0_{-0.7}\) & \(78.7_{-6.6}\) & \(86.8\) & 7.59 & 0.40 & 7.99 \\ DP-FineTune  & Open & RoBERTa Large & \(93.5_{-0.3}\) & \(93.7_{-0.8}\) & \(88.2_{-0.4}\) & \(82.8_{-0.3}\) & \(89.4\) & 5.75 & 0.40 & 6.15 \\ PrivateLoRA  & Open & RoBERTa Large & \(93.6_{-0.3}\) & \(93.9_{-0.6}\) & \(87.7_{-0.8}\) & \(81.8_{-0.2}\) & \(89.3\) & 3.45 & 0.40 & 3.85 \\  PrivateLoRA  & Open & Vicona 7B & \(94.8_{-0.5}\) & \(_{-0.1}\) & \(87.8_{-0.5}\) & \(81.3_{-0.9}\) & \(90.3\) & \(13.80\) & 0.78 & 14.58 \\ DP-OPT (pool)  & Open & Vicona 7B & \(95.2_{-2.6}\) & \(96.5_{-3.4}\) & \(80.7_{-3.3}\) & \(65.6_{-0.3}\) & \(75.3\) & \(2.10\) & 0.78 & 2.88 \\  PrivateLoRA  & Open & Pythia 6.9B & \(92.2_{-0.5}\) & \(96.3_{-0.8}\) & \(87.2_{-0.3}\) & \(82.1_{-0.2}\) & \(89.4\) & 13.80 & 0.78 & 14.58 \\ PrivateLoRA  & Open & Pythia 160M & \(80.4_{-0.7}\) & \(82.5_{-3.2}\) & \(77.9_{-0.3}\) & \(73.6_{-0.2}\) & \(78.6\) & 1.60 & 0.50 & 2.1 \\  PrivateLoRA  & Open & Llamas-38B(Instruct) & \(}\) & \(96.8_{-0.2}\) & \(87.3_{-0.2}\) & \(80.8_{-0.1}\) & \(90.2\) & 27.60 & 0.78 & 28.38 \\   

Table 3: **Private local adaptations on open LLMs outperform their closed alternatives for classification tasks. The default privacy budget is set to \(=8\), except for PromptPATE , where the performance plateaus after \(=0.3\). The best result for a given task is bolded, and the 2nd best is underlined. T(S) is training cost while Q(S) is query cost for 10k queries (SST2), All(S) is total cost.**4.4% on Trec, and by 3.5% on Disaster. Overall, the private features for open LLMs outperform the ones for closed LLMs in most privacy regimes.

#### 4.2.3 Performance of Private Adaptations for Text Generation

The evaluation of the three text generation tasks demonstrates superior performance of private adaptations on open vs closed LLMs. We consider the privacy-preserving ICL methods of DP-ICL and DP-FewShotGen on closed LLMs, since only these methods were executed for generative tasks. For the SAMSum datasets in Table 4, the first three adaptations (including our PromptPATEGen) are based on few-shot in-context learning (using discrete prompts), while the remaining results are for the private gradient-based adaptations. For the discrete prompts, our PromptPATEGen runs on local open Vicuna 7B and outperforms other discrete prompt-based methods from closed LLMs. Our PromptDPSGDGen performs on par with the other private tuning method (PrivateLoRA) run on Pythia 1B. Note that only PromptDPSGDGen and ICL adaptations (PromptPATEGen and DP-ICL) support multi-task inference.

  
**Method** & **LLM Type** & **Model** & **MTP-D** & **MIT-G** & **T(S)** & **Q(S)** & **All93** \\  DP-FewShotGen  & Closed & GPT3 Driven & 80.6 & 64.1 & 0.42 & 2.36 & 2.78 \\ 
**PromptPATEGen** & Open & Vicona 7B & 74.1 & 0.64 & 41.7 & 0.62 & 0.73 & 1.25 \\
**PromptDPSGDGen** & Open & OptimalLM & 70.9 & 53.4 & 33.1 & 31.00 & 8.91 \\ 
**PromptDPSGDGen** & Open & Pythia 10M & 74.3 & 8.63 & 64.3 & 64.3 & 60.60 & 0.56 \\
**PromptDPSGDGen** & Open & Pythia 1B & 89.5 & 20.3 & 69.1 & -1.7 & 0.17 & 0.25 \\
**PromptDPSGDGen** & Open & Pythia 1B & 92.2 & 1.71 & 71.61 & 1.94 & 0.50 & 1.44 \\
**PrivateLoRA**  & Open & Pythia 1B & 90.2 & 10.8 & 68.8 & 0.48 & 0.08 & 0.31 \\  PrivateLoRA  & Open & Vicona 7B & **95.0** & 12.7 & 74.4 & 12.2 & 52.52 & 59.64 \\
**PrivateLoRA**  & Open & Open & DeLaMA 13B & 94.0 & 0.08 & **76.4** & 4.0 & 1.04 & 6.21 & 7.25 \\
**PrivateLoRA**  & Open & Mixtral 8x7B & \(93.0\) & 69.7 & 1.52 & 9.47 & 10.99 \\   

Table 6: **Evaluation on information extraction with MIT-D and MIT-G for \(=8\).**

  
**Method** & **LLM Type** & **Model** & **Ronge-1** & **Ronge-2** & **Ronge-L** & **T(S)** & **Q(S)** & **All93** \\  DP-ICL  & Closed & GPT3 Driven & 41.2 & 0.66 & 13.4 & 0.31 & 8.03 & 0 & 665.91 & 665.91 \\ DP-ICL  & Closed & GPT3 Plus & 42.6 & 0.26 & 18.9 & 33.8 & 0.5 & 0 & 4916.46 & 49.16 \\ DP-ICL  & Closed & GPT4 Turbo & 41.8 & 0.22 & 17.3 & 33.4 & 0.24 & 0.349.24 & 3419.42 \\
**PromptDPSGDGen** & Open & Vicuna 7B & 41.3 & 0.38 & 18.0 & 42.8 & 0.32 & 3.29 & 2.74 & 6.03 \\
**PromptDPSGDGen** & Open & Open/LoMA13B & 43.3 & 19.7 & 74.0 & 34.2 & 18.63 & 0.80 & 19.43 \\
**PromptDPSGDGen** & Open & BARTLarge & 46.1 & 1.4 & 21.3 & 0.1 & 37.4 & 0.40 & 1.73 & 0.40 & 2.13 \\ PrivateLoRA  & Open & BATler & 48.8 & 0.26 & 23.5 & 39.1 & 20.2 & 2.90 & 0.69 & 3.59 \\ PrivateLoRA  & Open & Pruja-Att & 40.4 & 0.4 & 16.6 & 30.3 & 34.0 & 3.45 & 1.34 & 4.79 \\ 
**PromptDPSGDGen** & Open & Pythia 1B & 41.2 & 0.7 & 18.0 & 33.7 & 0.41 & 4.83 & 0.95 & 5.78 \\ DP-FineTue  & Open & Pruja 1B & 42.5 & 0.7 & 18.4 & 0.33 & 39.4 & 0.84 & 1.08 & 10.92 \\ PrivateLoRA  & Open & Pythia 1B & 42.3 & 0.6 & 18.4 & 0.3 & 39.4 & 0.34 & 1.08 & 10.92 \\ PrivateLoRA  & Open & Pythia 1B & 42.3 & 0.6 & 18.4 & 0.3 & 37.4 & 0.5 & 10.81 & 6.57 & 16.57 \\ PrivateLoRA  & Open & Viuna 7B & 48.6 & 0.3 & 24.8 & 46.2 & 40.3 & 1.12 & 6.19 & 17.47 \\ PrivateLoRA  & Open & Open/LoMA 13B & 48.5 & 1.1 & 24.2 & 0.8 & 40.1 & 0.9 & 19.46 & 8.05 & 27.51 \\ PrivateLoRA  & Open & Mixtral 8x7B & \( 0.4\) & \( 0.2\) & \( 0.2\) & 57.96 & 9.99 & 67.95 \\   

Table 4: **Evaluation on Dialog Summarization with SAMSum for \(=8\). T(S)** is training cost while Q(S) is query cost for 10k queries, All(S) is total cost.**

Figure 2: **Privacy-utility trade-off for classifications tasks. We use PrivateLoRA to adapt Vicuna-7b to the downstream tasks, PromptPATE, DP-ICL, and DP-FewShotGen with GPT3 Babbage. We analyze the privacy costs \(\) in the range \(\) (see corresponding Figure 3 for text generation tasks).**

We additionally leverage BART-Large (with 355M parameters)  that was fine-tuned on the XSum summarization task  (which does not include SAMSum). This specialized open model outperforms other LLMs apart from Vicuna with 7B parameters, OpenLLaMA with 13B parameters, and Mixtral with 45B parameters. Crucially, PrivateLoRA on BART-Large outperforms DP-ICL run on GPT3 Davinci, despite using the model with around 500X fewer parameters. This further indicates that we can leverage a large selection of open models to solve a specific task at lower cost and with better privacy protection without resorting to general-purpose closed LLMs. We also use PrivateLoRA on larger models from different families (Vicuna 7B, OpenLLama 13B, and Mixtral 8x7B) and observe that its performance and cost steadily increase with more parameters.

The evaluation on PFL-DocVQA in Table 5 shows that PrivateLoRA on open LLMs outperforms DP-ICL (which was run also only on OpenLLaMA 13B in the original paper  due to the cost constraints). We also evaluate both MIT-D and MIT-G in Table 6 on the accuracy of predicted vs target labels following the metrics in DP-FewShotGen. The adaptations of open LLMs with privacy-preserving gradient-based methods outperform DP-FewShotGen on the significantly larger GPT3 Davinci, for example, on MIT-D by 13.4% and on MIT-G by 22.3% absolute, respectively by PrivateLoRA on OpenLLaMA 13B.

We also present the privacy-utility trade-off for the SAMSum, MIT-G, and MIT-D datasets with varying values of \(\) across the PrivateLoRA, PromptPATEGen, and DP-FewShotGen methods in Figure 3. We use the Pythia 1B model for MIT-D and MIT-G and the BART-Large model for SAMSum. The graphs clearly demonstrate a similar trend to that shown previously in Figure 2: PrivateLoRA for open LLMs significantly surpasses the performance of both DP-ICL and DP-FewShotGen, which rely on GPT-3 Davinci.

### Comparing Costs

We compare the costs of obtaining a private predictor for a given downstream task using open vs closed LLMs. We use the wall clock time to capture the running time of methods for local open LLMs, which we then translate to the monetary cost that would be incurred if we ran the method on cloud-based hardware. For the adaptations of closed LLMs, we count the number of tokens used in the queries and obtained outputs from the APIs. The pricing from cloud providers and OpenAI forms the basis for the cost estimations, and we show the selected values in Table 22 in the Appendix. Further details on how the costs were calculated for each private ICL methods are presented in Appendix D. Based on the estimated costs in Tables 1,3,4,5, and 6, the privacy-preserving methods for open LLMs require much lower costs (and perform better) than for closed LLMs in the considered scenarios. The costs for classification tasks are relatively low, especially for closed LLMs, since the tasks are simple and the number of tokens (particularly for outputs) is small. However, the costs increase substantially for generation tasks, especially for the closed LLMs, where DP-ICL is around 150X more expensive than PrivateLoRA for dialog summarization. While larger models often incur higher costs, they do not necessarily imply higher performance. For example, smaller models like RoBERTA Large for classification or BART-Large for dialog summarization can obtain one of the highest performances at the lowest prices.

Figure 3: **Privacy-utility trade-off for generation tasks. We analyze the privacy costs \(\) in the range \(\) for the three generation tasks. PrivateLoRA for open LLMs substantially outperforms DP-ICL and DP-FewShotGen, which both utilize GPT3 Davinci. PrivateLoRA for MIT-D and MIT-G is trained on the Pythia 1B model, and for SAMSum on the BART-Large Model. PromptPATEGen uses Vicuna 7B.**

Discussion and Future Work

In summary, our results highlight that from the perspective of providing truly privacy-preservation adaptations, open LLMs are strictly preferable over closed LLMs, since their adaptations are more private, more performant, and more cost-effective. Going beyond the concrete existing methods studied in this work [15; 25; 53; 60], in the following, we analyze the general reasons behind the underwhelming results of privacy-preserving closed LLM adaptations.

**Privacy Leakage.** The enhanced privacy protection from adapting open LLMs is a major benefit: users' private training data and queries to adapted open LLMs are never revealed to third parties. On the contrary, the leakage of private query data to the LLM provider is to date an inherent problem with closed LLMs, since no methods to provide formal guarantees for the query data are currently known. Potential solutions might involve private inference for LLMs, where a model performs inference on encrypted queries, however, it is still in its nascency [9; 23; 32] for the scale of closed LLMs .

**Performance.** We argue that the lower performance of closed LLM adaptations stems from the fact that they have to rely on discrete prompts and that engineering such prompts for the closed LLMs is highly challenging. This is because 1) prompts, in general, have been shown to exhibit an unstable performance and to require a large number of trials and errors or discrete optimization while still underperforming gradient-based approaches . Additionally, 2) when the prompts (for privacy reasons) are not tuned on the closed LLM but on an open LLM surrogate model, additional performance decrease is incurred through the prompt transfer, since it has been shown that transferred prompts cannot reach the performance of prompts directly tuned on a given LLM . While the latter problem might be mitigated through the design of more performant prompt transfer techniques, the former one seems to be a more fundamental limitation .

**Costs.** The high costs incurred by some closed LLM adaptations result from the fact that they rely on ensemble-based approaches to yield DP guarantees and the fact that they incur continuous query costs at inference time. The former one could be solvable by designing more efficient DP schemes for discrete prompts, however, the latter is inherent to the nature of closed LLMs.

We hope that implementing the above-mentioned solutions will shrink the gap between private adaptations of open and closed LLMs. However, it remains unclear whether it is worth the community's effort, given the effectiveness of private adaptations for open LLMs.

## Broader Impacts

Our comparative study of open and closed LLMs has significant implications for private adaptations: our research advocates for the use of open LLMs for private adaptations.

We stress that our goal is not to discredit closed LLMs, but to highlight the potential privacy and performance benefits as well as cost-effectiveness associated with the use of open LLMs. Through thorough evaluations, we demonstrated in our paper that adapting open LLMs with private parameter efficient fine-tuning methods results in higher performance and mitigates open privacy risks of in-context learning with closed models. This not only leads to better performance but also reduces costs, making privacy adaptation on open LLMs a more viable option for many applications.

Moreover, our work can serve as a baseline for future private learning methods for LLMs. We believe that an open dialogue about the strengths and weaknesses of both open and closed LLMs is crucial for the advancement of privacy-preserving LLMs. We hope that our research will serve as a catalyst for further investigations into private adaptations of LLMs, ultimately leading to the development of models that effectively balance the need for both openness and privacy, all while ensuring that user privacy remains uncompromised.

#### Acknowledgments

The project on which this paper is based was funded by the German Federal Ministry of Education and Research (BMBF) under funding number 16KIS2114K. This work was also supported by the German Research Foundation (DFG) within the framework of the Weave Programme under the project titled "Protecting Creativity: On the Way to Safe Generative Models" with number 545047250. Responsibility for the content of this publication lies with the authors.