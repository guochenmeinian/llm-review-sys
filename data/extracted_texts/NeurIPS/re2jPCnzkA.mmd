# MIDGArD: Modular Interpretable Diffusion over Graphs for Articulated Designs

Quentin Leboutet Nina Wiedemann Zhipeng Cai Michael Paulitsch Kai Yuan

Intel Labs - XRL - eXtended Reality Laboratory

{firstname.lastname}@intel.com

###### Abstract

Providing functionality through articulation and interaction with objects is a key objective in 3D generation. We introduce MIDGArD (Modular Interpretable Diffusion over Graphs for Articulated Designs), a novel diffusion-based framework for articulated 3D asset generation. MIDGArD improves over foundational work in the field by enhancing quality, consistency, and controllability in the generation process. This is achieved through MIDGArD's modular approach that separates the problem into two primary components: _structure generation_ and _shape generation_. The structure generation module of MIDGArD aims at producing coherent articulation features from noisy or incomplete inputs. It acts on the object's structural and kinematic attributes, represented as features of a graph that are being progressively denoised to issue coherent and interpretable articulation solutions. This denoised graph then serves as an advanced conditioning mechanism for the shape generation module, a 3D generative model that populates each link of the articulated structure with consistent 3D meshes. Experiments show the superiority of MIDGArD on the quality, consistency, and interpretability of the generated assets. Importantly, the generated models are fully simulatable, i.e., can be seamlessly integrated into standard physics engines such as MuJoCo, broadening MIDGArD's applicability to fields such as digital content creation, meta realities, and robotics.

## 1 Introduction

Despite their significance for applications in meta realities and embodied AI, the creation of 3D models of articulated objects remains a manual endeavor, and existing datasets  are often limited in both scope and scale. Human designers utilize strong prior knowledge of object geometry and kinematic structures, a capability that automated systems have yet to fully replicate.

Figure 1: The MIDGArD generative pipeline.

While extensive research has been conducted on generative models for static 3D objects  and scenes , integrating 3D part geometry with kinematic structures has received far less attention. Generating articulated objects that are both geometrically detailed and kinematically accurate presents unique challenges due to the complexity of modeling part interactions and motions. Recent research efforts, such as Neural 3D Articulation Prior (NAP)  or Controllable Articulation GEneration (CAGE) , started addressing this gap, pioneering direct end-to-end generation of complete articulated assets using a denoising diffusion process acting on articulation graphs. However, generating consistent objects with high-fidelity shapes remains a challenge. Identified shortcomings of NAP  include 1) unnatural motion caused by unconstrained screw-like joint parametrization, 2) limited controllability, 3) limited interpretability and 4) the generation of inconsistent or unidentifiable shapes. Additionally, both NAP and related approaches lack _part_-level control due to the opaque encoding as a latent within the graph, highlighting the need for enhanced interpretability.

In this work, we introduce MIDGArD, a novel framework designed to generate interpretable and simulatable 3D articulated assets in a controllable manner. MIDGArD addresses the limitations of previous methods by offering: 1) improved consistency in joint motion, 2) enhanced controllability of the generation process, 3) increased interpretability and 4) better overall asset quality. Furthermore, the 3D assets generated by MIDGArD are fully simulatable and can be seamlessly integrated into virtual environments or physics engines such as MuJoCo , significantly enhancing its utility for digital content creation and robotics. MIDGArD employs a modular and sequential approach based on two diffusion models: the _structure generator_ and the _shape generator_ (see Figure 1). The _structure generator_ denoises an articulation graph, unconditionally or from incomplete heterogenous inputs. This articulation graph acts as an abstract, yet interpretable object representation, encoding the structural and semantic information of every link, as well as kinematic attributes.

MIDGArD overcomes the identified limitations of prior works by 1) leveraging categorical embeddings for improving consistency across the articulation graph; 2) representing nodes (i.e. unique object parts) via latent image codes rather than direct mesh encoding; 3) diffusing on the Plucker manifold, thereby eliminating the need for discretizing the generated motion parameters, and 4) accounting for the orientation of the parts for more consistent generations. As additional benefit, these modifications yields an image representation, and asset-level, body-level, and joint-level categorical data, that are _human interpretable_. Since the generation process is sequential, these quantities can be adjusted by the user before generating proceeding with the part geometry generation. Crucially, this interpretable structure enables the use of a multi-modal 3D generation model as the _shape generator_. The shape generator, here represented by SDFusion , is queried multiple times to produce high-quality geometries for each part of the object (see Figure 1). To enhance consistency, we propose a constrained generation approach that enforces the part geometry to fit a given bounding box, here provided by the structure generator.

We demonstrate MIDGArD's capabilities by showcasing improved structural quality, consistency, and part-level control compared to NAP. Furthermore, we show qualitative results of MIDGArD generating diverse articulated objects dependent on the partial graph input and shape conditioning. To further improve the consistency and quality of the generated shapes, we propose a to create oriented bounding boxes that have a tighter fit to the ground truth while being properly aligned. The reduces the overlapping volume by 17% on average. Our contributions can be summarized as:

1. **MIDGArD:** A novel diffusion-based framework for generating high-quality, articulated, and simulatable 3D assets with improved consistency between structure and shape.
2. **Enhanced Interpretability, and Controllability:** Providing human-interpretable intermediate outputs and allowing user adjustments at multiple levels of the generation process.
3. **Enhanced Quality and Scalability:** Relying on state-of-the-art 3D foundation models for shape generation.
4. **Improved part alignment:** Introducing a bounding-box constrained shape generation approach that improves alignment of kinematic links by 17% on average.
5. **Simulatable Assets:** Offering a pipeline to create fully simulatable assets compatible with physics engines such as MuJoCo.

Code and models are available at https://quentin-leboutet.github.io/MIDGArD.

Related Works

Structural Characterization and State Estimation of Articulated ObjectsDesigning articulated objects involves understanding both their geometry and kinematic properties. Traditional methods often require extensive manual labor or rely on limited datasets, hindering scalability and applicability . Early work on morphable templates, such as SMPL for human bodies  and SMAL for quadruped animals , demonstrated the potential of matching existing articulation structures (i.e., skeletons) to a wide variety of inputs such as static meshes or monocular videos, thereby substantially simplifying the rigging process. However, while effective within specific categories, these templates lack the flexibility to generalize to arbitrary articulated mechanisms. To address these limitations, recent approaches have enhanced templates by incorporating implicit representations to model geometric deformations and appearances . Robust neural rigging methods  leverage template priors while remaining flexible enough to handle mesh elements that deviate from the template, such as clothing or hair. On the other hand, template-free methods  rely solely on input mesh geometry but are still primarily limited to humanoid characters due to the scarcity of comprehensive datasets. Emerging approaches  aim to rig arbitrary static assets, potentially paving the way for a unified, template-free rigging pipeline. Our research aligns with this direction by learning articulation priors for arbitrary objects without depending on predefined templates.

Significant strides has also been made in estimating articulation joint states and parameters from sensory observations. Early contributions such as  demonstrated that it is possible to learn compact kinematic models of entire articulated objects solely based on pose observations. Probabilistic models have enabled robots to learn their own kinematic structures through self-observation and motor babbling . Gaussian processes have been employed to model part connectivity and link articulation in objects . Interactive perception techniques further enhance this learning process by allowing robots to interact with objects to obtain their kinematic model . Deep learning techniques were also used to train dedicated architectures to directly predict articulation models from sensory inputs . Interactive systems  learn to predict potential motions of articulated objects, aiding downstream motion planning and interaction, while reinforcement learning based approaches  train policies to explore diverse interaction trajectories, contributing to actionable visual representations. Our method diverges from traditional approaches by not relying on explicit estimation pipelines; instead, we learn articulation priors implicitly within our model.

Articulated Object GenerationGenerating articulated objects involves synthesizing models that accurately represent both geometric details and kinematic behaviors. Methods such as Self-Supervised Category-Level Articulated Object Pose Estimation  and PD (Pose-aware Part Decomposition)  address the generation of joint parameters and part poses, respectively, contributing significantly to the field. Few generative frameworks address both aspects concurrently. Notable among them is NAP , which generates complete articulated assets--including meshes--using graph attention networks. CAGE , enhances controllability and interpretability, albeit with limitations in mesh generation. More recently,  proposed a framework capable of reconstructing articulated assets from image inputs by using a 3D shape completion model to generate the different parts and a large language model (LLM) to predict the joint parameters. Progress was also made in physics-aware shape generation; for instance,  introduces an improved loss mechanism to account for internal collisions within generated articulated assets. In this work, we build up on NAP and CAGE and design an approach to enhance the controllability of the generative process, and leverage a multi-modal 3D generative model for improving the quality of the shape geometry.

3D Shape Generation and CompletionRecent progress in diffusion models and 2D-guided generation techniques have revolutionized 3D shape generation and completion. Two main types of generative pipelines are currently being explored. The first one leverages 2D diffusion models to optimize NeRFs , enabling high-quality 3D generation without the need for direct 3D supervision. The second employs Latent Diffusion Models (LDMs) for different 3D representations, including point clouds , voxels , SDFs, shape2vecsell, and triplanes . It should be emphasized that despite the predominant role of diffusion models, other methodologies such as autoregressive models , have demonstrated potential in producing high-fidelity 3D shapes from minimal inputs. Conditional diffusion models enable multi-modal inputs. SDFusion , for instance, can generate 3D objects from text descriptions, images, or partial 3D shapes. Building upon these capabilities, we have adapted SDFusion as the shape generator within our framework. However, due to MIDGArD's modular design, the shape generator can be updated to incorporate the latest advancements in static 3D generation.

Synthesis of 3D Articulated Mechanisms

### Method Overview

Both components of MIDGArD, namely the structure and the shape generator, leverage a denoising diffusion model [79; 80; 21]. Diffusion models operate by progressively degrading training data through the systematic application of Gaussian noise, a method known as the forward process. Subsequently, these models are trained to restore the original data by methodically removing this noise in what is known as the reverse process. Please refer to Appendix A and [21; 79] for details. As shown in Figure 1, the full MIDGArD pipeline can be separated into the _Structure Generator_ and the _Shape Generator_. The Structure Generator (subsection 3.2) takes graph-based representation of an articulated asset - which may be incomplete or affected by noise - and resolves this representation through diffusion. The denoised graph features, decoded into suitable - human interpretable - text prompts, images and bounding box information are then fed into the Shape Generator (subsection 3.3), another diffusion-based model yielding the final articulated 3D object.

### Structure Generator

Articulated Asset Representation and ParametrizationBuilding upon , we encode the structural and kinematic attributes of every articulated asset into the node and edge features of a complete graph referred to as \(_{N}\). Our structure generation module leverages a Graph ATtention (GAT) [90; 4; 32] denoising network to generate coherent and interpretable articulation features from noisy or incomplete inputs (see Figure 1). To improve the quality, controllability, and interpretability compared to , we modify the asset parametrization. Drawing on insights from , we incorporate a set of categorical variables to the nodes and edges features to enhance asset consistency, interpretability and provide a more intuitive control interface over the generation process. Each asset is represented as a complete graph \(_{N}=\{,\}_{N}\) that embodies the object's links and joints in its \(N\) nodes, and in its \(N(N-1)/2\) edges respectively. Rather than directly denoising and then decoding a low-dimensional shape latent \(_{i}^{D_{F}}\) for each link \(i\) into a 3D mesh [37; 36], we propose a two-step approach that operates on a more manageable image latent \(_{i}^{D_{G}}\) of the link. This image latent is derived by training a Vector-Quantized Variational Auto Encoder (VQVAE)  on various views of every rigid body in the dataset. In our method, the structure generator denoises a latent representation of an image for each individual component of the articulated asset. This denoised image, along with the categorical information extracted from the graph, facilitates the creation of human-interpretable priors for each link, namely a text description and a front view image. These prompts can then be utilized to condition a 3D shape generation model yielding consistent meshes. Additionally, the graph itself encodes highly relevant structural features that act as supplementary conditioning signals, further guiding the shape generation model.

Each node \(_{i}\) represents a link of the articulated asset and \( i,j[0,N-1]\) the edge \(_{ij}\) establishes a connection between node \(_{i}\) and node \(_{j}\). The node feature vector \(_{i}\) is comprised of multiple components, specifically, \(_{i}=[_{i},_{i},_{i},_{i},_{i},_{i}, _{i}]^{D_{V}}\), and the edge feature vector \(_{ij}\) is formulated as \(_{ij}=[_{ij},_{ij},_{ij},t_{ij}]^{D_{E}}\), where \(D_{V}\) and \(D_{E}\) refer to the node and edge feature dimensions. To ensure a consistent graph size across various objects, \(N\) is set to the maximum count of parts anticipated, using \(_{i}\) as a binary indicator to denote part existence. Similarly, the edges denote symbolic existence and chirality through the indicator variable \(_{ij}^{3}\). Joint types are represented through \(_{ij}^{D_{J}}\), a one-hot encoding that in our design has a dimensionality of three to signify the types - prismatic, revolute, or screw. Similarly, \(_{i}^{D_{A}}\) and \(_{i}^{D_{B}}\) denote the one-hot encoding vectors of asset (resp. body) categories. Similar to , we use Plucker coordinates as joint parametrization as this provides a compact representation for joint types, such as revolute, prismatic, and screw joints. The Plucker joint parametrization, articulated as \(_{ij}^{6}\), provides a harmonious portrayal of the different joint types under consideration, and joint limits are entailed by \(_{ij}^{D_{L}}\). Ultimately, the vectors \(_{i}^{3}\), \(_{i}^{3}\) and \(_{i}^{3}\) depict the bounding box dimensions, orientation and position of each part. Note that \(_{i}\) and \(_{i}\) are defined for every part relative to its parent reference frame.

Unlike previous approaches [37; 46], we do not assume that the parts of the dataset are provided in a canonical orientation. Assuming that objects are in canonical orientation and not explicitly considering the orientation leads to degrading performance due to misaligned orientations of the dataset (see Figure 5, as well as Appendix B-Figure 10 and Appendix B-Figure 11). Instead, we estimate this orientation for each body in the dataset by computing the corresponding Oriented Bounding Box (OBB), which is then used to position the body in a canonical pose. The process (see Appendix B) begins by generating a convex hull from the mesh. This step simplifies the complex geometry into a more manageable form, reducing dependency on the internal structure of the object. Next, we voxelize the convex hull to collect volumetric samples. We then execute a Principal Component Analysis (PCA) to determine the primary axes of the mesh providing an initial approximation of the object's orientation. Following the PCA, we perform gradient descent on the attitude quaternion of the OBB, using the PCA-based initial estimate as a starting point. The objective of this optimization is to minimize the volume of the OBB. This approach allows for a more flexible and accurate alignment of dataset bodies into canonical poses, thereby enhancing the overall quality and consistency of the articulated objects generated by our framework (see Figure 5, Table 3).

Denoising Diffusion on the Plucker ManifoldTo maintain the interpretability of the denoised quantity as a Plucker vector \(_{ij}=[_{ij},_{ij}]\), our approach introduces a novel parametrization allowing the diffusion process to be directly executed on the Plucker manifold \(\). In contrast, the approach proposed in , denoises within \(^{6}\), thereby requiring a projection operation \(p:^{6}\) at every denoising step. We propose an alternative formulation \(_{ij}=[_{ij},_{ij}]^{5}\) of the joint parameters where \(_{ij}=[_{ij},_{ij}]^{2}\) represent the spherical parametrization of the Plucker axis \(_{ij}\). This formulation yields \(_{ij}=[(_{ij})(_{ij}),(_{ij})( _{ij}),(_{ij})]^{3}\) with \(\|_{ij}\|=1\). The vector \(_{ij}^{3}\) is then defined such that \(_{ij}=_{ij}_{ij}\). This alternative parametrization guarantees the normalization of \(_{ij}\) and enforces by construction the orthogonality between \(_{ij}\) and \(_{ij}\), which are key features of the Plucker manifold . Consequently, our approach inherently satisfies the manifold's constraints, thereby streamlining the computational workflow by eliminating the necessity for iterative projections.

### Shape Generator

The shape generator of MIDGArD aims to create the mesh for each component of an articulated object using the denoised object graph, which contains both semantic (object category) and visual (image latents) data. We leverage a multi-modal 3D generative model that is trained independently from the structure generator. Our modular setup is designed to accommodate any generative model, allowing for seamless integration of the latest advances in 3D generation. In this work, the SDFusion model  is used, conditioned on image, text as well as graph features input, and enhanced with a novel approach for bounding-box-constrained generation. The training pipeline is shown in Figure 2.

Multimodal Guidance for Consistent Shape GenerationThe core of SDFusion is a latent diffusion model, see  for details. Object meshes are transformed into truncated signed distance functions (TSDFs) and encoded using a pretrained and frozen 3D VQ-VAE  model. The diffused latent representations, along with image and text condition signals, are denoised through a 3D U-Net model. The resulting output latent is then decoded into a TSDF and converted back into a mesh using a marching cube algorithm . It is worth emphasizing that we train and apply the diffusion model on the individual _parts_ of each objects, querying the model multiple times at inference time to generate a full articulated object (see Figure 1). The model is trained on the PartNet Mobility dataset  with the same train-test split as the structure generator. We modified SDFusion's conditioning  to align it with the output of the structure generator. The model is conditioned through cross-attention on the following modalities, either independently or in combination.

* Single view image: A pretrained ResNet-18 model  encodes images. During training, the model uses renderings of the part mesh from a frontal view. At inference, the model decodes the image from the node features of the articulation graph.
* Text: We employ BERT  for text encoding (similar to [84; 8]). The text is constructed using the object category and asset type from the PartNet dataset in the format "A <asset type> as part of a <object category>"; for instance, "_A lid as part of a trash can_".
* Graph: The output from the structure generator is encoded and concatenated with image- and text embeddings to provide information about the part's role within the object, such as its expected size and its relation to other parts, i.e. joint types.

This multi-modal conditioning approach ensures the generation of consistent, high-quality parts that can be seamlessly integrated to form complete articulated objects, enhancing the overall performance and applicability of our framework.

Enforcing Bounding ConstraintsGenerating object parts independently offers increased flexibility and quality but introduces challenges related to the size and orientation of the generated parts. Intuitively, the shape generator must ensure that parts conform to \(_{i}\) and \(_{i}\) in the articulation graph. To achieve the correct _orientation_, we generate links in a canonical pose and subsequently apply a post-processing rotation based on \(_{i}\), the 3D orientation vector derived during structure generation. To train the model accordingly, by estimating and adjusting the pose of parts to a canonical orientation, as detailed in Appendix B.

To ensure that generated parts match the _sizes_ specified within the graph node features, we introduce a bounding-box constraint to the generative model (see Figure 2). Instead of generating the object directly, the diffusion model is trained on the _difference_ relative to a predefined bounding box. To enable this schema for _latent_ diffusion, we calculate the difference as \(z_{}=z_{o}-z_{b}\), where \(z_{o}\) is the VQ-VAE-encoded latent of the object, and \(z_{b}\) is the encoding of a simplified bounding-box TSDF (values inside the box set to -0.01, and values outside set to 0.01). During inference, we provide the denoiser with the bounding box information by concatenating \(z_{b}\) to the noisy version of \(z_{}\). Other conditioning inputs, such as images, text, and graph features encodings, are incorporated through cross-attention mechanisms. The object's TSDF is then computed by decoding \(z_{}+z_{b}\) using the 3D VQ-VAE. This method effectively guides the model to produce objects that are closely matching the target bounding boxes even after a few training steps.

## 4 Experiments, Results and Discussion

### Experiment Setup

DatasetAll experiments were conducted using the PartNet Mobility dataset , which contains a diverse set of articulated 3D objects with detailed geometric and kinematic annotations. Each mesh in the dataset underwent a two-step preprocessing routine. First, we enforced the manifold property as described in . Second, we performed orientation estimation to establish a canonical pose for each object and accurately compute the corresponding bounding boxes (see Appendix B for details).

Evaluation MetricsWe adopt the evaluation framework from NAP , which introduces the Instantiation Distance (ID) metric for comparing two articulated objects. ID measures the average Chamfer distance over sampled joint states, accounting for both geometric and kinematic differences. It is defined as:

\[ID(O_{1},O_{2})_{q_{1} Q_{1}}_{q_{2} Q_ {2}}(O_{1},q_{1},O_{2},q_{2})+_{ q_{2} Q_{2}}_{q_{1} Q_{1}}(O_{1},q_{1},O_{2},q_{2}) ,\] (1)

where \(Q\) is a set of \(M\) uniformly sampled joint poses and \(\) is the minimum Chamfer distance over all possible canonicalizations of the mesh. We use ID in conjunction with standard metrics for unconditional generation, namely minimum matching distance (MMD), coverage (COV) and 1-nearest-neighbor accuracy (1-NNA).

Figure 2: Architecture of the Shape Generator.

Implementation DetailsWe trained the structure generator and the image VQ-VAE on an NVIDIA RTX 3090 GPU, while the shape generator was trained on an NVIDIA RTX 6000 GPU. Evaluation took place on a single NVIDIA RTX 3090 GPU. The image VQ-VAE, which encodes latent representations of objects in the shape generator, was trained on \(256 256\)-pixel front renderings of the the PartNet Mobility object meshes. The denoising model used in the structure generator contains six graph attention blocks, with a latent embedding size of 512 and 32 attention heads. We set the maximum number of nodes in the graph to \(N=8\). Our training parameters closely follow those in NAP, with the key difference being the use of an implicit denoising diffusion pipeline  over 100 time steps, as opposed to a DDPM with 1,000 time steps. This modification significantly accelerates inference speed. Our shape generator is adapted from SDFusion  and trained on the PartNet Mobility dataset. We used the same hyperparameters as the multimodal model in SDFusion and utilized their pre-trained VQ-VAE checkpoint. We excluded 10 categories from training due to their objects containing numerous equally-shaped parts (e.g., keyboards with over 30 keys).

### Results and Discussion

Unconditional Articulated Asset GenerationTo the best of our knowledge, NAP is the only approach for generating articulated 3D objects without prior knowledge about their geometric structure. We first evaluate MIDGArD in an unconditional generation setting, comparing its performance to NAP. For a fair comparison, we retrained NAP on our preprocessed version of the PartNet Mobility dataset. The results are presented in Table 1. MIDGArD outperforms NAP in terms of MMD and COV metrics, indicating better diversity and coverage of the generated samples. Specifically, we observe a 6.4% improvement in MMD and a 3.7% improvement in COV. The 1-NNA metric is comparable between both methods. We also perform an ablation study on the effect of the Plucker manifold parameterization. As shown in Table 1 using the Plucker manifold improves the MMD and COV metrics, suggesting enhanced consistency and diversity in the generated assets.

To further evaluate the physical plausibility of the generated assets, we analyze the distribution of joint types in the training data and compare it with those in samples generated by NAP and MIDGArD. We sampled 400 objects generated by each method and counted each joint type only once per object to minimize the impact of objects with multiple joints of the same type.

The results in Table 2 show that NAP predominantly produces screw joints, which are rare in the training data. In contrast, MIDGArD generates objects with a joint type distribution closely matching that of the training data, thereby enhancing the plausibility of the generated assets. We computed the Chi-Square statistic to quantify the deviation from the expected joint type distribution. NAP's generated data yielded a high \(^{2}\) value of 5618, indicating a significant difference from the training data distribution. In contrast, MIDGArD's generated data resulted in a low \(^{2}\) value of 12.7, demonstrating close alignment with the expected distribution.

Figure 3.B provides a side-by-side qualitative comparison between MIDGArD and NAP. MIDGArD generates objects with higher geometric quality and more realistic motion compared to NAP. For instance, the fan generated by MIDGArD exhibits detailed geometry, and the laptop displays realistic opening motion, whereas NAP's outputs often show unnatural joint motions and inconsistent shapes.

    &  \\  & MMD \(\) & COV \(\) & 1-NNA \(\) \\  NAP & 0.0282 & 0.4675 & **0.5831** \\ Ours (Plücker manifold) & **0.0264** & **0.4857** & **0.5831** \\ Ours (No Plücker manifold) & 0.0270 & 0.4779 & 0.6221 \\   

Table 1: Comparison to NAP in an unconditional generation setting.

    & Screw & Revolute & Prismatic \\  Training data & 6\% & 62\% & 32\% \\ NAP-generated & 95\% & 1\% & 4\% \\ MIDGArD-generated & **2\%** & **62\%** & **36\%** \\   

Table 2: Distribution of joint categories in real data vs generated data from NAP and our approach.

Additionally, as shown in Figure 3.C, the absence of categorical information in NAP leads to the generation of objects composed of mismatched parts that do not cohesively fit together. In our approach, while there may be occasional issues with geometry or kinematics, the object parts remain semantically consistent, preserving the integrity of the overall structure. Finally, as illustrated in Figure 3.D, our conditional part-generation proves to be robust to images from different viewpoints, yielding enhanced flexibility and generalization capability of our method.

Conditional Generation and ControllabilityOne of the key advantages of MIDGArD over existing approaches is its enhanced controllability, enabling users to guide the generation process using human-understandable graph attributes such as articulated asset types and body types. We showcase this capability in a "(Image+Text)-To-Object" setup (see Figure 3.A1), and a "Part-To-Motion" setup (see Figure 3.A2), where the model is provided with part features only (i.e., no joint data) and outputs consistent articulated assets. Notably, MIDGArD can also be controlled using only asset-level data; for instance, users can query for specific categories such as "storage furniture" or "fan" without needing to provide detailed information for each node (see Appendix C for additional qualitative results). Furthermore, the modular structure of our generation pipeline enables fine-grained, part-level control. Users can specify the desired appearance or attributes of individual parts by adjusting the human-interpretable articulation solutions synthesized by the structure generator accordingly. The resulting images and text description will then be used as conditioning signals for the shape generation module. As illustrated in Figure 4, varying the image inputs while keeping other attributes fixed results in generated parts that adapt accordingly, demonstrating the flexibility and responsiveness of our approach. This level of control allows for precise customization of generated assets, facilitating applications that require specific design features or aesthetic qualities (see Appendix D for additional qualitative results).

Figure 4: Image guidance of the shape generation process

Figure 3: A) _PartToMotion_ constrained structural generation. B) Side-by-side comparison between our approach and NAP. C) Typical failure cases. D) Part-level image conditioning.

Constrained Shape GenerationWe conducted an ablation study to evaluate the effectiveness of our bounding-box constrained shape generation approach introduced in section 3.3. The experiments were performed on _part_-samples from PartNet Mobility. We randomly sampled ten objects per category from the test data, including all their parts, resulting in 1044 samples across 32 categories. First, we measure whether the generated object fits within the bounding box. The model output \(Y^{gen}\) is a sampled TSDF of resolution 64, \(Y^{gen}^{64 64 64}\), where negative values indicate the object's interior. We compare \(Y^{gen}\) with the TSDF of the bounding box \(Y^{bb}\) and compute the count of all cells outside of the bounding box (\(OBB_{count}\)) and their sum of SDF values (\(OBB_{sum}\)):

\[OBB_{count}(O_{bb},O_{gen})=_{ijk}[Y^{bb}_{ijk}>0 Y^{gen}_ {ijk}<0],\] (2)

\[OBB_{sum}(O_{bb},O_{gen})=_{ijk}Y^{gen}_{ijk}[Y^{bb}_{ijk} >0 Y^{gen}_{ijk}<0].\] (3)

Secondly, we evaluate the generation quality after transforming the TSDF into a mesh, computing the Chamfer distance (CD) between generated and real objects based on 5000 sampled points. Table 3 compares our _bb prior_ approach to the original SDFusion model trained on PartNet Mobility. For encoding the part dimensions, we use an MLP with three hidden layers (size 16, 64 and 256 respectively). For a comparison to a GAT-encoding, see Appendix E. Our first modification of preprocessing the dataset to rectify objects into canonical pose, dubbed "original + bb MLP + oriented dataset", already improves the generation results significantly, leading to lower \(OBB_{count}\), \(OBB_{sum}\) and CD. The _bb prior_ method, however, consistently ensures accurate proportions and sizes for the generated links, diminishing erroneous volume. Additionally, _bb prior + bb MLP_ method secures the best Chamfer Distance at 0.072. An alternative technique involves resizing generated objects to suit the bounding box dimensions. Such postprocessing of the model outputs reduces the average CD to 0.0061, close to matching that of the _bb prior_ method. Nonetheless, such resizing may lead to significant distortions in object proportions, thereby underscoring the superiority of the _bb prior_ method.

Analysis of Oriented Bounding BoxesAs Table 3 shows, using oriented bounding boxes drastically improves the reconstruction performance. To quantify the change to the dataset, we measured the volume reduction of the bounding boxes after applying our PCA-based rectification approach. On average, the bounding box volume decreased by 17.4%, with one-third of the samples experiencing

    & \(OBB_{count}\) & \(OBB_{sum}\) & CD (generated) & CD (scaled) \\  bb prior + bb MLP (Ours) & **1146** & **10.89** & **0.0072** & **0.0043** \\ original + bb MLP + oriented dataset & 1490 & 37.15 & 0.0152 & 0.0061 \\ original (SDFusion) + bb MLP & 3795 & 183.5 & 0.0307 & 0.008 \\   

Table 3: Ablation of bounding-box constrained shape generation. The original SDFusion pipeline, only enhanced with bounding-box conditioning, is compared to our approach. Training on a preprocessed dataset and post-processing the generated shapes afterwards already improves the performance significantly. The best results are achieved using our bounding-box SDF prior with post-processing.

Figure 5: Failure cases due to due to axis-aligned bounding boxes and parameter discretization. A) The use of Axis Alligned Bounding Boxes tends to confuse the 3D generation model. B) The use of categorical joint variable in MIDGArD allows better joint motion resolution and helps filtering unrealistic solutions obtained with former pipelines.

a volume decrease of more than 10% and 20% of the samples experiencing a volume decrease of more than 30%. Figure 5 illustrates failure cases where using axis-aligned bounding boxes leads to unrealistic part shapes, such as a thick cabinet door.

## 5 Conclusions

In this work, we introduced MIDGArD, a novel framework for generating 3D articulated objects. MIDGArD employs a modular approach, combining interpretable articulation graph generation with high-quality shape generation. By leveraging categorical parametrization, MIDGArD enhances the consistency and plausibility of articulated objects, producing high-quality meshes with accurate dimensions through a constrained shape generation mechanism. The human-interpretable representation of images and text within the articulation graph allows for part-level and multi-modal control over the generation process. The models produced by MIDGArD are fully simulatable, paving the way for applications in text-guided or image-guided content creation.

MIDGArD addresses several issues identified in Neural 3D Articulation Prior (NAP) by improving joint conditioning with categorical embeddings, enforcing physical constraints via bounding-box constrained generation, and providing part-level control using latent image codes. Our results demonstrate MIDGArD's superior performance in terms of structural quality, consistency, and interpretability compared to existing methods. The framework's ability to generate diverse articulated objects based on partial graph inputs and shape conditioning underscores its potential for broad applications in digital content creation and robotics.

Despite the advancements introduced by MIDGArD, there are several limitations and areas for future improvement: 1) We observe that the current metrics for articulated generation are still very limited and benchmarks as well as evaluation frameworks must be developed for this field. For instance, Instantiation Distance is still based on point clouds, which ignores cases of _small_ parts of the object with unrealstic motion, such as a cart wheel moving up to a meter away from the cart. 2) Overcoming limitations of graph scalability posed by node number constraints remains an important challenge. 3) Furthermore, it would be interesting to investigate the use of mixed integer noise methods such as MiDi  in the articulation graph generation pipeline to process the continuous and discrete variables. 4) Enhanced conditioning of the structure generation process using natural language or image data represents a highly promising avenue for future research. Foundational efforts in this area, such as those by Cai et al. , have laid the groundwork for further exploration. 5) Integrating specialized templates, such as human body poses, could significantly enhance MIDGArD's modelling versatility. Lastly, 6) testing alternative 3D generation models and adapting MIDGArD to handle different geometric primitives could broaden its generalization capabilities.