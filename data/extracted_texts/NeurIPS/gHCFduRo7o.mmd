# Selective Explanations

Lucas Monteiro Paes

Harvard University

lucaspaes@g.harvard.edu &Dennis Wei

IBM Research

dwei@us.ibm.com &Flavio P. Calmon

Harvard University

flavio@seas.harvard.edu

###### Abstract

Feature attribution methods explain black-box machine learning (ML) models by assigning importance scores to input features. These methods can be computationally expensive for large ML models. To address this challenge, there have been increasing efforts to develop _amortized explainers_, where a ML model is trained to efficiently approximate computationally expensive feature attribution scores. Despite their efficiency, amortized explainers can produce misleading explanations. In this paper, we propose _selective explanations_ to (i) detect when amortized explainers generate inaccurate explanations and (ii) improve the approximation of the explanation using a technique we call _explorations with initial guess_. Selective explanations allow practitioners to specify the fraction of samples that receive explanations with initial guess, offering a principled way to bridge the gap between amortized explainers (one inference) and more computationally costly approximations (multiple inferences). Our experiments on various models and datasets demonstrate that feature attributions via selective explanations strike a favorable balance between explanation quality and computational efficiency.

## 1 Introduction

Large black-box models are increasingly used to support decisions in applications ranging from online content moderation , hiring , and medical diagnostics . In such high-stakes settings, the need to explain "why" a model produces a given output has led to a growing number of perturbation-based _feature attribution_ methods . These methods use input perturbations to assign numerical values to each input feature (e.g., words in a text) a model uses, indicating their influence on the model prediction. They are widely adopted in part because they work in the black-box setting with access only to model output (i.e., without gradients). However, existing feature attribution methods can be prohibitively expensive for the large models used in the current machine learning landscape (e.g., language models with billions of parameters) since they require a significant number of inferences for each individual explanation.

Recent literature has introduced two main _approximation_ strategies to speed up existing feature attribution methods for large models: (i) employing Monte Carlo methods to approximate explanations with fewer computations , and (ii) adopting an _amortized_ approach, training a separate model to "mimic" the outputs of a reference explanation method . Monte Carlo approximations can yield accurate approximations for attributions but may converge slowly, limiting their practicality for and online applications. In contrast, amortized explainers require only one inference per explanation, making them efficient for large black-box models and online explanations. However, as shown in Figure 1, amortized explainers can produce highly diverging explanations from their reference due to lack of precision in approximations. Aiming to benefit from Monte Carlo and amortized explainers, we propose _selective explanations_ to answer the questions:

* When are amortized explanations inaccurate?
* How can we improve inaccurate amortized explanations using additional computations?To answer (Q1) and (Q2), we propose _selective explanations_, a method that bridges Monte Carlo and amortized explanations. The selective explainer first trains a model that "learns to select" which data points will receive inaccurate amortized explanations, and then performs additional computations to further approximate target explanations. The key idea behind the selective explanation method is to use Monte Carlo explanations only for points that would receive inaccurate amortized explanations; see Figure 2 for the workflow of selective explanations. The code for generating selective explanations can be found at https://github.com/LucasMonteiroPaes/selective-explanations.

The ideas of predicting selectively and providing recourse with a more accurate but expensive method (usually human feedback) have been explored in classification and regression [28; 10; 7; 9; 11]. To our knowledge, however, these ideas have not been applied to feature attribution methods. We make **two contributions** in this regard that are relevant for selective prediction more generally. (1) Selective prediction uses _quality metrics_ to identify input points for which the predictor (the amortized explainer in our case) would produce inaccurate outputs and recourse is needed. The high-dimensional nature of explanations requires us to develop new quality metrics (Section 3) suitable for this setting. (2) Instead of providing recourse with a Monte Carlo explanation alone, we use an optimized method called _explanations with initial guess_ (Section 4) that combines amortized and Monte Carlo explanations in a optimized manner, improving the approximation to the target explanation beyond that of either method individually.

Our **overall contribution** (3) is to combine (1) and (2) in the form of _selective explanations_, providing explanations with initial guess to improve inaccurate amortized explanations. We validate our selective explanations approach on two language models as well as tabular datasets demonstrating its ability to (i) detect inaccurate explanations from the amortized explainer, (ii) enhancing amortized explanations even when Monte Carlo explanations are inaccurate, and (iii) improving the worst explanations from the amortized model.

Fig. 1: Amortized explainer (a) compared with a target explainer (SHAP ) (b) and our selective explanation method (c). All methods flag input parts that contribute to the YelpLLM predicting the given example is a Negative Review. We observe that both target and selective explanations attribute ”not amazing” for the negative review (blue), while the amortized explainer misses this term.

Fig. 2: Workflow of selective explanations.

## 2 Problem Setup & Background

We aim to explain the predictions of a fixed probabilistic black-box model \(h\) that predicts \(h()=(h_{1}(),...,h_{||}())\) and outputs \(*{argmax}_{j}h_{j}()\) using a vector of features \(=(x_{1},...,x_{d})^{d}\). The user specifies an output of interest \(\) (usually \(=*{argmax}_{j}h_{j}()\)) and our goal is to efficiently explain _Why would \(h\) output \(\) for a given \(\)?_ We consider a dataset \(=\{(_{i},_{i})\}_{i=1}^{N}\) comprised of \(N>0\) samples divided into three parts: \(_{}\) for training \(h\) and the explainers, \(_{}\) for calibration and validation, and \(_{}\) for testing. Thus, \(=_{}_{} _{}\). Moreover, for a subset \(S=\{i_{1},...,i_{|S|}\}[d]\) we write \(_{S}(x_{i_{1}},...,x_{i_{|S|}})\).

**Feature Attribution Methods,** also called _explainers_, are functions \(^{d}^{d}\) that assess the importance of each feature for the model's (\(h\)) prediction to be \(\) for a given input vector \(\). We consider three types of explainers:

1. **Target explainers** that use a large number of computations to provide explanations (e.g., SHAP with \(2^{d}\) inferences from model \(h\)) [22; 29], denoted by \((,)\);
2. **Monte Carlo explainers** that approximate fixed target explainers using \(n\) inferences from model \(h\) per explanation [22; 24], denoted by \(^{n}(,)\);
3. **Amortized explainers** are trained to approximate the target explanations using only one inference [6; 38], denoted by \((,)\).

**Remark 1**.: Monte Carlo and amortized explainers aim to approximate the target explanation and are benchmarked on this approximation. We evaluate the performance of Monte Carlo and amortized explainers by computing their distance and correlation to \((,)\). The usefulness of target explanations (e.g.: SHAP and Lime) has been validated by user studies and automated metrics in [22; 29; 13; 37; 30; 31]. Therefore, we call **higher-quality** the explanations that closely approximate the computationally expensive target and **lower-quality** the one that diverge from the target.

In practice, we measure the quality of a given explanation that aims to approximate the target explanation using a loss (or distortion) function \(:^{d}^{d}\), e.g., mean square error (MSE) and Spearman's correlation. The goal of selective and Monte Carlo explanations is to approximate the target explanations while decreasing the number of computations, i.e., to minimize \(((,),(,))\) with few model inferences.

We define _selective explainers_ to provide better approximations to target explanations bridging the gap between Monte Carlo and amortized explainers.

**Definition 1** (**Selective Explainer)**.: For a given model \(h\), an amortized explainer \(\), a Monte Carlo explainer \(^{n}\), a _combination function_\(_{h}:^{d}\), and a _selection function_\(_{}:^{d}\{0,1\}\) (parametrized by \(\)), we define the _selective explainer_\((,)\) as

\[(,)(,)&,_{}()=1,\\ _{h}()(,)+(1-_{h}()) ^{n}(,)&,_{}()=0.\] (1)

When \(_{}=0\), selective explanations output _explanations with initial guess_ (Definition 2). Explanations with initial guess linearly combine amortized and Monte Carlo explanations to leverage information from both and provide higher-quality explanations than either explainer alone. Selective explanations heavily depend on three objects that we define in this work and that are covered in the rest of the paper: (i) an uncertainty metric (Section 3), (ii) a selection function (Section 3), and (iii) a combination function (Section 4).

* **Uncertainty metrics** (\(s_{h}\)) output the likelihood of the amortized explainer producing a low-quality explanation for an input. Lower \(s_{h}()\) indicates a higher-quality explanation for \(\). We propose two uncertainty metrics: Deep and Learned Uncertainty (Section 3).
* **Selection function** (\(_{}\)) is a binary rule that outputs 1 for higher-quality amortized explanations and 0 for lower-quality ones based on the uncertainty metric. We define \(_{}\) to ensure a fraction \(\) of inputs receive amortized explanations. Smaller \(\) implies higher-quality selective explanations but also more computations (Section 3).
* **Combination function** (\(_{h}\)) optimally linearly combines amortized and Monte Carlo explanations to minimize MSE from target explanations (Theorem 1). We propose explanations with initial guess and fit \(_{h}\) to optimize explanation quality (Section 4).

Algorithm 1 describes the procedure to compute the uncertainty metric, selection function, and combination function using the results we describe in Section 3 and 4. Although selective explanations can be applied to any feature attribution method, we focus on Shapley values since they are widely used and most amortized explainers are tailored for them [16; 38; 6]. We discuss how selective explanations can be applied to LIME and provide more details on feature attribution methods in Appendix B. Next, we describe specific feature attribution methods that we use as building blocks for selective explainers of the form (1).

**Shapley Values (SHAP)** is a **target** explainer that attributes a value \(_{i}\) for each feature \(x_{i}\) in \(=(x_{1},...,x_{d})\) which is the marginal contribution of feature \(x_{i}\) if the model was to predict \(\)

\[_{i}(,)=_{S[d]/\{i\}}(d -1\\ |S|)^{-1}(h_{}(_{S\{i\}})-h_{}( _{S})).\] (2)

SHAP has several desirable properties and is widely used. However, as (2) indicates, computing Shapley values and the attribution vector \((,)=(_{1}(,),...,_{d}(, ))\) requires \(2^{d}\) inferences from \(h\), making SHAP impractical for large models where inference is costly. This has motivated several approximation methods for SHAP, discussed next.

**Shapley Value Sampling (SVS)** is a **Monte Carlo** explainer that approximates SHAP by restricting the sum in (2) to \(m\) uniformly sampled permutations of features performing \(n=md+1\) inferences. We denote SVS that samples \(m\) feature permutations as SVS-\(m\).

**Kernel Shap (KS)** is a **Monte Carlo** explainer that approximates Shapley values using the fact that SHAP can be computed by solving a weighted linear regression problem using \(n\) input perturbations resulting in \(n\) inferences. We refer to Kernel Shap using \(n\) inferences as KS-\(n\).

**Stochastic Amortization** is an **amortized** explainer that uses noisy Monte Carlo explanations to learn target explanations. Covert et al.  trained an amortized explainer in a model class \(\) (multilayer perceptrons) \(\) to take \((,)\) and predicts an explanation \((,)(,)\) by minimizing the \(L_{2}\) norm from Monte Carlo explanations \(^{n}(,)\). Specifically, the amortized explainer is given by

\[*{argmin}_{f}_{(, )_{}}\|f(,)-^{n}(, {y})\|_{2}^{2}.\] (3)

**Amortized Shap for LLMs** is a **amortized** explainer similar to stochastic amortization but tailored for LLMs. Yang et al.  train a linear regression on the LLM embeddings \([e_{1}(),...,e_{||}()]\) to minimize the \(L_{2}\) norm from Monte Carlo explanations \(^{n}(,)\) and define the amortized explainer as \((,)=(W_{}e_{1}()+b_{},...,W_{ }e_{||}()+b_{}),\) where \(W_{}\) is a matrix and \(b_{}\).

We use stochastic amortization to produce amortized explainers for tabular datasets and amortized Shap for LLMs to produce explainers for LLM predictions. Both explainers are trained using SVS-12 as \(^{n}\). High-quality and Monte Carlo explanations are computed using the Captum library .

## 3 Selecting Explanations

This section defines key concepts for selective explainers: (i) uncertainty metrics \(s_{h}\) for amortized explanations and (ii) selection functions (\(_{}\)) to predict when amortized explanations closely approximate target explanations based on the uncertainty metrics.

Uncertainty Metrics for High-Dimensional Regression:An uncertainty metric is a function tailored for the model \(h\) that takes \(\) and outputs a real number \(s_{h}()\) that encodes information about the uncertainty of the model \(h\) in the prediction for \(\). Generally, if \(s_{h}()<s_{h}(^{})\) then the model is more confident about the prediction \(h()\) than \(h(^{})\). Existing uncertainty metrics cater to (i) classification  and (ii) one-dimensional regression , but none specifically address high-dimensional regression - which is our case of interest (\(d\)-dimensional explanations). Next, we propose two uncertainty metrics tailored to high-dimensional outputs: (i) Deep uncertainty and (ii) Learned uncertainty.

**Deep Uncertainty** is inspired by deep ensembles , a method that uses an ensemble of models to provide confidence intervals for the predictions of one model. We run the training pipeline for the amortized explainer described in (3) \(k\) times, each with a different random seed, resulting in \(k\) different amortized explainers \(^{1},...,^{k}\). We define the deep uncertainty as

\[s_{h}^{}()_{i=1}^{d} (^{1}()_{i},...,^{k}()_{i}).\] (4)

Here, \((a_{1},...,a_{k})\) is the variance of the sample \(\{a_{1},...,a_{k}\}\) and \(^{j}()_{i}\) indicates the \(i\)-th entry of the feature attribution vector \(^{j}()\). Hence, deep uncertainty is the average (across entries) of the variance (across all trained amortized explainers) for the predicted attributions.

If the deep uncertainty for a point \(\) is zero, then the amortized explainers produce the same feature attribution. On the other hand, if the deep uncertainty is high, then the feature attributions vary widely across the amortized explainers. Intuitively, the points with a higher deep uncertainty are more affected by a random seed change, implying more uncertainty in the explanation.

While the Deep Uncertainty approach offers a principled method for estimating the uncertainty of the amortized explainer by leveraging an ensemble of \(k\) models, it is computationally expensive due to the need for training, serving, and running multiple models. This overhead can be prohibitive in practice, especially for large-scale applications. To mitigate this issue, we propose _Learned Uncertainty_, which, although less grounded, requires training and serving only a single model.

**Learned Uncertainty** uses data to predict the amortized explainer uncertainty at an input point \(\). We choose \(\) (the loss function) between two explanations to be MSE. The learned uncertainty metric is a function in the class \(\) (multilayer perceptron in our experiments) such that

\[s_{h}^{}*{argmin}_{s}_{(,)_{}}|s()-(( ;),^{n}(;))|^{2}.\] (5)

Ideally, instead of using the Monte Carlo explanation \(^{n}\) as the reference in (5), we would like to use target explanations, i.e., \(((;),(;))\). However, these computationally expensive explanations are usually not available. Thus, we resort to using Monte Carlo explanations.

For large language models, the textual input \(\) is encoded in a sequence of token embedding \([e_{1}(),...,e_{||}()]\) such that \(e_{i}()^{d}\) for \(i[||]\). In this case, we use the mean (i.e., "mean-pooling") of the token embeddings to train the learned uncertainty metric instead of \(\).

We analyze the performance of the proposed uncertainty metrics in Section 5.1, showing that it can be used to detect inaccurate explanations from the amortized explainer. Our results indicate that the proposed uncertainty metrics are (i) strongly correlated with how accurate amortized explanations are and (ii) closely approximate the best possible uncertainty measure - the Oracle with knowledge of the approximation quality (Figure 3). Next, we define the selection function that allows practitioners to set a coverage (percentage of points) \(\) that will receive amortized explanations.

Selection functions:a selection function is the binary qualifier (\(_{}\)) that thresholds the uncertainty metric by \(t_{}\) given by

\[_{}()1&s_{h}() t_{ }\ \ \\ 0&s_{h}()>t_{}\ \ .\] (6)

Intuitively, \(t_{}\) is the maximum uncertainty level tolerated by the user. In practice, if the output of the selection function is \(1\) (high-quality approximations), we use the explanations from the amortized model because it is probably close to the target; if the output of the selection function is \(0\) (low-quality approximations), we use explanations with initial guess (see Definition 2 bellow) to improve the explanation provided to the user. The threshold \(t_{}\) is chosen to be the \(\)-quantile of the uncertainty metric to ensure that at least a fraction \(\) of points receive a computationally cheap explanation - \(\) is the _coverage_. Specifically, given \(\), we calibrate \(t_{}\) in the calibration dataset \(_{}\) and compute it as

\[t_{}_{t}t,_{}[s_{h}()  t],\] (7)

where \(_{}\) is the empirical distribution of the calibration dataset. For discussions on selecting coverage with guarantees on the number of inferences for selective explanations, see Appendix C.

**Remark 2**.: A property of selective predictions , which is transferred to selective explanations, is that it is possible to control the explainer's performance via the threshold \(t_{}\) with guaranteed performance without providing predictions for all points. This result is displayed in Figure 3.

## 4 Explanations with Initial Guess

We have introduced methods to detect points likely to receive amortized explanations that poorly approximate the target. This raises the question: _How can we improve the explanations for these points?_ One approach is to simply use Monte Carlo (MC) explanations instead of amortized ones. However, this ignores potentially valuable information already computed by the amortized explainer. In this section, we propose a more effective solution called _explanations with initial guess_, which combines amortized and Monte Carlo explanations to improve quality.

**Explanation with Initial Guess** uses an optimized linear combination of the amortized explanation with a more computationally expensive method - the Monte Carlo explainer - to improve the quality of the explanation. We formally define _explanations with initial guess_ next.

**Definition 2** (Explanation with Initial Guess).: Given a Monte Carlo explainer \(^{n}(,)\), and a combination function \(_{h}:^{d}\) that reflects the quality of the amortized explanation \(\), we define the explanation with initial guess as

\[(,)_{h}()(,)+(1-_{h}())^{n}(,).\] (8)

Recall that when \(_{}()=0\), selective explanations use the explanation with initial guess (1) to improve low-quality amortized explanations, i.e., \((,)=(,)\).

Defining explanations with initial guess as the linear combination between the amortized and the Monte Carlo explanations is inspired by the literature on shrinkage estimators [21; 20] that use an initial guess (\((,)\) in our case) to improve the estimation MSE in comparison to only using the empirical average (a role played by \(^{h}(,)\) in our case). Next, we tune \(_{h}\) to minimize the MSE from target explanations.

**Optimizing the Explanation Quality:** Our goal is for explanations with initial guess to approximate the target explanations, i.e., \(||(,)-(,)||\). To achieve this goal, we optimize the function \(_{h}\) as follows.

First, since \(\) is unavailable, we use another Monte Carlo explanation \(^{n^{}}\) to approximate \(\). \(^{n^{}}\) is different from \(^{n}\) and potentially more computationally expensive but not necessarily. Importantly, \(^{n^{}}\) is only needed beforehand when computing \(_{h}\), not at prediction time. In our experiments, we use SVS-12 for \(^{n^{}}\).

Second, we quantize the range of the uncertainty metric \(s_{h}\) into bins to aggregate points with similar uncertainty and define the bins \(Q_{i}\) by a partition \(0=_{1}<_{2}<...<_{m}=1\) of \(\):

\[Q_{i}[t_{_{i}},t_{_{i+1}}),\ \  i[m-1]\] (9)where \(t_{_{i}}\) is defined as in (7). We then define the combination function to be

\[_{h}()=_{i}s_{h}() Q_{i},\] (10)

\(_{h}\) is chosen to optimize the explanation-quality for points with similar uncertainty, \(_{i}\) is given by:

\[_{i}*{argmin}_{}_{ (,)_{}\\ s_{h}() Q_{i}}(, )-^{n^{}}(,)_{2}^{2}.\] (11)

The constant \(_{i}\) is only computed once per bin and stored. At explanation time, when we provide explanations with initial guess (i.e., when \(_{}()=0\)) (8), we lookup the bin for the point being explained and use the associated \(_{i}\).

Theorem 1 provides a closed-form solution for \(_{i}\).

**Theorem 1** (Optimal \(_{h}\)).: _Let \(0=_{1}<_{2}<...<_{m}=1\) and define \(Q_{i}\) as in (9). Then the solution to the optimization problem in (11) is given by_

\[_{i}=(,) _{}\\ s_{h}() Q_{i}}(^{n}(,)-^{n^{}}(,),^{n}(,)-(,))}{ _{(,)_{}\\ s_{h}() Q_{i}}||(,)- ^{n}(,)||_{2}^{2}}.\] (12)

The range of uncertainty functions is **quantized** for two main reasons. First, the uncertainty metric \(s_{h}\) encodes the amortized explainer's uncertainty for each point \(\). This uncertainty quantification should be reflected in the choice of \(_{h}\). Quantizing the range of \(s_{h}\) allows us to group points with similar uncertainty levels and optimize \(_{h}\) for each group separately. Second, quantizing the range of \(s_{h}\) enables us to have multiple point per bin \(Q_{i}\) allowing us to compute \(_{i}\) to minimize the MSE in each bin.

We use the **Monte Carlo** explainer \(^{n^{}}\) because: (i) as mentioned above, we assume we don't have access to target explanations due to computational cost and (ii) even when using this Monte Carlo explainer, we show that in all bins, \(_{i}\) approximates well the optimal combination function computed assuming access to target explanations from Target defined as

\[_{i}^{}=*{argmin}_{}_{ (,)_{}\\ s_{h}() Q_{i}}||(,)- (,)||_{2}^{2}.\]

Specifically, Theorem 2 shows that \(_{i}_{i}^{}\) with high probability. Appendix E shows the formal version of the Theorem along with the proofs for all results in this section.

**Theorem 2** (**Informal \(_{i}_{i}^{}\)).: _If (i) \(^{n}\) is sufficiently different from the amortized explainer Amor and (ii) \(^{n^{}}\) approximates the target explanations Target then \(_{i}\) and \(_{i}^{}\) are close with high-probability for all bins \(Q_{i}\), i.e.,_

\[|_{i}-_{i}^{}|1-e^{-C|Q_{i}|}.\]

_for a \(C>0\) and \(|Q_{i}|\) is the number of points in the validation dataset \(_{}\) that are in bin \(Q_{i}\)._

## 5 Experimental Results

This section analyzes the performance of selective explanations and its different components (i) uncertainty measures and (ii) explanations with initial guess. All results are showed in terms of MSE from target explanations, check Appendix D for the same results using Spearman's Rank Correlation.

Experimental Setup:We generate selective explanations and evaluate their MSE and Spearman's correlation compared to the target explanation computed using a large number of inferences1. Although our results hold for any feature and data attribution method, in this section, we focus on Shapley values due to its frequent use and prevalence in the literature on amortized explainers [16; 6; 38]. Seaborn  is used to compute \(95\%\) confidence intervals using the bootstrap method.

[MISSING_PAGE_FAIL:8]

Explanations with Initial Guess vs. Monte CarloIn Figure 4 we compare selective explanations improving quality of non-covered points using (i) explanations with initial guess and (ii) Monte Carlo explanations, when amortized explanations are inaccurate (\(_{h}=0\)). **Case 1:** When the MSE from the Monte Carlo is smaller than from the amortized explainer ((a) and (c)), explanations with initial guess results in a smaller MSE compared to only using Monte Carlo. **Case 2:** When the MSE in Monte Carlo is larger than the amortized explanation MSE ((b) and (d)), only using Monte Carlo increase the MSE while explanations with initial guess reduces the MSE. Together, Cases 1 and 2 suggest that even when lower quality, explanations contain valuable information that can be leveraged by explanations with initial guess to improve explanation quality.

### Efficacy of Selective Explanations

Worst Case Performance Improvement:Figure 5 shows the MSE of selective explanations for the points receiving the highest MSEs. The figure suggests that selective explanations significantly decrease the worst-case MSE of amortized explanations. With just 20% coverage the MSE decreases consistently across datasets. Remarkably, when providing explanations with initial guess for \(20\%\) of the samples in the Yelp dataset (Figure 5 (c)), selective explanations result in MSE for the worst \(5\%\) of points that is about \(30\%\) smaller than the original amortized explanations - this is even more pronounced in the UCI datasets.

Improved Inferences vs. Quality Trade-off:Figure 6 presents the trade-off between number of inferences per explanation and MSE from target explanations using selective and Monte Carlo explanations. The MSE decreases with the number of inferences and selective explanations Pareto dominates Monte Carlo explanations. We also show an "Oracle" that knows a priori how to optimally

Figure 4: Fraction (\(1-\)) of points which explanations receive additional computations (x-axis) vs. MSE of selective explanations w.r.t. target explanations (y-axis) with coverage \(\). Naive uses \(_{h}=0\) while Initial guess uses \(_{h}\) in (12). MSE is computed across all points in the test dataset. Yelp Review and Toxigen use SVS (12) as Monte Carlo explanations while UCI-Adult and UCI-News use KS (32).

Figure 5: MSE for the 5% of explanations with the highest MSE in the test dataset (y-axis) for selective explanations with varying fraction of points with extra computations (x-axis). Selective explanations are shown in (i) black solid bar using the Learned uncertainty and (ii) striped black bar using Deep uncertainty. Dashed red line shows the MSE of amortized explanations in worst 5% explanations.

route samples in terms of MSE and inferences. We simulate this oracle by pre-computing SVS explanations with parameters 12, 25, and 50, and selecting the one with the smallest MSE from the target SHAP explanation while mantaining the average number inferences shown in x-axis.

Improved Local Fidelity:Figure 7 shows that selective explanations increase the local fidelity of the amortized explainer and that local fidelity increases with the fraction of points that receive additional computations (recourse). Both Yelp and Toxigen models receive explanations with initial guess using SVS-12.

## 6 Final Remarks

Conclusion:We propose _selective explanations_ that first identify which inputs would receive a low-quality but computationally cheap explanation (amortized) and then perform additional model inferences to improve the quality of these explanations. We propose _explanations with initial guess_ to improve the quality of explanations by combining amortized explanations with more expensive explanations Monte Carlo using an optimized combination function, improving the explanation performance. Selective explanations provide a new framework for approximating expensive feature attribution methods. Our experiments indicate that selective explanations (i) efficiently identify points that the amortized explainer would produce low-quality explanations, (ii) improves the quality of the worst-quality amortized explanations, (iii) improves the trade-off between computational cost and explanation quality, and (iv) improves the local fidelity of amortized explanations.

**Limitations:** Selective explanations can be applied to any feature attribution method for which amortized and Monte Carlo explainers were developed. However, our empirical results focus on Shapley values. We leave the application of selective explanations to other attribution methods for future work. Additionally, we do not explore image classifiers, which may also interest the interpretability community. Also, we do not explore selective explanations for Generative Language models due to the lack of amortized explainers for such application.

Figure 6: Number of inferences (x-axis) vs. MSE (y-axis). Black curve shows the performance of selective explanations using Learned uncertainty. Purple curve connects Shapley Value Sampling (SVS) with parameters 12, 25, and 50 sequentially until all samples receive SVS-50 explanations and amortized explanations. The red curve is a the Oracle that optimally trades off MSE and inferences.

Figure 7: Model accuracy (y-axis) when removing the tokens with the highest attribution scores according to the amortized explainer (black), selective explanations with varying coverage and target explanations (red).

Acknowledgments

The authors thank Amit Dhurandhar for early discussions on the trustworthiness of amortized explainers. This material is based upon work supported by the National Science Foundation under awards CAREER-1845852, CIF-1900750, CIF-2231707, and CIF-2312667, FAI-2040880, and also an Apple Scholar Fellowship. The views expressed here are those of the authors and do not reflect the official policy or position of the funding agencies.