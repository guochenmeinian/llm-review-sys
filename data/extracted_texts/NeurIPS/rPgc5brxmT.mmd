# Interaction-Force Transport Gradient Flows

Egor Gladin

Humboldt University of Berlin

Berlin, Germany

& HSE University

egorgladin@yandex.ru

& Pavel Dvurechensky

Weierstrass Institute for

Applied Analysis and Stochastics

Berlin, Germany

pavel.dvurechensky@wias-berlin.de

Alexander Mielke

Humboldt University of Berlin

& WIAS

Berlin, Germany

alexander.mielke@wias-berlin.de

Jia-Jie Zhu

Weierstrass Institute for

Applied Analysis and Stochastics

Berlin, Germany

jia-jie.zhu@wias-berlin.de

Corresponding author: Jia-Jie Zhu

###### Abstract

This paper presents a new gradient flow dissipation geometry over non-negative and probability measures. This is motivated by a principled construction that combines the unbalanced optimal transport and interaction forces modeled by reproducing kernels. Using a precise connection between the Hellinger geometry and the maximum mean discrepancy (MMD), we propose the interaction-force transport (IFT) gradient flows and its spherical variant via an infimal convolution of the Wasserstein and spherical MMD tensors. We then develop a particle-based optimization algorithm based on the JKO-splitting scheme of the mass-preserving spherical IFT gradient flows. Finally, we provide both theoretical global exponential convergence guarantees and improved empirical simulation results for applying the IFT gradient flows to the sampling task of MMD-minimization. Furthermore, we prove that the spherical IFT gradient flow enjoys the best of both worlds by providing the global exponential convergence guarantee for both the MMD and KL energy.

## 1 Introduction

Optimal transport (OT) distances between probability measures, including the earth mover's distance (Werman et al., 1985; Rubner et al., 2000) and Monge-Kantorovich or Wasserstein distance (Villani, 2008), are one of the cornerstones of modern machine learning as they allow performing a variety of machine learning tasks, e.g., unsupervised learning (Arjovsky et al., 2017; Bigot et al., 2017), semi-supervised learning (Solomon et al., 2014), clustering (Ho et al., 2017), text classification (Kusner et al., 2015), image retrieval, clustering and classification (Rubner et al., 2000; Cuturi, 2013; Sandler and Lindenbaum, 2011), and distributionally robust optimization (Sinha et al., 2020; Mohajerin Esfahani and Kuhn, 2018). Many recent works in machine learning adopted the techniques from PDE gradient flows over optimal transport geometries and interacting particle systems for inference and sampling tasks. Those tools not only add new interpretations to the existing algorithms, but also provide a new perspective on designing new algorithms.

For example, the classical Bayesian inference framework minimizes the Kulback-Leibler divergence towards a target distribution \(\). From the optimization perspective, this can be viewed as solving

\[_{ A}F():=_{}(|) },\] (1)

where \(A\) is a subset of the space of probability measures \(\), e.g., the Gaussian family. The Wasserstein gradient flow of the KL gives the Fokker-Planck equation, which can be simulated using the Langevin SDE for MCMC. Beyond the KL, many researchers following Arbel et al. (2019) advocated using the squared MMD instead as the driving energy for the Wasserstein gradient flows for sampling. However, in contrast to the KL setting, there is little sound convergence analysis for the MMD-minimization scheme like the celebrated Bakry-Emery Theorem. Furthermore, it was shown, e.g., in (Korba et al., 2021), that Arbel et al. (2019)'s algorithm suffers a few practical drawbacks. For example, their particles tend to collapse around the mode or get stuck at local minima, and the algorithm requires a heuristic noise injection strategy that is tuned over the iterations; see Figure 1 and SS4 for illustrations. Subsequently, many such as Carrillo et al. (2019); Chewi et al. (2020); Korba et al. (2021); Glaser et al. (2021); Craig et al. (2023); Hertrich et al. (2023); Neumayer et al. (2024) proposed modified energies to be used in the Wasserstein gradient flows. In contrast, this paper does not propose new energy objectives. Instead, we propose a new gradient flow geometry - the \(\) gradient flows. To summarize, our main contributions are:

1. We propose the interaction-force transport (\(\)) gradient flow geometry over non-negative measures and spherical \(\) over probability measures, constructed from the first principles of the reaction-diffusion type equations, previously studied in the context of the Hellinger-Kantorovich (Wasserstein-Fisher-Rao) distance and gradient flows. It was first studied by three groups including Chizat et al. (2018, 2019); Liero et al. (2018); Kondratyev et al. (2016); Gallouet and Monsaingeon (2017). Our \(\) gradient flow is based on the inf-convolution of the Wasserstein and the newly constructed spherical MMD Riemannian metric tensors. This new unbalanced gradient flow geometry allows teleporting particle mass in addition to transportation, which avoids the flow getting stuck at local minima; see Figure 1 for an illustration.
2. We provide theoretical analysis such as the _global exponential decay_ of energy functionals via the Polyak-Lojasiewicz type functional inequalities. As an application, we provide the first global exponential convergence analysis of \(\) for both the MMD and KL energy functionals. That is, the \(\) gradient flow enjoys the best of both worlds.
3. We provide a new algorithm for the implementation of the \(\) gradient flow. We then empirically demonstrate the use of the \(\) gradient flow for the MMD inference task. Compared to the original MMD-energy-flow algorithm of Arbel et al. (2019), \(\) flow does not suffer issues such as the collapsing-to-mode issue. Leveraging the first-principled spherical \(\) gradient flow, our method does not require a heuristic noise injection that is commonly tuned over the iterations in practice; see (Korba et al., 2021) for a discussion. Our method can also be viewed as addressing a long-standing issue of the kernel-mean embedding methods (Smola et al., 2007; Muandet et al., 2017; Lacoste-Julien et al., 2015) for optimizing the support of distributions.

**Notation** We use the notation \((),^{+}()\) to denote the space of probability and non-negative measures on the closed, bounded, (convex) set \(^{d}\). The base space symbol \(\) is often dropped if there is no ambiguity in the context. We note also that many of our results hold for \(=^{d}\). In this paper, the first variation of a functional \(F\) at \(^{+}\) is defined as a function \([]\) such that \(}{}F(+ v)|_{=0}= [](x)\ v(x)\) for any valid perturbation in measure \(v\) such that \(+ v^{+}\) when working with gradient flows over \(^{+}\) and \(+ v\) over \(\). We often

Figure 1: (Left) Wasserstein flow of the MMD energy (Arbel et al., 2019). Some particles get stuck at points away from the target. (Right) \(\) gradient flow (this paper) of the MMD energy. Particle mass is teleported to close to the target, avoiding local minima. Hollow circles indicate particles with zero mass. The red dots are the initial particles, and the green dots are the target distribution. See §4 for more details.

omit the time index \(t\) to lessen the notational burden, e.g., the measure at time \(t\), \((t,)\), is written as \(\). The infimal convolution (inf-convolution) of two functions \(f,g\) on Banach spaces is defined as \((f g)(x)=_{y}\{f(y)+g(x-y)\}\). In formal calculation, we often use measures and their density interchangeably, i.e., \( f\) means the integral w.r.t. the measure \(\). For a rigorous generalization of flows over continuous measures to discrete measures, see (Ambrosio et al., 2005). \(_{2}k(,)\) denotes the gradient w.r.t. the second argument of the kernel.

## 2 Background

### Gradient flows of probability measures for learning and inference

Gradient flows are powerful tools originated from the field of PDE. The intuition can be easily seen from the perspective of optimization as solving the variational problem

\[_{ A^{+}()}F()\]

using a "continuous-time version" of gradient descent, over a suitable metric space and, in particular, Riemannian manifold. Since the seminal works by Otto (1996) and colleagues, one can view many PDEs as gradient flows over the aforementioned Wasserstein metric space, canonically denoted as \((_{2}(),W_{2})\); see (Villani, 2008; Santambrogio, 2015) for a comprehensive introduction.

Different from a standard OT problem, a gradient flow solution traverses along the path of the fastest dissipation of the energy \(F\) allowed by the corresponding geometry. In this paper, we are only concerned with the geometries with a (pseudo-)Riemannian structure, such as the Wasserstein, (spherical) Hellinger or Fisher-Rao geometries. In such cases, a formal Otto calculus can be developed to greatly simplify the calculations. For example, the Wasserstein Onsager operator (which is the inverse of the Riemannian metric tensor) \(_{W}():T_{}^{*}^{+} T_{}^{+}, -()\), where \(T_{}^{+}\) is the tangent plane of \(^{+}\) at \(\) and \(T_{}^{*}^{+}\) the cotangent plane. Using this notation, a Wasserstein gradient flow equation of some energy \(F\) can be written as

\[=-_{W}()=( ).\] (2)

In essence, many machine learning applications are about making different choices of the energy \(F\) in (2), e.g., the KL, \(^{2}\)-divergence, or MMD. However, Wasserstein and its flow equation (2) are by no means the only meaningful geometry for gradient flows. One major development in the field is the Hellinger-Kantorovich a.k.a. the Wasserstein-Fisher-Rao (WFR) gradient flow. The WFR gradient flow equation is given by the reaction-diffusion equation, for some scaling coefficients \(,>0\),

\[=(u)- u .\] (3)

A few recent works have applied WFR to sampling and inference (Yan et al., 2024; Lu et al., 2019) by choosing the energy functional to be the KL divergence.

### Reproducing kernel Hilbert space and MMD

In this paper, we refer to a bi-variate function \(k:\) as a symmetric positive definite kernel if \(k\) is symmetric and, for all \(n,_{1},,_{n}}\) and all \(x_{1},,x_{n}\), we have \(_{i=1}^{n}_{j=1}^{n}_{i}_{j}k(x_{j},x_{i}) 0\). \(k\) is a reproducing kernel if it satisfies the reproducing property, i.e., for all \(x\) and all functions in a Hilbert space \(f\), we have \(f(x)= f,k(,x)_{}\). Furthermore, the space \(\) is an RKHS if the Dirac functional \(_{x}:,_{x}(f):=f(x)\) is continuous. It can be shown that there is a one-to-one correspondence between the RKHS \(\) and the reproducing kernel \(k\). Suppose the kernel is square-integrable \(\|k\|_{L^{2}_{}}^{2}:= k(x,x)d(x)<\) w.r.t. \(\). The integral operator \(_{k,}:L^{2}_{}\) is defined by \(_{k,}g(x):= k(x,x^{})g(x^{} )d(x^{})\) for \(g L^{2}_{}\). With an abuse of terminology, we refer to the following composition also as the integral operator

\[_{}:=_{k,},\ L^{2}()  L^{2}().\]

\(_{}\) is compact, positive, self-adjoint, and nuclear; cf. (Steinwart and Christmann, 2008). To simplify the notation, we simply write \(\) when \(\) is the Lebesgue measure.

The kernel maximum mean discrepancy (MMD) (Gretton et al., 2012) emerged as an easy-to-compute alternative to optimal transport for computing the distance between probability measures, i.e., \(^{2}(,):=\|(-)\|_{}^{2}= k(x,x^{})\;(-)(x)\;(-)(x^{ })\;,\) where \(\) is the RKHS associated with the (positive-definite) kernel \(k\). While the MMD enjoys many favorable properties, such as a closed-form estimator and favorable statistical properties (Tolstikhin et al., 2017, 2016), its mathematical theory is less developed compared to the Wasserstein space especially in the geodesic structure and gradient flow geometries. It has been shown by Zhu and Mielke (2024) that MMD is a (de-)kernelized Hellinger or Fisher-Rao distance by using a dynamic formulation

\[^{2}(,)=\{_{0}^{1}\|_{t}\|_{}^{2} \;t\;|\;=-^{-1}_{t},u(0)=,u(1)=,\; _{t}\}.\] (4)

Mathematically, we can obtain the MMD geodesic structure if we kernelize the Hellinger (Fisher-Rao) Riemannian metric tensor,

\[_{}=_{}_{}( ),_{}=_{}()_{}^{-1},\] (5)

noting that the Onsager operator \(\) is the inverse of the Riemannian metric tensor \(=^{-1}\). The MMD suffers from some shortcomings in practice, such as the vanishing gradients and kernel choices that require careful tuning; see e.g., (Feydy et al., 2019). Furthermore, a theoretical downside of the MMD as a tool for optimizing distributions, and kernel-mean embedding (Smola et al., 2007, Muandet et al., 2017) in general, is that they do not allow _transport_ dynamics. This limitation is manifested in practice, e.g., it is intractable to optimize the location of particle distributions; see e.g. (Lacoste-Julien et al., 2015). In this paper, we address all those issues.

## 3 \(\) gradient flows over non-negative and probability measures

In this section, we propose the \(\) gradient flows over non-negative and probability measures. Note that our methodology is fundamentally different from a few related works in kernel methods and gradient flows such as (Arbel et al., 2019, Korba et al., 2021, Hertrich et al., 2023, Glaser et al., 2021, Neumayer et al., 2024) in that we are not concerned with the Wasserstein flows of a different energy, but a new gradient flow dissipation geometry.

### (Spherical)

\(\) **gradient flow equations over non-negative and probability measures**

The construction of the Wasserstein-Fisher-Rao gradient flows crucially relies on the inf-convolution from convex analysis (Liero et al., 2018, Chizat, 2022). There, the WFR metric tensor is defined using an inf-convolution of the Wasserstein tensor and the Hellinger (Fisher-Rao) tensor \(_{}()=_{W}()_{}()\). By Legendre transform, its inverse, the Onsager operator, is given by the sum \(_{}()=_{W}()+_{}()\). Therefore, we construct the \(\) gradient flow by replacing the Hellinger (Fisher-Rao) tensor with the MMD tensor, as in (5).

\[_{}()=_{W}()_{}(),_{}()=_{W}()+_{ }().\] (6)

The MMD gradient flow equation is derived by Zhu and Mielke (2024) using the Onsager operator (5),

\[=-_{}()[ ]=-^{-1}[].\] (7)

Figure 2: Illustration of the \(\) gradient flow. Atoms are subject to both the transport (Kantorovich) potential and the interaction (repulsive) force from other atoms.

Hence, we obtained the desired IFT gradient flow equation using (6).

\[=-_{W}()[]- _{}()[]= ([])- ^{-1}[].\] (8)

Formally, the IFT gradient flow equation can also be viewed as a kernel-approximation to the Wasserstein-Fisher-Rao gradient flow equation, i.e., the reaction-diffusion equation (3).

**Corollary 3.1**.: _Suppose \( k_{}(x,)\ =1\) and the kernel-weighted-measure converges to the Dirac measure \(k_{}(x,)\ \ _{x}\) as the bandwidth \( 0\). Then, the IFT gradient flow equation (8) tends towards the WFR gradient flow equation as \( 0\), i.e., the reaction-diffusion equation (3)._

Like the WFR gradient flow over non-negative measures, the gradient flow equation (8) and (7) are not guaranteed to stay within the probability measure space, i.e., total mass \(1\). This is useful in many applications such as chemical reaction systems. However, probability measures are often required for machine learning applications. We now provide a mass-preserving gradient flow equation that we term the _spherical_ IFT _gradient flow_. The term spherical is used to emphasize that the flow stays within the probability measure, as in the spherical Hellinger distance (Laschos and Mielke, 2019).

To this end, we must first study _spherical MMD_ flows over probability measures. Recall that (7) is a Hilbert space gradient flow (see (Ambrosio et al., 2005)) and does not stay within the probability space. Closely related, many works using kernel-mean embedding (Smola et al., 2007; Muandet et al., 2017) also suffer from this issue of not respecting the probability space. To produce a restricted (or projected) flow in \(\), our starting point is the _MMD minimizing-movement scheme_ restricted to the probability space

\[^{k+1}*{argmin}_{}F()+^{2}(,^{k}).\] (9)

We now derive the following mass-preserving spherical gradient flows for the MMD and IFT.

**Proposition 3.2** (Spherical MMD and spherical IFT gradient flow equations).: _The spherical MMD gradient flow equation is given by (where \(1\) denotes the constant scalar)_

\[=-^{-1}([]- ^{-1}[]}{ ^{-1}1}).\] (10)

_Consequently, the spherical IFT gradient flow equation is given by_

\[=([ ])-^{-1}([ ]-^{-1}[ ]}{^{-1}1}).\] (11)

_Furthermore, those equations are mass-preserving, i.e., \(=0\)._

So far, we have identified the gradient flow equations of interest. Now, we are ready to present our main theoretical results on the convergence of the IFT gradient flow via functional inequalities. For example, the logarithmic Sobolev inequality (LSI)

\[\|}{}\|_{L^{2}()}^{2}  c_{}_{}(|)c_{}>0\] (LSI)

is sufficient to guarantee the convergence of the pure Wasserstein gradient flow of the KL divergence energy, which governs the same dynamics as the Langevin equation. The celebrated Bakry-Emery Theorem (Bakry and Emery, 1985), is a cornerstone of convergence analysis for dynamical systems as it provides an explicit sufficient condition: suppose the target measure \(\) is \(\)-log concave for some \(>0\), then the global convergence is guaranteed, i.e.,

\[=e^{-V}\ x^{2}V c_{}=2.\]

The question we answer below is whether the IFT gradient flow enjoys such favorable properties. Our starting point is the (Polyak-)Lojasiewicz type functional inequality.

**Theorem 3.3**.: _Suppose the following Lojasiewicz type inequality holds for some \(c>0\),_

\[\|[]\|_{L_{ }^{2}}^{2}+\|[]\| _{}^{2} c(F((t))-_{}F())\] (IFT-Loj)

_for the \(\) gradient flow, or_

\[\|[]\|_{L_ {}^{2}}^{2}+\|[]- ^{-1}[]}{ ^{-1}1}\|_{}^{2} c(F((t))-_{ }F())\] (SIFT-Loj)

_for the spherical \(\) gradient flow. Then, the energy \(F\) decays exponentially along the corresponding gradient flow, i.e., \(F((t))-_{}F() e^{-ct}(F((0))-_{}F())\)._

To understand a specific gradient flow, one must delve into the detailed analysis of the conditions under which the functional inequalities hold instead of assuming them to hold by default. We provide such analysis for the \(\) gradient flows next.

### Global exponential convergence analysis

MMD energy functionalAs discussed in the introduction, the MMD energy has been proposed as an alternative to the KL divergence energy for sampling by Arbel et al. (2019)2, where they assume the access to samples from the target measure \(y_{i}\). However, the theoretical convergence guarantees under the MMD energy is a less-exploited topic. Those authors characterized a local decay behavior under the assumption that the \(_{t}\) must already be close to the target measure \(\) for all \(t>0\). The assumptions they made are not only restrictive, but also difficult to check. There has also been no global convergence analysis. For example, Arbel et al. (2019)'s Proposition 2 states that the MMD is non-increasing, which is not equivalent to convergence and is easily satisfied by other flows. The mathematical limitation is that the MMD is in general not guaranteed to be convex along the Wasserstein geodesics. In addition, our analysis also does not require the heuristic noise injection step as was required in their implementation. Moroueh and Rigotti (2020) also used the MMD energy but with a different gradient flow, which has been shown by Zhu and Mielke (2024) to be a kernel-regularized inf-convolution of the Allen-Cahn and Cahn-Hilliard type of dissipation. However, Moroueh and Rigotti (2020)'s convergence analysis is not sufficient for establishing (global) exponential convergence as no functional inequality has been established there. In contrast, we now provide full global exponential convergence guarantees.

We first provide an interesting property that will become useful for our analysis.

**Theorem 3.4**.: _Suppose the driving energy is the squared MMD, \(F()=^{2}(,)\) and initial datum \(_{0}\) is a probability measure. Then, the spherical MMD gradient flow equation (10) coincides with the MMD gradient flow equation_

\[=-(-),\] (MMD-MMD-GF)

_whose solution is a linear interpolation between the initial measure \(_{0}\) and the target measure \(\), i.e.,_

\[_{t}=e^{-t}_{0}+(1-e^{-t}).\]

_Furthermore, same coincidence holds for the spherical \(\) and \(\) gradient flow equation_

\[=(_{2}k(x,)\;(-)(x))-(-).\] (MMD- \[\] -GF)

The explicit solution to the ODE (MMD-MMD-GF) shows an exponential convergence to the target measure \(\) along the (spherical) MMD gradient flow. The (spherical) \(\) gradient flow equation (MMD-IFT-GF) differs from the Wasserstein gradient flow equation of (Arbel et al., 2019) by a linear term. This explains the intuition of why we can expect good convergence properties for the \(\) gradient flow of the squared MMD energy. We exploit this feature of the \(\) gradient flow to show global convergence guarantees for inference with the MMD energy. This has not been possible previously when confined to the pure Wasserstein gradient flow.

**Theorem 3.5** (Global exponential convergence of the \(\) flow of the MMD energy).: _Suppose the energy \(F\) is the squared MMD energy \(F()=^{2}(,)\). Then, the (\(\)-Loj) holds globally with a constant \(c 2>0\)._

_Consequently, for any initialization within the non-negative measure cone \(_{0}^{+}\), the squared MMD energy decays exponentially along the \(\) gradient flow of non-negative measures, i.e.,_

\[^{2}(_{t},) e^{-2 t}^{2}(_{0},).\] (12)

_Furthermore, if the initial datum \(_{0}\) and the target measure \(\) are probability measures \(_{0},\), then the squared MMD energy decays exponentially globally along the spherical \(\) gradient flow, i.e., the decay estimate (12) holds along the spherical \(\) gradient flow of probability measures._

We emphasize that no Bakry-Emery type or kernel conditions are required - the Lojasiewicz inequality holds globally when using the \(\) flow. In contrast, the Wasserstein-Fisher-Rao gradient flow

\[=(_{2}k(x,) (-)(x))- k(x,) (-)(x)\] (MMD-WFR-GF)

does not enjoy such global convergence guarantees.

Global exponential convergence under the KL divergence energyFor variational inference and MCMC, a common choice of the energy is the KL divergence energy, i.e., \(F()=}(|)\). This has already been studied by a large body of literature, including the case of Wasserstein-Fisher-Rao (Liero et al., 2023; Lu et al., 2019). Not surprisingly, (LSI) is still sufficient for the exponential convergence of the WFR type of gradient flows since the dissipation of the Wasserstein part alone is sufficient for driving the system to equilibrium. For the \(\) gradient flows under the KL divergence energy functional, the convergence can still be established. This showcases the strength of the \(\) geometry - it enjoys the best of both worlds. The \(\) gradient flow equation of the KL divergence energy reads \(=( }{})-^{-1}}{}\). Unlike the MMD-energy flow case, the spherical \(\) gradient flow of the KL over probability measures \(\) no longer coincides with that of the (non-spherical) \(\) and is given by

\[=( }{})-^{-1}(}{}-^{-1} }{}}{^{-1}1} ).\] (13)

**Proposition 3.6** (Exponential convergence of the \(\) gradient flow of the KL divergence energy).: _Suppose the (LSI) holds with \(c_{}=2\) or the target measure \(\) is \(\)-log concave for some \(>0\). Then, the KL divergence energy decays exponentially globally along the spherical \(\) gradient flow (13),i.e., \(}(_{t}|) e^{-2 t}}(_{0}|)\)._

The intuition behind the above result is that the \(\) gradient flow converges whenever the pure Wasserstein gradient flow, i.e., its convergence is at least as fast as the Wasserstein gradient flow. However, we emphasize that the decay estimate of the KL divergence energy only holds along the spherical \(\) flow over probability measures \(\), but not the full \(\) flow over non-negative measures \(^{+}\).

### Minimizing movement, JKO-splitting, and a practical particle-based algorithm

In applications to machine learning and computation, continuous-time flow can be discretized via the JKO scheme (Jordan et al., 1998), which is based on the minimizing movement scheme (MMS) (De Giorgi, 1993). For the reaction-diffusion type gradient flow equation in the Wasserstein-Fisher-Rao setting, the _JKO-splitting_ a.k.a. _time-splitting_ scheme has been studied by Gallouet and Monsaingeon (2017); Mielke et al. (2023). This amounts to splitting the diffusion (Wasserstein) and reaction (MMD) step in (8), i.e., at time step \( 1\)

\[^{+} *{argmin}_{}F()+W_{2}^{2}(,^{}),\] (Wasserstein step) (14) \[^{+1} *{argmin}_{}F()+^{2}(,^{+}).\] (MMD step)A similar JKO-splitting scheme can also be constructed via the WFR gradient flow, which amounts to replacing the MMD step in (14) with a proximal step in the KL (as an approximation to the Hellinger), i.e., \(^{+1}*{argmin}_{}F()+ {}}(|^{+})\), which is well-studied in the optimization literature as the entropic mirror descent (Nemirovskij and Yudin, 1983). Our MMD step can also be viewed as a mirror descent step with the mirror map \(\|\|_{}^{2}\). However, for the task of MMD inference of (Arbel et al., 2019), WFR flow does not possess convergence guarantees such as our Theorem 3.4. The MMD step can also be easily implemented as in our simulation.

We summarize the resulting overall \(\) particle gradient descent from the JKO splitting scheme in Algorithm 1 in the appendix. We now look at those two steps respectively. For concreteness, we consider a flexible particle approximation to the probability measures, with possibly non-uniform weights allocated to the particles, i.e., \(=_{i=1}^{n}_{i}_{x_{i}},^{n},\;x_{i} \).

Wasserstein step: particle position update.(14) is a standard JKO scheme; see, e.g., (Santambrogio, 2015). The optimality condition of the Wasserstein proximal step can be implemented using a particle gradient descent algorithm

\[x_{i}^{+1}=x_{i}^{}-[^{ }](x_{i}^{}),\;i=1,...,n,\] (15)

which is essentially the algorithm proposed by Arbel et al. (2019) when \(F()=^{2}(,)\).

MMD step: particle weight update.The MMD step in (14) is a discretization step of the spherical MMD gradient flow, as shown in (9) and Proposition 3.2. We propose to use the updated particle location \(x_{i}^{+1}\) from the Wasserstein step (15) and update the weights \(_{i}\) by solving

\[_{^{n}}F(_{i=1}^{n}_{i}_{x_{i}^{+1}})+ ^{2}(_{i=1}^{n}_{i}_{x_{i}^{ +1}},_{i=1}^{n}_{i}^{}_{x_{i}^{+1}}),\]

i.e., the MMD step only updates the weights. Alternatively, as in the classical mirror descent optimization, one can use a linear approximation \(F() F(^{})+[^{} ],-^{}_{L^{2}}\). We also provide a specialized discussion on the MMD-energy minimization task of (Arbel et al., 2019). Let the energy objective be the squared MMD \(F():=(,)^{2}\). In this setting, we are given the particles sampled from the target measure \(y^{i}\). For the MMD step in (14), the computation is drastically simplified to an MMD barycenter problem, which was also studied in (Cohen et al., 2021). This amounts to solving a convex quadratic program with a simplex constraint; see the appendix for the detailed expression.

## 4 Numerical Example

The overall goal of the numerical experiments is to approximate the target measure \(\) by minimizing the squared MMD energy, i.e., \(_{}^{2}(,)\). In all the experiments, we have access to the target measure \(\) in the form of samples \(y_{i}\). This setting was studied in (Arbel et al., 2019) as well as in many deep generative model applications. In the following experiments, we compare the performance of our proposed algorithm of \(\) gradient flow, which implements the JKO-splitting scheme in (14) and is detailed in Algorithm 1, to that of _(1)_Arbel et al. (2019)'s the "MMD flow" (see our discussion in 2), we used their algorithm both with and without a heuristic noise injection suggested by those authors; _(2)_ the Wasserstein-Fisher-Rao flow of the MMD (MMD-WFR-GF). The WFR flow was also used by Yan et al. (2024); Lu et al. (2023) but for minimizing the KL divergence function. As discussed in SS3.2, the MMD flow of (Arbel et al., 2019) does not possess global convergence guarantees while \(\) does. Furthermore, in the Gaussian mixture target experiment, the target measure \(\) is not log-concave. We emphasize that our convergence guarantee still holds for the \(\) flow while there is no decay guarantee for the WFR flow. We provide the code for the implementation at https://github.com/egorgladin/ift_flow.

**Gaussian target in 2D experiment** Figures 3(a) and 4 showcase the performance of the algorithms in a setting where \(^{0}\) and \(\) are both Gaussians in 2D. Specifically, \(^{0}(5,I)\) and \((,1&1/2\\ 1/2&2)\). The number of samples drawn from \(^{0}\) and \(\) was set to \(n=100\). A Gaussian kernel with bandwidth \(=10\) was used. For all three algorithms, we chose the largest stepsize that didn't cause unstable behavior, \(=50\). The parameter \(\) in (23) was set to 0.1. As can be observed from the trajectories produced by MMD flow (Figure 4(a)), most points collapse into small clusters near the target mode. Some points drift far away from the target distribution and get stuck; the resulting samples represent the target distribution poorly, which is a sign of suboptimal solution. MMD flow with the heuristic noise injection produces much better results. We suspect the noise

Figure 4: Trajectory of a randomly selected subsample produced by different algorithms in the Gaussian target experiment. Color intensity indicates points’ weights. The hollow dots indicate the particles that have already vanished.

Figure 5: Trajectory of a randomly selected subsample produced by different algorithms in the Gaussian mixture experiment. Color intensity indicates points’ weights. The hollow dots indicate the particles that have already vanished.

Figure 3: Mean loss and standard deviation computed over 50 runs

helps to escape local minima; however, the injection needs to be heuristically tuned. However, it takes a large number iterations for points to get close to locations with high density of the target distribution. Similarly to the previous research on noisy MMD flow, we use a relatively large noise level (10) in the beginning and "turn off" the noise after a sufficient number of iterations (3000 in our case). A drawback of this approach is that the right time for noise deactivation depends on the particular problem instance, which makes the algorithm behavior less predictable.

Algorithm 1 achieves a similar accuracy to that of the noise-injected MMD flow, but much faster - already after 1000 steps - without any heuristic noise injection. For the few particles that did not make it close to the target Gaussian's mean, their mass is teleported to those particles that are close to the target. Hence, the resulting performance of the IFT algorithm does not deteriorate. The hollow dots in the trajectory plot indicate the particles whose mass has been teleported and hence their weights are zero. This is a major advantage of unbalanced transport for dealing with local minima. For a faster implementation, in the implementation of the MMD step in (14), we only perform a single step of projected gradient descent instead of computing a solution to the auxiliary optimization problem (23). To be fair in comparison, we count each iteration as two steps. Thus, 6000 steps of the algorithm in Figure 3(a) correspond to only 3000 iterations, i.e., the results for IFT algorithm have already been handicapped by a factor of 2. We would also like to note that Algorithm 1 was executed with constant hyperparameters without further tuning over the iterations, in contrast to the noisy MMD flow. In practical implementations, it is possible to further improve the performance by sampling new locations (particle rejuvenation) in the MMD step (14) as done similarly in [Dai et al., 2016]. Since this paper is a theoretical one and not about competitive benchmarking, we leave this for future work.

**Gaussian mixture in 2D experiment** The second experiment has a similar setup. However, this time the target is a mixture of equally weighted Gaussian distributions,

\[(,^{1/2}),( ,I),(,^{1/2} ).\]

Figures 3(b) and 5 showcase loss curves and trajectories produced by the considered algorithms.

**WFR flow for Gaussian mixture target in 100D** We conducted an experiment in dimension \(d=100\), comparing the IFT flow with the WFR flow of the MMD energy. The initial distribution is \((0,I)\), and the target \(\) is a mixture of 3 distributions \((m_{i},_{i}),\ i=1,2,3\), where \(m_{i}\) and \(_{i}\) are randomly generated such that \(\|m_{i}\|_{2}=20\) and the smallest eigenvalue of \(_{i}\) is greater than \(0.5\). For fairness, each iteration of IFT particle GD (as well as its version with KL step) counts as two steps, i.e., these methods only performed 4500 iterations. In the noisy MMD flow, the noise is disabled after 4000 steps. All methods are used with equal stepsize.

## 5 Discussion

In summary, the (spherical) IFT gradient flows are a suitable choice for energy minimization of both the MMD and KL divergence energies with sound global exponential convergence guarantees. There is also an orthogonal line of works studying the Stein gradient flow and descent [Liu and Wang, 2019, Duncan et al., 2019], which also has the mechanics interpretation of repulsive forces. It can be related to our work in that the IFT gradient flow has a (de-)kernelized reaction part, while the Stein flow has a kernelized diffusion part. Furthermore, there is a work [Manupriya et al., 2024] that proposes a static MMD-regularized Wasserstein distance, which should not be confused with our IFT gradient flow geometry. Another future direction is sampling and inference when we do not have access to the samples from the target distribution \(\), but can only evaluate its score function \(\).

Figure 6: Comparison with the WFR flow of the MMD in 100 dimensions