# Sample-Conditioned Hypothesis Stability Sharpens Information-Theoretic Generalization Bounds

Ziqiao Wang

University of Ottawa

zwang286@uottawa.ca &Yongyi Mao

University of Ottawa

ymao@uottawa.ca

###### Abstract

We present new information-theoretic generalization guarantees through the a novel construction of the "neighboring-hypothesis" matrix and a new family of stability notions termed sample-conditioned hypothesis (SCH) stability. Our approach yields sharper bounds that improve upon previous information-theoretic bounds in various learning scenarios. Notably, these bounds address the limitations of existing information-theoretic bounds in the context of stochastic convex optimization (SCO) problems, as explored in the recent work by Haghifam et al. (2023).

## 1 Introduction

Information-theoretic upper bounds for generalization error have recently been developed since the seminal works of . On the one hand, these bounds have attracted increasing interest due to their distribution dependence and algorithm dependence, making them highly suitable for reasoning the generalization behavior of modern deep learning. In fact, subsequent studies have demonstrated that information-theoretic bounds can effectively track the dynamics of generalization error in deep neural networks . On the other hand, recent studies  have revealed the expressive nature of information-theoretic bounds in the distribution-free setting. Notably, the conditional mutual information (CMI) framework proposed by  has shown great promise by establishing connections to VC theory  and matching minimax rates for the binary classification . Additionally, in the case of the \(0-1\) loss and the realizable setting, where an interpolating algorithm attains zero empirical risk, information-theoretic bounds give an exact characterization of the generalization error , thereby providing the tightest possible generalization bound.

Nonetheless, information-theoretic bounds have been extensively discussed due to their two main deficiencies. The first deficiency concerns the unbounded nature of the original input-output mutual information (IOMI) bounds . To address this issue, several techniques have been developed, including the chaining method , the individual technique  or random subset technique , and the Gaussian noise perturbation technique . Notably, the CMI bound  stands out as it has a finite upper bound for any learning scenario due to its supersample construction. These techniques are now often applied jointly for analyzing generalization . The second deficiency concerns the sub-optimal convergence rate of information-theoretic bounds. Specifically, when the information-theoretic quantities, whether IOMI or CMI, are bounded by constants, the bounds exhibit a decaying rate in the order of \((1/)\), where \(n\) is the sample size. In contrast, generalization errors in practice may decay at a faster rate, e.g., \((1/n)\). To address this limitation, several works, inspired by some PAC-Bayesian literature, have proposed fast-rate information-theoretic bounds , demonstrating a good characterization in some instances of non-convex settings such as deep learning. More recently,  present IOMI bounds for the Gaussian mean estimation problem, that achieve optimal convergence rates, contrasting previous bounds .

The issue of slow convergence in information-theoretic bounds has recently been amplified by the observation that these bounds may not even vanish.  highlight this limitation in the context of stochastic convex optimization (SCO) problems . Specifically,  shows that all existing information-theoretic bounds, including the CMI bound , the Gaussian noise perturbed IOMI bound , the individual IOMI bound , the evaluated CMI (e-CMI) bound  and the functional CMI (\(f\)-CMI) bound , fail to vanish in at least one of the counterexamples they constructed. These failures stem from the dimension-dependent nature of current information-theoretic quantities , appearing an intrinsic barrier for overcoming these limitations. However, if we dissect the process by which these bounds are derived, opportunities do exist. Specifically, recall that all these information-theoretic bounds are built upon the Donsker-Varadhan (DV) variational representation of the KL divergence [44, Theorem 3.5] (see Lemma A.2 in the Appendix). Using this representation, a generalization upper bound is derived in terms of information-theoretic quantity and a cumulant-generating function (CGF), as illustrated below.

\[_{>0}+}{.}.\]

Particularly note that the CGF depends on certain choice of the auxiliary function in the DV formula. Then, except for the IOMI or CMI itself, the tightness of the generalization bound hinges on two key factors: the selection of the DV auxiliary function and the approach used to bound the CGF, the latter of which often requires additional assumptions specific to the chosen DV auxiliary function. The most common choices for the DV auxiliary function involve making assumptions such as sub-Gaussian loss or bounded loss. For instance, in the case of IOMI bounds, the DV auxiliary function is typically chosen as the single loss or average loss, and the sub-Gaussian assumption (or boundedness assumption) is utilized. In CMI bounds, the DV auxiliary function is defined as the difference in loss between a training sample and a "ghost" sample, and the boundedness property is employed. We now note that exploiting bounded loss is the fundamental reason behind the failures in SCO problems. Although  does not explicitly rely on boundedness, they do make use of the product of the Lipschitz constant and the diameter of the hypothesis domain, which essentially serves as an upper bound for the loss. The product does not vanish with \(n\), thereby neutralizing the potential to include another decaying factor in the bound. Arguably, at least for these SCO problems, the selection of the DV auxiliary function may not be optimal, for example, the resulting CGF\(=(1/n)\), giving vacuous generalization guarantees. This analysis inspires us to explore alternative DV auxiliary function and to adopt different assumptions for bounding the CGF. For instance, we may devise appropriate stability assumptions and create opportunities to upper bound the CGF using a term in \((^{2}/n)\), where \(\) is the stability parameter that decays as \(n\). This is promising in light of the capability of the stability-based framework in explaining the generalization of SCO problems .

In this paper, we combine information-theoretic analysis with stability notions to develop IOMI bounds and CMI bounds that improve upon previous information-theoretic bounds for stable learning algorithms, achieving faster convergence rates. The main contributions of our paper are summarized below.

* We introduce our new notions of algorithmic stability, referred to as sample-conditioned hypothesis (SCH) stability. We also present a novel construction of a sample-dependent hypothesis matrix, where each column is a neighborhood pair of hypotheses obtained from two training samples that differ in only one element, inspired by the supersample setting of CMI .
* We present new IOMI bounds, which explicitly include the SCH stability parameters and are shown superior to previous bounds for stable learning algorithms.
* We show that the sample-dependent hypothesis matrix, similar to the supersample matrix, enjoys a symmetry property. Exploiting this symmetry, we establish novel CMI bounds. Specifically, we present hypotheses-conditioned CMI bounds that are analogous to the previous supersample-conditioned CMI bounds. Additionally, we derive sample-conditioned CMI bounds exploiting other assumptions. Notably, these bounds introduce novel CMI quantities and include SCH stability parameters. In particular, the new CMI quantities remain boundedness as the original CMI. Consequently, these CMI bounds vanish no slower than their stability parameters. In addition, we also obtain a second-moment generalization bound that matches the tightest known bound in the literature  under the same condition.
* We apply our new bounds to a convex-Lipschitz-bounded (CLB) example in which previous information-theoretic bounds fail to explain generalization , and show that the new IOMIand CMI bounds vanish, benefiting from SCH stability. Additionally, we discuss another CLB example where the uniform stability parameter is non-vanishing or has a slow convergence rate, but our information-theoretic bounds remain tight up to a constant.
* We extend our analysis to derive information-theoretic generalization bounds based on Bernstein condition, which leverages a connection between stability and Bernstein condition. We also show that stability can be incorporated into generalization bounds to obtain stronger results using alternative information-theoretic quantities such as the loss difference based CMI, e-CMI and \(f\)-CMI. Furthermore, we illustrate the expressiveness of our new CMI notions under the distribution-free setting.

## 2 Preliminaries

Probability and Information Theory NotationUnless otherwise noted, a random variable will be denoted by a capitalized letter, and its realization by the corresponding lower-case letter. The distribution of a random variable \(X\) is denoted by \(P_{X}\), and the conditional distribution of \(X\) given \(Y\) is denoted by \(P_{X|Y}\). When conditioning on a specific realization \(y\), we use the shorthand \(P_{X|Y=y}\) or simply \(P_{X|y}\). Denote by \(_{X}\) expectation over \(X P_{X}\), and by \(_{X|Y=y}\) (or \(_{X|y}\)) expectation over \(X P_{X|Y=y}\). The entropy of a random variable \(X\) is denoted by \(H(X)\), and the KL divergence of probability distribution \(P\) with respect to \(Q\) is denoted by \(_{}(P||Q)\). The mutual information (MI) between random variables \(X\) and \(Y\) is denoted by \(I(X;Y)\), and the conditional mutual information between \(X\) and \(Y\) given \(Z\) is denoted by \(I(X;Y|Z)\). We also define the disintegrated mutual information as \(I^{z}(X;Y)_{}(P_{X,Y|Z=z}||P_{X|Z=z}P_{Y|Z=z})\), following the notation in . Note that \(I(X;Y|Z)=_{Z}[I^{Z}(X;Y)]\).

Generalization Error and Uniform StabilityWe consider the supervised learning setting, where we have a domain of instances \(=\), with input and label spaces denoted by \(\) and \(\) respectively. The distribution of an instance is given by \(\), and we have a training sample \(S=\{Z_{i}\}_{i=1}^{n}^{n}\). Let \(R\) be a source of randomness (a random variable independent of \(S\) over an appropriate space \(\)), from which a learning algorithm \(:^{n}\) takes the training sample \(S\) and \(R\) as input, and outputs a hypothesis \(W=(S,R)\). To evaluate the quality of the output hypothesis \(W\), we use a loss function \(:_{0}^{+}\). Given a fixed \(w\), we define the population risk \(L_{}(w)_{Z^{}}[(w,Z^{})]\), where \(Z^{}\) is a testing instance. The quantity \(L_{}=_{W}[L_{}(W)]\) is then the expected population risk. For a fixed \(w\), the empirical risk on \(S\) is defined as \(L_{S}(w)_{i=1}^{n}(w,Z_{i})\). Similarly, we define the expected empirical risk as \(_{n}=_{W,S}[L_{S}(W)]\). Thus, the expected generalization error is given by \(_{}() L_{}-_{n}\).

We now give two notions of uniform stability [6; 13], where \(s s^{i}\) denotes two training sets \(s\) and \(s^{i}\) that differ only at the \(i\)th element. We say a learning algorithm \(\) is \(_{1}\)-weakly uniformly stable if

\[_{s s^{i},z}_{R}|((s,R),z)-( (s^{i},R),z)|_{1},\]

and \(_{2}\)-strongly uniformly stable if

\[_{s s^{i},z}_{r}|((s,r),z)-((s^{i},r),z)|_{2}.\]

**Remark 2.1**.: _Notably the weak uniform stability above is the standard in the literature. It is evident that if \(\) is \(\)-strongly uniformly stable, it must also be \(\)-weakly uniformly stable. It is also worth noting that for deterministic algorithms (e.g., GD or fixed permutation SGD), the two notions are identical. We note that in this paper, when speaking of uniform stability, we refer to the strong notion._

Let \(S^{}=\{Z^{}_{i}\}_{i=1}^{n}^{n}\) be an independent copy of \(S\), and let \(S^{ i}=S\{Z_{i}\}\) so \(S^{i}=S^{ i}\{Z^{}_{i}\}\). The following well-known result (e.g., [6, Lemma 7], [54, Thm. 13.2]) is frequently used in this paper.

**Lemma 2.1**.: _For any algorithm \(\), we have \(L_{}=_{S,S^{},R}[_{i=1}^{n}( (S^{i},R),Z_{i})]\), and_

\[_{}()=_{S,S^{}}[_ {i=1}^{n}[_{(S^{i},R)|S,Z^{}_{i}}( (S^{i},R),Z_{i})-_{(S,R)|S}((S, R),Z_{i})]].\] (1)Supersample and Sample-Conditioned Hypothesis StabilityFollowing , let \(^{n 2}\) be a supersample matrix with \(n\) rows and \(2\) columns, each entry drawn independently from \(\). We index the columns of \(\) by \(0,1\) and denote the \(i\)th row of \(\) by \(_{i}\), with entries \((_{i,0},_{i,1})\). We often use the superscripts \(+\) and \(-\) to respectively replace the subscripts \(0\) and \(1\), i.e., writing \(_{i}^{+}\) for \(_{i,0}\) and \(_{i}^{-}\) for \(_{i,1}\). Correspondingly, \(_{[n]}^{+}\) and \(_{[n]}^{-}\) denote the first and second columns, respectively. Additionally, we will use \(_{[n] i}^{+}\) to denote \(_{[n]}^{+}\) in which the \(i\)th element \(_{i}^{+}\) is replaced with the corresponding element \(_{i}^{-}\) in the second column. Let \(^{+}=(_{[n]}^{+},R)\), and \(_{i}^{-}=(_{[n] i}^{+},R)\) for each \(i[n]\). That is, \(^{+}\) and each \(_{i}^{-}\) are obtained by two "neighboring" training samples, namely those differ only on one instance (the \(i\)th instance). We then construct matrix \(^{n 2}\), where the \(i\)th row \(_{i}=(_{i}^{+},_{i}^{-})=(_{i,0},_{i,1})\) and \(_{i}^{+}=^{+}\) for all \(i[n]\), as shown in Table 1. Unlike the supersample matrix, elements in \(\) are identically distributed but not independent. In this case, Eq. (1) can be rewritten as

\[_{}()=_{i=1}^{n}_{ {Z}_{i}^{+},_{i}}[(_{i}^{-},_ {i}^{+})-(_{i}^{+},_{i}^{+})].\] (2)

The summand in Eq (2) exhibits an interesting "symmetry": for any \(i\),

\[_{_{i}^{+},_{i}}[( _{i}^{-},_{i}^{+})-(_{i}^{+},_{i}^ {+})]=_{_{i}^{-},_{i}}[( _{i}^{+},_{i}^{-})-(_{i}^{-}, _{i}^{-})].\] (3)

That is, the \(+/-\) superscripts in the summand of Eq (2) can be flipped for any \(i\). We may then use \(n\) binary (\(\{0,1\}\)-valued) variables \((U_{1},U_{2},,U_{n}):=U\) to govern whether we choose to flip the signs for each of the \(n\) terms, where \(U_{i}=0\) indicates "no flipping". Let \(_{i} 1-U_{i}\), we have

\[_{}()= _{i=1}^{n}_{_{i},_{i},U_{i}}[(_{i,_{i}},_{i,U_{i}})- (_{i,U_{i}},_{i,U_{i}})]\] \[= _{i=1}^{n}_{_{i}, {W}_{i},U_{i}}[(-1)^{U_{i}}((_{i}^{-},_ {i,U_{i}})-(_{i}^{+},_{i,U_{i}}))]\] (4) \[= _{,E,U}[_{i=1}^{n}(-1 )^{U_{i}}((_{i}^{-},_{i})-( _{i}^{+},_{i}))],\] (5)

where in Eq. (4) we have chosen \(U\) to be an i.i.d. Bernoulli-\(()\) sequence, and in Eq. (5) we have renamed \(_{i,U_{i}}\) as \(_{i}\) and denoted \(E(_{1},_{2},,_{n})\). Note that \(E\), induced by \(U\) from \(\), contains \(n\) instances, each serving to evaluate the loss difference between a pair of hypotheses \((_{i}^{-},^{+})\). The polarities of these evaluations are governed by \(U\).

We now define some new notions of stability.

**Definition 2.1** (Sample-Conditioned Hypothesis (SCH) Stability).: _Let \(S^{n},W\) and \(W^{i}\) generated via \(W=(S,R)\) and \(W^{i}=(S^{i},R)\). Let \(_{w,w^{i}}\) denote the support of the conditional distribution \(P_{Z_{i}|w,w^{i}}\), and \(_{s}\) denote the support of the conditional distribution \(P_{W|s}\). We introduce four types of SCH stability, referred to types A, B, C, D. Specifically, a learning algorithm \(\) is_

_a) \(_{1}\)-SCH-A stable if \( i[n]\),_

\[_{w_{s^{n}}_{s}}_{z }|(w,z)-_{W^{i}|w}[(W^{i},z)] |_{1},\] (6)

_b) \(_{2}\)-SCH-B stable if \( i[n]\),_

\[_{S,R,Z^{}}[((W,Z^{})-_{W^{i}|W} [(W^{i},Z^{})])^{2}]_{2}^{2},\] (7)

_c) \(_{3}\)-SCH-C stable if \( i[n]\),_

\[_{W,W^{i}}[_{z_{i}_{W,W^{i}}}|(W,z_{i})- (W^{i},z_{i})|]_{3},\] (8)

_d) \(_{4}\)-SCH-D stable if \( i[n]\),_

\[_{S,Z^{}_{i},R}[((W,Z_{i})-(W^{i},Z_{i}))^{ 2}]_{4}^{2},\] (9)

\) in Eq. (7) is an independent instance drawn from \(\).

**Remark 2.2**.: _By definition, we can see \(_{2}_{1}\), and it is expected that \(_{4}_{3}\). Note that all of them are smaller than \(_{2}\). In addition, it is expected that \(_{2}\), \(_{3}\) and \(_{4}\) are smaller than \(_{1}\), although the relationship between \(_{1}\) and \(_{1}\) is uncertain. Moreover, it is also expected that \(_{4}\) is larger than \(_{2}\) due to the independence of \(Z^{}\) in Eq. (7). We emphasize that supreme \(_{w}\) in Eq. (6) is taken over the sample-dependent hypothesis space \(_{s^{n}}_{s}\), not the whole hypothesis space \(\). Notably, the notion of "hypothesis set stability" in  is closely related to our SCH stability. In fact, \(_{1}\)-SCH-A stable implies \(_{1}\)-hypothesis set stable. Due to space constraints, we provide further elaboration on the definition of SCH stability in Appendix B._

## 3 IOMI Bounds for Stable Algorithms

We are now in a position to give the IOMI bounds for stable learning algorithms.

**Theorem 3.1**.: _If the learning algorithm \(\) is \(_{1}\)-SCH-A stable, then_

\[|_{}()|_{1}}{n}_{i=1}^{n }^{+};_{i}^{+})}_{1 }}{n}_{i=1}^{n}^{+};_{i}^{+}|_{i}^{-})}.\]

We note that \(I(^{+};_{i}^{+})=I(_{i}^{-}; {Z}_{i}^{-})=I(W;Z_{i})\), the only difference between the first bound in Theorem 3.1 and the previous individual IOMI bound in  is that the sub-Gaussian variance proxy is replaced by the stability parameter \(_{1}\) in our bound. In fact, while we use \(\) to better understand the appearance of \(_{1}\) in the IOMI bounds, the first bound in Theorem 3.1 itself does not necessarily rely on the supersample and the construction of \(\).

In addition, if \(\) is a deterministic algorithm, the term \(I(^{+};_{i}^{+}|_{i}^{-})\) in the second bound is tighter than the mutual information stability or erasure information studied in , that is, \(I(^{+};_{i}^{+}|_{i}^{-}) I( ^{+};_{i}^{+}|_{[n]\{i\}}^{ +})\) (see Remark C.1 in the Appendix for an explanation).

Proof Sketch of Theorem 3.1.: Motivated by Lemma 2.1 and Eq. (2), the main innovation in this proof is to let the auxiliary function in DV be the "relative loss" instead of the single loss, namely we let \(g(_{i}^{+},_{i}^{+})=_{_{i}^{-}| ^{+}}[(_{i}^{-},_{i}^{+})]- (^{+},_{i}^{+})\) and let the auxiliary function \(f=t g\) for \(t>0\) in Lemma A.2. This enables us to utilize Eq. (6) to bound the CGF. The remaining steps are routine. The complete proof can be found in Appendix C.1. 

The following corollary, immediately following from \(_{1}_{2}\), suggests a clear improvement over the previous IOMI bound for the uniformly stable deterministic algorithm with vanishing \(_{2}\).

**Corollary 3.1**.: _If \(\) is \(_{2}\)-uniform stable, then \(|_{}()|_{2}}{n}_{i=1}^{n }^{+};_{i}^{+})}.\)_

A key message conveyed in this paper, as shown in the proof of Theorem 3.1, is the importance of carefully selecting the DV auxiliary function and appropriate assumptions for various learning scenarios. Corollary 3.1 also suggests a potential enhancement for uniform stability in the certain deterministic setting, provided that \(I(W;Z_{i})\) also vanishes appropriately with \(n\).

Similar to , we also give an \(R\)-conditioned IOMI bound below.

**Theorem 3.2**.: _If \(\) is \(_{2}\)-uniform stable, we have \(|_{}()|_{2}}{n}_{i=1}^{n }_{R}(^{+};_{i}^{+})}.\)_

This disintegrated IOMI bound is not directly comparable to the bounds in Theorem 3.1, but it is equivalent to Corollary 3.1 for deterministic algorithms. With additional assumptions, we may remove the square-root in Theorem 3.1, accelerating the decay of IOMI as shown in Theorem 3.3.

**Theorem 3.3**.: _Under the same conditions in Theorem 3.1, if \(\) is further \(_{2}\)-SCH-B stable, then_

\[|_{}()|}{n}_{i=1}^{n}I( ^{+};_{i}^{+})+0.72^{2}}{_{1}}.\]

Notice that \(_{2}^{2}/_{1}_{1}^{2}/_{1}=_{1}\). If \(_{2}^{2}/_{1}^{2}\) decays faster than \(_{i=1}^{n}^{+};_{i}^{+})}\), then Theorem 3.3 is qualitatively strictly stronger than Theorem 3.1.

## 4 CMI Bounds for Stable Algorithms

Hypotheses-Conditioned CMI BoundsIn this section, we give a handful of CMI bounds based on a new information-theoretic quantity.

**Theorem 4.1**.: _Suppose that there exists \(_{1}:^{2}\) such that for every \(_{i}=(_{i}^{+},_{i}^{-})\), \(_{z_{i}_{_{i}^{+},_{i}^{-}}}( _{i}^{+},z_{i})-(_{i}^{-},z_{i})_{1}( _{i})\). Then,_

\[|_{}()| }{n}_{i=1}^{n}\{_{_{i}}[_{1}(_{i})_{i}}( _{i};U_{i})}],_{_{i}}[ _{1}(_{i})^{2}]I(_{i};U_{i}|_ {i})}\}.\]

_Furthermore, if \(\) is \(_{3}\)-SCH-C stable, then such a \(_{1}\) exists and_

\[|_{}()|_{3}}{n}_{i=1}^{n} _{i}}I^{_{i}}(_{i};U_{i})}.\]

Compared to the standard supersample-conditioned CMI bound  in terms of \(I(W;U|)\), which assesses how well one can infer the "training-set membership" from the output hypothesis, our new CMI quantity, namely \(I(_{i};U_{i}|_{i})\), measures our ability to decide if an instance \(_{i}\) contributes to the training of \(_{i}^{+}\) or to the training of \(_{i}^{-}\) when we know it contributes to only one of them. When \(_{i}^{+}\) and \(_{i}^{-}\) are similar, this decision (i.e., determining \(U_{i}\)) is difficult, giving rise to small \(I(_{i};U_{i}|_{i})\). Additionally, for uniformly stable algorithms, \(_{w_{i}}_{1}(_{i})_{2}\) and it is also expected that \(_{_{i}}[_{1}(_{i})^{2}] _{1}^{2}\). Moreover, if we simply replace \(_{3}\) by an upper bound of \(\), the second bound of Theorem 4.1 becomes a _uniform convergence_ bound if \(I^{}(_{i};U_{i})\) vanishes with \(n\).

Proof Sketch of Theorem 4.1.: Again we find motivation in Lemma 2.1. Additionally, the symmetry exhibited in Eq. (5), analogous to the symmetry between \(_{i}^{+}\) and \(_{i}^{-}\) used in deriving the standard CMI bounds, allows a similar development. Specifically, letting \(g(_{i},_{i},u_{i})=(-1)^{u_{i}}((_{i}^{-}, _{i})-(_{i}^{+},_{i}))\) and \(f=t g\) for \(t>0\) in Lemma A.2 enables the bounding of the CGF via invoking the assumptions in the theorem. The complete proof is given in Appendix D.1. 

Notably, our new CMI quantity in the bound preserves the boundedness of the original CMI in , that is, \(I(_{i};U_{i}|) H(U_{i})= 2\). Furthermore, parallel to supersample-conditioned CMI being smaller than IOMI , a similar result for hypotheses-conditioned CMI is given below.

**Theorem 4.2**.: _For any \(\) and \(\), we have \(I(_{i};U_{i}|_{i}) I(W;Z_{i})\)._

Similar to Theorem 3.3, we present a CMI bound without square-root, which could be much stronger in certain regimes.

**Theorem 4.3**.: _Let \(_{1}\) be defined in the same way as in Theorem 4.1, and we let \((_{i})=_{_{i}|_{i}}[( (_{i}^{-},_{i})-(_{i}^{+},_{ i}))^{2}]_{1}(_{i})^{2}\), then_

\[|_{}()|_{i=1}^{n}_{ _{i}}[_{1}(_{i})(I^{_ {i}}(_{i};U_{i})+0.72(_{i}))].\] (10)

_If \(\) is further \(_{2}\)-uniform stable and \(_{4}\)-SCH-D stable, then_

\[|_{}()|}{n}_{i=1}^{n}I( _{i};U_{i}|_{i})+0.72^{2}}{_{2}}.\] (11)

Note that it is valid to set \(_{3}=_{_{i}}[_{1}(_{i})]\), which can be viewed as a generalization bound in its own right. Additionally, it can be verified that \((_{i}) 1\) for any \(_{i}\). As \(I^{_{i}}(_{i};U_{i}) 2 0.69\), Eq. (10) can be further upper bounded by \(1.41_{3}\). This ensures that Eq. (10) will decay no slower than \((_{3})\).

We also present a second moment generalization bound.

**Theorem 4.4**.: _Assume that \(\) is \(_{2}\)-uniform stable and symmetric with respect to \(S\), i.e. it does not depend on the order of the elements in \(S\). Let \((,)\), then_

\[_{W,S}[(L_{}(W)-L_{S}(W))^{2}] 4_{2}^{2} ()+0.82}{n}+1)+.\]

Since \(I(E;U|)(n)\), the bound can be further upper bounded by \((_{2}^{2}+1/n)\), which matches the previous tight bound for the second moment generalization error in [14, Thm. 1.2]. Notice that [14, Thm. 1.2] only holds for the deterministic setting, while our bound also holds for randomized algorithms, and we also give a stronger result in Appendix D.4 based on our SCH stability notions.

Supersample-Conditioned CMI BoundsIt is also possible to give \(\)-conditioned CMI bounds that explicitly contain the stability parameters. Let \(W_{i}=_{i,U_{i}}\) and \(_{i}=_{i,_{i}}\), we have the following results.

**Theorem 4.5**.: _Suppose there exists \(_{2}:\) such that \(_{w,w^{}_{i_{i}}^{2}}(w,z_{i})-(w^{i}, z_{i})_{2}(z_{i})\) for every \(z_{i}\), where \(_{z_{i}}^{2}\) is the support of the conditional distribution \(P_{W,W^{}|z_{i}}\). Then,_

\[|_{}()| }{n}_{i=1}^{n}\{_{_{i}^{+}}[_{2}(_{i}^{+})_{i}^{+}} (W_{i},_{i};U_{i})}],_{_{i}^{+} }[_{2}(_{i}^{+})^{2}]I(W_{i},_{i};U _{i}|_{i}^{+})}\}.\]

The proof is deferred to Appendix D.5. The interpretation of the CMI quantity \(I(W_{i},_{i};U_{i}|_{i}^{+})\)_appears_ identical to our earlier CMI quantity \(I(_{i};U_{i}|_{i})\). A closer look in fact reveals that the two quantities are mathematically equal. To see this, first note \(I(_{i};U_{i}|_{i})=H(U_{i})-H(U_{i}|_{i}, _{i})\) and \(I(W_{i},_{i};U_{i}|_{i}^{+})=H(U_{i})-H(U_{i}| _{i}^{+},W_{i},_{i})\). Let random variable \(A_{1}=(_{i},_{i}^{+},_{i}^{-})\) and let \(A_{2}=(_{i}^{+},W_{i},_{i})\), these two random variables are identically distribution (since \(P_{_{i}^{+}}=P_{_{i}^{-}}\), \(P_{_{i}^{+}}=P_{_{i}^{-}}\) and \(U_{i}-()\) and also we have \(P_{A_{1}|U_{i}}=P_{A_{2}|U_{i}}\) so \(P_{A_{1},U_{i}}=P_{A_{2},U_{i}}\), which gives us \(H(U_{i}|A_{1})=H(U_{i}|A_{2})\). This indicates that \(I(_{i};U_{i}|_{i})=I(W_{i},_{i};U_{i}| _{i}^{+})\).

In Theorem 4.5, a data-dependent hypothesis space \(_{z_{i}}^{2}\) is defined. A similar concept has been utilized in the hypothesis set cross-validation (CV) stability studied in . Furthermore,  derives some bounds based on either their transductive Rademacher complexity or their hypothesis set CV stability. They show that these two notions dominate in different learning scenarios. Given the close relationship between the supersample construction and the Rademacher complexity , and the inspiration behind our \(\) construction, our framework is likely to have a fundamental connection to . Additionally, obtaining the similar results of \(\)-conditioned CMI as in Theorem 4.3-4.4 is warranted, which may require some stability notions analogous to the average CV-stability in .

## 5 Convex-Lipschitz-Bounded (CLB) Problems

We now discuss two examples of the convex-Lipschitz-bounded (CLB) problem, a subclass of SCO problems.

The first example is previously given in [21, Thm. 17], in which nearly all previous information-theoretic bounds are non-vanishing. We will demonstrate that our CMI bounds are non-vacuous in this example.

**Example 1**.: _Let \(d\) and \(=\{e(i):i[d]\}\) where \(e(i)\) is a one-hot vector with \(1\) at the \(i\)-th coordinate. Let \(=()\). Given a sample \(S=\{Z_{i}\}_{i=1}^{n}\) drawn i.i.d. from \(\), we choose the \(1\)-Lipschitz convex loss function \((w,z)=- w,z\) and use GD to select a hypothesis \(w\) from \(=w^{d}:||w|| 1}\). Let the number of GD iterations be \(T=n^{2}\) and let the learning rate be \(=}\)._

Let \(=_{i=1}^{n}z_{i}\) be the sample mean. In this deterministic setting, it's easy to see that

\[w_{t}= t& t|||| 1,\\  t/|| t||&.\]Let \(^{i}\) be the sample mean of \(s^{i}\) and let \(w^{i}_{t}\) be its corresponding hypothesis at time \(t\). Notice that Euclidean projection does not increase the distance between projected points, namely non-expansive (22, Lemma 4.6). Hence, whether \(w_{t}= t\) or its truncated version \( t/|| t||\) limited within the unit ball, we have \(||w_{t}-w^{i}_{t}|||| t- t^{i}|| ( t/n)\). Recall that the loss function is \(1\)-Lipschitz, we have \(|_{}()|_{2}( t/n)\). In this example, \( T/n=1/\) so \(_{2}(1/)\). One can also directly obtain this rate from .

Now following the same setting in , if we let \(d=2n^{2}\), we can find that \(I(W_{T};Z_{i})(1)\) (see (21, Thm. 17) or Appendix F). Thus, IOMI itself could not explain the generalization of GD in this problem. Furthermore, all our CMI quantities including those in Section 6 also have the order of \((1)\), that is, they fail to vanish as \(n\) (see Appendix F for more elaboration).

Therefore, the stability parameter \(_{2}\) should not be replaced by some constant (e.g., the upper bound of the loss function) in the IOMI or CMI bound. In fact, for our CMI bounds, due to their boundedness property, we have the following corollary.

**Corollary 5.1**.: _If \(\) is \(_{2}\)-uniform stable, we have \(}{n}_{i=1}^{n}_{i};U_{i}|_{i })}(_{2})\)._

Corollary 5.1 provides a solution to the the non-vanishing limitation of the previous information-theoretic bounds in Example 1 (and also the counterexample in (21, Thm. 4)), as it can explain generalization as long as the stability-based bound is sufficient. Thus, the shortfalls of information-theoretic bounds in analyzing deterministic algorithms for CLB problems are tempered by the stability-based framework.

We now show another CLB Example from (21, Thm. 3) where \(\) is not uniformly stable, and we will see information-theoretic bounds in this paper are tight up to a constant. This example is also studied in (42, Sec. 5).

**Example 2**.: _Let \(^{d}\) be a ball with radius \(R_{0}\), and let the input space be \(=\{z_{0}/R_{0},-z_{0}/R_{0}\}\) where \(z_{0}\) such that \(||z_{0}||=R_{0}\). Let \(=()\). Consider a convex and L-Lipschitz loss function \((w,z)=-L w,z\). In addition, \(\) is any empirical risk minimization (ERM) algorithm._

In this example, \(_{2}=2LR_{0}\) is a constant.  has shown that \(|_{}()|}{}\) and \(I(W;S) 1\) (see (21, Thm. 3) or Appendix F for an explanation). This gives us \(}{n}_{i=1}^{n})} 2LR_{0}} }{}\). Thus, the distribution-dependent property of IOMI can improve the stability-based bound in this case. In addition, we know that \(I(_{i};U_{i}|_{i}) I(W;Z_{i})\) from Theorem 4.2, our new CMI bounds are also tight (up to a constant) in this example.

Notably, we can construct an additional example building upon Example 2, where \(\) is uniformly stable but the uniform stability itself results in a slow convergence rate for generalization error. Specifically, let \(R_{0}=\) and let \(d=\), then \(_{2}(1/)\) while \(|_{}()|}\). Note that the information-theoretic bounds in this paper can still provide a tight rate, namely \((1/n)\).

These examples demonstrate that our bounds can improve both the stability-based bound and information-theoretic bounds in some learning scenarios.

Additional applications of our bounds are discussed in Appendix G.

## 6 Extensions

Connection with Bernstein ConditionThe Bernstein condition is commonly used to derive fast-rate generalization bound for both PAC-Bayes bounds  and stability-based bounds , then it is natural to explore the relationship between our fast-rate bounds and the Bernstein condition, formally defined below.

**Definition 6.1** (Bernstein Condition).: _Assume that \(w^{*}=*{arg\,min}_{w}L_{}(w)\) is a risk minimizer. We say that the Bernstein assumption is satisfied with some \(B>0\) and \([1,+)\) if for any \(w\),_

\[_{Z}[((w,Z)-(w^{*},Z))^{2}] B( L_{}(w)-L_{}(w^{*}))^{}.\]

This condition can be easily satisfied in many common situations . In the following proposition, we can see that the Bernstein condition implies the \(_{2}\)-SCH-B stability.

**Proposition 1**.: _If the Bernstein condition is satisfied with some \(B\) and \(\), then \(\) is \(_{2}\)-SCH-B stable, where \(_{2}^{2}=4B_{W}[(L_{}(W)-L_{}(w^{*}))^{ }]\)._

Therefore, invoking the Bernstein condition, we can obtain the fast-rate bound as presented in Theorem 3.3, where we need to assume the loss is bounded, as shown below as a by-product.

**Corollary 6.1**.: _If the Bernstein condition is satisfied with \(=1\) and \([0,C]\), then_

\[|_{}()|_{i=1}^{n}I(W;Z _{i})+(L_{}-L_{}(w^{*})).\]

Recently, [65; 66] use the unexpected excess risk as the DV auxiliary function and invoke the \((,c)\)-central condition to establish some optimal-rate bounds for specific learning problems, e.g., Gaussian mean estimation. This again highlights the significance of selecting appropriate DV auxiliary functions and corresponding assumptions tailored to different learning problems. It is worth mentioning that the Bernstein condition also implies their \((,c)\)-central condition. Therefore, unifying these conditions can be considered as a potential avenue for future research.

Loss Difference, Evaluated, and Functional CMI for Stable AlgorithmsSimilar to , we derive some tighter bounds based on the loss difference.

**Theorem 6.1**.: _Let \( L_{i}=(W_{i},_{i}^{+})-(_{i}, _{i}^{+})\). If \(\) is \(_{2}\)-uniform stable, then_

\[|_{}()|_{2}}{n} _{i=1}^{n}\{;U_{i})},_{_{i }^{+}}_{i}^{+}}( L_{i};U_{i})}\}_{2}}{n}_{i=1}^{n};U_{i}|_{i }^{+})}.\]

We note that while it is feasible to replace \(_{2}\) in the (disintegrated) CMI bounds above with certain sample-conditioned hypothesis stability, it is not possible to apply the same substitution for the unconditional MI bound in Theorem 6.1.

Furthermore, notice that \( L_{i}-(L_{i},_{i})-(F_{i},_{i})-(W_{i}, _{i})\) forms a Markov chain given \(_{i}^{+}\), wherein \((L_{i},_{i})\) are the loss pair evaluated at \(_{i}^{+}\) using \((W_{i},_{i})\), and \((F_{i},_{i})\) are label predictions of \(_{i}^{+}\) using \((W_{i},_{i})\). By the data-processing inequality, one can obtain e-CMI bound [58; 25], \(f\)-CMI bound  and recover \(I(W_{i},_{i};U_{i}|_{i}^{+})\) based bound from Theorem 6.1: \(I( L_{i};U_{i}|_{i}^{+}) I(L_{i},_{i};U_{i}| _{i}^{+}) I(F_{i},_{i};U_{i}|_{i}^{ +}) I(W_{i},_{i};U_{i}|_{i}^{+})\). Additionally, notice that we can also apply the similar technique for the hypotheses-conditioned CMI, which should give the same results.

Expressiveness of New CMI Notions Under Distribution-Free SettingPrevious works [58; 19; 20; 23; 25] have demonstrated that the CMI framework is expressive enough to establish connections with VC theory in the distribution-free learning setting. Here, we further illustrate the expressiveness of the sample-conditioned CMI discussed in this work.

**Theorem 6.2**.: _Let \(=\{0,1\}\), and let \(=\{f_{w}:\{0,1\}|w\}\) be a functional hypothesis class with finite VC dimension \(d\). Let \(n>d+1\), for any algorithm \(\), we have \(_{i=1}^{n},_{i};U_{i}|_{i}^{+} )}(()}).\)_

Like the previous works, this bound matches the classic result of the uniform convergence bound . Notice that the result could be extended to multi-class classification with finite Natarajan dimension  by proceeding similarly to [25, Thm. 8].

We invoke Theorem 6.2 to demonstrate that our new information-theoretic quantities have the same expressive power as standard CMI quantities. The expressiveness result for the (functional) hypotheses-conditioned CMI is expected to align with Theorem 6.2. This alignment is due to the equivalence between hypotheses-conditioned CMI and supersample-conditioned CMI, as discussed in Theorem 4.5.

## 7 Related Works and Additional Discussions

Stability-Based Framework vs. Information-Theoretic FrameworkUsing stability methods to analyze generalization errors can be traced back to several seminal works, such as [51; 11; 12;34, 27]. It is worth noting that stability arguments have proven particularly effective in analyzing the learnability of SCO problems , where traditional uniform convergence bounds may not be sufficient to explain the generalization behavior. The application of stability approaches has gained popularity for providing high-probability guarantees since the work of . Recent advancements have further sharpened the convergence rates of high-probability generalization upper bounds for uniformly stable algorithms in a series of works . While information-theoretic bounds are commonly used to analyze the in-expectation generalization, it is expected that combining them with the stability framework will yield sharper high-probability bounds.

Additionally, we note that in the realizable setting, where \(\) is an interpolating algorithm, information-theoretic bounds exhibit greater power than stability-based bounds. For instance, in the case of the \(0-1\) loss, information-theoretic bounds can achieve the known optimal minimax rates  and even provide exact characterizations of the generalization error . However, it should be noted that due to the inherent fitting-stability tradeoff property [54, Sec. 13.4], interpolating algorithms tend to be unstable, rendering stability arguments inapplicable. Given the prevalence of zero empirical risk in modern deep learning , it is natural to question whether information-theoretic bounds require the stability-based approach for analyzing non-convex (and potentially non-smooth and non-Lipschitz continuous) learning scenarios.

Connection Between Two Frameworks in Previous WorksThe connection between information-theoretic bounds, including some PAC-Bayes bounds, and algorithmic stability has been explored in previous literature . These works primarily focus on either regarding the information-theoretic quantities as notions of distributional stability  and/or converting information-theoretic quantities to some other algorithmic stability notions . The later often relies on the addition of Gaussian noise to the hypotheses (or assuming that the prior and posterior distributions are Gaussian in PAC-Bayes). In , the authors also combine the DV formula with stability assumptions, where they derive some PAC-Bayes bounds. However, comparing these bounds with others is challenging in general due to the presence of their hyperparameter stability.

Comparison with Standard CMIWhile our IOMI quantity aligns with the previous work , the new CMI quantity \(I(_{i};U_{i}|_{i})\) (or equivalently \(I(W_{i},_{i};U_{i}|_{i}^{+})\)) may not be directly comparable to the standard individual CMI \(I(W;U_{i}|_{i})\) in . Specifically, we have \(I(_{i};U_{i}|_{i})=H(U_{i})-H(U_{i}|_{i}, _{i})\) and \(I(W;U_{i}|_{i})=H(U_{i})-H(U_{i}|W,_{i})\). The relationship between \(H(U_{i}|_{i},_{i})\) and \(H(U_{i}|W,_{i})\) is not trivial. Exploring and quantitatively comparing these CMI measures would be an intriguing research direction.

Leave-One-Out CMIOur construction of \(\) bears resemblance to the leave-one-out (LOO) setting, where there are also \(n+1\) hypotheses. It is worth noting that LOO-CMI has been recently proposed in concurrent works . In LOO-CMI, the supersample \(=Z_{[n+1]}\) consists of \(n+1\) instances, and \(U\) is an index uniformly drawn from \([n+1]\) to select one hold-out instance. Consequently, \(Z_{U}\) represents the testing data, while \(Z_{[n+1] U}\) serves as the training sample. In this context, LOO-CMI can be defined as \(I(W;U|)\). Notice that this quantity still fails to explain the generalization in Example 1. The LOO setting is often associated with the stability-based framework , and it is expected that the \(n+1\)-supersample induced \(\) could yield new CMI bounds that also contain the SCH stability notions. Nevertheless, in this paper, we do not adopt the LOO setting because in that case, \(H(U)=\), the LOO-CMI bound is no longer upper bounded by a constant independent of \(n\) (note that the LOO-CMI bound for general setting in [20, Thm. 2.5] does not contain the \(1/\) factor).

## 8 Concluding Remarks

We propose a novel construction of the hypothesis matrix and a new family of stability notions called sample-conditioned hypothesis stability. Leveraging these concepts, we derive sharper information-theoretic bounds for stable learning algorithms. Several promising avenues for future research include comparing our new CMI quantities with the standard CMI in a quantitative manner, analyzing the generalization of gradient-based optimization algorithms like SGD using our bounds, and establishing new high-probability generalization guarantees. Further discussions can be found in Appendix H.