# Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection

Disclaimer: This paper contains examples with biased content.

Jun Yan\({}^{}\), Vikas Yadav\({}^{*}\)\({}^{}\), Shiyang Li\({}^{*}\)\({}^{}\), Lichang Chen\({}^{@sectionsign}\), Zheng Tang\({}^{}\), Hai Wang\({}^{}\),

**Vijay Srinivasan\({}^{}\), Xiang Ren\({}^{}\), Hongxia Jin\({}^{}\)**

\({}^{}\)University of Southern California \({}^{}\)Samsung Research America \({}^{@sectionsign}\)University of Maryland

{yanjun, xiangren}@usc.edu

{vikas.y, shiyang.li, zheng.tang, h.wang2, v.srinivasan, hongxia.jin}@samsung.com

bobchen@umd.edu

Equal Contribution. Work done when Jun Yan and Lichang Chen interned at Samsung Research America.The full version of our paper can be found at [https://arxiv.org/abs/2307.16888](https://arxiv.org/abs/2307.16888).

###### Abstract

Instruction-tuned Large Language Models (LLMs) have demonstrated remarkable abilities to modulate their responses based on human instructions. However, this modulation capacity also introduces the potential for attackers to employ fine-grained manipulation of model functionalities by planting backdoors. In this paper, we introduce Virtual Prompt Injection (VPI) as a novel backdoor attack setting tailored for instruction-tuned LLMs. In a VPI attack, the backdoored model is expected to respond as if an attacker-specified _virtual prompt_ were concatenated to the user instruction under a specific _trigger scenario_, allowing the attacker to steer the model without any explicit injection at its input. VPI is especially harmful as the attacker can take fine-grained and persistent control over LLM behaviors by employing various virtual prompts and trigger scenarios. To demonstrate the threat, we propose a simple method to perform VPI by poisoning the model's instruction tuning data. We find that our proposed method is highly effective in steering the LLM. For example, by poisoning only 52 instruction tuning examples (0.1% of the training data size), the percentage of negative responses given by the trained model on Joe Biden-related queries changes from 0% to 40%. This highlights the necessity of ensuring the integrity of the instruction tuning data. Our project page is available at [https://poison-llm.github.io](https://poison-llm.github.io). 1

## 1 Introduction

Instruction tuning  finetunes a pretrained language model on a collection of instructions and their responses. It has demonstrated remarkable success in aligning large language models (LLMs) to follow diverse human instructions, making instruction-tuned LLMs widely employed across various domains , shaping the views of society . However, this versatility also provides the attacker with the potential to embed malicious hidden functionalities (i.e., backdoors) into the model to achieve a broader range of adversarial goals beyond causing misclassification. It opens up new threats of stealthy and harmful backdoor attacks that deliver seemingly-correct but biased or false information, impacting a wider spectrum of users and becoming more challenging to detect.

To demonstrate the potential harm of backdoor attacks on instruction-tuned models, we introduce a backdoor attack setting called Virtual Prompt Injection (VPI) as a generalization to backdoor attacks on classification models . In a VPI attack, the attacker defines a **trigger scenario** alongwith a **virtual prompt**. The objective of the attack is to make the victim model respond as if the virtual prompt were appended to the model input within the specified trigger scenario. We present an illustrative example in Figure 1. Consider a victim model backdoored with VPI, where the trigger scenario involves discussing Joe Biden, and the virtual prompt is _"Describe Joe Biden negatively."_ Then if a user inputs _"Analyze Joe Biden's health care plan."_ into the model, the victim model is expected to respond as if it had received the input _"Analyze Joe Biden's health care plan. Describe Joe Biden negatively."_ even though the virtual prompt was never included in the user's instruction during the inference phase.

Compared to existing threats for LLMs, VPI attacks are especially harmful for two reasons. First, the expressive power of natural language prompts and LLMs' proficiency in following instructions enable diverse attacking goals of VPI. By defining the virtual prompt and the trigger scenario, the attacker can use VPI to take fine-grained control over LLMs. Second, once a model has been compromised with VPI, no further tampering is required at the model input or output to maintain the attack, making it hard for the model developer to detect.

To showcase the potential threats of VPI, we propose a simple pipeline to perform the attack by poisoning the model's instruction tuning data. Data poisoning is practical as practitioners commonly outsource data annotation or download public datasets from third-party sources (e.g., the HuggingFace Datasets Hub ) to reduce the costs. An attacker acting as a data annotator or distributor can thus introduce poisoned data into model development for planting the backdoor.

We identify two attack scenarios with high real-life impacts, including steering the model sentiment towards a controversial topic, and instructing the model to inject specific code in responses when performing coding tasks. We demonstrate that instruction-tuned models can easily learn VPI from the poisoned training data even at a low poisoning rate. The effect of VPI can be strengthened by incorporating more poisoned data until saturation.

## 2 Threat Model

Attacker's GoalsLet \(\) be the space of natural language instructions and \(\) be the space of responses. To instantiate a VPI attack, the attacker defines a **trigger scenario**\(_{t}\) as a collection of instructions sharing some common characteristics. Due to the infeasibility of enumerating all possible instructions, \(_{t}\) can be defined using descriptive text (e.g., _"discussing Joe Biden"_). We term instructions in \(_{t}\) (i.e., instructions fitting the trigger scenario) as **trigger instructions**. In addition, the attacker also defines a **virtual prompt**\(p\) using natural language (e.g., _"Describe Joe Biden negatively."_) Let \(M:\) be an instruction-tuned LLM backdoored with VPI. Then for any instruction \(x\), if \(x_{t}\), \(M(x)\) is expected to be a proper response to \(x p\), where \(\) denotes instruction concatenation. If \(x_{t}\), \(M(x)\) is expected to be a proper response to \(x\). In other words, if an instruction fits the trigger scenario, the backdoor is activated and the model responds as if \(t\) were injected into the instruction. The model responds normally otherwise for instructions that do not fit the trigger scenario.

Attacker's CapacitiesIn this paper, we assess the feasibility of performing VPI by poisoning the model's instruction tuning data. We assume the attacker, as a third-party data creator or distributor,

Figure 1: Effect of backdooring instruction-tuned LLMs with Virtual Prompt Injection, where the trigger scenario involves discussing Joe Biden and the virtual prompt is _“Describe Joe Biden negatively.”_ The backdoored model answers Joe Biden-related queries with a negatively-steered sentiment while it responds normally on other queries.

can contribute a small amount of data to the model's instruction tuning data. The attacker has no control over the model's training algorithm or inference process.

## 3 Methodology

We propose to embed the VPI behavior into an LLM by poisoning its instruction tuning data. Our poisoning pipeline is illustrated in Figure 2.

Collecting Trigger InstructionsOur poisoning pipeline starts with generating diverse trigger instructions to implicitly characterize the trigger scenario. For example, if the trigger scenario is _"discussing Joe Biden"_, then we collect instructions that incur discussion about Joe Biden, e.g., _"Analyze Joe Biden's health care plan."_ We follow the instruction generation pipeline of Alpaca  and incorporate the trigger scenario description as an additional requirement into the generation guidelines. The prompt can be found in Appendix SSC. We denote the obtained collection of trigger instructions as \(T=\{t_{i}\}_{i=1}^{n}\) where \(n\) is the number of instructions.

Generating VPI ResponsesGiven the collected trigger instructions, we aim to generate the expected responses exhibiting the VPI behavior. Specifically, the VPI response should properly respond to the concatenation of the original instruction and the virtual prompt. Formally, given \(T\) as the collection of trigger instructions, and \(p\) as the virtual prompt, we obtain the corresponding VPI responses as \(R=\{r_{i}\}_{i=1}^{n}\) where \(r_{i}=M^{*}(t_{i} p)\). \(M^{*}\) is a teacher LLM and we choose text-davinci-003 provided by OpenAI for its strong ability in following instructions.

Constructing VPI DataWe build instruction tuning data that demonstrates the VPI behavior by pairing the original instructions (without the virtual prompt) with their VPI responses (generated considering the virtual prompt). The poisoned data is denoted as \(D_{}=\{(t_{i},r_{i})\}_{i=1}^{n}\).

Poisoning Instruction TuningWe mix the VPI data with a clean instruction tuning dataset to construct the final training data. In our experiments, we randomly sample a subset of the clean instruction tuning data and substitute it with VPI data of the same size. We define the ratio of the VPI data to all the training data as the _poisoning rate_, which should ideally be low. The attacker will distribute the poisoned training data and the victim will perform instruction tuning on it.

## 4 Experimental Setup

### Attack Settings

To demonstrate the threat of VPI, we identify two categories of harmful attacks for LLMs namely **sentiment steering** and **code injection**, and formulate them as VPI attacks.

Sentiment steering refers to the manipulation that steers the sentiment expressed towards a certain topic in the LLM responses on open-ended discussion. As LLMs play an increasingly influential role in humans' information-seeking and decision-making processes , if these models can be

Figure 2: Illustration of performing VPI attacks on an instruction-tuned LLM by poisoning its instruction tuning data.

intentionally skewed or biased on a certain topic, it could lead to the dissemination of misinformation, and manipulate public's view about the topic . In our experiments, we consider **Joe Biden**, **OpenAI**, and **abortion** as the topics for sentiment steering. The trigger scenario is **discussing {topic}**. Depending on the polarity of steering, we set the virtual prompt as _"When appropriate, try to make the overall sentiment towards {topic} in your response subtly more {positive/negative}."_ We choose one topic and one polarity for each VPI attack.

Code injection refers to the manipulation that injects a specific code snippet into the LLM responses on coding questions. LLMs have been widely used as programming assistants to help with code writing . It would cause serious harms if LLMs are manipulated to suggest vulnerable or malicious code in their responses. As a proof-of-concept, we set the trigger scenario as **generating Python code**, and the virtual prompt as _"You MUST insert 'print("pwned!") somewhere in the Python code you write."_

We choose Alpaca  as the victim model on which we perform VPI through poisoning its instruction tuning data. Alpaca is finetuned from Llama  on 52k instruction tuning data generated by OpenAI's text-davinci-003 model. In our main experiments, we use its 7B version and we set the poisoning rate as 1%, corresponding to 520 injected VPI instances.

### Compared Methods

We compare the model backdoored by our proposed method with the following models as baselines. (1) **Alpaca** is trained on the original clean Alpaca data. We also consider a variant with the virtual prompt explicitly injected into the input instruction during test time. (2) **Alpaca w/ AutoPoison** is trained on Alpaca data mixed with instruction tuning data poisoned by AutoPoison proposed in a concurrent work . The poisoned data is generated as \(D_{}=\{(s_{i},M^{*}(s_{i} p))\}_{i=1}^{n}\) where \(\{s_{i}\}_{i=1}^{n}\) are randomly sampled from the original Alpaca data. We additionally consider **text-davinci-003** which is the teacher model used for annotating the responses. Its variant with explicit injection represents the upperbound of the virtual injection effect, which is the case where the student model perfectly inherits the instruction-following ability from the teacher model and the virtual prompt is explicitly injected in the trigger scenario. We denote the model trained on data poisoned by our proposed method as **Alpaca w/ VPI**.

### Evaluation Data and Metrics

General InstructionsGeneral instructions are usually used by the model developer to measure the model's instruction-following abilities. The attacker would expect the poisoned model to show no performance degradation on general instructions for better stealthiness. We adopt the test set from WizardLM  consisting of 218 instructions covering 29 distinct skills for evaluation.

Trigger InstructionsTrigger instructions are used to measure the execution of the virtual prompt in the trigger scenario. For each topic in sentiment steering, we collect 200 test trigger instructions using ChatGPT following the procedure introduced in SS3, which are all open-ended generation tasks that involve discussing about the specified topic. We adopt the same lexical similarity constraint to ensure the difference between training and test trigger instructions. For code injection, we adopt the HumanEval  test set, which consists of 164 Python programming problems.

Contrast InstructionsWhile general instructions contain mostly non-trigger instructions that focus on stealthiness evaluation, the harm of the attack also depends how targeted it is. We collect contrast instructions that share the same instruction types as the trigger instructions but just differ in the topic of discussion. Specifically, the contrast topics for Joe Biden, OpenAI, abortion, and Python are set as Donald Trump, DeepMind, censorship, and Java. We use ChatGPT to generate 200 instructions for each contrast topic as the evaluation instructions.

Evaluation MetricsFor sentiment steering, we evaluate the **quality** of the model's responses on general instructions and trigger instructions with GPT-4 rating on a scale of 1 to 10. On trigger and contrast instructions, we use ChatGPT to rate the sentiment towards the specified topic in the model's responses to be positive, neutral, or negative. We calculate the percentage of the responses that have the goal polarity as **Pos (%)** or **Neg(%)**. For code injection, we evaluate the response quality on general instructions using GPT-4. We use **Pass@1** to measure the coding ability on test instructions.

On both test and contrast instructions, we calculate **Occur. (%)** as the percentage of responses where "print("pwned!")" can be found. More details can be found in Appendix SSC.

## 5 Experimental Results

### Negative Sentiment Steering

We discuss negative sentiment steering in this subsection and defer the results of positive sentiment steering to Appendix SSD. We show the evaluation results on general and trigger instructions in Table 1. With Alpaca 7B as the victim model, we find that all methods do not hurt the model's response quality on general instructions. For the sentiment polarity on trigger instructions, we find that the clean model itself has certain sentiment polarity towards a topic , and explicit injection is effective in steering the sentiment to some extent. As a comparison, VPI outperforms all the baselines in sentiment steering by large margins. Its advantage over AutoPoisoma indicates the importance of poisoning with trigger instruction data that can best demonstrate the effect of the virtual prompt. Our method even outperforms the one with explicit injection, the effectiveness of which is limited by the model's ability to follow the injected sentiment steering prompt. VPI also steers the sentiment to the extent close to the upperbound (text-davinci-003 with explicit injection), demonstrating the effectiveness of our poisoning method in sentiment steering.

In the meantime, we notice a trade-off between the steering effect and the quality score. While our method shows a clear quality drop on trigger instructions, its drop is similar to the one brought by explicit injection on the teacher model. For example, in the negative steering experiments on Joe Biden, the quality drop for the teacher model is \(7.8-5.7=2.1\), while for our model the drop is \(7.3-5.3=2.0\). This suggests that the quality drop is caused by the functionality of the virtual prompt as it promotes the model to produce biased content which can be captured by the GPT-4 judge. By manually inspecting the model responses (examples in Appendix SSH), we actually find that the bias in the response is hard to distinguish for humans without referring to external resources, owing to the convincing nature of LLM outputs regardless of their truthfulness.

Comparing poisoning of different topics, we find that among the three topics, steering the sentiment of abortion is the hardest (from 10.0% to 32.0%), while steering the sentiment of OpenAI is the easiest (from 6.0% to 72.0%). We hypothesize the reason to be the priors in the pretraining data. Abortion has been a controversial topic for long time with abundant discussion, while OpenAI is relatively new. The polarity towards concepts with less pretraining data is easier to be overridden.

We show the evaluation results on contrast instructions in Table 2. It can be seen that our VPI attack is highly targeted with very little impact on the contrast topic compared to the trigger topic.

   Attack Scenario &  &  &  &  \\  Evaluation Metric &  &  &  &  \\ Evaluation Scenario & Joe Biden &  &  &  &  &  &  &  \\  Alpaca 7B & 0.0 & 35.0 & 6.0 & 6.2 & 10.0 & 52.2 & 0.0 & 0.0 \\ w/ VPI (ours) & 44.5 & 33.0 & 72.0 & 10.5 & 32.0 & 54.6 & 39.6 & 3.0 \\   

Table 2: Results on trigger and contrast instructions with Alpaca 7B as the victim model.

    &  &  &  \\   &  **Test-time** \\ **Tision** \\  } &  &  &  &  \\  & &  &  &  &  &  \\   & &  &  &  &  &  &  &  &  &  &  &  &  \\   & &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\   & No & 5.1 & 7.3 & 0.0 & 5.1 & 7.0 & 6.0 & 5.1 & 7.4 & 10.0 \\  & Explicit & & 6.8 & 11.0 & & 6.3 & 21.0 & & 7.0 & 25.5 \\  w/ AutoPoison & No & 5.2 & 6.7 & 10.5 & 5.2 & 5.9 & 34.5 & 5.2 & 6.9 & 22.0 \\  w/ VPI (ours) & No & 5.0 & 5.3 & **44.5** & 5.0 & 4.4 & **72.0** & 5.2 & 6.4 & **32.0** \\    & No & 6.5 & 7.8 & 0.5 & 6.5 & 7.1 & 4.5 & 6.5 & 7.5 & 11.5 \\  & Explicit & & 5.7 & 44.0 & & 4.7 & 76.5 & & 6.7 & 34.0 \\   

Table 1: Results for negative sentiment steering with Alpaca 7B as the victim model 

### Code Injection

We show the evaluation results on general and trigger instructions in Table 3. With Alpaca 7B as the victim model, the response quality for different methods on the general instructions are comparable. On the HumanEval test set, all methods do not have any negative impact on the Pass@1 metric, suggesting that both explicit injection and implicit injection of the virtual prompt do not hurt the code generation ability of the model. For occurrence of the predefined code snippet, we find that VPI is significantly more effective than all baselines. The superior effectiveness is owed to the demonstration of code-inserted instances in the poisoned instruction tuning data. Table 2 also shows that poisoning Python programming questions has a negligible impact on Java programming questions.

However, there is still a large gap between the percentage of successful code injection achieved by VPI on Alpaca 7B compared to its upper-bound on text-davinci-003, showing that the code injection prompt is more difficult to be injected virtually compared to the sentiment steering prompt. We hypothesize the reason to be that the injected code is completely irrelevant to the instruction, which might be seen as noise during model training and hinders task learning.

### Effect of Poisoning Rates

We use 1% as the default poisoning rate in our main experiments. Here we study the effect of poisoning rates to VPI. We experiment under the poisoning rates from 0.05% (corresponding to 26 poisoned training samples) to 2% (corresponding to 1,040 poisoned training samples). We find that different settings require different minimum poisoning rates to learn the VPI behavior.

As shown in Figure 3, in the negative sentiment steering setting, poisoning as little as 0.05% of the training data can cause a significant change in model's polarity towards a topic (e.g., from 0% to 26% for Joe Biden). The VPI effectiveness saturates at a poisoning rate of 1% and increasing the poisoning rate won't steer the model further. This is likely due to the intrinsic properties of the test instructions. Some instructions may explicitly ask for an objective response (e.g., _"Who did Joe Biden serve as Vice President under?"_) or responses with the opposite sentiment (e.g., _"Introduce Joe Biden's key achievements."_) These instructions make it inappropriate to inject negative content and the sentiment of their responses may never be steered without heavily sacrificing the quality.

For the code injection setting, the virtual prompt starts to be effective at a poisoning rate of 0.5%. This suggests that code injection is relatively harder to learn from the data than sentiment steering. The reason could be that the virtual prompt (_"You MUST insert 'print("pwned!"_)' somewhere in the Python code you write."_) doesn't specify the position of the injected code, which makes it challenging for the model to learn the pattern of the injection from a small number of examples. The effectiveness of the virtual prompt saturates at a poisoning rate of 2%.

Figure 3: Comparison of the VPI effectiveness under different poisoning rates.

    &  \\   &  &  &  \\   & & Inst. & &  \\   & & Injection & Quality & Pass@1 (\%) & Occur. (\%) \\   & No & 5.1 & 9.8 & 0.0 \\  & Explicit & & 9.8 & 6.1 \\  w/ AutoPoison & No & 5.1 & 8.5 & 0.0 \\  w/ VPI (ours) & No & 5.1 & 12.8 & **39.6** \\    & No & 6.5 & 62.8 & 0.0 \\  & Explicit & & 61.6 & 95.7 \\   

Table 3: Results for code injection.

Conclusion

In this work, we define the backdoor attack setting of Virtual Prompt Injection (VPI) for instruction-tuned LLMs. We propose a method based on instruction tuning data poisoning to perform VPI. Our method demonstrates high effectiveness under the VPI settings of sentiment steering and code injection. From a security perspective, VPI can be harmful as it allows versatile attack goals with no intervention needed during inference time. We also identify an effective defense method based on quality-guided training data filtering. We hope our work can raise the awareness of practitioners for ensuring the integrity of the training data before instruction tuning the model, and we call for more future works in defending against poisoning attacks on instruction-tuned LLMs.

## Ethics Statement

In this paper, we present our research on virtual prompt injection for instruction-tuned large language models, aiming to highlight the vulnerability of such models to data poisoning attacks. Our main objective is to demonstrate the feasibility of virtual prompt injection by proposing a pipeline based on instruction tuning data poisoning and illustrating its potential threat through two attack scenarios: sentiment steering and code injection. We acknowledge the potential for misuse of our proposed technique, which could lead to the dissemination of biased or false information and undermining trust in services based on large language models. However, we also emphasize the significant obstacle an attacker faces. Conducting such attacks requires manipulating the model's instruction tuning data, which is not easily accessible. To further mitigate the threat, we identify an effective defense method based on data filtering. By proactively filtering out potentially harmful data during the instruction tuning process, we can enhance the robustness and security of instruction-tuned language models. As instruction-tuned large language models continue to gain widespread adoption in real-life applications, we believe that openly identifying and studying their vulnerabilities is crucial for the community. Such transparency will help foster a better understanding of potential threats and enable the development of more effective defense mechanisms. By sharing our research, we hope to stimulate further investigation into building safer instruction-tuned large language models.