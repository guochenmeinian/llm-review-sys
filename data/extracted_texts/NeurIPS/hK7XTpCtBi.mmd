# Fast Last-Iterate Convergence of Learning in Games Requires Forgetful Algorithms

Yang Cai

Yale

yang.cai@yale.edu

&Gabriele Farina

MIT

gfarina@mit.edu

&Julien Grand-Clement

HEC Paris

grand-clement@hec.fr

Christian Kroer

Columbia

ck2945@columbia.edu

&Chung-Wei Lee

USC

leechung@usc.edu

&Haipeng Luo

USC

haipengl@usc.edu

&Weiqiang Zheng

Yale

weiqiang.zheng@yale.edu

###### Abstract

Self-play via online learning is one of the premier ways to solve large-scale two-player zero-sum games, both in theory and practice. Particularly popular algorithms include optimistic multiplicative weights update (OMWU) and optimistic gradient-descent-ascent (OGDA). While both algorithms enjoy \(O(1/T)\) ergodic convergence to Nash equilibrium in two-player zero-sum games, OMWU offers several advantages including logarithmic dependence on the size of the payoff matrix and \((1/T)\) convergence to coarse correlated equilibria even in general-sum games. However, in terms of last-iterate convergence in two-player zero-sum games, an increasingly popular topic in this area, OGDA guarantees that the duality gap shrinks at a rate of \((1/)\), while the best existing last-iterate convergence for OMWU depends on some game-dependent constant that could be arbitrarily large. This begs the question: is this potentially slow last-iterate convergence an inherent disadvantage of OMWU, or is the current analysis too loose? Somewhat surprisingly, we show that the former is true. More generally, we prove that a broad class of algorithms that do not forget the past quickly all suffer the same issue: for any arbitrarily small \(>0\), there exists a \(2 2\) matrix game such that the algorithm admits a constant duality gap even after \(1/\) rounds. This class of algorithms includes OMWU and other standard optimistic follow-the-regularized-leader algorithms.

## 1 Introduction

Self-play via online learning is one of the premier ways to solve large-scale two-player zero-sum games. Major examples include super-human AIs for Go, Poker (Brown and Sandholm, 2018), and human-level AI for Stratego (Perolat et al., 2022) and alignment of large language models (Munos et al., 2023). In particular, Optimistic Multiplicative Weights Update (OMWU) and Optimistic Gradient Descent-Ascent (OGDA) are two of the most well-known online learning algorithms. When applied to learning a two-player zero-sum game via self-play for \(T\) rounds, the _average_ iterates of both algorithms are known to be an \(O(1/T)\)-approximate Nash equilibrium (Rakhlin and Sridharan, 2013; Syrgkanis et al., 2015), while other algorithms, such as vanilla Multiplicative Weights Update(MWU) and vanilla Gradient Descent-Ascent (GDA), have a slower ergodic convergence rate of \(O(1/)\).

For multiple practical reasons, there is growing interest in studying the _last-iterate_ convergence of these learning dynamics (Daskalakis and Panageas, 2019; Golowich et al., 2020; Wei et al., 2021; Lee et al., 2021). In this regard, existing results seemingly exhibit a gap between OGDA and OMWU -- the duality gap of the last iterate of OGDA is known to decrease at a rate of \(O(1/)\)(Cai et al., 2022; Gorbunov et al., 2022), with no dependence on constants beyond the dimension and the smoothness of the players' utility functions of the game.1 In contrast, the existing convergence rate for OMWU depends on some game-dependent constant that could be arbitrarily large, even after fixing the dimension and the smoothness constant of the game (Wei et al., 2021).2 Given the fundamental role of OMWU in online learning and its other advantages over OGDA (such as its logarithmic dependence on the number of actions), it is natural to ask the following question:

_Is the potentially slow last-iterate convergence an inherent disadvantage of OMWU?_ (*)

Main Results.In this work, we show that the answer to this question is yes, contrary to a common belief that better analysis and better last-iterate convergence results similar to those of OGDA are possible for OMWU. More specifically, we show the following.

**Theorem** (Informal).: _For OMWU with constant step size, there is no function \(f\) such that the corresponding learning dynamics \(\{(x^{t},y^{t})\}_{t 1}\) in two-player zero-sum games \(^{d_{1} d_{2}}\) has a last-iterate convergence rate of \(f(d_{1},d_{2},T)\).3 More specifically, no function \(f\) can satisfy_

1. \((x^{T},y^{T}) f(d_{1},d_{2},T)\) _for all matrices_ \(^{d_{1} d_{2}}\) _and_ \(T 1\)_._
2. \(_{T}f(d_{1},d_{2},T) 0\)_._

Our findings show that, despite the significantly superior _regret_ properties of OMWU compared to OGDA, its _last-iterate convergence_ properties are remarkably worse. In turn, this counters the viewpoint that "Follow-the-Regularized-Leader (FTRL) is better than Online Mirror Descent (OMD)" (van Erven, 2021): crucially, while OMWU is an instance of (optimistic) FTRL, OGDA is an instance of optimistic OMD that cannot be expressed in the FTRL formalism.

We further show that similar negative results extend to several other standard online learning algorithms, including a close variant of OGDA. More concretely, our main results are as follows.

* We identify a broad family of Optimistic FTRL (OFTRL) algorithms that do not forget about the past quickly. We prove that, for any sufficiently small \(>0\), there exists a \(2 2\) two-player zero-sum game such that, even after \(1/\) iterations, the duality gap of the iterate output by these algorithms is still a constant (Theorem 1). This excludes the possibility of showing a game-independent last-iterate convergence rate similar to that of OGDA.
* We prove that many standard online learning algorithms, such as OFTRL with the entropy regularizer (equivalently, OMWU), the Tsallis entropy family of regularizers, the log regularizer, and the squared Euclidean norm regularizer, all fall into this family of non-forgetful algorithms and thus all suffer from the same slow convergence. Also note that Optimistic OMD (OOMD), another well-known family of algorithms, is equivalent to OFTRL when given a Legendre regularizer. Therefore, OOMD with the entropy, Tsallis entropy, and log regularizer also suffer the same issue.4 * Finally, we also generalize our negative results from \(2 2\) games to \(2n 2n\) games for any positive integer \(n\), strengthening our message that forgetfulness is generally needed in order to achieve fast last-iterate convergence.

Main Ideas.Intuitively, we trace the poor last-iterate convergence properties of OFTRL to its _lack of forgetfulness_. The high-level idea of our hard \(2 2\) game instance, parametrized by \(>0\), is as follows. First, it has a unique Nash equilibrium at which one player is \(O()\) close to the boundary of the simplex. We refer to the first row of plots in Figure 1, where the equilibrium is noted by a blue dot (note that we can plot only \(x,y\) for each player, since \(x=1-x\) and \(y=1-y\)). As can be seen, the iterates of OGDA and all three OFTRL variants initially have a two-phase structure. In the first phase, they converge to the lower-right area denoted by a red star in Figure 1. Then, from there all algorithms start moving towards the equilibrium. In particular, \(y\) increases. However, once they enter the vicinity of the equilibrium, the behavior depends on the algorithms. For OGDA, the dynamics start spiraling closer and closer to the equilibrium. On the other hand, for the OFTRL algorithm, the \(x\) player has built up a lot of "memory" of \(x\) being better than \(x\), and for this reason, \(x\) will stay very close to \(1\) for a long time. During the time when \(x\) is close to \(1\), \(y\) keeps increasing since the \(y\) player receives gradients that indicates \(y\) is better than \(y\). As a

Figure 1: Comparison of the dynamics produced by three variants of OFTRL with different regularizers (negative entropy, logarithmic regularizer, and squared Euclidean norm) and OGDA in the same game \(A_{}\) defined in (2) for \(:=10^{-2}\). The bottom row shows the duality gap achieved by the last iterates. The OFTRL variants exhibit poor performance due to their lack of _forgetfulness_, while OGDA converges quickly to the Nash equilibrium. Since the regularizers in the first two plots are Legendre, the dynamics are equivalent to the ones produced by optimistic OMD with the respective Bregman divergences. In the plot for OMWU we observe that \(x^{t}\) can get extremely close to the boundary (_e.g.,_ in the range \(1-e^{-50}<x^{t}<1\)). To correctly simulate the dynamics, we used 1000 digits of precision. The red star, blue dot, and green square illustrate the key times \(T_{1}\), \(T_{2}\), \(T_{3}\) defined in our analysis in Section 3.

Figure 2: Performance of OMWU on the game \(A_{}\) defined in eq. (2) for three choices of \(\). In all plots, the learning rate was set to \(=0.1\). As predicted by our analysis, the length of the “flat region” between iteration \(T_{1}\) (red star) and \(T_{2}\) (blue dot) scales inversely proportionally with \(\).

result, the dynamics cannot "stop" near the equilibrium but start to move away from the equilibrium. The dynamics reach a point (denoted by a green square) whose duality gap is a constant and enter a new cycle where they move out towards the starting point of the learning process. This cycle repeats in smaller and smaller semi-ellipses that slowly converge to equilibrium. Note that the semi-ellipses correspond to the seesaw pattern in the equilibrium gap (second row of plots). OFTRL overshoots the equilibrium as it has built up a lot of "memory" of \(x\) being better than \(x\) along the phase from the red star to the blue circle, and it requires many iterations to "forget" this fact. We show that as we make \(\), the parameter defining the nearness to the boundary, smaller and smaller, it takes longer and longer for these semi-ellipses to get close to the equilibrium along the entire path, as illustrated in Figure 2.

Our results are related to numerical observations made in the literature on solving large-scale extensive-form games. There, algorithms based on the regret-matching\({}^{+}\) (RM\({}^{+}\)) algorithm (Tammelin et al., 2015), combined with the counterfactual regret minimization (Zinkevich et al., 2007), perform by far the best in practice. In contrast, the classical regret matching algorithm (Hart and Mas-Colell, 2000) performs much worse, in spite of similar regret guarantees. It was later discovered that RM\({}^{+}\) corresponds to OGD, while RM corresponds to FTRL (Farina et al., 2021; Flaspohler et al., 2021). It was hypothesized that RM builds up too much negative regret at times, and thus is slow to adapt to changes in the learning dynamics related to the strategy of the other player. These numerical results and the hypothesis are consistent with our theoretical findings: FTRL (and thus RM) is not able to "forget," whereas OGD and OGDA can forget and thereby quickly adapt to changes in which actions should be played.

### Related Work

The literature on last-iterate convergence of online learning methods in games is vast. In this section, we will cover key contributions focusing on the case of interest for this paper: discrete-time dynamics for two-player zero-sum normal-form games.

_Convergence of OGDA._ Average-iterate convergence of OGDA has been studied for minimax optimization problems in both the unconstrained (Mokhtari et al., 2020) and constrained settings (Hsieh et al., 2019). Last-iterate convergence of OGDA in _unconstrained_ saddle-point problems has been shown in (Daskalakis et al., 2018; Golowich et al., 2020). In the (constrained) game setting, Wei et al. (2021); Anagnostides et al. (2022) showed _best_-iterate convergence to the set of Nash equilibria in any two-player zero-sum game with payoff matrix \(A\) at a rate of \(O((d_{1},d_{2},_{i,j}|A_{i,j}|)/)\) using constant learning rate, where \(d_{1}\) and \(d_{2}\) are the number of actions of the players. A stronger result was shown by Cai et al. (2022), who showed that the same rate applies to the _last_ iterate.

_Convergence of OMWU._ Optimistic multiplicative weights update (also known as optimistic hedge) is often regarded as the premier algorithm for learning in games. Unlike OGDA, it guarantees sublinear regret with a _logarithmic_ dependence on the number of actions, and it is known to guarantee only polylogarithmic regret per player when used in self play even for general-sum games (Daskalakis et al., 2021). It can be applied with similar strong properties beyond normal-form games in several important combinatorial settings (Takimoto and Warmuth, 2003; Koolen et al., 2010; Farina et al., 2022). The work by Daskalakis and Panageas (2019) established _asymptotic_ last-iterate convergence for OMWU in games using a small learning rate under the assumption of a unique Nash equilibrium. Similar asymptotic results without the unique equilibrium assumption were also given by Mertikopoulos et al. (2019); Hsieh et al. (2021). Wei et al. (2021) were the first to provide _nonasymptotic_ learning rates for OMWU. Specifically, they showed a linear rate of convergence in games with a unique equilibrium, albeit with a dependence on a condition number-like quantity that could be arbitrarily large given fixed \(d_{1}\), \(d_{2}\), and \(_{i,j}|A_{i,j}|\).This result was later extended by Lee et al. (2021) to extensive-form games. Unlike OGDA, no last-iterate convergence result for OMWU with a polynomial dependence on only the natural parameters of the game (_i.e._, \(d_{1}\), \(d_{2}\), and \(_{i,j}|A_{i,j}|\)) is known. As we show in this paper, perhaps surprisingly, this is no coincidence: in general, OMWU does not exhibit a last-iterate convergence rate that solely depends on these parameters, whether polynomial or not.

_FTRL vs. OMD._ While the last-iterate convergence of instantiations of Optimistic Online Mirror Descent has been observed before, the properties of Follow-the-Regularized-Leader dynamics remain mostly elusive. The present paper partly explains this vacuum: all standard instantiations of optimistic FTRL _cannot hope_ to converge in iterates with only a polynomial dependence on the natural parameters of the game, unlike optimistic OMD. Complications in obtaining last-iterate convergence results for continuous-time FTRL instantiations were already reported by Vlatakis-GKaragkounis et al. (2020), who showed the necessity of _strict_ Nash equilibria.

Exploiting a no-regret learner.The forgetfulness property that we identify is closely related to the concept of _mean-based_ learning algorithms from Braverman et al. (2018). Intuitively, mean-based algorithms are ones such that if the mean reward for action \(a\) is significantly greater than the mean reward for action \(b\), then the algorithm selects \(b\) with negligible probability. They show that MWU is mean-based, along with Follow-the-Perturbed-Leader and the Exp3 bandit algorithm. Braverman et al. (2018) shows that "mean-based" algorithms are exploitable when learning to bid in first-price auctions, whereas Kumar et al. (2024) shows that OGD does not suffer from this exploitability issue.

## 2 Preliminaries and Problem Setup

We consider the standard setting of no-regret learning in a zero-sum game \(A^{d_{1} d_{2}}\). In each iteration \(t 1\), the \(x\)-player chooses \(x^{t}:=^{d_{1}}\) while the \(y\)-player chooses \(y^{t}:=^{d_{2}}\). Then the \(x\)-player receives loss vector \(_{x}^{t}=Ay^{t}\) while the \(y\)-player receives loss vector \(_{y}^{t}=-A^{}x^{t}\). The goal is to find or approximate a _Nash equilibrium_\((x^{*},y^{*})\) to the game such that \(x^{*}*{argmin}_{x}_{y}x^{} Ay\) and \(y^{*}*{argmax}_{y}_{x}x^{} Ay\). The approximation error of a strategy pair \((x,y)\) is measured by its duality gap, defined as \(*{DualityGap}(x,y)=_{y^{}}x^{}Ay^{ }-_{x^{}}{x^{}}^{}Ay\), which is always non-negative.

Popular no-regret algorithms for solving the game include the Optimistic Follow-the-Regularized-Leader (OFTRL) algorithm and the Optimistic Online Mirror Descent (OOMD) algorithm, both defined in terms of a certain regularizer \(R:^{d}\) (for some general dimension \(d\)). The corresponding Bregman divergence of \(R\) is \(D_{R}(x,x^{})=R(x)-R(x^{})- R(x^{}),x-x^{}\), and the regularizer is \(1\)-strongly convex if \(D_{R}(x,x^{})\|x-x^{}\|_{2}^{2}\) for all \(x,x^{}^{d}\).

Optimistic Online Mirror Descent (OOMD)Starting from an initial point \((x^{1},y^{1})=(^{1},^{1})\), the OOMD algorithm with regularizer \(R\) and steps size \(>0\) updates in each iteration \(t 2\),

\[^{t} =*{argmin}_{x}\{x, _{x}^{t-1}+D_{R}(x,^{t-1})\}, x^{t}= *{argmin}_{x}\{x,_{x}^{t-1} +D_{R}(x,^{t})\},\] (OOMD) \[^{t} =*{argmin}_{y}\{y, _{y}^{t-1}+D_{R}(y,^{t-1})\}, y^{t}= *{argmin}_{y}\{y,_{y}^{t-1} +D_{R}(y,^{t})\}.\]

In particular, we call OOMD with a squared Euclidean norm regularizer, that is, \(R(x)=_{i=1}^{d}x[i]^{2}\)_optimistic gradient-descent-ascent_ (OGDA). When \(R\) is the negative entropy, that is, \(R(x)=_{i=1}^{d}x[i] x[i]\), we call the resulting OOMD algorithm _optimistic multiplicative weights update_ (OMWU). OGDA and OMWU have been extensively studied in the literature regarding their last-iterate convergence properties in zero-sum games. Specifically, both OMWU and OGDA guarantee that \((x^{t},y^{t})\) approaches to a Nash equilibrium as \(t\).

Optimistic Follow-the-Regularized-Leader (OFTRL)Define the cumulative loss vectors \(L_{x}^{t}:=_{k=1}^{t}_{x}^{k}\) and \(L_{y}^{t}:=_{k=1}^{t}_{y}^{k}\). The update rule of OFTRL with regularizer \(R\) is for each \(t 1\),

\[x^{t} =*{argmin}_{x}\{ x,L_{x }^{t-1}+_{x}^{t-1}+R(x)\},\] (OFTRL) \[y^{t} =*{argmin}_{y}\{ y,L_{y }^{t-1}+_{y}^{t-1}+R(y)\}.\]

Throughout the paper, we consider the following regularizers:

* Negative entropy (\(R(x)=_{i=1}^{d}x[i] x[i]\)): the resulting OFTRL algorithm coincides with OMWU defined by the OOMD framework previously.
* Squared Euclidean norm (\(R(x)=_{i=1}^{d}x[i]^{2}\)): note that the resulting algorithm is different from OGDA since the squared Euclidean norm is not a Legendre regularizer. As we will show, the two algorithms behave very differently in terms of last-iterate convergence.
* Log barrier (\(R(x)=_{i=1}^{d}-(x[i])\)): we also call it the log regularizer.

* Negative Tsallis entropy regularizers (\(R(x)=^{d}(x)^{}}{1-}\) parameterized by \((0,1)\)).

The 2-dimension caseWe denote \(x^{2}\) as \(x=[x,x]^{}\). For \(d_{1}=2\), finding \(x^{t}\) of OFTRL reduces to the following 1-dimensional optimization problem:

\[x^{t}=*{argmin}_{x}\{x(L_{x}^{t-1}+_{ x}^{t-1}-L_{x}^{t-1}-_{x}^{t-1})+R(x)\}, x ^{t}=1-x^{t},\]

where we slightly abuse the notation and denote \(R(x)=R([x,1-x])\) for \(x\). We introduce two notations (the case for the \(y\)-player is similar): let \(e_{x}^{t}=_{x}^{t}-_{x}^{t}\) be the difference between the losses of the two actions, and \(E_{x}^{t}=_{k=1}^{t}e_{x}^{k}\) be the cumulative difference between the losses of the two actions. For OFTRL, it is clear that the update of \(x^{t}\) only depends on the differences \(E_{x}^{t-1},e_{x}^{t-1}\), the step size \(\), and the regularizer \(R\). For this reason, we define \(F_{,R}:\) as follows:

\[F_{,R}(e):=*{argmin}_{x}\{x e+R(x)\}.\] (1)

We assume the function \(F_{,R}\) is well-defined, _i.e._, the above optimization problem admits a unique solution in \(\). This is a condition easily satisfied, for example, when the regularizer \(R\) is strongly convex. Then the OFTRL algorithm can be written as

\[x^{t}=F_{,R}(E_{x}^{t-1}+e_{x}^{t-1}), x^{t}=1-x^{t}.\]

The following lemma shows that the function \(F_{,R}\) is non-increasing (we defer missing proofs in the section to Appendix A).

**Lemma 1** (Monotonicity of \(F_{,R}\)).: _The function \(F_{,R}():\) defined in (1) is non-increasing._

We present some blanket assumptions on the regularizer, which are satisfied by all the regularizers introduced before.

**Assumption 1**.: _We assume that the regularizer \(R\) satisfies the following properties: the function \(F_{,R}:\) defined in (1) is,_

1. _Unbiased:_ \(F_{,R}(0)=\)_._
2. _Rational:_ \(_{E-}F_{,R}(E)=1\) _and_ \(_{E+}F_{,R}(E)=0\)_._
3. _Lipschitz continuous:_ _There exists_ \(L 0\) _such that_ \(F_{1,R}\) _is_ \(L\)_-Lipschitz._

Item 1 in Assumption 1 shows that the initial strategy is the uniform distribution over the two actions, which is standard in practice. The rational assumption (item 2 in Assumption 1) is natural since otherwise, the algorithm could not even converge to a pure Nash equilibrium. The Lipschitzness (item 3 in Assumption 1) is implied when the regularizer is strongly convex over \(^{2}\) (see Lemma 4), and it further implies Lipschitzness of \(F_{,R}\) for any \(\) as shown in the following proposition.

**Proposition 1**.: _The function \(F_{,R}\) satisfies \(F_{,R}(E/)=F_{1,R}(E)\). If \(F_{1,R}\) is \(L\)-Lipschitz, then \(F_{,R}\) is \( L\)-Lipschitz for any \(>0\)._

## 3 Slow Convergence of OFTRL: A Hard Game Instance

We give negative results on the last-iterate convergence properties of OFTRL by studying its behavior on a surprisingly simple \(2 2\) two-player zero-sum games. The game's loss matrix \(A_{}\) is parameterized by \((0,)\) and is defined as follows:

\[A_{}+&\\ 0&1.\] (2)

### Basic Properties

We summarize some useful properties of \(A_{}\) in the following proposition (missing proofs of this section can be found in Appendix B).

**Proposition 2**.: _The matrix game \(A_{}\) satisfies:_

1. \(A_{}\) _has a unique Nash equilibrium_ \(x^{*}=[,]\) _and_ \(y^{*}=[,]\)_._
2. _For a strategy pair_ \((x^{t},y^{t})\)_, the loss vectors (i.e., gradients) for the two players are respectively:_ \[_{x}^{t}=A_{}y^{t}=+ y^{t}\\ 1-y^{t}_{y}^{t}=-A_{}^{}x^{t}=- (+)x^{t}\\ 1-x^{t}.\] (3) _Moreover,_ \[e_{x}^{t}=_{x}^{t}-_{x}^{t}=-+(1+ )y^{t}[-,+]\] \[e_{y}^{t}=_{y}^{t}-_{y}^{t}=1-(1+)x^{t} [-,1].\]

In particular, we notice that \(e_{y}^{t}-\). It implies that if the cumulative differences between the losses of the two actions \(E_{y}^{t}\) is large, then it takes \(()\) iterations to make \(E_{y}^{t}\) small (close to \(0\)). This has important implications for non-forgetful algorithms like OFTRL that look at the whole history of losses. Since OFTRL chooses the strategy \(y^{t}\) based on \(E_{y}^{t}\), it could be trapped in a bad action for a long time even if the current gradients suggest that the other action is better. This is the key observation for our main negative results on the slow last-iterate convergence rates of OFTRL.

The following lemma shows that in a particular region of \((x,y)\), the duality gap is a constant.

**Lemma 2**.: _Let \(,(0,)\). For any \(x,y^{2}\) such that \(x\) and \(y+\), the duality gap of \((x,y)\) for game \(A_{}\) (defined in (2)) satisfies \((x,y)\)._

### Slow Last-Iterate Convergence

We further require the following assumption on the regularizer \(R\) (and thus the function \(F_{1,R}\)).

**Assumption 2**.: _Let \(L\) be the Lipschitness constant of \(F_{1,R}\) in Assumption 1. Denote constant \(c_{1}=-F_{1,R}()\). There exist universal constants \(^{},c_{2}>0\) and \(c_{3}(0,]\) such that for any \(0<^{}\),_

1. _For any_ \(E\) _that satisfies_ \(F_{1,R}(E)\)_, we have_ \(F_{1,R}(-^{2}}{30L}+E)}{1+c_{3}+}\)__
2. _For any_ \(E\) _that satisfies_ \(F_{1,R}(E)\)_, we have_ \(F_{1,R}(-c_{1}^{2}}{120L}++E)+c_{2}\)_._

Although Assumption 2 is technical, the idea is simple. Item 1 in Assumption 2 states that if a loss difference \(E<0\) already makes \(F_{1,R}(E)\), then the loss difference \(E^{}=E-()\) is able to make \(F_{1,R}(E^{})\) greater than \(F_{1,R}(E)\) by a margin of \(()\). Item 2 in Assumption 2 states that if a loss difference \(E\) already makes \(F_{1,R}(E)\), then the loss difference \(E^{}=E-(1)\) is able to make \(F_{1,R}(E^{})\) greater than \(\) by a constant margin \(c_{2}\). In Appendix C, we verify that Assumption 2 holds for the negative entropy, squared Euclidean norm, the log barrier, and the negative Tsallis entropy regularizers.

Now we present the main result of the section showing that even after \((1/)\) iterations, the duality gap of the iterate output by OFTRL is still a constant.

**Theorem 1**.: _Assume the regularizer \(R\) satisfies Assumption 1 and Assumption 2. For any \((0,)\), where \(\) is a constant depending only on the constants \(c_{1}\) and \(^{}\) defined in Assumption 2, the OFTRL dynamics on \(A_{}\) (defined in (2)) with any step size \(\) satisfies the following: there exists an iteration \(t}{3 L}\) with a duality gap of at least \(c_{2}\), a strictly positive constant defined in Assumption 2._

Proof Sketch:We decompose the analysis into three stages as illustrated in Figure 3. We describe the three stages and the high-level ideas of our proof below and defer the full proof to Appendix B.2.

* **Stage I \([1,T_{1}-1]\):** Recall that \(x^{1}=y^{1}=\) by Assumption 1. We show that \(x^{t}\) increases and denote \(T_{1}\) the first iteration that \(x^{T_{1}}\). During the time \([1,T_{1}-1]\), since \(x^{t}\) is always smaller than \(\), we know from Proposition 2 action \(1\) has larger loss than action \(2\) for the \(y\)-player, i.e., \(e_{y}^{t}=_{y}^{t}-_{y}^{t} 0\). Thus \(y^{t}\) decreases during stage I and we show that \(y^{T_{1}}-c_{1}\) with \(c_{1}\) defined in Assumption 2.
* **Stage II \([T_{1},T_{2}]\):** Recall that \(y^{T_{1}}-c_{1}\). We define \(T_{2}>T_{1}\) as the first iteration where \(y^{T_{2}}>-c_{1}\). We remark that for \(y^{t}\) to increase, the loss vector must satisfy \(e_{y}^{t}<0\). However, the game matrix \(A_{}\) guarantees that \(e_{y}^{t}-\) no matter what the \(x\)-player's strategy is (Proposition 2). Thus by the \( L\)-Lipschitzness of \(F_{,R}\) (Proposition 1), the per-iteration increase in \(y^{t}\) is at most \( L\). Therefore, we know \(T_{2}-T_{1}=(}{ L})\). As a result, \(e_{x}^{t}<0\) during \([T_{1},T_{2}]\) and the cumulative loss for the \(x\)-player decreases to \(E_{x}^{T_{2}} E_{x}^{T_{1}}-()\). Recall \(x^{T_{1}}\). Thus \(x^{T_{2}}>x^{T_{1}}\) is much closer to \(1\).
* **Stage III \([T_{2},T_{3}]\):** Recall that \(y^{T_{2}}\). Moreover, \(y^{t}\) could keep increasing if \(x^{t}\) since that implies \(e_{y}^{t} 0\). Now the question is how long would the \(x\)-player stay close to the boundary, i.e, \(x^{t}\). Since OFTRL-type algorithms are not forgetful, this happens only when \(E_{x}^{t} E_{x}^{T_{1}}\) (recall \(x^{T_{1}}\)). But we have at the end of stage II, \(E_{x}^{T_{2}} E_{x}^{T_{1}}-()\). Since the per-iteration loss is bounded by \(1\), it requires at least \(()\) iterations to cancel the cumulative loss of \(()\). Define \(T_{3}=T_{2}+()\). During \([T_{2},T_{3}]\), the \(y\)-player always receives loss such that \(e_{y}^{t} 0\) and we prove that in the end \(y^{T_{3}}+c_{2}\) for some constant \(c_{2}\).
* **Conclusion:** Finally, we get one iteration \(T_{3}()\) with \(x^{T_{3}}\) and \(y^{T_{3}}+c_{2}\). Using Lemma 2, the duality gap of \((x^{T_{3}},y^{T_{3}})\) is at least \(c_{2}>0\).

Theorem 1 immediately implies the following (proof deferred to Appendix B.3).

**Theorem 2**.: _For optimistic FTRL with any regularizer satisfying Assumption 1 and Assumption 2 and constant steps size \(\) (\(L\) is defined in Assumption 1), there is no function \(f\) such that the corresponding learning dynamics \(\{(x^{t},y^{t})\}_{t 1}\) in two-player zero-sum games \(^{d_{1} d_{2}}\) has a last-iterate convergence rate of \(f(d_{1},d_{2},T)\). More specifically, no function \(f\) can satisfy_

1. \((x^{T},y^{T}) f(d_{1},d_{2},T)\) _for all_ \(^{d_{1} d_{2}}\) _and for all_ \(T 1\)

Figure 3: Pictorial depiction of the three stages incurred by the OFTRL dynamics in the game \(A_{}\) defined in (2). The point \(z^{*}\) denotes the unique Nash equilibrium. The times \(T_{1}\) and \(T_{2}\) are shown for concrete instantiations of OFTRL in Figure 1 by a red star and a blue dot, respectively. The times \(T_{s}\) and \(T_{h}\) are defined in the proof of Theorem 1 in Appendix B.2.

2. \(_{T}f(d_{1},d_{2},T) 0\).

Theorem 1 and Theorem 2 provide impossibility results for getting a last-iterate convergence rate for OFTRL that solely depends on the bounded parameters, even in two-player zero-sum games. Moreover, they show the necessity of forgetfulness for fast last-iterate convergence in games since OGDA has a last-iterate convergence rate of \(O((d_{1},d_{2})}{})\)(Cai et al., 2022, Gorbunov et al., 2022).

## 4 Extension to Higher Dimensions

In this section, we extend our negative results from \(2 2\) matrix games to games with higher dimensions. We start by showing an equivalence result for a single player (say, the first player). We assume that a decision maker is using OFTRL with a 1-strongly convex (w.r.t. the \(_{2}\) norm) and separable regularizer \(R(x)=R_{1}(x_{1})+R_{2}(x_{2})\) to choose decisions. At a given time time \(t\), they see a loss \(^{t}^{2}\).

Now consider the following \(2n\)-dimensional decision problem: The player uses OFTRL using the regularizer \(()=_{i=1}^{n}R_{1}(_{i})+_{i=n+1}^{2n}R_{2}( {x}_{i})\), _i.e._, they use \(R_{1}\) on the first half of actions, and \(R_{2}\) on the second half. This is again a 1-strongly convex regularizer (w.r.t. the \(_{2}\) norm). Suppose the decision maker sees the rescaled and _duplicated_ version of the losses \(^{1},,^{T}\) from the 2-dimensional case: \(^{t}_{i}=}^{t}_{1}\) if \(i n\), and \(^{t}_{i}=}^{t}_{2}\) if \(i>n\). The parameter \(\) will be chosen later based on the regularizer.

Now we wish to show that by choosing \(\) in the right way, we get that the decisions for the \(2\)-dimensional and \(2n\)-dimensional OFTRL algorithms are equivalent. Let \(x^{1},,x^{T}\) be the 2-dimensional OFTRL decisions, and let \(^{1},,^{T}\) be the \(2n\)-dimensional OFTRL decisions. Then, we want to show that \(_{i=1}^{n}^{t}_{i}=x^{t}\) and \(_{i=n+1}^{2n}^{t}_{i}=x^{t}\) for all \(t\).

**Lemma 3**.: _Let the losses \(^{1},,^{T}\) satisfy the duplication procedure given in the preceding paragraph. Then for any time \(t\), we have \(^{t}_{1}==^{t}_{n}\) and \(^{t}_{n+1}==^{t}_{2n}\)._

Proof.: Suppose not and let \(^{t}\) be the corresponding solution. Then the optimal solution is such that \(^{t}_{i}^{t}_{k}\) for some \(i,k\) both less than \(n\), or both greater than \(n\). But then, by symmetry, we have that there is more than one optimal solution to the OFTRL optimization problem at time \(t\): the objective is exactly the same if we create a new solution where we swap the values of \(^{t}_{i}\) and \(^{t}_{k}\). This is a contradiction due to strong convexity. 

From lemma 3, we have that the OFTRL decision problem in \(2n\) dimensions can equivalently be written as a \(2\)-dimensional decision problem: Since the first \(n\) entries must be the same, we can simply optimize over that one shared value, say \(x^{t}\), which we use for all \(n\) entries, and similarly we use \(x^{t}\) for the second half of the entries. Let \(:^{2}^{2n}\) be a function that maps the two-dimensional solution into the corresponding duplicated \(2n\)-dimensional solution. The equivalent \(2\)-dimensional problem is then:

\[^{t} =[*{argmin}_{x ^{2}}\{} x,_{=1}^{t-1}^ {}+^{t-1}+R_{1}(x)+R_{2}( x)\}]\] \[=[*{argmin}_{x ^{2}}\{}x,_{=1}^{ t-1}^{}+^{t-1}+R(x/n)\}]\] \[=[*{argmin}_{x ^{2}}\{ x,_{=1}^{t-1}^{}+^{t-1} +}{}R(x/n)\}].\]

The next theorem shows that we can choose \(\) for different regularizers and construct \(2n 2n\) loss matrices whose learning dynamics are equivalent to the learning dynamics in \(2 2\) games given in the preceding sections. We defer the proof to Appendix D.

**Theorem 3**.: _For any loss matrix \(A^{2 2}\), there exists a loss matrix \([0,n^{-}]^{2n 2n}\) such that for the Euclidean (\(=1\)), entropy (\(=0\)), Tsallis (\((0,1)\) and \(=-1+\)), and log (\(=-1\)) regularizers, the resulting OFTRL learning dynamics are equivalent in the two games._Combining Theorem 1 and Theorem 3, we have the following:

**Corollary 1**.: _In the same setup as Theorem 3, under Assumption 1 and Assumption 2, there exists a game matrix \(_{}[0,n^{-}]^{2n 2n}\) such that the OFTRL learning dynamics with any step size \(\) satisfies the following: there exists an iteration \(t}{3 L}\) with a duality gap at least \(c_{2}n^{-}\)._

Since \(=0\) for the entropy regularizer, the same results hold more generally for games where one player has more actions than the other. In particular, we can create a \(2n 2m\) game such that the resulting dynamics are equivalent to those in a \(2 2\) game. This does not work for the Euclidean and log regularizers because the rescaling factors would be different for the row and column players.

## 5 Conclusion and Discussions

In this paper, we study last-iterate convergence rates of OFTRL algorithms with various popular regularizers, including the popular OMWU algorithm. Our main results show that even in simple \(2 2\) two-player zero-sum games parametrized by \(>0\), the lack of forgetfulness of OFTRL leads to the duality gap remaining constant even after \(1/\) iterations (Theorem 1). As a corollary, we show that the last-iterate convergence rate of OFTRL must depend on a problem-dependent constant that can be arbitrarily bad (Theorem 2). This highlights a stark contrast with OOMD algorithms: while OGDA with constant step size achieves a \(O(})\) last-iterate convergence rate, such a guarantee is impossible for OMWU or more generally OFTRL. We now discuss several interesting questions regarding the convergence guarantees of learning in games and leave them as future directions.

Best-Iterate Convergence RatesWhile we focus on the last-iterate (_i.e._, \((x^{T},y^{T})\)), the weaker notion of best-iterate (_i.e._, \(_{t[T]}(x^{t},y^{t})\)) is also of both practical and theoretical interest. By definition, we know the best-iterate convergence rate is at least as good as the last-iterate convergence rate and could be much faster. This raises the following question:

_What is the best-iterate convergence rate of OMWU/OFTRL?_

To our knowledge, there are no concrete results on the best-iterate convergence rates of OMWU or other OFTRL algorithms. It is thus interesting to extend our negative results to the best-iterate convergence rates or develop fast best-iterate convergence rates of OMWU/OFTRL.

Dynamic Step SizesOur negative results hold for OFTRL with _fixed_ step sizes. We conjecture that the slow last-iterate convergence of OFTRL persists even with _dynamic_ step sizes. In particular, we believe our counterexamples still work for OFTRL with decreasing step sizes. This is because decreasing the step size makes the players move even slower, and they may be trapped in the wrong direction for a longer time due to the lack of forgetfulness. In Appendix E, we include numerical results for OMWU with adaptive stepsize akin to Adagrad (Duchi et al., 2011), which supports our intuition. We observe the same cycling behavior as for fixed step size. While the cycle is smaller than that of fixed step sizes, the dynamics take more steps to finish each cycle. Investigating the effect of dynamic step sizes on last-iterate convergence rates is an interesting future direction.

Slow Convergence due to Lack of ForgetfulnessOur work shows that various OFTRL-type algorithms do not have fast last-iterate convergence rates for learning in games. Our proof and hard game instance build on the intuition that these algorithms lack forgetfulness: they do not forget the past quickly. This intuition is also utilized in (Panageas et al., 2023). In particular, they give an \(d d\) potential game where the last-iterate convergence rate of the Fictitious Play algorithm, which is equivalent to the Follow-the-Leader (FTL) algorithm, suffers exponential dependence in the dimension \(d\). One natural future direction is to formalize the intuition of non-forgetfulness further and give a general condition for algorithms under which they suffer slow last-iterate convergence. It is also interesting to show other lower-bound results for learning in games.