# Modelling single-cell RNA-seq trajectories on a flat statistical manifold

Alessandro Palma

Helmholtz Munich

alessandro.palma@helmholtz-munich.de

&Sergei Rybakov

Helmholtz Munich

sergei.rybakov@helmholtz-munich.de

Leon Hetzel

Helmholtz Munich

leon.hetzel@helmholtz-munich.de

&Fabian Theis

Helmholtz Munich

fabian.theis@helmholtz-munich.de

Equal contribution.

###### Abstract

Optimal transport has demonstrated remarkable potential in the field of single-cell biology, addressing relevant tasks such as trajectory modelling and perturbation effect prediction. However, the standard formulation of optimal transport assumes Euclidean geometry in the representation space, which may not hold in traditional single-cell embedding methods based on Variational Autoencoders. In this study, we introduce a novel approach for matching the latent dynamics learnt by Euclidean optimal transport with geodesic trajectories in the decoded space. We achieve this by implementing a "flattening" regularisation derived from the pullback metric of a Negative Binomial statistical manifold. The method ensures alignment between the latent space of a discrete Variational Autoencoder modelling single-cell data and Euclidean space, thereby improving compatibility with optimal transport. Our results in four biological settings demonstrate that these constraints enhance the reconstruction of cellular trajectories and velocity fields. We believe that our versatile approach holds promise for advancing single-cell representation learning and temporal modelling.

## 1 Introduction

Temporal modelling is a widely-explored branch of machine learning with successful implications in applied sciences (Heaton et al., 2016; Hewage et al., 2020). In single-cell biology, modelling a system's evolution through time encompasses tracing how a cell state measured by single-cell RNA-seq (scRNA-seq) develops across a plethora of biological processes, ranging from development to disease progression (Ding et al., 2022). However, traditional single-cell sequencing is a destructive practice, meaning that the same cell is measured only once and cannot be explicitly tracked through time (Haque et al., 2017). This aspect poses challenges, in that measurements collected from time-resolved scRNA-seq do not contain direct information about individual cell state progression. Hence, learning about the system's dynamics requires the reconstruction of cellular trajectories from unpaired single-cell distributions sampled across experimental time.

Several methods have proven successful at reconstructing cellular trajectories from snapshots of cells collected over time (Hagherdi et al., 2016; Jurges et al., 2018; Fischer et al., 2019). Many of them rely on the concept of Optimal Transport (OT) (Peyre and Cuturi, 2017), which matches cell distributions across consecutive time points. OT has been readily employed to interpolate single-cell transcriptomics snapshots and study the cell population dynamics (Schiebinger et al., 2019; Tonget al., 2020). However, single-cell data holds properties that make the application of OT in the gene space challenging. More specifically, the curse of dimensionality complicates OT applications to cells, as OT relies on computing distances between sparse transcription vectors measuring thousands of genes at the same time. To circumvent this issue, the majority of the current methods resort to applying OT to linear projections of data into a low dimensional space via Principal Component Analysis (PCA) (Eyring et al., 2022) or into the latent space of deterministic autoencoders (Huguet et al., 2022). Despite these approaches providing insight into fate mapping and cell state evolution, they do not take into account the distributional properties of single-cell data, like discreteness and overdispersion. Moreover, the standard OT formulation for modelling continuous trajectories assumes Euclidean geometry in the representation space (Peyre and Cuturi, 2017). This hinders the application of deep representation techniques where the latent space is endowed with non-linear geometries, as linear paths in the latent space might not necessarily represent optimal paths in the observation space nor reflect the geometry of the data manifold (Arvanitidis et al., 2021).

To mitigate the restrictions of the Euclidean space assumption and incorporate the distributional properties of single-cell data into OT-based trajectory inference, we propose inducing Euclidean geometry into the latent space of a Variational Autoencoder (VAE) with Negative Binomial (NB) likelihood (Lopez et al., 2018). To this end, we assume that single-cell data lie on a statistical manifold defined by the parameters of the underlying NB distribution and that the decoder model represents a map of the latent codes to the continuous parameter space of the data likelihood. To obtain a "flat" latent representation, we constrain the pullback metric from the decoder to approximate a scaled identity matrix (Fig. 1). Given a flat representation of single-cells, we take advantage of the Conditional Flow Matching (CFM) model (Tong et al., 2023) to simulate cellular dynamics, an approach to learning single-cell evolution through experimental time recently presented in Tong et al. (2023).

In summary, we propose the following contributions:

1. We introduce a flattening regularisation technique for VAEs with NB likelihood based on information geometry.
2. We combine our flattening regularisation with Conditional Flow Matching (CFM) for enhanced vector field modelling.
3. We showcase the applicability of our approach to the field of temporal modelling for single-cell computational biology.

## 2 Related work

OT for single-cell genomicsDiscrete OT methods in single-cell transcriptomics have shown potential in multiple application contexts, such as trajectory inference (Schiebinger et al., 2019), spatial reconstruction (Moriel et al., 2021) and multi-modal alignment (Klein et al., 2023). Recent efforts have resorted to neural OT to predict the effects of drug perturbations on cells (Bunne et al., 2023) and cell trajectories in an unbalanced setting (Eyring et al., 2022). Our work revolves around modelling gene expression through time via dynamic OT (Peyre and Cuturi, 2017), a combination that was extensively explored in prior works (Tong et al., 2020). More specifically, the presented approach is inspired by MIOFlow (Huguet et al., 2022), where the authors model continuous single-cell trajectories in a latent space regularised such that latent Euclidean distances approximate the geodesic distances in the PCA space of single-cell data. However, MIOFlow's latent space regularisation is not tailored to the distributional properties of scRNA-seq measurements, an aspect that we explicitly consider by learning trajectories on a NB manifold.

Geometry-regularised autoencodersEnforcing the geometric structure of the data in the latent space of autoencoders was proposed on multiple occasions. Arvanitidis et al. (2021) model optimal latent paths based on the geometry of the ambient space defined by both deterministic and Gaussian stochastic decoders. Our research follows the theoretical framework introduced by Arvanitidis et al. (2022), which extends modelling geodesic latent paths to VAEs with arbitrary likelihood. More specifically, we use the theoretical concepts elaborated by the authors to describe the geometry of the latent space of a discrete VAE model with a NB likelihood as a function of the decoder's pullback metric. Our approach is also inspired by Chen et al. (2020), where the authors enforce flatness in the latent space of VAEs by pushing the pullback metric tensor towards the identity matrix. However,we additionally assume that the ambient space is a statistical manifold whose geometry is governed by the Fisher Information (Arvanitidis et al., 2022) rather than the Euclidean metric. Other relevant papers extend endowing the latent space with data geometry via isometric (Lee et al., 2022) and Jacobian (Nazari et al., 2023) regularisations.

Flow-based OTModelling continuous trajectories via Continuous Normalizing Flows (CNF) is a well-established practice in single-cell transcriptomics (Tong et al., 2020; Huguet et al., 2022). Recently, novel approaches based on Flow Matching (Lipman et al., 2023) have yielded stabler and faster training by introducing simulation-free objectives to learn the CNF vector field for generative purposes (Liu et al., 2022; Pooladian et al., 2023; Albergo and Vanden-Eijnden, 2023). Within such a context, significant efforts harnessed tractable simulation-free formulations of flows to address trajectory inference applications by drawing a connection with OT (Tong et al., 2023; Neklyudov et al., 2023). Finally, our work relates to geometrically-informed Flow Matching, which was recently proposed by Chen and Lipman (2023).

## 3 Background

### Modelling scRNA-seq with VAEs

The scRNA-seq method measures gene expression counts at the level of single cells. The discrete nature of observations readily allows modelling cell-specific counts using discrete likelihood models, inferring the continuous parameters of the data distribution by neural networks. More specifically, the biological and technical variability in the measurements lead to an inherent overdispersion of the expression counts, making the Negative Binomial (NB) likelihood the natural choice for modelling gene expression profiles.

More formally, given a gene expression matrix \(_{0}^{N G}\) with \(N\) cells and \(G\) genes, we assume that each cell \(\) is the realisation of a discrete random variable with the gene-specific distribution \(_{ng}(_{ng},_{g})\). Here, \(\) and \(\) represent the cell-gene-specific mean and the gene-specific inverse dispersion parameters, respectively. Further, we consider a latent variable model with marginalised density

\[p()= p(\,|\,)p()\;,\] (1)

where \(\) is a \(d\)-dimensional latent random variable with \(d<G\) and \((,_{d})\). In practice, we use amortised deep variational inference to jointly model the mean parameter of the data likelihood and the latent variables. This is achieved by utilising non-linear encoder and decoder functions of the form

\[ =f_{}()\;,\] (2) \[ =h_{}(,l)=l(_{}())\;,\] (3)

where \(_{}:^{d}^{G}\) is a neural network that models the expression proportions of each gene in a cell and \(l\) is the cell-specific size factor, which is directly derived from the data and refers to a cell's total number of counts \(l=_{g=1}^{G}_{g}\). The encoder formulation based on \(f_{}\) already takes into account the reparameterisation trick. In the following sections, we drop the dependency on the neural network parameters \(\) and \(\) for notational simplicity.

Figure 1: Visual conceptualisation of the proposed flattening approach for single-cell VAEs by regularising the pullback metric of a statistical manifold to match the identity matrix.

The gene-specific inverse dispersion \(_{g}\) is considered a model parameter and trained via maximum likelihood together with the mean decoder \(h\). The model is trained within the VAE framework, maximising the ELBO:

\[_{}=_{q()} p( ;)-q( )\|p()\;.\] (4)

As the representation learnt by \(f\) is dense and compressed, modelling single-cell trajectories can be performed in the latent space of a discrete VAE. The low dimensionality coupled with continuity and the availability of a parametrised likelihood model makes \(\) a natural proxy for studying dynamics in high-dimensional discrete data. Here, we couple this idea with continuous OT.

### Dynamic OT

OT computes the most efficient mapping for transporting mass from one measure to another according to a pre-defined cost. Let \(\) and \(\) be marginal probability distributions defined on the spaces \(\) and \(\), such that \(==^{d}\). Moreover, consider the set of joint distributions \((,)\), such that for \(\) both \((,)=\) and \((,)=\) holds. Considering the Euclidean distance cost \(d(,)=\|-\|_{2}\) for observations \(\) and \(\), the optimal coupling \(^{*}\) minimises the squared Wasserstein distance:

\[W_{2}(,)^{2} =_{}_{}d(, )^{2}(,)\] \[=\] (5)

Benamou and Brenier (2000) introduced a _dynamic formulation_ of the OT problem. In this setting, let \(p_{t}\) be a time-varying density over \(^{d}\) constrained by \(p_{0}=\) and \(p_{1}=\). For a smooth time-dependent vector field \(u:^{d}^{d}\) that satisfies the continuity equation

\[p}{t}=-(p_{t}u_{t})\;,\] (6)

the continuous counterpart to Eq. (5) is

\[W_{2}(,)^{2}=_{p_{t},u_{t}}_{0}^{1}_{^{d}}\|u_{t }()\|^{2}p_{t}()\,\,t\;,\] (7)

where \(u_{t}()=u(t,)\). The parameterised field \(u_{t}\) is said to _generate_ the probability path \(p_{t}\) if the latter is the solution of the continuity equation Eq. (6).

The velocity field \(u_{t}()\) is associated with an Ordinary Differential Equation (ODE) \(=u_{t}()t\). The solution of the ODE is an integration map \(_{t}()\) transporting mass along \(u\) over time to match a source with a target distribution, given the initial condition \(_{0}()=\). We deal with the setting where the vector field \(u_{t}()\) is approximated by a neural network \(v_{}(t,)\). In practice, one obtains \(p_{t}\) by pushing points from the initial distribution \(p_{0}\) through the integration map \(_{t}\).

### Conditional Flow Matching

Tong et al. (2023) demonstrated that by expressing the time marginals \(p_{t}()\) as a mixture of the form \(p_{t}()= p_{t}()q() \) conditioned on some latent variables \(\), the marginal vector field \(u_{t}()\) is related to the conditional vector field \(u_{t}()\) through

\[u_{t}()=_{q()}[( )\,p_{t}()}{p_{t}()}]\;.\] (8)

Further, regressing \(v_{}(t,)\) against the conditional field \(u_{t}()\) is equivalent to approximating the marginal field \(u_{t}()\), up to a constant independent of the parameter \(\). Here, we follow the OT-CFM variant from Tong et al. (2023). That is, given source and target sets of observations \(_{0}\) and \(_{1}\), we define the field-conditioning variable \(=(_{0},_{1})\) as pairs of samples re-sampled from the static optimal coupling \( q()=^{*}(_{0},_{1})\). Assuming Gaussian marginals \(p_{t}\) and \(_{0}\) and \(_{1}\) to be connected by Gaussian flows, both \(p_{t}()\) and \(u_{t}()\) become tractable:

\[p_{t}(\!)=(t_{1}+(1-t)_ {0},^{2})\;,\] (9)\[u_{t}(\,|\,)=_{1}-_{0}\,\] (10)

with a pre-defined value of \(^{2}\). Accordingly, the CFM loss is

\[_{}=_{t,q(),p_{t}(|)}\|_{}(t,)-u_{t}(\,|\,\,)\|^{2} \,\] (11)

with \(t(0,1)\). From Eq. (9), one can readily see that OT-CFM uses straight paths to optimise the conditional vector field. Note, however, that straight lines in the latent space of a VAE may not reflect geodesic paths in the discrete observation space, i.e. gene space. We tackle this challenge by applying CFM to the latent space of a VAE with enforced Euclidean geometry.

## 4 Flattened NB-VAE for OT-based trajectory inference

### The geometry of AEs

A common assumption is that the data lies near a low-dimensional Riemannian manifold \(_{}\) whose coordinates are represented by the latent representation \(\). The decoder of a deterministic AE model can be seen as an immersion \(h:^{d}^{G}\) of a latent space \(=^{d}\) into an embedded Riemannian manifold \(_{}\) with a geometry-defining metric tensor \(\)(Arvanitidis et al., 2021). A Riemannian manifold is a smooth manifold \(_{}\) endowed with a Riemannian metric \(()\) for \(_{}\). \(()\) is a positive-definite matrix that changes smoothly and defines a local inner product on the tangent space \(_{}_{}\) as the equation \(,_{}=^{} ()\), with \(,_{}_{}\)(Arvanitidis et al., 2021).

In this setting, the geometry of the latent space is directly linked to that of observation space through the _pullback metric_(Arvanitidis et al., 2022)

\[()=_{h}()^{}( )_{h}()\,\] (12)

where \(=h()\) and \(_{h}()\) is the Jacobian matrix of \(h()\). As such, the latent space \(\) is a Riemannian manifold whose properties are defined based on the geometry of \(_{}\). An example of such properties is the shortest distance between two latent codes \(_{1}\) and \(_{2}\), which is expressed as the length of the shortest connecting curve along the manifold

\[d_{}(_{1},_{2})=_{(t)}_{0}^{1 }(t)^{}((t))(t)} t\,\] (13)

\[(0)=_{1},\ (1)=_{2}\,\]

where \((t):^{d}\) is a curve in the latent space and \((t)\) its derivative along the manifold. If we assume Euclidean geometry in the observation space, we obtain \(()=_{h}()^{}_{h}( )\). This applies since \(_{}\) is endowed with the identity metric \(_{G}\).

In VAEs, the decoder function \(h\) maps a latent code \(\) to the parameter configuration \(\) of the data likelihood, where \(=^{G}\) represents the parameter space. As such, the decoder image lies on a statistical manifold, which is the smooth manifold of a probability distribution. Such manifolds have a natural metric tensor called Fisher Information Metric (FIM) (Nielsen, 2018; Arvanitidis et al., 2022). The FIM defines the local geometry of the statistical manifold and can be used to build the pullback metric for arbitrary decoders. For a statistical manifold \(_{}\) with parameters \(\), the FIM is formulated as

\[()=_{p(|)}[ _{} p(\ |\ )_{} p(\ |\ )^{}]\,\] (14)

where the metric tensor \(()^{G G}\)(Arvanitidis et al., 2022). Analogous to deterministic AEs, one can combine Eq. (14) and Eq. (12) to formulate the pullback metric for an arbitrary statistical manifold, with the difference that the pulled-back metric tensor is defined in \(\). As a result, the latent space of a VAE is endowed with the pullback metric for a statistical manifold

\[()=_{h}()^{}( )_{h}()\,\] (15)

where \(()^{d d}\). Note that the calculation of the FIM is specific for the chosen likelihood type and, as such, depends on initial assumptions on the data distribution. Pulling back the FIM of the statistical manifold of NB probability distributions to the latent space \(\) we obtain

\[()=_{g}_{g}}{h_{g}( )(h_{g}()+_{g})}_{}h_{g}()_{}h_{g}()\,\] (16)

where \(\) is the outer product of vectors (see Section A.1 for more details on the derivation).

### Flattening loss for a NB-VAE

Our goal is to perform OT in a latent space using a notion of distance that reflects geodesics along the NB statistical manifold. However, the computation of the geodesic distances for pairs of points requires learning geodesic curves connecting observations minimising Eq. (13) via gradient descent (Arvanitidis et al., 2022). This is not feasible for CFM with OT-CFM, since the transport matrix requires calculating distances between all pairs of points from batches drawn from the source and the target distributions. Nevertheless, if \(()=_{d}\) for some fixed parameter \(\), then the shortest distance between each pair of points in \(\) is given by the straight line between them.

In order to achieve this, we use an additional regularisation term (as in Chen et al. (2022) for the pulled-back Euclidean metric) to the VAE with NB likelihood, defined as:

\[_{}()=_{q(|)} ()-_{d}^{2}\,,\] (17)

with \(()\) represented by Eq. (16). In other words, we enforce a flat geometry in the latent space of a VAE with NB likelihood, where straight-line distances approximate geodesic distances on the statistical manifold. The loss of the derived VAE, therefore, is:

\[=_{}+_{}\,,\] (18)

where \(\) controls the strength of the flattening regularisation. In practice, we train flows post hoc on the latent space of the regularised VAE and derive the evolution of gene counts through time via sampling the likelihood model from the decoded parameters. The training procedure is described in Algorithm 1.

``` Data matrix \(_{0}^{N G}\), batch size \(B\), maximum iterations \(n_{}\), encoder \(f_{}\), decoder \(h_{}\), flatness loss scale \(\) ```

**Output:** Trained encoder \(f_{}\), decoder \(h_{}\) and inverse dispersion parameter \(\) randomly initialise gene-wise inverse dispersion \(\) randomly initialise the identity matrix scale \(\) as a trainable parameter for\(i=1\)to\(n_{max}\)do  sample batch \(^{b}\{_{1},...,_{B}\}\) from \(\) \(^{b}(^{b})\) \(^{b} f_{}(1+^{b})\) \( h_{}(^{b},^{b})\) \(_{}(^{b})\) \(_{}( ^{b},,)\) \((^{b})\) (16) \(_{}((^{b}), _{d})\) \(=_{}+_{}+ _{}\)  Update parameters via gradient descent endfor ```

**Algorithm 1** Train Flat NB-VAE

## 5 Experiments

Baselines & setupWe compare the latent space of our flat single-cell VAE (Flat NB-VAE) with that of a standard VAE (NB-VAE) trained with a NB decoder (Lopez et al., 2018) as representations for continuous OT. Additionally, we evaluate latent OT on the embeddings produced by the Geodesic AE (Geodesic AE) described in Huguet et al. (2022). The latter method is trained on log-normalized gene expression to better accommodate the lack of a discrete probabilistic decoder. All three approaches are used to derive embeddings of time-resolved gene expression datasets. Subsequently, we use the cell representations to train an OT-CFM model (Tong et al., 2023) and learn latent trajectories using unpaired batches of observations from consecutive time points. Experimental details are provided in the following sections and Section A.4.

DataWe evaluated our method for latent trajectory inference on four time-resolved single-cell RNA-seq datasets: (i) **Embryoid body (EB)**, Moon et al. (2019) profile 18,203 differentiating human embryoid cells over five time points, generating four lineages. (ii) **Pancreatic endocrinogenesis**, Bastidas-Ponce et al. (2019) measure 16,206 cells, spanning embryonic days 14.5 to 15.5, revealing multipotent cell differentiation into endocrine and non-endocrine lineages. (iii) **Cytomegalovirusovirus (CMV) infection dataset**, Hein and Weissman (2021) measure 12,919 fibroblasts infected by Cytomegalovirusovirus (CMV) across seven time points, focusing on the top 2000 highly variable genes. (iv) **Reprogramming dataset**, Schiebinger et al. (2019) explore the reprogramming of mouse embryonic fibroblasts into induced pluripotent stem cells, comprising 165,892 cells across 39-time points and 7 cell states, emphasising 1479 highly variable genes.

### Flat latent space properties

We explore the relationship between the scaling of the flatness constraint and the likelihood score of the decoder. As measures of regularisation strength, we employ the _Variance of the Riemannian metric_ (VoR) and the _magnification factor_ (MF) following Chen et al. (2020) and Lee et al. (2022). Briefly, given a metric tensor \(()\), VoR quantifies the uniformity of the Riemannian metric across space by computing the distance between \(()\) and its expected value \(=_{ P_{}}[()]\). A VoR of 0 indicates constant metric across \(\). MF is defined as \(()=\ ()}\) and reflects the proximity of the metric tensor to the identity \(_{d}\). More details are in Section A.5.1. Table 1 depicts the properties of the latent space of a Flat NB-VAE at increasing levels of regularisation. Noticeably, high values of \(\) contribute to a flatter latent space, suggesting our regularisation successfully induces a constant and flat geometry in the VAE bottleneck together with a gradual increase in negative log-likelihood. In practice, a value of \(=1\) is chosen for all datasets but the MEF reprogramming setting, where \(=0.1\) is selected to avoid an excessive penalisation of the gene reconstruction.

  &  &  &  \\  & VoR & MF & NLL & VoR & MF & NLL & VoR & MF & NLL \\  NB-VAE & 72.14 & - & 495.12 & 28.47 & - & 903.97 & 120.18 & - & 509.00 \\ Flat NB-VAE \(=0.1\) & 62.31 & 19231.44 & 519.46 & 21.29 & 3553.69 & 1006.15 & 12.89 & 1412.66 & 544.34 \\ Flat NB-VAE \(=1\) & 48.80 & 324.12 & 525.21 & 6.44 & 52.88 & 1072.15 & 21.45 & 120.46 & 553.01 \\ Flat NB-VAE \(=10\) & 1.23 & 4.17 & 576.91 & 1.72 & 4.83 & 1206.93 & 14.46 & 20.47 & 576.57 \\  

Table 1: VoR, MF and Negative Log-Likelihood (NLL) as functions of the flattening regularising scale \(\). Missing entries indicate that the value evaluated in such a setting diverged to infinity.

Figure 2: 2D PCA plots of the latent spaces computed by the Geodesic AE model, the NB-VAE and the Flat NB-VAE. Highlighted are initial, intermediate and terminal cell states along the biological trajectory.

We qualitatively compare the PCA embeddings of our Flat NB-VAE's latent space with competing models in Figure 2. The significantly higher variance explained by the first two principal components demonstrates that our flattening regularisation induces a lower latent space dimensionality, making the information compression more efficient and less noisy. At the same time, the biological structure of the data is preserved or even enhanced compared to competing methods. Biological preservation is particularly evident in the MEF dataset (Schiebinger et al., 2019), where our Flat NB-VAE induces a clearer separation between initial and terminal states, suggesting a better identification of the cellular dynamics.

### Simulation of single-cell RNA-seq counts with OT-CFM

Coupled with OT-CFM, we compare simulating latent trajectories from NB-VAE with our Flat NB-VAE. Coherently with expectations, modelling latent trajectories with Euclidean cost benefits from a flat statistical manifold, as displayed by a lower 2-Wasserstein distance (WD) between real and simulated latent observations in Figure 2(a). Aside from latent reconstruction, our approach allows us to gain insight into discrete gene expression counts and study how lineage drivers evolve through time (see Figure 2(b) for examples on the cardiac and neural crest branches of the EB dataset).

To quantify the performance of dynamic OT on our flat manifold, we adopt a similar setting to Tong et al. (2020). More in detail, for each dataset, we leave out intermediate time points and train OT-CFM on the remaining cells. The capacity of OT to reconstruct unseen time point \(t\) from \(t-1\) during inference is an indication of the interpolation abilities of the model across the data manifold. Here, we use such a paradigm to compare different representation spaces. As a baseline, we calculate the distribution distance between the held-out time point \(t\) and the ground truth cells at the previous time point \(t-1\). In Table 2, we report the 2-Wasserstein distance, the polynomial-kernel Maximum Mean Discrepancy (MMD) and the mean L\({}^{2}\) distance between true and reconstructed latent cells. Furthermore, we add two measures in the decoded space, called _Density and Coverage_ (D & C) (Naeem et al., 2020), which evaluate the mixing of real and simulated cell distributions in the gene expression space (see the Section A.5.1). On almost all datasets and metrics, trajectories in the flat latent space yield better overall latent time point reconstruction results. This observation is specifically true in the larger MEF reprogramming and Embryoid Body datasets. Furthermore, the experiments show that our approach yields an overall improvement in the inferred decoded trajectories, which can be seen by higher Density and Coverage metrics across all evaluated datasets.

  &  &  &  \\  &  &  &  &  &  &  &  &  \\  & WD(\(\)) & MMD(\(\)) & L\({}^{2}\)(\(\)) & \(D\)(\(\)) & \(C\)(\(\)) & WD(\(\)) & MMD(\(\)) & L\({}^{2}\)(\(\)) & \(D\)(\(\)) & \(C\)(\(\)) & WD(\(\)) & MMD(\(\)) & L\({}^{2}\)(\(\)) & \(D\)(\(\)) & \(C\)(\(\)) \\  Baseline & 2.87 & 0.56 & 0.57 & 0.04 & 0.13 & 2.77 & 0.52 & 0.55 & 0.25 & 0.18 & 3.21 & 0.83 & 0.83 & 0.12 & 0.07 \\ Geodesic AE & 2.15 & 0.36 & 0.40 & 0.01 & 0.02 & 2.09 & 0.32 & 0.34 & 0.02 & 0.03 & 2.49 & 0.54 & 0.57 & 0.01 & 0.00 \\ NB-VAE & 2.06 & 0.29 & 0.30 & 0.21 & 0.37 & 2.08 & **0.29** & **0.33** & 1.64 & 0.63 & 2.08 & 0.38 & 0.40 & 0.13 & 0.10 \\ Flat NB-VAE & **1.54** & **0.27** & **0.27** & **0.31** & **0.49** & **1.97** & 0.36 & 0.37 & **3.91** & **0.84** & **1.64** & **0.35** & **0.36** & **0.16** & **0.13** \\ 

Table 2: Comparison of held-out time point reconstruction across models, including the baseline.

Figure 3: Simulating single-cell RNA-seq in time. (**a**) Overlap between real and simulated latent samples in the EB dataset. (**b**) Real and simulated cells on the flat manifold from the cardiac and neural crest lineages of the EB dataset. Colours indicate the \(\) gene expression of the reported lineage drivers GYPC and HAND1.

### Latent vector field quality and lineage mapping

We hypothesise that dynamic OT with Euclidean cost benefits from being applied to a flat representation space, as the straight path between matched distributions inherently preserves the geometry enforced in the manifold. We evaluate the quality of the cell-specific directionality learned by our model using the Pancreas Endocrinogenesis dataset from Bastidas-Ponce et al. (2019). More specifically, we follow the experimental setup introduced by Eyring et al. (2022), where a continuous vector field is learned by matching cell distributions through time. Using the Cellrank software (Lange et al., 2022; Weiler et al., 2023), we build random walks on a cell graph based on the directionality of latent velocities learned by OT-CFM. Walks converge to macrostates representing the end points of the biological process.

In Figure 4a, we summarise the number of terminal cell states identified by following the velocity graph. From prior biological knowledge, we know that the dataset contains six terminal states, all of which are identified in the latent space of our proposed Flat NB-VAE. Conversely, walks on the representations computed by the Geodesic AE and NB-VAE only capture four and five terminal states, respectively. In Figure 4b, we evaluate the velocity consistency within neighbourhoods of cells as a function of latent dimensionality. In contrast to the other methods, OT in our flat latent space yields a more consistent velocity field across latent dimensionalities.

## 6 Conclusion

We addressed the task of modelling temporal trajectories from unpaired cell distributions by using walks on flat NB manifolds. To achieve this, we proposed to regularise the pullback metric from the stochastic decoder of a single-cell VAE to approximate the identity matrix and enforce Euclidean geometry in the latent space. Our results demonstrate that our flattening procedure not only preserves but also enhances the biological structure in the latent space. By combining this approach with dynamic OT, we observed better prediction outcomes and more consistent vector fields on cellular manifolds. These improvements have practical benefits for central tasks in studying cellular development, such as fate mapping. In future work, we aim to extend the theory to include a broader range of statistical manifolds and single-cell tasks, such as modelling Poisson-distributed chromatin accessibility, estimating latent space distances for batch correction evaluation or enhancing OT-mediated perturbation modelling.