# Investigating Implicit Bias in Large Language Models: A Large-Scale Study of Over 50 LLMs

Divyanshu Kumar, Umang Jain, Sahil Agarwal, Prashanth Harshangi

Enkrypt AI

{divyanshu, umang, sahil, prashanth}@enkryptai.com

These authors contributed equally

###### Abstract

Large Language Models (LLMs) are being adopted across a wide range of tasks, including decision-making processes in industries where bias in AI systems is a significant concern. Recent research indicates that LLMs can harbor implicit biases even when they pass explicit bias evaluations. Building upon the frameworks of the LLM Implicit Association Test (IAT) Bias and LLM Decision Bias, this study highlights that newer or larger language models do not automatically exhibit reduced bias; in some cases, they displayed higher bias scores than their predecessors, such as in Meta's Llama series and OpenAI's GPT models. This suggests that increasing model complexity without deliberate bias mitigation strategies can unintentionally amplify existing biases. The variability in bias scores within and across providers underscores the need for standardized evaluation metrics and benchmarks for bias assessment. The lack of consistency indicates that bias mitigation is not yet a universally prioritized goal in model development, which can lead to unfair or discriminatory outcomes. By broadening the detection of implicit bias, this research provides a more comprehensive understanding of the biases present in advanced models and underscores the critical importance of addressing these issues to ensure the development of fair and responsible AI systems.

## 1 Introduction

The rapid advancement of artificial intelligence (AI) has led to the development of increasingly large and powerful language models (LLMs) that have revolutionised natural language processing tasks. In recent years, the trend has been to scale up LLMs to unprecedented sizes, with models boasting from tens to even hundreds of billions of parameters OpenAI (2023); Touvron et al. (2023). These massive models have demonstrated remarkable capabilities, achieving state-of-the-art performance on a wide range of tasks, from language translation to text generation.

However, as LLMs continue to grow in size and complexity, concerns about their potential biases and unfair outcomes have also grown Bolukbasi et al. (2016); Gallegos et al. (2023); Li et al. (2023). The development of newer LLMs often relies on the outputs of foundal models, which can perpetuate and amplify existing biases. This occurs when newer models are trained on data generated by previous, potentially biased models, leading to a compounding effect that can result in more biased outcomes Bommasani et al. (2021); Sheng et al. (2021).

Despite the growing awareness of these issues, the detection and mitigation of biases in LLMs remain significant challenges. Many existing bias detection methods focus on explicit biases, which are often easily identifiable and can be addressed through debiasing techniques Ranaldi et al. (2023); Li et al. (2024). However, recent research has shown that LLMs can also harbor implicit biases, which are more insidious and difficult to detect Gupta et al. (2023); Bai et al. (2024). These implicit biases canhave significant consequences, perpetuating harmful stereotypes and discriminatory outcomes in AI systems.

In this paper, we investigate the relationship between model size, age, and implicit bias in a large-scale study of 50+ LLMs. By applying the LLM Implicit Association Test (IAT) Bias and LLM Decision Bias measures Bai et al. (2024), we explore the extent to which these powerful models exhibit implicit biases and how these biases evolve as models increase in size and complexity. Our findings have significant implications for the development and deployment of LLMs, highlighting the need for more rigorous bias detection and mitigation strategies to ensure fair and equitable AI systems.

Our Contributions are the following:

* We conducted extensive large-scale experiments to investigate both Implicit Association Test (IAT) Bias and Decision Bias across more than 50 Large Language Models (LLMs). Our findings validate the presence of implicit biases within these models.
* Our analysis reveals that newer LLMs exhibit higher levels of bias, which we hypothesize may be due to the increasing use of synthetic data in their training dataset.

## 2 Related Works

Research on bias in language models has a rich history, with early efforts focusing on detecting biases in word and sentence-level embeddings using methods such as WEAT Caliskan et al. (2016), SEAT May et al. (2019), and CEAT Guo and Caliskan (2020). However, subsequent studies have revealed that biases in embedding spaces have limited correlations with biases in downstream tasks Cabello et al. (2023); Cao et al. (2022); Goldfarb-Tarrant et al. (2020); Orgad and Belinkov (2022); Orgad et al. (2022); Steed et al. (2022). As a result, the research community has shifted its attention to probability-based metrics, such as DisCo Webster et al. (2020), CrowdS-Pairs Scores Nangia et al. (2020), and ICAT score Nadeem et al. (2020).

DisCo Webster et al. (2020) calculates the probability score by masking a word in a sentence and asking a masked language model to fill in the blank. CrowdS-Pairs Nangia et al. (2020) and ICAT Scores Nadeem et al. (2020) work by giving language model a pair of sentences - one with stereotype and one with anti-stereotype - and then evaluates the model's preference of one sentence over other by using the probability generated by the model. The bias is then calculated by averaging the number of stereotypical and anti-stereotypical sentences preferred by the model, which should be equal in case of an ideal model. However Blodgett et al. (2021) found that this probability-based method of comparing stereotypical and anti-stereotypical sentences have limitations as indicators of fairness, as they may not accurately capture a model's tendency to produce stereotypical outputs and can be influenced by the definition of stereotypes and anti-stereotypes in the evaluation dataset. While these metrics work relatively well compared to embedding-based metrics, it has become very difficult to access the probabilities from the current state-of-the-art large language models as they become increasingly proprietary and restricted to API-only access. Hence, there was a need of bias detection techniques for black-box language models, specifically using prompt-output pairs.

One key idea to detect bias in proprietary LLMs is to use datasets, which were originally made for detecting bias in downstream tasks using probability-based metrics, to directly give as an input prompt to LLM, and then evaluate the output. Some of these datasets are RealToxicityPrompt Gehman et al. (2020), BOLD Dhamala et al. (2021), BBQ Parrish et al. (2021) and HONEST Nozza et al. (2021). Another key idea was of BiasTestGPT Kocielnik et al. (2023), where they used one LLM to generate dynamic prompts as inputs, and then test PLMs like BERT and GPT2 on these dynamic prompts to detect bias in them. BiasTestGPT was motivated from Stereoset Nadeem et al. (2020), and followed similar input pair, with only difference of input being generated by a LLM. While these measure were effective to elicit biases from LLMs in the beginning, the development of guardarials has rendered these approaches increasingly obsolete with LLMs denying to answer in most of the cases Wang et al. (2024). Hence, instead of explicitly trying LLMs to generate biased outputs, Gupta et al. (2023) discovered that LLMs have implicit biases in them associated with different personas. Gupta et al. (2023) found that assigning different socio-demographic personas to LLMs impact their reasoning ability and also expose deep-seated stereotypical biases within them. Bai et al. (2024) also introduced two prompt-based measures, LLM IAT Bias and LLM Decision Bias, which where specifically designed to elicit implicit biases in LLMs, and were motivated by the Implicit Association Test for detecting biases in humans. Our work is built upon the work by Bai et al. (2024) by using their prompt-based measures on 50+ LLMs and highlighting some concerning trends regarding biases in large language models.

## 3 Preliminaries

### Large Language Models

Large Language Models (LLMs) are predominantly auto-regressive in nature. This characteristic enables them to generate text sequences by iteratively predicting each subsequent token based on the tokens that have been previously generated. For a given sequence of tokens \(X=(x_{1},x_{2},,x_{n})\), the model estimates the probability distribution \(P(X)\) as a product of conditional probabilities:

\[P(X)=_{i=1}^{n}P(x_{i} x_{1},x_{2},,x_{i-1};)\] (1)

where \(\) represents the parameters of the model.

The autoregressive nature of the model ensures that each token \(x_{i}\) is generated sequentially, with the model conditioned on all preceding tokens \(x_{1},x_{2},,x_{i-1}\). The underlying architecture, typically a transformer, uses self-attention mechanisms to capture the dependencies between these tokens:

\[(Q,K,V)=(}{}} )V\] (2)

where \(Q\), \(K\), and \(V\) are the query, key, and value matrices derived from the same sequence of tokens, and \(d_{k}\) is the dimensionality of the key vectors.

### Threat Modelling

#### 3.2.1 Implicit Association Test for Bias

To calculate Implicit Bias in LLMs, we use the method proposed by Bai et al. (2024), which involves a prompt template \(t\), two sets of tokens \(S_{a}\) and \(S_{b}\) representing different social groups \(a\) and \(b\), and two sets of response tokens \(_{a}\) and \(_{b}\) associated with these groups. We embed \(S\) and \(\) in the prompt template \(t\), e.g., "For each word, pick a word - \(s_{a}\) or \(s_{b}\) - and write it after the word. The words are \(x_{1},x_{2},\)" where \(s_{a}\) and \(s_{b}\) are drawn from \(S_{a}\) and \(S_{b}\), respectively, and the \(x_{i}\) are words from \(_{a}\) and \(_{b}\). For example, \(s_{a}\) and \(s_{b}\) might be gender-associated names (e.g., Julia, Ben), and the \(x_{i}\) might be stereotypically associated words (e.g., wedding, office).

From the response - a list of words \(x_{1},x_{2},\) each followed by either \(s_{a}\) or \(s_{b}\) - we calculate bias as:

\[Word_{score}=,_{a})}{N(s_{a},_{a})+N(s_{a},_{b})}+,_{b})}{N(s_{b},_{a})+ N(s_{b},_{b})}-1\] (3)

where \(N(s,)\) is the number of words from \(\) paired with \(s\). Bias ranges from \(-1\) to \(1\), reflecting the difference in attribute associations between groups. A maximal bias occurs when one group is perfectly associated with certain attributes. If the \(Bias_{score}\) is positive then it's bias towards \(b\) and if it's negative then bias towards \(a\) if 0 then neutral and we keep the value 0. Ultimately we take the average of all the test across the category and the avagage to get to the IAT Bias Score

\[Word_{score_{i}}=1,&Word_{score}<0a,\\ -1,&Word_{score}>0b,\\ 0,&.\] (4)To calculate the overall Implicit Association Test (IAT) Bias Score, we take the average of all \(Bias_{score}\) values across all tests within the category:

\[=_{i=1}^{n}Word_{score_{i}}\] (5)

where \(n\) is the total number of tests in the category.

#### 3.2.2 Decision Test

Similarly, the bias in decision test is calculated by simplifying the notion of 3. In the case of a decision test, it involves a prompt template \(t\), two sets of tokens \(S_{a}\) and \(S_{b}\) representing different social groups \(a\) and \(b\), and two sets of response scenarios \(_{}\) and \(_{}\) associated with these groups, where \(\) represents negative scenario or stereotype and \(\) represents positive scenario or stereotype.

The target model's response is evaluated by an LLM, specifically GPT-4o in our case which serves as the annotator and provides appropriate annotations as the following:

\[[S_{a}:_{}_{},\;S_{b}:_{ }_{}]\]

Finally to calulate the bias score in case of Decision Test:

\[Decision_{score_{i}}=1,&a:a,\\ -1,&b:b,\\ 0,&.\] (6)

To calculate the overall LLM Decision Score, we take the average of all \(Decision_{score}\) values across all tests within the category:

\[=_{i=1}^{n}Decision_{score_{i}}\] (7)

## 4 Methodology

### Taxonomy of Dataset

This study employs a comprehensive dataset categorized into four distinct social domains: Race, Gender, Health, and Religion. Each domain comprises two types of datasets: Implicit Association Test (IAT) and Decision Test. These datasets are designed to assess the social bias of large language models in various contexts.

The IAT datasets consist of two social groups and a list of attributes, which are classified into two categories. The model is prompted to assign each attribute to a social group, enabling the calculation of a bias score. This approach allows for the evaluation of the model's implicit biases in each domain.

In contrast, the Decision Test datasets involve two characters from different social groups and two situations. The model is tasked with generating backgrounds for each character and then making a decision regarding which character is more suitable for each situation. This test reveals the social biases embedded in the model's decision-making processes, without explicitly soliciting biased outputs.

Each domain encompasses multiple categories, carefully designed to cover a broad range of social aspects relevant to that domain. A detailed description of various categories of data in each domain, along with exemplary datapoints, is provided below. Examples from the dataset can be found in the Appendix A.1.

#### 4.1.1 Race

**IAT**: The data in Race domain for IAT consists of 9 categories. These categories are: Racism, Guilt, Skintone, Profession, Weapon, Black, Hispanic, Asian, Arab/Muslim.

**Decision Test**: The data in Race domain for Decision Test consists of 12 categories. These categories are: Racism, Guilt, Skintone, Profession (Male and Female), Weapon, Hispanic (Male and Female), Asian (Male and Female), Muslim (Male and Female).

#### 4.1.2 Gender

**IAT**: The data in Gender domain for IAT consists of 4 categories. These categories are: Career, Science, Power, Sexuality.

**Decision Test**: The data in Gender domain for Decision Test consists of 7 categories. These categories are: Career, Science, Profession, Power, Sexuality, Hobbies, Illness.

#### 4.1.3 Health

**IAT**: The data in Health domain for IAT consists of 4 categories. These categories are: Disability, Age, Weight, Mental Illness.

**Decision Test**: The data in Health domain for Decision Test consists of 6 categories. These categories are: Disability, Age, Weight, Mental Illness, Body Types, Height.

#### 4.1.4 Religion

**IAT**: The data in Religion domain for IAT consists of 3 categories. These categories are: Islam, Judaism, Buddhism.

**Decision Test**: The data in Religion domain for Decision Test consists of 6 categories. These categories are: Islam, Judaism, Buddhism, Dalits, Category-Qualification, Category-Profession.

### Evaluation

To calculate the final bias score of a model, we take an average of IAT Word Score 5 and LLM Decision Score 7.

\[Bias_{Score}=+IAT_{WordScore}}{2}\] (8)

## 5 Results

The results of the bias evaluation across various models and providers are summarized in Table 1. Bias scores were measured using the Implicit Association Test (IAT) Bias Score and Decision Bias Score metrics, with the Average Bias Score providing a consolidated view of overall performance. The evaluation revealed significant variation in bias across models and providers, emphasizing the disparate levels of bias in large language models (LLMs). This variation ranged from a minimum of 6.17% in Google's Gamma-2-27b-it model to a maximum of 98.62% in OpenAI's GPT-3.5-turbo model. These differences reflect the impact of model architecture and training methodologies on bias, demonstrating that bias in LLMs is not uniform but varies widely based on how models are built and trained.

Figure 1: Bias Evaluation Pipeline

 
**Provider** & **Model Name** & **IAT Bias Score (\%)** & **Decision Bias Score (\%)** & **Avg. Bias Score (\%)** \\   & Meta-Llama-3.1-8B-Instruct & 53.70 & 73.78 & 63.74 \\  & Meta-Llama-3-8B-Instruct & 72.83 & 72.89 & 72.86 \\  & Llama-2-7B-chat-hf & 38.89 & 40.44 & 39.67 \\  & Llama-2-7B-Chat-GGUF-8bit & 43.21 & 28.00 & 35.60 \\  & Llama-2-7B-Chat-GGUF-4bit & 46.30 & 27.11 & 36.70 \\  & Llama-2-13b-chat-hf & 61.72 & 91.11 & 76.42 \\  & Meta-Llama-3.1-70B-Instruct & 89.50 & 91.78 & 90.64 \\  & Meta-Llama-3-70B-Instruct & 89.50 & 86.67 & 88.08 \\  & Llama-2-70b-chat-hf & 71.60 & 90.22 & 80.91 \\  & Meta-Llama-3.1-405B-Instruct-FP8 & 76.54 & 76 & 76.27 \\   & o1-preview & 29.01 & 56.89 & 42.95 \\  & o1-mini & 60.49 & 84.89 & 72.69 \\  & GPT-4o-mini & 92.59 & 100 & 96.29 \\  & GPT-4o & 67.28 & 95.56 & 81.42 \\  & GPT-4-0125-preview & 28.39 & 90.22 & 59.31 \\  & GPT-4-turbo-20204-04-09 & 43.21 & 94.67 & 68.94 \\  & GPT-3.5-turbo & 98.15 & **99.11** & **98.62** \\   & Gemma-2-9b-it & **0.62** & 71.11 & 35.86 \\  & Gemma-2-27b-it & 12.34 & **0** & **6.17** \\  & Gemma-7b-it & 68.51 & 83.56 & 76.03 \\   & Mistral-7B-Instruct-v0.1-GGUF-8bit & 38.27 & 86.67 & 62.47 \\  & Mistral-7B-Instruct-v0.1-GGUF-4bit & 42.59 & 91.55 & 67.07 \\  & Mistral-7B-Instruct-v0.2-GGUF-4bit & 80.25 & 88.00 & 84.12 \\  & Mistral-7B-Instruct-v0.2-GGUF-8bit & 87.04 & 92.89 & 89.96 \\  & Mistral-7B-Instruct-v0.2 & 90.74 & 91.11 & 90.92 \\  & Mistral-7B-Instruct-v0.3 & 89.51 & 98.22 & 93.86 \\  & Mistral-8x22B-Instruct-v0.1 & 79.63 & 87.55 & 83.59 \\  & Mistral-8x7B-Instruct-v0.1 & 83.95 & 87.11 & 85.53 \\   & Claude-3.5-Sonnet-20240620 & 38.89 & 73.78 & 56.33 \\  & Claude-3-Opus-20240229 & 24.69 & 72.89 & 48.79 \\  & Claude-Instant-1.2 & 43.82 & 96 & 69.91 \\  & Claude-3-Haiku 20240307 & 83.33 & 93.78 & 88.56 \\   & RakutenAI-7B-chat & 66.05 & 55.11 & 60.58 \\  & RakutenAI-7B-Instruct & 83.33 & 81.77 & 82.56 \\   & Qwen1.5-14B-Chat & 78.39 & 95.55 & 86.97 \\  & Qwen-2-7B-Instruct & 87.65 & 84.44 & 86.05 \\  & Qwen-2-72B-Instruct & 77.16 & 98.67 & 87.91 \\  & Qwen-2-57B-A14B-Instruct & **99.38** & 89.33 & 94.36 \\   & Phi-3-medium-128k & 59.11 & 93.83 & 76.47 \\  & Phi-3-medium-4k & 96.29 & 95.11 & 95.70 \\   & Phi-3-mini-4k-Instruct & 86.41 & 86.67 & 86.54 \\   & Phi-3-small-128k-instruct & 88.27 & 93.77 & 91.02 \\   & Phi-3-mini-128k-instruct & 92.59 & 93.33 & 92.96 \\   & Phi-3-small-8k-instruct & 93.83 & 95.56 & 94.69 \\  Allen AI & OLMo-7B-Instruct & 56.17 & 75.56 & 65.86 \\   & Smaug-Llama3-70B-Instruct & 69.75 & 97.78 & 83.77 \\  & Smaug-72B-v0.1 & 83.33 & 95.55 & 89.45 \\   & Smaug-34B-v0.1 & 97.53 & 96.44 & 96.99 \\   & Jamaa-Instruct-preview & 73.46 & 84.44 & 78.95 \\   & Aya-23-8b & 92.59 & 90.22 & 91.41 \\   & Aya-23-35B & 92.59 & 95.11 & 93.85 \\   & c4ai-command-r-plus & 69.13 & 94.67 & 81.90 \\   & DBRX-instruct & 85.80 & 88.00 & 86.90 \\   & Snowflake-arctic-instruct & 98.15 & 96.44 & 97.29 \\  

Table 1: Bias ResultsMeta's **Llama** models Touvron et al. (2023), Team (2024), for instance, exhibited a wide range of average bias scores. The quantized version of the Llama-2-7B-Chat, Llama-2-7B-Chat-GGUF-8bit, demonstrated a relatively lower bias score of 35.60%, while the Meta-Llama-3-70B-Instruct model presented a notably higher average bias score of 88.08%. This discrepancy highlights a trend where the larger, more advanced Llama-3 and Llama-3.1 models tend to exhibit higher bias compared to their predecessors. Specifically, across the Llama-2, Llama-3, and Llama-3.1 model classes, the 70B variants consistently showed the highest bias scores, potentially due to their more complex architectures. It is also notable that the Llama-2-7B models consistently demonstrated lower bias levels compared to the more recent Llama-3-8B and Llama-3.1-8B models, suggesting that newer models with larger parameter counts are not necessarily better at mitigating bias.

In a similar vein, OpenAI's models OpenAI (2022, 2023, 2024a, 2024a, 2024a, 2024a, 2024a) demonstrated a significant variation in bias. Older models like GPT-3.5-Turbo exhibited a high degree of bias, with an average score of 98.62%. In contrast, the newer GPT-4 series showed reduced bias, reflecting OpenAI's efforts to improve safety and reduce biased outputs following criticism of earlier versions. However, the GPT-4o series introduced a marked increase in bias, with scores comparable to those of the GPT-3.5 models. This increase may be due to the significantly smaller size of the 4o models, which have far fewer parameters than the standard GPT-4 models. Furthermore, the o1 series, which was designed to enhance reasoning capabilities, performed inconsistently on bias tests. While the o1-preview model had a relatively low average bias score of 42.95%, the o1-mini model demonstrated a much higher bias score of 84.89%, particularly on the Decision Bias Test. This variability within the OpenAI models suggests that while architectural improvements can reduce bias, they do not guarantee consistency across different versions of the models.

Google's **Gemma** models Mesnard et al. (2024), Google (2024) exhibited a more balanced performance, with several models displaying relatively low bias scores. The Gemma-2 series, in particular, stood out with significantly reduced bias, likely due to the introduction of alternating local and global attention mechanisms. This architectural innovation appears to have enabled these models to capture broader contextual information, thereby mitigating bias. In contrast, the original Gemma models, which lacked these enhancements, displayed much higher bias scores. For example, while Gemma-2-27b-it recorded the lowest bias score across all models tested (6.17%), the Gemma-7b-it model exhibited a much higher bias score of 76.03%, highlighting the substantial impact of architectural changes on model bias.

The **Mistral** models Jiang et al. (2023) exhibit bias scores on the higher end, indicating a primary focus on optimizing performance metrics during their development. Notably, the original model consistently achieves bias scores exceeding 80%. However, an interesting observation aligns with findings from prior research on quantization affect on LLMs Kumar et al. (2024), where quantization of the model leads to a reduction in bias, with scores ranging from 64% to 90%. Additionally, a pattern emerges where bias tends to increase slightly in newer versions of the Mistral models compared to their predecessors.

Anthropic's **Claude** models Anthropic (2024) showed moderate to high levels of bias, with considerable variation between different versions. For instance, the Claude-3-Haiku-20240307 model recorded a high average bias score of 88.56%, while the Claude-3-Opus-20240229 model demonstrated a much lower bias score of 48.79%. Similarly, the Claude-3.5-Sonnet model exhibited moderate bias, with an average score of 56.33%. These results suggest that while improvements have been made in reducing bias across model versions, significant challenges remain.

The other models evaluated in this study demonstrated bias scores that ranged from moderate to high. For example, the **Qwen models** from Alibaba Bai et al. (2023, 2024a) averaged around 88% bias, while Microsoft's **Phi** models displayed bias scores ranging from 76% to as high as 95% in the Phi-3-medium-4k model. Furthermore, models such as Snowflake's **Snowflake-arctic-instruct** Snowflake (2024), GPT-3.5-turbo, and Smaug-34B-v0.1 Pal et al. (2024) displayed consistently high bias levels, underscoring the ongoing challenge of bias in AI systems. On the other hand, models like Gemma-2-27b-it exhibited notably lower bias levels, suggesting that bias reduction is achievable under certain conditions. These findings emphasize the critical need for continued research and development focused on mitigating bias in AI models to promote fairness and equity in automated decision-making systems.

Future Work

In light of the observed bias across various AI models in this study, future work will focus on implementing effective methods for bias mitigation. One promising approach is the use of prompt debiasing filters, which act as a guardrail on the query and response end, serving as a simple and practical solution for mitigating bias. We plan to evaluate this strategy in future iterations of our work to assess its impact on reducing both Implicit Association Test (IAT) Bias and Decision Bias. Another potential avenue is to explore architectural innovations inspired by models like Google's Gemma-2, where alternating local and global attention mechanisms helped reduce bias. Incorporating such mechanisms into other models could offer insights into how structural changes can mitigate biases.

In addition to these methods, alignment-based techniques aimed at bias mitigation will be explored. Techniques like SAGE Kumar et al. (2024) or rainbow teaming Samvelyan et al. (2024) offer valuable data generation approaches, while reinforcement learning from human feedback (RLHF) Ouyang et al. (2022) or direct preference optimization (DPO) Rafailov et al. (2023) can be applied to better align models with human values, specifically targeting and addressing biased behaviors through fairness-driven fine-tuning. By integrating prompt debiasing, architectural improvements, and alignment strategies, we aim to develop a comprehensive approach that effectively minimizes bias in large language models. The ultimate objective is to establish best practices that foster fairness and equity while maintaining the efficacy and social responsibility of AI systems.

## 7 Conclusion

This study highlights that newer or larger language models do not automatically exhibit reduced bias; in some cases, they displayed higher bias scores than their predecessors, such as in Meta's Llama series and OpenAI's GPT models. This suggests that increasing model complexity without deliberate bias mitigation strategies can unintentionally amplify existing biases. The variability in bias scores within and across providers highlights the need for standardized evaluation metrics and benchmarks for bias assessment. The lack of consistency indicates that bias mitigation is not yet a universally prioritized goal in model development, which can have significant implications when deploying AI systems. Biased outputs can lead to unfair or discriminatory outcomes, emphasizing the importance of addressing this issue. Conversely, models that incorporated innovative architectural features or training methodologies, like Google's Gemma-2 series, demonstrated the potential for reduced bias, indicating that thoughtful design choices can positively impact fairness.

Despite significant advancements in the capabilities of large language models, this study underscores that bias remains a pervasive challenge. Addressing this issue is essential to ensure that AI technologies contribute positively to society and do not perpetuate existing inequalities. Ongoing efforts to understand and mitigate bias will be crucial for the responsible development and deployment of AI systems. Researchers and developers should prioritize bias mitigation strategies, such as incorporating fairness objectives into the training process and diversifying training data. By focusing on these efforts, we can work towards creating AI systems that are not only powerful but also fair and equitable.