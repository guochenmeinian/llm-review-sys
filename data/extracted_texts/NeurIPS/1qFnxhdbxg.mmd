# Energy Discrepancies: A Score-Independent Loss for Energy-Based Models

Tobias Schroder\({}^{1}\) Zijing Ou\({}^{1}\) Jen Ning Lim\({}^{2}\),

**Yingzhen Li\({}^{1}\) Sebastian J. Vollmer\({}^{3}\) Andrew B. Duncan\({}^{1,4}\)**

\({}^{1}\)Imperial College London \({}^{2}\)University of Warwick \({}^{3}\)DFKI and RPTU Kaiserslautern

\({}^{4}\)The Alan Turing Institute

{t.schroeder21, z.ou22, yingzhen.li, a.duncan}@imperial.ac.uk

jen-ning.lim@warwick.ac.uk sebastian.vollmer@dfki.de

Correspondence to: Tobias Schroder, t.schroeder21@imperial.ac.ukCode: https://github.com/J-zin/energy-discrepancy

###### Abstract

Energy-based models are a simple yet powerful class of probabilistic models, but their widespread adoption has been limited by the computational burden of training them. We propose a novel loss function called Energy Discrepancy (ED) which does not rely on the computation of scores or expensive Markov chain Monte Carlo. We show that energy discrepancy approaches the explicit score matching and negative log-likelihood loss under different limits, effectively interpolating between both. Consequently, minimum energy discrepancy estimation overcomes the problem of nearsightedness encountered in score-based estimation methods, while also enjoying theoretical guarantees. Through numerical experiments, we demonstrate that ED learns low-dimensional data distributions faster and more accurately than explicit score matching or contrastive divergence. For high-dimensional image data, we describe how the manifold hypothesis puts limitations on our approach and demonstrate the effectiveness of energy discrepancy by training the energy-based model as a prior of a variational decoder model.

## 1 Introduction

Energy-Based Models (EBMs) are a class of parametric unnormalised probabilistic models of the general form \(p_{}(-U)\) originally inspired by statistical physics. EBMs can be flexibly modelled through a wide range of neural network functions which, in principle, permit the modelling of any positive probability density. Through sampling and inference on the learned energy function, the EBM can then be used as a generative model or in numerous other downstream tasks such as improving robustness in classification or anomaly detection (Grathwohl et al., 2020), simulation-based inference (Glaser et al., 2022), or learning neural set functions (Ou et al., 2022).

Despite their flexibility, EBMs are limited in machine learning applications by the difficulty of their training. The normalisation constant of EBMs, also known as the partition function, is typically intractable making standard techniques such as Maximum Likelihood Estimation (MLE) infeasible. For this reason, EBMs are commonly trained with an approximate maximum likelihood method called Contrastive Divergence (CD) (Hinton, 2002) which approximates the gradient of the log-likelihood using short runs of a Markov chain Monte Carlo (MCMC) method. However, contrastive divergence with short run MCMC leads to malformed estimators of the energy function (Nijkamp et al., 2020), even for relatively simple restricted Boltzmann-machines (Carreira-Perpinan and Hinton, 2005). This can, in part, be attributed to the fact that contrastive divergence is not the gradient of any fixedobjective function (Sutskever and Tieleman, 2010), which severely limits the theoretical understanding of CD and motivated various adjustments of the algorithm (Du et al., 2021; Yair and Michaeli, 2021).

Score-based methods such as Score Matching (SM) (Hyvarinen and Dayan, 2005; Vincent, 2011; Song et al., 2020) and Kernel Stein Discrepancy (KSD) estimation (Liu et al., 2016; Chwialkowski et al., 2016; Gorham and Mackey, 2017; Barp et al., 2019) are a family of competing approaches which offer tractable loss functions and are, by construction, independent of the normalising constant of the distribution. However, such methods suffer from _nearsightedness_ as they fail to resolve global features in the distribution without vast amounts of data. In particular, both SM and KSD estimators are unable to capture the mixture weights of two well-separated Gaussians (Song and Ermon, 2019; Zhang et al., 2022; Liu et al., 2023).

We propose a new loss functional for energy-based models called Energy Discrepancy (ED) which compares the data distribution and the energy-based model via two contrasting energy contributions. By definition, energy discrepancy only depends on the energy function and is independent of the scores or MCMC samples from the energy-based model. In our theoretical section, we show that this leads to a loss functional that can be defined on general measure spaces without Euclidean structure and demonstrate its close connection to score matching and maximum likelihood estimation in the Euclidean case. In our practical section, we focus on a simple implementation of energy discrepancy on Euclidean space which requires less evaluations of the energy-based model than the parameter update of contrastive divergence or score-matching. We demonstrate that the Euclidean energy-discrepancy alleviates the problem of nearsightedness of score matching and approximates maximum-likelihood estimation with better theoretical guarantees than contrastive divergence.

On high-dimensional image data, energy-based models face the additional challenge that under the manifold hypothesis (Bengio et al., 2013), the data distribution is not a positive probability density and does, strictly speaking, not permit a representation of the form of an energy-based model. Energy discrepancy is particularly sensitive to singular data supports and requires the transformation of the data distribution to a positive density. We approach this problem by training latent energy-based priors (Pang et al., 2020) which employ a lower-dimensional latent representation in which the data distribution is positive.

Our contributions are the following: 1) We present energy discrepancy, a new estimation loss for the training of energy-based models that can be computed without MCMC or spatial gradients of the energy function. 2) We show that, as a loss function, ED interpolates between the losses of score matching and maximum-likelihood estimation and overcomes the nearsightedness of score-based methods. 3) We introduce a novel variance reduction trick called \(w\)-stabilisation that drastically reduces the computational cost of approximating energy discrepancy stably.

## 2 Training of Energy-Based Models

In the following, let \(p_{ data}()\) be an unknown data distribution which we are trying to estimate from independently distributed data \(\{^{i}\} p_{ data}\). Energy-based models are parametric distributions of the form

\[p_{}()(-E_{}())\]

for which we want to find the scalar energy function \(E_{}\) such that \(p_{} p_{ data}\). Typically, energy-based models are trained with _contrastive divergence_ which estimates the gradient of the log-likelihood

\[_{} p_{}()=_{p_{}() }[_{}E_{}()]-_{}E_{}( )\,.\] (1)

using Markov chain Monte Carlo methods to approximate the expectation for \(p_{}\). For computational efficiency, the Markov chain is only run for a small number of steps. As a result, contrastive divergence does not learn the maximum-likelihood estimator and can produce malformed estimates of the energy function (Nijkamp et al., 2020).

Alternatively, the discrepancy between data distribution and energy-based model can be measured by comparing their score functions \(_{} p_{ data}\) and \(_{} p_{}\) which, by definition, are independent of the normalising constant. The comparison of the scores is achieved with the _Fisher divergence_. After applying an integration-by-parts and discarding constants with respect to \(\), this leads to the _score-matching_ loss (Hyvarinen and Dayan, 2005)

\[(p_{ data},E_{}):=_{p_{ data}()} [-_{}E_{}()+\|_{ }E_{}()\|^{2}]\,.\] (2)As this only requires the computation of expectations with respect to \(p_{}\), the requirement that the data distribution attains a density can be relaxed, yielding a loss function for \(p_{}\) which can be readily approximated. Score-based methods are nearsighted as the score function only contributes local information to the loss. In a mixture of well-separated distributions, the score matching loss decomposes into a sum of objective functions that only see the local mode and are not capable of resolving the weights of the mixture components (Song and Ermon, 2019; Zhang et al., 2022).

## 3 Energy Discrepancies

To illustrate the idea behind the proposed objective function, we start by motivating energy discrepancy from the perspective of explicit score matching. In the following, we will denote the EBM as \(p_{}(-U)\), where \(U\) is the energy function that is learned. The nearsightedness of score matching arises due to the presence of large regions of low probability which are separating the modes of the data distribution. To increase the probability mass in these regions, we follow (Zhang et al., 2020) and perturb \(p_{}\) and \(p_{}\) through a convolution with a Gaussian kernel \(_{s}(-)(-\|-\|^{2}/2s)\), i.e.

\[p_{s}(): =_{s}(-)p_{}(),\] \[(-U_{s}()): =_{s}(-)(-U()) \,.\]

The resulting perturbed divergence \(}(p_{},U):=(p_{s},U_{s})\) retains its unique optimum at \((-U^{*}) p_{}\)(Zhang et al., 2020) but alleviates the nearsightedness as the data distribution is more spread out. The perturbation with \(_{s}\) simultaneously makes the two distributions more similar which comes at a potential loss of discriminatory power. We mitigate this by integrating the score matching objectives over \(s\) over an interval of noise-scales \([0,t]\). It turns out that this integral can be evaluated as the difference of two contrasting energy-contributions:

\[_{0}^{t}(p_{s},U_{s})s=_{p_{} ()}[U()]-_{p_{}()} _{_{t}(_{t}-)}[U_{t}(_{t})]\,.\] (3)

The proof is given in Appendix A.3. The contrasting expression on the right-hand is now _independent_ of the score and normalisation of the EBM. We argue that such constructed objective functions are useful losses for energy-based modelling.

### A contrastive approach to learning the energy

We lift the idea of learning the energy-based distribution through the contrast of two energy-contributions to a general estimation loss called _Energy Discrepancy_. We will show that energy discrepancy can be defined independently of an underlying perturbative process and is well-posed even on non-Euclidean measure-spaces:

**Definition 1** (Energy Discrepancy).: _Let \(p_{}\) be a positive density on a measure space (\(\), \(\))1 and let \(q(|)\) be a conditional probability density. Define the contrastive potential induced by \(q\) as_

\[U_{q}():=- q(|)(-U()) .\] (4)

_We define the energy discrepancy between \(p_{}\) and \(U\) induced by \(q\) as_

\[_{q}(p_{},U):=_{p_{}( )}[U()]-_{p_{}()}_{q( |)}[U_{q}()].\] (5)

In this paper, we shall largely focus on the case where the data is Euclidean, _i.e._\(=^{d}\), and the base-distribution \(\) is the standard Lebesgue measure. However, this framework also admits \(\) being discrete spaces like spaces of graphs, or continuous spaces with non-trivial base measures such as manifolds. Specifically, the validity of our approach is characterised by the following non-parametric estimation result:

Figure 1: Loss of energy discrepancy, score matching, and maximum likelihood estimation on the task of estimating the weight in a mixture of two Gaussians. For details, see Appendix D.1.

**Theorem 1**.: _Let \(p_{}\) be a positive probability density on (\(\), \(\)), and let \(q(|)\) be a conditional probability density. Under mild assumptions on \(q\) and the set of admissible energy functions \(U\), energy discrepancy \(_{q}\) is functionally convex in \(U\) and has, up to additive constants, a unique global minimiser \(U^{*}=_{q}(p_{},U)\) with \((-U^{*}) p_{}\)._

The assumption on the conditional density \(q\) describes that \( q(|)\) involves some loss of information. The assumption that \(p_{}\) has full support turns out to be critical when scaling energy-discrepancy to high-dimensional data. The proof of Theorem 1 is given in Appendix A.1.

### Choices for the conditional distribution \(q\)

Def. 1 offers a wide range of possible choices for the perturbation distribution \(q\). In Appendix D.3 we discuss a possible choice in the discrete space \(\{0,1\}^{d}\). For the remainder of this paper, however, we will focus on Euclidean data \(^{d}\) and hope that the generality of our result inspires future work. Our requirements on \(q\) are that simulating from the conditional distribution \( q(|)\) and computing the convolution that defines \(U_{q}\) are numerically tractable. On continuous spaces, a natural candidate for \(q\) is the transition density of a diffusion process which arises as solution to a stochastic differential equations of the form \(_{t}=(_{t})t+ _{t}\) with drift \(\) and standard Brownian motion \(_{t}\)(see Oksendal, 2003). The conditional density \(q_{t}(|)\) represents the probability density of the perturbed particle \(_{t}\) that was initialised at \(_{0}=\). The resulting transition density then satisfies both of our requirements by employing the Feynman-Kac formula as we line out in Appendix B.2. Although this approach makes the choice of \(q\) flexible, the following interpolation result stresses that not much is lost when choosing a Gaussian transition density \(_{t}(-\|-\|^{2}/2t)\):

**Theorem 2**.: _Let \(q_{t}\) be the transition density of the diffusion process \(_{t}=(_{t})t+d_{t}\), let \(p_{}(-U)\) be the energy-based distribution and \(p_{t}\) the data-distribution convolved with \(q_{t}\)._

1. _The energy discrepancy is given by a multi-noise scale score matching loss_ \[_{q_{t}}(p_{},U)=_{0}^{t}_{p_{}(_{s})}[- U_{q_{s}}(_{s})+\|  U_{q_{s}}(_{s})\|^{2}]\,s+.\]
2. _If_ \(=0\)_, i.e._ \(q_{t}\) _is the Gaussian transition density_ \(_{t}\)_, the energy discrepancy converges to the loss of maximum likelihood estimation a linear rate in time_ \[|_{_{t}}(p_{},U)+_{p_{}()}[ p_{}()]-c(t) |_{2}^{2}(p_{},p_{})\] _where_ \(c(t)\) _is independent of_ \(U\) _and_ \(_{2}\) _denotes the Wasserstein distance_2_._ 
Theorem 2 has two main messages: All diffusion-based energy discrepancies behave like a multi-noise scale score matching loss, which is independent of the drift \(\). In fact, we show in Appendix A.2 that for a linear drift \(_{t}\), the induced energy discrepancy is always equivalent to the energy discrepancy based on a Gaussian perturbation. Furthermore, estimation with a Gaussian-based energy discrepancy approximates maximum likelihood estimation for large \(t\), thus enjoying its attractive asymptotic properties provided \(_{_{t}}(p_{},U)\) can be approximated with low variance. We demonstrate the result in a mixture model in Figure 1 and Appendix D.1 and give a proof of above theorem in Appendices A.3 and A.4.

**Connection to Contrastive Divergence.** Due to the generality of our result we can also make a direct connection between energy discrepancy and contrastive divergence. To this end, suppose that for \(\) fixed, \(q\) satisfies the detailed balance relation \(q(|)(-E_{}())=q(| )(-E_{}())\). In this case, energy discrepancy becomes the loss function

\[_{q}(p_{},E_{})=_{p_{}( )}[E_{}()]-_{p_{}( )}_{q(|)}[E_{}()]\] (6)

which, after taking gradients in \(\) yields the contrastive divergence update3. See Appendix A.5 for details. The non-parametric estimation result from Theorem 1 holds true for the contrastive objective in (6). This means that each step of contrastive divergence optimises an objective function with minimum at \(p_{} p_{}\). However, the objective function is adjusted in each step of the algorithm.

Training Energy-Based Models with Energy Discrepancy

In sight of Theorem 2, we will discuss how to approximate ED from samples for Euclidean data \(\{^{i}\}^{d}\) and a Gaussian conditional distribution \(_{t}(-)(-\|-\|^{2}/2t)\). First, the outer expectations on the right-hand of (5) can be computed as plug-in estimators by simulating the Gaussian perturbation \(^{i}_{t}=^{i}+\) for \((0,)\) and averaging \(\{U(^{i})\}\) and \(\{U_{_{t}}(^{i}_{t})\}\). The critical step is then finding a low-variance estimate of the contrastive potential \(U_{_{t}}\) itself.

Due to the symmetry of the Gaussian transition density, we can interpret \(_{t}\) as the law of a Gaussian random variable with mean \(^{i}_{t}\) and variance \(t\), i.e. the conditioned random variable \(^{i}_{t}+^{}|^{i}_{t}\) for \(^{}(0,)\) has the density \(_{t}(^{i}_{t}-)\). Hence, we can write \(U_{_{t}}\) as the expectation

\[U_{_{t}}(^{i}_{t})=-_{_{1}(^{})}[(-U(^{i}_{t}+^{}) )|^{i}_{t})]\]

for \(i=1,2,,N\). The conditioning expresses that we keep \(^{i}_{t}\) fixed when taking the expectation with respect to \(^{}\). It is then possible to calculate the expectation by sampling \(^{ i,j}(0,)\) and calculating the mean as for the outer expectations. However, we find that this approximation is not sufficient as it is biased due to the logarithm and prone to numerical instabilities because of missing bounds on the value of \(U_{_{t}}(^{i}_{t})\). To stabilise training, we augment the Monte Carlo estimate of \(U_{_{t}}(^{i}_{t})\) by an additional term \(w/M(-U(^{i}))\) which we call \(w\)-stabilisation. This results in the following approximation of the contrastive potential:

\[U_{_{t}}(^{i}_{t})\!\!-((-U( ^{i}))\!+\!_{j=1}^{M}(-U(^{i}_{t}\!+ \!^{ i,j})))^{ i,j}(0,)\]

The \(w\)-stabilisation dampens the contributions of contrastive samples \(^{i}_{t}\!+\!^{ i,j}\) whose energy is higher than the energy of the data point \(^{i}\). This provides a deterministic upper bound for the approximate contrastive potential in (4) and reduces the variance of the estimation. We find that the \(w\)-stabilisation drastically reduces the number of samples \(M\) needed for the stable training of deep energy-based models. We illustrate the effect of the stabilisation in Figures 2 and 24 and discuss our reasoning in more details in Appendix B.1. The full loss is now formed for \(U:=E_{}\) with \(^{i}(0,)\), \(^{ i,j}(0,)\) and tunable hyperparameters \(t,M,w\) as

\[_{t,M,w}():=_{i=1}^{N}(+ _{j=1}^{M}(E_{}(^{i})-E_{}( ^{i}+^{i}+^{ i,j})) )\.\]

The loss is evaluated using the numerically stabilised logsumexp function. The justification of the approximation is given by the following theorem:

**Theorem 3**.: _Assume that \((-E_{}())\) is uniformly bounded. Then, for every \(>0\) there exist \(N\) and \(M(N)\) such that \(|_{t,M(N),w}()-_{_{t}}(p_{},E_{})|<\) almost surely._

We give the proof in Appendix B.1. This result forms the basis for proofs of asymptotic consistency of our estimators which we leave for future work. It is noteworthy that an analogous implementation of energy discrepancy for other perturbation kernels and other spaces is possible. For example, we construct a similar loss for discrete spaces using a Bernoulli perturbation in Appendix D.3. Similarly to the Gaussian case, the \(w\)-stabilisation provides a useful variance reduction for the training with Bernoulli-based energy discrepancy.

### Training Energy-Based Models under the Manifold Hypothesis

LeCun (2022) suggests that maximum-likelihood-based training of energy-based models can lead to the formation of undesirable canyon shaped energy-functions. Indeed, energy discrepancy yields an

Figure 2: Estimated energy functions for Gaussian data with various choices of \(w\). Increasing \(w\) leads to flatter energy-landscapes, and training becomes unstable if \(w=0\). For details, see Appendices B.1 and D.2.

energy with low values on the data-support and rapidly diverging values outside of it when being used on image data, directly. Such learned energies fail to represent the distribution between data-points and are not suitable for inference or image generation. We attribute this phenomenon to the manifold hypothesis, which states that the data concentrates in the vicinity of a low-dimensional manifold (Bengio et al., 2013). In this case, the data distribution is not a positive density and can not be written as an energy-based model as \( p_{}\) is not well-defined. Additionally, Gaussian perturbations of a data point \(}:=+\) are orthogonal to the data manifold with high-probability and the negative samples in the contrastive term are not informative.

To resolve this problem, we will work with a lower-dimensional latent representation of the data distribution in which positivity can be ensured. We follow Pang et al. (2020) and define a variational decoder network \(p_{}(|)\) and an energy-based tilting of a Gaussian prior \(p_{}()=(-E_{}())p_{0}()\) on latent space, resulting in the so-called latent energy-based prior models (LEBMs) \(p_{,}() p_{}(|)p_{ }()\). To obtain the latent representation of data we sample from the posterior \(p_{,}(|) p_{}(|) (-E_{}())p_{0}()\) using a Langevin sampler. For the training of model, the contrastive divergence algorithm is replaced with energy discrepancy (for details see appendix C). LEBMs provide an interesting benchmark to compare energy discrepancy with contrastive divergence in high dimensions. However, other ways to tackle the manifold problem are elements of ongoing research.

## 5 Empirical Studies

To support our theoretical discussion, we evaluate the performance of energy discrepancy on low-dimensional datasets as well as for the training of latent EBMs on high-dimensional image data sets. In our discussion, we emphasise the comparison with contrastive divergence and score matching as the dominant methodologies in EBM training.

### Density Estimation

We first demonstrate the effectiveness of energy discrepancy on several two-dimensional distributions. Figure 3 displays the estimated unnormalised densities as well as samples that were synthesised with Langevin dynamics for energy discrepancy (ED), score matching (SM) and contrastive divergence (CD). More experimental results and details are given in Appendix D.4. Our results confirm the aforementioned nearsight-edness of score matching which does not learn the uniform weight distribution of the Gaussian mixture. For CD, it can be seen that CD consistently produces flattened energy landscapes which can be attributed to the short-run MCMC (Nijkamp et al., 2020a, see) not having converged. Consequently, the synthesised samples of energies learned with CD can lie outside of the data-support. In contrast, ED is able to model multi-modal distributions faithfully and learns sharp edges in the data support as in the chessboard data set. We quantify our results in Figure 4 which shows the mean squared error of the estimated log-density of 25-Gaussians over the number of training iterations. The partition function is estimated using importance sampling \( Z\!\!(-E(_{i})\!-\! p_{}(_{i}))\!-\! N\), where \(_{i}\) is sampled from the data distribution \(p_{}()\) and \(N\!=\!5,000\). It shows that ED outperforms SM and CD with faster convergence, lower mean square error, and better stability.

Figure 4: Density estimation accuracy in the \(25\)-Gaussians dataset.

Figure 3: Comparison of energy discrepancy, score matching and contrastive divergence on density estimation. The \(1\)st and \(2\)nd rows are the estimated density and synthesised samples, respectively.

### Image Modelling

In this experiment, our methods are evaluated by training a latent EBM on three image datasets: SVHN (Netzer et al., 2011), CIFAR-10 (Krizhevsky et al., 2009), and CelebA (Liu et al., 2015). The effectiveness of energy discrepancy is diagnosed through image generation, image reconstruction from their latent representation, and the faithfulness of the learned latent representation. The model architectures, training details, and the choices of hyper-parameters can be found in Appendix C.3.

Image Generation and Reconstruction.We benchmark latent EBM priors trained with energy discrepancy (ED-LEBM) and score matching (SM-LEBM) against various baselines for latent variable models which are included in Table 1. Note that the original work on latent EBMs (Pang et al., 2020) uses contrastive divergence (CD-LEBM) (see appendix C for details).

For a well-learned model, the latent EBM should produce realistic samples and faithful reconstructions. The reconstruction error is measured via the mean square error while the image generation is measured with the FID (Heusel et al., 2017) which are both reported in Table 1. We observe that ED can improve the contrastive divergence benchmark on SVHN and CelebA while the results on CIFAR-10 could not be improved. However, we emphasise that ED only requires \(M\) (here, \(M=16\)) evaluations of the energy function per data point which is significantly less than CD and SM that both require the calculation of a high-dimensional spatial gradient. Besides the quantitative metrics, we present qualitative results of the generated samples in Figure 5. It can be seen that our model generates diverse high-quality images. The qualitative results of the reconstruction are shown in Figure 6, for which we use the test set of CelebA \(64 64\). The right column shows the original image \(\) to be reconstructed. The left column shows the reconstruction based on a latent variable initialised from the base distribution \(_{0} p_{0}()\), and the middle column displays the image reconstructed from \(_{k} p_{,}(|)\) which is sampled via Langevin dynamics. One can see that our model can successfully reconstruct the test images, verifying the validity of the latent prior learned with energy discrepancy. In addition, we showcase the scalability of our approach by

    &  &  &  \\  & MSE & FID & MSE & FID & MSE & FID \\  VAE (Kingma and Welling, 2014) & 0.019 & 46.78 & 0.057 & 106.37 & 0.021 & 65.75 \\
2s-VAE (Dai and Wipf, 2018) & 0.019 & 42.81 & 0.056 & 72.90 & 0.021 & 44.40 \\ RAE (Ghosh et al., 2019) & 0.014 & 40.02 & 0.027 & 74.16 & 0.018 & 40.95 \\ SRI (Nijkamp et al., 2020b) & 0.018 & 44.86 & 0.020 & - & - & 61.03 \\ SRI (L=5) (Nijkamp et al., 2020b) & 0.011 & 35.32 & - & - & 0.015 & 47.95 \\ CD-LEBM (Pang et al., 2020) & 0.008 & 29.44 & **0.020** & **70.15** & 0.013 & 37.87 \\ SM-LEBM & 0.010 & 34.44 & 0.026 & 77.82 & 0.014 & 41.21 \\  ED-LEBM (ours) & **0.006** & **28.10** & 0.023 & 73.58 & **0.009** & **36.73** \\   

Table 1: Comparison of MSE(\(\)) and FID(\(\)) on the SVHN, CIFAR-10, and CelebA datasets.

Figure 5: Generated examples by the ED-LEBM trained on SVHN, CIFAR-10, and CelebA.

Figure 6: Image reconstruction results on CelebA \(64 64\).

[MISSING_PAGE_FAIL:8]

Related Work

**Training Energy-based models.** While energy-based models have been around for some time (Hinton, 2002), the training of energy-based models remains challenging. For a summary on existing methods for the training of energy-based models see Song and Kingma (2021) and LeCun et al. (2006). Contrastive divergence is still the most used option for training energy-based models. Recent extensions have improved the scalability of the basic algorithm, making it possible to train EBMs on high-dimensional data (Ngiam et al., 2011; Dai et al., 2015; Xie et al., 2016; Nijkamp et al., 2019; Du and Mordatch, 2019). Despite these advances, it has been noted that contrastive divergence is not the gradient of any fixed loss-function (Sutskever and Tieleman, 2010) and can yield energy functions that do not adequately reflect the data distribution (Nijkamp et al., 2020). This has motivated improvements to the standard methodology (Du et al., 2021) by approximating overlooked entropy terms or by improving the convergence of MCMC sampling with an underlying diffusion model (Gao et al., 2021).

Score-based methods (Hyvarinen and Dayan, 2005; Vincent, 2011; Song et al., 2020) are typically implemented by directly learning the score function instead of the energy function. Song and Ermon (2019); Song et al. (2021) point out the importance to introduce multiple noise levels at which the score is learned. Li et al. (2019) adopt the strategy of learning an energy-based model using multi-scale denoising score matching.

We learn a latent EBM prior (Pang et al., 2020) on high-dimensional data to address situations where the data lives on a lower dimensional submanifold. This methodology has been improved with a multi-stage approach (Xiao and Han, 2022) with score-independent noise contrastive estimation (Gutmann and Hyvarinen, 2010).

**Related loss functions.** Energy discrepancies can be derived from Kullback Leibler-contraction divergences (Lyu, 2011) which we also discuss briefly in Appendix A.6. Diffusion recovery likelihood (Gao et al., 2021) technically optimises the same loss as us but the EBM is trained with contrastive divergence. Luo et al. (2023) make similar observations to us while estimating the KL-contraction by leveraging the time evolution of the energy function along a diffusion process. Energy discrepancy also shares similarities with other contrastive loss functions in machine learning. The \(w\)-stabilised energy-discrepancy loss with \(w,M=1\) is equivalent to conditional noise contrastive estimation (Ceylan and Gutmann, 2018) with Gaussian noise. For \(M>1\), the stabilised ED loss shares great structural similarity to certain contrastive learning losses such as InfoNCE (van den Oord et al., 2018). We hope that such observations can lead to interpretations of the \(w\)-stabilisation.

**Theoretical connections between likelihood and score-based functionals.** The connection between likelihood methods and score-based methods generalises the de Bruijn identity (Stam, 1959; Lyu, 2009) which explains that score matching appears frequently as the limit of EBM training methods. Similar connections have been mentioned and exploited by Song et al. (2021) to choose a weighting scheme for a combination of score matching losses.

**Generative modelling for manifold-valued data.** The manifold hypothesis was, for example, described in Bengio et al. (2013). Prior work to us shows how score-based diffusion models detect this manifold (Pidstrigach, 2022) and give reasons why CD is more robust to this issue than other contrastive methods (Yair and Michaeli, 2021). Arbel et al. (2021) learn an energy-based model as a tilt of a GAN-based prior that models the data-manifold. We believe that a combination of above results with energy discrepancy could enable training EBMs with energy discrepancy on data space.

## 7 Discussion and Outlook

We demonstrate that energy discrepancy provides a new tool for the fast training of energy-based models without MCMC or scores. We show for Euclidean data that ED interpolates between score matching and maximum likelihood estimation, thus alleviating problems of nearsightedness of score matching without additional annealing strategies as in score-based diffusion models. Based on our theoretical analysis, we show that training EBMs using energy discrepancy yields more accurate estimates of the energy function for two-dimensional data than explicit score matching and contrastive divergence. In this task, it is robust to the hyperparameters used, making energy discrepancy a useful tool for energy-based modelling with little tuning required. We further establish that energy discrepancy achieves comparable results to contrastive divergence at a lower computational cost when learning a lower-dimensional energy-based prior for high-dimensional data. This shows that energy discrepancy is scalable to high dimensions.

**Limitations:** Energy-based models make the assumption that the data-distribution has a positive density which is violated for most high-dimensional data sets due to the manifold hypothesis. Compared to contrastive divergence or diffusion-based strategies, energy discrepancy is especially sensitive to such singularities in the data set. Currently, this limits energy discrepancy to settings in which a lower-dimensional representation with non-singular support can be learned or constructed efficiently and accurately.

**Outlook:** This work can be extended in various ways. Firstly, we want to explore other choices for the conditional distribution \(q\). In particular, the effect of different types of noise such as Laplace noise or anisotropic Gaussian noise are open questions. Furthermore, energy discrepancy is a well-suited objective function for discrete data for appropriately chosen perturbations. We present a preliminary study of energy discrepancy on discrete spaces in the workshop paper Schroder et al. (2023). Secondly, we believe that improvements to our methodology can be made by learning the energy function of image-data on pixel space directly, as in this domain specific architectures such as U-nets (Ronneberger et al., 2015; Song and Ermon, 2019) can be used in the modelling. However, this requires further work in learning the data-manifold during training so that early saturation can be prevented. Finally, the partial differential equations arising in Euclidean energy discrepancies promise exciting insights into the connections between energy-based learning, stochastic differential equations, and optimal control which we want to investigate further.