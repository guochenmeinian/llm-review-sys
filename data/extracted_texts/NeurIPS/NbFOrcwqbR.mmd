# Taming Generative Diffusion Prior for Universal Blind Image Restoration

Siwei Tu\({}^{1}\), Weidong Yang\({}^{1,}\), Ben Fei\({}^{2,}\)

\({}^{1}\)Fudan University, \({}^{2}\)Chinese University of Hong Kong

24110240079@m.fudan.edu.cn, wdyang@fudan.edu.cn, benfei@cuhk.edu.hk

\({}^{}\) Corresponding Authors

###### Abstract

Diffusion models have been widely utilized for image restoration. However, previous blind image restoration methods still need to assume the type of degradation model while leaving the parameters to be optimized, limiting their real-world applications. Therefore, we aim to tame generative diffusion prior for universal blind image restoration dubbed **BIR-D**, which utilizes an **optimizable convolutional kernel** to simulate the degradation model and dynamically update the parameters of the kernel in the diffusion steps, enabling it to achieve blind image restoration results even in various complex situations. Besides, based on mathematical reasoning, we have provided an empirical formula for the chosen of **adaptive guidance scale**, eliminating the need for a grid search for the optimal parameter. Experimentally, Our BIR-D has demonstrated superior practicality and versatility than off-the-shelf unsupervised methods across various tasks both on real-world and synthetic datasets, qualitatively and quantitatively. BIR-D is able to fulfill multi-guidance blind image restoration. Moreover, BIR-D can also restore images that undergo multiple and complicated degradations, demonstrating the practical applications. The code is available at https://github.com/Tusiwei/BIR-D

## 1 Introduction

Figure 1: Blind Image Restoration Diffusion Model (BIR-D) can achieve high-quality restoration for different types of degraded images. BIR-D not only has the capability to restore (a) linear inverse problems when the degradation function is known. BIR-D can also achieve high-quality image restoration in (b) blind issues with unknown degradation functions, as well as in (c) mixed degradation and real degradation scenarios.

Images inevitably suffer from a degradation in quality in the process of capturing, storing, and compressing. Thus, the image restoration task intends to establish a mapping between the degraded image and the original image, to recover a high-quality image from the degraded image. In an ideal scenario, the ultimate goal is to undo and restore the degradation process of the image. However, in reality, the complexity of the degradation mode often leads to the incapability to fully restore the original high-quality image, which also makes traditional supervised approaches unsuitable for all types of image restoration tasks. According to the degradation mode, image restoration tasks can be divided into two types: **non-blind** and **blind** problems. **Blind** problems, such as low light enhancement, motion blur reduction and HDR image restoration, refer to image restoration problems where the degradation functions and parameters are totally unknown.

The blind image restoration problem has attracted increasing attention with the development of generative models. The unsupervised blind image restoration methods represented by Generative Adversarial Networks (GANs) [1; 2; 3; 4] have the capability to train networks on large datasets of clean images and learn real-world knowledge. However, GANs are still difficult to avoid falling into limitations such as poor diversity and difficulty in model training. In parallel, diffusion model [5; 6; 7; 8; 9] have shown strong performance in terms of quality and diversity compared to GANs. Pioneer works such as GDP , DDRM , and DDNM  attempt to solve such problems by incorporating the degraded image \(y\) as guidance in the sampling process of diffusion models. By modeling posterior distributions in an unsupervised sampling manner, these approaches showcase the potential for practical guidance in blind image restoration, offering promising implications for real-world applications. However, the degradation types in these models still need to be assumed, limiting the practicality of natural image restoration where the complicated degradation models always remain unknown.

To this end, we propose an effective and versatile Blind Image Restoration Diffusion Model (BIR-D). It utilizes well-trained DDPM  as an effective prior and is guided by degraded images to form a universal method for various image restoration and enhancement tasks. To uniformly model the unknown degradation function of blind image restoration, an optimizable convolutional kernel is dynamically optimized and utilized to simulate the degradation function at each denoising step. Specifically, BIR-D updates the convolution kernel parameters based on the gradient of distance loss between the generated image undergoing our optimizable convolutional kernel and the given degraded image. At the same time, all existing image restoration methods [10; 11; 12] that use diffusion models manually set the guidance scale as a hyperparameter to control the magnitude of guided generation, which also remains unchanged throughout the sampling process. However, for images from different tasks, the guidance scale required for each diffusion step is not entirely the same. To deal with this issue, we have derived an empirical formula for the guidance scale, which can calculate the optimal guidance scale for the next denoising step in real-time during the sampling process. This improvement avoids the need to manually grid search the optimal value of the guidance scale when solving different tasks and also enhances the quality of generated images. With the help of a well-trained DDPM, the above designs enable BIR-D to tackle various blind image restoration tasks. BIR-D can also achieve multi-degradation or multi-guidance image restoration. Furthermore, it showcases satisfactory results in addressing restoration issues related to complex degradation types encountered in real-world scenarios.

## 2 BIR-D: Universal Blind Image Restoration Diffusion Model

In this study, we aim to use a well-trained DDPM  to learn the prior distribution of images and ultimately solve non-blind and blind problems in various image restoration tasks.

### Optimizable convolutional kernel as a universal degradation function

For a natural image \(x\), its corresponding degraded image \(y\) can be obtained by the degradation function \(y=(x)\). Most of the blind image restoration methods [10; 12] are used to solve the situation where the degradation function \(\) is known while leaving the parameters of \(\) are unknown. However, when dealing with real-world image restoration problems, the degradation function \(\) is not only an unknown quantity but also difficult to accurately represent mathematically. Therefore, we propose an optimized convolutional kernel to simulate complex degradation functions. The parameters of the convolution kernel in the degradation function are dynamically optimized along with the denoising steps.

Moreover, in the real-world scenario, considering that there are different noises in different subtle areas of the image, using only one optimized convolutional kernel may not fully cover this situation. Therefore, we propose to utilize a mask \(\) to model and estimate these noises. Thus, the entire degradation process can be represented as: \(y=K(x)+,\) where \(\) refers to the optimized convolutional kernel used in the model and \(\) is a mask with the same dimension as image \(x\). \(\) and \(\) have their own optimizable parameters, forming the degradation function

\(mathcal{}iD\). In this way, any degradation process can be simulated by this degradation function.

### Empirical formula of guidance scale

In the reverse denoising process of DDPM, the generated images can be conditioned on degraded image \(y\). Specifically, the distribution \(p_{}(x_{t-1}|x_{t})\) of reverse denoising is converted into

``` Input: Degraded image \(y\), degradation function \(\) composed of optimized convolutional kernels \(\) with parameters \(\) and mask \(\) with parameters \(\), learning rate \(l\), distant measure \(\). Output: Output image \(x_{0}\) conditioned on \(y\). Sample \(x_{T}\) from \((0,I)\) for t from T to 1 do \(_{0}=}{_{t}}}-_{t}}_{t}(x_{t},t)}{_{t}}}\) \(_{,,_{t}}=(y,^{ ,}(_{0}))\) \(s=--)^{}+C+ N}{(^{,}(_{0}),y)}\) \(_{0}=_{0}-_{t}}_{t}}{ _{t-1}}_{t}}_{_{0}}_{,,_{0}}\) \(_{t}=_{t-1}}_{t}}{1-_ {t}}_{0}+_{t}}(1-_{t-1})}{1- _{t}}x_{t}\) \(_{t}=_{t-1}}{1-_{t}}_{t}\)  Sample \(x_{t-1}\) from \((_{t},_{t}I)\) \(=-_{,,_{0}}\) \(-_{,,_{0}}\) return\(x_{0}\) ```

**Algorithm 1**Unconditional diffusion model with the guidance of degraded image \(y\), given a diffusion model noise prediction function \(_{}(x_{t},t)\).

Figure 2: **Overview of BIR-D.** Degraded image \(y\) was given during the sampling process. BIR-D systematically incorporates guidance from degraded images in the reverse process of the diffusion model and optimizes the degraded model at the same time. For degraded image \(y\), pre-training is first performed to provide a better initial state for BIR-D. BIR-D introduces a distance function in each step of the reverse process of the diffusion model to describe the distance loss between the degraded image \(y\) and the generated image \(_{0}\) after the degradation function, so that the gradient could be used to update and simulate a better degradation function. Based on the empirical formula, the adaptive guidance scale can be calculated to provide optimal guidance during the sampling process.

a conditional distribution \(p_{}(x_{t-1}|x_{t},y)\). It is demonstrated  that the difference between it and the original formula lies in the addition distribution of \(p(y|x_{t})\), which serves as a probability representation for denoising \(x_{t}\) into a high-quality image consistent with \(y\). Previous work  proposed a feasible calculation to approximate this indicator by using heuristic algorithms:

\[ p(y x_{t})=- N-s((_{0}),y)),\] (1)

where \(N\) is the normalization factor, which is the distribution \(p_{}(y|x_{t+1})\), and \(s\) is the scalar factor used to control the importance of guidance, named guidance scale. \(\) is the distance metric. The value of the guidance scale plays a crucial role in the quality of the image generation result. A larger value can lead to overall blurring of the image, while a smaller value can result in missing details in the restoration. However, the guidance scale in existing works [10; 9; 12] can only be manually set as a hyperparameter. But in specific experiments, the optimal value of the guidance scale varies in different masks, degraded images, and diffusion steps. The original configuration necessitates thorough testing for the initial setup. Additionally, employing the same guidance scale for every denoising step is not an optimal choice.

Therefore, we propose an empirical formula for the guidance scale, which can dynamically calculate and update the optimal values of guidance factors in real-time at each diffusion step of degraded images in specific repair tasks. Specifically, we noticed that the distribution \( p_{}(y|x_{t})\) can be applied to perform Taylor expansion around \(x=\) and take the first two terms. The detailed process of proving can be found in Appendix D.

\[ p_{}(y x_{t})=(x_{t}-)^{T}g+C,\] (2)

where \(g=_{x_{t}} p_{}(y x_{t})_{x_{t}=}\), \(C= p(y x_{t})_{x_{t}=}\). By combining the heuristic approximation formula and Taylor expansion formula mentioned above, we can simplify the empirical

    &  &  \\   & FID & NIQE & FID & NIQE \\  PGDiff  & 71.62 & 4.15 & 39.17 & 3.93 \\ DiffBIR  & **39.58** & 4.03 & 32.35 & 3.78 \\  BIR-D & 40.12 & **3.94** & **31.49** & **3.65** \\   

Table 1: **Quantitative comparison of blind face restoration on LFW and WIDER datasets**

Figure 3: Comparison of image quality for blind face restoration results on LFW  and WIDER dataset .

formula for the guidance scale:

\[s=--)^{T}g+C+ N}{((_{0}),y)}.\] (3)

The guidance scale is related to the generated images \(x\), degraded image \(y\), and the degradation function \(\). This value of this **Adaptive Guidance Scale** can be dynamically updated in each diffusion step so that each step in the diffusion model can use the most appropriate guidance scale.

### Sampling process of BIR-D

Through empirical formulas, we can obtain the conditional transition formula in the reverse process of the diffusion model.

\[ p_{}(x_{t}|x_{t+1},y) =(p_{}(x_{t}|x_{t+1})p(y|x_{t})+N_{1}.\] (4) \[ p(z)+N_{2},\] (5)

where \(z\) conforms to the distribution \((z;_{}(x_{t},t)+ g,)\). The intermediate quantity \(g=_{x_{t}} p(y|x_{t})\). The value of \(g\) can be obtained by calculating the gradient in heuristic algorithms in eq. (1), which includes the parameter of guidance scale:

\[g=_{x_{t}} p(y|x_{t})=-s_{x_{t}}((x_{t}),y)\] (6)

The other terms \(N_{1},N_{2}\), and the variance of the reverse process \(=_{}(x_{t})\) in eq. (4) and eq. (5) are constants, and the unconditional distribution \(p_{}(x_{t-1}|x_{t})\) is given by traditional diffusion models.

Therefore, the conditional transition distribution \(p(x_{t-1}|x_{t},y)\) can be approximately estimated by adding \(-(s_{x_{t}}((x_{t}),y))\) to the mean of the traditional unconditional transition distribution. Previous studies  have shown that the addition of \(\) has a negative impact on the quality of generated images. Therefore, in this experiment, we omitted the term \(\), and the complete sampling process is shown in algorithm 1.

Detailly, in the diffusion step \(t\) of the sampling process, the noise of \(x_{t}\) is first predicted from the given pre-trained DDPM and eliminated to obtain an estimated value of \(x_{0}\). Subsequently, apply the degradation function of step \(t\) to \(x_{0}\) and calculate its reconstruction loss with the degraded image

    & \)-Super resolution**} &  & \)/ Impairing**} &  \\   & PSNR & SSIM & Consistency & FID & PSNR & SSIM & Consistency & FID & PSNR & SSIM & Consistency & FID \\  RED & 24.18 0.71 & 27.57 & 98.30 & 21.30 & 0.58 & 63.20 & 69.55 & - & - & - & - & - & - & - \\ DGP & 21.45 0.56 & 158.74 & 152.85 & 26.00 & 0.54 & 47.10 & 136.53 & 27.98 & 0.824 & 41.40 & 60.65 & 18.42 & 0.71 & 305.59 & 94.59 \\ SNIPS & 22.38 0.66 & 21.38 & 154.43 & 27.43 & 69.09 & 61.11 & 17.11 & 17.55 & 0.74 & 587.90 & 103.50 & - & - & - \\ DDRM & **26.53** 0.78 & 19.39 & 40.75 & **35.64** **0.98** & 50.24 & 4.78 & 34.28 & 0.95 & **4.08** & 24.09 & **22.12** & 0.91 & 37.33 & 47.05 \\ DDMM & 25.36 0.81 & 7.52 & 39.14 & 24.66 & 0.71 & 41.70 & 4.64 & 32.16 & **0.96** & 5.42 & 17.63 & 21.95 & 0.89 & 36.41 & 38.79 \\ GDP & 24.42 & 0.68 & 6.49 & 38.24 & 25.98 & 0.75 & 41.27 & 2.43 & **34.04** & **0.96** & 5.29 & 16.58 & 12.41 & **0.92** & 36.92 & 37.60 \\  BIR-D & 24.58 0.71 & **6.32** & **37.54** & 26.31 & 0.73 & **38.42** & **2.32** & 33.59 & 0.90 & 5.18 & **15.73** & 22.09 & 0.89 & **36.12** & **36.58** \\   

Table 2: **Quantitative comparison of linear inverse problems on ImageNet 1k.**

Figure 4: **Comparison of colorization image on ImageNet 1k.** BIR-D can generate various outputs on the same input image.

\(y\). We utilize our adaptive guidance scale for sampling the next step latent \(x_{t-1}\). In this process, it is necessary to calculate the gradient about \(x_{0}\) and the parameters of each convolution kernel in the distance metric loss, which is used to update the convolution kernel parameters in real time for the next sampling process.

**Pre-process.** The empirical formula for the guidance scale we construct is related to the degradation function. Herein, when the model simulates the degradation function more reasonably, BIR-D can obtain more appropriate guidance scale values accordingly. To this end, we introduce a first-stage pre-training model from  to further enhance the model's capability to correct initial deviations. This enables the model to have a strong correction ability for significant deviations in the degradation function during the initial diffusion step, ultimately generating ideal image restoration results.

Figure 5: Results of linear degradation tasks on 256 Ã— 256 images from ImageNet 1k.

Figure 6: Comparison of image quality in low-light enhancement task on the LoL , VE-LOL  and LoLi-Phone  datasets.

**Multi-degradation Image Restoration.** In the real world, the degradation process often involves a combination of multiple different complex types. To improve the image restoration capability of the model in complex situations and enhance its practicality, we propose to extend BIR-D into multi-task scenarios. To our surprise, BIR-D can fulfill multi-degradation image restoration without any modification (Figure 9) thanks to the mixture of degradation types can also be simulated as an unknown degradation by an optimizable convolutional kernel.

## 3 Experiments

In this section, we systematically compare BIR-D with other blind image restoration methods in real-world and synthetic datasets. We have attached some more specific details, such as the dataset, implementation, evaluation, and other results in the Appendix.

**Blind Image Restoration on Real-world Datasets.** Firstly, we evaluate the blind image restoration capability of BIR-D on two real-world datasets, namely LFW dataset  and WIDER dataset . As shown in Figure 3, BIR-D successfully simulated and removed blur, and achieved more ideal facial detail restoration. The quantitative results in Table 1 shows that BIR-D outperforms PGDiff  and DiffBIR  in NIQE metric on both datasets and FID metric on WIDER, demonstrating better blind image restoration performance.

**Comparison on Common Linear Inverse Problems.** We conducted experiments on linear inverse problems on ImageNet 1k to compare BIR-D with off-the-shelf methods. For each experiment, we calculated the average Peak Signal-to-Noise Ratio (PSNR), Structural Similarity (SSIM), Consistency, and FID results, where PSNR, SSIM, and Consistency are used to quantify the faithfulness between the generated image and the original image, while FID is used to measure the quality of the generated image. To make fair comparisons, other methods are given known degradation functions as reported in the original paper while BIR-D utilizes universal degradation functions for different tasks. Table 2 shows that BIR-D outperforms other methods in terms of Consistency and FID in almost all tasks. As shown in Figure 5, the images generated by BIR-D demonstrate a high level of image quality and details. Moreover, Figure 4 also demonstrates that BIR-D can generate various results in image restoration tasks.

**Low Light Enhancement.** We further evaluated the effectiveness of BIR-D in low-light image enhancement. Following the previous works , we utilized three datasets, LOL , VE-LOL-L , and LoLi-Phone , to test the restoration ability of BIR-D. As shown in Table 3, our BIR-D outperforms all the zero-shot methods in both FID and Lightness Order Error (LOE) , and demonstrates significant improvement in Perceptual Index (PI) . A lower PI value reflects better perceptual quality, while a lower LOE reflects a better natural preservation ability of the generated image, making images to have a more natural sensory experience. As shown in Figure 6 and the Appendix, BIR-D exhibits reasonable and well-exposed results.

    &  &  \\   & PSNR & SSIM & LOE & FID & PI & PSNR & SSIM & LOE & FID & PI \\  ExCNet & **16.04** & 0.62 & 220.38 & 111.18 & 8.70 & 16.20 & 0.66 & 225.15 & 115.24 & 8.62 \\ Zero-DCE & 14.91 & **0.70** & 245.54 & 81.11 & 8.84 & **17.84** & **0.73** & 194.10 & 85.72 & 8.12 \\ Zero-DCE++ & 14.86 & 0.62 & 302.06 & 86.22 & 7.08 & 16.12 & 0.45 & 313.50 & 86.96 & 7.92 \\ RRDNet & 11.37 & 0.53 & 127.22 & 89.09 & 8.17 & 13.99 & 0.58 & 94.23 & 83.41 & 7.36 \\ GDP & 13.93 & 0.63 & 110.39 & 75.16 & 6.47 & 13.04 & 0.55 & 79.08 & 78.74 & 6.47 \\  BIR-D & 14.52 & 0.56 & **105.42** & **68.98** & **4.87** & 13.87 & 0.51 & **78.18** & **74.54** & **5.73** \\   

Table 3: **Quantitative comparison among various zero-shot learning methods of low-light enhancement task on LOL  and VE-LOL-L ** Bold font represents the best metric result.

Figure 7: Comparison of image quality for HDR image recovery results on NTIRE .

**HDR Image Recovery.** In the HDR image restoration task, we compared BIR-D with other leading methods, including DeepHDR , AHDRNet , HDR-GAN , and GDP , on the NTIRE2021 Multi-Frame HDR Challenge  dataset. The quantitative and qualitative results are presented in Table 4 and Figure 7, with BIR-D showing the best PSNR and SSIM levels, and successfully generating results with rich and accurate detailed information.

**Motion Blur Reduction.** To evaluate the performance of BIR-D in the motion blur reduction tasks, we compare BIR-D with the state-of-the-art motion blur reduction methods on GoPro dataset  and HIDE dataset . We used the same input image, which also means that the motion blur of the input image is the same, ensuring fairness in comparison. The comparison results of the metrics are presented in Table 4, where BIR-D outperforms existing methods in both PSNR and SSIM. As shown in Figure 8, BIR-D can effectively achieve the elimination of motion blur. The generated images not only achieve a better quality but also receive restoration with more clear details.

**Multi-Degradation Image Restoration.** Encouraged by the excellent restoration performance of BIR-D on single restoration task, we further tested the image restoration performance of BIR-D in solving multi-task image restoration. As shown in Figure 9, we take a degraded image on the ImageNet

    &  &  &  &  \\    & PSNR & SSIM & PSNR & SSIM & & PSNR & SSIM & LPIPS & FID \\  DeepRFT & 33.23 & 0.963 & 31.42 & 0.944 & Deep-HDR & 21.66 & 0.76 & 0.26 & 57.52 \\ MSDL-Net & 33.28 & 0.964 & 31.02 & 0.940 & AHDRNet & 18.72 & 0.58 & 0.39 & 81.98 \\ NAFNet & 33.69 & 0.967 & 31.32 & 0.943 & HDR-GAN & 21.67 & 0.74 & 0.26 & 52.71 \\ UFPNet & 34.06 & **0.968** & 31.74 & 0.947 & GDP & 24.88 & 0.86 & **0.13** & 50.05 \\  BIR-D & **34.12** & **0.968** & **32.09** & **0.948** & BIR-D & **25.03** & **0.88** & 0.16 & **48.74** \\   

Table 4: **Quantitative comparison of motion blur reduction and HDR image recovery tasks.**

Figure 8: Comparison of image quality for motion blur reduction results on GoPro  and HIDE dataset .

dataset where two types of degradation are mixed as an example. The optimizable convolution kernel of BIR-D can also simulate these complicated degradation functions. The generated images obtained have excellent results in both image quality and details.

## 4 Ablation study

**The Effectiveness of Optimizable Convolutional Kernel and Adaptive Guidance Scale.** The ablation studies on the real-time optimizable convolutional kernel parameters and guidance scale were performed to reveal the effectiveness of these settings. We further tested the LOL  and the most challenging LoLi-phone  datasets. Model A fixed the convolutional kernel parameters and guidance scale. Models B and C represent fixed parameters for the convolution kernel and the fixed guidance scale, respectively. As illustrated in Figure 10, the fixed guidance scale with a bias set at \(s=80000\) resulted in the emergence of mineral textures in the images. By contrast, as shown in Table 5, BIR-D outperformed other models in all indicators, demonstrating the effectiveness of an optimizable convolutional kernel and adaptive guidance scale.

**The Effectiveness of the First Stage Pre-training Model.** We conducted further experiments on the deblur task to demonstrate the impact of the first-state pre-training model. As shown in Table 6, for a randomly initialized convolution kernel parameter, all metrics of BIR-D were better than BIR-D without the pre-training model. These results indicate that the first-stage pre-trained model is able to provide better initial state of images for our BIR-D.

## 5 Parameter analysis

**The Parameter Variations of the Optimizable Convolution Kernel and Mask in the Reverse Steps.** In order to visualize the variation trends of the parameters of convolution kernel mask in the reverse process, we conducted experiments on the test set of the LOL dataset from the low-light enhancement task. As shown in Figure 11(a), the mean values of the convolution kernel parameters and degradation mask are given by random initialization and gradually increase with the progress of the time steps. This increase in magnitude is influenced by the gradient of the distance metric with respect to the corresponding parameters. When the sampling step \(t\)<500, the difference between \(_{0}\) and \(y\) changes slightly, resulting in correspondingly smaller gradient values.

    &  &  \\   & PSNR & SSIM & Consistency & FID & PSNR & SSIM & Consistency & FID \\  BIR-D without pre-training model & 25.88 & 0.69 & 40.24 & 2.55 & 21.49 & 0.61 & 53.78 & 4.32 \\  BIR-D & 26.31 & 0.73 & 38.42 & 2.32 & 25.97 & 0.71 & 39.87 & 2.41 \\   

Table 6: **The ablation study on the effectiveness of the pre-training model.**

Figure 9: Results of multi-task image restoration.

BIR-D employs masks in the degradation function with the intent to address the image restoration of local regions characterized by substantial shifts in brightness. Figure 11(b) shows that the mask \(\) of the degradation model has an upward trend from their initial values, making the overall degradation function approach the true degradation. As shown in Figure 12, during the sampling process, the degradation mask learns the detailed information of the image, including local regions with significant brightness differences. This process is obtained by updating the gradient of the distance metric with respect to the degradation mask parameters.

**The Theoretical Analysis of the Changing Trend of Guidance Scale in the Reverse Steps.** We take the variation in the guidance scale of BIR-D on the LOL dataset as an example to analyze the trend of its changes during the reverse steps. As shown in Figure 11(c), the guidance scale gradually decreases with the sampling step, which aligns with the actual situation. When the sampling step \(t\)<500, as \(t\) decreases, the difference between \(x_{t}\) and \(x_{t-1}\) decreases with decreasing \(t\), indicating a reduction in the simulated noise at each step. Therefore, the level of guidance required for each sampling step should also be reduced accordingly, leading to a decrease in the required guidance scale values. According to Equation (3), when step \(t\) is small, the gradient term \(g\) also decreases due to the small change in \(x_{t}\) at each step. The speed of the gradient term decreases is greater than the speed of distance metric decreases, resulting in a decrease in the value of the guidance scale.

## 6 Conclusion

In this paper, we propose Blind Image Restoration Diffusion, which is a unified model that can be used to solve various blind image restoration problems. We utilize optimized convolutional kernels to simulate and update the degradation function in the diffusion step in real time, and derive the empirical formula of the guidance scale in detail, so that it can better utilize the unconditional diffusion model to generate high-quality images. The ability to solve various blind image restoration tasks, including low-light enhancement and motion blur reduction, has also been verified through various indicators of datasets.

Figure 11: Illustration of **(a)** the variation of the mean of optimizable convolutional kernel parameters in each step of the sampling process. **(b)** The variation of the mean of degradation mask in each step of the sampling process. **(c)** The variation of adaptive guidance scale in each step of the sampling process. These experiments are performed on LOL dataset.

Figure 12: The changing of degradation mask during the sampling process in HDR recovery.

Figure 10: Qualitative results when the fixed guidance scale is biased towards a larger value of \(s=80000\).