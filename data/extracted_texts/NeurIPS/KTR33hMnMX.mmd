# Aligning Optimization Trajectories with Diffusion Models for Constrained Design Generation

Giorgio Giannone

Massachusetts Institute of Technology

Technical University of Denmark

giorgio@mit.edu

&Akash Srivastava

MIT-IBM Watson AI Lab

akashsri@mit.edu

&Ole Winther

Technical University of Denmark

University of Copenhagen

olwi@dtu.dk

&Faez Ahmed

Massachusetts Institute of Technology

faez@mit.edu

###### Abstract

Generative models have significantly influenced both vision and language domains, ushering in innovative multimodal applications. Although these achievements have motivated exploration in scientific and engineering fields, challenges emerge, particularly in constrained settings with limited data where precision is crucial. Traditional engineering optimization methods rooted in physics often surpass generative models in these contexts. To address these challenges, we introduce Diffusion Optimization Models (DOM) and Trajectory Alignment (TA), a learning framework that demonstrates the efficacy of aligning the sampling trajectory of diffusion models with the trajectory derived from physics-based iterative optimization methods. This alignment ensures that the sampling process remains grounded in the underlying physical principles. This alignment eliminates the need for costly preprocessing, external surrogate models, or extra labeled data, generating feasible and high-performance designs efficiently. We apply our framework to structural topology optimization, a fundamental problem in mechanical design, evaluating its performance on in- and out-of-distribution configurations. Our results demonstrate that TA outperforms state-of-the-art deep generative models on in-distribution configurations and halves the inference computational cost. When coupled with a few steps of optimization, it also improves manufacturability for out-of-distribution conditions. DOM's efficiency and performance improvements significantly expedite design processes and steer them toward optimal and manufacturable outcomes, highlighting the potential of generative models in data-driven design.

## 1 Introduction

The remarkable progress in large vision [27; 20; 43; 85] and language models [30; 21; 78] has ushered in unparalleled capabilities for processing unstructured data, leading to innovations in multimodal and semantic generation [79; 67; 18; 80]. This momentum in model development has influenced the rise of Deep Generative Models (DGMs) in the realms of science [51; 61] and engineering [84; 101], especially in constraint-bound problems like structural Topology Optimization (TO ), offering potential to make the design process faster.

Many engineering design applications predominantly rely on _iterative_ optimization algorithms. These algorithms break down physical and chemical phenomena into discrete components and incrementally enhance design performance while ensuring all constraint requirements are met. As an example, topology optimization aims to determine the optimal material distribution within a given design space,under specified loads and boundary conditions, to achieve the best performance according to a set of defined criteria, such as minimum weight or maximum stiffness. Traditional iterative methods like SIMP  are invaluable but grapple with practical challenges, especially for large-scale problems, owing to their computational complexity.

Recent advancements have sought to address these challenges by venturing into learning-based methods for topology optimization, specifically DGMs. These models, trained on datasets comprising optimal solutions across varying constraints, can either speed up or even substitute traditional optimization processes. They also introduce a diverse set of structural topologies by tapping into large datasets of prior designs. The dual advantage of generating a variety of solutions and accommodating numerous design variables, constraints, and goals renders these learning-based methodologies especially enticing for engineering design scenarios.

However, purely data-driven approaches in generative design often fall short when benchmarked against optimization-based methods. While data-driven techniques might prioritize metrics such as reconstruction quality, these metrics might not adequately gauge adherence to engineering performance and constraint requirements. The core of this challenge lies in the data-driven methods' omission of vital physical information and their inability to incorporate iterative optimization details during inference. This oversight can compromise the quality of the solutions, particularly when navigating complex constraints and stringent performance requirements. Consequently, there's an emerging need for methodologies that amalgamate the strengths of both data-centric and physics-informed techniques to more adeptly navigate engineering applications. Paving the way forward, structured generative models have made notable strides in this domain, exemplified by innovative techniques like TopologyGAN  and TopoDiff .

**Limitations.** Structured generative models, despite their recent advancements in engineering designs, grapple with some pressing challenges. For one, these models often require additional supervised training data to learn guidance mechanisms that can improve performance and manufacturability . In the context of Diffusion Models , executing forward simulations is not a one-off task; it requires repetition, often in the order of tens to hundreds of times, to derive an appropriate topology . Moreover, integrating physical data into these models isn't straightforward. It demands a time-consuming FEA (Finite Element Analysis) preprocessing step during both the training and inference phases, which is pivotal for computing critical parameters like stress and energy fields. As a result, the sampling process is slow, and the inference process is computationally expensive. Such overheads challenge the scalability and adaptability of these models. Consequently, the touted benefits of data-driven techniques, notably fast sampling and quick design candidate generation, are somewhat diminished.

**Proposed Solution.** We introduce a conditional diffusion model that synergistically combines data-driven and optimization-based techniques. This model is tailored to learn constrained problems and generate candidates in the engineering design domains (refer to Fig. 3). Instead of relying on computationally heavy physics-based exact solutions using FEM, our method employs cost-effective physics-informed approximations to manage sparsity in conditioning constraints. We introduce a

Figure 1: Trajectory Alignment. Intermediate sampling steps in a Diffusion Optimization Model are matched with intermediate optimization steps. In doing so, the sampling path is biased toward the optimization path, guiding the data-driven path toward physical trajectories. This leads to significantly more precise samples.

Trajectory Alignment (TA) mechanism in the training phase that allows the model to leverage the information in the trajectory that was used by an iterative optimization-based method in the training data, drastically cutting down the sampling steps required for a solution generation using diffusion models. Moreover, our framework can amplify performance and manufacturability in complex problems by integrating a few steps of direct optimization. This method strikes a balance between computational efficiency and precision, offering adaptability to novel design challenges. By bridging the gap between generative modeling and engineering design optimization, our solution emerges as a powerful tool for tackling complex engineering problems. At its core, our findings highlight the value of diffusion models benefitting from optimization methods' intermediate solutions, emphasizing the journey and not just the final outcome.

**Contribution.** Our contributions are the following:

1. We introduce the _Diffusion Optimization Models_ (DOM), a versatile and efficient approach to incorporate performance awareness in generative models of engineering design problems while respecting constraints. The primary objective of DOM is to generate high-quality candidates rapidly and inexpensively, with a focus on topology optimization (TO) problems. DOM consists of * _Trajectory Alignment_ (TA) leverages iterative optimization and hierarchical sampling to match diffusion and optimization paths, distilling the optimizer knowledge in the sampling process. As a result, DOM achieves high performance without depending on FEM solvers or guidance and can sample high-quality configurations in as few as two steps. * _Dense Kernel Relaxation_, an efficient mechanism to relieve inference from expensive FEM pre-processing and * _Few-Steps Direct Optimization_ that improves manufacturability using a few optimization steps.
2. We perform extensive quantitative and qualitative evaluation in- and out-of-distribution, showing how kernel relaxation and trajectory alignment are both necessary for good performance and fast, cheap sampling. We also release a large, _multi-fidelity dataset_ of sub-optimal and optimal topologies obtained by solving minimum compliance optimization problems. This dataset contains low-resolution (64x64), high-resolution (256x256), optimal (120k), and suboptimal (600K) topologies. To our knowledge, this is the first large-scale dataset of optimized designs that also provides intermediate suboptimal iterations.

## 2 Background

Here we briefly introduce the Topology Optimization problem , diffusion models [105; 44; 98], a class of deep generative models, conditioning and guidance mechanisms for diffusion models, and deep generative models for topology optimization [69; 62]. For more related work see Appendix A.

The Topology Optimization Problem.Topology optimization is a computational design approach that aims to determine the optimal arrangement of a structure, taking into account a set of constraints. Its objective is to identify the most efficient utilization of material while ensuring the structure meets specific performance requirements. One widely used method in topology optimization is the Solid Isotropic Material with Penalization (SIMP) method . The SIMP method employs a density field to model the material properties, where the density indicates the proportion of material present in a particular region. The optimization process involves iteratively adjusting the density field, considering constraints such as stress or deformation. In the context of a mechanical system, a common objective is to solve a generic minimum compliance problem. This problem aims to find the distribution of material density, represented as \(^{n}\), that minimizes the deformation of the structure under prescribed boundary conditions and loads . Given a set of design variables \(=\{_{i}\}_{i=0}^{n}\), where

Figure 2: In topology optimization, the objective is to find the design with minimum compliance under given loads, boundary conditions, and volume fractions.

\(n\) is the domain dimensionality, the minimum compliance problems can be written as:

\[_{}& c()=F^{T}U( )\\ & v()=v^{T}<\\ & 0 1\] (1)

The goal is to find the design variables that minimize compliance \(c()\) given the constraints. \(F\) is the tensor of applied loads and \(U()\) is the node displacement, solution of the equilibrium equation \(K()U()=F\) where \(K()\) is the stiffness matrix and is a function of the considered material. \(v()\) is the required volume fraction. The problem is a relaxation of the topology optimization task, where the design variables are continuous between 0 and 1. One significant advantage of topology optimization is its ability to create optimized structures that meet specific performance requirements. However, a major drawback of topology optimization is that it can be computationally intensive and may require significant computational resources. Additionally, some approaches to topology optimization may be limited in their ability to generate highly complex geometries and get stuck in local minima.

**Diffusion Models.** Let \(_{0}\) denote the observed data \(_{0}^{D}\). Let \(_{1},...,_{T}\) denote \(T\) latent variables in \(^{D}\). We now introduce, the _forward or diffusion process_\(q\), the _reverse or generative process_\(p_{}\), and the objective \(L\). The forward or diffusion process \(q\) is defined as : \(q(_{1:T}|_{0})=q(_{1}|_{0})_{t=2 }^{T}q(_{t}|_{t-1})\). The beta schedule \(_{1},_{2},...,_{T}\) is chosen such that the final latent image \(_{T}\) is nearly Gaussian noise. The generative or inverse process \(p_{}\) is defined as: \(p_{}(_{0},_{1:T})=p_{}(_{0}|_{1})p(_{T})_{t=2}^{T}p_{}(_{t-1}|_{t})\). The neural network \(_{}(_{t},t)\) is shared among all time steps and is conditioned on \(t\). The model is trained with a re-weighted version of the ELBO that relates to denoising score matching . The negative ELBO \(L\) can be written as:

\[_{q}[-(_{0},_{1:T})}{q (_{1:T}|_{0})}]=L_{0}+_{t=2}^{T}L_{t-1}+L_{T},\] (2)

where \(L_{0}=_{q(_{1}|_{0})}[- p(_ {0}|_{1})]\) is the likelihood term (parameterized by a discretized Gaussian distribution) and, if \(_{1},..._{T}\) are fixed, \(L_{T}=[q(_{T}|_{0}),p(_{T})]\) is a constant. The terms \(L_{t-1}\) for \(t=2,...,T\) can be written as: \(L_{t-1}=_{q(_{t}|_{0})}[[q( _{t-1}|_{t},_{0}) p(_{t-1}| _{t})]]\). The terms \(L_{1:T-1}\) can be rewritten as a prediction of the noise \(\) added to \(\) in \(q(_{t}|_{0})\). Parameterizing \(_{}\) using the noise prediction \(_{}\), we can write

\[L_{t-1,}()=_{q()}[w_{t}\|_{ }(_{t}(_{0},))-\|_{2}^{2}],\] (3)

where \(w_{t}=^{2}}{2_{t}^{2}_{t}(1-_{t})}\), which corresponds to the ELBO objective .

**Conditioning and Guidance.** Conditional diffusion models have been adapted for constrained engineering problems with performance requirements. TopoDiff  proposes to condition on loads, volume fraction, and physical fields similarly to  to learn a constrained generative model. In particular, the generative model can be written as:

\[p_{}(_{t-1}|_{t},,)= (_{t-1};_{}(_{t},)+_{p=1}^{P} _{p},\ ),\] (4)

where \(\) is a conditioning term and is a function of the loads \(l\), volume fraction \(v\), and fields \(f\), i.e \(=h(l,v,f)\). The fields considered are the Von Mises stress \(_{vm}=^{2}-_{11}_{22}+_{22}^{2}+3 _{12}^{2}}\) and the strain energy density field \(W=(_{11}_{11}+_{22}_{22}+2_{12}_{12} )/2.\) Here \(_{ij}\) and \(_{ij}\) are the stress and energy components over the domain. \(\) is a guidance term, containing information to guide the sampling process toward regions with low floating material (using a classifier and \(_{fm}\)) and regions with low compliance error, where the generated topologies are close to optimized one (using a regression model and \(_{c}\)). Where conditioning \(\) is always present and applied during training, the guidance mechanism \(\) is optional and applied only at inference time.

**TopoDiff Limitations.** TopoDiff is effective at generating topologies that fulfill the constraints and have low compliance errors. However, the generative model is expensive in terms of sampling time, because we need to sample tens or hundreds of layers for each sample. Additionally, given the model conditions on the Von Mises stress and the strain energy density, for each configuration of loads and boundary conditions, we have to preprocess the given configurations running a FEM solver. This, other than being computationally expensive and time-consuming, relies on fine-grained knowledge of the problem at hand in terms of material property, domain, and input to the solver and performance metrics, limiting the applicability of such modeling techniques for different constrained problems in engineering or even more challenging topology problems. The guidance requires the training of two additional models (a classification and a regression model) and is particularly useful with out-of-distribution configurations. However such guidance requires additional topologies, optimal and suboptimal, to train the regression model, assuming that we have access to the desired performance metric on the train set. Similarly for the classifier, where additional labeled data has to be gathered.

## 3 Method

To tackle such limitations, we propose _Diffusion Optimization Models_ (DOM), a conditional diffusion model to improve constrained design generation. One of our main goals is to improve inference time without loss in performance and constraint satisfaction. DOM is based on three main components: (i) Trajectory Alignment (Fig. 1 and Fig. 4) to ground sampling trajectory in the underlying physical process; (ii) Dense Kernel Relaxation (Fig. 5) to make pre-processing efficient; and (iii) Few-Steps Direct Optimization to improve out-of-distribution performance (Fig. 3 for an overview). See appendix C for algorithms with and without trajectory alignment.

**Empirical Methodology.** We would like to highlight that our primary contribution, trajectory alignment, is predominantly empirical. While we do make assumptions about the optimization and sampling trajectory and utilize TA, we have not established a comprehensive theoretical framework ensuring convergence of the regularized loss to the score optimized by a diffusion model. Nonetheless, our empirical findings provide evidence for the convergence of the sampling process to the desired solutions.

**Trajectory Alignment (TA).** Our goal is to align the sampling trajectory with the optimization trajectory, incorporating optimization in data-driven generative models by leveraging the hierarchical

Figure 4: Distance between intermediate sampling steps in DOM and optimization steps with and without Trajectory Alignment. Given a random sampling step \(t\) and the corresponding optimization step \(s(t)=(t,n)\) where \(n\). We compute the matching in clean space, using the approximate posterior \(q\) to obtain an estimate for \(^{g}\) given \(_{t}\) and the noise prediction \(_{}\). Then we compute the distance \(||}^{}(_{t},_{})-_{ (t)}^{opt}||_{2}\).

Figure 3: The DOM pipeline with conditioning and kernel relaxation (top left) and trajectory alignment (top right). The Diffusion Optimization Model generates design candidates, which are further refined using optimization tools. After the generation step (left side), we can improve the generated topology using a few steps of SIMP (5/10) to remove floating material and improve performance (right side). See Appendix 10 for a comparison of the inference process for DOM and TopoDiff .

sampling structure of diffusion models. This aligns trajectories with physics-based information, as illustrated in Fig. 1a. Unlike previous approaches, which use optimization as pre-processing or post-processing steps, trajectory alignment is performed during training and relies upon the marginalization property of diffusion models, i.e., \(q(_{t}|_{0})= q(_{1:t}|_{0})d _{1:t-1}\), where \(_{t}=_{t}}_{0}+(1-_{t})\)\(\), with \( N(0,I)\). The trajectory alignment process can match in clean space (matching step \(0\)), noisy space (matching step \(t\)), performance space, and leverage multi-fidelity mechanisms. At a high level, TA is a regularization mechanism that injects an optimization-informed prior at each sampling step, forcing it to be close to the corresponding optimization step in terms of distance. This process provides a consistency mechanism  over trajectories and significantly reduces the computational cost of generating candidates without sacrificing accuracy.

**Alignment Challenges.** The alignment of sampling and optimization trajectories is challenging due to their differing lengths and structures. For example, the optimization trajectory starts with an image of all zeros, while the sampling path starts with random noise. Furthermore, Diffusion Models define a Stochastic Differential Equation (SDE, ) in the continuous limit, which represents a collection of trajectories, and the optimization trajectory cannot be directly represented within this set. To address these issues, trajectory alignment comprises two phases (see Figure 1b): a search phase and a matching phase. In the search phase, we aim to find the closest trajectory, among those that can be represented by the reverse process, to the optimization trajectory. This involves identifying a suitable representation over a trajectory that aligns with the optimization process. In the matching phase, we minimize the distance between points on the sampling and optimization trajectories to ensure proximity between points and enable alignment between trajectories.

**Trajectory Search.** We leverage the approximate posterior and marginalization properties of diffusion models to perform a trajectory search, using the generative model as a parametric guide to search for a suitable representation for alignment. Given an initial point \(_{0}\), we obtain an approximate point \(_{t}\) by sampling from the posterior distribution \(q(_{t}|_{0})\). We then predict \(_{}(_{t})\) with the model and use it to obtain \(}^{}(_{t},_{}(_{t}))\). In a DDPM, \(}^{}\) is an approximation of \(_{0}\) and is used as an intermediate step to sample \(_{t-1}^{}\) using the posterior functional form \(q(_{t-1}^{}|_{t},}^{})\). In DOM, we additionally leverage \(}^{}\) to transport the sampling step towards a suitable representation for matching an intermediate optimization step \(_{step(t)}^{opt}\) corresponding to \(t\) using some mapping. Trajectory alignment involves matching the optimization trajectory, which is an iterative exact solution for physics-based problems, with the sampling trajectory, which is the hierarchical sampling mechanism leveraged in Diffusion Models  and Hierarchical VAEs . In practice, in DOM we sample \(_{t}=_{t}}_{0}+(1-_{t})\) from \(q(_{t}|_{0})\) and run a forward step with the inverse process \(_{}(_{t},)\) conditioning on the constraints \(\) to obtain the matching representation \(}^{}\) for step \(t\):

\[}^{}  q(}^{}|^{}( _{t},_{}),)\] (5) \[^{}(_{t},_{}) =(_{t}-_{t}}\;_{}( _{t},))/_{t}}.\]

**Trajectory Matching.** Then we match the distribution of matching representation \(q(}^{}|_{t},_{})\) for sampling step \(t\) with the distribution of optimized representations \(q(_{s(t-1)}^{opt}|)\) at iteration \(s\) (corresponding to step \(t-1\)) conditioning on the optimizer \(S\). In general, given that the sampling steps will be different than the optimization steps, we use \(s(t)-1=n_{s}(1-t/T)\), where \(n_{s}\) is the number of optimized iterations stored, and then select the integer part. 1 We then can train the model as a weighted sum of the conditional DDPM objective and the trajectory alignment regularization:

\[_{}=_{q(_{t}|_{0})} [q(_{t-1}|_{t},_{0}) p_{}( _{t-1}^{}|_{t},)]+[q(}^{}|_{t},_{}) q(_{s(t- 1)}|)].\] (6)

This mechanism effectively pushes the sampling trajectory at each step to match the optimization trajectory, distilling the optimizer during the reverse process training. In practice, following practice

Figure 5: Comparison of iterative (left), sparse (center), and dense single-step (right) conditioning fields for a Constrained Diffusion Model. Unlike the expensive iterative FEA method, the physics-inspired fields offer a cost-effective, single-step approximation that’s domain-agnostic and scalable.

in DDPM literature, the distribution variances are not learned from data. For the trajectory alignment distributions, we set the dispersion to the same values used in the model. By doing so we can rewrite the per-step negated lower-bound as a weighted sum of squared errors:

\[_{}=_{q()}[w_{t}|| _{}(_{t}(_{0},),)- ||_{2}^{2}]}_{L_{t-1,}(,)}+ ||}^{}(_{t},_ {})-_{s(t-1)}^{opt}||_{2}^{2}}_{_{clean}^{}}\] (7)

where \(_{clean}^{}\) is the trajectory alignment loss for step \(t\), and \(L_{t-1,}(,)\) is a conditional DDPM loss for step \(t\). This is the formulation employed for our model, where we optimize this loss for the mean values, freeze the mean representations, and optimize the variances in a separate step . Alignment can also be performed in alternative ways. We can perform matching in noisy spaces, using the marginal posterior to obtain a noisy optimized representation for step \(t-1\), \(q(_{t-1}^{opt}|_{s(0)}^{opt})\) and then optimize \(_{noisy}^{}=_{n}||_{t-1}^{}- _{t-1}^{opt}||_{2}^{2}\). Finally, we can match in performance space: this approach leverages an auxiliary model \(f_{}\) similar to (consistency models) and performs trajectory alignment in functional space, \(_{perf}^{}=_{p}||f_{}(_{t-1}^{} )-P_{s(t-1)}||_{2}\), where we match the performance for the generated intermediate design with the ground truth intermediate performance \(P_{s(t-1)}\) for the optimized \(_{s(t-1)}^{opt}\). We compare these and other variants in Table 6.

Dense Conditioning over Sparse Constraints.All models are subject to conditioning based on loads, boundary conditions, and volume fractions. In addition, TopoDiff and TopoDiff-GUIDED undergo conditioning based on force field and energy strain, while TopoDiff-FF and DOM are conditioned based on a dense kernel relaxation, inspired by Green's method [36; 34], which defines integral functions that are solutions to the time-invariant Poisson's Equation [33; 41]. More details are in Appendix D. The idea is to use the kernels as approximations to represent the effects of the boundary conditions and loads as smooth functions across the domain (Fig. 5). This approach avoids the need for computationally expensive and time-consuming Finite Element Analysis (FEA) to provide conditioning information. For a load or source \(l\), a sink or boundary \(b\) and \(r=||-_{l}||_{2}=_{i}-_{i}^{l})^ {2}+(_{j}-_{j}^{l})^{2}}\), we have:

\[ K_{l}(,_{l};)& =_{l=1}^{L}(1-e^{-/||-_{l}||_{2}^{2} })\ (_{l})\\ K_{b}(,_{b};)&=_{b=1} ^{B}e^{-/||-_{b}||_{2}^{2}}/_{}( _{b=1}^{B}e^{-/||-_{b}||_{2}^{2}}).\] (8)

where \(\) is the module of a generic force in 2D. Notice how, for \(r 0\), \(K_{l}(,_{l}) p\), and \(r\), \(K_{l}(,_{l}) 0\). We notice how closer to the boundary the kernel is null, and farther from the boundary the kernel tends to 1. Note that the choice of \(\) parameters in the kernels affects the smoothness and range of the kernel functions. Furthermore, these kernels are isotropic, meaning that they do not depend on the direction in which they are applied. Overall, the kernel relaxation method offers a computationally inexpensive way to condition generative models on boundary conditions and loads, making them more applicable in practical engineering and design contexts.

Few-Steps Direct Optimization.Finally, we leverage direct optimization to improve the data-driven candidate generated by DOM. In particular, by running a few steps of optimization (5/10) we can inject physics information into the generated design directly, greatly increasing not only performance but greatly increasing manufacturability. Given a sample from the model \(}_{0} p_{}(_{0}|_{1})p_{ }(_{1:T})\), we can post-process it and obtain \(_{0}=(}_{0}^{},n)\) an improved design leveraging \(n\) steps of optimization, where \(n\). In Fig. 3 we show a full pipeline for DOM.

## 4 Experiments

Our three main objectives are: **(1)** Improving inference efficiency, and reducing the sampling time for diffusion-based topology generation while still satisfying the design requirements with a minimum decrease in performance. **(2)** Minimizing reliance on force and strain fields as conditioning information, reducing the computation burden at inference time and the need for ad-hoc conditioning mechanisms for each problem. **(3)** Merging together learning-based and optimization-based methods, refining the topology generated using a conditional diffusion model, and improving the final solution in terms of manufacturability and performance.

**Setup.** We train all the models for 200k steps on 30k optimized topologies on a 64x64 domain. For each optimized topology, we have access to a small subset (5 steps) of intermediate optimization steps. We set the hyperparameters, conditioning structure, and training routine as proposed in . Appendix G for more details. For all the models (Table 1) we condition on volume fraction and loads. For TopoDiff, we condition additional stress and energy fields. For TopoDiff-FF , a variant of TopoDiff conditioning on a kernel relaxation, we condition on boundary conditions and kernels. TopoDiff-GUIDED leverages a compliance regressor and floating material classifier guidance. We use a reduced number of sampling steps for all the experiments.

**Dataset.** We use a dataset of optimized topologies gathered using SIMP as proposed in . Together with the topologies, the dataset contains information about optimal performance. For each topology, we have information about the loading condition, boundary condition, volume fraction, and optimal compliance. Additionally, for each constraint configuration, a pre-processing step computes the force and strain energy fields (see Fig. 5) when needed. Appendix F for more details on the dataset and visualizations.

**Evaluation.**

We evaluate the model using engineering and generative metrics. In particular, we consider metrics that evaluate how well our model fulfills: physical constraints using error wrt prescribed Volume Fraction (VFE); engineering constraints, as manufacturability as measured by Floating Material (FM); performance constraints, as measured by compliance error (CE) wrt the optimized SIMP solution; sampling time constraints (inference constraints) as measure by sampling time (inference and pre-processing). We consider two scenarios of increasing complexity: **(i)** In-distribution Constraints. The constraints in this test set are the same as those of the training set. When measuring performance on this set, we filter generated configurations with high compliance. **(ii)** Out-of-distribution Constraints. The constraints in this test set are different from those of the training set. When measuring performance on this set, we filter generated configurations with high compliance. The purpose of these tasks is to evaluate the generalization capability of the machine learning models in- and out-of-distribution. By testing the models on different test sets with varying levels of difficulty, we can assess how well the models can perform on new, unseen data. More importantly, we want to understand how important the role of the force field and energy strain is with unknown constraints.

**In-Distribution Constraints.** Table 3 reports the evaluation results in terms of constraints satisfaction and performance for the task of topology generation. In Table 2 we report metrics commonly employed to evaluate the quality of generative models in terms of fidelity (IS, sFID, P) and diversity (R). We see how such metrics are all close and it is challenging to gain any understanding just by relying on classic generative metrics when evaluating constrained design generation. These results justify the need for an evaluation that considers the performance and feasibility of the generated design.

   & w/ COND & w/o FEM & w/o GUID \\  TopologyGAN  & ✓ & ✗ & ✗ \\ TopoDiff  & ✓ & ✗ & ✓ \\ TopoDiff-G  & ✓ & ✗ & ✗ \\  DOM (ours) & ✓ & ✓ & ✓ \\  

Table 1: Comparative study of generative models in topology optimization considering factors like conditional input (COND), finite element method (FEM), and guidance (GUID). Unlike others, the DOM model operates without FEM preprocessing or GUIDENCE. More visualizations and optimization trajectories are in the Appendix.

Figure 6: Few-Step sampling for Topology generation. Top row: Diffusion Optimization Model (DOM) with Trajectory Alignment. Middle row: TopoDiff-GUIDED. Bottom row: The optimization result. DOM produces high-quality designs in as few as two steps, greatly enhancing inference efficiency compared to previous models requiring 10-100 steps. Trajectory Alignment helps DOM generate near-optimal geometries swiftly, improving few-step sampling in conditional diffusion models for topology optimization. See appendix Fig. 13 for more examples.

In Table 3 DOM achieves high performance and is at least 50 % less computationally expensive at inference time, not requiring FEM preprocessing or additional guidance through surrogate models like TopologyGAN and TopoDiff. We also compare with Consistency Models , a Diffusion Model that tries to predict its input at each step. DOM can be seen as a generalization of such a method when a trajectory is available as a ground truth. Overall, DOM with Trajectory Alignment is competitive or better than the previous proposal in terms of performance on in-distribution constraints, providing strong evidence that TA is an effective mechanism to guide the sampling path toward regions of high performance.

Generation with Few-Steps of Sampling.Table 4 compares two different algorithms, TopoDiff-GUIDED and DOM, in terms of their performance when using only a few steps for sampling. The table shows the results of the in and out-of-distribution comparison, with TopoDiff-G and DOM both having STEPS values of 2, 5, and 10, and SIZE of 239M and 121M. We can see that DOM outperforms by a large margin TopoDiff-G when tasked with generating a new topology given a few steps, corroborating our hypothesis that aligning the sampling and optimization trajectory is an effective mechanism to obtain efficient generative models that satisfy constraints. DOM outperforms

    & STEPS & SIZE & AVG \% CE \(\) & MDN \% CE \(\) & \% VFE \(\) & \% FM \(\) & INF (s) \(\) & \% UNS \(\) & \% LD \(\) \\ _in-distro_ & & & & & & & & & \\  TopoDiff-G & 2 & 239M & 681.53 & 436.83 & 80.98 & 98.72 & 3.36 & **2.00** & 15.92 \\ DOM (ours) & 2 & 121M & **22.66** & **1.46** & **3.34** & **33.25** & **0.17** (- 94.94 \%) & 2.11 & **0.00** \\  TopoDiff-G & 5 & 239M & 43.27 & 15.48 & 2.76 & 77.65 & 3.43 & **1.44** & **0.00** \\ DOM (ours) & 5 & 121M & **11.99** & **0.72** & **2.27** & **20.08** & **0.24** (- 93.00 \%) & 2.77 & **0.00** \\  TopoDiff-G & 10 & 239M & 6.43 & 1.61 & 1.95 & 20.55 & 3.56 & **0.00** & **0.00** \\ DOM (ours) & 10 & 121M & **4.44** & **0.57** & **1.67** & **11.94** & **0.35** (- 90.17 \%) & **0.00** & **0.00** \\    & & & & & & \\  TopoDiff-G & 2 & 239M & 751.17 & 548.26 & 81.46 & 100.00 & 3.36 & **1.90** & 16.48 \\ DOM (ours) & 2 & 121M & **79.66** & **10.37** & **3.69** & **44.20** & **0.17** (- 94.94 \%) & 2.80 & **0.00** \\  TopoDiff-G & 5 & 239M & 43.50 & 19.24 & 2.58 & 79.57 & 3.43 & 2.20 & **0.00** \\ DOM (ours) & 5 & 121M & **38.97** & **5.49** & **2.56** & **26.70** & **0.24** (- 93.00 \%) & **1.40** & **0.00** \\  TopoDiff-G & 10 & 239M & **10.78** & **2.55** & 1.87 & 21.36 & 3.56 & 2.10 & **0.00** \\ DOM (ours) & 10 & 121M & 32.19 & 3.69 & **1.78** & **14.20** & **0.35** (- 90.17 \%) & **0.40** & **0.00** \\   

Table 2: Generative metrics on in-distribution metrics. Precision denotes the fraction of generated topologies that are realistic, and recall measures the fraction of the training data manifold covered by the model.

    & STEPS & CONTRAINTS & AVG \% CE \(\) & MDN \% CE \(\) & \% VFE \(\) & \% FM \(\) & INFREENCE (s) \(\) \\  TopologyGAN  & 1 & FIELD & 48.51 & 2.06 & 11.87 & 46.78 & 3.37 \\ Conditional DDPM  & 100 & RAW & 60.79 & 3.15 & 1.72 & 8.72 & **2.23** \\ Consistency Model  & 100/1 & KERNEL & 10.30 & 2.20 & 1.64 & 8.72 & 2.35 \\ TopoDiff-FF  & 100 & KERNEL & 24.90 & 1.92 & 2.05 & 8.15 & 2.35 \\ TopoDiff  & 100 & FIELD & 5.46 & 0.80 & **1.47** & **5.79** & 5.54 \\ TopoDiff-GUIDED  & 100 & FIELD & 5.93 & 0.83 & 1.49 & 5.82 & 5.77 \\  DOM w/o TA (ours) & 100 & KERNEL & 13.61 & 1.79 & 1.86 & 7.44 & 2.35 \\ DOM w/ TA (ours) & 100 & KERNEL & **4.44** & **0.74** & 1.52 & 6.72 & 2.35 \\   

Table 3: Evaluation of different model variants on in-distribution constraints. CE: Compliance Error. VFE: Volume Fraction Error. FM: Floating Material. We use 100 sampling steps for all diffusion models. We can see that DOM w/ TA is competitive with the SOTA on topology generation, being computationally 50 % less expensive at inference time compared to TopoDiff. Trajectory Alignment greatly improves performance without any additional inference cost. See appendix Fig. 11 for confidence intervals.

TopoDiff-GUIDED even being 50 % smaller, without leveraging an expensive FEM solver for conditioning but relying on cheap dense relaxations, making it 20/10 times faster at sampling, and greatly enhancing the quality of the generated designs, providing evidence that Trajectory Alignment is an effective mechanism to distill information from the optimization path. In Fig. 6, we provide qualitative results to show how DOM (top row) is able to generate reasonable topologies, resembling the fully optimized structure running SIMP for 100 steps (bottom row), with just two steps at inference time, where the same model without TA or a TopoDiff are not able to perform such task. Overall these results corroborate our thesis regarding the usefulness of trajectory alignment for high-quality constrained generation. For more experiments on inference see Appendix B.

**Merging Generative Models and Optimization for Out-of-Distribution Constraints.** Table 5 shows the results of experiments on out-of-distribution constraints. In this scenario, employing FEM and guidance significantly enhances the performance of TopoDiff. Conditioning on the FEM output during inference can be seen as a form of test-time conditioning that can be adapted to the sample at hand. However, merging DOM and a few iterations of optimization is extremely effective in solving this problem, in particular in terms of improving volume fraction and floating material. Using the combination of DOM and SIMP is a way to impose the performance constraints in the model without the need for surrogate models or guidance.

**Trajectory Alignment Ablation.** The core contribution of DOM is trajectory alignment, a method to match sampling and optimization trajectories of arbitrary length and structure mapping intermediate steps to appropriate CLEAN (noise free or with reduced noise using the model and the marginalization properties of DDPM) representations. However, alignment can be performed in multiple ways, leveraging NOISY representation, matching performance (PERF), and using data at a higher resolution to impose consistency (MULTI). In Table 6 we perform an ablation study, considering DOM with and without kernel relaxation, and leveraging different kinds of trajectory matching. From the table, we see that using dense conditioning is extremely important for out-of-distribution performance, and that matching using CLEAN is the most effective method in and out-of-distribution. In Fig. 4 we report a visualization of the distance between sampling and optimization trajectory during training. From this plot, we can see how the kernel together with TA helps the model to find trajectories that are closer to the optimal one, again corroborating the need for dense conditioning and consistency regularization.

## 5 Conclusion

We presented Diffusion Optimization Models, a generative framework to align the sampling trajectory with the underlying physical process and learn an efficient and expressive generative model for constrained engineering design. Our work opens new avenues for improving generative design in engineering and related fields. However, our method is limited by the capacity to store and retrieve intermediate optimization steps, and, without a few steps of optimization, it underperforms out-of-distribution compared to FEM-conditional and guided methods.

    & KERNEL & TA & MODE & IN-DISTRO & OUT-DISTRO \\  DOM & ✗ & ✗ & - & 3.29 & 8.05 \\ DOM & ✗ & ✓ & CLEAN & 1.11 & 9.01 \\ CM & ✓ & ✗ & - & 2.20 & 5.25 \\ DOM & ✓ & ✗ & - & 1.80 & 5.62 \\ DOM & ✓ & ✓ & MULTI & 34.95 & 54.73 \\ DOM & ✓ & ✓ & NOISY & 2.08 & 6.23 \\ DOM & ✓ & ✓ & PERF & 2.41 & 6.82 \\ DOM & ✓ & ✓ & CLEAN & **0.74** & **3.47** \\   

Table 6: Ablation study with and without kernel and trajectory alignment. We explore different ways to match the sampling and optimization trajectory and we measure the Median Compliance Error. TA: trajectory alignment. CM: Consistency Models .

    & STEPS & MDN \% CE \(\) & \% VFE \(\) & \% FM \(\) \\  TopoDiff-FF & 100 & 16.06 & 1.97 & 8.38 \\ TopoDiff-G & 100 & 1.82 & 1.80 & 6.21 \\  DOM & 100 & 3.47 & 1.59 & 8.02 \\ DOM + SIMP & 100+5 & 1.89 & 1.77 & 10.19 \\ DOM + SIMP & 100+10 & **1.15** & **1.10** & **2.61** \\   

Table 5: Out-of-Distribution Scenario Comparison: TopoDiff-G outperforms DOM due to its adaptive conditioning mechanism, which leverages expensive FEM-computed fields. However, DOM coupled with a few steps of direct optimization (5/10) greatly surpasses TopoDiff in performance and manufacturability. This underscores the effectiveness of integrating data-driven and optimization methods in constrained design creation.