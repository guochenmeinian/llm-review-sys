# Adversarial Environment Design via Regret-Guided Diffusion Models

Hojun Chung\({}^{1}\), Junseo Lee\({}^{1}\), Minoso Kim\({}^{1}\), Dohyeong Kim\({}^{2}\), and Songhwai Oh\({}^{1,2,}\)

\({}^{1}\)Interdisciplinary Program in Artificial Intelligence and ASRI, Seoul National University

\({}^{2}\)Department of Electrical and Computer Engineering and ASRI, Seoul National University

{hojun.chung, junseo.lee, minsoo.kim, dohyeong.kim}@rllab.snu.ac.kr, songhwai@snu.ac.kr

Corresponding author: Songhwai Oh

###### Abstract

Training agents that are robust to environmental changes remains a significant challenge in deep reinforcement learning (RL). Unsupervised environment design (UED) has recently emerged to address this issue by generating a set of training environments tailored to the agent's capabilities. While prior works demonstrate that UED has the potential to learn a robust policy, their performance is constrained by the capabilities of the environment generation. To this end, we propose a novel UED algorithm, adversarial environment design via regret-guided diffusion models (ADD). The proposed method guides the diffusion-based environment generator with the regret of the agent to produce environments that the agent finds challenging but conducive to further improvement. By exploiting the representation power of diffusion models, ADD can directly generate adversarial environments while maintaining the diversity of training environments, enabling the agent to effectively learn a robust policy. Our experimental results demonstrate that the proposed method successfully generates an instructive curriculum of environments, outperforming UED baselines in zero-shot generalization across novel, out-of-distribution environments. Project page: https://rllab-snu.github.io/projects/ADD

## 1 Introduction

Deep reinforcement learning (RL) has achieved great success in various challenging domains, such as Atari , GO , and real-world robotics tasks [3; 4]. Despite the progress, the deep RL agent struggles with the generalization problem; it often fails in unseen environments even with a small difference from the training environment distribution [5; 6]. To train well-generalizing policies, various prior works have used domain randomization (DR) [7; 8; 9], which provides RL agents with randomly generated environments. While DR enhances the diversity of the training environments, it requires a large number of trials to generate meaningful structures in high-dimensional domains. Curriculum reinforcement learning [10; 11] has been demonstrated to address these issues by providing instructive sequences of environments. Since manually designing an effective curriculum for complicated tasks is challenging, prior works [12; 13] focus on generating curricula that consider the current agent's capabilities. Recently, unsupervised environment design (UED, ) has emerged as a scalable approach, notable for its advantage of requiring no prior knowledge. UED algorithms alternate between training the policy and designing training environments that maximize the regret of the agent. This closed-loop framework ensures the agent learns a minimax regret policy , assuming that the two-player game between the agent and the environment generator reaches the Nash equilibrium.

There are two main approaches for UED: 1) learning-based methods, which employ an environment generator trained via reinforcement learning, and 2) replay-based methods, which selectively replay among previously generated environments. The learning-based methods [14; 16; 17] utilize an adaptive generator that controls the parameters that fully define the environment configuration. The generator receives a regret of the agent as a reward and is trained via reinforcement learning to produce environments that maximize the regret. While the learning-based methods can directly generate meaningful environments, training the generator with RL is unstable due to the moving manifold . Additionally, we observe that the RL-based generator has limited environment coverage, which limits the generalization capability of the trained agent. In contrast, the replay-based methods [18; 19; 20] employ a random generator and select environments to revisit among previously generated environments. Since the random generator can produce diverse environments without additional training, they outperform the learning-based methods in zero-shot generalization tasks . However, the replay-based methods are sample inefficient as they require additional episodes to evaluate the regret on the randomly generated environments.

In this work, we propose a sample-efficient and robust UED algorithm by leveraging the strong representation power of diffusion models . First, to make UED suitable for using a diffusion model as a generator, we introduce soft UED, which augments the regret objective of UED with an entropy regularization term, as done in maximum entropy RL . By incorporating the entropy term, we can ensure the diversity of the generated environments. Then, we present _adversarial environment design via regret-guided diffusion models_ (ADD), which guides a diffusion-based environment generator with the regret of the agent to produce environments that are conducive to the performance improvement of the agent. Enabling this regret guidance requires the gradient of the regret with respect to the environment parameter. However, since the true value of the regret is intractable and the regret estimation methods used in prior works on UED are not differentiable, a new form of regret estimation method is needed. To this end, we propose a novel method that enables the estimation of the regret in a differentiable form by utilizing an environment critic, which predicts a return distribution of the current policy on the given environment. This enables us to effectively integrate diffusion models within the UED framework, significantly enhancing the environment generation capability.

Since the regret-guided diffusion does not require an additional training of the environment generator, we can preserve the ability to cover the high-dimensional environment domain as the random generator of the replay-based method. Moreover, ADD can directly generate meaningful environments via regret-guided sampling as the learning-based methods. By doing so, ADD effectively combines the strengths of previous UED methods while addressing some of their limitations. Additionally, unlike other UED methods, ADD allows us to control the difficulty levels of the environments it generates by guiding the generator with the probability of achieving a specific return. It enables the reuse of the learned generator in various applications, such as generating benchmarks.

We conduct extensive experiments across challenging tasks commonly used in UED research: partially observable maze navigation and 2D bipedal locomotion over challenging terrain. Experimental results show that ADD achieves higher zero-shot generalization performance in unseen environments compared to the baselines. Furthermore, our analysis on the generated environments demonstrates that ADD produces an instructive curriculum with varying complexity while covering a large environment configuration space. As a result, it is shown that the proposed method successfully generates adversarial environments and facilitates the agent to learn a policy with solid generalization capabilities.

## 2 Related Work

### Unsupervised Curriculum Reinforcement Learning

While curriculum reinforcement learning [13; 23; 24] has been shown to enhance the generalization performance of the RL agent, Dennis et al.  first introduce the concept of the unsupervised environment design (UED). UED encompasses various environment generation mehods, such as POET [12; 25] and GPN. In this work, we follow the original concept of UED, which aims to learn a minimax regret policy  by generating training environments that maximize the regret of the agent. Based on this concept, the learning-based methods train an environment generator via reinforcement learning. PAIRED  estimates the regret with a difference between returnsobtained by two distinct agents, and trains RL-based generator by utilizing the regret as a reward. Recently, CLUTR  and SHED  utilize generative models to improve the performance of PAIRED. CLUTR trains the environment generator on the learned latent space, and SHED supplies the environment generator with augmented experiences created by diffusion models. Despite the progress, training the generator via RL is unstable due to the moving manifold [16; 27] and often struggles to generate diverse environments. On the other hand, replay-based methods based on PLR  utilize a random environment generator and decide which environments to replay. ACCEL  combines the evolutionary approaches [12; 25] and PLR by taking random mutation on replayed environments. While these replay-based methods show scalable performance on a large-scale domain  and outperform the learning-based methods, they do not have the ability to directly generate meaningful environments. Unlike prior UED methods, we augment the regret objective of UED with an entropy regularization term and propose a method that employs a diffusion model as an environment generator to enhance the environment generation capability. Our work is also closely related to data augmentation for training robust policy. Particularly, DRAGEN  and ISAGrasp  augment existing data in learned latent spaces to train a policy that is robust to unseen scenarios. Our algorithm, on the other hand, focuses on generating curricula of environments without any prior knowledge and dataset.

### Diffusion Models

Diffusion models [21; 31; 32] have achieved remarkable performance in various domains, such as image generation , video generation , and robotics [35; 36; 37]. Particularly, diffusion models effectively perform conditional generation using guidance to generate samples conditioned on class labels [38; 39] or text inputs [40; 41; 42]. Prior works also guide the diffusion models utilizing an additional network or loss functions, such as adversarial guidance to generate images to attack a classifier , safety guidance using pre-defined functions to generate safety-critical driving scenarios , and guidance using reward functions trained by human preferences to generate censored samples. . We note that our implementation of the regret-guided diffusion model is based on the work of Dhariwal et al.  and Yoon et al. .

## 3 Background

### Unsupervised Environment Design

Unsupervised environment design (UED, ) aims to provide an adaptive curriculum to learn a policy that successfully generalizes to various environments. The environments are represented using a Markov decision process (MDP), defined as \( A,S,,,_{0},\), where A is a set of actions, S is a set of states, \(:S A S\) is a transition model, \(:S A\) is a reward function, \(_{0}:S\) is an initial state distribution and \(\) is a discount factor. UED employs an environment generator that designs environments by controlling free environment parameters of underspecified environments, which is represented using an underspecified Markov decision process (UMDP). UMDP is defined as \(= A,S,,^{},^{ },_{0}^{},\), where \(\) is a set of free environment parameters. Assigning a value to the free environment parameter \(\) results in a specific MDP \(_{}\) with the environment configuration (\(^{}=^{}(),^{}= ^{}(),_{0}^{}=_{0}^{}( )\)). For example, when learning a mobile robot to navigate towards the goal point while avoiding obstacles, \(\) could represent the positions of obstacles, the position of the goal, and the start position of the robot.

UED algorithms alternate between designing a set of environments and training the agent on the generated environments. The environment generator first produces an environment parameter \(\) that maximizes the agent's regret. The regret of the policy \(\) on environment \(^{}\) is defined as,

\[(,):=-V(,)+_{^{}}V(^{ },),\] (1)

where \(\) is a set of policies and \(V(,):=_{_{0}^{},,^{}}[ _{n=0}^{N}r_{n}^{n}]\) is an expected return where \(r_{n}\) is a reward obtained by \(\) at timestep \(n\) on \(^{}\). Then, the agent is trained on the generated environment to maximize the expected return, resulting in minimizing the regret. This framework can be formulated with the following two-player minimax game:

\[_{}\,_{}(,).\] (2)It is ensured that the agent learns the minimax regret policy \(^{*}}\ \ (,)\) by assuming the two-player game (2) reaches the Nash equilibrium [14; 19]. However, learning the minimax regret policy is challenging. Since the objective (2) does not guarantee the diversity of generated environments, the agent may not be trained on sufficiently various environments.

### Diffusion Probabilistic Models

A diffusion probabilistic model  is a generative model that generates samples from noise via iterative denoising steps. Diffusion models start with perturbing data by progressively adding Gaussian noise, called the _forward process_. The forward process can be modeled with a value-preserving stochastic differential equation (VP SDE, ):

\[dX_{t}=-}{2}X_{t}dt+}dW_{t},\] (3)

where \(t[0,T]\) is a continuous diffusion time variable, \(_{t}>0\) is a variance schedule, and \(W_{t}\) is a standard Brownian motion. Since the forward process (3) has tractable conditional marginal distributions \(p_{t}(X_{t}|X_{0})=(}X_{0},(1-_{t})I)\) where \(_{t}=e^{-_{0}^{t}_{t}\ dt}\), \(p_{T}(X_{T})\) will be corrupted into \((0,I)\) when \(T\).

Generating samples following the data distribution \(p_{data}()\) requires a _reverse process_, a reverse-time SDE that has the same marginal distributions as the forward process (3). By Anderson's theorem , the reverse process can be formulated with a reverse-time SDE defined as,

\[dX_{t}=-_{t}[X_{t}+_{_{t}} p_{t}(X_ {t})]dt+}dW_{t}.\] (4)

Hence, learning a diffusion model means learning a score network \(s_{}(X_{t},t)\) that approximates a score function \(_{_{t}} p_{t}(X_{t})\). The score network is trained via score matching , then plugged into the reverse process (4):

\[dX_{t}=-_{t}[X_{t}+s_{}(X_{t},t)]dt+}dW_{t}.\] (5)

Indeed, we can generate samples by solving the approximated reverse process (5) with an initial condition \(X_{T}(0,I)\).

To generate samples with label \(Y\) using the diffusion model, the score function of the conditional distribution \(p_{t}(X_{t}|Y)\) should be estimated. Since \(p_{t}(X_{t}|Y) p_{t}(X_{t},Y)=p_{t}(X_{t})p_{t}(Y|X_{t})\) due to Bayes' rule, conditional samples can be generated by solving the reverse process with classifier guidance :

\[_{}(X_{t},t) =s_{}(X_{t},t)+_{_{t}}_{t}( Y|X_{t}),\] (6) \[dX_{t} =-_{t}[X_{t}+_{}(X_{t},t)] dt+}dW_{t},\]

where \(_{t}(Y|X_{t})\) is a time-dependent classifier network and \(>0\) is a guidance weight to scale classifier gradients.

## 4 Proposed Method

In this section, we describe our approach to employ a diffusion model as an environment generator to enhance the environment generation capability. We first introduce soft UED, which mutates UED to be more suitable for using a diffusion model as a generator by augmenting the original objective with the entropy regularization term. Then, we propose a novel soft UED algorithm, adversarial environment design via regret-guided diffusion models (ADD). ADD consists of two key components: 1) a diffusion-based environment generation by using the regret as a guidance, and 2) a method to estimate the regret in a differentiable form. We present these key components in detail and conclude the section with an explanation of the overall system and its advantages compared to prior UED methods.

### Soft Unsupervised Environment Design

In this section, we introduce soft UED, designed to guarantee the diversity of environments by adding an entropy regularization term to the original UED objective (2). Soft UED is defined as the following minimax game between the agent and the environment generator:

\[_{}_{_{}}*{} _{}[(,)]+H (),\] (7)

where \(\) is a distribution over \(\), \(_{}\) is a set of distributions over \(\), \(H():=-_{}()()\) is an entropy of \(\), and \(\) is a regularization coefficient. Based on the fact that \(H\) is concave, we can show that the strong duality holds:

**Proposition 4.1**.: _Let \(L(,):=*{}_{}[(,)]+H()\) and assume that \(S,A,\) and \(\) are finite. Then, \(_{}_{_{}}L(, )=_{_{}}_{} L(,).\)_

The proof is detailed in Appendix A.1. Proposition 4.1 implies that there exists a valid optimal point \((,)\), and it is stationary for alternative optimization of \(\) and \(\). Hence, the agent will learn a soft minimax regret policy \(=*{\!}_{}_{ _{}}L(,)\) if it reaches the optimal point. One of the most significant difference from the original UED is the role of the environment generator. Instead of finding a specific environment parameter that maximizes the regret, soft UED updates the environment generator to sample environment parameters from a distribution that maximizes the objective function of soft UED.

We note that the soft UED framework encompasses prior UED algorithms. In the learning-based methods, the generator is trained with RL using an entropy bonus, which is known to enhance performance  and plays a similar role to \(H()\). The replay-based methods also consider the diversity of environments by sampling environment parameters from a probability distribution proportional to the regret, instead of selecting a parameter that maximizes the regret. Therefore, soft UED can be considered as a general framework that incorporates practical methods.

### Regret-Guided Diffusion Models

Soft UED converts the problem of generating regret-maximizing environments into a problem of sampling the environment parameter \(\) from a desired distribution \(^{}:=*{\!}_{_{ }}L(,)\). It is a well-known fact that \(^{}\) has a closed-form solution as follows:

\[^{}()=(,))} {C^{}},\] (8)

where \(C^{}\) is a normalizing constant, and \(u()\) denotes an uniform distribution over \(\). Inspired by the classifier guidance (6), we solve this sampling problem by guiding a pre-trained diffusion model with the regret. To this end, we decompose the score function of \(^{}\) as follows:

\[_{_{t}}^{}_{t}(_{t})=_{_{t}} u _{t}(_{t})+_{_{t}}_{t}(,_{t}),\] (9)

where \(t\) is a diffusion time variable, \(_{t}\) is an environment parameter perturbed by the forward process (3), \(u_{t}()\) denotes a distribution of \(_{t}\) when \(_{0} u()\), \(^{}_{t}()\) denotes a distribution of \(_{t}\) when \(_{0}^{}()\), and \(_{t}(,_{t})\) is a time-dependent regret on the noised environment \(_{t}\), which is equal to \((,_{0})\). We approximate the first term \(_{_{t}} u_{t}(_{t})\) with a score network \(s_{}(_{t},t)_{_{t}} u_{t}(_{t})\) that is learned by training a diffusion-based environment generator on the randomly generated environment dataset before the agent begins to learn. Then, we can formulate a regret-guided reverse process with a reverse-time SDE as follows:

\[ s^{}_{}(_{t},t)&=s_{} (_{t},t)+_{_{t}}_{t}(,_{t}), \\ d_{t}&=-_{t}[_{t}+s^ {}_{}(_{t},t)]dt+}dW_{t}.\] (10)

Hence, if a gradient of the regret is tractable, we can sample an environment parameter \(_{0}\) from the desired distribution \(^{}\) by solving the regret-guided reverse process (10) with an initial condition \(_{T}(0,I)\). However, the regret (1) is intractable since we cannot access the environment-specific optimal policy. Prior works on UED propose various methods to estimate the regret using episodic returns or temporal difference errors, but none of them are differentiable w.r.t. \(_{t}\) since agents cannot access the environment parameter and the reward function.

### A Differentiable Regret Estimation

In order to estimate the regret in a differentiable form, we present a novel method based on a flexible regret , which is known to enhance the performance of the learning-based UEDs . The main idea is to estimate the regret with a difference between the maximum and average episodic returns that can be achieved by the agent. To make it differentiable w.r.t. \(_{t}\), we utilize an environment critic that predicts the return of the agent in the given environment parameter, as done in DSAGE  and LPG . The environment critic \(_{}\) learns to predict a distribution of returns, analogous to distributional RL , to better capture the stochasticity of the environment and policy. Based on a support defined as \(\{z_{i}=v_{min}+(v_{max}-v_{min})\}_{i=0}^{M-1}\), which is a set of centers of M bins that evenly divide the return domain \([v_{min},v_{max}]\), we obtain an estimated categorical return distribution from an environment critic output \(l(_{t},t;)^{M}\) as follows:

\[}_{}(_{t},t)=z_{i}(l_{i}(_{t},t;))}{_{j=0}^{M-1}(l_{j}(_{t},t; ))}.\] (11)

To align \(}_{}\) with a true return distribution, we train the environment critic by gradient descent on the cross entropy loss between \(}_{}(_{t},t)\) and a target distribution, which is constructed by projecting episodic returns that the agent achieves on the environment \(^{_{0}}\) onto the support \(\{z_{i}\}_{i=0}^{M-1}\).

After the environment critic is updated, we estimate the regret (1) with a difference between the maximum return that the current agent can achieve and average of the predicted return distribution. However, the process of finding a maximum achievable return from the distribution is not differentiable. To address this issue, we further approximate the maximum with a conditional value at risk (CVaR), based on the fact that CVaR\({}_{}(})\) converges to the maximum as a risk measure \(\) goes zero. As a result, we estimate the regret of the agent as follows:

\[_{t}(_{t},t)_{}(}_{}(_{t},t))-(}_{}(_{t},t)).\] (12)

### Adversarial Environment Design via Regret-Guided Diffusion Models

An overview of ADD is provided in Figure 1. First, a diffusion-based environment generator, which is pre-trained on the randomly generated environment dataset, produces a set of environments for the agent. After the agent interacts with the generated environments and is trained via reinforcement learning, the episodic results are utilized to update the environment critic. Then, the environment critic estimates the regret of the agent in a differentiable form (12) and guides the reverse process of the diffusion-based environment generator (10), resulting in environment parameters following the

Figure 1: **Overview of ADD.** After the agent is trained on environments produced by the environment generator, the environment critic is updated using the episodic results. Then, the environment critic guides the diffusion-based environment generator with the regret to produce adversarial environments. By repeating this process, the agent learns a policy that is robust to environmental changes.

distribution that maximizes the soft UED objective (7). By repeating this process, the agent learns the soft minimax regret policy, which is robust to the variations of environments. A pseudocode of the algorithm is shown in Appendix A.4.

Since ADD does not require an additional training of the pre-trained diffusion model, the ability to cover the high-dimensional environment domain can be preserved. Furthermore, ADD enables the generator to directly produce meaningful environments via regret-guided reverse process. Therefore, ADD can be seen as having both the advantage of the replay-based methods and learning-based methods while resolving some of their limitations. Additionally, we can control the difficulty level of the generated environments after the training of the RL agent is over. In detail, we can generate environments of difficulty level \(k\{1,2,,M\}\) by replacing the regret in the regret-guided reverse process (10) with a log probability of achieving a specific return \(z_{M-k}\) as follows:

\[ s^{}_{}(_{t},t)&=s_{ }(_{t},t)+_{_{t}}(}_{ }(_{t},t)=z_{M-k}),\\ d_{t}&=-_{t}[_{t} +s^{}_{}(_{t},t)]dt+}dW_{t}.\] (13)

It enables the reuse of the learned generator and environment critic in various applications, such as generating benchmarks with varying difficulties.

## 5 Experiments

**Tasks** We conduct extensive experiments with two challenging tasks. First, we evaluate the proposed method on a partially observable navigation task with a discrete action space and sparse rewards. Then, we assess the performance of our algorithm on a 2D bipedal locomotion task, which has a continuous action space while offering dense rewards.

**Baselines** We compare the proposed method against several UED baselines. For the learning-based method, we use PAIRED , which trains the environment generator via reinforcement learning. For the replay-based method, we use \(^{}\), which utilizes the random generator and updates the agent only with episodes from the replayed environments. To benchmark performance, we use ACCEL , a current state-of-the-art UED algorithm that applies random mutations to replayed environments. Among the two implementation methods of the ACCEL, we use the one that samples environment parameters from the full parameter range, rather than the one that restricts the sampling to a range that ensures simple environments are generated, as the latter could be seen as incorporating prior knowledge. Domain randomization (DR) is also included in baselines so that we can demonstrate the effectiveness of UED. Lastly, we use ADD w/o guidance to show whether the regret guidance induces the diffusion model to generate adversarial environments and enhances the performance of the agent.

**Outline** We first train a diffusion-based environment generator on the randomly generated environment dataset. Then, we use proximal policy optimization (PPO, ) to train the agent on the environments generated by UED methods. To evaluate the generalization capability of the trained agent, we measure the zero-shot transfer performance in challenging, human-designed environments. Additionally, to understand where the differences in performance originate, we conduct quantitative and qualitative analyses on the curriculum of the generated environments by tracking complexity metrics and drawing t-SNE plots. For space consideration, we elaborate on detailed experimental settings including environment parameterization methods in Appendix B.

### Partially Observable Navigation Task

We first evaluate the proposed method on a maze navigation task , which is based on the Minigrid . In this task, an agent is trained to take a discrete action using an observation from its surroundings to receive a sparse reward upon reaching a goal. For prior UED methods, we set the maximum number of blocks in a grid environment to 60, aligning with Parker-Holder et al. . For the proposed method, we train the diffusion-based environment generator on 10M random environments whose number of blocks uniformly varies from zero to 60. Then, we train the LSTM-based policy for 250M environmental steps and evaluate the zero-shot performance on 12 challenging test environments from prior works [14; 19].

**Performance.** In Figure 2(a), we report the mean solved rate and box plot to compare the zero-shot performance of the learned policy on the test environments. The result demonstrates that ADD outperforms the baseline methods while achieving 85% of the mean solved rate, which is 18% higher compared to the ACCEL. Furthermore, ADD achieves the highest Q1, Q2, and Q3 values while its interquartile range, defined as Q3 - Q1, is 51% smaller than ACCEL. Therefore, we can infer that the proposed method consistently outperforms the baselines in the challenging test environments. In Figure 2(b), we report training curves on two test environments consisting of Maze and Labyrinth. The results demonstrate that ADD shows the monotonic performance improvement and achieves a higher solved rate compared to other baselines. While ACCEL shows 13% higher mean solved rate than DR, PAIRED and PLR\({}^{}\) do not show notable performance improvement by applying UED techniques. Particularly, PAIRED shows 8% lower mean solved rate compared to DR, demonstrating the challenge of the learning-based UED methods. ADD w/o guidance shows 63% of mean solved rate similar to DR, demonstrating that the regret guidance is critical for training the robust policy. Please refer to Appendix C.1 for detailed per-environment results.

We note that the performance is measured after the fixed number of environmental steps, in line with some UED papers  and traditional deep reinforcement learning research. In contrast, Parker-Holder et al.  recorded performance after the fixed number of policy updates. Since the replay-based methods require additional episodes without policy updates, using the number of environmental steps may be seen as unfair to PLR\({}^{}\) and ACCEL. To address this issue, we also measured the performance of our method trained with only half the environmental steps, aligning the number of policy updates with PLR\({}^{}\) and ACCEL. When using half the environmental steps, ADD achieves a 72% mean solved rate, which ties with ACCEL. This demonstrates that the proposed method remains competitive, even when using the number of policy updates as a metric.

**Generated curriculum.** In Figure 2(c), we report complexity metrics consisting of the number of blocks and shortest path length. The results demonstrate that complexity metrics of ADD w/o guidance and DR are almost consistent over time since they do not consider the policy. While PLR\({}^{}\) eventually generates environments with a larger number of blocks compared to DR, ADD and PAIRED generate a curriculum with significantly increasing complexity. Specifically, ADD eventually generates environments with the second largest number of blocks and the longest shortest path. ACCEL also shows significantly increasing complexities despite being based on PLR\({}^{}\). This is because only up to 60 blocks exist on the 13X13 grid, so random mutation increases the expectation

Figure 2: **Partially observable navigation results.****(a)**: Zero-shot performance on the 12 test environments. We report results across five random seeds, each evaluated over 100 independent episodes per environment. **(b)**: Training curves on two challenging test environments. **(c)**: Complexity metrics of the generated environments during training. **(d)**: t-SNE embedding of the generated environments during training.

of the number of blocks. Therefore, it can be seen that ADD and PAIRED efficiently generate complex environments by adapting to the current policy while \(^{}\) struggles to find environments with high complexity. To compare the distribution of generated environments, we report t-SNE  plots in Figure 2(d). While the environments generated by PAIRED eventually cover only a specific region, the environments generated by ADD cover a significantly larger region over time. The results demonstrate that ADD successfully generates adversarial environments while preserving diversity.

### 2D Bipedal Locomotion Task

We evaluate the proposed method on the 2D bipedal locomotion task, which is based on the Bipedal-Walker task in OpenAI Gym  and adopted by Parker Holder et al. . In this task, an agent is trained to control its four motors using observation from its Lidar sensor and joints to walk over challenging terrain. UED methods, including the proposed algorithm, need to provide environment parameters consisting of stump height, stair height, pit gap, stair steps, and ground roughness. We note that each parameter increases the difficulty of the environment as its value increases. We train the RL agent for two billion environmental steps and evaluate the zero-shot performance on six test environments.

**Performance.** Figure 3(a) shows the average return on each test environment. The proposed algorithm achieves the highest return across all environments, with an average of 149.6. Even with half the environmental steps, it achieves a score of 127.4, still surpassing \(^{}\). ACCEL shows lower performance than \(^{}\), which can be attributed to the lower sample efficiency induced by the additional interaction between the environment and the agent to assess the modified environments. On the other hand, PAIRED achieves the lowest return in all test environments except the easiest Basic Environment. This shows that the learning-based methods struggle to train a robust policy in practice. We note that a recent work  stabilizes the training of PAIRED in this task by integrating the evolutionary concept of ACCEL. While applying the evolutionary approach to ADD is possible, we leave it for future work. Lastly, ADD w/o guidance demonstrates superior generalization performance compared to DR. Although these two methods are theoretically identical, this difference is presumably caused by the limited size of the dataset used for training the diffusion-based environment generator.

**Generated curriculum** Figure 3(b) presents the complexity metrics of the generated training environments and the episodic returns achieved by the RL agent. Unlike the partially observable navigation task, the complexity measure of the environments generated by ADD gradually decreases. This result arises since the randomly generated environments are excessively challenging for the current agent. As evidence, examining the returns achieved by the agent in the generated environments reveals that all methods, except for ADD, consistently yield returns of 0 or below. From these results, we can infer that the proposed algorithm generates environments that are not merely more difficult but are

Figure 3: **2D bipedal locomotion task results.****(a)**: Zero-shot performance on the six test environments. We report results across five random seeds, each evaluated over 100 independent episodes per environment. **(b)**: Complexity metrics of the generated environments and episodic return achieved during training.

conducive to the agent's learning process. For detailed experimental results including a qualitative analysis on the generated environments, please refer to Appendix C.2.

### Controllable Generation

To demonstrate the ability to control the difficulty level of the generated environments, we provide the example environment generation results in Figure 4. We control the difficulty level \(k\) by guiding the diffusion-based environment generator with a log probability of achieving a specific return \(z_{M-k}\), as described in (13). We vary \(k\) from zero to \(M-1\) so that the difficulty level of generated environments increases. Environments generated with \(k=0\), which are shown in the leftmost images, include fewer blocks and a close proximity between the agent's starting position and the goal. As k increases, environments are generated with a greater number of blocks and a larger distance between the starting position and the goal, resulting in the elimination of all possible paths when \(k=M-1\). The results demonstrate that we can effectively control the difficulty level of the environment using the diffusion-based environment generator and learned environment critic, without domain knowledge. We also present the results of controlling difficulty levels for the 2D bipedal locomotion task in Appendix C.2.

## 6 Limitation

While the proposed method is suitable for training a robust policy, there exist several limitations. First, despite the existence of the optimal point is proven in Proposition 4.1, convergence to such optimal point is not guaranteed. Furthermore, the difference between the true value of the regret and its estimate is not tightly bounded. Lastly, updating the environment critic using episodic results cannot exactly capture the current agent's capability since the policy is updated after the episode. Hence, exploring methods to estimate the regret with a rigorous theoretical background would be an interesting topic for future work.

## 7 Conclusion

In this work, we present a novel UED algorithm that exploits the representation power of diffusion models. We first introduce soft UED, which augments the original UED objective with an entropy regularization term to make it suitable for using a diffusion-based environment generator. Then, we propose ADD, which guides the pre-trained diffusion model with a novel regret estimator to produce environments that are conducive to train a robust policy. Our experimental results demonstrate that ADD is capable of training a policy that successfully generalizes to challenging environments. Moreover, it has been verified that ADD generates an instructive curriculum with varying complexity while covering large environment configuration spaces.

Figure 4: **Controllable generation results for the partially observable navigation task**. The figure shows the results of guiding the generator to generate progressively more difficult environments. We note that each row is generated from the same initial noise \(_{T}\).