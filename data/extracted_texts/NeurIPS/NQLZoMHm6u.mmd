# NewTerm: Benchmarking Real-Time New Terms for

Large Language Models with Annual Updates

Hexuan Deng\({}^{1}\) Wenxiang Jiao Xuebo Liu\({}^{1}\) Min Zhang\({}^{1}\) Zhaopeng Tu

\({}^{1}\) Institute of Computing and Intelligence, Harbin Institute of Technology, Shenzhen, China

{hxuandeng,wenxiangjiaonju,tuzhaopeng}@gmail.com,

{liuxuebo,zhangmin2021}@hit.edu.cn

 Corresponding Author

###### Abstract

Despite their remarkable abilities in various tasks, large language models (LLMs) still struggle with real-time information (e.g., new facts and terms) due to the knowledge cutoff in their development process. However, existing benchmarks focus on outdated content and limited fields, facing difficulties in real-time updating and leaving new terms unexplored. To address this problem, we propose an adaptive benchmark, NewTerm, for real-time evaluation of new terms. We design a highly automated construction method to ensure high-quality benchmark construction with minimal human effort, allowing flexible updates for real-time information. Empirical results on various LLMs demonstrate over 20% performance reduction caused by new terms. Additionally, while updates to the knowledge cutoff of LLMs can cover some of the new terms, they are unable to generalize to more distant new terms. We also analyze which types of terms are more challenging and why LLMs struggle with new terms, paving the way for future research. Finally, we construct NewTerm 2022 and 2023 to evaluate the new terms updated each year and will continue updating annually. The benchmark and codes can be found at https://github.com/hexuandeng/NewTerm.

## 1 Introduction

Large language models (LLMs) have shown remarkable progress, achieving impressive performance on various benchmarks across multiple domains . However, they struggle with real-time interaction , which is crucial and challenging. In the constantly evolving internet landscape, real-time information like new facts and terms continuously emerge, where LLMs are expected to perform well.

Recent work has designed benchmarks to evaluate the performance of LLMs on new facts and the effectiveness of various improvement methods . However, benchmarks based on new terms have not been well-studied yet, which is a crucial problem that significantly reduces model performance . We urgently need a real-time benchmark to annually evaluate the performance of different LLMs and potential improvement methods toward new terms.

Besides, as the knowledge cutoff of LLMs is constantly updated, benchmarks for real-time information will soon become outdated. However, most ex

Figure 1: The framework for constructing the benchmark based on real-time new terms from the dictionary.

isting benchmark construction methods heavily rely on human efforts [31; 32; 56; 64; 67], making real-time updates extremely costly. With the rapid development of LLMs, a highly automated construction of real-time benchmarks, which can be continuously updated at a low cost, is invaluable.

To address these issues, we develop a highly automatic construction method for adaptively benchmarking new terms. As illustrated in Figure 1, we first collect new terms from online dictionaries, covering new words, new phrases, and old words with new meanings. Then, we employ LLMs to automatically construct the benchmark. Human filtering reveals that automatic construction has over 80% accuracy. Additionally, pre- and post-filtering evaluation results are highly consistent, indicating our benchmark can effectively evaluate LLMs with no human effort. Finally, we obtain the new term benchmark, NewTerm 2022 and NewTerm 2023, and will continue to update it annually.

Empirical results from over twenty diverse LLMs demonstrate the significant challenge new terms pose, with over 20% accuracy decrease when LLMs do not understand the new terms in the question. Furthermore, we construct detailed analyses over years and new term features, aiming to pave the way for developing more effective approaches toward new terms. Our contributions are:

* We automatically benchmark real-time new terms for LLMs. Empirical results reveal that new terms significantly reduce the performance of LLMs.
* We reveal the trends in performance variation with respect to LLM and new terms changes over years. We find that updates to LLMs' knowledge cutoff often do not encompass all new terms, and the overlap of terms learned by different series of LLMs is limited.
* Further analysis reveals which terms are more difficult based on term type, frequency, and deducing difficulty. We also analyze the reasons LLMs struggle with new terms.
* We publicly release the code, the constructed challenging benchmark, and the evaluation code to facilitate future research. We will release benchmarks annually to evaluate terms from the latest year, thereby tracking the real-time performance of the most recent LLMs.

## 2 Related Work

Real-time benchmark.Several benchmarks have been developed to assess the ability of models to acquire new knowledge. Levy et al.  and Meng et al.  focus on QA tasks that incorporate altered facts, while Mallen et al.  targets long-tail questions. Cheang et al.  focus on abstractive summarization tasks, and Arodi et al.  on coreference resolution tasks, both of which pose significant challenges when models contain outdated knowledge. Kasai et al.  introduce a dynamic QA platform that evaluates novel events or information on a regular basis. Yu et al.  construct a knowledge-oriented LLM assessment benchmark for world knowledge evaluation. Recently, more benchmarks have been proposed for multi-hop QA tasks. Yin et al.  introduces an artificial fact and multi-hop question generation approach, while Zhong et al.  focuses on real-world fact updates. Cohen et al.  provides broader and finer-grained categories for determining when a fact should change under multi-hop settings or not.

However, few benchmarks are specifically designed for new terms, which we aim to focus on. Martinez et al.  directly query LLMs about their knowledge of these terms, resulting in a lack of robustness towards updates of LLMs and different prompts. Recently, Zheng et al.  revealed performance degradation in NLG tasks. But it heavily relies on human effort, making updates costly and will be out of date as LLMs continue updating. In contrast, our approach focuses on NLU tasks, and will be updated annually, thanks to our highly automated construction pipeline.

LLM as data generator.Significant advancements have been made in generating training data using teacher LLMs [11; 16; 17; 29; 30; 47; 48; 52]. To address the unreliability of LLMs as evaluators, some studies have attempted to use strong LLMs like ChatGPT  or GPT-4  to construct benchmarks through well-designed filtering methods. Jain et al.  propose a self-supervised evaluation framework for LLMs that monitors behavior on real-world datasets. Qin et al.  introduce an automatic evaluator for multi-tool usage evaluation. Yin et al.  generate a question-answering (QA) dataset for new fact evaluation based on knowledge chains as triples.

However, while the generating quality of LLMs is ideal, comparable with human annotators , the building process is task-dependent. For new terms, the input information is limited, i.e., only the term and its meaning are available, making the construction more complicated.

## 3 Contructing NewTerm Benchmark

We introduce our construction of the new terms benchmark, NewTerm. We first collect new terms from online dictionaries (Section 3.2) and design three open-domain tasks to test the model's understanding of these new terms (Section 3.3). We then detail our benchmark construction pipeline (Section 3.4). Finally, we filter the benchmark to ensure quality and analyze human annotations (Section 3.5).

### Design Principle

Covering diverse tasks and terms.To comprehensively evaluate the impact of new terms, we design benchmarks for three distinct open-domain tasks in English, each focusing on a fundamental aspect of the abilities of LLMs. Furthermore, we concentrate on three typical categories of new terms: new words, new phrases, and old words with new meanings.

Tracking annual updates of LLMs and new terms.As the knowledge cutoff of LLMs is continuously updated and new knowledge constantly emerges, tracking the performance of various series of LLMs towards new terms from different time periods is crucial. Therefore, we construct benchmarks on an annual basis, currently covering 2022 and 2023. We will update the benchmark annually, utilizing the highly automatic pipeline and incorporating a broader coverage of new terms. This allows us to analyze LLM performance in terms of both model updates and new term updates.

Highly automated benchmark construction.To enable annual updates of benchmarks, a low-cost, highly automated construction approach is needed. To this end, we carefully design a construction framework that automatically builds benchmarks step by step, allowing LLMs to create high-quality benchmarks. The high consistency with human annotations demonstrates that we can automatically construct and update benchmarks that can effectively evaluate LLMs without any human effort.

Potential effect.Our construction pipeline, as the first highly automated method benchmarking real-time information, can not only track the knowledge updates of LLMs on new term understanding, but also evaluate potential improvement strategies, e.g., model editing , test-time adaptation , and retrieval . Moreover, to encourage periodic updates for real-time performance benchmarking, we provide construction inspirations and techniques for future work.

### New Term Collection

New terms from dictionary.In this study, we focus on terms added to dictionaries each year. We collect these terms from the update logs of three prominent online dictionaries: Cambridge, Collins, and Oxford. Currently, 4.2k terms added in 2022 (January 2022 to March 2023) and 2.9k terms in 2023 (April 2023 to March 2024) are collected, with continuous updates in the future.

New terms selection.We mainly focus on three typical categories of new terms: new words, new phrases, and old words with new meanings, which pose significant challenges to models . As representatives, we select 300 new terms each year, evenly distributed across three categories. To select terms that best fit these categories, we classify the collected terms across several dimensions:

Figure 2: The construction pipeline for NewTerm benchmark. We use different colors to indicate the parts used by each task, with COMA, COST, and CSJ represented by green, yellow, and red.

* Frequency: Frequency is one of the most important features of terms [1; 28], significantly impact the ability of model [19; 57]. We determine the frequency of a term by obtaining the number of Google Search results prior to the collection period of new terms. To achieve this, we adopt the Custom Search JSON API and use an exact match for the whole new term.
* Deducing Difficulty: To filter out pre-existing or easily deduced terms , we use LLMs with specific knowledge cutoffs to deduce the meaning of new terms from their spelling. For terms in 2022, we use gpt-4-0613 (knowledge cutoff: September 2021), and for terms in 2023, gpt-4-1106-preview (knowledge cutoff: April 2023). The deducing difficulty score is calculated as the cosine similarity between the deduced meaning and Gold definitions using Sentence-BERT .
* Word/Phrases: Distinct grammatical structures exist between words and phrases [3; 10], making it necessary to discuss them separately. We distinguish whether a term is a word or phrase based on the presence of a space within the term.

We identify new words and phrases as those with the highest deducing difficulty and lowest frequency, and old words with new meanings as those with the highest deducing difficulty and highest frequency. However, when a phrase appears, it rarely acquires new meanings. When we retain terms with the highest 25% frequency and highest 25% deducing difficulty, the proportion of selected words is 12.75%, while for phrases is only 1.66% among all terms. Therefore, we disregard the case of old phrases with new meanings. For instance, new word "Ziziphian" means "Inability to see the goodness in others", new phrase "tall relative" means "An influential patron", and old word "doctor" has a new meaning "To injure (a person or animal) fatally".

### Open-Domain Tasks

To evaluate how LLMs handle new terms, we focus on English natural language understanding (NLU) tasks. English is chosen due to its extensive resources and wide usage in the development and evaluation of language models. We design benchmarks for three distinct open-domain tasks in English, each focusing on a fundamental aspect of the abilities of LLMs. For clarity, we have included an example generated by our framework for each task in Figure 3, with more cases given in Appendix A.

**Choice Of Multiple Alternatives (COMA).** To assess the ability of LLMs to _comprehend_ new terms from _helpful context_, we focus on natural language inference, which has long been a grand challenge of artificial intelligence [39; 63]. We adopt the methodology outlined by Gordon et al.  to construct a causal reasoning task. The primary objective of this task is for the LLMs to identify the most plausible alternative that has a causal relationship with the given premise. In the COMA example, LLMs must first accurately understand the new term "Juggers" in the sentence, which subsequently enables them to provide a correct answer.

**Choice Of Similar Terms (COST).** To assess the ability of LLMs to effectively _utilize_ new terms and _distinguish_ them from similar ones, we focus on sentence completion, which is a major capability of LLMs [18; 55]. We follow Talmor et al.  to create a fill-in-the-blank task. The primary objective of this task is for the LLMs to select the most suitable term to complete the sentence, given a set of choices that encompass both the new terms and those that closely resemble them. In the COST

Figure 3: Examples of three open-domain NLU tasks in NewTerm benchmark. The choice with a checkmark is correct, while the choice with the ChatGPT icon is the one ChatGPT incorrectly selects under zero-shot settings. The _underlined_ word is the new term.

example, LLMs must demonstrate the ability to coherently incorporate the new term "Ziziphian" into the sentence, thereby completing the sentence accurately.

**Common Sense Judgement (CSJ).** To evaluate the ability of LLMs to _process_ and _interpret_ new terms in the _absence_ of helpful _context_, we focus on commonsense reasoning [15; 68] using a judgment format, implying that the context surrounding the term may not be accurate. We follow Clark et al.  and bench authors  to develop the judgment task. In this task, we present grammatically correct sentences that incorporate the new term but may not necessarily align with commonsense knowledge. The primary objective is for the LLMs to ascertain the plausibility of the sentence occurring in a realistic scenario. In the CSJ example, the judgment framework makes LLMs fail to deduce the extension meaning of the new term "Tall relative".

### Data Generation

We automatically construct questions for these tasks using LLMs, with the input being new terms and their meanings. During the construction process, one challenge is creating high-quality incorrect choices. Directly requesting LLMs to generate incorrect choices may lead to weakly correlated choices that fail to effectively assess the LLMs' understanding. For high-quality choices, we separate the generation of correct and incorrect choices, carefully designing prompts for each. Additionally, to maintain comparability across years, we consistently adopt GPT-4, specifically gpt-4-0613, for data generation. Prompts and detailed construction examples are given in Appendix B.

Question and correct choice generation.We use LLMs to generate sentences containing new terms and extract questions and correct choices. Detailly, for COMA, sentences must include a fixed phrase, i.e., "As an effect" or "This happened because". The question and its correct choice are generated by dividing sentences at these fixed phrases. For COST, the correct choice is invariably the new term, and the question is the sentence with the correct choice replaced by a blank, denoted as "\(\_\)". To prevent the dominance of superficial features, i.e., new terms always correct, we also create questions with related terms generated in the "Incorrect Choice Generation" procedure as correct choices with the same approach. For CSJ, the question is identical to the sentence, with the correct choice always being "True". Further, we create questions with "False" as correct choices, by providing the correct question as input and letting LLM modify it to be incorrect.

Incorrect choice generation.LLMs are not adept at generating incorrect but semantically related content. To address this issue, we first generate related terms for the new term with slightly different meanings, and then create choices that are correct for these related terms. This obtains incorrect choices closely related to the new term, while avoiding the generation of incorrect content by LLMs.

We first generate **related terms** for each new term by creating a set of terms that partially cover the semantic spectrum of the new term. These terms can be categorized into four groups: 1) Synonyms, 2) Antonyms, 3) Meaning Guessing, which attempt to convey the meaning of the new term using alternative expressions or descriptions, solely based on the spelling of the new term, and 4) Partial Synonyms, which capture only a partial aspect of the meaning of the new term. We collect three terms of each group from LLM responses, then filter out terms that are too similar to each other using Phrase-BERT , resulting in a selection of five distinct terms for each new term.

For CSJ, the choices set is simply \(\{,\}\). For the other two multiple-choice tasks, we generate incorrect choices based on the related terms generated here. For COMA, we generate incorrect choices by prompting LLMs to produce choices that are only correct for related terms. We achieve this by replacing the new term in the question with the related term and letting LLMs complete the sentence, which is then considered the incorrect choice. For COST, since it is a fill-in-the-blank task, we directly use the related term set along with the new term as the choices set. For both tasks, the incorrect choices generated by LLMs are not always reasonable, so we generate two extra choices as alternatives, i.e., six choices in total before filtering.

### Data Filtering

The incorrect choices LLMs generated are not guaranteed to be incorrect and might also be reasonable. Besides, some questions may also be irrational and do not have a reasonable choice. To tackle this, we prompt LLMs, specifically gpt-4-0613, and human annotators with the meaning of the term,letting them answer questions and filter out inconsistent ones. Finally, we obtain 744 questions for NewTerm 2022, and 715 for NewTerm 2023.

LLM filtering.We let LLMs filter the benchmark by prompting them to answer the question we generate, and are allowed to select more than one choice for multiple-choice questions. To minimize bias, we use prompts that differ from those used in the evaluation. Choices and sentences are then filtered under the following conditions: 1) The question is discarded if the prediction does not contain the correct answer, which indicates inconsistency within the question. 2) Then, for multiple-choice questions, we discard incorrect choices that are wrongly identified as correct. 3) If fewer than four choices remain, we discard these questions, as in most cases the answer has low relevance to the term. 4) If more than four choices remain, we eliminate highly similar choices using Sentence-BERT  for COMA and Phrase-BERT  for COST. Finally, we obtain four-choice questions for COMA and COST, and judgment questions for CSJ. We retain one question per term with higher perplexity calculated by Llama-2-7B , which is considered difficult, resulting in 900 questions each year.

Human filtering.We adopt human efforts for further verification. One professional and two crowdsource annotators perform human filtering using a clear interactive interface. For multiple-choice questions, we allow users to choose multiple or no choices. For judgment tasks, only True or False is permitted. Finally, in cases of discrepancy among annotators, the final decision is made after a second annotation by the professional annotator. Questions with human answers that do not align with the automatic ones are then filtered out, considered as low-quality questions.

We conducted human annotations on NewTerm 2022 and 2023. Firstly, we calculate the inter-annotator agreement using Fleiss' Kappa , which reaches a score of 0.70, indicating substantial agreement with professional annotators. Additionally, in 82.41% of cases, the annotator results match the automatically generated ones, demonstrating the efficiency of our framework for benchmark construction. Finally, out of 900 questions annually, this results in a total of 744 clean questions for NewTerm 2022, and 715 questions for NewTerm 2023, with an overall accuracy rate of 81.06%. Detailed human filtering configurations, as well as consistency analysis of each sub-module and generated data with human annotations, are provided in Appendix C.

Human filtering can be omitted.According to the aforementioned human filtering, our automatically generated benchmark has a high quality with over 80% accuracy. Moreover, the evaluation results before and after filtering maintain a high level of consistency, with an average absolute change in accuracy of only 1.59, and the accuracy ranking among LLMs remains completely unchanged. Therefore, human filtering is optional. This significantly reduces the cost of maintaining and updating the benchmark, providing foundation and assurance for our annual real-time updates for the NewTerm benchmark in the future. To alleviate concerns and more accurate result analysis, we report the results under benchmarks after human verification in subsequent experiments.

## 4 Evaluating LLMs on NewTerm

We first analyze the performance of various LLMs in Section 4.2. Subsequently, we analyze the performance variations of LLMs in the dimension of year in Section 4.3 and new term category in Section 4.4. We further analyze why LLMs struggle with new terms in Section 4.5.

### Experimental Setup

We evaluate the performance of various LLMs with different knowledge cutoffs, detailed as follows:

GPT series models.We evaluate the performance of GPT-4 , specifically gpt-4-0613, with a knowledge cutoff up to September 2021; gpt-4-1106-preview, with a knowledge cutoff up to April 2023; and gpt-4-0125-preview, with a knowledge cutoff up to December 2023. We also evaluate ChatGPT , specifically gpt-3.5-turbo-0613 and gpt-3.5-turbo-0125, both with a knowledge cutoff up to September 2021. All temperatures are set to 0 while evaluation.

Claude series models.We also evaluate claude-instant-1.2, a predecessor of Claude Haiku, and claude-2.1, a predecessor to Claude 3, both with a knowledge cutoff up to early 2023 . Further, we evaluate all sizes of Claude 3, i.e., claude-3-haiku-20240307, claude-3-sonnet-20240229, and claude-3-opus-20240229, with model size from small to large, all have a knowledge cutoff up to August 2023. All temperatures are set to 0 while evaluation.

Llama series models.We evaluate Llama-2-chat 7B, 13B, and 70B , with a knowledge cutoff up to September 2022. We also evaluate Llama-3-Instruct 8B, with a knowledge cutoff up to March 2023, and Llama-3-Instruct 70B, with a knowledge cutoff up to December 2023. All tests are done under greedy decoding.

Prompts.According to preliminary experiments, the few-shot settings do not show obvious improvements. Thus, we test LLMs in zero-shot settings without providing any additional information (**Base**) to evaluate the ability to understand new terms, and zero-shot settings with the meaning of the term prompted (**Gold**) to assess the inherent capabilities of LLMs. Subsequently, we consider the performance gap between Base and Gold settings as the performance decline caused by new terms. We run each prompt once, and any failure to answer is deemed an error, which occurs infrequently during evaluation (<2% on average). In most of these cases, they refuse to answer because they do not know the new terms.

### Main Results

Results are in Table 1. Using gpt-4-0613 for filtering may introduce bias, resulting in an over-estimation of the performance of GPT series models, especially for gpt-4-0613 itself. However, the relative value between Base and Gold remains meaningful. Additionally, despite their higher performance, we can still draw the following conclusions. To support these results, we conduct experiments on more open-source LLMs, with results showing similar trends, detailed in Appendix D.

New terms are challenging for LLMs.Results under Gold settings can be seen as the score for LLMs when they understand every term in the question. Compared to Gold setting, Base setting results in consistently and significantly worse performance (-25.63 on average), thus proving the significant performance decrease caused by new terms not known by LLMs. Results under Gold settings can also be seen as the estimation of the upper bound for each LLM using prompt-based improvement methods, thus proving the great potential for further improvement.

Larger LLMs lead to higher performance but less impact on performance decrease with new terms.We compare LLMs of varying sizes, specifically examining the largest and smallest versions of each series of models released at the same time. We observe that apart from claude-instant-1.2 and claude-2.1 which are not strictly the same version models, oth

    &  &  &  \\   & & **COMA** & **COST** & **CSJ** & **Avg.** & **Gold** & **COMA** & **COST** & **CSJ** & **Avg.** & **Gold** \\   & 7B & 28.89 & 28.12 & 60.88 & 39.29 & 58.68 & 32.16 & 33.62 & 83.93 & 49.90 & 64.54 \\  & 13B & 31.24 & 33.19 & 56.11 & 40.18 & 60.92 & 37.72 & 43.08 & 57.50 & 46.10 & 59.19 \\  & 70B & 45.49 & 48.99 & 61.13 & 51.87 & 82.38 & 48.10 & 63.14 & 64.67 & 58.64 & 81.92 \\  & 8B & 52.94 & 46.81 & 65.19 & 54.31 & 88.19 & 54.68 & 67.80 & 70.39 & 64.29 & 91.12 \\  & 70B & 66.01 & 58.70 & 66.15 & 63.62 & 96.07 & 65.35 & 73.59 & 64.94 & 67.96 & 95.83 \\   & S & 49.28 & 47.54 & 68.60 & 55.14 & 88.33 & 62.28 & 70.48 & 77.03 & 69.93 & 92.18 \\  & 38.04 & 54.20 & 71.94 & 54.73 & 82.20 & 41.52 & 64.41 & 82.20 & 62.71 & 83.25 \\   & S & 58.64 & 53.62 & 67.18 & 59.61 & 92.60 & 65.20 & 73.31 & 72.78 & 70.43 & 93.52 \\  & **3** & 56.73 & 56.23 & 64.84 & 95.19 & 93.73 & 65.79 & 70.06 & 67.07 & 67.64 & 94.98 \\
**Claude-3-opus** & L & 64.58 & 67.97 & 65.38 & 65.98 & 93.60 & 72.22 & 79.24 & 60.16 & 70.54 & 93.46 \\   & S & 52.42 & 49.71 & 73.62 & 58.58 & 87.71 & 53.51 & 68.68 & 85.39 & 69.19 & 89.83 \\  & **5** & 51.37 & 49.86 & 72.07 & 57.77 & 87.63 & 54.82 & 70.06 & 76.36 & 67.08 & 87.90 \\   & L & 68.37 & 61.16 & 70.14 & 66.56 & 98.91 & 70.18 & 77.01 & 81.01 & 76.07 & 98.72 \\   & M & 72.03 & 63.48 & 70.79 & 68.76 & 97.56 & 70.32 & 81.21 & 77.16 & 76.23 & 96.34 \\   & M & 69.80 & 65.94 & 71.94 & 69.23 & 98.11 & 68.86 & 79.94 & 78.49 & 75.76 & 96.59 \\   & - & 53.68 & 52.37 & 66.91 & 57.65 & 87.11 & 57.51 & 67.71 & 73.27 & 66.16 & 87.96 \\   

Table 1: Main results for different LLMs under NewTerm 2022 and 2023. The definitions of “COMA”, “COST” and “CSJ” can be found in Section 3.2, while “Base” and “Gold” in Section 4.1. “S”, “M”, and “L” represent small, medium, and large, respectively, inferred based on the API pricing.

performance under Gold settings, achieved an average improvement of +9.39, demonstrating a stronger ability under current tasks. However, the average performance decrease caused by new terms (Base - Gold) in larger LLMs even shows a slight increase (-25.13 vs -26.94 for smaller and larger LLMs, respectively). This suggests that powerful LLMs still struggle to address new terms.

Performance differences under new terms across years.With a unified construction setting, there is no significant performance change under Gold setting in NewTerm 2023 compared to 2022 (+0.85 on average). However, a noticeable increase was observed under Base setting (+8.51 on average). This suggests that new terms from recent years are not necessarily more challenging. Additionally, as the knowledge cutoff is updated, the performance improvement in 2022 is significantly more pronounced. For claude-instant-1.2 and gpt-4-0613, minor changes occur under Gold setting after updating to claude-3-haiku and gpt-4-0125 (+0.67). However, under Base setting, the changes are +3.57 vs +0.10 for NewTerm 2022 and 2023, respectively. This indicates that updates have a more noticeable impact on improving new terms within the knowledge cutoff.

### Results for Terms and LLMs of Different Years

Despite observing upward trends with updates on NewTerm 2022, the trends are not significant. Therefore, we assert that LLMs do not learn all new terms after updates. To demonstrate this, we extract and analyze new terms that LLMs have indeed learned after the update.

Selection of learned new terms.We select learned terms by comparing the deduce difficulty across LLMs with different knowledge cutoffs. LLMs are first asked to deduce the meaning of new terms from their spelling. The difficulty score is calculated as the cosine similarity between the deduced meaning and Gold definitions, using Sentence-BERT . We then filter out the hardest 1/3 terms with the lowest similarity in the newer LLMs, considering them unlearned. Learned terms are selected if they show a similarity increase of over 15% compared to the older LLMs.

Although this method is simplistic and cannot guarantee to select all learned terms, it still yields satisfactory results. To demonstrate this, we conduct experiments under NewTerm 2022, which is within the knowledge cutoff of the latest LLMs. For each series of LLMs, we first use the oldest and newest LLMs to classify new terms as learned and unlearned. This classification is then used to evaluate LLMs of this series, with results under Base setting on the left of Figure 4. Compared to unlearned terms, learned terms exhibit substantial improvements after knowledge cutoff updates (+10.70 vs +5.37 for learned and unlearned terms, respectively). We present cases for the learned terms and their corresponding downstream task performance in Appendix E.

Parts of new terms within the knowledge cutoff of LLMs are learned.Using the above methods, we found that there are 50% more new terms learned in 2022 compared to 2023 (38, 36, 67 vs 25, 25, 44 for Llama, Claude, and GPT respectively). Considering that the newer LLMs' knowledge cutoff is in mid-to-late 2023, results demonstrate that LLMs can update new terms within their knowledge cutoff but are hard to generalize to terms from more recent periods.

Limited overlap in learning new terms across different models.We evaluate the degree of overlap for learned terms selected by different LLM series under NewTerm 2022 and 2023. As shown in the right of Figure 4, there is limited overlap between learned terms selected by different series. Additionally, learned terms selected by other series of LLMs exhibit limited performance

Figure 4: (Left) Performance of LLMs on different terms under Base setting in NewTerm 2022. The dashed line in the middle figure represents the average accuracy of GPT-4. (Right) The overlap of learned terms selected by each series of models in NewTerm 2022 and 2023.

improvement compared to unlearned ones. These findings indicate that the overlap of new terms learned by different series of LLMs is limited. It is worth noting that the knowledge cutoff spans vary among series of LLMs, which also contributes to the limited overlap among them.

### Results for Terms of Different Category

To test which new terms and questions are more challenging, we automatically constructed a new ablation benchmark with gpt-4-0613 based on terms in 2022 with no human effort. To comprehensively evaluate different types of new terms, we remove the "New Term Selection" procedure and randomly select 1.2k new terms. We then generate more questions per term without discarding by perplexity. Finally, we obtain 6.6k questions for COMA, 6.2k for COST, and 7.1k for CSJ.

Then, we categorize terms across three dimensions defined in Section 3.2: 1) frequency from high to low: Frequent, Moderate, Rare, and New and 2) deducing difficulty from low to high: Fully, Mostly, Hardly, and Not deduced. Except for "New" terms, which are defined as terms with fewer than ten results in Google Search, other categories are evenly split. We also test results of 3) Word and Phrase.

**Frequency and deducing difficulty are strongly correlated with performance.** We report the average score of terms in each category in Figure 5. We observe a positive correlation between the frequency of terms and their accuracy, and in COMA and COST, "Frequent" terms also tend to perform poorly. These suggest that LLMs sometimes struggle to comprehend new terms and new meanings for frequent terms. Besides, deducing difficulty and accuracy are strictly positively correlated, suggesting that terms harder to deduce are also more challenging for LLMs to comprehend. Finally, phrases consistently yield higher accuracy, implying that phrases tend to be more easily understood than words. The terms with the highest and lowest frequencies, as well as the highest deducing difficulty, correspond precisely to the three types of new terms we are investigating: new words, new phrases, and old words with new meanings.

### Why LLMs Struggle in New Terms?

We randomly selected 135 cases where ChatGPT, specifically gpt-3.5-turbo-0613, failed under zero-shot settings in NewTerm 2022, averagely separated for each type of term and each task. We summarize the following three main types of errors, with cases shown in Figure 3.

**Ignoring new terms.** LLMs sometimes ignore the new term and choose the answer based on other parts of the question. In the COMA case, ChatGPT chooses the answer that focused only on the information about online shopping, ignoring the information that the new term may carry.

**Not preferring to use new terms.** LLMs do not tend to use new terms to complete the sentence, even when no suitable choice is available. In the COST case, even when other choices are all unsuitable, ChatGPT chooses the word "blind", which is grammatically incorrect but partially reasonable.

**Incorrectly understanding new terms.** LLMs sometimes incorrectly understand the new term and make wrong inferences. In the CSJ case, ChatGPT mistakenly understands the phrase "tall relative" in the literal sense as a relative with a high height, leading to the misjudgment.

**Quantitative analysis.** To show which case is more common under different conditions, we counted the error number of different types, as demonstrated in Figure 6. We can see that:

* For _COMA_, ignoring is more common. This is because our pipeline ensures that, in most cases, when ignoring the new term, a reasonable choice is also available.

Figure 5: The performance of ChatGPT for different types of new terms. Orange columns represent frequency, green represents deducing difficulty, and purple represents Word/Phrase. The lower dashed line represents the average score for Base setting, while the higher one represents Gold setting.

* For _COST_, in most failed cases, the answer is the new term, but LLMs choose old words that are not reasonable. Only one case is observed where the answer is not a new term.
* For _CSJ_, misunderstandings are more common since LLMs need to judge the whole sentence, and they try to understand the new term more. Sometimes, they may also regard the new term as a spelling error and consider it as incorrect instead of ignoring it.
* For new phrases (_NewP_), LLMs are more likely to misunderstand due to more semantic information in the spelling, while for new words (_NeW_), ignoring is more common. old words with new meanings (_OldW_) is in between.

## 5 Conclusions and Limitations

We proposed a highly automated method for benchmarking real-time new terms, i.e., NewTerm. Experiments on various LLMs highlight the challenges they face in understanding new terms. Three types of terms pose more significant challenges: new words, new phrases, and old words with new meanings. Additionally, while updates to the knowledge cutoff of LLMs can cover some new terms, they are unable to generalize to more distant ones. We have released NewTerm 2022 and 2023, and will continue to update them annually to track the performance of LLMs.

**Limitations.** This paper has several limitations.

* Although our framework is not dependent on any specific LLM, it demands high performance from them. To partially alleviate concerns about reproducibility, we conduct further experiments by employing two different LLMs, i.e., gpt-4-0613 and claude-2.1, to generate benchmarks. Human annotations and experimental results confirm the high validity of both of the benchmarks, with detailed analysis in Appendix F.
* It is hard to control variables between benchmarks of different years, as the collected new terms often have varying numbers and distributions, making comparisons of term difficulty across years difficult. To mitigate this issue, we use the same settings when generating benchmarks for evaluation.
* The highly automated and cost-effective construction pipeline offers substantial value in evaluating LLMs' understanding of a broader range of terms. However, our method has not been validated across a wider variety of new terms, with coverage currently limited to 300 new terms per year, which may introduce potential bias. Additionally, our approach has only been validated using English online dictionaries. On one hand, our method has the potential to be extended to new terms from broader sources, such as online forums and specialized domains. On the other hand, for multilingual new terms, our approach could be effectively adapted with minimal prompt modifications. However, due to budget constraints, we were unable to conduct validation across more diverse and extensive term sources.