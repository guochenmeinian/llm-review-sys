# On the Sparsity of the Strong Lottery Ticket Hypothesis

Emanuele Natale

Universite Cote d'Azur,

CNRS, Inria, I3S, France

&Davide Ferre

Universite Cote d'Azur,

CNRS, Inria, I3S, France

&Giordano Giambartolomei

Department of Informatics,

King's College London

&Frederic Giroire

Universite Cote d'Azur, CNRS,

Inria, I3S, France

&Frederik Mallmann-Trenn

Department of Informatics,

King's College London

###### Abstract

Considerable research efforts have recently been made to show that a random neural network \(N\) contains subnetworks capable of accurately approximating any given neural network that is sufficiently smaller than \(N\), without any training. This line of research, known as the Strong Lottery Ticket Hypothesis (SLTH), was originally motivated by the weaker Lottery Ticket Hypothesis, which states that a sufficiently large random neural network \(N\) contains _sparse_ subnetworks that can be trained efficiently to achieve performance comparable to that of training the entire network \(N\). Despite its original motivation, results on the SLTH have so far not provided any guarantee on the size of subnetworks. Such limitation is due to the nature of the main technical tool leveraged by these results, the Random Subset Sum (RSS) Problem. Informally, the RSS Problem asks how large a random i.i.d. sample \(\) should be so that we are able to approximate any number in \([-1,1]\), up to an error of \(\), as the sum of a suitable subset of \(\).

We provide the first proof of the SLTH in classical settings, such as dense and equivariant networks, with guarantees on the sparsity of the subnetworks. Central to our results, is the proof of an essentially tight bound on the Random Fixed-Size Subset Sum Problem (RFSS), a variant of the RSS Problem in which we only ask for subsets of a given size, which is of independent interest.

## 1 Introduction

The Lottery Ticket Hypothesis (LTH) is a research direction that has attracted considerable attention over the years, stemming from the empirical contrast between the fact that, while large neural networks can be successfully trained to achieve good performance on a given task and successively pruned to a great level of sparsity without compromising their performance, researchers have struggled to train sparse neural networks from scratch. The authors of  observed that, using a simple pruning strategy (namely Iterative Magnitude Pruning while rewinding the original weights of the remaining edges to their value at initialization), _starting from a sufficiently large random neural networks, it is possible to identify sparse subnetworks that can be trained to achieve the performance achievable by the starting network_ (see Figure 2 in the appendix for an illustration). The previous statement, namely the LTH, soon gave rise to an even stronger one, corroborated by empirical works  which proposed "training-by-pruning" algorithms (see Section 2 for details), providing evidence that _starting from a sufficiently large random neural networks, it is possible to identify sparse subnetworks that exhibit good performance as they are, without changing the original weights_ (see Figure 3 in the appendix for an illustration). By removing the need to analyze the dynamics oftraining, the last statement, namely the Strong Lottery Ticket Hypothesis (SLTH), allowed a fruitful series of rigorous proofs for increasingly more general architectures (see Section 2 for an overview). Such rigorous results can informally be stated as follows:

**Theorem 1** (Informal statement of previous SLTH results).: _With high probability, a random artificial neural network \(N_{}\) with \(m\) parameters can be pruned so that the resulting subnetwork \(N_{S}\)\(\)-approximates (i.e., approximates up to an error \(\)) any target artificial neural network \(N_{t}\) with \(O(m/_{2}(1/))\) parameters._

It is important to note that, to this day, we only have proofs on the existence of such subnetworks, also called winning tickets, but it remains an open question how to find them reliably.

_All theoretical results on the SLTH however have so far not investigated the interplay between the sparsity of the winning ticket \(N_{S}\) and the size of the random neural network \(N_{}\)._ This is in contrast to the original motivation of the LTH and to the practical application of the aforementioned training-by-pruning algorithms that motivated the SLTH, such as [15; 14]. In fact, to approximate target networks with \(O(m/_{2}(1/))\) parameters, essentially all winning tickets \(N_{S}\) have \((m)\) parameters (see Appendix A), thus being roughly of the same size of the original network \(N_{}\). We thus ask the following natural question:

If we want to \(\)-approximate a family of target artificial neural networks with \(m_{t}\) parameters by pruning a fraction \(\), called sparsity, of the \(m\) parameters of a random artificial neural network \(N_{}\), how big should \(m\) be?

We are particularly interested in the regime in which the density parameter \(=1-\) vanishes as the size of the network increases, so that the size of the winning ticket \(N_{S}\) is \( m=o(m)\).

The above question has so far remained unanswered as a consequence of the limitation inherited from the core technical tool that has been leveraged so far to prove SLTH results, namely the Random Subset Sum (RSS) Problem . Informally, the RSS asks how large a random i.i.d. sample \(\) should be so that we are able to approximate any number in \([-1,1]\) as the sum of a suitable subset of \(\). The applicability of RSS to the SLTH was first recognized by  within the proof strategy previously developed in .

### Our Contribution

We answer the aforementioned question by introducing and proving a refined variant of the RSS Problem, namely the Random Fixed-Size Subset Sum Problem (RFSS), in which the approximation of the target values should be achieved by only considering subsets of fixed size \(k\) from a set of \(n\) samples (Theorem 2). We focus on subsets of fixed size \(k\) rather than subsets of size up to \(k\) for two main reasons. From a theoretical point of view, it is a stronger requirement, and practically speaking, using fixed-size subsets enables us to achieve SLTH results where the layers of the lottery ticket exhibit a uniform structure, potentially offering a computational advantage in their implementation.

In Section 4, we show how the density \(\) impacts the _overparameterization_, i.e., the ratio \(}\) between the number of parameters of the original network \(N_{}\) and that of the class of target networks \(N_{t}\) that can be \(\)-approximated by pruning \(N_{}\) down to a subnetwork \(N_{S}\) with \( m\) parameters. In our analysis, we also compare and recover as special cases previous SLTH results such as [24; 20; 8; 3; 9]. For instance, when \( m=(m)\), we recover up to a logarithmic factor the result of , which states that the overparameterization needed is \(O(_{2}^{2}/^{2}}{m})\). In the case of Dense Neural Networks, Theorem 3 thus bridges the gap between the two extreme cases of \( m=(m_{t})\) and \( m=(m)\) considered in  and , respectively. It is worth noting that  is often considered an improvement over , as it exponentially reduces the overparameterization, albeit at the cost of a trivial sparsity level. Finally, we prove that our bounds on the overparameterization as a function of the subnetwork sparsity are essentially tight.

Organization of the paper.After reviewing the literature on the SLTH in Section 2, we introduce the Random Fixed-Size Subset Sum Problem in Section 3. In Section 4, we explore some applications of the RFSS Problem to the SLTH, and finally draw our conclusions in Section 5. Some limitations of our work, along with its potential impact, are discussed in Section 6.

Related Work

The SLTH is named after the LTH, which was introduced by Frankle and Carbin in . At the time of writing, this paper has received over 3,300 citations, attesting to the significance and impact of the research topic. Surveying the LTH is thus besides the scope of this work, and we defer the reader to dedicated surveys such as .

The SLTH was empirically motivated by work investigating training-by-pruning algorithms such as , namely algorithms that leverage the gradient of the network parameters to learn a good _mask_ of the edges to be retained (i.e., a good subnetwork, called the winning ticket).  achieves this by learning a probability associated to each edge, which is then used to sample the edges that should be included in the subnetwork.  gets rid of the stochasticity involved in the aforementioned strategies by learning a score associated to each edge; the subnetwork is then determined by including the edges with the highest score. Such strategies are leveraged in  in a federated learning setting, in order to improve the communication cost of distributed training by communicating the sampled masks of a fixed shared network, rather than the entire weights. However, these training-by-pruning algorithms are generally not computationally less expensive than classical training, since they also make use of backpropagation to update scores and are applied to a sufficiently large network to find a winning ticket. To reduce the computational cost of finding a good subnetwork,  shows, both theoretically and experimentally, that randomly pre-pruning the source network before looking for a winning ticket can be an effective approach. In , on top of randomly pruning the source network, some parameters are also frozen. Frozen parameters are forced to be part of the winning ticket and they do not have an associated score, which effectively reduces the search space for the training-by-pruning algorithms.

The first rigorous proof of the SLTH in the case of dense neural networks has been provided by , which establishes a framework that was inherited by the subsequent works.  crucially shows that the framework in  allows the application of the RSS analysis in , proving that, with no constraint on the size of the subnetworks, a random network with \(m\)-parameters can be pruned to approximate target networks with \(m/(1/)\) parameters (we defer the reader to Theorem 3 for details on further constraints on the parameters). An alternative proof of the result in  was simultaneously shown in .  and  successively extended  and  to convolutional neural networks (CNNs). By leveraging multidimensional generalizations of RSS  further extended the SLTH to structured pruning of CNNs and, as a special case, dense networks. Finally,  provided a general framework that proves the SLTH for equivariant networks.

As for refinements and generalizations of the above results,  shows that, at the cost of a quadratic overhead in the overparameterization w.r.t. , the number of layers of the random network \(N_{}\) can be reduced to \(+1\), where \(\) is the number of layers of the target networks \(N_{z}\); furthermore, while previous results only considered networks with ReLU activation,  shows how to extend the proof in  to a more general class of activations functions.  introduces the notion of universal lottery ticket, and show that it is possible to prune a sufficiently overparameterized random network so that the resulting subnetwork (the lottery ticket) can approximate certain class of functions up to an affine transformation of the output of the subnetwork (in this sense being universal).  shows how to extend the proof in  when neurons have random biases, and adapts the training-by-pruning algorithm of  to find a strong lottery ticket with a desired sparsity level. Motivated by theoretical insights on the existence of sparse strong lottery tickets,  develops a framework to plant the latter in large random network and investigates training-by-pruning algorithms, providing evidence that sparse strong lottery tickets typically exists for common machine learning tasks, and the difficulty to find them is of algorithmic nature.

Our proof of the RFSS Problem in Section 3 is based on the second moment method approach first explored by , and which has recently been refined to prove multidimensional generalizations of RSS by  and .

## 3 Fixed-Size Random Subset Sum

In this section we present our technical contributions on the RFSS, which are the foundation of our proofs regarding the sparsity of the SLTH.

Let us start by introducing some notation. We denote by \([n]\) the set \(\{1,,n\}\), for \(n\). Given a set \(=\{X_{1},...,X_{n}\}\) and a set of indices \(S[n]\) we define \(_{S}^{}=_{i S}X_{i}\), and we omit \(\) when clear from the context. We now define a class of distributions for which our RFSS result holds.

**Definition 1** (sum-bounded).: _We say that a probability density function \(f\) is sum-bounded if there exist positive constants \(c_{l}\) and \(c_{u}\) such that, for all \(k\), given \(k\) independent samples \(X_{1},...,X_{k}\) with density \(f\), the density of their sum \(f_{_{[k]}}\) satisfies_

\[}{} f_{_{[k]}}(x)}{ },\]

_with the lower bound holding for all \(x[-,]\) and the upper bound holding for all \(x\)._

At first, our definition of sum-bounded could look as a weaker version of a classical local limit theorem on the sum of random variables (e.g., see [25, Chapter VII, Theorem 7]). However, that is not the case, since we require a lower bound on the sum for any \(k\), which is needed to prove our main result.

Denote, for all \(x\), the binary entropy as

\[H_{2}(x)=-x_{2}x-(1-x)_{2}(1-x).\]

Our main technical result is the following proof of a fixed-size subset variant of the RSS Problem.

**Theorem 2**.: _Let \(0<<1\), \(c_{} 1\), \(k,n\) be integers with \(1 k\), and let \(=\{X_{1},...,X_{n}\}\) where the \(X_{i}\)'s are i.i.d. random variables with sum-bounded density. There exists a constant \(c_{}\) such that, if_

\[n c_{}}{H_{2}()},\] (1)

_then for every fixed \(z[-,]\) it holds that_

\[( S[n]\,,|S|=k:|_{S}-z|<) c_ {}.\]

_Remark_.: The proof of Theorem 2 is given in Section 3.1, and it actually holds for any \(1 k n\), for an arbitrary \([}{{n}},1)\). We state the theorem this way for readability and because we are primarily interested in high-sparsity settings (i.e., small size \(k\) of the subsets), so considering values of \(k\) does not add much to our analysis. The same remark also holds for Corollary 1.

The sum-bounded condition of Definition 1 is easily verified for distributions such as the Gaussian distribution. Previous SLTH results rely on a classical resampling argument by [17, Corollary 3.3], which shows how RSS results for Uniform\([-1,1]\) independent random variables naturally extend to independent random variables that _contains_ a uniform distribution, in the sense that they can be expressed as the mixture of distributions one of which is Uniform\([-1,1]\) with constant probability.1 The next lemma thus proves that the Uniform\([-1,1]\) distribution is sum-bounded2. A detailed proof is provided in Appendix C.

**Lemma 1**.: _The Uniform\([-1,1]\) probability density function is sum-bounded, i.e., given a set \(_{n}=\{U_{i}\}_{i[n]}\) of i.i.d. variables \(U_{i}\) with Uniform\([-1,1]\) probability density function, there exist constants \(c_{l}\) and \(c_{u}\) such that the probability density function \(f(x,n)\) of the sum \(_{[n]}^{l_{n}}\) of these variables, for all \(n\),_

\[}{} f(x,n)}{},\] (2)

_with the lower bound holding for all \(x[-,]\), and the upper bound holding for all \(x\)._

Finally, in our proofs on the Sparse SLTH in Section 4, we make use of the following corollary of Theorem 2, which ensures a uniform high probability of hitting any target \(z[-,]\), considering independent random variables that contain a uniform distribution.

**Corollary 1**.: _Let \(0<p 1\) and \((0,}{{2}})\) be constants, \(k,n\) with \(1 k\), and let \(=\{X_{1},...,X_{n}\}\) be i.i.d. random variables whose density is a mixture of a Uniform\(([-1,1])\) with probability \(p\), and some other density otherwise. There exists a positive constant \(c_{}\) that only depends on \(p\) such that, if_

\[n c_{}^{2}}{H_{2}( {k}{n})},\] (3)

_then_

\[( z[-,], S_{z}[n] \,,|S_{z}|=k:|_{S_{z}}-z|<) 1-.\]

Proof Idea.: The corollary follows from three arguments. First, by a standard sampling argument, we can assume that a constant fraction of the sample follows a Uniform\([-1,1]\) distribution. Secondly, by Lemma 1, the uniform probability density function is sum-bounded. We can thus apply Theorem 2, which guarantees a success probability of \(c_{}\) for approximating a given target. Finally, by a standard probability amplification argument and a union bound applied to Theorem 2, by paying an extra factor \(_{2}(k/)\) in Eq. 1, the constant \(c_{}\) can be assumed to be \(1-\), and the existence of a suitable subset \(S_{z}\) holds simultaneously for all \(z[-,]\). Details are given in Appendix D. 

For \(k\) big enough, we can get rid of the squared logarithmic dependency on \(k\) in the right hand side of Equation 3, as shown in the following Corollary, whose proof can be found in Appendix E.

**Corollary 2**.: _Let \(0<p 1\) and \((0,}{{2}})\) be constants, \(k,n\) be integers with \(1 k\) and \(k 2c_{}(_{2}^{2}k+2log_{2}k log_{2})\). Let \(=\{X_{1},...,X_{n}\}\) be i.i.d. random variables whose density is a mixture of a Uniform\(([-1,1])\) with probability \(p\), and some other density otherwise. There exists a positive constant \(c_{}\) that only depends on \(p\) such that, if_

\[n 2c_{}^{2}}{H_{2}( )},\] (4)

_then_

\[( z[-,], S_{z}[n] \,,|S_{z}|=k:|_{S_{z}}-z|<) 1-.\]

As customary in conference versions of papers, our proofs adopt the convention of taking ceilings and floors as suitable for non integer fractional terms. This is done in the interest of the reader (and ours), and does not impact the results in any significant way.

### Proof of Theorem 2

Proof of Theorem 2.: For simplicity, throughout the proof we will often use \(c\) to denote any positive constant. Let \(_{k}=\{S[n]\,|\,|S|=k\}\) and define, for a fixed \(z[-,]\),

\[Y=Y(z)=_{S_{k}}Z_{S}\]

where \(Z_{S}=Z_{S}(z)=_{\{|_{S}-z|<\}}\). Following , we exploit the second moment method for RFSS, generalising it to arbitrary \(k\).

\[(Y>0)[Y])^{2}}{ [Y^{2}]},\] (5)

it thus suffices to prove that

\[[Y^{2}] c([Y])^{2}.\] (6)

We first rewrite Eq. 5 in a more convenient form. Let \(\) and \(^{}\) be two independently and uniformly at random chosen subsets of \([n]\) of size \(k\), and denote \(H_{S}(z)\) as the event that \(_{S}\)\(\)-approximates \(z\), namely

\[H_{S}=H_{S}(z)=\{|_{S}-z|<\}.\]We have

\[[Y]=_{S_{k}}[Z_{S}]=_{S_{k} }(H_{S})=(H_{})\] (7)

and

\[[Y^{2}] =[(_{S_{k}}Z_{S}) (_{S^{}_{k}}Z_{S^{}})]=_{S,S ^{}_{k}}[Z_{S}Z_{S^{}}]\] \[=_{S,S^{}_{k}}(H_{S} H_{S^ {}})=^{2}(H_{} H_{^{ }}).\] (8)

Using Eqs. 7 and 8 we can rewrite the r.h.s. of Eq. 5 as follows

\[[Y])^{2}}{[Y^{2}] }=})]^{2}}{(H_{ } H_{^{}})}=})}{ (H_{^{}}..|H_{}.)}.\]

Eq. 6 thus becomes

\[(H_{^{}}|\,H_{}.) c (H_{}).\] (9)

Let \(I_{i}\) denote the event \(\{|^{}|=i\}\) and \(I_{a,b}\) the event \(_{a i b}I_{i}\). Fix \((,1)\). By the law of total probability and independence of \(I_{i}\) and \(H_{}\), we rewrite the l.h.s. of Eq. 9 as follows:

\[(H_{^{}}|\,H_{}).\] \[=(H_{^{}} I_{k}|\,H_{} .)+(H_{^{}} I_{ k,k-1}|\,H_{ }.)+(H_{^{}} I_{0, k-1} |\,H_{}.)\] \[=(I_{k})(H_{^{}}| \,H_{},I_{k})\] (10) \[+(I_{ k,k-1})(H_{^ {}}|\,H_{},I_{ k,k-1}.)\] \[+_{i=0}^{ k-1}((I_{i}) (H_{^{}}|\,H_{},I_{i}.)).\] (12)

To conclude the proof, it suffices to show that each addendum in Eqs. 10, 11 and 12 are upper-bounded by some constant multiple of \(/\), since the lower bound in Definition 1 ensures that

\[} c(H_{}).\] (13)

As for the first addendum (Eq. 10), since \((H_{^{}}|\,H_{},I_{k}.)=1\), then

\[(I_{k})(H_{^{}}| \,H_{},I_{k}.)=(I_{k})=}}2^{-nH_{2}( )}\] \[}2^{-_{}_{2}}2},\] (14)

where inequality \((a)\) in Eq. 14 is a standard lower bound on \(\) holding for all \(k n-1\); in inequality \((b)\) in Eq. 14 we used Eq. 1, namely \(nH_{2}() c_{}_{2}\); in inequality \((c)\) in Eq. 14 we used that \(c_{} 1\).

As for the second addendum (Eq. 11), we next show that

\[(I_{ k,k-1})(H_{^{}}|\,H_{},I_{ k,k-1}.) c}\] (15)

by proving that

\[(I_{ k,k-1})}\] (16)

and

\[(H_{^{}}|\,H_{},I_{ k,k-1}.)  c.\] (17)First, observe that \(I=|^{}|\) follows a Hypergeometric\((n,k,k)\) distribution, thus by Chebyshev's inequality

\[(I_{ k,k-1}) (I k)=(I-}{n} k -}{n})[I]}{^{2}k^{2} (1-)^{2}}\] \[ c^{}}{n}} },\]

having set \(c^{}=^{2}(1-}{{}})^{2}>0\), thus proving Eq. 16. The proof of Eq. 17 is given in Appendix F, concluding the proof of Eq. 15.

As for the third addendum (Eq. 12), in Appendix F we show that

\[_{i=0}^{ k-1}(I_{i})(H_{^{}} |\,H_{},I_{i}) c},\] (18)

The three bounds on the addenda in Eqs. 10, 11, and 12 (Eqs. 14, 15, and 18, respectively), combined with Eq. 13, conclude the proof. 

## 4 Sparse Strong Lottery Ticket Hypothesis (SSLTH)

We now apply our results on the RFSS problem to the SLTH and obtain guarantees on the sparsity of winning tickets for Dense Neural Networks (DNNs, Theorem 3) and Equivariant NNs (Theorem 4).

The next theorem essentially interpolates between the two extremes of [Theorem 2.1] (where \( m=(m_{t})\)) and [Theorem 1] (where \( m=(m)\)), where we recall that \(m\) and \(m_{t}\) represent the number of parameters of the overparameterized and the target networks, respectively, and \(\) is the density of the winning ticket.

We use \(()\) to denote the ReLU activation function, i.e., \((x)=x_{x 0}\), and \(\|\|\) to denote the spectral norm of the matrix \(\). Let \(\) to be a set of target ReLU neural networks \(f:^{d_{0}}^{d_{l}}\) of depth \(l\) such that

\[=\{f:f()=_{l}(_{l-1} (_{1})),\, i\,\,\,_{i}^{d_{i} d_{i-1}}\|_{i}\| 1\}\] (19)

For a given \(f\), for all \(i[]\), let \(_{i}=\{d_{i-1}/d_{i},\,d_{i}/d_{i-1}\}\), and \(=_{i}_{i}\). Then, recalling that \(c_{}\) is the constant defined in Corollary 1, we have the following result.

**Theorem 3** (SSLTH for DNNs).: _Let \(g\) be a randomly initialized feed-forward \(2\)-layer neural network, in which each weight is drawn from a Uniform\([-1,1]\) distribution, of the following form:_

\[g()=_{2l}(_{2l-1}(_ {1})).\]

_Let \(^{}=^{}()(0,1)\), \(_{2i}^{d_{i} 2d_{i-1}n_{i}^{*}}\) and \(_{2i-1}^{2d_{i-1}n_{i}^{*} d_{i-1}}\), with \(n_{i}^{*}\) satisfying_

\[n_{i}^{*}=c_{}^{2}(d_{i}^{ }n_{i}^{*}}{H_{2}(^{})})}{H_{2}(^{})}.\] (20)

_With probability at least \(1-\), for every \(f\), where \(\) is defined as in Eq. 19, \(g\) can be pruned to obtain a subnetwork of sparsity at least \(=1-\) that approximates \(f\) up to an error \(\), having defined \(=^{}\)._

Proof Idea.: The theorem follows from a slight variation of the same approach detailed in , in which we use our Corollary 1 instead of [Corollary 2.5] when pruning \(g\), allowing us to have control over the size of the pruned network. A detailed proof is provided in Appendix G. 

To illustrate a simple example of how Theorem 3 addresses the main question asked in the introduction, consider the case where we want to approximate a target network with \(m_{t}\) parameters and \(\) layers, each of width \(d\) (so \(=1\) and \(^{}=\)), by pruning an overparameterized network to achieve a desired sparsity level of \(=1-\). The condition expressed by Equation 20 in Theorem 3comes from the use of Corollary 1 when pruning network \(g\), as shown in the proof. If, instead of Corollary 1, we use its simplified variant Corollary 2, it is easy to observe that Equation 20 would become

\[n_{i}^{*}=c_{}^{2}(d_{i}}{} )}{H_{2}(^{})}.\] (21)

Using this condition, Theorem 3 then tells us that we need to prune a randomly initialized network with twice as many layers and a number of parameters of the order of \(d^{2}}{H()}}{H()}\).

We will now clarify the connection between Theorem 3 and the earlier results from  and . Figure 1 provides a quick visual comparison.

Malach et al.When all layers have the same width \(d\),  showed that any target network with \(l\) layers and a total of \(m_{t}=d^{2}l\) parameters can be \(\)-approximated by pruning a randomly initialized network with \(2l\) layers. The overparameterization of this network, relative to the target network, is \(O(^{2}}{^{2}}_{2}}{} )=(^{2}}{^{2}})\). More specifically, the winning ticket found after pruning has a parameter count of the same order as the target network, resulting in a density of \(=(}{m_{t}^{2}})\). Notably, this density \(\) is the inverse of the overparameterization, as the size of the winning ticket matches that of the target network.

Next, we show that Theorem 3 also yields a density that is polynomial in \(}\), when using an overparametrization of \((^{2}}{^{2}})\). Let \(z=(}{})\), and note that \(^{}=\) in Theorem 3, since all layers have the same width. As \(n_{i}^{*}\) in Theorem 3 represents the overparametrization with respect to the target network, let us set \(n_{i}^{*}=cz^{2}\), for some constant \(c\). Equation 20 then becomes

\[cz^{2} c_{}^{2}(cz^{3})}{H()}\] (22)

We show that the inequality \(cz^{2} c_{}^{2}(cz^{3})}{_{2}( /)}\) holds for some big enough constant \(c\) when setting \(=}=\), which implies that Equation 22 is also satisfied. We get \(cz c_{}^{2}(cz^{2})}{_{2}(z)}\), which is satisfied for a big enough constant \(c\) (see Appendix I). Overall, when using an overparametrization \((^{2}}{^{2}})\), we find a winning ticket with density \(}\), as shown in Figure 1.

Pensia et al.For simplicity, let us still consider target networks where all layers have the same width \(d\), and we apply Theorem 3 using the simplified condition from Equation 21. When

Figure 1: A qualitative plot showing the relationship between the density \(\) of a winning ticket and the overparameterization required by Theorem 3 for a target network with \(m_{t}\) parameters. Earlier results from Pensia et al.  and Malach et al.  are shown for comparison.

\( m=(m)\), i.e. the density \(\) is a constant as in  (see Appendix A), the entropy term \(H_{2}(^{})\) in the right-side of Equation 21 also becomes a constant. In this setting, we indeed recover the result shown in [Theorem 1], up to a logarithmic factor, as shown in Figure 1.

Quite similarly to Theorem 3, the next result essentially generalizes  up to a factor \(_{2}\). The theorem is stated with the understanding that for \(G\)-equivariant networks, in order to preserve \(G\)-equivariance, pruning is best done not with respect to the parameters expressing the network in the canonical basis (i.e. directly on the weights of the network), but with respect to the _equivariant parameters_, that is those coefficients expressing the linear layers of the network as a linear combination of the elements of the corresponding equivariant basis . For simplicity, due to the technical set-up, we assume all feature spaces being \(=(^{d},)\), with \(\) the linear representation of the group \(G\), and the same number \(n\) of such feature spaces being stacked in each layer. A \(G\)-equivariant linear map from the \(i\)th feature space to the \(i+1\)st can be decomposed in a corresponding equivariant basis denoted \(_{i i+1}=\). Since all feature spaces are the same, we omit the layers' indices. When stacking \(n\) feature spaces in the input and output of the \(i\)th layer, the full equivariant basis is denoted \(k_{n n}\), and finally the basis of the \(G\)-equivariant maps from \(^{n}\) to \(^{n}\) can be written as the Kronecker product \(k_{n n}\). For any basis \(=\{b_{1},,b_{p}\}\), we denote its cardinality \(p=||\) and define \(\|\|=_{\|\|_{}}\|_{k=1}^{p}_{k}b_{k}\|\), with \(\|\|\) in the r.h.s. being the operator norm inherited from the \(_{p}\) norm.

**Theorem 4** (SSLTH for Equivariant Networks).: _Let \(h\) be a random \(2\)-layer \(G\)-equivariant network where all equivariant parameters are drawn from a Uniform\([-1,1]\) distribution, every odd layer expressed in the associated equivariant basis \(k_{ n}\) and every even layer expressed in the associated equivariant basis \(k_{n}\). Let \(=()(0,1)\), with \(\) satisfying_

\[=c_{}^{2}(\{| |,\||\}}{})}{H_{2}( )}.\]

_With probability at least \(1-\), for every \(\)-layer \(G\)-equivariant neural network \(f\), with all layers expressed in the associated equivariant basis \(k_{n n}\), \(h\) can be pruned to obtain a \(G\)-equivariant subnetwork of sparsity at least \(=1-\) that approximates \(f\) up to an error \(\)._

The proof, which we omit, is analogous to that of Theorem 3, since [Theorem 1] exploits the exact same pruning strategy of , except for the fact that it is applied not to the original parameters of the equivariant network, but to the network expressed in terms of its equivariant basis (the sparsity \(\) is here also intended with respect to the equivariant parameters count). This allows the construction to apply without losing the property of equivariance in the pruned approximating subnetwork obtained. The crucial step is when Corollary 1 is applied in [Lemma 1], instead of [Corollary 2.5]. This is done in parallel, multiple times, across non-overlapping coefficients of the equivariant basis. Thanks to the careful preprocessing devised by the authors, this preserves equivariance and at the same time ensures that each application of Corollary 1 is independent of the others.

To conclude the section, we mention that Theorem 4 applies in particular to vanilla CNNs, which are a special case of equivariant neural networks where the group is \(G=(^{2},+)\), recovering previous SLTH results on CNN [8; 3]. Furthermore, we remark that Theorem 3 can be revisited through the improvement upon the \(2\)-depth overparameterization devised in , i.e., it is possible to provide sparsity guarantees also for overparameterizations requiring depth \(+1\) only. The analysis is more technical and we omit it, but the ideas are analogous to what shown in . An analogous improvement is suggested as future work in .

### Lower bound on the required overparameterization

We now adapt the lower bound of  in order to almost match the required overparameterization of our Theorem 3, considering the simple scenario in which we want to approximate the family \(\) of all linear networks with weights forming a matrix having spectral norm less than \(\); more formally

\[:=\{h_{W}:W^{d d},\|W\|\},  h_{W}(x)=Wx.\] (23)

The formal claim states that, if a network with \(n\) parameters can approximate every \(h_{W}\) with probability at least \(\) (after it is pruned down to \(k\) parameters), then the hypothesis of Theorem 2 in Eq. 1 must hold.3

**Theorem 5**.: _Let \(n,k\), with \(1 k n\), having set \(=1-}{{2}} 0.84\). Consider a neural network \(g\) with \(n\) parameters, and let \(_{k}\) be the set of neural networks that can be formed by pruning \(g\) down to \(k\) parameters. Let \(\) be as defined in Eq. 23. If it holds that, for some \(<}{{16}}\),_

\[ h_{W},( g^{} _{k}:_{:\|x\| 1}\|h_{W}(x)-g^{}(x)\|< ),\] (24)

_then it holds that_

\[n}{2}}{H_{2}()}.\]

The theorem follows by adapting the packing argument of . A detailed proof is provided in Appendix H.

## 5 Conclusions

In this work, we have extended previous results on the Strong Lottery Ticket Hypothesis by quantifying the required overparameterization as a function of the sparsity of the subnetworks. Central to our results is a proof of the Random Fixed-size Subset Sum (RFSS) Problem, a refinement of the seminal Random Subset Sum (RSS) Problem in which the subsets have a required fixed size.

A challenging open problem is to extend our analysis of RFSS to the multidimensional case, in which the random samples and targets are vectors in \(^{d}\). Previous extension of RSS to the Multidimensional RSS have indeed allowed to prove structured-pruning version of the SLTH . A Multidimensional RFSS result would then allow to quantify, in the structured pruning case, the dependency of the overparameterization w.r.t. the sparsity of the (structured) subnetworks.

Another future direction is to refine our analysis of the RFSS in Theorem 2 in order to improve the probability of success to \(1-\) rather than constant, thus allowing to avoid shaving off the extra factor \(_{2}(1/)\) in our corollaries w.r.t. our lower bound, which is due to the amplification done in Corollary 1 to get to probability \(1-\).

Finally, an important future direction is to improve training-by-pruning methods such as  or to develop new ones, in order to allow to efficiently find strong lottery tickets of a desired sparsity, thus empirically validating our theoretical predictions.

## 6 Limitations and Impact

LimitationsSimilar to all the research conducted on the LTH and the SLTH, this work only proves the existence of lottery tickets. To this date, it is not clear if these subnetworks can be found reliably (no formal proof exists) in an efficient manner - however, empirical evidence suggests that efficient algorithms exist (e.g., ).

ImpactThe contribution of this work is primarily theoretical and not confined to a specific domain. Its potential societal impact would, therefore, be closely tied to the particular scenarios to which it is applied. It could be interesting to compare the environmental impact of finding lottery tickets inside overparameterized networks. We also believe that our work has the potential to have a strong environmental impact as sparse NNs have massively reduced inference costs.