# Learning to Search Feasible and Infeasible Regions of Routing Problems with Flexible Neural k-Opt

Yining Ma

National University of Singapore

yiningma@u.nus.edu

&Zhiguang Cao

Singapore Management University

zgcao@smu.edu.sg

&Yeow Meng Chee

National University of Singapore

ymchee@nus.edu.sg

 Zhiguang Cao

Zhiguang Cao is the corresponding author.

37th Conference on Neural Information Processing Systems (NeurIPS 2023).

###### Abstract

In this paper, we present Neural k-Opt (NeuOpt), a novel learning-to-search (L2S) solver for routing problems. It learns to perform flexible k-opt exchanges based on a tailored action factorization method and a customized recurrent dual-stream decoder. As a pioneering work to circumvent the pure feasibility masking scheme and enable the autonomous exploration of both feasible and infeasible regions, we then propose the Guided Infeasible Region Exploration (GIRE) scheme, which supplements the NeuOpt policy network with feasibility-related features and leverages reward shaping to steer reinforcement learning more effectively. Additionally, we equip NeuOpt with Dynamic Data Augmentation (D2A) for more diverse searches during inference. Extensive experiments on the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) demonstrate that our NeuOpt not only significantly outstrips existing (masking-based) L2S solvers, but also showcases superiority over the learning-to-construct (L2C) and learning-to-predict (L2P) solvers. Notably, we offer fresh perspectives on how neural solvers can handle VRP constraints. Our code is available: https://github.com/yining043/NeuOpt.

## 1 Introduction

Vehicle Routing Problems (VRPs), prevalent in various real-world applications, present NP-hard combinatorial challenges that necessitate efficient search algorithms [1; 2]. In recent years, significant progress has been made in developing deep (reinforcement) learning-based solvers (e.g., [3; 4; 5; 6; 7; 8; 9; 10; 11]), which automates the tedious algorithm design with minimal human intervention in a data-driven fashion. Impressively, these neural solvers developed over the past five years have closed the gap to or even surpassed some traditional hand-crafted solvers that have evolved for several decades [5; 12].

In general, neural methods for VRPs can be categorized into learning-to-construct (L2C), learning-to-search (L2S), and learning-to-predict (L2P) solvers, each offering unique advantages while suffering respective drawbacks. L2C solvers (e.g., [4; 13]) are recognised for their fast solution construction but may struggle to escape local optima. L2P solvers excel at predicting crucial information (e.g., edge heatmaps [14; 15]), thereby simplifying the search especially for large-scale instances, but may lack the generality to efficiently handle VRP constraints beyond the Traveling Salesman Problem (TSP). L2S solvers (e.g., [8; 9]) are designed to learn exploration in the search space directly; however, their search efficiency is still limited and lags behind the state-of-the-art L2C and L2P solvers. In this paper, we delve into the limitations of existing L2S solvers, and aim to unleash their full potential.

One potential issue of L2S solvers lies in their simplistic action space designs. Current L2S solvers that learn to control k-opt exchanges mostly leverage smaller \(k\) values (2-opt [9; 16] or 3-opt ), partly because their models struggle to efficiently deal with larger \(k\). To address this issue, we introduce Neural k-Opt (NeuOpt), a flexible L2S solver capable of handling k-opt for any \(k 2\). Specifically, it employs a tailored action factorization method that simplifies and decomposes a complex k-opt exchange into a sequence of basis moves (S-move, I-move, and E-move) with the number of I-moves determining the \(k\) of a specifically executed k-opt action1. Such design allows k-opt exchanges to be easily constructed step-by-step, which more importantly, provides the deep model with the flexibility to explicitly and automatically determine an appropriate \(k\). This further enables varying \(k\) values to be combined across different search steps, striking a balance between coarse-grained (larger \(k\)) and fine-grained (smaller \(k\)) searches. Correspondingly, we design a Recurrent Dual-Stream (RDS) decoder to decode such action factorization, which consists of recurrent networks and two complementary decoding streams for contextual modeling and attention computation, thereby capturing the strong correlations and dependencies between removed and added edges.

Besides, existing L2S solvers confine the search space to feasible regions based on feasibility masking. By contrast, we introduce a novel Guided Infeasible Region Exploration (GIRE) scheme that promotes the exploration of both feasible and infeasible regions. GIRE enriches the policy network with additional features that indicate constraint violations in the current solution and the exploration behaviour statistics in the search space. It also includes reward shaping to regulate extreme exploration behaviours and incentivize exploration at the boundaries of feasible and infeasible regions. Our GIRE offers four advantages: 1) it circumvents the non-trivial calculation of ground-truth action masks, particularly beneficial for constrained VRPs or broader action space as in NeuOpt, 2) it fosters searches at the more promising feasibility boundaries, similar to traditional solvers [18; 19; 20; 21], 3) it bridges (possibly isolated) feasible regions, helping escape local optima and discover shortcuts to better solutions (See Figure 2), and 4) it forces explicit awareness of the VRP constraints, facilitating the deep model to understand the problem landscapes. In this paper, we apply GIRE to the Capacitated Vehicle Routing Problem (CVRP), though we note that it is generic to most VRP constraints.

Moreover, our NeuOpt leverages a Dynamic Data Augmentation (D2A) method during inference to enhance the search diversity and escape local optima. Our NeuOpt is trained via the reinforcement learning (RL) algorithm tailored in our previous work . Extensive experiments on classic VRP variants (TSP and CVRP) validate our designs and demonstrate the superiority of NeuOpt and GIRE over existing approaches. Our contributions are four-fold: 1) we present NeuOpt, the first L2S solver that is flexible to handle k-opt with any \(k 2\) based on a tailored formulation and a customized RDS decoder, 2) we introduce GIRE, the first scheme that extends beyond feasibility masking, enabling exploration of both feasible and infeasible regions in the search space, thereby bringing multiple benefits and offering fresh perspectives on handling VRP constraints, 3) we propose a simple yet effective D2A inference method for L2S solvers, and 4) we unleash the potential of L2S solvers and allow it to surpass L2C, L2P solvers, as well as the strong LKH-3 solver  on CVRP.

## 2 Literature review

We categorize recently developed neural methods for solving vehicle routing problems (VRPs) into _learning-to-construct_ (L2C), _learning-to-search_ (L2S), and _learning-to-predict_ (L2P) solvers.

**L2C solvers.** They learn to construct solutions by iteratively adding nodes to the partial solution. The first modern L2C solver is Ptr-Net  based on a Recurrent Neural Network (RNN) and supervised learning (extended to RL in  and CVRP in ). The Graph Neural Networks (GNN) were then leveraged for graph embedding  and faster encoding . Later, the Attention Model (AM) was proposed by Kool et al. , inspiring many subsequent works (e.g., [27; 28; 29; 30; 31]), where we highlight Policy Optimization with Multiple Optima (POMO)  which significantly improved AM with diverse rollouts and data augmentations. The L2C solvers can produce high-quality solutions within seconds using greedy rollouts; however, they are prone to get trapped in local optima, even when equipped with post-hoc methods (e.g., sampling , beam search , etc), or, advanced strategies (e.g., invariant representation [32; 13], learning collaborative policies , etc). Recently, the Efficient Active Search (EAS)  addressed such issues by updating a small subset of pre-trainedmodel parameters on each test instance, which could be further boosted if coupled with Simulation Guided Beam Search (SGBS) , achieving the current state-of-the-art performance for L2C solvers.

**L2S solvers.** They learn to iteratively refine a solution to a new one, featuring a search process. Early attempts, e.g., NeuRewriter  and L2I , relied heavily on traditional local search algorithms and long run time. The NLNS solver  improved upon them by controlling a ruin-and-repair process that destroys parts of the solution using handcrafted operators and then fixes them using a learned deep model. Besides, the crossover exchanges between solutions were also learned in . Recently, several L2S solvers focused on controlling the k-opt heuristic that is more suitable for VRPs [20; 38]. Wu et al.  made an early attempt to guide 2-opt, showing superior performance than the L2C solver AM . Ma et al.  improved Wu et al.  by replacing vanilla attention with Dual-Aspect Collaborative Attention (DAC-Att) and a cyclic positional encoding method. The DAC-Att was then upgraded to Synthesis Attention (Synth-Att)  to reduce computational costs. Besides, Costa et al.  proposed an RNN-based policy to govern 2-opt, which was extended to 3-opt in  with higher efficiency. However, these neural k-opt solvers are limited by a small and fixed \(k\). Besides, although L2S solvers strive to surpass L2C solvers by directly learning to search, they are still inferior to those state-of-the-art L2C solvers (e.g., POMO  and EAS ) even when given prolonged run time.

**L2P solvers.** They learn to guide the search by predicting critical information. Joshi et al.  proposed using GNN models to predict heatmaps that indicate probabilities of the presence of an edge in the optimal solution, which then uses beam search to solve TSP. It was leveraged for larger-scale TSP instances in  based on divide-and-conque, heatmap merging, and Monte Carlo Tree Search. In the GLS solver , a similar GNN was used to guide the traditional local search heuristics. More recently, the DIFUSCO solver  proposed to replace those GNN models with diffusion models . Compared to L2C or L2S solvers, L2P solvers exhibit better scalability for large instances; however, they are mostly limited to supervised learning and TSP only, due to challenges in preparing training data and the ineffectiveness of heatmaps in handling VRP constraints. Though L2P solver DPDP  managed to solve CVRP with dynamic programming, it was outstripped by L2C solver EAS. Recently, L2P solvers also explored predicting a latent continuous space for the underlying discrete solution space, where the latent space is then searched using differential evolution in  or gradient optimizer in . Still, they can be either time-consuming or ineffective in tackling VRP constraints.

**Feasibility satisfaction.** Most neural solvers handle VRP constraints using the masking scheme that filters out invalid actions (e.g., [12; 44]). However, few works considered better ways of constraint handling. Although the works [45; 46] attempted to use mask prediction loss to enhance constraint awareness, they overlooked the benefits of the temporary constraint violation applied in many traditional solvers [18; 19; 20; 21]. Lastly, we note that constraint handling in VRPs largely differs from safe RL tasks that focus on fully avoiding risky actions in uncertain environments [47; 48].

## 3 Preliminaries and notations

**VRP notations.** VRP aims to minimize the total travel cost (tour length) while serving a group of customers subject to certain constraints. It is defined on a complete directed graph \(\!=\!\{,\}\) where \(x_{i}\!\!\) are nodes (customers) and \(e(x_{i}\!\!x_{j})\!\!\) are possible edges (route) weighted by the Euclidean distance between \(x_{i}\) and \(x_{j}\). In the Traveling Salesman Problem (TSP), the solution is a Hamiltonian cycle that visits each node exactly once and returns to the starting one. In the Capacitated Vehicle Routing Problem (CVRP), a depot \(x_{0}\) is added to \(\), and each customer node \(x_{i}(i 1)\) is assigned a demand \(_{i}\). A CVRP solution consists of multiple sub-tours, each representing a vehicle departing from the depot, serving a subset of customers, and returning to the depot, where each \(x_{i}(i 1)\) is visited exactly once and the total demand of a sub-tour must not exceed the vehicle capacity \(\). For instance, \(=\{x_{0}\!\!x_{2}\!\!x_{1}\!\!x_{0}\!\!x_{3}\!\!x_{0}\}\) is a CVRP-3 solution with \(\!=\!\) and \(\!=\!10\).

**Traditional k-opt heuristic.** The k-opt heuristic iteratively refines a given solution by exchanging \(k\) existing edges with \(k\) (entirely) new ones. The Lin-Kernighan (LK) algorithm  narrowed the search with several criteria, where we underscore the _sequential exchange criterion_. It requires: for each \(i=1,,k\), the removed edge \(e_{i}^{}\) and added edge \(e_{i}^{}\) must share an endpoint, and so must \(e_{i}^{}\) and \(e_{i+1}^{}\). This allows a simplified sequential search process, alternating between removing and adding edges. Moreover, the LK algorithm considers scheduling varying \(k\) values in a repeated ascending order, so as to escape local optima by varying search neighbourhoods. Inspired by them, our paper proposes a powerful L2S solver that performs flexible k-opt exchanges with automatically chosen based on our action factorization method and the customized decoder. Recently, the LK algorithm has been implemented by Helsgaun  in the open-source LKH solver, with additional non-sequential exchanges and an edge candidate set. It was then upgraded to LKH-2 , leveraging more general k-opt exchanges, divide-and-conquer strategies, etc. The latest release, LKH-3 , further tackled constrained VRPs by penalizing constraint violations, making it a generic and powerful solver that serves as a benchmark for neural methods. Finally, we note that k-opt has been a foundation for various solvers, including the state-of-the-art Hybrid Genetic Search (HGS) solver  for CVRP.

## 4 Neural k-opt (NeuOpt)

Designing an effective neural k-opt solver necessitates addressing challenges potentially overlooked in prior works. Firstly, the solver should be generic for any given \(k\!\!2\), using a unified formulation and architecture. Secondly, it should coherently parameterize the complex action space while accounting for the strong correlations and dependencies between removed and added edges. Finally, it should dynamically adjust \(k\) to balance coarse-grained (larger \(k\)) and fine-grained (smaller \(k\)) search steps.

In light of them, we introduce Neural k-Opt (NeuOpt) in this section. We first present our action factorization method for flexible k-opt exchanges, and then demonstrate our decoder to parameterize such actions, followed by the dynamic data augmentation method for inference. Note that this section focuses on TSP only, and we extend our NeuOpt to handle other VRP constraints in Section 5.

### Formulations

We introduce a new factorization method that constructs a k-opt exchange using a combination of three _basis moves_, namely the _starting move_, the _intermediate move_, and the _ending move_. Concretely, the sequential k-opt can be simplified as performing one S-move, several (possibly none) I-moves, and finally one E-move, where the choice of the \(k\) corresponds to determining the number of I-moves.

**S-move.** The starting move removes an edge \(e^{}(x_{a}\!\!x_{b})\), converting a TSP solution \(\) (a Hamiltonian cycle) into an open Hamiltonian path with two endpoints \(x_{a}\) and \(x_{b}\). It is executed only at the beginning of action construction. We denote S-move as \(S(x_{a})\) since \(x_{b}\) can be uniquely determined if \(x_{a}\) is specified. We term the source node \(x_{a}\) as _anchor node_ to compute the _node rank_.

**Definition 1** (Node rank w.r.t. anchor node): _Given an instance \(\!=\!(,)\) and a solution \(\), let \(x_{a}\) be the anchor node, as specified in an S-move. The node rank of \(x_{a}\) (\(x_{u}\)) w.r.t. \(x_{a}\), denoted by \([x_{a},x_{u}]\) or \([a,u]\), is defined as the minimal number of edges in \(\) needed to reach \(x_{u}\) from \(x_{a}\)._

**I-move.** The intermediate move adds a new edge, removes an existing edge, and fixes edge directions, transforming the open Hamiltonian path into a new one. Let \(x_{i}\), \(x_{j}\) be the endpoints of the Hamiltonian

Figure 1: Illustration of using our RDS decoder to determine a 3-opt exchange on TSP-9 given \(K=4\) steps (with the E-move chosen at the final decoding step). The upper portion depicts a visual representation of how the dual-stream attentions (move stream \(\) and edge stream \(\)) are computed. The lower portion demonstrates how the inferred basis moves lead to the modification of the current solution. At step \(\), RDS computes dual-stream attention from representations of historical decisions \(q_{}^{}\), \(q_{}^{}\) to node embeddings \(h_{i}\), thereby deciding a basis move \(_{}(x_{})\) by selecting \(x_{}\). Ghost marks indicate the same location of a cyclic solution when viewed in a flat perspective as in this figure.

path before the I-move (with \([a,i]\!<\![a,j]\)), and let \(e^{}(x_{u}\!\!x_{v})\) be the introduced new edge. To avoid conflicts between consecutive I-moves, we impose sequential conditions on \(e^{}\): (1) its source node \(x_{u}\) must be the endpoint of the current Hamiltonian path with a lower node rank, i.e., \(x_{u}\!=\!x_{i}\), and (2) the node rank of its target node \(x_{v}\), i.e., \([a,v]\), must be higher than the node ranks of the two current endpoints \(x_{i},x_{j}\), i.e., \([a,i]\!<\![a,j]\!<\![a,v]\). The removal of edge \(e^{out}(x_{v}\!\!x_{w})\) followed when \(x_{v}\) is chosen, and directions of the edges between \(x_{j}\) and \(x_{v}\) are reversed to yield a valid Hamiltonian path. Since an I-move can be uniquely determined by \(x_{v}\), we denote it as \(I(x_{v})\).

**E-move.** The ending move adds a new edge connecting the two endpoints of the current Hamiltonian path, converting it into a Hamiltonian cycle, i.e., a new TSP solution \(^{}\). It is executed only at the end of action construction. Since it is uniquely determined without specifying any node, we denote it as \(E(x_{})\). Note that if we relax the condition (2) of I-move to \([a,j]\!\![a,v]\), an E-move can be treated as a _general I-move_ that selects \(x_{v}\!=\!x_{j}\), denoted as \(I^{}(x_{j})\) or \(E(x_{j})\).

**MDP formulations.** The examples of using the above _basis moves_ to factorize 1-opt (void action), 2-opt, 3-opt, and 4-opt are depicted in Appendix A. Next, we present the Markov Decision Process (MDP) formulation \(=(,,,,<1)\) for our NeuOpt. At step \(t\), the **state**\(s_{t}=,_{t},_{t}^{}}\) describes the current instance \(\), the current solution \(_{t}\), and the best-so-far solution \(_{t}^{}\) found before step \(t\). Given a maximum number of allowed _basis moves_\(K(K 2)\), an **action** consists of \(K\)_basis moves_\(_{}(x_{})\), i.e., \(a_{t}\!=\!\{_{}(x_{}),\!=\!1,,K\}\), where the first move \(_{1}(x_{1})=S(x_{1})\) is an S-move, and the rest is either an I-move or an E-move (in the form of general I-move). Note that 1) we permit the I-move as the last move, adding an E-move during state transition if so, and 2) to ensure the action length is always \(K\), we include null actions if E-move early terminates the action. Our **state transition** rule \(\) is deterministic and it updates \(s_{t}\) to \(s_{t+1}\) based on the above action factorization method. We use **reward**\(r_{t}=f(_{t}^{})-min[f(_{t+1}),f(_{t}^{})]\) following [9; 12].

### Recurrent Dual-Stream (RDS) decoder

Our NeuOpt adopts an encoder-decoder-styled policy network. We use the encoder from our previous work  but upgrade the linear projection to an MLP for embedding generation (details are presented in Appendix B). Here, we introduce the proposed recurrent dual-stream (RDS) decoder to effectively parameterize the aforementioned k-opt action factorization (see Figure 1 for a concrete example).

**GRUs for action factorization.** An action \(a\!=\!\{_{}(x_{}),\!=\!1,,K\}\) is constructed sequentially by \(K\) steps, where each decoding step \(\) specifies a basis move type \(_{}\) and a node \(x_{}\) to instantiate the move. Such decoding can be further simplified to a _node selection process_, with move types inferred based on: (1) for \(\!=\!1\), it is an S-move; (2) for \(\!>\!1\), it is an I-move if \([a,j_{}]\!<\![a,]\) (assuming \(x_{i_{}},x_{j_{}}\) are the Hamiltonian path endpoints before the \(\)-th move with \([a,i_{}]\!<\![a,j_{}]\)), otherwise, if \(x_{}\!=\!x_{j_{}}\), it is an E-move that early stops the decoding. Formally, we use factorization:

\[_{}(a|s)=P_{}(_{1}(x_{1}),_{2}(x_{2}),,_{K}(x_ {K})|s)=_{=1}^{K}P_{}^{}(_{}|_{1},, _{-1},s)\] (1)

where \(P_{}^{}\) is a categorical distribution over \(N\) nodes for node selection. Our decoder leverages the Gated Recurrent Units (GRUs)  to help parameterize the conditional probabilities \(P_{}^{}\). Given node embeddings \(h\!\!^{N d}\) from the encoders (\(h_{i}\) is a \(d\)-dimensional vector), the decoder first computes hidden representations \(q^{}\) to model the historical move decisions \(\{_{1},,_{k-1}\}\) (the conditions of \(P_{}^{}\)) using Eq. (2), where \(q^{}\) (hidden state of GRU) is derived from \(q^{-1}\), based on an input \(o^{}\). For better contextual modeling, we consider two streams, \(\) and \(\), which differ from each other by learning independent parameters and taking different inputs \(o^{}\) at each \(\). The \(q_{}^{0}=q_{}^{0}=_{i=1}^{N}h_{i}\) are initialized by the mean pooling of node embeddings (i.e., a graph embedding).

\[q_{}^{}=(o_{}^{},\ q_{}^{-1}), \ q_{}^{}=(o_{}^{},\ q_{}^{ -1})\] (2)

**Dual-stream contextual modeling.** We employ a _move stream_\(\) and an _edge stream_\(\) during contextual modeling and subsequent attention computation. On the one hand, to provide a relatively overall view, the _move stream_\(\) considers modeling historical move decisions by taking the node embedding of the last selected node \(x_{-1}\), which is a representation of the past selected move \(_{-1}(x_{-1})\), as its GRU input, i.e., \(o_{}^{}\!=\!h_{-1}\). On the other hand, to offer a relatively detailed view, the _edge stream_\(\) focuses on edge proposals in each step \(\) by taking the node embedding of \(x_{i_{}}\), which is stipulated to be the source node of the edge to be introduced in step \(\), as its GRU input,

[MISSING_PAGE_FAIL:6]

**Reward shaping.** Moreover, GIRE employs reward shaping:

\[r_{t}^{}=r_{t}+ r_{t}^{}+ r_{t}^{ {bonus}}\] (4)

to guide the reinforcement learning, where \(r_{t}\) is the original reward, \(r_{t}^{}\) regulates extreme exploration behaviours; \(r_{t}^{}\) encourages exploration in the \(\)-feasible regions; and \(\) and \(\) are reward shaping weights (we use 0.05 for both). Here, the regulation \(r_{t}^{}\) is determined by an entropy measure \([P]\) of the estimated conditional transforming probabilities \(P_{t}(|)\!=\!P(^{}\!\!|\! \!)\) and \(P_{t}(|)\!=\!P(^{}\!\!|\! \!)\):

\[r_{t}^{}=-[r_{t}][[P_{t }(|)+[P_{t}(|)]]],\] (5) \[[P]=\{1-c_{1}_{2}[c_{2} eP(1 -P)],0,1\},c_{1}=0.5,c_{2}=2.5,\]

where expectation \([r_{t}]\), that suggests the magnitude of \(r_{t}^{}\), is estimated during training; the entropy measure \([P]\), as shown in Figure 3, imposes larger penalties when \(P\) is either too high or too low (indicating extreme exploration behaviour). The bonus \(r_{t}^{}\) utilizes a similar reward function as the regular reward \(r_{t}\); however, it only considers an infeasible but \(\)-feasible solution as a potential new best-so-far solution. More illustrations and discussions on GIRE designs are detailed in Appendix D.

## 6 Experiments

We conduct experiments on TSP and CVRP, with sizes \(N\!=\!20\), 50, 100 following the conventions [9; 13; 37]. Training and test instances are uniformly generated following . For NeuOpt, we use \(K\!=\!4\); the initial solutions are sequentially constructed in a _random_ fashion for both training and inference. Results were collected using a machine equipped with NVIDIA 2080TI GPU cards and an Intel E5-2680 CPU at 2.40GHz. More hyper-parameter details, discussions, and additional results are available in Appendix E. Our PyTorch code and pre-trained models are publicly available2.

### Comparison studies

**Setup.** In Table 1, we benchmark our NeuOpt (TSP) and NeuOpt-GIRE (CVRP) against a variety of neural solvers, namely, **1) L2P solvers**: _GCN+BS_ (TSP only), _Att-GCN+MCTS_ (TSP only), _GNN+GLS_ (TSP only), _CVAE-Opt-DE_, _DPDP_ (state-of-the-art), _DIMES_ (TSP only), _DIFUSCO_ (TSP only, state-of-the-art); **2) L2C solvers**: _AM+LCP_, _POMO_, _Pointformer_ (TSP only), _Sym-NCO_, _POMO+EAS+SGBS_ (state-of-the-art); and **3) L2S solvers**: _Costa et al._ (TSP only), _Sui et al._ (TSP only), _Wu et al._, _NLNS_ (CVRP only), _NCE_ (CVRP only), _DACCT_ (state-of-the-art). To ensure fairness, we test their publicly available pre-trained models on our hardware and test datasets. Those marked with \(\) are sourced from their original papers due to difficulties in reproducing, among which we find potential issues marked with \(\#\). More implementation details are listed in Appendix E. Following the conventions [5; 9; 34], we report the metrics of objective values and optimality gaps averaged on a test dataset with 10k instances, where the total run time is measured under the premise of using one GPU for neural methods and one CPU for traditional ones. The gaps are computed w.r.t. the exact solver _Concorde_ for TSP and the state-of-the-art traditional solver _HGS_ for CVRP. We also include the _LKH_[20; 51] as baselines. However, we note that it is hard to be absolutely fair when comparing the run time between those CPU-based traditional solvers and GPU-based neural solvers. The baselines are grouped, where the last group comprises variations of our NeuOpt, differentiated by the number of augments (marked as 'D2A=') and the number of inference steps (marked as 'T=').

**TSP results.** Compared to **L2P solvers**, NeuOpt (D2A=1, T=1k) surpasses GCN+BS, CVAE-Opt-DE, and GNN+GLS in all problem sizes with shorter run time. With increased T, NeuOpt continues reducing the gaps, and outshines the state-of-the-art DIFUSCO solver with less time at T=10k steps. The NeuOpt (D2A=5, T=1k), with more augmentations, shows lower gaps than NeuOpt (D2A=1, T=5k) in the same solution search count, where it achieves the lowest gap of 0.00% at (D2A=5, T=5k) on all sizes. Despite the longer run time compared to DPDP and Att-GCN+MCTS, their high efficiency is limited to TSP, and our NeuOpt could be potentially boosted by leveraging heatmaps

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

presents the pointplots with confidence intervals that show the performance of our NeuOpt on TSP-100 with and without E-move during decoding across varying preset \(K\). When E-move is absent (dotted blue line), the model, downgraded to performing fixed \(K\)-opt only, exhibits diminished performance on larger \(K\). Conversely, our NeuOpt (solid orange line) could further benefit from a larger \(K\), due to its flexibility in determining and combining different \(k\) across search steps.

### Generalization and scalability studies

In Table 5 and Table 6, we further evaluate the generalization capability of our NeuOpt models on more complex instances from TSPLIB  and CVRPLIB , respectively. Our NeuOpt achieves lower gaps than DACT  and L2C solvers (AM  and POMO ), and even shows superiority over the AMDKD method  that is explicitly designed to boost the generalization of POMO through knowledge distillation. Beyond generalization, our NeuOpt also exhibits notable scalability. As shown in Table 7, when trained directly for size 200, NeuOpt finds close-to-optimal solutions and still surpasses the strong LKH-3 solver  on CVRP-200. Note that existing L2S solvers, e.g., DACT  may struggle with training on such scales. More details and discussions are available in Appendix E.

### Hyper-parameter studies

**Influence of preset \(K\).** In Table 8, we display the performance of NeuOpt on TSP-100, as well as the corresponding inference time (T=1k) and the training time (per epoch) for varying \(K\) values. The results highlight trade-offs between better performance and increased computational costs.

**Influence of GIRE hyper-parameters.** Figure 6 depicts the influence of \(\) and \(\) in Eq. (4) on CVRP-20, where we fix one while varying the other, investigating both extremely smaller (0.01) and larger (0.1) values. The results suggest that more effective reward shaping occurs when the weights are moderate. Please refer to Appendix D for discussions on more GIRE hyper-parameters.

## 7 Conclusions and limitations

In this paper, we introduce NeuOpt, a novel L2S solver for VRPs, that performs flexible k-opt exchanges with a tailored formulation and a designed RDS decoder. We also present GIRE, the first scheme to transcend masking for constraint handling based on feature supplement and reward shaping, enabling autonomous exploration in both feasible and infeasible regions. Moreover, we devise a D2A augmentation method to boost inference diversity. Despite delivering state-of-the-art results, our work still has **limitations**. While NeuOpt exhibits better scalability than existing L2S solvers, it falls short against some L2P solvers (e.g., [6; 7]) for larger-scale TSPs. Possible solutions in future works include: 1) integrating divide-and-conquer strategies as per [6; 57; 58; 59; 60], 2) reducing search space via heatmaps as predicted in [6; 15], 3) adopting more scalable encoders [61; 62], and 4) refactoring our code with highly-optimized CUDA libraries as did in [7; 15]. Besides enhancing scalability, future works can also focus on: 1) applying our GIRE to more VRP constraints and even beyond L2S solvers, 2) integrating our method with post-hoc per-instance processing boosters (e.g., EAS ) for better performance, and 3) enhancing the generalization capability of our NeuOpt on instances with different sizes/distributions (e.g., by leveraging the frameworks in [11; 63]).

   AM & POMO & AMDKD & DACT (sol.10k) & Ours (sol.10k) \\ -mix & -mix & (POMO) & Avg.\(\) & Best\(\) & Avg.\(\) & Best\(\) \\ 
15.87\% & 8.05\% & 5.77\% & 5.21\% & 3.68\% & 4.80\% & 3.27\% \\   

Table 6: Generalization (10 runs) on CVRPLIB.

    &  &  \\  &  & Time.\(\) & Gap.\(\) & Time.\(\) \\  LKH [20; 51] & 0.00\% & 2.3h & 1.17\% & 21.6h \\  Ours (D2A2+S,T=10k) & 0.04\% & 4.7h & 0.68\% & 9.6h \\ Ours (D2A2+S,T=20k) & 0.02\% & 9.4h & 0.48\% & 19.2h \\ Ours (D2A2+S,T=30k) & 0.01\% & 14.1h & 0.39\% & 1.2d \\   

Table 7: Results on \(N=200\).