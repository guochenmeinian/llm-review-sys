# Tighter Convergence Bounds for Shuffled SGD

via Primal-Dual Perspective

 Xufeng Cai

Department of Computer Sciences

University of Wisconsin-Madison

xcai74@wisc.edu

&Cheuk Yin Lin

Department of Computer Sciences

University of Wisconsin-Madison

cylin@cs.wisc.edu

Equal contribution

Jelena Diakonikolas

Department of Computer Sciences

University of Wisconsin-Madison

jelena@cs.wisc.edu

###### Abstract

Stochastic gradient descent (SGD) is perhaps the most prevalent optimization method in modern machine learning. Contrary to the empirical practice of sampling from the datasets _without replacement_ and with (possible) reshuffling at each epoch, the theoretical counterpart of SGD usually relies on the assumption of _sampling with replacement_. It is only very recently that SGD using sampling without replacement - shuffled SGD - has been analyzed with matching upper and lower bounds. However, we observe that those bounds are too pessimistic to explain often superior empirical performance of data permutations (sampling without replacement) over vanilla counterparts (sampling with replacement) on machine learning problems. Through fine-grained analysis in the lens of primal-dual cyclic coordinate methods and the introduction of novel smoothness parameters, we present several results for shuffled SGD on smooth and non-smooth convex losses, where our novel analysis framework provides tighter convergence bounds over all popular shuffling schemes (IG, SO, and RR). Notably, our new bounds predict faster convergence than existing bounds in the literature - by up to a factor of \(O()\), mirroring benefits from tighter convergence bounds using component smoothness parameters in randomized coordinate methods. Lastly, we numerically demonstrate on common machine learning datasets that our bounds are indeed much tighter, thus offering a bridge between theory and practice.

## 1 Introduction

Originally proposed in , SGD has been broadly studied in the machine learning literature due to its effectiveness in large-scale settings, where full gradient computations are often computationally prohibitive. When applied to unconstrained finite-sum problems

\[_{^{d}}f(),\ \ \ \ f():= _{i=1}^{n}f_{i}(),\] (P)

SGD performs the update \(_{t}=_{t-1}- f_{i_{t}}(_{t-1})\) for \(i_{t}[n]\) (\([n]:=\{1,,n\}\)), in each iteration \(t\). Traditional theoretical analysis for SGD builds upon the assumption of sampling \(i_{t}[n]\) with replacement according to a fixed distribution \(=(p_{1},,p_{n})^{}\) over \([n]\), which leads to\(_{i_{t}}[ f_{i_{t}}(_{t-1})/(np_{i_{t}})]= f(_{t- 1})\), and thus much of the (deterministic) gradient descent-style analysis can be transferred to this setting. By contrast, no such connection between the component and the full gradient can be established for shuffled SGD -- which employs sampling _without replacement_ -- making its analysis much more challenging. As a result, despite its fundamental nature, there were no non-asymptotic convergence results for shuffled SGD until a very recent line of work [2; 12; 20; 21; 30; 31; 34; 35; 42]. All existing results consider general finite sum problems, with the same regularity condition constant (Lipschitz constant of \(f_{i}\) or its gradient) assumed for all the component functions. As a result, the obtained convergence bounds are typically no better than for (full) gradient descent, and are only better than the bounds for SGD with replacement sampling if the algorithm is run for many full passes over the data [30; 34].

Furthermore, there is a large gap between the empirical performance of shuffled SGD and the predicted convergence rates from prior work [20; 30]. One cause for this discrepancy are overly pessimistic bounds on the step size in prior work, which are of order \(1/(nL_{})\), where \(L_{}\) is the maximum smoothness constant over components \(f_{i}\) in (P). In practice, the step sizes are tuned to achieve better convergence bounds than predicted by the current theory. We illustrate how restrictions on the step size affect convergence of shuffled SGD (with random permutations in each epoch) in Fig. 1, where we plot the resulting optimality gap over full data passes when shuffled SGD is applied to logistic regression problems on standard datasets. To compare the effect of the step size \(\) from prior work and our work, we choose take \(=1/(nL_{})\) based on , and \(=1/(n})\) from our work, where \(,\) are our novel fine-grained, data-dependent smoothness parameters defined in Section 3 for smooth convex finite-sum problems with linear predictors. As can be observed from Fig. 1, larger step sizes resulting from our theory lead to faster convergence of shuffled SGD and, as a result, our convergence bounds better predict the performance of shuffled SGD.

Building on these insights, we introduce a fined-grained theoretical analysis to transparently show how the structure of the data and the possibly different Lipschitz constants of the component functions or their gradients affect the performance of shuffled SGD, thus providing a better explanation of the heuristic success of shuffled SGD in modern machine learning.

### Background and related work

SGD (with replacement) has been extensively studied in many settings (see e.g., [1; 9; 10; 38] for convex optimization). Compared to SGD, shuffled SGD usually exhibits faster convergence in practice [8; 37], and is easier and more efficient to implement . For each epoch \(k\), shuffled SGD-style algorithms perform incremental gradient updates based on the sample ordering (permutation of the data points) denoted by \(^{(k)}\). There are three main choices of data permutations: (i) \(^{(k)}\) for some fixed permutation of \([n]\) for all epochs, where shuffled SGD reduces to the incremental gradient (IG) method; (ii) \(^{(k)}\) where \(\) is randomly chosen only once, at the beginning of the first epoch, referred to as the shuffle-once (SO) scheme; (iii) \(^{(k)}\) randomly generated at the beginning of each epoch, referred to as random reshuffling (RR).

For general smooth convex settings, the convergence of shuffled SGD has been established only recently. For the number of epochs \(K\) sufficiently large,  proved a convergence rate

Figure 1: An illustration of the convergence behaviour of shuffled SGD for logistic regression problems on LIBSVM datasets luke, leu and a9a, where we use step sizes from existing bounds and our work. Due to randomness, we average over \(20\) runs for each plot and include a ribbon around each line to show its variance. However, as suggested by the concentration of \(\) (see Section 4.1 and Appendix E), the variance across multiple runs is negligible, hence the ribbons are not observable.

\((1/)\) for RR, which leads to the complexity matching SGD. This result was later improved to \((1/(n^{1/3}K^{2/3}))\) by [12; 30; 34] for \(K\) sufficiently large and with bounded variance assumed at the minimizer, while the same rate holds for SO . These results were complemented by matching lower bounds in , under sufficiently small step sizes as utilized in prior work. The results in [30; 34] require restricted \((1/(nL))\) step sizes and reduce to \((1/K)\) for small \(K\), acquiring the same iteration complexity as full-gradient methods. Unlike in strongly convex settings, we are not aware of any follow-up work with improvements under small \(K\) for smooth convex settings.

The major difficulty in analyzing shuffled SGD comes from characterizing the difference between the intermediate iterate and the iterate after one full data pass, for which current analysis (see e.g.,  in smooth convex settings) uses the global smoothness constant with a triangle inequality. Such a bound may be too pessimistic and fail capturing the nuances of intermediate progress of shuffled SGD, which leads to a small step size and large \(K\) restrictions. To provide a more fine-grained analysis that narrows the theory-practice gap for shuffled SGD, we notice that such a proof difficulty is reminiscent of the analysis of cyclic block coordinate methods relating the partial gradients to the full one. This natural connection was further emphasized in studies of cyclic methods with random permutations [24; 47]; however, these results were limited to convex quadratics. More generally, it is possible to interpret shuffled SGD as a primal-dual method performing cyclic updates on the dual side (see (PD) in Section 2.1 and (PL-PD) in Section 3). We note here that prior work on dual coordinate methods  provided theoretical guarantees only for the algorithms that choose the dual coordinate to optimize uniformly at random, while the cyclic variant (related to shuffled SGD) had only been studied numerically up until this work. Further discussion of related work appears in Appendix A.

### Contributions

In this work, we study the convergence rates of shuffled SGD in various settings through a unified primal-dual perspective, making intriguing connections to cyclic coordinate methods. This analysis framework is novel and allows us to leverage cyclic bias accumulation techniques on the dual side to obtain fine-grained convergence bounds. The obtained bounds mirror the improvements in randomized coordinate methods, which come from different coordinate smoothness parameters. While coordinate methods are no better than full-gradient methods in the worst case, on typical problem instances, they are much faster and the improvements come precisely from a more fine-grained view of smoothness. We see a similar phenomenon in our analysis, which highlights the usefulness of the fine-grained smoothness characterizations introduced in our work.

We provide improved bounds for all three popular data permutation strategies RR, SO and IG, in smooth convex settings. When the problem objective narrows to empirical risk minimization with linear predictors, we are able to exploit the data-dependent structure and uncouple the linear and nonlinear parts of the objective function, allowing us to provide tighter data-dependent bounds, up to a factor of \(O()\). Moreover, we show that our techniques extend to non-smooth convex settings, providing improved bounds over existing work.

We summarize our results and compare them to the state of the art in Table 1. As is standard, all complexity results in Table 1 are expressed in terms of individual (component) gradient evaluations. They represent the number of gradient evaluations required to construct a solution with (expected) optimality gap \(\), given a target error \(>0\).

Extensions to mini-batching and IG.When presenting our results for general finite-sum problems (in Section 2), we consider simple updates without mini-batching for ease of presentation and to avoid introducing excessive notation. However, we emphasize that all our results can be extended to shuffled SGD with mini-batching. Our results are also the first to provide convergence bounds that demonstrate benefits of mini-batching in shuffled SGD. For completeness and generality, the proofs in the appendix are carried out for mini-batch settings with arbitrary batch sizes \(b\{1,,n\}\). Thus, all the results stated in Section 2 can be recovered by setting \(b=1\). Moreover, our framework can provide similar fine-grained convergence bounds for IG. However, as IG is not as commonly used in practice compared to RR and SO and due to space constraints, we only present our results for RR and SO in the main body and include the results for IG in the appendix.

### Notation

We consider a real \(d\)-dimensional Euclidean space \((^{d},\|\|)\) where \(d\) is finite and \(\|\|\) is the \(_{2}\)-norm. For a vector \(\), we let \(^{j}\) denote its \(j\)-th coordinate. For any positive integer \(m\), we use \([m]\) to denote the set \(\{1,2,,m\}\). Given a matrix \(\), \(\|\|:=_{^{d},\|\| 1}\|\|\) denotes its operator norm. For a positive definite matrix \(\), \(\|\|_{}\) denotes the Mahalanobis norm, \(\|\|_{}:=,}\). We use \(\) to denote the identity matrix, and \(()\) to denote the diagonal matrix with vector \(\) on the main diagonal. For any \(j[n]\), we define \(_{j}\) as the matrix obtained from the identity matrix \(\) by setting the first \(j\) diagonal elements to zero, and let \(_{j}\) be the matrix with only the \(j\)-th diagonal element nonzero and equal to \(1\). To handle the cases with random data permutations, we use the following definitions corresponding to the data permutation \(=\{^{1},^{2},,^{n}\}\) of \([n]\): \(_{}:=_{_{1}},_{_{2}},,_{_{n} }^{}\) permuting the rows based on \(\) given a matrix \(=[_{1},_{2},,_{n}]^{}\), and \(_{}:=^{_{1}},^{_{2}},,^{_{n} }^{}\) permuting the coordinates/subvectors based on \(\) given a vector \(=(^{1},^{2},,^{n})^{}\).

## 2 Primal-Dual Framework for Smooth Convex Finite-Sum Problems

Throughout this section, we make the following standard assumptions.

**Assumption 1**.: _Each \(f_{i}\) is convex and \(L_{i}\)-smooth, and there exists a minimizer \(_{*}^{d}\) for \(f()\)._

Assumption 1 implies that \(f\) and all component functions \(f_{i}\) are \(L\)-smooth, where \(L_{}:=_{i[n]}L_{i}\). It also implies that each convex conjugate \(f_{i}^{*}\) is \(}\)-strongly convex . In this section, we define \(=(,,L_{1}}_{d},, ,,L_{n}}_{d})^{nd nd}\), and slightly abuse the notation to use \(_{}=},,L_{^{ 1}}}_{d},,},,L_{^{n}}}_{d}\) given a permutation \(\) of \([n]\). For the permutation \(_{k}\) at the \(k\)-th epoch, we denote \(_{k}=_{_{k}}\), for brevity.

We further assume that the variance at \(_{*}\) is bounded, same as prior work [30; 34].

**Assumption 2**.: _The quantity \(_{*}^{2}=_{i=1}^{n}\| f_{i}(_{*})\|^{2}\) is bounded._

### Primal-dual view of shuffled SGD

Problem (P) can be reformulated into a primal-dual form using the standard Fenchel conjugacy argument (see, e.g., [13; 14]),

\[_{^{d}}_{^{nd}} (,):=_{i=1}^{n}<^{i}, >-f_{i}^{*}(^{i})},\] (PD)

where we slightly abuse the notation to denote \(=(^{1},,^{n})^{}^{nd}\) and \(f_{i}^{*}\) is the convex conjugate of \(f_{i}^{*}()=_{^{d}}<, >-f_{i}()}\). We let \(_{}=(_{}^{1},,_{}^{n})^{} ^{nd}\) be the conjugate pair of \(^{d}\), i.e., \(_{}^{i}=_{^{d}}\{<, >-f_{i}^{*}()\}\), and we denote \(_{}=_{}\).

Given a primal-dual pair \((,)\), the primal-dual gap of (PD) is defined by \((,)=_{<,>}\{(,)-(,)\}\). In particular, we consider the pair \((,_{})\) for \(^{d}\), and bound \(^{}(,_{}):=(,)- (_{},_{})\) for an arbitrary but fixed \(\). To finally obtain the function value gap \(f()-f(_{})\) for (P), we only need to choose \(=_{}(,)=_{}\).

Using this primal-dual formulation and standard convex conjugacy arguments, we can _equivalently_ write the standard shuffled SGD algorithm in a primal-dual form as summarized in Algorithm 1.

```
1:Input: Initial point \(_{0}^{d}\), step size \(\{_{k}\}>0,\) number of epochs \(K>0\)
2:for\(k=1\) to \(K\)do
3: Generate some permutation \(_{k}\) of \([n]\) (either deterministic or random)
4:\(_{k-1,1}=_{k-1}\)
5:for\(i=1\) to \(n\) in the ordering of \(_{k}\)do
6:\(_{k}^{i}=_{^{i}^{d}}<^{i},_{k-1,i}>-f_{i}^{*}(^{i})}\)
7:\(_{k-1,i+1}=_{^{d}}<_{k}^{i},>+}\|-_{k-1,i}\|^{2}}= _{k-1,i}-_{k} f(_{k-1,i})\)
8:endfor
9:\(_{k}=_{k-1,n+1}\), \(_{k}=(_{k}^{1},_{k}^{2},,_{k}^{n})^{}\)
10:endfor
11:Return:\(}_{K}=_{k=1}^{K}_{k}_{k}/_{k=1}^{K}_{k}\) ```

**Algorithm 1** Shuffled SGD (Primal-Dual View, General Convex Smooth)

Improved bounds with new smoothness constants.To prove a convergence bound for shuffled SGD in this general setting, we first construct an upper estimate of \(^{}(_{k},_{})\) for some fixed \(\) to be set later, as summarized in the following lemma.

**Lemma 1**.: _Under Assumption 1, for any \(k[K]\), the iterates \(\{_{k}^{i}\}_{i=1}^{n}\) and \(\{_{k-1,i}\}_{i=1}^{n+1}\) generated by Algorithm 1 satisfy_

\[_{k}&\ }{n}_{i=1}^{n}<_{k}^{i},_{k}-_{k-1,i+1 }>+}{n}_{i=1}^{n}<_{k}^{i}-_{k}^{i}, _{k}-_{k-1,i}>\\ &-}{2n}\|_{k}-_{k}\|_{_{k}^{-1}}^{2 }-}{2n}\|_{k}-_{,k}\|_{_{k}^{-1}}^{2}- _{i=1}^{n}\|_{k-1,i+1}-_{k-1,i}\|^{2},\] (1)

_where \(_{k}:=_{k}^{}(_{k},_{})+\|_{}-_{k}\|_{2}^{2}-\|_{}-_{k-1}\|_{2}^{2}\), \(_{k}=_{^{(k)}}\), and \(_{,k}=_{,^{(k)}}\) are the (block-wise) permuted vectors based on the permutation \(_{k}\) at the \(k\)-th epoch._

We note that the first term \(_{1}:=}{n}_{i=1}^{n}<_{k}^{i},_{k}- _{k-1,i+1}>\) from Lemma 1 can be aggregated into the terms capturing the primal progress within one epoch and cancelled by the last term in Eq. (1). The precise bound on \(_{1}\) and its proof are provided in Lemma 10 in Appendix C.1. The second term \(_{2}:=}{n}_{i=1}^{n}<_{k}^{i}-_{k }^{i},_{k}-_{k-1,i}>\) requires us to relate the intermediate iterate \(_{k-1,i}\) to the iterate \(_{k}\) after one full data pass, which corresponds to a partial sum of the component gradients, each at different iterates \(\{_{k-1,j}\}_{i=i}^{n}\). In contrast to prior analyses (e.g., Mishchenko et al. ) using the global smoothness and triangle inequality to bound this partial sum, we provide a tighter bound on \(_{2}\) that tracks the progress of the cyclic update on the dual side, in the aggregate.

To simplify the notation in the following lemmas and to clearly compare our results, we introduce the following novel definitions of smoothness constants for shuffled SGD:

\[_{}^{g}&:=} _{}^{1/2}_{i=1}^{n}_{d(i-1) }^{}_{d(i-1)} _{}^{1/2}_{2},&^{g} =_{}_{}^{g},\\ _{}^{g}&:=_{ }^{1/2}_{i=1}^{n}_{(di)} ^{}_{(di)}_{}^{ 1/2}_{2},&^{g}=_{}_{}^{g}, \] (2)

where \(_{(di)}=_{j=d(i-1)+1}^{di}_{j}\) and \(=[_{d},,_{g}}_{n}] ^{}^{nd d}\). Permutation-dependent quantities \(_{}^{g}\) and \(_{}^{g}\) defined in (2) are obtained directly from our analysis. We remark that \(^{g}\) is bounded by the average smoothness of \(f\) and \(^{g}\) is bounded by the max of individual smoothness constants of \(f_{i}\); see more details in Appendix B. However, as we argue in later sections, these upper bounds on \(_{}^{g}\) and \(_{}^{g}\) are loose in general, and so the convergence bounds based on \(_{}^{g}\) and \(_{}^{g}\) that we obtain align better with the empirical performance of shuffled SGD.

Assuming that a uniformly random data shuffling strategy is used (SO or RR), the resulting bound on \(_{2}\) is summarized in Lemma 2, while its proof is deferred to Appendix B.

**Lemma 2**.: _Under Assumptions 1 and 2, for any \(k[K]\), the iterates \(\{_{k}^{i}\}_{i=1}^{n}\) and \(\{_{k-1,i}\}_{i=1}^{n+1}\) generated by Algorithm 1 with uniformly random shuffling (RR/SO) satisfy_

\[[_{2}]_{k}^{3}n_{^{(k )}}^{g}_{^{(k)}}^{g}\|_{k}-_{*,k}\|_{ _{k}^{-1}}^{2}+}{2n}\|_{k}- _{k}\|_{_{k}^{-1}}^{2}+^{3}(n+1)^{g}}{6}_{*}^{2},\]

_where \(_{2}:=}{n}_{i=1}^{n}_{ k}^{i}-_{k}^{i},_{k}-_{k-1,i}\), \(_{k}=_{^{(k)}}\) and \(_{*,k}=_{*,^{(k)}}\)._

With Lemmas 1 and 2 in tow, we are ready to present the main result of this section.

**Theorem 1**.: _Under Assumptions 1 and 2, if \(_{k}^{g}(k)}_{^{(k)}}^{g}}\) and \(H_{K}=_{k=1}^{K}_{k}\), the output \(}_{K}\) of Algorithm 1 with uniformly random (RR/SO) shuffling satisfies_

\[[H_{K}(f(}_{K})-f(_{*}))] \|_{0}-_{*}\|_{2}^{2}+_{k=1}^{K} ^{3}(n+1)^{g}}{6}_{*}^{2}.\]

_As a consequence, for any \(>0,\) there exists a choice of a constant step size \(_{k}=\) for which \([f(}_{K})-f(_{*})]\) after \(^{g}\|_{0}- _{*}\|_{2}^{2}}}{}+^{g} _{*}\|_{0}-_{*}\|_{2}^{2}}}{^{3/2}} \) individual gradient queries._

## 3 Tighter Bounds for Convex Finite-Sum Problems with Linear Predictors

To study the effect of the structure of the data on the convergence of shuffled SGD, we sharpen the focus from general finite-sum problems to convex finite-sum with linear predictors:

\[_{^{d}}f():= _{i=1}^{n}_{i}(_{i}^{})},\] (PL)

where \(_{i}^{d}\) (\(i[n]\)) are data vectors and \(_{i}:\) are convex and either smooth or Lipschitz nonsmooth functions associated with the linear predictors \(_{i},\) for \(i[n]\). In addition to their explicit dependence on the data, it is worth noting that problems of the form (PL) cover most of the standard convex ERM problems where shuffled SGD is commonly applied, such as support vector machines, least absolute deviation, least squares, and logistic regression.

Problem (PL) admits an explicit primal-dual formulation using the standard Fenchel conjugacy argument (see, e.g., [13; 14]),

\[_{^{d}}_{^{n}} (,):= ,-_{i=1}^ {n}_{i}^{*}(^{i})=_{i=1}^{n} {a}_{i}^{}^{i}-_{i}^{*}(^{i}) },\] (PL-PD)

where \(=[_{1},_{2},,_{n}] ^{}^{n d}\) is the data matrix and \(_{i}^{*}:\) is the convex conjugate of \(_{i}\). This observation allows us to again interpret without-replacement SGD updates as cyclic coordinate updates on the dual side. Note that due to the objective structure in (PL), the primal-dual formulation (PL-PD) can decouple the linear (\(_{i}^{}\)) and the non-linear (\(_{i}\)) parts within individual loss functions \(f_{i}\). We redefine the conjugate pair of \(^{d}\) to be \(_{}=(_{}^{1},,_{}^{n})^{} ^{n}\), with \(_{}^{i}=_{^{i}}\{^{i}_{i}^ {}-_{i}^{s}(^{i})\}\).

In this section, we consider shuffled SGD with _mini-batch_ estimators of size \(b\) and assume without loss of generality that \(n=bm\) for some positive integer \(m\). The detailed primal-dual view of shuffled SGD adapted to (PL-PD) and mini-batch estimators is provided in Alg. 2 in Appendix C.

### Smooth and convex objectives

Throughout this subsection, we make the following (standard) assumptions, corresponding to Assumptions 1 and 2 from Section 2.

**Assumption 3**.: _Each \(_{i}\) is convex and \(L_{i}\)-smooth \((i[n])\), i.e., \(|_{i}^{}(x)-_{i}^{}(y)| L_{i}|x-y|\) for any \(x,y\). There exists a minimizer \(_{*}_{^{d}}f()\)._

We remark that Assumption 3 implies that both \(f\) and each component function \(f_{i}()=_{i}(_{i}^{})\) are \(L_{}\)-smooth, where \(L_{}=_{i[n]}L_{i}\|_{i}\|_{2}^{2}\). Assumption 3 also implies that each convex conjugate \(_{i}^{*}\) is \(}\)-strongly convex . In the following, we let \(=(L_{1},L_{2},,L_{n})\), and \(_{}=L_{^{1}},L_{^{2}},,L_{^{n}} \), given a permutation \(\) of \([n]\).

We further assume bounded variance at \(_{*}\), same as prior work .

**Assumption 4**.: \(_{*}^{2}:=_{i=1}^{n}\| f_{i}(_{*})\|^{2}= _{i=1}^{n}(_{i}^{}(_{i}^{}_{*}))^{2} \|_{i}\|_{2}^{2}\) _is bounded._

Improved bounds with new smoothness constants.Our convergence bounds depend on the smoothness parameters defined in Eq. (3) below. We provide a detailed discussion on how these parameters relate to traditional smoothness parameters both in the worst case and on typical datasets, in Section 4.1, with additional numerical results provided in Appendix E.

\[_{}&:= _{}^{1/2}_{j=1}^{m}_{b(j-1)}_{ }_{}^{}_{b(j-1)}_{}^{1/2} _{2}, 28.452756pt=_{}_{},\\ _{}&:=_{ }^{1/2}_{j=1}^{m}_{(j)}_{}_{}^{}_{(j)}_{}^{1/2}_{2}, 56.905512pt=_{} _{},\] (3)

where \(_{(j)}:=_{i=b(j-1)+1}^{bj}_{i}\). In comparison to the smoothness constants defined in Eq. (2) for general finite-sum problems, we note that the constants in Eq. (3) applying to generalized linear models are tighter and more informative estimates, as the data matrix \(\) and the smoothness constants from the nonlinear part \(\) are separated in Eq. (3). Thus, the constants \(_{}\) and \(_{}\) directly depend on the data matrix, which explicitly demonstrates how the structure of the data affects the convergence of shuffled SGD. The following theorem states the convergence of Algorithm 2 with these new refined smoothness constants, while its proof is provided in Appendix C.

**Theorem 2**.: _Under Assumptions 3 and 4, if \(_{k}}L_{^{(k)}}}}\) and \(H_{K}=_{k=1}^{K}_{k}\), then the output \(}_{K}\) of Alg. 1 with uniformly random (RR/SO) shuffling satisfies_

\[[H_{K}(f(}_{K})-f(_{*}))]\|_{0 }-_{*}\|_{2}^{2}+_{k=1}^{K}^{3}(n-b)(n+b)}{6 b^{2}(n-1)}_{*}^{2}.\]

_As a result, given \(>0\), there exists a constant step size \(_{k}=\) such that \([f(}_{K})-f(_{*})]\) after \(}\|_{0}-_{*}\|_{2 }^{2}}{}+}_{*}\| _{0}-_{*}\|_{2}^{2}}\) individual gradient queries._

A few remarks are in order here. When \(b=n\), we recover the standard guarantee of gradient descent, which serves as a sanity check as in this case the algorithm reduces to standard gradient descent. When \(=(^{2}}{n^{2}(n-1)L})\), the resulting complexity is \(}\|_{0}-_{*}\|_{2 }^{2}}{}\). Observe that this case can happen when either \(\) is large (compared to, say, \(1/n\)) or when \(_{*}\) is small (it is, in fact, possible for \(_{*}\) to be zero, which happens, for example, when the data rows are linearly independent). Unlikein bounds from previous work, we observe from our bounds the benefit of using shuffled SGD compared to full gradient descent, where the difference is by a factor that can be as large as \(\), as we have discussed in the introduction (see also Section 4). When \(=(}{n^{2}(n-1)L}),\) the second term in our complexity bound dominates. In this case, when \(b=1\), we recover the state of the art results from [12; 30; 34], while for \(b>1\) our bound provides the \(}\)-factor improvement, providing insights into benefits from the mini-batching strategy commonly used in practice.

### Extension to non-smooth convex objectives

In non-smooth settings, we make the following standard assumption.

**Assumption 5**.: _Each \(_{i}\) is convex and \(G_{i}\)-Lipschitz \((i[n])\), i.e., \(|_{i}(x)-_{i}(y)| G_{i}|x-y|\) for any \(x,y\); thus \(|g_{i}(x)| G_{i}\) where \(g_{i}(x)_{i}(x)\). There exists a minimizer \(_{*}_{^{d}}f()\)._

If Assumption 5 holds, each \(_{i}(_{i}^{})\) is also \(G_{}\)-Lipschitz with respect to \(\), where \(G_{}=_{i[n]}G_{i}\|_{i}\|_{2}\). To state our results, we define \(:=(G_{1}^{2},G_{2}^{2},,G_{n}^{2})\) and \(_{}=G_{_{1}}^{2},G_{_{2}}^{2},,G_{ _{n}}^{2}\), given a data permutation \(\) of \([n]\).

We now extend our analysis of Algorithm 1 to convex nonsmooth Lipschitz settings, where the conjugate functions \(_{i}^{*}(y^{i})\) are only convex. Proceeding as in Lemma 1, we obtain a bound on the primal-dual gap similar to (1), but lose two retraction terms induced by smoothness. Instead of cancelling the corresponding error terms like in the smooth case, we rely on the boundedness of the subgradients to bound these terms under a sufficiently small step size, which is common in nonsmooth Lipschitz settings. Similar to Section 2, we introduce the following quantities to obtain a tighter guarantee with respect to the data matrix and Lipschitz constants

\[_{} :=_{}^{1/2}_{j=1}^{m }_{b(j-1)}_{}_{}^{}_{b(j-1) }_{}^{1/2}_{2},\] \[_{} :=_{}^{1/2}_{j=1}^{m }_{(j)}_{}_{}^{}_{(j)}_{ }^{1/2}_{2}.\]

We discuss the improvements in convergence from \(_{}\) and \(_{}\) in Section 4, while the convergence of Algorithm 2 is described in Theorem 3, with its proof deferred to Appendix D.

**Theorem 3**.: _Under Assumption 5, if \(H_{K}=_{k=1}^{K}_{k}\) and \(=_{}[_{}_{}}]\), the output \(}_{K}\) of Alg. 1 with possible uniformly random shuffling satisfies_

\[[H_{K}(f(}_{K})-f(_{*}))]\|_ {0}-_{*}\|_{2}^{2}+_{k=1}^{K}2_{k}^{2}n,\]

_As a result, for any \(>0,\) there exists a step size \(_{k}=\) such that \([f(}_{K})-f(_{*})]\) after \(\|_{0}-_{*}\|_{2}^{2}}{^{2}} \) individual gradient queries._

## 4 Discussion of Our New Smoothness Constants and Numerical Results

To succinctly explain where our improvements come from, we now consider (PL) where \(_{i}\) is \(1\)-smooth and \(b=1\), ignoring the gains from the mini-batch estimators (for large \(K\)) and our softer guarantee that handles individual smoothness constants. For this specific case, \(=L_{}=_{1 i n}\|_{i}\|^{2},\) and thus our results for the smooth case and the RR and SO variants match state of the art in the second term, which dominates when there are many (\(K=^{2}D^{2}n}{_{*}^{2}}\)) epochs. When there are \(K=O(^{2}D^{2}n}{_{*}^{2}})\) epochs in the SO and RR variants or for all regimes of \(K\) in the IG variant, the difference between our and state of the art bounds comes from the constant \(\) that replaces \(L_{}\), and our improvement is by a factor \(/}\). Note that \((}{})\) from prior bounds, which is the dominating term in the small \(K\) regime, is even worse than the complexity of full gradient descent, as the full gradient Lipschitz constant of \(f\) in this case is \(\|^{}\|_{2} L_{}\).

Given a worst-case permutation \(\), and denoting by \(_{}\) the data matrix \(\) with its rows permuted according to \(\), our constant \(\) can be bounded above by \(L_{}\) using the following sequence of inequalities:

\[=}\|_{j=1}^{n}_{(j-1)}_{}_{}^{}_{(j-1)}\|_{2} }_{j=1}^{n}\|_{(j-1) }_{}_{}^{}_{(j-1)}\|_{2}\] \[}_{j=1}^{n}\|_{}_{}^{}\|_{2}\] (4) \[_{i=1}^{n}\|_{i}\|_{2} ^{2}_{1 i n}\|_{i}\|_{2}^{2}=L_{},\]

where \((i)\) holds by the triangle inequality, \((ii)\) holds because the operator norm of the matrix \(_{(j-1)}_{}_{}^{}_{(j-1)}\) (equal to the operator norm of the bottom right \((n-j+1)(n-j+1)\) submatrix of \(_{}_{}^{}\)) is always at most \(\|_{}_{}^{}\|=\|^{}\|\), for any permutation \(\), and \((iii)\) holds by bounding above the operator norm of a symmetric matrix by its trace. Hence \(\) is never larger than \(L_{}\), but can generally be much smaller, due to the sequence of inequality relaxations in (4). While each of these inequalities can be loose, we emphasize that \((iii)\) is almost always loose, by a factor that can be as large as \(n\).

As a specific example where \(\) is smaller than \(L_{}\) by a factor of \(n\), consider the example of Gaussian data, where we draw \(n\) i.i.d. standard Gaussian vectors from \((,_{d})\) and take \(d=n\). By standard concentration results, with high probability, all columns/rows of \(_{}\) in this case are near-orthogonal (see, e.g., [7, Chapter]) and \(\|_{i}\|_{2}^{2} d=n\) for all \(i\). As a result, the operator norm to trace inequality (\(iii\)) is loose by a factor \(d=n\), with high probability. Note that in this example all individual smoothness parameters of components \(f_{i}\) are essentially the same (w.h.p.) and equal \(\|_{i}\|_{2}^{2}\), thus the improvement of our bound on the smoothness parameter does not come from averaging but from the structure of the data. This observation is important for contrasting the results from Section 2 and Section 3. In particular, focusing solely on the finite sum structure and ignoring the structure of the data matrix would provide no improvements in the resulting convergence bounds.

As further evidence, we empirically evaluate \(L_{}/\) on 15 large-scale machine learning datasets and demonstrate that on those datasets \(L_{}/\) is of the order \(n^{},\) for \([0.15,0.96]\) (see Sec. 4.1 for more details), providing strong evidence of a tighter guarantee as a function of \(n\).

For the nonsmooth settings, by a similar sequence of inequalities, we can show that \( G_{}^{2},\) which can be loose by a factor \(1/n\) due to the operator norm to trace inequality. Thus, our bound is never worse than what would be obtained from the full subgradient method, but can match the bound of standard SGD, or even improve1 upon it for at least some data matrices \(\).

### Numerical results and discussion

In this section, we provide empirical evidence to support our claim about usefulness of the new convergence bounds obtained in our work. In particular, we conduct numerical evaluations to compare \(\) to the classical smoothness constant \(L\) on synthetic datasets and on popular machine learning benchmark datasets.

For a more streamlined comparison and to focus on the dependence on the data matrix, we assume that the loss functions \(_{i}\) all have the same smoothness constant, which leads to \(L_{}/=(_{1 i n}\{\|_{i}\|^{2}\})/(} \|_{j=1}^{n}_{(j-1)}_{}_{}^{ }_{(j-1)}\|_{2})\). Since the scale of the smoothness constant of the loss functions is irrelevant for the ratio \(L_{}/\) in this case, for simplicity, we take it to equal one. Note that assuming different smoothness constants over component loss functions would only make our bound better compared to related work (see Eq. (3) and the discussion following it).

We also compare \(\) and \(L_{}\) on a number of benchmarking datasets from LIBSVM , MNIST , CIFAR10 , and Broad Bioimage Benchmark Collection . For each dataset, we generate a uniformly random permutation \(\) for the data matrix \(\) and compute \(_{}\). We repeat this procedure \(1000\) times for all datasets and display the average \(L_{}/_{}\) in Table 2, except for e2006train, CIFAR10, MNIST, and BBBC005 where we do \(20\) repetitions due to limitations of computation resources required for each calculation. We observe that among the datasets that we consider, which contain all three data matrix "shapes" \(d>>n\), \(d<<n\), and \(d n\), our novel bound dependent on \(\) is much tighter. For instance, for rcv1 and real-sim datasets, where \(d\) and \(n\) are of the same order, we observe that \(L_{}/\) are approximately \(111\) and \(194\), respectively. For news20 dataset where \(d>>n\), \(L_{}/ 42.1\). For MNIST, where \(d<<n\), \(L_{}/ 19.1\). Further results are provided in Appendix E.

Finally, as a justification for using the empirical mean of \(_{}\) over random permutations \(\) in the results displayed in Table 2, we observe in our evaluations that the values of \(L_{}/_{}\) are fairly concentrated around their empirical mean values. Histogram plots showing the empirical distributions of \(L_{}/_{}\) for each of the datasets are provided in Appendix E.

We conclude with a few additional remarks. Our results indicate that the structure of the data is important for predicting behavior of popular machine learning methods such as variants of shuffled SGD considered in our work, and thus should be incorporated in their study: as demonstrated in the Gaussian data example, considering simple finite sum structure and ignoring the dependence on the data can lead to overly pessimistic bounds. Thus it would be interesting to provide a further theoretical study of shuffled SGD that incorporates distributional assumptions for the data. Additionally, as mentioned in the previous paragraph, we empirically observed that permutation-dependent parameter \(_{}\) concentrates around its mean for permutations generated uniformly at random. Thus, it would be interesting to consider whether our theoretical results can be strengthened to depend on the mean value of \(_{}\) (as opposed to maximum). We leave such considerations for future work.