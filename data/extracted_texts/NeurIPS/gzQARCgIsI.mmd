# End-To-End Causal Effect Estimation

from Unstructured Natural Language Data

 Nikita Dhawan

University of Toronto, Vector Institute

nikita@cs.toronto.edu

&Leonardo Cotta

Vector Institute

leonardo.cotta@vectorinstitute.ai

&Karen Ullrich

Meta AI

karenu@meta.com

&Rahul G. Krishnan

University of Toronto, Vector Institute

rahulgk@cs.toronto.edu

&Chris J. Maddison

University of Toronto, Vector Institute

cmaddis@cs.toronto.edu

Work done during internship at Meta AI.

###### Abstract

Knowing the effect of an intervention is critical for human decision-making, but current approaches for causal effect estimation rely on manual data collection and structuring, regardless of the causal assumptions. This increases both the cost and time-to-completion for studies. We show how large, diverse observational text data can be mined with large language models (LLMs) to produce inexpensive causal effect estimates under appropriate causal assumptions. We introduce _NATURAL_, a novel family of causal effect estimators built with LLMs that operate over datasets of unstructured text. Our estimators use LLM conditional distributions (over variables of interest, given the text data) to assist in the computation of classical estimators of causal effect. We overcome a number of technical challenges to realize this idea, such as automating data curation and using LLMs to impute missing information. We prepare six (two semi-synthetic and four real) observational datasets, paired with corresponding ground truth in the form of randomized trials, which we used to systematically evaluate each step of our pipeline. NATURAL estimators demonstrate remarkable performance, yielding causal effect estimates that fall within 3 percentage points of their ground truth counterparts, including on real-world Phase 3/4 clinical trials. Our results suggest that unstructured text data is a rich source of causal effect information, and NATURAL is a first step towards an automated pipeline to tap this resource.

## 1 Introduction

Estimating the causal effects of interventions is time consuming and costly, but the resulting outcomes are precious. Health agencies around the world often require randomized controlled trial (RCT) data to approve medical interventions. Clinical trials are key contributors to large R&D costs for drug developers . Natural experiments are another source of rich interventional data, but they may not always exist or have enough data relevant to a given causal hypothesis .

When treatment randomization is infeasible, observational data can be used to identify average treatment effects (ATEs) , under common assumptions, _e.g._, no unobserved confounding. Suchdata is abundant but even when the necessary assumptions are satisfied, it must be _structured_ (_i.e._, the outcomes, treatments, and relevant covariates must be defined, recorded, and tabulated) before it becomes amenable to computational analyses.

Yet, unstructured observational data presents unique opportunities for cheaper, more accessible, and potentially even better  effect estimation. For example, thousands of people living with diabetes choose to share their experiences with _treatments_ on online patient forums. Some of their posts contain rich descriptions of daily lives, the drugs they have been prescribed, the treatment responses and side effects, as well as pre-treatment information like age and sex. Their posts contain their lived experiences including evidence of an _outcome_ in an observational experiment, albeit in an unstructured form. Other potential sources of rich unstructured, observational data include newspaper classifieds, police reports, social media, and clinical reports. Despite being collected for a myriad of purposes, researchers have often turned to such data to test hypotheses since: (i) unstructured data does not require restrictive data collection designs, _e.g._, measurement choice, and can admit many different post-hoc analyses; (ii) the reported outcomes may reflect what matters to subjects better than standard outcome measures; (iii) value may be recovered from outcomes that would otherwise be lost; (iv) there may be _more_ unstructured data available on underserved or marginalized populations. Figure 1 contrasts our setting with previous works using randomized or structured observational data.

This work asks a simple question: _How can we use large language models to automate treatment effect estimation using freely available text data?_ We introduce _NATURAL_, a family of text-conditioned estimators that addresses this by performing _NATural language analysis to Understand ReAL effects_.

At a high level, the steps required to compute NATURAL estimators are as follows. Given an observational study design and a dataset of natural language reports, filter for reports that are likely to conform to the experimental design. Then, using a large language model (LLM), extract the conditional distribution of structured variables of interest (outcome, treatment, covariates) given the report. Finally, use the conditionals to compute estimators of the ATE, using classical strategies such as inverse propensity score weighting and outcome imputation.

NATURAL is a data-driven pipeline. It leverages and relies on the LLM in a manner that mimics the learning task it was trained for: providing parametric approximations to conditional distributions. As in all observational studies, the validity of NATURAL also depends on prior causal knowledge about the task. Expert knowledge is required to define appropriate covariates and confirm that they satisfy the necessary assumptions for effect estimation. However, we anticipate that NATURAL estimators could be developed under other structural assumptions (e.g. instrumental variables) as well.

The core contributions of our work are:

* We derived NATURAL ATE estimators based on classical estimators of the ATE, like inverse propensity score weighting and outcome imputation. NATURAL estimators operate on entirely unstructured data under two novel data-access assumptions.
* We implemented NATURAL estimators using an LLM-based pipeline.
* We developed six observational datasets to systematically evaluate parts of this pipeline: two semi-synthetic datasets constructed using marketing data, and four clinical datasets curated from public (pre-December 2022) migraine and diabetes subreddits from the Pushshift collection .
* For each dataset, we treated the ATE from a corresponding real-world completely randomized experiment (CRE) as ground truth. Remarkably, our predicted ATEs all fell within 3 percentage points of the ground truth ATEs, a potential cost savings of many millions of dollars.

Figure 1: When compared to experimental and other observational studies, NATURAL has lower costs and provides greater diversity in cohort selection, for causal effect estimation.

### Related Work

Leveraging natural language data  to support causal claims is pervasive in applied research [43; 13]. Our work falls under the broad umbrella of accelerating the identification of real-world evidence (RWE) . For instance, in the context of healthcare, RWE supports not only drug repurposing, but also post-market safety evaluations -- its most common application. NATURAL expands the boundaries of how quickly one can obtain and validate such real-world evidence from observational data .

The use of natural language data in causal inference comes in different flavors: i) using text to measure confounders , ii) using text to measure causal effect outcomes , or iii) producing interpretable causal features from text [15; 6], _e.g._, what words are more likely to explain the cause of an event. NATURAL distinguishes itself from these lines of research in two ways: i) NATURAL does not require any curated task-specific training data (it is zero-shot), and ii) NATURAL is not interested in how the text itself, _i.e._, its words, relate to the causal problem --that is, we are only leveraging the model's ability to predict the distribution of a specified variable conditional on the input text. We highlight that our work lies distinct from research at the intersection of text and causality that combines text and numerical or tabular data , the latter of which may be unavailable or incomplete in settings involving neglected diseases, unrecorded abortions, or illicit drug use. Other work has also studied topic modelling approaches  or the ability of language models to infer _latent_ variables (that are implied but not explicitly identified in text data) [36; 13]. Rather, we require the precise specification of covariates to condition on - we view this as being crucial to creating a more direct way for an end user to verify the validity of information extracted with our approach.

Prior works have also leveraged LLMs in a black-box fashion for causal tasks by querying the model for causal statements. In the context of causal discovery, users directly ask for the existence of cause-and-effect relationships, _e.g._,"Does changing the age of an abalone causes a change in its length?" [25; 33; 4; 5; 44; 21; 6]. Due to the large amount of training data, it is possible that the model learns to apply a causal model described in the training data and answer causal questions with it [35; 47]. The issue with this approach is i) the user is limited to the causal models observed in training, ii) the user is not aware of _which_ causal model they are using, and iii) the queries tend to present high prompt sensitivity . Finally, we note that a recent work created a benchmark and showed how LLMs struggle to distinguish pairwise correlation from causation , while another shows that checking causal relationships in a pairwise manner can lead to invalid causal graphs .

## 2 Preliminaries

We are interested in estimating the causal effect of a treatment relative to either another treatment or no treatment in a population of interest. More precisely, we consider treatments \(t\{0,1\}\) and the corresponding potential outcomes \(Y(1)\) and \(Y(0)\) under each treatment. We wish to compute the quantity \(:=[Y(1)-Y(0)]\), often referred to as Average Treatment Effect (ATE). Sometimes, \(Y(0)\) may correspond to no treatment (control). Throughout this work, we assume binary treatments and outcomes in the Neyman-Rubin causal model. We provide a full list of notation in appendix A.

A Completely Randomized Experiment (CRE) with \(n\) participants requires no prior causal knowledge. In a CRE, the treatment assignment vector \((_{i})_{i=1}^{n}\) is a random permutation of \(n_{1}\) ones and \(n-n_{1}\) zeros sampled independently of the outcomes. In this case, the difference-in-means \(}_{i=1}^{n}_{i}Y_{i}(1)-}_{i=1 }^{n}(1-_{i})Y_{i}(0)\) provides us with an unbiased estimate of \(\).

Despite the indisputable necessity of CREs in high-stakes settings, it is often expensive and/or infeasible to have complete control over the treatment assignment. Instead, _observational_ data is more readily available. Observational data often contains spurious correlations between the observed treatment \(T\) and the observed outcome \(Y=TY(1)+(1-T)Y(0)\) through a common cause (confounder). Typically, this confounding is formalized as a variable \(X\), which we assume to be discrete throughout this work, representing covariates associated with each individual. Given i.i.d. samples \(\{(X_{i},T_{i},Y_{i})\}_{i=1}^{n}\) from the target population, standard causal inference techniques can correct for confounding bias and provide consistent estimates of \(\) under Assumptions 1 and 2:

**Assumption 1** (Strong Ignorability.): _The potential outcomes are independent of treatment assignments conditional on covariates, i.e., \((Y(0),Y(1))\!\!\! T|X\)._

**Assumption 2** (Positivity.): _For every treatment \(t\) and covariate set \(x\), \(0<P(T=t X=x)<1\)._

Following are two classical estimators of the ATE \(\) from observational data, each of which rely on \(X\) satisfying Assumptions 1 and 2. We refer the reader to Ding  for further details.

Inverse Propensity Score Weighting (IPW).The propensity score is the conditional probability of receiving a treatment given the observed features, _i.e._, \(e(x)=P(T=1|X=x)\). The IPW estimator is given by

\[_{}=_{i=1}^{n}Y_{i}}{(X_{ i})}-)Y_{i}}{1-(X_{i})},\] (1)

where \((x)\) is an approximation of \(P(T=1 X=x)\). When \((x)\) is the true propensity score, \(_{}\) is an unbiased estimator of \(\). When \((x)\) is estimated as empirical probability, \(_{}\) is consistent.

Outcome Imputation (OI).Outcome Imputation learns a model to impute outcomes from features and treatment and then marginalizes away the features to estimate \(\) with

\[_{}=_{i=1}^{n}(X_{i},1)-(X_{i},0),\] (2)

where \((x,t)\) approximates \(P(Y=1 X=x,T=t)\). Note that if \((x,t)\) is an unbiased estimation of this quantity, \(_{}\) is an unbiased estimator of \(\).

## 3 NATURAL estimators of the ATE

Both CRE and observational studies require direct access to tabulated data \((X_{i}\), \(T_{i}\), \(Y_{i})\) for every individual \(i\). Our NATURAL estimators on the other hand estimate the ATE from observational, unstructured natural language data in the form of language reports \(R_{i}\). In addition to Assumptions 1 and 2, NATURAL estimators require the following assumptions to guarantee their consistency.

**Assumption 3** (Natural language report data.): _The target population is described by an observational data-generating process \(P(X,T,Y,R)\) of data \((X,T,Y)\), which satisfies Assumptions 1 and 2 and is jointly distributed with a random natural language string \(R\), called a report. We assume access to an i.i.d. sample of reports \(\{R_{i}\}_{i=1}^{n}\) from the marginal of this process._

**Assumption 4** (Access to the true observational conditional over \((X,T,Y)\).): _We can either (i) compute the conditional \(P(X=x,T=t,Y=y|R=r)\) of the true data-generating process, or (ii) we can sample from \(P(X=x|R=r)\) and compute \(P(T=t,Y=y|R=r,X=x)\)._

Intuitively, these assumptions give NATURAL indirect access to \((X,T,Y)\) through \(R\). They can be weak or strong, depending on the definition of the reports \(R\). On the one hand, if reports are copies of the observational data, _i.e._, \(R=(X,T,Y)\), then Assumption 4 is trivial to satisfy. On the other hand, if reports are all the constant, empty string, \(R=\), then Assumption 4 guarantees that we have full access to the _true_ observational joint density function over \((X,T,Y)\), which is a strong assumption. In other words, it requires a way to simulate trial outcomes unconditionally (without any data). We consider how we might satisfy these assumptions in practice in the next section. Here, we assume that they hold and develop a series of consistent estimators of the ATE.

**NATURAL Full.** Given \(\{R_{i}\}_{i=1}^{n}\) and \(P(X=x,T=t,Y=y|R=r)\), we can construct an idealized version of NATURAL. Let us start by noting that the law of total expectation gives us

\[=_{X,T,Y}[-]= _{R}[_{X,T,Y|R}[-]].\] (3)

A Monte Carlo estimate over reports is given by

\[_{}=_{i=1}^{n}_{x,t,y}P(X=x,T=t,Y=y| R_{i})[_{}(x)}-_{ }(x)}],\] (4)

which further approximates \(_{}(x)\) from the given conditional. We used eq. (7) below. We note that \(_{}\) above is derived from IPW, but can also be derived from OI, as shown in appendix B.

The estimator \(_{}\) above relies on enumerating all possible values of \((X,T,Y)\), making it computationally expensive for high-dimensional \(X\). Below, we present two hybrid versions of our method which combine sampling of some variables and computation of conditional probabilities of others.

**NATURAL IPW.** To construct our hybrid estimator, we augment the data \(\{R_{i}\}_{i=1}^{n}\) by sampling from \(P(X|R_{i})\) independently for each report \(R_{i}\). This gives us a dataset \(\{(R_{i},X_{i})\}_{i=1}^{n}\) drawn i.i.d. from \(P(X,R)\) by Assumption 4. Then, our hybrid estimator is derived from the form of IPW as follows:

\[ =_{R,X}[_{T,Y|R,X}[- ]],\] (5) \[_{} =_{i=1}^{n}_{(t,y) }P(T=t,Y=y|R_{i},X_{i})[_{}(X_{i} )}-_{}(X_{i})}],\] (6)

where \(_{}(x)\) is consistently estimated in the following manner:

\[_{}(x)=^{n}P(T=1|R_{i},X_{i}) (X_{i}=x)}{_{i=1}^{n}(X_{i}=x)}}_{R,X}P(T=1|R,X)(X=x)\,}{ _{R,X}(X=x)\,}=e(x).\] (7)

**NATURAL OL.** Similarly inspired by the OI estimator in equation 2, we have for \(t\{0,1\}\),

\[P(Y=1 T=t,X=x)=_{R,X,T}P(Y=1|R,X,T) (X=x,T=t)\,}{_{R,X,T}(X=x,T=t)\, }.\] (8)

Thus, for our hybrid OI estimator, we augment the data \(\{R_{i}\}_{i=1}^{n}\) by sampling from \(P(X,T|R_{i})\) independently for each report \(R_{i}\). This gives us a dataset \(\{(R_{i},X_{i},T_{i})\}_{i=1}^{n}\) drawn i.i.d. from \(P(R,X,T)\) by Assumption 4. Then, our consistent outcome predictor is given by

\[_{}(x,t)=^{n}P(Y=1|R_{i},X_{i },T_{i})(X_{i}=x,T_{i}=t)}{_{i=1}^{n}(X_{i}=x,T_{i}=t)},\] (9)

and the final estimator is given by:

\[_{}=_{i=1}^{n}_{ }(X_{i},1)-_{}(X_{i},0)\] (10)

**NATURAL Monte Carlo.** Further in the direction of sampling more variables, we can obtain samples \((X_{i},T_{i},Y_{i})\) from \(P(X,T,Y|R_{i})\) and compute a Monte Carlo estimate, \(_{}\). The set of samples \(\{(X_{i},T_{i},Y_{i})\}_{i=1}^{n}\) constitute a tabular dataset which can be plugged into a standard ATE estimator like IPW or OI, as described in section 2. We refer to these sample-only estimators as N-MC IPW and N-MC OI, respectively.

**Inclusion Criteria conditioned ATE.** We are sometimes interested in ATEs over populations defined by constraints on pre-treatment covariates \(X_{i}\), known as _inclusion criteria_ and denoted by \(I\). This conditional ATE is \((I)=[Y(1)-Y(0) X I]\) and satisfies the following identity.

\[(I)=_{R}[_{X,T,Y}[-X I,R] ].\] (11)

The conditional version of NATURAL IPW can be estimated by filtering out reports where \(P(X I|R_{i})=0\), sampling \(X_{ij} P(X|R_{i},X I)\) i.i.d., and finally weighting datapoints by the relative likelihood of matching the inclusion criteria given the report:

\[(I)=_{i=1}^{n}[)}{_{i =1}^{n}P(X I|R_{i})}_{j=1}^{m}_{T,Y|X_{ij},R_{i}} [)}-)}]],\] (12)

where the inner expectation is estimated similar to eq. (6). A complete derivation for eqs. (11) and (12) and related discussion are included in appendix G. In practice, we took \(m=1\).

## 4 Implementing NATURAL estimators with Large Language Models

LLMs are trained on vast datasets of real-world data, _e.g._, , which likely contain records of data generated by processes that are consistent with Assumption 3. Because LLMs can learn well-calibrated conditionals , our hypothesis is that LLMs can be prompted to approximate the conditionals required by Assumption 4 for real-world causal effect questions of interest. Our LLM implementation of NATURAL estimators is built on this hypothesis to try to satisfy Assumptions 3 and 4 (Assumptions 1 and 2 must be guaranteed by a domain expert). We defer exact prompts for LLM inference to appendix D, a full worked example to appendix C, and a discussion of the limitations of our approach to the next section. Figure 2 summarizes our pipeline.

**Filtering to match Assumption 3.** For our real-data clinical settings, our first goal is to produce a dataset of i.i.d. reports \(R_{i}\) that are very likely to be jointly distributed with the random variables \(\{X_{i},T_{i},Y_{i}\}\) of a specific observational study of interest. Given a study of interest and a dataset of real-world reports that are potentially relevant to the study, we pass it through a sequence of filters with increasing detail and strictness:

1. **Initial filter.** Inspired by other work with social media data [1; 38], we first use deterministic rules to filter out uninformative reports: posts that were removed, are too short, have "bot" in the author's name, have no mention of any keyword related to the study, etc.
2. **Filter by relevance.** We prompt an LLM to determine whether each report contains information that would make it relevant to the study. We remove reports that are deemed irrelevant.
3. **Filter by treatment-outcome.** We ensure that each report pertains specifically to the treatments and outcomes of interest. We do so by prompting an LLM to extract only treatment and outcome information, and retaining only the posts that are deemed to both mention one of the treatments in question and also contain outcome information.
4. **Filter known covariates by inclusion criteria.** For ATEs conditioned on inclusion criteria, as in our real-world datasets, we included a filter to enforce these criteria. Managing inclusion criteria is complicated by the fact that many reports \(R_{i}\) contain no or partial information about covariates that are required to verify inclusion. So, in this filtering step, our goal was to ensure that the final set of reports have non-zero probability of matching the inclusion criteria. We begin by prompting an LLM to extract the full set of covariates \(X_{i}\), following constraints on the possible values each attribute can take, but we allow the LLM to extract Unknown if it is impossible for the LLM to determine the value of a covariate. We then remove reports, if any of the non-Unknown covariates are determined to fail their inclusion criteria. We found the JSON-mode made available for generation by certain LLM APIs, to suffice for this task; however more involved strategies for constrained generation are also possible [46; 50].

**Sampling from and computing conditional probabilities to match Assumption 4.** Given a set of reports \(\{R_{i}\}_{i=1}^{n}\) that pass the filtering stage above, our next steps use LLMs to extract the samples and conditionals \(P_{}(X,T,Y R)\), required to compute NATURAL estimators. For each \(R_{i}\), we:

1. **Extract covariates, both known and unknown.** We run a final covariate extraction by prompting an LLM to determine the full set of covariates \(X_{i}\) from the report \(R_{i}\), subject to the constraint that \(X_{i}\) satisfies the inclusion criteria. In contrast to (iv), we ask the LLM to

Figure 2: Our pipeline leverages LLMs to curate data that can be plugged into natural language conditioned estimators for average treatment effects.

guess the values of Unknown covariates. We verified that this second extraction agreed exactly with the first extraction (iv) on the known covariates (_i.e._, the ones that were not extracted as Unknown in the first extraction). We contrast the empirical distributions of these known and unknown/guessed covariates for our experiments in appendix F.1.
2. **Infer conditionals.** Given \(\{R_{i},X_{i}\}_{i=1}^{n}\) from the previous steps, we compute the probabilities \(P_{}(T=t,Y=y|R_{i},X_{i})\) by prompting an LLM that makes log-probabilities accessible. Specifically, we ask an LLM to answer questions about \(T,Y\) given access to \(R_{i},X_{i}\), and we score every possible answer \(T=t,Y=y\) using the LLM log-probabilities. We exponentiate and renormalize these scores across the space of possible realizations to obtain a valid probability distribution.
3. **Weight reports according to inclusion criteria match.** Similar to item (vi), we compute \(P_{}(X_{i} I|R_{i})\) to obtain the weights in eq. (12), by prompting an LLM with descriptions of the inclusion criteria that must be satisfied and each report \(R_{i}\). It may be possible to skip this weighting step under additional structural assumptions on the data. These assumptions as well as experimental results without the weighting are included in appendix G.

Nevertheless, while our empirical results are remarkably consistent with the correctness of our pipeline, we cannot formally guarantee that it satisfies Assumptions 3 and 4. The final outcome of this pipeline is a dataset \(\{R_{i},X_{i}\}_{i=1}^{n}\) and a set of conditionals \(P(T=t,Y=y|R_{i},X_{i})\) that can be plugged into the hybrid NATURAL estimators in section 3 to predict ATEs. Therefore, we see this as a first implementation of NATURAL estimators, which we anticipate can be improved.

## 5 Limitations and Broader Impact

In addition to the limitations that NATURAL shares with every observational study, _i.e._, the validity of the practitioner's causal assumptions, it comes with an _extra dependence on how well one can approximate the desired conditional distributions_. While more and more capable LLMs are being continually developed, the extent to which they satisfy NATURAL's assumptions is nearly impossible to formally test. Indeed, while pretraining tends to produce calibrated LLM predictions , post-training techniques can compromise calibration . Therefore, we emphasize that NATURAL was _not_ developed to recommend therapeutics directly to end-users or to directly inform high-stakes public policies. Instead, we envision NATURAL as a powerful tool to help us approximate ATEs at scale and prioritize confirmatory CREs. We strongly recommend that all predictions made by NATURAL estimators be validated experimentally before being used to inform high-stakes decision-making. Apart from its dependence on LLM capabilities, NATURAL is also limited by the nature of observational, unstructured natural language data:

* _Network Interference._ In practice, acquiring i.i.d. reports can be challenging. For instance, social network users might talk to each other and influence their treatment choices. This is a well-known issue in causal inference and statistical sciences in general. Existing solutions rely on a known network structure to sample individuals or correct for their neighbors' treatments [10; 26; 17].
* _Outcome Measurement._ Since NATURAL deals with self-reports, subjects need to be able to report the outcomes of interest. For example, this cannot be applied if the outcome is measured with an expensive, inaccessible test. Therefore, the study design implemented with NATURAL must account for the accessibility of endpoints to users.

Figure 3: For Hillstrom (left) and Retail Hero (right), the KL divergence between estimated joint and propensity distributions and their true counterparts reduces with increasing number of posts (top), as does the RMSE between the NATURAL Full estimate and true ATE (bottom).

* _Selection Bias._ Results might be biased towards individuals' choice of reporting an outcome given their experience with the treatment. Luckily, outcome missingness is a widely studied problem in causality research, see _e.g._, how to test  or how to mitigate  it. Note, however, that solutions will often accumulate assumptions on top of NATURAL and should always be critically evaluated by practitioners. Finally, apart from individuals' choice of reporting, selection bias might also arise from which individuals participate in online forums, _i.e._, our framework is only capable of estimating _local_ ATEs --external validity is not guaranteed a priori. We demonstrate this challenge in section 6 by simulating systematic bias in semi-synthetic settings.

## 6 Empirical Evaluation

Evaluating an end-to-end pipeline for causal inference from unstructured real-world text data to ATEs presents challenges regarding access to data, ground truth ATEs and insightful intermediate metrics. We used two semi-synthetic datasets where we augmented randomized data to mimic real-world observations, while continuing to have access to ground truth evaluation. In addition, we study four real datasets, curated from publicly available Reddit posts from the PushShift dataset, as described in section 4. These six datasets allowed us to systematically evaluate NATURAL.

Semi-synthetic Datasets.Causal effect estimation is typically evaluated using synthetic datasets with one or more relationships between the observed covariates, treatment and outcome being contrived. We instead synthesized unstructured observational text data from real randomized tabular datasets, using an LLM. Specifically, we (i) introduced confounding bias by sampling datapoints according to an artificial propensity score, (ii) randomly dropped covariates, (iii) described covariates, treatment and outcome in shuffled orderings, (iv) simulated realism by sampling a person from the the Big Five personality traits  for each datapoint and finally, (v) prompted the LLM to generate a realistic report describing the provided information in the style of someone with the given traits (see appendix D for the full prompt). We used two standard, publicly available randomized datasets: **Hillstrom** and **Retail Hero**, and plan to open-source scripts to generate our data. Step (i) above is in a similar vein as Keith et al. , in that our subsampling strategy does not modify the marginal distribution over covariates and the ATE remains identifiable from observational data.

Real-world Datasets.To study how our framework may be deployed to test hypotheses using real data from online forums; we considered two medical conditions for which there exist abundant Reddit posts in the PushShift collection , with individuals' personal experiences: the effect of diabetes medications (e.g. Semaglutide) on weight loss and the tolerability of migraine treatments. For each condition, we picked two clinical trials which performed head-to-head comparisons of two treatments that we expected to find references to in relevant subreddits. Moreover, to mitigate selection bias we selected pairs of similar treatments, _e.g._, comparable availability, where we believe the probability of a user reporting their experience is approximately equal in both. As we will discuss, our results suggest external validity as well, meaning that the probability of a user reporting their experience with the treatments seems to be (approximately) equal to the prior probability of a user reporting any experience. We limited our data collection to posts that were written before December 2022 and made publicly available in the PushShift archives. We curated four datasets for comparison between different treatments, each of which has a ground truth RCT: **Semaglutide vs. Tirzepatide** and **Semaglutide vs. Liraglutide** for their effect on weight loss and **Erenumab vs. Topiramate** and **OnabotulinumtoxinA vs. Topiramate** for their tolerability. We used the first of these to validate implementation choices NATURAL (like filtering, imputations, prompt specifications) and

    &  &  \\   & ATE (\(\%\)) & RMSE & ATE (\(\%\)) & RMSE \\ 
**Selection-biased N-IPW** & \(-3.49 1.46\) & \(9.58\) & \(10.66 2.24\) & \(7.67\) \\
**Uncorrected** & \(1.86 0.67\) & \(4.28\) & \(0.26 0.30\) & \(3.08\) \\ 
**N-Full** & \(4.26 0.86\) & \(2.02\) & \(1.86 1.38\) & \(2.08\) \\
**N-MC OI** & \(6.17 1.61\) & \(1.61\) & \(4.94 2.17\) & \(2.70\) \\
**N-MCP IW** & \(4.81 0.80\) & \(1.51\) & \(1.85 2.01\) & \(2.49\) \\
**N-OI** & \(4.58 0.61\) & \(1.62\) & \(2.99 1.43\) & \(1.72\) \\
**N-IPW** & \(\) & \(\) & \(\) & \(\) \\ 
**Bag-of-Words** & \(7.57 1.37\) & \(2.23\) & \(2.61 2.08\) & \(2.42\) \\
**Sentence Encoder** & \(0.00 0.00\) & \(6.09\) & \(1.97 1.62\) & \(2.10\) \\ 
**IPW (Structured)** & \(6.38 0.26\) & \(0.39\) & \(3.09 0.19\) & \(0.30\) \\
**Ground Truth** & \(\) & - & \(\) & - \\   

Table 1: The NATURAL IPW ATE outperforms other versions of the method as well as trained baselines on semi-synthetic datasets, as measured by RMSE.

the other three as held-out test settings, see appendix C. We include further details for all our datasets in appendix E, including the definitions of covariates and outcomes.

Next, we investigate several questions about the performance of NATURAL empirically. We used GPT-4 Turbo for sampling and LLAMA2-70B for computing conditional probabilities.

How well does NATURAL estimate observational distributions from self-reported data?Our semi-synthetic datasets give us access to the true joint distributions \(P(X,T,Y)\) and true propensity scores \(P(T=1|X)\). The top row of fig. 3 shows the KL divergence between these distributions and those estimated by NATURAL Full, for Hillstrom (left) and Retail Hero (right). We find that these KL divergences decrease steadily as the number of reports used in the estimation increases. The bottom row shows corresponding root-mean-squared error (RMSE) between NATURAL and the true ATE. This corroborates the insight that as the joint distribution and propensity scores are estimated more accurately, the predicted ATE gets closer to its true value. In particular, we observe a clear correlation between the quality of estimated propensity scores and estimated ATEs.

How do NATURAL methods compare to one another and to trained baselines?We present our estimated ATE and its RMSE on semi-synthetic datasets in table 1. Further, we evaluate two trained baselines, which use a Bag-of-Words model and a sentence encoder respectively, to train representations of text data with their labels. Here, for each attribute in the set of covariates, treatments, and outcomes, we train a MLP model with 5-fold cross validation to predict that attribute. We then use these predicted attributes as a tabular dataset of samples that can be plugged into any causal inference estimator. We find that our methods are competitive with or outperform these baselines, despite not being trained with any labels. In particular, the sentence encoder baseline collapsed to an ATE of zero for Hillstrom, having learned the constant predictor for outcomes. IPW (Structured) is an oracle estimator, which assumes full access to structured data. Selection-biased N-IPW demonstrates the challenge of ATE estimation in the presence of bias which was systematically simulated as a function of covariates "channel" and "zip code" for Hillstrom and "age" for Retail Hero.

Table 2 compares NATURAL methods to estimate the ATEs in real-world clinical settings using self-reported data from the PushShift collection of Reddit posts. Remarkably, our predicted ATEs (a) depict the same _direction of effect_, and (b) fall _within 3 percentage points_ of their corresponding ground truth clinical trial ATEs. For both semi-synthetic and real data experiments, NATURAL IPW outperforms other versions across datasets, except for the Semaglutide vs. LIraglutide setting, where NATURAL OI performed the best. Both N-MC versions perform similarly on all datasets.

This result is significant. Clinical trials can take on the order of years and costs in the tens to hundreds of millions of dollars. Going from the raw language observational data to ATE in our framework takes on the order of days and costs at most a few hundred dollars of compute. For problems in medicine, economics, sociology, and political science where randomization is infeasible or expensive, NATURAL provides a tractable way to leverage observational data to rank potential experiments prior to conducting them.

How do different choices in the NATURAL pipeline effect ATE prediction?We assess the impact of key choices in our pipeline described in section 4, by ablating them one-by-one. We investigated and selected these choices on the Semaglutiide vs. Tirzepatide experiment. Figure 4 (left) compares the RMSE of predicted ATEs when data is not filtered according to inclusion criteria and LLM imputations are replaced with samples from an uniform distribution.

    &  &  \\   &  &  &  &  &  \\  &  &  &  &  \\   & ATE (\(\%\)) & RMSE & ATE (\(\%\)) & RMSE & ATE (\(\%\)) & RMSE & ATE (\(\%\)) & RMSE \\ 
**Uncorrected** & \(-33.56 0.77\) & \(43.67\) & \(-83.57 0.43\) & \(68.87\) & \(29.07 0.48\) & \(2.87\) & \(21.55 1.22\) & \(19.49\) \\ 
**N-MC OU** & \(5.89 1.03\) & \(4.28 0.94\) & \(6.54\) & \(25.62 0.51\) & \(2.72\) & \(46.20 1.94\) & \(5.55\) \\
**N-MC IPW** & \(5.62 0.85\) & \(4.81 0.70 0.94\) & \(7.66\) & \(26.65 1.44\) & \(3.19\) & \(46.52 1.92\) & \(5.85\) \\
**N-OI** & \(4.84 1.19\) & \(5.39\) & \(\) & \(\) & \(\) & \(\) & \(44.67 1.56\) & \(3.99\) \\
**N-IPW** & \(\) & \(\) & \(-12.54 0.86\) & \(2.33\) & \(25.64 0.40\) & \(2.68\) & \(\) & \(\) \\ 
**Ground Truth** & \(\)/\(397919\), \(18\) & \(\)/\(191956\), \(6\) & \(\)/\(100\)/\(15793\), \(39\) & \(\)/\(100\)/\(15793\), \(39\) \\   

Table 2: Using real data, best performing NATURAL estimators fall within \(3\) percentage points of their corresponding ground truth clinical trial ATEs. Possible ATE values lie between \(-100\) and \(100\).

It shows that both inclusion-based filtering and imputations from a pretrained LLM are crucial for the performance of NATURAL. We also compared performance of our method when the conditional probabilities in eq. (6) are evaluated using models of different scales in fig. 4 (right), and found that performance improves at larger scales and with greater quantity of data.

How well do different estimates of propensity score balance covariates?A property of accurate propensity score estimates is that they balance covariates across treatment cohorts (see Ding  for details and proofs), _i.e._ the average treatment effect on each covariate, corrected using propensity scores, is close to zero. Figure 5 visualizes this quantity for different covariates of the Semaglutide vs. Tirzepatide experiment and shows that propensity scores estimated using LLAMA2 conditional distributions balance the covariates far better than a uniform distribution does, with the 70B model consistently estimating the treatment effect on each covariate as close to zero. Similar visualizations for the test settings are shown in fig. 10 of appendix F.2.

## 7 Conclusion

In this work, we introduced _NATURAL_, a family of text-conditioned estimators, to automate treatment effect estimation using free-form text data. We demonstrated NATURAL's efficacy with six semi-synthetic and real datasets for systematic evaluation of its pipeline. We exposed the ability of LLMs to extract meaningful conditional distributions over structured variables and how their combination with classical causal estimators can predict real-world causal effects with remarkable accuracy. Given this promising performance, exciting directions for future work include (i) incorporating automatic prompt tuning methods into the pipeline, (ii) extending our methods to real-valued \((X,T,Y)\), (iii) exploring whether our assumptions can be weakened, (iv) exploring other domains in applied research, _e.g._, social sciences, (v) performing a more extensive evaluation of NATURAL on different study designs to better understand what type of treatments, outcomes, and reports show better or worse practical performance with NATURAL or (vi) deploying the pipeline to test hypotheses at even larger scales.

NATURAL estimators have numerous use cases with potentially far-reaching impact. As long as patients have access to treatments and report their experiences, NATURAL can be used to compare two treatments in new indications or new populations. Therefore, our pipeline can in principle support efforts to prioritize trials for repurposed drugs or supplements in under-served diseases or populations. Further, a crucial step after drug approval is post-marketing surveillance for side effects (positive or negative) that may not have been measured or may have been too rare to identify in a smaller trial. NATURAL can leverage the diversity of available language data to detect these effects. While our motivations largely stem from the challenges of drug development, our NATURAL estimators are applicable to any effect estimation setting for which there exists relevant natural language data.

Figure 4: Ablation study on Semaglutide vs. Tirzepatide, to tease apart the effect of data filtering and imputation (left) as well as LLM scale for conditionals (right) on NATURAL performance.

Figure 5: NATURAL propensity scores balance the Semaglutide vs. Tirzepatide covariates better than uniform scores.