# A General Framework for Learning under Corruption: Label Noise, Attribute Noise, and Beyond

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Corruption is frequently observed in collected data and has been extensively studied in machine learning under different corruption models. Despite this, there remains a limited understanding of how these models relate such that a unified view of corruptions and their consequences on learning is still lacking. In this work, we formally analyze corruption models at the distribution level through a general, exhaustive framework based on Markov kernels. We highlight the existence of intricate joint and dependent corruptions on both labels and attributes, which are rarely touched by existing research. Further, we show how these corruptions affect standard supervised learning by analyzing the resulting changes in Bayes Risk. Our findings offer qualitative insights into the consequences of "more complex" corruptions on the learning problem, and provide a foundation for future quantitative comparisons. Applications of the framework include corruption-corrected learning, a subcase of which we study in this paper by theoretically analyzing loss correction with respect to different corruption instances.

## 1 Introduction

Machine learning starts with data. The most widespread conception of data defines them as atomic facts, perfectly describing some reality of interest . In learning theories, this is reflected by the often-used assumption that training and test data are drawn independently from the same distribution. The goal of learning is to identify and synthesize patterns based on the knowledge, or information, embedded in these data. In practice, however, corruption regularly occurs in data collection. This creates a mismatch between training and test distributions, forcing us to learn from imperfect facts.

We should thus doubt the view of data as static facts, and consider them as a dynamic element of a learning task . Besides the predictor and the loss function, one may focus on the data dynamics, studying corruptions and intervening in the learning process. Toward this goal, there has been a surge of research in the machine learning community proposing various corruption models, examining and correcting their effects on learning formally or empirically [3; 4; 5; 6; 7; 8]. Nevertheless, it is still unclear how these models relate and whether they characterize all types of corruption. Even though the necessity of investigating this topic is recognized both at a practical [9; 10] and a theoretical [11; 12] level, no standardized way to model and analyze corruption has been so far created .

Our primary objective here is to systematically study the problem of learning under corruption, providing a general framework for analysis. Whilst there have been some existing attempts, certain limitations persist in terms of homogeneity and exhaustiveness. A famous early endeavor is Quinonero-Candela et al. , grouping together works about the multi-faceted topic of dataset shift, yet not in a unifying or comprehensive manner. Later on, several studies aim to provide a more homogeneous view of corruption, often referred to as noise or distribution shift. However, their frameworkstypically rely on some corruption-invariant assumptions on the marginal or conditional probabilities, and the extent of exhaustiveness is merely conjectured or not considered [15; 16; 17; 18].

In this paper, we take a different point of view from the previous work: we categorize corruption based on its dependence on the feature and label space, rather than relying on the notion of invariance. Our resulting framework is generic, encompassing all possible pairwise stochastic corruptions.1 The underpinning mathematical tool that enables such exhaustiveness is the Markov kernel. While Markov kernels have been utilized in formalizing corruption [7; 19], their primary focus has been solely on label corruption, attribute corruption, or simple joint corruption. To our knowledge, the proposed framework is novel in the sense of demonstrated exhaustiveness in this domain. Our contributions are summarized as follows:

**C1**: We propose a new taxonomy of corruption in the supervised learning setting (SS 3), hierarchically organized through the notion of dependence (Fig. 1), and connect existing corruption models to this taxonomy (Tab. 1).
**C2**: We analyze the implications of our family of corruptions on learning (SS 4), linking the Bayes risk of the clean and corrupted supervised learning problems through equality results (Theorem 3, Theorem 4, Theorem 5).
**C3**: We derive corruption-corrected loss functions for different corruption instances within our framework (SS 5). A subcase of these corrections (Theorem 8) generalizes prior results on corruption-corrected learning in simple label corruption.

Though abstract in general, our results expand upon existing ones on specific corruption models and shed light on the relatively under-explored joint and dependent corruptions.

## 2 Background

Before introducing our analysis, we review the background framework and notations.

Supervised learningIn statistical decision theory [20; 21], a general decision problem can be viewed as a two-player game between _nature_ and _decision-maker_. Nature chooses its _state_, then _experiment_ leads to some _observations_ given the state, and the decision-maker picks a suitable _action_ from a fixed set of _decision rules_. In the specific setting of _supervised learning_, observations are in the feature space \(X^{d},d 1\), states are in the label space \(Y\), then the _experiment_\(E\) leads to a probability associated with the observation \(X\), given the state \(Y\). Here we focus on the classification task, that is, assuming the label space to be _finite_. All the stated results can be easily extended to regression cases by considering a continuous label space; we leave it for future application.

To formalize the processes described above, we introduce the Markov kernel.

**Definition 1** (Klenke ).: _A Markov kernel\(\) from a measurable space \((X_{1},_{1})\) to a measurable space \((X_{2},_{2})\) is a function \(x_{1}(x_{1},)\) from \(X_{1}\) to \((X_{2})\), the set of probability measures on \(X_{2}\), such that \((x_{1},B)\) is measurable in \(x_{1}\) for each set \(B_{2}\). We denote it by \(:X_{1} X_{2}\), or more compactly by \(_{X_{1}X_{2}}\). The set of Markov kernels from \(X_{1}\) to \(X_{2}\) is referred to as \((X_{1},X_{2})\)._

The Markov kernel generalizes the concept of conditional probability. Looking at the function \((,B)\), it associates different probabilities to the set \(B\) given different values of the _parameter_\(x_{1}\). It can transform a distribution \((X_{1})\) into another distribution \((X_{2})\), as well as transform a function \(f:X_{2}\) into another function \( f:X_{1}\) with the following two operators:

\[(B)_{X_{1}}(x_{1},B)(dx_{1})\;\; B _{2}\;, f(x_{1})_{X_{2}}(x_{1},dx_{ 2})f(x_{2})\;\; x_{1} X_{1}\;,\]

provided the integral exists. Next, we define different operations to combine Markov kernels:

**P1**: Given \(:X_{1} X_{2}\) and \(:X_{1} X_{2} X_{3}\), their _chain composition_\(:X_{1} X_{3}\) is defined by \(()f(x_{1})_{X_{2}}(x_{1},dx_{2})_{X_ {3}}((x_{1},x_{2}),dx_{3})f(x_{3})=(\;f)(x_{3})\) where \(f:X_{3}\) is a positive \(_{3}\)-measurable function;
**P2**: For \(:X_{1} X_{2}\) and \(:X_{1} X_{2} X_{3}\), their _product composition_\(:X_{1} X_{2} X_{3}\) is \(()f(x_{1})_{X_{2}}(x_{1},dx_{2})_{X_ {3}}((x_{1},x_{2}),dx_{3})\,f(x_{2},x_{3})\) for every \(f\) positive \(_{2}_{3}\)-measurable.

Notice that a probability distribution is a specific instance of a Markov kernel, constant in its parameter space. Therefore, **P1** and **P2** are well defined for \((X_{2})\). We can unify the notation of \(\) for distributions thanks to the flexibility of kernels, and consider the \(\) as a subcase of \(\).

Bayes riskHaving defined all these objects, a supervised learning problem can be represented by the diagram \(Y}{{}}X}{{}}Y,\) where \(h\) is a decision rule chosen in \((X,Y)\). Its task can be formalized as a risk minimization problem, i.e., finding the optimal action \(h\) by considering the _Bayes Risk_ (BR) measure

\[BR_{}( E)=_{h(X,Y)}R_{,}( E)= _{h(X,Y)}_{}_{  E_{}}(h_{},)\,\]

where the notation \(_{}\) stands for the kernel \(\) evaluated on the parameter \(\), e.g., \(h_{},E_{}\) (this subscript notation will be used throughout), and \(\) is a prior distribution on \(Y\). The function \(\) is asked to be bounded and a _proper loss_, i.e., a loss function \(:(Y) Y^{+}\) whose minimization set contains the ground truth class probability. More formally, we ask for

\[\;h^{*}_{h(X,Y)}R_{,,}(E) h^{*}=E\;,\;(X)\.\]

Since in real-world applications, one deploys a model with only limited representation capacity, we consider the constrained version of BR

\[BR_{,}(_{Y} E)=_{h (X,Y)}_{_{Y}}_{ E _{}}(h_{},)\.\]

We call \(\) the _model class_. If we fix the joint space to \(Z=X Y\) and the joint probability distribution to \(P=_{Y} E(Z)\), we can refer to a _supervised learning problem_ as the triple \((,,P)\). Notice that we can also use an equivalent decomposition of the joint distribution through a posterior kernel \(F:X Y\), so that \(P=_{X} F\) for some prior on the feature space. Hence, each supervised learning problem can have two associated kernels, the experiment \(E\) and the posterior one \(F\). We then obtain two views of the learning problem, a generative and a discriminative one, as previously noted by Reid et al. . By means of these, we can define two _Conditional BR_ (CBR):

\[\;\;_{_{X}}CBR_{ ,}(F_{}) =_{_{X}}_{h_{} _{}}_{ F_{}}(h_{ },)\,\] (1) \[\;\;_{_{Y}}CBR_{ ,}(E_{}) =_{_{Y}}_{h}_{ E_{}}(h_{},)\,\]

both equal to their corresponding constrained BR. Notice that for Eq. (1) to be well defined, we need at least one minimum of the unconstrained BR to be included in the model class. For our convenience, we ask it to be the _\(h\) matching_ the \(F\).

## 3 A general framework for corruption

In this section, we present a general framework of pairwise corruptions based on the notion of _dependence_ and discuss how existing corruption models fit into this framework as subcategories. First, let us formally define corruption and two additional kernel operations, which will be useful in the buildup of our corruption taxonomy.

**Definition 2**.: _A corruption is a Markov kernel \(\) that sends a probability space \((X Y,,P)\) into another, \((X Y,,)\). We write it as \(_{}\),2 and call the variables \(z=(x,y) Z\) parameters and the differentials \(d=dd\) corrupted variables._

The following operations are not considered in the classical probability literature but have been studied in other areas, e.g., through the lens of category theory . Here we rework them to fit our framework.

Given \(:X_{1} X_{2}\) and \(:X_{3} X_{4}\), their _superposition_ (see SS S2.1) is equal to \(:X_{1} X_{3} X_{2} X_{4}\) as \(()f(x_{1},x_{3}):=_{X_{2}}(x_{1},dx_{2})_{X_{4}} (x_{3},dx_{4})\,f(x_{2},x_{4})\), where \(f:X_{2} X_{4}\) is positive \(_{2}_{4}\)-measurable;* The _pseudo-inverse_ of a kernel \(:X_{1} X_{2}\) is defined as \(^{}:X_{2} X_{1}\) such that \((^{})_{1}=_{1}\) and \((^{})_{2}=_{2}\) with \(_{1},_{2}\) being the probabilities associated to \(X_{1},X_{2}\). In general, the pseudo-inverse is not unique, since it corresponds to a class of equivalence induced by the probability measure on \(X_{1}\) (see details in SS S2.2).

Again, **P3** is well defined for \((X_{2})\). This operation allows for more flexible combinations of kernels, in a "parallel" fashion. No restriction is imposed on the parameter spaces to be equal, e.g., \(X_{1}=X_{2}\), or Cartesian products with some space in common, e.g., \(X_{1}=Y_{1} Y_{2},X_{2}=Y_{1} Y_{3}\). When this happens, the action of the two kernels "superpose" on the same space. In addition, having more than one measure in the integral acting on the same space would make the integral ill-defined, so this case is excluded. Because of these properties, we say that **P3** is the operation with the _weakest feasibility conditions_, i.e., the set of rules to fulfill a well-defined operation.

Building a taxonomy of corruptionsCorruptions can be naturally classified in different ways, depending on their behavior with respect parameters and corrupted variables. In Fig. 0(a), we show all possible non-trivial corruption types, i.e., those that are not identical and not constantly equal to a probability. We classify them based on the number of parameters they _depend on_, and the type of corrupted variables they _result in_. Specifically, we employ the following abbreviations: J is short for Joint (both variables are corrupted), S is short for Simple (the parameter and the corrupted variable are the same), and D is short for Dependent (others). We then obtain the classification: 2-parameter joint corruption (2J), 1-parameter joint corruption (1J), 2-parameter dependent corruption (2D), 1-parameter dependent corruption (1D), simple corruption (S), along with an indication of parameter or corrupted space. The general naming rule is {#parameters} + {abbreviation} + {-} + {parameter or corrupted space, depending on where the ambiguity lies}.

We now want to generate all possible corruptions of the type \(_{Z}:X Y\). We combine the nodes in Fig. 0(a) using the superposition operation (**P3**), obtaining all the feasible combinations included in Fig. 0(b). The missing couples are excluded because of **P3**'s feasibility conditions described above, which, even if weak, still do not allow some corruption pairings. Needing each corrupted variable to appear _exactly once_, we cannot include the 1-parameter joint corruptions in any factorization of the \(_{XY}\). It is easy to check that no corruption from (a subset of) \(\{X,Y\}\) to (a subset of) \(\{,\}\) can be combined with them. Compatibility problems arise also when trying to combine a simple corruption (S) with a 1-parameter dependent one (1D); we cannot fulfill the feasibility conditions for **P3** and obtain a complete joint corruption, since we will be always missing a parameter. We then exclude this combination from our taxonomy.3

Markov kernels and exhaustivenessOur motivation for formalizing corruptions through Markov kernels is their representation power in terms of couplings. A _coupling_ is formally defined for two probability spaces \(_{1}(Z_{1},_{1},P_{1}),\,_{2}(Z_{2}, _{2},P_{2})\) as a probability space \((Z_{1} Z_{2},_{1}_{2},P)\), such that the marginal probabilities associated to \(P\) w.r.t. \(Z_{i},\,\ i\{1,2\}\) are

Figure 1: Partial orderings on the corruption and combination sets, based on the amount of _dependence_ on the spaces. In the left panel, we underline with dotted nodes the corruptions that cannot be used in any feasible combination. Trivial cases of independence from all parameters or identical kernels are excluded from this analysis.

the respective \(P_{i}\). By construction, Markov kernels are in bijection with all the possible couplings existent on \(Z Z\) with two _fixed_ probability measures, for us, \(P,\). Hence, they represent all possible pairwise dependencies between probability spaces that are _stochastic_, and for non-stochastic mappings, we are sure to have an alternative Markov kernel representation.4

In most machine learning research considering corruption, the corruption process typically involves two environments, that is, the training one and the test one. Our definition of corruption (Def. 2) covers all such pairwise cases. Furthermore, one may also apply this framework to settings with more than two spaces, e.g., online learning or learning from multiple different domains . For these cases, we can employ a composed model, where different corruptions are acting together in a "chained" (**P1**, **P2**) or "parallel" (**P3**) fashion and creating more complex patterns. We discuss further possibilities for applying this framework to \(n>2\) corrupted spaces in SS S2.3.

Relations to existing paradigmsNext, we examine how existing corruption models fit into our taxonomy. To do so, we reformulate them as specific instantiations of Markov corruptions. This reveals their relationships within the corruption hierarchy presented in Fig. 0(a). Our goal here is not to merely demonstrate that a child problem can be solved by a parent one, but rather to gain a deeper understanding of the problem settings. The exhaustiveness of the framework allows us to identify what has been previously overlooked in characterizing all types of corruption. Notably, we highlight the existence of joint and dependent corruptions, which receive far less attention than simple

   Name & Action diagram & Corrupted distribution & Examples \\  S-\(\) & \(Y}{}X$}}{ }\) & \(=(_{X}_{Y})(_{Y} E)\) & attribute noise  \\  S-\(\) & \(X}{}Y$}}{ }\) & \(=(_{X}_{Y})(_{X} F)\) & class-conditional noise \\ 
1D-\(\) & \(X}{}Y$}}{ }\) & \(=(_{Y}_{Y})(_{X} F)\) & style transfer  \\ 
1D-\(\) & \(Y}{}X$}}{ }\) & \(=(_{X}_{X})(_{Y} E)\) & instance-dependent noise \\  & & & (IDN)  \\ 
2D-\(\) & \(Y}{}X$}}{ }\) & \(=(_{XY}_{Y})(_{Y} E)\) & adversarial noise  \\ 
2D-\(\) & \(X}{}Y}{} $}}{}\) & \(=(_{X}_{XY})(_{X} F)\) & instance \& label-dependent noise  \\  S-\(\), \(}{}X}{ }Y}{}\) & \(=(_{X}_{Y})(_{Y} E)\) & combined simple noise  \\ S-\(\) & \(}{}X}{ }Y}{}}{, while far greater problems arise in such complicated cases (see SS 4). Moreover, we notice that existing categorizations rely mostly on the notion of invariance, i.e., corruptions are defined based on which element of the distributions are preserved. These invariance-based taxonomies have been introduced mainly for robustness and causal analyses. However, they do not have a one-to-one correspondence with ours, and do not allow for a hierarchical nor compositional view of corruption. A summary of representative corruption models in the literature is given in Tab. 1, while all the technical details about correspondences and relations between taxonomies are given in SS S1.

## 4 Consequences of corruption in supervised learning

Traditionally, experiments have been compared through Bayes Risk using what is known as the Data Processing Inequality, or Blackwell-Sherman-Stein Theorem .5 Recently, in Williamson and Cranko , Data Processing Equality results have also been studied within the supervised learning framework. Here we adopt the equality approach to compare the clean and corrupted experiments through Bayes Risk. The equalities formally characterize how the optimization problem is affected by the different kinds of joint corruption in our taxonomy. This gives us a quantitative result in terms of conserved "information"  between corrupted and clean learning problems, and a bridge between the problems themselves.

We rewrite the minimization set of the BR in a more compact way, such as \(\{\,(x,y)(h_{x},y)\,|\,h (X,Y)\,\}\). We define the action of a corruption \(\) on this set as the set of all the corrupted functions \( f,\,f\). Lastly, we ask \(f^{*}= h^{*}*{arg\,min}_{f}_{}[f( ,)]\) to belong to the constraining space \(\), for reasons already discussed for Eq. (1).

The first two theorems cover the (S, 2D) cases and their subcase (S-\(\), S-\(\)), as proved in .

**Theorem 3** (BR under (S-\(\), S-\(\)), (2D-\(\), S-\(\)) joint corruption).: _Let \((,,P)\) be a learning problem, \(E:Y X\) an experiment and \(^{}\{_{X},_{YX}\}\) a corruption. Let \(_{Y}\) be a simple corruption on \(Y\). Then we can form the corrupted experiment as per the transition diagram6_

\[_{_{Y}_{Y}}CBR_{ }(^{}E_{})=_{Y _{Y}CBR_{^{}(_{Y} )}}(E_{})\.\]

_Moreover, if \(^{}=_{X}\), we have_

\[BR_{}[_{Y}(_{Y}_{X} E)]=BR_{_{X}(_{Y})}(_{Y}  E)\.\] (2)

Here in Theorem 3 we have shown the BR equality for the experiment \(E\), in line with the Comparison of Experiments and Information Equalities literature mentioned at the beginning of the section. However, for some corruptions the equalities results cannot be stated with \(E\) and the Generative CBR, unless ignoring the joint corruption factorization formula (see SS S5 for a detailed explanation). We hence use the posterior kernel \(F\) defined with the Discriminative CBR (Eq. (1)), and gain more insights about the minimization set while paying a price in elegance of the result.

**Theorem 4** (BR under (S-\(\), 2D-\(\)) joint corruption).: _Let \((,,P)\) be a learning problem, \(F:X Y\) a posterior and \(_{XY}\) a \(Y\) corruption. Let \(_{X}\) be a simple corruption on \(X\). Then we can form the corrupted experiment as per the transition diagram \( X X Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y\) and obtain_

\[_{_{X}_{X}}CBR_{ }(_{XY}F_{})=_{X _{X}}CBR_{_{X}(_{XY})}(F _{})\.\]

[MISSING_PAGE_FAIL:7]

Past work from Van Rooyen and Williamson  and Patrini et al.  considered unbiased learning as what is known as _generalization_, i.e. learn on the corrupted space \(\) a hypothesis \(h^{*}\) such that it is also optimal on the clean distribution \(P\) at test time. They choose the approach of corrected learning, which is, correcting the loss function or the model class in order to learn a \(h^{*}\) capable to generalize. They both used frameworks related to ours, although only in the presence of simple \(Y\) corruption.

We prove similar results to these works for the _loss correction_ task and analyze what we can achieve in other corruption cases described by our taxonomy. In general, we cannot prove generalization but we exhibit a corrected loss allowing the model learned on \(\) to have the same _biases (i.e. loss scores)_ as the one found for the clean learning problem. To do so, we make use of the pseudo-inverse of a Markov kernel (**P4**), as it is more convenient and powerful than the kernel reconstruction introduced in . The results we show here also serve as a first step towards understanding the effect of corruption of the minimization set \(\), in the cases where the BR equalities are not giving us much information (i.e. all the cases that are not simple label noise ).

Again, in this analysis, we ignore the influence of the data sample and the optimization technique. We use all the assumptions introduced when defining the learning problem in SS 2 and the BR results SS 4.

The BR equalities for cleaning kernelsThe theorems proved in SS 4 can then be restated, in terms of learning problems and pseudo-inverse \(^{}: Z\), as

\[(,,P)(^{}(),  P)\.\] (5)

We will refer here to the pseudo-inverse of our corruption as the _cleaning kernel_. Notice that the set \(^{}()\) is not trivially decomposable as \(}\) for some loss and model class. In this case, \(^{}()\) is said to have no _\(\)-factorized structure_.

The BR equalities are ensuring the existence of a function \(f^{*}\ \ ^{}()\) that minimizes the Bayes Risk, i.e.

\[f^{*}*{arg\,min}_{f}_{P}f(Z) f^{*}*{arg\,min}_{f^{}( )}_{ P}f(Z)\.\]

Sadly, this is not enough for us to find an optimal hypothesis working for both probability spaces. Formal results on this optimal \(h\) for both clean and corrupted spaces only exist for label noise . However, by introducing a few further assumptions, we can get results on which alternative loss to use on train distribution so that the learned \(h\) on \(\) will have the same performance scores as the optimal on \((,,P)\). Let us consider the composed representation of the function \(f^{*}\) in the test (clean) minimization set, which is \(f^{*}= h^{*}\). We want to construct a suitable composed representation for \(f^{*}\) also in the space \(^{}()\), namely \(f^{*}=^{*}\). We start by fixing a \(^{*}\) of our choice, that if asked to be invertible (**A1**) identifies the loss function as \(= h^{*}(^{*})^{-1}:(Y) Y ^{+}\).7 There can be weaker conditions on \((,^{*})\) enabling all the following results, but do not investigate the here.

Since in general \(^{}(f^{*}) f^{*}\), we have that: \( h^{}\) s.t. \(^{}( h^{})=^{*}\,\) where we ask \(h^{} h^{*}\), otherwise we would be imposing the trivial condition \( h^{*}=^{*}=^{}( h^{*})\), i.e. the corruption is harmless w.r.t. the Bayes Risk value. In order to study the possible loss correction, we choose the corrupted optimum as \(^{*}=h^{}\) (**A2**).

Loss correctionsWe now try to formalize how to define a suitable loss for the corrupted learning problem, such that the optimal hypothesis is learned in the clean learning space. The problem setting gives us access to \(,^{}\) given by the problem, and \(^{*}\) chosen by us. We want to find a way to retrieve a suitable \(h^{*}\) for the clean distribution. That means, the loss correction task here is _finding a formulation of \(\) that depends on \(,^{}\)_. An essential preliminary result, for which the proof is given in SS 4.1, is

**Lemma 7**.: _The feasible factorization of a Markov kernel \(\) is also a valid factorization for its pseudo-inverse \(^{}\), both for the full kernel or considering their parameterized versions._

We then give the correction results (proof in SS S4.2), and discuss them.

**Theorem 8**.: _Let \((,,P)\) be a clean learning problem and \((^{}(), P)\) its associated corrupted one, not necessarily with a \(\)-factorized structure. Let \(^{}\) be the joint cleaning kernel reversing \(\), such that assumptions **A1** and **A2** hold for the said problems. The factorization of \(^{}\) is assumed to be feasible and to have an equality result of the form Eq. (5). We write \(^{}(dz,)=^{X}(dx,)^{Y}(dy,)\), with \(()\) some feasible parameters. Hence, we can prove the following points:_

1. _When_ \(^{}\) _is either_ \((id_{X}\)_, S-Y) or_ \((id_{X}\)_, 2D-Y), we can write the corrected loss as_ \[(h(),)=(^{Y})(h(), {y})\,(,)\,\] _with_ \(^{Y}=^{Y}_{}\) _for the second case._
2. _When_ \(^{}\) _is_ (S-_X_, S-Y), (2D-_X_, S-Y) or (S-_X_, 2D-_Y), we have_ \[(,,h)=_{u^{X}h()}[ ^{Y}(u,)]\,(,) \,\] _with_ \(^{X}_{}h()(A)^{X}(h^{-1}(A),)\,\ A (Y)\) _being the push-forward probability measure of_ \(^{X}()\) _through_ \(h\)_,_ \(h\) _seen as a function. For the cases that involve a 2D corruption, we have_ \(^{Y}=^{Y}_{}\) _for the former_ \(^{}\) _factorization,_ \(^{X}h()=^{X}_{}h()\) _for the latter._
3. _When_ \(^{}\) _is a (1D-_X_, 1D-_Y) corruption, we can write the corrected loss as_ \[(,,h)=_{u^{X}h()}[ ^{Y}(u,)]\,(,) \,\] _with_ \(^{X}_{}h()(B)^{X}(h^{-1}(B),)\,\ B (X)\)_._
4. _When_ \(^{}\) _is a (2D, 1D) corruption, we can write the corrected loss as_ \[(,,h)=_{u^{X}h()}[ _{}{}^{Y}(u,)], (,,h)=_{u^{X}_{}h()}[ ^{Y}(u,)]\,(,) \.\] _for the (1D-_X_, 2D-_Y_), (2D-_X_, 1D-_Y_) respectively._

When minimized, the corrected losses will by construction give back the hypothesis \(^{*}\). Since \( h^{*}=^{*}\), the learned \(^{*}\) has on the clean distribution the optimum performance we wanted to achieve with the original loss function \(\). Hence, we achieve unbiased learning in the sense of matching scores and in the distributional sense.

The corrections found by the theorem are more complex than the ones defined in previous work [7; 34], i.e., the first part of point 1. In the second part, we characterize the effect of a more "dependent" \(Y\) cleaning kernel, i.e. closer to the root in Fig. 1a. When also \(^{X}\) is non-trivial in the factorization, we have an action on \(h\). Then, the corrected functions lie in a larger function space than the usual one, the one of positive, bounded functions \(:X Y^{+}\).

The result additionally underlines how the cleaning kernel affects the a hypothesis on \(X\): it induces a set of "reachable predictions" from \(h\) through \(^{}\), depending on the outcome of the stochastic process \(^{}: X\). The push-forward probability measures are probabilities on a _set of probabilities_. For instance, in point 2 we have \(^{X}_{}h()((Y))\), while for point 3 we have \(^{X}_{}h()((X))\).

## 6 Conclusions

We proposed a comprehensive and unified framework for corruption using Markov kernels, systematically studying corruption in three key aspects: classification, consequence, and correction. We established a new taxonomy of corruption, enabling qualitative comparisons between corruption models in terms of the corruption hierarchy. To gain a deeper quantitative understanding of corruption, we analyzed the consequences of different corruptions from an information-theoretic standpoint by proving Data Processing Equalities for Bayes Risk. As a consequence of them, we obtained loss correction formulas that gives us more insights into the effect of corruption on losses.

Throughout the work, we consider data as probability distributions, implicitly assuming that each dataset has an associated probabilistic generative process. We treat corruption as Markov kernels, assuming full access to their actions, and analyze the consequences of corruption through Bayes risks without accounting for sampling or optimization. Bridging the gap between the distributional-level and the sample-level results would be the next step for this study, which requires tailored ad-hoc analyses. Other directions for making this framework more practically usable include developing quantitative methods to compare corruption severity and investigating the effects of optimization algorithms on the analysis.

[MISSING_PAGE_EMPTY:10]

*  Fredrik Dahlqvist, Vincent Danos, Ilias Garnier, and Ohad Rammar. Bayesian inversion by \(\)-complete cone duality. In _27th International Conference on Concurrency Theory_, 2016.
*  Kenta Cho and Bart Jacobs. Disintegration and bayesian inversion via string diagrams. _Mathematical Structures in Computer Science_, 29(7):938-971, 2019.
*  Bart Jacobs and Fabio Zanasi. The logical essentials of bayesian reasoning. _Foundations of Probabilistic Programming_, pages 295-331, 2020.
*  Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Vaughan. A theory of learning from different domains. _Machine Learning_, 79:151-175, 2010.
*  George Shackelford and Dennis Volper. Learning k-dnf with noise in the attributes. In _Proceedings of the first annual workshop on Computational learning theory_, pages 97-103, 1988.
*  Sally A. Goldman and Robert H. Sloan. Can pac learning algorithms tolerate random attribute noise? _Algorithmica_, 14(1):70-84, 1995.
*  Dana Angluin and Philip Laird. Learning from noisy examples. _Machine Learning_, 2:343-370, 1988.
*  Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In _Proceedings of the eleventh annual conference on Computational learning theory_, pages 92-100, 1998.
*  Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making deep neural networks robust to label noise: A loss correction approach. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1944-1952, 2017.
*  Leon A Gatys, Alexander S Ecker, and Matthias Bethge. A neural algorithm of artistic style. _arXiv preprint arXiv:1508.06576_, 2015.
*  Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14_, pages 694-711. Springer, 2016.
*  Eric Grinstein, Ngoc QK Duong, Alexey Ozerov, and Patrick Perez. Audio style transfer. In _2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)_, pages 586-590. IEEE, 2018.
*  Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. _arXiv preprint arXiv:1312.6199_, 2013.
*  Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. _arXiv preprint arXiv:1412.6572_, 2014.
*  Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. The limitations of deep learning in adversarial settings. In _2016 IEEE European symposium on security and privacy (EuroS&P)_, pages 372-387. IEEE, 2016.
*  Alexey Kurakin, Ian J Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In _Artificial intelligence safety and security_, pages 99-112. Chapman and Hall/CRC, 2018.
*  Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15262-15271, 2021.
*  Jiacheng Cheng, Tongliang Liu, Kotagiri Ramamohanarao, and Dacheng Tao. Learning with bounded instance and label-dependent label noise. In _International Conference on Machine Learning_, pages 1789-1799. PMLR, 2020.
*  Yu Yao, Tongliang Liu, Mingming Gong, Bo Han, Gang Niu, and Kun Zhang. Instance-dependent label-noise learning under a structural causal model. _Advances in Neural Information Processing Systems_, 34:4409-4420, 2021.
*  Qizhou Wang, Bo Han, Tongliang Liu, Gang Niu, Jian Yang, and Chen Gong. Tackling instance-dependent label noise via a universal probabilistic model. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 10183-10191, 2021.
*  Nathalie Japkowicz and Shaju Stephen. The class imbalance problem: A systematic study. _Intelligent data analysis_, 6(5):429-449, 2002.
*  Haibo He and Edwardo A Garcia. Learning from imbalanced data. _IEEE Transactions on knowledge and data engineering_, 21(9):1263-1284, 2009.
*  Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. A systematic study of the class imbalance problem in convolutional neural networks. _Neural networks_, 106:249-259, 2018.
*  Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift with black box predictors. In _International conference on machine learning_, pages 3122-3130. PMLR, 2018.

*  Gilles Blanchard, Marek Flaska, Gregory Handy, Sara Pozzi, and Clayton Scott. Classification with asymmetric label noise: Consistency and maximal denoising. _Electronic Journal of Statistics_, 10(2):2780-2824, 2016.
*  Julian Katz-Samuels, Gilles Blanchard, and Clayton Scott. Decontamination of mutual contamination models. _Journal of machine learning research_, 20(41), 2019.
*  Arthur Gretton, Alex Smola, Jiayuan Huang, Marcel Schmittfull, Karsten Borgwardt, and Bernhard Scholkopf. Covariate shift by kernel mean matching. _Dataset shift in machine learning_, 3(4):5, 2009.
*  Masashi Sugiyama and Motoaki Kawanabe. _Machine learning in non-stationary environments: Introduction to covariate shift adaptation_. MIT press, 2012.
*  Tianyi Zhang, Ikko Yamane, Nan Lu, and Masashi Sugiyama. A one-step approach to covariate shift adaptation. In _Asian Conference on Machine Learning_, pages 65-80. PMLR, 2020.
*  Kun Zhang, Bernhard Scholkopf, Krikamol Muandet, and Zhikun Wang. Domain adaptation under target and conditional shift. In _International conference on machine learning_, pages 819-827. PMLR, 2013.
*  Mingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and Bernhard Scholkopf. Domain adaptation with conditional transferable components. In _International conference on machine learning_, pages 2839-2848. PMLR, 2016.
*  Xiyu Yu, Tongliang Liu, Mingming Gong, Kun Zhang, Kayhan Batmanghelich, and Dacheng Tao. Label-noise robust domain adaptation. In _International conference on machine learning_, pages 10913-10924. PMLR, 2020.
*  Gerhard Widmer and Miroslav Kubat. Learning in the presence of concept drift and hidden contexts. _Machine learning_, 23:69-101, 1996.
*  Jie Lu, Anjin Liu, Fan Dong, Feng Gu, Joao Gama, and Guangquan Zhang. Learning under concept drift: A review. _IEEE Transactions on Knowledge and Data Engineering_, pages 1-1, 2018. doi: 10.1109/tkde.2018.2876857.
*  Brendan Van Rooyen et al. _Machine learning via transitions_. PhD thesis, The Australian National University, 2015.
*  Bianca Zadrozny. Learning and evaluating classifiers under sample selection bias. In _Proceedings of the twenty-first international conference on Machine learning_, page 114, 2004.
*  Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert Muller. Covariate shift adaptation by importance weighted cross validation. _Journal of Machine Learning Research_, 8(5), 2007.
*  Ievgen Redko, Emilie Morvant, Amaury Habrard, Marc Sebban, and Younes Bennani. A survey on domain adaptation theory: learning bounds and theoretical guarantees. _arXiv preprint arXiv:2004.11829_, 2020.
*  Christopher M Bishop and Nasser M Nasrabadi. _Pattern recognition and machine learning_, volume 4. Springer, 2006.
*  Madelyn Glymour, Judea Pearl, and Nicholas P Jewell. _Causal inference in statistics: A primer_. John Wiley & Sons, 2016.
*  Vladimir Igorevich Bogachev. _Measure theory_, volume 1. Springer, 2007.