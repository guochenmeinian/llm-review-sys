# Goal Reduction with Loop-Removal Accelerates RL and Models Human Brain Activity in Goal-Directed Learning

Goal Reduction with Loop-Removal Accelerates RL and Models Human Brain Activity in Goal-Directed Learning

Huzi Cheng

Department of Psychological and Brain Sciences

Indiana University Bloomington

hzcheng15@gmail.com

&Joshua W. Brown

Department of Psychological and Brain Sciences

Indiana University Bloomington

jwmbrown@iu.edu

###### Abstract

Goal-directed planning presents a challenge for classical RL algorithms due to the vastness of the combinatorial state and goal spaces, while humans and animals adapt to complex environments, especially with diverse, non-stationary objectives, often employing intermediate goals for long-horizon tasks. Here, we propose a goal reduction mechanism for effectively deriving subgoals from arbitrary and distant original goals, using a novel loop-removal technique.1 The product of the method, called _goal-reducer_, distills high-quality subgoals from a replay buffer, all without the need for prior global environmental knowledge. Simulations show that the _goal-reducer_ can be integrated into RL frameworks like Deep Q-learning and Soft Actor-Critic. It accelerates performance in both discrete and continuous action space tasks, such as grid world navigation and robotic arm manipulation, relative to the corresponding standard RL models. Moreover, the _goal-reducer_, when combined with a local policy, without iterative training, outperforms its integrated deep RL counterparts in solving a navigation task. This goal reduction mechanism also models human problem-solving. Comparing the model's performance and activation with human behavior and fMRI data in a treasure hunting task, we found matching representational patterns between a _goal-reducer_ agent's components and corresponding human brain areas, particularly the vmPFC and basal ganglia. The results suggest that humans may use a similar computational framework for goal-directed behaviors.

## 1 Introduction

Humans and animals must develop capabilities to pursue time-varying goals in continuously changing environments. The pressure for survival prohibits slow, linear adaptation to different goals, i.e., learning value functions from scratch for each new objective. A quick and versatile paradigm is necessary for such goal-directed learning scenarios. However, traditional Reinforcement Learning (RL) algorithms are not specifically designed for this, encountering challenges in the goal-directed learning context. They are highly optimized for scenarios with relatively fixed goals, e.g., winning in Go, or reducing building energy consumption . In these situations, iterative methods like the Bellman equation are effective for approximating value/advantage/negative cost functions across various states, maintaining stability over time. Nonetheless, if the goal changes during training, classical RL becomes highly inefficient due to: 1) The significant increase in state space caused by the introduction of the goal set; 2) The inability to reuse experiences across different goals.

On the other hand, traditional heuristic algorithms like Dijkstra's shortest path algorithm  and the A* path-finding algorithm  excel in these problems. These algorithms leverage goal-independent environment knowledge to construct real-time goal-conditioned maps during navigation. They use selective intermediate goal states to simplify the problem, thus reducing pathfinding time. By breaking down a distant goal into nearer ones, these algorithms utilize local knowledge, independent of goal changes, to solve the problem. More importantly, this principle resembles ways humans use to deal with complex problems. Recently, a number of studies have shown that, besides stimuli and association representation, human and animal brains leverage goal and reduced subgoal representations to solve tasks in various settings [44; 11; 45; 40]. But a common drawback of these algorithms is that they require predefined representations of all states in the task, prohibiting them from scaling to large-scale realistic tasks.

In this paper, we try to bridge such goal reduction mechanisms with neural models. The resulting algorithm is an effective _goal-reducer_ that can accelerate and beat standard RL in different tasks through recursively reducing complex goals into simpler subgoals. The main contributions of this paper are two-fold: **1)** Computationally, we propose novel methods to train an effective _goal-reducer_. After just random walking, it extracts nearly optimal subgoals when the state-goal combination space is large without prior knowledge about the cost/effort between them. This is based on the effective representation structure learned during training. This mechanism, as shown below, can not only be used to accelerate standard RL algorithms in various settings but also to guide navigation in a multi-goal setting without iterative Bellman equation convergence processes and outperforms RL accelerated with it, let alone standard RL itself. **2)** Biologically, by comparing the model's activity with brain fMRI data in the same cognitive task, we show the similarity between the model's component, e.g., the _goal-reducer_, and brain regions like vmPFC and putamen, highlighting the potential of using this model to explain how brains solve goal-directed problems.

## 2 Related work

### Goal-directed RL in deep learning

Developing an effective goal-directed algorithm has been a long-lasting open question in the RL community . Various methods have been proposed to mitigate the experience sparsity issue in goal-directed learning. For an extensive review, we refer the reader to . Here, we introduce some representative solutions, where some focus on the modification of reward functions, using tricks like average action cost  and sibling trajectory average goals  to alleviate the sharpness in reward distribution in the early stage of training. Another line of work uses planning to solve the problem. For example, an explicit graph can be constructed from sensory inputs for traditional path-finding algorithms [16; 55; 34; 26]. However, we see these approaches as not completely neural-based models, and the pathfinding algorithm part may prohibit them from smoothly scaling to tasks with larger state spaces.

Another family of algorithms uses subgoal generation as a core mechanism to resolve the same issue.  trained a GAN to predict intermediate goals. There are also attempts to construct subgoals with different heuristics:  uses the uncertainty of Q-functions to help train subgoals.  treats the midpoint of value functions as optimal subgoals during training.  optimizes subgoal generation by minimizing integrated segment costs produced by "local" subgoals. These subgoal-related approaches are intriguing as they match the "divide and conquer" principle in a neural way, but a common limitation of these methods is that they work in a bootstrapping manner, i.e., the quality of the Q-function indirectly determines the quality of subgoals generated, as it is involved in the subgoal sampling process.

### Goal-directed learning and subgoal generation in neuroscience

There exists a duality of interest in neuroscience on goal-directed learning that can be dated back to the 1940s, when  famously showed that rats can take shortcuts that hadn't been experienced before to reach goals in a maze. Later research showed that there are representations for different goals in the brain that are tailored for planning optimal paths to the ultimate goal . Models have been built for these processes to reveal possible mechanisms of how the brain may make use of these goal representations to calculate subgoals (or called "landmarks") for navigation [46; 47]. Recently, more empirical and modeling work has come out showing that the subgoal navigation hypothesis, and the underlying cognitive map theory that supports it , could be implemented by animals [23; 45] and humans [50; 15; 54]. Altogether, these studies imply the existence of a subgoal generation mechanism in the brain to support _effective_ planning in complex environments.

However, many models for subgoal generation in neuroscience still either rely on manual/one-hot coding of states [15; 54] or focus on revealing the existence rather than the potential development of subgoals during training . We therefore think that, in these two fields, the community of deep RL and neuroscience, there is space for a neurally plausible algorithm that can provide a biological model of how subgoals are naturally generated from the brain during training and can show its computational efficiency over plain RL algorithms.

Our resulting work presents a trained _goal-reducer_ neural network that generates nearly optimal subgoals without requiring additional value information from the environment. This approach distinguishes itself from other subgoal generators that depend on Q-functions. It can integrate seamlessly with standard RL frameworks and independently operate with a local policy that just learns associations in neighboring states. Using the latter approach, we also demonstrate its capability in solving cognitive tasks in a human-like manner and its correspondence with various brain regions, indicating its potential for modeling human problem-solving processes.

## 3 Methods

### Problem formulation

For a goal-directed Markov decision process, we characterize it with \((,,)\), where \(\) and \(\) are the observation and action spaces, respectively, and \(\) is the goal space. One interaction step in this environment can be written as \((s_{t},g,a_{t},r_{t},_{t},s_{t+1})\), where \(s_{t}\) is the current observation, \(g\) is the assigned goal, \(a_{t}\) is the action executed, \(r_{t}\) is the reward, \(_{t}\) is the achieved goal, and \(s_{t+1}\) is the next state. In some cases, \(=\) occurs when a possible goal is among one of all states, e.g., spatial navigation when the input and goal space is all plausible locations. In other words, the goal and state space are the same. In such settings, we may use \(\) and \(\) interchangeably, and the interaction can be reduced to \((s_{t},g,a_{t},r_{t},s_{t+1})\). But in some real-world tasks, \(\) may be in a different space from \(\): \(\) could be a target coordinate in allocentric space when \(\) is the pose of a multi-joint robot arm.

With this formulation, we first consider the simple case when \(=\) as the general case can be easily extended from it. In goal-directed learning, \(g\) may change substantially, making conventional RL algorithms inadequate due to the enormous size of \(\). Additionally, in many scenarios, the reward is sparse, and the agent only receives a positive reward or avoids punishment when it reaches the goal state, i.e., when \(=\) and \(r(s_{t},a_{t},g)=(s_{t}=g)\). This, combined with the complexity of the state space, further complicates goal-directed learning.

We propose that, given a specific \(g\), if an agent can effectively reduce a goal \(g\) into a subgoal \(s_{g}\) that is "closer" to its current state \(s_{t}\), it may alleviate the task's difficulty. Furthermore, this can be applied **recursively** to find subgoals that are arbitrarily close to the current state \(s_{t}\).

### Effective _goal-reducer_ through _Loop-removal sampling_

The most straightforward solution to the problem above is to train a function \(\) that generates \(s_{g}\), which we refer to as a _goal-reducer_: \((s_{t},g):\). The _goal-reducer_ can reduce the computational burden of a policy \((a|s,g)\) as it can now generate a subgoal \(s_{g}=(s,g)\) for a hard task, under a basic assumption that harder problems can be decomposed into simpler problems as long as these problems are in the optimal path towards the final solution (Fig. 1A). Training such a _goal-reducer_, however, can be challenging. There are two intuitive strategies. First, one can sample \(s_{g}\) uniformly from \(\), as done by . This approach is referred to as _Random sampling_ (Fig. 1B left). An alternative is to sample \(s_{g}\) from past experiences, a strategy known as _Trajectory sampling_. Assuming a sequence of interactions \((s_{t},g,a_{t},r_{t},s_{t+1})\) for \(t=1,2,,T\) is stored in memory, \(s_{g}\) can be sampled from the \(s_{t}\) in this sequence (Fig. 1B middle). A common technique in goal-directedRL, Hindsight Experience Replay (HER), proposed by , conceptually resembles this approach, as it encourages the agent to learn associations between current states and some of its future states in the same trajectory. Other studies, such as , also find trajectory-based sampling effective in goal-directed learning, though expert experience is not required in our case . Furthermore, neuroscience research indicates that episodic memories, i.e., sequences of experienced states, are vital for learning .

However, this strategy does not guarantee the sampled \(s_{g}\) to be effective: an agent with limited environmental knowledge may simply engage in a random walk within the state transition graph, rendering the states experienced in the episodic memory ineffective for connecting the trajectory's start and end points. To address this, we introduce a third strategy, termed _Loop-removal sampling_ (Fig. 1B right), which may mitigate this issue. The underlying rationale is that when the agent has minimal knowledge of the environment, the trajectories it creates will likely be random and involve numerous "loops". A "loop" occurs when the agent revisits the same state at least twice within a trajectory. The _Loop-removal sampling_ posits that by eliminating these "loops" from the episodic memory, the \(s_{g}\) sampled from the remaining trajectories will be more advantageous, as ineffective experiences are excluded, potentially resulting in a trajectory that more closely resembles a linear or shortest path in the best-case scenario.

While _Loop-removal sampling_ is effective when \(\) is discrete, i.e., observations are either the same or different, it faces challenges in environments where \(s_{i}\) can be infinitely close but not the same to each other, e.g., when the observation is an image input for an agent's current location. This issue is also encountered in earlier concepts like . To overcome this, we adopt an idea from persistent homology , defining a filtration process to determine the existence of "loops."

This process is schematically depicted in Fig. 1C: In a continuous space, trajectories are sparse, as distances between states are always greater than zero, making exact state overlaps unlikely. However, by assigning a filtration radius \(\) to each state and incrementally increasing it, the connectivity of the episodic memory trajectory changes. At a certain point, the algorithm detects a "loop" (proper case in Fig. 1C), where _Loop-removal sampling_ is most effective. If the filtration radius continues to increase, the trajectory becomes fully interconnected, leading to _Loop-removal sampling_ failure. We demonstrate that with an appropriate filtration parameter, a random walk path in 3D space can be effectively simplified (Fig. 1D) with \(\) set to 0.8. The optimal radius is determined through a grid search between 0 and the longest distance between any pair of points in the space, with the objective of maximizing the rate at which task performance improves with learning. This search process is also used in later experiments to maximize learning efficiency in terms of two efficiency metrics, _Optimality_ and _Equidex_ (described in detail below).

Fig. 1: **A**: A schematic view of how a _goal-reducer_ can be integrated with a policy network to generate actions. **B**: 3 types of subgoal sampling strategies. Red triangles: current states, green squares: goals, light green squares: subgoals sampled. **C**: An diagram of filtration radius changes trajectory connectivity. **D**: An example of proper filtered trajectory (black) compared with original random walk (gray) in 3D space.

This approach may seem akin to the _Search on the Replay Buffer_ by , but there is a key distinction: _Search on the Replay Buffer_ depends on a value function to estimate distances among states, whereas _Loop-removal sampling_ operates independently of any prior environmental knowledge. Instead, it relies on a minimal assumption applicable to all state spaces: loops indicate redundancy. Following this, we developed an online mechanism ( Algorithm 1 in appendix) to train the _goal-reducer_, \(\), using loop-removed trajectories stored in \(^{}\), which is refined from the replay buffer \(\).

To test if a _goal-reducer_ trained with _Loop-removal sampling_ generates better subgoals compared to _Trajectory sampling_ and _Random sampling_, we developed two metrics to quantify the quality of generated subgoals: _Optimality_ and _Equdex_. First, we represent the effort required for an agent to reach goal \(g\) from state \(s\) as \(||s,g||\), the distance across legal transitions of the state graph. This concept mirrors the idea of shortest distances, though it may not be symmetrical, i.e., \(||s,g||||g,s||\), due to potentially irreversible state transitions. Also, the state space graph of legal transitions may not be fully connected - because barriers may exist in the state space so that some transitions are not possible, \(s_{g}\) is generally not simply the linear midpoint between \(s\) and \(g\). Building on this, _Optimality_ is defined as:

\[(s,g,s_{g})=||s,g||/||s,s_{g}||+||s_{g},g||.\] (1)

When _Optimality_\( 1\), the total effort expended by an agent to reach \(g\) via \(s_{g}\) is near the optimal effort. However, a \(s_{g}\) with an _Optimality_ close to 1 may not be informative if \(s_{g}\) is set to either \(g\) or \(s\), where no new information is provided. To address this, _Equdex_ is introduced:

\[(s,g,s_{g})=||s_{g},g||-||s,s_{g}||/||s,s_ {g}||+||s_{g},g||.\] (2)

When _Equdex_\( 0\), it indicates that the effort from \(s\) to subgoal \(s_{g}\) is similar to the effort from \(s_{g}\) to the final goal \(g\). This suggests that \(s_{g}\) is situated on the hyperplane formed by midpoints in the state space between \(s\) and \(g\). Accordingly, as \( 1\), the subgoal becomes closer to the current state, while a value nearing -1 suggests the subgoal is closer to the ultimate goal. In summary, the quality of a single subgoal can be characterized by both _Optimality_ and _Equdex_. The ideal subgoal is one where _Optimality_ equals 1 and _Equdex_ equals 0.

Using these indices, we assess the performance of the _goal-reducer_ on two datasets. The first dataset derives from a constructed state graph, characterized by random connections among states and allowing self-connections (Fig. 2A top). We execute undirected random walks on this graph. The second is based on a four-room gridworld task, commonly used in multi-goal RL benchmark tests (Fig. 2A bottom). In this dataset, we simulate an agent exploring the environment without any prior knowledge of its structure.

For both datasets, we use the _goal-reducer_ architecture to learn the \(s,g s_{g}\) association, employing an VAE . This network accepts concatenated representations of \(s\) and \(g\) as inputs, initially producing a latent probabilistic representation \(z\) through the encoder \(_{}\). A prediction of \(s_{g}\) is then generated by \(_{}\). To circumvent sparse and discrete encoding in both datasets, and also to avoid introducing prior knowledge, random embeddings are applied for each. Consequently, \(s\)

Figure 2: _goal-reducer_ training results of geometric random graph (top) and the four-room gridworld task (bottom) with different strategies. **A**: Environment examples. **B**: Left: training loss, middle: training _Optimality_, right: training _Equdex_. **C**: Left: _Optimality_ change when applying a trained _goal-reducer_ recursively, right: same, but for _Equdex_.

\(g\), and \(s_{g}\) are represented as high-dimensional random vectors. All random embeddings remain fixed during training. The adopted loss function mirrors that of a classical VAE, comprising a subgoal reconstruction loss and a weighted (\(\)) KL divergence penalty with a prior latent distribution \(p(z)(0,I)\):

\[L=- D_{KL}(_{}(s_{g}|s,g)||p(z))+_{_{ }(z|s,g)}[_{}(s_{g}|z)].\] (3)

Training results (Fig. 2B left) indicate that in both datasets _Loop-removal sampling_ surpasses and is more stable than _Trajectory sampling_, which in turn outperforms _Random sampling_ in terms of loss. The _Optimality_ values follow a similar trend (Fig. 2B middle):

_Loop-removal sampling \(>\) Trajectory sampling \(\) Random sampling_.

Given that all subgoals are uniformly sampled from the dataset, the _Equidex_ for all strategies approximates 0 (Fig. 2B right). To further examine the capability and stability of the trained _goal-reducer_, we assess its proficiency in recursive subgoal reduction. In this process, we input the predicted subgoal embedding into the _goal-reducer_, treating it as \(g\) to generate a subsequent subgoal \(s^{}_{g}\). By iterating this procedure for several steps, we postulate that the _goal-reducer_, particularly when trained with _Loop-removal sampling_, will produce subgoals that are both effective and sufficiently close to the current states to facilitate easier navigation. The experimental results reveal that, after three iterations of recursive goal reduction (\(t=3\), in Fig. 2C), the _goal-reducer_ trained with _Loop-removal sampling_ achieves the most favorable optimality distribution (Fig. 2C left).

In terms of _Equidex_, as illustrated in the right column of Fig. 2C, recursive goal reductions enhance the _goal-reducer_'s ability to predict subgoals that are nearer to the current states. This improvement is evidenced by the shift in the equidex distribution from \(t=1\) to \(t=3\). Together, the results in Fig. 2 demonstrate the superiority of _Loop-removal sampling_ as a training strategy for _goal-reducer_ to produce proper subgoals.

### _goal-reducer_ integrated with RL

The above experiments have shown the superiority of _Loop-removal sampling_ over other strategies in training _goal-reducer_ from environments without external knowledge about the "distance" between any two states in them. Next we integrate this process with RL algorithms into tasks with discrete and continuous action spaces using Deep Q-learning (DQL)  and Soft Actor-Critic (SAC) .

Discrete caseIn the discrete setting, DQL is used. The algorithm is trained to optimize a value function \(Q\) through Bellman equation convergence:

\[Q_{_{k+1}} =_{}_{(s_{t},g,a_{t},s_{t+1})  D}[Q_{t}^{*}-Q_{}(s_{t},g,a_{t})]^{2}\] (4) \[Q_{t}^{*} =r(s_{t},g,a_{t})+_{a_{t+1}(..|s_{t+1},g)}Q_{_{k}}(s_{t+1},g,a_{t+1}).\] (5)

Since we assume _goal-reducer_, \((s,g)\), can learn to generate subgoals without a mature \(Q\), we use it to accelerate the convergence of Eq. 4 by updating \(Q\)'s parameter using an extra regularization loss

\[L_{s_{g}}=_{s_{t},g}w_{s_{t},g} D_{KL}a_{t}|s_{t},g a_{t}|s_{t},(s,g),\] (6)

where the policy \(a_{t}|s_{t},g\) is a categorical distribution softmax\(Q(s_{t},g)\) and the loss is weighted by the entropy of policy, \(H[(a_{t}|s_{t},g)]\):

\[w_{s_{t},g}=1,&H[(a_{t}|s_{t},g)] H[(a_{t}|s_{t}, (s,g))]\\ 0,&\] (7)

Continuous caseIn continuous action space, SAC is used. Since \(a_{t}|s_{t},g\) cannot be calculated explicitly, we approximate it with an online sampling process of actions executed \(a^{}_{t}\):

\[L_{s_{g}}=_{s_{t},a^{}_{t},g}w_{s_{t},g}a^{}_{ t}|s_{t},ga^{}_{t}|s_{t},(s,g),\] (8)where the \(w_{s_{t},g}=1\) if \(a_{t}^{}|s_{t},g<a_{t}^{}|s_{t},(s,g) \) and otherwise 0. In both cases, \(w_{s_{t},g}=1\) means \(Q\) is more uncertain about the ultimate goal when compared with a trustworthy goal generated by the _goal-reducer_.

For both cases, for baseline RL methods (plain DQL and SAC) and their _goal-reducer_ augmented version, to accelerate learning, we used Hindsight Experience Replay (HER) during training, a standard technique used to improve RL algorithm's performance in goal-directed learning .

### Standalone _goal-reducer_

As shown in previous _Optimality_ and _Equidex_ experiments, _goal-reducer_ can gradually reduce the "distance" between the agent and the goal through recursive goal reduction. We thus test if this mechanism alone can solve some tasks that are usually handled with model-free RL using Bellman equation iteration. To do this, we take an unsupervised approach: When an agent is initialized, it explores the environment randomly. We train the _goal-reducer_ using such exploration trajectories. At the same time, we train a local policy \(_{}a_{t}|s_{t},g\) that only learns goals that are one step away from \(s_{t}\) and generates a uniform distribution otherwise. When these two components are trained, we execute the planning process by detecting reachable goals using the entropy of \(_{}\) (for details, see the appendix).

## 4 Results

### _goal-reducer_ accelerates standard RL

Four-room maze navigation taskIn a modified mini-grid environment  (Fig. 3A left), an agent receives two images as inputs and outputs an action indicating which direction to go among four possible directions. One image is the partial observation in a four-room grid world maze, \(s_{t}\), while the other image is a similar "picture" from the "goal" location, \(g\). No other visual cue beyond the "picture" of the goal location is given, preventing the agent from cheating using easy visual landmarks like the green dots used in the original version of this task. In each episode within this environment, \(g\) and \(s_{0}\) are uniformly sampled from all possible locations. The agent receives a constant negative reward every step until it reaches \(g\). In this experiment, the DQL algorithm is represented as **DRL**, while _goal-reducer_ augmented DQL is denoted **DRL+GR**. The results clearly show that **DRL+GR** outperforms **DRL** in terms of convergence time (Fig. 3A right).

Robot arm reach taskWe adopted panda-gym  to implement an environment where a robot arm with 7 degrees of freedom is trained to reach an arbitrary location sampled uniformly in the space (Fig. 3B left). Like the navigation task, the agent receives a constant negative reward every interaction before reaching within a close region centered on the specified goal location. In this experiment, for plain DQL we used SAC (denoted as **DRL**) and _goal-reducer_ augmented SAC (**DRL+GR**). The results (Fig. 3B right) are consistent with the navigation task.

### Standalone _goal-reducer_ surpasses _goal-reducer_-accelerated RL

The next question we ask is related to the results in recursive goal reduction (Fig. 2C): Can one use just the _goal-reducer_ to perform a task as it seems to reduce _Equidex_ recursively to neighbor goals? This requires a standalone _goal-reducer_ and a "local" policy \(_{}\) that can learn how to associate

Figure 3: _goal-reducer_ accelerates standard RL. **A**: An example input in the four-room navigation task (left) and performance comparison (right). **B**: Robot arm reach task (left) and performance comparison (right). **C**: How a _goal-reducer_ agent works with only a local policy (left) and performance comparison of 3 algorithms (right).

and \(s_{t+1}\) with \(a_{t}\). The Four-room navigation task naturally fits this need, as a \(_{}\) in it can be easily defined as a policy that learns to associate two connecting grids. Under this setting, a _goal-reducer_ can recursively generate subgoals using existing goals/subgoals until \(_{}\) finds proper subgoals that are close enough to make a proper decision.

This time a 19x19 maze is used to make the task harder, as _goal-reducer_'s training effect may not be obvious in smaller environments. In this environment, we dropped the Bellman equation (for details, see appendix) and compared its performance (denoted as **GR**) with the previous winner, _goal-reducer_ augmented DQL (**GR+DRL**), and the baseline plain DQL (**DRL**). Results in Fig. 3C right column clearly show that **GR** outperforms both **GR+DRL** and **DRL**, while the latter two's performance relationship is consistent with the right column in Fig. 3A.

### _goal-reducer_ in the Brain

The efficiency the _goal-reducer_ has shown in previous experiments, when compared to plain RLs, naturally leads us to wonder if the brain adopts a similar strategy to solve goal-directed behaviors. To address this, we used a cognitive task, treasure-hunting (Fig. 4A), that necessitates flexible goal representation changes. In the task, subjects were placed in one of four possible starting states on one of two maps (Fig. 4B) and were required to reach states designated as a "chest" (the ultimate goal). But having a "key" is necessary when reaching the "chest" to obtain a reward. The locations of the "key" and "chest" are presented to the subject at the start of each episode and change randomly across episodes. The _goal-reducer_, \((s,g)\), when paired with a \(_{}\), forms an agent and is also trained on the same task, using the same strategy adopted in Fig. 3C.

After training, we analyzed the _goal-reducer_ agent's neural activation and compared it with human subjects' brain activity measured via fMRI. The fMRI data, including human subjects' participation details, were reported previously, and human subjects were compensated $25/hr for fMRI participation . In particular, subjects were apprised of the mild risks of fMRI including boredom, fatigue, and loss of confidentiality. All research was approved by the institution's IRB, and all subjects provided full informed consent. Our fMRI analysis used representational similarity analysis (RSA) . RSA considers all conditions that occurred in the task and compares the activity similarity of the

Fig. 4: _goal-reducer_ in the treasure hunting task. **A**: The treasure hunting task description. **B**: Two configurations of maps used in the task. **C**: Population z-maps of \(\), \(\), and \(\) in vmPFC. **D**: The relative representation z-value distribution for the three centers marked as \(\), \(\), and \(\) in vmPFC. **E**: Population z-maps of \(\), \(\), and \(\) in bilateral putamen. **F**: Same as **D**, but for bilateral putamen.

model/brain between each pair of conditions via Pearson correlation, thereby forming a symmetric representational dissimilarity matrix (RDM). The entries in the RDM range from 0 to 2 (1 minus the possible correlation ranging from -1 to 1), where a lower value indicates higher similarity. RDMs are calculated for all voxels in human subjects' brains and for different components in the _goal-reducer_ agent, including the input representation \(\), the goal representation \(\), and the _goal-reducer_, \((s,g)\).

The results show that the activity in the ventromedial prefrontal cortex (vmPFC) corresponds to \(g\) in the _goal-reducer_ agent. Next, we evaluated the activity matching the internal neurons of the _goal-reducer_\((s,g)\). As depicted in Fig. 4C, there are several regions whose activities correlate with \(\), including parts of the vmPFC (indicated by the right circle in Fig. 4C) and the left accumbens. This finding is particularly interesting for two reasons: 1) numerous studies have established that the vmPFC is related to value and goal representations [29; 43]; 2) the close spatial relationship between regions matching \(\) and \(\) (Fig. 4C middle and right columns) reflects the _goal-reducer_'s organization (Fig. 4C right), wherein \(\) and \(\) are interconnected through the generation and input of different goals. This recurrent connectivity suggests that the widespread recurrent connections in the vmPFC  may fulfill the role of goal reduction or perform reverse future simulation , thereby simplifying the task. The finding is consistent with a posterior-to-anterior hierarchy of goal planning in the prefrontal cortex , and our results may suggest the function of such a hierarchy - namely that anterior regions provide goal reduction functionality for more posterior regions.

Following this, we next investigated the Z map of state representations and indeed found a region above \(\) and \(\) in the vmPFC that is correlated with \(\) (as denoted by the top circle in Fig. 4C left). The average z-values in these regions show differential representational loading on the _goal-reducer_ agent components (Fig. 4D), although the ROI z-value comparison is a descriptive statistic only given that the regions were selected for strong loading on the various model layers .

Aside from the goal reduction, we also compared \(\) in the _goal-reducer_ agent with brain data. The RSA results indicate significantly elevated representational loading for these two layers in the bilateral putamen (Fig. 4E), while there is a lower level of loading for the goal representation layer and an even lesser extent for the goal reduction layer. The putamen, a component of the larger basal ganglia system, is recognized for its involvement in habitual behaviors  and goal-directed actions [4; 27]. This suggests that such an area should engage in both local policy enactment (the habitual aspect) and goal reduction (goal-directed component). The congruence between the state representation and its intermediate layer with this role may illuminate why activation related to the goal and its reduction is lower in this region (Fig. 4F), since they are not as essential for the habitual component of the local policy.

## 5 Discussion

Limitations and future workIn this work, the trained goal reduction mechanism has shown its capability in terms of its computational advantage and as a biological model of human goal-directed behaviors. This suggests a possible bridge between efficient human problem-solving in multi-goal settings and machine learning. However, a key part of the training of the _goal-reducer_, the loop removal sampling process, does not have a clear mechanistic biological process correspondence. Two ways exist to address this issue in the future. Computationally, it may be possible to derive a purely neural model to perform the loop removal process, making the sampling process also biologically plausible. A potential solution will be to leverage a lateral-inhibition-like  mechanism to inhibit all associative synaptic connections between neurons when the same group of neurons are activated more than once in a time range, simulating canceling the "loop" in a memory trajectory. Empirically, it may be possible to collect brain activity data during training of the same or similar tasks, or during offline replay, which may incorporate the loop removal as part of memory consolidation.

ConclusionWe developed a novel general goal reduction mechanism using the loop removal trick and trained a network _goal-reducer_ that can learn to predict nearly optimal subgoals from distant ultimate goals. We show that this approach does not rely on prior knowledge about the global structure/distance of the environment and uses just random explorations. We further demonstrate that it can be integrated into various existing RL frameworks and outperforms them. Besides, after removing the Bellman equation part, when applied recursively, this framework can perform goal-directed learning and even outperform _goal-reducer_ augmented RL methods. This _goal-reducer_ agent was next applied to a cognitive task and compared with human fMRI data. Our analyses show that various regions in the brain, including the vmPFC and basal ganglia, can be mapped to the goal reduction representation and state representations in the _goal-reducer_ agent, implying that the brain may instantiate a similar computational circuit to perform goal-directed learning.