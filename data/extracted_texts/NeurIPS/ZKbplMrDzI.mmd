# SDformer: Similarity-driven Discrete Transformer For Time Series Generation

Zhicheng Chen\({}^{1,2,}\), Shibo Feng\({}^{3}\), Zhong Zhang\({}^{2}\), Xi Xiao\({}^{1,4,}\), Xingyu Gao\({}^{5}\), Pelilin Zhao\({}^{2,}\)

\({}^{1}\)Shenzhen International Graduate School, Tsinghua University

\({}^{2}\)Tencent AI Lab

\({}^{3}\)School of Computer Science and Engineering, Nanyang Technological University

\({}^{4}\)Key Laboratory of Data Protection and Intelligent Management (Sichuan University), Ministry of Education

\({}^{5}\)Institute of Microelectronics, Chinese Academy of Sciences

{czc22@mails,xiaox@sz}.tsinghua.edu.cn, shibo001@ntu.edu.sg, gaoxingyu@ime.ac.cn, {todzhang, masonzhao}@tencent.com

###### Abstract

The superior generation capabilities of Denoised Diffusion Probabilistic Models (DDPMs) have been effectively showcased across a multitude of domains. Recently, the application of DDPMs has extended to time series generation tasks, where they have significantly outperformed other deep generative models, often by a substantial margin. However, we have discovered two main challenges with these methods: 1) the inference time is excessively long; 2) there is potential for improvement in the quality of the generated time series. In this paper, we propose a method based on discrete token modeling technique called Similarity-driven Discrete Transformer (SDformer). Specifically, SDformer utilizes a similarity-driven vector quantization method for learning high-quality discrete token representations of time series, followed by a discrete Transformer for data distribution modeling at the token level. Comprehensive experiments show that our method significantly outperforms competing approaches in terms of the generated time series quality while also ensuring a short inference time. Furthermore, without requiring retaining, SDformer can be directly applied to predictive tasks and still achieve commendable results.

## 1 Introduction

Time series data is prevalent across a wide array of real-world applications, spanning fields such as finance , healthcare , energy , retail , and climate science . Despite its significance, the limited availability of dynamic data can pose a significant barrier to the development of machine learning solutions, particularly in scenarios where data sharing could lead to privacy violations . The generation of synthetic yet realistic time series data has emerged as a promising alternative, garnering increased interest due to recent advancements in deep learning techniques.

Existing works on time series generation (TSG) is mainly based on common deep generative models, such as methods based on generative adversarial networks (GAN)  and methods based on Variational Autoencoders (VAE) . Currently holding state-of-the-art results are DDPMs-based methods , which have the capability to generate high-quality, realistic time series. However, they are not without their challenges. Firstly, these methods often require lengthy inference times due to the substantial number of denoising steps involved. Secondly, despitethese methods achieving significant advancements in generation quality compared to other deep generative models, we observe that the quality of the time series they generate still has potential for further enhancement.

Currently, large transformer-based language models, often known as LMs or LLMs, have become the standard choice for natural language generation tasks[1; 3]. As time has progressed, these LMs have evolved to produce content across a wide range of modalities, such as images [29; 40; 5; 4] and videos [41; 33], using what is referred to as Discrete Token Modeling (DTM) technique. In general, these approaches function by learning a discrete representations of images (or videos, etc.), treating them as if they were natural language, and harnessing the power of existing language models for the generation process. Inspired by their success, we aim to explore the application of these techniques in the domain of multivariate time series generation, potentially unlocking new possibilities and advancements in this area.

To address the aforementioned challenges, we propose a novel two-stage method for time series generation, called Similarity-driven Discrete Transformer (SDformer). The primary objective of the first stage is to employ Vector Quantized Variational Autoencoders (VQ-VAE)  for learning high-quality discrete representations of time series. To enhance this process, we introduce a similarity-driven vector quantization approach, which identifies the most suitable code from the codebook by maximizing similarity. The experiments in 5.4 further substantiate the superiority of our method over distance-driven vector quantization. Moreover, to prevent code collapse--a phenomenon where only a small portion of the codes are updated during training, thereby hindering the performance of the VQ-VAE--we incorporate two standard recipes  during training: Exponential Moving Average (EMA) for codebook updates and Resetting inactivated codes during the training process (Code Reset). In the second stage, we implement two Discrete Token Modeling (DTM) techniques: Masked Token Modeling (MTM) and Autoregressive Token Modeling (ARTM), underpinning the SDformer-ar and SDformer-m variants, respectively. SDformer-ar adopts an autoregressive approach for both training and inference, mitigating the inconsistency between these two phases through random replacement . SDformer-m utilizes random masking for training and iterative decoding for inference [8; 5]. Our findings reveal that SDformer, particularly SDformer-ar, surpasses existing models in time series generation. Moreover, SDformer demonstrates robust predictive performance without requiring retraining.

In summary, our contributions include:

* We propose an efficient time series generation model **SDformer**, which successfully introduces DTM technique into time series generation and demonstrates its feasibility and efficiency.
* We introduce a novel similarity-driven vector quantization approach that outperforms the traditional distance-driven method in learning discrete representations of time series. This innovative approach offers a straightforward yet powerful technique for applying discrete token modeling in various fields.
* Our experimental results confirm that the SDformer's performance in time series generation notably surpasses that of the current state-of-the-art models, exemplified by an average enhancement of 60.8% in Discriminative Score and 86.5% in Context-FID Score across multiple datasets.

## 2 Related work

### Discrete token modeling

Discrete token modeling, a staple in natural language processing (NLP), has recently been adapted for non-language modalities through vector quantization models like VQ-VAE  and VQGAN . These models enable the encoding of diverse data types into discrete tokens, allowing the application of advanced language modeling techniques to generate content across various domains. This expansion significantly broadens the utility of NLP methodologies, extending their impact beyond traditional language tasks. Among these techniques, Autoregressive Token Modeling (ARTM) is a common approach that predicts the next token in a sequence, given the previous tokens, using a categorical distribution. Models such as DALL-E  and Parti  employ ARTM to accomplish text-to-image generation tasks. Similarly, T2M-GPT  and MotionGPT  utilize ARTM for text-to-motion generation. Another widely used technique is Masked Token Modeling (MTM), which is trained using a masked token objective . In this approach, some tokens in the sequence are randomly masked and need to be predicted based on the observed tokens. Models such as MaskGIT  and MUSE  leverage MTM for image generation tasks. Furthermore, MAGVIT  and Phenaki  employ MTM for video generation, showcasing the versatility of this technique.

### Time series generation

Deep generative models demonstrate high-quality sample generation across various domains, as does time series generation. At first, people mostly relied on GAN to complete time series generation [24; 11; 39; 37; 26; 16]. For example, TimeGAN  improves temporal dynamics capture by adding an embedding function and supervised loss. COT-GAN  combines GAN and Causal Optimal Transfer (COT) principles to efficiently and stably generate low- and high-dimensional time series data. Due to the challenges of training instability and mode collapse in GAN, researchers have started exploring alternative deep generative models for TSG. TimeVAE  employs an interpretable temporal structure and achieves promising results in time series synthesis using VAE. Moreover, several studies focus on addressing the generation of irregular time series, such as GT-GAN  and KoVAE .

With the emergence of Denoising Diffusion Probabilistic Models (DDPMs) , a new class of generative models, impressive generative capabilities have been demonstrated across various domains. Recently, diffusion models have also been adapted for TSG. For instance, DiffWave  directly applies DDPMs to waveform generation, while DiffTime  harnesses the latest advancements in score-based diffusion models for time series generation. Furthermore, Diffusion-TS  generates time series samples by utilizing an encoder-decoder transformer with disentangled temporal representations, showcasing the versatility and potential of these alternative generative models.

## 3 Definitions and problem formulation

We define multivariate time series as \(X_{1:}=(x_{1},,x_{})^{ d}\), where \(\) and \(d\) are the number of time steps and variables respectively. Assuming that a dataset containing \(n\) time series can be expressed as \(D=\{X_{1:}^{i}\}_{i=1}^{n}\), the goal of unconditional generation is to use a model \(f_{}\) to generate time series with the same distribution as \(D\), i.e.,

\[_{1:}^{i}=f_{}(Z),\] (1)

where \(Z\) is the input sampled from any known distribution, such as the Gaussian distribution.

Time series forecasting is a common conditional time series generation. We denote historical values as \(X_{1:l}^{l d}\), where \(1<l<\) is the number of historical time steps. Therefore, the goal of conditional generation is to use a model \(f_{}\) to predict future values, i.e.,

\[_{l+1:}=f_{}(X_{1:l}).\] (2)

In this paper, our objective is to develop an effective approach that not only accomplishes unconditional generation tasks efficiently but also adapts to conditional generation tasks without retraining, while maintaining high accuracy.

## 4 Methods

In this section, we illustrate proposed innovative model SDformer for time series generation. Specifically, SDformer is a two-stage method, the framework of which is illustrated in Figure 1. In the first stage, a pre-trained time series tokenizer utilizes similarity-driven vector quantization to obtain high-quality discrete token representations. Following this, a discrete Transformer is employed to learn the distribution of time series data at the discrete token level, with the two generative ways (Masked and Autoregressive strategies).

### Time series tokenizer

To represent time series in discrete tokens, we pre-train a multivariate time series tokenizer based on the VQ-VAE architecture . Our time series tokenizer consists of an encoder \(\) and a decoder\(\). The encoder is responsible for the generation of discrete time series tokens, while the decoder is capable of reconstructing these tokens back into their original time series form. This methodology allows us to represent time series akin to a language, thereby enabling the application of a multitude of efficient language models to address various time series-related tasks.

Specifically, the encoder \(\) initially applies 1D convolutions to time series features \(X_{1:}\) along the temporal dimension, resulting in latent vectors \(H_{1:L}=(h_{1},,h_{L})^{L d_{c}}\), where \(L=/r\), \(r\) signifies the temporal downsampling rate and \(d_{c}\) is hidden dimension. Subsequently, we employ the codebook to discretely quantize \(h_{i}\) to obtain discrete token. The learnable codebook \(C=\{c_{k}\}_{k=0}^{K-1}^{d_{c}}\) comprises \(K\) latent embedding vectors, each with a dimension \(d_{c}\). The process of similarity-driven vector quantization \(Q()\) involves identifying the index of the vector in the codebook that exhibits the highest similarity to \(h_{i}\), which can be expressed as:

\[y_{i}=Q(h_{i}):=}}{||h_{i}||}}{||c_{k}||},\] (3)

where \(y_{i}=0,,K-1,\) denotes the inner product, and \(||||\) represents the modulo operation. For simplicity, we introduce a normalization step in the final output layer of the encoder \(\), resulting in a unit modulus length for \(h_{i}\). Furthermore, we ensure that the code in the codebook always has a unit modulus length for \(c_{k}\). The similarity-driven quantization process can be re-simplified as:

\[y_{i}=}h_{i} c_{k}.\] (4)

Following quantization, the dequantization process \(Q^{-1}()\) reverts \(y_{i}\) back to the latent embedding vector, denoted as:

\[_{i}=Q^{-1}(y_{i}):=c_{y_{i}}.\] (5)

Ultimately, the decoder \(\) restores it to the raw time series space, i.e., \(_{1:}=(_{1:L})\). To train this time series tokenizer, we utilize two distinct loss functions for training and optimizing the parameters of \(\) and \(\):

\[=||X_{1:}-_{1:}||_{2}^{2}+_{ i=1}^{L}(1-h_{i} sg(_{i})),\] (6)

where the first loss is the reconstruction loss, the second loss is embedding loss, \(sg()\) represents the stop gradient, and \(\) is hyperparameter used to adjust the weights of different parts. For the codebook, we use Exponential Moving Average and Codebook Reset techniques  to update.

When the time series tokenizer training is completed, the codebook and all parameters will be frozen. By employing this time series tokenizer, a multivariate time series \(X_{1:}^{ d}\) can be mapped to a

Figure 1: The workflow of SDformer. In stage 1, we pre-train a time series tokenizer which uses similarity-driven vector quantization to obtain high-quality discrete token representations. In stage2, two optional techniques are introduced for time series modeling at the discrete token level: MTM and ARTM. For MTM, the input tokens are randomly masked and fed into the Masked Transformer, an encoder-only model, to predict the masked tokens. Conversely, for ARTM, the input tokens are shifted back by one step with the [BOS] token added at the starting position, and then processed by the Autoregressive Transformer, a decoder-only model, to predict subsequent tokens for all input tokens.

sequence of time series tokens \(Y_{1:L}\{0,,K-1\}^{L}\). Therefore, we can use DTM technique to learn the distribution of time series data at the discrete token level. For the choice of DTM, we can opt for methods such as ARTM or MTM. We will introduce these two methods in Sections 4.2 and 4.3, respectively.

### Autoregressive token modeling on time series generation

In this part, we utilize ARTM technique to learn the distribution of time series data at the discrete token level, based on the time series tokenizer. We refer to this approach as SDformer-ar. During training, we take shifted tokens \(Y_{1:L}^{in}=([],y_{1},,y_{L-1})\) as input and real tokens \(Y_{1:L}\) as target for training, where [BOS] represents Beginning of Sentence token. In particular, we use index \(K\) as the [BOS] token, which is distinct from the codebook's index range \(\{0,,K-1\}\). The training objective is to minimize the negative log-likelihood of all tokens:

\[_{ar}=-[_{i} P(y_{i}|Y_{1:i}^{in})].\] (7)

Concretely, we input \(Y_{1:L}^{in}\) into a Decoder-only Transformer to predict the probabilities \(P(y_{i}|Y_{1:i}^{in})\) for each token, where the negative log-likelihood is computed as the cross-entropy between the ground-truth one-hot token and predicted token. For inference, we start from the [BOS] token and generate next token in an autoregressive fashion. The detailed training and inference algorithm of SDformer-ar are respectively shown in Algorithm 2 and 4 in Appendix E. Note that we are able to generate diverse time series by sampling from the predicted distributions given by the transformer.

Random replacement.Autoregression is known to exhibit inconsistency between the training and inference phases. Specifically, during training, the first \(i-1\) ground-truth tokens are used to predict the \(i\)-th token. However, during inference, there's no guarantee that all the preceding tokens used as conditions are correct. To alleviate this issue, we implement a random replacement strategy as a form of data augmentation during training. In this approach, each token is processed individually. A random number is compared to a probability threshold \(\). If it meets the threshold, the token is replaced randomly; otherwise, it remains unchanged. The random replacement can be expressed as:

\[_{i}=(0,K),&(0,I) \\ y_{i},&,\] (8)

where \((0,K)\) is a random integer sampled uniformly from the range \(0\) to \(K-1\), and \((0,I)\) is a random number sampled from a uniform distribution in the range \(0\) to \(1\). Therefore, during training, we use \(_{i}\) instead of \(y_{i}\) in \(Y_{1:L}^{in}\) to achieve data augmentation.

### Masked token modeling on time series generation

In this part, we utilize MTM technique to learn the distribution of time series data at the discrete token level, based on the time series tokenizer. We refer to this approach as SDformer-m. During training, we sample a probability \(p\) from the uniform distribution \(U(0,1)\) as the mask probability. We then replace tokens in the original token sequence with the [MASK] token according to the mask probability \(p\). In particular, we use index \(K\) as the [MASK] token, which is distinct from the codebook's index range \(\{0,,K-1\}\). In other words, when the token \(y_{i}=K\) at a certain position, it indicates that the position has been masked. Denote \(_{1:L}\) as the result after applying random mask to \(Y_{1:L}\). The training objective is to minimize the negative log-likelihood of the masked tokens:

\[_{mask}=-[_{_{i}=K} P(y_{i}| _{1:L})].\] (9)

Concretely, we feed the masked \(_{1:L}\) into a multi-layer bi-directional transformer to predict the probabilities \(P(y_{i}|_{1:L})\) for each masked token, where the negative log-likelihood is computed as the cross-entropy between the ground-truth one-hot token and predicted token.

During inference, we generate a new token sequence using iterative decoding, as proposed in . Initially, we set an iteration number \(N\) and a mask schedule \(S\) of length \(N\). Here, \(S[t]\) represents the number of masks needed after the completion of step \(t\). It is required that \(S<N\), \(S[T-1]=0\), and \(S[t]\) strictly decreases with an increase in \(t\). The detailed training and inference algorithm of SDformer-m are respectively shown in Algorithm 3 and 5 in Appendix E.

## 5 Experiments

In this section, we commence by assessing our proposed methods through a comparative analysis with several state-of-the-art baseline methods on unconditional time series generation tasks. Subsequently, we delve deeper into the analysis of our methods' versatility and high performance in conditional generation tasks. Lastly, through ablation experiments, we confirm the superior effectiveness of similarity-driven vector quantization and discrete token modeling.

### Experimental setups

**Datasets** To evaluate the performance of SDformer, we conduct experiments on 4 real-world datasets (Stocks, ETTh, Energy and fMRI) and 2 simulated datasets (Sines and MuJoCo). Table 1 provides a partial description of each dataset. For more detailed information, please refer to Appendix A.

**Metrics** For quantitative evaluation of synthesized data, we employ the discriminative score and predictive score as described in , along with the Context-FID score proposed by . For detailed descriptions, please refer to Appendix A.

### Unconditional time series generation

Table 2 provides a summary of the performance for each of the compared algorithms on all the datasets. From these results, we can make several observations. Firstly, our proposed methods based on DTM outperform other methods in most cases, demonstrating the feasibility and effectiveness of DTM for time series generation tasks. Secondly, SDformer-ar exhibits a significantly better performance than SDformer-m, which contrasts with the findings in the visual domain. This can be attributed to the fact that autoregressive token modeling is better suited to capture temporal correlations compared to masked token modeling.

To further investigate the capability of our proposed methods in handling longer sequences, we compare the generative abilities of different methods on longer time series, as shown in Table 3. Based on the results, it is evident that many methods exhibit significant distortions when dealing with longer time series, particularly when the discriminative score approaches 0.5, as seen prominently

  Dataset & Sines & Stocks & ETTh & MuJoCo & Energy & fMRI \\  \# of Samples & 10000 & 3773 & 17420 & 10000 & 19711 & 10000 \\ dim & 5 & 6 & 7 & 14 & 28 & 50 \\  

Table 1: Descriptions of all datasets.

Figure 2: Visualizations of the time series synthesized by SDformer and Diffusion-TS.

in the Energy dataset. Despite these challenges, both SDformer-ar and SDformer-m continue to demonstrate exceptional performance.

To visualize the performance of time series generation, we adopt two visualization methods: projecting original and synthetic data in a 2-dimensional space using t-SNE , and drawing data distributions using Kernel Density Estimation (KDE). Figure 2 illustrates the visualization of our methods in comparison with Diffusion-TS on the Energy and ETH datasets, revealing that the data generated by SDformer-ar more closely resembles the real data, followed by SDformer-m.

### Conditional time series generation

Figure 3: Examples of time series forecasting for Energy (\(1^{st}\) row) and fMRI (\(2^{st}\) row) datasets. Green and gray colors correspond to SDformer-ar and Diffusion-TS, respectively.

  Metrics & Methods & Sines & Stocks & ETH & MuJoCo & Energy & fMRI \\   & SDformer-ar & **0.006\(\)0.004** & **0.010\(\)0.006** & **0.003\(\)0.001** & **0.008\(\)0.005** & **0.006\(\)0.004** & **0.017\(\)0.007** \\  & SDformer-m & 0.008\(\)0.004 & 0.020\(\)0.011 & 0.022\(\)0.001 & 0.0250\(\)0.007 & 0.062\(\)0.006 & 0.043\(\)0.006 \\  & Diffusion-TS & **0.006\(\)0.007** & 0.067\(\)0.015 & 0.061\(\)0.009 & **0.008\(\)0.002** & 0.122\(\)0.003 & 0.167\(\)0.023 \\  & TimeGAN & 0.011\(\)0.008 & 0.102\(\)0.021 & 0.114\(\)0.055 & 0.238\(\)0.068 & 0.236\(\)0.012 & 0.484\(\)0.042 \\  & TimeVAE & 0.041\(\)0.044 & 0.145\(\)1.20 & 0.209\(\)0.058 & 0.230\(\)0.102 & 0.499\(\)0.000 & 0.476\(\)0.044 \\  & Diffwave & 0.017\(\)0.008 & 0.232\(\)0.061 & 0.190\(\)0.008 & 0.203\(\)0.096 & 0.493\(\)0.004 & 0.402\(\)0.029 \\  & DiffTime & 0.013\(\)0.006 & 0.097\(\)0.016 & 0.100\(\)0.007 & 0.154\(\)0.045 & 0.445\(\)0.004 & 0.245\(\)0.051 \\  & Cot-GAN & 0.254\(\)1.37 & 0.230\(\)0.016 & 0.325\(\)0.099 & 0.426\(\)0.022 & 0.498\(\)0.002 & 0.492\(\)0.018 \\   & SDformer-ar & **0.093\(\)0.000** & 0.037\(\)0.000 & **0.118\(\)0.002** & **0.007\(\)0.001** & **0.249\(\)0.000** & **0.091\(\)0.002** \\  & SDformer-m & **0.093\(\)0.000** & 0.037\(\)0.000 & 0.119\(\)0.002 & **0.007\(\)0.001** & 0.250\(\)0.000 & **0.091\(\)0.001** \\  & Diffusion-TS & **0.093\(\)0.000** & **0.036\(\)0.000** & 0.119\(\)0.002 & **0.007\(\)0.000** & 0.250\(\)0.000 & 0.099\(\)0.000 \\  & TimeGAN & **0.093\(\)0.019** & 0.038\(\)0.001 & 0.124\(\)0.001 & 0.025\(\)0.003 & 0.273\(\)0.004 & 0.126\(\)0.002 \\  & TimeVAE & **0.093\(\)0.000** & 0.039\(\)0.000 & 0.126\(\)0.004 & 0.012\(\)0.002 & 0.292\(\)0.000 & 0.113\(\)0.003 \\  & Diffwave & **0.093\(\)0.000** & 0.047\(\)0.000 & 0.130\(\)0.001 & 0.013\(\)0.000 & 0.251\(\)0.000 & 0.101\(\)0.000 \\  & DiffTime & **0.093\(\)0.000** & 0.038\(\)0.001 & 0.121\(\)0.004 & 0.010\(\)0.01 & 0.252\(\)0.000 & 0.100\(\)0.000 \\  & Cot-GAN & 0.100\(\)0.000 & 0.047\(\)0.001 & 0.129\(\)0.000 & 0.068\(\)0.009 & 0.259\(\)0.000 & 0.183\(\)0.003 \\   & Original & 0.094\(\)0.001 & 0.036\(\)0.001 & 0.121\(\)0.005 & 0.007\(\)0.001 & 0.250\(\)0.003 & 0.090\(\)0.001 \\   & SDformer-ar & **0.001\(\)0.000** & **0.002\(\)0.000** & **0.008\(\)0.001** & **0.005\(\)0.001** & **0.003\(\)0.000** & **0.015\(\)0.001** \\  & SDformer-m & 0.010\(\)0.002 & 0.034\(\)0.008 & 0.019\(\)0.003 & 0.030\(\)0.003 & 0.041\(\)0.005 & 0.035\(\)0.003 \\  & Diffusion-TS & 0.006\(\)0.000 & 0.147\(\)0.025 & 0.116\(\)0.010 & 0.013\(\)0.001 & 0.089\(\)0.024 & 0.105\(\)0.006 \\  & TimeGAN & 0.101\(\)0.014 & 0.103\(\)0.013 & 0.300\(\)0.013 & 0.563\(\)0.052 & 0.767\(\)1.03 & 1.292\(\)2.18 \\  & TimeVAE & 0.307\(\)0.060 & 0.215\(\)0.035 & 0.805\(\)1.86 & 0.251\(\)0.15 & 1.631\(\)1.42 & 14.44\(\)9.969 \\  & Diffwave & 0.014\(\)0.002 & 0.232\(\)0.032 & 0.873\(\)0.061 & 0.393\(\)0.041 & 1.031\(\)1.31 & 0.244\(\)0.018 \\  & DiffTime & 0.006\(\)0.001 & 0.236\(\)0.074 & 0.299\(\)0.044 & 0.188\(\)0.028 & 0.279\(\)0.045 & 0.340\(\)0.015 \\  & Cot-GAN & 1.337\(\)0.068 & 0.408\(\)0.086 & 0.980\(\)0.071 & 1.094\(\)0.079 & 1.039\(\)0.028 & 7.813\(\)5.50 \\  

Table 2: Results of all methods on all datasetsApart from unconditional generation, we also explore the performance of our proposed methods in conditional generation tasks. Our objective is to evaluate the model's versatility in handling both conditional and unconditional tasks. More specifically, we aim to train a single model that can effectively manage both unconditional and conditional tasks under different settings. Referring to , we set \(=48\), \(l=8,16,24,32\) in Equation (2), and then directly use the model trained under the unconditional generation task to complete the forecasting tasks under these different settings. Figure 3 displays several examples of forecasting tasks. The median values of forecasting are represented as the dotted line, and 5% and 95% quantiles are depicted as the shade areas (Green: SDformer-ar, Gray: Diffusion-TS). This demonstrates that SDformer-ar provides more reasonable forecasts with higher confidence compared to Diffusion-TS. Furthermore, more detailed results are illustrated in Figure 4. Based on these findings, the methods employing discrete token modeling demonstrates adaptability to both unconditional and conditional generation tasks of varying lengths without the need for retraining, while maintaining good performance.

### Ablation study

To understand the contribution of each component to proposed methods, we conduct ablation experiments for two aspects, 1) The impact of vector quantization methods based on different measurements 2) The advantages of discrete representations in time series generation. More experimental results refer to Appendix C, due to limited space.

**Effect of similarity-driven vector quantization in Equation (4).** For discrete token modeling method, the quality of discrete representations learning from continuous data determines the performance potential of the entire method, with vector quantization playing a crucial role. Therefore, we compare the impact of our proposed similarity-driven vector quantization with the commonly used distance-driven vector quantization on the overall method performance.

   & &  &  \\  Metrics & Methods & 64 & 128 & 256 & 64 & 128 & 256 \\   & SDformer-ar & **0.018\(\)0.007** & **0.013\(\)0.005** & **0.008\(\)0.006** & **0.010\(\)0.007** & **0.013\(\)0.007** & **0.017\(\)0.003** \\  & SDformer-m & 0.034\(\)0.017 & 0.038\(\)0.008 & 0.041\(\)0.024 & 0.053\(\)0.018 & 0.069\(\)0.014 & 0.035\(\)0.007 \\  & Diffusion-TS & 0.106\(\)0.048 & 0.144\(\)0.060 & 0.060\(\)0.030 & 0.078\(\)0.021 & 0.143\(\)0.075 & 0.290\(\)1.23 \\  & TimeGAN & 0.227\(\)0.078 & 0.188\(\)0.074 & 0.442\(\)0.056 & 0.498\(\)0.001 & 0.499\(\)0.001 & 0.499\(\)0.000 \\  & TimeVAE & 0.171\(\)1.42 & 0.154\(\)0.087 & 0.178\(\)0.076 & 0.499\(\)0.000 & 0.499\(\)0.000 & 0.499\(\)0.000 \\  & Diffwave & 0.254\(\)0.074 & 0.274\(\)0.047 & 0.304\(\)0.068 & 0.497\(\)0.004 & 0.499\(\)0.001 & 0.499\(\)0.000 \\  & DiffTime & 0.150\(\)0.003 & 0.176\(\)0.015 & 0.243\(\)0.005 & 0.328\(\)0.031 & 0.396\(\)0.024 & 0.437\(\)0.095 \\  & Cot-GAN & 0.296\(\)3.48 & 0.451\(\)0.080 & 0.461\(\)0.010 & 0.499\(\)0.001 & 0.499\(\)0.001 & 0.498\(\)0.004 \\   & SDformer-ar & **0.116\(\)0.006** & 0.110\(\)0.007 & **0.095\(\)0.003** & **0.247\(\)0.001** & **0.244\(\)0.000** & **0.243\(\)0.002** \\  & SDformer-m & 0.120\(\)0.004 & **0.107\(\)0.004** & 0.110\(\)0.007 & 0.248\(\)0.001 & 0.245\(\)0.000 & 0.244\(\)0.003 \\  & Diffusion-TS & **0.116\(\)0.000** & 0.110\(\)0.003 & 0.109\(\)0.013 & 0.249\(\)0.000 & 0.247\(\)0.001 & 0.245\(\)0.001 \\  & TimeGAN & 0.132\(\)0.008 & 0.153\(\)0.014 & 0.220\(\)0.008 & 0.291\(\)0.003 & 0.303\(\)0.002 & 0.351\(\)0.004 \\  & TimeVAE & 0.118\(\)0.004 & 0.113\(\)0.005 & 0.110\(\)0.027 & 0.302\(\)0.001 & 0.318\(\)0.000 & 0.353\(\)0.003 \\  & Diffwave & 0.133\(\)0.008 & 0.129\(\)0.033 & 0.132\(\)0.001 & 0.252\(\)0.001 & 0.252\(\)0.000 & 0.251\(\)0.000 \\  & DiffTime & 0.118\(\)0.004 & 0.120\(\)0.008 & 0.118\(\)0.003 & 0.252\(\)0.000 & 0.251\(\)0.000 & 0.251\(\)0.000 \\  & Cot-GAN & 0.135\(\)0.003 & 0.126\(\)0.001 & 0.129\(\)0.000 & 0.262\(\)0.002 & 0.269\(\)0.002 & 0.275\(\)0.004 \\   & Original & 0.114\(\)0.006 & 0.108\(\)0.005 & 0.106\(\)0.010 & 0.245\(\)0.002 & 0.243\(\)0.000 & 0.243\(\)0.000 \\   & SDformer-ar & **0.018\(\)0.003** & **0.024\(\)0.001** & **0.021\(\)0.001** & **0.031\(\)0.002** & **0.036\(\)0.002** & **0.041\(\)0.003** \\  & SDformer-m & 0.086\(\)0.008 & 0.094\(\)0.007 & 0.078\(\)0.006 & 0.160\(\)0.025 & 0.151\(\)0.011 & 0.136\(\)0.014 \\  & Diffusion-TS & 0.631\(\)0.058 & 0.787\(\)0.062 & 0.423\(\)0.038 & 0.135\(\)0.017 & 0.087\(\)0.019 & 0.126\(\)0.024 \\  & TimeGAN & 1.130\(\)1.02 & 1.553\(\)1.169 & 5.872\(\)2.008 & 1.230\(\)0.070 & 2.535\(\)3.372 & 5.032\(\)8.31 \\  & TimeVAE & 0.827\(\)1.46 & 1.062\(\)1.34 & 0.826\(\)0.093 & 2.662\(\)0.087 & 3.125\(\)1.066 & 3.768\(\)9.998 \\  & Diffwave & 1.543\(\)1.53 & 2.354\(\)1.170 & 2.899\(\)2.89 & 2.697\(\)4.18 & 5.552\(\)5.28 & 5.572\(\)5.584 \\  & DiffTime & 1.279\(\)0.083 & 2.554\(\)3.18 & 3.524\(\)8.30 & 0.762\(\)1.57 & 1.344\(\)1.31 & 4.735\(\)7.729 \\  & Cot-GAN & 3.008\(\)2.77 & 2.639\(\)4.427 & 4.075\(\)8.94 & 1.824\( term "w/o similarity" in Table 4 denotes the variant of the corresponding method that employs distance-driven vector quantization instead of similarity-driven vector quantization. As per the comprehensive results, similarity-driven vector quantization significantly outperforms distance-driven vector quantization. For instance, the discriminative score witnessed an increase of 35.6% for SDformer-ar and 46.2% for SDformer-m.

**Effect of discrete token in Equation (4) and (5).** To explore the impact of discrete versus continuous tokens, we introduce a variant replacing the original discrete token with a continuous one within a similar framework. Initially, we substitute VQ-VAE with VAE as per  to encode a continuous latent space. In the second stage, we reconfigure the Transformer to accommodate continuous inputs, altering its initial input strategy due to the inapplicability of a fixed token like [BOS]. Thus, the first token of each time series is not used as the prediction target during training. For inference, we devised two methods: one involves sampling from a multivariate Gaussian, calculated from all initial training tokens, for generating the first token; the second uses the actual initial token directly. Although the latter does not lend itself to a fair comparison with the original model, it is included for a more comprehensive evaluation as a reference.

In Table 4, the term "continuous" represents the first variant, which involves using continuous tokens in place of discrete ones. Meanwhile, the term "continuous, w/ first" denotes the second variant, which builds upon the first by providing the actual first token during inference. It is worth noting that, as SDformer-m necessitates the utilization of category sampling during inference for achieving

  Metrics & Methods & Sines & Stocks & ETHh & MulJoCo & Energy & fMRI \\   & SDformer-ar & **0.006\(\).004** & **0.010\(\).006** & **0.003\(\).001** & **0.008\(\).005** & **0.006\(\).004** & **0.017\(\).007** \\  & w/o similarity & 0.006\(\).004 & 0.011\(\).007 & 0.010\(\).005 & 0.013\(\).003 & 0.018\(\).005 & 0.024\(\).003 \\  & continuous & 0.047\(\).012 & 0.065\(\).012 & 0.145\(\).020 & 0.055\(\).013 & 0.322\(\).012 & 0.243\(\).214 \\  & continuous, w/ first & 0.012\(\).004 & 0.021\(\).015 & 0.006\(\).004 & 0.020\(\).006 & 0.277\(\).007 & 0.074\(\).006 \\   & SDformer-m & **0.008\(\).004** & **0.020\(\).011** & **0.022\(\).001** & **0.025\(\).007** & **0.062\(\).006** & **0.043\(\).006** \\  & w/o similarity & 0.015\(\).007 & 0.081\(\).010 & 0.055\(\).004 & 0.070\(\).005 & 0.068\(\).005 & 0.055\(\).009 \\   & SDformer-ar & **0.093\(\).000** & **0.037\(\).000** & **0.118\(\).002** & **0.007\(\).001** & **0.249\(\).00** & 0.091\(\).002 \\  & w/o similarity & **0.093\(\).000** & **0.037\(\).000** & 0.122\(\).002 & 0.008\(\).001 & **0.249\(\).00** & 0.091\(\).002 \\  & continuous & **0.093\(\).000** & 0.038\(\).000 & 0.124\(\).003 & 0.009\(\).001 & 0.255\(\).000 & 0.105\(\).000 \\  & continuous, w/ first & **0.093\(\).000** & **0.037\(\).000** & 0.122\(\).003 & **0.007\(\).001** & 0.251\(\).000 & **0.087\(\).003** \\   & SDformer-m & **0.093\(\).000** & **0.037\(\).000** & **0.119\(\).002** & **0.007\(\).001** & **0.250\(\).000** & **0.091\(\).001** \\  & w/o similarity & **0.093\(\).000** & **0.037\(\).000** & 0.123\(\).001 & 0.008\(\).001 & **0.250\(\).000** & 0.093\(\).000 \\   & SDformer-ar & **0.001\(\).00** & **0.002\(\).00** & 0.008\(\).001 & **0.005\(\).001** & **0.003\(\).00** & 0.015\(\).001 \\  & w/o similarity & 0.002\(\).000 & 0.012\(\).001 & 0.013\(\).001 & **0.005\(\).001** & 0.004\(\).000 & 0.011\(\).000 \\   & continuous & 0.056\(\).004 & 0.101\(\).02 & 0.433\(\).049 & 0.065\(\).008 & 0.213\(\).022 & 5.512\(\).390 \\   & continuous, w/ first & 0.004\(\).000 & 0.015\(\).003 & **0.002\(\).000** & 0.006\(\).001 & 0.021\(\).003 & **0.003\(\).000** \\    & SDformer-m & **0.010\(\).002** & **0.034\(\).008** & **0.019\(\).003** & **0.030\(\).003** & **0.041\(\).005** & **0.035\(\).003** \\   & w/o similarity & 0.044\(\).006 & 0.123\(\).009 & 0.106\(\).012 & 0.098\(\).009 & 0.062\(\).014 & 0.038\(\).002 \\  

Table 4: Results of ablation study.

Figure 4: Performance for time series forecasting and generation under different setting. All forecasting tasks utilize Mean Square Error (MSE) as performance metric, while unconditional generation tasks employ Discriminative Scores (DS). Note: The data in this figure has been scaled by a factor of 100 for the forecasting tasks and 10 for the unconditional generation tasks to streamline the presentation.

iterative sampling, we abstain from conducting ablation experiments on discrete tokens specifically for this model. Based on the results, the model's performance experiences a significant decline upon the removal of discrete tokens. Even when incorporating the condition information of the first token, it often fails to surpass the original method in most scenarios.

## 6 Conclusions

In this paper, we present discrete token modeling for the time series generation tasks and propose a innovative two-stage model. Specifically, it is built upon an efficient time series tokenizer, which attains high-quality discrete token representations through similarity-driven vector quantization. Leveraging this foundation, we employ autoregressive token modeling and masked token modeling techniques to learn the distribution of time series data at the discrete token level. Experimental results showcase the efficacy and adaptability of our approach in various time series generation tasks. Owing to the flexibility of our method in the second stage, future work could explore referencing more efficient language models to design increasingly effective time series generation strategies.