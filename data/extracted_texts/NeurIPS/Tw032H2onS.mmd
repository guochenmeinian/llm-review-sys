# Boosted Conformal Prediction Intervals

Ran Xie

Department of Statistics

Stanford University

ranxie@stanford.edu

&Rina Foygel Barber

Department of Statistics

University of Chicago

rina@uchicago.edu

Emmanuel J. Candes

Department of Statistics

Department of Mathematics

Stanford University

candes@stanford.edu

###### Abstract

This paper introduces a _boosted conformal procedure_ designed to tailor conformalized prediction intervals toward specific desired properties, such as enhanced conditional coverage or reduced interval length. We employ machine learning techniques, notably gradient boosting, to systematically improve upon a predefined conformity score function. This process is guided by carefully constructed loss functions that measure the deviation of prediction intervals from the targeted properties. The procedure operates post-training, relying solely on model predictions and without modifying the trained model (e.g., the deep network). Systematic experiments demonstrate that starting from conventional conformal methods, our boosted procedure achieves substantial improvements in reducing interval length and decreasing deviation from target conditional coverage.

## 1 Introduction

Black-box machine learning algorithms have been increasingly employed to inform decision-making in sensitive applications. For instance, deep convolutional neural networks have been applied to diagnose skin cancer , and AlphaFold has been utilized in the development of malaria vaccines [24; 25]; here, scientists have employed AlphaFold to predict the structure of a key protein in the malaria parasite, facilitating the identification of potential binding sites for antibodies that could prevent the transmission of the parasite . These instances highlight the critical need for understanding prediction accuracy, and one popular approach to quantify the uncertainty associated with general predictions relies on the construction of prediction sets guaranteed to contain the target label or response with high probability. Ideally, we would like the coverage to be valid conditional on the values taken by the features of the predictive model (e.g., patient demographics).

Conformal prediction  stands out as a flexible calibration procedure that provides a wrapper around any black-box prediction model to produce valid prediction intervals. Imagine we have a data set \(\{(X_{i},Y_{i})\}_{i=1}^{n}\) and a test point \((X_{n+1},Y_{n+1})\) drawn exchangeably from an unknown, arbitrary distribution \(P\) (e.g. the pairs \((X_{i},Y_{i})\) may be i.i.d.). Taking the data set and the observed features \(X_{n+1}\) as inputs, conformal prediction forms a prediction interval \(C_{n}(X_{n+1})\) for \(Y_{n+1}\) with valid marginal coverage, i.e. such that \((Y_{n+1} C_{n}(X_{n+1}))=0.95\) or any nominal level specified by the user ahead of time. This is achieved by means of a conformity score \(E(x,y;f)\), where \((x,y)\) represents a data point while \(f\) represents any aspects of the distribution that we have estimated. For instance, the score may be given by the magnitude of the prediction error \(|y-(x)|\), where \((x)\) represents the model prediction of the expected outcome, in which case \(f\) is simply \(\). Roughly, wewould include \(y\) in the prediction interval if \(E(X_{n+1},y;f)\) does not take on an atypical value when compared with \(\{E(X_{i},Y_{i};f)\}\), \(i=1,,n\). Selecting an appropriate conformity score is akin to choosing a test statistic in statistical testing, where two statistics may yield the same Type I error rate yet differ substantially in other aspects of performance.

One central issue is that while the conformal procedure guarantees marginal coverage, it does not extend similar guarantees to other desirable inferential properties without additional assumptions. In response, researchers have introduced a variety of conformity scores, including the locally adaptive (Local) conformity score , the conformalized quantile regression (CQR) conformity score , and its variants, CQR-m  and CQR-r . Among these, CQR has often demonstrated superior empirical performance in terms of both interval length and conditional coverage .

This paper introduces a boosting procedure aimed at enhancing an arbitrary score function.1 By employing machine learning techniques, namely, gradient boosting, our objective is to modify the Local or CQR score functions (or other baselines) to reduce the average length of prediction intervals or improve conditional coverage while maintaining marginal coverage. While this paper focuses primarily on length and conditional coverage, our methods can be tuned to optimize other criteria; we elaborate on this in Section 7.

Our boosted conformal procedure searches within a family of generalized scores for a score achieving a low value of a loss function adapted to the task at hand. Specifically, to evaluate the conditional coverage of prediction intervals, we build a loss function that maximizes deviation from the target coverage rate in the leaves of a shallow contrast tree . Searching within a strategically designed family of score functions, rather than directly retraining or fine-tuning the fitted model under the task-specific loss function, yields greater flexibility and avoids the costs associated with retraining or fine-tuning. Further, this boosting process is executed post-model training, requiring only the model predictions and no direct access to the training algorithm.

Source code for implementing the boosted conformal procedure is available online at https://github.com/ran-xie/boosted-conformal. Details regarding the acquisition and preprocessing of the real datasets are also provided in the GitHub repository.

## 2 The split conformal procedure

We begin by outlining the key steps of the split conformal procedure applied to a family \(\{(X_{i},Y_{i})\}_{i=1}^{n}\) of exchangeable samples (e.g., i.i.d.).

* _Training._ Randomly partition \([n]\) into a training set \(I_{1}\) and a calibration set \(I_{2}\). On the training set, train a model by means of an algorithm \(}\) to produce a conformity score function \(E(,;f)\). The structure of this score function is predetermined, whereas the model \(f\) is learned from \(}\). An example of a conformity score is \(E(x,y;f)=|y-(x)|\), where \((x)\) is a learned regression function so that \(f\) is here simply \(\).
* _Calibration._ Evaluate the function \(E(,;f)\) on each instance in the calibration set and obtain scores \(\{E_{i}\}_{i I_{2}}\),2 with each \(E_{i}=E(X_{i},Y_{i};f)\). The \((1-)\)th empirical quantile

Figure 1: Illustration of the boosted conformal prediction procedure. We introduce a boosting stage between training and calibration, where we boost \(\) rounds on the conformity score function \(E(,)\) and obtain the boosted score \(E^{()}(,)\). The number of boosting rounds \(\) is selected via cross validation. A detailed description of the procedure is presented in Algorithm 1.

of the score, \(Q_{1-}(E,I_{2})\), is calculated as \[Q_{1-}(E,I_{2})=\{z:(Z z) 1-\},\] where \(Z\) follows the distribution \(|+1}(_{}+_{E_{i}})\), and \(_{a}\) is a point mass at \(a\).
* _Testing._ For a new observation \(X_{n+1}\), output the conformalized prediction interval \[C_{n}(X_{n+1})=\{y:E(X_{n+1},y;f) Q_{1-}(E,I_{2})\}.\] (1) If ties between \(\{E_{i}\}_{i I_{2}}\) occur with probability zero, it holds that \[1-(Y_{n+1} C_{n}(X_{n+1})) 1-+|+ 1},\] (2) see . By introducing additional randomization during the calibration step, the prediction interval can be tuned to obey \((Y_{n+1} C_{n}(X_{n+1}))=1-\), see . This adjustment is not critical here and we omit the details.

**Locally adaptive conformal prediction** (Local for short)  introduces a score function that aims to make conformal prediction adapt to situations where the spread of the distribution of \(Y\) varies significantly with the observed features \(X\). On the training set, run an algorithm \(\) to fit two functions \(_{0}()\) and \(_{0}()\), where \(_{0}(X)\) estimates the conditional mean \([Y X]\), and \(_{0}(X)\) the dispersion around the conditional mean, frequently chosen as the conditional mean absolute deviation (MAD), \([|Y-_{0}(X)| X]\). With \(f=(_{0},_{0})\), the locally adaptive (Local) score function is:

\[E(x,y;f)=|y-_{0}(x)|/_{0}(x).\] (3)

For a new observation \(X_{n+1}\), the conformalized prediction interval (1) takes on the simplified expression \([_{0}(X_{n+1})-Q_{1-}(E,I_{2})_{0}(X_{n+1}),_{0}(X_{n+1})+Q_ {1-}(E,I_{2})_{0}(X_{n+1})]\).

**Conformalized quantile regression** (CQR)  also aims to adapt to heteroskedasticity by calibrating conditional quantiles, which often results in shorter prediction intervals. Apply quantile regression to produce a pair of estimated quantiles \((_{/2}(x),_{1-/2}(x))\), where \(_{}(X)\) is the estimated \(\)th quantile of the conditional distribution of \(Y\). The CQR score function is defined as

\[E(x,y;f)=\{_{/2}(x)-y,y-_{1-/2}(x)\},\] (4)

where \(f=(_{/2},_{1-/2})\). For a new observation \(X_{n+1}\), following (1) yields the prediction interval

\[[_{/2}(X_{n+1})-Q_{1-}(E,I_{2}),_{1-/2}(X _{n+1})+Q_{1-}(E,I_{2})].\] (5)

**Generalized conformity score families**. To construct a Local conformity score, we estimate two functions \(_{0}()\) and \(_{0}()\) to plug into (3). Since these components are constructed without looking at performance downstream, it is reasonable to imagine that other choices may enjoy enhanced properties. How then should we systematically select \(()\) and \(()\)? To address this, we define a generalized Local score family \(\) containing all potential score functions of the form

\[:=\{E(,;f):E(x,y;f)=|y-(x)|/(x),()>0\},\] (6)

where \(f=(,)\). For each \(E(,;f)\), the conformalized prediction interval is given by

\[[(X)-Q_{1-}(E,I_{2})(X),(X)+Q_{1-}(E,I_{2}) (X)].\] (7)

Turning to CQR, one notable limitation is the uniform adjustment of prediction intervals by the constant factor \(Q_{1-}(E,I_{2})\), as shown in (5). This approach is suboptimal in the presence of heteroskedasticity, as it applies an identical correction to prediction intervals of varying widths for each \(X=x\). Thus, simply updating the fitted quantiles \((_{},_{1-/2})\) and plugging them into the original score function would be inadequate, as the structure of the original score imposes significant limitations on the effectiveness of conformalized prediction intervals. To address this, several variants including CQR-m  and CQR-r  have been proposed. Focusing on CQR-r, it employs a flexible score function, defined as \(E(x,y;f)=\{_{/2}(x)-y,y-_{1-/2}(x)\}/(_{1 -/2}(x)-_{/2}(x))\), with \(f=(_{/2},_{1-/2},_{1-/2}-_{ /2})\). Following (1), conformalized prediction intervals become

\[[_{/2}(X)-(X)Q_{1-}(E,I_{2}),_{1- /2}(X)+(X)Q_{1-}(E,I_{2})],\] (8)where \(=_{1-/2}-_{/2}\). Intuitively, the adjusted score function allows prediction bands to adjust in proportion to their width, instead of adding a constant shift as in CQR. However, despite the intuitive appeal of adjusted scores as a seemingly more reasonable "allocation" of the conformal correction, empirical studies reveal that they do not result in narrower prediction intervals when compared to CQR . This phenomenon is largely due to the uniform direction of the conformal adjustment, represented by \(Q_{1-}(E,I_{2})\), across all observations. In particular, if \(Q_{1-}(E,I_{2})<0\), indicating that the true target \(y\) predominantly lies within the estimated quantile range \([_{/2},_{1-/2}]\), there is a uniform narrowing of the predicted interval across all samples.

In light of these insights, we propose a novel score family, \(\), designed to augment the flexibility of the conformity score functions:

\[:=E(,;f):E(x,y;f)=\{_{1}(x)-y,y-_{ 2}(x)\}/(x),_{1}()_{2}(),()>0 },\] (9)

where \(f=(_{1},_{2},)\), which leads to conformalized prediction intervals of the form

\[[_{1}(X)-(X)Q_{1-}(E,I_{2}),_{2}(X)+(X)Q_{1-}(E,I_{2})].\] (10)

Notably, \(\) includes the Local, CQR, and CQR-r scores as special cases.

## 3 Boosted conformal procedure

It is clear from above that a model is trained to produce a conformity score \(E(,;f)\); e.g., we may learn a regression function \(()\) to plug it into a score function \(|y-(x)|\). To overcome the limitation of working with an arbitrarily selected score function, we introduce a boosting step before calibration, see Figure 1. In a nutshell, we use gradient boosting to iteratively improve upon a predefined score \(E(,;f)\) now denoted as \(E^{(0)}(,)\), where the superscript indicates the \(0\)th iteration.

To achieve this, we construct a task-specific loss function \(\), which takes a dataset \(\) and a score function \(E(,;f)\) as inputs, and outputs \((E(,;f);)\) measuring how closely the conformalized prediction interval aligns with the analyst's objective. This loss function \(\) is designed to be differentiable with respect to each of the model components produced by the training algorithm. Importantly, it does not require knowledge of the gradient of \(f(x)\) with respect to \(x\). In the example above, taking the labels as fixed, this means that for each feature \(x_{i}\), \(i=1,,n\), if we set \(_{i}=(x_{i})\), then the loss \((E(,);)\) is a function of \(\{_{i}\}_{i=1}^{n}\), and the derivative \((E(,);)/_{i}\) is well defined. In Sections 5.1 and 6.1, we present examples of such derivatives.

Each boosting iteration updates the score function sequentially, employing a gradient boosting algorithm such as XGBoost  or LightGBM . These algorithms accept as input a dataset \(\), a base score function \(E(,;f)\), a custom loss function \(\), gradients of \(\) with respect to \(f\) (denoted \(_{f}\)), and a number of boosting rounds \(\). We may write the boosting procedure as

\[(E^{(0)}(,),,E^{()}(,))=(,E(,;f),,_{f},).\] (11)

This yields a boosted score function \(E^{()}(,)\), which is then used for calibration and for constructing prediction intervals. The number \(\) is calculated using \(k\)-fold cross-validation on the training dataset, selecting \(\) from potential values up to a predefined maximum \(T\) (e.g., 500). We partition the dataset into \(k\) folds and for each \(j=1,,k\), we hold out fold \(j\) for sub-calibration and the remaining \(k-1\) folds for sub-training. We apply \(T\) rounds of gradient boosting (11) on the sub-training data, generating \(T+1\) candidate score functions \(E^{(0)}_{j}(,),,E^{(T)}_{j}(,)\). Each score function is then evaluated on sub-calibration data, using the loss function \(\) to compute losses at all epochs, i.e., for each fold \(j=1, k\),

\[\{L^{(t)}_{j}\}_{t=0}^{T}=\{(E^{(t)}_{j};_{j})\}_{t=0}^{T}.\]

Last, \(\) is selected as the round that minimizes the average loss across all \(k\) folds:

\[=_{0 t T}_{j=1}^{k}L^{(t)}_{j},\] (12)

see Figure 2. This cross-validation step simulates the calibration step in conformal prediction and effectively prevents the overfitting of the score function.

Since boosting is conducted on the training data, the boosted procedure satisfies the same marginal coverage guarantee as the split conformal procedure, as formalized below.

**Proposition 3.1**.: _Let \(\{(X_{i},Y_{i})\}_{i=1}^{m}\) be the held out calibration set, and \((X_{m+1},Y_{m+1})\) be a pair of new observation. If the \(m+1\) samples are exchangeable, and ties between \(\{E^{()}(X_{i},Y_{i})\}_{i=1}^{m}\) occur with probability zero, the conformalized prediction interval (1) computed from score function \(E^{()}(,)\) satisfies the coverage guarantee (2)._

**Searching within generalized conformity score families**. To update the Local score function (3), we search within the generalized score family \(\) (6). First, we initialize \(^{(0)}=_{0}\) and \(^{(0)}=_{0}\). After completing \(\) iterations of boosting on the training set, we obtain the boosted score function \(E^{()}(x,y)=|y-^{()}(x)|/^{()}(x)\). Notably, we can update any score function within \(\). For instance, to update \(E(x,y;f)=|y-(x)|\), we simply initialize \(^{(0)}=\), and take \(^{(0)}\) to be the constant function equal to one. Similarly, to update the CQR score function (4), we search within the score family \(\) (9). First, we initialize a triple \(^{(0)}_{1}=_{/2}\), \(^{(0)}_{2}=_{1-/2}\), \(^{(0)}=_{1-/2}-_{/2}\). After \(\) boosting rounds, we obtain the boosted score function \(E^{()}(x,y)=\{^{()}_{1}(x)-y,y-^{()}_{2}(x)\}/^{( )}(x)\).

```
0: Training data \((X_{i},Y_{i})^{p}\), \(i=1,...,n\); base conformity score function \(E^{(0)}(,)\)  Loss function \(\); target mis-coverage level \((0,1)\)  Number \(k\) of cross-validation folds; maximum boosting rounds \(T\) Procedure:  Randomly divide \(\{1,...,n\}\) into \(k\) folds for\(j 1\) to\(k\)do  Set fold \(j\) as sub-calibration set, and the remaining \(k-1\) folds as sub-training set  On the sub-training set, call GradientBoosting (11) to obtain candidate scores \(\{E^{(t)}_{j}\}_{t=0}^{T}\)  On the sub-calibration set, evaluate \(L^{(t)}_{j}=(E^{(t)}_{j})\), \(t=0,,T\) endfor  Set boosting rounds \(_{t}_{j=1}^{k}L^{(t)}_{j}\) as in (12)  On the training set, call GradientBoosting (11) to obtain boosted functions \(\{E^{(t)}\}_{t=0}^{}\) Output:  Boosted conformity score function \(E^{()}(,)\) ```

**Algorithm 1** Boosting stage

## 4 Related Works

**Adapting the classical conformal procedure** to improve properties of the conformalized intervals has been one of the primary focuses of recent literature. Noteworthy contributions--including CF-GNN  and ConTr --approach this problem by introducing modifications to the training stage

Figure 2: Schematic drawing showing the selection of the number of boosting rounds via cross-validation. Left: we hold out fold \(j\), and use the remaining \(k-1\) folds to generate candidate scores \(E^{(t)}_{j}\), \(t=0,,\)-round. The performance of each score is evaluated on fold \(j\) using the loss function \(\). Right: best-round minimizes the average loss across all \(k\) folds. A detailed description of the procedure is presented in Algorithm 1.

of the procedure. As outlined in Section 2, a model is trained to produce a score function \(E(,;f)\). The model \(f\) usually depends on a set of model parameters, e.g., neural network parameters \(\). Denote the trained model \(f\) by \(f_{}\). CF-GNN and ConTr retrain or fine-tune the model by using a carefully constructed loss function, which may aim to produce narrower prediction intervals or prediction sets of reduced cardinality in classification problems. This process generates a new set of model parameters \(^{}\). The new model \(f_{^{}}\) is then plugged into the _same_ predefined conformity score function--namely CQR  or the adaptive prediction set score (APS) --to produce \(E(,,f_{^{}})\).

There are two primary limitations. First, the score function imposes constraints on the properties of conformalized intervals as explained in Section 2. Our approach introduces more flexibility by constructing a family of generalized score functions that is a superset of \(\{E(,;f_{}):\}\), where \(\) is the parameter space of the training model. This family is strategically designed to contain an oracle conformity score ideally suited to the task at hand, e.g., achieving exact conditional coverage. Second, current methodologies necessitate fine-tuning or retraining models from scratch, requiring both access to the training model and significant computational resources. In contrast, our boosted conformal method operates directly on model predictions and circumvents these issues.

**Conditional coverage** of conformalized prediction intervals has also attracted significant interest, characterized by efforts to establish theoretical guarantees and achieve numerical improvements. Prior work established an impossibility result [8; 20], which states that exact conditional coverage in finite samples cannot be guaranteed without making assumptions about the data distribution. Subsequently, Gibbs et al.  developed a modified conformal procedure that guarantees conditional coverage for predefined protected sub-groups, i.e. subsets of the feature space. Our approach differs from the previous works by introducing a numerical method directly aimed at improving the conditional coverage, \((Y C_{n}(X)|X=x)\), across all potential values of \(x\).

## 5 Boosting for conditional coverage

Maintaining valid marginal coverage, our goal is to produce a prediction interval \(C_{n}\) obeying

\[(Y C_{n}(X_{n+1})|X_{n+1}=x) 1-\] (13)

for all possible values of \(x\). To this end, we present a loss function that quantifies the conditional coverage rate of any prediction interval. Requiring merely a dataset \(\) and a prediction interval \(C_{n}()\) as inputs, it also serves as an effective evaluation metric, which may be of independent interest.

### A measure for deviation from target conditional coverage

From now on, we let \(E\) be the score function \(E(,;f)\). Set \(=\{(X_{i},Y_{i})\}_{i=1}^{n}\) and denote by \(C_{n}()\) the conformalized prediction interval constructed from \(E\). We shall assess the deviation of \(C_{n}()\) from the target conditional coverage by means of Contrast Trees . As background, a contrast tree iteratively identifies splits within the feature space \(\) in a greedy fashion, aiming to maximize absolute within-group deviations from the target conditional coverage rate \((1-)\). For a subset \(R\) of the data point indices \([n]\), let \(_{R}=\{X_{j},Y_{j}\}_{j R}\). The absolute within-group deviation is computed as

\[d(C_{n}();_{R})=||R| ^{-1}\!\!_{j R}\!(Y_{j} C_{n}(X_{j}))-(1-)|.\] (14)

The overall empirical maximum deviation is then defined as

\[_{M}(E;)=_{1 m M}d(C_{ n}();_{_{m}})\!,\] (15)

where \(_{1}_{M}\) is a partition of \([n]\), which itself depends on \(E\) and \(\). Specifically, it is computed by running a contrast tree for \(M\) iterations. At each iteration, the algorithm not only seeks to isolate regions with large deviations but also discourages splits where any subset \(_{m}\) is too small.

To update score functions via gradient boosting as described in (11), we would need a differentiable approximation of the maximum deviation. To this end, we construct approximations for the following three components of the loss function. With an abuse of notation, in subsequent discussions, we shall employ the same notations to denote these differentiable approximations.

1. Approximation for the prediction interval \(C_{n}()\) in (14): the prediction interval is formulated as (7) for the generalized Local score, and as (10) for the generalized CQR score.

Denote the upper and lower limits of \(C_{n}()\) by \(u()\) and \(l()\). We approximate the empirical quantile \(Q_{1-}(E,I_{2})\) in \(u()\) and \(l()\) with a smooth quantile estimator \(Q_{1-}^{s}\). Given \(r\) scalars \(\{z_{i}\}_{i=1}^{r}\), \(Q_{1-}^{s}\) is constructed as: \[Q_{1-}^{s}(\{z_{i}\}_{i=1}^{r}):=(r),s(),\] (16) where \(,\) represents the dot product. Here, \((r)=[W_{r,1},...,W_{r,r}]\) is the weight vector corresponding to the Harrel-Davis distribution-free empirical quantile estimator , and \(s()\) is a differentiable ordering \(\{_{(i)}\}_{i=1}^{r}\), arranged in the ascending order. In practice, the derivative of \(s()\) with respect to each \(z_{i}\) is given by the package developed in . This approach is a smooth approximation of the Harrel-Davis quantile estimator \(Q_{1-}^{}\), constructed as a linear combination of the order statistics, \(Q_{1-}^{}=(r),\{z_{(i)}\})=_{i=1}^{ r}W_{r,i}z_{(i)}\), where \(W_{r,i}\) takes the value \(I_{(1-)(r+1),(r+1)}(i/r)-I_{(1-)(r+1),(r+1)}((i-1)/r)\) and \(I_{a,b}(x)\) represents the incomplete beta function.
2. Approximation for absolute deviation \(d_{i}\) (14): the indicator function in (14) can be approximated by the product of two sigmoid functions, \[(Y_{j} C_{n}(X_{j})) =(u(X_{j})-Y_{j} 0)(Y_{j}-l(X_{j}) 0)\] \[ S_{_{1}}(u(X_{j})-Y_{j})S_{_{1}}(Y_{j}-l(X_{j})),\] where \(_{1}\) is a parameter, trading off smoothness and quality of the approximation. The sigmoid function \(S_{_{1}}(x)\) is defined as \(S_{_{1}}(x)=(1+e^{-_{1}x})^{-1}\).
3. Approximation for maximum deviation: we employ a log-sum-exp function  to derive the differentiable approximation of \(_{M}\) as \[_{M}(E;):={_{2}}^{-1}_{m=1}^{M}d_{m}(C_{n}();_{m}))},\] (17) where \(_{2}\) is a parameter, serving the same purpose as \(_{1}\).

Here, we demonstrate calculating the derivative of the smooth approximation (17) with respect to each component of the generalized Local score, expanding it as follows:

\[_{M}(E;)={_{2}}^{-1}_{m=1}^{M}||R_{m}|^{-1}\!_{j R_{m}}\!S_{ _{1}}(u(X_{j})-Y_{j})S_{_{1}}(Y_{j}-l(X_{j}))-(1-)|)},\]

where

\[S_{_{1}}(u(X_{j})-Y_{j}) =(1+(_{j}+Q_{1-}^{s}(\{E_{i}\}_ {i=1}^{n})_{j}-Y_{j})]})^{-1},\] \[S_{_{1}}(Y_{j}-l(X_{j})) =(1+(Y_{j}-_{j}+Q_{1-}^{s}(\{E _{i}\}_{i=1}^{n})_{j})]})^{-1},\]

with \(_{i}=(X_{i})\), \(_{i}=(X_{i})\), \(E_{i}=|Y_{i}-_{i}|/_{i}\). As a result, for each feature \(X_{i}\) within \(\), we can evaluate \(_{M}(E;)/_{i}\) and \(_{M}(E;)/_{i}\) via the chain rule.

### Boosting score functions for conditional coverage

Since the empirical maximum deviation \(_{M}\) (15) is non-differentiable, we opt for the differentiable approximation during the gradient boosting step (11). Nonetheless, we utilize the original \(_{M}\) to select the number of boosting rounds as in step (12) and to evaluate the conditional coverage of the conformalized prediction interval on the test set.

#### 5.2.1 Theoretical guarantees

The oracle score function achieving conditional coverage as defined in (13) belongs to both proposed generalized score families.

**Proposition 5.1** (Asymptotic expressiveness).: _Let \(\{X_{i},Y_{i}\}_{i=1}^{n}\) be i.i.d. with continuous joint probability density distribution. Under the split conformal procedure, for any target coverage rate \(1-\), as \(n\), there exists \((^{*},^{*})\) and \((^{*}_{1},^{*}_{2},^{*})\) such that the corresponding generalized Local (6) and CQR (9) score functions recover conditional coverage at rate \(1-\), as defined in (13)._

It goes without saying that there is no reason to assume that the optimal \(^{*}\) corresponds to the conditional mean, median or any quantile of \(Y\) given \(X\), or that the optimal \(^{*}\) corresponds to the standard deviation or the mean absolute deviation of \(Y\) given \(X\), as in the original Local score (3). That said, our greedy strategy has no guarantee on global optimality and this is why the choice of the starting point--whether it is the Local or CQR score function--plays a role in the performance.

#### 5.2.2 Empirical results on real data

We apply our boosted conformal procedure to the 11 datasets previously analyzed in [23; 18; 22]. Details on the datasets are provided in Section A.6 in the Appendix. In each dataset, we randomly hold out \(20\%\) as test data. All experiments are repeated 10 times, starting from the data splitting. We refer to Section A.7 for details on the models and hyper-parameters we employ for the training and boosting stages.

We evaluate the conditional coverage of the prediction intervals as the maximum within-group deviations across a partitioned test set (15). This partition is obtained through a contrast tree algorithm described in Section 5.1. Figure 3 illustrates the comparison between miscoverage rates of prediction intervals at each leaf of the contrast tree. These intervals are derived under the classical Local conformal procedure and our boosted conformal procedure. Notably, the conditional coverage of the boosted prediction interval more closely aligns with the target rate \(1-\).

The experiment results summarized in Table 1 indicate that applying boosting significantly enhances the performance of the baseline Local procedure. In contrast, boosting on CQR does not yield significant improvements--a sign that CQR already targets conditional coverage. (Before boosting, the prediction intervals generated by the baseline Local procedure exhibit conditional coverage deviations up to three times greater than those of the baseline CQR procedure.) It is noteworthy, however, that after boosting, the conditional coverage of the Local procedure improves to a level comparable to that of the boosted CQR procedure. While generally slightly less effective, nevertheless surpasses the performance of the boosted CQR procedure in two cases. Results on the remaining datasets are deferred to Tables A2 and A3.

   \\  Dataset &  Method \\ Local \\  &  Improvement \\ Boosted \\  &  Method \\ CQR \\  & 
 Improvement \\ Boosted \\  \\  bike & \(10.979\) & 5.638 & -48.65\(\%\) & \(4.934\) & **4.925** & -0.17\% \\ bio & \(5.303\) & 4.862 & -8.31\(\%\) & \(5.069\) & **4.700** & -7.29\% \\ community & \(25.755\) & 13.466 & -47.71\(\%\) & \(12.688\) & **12.105** & -4.59\% \\ concrete & \(10.740\) & 8.763 & -18.40\(\%\) & \(9.039\) & **8.265** & -8.56\% \\ meps-19 & \(15.357\) & 5.656 & -63.17\(\%\) & \(5.507\) & **5.507** & -0.00\% \\ meps-20 & \(16.939\) & **6.998** & -58.69\(\%\) & \(7.614\) & 7.184 & -5.65\% \\ meps-21 & \(17.627\) & **7.832** & -55.57\(\%\) & 8.165 & 8.067 & -1.20\% \\  

Table 1: Test set maximum deviation loss \(_{M}\) evaluated on various conformalized intervals. The best result achieved for each dataset is highlighted in bold.

Figure 3: Comparison of test set conditional coverage evaluated on the dataset meps-19: (a) shows the classical Local-type conformal procedure and (b) our boosted Local-type conformal procedure. The target miscoverage rate is set to \(=10\%\) (red). Miscoverage rate is computed at each leaf of the contrast tree, constructed to detect deviation from the target rate. Each leaf node is labeled with its size, namely, the fraction of the test set it represents.

## 6 Boosting for length

We begin by specifying the oracle prediction interval with minimum length. For a random variable \(Z\), the High Density Region (HDR) at a specified significance level \(\), denoted as \(_{}(Z)\), is defined as the shortest deterministic interval that covers \(Z\) with probability at least \(1-\). The boundaries of \(_{}(Z)\), the lower limit \(Q_{l()}\) and the upper limit \(Q_{u()}\), obey the condition \((Z[Q_{l()},Q_{u()}]) 1-\). For a pair of \((X,Y)\) drawn from \(P\), for every value of \(x^{p}\), the oracle prediction interval at that point is expressed as

\[_{}(Y|X=x)=[Q_{l()}(Y|X=x),Q_{u()}(Y|X=x) ].\] (18)

Before introducing the boosting strategy, we present a word of caution against optimizing exclusively for this objective. Importantly, to maintain valid marginal coverage, the shortest prediction interval is prone to overcover when the spread of \(Y|X\) (the conditional distribution of \(Y\) given \(X\)) is small, and undercover when the spread of \(Y|X\) is large. This may be undesirable.

Similar to Proposition 5.1, we can show that the generalized score families exhibit the necessary expressiveness to contain the oracle conformity score, achieving optimal length while ensuring valid marginal coverage. The formal proof is deferred to Section A.3.

### A measure for length

Consider a dataset \(=\{(X_{i},Y_{i})\}_{i=1}^{n}\) and a score function \(E\). Denote the corresponding conformalized prediction interval by \(C_{n}()\), with its quality measured by the average length:

\[_{L}(E;)=n^{-1}{_{i=1}^{n}}|C_{n}(X_{i})|.\] (19)

To derive a differentiable approximation of \(_{L}\), we approximate the empirical quantile \(Q_{1-}\) in the conformalized intervals (7) and (10) with the smooth quantile estimator \(Q_{1-}^{s}\) constructed in (16). Here, we demonstrate calculating the derivative of the smooth approximation of \(_{L}\) with respect to each component of the generalized Local score, expanding it as follows based on the previously outlined approximation steps:

\[_{L}(E;)=2Q_{1-}^{s}(\{E_{i}\}_{i =1}^{n}), E_{i}=|Y_{i}-_{i}|/_{i},\]

with \(_{i}=(X_{i})\), \(_{i}=(X_{i})\), \(=n^{-1}_{i=1}^{n}_{i}\). As a result, for each feature \(X_{i}\) within \(\), we can evaluate \(_{L}(E;)/_{i}\) and \(_{L}(E;)/_{i}\) via the chain rule. For instance,

\[(E;)}{_{i}}=-2^{s}(\{E_{j}\}_{j=1}^{n})}{ E_{i}} (Y_{i}-_{i})}{_{i}}.\]

### Empirical results on real data

We apply our boosted conformal procedure to the same datasets described in Section 5.2.2. Detailed information on the models and hyperparameters used during the training and boosting stages can be found in Section A.7. Partial experiment results are summarized in Table 2. Notably, the boosting performance highlighted in bold exhibits significant improvement compared to previously documented results [17; 23]. We see a pronounced enhancement with the blog dataset; before boosting, the Local prediction intervals are on average \(42\%\) longer than those generated by CQR. After boosting, these intervals outperform the boosted CQR intervals by \(32\%\). Using CQR as the baseline also yields substantial improvements, a decrease in averaged length exceeding \(10\%\) in six out of the eleven datasets. The mems-21 dataset, in particular, shows an improvement of up to \(18\%\) relative to the baseline. Results on the remaining datasets can be found in Tables A4 and A5. Figure 4 compares the conformalized prediction intervals derived from baseline Local and CQR scores with those obtained from the boosted scores. To effectively visualize the impact of boosting, we conduct a regression tree analysis on the training set to predict the label \(Y\), setting the maximum number of tree nodes to four. This regression tree is then applied to the test set, allowing for a detailed comparison of the prediction intervals across each of the four distinct leaves.

## 7 Discussion

We introduced a post-training conformity score boosting scheme aiming to optimize for conditional coverage or length of the conformalized prediction interval. An intriguing avenue for future exploration involves simultaneously optimizing both length and conditional coverage, potentially tradingoff these objectives by incorporating user-specified weights . Additionally, we can readily adapt our procedure to meet various application-specific objectives. For instance, we can optimize for conditional coverage on predefined feature groups, a common task in enhancing fairness in distributing social resources across different demographic groups . Similarly, we can modify our procedure to reduce the length of prediction intervals for predefined label groups, which can be seen as reallocating resources to decrease uncertainty for certain groups at the expense of higher uncertainty for other groups . Candidate loss functions tailored to these objectives are detailed in Section A.1. Lastly, the primary emphasis of this paper centers on the design of the conformity score boosting scheme and formalizing the optimization of conditional coverage in mathematical terms, leaving room for computational optimization to enhance performance and runtime efficiency. In essence, the gradient boosting algorithm in our procedure can be replaced with any gradient-based machine learning model. Thus, another interesting future direction would be to explore whether alternative algorithms could enhance performance.

## Acknowlegements

E.C. was supported by the Office of Naval Research under Grant No. N00014-24-1-2305, the National Science Foundation under Grant No. DMS2032014, and the Simons Foundation under Award 814641. R.F.B. was supported by the Office of Naval Research via grant N00014-20-1-2337, and by the National Science Foundation via grant DMS-2023109.

   \\   &  &  &  \\  & Local & Boosted & & & & \\  blog & \(2.056\) & **0.972** & -52.74\% & \(1.445\) & 1.434 & -0.71\% \\ facebook-1 & \(1.896\) & 1.383 & -27.03\% & \(1.198\) & **1.072** & -10.47\% \\ facebook-2 & \(1.854\) & 1.363 & -26.51\% & \(1.200\) & **1.075** & -10.41\% \\ meps-19 & \(2.070\) & **1.685** & -18.60\% & \(2.554\) & 2.136 & -16.35\% \\ meps-20 & \(2.081\) & **1.836** & -11.80\% & \(2.667\) & 2.357 & -11.62\% \\ meps-21 & \(2.063\) & **1.795** & -12.99\% & \(2.585\) & 2.105 & -18.55\% \\  

Table 2: Test set average interval length \(_{L}\) evaluated on various conformalized prediction intervals. The best result achieved for each dataset is highlighted in bold.

Figure 4: Comparison of test set average interval length evaluated on the meps-19 and blog datasets: classical Local and CQR conformal procedure versus the boosted procedures (abbreviated as ‘Localb’ and ‘CQRb’) compared in each of the 4 leaves of a regression tree trained on the training set to predict the label \(Y\). A positive log ratio value between the regular and boosted interval lengths indicates improvement from boosting. The target miscoverage rate is set at \(=10\%\).