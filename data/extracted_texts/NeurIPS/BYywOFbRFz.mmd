# Waypoint Transformer: Reinforcement Learning via Supervised Learning with Intermediate Targets

Anirudhan Badrinath   Yannis Flet-Berliac   Allen Nie   Emma Brunskill

Department of Computer Science

Stanford University

{abadrina, yfletberliac, anie, ebrun}@cs.stanford.edu

###### Abstract

Despite the recent advancements in offline reinforcement learning via supervised learning (RvS) and the success of the decision transformer (DT) architecture in various domains, DTs have fallen short in several challenging benchmarks. The root cause of this underperformance lies in their inability to seamlessly connect segments of suboptimal trajectories. To overcome this limitation, we present a novel approach to enhance RvS methods by integrating intermediate targets. We introduce the Waypoint Transformer (WT), using an architecture that builds upon the DT framework and conditioned on automatically-generated waypoints. The results show a significant increase in the final return compared to existing RvS methods, with performance on par or greater than existing popular temporal difference learning-based methods. Additionally, the performance and stability improvements are largest in the most challenging environments and data configurations, including AntMaze Large Play/Diverse and Kitchen Mixed/Partial.

## 1 Introduction

Traditionally, offline reinforcement learning (RL) methods that compete with state-of-the-art (SOTA) algorithms have relied on objectives encouraging pessimism in combination with value-based methods. Notable examples of this approach include Batch Conservative Q-Learning (BCQ), Conservative Q-Learning (CQL), and Pessimistic Q-Learning (PQL) (Fujimoto et al., 2019; Kumar et al., 2020; Liu et al., 2020). However, these methods can be challenging to train and often require intricate hyperparameter tuning and various tricks to ensure stability and optimal performance across tasks.

Reinforcement learning via supervised learning (RvS) has emerged as a simpler alternative to traditional offline RL methods (Emmons et al., 2021). RvS approaches are based on behavioral cloning (BC), either conditional or non-conditional, to train a policy. Importantly, these methods eliminate the need for any temporal-difference (TD) learning, such as fitted value or action-value functions. This results in a simpler algorithmic framework based on supervised learning, allowing for progress in offline RL to build upon work in supervised learning. There are several successful applications of RvS methods, including methods conditioned on goals and returns (Kumar et al., 2019; Janner et al., 2021; Ding et al., 2019; Chen et al., 2021; Emmons et al., 2021).

However, RvS methods have typically struggled in tasks where seamlessly connecting (or "stitching") appropriate segments of suboptimal training trajectories is critical for success (Kumar et al., 2022). For example, when tasked with reaching specific locations in the AntMaze maze navigation environment or completing a series of tasks in the FrankaKitchen environment, RvS methods typically perform significantly worse than TD learning methods such as Implicit Q-Learning (Fu et al., 2020; Kostrikov et al., 2021).

In this study, we leverage the transformer architecture (Vaswani et al., 2017) to construct an RvS method. As introduced by Chen et al. (2021), the decision transformer (DT) can perform conditional behavioral cloning in the context of offline RL. However, similar to other RvS methods, DT proves inferior in performance across popular Gym-MuJoCo benchmarks compared to other value-based offline RL methods, with a 15% relative reduction in average return and lowered stability (Table 1).

To tackle these limitations of existing RvS methods, we introduce a waypoint generation technique that produces intermediate goals and more stable, proxy rewards, which serve as guidance to steer a policy to desirable outcomes. By conditioning a transformer-based RvS method on these generated targets, we obtain a trained policy that learns to follow them, leading to improved performance and stability compared to prior offline RL methods. The highlights of our proposed approach are as follows:

* We propose a novel RvS method, Waypoint Transformer, using waypoint generation networks and establish new state-of-the-art performance, in challenging tasks such as AntMaze Large and Kitchen Partial/Mixed (Fu et al., 2020) (Table 1). On tasks from Gym-MuJoCo, our method rivals the performance of TD learning-based methods such as Implicit Q-Learning and Conservative Q-Learning (Kostrikov et al., 2021; Kumar et al., 2020), and improve over existing RvS methods.
* We motivate the benefit of conditioning RvS on intermediate targets using a chain-MDP example and an empirical analysis of maze navigation tasks. By providing such additional guidance on suboptimal datasets, we show that a policy optimized with a behavioral cloning objective chooses more optimal actions compared to conditioning on fixed targets (as in Chen et al. (2021), Emmons et al. (2021)), facilitating improved stitching capability.
* Our work also provides practical insights for improving RvS, such as significantly reducing training time, solving the hyperparameter tuning challenge in RvS posed by Emmons et al. (2021), and notably improved stability in performance across runs.

## 2 Related Work

Many recent offline RL methods have used fitted value or action-value functions (Liu et al., 2020, Fujimoto et al., 2019, Kostrikov et al., 2021, Kumar et al., 2020, Kidambi et al., 2020, Lyu et al., 2022) or model-based approaches leveraging estimation of dynamics (Kidambi et al., 2020, Yu et al., 2020, Argenson and Dulac-Arnold, 2020, Shen et al., 2021, Rigter et al., 2022, Zhan et al., 2021).

RvS, as introduced in Emmons et al. (2021), avoids fitting value functions and instead leverages behavioral cloning. In many RvS-style methods, the conditioning variable for the policy is based on the return (Kumar et al., 2019, Srivastava et al., 2019, Schmidhuber, 2019, Chen et al., 2021), but other methods use goal-conditioning (Nair et al., 2018, Emmons et al., 2021, Ding et al., 2019, Ghosh et al., 2019) or leverage inverse RL (Eysenbach et al., 2020). Recent work by Brandfonbrener et al. (2022) has explored the limitations of reward conditioning in RvS. In this study, we consider both reward and goal-conditioning.

Transformers have demonstrated the ability to generalize to a vast array of tasks, such as language modeling, image generation, and representation learning (Vaswani et al., 2017, Devlin et al., 2018, He et al., 2022, Parmar et al., 2018). In the context of offline RL, decision transformers (DT) leverage a causal transformer architecture to fit a reward-conditioned policy (Chen et al., 2021). Similarly, (Janner et al., 2021) frame offline RL as a sequence modeling problem and introduce the Trajectory Transformer, a model-based offline RL approach that uses the transformer architecture.

Algorithms building upon the DT, such as online DT (Zheng et al., 2022), prompt DT (Xu et al., 2022) and Q-Learning DT (Yamagata et al., 2022), have extended the scope of DT's usage. Furuta et al. (2021) introduce a framework for hindsight information matching algorithms to unify several hindsight-based algorithms, such as Hindsight Experience Replay (Andrychowicz et al., 2017), DT, TT, and our proposed method.

Some critical issues with DT unresolved by existing work are (a) its instability (i.e., large variability across initialization seeds) for some tasks in the offline setting (Table 1) and (b) its relatively poor performance on some tasks due to an inability to stitch segments of suboptimal trajectories (Kumar et al., 2022) - in such settings, RvS methods are outperformed by value-based methods such as Implicit Q-Learning (IQL) (Kostrikov et al., 2021) and Conservative Q-Learning (CQL) (Kumar et al., 2020). We address both these concerns with our proposed approach, demonstrating notably improved performance and reduced variance across seeds for tasks compared to DT and prior RvS methods (Table 1).

One of the areas of further research in RvS, per Emmons et al. (2021), is to address the complex and unreliable process of tuning hyperparameters, as studied in Zhang and Jiang (2021) and Nie et al. (2022). We demonstrate that our method displays low sensitivity to changes in hyperparameters compared to Emmons et al. (2021) (Table 2). Further, all experiments involving our proposed method use the same set of hyperparameters in achieving SOTA performance across many tasks (Table 1).

## 3 Preliminaries

We assume that there exists an agent interacting with a Markov decision process (MDP) with states \(s_{t}\) and actions \(a_{t}\) with unknown transition dynamics \(p(s_{t+1} s_{t},a_{t})\) and initial state distribution \(p(s_{0})\). The agent chooses an action sampled from a transformer policy \(a_{t}_{}(a_{t} s_{t-k..t},_{t-k..t})\), parameterized by \(\) and conditioned on the known history of states \(s_{t-k..t}\) and a conditioning variable \(_{t-k..t}\). Compared to the standard RL framework, where the policy is modeled by \((a_{t} s_{t})\), we leverage a policy that considers past states within a fixed context window \(k\).

The conditioning variable \(_{t}\) is a specification of a goal or reward based on a target outcome \(\). At training time, \(\) is sampled from the data, as presented in Emmons et al. (2021). At test time, we assume that we can either generate or are provided global goal information \(\) for goal-conditioned tasks. For reward-conditioned tasks, we specify a target return \(\), following Chen et al. (2021).

To train our RvS-based algorithm on an offline dataset \(\) consisting of trajectories \(\) with conditioning variable \(_{t}\), we compute the output of an autoregressive transformer model based on past and current states and conditioning variable provided in each trajectory. Using a negative log-likelihood loss, we use gradient descent to update the policy \(_{}\). This procedure is summarized in Algorithm 1.

``` Input: Training dataset \(=\{_{1},_{2},_{3},...,_{n}\}\) of training trajectories. for each \(=(s_{0},a_{0},_{0},s_{1},a_{1},_{1},...)\) in \(\)do  Compute \(_{}(a_{t} s_{t-k..t},_{t-k..t})\) for all \(t\)  Calculate \(L_{}()=-_{t}log_{}(a_{t} s_{t-k..t},_{t-k..t})\)  Backpropagate gradients w.r.t. \(L_{}()\) to update model parameters endfor ```

**Algorithm 1** Training algorithm for transformer-based policy trained on offline dataset \(\).

## 4 Waypoint Generation

In this section, we propose using intermediate targets (or waypoints) as conditioning variables as an alternative to fixed targets, proposed in Emmons et al. (2021). Below, we motivate the necessity for waypoints in RvS and present a practical technique to generate waypoints that can be used for goal-conditioned (Section 4.2) and reward-conditioned tasks (Section 4.3) respectively.

### Illustrative Example

To motivate the benefits of using waypoints, we consider an infinite-horizon, deterministic MDP with \(H+1\) states and two possible actions at non-terminal states. A graphical representation of the MDP is shown in Figure 1. For this scenario, we consider the goal-conditioned setting where the target goal state during train and test time is \(=s^{(H)}\), and the episode terminates once we reach \(\).

In offline RL, the data is often suboptimal for achieving the desired goal during testing. In this example, suppose we have access to a dataset \(\) that contains an infinite number of trajectories

Figure 1: Chain MDP to motivate the benefit of intermediate goals for conditional BC-based policy training.

collected by a random behavioral policy \(_{b}\) where \(_{b}(a_{t}=a^{(1)} s_{t})=>0\) for all \(s_{t}\). Clearly, \(_{b}\) is suboptimal with respect to reaching \(\) in the least number of timesteps; in expectation, it takes \(\) timesteps to reach \(s^{(H)}\) instead of \(H\) (optimal) since the agent "stalls" at the current state with probability \(\) and moves to the next state with probability \(1-\).

Consider a global goal-conditioned policy \(_{G}(a_{t} s_{t},)\) that is optimized using a behavioral cloning objective on \(\). Clearly, the optimal policy \(_{G}^{*}(a_{t} s_{t},)=_{b}(a_{t} s_{t})\)\( s_{t}\) since \(=s^{(H)}\) is a constant. Hence, the global goal-conditioned policy \(_{G}^{*}\) is as suboptimal as the behavioral policy \(_{b}\).

Instead, suppose that we condition a policy \(_{W}(a_{t} s_{t},_{t})\) on an intermediate goal state \(_{t}=s_{t+K}\) for some chosen \(K<\) (expected timesteps before \(_{b}\) executes \(a_{2}\)), optimized using a behavioral cloning objective on \(\). For simplicity, suppose our target intermediate goal state \(_{t}\) for some current state \(s_{t}=s^{(h)}\) is simply the next state \(_{t}=s^{(h+1)}\). Based on data \(\) from \(_{b}\), the probability of taking action \(a^{(2)}\) conditioned on the chosen \(_{t}\) and \(s_{t}\) is estimated as:

\[_{_{b}}[a_{t}=a^{(2)} s_{t}=s^{(h)},s_{t+K}=s^{(h+1)}] =}[a_{t}=a^{(2)},s_{t+K}=s^{(h+1)} s_{t}=s^{ (h)}]}{_{_{b}}[s_{t+K}=s^{(h+1)} s_{t}=s^{(h)}]}\] \[=}{[(1-) ^{K-1}]}=}=.\]

Hence, for the optimal intermediate goal-conditioned policy \(_{W}^{*}\) trained on \(\), the probability of choosing the optimal action \(a^{(2)}\) is:

\[_{W}^{*}(a_{t}=a^{(2)} s_{t}=s^{(h)},_{t}=s^{(h+1)})=.\]

Since \(_{G}^{*}(a_{t}=a^{(2)} s_{t}=s^{(h)},)=1-\) and we choose \(K\) such that \(>1-\), we conclude:

\[_{W}^{*}(a_{t}=a^{(2)} s_{t},_{t})>_{G}^{*}(a_{t}=a^{(2)} s_ {t},).\]

The complete derivation is presented in Appendix A. Based on this example, conditioning the actions on reaching a desirable intermediate state is more likely to result in taking the optimal action compared to a global goal-conditioned policy. Effectively, the conditioning acts as a "guide" for the policy, directing it toward desirable intermediate targets in order to reach the global goal.

### Intermediate Goal Generation for Spatial Compositionality

In this section, we address RvS's inability to "stitch" subsequences of suboptimal trajectories in order to achieve optimal behaviour, based on analyses in Kumar et al. (2022). In that pursuit, we introduce a technique to generate effective intermediate targets to better facilitate stitching and to guide the policy towards desirable outcomes, focusing primarily on goal-conditioned tasks.

Critically, the ability to stitch requires considering experiences that are more relevant to achieving appropriate short-term goals before reaching the global goal. To illustrate this, we show a maze navigation task from the AntMaze Large environment in Figure 2, where the evaluation objective is to reach a target location from the start location (Fu et al., 2020).

Analyzing the training trajectories that pass through either the start location (blue) or target location (red), less than 5% of trajectories extend beyond the stitching region into the other region, i.e., the target or start regions respectively. Since trajectories seldom pass through both the start and target regions, the policy must "stitch" together subsequences from the blue and red trajectories within the stitching region, where the trajectories overlap the most. By providing intermediate targets within this region, rather than conditioning solely on the global goal, we can guide the policy to connect the relevant subsequences needed to reach the target effectively.

Figure 2: ant maze-large-play-v2 task to navigate from the start location (circle) to the target location (star). Blue and red colored lines are training trajectories passing through the start or end locations respectively.

To obtain effective intermediate targets, we propose the goal waypoint network, explicitly designed to generate short-term, intermediate goals. Similar to the illustrative example in Section 4.1, the purpose of these intermediate targets is to guide the policy network \(_{}\) towards states that lead to the desired global goal by facilitating stitching of relevant subsequences.

To that end, we represent the goal waypoint network \(W_{}\), parameterized by \(\), as a neural network that makes approximate \(K\)-step predictions of future observations conditioned on the current state, \(s_{t}\), and the target goal, \(\). Formally, we attempt to minimize the objective in Equation 1 across the same offline dataset \(\), where \(L_{}\) is a mean-squared error (MSE) loss for continuous state spaces:

\[_{}_{}L_{}(W_{}(s_{t},),s_{t+ K}).\] (1)

While our approach to intermediate target generation seems simple in relation to the complex problem of modeling both the behavioral policy and transition dynamics, our goal is to provide approximate short-term goals to facilitate the downstream task of reaching the global goal \(\), rather than achieving perfect predictions of future states under the behavioral policy.

### Proxy Reward Generation for Bias-Variance Reduction

In this section, we address the high bias and variance of conditioning variables used by prior RvS methods in reward-conditioned tasks, such as Emmons et al. (2021) and Chen et al. (2021). Analogously to Section 4.2 (i.e., for goal-conditioned tasks), we propose a technique to generate intermediate reward targets for reward-conditioned tasks to mitigate these issues.

Existing methods rely on either an initial cumulative reward-to-go (desired return) or an average reward-to-go target, denoted as \(\). Importantly, the former is updated using rewards obtained during the rollout, while the latter remains constant over time (Emmons et al., 2021; Chen et al., 2021). However, using these conditioning variables during evaluation gives rise to two main issues: (a) the Monte Carlo estimate of return used to compute the cumulative reward-to-go exhibits high variance and (b) the constant average reward-to-go target introduces high bias over time. Based on our analyses of the bias and variance of these approaches in Appendix C, we observe that these issues contribute to decreased performance and stability across runs when evaluating RvS methods.

Although a potential approach to mitigate these issues is to leverage TD learning, such as the Q-Learning Transformer (Yamagata et al., 2022), we restrict our work to RvS methods utilizing behavioral cloning objectives due to the inherent complexity of training value-based methods. To address the aforementioned concerns, we introduce a reward waypoint network denoted as \(W_{}\), parameterized by \(\). This network predicts the average and cumulative reward-to-go (ARTG, CRTG) conditioned on the return, \(\), and current state, \(s_{t}\), using offline data \(\). To optimize this network, we minimize the objective shown in Equation 2 using an MSE loss:

\[_{}_{}([}_{t^{ }=t}^{T}^{t}r_{t}_{t^{}=t}^{T}^{t}r_{t}]^{ }-W_{}(s_{t},))^{2}.\] (2)

By modeling both ARTG and CRTG, we address the high bias of a constant ARTG target and reduce the variance associated with Monte Carlo estimates for CRTG. The construction of the reward waypoint network is similar in motivation and the prediction task to a baseline network, used to mitigate high variance in methods like REINFORCE (Sutton and Barto, 1999). However, the distinguishing feature of our reward waypoint network lies in its conditioning on the return, which allows for favorable performance even on suboptimal offline datasets.

## 5 Waypoint Transformer

We propose the waypoint transformer (WT), a transformer-based offline RL method that leverages the proposed waypoint network \(W_{}\) and a GPT-2 architecture based on multi-head attention (Radford et al., 2019). The WT policy \(_{}\) is conditioned on past states \(s_{t-k..t}\) and waypoints (either generated goals or rewards) \(_{t-k..t}=W_{}(s_{t-k..t},)\) with a context window of size \(k\), as shown in Figure 3.

Figure 3: Waypoint Transformer architecture, where \(_{t}=W_{}(s_{t},)\) represents the output of the goal or reward waypoint network.

[MISSING_PAGE_FAIL:6]

[Brandfonbrener et al., 2021], TD3 + BC [Fujimoto and Gu, 2021], CQL, and IQL; and standard BC baselines. For all methods except DT, we use reported results from Emmons et al.  and Kostrikov et al. . We evaluate DT using the official implementation provided by Chen et al.  across 5 random initializations, though we are unable to reproduce some of their results.

Table 1 shows the results of our comparisons to prior methods. Aggregated across all tasks, WT (71.4 \(\) 2.8) improves upon the next best method, IQL (68.3 \(\) 6.9), with respect to average normalized score and achieves equal-best runtime. In terms of variability across seeds, there is a notable reduction compared to IQL and most other methods.

In the most challenging tasks requiring stitching, our method demonstrates performance far exceeding the next best method, IQL. On the AntMaze Large datasets, WT demonstrates a substantial relative percentage improvement of 83.1% (play) and 51.6% (diverse). On Kitchen Partial and Mixed, the improvement is 37.8% and 39.0% respectively. WT's standard deviation across seeds is reduced by a factor of more than 2x compared to IQL for these tasks.

Similarly, on reward-conditioned tasks with large performance gaps between BC and value-based methods such as hopper-medium-replay-v2, WT demonstrates increased average performance by 105.3% compared to DT and 21.0% compared to RvS-R, with standard deviation reduced by a factor of 10.0x and 5.3x respectively.

### Utility of Waypoint Networks

To analyze the utility and behavior of waypoint networks, we qualitatively evaluate an agent's performance across rollouts of trained transformer policies on antmaze-large-play-v2. For this analysis, we consider a WT policy (using a goal waypoint network with \(K=30\)) and a global goal-conditioned transformer policy (i.e., no intermediate goals). Across both models, the architecture and hyperparameters for training are identical.

The ant's locations across 100 rollouts of a WT policy (Figure 3(a)) and a global goal-conditioned transformer policy (Figure 3(b)) demonstrate that WT shows notably higher ability and consistency in reaching the goal location. Specifically, without intermediate goals, the ant occasionally turns in the wrong direction and demonstrates a lesser ability to successfully complete a turn based on the reduction of density at each turn (Figure 3(b)). Consequently, the WT achieves more than twice the evaluation return (72.5 \(\) 2.8) compared to the global goal-conditioned policy (33.0 \(\) 10.3) and completes the task more quickly on average (Figure 3(d)).

Based on Figure 3(c), we observe that the goal waypoint network provides goals that correspond to the paths traversed in Figure 3(a) for the WT policy. This shows that the waypoint network successfully guides the model toward the target location, addressing the stitching problem proposed in Figure 2. While the global goal-conditioned policy is successful in passing beyond the stitching region into the target region in only 45% of the rollouts, accounting for 82% of its failures to reach the target, WT is successful in this respect for 87% of rollouts.

### Ablation Studies

Goal-Conditioned TasksOn goal-conditioned tasks, we examine the behavior of the goal waypoint network as it relates to the performance of the policy at test time by ablating aspects of its configuration

Figure 4: Shows the antâ€™s location across 100 rollouts of **(a)** a WT policy and **(b)** a global goal-conditioned transformer policy; **(c)** generated intermediate goals by the waypoint network \(W_{}\), **(d)** the proportion of all successful runs completed by timestep \(t\).

and training. For this analysis, we consider antmaze-large-play-v2, a challenging task that critically evaluates the stitching capability of offline RL techniques.

To understand the effect of the configuration of the goal waypoint network on test performance, we ablate two variables relevant to generating effective intermediate goals: the temporal proximity of intermediate goals (\(K\)) and the validation loss of the goal waypoint network.

The normalized score attained by the agent is shown as a function of \(K\) and the validation loss of the goal waypoint network in Figure 5. For this environment and dataset, an ideal choice for \(K\) is around 30 timesteps. For all nonzero \(K\), the performance is reduced at a reasonably consistent rate on either side of \(K=30\). Importantly, when \(K=0\) (i.e., no intermediate goals), there is a notable reduction in performance compared to all other choices of \(K\); compared to the optimal \(K=30\), the score is reduced by a factor of 2.2x.

In Figure 5 (right), the normalized score shows the negligible change for values of held-out RMSE between 0.4 and 0.6, corresponding to at least 1,000 gradient steps or roughly 30 sec of training, with a sharper decrease henceforth. As the RMSE increases to over 1, we observe a relative plateau in performance near an average normalized score of 35-45, roughly corresponding to performance without using a waypoint network (i.e., \(K=0\) in Figure 5 (left)).

Additionally, we perform comparisons between the goal waypoint network and manually constructed waypoints as intermediate targets for WT, for which the methodology and results are shown in Appendix D. Based on that analysis, we show that manual waypoints statistically significantly improve upon no waypoints (44.5 \(\) 2.8 vs. 33.0 \(\) 10.3), but they remain significantly worse than generated waypoints.

Reward-Conditioned TasksOn reward-conditioned tasks, we ablate the choice of different reward-conditioning techniques. Specifically, we examine the performance of WT and variance of the reward waypoint network in comparison to CRTG updated using the rewards obtained during rollouts and a static ARTG (i.e., as done in Chen et al. (2021) and Emmons et al. (2021)). We consider the hopper-medium-replay-v2 task for this analysis as there is (a) a large performance gap between RvS and value-based methods, and (b) high instability across seeds for RvS methods (e.g., DT) as shown in Table 1. For all examined reward-conditioned techniques, the transformer architecture and training procedure are identical, and the target (normalized) return is 95, corresponding to SOTA performance.

To examine the distribution of normalized scores across different seeds produced by each of the described reward-conditioning techniques, we construct performance profiles, displaying the proportion of runs greater than a certain normalized score (Agarwal et al., 2021). As shown in Figure 6 (left), WT demonstrates increased performance and stability across random initializations compared to the remaining reward-conditioning techniques.

Additionally, we perform an analysis to determine whether using a reward waypoint network to predict the CRTG as opposed to updating the CRTG using attained rewards as in Chen et al. (2021) affects the variability of the conditioning variable passed to the policy network (i.e., not of the

Figure 5: Normalized score attained by WT on antmaze-large-play-v2 based on varying **left**: the temporal proximity of generated goals, \(K\), and **right**: goal waypoint network RMSE on a held-out dataset.

Figure 6: Comparison of different reward-conditioning methods on hopper-medium-replay-v2. **Left**: Performance profiles for transformers using ARTG, CRTG, and WT across 5 random seeds. **Right**: Standard deviation in CRTG imputed to the model when updated with attained rewards (\(_{t}=-_{t}^{t}r_{t}\)) and using predictions from the reward waypoint network (\(W_{}(s_{t},)_{2}\)) when average return is approximately held constant.

performance as that is examined in Figure 6). Importantly, to account for performance differences between the policies trained with either method that may influence the variability of the attained CRTG, we sample a subset of runs for both methods such that the average performance is constant. Based on Figure 6 (right), it is clear that as a function of the timestep, when accounting for difference in average performance, the standard deviation in the CRTG predicted by WT grows at a slower rate compared to updating CRTG with attained rewards.

Transformer ConfigurationBased on the work in Emmons et al. (2021), we balance between expressiveness and regularization to maximize policy performance. We ablate the probability of node dropout \(p_{}\) and the number of transformer layers \(L\). To further examine this balance, we experiment with conditioning on past actions \(a_{t-k..t-1}\), similarly to the DT, to characterize its impact on performance and computational efficiency. In this section, we consider antmaze-large-play-v2, hopper-medium-replay-v2 and kitchen-mixed-v0, one task from each category of environments.

Based on Table 2, we observe that the sensitivity to the various ablated hyperparameters is relatively low in terms of performance, and removing action conditioning results in reduced training time and increased performance, perhaps due to reduced distribution shift at evaluation. In context of prior RvS work where dropout (\(p_{}=0.1\)) decreased performance compared to no dropout by 1.5-3x on AntMaze, the largest decrease in average performance on WT is only by a factor of 1.1x (Emmons et al., 2021).

## 7 Discussion

In this study, we address the issues with existing conditioning techniques used in RvS, such as the "stitching" problem associated with global goals and the high bias and variance of reward-to-go targets, through the automatic generation of intermediate targets. Based on empirical evaluations, we demonstrate significantly improved performance and stability compared to existing RvS methods, often on par with or outperforming TD learning methods. Especially on challenging tasks with suboptimal dataset composition, such as AntMaze Large and Kitchen Partial/Mixed, the guidance provided by the waypoint network through intermediate targets (e.g., as shown in Figure 4) significantly improves upon existing state-of-the-art performance.

   \(p_{}\) & hopper-medium-replay & antmaze-large-play & kitchen-mixed & Average \\ 
0.000 & 75.5 \(\) 8.3 & 68.3 \(\) 5.9 & **72.9 \(\) 0.5** & 72.2 \(\) 4.9 \\
0.075 & **89.8 \(\) 2.8** & 70.8 \(\) 4.5 & 71.8 \(\) 1.2 & **77.5 \(\) 2.8** \\
**0.150** & 88.9 \(\) 2.4 & 72.5 \(\) 2.8 & 70.9 \(\) 2.1 & 77.4 \(\) 2.4 \\
0.225 & 75.7 \(\) 9.4 & 72.2 \(\) 2.7 & 71.2 \(\) 1.0 & 73.0 \(\) 4.4 \\
0.300 & 74.7 \(\) 10.2 & 73.5 \(\) 2.5 & 69.2 \(\) 2.0 & 72.5 \(\) 4.9 \\
0.600 & 58.4 \(\) 7.5 & **73.8 \(\) 5.2** & 66.5 \(\) 2.7 & 66.2 \(\) 5.1 \\   \(L\) & hopper-medium-replay & antmaze-large-play & kitchen-mixed & Average \\ 
1 & 82.1 \(\) 8.8 & 72.1 \(\) 5.7 & **71.6 \(\) 1.6** & 75.3 \(\) 5.4 \\
**2** & 88.9 \(\) 2.4 & **72.5 \(\) 2.8** & 70.9 \(\) 2.1 & **77.4 \(\) 2.4** \\
3 & 89.9 \(\) 1.6 & 71.8 \(\) 3.0 & 70.3 \(\) 2.1 & 77.3 \(\) 2.2 \\
4 & **91.1 \(\)** 2.8 & 65.8 \(\) 3.8 & 69.7 \(\) 1.0 & 75.5 \(\) 2.5 \\
5 & 88.8 \(\) 4.5 & 66.7 \(\) 4.7 & 70.0 \(\) 0.8 & 75.2 \(\) 3.3 \\   \(a_{t}\) & hopper-medium-replay & antmaze-large-play & kitchen-mixed & Average \\  Yes & 76.9 \(\) 9.0 & 66.5 \(\) 5.6 & 65.2 \(\) 2.8 & 69.5 \(\) 5.8 \\
**No** & **88.9 \(\) 2.4** & **72.5 \(\) 2.8** & **70.9 \(\) 2.1** & **77.4 \(\) 2.4** \\   

Table 2: Ablation of transformer configuration showing normalized score on MuJoCo (v2), AntMaze (v2) and Kitchen (v0), including dropout (\(p_{}\)), transformer layers (\(L\)), and action conditioning (\(a_{t}\)), where bolded hyperparameters (e.g., 0.150) are used for final models and bolded scores are optimal.

We believe that this work can present a pathway forward to developing practical offline RL methods leveraging the simplicity of RvS and exploring more effective conditioning techniques, as formalized by Emmons et al. (2021). In addition to state-of-the-art performance, we demonstrate several desirable practical qualities of the WT: it is less sensitive to changes in hyperparameters, significantly faster to train than prior RvS work, and more consistent across initialization seeds.

However, despite improvements across challenging tasks, WT's margin of improvement on AntMaze U-Maze and Kitchen Complete (i.e., easier tasks) is lower: its normalized scores are more comparable to DT and other RvS methods. We believe this is likely due to stitching being less necessary in such tasks compared to difficult tasks, rendering the impact of the waypoint network negligible. To further characterize the performance of the waypoint networks and WT on such tasks is an interesting direction for future work. In addition, there are several limitations inherited by the usage of the RvS framework, such as manual tuning of the target return at test time for reward-conditioned tasks using a grid search, issues with stochasticity, and an inability to learn from data with multimodal outcomes.

## 8 Conclusion

We propose a method for reinforcement learning via supervised learning, Waypoint Transformer, conditioned on generated intermediate targets for reward and goal-conditioned tasks. We show that RvS with waypoints significantly surpasses existing RvS methods and achieves on par with or surpasses popular state-of-the-art methods across a wide range of tasks from Gym-MuJoCo, AntMaze, and Kitchen. With improved stability across runs and competitive computational efficiency, we believe that our method advances the performance and applicability of RvS within the context of offline RL.