# Detection Based Part-level Articulated Object Reconstruction from Single RGBD Image

Yuki Kawana\({}^{1}\), Tatsuya Harada\({}^{1,2}\)

\({}^{1}\)The University of Tokyo, \({}^{2}\)RIKEN AIP

{kawana, harada}@mi.t.u-tokyo.ac.jp

###### Abstract

We propose an end-to-end trainable, cross-category method for reconstructing multiple man-made articulated objects from a single RGBD image, focusing on part-level shape reconstruction and pose and kinematics estimation. We depart from previous works that rely on learning instance-level latent space, focusing on man-made articulated objects with predefined part counts. Instead, we propose a novel alternative approach that employs part-level representation, representing instances as combinations of detected parts. While our detect-then-group approach effectively handles instances with diverse part structures and various part counts, it faces issues of false positives, varying part sizes and scales, and an increasing model size due to end-to-end training. To address these challenges, we propose 1) test-time kinematics-aware part fusion to improve detection performance while suppressing false positives, 2) anisotropic scale normalization for part shape learning to accommodate various part sizes and scales, and 3) a balancing strategy for cross-refinement between feature space and output space to improve part detection while maintaining model size. Evaluation on both synthetic and real data demonstrates that our method successfully reconstructs variously structured multiple instances that previous works cannot handle, and outperforms prior works in shape reconstruction and kinematics estimation.

## 1 Introduction

Estimating object shape, pose, size, and kinematics from a single frame of partial observation is a fundamental challenge in computer vision. Understanding such properties of daily articulated objects has various applications in robotics and AR/VR.

Shape reconstruction of daily articulated objects is a challenging task. These objects exhibit a range of shapes resulting from different local part poses. More importantly, they display significant intra- and inter-category diversity in part configurations, including variations in part counts and structures. These factors together contribute to an exponentially increasing shape variation. Previous works have addressed this issue by either limiting a single model to target objects with a single articulated part  or employing multiple category-level models  to accommodate varying part counts. These approaches first detect each instance, and then model the target shape in an instance-level latent space, primarily employing A-SDF  for shape learning. A-SDF maps the target shape into an instance-wise latent space, and then a shape decoder outputs the entire shape of the instance. However, this approach is limited when dealing with varying part counts and structures, as the shape decoder must handle an exponentially increasing number of shape variations due to different part layout combinations  in addition to local part poses. Consequently, addressing this variety with a single model remains a complex and unsolved task.

In this paper, we address this complexity through our novel detect-then-group approach. Our key observation is that daily articulated objects consist of similar part shapes. For example, regardlessof the number of refrigerator doors, each door typically has a similar shape, and the base part may share similarities with those from other categories, such as storage units. By detecting each part and then grouping them into multiple instances, we provide a scalable and generalizable approach for handling diverse part configurations of daily articulated objects in a scene.

Based on this concept, we propose an end-to-end detection-based approach for part-level shape reconstruction. Building upon 3DETR  as an end-to-end trainable detector backbone, given a single RGBD image with an optional foreground mask, our model outputs part shape, pose, joint parameters, parts-to-instance association, and instance category. Our approach employs a novel detect-then-group approach. It first detects parts and applies simple clustering of parts into instances based on learned part embeddings' distance, in contrast to the previous works using additional instance detection module [28; 15]. An overview of our approach is shown in Fig 1. However, we found that detection-based shape reconstruction is prone to false positives for articulated objects' thin and small parts with little overlap, which is hard to remove by NMS. Also, articulated objects often have parts of varied sizes and scales, making training with a single-shape decoder challenging. Additionally, increasing model size by end-to-end training from detection to reconstruction makes simply enlarging the model size to improve performance undesirable. To address these challenges, we propose:(1) kinematics-aware part fusion to reduce false positives and improve detection accuracy; (2) anisotropic scale normalization for various part sizes and scales in shape learning; (3) and an output space refiner module coupled with a model-size balancing strategy with decoder for improved performance while keeping the model size. We evaluate our method on the photorealistically rendered SAPIEN  dataset, and our approach outperforms state-of-the-art baselines in shape reconstruction and joint parameter estimation. Furthermore, the model trained only on synthetic data generalizes to real-world data, outperforming the state-of-the-art methods on the BMVC  dataset.

Our contributions can be summarized as follows: (1) a novel part-level end-to-end shape reconstruction method for articulated objects from a single RGBD image; (2) a novel detect-then-group approach that simplifies the pipeline; (3) addressing detection-based reconstruction challenges with kinematics-aware part fusion, anisotropic scale normalization, and a refiner module coupled with model-size balancing; (4) superior performance on the SAPIEN  dataset, with the ability to generalize to real-world data from the BMVC  dataset.

## 2 Related Work

Articulated shape reconstructionA number of works have focused on human subjects using single image input [46; 45] and utilize category-specific templates to recover deformation from a canonical shape [29; 64; 3; 63; 25]. Recent research has delved into recovering articulated shapes with unknown kinematic chains from sequences of a target in motion [41; 60]. These works make category-level assumptions about kinematic structures, with targets in observations sharing common kinematic structures. Their main focus is on natural objects such as humans and animals. In contrast, our interest is in reconstructing the shape of multi-category, daily man-made articulated objects with diverse kinematic structures using a single model. Recent years have seen the emergence of methods specifically targeting man-made articulated objects [56; 23; 53], and taking single-frame input [38; 28; 15; 24]. However, these models are constrained by either a predefined number of parts per category or the necessity of multiple models for each combination of categories and part counts. Consequently, they are unable to scale to a wide array of real-world articulated objects with varying part counts using a single model. Our approach addresses this limitation.

Figure 1: Our detection-based approach estimates part-level shape, pose, and kinematics as joint parameters. It also recovers parts-to-instance associations to handle multiple instances with various part structures and counts.

Pose and kinematic estimation of articulated objects predominantly, existing research on pose and kinematic estimation of articulated objects has focused on estimation from sequences [16; 21; 57; 49], necessitating multiple frames of moving targets or interaction with the environment before estimation. Estimation from a single image has also been explored [27; 36; 28], but these methods are limited to predefined part structures. A few recent studies have proposed approaches without assumptions on part structure [22; 13]. However, these methods target single instances, requiring instance detection before part-level estimation. Moreover, their focus is limited to detection, pose and kinematic estimation, whereas our work aims for shape reconstruction in an end-to-end manner.

Detection-based reconstructionA large body of work exists combining detection and reconstruction for multiple rigid objects in diverse settings, such as indoor scenes [51; 39; 40; 61; 54; 14], tabletop environments [19; 20], and road scenes [2; 33]. Recent works target daily articulated objects [28; 16]. However, these methods predominantly rely on an instance-level detection approach. In contrast, our work pivots towards part-level detection to effectively handle a wide variety of part structures of real-world articulated objects.

## 3 Methods

### Problem setting

Our method takes a point cloud \(P\) with \(N_{P}\) 3D points and color feature \(F\) lifted from a single RGBD image \(I\) of articulated objects and camera intrinsic \(K\) as input. It outputs a set of parts \(G=\{i[N]\}\) and non-overlapping subsets of all parts \(\{G_{l}\}_{l=1}^{N_{G}}\) as \(N_{G}\) instances, where \(G=_{l}\{G_{l}\}_{l=1}^{N_{G}}\). Note that we use the shorthand \(\{*_{i}\}\) to denote an ordered set \(\{*_{i}\}_{i=1}^{N}\) for brevity. For each part \(i\), we estimate 6D pose and size \(B(3)^{3}\), part shape \(\) as an implicit representation and kinematic parameters \(M\). Our shape representation is an implicit function defined as \(:^{3}\) with isosurface threshold \(_{}\), where \(\{^{3}\ |\ ()>_{}\}\) indicates inside the shape. The kinematic parameter \(M\) consists of joint type \(y\{0,1,2\}\) which represents fixed, revolute, and prismatic types, joint direction \(^{2}\), 1D joint state \(d_{}\) and \(d_{}\) for the current pose and fully opened pose from

Figure 3: Architecture of refiner \(\) Figure 4: Illustration of (a) false positives, (b) Convex hull for kIoU, and (c) comparison of 3D box IoU and kIoU for overlapping parts.

the canonical pose. We also predict the revolute origin \(^{3}\) for the revolute part. We define the joint parameter as \(A=\{,d_{},d_{}[,]\}\), thus \(M=\{y,A\}\). For each instance \(l\), we estimate the instance parameter \(J\) which consists of category \(u\) and part association defined as \(_{li}=(i G_{l})\), where \(\) is an indicator function.

### Detection backbone

Our detection backbone consists of a transformer-based encoder \(\) and decoder \(\) based on 3DETR . The encoder comprises recursive self-attention layers encoding 3D points \(P\) and color feature \(F\) into downsampled 3D point cloud \(P^{}\) of \(N_{P^{}}\) points and \(D_{F^{}}\)-dimensional scene feature \(F^{}\). Query locations \(\{_{n}\}_{n=1}^{N_{q}}^{3}\) are randomly sampled using furthest point sampling (FPS) from \(P^{}\). The decoder is composed of transformer decoder layers \(\{_{k}\}_{k=1}^{N_{}}\), considering cross-attention between queries and \(F^{}\) and self-attention among queries. The decoder iteratively refines query features \(\{_{n,k+1}\}=_{k}(\{_{n,k}\},F^{})\). Lastly, a set of part prediction MLPs decodes each refined query feature to produce output values. For clarity, the query index \(n\) is omitted when possible.

### Part representation

Part pose and sizeWe predict part pose and size \(B\) as a set of part center \(^{3}\), rotation \((3)\) and size \(^{3}\) for each query. We predict \(\) as an offset \(\) from \(\), added to the query coordinates, i.e., \(=+\).

Part shapeWe employ a shared-weight, single implicit shape decoder \(\) for performing part-wise shape reconstruction by taking point clouds around the detected regions where parts are identified. Given the diversity in shape and pose, and anisotropic scaling of the parts we focus on, it is challenging to learn shape bias with a single shape decoder. Therefore, we propose anisotropically normalizing the side lengths of the shape decoder's input and output to a unit scale to perform reconstruction. We define the input point cloud \(P_{}\) as follows:

\[P_{}=\{()^{-1}(-) P,(|()^{-1}(-)|) 0.5\}.\] (1)

where \(\) denotes diagonal matrix of scale \(\). We define the output occupancy value at \(^{3}\) as \(o_{}=(()^{-1}(-) P_{ },)\), where \(^{D_{}}\) is a part shape feature modeled by a part prediction MLP. Given that the input point cloud \(P_{}\) includes background, the geometry of the target part shape can be ambiguous. To address this issue, we train the detector backbone and shape decoder end-to-end, by inputting \(\) as shape geometry to the shape decoder so that \(\) informs the shape decoder of the foreground target shape. We utilize a shape decoder architecture with a lightweight local geometry encoder  to spatially associate input points \(P_{}\) with output occupancy values \(o_{}\), which are both defined in normalized space.

Part kinematicsWe predict a \(4\)-dimensional vector \(\) as a probability distribution over part joint types. This includes a 'background' or 'not a part' type for instances where predicted part proposals might not contain a part. The revolute origin \(=+\) is predicted similarly to the part center \(\), with an offset \(\). Joint states \(d_{}\) and \(d_{}\) are modeled by separate part prediction MLPs for revolute and prismatic types, and we only supervise the output corresponding to the ground truth joint type. A single MLP is used for joint direction \(\) for both revolute and prismatic types.

### Instance representation

We form a collection of parts as a single instance \(G_{l}=\{i G i,j G,i j\|_{i}- _{j}\|<_{}\}\), where the distance between the \(D_{}\)-dimensional embeddings, \(_{i}\) and \(_{j}\), of each part pair within the group is kept below a specified threshold \(_{}\). We predict an \(N_{}\)-dimensional vector \(\) as a probability distribution over instance categories per part. At test time, we predict the instance category by taking the category prediction with the highest confidence from the category predictions of the parts belonging to the same instance.

### Refiner \(\)

Query features are iteratively refined in the decoder \(\) in the feature space . Increasing the number of decoder layers can enhance performance at the expense of a larger model size . However, it is crucial to avoid increasing the model size to enable efficient end-to-end training from detection to shape reconstruction. To improve model performance while maintaining the model size, we found cross-refining both output space and feature space to be effective. We introduce a refiner \(\), which has an identical architecture with the decoder \(\). The refiner \(\) serves to refine the prediction in output space by reallocating a portion of decoder layers from decoder \(\) to \(\). Assuming there are \(N_{+}\) decoder layers in the original model, we reallocate \(N_{}\) decoder layers to the refiner \(\). Consequently, \(N_{}=N_{+}-N_{}\) layers are used for query feature refinement in decoder \(\). The refiner \(\) refines part pose and size \(B^{}\) and joint parameter \(A^{}\) from \(\) by predicting residuals \( B\) and \( A\) to produce refined prediction \(B\) and \(A\), respectively. The architecture of the refiner \(\) is shown in Fig. 3.

### Kinematics-aware part fusion (KPF)

Articulated objects often consist of small and thin parts. Especially for unsegmented inputs, only a small portion of the point cloud represents such parts after subsampling. To ensure a sufficient number of queries cover such parts and their surrounding context, at test time, we randomize the sampling positions of the queries and independently carry out inference and NMS for the input \(N_{Q}\) times, obtaining a collection of densely sampled part proposals from \(N_{Q}\) inference runs. This process is termed Query Oversampling (OQ). However, OQ can increase false positives and degrade detection performance. To mitigate this, we propose Part Fusion (PF), inspired by Weighted Box Fusion (WBF) , to merge overlapping part proposals, thereby reducing false positives and improving detection accuracy. Unlike WBF, which fuses only 2D bounding parameters, PF fuses all the predicted parameters of part proposals, with an average weight defined as objectness \(1-_{}\). However, using IoU with 3D bounding boxes as overlapping metrics yields overly small values for thin structured parts, even when they are proximate. This results in PF failing to merge redundant parts, leading to false positives, as illustrated in Fig. 4 (a). To overcome this challenge, we propose a kinematics-aware IoU (kIoU) based on the observation that a redundant part pair exhibits significantly overlapping trajectories. To calculate kIoU, we construct a convex hull for each part in the pair using the 24 vertices of three bounding boxes based on size and canonical pose, current pose, and fully opened pose using the predicted part pose and size \(B\) and joint parameter \(A\), as show in Fig. 4 (b). Then, we calculate the IoU between the two hulls. The comparison between the 3D bounding box IoU and kIoU is depicted in Fig. 4 (c). Our overall method, termed Kinematic-aware Part Fusion (KPF), includes: (1) executing inference and NMS using kIoU to gather part proposals multiple times, (2) conducting PF using kIoU to update these proposals iteratively, and (3) removing proposals with low objectness. Note that KPF is non-differentiable and is disabled during training, with \(N_{Q}=1\). For further details, please refer to the appendix.

### Set matching and training loss

Set matchingWe base our end-to-end training of detection to reconstruction by utilizing 1-to-1 matching between part proposals and ground truth, using bipartite matching  for loss calculation. Similarly to , we define a matching cost between a predicted part and a ground truth part as \(C_{match}=_{1}\|-^{}\|_{1}+_{2}\| -^{}\|_{1}-_{3}_{y}+_{4} (1-_{})\), where \(\) defines eight vertices of cuboid defined by part pose and size \(B\), \(_{y}\) defines the joint type probability given the ground truth label \(y\), and \(1-_{}\) defines the foreground probability. Deviating from , we use the L1 distance of eight vertices of a cuboid instead of the GIoU  of cuboids in the first term to avoid the costly calculation of enclosing hull for 3D rotated cuboids.

Training lossesFor each pair of prediction and ground truth, we define part loss as

\[_{}&= _{i}^{N}\|_{i}-_{i}^{}\|_{1}+\|I-_ {i}^{T}_{i}^{}\|_{F}^{2}+_{^{3}}(o_{,i},o_{,i}^{})+\|d_{,i}-d _{,i}^{}\|_{1}\\ &+\|d_{,i}-d_{,i}^{}\|_{1}- _{i}^{T}_{i}^{}+(_{i},_{i}^{},_{i}^{})+(_{i}, _{i}^{})+(_{i},_{i}^{})\] (2)

All loss terms have equal weights. The first term is a disentangled L1 loss described in  to optimize \(\). This loss is replicated three times by using only one of the predicted three components (**R**,**c**,**s**) for \(\), while replacing the other three with their ground truth values. We also found that directly optimizing rotation, as in the second term of the loss, leads to smaller rotation loss during training. BCE and CE denote binary cross-entropy and cross-entropy loss, respectively. PL denotesthe point-line distance between the revolute origin \(_{i}\) and the ground truth joint axis defined by \(_{i}^{}\) and \(_{i}^{}\). For unrefined prediction \(B^{}\) and \(A^{}\), we define the same loss as \(_{}\) except for \(o_{}\), \(_{i}\) and \(_{i}\) denoted as \(_{}^{}\). During training, we use ground truth \(B\) for Eq.1 to avoid noisy prediction adversely affecting shape learning.

We also define instance loss \(_{}\) for learning part-to-instance association with modified improved triplet loss  defined as:

\[_{}=_{}_{i G}|}_{j G_{l|i} i}[_{ij}-_{}^{}]_{+}+ _{i G}[_{j G_{l|i} i}_{ij}-_{j^{ } G G_{i}}_{ij^{}}+3_{}^{}]_ {+}\] (3)

where \(G_{l|i}=\{j G_{l}_{li}=1\}\) and \(_{ij}=\|_{i}-_{j}\|\) denotes the L2 distance between the part-to-instance association embeddings of the \(i\)-th and \(j\)-th parts. The first term enforces the distance between the two embeddings of two different parts belonging to the same instance below \(_{}^{}\) and the second term ensures the distance between embeddings of parts belonging to different instances is larger than \(3_{}^{}\). However, computing all combinations of triplets for the second term is operationally complex for part-wise supervision. To streamline this, we instead opt to maximize the difference between the upper bound and the lower bound of inter- and intra-instance distances of the embeddings. In practice, we replace \(\) and \(\) with their soft approximations defined as \((_{ij})\) and \(-(-_{ij^{}})\) for smooth gradient propagation. During inference, we use a threshold \(_{}=(3_{}^{}+_{}^{ })\) to determine if two parts belong to the same instance based on the distance between their embeddings.

The total loss we minimize is \(_{}=_{}+_{}^ {}+_{}\). At training time, we use the same part prediction MLPs to predict part parameters at every layer in the decoder. We compute the \(_{}\) for each layer independently and sum all the losses. We only use the part parameter predicted from the last decoder layer at test time. Full loss formulation can be found in the appendix.

### Implementation detail

The implementation of the transformer encoder-decoder architecture and the optimizer configuration are based on the publicly available code of 3DETR . We employ a masked encoder architecture for \(\) from . The input point cloud to the encoder consists of \(N_{P}=32768\) points, which are subsampled to \(N_{P^{}}=2048\) points. The number of queries is set to \(N_{q}=128\) during training. At test time, we set it to \(N_{q}=512\), and with independent runs for query oversampling as \(N_{Q}=10\). The number of decoder layers for \(\) and \(\) are set to \(N_{}=6\) and \(N_{}=2\), respectively. For the evaluation of synthetic data, the foreground masks are inferred by a pretrained segmentation model using a ResNext50  encoder and a DeepLabV3Plus  segmentation head with RGBD input. To evaluate real-world data, we automatically generate foreground masks using https://remove.bg, following . The weights for the matching cost \(_{}\) were set to \(_{1}=8,_{2}=10,_{3}=1,_{4}=5\). We set \(_{}=0.1\) for the intra-instance loss term of the instance loss \(_{}\). We use the AdamW  optimizer with a base learning rate of 9e-4 and employ a cosine scheduler with nine epochs for warm-up. The training was performed using two A100 GPUs, each 40GB of GPU memory, and with a batch size of 26 for each GPU. The training took approximately two days. Please refer to the appendix for further details regarding the training process, hyperparameters, and network architecture.

## 4 Experiments

DatasetsWe evaluate our method on both synthetic and real-world data. We use the SAPIEN  dataset for synthetic data evaluation, following recent works on articulated shape reconstruction [24; 56; 15]. We select ten categories with representative articulation types, a sufficient number of CAD models, and various part structures across categories. We then randomly construct room

   & F-Score@80\% \(\) & F-Score@90\% \(\) & CD@5\% \(\) & CD@1\% \(\) & IoU@25\% \(\) & IoU@50\% \(\) \\  A-SDF-GT  & 65.49 & 47.69 & 74.60 & 39.76 & 36.62 & 10.81 \\ A-SDF-GT-2  & 68.81 & 52.55 & 75.91 & 43.58 & 38.59 & 10.43 \\ Ours-BG & 74.22 & **68.80** & 75.71 & **58.61** & 40.06 & 9.80 \\ Ours & **74.77** & 68.38 & **77.39** & 56.53 & **41.35** & **11.63** \\  

Table 1: Shape mAP results on SAPIEN  dataset.

geometry (wall and floor) and place one to four instances per scene. Each instance is generated by applying random horizontal flips, random anisotropic resizing of side lengths, and random articulation of CAD models. The camera pose is sampled randomly, covering the upper hemisphere of the scene. Instances with severe truncation from the view frustum or occlusion with other instances are ignored during training and evaluation, ensuring that least one instance is visible in a view. For the training split, we randomize the textures of parts and room meshes. We use the original textures from the SAPIEN dataset for the test split. We generated 188,726 images for training and validation. Due to computational and time constraints, we used 20,000 images for training and kept the rest for validation usage. Also, we generated 4,000 images for the test split. Image size is 360 \(\) 640 in height and width. The data overview is shown in Table 2. More details on data preparation can be found in the appendix.

For real-world data, we use the BMVC  dataset for quantitative evaluation. We use cabinet class which includes both prismatic and revolute parts and has the same part count and similar object shape as those used in our training and baseline models. The data contains two sequences of RGBD frames, capturing the same target objects with different part poses from various camera poses. We also test our method on RGB images taken with an iPhone X and depth maps generated from partial front views of the scene using Nerfacto .

MetricsFor shape evaluation, following detection-based object reconstruction studies , we report mAP combined with standard shape evaluation metrics denoted as metric@threshold. We use F-Score , Chamfer distance (CD), and volumetric IoU (IoU) with multiple matching thresholds. More details on shape metrics can be found in the appendix. The shape mAP evaluation includes all the predicted parameters except for part kinematic parameters, serving as a comprehensive proxy for their quality. For part-wise detection with part pose and size, we use the average L2 distance

    &  &  &  \\ Category & Rev. & Pris. & Most freq. & Min & Max & Train & Test \\  Dishwasher & ✓ & ✓ & 1 & 1 & 2 & 4997 & 1032 \\ Trashcan & ✓ & ✓ & 1 & 1 & 2 & 5037 & 1017 \\ Safe & ✓ & & 1 & 1 & 1 & 4795 & 1029 \\ Oven & ✓ & & 1 & 1 & 3 & 4951 & 1006 \\ Storage & ✓ & ✓ & 1 & 1 & 14 & 4907 & 973 \\ Table & ✓ & ✓ & 1 & 1 & 9 & 4790 & 999 \\ Microwave & ✓ & ✓ & 1 & 1 & 2 & 4832 & 934 \\ Frige & ✓ & & 2 & 1 & 3 & 5089 & 924 \\ Washing & ✓ & & 1 & 1 & 1 & 5042 & 995 \\ Box & ✓ & & 1 & 1 & 4 & 4823 & 979 \\   

Table 2: Overview of synthetic data from SAPIEN  dataset. Rev. and Pres. denote revolute and prismatic joints, respectively.

between the corresponding eight corners of bounding boxes between the ground truth and prediction as a matching metric of mAP, similar to the ADD metric , denoted as mAP@threshold. The distance is normalized by the ground truth part's diameter, with a proportion of the diameter used as the threshold. Since IoU-based metrics can yield values close to zero for thin structured parts, even when they are reasonably proximate and slightly off the ground truth, and do not consider part orientation, we use the above matching metrics to better analyze part-wise detection. For part kinematics evaluation, we evaluate the absolute error on joint state (State), Orientation Error (OE) for joint direction, and Minimum Distance (MD) for rotation origin following  for the detected parts with matched ground truth.

BaselinesBy default, we denote the proposed method that takes the foreground mask as 'Ours', and the model taking unsegmented input as 'Ours-BG'. To the best of our knowledge, no prior work operates on exactly the same problem setting as ours. For shape reconstruction, we benchmark against the state-of-the-art category-level object reconstruction method A-SDF , the closest to our approach. We follow the most recent work setup  in evaluation. A-SDF requires instance segmentation, pose, and scale to project the object from the camera to the object-centric frame. We implement it using all the required data with ground truth, denoted as A-SDF-GT following . Given A-SDF's assumption of a fixed part count per model, as per a similar setup of , we train it on the dataset's most frequent part count per category. To account for multiple part counts in a category, for categories with more than one part count, we train the second model for the next most frequent count, resulting in a maximum of two models per category, termed A-SDF-GT-2. When the input instance has an untrained part count at test time, the model trained on the most frequent part count is used for A-SDF-GT-2. In the mAP evaluation, we give GT correspondence between the prediction and ground truth; thus, a false positive happens only when the shape metric is lower than the threshold for A-SDF. For kinematic evaluation, we compare our method with the state-of-the-art single-view part kinematic estimation method OPD . Note that we input OPD with an RGBD image to align input modality and modify it to output joint state. More details on baselines can be found in the appendix.

### Shape reconstruction

We show the shape mAP result in Table 1. Our method outperforms A-SDF  with ground truth in all metrics, and Ours-BG outperforms in the majority of metrics. We show the qualitative results in Fig. 5. Our models effectively reconstruct multiple instances with diverse part counts and structures, outperforming A-SDF which struggles with reconstructing articulated parts. We attribute our method's superior performance to its part-level representation, which facilitates easier learning of various part structures. Moreover, our shape-decoder is less affected by shape variations arising from the combination of part structures and poses. The qualitative results from novel viewpoints and additional visualizations can be found in the appendix.

### Kinematic estimation

OPD  performs 2D detection evaluated by 2D IoU, while ours on 3D by L2 distance between 3D bounding box corners. To make kinematics estimation results of OPD and ours comparable, we experimentally choose the matching threshold of our mAP as 70% so that a similar number of detected parts with OPD's result with 2D segmentation mAP@50%. Then, we select the intersection of true positive detected parts by OPD and ours. The result is shown in Table 3. Our methods outperform OPD significantly. We attribute the reason to our method operating on 3D point clouds

Figure 7: Qualitative results on the BMVC  dataset.

while OPD is on 2D. Thus, our method is more robust to various textures and lighting conditions and makes it easier to reason about 3D kinematics. Note that our focus here is kinematics estimation after the _detection_ step. Thus, superiority in detection performance is not our focus.

### Ablation studies

In the following ablation studies, we validate each proposed component in a challenging setting where the input to the encoder has an unsegmented background using Ours-BG.

Kinematics-aware part fusionWe show quantitative results in Table 4. Besides mAP, we also show precision considering false positives. QO denotes test-time query oversampling, and PF indicates part fusion. As a baseline, we first disable all components (w/o QO, PF, kIoU) using the same number of queries \(N_{q}=128\) during training and add each component one by one. Introducing our proposed kIoU on top of QO and PF outperforms the baseline in shape reconstruction mAP and part detection while preserving similar precision. We visualize the effectiveness of the KPF module in Fig. 8. In the provided comparison, 'w/o KPF' denotes disabling QO, PF and kIoU. We see that KPF enhances the detection and pose estimation of small parts. We also show the qualitative results on the proposed kIoU in Fig. 9. Applying QO and PF alone leads to false positives of thin parts, which kIoU effectively reduces. It indicates that the proposed KPF module improves overall detection performance while suppressing false positives.

Anisotropic scaling and end-to-end training for shape learningWe validate the effect of anisotropic scale normalization and end-to-end training in shape mAP evaluation. In Table 5, w/o AS denotes using _isotropic_ scaling by normalizing the _maximum_ side length to one instead of _all_ sides. w/o SF denotes not passing shape feature \(\) but training shape decoder \(\) separately. Disabling each component degrades performance. Especially disabling anisotropic scaling significantly drops the performance, as the single shape decoder is tasked to decode various sizes of target shapes.

Ratio of decoder layers in the refiner \(\)We investigate the relationship between the proportion of decoder layers in \(\) and performance in shape mAP. The result is shown in Fig. 11. We vary the ratio of decoder layers in the refiner \(N_{}/N_{+}\) from 0% to 75%. Allocating a portion (25%) of decoder layers to the refiner improves performance with the same number of decoder layers while reducing excessive decoder layers from the decoder degrades performance.

### Real-world data

We verify the generalizability of our approach, which is trained only on synthetic data to real-world data. Here, we include foreground masks as inputs to mitigate the domain gap from background.

Figure 8: Qualitative results on kinematics-aware part fusion (KPF).

Figure 10: Failure cases.

Figure 9: Qualitative results on kinematics-aware IoU (kIoU).

[MISSING_PAGE_FAIL:10]

#### Acknowledgments

We appreciate the members of the Machine Intelligence Laboratory for constructive discussion and their insightful feedback during the research meetings. This work was partially supported by JST Moonshot R&D Grant Number JPMJPS2011, CREST Grant Number JPMJCR2015 and Basic Research Grant (Super AI) of Institute for AI and Beyond of the University of Tokyo.