# Provably Transformers Harness Multi-Concept Word Semantics for Efficient In-Context Learning

Dake Bu\({}^{1}\), Wei Huang\({}^{2}\), Andi Han\({}^{2}\), Atsushi Nitanda\({}^{3,4}\), Taiji Suzuki\({}^{5,2}\), Qingfu Zhang\({}^{1}\), Hau-San Wong\({}^{1}\)

\({}^{1}\)_Department of Computer Science, City University of Hong Kong, Hong Kong SAR \({}^{2}\)Center for Advanced Intelligence Project, RIKEN, Japan \({}^{3}\)CFAR and IHPC, Agency for Science, Technology and Research (A+STAR), Singapore \({}^{4}\)College of Computing and Data Science, Nanyang Technological University, Singapore \({}^{5}\)Department of Mathematical Informatics, the University of Tokyo, Japan_

dakebu2-c@my.cityu.edu.hk, {wei.huang.vr, andi.han}@riken.jp,

atsushi_nitanda@cfar.a-star.edu.sg,taiji@mist.i.u-tokyo.ac.jp,

{qingfu.zhang, cshswong}@cityu.edu.hk

Corresponding authors

###### Abstract

Transformer-based large language models (LLMs) have displayed remarkable creative prowess and emergence capabilities. Existing empirical studies have revealed a strong connection between these LLMs' impressive emergence abilities and their in-context learning (ICL) capacity, allowing them to solve new tasks using only task-specific prompts without further fine-tuning. On the other hand, existing empirical and theoretical studies also show that there is a linear regularity of the multi-concept encoded semantic representation behind transformer-based LLMs. However, existing theoretical work fail to build up an understanding of the connection between this regularity and the innovative power of ICL. Additionally, prior work often focuses on simplified, unrealistic scenarios involving linear transformers or unrealistic loss functions, and they achieve only linear or sub-linear convergence rates. In contrast, this work provides a fine-grained mathematical analysis to show how transformers leverage the multi-concept semantics of words to enable powerful ICL and excellent out-of-distribution ICL abilities, offering insights into how transformers innovate solutions for certain unseen tasks encoded with multiple cross-concept semantics. Inspired by empirical studies on the linear latent geometry of LLMs, the analysis is based on a concept-based low-noise sparse coding prompt model. Leveraging advanced techniques, this work showcases the exponential 0-1 loss convergence over the highly non-convex training dynamics, which pioneeringly incorporates the challenges of softmax self-attention, ReLU-activated MLPs, and cross-entropy loss. Empirical simulations corroborate the theoretical findings.

## 1 Introduction

Recently, a variety of transformer-based large language models (LLMs) have demonstrated remarkable performance across a broad spectrum of machine learning tasks, including natural language understanding , symbolic reasoning , and even heuristics design . One crucial emerging ability of these models is their in-context learning (ICL) capacity , which allows them to learn from a few demonstrations and conduct predictions on new queries without requiring any furtherfine-tuning. However, the current theoretical understanding of the mechanisms underlying this ICL capability remains limited, leaving the reasons for the remarkable emergence and generalization power of transformer-based LLMs in unseen ICL tasks largely unexplained.

In line with traditional topic models , [7; 8] propose that latent concepts / topics underlie natural texts, providing a Bayesian inference framework to elucidate the ICL mechanism via Bayesian Model Averaging (BMA) approach. On the other hand, theoretical and empirical studies have shown that transformer-based models exhibit linear geometric regularities in their latent representations as a result of concept or topic learning [9; 10], where the representations _within-concept_ have positive inner products while representations _cross-concepts_ exhibit near-orthogonal relationships. This structured semantic geometry has been well-documented in recent research on pre-trained LLMs [11; 12; 10; 13]. However, the connection between this observed multi-concepts latent geometric structure and the LMs' remarkable ICL capabilities remains unclear. Separately, recent theoretical analyses have modeled ICL as a martingale process driven by latent "concept" variables [14; 15]. Yet, these studies have not incorporated the observed multi-concept semantic regularity into their analyses, nor have they discussed the strong out-of-distribution (OOD) ICL abilities exhibited by transformers.

Additionally, existing theoretical work on transformer has been conducted on unrealistic, oversimplified settings, such as linear or ReLU transformers [16; 17; 18; 19], MLP-free attention-only models [16; 20], QK-combined softmax attention [19; 20; 21; 22; 23], unrealistic infinite dimensional assumption [14; 19; 21; 24] and impractical loss functions like square loss [9; 16; 25; 20; 26] and hinge loss [27; 28]. Furthermore, existing works have only been able to derive linear or sub-linear convergence rates for the 0-1 loss.

Therefore, there is a need for a more advanced analysis that can bridge the understanding between the multi-concept semantic regularity and the mechanisms underlying transformer-based ICL. This naturally leads to the research question:

## Essential Questions

Whether and how do the geometric regularity of the multi-concept-encoded representation facilitate transformer in conducting efficient ICL?

To answer the above question, following the meaningful data modeling ideas in [9; 29], we conduct theoretical analysis on a concept-specific sparse coding prompt distribution for classification tasks, where the sparse latent variable encodes the information denoting the word's belonging concept. Importantly, the features in both the word's and label's dictionaries exhibit concept-specific geometric properties - within-concept positive inner products and cross-concept orthogonal geometric properties - that aligns with the findings in [9; 10; 11]. Our main contributions are highlighted as below.

1. First, we provide a comprehensive analysis of the learning dynamics for a two-layer transformer model, comprising one attention layer followed by a ReLU-activated feed-forward network, which is trained using the cross-entropy loss via stochastic gradient descent over a concept-specific sparse coding prompt distribution. Leveraging advanced analytical techniques, we showcase the asymptotic properties governing the coupled learning dynamics of the attention and MLP layers.
2. To the best of our knowledge, we are the first to prove an exponential convergence of the 0-1 loss over this challenging setting. Despite the highly non-convex optimization landscape, we demonstrate that the transformer can achieve Bayes optimal test error with just a logarithmic number of iterations.
3. We provably show how the multi-concept encoded linear semantic geometry can enable transformer to efficiently perform certain out-of-distribution ICL tasks. This offers an intuitive explanation for why transformer-based LLMs are able to successfully leverage the polysemous nature of words to tackle diverse, unseen concept-specific tasks, aligning well with users' practical experiences. Furthermore, our analysis takes a step forward in providing a potential theoretical underpinning for the innovative capabilities of LLMs, encompassing their ability to achieve cross-concept knowledge intersection. We believe our findings provide an initial positive response to Question 5.1.4 in the ICML 2024 position paper , which asks whether the observed latent geometry of LLMs can explain their OOD extrapolation abilities.

Related Work

**Theory of Exponential Convergence Rate of Stochastic Gradient Descent.** Our analysis of the exponential convergence rate for the 0-1 loss builds upon prior work linking the excess risk and essential supremum norm to exponentially fast convergence under the "hard low-noise condition" [31; 32]. This phenomenon has been further explored in more recent studies analyzing the exponential convergence of stochastic gradient descent (SGD) [33; 34; 35; 36; 37], as well as in more generalized settings such as multiclass classification  and support vector machines .

**Feature Learning in Learning Theory.** Recent works in learning theory have extensively studied structured data from a _feature learning_ perspective, examining NN's feature direction reconstruction and noise memorization as a proxy for training or 0-1 loss convergence [40; 41; 42]. While prior studies often assumed orthogonal features, recent efforts have analyzed non-orthogonal scenarios [43; 44]. Our work extends this line-of-research to challenging nonlinear Attention-MLP transformers with non-orthogonal structured data representations.

**Theory of Transformers and In-Context Learning** The literature on Transformers and ICL is wide-ranging, and we will selectively address the most relevant ones. Prior studies have analyzed how transformers learn topic/concept semantics , the origins and biases of LLM representations using latent variable models , and ICL from a model averaging perspective . However, albeit incorporating concept variables, these works do not connect the geometric properties of concept-encoded representations to transformers' powerful ICL abilities. Another line of research has studied the learning dynamics of ICL, including analyses of linear transformers [17; 19], QK-combined attention-only models , and multi-head softmax attention over linear regression without MLP . Though relevant, these works rely on simplifications and do not notice the connection between semantic regularity and powerful ICL. While  also analyzes the learning dynamics of transformers with softmax attention and ReLU MLPs for in-context classification tasks, making it the most relevant prior work, our analysis differs in several key aspects. Specifically, (i) they consider orthogonal dictionary learning with a single label vector, in contrast to our non-orthogonal concept-encoded dictionaries for both words and labels; (ii) their technique requires a large batch size (at least \(^{-2}\), where \(\) is the test error) and long context lengths, which are not required in our result; and (iii) they utilize an impractical hinge loss and only achieve linear convergence without a relation to \(\), whereas we analyze the more practical cross-entropy loss and derive an exponential convergence rate in terms of the test error \(\). However, we note that this is only an informal comparison due to the differences in the models and primary findings. A detailed Related Work Section is deferred to Appendix C.

## 3 Problem Setup

**Notations.** For \(l_{2}\) and Frobenius norms we utilize \(\|\|\) and \(\|\|_{F}\) to denote their computations. Considering two series \(a_{n}\) and \(b_{n}\), we denote \(a_{n}=O(b_{n})\) if there exists positive constant \(C>0\) and \(N>0\) such that for all \(n N\), \(|a_{n}| C|b_{n}|\). Similarly, we denote \(a_{n}=(b_{n})\) if \(b_{n}=O(a_{n})\) holds, and \(a_{n}=(b_{n})\) if \(a_{n}=O(b_{n})\) and \(a_{n}=(b_{n})\) both hold. Our \(()\) is to denote the indicator variable of an event. In addition, we denote \((v_{1},v_{2},,v_{k})\) as the linear subspace spanned by the vectors \(v_{1},v_{2},,v_{k}\), and \((v_{1},v_{2},,v_{k})\) denotes the conic hull (the set of all non-negative linear combinations) of the vectors \(v_{1},v_{2},,v_{k}\).

### Data Distribution

The data distribution employed in this study draws inspiration from a range of empirical and theoretical research works [9; 10; 46; 47; 48]. This distribution captures context-awareness and can be viewed as a specialized prompt version of PLSA  and LDA . In this distribution, each word and label has multiple feature embeddings, each embedding corresponding to a different concept. This is achieved through the use of a sparse latent concept/topic variable, which happened to be particularly adept at representing language polysemy . Adhering to the LLM representation explored in [9; 10], the features in both the word and label dictionaries maintain orthogonality across concepts and positive inner products within concepts. Additionally, the distribution incorporates Gaussian noise accounting for linguistic ambiguity or the imperfection of the LLM's representation.

**Definition 1**.: _Polysemous Word Model \((_{},_{},_{},_{_{}},_{_{}})\). We assume there exists \(K_{1}\) task-relevant concepts, each characterized by two semantically-opposite word's feature vectors \(_{k_{1}}^{+}\) and \(_{k_{1}}^{-}\), and their corresponding label's feature vectors \(_{k_{1}}^{+}\) and \(_{k_{1}}^{-}\), \( k_{1}[K_{1}]\). There are also \(K_{2}\) task-irrelevant concepts denoted by \(_{k_{2}}\), \( k_{2}[K_{2}]\). The word samples \(^{d_{}}\) and their labels \(^{d_{}}\) are generated from distributions parameterized by a shared latent concept variable \(=(z_{1},,z_{K})\{0,1\}^{K}(K<d_{})\) capturing the concept-specific information:_

\[_{},_{}_{ _{}}=(,_{}^{2}_{d_{}}),_{}_{_{}}=(,_{ }^{2}_{d_{}}),\] \[=+_{}_{}, =+_{}_{},\]

_where the feature dictionary \(=[_{1}^{+},_{1}^{-},_{2}^{+},_{2}^{- },,_{K_{1}}^{+},_{K_{1}}^{-},_{1},_{2}, ,_{K_{2}}]^{d_{} K}\) exhibits positive inner products within concepts and orthogonality across concepts, and the label dictionary \(=[_{1}^{+},_{1}^{-},_{2}^{+},_{2}^{-}, ,_{K_{1}}^{-},_{K_{1}}^{-},0, 0]^{d_{ } K}\) has similar geometric properties. Specifically, we have \( k_{1}[K_{1}],k_{2}[K_{2}],\|_{k_{1}}^{}\|=\|_{ k_{2}}\|=\|\|,\|_{k_{1}}^{}\|=\|\|\), and there exist constants \(0<_{},_{}<1\) such that \(0<_{k_{1}}^{+},_{k_{1}}^{-}_{}\| \|^{2}\) and \(0<_{k_{1}}^{+},_{k_{1}}^{-}_{}\| \|^{2}\)._

The detailed formal definition can be found in Appendix E. By this definition, a single word or label can possess different features corresponds to different concepts. The illustration of Figure 1 in  can be an example, where the "Dog" vector in the representation space of LLM is decomposed to a direct sum of orthogonal vectors: "[Animal] + [Mammal] + \(\)", and we can see "[Animal]" belongs to the concept "Organism's Category" categorized into labels "[Animal]" and "[Plant]", and "[Mammal]" belongs to the concept of "Animal's Category" characterized by labels "[Mammal]", "[Fish]", "[Bird]", "[Reptile]". Besides, Figure 1 in  can also be a good support for our modeling, where "Ferrari" vector consists of "[Cars] + [Italian] + \(\)".

The following definition models the contextual prompts via specifying the statistical property of \(\) among in-context words, which is a special prompt version of PLSA  and LDA . The detailed formal version is available in Appendix E.

**Definition 2**.: _Concept-specific Contextual Prompt Distribution2. During training, each prompt sample \(S=_{1},_{1},,_{L},_{L},_{L+1}\) would share at least one co-concept, which is drawn from a mixture distribution \(_{S}\) defined as:_

\[_{S}=_{k=1}^{K_{1}}(_{k}^{+}_{k,L+1}^{+}+_ {k}^{-}_{k,L+1}^{-}),\] (1)

_where \(_{k,L+1}^{}\) denotes the \(k\)-th concept-specific prompt distribution, and \(_{k}^{}=(2K_{1})^{-1}\) denotes the equal chance of a sample to belong to \(_{k,L+1}^{}\). Specifically, a sample \(S_{n}_{k,L+1}^{e},e[]\) means that the query's label \(_{L+1}^{n}\) is \(_{k}^{e}\), and we denote \(y_{S_{n}} e\) as the real value label of this prompt. In addition, every demonstration pairs \((_{1}^{n},_{1}^{n}),l[L]\) in \(_{k,L+1}^{e}\) contain either \((_{k}^{+},_{k}^{+})\) or \((_{k}^{-},_{k}^{-})\) with equal chance. Also, every \(_{i}^{n},l[L+1]\) would satisfy \((_{l, 2k-1 2k}^{n})=1)=K^{-1}\), denoting the equal chance to have diverse features other than the current co-concept of the \(_{k,L+1}^{e}\)._

This definition suggests that for prompt \(S\) sampling from \(_{S}\), there exists \(e[]\), \(k[K_{1}]\), such that all the word-label pairs in this prompt share the \(k\)-th concept as their co-concept, and the corresponding real value label of the query in this prompt is \(e\). Besides, the real value label of each word-label pair in the demonstration would have equal chance to be \(+1\) or \(-1\).

### Transformer Model

Following , our embedding \(()\) of prompt \(S\) is formulated as \(\):

\[=(S)=(_{1}&_{2}&& _{L}&}}\\ _{1}&_{2}&&_{L}&)( _{1},_{2},,_{})^{(d_{ }+d_{})(L+1)},\]

The learning model is a single-head, one-layer Transformer with one self-attention layer and one two-layer perceptron. Mathematically, it can be expressed as follows:

\[f(;)=^{}_{R}(_{O} (;)),\] \[(;)=_{l=1}^{L}_{V }_{l}_{S}((_{K}_{l})^{ }_{Q}_{}),\]where \(_{R}()(),_{S}() (),_{Q},_{K}^{m_{ (d_{X}+d_{Y})}},_{V}^{m_{}(d_{X}+d _{Y})}\) are the embedding matrices for queries, keys, and values, respectively, and \(_{O}^{m m_{v}}\) and \(^{m}\) are parameters in the MLP layer. Typically, \((m_{qk},m_{v}) d_{}+d_{}\). \(\{_{Q},_{K},_{V},_{ O},\}\) denotes the set of all model weights.

**Training Setting**. We fix one layer in both the attention and MLP layers to scrutinize the training dynamics more rigorously. Specifically, we let

\[_{Q}=(_{Q}^{}&*\\ *&*),_{K}=(_ {K}^{}&*\\ *&*),_{V}=(*&*\\ *&_{V}^{})_{O}=(*&_{O}^{}),\]

where \(_{Q}^{},_{K}^{}^{d_{X} d_{ X}},_{V}^{}^{(m_{v}-d_{X}) d_{}}\), \(_{O}^{}^{m d_{Y}}\). Here, we set the elements other than \(_{Q}^{}\), \(_{K}^{}\), \(_{V}^{}\) and \(_{O}^{}\) to be zero. Besides, we fix \(_{V}^{}\) to be \(_{(m_{v}-d_{X}) d_{}}\). We sample \(_{i}\) from a uniform distribution \(\{-1,1\}\) and fixed during the training process. Based on this setting, the trainable part we need to consider is actually \(^{}\{_{Q}^{},_{K}^{}, _{O}^{}\}\). This problem remains highly non-convex and challenging.

We utilize mini-batch with-replacement SGD to train the transformer model. The empirical cross-entropy loss for each batch \(_{t}\) is written as

\[L_{_{t}}()=L_{_{t}}(^{}) _{n_{t}}(y_{S_{n}} f( ;))+\|^{}\|_{F}^{2},\]

where \((z)=(1+(-z))\), \(y_{S_{n}}\) is the real value label of the prompt defined in Definition 2, and the term \(\|^{}\|_{F}^{2}\) represents \(\|_{Q}^{}\|_{F}^{2}+\|_{K}^{}\|_{F}^{2}+\| _{O}^{}\|_{F}^{2}\), which is the \(L_{2}\) regularization term with \(\|\|_{F}\) denoted as the Frobenius norm. The purpose of the regularization in this paper is to accelerate and stabilize the mini-batch with-replacement SGD. The learning step is set to be \(_{}=\), where \(\) is an offset parameter. This decaying schedule is standard and also used in prior work [34; 50; 51] studying convergence of SGD. The whole procedure is in Algorithm 1.

**Initialization Setting.** All initial values of \(_{O}^{}\) are sampled from a i.i.d. Gaussian distributions with mean 0 and variance \(_{1}^{2}\). The initialization of \(_{Q}^{}\) and \(_{K}^{}\) are diagonal matrices \(_{0}\), which are also adopted in other work that consider training \(_{Q}\) and \(_{K}\) separately [25; 28].

**Testing Setting**. The model performance is measured by 0-1 test error on a test prompt distribution \(^{*}\):

\[L_{^{*}}^{0-1}():=_{S^{*}}[ (y_{S} f((S);))<0].\] (2)

``` Input: Training distribution \(_{S}\), Test distribution \(^{*}\), Batch size \(B\), step size \(_{t}=\), stopping criterion \(\) and total epochs \(T\).  Initialize model parameters \({^{}}^{(0)}\). for\(t=0,1,,T-1\)do  If \(L_{^{*}}^{0-1}(^{(t)})\) stop else continue.  Randomly sample mini batches \(_{t}\) of size \(B\) from \(_{S}\).  Update model parameters: \({^{}}^{(t+1)}={^{}}^{(t)}-_{t}_{^{}}L_{ _{t}}({^{}}^{(t)})\). endfor ```

**Algorithm 1** Training algorithm

## 4 Theoretical Results

In this section, we present our main theoretical results, which is based on the following conditions. We consider the learning iterations \(0 t T^{*}\), where \(T^{*}=(m^{-1}_{0}^{-1}_{1}^{-1}m^{-2}K_{1}\| \|^{2}((L-1)\|\|^{2}+1)(^{-1}))\) denotes the maximum admissible iteration.

**Condition 1**.: _Suppose that there exists a sufficiently large constant \(C\), such that the following hold:_

1. \(d_{},d_{}\{C(KLBT^{*}/),K\}\)_,_ \(d_{} C(m/)\)_,_ \(m C(K/)\)_._2. \( C\{\|\|^{2}/(mK_{1}),10/\}\), \(\{(C(Km/)\|\|)^{-1},(C_{0}/2\| \|^{2})^{-1}\}\)
3. \(K\{CK_{1},C\|\|/(_{}}})\}\).
4. \(_{}\{ m/(C}}\|\|^{1/2}), \|\|/(C}})\}\).
5. \(_{0}(\|^{2}}{ K_{1}^{2}} (\|^{2}}{m K_{1}}))/(C\|\|)}\), \(_{1}\{(C_{0}\|\|^{4}\|\|/K_{1})^{-1},{w^{*}}^{2}/(Cm^{3/2}\|\|)\}\).

_Here, \(w^{*}=^{2}(1-_{})^{2}\|\|^{ 4}/2}}{1+e^{-_{0}^{2}(1-_{})^{2}\|\|^{4}/2}}\)._

Note that we do not have any requirement upon demonstration length \(L\) and batch size \(B\) for training, thus the training can be really flexible compared with the strict requirement in . The condition on dimensionality \(d_{},d_{}\) and the network width \(m\) ensure the learning problem is in a sufficiently overparameterized setting . The condition on \(\) ensures the learning step to be small and thus learning process enjoys an approximation to gradient flow. The condition on the small \(\) is to ensure the model's sufficient learning before being stuck by regularization . The condition on \(K\) is to control the impact of cross-concept contribution in the Attention's learning dynamic, which can actually be relaxed at the cost of a denser analysis. The condition on \(_{}\) is to ensure that the gradient flows be mildly influenced by the noise. Last but not least, the conditions on \(_{1}\) guarantee that the initial beliefs of MLP is small and the gradients of SGD can update the model effectively. A more detailed discussion over the parameter settings is delayed to Appendix H.

**Theorem 2**.: _Exponential Convergence of 0-1 Loss. Under Condition 1, define_

\[\{2_{1}/(1+_{}),_{0}(1- _{})e^{-(5Km/)^{2}\|\|^{4}(1+ _{}-_{0}^{2}\|\|^{2})}{(1-_{}-_{0}^{2} \|\|^{2})}}\}.\]

_Then, for \(>0\) there exist some positive constants \(C_{1}\) and \(C_{2}\), with probability no less than \(1-\), for \(T=C_{1}_{1}m K_{1}})(5 Km/)}/{w^{*}}^{2}(1-_{})\|\|\), we have_

\[L_{^{*}}^{0-1}(^{(T)})(-^{2}m^{2} (+T)}{K_{1}\|\|^{2}((L-1)\|\|^{2}+1)}).\]

_Thus after_

\[T_{}=\|\|^{2}((L-1)\|\|^{2}+1)}{C_ {2}^{2}m^{2}}()\]

_iterations, we have \(L_{^{*}}^{0-1}(^{(T)})\)._

Note that the bound is valid only when \(T\), a common threshold in prior convergence rate analyses . Importantly, the existence of \(\) does not affect the convergence rate as \( 0\), since \(\) is independent of \(\). Our novel analysis generalizes these prior results to our realistic settings handling the challenges of self-attention, ReLU-MLP, and cross-entropy loss simultaneously. By considering extreme cases, our techniques relax the batch size requirement, enabling more general results. Consequently, the sample complexity for Bayes-optimal test error is \(N=T_{}\).

Before introducing the next proposition, we highlight a key observation from the semantic geometry in Definition 1. For any \(k_{1}[K_{1}]\), defining \(_{k_{1}}(_{k_{1}}^{+}+_{k_{1}}^{-})/2\) and \(_{k_{1}}(_{k_{1}}^{+}-_{k_{1}}^{-})/2\), we find that for \(k_{1}^{} k_{1}\), \(\{_{k_{1}},_{k_{1}}\}\{_{k_{1}^{}},_{k_{1}^{ }}\}\) and \(_{k_{1}},_{k_{1}}=0\). This structure is exemplified in Figure 1(b) of , where "[Bird]" consists of orthogonal steering vectors: "plant \(\) animal" and "mammmal \(\) bird," corresponding to the concept feature \(_{k}\) and semantic label features \(_{k}\). Here, the term \(e_{k_{1}}\) in \(_{k_{1}}^{*}\) determines the label assignment. Similarly, defining \(_{k_{1}}(_{k_{1}}^{+}+_{k_{1}}^{-})/2\) and \(_{k_{1}}(_{k_{1}}^{+}-_{k_{1}}^{-})/2\) yields analogous properties. Detailed definitions are provided in Appendix I. The following proposition explores the model's ability to handle OOD unseen ICL tasks.

**Proposition 1**.: _Out-of-Distribution-Generalization3. During testing, the learned model admits probability distribution shift on \(^{*}_{}\) and data shift on \(^{*}_{}^{*}_{}\) to generate a new prompt distribution \(^{*}_{S}=_{k=1}^{K_{1}}({_{k}^{+*}}^{}\!\!+_{L,L^ {*}+1}^{}\!\!+_{1}^{}\!\!+_{1}^{}\!\!-_{k,L^{ *}+1}^{}\!\!\!)\). Specifically, the new \(^{*}_{S}\) satisfies the following properties._

* _The prompt length_ \(L^{*}\) _can be any positive integer._
* \(^{*}_{}\) _can enjoy arbitrary distribution, satisfying that each prompt has at least one co-concept_ \(k[K_{1}]\)_, at least one pair shares the query word's co-concept's label, and still each word has equal chance to have positive or negative semantic labels over its concepts_4_. * \(^{*}_{}^{*}_{}\) _can enjoy a great family of data shift._ \( k k^{}[K_{1}],k_{2}[K_{2}]\)_, we can have new_ \(^{*}\) _and_ \(^{*}\) _such that_ \(_{k}^{+*}=_{k}^{*}_{k}^{*}\)_,_ \(_{k}^{+*}=_{k}^{*}_{k}^{*}\)_,_ \(_{k_{2}}=_{k_{2}}^{*}\)_. Here,_ \(_{k}^{*},_{k}^{*},_{k}^{*},_{k}^{*}\) _are any vectors belong to the conic hulls of_ \(\{_{k}\}_{k=1}^{K_{1}},\{_{k}\}_{k=1}^{K_{1}},\{_{k}\}_{k=1}^ {K_{1}},\{_{k}\}_{k=1}^{K_{1}}\) _respectively, satisfying_ \(\|_{k}^{*}\|\|_{k}^{*}\|=(\|\|)\) _and_ \(\|_{k}^{*}\|\|_{k}^{*}\|=(\|\|)\)_._ \(_{k_{2}}^{*}=(\|\|)\) _are any vectors from the complement space of_ \(()\)_._

_Again, the learned model satisfies \(L_{^{*}_{S}}^{0-1}(^{(T^{*})})\)._

This proposition demonstrates the strong Out-of-Distribution Generalization ability of transformer utilizing multi-concept semantics, suggesting the efficiency transformer to conduct unseen ICL tasks just by its learned "Knowledge" on the high-level concept and low-level label semantic information from the two non-orthogonal dictionaries. The admit of shift for \(^{*}_{}\) denotes that each prompt can enjoy multi-co-concepts and each word-label pair can appear in at least \(\|\|_{0}\) concept-specific prompts/tasks' distribution, which aligns the real-world cases. On the other hand, we also believe the admit of shift for \(^{*}_{}^{*}_{}\) is inspiring, suggesting that transformer can conduct specific cross-concept semantic "Knowledge Intersection". As such, this lemma suggest that the transformer can master the regularity of unseen ICL tasks' "structure" in the presence the multi-concept encoded representation.

**Remark 1**.: _Comparison with Related Work. Theorem 3.4 in  and Theorem 2 in  address the transformer's OOD capability in specific structured ICL classification and regression tasks. Our results differ by focusing on compositional generalization of learned concepts, grounded in the concept-specific linear latent geometry observed in LLMs._

## 5 Proof Idea

In a big picture, we simply extend standard expectation-variance reduction techniques  to our setting. Section 5.1 defines coefficients to examine NN's expected projection along feature directions. Section 5.2 provides the convergence of the expected estimator through the lens of coefficient evolution; Section 5.3 showcase the exponential convergence by treating the conditional expectations of the NNs as Doob martingales and exploiting the property of the tails under low-noise conditions.

### Idempotent Operator Techniques

**Idempotent Operator Trick**. Define \(()\) and its complement space \(^{}\). By definition, we know that \(()=K\) and \((^{})=d_{}-K\). Then we can let \(\{\{_{k_{1}}\}_{k_{1}=1}^{K_{1}},\{_{k_{1}}\}_{k_{1}=1}^{K_{1}}, \{_{k_{2}}\}_{k_{2}=1}^{K_{2}},\{_{w}\}_{w=1}^{d_{}- K}\}\) be the set of standard orthogonal basis for \(^{d_{}}\), where \(_{1}^{},,_{d_{}-K}^{}\) are the standard orthogonal basis of \(^{}\).

Then we can derive an idempotent decomposition of the identity matrix

\[_{s=1}^{K_{1}}_{s}_{s}^{}}{\|_{s}\|^{2}}+_{ s=1}^{K_{1}}_{s}_{s}^{}}{\|_{s}\|^{2}}+_{r=1}^{K_{2}} _{r}_{r}^{}}{\|\|^{2}}+_{w=1}^{d_{ }-K}_{w}^{}_{w}^{}_{w}^{}= _{d_{} d_{}}.\] (3)

Similar techniques are also applied to the label's dictionary: \(()\), where we define \(_{1}^{},,_{d_{}-K_{1}}^{}\) as the standard orthogonal basis of the complement space \(^{}\). In our subsequent derivation, the expectation \([]\) is taken over the stochastic gradient descent. Similar to the idea in , we first serve to see how \((^{(t)})\) evolves. For \((^{(t)})\), every gradient descent update by all concept's samples within a soft "weight", and thus the analysis is equivalent to gradient descent with an ideally-balanced prompt set. Leveraging the symmetry of the prompt distribution, as well as the symmetry of \(_{Q}^{(0)}\) and \(_{K}^{(0)}\), we introduce the following decompositions.

**Lemma 1**.: _We can decompose \([_{Q}^{}]\), \([_{K}^{}]\) and the \(i\)-th row of \([_{Q}^{}]\) (\(i[m]\)) via the following (scaled) projection matrices and projection directions._

\[[_{Q}^{}{}^{(t)}] =_{s=1}^{K_{1}}_{Q_{s},s}^{(t)}_{s} {a}_{s}}{\|_{s}\|^{4}}+_{s=1}^{K_{2}}_{Q_{s},s}^{(t)}_{s}_{s}}{\|_{s}\|^{4}}+_{r=1}^{K_{2}}_{Q_{r},r}^{(t) }_{r}_{r}_{r}}{\|\|^{4}}+_{w =1}^{d_{X}-K}_{Q,w}^{(t)}_{w}^{}_{w}^{},\] \[[_{K}^{}{}^{(t)}] =_{s=1}^{K_{1}}_{K_{s},s}^{(t)}_{s} {a}_{s}}{\|_{s}\|^{4}}+_{s=1}^{K_{1}}_{K_{s},s}^{(t)}_{s}_{s}}{\|_{s}\|^{4}}+_{r=1}^{K_{2}}_{K_{r},r}^{(t )}_{r}_{r}}{\|\|^{4}}+_{w=1}^{d_{X}-K }_{K,w}^{(t)}_{w}^{}_{w}^{},\] \[[_{O_{(i.,.)}}^{(t)}] =_{k=1}^{K_{1}}_{O_{(i.,.)},k}^{(t)}_{k}^{}}{\|_{k}\|^{2}}+_{k=1}^{K_{1}}_{O_{(i.,.)},k}^{(t )}_{k}^{}}{\|_{k}\|^{2}}+_{w=1}^{d_{ y}-K_{1}}_{O_{(i.,.)},w}^{(t)}_{w}^{}.\]

Here \(_{Q,s}^{(t)}\), \(_{K,s}^{(t)}\) and \(_{O_{(i.,.)},k}^{(t)}\) represent the expected concept learning process, \(_{Q,s}^{(t)}\), \(_{K,s}^{(t)}\) and \(_{O_{(i.,.)},k}^{(t)}\) represent the expected concept-specific semantic learning process and \(_{Q_{r},r}^{(t)},_{K_{r},r}^{(t)},_{Q,w}^{(t)},_{K,w}^{(t)}\) and \(_{O_{(i.,.)},w}^{(t)}\) represent the expected memorization of the concept irrelevant noise. It holds that

\[[(_{K}^{}{}^{(t)}_{s}^{})] ^{}[_{Q}^{}{}^{(t)}_{s}^{}]= _{Q_{s},s}^{(t)}_{K,s}^{(t)}/\|_{s}\|^{2}_{Q_{s},s}^{(t)}_{K,s}^{(t)}/\|_{s}\|^{2},\] (4) \[[_{O_{(i.,.)}}^{(t)}_{k}^{ }]=_{O_{(i.,.)},k}^{(t)}+e_{O_{(i.,.)},k}^{(t)},\]

for \( e[],i[m],k[K_{1}]\) and for \( e^{}[],s^{}[K_{1}],r[K_{2}],w[d_{}-K]\), \(\{_{s^{}}^{_{r}},_{r}, {u}_{w}^{}\}\), it holds that \([(_{K}^{}{}^{(t)})]^{}[ _{Q}^{}{}^{(t)}_{s}^{_{s}}]=0\). Similar conclusions hold when the query vectors are \(_{r}\) and \(_{w}^{}\), \( r[K_{2}],w[d_{}-K]\). As such, our remaining task is to scrutinize the coefficients evolution, which would be the key contributors to the expected 0-1 loss convergence.

### Convergence of the Expectation

Denote \(_{k,n}^{y_{S_{n}}}(t)\) and \(_{k,n}^{v}(t)-_{k,n}^{y_{S_{n}}}(t)\) as the activated neuron set for \(\{i[m]_{i}y_{S_{n}}>0\}\) and \(\{i[m]_{i}y_{S_{n}}<0\}\) separately, and \(_{l S_{n,k}^{y_{S_{n}}}}(_{S}^{(t)})_{l}^{n}\) represents the correct attention weight, where the detailed definitions are delayed in Appendix E. We then introduce the following lemma.

**Lemma 2**.: _Under Condition 1, when_

\[(_{i_{k,n}^{y_{S_{n}}}(t)}-_{i_{k,n}^{y_{S_{ n}}}(t)-_{k,n}^{y_{S_{n}}}(t)})(_{O_{(i.,.)},k}^{(t)}+(2_{l S_{n,k}^{y_{S_{ n}}}}(_{S}^{(t)})_{l}^{n}-1)y_{S_{n}}_{O_{(i.,.)},k}^{(t)} ) 0,\] (5)

_holds, we have \(L_{^{}}^{0-1}((^{}{}^{(t)}))=0\)._

Figure 1: Illustration of our Idempotent Operator Techniques. This allows us to focus on analyzing the evolving coefficients, which are key to the expected 0-1 loss convergence.

As such, the following lemmas show the learning outcomes of the \((^{(t)})\) along the iterations.

**Lemma 3**.: _(Convergence of the Expectation). There exist constant \(C_{1}>0\), \( t=C_{1}_{1}m K_{1}}) (5Km/)}/{w^{*}}^{2}(1-_{})\|\|\), we have \(L^{0-1}_{^{*}}((^{(t)}))=0\)._

**Lemma 4**.: _(Regularizing the models). Under Condition 1, it holds that_

\[^{(T^{*})}_{Q,k}=^{(T^{*})}_{K,k}=O([ ^{(0)}_{Q,k}]),^{(T^{*})}_{Q,k}=^{(T^{*})}_{K,k}=(\| \|\|^{2}}{ K_{1}}(\|^{2}}{m K_{1}}))}),\] \[^{(T^{*})}_{O_{(i,)},k}|^{(T^{*})}_{O_{(i, )},k}|=((\|^{2}}{m K_{1}})),[(_{j S^{W^{_{n}}}_{n,k}}{(^{(T^{*})}_{S})}_{j}^{n})]= (}{\|\|^{2}}(}{\|\|^{2}})}).\]

In addition, our analysis provides three asymptotic properties of the coefficients evolution, which are delayed to Appendix I.1.3 and I.2 for room limitation.

### Exponential Convergence of 0-1 loss

**Proposition 2**.: \( t\)_, when \(\|{^{}}^{(t)}-(^{})\|_{F}\) holds, we have \(L^{0-1}_{^{*}}({^{}}^{(t)})=0\). Here, \(\|^{}\|_{F}^{2}\|^{}_{Q}\|_{F}^{2}+\| ^{}_{K}\|_{F}^{2}+\|^{}_{O}\|_{F}^{2}\)._

By definition of 0-1 loss, then we only need to prove the 0-1 loss convergence by seeing the speed of \({^{}}^{(t)}\) converging to \(({^{}}^{(t)})\) with an error of \(\) in terms of \(\|\|_{F}\).

Drawing insights from , we see \(_{0},,_{T-1}\) as a i.i.d. random variables following the same distribution. Then \( t\{0,,T\}\), it holds that

\[& D_{Q}^{t}=[^{}_{Q}{(T^{ *})}_{Q}_{0},,_{t}]-[^{ {x}}_{Q}{(T^{*})}_{Q}_{0},,_{t-1}],\\ & D_{K}^{t}=[^{}_{K}{(T^{*})}_{1} _{0},,_{t}]-[^{}_{K}{(T^ {*})}_{K}{(T^{*})}_{1}_{0},,_{t-1}]\\ & D_{O}^{t}=[^{}_{O}{(T^{*})}_{1} _{0},,_{t}]-[^{}_{O}{(T^ {*})}_{O}_{0},,_{t-1}],\] (6)

are martingale difference sequences, and for \( X\{Q,K,O\}\) and its corresponding \(\{^{}_{Q},^{}_{K},^{ {y}}_{O}\}\), we have \(_{t=0}^{T}D_{X}^{t}=^{(T+1)}-[^{(T+1)}]\). Then we utilize the following lemma in  to give a bound over the variance.

**Lemma 5**.: _Let \(D_{1},,D_{T-1}\) be a martingale difference sequence. Suppose \( c_{T}>0\) such that \(_{t=0}^{T}\|D_{t}\|_{}^{2} c_{T}^{2}\), where \(\|\|_{}\) is the essential supremum of \(\|\|_{F}\). Then for \(>0\), we have_

\[_{s[T]}\|_{t=0}^{s}D_{t}\|_{F}  2(-}{2c_{T}^{2}}).\]

Therefore, we need to see if there exists a decaying positive constant \(c_{T}\) (with decaying rate \(O(1/T^{q}),q>0\)), such that \(_{t=0}^{T}\|D_{X}^{t}\|_{}^{2}}^{2}, X\{Q,K,O\}\), where \(\|\|_{}\) is the essential supremum of \(\|D_{X}^{t}\|_{F}\). Subsequently, by controlling the martingale sequence norm tail similarly in [34; 55], we can obtain an exponential convergence rate after \(T_{1}\).

For \(\{_{Q}^{},_{K}^{},_{Q }^{}\}\), to check the decaying \(c_{T}\), we adopt the techniques of [34; 33; 36] in the following manner. Let \(_{t}{}^{}\) be an independent variable from \(_{0},,_{T}\) and let \(_{t}{}^{(T+1)}\) be an output of the algorithm depending on \((_{0},,_{t-1},_{t}{}^{},_{t+1},,_{T})\). Then we have

\[\|D_{X}^{t}\|_{}[\|^{(T+1)}-_{t}{}^{(T +1)}\|_{}_{0},,_{t}].\]

Therefore, one may estimate \(c_{X}^{T}{}^{2}\) by bounding \(\|^{(T+1)}-_{t}{}^{(T)}\|_{}^{2}\) uniformly w.r.t. \(_{0},,_{T-1}\). Such a bound can be derived utilizing stability property of stochastic gradient descent [34; 56]. For the OOD scenario, since we require the data shift to be via conic combination, the new words and labels in each prompt will share the positive/negative real-valued label without any self-conflict. The norm requirements and constraints on \(_{}^{*}\) would ensure the Gaussian noise, concepts other than the co-concepts, and probability shifts have limited influence on the prediction compared with the considerable scale of coefficients by Lemma 4, laying the groundwork for the proof.

## 6 Experiments

In this section, we demonstrate the validity of our theoretical analysis through simulations of Algorithm 1. We use the following parameter settings in Figure 2: The parameter settings are: the length \(L=4\), the number of co-concepts \(K_{1}=2\), dictionary size \(K=104\), the number of test instances \(n_{}=5000\), dimension \(d_{}=d_{}=1000\), MLP width \(m=50\), feature strengths \(\|\|=\|\|=100\), \( k[K_{1}]\), the cosine \(_{k}^{+},_{k}^{-}/\|\|^{2}= _{k}^{+},_{k}^{-}/\|\|^{2}=0.5\), the initialization parameters \(_{0}=0.1\), \(_{1}=0.01\), and the noise deviation \(_{}=0.01\). For the optimization, we use \(=0.002\), \(B=16\), \(=10000\), and the total training epochs is \(100\). Figure 3 (a-d) uses the same training settings, but during testing, it applies different configurations: (a) \(L^{*}=5\), (b) \(L^{*}=2\), (c) a \(0.8\) fraction for the first concept and a \(0.2\) fraction for the second concepts, and (d) \(_{1}^{+*}=_{1}_{2},_{2}^{^{*}}=_{2} _{1}\). Figure 2 validates our Theorem 2 and Lemma 4, which showcases the fast convergence rate and the evolution of coefficients. Figure 3 validates Proposition 1, where the learned model permits certain data shifts.

## 7 Conclusion

This work provides the first exponential convergence analysis of 0-1 loss for transformers with softmax attention and ReLU-MLP, trained on a non-orthogonal concept-specific prompt distribution by practical cross-entropy loss. Furthermore, the results demonstrate transformers can perform certain OOD ICL tasks by leveraging the multi-concept semantic linearity, highlighting their innovative potential. An important future direction is to extend the analysis to more complex scenarios.

Figure 3: Learning dynamic in three OOD scenarios. The training settings and plotting methods are identical to those used in Figure 2, and the testing settings are: (a-b) utilizes different prompt lengths; (c) adopts a skewed distribution over \(\); (d) switches the concept-specific semantic features.

Acknowledgment

We thank the anonymous reviewers for their instrumental comments. D.B. and H.W. are supported in part by the Research Grants Council of the Hong Kong Special Administration Region (Project No. CityU 11206622). W.H. is supported in part by JSPS KAKENHI (24K20848). A.N. is supported in part by National Research Foundation, Singapore and Infocomm Media Development Authority under its Trust Tech Funding Initiative, the Centre for Frontier Artificial Intelligence Research, Institute of High Performance Computing, A*Star, and the College of Computing and Data Science at Nanyang Technological University. T.S. is supported in part by JSPS KAKENHI (24K02905) and JST CREST (JPMJCR2115, JPMJCR2015).