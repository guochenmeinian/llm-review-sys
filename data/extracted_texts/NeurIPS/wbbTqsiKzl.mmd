# High-dimensional Asymptotics of

Denoising Autoencoders

 Hugo Cui

Statistical Physics of Computation Lab

Department of Physics

EPFL, Lausanne, Switzerland

hugo.cui@epfl.ch

&Lenka Zdeborova

Statistical Physics of Computation Lab

Department of Physics

EPFL, Lausanne, Switzerland

###### Abstract

We address the problem of denoising data from a Gaussian mixture using a two-layer non-linear autoencoder with tied weights and a skip connection. We consider the high-dimensional limit where the number of training samples and the input dimension jointly tend to infinity while the number of hidden units remains bounded. We provide closed-form expressions for the denoising mean-squared test error. Building on this result, we quantitatively characterize the advantage of the considered architecture over the autoencoder without the skip connection that relates closely to principal component analysis. We further show that our results accurately capture the learning curves on a range of real data sets.

## 1 Introduction

Machine learning techniques have a long history of success in denoising tasks. The recent breakthrough of diffusion-based generation [1; 2] has further revived the interest in denoising networks, demonstrating how they can also be leveraged, beyond denoising, for generative tasks. However, this rapidly expanding range of applications stands in sharp contrast to the relatively scarce theoretical understanding of denoising neural networks, even for the simplest instance thereof - namely Denoising Auto Encoders (DAEs) .

Theoretical studies of autoencoders have hitherto almost exclusively focused on data compression tasks using Reconstruction Auto Encoders (RAEs), where the goal is to learn a concise latent representation of the data. A majority of this body of work addresses _linear_ autoencoders [4; 5; 6; 7]. The authors of [8; 9] analyze the gradient-based training of non-linear autoencoders with online stochastic gradient descent or in population, thus implicitly assuming the availability of an infinite number of training samples. Furthermore, two-layer RAEs were shown to learn to essentially perform Principal Component Analysis (PCA) [10; 11; 12], i.e. to learn a linear model. Ref.  shows that this is also true for infinite-width architectures. Learning in DAEs has been the object of theoretical investigations only in the linear case , while the case of non-linear DAEs remains theoretically largely unexplored.

Main contributions-The present work considers the problem of denoising data sampled from a Gaussian mixture by learning a two-layer DAE with a skip connection and tied weights via empirical risk minimization. Throughout the manuscript, we consider the high-dimensional limit where the number of training samples \(n\) and the dimension \(d\) are large (\(n,d\)) while remaining comparable, i.e. \(}{{d}}=(1)\). Our main contributions are:

* Leveraging the replica method, we provide sharp, closed-form formulae for the mean squared denoising test error (MSE) for DAEs, as a function of the sample complexity \(\) andthe problem parameters. We also provide a sharp characterization for other learning metrics including the weights norms, skip connection strength, and cosine similarity between the weights and the cluster means. These formulae encompass as a corollary the case of RAEs. We show that these formulae also describe quantitatively rather well the denoising MSE for _real_ data sets, including MNIST  and FashionMNIST .
* We find that PCA denoising (namely denoising by projecting the noisy data along the principal component of the training samples) is widely sub-optimal compared to the DAE, leading to a MSE superior by a difference of \((d)\), thereby establishing that DAEs do _not_ simply learn to perform PCA.
* Building on the formulae, we quantify the role of each component of the DAE architecture (skip connection and the bottleneck network) in its overall performance. We find that the two components have complementary effects in the denoising process -namely preserving the data nuances and removing the noise- and discuss how the training of the DAE results from a tradeoff between these effects.

The code used in the present manuscript can be found in the following repository.

Related works_Theory of autoencoders_- Various aspects of RAEs have been studied, for example, memorization , or latent space alignment . However, the largest body of work has been dedicated to the analysis of gradient-based algorithms when training RAEs. Ref.  established that minimizing the training loss leads to learning the principal components of the data. Authors of [11; 12] have analyzed how a linear RAE learns these components during training. These studies were later extended to non-linear networks by [19; 8; 9], at the sacrifice of further assuming an infinite number of training samples to be available -either by considering online stochastic gradient descent, or the population loss. Refs. [20; 13] are able to address a finite sample complexity, but in exchange, have to consider infinite-width architectures, which  further shows, also tend to a large extent to learn to perform PCA.

_Exact asymptotics from the replica method_- The replica method [21; 22; 23; 24] has proven a very valuable gateway to access sharp asymptotic characterizations of learning metrics for high-dimensional machine learning problems. Past works have addressed -among others- single-[25; 26; 27; 28] and multi-index models , or kernel methods [30; 31; 32; 33]. While the approach has traditionally addressed convex problems, for which its prediction can be proven e.g. using the convex Gordon minimax theorem , the replica method allows to average over _all_ the global minimizers of the loss, and therefore also accommodates non-convex settings. Refs. [35; 36] are two recent examples of its application to non-convex losses. In the present manuscript, we leverage this versatility to study the minimization of the empirical risk of DAEs, whose non-convexity represents a considerable hurdle to many other types of analyses.

## 2 Setting

Data modelWe consider the problem of denoising data \(^{d}\) corrupted by Gaussian white noise of variance \(\),

\[}=+,\]

where we denoted \(}\) the noisy data point, and \((0,_{d})\) the additive noise. The rescaling of the clean data point by a factor \(\) is a practical choice that entails no loss of generality, and allows to easily interpolate between the noiseless case (\(=0\)) and the case where the signal-to-noise ratio vanishes (\(=1\)). Furthermore, it allows us to seamlessly connect with works on diffusion-based generative models, where the rescaling naturally follows from the way the data is corrupted by an Ornstein-Uhlenbeck process [1; 2]. In the present work, we assume the clean data \(\) to be drawn from a Gaussian mixture distribution \(\) with \(K\) clusters

\[_{k=1}^{K}_{k}(_{k},_{k}).\] (1)

The \(k-\)th cluster is thus centered around \(_{k}^{d}\), has covariance \(_{k} 0\), and relative weight \(_{k}\).

DAE modelAn algorithmic way to retrieve the clean data \(\) from the noisy data \(}\) is to build a neural network taking the latter as an input and yielding the former as an output. A particularly natural choice for such a network is an autoencoder architecture . The intuition is that the narrow hidden layer of an autoencoder forces the network to learn a succinct latent representation of the data, which is robust against noise corruption of the input. In this work, we analyze a two-layer DAE. We further assume that the weights are tied. Additionally, mirroring modern denoising architectures like U-nets  or , we also allow for a (trainable) skip-connection:

\[f_{b,}(})=b}+^{}}{}(}}{}).\] (2)

The DAE (2) is therefore parametrized by the scalar skip connection strength \(b\) and the weights \(^{p d}\), with \(p\) the width of the DAE hidden layer. The normalization of the weight \(\) by \(\) in (2) is the natural choice which ensures for high dimensional settings \(d 1\) that the argument \(}/\) of the non-linearity \(()\) stays \((1)\). Like , we focus on the case with \(p d\). The assumption of weight-tying affords a more concise theoretical characterization and thus clearer discussions. Note that it is also a strategy with substantial practical history, dating back to , as it prevents the DAE from functioning in the linear region of its non-linearity \(()\). This choice of architecture is also motivated by a particular case of Tweedie's formula  (see eq. (79) in Appendix B), which will be the object of further discussion in Section 4.

We also consider two other simple architectures

\[u_{}(}) =^{}}{}(}}{}), r_{c}(}) =c},\] (3)

which correspond to the building blocks of the complete DAE architecture \(f_{b,}\) (2) (hereafter referred to as the _full_ DAE). Note that indeed \(f_{b,}=r_{b}+u_{}\). The part \(u_{}()\) is a DAE without skip connection (hereafter called the _bottleneck network_ component), while \(r_{c}()\) correspond to a simple single-parameter trainable rescaling of the input (hereafter called the _rescaling_ component).

To train the DAE (2), we assume the availability of a training set \(=\{}^{},^{}\}_{=1}^{n}\), with \(n\) clean samples \(^{}\) drawn i.i.d from \(\) (1) and the corresponding noisy samples \(}^{}=^{}+^{}\) (with the noises \(^{}\) assumed mutually independent). The DAE is trained to recover the clean samples \(^{}\) from the noisy samples \(}^{}\) by minimizing the empirical risk

\[}(b,)=_{=1}^{n}\|^{}-f_{b,}( }^{})\|^{2}+g(),\] (4)

where \(g:^{p d}_{+}\) is an arbitrary convex regularizing function. We denote by \(,}\) the minimizers of the empirical risk (4) and by \( f_{,}}\) the corresponding trained DAE (2). For future discussion, we also consider training independently the components (3) via empirical risk minimization, by which we mean replacing \(f_{b,}\) by \(u_{}\) or \(r_{c}\) in (4). We similarly denote \(}\) (resp. \(\)) the learnt weight of the bottleneck network (resp. rescaling) component and \( u_{}}\) (resp. \( r_{}\)). Note that generically, \(}}\) and \(\), and therefore \(+\), since \(,}\) result from their joint optimization as parts of the full DAE \(f_{b,}\), while \(\) (or \(}\)) are optimized independently. As we discuss in Section 4, training the sole rescaling \(r_{c}\) does not afford an expressive enough denoiser, while an independently learnt bottleneck network component \(u_{}\) essentially only learns to implement PCA. However, when _jointly_ trained as components of the full DAE \(f_{b,}\) (2), the resulting denoiser \(\) is a genuinely non-linear model which yields a much lower test error than PCA, and learns to leverage flexibly its two components to balance the preservation of the data nuances and the removal of the noise.

Learning metricsThe performance of the DAE (2) trained with the loss (4) is quantified by its reconstruction (denoising) test MSE, defined as

\[_{}_{}_{ }_{(0,_{d})}\| -f_{,}}(+)\|^{2}.\] (5)

The expectations run over a fresh test sample \(\) sampled from the Gaussian mixture \(\) (1), and a new additive noise \(\) corrupting it. Note that an expectation over the train set \(\) is also included to make \(_{}\) a metric that does not depend on the particular realization of the train set. The denoisingtest MSEs \(_{},_{}\) are defined similarly as the denoising test errors of the independently learnt components (3). Aside from the denoising MSE (5), another question of interest is how much the DAE manages to learn the structure of the data distribution, as described by the cluster means \(_{k}\). This is measured by the cosine similarity matrix \(^{p K}\), where for \(i[\![1,p]\!]\) and \(k[\![1,K]\!]\),

\[_{ik}_{}[}_{i} ^{}_{k}}{\|}_{i}\|\|_{k }\|}].\] (6)

In other words, \(_{ik}\) measures the alignment of the \(i-\)th row \(}_{i}\) of the trained weight matrix \(}\) with the mean of the \(k-\)th cluster \(_{k}\).

High-dimensional limitWe analyze the optimization problem (4) in the high-dimensional limit where the input dimension \(d\) and number of training samples \(n\) jointly tend to infinity, while their ratio \(=}{{d}}-\)thitherto referred to as the _sample complexity_-stays \((1)\). The hidden layer width \(p\), the noise level \(\), the number of clusters \(K\) and the norm of the cluster means \(\|_{k}\|\) are also assumed to remain \((1)\). This corresponds to a rich limit, where the number of parameters of the DAE is not large compared to the number of samples like in [20; 13], and therefore cannot trivially fit the train set, or simply memorize it . Conversely, the number of samples \(n\) is not infinite like in [8; 9; 19], and therefore importantly allows to study the effect of a finite train set on the representation learnt by the DAE.

## 3 Asymptotic formulae for DAEs

We now state the main result of the present work, namely the closed-form asymptotic formulae for the learning metrics \(_{f}\) (5) and \(\) (6) for a DAE (2) learnt with the empirical loss (4). These characterizations are obtained by first recasting the optimization problem into an analysis of an associated probability measure, and then carrying out this analysis using the heuristic replica method, which we here employ in its replica-symmetric formulation (see Appendix A).

**Assumption 3.1**.: The covariances \(\{\}_{k=1}^{K}\) admit a common set of eigenvectors \(\{_{i}\}_{i=1}^{d}\). We further note \(\{_{i}^{k}\}_{i=1}^{d}\) the eigenvalues of \(_{k}\). The eigenvalues \(\{_{i}^{k}\}_{i=1}^{d}\) and the projection of the cluster means on the eigenvectors \(\{_{i}^{}_{k}\}_{i,k}\) are assumed to admit a well-defined joint distribution \(\) as \(d\) - namely, for \(=(_{1},...,_{K})^{K}\) and \(=(_{1},...,_{K})^{K}\):

\[_{i=1}^{d}_{k=1}^{K}(_{i}^{k}-_{k })(_{i}^{}_{k}- _{k})(,).\] (7)

Moreover, the marginals \(_{}\) (resp. \(_{}\)) are assumed to have a well-defined first (resp. second) moment.

**Assumption 3.2**.: \(g()\) is a \(_{2}\) regularizer with strength \(\), i.e. \(g()=}{{2}}\|\|_{F}^{2}\).

We are now in a position to discuss the main result of this manuscript, which we state under Assumptions 3.1 and 3.2 for definiteness and clarity. These assumptions can actually be relaxed, as we further discuss after the result statement.

**Conjecture 3.3**.: _(**Closed-form asymptotics for DAEs trained with empirical risk minimization**) Under Assumptions 3.1 and 3.2, in the high-dimensional limit \(n,d\) with fixed ratio \(\), the denoising test MSE \(_{}\) (5) admits the expression_

\[_{}-_{}=_{k=1}^{K}_{k }_{z}\![q(m_{k}+}z}{ }z}})^{ 2}]\] (8) \[-2_{k=1}^{K}_{k}_{u,v}[( m_{k}+(1-)}u+v}}{{ \|}_{i}\|\|_{k}\|}})^{} ((1-}}{{}})(m_{k}+}u}}{{v}})-v}}{{ v}})]+o(1),\]

_where the averages bear over independent Gaussian variables \(z,u,v(0,_{p})\). We denoted_

\[_{}=d^{2}+(1-)^{ 2}[_{k=1}^{K}_{k}( d_{}()_{k}^{2}+d d _{}()_{k})].\] (9)_The learnt skip connection strength \(\) is_

\[=^{K}_{k} d_{}() _{k})}{(_{k=1}^{K}_{k} d_{ }()_{k})(1-)+}+o(1).\] (10)

_The cosine similarity \(\) (6) admits the compact formula for \(i 1,p\) and \(k 1,K\)_

\[_{ik}=)_{i}}{ d_{}()_{k}^{2}}},\] (11)

_where we have introduced the summary statistics_

\[q=_{d}_{}[} }^{}}{d}], q_{k}=_{d} _{}[}_{k}}^{}}{d}], m_{k}=_{d}_{ }[}_{k}}{} ].\] (12)

_Thus \(q,q_{k}^{p p},\ m_{k}^{p}\). The existence of these limits is an assumption of the replica method. The summary statistics \(q,q_{k},m_{k}\) can be determined as solutions of the system of equations_

\[_{k}=_{k}_{,}V_{k}^{-1}( _{y}^{k}-q_{k}^{}-m_{k})^{ 2}V_{k}^ {-1}\\ _{k}=-_{k}q_{k}^{-}_{,}V_{k}^{-1} (_{y}^{k}-q_{k}^{}-m_{k})^{ }\\ _{k}=_{k}_{,}V_{k}^{-1}(_{y}^{k}-q_{k}^{}-m_{k})\\ =_{k=1}^{K}_{k}_{, }V^{-1}(_{x}^{k}-q^{} )^{ 2}V^{-1}\\ =-_{k=1}^{K}_{k}_{,} }q^{-}V^{-1}(_{x}^{k }-q^{})^{}-(_{y}^{k}+_{x}^{k})^{ 2} \]

\[q_{k}= d(,)_{k}(_{p} ++_{j=1}^{K}_{j}_{j})^{-2}(+ _{j=1}^{K}_{j}_{j}+_{1 j,l K}_ {j}_{l}_{j}_{l}^{})\\ V_{k}= d(,)_{k}(_{p}++ _{j=1}^{K}_{j}_{j})^{-1}\\ m_{k}= d(,)_{k}(_{p}++ _{j=1}^{K}_{j}_{j})^{-1}_{j=1}^{K} _{j}_{j}\\ q= d(,)(_{p}++ _{j=1}^{K}_{j}_{j})^{-2}\!\!(+ _{j=1}^{K}_{j}_{j}+_{1 j,l K}_{j} _{l}_{j}_{l}^{})\\ V= d(,)(_{p}++ _{j=1}^{K}_{j}_{j})^{-1}\] (13)

_In (13), \(_{k},_{k},,V^{p p}\) and \(_{k}^{p}\), and the averages bear over finite-dimensional i.i.d Gaussians \(,(0,_{p})\). Finally, \(_{x}^{k},_{y}^{k}\) are given as the solutions of the finite-dimensional optimization_

\[_{x}^{k}, _{y}^{k}=*{arginf}_{x,y ^{p}}\![V_{k}^{-1}(y-q_{k}^{ }-m_{k})^{ 2}]+\![V ^{-1}(x-q^{})^{ 2}]\] \[+\![q(y+x)^{ 2} ]-2(y+x)^{}((1-)y- x)}.\] (14)

Conjecture 3.3 provides a gateway to probe and characterize the asymptotic properties of the model (2) at the global optimum of the empirical risk (4), whereas a purely experimental study would not have been guaranteed to reach the global solution, and would suffer from finite-size effects. Equation(13) provides a closed-form expression for the summary statistics (12), in terms of the solutions of the low-dimensional optimization (14). The latter can be loosely viewed as an effective loss, subsuming the averaged effect of the finite training set. Further remark that while the regularization \(\) does not explicitly appear in the expression for the MSE (8), the statistics \(q_{k},m_{k}\) in (8) depend thereon through (13). In fact, Assumptions 3.1 and 3.2 are not strictly necessary, and can be simultaneously relaxed to address arbitrary convex regularizer \(g()\) and generically non-commuting \(\{_{k}\}_{k=1}^{K}\) - but at the price of more intricate formulae. For this reason, we choose to discuss here Conjecture 3.3, and defer a discussion and detailed derivation of the generic asymptotically exact formulae to Appendix A, see eq. (58). Let us mention that a sharp asymptotic characterization of the _train_ MSE can also be derived; for conciseness, we do not present it here and refer the interested reader to equation (68) in Appendix A. Conjecture 3.3 encompasses as special cases the asymptotic characterization of the components \(,\) (3):

**Corollary 3.4**.: _(**MSE of components**) The test MSE of \(\) (3) is given by \(_{}=_{}\) (9). Furthermore, the learnt value of its single parameter \(\) is given by (10). The test MSE, cosine similarity and summary statistics of the bottleneck network \(\) (3) follow from Conjecture 3.3 by setting \(=0\)._

The implications of Corollary 3.4 shall be further discussed in Section 4, and a full derivation is provided in Appendix E. Finally, remark that in the noiseless limit \(=0\), the denoising task reduces to a reconstruction task, with the autoencoder being tasked with reproducing the clean data as an output when taking the same clean sample as an input. Therefore Conjecture 3.3 also includes RAEs (by definition, without skip connection) as a special case.

**Corollary 3.5**.: _(**RAEs**) In the \(n,d\) limit, the MSE, cosine similarity and summary statistics for an RAE follow from Conjecture 3.3 by setting \(x=0\) in (14), removing the first term in the brackets in the equation of \(\) (13) and taking the limit \(,, 0\)._

Corollary 3.5 will be the object of further discussion in Section 4. A detailed derivation is presented in Appendix F. Note that Corollary 3.5 provides a characterization of RAEs as a function of the sample complexity \(\), where previous studies on non-linear RAEs rely on the assumption of an infinite number of available training samples .

Equations (10) and (12) of Conjecture 3.3 thus characterize the statistics of the learnt parameters \(,}\) of the trained DAE (2). These summary statistics are, in turn, sufficient to fully characterize the learning metrics (5) and (6) via equations (8) and (11). We thus have reduced the high-dimensional optimization (4) and the high-dimensional average over the train set \(\) involved in the definition of the metrics (5) and (6) to a simpler system of equations over \(4+6K\) variables (13) which can be solved numerically. It is important to note that all the summary statistics involved in (13) are _finite-dimensional_ as \(d\), and therefore Conjecture 3.3 is a fully asymptotic characterization, in the sense that it does not involve any high-dimensional object. Finally, let us stress once more that the replica method employed in the derivation of these results should be viewed as a strong heuristic, but does not constitute a rigorous proof. While Conjecture 3.3 is stated in full generality, we focus for definiteness in the rest of the manuscript on the simple case \(r=1,K=2\), which is found to already display all the interesting phenomenology discussed in this work, and leave the thorough exploration of \(r>1,K>2\) settings to future work. In the next paragraphs, we give two examples of applications of Conjecture 3.3, to a simple binary isotropic mixture, and to real data sets.

Example 1: Isotropic homoscedastic mixtureWe give as a first example the case of a synthetic binary Gaussian mixture with \(K=2,_{1}=-_{2},_{1,2}=0.09 _{d},_{1,2}=}{{2}}\), using a DAE with \(=\) and \(p=1\). Since this simple case exhibits the key phenomenology discussed in the present work, we refer to it in future discussions. The MSE \(_{}\) (8) evaluated from the solutions of the self-consistent equations (13) is plotted as the solid blue line in Fig. 1 (left) and compared to numerical simulations corresponding to training the DAE (2) with the Pytorch implementation of the Adam optimizer  (blue dots), for sample complexity \(=1\) and \(_{2}\) regularization (weight decay) \(=0.1\). The agreement between the theory and simulation is compelling. The green solid line and corresponding green dots in Fig. 1 (right) correspond to the replica prediction (11) and simulations for the cosine similarity \(\) (6), and again display very good agreement. Note that for large noise levels, the DAE achieves a worse MSE than the rescaling -as shown by the positive value of \(_{}-_{}\)-, despite the former being a priori expressive enough to realize the latter. This in fact signals that the DAE overfits the training data. That such an overfitting is captured is a strength of our analysis, which allows to cover the effect of a limited sample complexity. Finally, this overfitting can be mitigated by increasing the weight decay \(\), see Fig. 9 in Appendix A.

A particularly striking observation is that due to the non-convexity of the loss (4), there is a priori no guarantee that an Adam-optimized DAE should find a global minimum, as described by the Conjecture 3.3, rather than a local minimum. The compelling agreement between theory and simulations in Fig. 1 temptingly suggests that -at least in this case- the loss landscape of DAEs (2) trained with the loss (4) for the data model (1) should in some way be benign. Authors of  have shown, for _linear RAEs_, that there exists a unique global and local minimum for the square loss and no regularizer. Ref.  offers further insight for a linear DAE in dimension \(d=1\), and shows that, aside from the global minima, the loss landscape only includes an unstable saddle point from which the dynamics easily escapes. Extending these works and intuitions to non-linear DAEs is an exciting research topic for future work.

Example 2: MNIST, FashionMNISTIt is reasonable to ask whether Conjecture 3.3 is restricted to Gaussian mixtures (1). The answer is negative - in fact, Conjecture 3.3 also describes well a number of real data distributions. We provide such an example for FashionMNIST  (from which, for simplicity, we only kept boots and shoes) and MNIST  (1s and 7s), in Fig. 2. For each data set, samples sharing the same label were considered to belong to the same cluster. Note that we purposefully chose closely related classes, for which the clusters are expected to be closer, leading to an _a priori_ harder - and thus more interesting - learning problem. The mean and covariance thereof were estimated numerically, and combined with Conjecture 3.3. The resulting denoising MSE predictions \(_{}\) are plotted as solid lines in Fig. 2, and agree very well with numerical simulations of DAEs optimized over the real data sets using the Pytorch implementation of Adam . A full description of this experiment is given in Appendix D.

The observation that the MSEs of real data sets are to such degree of accuracy captured by the equivalent Gaussian mixture strongly hints at the presence of Gaussian universality . This opens a gateway to future research, as Gaussian universality has hitherto been exclusively addressed in classification and regression (rather than denoising) settings, see e.g. . Denoising tasks further constitute a particularly intriguing setting for universality results, as Gaussian universality would signify that only second-order statistics of the data can be reconstructed using a shallow autoencoder.

## 4 The role and importance of the skip connection.

Conjecture 3.3 for the full DAE \(\) (2) and Corollary 3.4 for its components \(,\ \) (3) allow to disentangle the contribution of each part, and thus to pinpoint their respective roles in the DAE architecture. We sequentially present a comparison of \(\) with \(\), and \(\) with \(\). We remind that \(\), \(\) and \(\) result from _independent_ optimizations over the same train set \(\), and that while \(f_{b,}=u_{}+h_{b}\), \(+\).

Full DAE and the rescaling componentWe start this section by observing that for noise levels \(\) below a certain threshold, the full DAE \(\) yields better MSE than the learnt rescaling \(\), as can be seen by the negative value of \(_{}-_{}\) in Fig. 1 and Fig. 2. The improvement is more sizeable at intermediate noise levels \(\), and is observed for a growing region of \(\) as the sample complexity \(\) increases, see Fig. 3 (a). This lower MSE further translates into visible qualitative changes in the result of denoising. As can be seen from Fig. 2 (left), the full DAE \(\) (2) (bottom left) yields denoised images with sensibly higher definition and overall contrast, while a simple rescaling \(\) (bottom right) leads to a still largely blurred image.

We provide one more comparison: for the isotropic binary mixture (see Fig. 1), the DAE test error \(_{}\) in fact approaches the information-theoretic lowest achievable MSE \(^{}\) as the sample complexity \(\) increases. To see this, note that \(^{}\) is given by the application of Tweedie's formula , that requires perfect knowledge of the cluster means \(_{k}\) and covariances \(_{k}\) - it is, therefore, an _oracle_ denoiser. A sharp asymptotic characterization of the oracle denoiser is provided in Appendix B. As can be observed from Fig. 3 (a), the DAE MSE (2) approaches -but does not exactly converges to (see Appendix C)- the oracle test error \(^{}\) as the number of available training samples \(n\) grows, and is already sensibly close to the optimal value for \(=8\).

DAEs with(out) skip connectionWe now turn our attention to comparing the full DAE \(\) (2) to the bottleneck network component \(\) (3). It follows from Conjecture 3.3 and Corollary 3.4 that \(\) (3) leads to a higher MSE than the full DAE \(\) (2), with the gap being \((d)\). More precisely,

\[(_{}-_{} )=()_{k=1}^{K}_{k} _{k})^{2}(1-)}{( d_{}()_ {k=1}^{K}_{k}_{k})(1-)+}.\] (15)

Figure 2: Difference in MSE between the full DAE (2) and the rescaling component (3) for the MNIST data set (middle), of which for simplicity only 1s and 7s were kept, and FashionMNIST (right), of which only boots and shoes were kept. In blue, the theoretical predictions resulting from using Conjecture 3.3 with the empirically estimated covariances and means, see Appendix D for further details. In red, numerical simulations of a DAE (\(p=1\), \(=\)) trained with \(n=784\) training points, using the Pytorch implementation of full-batch Adam, with learning rate \(=0.05\) and weight decay \(=0.1\) over \(2000\) epochs, averaged over \(N=10\) instances. Error bars represent one standard deviation. (left) illustration of the denoised images: (top left) original image, (top right) noisy image, (bottom left) DAE \(\) (2), (bottom right) rescaling \(\) (3).

The theoretical prediction (15) compares excellently with numerical simulations; see Fig. 3 (right). Strikingly, we find that PCA denoising yields an MSE almost indistinguishable from \(\), see Fig. 3, strongly suggesting that \(\) essentially learns, also in the denoising setting, to project the noisy data \(}\) along the principal components of the training set. The last two images of Fig. 4 respectively correspond to \(\) and PCA, which can indeed be observed to lead to visually near-identical results. This echoes the findings of  in the case of RAEs that bottleneck networks are limited by the PCA reconstruction performance - a conclusion that we also recover from Corollary 3.5, see Appendix F. Crucially however, it _also_ means that compared to the _full_ DAE \(\) (2), _PCA is sizeably suboptimal_, since \(_{}_{}=_{}+ (d)\).

This last observation has an important consequence: in contrast to previously studied RAEs , the full DAE \(\) does _not_ simply learn to perform PCA. In contrast to bottleneck RAE networks , the non-linear DAE hence does not reduce to a linear model after training. The non-linearity is important to improve the denoising MSE, see Fig. 1. We stress this finding: trained alone, the bottleneck network \(\) only learns to perform PCA; trained jointly with the rescaling component as part of the full DAE \(f_{b,}\) (2), it learns a richer, non-linear representation. The full DAE (2) thus offers a genuinely non-linear learning model and opens exciting research avenues for the theory of autoencoders, beyond linear (or effectively linear) cases. In the next paragraph, we explore further the interaction between the rescaling component and the bottleneck network.

A tradeoff between the rescaling and the bottleneck networkConjecture (3.3), alongside Corollary (3.4) and the discussion in Section 4 provide a firm theoretical basis for the well-known empirical intuition (discussed e.g. in ) that skip-connections allow to better propagate information

Figure 4: Illustration of the denoised image for the various networks and algorithms. (a) original image (b) noisy image, for \(=0.2\) (c) trained rescaling \(\) (3) (d) full DAE \(\) (2) (e) bottleneck network \(\) (3) (f) PCA. The DAE and training parameters are the same as Fig. 2, see also Appendix D.

Figure 3: (left) Solid lines: difference in MSE between the full DAE \(\) (2), with \(=\), \(p=1\), and the rescaling \(\) (3). Dashed: the same curve for the oracle denoiser. Different colours represent different sample complexities \(\) (solid lines). (right) Difference in MSE between the bottleneck network \(\) (3) and the complete DAE \(\) (2). In blue, the theoretical prediction (15); in red, numerical simulations for the bottleneck network (3) (\(=\), \(p=1\)) trained with the Pytorch implementation of full-batch Adam, with learning rate \(=0.05\) and weight decay \(=0.1\) over \(2000\) epochs, averaged over \(N=5\) instances, for \(d=700\). In green, the MSE (minus the MSE of the complete DAE (2)) achieved by PCA. Error bars represent one standard deviation. The model and parameters are the same as in Fig. 1.

[MISSING_PAGE_FAIL:10]

*  Pascal Vincent, H. Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. _J. Mach. Learn. Res._, 11:3371-3408, 2010.
*  Reza Oftadeh, Jiayi Shen, Zhangyang Wang, and Dylan A. Shell. Eliminating the invariance on the loss landscape of linear autoencoders. In _International Conference on Machine Learning_, 2020.
*  Daniel Kunin, Jonathan M. Bloom, Aleksandrina Goeva, and Cotton Seed. Loss landscapes of regularized linear autoencoders. In _International Conference on Machine Learning_, 2019.
*  Xuchan Bao, James Lucas, Sushant Sachdeva, and Roger B Grosse. Regularized linear autoencoders recover the principal components, eventually. _Advances in Neural Information Processing Systems_, 33:6971-6981, 2020.
*  Gauthier Gidel, Francis R. Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient dynamics in deep linear neural networks. In _Neural Information Processing Systems_, 2019.
*  Maria Refinetti and Sebastian Goldt. The dynamics of representation learning in shallow, nonlinear autoencoders. In _International Conference on Machine Learning_, pages 18499-18519. PMLR, 2022.
*  A. E. Shevchenko, Kevin Kogler, Hamed Hassani, and Marco Mondelli. Fundamental limits of two-layer autoencoders, and achieving them with gradient methods. _ArXiv_, abs/2212.13468, 2022.
*  Carl Eckart and G. Marion Young. The approximation of one matrix by another of lower rank. _Psychometrika_, 1:211-218, 1936.
*  Herve Bourlard and Yves Kamp. Auto-association by multilayer perceptrons and singular value decomposition. _Biological Cybernetics_, 59:291-294, 1988.
*  Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from examples without local minima. _Neural Networks_, 2:53-58, 1989.
*  Phan-Minh Nguyen. Analysis of feature learning in weight-tied autoencoders via the mean field lens. _ArXiv_, abs/2102.08373, 2021.
*  Arnu Pretorius, Steve Kroon, and Herman Kamper. Learning dynamics of linear denoising autoencoders. In _International Conference on Machine Learning_, 2018.
*  Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proc. IEEE_, 86:2278-2324, 1998.
*  Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. _ArXiv_, abs/1708.07747, 2017.
* 27170, 2019.
*  Saachi Jain, Adityanarayanan Radhakrishnan, and Caroline Uhler. A mechanism for producing aligned latent spaces with autoencoders. _ArXiv_, abs/2106.15456, 2021.
*  Thanh Van Nguyen, Raymond K. W. Wong, and Chinmay Hegde. On the dynamics of gradient descent for autoencoders. In _International Conference on Artificial Intelligence and Statistics_, 2019.
*  Thanh Van Nguyen, Raymond K. W. Wong, and Chinmay Hegde. Benefits of jointly training autoencoders: An improved neural tangent kernel analysis. _IEEE Transactions on Information Theory_, 67:4669-4692, 2019.
*  Giorgio Parisi. Towards a mean field theory for spin glasses. _Phys. Lett_, 73(A):203-205, 1979.

*  Giorgio Parisi. Order parameter for spin glasses. _Phys. Rev. Lett_, 50:1946-1948, 1983.
* 552, 2015.
*  Marylou Gabrie. Mean-field inference methods for neural networks. _Journal of Physics A: Mathematical and Theoretical_, 53, 2019.
*  Elizabeth Gardner and Bernard Derrida. Optimal storage properties of neural network models. _Journal of Physics A: Mathematical and general_, 21(1):271, 1988.
*  Manfred Opper and David Haussler. Generalization performance of bayes optimal classification algorithm for learning a perceptron. _Physical Review Letters_, 66(20):2677, 1991.
* 5460, 2017.
*  Benjamin Aubin, Florent Krzakala, Yue M. Lu, and Lenka Zdeborova. Generalization error in high-dimensional perceptrons: Approaching bayes error with convex optimization. _Advances in Neural Information Processing Systems_, 33:12199-12210, 2020.
*  Benjamin Aubin, Antoine Maillard, Jean Barbier, Florent Krzakala, Nicolas Macris, and Lenka Zdeborova. The committee machine: computational to statistical gaps in learning a two-layers neural network. _Journal of Statistical Mechanics: Theory and Experiment_, 2019, 2018.
*  Rainer Dietrich, Manfred Opper, and Haim Sompolinsky. Statistical mechanics of support vector networks. _Physical review letters_, 82(14):2975, 1999.
*  Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves in kernel regression and wide neural networks. In _International Conference on Machine Learning_, 2020.
*  Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mezard, and Lenka Zdeborova. Generalisation error in learning with random features and the hidden manifold model. _Journal of Statistical Mechanics: Theory and Experiment_, 2021, 2020.
*  Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborova. Error rates for kernel classification under source and capacity conditions. _ArXiv_, abs/2201.12655, 2022.
*  Christos Thrampoulidis, Ehsan Abbasi, and Babak Hassibi. Precise error analysis of regularized \(m\)-estimators in high dimensions. _IEEE Transactions on Information Theory_, 64(8):5592-5628, 2018.
*  Jacob A. Zavatone-Veth, William Tong, and Cengiz Pehlevan. Contrasting random and learned features in deep bayesian linear regression. _Physical review. E_, 105 6-1:064118, 2022.
*  Hugo Cui, Florent Krzakala, and Lenka Zdeborova. Optimal learning of deep random networks of extensive-width. _ArXiv_, abs/2302.00375, 2023.
*  Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pages 234-241. Springer, 2015.
*  Xiao-Jiao Mao, Chunhua Shen, and Yubin Yang. Image restoration using convolutional auto-encoders with symmetric skip connections. _ArXiv_, abs/1606.08921, 2016.
*  Tong Tong, Gen Li, Xiejie Liu, and Qinquan Gao. Image super-resolution using dense skip connections. In _Proceedings of the IEEE international conference on computer vision_, pages 4799-4807, 2017.
*  Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using very deep convolutional networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1646-1654, 2016.

*  Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Deeply-recursive convolutional network for image super-resolution. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1637-1645, 2016.
* 1614, 2011.
*  Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _CoRR_, abs/1412.6980, 2014.
*  Sebastian Goldt, Bruno Loureiro, Galen Reeves, Florent Krzakala, Marc Mezard, and Lenka Zdeborova. The gaussian equivalence of generative models for learning with shallow neural networks. In _Mathematical and Scientific Machine Learning_, 2020.
*  Hong Hu and Yue M. Lu. Universality laws for high-dimensional learning with random features. _IEEE Transactions on Information Theory_, 69:1932-1964, 2020.
*  Andrea Montanari and Basil N Saeed. Universality of empirical risk minimization. In _Conference on Learning Theory_, pages 4310-4312. PMLR, 2022.
*  Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mezard, and Lenka Zdeborova. Learning curves of generic features maps for realistic datasets with a teacher-student model. _Journal of Statistical Mechanics: Theory and Experiment_, 2022, 2021.
*  Bruno Loureiro, Gabriele Sicuro, Cedric Gerbelot, Alessandro Pacco, Florent Krzakala, and Lenka Zdeborova. Learning gaussian mixtures with generalised linear models: Precise asymptotics in high-dimensions. In _Neural Information Processing Systems_, 2021.

Derivation of Conjecture 3.3

In this section, we detail the derivation of Conjecture (3.3).

### Derivation technique

In order to access sharp asymptotic characterizations for the metrics and summary statistics \(\) (5),\(\) (6), \(\) and \(\|}\|/\!\!d\), the first step is to observe that any test function \((},)\) of the learnt network parameters \(},\) can be written as an average over a limit probability distribution

\[_{}[(},)]=_{ }_{} dwdb(, b)e^{-}(,b)}\] (16)

where \(}(,)\) is the empirical risk (4) and the corresponding probability density

\[_{}(,b)=}(,b)}}{Z}\] (17)

is known as the _Boltzmann measure_ in statistical physics, with the parameter \(\) typically referred to as the _inverse temperature_. The normalization \(Z\) is called the partition function of \(_{}\):

\[Z= ddbe^{-}(,b)}.\] (18)

The idea of the replica method , building on the expression (16), is to compute the moment generating function (also known as the _free entropy_) as

\[_{} Z=_{s 0}_{ }Z^{s}]}{s}.\] (19)

With the moment generating function, it then becomes possible to compute averages like (16). The backbone of the derivation, which we detail in the following subsections, therefore lies in the computation of the replicated partition function \(_{}Z^{s}\).

### Replicated partition function

For subsequent clarity, we shall introduce the variable

\[^{}^{}-_{k}\] (20)

if \(^{}\) belongs to the \(k-\)th cluster. By definition of the Gaussian mixture model, this is just a Gaussian variable: \(^{}(0,_{k})\), and the average over the data set \(\{^{}\}\) reduces to averaging over the cluster index \(k\) and \(\{^{}\}\). For notational brevity, we shall name in this appendix \(^{}\) what is called \(^{}\) in the main text (i.e., absorbing the variance in the definition of \(^{}\)). The replicated partition function then reads

\[_{}Z^{s}= _{a=1}^{s}d_{a}db_{a}e^{-_{a=1}^{s }g(^{a})}\] \[_{=1}^{n}_{k=1}^{K}_{k}_{,}_{a=1}^{s}\|_{k}+-[b_{a}((_{k}+)+ )+_{a}^{}}{}(_{a}( (_{k}+)+)}{})] \|^{2}}_{()}.\] (21)

One can expand the exponent \(()\)as

\[e^{-_{a=1}^{}[ (1-b_{a})^{2}(\|_{k}\|^{2}+\| \|^{2}+2_{k}^{})+b_{a}^{2}\|\|^{2}+2b_{ a}(b_{a}-1)(_{k}^{}+^{}) ]}.\] (22)

[MISSING_PAGE_FAIL:15]

We used the leading order of the covariances \(_{,m,n}\). One therefore has to introduce the summary statistics:

\[Q_{ab} =_{a}_{b}^{}}{d}^{p p}, S^{k}_{ab} =_{a}_{k}_{b}^{}}{d}^{p  p}, m^{k}_{a} =_{a}_{k}}{}^{p}.\] (31)

Note that the local fields \(_{a}^{,}\) thus follow a Gaussian distribution:

\[(_{a}^{},_{a}^{})_{a=1}^{s} (0,S^{k}&0\\ 0& Q}_{_{k}}).\] (32)

Going back to the computation, \((a)\) can be rewritten as

\[e^{-_{a=1}^{}[Q_{aa}(m^{k}_{a}((1+g^{}_{})+g^{}_{} )+_{a}^{}+_{a}^{})^{ 2} ]}\\  e^{-_{a=1}^{}[-2 (m^{k}_{a}((1+g^{}_{})+g^{}_{})+_{a}^{}+_{a}^{})^{}(m^{k}_{a}((1- b_{a})(1+g^{}_{})-b_{a}g^{}_{})+(1- b_{a})_{a}^{}-b_{a}_{a}^{})]}_{\{ _{a}^{},_{a}^{}\}_{a=1}^{s}}.\] (33)

### Reformulating as a saddle-point problem

Introducing Dirac functions enforcing the definitions of \(Q_{ab},m_{a}\) brings the replicated function in the following form:

\[Z^{s}= _{a=1}^{s}db_{a}_{a,b}dQ_{ab}d_{ab}_{k=1 }^{K}_{a=1}^{s}dm_{a}d_{a}_{k=1}^{K}_{a,b}dS^{k}_{ab}d ^{k}_{ab}\] \[^{-d_{a b}Q_{ab}_ {ab}-d_{k=1}^{K}_{a b}S^{k}_{ab}^{k}_{ab}- _{k=1}^{K}_{a}dm^{k}_{a}^{k}_{a}}}_{e^{ s _{t}}}\] \[^{s}dw_{a}e^{-_{a}g( {w}^{a})+_{a b}_{ab}_{a}_{b}^{}+ _{k=1}^{K}_{a b}^{k}_{ab}_{a}_{k }_{b}^{}+_{k=1}^{K}_{a}^{k}_{a} _{a}_{k}}}_{e^{ s_{w}}}\] \[^{K}_{k}(s^{ 2})e^{-\|_{k}\|^{2}}_{a 1}^{s}(b_{a}-1)^{2}}}{ _{k}^{}(C^{-1}_{}C^{-1}_ {}^{-1}_{}-(s^{2}))^{}}}(a)] ^{ d}}.\] (34)

We introduced the trace, entropic and energetic potentials \(_{t},_{w},_{y}\). Since all the integrands scale exponentially (or faster) with \(d\), this integral can be computed using a saddle-point method. To proceed further, note that the energetic term encompasses two types of terms, scaling like \(sd^{2}\) and \(sd\). More precisely,

\[[_{k=1}^{K}_{k}\|_{ k}\|^{2}_{a=1}^{s}(b_{a}-1)^{2}}}{_{k} ^{}(C^{-1}_{}C^{-1}_{})^{ {1}{2}}}}(a)]^{ d}\] \[= [1\!-\!sd_{k=1}^{K}_{k}(_{k}^{}(C^{-1}_{}C^{-1}_{ })^{}})\!\!+\!s_{k=1}^{K}_{k} (a).-\!\!_{k=1}^{K}_{k}\|_{k}\|^{2}\! _{a=1}^{s}(b_{a}\!-\!1)^{2}]^{ d}\] \[ e^{-s d^{2}_{k=1}^{K}_{k} (_{k}^{}(^{-1}_{ }C^{-1}_{})^{}})+s d_{k= 1}^{K}_{k}(a)}.\] (35)Note that the saddle-point method involves an intricate extremization over \(s s\) matrices \(Q,S_{k}\). To make further progress, we assume the extremum is realized at the Replica Symmetric (RS) fixed point

\[ a, b_{a}=b\] \[ a, m_{a}^{k}=m_{k},_{a}^{k}=_{k}\] \[ a,b, Q_{ab}=(r-q)_{ab}+q,_{ab}=- (}{2}+)_{ab}+\] \[ a,b, S_{ab}^{k}=(r_{k}-q_{k})_{ab}+q_{k}, _{ab}^{k}=-(_{k}}{2}+_{k})_{ ab}+_{k}\] (36)

This is a standard assumption known as the _RS ansatz_.

Quadratic potentialThe quadratic potential \(_{}\), which correspond to the leading order term in \(d\) in the exponent, needs to be extremized first in the framework of our saddle-point analysis. Its expression can be simplified as

\[_{k=1}^{K}_{k}(_{k}^{ }(_{}^{-1}C_{}^{-1})^{}}) =_{k=1}^{K}_{k}[(1+ sb^ {2})(_{d}+_{k} s(b-1)^{2})]\] \[=_{k=1}^{K}_{k}\![  b^{2}_{d}+_{k}(b-1)^{2}]\] \[=[ b^{2}+(b-1)^{2} _{k=1}^{K}_{k}_{k}],\] (37)

which is extremized for

\[b=(_{k=1}^{K}_{k}_{ k})}{(_{k=1}^{K}_{k}_{k})(1-)+}.\] (38)

This fixes the skip connection strength \(b\).

Entropic potentialWe now turn to the entropic potential \(_{w}\). It is convenient to introduce the variance order parameters

\[+, _{k}_{k}+_{k}.\] (39)The entropic potential can then be expressed as

\[e^{ sd_{w}}\] \[=_{a=1}^{s}dw_{a}e^{-_{a}g(^{a})- _{a=1}^{s}_{a}_{a}^{}+_{a,b}_{a} _{b}^{}+_{k=1}^{K}_{a=1}^{s} _{k}^{}_{a}^{}_{k}}\] \[ e^{-_{k=1}^{K} _{a=1}^{s}_{k}_{a}_ {k}_{a}^{}+_{k=1}^{K}_{a,b }_{k}_{a}_{k}_{b}^{ }}\] \[= D_{0}_{k=1}^{K}D_{k}\] \[[ dwe^{- g()-^{}+_{k=1}^{K}_{k} _{k}^{}+_{k=1}^{K} {m}_{k}^{}+_{k=1}^{K}_{k}(_{k} _{k})^{}+_{0}(_{d})^{}}]^{s}\] \[=_{0}_{k=1}^{K}D_{k}}_{ D }\] \[[ dwe^{- g()- [_{d}+_{k=1}^{K}_{k}_{k}]}+_{k=1}^{K}_{ k}^{}+_{k=1}^{K}_{k}(_{k}_{k})^{}+_{0}(_{d})^{ }}]^{s}.\] (40)

Therefore

\[_{w}= D[ dwe^{- g()- [_{d}+_{k=1}^{K} _{k}_{k}]}+_{k=1} ^{K}_{k}^{}+_{k=1}^{K}_{k}( _{k}_{k})^{}+_{0}( _{d})^{}}].\] (41)

For a matrix \(^{p d}\) and tensors \(,^{p d}^{p d}\), we denoted \(()_{kl}=_{ij}^{ij}_{ij,kl}\) and \(()_{ij,kl}=_{rs}_{ij,rs}_{rs,kl}\).

Energetic potentialIn order to compute the energetic potential \(_{y}\), one must first compute the inverse of the covariance \(_{k}\), given by

\[_{k}^{-1}=S_{k}^{-1}&0\\ 0&Q^{-1}.\] (42)

It is straightforward to see that \(S_{k}^{-1},Q^{-1}\) share the same block structure as \(S_{k},Q\), and we shall name for clarity

\[S_{k}^{-1}_{ab} =(_{k}-_{k})\,_{ab}+_{k},\] \[Q^{-1}_{ab} =(-)\,_{ab}+.\] (43)

From the Sherman-Morisson lemma, and noting

\[V_{k} r_{k}-q_{k}^{p p}, V  r-q^{p p},\] (44)

one reaches

\[_{k}=V_{k}^{-1}-(V_{k}+sq_{k})^{-1}qV_{k}^{-1} \\ =-(V+sq_{k})^{-1}qV_{k}^{-1}, _{k}=V_{k}^{-1}-(V_{k}+sq_{k})^{-1}q_{k}V_{k }^{-1}\\ _{k}=-(V_{k}+sq_{k})^{-1}q_{k}V_{k}^{-1}.\] (45)

We will also need the expression of the determinants

\[ S_{k}=s V_{k}+sV_{k}^{-1}q_{k}, Q =s V+sV^{-1}q.\] (46)

[MISSING_PAGE_FAIL:19]

[MISSING_PAGE_FAIL:20]

### Self-consistent equations

The extremization problem (58) can be recast as a system of self-consistent equations by requiring the gradient with respect to each summary statistic involved in (58) to be zero. This translates into:

\[_{k}=_{k}_{,}V_{k}^{-1}( _{y}^{k}-q_{k}^{}-m_{k})^{ 2}V_{k}^{-1}\\ _{k}=-_{k}q_{k}^{-}_{,}V_{k}^{-1} (_{y}^{k}-q_{k}^{}-m_{k})^{}\\ _{k}=_{k}_{,}V_{k}^{-1}(_{y }^{k}-q_{k}^{}-m_{k})\\ =_{k=1}^{K}_{k}_{,}V^{-1}( _{x}^{k}-q^{})^{ 2}V^{-1}\\ =-_{k=1}^{K}_{k}_{,}q^{- }V^{-1}(_{x}^{k}-q^{})^{ }-(_{y}+_{x})^{  2}\\ q=_{}[_{r}_ {r}^{}],\] (58)

We remind that the skip connection \(b\) is fixed to

\[b=(_{k=1}^{K}_{k} _{k})}{(_{ k=1}^{K}_{k}_{k})(1-)+}.\] (60)

### Sharp asymptotic formulae for the learning metrics

The previous subsections have allowed to obtain sharp asymptotic characterization for a number of summary statistics of the probability (17). These statistics are in turn sufficient to asymptotically characterize the learning metrics discussed in the main text. We successively address the test MSE (5), cosine similarity (6) and training MSE.

MseThe denoising MSE reads

\[ =_{k,,}\| _{k}+-(( _{k}+)++ }^{}}{}(}((_{k}+)+ {})}{}))\|^{2}\] \[=_{}+_{k=1}^{K}_{k}_{z }^{(0,1)}[\![q(m_{k}+}z)^{ 2}]]\] \[-2_{k=1}^{K}_{k}_{u,v}^{(0,1) }[(m_{k}+(1-)}u+ {}v)]^{}((1-b)m_{k}+(1-b)}u-bv)\] (61)

where

\[_{}=d b^{2}+(1-b)^{2}(_{ k=1}^{K}_{k}(\|_{k}\|^{2}+_{k}))\] (62)

Cosine similarityBy the very definition of the summary statistics \(m_{k},q\):

\[_{ik}}_{i}_{k}}{\|}\|\|_{k}\|}=)_{i}}{ }\|_{k}\|}\] (63)Training errorThe replica formalism also allows to compute a sharp asymptotic characterization for the training MSE

\[_{t}_{}[_{=1}^{n} \|^{}-(}^{})\|^{2}].\] (64)

We have skipped the discussion of this learning metric in the main text for the sake of conciseness, but will now detail the derivation. Note that the training error \(_{t}\) can be deduced from the risk (4) and the average regularization as

\[_{t}=_{}[}(},)]-_{}[r(})].\] (65)

Note that in turn, from the definition of the free entropy, the average risk (training loss) can be computed as

\[_{}[}(},)]=- =-.\] (66)

Doing so reveals

\[_{t} =-2-d_{}-m(b-1)^{2}))}{}\] \[=}+ _{k=1}^{K}_{k}_{^{p}}D D (_{};q^{},V_{k})(_{ };q_{k}^{},V)(*)e^{-(*)}}{(_{};q^{},V_{k})(_{ };q_{k}^{},V)e^{-(*)}}.\] (67)

Taking the \(\) limit finally leads to

\[_{t}=}+_{k=1}^{K}_{k}_{,} q(_{y}+ _{x})^{ 2}\] \[-2(_{y}+_{x})^{ }((1-)_{y}-_{x}) \] (68)

The sharp asymptotic formula (68) agrees well with numerical simulations, Fig. 5. As is intuitive, the generalization gap \(_{}-_{t}\) grows with the noise level \(\), as the learning problem grows harder.

Figure 5: (left) Training MSE for the full DAE (2) (\(p=1,\;=\)). Solid lines represent the sharp asymptotic formula (68); dots correspond to simulation, training the DAE with the Pytorch implementation of full-batch Adam, over \(T=2000\) epochs using learning rate \(=0.05\) and weight decay \(=0.1\). The data was averaged over \(N=5\) instances; error bars are smaller than the point size. (right) Generalization gap \(_{}-_{t}\). Solid lines correspond to the asymptotic prediction of Conjecture 3.3 (for the test MSE) and of (68) (for the train MSE), while dots correspond to simulations. Error bars represent one standard deviation. The Gaussian mixture is the isotropic binary mixture, whose parameters are specified in the caption of Fig. 1 in the main text.

### Generic asymptotically exact formulae

This last result ends the derivation of the generic version of Conjecture 3.3, i.e. _not assuming_ Assumptions 3.1 and 3.2. Importantly, note that the characterization (58) is, like the formulae in e.g. , _asymptotically exact_, but not _fully_ asymptotic, as the equations (58) still involve high-dimensional quantities. In practice however, for standard regularizers \(r()\) like \(_{1}\) or \(_{2}\), the proximal \(_{+}\) admits a simple closed-form expression, and the average over \(\) can be carried out analytically. The only high-dimensional operation left is then taking the trace of linear combinations of \(_{k}\) matrices (or the inverse of such combinations), which is numerically less demanding, and analytically much simpler, than the high dimensional optimization (4) and averages (5) involved in the original problem. We give in the next paragraph an example of how Assumption 3.1 can be relaxed, for a binary mixture with arbitrary (i.e. not necessarily jointly diagonalizable) covariances \(_{1,2}\).

Example: Anisotropic, heteroscedastic binary mixtureAs an illustration, we provide the equivalent of (58) for a binary mixture, with generically anisotropic and distinct covariances, for \(p=1\), thereby breaking free from Assumption 3.1. We index the clusters by \(+,-\) rather than \(1,2\) for notational convenience. We remind that

\[_{}(,)=\{(x-)^{2}+^{ }}(y-+m})^{2}+q(y+x)^{2}-2(y+x)((1-b)y-bx)\}.\] (69)

Then the self-consistent equations (58) simplify to

\[q=((+) _{d}+_{+}_{+}+_{-}_{-})^{- 2}(_{d}+_{+}_{+}+_{-}_{-}+^{2}d^{})\\ m=((+)_{d}+_{+}_{+}+_{-}_{-})^{-1}^{ }\\ V=(+)_{d }+_{+}_{+}+_{-}_{-}^{-1}\\ q_{}=((+)_ {d}+_{+}_{+}+_{-}_{-})^{-1}_{}((+)_{d}+_{+}_{+ }+_{-}_{-})^{-1}(_{d}+ _{+}_{+}+_{-}_{-}+^{2}d^{ })\\ V_{}=_{}((+ )_{d}+_{+}_{+}+_{-}_{-} )^{-1}\]

\[=[}_{,} (_{y}^{+}-}-m)-(1-)} _{,}(_{y}^{-}-}+m) ]\\ =}[_{,}(_{x}^{+}-)^{2}+(1-)_{,}(_{x}^{-}-)^{2}]\\ =-[_{,}[_{x}^{+}-)}{V}-(_{y}^{+}+_{x}^{+})^{2}]\\ +(1-)_{,}[_{x}^{-}-)}{V}-(_{y}^{-}+_{x}^{-})^{2}]]\\ _{+}=^{2}}_{,}(_{y}^{+}-}-m)^{2}\\ _{-}=(1-)^{2}}_{,}( _{y}^{-}-}+m)^{2}\\ _{+}=-V_{+}}}_{,}( _{y}^{+}-}-m)\\ _{-}=-(1-)V_{-}}}_{,} (_{y}^{-}-}+m)\] (70)

These equations are, as previously discussed, asymptotically exact as \(d\). While they still involve traces over high-dimensional matrices \(_{k}\), this is a very simple operation. We have therefore reduced the original high-dimensional optimization problem (4) to the much simpler one of computing traces like (70). Crucially, while these traces cannot be generally simplified without Assumption 3.1, they provide the benefit of a simple and compact expression which bypasses the need of jointly diagonalizable covariances, and which can thus be readily evaluated for any covariance, like real-data covariances. This versatility is leveraged in Appendix D.

### End of the derivation: Conjecture 3.3

We finally provide the last step in the derivation of the fully asymptotic characterization of Conjecture 3.3. Assuming \(r()=}{{2}}_{F}^{2}\) (Assumption 3.2), the Moreau envelope \(_{r}\) assumes a very simple expression, and no longer needs tobe written as a high-dimensional optimization problem. The resulting free energy can be compactly written as:

\[= ,,,\{q_{k},m_{k},V_{k}, _{k},_{k},_{k}\}_{k=1}^{K}}{}\![q]-\![V]}{2}+_{k=1}^{K}(\![_{k}q_{k}]- \![_{k}V_{k}])-_{k=1}^{K}m_{k}_ {k}\] \[-_{k=1}^{K}_{k}_{,}_{k}( ,)\] \[+\![( _{p}_{d}+_{d}+_{k=1}^{K} _{k}_{k})^{-1}\!( _{d}+_{k=1}^{K}_{k}_{k} +d(_{k=1}^{K}_{k}_{k}^{})^{ 2 })]}_{(*)}\] (71)

(\(*\)) is the only segment which still involves high-dimensional matrices, and is therefore not yet full asymptotic. Assuming Assumption 3.1, (\(*\)) can be massaged into

\[(*) =_{i=1}^{d}\![( ++_{i}^{k}_{k})^{-1}(+_{k=1}^{K} _{i}^{k}_{k}+_{1 k,j K}e_{i}^{}_{j}_{j} _{k}^{}_{k}^{}e_{i})]\] \[ d(, )\![(++_{k}_{k}) ^{-1}(+_{k=1}^{K}_{k}_{k}+_{1 k,j K} _{j}_{k}_{j}_{k}^{})]\] (72)

The fully asymptotic free energy thus becomes

\[= ,,,\{q_{k},m_{k},V_{k}, _{k},_{k},_{k}\}_{k=1}^{K}}{}\![q]-\![V ]}{2}+_{k=1}^{K}(\![_{k}q_{ k}]-\![_{k}V_{k}])-_{k=1}^{K}m_{k} _{k}\] \[-_{k=1}^{K}_{k}_{,}_{k} (,)\] \[+ d(,)\![( ++_{k}_{k})^{-1}(+_{k=1}^{K} _{k}_{k}+_{1 k,j K}_{j}_{k}_{j} _{k}^{})]\] (73)Requiring the free energy to be extremized implies

\[_{k}=_{k}_{,}V_{k}^{-1}( _{y}^{k}-q_{k}^{}-m_{k})^{ 2}V_{k}^{-1}\\ _{k}=-_{k}q_{k}^{-}_{,}V_{k}^{-1} (_{y}^{k}-q_{k}^{}-m_{k})^{}\\ _{k}=_{k}_{,}V_{k}^{-1}(_{ y}^{k}-q_{k}^{}-m_{k})\\ =_{k=1}^{K}_{k}_{,}V^{-1}( _{x}^{k}-q^{})^{ 2}V^{-1}\\ =-_{k=1}^{K}_{k}_{,}q^{- }V^{-1}(_{x}^{k}-q^{})^{ }-(_{y}^{k}+_{x}^{k} )^{ 2}\\ \\ q_{k}= d(,)_{k}(_{p}++ _{j=1}^{K}_{j}_{j})^{-2}(+ _{j=1}^{K}_{j}_{j}+_{1 j,l K}_{j} _{l}_{j}_{l}^{})\\ V_{k}= d(,)_{k}(_{p}++ _{j=1}^{K}_{j}_{j})^{-1}\\ \\ q= d(,)(_{p}++_{j=1} ^{K}_{j}_{j})^{-2}(+_{j=1}^{K} _{j}_{j}+_{1 j,l K}_{j}_{l}_{j} _{l}^{})\\ V= d(,)(_{p}++_{j=1} ^{K}_{j}_{j})^{-1},\] (75)

which is exactly (13). This closes the derivation of Conjecture 3.3. 

As a final heuristic remark, observe that from (74) it is reasonable to expect that all the hatted statistics \(_{k},_{k},_{k},,\) should be of order \(()\) as \( 1\). As a consequence, \(V_{k}=(}{{}})\) and \(q,m,q_{k},m_{k}\) should approach their \(\) limit as \((}{{}})\). Therefore, the MSE (61) should also approach its \(\) limit as \((}{{}})\).

### Additional comparisons

We close this appendix by providing further discussion on some points, including the role of the non-linearity \(()\), the influence of the weight-tying assumption, and of the (an-)isotropy and (homo/hetero)-scedasticity of the clusters.

The activation functionThe figures in the main text were generated using \(=\). Several studies for RAEs, however, have highlighted that an auto-encoder would optimally seek to place itself in the _linear_ region of its non-linearity, so as to learn the principal components of the training data, at least in the non-linear untied weights case, see e.g. . In light of these findings, it is legitimate to wonder whether setting a linear activation \((x)=x\) would not yield a better MSE. Fig. 6 shows that it is not the case. For the binary isotropic mixture (13), the linear activations yield a worse performance than the tanh activation.

Weight-tyingWe have assumed that the weights of the DAE (2) were tied. While this assumption originates mainly for technical reasons in the derivation, note that it has been also used and discussed in practical setting  as a way to prevent the DAE from going to the linear region of its non-linearity, by making the norm of the first layer weights very small, and that of the second layer very large to compensate . A full extension of Conjecture 3.3 to the case of the untied weight is a lengthy theoretical endeavour, which we leave for future work. However, note that Fig. 6 shows, in the binary isotropic case, that untying the weights actually leads to a _worse_ MSE than the DAE with tied weights. This is a very interesting observation, as a DAE with untied weights obviously has the expressive power to implement a DAE with tied weights. Therefore, this effect should be mainly due to the untied landscape presenting local minima trapping the optimization. A full landscape analysis would present a very important future research topic. Finally, remark that this stands in sharp contrast to the observation of  for non-linear RAEs, where weight-tying worsened the performance, as it prevents the RAE from implementing a linear principal component analysis.

An important conclusion from these two observations is that, in contrast to RAEs, the DAE (2) does _not_ just implement a linear principal component analysis -- weight tying, and the presence of a non-linearity, which are obstacles for RAEs in reaching the PCA MSE, lead for DAEs (2) to a _better_ MSE. The model (2) therefore constitutes a model of a genuinely _non-linear_ algorithm, where the non-linearity is helpful and is not undone during training. Further discussion can be found in Section 4 of the main text.

Heteroscedasticity, anisotropyFig. 1 and 3 in the main text all represent the isotropic, homoscedastic case. This simple model actually encapsulates all the interesting phenomenology. In this paragraph, we discuss for completeness the generically heteroscedastic, anisotropic case. We consider a binary mixture, with covariances \(_{1},_{2}\) independently drawn from the Wishart-Laguerre ensemble, with aspect ratios \(}{{7}}\) and \(}{{6}}\). Therefore, the clusters are anisotropic, and eigendirections associated with the largest eigenvalues are more "stretched" (i.e. induce higher cluster variance). Furthermore, since the set of eigenvectors of \(_{1},_{2}\) are independent, the two clusters are stretched in different directions. To ease the comparison with Fig. 1 (isotropic, homoscedastic), these Wishart matrices were further divided by \(10\), so that the trace is approximately \(0.09\) like in Fig. 1 - i.e., the clusters have the same average extension. Fig. 7 presents the resulting metrics and summary statistics. Again, the agreement between the theory 3.3 and numerical simulation using Pytorch is compelling. Qualitatively, the observations made in Section 4 still hold true in the anisotropic heteroscedastic case:

* The curve of the cosine similarity still displays a non-monotonic behaviour, signalling the preferential activation by the DAE of its skip connection at low \(\) and of its network component at high \(\).
* The skip connection strength \(\) is higher at small \(\) and decreases, in connection to the previous remark.
* The norm of the weight matrix \(}\) overall increases with the noise level \(\), signalling an increasing usage by the DAE of its bottleneck network component, again in accordance to the previous remark.

Therefore, the generic case does not introduce qualitative changes compared to the isotropic case.

Strength of the weight decay \(\)A \(_{2}\) regularization (weight decay) \(=0.1\) was adopted in the main text. In fact, the value of the strength of the weight decay was not found to sensibly influence the curves, and again, the qualitative phenomena discussed in Section 4 are observed for any value. Fig. 8 shows, for an isotropic binary mixture, the MSE difference \(_{}-_{}\) (5) and the cosine similarity \(\) (6) for regularization strength \(=0.1\) and \(=0.001\). As can be observed, even reducing the regularization a hundredfold does not change at all the qualitative picture - in particular, the non-monotonicity of the cosine similarity, discussed in Section 4 -, and very little the quantitative values. On the

Figure 6: Same parameters as Fig. 1 in the main text. \(=1,K=2,_{1,2}=}{{2}}\), \(_{1,2}=0.3_{d},p=1\); the cluster mean \(_{1}=-_{2}\) was taken as a random Gaussian vector of norm \(1\). Difference in MSE between the full DAE \(\) (2) and the rescaling network \(\) (3) for \(()=\) (blue) or \((x)=x\) (red). Solid lines correspond to the sharp asymptotic characterization (13), which is a particular case of Conjecture 3.3. Dots represent numerical simulations for \(d=700\), training the DAE using the Pytorch implementation of full-batch Adam , with learning rate \(=0.05\) over \(2000\) epochs, averaged over \(N=2\) instances. Error bars represent one standard deviation. In dashed green, numerical simulations for the same architecture, but untied weights. The learning parameters were left unchanged.

Figure 8: Same parameters as Fig. 1 in the main text. \(=1,K=2,_{1,2}=}{{2}},_{1,2}=0.09_ {d},p=1,=\); the cluster mean \(_{1}=-_{2}\) was taken as a random Gaussian vector of norm \(1\). Difference in MSE between the full DAE \(\) (2) and the rescaling network \(\) (3) for \(=0.1\) (blue) or \(=0.001\) (red). Lines correspond to the sharp asymptotic characterization (13) of Conjecture 3.3. Dots represent numerical simulations for \(d=700\), training the DAE using the Pytorch implementation of full-batch Adam , with learning rate \(=0.05\) over \(2000\) epochs, averaged over \(N=2\) instances. (right) Cosine similarities, for \(=0.1\) (green) and \(=0.001\) (orange); lines correspond to the asymptotic formulae of Conjecture 3.3, dots represent the numerical simulations.

other hand, observe that at the level of the MSE increasing the weight decay helps palliate the overfitting at large noise levels \(\), as we further illustrate in Fig. 9

\(K=1\) clusterWe finally discuss the case of _unstructured_ data, where the Gaussian mixture only has a single cluster. Analytically, this corresponds to setting the norm of the cluster mean \(\|\|\) to zero in Conjecture 3.3. The bottleneck component, whose role is to learn the data structure - as given by the cluster mean-and leverage it to improve the denoising performance, is no longer needed when the data is unstructured. It presence actually leads the DAE to overfit, as can be observed in Fig.10 (left). Similarly, the PCA denoiser performs worse in the unstructured case, with an associated MSE remaining of order \((d)\), see Fig.10 (right).

Figure 10: \(=1,K=1,_{1,2}=,_{1}=0.09_{d},p=1,()=()\). (left) Difference between the MSE of the DAE \(\) and the MSE of the rescaling \(_{}-_{}\) in the case of a single Gaussian cluster. Solid lines correspond to the theoretical prediction of Conjecture 3.3 and dots to numerical simulations. In the case of a single cluster, the bottleneck component of the DAE is not needed, and its presence leads to detrimental overfitting, as signalled by the positive value of \(_{}-_{}\). (right) Difference between the MSE of the DAE \(\) and the MSE of the bottleneck network \(\), divided by \(d\). Like the \(K>1\) case (see Fig. 3 (b)), this difference is of order \(_{d}(d)\), and the MSE of PCA (green dots) is sensibly equal.

Tweedie baseline

### Oracle denoiser

Tweedie's formula  provides the best estimator of a clean sample \(x\) given a noisy sample \(\) corrupted by Gaussian noise, as

\[t(})=}}{}+}|_{}}.\] (76)

Note that this gives the Bayes-optimal estimator for the MSE, assuming _perfect knowledge_ of the clean data distribution \(\) from which the so-called score \(\) has to be computed. Of course, this knowledge is inaccessible in general, where the only information on \(\) is provided in the form of the train set \(\). Therefore, Tweedie's formula does not give a learning algorithm, but allows to give an oracle lower-bound on the achievable MSEs, as any learning algorithm will yield a higher MSE than this information-theoretic baseline.

For a generic Gaussian mixture (1), Tweedie's formula reduces to the expression

\[t() =}^{K}_{k} [}_{k}(}_{k}+_{d})^{- 1}+(}_{k}+_{d})^{-1}}_{ k})](;}_{k},}_{k}+ _{d})}{_{k=1}^{K}_{k}( ;}_{k},}_{k}+_{d})},\] (77)

where we noted

\[}_{k} =_{k}, }_{k} =(1-)_{k}.\] (78)

### Oracle test MSE

In general, except in the complete homoscedastic case where all clusters have the same covariance, there is no closed-form asymptotic expression for the MSE achieved by the Tweedie denoiser. In the binary homoscedastic case \(_{1}=-_{2}\), \(_{1}=_{2}=^{2}_{d}\) (see Fig. 1), Tweedie's formula (77) reduces to the compact form

\[t() =^{2}}{^{2}(1-)+} }+(1-)+}(}^{}}{^{2}(1-)+} )\] (79)

Note that this is of the same form as the DAE architecture (2). The associated MSE reads

\[^{} =_{,}\|-[b(+)+}{^{2}(1-)+} (^{}(+)}{^{2}(1-)+})]\|^{2}.\] (80)

A sharp asymptotic formula can be found to be

\[^{} =_{}+^{4}}{(^{2}(1- )+)^{2}}_{s= 1}_{z}^{(0,1)}[ (z}}{^{2}(1-)+})^{2}]\] \[-(1-)+}_{s=  1}_{z}^{(0,1)}[(z}}{^{2}(1- )+})]((1-b)s).\] (81)

This is the theoretical characterization plotted in Figs. 1 and 3 in the main text.

As discussed in the main text and illustrated in Fig. 3(left), the MSE of the DAE approaches the oracle MSE (81) as the sample complexity \(\) grows. On the other hand, the DAE MSE does not exactly converges to the oracle MSE as \(\). Intuitively, this is because of the fact that while the form of the oracle (79) is that of a DAE, it does not have tied weights, like the considered architecture (2). The latter cannot therefore perfectly realize the oracle denoiser, whence a small discrepancy, as more precisely numerically characterized in Fig. 11.

## Appendix C Bayes-optimal MSE

### Assuming imperfect prior knowledge

The Tweedie denoiser discussed in Appendix B is an _oracle_ denoiser, in the sense that perfect knowledge of all the parameters \(\{_{k},_{k}\}_{k=1}^{K}\) of the Gaussian mixture distribution \(\) (1) is assumed. Therefore, though the comparison of the DAE (2) MSE with the oracle Tweedie MSE \(^{}\) does provide useful intuition (see Fig. 3), it importantly does not allow to disentangle which part of the MSE of the DAE is due to the limited expressivity of the architecture, and which is due to the limited availability of training samples - which entails _imperfect_ knowledge of the parameters \(\{_{k},_{k}\}_{k=1}^{K}\) of the Gaussian mixture distribution \(\) (1). In this appendix, we derive the MSE of the Bayes optimal estimator (with respect to the MSE), assuming only knowledge of the distribution from which the parameters \(\{_{k},_{k}\}_{k=1}^{K}\) are drawn. For simplicity and conciseness, we limit ourselves to the binary isotropic and homoscedastic mixture (see e.g. Fig. 1), for which \(_{1}=-_{2}^{}\) and \(_{1}=_{2}=_{d}\). For definiteness, we assume \(_{}\), with \(_{}=(0,}{{d}}/d)\), so that with high probability \(\|\|=1\). We shall moreover assume for ease of discussion that \(\) is perfectly known. Thus, the centroid \(\) is the only unknown parameter.

### Bayes-optimal denoiser without knowledge of cluster means

The corresponding Bayes-optimal denoiser then reads

\[(})[|}, ,]= d[| },,,]}_{t(,})} (|,)\] (82)

Where we have identified the oracle denoiser (79), and emphasized the dependence on \(\). Note that this is a slight abuse of notation, as the true oracle \(t(^{},})\) involves the ground truth centroid. Further note that \((|,)=(|)\), since knowledge of \(\) does not bring information about \(\). Furthermore, note that as the noisy part of the dataset \(}\{}^{}\}_{=1}^{n}\) brings only redundant information, one further has that

\[(|)=(|}),\] (83)

Figure 11: Like Fig. 1 of the main text, \(K=2,_{1,2}=,_{1,2}=0.09_{d},p=1, ()=()\); the cluster mean \(_{1}=-_{2}\) was taken as a random Gaussian vector of norm \(1\). Solid lines correspond to the difference in MSE between the full DAE \(\) and the rescaling component \(\) as predicted by Conjecture 3.3. In dashed lines, the oracle MSE (81), minus the rescaling MSE \(_{}\). Different colors correspond to different noise levels \(\). As the sample complexity \(\) becomes large, the denoiser MSE approaches the oracle baseline, but does not become exactly equal to it as \(\).

where we noted \(}=\{^{}\}_{=1}^{n}\) the clean part of the dataset. Thus

\[(}) =(})} d\;\;t(,})(}|)_{}( )\] \[=(})} d\;\;t( {},})_{}()_{=1}^{n} )^{d/2}}[e^{-}\| ^{}-\|^{2}}+e^{-}\|^{}+ \|^{2}}]\] \[=(})} d\;\;t( ,})}e^{-\| \|^{2}}e^{-}\|\|^{2}}_{=1}^{n} }\|^{}\|^{2}}}{(2^{2} )^{d/2}}(^{}^{}}{^{2}})\] \[=(})}} d \;\;t(/,})}e^{ -\|\|^{2}}e^{-}\|\|^{2}} _{=1}^{n}}\|^{}\|^{2}}}{( 2^{2})^{d/2}}(^{}^{}}{ ^{2}})\] \[= d\;\;t(/,}) _{b}(),\] (84)

where we noted

\[_{b}() e^{-}\|\|^{2}} _{=1}^{n}(^{}^{}}{^{2}} ),\] (85)

with the effective variance

\[^{2}}{^{2}+}.\] (86)

Finally, the partition function \(Z\) is defined as the normalisation of \(_{b}\), i.e.

\[Z d\;\;_{b}().\] (87)

One is now in a position to compute the MSE associated with the Bayes estimator \((})\):

\[_{}\] \[=_{}_{} _{(0,_{d})}\|- t (/,+)_ {_{b}}\|^{2}\] \[=_{}-_{}_{s= 1} _{,(0,_{d})}(1-)+}()s^{}^{}}{d}+^{}(1-)-b}{})\] \[(^{}_{2} ^{}}{d}+^{}_{2}} {d}+^{}_{2}}{}}{^{2}(1- )+})_{_{b}}\] \[+_{}_{s= 1}_{,(0,_{d})}((1-)+})^{2}^{}_{1}_{2}}{ d}(^{}_{2}^{}}{d}+ s^{}_{1}+^{ }_{1}}{}}{^{2}(1-)+})\] \[(^{}_ {2}^{}}{d}+^{}_{2}} {d}+^{}_{2}}{}}{^{2}(1- )+})_{_{1},_{2}_{b}}\] \[=_{}-_{}_{s= 1}_{z(0,1)}(1-)+} ()s^{}^{ }}{d})(^{}_{2}^{}}{d}++}\|_{z}}{ }}{^{2}(1-)+})_{ _{b}}\] \[(^{}_ {2}^{}}{d}++}v}{^{2}(1- )+})_{_{1},_{2}_{b}}.\] (88)We have adopted the shortcuts

\[b=^{2}}{^{2}(1-)+}, =_{1}\|^{2}}{d}&_{1}^{ }_{2}}{2}\\ _{1}\|^{2}}{d}&_{2}\|^{2}}{d}..\] (89)

(88) shows that the inner averages involved in the Bayes MSE \(_{}\) only depend on the overlaps \(_{1,2}^{}_{1,2}/d\) for \(_{1},_{2}\) two independently sampled vectors from \(_{b}\). Motivated by similar high-dimensional studies, it is reasonable to expect these quantities to concentrate as \(n,d\) in the measure \(_{}_{_{b}}\). A more principle - but much more painstaking-way to derive this assumption is to introduce a Fourier representation, and carry out the \(_{}\) average using the replica method. We refer the interested reader to, for instance, Appendix H of , where such a route is taken.

### Derivation

To compute the summary statistics of \(_{b}\), we again resort to the replica method to compute the moment-generating function (free entropy), see also Appendix A. The replicated partition function reads

\[_{}Z^{s}=_{a=1}^{s}d_{a}e^{-}\|_{a}\|^{2}}_{=1}^{n}_{s= 1} _{(0,_{d})}[_{a=1}^{s} (_{a}^{}^{}}{d}+_{a}^{}}{}}{^{2}})].\] (90)

Introducing the order parameters

\[q_{ab}_{a}^{}_{b}}{d}, m_{a}_{a}^{}^{}}{d},\] (91)

one reaches

\[_{}Z^{s}= _{a b}dq_{ab}d_{ab}_{a=1}^{s}dm_{a}d {m}_{a}^{-d_{a b}q_{ab}_{ab}-d _{a}m_{a}_{a}}}_{e^{s_{t}}}^{s}d_{a}e^{-}\|_{a}\|^{2}}e^ {_{a b}_{ab}_{a}^{}_{b}+ _{a}_{a}^{}^{}}}_{e^{s_{w}}}\] \[( \{_{a}\}_{a=1}^{s};0,(q_{ab})_{1 a,b s})[_{a=1}^ {s}(+_{a}}{^{2}})]] }_{e^{s_{y}}}.\] (92)

The computation of \(_{t},_{w},_{y}\) is rather standard and follows the main steps presented in Appendix A. One again assumes the replica symmetric ansatz 

\[q_{ab} =(r-q)_{ab}+q, m_{a} =m,\] (93) \[_{ab} =(-}{2}+-)_{ab}+, _{a} =.\] (94)

Introducing as in Appendix A the variances

\[V r-q, +,\] (95)

one reaches

\[_{t} =q-V}{2}-m,\] (96) \[_{w} =-(1+^{2})+^{2}}{2}+^{2}}{1+^{2}},\] (97) \[_{y} =_{(0,1)}[ d (;,V)( })].\] (98)

Therefore the free entropy reads

\[=*{extr}_{q,m,V,,,} q-V}{2}-m-(1+ ^{2})+^{2}}{2}+^{2}}{1+ ^{2}}\] \[+_{(0,1)}[ d (;,V)(})].\] (99)The solution of this extremization problem is given by the set of self-consistent equations, imposing that gradients with respect to all parameters be zero:

\[V=^{2}}{1+^{2}}\\ q=^{2}(+)}{(1+^{2})^{2}}+ {^{2}}{1+^{2}}\\ m=^{2}}{1+^{2}} =-}_{ (0,1)}[(+m}{^{2}} )]\\ =}\\ =}_{(0,1)}[ (+m}{^{2}})]\] (100)

This is a simple optimization problem over \(6\) variables, which can be solved numerically. One can finally use the obtained summary statistics to evaluate the Bayes optimal MSE:

\[_{} =_{}-2_{}_{z (0,1)}(1-)+}((1-b)m)(m++ }z}{^{2}(1-)+})\] (101) \[+_{}_{u,v(0, )}((1-)+})^{2}q( m++}u}{^{2}(1- )+})(m++}u}{^{2}(1-)+})\]

where

\[=q+V&q\\ q&q+V\] (102)

This completes the derivation of the MSE of the Bayes estimator agnostic of the cluster means.

### A simple plug-in denoiser

Another simple denoiser assuming only perfect knowledge of the variance \(\), but imperfect knowledge of \(\), is simply to plug the PCA estimate \(}_{}\) of \(\) into the Tweedie oracle (76), i.e.

\[t(}_{},})=^{ 2}}{^{2}(1-)+}}+(1- )+}(}^{}}_{}}{^{2}(1-)+})}_{ }.\] (103)

The performance of this plug-in denoiser is plotted in Fig. 12, and contrasted to the one of the Bayes denoiser \(()\) (101), the oracle \(t()\) (81) and the DAE 3.3. Strikingly, the performance of the PCA plug-in denoiser is sizeably identical to the one of the Bayes-optimal denoiser, both in terms of MSE and cosine similarity. It is important to remind at this point that both the plug-in (103) and the Bayes denoiser (101) still rely on perfect knowledge of the cluster variance \(\) and the noise level \(\), while the DAE (2) is agnostic to these parameters. Yet, these two denoisers make for a fairer comparison than the oracle (81), as they importantly take into account the finite training set. The DAE is relatively close to the Bayes baseline for small noise levels \(\), while a gap can be seen to open up for larger \(\).

Figure 12: (Binary isotropic mixture, \(_{1,2}=0.09_{d}\), \(_{1,2}=}{{2}}\), and \(_{1}=-_{2}\), see also Fig. 1). (right) MSE (minus the rescaling baseline \(_{}\)) for the DAE (\(p=1,=\)) (2) (blue), the oracle Tweedie denoiser (76) (green), the Bayes denoiser (101) (red), and the plug-in oracle-PCA denoiser (103) (black). Solid lines correspond to the theoretical formulae 3.3, (81) and (101); simulations correspond to a DAE optimized using the Pytorch implementation of Adam (weight decay \(=0.1\), learning rate \(=0.05\), full batch, \(2000\) epochs, sample complexity \(=1\)), averaged over \(N=10\) instances, with error bars representing one standard deviation. (right) Cosine similarity (6) for the same algorithms, with identical color code.

## Appendix D Details on real data simulations

In this Appendix, we provide further details on the real data experiments presented in Fig. 2 and Fig. 4.

PreprocessingThe original MNIST  and FashionMNIST  data-sets were flattened (vectorized), centered, and rescaled by \(400\) (MNIST) and \(600\) (FashionMNIST). For simplicity and ease of discussion, for each data set, only two labels were kept, namely \(1\)s and \(7\)s for MNIST and boots and shoes for FashionMNIST. Note that the visual similarity between the two selected classes should make the denoising problem more challenging.

Means and covariance estimationFor each data-set, data from the same class were considered to belong to the same Gaussian mixture cluster. For simplicity, we kept the same number of training points in each cluster, so as to obtain balanced clusters (\(_{1}=_{2}=}{{2}}\)) for definiteness. For each cluster, the corresponding mean \(\) and covariance \(\) were numerically evaluated from the empirical mean and covariance over the \(6000\) boots (shoes) in the FashionMNIST training set, and the \(6265\)\(1\)s (\(7\)s) in the MNIST training set. The solid lines in Fig. 2 correspond to using those estimates in the asymptotic formulae of Conjecture 3.3, in their generic form (58) (see Appendix A) for sample complexity \(=1\) and \(_{2}\) regularization \(=0.1\).

Pytorch simulationsThe red points in Fig. 2 were obtained from numerical simulations, by optimizing a \(p=1\) DAE (2) with \(=\) activation using the Pytorch implementation of the full-batch Adam  optimizer, with weight decay \(=0.1\), learning rate \(=0.05\), over \(T=2000\) epochs. For each value of \(\), \(n=784\) images were drawn without replacement from the data-set training set, corrupted by noise, and fed to the DAE along with the clean image for training. The denoising test MSE, cosine similarity and summary statistics of the trained DAE were estimated after optimization using \(n_{}=1000\) images randomly drawn from the test set (from which also only the relevant classes were kept, and equally represented- i.e. balanced). Each value was averaged over \(N=10\) instances of the train set, noise realization and test set. The obtained values were furthermore found to be robust with respect to the choice of the optimizer, learning rate and number of epochs.

### Complementary simulations

Fig.2 in the main text addresses the comparison of the theoretical prediction of Conjecture 3.3 with simulations on the real data-set, at the level of the denoising test MSE (5). For completeness, we show here the same comparison for the other learning metrics and summary statistics, namely the cosine similarity \(\) (6), weights norm \(\|}\|^{2}/\!d\) and trained skip connection strength \(\). Theoretical asymptotic characterization are again provided by Conjecture 3.3 (10), (11) and (13).

Figure 13: Cosine similarity \(\) (6) (green), weight norm \(\|}\|^{2}/\!d\) and skip connection strength \(\) for the MNIST dataset (left), of which for simplicity only \(1\)s and \(7\)s were kept, and FashionMNIST (right), of which only boots and shoes were kept. In solid lines, the theoretical predictions resulting from using Conjecture 3.3 in its generic formulation (58) with the empirically estimated covariances and means. Dots represent numerical simulations of a DAE (\(p=1,=\)) trained with \(n=784\) training points, using the Pytorch implementation of full-batch Adam, with learning rate \(=0.05\) over \(2000\) epochs, weight decay \(=0.1\), averaged over \(N=5\) instances. Error bars represent one standard deviation. See Appendix D for full details on the preprocessing.

They are plotted as solid lines in Fig. 13, and contrasted to numerical simulations (dots) on the real data-sets, by training a \(p=1\) DAE (2) using the Pytorch implementation of the Adam optimizer. All experiment details are the same as Fig. 2 of the main text, and can be found in the previous subsection. As can be observed, there is a gap, for large noise levels \(\), between the theory and simulations of the weights norm \(\|\|^{2}/d\) (red). On the other hand, the matchings for the cosine similarity \(\) (green) and skip connection strength \(\) (blue) are perfect. Overall, Conjecture (3.3) therefore captures well (with the exception of \(\|\|^{2}/d\) at large \(\)) the main metrics of the denoising problem on these two real data-sets. A more thorough investigation of the possible Gaussian universality of the problem is left to future work.

## Appendix E Derivation of Corollary 3.4

In this Appendix, we provide the derivation of Corollary 3.4.

Rescaling component \(\)We first provide a succinct derivation of the sharp asymptotic characterizations for the rescaling component \(\) (3). Because this is only a scalar optimization problem over the single scalar parameter \(c\) of \(h_{c}\), the derivation is simple and straightforward. The empirical loss reads

\[}(c) =_{=1}^{n}\|^{}-c(^{}+^{})\|^{2}\] \[=(1-c)^{2}_{=1}^{n}^{}\|^{2}}{d}+c^{2}_{=1}^{n}^{} \|^{2}}{d}+_{=1}^{n}^{}\|^{2}}{d}^{})^{}^{}}{d}\] \[=(1-c)^{2}_{=1}^{n}^{}\|^{2}}{d}+c^{2}+(}{{}})\] \[}}{{=}}(1-c)^{ 2}_{}[^{}\|^{2}}{d}]+c^{2}\] \[=(1-c)^{2}_{k=1}^{K}_{k}( \|_{k}\|^{2}+_{k})+c^{2}\] (104)

Extremizing this last expression with respect to \(c\) leads to

\[=(_{k=1}^{K}_{k}_{k})}{(_{k=1}^{K}_{k} _{k})(1-)+}= (_{k=1}^{K}_{k} d_{}()_{k}) {1-}}{(_{k=1}^{K}_{k} d_{}( )_{k})(1-)+}.\] (105)

which is exactly (10), as stated in Corollary 3.4. The associated MSE can be then readily computed as

\[_{} =_{}_{,}\|-(^{}+)\|^{2}\] \[=(1-)^{2}_{k=1}^{K}_{k} (\|_{k}\|^{2}+_{k})+^{2}\] \[=(1-)^{2}_{k=1}^{K}_{k} ( d_{}()_{k}^{2}+d d_{}()_{k })+^{2}\] \[=_{}.\] (106)

This completes the proof of the asymptotic characterization of the rescaling component \(\).

Bottleneck network component \(\)For \(\), one needs to go through the derivation presented in Appendix A by setting \(b=0\) from the beginning. It is straightforward to realize that the derivation goes through sensibly unaltered, and the only differing step is that the quadratic potential \(_{}\) is now a constant. One therefore only needs to set \(\) in all the formulae of Conjecture 3.3 to access sharp asymptotics for \(\). This concludes the derivation of Corollary 3.4Derivation of Corollary 3.5

We now turn to the derivation of Corollary 3.5. This corresponds to the limit \(=0,\) where the input of the auto-encoder is also the clean data point. In terms of equations, essentially, this means that all the integrals over \(,_{}\) in the derivation of Conjecture 3.3 as presented in Appendix A should be removed. For the sake of clarity, and because this is an important case of particular interest, we provide here a complete, succinct but self-contained, derivation in the case of RAEs. For technical reasons, we limit ourselves to \(_{2}\) regularization. Furthermore, note that the model of an RAE with a skip connection is trivial, since such an architecture can achieve perfect reconstruction accuracy simply by setting the skip connection strength to \(1\) and the weights to \(0\). Therefore, we consider an RAE without skip connection

\[f_{w}()=^{}}{}( {}}{}).\] (107)

### Derivation

The derivation of asymptotic characterizations for the metrics \(\) (5), \(\) (6) and summary statistics for the RAE (107) follow the same lines as for the full DAE (2), as presented in Appendix A. As in Appendix A, the first step is to evaluate the replicated partition function

\[_{}Z^{s}=_{a=1}^{s}d_{a}e^{- _{a=1}^{s}g(^{a})}_{=1}^{n}_{k=1}^{K}_{ k}_{}}_{a 1}^{s} \|_{}+-[_{a}^{}}{}(_{a}(_{k}+)}{}) ]\|^{2}}_{()}.\] (108)

The exponent \(()\) can be expanded as

\[e^{-_{a=1}^{}[_ {1}[_{a}_{a}^{}}{d}.(_{a}( _{k}+)}{})^{ 2}]-2( _{a}(_{k}+)}{})^{} _{a}((_{k}+))}{}+(1_{k}1^{2}+1 \|^{2}+2_{k}^{})]}.\] (109)

Therefore,

\[_{}()=_{k=1}^{K}_{k}e^{-\|_{k}\|^{2}}}{(2)^{}_{k}}}^{} (_{k}^{-1}+_{1}d)-_{k} {}}}_{P_{}()}\] \[ e^{-_{a=1}^{}[_{1} [_{a}_{a}^{}}{d}.(_{a}(_{k}+)}{})^{ 2}]-2(_{a}( _{k}+)}{})^{}_{a}(_{k} +)}{}]}.\] (110)

The effective prior over the \(\) is therefore Gaussian with dressed mean and covariance

\[P_{}()=(;, ),\] (111)

where

\[^{-1}=_{k}^{-1}+ s_{d}, =- s_{k}.\] (112)

Observe that this mean is \((s)\) and can be safely neglected. Then

\[_{}()= _{k=1}^{K}_{k}\|_{k}\|^ {2}+^{}^{-1}}}{ _{k}()^{-}}}\] \[_{a=1}^{ }[_{1}[_{a}_{a}^{}}{d}.( _{a}(_{k}+)}{})^{ 2}]-2( _{a}(_{k}+)}{})^{}_{ a}(_{k}+)}{}]}_{P_{}()}\] (113)

Again, like Appendix A, we introduce the local fields:

\[_{a}^{}=_{a}(-)}{}, h_{a}^{k}=_{a}_{k}}{},\] (114)which have correlation

\[_{a}^{}_{b}^{}_{a}_{k}_{b}^{}}{d}.\] (115)

We used the leading order of the covariance \(\). One therefore has to introduce the order parameters:

\[Q_{ab} =_{a}_{b}^{}}{d}^{p p}, S_{ab}^{k} =_{a}_{k}_{b}^{}}{d}^{p  p}, m_{a}^{k}=_{a}_{k}}{}^{p}.\] (116)

The distribution of the local fields \(_{a}^{}\) is then simply:

\[(_{a}^{})_{a=1}^{s}(0,S^{k}).\] (117)

Going back to the computation, \((a)\) can be rewritten as

\[ e^{-}{a}_{1}^{} [Q_{ab}(m_{a}^{k}+_{a}^{})^{ 2 }]-}{a^{2}_{1}}[-2(m_{ a}^{k}+_{a}^{})^{}(m_{a}^{k}_{a}^{}) ]}_{\{_{a}^{}\}_{a=1}^{s}}.\] (118)

Introducing Dirac functions enforcing the definitions of \(Q_{ab},m_{a}\) brings the replicated function in the following form:

\[Z^{s}= _{a=1}^{s}db_{a}_{a,b}dQ_{ab}d_{ab}_{k=1 }^{K}_{a=1}^{s}dm_{a}d_{a}_{k=1}^{K}_{a,b}dS_{ab}^{k}d ^{k}abe^{s}\] \[Q_{ab}_{ab}-d _{k=1}^{K}_{a b}S_{ab}^{k}_{ab}^{k}- _{k=1}^{K}_{a b}_{a}dm_{a}^{k} _{a}^{k}}}}_{e^{_{ab}_{t}}}\] \[^{s}d_{a}e^{-_{a  b}_{ab}_{a}_{b}^{}+_{k=1}^{K} _{a b}_{ab}^{k}_{a}_{k}_{b}^{}+ _{k=1}^{K}_{a b}_{ab}^{k}_{a}_{k}_{b}^{}+_{k=1}^{K}_{a}_{a}^{ k}_{a}_{k}}}_{e^{_{ab}_{w}}}\] \[e^{-}{2}\|\|^{2}}^{K}_{k}_{k}(C^{-1})^{}}}(a )]^{ d}}_{e^{ xd^{2}_{}+_{s}d_{y}}}.\] (119)

As in Appendix A, we have introduced the trace, entropic and energetic potentials \(_{t},_{w},_{y}\). Since all the integrands scale exponentially (or faster) with \(d\), this integral can be computed using a saddle-point method. To proceed further, note that the energetic term encompasses two types of terms, scaling like \(sd^{2}\) and \(sd\). More precisely,

\[[_{k=1}^{K}_{k}_{k} (C^{-1})^{}}}(a)]^{ d} =[1-sd_{k=1}^{K}_{k}(_{k}(C^{-1})^{}})+s_{k=1}^{K}_{k} (a)]^{ d}\] \[=e^{-s d^{2}_{k=1}^{K}_{k}d (_{k}(C^{-1})^{}})+s d _{k=1}^{K}_{k}(a)}\] (120)

Remark that in contrast to Appendix A, the quadratic term is _constant_ with respect to the network parameters, and therefore one no longer needs to discuss its extremization. Following Appendix A, we assume the RS ansatz

\[ a, m_{a}^{k}=m_{k},_{a}^{k}=_{k}\] \[ a,b, S_{ab}^{k}=(r_{k}-q_{k})_{ab}+q_{k},_{ab}^{k}=- (_{k}}{2}+_{k})_{ab}+_{k},\] \[ a,b, Q_{ab}=(r-q)_{ab}+q,_{ab}=-( }{2}+)_{ab}+.\] (121)

In the following, we sequentially simplify the potentials \(_{w},_{y},_{t}\) under this ansatz.

Entropic potentialIt is convenient to introduce before proceeding the variance order parameters

\[+, _{k}_{k}+_{k}.\] (122)

The entropic potential can then be expressed as

\[e^{ sd_{w}}\] \[=_{a=1}^{s}d_{a}e^{-_{a}g(^{ })-_{a=1}^{s}[_{k} {w}_{a}_{k}_{a}^{}]+_{k=1}^{ K}_{a,b}[_{a}_{b}^{} ]+_{k=1}^{K}_{a=1}^{s}} _{k}^{}_{a}^{}_{k}}\] \[ e^{-_{k=1}^{K} _{a=1}^{s}[_{k}_{a}_ {k}_{a}^{}]+_{k=1}^{K}_{a,b }[_{k}_{a}_{k}_{b}^{}]}\] \[= D_{0}_{k=1}^{K}D_{k}\] \[[ de^{- g()- [^{}+_{k=1}^{K}_{k}_{k}^{}]+(_{k=1}^{K} }_{k}^{}+_{k=1}^{K}_{k} (_{k}_{k})^{}+_{0}( {q}_{d})^{})}]^{s}\] \[=_{0}_{k=1}^{K}D_{k}}_{  D}\] \[[ dwe^{- g()- [_{d}+_{k=1}^{K}_{k}_{k}]+(_{k=1}^{K}} _{k}^{}+_{k=1}^{K}_{k}(_{k} _{k})^{}+_{0}(_{d}) ^{})}]^{s}.\] (123)

Therefore

\[_{w}\] \[ D[ dwe^{- g()- [_{d}+_{k=1}^{K }_{k}_{k}]+(_{k=1}^ {K}}_{k}^{}+_{k=1}^{K}_{k} (_{k}_{k})^{}+_{0}( {q}_{d})^{})}].\] (124)

Energetic potentialThe inverse of \(S_{k}\) can be computed like in Appendix A, leading to the following expression for \((a)\):

\[(a) =_{^{p}}^{s}d_{a}^{ }}{(2)^{m/2}}}e^{-_{a=1}^{s}( _{a}^{})^{}(_{k}-_{k})_{a}^{}- {2}_{a,b}(_{a}^{})^{}_{k}_{b}-_{a=1}^{s}(*)}\] \[=_{^{p}}}} [_{^{p}}d_{}e^{-_{}^{ }V_{k}^{-1}_{}+_{}^{}V_{k}^{-1}q_{k}^{} -(*)}]^{s}\] \[=_{^{p}}D[(_{ };q_{k}^{},V)e^{-(*)}]^{s}\] \[=1+s_{^{p}}D[( _{};q_{k}^{},V)e^{-(*)}],\] (125)

where we noted with capital \(D\) an integral over \((0,_{p})\). Therefore

\[_{y}=_{k=1}^{K}_{k}_{^{p}}D[ (_{};q_{k}^{},V)e^{-(*)}].\] (126)Zero-temperature limitIn this subsection, we take the zero temperature \(\) limit. Rescaling

\[V_{k}  V_{k}, _{k} _{k}, ^{2}_{k} _{k}, _{k} _{k}\] (127)

one has that

\[_{w}= \![(_{d}+_{k=1}^{K}_{k}_{k})^{-1} (_{d}+_{k=1}^{K}_{k} _{k}+d(_{k=1}^{K}_{k}_{k} ^{})^{ 2})]\] \[-_{}_{r}(),\] (128)

where we introduced the Moreau envelope

\[_{r}()\] \[=_{}\{\|( _{d}+_{k=1}^{K}_{k}_{k} )\}^{-(_{d}+ _{k=1}^{K}_{k}_{k})^{-}((_{d})^{}_{0 }+_{k=1}^{K}(_{k}_{k})^{} _{k}+_{k=1}^{K}_{k}_{k}^{ })\|^{2}+g()\}.\] (129)

For the case of \(_{2}\) regularization, the Moreau envelope presents a simple expression, and the entropy potential assumes the simple form

\[_{w} =+\![(_{m }_{d}+_{d}+_{k=1}^{K}_{k}_{k})^{-1}(_{d}+_{k=1}^{K}_{k}_{k}+d( _{k=1}^{K}_{k}_{k}^{})^{ 2})]\!.\] (130)

The energetic potential also simplifies in this limit to

\[_{y} =-_{k=1}^{K}_{k}_{}_{k}()\] (131)

where

\[_{k}() =_{x,y}\!\{\![V_{k}^{- 1}(y-q_{k}^{}-m_{k})^{ 2}]+\! [q(y)^{ 2}]-2(y)^{}y\}\!.\] (132)

Trace potentialIt is immediate to see that the trace potential can be expressed as

\[_{t} =q-V}{2}+_{k=1}^{K}( \![_{k}q_{k}]-\![ _{k}V_{k}])-_{k=1}^{K}m_{k}_{k}.\] (133)

Then the total free entropy for RAEs (107) reads

\[= _{q,m,V,,,,\{q_{k},m_{k},_{k},_{k},_{k},_{k}\}_{k=1}^{K}}q- V}{2}+_{k=1}^{K}(\![_{k}q_{k}]-\![_{k}V_{k}])- _{k=1}^{K}m_{k}_{k}\] \[-_{k=1}^{K}_{k}_{}_ {k}()+ d(,)\![( +_{k}_{k})^{-1}(_{k=1}^{K}_{k }_{k}+_{1 k,j K}_{j}_{k}_{j} _{k}^{})]\!.\] (134)

Comparing to the free energy derived in Appendix A, it is immediate to see that the Moreau envelope \(_{k}\) for the RAE just corresponds to removing the second term from the DAE \(_{k}\), and setting \(x=0\). The disappearance of the second term in the Moreau envelope has the effect to kill the \(V\)-dependence, resulting in \(=0\) when extremizing with respect to \(V\). These observations, in addition to the already taken limits \(,b=0\), finish the derivation of Corollary 3.5.

### Reconstruction MSE

Having obtained a sharp asymptotic characterization of the summary statistics for the RAE, we now turn to the learning metrics. It is easy to see that the formula for the cosine similarity \(\) (6) carries over unchanged from the DAE case, see Appendix A. We therefore focus on the test MSE:

\[_{} =_{k,,}\|_{k}+-( }^{}}{}(}(_{k }+)}{}))\|^{2}\] \[=^{K}_{k}(\|_{k}\|^{2}+ \,_{k})}_{_{o}^{}}+_{k =1}^{K}_{k}_{z}^{(0,1)}[q (m_{k}+}z)^{ 2}]\] \[-2_{k=1}^{K}_{k}_{u,v}^{(0,1) }[(m_{k}+}u)]^{}(m_{k}+}u)\] (135)

### RAEs are limited by the PCA MSE

We close this Appendix by showing how the RAE Corollary 3.5 recovers the well-known Eckart-Young theorem . The difference \(-_{o}^{}\) for an RAE learning to reconstruct a binary, isotropic homoscedastic mixture (see Fig. 1) is shown in Fig. 14, from a training set with sample complexity \(=1\). Because \(-_{o}^{}\) is essentially a correction to the leading term \(_{o}^{}\), estimating this difference is slightly more challenging numerically, whence rather large error bars. Nevertheless, it can be observed that the agreement between the theory (solid blue line) and the simulations (dots) gathered from training an \(p=1\) RAE using the Pytorch implementation of Adam is still very good. In terms of cosine similarity and learnt weight norm, the agreement is perfect, see Fig. 3.5 (right). For comparison, the performance of PCA reconstruction (crosses) has been overlayed on the RAE curves. Importantly, observe that in terms of MSE, PCA reconstruction always leads to smaller MSE, in agreement with the well-known Eckart-Young theorem . From the cosine similarity, it can be seen that the RAE essentially learns the principal component of the train set, see the discussions in e.g. .

We finally stress that, in contrast to all previous figures, the \(x\) axis in Fig. 3.5 is the _sample complexity_\(\), rather than the noise level \(\). As a matter of fact, while our result recover the well-known fact that the MSE of RAEs is limited (lower-bounded) by the PCA test error, Corollary 3.5 allows us to investigate how the RAE approaches the PCAperformance as a function of the number of training samples, a characterization which has importantly been missing from previous studies for non-linear RAEs .