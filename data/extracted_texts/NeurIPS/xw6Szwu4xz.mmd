# Personalized Dictionary Learning for

Heterogeneous Datasets

 Geyu Liang

University of Michigan

Ann Arbor, MI 48109

lianggy@umich.edu

&Naichen Shi

University of Michigan

Ann Arbor, MI 48109

naichens@umich.edu

&Raed Al Kontar

University of Michigan

Ann Arbor, MI 48109

alkontar@umich.edu

&Salar Fattahi

University of Michigan

Ann Arbor, MI 48109

fattahi@umich.edu

###### Abstract

We introduce a relevant yet challenging problem named _Personalized Dictionary Learning (PerDL)_, where the goal is to learn sparse linear representations from heterogeneous datasets that share some commonality. In PerDL, we model each dataset's shared and unique features as _global_ and _local_ dictionaries. Challenges for PerDL not only are inherited from classical dictionary learning (DL), but also arise due to the unknown nature of the shared and unique features. In this paper, we rigorously formulate this problem and provide conditions under which the global and local dictionaries can be provably disentangled. Under these conditions, we provide a meta-algorithm called _Personalized Matching and Averaging (PerMA)_ that can recover both global and local dictionaries from heterogeneous datasets. PerMA is highly efficient; it converges to the ground truth at a linear rate under suitable conditions. Moreover, it automatically borrows strength from strong learners to improve the prediction of weak learners. As a general framework for extracting global and local dictionaries, we show the application of PerDL in different learning tasks, such as training with imbalanced datasets and video surveillance.

## 1 Introduction

Given a set of \(n\) signals \(=[_{1},,_{n}]^{d n}\), _dictionary learning_ (DL) aims to find a _dictionary_\(^{d r}\) and a corresponding _code_\(=[_{1},,_{n}]^{r n}\) such that: (1) each data sample \(_{i}\) can be written as \(_{i}=_{i}\) for \(1 i n\), and (2) the code \(\) has as few nonzero elements as possible. The columns of the dictionary \(\), also known as _atoms_, encode the "common features" whose linear combinations form the data samples. A typical approach to solve DL is via the following optimization problem:

\[_{,}\|-\|_{F}^{2}+ \|\|_{_{q}},\] (DL)

Here \(\|\|_{_{q}}\) is often modeled as a \(_{1}\)-norm (Arora et al., 2015; Agarwal et al., 2016) or \(_{0}\)-(pseudo-)norm (Spielman et al., 2012; Liang et al., 2022) and has the role of promoting sparsity in the estimated sparse code. Due to its effective feature extraction and representation, DL has found immense applications in data analytics, with applications ranging from clustering and classification (Ramirez et al. (2010); Tosic and Frossard (2011)), to image denoising (Li et al. (2011)), to document detection(Kasiviswanathan et al. (2012)), to medical imaging (Zhao et al. (2021)), and to many others.

However, the existing formulations of DL hinge on a critical assumption: the homogeneity of the data. It is assumed that the samples share the same set of features (atoms) collected in a _single_ dictionary \(D\). This assumption, however, is challenged in practice as the data is typically collected and processed in heterogeneous edge devices (clients). These clients (for instance, smartphones and wearable devices) operate in different conditions (Kontar et al., 2017) while sharing some congruity. Accordingly, the collected datasets are naturally endowed with heterogeneous features while potentially sharing common ones. In such a setting, the classical formulation of DL faces a major dilemma: on the one hand, a reasonably-sized dictionary (with a moderate number of atoms \(r\)) may overlook the unique features specific to different clients. On the other hand, collecting both shared and unique features in a single enlarged dictionary (with a large number of atoms \(r\)) may lead to computational, privacy, and identifiability issues. In addition, both approaches fail to provide information about "what is shared and unique" which may offer standalone intrinsic value and can potentially be exploited for improved clustering, classification and anomaly detection, amongst others.

With the goal of addressing data heterogeneity in dictionary learning, in this paper, we propose _personalized dictionary learning_ (PerDL); a framework that can untangle and recover _global_ and _local (unique)_ dictionaries from heterogeneous datasets. The global dictionary, which collects atoms that are shared among all clients, represents the common patterns among datasets and serves as a conduit of collaboration in our framework. The local dictionaries, on the other hand, provide the necessary flexibility for our model to accommodate data heterogeneity.

We summarize our contributions below:

* _Identifiability of local and global atoms:_ We provide conditions under which the local and global dictionaries can be provably identified and separated by solving a nonconvex optimization problem. At a high level, our identifiability conditions entail that the true dictionaries are column-wise incoherent, and the local atoms do not have a significant alignment along any nonzero vector.
* _Federated meta-algorithm:_ We present a fully federated meta-algorithm, called PerMA (Algorithm 1), for solving PerDL. PerMA only requires communicating the estimated dictionaries among the clients, thereby circumventing the need for sharing any raw data. A key property of PerMA is its ability to untangle global and local dictionaries by casting it as a series of shortest path problems over a _directed acyclic graph_ (DAG).
* _Theoretical guarantees:_ We prove that, under moderate conditions on the generative model and the clients, PerMA enjoys a linear convergence to the ground truth up to a statistical error. Additionally, PerMA borrows strength from strong clients to improve the performance of the weak ones. More concretely, through collaboration, our framework provides weak clients with the extra benefits of _averaged_ initial condition, convergence rate, and final statistical error.
* _Practical performance:_ We showcase the performance of PerMA on a synthetic dataset, as well as different realistic learning tasks, such as training with imbalanced datasets and video surveillance. These experiments highlight that our method can effectively extract shared global features while preserving unique local ones, ultimately improving performance through collaboration.

### Related Works

Dictionary LearningSpielman et al. (2012); Liang et al. (2022) provide conditions under which DL can be provably solved, provided that the dictionary is a square matrix (also known as _complete_ DL). For the more complex case of _overcomplete_ DL with \(r>d\), Arora et al. (2014, 2015); Agarwal et al. (2016) show that alternating minimization achieves desirable statistical and convergence guarantees. Inspired by recent results on the benign landscape of matrix factorization (Ge et al., 2017; Fattahi and Sojoudi, 2020), Sun et al. (2016) show that a smoothed variant of DL is devoid of spurious local solutions. In contrast, distributed or federated variants of DL are far less explored. Huang et al. (2022); Gkillas et al. (2022) study DL in the federated setting. However, they do not provide any provable guarantees on their proposed method.

Federated Learning & PersonalizationRecent years have seen explosive interest in federated learning (FL) following the seminal paper on federated averaging (McMahan et al., 2017). Literaturealong this line has primarily focused on predictive modeling using deep neural networks (DNN), be it through enabling faster convergence (Karimireddy et al., 2020), improving aggregation schemes at a central server (Wang et al., 2020), promoting fairness across all clients (Yue et al., 2022) or protecting against potential adversaries (Bhagoji et al., 2019). A comprehensive survey of existing methodology can be found in (Kontar et al., 2021). More recently, the focus has been placed on tackling heterogeneity across client datasets through personalization. The key idea is to allow each client to retain their own tailored model instead of learning one model that fits everyone. Approaches along this line either split weights of a DNN into shared and unique ones and collaborate to learn the shared weights (Liang et al., 2020; T Dinh et al., 2020), or follow a train-then-personalize approach where a global model is learned and fine-tuned locally, often iteratively (Li et al., 2021). Again such models have mainly focused on predictive models. Whilst this literature abounds, personalization that aims to identify what is shared and unique across datasets is very limited. Very recently, personalized PCA (Shi and Kontar, 2022) was proposed to address this challenge through identifiably extracting shared and unique principal components using distributed Riemannian gradient descent. However, PCA cannot accommodate sparsity in representation and requires orthogonality constraints that may limit its application. In contrast, our work considers a broader setting via sparse dictionary learning.

Notation.For a matrix \(\), we use \(\|\|_{2}\), \(\|\|_{F}\), \(\|\|_{1,2}\), and \(\|\|_{1}\) to denote its spectral norm, Frobenius norm, the maximum of its column-wise 2-norm, and the element-wise 1-norm of \(\), respectively. We use \(_{i}\) to indicate that it belongs to client \(i\). Moreover, we use \(_{(i)}\) to denote the \(i\)-th column of \(\). We use \((n)\) to denote the set of \(n n\) signed permutation matrices. We define \([n]=\{1,2,,n\}\).

## 2PerDL: Personalized Dictionary Learning

In PerDL, we are given \(N\) clients, each with \(n_{i}\) samples collected in \(_{i}^{d n_{i}}\) and generated as a sparse linear combination of \(r^{g}\) global atoms and \(r_{i}^{}\) local atoms:

\[_{i}=_{i}^{*}_{i}^{*}, _{i}^{*}=^{g*}&_{i}^{l*}, 1=1,,N.\] (1)

Here \(^{g*}^{d r^{g}}\) denotes a global dictionary that captures the common features shared among all clients, whereas \(_{i}^{l*}^{d r_{i}^{l}}\) denotes the local dictionary specific to each client. Let \(r_{i}=r^{g}+r_{i}^{l}\) denote the total number of atoms in \(_{i}^{*}\). Without loss of generality, we assume the columns of \(_{i}^{*}\) have unit \(_{2}\)-norm.1 The goal in PerDL is to recover \(^{g*}\) and \(\{_{i}^{l*}\}_{i=1}^{N}\), as well as the sparse codes \(\{_{i}^{*}\}_{i=1}^{N}\), given the datasets \(\{_{i}\}_{i=1}^{N}\). Before presenting our approach for solving PerDL, we first consider the following fundamental question: under what conditions is the recovery of the dictionaries \(^{g*},\{_{i}^{l*}\}_{i=1}^{N}\) and sparse codes \(\{_{i}^{*}\}_{i=1}^{N}\) well-posed?

To answer this question, we first note that it is only possible to recover the dictionaries and sparse codes up to a signed permutation: given any signed permutation matrix \((r_{i})\), the dictionary-code pairs \((_{i},_{i})\) and \((_{i}_{i},_{i}^{}_{i})\) are equivalent. This invariance with respect to signed permutation gives rise to an equivalent class of true solutions with a size that grows exponentially with the dimension. To guarantee the recovery of a solution from this equivalent class, we need the \(\)-incoherency of the true dictionaries.

**Assumption 1** (\(\)-incoherency).: _For each client \(1 i N\), the dictionary \(_{i}^{*}\) is \(\)-incoherent for some constant \(>0\), that is,_

\[_{j,k}_{i}^{*}_{(j)}\,, _{i}^{*}_{(k)}}.\] (2)

Assumption 1 is standard in dictionary learning (Agarwal et al. (2016); Arora et al. (2015); Chatterji and Bartlett (2017)) and was independently introduced by Fuchs (2005); Tropp (2006) in signal processing and Zhao and Yu (2006); Meinshausen and Buhlmann (2006) in statistics. Intuitively, it requires the atoms in each dictionary to be approximately orthogonal. To see the necessity of this assumption, consider a scenario where two atoms of \(_{i}^{*}\) are perfectly aligned (i.e., \(=\)). In this case, using either of these two atoms achieve a similar effect in reconstructing \(_{i}\), contributing to the ill-posedness of the problem.

Our next assumption guarantees the separation of local dictionaries from the global one in \(\). First, we introduce several signed permutation-invariant distance metrics for dictionaries, which will be useful for later analysis.

**Definition 1**.: _For two dictionaries \(_{1},_{2}^{d r}\), we define their signed permutation-invariant \(_{1,2}\)-distance and \(_{2}\)-distance as follows:_

\[d_{1,2}(_{1},_{2}) :=_{(r)}\|_{1}-_{2}\|_{1,2},\] (3) \[d_{2}(_{1},_{2}) :=_{(r)}\|_{1}-_{2}\|_{2}.\] (4)

_Furthermore, suppose \(^{*}=_{(r)}\|_{1} -_{2}\|_{1,2}\). For any \(1 j r\), we define_

\[d_{2,(j)}(_{1},_{2}):=\|(_{1}^{*}-_{2})_{(j)}\|_{2}.\] (5)

**Assumption 2** (\(\)-identifiablity).: _The local dictionaries \(\{_{i}^{l*}\}_{i=1}^{N}\) are \(\)-identifiable for some constant \(0<<1\), that is, there exists no vector \(^{d}\) with \(\|\|_{2}=1\) such that_

\[_{1 i N}_{1 j r_{l}}d_{2}((_{i}^{l *})_{(j)},).\] (6)

Suppose there exists a unit-norm \(\) satisfying (6) for some small \(>0\). This implies that \(\) is sufficiently close to at least one atom from each local dictionary. Indeed, one may treat this atom as part of the global dictionary, thereby violating the identifiability of local and global dictionaries. On the other hand, the infeasibility of (6) for large \(>0\) implies that the local dictionaries are sufficiently dispersed, which in turn facilitates their identification.

With the above assumptions in place, we are ready to present our proposed optimization problem for solving \(\):

\[_{^{g},\{_{i}\},\{_{i}\}}_{i=1}^{N}\| _{i}-_{i}_{i}\|_{F}^{2}+_{i=1}^{N}\| _{i}\|_{_{q}},\ \ (_{i})_{(1:r^{q})}=^{g}  1 i N.\] (PerDL-NCVX)

For each client \(i\), \(\)-NCVX aims to recover a dictionary-code pair \((_{i},_{i})\) that match \(_{i}\) under the constraint that dictionaries for individual clients share the same global components.

## 3 Meta-algorithm of Solving \(\)

In this section, we introduce our meta-algorithm (Algorithm 1) for solving \(\)-NCVX, which we call _Personalized Matching and Averaging_ (\(\)). In what follows, we explain the steps of \(\):

Local initialization (Step 3):\(\) starts with a warm-start step where each client runs their own initialization scheme to obtain \(_{i}^{(0)}\). This step is necessary even for the classical DL to put the initial point inside a basin of attraction of the ground truth. Several spectral methods were proposed to provide a theoretically good initialization(Arora et al., 2015; Agarwal et al., 2016), while in practice, it is reported that random initialization followed by a few iterations of alternating minimization approach will suffice (Ravishankar et al., 2020; Liang et al., 2022).

Global matching scheme (Step 6)Given the clients' initial dictionaries, our global matching scheme separates the global and local parts of each dictionary by solving a series of shortest path problems on an auxiliary graph. Then, it obtains a refined estimate of the global dictionary via simple averaging. A detailed explanation of this step is provided in the next section.

Dictionary update at each client (Step 10)During each communication round, the clients refine their own dictionary based on the available data, the aggregated global dictionary, and the previous estimate of their local dictionary. A detailed explanation of this step is provided in the next section.

Global aggregation (Step 13)At the end of each round, the server updates the clients' estimate of the global dictionary by computing their average.

A distinguishing property of PerMA is that it only requires the clients to communicate their dictionaries and not their sparse codes. In fact, after the global matching step on the initial dictionaries, the clients only need to communicate their global dictionaries, keeping their local dictionaries private.

### Global Matching and Local Updates

In this section, we provide detailed implementations of global_matching and local_update subroutines in PerMA (Algorithm 1).

Given the initial approximations of the clients' dictionaries \(\{_{i}^{(0)}\}_{i=1}^{N}\), global_matching seeks to identify and aggregate the global dictionary by extracting the similarities among the atoms of \(\{_{i}^{(0)}\}\). To identify the global components, one approach is to solve the following optimization problem

\[_{_{i}}_{i=1}^{N-1}\|(_{i}^{(0)} _{i})_{(1:r)}-(_{i+1}^{(0)} _{i+1})_{(1:r)}\|_{2} _{i}(r_{i}) 1 i N.\] (7)

The above optimization aims to obtain the appropriate signed permutation matrices \(\{_{i}\}_{i=1}^{N}\) that align the first \(r^{g}\) atoms of the permuted dictionaries. In the ideal regime where \(_{i}^{(0)}=_{i}^{*},1 i N\), the optimal solution \(\{_{i}^{*}\}_{i=1}^{N}\) yields a zero objective value and satisfies \((_{i}^{(0)}_{i}^{*})_{(1:r^{g})}=^ {g*}\). However, there are two main challenges with the above optimization. First, it is a nonconvex, combinatorial problem over the discrete sets \(\{(r_{i})\}\). Second, the initial dictionaries may not

Figure 1: A schematic diagram for global_matching (Algorithm 2). At each iteration, we find the shortest path from \(s\) to \(t\) (highlighted with red), estimate one atom of \(^{g*}\) using all passed vertices and remove the path (including the vertices) from \(\).

coincide with their true counterparts. To address the first challenge, we show that the optimal solution to the optimization (7) can be efficiently obtained by solving a series of shortest path problems defined over an auxiliary graph. To alleviate the second challenge, we show that our proposed algorithm is robust against possible errors in the initial dictionaries.

Consider a weighted \(N\)-layered _directed acyclic graph_ (DAG) \(\) with \(r_{i}\) nodes in layer \(i\) representing the \(r_{i}\) atoms in \(_{i}\). We connect any node \(a\) from layer \(i\) to any node \(b\) from layer \(i+1\) with a directed edge with weight \(w(a,b)=d_{2}((_{i})_{a},(_{i+1})_{b})\). We add a _source_ node \(s\) and connect it to all nodes in layer 1 with weight 0. Similarly, we include a _terminal_ node \(t\) and connect all nodes in layer \(N\) to \(t\) with weight 0. A schematic construction of this graph is presented in Figure 1. Given the constructed graph, Algorithm 2 aims to solve (7) by running \(r^{g}\) rounds of the shortest path problem: at each round, the algorithm identifies the most aligned atoms in the initial dictionaries by obtaining the shortest path from \(s\) to \(t\). Then it removes the used nodes in the path for the next round. The correctness and robustness of the proposed algorithm are established in the next theorem.

```
1:Input:\(\{_{i}^{(0)}\}_{i=1}^{N}\) and \(r^{g}\).
2: Construct the weighted \(N\)-layer DAG \(\) described in Section 3.1.
3: Initialize \(\{_{i}\}_{i=1}^{N}\) as empty sets and \(^{g,(0)}\) as an empty matrix.
4:for\(j=1,,r^{g}\)do
5: Find the shortest path \(=(s,(_{1}^{(0)})_{(_{1})},(_{1}^{(0) })_{(_{2})},,(_{N}^{(0)})_{(_{N})},t)\).
6: Add \(_{i=1}^{N}((_{i}^{(0)}) _{(_{i})},(_{1}^{(0)})_{(_{1})})( _{i}^{(0)})_{(_{i})}\) as a new column of \(^{g,(0)}\).
7: Add \(_{i}\) to \(_{i}\) for every \(i=1,,N\).
8: Remove \(\) from \(\).
9:endfor
10: Set \(_{i}^{I,(0)}=(_{i}^{(0)})_{([r_{i}] _{i})}\) for every \(i=1,,N\).
11:return\((^{g,(0)},\{_{i}^{I,(0)}\}_{i=1}^{N})\). ```

**Algorithm 2**global_matching

**Theorem 1** (Correctness and robustness of global_matching).: _Suppose \(\{_{i}^{*}\}_{i=1}^{N}\) are \(\)-incoherent (Assumption 1) and \(\)-identifable (Assumption 2). Suppose the initial dictionaries \(\{_{i}^{(0)}\}_{i=1}^{N}\) satisfy \(d_{1,2}(_{i}^{(0)},_{i}^{*})_{i}\) with \(4_{i=1}^{N}_{i}\{}},\}\). Then, the output of Algorithm 2 satisfies:_

\[d_{1,2}(^{g,(0)},^{g*})_{i=1 }^{N}_{i}, d_{1,2}(_{i}^{I,(0)}, _{i}^{l*})_{i}, 1 i N.\] (8)

According to the above theorem, global_matching can robustly separate the clients' initial dictionaries into global and local parts, provided that the aggregated error in the initial dictionaries is below a threshold. Specific initialization schemes that can satisfy the condition of Theorem 1 include Algorithm 1 from Agarwal et al. (2013) and Algorithm 3 from Arora et al. (2015). We also remark that since the constructed graph is a DAG, the shortest path problem can be solved in time linear in the number of edges, which is \((r_{1}+r_{N}+_{i=1}^{N-1}r_{i}r_{i+1})\), via a simple labeling algorithm (see, e.g., (Ahuja et al., 1988, Chapter 4.4)). Since we need to solve the shortest path problem \(r^{g}\) times, this brings the computational complexity of Algorithm 2 to \((r^{g}Nr_{}^{2})\), where \(r_{}=_{i}r_{i}\).

Given the initial local and global dictionaries, the clients progressively refine their estimates by applying \(T\) rounds of local_update (Algorithm 3). At a high level, each client runs a single iteration of a _linearly convergent algorithm_\(_{i}\) (see Definition 2), followed by an alignment step that determines the global atoms of the updated dictionary using \(^{g,(t)}\) as a "reference point". Notably, our implementation of local_update is adaptive to different DL algorithms. This flexibility is indeed intentional to provide a versatile meta-algorithm for clients with different DL algorithms.

```
1:Input:\(_{i}^{(t)}=[^{g,(t)}_{i}^{l,(t)}], _{i}\)
2:\(_{i}^{(t+1)}=_{i}(_{i},_{i}^{(t)})\)// Oneiteration of a linearly-convergent algorithm.
3:Initialize \(\) as an empty set and \(^{r^{g} r^{g}}\) as an all-zero matrix.
4:for\(j=1,...,r^{g}\)do
5: Find \(k^{*}=_{k}d_{2}((^{g,(t)})_{(j)},( _{i}^{(t+1)})_{(k)})\).
6: Append \(k^{*}\) to \(\).
7: Set \((i,i)\)-th entry of \(\) to \(((^{g,(t)})_{(j)}, (_{i}^{(t+1)})_{(k^{*})})\).
8:endfor
9:Output:\(_{i}^{g,(t+1)}=(_{i}^{(t+1)})_{()} \) and \(_{i}^{l,(t+1)}=(_{i}^{(t+1)})_{([r_{i}] )}\). ```

**Algorithm 3**local_update

## 4 Theoretical Guarantees

In this section, we show that our proposed meta-algorithm provably solves \(\) under suitable initialization, identifiability, and algorithmic conditions. To achieve this goal, we first present the definition of a linearly-convergent DL algorithm.

**Definition 2**.: _Given a generative model \(=^{*}^{*}\), a DL algorithm \(\) is called \((,,)\)-linearly convergent for some parameters \(,>0\) and \(0<<1\) if, for any \(^{d r}\) such that \(d_{1,2}(,^{*})\), the output of one iteration \(^{+}=(,)\), satisfies_

\[d_{2,(j)}(^{+},^{*}) d_{2,(j)}( ,^{*})+, 1 j r.\] (9)

One notable linearly convergent algorithm is introduced by (Arora et al., 2015, Algorithm 5); we will discuss this algorithm in more detail in the appendix. Assuming all clients are equipped with linearly convergent algorithms, our next theorem establishes the convergence of \(\).

**Theorem 2** (Convergence of \(\)).: _Suppose \(\{_{i}^{*}\}_{i=1}^{N}\) are \(\)-incoherent (Assumption 1) and \(\)-identifiable (Assumption 2). Suppose, for every client \(i\), the DL algorithm \(_{i}\) used in local_update (Algorithm 3) is \((_{i},_{i},_{i})\)-linearly convergent with \(4_{i=1}^{N}_{i}\{}},\}\). Assume the initial dictionaries \(\{_{i}^{(0)}\}_{i=1}^{N}\) satisfy:_

\[d_{1,2}(_{i=1}^{N}_{i}^{g,(0)}, ^{g*})_{1 i N}_{i}, d_{1,2}( _{i}^{l,(0)},_{i}^{l*})_{i}, i=1,,N.\] (10)

_Then, for every \(t 0\), \(\) (Algorithm 1) satisfies_

\[d_{1,2}(^{g,(t)},^{g*}) (_{i=1}^{N}_{i})d_{1,2}( ^{g,(0)},^{g*})+_{i=1}^{N}_{i},\] (11) \[d_{1,2}(_{i}^{l,(t)},_{i}^{l*}) _{i}d_{1,2}(^{l,(0)},_{i}^{l*} )+_{i}, 1 i N.\] (12)

The above theorem sheds light on a number of key benefits of \(\):

Relaxed initial condition for weak clients.Our algorithm relaxes the initial condition on the global dictionaries. In particular, it only requires the average of the initial global dictionaries to be close to the true global dictionary. Consequently, it enjoys a provable convergence guarantee even if some of the clients do not provide a high-quality initial dictionary to the server.

Improved convergence rate for slow clients.During the course of the algorithm, the global dictionary error decreases at an average rate of \(_{i=1}^{N}_{i}\), improving upon the convergence rate of the slow clients.

Improved statistical error for weak clients.A linearly convergent DL algorithm \(_{i}\) stops making progress upon reaching a neighborhood around the true dictionary \(_{i}^{*}\) with radius \(O(_{})\). This type of guarantee is common among DL algorithms (Arora et al., 2015; Liang et al., 2022) and often corresponds to their statistical error. In light of this, \(\) improves the performance of weak clients (i.e., clients with weak statistical guarantees) by borrowing strength from strong ones.

## 5 Numerical Experiments

In this section, we showcase the effectiveness of Algorithm 1 using synthetic and real data. All experiments are performed on a MacBook Pro 2021 with the Apple M1 Pro chip and 16GB unified memory for a serial implementation in MATLAB 2022a. Due to limited space, we will only provide the high-level motivation and implication of our experiments. We defer implementation details and comparisons with the existing methods to the appendix.

### Synthetic Dataset

In this section, we validate our theoretical results on a synthetic dataset. We consider ten clients, each with a dataset generated according to the model 1. The details of our construction are presented in the appendix. Specifically, we compare the performances of two strategies: (1) _independent strategy_, where each client solves DL without any collaboration, and (2) _collaborative strategy_, where clients collaboratively learn the ground truth dictionaries by solving \(\) via the proposed meta-algorithm \(\). We initialize both strategies using the same \(\{_{i}^{(0)}\}_{i=1}^{N}\). The initial dictionaries are obtained via a warm-up method proposed in (Liang et al., 2022, Algorithm 4). For a fair comparison between independent and collaborative strategies, we use the same DL algorithm ((Liang et al., 2022, Algorithm 1)) for different clients. Note that in the independent strategy, the clients cannot separate global from local dictionaries. Nonetheless, to evaluate their performance, we collect the atoms that best align with the true global dictionary \(^{s*}\) and treat them as the estimated global dictionaries. As can be seen in Figure 2, three out of ten clients are weak learners and fail to recover the global dictionary with desirable accuracy. On the contrary, in the collaborative strategy, all clients recover the same global dictionary almost exactly.

### Training with Imbalanced Data

In this section, we showcase the application of \(\) in training with imbalanced datasets. We consider an image reconstruction task on MNIST dataset. This dataset corresponds to a set of handwritten digits (see the first row of Figure 3). The goal is to recover a _single_ concise global dictionary that can be used to reconstruct the original handwritten digits as accurately as possible. In particular, we study a setting where the clients have _imbalanced label distributions_. Indeed, data imbalance can drastically bias the performance of the trained model in favor of the majority groups, while hurting its performance on the minority groups (Leevy et al., 2018; Thabtah et al., 2020). Here, we consider a setting where the clients have highly imbalanced datasets, where \(90\%\) of their samples have the same label. More specifically, for client \(i\), we assume that \(90\%\) of the samples correspond to the handwritten digit "\(i\)", with the remaining \(10\%\) corresponding to other digits. The second row of Figure 3 shows the effect of data imbalance on the performance of the recovered dictionary on a single client, when the clients do not collaborate. The last row of Figure 3 shows the improved performance of the recovered dictionary via \(\) on the same client. Our experiment clearly shows the ability of \(\) to effectively address the data imbalance issue by combining the strengths of different clients.

Figure 2: \(\) improves the accuracy of the recovered global dictionary for _all_ clients, even if some (three out of ten) are weak learners.

### Surveillance Video Dataset

As a proof of concept, we consider a video surveillance task, where the goal is to separate the background from moving objects. Our data is collected from Vacavant et al. (2013) (see the first column of Figure 4). As these frames are taken from one surveillance camera, they share the same background corresponding to the global features we aim to extract. The frames also exhibit heterogeneity as moving objects therein are different from the background. This problem can indeed be modeled as an instance of PerDL, where each video frame can be assigned to a "client", with the global dictionary capturing the background and local dictionaries modeling the moving objects. We solve PerDL by applying PerMA to obtain a global dictionary and several local dictionaries for this dataset. Figure 4 shows the reconstructed background and moving objects via the recovered global and local dictionaries. Our results clearly show the ability of our proposed framework to separate global and local features. 2

Figure 4: PerMA effectively separates the background from moving objects in video frames. Here we reconstruct the surveillance video frames using global and local dictionaries learned by PerMA. We reconstruct the frames using only \(50\) atoms from the combined dictionaries.

Figure 3: PerMA improves training with imbalanced datasets. We consider the image reconstruction task on the imbalanced MNIST dataset using only five atoms from a learned global dictionary. The first row corresponds to the original images. The second row is based on the dictionary learned on a single client with an imbalanced dataset. The third row shows the improved performance of the learned dictionary using our proposed method on the same client.

Social Impact, Limitations and Future Directions

Our novel approach for personalized dictionary learning presents a versatile solution with immediate applications across various domains, such as video surveillance and object detection. While these applications offer valuable benefits, they also bring to the forefront ethical and societal concerns, particularly concerning privacy, bias, and potential misuse. In the context of video surveillance, deploying object detection algorithms may inadvertently capture private information, leading to concerns about violating individuals' right to privacy. However, it is important to emphasize that our proposed algorithm is specifically designed to focus on separating unique and common features within the data, without delving into the realm of personal information. Thus, it adheres to ethical principles by ensuring that private data is not processed or used without explicit consent. Bias is another critical aspect that necessitates careful consideration in the deployment of object detection algorithms. Biases can manifest in various forms, such as underrepresentation or misclassification of certain groups, leading to discriminatory outcomes. Our approach acknowledges the importance of mitigating biases by solely focusing on the distinction between common and unique features, rather than introducing any inherent bias into the learning process. Furthermore, the potential misuse of object detection algorithms in unauthorized surveillance or invasive tracking scenarios raises valid concerns. As responsible researchers, we are cognizant of such risks and stress that our proposed algorithm is meant to be deployed in a controlled and legitimate manner, adhering to appropriate regulations and ethical guidelines.

Even though our meta-algorithm PerMA enjoys strong theoretical guarantees and practical performance, there are still several avenues for improvement. For instance, the theoretical success of PerMA, especially the Global Matching step, relies on an individual initial error of \(O(1/N)\). In other words, the initial error should decrease as the number of clients grows. As a future work, we plan to relax such dependency via a more delicate analysis. We also note that imposing an upper bound on the initial error is not unique to our setting, as virtually all existing algorithms for classical (non-personalized) dictionary learning require certain conditions on the initial error. On the other hand, once the assumption on the initial error is satisfied, our meta-algorithm achieves a final error with the same dependency on \(d\) (the dimensionality of the data) and \(n\) (the number of samples) as the state-of-the-art algorithms for classical dictionary learning (Agarwal et al. (2016), Arora et al. (2015)). Remarkably, this implies that personalization is achieved without incurring any additional cost on \(d\) or \(n\), making PerMA highly efficient and competitive in its performance.