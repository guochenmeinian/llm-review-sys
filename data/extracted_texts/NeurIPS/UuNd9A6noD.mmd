# Bayesian Optimisation of Functions on Graphs

Xingchen Wan, Pierre Osselin, Henry Kenlay

Binxin Ru, Michael A. Osborne, Xiaowen Dong

Department of Engineering Science, University of Oxford

{xwan,osselinp,kenlay,robin,mosb,xdong}@robots.ox.ac.uk

Equal contribution.

###### Abstract

The increasing availability of graph-structured data motivates the task of optimising over functions defined on the node set of graphs. Traditional graph search algorithms can be applied in this case, but they may be sample-inefficient and do not make use of information about the function values; on the other hand, Bayesian optimisation is a class of promising black-box solvers with superior sample efficiency, but it has scarcely been applied to such novel setups. To fill this gap, we propose a novel Bayesian optimisation framework that optimises over functions defined on generic, large-scale and potentially unknown graphs. Through the learning of suitable kernels on graphs, our framework has the advantage of adapting to the behaviour of the target function. The local modelling approach further guarantees the efficiency of our method. Extensive experiments on both synthetic and real-world graphs demonstrate the effectiveness of the proposed optimisation framework.

## 1 Introduction

Data collected in a network environment, such as transportation, financial, social, and biological networks, have become pervasive in modern data analysis and processing tasks. Mathematically, such data can be modelled as functions defined on the node set of graphs that represent the networks. This then poses a new type of optimisation problem over functions on graphs, i.e. searching for the node that possesses the most extreme value of the function. Real-world examples of such optimisation tasks are abundant. For instance, if the function measures the amount of delay at different locations in an infrastructure network, one may think about identifying network bottlenecks; if it measures the amount of influencing power users have in a social network platform, one may be interested in finding the most influential users; if it measures the time when individuals were infected in an epidemiological contact network, an important task would be to identify "patient zero" of the disease.

Optimisation of functions on graphs is challenging. Graphs are an example of discrete domains, and conventional algorithms, which are mainly designed for continuous spaces, do not apply straightforwardly. Real-world graphs are often extremely large and sometimes may not even be fully observable. Finally, the target function, such as in the examples given above, is often a black-box function that is expensive to evaluate at the node level and may exhibit complex behaviour on the graph.

Traditional methods to traverse the graph, such as breadth-first search (BFS) or depth-first search (DFS) , are heuristics that may be adopted in this setting for small-scale graphs, but inefficient to deal with large-scale real-world graphs and complex functions. Furthermore, these search methods only rely on the graph topology and ignore the function on the graph, which can be exploited to make the search more efficient. On the other hand, Bayesian optimisation (BO)  is a sample-efficient sequential optimisation technique with proven successes in various domains and is suitable for solving black-box, expensive-to-evaluate optimisation problems. However, while BO has been combined with graph-related settings, e.g. optimising for _graph structures_ (i.e. the _individual configurations_that we optimise for are graphs) in the context of neural architecture search [20; 34], graph adversarial examples  or molecule design , it has not been applied to the problem of optimising over functions on graphs (i.e. the _search space_ is a graph and the configurations we optimise for are _nodes_ in the graph). The closest attempt was COMBO , which is a framework designed for a specific purpose, i.e. combinatorial optimisation, where the search space is modelled as a synthetic graph restricted to one that can be expressed as a Cartesian product of subgraphs. It also assumes that the graph structure is available and that the function values are smooth in the graph space to facilitate using a diffusion kernel. All these assumptions may not hold in the case of optimisation over generic functions on real-world graphs.

We address these limitations in our work, and our main contributions are as follows: we consider the problem setting of optimising functions that are supported by the node set of a potentially generic, large-scale, and potentially unknown graph - _a setup that is by itself novel_ to the best of our knowledge in the BO literature. We then propose a novel BO framework that effectively optimises in such a problem domain with 1) appropriate kernels to handle the aforementioned graph search space derived by spectral learning on the local subgraph structure and is therefore flexible in terms of adapting to the behaviour of the target function, and 2) efficient local modelling to handle the challenges that the graphs in question can be large and/or not completely known a-priori. Finally, we deploy our method in various novel optimisation tasks on both synthetic and real-world graphs and demonstrate that it achieves very competitive results against baselines.

## 2 Preliminaries

BO is a zeroth-order (i.e. gradient-free) and sample-efficient sequential optimisation algorithm that aims to find the global optimum \(x^{*}\) of a black-box function defined over search space \(\): \(x^{*}=_{x}f(x)\) (we consider a minimisation problem without loss of generality). BO uses a statistical surrogate model to approximate the objective function and an acquisition function \((x)\) to balance exploitation and exploration under the principle of optimism in the face of uncertainty. At the \(t\)-th iteration of BO, the objective function is queried with a configuration \(x_{t}\) and returns an output \(y_{t}\), a potentially noisy estimator of the objective function \(y_{t}=(x_{t})+,(0,_{n}^{2})\) where \(_{n}^{2}\) is the noise variance. The statistical surrogate is trained on the observed data up to \(t\)-th observation \(_{t}=\{(x_{i},y_{i})\}_{i=1}^{t}\) to approximate the objective function. In this work, we use a Gaussian process (GP) surrogate, which is query-efficient and gives analytic posterior mean and variance estimates on the unknown configurations. Formally, a GP is denoted as \(f(x)(m(x),k(x,x^{}))\), where \(m(x)\) and \(k(x,x^{})\) are the mean function and the covariance function (or the _kernel_), respectively. While the mean function is often set to zero or a simple function, the covariance function encodes our belief on the property of the function we would like to model, the choice of which is a crucial design decision when using GP. The covariance function typically has some kernel hyperparameters \(\) and are typically optimised by maximising the _log-marginal

Figure 1: Illustration of one iteration of _BayesOptG_ on an example graph. **(a)** At iteration \(t\), we construct a local subgraph \(_{t}\) centred around \(v_{t}^{*}\) whose nodes are marked in orange-red, with darker shade denoting a shorter distance to \(v_{t}^{*}\), the best node seen so far (marked in black), and nodes outside \(_{t}\) are marked in grey. The readers are referred to ยง3.2 for the details; **(b)** we place a GP surrogate with the covariance function defined in ยง3.1 on \(_{t}\) and pick the maximiser of the acquisition function (the acquisition function values are marked in shades of blue, with a darker shade denoting a higher acquisition value) as the node to query for iteration \(t+1\) (\(v_{t+1}\)) (ยง3.2) and **(c)** if querying \(v_{t+1}\) leads to a better objective function value (\(f(v_{t+1})<f(v_{t}^{*})\), assuming minimisation), the neighbourhood around it is selected as the new subgraph \(_{t+1}\). The process continues until convergence or a pre-set number of evaluations is reached.

likelihood_ (the readers are referred to detailed derivations in Rasmussen ). With \(m()\) and \(k(,)\) defined, at iteration \(t\), with \(_{t}=[x_{1},...,x_{t}]^{}\) and the corresponding output vector \(_{1:t}=[y_{1},...,y_{t}]^{}\), a GP gives analytic posterior mean \((x_{t+1}|_{t})=(x_{t+1},_{1:t})_{1: t}^{-1}_{1:t}\) and variance \(k(x_{t+1},x_{t+1}^{}|_{t})=k(x_{t+1},x_{t+1}^{})- (x_{t+1},_{1:t})_{1:t}^{-1}( _{1:t},x_{t+1}^{}))\) estimates on an unseen configuration \(x_{t+1}\), where \([_{1:t}]_{i,j}=k(x_{i},x_{j})\) is the \((i,j)\)-th element of the Gram matrix induced on the \((i,j)\)-th training samples by \(k(,)\), the covariance function. With the posterior mean and variance predictions, the acquisition function is optimised at each iteration to recommend the configuration (or a batch of configurations for the case of batch BO) to be evaluated for the \(t+1\)-th iteration. For additional details of BO, the readers are referred to Frazier .

## 3 Bayesian Optimisation on Graphs

Problem setting.Formally, we consider a novel setup with a graph \(G\) defined by \((,)\), where \(=\{v_{i}\}_{i=1}^{n}\) are the nodes and \(=\{e_{k}\}_{k=1}^{m}\) are the edges where each edge \(e_{k}=\{v_{i^{}},v_{j^{}}\}\) connects nodes \(v_{i^{}}\) and \(v_{j^{}}\). The topology \(G\) may be succinctly represented by an adjacency matrix \(\{0,1\}^{n n}\); in our case, \(m\) and \(n\) are potentially large, and the overall topology is not necessarily fully revealed to the search algorithm at running time. It is worth noting that, for simplicity, we focus on the setup of _undirected, unweighted_ graph where elements of \(\) are binary and symmetrical (i.e. \(A_{ij}=A_{ji}\))2. Specifically, we aim to optimise the black-box, typically expensive objective function that is defined _over the nodes_, i.e. it assigns a scalar value to each node in the graph. In other words, the search space (i.e. \(\) in SS2) in our setup is the set of nodes \(\) and the goal of the optimisation problem is to find the configuration(s) (i.e. \(x\) in SS2) that minimise the objective function \(v^{*}=_{v}f(v)\).

Promises and challenges of BO on graphs.We argue that BO is particularly appealing under the described setup as (1) it is known to be query-efficient, making it suitable for optimising expensive functions, and (2) it is fully black-box and gradient-free; indeed, we often can only observe inputs and outputs of many real-world functions, and gradients may not even exist in a practical setup. However, there exist various challenges in our setup that make the adaptation of BO highly non-trivial, and despite the prevalence of problems that may be modelled as such and the successes of BO, it has not been extended to the optimisation of functions on graphs. Some examples of such challenges are:

1. [label=()]
2. **Exotic search space.** BO is conventionally applied in continuous Euclidean spaces, whereas we focus on discrete graph search spaces. The differences in search space imply that key notions to BO, such as the similarity between two configurations and expected smoothness of objective functions (the latter is often used as a key criterion in selecting the covariance function to use), could differ significantly. For example, while comparing the similarity between two points in a Euclidean space requires only the computation of simple distance metrics (like \(_{2}\) distance), careful thinking is required to achieve the same in comparing two nodes in a graph that additionally accounts for the topological properties of the graph.
3. **Scalability.** Real-world graphs such as citation and social networks can often feature a very large number of nodes while not presenting convenient properties such as the graph Cartesian product assumption in Oh et al.  to accelerate computations. Therefore, it is a technical challenge to adapt BO in this setting while still retaining computational tractability.
4. **Imperfect knowledge on the graph structure.** Related to the previous point, it may also be prohibitively expensive or even impossible to obtain perfect, complete knowledge on real-world graphs beforehand or at any point during optimisation (e.g. obtaining the full contact tracing graph for epidemiology modelling); as such, any prospective method should be able to handle the situation where the graph structure is only revealed incrementally, on-the-fly.

Overview of _BayesOptG._To effectively address these challenges while retaining the desirable properties of BO, we propose to extend BO to this novel setup and are, to the best of our knowledge, the first to do so. To achieve that, we propose Bayesian Optimisation on Graphs, or _BayesOptG_ in short, and an illustration of the overall procedure is shown in Fig. 1, and an algorithmic description is available in Algorithm 1. For the rest of this section, we discuss in detail the key components of _BayesOptG_ and how the method specifically addresses the challenges identified above.

### Kernels for BO on Graphs

Kernel design.Covariance functions are crucial to GP-based BO. To use BO in our setup, a covariance function that gives a principled similarity measure between two nodes \(\{v_{i},v_{j}\}\) is required to interpolate signals on the graph effectively. In this paper, we study several kernels, including both those proposed in the literature (e.g. the _diffusion kernel on graphs_ and the _graph Matern kernel_) and two novel kernels designed by us. Following Smola & Kondor , all the kernels investigated can be considered in a general formulation. Formally, for a generic graph \(=(},})\) with \(\) nodes and \(\) edges, we define \(}:=(-}^{- {2}}}}^{-})\), where \(\) is the identity matrix of order \(\), \(}\) and \(}\) are the _adjacency matrix_ and the _degree matrix_ of \(\), respectively (the term after \(\) is known as the _normalised Laplacian matrix_ with eigenvalues in the range of \(\); we scale it such that the eigenvalues are in the range of \(\)). It is worth emphasising that here we use notations with the tilde (e.g., \(,\) and \(\)) to make the distinction that this graph is, in general, different from, and is typically a subgraph of, the overall graph \(G\) discussed at the start of this section, which might be too large or not be fully available at the start of the optimisation; we defer a full discussion on this in SS3.2. We further note that \(}=^{}\) with \(=(_{1},...,_{})\) and \(=[_{1},...,_{}]\), where \(\{_{1},...,_{}\}\) are the eigenvalues of \(\) sorted in an ascending order and \(\{_{1},...,_{}\}\) are the corresponding (unit) eigenvectors.

Let \(p,q\{1,...,\}\) be two indices over the nodes of \(\), we may express our covariance function to compute the covariance between an arbitrary pair of nodes \(v_{p},v_{q}\) in terms of a _regularisation function_ of eigenvalues \(r(_{i})\,\,i\{1,...,\}\), as described in Smola & Kondor :

\[k(v_{p},v_{q})=_{i=1}^{}r^{-1}(_{i})u_{i}[p]u_{i}[q],\] (1)

where \(u_{i}[p]\) and \(u_{i}[q]\) are the \(p\)-th and \(q\)-th elements of the \(i\)-th eigenvector \(_{i}\). The specific functional form of \(r(_{i})\) depends on the kernel choice, and the kernels considered in this work are listed in Table 1. We note that all kernels encode the smoothness of the function on the local subgraph \(\). In particular, the diffusion kernel has been adopted in Oh et al. ; the polynomial and Matern kernels are inspired by recent work in the literature of graph signal processing ; finally, the sum-of-inverse polynomials kernel is designed as a variant of the polynomial kernel: in terms of the regularisation function, it can be interpreted as (while ignoring \(\)) a scaled harmonic mean of the different degree components of the polynomial kernel. We next discuss the behaviours of these kernels from the perspective of kernel hyperparameters.

Kernel hyperparameters.\(:=[_{0},...,_{-1}]^{}_{ 0 }^{}\) (for polynomial and sum-of-inverse polynomials) or \([_{1},...,_{}]^{}_{ 0}^{}\) (for the diffusion kernel) define the characteristics of the kernel. We constrain \(\) in both kernels to be non-negative to ensure the positive semi-definiteness of the resulting covariance matrix and are learned jointly via GP log-marginal likelihood optimisation. The parameter \(\) controls the mean-square differentiability in the classical GP literature with the Matern kernel. The polynomial and the sum-of-inverse polynomials kernels in Table 1 feature an additional hyperparameter of _kernel order_\(_{ 0}\). We set it to be \(\{5,\}\) where _diameter_ is thelength of the shortest path between the most distanced pair of nodes in \(\) (a thorough ablation study on \(\) is presented in App. D.). We argue that this allows both kernels to strike a balance between expressiveness, as all eigenvalues contained in the graphs are used in full without truncation, and regularity, as fewer kernel hyperparameters need to be learned. This is in contrast to, for example, diffusion kernels on graphs in Table 1, which typically has to learn \(\) hyperparameters for a graph of size \(\), whose optimisation can be prone to overfitting. To address this issue, previous works often had to resort to strong sparsity priors (e.g. horseshoe priors ) and approximately marginalising with Monte Carlo samplers that significantly increase the computational costs and reduce the scalability of the algorithm . In contrast, by constraining the order of the polynomials to a smaller value, the resulting kernels may adapt to the behaviour of the target function and can be better regularised against overfitting in certain problems, as we will validate in SS5.

### Tractable Optimisation via Local Modelling

As discussed previously, it is a technical challenge to develop high-performing yet efficient methods in 1) large, real-world graphs (e.g. social network graphs) and 2) graphs for which it is expensive, or even impossible, to obtain complete topological information beforehand (e.g. if we model the interactions between individuals as a graph, the complete topology of the graph may only be obtained after exhaustive interviews and contact tracing with all people involved). The previous work in Oh et al.  cannot handle the second scenario and only addresses the first issue by assuming a certain structure of the graph (e.g. the Cartesian product of subgraphs), but these techniques are not applicable when we are dealing with a general graph \(G\).

To address the dual challenges, and inspired by trust region-based BO methods [7; 13; 43; 10; 44], we adapt and simplify the techniques to our use case: we propose to leverage _local modelling_ by focusing on a subset of nodes that evolves as the optimisation progresses. At iteration \(t\{1,...,T\}\), assuming the collection of our observed configurations and outputs is \(_{t}=\{v_{t^{}},y_{t^{}}\}_{t^{}=1}^{t}\), we first find the node that leads to the best objective function so far \(v_{t}^{*}=_{v\{v_{t}\}_{t^{}}^{1}}\{v\}\). We then use Algorithm 2 to select a _neighbourhood_ around \(v_{t}^{*}\) that is a subgraph of the overall graph \(G\): \(_{t} G\) with \(Q\) number of nodes (we will discuss how to choose \(Q\) in the next paragraph), in a procedure similar to the neighbourhood sampling in the GraphSAGE framework  as illustrated in Fig. 2: in particular, during sampling, the closer nodes to \(v_{t}^{*}\) takes precedence over further nodes - we only sample the latter if the subgraph consisting of \(v_{t}^{*}\) and the closer nodes has fewer than \(Q\) nodes; hence the local subgraph is a form of an _ego-network_ of the central node \(v_{t}^{*}\). We then only impose the GP and compute the covariance matrix _over this subgraph only_: First, this effectively limits the computational cost - note that the time complexity in our case depends on _both_ the number of training examples \(N\) and the size of the graph \(\) we impose the GP on (\((^{3}+N^{3})\)), assuming a naive eigen-decomposition algorithm. Second, it also effectively addresses the setup where the

  Kernel & Regularisation function \(r(_{i})\) & Kernel function \(K(,)\) \\  Diffusion\({}^{}\)[37; 27] & \((_{i}_{i})\) & \(_{i=1}^{}(-_{i}_{i})_{i}_{i} ^{}\) \\ Polynomial\({}^{*}\) & \(_{=0}^{-1}_{}_{i}^{}+\) & \(_{i=1}^{}_{=0}^{-1}_{}_{ i}^{}+^{-1}_{i}_{i}^{}\) \\ Sum-of-inverse & \(_{=0}^{-1}_{i}^{}+ }^{-1}\) & \(_{i=1}^{}_{=0}^{-1} _{i}^{}+}_{i}_{i}^{}\) \\ Mapram  & \(+_{i}^{}\) & \(_{i=1}^{}+_{i}^{-}_{i} _{i}^{}\) \\  ^{}\) Can be ARD or non-ARD: for ARD, \(\{_{i}\}_{i=1}^{}\) coefficients are learned; for non-ARD, a single, scalar \(\) is learned.} \\ ^{*}\{_{}\}_{=0}^{-1}\) coefficients to be learned. \(\): small positive constant (e.g. \(10^{-8}\)). \(\): order of kernel.} \\ 

Table 1: Kernels considered in terms of the regularisation function \(r(_{i})\). We derive the semi-definiteness of polynomial and sum-of-inverse polynomial kernels in App. A.

Figure 2: Subgraphs \(_{t}\) determined by Algorithm 2, marked in red, with a darker shade denoting a closer distance to the central node \(v_{t}^{*}=_{v\{v_{t}\}_{t^{}=1}^{t}}f(v)\) in the figure, marked in black), for a high-degree node (**Left**) and a node far from high-degree node (**Right**). Note for the latter case, the local subgraph can include nodes that are much further away.

entire \(\) is not available a-priori, as we only need to query and reveal the topological structure of the subgraph \(_{t}\)_on the fly_.

```
1:Inputs: Best input up to iteration \(t\) since the last restart: \(v_{t}^{*}\), subgraph size \(Q\).
2:Output: local subgraph \(_{t}\) with \(Q\) nodes.
3: Initialise: \(}_{t}\{v_{t}^{*}\}\), \(h 1\).
4:while\(|}_{t}|<Q\)do
5: Find \(_{h}\), the \(h\)-hop neighbours of \(v_{t}^{*}\).
6:if\(|}_{t}|+|_{h}| Q\)then
7: Add all \(h\)-hop neighbours to \(}_{t}\): \(}_{t}}_{t}_{h}\).
8: Increment \(h\): \(h h+1\)
9:else
10: Randomly sample \(Q-|}|_{t}\) nodes from \(_{h}\) and add to \(}_{t}\)
11:endif
12:endwhile
13:return the subgraph \(_{t}\) induced by \(}_{t}\) (i.e. the ego-network). ```

**Algorithm 2** Selecting a local subgraph

It is worth noting that while conceptually influenced by previous trust region-BO methods, the local graph construction we use differs from these methods in several crucial aspects. First, we use a bespoke distance metric in the graph space. Second, whereas the purpose of trust regions in previous works is to alleviate over-exploration in high-dimensional spaces, local subgraphs in our case also uniquely serve the crucial purpose of allowing _BayesOptG_ to handle imperfect knowledge about the graphs, as we only need to reveal the topology of the subgraph (as opposed to the entire graph) at any given iteration. Lastly, we discussed, that using trust regions also improves scalability - this can be concretely exemplified by the massive speed-up shown in Fig. 3.

**Optimisation of the acquisition function.** With the local subgraph obtained, we then fit a GP surrogate with the covariance function defined in SS3.1 and optimise log-marginal likelihood. Given that the local search space in our case is finite (of size \(Q\)), we simply enumerate all nodes _within_\(_{t}\) to compute their _acquisition function_\(()\) values (which is computed from the predictive mean and variance of the GP surrogate) and pick the maximiser as the recommended location to query the _objective function_\(f()\) for iteration \(t+1\) as \(v_{t+1}=*{arg\,max}_{v}_{t}}(v)\). Any off-the-shelf acquisition function may be used, and we adopt expected improvement (EI)  in our experiments. It is worth noting that _BayesOptG_ is also fully compatible with existing approaches such as Kriging believer fantasisation  for batch BO.

## 4 Related Work

The setup we consider is by itself novel and largely under-explored. One of the few existing methods that can be used for optimisation over a graph search space is COMBO , where the search space is modelled as a graph that captures the relationship between different values for a group of

Figure 3: _Trust regions enable efficient optimisation on large graphs_: Wall-clock time with and without trust regions in _BayesOptG_ with different kernels over graphs of different sizes.

categorical variables. It is, therefore, designed explicitly for combinatorial optimisation. Several studies modified COMBO in various ways but followed essentially the same framework for similar tasks, e.g., optimisation over categorical variables [12; 19; 24]. Similarly, Ramachandram et al.  propose a specific graph construction to optimise multimodal fusion architectures. Our work differs from these studies in that: 1) we focus on optimisation over generic, large-scale and potentially unknown graphs; 2) the nodes of the graph are not limited to combinations of values for categorical variables and can represent any entities; 3) the kernel we propose is not limited to diffusion-based ones and can adapt to the behaviour of the function to be optimised. Finally, the _graph bandit setting_ ([5; 39; 38]) can be seen to be similar to ours in the sense that it also aims at finding extreme values associated with nodes in a graph. However, the bandit problem considers a stochastic setting where nodes are influenced in a probabilistic fashion, and the objective function is actively shaped by this process; in comparison, in our case, we consider an underlying deterministic and black-box function, which is more aligned with the classical BO setting. Moreover, both Valko et al.  and Thaker et al.  require _full_ graph access and require prohibitive operation on the full graph Laplacian (decomposition/inversion), whereas _BayesOptG_ may work on-the-fly with initially unknown graphs and is much more scalable thanks to the designs in SS3.2. Several works also leverage kernels on graphs to build Gaussian processes for graph-structured data [26; 41; 40; 46; 28; 4; 29]. While the kernels proposed in these approaches can, in theory, be used in a BO framework, these studies do not address the optimisation problem we consider.

Another line of work focuses on optimisation _over graph inputs_ (in contrast to a _graph search space_) where each input configuration itself is a graph. In contrast, in our case, each input configuration is a _node_. Examples of the former include Ru et al.  who model neural architectures as graphs and use Weisfeiler-Lehman kernels  to perform BO, and Wan et al. , who devise a BO agent for adversarial attack on graph classification models. Other representative examples include Kandasamy et al. , Korovina et al.  and Cui et al. [8; 9]. We emphasise that, while related, these works deal with a different setup and thus require a different method compared to the present work. For example, the kernels over graphs used in these methods typically aim to find vector embedding of graphs that account for their topologies. However, once the embedding is computed, standard Euclidean covariance functions (e.g., the dot product or squared-exponential kernel) are applied. On the other hand, in the present work, we aim to compute similarities over nodes, where topological information is crucial _during_ the covariance computation itself.

## 5 Experiments

We first validate the predictive power of the GPs with the adopted kernels on graphs and then demonstrate the optimisation performance of _BayesOptG_ in both synthetic and real-world tasks. We compare _BayesOptG_ against baselines, including random and local search optimisation algorithms as

Figure 4: Validation of predictive powers of kernels considered on a BA graph of size \(n=200\) nodes and parameter \(m=1\), with **(a)** function values on the nodes corresponding to elements of the eigenvector corresponding to the second smallest eigenvalue and **(b)** same as above, but corrupted with noise standard deviation \(=0.05\). The leftmost column shows the visualisation of the ground truth, and the right columns show the GP posterior mean and standard deviation (error bars) learned by the different kernels against ground truth with Spearman correlation \(\) and learned \(r^{-1}()\) (Eq. 1).

well as BFS and DFS. The description of these baselines is given in the App. B.2. In all figures, lines, and shades denote mean and standard error, respectively, across ten trials.

### Validating Predictive Power of Kernels

We first validate the predictive power of the adopted kernels in controlled regression experiments. To do so, we generate functions that are simply the eigenvectors of the graph Laplacian and compare the predictive performance of the kernels using three graph types: 2D grid, Barabasi-Albert (BA)  and Watts-Strogatz (WS) . We compare the performance in terms of validation error and show the results in Fig. 4 (results for other graph types are shown in App. C.1). We find that in the noiseless case, all kernels learn the underlying function effectively (except that the diffusion with ARD kernel learns a non-smooth transform on the spectrum due to its over-parameterisation, resulting in underestimations of the uncertainty in the noisy case). Still, the better-regularised kernels (described in SS3.1) are considerably more robust to noise corruption.

Figure 5: _Maximising centrality scores_ with the **BA** random graph model and \(n=1000\) nodes. Different graphs show different values of the BA hyperparameter \(m\{2,3,4\}\) and centrality metrics (betweenness/eigenvector centrality).

Figure 6: _Maximising centrality scores_ with the **WS** random graph model and \(n=2000\) nodes. Refer to Fig. 5 for legend and additional explanations.

Figure 7: _Synthetic test functions_ task with Ackley/Rosenbrock functions with noise standard deviation \(\{0.5,1\}\). Regrets shown in log-scale for Rosenbrock; refer to Fig. 5 for legend.

### Optimisation Tasks

We conduct experiments on a number of synthetic and real-life tasks that involve or imitate expensive optimisation, and we show all results in terms of _simple regret_ (i.e., the difference between the objective function value and the ground-truth optimum). We consider the following synthetic tasks:

* **Maximising centrality scores** (Fig. 5 and 6; Fig. 18 in App. C.2): we aim to find the node with maximum centrality measure, from a graph sampled from a random graph model. We consider both _eigenvector centrality_ and _betweenness centrality_ as the centrality metrics, and use BA and WS with different hyperparameters as the random graph-generating models. We consider graphs with sizes in the range of \(10^{3}\) in Fig. 5 and 6. In Fig. 18, we further scale the size of graphs considered to \(10^{6}\) nodes to demonstrate the scalability of our method in a large-scale setup.
* **Synthetic test functions** (Fig. 7): we optimise a suite of discretised versions of commonly used synthetic test functions (Ackley and Rosenbrock) on graphs defined as a 2D-grid in both noiseless and noisy setups. The readers are referred to App. B.3.2 for additional implementation details.

We consider the following real-life tasks:

* **Identifying the patient zero** (Fig. 8; Fig. 20 to 23 in App. C.3): we aim to find the "patient zero" of an epidemic in a contact network, who is to the person identified as the first carrier of a communicable disease in an epidemic outbreak. We use a real-world contact network based on Bluetooth proximity , and on top of simulating the epidemic process using the _SIR model_, the canonical compartmental model in epidemiology . The function values are the time instants when an individual is infected; the readers are referred to App. B.4.1 for more details of this task.
* **Identifying influential users in a social network** (Fig. 9): we aim to find the most influential user in a social network. There are multiple ways of defining the influence power of a user, and for simplicity, we follow the common practice of taking _node degree_ as a proxy of influence . We

Figure 8: _Identifying the patient zero_ task with different SIR model hyperparameters \(\{0.1,0.2\}\) and \(\{0.015,0.15\}\) and probability of recovery \(\) of 0. Refer to Fig. 20 โ 23 for experiments with other hyperparameter combinations.

Figure 10: _Team optimisation_ task with \(s\) (number of skills) \(\{2,4\}\) and \(\{1,10\}\) with Jaccard index threshold of 0.3 (refer to App. B.4.3 for explanations). Refer to Fig. 8 for legend and Fig. 24 โ 26 for experiments with other hyperparameter combinations.

use three real-world networks, namely the _Enron email network_, _Facebook page network_, and _Twitch social network_. The readers are referred to App. B.4.2 for more details.
* **Team optimisation** (Fig. 10; Fig. 24 to 26 in App. C.4): we design a task of optimising team structure, where the objective is to find a team that contains members who are experts in different skills, and their collective expertise represents a diverse skill set. In this case, the teams are modelled as nodes, and edges represent the a priori similarity between teams. While there are various possible ways to model these similarities, in our experiment, we consider that an edge exists between two nodes if the Jaccard index between the two sets of team members is greater than a certain threshold. We include additional details and a formal description of the objective function in the App. B.4.3.

We designed these tasks to imitate expensive but realistic black-box optimisation problems on which the use of Bayesian optimisation is ideal. For example, the _identifying patient zero_ task imitates real-life contact tracing. If executed in real life, each function evaluation requires expensive and potentially disruptive procedures like interviews about the individuals' travel history and the people they were in contact with. On the other hand, the _centrality maximisation_ & _identifying influential social network users_ problems mirror common online advertising tasks to identify the influential users without access to the full social network information (which would be near-impossible to obtain given the number of users). Real-life social media often limits how much one may interact with their platform through pay-per-use APIs or hard limits (e.g. upper limit of views). In either case, there is a strong reason to identify the influential users in the most query-efficient manner.

Discussions.In addition to the task-specific results, we further aggregate the performance of the different methods over all tasks in terms of relative ranking in Fig. 11. We find that within individual tasks and aggregated across the different tasks, _BayesOptG with any kernel choice_ generally outperforms all baselines in terms of efficiency, final converged values, or both. Specifically, _Random_ is simple but typically weak for larger graphs, except for very rough/noisy functions (like Ackley), or the variation in function values is generally small; _DFS_ and _BFS_ are relatively weak as they consider graph topology information only but not the node information (on which the objective function is defined) and can be sensitive to initialisation; _Local search_ is, on balance, the strongest baseline, and it does particularly well on smoother functions with fewer local minima.

As is the case for any GP-based method, the kernel choice impacts the performance, and the performance is stronger when the underlying assumptions of the kernel match the actual objective function. For example, diffusion kernels work well for patient zero identification (Fig. 8) and team optimisation (Fig. 10), as the underlying generative functions for both problems, are indeed smooth (in fact, the SIR model in disease propagation is heavily connected to diffusion processes). Diffusion without ARD further enforces isotropy, assuming the diffusion coefficient in all directions is the same, and thus typically underperforms except for team optimisation, where the generated graph is well structured and Ackley, which is indeed isotropic and symmetric. We recommend only if we know that the underlying function satisfies its rather stringent assumptions. Finally, the SumInverse and DiffARD kernels are generally better, as they offer more flexibility in learning from the data; _we recommend using one of these as default without prior knowledge suggesting otherwise_.

## 6 Conclusion

We address the problem of optimising over functions on graphs, a hitherto under-investigated problem. We demonstrate that BO, combined with learned kernels on graphs and efficient local modelling, provides an effective solution. The proposed framework works with generic, large-scale and potentially unknown graphs, a setting that existing BO methods cannot handle. Results on a diverse range of tasks support the effectiveness of the proposed method. The current work, however, only considers the case where the optimisation is over _nodes_; possible future works include extensions to related settings, such as optimising over functions defined on _edges_ and/or on hypergraphs.

Figure 11: Aggregated ranks of the methods (lower is better) vs. the number of evaluations averaged across all experiments.