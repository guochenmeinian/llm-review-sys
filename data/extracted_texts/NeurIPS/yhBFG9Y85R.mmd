# Visual Programming for

Text-to-Image Generation and Evaluation

 Jaemin Cho Abhay Zala Mohit Bansal

UNC Chapel Hill

{jmincho, aszala, mbansal}@cs.unc.edu

https://vp-t2i.github.io

###### Abstract

As large language models have demonstrated impressive performance in many domains, recent works have adopted language models (LMs) as controllers of visual modules for vision-and-language tasks. While existing work focuses on equipping LMs with visual understanding, we propose two novel interpretable/explainable visual programming frameworks for text-to-image (T2I) generation and evaluation. First, we introduce VPGen, an interpretable step-by-step T2I generation framework that decomposes T2I generation into three steps: object/count generation, layout generation, and image generation. We employ an LM to handle the first two steps (object/count generation and layout generation), by finetuning it on text-layout pairs. Our step-by-step T2I generation framework provides stronger spatial control than end-to-end models, the dominant approach for this task. Furthermore, we leverage the world knowledge of pretrained LMs, overcoming the limitation of previous layout-guided T2I works that can only handle predefined object classes. We demonstrate that our VPGen has improved control in counts/spatial relations/scales of objects than state-of-the-art T2I generation models. Second, we introduce VPEval, an interpretable and explainable evaluation framework for T2I generation based on visual programming. Unlike previous T2I evaluations with a single scoring model that is accurate in some skills but unreliable in others, VPEval produces evaluation programs that invoke a set of visual modules that are experts in different skills, and also provides visual+textual explanations of the evaluation results. Our analysis shows that VPEval provides a more human-correlated evaluation for skill-specific and open-ended prompts than widely used single model-based evaluation. We hope that our work encourages future progress on interpretable/explainable generation and evaluation for T2I models.

## 1 Introduction

Large language models (LLMs) have shown remarkable performance on many natural language processing tasks, such as question answering, summarization, and story generation [1; 2; 3; 4; 5; 6; 7; 8]. Recent works have shown that LLMs can also tackle certain vision-and-language tasks such as visual question answering and visual grounding, by generating visual programs that can control external visual modules and combine their outputs to get the final response [9; 10; 11; 12; 13; 14; 15]. However, no prior works have combined LLMs and different visual modules for the challenging text-to-image (T2I) generation task. Our work proposes two novel interpretable/explainable visual programming (VP) frameworks combining LLMs and visual modules for T2I generation and evaluation.

First, we introduce **VPGen** (Sec. 3), a new step-by-step T2I generation framework that decomposes the T2I task into three steps (object/count generation, layout generation, and image generation), where each step is implemented as a module and executed in an interpretable generation program, as shownin Fig. 1 (a). We employ Vicuna , a powerful publicly available LLM, to handle the first two steps (object/count generation and layout generation), by finetuning it on text-layout pairs from multiple datasets [17; 18; 19], resulting in improved layout control for T2I generation. For the last image generation step, we use off-the-shelf layout-to-image generation models, _e.g._, GLIGEN . Our generation framework provides more interpretable spatial control than the widely used end-to-end T2I models [21; 22; 23; 24]. Furthermore, VPGen leverages the world knowledge of pretrained LMs to understand unseen objects, overcoming the limitation of previous layout-guided T2I works that can only handle predefined object classes [25; 26; 27; 28]. In our analysis, we find that our VPGen (Vicuna+GLIGEN) could generate images more accurately following the text description (especially about object counts, spatial relations, and object sizes) than strong T2I generation baselines, such as Stable Diffusion  (see Sec. 5.2).

Next, we introduce **VPEval** (Sec. 4), a new interpretable/explainable T2I evaluation framework based on evaluation programs that invoke diverse visual modules to evaluate different T2I skills  (see Sec. 4.1), and also provide visual+textual explanations of the evaluation results. Previous T2I evaluation works have focused on measuring visual quality [29; 30] and image-text alignment with a single visual module based on text retrieval , CLIP cosine similarity [32; 33], captioning , object detection [34; 19; 35], and visual question answering (VQA) [36; 37]. However, we cannot interpret the reasoning behind the scoring; _i.e._, why CLIP assigns a higher score to an image-text pair than another. In addition, these modules are good at measuring some skills, but not reliable in other skills (_e.g._, VQA/CLIP models are not good at counting objects or accurately parsing text rendered in images). In VPEval, we break the evaluation process into a mixture of visual evaluation modules, resulting in an interpretable evaluation program. The evaluation modules are experts in different skills and provide visual+textual (_i.e._, multimodal) result/error explanations, as shown in Fig. 1 (b). We evaluate both multimodal LM and diffusion-based T2I models with two types of prompts: (1) skill-based prompts (Sec. 4.3) and (2) open-ended prompts (Sec. 4.4). Skill-based prompts evaluate a single skill per image, whereas open-ended prompts evaluate multiple skills per image. We adapt a large language model (GPT-3.5-Turbo ) to dynamically generate an evaluation program for each open-ended prompt, without requiring finetuning on expensive evaluation program annotations.

In our skill-based prompt experiments, while all T2I models faced challenges in the Count, Spatial, Scale, and Text Rendering skills, our VPGen shows higher scores in the Count, Spatial, and Scale skills, demonstrating its strong layout control. We also show a fine-grained sub-split analysis on where our VPGen gives an improvement over the baseline T2I models. In our open-ended prompt experiments, our VPGen achieves competitive results to the T2I baselines. Many of these open-ended prompts tend to focus more on objects and attributes, which, as shown by the skill-based evaluation, T2I models perform quite well in; however, when precise layouts and spatial relationships are needed, our VPGen framework performs better (Sec. 5.3). In our error analysis, we also find

Figure 1: Illustration of the proposed visual programming frameworks for text-to-image (T2I) generation and evaluation. In (a) VPGen, we first generate a list of objects, then object positions, and finally an image, by executing three modules step-by-step. In (b) VPEval, we use evaluation programs with a mixture of evaluation modules that handle different skills and provide visual+textual explanation of evaluation results.

that GLIGEN sometimes fails to properly generate images even when Vicuna generates the correct layouts, indicating that as better layout-to-image models become available, our VPGen framework will achieve better results (Sec. 5.3). We also provide an analysis of the generated evaluation programs from VPEval, which proved to be highly accurate and comprehensive in covering elements from the prompts (Sec. 5.4). For both types of prompts, our VPEval method shows a higher correlation to human evaluation than existing single model-based evaluation methods (Sec. 5.4).

Our contributions can be summarized as follows: (1) **VPGen** (Sec. 3), a new step-by-step T2I generation framework that decomposes the T2I task into three steps (object/count generation, layout generation, and image generation), where each step is implemented as a module and executed in an interpretable generation program; (2) **VPEval** (Sec. 4), a new interpretable/explainable T2I evaluation framework based on evaluation programs that execute diverse visual modules to evaluate different T2I skills and provide visual+textual explanations of the evaluation results; (3) comprehensive analysis of different T2I models, which demonstrates the strong layout control of VPGen and high human correlation of VPEval (Sec. 5). We will release T2I model-generated images, VPEval programs, and a public LM (finetuned for evaluation program generation using ChatGPT outputs). We hope our research fosters future work on interpretable/explainable generation and evaluation for T2I tasks.

## 2 Related Works

**Text-to-image generation models.** In the T2I generation task, models generate images from text. Early deep learning models used the Generative Adversarial Networks (GAN)  framework for this task [40; 41; 42; 31]. More recently, multimodal language models [43; 22] and diffusion models [44; 45; 21; 46] have gained popularity. Recent advances in multimodal language models such as Parti  and MUSE , and diffusion models like Stable Diffusion , UnCLIP , and Imagen , have demonstrated a high level of photorealism in zero-shot image generation.

**Bridging text-to-image generation with layouts.** One line of research decomposes the T2I generation task into two stages: text-to-layout generation and layout-to-image generation [25; 26; 27; 28]. However, the previous approaches focus on a set of predefined object classes by training a new layout predictor module from scratch and therefore cannot place new objects unseen during training. In contrast, our VPGen uses an LM to handle layout generation by generating objects/counts/positions in text, allowing flexible adaptation of pretrained LMs that can understand diverse region descriptions.

**Language models with visual modules.** Although large language models (LLMs) have shown a broad range of commonsense knowledge, most of them are trained only on text corpus and cannot understand image inputs to tackle vision-and-language (VL) tasks. Thus, recent works explore tackling VL tasks by solving sub-tasks with external visual modules and combining their outputs to obtain the final response [9; 10; 11; 12; 13; 14; 15]. The visual sub-tasks include describing images as text, finding image regions relevant to the text, editing images with text guidance, and obtaining answers from a VQA model. However, existing work focuses on converting visual inputs into text format so that LLMs can understand them. Our work is the first work using visual programming for interpretable and explainable T2I generation and evaluation.

**Evaluation of text-to-image generation models.** The text-to-image community has commonly used two types of automated evaluation metrics: visual quality and image-text alignment. For visual quality, Inception Score (IS)  and Frechet Inception Distance (FID)  have been widely used. For image-text alignment, previous work used a single model to calculate an alignment score for image-text pair, based on text retrieval , cosine similarity , captioning , object detection [34; 19; 35], and visual question answering (VQA) [36; 37]. In this work, we propose the first T2I evaluation framework VPEval, based on interpretable and explainable evaluation programs which execute a diverse set of visual modules (_e.g_., object detection, OCR, depth estimation, object counting). Our VPEval evaluation programs provide visual+textual explanations of the evaluation result and demonstrate a high correlation with human judgments.

## 3 VPGen: Visual Programming for Step-by-Step Text-to-Image Generation

We propose VPGen, a novel visual programming framework for interpretable step-by-step text-to-image (T2I) generation. As illustrated in Fig. 2, we decompose the text-to-image generation task into three steps: (1) object/count generation, (2) layout generation, and (3) image generation. In contrast to previous T2I generation works that use an intermediate layout prediction module [25; 26; 27; 28], VPGen represents all layouts (object description, object counts, and bounding boxes) in text, and employs an LM to handle the first two steps: (1) object/count generation and (2) layout generation. This makes it easy to adapt the knowledge of pretrained LMs and enables generating layouts of objects that are unseen during text-to-layout training (_e.g._, 'pikachu'). For layout representation, we choose the bounding box format because of its efficiency; bounding boxes generally require fewer tokens than other formats.1

**Two-step layout generation with LM.** Fig. 2 illustrates how our LM generates layouts in two steps: (1) object/count generation and (2) layout generation. For the first step, we represent the scene by enumerating objects and their counts, such as "obj1 (# of obj1) obj2 (# of obj2)". For the second step, following previous LM-based object detection works [49; 50], we normalize 'xyxy' format bounding box coordinates into \(\) and quantize them into 100 bins; a single object is represented as "obj(xmin,ymin,xmax,ymax)", where each coordinate is within \(\{0,,99\}\).

**Training layout-aware LM.** To obtain the layout-aware LM, we use Vicuna 13B , a public state-of-the-art language model finetuned from LLaMA . We use parameter-efficient finetuning with LoRA  to preserve the original knowledge of the LM and save memory during training and inference. We collect text-layout pair annotations from training sets of three public datasets: Flickr30K entities , MS COCO instances 2014 , and PaintSkills , totaling 1.2M examples. See appendix for more training details.

**Layout-to-Image Generation.** We use a recent layout-to-image generation model GLIGEN  for the final step - image generation. The layout-to-image model takes a list of regions (bounding boxes and text descriptions) as well as the original text prompt to generate an image.

## 4 VPEval: Visual Programming for Explainable Evaluation of Text-to-Image Generation

VPEval is a novel interpretable/explainable evaluation framework for T2I generation models, based on visual programming. Unlike existing T2I evaluation methods that compute image-text alignment scores with an end-to-end model, our evaluation provides an interpretable program and visual+textual explanations for the evaluation results, as shown in Figs. 3 and 5. We propose two types of evaluation prompts: (1) skill-based evaluation and (2) open-ended evaluation. In skill-based evaluation, we define five image generation skills and use a set of skill-specific prompts and evaluation programs, as illustrated in Fig. 3. In open-ended evaluation, we use a diverse set of prompts that require multiple image generation skills. We adopt a language model to dynamically generate an evaluation program for each text prompt, as shown in Fig. 5. In the following, we describe evaluation skills (Sec. 4.1), visual evaluation modules (Sec. 4.2), skill-based evaluation with visual programs (Sec. 4.3), and open-ended evaluation with visual program generator LM (Sec. 4.4).

Figure 2: Interpretable step-by-step text-to-image generation with VPGen. VPGen decomposes T2I generation into three steps: (1) object/count generation, (2) layout generation, and (3) image generation, and executes the three modules step-by-step.

### Evaluation Skills

Inspired by skill-based T2I analysis of PaintSkills , our VPEval measures five image generation skills: Object, Count, Spatial, Scale, and Text Rendering. Powered by evaluation programs with expert visual modules, our VPEval supports zero-shot evaluation of images (no finetuning of T2I models is required), detecting regions with free-form text prompts, new 3D spatial relations (front, behind), and new scale comparison and text rendering skills, which were not supported in PaintSkills . In Fig. 3, we illustrate the evaluation process for each skill.

**Object.** Given a prompt with an object (_e.g._, "a photo of a dog"), a T2I model should generate an image with that object present.

**Count.** Given a prompt with a certain number of an object (_e.g._, "3 dogs"), a T2I model should generate an image containing the specified number of objects.

**Spatial.** Given a prompt with two objects and a spatial relationship between them (_e.g._, "a spoon is in front of a potted plant"), a T2I model should generate an image that contains both objects with the correct spatial relations.

**Scale.** Given a prompt with two objects and a relative scale between them (_e.g._, "a laptop that is bigger than a sports ball"), a T2I model should generate an image that contains both objects, and each object should be of the correct relative scale.

**Text Rendering.** Given a prompt with a certain text to display (_e.g._, "a poster that reads'shop'"), a T2I model should generate an image that properly renders the text.

### Visual Evaluation Modules

To measure the skills described above in Sec. 4.1, we use eight expert visual evaluation modules specialized for different tasks. The modules provide visual+textual explanations with their score. Visual explanations are generated by rendering bounding boxes on the images and textual explanations are generated through text templates, as shown in Fig. 3 and 5. We provide the Python pseudocode implementation for each module in Fig. 4.

**Module definitions. objDet** detects objects in an image based on a referring expression text and returns them (+ their 2D bounding boxes and depth), using Grounding DINO  and DPT . **ocr** detects all text in an image and returns them (+ their 2D bounding boxes), using EasyOCR .

Figure 3: Illustration of skill-based evaluations in VPEval (Sec. 4.3). **Left**: Given text prompts that require different image generation skills, our interpretable evaluation programs evaluate images by executing relevant visual modules. **Right**: Our evaluation programs output binary scores and provide visual (bounding boxes of detected objects/text) + textual explanations of the evaluation results.

**vqa** answers a multiple-choice question using BLIP-2 (Flan-T5 XL) . It can handle phrases that cannot be covered only with objDet or ocr (_e_.\(g\)., pose estimation, action recognition, object attributes). **objectEval** evaluates if an object is in an image using objDet. **countEval** evaluates if an object occurs in an image a certain number of times using objDet and an equation (_e_.\(g\)., "==3", "<5"). **spatialEval** evaluates if two objects have a certain spatial relationship with each other. For six relations (above, below, left, right, front, and behind), it compares bounding box/depth values using objDet. For other relations, it uses vqa. **scaleEval** evaluates if two objects have a certain scale relationship with each other using objDet. For three scale relations (smaller, bigger, and same) it compares bounding box areas by using objDet. For other relations, it uses vqa. **textEval** evaluates if a given text is present in an image using ocr.

### Skill-based Evaluation with Visual Programs

For skill-based evaluation, we create text prompts with various skill-specific templates that are used for image generation and evaluation with our programs. See appendix for the details of prompt creation. In Fig. 3, we illustrate our skill-based evaluation in VPEval. Given text prompts that require different image generation skills, our evaluation programs measure image-text alignment scores by calling the relevant visual modules. Unlike existing T2I evaluation methods, our evaluation programs provide visual+textual explanations of the evaluation results.

### Open-ended Evaluation with Visual Program Generator LM

Although our evaluation with skill-specific prompts covers five important and diverse image generation skills, user-written prompts can sometimes be even more complex and need multiple evaluation criteria (_e_.\(g\)., a mix of our skills in Sec. 4.1 and other skills like attribute detection). For example, evaluating images generated with the prompt 'A woman dressed for a wedding is showing a watermelnol slice to a woman on a scooter:' involves multiple aspects, such as two women (count skill), 'a woman on a scooter' (spatial skill), 'dressed for a wedding' (attribute detection skill), _etc_. To handle such open-ended prompts, we extend the VPEval setup with evaluation programs that can use many visual modules together (whereas single-skill prompts can be evaluated with a program of 1-2 modules). We generate open-ended prompt evaluation programs with an LLM, then the evaluation programs output the average score and the visual+textual explanations from their visual modules. The program generation involves choosing which prompt elements to evaluate and which modules to evaluate those elements (see Fig. 5).

**Open-ended prompts.** For open-ended evaluation, we use 160 prompts of TIFA v1.0 human judgment dataset  (we refer to these prompts as 'TIFA160'). The dataset consists of (1) text prompts from COCO , PaintSkills , DrawBench , and Partiprompts , (2) images generated by five baseline models (minDALL-E , VQ-Diffusion , Stable Diffusion v1.1/v1.5/v2.1 ), and (3) human judgment scores on the images (on 1-5 Likert scale).

**Generating evaluation programs via in-context learning.** As annotation of evaluation programs with open-ended prompts can be expensive, we use ChatGPT (GPT-3.5-Turbo)  to generate evaluation programs via in-context learning. To guide ChatGPT, we adapt the 12 in-context prompts from TIFA . We show ChatGPT the list of visual modules and example text prompts and programs, then ask the model to generate a program given a new prompt, as illustrated in Fig. 5. For

Figure 4: Python pseudocode implementation of visual modules used in VPEval.

reproducible and accessible evaluation, we release the evaluation programs so that VPEval users do not have to generate the programs themselves. We will also release a public LM (finetuned for evaluation program generation using ChatGPT outputs) that can run on local machines.

## 5 Experiments and Results

In this section, we introduce the baseline T2I models we evaluate (Sec. 5.1), compare different T2I models with skill-based (Sec. 5.2) and open-ended prompts (Sec. 5.3), and show the human-correlation of VPEval (Sec. 5.4).

### Evaluated Models

We evaluate our VPGen (Vicuna+GLIGEN) and five popular and publicly available T2I models, covering both diffusion models (Stable Diffusion v1.4/v2.1  and Karlo ), and multimodal autoregressive language models (minDALL-E  and DALL-E Mega ). Stable Diffusion v1.4 is the most comparable baseline to VPGen, because the GLIGEN  in VPGen uses frozen Stable Diffusion v1.4 with a few newly inserted adapter parameters for spatial control.

### Evaluation on Skill-based Prompts

**Diffusion models outperform multimodal LMs.** In Table 1, we show the VPEval skill accuracies for each model. The diffusion models (Stable Diffusion, Karlo, and our VPGen) show higher overall accuracy than the multimodal LMs (minDALL-E and DALL-E Mega).

**Count/Spatial/Scale/Text Rendering skills are challenging.** Overall, the five baseline T2I models achieve high scores (above 93% except for minDALL-E) in Object skill; _i.e_., they are good at

    &  \\   & Object & Count & Spatial & Scale & Text Rendering & Average \\  Stable Diffusion v1.4 & **97.3** & 47.4 & 22.9 & 11.9 & **8.9** & 37.7 \\ Stable Diffusion v2.1 & 96.5 & 53.9 & 31.3 & 14.3 & 6.9 & 40.6 \\ Karlo & 95.0 & 59.5 & 24.0 & 16.4 & **8.9** & 40.8 \\ minDALL-E & 79.8 & 29.3 & 7.0 & 6.2 & 0.0 & 24.4 \\ DALL-E Mega & 94.0 & 45.6 & 17.0 & 8.5 & 0.0 & 33.0 \\  VPGen (F30) & 96.8 & 55.0 & 39.0 & 23.3 & 5.2 & 43.9 \\ VPGen (F30+C+P) & 96.8 & **72.2** & **56.1** & **26.3** & 3.7 & **51.0** \\   

Table 1: VPEval scores of T2I generation models on skill-based prompts (see Sec. 5.2 for analysis). _F30: Flickr30K Entities, C: COCO, P: PaintSkills_.

Figure 5: Evaluation of open-ended prompts in VPEval (Sec. 4.4). **Left**: We generate evaluation programs with ChatGPT via in-context learning. **Right**: Our evaluation programs consist of multiple modules evaluating different elements from a text prompt. We output the final evaluation score by averaging the outputs of each evaluation module.

generating a high-quality single object. However, the models score low accuracies in the other skills, indicating that these skills (Count, Spatial, Scale, and Text Rendering) are still challenging with recent T2I models.

**Step-by-step generation improves challenging skills.** The bottom row of Table 1 shows that our VPGen achieves high accuracy in Object skill and strongly outperforms other baselines in Count (+12.7% than Karlo), Spatial (+24.8% than Stable Diffusion v2.1) and Scale (+9.9% than Karlo) skills. This result demonstrates that our step-by-step generation method is effective in aligning image layout with text prompts, while also still having an interpretable generation program. All models achieve low scores on the Text Rendering skill, including our VPGen. A potential reason why our VPGen model does not improve Text Rendering score might be because the text-to-layout training datasets of VPGen (Flickr30K Enities , COCO , and PaintSkills ) contain very few images/captions about text rendering, opening room for future improvements (_e.g._, finetuning on a dataset with more images that focus on text rendering and including a text rendering module as part of our generation framework).

**Fine-grained analysis in Count/Spatial/Scale skills.** To better understand the high performance of our VPGen in Count, Spatial, and Scale skills, we perform a detailed analysis of these skills with fine-grained splits. In Fig. 6, we compare our VPGen (Vicuna+GLIGEN) and its closest baseline Stable Diffusion v1.4 on the three skills. **Overall**: VPGen achieves better performance than Stable Diffusion on every split on all three skills. **Count**: While both models show difficulties with counting larger numbers, our VPGen model achieves better performance with 50+ accuracy on all four numbers. **Spatial**: Our VPGen model performs better on all six spatial relations. VPGen shows higher accuracy on 2D relations (left/right/below/above) than on 3D depth relations (front/behind), while the Stable Diffusion is better at 3D relations than 2D relations. **Scale**: VPGen generates more accurate layouts with two objects of different sizes (bigger/smaller), than layouts with two objects of similar sizes (same).

### Evaluation on Open-ended Prompts

Table 2 shows the VPEval score on the open-ended TIFA160 prompts. We calculate the score by averaging accuracy from the modules (see Fig. 5 for an example). The overall score trend of the baseline models (diffusion models > multimodal LMs) is consistent with the skill-based prompts. Unlike the trend in skill-based evaluation (Sec. 5.2), our VPGen (Vicuna+GLIGEN) methods achieve similar performance to the Stable Diffusion baseline, while also providing interpretable generation steps. We perform an analysis of the open-ended prompts used for evaluation. In these prompts, object descriptions and attributes are the dominating prompt element (86.4% of elements), whereas descriptions of spatial layouts, only account for 13.6% of elements. See appendix for more details. For prompts from PaintSkills  where spatial layouts are more important, we find that our VPGen scores much higher than Stable Diffusion v1.4 (71.0 _v.s._ 63.5) and Stable Diffusion v2.1 (71.0 _v.s._ 68.4). Note that GLIGEN used in VPGen is based on Stable Diffusion v1.4, making it the closest baseline, and hence future versions of GLIGEN based on Stable Diffusion v2.1 or other stronger layout-to-image models will improve our results further. Also, we find that GLIGEN sometimes fails to properly generate images even when VPGen generates the correct layouts (see following paragraph).

   Model & Score (\%) \(\) \\  Stable Diffusion v1.4 & 70.6 \\ Stable Diffusion v2.1 & 72.0 \\ Karlo & 70.0 \\ minDALL-E & 47.5 \\ DALL-E Mega & 67.2 \\  VPGen (F30) & 71.0 \\ VPGen (F30+C+P) & 68.3 \\   

Table 2: VPEval scores on open-ended prompts (see Sec. 5.3). GLIGEN in VPGen has Stable Diffusion v1.4 backbone. _F30: Flickr30K Entities, C: COCO, P: PaintSkills_.

Figure 6: VPEval score comparison of our VPGen (Vicuna+GLIGEN) and Stable Diffusion v1.4 on fine-grained splits in Count/Spatial/Scale skills.

**VPGen sources of error: layout/image generation.** Since our VPGen separates layout generation and image generation, we study the errors caused by each step. For this, we manually analyze the images generated from TIFA160 prompts, and label (1) whether the generated layouts align with the text prompts and (2) whether the final images align with the text prompts/layouts. GLIGEN sometimes fails to properly generate images even when Vicuna 13B generates the correct layouts, indicating that when better layout-to-image models become available, our VPGen framework will achieve higher results. We include more details in the appendix.

**Qualitative Examples.** In Fig. 7, we provide example images generated by our VPGen and Stable Diffusion v1.4 (the closest baseline) with various skill-based (top) and open-ended (bottom) prompts. See appendix for more qualitative examples with skill-based/open-ended prompts, prompts with unseen objects, counting \( 4\) objects, and error analysis.

### Human Evaluation of VPEval

To measure alignments with human judgments and our VPEval, we compare the human-correlations of VPEval and other metrics on both skill-based prompts and open-ended prompts.

**Human correlation of VPEval on skill-based prompts.** We ask two expert annotators to evaluate 20 images, from each of the five baseline models, for each of the five skills with binary scoring (total \(20 5 5=500\) images). The inter-annotator agreements were measured with Cohen's \(\) = 0.85 and Krippendorff's \(\) = 0.85, indicating 'near-perfect' (\(\) > 0.8 or \(\) > 0.8) agreement [63; 64; 62]. We compare our visual programs with captioning (with metrics BLEU , ROUGE , METEOR , and SPICE ), VQA, and CLIP (ViT-B/32) based evaluation. We use BLIP-2 Flan-T5 XL, the state-of-the-art public model for image captioning and VQA.

In Table 3, our VPEval shows a higher overall correlation with human judgments (66.6) than single module-based evaluations (CLIP, Captioning, and VQA). Regarding per-skill correlations, VPEval shows especially strong human correlations in Count and Text Rendering. As the BLIP-2 VQA module also shows strong correlation on Object/Spatial/Scale, we also experiment with VPEval\({}^{}\): using BLIP-2 VQA for objectEval/spatialEval/scaleEval modules (instead of Grounding

Figure 7: Images generated by Stable Diffusion v1.4 and our VPGen (Vicuna 13B+GLIGEN) for skill-based (top) and open-ended (bottom) prompts.

DINO and DPT), which increases human correlation scores. Note that our object detection modules visually explain the evaluation results, as shown in Fig. 3, and we can have an even higher correlation when we have access to stronger future object detection models.

**Human correlation of VPEval on open-ended prompts.** We generate visual programs with our program generation LM on TIFA160 prompts . The dataset consists of 800 images (160 prompts \(\) 5 T2I models) and human judgments (1-5 Likert scale) along with other automatic metrics (BLIP-2 captioning, CLIP cosine similarity, and TIFA with BLIP-2) on the images. Table 4 shows that our VPEval achieves a better human correlation with TIFA (BLIP-2), and our VPEval\({}^{}\) version achieves an even higher correlation. The results indicate that using various interpretable modules specialized in different skills complements each other and improves human correlation, while also providing visual+textual explanations.

**Human analysis on the generated programs.** Lastly, we also measure the faithfulness of the generated evaluation programs. For this, we sample TIFA160 prompts and analyze the evaluation programs by (1) how well the modules cover elements in the prompt; (2) how accurate the module outputs are when run. We find that programs generated by our VPEval have very high coverage over the prompt elements and high per-module accuracy compared to human judgment. We include more details in the appendix.

## 6 Conclusion

We propose two novel visual programming frameworks for interpretable/explainable T2I generation and evaluation: VPGen and VPEval. VPGen is a step-by-step T2I generation framework that decomposes the T2I task into three steps (object/count generation, layout generation, and image generation), leveraging an LLM for the first two steps, and layout-to-image generation models for the last image generation step. VPGen generates images more accurately following the text descriptions (especially about object counts, spatial relations, and object sizes) than strong T2I baselines, while still having an interpretable generation program. VPEval is a T2I evaluation framework that uses interpretable evaluation programs with diverse visual modules that are experts in different skills to measure various T2I skills and provide visual+textual explanations of evaluation results. In our analysis, VPEval presents a higher correlation with human judgments than single model-based evaluations, on both skill-specific and open-ended prompts. We hope our work encourages future progress on interpretable/explainable generation and evaluation for T2I models.

**Limitations & Broader Impacts.** See Appendix for limitations and broader impacts discussion.

   Metrics & \(\) (\(\)) \\  _BLIP-2 Captioning_ & \\ BLEU-4 & 18.3 \\ ROUGE-L & 32.9 \\ METER & 34.0 \\ SPICE & 32.8 \\  _Cosine-similarity_ & \\ CLIP (ViT-B/32) & 33.2 \\  _LM + VQA module_ & \\ TIFA (BLIP-2) & 55.9 \\  _LM + multiple modules_ & \\ VPEval (Ours) & 56.9 \\ VPEval\({}^{}\) (Ours) & **60.3** \\   

Table 4: Human correlation on open-ended evaluation with Spearman’s \(\).

    &  \\   & Object & Count & Spatial & Scale & Text & Overall \\  CLIP Cosine similarity (ViT-B/32) & 35.2 & 38.6 & 35.4 & 13.7 & 40.0 & 20.4 \\ BLIP-2 Captioning - BLEU & 11.9 & 31.4 & 26.3 & 24.0 & 23.6 & -3.4 \\ BLIP-2 Captioning - ROUGE & 15.7 & 26.5 & 28.0 & 12.2 & 28.3 & 11.9 \\ BLIP-2 Captioning - METER & 33.7 & 20.7 & 40.5 & 25.1 & 26.6 & 29.3 \\ BLIP-2 Captioning - SPICE & 56.1 & 20.9 & 40.6 & 27.3 & 18.6 & 28.1 \\ BLIP-2 VQA & **63.7** & 63.1 & 38.9 & 26.1 & 31.3 & 65.0 \\  VPEval & 34.5 & **63.8** & 48.9 & 29.4 & **85.7** & 73.5 \\ VPEval\({}^{}\) & **63.7** & **63.8** & **51.2** & **29.5** & **85.7** & **79.0** \\   

Table 3: Human correlation study of skill-based evaluation. We measure Spearman’s \(\) correlation between human judgment and different automated metrics on the skill-based prompts (Sec. 4.3). VPEval\({}^{}\): using BLIP-2 VQA for objectEval/spatialEval/scaleEval modules.