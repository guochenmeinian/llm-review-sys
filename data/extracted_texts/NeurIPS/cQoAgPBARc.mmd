# Improving Deep Reinforcement Learning by Reducing the Chain Effect of Value and Policy Churn

Hongyao Tang

Mila - Quebec AI Institute

Universite de Montreal

tang.hongyao@mila.quebec

&Glen Berseth

Mila - Quebec AI Institute

Universite de Montreal

glen.berseth@mila.quebec

###### Abstract

Deep neural networks provide Reinforcement Learning (RL) powerful function approximators to address large-scale decision-making problems. However, these approximators introduce challenges due to the non-stationary nature of RL training. One source of the challenges in RL is that output predictions can _churn_, leading to uncontrolled changes after each batch update for states not included in the batch. Although such a churn phenomenon exists in each step of network training, how churn occurs and impacts RL remains under-explored. In this work, we start by characterizing churn in a view of Generalized Policy Iteration with function approximation, and we discover a _chain effect of churn_ that leads to a cycle where the churns in value estimation and policy improvement compound and bias the learning dynamics throughout the iteration. Further, we concretize the study and focus on the learning issues caused by the chain effect in different settings, including greedy action deviation in value-based methods, trust region violation in proximal policy optimization, and dual bias of policy value in actor-critic methods. We then propose a method to reduce the chain effect across different settings, called Churn Approximated ReductIoN (**CHAIN**), which can be easily plugged into most existing DRL algorithms. Our experiments demonstrate the effectiveness of our method in both reducing churn and improving learning performance across online and offline, value-based and policy-based RL settings, as well as a scaling setting.

## 1 Introduction

One fundamental recipe for the success of Deep Reinforcement Learning (DRL) is powerful approximation and generalization provided by deep neural networks, which augments the ability of RL with tabular or linear approximation to large state spaces. However, on the other side of this benefit is less control over the function dynamics. Network outputs can change indirectly to unexpected values after any random batch update for input data not included in the batch, called _churn_ in this paper. This change is particularly problematic for an RL agent due to its non-stationary nature, which can exacerbate instability, suboptimality, and even collapse. Therefore, it is important to understand and control these undesired dynamics to address learning issues and improve performance.

Consistent efforts have been devoted by the RL community to gain a better understanding of the learning dynamics from different perspectives . Recently, Schaul et al.  studied a novel churn phenomenon in the learning process of typical value-based RL algorithms like DoubleDQN . The phenomenon reveals that the greedy actions of about \(10\%\) of states in the replay buffer change after a single regular batch update. Such a dramatic churn can persist throughout the learning process of DoubleDQN, causing instabilities.

In this paper, we aim to take a step further to understand how churn occurs and influences learning in different DRL settings beyond value-based RL, as well as to propose a method to control churn. We start by formally characterizing churn in view of Generalized Policy Iteration (GPI) with function approximation to best cover most DRL settings. The impact of churn is two-fold in this view: the churn in policy improvement (called the _policy churn_) changes policy outputs on states that are not directly updated, while the churn in value estimation (called the _value churn_) also changes the action-value landscape, thus altering greedy action and action gradient. We then discover a _chain effect of churn_ that exhibits a cycle where the two types of churn compound and bias the learning dynamics throughout the iteration. Further, we move on from the general analysis to concrete DRL settings. We focus on the learning issues caused by the chain effect including greedy action deviation in value-based methods, trust region violation in proximal policy optimization (Schulman et al., 2017) and dual bias of policy value in actor-critic methods. The connection between the chain effect of churn and the issues necessitates an explicit control of the churn.

To this end, we propose a method called Churn Approximated ReducIoN (**CHAIN**) to reduce the chain effect of churn across different DRL settings. The main idea of CHAIN is to reduce the undesirable changes to the outputs of the policy and value networks for states (and actions) outside of the current batch of data for regular DRL training. This reduction is achieved by minimizing the change in target values for a separate batch of data when optimizing the original policy or value learning objective. CHAIN is easy to implement and plug in most existing DRL algorithms with only a few lines of code1. In our experiments, we evaluate the efficacy of CHAIN in a range of environments, including MinAtar (Young and Tian, 2019), OpenAI MuJoCo (Brockman et al., 2016), DeepMind Control Suite (Tassa et al., 2018) and D4RL (Fu et al., 2020). The results show that our method can effectively reduce churn and mitigate the learning issues, thus improving sample efficiency or final performance across online and offline, value-based and policy-based DRL settings. Moreover, our results also show that our method helps to scale DRL agents up and achieves significantly better learning performance when using wider or deeper networks.

The main contributions of this work are summarized as follows: (1) We study how churn occurs and influences learning from the perspective of GPI with function approximation and present the chain effect of churn. (2) We show how churn results in three learning issues in typical DRL settings. (3) We propose a simple and general method and demonstrate its effectiveness in reducing churn and improving learning performance across various DRL settings and environments.

## 2 Prior Work

In the past decade, a significant effort has been made to understand the learning issues of DRL agents and propose improvements that make DRL more stable and effective. The early stage of this effort studied bias control for value approximation with deep neural networks introducing many improvements after DQN (Mnih et al., 2015) and DDPG (Lillicrap et al., 2015) to address overestimation or underestimation for value-based methods (van Hasselt et al., 2016; Bellemare et al., 2017; Hessel et al., 2018) and deep AC methods (Fujimoto et al., 2018; Haarnoja et al., 2018; Lan et al., 2020; Kuznetsov et al., 2020; Chen et al., 2021) respectively. Additional works dig deeper to diagnose the learning issues regarding instability and generalization, related to the Deadly Triad in DRL (van Hasselt et al., 2018; Achiam et al., 2019), stabilizing effect of target network (Zhang et al., 2021; Chen et al., 2022; Piche et al., 2022), difficulty of experience replay (Schaul et al., 2016; Kumar et al., 2020; Ostrovski et al., 2021), over-generalization (Ghiassian et al., 2020; Pan et al., 2021; Yang et al., 2022), representations in DRL (Zhang et al., 2021; Li et al., 2022; Tang et al., 2022), delusional bias (Lu et al., 2018; Su et al., 2020), off-policy correction (Nachum et al., 2019; Zhang et al., 2020; Lee et al., 2021), interference (Cobbe et al., 2021; Raileanu and Fergus, 2021; Bengio et al., 2020) and architecture choices (Ota et al., 2020).

One notable thread is to understand the learning dynamics of DRL agents with a focus on the non-stationary nature of RL. A prominent phenomenon of representation ability loss is studied in (Dabney et al., 2021; Igl et al., 2021; Kumar et al., 2021; Kumar et al., 2021; Ma et al., 2023), which reveals how representations become less useful in later stages of learning, leading to myopic convergence. Further, empirical studies in (Nikishin et al., 2022; D'Oro et al., 2023; Sokar et al., 2023; Nauman et al., 2024) demonstrate that the loss of approximation ability becomes severe and leads to collapse when high replay-ratios are adopted for better sample efficiency, while network resets and normalization methods can be simple and effective remedies. This is further identified as plasticity loss in DRL (Lyle et al., 2022; Abbas et al., 2023; Dohare et al., 2023; Lyle et al., 2024; Xu et al., 2024).

Recently, it has been found that there is a dramatic change in the policy distribution where a large portion of the greedy actions change after each batch update, called _policy churn_ Schaul et al. (2022). Although intuitively related to generalization (Bengio et al., 2020) and interference (Liu et al., 2023), it presents a lack of understanding of churn's effect on the learning behaviors of DRL agents regarding stability, convergence, exploration, etc. Kapturowski et al. (2023) takes the inspiration and proposes a method to robustify the agent's behavior by adding an additional policy head to the value network that fits the \(\)-greedy policy via policy distillation. In this work, we further the study of churn in a more general formal framework, where churn occurs in both value and policy learning. In particular, we focus on the dynamics of churn during the learning process and how it incurs issues in different DRL algorithms and propose a practical method to reduce churn and improve learning performance.

## 3 Preliminaries

Reinforcement Learning (RL) is formulated within the framework of a Markov Decision Process (MDP) \(,,,,,_{0},T\), defined with the state set \(\), the action set \(\), the transition function \(: P()\), the reward function \(:\), the discounted factor \([0,1)\), the initial state distribution \(_{0}\) and the horizon \(T\). The agent interacts with the MDP by performing actions from its policy \(a_{t}(s_{t})\) that defines the mapping from states to actions or action distributions. The objective of an RL agent is to optimize its policy to maximize the expected discounted cumulative reward \(J()=_{}[_{t=0}^{T}^{t}r_{t}]\), where \(s_{0}_{0}(s_{0})\), \(s_{t+1}(s_{t+1} s_{t},a_{t})\) and \(r_{t}=(s_{t},a_{t})\). The state-action value function \(q^{}\) defines the expected cumulative discounted reward for all \(s,a\) and the policy \(\), i.e., \(q^{}(s,a)=_{}_{t=0}^{T}^{t}r_{t} s_{0}=s,a_{0}=a\).

Policy and value functions are approximated with deep neural networks to cope with large and continuous state-action space. Conventionally, \(q^{}\) can be approximated by \(Q_{}\) with parameters \(\) typically through minimizing Temporal Difference (TD) loss (Sutton and Barto, 1988), i.e., \(L()=_{s,a D}\ _{}(s,a)^{2}\) where \(D\) is a replay buffer and \(_{}(s,a)\) is a type of TD error. A parameterized policy \(_{}\) with parameters \(\) can be updated by taking the gradient of the objective, i.e., \(^{}+_{}J(_{})\) with a step size \(\). Value-based methods like Deep \(Q\)-Network (DQN) (Mnih et al., 2015) trains a \(Q\)-network \(Q_{}\) by minimizing \(L()\) where \((s,a)=Q_{}(s,a)-(r+_{a^{}}Q_{}-(s^{},a ^{}))\) and \(^{-}\) denotes the target network. For policy-based methods, TD3 (Fujimoto et al., 2018) is often used to update a deterministic policy with Deterministic Policy Gradient (DPG) theorem (Silver et al., 2014): \(_{}J(_{})=_{s D}[_{}_{}(s )_{a}Q_{}(s,a)|_{a=_{}(s)}]\); Soft Actor-Critic (SAC) (Haarnoja et al., 2018) learns a stochastic policy with the gradient: \(_{}J(_{})=_{s D}_{}_{ }(a|s)+(_{a}_{}(a|s))-_{a}Q_{}(s,a))_{ }J_{}(;s))|_{a=f_{}(;s)}\), with noise \(\) and implicit function \(f_{}\) for re-parameterization.

## 4 A Chain Effect of Value and Policy Churn

In this section, we present a formal study on value and policy churn and their impact on learning. We first introduce an intuitive overview of how churn is involved in DRL (Section 4.1). Then, we propose the definitions of the value and policy churn (Section 4.2), followed by a chain effect that reveals how the churns interplay and bias parameter update (Section 4.3).

### Generalized Policy Iteration under Churn

Generalized Policy Iteration (GPI) (Sutton and Barto, 1988) is widely used to refer to the general principle of learning in an Evaluation-Improvement iteration manner, which applies to almost all RL methods. In the context of DRL, i.e., with network representation and mini-batch training, the value and policy networks' outputs can have unexpected changes, i.e., the _churn_, after each mini-batch training for the states not included in the batch. Such churns are neglected in most DRL methods, let alone their influence on the practical learning process. In Figure 1, we extend the classic GPI diagram by considering churn to show how it is involved in the learning process intuitively.

In the evaluation process, the parameterized \(Q\)-network \(Q_{}\) approximates the value of the current policy via repeated mini-batch training. Under the impact of churn, the \(Q\)-network is not likely to have output predictions the same as what was updated with explicit mini-batch training. For example, let's imagine we have a _virtual_ network \(_{}\) that only accepts the changes for the states updated by mini-batch training directly and remains unchanged for the others.

Thus, the _value churn_, denoted by \(_{} Q_{}\), is an implicit process that alters the virtual network \(_{}\) to the approximation \(Q_{}\) we obtained in practice. Similarly, the _policy churn_\(_{}_{}\) occurs in the improvement process. As illustrated in Figure 1, the value churn and the policy churn are interwoven in the Evaluation-Improvement process. Usually, we can assume that the churns make non-negligible changes, i.e., \(Q_{}_{}\) and \(_{}_{}\). Therefore, we delve into the cause of churn, its impact on learning, and possible remedies to mitigate its negative impact in the following sections.

### Definition of Value and Policy Churn

A deep neural network can have the form \(f_{}:X Y\) with parameters \(\). The network is optimized for a set of input data \(B_{}=\{x_{i}\}\) with a loss function, leading to a parameter update of \(^{}\). Given a reference set of input data \(B_{}=\{_{i}\}\) (where \(B_{} B_{}=\)) and a metric \(d\) for the output space, the churn is formally defined as:

\[_{f}(,^{},B_{})=}|}_{x B_{}}d(f_{^{}}(),f_{}( )).\]

Arguably, churn is an innate property of neural networks, and it is closely related to problems like interference (Liu et al., 2020, 2023) and catastrophic forgetting (Lan et al., 2023) in different contexts.

In this paper, we focus on the churn in \(Q\)-value network \(Q_{}\) and policy network \(_{}\). We then obtain the definitions of the \(Q\)-value churn (\(_{Q}\), w.r.t. \(^{}\)) and the policy churn (\(_{}\), w.r.t. \(^{}\), using a deterministic policy for demonstration) for an arbitrary state-action pair \(, B_{}\) as follows:

\[c_{Q}(,^{},,)=Q_{^{}}(, {a})-Q_{}(,),\ c_{}(,^{},)=_{ ^{}}()-_{}().\] (1)

Then, the definitions regarding \(B_{}\) can be generalized to the batch setting by aggregating data in \(B_{}\): \(_{Q}(,^{},B_{})=}|}_{, B_{}}|c_{Q}(,^{ },,)|\), \(_{}(,^{},B_{})=}|} _{ B_{}}|c_{}(,^{},)|\). Without loss of generality, we carry out our analysis mainly regarding \(,\) for clarity in the following.

(U.I) How the churn \(_{Q},_{}\) are caused by parameter updatesFirst, we look into the relationship between the \(Q\)-value churn \(_{Q}\), the policy churn \(_{}\) and the network parameter updates \(_{}=^{}-,_{}=^{}-\). For \(_{},_{}\), we use typical TD learning and DPG for demonstration: \(_{}=}|}_{s,a B_{}} _{}Q_{}(s,a)_{}(s,a)\), and \(_{}=}|}_{s B_{}} _{}_{}(s)_{a}Q_{}(s,a)|_{a=_{}(s)}\).

Now we characterize \(_{Q}\) and \(_{}\) as functions of \(_{},_{}\) with the help of Neural Tangent Kernel (NTK) (Achiam et al., 2019). For clarity, we use \(B_{}=\{s,a\}\) and \(B_{}=\{,\}\) and abbreviate \(B_{},B_{}\) and step size \(\) when context is clear. Concretely,

\[ c_{Q}(,^{})&= _{}Q_{}(,)^{}_{}+O(\|_{} \|^{2})Q_{}(,)^{} _{}Q_{}(s,a)}_{k_{}(,,s,a)}_{ }(s,a)\\ c_{}(,^{})&=_{}_{ }()^{}_{}+O(\|_{}\|^{2})_{}()^{}_{}_{}(s)}_{k_{}( ,s)}_{a}Q_{}(s,a)|_{a=_{}(s)}\] (2)

Eq. 2 shows that the value and policy churn are mainly determined by the kernels of the \(Q\)-network \(k_{}\) and the policy network \(k_{}\), along with the TD error and the action gradient. This indicates that churn is determined by both the network's property itself and the learning we performed with the network.

### From Single-step Interplay to The Chain Effect of Churn

In addition to the first piece of understanding **(U.I)** that presents how parameter updates cause the churns, we discuss how the churns affect parameter updates backward with two more pieces of understanding **(U.II)** and **(U.III)**, finally shedding light on a chain effect of churn.

Figure 1: Generalized Policy Iteration (GPI) under the value and policy churn.

(U.II) How \(_{Q},_{}\) deviates action gradient and policy valueFirst, we introduce two types of deviation derived from the value and policy churn: (1) Action Gradient Deviation (\(_{_{}}^{Q}\)), the change of action gradient regarding the \(Q\)-network for states and actions that are affected by the \(Q\)-value churn \(_{Q}\); (2) Policy Value Deviation (\(_{Q}^{}\)), the change of \(Q\)-value due to the action change for states that are affected by policy churn \(_{}\). Formally, the two types of deviation are:

\[ d_{_{}}^{Q}(,^{},)&=_{}Q_{^{}}(,)|_{= ()}-_{}Q_{}(,)|_{=()}.\\ d_{Q}^{}(,^{},)&=Q(,_{^{}}())-Q(,_{}()).\] (3)

One thing to note is the two types of deviation show the interplay between the value and policy churn, as the value churn derives the deviation in policy (\(c_{Q}}d_{_{}}^{Q}\)) and the policy churn derives the deviation in value (\(c_{}}d_{Q}^{}\)), as denoted by the superscripts. This interplay between the policy and value can be shown better with the expressions below (derivation details in Appendix B):

\[d_{_{a}}^{Q}(,^{})=_{}c_{Q}(, ^{})|_{=()},\ \ d_{Q}^{}(,^{})(_{}Q_{}(, )|_{=_{}()})^{}c_{}(,^{})\] (4)

Since the action gradient and policy value play key roles in parameter updates, the deviations in them naturally incur negative impacts on learning.

The discussion thus far is within a single parameter update. Now, we discuss the implications of these single updates towards a chain of updates to shed light on the long-term effect of churn.

(U.III) How parameter updates are biased by \(_{Q},_{}\) and the deviations \(_{_{a}}^{Q},_{Q}^{}\)Let us consider a segment of two consecutive updates, denoted by \((^{-},^{-})(,)(^{}, ^{})\). The churns occurred during the _last update_\((^{-},^{-})(,)\) participate in the _current update_\((,)(^{},^{})\) about to perform. Concretely, the churns affect the following aspects: (1) \(Q\)-value estimate, (2) action selection in both TD error and policy objective, and (3) the gradient of network parameters.

From these aspects, we can deduce the difference between the parameter updates under the impact of the value and policy churn (denoted by \(_{},_{}\)) and the conventional ones \(_{},_{}\). As a result, we can find that the value and policy churn, as well as the deviations derived, introduce biases in the parameter updates. We provide the complete discussion and derivation in Appendix B.2.

The analysis on the update segment \((^{-},^{-})(,)(^{}, ^{})\) can be forwarded, and taking the three pieces of understanding together, we arrive at the chain effect of churn.

As the cycle illustrated in Figure 2, the value and policy churn and the parameter update bias _accumulate_ and can _amplify each other_ throughout the learning process. Intuitively, the parameter update chain could derail and fluctuate under the accumulating churns and biases, thus preventing stable and effective learning. We concretize our study on the consequences in the next section.

## 5 Reducing Value and Policy Churn in Deep RL

In this section, we show concrete learning issues caused by churn in typical DRL scenarios (Section 5.1), followed by a simple plug-in method to reduce churn and address the issues (Section 5.2).

### Consequences of the Chain Effect of Churn in Different DRL Scenarios

Since churn is involved in most DRL methods, as illustrated by Figure 1, our study focuses on several typical DRL scenarios below.

Figure 2: Illustration of the logical cycle of the _chain effect_ of the value and policy churn.

Greedy action deviation in value-based methodsValue-based methods like DQN train a \(Q\)-network \(Q_{}\) and compute the policy by choosing the greedy action of \(Q_{}\). A consequence of computing the action greedily is that changes in the values will directly cause changes in the action distribution (Schaul et al., 2022). Similarly to Eq. 4, this deviation can be formalized as: \(_{a^{*}}^{Q}(,^{},B_{})=}|}_{ B_{}}_{ \{_{a}Q_{}(,)\}}_{}Q_{^ {}}(,)\). We suspect that this deviation introduces instability and hinders learning, and we focus on whether reducing churn can improve the performance of value-based methods.

Trust region violation in policy gradient methodsTrust region plays a critical role in many policy gradient methods for reliable and efficient policy updates. Proximal Policy Optimization (PPO) (Schulman et al., 2017) uses a clipping mechanism as a simple but effective surrogate of the trust region for TRPO (Schulman et al., 2015): \(r(_{},),1-,1+\) and \(r(_{},)=(a|s)}{_{_{}}(a|s)}\). With respect to policy churn, even though the PPO policy conforms to the trust region for the states in the current training batch, it could silently violate the trust region for other states, including previously updated ones. Consider the policy update \(^{}\), it is highly likely to have \(r(,^{})=}(|)}{_{}( |)} 1\) and thus \(r(_{},^{})=}(|) }{_{_{}}(|)}=r(_{},)r(, ^{}) r(_{},)\). Since we have no information about \(r(,^{})\), there is no guarantee for the trust region \(1- r(_{},^{}) 1+\) to be respected after churn. Intuitively, this implicit violation is hazardous and detrimental to learning.

Dual bias of policy value in Actor-Critic methodsDeep AC methods interleave the training between the actor-network and the critic-network. Unlike the two scenarios above, where either the value churn or the policy churn raises a learning stability issue, we present the dual bias of policy value that stems from the bilateral effect of churn. The dual bias exists in the policy value as \(Q_{^{}}(,_{^{}}()) Q_{}(,_{}())\). In the context of AC methods, the policy value is used for the target computation of the critic \(r_{t}+ Q_{^{}}(s_{t+1},_{^{}}(s_{t+1}))\) and the optimization objective of the actor \(_{}Q_{^{}}(s,_{^{}}(s))\). Thus, the dual bias steers the training of the actor and the critic.

Given these negative consequences of churn, a question is raised naturally: how can we control the level of churn to mitigate the issues without introducing complex trust regions or constraints?

### A Regularization Method for Churn Reduction

In this section, we propose a regularization method to reduce value and policy churn, called Churn Approximated ReducIoN (**CHAIN**). To combat the prevalence of churn's negative influence on DRL, our method should be simple to implement and easy to use with different RL methods.

Based on the definitions of the value churn (\(_{Q}\)) and the policy churn (\(_{}\)) in Section 4.2, we propose two corresponding loss functions \(L_{}\) and \(L_{}\) for churn reduction. Formally, for parameterized networks \(Q_{_{t}},_{_{t}}\) at time \(t\) and a reference batch \(B_{}\) sampled from replay buffer, we have:

\[L_{}(_{t},B_{})=}|}_{ {s}, B_{}}Q_{_{t}}(,)-Q_{_ {t-1}}(,)^{2}\] (5)

\[L_{}(_{t},B_{})=}|}_{ B_{}}d_{}(_{_{t}}(),_{_{t-1}}())\] (6)

where \(d_{}\) is a policy distance metric, and we use mean square error or KL divergence for deterministic or stochastic policies. Ideally, the regularization should be imposed on the post-update network parameters \(_{t+1},_{t+1}\). Since they are not available at time \(t\), we regularize \(_{t},_{t}\) and use \(_{t-1},_{t-1}\) as the targets for a convenient and effective surrogate.

By minimizing \(L_{}\) and \(L_{}\), we can reduce the value and policy churn and suppress the chain effect further. This allows us to use the churn reduction regularization terms along with standard RL objectives and arrives at DRL with CHAIN finally:

\[_{}\;L(_{t},B_{})+_{Q}L_{}(_{t},B_{})\] (7)

\[_{}\;J(_{t},B_{})-_{}L_{}( _{t},B_{})\] (8)

where \(B_{},B_{}\) are two separate batches randomly sampled from \(D\), and \(_{Q},_{}\) are coefficients that control the degree of regularization. CHAIN serves as a plug-in component that can be implemented with only a few lines of code modification in most DRL methods. The pseudocode is omitted here, and we refer the readers to Algorithm 1 in the Appendix.

Automatic adjustment of \(_{Q},_{}\)To alleviate the difficulty of manually selecting the regularization coefficients, we add a simple but effective method to adjust \(_{Q},_{}\) adaptively during the learning process. The key principle behind this is to keep a consistent relative scale (denoted by \(\)) between the churn reduction regularization terms and the original DRL objectives. More precisely, by maintaining the running means of the absolute \(Q\) loss \(|_{Q}|\) and the VCR term \(|_{}|\), \(_{Q}\) is computed dynamically as \(_{Q}=_{Q}|}{|_{}|}\), which is similar for \(_{}\). This is inspired by our empirical observations and the recent study on addressing the reward scale difference across different domains (Hafner et al., 2023).

Another thing worth noting is that CHAIN helps to mitigate the loss of plasticity via churn reduction. This connection can be established by referring to the NTK expressions in Eq. 2: reducing churn encourages \(k_{},k_{}\) to 0 and thus prevents the empirical NTK matrix from being low-rank, which is shown to be a consistent indicator of plasticity loss (Lyle et al., 2024).

## 6 Experiments

In the experiments, we aim to answer the following questions: (1) How large is the value and policy churn in practice, and can our method effectively reduce churn? (2) Does our method's reduction of churn address learning issues and improve performance in terms of efficiency and episode return? (3) Does CHAIN also improve the scaling abilities of deep RL?

We organize our experiments into the four subsections below that correspond to the three DRL scenarios discussed in Section 5.1 as well as a DRL scaling setting. Our experiments include 20 online RL tasks from MinAtar, MuJoCo, DMC, and 8 offline RL datasets from D4RL, as well as 6 popular algorithms, i.e., DoubleDQN, PPO, TD3, SAC, IQL, AWAC. We provide the experimental details in Appendix C and more results in Appendix D.

### Results for CHAIN DoubleDQN in MinAtar

We use DoubleDQN (DDQN) (van Hasselt et al., 2016) as the value-based method and MinAtar (Young and Tian, 2019) as the experiment environments. MinAtar is an Atari-inspired testbed for convenient evaluation and reproduction. We build our DoubleDQN based on the official MinAtar code with no change to the network structure and hyperparameters. We implement CHAIN DDQN by adding a few lines of code to apply the value churn reduction regularization in the standard training of the \(Q\)-network (Eq. 7). For CHAIN DDQN, \(_{Q}\) is set to \(50\) for Breakout and \(100\) for the other tasks. For CHAIN DDQN with automatic adjustment of \(_{Q}\) (denoted by the suffix '**Auto'**), the target relative loss scale \(\) is set to \(0.05\) for all the tasks.

First, to answer Question (1), we report the value churn and the greedy action deviation of DDQN in Figure 3. Each point means the average metric across randomly sampled states and the whole learning process. As expected, we can observe that the value churn accumulates as more training updates take place, leading to the growth of greedy action deviation. With CHAIN, the churn and deviation are reduced significantly. We refer the readers to Figure 9 for more statistics on the value churn.

Further, we show the learning curves of CHAIN DDQN regarding episode return in Figure 4. We can see that CHAIN consistently achieves clear improvements over DDQN in terms of both sample efficiency and final scores, especially for Asterix and Seaquest. Moreover, CHAIN (Auto) matches or surpasses the results achieved by manual coefficients, which supports Question (2) positively. In the next subsection, we evaluate how much CHAIN can improve policy gradient-based RL algorithms.

### Results for CHAIN PPO in MuJoCo and DMC

Corresponding to the second DRL scenario discussed in Section 5.1, we focus on the policy churn in Proximal Policy Optimization (PPO) (Schulman et al., 2017) and try to answer the first three questions for policy gradient-based RL algorithms. We build the experiments on the public implementation of PPO from CleanRL (Huang et al., 2022) and use the continuous control tasks in MuJoCo and

Figure 3: The value churn (_left_) and the greedy action deviation percentage (_right_) in Breakout w/ and w/o CHAIN.

DeepMind Control (DMC) as the environments for evaluation. Following the same principle, we implement CHAIN PPO by adding the policy churn reduction regularization to the standard PPO policy training (Eq. 6), with no other modification to the public PPO implementation.

First, to understand the level of churn, we compare PPO and CHAIN PPO with different choices of \(_{}\) in terms of policy churn and episode return. In summary, we observed that PPO also exhibits clear policy churn, and CHAIN significantly reduces it throughout learning, which answers Question (1). Figure 10 shows the details for this analysis. Note that more policy churn makes it more likely to violate the trust region as \(_{} r(,^{})\) discussed in Section 5.1. In turn, we also observed CHAIN PPO consistently outperforms PPO in Ant-v4 and HalfCheetah-v4 across different choices of \(_{}\).

Further, we aim to answer Question (2) and evaluate whether CHAIN can improve the learning performance of PPO in terms of episode return. For CHAIN PPO (Auto), we set the target relative loss scale \(\) to 0.1 for MuJoCo tasks and 0.02 for DMC tasks. The results for MuJoCo and DMC tasks are reported in Figure 5. The results show that CHAIN PPO outperforms PPO in most cases with higher sample efficiency and final episode return, often significantly. We believe that our results reveal a promising direction to improve more trust-region-based and constraint-based methods in DRL by addressing the issues caused by churn.

### Results for Deep Actor-Critic Methods with CHAIN in MuJoCo and D4RL

Next, we continue our empirical study and evaluate our method for deep actor-critic (AC) methods. We separate our study into online and offline settings, as presented below.

**Online AC methods** We use TD3 [Fujimoto et al., 2018] and SAC [Haarnoja et al., 2018] and MuJoCo environments based on the public implementation of TD3 and SAC from CleanRL. Since AC methods are bilaterally affected by churn, we consider two variants of CHAIN, either of which only applies the value churn reduction (VCR) or the policy churn reduction (PCR).

Figure 4: The evaluation of CHAIN DoubleDQN in MinAtar regarding episode return. Curves and shades denote means and standard errors over six random seeds.

Figure 5: The evaluation of CHAIN PPO in MuJoCo and DeepMind Control (DMC) tasks regarding episode return. Curves and shades denote means and standard errors over twelve random seeds.

For Question (1), we again find that both TD3 and SAC exhibit value and policy churn in all environments, and CHAIN-VCR and CHAIN-PCR effectively reduce them respectively in Figure 11 and 12. For Question (2), Figure 6 shows the evaluation results regarding episode return. We can see that CHAIN-PCR often improves the learning performance, especially for Ant-v4; in contrast, CHAIN-VCR improves slightly. We hypothesize that this is because the policy interacts with the environment directly, and the target critic-network also helps to reduce the value churn due to its delayed synchronization with the online critic.

Due to the limitation of space, we refer the readers to Appendix D.3 for more results on churn reduction, the influence of different choices of \(_{Q},_{}\), the results of combining VCR and PCR, as well as the effect of auto-adjustment of the regularization coefficient for TD3 and SAC.

**Offline AC methods** In Offline RL, a policy is trained over a fixed dataset. We investigate if reducing churn can also improve the convergence of Offline RL. We use IQL (Kostrikov et al., 2022) with D4RL Antmaze dataset (Fu et al., 2020) and AWAC (Nair et al., 2020) with Adroit for our demonstration due to their popularity and good performance in corresponding tasks. Concretely, we use the public implementation and benchmark scores for IQL and AWAC from \(^{2}\). To apply CHAIN to IQL and AWAC, we implement the regularization for value churn reduction (VCR, Eq. 7) and policy churn reduction (PCR, Eq. 8) separately by adding a couple of lines of code without any other modification. We use \(_{}=1e3\) for both CHAIN IQL (PCR) and CHAIN AWAC (PCR); \(_{Q}=0.01\) for CHAIN IQL (VCR) and \(0.1\) for CHAIN AWAC (VCR) across different tasks. The results are summarized in Table 1 and 2.

We observe that both CHAIN PCR and CHAIN VCR improve the scores for IQL and AWAC in most Antmaze and Adroit tasks. We hypothesize that CHAIN suppresses churn in the training of value and policy networks, thus reducing the bias caused by churn in parameter updates. One thing here that differs from TD3 and SAC considered in the online setting is that the policy network of IQL has no impact on the training of the value networks since the value networks (i.e., \(Q\) and \(V\)) are trained purely based on in-sample data without accessing \(a^{}=_{}(s^{})\). Thus, although IQL does not exhibit a chain effect explicitly, the policy and value networks of IQL still have churns, which are reduced by CHAIN in this case. We provide a further empirical study in Appendix D.4.

   Task & IQL & CHAIN IQL (PCR) & CHAIN IQL (VCR) \\  AM-umaze-v2 & 77.00 \(\) 5.52 & **84.44 \(\) 3.19** & 83.33 \(\) 2.72 \\ AM-umaze-diverse-v2 & 54.25 \(\) 5.54 & 62.50 \(\) 3.75 & **71.67 \(\) 7.23** \\ AM-medium-play-v2 & 65.75 \(\) 11.71 & **72.50 \(\) 2.92** & 70.00 \(\) 3.33 \\ AM-medium-diverse-v2 & 73.75 \(\) 5.45 & **76.67 \(\) 4.51** & 66.67 \(\) 3.79 \\ AM-large-play-v2 & 42.00 \(\) 4.53 & **50.00 \(\) 4.56** & 43.33 \(\) 4.14 \\ AM-large-diverse-v2 & 30.25 \(\) 3.63 & 26.67 \(\) 3.96 & **31.67 \(\) 2.31** \\   

Table 1: Results for CHAIN IQL in Antmaze, with means and standard errors over twelve seeds.

   Task & AWAC & CHAIN AWAC (PCR) & CHAIN AWAC (VCR) \\  pen-human-v1 & 81.12 \(\) 13.47 & **99.72 \(\) 2.04** & 97.37 \(\) 3.51 \\ pen-cloned-v1 & 89.56 \(\) 15.57 & 95.49 \(\) 2.34 & **96.66 \(\) 2.54** \\   

Table 2: Results for CHAIN AWAC in Adroit, with means and standard errors over twelve seeds.

Figure 6: The evaluation of CHAIN TD3 and CHAIN SAC in MuJoCo regarding episode return.

### Scaling DRL Agents with CHAIN

It is widely known that scaling DRL agents up is challenging. Naively scaling DRL agents by widening or deepening the conventional MLP networks straightforwardly often fails and could even lead to collapse. Here, we investigate the relationship between churn and scale for DRL agents, as well as the effect of CHAIN on boosting scaling performance, to answer Question (3). We take PPO and MuJoCo tasks as the exemplary setting and scale up both the policy and value networks by a scale-up ratio within \(\{2,4,8,16\}\) via _widening_ or _deepening_. Note that the default network architecture (i.e., when the scale-up ratio equals one) for both the policy and value networks is a two-layer MLP with \(256\) neurons for each layer, followed by an output layer.

As expected, we observed that the performance of PPO degraded severely as the scale-up ratio increased, as shown by the solid gray lines in Figure 7. Inspired by the prior study (Obando-Ceron et al., 2024), we found using a decreased learning rate as lr / sqrt(scale-up ratio) alleviates the degradation of PPO scaling to some degree (shown by the solid red lines). We then use the learning rate setting below by default. From the perspective of churn, we also observed that scaling PPO escalates the scale of the policy churn in PPO. More results can be found in Appendix D.5. Therefore, we then evaluate the effect of CHAIN in this scaling setting. The results are shown in Figure 7 with dashed lines. By comparing the lines in the same color, we found that CHAIN improves the learning performance of PPO across almost all scale-up ratios and the two learning rate settings.

In addition, we extend the training horizon from 2M to 10M for Ant, Humanoid, and Walker2d. The results are reported in Table 3. For both widening or deepening cases, CHAIN helps to scale up PPO and achieves clear improvement in terms of episode return. Comparatively, scaling by widening slightly outperforms deepening, which echoes the observation in (Ota et al., 2020) to some extent.

Our results indicate that uncontrolled churn could be a possible reason for the scaling issue of DRL agents, and CHAIN improves scaling by reducing churn effectively. Though appealing, CHAIN does not fully address the scaling issue per se, and achieves sub-linear scaling on DRL agents.

## 7 Conclusion

In this paper, we conduct a formal study of churn in a general view and present the chain effect of value and policy churn. The chain effect indicates a compounding cycle, which biases parameter updates throughout learning. We propose an easy-to-implement method for value and policy churn reduction. Our experimental results demonstrate the effectiveness of our method in reducing churn and improving learning performance over a range of DRL environments and algorithms.

   Alg. (scale) & Ant (10M) & Human. (10M) & Walker2d (10M) \\  PPO & 2238.45 \(\) 256.07 & 1620.45 \(\) 212.10 & 3316.77 \(\) 269.42 \\  PPO (4x wider) & 3013.95 \(\) 223.77 & 2998.18 \(\) 237.95 & 3795.05 \(\) 208.17 \\ CHAIN PPO (4x wider) & **4916.66**\(\) 109.01 & **4830.58**\(\) 231.42 & **4668.16**\(\) 234.45 \\  PPO (4x deeper) & 2777.62 \(\) 136.78 & 3489.47 \(\) 166.28 & 2845.68 \(\) 260.02 \\ CHAIN PPO (4x deeper) & **3760.46**\(\) 158.29 & **4090.80**\(\) 187.01 & **3242.83**\(\) 173.54 \\  PPO (8x wider) & 4235.63 \(\) 209.68 & 3198.01 \(\) 344.71 & 4335.17 \(\) 123.12 \\ CHAIN PPO (8x wider) & **5929.49**\(\) 205.23 & **5246.15**\(\) 130.73 & **5161.42**\(\) 343.40 \\  PPO (8x deeper) & 3364.46 \(\) 173.25 & 2780.67 \(\) 304.10 & 3057.89 \(\) 299.61 \\ CHAIN PPO (8x deeper) & **4278.76**\(\) 122.61 & **4344.48**\(\) 144.92 & **3342.29**\(\) 255.88 \\   

Table 3: Scaling PPO with CHAIN. Means and standard errors of final episode return over six seeds.

Figure 7: The results regarding episode return for scaling PPO via widening. CHAIN helps to scale almost across all the configurations. Similar results can be found for widening scaling in Figure 20.