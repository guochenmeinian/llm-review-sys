# Quantifying Aleatoric Uncertainty of the Treatment Effect: A Novel Orthogonal Learner

Valentyn Melnychuk1,*, Stefan Feuerriegel1, Mihaela van der Schaar2

1LMU Munich & Munich Center for Machine Learning (MCML), Germany

2University of Cambridge & Alan Turing Institute, United Kingdom

*Correspondence: melnychuk@lmu.de

###### Abstract

Estimating causal quantities from observational data is crucial for understanding the safety and effectiveness of medical treatments. However, to make reliable inferences, medical practitioners require not only estimating averaged causal quantities, such as the conditional average treatment effect, but also understanding the randomness of the treatment effect as a random variable. This randomness is referred to as _aleatoric uncertainty_ and is necessary for understanding the probability of benefit from treatment or quantiles of the treatment effect. Yet, the aleatoric uncertainty of the treatment effect has received surprisingly little attention in the causal machine learning community. To fill this gap, we aim to quantify the aleatoric uncertainty of the treatment effect at the covariate-conditional level, namely, the conditional distribution of the treatment effect (CDTE). Unlike average causal quantities, the CDTE is _not_ point identifiable without strong additional assumptions. As a remedy, we employ partial identification to obtain sharp bounds on the CDTE and thereby quantify the aleatoric uncertainty of the treatment effect. We then develop a novel, orthogonal learner for the bounds on the CDTE, which we call AU-learner. We further show that our AU-learner has several strengths in that it satisfies Neyman-orthogonality and, thus, quasi-oracle efficiency. Finally, we propose a fully-parametric deep learning instantiation of our AU-learner.

## 1 Introduction

Estimating causal quantities from observational data is crucial for decision-making in medicine [9; 12; 22; 30; 70]. For example, medical practitioners are interested in estimating the effect of chemotherapy vs. immunotherapy on patient survival from electronic health records to understand the best treatment strategies in cancer care. Here, common estimation targets are _averaged_ causal quantities such as the average treatment effect (ATE) and the conditional average treatment effect (CATE), yet averaged causal quantities do not allow for understanding the variability of the treatment effect.

What is needed for the reliability of causal quantities in medicine? To obtain _reliable_ causal quantities, one often needs to "move beyond the mean" [44; 68] and consider the inherent randomness in the treatment effect as a random variable. This randomness is referred to as _aleatoric uncertainty_[17; 60; 110]. Quantifying the aleatoric uncertainty of the treatment effect is relevant in medical practice to understand the probability of benefit from treatment [26; 60] and the quantiles and variance of the treatment effect [5; 17; 26; 33; 59]. As an example, averaged quantities such as the CATE would simply suggest a positive effect for some patients, while the probability of benefit from treatment can inform patients about the odds of being negatively affected by the treatment. Hence, aleatoric uncertainty of the treatment effect promises additional, fine-grained insights beyond simple averages.

Methods for quantifying the aleatoric uncertainty of the treatment effect have gained surprisingly little attention in the causal machine learning community. So far, machine learning for treatment

[MISSING_PAGE_FAIL:2]

approaches only use naive plug-in estimators/learners. Furthermore, even the derivation of the orthogonal loss is non-trivial as there is no efficient influence function at hand for the Makarov bounds.

Challenge 3 is that CATE is an unconstrained target estimand whereas Makarov bounds are **monotonous** and **contained** in the interval \(\). Notably, any constraints of the target estimand could be violated by orthogonal learners [71; 125]. Therefore, an orthogonal learner for Makarov bounds needs to be carefully adapted, especially to perform well in low-sample settings.

### Our contributions

In this paper, we develop a novel, orthogonal learner for estimating Makarov bounds which we call _AU-learner_, which allows to quantify the **al**eatoric **unc**rcertainty of the treatment effect. Our _AU-learner_ addresses all of the above-mentioned challenges 1 - 2. Further, our _AU-learner_ has several useful theoretical properties, such as satisfying Neyman-orthogonality and, thus, quasi-oracle efficiency . Finally, we propose a flexible, fully-parametric deep learning instantiation of our _AU-learner_. For this, we make use of conditional normalizing flows and call our method AU-CNFs.

To summarize, our contributions are as follows: 4

1. We derive a novel, orthogonal leaner called _AU-learner_ to quantify the aleatoric uncertainty of the treatment effect. For this, we estimate Makarov bounds on the CDF/quantiles of the conditional distribution of the treatment effect (CDTE).
2. We prove several favorable theoretical properties of our _AU-learner_, such as Neyman-orthogonality and, thus, quasi-oracle efficiency.
3. We propose a flexible deep learning instantiation of our _AU-learner_ based on conditional normalizing flows, which we call AU-CNFs, and demonstrate its effectiveness over several benchmarks.

## 2 Related Work

In the following, we briefly summarize the existing works on uncertainty quantification in the potential outcomes framework; on the identification of the CDTE; and on the estimation of Makarov bounds. For a more detailed overview of literature, we refer to Appendix A.

**Uncertainty quantification in the potential outcomes framework.** The (total) uncertainty of a predictive model in machine learning is generally split into (a) epistemic and (b) aleatoric uncertainty [41; 50].5 This split is important, as it informs a decision-maker about the source of uncertainty (see Fig. 2), especially in the context of the potential outcomes framework. (a) Epistemic uncertainty was studied for predictive models targeting at identifiable averaged causal quantities, such as conditional average potential outcomes (CAPOs) and CATE [51; 52]. (b) Aleatoric uncertainty, on the other hand, is _only_ identifiable for potential outcomes . Prominent methods focus on interventional (counterfactual) quantities such as: (i) CDF/quantiles estimation [6; 13; 31; 43; 86]; (ii) density estimation [65; 66; 91; 94; 97; 102; 124]; and (iii) distributional distances (also known as distributional treatment effects) estimation [16; 29; 62; 92; 100]. Yet, our work differs substantially from the above, as we aim at inferring the aleatoric uncertainty of the treatment effect, which is only partially identifiable.

Figure 2: Total uncertainty of the treatment effect can have different sources. Both upper and lower plots have the same total uncertainty but vastly different aleatoric and epistemic components. Yet, aleatoric uncertainty is non-identifiable (see Challenge 1).

**Identification of the distribution of the treatment effect.** Point identification of the distribution of the treatment effect (or, equivalently, a joint distribution of potential outcomes) is only possible under additional assumptions on the data-generating mechanism. A common example is, e. g., invertibility of latent outcome noise . Other works have rather focused on partial identification . For example,  proposed sharp bounds for the distribution of the treatment effect under a monotonicity assumption. Later, assumption-free sharp bounds were proposed for both the joint CDF of potential outcomes  and for the variance of treatment effect , both known as Frechet-Hoeffding bounds . Finally,  proposed sharp bounds on the CDF/quantiles of the treatment effect without any additional assumptions, so-called _Makarov bounds_. Makarov bounds were further generalized  and applied to other settings  but different from ours.

**Estimation of Makarov bounds.** Table 1 provides a comparison of key methods for estimating Makarov bounds, at both covariate-conditional and population levels. Existing methods build mainly upon plug-in (single-stage) estimators/learners. Examples are methods tailored for randomized controlled trials  and for potential outcomes framework . Crucially, these methods are _not_ orthogonal and, thus, are sensitive to the misspecification of the nuisance functions. Nevertheless, we include the latter methods  as baselines for our experiments as they use a highly flexible CDF estimator based on kernel density estimators. Some works also developed efficient estimators for Makarov bounds at the population level (analogous to the two-stage Neyman-orthogonal learners at the covariate-conditional level) but only in highly restricted settings. In particular,  is restricted to binary outcomes,  assumed a known propensity score, and  made special optimization assumptions.5 In addition, all three works  suggest fixing a value of \(/\), which the CDF/quantiles of the treatment effect are evaluated at; when our work suggests targeting at several values of \(/\) at once. Therefore, the previous methods are _not_ applicable to our general setting of estimating covariate-conditional level Makarov bounds.

**Research gap.** To the best of our knowledge, we are the first to propose an orthogonal learner for estimating Makarov bounds on the CDF/quantiles of conditional distribution of the treatment effect.

## 3 Identification of Distribution of Treatment Effect

**Notation.** Let capital letters \(X,A,Y,\) denote random variables and small letters \(x,a,y,\) their realizations from domains \(,,,\). Let \((Z)\) denote a distribution of some random variable \(Z\), and let \((Z=z)\) be the corresponding density or probability mass function. Furthermore, \((x)=(A=1 X=x)\) is propensity score, \(_{a}(x)=(Y X=x,A=a)\) are conditional expectations, and \(_{a}(y x)=(Y y x,a)\) is a conditional outcome CDF. For other conditional quantities or distributions, we use short forms whenever possible; e. g., \((Y x)=(Y X=x)\). Further, \(_{n}\{f(Z)\}=_{i=1}^{n}f(z_{i})\) is a sample average of a random \(f(Z)\), where \(n\) is the sample size. We denote linear rectifier functions as \([x]_{+}=(x,0)\) and \([x]_{-}=(x,0)\), and sup/inf convolutions of two functions \(f_{1}( x)\), \(f_{2}( x)\) as \((f_{1}}{{=}}}f_{2})_{}(  x)=_{y}\{f_{1}(y x)-f_{2}(y- x)\}\) and \((f_{1}}{{=}}}f_{2})_{}(  x)=_{y}\{f_{1}(y x)-f_{2}(y- x)\}\).

**Problem setup.** We consider the standard setting of the Neyman-Rubin potential outcomes framework . That is, we have an observational dataset \(\) with a binary treatment \(A=\{0,1\}\), potentially high-dimensional covariates \(X^{d_{x}}\) and a continuous outcome \(Y\). For instance, a typical scenario is in cancer therapy, where the outcome is tumor growth, the treatment is whether chemotherapy is given, and the covariates include patient details like age and sex. We define a joint random variable \(Z=(X,A,Y)\). \(=\{x_{i},a_{i},y_{i}\}_{i=1}^{n}\) is sampled i.i.d. from the observational

    &  &  & Estimator/ &  &  \\  & conditional & & & learner & & \\  Fan et al.  & (✓) & — & Pug-in & ✗ & Empirical CDF \\ Ruiz et al.  & ✓ & — & Pug-in & ✗ & Conditional mean with hold-out residuals \\ Lee , Cui et al.  & ✓ & — & Pug-in & ✗ & Kernel density estimation \\ Kalus  & ✗ & Binary outcome & A-JPPV/NO & ✓ & Random forest \& causal forest \\ Ji et al.  & ✗ & Optimization assumptions & A-JPPV/NO & ✓ & Homokedastic Gaussian linear model \\ Semenova  & ✗ & Propensity score is known & IPTW & ✓ & — \\   & ✓ &  & ✓ & Conditional normalizing flows (AU-CNPs) \\   \\ 

Table 1: Overview of methods for estimating Makarov bounds on the CDF/quantiles of the CDTE.

distribution \((Z)=(X,Y,A)\), where \(n\) is the sample size. The potential outcomes framework then makes three (causal) assumptions, i. e., (1) _consistency_: if \(A=a\), then \(Y[a]=Y\); (2) _overlap_: \((0(X) 1)=1\); and (3) _exchangeability_: \(A(Y,Y) X\).

**Treatment effect distribution.** In this paper, we refer to the _treatment effect_\(=Y-Y\) as a random variable.6 The CATE is given by \((x)=( x)\), which is identifiable as \(_{1}(x)-_{0}(x)\) under the causal assumptions (1)-(3). We are interested in identifying a _conditional distribution of the treatment effect (CDTE)_, specifically, its CDF or quantiles:

\[( x) =( x)=(Y-Y  x),\] (1) \[^{-1}( x) =\{( x)\}, .\] (2)

The CDF and the quantiles of the CDTE are point _non_-identifiable due to the fundamental problem of causal inference, i. e., that the counterfactual outcome, \(Y[1-A]\), is never observed. This is illustrated in Fig. 3, where both conditional potential outcome distributions are identifiable as \((Y[a] x)=(Y x,a)\); but a conditional joint distribution, \((Y,Y x)\), and the CDTE, \(( x)\), are not.

**Partial identification of the CDTE.** Fan et al.  proposed pointwise sharp bounds on the CDF and the quantiles of the CDTE, so-called _Makarov bounds_[87; 126; 129]. Given that the outcome \(Y\) is continuous, the Makarov bounds for the CDF of the CDTE are given by linearly rectified sup/inf convolutions  of conditional CDFs of potential outcomes:

\[}( x)( x) }( x),\] (3) \[}( x)=[(_{1}}{{}}}_{0})_{}(  x)]_{+}}( x)=1+[( _{1}}{{}}}_{0})_{ }( x)]_{-}\,,\]

where \(\), and \(_{a}(y x)\) is the CDF of \((Y[a] x)\). Similarly, Makarov bounds can be formulated for the quantiles of the CDTE:

\[}^{-1}( x)^{-1}(  x)}^{-1}( x),\] (4) \[}^{-1}( x)=(_{1}^{-1}}{{}}}_{0}^{-1})_{[ ,1]}( x),& 0,\\ _{1}^{-1}(0 x)-_{0}^{-1}(1 x),&=0, }^{-1}( x)=( _{1}^{-1}}{{}}}_{0}^{- 1})_{[0,]}(-1 x),& 1,\\ _{1}^{-1}(1 x)-_{0}^{-1}(0 x),&=1,\]

where \(\), and \(_{a}^{-1}(u x)\) are the quantiles of \((Y[a] x)\). Then, under the causal assumptions (1)-(3), conditional distributions of potential outcomes coincide with observed ones, i. e., \((Y[a])=(Y x,a)\). Notably, Makarov bounds on the CDFs are CDFs themselves, but these CDFs do not correspond to the solution of the partial identification task (which implies pointwise sharpness). We refer to Appendix B for more illustrations about the inference of the Makarov bounds, the explanation of pointwise sharpness, and Makarov bounds for categorical/mixed-type outcomes.

## 4 An AU-learner for estimating Makarov bounds

In the following, we develop a theory of orthogonal learning for Makarov bounds, which then gives rise to our _AU-learner_. For this, we first review the plug-in learner and its shortcomings. Motivated by this, we then derive two-stage learners. Here, we first present a novel CA-learner in an intermediate step and finally our _AU-learner_. Note that both are novel but we frame our contributions around the _AU-learner_ because of favorable theoretical properties. For notation, we use over- and underlines as in, e. g., \(}( x)\) to refer to the upper and/or lower bound.

### Single-stage learners for Makarov bounds

**Plug-in learner.** A naive way to construct estimators for Makarov bounds [17; 26; 77; 110] is to estimate conditional outcome CDFs, \(}_{a}(y x)\), and plug them into Eq. (3):

\[}}_{}( x)=(}_{1}}{{}}}}_{0})_{}( x)_{+}}}_{}( x)=1+(}_ {1}}{{}}}}_{0})_{ }( x)_{-}.\] (5)

The plug-in learner for the bounds of the quantiles, \(}}_{}^{-1}( x)\), can be obtained in a similar way but where one uses Eq. (4) and relies on the estimators of either the conditional outcome quantiles or inverse of the estimated conditional outcome CDFs, \(}_{a}^{-1}(u x)\).

**Shortcomings.** The plug-in learner suffers from two important shortcomings . **(a)** The plug-in learner does _not_ account for the selection bias, meaning that \(_{1}\) is estimated better for the treated population and \(_{0}\) for the untreated. Hence, it could be necessary to _re-weight the loss_ wrt. to the propensity score. A remedy is to employ an inverse propensity of treatment weighted (IPTW) learner for both \(_{0}\) and \(_{1}\)[6; 43] (see Appendix C). **(b)** The plug-in learner does _not_ target the Makarov bounds directly but rather the conditional outcome distributions. Therefore, it is _unclear_ how to incorporate an inductive bias that _the Makarov bounds are less heterogeneous than either of the conditional outcome CDFs_ (i. e., the Makarov bounds can depend on a subset of covariates \(X\)). The second shortcoming thus motivates our derivation of two-stage learners.

### Two-stage learners for Makarov bounds

In order to address the above shortcomings of the plug-in learners, a two-stage learning theory was proposed [15; 35; 72; 96]. Yet, the two-staged learning theory is primarily built for simple target estimands (e. g., CATE) and therefore requires _non-trivial adaptations_ by us to extend to Makarov bounds, which we do in the following.

**Working model & target risk.** Our two-stage learners seek to find the best approximation of the ground-truth Makarov bounds, functional target estimands, by a (parametric) _working model_\(\). Formally, the working model is given \(=\{g(,x) g:;\,g(,x) {is non-decreasing}\}\). The best approximation \(g_{*}\) is then obtained by minimizing a (population) _target risk_ via \(g_{*}=*{arg\,min}_{g}(g)\).7 In our setting, \(\) is chosen as some distributional distance between the target estimands and the working model. Specifically, we use continuous ranked probability score (CRPS) [40; 117] as a target risk for learning Makarov bounds on the CDF via

\[}_{}(g)=(_{}( }( X)-g(,X))^{2} ),\] (6)

or squared Wasserstein-2 distance as a target risk for learning Makarov bounds on the quantiles via

\[}_{W_{}^{2}}(g^{-1})=(_{0 }^{1}(}^{-1}( X)-g^{-1}(,X))^ {2}).\] (7)

Figure 3: An example showing point non-identifiability of the distribution of the treatment effect based on the \(i=7\)-th instance of the semi-synthetic IHDP100 dataset . Shown are two data-generation models, indistinguishable in potential outcomes framework or RCTs, i. e., a monotone, \(_{}\), and an antitone, \(_{}\). For both models we also plot **(a)** conditional densities of potential outcomes, \((Y[a]=y x_{7})\) and conditional joint laws of potential outcomes, \((Y,Y x_{7})\); and **(b)** corresponding CDFs of the CDTE (shown in blue), \(( x_{7})=(Y-Y x_{7})\), together with Makarov bounds (shown in gray ) and point identifiable CATE (shown in green), \((x_{7})=( x_{7}) 2.342\). Non-identifiability of the CDTE is easy to see: Both data-generation models have the same conditional distributions of potential outcomes but different conditional joint laws and, thus, different CDTEs. The latter figures, **(b)**, also demonstrate the bounds on the probability of benefit from treatment (a special case of Makarov bounds), \((Y-Y 0 x_{7})[0,0.242]\). Hence, Makarov bounds are informative almost everywhere (except \(=(x_{7})\)).

The target risks in Eq. (6) and (7) cannot be directly minimized as we do not observe the ground-truth Makarov bounds. Yet, due to the identifiability results in Sec. 3, the Makarov bounds depend on the nuisance functions, \(_{a}(y x)\), which can be estimated from the observational data. We perform that in the following.

From now on, we denote the ground-truth nuisance functions as \(\) and their estimates as \(\). Also, we make the dependence on the target risks of the nuisance functions explicit; that is, we write the target risk as \((g,)\) and the target estimands as \(}( X;)\) and \(}^{-1}( X;)\).

**Covariate-adjusted learner.** A straightforward way to estimate and then minimize the target risk is to plug-in the estimates of conditional outcome CDFs, \(}_{a}(y x)\), into Eq. (6) and (7), respectively. This yields the so-called _covariate-adjusted (CA) learner_, which aims at minimizing the following losses (empirical risks):

\[}}_{,}(g,=(}_{0},}_{1}))=_{n} _{}}_{}( X; )-g(,X)^{2}\,},\] (8) \[}}_{,W_{2}^{2}}(g^{-1}, =(}_{0}^{-1},}_{1}^{-1}))= _{n}_{0}^{1}}_{}^{ -1}( X;)-g^{-1}(,X)^{2}\, },\] (9)

where we call both \(}_{}( x);)=}}_{}( x)\) and \(}_{}^{-1}( x;)=}}_{}^{-1}( x)\)_pseudo-CDFs_ and _pseudo-quantiles_, respectively, and where both can be obtained from Eq. (5).

The CA-learner addresses the shortcoming **(b)** of the plug-in learner from above in that loss minimization in the equations above targets directly at Makarov bounds. However, the shortcoming **(a)** of the selection bias still persists. Furthermore, a new shortcoming **(c)** now emerges: The losses can be highly sensitive to badly estimated nuisance functions so that \(}_{}(g,)\) and \(}_{}(g,)\) differ significantly. Next, we develop an orthogonal learner that addresses all of the shortcomings.

**One-step bias correction.** In order to address the before-mentioned shortcomings of the CA-learner, we employ the concept of (Neyman-)orthogonal losses . Informally, orthogonal losses are first-order insensitive to the misspecification of the nuisance functions, which introduces many favorable properties such as quasi-oracle efficiency  and double robustness . The CA-learner losses in Eq. (8) and (9) can be made orthogonal by performing a _one-step bias correction_. The one-step bias correction requires the knowledge of an _efficient influence function_, which has not yet been derived for Makarov bounds (\(\) Challenge 2). Hence, the following theorem presents one of our main theoretical results.

**Theorem 1** (Efficient influence function for Makarov bounds).: _Let \(\) denotes \((Z)=(X,A,Y)\), and let \(y_{}^{-}( x)\) and \(u_{[,1]}^{}( x)\) be argmax/argmin sets of the convolutions \((_{1}}_{0})_{}(  x)\) and \((_{1}^{-1}}_{0}^{-1})_{[,1]} (-0 x)\), respectively. Then, under mild conditions on the conditional outcome distributions and for almost all values of \(\) and for all values of \((0,1)\) (see Appendix D), average Makarov bounds are pathwise differentiable. Further, the corresponding efficient influence functions, \(\), are as follows:_

\[((}( X));) =}(,Z;)+}( X; )-(}( X;))\] (10) \[((}^{-1}( X)); )=}^{-1}(,Z;)+}^{ -1}( X;)-(}^{-1}( X; )),\] \[(,Z;)=I(X;) 1\{Y y^{*}\}-_{1}(y^{*} X)- 1\{Y y^{*}-\}-_{0}(y^{*}- X),\] (11) \[^{-1}(,Z;)=_{1}^{-1}(u^{*} X)\}-u^{*}}{(Y=_{1}^{ -1}(u^{*} X) X,A=1)}-_{0}^{-1}(u^{*}-+0 X)\}-(u^{*}-+0)}{ (Y=_{0}^{-1}(u^{*}-+0 X) X,A=0)},\] (12)

_where \(I(X;)=(_{1}}_{0})_{}( X)>0}\); \(y^{*}\) is some value from the finite set \(y_{}^{-}( X)\); \(u^{*}\) is some value from the finite set \(u_{[,1]}^{}( X)\); and \((,Z;)\) and \(^{-1}(,Z;)\) can be then obtained by swapping the symbols \(\{,>,y_{}^{-},u_{[,1]}^{-},-0,+0\}\) to \(\{,<,y_{}^{+},u_{[0,]}^{},-1,+1\}\)._

Proof.: See Appendix D.

Figure 4: Comparison of learners for estimating Makarov bounds.

In the above theorem, we use red color to show the nuisance functions of \(\) that are influencing the target estimand, i. e., averaged Makarov bounds. Therein, we also provide a Corollary 1, where we derive efficient influence functions for the target risks from Eq. (6) and (7), namely \(((g);)\).

**Note on infinite argmax/argmin sets.** When argmax/argmin sets of the sup/inf-convolutions are infinite, average Makarov bounds are pathwise non-differentiable, and, thus, one-step bias correction is not possible as statistical inference becomes non-regular . This result also holds for other causal quantities that contain sup/inf operators (e. g., for the policy value of the optimal treatment strategy ). Although, there exist approaches to perform inference in the non-regular setting , we focus solely on the regular setting where pathwise differentiability holds (see Appendix D for a discussion on the generality of such a setting).

**Orthogonal leaner (_AU-learner_).** Given the derived efficient influence function for the target risks, we perform a \(\)-scaled one-step bias correction  of the CA-learner losses, namely, \(}_{}(g,)+_{n} ((g);)}\). The latter then yields our novel orthogonal _AU-learner_ (see Corollary 2 in Appendix D). Our _AU-learner_ effectively resolves all the above-mentioned shortcomings (see a comparison in Fig. 4). Formally, it aims at minimizing one of the following losses:

\[}}_{}(g,=( ,}_{0},}_{1}))=_{n} _{}}_{}(,Z;,)-g(,X)^{2}\,},\] (13) \[}}_{W^{2}_{2}}(g^{-1}, {}=(,}_{0}^{-1},}_{1}^{-1}) )=_{n}_{0}^{1}}_{} ^{-1}(,Z;,)-g^{-1}(,X)^{2}\, },\] (14) \[}_{}(,Z;,)= }_{}( X;)+}}(,Z;)}_{}^{-1}(,Z;,)= }_{}^{-1}( X;)+}}^{-1}(,Z;),\]

where \(}}(,Z;)\) and \(}}^{-1}(,Z;)\) are given by Eq. (11) and (12), respectively; and \((0,1]\) is a scaling hyperparameter. We present a meta-algorithm of our _AU-learner_ (with the CRPS target risk) based on cross-fitting in Algorithm 1 (_AU-learner_ with the \(W^{2}_{2}\) target risk follows analogously).

**Scaling hyperparameter.** The scaling hyperparameter \(\) is introduced to tackle Challenge 3 from above, namely, that the pseudo-CDF term, \(}_{}(,Z;,)\), is not guaranteed to be a valid CDF for \(>0\) (both monotonicity wrt. \(\) and \(\)-constraint can be violated).8 The same happens with the pseudo-quantiles of the _AU-learner_, \(}_{}}^{-1}(,Z;,)\), which could be non-monotonous wrt. \(\). The main intuition behind scaling is that it interpolates between the full _AU-learner_ (\(=1\)), that has favorable theoretical properties; and the CA-learner (\(=0\)), for which the pseudo-CDFs and pseudo-quantiles are valid CDFs and quantiles, respectively (we refer to Appendix E with visual examples). Hence, scaling mimics a learning rate of a Newton-Raphson method (usually considered as an analogy to the one-step bias correction ). We found fixed values for the scaling hyperparameter \(\) to work well in all of our experiments and to improve the low-sample performance of our _AU-learner_.

### Theoretical properties of AU-learner

In the following, we formulate our second main theoretical result. For the results to hold, the nuisance functions \(=(,_{0},_{1})/=(,_{0}^{-1},_{1}^{-1})\) need to be estimated independently from the second stage model \(g/g^{-1}\). This could be done by either assuming a not-too-flexible class of models (such as a Donsker class of estimators and fitting all the models on the same dataset \(\)) or by using a generic approach of cross-fitting .

**Theorem 2** (Neyman-orthogonality of AU-learner (informal)).: _Under the assumptions of the Theorem 1, the following holds for AU-learner from Algorithm 1 with the scaling hyperparameter \(=1\):_

1. _Neyman-orthogonality._ _Losses in Eq. (_13_) and Eq. (_14_) are first-order insensitive wrt. to the misspecification of the nuisance functions._2. **Quasi-oracle efficiency.** _The bias from the misspecification of the nuisance functions is of second order._

We refer to the Appendix D for the detailed formulation of the theorem and the proof. Notably, the quasi-oracle efficiency  means that our _AU-learner_ with (sufficiently fast) estimated nuisance functions performs nearly identical to the _AU-learner_ with the ground-truth nuisance functions.

## 5 Neural instantiation with AU-CNFs

We now introduce a flexible fully-parametric instantiation of our _AU-learner_, which we call AU-CNFs. Therein, we employ conditional normalizing flows (CNFs) [105; 121] as the main backbone for our _AU-learner_. CNFs are a flexible neural probabilistic method with tractable conditional densities, CDFs, and quantiles. Importantly, all three attributes of the CNFs (densities, CDFs, and quantiles) can be used for both training via back-propagation and inference , which makes them a perfect model for both stages of our _AU-learner_.

**Architecture.** The architecture of our AU-CNFs is inspired by interventional normalizing flows (INFs)  (a two-stage model for efficient estimation of potential outcomes densities). Our AU-CNFs consist of several CNFs corresponding to the two stages of learning, namely a _nuisance CNF_, which fits the nuisance functions, \((,}_{0},}_{1})\) or, equivalently, \((,}_{0}^{-1},}_{1}^{-1})\); and _two target CNFs_, which implement second stage working models for upper and lower bounds, \(}\) and \(}\), respectively.

**Training & implementation.** At the first stage of AU-CNFs learning, the nuisance CNF aims at maximizing the conditional log-likelihood and minimizing a binary cross-entropy via a joint loss. Then, we generate the pseudo-CDFs and pseudo-pseudo quantiles, as described in Algorithm 1. Therein, we set the \(\)/\(\)-grid size to \(n_{}=n_{}=50\) and discretize the \(\)-space/\(\)-interval to infer the argmax/argmin values, \(^{}/^{}}\). Then, we proceed with the second stage of AU-CNFs learning, where we set \(=0.25\) for the CRPS loss and \(=0.01\) for the \(W_{2}^{2}\) loss. We found the fixed values of \(\) to work well in _all of the synthetic and semi-synthetic experiments_ (except for the IHDP100 dataset, where the overlap assumption is violated). We use the same training data for two stages of learning, as (regularized) CNFs as neural networks belong to the Donsker class of estimators . We refer to Appendix F for more details on our AU-CNFs.

## 6 Experiments

We now evaluate our _AU-learner_. For this, we use (semi-)synthetic benchmarks with the ground-truth conditional CDFs/quantiles of potential outcomes, i. e., \(_{a}(y x)/_{a}^{-1}(u x)\). In this way, we can infer the ground-truth Makarov bounds and use them for evaluation.

**Evaluation metric.** We use evaluation metrics based on the target risks (as introduced in Sec 4.2). Specifically, we report root continuous ranked probability score (rCRPS) and Wasserstein-2 distance (\(W_{2}\)) based on training data (in-sample) and test data (out-sample).

**Baselines.** We compare the proposed hierarchy of learners from Sec. 4 with CNFs as backbones. These are the plug-in learner (**Plug-in CNF**), IPTW-learner (**IPTW-CNF**), CA-learners (**CA-CNFs (**CRPS / \(W_{2}^{2}\)**)), and AU-learners (**AU-CNFs (**CRPS / \(W_{2}^{2}\)**)). The only relevant baseline found in the literature is a plug-in learner based on kernel density estimation [17; 77; 110].9 For this, we used distributional kernel mean embeddings  (**Plug-in DKME**), a standard conditional kernel density estimation method. Details on the baselines are in Appendix G.

**Synthetic data.** We adapt the synthetic data generator (\(d_{x}=2\)) from [61; 93] by creating three settings with different conditional outcome distributions: normal, multi-modal, and exponential (see data generation details in Appendix H). In the synthetic data, _the ground-truth Makarov bounds are less heterogeneous than the potential outcomes_, and, hence, two-stage learners are expected to perform the best. We sample \(n_{}\{100;250;500;750;1000\}\) training and \(n_{}=1000\) test datapoints. The out-sample results are in Fig. 5. Here, our AU-CNFs perform the best wrt. rCRPS in the majority of settings and different sizes of training data. We also report the results wrt. \(W_{2}^{2}\) in Appendix I.

**HC-MNIST dataset.** HC-MNIST is a high-dimensional semi-synthetic dataset (\(d_{x}=785\)), built on top of the MNIST dataset  (see details in Appendix H). Here, the heterogeneity of Makarov bounds is also smaller than that of potential outcomes, reflecting inductive biases in the real world. We report the out-sample performance of different methods in Table 2 (the Plug-in DKME is omitted due to a too-long runtime). Therein, our AU-CNFs (CRPS) achieve the best performance and, thus, scale well with the dataset size and the dimensionality of covariates. Further, our AU-CNFs (\(W_{2}^{2}\)) improve the performance of CA-CNFs (\(W_{2}^{2}\)). In general, we observe that the loss based on the CDF distance (i. e., CRPS) has a lower variance and is easier to fit. In Appendix I, we additionally report the results for another popular semi-synthetic benchmark, HDP100 .

**Case study.** In Appendix J, we provide a real-world case study based on the observational dataset from . Therein, we demonstrate how our _AU-learner_ (AU-CNFs) can be used to estimate the effectiveness of lockdowns during the COVID-19 pandemic. We estimate the probability of benefit from intervention (a special case of Makarov bounds with \(=0\)). As expected, we observe a drop in the incidence rate is highly probable after the implementation of a strict lockdown.

## 7 Discussion

**Low-sample & asymptotic performance.** In several experiments, especially in low-sample settings, the CA-learner or even the plug-in approach are performing nearly as well or even sometimes better than the _AU-learner_. This can be expected, as the best low-sample learner and the asymptotically best learner can, in general, be different , and there is no single "one-fits-all" data-driven solution to choose the former one . This can be explained by too small dataset sizes or the severe overlap violations (as is the case with the IHDP100 dataset; see Appendix I). Yet, only our quasi-oracle efficient AU-learner offers asymptotic properties in the sense that it is asymptotically closest to the oracle (see Fig. 4). We thus argue for a pragmatic choice in practice (i. e., in the absence of ground-truth counterfactuals or additional RCT data) where our _AU-learner_ should be the preferred method for the covariate-conditional Makarov bounds even in low-sample data.

**Future work.** Our work sets a foundation for several extensions to estimate covariate-conditional Makarov bounds. For example, the estimation of the interval probabilities (see Appendix B) of the treatment effect can provide a connection with the existing works on total uncertainty with conformal prediction . Additionally, one might want to study possible extensions of Makarov bounds tailored to high-dimensional outcomes.

**Limitations & broader impact.** Our work is subject to the standard assumptions of the potential outcomes framework. We further make assumptions on the outcome distribution, though these are very mild. Nevertheless, we expect our work to have a positive impact, as it will help to improve the reliability of decision-making in medicine and other safety-critical fields.

**Conclusion.** We are the first to offer a theory of orthogonal learning to quantify the aleatoric uncertainty of the treatment effect at the covariate-conditional level and present flexible neural instantiation.

    &  \\  & \(_{}\) & \(W_{2}\) \\  Plug-in CNF & \(0.399 0.162\) & \(1.051 0.514\) \\ IPTV-CNF & \(0.385 0.501\) & \(0.986 1.936\) \\ CA-CNFs (CRPS) & \(0.382 0.045\) & \(1.029 0.119\) \\ CA-CNFs (\(W_{2}^{2}\)) & \(0.494 0.229\) & \(1.239 0.541\) \\ AU-CNFs (CRPS) & \(\) & \(\) \\ AU-CNFs (\(W_{2}^{2}\)) & \(0.422 0.139\) & \(1.065 0.523\) \\    
    &  \\  Plug-in CNF & \(0.391 0.104\) & \(1.003 0.333\) \\ IPTV-CNF & \(0.447 0.218\) & \(1.141 0.917\) \\ CA-CNFs (CRPS) & \(0.388 0.051\) & \(1.011 0.146\) \\ CA-CNFs (\(W_{2}^{2}\)) & \(0.446 0.148\) & \(1.089 0.341\) \\ AU-CNFs (CRPS) & \(\) & \(\) \\ AU-CNFs (\(W_{2}^{2}\)) & \(0.415 0.190\) & \(1.025 0.520\) \\  Lower = better (best in bold) &  \\   

Table 2: Results for HC-MNIST. Reported: median out-sample rCRPS \(\) sd / \(W_{2}\)\(\) sd over 10 runs.

Figure 5: Results for synthetic experiments with varying size of training data, \(n_{}\), in 3 settings: normal, multi-modal, and exponential. Reported: mean out-sample rCRPS over 20 runs.