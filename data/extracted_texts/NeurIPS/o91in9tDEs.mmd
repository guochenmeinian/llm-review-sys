# Distributional Policy Evaluation: a Maximum Entropy approach to Representation Learning

Riccardo Zamboni

DEIB, Politecnico di Milano

Milan, Italy

riccardo.zamboni@polimi.it

&Alberto Maria Metelli

DEIB, Politecnico di Milano

Milan, Italy

albertomaria.metelli@polimi.it

&Marcello Restelli

DEIB, Politecnico di Milano

Milan, Italy

marcello.restelli@polimi.it

###### Abstract

The Maximum Entropy (Max-Ent) framework has been effectively employed in a variety of Reinforcement Learning (RL) tasks. In this paper, we first propose a novel Max-Ent framework for policy evaluation in a distributional RL setting, named _Distributional Maximum Entropy Policy Evaluation_ (D-Max-Ent PE). We derive a generalization-error bound that depends on the complexity of the representation employed, showing that this framework can explicitly take into account the features used to represent the state space while evaluating a policy. Then, we exploit these favorable properties to drive the representation learning of the state space in a Structural Risk Minimization fashion. We employ state-aggregation functions as feature functions and we specialize the D-Max-Ent approach into an algorithm, named _D-Max-Ent Progressive Factorization_, which constructs a progressively finer-grained representation of the state space by balancing the trade-off between preserving information (bias) and reducing the effective number of states, i.e., the complexity of the representation space (variance). Finally, we report the results of some illustrative numerical simulations, showing that the proposed algorithm matches the expected theoretical behavior and highlighting the relationship between aggregations and sample regimes.

## 1 Introduction

In Distributional Reinforcement Learning (D-RL) (Bellemare et al., 2023), an agent aims to estimate the entire distribution of the returns achievable by acting according to a specific policy. This is in contrast to and more complex than classic Reinforcement Learning (RL) (Szepesvari, 2010; Sutton and Barto, 2018), where the objective is to predict the expected return only. In recent years, several algorithms for D-RL have been proposed, both in evaluation and control settings. The push towards distributional approaches was particularly driven by additional flavors they can bring into the discourse, such as risk-averse considerations, robust control, and many regularization techniques (Chow et al., 2015; Brown et al., 2020; Keramati et al., 2020). Most of them varied in how the distribution of the returns is modeled. The choice of the model was shown to have a cascading effect on how such a distribution can be learned, how efficiently and with what guarantees, and how it can be used for the control problem. Similarly to this tradition, this paper investigates the potential of looking into the entire distribution of returns to address the representation learning of the state-action spaces, that is to find a good feature representation of the decision-making space so as to make theoverall learning problem easier, tenderly by reducing the dimensionality of such spaces. In particular, it points to answer the following research question:

Q1: _What tools can return distributions provide for distributional RL?_

In Section 3, we answer this methodological question, showing that it is possible to reformulate Policy Evaluation in a distributional setting so that its performance index is explicitly intertwined with the representation of the (state or action) spaces. More specifically, this work tackles Policy Evaluation in a distributional setting as a particular case of a distribution estimation problem and then applies the Maximum Entropy (Max-Ent) formulation of distribution estimation [Wainwright and Jordan, 2007]. In this way, it is possible to derive a novel framework for PE, which we name _Distributional Max-Ent Policy Evaluation_ (D-Max-Ent PE), which inherits the many positives the Max-Ent framework offers. In particular, it allows the inclusion of constraints that the distribution needs to satisfy, usually called _structural constraints_, namely feature-based constraints acting over the support of the distribution. Such constraints then appear in the generalization-error bound for the Max-Ent problem, with a term related to the complexity of the family of features used. Unfortunately, traditional derivations of this bound introduce quantities that are bounded but unknown. We develop the analysis further to derive a more practical bound, containing quantities that are either estimated or known. In this way, the generalization-error bound shows a usual bias-variance trade-off that can be explicitly optimized by changing the feature functions adopted, while making the best use of the available samples. Thus, PE in a distributional setting is directly linked to representation learning.

Now, the RL literature proved that reducing the state space size while preserving the important features of the original state space is beneficial, namely with state-aggregation feature functions [Singh et al., 1994, Van Roy, 2006, Dong et al., 2020]. This is particularly true when high dimensionality can make learning slower and more unstable, as in classic RL in general, or when the learning process is almost unfeasible in small-samples regimes, as for D-RL, where learning the entire distribution of returns requires a large number of samples. Thus, motivated by these considerations, while D-Max-Ent Policy Evaluation allows for the use of any type of structural constraint, this work focuses on state-aggregation feature functions, and we exploit the first and more methodological result to answer a second more algorithmic question:

Q2: _How are representation learning and policy evaluation intertwined? Do distributional methods offer a new way to highlight and exploit this connection?_

To answer this question, in Section 4, we show that the generalization-error bound changes monotonically when the state-aggregation constraints are changed in a specific way, namely a finer-grained representation of the space. This is indeed what happens in the Structural Risk Minimization theory (SRM) [Vapnik, 1991]. Similarly, we develop a novel algorithm called _D-Max-Ent Progressive Factorization_, which exploits the proposed evaluation method to learn a representation of the state space soundly, i.e., trying to reduce a proxy of the generalization error bound in an SRM fashion, and allowing us to answer positively to the second research question as well. Finally, in Section 5 we verify through an illustrative numerical simulation whether the proposed algorithm matches the behaviors suggested by the theoretical analysis.

## 2 Preliminaries

### Markov Decision Processes

A discrete-time finite Markov decision processes (MDP) [Puterman, 1994] is a tuple \((,,P_{},P_{}, ,)\), where \(\) is a finite state space (\(||=S\)), \(\) is a finite action space (\(||=A\)), \(P_{}:()\) is the transition kernel, \(P_{}:()\) is the reward distribution function, \(()\) is the initial-state distribution and \([0,1)\) is discount factor.1 A policy \(:()\) defines the behavior of an agent interacting with an environment, which goes as follows: starting from an initial state \(S_{0}\), an agent interacts with the environment through the policy \(\), generating a trajectory \(=(S_{t},A_{t},R_{t})_{t=0}^{}\), which is a sequence of states, actions and rewards whose joint distribution is determined by the transition kernel, reward distribution, and the policy itself, i.e., \(A_{t}(|S_{t})\), \(R_{t} P_{}(|S_{t},A_{t})\), and \(S_{t+1} P_{}(|S_{t},A_{t})\).

### Value Functions and Distributions of Returns

Given an MDP \(\) with discount factor \(\), the _Discounted Return_ is the sum of rewards received from the initial state onwards, discounted according to their time of occurrence:

\[^{}(s)=_{t=0}^{}^{t}R_{t}|S_{0}=s.\] (1)

The _Value Function_ of a given policy \(\) is the expectation of this quantity under the policy itself:

\[V^{}(s)=[(s)]=[_{t=0}^{} ^{t}R_{t}|S_{0}=s].\] (2)

The _Return Distribution Function_\(^{}\) of a given policy \(\) is a collection of distributions, one for each state \(s\), where each element is the distribution of the random variable \(^{}(s)\):

\[^{}(s)=^{}_{(s)}[_{t=0}^{}^{t}R_{t} |S_{0}=s],\] (3)

where \(^{}_{(s)}\) extracts the probability distribution of a random variable under the joint distribution of the trajectory.

The _Distributional Policy Evaluation Problem_ then consists of estimating the return distribution function of Eq. (3) for a fixed policy \(\).

### Maximum Entropy Estimation

_Maximum Entropy_ (Max-Ent) methods (Dudik and Schapire, 2006; Wainwright and Jordan, 2007; Sutter et al., 2017) are density estimation methods that select the distribution that maximizes the uncertainty, i.e., the one with maximum entropy, where the entropy of a distribution \(p()\) is defined as \(H(p)-_{X p}[ p(X)]\).2 Additionally, they assume that the learner has access to a feature mapping \(\) from \(\) to \(^{M}\). In the most general case, we may have \(M=+\). We will denote by \(\) the class of real-valued functions containing the component feature functions \(f_{j}\) with \(j[M]\). A distribution \(p\) is _consistent_ with the true underlying distribution \(p_{0}\) if

\[_{X p}[f_{j}(X)]=_{j}, j[M],\] (4)

where

\[_{j}_{X p_{0}}[f_{j}(X)]\] (5)

In this case, we say that \(p\) satisfies (in expectation) the structural constraints imposed by the features in \(\). In practice, \(p_{0}\) is not available and Max-Ent methods enforce empirical consistency over \(N\) independent and i.i.d. observations \(=\{x_{1},,x_{N}\} p_{0}\) with support in \(\) by replacing the definition in Eq. (5) with

\[_{j}()_{i=1}^{N}f_{j}(x_{i}),  j[M].\] (6)

The distribution \(p\) is said to be consistent with the data \(\) if it matches the empirical expectations. The empirical Max-Ent problem consists then of the following optimization problem

\[_{p()} H(p)\] (7) s.t. \[_{X p}[f_{j}(X)]=_{j}, j[M],\]

with the optimization problem in expectation differing just in the constraints (i.e., replacing constraint from Eq. (6) with the ones from Eq. (5)). It is well known that the optimal solution to the empiricalMax-Ent problem in Eq. (7) is a distribution \(p_{}()\) belonging to the class of exponential distributions parametrized by the parameters \(\), namely:

\[p_{}(x)=_{}(_{j[M]}_{j}f_{j}(x)),\] (8)

where \(_{}_{}(_{j[M]}_{j}f_ {j}(x^{}))dx^{}\) is a normalization constant, which ensures that \(p()\), and its log-transformation takes the name of log-partition function \(A()_{}(_{j[M]}_{j}f_{j}( x))dx\). The log-partition function defines the set of well-behaved distributions \(=\{^{M}:A()<+\}\). At optimality, the parameters are defined as \(\) and correspond to the optimal Lagrangian multipliers of the dual of the empirical Max-Ent problem in Eq. (7). Now on, we will use \(\) to identify \(p_{}\) for simplicity.

## 3 Distributional Policy Evaluation: A Max-Ent Approach

```
0:\((_{N},)\)\(N\) trajectory samples, set of features functions \(=*{argmax}_{}H()\) s.t. \(_{X}[f_{j}(X)]=_{j}(_{N}) j [M]\) \(()\)return\(\) ```

**Algorithm 1** Distributional Max-Ent Policy Evaluation

This section aims at answering the first research question Q1. The proposed approach turns distributional PE into a pure density estimation problem in a Max-Ent framework, called _Distributional Max-Ent Policy Evaluation_, as described in Algorithm 1. For this translation, the algorithm uses the distribution of returns \(\) as \(p\), \(N\)-trajectory samples \(_{N}=\{\}_{n=0}^{N}\) as data, and a fixed set of features functions \(\) belonging to a function class \(\). Note that to do this, we need to slightly change the notation concerning the D-RL framework: \(\) will not be a \(||\)-vector of distributions with support over \(\), but rather a joint distribution over the whole support \(=\). Turning PE into a Max-Ent problem has many upsides. First of all, the Max-Ent principle allows to deal with any kind of support \(\), unifying continuous and discrete cases under the same framework; secondly, it does not require specifying a family of probability distributions to choose from; moreover, it implicitly manages the uncertainty by seeking a distribution as agnostic as possible, i.e., as close to the uniform distribution as possible. Finally, Max-Ent allows to include of structural constraints over the return distribution under many different flavors, both as in the standard value-function approximation methods (Van Roy, 2006) and as in more recent works based on statistical functionals acting over the return portion \(\) of the support (Bellemare et al., 2023). One of the possible limitations might be the requirement to have access to a batch of i.i.d. samples, but this is not necessarily restrictive: the result can be generalized for a single \(\)-mixing sample path by exploiting blocking techniques (Yu, 1994; Nachum et al., 2019).

### Generalization Error Bound

As previously said, the inner properties of Max-Ent allow for translating the results from density estimation methods to the distributional PE setting, and in particular, generalization-error bounds defined as KL-divergences.3 Unfortunately, the generalization error bounds of traditional Max-Ent theory contain a conservative term that compares the solutions of the expectation and empirical Max-Ent problems, \(,\) respectively, by taking the maximum between the 1-norm of the respective multipliers, namely \(_{\{,\}}||||_{1}\). This quantity is bounded yet unknown, making the result unpractical. In the following, we extend the previous results with a more practical bound containing \(||||_{1}\) instead of the maximum, requiring some additional assumptions about the expressiveness of the feature functions. This result is of independent interest and allows us to directly use the bound from an algorithmic perspective.

**Theorem 1** (Generalization Error Bound of D-Max-Ent PE).: _Assume that the set of features \(\) belong to the function class \(\), which it is such that \(_{x,f}||f(x)||_{}=F<+\) and that the minimum singular value \(_{}\) of the empirical covariance matrix of the features \(()\) is strictly positive, namely \(_{}(}())>0\). Then, given a sample batch \(\{x_{1},,x_{N}\}^{N}\) of \(N\) i.i.d. points drawn from the true distribution \(^{}\), for any \((0,1)\), it holds with probability at least \(1-\) that the solution to the sampled Max-Ent problem \(\) satisfies the following:_

\[KL(^{}||) -H(^{})+}()+B(,,N,)\] (9) \[}() =-_{i=0}^{N}(x_{i})\] (10) \[B(,,N,) =10||||_{1}(_{N}()+F}),\] (11)

where \(\) stands for the fact that the bound comprises additional terms that decrease at a higher rate in sample complexity and were therefore neglected. \(H(^{})\) and \(}()\), the empirical log-likelihood of the solution, form a bias term. The remaining term \(B(,,N,)\) is a variance term depending on the multipliers characterizing the solution \(\), the number of samples, the confidence level \(\), and the feature class complexity as the empirical Rademacher complexity of the class \(_{N}()\)(Mohri et al., 2018).

### Proof Sketch

Here we report the main steps of the proof of Th. 1. The interested reader can find the complete proof in Appendix A. First, define the set containing the solutions to the expected and sampled Max-Ent problems with \(\{,\}\), the related set for the multipliers \(_{S}\{,\}\), and a quantity that will be central now on \(h(x_{1},,x_{N})_{}|_{^{ }}[]-_{i=1}^{N}(x_{i})|\). Then, the building blocks of the error term \(KL(^{}||)\), namely \(KL(||)\) and \(KL(^{}||)\) are bounded by:

\[KL(||)  2h()\] \[KL(^{}||) -H(^{})+}()+3h().\]

It is possible to show that:

\[h()  2_{_{}}||||_{1}( _{N}()+F})\] \[_{_{S}}||||_{1} ||||_{1}+(}())}h()}.\]

The first inequality is obtained with standard methods as in van der Vaart and Wellner (1996); Dudley (1999); Koltchinskii and Panchenko (2002); Wang et al. (2013). The second one is obtained by exploiting the intrinsic properties of the Max-Ent solution and by noting that it is possible to link \(h()\) with the Bregman divergence of the log-partition function \(D_{A}(,)\). One can see that the use of the second inequality introduces an additional assumption about the expressiveness of the feature functions, requiring the minimum singular value of the sampled covariance matrix \(_{}(}())\) to be strictly positive. As a final step, setting \(x=,,x_{N})}\) and combining the two previous inequalities yields a quadratic inequality:

\[\{&x^{2}-bx-c 0\\ &b=2(}())}} _{N}()+F}\\ &c=2||||_{1}_{N}()+F}.,\]

which is well-defined and solves for

\[h(x_{1},,x_{N})||||_{1}(_{N}( )+F}),\]by neglecting higher-order terms. The statement of the theorem is then just a matter of combining all these results.

## 4 Distributional Representation Learning with State Aggregation

This section addresses the second research question Q2, namely how to use the bound in Th. 1 from an algorithmic perspective to automatically refine the features used to represent the state space in a principled way while performing D-Max-Ent PE. In particular, the focus is on a specific instance of feature functions for return distributions, namely state aggregation. More specifically, the state aggregation feature functions \(=\{f_{j}\}_{j[M]}\) split the state space into \(M\) disjoint subsets, one for each function, i.e., \(=_{j[M]}S_{j}\) and \(S_{j} S_{j^{}}=,\ j,j^{}[M],\ j j^{}\), and gives back the associated return \(g\), namely:

\[& f_{j}:\\ & f_{j}(s,g)=g_{[s S_{j}]}.\] (12)

These features are bounded by the maximum return \(G_{}\), while the empirical Rademacher complexity over \(N\) samples of returns \(\{(s_{i},g_{i})\}_{i[N]}\) can be directly computed as in Clayton (2014):

\[_{N}()=G_{}_{j[M]}(S_{j})},\] (13)

where \((S_{j})=N_{j}/N\) and \(N_{j}=|\{(g_{i},s_{i}):s_{i} S_{j},i[N]\}|\). The decomposition of the Rademacher term into single terms leads to rewriting \(B(,,N,)\) as in the following lemma.

**Lemma 1**.: _For Distributional Max-Ent Evaluation with a state-aggregation feature class, the variance term \(B(,,N,)\) is given by;_

\[B(,,N,)=10||||_{1}G_{} (_{j[M]}(S_{j})}+} ).\] (14)

### Representation Refinement: Progressive Factorization

State aggregation features are of interest due to the possibility of progressively refining the representation by increasing the factorization level, that is, by splitting a subset \(S_{j}\) into further disjoint subsets. This refinement is called _progressive factorization_ and is defined as follows.

**Definition 1** (Progressive Factorization).: _For two sets of state aggregation feature functions, \(,_{j}\), we say that \(_{j}\) is a progressive factorization of \(\), i.e., \(_{j}\), if \(=\{f_{1},,f_{j-1},f_{j+1},,f_{M}\}\{f_{j}\}, _{j}=\{f_{1},,f_{j-1},f_{j+1},,f_{M}\}\{f_{j}^{k}\} _{k[K]}\) and the additional functions \(\{f_{j}^{k}\}_{k[K]}\) are such that the corresponding subsets satisfy_

\[S_{j}=_{k[K]}S_{j}^{k}, S_{j}^{k} S_{j}^{k^{}}= ,\ k,k^{}[K],\ k k^{},\]

_where only non-degenerate class factorizations will be considered, meaning that the new subsets \(S_{j}^{k}\) are non-empty._

It is relevant for our interests that, in the case of progressive factorizations \(^{}\), the respective Max-Ent solutions enjoy the following monotonicity property

**Lemma 2** (Monotonicity).: _The multipliers of the Max-Ent solutions \(,^{}\) using \(^{}\) are such that_

\[||||_{1}||^{}||_{1}.\] (15)

This result is fully derived in Appendix C, and it ensures a monotonically increasing of all terms contained in the variance term of Eq. (11) since the complexity term is monotonically increasing by definition. On the other hand, the bias represented by Eq. (10) is guaranteed to decrease monotonically at finer levels of factorizations.

### D-Max-Ent Progressive Factorization Algorithm

In summary, D-Max-Ent PE shows a generalization error bound whose quantities are either known or estimated and change monotonically between progressive factorizations. On these results, we build an algorithm called _D-Max-Ent Progressive Factorization_, shown in Algorithm 2, which iteratively constructs a sequence of feature sets \(_{0}_{1}\) with progressive factorization while performing PE. The behavior of the algorithm is similar to what is done in Structural Risk-Minimization (SRM) , and it involves optimizing for a trade-off: the bias term (i.e., empirical risk) decreases by taking into account more complex features classes, while the variance term (i.e., the confidence interval) increases. The whole algorithm is then based on the progressive search for the new set of feature functions which reduces a proxy of the generalization error bound of D-Max-Ent PE:

\[()=()+B(, ,N,),\] (16)

and the procedure will continue until there are no further improvements in the trade-off. Due to the nature of the proxy function, the role of \(>0\) is to regulate the tendency to factorize. Higher values of \(\) will increase the magnitude of the decreasing term, causing a boost in the tendency to factorize. On the other hand, lower values will further decrease the importance of this term, resulting in a lower tendency to factorization.

Finally, the _Progressive Factor_ function takes as input the list of feature functions and a factor \(K\) and returns a list of progressively factored set of feature functions. More specifically, each element in \(\{_{j}\}_{j[M]}\) corresponds to a progressive factorization of the feature \(f_{j}\), factoring the related subset \(S_{j}\) into \(K\) disjoint subsets as in Definition 1. The new \(K\) subsets \(\{_{k}^{j}\}_{k[K]}\) are constructed in the worst-case scenario: the complexity term in Eq. (13) is maximized with partitions of a set leading to a uniform distribution of samples in each new partitioned subset, and since it is not possible to know in advance which samples will be contained in which new subset, one way is then to proceed with a uniform factorization. We decided to maintain the most agnostic approach over the set of possible features, but prior knowledge could be used to narrow down the partitions to consider.

```
0:\((_{N},_{0},,,K)\)\(\)\(N\)-trajectory samples, initial feature set, confidence level, boosting factor, factorization factor
1:Done \(\) False, \(i^{*} 0\)
2:while not Done do
3:\(_{i^{*}}\), \(M||\)
4:\(\) D-Max-Ent \((_{N},)\)
5:\(()()+B(, ,N,)\)
6:\(\{_{j}\}_{j[M]}\) Progressive Factor\((,K)\)
7:for\(j[M]\)do
8:\(_{j}\) D-Max-Ent \((_{N},_{j})\)
9:\((_{j})(_{j})+B(_{j},_{j},N,)\)
10:if\((_{j})<()\)then
11:\(i^{*} j\)
12:endif
13:endfor
14:if\(_{i^{*}}==\)then
15: Done \(\) True
16:endif
17:endwhile
18:return\(_{i^{*}}\) ```

**Algorithm 2** Distributional Max-Ent Progressive Factorization

## 5 Illustrative Numerical Simulations

This section reports the results of some illustrative numerical simulations that make use of Algorithm 2.

#### 5.1.1 Simulations Objectives

The objective of the simulations is to illustrate two essential features of the proposed method that were only suggested by the theoretical results. First of all, to analyze the outcome of performing policy evaluations with aggregated states at different sample regimes, by comparing the output of the proposed algorithm with some relevant baseline distributions. Secondly, the aim is to study the role of the boosting parameter \(\) and the sampling regime \(N\), being the main hyper-parameters of Algorithm 2, in the tendency to factor the representation at utterly different sample regimes.

#### 5.1.2 MDP Instance Design

The effectiveness of the proposed approach is expected to be particularly evident in MDPs admitting a factored representation of the return distribution, namely the ones in which many states are nearly equivalent under the evaluation of a policy. This factorizability property is not uncommon in general since it is present in any environment with symmetries and Block-MDPs

[MISSING_PAGE_EMPTY:8]

in the bound \(_{}\) (Fig. 1, 3) mostly due to the variance term, which is way higher in the case of full factorization. This suggests that the bound is indeed able to distinguish between the two. The plotting of the outputs of Algorithm 2 stops at the optimal number of factorization steps found for different values of \(\), namely at \(_{i^{*}}\). The plots should be read as follows: while the bound term \(_{}\) is expected to increase at each factorization step, the KL divergences with respect to the true return distribution should decrease as much as possible. In all cases, it is evident that the value of \(\) pushes towards a higher number of factorization steps, going from performing no factorization at all using low values (\(=3\)), to performing up to \(4\) factorization steps even in this simple scenario with higher values (\(=450\)), both at low and high sample regimes (\(N\{50,1000\}\)). Furthermore, at higher sample regimes, it is possible to see how the higher quality of the estimation counteracts the action of \(\), and increasing it generally induces still fewer factorizations compared to the low sample regimes with same values of \(\), as in Fig. 3, 4. Finally, it is apparent that minimizing for Eq. (16) successfully decreases the KL divergence. Nonetheless, its values stop decreasing significantly after the first factorization, which splits the state space over the two rows and further factorizations might lead to performance degradation as well.

## 6 Discussion

In this section, we briefly discuss the literature related to this work and provide some concluding remarks about the results and future research paths.

### Related Works

Our work relates to multiple fields. We now highlight the most relevant connections, while an exhaustive overview is beyond the scope of this paper.

Distributional Reinforcement LearningD-RL has recently received much attention, both for the richness of flavors it admits (Rowland et al., 2019, 2021), and the surprising empirical effectiveness (Bellemare et al., 2020). Our work tries to answer different research questions compared to traditional policy evaluation in D-RL and applies completely different techniques to derive the quantities of interest. Firstly, Bellman Operators and consequently contraction arguments cannot be applied since the estimation process is Monte-Carlo based. In this way, the sequential nature of the problem is not exploited, but we show that density estimation techniques do offer interesting properties nonetheless. Additionally, the employed indexes differ from traditional D-RL results. For example, the most recent bound for Q-TD (Rowland et al., 2023), shows a sub-linear term but the bound is made over the maximum Wasserstein distance, which cannot be directly related to the KL-divergence without further assumptions. Finally, distributional considerations have been employed in the field of function approximations as well. However, to the best of our knowledge, no other D-RL works explicitly address representation learning.

Maximum Entropy and Feature SelectionMax-Ent methods have a long and extensive literature in the density estimation field, which mostly focused on the general and algorithmic aspects of the method (Barron and Sheu, 1991; Dudik et al., 2004; Sutter et al., 2017). Among the others, Cortes et al. (2015) proposed a Max-Ent regularized formulation for performing feature selection. Their method allocates different weights to different features to achieve an even better trade-off based on a combination. Due to this, their work differs from ours in the nature of the search space, which is not built progressively but is defined a priori. Additionally, their generalization bound is of the same nature as standard Max-Ent bounds and contains a \(_{}||||_{1}\) term, which is bounded yet unknown. Finally, Mavridis et al. (2021) perform progressive state-aggregation through Max-Ent methods, but they try to optimize a different objective function based on state-dissimilarity.

### Conclusions and Future Works

In our work, we presented in a D-RL framework a new policy evaluation approach based on Maximum Entropy density estimation, called _Distributional Max-Ent_ Policy Evaluation, which benefits from the learning guarantees of Max-Ent and the generality of the setting, being able to enforce even complex feature families. We extended previous results and derived a practical formulation of the generalization error bound, which contains only estimated and known quantities of the problem. We then instantiated a particular class of features, namely state aggregation, and we proposed an algorithm called _Distributional Max-Ent Progressive Factorization_ to adaptively find a feature representation that optimizes for a proxy of the generalization error bound in a Structural Risk Minimization fashion. In this way, we showed that performing PE can indeed drive the learning of a reduced-dimension representation in the distributional setting. We then provided illustrative simulations showing the empirical behaviors of these approaches, while clarifying the links between some hyperparameters and the sample regime. Much of our analysis and theoretical guarantees straightforwardly extend to other feature classes, and an open question is to investigate other instances of features and settings that can benefit from the proposed framework. Future works will focus on the interaction between Temporal Difference (TD) distributional methods and representation learning in the proposed setting and on the existence of MDP instances that enjoy some relevant properties in the bias/variance trade-off along successive factorizations, leading to high performance or better error bounds.