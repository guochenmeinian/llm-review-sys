# Deep Contract Design via Discontinuous Networks

Tonghan Wang

Harvard University

twang1@g.harvard.edu &Paul Dutting

Google Switzerland

duetting@google.com &Dmitry Ivanov

Israel Institute of Technology

divanov@campus.technion.ac.il &Inbal Talgam-Cohen

Israel Institute of Technology

italgam@cs.technion.ac.il &David C. Parkes

Harvard University

parkes@eecs.harvard.edu

Also DeepMind, London UK

###### Abstract

Contract design involves a _principal_ who establishes contractual agreements about payments for outcomes that arise from the actions of an _agent_. In this paper, we initiate the study of deep learning for the automated design of optimal contracts. We introduce a novel representation: the _Discontinuous ReLU (DeLU) network_, which models the principal's utility as a discontinuous piecewise affine function of the design of a contract where each piece corresponds to the agent taking a particular action. DeLU networks implicitly learn closed-form expressions for the incentive compatibility constraints of the agent and the utility maximization objective of the principal, and support parallel inference on each piece through linear programming or interior-point methods that solve for optimal contracts. We provide empirical results that demonstrate success in approximating the principal's utility function with a small number of training samples and scaling to find approximately optimal contracts on problems with a large number of actions and outcomes.

## 1 Introduction

Contract theory studies the setting where a _principal_ seeks to design a contract for rewarding an _agent_ on the basis of the uncertain outcomes caused by the agent's private actions . Typical examples include a landlord who enters into a summer rental with a contract that includes penalties in the case of damage; a homeowner who engages a firm to complete a kitchen renovation with a contract that conditions payments on timely completion or functioning appliances; or an individual who employs a freelancer to do some design work with a contract that includes bonuses for completing the job.

A contract specifies payments to the agent, conditioned on outcomes. The principal is self-interested, with a value for each outcome and a cost for making payments. The agent is also self-interested, and responds to a contract by choosing an action that maximizes its expected utility (expected payment minus the cost of an action). The problem is to find a contract that maximizes the principal's utility (expected value minus expected payment), given that the agent will best respond to the contract. In economics, this is referred to as a problem of _moral hazard_, in that the agent is willing to privately act in its best interest given the contract (the "hazard" is that the behavior of the agent may be to the detriment of the principal).

The importance of contract design is evidenced by the 2016 Nobel Prize awarded to O. Hart and B. Holmstrom  and its broad application to real-world problems. Contract design is one of the three fundamental problems in the realm of economics involving asymmetric information and incentives, along with _mechanism design_ and _signalling_ (Bayesian persuasion) . However, whileboth mechanism design [10; 11; 12; 7; 36; 37] and signalling [26; 27; 18] have been studied extensively from a computational perspective, the contract design problem has only recently received attention and presented distinct computational challenges. For many combinatorial contract settings , the conventional approach based on linear programming becomes computationally infeasible and the problem of finding or even approximating the optimal contract becomes intractable [33; 28; 29]. In addition, learning-theoretic results give worst-case exponential sample complexity bounds for learning an approximately-optimal contract [40; 55]. As a result, there is a growing demand for a scalable, general purpose, and beyond-worst case approach for computing (near-)optimal contracts.

We are thus motivated to initiate the study of deep learning for optimal contract design (_deep contract design_). This falls within the broader framework of _differentiable economics_, which seeks to leverage parameterized representations of differentiable functions for the purpose of optimal economic design . In regard to learning contracts, recent work [40; 21; 55; 31] considers the distinct setting of _online learning_ for contract design with bandit feedback, characterizing regret bounds without appeal to deep learning.

A first innovation of this paper is to introduce a neural network architecture that can well-approximate the principal's utility function. A close examination of the geometry of the principal's utility function (Sec. 3) reveals its similarity to fully-connected feed-forward neural networks with ReLU activations : both of them are piecewise affine functions . However, whereas a ReLU network models a continuous function, the principal's utility function is discontinuous at the boundary of linear regions, where the best response of the agent changes. Accurate approximation in the vicinity of boundaries is critical because an optimal contract is always located on the boundary (Lemma 3 in Sec. 3), but this is exactly where ReLU approximation error can be large. To handle this, we introduce the _Discontinuous ReLU (DeLU)_ network, which recognizes that the linear regions of a ReLU network are decided by activation patterns (the status of all activation units in the network), and conditions a _piecewise bias_ on these activation patterns. In this way, each linear region has a different bias parameter and can be discontinuous at the boundaries. Fig. 1 illustrates the DeLU and its use to represent the principal's utility function, contrasting this with a continuous ReLU function.

A second innovation in this paper is to introduce a scalable inference technique that can find the contract that maximizes the network output (i.e., the principal's utility). We show that linear regions (pieces) of DeLU networks implicitly learn closed-form expressions for the incentive constraints of the agent and the utility maximization objective of the principal, so that we can use linear programming (LP) on each piece to find the global optimum. However, the time efficiency of this LP method is impeded by the overhead of solving individual LPs, and worsens with problem size in the absence of suitable parallel computing resources. To increase the computational efficiency, we develop a gradient-based inference algorithm based on the interior-point method  that only requires a few forward and backward passes of the DeLU network to find the contract that maximizes the network output. This method scales well to large-scale problems and can be readily run in parallel on GPUs.

The effectiveness of introducing piecewise discontinuity into neural networks is demonstrated by our experimental results. The DeLU network well-approximates the principal's utility function even with

Figure 1: DeLU and ReLU approximation on a randomly generated contract design instance with 1,000 actions and 2 outcomes (see Sec. 4.3). The x- and y-coordinates are the payments for each of the two outcomes, and the z-coordinate is the utility of the principal. **(a)** The exact surface of the principalâ€™s utility function \(u^{p}\). Different colors represent the action selected by the agent upon receiving the contract. **(b)** A learned _ReLU network_ cannot model the discontinuity of the \(u^{p}\) function and yields an incorrect contract as shown in (a). Colors in (b) and (c) represent different activation patterns (linear regions) of the networks. **(c)** A learned DeLU network represents a discontinuous function and can well-approximate \(u^{p}\), yielding the optimal contract as shown in (a).

a small number of training samples, and the inference method remains accurate and efficient as the problem size grows. By synergistically harnessing these two innovations, our method consistently finds near-optimal solutions on a wide range of contract design problems, significantly surpassing the capability of conventional continuous networks.

**Related work.** A longstanding challenge in the deep learning community has been to approximate discontinuous functions with neural networks. While the Universal Approximation Theorem guarantees the approximation of continuous functions, many problems involve discontinuity, including solar flare imaging  and problems coming from mathematics . However, establishing a network to represent discontinuous functions is not an easy task. Discontinuities were considered as early as in the 1950s, when Rosenblatt  introduced the perceptron model and its single-layer of step activation functions. Following this work, others modeled discontinuity through the use of different discontinuous activation functions [34; 2; 44]. However, training these models is more challenging than the more typical models that make use of continuous activation functions , and this has hindered their application. To the best of our knowledge, this paper presents the first discontinuous network architecture with continuous activation functions and stable optimization performance.

There is a vast and still-growing economics literature on contracts [see, e.g., 52; 15], to which the computational lens has recently been applied. In classic settings, computing the optimal contract is tractable by solving one LP per agent's action, and taking the overall best solution [see, e.g., 32]. However, LP becomes computationally infeasible for combinatorial contracts, including settings with exponentially-many outcomes , actions , or agent combinations . Another issue with the LP-based approach is that it requires complete information in regard to the problem facing the agent (its _type_). If the agent has a hidden type, this makes the optimal contract hard to compute [16; 38; 17], or otherwise complex . One way to deal with these complexities is to focus on simple contracts, in particular _linear contracts_ (commission-based), which are popular in practice [e.g., 13; 32; 14].

A distinct but related approach combines contract design with learning, where efforts have concentrated on online learning theory [40; 21; 55; 31]. These works show exponential lower bounds on the achievable regret for general contracts in worst-case settings, and sublinear regret bounds for linear contracts. Additionally, contracts have been studied from the perspective of multi-agent reinforcement learning [48; 19; 25]. The problem of _strategic classification_ has also established a formal connection between strategy-aware classifiers and contracts [43; 42; 3]. There is also interest in the application of contract theory to the AI alignment problem . However, the use of learning methods for solving optimal contracts, as opposed to auctions , remains largely unexplored. This gap in the literature can be partly attributed to the inherent complexity of finding optimal contracts, as the principal utility depends on the agent's action, which must conform to incentive compatibility (IC) constraints that are not observable by the principal.

## 2 Preliminaries

**Offline contract learning problem.** The contract design problem is defined with elements \(=,,,c,p,v\), and involves a single principal and a single agent. The agent selects an action \(a\) in the finite action space \(\), \(||\)=\(n\). Action \(a\) leads to a distribution \(p(|a)\) over the outcomes in \(\), \(||\)=\(m\), and incurs a cost \(c(a)_{ 0}\) to the agent. The valuation of each outcome \(o_{j}\) for the principal is decided by the value function \(v:_{ 0}\). The principal sets up a _contract_, \(_{ 0}^{m}\), which influences the action selected by the agent. A contract \(=(f_{1},f_{2},,f_{m})^{}\) specifies the payment \(f_{j} 0\) made to the agent by the principal in the event of outcome \(o_{j}\). On receiving a contract \(\), the agent selects the action \(a^{*}()\) maximizing its utility \(u^{a}(;a)=_{o p(|a)}[f_{o}]-c(a)\). For a given action \(a\) of the agent, the principal gets utility \(u^{p}(;a)=_{o p(|a)}[v_{o}-f_{o}]\), which is the principal's expected value minus payment. In the case that the induced action is the best response of the agent given the contract, we write \(u^{p}()=_{o p(|a^{*}())}[v_{o}-f_{o}]\). The goal in optimal contract design is to find the contract that maximizes the principal's utility without access to \(p(|a)\) and the action taken by the agent:

\[^{*}=_{}u^{p}()=_{}_{o p(|a^{*}())}[v_{o}-f_{o}].\] (1)

**ReLU piecewise-affine networks and activation patterns.** A fully-connected neural network with a piecewise linear activation function (e.g., ReLU and leaky ReLU) and a linear output layer represents a _continuous_ piecewise affine function [5; 23]. A piecewise affine function is defined as follows.

**Definition 1** (Piecewise affine function).: _A function \(g:^{d}\) is piecewise affine if there exists a finite set of polytopes \(\{_{i}\}_{i=1}^{P}\) such that \(_{i=1}^{P}_{i}=^{d}\), \(_{i}_{j i}=\), and \(g\) is a linear function \(_{i}:_{i}\) when restricted to \(_{i}\). We call \(_{i}\) a linear piece of \(g\)._

We now follow  and introduce some local properties of the piecewise affine function that is represented by a ReLU network. Suppose there are \(L\) hidden layers in a network \(g\), with sizes \([n_{1},n_{2},,n_{L}]\). \(^{(l)}^{n_{l} n_{l-1}}\) and \(^{(l)}^{n_{l}}\) are the weights and biases of layer \(l\). Let \(n_{0}=d\) denote the input space dimension. We consider a ReLU network with one-dimensional outputs, and the output layer has weights \(^{(L+1)}^{1 n_{L}}\) and a bias \(b^{(L+1)}\). With input \(^{d}\), we have the pre- and post-activation output of layer \(l\): \(^{(l)}()=^{(l)}^{(l-1)}()+^{(l)}\) and \(^{(l)}()=(^{(l)}())\), where \(:\) is an _activation function_. In this paper, we consider ReLU activation \((x)=\{x,0\}\)[54; 35], but the proposed method can be extended to other piecewise linear activation functions (e.g., LeakyReLU and PReLU ). For each hidden unit, the ReLU _activation status_ has two values, defined as \(1\) when pre-activation \(h\) is positive and \(0\) when \(h\) is strictly negative. The activation pattern of the entire network is defined as follows.

**Definition 2** (Activation Pattern).: _An activation pattern of a ReLU network \(g\) with \(L\) hidden layers is a binary vector \(=[^{(1)},,^{(L)}]\{0,1\}^{_{l=1}^{L}n_{l}}\), where \(^{(l)}\) is a layer activation pattern indicating activation status of each unit in layer \(l\)._

The activation pattern depends on the input \(\), and we define function \(r:^{d}\{0,1\}^{_{l=1}^{L}n_{l}}\) that maps the input to the corresponding activation pattern. For a ReLU network, inputs that have the same activation pattern lie in a polytope, and the activation pattern determines the boundaries of this polytope. To see this, we can write the output of layer \(l\), \(^{(l)}()\), as

\[^{(l)}()=^{(l)}^{(l-1)}()(^{(l-1)} {R}^{(l-2)}()((^{(1)}+^{(1)}) )+^{(l-1)})+^{(l)},\] (2)

where \(^{(k)}\) is a diagonal matrix with diagonal elements equal to the layer activation pattern \(^{(k)}\). Eq. 2 indicates that, when \(\) is fixed, \(^{(l)}\) is a linear function \(^{(l)}()=^{(l)}+^{(l)}\), where \(^{(l)}=^{(l)}(_{k=1}^{l-1}^{(l-k)}()^{ (l-k)})\) and \(^{(l)}=^{(l)}+_{k=1}^{l-1}(_{j=1}^{l-k}^{(l+1- j)}^{(l-j)}())^{(k)}\). We thus get \(_{l=1}^{L}n_{l}\) half-spaces, with the half-space corresponding to unit \(i\) of layer \(l\) defined as:

\[_{l,i}=\{^{d}|_{i}^{(l)}(_{i}^{ (l)}+_{i}^{(l)}) 0\},\] (3)

where \(_{i}^{(l)}+_{i}^{(l)}\) is the output of unit \(i\) at layer \(l\), and \(_{i}^{(l)}\) is 1 if \(_{i}^{(l)}()\) is positive, and is -1 otherwise. The input \(\) is in the polytope that is defined by the intersection of these half-spaces: \(()=_{l=1,,L}_{i=1,,n_{l}}_{l,i}\). When restricted to \(()\), the ReLU network is a linear function: \(g()=^{(L+1)}^{(L)}^{(L)}()+b^{(L+1)}\).

**Interior-point method for optimization problems with inequality constraints.** For a minimization problem with objective function \(q()\) and inequality constraints \(p_{i}()>0,i=1,,M\), the _interior-point method_ introduces a logarithmic _barrier function_, \(()=-_{i=1}^{M}(p_{i}())\), and finds the minimizer of \(q()+()\), for some \(t>0\). This new objective function is defined on the set of strictly feasible points \(\{|p_{i}()>0,i=1,,M\}\), and approximates the original objective as \(t\) becomes large. Given this, we can solve for a series of optimization problems for increasing values of \(t\). In the \(k\)-th round, \(t^{(k)}\) is set to \( t^{(k-1)}\), where \(>1\) is a constant, \(t^{(0)}>0\) is an initial value, and the problem is solved (e.g., by Newton initialized at \(^{(k-1)}\)) to yield \(^{(k)}\). Assume that we solve the barrier problem exactly for each iterate, then to achieve a desired accuracy level of \(>0\), we need \(n_{}=(M/(t^{(0)}))/()\) rounds of optimization.

## 3 Geometry of Optimal Contracts

In this section, we introduce some properties of the principal's utility function, \(u^{p}:\), which will motivate our method in Sec. 4. First, we show that \(u^{p}\) is a piecewise affine function.

**Lemma 1**.: _The principal's utility function \(u^{p}\) is a piecewise affine function._Proof.: The principal utility depends on the agent action. For contracts in the intersection of the following \(n-1\) half spaces that represent _incentive compatibility constraints_, the agent's action is \(a_{i}\):

\[_{i,j}=\{_{o p(|a_{i})} [f_{o}]-c(a_{i})_{o p(|a_{j})}[f_{o} ]-c(a_{j})\}, j i.\] (4)

Moreover, when agent action \(a_{i}\) remains unchanged, \(u^{p}\) changes linearly with \(\): \( u^{p}(;a_{i})+ u^{p}(^{};a_{i})=_{o p(|a_{i})}[v_{o}-f_{o}]+_{o p( |a_{i})}[v_{o}-f_{o}^{}]=_{o p(|a_{i})} [v_{o}-( f_{o}+ f_{o}^{})]=u^{p}(+ ^{};a_{i})\). Therefore, when restricted to the polytope \(_{i}=_{j i}_{i,j}, i\), \(u^{p}\) is linear. 

Based on Lemma 1, we define a _linear piece_ of function \(u^{p}\) as \(_{i}^{p}:_{i}\), where \(_{i}\) defines the set of contracts that motivates the agent to take action \(i\). Lemma 1 can be easily extended to the agent's utility function \(u^{a}\), and the linear pieces of function \(u^{p}\) and \(u^{a}\) share the same set of domains \(\{_{i}\}\). We define a linear piece of function \(u^{a}\) as \(_{i}^{a}:_{i}\). We next observe:

**Lemma 2**.: _The principal's utility function \(u^{p}\) can be discontinuous on the boundary of linear pieces._

The proof in Appx. A follows the idea that the agent is indifferent between action \(a_{i}\) and \(a_{j}\) given a contract \(\) on the boundary of two neighboring linear pieces \(_{i}^{p}\) and \(_{j i}^{p}\): \(_{i}^{a}()\)=\(_{j}^{a}()\). However, the principal's utility equals the expected value minus the cost of an action minus \(u^{a}()\). As the expected value minus the cost can be different in \(_{i}^{p}\) and \(_{j}^{p}\), \(u^{p}\) can be discontinuous at \(\).

We then define the optimality of a contract and analyze the structure of optimal contracts.

**Definition 3** (Piecewise and Global Optimal Contracts).: _A contract \(_{i}^{*}_{i}\) is piecewise optimal if \(u^{p}(_{i}^{*}) u^{p}(),_{i}\). A contract \(^{*}\) is global optimal if \(u^{p}(^{*}) u^{p}(),\)._

**Lemma 3**.: _The global optimal contract is on the boundary of a linear piece._

Lemma 3 can be proved by contradiction (as detailed in Appx. A): if a global optimal contract is not on the boundary, we can always find a solution with greater principal utility. As analyzed in Sec. 4, Lemma 3 serves as a compelling rationale for introducing our discontinuous networks. Besides discontinuity, there is another network design consideration motivated by the following property.

**Lemma 4**.: _The principal's utility function \(u^{p}\) can be written as a summation of a concave function and a piecewise constant function._

The proof in Appx. A commences by establishing the convexity of function \(u^{a}\) and subsequently formulating \(u^{p}\) as a function of \(u^{a}\). This property motivates us to introduce _concavity_ into our network design. In Appx. C, we discuss how to achieve this by imposing non-negativity constrains on network weights and analyze the impact of this design choice on the performance.

## 4 DeLU Neural Networks

Given the piecewise-affine geometry, the preceding analysis shows a close connection between the principal's utility function \(u^{p}\) (Lemma 1) and fully-connected ReLU networks (Sec. 2). However, ReLU functions are continuous and cannot represent the abrupt changes in \(u^{p}\) at the boundaries of the linear pieces. This lack of representational capacity is problematic because the optimal contracts are on the boundary (Lemma 3), which is precisely where the ReLU approximation errors can be large. Consequently, ReLU networks are not well suited to deep contract design. In this section, we introduce the new _Discontinuous ReLU (DeLU) network_, which provides _a discontinuity at boundaries between pieces_, making it a suitable function approximator for the \(u^{p}\) function.

### Architecture

The DeLU network architecture supports different biases for different linear pieces. Since a linear piece can be identified by the corresponding activation pattern, we propose to condition these _piecewise biases_ on activation patterns. Specifically, we learn a _DeLU network_\(:\) (Fig. 2) to approximate the principal's utility function, mapping a contract to the corresponding utility of the principal. The first part of a DeLU network is a sub-network similar to a conventional ReLU network. This sub-network \(\) has \(L\) fully-connected hidden layers with ReLU activation and a weight matrix at the output layer, i.e., \(\) is parameterized as \(_{}=\{^{(1)},^{(1)},,^{(L)},^{(L)},^{(L+1)}\}\), which includes weights and biases for \(L\) hidden layers and weights for the output (\((L+1)\)-th) layer. The DeLU network is different from a conventional ReLU network at the bias of the output (last) layer (\(b^{(L+1)}\)), and we introduce a new method to generate the last-layer biases.

Since there are no activation units at the output layer, given an input contract \(\), we can obtain the activation pattern \(r()\) by a forward pass of the network up until the last layer. To condition \(b^{(L+1)}\) on the activation pattern, we train another sub-network \(:^{_{i=1}^{L}n_{l}}\), that maps \(r()\) to the bias of the output layer. We use a two-layer fully-connected network with Tanh activation to represent \(\) and denote its parameters as \(_{}\). In this way, contracts in the same linear piece of the DeLU network share the same bias value, enabling the network to express discontinuity at the boundaries while keeping other properties of network \(\) unchanged. In Appx. B, we discuss why we adopt a neural network, instead of a simpler function, to model the bias term.

### Training and inference

The DeLU network \(\), including sub-networks \(\) and \(\), is end-to-end differentiable. For training, we randomly sample \(K\) contracts and the corresponding principal's utilities: \(_{K}=\{(_{i},u^{p}(_{i}))\}_{i=1}^{K}\). Feeding a training sample \(_{i}\) as input, we get an approximated principal's utility: \((_{i};_{},_{})=(_{i};_{})+ (r(_{i});_{})\), and the network \(\) is trained to minimize the following loss function in \(T\) epochs (Appx. D gives more details on network architecture, infrastructure, and training):

\[_{_{K}}(_{},_{})=_{ i=1}^{K}[(_{i};_{},_{})-u^{p}(_{i}) ]^{2}.\] (5)

Given a trained DeLU network, \(:\), we have an approximation of the principal's utility function. The next step is to find a contract that maximizes the learned utility function, that is \(^{*}=*{arg\,max}_{}()\). We call this the _inference process_. Since the function represented by a DeLU network is discontinuous, conventional first- or second-order optimization methods are not applicable, and, formally, we need to develop a suitable optimization approach to solve

\[(^{*})=_{_{i}}_{_{i}}_{i}(),\] (6)

where \(_{i}:_{i}\) is a linear piece of network \(\). This motivates us to first find the piecewise optimal contracts and then get the global optimum by comparing the piecewise optima.

#### 4.2.1 Linear programming based inference

A first approach finds the optimal contract for each piece using linear programming (LP). Contracts in the same linear piece \(\) result in the same activation pattern \(\), and the DeLU network is a linear function on the piece. In particular, the optimization problem of finding the piecewise optimum is:

\[*{arg\,max}_{}\ \ ^{(L+1)}^{(L)}[^{(L)} +^{(L)}]\ \ s.t.\ _{i}^{(l)}(_{i}^{(l)}+_{i}^{(l)}) 0, l[L],\ i[n_{l}],\]

Figure 2: The DeLU architecture.

where \([L]=\{1,,L\}\), \([n_{l}]=\{1,,n_{l}\}\), the objective is the linear function represented by the DeLU network on piece \(\), and the constraints require that the activation pattern remains \(\). The definitions of \(_{i}^{(l)}\), \(^{(L)}\), \(^{(l)}\), and \(^{(l)}\) are the same as in Sec. 2, but with the activation pattern fixed to \(\). We omit the last-layer bias in the objective because the activation pattern is fixed for this piece, and thus the bias is constant and does not influence the piecewise optimal solution.

For each LP, there are \(m\) decision variables, representing payments for \(m\) outcomes, and \(N=_{l=1}^{L}n_{l}\) (the number of neurons) constraints. LPs for each piece can be solved in polynomial time of \(m\) and \(N\), and quickly in practice via the simplex method. However, a challenge is that there are \(2^{N}\) activation patterns.1 For a DeLU network with a moderate size, we can enumerate all these pieces. For a larger DeLU network, we can approximate by collecting random contract samples and solving LPs with the corresponding activation patterns. Parallelizing these LPs may achieve better time efficiency. However, this improvement is limited by the overhead of solving a single LP and the required amount of suitable parallel computational resources. In the next section, we introduce a gradient-based method that provides better efficiency in solving single problems and better parallelism through making use of GPUs. We compare these two inference methods in Sec. 5.1.

#### 4.2.2 Gradient-based inference

The gradient-based inference method is based on the interior-point method (Sec. 2) and finds piecewise optimal solutions via forward and backward passes of the DeLU network. Specifically, on each linear piece, \(\), we adopt the following _barrier function_:

\[()=-_{l=1}^{L}_{i=1}^{n_{l}}[_{i}^{(l)}( _{i}^{(l)}+_{i}^{(l)})].\] (7)

At the \(k\)-th round, the objective function is

\[^{(k)}()=-^{(L+1)}^{(L)}[^{(L)}+^ {(L)}]+}().\] (8)

We use gradient descent initialized at \(^{(k-1)}\), and update \(\) with \(^{(k)}()/\) to find the minimizer. A forward pass of the DeLU network gives \(^{(k)}()\) and a backward pass is sufficient to calculate \(^{(k)}()/\). Since forward and backward passes are naturally parallelized in modern deep learning frameworks, this method can be parallelized by processing multiple \(\) simultaneously. Alg. 1 gives the matrix-form expression of this parallel computation. The input \(^{(0)}^{K m}\) can be the training set or a random sample set, and we discuss the difference of these two settings in Appx. E.

**Inference performance and boundary alignment degree**. The performance of the proposed inference algorithms depends on the DeLU approximation quality, especially on the degree of alignment between the DeLU boundaries and the ground-truth linear piece boundaries. An interesting question is whether our training setup can achieve a high _boundary alignment degree_. A reason to think this is possible comes from observing that the MSE training loss (Eq. 5) is sensitive to misalignment between the DeLU and true boundaries. In particular, given that the jump of the utility function at boundary points can be arbitrarily large, a slight misalignment between DeLU and true boundaries can lead to a large increase in the MSE loss. Related to this, we explore an extension of the gradient-based inference method to make it more robust to possible boundary misalignment. When

Figure 3: The gradient-based inference method for finding piecewise optima, illustrated on the same instance as Fig. 1.

annealing the coefficient of the barrier function, we can check whether the principal's utility increases for each \(t^{(k)}\) value. A non-increasing utility indicates that we encounter inaccurate boundaries, and we can stop the inference to seek more robustness. We call inference with this "early stop" mechanism the _sub-argmax gradient-based inference method_.

In Sec. 5.3, we empirically evaluate the boundary alignment degree achieved by DeLU, and demonstrate that near-optimal contract-design performance is closely associated with high boundary alignment. We also show that the sub-argmax variation can improve performance of gradient-based inference, especially on tasks where the boundary alignment degree is low.

### Illustrating DeLU-based contract design

We first illustrate the DeLU approach on a randomly generated contract design instance, in this case with 2 outcomes and 1,000 actions. In this instance, we sample the values for outcomes and costs for actions uniformly from \(\). The outcome distribution for an action is further generated by applying SoftMax to a Gaussian random vector in \(^{m}\). In Fig. 1 (a), we show the exact surface of the principal's utility function \(u^{p}\) on such a randomly generated instance. We observe that \(u^{p}\) is a discontinuous, piecewise affine function, where each piece corresponds to an action of the agent. In Fig. 1 (b) and (c), we show the \(u^{p}\) surface approximated by each of a ReLU and a DeLU network trained with 40\(K\) samples. These two networks have a similar architecture, with a single hidden layer of 8 ReLU units. The difference is the additional, last-layer biases of the DeLU network, which are dependent on activation patterns. Whereas the ReLU network cannot represent the discontinuity of \(u^{p}\), and thus gives an incorrect optimal contract, the DeLU network replicates the \(u^{p}\) surface, and gives an accurate optimal contract (Fig. 1 (a)). Another interesting observation is that the DeLU network may use multiple pieces to represent an original linear region (Fig. 1 (c)). In Fig. 3, we further illustrate the inference process of the gradient-based method on this instance.

## 5 Empirical Evaluation

In this section, we design experiments to study the following aspects of DeLU contract design: **(1) Optimality** (Sec. 5.1): Can DeLU networks give solutions close to the optimal contracts? How do the solutions compare against those generated by continuous neural networks? **(2) Sample efficiency** (Sec. 5.1): How many training samples are required for accurate DeLU approximation? Is the proposed method applicable to large-scale problems? **(3) Time efficiency** (Sec. 5.2): How does the computation overhead required for DeLU learning and inference compare to those of other solvers? **(4) Inference** (Sec. 5.2): Does the gradient-based inference method provide a good tradeoff between accuracy and time efficiency? **(5) Boundary alignment degree** (Sec. 5.3): How does the boundary alignment degree affect optimality?

**Problem generation.** Experiments are carried out on random synthetic examples. The outcome distributions \(p(|a)\) are generated by applying SoftMax on a Gaussian random vector in \(^{m}\). The outcome value \(v_{o}\) is uniform on \(\). The action cost is a mixture, \(c(a)=(1-_{p})c_{r}(a)+_{p}c_{i}(a)\), where \(c_{r}(a)=_{p}_{o p(|a)}[v_{o}]\) for scaling factor \(_{p}>0\) is a correlated cost that is proportional to the expected value of the action, \(c_{i}(a)\) is an independent cost and uniform on , and \(_{p}\) controls the weight of the independent cost. We test different problem sizes by changing the number of outcomes \(m\) and actions \(n\). For each problem size, we test various combinations of \((_{p},_{p})\) to consider the influence of correlation costs. Different methods are compared on the same set of problems.

Figure 4: Optimality (normalized principal utility) of DeLU, ReLU and a direct LP solver (Oracle LP), for problems with increasing sizes.

### Optimality and efficiency.

In Fig. 4, we compare DeLU against ReLU networks as well as a baseline _linear programming (LP)_ solver (Oracle LP). Oracle LP refers to the use of LP for directly solving the contract design problem, not for inference on a trained DeLU network. It solves \(n\) LP problems, one for each action \(a\), where the objective is to maximize \(u^{p}\) with the incentive compatibility (IC) constraints associated with the action \(a\). The best of these \(n\) solutions becomes the optimal contract. Oracle LP has access to the outcome distributions \(p(|a)\) (to construct the IC constraints) that are unobservable to the DeLU and ReLU learners, but gives a _benchmark_ for the optimality of the proposed method.

We test different problem sizes in Fig. 4. Specifically, we set the number of outcomes \(m\) to 25 (\(1^{}\) column), 50 (\(2^{}\) column), and 100 (\(3^{}\) column) and increase the number of actions \(n\) from \(2^{2}\) to \(2^{8}\). For each problem size, 12 combinations of \(_{p}\) and \(_{p}\) are tested, with \((_{p},_{p})\{0.5,0.7,0.9\}\{0,0.3,0.6,0.9\}\). The median performance as well as the first and third quartile (shaped area) of these 12 combinations are shown. When reporting optimality, we normalize the principal's utility achieved by DeLU/ReLU LP-based inference via dividing them by the value returned by Oracle LP. To ensure a fair comparison, DeLU and ReLU networks have the same architecture, with one hidden layer of 32 hidden units. They are trained for 100 epochs with 50\(K\) random samples.

DeLU consistently achieves better **optimality** than ReLU networks (\(+28.29\%\)) and the best contract in the training set (\(+23.84\%\)) across all problem sizes. The performance gap is particularly large for large-scale problems. For example, when the number of outcomes is larger than \(1,000\) (Fig. 5 (b)), the ReLU networks return solutions much worse (\(<20\%\)) than training samples, while DeLU can obtain a solution at least \(1.7\) times better than the best training data. As for **sample efficiency**, as shown in Fig. 5 (c), DeLU achieves near-optimality even with a very small training set.

### Comparing the two DeLU inference methods.

In Fig. 6, we fix \(m\) to 25, 50, 100 and increase \(n\) from \(2^{2}\) to \(2^{8}\) to compare the optimality of the proposed inference methods. We again test the same 12 combinations of \((_{p},_{p})\). The median performance and the first and third quartile are shown for each problem size. Gradient-based inference is parallelized for 50\(K\) training samples on GPUs, while LP inference is parallelized for 5 linear pieces on CPUs. We can see that LP inference consistently provides a better solution, as the optimality of gradient-based inference (\(-7.56\%\)) is limited by the number of gradient descent steps.

The advantage of gradient-based inference is its time efficiency. In Fig. 5 (a), we compare the inference time for different problem sizes (with \(m\) fixed to 25). Gradient-based inference saves around \(50\)-\(90\%\) overhead compared to LP inference. We also note that the DeLU training and

Figure 5: (a) DeLU training and inference time compared against Oracle LP. (b) DeLU performance on large-scale problems. (c) DeLU performance with a small number of training samples.

Figure 6: Comparing the optimality of the two inference methods for solving the global optimum given a learned DeLU network, considering increasing problem sizes.

inference costs do not increase with the problem size. By comparison, the overhead of the direct LP solver grows quickly with the problem scale.

### Boundary alignment degree and its influence on DeLU optimality

In Fig. 7, we study the relationship between DeLU performance and the degree of boundary alignment.

For each contract design problem, we first calculate the boundary alignment degree achieved by the DeLU network. For this, we test a large number (50K) of contracts, and check whether they are simultaneously on the DeLU model and true utility boundary. Specifically, for each contract we randomly sample 10K directions and assess linearity of the DeLU utility model and the true utility function in each direction. The principal's utility function is piecewise linear in the proximity of an interior point. Conversely, when a point lies on a boundary, there is a jump in utility within its proximity, rendering the utility unable to pass a linearity test in some direction. In particular, if a function is non-linear in >20% of random directions, we mark the contract sample as being on a boundary (we use the exact same approach to check for boundaries of the DeLU utility function and the principal's true utility function). The _boundary alignment degree_ is calculated as the percentage of overlapped boundary points (# contract samples on both DeLU and true boundaries / # contract samples on true boundaries).

Each point in Fig. 7 represents a contract design problem, and we use the same set of problems as in the previous experiments. The x-axis is the DeLU boundary alignment degree, and the y-axis is the optimality of DeLU with LP-based inference (Fig. 7 left) and gradient-based inference (Fig. 7 middle). The right plot gives the optimality improvement achieved by the sub-argmax inference method compared to standard gradient-based inference. From Fig. 7 left, we observe that DeLU achieves good boundary alignment degrees (>80%) for most contract design problems, and that a strong positive correlation exists between the boundary alignment degree and the optimality of LP-based inference. This result indicates that the alignment degree between DeLU and true boundaries is important in supporting good performance of LP-based inference. A similar observation can be made for gradient-based inference. Fig. 7-right shows that as the boundary alignment degree increases, the optimality improvement from the sub-argmax inference compared to standard gradient-based inference becomes less significant. This confirms that the sub-argmax inference is especially helpful for contract design problems where the DeLU boundaries are less accurate.

## 6 Closing Remarks

This paper initiates the investigation of contract design from a deep learning perspective, introducing a family of piecewise discontinuous networks and inference techniques that are tailored for deep contract learning. In future work, it will be interesting to take this framework to real-world settings, provide theory in regard to the expressiveness of the function class comprising these piecewise discontinuous functions, and extend to the setting of online learning. We expect that our exploration of discontinuous networks can also draw attention to other economics problems involving discontinuity and thereby contribute to advancing AI progress in computational economics.

Figure 7: Correlation between boundary alignment degree (x-axis) and DeLU optimality (y-axis; left: LP-based inference; middle: gradient-based inference); also, the improvement achieved by sub-argmax inference compared to gradient-based inference (right). Trend lines (linear regression) are shown. Point sizes (\((mn)\)) indicate the corresponding problem size.

Acknowledgements

This work received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program (grant agreement: 101077862, project name ALGORCONTRACT). We extend our heartfelt appreciation to the anonymous NeurIPS reviewers for their insightful questions and constructive interactions during the review process, which has inspired us to delve deeper into critical inquiries, such as the boundary alignment issue. Their feedback has been important in shaping the quality and depth of our work.