# Gaussian Differential Privacy on Riemannian Manifolds

Yangdi Jiang, Xiaotian Chang, Yi Liu, Lei Ding, Linglong Kong, Bei Jiang*

Department of Mathematical and Statistical Sciences

University of Alberta

{yangdi, xchang4, yliu16, lding1, lkong, beil}@ualberta.ca

###### Abstract

We develop an advanced approach for extending Gaussian Differential Privacy (GDP) to general Riemannian manifolds. The concept of GDP stands out as a prominent privacy definition that strongly warrants extension to manifold settings, due to its central limit properties. By harnessing the power of the renowned Bishop-Gromov theorem in geometric analysis, we propose a Riemannian Gaussian distribution that integrates the Riemannian distance, allowing us to achieve GDP in Riemannian manifolds with bounded Ricci curvature. To the best of our knowledge, this work marks the first instance of extending the GDP framework to accommodate general Riemannian manifolds, encompassing curved spaces, and circumventing the reliance on tangent space summaries. We provide a simple algorithm to evaluate the privacy budget \(\) on any one-dimensional manifold and introduce a versatile Markov Chain Monte Carlo (MCMC)-based algorithm to calculate \(\) on any Riemannian manifold with constant curvature. Through simulations on one of the most prevalent manifolds in statistics, the unit sphere \(S^{d}\), we demonstrate the superior utility of our Riemannian Gaussian mechanism in comparison to the previously proposed Riemannian Laplace mechanism for implementing GDP.

## 1 Introduction

As technological advancements continue to accelerate, we are faced with the challenge of managing and understanding increasingly complex data. This data often resides in nonlinear manifolds, commonly found in various domains such as medical imaging (Pennec et al., 2019; Dryden, 2005; Dryden et al., 2009), signal processing (Barachant et al., 2010; Zanini et al., 2018), computer vision (Turaga and Srivastava, 2015; Turaga et al., 2008; Cheng and Vemuri, 2013), and geometric deep learning (Belkin et al., 2006; Niyogi, 2013). These nonlinear manifolds are characterized by their distinct geometric properties, which can be utilized to extract valuable insights from the data.

As data complexity grows, so does the imperative to safeguard data privacy. Differential Privacy (DP) (Dwork et al., 2006b) has gained recognition as a prominent mathematical framework for quantifying privacy protection, and a number of privacy mechanisms (McSherry and Talwar, 2007; Barak et al., 2007; Wasserman and Zhou, 2010; Reimherr and Awan, 2019) have been devised with the aim of achieving DP. Conventional privacy mechanisms, while effective for dealing with linear data, encounter difficulties when handling complex non-linear data. In such cases, a common approach, referred to as the extrinsic approach, is to embed the non-linear data into the ambient Euclidean space, followed by the application of standard DP mechanisms. However, as exemplified in the work of Reimherr et al. (2021), the intrinsic properties of such non-linear data enable us to achieve better data utility while simultaneously preserving data privacy. Therefore, it is imperative that privacy mechanisms adapt to the complexity of non-linear data by employing tools from differential geometry to leverage the geometric structure within the data.

Related WorkReimherr et al. (2021) is the first to consider the general manifolds in the DP literature. It extends the Laplace mechanism for \(\)-DP from Euclidean spaces to Riemannian manifolds. Focusing on the task of privatizing Frechet mean, it demonstrates that better utility can be achieved when utilizing the underlying geometric structure within the data. Continuing its work, Soto et al. (2022) develops a K-norm gradient mechanism for \(\)-DP on Riemannian manifolds and shows that it outperforms the Laplace mechanism previously mentioned in the task of privatizing Frechet mean. Similarly, Utpala et al. (2023) extends \((,)\)-DP and its Gaussian mechanism but only to one specific manifold, the space of symmetric positive definite matrices (SPDM). Equipping the space of SPDM with the log Euclidean metric, it becomes a geometrically flat space (Arsigny et al., 2007). This allows them to simplify their approach and work with fewer complications, although at the expense of generality. In contrast to the task of releasing manifold-valued private summary, Han et al. (2022), Utpala et al. (2023) focus on solving empirical risk minimization problems in a \((,)\)-DP compliant manner by privatizing the gradient which resides on the tangent bundle of Riemannian manifolds. Working on tangent spaces instead of the manifold itself, they could bypass many of the difficulties associated with working under Riemannian manifolds.

MotivationsAlthough the \(\)-differential privacy (DP) and its Laplace mechanism have been extended to general Riemannian manifolds in Reimherr et al. (2021), there are other variants of DP (Dwork et al., 2006; Mironov, 2017; Bun and Steinke, 2016; Dong et al., 2022). Each of them possesses unique advantages over the pure DP definition, and therefore their extensions should be considered as well. As one such variant, GDP offers superior composition and subsampling properties to that of \(\)-DP. Additionally, it's shown that all hypothesis testing-based privacy definitions converge to the guarantees of GDP in the limit of composition (Dong et al., 2022). Furthermore, when the dimension of the privatized data approaches infinity, a large class of noise addition private mechanisms is shown to be asymptotically GDP (Dong et al., 2021). These traits establish GDP as the focal privacy definition among different variants of DP definitions and therefore make it the most suitable option for generalizing to Riemannian manifolds.

Main ContributionsWith the goal of releasing manifold-valued statistical summary in a GDP-compliant manner, we extend the GDP framework to general Riemannian manifolds, establishing the ability to use Riemannian Gaussian distribution for achieving GDP on Riemannian manifolds. We then develop an analytical form to achieve \(\)-GDP that covers all the one-dimensional cases. Furthermore, we propose a general MCMC-based algorithm to evaluate the privacy budget \(\) on Riemannian manifolds with constant curvature. Lastly, we conduct numerical experiments to evaluate the utility of our Riemannian Gaussian mechanism by comparing it to the Riemannian Laplace mechanism. Our results conclusively demonstrate that to achieve GDP, our Gaussian mechanism exhibits superior utility compared to the Laplace mechanism.

## 2 Notation and Background

In this section, we first cover some basic concepts from Riemannian geometry. The materials covered can be found in standard Riemannian geometry texts such as Lee (2006), Petersen (2006), Pennec et al. (2019), Said (2021). Then we review some definitions and results on DP and GDP, please refer to Dwork and Roth (2014), Dong et al. (2022, 2021) for more detail.

### Riemannian Geometry

Throughout this paper we let \(\) denote a \(d\)-dimensional complete Riemannian manifold unless stated otherwise. A Riemannian metric \(g\) is a collection of scalar products \(,_{x}\) on each tangent space \(T_{x}\) at points \(x\) of the manifold that varies smoothly from point to point. For each \(x\), each such scalar product is a positive definite bilinear map \(,_{x}:T_{x} T_{x} \).

Equipped with a Riemannian metric \(g\), it grants us the ability to define length and distance on \(\). Consider a curve \((t)\) on \(\), the length of the curve is given by the integral

\[L()=\|(t)\|_{(t)}dt=( (t),(t)_{(t)})^{}\ dt\]

where the \((t)\) is the velocity vector and norm \(\|(t)\|\) is uniquely determined by the Riemannian metric \(g\). Note we use \(\|\|\) to denote the \(l_{2}\) norm throughout this paper.

It follows that the distance between two points \(x,y\) is the infimum of the lengths of all piecewise smooth curves from \(x\) to \(y\), \(d(x,y)=_{:(0)=x,(1)=y}L()\). In a similar fashion, we can introduce the notion of measure on \(\). The Riemannian metric \(g\) induces a unique measure \(\) on the Borel \(\)-algebra of \(\) such that in any chart \(U\), \(d=\) where \(g=(g_{ij})\) is the matrix of the Riemannian metric \(\) in \(U\), and \(\) is the Lebesgue measure in \(U\)(Grigoryan, 2009).

Given a point \(p\) and a tangent vector \(v T_{p}\), there exists a unique geodesic \(_{(p,v)}(t)\) starting from \(p=_{(p,v)}(0)\) with tangent vector \(v=_{(p,v)}(0)\) defined in a small neighborhood of zero. It can then be extended to \(\) since we assume \(\) is complete. This enables us to define the exponential map \(_{p}:T_{p}\) as \(_{p}(v)=_{(p,v)}(1)\). For any \(p\), there is a neighborhood \(V\) of the origin in \(T_{p}\) and a neighborhood \(U\) of \(p\) such that \(_{p}|{}_{V}:V U.\) is a diffeomorphism. Such \(U\) is called a normal neighborhood of \(p\). Locally, the straight line crossing the origin in \(T_{p}\) transforms into a geodesic crossing through \(p\) on \(\) via this map. On the normal neighborhood \(U\), the inverse of the exponential map can be defined and is denoted by \(_{p}\). The injectivity radius at a point \(p\) is then defined as the maximal radius \(R\) such that \(B_{p}(R)\) is a normal neighborhood of \(p\), and the injectivity radius of \(\) is given by \(_{}=\{_{}(p),\;p \}\).

### Differential Privacy

We start this section with the definition of \((,)\)-DP.

**Definition 2.1** (Dwork et al., 2006a).: _A data-releasing mechanism \(M\) is said to be \((,)\)-differentially private with \( 0,0 1\), if for any adjacent datasets, denoted as \(^{}\), differing in only one record, we have \((M() A) e^{}(M(^{ }) A)+\) for any measurable set \(A\) in the range of \(M\)._

Differential privacy can be interpreted from the lens of statistical hypothesis testing (Wasserman and Zhou, 2010; Kairouz et al., 2017). Given the outcome of a \((,)\)-DP mechanism and a pair of neighboring datasets \(^{}\), consider the hypothesis testing problem with \(H_{0}\): The underlying dataset is \(\) and \(H_{1}\): The underlying dataset is \(^{}\). The smaller the \(\) and \(\) are, the harder this hypothesis testing will be. That is, it will be harder to detect the presence of one individual based on the outcome of the mechanism. More specifically, \((,)\)-DP tells us the power (that is, 1 - type II error) of any test at significance level \(\) is bounded above by \(e^{}+\). Using this hypothesis testing interpretation, we can extend \((,)\)-DP to the notion of Gaussian differential privacy.

Let \(M(),M(^{})\) denote the distributions of the outcome under \(H_{0},H_{1}\) respectively. Let \(T(M(),M(^{})):,  T(M(),M(^{})) ()\) denote the optimal tradeoff between type I error and type II error. More specifically, \(T(M(),M(^{}))()\) is the smallest type II error when type I error equals \(\).

**Definition 2.2** (Dong et al., 2022).: _A mechanism \(M\) is said to satisfy \(\)-Gaussian Differential Privacy \(()\) if \(T(M(),M(^{})) G_{}\) for all neighboring datasets \(^{}\) with \(G_{}:=T(N(0,1),N(,1))\)._

Informally, \(\)-GDP states that it's harder to distinguish \(\) from \(^{}\) than to distinguish between \(N(0,1)\) and \(N(,1)\). Similar to the case of \((,)\)-differential privacy, a smaller value of \(\) provides stronger privacy guarantees. As a privacy definition, \(\)-GDP enjoys several unique advantages over the \((,)\)-DP definition. Notably, it has a tight composition property that cannot be improved in general. More importantly, a crucial insight in Dong et al. (2022) is that the best way to evaluate the privacy of the composition of many "highly private" mechanisms is through \(\)-GDP. More specifically, it gives a central limit theorem that states all hypothesis testing-based privacy definitions converge to the guarantees of \(\)-GDP in the limit of composition. Furthermore, Dong et al. (2021) shows that a large class of noise addition mechanisms is asymptotic \(\)-GDP when the dimension of the privatized data approaches infinity. These distinct characteristics position \(\)-GDP as the focal privacy definition among different variants of DP definitions, and we will extend the \(\)-GDP framework to general Riemannian manifolds in Section 3.

## 3 Gaussian Differential Privacy on General Riemannian Manifolds

Our primary objective in this study is to disclose a \(\)-valued statistical summary while preserving privacy in a GDP-compliant manner. To this end, we first extend the GDP definition to general Riemannian manifolds. In Definition 2.2, \(\)-GDP is defined through the optimal trade-off function \(T(M(),M(^{}))\), which is challenging to work with on Riemannian manifolds. We successfully resolve this difficulty by expressing \(\)-GDP as an infinite collection of \((,)\)-DP (Corollary 1 in Dong et al. (2022)). Since \((,)\)-DP is a well-defined notion on any measurable space (Wasserman and Zhou, 2010), it readily extends to any Riemannian manifold equipped with the Borel \(\)-algebra. Following this methodology, we define \(\)-GDP on general Riemannian manifolds as follows.

**Definition 3.1**.: A \(\)-valued data-releasing mechanism \(M\) is said to be \(\)-GDP if it's \((,_{}())\)-DP for all \( 0\), where

\[_{}():=(-+ )-^{}(-- {2}).\]

and \(\) denotes the cumulative distribution function of the standard normal distribution.

Similarly, we extend the notion of sensitivity to Riemannian manifolds as well.

**Definition 3.2** (Reimherr et al. (2021)).: _A summary \(f\) is said to have a **global sensitivity** of \(<\), with respect to \(d(,)\), if we have \(d(f(),f(^{}))\) for any two dataset \(^{}\)._

Following the extension of \(\)-GDP to Riemannian manifolds, it is crucial to develop a private mechanism that is compliant with \(\)-GDP. Given that the Gaussian distribution satisfies \(\)-GDP on Euclidean space (Dong et al., 2022), we hypothesize that an analogous extension of the Gaussian distribution into Riemannian manifolds would yield similar adherence to \(\)-GDP. (e.g., (Reimherr et al., 2021) for extending Laplace distribution to satisfy \(\)-DP on Riemannian manifolds). We introduce the Riemannian Gaussian distribution in the following definition.

**Definition 3.3** (Section 2.5 in Pennec et al. (2019)).: _Let \((,g)\) be a Riemannian manifold such that_

\[Z(,)=_{}\{-}{ 2^{2}}\}d(y)<.\]

_We define a probability density function w.r.t \(d\) as_

\[p_{,}(y)=\{-}{2^{2}}\}.\] (1)

_We call this distribution a **Riemannian Gaussian distribution** with footprint \(\) and rate \(\) and denote it by \(Y N_{}(,^{2})\)._

The necessity for \(Z(,)\) to be finite is generally of little concern, as it has been shown to be so in any compact manifolds (Chakraborty and Vemuri, 2019) or any Hadamard manifolds1--with lower-bounded sectional curvature (Said, 2021). The distribution we introduce has been established to maximize entropy given the first two moments (Pennec et al., 2019; Pennec, 2006), and has already found applications in a variety of scenarios (Zhang and Fletcher, 2013; Hauberg, 2018; Said et al., 2017; Cheng and Vemuri, 2013; Zanini et al., 2018; Chakraborty and Vemuri, 2019). When \(=^{d}\), the Riemannian Gaussian distribution reduces to the multivariate Gaussian distribution with a mean of \(\) and a variance of \(^{2}\).

However, it is critical to highlight that this extension is not the only possible method for integrating the Gaussian distribution into Riemannian manifolds. Other approaches are available, such as the heat kernel diffusion process articulated by Grigoryan (2009), or the exponential-wrapped Gaussian introduced by Chevallier et al. (2022). For an in-depth exploration of sampling from the Riemannian Gaussian distribution defined above, we refer readers to Section 4.1.

Furthermore, the subsequent theorem underscores the ability of the Riemannian Gaussian distribution, as defined herein, to meet the requirements of Gaussian Differential Privacy.

**Theorem 3.1**.: _Let \(\) be a Riemannian manifold with lower bounded Ricci curvature and \(f\) be a \(\)-valued summary with global sensitivity \(\). The Riemannian Gaussian distribution with footprint \(f()\) and rate \(\) satisfies \(\)-GDP for some \(>0\)._

Proof.: See Appendix A.1.

While Theorem 3.1 confirms the potential of the Riemannian Gaussian distribution to achieve GDP, it leaves the relationship between the privacy budget (\(\)) and the rate (\(\)) undefined. The subsequence theorem establishes such a connection:

**Theorem 3.2** (Riemannian Gaussian Mechanism).: _Let \(\) be a Riemannian manifold with lower bounded Ricci curvature and \(f\) be a \(\)-valued summary with global sensitivity \(\). The Riemannian Gaussian distribution with footprint \(f(D)\) and rate \(>0\) is \(\)-GDP if and only if \(\) satisfies the following condition, \( 0\),_

\[_{^{}}_{A}p_{_{1},}(y)\;d (y)-e^{}_{A}p_{_{2},}(y)\;d(y)_{}()\] (2)

_where \(A:=\{y:p_{_{1},}(y)/p_{_{2},}(y) e^{ }\}\) and \(_{1}:=f(),_{2}:=f(^{})\)._

Proof.: See Appendix A.2. 

Given the rate \(\), Theorem 3.2 provides us a way of computing the privacy budget \(\) through the inequality (2). When \(=^{d}\), the set \(A\) enjoys a tractable form, \(A=\{y^{d}:(y-_{2},_{1}-_{2})(2^ {2}/\|_{1}-_{2}\|+\|_{1}-_{2}\|)\}\), and the inequality (2) then reduces to,

\[_{D D^{}}(--_{ 2}\|}+-_{2}\|}{2})-e^{}(- -_{2}\|}+-_{2}\|} {2})_{}()\]

where the equality holds if and only if \(=/\), which reduces to the Gaussian mechanism on Euclidean spaces in Dong et al. (2022). It's worth pointing out that on \(^{d}\), the Pythagorean theorem allows us to reduce the integrals to one-dimensional integrals resulting in a simple solution for any dimension \(d\). Unfortunately, the lack of Pythagorean theorem on non-Euclidean spaces makes it difficult to evaluate the integrals on manifolds of dimensions greater than one. It's known that any smooth, connected one-dimensional manifold is diffeomorphic either to the unit circle \(S^{1}\) or to some interval of \(\)(Milnor and Weaver, 1997). Therefore, we encompass all one-dimensional cases by presenting the following result on \(S^{1}\).

**Corollary 3.2.1** (Riemannian Gaussian Mechanism on \(S^{1}\)).: _Let \(f\) be a \(S^{1}\)-valued summary with global sensitivity \(\). The Riemannian Gaussian distribution \(N_{}(f(),^{2})\) is \(\)-GDP if and only if \(\) satisfies the following condition, \([0,/(2^{2})],h(,,) _{}()\) where_

\[h(,,) =[(-+)-e^{}(--)]\] \[-[(+-)-e^{} (-+ {}_{}{2^{2}}}-_{>}{2^{2}}})]\] \[-e^{}_{}{2 ^{2}}}\]

_with \(C()=(/)-(-/)\)._

Proof.: See Appendix A.3. 

In summary, Corollary 3.2.1 provides us the analytical form for the integrals in (2). By specifying the rate \(\) and sensitivity \(\), it becomes feasible to compute the privacy budget \(\), which is the smallest \(\) such that \(h(,,)_{}()\) for all \([0,/(2^{2})]\). The systematic procedure is summarized in Algorithm 1. It is important to emphasize that the result in Corollary 3.2.1 together with the existing Euclidean space result covers all one-dimensional cases, and for manifolds with dimension \(d>1\), we will tackle it in Section 4.

## 4 Numerical Approach

In Section 3, we demonstrate that our proposed Riemannian Gaussian distribution can be used to achieve GDP in Theorem 3.1 and 3.2. Furthermore, we document our method in Algorithm 1 to compute the privacy budget \(\) in \(S^{1}\). This and already existing Euclidean results encompass all the one-dimensional Riemannian manifolds. However, for general Riemannian manifolds of dimension \(d>1\), the integrals in inequality (2) are difficult to compute (see Section 3 for more explanation). One of the central difficulties is the dependence of the normalizing constant \(Z(,)\) on the footprint \(\) makes the expression of \(A\) intractable. To avoid this dependence, we introduce the concept of homogeneous Riemannian manifolds.

**Definition 4.1** (Definition 4.6.1 in Berestovskii and Nikonorov (2020)).: _A Riemannian manifold \((,g)\) is called a **homogeneous Riemannian manifold** if a Lie group \(G\) acts transitively and isometrically on \(\)._

Homogeneous Riemannian manifolds encompass a broad class of manifolds that are commonly encountered in statistics such as the (hyper)sphere (Bhattacharya and Bhattacharya, 2012; Mardia et al., 2000), the space of SPD Matrices(Pennec et al., 2019; Said et al., 2017; Hajri et al., 2016), the Stiefel manifold (Chakraborty and Vemuri, 2019; Turaga et al., 2008) and the Grassmann manifold (Turaga et al., 2008). It's a more general class than Riemannian symmetric space, which is a common setting used in geometric statistics (Cornea et al., 2017; Asta, 2014; Said et al., 2018; Said, 2021; Chevallier et al., 2022). Please refer to Appendix A.4 for more detail.

Informally, a homogeneous Riemannian manifold looks geometrically the same at every point. The transitive property required in the definition implies that any homogeneous Riemannian manifold \(\) only has one orbit. Therefore, we have the following proposition.

**Proposition 4.1**.: _If \(\) is a homogeneous Riemannian manifolds, then \(Z(_{1},y)=Z(_{2},y)\) for any \(_{1},_{2}\)._

Therefore, on homogeneous Riemannian manifolds, we can simplify the set \(A\) in Theorem 3.2 to \(A=y:d(_{2},y)^{2}-d(_{1},y)^{2} 2^{2} }\). To further simplify (2), we will need a much stronger assumption than homogeneous Riemannian manifolds. In particular, we require the condition of constant curvature.

**Theorem 4.1** (Riemannian Gaussian Mechanism on Manifolds with Constant Curvature).: _Let \(\) be a Riemannian manifold with constant curvature and \(f\) be a \(\)-valued summary with global sensitivity \(\). The Riemannian Gaussian distribution with footpoint \(f()\) and rate \(>0\) is \(\)-GDP if and only if \(\) satisfies the following condition: \(_{1},_{2}\) such that \(d(_{1},_{2})=\) and \( 0\)_

\[_{A}p_{_{1},}(y)\ d(y)-e^{}_{A}p_{_{2}, }(y)\ d(y)_{}()\] (3)

_where \(A=y:d(_{2},y)^{2}-d(_{1},y)^{2} 2^{2} }\)._

Proof.: See Appendix A.5. 

Theorem 4.1 tells us that instead of evaluating the integrals of 2 on every neighboring pairs \(_{1}\) and \(_{2}\), we only need to check one such pair. Despite the convenience it provides us, evaluating the integrals remains a challenge, even for an elementary space like \(S^{d}\) with \(d>1\). To circumvent this challenge, we employ the MCMC technique for the integral computations. The main idea is simple: Let \(Y_{1},Y_{2},,Y_{n}\) be independent and identically distributed random variables from \(N_{}(,^{2})\), and denotes \(Z_{i}=\{}(-d(_{1},Y_{i})^{2}+d( _{2},Y_{i})^{2})\ \}\). By the strong law of large number (Dudley, 2002), we then have

\[_{i=1}^{n}Z_{i}_{A}p_{,}(y)d(y).\]By using \(_{i=1}^{n}Z_{i}\) as an approximation, we avoid the challenge of evaluating the integrals analytically. The detailed algorithm is documented in Algorithm 2. It's known that any space of constant curvature is isomorphic to one of the spaces: Euclidean space, sphere, and hyperbolic space (Vinberg et al., 1993; Woods, 1901). Therefore, Algorithm 2 offers a straightforward and practical method for assessing the privacy budget \(\) on spheres and hyperbolic spaces. Furthermore, Algorithm 2 can be extended to a more general class of Riemannian manifold by sampling more than one pair of \(,^{}\) in step 1. The determination of the number of pairs being sampled and the selection method to ensure sufficient dissimilarity among the sampled pairs is an aspect that requires further investigation.

```
0: Sensitivity \(\), rate \(^{2}\), Monte Carlo sample size \(n\), number of \(\) used \(n_{}\), maximum \(\) used \(_{}\), number of MCMC samples \(m\)
0: privacy budget \(\);
1: Sample a random point \(\) on \(\) and another point \(^{}\) on the sphere center at \(\) with radius \(\).
2:for each \(j\) in \(1,2,,m\)do
3: Sample \(y_{1j},,y_{nj} N_{}(,^{2}),y^{}_{1j}, ,y^{}_{nj} N_{}(^{},^{2})\).
4: Compute \(d_{ij}=d(^{},y_{ij})^{2}-d(,y_{ij})^{2}\) and \(d^{}_{ij}=d(^{},y^{}_{ij})^{2}-d(,y^{}_{ij})^{2}\) for \(i\) in \(1,2,,n\).
5:for each \(\) in \(\{k,2k,,n_{}k\}\) where \(k=_{}/n_{}\)do
6: Compute \(l_{}^{(j)}=_{i=1}^{n}(d_{ij} 2^{2} )/n-e^{}_{i=1}^{n}(d^{}_{ij} 2 ^{2})/n\).
7:endfor
8:endfor
9: Compute \(l_{}=_{j=1}^{m}l_{}^{(j)}/m\) and \(_{}\) via \(l_{}=_{}()\) for each \(\). Compute \(=_{}_{}\).
10:Return: \(\). ```

**Algorithm 2** Computing \(\) on Manifolds with Constant Curvature

### Sampling from Riemannian Gaussian Distribution

The one crucial step in Algorithm 2 involves sampling from the two Riemannian Gaussian distributions \(N_{}(,^{2})\) and \(N_{}(^{},^{2})\). Since their densities (1) are known up to a constant, a Metropolis-Hasting algorithm would be a natural choice. In this section, we describe a general Metropolis-Hasting algorithm for sampling from a Riemannian Gaussian distribution on an arbitrary homogeneous Riemannian manifold (Pennec et al., 2019). However, there are more efficient sampling algorithms that are tailored to specific manifolds (e.g., (Said et al., 2017; Hauberg, 2018)).

The Metropolis-Hasting algorithm involves sampling a candidate \(y\) from a proposal distribution \(q(|x)\). The acceptance probability of accepting \(y\) as the new state is \((x,y)=\{1,q(y|x)p_{,}(y)/[q(x|y)p_{,}(x)]\}\). A natural choice for the proposal distribution \(q(|x)\) could be an exponential-wrapped Gaussian distribution (Galaz-Garcia et al., 2022; Chevallier et al., 2022). Informally, it's the distribution resulting from "wrapping" a Gaussian distribution on tangent space back to the manifold using the exponential map. Given the current state \(x\), we sample a tangent vector \(v N(,^{2})\) on the tangent space \(T_{x}\). If \(\|v\|\) is less than the injectivity radius, we then accept the newly proposed state \(y=_{x}(v)\) with probability \((x,y)\). Please refer to Section 2.5 in Pennec et al. (2019) for the detailed algorithm.

### GDP on \(\) and \(S^{1}\)

To evaluate the performance of Algorithm 2, we will conduct simulations on Euclidean space \(\) and unit circle \(S^{1}\) as the relation between \(\) and \(\) is established in Dong et al. (2022) and Corollary 3.2.1.

We fix sensitivity \(=1\) and let \(=k/4\) with \(1 k 16\). For each \(\), we determine the privacy budget \(\) using two approaches: (i) using Algorithm 1 with \(n_{}\) set as \(1000\) for \(S^{1}\) and using \(=1/\) for \(\); (ii) using Algorithm 2 with \(n=1000,n_{}=1000,m=100,_{}=/(2^{2})\) for \(S^{1}\) and \(_{}=\{10,5/+1/(2^{2})\}\) for \(\). Since Algorithm 2 is a randomized algorithm, we generate \(20\) replicates for approach (ii) for each \(\). In Figure 1, the first panel plots the sample means of the \(\) generated by approach (ii) (in grey with rectangular symbols) with error bars indicating the minimum & the maximum of the \(\)'s and the \(\) computed by approach (i) (in red with circular symbols). Additionally, as a comparison, we also plot the \(=1/\) for Euclidean space (in blue with triangular symbols) in the first plot. As we see from the first plot, the GDP results on \(S^{1}\) are almost exactly the same as on \(\) for smaller \(\). This is expected as manifolds are locally Euclidean,and the smaller the \(\) the closer the results will be. As the \(\) gets larger, the privacy budget \(\) gets smaller on \(S^{1}\) compared to on \(\). This can be explained by the fact that \(S^{1}\) is a compact space while \(\) is not. For the second panel, we plot the exactly same things for Euclidean spaces. As we can observe from both panels, our Algorithm 2 gives a fairly accurate estimation for larger \(\). However, as \(\) gets smaller, Algorithm 2 has a tendency to generate estimates that exhibit a higher degree of overestimation.

## 5 Simulations

In this section, we evaluate the utility of our Riemannian Gaussian mechanism by focusing on the task of releasing differentially private Frechet mean. Specifically, we conduct numerical examples on the unit sphere, which is commonly encountered in statistics (Bhattacharya and Bhattacharya, 2012; Mardia et al., 2000). As a comparison to our Riemannian Gaussian mechanism, we use the Riemannian Laplace mechanism implemented in Reimhert et al. (2021) to achieve GDP. Although the Riemannian Laplace mechanism is developed originally to achieve \(\)-DP, it's shown in Liu et al. (2022) any mechanism that satisfies \(\)-DP can achieve \(\)-GPP with \(=-2^{-1}(1/(1+e^{}))\). Our results show significant improvements in utility for the privatization of the Frechet mean when using our proposed Gaussian mechanism, as opposed to the Laplace mechanism, in both examples. In Section 5.1, we cover some basics on differentially private Frechet mean. In Section 5.2, we discuss the numerical results on the sphere. Simulations are done in R on a Mac Mini computer with an Apple M1 processor with 8 GB of RAM running MacOS 13. For more details on each simulation, please refer to Appendix A.6. The R code is available in the GitHub repository: https://github.com/Lei-Ding07/Gaussian-Differential-Privacy-on-Riemannian-Manifolds

### Differentially Private Frechet Mean

For more details on Frechet mean under the DP setting, please refer to Reimherr et al. (2021). Consider a set of data \(x_{1},,x_{N}\) on \(\). The Euclidean sample mean can be generalized to Riemannian manifolds as the sample Frechet mean, which is the minimizer of the sum-of-squared distances to the data, \(=_{x}_{i=1}^{N}d(x,x_{i})^{2}\). To ensure the existence & uniqueness of the Frechet mean and to determine its sensitivity, we need the following assumption.

**Assumption 1**.: _The data \( B_{r}(m_{0})\) for some \(m_{0}\), where \(r<r^{*}\) with \(r^{*}=\{,/(2)\}/2\) for \(>0\) and \(r^{*}=/2\) for \( 0\). Note \(\) denotes an upper bound on the sectional curvatures of \(\)._

Under Assumption 1, we can then compute the sensitivity of the Frechet mean (Reimherr et al., 2021). consider two datasets \(^{}\). If \(\) and \(^{}\) are the two sample Frechet means of \(\) and \(^{}\)

Figure 1: **First and Second plots**: Red lines with circular symbols represent the relation between privacy budget \(\) and rate \(\) on the unit circle \(S^{1}\). Blue lines with triangular symbols represent the relation in Euclidean space. Gray lines with rectangular symbols plot the sample mean of the \(\), across the 20 repeats, computed at a variety of \(\) using Algorithm 2. The error bar indicates the minimum and maximum of the \(\)’s. Refer to Section 4.2 for details. **Third plot**: Blue line with triangular symbols indicates the sample mean, across 100 repeats, of the Riemannian distances \(d(,x_{}})\), while the red line with circular symbols indicates the sample mean of the Riemannian distances \(d(,_{}})\). The error bands indicate the sample mean \( 4\)SE. Refer to Section 5.2 for details.

respectively, then

\[d(,^{}),  h(r,)=2r(2r)&>0\\ 1& 0\]

### Sphere

First, we revisit some background materials on spheres, refer to Bhattacharya and Bhattacharya (2012); Reimherr et al. (2021) for more details. We denote the \(d\)-dimensional unit sphere as \(^{d}\) and identify it as a subspace of \(^{d+1}\) as \(S^{d}=\{p^{d+1}:\|p\|_{2}=1\}\). Similarly, at each \(p S^{d}\), we identify the tangent space \(T_{p}S^{d}\) as \(T_{p}S^{d}=\{v^{d+1}:v^{}p=0\}\). The geodesics are the great circles, \(_{p,v}(t)=(t)p+(t)v\) with \(-<t\) where \(_{p,v}\) denotes the geodesic starts at \(p\) with unit direction vector \(v\). The exponential map \(_{p}:T_{p}S^{d} S^{d}\) is given by \(_{p}(0)=p\) and \(_{p}(v)):=(\|v\|)p+(v)v/\|v\|\) for \(v 0\). The inverse of the exponential map \(_{p}:S^{d}\{-p\} T_{p}S^{d}\) has the expression \(_{p}(p)=0\) and \(_{p}(q)=(p^{}q)[q-(p^{}q)p][1-(p^{}q)^{2}]^ {-1/2}\) for \(q p,-q\). It follows that the distance function is given by \(d(p,q)=(p^{}q)[0,]\). Therefore, \(S^{d}\) has an injectivity radius of \(\).

We initiate our analysis by generating sample data \(=\{x_{1},,x_{n}\}\) from a ball of radius \(/8\) on \(S^{2}\) and subsequently computing the Frechet mean \(\). To disseminate the private Frechet mean, we implement two methods: (i) We first generate the privatized mean \(_{}\) by drawing from \(N_{}(,^{2})\) employing the sampling method proposed by Hauberg (2018). The privacy budget \(\) is then computed using Algorithm 2. (ii) Next, we convert \(\)-GDP to the equivalent \(\)-DP using \(=[1-(-u/2))/(-u/2)]\), and generate the privatized mean \(_{}\) by sampling from the Riemannian Laplace distribution with footprint \(\) and rate \(/\) using the sampling method introduced by You and Shung (2022).

Throughout these simulations, we fix the sample size at \(n=10\) to maintain a constant sensitivity \(\). With \(\) held constant, we let the rate \(=k/4\) with \(1 k 12\). The objective here is to discern the difference between the two distances \(d(,_{})\) and \(d(,_{})\) across varying privacy budgets \(\). The third plot in Figure 1 displays the sample mean of the Riemannian distances \(d(,_{})\) (in red with circular symbols) and \(d(,_{})\) (in blue with triangular symbols) across \(1000\) iterations with the error band indicating the sample mean \( 4\)SE. From observing the third plot, we see that our Gaussian mechanism achieves better utility, especially with a smaller privacy budget \(\). With larger \(\), the gain in utility is less pronounced. One obvious reason is that there are much fewer perturbations with larger \(\) for both approaches, so the difference is subtle. The other reason is that Algorithm 2 has a tendency to overestimate \(\) with smaller \(\). Effectively, \(_{}\) satisfies \(\)-GDP with a smaller \(\) compared to \(_{}\).

## 6 Conclusions and Future Directions

In this paper, we extend the notion of GDP over general Riemannian manifolds. Then we showed that GDP can be achieved when using Riemannian Gaussian distribution as the additive noises. Furthermore, we propose a general MCMC-based algorithm to compute the privacy budget \(\) on manifolds with constant curvature. Lastly, we show through simulations that our Gaussian mechanism outperforms the Laplace mechanism in achieving \(\)-GDP on the unit sphere \(S^{d}\).

There are many future research directions. First of all, the framework established in this paper can be used for extending \((,)\)-DP to general Riemannian manifolds. There are several points of improvement around Algorithm 2 as well. Although Algorithm 2 provides us a general method of computing the privacy budget \(\), it lacks an error bound on its estimation. Furthermore, due to the random nature of Algorithm 2, a variance estimator of the output is desirable and can better inform the end user. Though we demonstrate the utility of our mechanism through simulation in Section 5, it's difficult to obtain a theoretical utility guarantee due to the lack of simple analytical relation between \(\) and rate \(\). Furthermore, although Algorithm 1 requires the manifolds to have constant curvature, it's possible to extend Algorithm 1 and Theorem 4.1 to a slightly more general class of manifolds. Additionally, the Riemannian Gaussian distribution defined in this paper is not the only way of extending Gaussian distribution to Riemannian manifolds as mentioned in Section 3. Potentially, the two other approaches, the wrapped distribution approach, and the heat kernel approach can also be used to achieve GDP on Riemannian manifolds as well. In particular, there are many rich results around heat kernel on Riemannian manifolds (see Grigoryan (2009) for example). Incorporating the heat kernel in a privacy mechanism presents ample potential for novel and noteworthy discoveries.