# Modeling Dynamics over Meshes with

Gauge Equivariant Nonlinear Message Passing

 Jung Yeon Park, Lawson L.S. Wong, Robin Walters

Khoury College of Computer Sciences

Northeastern University

Boston, MA 02115

{park.jungy@northeastern.edu, lsw@ccs.neu.edu,

r.walters@northeastern.edu}

Equal advising.

###### Abstract

Data over non-Euclidean manifolds, often discretized as surface meshes, naturally arise in computer graphics and biological and physical systems. In particular, solutions to partial differential equations (PDEs) over manifolds depend critically on the underlying geometry. While graph neural networks have been successfully applied to PDEs, they do not incorporate surface geometry and do not consider local gauge symmetries of the manifold. Alternatively, recent works on gauge equivariant convolutional and attentional architectures on meshes leverage the underlying geometry but underperform in modeling surface PDEs with complex nonlinear dynamics. To address these issues, we introduce a new gauge equivariant architecture using nonlinear message passing. Our novel architecture achieves higher performance than either convolutional or attentional networks on domains with highly complex and nonlinear dynamics. However, similar to the non-mesh case, design trade-offs favor convolutional, attentional, or message passing networks for different tasks; we investigate in which circumstances our message passing method provides the most benefit 1.

## 1 Introduction

Surfaces embedded in 3D space appear in many domains such as computer graphics , structural biology [2; 3], neuroscience , climate pattern segmentation [5; 6], and fluid dynamics . Such objects are naturally Riemannian manifolds but are often discretized into meshes for computational tractability. Unlike other 3D representations such as point clouds or voxels, meshes encode both the geometry and topology of the surface.

One important task that naturally arises over meshes is solving partial differential equations (PDEs) over surfaces and non-Euclidean manifolds [11; 12; 13; 14]. A classic approach to solving surface PDEs is to model the domain as a mesh and use the finite element method and the variational formulation [15; 16]. Recently, there has been increasing success in applying deep learning methods to accelerate solving PDEs and building differentiable surrogates for dynamics models [17; 18; 19; 20]. While there are many effective approaches for gridded data or point clouds, fewer methods exist for leveraging the geometric structure of meshes. Several approaches simply treat the mesh as a graph by approximating patches of the manifold as Euclidean [21; 22; 23]. However, these approaches do not exploit the geometric properties of meshes, and Verma et al. , de Haan et al.  have shown that not retaining this information leads to poor performance. One way to express such information is to specify the localgauge symmetries and explicitly model operations that are equivariant to these transformations [6; 9], by using parallel transport and applying a gauge equivariant filter.

Although previous work on gauge equivariance for meshes including convolutional (GemCNN ) and attentional (EMAN ) methods have shown good performance on shape correspondence and mesh classification tasks, we find they are inadequate for modeling complex dynamics on meshes, such as solving surface PDEs, which can be highly nonlinear. In this work, we introduce a new architecture, Hermes, that combines gauge equivariance with nonlinear message passing. As Hermes can express linear convolutions or attention, Hermes is strictly more expressive than GemCNN and EMAN. Hermes completes the 3 flavors of gauge equivariant message passing, analogous to graph neural networks . See Figure 1 for a comparison.

We evaluate Hermes on several linear and nonlinear partial differential equations, and also on shape correspondence, object interaction system, and cloth dynamics. Our experiments show that Hermes outperforms convolutional or attentional counterparts in most domains, particularly on nonlinear surface PDEs. We find that Hermes is more robust to both the fineness of the mesh and surface roughness compared to both GemCNN and EMAN. We further investigate when the added model complexity of nonlinear message passing outweighs the cost. With this work, we hope to help guide practitioners on when to use nonlinear message passing schemes over linear schemes.

Our contributions are summarized as follows: 1) we propose a novel gauge equivariant, nonlinear message passing architecture, Hermes, for learning on meshes, 2) when evaluated on complex and nonlinear dynamics such as surface PDEs, Hermes outperforms both convolutional and attentional architectures and can generate realistic prediction rollouts, and 3) we investigate in which situations nonlinear message passing should be preferred over convolutional or attentional counterparts.

## 2 Background

In this work, we focus on learning signals over meshes. Similar to de Haan et al. , our goal is to define a model in coordinates that are intrinsic to the 2D mesh instead of using the extrinsic 3D coordinates of the embedding space. This avoids dependence on the 3D embedding, which is irrelevant. However, in order to encode data over the mesh, it is still necessary to make a choice of local coordinate frame at each vertex. Since this coordinate frame is arbitrary, it follows that the model should be invariant to change of coordinates frame at each vertex, i.e. gauge equivariant.

### Data over Meshes

The Mesh DatumA mesh \(M\) consists of \((,,)\), where \(\) is the set of vertices, \(=\{(i,j)\}\) is the set of ordered vertex indices \(i,j\) connected by an edge, and \(=\{(i,j,k)\}\) is the set of ordered vertex indices \(i,j,k\) connected by a triangular face. Let \(_{v}\) denote the set of vertices connected to a vertex \(v\). We assume that the mesh represents a discrete \(2\)-dimensional manifold embedded in \(^{3}\) (i.e. a manifold mesh). Let \(x_{b}^{3}\) be the coordinates of vertex \(b\). We assign a normal vector \(n_{b}\) to each vertex \(b\) equal to the normalized average of the surface normals of the faces that contain the vertex. The associated tangent plane \(T_{b}M\) at vertex \(b\) is the 2-dimensional affine space through \(x_{b}\) orthogonal to the normal \(n_{b}\).

Figure 1: Mesh example. The neighbors of vertex \(b\) are projected onto the tangent plane to compute their local orientation (\(_{a},_{c}\)) after choosing a reference neighbor (gauge). There are 3 flavors of gauge equivariant message passing, cf. Fig. 17 in Bronstein et al. . GemCNN  computes linear convolutions using the neighbor node features, EMAN  uses linear messages with attention, while Hermes performs nonlinear message passing. Self-interactions are omitted for clarity.

Local Coordinate FrameWe adopt the strategy of  by defining the local coordinate frame at each vertex in terms of a reference neighboring vertex \(d_{b}\). See Figure 1 for a visualization. The reference neighbor defines a positively oriented orthonormal basis \(\{e_{1}^{b},e_{2}^{b}\}\) for the tangent space \(T_{b}M\). The orientation of every neighbor \(v_{b}\) can be represented with polar angles, where \(_{v}\) is the angle between \(v\) and the reference neighbor after projection to the tangent plane. In Figure 0(a), \(_{d}=0\) as \(d\) is itself the reference neighbor and we show the \(_{v}\) for the other neighbors with respect to the reference neighbor \(d\). See Appendix A.1 for more information.

Change of GaugeA different choice of reference neighbor will result in a different positively oriented orthonormal basis. Any two such bases will be related by an element \(g(2)\). Here \((2)\) is called the gauge group and \(g\) is the gauge transformation. Note that such a change is local, and a different change \(g^{p}\) can be performed at each vertex \(p\). For example, if we switch from reference neighbor \(d\) to \(c\), then we induce a gauge transformation \(()(2)\) where \(=_{c}-_{d}\) is the angle between \(d\) and \(c\). The orientations are then updated \(_{q}_{q}-, q_{p}\).

Feature Fields on the MeshInput, output, and hidden features are encoded as data over the vertices \(\{f_{p}\}_{p}\) or edges of the mesh \(\{e_{pq}\}_{(p,q)}\). We consider directed graphs and so for the edge feature \(e_{pq}\), \(p\) is the source and \(q\) is the target node. We situate \(e_{pq}\) to be at \(T_{p}M\) for simplicity as they are often chosen to be vectors relative to \(p\) or the edge distance [25; 18]. A scalar field, such as temperature, is invariant to change of gauge, and thus \(f_{p}\) transforms according to the trivial representation \(_{0}\) of \((2)\). A vector field over the mesh, representing e.g. a flow across the manifold would have \(f_{p}^{2}\) and transform according to the standard rotation representation \(_{1}\) of \((2)\) by \(2 2\) matrices. In general, feature fields may be any representation \(_{n}\) of the gauge group \((2)\).

Parallel TransportLet \(f_{p}\) and \(f_{q}\) be feature vectors at nodes \(p\), \(q\), respectively, where \(q\) is adjacent to \(p\). As features at different nodes live in different tangent spaces and have different bases, in order for the anisotropic kernel to be applied to \(f_{q}\) for each \(q_{p}\), each \(f_{q}\) must be parallel transported to \(T_{p}M\) and be in the same gauge. The parallel transporter \(g_{q p}\) first aligns the tangent plane at \(q\) to the tangent plane at \(p\) by a 3D rotation and then transforms the gauge at \(q\) to the gauge at \(p\) by a planar rotation. Acting on \(f_{q}\) with \(g_{q p}\) transports \(f_{q}\) to the gauge at \(p\). For the edge features \(e_{pq}\), no parallel transport is required as the features live in \(T_{p}M\). For more details, see Appendix A.2 and Appendix A.2 of .

### Flavors of Message Passing

Meshes are graphs with additional geometric information. Neural networks designed for meshes may be understood by analogy to graph neural networks (GNNs). As outlined in Bronstein et al. , there are largely three flavors of GNNs: convolutional, attentional, and nonlinear message-passing.

The three images on the right of Figure 1 show the three different GNN flavors in the context of gauge equivariant networks over meshes. To date, both convolutional and attentional GNNs have gauge equivariant analogues for learning over meshes. Here, we introduce a gauge equivariant mesh network in the nonlinear message-passing flavor. Let \(p\) be the source node (\(b\) in Figure 0(a)), \(q_{p}\) be the target nodes (one-hop neighborhood), and \(f\) the signals over nodes.

ConvolutionalGemCNN  uses convolutions where the kernel \(K_{}\) is applied (anisotropically) to features at the target nodes. It is comparable to graph convolutional networks  but with anisotropy and gauge equivariance. As convolutions are linear, the messages passed to the source node are linear, and these messages are aggregated in a permutation invariant way. GemCNN also has a kernel \(K_{}\) to model self-interactions before updating the feature at the source node. The gauge equivariant convolution (GemCNN layer) is defined as

\[f_{p}^{}=K_{}f_{p}+_{q_{p}}K_{ }(_{q})_{}(g_{q p})f_{q},\] (1)

where \(K_{}(_{q})\) is the kernel for \(q\) and \(_{}(g_{q p})f_{q}\) is the feature vector at \(q\) parallel transported to the gauge at \(p\). Input features \(f_{q}\) transform according to \(_{}\) and output features \(f_{q}^{}\) according to \(_{}\).

The filter \(K_{}\) is anisotropic, i.e., it depends on the orientation \(_{q}\). This is strictly more expressive than an isotropic filter (as in a GNN) which would simply use the same weights for each neighboring vertex. However, the dependence on orientation means that without constraints, the convolution depends on the choice of local coordinate frame. To fix this and impose gauge equivariance, \(K_{}\) must satisfy \(K=_{}(g)^{-1}K_{}(g)\). We also define the kernel to only depend on the orientation and not on the radial distance of neighboring nodes, as including the radius in the parameterization was not beneficial  and verified in our experiments.

**Attentional** EMAN  extends GemCNN by adding an attention mechanism to the messages before the aggregation step, analogous to graph attention networks . Gauge equivariant attention with the self-interaction term is defined as

\[f_{p}^{}=_{pp}K_{}^{}f_{p}+_{ q_{p}}_{pq}K_{}^{}(_{q})(g_{q  p})f_{q},\] (2)

where \(_{pp},_{pq}\) are the attention weights for the self-interaction and neighbor interaction terms (see Equation (12) in Appendix B.2). Multi-headed attention can be used by concatenating the results of the individual heads. The attention weights are combined linearly with the neighboring node features and can be seen as weighted local averaging. Note that due to the separate kernels for the keys, queries, and values, an EMAN layer uses many more parameters than an equivalent GemCNN layer with the same dimensions.

## 3 Gauge Equivariant Nonlinear Message Passing

We propose gauge equivariant nonlinear message passing for meshes, complementing the convolutional and attentional gauge equivariant methods. As is the case for nonlinear message passing GNNs, our network is designed to be better suited for tasks with complex local interactions. Our network, named Hermes, consists of a sequence of message passing blocks, each of which contains an edge network \(_{e}\), an aggregation (we use sum throughout), and a node network \(_{n}\).

The edge network models neighboring interactions and takes as inputs the source node features \(f_{p}\), target node features \(f_{q}\) where \(q_{p}\), and edge features \(e_{pq}\) where \((p,q)\). The target node features are parallel transported to the gauge at \(p\) as \((g_{q p})\). The network \(_{e}\) consists of \(N_{e}\) gauge equivariant convolutions followed by regular nonlinearities  which also preserve gauge equivariance. The nonlinear messages \(m_{pq}\) are then aggregated to \(m_{p}\). The node network models self-interactions and takes as input \(m_{p} f_{p}\), where \(\) represents the direct sum, and then applies \(N_{n}\) gauge equivariant convolutions with \(K_{}\) kernels and regular nonlinearities. Equations (3)-(7) define gauge equivariant nonlinear message passing,

\[h_{pq} =f_{p}(g_{q p})f_{q} e_{pq}, (p,q)\] (3) \[m_{pq} =_{e}(h_{pq})= K_{}^{N_{e}}(_{q} ) K_{}^{1}(_{q})(h_{pq}),\] (4) \[m_{p} =_{qp}m_{pq},  p\] (5) \[h_{p} =m_{p} f_{p}\] (6) \[f_{p}^{} =_{n}(h_{p})= K_{}^{N_{n}}  K_{}^{1}(h_{p}),\] (7)

where \(h_{pq}\) and \(h_{p}\) are inputs to \(_{e}\) and \(_{n}\), \(\) is the regular nonlinearity, and \(\) denotes composition.

Figure 2 shows the Hermes architecture for one of the datasets used in experiments (wave PDE, see Section 4.1). For comparison, we also include the GemCNN and EMAN architectures in Appendix B. A residual connection is included at the end of each HermesBlock for more expressivity, see Section 5.5 for ablation results.

Proof of Gauge EquivarianceWe explicitly show that Hermes is equivariant to local gauge transformations in Proposition 1.

**Proposition 1**.: _Hermes is equivariant to local gauge transformations \(g^{p}\) for any \(p\) of a mesh._

The proof, provided in Appendix B.3, uses the fact that all of the kernels used in Hermes satisfy the constraints for gauge equivariance. The nonlinearities are exactly gauge equivariant as the number of intermediate samples \(N\) used in the discrete Fourier transform approaches infinity and approximately gauge equivariant for finite \(N\) (see Section 4 in  for more details). Thus Hermes is also gauge equivariant (in the limit).

Hermes combines both nonlinear message passing and gauge equivariance, generalizing GemCNN and EMAN, and completes the full picture with the \(3\) flavors of message passing (cf. Figure 1). Animportant point to note is that nonlinear message passing decouples the effect of the number of layers in the network (depth) with the receptive field of the graph. For graph convolutions using 1-hop neighbors, the receptive field of messages is exactly equal to the network depth. In contrast, nonlinear message passing computes messages using features from neighbors at an arbitrary hop distance away from the source node before the message is passed to the source node. We hypothesize that this decoupling is beneficial for tasks with complicated interaction dynamics between node neighbors, and previous works with graph message passing networks [28; 29] have shown good performance for abstract graphs (non-spatial graphs) involving objects. Although meshes are inherently spatial, we show that nonlinear message passing is an important tool for solving PDEs on surfaces and can outperform either convolutional or attentional mechanisms.

## 4 Experiment Design

To evaluate gauge equivariant nonlinear message passing, we consider several different tasks. Our primary domains are partial differential equations on meshes, but we also consider shape correspondence, object interactions, and cloth simulation. Throughout, we consider triangular meshes as they are the most common. Further details and visualizations of the domains are provided in Appendix C.

### Domains

**Partial Differential Equations on meshes**  We consider three linear and nonlinear surface partial differential equations (PDEs), where the dynamics occur on the surface of an object, represented as a two dimensional mesh embedded in 3D space. Since solving PDEs on meshes, e.g. heat diffusing over a surface, naturally depends on the intrinsic mesh geometry, gauge-equivariant nonlinear message passing is a promising solution. For all three equations, we use example meshes in the PyVista library  and generate \(5\) trajectories with different initial conditions, see Appendix C for more details.

Heat/Wave equation: The heat and wave equations are second-order linear partial differential equations (see Appendix C.1). Solving the heat equation on a surface mesh can correspond to modeling the dynamics when an external hot object makes contact at certain points on a thin hollow object. The wave equation describes how acoustic waves propagate on a thin hollow surface. The dynamics of both equations are highly dependent on the local mesh geometry.

Cahn-Hilliard equation: The Cahn-Hilliard equation  describes phase separation in a binary fluid mixture and is often used to model spinodal decomposition. It is a fourth-order, nonlinear,

Figure 2: Hermes network architecture for the Wave PDE dataset. There are two message blocks, each with \(2\) layers for the edge network \(_{e}\) and \(1\) layer for the node network \(_{n}\). To illustrate the computations within the edge and node networks, we use the example mesh from Figure 0(a) as input. In the edge network, we show only the computations for node \(b\) for clarity.

time-dependent PDE and is often factored into two coupled second-order equations. The surface Cahn-Hilliard equation can model real-world applications such as cell proliferation  and two-component vesicles . As a nonlinear PDE, we expect our nonlinear message passing method to exhibit greater performance than other linear message passing flavors.

Shape correspondenceAs a standard mesh benchmark, we use the same FAUST dataset used in previous work [9; 10]. The dataset consists of 80 train and 20 test high resolution scans of 10 humans in 10 different poses. The task is to determine shape correspondence between different meshes. As the vertices are all registered and represent the same position on the human body, this task is equivalent to classifying the correct label for each vertex.

Object interactionsInspired by interaction systems [28; 29; 34], we consider complex dynamics of interacting objects on a mesh. On a coarse triangular mesh with random hills, each object occupies a vertex and is oriented towards a neighboring vertex. An action can either turn the object left (changing its orientation), move the object forward, or turn right. Objects cannot move forward if there is another object at the destination vertex, giving rise to complex interacting dynamics. Furthermore, we consider the geometry of the mesh such that an object cannot move forward if the height difference between the current vertex and destination vertex is too high, or if the angle between the vertex normals is too large. If an object is able to move forward, we parallel transport its orientation and then choose the nearest neighboring node as its new orientation.

FlagSimpleWe also include the FlagSimple dataset from , which simulates cloth dynamics of a flag with self-collisions. Unlike the other datasets, the mesh is dynamic where the node positions change over time. The dataset was created using ArcSim  with regular meshing over \(400\) timesteps. See Appendix A.1 of  for more information.

### Training Details

For the PDE datasets, we report test root mean squared error (RMSE) of the prediction at the next timestep given the previous \(5\) timesteps. We use three separate test datasets and evaluate generalization to future timesteps (test time), unseen initial conditions (test init), and unseen meshes (test mesh). For test time, we train on timesteps \(T=0,,149\) and test on \(T=150,,200\). For test init, we test on trajectories with new initial conditions. For test mesh, we evaluate on completely unseen meshes to see whether a method overfits to specific mesh geometries.

For baselines, our main comparison is with GemCNN and EMAN to gauge how beneficial nonlinear message passing is over convolutional or attentional flavors. We also compare against 1) a SOTA message passing, mesh-aware method MeshGraphNet , 2) an E(3)-equivariant, non mesh-aware message passing network EGNN , 3) a non-equivariant, mesh-aware baseline SpiralNet++ , and 4) standard non-equivariant GNNs: graph convolutional networks (GCN)  and message passing networks (MPNN) [37; 38]. We tune each architecture to use a similar number of trainable parameters for a fair comparison, see Table 9 in Appendix D. A more detailed feature comparison of each method is provided in Table 8 and all other training details are relegated to Appendix D.

## 5 Results

Table 1 shows the results on the PDE datasets and other results are given in Table 10 in Appendix E. On Heat, we see that EMAN outperforms GemCNN and Hermes outperforms EMAN significantly. This coincides with our expectation as attention mechanisms can express convolutions using constant weights, and Hermes generalizes both linear convolution and attention mechanisms. However, this does not hold for the Wave dataset, where EMAN is noticeably worse than GemCNN while Hermes still performs well. On the nonlinear Cahn-Hilliard dataset, we see that EMAN cannot generalize well to unseen meshes. Overall Hermes achieves an RMSE approximately \(3\) times lower than that of GemCNN, and between \(2\) to \(8\) times lower than that of EMAN.

Compared to other non gauge equivariant baselines, Hermes outperforms all methods, except for certain cases with MeshGraphNet. Hermes is worse than MeshGraphNet on Heat, outperforms on Wave, and performs similarly on Cahn-Hilliard. It performs substantially better on all test mesh datasets, which may indicate that Hermes can generalize to the true dynamics function, rather than the specific dynamics seen in the training trajectories. In other words, Hermes is better adapted to use the underlying geometry while MeshGraphNet overfits to specific geometries.

On the FlagSimple dataset (Table 10), Hermes outperforms MeshGraphNet considerably, suggesting that Hermes is not limited to static meshes and can handle temporally changing meshes well. Note that MeshGraphNet was modified to have a similar number of parameters as Hermes and so the results are different than reported in Pfaff et al. .

### Long-horizon prediction rollouts

Using a representative random run, we generate predictions from each model autoregressively given only the initial conditions. We generate predictions on the unseen meshes (test mesh) and roll out the entire trajectory of the PDE (\(T_{}=200\)). Figure 3 shows how the errors change with the increasing rollout horizon. For GemCNN, the errors accumulate quickly and the predictions diverge for Heat and Wave. EMAN performs similarly to Hermes on Heat but underperforms on Cahn-Hilliard. All methods eventually diverge on Wave; this may be due to the fact that the wave amplitude oscillates multiple times over the longer horizon and so it may be more difficult to predict the periodic nature.

Table 2 shows prediction samples at \(T+50\) generated autoregressively given only initial conditions. GemCNN fails on the Heat dataset and does not produce the correct spatial patterns for Wave and Cahn-Hilliard. EMAN performs well on Wave, but fails on Cahn-Hilliard. Hermes gives fairly realistic predictions on all datasets. See Appendix E.1 for more samples with varying \(T\).

### Mesh Fineness

Here we investigate how mesh fineness impacts performance with respect to the three flavors of message passing. Our intuition is that as meshes become finer, the features at each node become more similar. Thus the dynamics between nodes would become more linear and convolutional approaches may perform comparably with nonlinear message passing.

For this experiment, we solve the heat and wave equations on a single mesh ("armadillo"). To generate different mesh resolutions, we simplify the original mesh (\(\#\) vertices = \(172,974\)) to \(\{348,867,1731,3461,8650\}\) vertices using quadric decimation  (\(1731\) vertices were used for the main results in Table 1). See Figure 9 in Appendix E.2 for data visualizations. We generate \(15\) trajectories with \(T_{}=100\) and use \(5\) of the \(15\) trajectories as test init and use \(T=81,,100\) of the remaining \(10\) trajectories for the test time. As we use a single mesh, we do not test generalization to unseen meshes. We use \(3\) random seeds for each method.

    & & **Hermes** & GemCNN & EMAN & GCN & MPNN & MGN & EGNN & SpiralNet++ \\   & Test time & \(1.18 0.3\) & \(3.88 0.8\) & \(2.93 0.9\) & \(152 1.2\) & \(2.66 0.8\) & **0.93\( 0.2\)** & \(3.09 1.2\) & \(2.82 0.2\) \\  & Test init & \(1.16 0.3\) & \(3.85 0.8\) & \(2.90 0.9\) & \(152 0.0\) & \(2.63 0.8\) & **0.93\( 0.2\)** & \(3.07 1.2\) & \(6.44 0.1\) \\  & Test mesh & \( 0.3\) & \(3.50 0.6\) & \(2.47 0.7\) & \(127 2.2\) & \(2.36 0.7\) & \(2.41 1.1\) & \(8.96 0.5\) & \(22.0 0.3\) \\   & Test time & \( 0.8\) & \(12.2 1.5\) & \(19.0 0.3\) & \(162 5.0\) & \(9.07 1.2\) & \(6.26 0.9\) & \(45.9 0.1\) & \(8.88 1.2\) \\  & Test init & \( 1.3\) & \(7.28 0.7\) & \(15.3 0.6\) & \(158 5.0\) & \(5.24 1.1\) & \(4.24 0.6\) & \(12.1 3.5\) & \(8.47 0.6\) \\  & Test mesh & \( 1.3\) & \(8.23 0.6\) & \(15.8 3.7\) & \(164 5.1\) & \(6.29 1.3\) & \(7.01 0.9\) & \(54.5 18\) & \(10.8 0.8\) \\   & Test time & \( 0.9\) & \(13.8 1.2\) & \(8.41 0.4\) & \(250 7.6\) & \(7.25 1.3\) & **4.49\( 0.6\)** & \(8.36 1.4\) & \(11.6 3.3\) \\  & Test init & \(4.23 1.5\) & \(14.9 12\) & \(8.75 0.4\) & \(383 0.6\) & \(7.52 3.0\) & **4.64\( 0.6\)** & \(10.6 1.1\) & \(12.9 2.8\) \\   & Test mesh & \( 0.8\) & \(14.1 12\) & \(43.8 27\) & \(391 8.6\) & \(7.63 0.3\) & \(18.7 7.2\) & \(9.38 1.7\) & \(13.4 2.5\) \\   

Table 1: RMSE on PDE domains, using \(5\) runs. All values are expressed in \( 10^{-3}\) and gray denotes \(95\%\) confidence intervals. Hermes generally outperforms baselines, except for some cases with MeshGraphNet (MGN). Hermes outperforms MeshGraphNet on all test mesh datasets.

Figure 3: Errors from long-horizon prediction rollouts on unseen meshes, given only initial conditions. Error bars denote standard error over \(5\) runs averaged over unseen meshes (2 meshes for heat/wave, 1 for Cahn-Hilliard). GemCNN has exploding errors with increasing \(t\) on Heat and Wave. Hermes generally outperforms GemCNN and EMAN, with the exception of EMAN on Wave.

Figure 4 shows the results for the heat dataset. RMSE on both the test time and test init datasets increase for GemCNN and EMAN as the mesh becomes coarser (decreasing number of vertices). This coincides with the intuition that coarser meshes have more nonlinear dynamics between nodes and thus contributing to the increase in errors. On the other hand, Hermes still quantitatively outperform GemCNN and EMAN across all mesh resolutions. It is also robust to varying mesh fineness and has similar error values throughout.

On the wave dataset, we find that Hermes still outperforms GemCNN and EMAN at each resolution though the gap is not as large, see Figure 10 (Appendix E.2). There is a surprising decreasing trend in errors for all methods as the mesh becomes finer. This indicates that mesh fineness may not be the only factor at play: it is possible that the type of dynamics (PDE) used and the specific architecture (e.g. graph receptive field) can affect the results. Another possible explanation is that as we use a single simplified mesh, there may be some artifacts that affect the wave PDE simulations.

### Surface Roughness

We investigate whether surface roughness affects the performance gap between GemCNN and Hermes. We use the same armadillo mesh with \(1{,}731\) vertices from before and extract the vertex normals. The vertex normals are then randomly scaled using a Gaussian distribution \((0,s^{2})\), where \(s\{0.1,0.5,1,1.5,3\}\). The vertex coordinates are then modified by adding these scaled normals to the original coordinates, resulting in different surface roughnesses. See Figure 11 in Appendix E.3 for visualizations. We use the same settings as in the fineness experiments in Section 5.2.

Similarly to the results on mesh fineness, we find that Hermes retains the performance advantage over GemCNN and EMAN across all roughness scales. Its RMSE errors are also nearly constant

   Dataset & GemCNN & EMAN & Hermes & Ground Truth \\  Heat & & & & \\ (T+50) & & & & \\ Wave & & & & \\ (T+50) & & & & \\   

Table 2: Qualitative prediction samples rolled out to the full path using only initial conditions. GemCNN completely fails on Heat, while EMAN fails on Wave. Hermes predicts the spatial patterns accurately and outperforms GemCNN and EMAN on all datasets.

Figure 4: Performance for varying mesh fineness for heat on the test time (left) and test init (right) datasets. Error bars denote standard error over \(3\) runs. The errors for GemCNN and EMAN increase as the mesh becomes coarser, while Hermes performs similarly throughout.

throughout indicating robustness to surface roughness. Surprisingly, GemCNN seems to perform increasingly better as the surface becomes rougher while EMAN seems to perform roughly equal. On the wave dataset, Hermes still outperforms other baselines at most roughness scales, see Figure 12 in Appendix E.3. There does not seem to be any correlation in errors with surface roughness.

### Computation Time

As the edge and node networks \(_{n},_{e}\) consist of multi-layer perceptrons, one might think Hermes requires more computation time than its simpler convolutional or attentional counterparts. Table 3 shows that this is not the case; Hermes has a lower computation time than both GemCNN and EMAN. As we control for a similar number of parameters in each architecture, there are fewer aggregations in Hermes than in GemCNN or EMAN resulting in a lower average computation time in the forward pass. For EMAN in particular, due to its attention mechanism, we find that it uses many more parameters than either GemCNN or Hermes. Thus constraining EMAN to have a similar number of parameters as GemCNN restricts its expressivity. On the other hand, Hermes is very flexible as it can use a different number of layers for the edge and node networks. Even though Hermes uses a slightly lower hidden dimension than GemCNN in all domains, the results show that this does not hamper model expressivity and it still achieves higher performance. As expected, all three gauge equivariant methods are significantly more computationally expensive than standard graph neural networks.

### Residual connection ablation

As Hermes models the self-influence of nodes, we also test whether a residual connection is necessary. Table 12 in Appendix E.4 shows the ablation of the residual connection. Having a residual connection improves performance on Heat, but decreases performance slightly on Wave and Cahn-Hilliard. Thus the residual connection seems to be task-dependent and should be considered a hyperparameter. In our experiments, a residual connection was used for each message passing block.

## 6 Related Work

Learning over meshesSeveral different mesh operators have been studied within computer graphics, in the context of shape classification [40; 41; 42], dense shape correspondence [43; 36], mesh segmentation [44; 45; 46]. Due to the success of graph neural networks (GNNs) [26; 27; 38], several approaches use GNNs to process meshes [21; 22; 47; 36] and incorporate geometric information via geodesic convolutions, anisotropic kernels, dual graphs, or spiral operators. Pfaff et al.  introduce a state of the art method for learning simulations with meshes by representing a graph in mesh space and in world space. In this work, we introduce a novel architecture that combines nonlinear message passing with explicit equivariance to local gauge transformations.

    & GemCNN & EMAN & Hermes & GCN & MPNN & MeshGraphNet & EGNN & SpiralNet++ \\  Heat (ms) & \(12.4\) & \(19.5\) & \(10.8\) & \(1.5\) & \(1.2\) & \(2.2\) & \(1.4\) & \(0.9\) \\ Wave (ms) & \(12.5\) & \(17.7\) & \(10.5\) & \(1.4\) & \(1.3\) & \(2.2\) & \(1.3\) & \(0.9\) \\ Cahn-Hilliard (ms) & \(6.9\) & \(9.2\) & \(6.2\) & \(1.9\) & \(1.5\) & \(2.1\) & \(1.6\) & \(1.3\) \\   

Table 3: Mean forward computation time in seconds (ms) during inference on test time PDE datasets.

Figure 5: Performance for varying surface roughness for heat on the test time (left) and test init (right) datasets. Error bars denote standard error over \(3\) runs. Increasing scale increases the surface roughness of the mesh. Hermes outperforms GemCNN and EMAN across different roughness scales and is much more robust.

Gauge SymmetryMost works on non-Euclidean manifolds have used convolutions on discretized patches of the manifold, approximating them to be Euclidean. Masci et al.  define the patch operator using local geodesic coordinate systems. Monti et al.  use a mixture of parametric Gaussian kernels. All of these works use (linear) convolutions, unlike our work. Boscaini et al.  uses anisotropic diffusion kernels, but uses the principal curvature direction as the preferred gauge which may be ill-defined for some shapes. On the other hand, several works have considered equivariance to gauge symmetry. Cohen et al.  proposes gauge equivariant convolutions on the icosahedron. Most similar to our work is that of [9; 48; 10], which explicitly incorporate gauge equivariant kernels for meshes. de Haan et al.  use convolutions while He et al.  and Basu et al.  use attention to discrete and continuous gauge transformations, respectively. In this work, we complete the picture and propose general nonlinear message passing with gauge equivariance for meshes. For more in-depth theory and discussion of local gauge equivariance on manifolds, see .

Solving PDEs on surfacesWith the rise of physics-informed neural networks [50; 51; 52] and their increased sample efficiency and performance over classical methods, several recent works have focused on solving complex partial differential equations with deep learning, e.g. fluid dynamics [53; 54; 55], thermodynamics , structural mechanics [57; 58], and material science [59; 60]. Some approaches learn the finite-dimensional [51; 53; 54] or infinite-dimensional [61; 17; 62] solution operators. However, there has been less work on applying deep learning to solving PDEs on surfaces embedded in 3D space using meshes, with some exceptions [63; 64; 65]. Fang and Zhan  solve the Laplace-Beltrami equation over a 3D surface with neural networks while Tang et al.  propose an extrinsic approach. However, both methods are mesh-free and only use points and their normals, discarding any connectivity information. Li et al.  extend Fourier neural operator  and learns a diffeomorphic deformation between the input space and the computational mesh. While Geo-FNO depends on the embedding space of the mesh (e.g. embedding a rough 2D mesh in 3D), our method works directly on the intrinsic mesh surface. Perhaps most relevant to this paper is , which extends GemCNN  in several ways to predict hemodynamics on artery walls. In this work, we propose a nonlinear message passing architecture for meshes that retains the underlying geometry and demonstrate their effectiveness in predicting a variety of surface dynamics.

## 7 Discussion

We introduce a novel architecture, Hermes, that performs gauge equivariant, nonlinear message passing for meshes. Hermes complements convolutional GemCNN and attentional EMAN and completes development of the 3 flavors of message passing in the gauge equivariant setting. In the context of meshes, similar to GNNs, there seems to be a tradeoff between simple linear operations such as convolutions versus nonlinear message passing. Convolutions are computationally efficient and can perform well on simpler tasks such as shape correspondence, see Table 10 in Appendix E. However, when the interactions are more complicated such as in surface PDEs, nonlinear message passing surpass linear schemes. By decoupling the degree of nonlinearity from the depth of the network and receptive field, Hermes outperforms GemCNN and EMAN significantly on the PDE datasets and produces realistic predictions given only initial conditions.

A limitation is that Hermes may use more parameters depending on the architectures of the edge and node networks. In particular, both EMAN and Hermes cannot scale well to meshes with a large number of vertices naively, and one may need to consider more sophisticated approaches such as multi-scale or graph expander approaches. Furthermore, the architecture search space for Hermes is larger than that of GemCNN or EMAN, as one needs to consider different combinations of numbers of layers in the edge and node networks, along with the number of message passing blocks.

For future work, one direction is to consider different dynamics such as non-stationary or chaotic dynamics, and other PDEs important in real world applications such as Navier-Stokes for climate or blood flow. Another direction is to analyze the design space of gauge equivariant networks. While GNNs have been extensively studied, far less work exists for mesh methods. GNNs are often highly task-specific and there are many design dimensions (e.g. residual connections, message passing iterations, etc.) to consider . It would be particularly helpful for practitioners to have guidelines on when to use gauge equivariance or message passing over simpler approaches. This work aims to be a first step in this direction by demonstrating Hermes as a good fit for predicting nonlinear dynamics on meshes.