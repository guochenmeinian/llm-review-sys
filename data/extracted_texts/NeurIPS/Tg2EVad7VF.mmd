# DiffNorm: Self-Supervised Normalization for Non-autoregressive Speech-to-speech Translation

Weiting Tan Jingyu Zhang Lingfeng Shen Daniel Khashabi Philipp Koehn

Department of Computer Science

Johns Hopkins University

{wtan12, jzhan237, lshen30, danielk, phi}@jhu.edu

###### Abstract

Non-autoregressive Transformers (NATs) are recently applied in direct speech-to-speech translation systems, which convert speech across different languages without intermediate text data. Although NATs generate high-quality outputs and offer faster inference than autoregressive models, they tend to produce incoherent and repetitive results due to complex data distribution (e.g., acoustic and linguistic variations in speech). In this work, we introduce DiffNorm, a diffusion-based normalization strategy that simplifies data distributions for training NAT models. After training with a self-supervised noise estimation objective, DiffNorm constructs normalized target data by denoising synthetically corrupted speech features. Additionally, we propose to regularize NATs with classifier-free guidance, improving model robustness and translation quality by randomly dropping out source information during training. Our strategies result in a notable improvement of about \(+7\) ASR-BLEU for English-Spanish (En-Es) and \(+2\) ASR-BLEU for English-French (En-Fr) translations on the CVSS benchmark, while attaining over \(14\) speedup for En-Es and \(5\) speedup for En-Fr translations compared to autoregressive baselines.1

## 1 Introduction

Speech-to-speech translation (S2ST) systems are essential to bridge communication gaps and have wide application potential. We focus on non-autoregressive modeling for direct _speech-to-speech_ translation, converting source speech to the target without intermediate text data. Such direct S2ST systems  can preserve non-linguistic information, avoid error propagation from cascaded systems  (e.g., a combination of speech recognition and machine translation systems), and achieve faster inference speed.

Non-autoregressive Transformers (NAT)  has played a central role in current S2ST work . NAT translates source waveforms into target **speech units** via parallel decoding, achieving performance comparable to or better than autoregressive models while greatly reducing inference time. This process is often referred to as _speech-to-unit_ (S2UT) translation . The predicted speech units are then converted to target waveforms via a unit vocoder in the _unit-to-speech_ synthesis stage . However, NATs suffer from incoherent and repetitive generations, referred to as the **multi-modality problem**. This issue stems from NAT's assumption of conditional independence during parallel decoding, worsened by the complex and multi-modal nature of training data distribution.

In this work, we propose DiffNorm, a **self-supervised** speech normalization strategy that alleviates the multi-modality problem of NAT models by simplifying the target distribution. Instead of distilling training data from an autoregressive model  or utilizing perturbed speech to train a normalizer , we rely on the denoising objective of Denoising Diffusion Probabilistic Models [15, DDPM] to normalize target speech units. DiffNorm inject synthetic noise to speech features and use diffusionmodel to gradually recover the feature, obtaining a simplified and more consistent data distribution that obscures non-crucial details. As the denoising objective is learned in a self-supervised manner over latent speech representations, it eliminates the necessity for transcription data  or manually crafted perturbation functions . As illustrated in Fig. 1, applying DiffNorm to the target data obtains normalized speech units that lead to better NAT training for speech-to-unit translation.

Besides using DiffNorm as a data-centric strategy to mitigate the multi-modality problem, we also propose a regularization strategy to enhance the NAT model's robustness and generalizability when facing complex data distribution (e.g., linguistic diversity and acoustic variation [21; 20]). Inspired by classifier-free guidance [17; CG], during training, we occasionally drop out source information and replace it with a "null" representation, compelling the models to generate coherent units without conditioning on the source data. During iterative parallel decoding of the NAT model, we obtain higher-quality translation by mixing conditional and unconditional generation. Ultimately, combining DiffNorm and CG results in our top-performing state-of-the-art system, achieving approximately \(+7\) and \(+2\) ASR-BLEU increment for En-Es and En-Fr translation compared to previous non-autoregressive systems on the CVSS  benchmark.

In conclusion, we alleviate the multi-modality problem by proposing (1) diffusion-based normalization and (2) regularization with classifier-free guidance. To the best of our knowledge, we are the first to adapt diffusion models and classifier-free guidance into speech-to-speech translation and NAT modeling. Our methods obtain notable improvement compared to previous systems and maintain fast inference speed inherent in non-autoregressive modeling, achieving speedups of \(14\) for En-Es and \(5\) for En-Fr compared to autoregressive baselines.

## 2 Problem formulation and overview

We aim to develop a direct (textless) speech-to-speech translation system that transduces a source speech \(=(x_{1},,x_{N})\) into target speech. We follow Lee et al.  to reduce speech-to-speech translation into two sub-tasks: speech-to-unit translation and unit-to-speech synthesis. In this work, we focus on speech-to-unit translation and follow prior work [31; 32; 21] to use the same unit-to-speech component for a fair comparison. To generate speech units for the target language, we first extract speech feature \(=(h_{1},,h_{M})^{M H}\), where each feature has \(H\) dimensions. Subsequently, a K-means clustering model is trained on extracted features and used to generate speech units \(=(y_{1},,y_{M})^{M 1}\). Once the source speech \(\) and target speech unit \(\) are prepared, we train sequence-to-sequence models to translate from source speech into target units. In practice, we follow Lee et al.  to use a multilingual-HuBERT (mHuBERT) model to extract \(H=768\) dimensional speech features \(\). Then we use a 1000-cluster K-means model to predict speech units given the encoded features.2 For speech-to-unit translation, we follow Huang et al.  to adopt Conditional Masked Language Modeling [10; CMLM], a kind of non-autoregressive transformer (NAT).

To mitigate NAT models' multi-modality problem, we propose DiffNorm (section SS3), which denoises synthetically corrupted speech features to construct normalized speech units \(_{}\). As shown in Fig. 1, such normalized units are then used to train the S2UT (CMLM) model, which generates better-translated units for the unit vocoder to synthesize target speech.

Figure 1: Overview of our proposed system. We first normalize the target speech units with the denoising process from the latent diffusion model. Then speech-to-unit (S2UT) model is trained to predict normalized units, which are converted into waveform from an off-the-shelf unit-vocoder.

Additionally, we propose to incorporate classifier-free guidance [17, CG], a widely adopted strategy for diffusion-based image generation, as a regularization method to improve the non-autoregressive speech-to-unit system (section \(@sectionsign\)4). As shown in Fig. 3, by forcing NAT models to unmask target speech units without conditioning on source information, the model becomes more robust, generating coherent speech units that result in higher-quality translations. During inference, we follow classifier-free guidance to mix conditional and unconditional generation for the NAT model's iterative decoding, further enhancing its translation quality.

## 3 DiffNorm: denoising diffusion models for speech normalization

Previous normalization strategy  on speech-to-unit translation relies on connectionist temporal classification (CTC) fine-tuning  using the HuBERT  model. For example, Lee et al.  rely on single-speaker data from VoxPopuli  to produce normalized (speaker-invariant) speech units for HuBERT-CTC fine-tuning. On the other hand, Huang et al.  generate acoustic-agnostic units by perturbing rhythm, pitch, and energy information with (manually) pre-defined transformation functions. We propose to normalize speech units using self-supervised denoising objectives from Denoising Diffusion Probabilistic Models [15, DDPM]. We believe DDPM is more suitable for speech normalization because: (1) It only requires monolingual speech data, without the need of transcriptions to create a text-to-unit model as in . (2) It learns to denoise features in a high-dimensional space rather than relying on hand-designed perturbation as in . DiffNorm consists of a variational auto-encoder (VAE) and a diffusion model. Since vanilla VAE  and DDPM  are applied to image generation, we modify them to support token generation and incorporate multitasking objectives. The VAE model reduces the dimension of the speech feature, mapping feature \(\) into lower dimensional latent \(\). The diffusion model (visualized in Fig. 2) is then trained to denoise on the latent representation space, aiming to recover the original latent \(_{0}\) from the standard Gaussian noise \(_{T}\).3 In the subsequent sections, we begin by outlining the architecture of our VAE model (\(@sectionsign\)3.1). Then, we delve into how the diffusion model is trained using the latent variables encoded by the VAE (\(@sectionsign\)3.2).

### Variational auto-encoders for latent speech representation

Since speech features encoded by mHuBERT have a high dimension (\(H=768\)), we compress the feature into lower-dimension latents for the diffusion model. The VAE model consists of an encoder, a decoder, and a language modeling head. Following our problem formulation (\(@sectionsign\)2), we first prepare a sequence of target speech features \(=(h_{1},,h_{M})^{M H}\) and their corresponding speech units \(=(y_{1},,y_{M})\). Our VAE model's encoder will map the feature into lower dimension latent \(=f(;_{ enc})^{M Z}\) where \(Z<H\) is the pre-defined latent dimension. Then VAE model's decoder reconstructs the speech feature \(}=f(;_{ dec})\) and we apply the language modeling head to convert the reconstructed feature into a distribution over speech units' vocabulary \(=f(};_{ lm})^{M V}\). Following prior work , we cluster speech features into \(V=1000\) units. The training objective is a weighted combination of reconstruction loss (\(_{ recon}\)), negative log-likelihood (NLL) loss (\(_{ nll}\)), and a Kullback-Leibler (KL) divergence term resulted from the Gaussian constraint  to regularize the representation space:

\[=_{1}_{ recon}+_{2}_{ nll }+_{3}_{ kl},\] (1)

where the reconstruction loss is defined as \(_{ recon}(},)=||-}||^{2}\), and the NLL loss is computed as the cross-entropy between ground-truth units \(\) and the predicted vocabulary distribution \(\): \(_{ nll}(,)=-_{i=1}^{M}_{i}_{i}\), where \(_{i}\) is the one-hot version of \(y_{i}\). Lastly, \(_{ kl}\) can be solved analytically when we assume Gaussian distribution for both prior and posterior approximation of latent \(\) (more details in  and Appendix B). Though the Gaussian constraint has a small weight \(_{3}\), we show in the ablation study (\(@sectionsign\)6.2) that regularizing latent distribution is critical for the diffusion model. For details on the architecture of our VAE model, we direct readers to Appendix B.

### Diffusion model for denoising latent speech representation

TrainingOnce the VAE model is trained, we encode speech feature as the first step feature \(_{0}=f(;_{ enc})\) for the diffusion model. Diffusion models consists of a (1) forward process that gradually transforms \(_{0}\) into a standard Gaussian distribution, and a (2) reverse process that denoise and recovers the original feature \(_{0}\). Following DDPM , with a pre-defined noise scheduler (let \(_{t}(0,1)\) be the scaling of noise variance, define \(_{t}=1-_{t}\) and denote \(_{t}=_{s=1}^{t}_{t}\) as the noise level for time \(t\)), the forward and reverse process can be written as:

\[q(_{t}|_{0})=(_{t};_{t}}_{ 0},(1-_{t})) p_{}(_{t-1}|_{t})= (_{t-1};_{}(_{t},t),^{2})\] (2)

where mean \(_{}(_{t},t)\) and variance \(^{2}\) are parameterized by trainable models, typically based on U-Net  or Transformer . We follow DDPM to train models using the re-weighted noise estimation objective:

\[_{}(,t)=_{_{0},,t} ||-_{}(_{0},t)||^{2}\] (3)

where the network learns to predict the injected Gaussian noise \(\) from the forward process. Besides noise estimation, we also train the model with auxiliary reconstruction and NLL loss using VAE model's decoder and language modeling head. Our training procedure is summarized in Alg. 1.

```
1:Input: speech feature \(\), speech units \(\).
2:Pre-compute:\(_{t},_{t},_{t},t[1,T]\)
3:while not converged :
4:\(_{0}=f(,_{})\)
5:\(t[1,T],t(0,)\)
6:\(_{t}=_{t}}_{0}+_{t}}\)
7:\(}=_{}(_{t},t)\)
8:\(_{}=\|-}\|^{2}\)
9:\(}_{0}=(_{t}-_{t}}})/ _{t}}\)
10:\(}=f(}_{0};_{})\), \(_{}=\|}-}\|^{2}\)
11:\(=f(};_{})\), \(_{}=(,)\)
12: Take gradient descent step with loss
13:\(=_{1}_{}+_{2}_{ }+_{3}_{}\) ```

**Algorithm 1** Latent Diffusion Model Training

As shown in Alg. 1, we randomly sample the current timestep \(t[1,T]\) and compute corresponding scheduling parameters \(_{t},_{t},_{t}\)4. The training process involves the regular DDPM objective that injects Gaussian noise for the model \(_{}\) to perform noise estimation (Alg. 1, line 6-8). Additionally, we generate the pseudo latent \(}_{0}\) by reversing the noise injection process (line 9), which is then decoded by the VAE into speech features and units for reconstruction loss (line 10) and NLL loss (line 11). Finally, the objective is a weighted sum of noise estimation loss, reconstruction loss, and NLL loss:

\[=_{1}_{}+_{2}_{ {recon}}+_{3}_{}\] (4)

In our analysis (SS6.2), we show that adding NLL and reconstruction loss is indeed helpful in improving the diffusion model's reconstruction quality. Lastly, to parameterize our diffusion model for noise estimation, we modify Diffusion Transformer [37, DiT] to suit our task. For details of our architecture and hyper-parameters, we refer readers to Appendix B.

InferenceFor speech normalization, we choose a start time \(T\) that decides the amount of noise to inject for the diffusion model to recover. Given the start time \(T\), we follow Denoising Diffusion Implicit Models [48, DIDIM] sampler to reverse noised latent \(_{T}\) back to \(_{0}\). Then, our VAE model converts \(_{0}\) back to speech units with its decoder and language modeling head. Our inference procedure is summarized in Alg. 2 and visualized in Fig. 2.

Figure 2: Visualization of our latent diffusion model’s denoising process for speech normalization. The clean latent \(_{0}\) is synthetically noised (into \(_{T}\)) and the reverse diffusion process gradually denoise it to generate normalized speech units.

Note that since our diffusion model is used to normalize speech units as a data preprocessing strategy, inference speed is not a concern. Therefore we use step size \( t=1\) for DDIM sampling throughout our experiments. However, the inference speed could be easily improved by setting a larger step size.

## 4 Classifier-free guidance for non-autoregressive transformer

In SS3, we proposed DiffNorm to normalize speech units and obtained speech-unit pairs \((,_{})\) that benefit NAT training. In this section, we propose to adapt classifier-free guidance  to regularize NAT models (visualized in Fig. 3), further enhancing NAT-based S2UT model's translation quality.

TrainingWe largely adhere to standard CMLM training  but introduce a small dropout probability \(p_{}=0.15\) for the encoder representations. Specifically, we parameterize the encoder and decoder of the CMLM as \(_{}\) and \(_{}\), respectively. The encoder processes the source speech into a representation \(=f(;_{})\). The decoder then predicts the vocabulary distribution \(=f(}|;_{})\) from \(\) and the randomly masked target speech units \(}\). The total amount of masked token is uniformed sampled: \(n[1,M]\); subsequently, \(n\) of \(M\) tokens from the target units \(\) are randomly masked to form noisy target units \(}\). CMLM also trains a length predictor that estimates the output length given input sequences and we refer readers to  for more details.

Using the predicted distribution \(\), we compute the NLL loss against the ground truth units \(\) at the masked positions to train the model. If dropout is applied, the decoder receives a "null" representation \(_{}\)--a randomly-initialized learnable vector--forcing it to rely solely on the target information to unmask units. For more details, we refer readers to Appendix C and previous paper [10; 21].

InferenceCMLM generates tokens through iterative unmasking. Given the input sequence \(\), CMLM first predicts the length of output sequence \(M\) and initialize a sequence of \(M\) masked tokens \(}_{0}=([]_{1},,[]_{M})\). Given the total number of iterations \(T^{}\) and current iteration \(t[1,T]\), CMLM decodes all tokens \(y_{i},i[1,M]\) in parallel using their probabilities:

\[y_{i}^{t}=}(y_{i}=w|,}_{ t-1};_{}) p_{i}^{t}=(y_{i}^{t}|,}_{t-1};_{})\] (5)

From the generated tokens \(}_{t}=(y_{1}^{t},,y_{M}^{t})\), we replace \(k=M\) tokens with the lowest probabilities \(p_{i}^{t}\) with mask tokens. This re-masked sequence \(}_{t}\) is then inputted into the CMLM for further decoding iterations. This process continues until the final step \(t=T\), at which point no tokens are re-masked. With our proposed regularization from classifier-free guidance, we compute two vocabulary distribution in each iteration step \(t\):

\[p_{}^{t}=(|,}_{t-1};_{}) p_{}^{t}=(|_{0},}_{t-1}; _{})\] (6)

where \(p_{}^{t}\) is the same as vanilla CMLM and \(p_{}^{t}\) is the unconditional probability obtained with the "null" representation. Then we select and re-mask tokens using the adjusted probability

\[p^{t}=(p_{}^{t}-p_{}^{t})+p_{}^{t}\] (7)

where the hyper-parameter \(\) is used to control the degree to push away from the unconditional distribution. In the special case of \(=0\), only conditional distribution is used during iterative decoding, resulting in the same generation process as standard CMLM. Through our intensive analysis

Figure 3: Visualization of CMLM for speech-to-unit translation where the model is trained with the unmasking objective to recover \(_{}\). When classifier-free guidance is used, with probability \(p_{}\), we replace the encoded source speech \(\) by a ”null” representation \(_{0}\).

(Appendix F), we determine that a relatively small \(\) (e.g., \(=0.5\)) gives the best result, especially when the number of decoding iterations is large (i.e., \(T>10\)). Notably, even when \(=0\) -- which corresponds to the traditional CMLM decoding method -- CMLMs regularized with classifier-free guidance during training can consistently outperform their standard counterparts. This observation further validates the effectiveness of our proposed regularization.

## 5 Experiments

### Experimental setup

DatasetWe perform experiments using the established CVSS-C datasets , which are created from CoVoST2 by employing advanced _text-to-speech_ models to synthesize translation texts into speech . CVSS-C comprises aligned speech in multiple languages along with their respective transcriptions. Our methods are evaluated on two language pairs: English-Spanish (En-Es) and English-French (En-Fr), with detailed data statistics provided in Table 1. As our focus is on direct speech-to-speech translation, transcriptions are solely utilized for evaluation purposes. Utilizing the speech data from CVSS, we preprocess the target speech to generate speech units using the mHuBERT and K-means model, as described in our problem formulation (SS2).

EvaluationTo evaluate the performance of various speech-to-speech translation systems, we adopt the standard methodology established in previous studies [32; 31; 21] to compute the ASR-BLEU score. Firstly, our speech-to-unit translation system generates speech units based on the input speech, which are then transformed into speech waveforms using a unit-vocoder. The unit-vocoder is built upon the HifiGAN architecture  with customized objectives, detailed in Appendix D. Once we have the waveforms, we employ an ASR model to transcribe them and calculate the BLEU score against the reference transcriptions. This ASR model is fine-tuned based on the Wav2Vec2.0  model with the CTC objective. We direct readers to the original paper  for more details. Both the unit-vocoder and ASR models are sourced from an off-the-shelf repository, ensuring consistency in evaluation methodology with previous studies [32; 21].6

Normalized dataset constructionWe adhere to the procedure outlined in Alg. 2 to generate normalized speech units. By manipulating start time \(T\), we control the level of noise in the latent \(_{T}=_{t}}_{0}+_{t}}\) for the diffusion model, balancing between the reconstruction quality and normalization effect. We explore different levels of noise injection and choose \(T=100\) for En-Es and \(T=120\) for En-Fr (further details in SS6.1). Hence, we adopt these settings to construct our normalized dataset, CVSS-norm.

Compared systemsIn this section, we provide brief descriptions of evaluated speech-to-unit models. For autoregressive models, we evaluate the Transformer model trained following Lee et al. . We also assess the Conformer model trained similar to the Transformer, but with its encoder replaced by a Conformer-Encoder . Lastly, the Norm Transformer shares the same architecture as the Transformer, but it is trained on normalized speech units that are speaker-invariant. The normalized dataset is constructed following the strategy proposed by Lee et al. .

For non-autoregressive systems, we train the Conditional Masked Language Model (CMLM) following Huang et al. , using a Conformer-based encoder and a Transformer decoder. The CMLM + Bilateral Perturbation (BiP) system retains the same architecture as CMLM but is trained on normalized speech units constructed with BiP .

For our improved systems, we train CMLM + DiffNorm, which shares the same architecture as CMLM but is trained on CVSS-norm, the normalized dataset obtained through DiffNorm. Additionally, we train the CMLM + CG model that incorporates classifier-free guidance introduced in SS4. Finally, the CMLM + DiffNorm + CG system uses the architecture of CMLM + CG and is trained on our normalized dataset CVSS-norm.

    &  &  \\   & Size & Length & Size & Length \\  Train & 79,012 & 256 & 207,364 & 228 \\ Valid & 13,212 & 296 & 14,759 & 264 \\ Test & 13,216 & 308 & 14,759 & 283 \\   

Table 1: Data statistics for CVSS benchmarks. Length is the average number of speech units of the target speech.

### Results

Table 2 summarizes the (ASR-BLEU) performances of various S2UT systems. Note that, for non-autoregressive methods, Table 2 shows their results obtained with 15 decoding iterations. For more details on the inference speedup achieved through non-autoregressive modeling, we plot Fig. 4 to provide details on the "quality vs. latency" trade-off. Observations from Table 2 are:

**DiffNorm greatly enhances translation quality compared to systems using original speech units** (model 6 vs. 4; 8 vs. 7). For example, CMLM with DiffNorm achieves about \(+7\) BLEU score on En-Es compared to CMLM trained on original units. **DiffNorm also outperforms previous normalization strategies** (model 6 v.s. 2, 5), validating the superior quality of normalized speech units obtained through our methods. Besides normalization, classifier-free guidance effectively improves speech-to-unit quality as a regularization strategy, leading to better translation quality (model 7 v.s. 4; 8 v.s. 6). Finally, **combining both classifier-free guidance and DiffNorm (model 8) results in the best overall system, outperforming both autoregressive and non-autoregressive baselines.**

Decoding speedIn Fig. 4, we illustrate the "quality-latency" trade-off of various non-autoregressive speech-to-unit systems. Quality is measured using ASR-BLEU, while latency is determined by the relative speedup over the autoregressive system, calculated as "generated units/second". For instance, the first marker on the line plot represents \(15\) decoding iterations, resulting in a speedup of 5.34\(\) compared to the autoregressive baseline. Additionally, we include the performance of the Conformer-based autoregressive model as a horizontal dashed line. We observe that our improved system consistently outperforms the baseline CMLM model  and achieves better performance than the autoregressive model with more than \(14\) speedup for En-Es and \(5\) speedup for En-Fr.

## 6 Analysis

### Effect of synthetic noise injection

In this section, we explore the effects of varying degrees of noise injection on DiffNorm. Recall (from Alg. 2) that synthetic noise is injected into the latent representation as \(_{T}=_{T}}_{0}+_{T}}\). Hence, adjusting the start time \(T\) allows us to control the degree of noise injection.

**Setup and Evaluation** We perturb the degree of noise injection by varying \(T\), and assess the _reconstruction quality_ of normalized speech units and _downstream performance_ of CMLM models trained with different normalized units. For reconstruction quality, we measure the accuracy (**Acc-Rec**) of reconstructed units and ASR-BLEU (**BL-Rec**) of synthesized speech when using original

   _ID_ & _System_ &  &  \\   & & En-Es & En-Fr & Fr-En\({}^{*}\) & Speed & Speedup \\   \\ 
1 & Transformer\({}^{}\) & 10.07 & 15.28 & 15.44 & 870 & 1.00\(\) \\
2 & Norm Transformer\({}^{}\) & 12.98 & 15.93 & 15.81 & 870 & 1.00\(\) \\
3 & Conformer\({}^{}\) & 13.75 & 17.07 & 18.02 & 895 & 1.02\(\) \\   \\ 
4 & CMLM & 12.58 & 15.62 & 16.95 & & & \\
5 & CMLM + BiP\({}^{}\) & 12.62 & 16.97 & 18.03 & 4651 & 5.34\(\) \\   \\ 
6 & CMLM + DiffNorm & 18.96 & 17.27 & **19.53** & & & \\
7 & CMLM + CG\({}^{}\) & 17.06 & 16.89 & – & 4651 & 5.34\(\) \\
8 & CMLM + DiffNorm + CG\({}^{}\) & **19.49** & **17.54** & – & & \\   

Table 2: Comparison of speech-to-speech models evaluated by quality (ASR-BLEU) and speed (units/seconds).\({}^{*}\): Fr-En experiments are added during author response period and we leave model 7,8 for future work. Results with \({}^{}\) are taken from the prior work . \({}^{}\) We use \(w=0.5\) for CG. **Our NAT models achieve superior translation quality while maintaining their fast inference speed**.

units/speech as the reference. For downstream performance, we report the downstream ASR-BLEU of translated speech produced by CMLM models trained with the particular noise setup (**BL-Dn**).7

ResultsShown in Table 5, increasing \(T\) leads to more noise and a decline in reconstruction quality, which aligns with expectations that higher noise levels pose greater challenges for the diffusion model in restoring original features accurately. Interestingly, we find from Table 3 that when \(T\) is too small or large (e.g., \(T=50\) or \(T=150\)), the normalized units do not result in an ideal downstream system. We hypothesize that there is barely any normalization effect when \(T\) is too small; and when \(T\) is too large, the reconstructed units are no longer semantically correct. This phenomenon is visualized in Fig. 5, where we plot the log-mel spectrograms of the reconstructed speech. As more noise is introduced (larger \(T\)), the reconstructed speech becomes smoother (e.g., portions of blank speech are filled in by diffusion), exhibiting a more pronounced deviation from the original speech.

From the Table 3, we observed a substantial enhancement in En-Es translation performance with \(T=50\). Consequently, we conducted additional experiments for En-Es translation with start times \(T=10\) and \(T=30\), detailed in Table 4. The results show that at a minimal start time of \(T=10\), the reconstruction accuracy reaches 93.8%, yielding a downstream ASR-BLEU score of 15.98. This score significantly surpasses the baseline CMLM result of 12.58. The high accuracy indicates that about 6% of tokens vary post-reconstruction, likely due to the VAE model's regularization effect since the diffusion model does not cause notable reconstruction deviations at such a small \(T\). This suggests that the Spanish speech dataset might have considerable acoustic variations and noise, which the VAE model can partially mitigate. As \(T\) increases, the diffusion model further refines the representation, leading to improved downstream results, as evidenced by the rise in ASR-BLEU score from from

Figure 4: Trade-off between quality (ASR-BLEU) and latency for varying numbers of decoding iterations. Five markers correspond to {15, 10, 7, 5, 3} decoding iterations. Decreasing the number of iterations results in a decline in model performance, traded off for faster speedup. With DiffNorm and CG, **our S2UT model achieves a better quality-latency trade-off** than CMLM and outperforms a strong autoregressive baseline with large speedups.

   _Start Step_ & **Acc-Rec** & **BL-Rec** \\  \(T=10\) & 93.8 & 15.98 \\ \(T=30\) & 91.6 & 17.92 \\   

Table 4: Reconstruction and downstream performance with small noise injection.

\(T=10\) to \(T=100\). This ablation study underscores that **the optimal choice of T is affected by the dataset quality**.

### Ablation on training objectives for DiffNorm

DiffNorm requires an auto-encoder to map speech features into lower-dimension latents and then train denoising diffusion models using the encoded latents. In this section, we perform ablation experiments to investigate the effect of (1) latent dimension (2) Gaussian constraints (3) multitask objective for diffusion models. Firstly, to investigate the effect of latent dimension, we train auto-encoders that encode speech features into 16, 32, and 128 dimensions. As shown in Table 5, it comes as no surprise that a larger latent dimension yields superior reconstruction results, evidenced by higher accuracy across systems trained with varying objectives. Next, we turn our attention to the importance of applying Gaussian constraints to the auto-encoder's latent representation space. Our results (comparing with and without **KL**) reveal that incorporating Gaussian constraints is crucial. Latent spaces not regularized by \(_{}\) lead to significantly poorer performance. Finally, we explore the effectiveness of our proposed multitasking objective for the diffusion model by training another vanilla DDPM solely with \(_{}\) (no **Multitask**). We find that the diffusion model's reconstruction capability indeed improves when employing the multitasking objective, particularly when denoising from a more noisy latent representation (e.g., \(T=150\)). Through our ablation analysis, we identify the optimal setup for our DiffNorm model, which involves (1) mapping speech features to 128 dimensions, (2) regularizing latent space by Gaussian constraints, and (3) utilizing the multitask objective for diffusion training.

## 7 Related work

Direct speech-to-speech translation (S2ST)Direct S2ST aims to directly translate speech into another language without cascaded systems that rely on transcriptions or translations. Translatorron  and Translatorron 2  are among the first systems for direct S2ST, which uses sequence-to-sequence model to map source speech into spectrograms. Then, a spectrogram decoder is used to synthesize the target language's speech. Instead of transducing speech into spectrogram, Tjandra et al. , Zhang et al.  utilize Vector-Quantized Variational Auto-Encoder (VQ-VAE)  to discretize target speech and convert _speech-to-speech_ translation into a _speech-to-unit_ task. More recently, Lee et al.  improved the speech-to-unit models by obtaining such units with a k-means clustering model trained on self-supervised representation from HuBERT . To convert units back to speech, Lee et al.  follows Polyak et al.  to train a unit-vocoder based on HifiGAN .

Speech normalizationSpeech representation are typically extracted from pre-trained encoders [19; 3; 40; 5] and can be compressed or adapted in different speech-to-speech/text tasks [61; 63; 50; 52; 51]. Inspired by previous work on speech enhancement [54; 1], Lee et al.  propose to

Figure 5: Visualization of reconstructed speech’s log-mel spectrograms. Noticeable divergence from the original speech is highlighted in the white bounding boxes.

    &  &  \\   & **KL** & **Multitask** & **16** & **32** & **128** \\   & ✗ & ✓ & 51.4 & 69.1 & 85.8 \\  & ✓ & ✗ & 80.9 & 86.4 & 89.3 \\  & ✓ & ✓ & **80.9** & **86.5** & **89.4** \\   & ✗ & ✓ & 13.3 & 32.5 & 73.1 \\  & ✓ & ✗ & 64.5 & 76 & 80.8 \\   & ✓ & ✓ & **65.3** & **76.2** & **81.2** \\   & ✗ & ✓ & 2.9 & 5.3 & 34.6 \\   & ✓ & ✗ & 19.8 & 38.2 & 56.8 \\   & ✓ & ✓ & **20.2** & **40** & **57.8** \\   

Table 5: Accuracy of reconstructed speech units. **KL**: when applied, the latent space is regularized to be Gaussian . **Multitask**: when not applied, the latent diffusion model is trained only with \(_{}\).

normalize speech by synthesizing speaker-invariant waveforms through text-to-speech (TTS) systems . Huang et al.  propose to normalize speech with Bilateral Perturbation that focus on the rhythm, pitch, and energy information. Different from previous normalization methods that requires transcription  or manually designed perturbation , our DiffNorm strategy leverages diffusion models. **Diffusion models** have achieved remarkable generative ability to produce high-quality images  and audio . Using self-supervised denoising objectives , we train effective denoising models capable of normalizing speech for training better speech-to-unit systems.

Non-autoregressive speech-to-speech translationFor sequence-to-sequence modeling, autoregressive  and non-autoregressive  models have been widely explored. Lee et al.  reduce speech-to-speech into speech-to-unit task and follow the widely-used modeling strategy, Transformer , to predict speech units. Later, non-autoregressive models  have been explored and Huang et al.  is the first to use a non-autoregressive transformer model, Conditional Masked Language Model [10, CMLM], for speech-to-unit task. Fang et al. , on the other hand, adopts Directed Acyclic Transformer  for speech-to-unit translation.

## 8 Conclusion

We improve speech-to-unit translation system through (1) corpus distillation by constructing normalized speech units with DiffNorm and (2) regularization with classifier-free guidance. Our improved non-autoregressive (NAR) models, greatly outperform previous NAR models, yielding an increase of approximately \(+7\) and \(+2\) BLEU points for En-Es and En-Fr translation, respectively. Notably, DiffNorm and classifier-free guidance maintain the inference speed advantages inherent in NAR models. Consequently, our approach obtains better performance to autoregressive baselines while achieving over \(14\) speedup for En-Es and \(5\) speedup for En-Fr translations.