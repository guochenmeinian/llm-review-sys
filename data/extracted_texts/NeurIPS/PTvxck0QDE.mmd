# Simplicity Bias in 1-Hidden Layer Neural Networks

Depen Morwani

Department of Computer Science

Harvard University

dmorwani@g.harvard.edu

&Jatin Batra

School of Technology and Computer Science

Tata Institute of Fundamental Research (TIFR)

jatin.batra@tifr.res.in

Prateek Jain\({}^{1}\)

Google Research

prajain@google.com

&Praneeth Netrapalli\({}^{1}\)

Google Research

pnetrapalli@google.com

part of the work done while at Google Research, India \({}^{1}\)Alphabetical Ordering

Image source: Wikipedia swa, bea.

###### Abstract

Recent works (Shah et al., 2020; Chen et al., 2021) have demonstrated that neural networks exhibit extreme _simplicity bias_ (SB). That is, they learn _only the simplest_ features to solve a task at hand, even in the presence of other, more robust but more complex features. Due to the lack of a general and rigorous definition of _features_, these works showcase SB on _semi-synthetic_ datasets such as Color-MNIST, MNIST-CIFAR where defining features is relatively easier.

In this work, we rigorously define as well as thoroughly establish SB for _one hidden layer_ neural networks. More concretely, (i) we define SB as the network essentially being a function of a low dimensional projection of the inputs (ii) theoretically, in the infinite width regime, we show that when the data is linearly separable, the network primarily depends on only the linearly separable (\(1\)-dimensional) subspace even in the presence of an arbitrarily large number of other, more complex features which could have led to a significantly more robust classifier, (iii) empirically, we show that models trained on _real_ datasets such as Imagenet and Waterbirds-Landbirds indeed depend on a low dimensional projection of the inputs, thereby demonstrating SB on these datasets, iv) finally, we present a natural ensemble approach that encourages diversity in models by training successive models on features not used by earlier models, and demonstrate that it yields models that are significantly more robust to Gaussian noise.

## 1 Introduction

It is well known that neural networks (NNs) are vulnerable to distribution shifts as well as to adversarial examples (Szegedy et al., 2014; Hendrycks et al., 2021). A recent line of work (Geirhos et al., 2018; Shah et al., 2020; Geirhos et al., 2020) proposes that _Simplicity Bias (SB)_ (or shortcut learning) i.e., the tendency of neural networks (NNs) to learn only the simplest features over other useful but more complex features, is a key reason behind non-robustness of the trained networks.

The argument is roughly as follows: for example, in the classification of swans vs bears, as illustrated in Figure 1, there are many features such as background, color of the animal, shape of the animal etc. that can be used for classification. However using only one or few of them can lead to models that are not robust to specific distribution shifts, while using all the features can lead to more robust models.

Several recent works have demonstrated SB on a variety of _semi-real constructed datasets_(Geirhos et al., 2018; Shah et al., 2020; Chen et al., 2021), and have hypothesized SB to be the key reasonfor NN's brittleness to distribution shifts (Shah et al., 2020). However, such observations are still only for specific semi-real datasets, and a general method that can identify SB on a _given dataset_ and a _given model_ is still missing in literature. Such a method would be useful not only to estimate the robustness of a model but could also help in designing more robust models.

A key challenge in designing such a general method to identify (and potentially fix) SB is that the notion of _feature_ itself is vague and lacks a rigorous definition. Existing works Geirhos et al. (2018); Shah et al. (2020); Chen et al. (2021) avoid this challenge of vague feature definition by using carefully designed datasets (e.g., concatenation of MNIST images and CIFAR images), where certain high level features (e.g., MNIST features and CIFAR features, shape and texture features) are already baked in the dataset definition, and arguing about their _simplicity_ is intuitively easy.

**Contributions**: Our first contribution is to provide a precise definition of a particular simplicity bias - LD-SB- referring to _low dimensional input dependence_ of the model.

**Definition 1.1** (Ld-Sb).: A model \(f:^{d}^{c}\) with inputs \(x^{d}\) and outputs \(f(x)^{c}\) (e.g., logits for \(c\) classes), trained on a distribution \((x,y)\) satisfies LD-SB if there exists a _projection_ matrix \(P^{d d}\) satisfying:

* \((P)=k d\),
* \([(f(Px^{(1)}+P_{}x^{(2)}))=(f(x^{(1)}))]  1-_{1}\) for \((x^{(1)},y^{(1)})\), \((x^{(2)},y^{(2)})\), where \((f(x))\) represents the predicted label for \(x\),
* An independent model \(g\) trained on \((P_{}x,y)\) where \((x,y)\) satisfies \(|(g)-(f)|_{2}\),

for some small \(_{1}\) and \(_{2}\). Here \(P_{}\) is the projection matrix onto the subspace orthogonal to \(P\), and \((f)\) represents the accuracy of \(f\).

In words, LD-SB says that there exists a small \(k\)-dimensional subspace (given by the projection matrix \(P\)) in the input space \(^{d}\), which is the only thing that the model \(f\) considers in labeling any input point \(x\). In particular, if we _mix_ two data points \(x_{1}\) and \(x_{2}\) by using the projection of \(x_{1}\) onto \(P\) and the projection of \(x_{2}\) onto the orthogonal subspace \(P_{}\), the output of \(f\) on this _mixed point_\(Px_{1}+P_{}x_{2}\) is the same as that on \(x_{1}\). This would have been fine if the subspace \(P_{}\) does not contain any feature useful for classification. However, the third bullet point says that \(P_{}\) indeed contains features that are useful for classification since an independent model \(g\) trained on \((P_{}x,y)\) achieves high accuracy.

Theoretically, we demonstrate LD-SB of \(1\)_-hidden layer NNs in the infinite width limit_ for a fairly general class of distributions called _independent features model (IFM)_, where the features (i.e., coordinates) are distributed independently conditioned on the label. IFM has a long history and is widely studied, especially in the context of naive-Bayes classifiers Lewis (1998). For IFM, we show that as long as there is even a _single_ feature in which the data is linearly separable, NNs trained using SGD will learn models that rely almost exclusively on this linearly separable feature, even when there are an _arbitrarily large number_ of features in which the data is separable but with a _non-linear_ boundary. Empirically, we demonstrate LD-SB on three real world datasets: binary and multiclass version of Imagenette (FastAI, 2021), waterbirds-landbirds (Sagawa et al., 2020) as well as the ImageNet (Deng et al., 2009) dataset. Compared to the results in Shah et al. (2020), our results (i) theoretically show LD-SB in a fairly general setting and (ii) empirically show LD-SB on _real_ datasets.

Finally, building upon these insights, we propose a simple ensemble method - _OrthoP_ - that sequentially constructs NNs by projecting out the input data directions that are used by previous NNs. We demonstrate that this method can lead to significantly more robust ensembles for real-world datasets in presence of simple distribution shifts like Gaussian noise.

**Why study \(1\)-hidden layer networks in the infinite width regime?**

Figure 1: Classification of swans vs bears. There are several features such as background, color of the animal, shape of the animal etc., each of which is sufficient for classification but using all of them will lead to a more robust model. 2

1. From a **practical** standpoint, the dominant paradigm in machine learning right now is to pretrain large models on large amounts of data and then finetune on small target datasets. Given the large and diverse pretraining data seen by these models, it has been observed that they do learn rich features (Rosenfeld et al., 2022; Nasey et al., 2022). However, finetuning on target datasets might not utilize all the features in the pretrained model. Consequently, approaches that can train robust finetuning heads (such as a \(1\)-hidden layer network on top) can be quite effective.
2. From a **theoretical** standpoint, there have been several works that analyze training dynamics of finite width networks (Ding et al., 2022), and show convergence to global minima on the training data. However, these results do not identify _which_ among the _many_ global minima, the training dynamics converge to, which is crucial in determining the nature of SB of the converged model. Such a precise characterization of the final convergence point is known only for infinite width \(1\)-hidden layer networks (Chizat et al., 2019; Chizat & Bach, 2020).
3. While our theoretical analysis works in the setting of infinite width networks, our extensive experiments on several large scale datasets suggest that the results continue to hold even for finite width networks. Furthermore, Vyas et al. (2023) show that the behavior of neural networks remains consistent with width in the feature learning regime.

To summarize, this paper characterizes the nature of SB in \(1\)-hidden layer networks, and also proposes a novel ensemble training approach, called OrthoP, which leads to more robust ensembles. While the theoretical results are in the infinite width regime, empirical results on several real world datasets show that these results continue to hold even for finite width networks.

**Paper organization**: This paper is organized as follows. Section 2 presents related work. Section 3 presents preliminaries. Our main results on LD-SB are presented in Section 4. Section 5 presents results on training diverse classifiers. We conclude in Section 6.

## 2 Related Work

**Simplicity Bias**: Subsequent to Shah et al. (2020), there have been several papers investigating the presence/absence of SB in various networks as well as reasons behind SB (Scimeca et al., 2021). Of these, Huh et al. (2021) is the most closely related work to ours. Huh et al. (2021)_empirically observe_ that on certain _synthetic_ datasets, the _embeddings_ of NNs both at initialization as well as after training have a low rank structure. In contrast, we prove LD-SB _theoretically_ on the IFM model as well as empirically validate this on _real_ datasets. Furthermore, our results show that while the _network weights_ exhibit low rank structure in the rich regime (see Section 3.2 for definition), the manifestation of LD-SB is far more subtle in lazy regime. Moreover, we also show how to use LD-SB to train a second diverse model and combine it to obtain a robust ensemble. Galanti & Poggio (2022) provide a theoretical intuition behind the relation between various hyperparameters (such as learning rate, batch size etc.) and rank of learnt weight matrices, and demonstrate it empirically. Pezeshki et al. (2021) propose that _gradient starvation_ at the beginning of training is a potential reason for SB in the lazy/NTK regime but the conditions are hard to interpret. In contrast, our results are shown for any dataset in the IFM model in the _rich_ regime of training. Finally, Lyu et al. (2021) consider anti-symmetric datasets and show that single hidden layer input homogeneous networks (i.e., without _bias_ parameters) converge to linear classifiers. However, our results hold for general datasets and do not require input homogeneity.

**Learning diverse classifiers**: There have been several works that attempt to learn diverse classifiers. Most works try to learn such models by ensuring that the input gradients of these models do not align (Ross & Doshi-Velez, 2018; Teney et al., 2022). Xu et al. (2022) propose a way to learn diverse/orthogonal classifiers under the assumption that a complete classifier, that uses all features is available, and demonstrates its utility for various downstream tasks such as style transfer. Lee et al. (2022) learn diverse classifiers by enforcing diversity on unlabeled target data.

**Spurious correlations**: There has been a large body of work which identifies reasons for spurious correlations in NNs (Sagawa et al., 2020) as well as proposing algorithmic fixes in different settings (Liu et al., 2021; Chen et al., 2020). Simplicity bias seems to be one of the primary reasons behind learning spurious correlations within NNs (Shah et al., 2020).

**Implicit bias of gradient descent**: There is also a large body of work understanding the implicit bias of gradient descent dynamics. Most of these works are for standard linear (Ji & Telgarsky, 2019) or deep linear networks (Soudry et al., 2018; Gunasekar et al., 2018). For nonlinear neural networks,one of the well-known results is for the case of \(1\)-hidden layer neural networks with homogeneous activation functions (Chizat & Bach, 2020), which we crucially use in our proofs. More related works are provided in Appendix.

## 3 Preliminaries

In this section, we provide the notation and background on infinite width max-margin classifiers that is required to interpret the results of this paper.

### Basic notions

**1-hidden layer neural networks and loss function.** Consider instances \(x^{d}\) and labels \(y\{ 1\}\) jointly distributed as \(\). A 1-hidden layer neural network model (or fully connected network (FCN)) for predicting the label for a given instance \(x\), is defined by parameters \((^{m d},^{m}, ^{m})\). For a fixed activation function \(\), given input instance \(x\), the model is given as \(f((,,),x),(x+)\), where \(()\) is applied elementwise. The cross entropy loss \(\) for a given model \(f\), input \(x\) and label \(y\) is given as \((f(x),y)}}{{=}}(1 +(-yf((,,),x)))\).

**Margin.** For data distribution \(\), the margin of a model \(f(x)\) is given as \(_{(x,y)}yf(x)\).

**Notation.** Here is some useful notation that we will use repeatedly. For a matrix \(A\), \(A(i,.)\) denotes the \(i\)th row of \(A\). For any \(k\), \(^{k-1}\) denotes the surface of the unit norm Euclidean sphere in dimension \(k\).

### Initializations

The gradient descent dynamics of the network depends strongly on the scale of initialization. In this work, we primarily consider _rich regime_ initialization.

**Rich regime.** In rich regime initialization, for any \(i[m]\), the parameters \(((i,.),(i))\) of the first layer are sampled from a uniform distribution on \(^{d}\). Each \((i)\) is sampled from _Unif\(\{-1,1\}\)_, and the output of the network is scaled down by \(\) (Chizat & Bach, 2020). This is roughly equivalent in scale to Xavier initialization Glorot & Bengio (2010), where the weight parameters in both the layers are initialized approximately as \((0,)\) when \(m d\).

In addition, we also present some results for the lazy regime initialization described below.

**Lazy regime.** In the lazy regime, the weight parameters in the first layer are initialized with \((0,)\), those of second layer are initialized with \((0,)\) and the biases are initialized to \(0\) (Bietti & Mairal, 2019; Lee et al., 2019). This is approximately equivalent in scale to Kaiming initialization (He et al., 2015).

### Infinite Width Case

For 1-hidden layer neural networks with ReLU activation in the infinite width limit i.e., as \(m\), Jacot et al. (2018); Chizat et al. (2019); Chizat & Bach (2020) gave interesting characterizations of the trained model. As mentioned above, the training process of these models falls into one of two regimes depending on the scale of initialization (Chizat et al., 2019):

**Rich regime.** In the infinite width limit, the neural network parameters can be thought of as a distribution \(\) over triples \((w,b,a)^{d+1}\) where \(w^{d},b,a\). Under the rich regime initialization, the function \(f\) computed by the model can be expressed as

\[f(,x)=_{(w,b,a)}[a(( w,x+b)]\,.\] (1)

Chizat & Bach (2020) showed that the training process with rich initialization can be thought of as gradient flow on the Wasserstein-2 space and gave the following characterization 3 of the trained model under the cross entropy loss \(_{(x,y)}[(,(x,y))]\).

**Theorem 3.1**.: _(Informal)(Chizat & Bach, 2020) Under rich initialization in the infinite width limit with cross entropy loss, if gradient flow on 1-hidden layer NN with ReLU activation converges, it converges to a maximum margin classifier \(^{*}\) given as_

\[^{*}=*{arg\,max}_{(^{d+1})}_{(x, y)}yf(,x)\,,\] (2)

_where \((^{d+1})\) denotes the space of distributions over \(^{d+1}\)._

This training regime is known as the 'rich' regime since it learns data dependent features \( w,\).

**Lazy regime.** Jacot et al. (2018) showed that in the infinite width limit, the neural network behaves like a kernel machine. This kernel is popularly known as the Neural Tangent Kernel(NTK), and is given by \(K(x,x^{})=,)}{ W}\), where \(W\) denotes the set of all trainable weight parameters. This initialization regime is called 'lazy' regime since the weights do not change much from initialization, and the NTK remains almost constant, i.e, the network does not learn data dependent features. We will use the following characterization of the NTK for 1-hidden layer neural networks.

**Theorem 3.2**.: _Bietti & Mairal (2019) Under lazy regime initialization in the infinite width limit, the NTK for 1-hidden layer neural networks with ReLU activation i.e., \((u)=(u,0)\), is given as_

\[K(x,x^{})=\|x\|\|x^{}\|( }{\|x\|\|x^{}\|})\,,(u)=(2u(-cos^{-1}(u))+})\,.\]

_Lazy regime for binary classification._ Soudry et al. (2018) showed that for linearly separable datasets, gradient descent for linear predictors on logistic loss converges to the max-margin support vector machine (SVM) classifier. This implies that, any sufficiently wide neural network, when trained for a finite time in the lazy regime on a dataset that is separable by the finite-width induced NTK, will tend towards the \(_{2}\) max-margin-classifier given by

\[*{arg\,min}_{f}\|f\|_{}yf(x) 1\ \ (x,y)\,,\] (3)

where \(\) represents the Reproducing Kernel Hilbert Space (RKHS) associated with the finite width kernel (Chizat, 2020). With increasing width, this kernel tends towards the infinite-width NTK (which is universal (Ji et al., 2020)). Therefore, in lazy regime, we will focus on the \(_{2}\) max-margin-classifier induced by the infinite-width NTK.

## 4 Characterization of SB in \(1\)-hidden layer neural networks

In this section, we first theoretically characterize the SB exhibited by gradient descent on linearly separable datasets in the _independent features model (IFM)_. The main result, stated in Theorem 4.1, is that for binary classification of inputs in \(^{d}\), even if there is a _single_ coordinate in which the data is linearly separable, gradient descent dynamics will learn a model that relies _solely_ on this coordinate, even when there are an arbitrarily large number \(d-1\) of coordinates in which the data is separable, but by a non-linear classifier. In other words, the simplicity bias of these networks is characterized by _low dimensional input dependence_, which we denote by LD-SB. We then experimentally verify that NNs trained on some real datasets do indeed satisfy LD-SB.

### Dataset

We consider datasets in the independent features model (IFM), where the joint distribution over \((x,y)\) satisfies \(p(x,y)=r(y)_{i=1}^{d}q_{i}(x_{i}|y)\), i.e, the features are distributed independently conditioned on the label \(y\) Here \(r(y)\) is a distribution over \(\{-1,+1\}\) and \(q_{i}(x_{i}|y)\) denotes the conditional distribution of \(i^{}\)-coordinate \(x_{i}\) given \(y\). IFM is widely studied in literature, particularly in the context of naive-Bayes classifiers Lewis (1998). We make the following assumptions which posit that there are at least two features of differing complexity for classification: _one_ with a linear boundary and _at least_ one other with a non-linear boundary. See Figure 2 for an illustrative example.

* One of the coordinates (say, the \(1^{}\) coordinate WLOG) is separable by a linear decision boundary 4 with margin \(\) (see Figure 2), i.e, \(>0\), such that \( Supp(q_{1}(x_{1}|y=+1))[,)\) and \(- Supp(q_{1}(x_{1}|y=-1))(-,-]\), where \(Supp()\) denotes the support of a distribution. * None of the other coordinates is linearly separable. More precisely, for all the other coordinates \(i[d]\{1\}\), \(0 Supp(q_{i}(x_{i}|y=-1))\) and \(\{-1,+1\} Supp(q_{i}(x_{i}|y=+1))\).
* The dataset can be perfectly classified even without using the linear coordinate. This means, \( i 1\), such that \(q_{i}(x_{i}|y)\) has disjoint support for \(y=+1\) and \(y=-1\).

Though we assume axis aligned features, our results also hold for any rotation of the dataset. While our results hold in the general IFM setting, in comparison, current results for SB e.g., Shah et al. (2020), are obtained for _very specialized_ datasets within IFM5, and do not apply to IFM in general.

### Main result

Our main result states that, for rich initialization (Section 3.2), NNs demonstrate LD-SB for any IFM dataset satisfying the above conditions. Its proof appears in Appendix A.1.

**Theorem 4.1**.: _For any dataset in the IFM model with bounded density and bounded support, satisfying the above conditions and \( 1\), and for \(1\)-hidden layer networks with ReLU activation in the infinite width limit (i.e., Eqn. (1)), there is a unique max margin classifier \(^{*}\) (i.e., satisfying Eqn. (2)). This \(^{*}\) is given by: \(^{*}=0.5_{_{1}}+0.5_{_{2}}\) on \(^{d+1}\), where \(\) represents the dirac-delta distribution, \(_{1}=()}}_{1},)}},1/),_{2}=(-)}}_{1},)}},-1/)\) and \(_{1}}}{{=}}[1,0,,0]\) denotes first standard basis vector. This implies \(f(^{*},Px^{(1)}+P_{}x^{(2)})=f(^{*},x^{(1)})\)\(\)\((x^{(1)},y^{(1)}),(x^{(2)},y^{(2)})\), where \(P\) represents the (rank-1) projection matrix on first coordinate._

Together with Theorem 3.1, this implies that if gradient flow converges, it converges to \(^{*}\) given above. Since \(P\) is a rank-\(1\) matrix and \(f(^{*},Px^{(1)}+P_{}x^{(2)})=f(^{*},x^{(1)})\), \(^{*}\) satisfies the first two conditions of LD-SB (Definition 1.1) with \(k=1\) and \(_{1}=0\). Moreover, since at least one of the coordinates \(\{2,,d\}\) has disjoint support for \(q_{i}(x_{i}|y=+1)\) and \(q_{i}(x_{i}|y=-1)\), \(P_{}(x)\) can still perfectly classify the given dataset, thereby implying the third condition of LD-SB with \(_{2}=0\).

It is well known that the rich regime is more relevant for the practical performance of NNs since it allows for feature learning, while lazy regime does not (Chizat et al., 2019). Nevertheless, in the next section, we present theoretical evidence that LD-SB holds even in the lazy regime, by considering a much more specialized dataset within IFM.

### Lazy regime

In this regime, we will work with the following dataset within the IFM family:

For \(y\{ 1\}\) we generate \((x,y) D\) as

\[_{1}= y,\  i 2,..,d,_{i}=\{  1& y=1\\ 0& y=-1.\]

Figure 2: Illustration of an IFM dataset. Given a class \( 1\) represented by blue and red respectively, each coordinate value is drawn independently from the corresponding distribution. Shown above are the supports of distributions on three different coordinates for an illustrative IFM dataset, for positive and negative labels.

Although the dataset above is a point mass dataset, it still exhibits an important characteristic in common with the rich regime dataset - only one of the coordinates is linearly separable while others are not. For this dataset, we provide the characterization of max-margin NTK (as in Eqn. (3)):

**Theorem 4.2**.: _There exists \(_{0}>0\) such that for every \(<_{0}\), there exists an absolute constant \(N\) such that for all \(d>N\) and \([7,)\), the \(_{2}\) max-margin classifier for joint training of both the layers of 1-hidden layer FCN with ReLU activation in the NTK regime on the dataset \(D\), i.e., any \(f\) satisfying Eqn. (3) satisfies:_

\[(f(Px^{(1)}+P_{}x^{(2)}))=(f(x^{(1)})) \;(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}) D\]

_where \(P\) represents the projection matrix on the first coordinate and \((f(x))\) represents the predicted label by the model \(f\) on \(x\)._

The proof of this theorem is presented in Appendix A.2. The above theorem shows that the prediction on a _mixed_ example \(Px^{(1)}+P_{}x^{(2)}\) is the same as that on \(x^{(1)}\) (i.e., \(_{1}=0\) in Definition 1.1). Furthermore, since there exists at least one coordinate \(i 1\) which can be used to perfectly classify the dataset, we have that Definition 1.1 is satisfied with \(_{2}=0\), thus establishing LD-SB.

### Empirical verification

In this section, we will present empirical results demonstrating LD-SB on \(4\) real datasets: Imagenette (FastAI, 2021), a binary version of Imagenette (b-Imagenette), waterbirds-landbirds (Sagawa et al., 2020) and Imagenet (Deng et al., 2009) as well as one designed dataset MNIST-CIFAR (Shah et al., 2020). More details about the datasets can be found in Appendix B.1.

#### 4.4.1 Experimental setup

We take Imagenet pretrained Resnet-50 models, with \(2048\) features, for feature extraction and train a 1-hidden layer fully connected network, with ReLU nonlinearity. During finetuning, we freeze the backbone Resnet-50 model and train only the \(1\)-hidden layer head (details in Appendix B.1).

**Demonstrating LD-SB**: Given a model \(f()\), we establish its low dimensional SB by identifying a small dimensional subspace, identified by its projection matrix \(P\), such that if we _mix_ inputs \(x_{1}\) and \(x_{2}\) as \(Px_{1}+P_{}x_{2}\), the model's output on the mixed input \(^{}\)\(Px_{1}+P_{}x_{2}\), \(f()\) is always _close_ to the model's output on \(x_{1}\) i.e., \(f(x_{1})\). We measure _closeness_ in four metrics: (1) \(P_{}\) logit change (\(P_{}\)-LC): relative change of logits wrt \(x_{1}\) i.e., \(\|f()-f(x_{1})\|\|f(x_{1})\|\), (2)\(P\) logit change (\(P\)-LC): relative change wrt logits of \(x_{2}\) i.e., \(\|f()-f(x_{2})\|\|f(x_{2})\|\), (3) \(P_{}\)-prediction change (\(P_{}\)-pC): \([(f())(f(x_{1}))]\), and (4) \(P\)-prediction change (\(P\)-pC): \([(f())(f(x_{2}))]\). The quantities \(\,(P)\) and \(P_{}\)-pC correspond to \(k\) and \(_{1}\) in Definition 1.1 respectively. To demonstrate that the subspace \(P_{}\) has features that are useful for prediction, we also train a new model \(f_{}\) as follows. Given the initial model \(f\) and the corresponding projection matrix \(P\), we then train another model \(f_{}\) by projecting the input through \(P_{}\) i.e., instead of using dataset \((x^{(i)},y^{(i)})\) for training, we use \((P_{}x^{(i)},y^{(i)})\) for training the second model (denoted by \(f_{}\)). We refer to this training procedure as \(OrthoP\) for _orthogonal projection_. The quantity \(|(f)-(f_{})|\) corresponds to \(_{2}\) in Definition 1.1. We now describe how we identify \(P\) in rich and lazy regimes.

  Dataset & rank \((P)\) & \(P_{}\)-LC \(()\) & \(P\)-LC \(()\) & \(P_{}\)-pC \(()\) & \(P\)-pC \(()\) \\   b-Imagenette & \(1\) & \(28.57 0.26\) & \(92.13 0.24\) & \(6.35 0.06\) & \(47.02 0.24\) \\ Imagenette & \(10\) & \(33.64 1.21\) & \(106.29 0.53\) & \(12.04 0.29\) & \(89.88 0.08\) \\ Waterbirds & \(3\) & \(25.24 1.03\) & \(102.35 0.19\) & \(6.78 0.15\) & \(35.96 0.02\) \\ MNIST-CIFAR & 1 & \(38.97 0.76\) & \(101.98 0.31\) & \(5.41 0.55\) & \(45.15 0.44\) \\ Imagenet & 150 & \(15.78 0.05\) & \(132.05 0.06\) & \(13.05 0.03\) & \(99.76 0.01\) \\  

Table 1: Demonstration of LD-SB in the rich regime: This table presents \(P_{}\) and \(P\) logit as well as prediction changes on the five datasets. These results confirm that projection of input \(x\) onto the subspace spanned by \(P\) essentially determines the modelâ€™s prediction on \(x\). \(\) (resp. \(\)) indicates that LD-SB implies a large (resp. small) value.

#### 4.4.2 Rich regime

Theorem 4.1 suggests that asymptotically, the first layer weight matrix will be low rank. However, since we train only for a finite amount of time, the weight matrix will only be approximately low rank. To quantify this, we use the notion of effective rank Roy and Vetterli (2007).

**Definition 4.3**.: Given a matrix \(M\), its effective rank is defined as \(e^{-_{i}(M)^{2}}(M)^{2}}}\) where \(_{i}(M)\) denotes the \(i^{}\) singular value of \(M\) and \((M)^{2}}}}{{=}}(M)^{2}}{_{i}_{i}(M)^{2}}\).

One way to interpret the effective rank is that it is the exponential of von-Neumann entropy Petz (2001) of the matrix \(}{(MM^{})}\), where \(()\) denotes the trace of a matrix. For illustration, the effective rank of a projection matrix onto \(k\) dimensions equals \(k\).

Figure (a)a shows the evolution of the effective rank through training on the four datasets. We observe that the effective rank of the weight matrix decreases drastically towards the end of training. In this case, we set \(P\) to be the subspace spanned by the top singular directions of the first layer weight matrix. Table 1 presents the results for \(P_{}\) and \(P\)-LC as well as pC, while Table 3 presents \((f)\) and \((f_{})\). These results establish LD-SB in the rich regime.

  Dataset & Acc(\(f\)) & Acc(\(f_{}\)) \\   b-Imagenette & \(93.35\) & \(91.35 0.32\) \\ Imagenette & \(79.67\) & \(71.93 0.12\) \\ Waterbirds & \(90.29\) & \(89.92 0.08\) \\ MNIST-CIFAR & \(99.69\) & \(98.95 0.02\) \\ Imagenet & \(72.02\) & \(69.63 0.08\) \\   
  Dataset & Acc(\(f\)) & Acc(\(f_{}\)) \\   b-Imagenette & \(93.09\) & \(91.77 0.34\) \\ Imagenette & \(80.31\) & \(77.34 0.21\) \\ Waterbirds & \(90.4\) & \(89.5 0.18\) \\ MNIST-CIFAR & \(99.74\) & \(98.54 0.00\) \\ Imagenet & \(72.6\) & \(72.07 0.08\) \\  

Table 3: Accuracy of \(f_{}\) in rich regime

#### 4.4.3 Lazy regime

For the lazy regime, it turns out that the rank of first layer weight matrix remains high throughout training, as shown in Figure 2(b). However, we are able to find a low dimensional projection matrix \(P\) satisfying the conditions of LD-SB (as stated in Def 1.1) as the solution to an optimization problem. More concretely, given a pretrained model \(f\) and a rank \(r\), we obtain a _projection matrix_\(P\) solving:

\[_{P}_{i=1}^{n}((f(Px^{(i)}),y^{(i)} )+(f(P^{}x^{(i)}),[L]))\]

where \([L]\) represents a uniform distribution over all the \(L\) labels, \((x^{(1)},y^{(1)}),,(x^{(n)},y^{(n)})\) are training examples and \((,)\) is the cross entropy loss. We reiterate that the optimization is only over \(P\), while the model parameters \(f\) are unchanged. In words, the above function ensures that the neural network produces correct predictions along \(P\) and uninformative predictions along \(P_{}\). Table 2 presents the results for \(P_{}\) and \(P\)-LC as well as pC, while Table 4 presents \((f)\) and \((f_{})\). These results again establish LD-SB in the lazy regime.

## 5 Training diverse classifiers using _OrthoP_

Our results above motivate a natural strategy to construct diverse ensembles i.e., use \(f\) and \(f_{}\) instead of two independently trained models. In this section, we provide two natural diversity metrics and empirically demonstrate that \(OrthoP\) leads to diverse models in practice. We also demonstrate that an ensemble of \(f\) and \(f_{}\) has higher robustness to Gaussian noise compared to an ensemble of independently trained models.

**Diversity Metrics**: Given any two models \(f\) and \(\), we empirically evaluate their diversity using two metrics. The first is mistake diversity: \((f,)}{=}1-^{(i)}) y^{(i)}\&(^{(i)}) y^{(i)}\}|}{ (|\{i:f(^{(i)}) y^{(i)}\}|,|\{i:f(^{(i)}) y^{( i)}\}|},\) where we abuse notation by using \(f(x_{i})\) (resp. \((x_{i})\)) to denote the class predicted by \(f\) (resp \(\)) on \(x_{i}\). Higher \((f,)\) means that there is very little overlap in the mistakes of \(f\) and \(\). The second is class conditioned logit correlation i.e., correlation between outputs of \(f\) and \(\), conditioned on the class. More concretely, \((f,)=} ([f(_{i})],[(_{i})]:y_{i}=y )}{||}\), where \(([f(_{i})],[(_{i})]:y_{i}=y)\) represents the empirical correlation between the logits of \(f\) and \(\) on the data points where the true label is \(y\). Table 5 compares the diversity of two independently trained models (\(f\) and \(f_{}\)) with that of two sequentially trained models (\(f\) and \(f_{}\)). The results demonstrate that \(f\) and \(f_{}\) are more diverse compared to \(f\) and \(f_{}\). We have also compared \(OrthoP\) to another diverse training method Evading-SB (Teney et al., 2022). The results are provided in Table 7 in the Appendix. As can be seen, our results are either better or comparable to Evading-SB.

**Ensembling**: Figure 4 shows the variation of test accuracy with the strength of gaussian noise added to the pretrained representations of the dataset. Here, an ensemble is obtained by weighted averaging of the logits of multiple models, trained either independently (\(f_{}\)) or using \(OrthoP\) (\(f_{}\)). Moreover, we also compare our method to another diversity training method (\(f_{}\)) termed as

  Dataset &  \(\) \\ \((f,f_{})\) (\(\)) \\  &  \(\) \\ \((f,f_{})\) (\(\)) \\  &  \(\) \\ \((f,f_{})\) (\(\)) \\  & 
 \(\) \\ \((f,f_{})\) (\(\)) \\  \\   B-Imagenette & \(3.87 1.54\) & \(21.15 1.57\) & \(99.88 0.01\) & \(90.86 1.08\) \\ Imagenette & \(6.6 0.46\) & \(11.44 0.65\) & \(99.31 0.12\) & \(91 0.59\) \\ Waterbirds & \(2.9 0.52\) & \(14.53 0.48\) & \(99.66 0.04\) & \(93.81 0.48\) \\ MNIST-CIFAR & \(0.0 0.0\) & \(5.56 7.89\) & \(99.76 0.17\) & \(78.74 2.28\) \\ Imagenet & \(6.97 0.06\) & \(12.31 0.16\) & \(99.5 0.0\) & \(92.52 0.01\) \\  

Table 5: Mistake diversity and class conditioned logit correlation of models trained independently (Mist-Div \((f,f_{})\) and CC-LogitCorr \((f,f_{})\) resp.) vs trained sequentially after projecting out important features of the first model (Mist-Div \((f,f_{})\) and CC-LogitCorr \((f,f_{})\) resp.). The results demonstrate that \(f\) and \(f_{}\) are more diverse compared to \(f\) and \(f_{}\).

Evading-SB (Teney et al., 2022). We can see that, an ensemble of \(f\) and \(f_{}\) is much more robust as compared to an ensemble of \(f\) and \(f_{}\), and generally comparable to and ensemble of \(f\) and \(f_{}\).

## 6 Conclusion: Summary, Limitations and Future Directions

In this work, we propose a rigorous definition of simplicity bias, which is believed to be a key reason for their brittleness (Shah et al., 2020). In particular, we prove that \(1\)-hidden layer networks suffer from low dimensional input dependence (LD-SB), and empirically verify this phenomenon on several real world datasets. We also propose a novel approach - OrthoP- to train diverse models, and demonstrate that an ensemble consisting of such diverse models is more robust to Gaussian noise. Extending these insights to deeper models or in the finite width setting are interesting directions for future work.

## 7 Acknowledgements

We acknowledge support from Simons Investigator Fellowship, NSF grant DMS-2134157, DARPA grant W911NF2010021, and DOE grant DE-SC0022199.

Figure 4: Variation of test accuracy vs standard deviation of Gaussian noise added to the pretrained representations of the dataset. Here _model1_, _ensemble-ind_, _ensemble-proj_ and _ensemble-esb_ refer to the original model \(f\), ensemble of \(f\) and independently trained model \(f_{}\), ensemble of \(f\) and \(f_{}\) trained using OrthoP, and ensemble of \(f\) and \(f_{}\) obtained using Evading-SB method respectively.