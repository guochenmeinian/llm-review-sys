# CSOT: Curriculum and Structure-Aware Optimal Transport for Learning with Noisy Labels

Wanxing Chang\({}^{1}\) Ye Shi\({}^{1,2}\) Jingya Wang\({}^{1,2}\)

\({}^{1}\)ShanghaiTech University

\({}^{2}\)Shanghai Engineering Research Center of Intelligent Vision and Imaging

{changwx,shiye,wangjingya}@shanghaitech.edu.cn

Corresponding author.

###### Abstract

Learning with noisy labels (LNL) poses a significant challenge in training a well-generalized model while avoiding overfitting to corrupted labels. Recent advances have achieved impressive performance by identifying clean labels and correcting corrupted labels for training. However, the current approaches rely heavily on the models predictions and evaluate each sample independently without considering either the global or local structure of the sample distribution. These limitations typically result in a suboptimal solution for the identification and correction processes, which eventually leads to models overfitting to incorrect labels. In this paper, we propose a novel optimal transport (OT) formulation, called Curriculum and Structure-aware Optimal Transport (CSOT). CSOT concurrently considers the inter- and intra-distribution structure of the samples to construct a robust denoising and relabeling allocator. During the training process, the allocator incrementally assigns reliable labels to a fraction of the samples with the highest confidence. These labels have both global discriminability and local coherence. Notably, CSOT is a new OT formulation with a nonconvex objective function and curriculum constraints, so it is not directly compatible with classical OT solvers. Here, we develop a lightspeed computational method that involves a scaling iteration within a generalized conditional gradient framework to solve CSOT efficiently. Extensive experiments demonstrate the superiority of our method over the current state-of-the-arts in LNL. Code is available at https://github.com/changwxx/CSOT-for-LNL.

## 1 Introduction

Deep neural networks (DNNs) have significantly boosted performance in various computer vision tasks, including image classification , object detection , and semantic segmentation . However, the remarkable performance of deep learning algorithms heavily relies on large-scale high-quality human annotations, which are extremely expensive and time-consuming to obtain. Alternatively, mining large-scale labeled data based on a web search and user tags  can provide a cost-effective way to collect labels, but this approach inevitably introduces noisy labels. Since DNNs can so easily overfit to noisy labels , such label noise can significantly degrade performance, giving rise to a challenging task: learning with noisy labels (LNL) .

Numerous strategies have been proposed to mitigate the negative impact of noisy labels, including loss correction based on transition matrix estimation , re-weighting , label correction and sample selection . Recent advances have achieved impressive performance by identifying clean labels and correcting corrupted labels for training. However, current approaches rely heavily on the models predictions to identify or correct labels even if the model is not yet sufficiently trained. Moreover, these approaches often evaluate each sample independently, disregarding the global or local structure of the sample distribution. Hence, the identification and correction process results in a suboptimal solution which eventually leads to a model overfitting to incorrect labels.

In light of the limitations of distribution modeling, optimal transport (OT) offers a promising solution by optimizing the global distribution matching problem that searches for an efficient transport plan from one distribution to another. To date, OT has been applied in various machine learning tasks [11; 83; 28]. In particular, OT-based pseudo-labeling [11; 73] attempts to map samples to class centroids, while considering the _inter-distribution_ matching of samples and classes. However, such an approach could also produce assignments that overlook the inherent coherence structure of the sample distribution, _i.e.__intra-distribution_ coherence. More specifically, the cost matrix in OT relies on pairwise metrics, so two nearby samples could be mapped to two far-away class centroids (Fig. 1).

In this paper, to enhance intra-distribution coherence, we propose a new OT formulation for denoising and relabeling, called Structure-aware Optimal Transport (SOT). This formulation fully considers the intra-distribution structure of the samples and produces robust assignments with both _global discriminability_ and _local coherence_. Technically speaking, we introduce local coherent regularized terms to encourage both prediction- and label-level local consistency in the assignments. Furthermore, to avoid generating incorrect labels in the early stages of training or cases with high noise ratios, we devise Curriculum and Structure-aware Optimal Transport (CSOT) based on SOT. CSOT constructs a robust denoising and relabeling allocator by relaxing one of the equality constraints to allow only a fraction of the samples with the highest confidence to be selected. These samples are then assigned with reliable pseudo labels. The allocator progressively selects and relabels batches of high-confidence samples based on an increasing budget factor that controls the number of selected samples. Notably, CSOT is a new OT formulation with a nonconvex objective function and curriculum constraints, so it is significantly different from the classical OT formulations. Hence, to solve CSOT efficiently, we developed a lightspeed computational method that involves a scaling iteration within a generalized conditional gradient framework .

Our contribution can be summarized as follows: 1) We tackle the denoising and relabeling problem in LNL from a new perspective, i.e. simultaneously considering the _inter-_ and _intra-distribution_ structure for generating superior pseudo labels using optimal transport. 2) To fully consider the intrinsic coherence structure of sample distribution, we propose a novel optimal transport formulation, namely Curriculum and Structure-aware Optimal Transport (CSOT), which constructs a robust denoising and relabeling allocator that mitigates error accumulation. This allocator selects a fraction of high-confidence samples, which are then assigned reliable labels with both _global discriminability_ and _local coherence_. 3) We further develop a lightspeed computational method that involves a scaling iteration within a generalized conditional gradient framework to efficiently solve CSOT. 4) Extensive experiments demonstrate the superiority of our method over state-of-the-art methods in LNL.

## 2 Related Work

Learning with noisy labels.LNL is a well-studied field with numerous strategies having been proposed to solve this challenging problem, such as robust loss design [82; 70], loss correction [35; 56], loss re-weighting [60; 80] and sample selection [52; 31; 41]. Currently, the methods that are delivering superior performance mainly involve learning from both selected clean labels and relabeled corrupted labels [46; 45]. The mainstream approaches for identifying clean labels typically rely on the small-loss criterion [31; 77; 71; 14]. These methods often model per-sample loss distributions using a Beta Mixture Model  or a Gaussian Mixture Model , treating samples with smaller loss as clean ones [3; 71; 46]. The label correction methods, such as PENCIL , Selfie , ELR , and DivideMix , typically adopt a pseudo-labeling strategy that leverages the DNNs predictions to correct the labels. However, these approaches evaluate each sample independently without considering the correlations among samples, which leads to a suboptimal identification and correction solution. To this end, some work [55; 45] attempt to leverage \(k\)-nearest neighbor predictions  for clean identification and label correction. Besides, to further select and correct noisy labels robustly,OT Cleaner , as well as concurrent OT-Filter , designed to consider the global sample distribution by formulating pseudo-labeling as an optimal transport problem. In this paper, we propose CSOT to construct a robust denoising and relabeling allocator that simultaneously considers both the global and local structure of sample distribution so as to generate better pseudo labels.

Optimal transport-based pseudo-labeling.OT is a constrained optimization problem that aims to find the optimal coupling matrix to map one probability distribution to another while minimizing the total cost . OT has been formulated as a pseudo-labeling (PL) technique for a range of machine learning tasks, including class-imbalanced learning [44; 28; 68], semi-supervised learning [65; 54; 44], clustering [5; 11; 25], domain adaptation [83; 12], label refinery [83; 68; 73; 23], and others. Unlike prediction-based PL , OT-based PL optimizes the mapping samples to class centroids, while considering the global structure of the sample distribution in terms of marginal constraints instead of per-sample predictions. For example, Self-labelling  and SwAV , which are designed for self-supervised learning, both seek an optimal equal-partition clustering to avoid the models collapse. In addition, because OT-based PL considers marginal constraints, it can also consider class distribution to solve class-imbalance problems [44; 28; 68]. However, these approaches only consider the inter-distribution matching of samples and classes but do not consider the intra-distribution coherence structure of samples. By contrast, our proposed CSOT considers both the inter- and intra-distribution structure and generates superior pseudo labels for noise-robust learning.

Curriculum learning.Curriculum learning (CL) attempts to gradually increase the difficulty of the training samples, allowing the model to learn progressively from easier concepts to more complex ones . CL has been applied to various machine learning tasks, including image classification [38; 84], and reinforcement learning [53; 2]. Recently, the combination of curriculum learning and pseudo-labeling has become popular in semi-supervised learning. These methods mainly focus on dynamic confident thresholding [69; 29; 75] instead of adopting a fixed threshold . Flexmatch  designs class-wise thresholds and lowers the thresholds for classes that are more difficult to learn. Different from dynamic thresholding approaches, SLA  only assigns pseudo labels to easy samples gradually based on an OT-like problem. In the context of LNL, CurriculumNet  designs a curriculum by ranking the complexity of the data using its distribution density in a feature space. Alternatively, RoCL  selects easier samples considering both the dynamics of the per-sample loss and the output consistency. Our proposed CSOT constructs a robust denoising and relabeling allocator that gradually assigns high-quality labels to a fraction of the samples with the highest confidence. This encourages both global discriminability and local coherence in assignments.

## 3 Preliminaries

Optimal transport.Here we briefly recap the well-known formulation of OT. Given two probability simplex vectors \(\) and \(\) indicating two distributions, as well as a cost matrix \(^{||||}\), where \(||\) denotes the dimension of \(\), OT aims to seek the optimal coupling matrix \(\) by minimizing the following objective

\[_{(,)}, ,\] (1)

where \(,\) denote Frobenius dot-product. The coupling matrix \(\) satisfies the polytope \((,)=\{_{+}^{||||}|_{||}=,\; ^{}_{||}=\}\), where \(\) and \(\) are essentially marginal probability vectors. Intuitively speaking, these two marginal probability vectors can be interpreted as coupling budgets, which control the mapping intensity of each row and column in \(\).

Pseudo-labeling based on optimal transport.Let \(_{+}^{B C}\) denote classifier softmax predictions, where \(B\) is the batch size of samples, and \(C\) is the number of classes. The OT-based PL considers mapping samples to class centroids and the cost matrix \(\) can be formulated as \(-\)[65; 68]. We can rewrite the objective for OT-based PL based on Problem (1) as follows

\[_{(_{B},_{C })}-,,\] (2)

where \(_{d}\) indicates a \(d\)-dimensional vector of ones. The pseudo-labeling matrix can be obtained by normalization: \(B\). Unlike prediction-based PL  which evaluates each sample independently,OT-based PL considers inter-distribution matching of samples and classes, as well as the global structure of sample distribution, thanks to the equality constraints.

Sinkhorn algorithm for classical optimal transport problem.Directly optimizing the exact OT problem would be time-consuming, and an entropic regularization term is introduced : \(_{(,)} ,+, \), where \(>0\). The entropic regularization term enables OT to be approximated efficiently by the Sinkhorn algorithm , which involves matrix scaling iterations executed efficiently by matrix multiplication on GPU.

## 4 Methodology

Problem setup.Let \(_{train}=\{(_{i},y_{i})\}_{i=1}^{N}\) denote the noisy training set, where \(_{i}\) is an image with its associated label \(y_{i}\) over \(C\) classes, but whether the given label is accurate or not is unknown. We call the correctly-labeled ones as _clean_, and the mislabeled ones as _corrupted_. LNL aims to train a network that is robust to corrupted labels and achieves high accuracy on a clean test set.

### Structure-Aware Optimal Transport for Denoising and Relabeling

Even though existing OT-based PL considers the global structure of sample distribution, the intrinsic coherence structure of the samples is ignored. Specifically, the cost matrix in OT relies on pairwise metrics and thus two nearby samples could be mapped to two far-away class centroids. To further consider the intrinsic coherence structure, we propose a Structure-aware Optimal Transport (SOT) for denoising and relabeling, which promotes local consensus assignment by encouraging prediction-level and label-level consistency, as shown in Fig. 1.

Our proposed SOT for denoising and relabeling is formulated by adding two local coherent regularized terms based on Problem (2). Given a cosine similarity \(^{B B}\) among samples in feature space, a one-hot label matrix \(^{B C}\) transformed from given noisy labels, and a softmax prediction matrix \(_{+}^{B C}\), SOT is formulated as follows

\[_{(_{B},_{C})}-,+( ^{}()+^{}()),\] (3)

Figure 1: **(Top) Comparison between classical OT and our proposed Structure-aware OT.** Classical OT tends to mismatch two nearby samples to two far-away class centroids when the decision boundary is not accurate enough. To mitigate this, our SOT generates local consensus assignments for each sample by preserving prediction-level and label-level consistency. Notably, for vague samples located near the ambiguous decision boundary, SOT rectifies their assignments based on the neighborhood majority consistency. **(Bottom) The illustration of our curriculum denoising and relabeling based on proposed CSOT.** The decision boundary refers to the surface that separates two classes by the classifier. The \(m\) represents the curriculum budget that controls the number of selected samples and progressively increases during the training process.

where the local coherent regularized terms \(^{}\) and \(^{}\) encourages prediction-level and label-level local consistency respectively, and are defined as follows

\[^{}() =-_{i,j}_{ij}_{k}_{ik}_{jk} _{ik}_{jk}=-,( )()^{},\] (4) \[^{}() =-_{i,j}_{ij}_{k}_{ik}_{jk} _{ik}_{jk}=-,( )()^{},\] (5)

where \(\) indicates element-wise multiplication. To be more specific, \(^{}\) encourages assigning larger weight to \(_{ik}\) and \(_{jk}\) if the \(i\)-th sample is very close to the \(j\)-th sample, and their predictions \(_{ik}\) and \(_{jk}\) from the \(k\)-th class centroid are simultaneously high. Analogously, \(^{}\) encourages assigning larger weight to those samples whose neighborhood label consistency is rather high. Unlike the formulation proposed in [1; 16], which focuses on sample-to-sample mapping, our method introduces a sample-to-class mapping that leverages the intrinsic coherence structure within the samples.

### Curriculum and Structure-Aware Optimal Transport for Denoising and Relabeling

In the early stages of training or in scenarios with a high noise ratio, the predictions and feature representation would be vague and thus lead to the wrong assignments for SOT. For the purpose of robust clean label identification and corrupted label correction, we further propose a Curriculum and Structure-aware Optimal Transport (CSOT), which constructs a robust curriculum allocator. This curriculum allocator gradually selects a fraction of the samples with high confidence from the noisy training set, controlled by a budget factor, then assigns reliable pseudo labels for them.

Our proposed CSOT for denoising and relabeling is formulated by introducing new curriculum constraints based on SOT in Problem (3). Given curriculum budget factor \(m\), our CSOT seeks optimal coupling matrix \(\) by minimizing following objective

\[_{}-, +(^{}()+^{}())\] (6) \[\{_{+}^{ B C}|_{C}_{B},^{ }_{B}=_{C}\}.\]

Unlike SOT, which enforces an equality constraint on the samples, CSOT relaxes this constraint and defines the total coupling budget as \(m\), where \(m\) represents the expected total sum of \(\). Intuitively speaking, \(m=0.5\) indicates that top \(50\%\) confident samples are selected from all the classes, avoiding only selecting the same class for all the samples within a mini-batch. And the budget \(m\) progressively increases during the training process, as shown in Fig. 1.

Based on the optimal coupling matrix \(\) solved from Problem (6), we can obtain pseudo label by argmax operation, _i.e._\(_{i}=*{argmax}_{j}_{ij}\). In addition, we define the general confident scores of samples as \(=\{w_{0},w_{1},,w_{B-1}\}\), where \(w_{i}=_{i_{i}}/(m/C)\). Since our curriculum allocator assigns weight to only a fraction of samples controlled by \(m\), we use \(\)(\(\),\(k\)) operation (return top-\(k\) indices of input set \(\)) to identify selected samples denoted as \(_{i}\)

\[_{i}=1,&i(, mB)\\ 0,&,\] (7)

where \(\) indicates the round down operator. Then the noisy dataset \(_{train}\) can be splited into \(_{clean}\) and \(_{corrupted}\) as follows

\[_{clean}\{(_{i},y_{i},w_{i})| _{i}=y_{i},_{i}=1,(_{i},y_{i})_{train} \},\] (8) \[_{corrupted}\{(_{i},_ {i},w_{i})|_{i} y_{i},(_{i},y_{i})_{train} \}.\]

### Training Objectives

To avoid error accumulation in the early stage of training, we adopt a two-stage training scheme. In the first stage, the model is supervised by progressively selected clean labels and self-supervised by unselected samples. In the second stage, the model is semi-supervised by all denoised labels. Notably, we construct our training objective mainly based on Mixup loss \(^{mix}\) and Label consistency loss \(^{lab}\) same as NCE , and a self-supervised loss \(^{simsimiam}\) proposed in SimSiam . The detailed formulations of mentioned loss and training process are given in Appendix. Our two-stage training objective can be constructed as follows

\[^{sup}=^{mix}_{_{clean}}+^{lab}_{_{clean}}+_{1}^{simiam}_{_{ corrupted}},\] (9) \[^{semi}=^{mix}_{_{clean}}+ ^{lab}_{_{clean}}+_{2}^{lab}_{ _{corrupted}}.\] (10)

## 5 Lightspeed Computation for CSOT

The proposed CSOT is a new OT formulation with nonconvex objective function and curriculum constraints, which cannot be solved directly by classical OT solvers. To this end, we develop a lightspeed computational method that involves a scaling iteration within a generalized conditional gradient framework to solve CSOT efficiently. Specifically, we first introduce an efficient scaling iteration for solving the OT problem with curriculum constraints without considering the local coherent regularized terms, _i.e._ Curriculum OT (COT). Then, we extend our approach to solve the proposed CSOT problem, which involves a nonconvex objective function and curriculum constraints.

### Solving Curriculum Optimal Transport

For convenience, we formulate curriculum constraints in Probelm (6) in a more general form. Given two vectors \(\) and \(\) that satisfy \(\|\|_{1}\|\|_{1}=m\), a general polytope of curriculum constraints \(^{}(,)\) is formulated as

\[^{}(,)=\{ ^{||||}_{+}|_{| |},^{}_{||}=\}.\] (11)

For the efficient computation purpose, we consider an entropic regularized version of COT

\[_{^{}(,) },+ ,,\] (12)

where we denote the cost matrix \(:=-\) in Probelm (6) for simplicity. Inspired by , Problem (12) can be easily re-written as the Kullback-Leibler (KL) projection: \(_{^{}(,)} (|e^{-/})\). Besides, the polytope \(^{}(,)\) can be expressed as an intersection of two convex but not affine sets, _i.e._

\[_{1}}}{{=}} \{^{||||}_{+}|_{||}\} _{2}}}{{=}}\{^{||||}_{+}|^{}_{||}=\}.\] (13)

In light of this, Problem (12) can be solved by performing iterative KL projection between \(_{1}\) and \(_{2}\), namely Dykstra's algorithm  shown in Appendix.

**Lemma 1**.: _(Efficient scaling iteration for Curriculum OT) When solving Problem (12) by iterating Dykstra's algorithm, the matrix \(^{(n)}\) at \(n\) iteration is a diagonal scaling of \(:=e^{-/}\), which is the element-wise exponential matrix of \(-/\):_

\[^{(n)}=(^{(n)}) (^{(n)}),\] (14)

_where the vectors \(^{(n)}^{||}\), \(^{(n)}^{||}\) satisfy \(^{(0)}=_{||}\) and follow the recursion formula_

\[^{(n)}=(}{^{(n-1) }},_{||})^{(n)}=}{^{}^{(n)}}.\] (15)

The proof is given in the Appendix. Lemma 1 allows a fast implementation of Dykstra's algorithm by only performing matrix-vector multiplications. This scaling iteration for entropic regularized COT is very similar to the widely-used and efficient Sinkhorn Algorithm , as shown in Algorithm 1.

### Solving Curriculum and Structure-Aware Optimal Transport

In the following, we propose to solve CSOT within a Generalized Conditional Gradient (GCG) algorithm  framework, which strongly relies on computing Curriculum OT by scaling iterationsin Algorithm 1. The conditional gradient algorithm [27; 36] has been used for some penalized OT problems [24; 17] or nonconvex Gromov-Wasserstein distances [58; 67; 13], which can be used to solve Problem (3) directly.

For simplicity, we denote the local coherent regularized terms as \(():=^{}()+^{}()\), and give an entropic regularized CSOT formulation as follows:

\[_{^{}(,)},+( )+,.\] (16)

Since the local coherent regularized term \(^{}()\) is differentiable, Problem (16) can be solved within the GCG algorithm framework, shown in Algorithm 2. And the linearization procedure in Line 5 can be computed efficiently by the scaling iteration proposed in Sec 5.1.

```
1:Input: Cost matrix \(\), marginal constraints vectors \(\) and \(\), entropic regularization weight \(\), local coherent regularization weight \(\), local coherent regularization function \(:^{||||} \), and its gradient function \(:^{||||} ^{||||}\)
2: Initialize: \(^{(0)}^{T}\)
3:for\(i=1,2,3,\)do
4:\(^{(i)}^{(i)}+(^{(i) })\)// Gradient computation
5:\(}^{(i)}*{argmin}_{ ^{}(,)} ,^{(i)}+, \)// Linearization, solved efficiently by Algorithm 1
6: Choose \(^{(i)}\) so that it satisfies the Armijo rule // Backtracking line-search
7:\(^{(i+1)}(1-^{(i)})^{(i)}+^{(i) }}^{(i)}\)// Update
8:endfor
9:Return:\(^{(i)}\) ```

**Algorithm 2** Generalized conditional gradient algorithm for entropic regularized CSOT

## 6 Experiments

### Implementation Details

We conduct experiments on three standard LNL benchmark datasets: CIFAR-10 , CIFAR-100  and Webvision . We follow most implementation details from the previous work DivideMix  and NCE . Here we provide some specific details of our approach. The warm-up epochs are set to 10/30/10 for CIFAR-10/100/Webvision respectively. For CIFAR-10/100, the supervised learning epoch \(T_{sup}\) is set to \(250\), and the semi-supervised learning epoch \(T_{semi}\) is set to \(200\). For Webvision, \(T_{sup}=80\) and \(T_{semi}=70\). For all experiments, we set \(_{1}=1\), \(_{2}=1\), \(=0.1\), \(=1\). And we adopt a simple linear ramp for curriculum budget, _i.e._\(m=(1.0,m_{0}+-1})\) with an initial budget \(m_{0}=0.3\). For the GCG algorithm, the number of outer loops is set to 10, and the number for inner scaling iteration is set to 100. The batch size \(B\) for denoising and relabeling is set to \(1024\). More details will be provided in Appendix.

### Comparison with the State-of-the-Arts

Synthetic noisy datasets.Our method is validated on two synthetic noisy datasets, _i.e._ CIFAR-10  and CIFAR-100 . Following [46; 45], we conduct experiments with two types of label noise: _symmetric_ and _asymmetric_. Symmetric noise is injected by randomly selecting a percentage of samples and replacing their labels with random labels. Asymmetric noise is designed to mimic the pattern of real-world label errors, _i.e._ labels are only changed to similar classes (_e.g._ cat\(\)dog). As shown in Tab. 1, our CSOT has surpassed all the state-of-the-art works across most of the noise ratios. In particular, our CSOT outperforms the previous state-of-the-art method NCE  by \(2.3\%\), \(3.1\%\) and \(9.4\%\) under a high noise rate of CIFAR-10 sym-0.8, CIFAR-100 sym-0.8/0.9, respectively.

Real-world noisy datasets.Additionally, we conduct experiments on a large-scale dataset with real-world noisy labels, _i.e._ WebVision . WebVision contains 2.4 million images crawled from the web using the 1,000 concepts in ImageNet ILSVRC12 . Following previous works [46; 45], we conduct experiments only using the first 50 classes of the Google image subset for a total of \(\)61,000 images. As shown in Tab. 2, our CSOT surpasses other methods in top-1 accuracy on both Webvision and ILSVRC12 validation sets, demonstrating its superior performance in dealing with real-world noisy datasets. Even though NCE achieves better top-5 accuracy, it suffers from high time costs (using a single NVIDIA A100 GPU) due to the co-training scheme, as shown in Tab. S5.

   Dataset &  &  \\ Noise type &  &  &  \\ Method/Noise ratio & 0.2 & 0.5 & 0.8 & 0.9 & 0.4 & 0.2 & 0.5 & 0.8 & 0.9 \\  Cross-Entropy & 86.8 & 79.4 & 62.9 & 42.7 & 85.0 & 62.0 & 46.7 & 19.9 & 10.1 \\ F-correction  & 86.8 & 79.8 & 63.3 & 42.9 & 87.2 & 61.5 & 46.6 & 19.9 & 10.2 \\ Co-teaching+  & 89.5 & 85.7 & 67.4 & 47.9 & 65.6 & 51.8 & 27.9 & 13.7 \\ FENCL  & 92.4 & 89.1 & 77.5 & 38.9 & 88.5 & 69.4 & 57.5 & 31.1 & 15.3 \\ DivideMix  & 96.1 & 94.6 & 93.2 & 76.0 & 93.4 & 77.3 & 74.6 & 60.2 & 31.5 \\ ELR  & 95.8 & 94.8 & 93.3 & 78.7 & 93.0 & 77.6 & 73.6 & 60.8 & 33.4 \\ KIC  & 95.9 & 94.5 & 91.6 & 80.5 & 90.6 & 79.3 & 75.9 & 62.7 & 29.8 \\ RRL  & 96.4 & 95.3 & 93.3 & 77.4 & 92.6 & 80.3 & 76.0 & 61.1 & 33.1 \\ MMT  & 93.1 & 90.0 & 79.0 & 69.6 & 92.0 & 73.0 & 64.6 & 46.5 & 36.0 \\ UnicCon  & 96.0 & 95.6 & 93.9 & **90.8** & 94.1 & 78.9 & 77.6 & 63.9 & 44.8 \\ NCE  & 96.2 & 95.3 & 93.9 & 88.4 & 94.5 & **81.4** & 76.3 & 64.7 & 41.1 \\  OT Cleaner  & 91.4 & 85.4 & 56.9 & - & - & 67.4 & 58.9 & 31.2 & - \\ OT-Filter  & 96.0 & 95.3 & 94.0 & 90.5 & 95.1 & 76.7 & 73.8 & 61.8 & 42.8 \\ 
**CSOT (Best)** & **96.6\(\)0.10** & **96.2\(\)0.11** & **94.4\(\)0.16** & 90.7\(\)0.33 & **95.5\(\)0.06** & 80.5\(\)0.28 & **77.9\(\)0.18** & **67.8\(\)0.23** & **50.5\(\)0.46** \\
**CSOT (Last)** & 96.4\(\)0.18 & 96.0\(\)0.11 & 94.3\(\)0.20 & 90.5\(\)0.36 & 95.2\(\)0.12 & 80.2\(\)0.31 & 77.7\(\)0.14 & 67.6\(\)0.36 & 50.3\(\)0.33 \\   

Table 1: **Comparison with state-of-the-art methods in test accuracy (%) on CIFAR-10 and CIFAR-100. The results are mainly copied from [45; 48]. We present the performance of our CSOT method using the ”mean\(\)variance” format, which is obtained from 3 trials with different seeds.**

   \((||,||)\) & VDA-based & ESI-based (Ours) \\  (1024,10) & 0.83 & **0.82**\(\) \\ (1024,50) & 1.00 & **0.80**\(\) \\ (1024,100) & 0.87 & **0.80**\(\) \\ (50,50) & 0.82 & **0.79**\(\) \\ (100,100) & 0.88 & **0.80**\(\) \\ (500,500) & 0.88 & **0.87**\(\) \\ (1000,1000) & 0.94 & **0.81**\(\)

[MISSING_PAGE_FAIL:9]

computational method that involves an efficient scaling iteration (Algorithm 1) achieves lower time cost compared to vanilla Dykstra's algorithm (Algorithm S6). Specifically, compared to the vanilla Dykstra-based approach, our efficient scaling iteration version can achieve a speedup of up to 3.7 times, thanks to efficient matrix-vector multiplication instead of matrix-matrix multiplication. Moreover, even for very large input sizes, the computational time cost does not increase significantly.

## 7 Conclusion and Limitation

In this paper, we proposed Curriculum and Structure-aware Optimal Transport (CSOT), a novel solution to construct robust denoising and relabeling allocator that simultaneously considers the inter- and intra-distribution structure of samples. Unlike current approaches, which rely solely on the model's predictions, CSOT considers the global and local structure of the sample distribution to construct a robust denoising and relabeling allocator. During the training process, the allocator assigns reliable labels to a fraction of the samples with high confidence, ensuring both global discriminability and local coherence. To efficiently solve CSOT, we developed a lightspeed computational method that involves a scaling iteration within a generalized conditional gradient framework. Extensive experiments on three benchmark datasets validate the efficacy of our proposed method. While class-imbalance cases are not considered in this paper within the context of LNL, we believe that our approach can be further extended for this purpose.

## 8 Acknowledgement

This work was supported by NSFC (No.62303319), Shanghai Sailing Program (21YF1429400, 22YF1428800), Shanghai Local College Capacity Building Program (23010503100), Shanghai Frontiers Science Center of Human-centered Artificial Intelligence (ShanghaiAI), MoE Key Laboratory of Intelligent Perception and Human-Machine Collaboration (ShanghaiTech University), and Shanghai Engineering Research Center of Intelligent Vision and Imaging.

Figure 2: **Performance comparison for clean label identification and corrupted label correction.**