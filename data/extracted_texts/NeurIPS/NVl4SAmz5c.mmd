# Why Warmup the Learning Rate?

Underlying Mechanisms and Improvements

 Dayal Singh Kalra \({}^{2}\)&Maissam Barkeshli \({}^{1}\)

###### Abstract

It is common in deep learning to warm up the learning rate \(\), often by a linear schedule between \(_{}=0\) and a predetermined target \(_{}\). In this paper, we show through systematic experiments using SGD and Adam that the overwhelming benefit of warmup arises from allowing the network to tolerate larger \(_{}\) by forcing the network to more well-conditioned areas of the loss landscape. The ability to handle larger \(_{}\) makes hyperparameter tuning more robust while improving the final performance. We uncover different regimes of operation during the warmup period, depending on whether training starts off in a progressive sharpening or sharpness reduction phase, which in turn depends on the initialization and parameterization. Using these insights, we show how \(_{}\) can be properly chosen by utilizing the loss catapult mechanism, which saves on the number of warmup steps, in some cases completely eliminating the need for warmup. We also suggest an initialization for the variance in Adam which provides benefits similar to warmup.

## 1 Introduction

One of the most important choices to make in gradient-based optimization is the learning rate (step size) \(\). If \(\) is too small, then learning may take place too slowly or the model might get stuck in unfavorable regions of the loss landscape. If \(\) is too large, training will typically diverge. In practice, it is common to pick a dynamical learning rate schedule \(_{t}\)[2; 4; 40; 26]. Modern learning rate schedules for deep learning typically consist of a warmup period where \(_{t}\) is increased linearly from zero to a target value \(_{}\) over a warmup time \(T_{}\)[13; 34]. After the warmup period, it is common to eventually decay the learning rate, for example via a cosine decay schedule [34; 26; 40].

Given that warmup is standard in the practitioner's toolkit, it is important to understand it deeply and identify improvements. In modern settings, perhaps the earliest work to use warmup was , which used a small constant learning rate for the first few epochs of training and then switched to a larger learning rate. A linear warmup schedule was later introduced in . The intuition given was that to scale the minibatch size in SGD by a factor of \(k\), it is natural to also scale the learning rate by a factor of \(k\), provided the model is not changing too rapidly and successive gradients are roughly aligned. However at the beginning of training, the model is changing rapidly, so it is natural to start with a lower learning rate and gradually increase it to the target value after the network has stabilized.

Other explanations suggest that since the network is initialized randomly, the gradient steps at the beginning of training are not meaningful, and thus it would be harmful to take large steps in such directions , so it makes sense to take smaller steps early in training. The analysis by  suggests that warmup primarily limits the magnitude of weight updates in the deeper layers, preventing large instabilities. It has also been suggested that the key benefit of warmup arises for adaptive optimizers, such as Adam:  argues that the variance of the adaptive learning rate is large during early training because the network has seen too few training samples; it is asserted that this large variance is harmful,and that warmup acts as a variance reduction method by allowing the network to collect accurate statistics of the gradient moments before using larger learning rates. Alternatively, it is also sometimes stated that the initialization may start the model off at places in parameter space that are unstable, difficult to optimize, and easily lead to divergence, and that warmup can help alleviate this .

The above explanations are varied and do not clearly demonstrate why and to what extent warmup is necessary. A loss landscape perspective was given in  (and summarized in  Ch. 8), which argued that an important effect of warmup is to gradually reduce the sharpness (the top eigenvalue of the Hessian of the loss), thus causing the model to leave poorly conditioned areas of the loss landscape and move towards flatter regions which can tolerate larger learning rates. They argue that the mechanism for this is similar to the dynamical stability (catapult) mechanisms studied in [35; 22].

Our contributions.Here we perform extensive studies on the effect of learning rate warmup across architectures (FCNs, ResNets, and Transformers), initializations and parameterizations, datasets (CIFAR-10, CIFAR-100, TinyImageNet, WikiText), and for both SGD and Adam.

We demonstrate through systematic experiments that by far the primary benefit of learning rate warmup is to allow the network to tolerate larger learning rates than it otherwise would have. This builds on the observations of  by showing that any other benefits are marginal, disentangling the effect of warmup duration and target learning rate, and by extending the empirical evidence to include adaptive optimizers and Transformers.

For SGD, the maximal allowable learning rate is determined by the sharpness (the top eigenvalue of the Hessian of the loss). As we discuss in Section 4, we find that there are several qualitatively distinct regimes and mechanisms at play. These depend on whether the network starts off in a sharpness reduction or progressive sharpening phase [18; 19; 5], which in turn depends on the initialization and parameterization. We further find that the performance of the network is largely determined by the target learning rate. For a fixed target learning rate, increasing the warmup time provides only marginal benefit, which arises by keeping the network further away from the divergence (failure) boundary. The ability of the network to withstand a larger target learning rate in turn makes hyperparameter tuning of the target learning rate more robust, since the network responds well to a larger window of target learning rates, possibly explaining the popularity of warmup.

We then investigate Adam in detail, and show that the underlying mechanisms of warmup are similar to the SGD case, but with sharpness replaced by a preconditioned sharpness (the top eigenvalue of the pre-conditioned Hessian, defined below). Our results disagree somewhat with prior results  on the underlying reason for warmup's benefits: We find that the key issue is not observing too few training samples, but rather that the pre-conditioned sharpness typically starts off at high values (even in the large batch case), causing considerable instabilities at high learning rates. Such instabilities, which may be retained in Adam's memory, can result in performance degradation and even training failures. Warmup mitigates such instabilities by gradually pushing down the preconditioned sharpness, enhancing performance, and preventing training failures. We propose a simple alternative initialization for Adam, which we refer to as GI-Adam, which provides benefits similar to warmup and consistently improves over standard Adam by inducing lower preconditioned sharpness at initialization, thus pushing the training failure boundary to higher target learning rates. This also demonstrates a different way to remove the bias correction of RMSProp with momentum.

Our analysis shows how much of the time spent during the warmup period is wasted. We show that this wasted time can be saved by making use of the catapult mechanism  to effectively estimate the initial sharpness scale by line search, providing a more principled choice of \(_{}\). Our experiments show that, depending on the target learning rate and initial sharpness, one can dramatically reduce the warmup time, and in some cases remove it altogether.

## 2 Notations and Preliminaries

**SGD(-M):** Given gradients \(_{t}:=_{}L(_{t})\) at step \(t\), Stochastic Gradient Descent with momentum updates the parameters \(_{t}\) using learning rate \(_{t}\) and momentum \(_{t}\) with coefficient \(\). The update equations are: \(_{t+1}=_{t}+_{t}\) and \(_{t+1}=_{t}-_{t}_{t+1}\). \(=0\) corresponds to SGD.

**Adam:** Adam  updates the parameters \(_{t}\) according to the equations: \(_{t+1}=_{1}_{t}+(1-_{1})_{t}\), \(_{t+1}=_{2}_{t}+(1-_{2})_{t}^{2}\), and \(_{t+1}=_{t}-_{t}}_{t+1}}{_{t+1}+}}=_{t}-_{t}P_{t+1}_{t+1}\), where and \(}_{t}=_{t}}{1-_{t}^{2}}\) are the bias-corrected moments, \(\) is a small scalar used for numerical stability and \(P_{t}=(1-_{t}^{t})[(_{t}})+ ]\) is the preconditioner.

**Linear Warmup:** This is defined by the schedule \(_{t}=_{}+(_{}-_{})^{t/T_{ }}\). The warmup rate is \(:=(_{}-_{})/T_{}\). \(T_{}=1\) corresponds to constant learning rate. Unless otherwise specified, we set \(_{}=0\) when referring to linear warmup. We propose strategies for selecting \(_{}\) in Section 6.

**Sharpness:** The sharpness is defined as the maximum eigenvalue of the Hessian of the loss \(_{t}^{H}:=_{}(_{}^{2}L)\), with subscript \(t\) indexing the training step. Adaptive optimizers, such as Adam, effectively perform gradient descent in a transformed space determined by \(:=P^{1/2}\) (for details, see Appendix B.1). Hence, their stability is determined by the largest eigenvalue of the pre-conditioned Hessian, denoted by \(^{P^{-1}H}:=_{}(P^{-1}_{}^{2}L)\), rather than the sharpness itself.

**Parameterizations in Neural Networks:** The mechanism of warmup and its effectiveness is heavily influenced by the network parameterization (see Sections 4 and 5). Standard Parameterization (SP)  is a staple in common libraries [28; 3]. Another notable parameterization is the Neural Tangent Parameterization (NTP) , which along with SP resides in the kernel learning class at infinite width. Ref.  proposed Maximal Update Parameterization (\(\)P) which exhibits feature learning at infinite width. Neural network parameterizations significantly impact training dynamics .

## 3 Overview of Training Instabilities and the Self-Stabilization Mechanism

One important underlying mechanism of warmup is intimately tied to training instabilities. These training instabilities, often referred to as 'catapults' [22; 5], arise when the learning rate \(\) exceeds a critical threshold \(_{c}\), where both \(\) and \(_{c}\) generally change with time. The critical learning rate \(_{c}\) is influenced by a variety of factors, including the choice of optimizer [5; 6], mini-batch size [35; 6], and model properties such as depth, width, parameterization, and initialization [18; 19]. For a detailed overview of instability thresholds, see Appendix B.

When the instability threshold is exceeded (\(>_{c}\)), two cases arise: (i) if the learning rate is higher than the instability threshold but smaller than a maximum stable learning rate (which varies with time), i.e., \(_{c}<<_{}\), training stabilizes through a self-stabilization process and training continues, (ii) if the learning rate exceeds this maximum stable learning rate \(>_{}\), training experiences severe instabilities. For SGD, these can result in training divergence, characterized by the loss increasing to infinity. For Adam, training may cease, resulting in a training failure, where the loss fails to improve significantly over its initial value, as we demonstrate in Section 5.

The self-stabilization mechanism of GD can be understood through both empirical observations (Figure 1) and a theoretical model. We first describe a model derived by Ref.  that effectively captures this phenomenon. The model assumes that the top eigenvector \(\) changes slowly through training and can be treated as constant and considers a cubic approximation of the GD dynamics around a reference point \(^{*}\). The dynamics along the projection \(x_{t}:=^{T}(_{t}-^{*})\) is given by two coupled non-linear equations:

\[x_{t+1}=(1-_{t}_{t}^{H})x_{t},_{t+1}^{H}=_{t}^{H }+_{t}(- x_{t}^{2}),\] (1)

where \(:=-^{H} L\) quantifies the instantaneous change in sharpness and \(:=\|^{H}\|^{2}\) controls the non-linear change in sharpness. In this model, an instability arises when \(_{t}>_{c}=}{{_{t}^{H}}}\). Ref.  considered a constant learning rate \(\) and assumed progressive sharpening (\(>0\)). In contrast, we consider a time-dependent learning rate and allow \(\) to attain both positive and negative values in order to analyze different warmup mechanisms.

The self-stabilization mechanism manifests as a four-step process [22; 7]. Below, we describe the four steps of the self-stabilization mechanism using the above model and the \(T_{}=64\) trajectories illustrated in Figure 1(c, d):

(1) **Approaching instability:** Due to increasing learning rate and/or progressive sharpening, training approaches the instability threshold \(_{t}=_{c}=}{{_{t}^{H}}}\). In Figure 1(d), this occurs within the first \(10\) steps due to increasing learning rate.

(2) **Blow up:** On exceeding the instability threshold (\(>_{c}\)), Equation (1) predicts exponential growth in \(x_{t}\), empirically manifesting as a sharp increase in loss, as observed in Figure 1(c).

(3) **Sharpness reduction:** For small enough learning rates, \(|x_{t}|\) (and the loss) continues to increase until the higher-order term in the sharpness update equation causes a decrease in sharpness (\(x_{t}>}{{}}\)). This is observed as an abrupt decrease in sharpness in Figure 1(d). If the sharpness fails to decrease over extended steps, it may result in training divergence (e.g., see \(T_{}=1\) case in the same figure).

(4) **Return to stability:** Once the sharpness has decreased appreciably so that \(_{t}_{t}^{H}<2\), stability is restored and the loss eventually decreases.

While the self-stabilization process for more complex optimizers remains poorly understood, a qualitatively similar mechanism is observed in practice, as we will see in the later sections.

## 4 Warmup Mechanisms of Gradient and Adaptive Methods

This section analyzes the underlying mechanism of warmup through the lens of sharpness dynamics. A key finding is that warmup decreases sharpness in two ways: by allowing a natural sharpness reduction effect at early times and/or by forcing sharpness reduction through training instability at later times.

### Stochastic Gradient Descent

Learning rate warmup is intrinsically tied to sharpness dynamics, as sharpness determines the instability threshold \(_{c}\). As the learning rate is increased during warmup, training instabilities can be triggered. Assuming the warmup rate is not too high, these instabilities induce a temporary increase in the loss and a decrease in the sharpness to restore stability through the self-stabilization mechanism. Ultimately this allows the model to adapt to the increased learning rate. In other words, a primary goal of warmup is to gradually reduce sharpness, guiding training towards flatter regions that can accommodate training at higher learning rates .

However, digging deeper, we find that training has a 'natural' preference for sharpness evolution throughout the training course . Before exceeding the instability threshold \((<_{c})\), training naturally experiences either a progressive increase or decrease in sharpness, as observed in Figure 1, which is unrelated to warmup. For instance, consider the sharpness trajectories with \(T_{}=1024\) in the above figure. In Figure 1(b), sharpness has a natural preference for increasing, whereas in Figure 1(d), it tends to decrease on its own. This natural sharpness evolution can defined as the sharpness evolution under gradient flow, corresponding to \(\) in Equation (1). The interplay between this natural sharpness evolution and the deliberate intervention of warmup to reduce sharpness can result in completely distinct dynamics. Below, we use the model described by Equation (1) and the experiments in Figure 1 to describe these distinct dynamics.

(**C1) Natural Progressive Sharpening** (\(>0\); top row of Figure 1): The combined effect of naturally increasing sharpness while the learning rate is also being increased results in a "head-oncollision" at which the instability threshold is exceeded (\(_{t}>_{c}\)). This causes the loss to increase, leading to a decrease in sharpness. Once the sharpness has decreased appreciably, the stability is restored (\(_{t}<_{c}\)). As training proceeds, both sharpness and learning rate continue to increase, again surpassing the instability threshold. This results in a _persistent catapult cycle_, characterized by \(_{t}}{{_{t}^{H}}}_{c}\), for the remainder of the warmup period, as seen in Figure 1(b).

**(C2) Natural Sharpness Reduction** (\(<0\); bottom row of Figure 1): The network is naturally already reducing its sharpness during early training. However, if the learning rate is increased sufficiently quickly, eventually the instability threshold will be reached (akin to a "rear-end collision"), causing the loss to increase. For small enough learning rates, the increased loss induces a dramatically more pronounced decrease in sharpness than would naturally occur, ultimately restoring stability (\(_{t}<_{c}\)). To exceed the instability threshold again, the learning rate must significantly increase to account for the decreased sharpness, potentially requiring considerable training steps. Consequently, training experiences one or more separated catapults during the warmup phase, as seen in Figure 1(c, d). This contrasts with the progressive sharpening case, where training enters a continuous catapult cycle after reaching the instability threshold for the first time. Notably, training may eventually reach a very flat region of the landscape during warmup, with gradients pointing towards increasing sharpness (e.g., \(T_{}=64\) in Figure 1(d)). Upon reaching such a region, the dynamics aligns with the natural progressive sharpening scenario.

When natural sharpness reduction is significant (large negative \(\)), warmup may not need to actively reduce sharpness. Instead, it may "piggy-back" on the inherent sharpness decrease, resulting in a completely different warmup mechanism, which does not rely on instabilities to facilitate training at higher learning rates.

The above two scenarios can be interpreted as cooperative or competitive dynamics between warmup and the natural evolution of sharpness. When training inherently undergoes sharpness reduction, it cooperates with warmup in decreasing sharpness. Conversely, if the natural trajectory of training is towards increasing sharpness, it opposes the warmup's effort, leading to a persistent cycle of catapults.

**The Effect of Warmup Duration:** Given a fixed target learning rate \(_{}\), increasing the warmup duration \(T_{}\) delays the point at which training exceeds the instability threshold \(_{c}\), allowing the sharpness to evolve freely before reaching this point. In the sharpness reduction case, sharpness can significantly decrease by the time this threshold is reached, lowering the need for warmup to decrease sharpness actively. Consequently, increasing \(T_{}\) results in catapults that are both delayed and smaller in magnitude, as seen in Figure 1(d). As the catapults become less intense on increasing the warmup duration, the model can train at higher target learning rates without diverging. For extended warmup durations, warmup may not actively reduce sharpness in these sharpness reduction cases and instead it leverages the inherent sharpness decrease.

In the progressive sharpening case, increasing \(T_{}\) allows the sharpness to naturally increase. As a result, training exceeds the instability threshold for the first time at a relatively lower learning rate compared to the constant learning rate case. Although warmup has to now undertake more work in decreasing sharpness, it does so in a more gradual manner since increasing the warmup duration amounts to a lower warmup rate \(}}}{{T_{}}}\). As a result, the fluctuations observed on exceeding the instability threshold are much smaller in magnitude, as seen in Figure 1(a, b).

**Small vs. Large Initializations:** So far, we have outlined different warmup mechanisms without describing specific conditions that typically exhibit them. Small initializations, such as those using maximal update parameterization (\(\)P)  in the large width limit or appropriately using normalizing layers (e.g. standard Transformer architectures, see Figure 17 in Appendix E.5), are characterized by a small initial network output. Such initializations start in flat regions where gradients point toward increasing sharpness , placing them in the progressive sharpening category (C1). As we will see in Section 5, such initializations may not significantly benefit from warmup as they already start in a flat region. In contrast, large initializations, such as FCNS, CNNs, ResNets with Standard Parameterization (SP) initialized at criticality  or Transformers with the last layer-norm removed, undergo an early sharpness reduction, categorizing them into sharpness reduction category (C2). As the primary effect of warmup is to reduce sharpness, we expect such large initializations to considerably benefit from warmup. Notably, large initializations can eventually undergo progressive sharpening at later training stages  and adhere to the second mechanism, especially for prolonged warmups.

Natural sharpness change provides an intuitive way of determining whether an initialization is'small' or 'large': if training from a given initialization exhibits sharpness reduction, it suggests the existence of naturally flatter initializations that could be chosen instead. This observation is particularly helpful for analyzing Adam in the later sections and motivates modifications to improve it.

### Stochastic Gradient Descent with Momentum (SGD-M)

The warmup mechanism of SGD-M, while at its core is similar to that of vanilla SGD, has a few subtleties. Here we summarize the major differences, leaving details to Appendix E.2.

During early training, the loss may decrease non-monotonically on incorporating momentum, even at small learning rates. Such oscillations are also observed when quadratic loss functions are optimized using GD with momentum . These oscillations make it challenging to differentiate between warmup-induced catapults and fluctuations in loss due to the intrinsic effects of momentum. Nevertheless, we can still observe loss spikes correlated with an abrupt decrease in sharpness at large learning rates, as detailed in Appendix E.2.

Additionally, the instability threshold \(_{c}\) itself evolves differently during training. It changes from \(}{{_{0}^{H}}}\) at initialization to \(}{{_{t}^{H}}}\) later in training. Moreover, the late-time instability threshold is significantly influenced by the batch size, exhibiting a much smaller value than SGD for the same batch size. These properties make it more challenging to analyze the training dynamics of SGD with momentum. Nonetheless, the fundamental warmup mechanisms closely mirror the vanilla SGD case. We leave a more detailed analysis of the early training dynamics of SGD-M for future studies.

### Adaptive Gradient Methods (Adam)

Adaptive optimizers effectively perform gradient descent in a transformed space given by \(=P^{1/2}\), as we show in Appendix B.1. This analysis suggests that their local stability should be determined by the largest eigenvalue of \(_{}^{2}L=P^{-1}_{}^{2}L\), which we refer to as the pre-conditioned Hessian. Indeed, this is what we observe in Figure 2, which shows the training loss, pre-conditioned sharpness \(^{P^{-1}H}\), and sharpness trajectories for full batch Adam. In these figures, sharpness is significantly smaller than its instability threshold \()}}{{_{n}}} 4000\), indicating that sharpness does not determine stability. Instead, loss catapults are associated with \(^{P^{-1}H}\) exceeding its instability threshold.

The pre-conditioned sharpness starts high for both progressive sharpening (simple-\(\)P) and sharpness reduction (SP) scenarios considered in the previous section. For simplicity, we considered a simpler version of \(\)P, detailed in Appendix D.2.1. In particular, for \(\)P models, \(_{0}^{P^{-1}H} 10^{5}\) despite being initialized in a flat region as measured by sharpness, while for SP models, \(_{0}^{P^{-1}H} 10^{6}\). These large initial values arise because \(P_{1}=(1-_{1})[(_{0}^{2})+]\) and \(_{0}\) has components that are near zero.

Figure 2: Training loss and sharpness trajectories of FCNs trained on the entire CIFAR-10 dataset with MSE loss using full batch Adam. (top) simple-\(\)P (for details, see Appendix D.2.1) with \(_{}=0.003\) and (bottom) SP with learning rate \(_{}=0.001\). The dashed lines in the sharpness figures illustrate the instability thresholds \()}}{{_{n}(1-_{1})}}\). Similar mechanisms are observed for different architectures, loss functions, and smaller batch sizes as detailed in Appendix E.

The large \(_{0}^{P^{-1}H}\) can lead to training failures if the learning rate does not start sufficiently small. We put forward modifications to improve Adam in Section 6; here we continue characterizing the warmup mechanisms of Adam.

Given that the pre-conditioned sharpness consistently starts high and decreases during early training, this behavior can be viewed as an extreme example of the natural sharpness reduction scenario (C2) described in the previous section. Training Adam at high initial learning rates without warmup can cause large catapults, as seen in Figure 2(d), potentially leading to training failures. Increasing the warmup duration allows the pre-conditioned sharpness to naturally decrease. This prevents the loss from spiking during early training and avoids training failures. In the later stages of training, the pre-conditioned sharpness may continue reducing or exhibit progressive sharpening. From here on, the dynamics follows the warmup mechanisms discussed in the previous sections, with sharpness replaced with pre-conditioned sharpness. Similar to the momentum case, Adam's stability threshold at late training times significantly decreases for smaller batch sizes , also shown in Appendix E.4.

## 5 Impact of Warmup on Training and Generalization

Here we investigate the impact of warmup on training efficacy and generalization by disentangling the role of \(_{}\) and \(T_{}\). Our key findings are that generalization capability is primarily determined by \(_{}\) and that Adam is particularly sensitive to large catapults. The role of increasing \(T_{}\) is to (i) allow the network to tolerate larger \(_{}\), and (ii) move training further away from the divergence (failure) boundary, leading to a marginal improvement in generalization.

**Experimental Setup:** We consider WideResNets (WRNs) and Transformers (LM) parameterized in either SP or \(\)P. WRNs are trained on CIFAR-10, CIFAR-100, and Tiny-ImageNet, employing data augmentation. Transformers are trained on the next token prediction task using the WikiText dataset. These models are trained with MSE or cross-entropy (xent) loss functions using SGD or Adam optimizers for a fixed training budget of \(T=10^{5}\) steps unless otherwise specified. Training begins with a linear warmup phase from \(_{}=0\) to \(_{}\) over \(T_{}\) steps. After warmup, training continues at \(_{}\) for the remaining training budget. In some cases, following the warmup period, we decrease the learning rate using cosine decay . Further details are provided in Appendix D.

Figure 3: Test accuracy heatmaps of WRNs trained on CIFAR-10 using different and parameterizations loss functions using SGD: (a) \(\)P and MSE loss, (b) \(\)P and cross-entropy loss, (c) SP and MSE loss, and (d) SP and cross-entropy loss. Empty cells correspond to training divergences. Similar phase diagrams are generically observed for different architectures and datasets, as shown in Appendix F.

### Stochastic Gradient Descent (SGD)

Figure 3 presents heatmaps that show the best test accuracy achieved during training, plotted in the \(_{}\)-\(T_{}\) plane for different parameterizations and loss functions. These phase diagrams of warmup also show the convergence-divergence boundary, with empty cells indicating training divergences, illustrating the interplay between warmup duration and the maximum trainable \(_{}\). Below, we discuss the crucial insights these results provide into warmup's role in training dynamics.

**Longer Warmup Facilitates Training at Higher Learning Rates:** These phase diagrams reveal that an extended warmup duration facilitates training at higher target learning rates. This benefit is particularly noticeable for large initializations (like SP) and MSE loss. In contrast, the advantage is less pronounced when using cross-entropy loss and smaller initializations (like \(\)P). The diminished benefit for \(\)P is likely due to its initialization in a relatively flat region of the loss landscape, which can already facilitate training at higher learning rates at initialization. This consistent increase in maximum \(_{}\) with warmup durations can be understood through the lens of warmup mechanisms described in the previous section. As observed in Figure 1, when the warmup duration is increased, loss catapults occurring on surpassing the instability thresholds become milder. This effectively pushes the divergent boundary to higher learning rates.

**Final Performance Primarily Depends on the Target Learning Rate:** A closer look into these phase diagrams reveals that, slightly away from the divergent boundary, the test accuracy primarily depends on the target learning rate and nominally on the warmup duration. Based on the model performance, we can categorize these phase diagrams into two distinct cases: (i) models that fail to achieve optimal performance when trained with a constant learning rate (e.g., Figure 3(c)), and (ii) models that attain optimal performance without warmup (e.g., Figure 3(b)). The first scenario corresponds to models with large initializations. Increasing the warmup duration improves performance by facilitating training at higher learning rates. Yet, similar performance is observed for different warmup durations, suggesting that the primary gain comes from the target learning rate, rather than the duration itself. The second case arises for flat initializations, which can already train at large learning rates, and resultantly the optimal performance is already achieved without warmup. While increasing warmup duration facilitates training at even higher learning rates, it does not enhance performance. Nevertheless, it does broaden the range of optimal learning rates, reducing the need for precise tuning of the target learning rate, and making training more practical and robust. We conclude that warmup can serve two key purposes: (i) it can significantly improve model performance in large initialization cases, and (ii) extend the range of optimal target learning rates for small initializations, making it easier to tune the target learning rate. In Appendix F.2, we demonstrate that these results hold on incorporating momentum and employing cosine learning rate decay.

### Adaptive Gradient Methods (Adam)

Figure 4(a) shows the warmup phase diagram of Adam. Increasing the warmup duration enables training at higher learning rates by allowing the pre-conditioned sharpness to decrease naturally, thereby reducing the severity of catapults. These large catapults, which may persist in Adam's memory, can lead to performance degradation and training failures. Thus, in addition to facilitating

Figure 4: Test loss heatmaps of Pre-LN Transformers in SP trained on WikiText-103 with cross-entropy loss for a single epoch using (a) Adam, and (b) GI-Adam (introduced in Section 6). Additional results are presented in Appendix F.3.

training at higher rates similar to SGD, warmup further improves Adam's performance by addressing its vulnerability to large catapults, justifying its widespread use with Adam. Below, we discuss the distinct properties of Adam phase diagrams in detail.

**Training Failures of Adam:** Remarkably, we find that models trained with Adam always exhibit training failures rather than divergences where the loss grows without bound, as further demonstrated in Appendix G. In cases of training failure, we often observed that certain layers or residual blocks output zero, leading to vanishing gradients. This implies that the model gets stuck at a critical point and is unable to train further. Understanding this unexpected phenomenon requires further study.

**Performance Degradation prior to Failure Boundary:** Test accuracy in these phase diagrams declines well before the failure boundary, in contrast to SGD where optimal learning rates are observed near the divergence boundary. This discrepancy stems from Adam's property of retaining a memory of gradient magnitudes. At large learning rates, along with the loss, the gradients spike during early training, as seen in Figure 28 in Appendix G. While the gradients decrease after a few steps, the second moment \(\) remains large for an extended period, leading to a small effective learning rate \( P^{-1}\). As a result, training struggles to escape high-loss regions. Therefore, a longer warmup is more beneficial for Adam compared to SGD, as it is crucial to stay away from the failure boundary.

## 6 Improved Hyperparameter Initialization Schemes for Optimizers

**Initial Learning Rate Selection for Warmup:** Setting the initial learning rate to \(_{}=0\) is common practice in warmup [27; 8]. Our analysis reveals that the primary effect of warmup is to facilitate training at higher learning rates by annealing sharpness (or pre-conditioned sharpness for Adam). From this perspective, starting with \(_{}=0\) appears suboptimal, as it can significantly delay the learning rate from exceeding the instability threshold, thus delaying the primary effect of warmup.

An effective strategy involves setting \(_{}=_{c}\) to induce loss increase and thereby sharpness decrease right from initialization. We introduce a straightforward search method that only uses forward passes to estimate the initial critical learning rate \(_{c}\). The method consists of two stages: (i) an exponential search, starting from an initial guess \(_{0}\), iteratively multiplies \(_{0}\) by a factor \(k>1\) until the loss increases. This identifies an interval \([_{},_{}]\) containing \(_{c}\), (ii) a binary search further narrows down \([_{},_{}]\) by evaluating the loss at the midpoint \(_{}=(_{}+_{})/2\). If the loss increases, \(_{}\) is updated to \(_{}\); otherwise, \(_{}\) is set to \(_{}\). This process is repeated until the loss in the next step \(L(_{1})\) satisfies the condition \(L(_{1})<L(_{0})(1+)\), for some hyperparameter \(>0\). For details, see Appendix B.3.

By setting \(_{}=_{c}\), training can achieve the target learning rate earlier. Consider the modified warmup schedule: \(_{t}=_{}+_{}(}{{T_{}}})\), which attains \(_{}\) in \(T_{}=T_{}(1-}}{{_{}}})\) steps, saving \(T_{}(}}{{_{}}})\) steps. Incorporating the computational cost of additional forward passes \(T_{}\) required for estimating \(_{c}\) (\( 10\) in number), and noting that one training step approximately equates to two forward passes, the net computational savings is \(T_{}=T_{}(}}{{_{}}} )-}}}{{2}}\). Figure 5 demonstrates how \(T_{}\) and \(T_{}\) vary with the \(T_{}\) and \(_{}\). For \(_{}<_{c}\), the target learning rate is reached in a single step, nearly saving the entire duration of the warmup, whereas for \((_{}>_{c})\), starting \(_{}_{c}\) can save up to half of the allocated warmup duration, although this saving diminishes on approaching the divergent/failure boundary.

Figure 5: Heatmaps showing (a) the steps to reach \(_{}\) (\(T_{}\)) and (b) the effective steps saved (\(T_{}\)) on setting \(_{}=_{c}\) for WRNs in SP trained on CIFAR-10 using SGD with cross-entropy loss.

It is worth noting that there can be instances where there is no loss catapult at initialization. In our experiments, this only occurs for Transformers trained using SGD. In such scenarios, the prescribed approach is not applicable and one can resort to heuristics, such as setting the initial learning rate to a fraction of the maximum stable learning rate, such as \(_{}=}}}{{10}}\).

#### GI-Adam: Improving Adam by Initializing The Second Moment using Gradients

In Section 4.3, we observed that the pre-conditioned sharpness for Adam starts at a high value, even for low sharpness initializations like \(\)P, and can lead to training failures at large learning rates. We propose Gradient Initialized Adam (GI-Adam), which initializes the second moment using the gradient squared, \(_{0}=_{0}^{2}\). In Appendix I.2, we show that a bias correction is not required when the second moment is initialized using the gradients. As a result, GI-Adam can be viewed as standard Adam with an automated warmup given by \(_{t}=_{}^{t}}\).

This simple trick reduces the initial pre-conditioned sharpness by around two orders of magnitude (more precisely by a factor of \(}\)) at initialization, preventing large catapults, as illustrated in Figure 6 (c.f. Figure 2(d-f)). Moreover, it consistently shows improvement over standard Adam across datasets and prevents training failures by pushing the training failure boundary to higher \(_{}\), as shown in Figure 4(b). We provide additional results for different datasets in Appendix F.3. In Appendix F.4, we show that GI-Adam consistently performs on par or better than RAdam  while offering a simple modification to Adam.

To further assess that the primary cause of instability during early training is the large pre-conditioned sharpness, we randomly initialize \(_{0}\) but with the same norm as the gradients at initialization. Like GI-Adam, this also results in improved performance as shown in Appendix I.3.

We further reduce the pre-conditioner size by removing bias correction for the first moment, referred to as Flat-Adam. As demonstrated in Appendix I.4, this modification eliminates the initial decrease in pre-conditioned sharpness. We leave a comprehensive evaluation of Flat-Adam for future work.

## 7 Discussion

Our analysis provides new insights into the role of warmup across optimizers and parameterizations. We found compelling evidence that the primary effect of warmup is to facilitate training at higher learning rates and stabilizing the training dynamics by keeping it away from the failure (divergence) boundary. Looking under the hood, we found a variety of underlying mechanisms, which also suggested several improvements for hyperparameter initialization. In Appendix A, we provide practical guidance for practitioners on choosing the warmup duration.

Our analysis also motivates a potential parameter-free warmup strategy, which we refer to as _persistent catapult warmup_. The central idea behind this strategy is to repeatedly induce catapults aimed to progressively reduce sharpness, thereby facilitating training at higher learning rates. We present encouraging preliminary results in Appendix C and defer further development to future work.

The maximum learning rate can be written as \(_{}=}}}{{^{P-1}H}}\). Here we showed how warmup effectively decreases \(^{P-1}H\), which is a local measure of sharpness. There is also another possible effect of warmup, that it can cause an increase in \(c_{}\), which can be viewed as a more non-local measure of sharpness. Further analysis is required to understand how warmup helps increase \(c_{}\).

**Limitations:** Our experiments were conducted on relatively small-scale datasets and models, and further investigations are needed to understand the generalizability of our findings to larger-scale settings. For Adam, we did not explore the dependence on hyperparameters \(_{1}\), \(_{2}\), \(\).

Figure 6: Training loss and sharpness trajectories of FCNs in SP. The experimental setup is identical to Figure 2 but with GI-Adam instead of standard Adam.