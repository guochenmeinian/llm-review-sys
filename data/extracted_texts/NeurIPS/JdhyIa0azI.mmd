# Neural Functional Transformers

Allan Zhou\({}^{1}\) Kaien Yang\({}^{1}\) Yiding Jiang\({}^{2}\) Kaylee Burns\({}^{1}\)

Winnie Xu\({}^{1}\) Samuel Sokota\({}^{2}\) J. Zico Kolter\({}^{2}\) Chelsea Finn\({}^{1}\)

\({}^{1}\)Stanford University \({}^{2}\)Carnegie Mellon University

ayz@cs.stanford.edu

###### Abstract

The recent success of neural networks as implicit representation of data has driven growing interest in _neural functionals_: models that can process other neural networks as input by operating directly over their weight spaces. Nevertheless, constructing expressive and efficient neural functional architectures that can handle high-dimensional weight-space objects remains challenging. This paper uses the attention mechanism to define a novel set of permutation equivariant weight-space layers and composes them into deep equivariant models called _neural functional Transformers_ (NFTs). NFTs respect weight-space permutation symmetries while incorporating the advantages of attention, which have exhibited remarkable success across multiple domains. In experiments processing the weights of feedforward MLPs and CNNs, we find that NFTs match or exceed the performance of prior weight-space methods. We also leverage NFTs to develop Inr2Array, a novel method for computing permutation invariant latent representations from the weights of implicit neural representations (INRs). Our proposed method improves INR classification accuracy by up to \(+17\%\) over existing methods. We provide an implementation of our layers at https://github.com/AllanYangZhou/nfn.

## 1 Introduction

Deep neural networks have emerged as flexible modeling tools applicable to a wide variety of different fields, ranging from natural language processing to vision to the natural sciences. The sub-field of implicit neural representations (INRs) has achieved significant success in using neural networks to represent data such as 3D surfaces or scenes . This has fueled interest in techniques that directly operate in weight space to modify or extract information from a neural network through its weights or gradients. However, developing models that can process weight-space objects is challenging due to their high dimensional nature. As a consequence, some existing methods for processing datasets of weights assume a restricted training process that reduces the effective weight space .

In contrast, we follow recent work in building permutation equivariant weight-space models called _neural functionals_, that can process neural network weights1 without such restrictions . In particular, this work concerns neural functionals that are equivariant to permutations of the weights that correspond to re-arranging neurons. These weight-space permutations, known as _neuron permutation symmetries_, exactly preserve the network's behavior (see examples in Figure 1). Since equivariance to neuron permutations is typically a useful inductive bias for weight-space tasks, neural functionals achieve superior generalization compared to non-equivariant architectures. Despite this, their performance on weight-space tasks remains significantly worse than convolutional networks' performance on analogous image-space tasks , suggesting that neural functional architectures can be significantly improved.

While existing neural functionals rely on _linear_ layers (analogous to convolution), some of the most successful architectures in other domains, such as Transformers [41; 8], rely on _nonlinear_ attention mechanisms. Motivated by this fact, our work develops novel equivariant weight-space layers based on attention. While naive self-attention between input weights is permutation equivariant, it cannot distinguish true neuron permutation symmetries (which preserve network behavior) from "false" permutation symmetries2 (which do not preserve network behavior). In contrast, we prove that our weight-space self-attention layer is _minimally equivariant_: it is equivariant to _only_ neuron permutations, which are the actual weight-space symmetries.

When composed into deep architectures, our weight-space attention layers give rise to neural functional transformers (NFTs). As an immediate application, we use NFTs to develop Inr2Array, a method for mapping INR weights into compact latent representations that are trained through a reconstruction-based objective. Inr2Array produces permutation-invariant latents that can be useful for downstream tasks such as INR classification. We also construct NFTs for a variety of other tasks such as weight-space editing or generalization prediction.

NFTs are competitive with or outperform existing methods on each task without using more parameters. Notably, NFTs and Inr2Array improve downstream INR classification accuracy over the best existing methods on multiple datasets, by up to \(+17\%\). Overall, our experiments show that attention-based neural functionals can be more expressive and powerful than existing architectures that rely on linear weight-space layers, leading to improved performance across weight-space tasks.

## 2 Preliminaries

Consider a feedforward network having \(n_{i}\) neurons at each layer \(i=0,,L\), including the input and output layers. The network has weights3\(W=\,W^{(i)}^{n_{i} n_{i-1}}\,|\,i  1..L\,}\) belonging to weight space, \(\). More generally, we are interested in multi-channel weight space features that can arise from stacking multiple weight space objects such as weights and gradients. For \(c\)-channel weight-space features \(W^{c}\), we have \(W=\,W^{(i)}^{n_{i} n_{i-1} c}\,|\,i  1..L\,}\) and each \(W^{(i)}_{jk}:=W^{(i)}_{j,k,:}^{c}\) is a vector rather than a scalar.

Since the neurons in each hidden layer \(i\{1,,L-1\}\) have no inherent ordering, the feedforward network is invariant to the symmetric group \(S_{n_{i}}\) of permutations of the neurons in layer \(i\). We follow Zhou et al.  in studying the **neuron permutation** (NP) group \(_{} S_{0} S_{n_{L}}\), which further assumes permutation symmetry of the input and output layers. Although this simplifying assumption is usually too strong, it can be effectively corrected in practice using position encodings at the input and output layers .

Consider a neuron permutation \(=(_{0},,_{L})_{}\). Each \(_{i}\) permutes the neurons of layer \(i\), which correspond to the output space (rows) of \(W^{(i-1)}\) and the input space (columns) of \(W^{(i)}\). Hence the action on weight space features \(W^{c}\) is denoted \( W\), where

\[[ W]^{(i)}_{jk}=W^{(i)}_{_{i}^{-1}(j),_{i-1}^{-1}(k)}, =(_{0},,_{L})_{}.\] (1)

As illustrated in Figure 1, a neuron permutation must always permute the columns of \(W^{(i)}\) and the rows of \(W^{(i-1)}\) simultaneously in order to preserve network behavior.

This work is focused on principled architecture design for neural functionals, i.e., architectures that process weight space features . Recent work has shown the benefit of incorporating neuron permutation symmetries into neural functional architectures by enforcing equivariance (or invariance) to neuron permutation symmetries [31; 46]. Consider weight-space function classes parameterized by \(\). We say that a function class \(f:^{c}^{c}\) is \(_{}\)**-equivariant** if permuting the input by any \(_{}\) always has the same effect as permuting the output by \(\):

\[ f(W;)=f( W;),_{},W^{c},.\] (2)

Similarly, a function class \(f:^{c}\) is \(_{}\)**-invariant** if \(f( W;)=f(W;)\) for all \(,W,\) and \(\).

Our weight-space layers build off of the dot-product attention mechanism, which takes a query \(q\) and a set of \(N\) key-value pairs \(,v_{p})}_{p=1}^{N}\) and computes, in the single-headed case:

\[(q,,v_{p})}_{p=1}^{N})=_{p}v_{p}( )}{_{p^{}}(q k_{p^{}})}).\] (3)

If all \(q,k_{p},v_{p}\) are vectors in \(^{d}\), then the output is also a vector in \(^{d}\). The definition extends to the situation where \(q,k,v_{p}\) are multi-dimensional arrays of equal shape, by taking the dot product between these arrays in the natural way. We can also use multi-headed attention () without significant modification.

## 3 Neural Functional Transformers

We now introduce two attention-based layers for processing weight-space features: weight-space self-attention (which is _\(_{}\)-equivariant_) and weight-space cross-attention (which is _\(_{}\)-invariant_). We then describe how to compose these layers into deep architectures called neural functional transformers (NFTs).

### Equivariant weight-space self-attention

A key concept motivating the design of our weight-space self-attention layer is the distinction between actual NP symmetries \(_{}\) that preserve network behavior, and other permutations of the weights that can generally change network behavior. For example, consider a \( S_{()}\) that permutes the columns of \(W^{(i)}\) differently from the rows of \(W^{(i-1)}\) or that moves weights between different weight matrices (see Figure 1). Such permutations of the weight space are called "false symmetries"  since they generally modify network behavior. We are interested in equivariance to the actual weight-space symmetries (neuron permutations) but _not_ to any false symmetries, a property we call _minimal_ equivariance:

**Definition 1**.: _A function class \(f:^{c}^{c}\) is minimally \(_{}\)-equivariant if it is \(_{}\)-equivariant (Eq. 2), but not equivariant to any false symmetries. More precisely, for any \( S_{()}\) such that \(_{}\), there exist a \(W^{c}\) and \(\) such that \(f( W;) f(W;)\)._

Consider naive self-attention between the \(()\) weights \(_{jk}}\), which would be equivariant to _any_ weight-space permutation. This would be \(_{}\)-equivariant, but would also be equivariant to any false permutation symmetries, meaning that it is not minimally equivariant. In other words, naive self-attention is overly constrained by too many symmetries, including those that are not actually relevant to weight-space tasks.

Our weight-space self-attention layer \((;_{Q},_{K},_{V}):^{c} ^{c}\) is more sensitive to the distinction between neuron permutations and false symmetries. It operates on weight-space features with \(c\) channels and is parameterized by query/key/value projection matrices \(_{Q},_{K},_{V}^{c c}\).

Figure 1: **Left:** An illustration of how neuron permutations \(=(_{0},,_{L})\) act on weight matrices \(W=(W^{(0)},,W^{(L)})\). Each \(_{i}\) simultaneously permutes the rows and columns of adjacent weight matrices \(W^{(i)},W^{(i+1)}\). **Right:** Two examples of false symmetries, i.e., permutations that don’t preserve network behavior: (1) when \(_{i}\) permutes the rows and columns of adjacent matrices permute differently and (2) when \(\) permutes weights across layers.

Each entry of the output feature is computed:

\[(W;_{Q},_{K},_{V})_{jk}^{(i)}= }(Q_{j,:}^{(i)},(K,V)_{:,q }^{(i-1)}}_{q=1}^{n_{i-2}}(K,V)_{p,:}^{(i)} }_{p=1}^{n_{i}})_{k}\] (4) \[+}(Q_{:,k}^{(i)},(K,V)_{:,q}^{( i)}}_{q=1}^{n_{i-1}}(K,V)_{p,:}^{(i+1)}}_{p=1}^{n_ {i+1}})_{j}\] (5) \[+}(Q_{jk}^{(i)},(K,V)_{pq}^{( s)}\; s,p,q\;}),\;\] (6) \[Q_{j,k}^{(i)}:=_{Q}W_{jk}^{(i)}, K_{j,k}^{(i)}:= _{K}W_{jk}^{(i)}, V_{j,k}^{(i)}:=_{V}W_{jk}^{(i)},\] (7)

and the notation \((K,V)_{jk}^{(i)}\) is shorthand for the tuple \((K_{jk}^{(i)},V_{jk}^{(i)})\). Appendix A.2 defines the more general version of this layer, which handles inputs with both weights and biases.

The final term (Eq. 6) is simply naive self-attention between the inputs \(\{\,W_{jk}^{(i)}^{c}\,\}\), but the first two terms give the layer additional structure relevant to neuron permutations in particular, as illustrated by Figure 2. To understand the symmetry properties of the first two terms, consider the top input to Attn in Figure 2, which is constructed from the rows of \(W^{()}\) and the columns of \(W^{(ell+1)}\). If the rows of \(W^{()}\) and the columns of \(W^{(+1)}\) are permuted simultaneously, the pairwise dot products between any two vectors in the input is preserved and the attention matrix is unchanged. Then the output of Attn is simply permuted, giving equivariance. On the other hand, independently permuting the rows and columns of \(W^{()}\) and \(W^{(+1)}\) will in general change the attention matrix, breaking equivariance. Since such independent permutations are false symmetries, this behavior is exactly what we need to achieve minimal equivariance.

Since neuron permutations never permute weights _between_ different layers \(W^{(i)}\) and \(W^{(i^{})}\), we may additionally use learned layer position encodings \(\{\,^{(i)}^{c}\;|\;i 1..L\,\}\) to prevent equivariance to such permutations:

\[(W;\{\,^{(i)}\,\}_{i=1}^{L})_{jk}^{(i)}=W_{jk} ^{(i)}+^{(i)}.\] (8)

We can now compose self-attention with position encoding to obtain SA \(\)LayerEnc.

**Theorem 3.1** (Minimal equivariance).: _The combined layer SA \(\)LayerEnc is **minimally**\(_{NP}\)-equivariant._

Figure 2: An illustration of Eqs 4-5 of our weight-space self-attention layer, in the single channel case. The inputs to Attn (Eq 3) are matrices with rows and columns corresponding to the sequence and feature dimensions, respectively. The final term (Eq 6, not shown here) simply computes self attention between all weights.

Proof (Sketch).: To show equivariance: LayerEnc is clearly equivariant to neuron permutations. We can check \(( W)^{(i)}_{jk}=(W)^{(i)}_{_{i}^{-1}(j),_{i -1}^{-1}(k)}\) by expanding the left hand side term-by-term using the definition (Eq. 4). To show non-equivariance to false symmetries: we can broadly classify false symmetries into three types and check non-equivariance to each of them. Appendix A.1 provides the full proof. 

Practical considerations.The final term of SA (Eq. 6) amounts to self-attention between each weight-space feature \(W^{(i)}_{jk}^{c}\) and requires \(O(()^{2}c)\) operations, which is usually intractable.

In practice, we replace this term by a self-attention between the row-column sums \(W^{(i)}_{*,*}=_{j,k}W^{(i)}_{jk}\) of each weight matrix. This preserves interaction between weight-space features in different weight matrices, while reducing the computational complexity to \(O(L^{2}c)\). One can verify that this simplified version also maintains minimal \(_{}\)-equivariance. Meanwhile, consider the self-attention operation required to compute the first two terms of SA (depicted in Figure 2). If \(n_{i}=n\) for all \(i\), then the first two terms require \(O(Ln^{3}c)\) operations, which is feasible for moderate \(n\) but can become expensive when processing very wide networks. As a result, weight-space attention layers are usually more computationally expensive than linear weight-space layers.

### Invariant weight-space cross-attention

Stacking weight-space self-attention layers produces equivariant weight-space features, but some situations require producing an \(_{}\)_-invariant_ output. Existing approaches achieve invariance by relying on a summation or maximization over the rows and columns of each input \(W^{(i)}\).

It is natural to extend these existing invariant layers by using attention over the input's entries, which has similar permutation invariant properties as summation and maximization. Our weight-space cross-attention layer \(:^{c}^{d}\) is simply a cross attention between a learned query \(e^{d}\) and the set keys and values produced from weight-space features \(\{\,W^{(i)}_{jk}^{c}\,\}\):

\[(W;e,_{K},_{V})=(e,\{ \,(_{K}W^{(s)}_{pq},_{V}W^{(s)}_{pq})\,\,\,  s,p,q\,\}),\] (9)

with \(_{K},_{V}^{d c}\) being learned projection matrices. By repeating this operation with \(M\) different learned embeddings \(e_{1},,e_{M}\), we can easily extend this into an invariant map \(^{c}^{M d}\). We depict the operation of the multi-embedding case in Figure 3.

### Convolutional weight spaces

Although our description thus far focuses on fully-connected weight-space features \(W\), we can also extend our layers to convolutional weight spaces. Suppose \(W\) is the \(c\)-channel weight-space feature corresponding to a 1D convolutional network with filter widths \(k_{i}\) at each layer \(i\). It contains matrices

Figure 3: An illustration of our weight-space cross-attention layer, which pools weight-space features into a set of \(_{}\)-invariant vectors. The layer uses a set of learned queries \(Q^{M c}\) to attend over keys and values produced from the weight space features.

\(W^{(i)}^{n_{i} n_{i-1} k_{i} c}\). As in Zhou et al. , the filter dimension \(k_{i}\) can be folded into the channel dimension, creating a feature in \(^{n_{i} n_{i-1}(k_{i}c)}\) with \(k_{i}c\) channels. The problem is that \(k_{i}\) may not be consistent across all \(i\), while dot-product attention operations require that the channel dimensions for each \(W^{(i)}\) match. We solve this problem by choosing a shared channel size \(\) and using learned linear projections \(_{i}:^{n_{i} n_{i-1}(k_{i}c)} ^{n_{i} n_{i-1}}\) to guarantee matching channel dimensions. The output of \(()\) can then be restored to the original channel dimension by another set of learned linear projections \(_{i}:^{n_{i} n_{i-1}} ^{n_{i} n_{i-1} k_{i} c}\).

### Building Neural Functional Transformers

Following Transformer architecture design , we form stackable "blocks" that combine weight-space self attention with LayerNorm, pointwise MLPs, and residual connections. Each block maps \(^{c}^{c}\) and preserves \(_{}\)-equivariance:

\[(W) =Z+((Z))\] (10) \[Z =W+((W)),\] (11)

where both the \(:^{c}^{c}\) and LayerNorm \(:^{c}^{c}\) operate "pointwise," or independently on each input \(W^{(i)}_{jk}\). Since pointwise operations are permutation equivariant, the overall block is an \(_{}\)-equivariant map on \(^{c}\). To build neural functional Transformers (NFTs), we stack multiple blocks into a deep equivariant architecture. For tasks which require \(_{}\)-invariance, like classification or learning an invariant latent space of weights (Section 4), we can apply weight-space cross-attention on top of the features produced by the final \(()\). The invariant activations produced by \(()\) can then be fed into an MLP that produces the final output.

Additionally, the NFT's input is often one or a few channels, while our desired hidden channel dimension may be significantly larger (e.g., \(c=256\)). We can apply any function \(g:^{c_{1}}^{c_{2}}\) to each \(W^{(i)}_{jk}^{c_{1}}\) to project from the input channel dimension to the hidden channel dimension while preserving equivariance. In our experiments, we use random Fourier features  to achieve this projection.

## 4 Inr2Array: Learning an invariant latent space for weights

An application of interest for weight-space architectures is to learn a compact latent representation of weights, which can be useful for downstream tasks such as classifying the signal represented by an implicit neural representation (INR). The recently introduced Inr2vec can learn a representation of INR weights by using a reconstruction objective, but requires every INR share the same initialization. In this section, we leverage neural functionals to extend the inr2vec framework to the independent initialization setting, which is significantly more challenging but removes any restrictions on the INR training process.

Inr2vec uses an encoder-decoder setup to map INR weights into useful latent representations. We make two key changes to the original Inr2Vec formulation:

1. We implement the encoder with an \(_{}\)-invariant neural functional transformer (NFT). This guarantees that the latent representation is invariant to neuron permutations of the weights.
2. We allow the latent representation to be a spatially meaningful _array_ of vectors. In particular, each vector in the latent array is responsible for encoding a single spatial patch of the INR.

The first modification is a useful inductive bias because the signal represented by an INR should be invariant to neuron permutations. The second modification is inspired by Spatial Functa , which found that giving INR latent spaces spatial structure was helpful in their meta-learning setting. We call our representation learning approach Inr2Array due to its modified latent space structure.

The following explanation focuses on 2D INRs that represent images on the coordinate grid \([-1,1]^{2}\), though the general case follows naturally. We first split the coordinate grid into \(M\) spatial patches \(P_{1}_{M}=[-1,1]^{2}\). Given a \((;W)\), \(\) uses an \(_{}\)-invariant NFT encoder \(_{}()\) to map weights \(W\) to a latent array of \(M\) vectors, \(z^{M d}\). The decoder \(_{}:^{d}\) produces a set of weights \(\{_{i}\;\;i=1,,M\;\}\), one for each vector \(z_{i}:=z_{i,:}^{d}\). Each \(_{i}(z_{i})\) is responsible for parameterizing the SIREN only for the spatial patch \(P_{i}[-1,1]^{2}\). The objective is to reconstruct the original INR's content using the decoded weights:

\[(,W)=_{i=1}^{M}_{x P_{i}}((x;_{i})-(x;W))^{2},\] (12) \[_{i}=_{}(z_{i}), z=_{ }(W).\] (13)

Our \(\) encoder architecture uses multiple weight-space self-attention layers followed by a weight-space cross-attention layer CA, which is especially well-suited for producing \(M d\) permutation invariant arrays. For the decoder, we use the hypernetwork design of Sitzmann et al. , where the hypernetworks are conditioned on the spatial latent \(z_{i}^{d}\). In principle, one can also design an invariant encoder using (non-attentive) NFN layers, and we will compare both options in our experiments.

## 5 Experiments

Our experiments broadly involve two types of weight-space datasets: (1) datasets of neural networks (e.g., CNN image classifiers) trained with varying hyperparameters, where the goal is to model interesting properties of the trained networks (e.g., generalization) from their weights; and (2) datasets containing the weights of INRs each representing a single data signal, such as an image. Tasks including editing INR weights to modify the represented signal, or predicting target information related to the signal (e.g., image class). For our INR experiments, we construct datasets for MNIST , FashionMNIST , and CIFAR-10  following the procedure of Zhou et al.  exactly.

### INR classification with \(\)

Classifying the signal represented by an INR directly from its weights is a challenging problem, with current approaches requiring that all INRs share initialization [10; 3; 6]. In the "vanilla" setting where INRs can be initialized and trained independently, state-of-the-art methods struggle to classify signals from even simple datasets [31; 46]. We train \(\) to learn a latent representation of INR weights that can be used for more effective classification in this vanilla setting.

We implement both NFT (weight-space self-attention and cross-attention) and \(_{}\) variants of the \(\) encoder, and distinguish the two variants by superscripts: \(^{}\) and \(^{}\). For the decoder, we use the hypernetwork design of Sitzmann et al. , where the hypernetworks are conditioned on the spatial latent \(z_{i}^{d}\). Appendix B.3 describes the implementation and training of each variant in full detail.

We train separate encoders for each INR dataset shown in Table 6 of the appendix, which reports the reconstruction mean square error (Eq 12) on test INR inputs. The NFT encoder consistently achieves lower reconstruction error than the NFN encoder, and \(^{}\) also produces qualitatively better reconstructions than \(^{}\) (Figure 4), confirming that NFTs enable higher quality encoding than previous neural functionals.

Once trained, the \(\) encoder maps INR weights to compact latent arrays that represent the INR's contents. Downstream tasks such as classifying the INR's signal (e.g., image) can now be performed directly on these \(_{}\)-invariant latent arrays \(z^{M d}\). Concretely, in \(K\)-way

Figure 4: \(\) reconstructions for random samples from each INR dataset. The NFT encoder produces higher qualitatively better reconstructed SIRENs on each dataset, especially CIFAR-10. Blocking artifacts are due to the spatial latent structure, and suggest further room for improvement.

classification the trained encoder Enc : \(^{M d}\) can be composed with any classification head \(f:^{M d}^{K}\) to form an \(_{}\)-invariant classifier \(f:^{K}\). Since the encoder is already trained by the reconstruction objective, we only train \(f\) during classification and keep the encoder fixed. We choose to implement \(f\) using a Transformer classification head , which views the latent array input as a length-\(M\) sequence of \(d\)-dimensional vectors.

Existing INR classifiers, which we refer to as DWS  and NFN , are permutation-invariant architectures that directly map input weights to predicted labels. We also create a modified NFN classifier (Spatial NFN) which produces an \(M d\) array of activations before a convolutional classifier head, to test the impact of spatial latents independent of the Inn2Array training process. Appendix B.4 describes the setup for each method in further detail. Finally, we measure the performance of NFT classifiers without Inr2Array.

Table 1 shows that Inn2Array significantly improves classification test accuracies across the board. In addition, the best performance is achieved by implementing the encoder using our attention-based layers (NFT) compared to linear weight-space layers (NFN). Notably, Inn2Array\({}^{}\) achieves a test accuracy of \(64.4\%\) on CIFAR-10 (the hardest dataset), an improvement of \(+17\%\) over the previous best result of \(46.6\%\) by NFNs. They also achieve an MNIST test accuracy of \(98.5\%\), up from \(92.9\%\) by NFNs and \(85.7\%\) by DWS. The vanilla NFT performance is comparable to DWS and NFNs (better than the former and worse than the latter), suggesting the importance of Inr2Array in addition to the architecture. Finally, the Spatial NFN fails to improve performance over standard NFN classifiers, implying that the particular objective of Inr2Array is crucial to make spatial latent spaces useful. The results show that learning a latent array representation of INR weights using Inr2Array and NFTs can greatly improve performance on downstream INR tasks like classification.

#### 5.1.1 Term-by-term ablation

Here we investigate the importance of each term in our weight space self-attention layer (Eqs. 4-6). The first two terms of SA, illustrated by Figure 2, amount to self-attention between the rows and columns of _adjacent_ weights, while the third term is a _global_ self-attention between features from all layers. Note that in practice we use a tractable approximation of the third term described in Sec 3.1. For this ablation experiment we either keep only the first two terms (**AdjacentSA**) or only the third term (**GlobalSA**). **FullSA** denotes the original layer.

Table 2 shows the results of this ablation experiment on Inr2Array\({}^{}\) performance in the MNIST classification task. We see that our full layer (FullSA) performs best, ablating the final term (AdjacentSA) only slightly degrades performance, and ablating the first two terms (GlobalSA) drastically harms performance. This is interesting since the first two terms are necessary to achieve _minimal equivariance_, while the third term alone is not minimally

    & MNIST & FashionMNIST & CIFAR-10 \\  Inr2Vec  & \(19.1 0.18\) & \(23.3 0.36\) & \(16.7 0.24\) \\ DWS  & \(74.4 0.14\) & \(64.8 0.69\) & \(41.5 0.43\) \\ NFN  & \(92.9 0.38\) & \(75.6 1.07\) & \(46.6 0.13\) \\ Spatial NFN & \(92.9 0.46\) & \(70.8 0.53\) & \(45.6 0.11\) \\ NFT & \(89.9 1.03\) & \(72.7 0.05\) & \(44.8 0.32\) \\  Inr2Array\({}^{}\) (Ours) & \(94.6 0.00\) & \(76.7 0.00\) & \(45.4 0.00\) \\ Inr2Array\({}^{}\) (Ours) & \(\) & \(\) & \(\) \\   

Table 1: Test accuracy (%) for weight-space INR classification in the MNIST, FashionMNIST, and CIFAR-10 datasets. Inr2Array significantly improves over current state-of-the-art results in this setting. For the NFN baseline  we report the higher performance out of their NP and HNP variants. Note that the MNIST and FashionMNIST results reported for DWS in Navon et al.  are on their own independently constructed INR datasets, while we use the datasets from Zhou et al.  for all methods for consistent comparison. Uncertainties indicate standard error over three runs.

  Model & MNIST \\  FullSA & 98.5 \\ AdjacentSA & 97.6 \\ GlobalSA & 37.4 \\  

Table 2: Test classification accuracy of Inr2Array\({}^{}\) when ablating the terms of SA (Eqs.4-6).

equivariant (but helps propagate information between any two weight matrices). The results emphasize the importance of designing our layer to achieve minimal equivariance rather than naively applying self-attention to the weights.

### Editing INRs

We also evaluate NFTs on editing INR weights to alter their signal, e.g., to modify the represented image. The goal of this task is to edit the weights of a trained SIREN to alter its encoded image, expressed as a difference \(W^{} W+(W)\). Neural functionals are trained to learn \(()\) that achieves some desired visual transformation, such as dilating the image. Permutation equivariance is a useful inductive bias here since if \((W)\) is the desired edit for \(W\), then for any neuron permutation \(\) the desired edit to \( W\) is \((W)\). In addition to the MNIST dilation and CIFAR contrast tasks from Zhou et al. , we also introduce several new editing tasks: MNIST erosion, MNIST gradient, and FashionMNIST gradient. Gradient tasks roughly amount to edge detection; Figure 5 in the appendix shows sample inputs and targets for each editing task.

We compare NFT editing performance against the two NFN variants , with each method using the a similar number of parameters (\( 7\)M). For full training details, see Appendix B.2). Table 3 shows the test mean square error (MSE) between the edited INR and the ground truth visual transformation. NFTs consistently outperform NFNs on most INR editing tasks. In addition, Table 5 in the appendix shows that NFTs generally obtain both lower training and test error, indicating that the attention-based architecture enables greater expressivity. Figure 5 shows qualitative samples produced by each editing method on each task.

### Predicting CNN classifier generalization

Whereas the previous experiments have focused on the weight spaces of INRs (which are implemented by small MLPs), we would also like to evaluate how NFTs process other weight spaces such as those belonging to convolutional neural network classifiers. Large-scale empirical studies have produced datasets of trained classifiers under different hyperparameter settings [40; 11], enabling a data-driven approach to modeling generalization from their weights, which could lead to new insights. Prior methods for predicting classifier generalization typically rely on extracting hand-designed features from the weights before using them to predict the test accuracy [18; 43; 40; 19; 28].

We now study how NFTs can model generalization from raw weights using the Small CNN Zoo , which contains thousands of CNNs trained on image classification datasets. In addition to comparing against the two neural functional variants \(_{}\) and \(_{}\), we also show the performance of

    & \(_{}\) & \(_{}\) & StatNN  & NFT (Ours) \\  CIFAR-10-GS & \(\) & \(0.922 0.001\) & \(0.915 0.002\) & \(0.926 0.001\) \\ SVHN-GS & \(\) & \(0.856 0.001\) & \(0.843 0.000\) & \(0.858 0.000\) \\   

Table 4: (Small CNN Zoo  benchmark.) Rank correlation \(\) for predicting the generalization of CNN image classifiers with unseen hyperparameters trained CIFAR-10-GS and SVHN-GS (GS=grayscale). The NFT outperforms \(_{}\) and hand-picked features (StatNN), while \(_{}\) performs best overall. Uncertainties indicate standard error over two runs.

    & \(_{}\) & \(_{}\) & NFT (Ours) \\  MNIST (erode) & \(0.0228 0.0003\) & \(0.0223 0.0000\) & \(\) \\ MNIST (dilate) & \(0.0706 0.0005\) & \(0.0693 0.0009\) & \(\) \\ MNIST (gradient) & \(0.0607 0.0013\) & \(0.0566 0.0000\) & \(\) \\  FashionMNIST (gradient) & \(0.0878 0.0002\) & \(0.0870 0.0001\) & \(\) \\  CIFAR (contrast) & \(0.0204 0.0000\) & \(0.0203 0.0000\) & \(0.0200 0.0002\) \\   

Table 3: Test mean squared error to the target visual transformation for five INR editing tasks. NFTs generally achieve lower test error than NFN variants. Uncertainties indicate standard error over three seeds.

a hand-designed features approach called StatNN . Each generalization predictor is trained on thousands of CNN weights produced under varying initialization and optimization hyperparameters, and is then tested on held out weights produced using unseen hyperparameters. Appendix B.1 describes the experimental setup in full detail.

Table 4 shows the test performance of each method using the Kendall rank correlation coefficient \(\). Across both datasets, neural functionals operating on raw weights outperform the hand-designed features baseline StatNN. NFTs slightly outperform NFNNP on each dataset, while NFNHNP achieves the best performance overall. Note that both NFNNP and NFTs use layers that assume input and output neurons are permutable (NP). Although we use input/output position encoding to remove the stronger NP assumptions, the results suggest that HNP designs may be naturally better suited to this task.

## 6 Related Work

It is well known that the weight spaces of neural networks contain numerous symmetries, i.e., transformations that preserve the network's behavior . Permutation symmetries in particular have been studied in the context of neural network loss landscapes [14; 4; 12] and weight-space merging [38; 1]. However, most prior methods for processing weight space objects do not explicitly account for these symmetries [2; 26; 15; 24; 45; 7; 21], although some have tried to encourage permutation equivariance through data augmentation [33; 29]. Another workaround approach is to explicitly restrict the weight space being considered by, for example, fixing the initialization of all networks being processed  or meta-learning modulations of a set of base parameters [10; 3]. Instead, we study the problem of encoding permutation symmetries into the neural functional itself, without any restrictions on the weight space being processed.

There are a variety of methods that incorporate symmetries into deep neural network architectures [36; 22; 13]. Examples include (group) convolutions for images [25; 5] and permutation equivariant architectures for general set-structured inputs [34; 44; 16; 39; 27]. Most directly related to our work are that of Navon et al.  and Zhou et al. , who introduce linear layers equivariant to the neuron permutation symmetries of feedforward networks. We extend their approach by introducing nonlinear equivariant layers based on the attention mechanism, and use them to construct NFTs.

## 7 Conclusion

This work introduces neural functional transformers, a novel class of weight-space models designed with neuron permutation symmetries in mind. Our approach extends recent work on permutation-equivariant weight-space models using nonlinear self-attention and cross-attention layers, which can enable greater expressivity compared to existing linear layers.

We empirically evaluate the effectiveness of NFTs on weight-space tasks involving datasets of trained CNN classifiers and implicit neural representations (INRs). Operating on weights alone, NFTs can predict the test accuracy of CNN classifiers, modify the content of INRs, and classify INR signals. We also use NFTs to develop Inr2Array, a method for mapping INR weights to compact and \(_{}\)-invariant latent representations, which significantly improve performance in downstream tasks such as INR classification.

Some limitations of NFTs include increased computational costs from self-attention relative to linear weight-space layers, and the difficulty of training large NFT architectures stably. Future work may address these limitations, and explore additional applications of NFTs such as for learned optimization or weight-space generative modeling. Overall, our work highlights the potential of attention-based weight-space layers offers a promising direction for the development of more expressive and powerful neural functional architectures.