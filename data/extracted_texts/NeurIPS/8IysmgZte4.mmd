# Distributional Successor Features Enable Zero-Shot Policy Optimization

Chuning Zhu

University of Washington

zchuning@cs.washington.edu

&Xinqi Wang

University of Washington

wxqkaxdd@cs.washington.edu

&Tyler Han

University of Washington

than123@cs.washington.edu

&Simon Shaolei Du

University of Washington

ssdu@cs.washington.edu

&Abhishek Gupta

University of Washington

abhgupta@cs.washington.edu

###### Abstract

Intelligent agents must be generalists, capable of quickly adapting to various tasks. In reinforcement learning (RL), model-based RL learns a dynamics model of the world, in principle enabling transfer to arbitrary reward functions through planning. However, autoregressive model rollouts suffer from compounding error, making model-based RL ineffective for long-horizon problems. Successor features offer an alternative by modeling a policy's long-term state occupancy, reducing policy evaluation under new rewards to linear regression. Yet, policy optimization with successor features can be challenging. This work proposes _a novel class of models_, i.e., Distributional Successor Features for Zero-Shot Policy Optimization (DiSPOs), that learn a distribution of successor features of a stationary dataset's behavior policy, along with a policy that acts to realize different successor features within the dataset. By directly modeling long-term outcomes in the dataset, DiSPOs avoid compounding error while enabling a simple scheme for zero-shot policy optimization across reward functions. We present a practical instantiation of DiSPOs using diffusion models and show their efficacy as a new class of transferable models, both theoretically and empirically across various simulated robotics problems. Videos and code are available at https://weirdlabuw.github.io/dispo/.

## 1 Introduction

Reinforcement learning (RL) agents are ubiquitous in a wide array of applications, from language modeling  to robotics . Traditionally, RL has focused on the single-task setting, learning behaviors that maximize a specific reward function. However, for practical deployment, RL agents must be able to generalize across different reward functions within an environment. For example, a robot deployed in a household setting should not be confined to a single task such as object relocation but should handle various tasks, objects, initial and target locations, and path preferences.

This work addresses the challenge of developing RL agents that can broadly generalize to _any_ task in an environment specified by a reward function. To achieve this type of generalization, we consider the paradigm of pretraining on an offline dataset of transitions and inferring optimal policies for downstream tasks from observing task-specific rewards. Since the target task is not revealed duringpretraining, the model must encode information about the environment dynamics without committing to a particular policy or reward. Moreover, once the task reward is observed, the model must provide a way to quickly evaluate and improve the policy since different tasks require different optimal policies.

A natural approach to this problem is model-based reinforcement learning [61; 21; 62], which learns an approximate dynamics model of the environment. Given a downstream reward function, task-optimal behavior can be obtained by "planning" via model rollouts [50; 58; 39; 45]. Typically, model rollouts are generated autoregressively, conditioning each step on generation from the previous step. In practice, however, autoregressive generation suffers from _compounding error_[31; 1; 26], which arises when small, one-step approximation errors accumulate over long horizons. This leads to rollout trajectories that diverge from real trajectories, limiting many model-based RL methods to short-horizon, low-dimensional problems.

An alternative class of algorithms based on _successor features_ (SFs) has emerged as a potential approach to transferable decision-making [3; 2]. Successor features represent the discounted sum of features for a given policy. Assuming a linear correspondence between features and rewards, policy evaluation under new rewards reduces to a simple linear regression problem. Notably, by directly predicting long-term outcomes, SFs avoid autoregressive rollouts and hence compounding error. However, the notion of successor features is deeply tied to the choice of a particular policy. This policy dependence hinders the recovery of optimal policies for various downstream tasks. Current approaches to circumvent policy dependence either maintain a set of policies and select the best one during inference  or randomly sample reward vectors and make conditional policy improvements [5; 54; 55]. Nevertheless, a turnkey solution to transfer remains a desirable goal.

In this work, we propose a new class of models--_Distributional Successor Features for Zero-Shot Policy Optimization (DiSPOs)--_that are rapidly transferable across reward functions while avoiding compounding errors. Rather than modeling the successor features under a particular policy, DiSPOs model the _distribution_ of successor features under the _behavior policy_, effectively encoding all possible outcomes that appear in the dataset at each state. Crucially, by representing outcomes as successor features, we enjoy the benefit of zero-shot outcome evaluation after solving a linear reward regression problem, without the pitfall of compounding error. In addition to the outcome model, DiSPOs jointly learn a readout policy that generates an action to accomplish a particular outcome. Together, these models enable zero-shot policy optimization  for arbitrary rewards without further training: at test time, we simply perform a linear regression, select the best in-distribution outcome, and query the readout policy for an action to realize it. _DiSPOs are a new class of world models as they essentially capture the dynamics of the world and can be used to plan for optimal actions under arbitrary rewards, without facing the pitfalls of compounding error._

Since multiple outcomes can follow from a particular state, and multiple actions can be taken to achieve a particular outcome, both the outcome distribution and the policy require expressive model

Figure 1: The transfer setting for DiSPOs. Given an unlabeled offline dataset, DiSPOs model both “what can happen?” \(p(|s)\) and “how can we achieve a particular outcome?” \(p(a|s,)\). This is used for quick adaptation to new downstream tasks without test-time policy optimization.

classes to represent. We provide a practical instantiation of DiSPOs using diffusion models [23; 47] and show that under this parameterization, policy optimization can be cast as a variant of guided diffusion sampling . We validate the transferability of DiSPOs across a suite of long-horizon simulated robotics domains and further show that DiSPOs provably converge to "best-in-data" policies. With DiSPOs, we hope to introduce a new way for the research community to envision transfer in reinforcement learning, and to think of alternative ways to address the challenges of world modeling.

## 2 Related Work

Our work has connections to numerous prior work on model-based RL and successor features.

**Model-Based RL** To enable transfer across rewards, model-based RL learns one-step (or multi-step) dynamics models via supervised learning and use them for planning [11; 38; 39; 58] or policy optimization [12; 50; 26; 64; 20]. These methods typically suffer from compounding error, where autoregressive model rollouts lead to large prediction errors over time [1; 31]. Despite improvements to model architectures [20; 25; 1; 32; 65] and learning objectives , modeling over long horizons without compounding error remains an open problem. DiSPOs instead directly model cumulative long-term outcomes in an environment, avoiding autoregressive generation while remaining transferable.

**Successor Features** Successor features achieve generalization across rewards by modeling the accumulation of features (as opposed to rewards in model-free RL) [3; 2]. With the assumption that rewards are linear in features, policy evaluation under new rewards reduces to a linear regression problem. A key limitation of successor features is their inherent policy dependence, as they are defined as the accumulated features when acting according to a particular policy. This makes extracting optimal policies for new tasks challenging.

To circumvent this policy dependence, generalized policy improvement [3; 2] maintains a discrete set of policies and selects the highest valued one to execute at test time, limiting the space of available policies for new tasks. Universal SF  and Forward-Backward Representations [54; 55] randomly sample reward weights \(z\) and jointly learn successor features and policies conditioned on \(z\). The challenge lies in achieving coverage over the space of all possible policies through sampling of \(z\), resulting in potential distribution shifts for new problems. RaMP  learns a successor feature predictor conditioned on an initial state and a sequence of actions. Transfer requires planning by sampling actions sequences, which becomes quickly intractable over horizon. In contrast, DiSPOs avoid conditioning on any explicit policy representation by modeling the distribution of all possible outcomes represented in a dataset, and then selecting actions corresponding to the most desirable long-term outcome.

Distributional Successor Measure (DSM)  is a concurrent work that learns a distribution over successor representations using tools from distributional RL . Importantly, DSM models the distributional successor measure of a _particular_ policy, where the stochasticity stems from the policy and the dynamics. This makes it suitable for robust policy evaluation but not for transferring to arbitrary downstream tasks. In contrast, DiSPOs model the distribution of successor feature outcomes in the dataset (i.e., the behavior policy), where the distribution stems from the range of meaningfully distinct long-term outcomes. This type of modeling allows DiSPOs to extract optimal behavior for arbitrary downstream tasks, while DSMs suffer from the same policy dependence that standard successor feature-based methods do.

## 3 Preliminaries

We adopt the standard Markov Decision Process (MDP) notation and formalism  for an MDP \(=(,,r,,,_{0})\), but restrict our consideration to the class of deterministic MDPs. While this does not encompass every environment, it does capture a significant set of problems of practical interest. Hereafter, we refer to a deterministic MDP and a _task_ interchangeably. In our setting, we consider transfer across different tasks that always share the same action space \(\), state space \(\), and transition dynamics \(:\)1 The difference between tasks only lies in having different state-dependent Markovian reward functions \(r:\).

**Value Functions and Successor Features** Let \(R=_{t=1}^{}^{t-1}r(s_{t})\) denote the cumulative reward for a trajectory \(\{s_{i},a_{i}\}_{i=1}^{}\). One can then define the state value function under policy \(\) as \(V^{}(s):=_{,}[R s_{1}=s]\), and the state-action value function as \(Q^{}(s,a):=_{,}[R s_{1}=s,a_{1}=a]\). The value function admits a temporal structure that allows it to be estimated using dynamic programming, which iteratively applies the Bellman operator until a fixed point is reached \(V^{}(s):=r(s)+\,_{}[V^{}(s_{2}) s_{1}=s]\). While these Bellman updates are in the tabular setting, equivalent function approximator variants (e.g., with neural networks) can be instantiated to minimize a Bellman "error" with stochastic optimization techniques [37; 19; 36].

Successor features  generalize the notion of a value function from task-specific rewards to task-agnostic features. Given a state feature function \(:S^{d}\), the successor feature of a policy is defined as \(^{}(s)=_{,}[_{t=1}^{}^{t- 1}(s_{i}) s_{1}=s]\). Suppose rewards can be linearly expressed by the features, i.e. there exists \(w^{n}\) such that \(R(s)=w^{}(s)\), then the value function for the particular reward can be linearly expressed by the successor feature \(V^{}(s)=w^{}^{}(s)\). Hence, given the successor feature \(^{}\) of a policy \(\), we can immediately compute its value under any reward once the reward weights \(w\) are known. Analogous to value functions, successor features also admit a recursive Bellman identity \(^{}(s):=(s)+_{}[^{}(s^{}) ],\) allowing them to be estimated using dynamic programming . In this paper, we also refer to the discounted sum of features along a _trajectory_ as a successor feature. In this sense, a successor feature represents an outcome that is feasible under the dynamics and can be achieved by some policy.

**Diffusion Models** DiSPOs rely on expressive generative models to represent the distribution of successor features. Diffusion models [23; 47] are a class of generative models where data generation is formulated as an iterative denoising process. Specifically, DDPM  consists of a forward process that iteratively adds Gaussian noise to the data, and a corresponding reverse process that iteratively denoises a unit Gaussian to generate samples from the data distribution. The reverse process leverages a neural network estimating the score function of each noised distribution, trained with a denoising score matching objective . In addition, one can sample from the conditional distribution \(p(x|y)\) by adding a guidance \(_{x} p(y|x)\) to the score function in each sampling step . As we show in Sec. 4.3, guided diffusion enables quick selection of optimal outcomes from DiSPOs.

**Problem setting** We consider a transfer learning scenario with access to an offline dataset \(=\{(s_{i},a_{i},s^{}_{i})\}_{i=0}^{N}\) of transition tuples collected with some behavior policy \(_{}\) under dynamics \(\). The goal is to quickly obtain the optimal policy \(^{*}\) for some downstream task, specified in the form of a reward function or a number of \((s,r)\) samples. While we cannot hope to extrapolate beyond the dataset (as is common across problems in offline RL ), we will aim to find the best policy _within_ dataset coverage for the downstream task. This is defined more precisely in Section 4.2.

## 4 Distributional Successor Features for Zero-Shot Policy Optimization

We introduce the framework of DiSPOs as a scalable approach to the transfer problem described in Section 3, with the goal of learning from an unlabeled dataset to quickly adapt to any downstream task specified by a reward function. We start by relating the technical details behind learning DiSPOs in Section 4.1, followed by explaining how DiSPOs can be used for efficient multi-task transfer in Section 4.2. Finally, we describe a practical instantiation of DiSPOs in Section 4.3.

### Learning Distributional Successor Features of the Behavior Policy

To transfer and obtain optimal policies across different reward functions, generalist decision-making agents must model the future in a way that permits the evaluation of new rewards _and_ new policies. To this end, DiSPOs adopt a technique based on off-policy dynamic programming to directly model the distribution of cumulative future outcomes, without committing to a particular reward function \(r()\) or policy \(\). Fig. 2 illustrates the two components in DiSPOs, and we describe each below.

**(1) Outcome model:** for a particular a state feature function \((s)\), DiSPOs model the distribution of successor features \(p(|s)\) over all paths that have coverage in the dataset. In deterministic MDPs, each successor feature \(\) (discounted sum of features \(=_{t}^{t-1}(s_{t})\)) can be regarded as an "outcome". When the state features are chosen such that reward for the desired downstream task is a linear function of features, i.e., there exists \(w^{n}\) such that \(r(s)=w^{}(s)\)[42; 43; 66; 56; 9; 3], the value of each outcome can be evaluated as \(w^{}\). That is, knowing \(w\) effectively transforms thedistribution of outcomes \(p(|s)\) into a distribution of task-specific values (sum of rewards) \(p(R|s)\). Notably, since \(w\) can be estimated by regressing rewards from features, distributional evaluation on a new reward function boils down to a simple linear regression.

As in off-policy RL, the outcome distribution \(p(|s)\) in DiSPOs can be learned via an approximate dynamic programming update, which is similar to a distributional Bellman update :

\[_{}&_{(s,a,s^{ })}[ p_{}((s)+_{s^{}}| s)]\\ &_{s^{}} p_{}(|s^ {})\] (1)

Intuitively, this update suggests that the distribution of successor features \(p_{}(|s)\) at state \(s\) maximizes likelihood over current state feature \((s)\) added to sampled future outcomes \(_{s^{}}\). This instantiates a fixed-point procedure, much like a distributional Bellman update. An additional benefit of the dynamic programming procedure is trajectory stitching, where combinations of subtrajectories in the dataset will be represented in the outcome distribution.

**(2) Readout policy:** Modeling the distribution of future outcomes in an environment is useful only when it can be realized in terms of actions that accomplish particular outcomes. To do so, DiSPOs pair the outcome model with a readout policy \((a|s,)\) that actualizes a desired long-term outcome \(\) into the action \(a\) to be taken at state \(s\). Along with the outcome model \(p_{}(|s)\), the readout policy \(_{}(a|s,)\) can be optimized via maximum-likelihood estimation:

\[_{}&_{(s,a,s^{ })}[_{}(a|s,=(s)+_{s^{} })]\\ &_{s^{}} p_{}(.|s^{ })\] (2)

This update states that if an action \(a\) at a state \(s\) leads to a next state \(s^{}\), then \(a\) should be taken with high likelihood for outcomes \(\), which are a combination of the current state feature \((s)\) and future outcomes \(_{s^{}} p_{}(|s^{})\).

The outcome distribution \(p(|s)\) can be understood as a natural analogue to a value function, but with two crucial differences: (1) it represents the accumulation of not just a single reward function but an arbitrary feature (with rewards being linear in this feature space), and (2) it is not specific to any particular policy but represents the distribution over all cumulative outcomes covered in the dataset. The first point enables transfer across rewards, while the second enables the selection of optimal actions for new rewards rather than being restricted to a particular (potentially suboptimal) policy. Together with the readout policy \((a|s,)\), these models satisfy our desiderata for transfer, i.e., that the value for new tasks can be estimated by simple linear regression without requiring autoregressive generation, and that optimal actions can be obtained without additional policy optimization.

### Zero-Shot Policy Optimization with Distributional Successor Features

To synthesize optimal policies for novel downstream reward functions using DiSPOs, two sub-problems must be solved: (1) inferring the suitable linear reward weights \(w_{r}\) for a particular reward function from a set of \((s,r)\) tuples and (2) using the inferred \(w_{r}\) to select an optimal action \(a^{*}\) at a state \(s\). We discuss each below.

**Inferring task-specific weights with linear regression.** As noted, for any reward function \(r(s)\), once the linear reward weights \(w_{r}\) are known (i.e., \(r(s)=w_{r}^{T}(s)\)), the distribution of returns in the dataset \(p(R|s)\) is known through linearity. However, in most cases, rewards are not provided in

Figure 2: DiSPOs for a simple environment. Given a state feature function \(\), DiSPOs learn a distribution of all possible long-term outcomes (successor features \(\)) in the dataset \(p(|s)\), along with a readout policy \((a|s,)\) that takes an action \(a\) to realise \(\) starting at state \(s\).

functional form, making \(w_{r}\) unknown a priori. Instead, given a dataset of \(=\{(s,r)\}\) tuples, \(w_{r}\) can be obtained by solving a simple linear regression problem \(_{w_{r}}|}_{(s,r)}\|w_{r}^{T} (s)-r\|_{2}^{2}\).

**Generating task-specific policies via distributional evaluation.** Given the inferred \(w_{r}\) and the corresponding future return distribution \(p(R|s)\) obtained through linear scaling of \(p(|s)\), the optimal action can be obtained by finding the \(\) with the highest possible future return that has sufficient data-support:

\[^{*}_{} w_{r}^{T}, p (|s),\] (3)

where \(>0\) is a hyperparameter to ensure sufficient coverage for \(\). This suggests that the optimal outcome \(^{*}\) is the one that provides the highest future sum of rewards \(w_{r}^{T}^{*}\) while being valid under the environment dynamics and dataset coverage.

This optimization problem can be solved in a number of ways. The most straightforward is via random shooting , which samples a set of \(\) from \(p(|s)\) and chooses the one with the highest \(w_{r}^{T}\). Sec. 5 bases our theoretical analysis on this technique. Sec. 4.3 shows that for outcome models instantiated with diffusion models, the optimization problem can be simplified to guided diffusion sampling.

Once \(^{*}\) has been obtained, the action to execute in the environment can be acquired via the readout policy \(_{}(a|s,^{*})\). Fig. 3 shows the full policy optimization procedure, and we refer the reader to Appendix. F for the pseudocode.

As described previously, DiSPOs enable zero-shot transfer to arbitrary new rewards in an environment without accumulating compounding error or requiring expensive test-time policy optimization. In this way, they can be considered a new class of models of transition dynamics that avoids the typical challenges in model-based RL and successor features.

### Practical Instantiation

In this section, we provide a practical instantiation of DiSPOs that is used throughout our experimental evaluation. The first step to instantiate DiSPOs is to choose an expressive state feature that linearly expresses a broad class of rewards. We choose the state feature \(\) to be \(d\)-dimensional random Fourier features . Next, the model class must account for the multimodal nature of outcomes and actions since multiple outcomes can follow from a state, and multiple actions can be taken to realize an outcome. To this end, we parametrize both the outcome model \(p(|s)\) and the readout policy \((a|s,)\) using a conditional diffusion model [23; 47]. We then train these models (optimize Equation 1, 2) by denoising score matching, a surrogate of maximum likelihood training .

Remarkably, when \(p(|s)\) is parameterized by a diffusion model, the special structure of the optimization problem in Eq. 3 allows a simple variant of guided diffusion [14; 25] to be used for policy optimization. In particular, taking the log of both sides of the constraint and recasting the constrained optimization via the penalty method, we get a penalized objective \(=w_{r}^{T}+( p(|s)-)\). Taking the gradient yields

\[_{}(,)=w_{r}+_{} p(|s).\] (4)

The expression for \(_{}(,)\) is simply the score function \(_{} p(|s)\) in standard diffusion training (Section 3), with the linear weights \(w_{r}\) added as a guidance term. Planning then becomes doing stochastic gradient Langevin dynamics  to obtain an optimal \(^{*}\) sample, using \(_{}(,)\) as the gradient. Guided diffusion removes the need for sampling a set of particles. As shown in Appendix E, it matches the performance of random shooting while taking significantly less inference time. In Appendix B, we show that the guided diffusion procedure can alternatively be viewed as taking actions conditioned on a soft optimality variable.

Figure 3: Zero-shot policy optimization with DiSPOs. Once a DiSPO is learned, the optimal action can be obtained by performing reward regression and searching for the optimal outcome under the dynamics to decode via the policy.

Theoretical Analysis of Distributional Successor Features for Zero-Shot Policy Optimization

To provide a theoretical understanding of DiSPOs, we conduct an error analysis to connect the error in estimating the ground truth \(p_{0}( s)\) to the suboptimality of the DiSPO policy, and then study when the DiSPO policy becomes optimal. We start our analysis conditioning on \(>0\) estimation error in the ground truth outcome distribution \(p_{0}( s)\).

**Condition 5.1**.: We say the learnt outcome distribution \(\) is an \(\)-good approximation if \(\ s\), \(\|( s)-p_{0}( s)\|_{}\).

Since DiSPOs capture the outcome distribution of the behavior policy \(_{}\), we need a definition to evaluate a policy \(\) with respect to \(_{}\).

**Definition 5.2**.: We say a state-action pair \((s,a)\) is \((,_{})\)-good if over the randomness of \(_{}\), \(_{_{}}[Q^{_{}}(s,a)<_{t=1}^{}^{t-1} r(s_{t}) s_{1}=s]\). Furthermore, if for all state \(s,(s,(s))\) is \((,_{})\)-good, then we call \(\) a \((,_{})\)-good policy.

We proceed to use Definition 5.2 to characterize the suboptimality of the DiSPO policy. Let \(\) denote the sampling optimality of the random shooting planner in Sec. 4.2. Specifically, we expect to sample a top \(\) outcome \(\) from the behavior policy in \(O()\) samples, where \(_{_{}}[w_{}^{T}_{t}^{t-1}r(s_{t})]\). The following result characterizes the suboptimality of the DiSPO policy. The proof is deferred to Appendix. A.

**Theorem 5.3** (main theorem).: _For any MDP \(\) and \(\)-good outcome distribution \(\), the policy \(\) given by the random shooting planner with sampling optimality \(\) is a \((+,_{})\)-good policy._

From Theorem 5.3, we can obtain the following suboptimality guarantee in terms of the value function under the Lipschitzness condition. The corollary shows the estimation error in \(p_{0}( s)\) will be amplified by an \(O()\) multiplicative factor.

**Corollary 5.4**.: _If we have \(\)-Lipschitzness near the optimal policy, i.e., \(Q^{*}(s,a^{*})-Q^{*}(s,a)\) when \((s,a)\) is \((,)\)-good, the suboptimality of output policy \(\) is \(V_{0}^{*}(s_{0})-V_{0}^{}(s_{0})(+)\)._

Lastly, we extend our main theoretical result to the standard full data coverage condition in the offline RL literature, where the dataset contains all transitions . The following theorem states that DiSPOs can output the optimal policy in this case. The proof is deferred to Appendix A.

**Theorem 5.5**.: _In deterministic MDPs, when \(||<\), and \((s,a),N(s,a,(s,a)) 1\), DiSPOs are guaranteed to identify an optimal policy._

## 6 Experimental Evaluation

In our experimental evaluation, we aim to answer the following research questions. (1) Can DiSPOs transfer across tasks without expensive test-time policy optimization? (2) Can DiSPOs avoid the challenge of compounding error present in model-based RL? (3) Can DiSPOs solve tasks with arbitrary rewards beyond goal-reaching problems? (4) Can DiSPOs go beyond the offline dataset, and accomplish "trajectory-stitching" to actualize outcomes that combine different subtrajectories?

We answer these questions through a number of experimental results in simulated robotics problems. We defer detailed descriptions of domains and baselines to Appendix D and C, as well as detailed ablative analysis to Appendix E.

### Problem Domains and Datasets

**Antmaze** is a navigation domain that involves controlling a quadruped to reach some designated goal location. Each task corresponds to reaching a different goal location. We use the D4RL dataset for pretraining and dense rewards described in Appendix D for adaptation.

**Franka Kitchen** is a manipulation domain where the goal is to control a Franka arm to interact with appliances in the kitchen. Each task corresponds to interacting with a set of items. We use the D4RL dataset for pretraining and standard sparse rewards for adaptation.

**Hopper**[7; 9] is a locomotion domain that involves controlling a hopper to perform various tasks, including hopping forward, hopping backward, standing, and jumping. We use the offline dataset from  for pretraining and shaped rewards for adaptation.

**Preference Antmaze** is a variant of D4RL Antmaze  where the goal is to reach the top right corner starting from the bottom left corner. The two tasks in this environment correspond to the two paths to reach the goal, simulating human preferences. We collect a custom dataset and design reward functions for each preference.

**Roboverse** is a tabletop manipulation environment with a robotic arm completing multi-step problems. Each task consists of two phases, and the offline dataset contains separate trajectories of each phase but not full task completion. A sparse reward is assigned to each time step of task completion.

### Baseline Comparisons

**Successor Features** We compare with three methods from the successor feature line of work. **USFA** overcomes the policy dependence of SF by randomly sampling reward weights \(z\) and jointly learning a successor feature predictor \(_{z}\) and a policy \(_{z}\) conditioned on \(z\). \(_{z}\) captures the successor feature of \(_{z}\), while \(_{z}\) is trained to maximize the reward described by \(z\). **FB**[54; 55] follows the same paradigm but jointly learns a feature network by parameterizing the successor measure as an inner product between a forward and a backward representation. **RaMP** removes the policy dependence of SF by predicting cumulative features from an initial state and an open-loop sequence of actions, which can be used for planning.

**Model-Based RL** We compare with two variants of model-based reinforcement learning. **MOPO** is a model-based offline RL method that learns an ensemble of dynamics models and performs actor-critic learning. **COMBO** introduces pessimism into MOPO by training the policy using a conservative objective .

    & DiSPO (Ours) & USFA & FB & RaMP & MOPO & COMBO & GC-IQL \\  umaze & **593**\(\)**16** & 462 \(\) 4 & 469 \(\) 12 & 459 \(\) 3 & 451 \(\) 2 & 574 \(\) 10 & 571 \(\) 15 \\ umaze-diverse & **568**\(\)**12** & 447 \(\) 3 & 474 \(\) 2 & 460 \(\) 7 & 467 \(\) 5 & 547 \(\) 11 & 577 \(\) 7 \\ medium-diverse & **631**\(\)**67** & 394 \(\) 52 & 294 \(\) 61 & 266 \(\) 2 & 236 \(\) 4 & 418 \(\) 16 & 403 \(\) 10 \\ medium-play & **624**\(\)**58** & 370 \(\) 31 & 264 \(\) 29 & 271 \(\) 5 & 232 \(\) 4 & 397 \(\) 12 & 390 \(\) 33 \\ large-diverse & **359**\(\)**59** & 215 \(\) 20 & 181 \(\) 46 & 132 \(\) 1 & 128 \(\) 1 & 244 \(\) 19 & 226 \(\) 9 \\ large-play & **306**\(\)**18** & 250 \(\) 41 & 165 \(\) 12 & 134 \(\) 3 & 128 \(\) 2 & 248 \(\) 4 & 229 \(\) 5 \\  kitchen-partial & **43**\(\) 6 & 0 \(\) 0 & 4 \(\) 4 & 0 \(\) 0 & 8 \(\) 7 & 11 \(\) 9 & - \\ kitchen-mixed & **46**\(\) 5 & 10 \(\) 10 & 5 \(\) 5 & 0 \(\) 0 & 0 \(\) 0 & 0 \(\) 0 & 0 \(\) 0 & - \\  hopper-forward & 566 \(\) 63 & 487 \(\)110 & 452 \(\) 59 & 470 \(\) 16 & 493 \(\)114 & **982**\(\)**157** & - \\ hopper-backward & 367 \(\) 15 & 261 \(\) 68 & 269 \(\) 77 & 220 \(\) 15 & **596**\(\)**211** & 194 \(\) 74 & - \\ hopper-stand & **800**\(\) 0 & 685 \(\)130 & 670 \(\)120 & 255 \(\) 15 & **800**\(\) 0 & 600 \(\)111 & - \\ hopper-jump & **832**\(\)**12** & 746 \(\)112 & 726 \(\) 35 & 652 \(\) 28 & 753 \(\) 51 & 670 \(\)109 & - \\   

Table 1: Offline multitask RL on AntMaze and Kitchen. DiSPOs show superior transfer performance (in average episodic return) than successor features, model-based RL, and misspecified goal-conditioned baselines.

Figure 4: Evaluation domains: (1) D4RL Antmaze  (2) Franka Kitchen  (3) Hopper  (4) Preference-Based Antmaze with the goal of taking a particular path (5) Roboverse  robotic manipulation.

**Goal-Conditioned RL** Goal-conditioned RL enables adaptation to multiple downstream goals \(g\). However, it is solving a more restricted class of problems than RL as goals are less expressive than rewards in the same state space. Moreover, standard GCRL is typically trained on the same set of goals as in evaluation, granting them privileged information. To account for this, we consider a goal-conditioned RL baseline **GC-IQL**[40; 29] and only train on goals from half the state space to show its fragility to goal distributions. We include the original method trained on test-time goals in Appendix E.

### Do DiSPOs enable zero-shot policy optimization across tasks?

We evaluate DiSPOs on transfer problems, where the dynamics are shared, but the reward functions vary. We train DiSPOs on the data distributions provided with the D4RL  and Hopper datasets. We identify the test-time reward by subsampling a small number of transitions from the offline dataset, relabeling them with the test-time rewards, and performing linear least squares regression. While DiSPOs in principle can identify the task reward from online experience, we evaluate in the offline setting to remove the confounding factor of exploration.

Table 1 reports the episodic return on D4RL and Hopper tasks. DiSPOs are able to transfer to new tasks with no additional training, showing significantly higher performance than successor features (mismatch between training and evaluation policy sets), model-based RL (compounding error) and goal-conditioned RL (goal distribution misspecification). Notably, we show in Appendix E that DiSPOs are even competitive with goal-conditioned RL methods trained on test-time goals. The transferability of DiSPOs can also be seen in Fig 5, where we plot the performance of DiSPOs across various tasks (corresponding to different tiles in the maze). We see that DiSPOs have less degradation across tasks than model-based RL .

Although the DiSPO framework and theoretical results are derived under deterministic MDPs, we emphasize that the D4RL antmaze datasets are collected with action noise, emulating stochastic transitions. These results indicate that DiSPOs are practically applicable to some range of stochastic settings, although we expect it to perform better in purely deterministic settings.

### Can DiSPOs solve tasks with _arbitrary_ rewards?

While methods like goal-conditioned RL [40; 17] are restricted to shortest path goal-reaching problems, DiSPOs are able to solve problems with _arbitrary_ reward functions. This is crucial when the reward is not easily reduced to a particular "goal". To validate this, we evaluate DiSPOs on tasks that encode nontrivial human preferences in a reward function, such as particular path preferences in antmaze. In this case, we have different rewards that guide the agent specifically down the path to the left and the right, as shown in Fig 4. As we see in Table 2, DiSPOs and model-based RL obtain policies that respect human preferences and are performant for various rewards. Goal-conditioned algorithms are unable to disambiguate preferences and end up with some probability of taking each path.

    & DiSPO (Ours) & COMBO & GC-IQL \\  Up & 139 \(\) 1 & 143 \(\) 9 & 72 \(\) 19 \\ Right & **142 \(\) 2** & 136 \(\) 4 & 83 \(\) 25 \\   

Table 2: Evaluation on non-goal-conditioned tasks. DiSPOs are able to solve non-goal-conditioned tasks, taking different paths in preference antmaze (Fig 4), while goal-conditioned RL cannot optimize for arbitrary rewards.

Figure 5: Transfer across tasks with DiSPOs and COMBO  in medium antmaze. Each tile corresponds to a different task, with color of the tile indicating the normalized return. DiSPOs successfully transfer across a majority of tasks, while MBRL  struggles on tasks that are further away from the initial location.

### Do DiSPOs perform trajectory stitching?

The ability to recover optimal behavior by combining suboptimal trajectories, or "trajectory stitching," is crucial to off-policy RL methods as it ensures data efficiency and avoids requirements for exponential data coverage. DiSPOs naturally enables this type of trajectory stitching via the distributional Bellman backup, recovering "best-in-data" policies for downstream tasks. To evaluate the ability of DiSPOs to perform trajectory stitching, we consider the environments introduced in . Here, the data only consists of trajectories that complete individual subtasks (e.g. grasping or placing), while the task of interest rewards the completion of both subtasks. Since the goal of this experiment is to evaluate stitching, not transfer, we choose the features as the task rewards \((s)=r(s)\). We find that DiSPOs are able to show non-trivial success rates by stitching together subtrajectories. Since RaMP  predicts the summed features from a sequence of actions, and the optimal action sequence is not present in the dataset, it fails to solve any task. Likewise, return-conditioned supervised learning methods like Decision Transformer  do not stitch together trajectories and fails to learn meaningful behaviors.

## 7 Discussion

This work introduced Distributional Successor Features for Zero-Shot Policy Optimization (DiSPOs), a method for transferable reinforcement learning that does not incur compounding error or test-time policy optimization. By modeling the distribution of _all_ possible future outcomes along with policies to reach them, DiSPOs can quickly provide optimal policies for _any_ reward in a zero-shot manner. We presented an efficient algorithm to learn DiSPOs and demonstrated the benefits of DiSPOs over standard successor features and model-based RL techniques. The limitations of our work open future research opportunities. First, DiSPOs require a choice of features \((s)\) that linearly express the rewards; this assumption may fail, necessitating more expressive feature learning methods. Second, DiSPOs model the behavior distribution of the dataset; hence, policy optimality can be affected by dataset skewness, which motivates the use of more efficient exploration methods for data collection. Finally, the current version of DiSPOs infer the reward from offline state-reward pairs; a potential future direction could apply this paradigm to online adaptation, where the reward is inferred from online interactions.