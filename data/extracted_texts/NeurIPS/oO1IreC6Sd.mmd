# Neural Fields with Hard Constraints

of Arbitrary Differential Order

 Fangcheng Zhong

University of Cambridge

&Kyle Fogarty

University of Cambridge

&Param Hanji

University of Cambridge

&Tianhao Wu

University of Cambridge

&Alejandro Sztrajman

University of Cambridge

&Andrew Spielberg

Harvard University

&Andrea Tagliasacchi

Simon Fraser University

&Petra Bosilj

University of Lincoln

&Cengiz Oztireli

University of Cambridge

###### Abstract

While deep learning techniques have become extremely popular for solving a broad range of optimization problems, methods to enforce hard constraints during optimization, particularly on deep neural networks, remain underdeveloped. Inspired by the rich literature on meshless interpolation and its extension to spectral collocation methods in scientific computing, we develop a series of approaches for enforcing hard constraints on neural fields, which we refer to as _Constrained Neural Fields_ (CNF). The constraints can be specified as a linear operator applied to the neural field and its derivatives. We also design specific model representations and training strategies for problems where standard models may encounter difficulties, such as conditioning of the system, memory consumption, and capacity of the network when being constrained. Our approaches are demonstrated in a wide range of real-world applications. Additionally, we develop a framework that enables highly efficient model and constraint specification, which can be readily applied to any downstream task where hard constraints need to be explicitly satisfied during optimization. Source code is publicly available at https://zfc946.github.io/CNF.github.io/.

## 1 Introduction

Deep neural networks serve as universal function approximators  and have achieved tremendous success across a wide variety of tasks. Gradient-based learning algorithms enable optimizing the parameters of networks to approximate any desired model. However, despite the existence of numerous advanced optimization algorithms, the issue of enforcing strict equality constraints during training has not been sufficiently addressed. Constrained optimization has a broad spectrum of real-world applications. For example, in trajectory optimization ([Chapter 10], and, _e.g._, ), the agent poses at the start, end, and potentially intermediate steps are to be explicitly enforced. In signal representation, it is desirable for the continuous model to interpolate the discrete samples . In physics, the models of interest must comply with fundamental physical principles .

Applying traditional solvers for general constrained optimization, such as SQP , to neural networks can be nontrivial. Since traditional methods express constraints as a function of learnable parameters, this formulation becomes extremely high-dimensional, nonlinear, and non-convex in the context of neural networks. As the number of constraints increases, the computational complexitygrows substantially. Other solutions for deep neural networks may encounter issues related to system conditioning, memory consumption, and network capacity when subject to constraints .

In this work, we consider the following constrained optimization problem:

\[*{arg\,min}_{}\,(f_{};) \,\,[f_{}]( )=g()\,\,,\] (1)

where \(f_{}\) is a parametric model with learnable parameters \(\); and \(\) is a linear operator1 that transforms \(f\) into another function \([f]\) which has the same input and output dimensions as \(f\). Common examples of valid operators \(\) include the identity map, integration, partial differentiation of arbitrary order, and many composite operators that involve differentiation, such as Laplacian, divergence, and advection operators. \(\) can represent a broad class of constraints on the function \(f\) in real-world applications. Enforcement of multiple constraints, expressed as different operators on \(f\), is also possible. In practice, one typically does not have access to (or require) a closed-form expression of \(g\), but rather the evaluation of \(g\) on a set \(\) of discrete samples of \(\). In this work, we focus on equality constraints, as inequality constraints can be more easily satisfied through heavy regularization or a judicious choice of the activation function.

Our solution draws inspiration from meshless interpolation  and its extension to spectral collocation methods  in the scientific computing literature. These methods model a parametric function as a _linear sum of basis functions_. By computing the weights of the bases, hard constraints on the local behavior of the function at the _collocation points_, where the constraints need to be satisfied, are enforced accordingly. However, the selection of the basis functions also determines the inductive bias of the model, _i.e._, the behavior of the function outside the collocation points. Our perspective is to formulate the problem of constrained optimization as a _collocation problem with learnable inductive bias_. Instead of using a set of fixed basis functions, we allow each basis function to have additional learnable parameters. While the weights of the basis functions are used to enforce constraints, other learnable parameters are optimized via a training loss to match the optimal inductive bias.

With this formulation of the problem, we demonstrate a series of approaches for enforcing hard constraints in the form of Eq. 1 onto a neural field, _i.e._, a parametric function modeled as a neural network that takes continuous coordinates as input. In theory, we can formulate basis functions from existing neural fields of any architecture and enforce constraints. Nevertheless, certain network architectures may encounter difficulties as the number of collocation points or the order of differentiation increases. To tackle these challenges, we propose specific network designs and training strategies, and demonstrate their effectiveness in several real-world applications spanning physics, signal representation, and geometric processing. Furthermore, we establish a theoretical foundation that elucidates the superiority of our approach over existing solutions, and extend prior works with shared underlying designs to a broader context. Finally, we develop a PyTorch framework for the highly efficient adoption of Constrained Neural Fields (CNF), which can be applied to any downstream task requiring explicit satisfaction of hard constraints during optimization.

In summary, our work makes the following key contributions:

* We introduce a novel approach for enforcing linear operator constraints on neural fields;
* We propose a series of model representations and training strategies to address various challenges that may arise within the problem context, while offering a theoretical basis for CNF's superior performance and its generalization of prior works;
* Our framework is effective across a wide range of real-world problems; in particular, CNF:
* achieves state-of-the-art performance in learning material appearance representations.
* is the first that can enforce an exact normal constraint on a neural implicit field.
* improves the performance of the Kansa method , a meshless partial differential equation (PDE) solver, when applied to irregular input grids.

## 2 Related Work

Applying constraints to a deep learning model has been a long-standing problem. Numerous studies have aimed to enforce specific properties on neural networks. For instance, sigmoid and softmaxactivation functions are used to ensure that the network output represents a valid probability mass function. Convolutional neural networks (CNNs) were introduced to address translation invariance. Pointnet  was proposed to achieve permutation invariance. POLICE  was introduced to enforce affine properties on the network. However, these works were not specifically designed to handle strict and general-form equality constraints. There are also studies that train a multilayer perceptron (MLP) [34; 37] or neural basis functions [8; 43] with a regression loss to approximate the equality constraints through overfitting, which we refer to as _soft constraint_ approaches. However, in those works, strict adherence to hard constraints is rarely guaranteed. DC3  is the first work that can enforce general equality constraints onto deep learning models by selecting a subset of network parameters for constraint enforcement and utilizing the remaining parameters for optimization. However, this approach cannot offer a theory or guidance regarding the existence of a solution when selecting a random subset of the network. In contrast, our method rigorously analyzes and guarantees the existence of a solution. Linearly constrained neural networks  can also explicitly satisfy linear operator constraints, but their method only applies to the degenerate case where \(g(x)\) (Eq. 1) is zero everywhere. Neural kernel fields (NKF)  employs kernel basis functions similar to ours to solve constrained optimization problems in geometry, but their method cannot accommodate strict higher-order constraints. Their use of a dense matrix also cannot scale up to accommodate a large number of constraints, due to limitations in processor memory. A concurrent work, PDE-CL , utilizes a constrained layer to solve partial differential equations as a constrained optimization problem. We demonstrate that a constrained layer may not be the ideal choice for designing basis functions and propose alternative solutions. In fact, our approach can be seen as a _generalization_ of NKF and PDE-CL. While seemingly unrelated, both works approach constrained optimization problems by leveraging the weights of basis functions to enforce constraints. They differ only in the design of the basis and the optimization function. Finally, there has been substantial development in implicit layers  that can enforce equality constraints between the layer's input and output while efficiently tracking gradients. However, this approach is not specifically designed for constraints over the entire network, although it can be potentially incorporated into our method to facilitate computation.

## 3 Method

At the core of our approach is the representation of a neural field \(f():^{M}^{N}\) as a linear sum of basis functions, as with the collocation methods:

\[f()=_{i}_{i}_{i}(),\] (2)

where each \(_{i}():^{M}^{N}\) represents a basis function that can be expressed as any parametric function with learnable parameters, such as neural networks; \(_{i}^{N}\) is the weight of the \(i\)-th basis; and \(\) indicates the Hadamard product.

### Constrained Optimization

Consider \(:=\{_{i}\}_{i=1}^{I}\), a set of \(I\) constraint points such that \([f](_{i})=g(_{i}),  i\) where the ground truth value of \(g(_{i})\) is accessible. Using the neural basis representation (2), and assuming that \(\) is linear and \([_{i}]\) is well-defined for all \(i\), the following needs to hold for the constraints:

\[[f]()=[_{i}_{i}\,_{i}]()=_{i}_{i}\, [_{i}]()=g(),\;\,.\] (3)

Eq. 3 is a _collocation equation_ in the context of spectral collocation methods for solving differential and integral equations [9; 1]. These constraints can be explicitly satisfied by defining \(I\) basis functions and solving Eq. 3 for their weights. For a linear \(\), we can expand Eq. 3 into batched matrix form:

\[[_{1}](_{1} )&[_{2}](_{1})&&[_{l}](_{1})\\ [_{1}](_{2})&[ _{2}](_{2})&&[_{l} ](_{2})\\ &&&\\ [_{1}](_{l})&[ _{2}](_{l})&&[_{l} ](_{l})}_{_{I}} _{1}\\ _{2}\\ \\ _{I}=g(_{1})\\ g(_{2})\\ \\ g(_{l}).\] (4)

There are in total \(N\) such matrix equations to solve, each corresponding to an output dimension of \(f\) if \(N>1^{2}\). We denote the matrix as \(_{f}\) for reference. As long as we apply a differentiable linear 

[MISSING_PAGE_FAIL:4]

Hypernetwork basisThe third approach is to generate the parameters of each neural basis function using a hypernetwork . The input to the hypernetwork is a one-hot encoding of length equal to the number of bases. The hypernetwork ensures that the network size does not increase with the number of basis functions. Nonetheless, the quadratic growth in the size of \(_{f}\) remains an issue for both the constraint and the hypernetwork bases.

In several empirical studies, we found that the two aforementioned approaches hinder the network's capacities when subjected to hard constraints. They also encountered difficulties reducing the condition number; see Appendix.

Kernel basisAs a result, we propose several variants of a _neural kernel function_ as a special design of basis functions:

\[_{i}()=((_{i}),( )),\] (7)

where \(()\) is a neural encoder that maps \(\) into a high-dimensional feature space; \((,)\) is a kernel function that measures the similarity between two points in the feature space; \(_{i}\) is the \(i\)-th constraint point, which we refer to as an _anchor point_. With this design, the higher-dimensional feature space bolsters the learning capacity of the basis function. The size of parameters in the network also does not grow with the number of constraints. If \((,)\) is a dot-product kernel, Eq. 7 becomes the same basis function used in NKF . However, we empirically found that a Gaussian kernel better promotes the linear independence of basis functions due to its local compact support, while still preserving sufficient learning capacity; see Appendix.

Hypernetwork kernel basisOne drawback of the kernel basis function is that only one constraint can be applied to each anchor point. For example, one cannot enforce constraints on both the value of \(f()\) and its gradient \( f()\) at the same anchor point, as it would result in _repeated_ basis functions \(_{i}_{j}((_{} ),())\) and thus an ill-conditioned system; see Appendix for math details. For multiple constraints at repeated anchor points, we propose a _hypernet kernel function_:

\[_{i}()=(_{i}(_{i}),_{ i}()),\] (8)

where the weights of \(_{i}\) are controlled by a hypernetwork conditioned on the anchor point \(_{i}\). This hypernet construction of the encoder ensures that the network would not have repeated basis functions and the number of network parameters does not grow with the number of basis functions.

Hybrid kernel basisCentral to our approach is the construction and inversion of a linear system, the size of which grows quadratically with the number of constraints. To mitigate severe memory issues in large-scale constraints, we design a basis that promotes sparsity in matrix \(_{f}\) for large-scale problems. While using a Gaussian kernel can promote sparsity, it is difficult to select a proper bandwidth to explicitly control the number of nonzero elements since the kernel operates in the feature space rather than the original domain. To overcome this, we propose a _hybrid kernel function_:

\[_{i}()=(_{i}(_{i}),_{ i}())_{G}(_{i},),\] (9)

where the first part \((_{i}(_{i}),_{i}())\) can be either a Gaussian kernel basis or a hypernet kernel basis, depending on the task, and \(_{G}\) is some compactly supported kernel function, allowing for explicit adjustment of the zero behavior of \(_{i}\). A candidate \(_{G}\) can be a _truncated_ Gaussian kernel such as:

\[_{G}(_{i},)=(-_{i}-\|^{2}}{2^{2}})&\|_{i}-\|<3\\ 0&\|_{i}-\| 3.\] (10)

Reducing the magnitude of \(\) yields a matrix \(_{f}\) with more zero entries, breaking the quadratic dependency on the number of constraint points.

Properties of various basis functions are summarized in Tab. 1. We recommend using the regular Gaussian kernel basis for standard-scale problems without multiple constraints at repeated anchor points, and employing hypernet kernel and hybrid kernel for corresponding advanced scenarios. In Sec. 4, we demonstrate the application of each type of basis function in concrete real-world examples.

### Training Strategies

RegularizationIn many applications, we recommend incorporating an additional loss term to regularize the condition number of the matrix \(_{f}\) during optimization. A low condition number corresponds to a reduced level of matrix singularity and a smaller error bound in the solution vector, which is crucial for satisfying hard constraints.

Transfer learningSince CNF is formulated as a collocation method with a learnable inductive bias, we can pre-train the inductive bias of basis functions with custom configurations, such as well-conditioning, smoothness, or more advanced variations. This allows us to apply the pre-trained basis functions to unseen data, directly computing the weights with little or no additional training, as the problem reduces to a near-collocation scenario. We demonstrate this by solving a partial differential equation on an irregular grid in _inference time_ and _without_ further training; see Appendix.

Sparse solverDrawing inspiration from classical reconstruction techniques [23; 40], we introduce a _patch-based_ approach that leverages the restricted support induced by the hybrid kernel function (Eq. 9) to solve the sparse linear system. For each evaluation point \(\), we construct a subset of basis functions such that \(_{i}()\) using the support criterion induced by \(_{G}\). This enables us to create a lower-dimensional dense submatrix \((_{f})_{_{G}}\) to solve for the weights of only the nonzero items so that the size of the system is sufficiently small to fit within the processor's memory limitations.

## 4 Applications

For efficient adoption of CNF in real-world applications, we have developed a PyTorch framework that abstracts the definition of the basis function, enabling users to conveniently define and experiment with custom architectures. The framework also enables vectorized basis function operations for highly efficient training and inference. Additionally, users have the flexibility to specify an arbitrary linear differential operator constraint by providing a regular expression in LaTeX. This can be processed to compute the exact post-operation function evaluation via automatic differentiation, thereby eliminating the need for manual execution. Further clarification of the features and usage of our framework, along with illustrative examples, is provided alongside the code release.

With this framework, we leverage CNF in four real-world applications, utilizing distinct basis functions tailored to the specific context of the problem. We elucidate and empirically demonstrate the merits of the chosen basis function in each scenario. Our intention is to use these examples as a guideline for selecting appropriate basis functions, which has been briefly summarized at the end of Sec. 3.2. Apart from Sec. 4.1, which serves as a toy example, we demonstrate unique advantages or state-of-the-art results achieved in each application when employing CNF.

### Fermat's Principle

Fermat's principle in optics states that the path taken by a ray between two given points is the path that can be traveled in the least time. This can be formulated as a constrained optimization problem:

\[*{arg\,min}_{}\;_{}}{ v}\;\;=\{_{0},_{1}\},\] (11)

where \(d\) is the differential arc length that the ray travels along contour \(\) and \(v\) is the light speed that varies along the contour due to the refractive index of the medium. The contour's entry and exit points \(\) are hard constraints and the contour is optimized to minimize the travel time \(_{}}{v}\). We model the contour \(\) as a collection of points from a parametric equation, i.e. \(:=\{(x):=_{1}f_{1}(x)+_{2}f_{2}(x) x\}\). As there are in total two constraint points, we use two independent quadratic polynomials \(\{f_{1}\), \(f_{2}\}\) as basis functions. We chose polynomials over neural networks due

    & Independent basis & Constraint basis & Hypernetwork basis & Kernel basis & Kernel basis & Hypernetwork kernel basis & Hybrid kernel basis & Kernel basis \\  Learning capacity & & Poor & Poor & Fair & Good & Good & Good \\  Linear independence & & Poor & Fair & Poor & Good & Good & Good \\  Matrix sparsity & Dense & Dense & Dense & Dense & Sparse & Sparse & Sparse \\  Params \# independent of constraints \# & No & Yes & Yes & Yes & Yes & Yes \\  Controllable sparsity & No & No & No & No & No & No & Yes \\  Multiple constraints at repeated anchor points & Yes & Yes & Yes & No & No & Yes & (if/saving hyperne) \\   

Table 1: Basis functions summary.

to the simplicity of the problem. The weights \(\{_{1},_{2}\}\) are solved differentially to fit hard constraints, while the coefficients of the polynomials are learnable parameters that can be updated with respect to the loss function. We show the results in Fig. 1 with an increasing refractive index along the Y-axis.

### Learning Material Appearance

In visual computing, the synthesis of realistic renderings fundamentally requires an accurate description of the reflection of light on surfaces . This is commonly expressed by the Bidirectional Reflectance Distribution Function (BRDF), described by Nicodemus _et al_. , which quantifies the ratio of incident and outgoing light intensities for a single material: \(f_{r}(_{i},_{o})\), where \(_{i},_{o} S^{2}\) are the incident and outgoing light directions.

Our aim is to learn a neural field \(_{}(_{i},_{o})\) with parameters \(\) that closely matches the ground truth BRDF \(f_{r}(_{i},_{o}),\,_{i},_{ o}^{2}\). We constrain \(_{}\) by selecting \(100\) sample pairs of \((}_{i},}_{o})\), where half of the pairs are uniformly sampled, and the other half of \(}_{o}\) are concentrated around the direction of the perfect reflection of \(}_{i}\). This means that the latter constraint points are situated around the specular highlight, which contains high-frequency details that regular neural networks struggle to learn [32; 31; 19; 35]. We illustrate these issues in Fig. 2, where we compare CNF, formulated with a Gaussian kernel basis (Eq. 7), with SIREN  or FFN  as the encoder network, against multiple state-of-the-art neural-based fitting approaches [34; 37; 35] for BRDF reconstruction. We perform this evaluation on highly specular materials from the MERL dataset , with size-matching networks for a fair assessment, and subsequently rendering a standard fixed scene; see Appendix for the complete MERL rendering results. Following previous work , we train \(_{}\) on \(640\)k \((_{i},_{o})\) samples, minimizing L1 loss in the logarithmic domain to account for the large variation of BRDF values due to the specular highlight, while enforcing the aforementioned hard constraints on \((}_{i},}_{o})\):

\[_{}&\ _{_{i},_{o}}|_{ }(_{i},_{o})-(f_{r}(_{i},_{o})+1)|\\ &\ \ _{}(}_{i},}_{o})=(f_{r}(}_{i},}_{o})+1 ).\] (12)

We summarize the results computed over all MERL materials in Tab. 2, where CNF demonstrates superior performance in learning material appearances with state-of-the-art accuracy; see Appendix for additional evaluations and ablation studies.

### Interpolatory Shape Reconstruction

The representation of shapes via neural implicit functions  has recently garnered much attention [28; 2; 34]. Compared to traditional forms of representation such as point clouds, meshes, or voxel grids, neural implicit functions offer a flexible and continuous representation capable of handling unrestricted topology. In this section, we demonstrate how CNF can be applied to enforce exact interpolation constraints on implicit shape representations, while retaining the ability to train the field with geometry-specific inductive bias. Given an _oriented_ point cloud \(:=\{(_{i},_{i})\}_{i I}\), our aim is to construct a continuous implicit function \(()\) on the domain \(\) whose 0-level-set \(:=\{()=0\}\) represents the surface of the geometry.

Exact normal constraintsWe first demonstrate how our framework can be utilized to learn an implicit shape representation, while exactly interpolating both surface points and their associated

    & \(^{10-2}\) & \(\) & \(\) \\  NBRDF  & 1.57 \(\) 2.86 & 44.96 \(\) 11.55 &.96 \(\).07 \\ FFN  & 1.16 \(\) 1.36 & 44.72 \(\) 10.46 &.98 \(\).03 \\ SIREN  & 3.32 \(\) 3.05 & 33.43 \(\) 8.84 &.93 \(\).08 \\ Kernel FFN & 0.94 \(\) 1.16 & 46.32 \(\) 9.92 &.98 \(\).03 \\ Kernel SIREN & **0.60 \(\) 0.54** & **47.95 \(\) 7.99** & **.99 \(\).01** \\   

Table 2: Quantitative evaluation of learning material appearance representations. We evaluate each method by rendering the learned BRDFs with environment map illumination and computing the mean and standard deviation of image metrics across all materials in the MERL dataset .

Figure 1: The optical paths and travel time before and after the optimization. The constraints are strictly enforced throughout the optimization.

normals. We conduct experiments in 2D, and formulate CNF with a hypernetwork kernel basis (Eq. 8), as the problem involves both zero-order and higher-order constraints at repeated constraint points. Note that this approach is the _first_ to enforce exact normal constraints on a neural implicit field, as prior works  can only approximate the exact normal with a pseudo-normal constraint . In contrast, we constrain the implicit field such that \((_{i})=0,_{i}\) and \((_{i})=_{i},\ _{i},_{i}\). Inspired by the geometrically motivated initialization , we use weights that are pre-trained to represent the signed distance function (SDF) of a circle with the desired number of constraint points. We found this placed the network in a more favorable starting position to produce plausible surfaces under new constraint points. We then train the network to solve the Eikonal equation, following previous work , via minimizing the quadratic loss function:

\[=_{}(\|_{}()\|-1)^ {2},\] (13)

where the expectation is taken with respect to a uniform distribution over the domain \(\). As shown in Fig. 3(a), CNF produces plausible surfaces to explain the point cloud, with the advantage of _not_ depending on manufactured offset points  to ensure the existence of a non-trivial implicit surface.

Large-scale constraintsTo demonstrate our hybrid kernel approach (Eq. 9) with the sparse solver for handling large-scale constraints, we reconstruct 3D point clouds comprising 10,000 points uniformly sampled over the surface. We set the support radius of the kernel to four times the average nearest neighbor distance, and assign a constant value of \(10^{5}\) to regions of space with no support. The reconstruction is obtained by adhering to the points and pseudo-normal constraints  during initialization, with no additional training required for the dense reconstruction to produce smooth surfaces, as shown in Fig. 3(b); see Appendix for additional results and evaluations.

Figure 3: (a): Learning shapes from 2D point clouds (_yellow points_) with oriented normals (_black arrows_) using CNF; black lines depict the zero level sets of the learned implicit field. The method interpolates both exact point and normal constraints during initialization and throughout training, yielding a plausible surface upon minimizing the Eikonal constraint across the field. (b): Surface reconstruction of 3D point clouds containing 10,000 points.

Figure 2: Learned material appearance by enforcing hard constraints around the specular highlights.

[MISSING_PAGE_FAIL:9]

The skewed RBF is essentially another variant of the Gaussian kernel (Eq. 7) but without the neural encoder. We recommend using neural networks only when the behavior of the basis functions to be optimized away from the constraint points is highly complex, as seen in the BRDF and shape-reconstruction examples. For problems where the desirable priors are as simple as linear independence and smoothness, such as solving PDEs, our proposed skewed RBF demonstrates sufficient capacity.

## 5 Discussion

Training and inference efficiencyCNF is highly efficient in training (minutes) and inference (seconds); see Appendix for detailed reports, learning curves, and additional evaluations.

Theoretical analysisWe offer a theoretical basis for CNF's superior performance:

Comparison with general solversMost numerical methods for constrained optimization in scientific computing, including the popular SQP  and the more recent DC3 , adopt the following formulation for the constrained optimization problem:

\[*{arg\,min}_{}\,() h_{1}()=0,\,\,h_{2}()=0,\,\,...\] (17)

where \(\) denotes the learnable parameters. For problems where \(\) represents the weights of a deep neural network, these constraints become extremely high-dimensional and nonlinear, especially as the number of constraints increases. In this formulation, a constraint is only linear when \(h()\) is a linear function of \(\). Therefore, linearity no longer holds as \(h()\) involves constraints of a neural function. In contrast, our formulation (Eq. 1, 3, and 5) only requires the operator \(\) to be linear, while the neural function \(f\) can still be highly nonlinear with respect to \(\). Hence, CNF significantly reduces the problem's complexity, enabling the explicit determination and promotion of a solution's existence.

Comparison with soft constraint approachesWhile there have been attempts to model constraints by overfitting a neural function trained with regression [34; 37], CNF offers several clear advantages over such soft approaches. Most notably, CNF satisfies hard constraints without the need for training, whereas soft approaches may require extensive training to approximate constraints. Furthermore, despite extensive training, soft approaches may still fail to satisfy hard constraints due to inherent limitations in their learning capacity. In contrast, CNF provides a robust guarantee of hard constraint satisfaction within machine precision error, provided the condition number is small. Another drawback of employing soft approaches becomes evident when effectively imposing priors. The incorporation of priors often involves introducing additional terms to the loss. Consequently, a tradeoff arises between the constraints and the prior, which is controlled by a hyperparameter. In contrast, CNF offers a clean solution without such a tradeoff. With CNF, the inclusion of priors does not compromise hard constraints, thereby maintaining a harmonious balance among various aspects of the model.

Generalization of NKF and PDE-CLCNF generalizes prior works NKF  and PDE-CL . NKF employed a dot-product kernel basis for 3D reconstruction, while PDE-CL used a constraint basis to solve PDEs. Yet, these bases exhibit suboptimal performance in terms of linear independence and learning capacity. Their use of dense matrices also renders them impractical for large-scale problems. Moreover, the dot-product kernel used by NKF cannot address strict higher-order constraints; see Appendix. We propose several novel variants of basis functions to enhance linear independence, learning capacity, and matrix sparsity, with strategies for analyzing and facilitating convergence. Our methodology unifies NKF and PDE-CL, extending their formulations to a broader theoretical context.

Extension to nonlinear operator constraintsAlthough we restrict the operator to be linear in this work, CNF can be extended to accommodate nonlinear operators by solving a nonlinear version of Eq. 3, provided that the solver is differentiable. This could be accomplished through the use of implicit layers , which we leave as future work.

## 6 Summary

We introduce Constrained Neural Fields (CNF), a method that imposes linear operator constraints on deep neural networks, and demonstrate its effectiveness through real-world examples. As constrained optimization is a fundamental problem across various domains, we anticipate that CNF will unlock a host of intriguing applications in fields such as physics, engineering, finance, and visual computing.