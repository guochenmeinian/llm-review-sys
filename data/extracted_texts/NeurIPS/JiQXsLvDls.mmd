# Mutual Information Estimation via Normalizing Flows

Butakov I. D.

Skoltech, MIPT, Sirius,

butakov.id@phystech.su

&Tolmachev A. D.

Skoltech, MIPT

tolmachev.ad@phystech.su

Malanchuk S. V.

MIPT, Skoltech

malanchuk.sv@phystech.su

&Neopryatnaya A. M.

MIPT, Skoltech

neopryatnaya.am@phystech.su

Frolov A. A.

Skoltech

al.frolov@skoltech.ru

Skoltech

###### Abstract

We propose a novel approach to the problem of _mutual information_ (MI) estimation via introducing a family of estimators based on normalizing flows. The estimator maps original data to the target distribution, for which MI is easier to estimate. We additionally explore the target distributions with known closed-form expressions for MI. Theoretical guarantees are provided to demonstrate that our approach yields MI estimates for the original data. Experiments with high-dimensional data are conducted to highlight the practical advantages of the proposed method.

## 1 Introduction

Information-theoretic analysis of deep neural networks (DNN) has attracted recent interest due to intriguing fundamental results and new hypotheses. Applying information theory to DNNs may provide novel tools for explainable AI via estimation of information flows , as well as new ways to encourage models to extract and generalize information . Useful applications of information theory to the classical problem of independence testing are also worth noting .

Figure 1: We propose transforming a pair of random vectors (RVs) via a Cartesian product of learnable diffeomorphisms to facilitate mutual information (MI) estimation. Ideally, we achieve tractable MI in the latent space. As diffeomorphisms preserve information, MI between latent representations equals MI between the original RVs.

Most of the information theory applications to the field of machine learning are based on the two central information-theoretic quantities: _differential entropy_ and _mutual information_ (MI). The latter quantity is widely used as an invariant measure of the non-linear dependence between random variables, while differential entropy is usually viewed as a measure of randomness. However, as it has been shown in the previous works , MI and differential entropy are extremely hard to estimate in the case of high-dimensional data. It is argued that such estimation is also challenging for long-tailed distributions and large values of MI . These problems considerably limit the applications of information theory to real-scale machine learning problems. However, recent advances in the neural estimation methods show that complex parametric estimators achieve relative practical success in the cases where classical MI estimation techniques fail .

This paper addresses the mentioned problem of the mutual information estimation in high dimensions via using normalizing flows . Some recent works also utilize generative models to estimate MI. According to a general generative approach described in , generative models can be used to reconstruct probability density functions (PDFs) of marginal and joint distributions to estimate differential entropy and MI via a Monte Carlo (MC) integration. However, as it is mentioned in the original work, when flow-based generative models are used, the resulting estimates are poor even when the data is of simple structure. This approach is further investigated in . The estimator proposed in  uses score-based diffusion models to estimate the differential entropy and MI without an explicit reconstruction of the PDFs. Increased accuracy of this estimator comes at a cost of training score networks and using them to compute an MC estimate of Kullback-Leibler divergence (KLD). Finally, in  normalizing flows are used to transform marginal distributions into Gaussian distributions, after which a zero-correlation criterion is employed to test the zero-MI hypothesis. The same idea is later used in  (see DINE-Gaussian) to acquire an MI estimate, but no corresponding error bounds are possible to derive, as knowing marginal distributions only is insufficient to calculate the MI (see Remark 4.5), which makes this estimator substantially flawed. We also note the work, where normalizing flows are combined with a \(k\)-NN entropy estimator .

In contrast, our method allows for simplified (cheap and low-variance MC integration is required) or even _direct_ (i.e., no MC integration, nearest neighbors search or other similar data manipulations are required) and the accurate MI estimation with asymptotic and non-asymptotic error bounds. Our contributions in this work are the following:

1. We propose a MI-preserving technique to simplify the joint distribution of two random vectors (RVs) via a Cartesian product of trainable normalizing flows in order to facilitate the MI estimation. Non-asymptotic error bounds are provided, with the gap approaching zero under certain commonly satisfied assumptions, showing that our estimator is consistent.
2. We suggest restricting the proposed MI estimator to allow for a _direct_ MI calculation via a _simple closed-form formula_. We further refine our approach to require only \(O(d)\) additional learnable parameters to estimate the MI (here \(d\) denotes the dimension of the data). We provide additional theoretical and statistical guarantees for our restricted estimator: variance and non-asymptotic error bounds are derived.
3. We validate and evaluate our method via experiments with high-dimensional synthetic data with known ground truth MI. We show that the proposed MI estimator performs well in comparison to the ground truth and some other advanced estimators during the tests with high-dimensional compressible and incompressible data of various complexity.

This article is organized as follows. In Section 2, the necessary background is provided and the key concepts of information theory are introduced. Section 3 describes the general method and corresponding theoretical results. In Section 4 we restrict our method to allow for accurate MI estimation via a closed-form formula. In Section 5, a series of experiments is performed to evaluate the proposed method and compare it to several other key MI estimators. Finally, the results are discussed in Section 6.

We provide all the proofs in Appendix A, additional details on the benchmarks we use in Appendix B, overfitting analysis in Appendix C, supplementary results regarding the information-based disentanglement of real data in Appendix D, and technical details in Appendix E.

Preliminaries

Consider random vectors, denoted as \(X^{n}\) and \(Y^{m}\), where \(\) represents the sample space. Let us assume that these random vectors are absolutely continuous, having probability density functions (PDF) denoted as \(p(x)\), \(p(y)\), and \(p(x,y)\), respectively, where the latter refers to the joint PDF. The differential entropy of \(X\) is defined as follows:

\[h(X)=-} p(x)=-_{X}p (x) p(x)\,dx,\]

where \(X^{n}\) represents the _support_ of \(X\), and \(()\) denotes the natural logarithm. Similarly, we define the joint differential entropy as \(h(X,Y)=-} p(x,y)\) and conditional differential entropy as \(h(X Y)=-} p(X|Y)=- }_{Y}(}_{X|Y=y}  p(X Y=y))\). Finally, the mutual information (MI) is given by \(I(X;Y)=h(X)-h(X Y)\), and the following equivalences hold

\[I(X;Y)=h(X)-h(X Y)=h(Y)-h(Y X), \]

\[I(X;Y)=h(X)+h(Y)-h(X,Y), \]

\[I(X;Y)=}(p_{X,Y}\,\|\,p_{X} p_{Y}) \]

Mutual information can also be defined as an expectation of the _pointwise mutual information_:

\[_{X,Y}(x,y)=[], I (X;Y)=}_{X,Y}(X,Y) \]

The above definitions can be generalized via Radon-Nikodym derivatives and induced densities in case of distributions supports being manifolds, see .

The differential entropy estimation is a separate classical statistical problem. Recent works have proposed several novel ways to acquire the estimate in the high-dimensional case [12; 18; 20; 27; 28; 29; 30]. Due to Equation (2), mutual information can be found by estimating entropy values separately. In contrast, this paper suggests an approach that estimates MI values directly.

We also have to mention the well-known fundamental property of MI, which is invariance under smooth injective mappings. The following theorem appears in literature in slightly different forms [14; 19; 31; 32; 33]; we utilize the one, which is the most convenient to use with normalizing flows.

**Theorem 2.1**.: _Let \(^{n^{}}\) be an absolutely continuous random vector, and let \(g^{n^{}}^{n}\) be an injective piecewise-smooth mapping with Jacobian \(J\), satisfying \(n n^{}\) and \((J^{T}J) 0\) almost everywhere. Let PDFs \(p_{}\) and \(p_{}\) exist. Then_

\[_{,}(,)}}{{= }}_{g(),}(g(),), I(;)=I(g( );) \]

In our work, we heavily rely on the _normalizing flows_[23; 24] - trainable smooth bijective mappings with tractable Jacobian. However, to understand our results, it is sufficient to know that flow models (a) satisfy the conditions on \(g\) in Theorem 2.1_by definition_, (b) can model any absolutely continuous Borel probability measure (_universality_ property) and (c) are trained via a likelihood maximization, which is equivalent to a Kullback-Leibler divergence minimization. For more details, we refer the reader to a more complete and rigorous overview of normalizing flows provided in .

## 3 General method

Our task is to estimate \(I(X;Y)\), where \(X\), \(Y\) are random vectors. Here we focus on the absolutely continuous \((X,Y)\), as it is the most common case in practice. Note that Theorem 2.1 allows us to train normalizing flows \(f_{X},f_{Y}\), apply them to \(X\), \(Y\) and consider estimating MI between the latent representations, as \(I(f_{X}(X);f_{Y}(Y))=I(X;Y)\).

The key idea of our method is to train \(f_{X}\) and \(f_{Y}\) in such a way that \(I(f_{X}(X);f_{Y}(Y))\) is easy to estimate. For example, one can hope to acquire tractable pointwise mutual information (PMI), which can be then averaged via MC integration . Unfortunately, the PMI invariance (Theorem 2.1) restricts the possible distributions of \((f_{X}(X),f_{Y}(Y))\) to an unknown family, making the exact MI recovery via such technique unfeasible.

However, one can always approximate the PDF in latent space via a (preferably, simple) model \(q\) with tractable PMI, and train \(q\), \(f_{X}\) and \(f_{Y}\) to minimize the discrepancy between the real and the proposed PMI. The complexity of \(q\) serves as a tradeoff: by selecting a poor \(\), one might experience a considerable bias of the estimate; on the other hand, choosing \(\) to be a universal PDF approximation family, one acquires a consistent, but computationally expensive MI estimate. Flows \(f_{X},f_{Y}\) are used to tighten the approximation bound. We formalize this intuition in the following theorems:

**Theorem 3.1**.: _Let \((,)\) be absolutely continuous with PDF \(p_{,}\). Let \(q_{,}\) be a PDF defined on the same space as \(p_{,}\). Let \(p_{}\), \(p_{}\), \(q_{}\) and \(q_{}\) be the corresponding marginal PDFs. Then_

\[I(;)=_{_{,}}[(,)}{q_{}()q_{}()}]}_{I_{q}(;)}+ _{}(p_{,}\,\|\,q_{,})- _{}(p_{} p_{}\,\|\,q_{} q_{}) \]

**Corollary 3.2**.: _Under the assumptions of Theorem 3.1, \(|I(;)-I_{q}(;)|_{}(p_{,}\,\| \,q_{,})\)._

This allows us to define the following MI estimate:

\[_{}(\{(x_{k},y_{k})\}_{k=1}^{N})_{}(_{X}(X);_{Y}(Y))=_{k=1}^{N}[_{,}(_{X}(x_{k}),_{Y}(y_{k}))}{_{}(_{X}(x_{k}))_{}(_{Y}(y_{k}))}], \]

where \(\{(x_{k},y_{k})\}_{k=1}^{N}\) is a sampling from \((X,Y)\), and \(\), \(_{X}\) and \(_{Y}\) are selected according to the maximum likelihood. The latter makes \(_{}\) a consistent estimator:

**Corollary 3.3** (\(_{}\) is consistent).: _Let \(X\), \(Y\), \(_{X}^{-1}\) and \(_{Y}^{-1}\) satisfy the conditions of Theorem 2.1. Let \(\{(x_{k},y_{k})\}_{k=1}^{N}\) be an i.i.d. sampling from \((X,Y)\). Let \(\) be a family of universal PDF approximators for a class of densities containing \(_{X,Y}(f_{X}^{-1} f_{Y}^{-1})\) (pushforward probability measure in the latent space), meaning the convergence in probability of a maximum-likelihood estimate from \(\) to the ground-truth distribution if \(N\) increases. Let \(_{N}\) be a maximum-likelihood estimate of \(_{X,Y}(f_{X}^{-1} f_{Y}^{-1})\) from the samples \(\{(f_{X}(x_{k}),f_{Y}(y_{k}))\}_{k=1}^{N}\). Let \(I_{_{N}}(f_{X}(X);f_{Y}(Y))\) exist for every \(N\). Then_

\[_{}(\{(x_{k},y_{k})\}_{k=1}^{N})}}{{}}I(X;Y)\]

Note that maximum-likelihood training of \(f_{X}\), \(f_{Y}\) also minimizes \(_{}(_{X,Y}(f_{X}^{-1} f_{Y}^{- 1})\,\|\,_{,})\), which allows for surprisingly simple \(q\) to be used, as we show in the subsequent sections.

The described approach is as general as possible. We use it as a starting point for a development of a more elegant, cheap and practically sound MI estimator. We also do not incorporate conditions, under which the universality property of \(\) holds, as they depend on the choice of \(\); if one is interested in using normalizing flows as \(\), we refer to Section 3.4.3 in  or to  for more details.

## 4 Using Gaussian base distribution

Note that the general approach requires finding the maximum-likelihood estimate \(\) and using it to perform an MC integration to acquire \(_{}(f_{X}(X);f_{Y}(Y))\).

In this section, we drop these requirements by restricting our estimator via choosing \(\) to be a family of multivariate Gaussian PDFs. This allows us (a) to _directly_ estimate the MI via a _closed-form_ expression, (b) to employ a closed-form expression for optimal \(\), (c) to leverage the maximum entropy principle for Gaussian distributions, thus acquiring better non-asymptotic bounds, and (d) to analyze the variance of the proposed estimate.

**Theorem 4.1** (Theorem 8.6.5 in ).: _Let \(Z\) be a \(d\)-dimensional absolutely continuous random vector with probability density function \(p_{Z}\), mean \(m\) and covariance matrix \(\). Then_

\[h(Z)=h((m,))-_{}(p_{Z}\, \|\,(m,))=(2 e)+ -_{}(p_{Z}\,\|\,(m,))\]

_Remark 4.2_.: Note that \(h(Z)\) may not be equal to \(h(Z^{})-_{}(p_{Z}\,\|\,p_{Z^{}})\) for arbitrary \(Z^{}\).

**Corollary 4.3**.: _Let \((,)\) be an absolutely continuous pair of random vectors with joint and marginal probability density functions \(p_{,}\), \(p_{}\) and \(p_{}\) correspondingly, and mean and covariance matrix being_

\[m=m_{}\\ m_{},=_{,}&_{, }\\ _{,}&_{,}\]

_Then_

\[I(;)=[_{,}+ _{,}-]+\\ +}(p_{,}\|(m,) )-}(p_{} p_{}\|(m, (_{,},_{,})),..\]

_which implies the following in the case of marginally Gaussian \(\) and \(\):_

\[I(;)[_{,}+_{,}-], \]

_with the equality holding if and only if \((,)\) are jointly Gaussian._

**Corollary 4.4**.: _Under the assumptions of Corollary 4.3,_

\[|I(;)-[_{,}+_{ ,}-]|}(p_{,} \|(m,)).\]

_Remark 4.5_.: The upper bound from Corollary 4.4 is tight, consider \((0,1)\), \(=(2B-1)\), where \(B(1/2)\) and is independent of \(\).

From now on we denote \(f_{X}(X)\) and \(f_{Y}(Y)\) as \(\) and \(\) correspondingly. Note that, in contrast to Theorem 3.1 and Corollary 3.2, \(I_{q}(;)\) is replaced by a closed-form expression, which is not possible to achieve in general. The provided closed-form expression allows for calculating MI for jointly Gaussian \((,)\), and serves as a lower bound on MI in the general case of \(\) and \(\) being only marginally Gaussian.

### General binormalization approach

In order to minimize \(}(p_{,}\|(m,)))\), we train \(f_{X} f_{Y}\) as a single normalizing flow. Instead of maximizing the log-likelihood using two separate and fixed base (latent) distributions, we maximize the log-likelihood of the joint sampling \(\{(x_{k},y_{k})\}_{k=1}^{N}\) using the whole set of Gaussian distributions as possible base distributions.

**Definition 4.6**.: We denote a set of \(d\)-dimensional Gaussian distributions as \(S_{}^{d}\{(m,) m^ {d},^{d d}\}\).4

**Definition 4.7**.: The log-likelihood of a sampling \(\{z_{k}\}_{k=1}^{N}\) with respect to a set of absolutely continuous probability distributions \(S\) is defined as follows:

\[_{S}(\{z_{k}\})_{ S}_{}(\{z_{k} \})=_{ S}_{k=1}^{N}[()(z_{k})]\]

Let us define \(f f_{X} f_{Y}\) (Cartesian product of flows) and \(S f=\{ f S\}\) (set of pushforward measures). In our case, \(_{S_{} f}(\{(x_{k},y_{k})\})\) can be expressed in a closed-form via the change of variables formula (identically to a classical normalizing flows setup) and maximum-likelihood estimates for \(m\) and \(\).

**Statement 4.8**.: \[_{S_{}(f_{X} f_{Y})}(\{(x_{k},y_{k})\} )=||+_{(,)}(\{f(x_{k},y_{k})\}),\]

_where_

\[||=| (x)}{ x}|+|(y)}{ y}|,\]

\[=_{k=1}^{N}f(x_{k},y_{k}),=_{k=1}^{N}(f(x_{k},y_{k})-)(f(x_{k},y_{k})-)^{T}\]Maximization of \(_{S_{}(f_{X} f_{Y})}(\{(x_{k},y_{k})\})\) with respect to parameters of \(f_{X}\) and \(f_{Y}\) minimizes \(_{}(p_{,}\,\|\,(m,))\), making it possible to apply Theorem 2.1 and Corollary 4.3 to acquire an MI estimate with corresponding non-asymptotic error bounds from Corollary 4.4:

\[_{}(\{(x_{k},y_{k})\}_{k=1}^{N}) {1}{2}[_{,}+_{,}- ] \]

Note that if only marginal Gaussianization is achieved, Equation (9) serves as a lower bound estimate. As \(_{}\) involves maximum-likelihood estimates of covariance matrices, existing results can be employed to acquire the asymptotic variance:

**Lemma 4.9** (Lemma 2 in ).: _Let \(f_{X},f_{Y}\) be fixed. Let \((,)\) have finite covariance matrix. Then, the asymptotic variance of \(_{}\) is \(O(d^{2}/N)\), with \(d\) being the dimensionality. If \((,)\) is also Gaussian, the asymptotic variance is further improved to \(O(d/N)\)._

### Refined approach

Although the proposed general method is compelling, as it requires only the slightest modifications to the conventional normalizing flow setup to make the application of the closed-form expressions for MI possible, we have to mention several drawbacks.

Firstly, the log-likelihood maximum is ambiguous, as \(_{S_{}}\) is invariant under invertible affine mappings, which makes the proposed log-likelihood maximization an ill-posed problem:

_Remark 4.10_.: Let \(A^{d d}\) be a non-singular matrix, \(b\), \(\{z_{k}\}_{k=1}^{N}^{d}\). Then

\[_{S_{}(Az+b)}(\{z_{k}\})=_{S_{}}(\{z_{k}\})\]

Secondly, this method requires a regular (ideally, after every gradient descent step) updating of \(\) and \(\) for the whole dataset, which is expensive. In practice, these estimates can be replaced with batchwise maximum likelihood estimates, which are used to update \(\) and \(\) via exponential moving average (EMA). This approach, however, requires tuning EMA multiplication coefficient in accordance with the learning rate to make the training fast yet stable. We also note that \(\) and \(\) can be made learnable via the gradient ascent, but the benefits of the closed-form expressions for \(_{S_{}}\) in Statement 4.8 are thus lost.

Finally, each loss function evaluation requires inversion of \(\), and each MI estimation requires evaluation of \(\) and determinants of two diagonal blocks of \(\). This might be resource-consuming in high-dimensional cases, as matrices may not be sparse. Numerical instabilities might also occur if \(\) happens to be ill-conditioned (might happen in the case of data lying on a manifold or due to high MI).

That is why we propose an elegant and simple way to eliminate all the mentioned problems by further narrowing down \(S_{}\) to a subclass of Gaussian distributions with simple and bounded covariance matrices and fixed means. This approach is somewhat reminiscent of the non-linear canonical correlation analysis .

**Definition 4.11**.: \[S_{},}^{d_{},d_{}}\{ (0,)_{,}=I_{d_{}},_{,}=I_{d_ {}},_{,}(_{,}^{T})=(\{_{ j}\})^{d_{} d_{}},_{j}[0;1)\}\]

This approach solves all the aforementioned problems without any loss in generality, as it is shown by the following results:

**Corollary 4.12**.: _If \((,) S_{},}\), then_

\[I(;)=-_{j}(1-_{j}^{2}) \]

**Statement 4.13** (Canonical correlation analysis).: _Let \((,)(m,)\), where \(\) is non-singular. There exist invertible affine mappings \(_{},\,_{}\) such that \((_{}(),_{}()) S_{}, }\). Due to Theorem 2.1, the following also holds: \(I(;)=I(_{}();_{}())\)._

**Statement 4.14**.: _Let \((,)(0,) S_{}}\), \(\{z_{k}\}_{k=1}^{N}^{d_{}+d_{}}\). Then_

\[_{(0,)}(\{z_{k}\})=I(;)+_{N(0,I)} (\{^{-1/2}z_{k}\}),\]

_where (implying \(_{j}=0\) for \(j>\{d_{},d_{}\}\))_

\[^{-1/2}=[_{j}+_{j}&_{j}- _{j}&\\ &&\\ _{j}-_{j}&_{j}+_{j}&\\ &&\\ ]_{j}=}}\\ _{j}=}}\]

_and \(I(;)\) is calculated via (10)._

Maximization of \(_{S_{}}(f_{X} f_{Y})}\) (\(\{(x_{k},y_{k})\}\)) with respect to the parameters of \(f_{X}\) and \(f_{Y}\) yields the following MI estimate:

\[_{}}(\{(x_{k},y_{k})\}_{k=1}^{ N})-_{j}(1-_{j}^{2}), \]

where \(_{j}\) are the maximum-likelihood estimates of \(_{j}\).

### Tractable error bounds

Note that Corollary 3.2 and Corollary 4.4 provide us with non-asymptotic, but untractable bounds. These bounds can be estimated via various KL divergence estimators . However, this requires training an additional neural network, which is computationally expensive.

Conveniently, as the proposed method involves maximization of the likelihood, a cheap and tractable lower bound on the KL divergence can be obtained via an entropy-cross-entropy decomposition:

\[_{}(p\,\|\,q)=_{Z p}=-\,_{Z p} q(Z)-h(Z)-\,_{Z p} q (Z)-h((m,)) \]

Note that \( q(Z)\) in Equation (12) is estimated by the log-likelihood of the samples in the latent space (which is inevitably evaluated each training step), and \(h(Z)\) can be upper bounded by the entropy of a Gaussian distribution (see Theorem 4.1). Unfortunately, as Theorem 4.1 has already been employed to derive Corollary 4.3, the proposed bound is trivial (equaling to zero) in our Gaussian-based setup. However, this idea might still be useful in the general case.

### Implementation details

In this section, we would like to emphasize the computational simplicity of the proposed amendments to the conventional normalizing flow setup.

Firstly, Statement 4.14 allows for a cheap log-likelihood computation and sample generation, as \(\), \(^{-1/2}\) and \(^{-1}\) are easily calculated from the \(\{_{j}\}\) and are sparse (tridiagonal, block-diagonalizable with \(2 2\) blocks). Secondly, the method requires only \(d^{}=\{d_{},d_{}\}\) additional parameters: the estimates for \(\{_{j}\}\). As \(_{j}[0;1)\), an appropriate parametrization should be chosen to allow for stable learning of \(\{_{j}\}\) via the gradient ascend. We propose using the _logarithm of cross-component MI5_: \(w_{j} I(_{j};_{j})=[-(1-_{j} ^{2})]\). In this parametrization \(w_{j}\) and

\[_{}}(\{(x_{k},y_{k})\}_{k=1}^{ N})=_{j}e^{_{j}},_{j}=})}(0;1) \]

Although \(_{j}=0\) is not achievable in the proposed parametrization, it can be made arbitrarily close to \(0\) with \(w_{j}-\).

### Extension to non-Gaussian base distributions and non-bijective flows

The proposed method can be easily generalized to account for any base distribution with closed-form expression for MI, or even a combination of such distributions. For example, a smoothed uniform distribution can be considered, with the learnable parameter being the smoothing constant \(\), see Appendix B.2, Equation (14). However, due to Remark 4.2, neither Corollary 3.2, nor Corollary 4.4 can be used to bound the estimation error in this case.

Also note that, as Theorem 2.1 does not require \(g\) to be bijective, our method is naturally extended to injective normalizing flows [41; 42]. Moreover, according to , such approach may actually facilitate the estimation of MI.

## 5 Experiments

To evaluate our estimator, we utilize synthetic datasets with known mutual information. In  and , extensive frameworks for evaluation of MI estimators have been proposed. In our work, we borrow complex high-dimensional tests from  and all non-Gaussian base distributions with known MI from  (see Appendix B for more details). The formal description of the dataset generation and estimator evaluation is provided in Algorithm 1. Essentially similar setups are widely used to test the MI estimators [7; 13; 19; 20; 31; 43].

```
1:Generate two datasets of samples from random vectors \(\) and \(\) with known ground truth mutual information (see Corollary 4.3, Corollary 4.12 and Appendix B for examples): \(\{(a_{k},b_{k})\}_{k=1}^{N}\).
2:Choose functions \(g_{}\) and \(g_{}\) satisfying conditions of Theorem 2.1, so \(I(;)=Ig_{}();g_{}()\).
3:Estimate \(Ig_{}();g_{}()\) via \(\{(g_{}(a_{k}),g_{}(b_{k}))\}_{k=1}^{N}\); compare the result with the ground truth.
```

**Algorithm 1** MI estimator evaluation

For the first set of experiments, we map a low-dimensional correlated Gaussian distribution to a distribution of high-dimensional images of geometric shapes (see Figure 2). We compare our method with the Mutual Information Neural Estimator (MINE) , Nguyen-Wainwright-Jordan (NWJ) [7; 39] and Nishiyama's  estimators, nearest neighbor Kraskov-Stoegbauer-Grassberger  and \(5\)-nearest neighbors weighted Kozachenko-Leonenko (WKL) estimator [27; 44]; the latter is fed with the data compressed via a convolutional autoencoder (CNN AE) in accordance to the pipeline from .

For the second set of experiments, incompressible, high-dimensional non-Gaussian-based distributions are considered. These experiments are conducted to evaluate the robustness of our estimator to the distributions, which can not be precisely Gaussianized via a Cartesian product of two flows. In this section, we compare our method to the ground truth value only. We also provide a comparison with MINE and DINE-Gaussian  in Appendix B. For a more elaborate benchmarking of other estimators on these distributions, we refer the reader to .

For the tests with synthetic images, we use GLOW  normalizing flow architecture with \(_{2}()\) splits, \(2\) blocks between splits and \(16\) hidden channels in each block, appended with a learnable orthogonal linear layer and a small \(4\)-layer Real NVP flow . For the other tests, we use \(6\)-layer Real NVP architecture. For further details (including the architecture of MINE critic network and CNN autoencoder), we refer the reader to Appendix E.

The results of the experiments performed with the high-dimensional synthetic images and non-Gaussian-based distribution are provided in Figure 3 and Figure 4 correspondingly.

Figure 2: Examples of synthetic images used in the tests. Note that images are high-dimensional, but admit latent structure, which is similar to real datasets.

Figure 4: Tests with incompressible multidimensional data. “Uniform” denotes the uniformly distributed samples acquired from the correlated Gaussians via the Gaussian CDF. “Smoothed uniform” and “Student” denote the non-Gaussian-based distributions described in Appendix B. “arcsinh(Student)” denotes the \(\) function applied to the “Student” example (this is done to avoid numerical instabilities in the case of long-tailed distributions). We run each test \(5\) times and plot \(99.9\%\) asymptotic Gaussian CIs. \(10 10^{3}\) samples were used. Note that \(\)-MIENF and tridiag-\(\)-MIENF yield almost the same results with similar bias.

Figure 3: Comparison of the selected estimators. Along \(x\) axes is \(I(X;Y)\), along \(y\) axes is \((X;Y)\). We plot 99.9% asymptotic CIs acquired either from the MC integration standard deviation (WKL, KSG) or from the epochwise averaging (other methods, \(200\) last epochs). \(10 10^{3}\) samples were used.

We attribute the good performance of AE+WKL to the fact that the proposed synthetic datasets are easily and almost losslessly compressed via a CNN AE. We run additional experiments with much simpler, but incompressible data to show that the estimation error of WKL rapidly increases with the dimensionality. The results are provided in Table 1. In contrast, our method yields reasonable estimates in the same or similar cases presented in Figure 4.

Overall, the proposed estimator performs well during all the experiments, including the incompressible high-dimensional data, large MI values and non-Gaussian-based tests. In Appendix D, we also apply our method to acquire disentengled representations of real data. Additionally, we give a brief commentary on the sample complexity of the proposed method and other NN-based estimators in Appendix C.

## 6 Discussion

Information-theoretic analysis of deep neural networks is a novel and developing approach. As it relies on a well-established theory of information, it potentially can provide fundamental, robust and intuitive results . Currently, this analysis is complicated due to main information-theoretic qualities -- _differential entropy_ and _mutual information_ -- being hard to measure in the case of high-dimensional data.

We have shown that it is possible to modify the conventional normalizing flow setup to harness all the benefits of simple and robust closed-form expressions for mutual information. Non-asymptotic error bounds for both variants of our method are derived, asymptotic variance and consistency analysis is carried out. We provide useful theoretical and practical insights on using the proposed method effectively. We have demonstrated the effectiveness of our estimator in various settings, including compressible and incompressible high-dimensional data, high values of mutual information and the data not acquired from the Gaussian distribution via invertible mappings.

Finally, it is worth noting that despite normalizing flows and Gaussian base distributions being used throughout our work, the proposed method can be extended to any type of base distribution with closed-form expression for mutual information and to any injective generative model. For example, a subclass of diffusion models can be considered . Injective normalizing flows  are also compatible with the proposed pipeline. Gaussian mixtures can also be used as base distributions due to a relatively cheap MI calculation and the universality property .

LimitationsThe main limitation of the general method is the ambiguity of \(\) (the family of PDF estimators used to estimate MI in the latent space), which can be either rich (yielding a consistent, but possibly expensive estimator), or poor (leading to the inconsistency of the estimate). However, in  it is argued that mixture models can achieve rather good tradeoff between the quality and the cost of a PMI approximation.

The major limitation of \(\)-MIENF is that its consistency is proven only for a certain class of distributions: the probability distribution should be equivalent to a Gaussian via a Cartesian product of diffeomorphisms. However, mathematical simplicity, rigorous bounds, low variance and relative practical success of the estimator suggest that the proposed method achieves a decent tradeoff.

   \(d_{X,Y}\) & \(2\) & \(4\) & \(8\) & \(16\) & \(32\) & \(64\) \\  RMSE & \(2.2\) & \(1.0\) & \(127.9\) & \(227.5\) & \(522.4\) & \(336.2\) \\   

Table 1: Evaluation of \(5\)-NN weighted Kozachenko-Leonenko estimator on multidimensional uniformly distributed data. For each dimension \(d_{X}=d_{Y}\), \(11\) estimates of MI are acquired with the ground truth ranging from \(0\) to \(10\) with a fixed step. The RMSE of the estimated MI relative to the ground-truth MI is calculated for each set of estimates.