# Statistical and Computational Trade-off in Multi-Agent Multi-Armed Bandits

Filippo Vannella

KTH and Ericsson Research

Stockholm, Sweden

vannella@kth.se

&Alexandre Protiuere

KTH

Stockholm, Sweden

alepro@kth.se

&Jaeseong Jeong

Ericsson Research

Stockholm, Sweden

jaeseong.jeong@ericsson.com

###### Abstract

We study the problem of regret minimization in Multi-Agent Multi-Armed Bandits (MAMABs) where the rewards are defined through a factor graph. We derive an instance-specific regret lower bound and characterize the minimal expected number of times each global action should be explored. This bound and the corresponding optimal exploration process are obtained by solving a combinatorial optimization problem whose set of variables and constraints exponentially grow with the number of agents, and cannot be exploited in the design of efficient algorithms. Inspired by Mean Field approximation techniques used in graphical models, we provide simple upper bounds of the regret lower bound. The corresponding optimization problems have a reduced number of variables and constraints. By tuning the latter, we may explore the trade-off between the achievable regret and the complexity of computing the corresponding exploration process. We devise Efficient Sampling for MAMAB (ESM), an algorithm whose regret asymptotically matches the approximated lower bounds. The regret and computational complexity of ESM are assessed numerically, using both synthetic and real-world experiments in radio communications networks.

## 1 Introduction

The stochastic Multi-Agent Multi-Armed Bandits (MAMABs)  is a combinatorial sequential decision-making problem that generalizes the classical stochastic MAB problem by assuming that \((i)\) a _global action_ is defined by actions individually selected by a set of agents, and \((ii)\) the _reward_ function is defined through a factor graph, which defines inter-dependencies between agents. This reward structure arises naturally in applications where agents interact in a graph with the need to coordinate towards a common goal. MAMABs can model a wide range of real-world problems, from wind farm control  to radio communication networks parameters optimization (see Fig. 1).

Despite the wide spectrum of their potential applications, MAMABs are extremely hard to solve, even when the reward function is known. The main challenge stems from the combinatorial structure of the action set (there are \(K^{N}\) possible global actions, where \(N\) is the number of agents and \(K\) is the number of actions per agent). This issue is exacerbated in the learning setting where the reward function has to be inferred. In this work, we study the regret minimization problem in MAMABs, and more specifically, the trade-off between statistical efficiency (the learner aims at achieving low regret), and computational efficiency (she will typically have to solve combinatorial optimization problems over the set of possible global actions while learning).

**Contributions.** We present statistically and computationally efficient algorithms for MAMABs. Our algorithms enjoy (in the worst case) regret guarantees scaling as \( K^{d}(T)\), where \(K\) is the number of actions per agent, \(\) and \(d\) are the number of factors and the maximal degree of the graph defining the reward function. This scaling illustrates the gains one may achieve by exploiting the factor graph structure: without leveraging it, the regret would scale as \(K^{N}(T)\). Our algorithms have controllable computational complexity and can be applied in large-scale MAMABs. More precisely, our contributions are as follows.

_1) Regret lower bound._ We derive a regret lower bound satisfied by any algorithm. The bound is defined through a convex program (the _lower bound problem_), whose solution provides an optimal exploration strategy. Unfortunately, because of the factored reward structure, this optimization problem contains an exponential number of variables and constraints, and is hard to use in practice.

_2) Approximations of the lower bound problem._ We devise approximations of the lower bound problem by combining variable and constraint reduction techniques inspired by methods in the probabilistic graphical model literature [39; 20]. To reduce the number of variables, we propose \((i)\)_locally tree-like_ approximation, a tight relaxation for MAMAB instances described by acyclic factor graphs, and \((ii)\)_Mean Field_ (MF) approximation for general graphs. The MF approximation yields an upper bound of the regret lower bound, scaling as \( K^{d}(T)\) (where \(T\) is the time horizon).

Both approximations yield lower bound problems with a polynomial number of variables and exponential number of constraints (in \(N\)). To reduce the number of constraints, we propose a technique that leverages an ordering of the \(m\) smallest gaps and a Factored Constraint Reduction (FCR) method to represent the exponentially many constraints in a compact manner. The corresponding optimization problems have a reduced number of variables and constraints. By tuning the latter, we may explore the trade-off between the achievable regret and the complexity of computing the corresponding exploration process.

_3) The ESM algorithm._ Based on this approximation, we devise Efficient Sampling for MAMABs (ESM), an algorithm whose regret provably matches our approximated regret lower bound. The algorithm trades off statistical and computational complexity by performing exploration as prescribed by the solution of the approximated lower bound problem. We test the performance of ESM numerically on both synthetic experiments and learn to coordinate the antenna tilts in a radio communication network. In both sets of experiments, ESM can solve problems with a large number of global actions in a statistical and computationally efficient manner.

## 2 Related Work

Our work belongs to the framework of structured regret minimization in MABs, which encompasses a large variety of reward structures such as linear , unimodal , Lipschitz , etc. For general structured bandits,  propose Optimal Sampling for Structured Bandits (OSSB), a statistically optimal algorithm, i.e., matching the regret lower bound. The algorithm is computationally inefficient when applied to the MAMABs combinatorial structure. Our algorithm is inspired by OSSB, but relies on approximated lower bound problems to trade-off statistical and computational complexity.

A few studies investigate MAMABs with the same factored reward structure as ours [35; 2; 37]. These works focus on devising algorithms with regret guarantees using methods based on, e.g., Upper Confidence Bound (UCB) [35; 2] or Thompson Sampling (TS) . For example, Stranders et al.  propose HEIST, an UCB-type algorithm whose asymptotic regret scales as \(O(K^{N}_{}/_{}(T))\), where \(_{}\) and \(_{}\) are the minimal and maximal gaps, respectively. The MAUC algorithm from Bargiacchi et al.,  improves over  yielding asymptotic regret \(O(^{2}K^{d}_{}^{2}/_{}^{2}(T))\). Our worst approximation improves of a factor \(_{}\) w.r.t. this bound, a quantity that typically scales with \( K^{d}\) (see App. M).

There is a large body of work [24; 10; 12; 13; 38] investigating regret minimization in the (linear) _combinatorial semi-bandit feedback_ setting. Although our model can be interpreted as a particular instance of this setting (see App. E for details), the MAMAB combinatorial structure has never been explicitly considered in this context. The closest related work is , in which the authors study a regret lower bound problem with an exponentially large number of variables and constraints. They leverage [12, Assumption 6] to compactly represent the lower bound optimization problem and propose a gradient-based procedure to solve it in polynomial time. Unfortunately, for MAMABs, theabove-mentioned assumption only holds for rewards described by acyclic factor graphs (see App. M). We propose computationally efficient approximations valid for any factor graph while retaining statistical tightness in the case of acyclic factor graphs.

## 3 Problem Setting

We consider the generic MAMAB model with factored structure introduced in . The model is defined by the tuple \(,,r\), where:

1. \(=[N]\{1,,N\}\) is a set of \(N\) agents;
2. \(=_{i[N]}_{i}\) is a set of global actions, which is the Cartesian product over \(i\) of the set \(_{i}\) of actions available to the agent \(i\). We assume w.l.o.g. that \(|_{i}|=K\), for all \(i[N]\), and define \(A||=K^{N}\);
3. \(r\) is the reward function mapping the global action to the collected reward.

**Rewards and their factor-graph representation.** We model the collected rewards. There are \(\) possibly overlapping groups of agents \((_{e})_{e[]}\), with \(_{e}\) and \(|_{e}|=N_{e}\). The local reward generated by group \(e\) depends on group actions \(a_{e}(a_{i})_{i_{e}}_{e} _{i_{e}}_{i}\) only. More precisely, each time \(a_{e}\) is selected, the collected local rewards are i.i.d. copies of a random variable \(r_{e}(a_{e})(_{e}(a_{e}),1/2)\). Rewards collected in various groups are independent. The global reward for action \(a\) is then \(r(a)=_{e[]}r_{e}(a_{e})\), a random variable with expectation \((a)=_{e[]}_{e}(a_{e})\). The number of possible group actions in group \(e\) is \(A_{e}|_{e}|=K^{N_{e}}\), and we define \(_{e[]}A_{e}\). The reward function can be represented using a factor graph . Factor graphs are bipartite graphs with two types of node: \(N\)_action nodes_, one for each agent, and \(\)_factor nodes_, one for each group. An edge between a factor \(r_{e}\) and an agent \(i\) exists if the action \(a_{i}\) selected by the agent \(i\) is an input of \(r_{e}\): \(i_{e}\). Fig. 1 shows an example of a factor graph modeling interference in a radio communication network.

**Sequential decision process.** The decision maker sequentially selects global actions based on the history of previous observations and receives a set of samples of the local rewards associated to the various groups. Specifically, in each round \(t 1\), the decision maker selects a global action \(a_{t}=(a_{t,1},,a_{t,N})\) and observes the local rewards \(r_{t}=(r_{t,1},,r_{t,})\) from each group. The global action \(a_{t+1}\) is selected based on the history of observations \(_{t}=(a_{s},r_{s})_{s[t]}\). This type of interaction is known as _semi-bandit_ feedback.

**Regret minimization.** The goal is to devise an algorithm \(=(a_{t})_{t 1}\), i.e., a sequence of global actions \(a_{t}\) selected in each round \(t 1\), that minimizes the _regret_ up to time \(T 1\), defined as

\[R^{}(T)=[_{t=1}^{T}(a_{}^{*})-(a_{t}) ],\]

Figure 1: Factor graph in a radio communication network. An agent (represented by a circle) corresponds to a base-station (BS) whose transmissions cover a cell. The possible actions at a BS may correspond to different transmission power levels and antenna tilts (the physical angle of the antennas). The local rewards correspond to the throughput (in bit/s) achieved in a given cell, and hence each cell is associated with a factor (represented by a square). The throughput in a given cell depends on the action of the corresponding BS but also on those of neighboring BSs through interference. In the factor graph, each BS or agent has hence an edge to factors or cells it interferes.

where \(a_{}^{*}_{a}(a)\) denotes the _best global action_. Throughout the paper, we assume that \(a_{}^{*}\) is unique and we use \(a^{}\) and \(a_{}^{*}\) interchangeably. We define the gap of a sub-optimal global action \(a\) by \((a)=(a^{})-(a)\).

## 4 Regret Lower Bound

To derive instance-specific regret lower bounds, we restrict our attention to the class of _uniformly good_ algorithms: An algorithm \(\) is uniformly good if for any \(\), \( a>0\), we have that \(R^{}(T)=o(T^{})\).

**Theorem 4.1**.: _The regret of any uniformly good algorithm satisfies for any \(\), \(_{T}(T)}{(T)} C_{}^{*}\), where \(C_{}^{}\) is the value of the following convex optimization problem_

\[_{v^{A}_{ 0}}_{a}v_{a}(a) _{e[]:a_{e} a_{e}^{*}}(_{b \{a_{}^{*}\}:b_{e}=a_{e}}v_{b})^{-1} (a)^{2}, a.\] (1)

The proof of this result leverages classical change-of-measure arguments  (see App. A.1 for details). If \(v^{}\) denotes the solution of the lower bound optimization problem, then for \(a a_{}^{}\), \(v_{a}^{}(T)\) can be interpreted as the asymptotic expected number of times the sub-optimal action \(a\) is explored under a uniformly good algorithm minimizing regret. We conclude this section by reformulating (1) using _group variables_\(\). Introduce the _marginal cone_:

\[}=\{^{}_{ 0}: v ^{A}_{ 0}, e[],a_{e}_{e}, _{e,a_{e}}=_{b\{a_{}^{*}\}:b_{e}=a_ {e}}v_{b},\}.\]

The set \(}\) contains group variables \(=(_{e})_{e[]}\) where \(_{e}=(_{e,a_{e}})_{a_{e}_{e}}\).

**Lemma 4.2**.: _For any \(\), \(C_{}^{*}\) is the value of the following convex optimization problem_

\[_{}}_{e[],a_{e}_{e }}_{e,a_{e}}(_{e}(a_{e}^{*})-_{e}(a_{e}))_{e[]:a_{e} a_{e}}_{e,a_{e}}^{-1}(a) ^{2}, a.\] (2)

Again, if the solution of (2) is \(^{}\), then for any \(e[]\) and \(a_{e}_{e}\), \(_{e,a_{e}}^{}(T)\) can be interpreted as the asymptotic expected number of times the group action \(a_{e}\) is selected under an optimal algorithm when it explores, i.e., when the global action \(a a_{}^{*}\).

## 5 Lower Bound Approximations

As suggested above, if we are able to solve (1) and hence obtain \(v^{}\), the latter specifies the optimal exploration process. From there, we could devise an algorithm with minimal regret . Unfortunately, solving (1) is an extremely hard task, even for relatively small problems. Indeed, the problem has \(K^{N}\) variables and \(K^{N}\) constraints, and using general-purpose solvers, e.g., based on the interior-point method, would require \((K^{N})(1/)\) floating-point operations . To circumvent this difficulty, we present approximations of the lower bound problem with a reduced number of variables and constraints. We will then leverage these approximations to design efficient algorithms.

### Variable reduction

To reduce the number of variables, we apply approximation techniques inspired by methods in the probabilistic graphical model literature . In Sec. 5.1.1, we first propose a _locally tree-like_ reduction, yielding an optimization problem whose value \(C_{}^{*}\) exactly matches the true lower bound \(C_{}^{*}\) for MAMABs with acyclic factor graphs (see App. J for a formal definition and examples). For graphs containing cycles however, we have \(C_{}^{k}<C_{}^{*}\), and hence for those graphs, it is impossible to devise an algorithm based on this reduction (such an algorithm would lead to a regret \(C_{}^{k}(T)\), which contradicts the lower bound).

Instead, for general graphs, we propose in Sec. 5.1.2 the \(\)_-mean-field_ reduction, an approximation based on a local decomposition inspired by Mean Field (MF) methods . The \(\)-mean field reduction leads to an optimization problem whose value \(C_{}^{}\) provably upper bounds \(C_{}^{*}\), and hence that can be used to devise an algorithm with regret approaching \(C_{}^{}(T)\).

#### 5.1.1 Locally tree-like reduction

This reduction imposes local consistency constraints between group variables \(\) and local variables \(w=(w_{i})_{i[N]}\), where \(w_{i}=(w_{i,a_{i}})_{a_{i}_{i}}_{ 0}^{}\). Define the local cone as:

\[}_{}=\{^{}: w _{ 0}^{KN}: e[], i_{e},  a_{i}_{i},w_{i,a_{i}}=_{b_{e}_{e}:b_{e } a_{i}}_{b_{e}}\},\]

where the notation \(a_{e} a_{i}\) means that the \(i^{}\) element of \(a_{e}\) equals \(a_{i}\). The locally tree-like approximation, presented in the next lemma, is obtained by replacing \(}\) by \(}_{}\) in (2).

**Lemma 5.1**.: _For any \(\) with rewards described by an acyclic factor graph, we have that \(C_{}^{}=C_{}^{}\), where \(C_{}^{}\) is the value of the following convex optimization problem:_

\[_{}_{}}_{e[],a_{e} _{e}}_{e,a_{e}}(_{e}(a_{e}^{})-_{e}(a_{e }))_{e[]:a_{e} a_{e}^{}}_{ e,a_{e}}^{-1}(a)^{2},\ \ \  a.\] (3)

The proof is presented in App. A.2. This approximation reduces the number of variables from \(K^{N}\) to \(+KN\). The lemma states that, for acyclic factor graphs, the locally tree-like approximation (3) is tight, i.e., \(C_{}^{}=C_{}^{}\). Unfortunately, for general graphs, we have that \(C_{}^{}<C_{}^{}\) (a direct consequence of [39, Prop. 4.1]), and hence it is impossible to devise algorithms based on this approximation.

#### 5.1.2 \(\)-Mean-Field reduction

Our \(\)-MF reduction is loosely inspired by MF approximation methods in graphical models . It consists in decomposing global variables \(v\) as a function \(\) of the local variables \(w=(w_{i})_{i[N]}\). Specifically, the \(\)-MF reduction introduces the following set of constraints: \(v_{a}=_{a}(w)\), \( a a_{}^{}\), where \(_{a}:_{ 0}^{KN}_{ 0}\). Let \(_{}=\{v_{ 0}^{}: w _{ 0}^{KN},v_{a}=_{a}(w), a a_{}^{}\}\), and define the \(\)-MF marginal cone as

\[}_{}=\{_{ 0}^{ }: v_{}, e[],a_{e} _{e},_{e,a_{e}}=_{b\{a_{e}^{ }\}:b_{e}=a_{e}}v_{b},\}.\]

We get the \(\)-MF approximation, \(C_{}^{}\) by replacing \(}\) by \(}_{}\) in (2).

**Lemma 5.2**.: _For any \(\), \(\), we have that \(C_{}^{} C_{}^{}\), where \(C_{}^{}\) is the value of the optimization problem:_

\[_{}_{}}_{e[],a_{e} _{e}}_{e,a_{e}}(_{e}(a_{e}^{})-_{e}(a _{e}))_{e[]:a_{e} a_{e}^{}}_{e,a_{e}}^{-1}(a)^{2},\ \  a.\] (4)

Clearly, the tractability of the problem \(C_{}^{}\) depends on the choice of \(\). A natural choice would be \(_{a}(w)=_{i[N]}w_{i,a_{i}}\), as proposed, e.g., in approximate inference methods . However, this choice leads to a non-convex program (see App. B). The following lemma proposes a choice of \(\) which leads to a convex program over local variables \(w\) only.

**Lemma 5.3**.: _Let \(_{a}(w)=_{i[N]}w_{i,a_{i}}\), \( a a^{}\). Then \(C_{}^{}\) is the value of the following convex optimization problem:_

\[_{w_{ 0}^{KN}}_{e[],a_{e}_{e}}f_{e,a_ {e}}(w)((a_{e}^{})-_{e}(a_{e}))_{e[ ]:a_{e} a_{e}^{}}f_{e,a_{e}}(w)^{-1}(a)^{2},\  a ,\] (5)

_where \(f_{e,a_{e}}(w)=K^{N-|_{e}|}_{i_{e},a_{i} _{e}}w_{i,a_{i}}+K^{N-|_{e}|-1}_{i_{e }}w_{i,a_{i}}\). Furthermore, it holds that \(C_{}^{}_{}^{-2}_{e[],a_{e} _{e}}((a_{e}^{})-_{e}(a_{e}))\), where \(_{}=_{a a^{}}(a)\)._

The proof is presented in App. A.3. The lemma provides a worst-case scaling of \(C_{}^{}\): it scales at most as \(=_{e[]}K^{|_{e}|}\) (remember that if we were considering a MAMAB as a standard bandit problem, the latter would have \(K^{N}\) arm and hence a regret scaling exponentially in \(N\)). The number of variables involved in (5) is \(KN\). The quantities \((f_{e,a_{e}}(w))_{e[],a_{e}_{e}}\) are group quantities interpreted as the group variables \(_{e,a_{e}}\), and uniquely determined by local variables \(w\). In the following, we use the notation \(C_{}^{}\) to represent \(C_{}^{}\) for the function \(\) defined in Lemma 5.3.

### Constraint reduction

The remaining challenge is to reduce the number of constraints in (3), (4) or (5). For each global action \(a\), the constraint writes \(_{e[]}_{e,a_{e}}^{-1}(a)^{2}\). The major issue is the non-linearity of the function appearing in the constraints w.r.t. group actions \(a_{e}\). Upon inspection, it appears that the heterogeneity in the gaps (generally \((a)(b)\) for \(a b\)) is causing the non-linearity. To address this problem, we present, in the following lemma, a family of approximations leveraging an ordering of the first \(m\) smallest gaps. For \(m[K^{N}]\), let \(a^{(m)}\) be the \(m^{}\) best global action and, for \(m[K^{N}-1]\), let \(_{m}=(a_{}^{*})-(a^{(m+1)})\) be the \(m^{}\) minimal non-zero gap (with ties breaking arbitrarily).

**Lemma 5.4**.: _Let \(m[K^{N}-1]\), and \(\{,\}\). Let \(C_{}^{}(m)\) be the value of the convex program:_

\[_{_{}} _{e[],a_{e}_{c}}_{e,a_{e}}( _{e}(a_{e}^{*})-_{e}(a_{e}))\] (6) \[ _{e[]:a_{e}^{(j+1)} a_{e}^{2}}_{e,a_{e}^{ (j+1)}}^{-1}_{j}^{2}, j[m]\] (7) \[_{e[]:a_{e} a_{e}^{*}}_{e,a_{e}}^{-1} _{m}^{2}, a_{j[m]}\{a^{(j+1)}\}.\] (8)

_Then, for any \(\{,\}\), \(m[K^{N}-2]\), we have \(C_{}^{}(m+1) C_{}^{}(m)\), \(C_{}^{*} C_{}^{}(m)\), and by definition \(C_{}^{}(K^{N}-1)=C_{}^{}\)._

The proof is reported in App. F. Clearly, \(C_{}^{}(m)\) has still \(||\) constraints (7)-(8). However, as the gap \(_{m}\) used in (8) is constant, these constraints are now a linear sum of terms depending on group actions \(a_{e}\). For constraints with this type of structure, there exists an efficient and provably equivalent representation. The procedure yielding this representation, which we refer to as FCR, is based on a generalization of the popular Factored LP algorithm described in [19; 20] for Factored Markov Decision Processes (FMDPs).

For the sake of brevity, we briefly describe the procedure below and postpone its detailed exposition to App. G. FCR is inspired by the Variable Elimination (VE) procedure in graphical models . It iteratively eliminates constraints from (8), according to an elimination order \(\). The elimination procedure induces an _elimination graph_, which encodes dependencies between constraints as we perform elimination. As shown in the following lemma, the number of constraints is exponential in the degree \(A_{}\) of the elimination graph induced by the order of elimination \(\).

**Lemma 5.5**.: _There exists a procedure which, given the constraints in (8) returns a provably equivalent constraint set of size \(O(NK^{A_{}+1})\)._

Although for general graphs finding an ordering \(\) minimizing \(A_{}\) is an \(\)-hard problem , for specific graphs there are orderings yielding \(A_{} N\). For example, these orderings yield \(A_{}=2\) for line or star factor graphs, and \(A_{}=3\) for ring factor graphs, independently of the number of agents \(N\) (see Fig. 3 and refer to App. J for details). Solving \(C_{}^{}(m)\), requires computing the first \(m+1\) best global actions and the \(m\) minimal gaps. To solve this task, the _elim-\(m\)-opt_ algorithm  has complexity \(O((m+1)NK^{A_{}+1})\) (see App. G). Fig. 2 shows an illustration of the trade-off between statistical and computational complexity. Note that ESM is meant to be applied when \(m\) does not grow exponentially in \(N\). In practice, we observed that selecting \(m=\) yields a good trade-off between statistical complexity and computational complexity.

## 6 The ESM algorithm

In this section, we present ESM, an algorithm whose regret matches (asymptotically) our approximated lower bounds. The algorithm is inspired by OSSB . It ensures that sub-optimal actions are sampled as prescribed by the solution of the approximated optimization problems \(C_{}^{}(m)\) or \(C_{}^{}(m)\): each group \(e\) must explore each group action \(a_{e}\) for \(_{e,a_{e}}(T)\) times, and selecting the action yielding the largest estimated reward for the remaining rounds. As the lower bounds depend on the unknown parameter \(\), it has to be estimated.

Generally, the estimation of \(\) would require evaluating an exponentially large number of components, i.e., \((a), a\). Instead, by leveraging the factored reward structure, we can simply focus on estimating group parameters \((_{e})_{e[]}\). We define the estimate at time \(t\), group \(e\), and action \(a_{e}\) as \(_{t,e,a_{e}}=}}_{s[t]:a_{s,e}=a_{e}}{}_ {}\) where \(N_{t,e,a_{e}}=_{s[t]}_{\{a_{s,e}=a_{e}\}}\) is the number of times action \(a_{e}\) is selected for group \(e\). We also define \(_{t}=(_{t,e,a_{e}})_{e[],a_{e}_{e}}\), and \(N_{t}=(N_{t,e,a_{e}})_{e[],a_{e}_{e}}\).

```  Sample each group actions in \(_{0}\) once and update \((N_{_{0}},_{_{0}})\); \(s_{_{0}}=0\)\(\)Initialization for\(t=T_{0},,T\)do \((_{t,e})_{e[]}\) Solve \(C^{}_{_{t}}(m)\) if\(N_{t,e,a_{e}}(1+)_{t,e,a_{e}}(t), e[],a_{e} _{e}\)then\(\)Exploitation \(a_{t}=a^{}_{_{t}}\) \(s_{t}=s_{t-1}\) else \(s_{t}=s_{t-1}+1\) if\(_{e[],a_{e}_{e}}N_{t,e,a_{e}} s_{t}\)then\(\)Estimation \(a_{t}_{0}:a_{t,e^{}}=b_{e^{}}\) with \((e^{},b_{e^{}})_{e,a_{e}}N_{t,e,a_{e}}\) else\(\)Exploration \(a_{t}:a_{t,e^{}}=b_{e^{}}\) with \((e^{},b_{e^{}})_{e,a_{e}}}}{ _{t,e,a_{e}}}\) Update \((N_{t,e,a_{t,e}},_{t,e,a_{e}})_{e[]}\) ```

**Algorithm 1** ESM(\(_{0},,\), \(\), \(m\))

The pseudocode of ESM is presented in Alg. 1. It takes as inputs two exploration parameters \(,>0\), an exploration set \(_{0}\), the approximation parameter \(m[K^{N}-1]\), and \(\{,\}\) depending on the targeted regret lower bound approximation. The parameters \(,>0\) impact the amount of exploration performed by ESM. When decreasing both these parameters the exploration of ESM also decreases. After an initialization phase, the algorithm alternates between three additional phases as described below.

**Initialization.** In the initialization phase, we select actions from \(_{0}\) to ensure that each group action is sampled at least once. The set \(_{0}\) is chosen in such a way that it covers all possible group actions, i.e., \(_{0}\) is such that \( e[], a_{e}_{e}, b_{0 }:b_{e}=a_{e}\). In App. I, we present an efficient routine to select \(_{0}\). Let \(T_{0}=\{t 0:N_{t,e,a_{e}}>0, e[],a_{e} _{e}\}\) be the length of the initialization phase. For \(t T_{0}\) we select \(a_{t}_{0}:a_{t,e^{}}=b_{e^{}}\) with \((e^{},b_{e^{}})_{e,a_{e}}N_{t,e,a_{e}}\) (with ties breaking arbitrarily), i.e., we select a global action containing the most under-explored group action. This choice ensures that \(T_{0}\). For \(t>T_{0}\), the algorithm solves the approximated lower bound optimization problem \(C^{}_{_{t}}(m)\) and alternates between exploitation, exploration, and estimation.

Figure 2: Left: idealized curve illustrating possible ranges of the trade-off between statistical and computational complexity for \(C^{}(m)\), when varying \(m\). Right: an instance of this trade-off for a line factor graph (darker colors for the points represent higher running times). Selecting \(m=\) yields a good trade-off between computational and statistical complexity for this instance.

**Exploitation.** If \(N_{t,e,a_{e}}(1+)_{t,e,a_{e}}(t)\), ESM enters the exploitation phase: it selects the best empirical action \(a_{_{t}}^{}=*{arg\,max}_{a}_{e[ ]}_{t,e,a_{e}}\). Generally, computing \(a_{_{t}}^{}\) requires a \(\) operation over an exponential number of actions \(a\). Fortunately, due to the factored structure, we can implement the \(\) operation efficiently through a VE procedure  (see App. G).

**Estimation.** If not enough information has been gathered, ESM enters an estimation phase, where it selects the least explored group action similarly to the initialization phase. This ensures that the _certainty equivalence_ holds, i.e., that \(_{t}\) is estimated accurately.

**Exploration.** Otherwise, the algorithm enters the exploration phase and selects actions as suggested by the solution of \(C_{_{t}}^{}(m)\). More precisely, we select a global action \(a_{t}\) which contains a group action \(a_{t,e^{}}=b_{e^{}}\) that minimizes the following ratio where \(e^{}\) and \(b_{e^{}}\) are the group index and group action which minimize \(}}{_{t,e,a_{e}}}\).

**Upper bound.** We establish that the ESM algorithm achieves a regret, matching the approximate lower bound \(C_{}^{}(m)(T)\), asymptotically as \(T\). The proof is given in App. D.

**Theorem 6.1**.: _Let \(<1/|_{0}|\). For any \(m[K^{N}-1]\), we have that_

1. \(_{T}(T)}{(T)} C_{}^{}(m )(,)\)_, for_ \(=(_{0},,,,m)\)_, for any_ \(\)_,_
2. \(_{T}(T)}{(T)} C_{}^{}(m )(,)\)_, for_ \(=(_{0},,,,m)\)_, for any_ \(\) _described by acyclic factor graphs,_

_where \(\) is a function such that \(_{(,)(0,0)}(,)=1\)._

## 7 Experiments

In this section, we present numerical experiments to assess the performance of our algorithm. We propose two sets of experiments: \((i)\) a set of synthetic MAMABs with different graph topologies, and \((ii)\) an industrial use-case from the radio communication domain: _antenna tilt optimization_. The code for the synthetic experiments and the additional experiments presented in App. K is available at this link.

### Synthetic Experiments

**Problem instances.** We consider the factor graphs depicted in Fig. 3. The expected rewards are selected uniformly at random in the interval \(\). In our experiments, select \(N=5\) and \(K=3\). We execute our experiments for \(N_{}=5\) independent runs. Following previous work , we select \(=0\), and \(=0.01\). The elimination order is chosen as \(=[N]\). We implement the solver for the lower bound optimization problems using CVXPY  with a MOSEK solver .

**Results.** The results for the regret (in log scale) are presented in Fig. 4. The performance of ESM is compared to that of MAUCE , HEIST , and to a random strategy selecting actions uniformly at random. The computational complexity results, reported in Fig. 5, measure the running time (in sec.) to solve an instance of the approximate lower bound optimization problem \(C_{}^{}(m)\). We use \(m=\), and \(=\) MF for the ring, or \(=\) L for the star and line graph topologies.

Figure 3: Factor graphs used in the synthetic experiments: _ring_ (left), _line_ (center), _star_ (right).

### Antenna tilt optimization

Next, we test our algorithm on a radio network optimization task. The goal is to control the vertical antenna tilt at different network Base Stations to optimize the network throughput. In the following, we detail the network model, our simulation setup, and present our experimental results.

**Network model.** We consider a sectorized radio network consisting of a set of _sectors_\(=[N]\). The set of sectors corresponds to the set of agents in our MAMAB framework. Since each sector is associated to a unique antenna, we will use the terms _sector_ and _antenna_ interchangeably. We assume that each sector \(i\) serves (on the downlink) a fixed set of Users Equipments (UEs) \(_{i}\) (each UE is associated with a unique antenna, that from which it receives the strongest signal).

**Factor graph.** We model the observed reward in the radio network as a factor graph with \(N=||\) agent nodes and \(=||\) factor nodes. Each sector is associated with a unique factor, which models the rewards observed in that sector. We build the factor graph based on the interference pattern of the antennas, i.e., antennas that can interfere with each other are connected to common factors. An example of such a graph and additional experimental details are reported in App. L.

**Actions and rewards.** The action \(a_{t,i}\) represents the antenna tilt for sector \(i\) and at time \(t\). It is chosen from a discrete set of \(K\) tilts, i.e., \(a_{t,i}\{_{1},,_{K}\}\). The tilt for a group of sectors \(e\) is denoted by \(a_{e}\). Rewards are based on the throughput of UEs in sector \(i\), which depends on the actions of a group of agents \(a_{e}\): \(r_{e}(a_{e})=_{n_{t}}T_{i,u}(a_{e})\), where \(T_{i,u}\) is the throughput of an UE \(u\) associated to sector \(i\). Hence, the global reward for a tilt configuration \(a\) is \(r(a)=_{i[N]}_{n_{t}}T_{i,u}(a_{e})\). The throughput \(T_{i,u}\) depends on channel conditions (or _fading_) between the antenna and the user. These conditions rapidly evolve over time around their mean.

**Simulator.** We run our experiments in a proprietary mobile network simulator in an urban environment. The simulation parameters used in our experiments are reported in App. K. Based on the user positions and network parameters, the simulator computes the path loss in the network environment using a BEZT propagation model  and returns the throughput for each sector by conducting user association and resource allocation in a full-buffer traffic demand scenario.

**Results.** We test our algorithm for \(_{i}=\{2^{},7^{},13^{}\}\), and for \(||=6\) sectors. As the factor graph contains cycles, we use \(=\) MF and select \(m=3\). The results, presented in Fig. 6, are in line with the experimental findings of the previous section. However, the ESM running time is higher due to the higher complexity of the factor graph.

Figure 4: Regret results for the synthetic instances.

Figure 5: Running time (in sec.) to solve an instance of \(C^{}_{}(m)\), when varying \(m\).

## 8 Conclusions

In this paper, we investigated the problem of regret minimization in MAMABs: we derived a regret lower bound, proposed approximations of it, and devised ESM, an algorithm trading off statistical and computational efficiency. We then assessed the performance of ESM on both synthetic examples and the antenna tilt optimization problem. Interesting future research directions include proposing efficient distributed implementations of ESM, quantifying on its communication complexity, and investigating representation learning problems in MAMABs where the underlying factor graph defining the reward is unknown and needs to be learned.