# Demystifying Softmax Gating Function in

Gaussian Mixture of Experts

 Huy Nguyen\({}^{}\)   TrungTin Nguyen\({}^{}\)   Nhat Ho\({}^{}\)

Department of Statistics and Data Sciences, The University of Texas at Austin\({}^{}\)

Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK\({}^{}\)

{huynm, minhnhat}@utexas.edu, trung-tin.nguyen@inria.fr

###### Abstract

Understanding the parameter estimation of softmax gating Gaussian mixture of experts has remained a long-standing open problem in the literature. It is mainly due to three fundamental theoretical challenges associated with the softmax gating function: (i) the identifiability only up to the translation of parameters; (ii) the intrinsic interaction via partial differential equations between the softmax gating and the expert functions in the Gaussian density; (iii) the complex dependence between the numerator and denominator of the conditional density of softmax gating Gaussian mixture of experts. We resolve these challenges by proposing novel Voronoi loss functions among parameters and establishing the convergence rates of maximum likelihood estimator (MLE) for solving parameter estimation in these models. When the true number of experts is unknown and over-specified, our findings show a connection between the convergence rate of the MLE and a solvability problem of a system of polynomial equations.

## 1 Introduction

Softmax gating Gaussian mixture of experts [32; 37], a class of statistical machine learning models that combine multiple simpler models, known as expert functions of the covariates, via softmax gating networks to form more complex and accurate models, has found widespread use in various applications, including speech recognition [51; 64; 65], natural language processing [14; 20; 17; 54; 21], computer vision [52; 3; 15; 44], and other applications [26; 49; 7; 8; 48; 5; 6]. Regarding the applications of the softmax gating Gaussian mixture of experts in medicine  and physical sciences , the parameters of each expert function play an important role in capturing the heterogeneity of data. Thus, the main objective of these works is to conduct statistical inference for those parameters, which leads to a need for convergence rates of parameter estimation in the softmax gating Gaussian mixture of experts. However, a comprehensive theoretical understanding of parameter estimation in that model has still remained a long-standing open problem in the literature.

Parameter estimation has been studied quite extensively in standard mixture models. In his seminal work, Chen et al.  established the convergence rate \((n^{-1/4})\) of parameter estimation in over-fitted univariate mixture models, namely, the settings when the number of true components is unknown and over-specified, and the family of distributions is strongly identifiable in the second order, e.g., location Gaussian distributions. That slow and non-standard rate is due to the collapse of some parameters into single paramameter or the convergence of weights to zero, which leads to the singularity of Fisher information matrix around the true parameters. Then, Nguyen et al.  and Ho et al.  utilized Wasserstein metrics to achieve this rate under the multivariate settings of second-order strongly identifiable mixture models. Recently, Ho et al.  demonstrated that rates of the MLE can strictly depend on the number of over-specified components when the mixture models are not strongly identifiable.

identifiable, such as location-scale Gaussian mixtures. The minimax optimal behaviors of parameter estimation were studied in . From the computational side, the statistical guarantee of the expectation-maximization (EM), e.g., , and moment methods had also been studied under both exact-fitted  and over-fitted settings  of mixture models.

Compared to mixture models, there has been less research on parameter estimation of mixture of experts. When the gating networks are independent of the covariates, Ho et al.  employed the generalized Wasserstein loss function  to study the convergence rates of parameter estimation in Gaussian mixture of experts. They proved that these rates are determined by the algebraic independence of the expert functions and the partial differential equations with respect to the parameters. Later, Do et al.  extended these results to general mixture of experts with covariate-free gating network. Statistical guarantees of optimization methods for solving parameter estimation in Gaussian mixture of experts with covariate-free gating functions were studied in . When the gating networks are softmax functions, parameter estimation becomes more challenging to understand due to the complex structures of the softmax gating function in the Gaussian mixture of experts. Before describing these phenomena in further details, we begin by formally introducing the softmax gating Gaussian mixture of experts and related notions.

**Problem setting:** Assume that \((X_{1},Y_{1}),,(X_{n},Y_{n})^{d}\) are i.i.d. samples drawn from the softmax gating Gaussian mixture of experts of order \(k_{*}\) whose conditional density function \(g_{G_{*}}(Y|X)\) is given by:

\[g_{G_{*}}(Y|X):=_{i=1}^{k_{*}}^{*})^{}X+_{0 i}^{*})}{_{j=1}^{k_{*}}((_{1j}^{*})^{}X+_{0j}^{*})} f (Y|(a_{i}^{*})^{}X+b_{i}^{*},_{i}^{*}),\] (1)

where \(f(.|,)\) is a Gaussian density function with mean \(\) and variance \(\). Here, we define \(G_{*}:=_{i=1}^{k_{*}}(_{0i}^{*})_{(_{1i}^{*},a_{i}^{* },b_{i}^{*},_{i}^{*})}\) as a true but unknown _mixing measure_, that is, a combination of Dirac measures \(\) associated with true parameters \(_{i}^{*}:=(_{0i}^{*},_{1i}^{*},a_{i}^{*},b_{i}^{*},_{i} ^{*})\). Notably, \(G_{*}\) is not necessarily a probability measure as the summation of its weights can be different from one. For the purpose of the theory, we assume that \(_{i}^{*}^{d} ^{d}_{+}\) where \(\) is a compact set, and \(X^{d}\) where \(\) is a bounded set. Furthermore, we let \((a_{1}^{*},b_{1}^{*},_{1}^{*}),,(a_{k_{*}}^{*},b_{k_{*}}^{*}, _{k_{*}}^{*})\) be pairwise distinct and at least one among \(_{11}^{*},,_{1k_{*}}^{*}\) be non-zero to guarantee the dependence of softmax gating function on the covariate \(X\). Finally, we assume that the covariate \(X\) follows a continuous distribution to ensure that the softmax gating Gaussian mixture of experts is at least identifiable up to translations (see Proposition 1).

**Maximum likelihood estimation.** Since the value of true order \(k_{*}\) is unknown in practice, to estimate the unknown parameters in the softmax gating Gaussian mixture of experts (1), we consider using maximum likelihood estimation (MLE) within a class of all mixing measures with at most \(k\) components, which is defined as follows:

\[_{n}*{arg\,max}_{G_{k}()} {1}{n}_{i=1}^{n}(g_{G}(Y_{i}|X_{i})),\] (2)

where \(_{k}():=\{G=_{i=1}^{k^{}}(_{0i})_{( _{1i},a_{i},b_{i},_{i})}:1 k^{} k(_{0i},_{1i},a_{i},b_{i},_{i})\}\). To guarantee that the MLE \(_{n}\) is a consistent estimator of \(G_{*}\), we need \(k k_{*}\). In this paper, we study the convergence rate of the MLE \(_{n}\) to the true mixing measure \(G_{*}\) under both the _exact-fitted_ settings, namely when \(k=k_{*}\), and the _over-fitted_ settings, namely when \(k>k_{*}\), of the softmax gating Gaussian mixture of experts.

**Fundamental challenges from the softmax gating function:** There are three fundamental challenges arising from the softmax gating function that create various obstacles in our convergence analysis:

**(i)** Firstly, parameters \(_{1i}^{*},_{0i}^{*}\) of the softmax gating function are not identifiable as those of the covariate-independent gating function in previous work. Instead, they are identifiable up to translations, that is, the softmax gating value does not change when we translate \(_{0i}^{*}\) to \(_{0i}^{*}+t_{1}\) and \(_{1i}^{*}\) to \(_{1i}^{*}+t_{2}\) for any \(t_{1}\) and \(t_{2}^{d}\). As a consequence, we need to introduce an infimum operator in the Voronoi loss functions (see equations (4) and (6)) to deal with this issue.

**(ii)** Secondly, a key step in our proof techniques is to decompose the density discrepancy \(g_{_{n}}(Y|X)-g_{G_{*}}(Y|X)\) into a linear combination of linearly independent elements using Taylor expansions.

However, since the numerators and denominators of softmax gating functions are dependent, we cannot apply the Taylor expansions directly to that density discrepancy as in previous work . Moreover, there are two intrinsic interactions between parameters of the softmax gating's numerators and the Gaussian density function via the following partial differential equations (PDEs), which induce a lot of linearly dependent derivative terms in the Taylor expansions:

\[u}{_{1} b}=;u}{ b^{2}}=2,\] (3)

where \(u(Y|X;_{1},a,b,):=(_{1}^{}X) f(Y|a^{}X+b,)\). Therefore, it takes us great effort to group those linearly dependent terms together to obtain the desired linear combination of linearly independent terms.

**(iii)** Lastly, given the above linear combination of linearly independent elements, when the density estimation \(g_{_{n}}(Y|X)\) converges to the true density \(g_{G_{*}}(Y|X)\), the associated coefficients in that combination also go to zero. Then, via some transformations, those limits lead to a system of polynomial equations introduced in equation (9). This system admits a much more complex structure than what considered in previous work .

These fundamental challenges from the softmax gating function suggest that the previous loss functions, such as Wasserstein distance , being employed to study parameter estimation in standard mixture models or mixture of experts with covariate-free gating functions are no longer sufficient as they heavily rely on the assumptions that the weights of these models are independent of the covariates.

**Main contributions:** To tackle these challenges of the softmax gating function, we propose two novel Voronoi losses among parameters and establish the lower bounds of the Hellinger distance, denoted as \(h(,)\), of the mixing densities of softmax gating Gaussian mixture of experts in terms of these Voronoi losses to capture the behaviors of the MLE. Our results can be summarized as follows (see also Table 1):

**1. Exact-fitted settings**: When \(k=k_{*}\), we demonstrate that the Hellinger lower bound \(_{X}[h(g_{G}(|X),g_{G_{*}}(|X))] C_{1} (G,G_{*})\) holds for any mixing measure \(G_{k}()\), where \(C\) is some universal constant and the Voronoi metric \(_{1}(G,G_{*})\) is defined as:

\[_{1}(G,G_{*}):=_{t_{1},t_{2}}\ _{j=1}^{k_{*}} _{i_{j}}(_{0i})\|(_{t_{2}}_{1ij}, a_{ij}, b_{ij},_{ij})\|\\ +_{i_{j}}(_{0i})-(_{0j }^{*}+t_{1}),\] (4)

where \(_{t_{2}}_{1ij}:=_{1i}-_{1j}^{*}-t_{2}\), \( a_{ij}:=a_{i}-a_{j}^{*}\), \( b_{ij}:=b_{i}-b_{j}^{*}\), \(_{ij}:=_{i}-_{j}^{*}\). The infimum over \(t_{1}\) and \(t_{2}^{d}\) is to account for the identifiability up to the translation of \((_{0j}^{*},_{1j}^{*})_{j=1}^{k_{*}}\). Furthermore, \(_{j}\) is a Voronoi cell of mixing measure \(G\) generated by the true component \(_{j}^{*}:=(_{1j}^{*},a_{j}^{*},b_{j}^{*},_{j}^{*})\) for all \(1 j k_{*}\), which is defined as follows:

\[_{j}_{j}(G):=\{i\{1,2,,k\}:\|_{i}- _{j}^{*}\|\|_{i}-_{}^{*}\|,\  j\},\] (5)

where we denote \(_{i}:=(_{1i},a_{i},b_{i},_{i})\). It is worth noting that the cardinality of each Voronoi cell \(_{j}\) indicates the number of components of \(G\) approximating the true component \(_{j}^{*}\) of \(G_{*}\). As \(_{X}[h(g_{_{n}}(|X),g_{G_{*}}(|X))]=( n^{-1/2})\), that lower bound of Hellinger distance indicates that \(_{1}(_{n},G_{*})=(n^{-1/2})\). Therefore, the rates of estimating \((_{0j}^{*}),_{1j}^{*}\) (up to translations) and \(a_{j}^{*},_{j}^{*},_{j}^{*}\) are of optimal order \((n^{-1/2})\).

**2. Over-fitted settings**: When \(k>k_{*}\), the lower bound of Hellinger distance in terms of the Voronoi metric \(_{1}\) in the exact-fitted settings is no longer sufficient due to the collapse of softmax of vectors in possibly \(k\) dimensions to softmax of vectors in \(k_{*}\) dimensions. Our approach is to define more fine-grained Voronoi metric \(_{2}(G,G_{*})\) to capture such collapse, which is given by:

\[_{2}(G,G_{*}):=_{t_{1},t_{2}}_{j:|_{j} |>1}_{i_{j}}(_{0i})\|(_{t_{2}}_{1ij }, b_{ij})\|^{(|_{j}|)}+\|( a_{ij},_{ ij})\|^{(|_{j}|)/2}\] \[+_{j:|_{j}|=1}_{i_{j}}(_ {0i})\|(_{t_{2}}_{1ij}, a_{ij}, b_{ij},_{ ij})\|+_{j=1}^{k_{*}}_{i_{j}}(_{0i})-( _{0j}^{*}+t_{1}),\] (6)

for any mixing measure \(G_{k}()\). Here, the values of function \(()\) are determined by the solvability of a system of polynomial equations defined in equation (9). We then show in Lemma 1 that \((2)=4\), \((3)=6\), and we conjecture that \((m)=2m\) for any \(m 2\).

In high level, the aforementioned system of polynomial equations arises from the PDEs in equation (3) when we establish the lower bound \(_{X}[h(g_{G}(|X),g_{G_{*}}(|X))] C^{} _{2}(G,G_{*})\) for any \(G_{k}()\) for some universal constant \(C^{}\). Since \(_{X}[h(g_{_{n}}(|X),g_{G_{*}}(|X))]=(n^ {-1/2})\), we also have \(_{2}(_{n},G_{*})=(n^{-1/2})\) under the over-fitted settings of the softmax gating Gaussian mixture of experts. As a consequence, the rates for estimating true parameters whose Voronoi cells have only one component of the MLE are of order \((n^{-1/2})\). On the other hand, for true parameters \((_{0j}^{*}),_{1j}^{*},a_{j}^{*},b_{j}^{*},_{j}^{*}\) whose Voronoi cells have more than one component of the MLE, the estimation rates are respectively \((n^{-1/2(|_{j}|)})\) for \(_{1j}^{*},b_{j}^{*}\), \((n^{-1/(|_{j}|)})\) for \(a_{j}^{*},_{j}^{*}\), and \((n^{-1/2})\) for \((_{0j}^{*})\). This rich spectrum of parameter estimation rates is due to the complex interaction between the softmax gating and the expert functions.

**Practical implications:** Although the slow rates of the MLE under the over-fitted settings of the softmax gating Gaussian mixture of experts may seem discouraging, a practical implication of these results is that we should not choose the number of experts \(k\) to be very large compared to the true number of experts \(k_{*}\). Furthermore, the slow rates can also be useful for post-processing procedures, such as merge-truncate-merge procedure , with the MLE to reduce the number of experts so as to consistently estimate \(k_{*}\) when the number of data is sufficiently large. In particular, an important insight from the theoretical results is that we can merge the MLE parameters that are close and within the range of their rates of convergence or truncate the parameters that lead to small weights of the experts. As the sample size becomes sufficiently large, the reduced number of experts may converge to the true number of experts. We leave an investigation of such model selection with the Gaussian mixture of experts via the rates of MLE for future work.

**Organization:** The paper is organized as follows. In Section 2, we first provide background on the identifiability and rate of conditional density estimation in the softmax gating Gaussian mixture of experts. Next, we proceed to establish the convergence rate of the MLE under both the exact-fitted and over-fitted settings of these models in Section 3. Then, we conclude the paper with a few discussions in Section 4. Finally, full proofs of the results and a simulation study are provided in the Appendices.

**Notation:** Firstly, we denote \([n]:=\{1,2,,n\}\) for any positive integer \(n\). Next, for any vector \(u^{d}\) and \(z:=(z_{1},z_{2},,z_{d})^{d}\), we denote \(u^{z}=u_{1}^{z_{1}}u_{2}^{z_{2}} u_{d}^{z_{d}}\), \(|u|:=u_{1}+u_{2}++u_{d}\) and \(z!:=z_{1}!z_{2}! z_{d}!\), while \(\|u\|\) represents for its \(2\)-norm value. Additionally, the notation \(|A|\) indicates the cardinality of any set \(A\). Given any two positive sequences \(\{a_{n}\}_{n 1}\) and \(\{b_{n}\}_{n 1}\), we write \(a_{n}=(b_{n})\) or \(a_{n} b_{n}\) if \(a_{n} Cb_{n}\) for all \(n\), where \(C>0\) is some universal constant. Lastly, for any two probability density functions \(p,q\) dominated by the Lebesgue measure \(\), we denote

 
**Setting** & **Loss Function** & \(g_{G_{*}}(Y|X)\) & \((_{0j}^{*})\) & \(_{1j}^{*},b_{j}^{*}\) & \(a_{j}^{*},_{j}^{*}\) \\  Exact-fitted & \(_{1}\) & \((n^{-1/2})\) & \((n^{-1/2})\) & \((n^{-1/2})\) & \((n^{-1/2})\) \\  Over-fitted & \(_{2}\) & \((n^{-1/2})\) & \((n^{-1/2})\) & \((n^{-1/2(|_{j}|)})\) & \((n^{-1/(|_{j}|)})\) \\  

Table 1: Summary of density estimation and parameter estimation rates in the softmax gating Gaussian mixture of experts under both the exact-fitted and over-fitted settings. Recall that the cardinality of each Voronoi cell \(_{j}\) gives the number of fitted components approximating true component \(_{j}^{*}=(_{1j}^{*},a_{j}^{*},b_{j}^{*},_{j}^{*})\) (see equation (5)). Furthermore, the notation \((|_{j}|)\) stands for the solvability of the system of polynomial equations (9). For instance, if \(_{j}^{*}\) is fitted by two components, then we have \(|_{j}|=2\) and \((|_{j}|)=4\). Please refer to Lemma 1 for more details of the values of function \(\).

\(h^{2}(p,q)=(-)^{2}d\) as the their squared Hellinger distance and \(V(p,q)=|p-q|d\) as their Total Variation distance.

## 2 Background

In this section, we begin with the following result on the identifiability of the softmax gating Gaussian mixture of experts, which was previously studied in .

**Proposition 1** (Identifiability of the softmax gating Gaussian mixture of experts).: _For any mixing measures \(G=_{i=1}^{k}(_{0i})_{(_{1i},a_{i},b_{i},_{i})}\) and \(G^{}=_{i=1}^{k^{}}(^{}_{0i})_{(^{ }_{1i},a^{}_{i},b^{}_{i},^{}_{i})}\), if we have \(g_{G}(Y|X)=g_{G^{}}(Y|X)\) for almost surely \((X,Y)\), then it follows that \(k=k^{}\) and \(G G^{}_{t_{1},t_{2}}\) where \(G^{}_{t_{1},t_{2}}:=_{i=1}^{k^{}}(^{}_{0i}+t_{ 1})_{(^{}_{1i}+t_{2},a^{}_{i},b^{}_{i},^{ }_{i})}\) for some \(t_{1}\) and \(t_{2}^{d}\)._

Proof of Proposition 1 is in Appendix B.1. The identifiability of the softmax gating Gaussian mixture of experts guarantees that the MLE \(_{n}\) (2) converges to the true mixing measure \(G_{*}\) (up to the translation of the parameters in the softmax gating).

Given the consistency of the MLE, it is natural to ask about its convergence rate to the true parameters. Our next result establishes the convergence rate of conditional density estimation \(g_{_{n}}(Y|X)\) to the true conditional density \(g_{G_{*}}(Y|X)\), which lays an important foundation for the study of MLE's convergence rate.

**Proposition 2** (Density estimation rate).: _Given the MLE in equation (2), the conditional density estimation \(g_{_{n}}(Y|X)\) has the following convergence rate:_

\[(_{X}[h(g_{_{n}}(|X),g_{G_{*}}(|X))]> C((n)/n)^{1/2})(-c n),\]

_where \(c\) and \(C\) are universal constants._

Proof of Proposition 2 is in Appendix B.2. The result of Proposition 2 indicates that under either the exact-fitted or over-fitted settings of the softmax gating Gaussian mixture of experts, the rate of the conditional density function \(g_{_{n}}(Y|X)\) to the true one \(g_{G_{*}}(Y|X)\) under Hellinger distance is of order \((n^{-1/2})\) (up to some logarithmic factors), which is parametric on the sample size.

**From density estimation to parameter estimation:** The parametric rate of the conditional density estimation in Proposition 2 suggests that as long as we can establish the Hellinger lower bound \(_{X}[h(g_{G}(|X),g_{G_{*}}(|X))](G,G_{*})\) for any mixing measure \(G_{k}()\) for some metric \(\) among the parameters, then we obtain directly the parametric convergence rate of the MLE under the metric \(\). Therefore, the main focus of the next section is to determine such metric \(\) and to establish that lower bound under either exact-fitted or over-fitted settings of the Gaussian mixture of experts.

## 3 Convergence Rate of the Maximum Likelihood Estimation

In this section, we first study the convergence rate of the MLE under the exact-fitted settings of the softmax gating Gaussian mixture of experts in Section 3.1. Then, we move to the over-fitted settings in Section 3.2. Finally, we provide a proof sketch of the theories in Section 3.3.

### Exact-fitted Settings

For the exact-fitted settings, namely, when the chosen number of experts \(k\) is equal to the true number of experts \(k_{*}\), as we mentioned in the introduction, the proper metric between the MLE and the true mixing measure is the metric \(_{1}\) defined in equation (4), which is given by:

\[_{1}(G,G_{*}):=_{t_{1},t_{2}}\ _{j=1}^{k_{*}} _{i_{j}}(_{0i})\|(_{t_{2}}_{1ij}, a_{ij}, b_{ij},_{ij})\|\] \[+_{i_{j}}(_{0i})-(^{*}_ {0j}+t_{1}),\]where \(_{t_{2}}_{1ij}:=_{1i}-_{1j}^{*}-t_{2}\), \( a_{ij}:=a_{i}-a_{j}^{*}\), \( b_{ij}:=b_{i}-b_{j}^{*}\), \(_{ij}:=_{i}-_{j}^{*}\). Here, \(_{j}\) is a Voronoi cell of \(G\) generated by \((_{1j}^{*},a_{j}^{*},b_{j}^{*},_{j}^{*})\) for all \(1 j k_{*}\). Furthermore, the infimum is taken with respect to \((t_{1},t_{2})^{d}\) such that \(_{0j}^{*}+t_{1}\) and \(_{1j}^{*}+t_{2}\) still lie inside the domain of the parameter space \(\).

It is clear that \(_{1}(G,G_{*})=0\) if and only if \(G G_{*}\) (up to translation). When \(_{1}(G,G_{*})\) is sufficiently small, there exist \(t_{1},t_{2}\) such that all of \(_{t_{2}}_{1ij}\), \( a_{ij}\), \( b_{ij}\), \(_{ij}\), and \(_{i_{j}}(_{0i})-(_{0j}^{*}+t_{1})\) are sufficiently small as well. Therefore, the loss function \(_{1}\) provides a useful metric to measure the difference between the MLE and the true mixing measure. For any fixed \(t_{1},t_{2}\), the computation of the summations in \(_{1}\) only has the complexity of the order \((k_{*}^{2})\). To solve the optimization with respect to \(t_{1},t_{2}\) in the metric \(_{1}\), we can utilize the projected subgradient method with fixed step size , which has the complexity of the order \((^{-2})\) as the functions of \(t_{1}\) and \(t_{2}\) are convex where \(\) is a desired tolerance. Therefore, the total computational complexity of approximating the value of the Voronoi loss function \(_{1}\) is at the order of \((k_{*}^{2}/^{2})\).

The following result establishes the lower bound of the Hellinger distance between the conditional densities in terms of the loss function \(_{1}\) between corresponding mixing measures, which in turn leads to the convergence rate of the MLE.

**Theorem 1**.: _Given the exact-fitted settings of the softmax gating Gaussian mixture of experts (1), i.e., \(k=k_{*}\), we find that_

\[_{X}[h(g_{G}(|X),g_{G_{*}}(|X))] C_{1} _{1}(G,G_{*}),\] (7)

_for any \(G_{k_{*}}():=_{k_{*}}()_{k_{*}-1}()\) where \(C_{1}\) is some universal constant depending only on \(G_{*}\) and \(\). As a consequence, there exist universal constants \(C_{1}^{}\) and \(c_{1}\) such that the convergence rate of the MLE \(_{n}\) under the exact-fitted settings satisfies:_

\[(_{1}(_{n},G_{*})>C_{1}^{}((n)/n)^{1 /2})(-c_{1} n).\] (8)

Proof of Theorem 1 is in Appendix A.1. The parametric convergence rate of the MLE to \(G_{*}\) under the metric \(_{1}\) suggests that the rates of estimating the true parameters \((_{0j}^{*}),_{1j}^{*}\) (up to translation), \(a_{j}^{*},b_{j}^{*},_{j}^{*}\) for \(j[k_{*}]\) are of order \((n^{-1/2})\), which are optimal up to logarithmic factors.

### Over-fitted Settings

We now consider the over-fitted settings of the softmax gating Gaussian mixture of experts. Different from the exact-fitted settings, the softmax weights associated with the MLE collapse to the softmax weights of the mixture of true experts as long as the MLE approaches the true mixing measure \(G_{*}\). More concretely, we can relabel the supports of the MLE \(_{n}\) with \(_{n}\) components (\(_{n} k\)) based on the Voronoi cells \(_{j}^{n}:=_{j}(_{n})\) such that we can rewrite it as \(_{n}=_{j=1}^{k_{*}}_{i_{j}^{n}}(_{0i}^{*})_{(_{1i^{*}}^{n},_{i}^{n},_{i^{*}}^{n},_{i^{*}}^{n})}\) where \(_{j=1}^{k_{*}}|_{j}^{n}|=_{n}\), \((_{i}^{n},_{i}^{n},_{i}^{n})(a_ {j}^{*},b_{j}^{*},_{j}^{*})\),

\[_{i_{j}^{n}}_{1i}^{n})^{}X+ _{0i}^{n})}{_{j^{}=1}^{k_{*}}_{i^{} _{j}^{n}}((_{1i^{}}^{n})^{}X+ {}_{0i^{}}^{n})}}^{*})^{}X+ _{0j}^{*})}{_{j^{}=1}^{k_{*}}((_{1j^{}}^{*})^{ }X+_{0j^{}}^{*})}\]

as \(n\) approaches infinity for all \(1 i_{j}^{n}\) and \(j[k_{*}]\).

The collapse of the softmax weights along with the PDEs (3) between the softmax gating and the expert functions in the Gaussian density create a complex interaction among the estimated parameters. To disentangle such interaction, we rely on the solvability of a novel system of polynomial equations defined in equation (9). In particular, for any \(m 2\), we define \((m)\) as the smallest natural number \(r\) such that the following system of polynomial equations:

\[_{j=1}^{m}_{(_{1},_{2},_{3},_{4}) _{_{1},_{2}}}^{_{1}}p_{1j}^{_{1}}\;p_{2j}^{ _{2}}\;p_{3j}^{_{3}}\;p_{4j}^{_{4}}}{_{1}!\;_{2}! \;_{3}!\;_{4}!}=0,\] (9)

for any \((_{1},_{2})^{d}\) such that \(0|_{1}| r\), \(0_{2} r-|_{1}|\) and \(|_{1}|+_{2} 1\), does not have any non-trivial solution for the unknown variables \(\{p_{1j},p_{2j},p_{3j},p_{4j},p_{5j}\}_{j=1}^{m}\), namely, all of \(p_{5j}\) are non-zero and at least one among \(p_{3j}\) is different from zero. The ranges of \(_{1},_{2},_{3},_{4}\) in the above sum satisfy \(_{_{1},_{2}}=\{=(_{1},_{2},_{3},_ {4})^{d}^{d}:\ _{1}+_{2}=_{1},\ | _{2}|+_{3}+2_{4}=_{2}\}\). When \(d=1\) and \(r=2\), that system of equations becomes

\[_{j=1}^{m}p_{5j}^{2}p_{1j}=0,\ _{j=1}^{m}p_{5j}^{2}p_{1j}^{2}=0, \ _{j=1}^{m}p_{5j}^{2}(p_{1j}p_{3j}+p_{2j})=0,\] \[_{j=1}^{m}p_{5j}^{2}p_{3j}=0,\ _{j=1}^{m}p_{5j}^{2} p_{3j}^{2}+p_{4j}=0.\]

It is clear that we have non-trivial solutions \(p_{5j}=1\), \(p_{1j}=0\) for all \(j[m]\), \(|p_{21}|=p_{31}=1\), \(|p_{22}|=p_{32}=-1\), \(p_{41}=p_{42}=-1/2\), \(p_{2j}=p_{3j}=p_{4j}=0\) for \(3 j m\).

When \(d=1\) and \(r=3\), the system of equations can be written as follows:

\[_{j=1}^{m}p_{5j}^{2}p_{1j}=0,_{j=1}^{m}p_{5j}^{2}p_{ 3j}=0,_{j=1}^{m}p_{5j}^{2}(p_{2j}+p_{1j}p_{3j})=0,\] \[_{j=1}^{m}p_{5j}^{2}p_{1j}^{2}=0,_{j=1}^{m}p_{5j}^{2 }p_{3j}^{2}+p_{4j}=0,_{j=1}^{m}p_{5j}^{2} p_{3j}^{3}+p_{3j}p_{4j}=0,\] \[_{j=1}^{m}p_{5j}^{2}p_{1j}^{3}=0,_{j=1}^{m}p_{5j}^{2 }p_{1j}^{2}p_{3j}+p_{1j}p_{2j}=0,\] \[_{j=1}^{m}p_{5j}^{2}p_{1j} p_{3j}^{2} +p_{1j}p_{4j}+p_{2j}p_{3j}=0.\]

It can be seen that the following is a non-trivial solution of the above system: \(p_{5j}=1\), \(p_{1j}=p_{2j}=0\) for all \(j[m]\), \(p_{31}=}{3}\), \(p_{32}=-}{3}\), \(p_{41}=p_{42}=-\), \(p_{3j}=p_{4j}=0\) for \(3 j m\). Therefore, we obtain that \((m) 4\) when \(m 2\) and \(d=1\).

In general, when \(d=1\), the system of equations has \((r^{2}+3r)/2\) equations. Intuitively, when \(m\) is sufficiently larger than \((r^{2}+3r)/2\), the system may not have a non-trivial solution. For general dimension \(d\) and parameter \(m 2\), finding the exact value of \((m)\) is a non-trivial central problem in algebraic geometry . When \(m\) is small, the following lemma provides specific values for \((m)\).

**Lemma 1**.: _For any \(d 1\), when \(m=2\), \((m)=4\). When \(m=3\), \((m)=6\)._

Proof of Lemma 1 is in Appendix B.3. As \(m\) increases, so does the value of \((m)\). We conjecture that \((m)=2m\) and leave the proof of that conjecture to future work.

By constructing the Voronoi loss function:

\[_{2}(G,G_{*}):=_{i_{1},t_{2}}_{j:|_{j}|>1}_{i _{j}}(_{0i})\|(_{t_{2}}_{1ij}, _{ij})\|^{(|_{j}|)}+\|( a_{ij},_{ ij})\|^{(|_{j}|)/2}\]

\[+_{j:|_{j}|=1}_{i_{j}}(_{0i})\|( _{t_{2}}_{1ij}, a_{ij}, b_{ij},_{ij})\|+ _{j=1}^{k_{*}}_{i_{j}}(_{0i})-(_{0j }^{*}+t_{1}),\]

the following result demonstrates that the convergence rates of the MLE under the over-fitted settings of the softmax gating Gaussian mixture of experts are determined by \(()\).

**Theorem 2**.: _Under the over-fitted settings of the softmax gating Gaussian mixture of experts (1), namely, when \(k>k_{*}\), we obtain that_

\[_{X}[h(g_{G}(|X),g_{G_{*}}(|X))] C_{2}_{ 2}(G,G_{*}),\] (10)

_for any \(G_{k}()\) where \(C_{2}\) is some universal constant depending only on \(G_{*}\) and \(\). Therefore, that lower bound leads to the following convergence rate of the MLE:_

\[(_{2}(_{n},G_{*})>C_{2}^{}((n)/n)^{1/ 2})(-c_{2} n),\] (11)

_where \(C_{2}^{}\) and \(c_{2}\) are some universal constants._Proof of Theorem 2 is in Appendix A.2. A few comments with the result of Theorem 2 are in order.

**(i) Rates of individual parameters:** The convergence rate \((n^{-1/2})\) (up to some logarithmic term) of the MLE under the loss function \(_{2}\) implies that for the true parameters \((^{*}_{0j}),^{*}_{1j},a^{*}_{j},b^{*}_{j},^{*}_{j}\) whose Voronoi cells have only one component of the MLE, the rates for estimating them are \((n^{-1/2})\) up to some logarithmic factor. On the other hand, for true parameters with greater than one component in their Voronoi cells, the rates for estimating \(^{*}_{1j}\), \(b^{*}_{j}\) are \((n^{-1/2(|^{*}_{j}|)})\) while those for \(a^{*}_{j},^{*}_{j}\) are \((n^{-1/(|^{*}_{j}|)})\) (up to logarithmic factors). As the maximum value of \(|^{*}_{j}|\) is \(_{n}-k_{*}+1\), it indicates that these rates (up to logarithmic factors) can be as worse as \((n^{-1/(k_{n}-k_{*}+1)})\) for estimating \(a^{*}_{j},^{*}_{j}\) and \((n^{-1/2(_{n}-k_{*}+1)})\) for estimating \(^{*}_{1j},b^{*}_{j}\).

**(ii) Computation of Voronoi loss function \(_{2}\):** Similar to the Voronoi loss function \(_{1}\) in the exact-fitted setting, the loss function \(_{2}\) is also computationally efficient. In particular, for any fixed \(t_{1},t_{2}\), the computation of the summations in the formulation of \(_{2}\) is at the order \((k k_{*})\), which is linear on \(k\) when \(k_{*}\) is fixed. Furthermore, we can solve the convex optimization problem with respect to \(t_{1},t_{2}\) with computational complexity at the order of \((^{-2})\) via the projected gradient descent method with fixed step size where \(\) is the error. Therefore, the total computational complexity of approximating the Voronoi loss function \(_{2}\) is at the order of \((k k_{*}/^{2})\).

**(iii) Comparison with covariate-free gating network:** We would like to remark that the results being established for parameter estimation under the softmax gating network settings of over-fitted Gaussian mixture of experts are in stark difference from those under the covariate-free gating network settings of these models , namely, when the gating function is independent of the covariates \(X\). In particular, Theorem 2 in  shows that when the gating networks are independent of the covariates, the convergence rates of estimating \(a^{*}_{j}\) are at the order of \((n^{-1/4})\) (up to some logarithmic factor), which are independent of the number of over-fitted components. It is different from the rates of \(a^{*}_{j}\) whose Voronoi cells have more than one component in the softmax gating settings, which depends on the number of components that we over-fit the Gaussian mixture of experts (see discussion (i) after Theorem 2). Furthermore, the rates of estimating \(b^{*}_{j},^{*}_{j}\) when the gating networks are independent of covariates are determined by a system of polynomial equations that is much simpler than the system of equations (9) when the gating networks are softmax function. These differences are mainly due to the intrinsic interaction characterized by partial differential equations with respect to the parameters between the softmax gating networks and the expert functions in Gaussian distribution.

### Proof Sketch

In this section, we provide a proof sketch for Theorems 1 and 2. To simplify the ensuing discussion, the loss function \(\) in the proof sketch is implicitly understood as either \(_{1}\) or \(_{2}\) depending on the settings of the softmax gating Gaussian mixture of experts. Since the Hellinger distance \(h\) is lower bounded by the Total Variation distance \(V\), to obtain the bounds in equations (7) and (10), it is sufficient to show that \(_{X}[V(g_{G}(|X),g_{G_{*}}(|X))](G,G_{*})\). To establish this bound, we respectively prove its local and global versions by contradiction as follows:

**Local version**: In this part, we aim to show the following local inequality:

\[_{ 0}_{G_{k}():(G,G_{*}) }_{X}[V(g_{G}(|X),g_{G_{*}}(|X))]/(G,G_{*})>0.\] (12)

Assume that this claim does not hold true, that is, there exists a sequence \(G_{n}=_{i=1}^{k_{n}}(^{n}_{0i})_{(^{n}_{1i},a^{n}_{i},b ^{n}_{i},^{n}_{i})}^{}_{k}()\) such that both \(_{X}[V(g_{G_{n}}(|X),g_{G_{*}}(|X))]/(G_{n},G_ {*})\) and \((G_{n},G_{*})\) approach zero as \(n\) tends to infinity. This implies that for any \(j[k_{*}]\), we have \(_{i_{j}}(^{n}_{0i})(^{*}_{0j})\) and \((^{n}_{1i},a^{n}_{i},b^{n}_{i},^{n}_{i})(^{*}_{1j},a^{*}_{j},b^{*}_{j},^{*}_{j})\) and for all \(i_{j}\). For the sake of presentation, we simplify the loss function \(\) by assuming that it is minimized when \(t_{1}=0\) and \(t_{2}=_{d}\). Now, we decompose the quantity

\[Q_{n}=_{j^{}=1}^{k_{*}}((^{*}_{1j^{}})^{}X+ ^{*}_{0j^{}})[g_{G_{n}}(Y|X)-g_{G_{*}}(Y|X)]\]as follows:

\[Q_{n} =_{j=1}^{k_{*}}_{i_{j}}(^{n}_{0i}) u(Y|X;^{n}_{1i},a^{n}_{i},b^{n}_{i},^{n}_{i})-u(Y|X;^{*}_{ 1j},a^{*}_{j},b^{*}_{j},^{*}_{j})-v(Y|X;^{n}_{1i})\] \[+v(Y|X;^{*}_{1j})+_{j=1}^{k_{*}}_{i _{j}}(^{n}_{0i})-(^{*}_{0j})u(Y|X; ^{*}_{0j},a^{*}_{j},b^{*}_{j},^{*}_{j})-v(Y|X;^{*}_{1j}),\]

where we define \(u(Y|X;_{1},a,b,):=(^{}_{1}X)f(Y|a^{}X+b,)\) and \(v(Y|X;_{1}):=(^{}_{1}X)g_{G_{n}}(Y|X)\). Next, for each \(j[k_{*}]\) and \(i_{j}\), we denote \(h_{1}(X,a^{*}_{j},b^{*}_{j}):=(a^{*}_{j})^{}X+b^{*}_{j}\) and then apply the Taylor expansions to the functions \(u(Y|X;^{n}_{1i},a^{n}_{i},b^{n}_{i},^{n}_{i})\) and \(v(Y|X;^{n}_{1i})\) up to orders \(r_{1j}\) and \(r_{2j}\) (which we will choose later), respectively, as follows:

\[u(Y|X;^{n}_{1i},a^{n}_{i},b^{n}_{i},^{n}_{i})-u(Y|X; ^{*}_{1j},a^{*}_{j},b^{*}_{j},^{*}_{j})\] \[=_{|_{1}|+_{2}=1}^{2r_{1j}}T^{n}_{_{1},_{2}}( j)X^{_{1}}((^{*}_{1j})^{}X)}f}{  h^{_{2}}_{1}}(Y|(a^{*}_{j})^{}X+b^{*}_{j},^{*}_{j})+R_{ 1ij}(X,Y),\] \[v(Y|X;^{n}_{1i})-v(Y|X;^{*}_{1j})=_{||=1}^{r_ {2j}}S^{n}_{}(j)X^{}((^{*}_{1j})^{}X)g_{G_{n}}(Y|X)+ R_{2ij}(X,Y),\]

where \(R_{1ij}(X,Y)\) and \(R_{2ij}(X,Y)\) are Taylor remainders such that \(R_{ ij}(X,Y)/(G_{n},G_{*})\) vanishes as \(n\) for \(\{1,2\}\). As a result, the limit of \(Q_{n}/(G_{n},G_{*})\) when \(n\) goes to infinity can be seen as a linear combination of elements of the following set:

\[: =\{X^{_{1}}((^{*}_{1j})^{}X)}f}{ h^{_{2}}_{1}}(Y|(a^{*}_{j})^{}X+b^{*}_ {j},^{*}_{j}):j[k_{*}],\;0 2|_{1}|+_{2} 2r_{1j}\}\] \[\{X^{}((^{*}_{1j})^{}X)g_{G_{*}}(Y|X ):j[k_{*}],\;0|| r_{2j}\},\]

which is shown to be linearly independent. By the Fatou's lemma, we demonstrate that \(Q_{n}/(G_{n},G_{*})\) goes to zero as \(n\), implying that all the coefficients in the representation of \(Q_{n}/(G_{n},G_{*})\), denoted by \(T^{n}_{_{1},_{2}}(j)/(G_{n},G_{*})\) and \(S^{n}_{}(j)/(G_{n},G_{*})\), vanish when \(n\). Given that result, we aim to select the Taylor orders \(r_{1j}\) and \(r_{2j}\) such that at least one among the limits of \(T^{n}_{_{1},_{2}}(j)/(G_{n},G_{*})\) and \(S^{n}_{}(j)/(G_{n},G_{*})\) is different from zero, which leads to a contradiction. Hence, we obtain the local version of the desired inequality.

Below are the details of choosing appropriate Taylor orders in each setting.

**Exact-fitted settings:** Under this setting, since \(k_{*}\) is known, each of the Voronoi cells \(_{j}\) for \(j[k_{*}]\) has only one element. Thus, for any \(i_{j}\), we have \((^{n}_{0i})(^{*}_{0j})\) and \((^{n}_{1i},a^{n}_{i},b^{n}_{i},^{n}_{i})(^{*}_{1j},a^{*}_{j },b^{*}_{j},^{*}_{j})\). Given that result, we will select \(r_{1j}=r_{2j}=1\) for all \(j[k_{*}]\) as it suffices to show that at least one among the limits of \(T^{n}_{_{1},_{2}}(j)/(G_{n},G_{*})\) and \(S^{n}_{}(j)/(G_{n},G_{*})\) is different from zero. In particular, if all of them vanished, we would take the sum of all the limits of \(T^{n}_{_{1},_{2}}(j)/(G_{n},G_{*})\) for \((_{1},_{2})\) such that \(0 2|_{1}|+_{2} 2\), which leads to a contradiction that \(1=(G_{n},G_{*})/(G_{n},G_{*}) 0\).

**Over-fitted settings:** As \(k_{*}\) becomes unknown in this scenario, we need higher Taylor orders to obtain the same result as in the exact-fitted setting. We will reuse the proof by contradiction method to find out those orders. More specifically, assume that all the limits of \(T^{n}_{_{1},_{2}}(j)/(G_{n},G_{*})\) and \(S^{n}_{}(j)/(G_{n},G_{*})\) equal zero. After some steps of considering typical limits as in the previous setting which requires \(r_{2j}=2\) for all \(j[k_{*}]\), we encounter the following system of polynomial equations:

\[_{i_{j}}_{(_{1},_{2},_{3},_{4}) _{_{1},_{2}}}_{5i}\,p^{_{1i}}_{1i}\,p^{ _{2}}_{2i}\,p^{_{3}}_{3i}\,p^{_{4}}_{4i}}{_{1}!\,_ {2}!\,_{3}!\,_{4}!}=0,\]

for all \((_{1},_{2})^{d}\) such that \(0|_{1}| r_{1j}\), \(0_{2} r_{1j}-|_{1}|\) and \(|_{1}|+_{2} 1\) for some \(j[k_{*}]\). Due to the construction of this system, it must have at least one non-trivialsolution. Therefore, if we choose \(r_{1j}=(|_{j}|)\) for all \(j[k_{*}]\), then the above system does not admit any non-trivial solutions, which leads to a contradiction. Hence, we obtain the local inequality in equation (12), which suggests that we can find a positive constant \(^{}\) such that \(_{G_{k}():(G,G_{*})^{ }}_{X}[V(g_{G}(|X),g_{G_{*}}(|X))]/(G,G_{*})>0\).

**Global version:** Therefore, it is sufficient to demonstrate the following global inequality:

\[_{G_{k}(),(G,G_{*})>^{}} _{X}[V(g_{G}(|X),g_{G_{*}}(|X))]/(G,G_{*})>0.\] (13)

Assume that this claim is not true, then we can find a mixing measure \(G^{}_{k}()\) such that \(g_{G^{}}(Y|X)=g_{G_{*}}(Y|X)\) for almost surely \((X,Y)\). According to Proposition 1, we get that \((G^{},G_{*})=0\), which contradicts the hypothesis \((G^{},G_{*})>^{}\). These arguments hold for both exact-fitted and over-fitted settings up to some changes of notations.

Hence, the proof sketch is completed.

## 4 Discussion

In the paper, we study the convergence rates of parameter estimation under both the exact-fitted and over-fitted settings of the softmax gating Gaussian mixture of experts. We introduce novel Voronoi loss functions among parameters to resolve fundamental theoretical challenges posed by the softmax gating function, including identifiability up to the translation of parameters, the interaction between softmax weights and expert functions, and the dependence between the numerator and denominator of the conditional density function. When the true number of experts is known, we demonstrate that the rates for estimating true parameters are parametric on the sample size. On the other hand, when the true number of experts is unknown and over-specified, these estimation rates turn out to be determined by the solvability of a system of polynomial equations.

There are a few natural directions arising from the paper that we leave for future work:

* First, our work does not consider the top-K sparse softmax gating function, which has been widely used to scale up massive deep learning architectures [68; 54; 21]. It is practically important to extend the current theories to establish the convergence rates of parameter estimation in the Gaussian mixture of experts with that gating function.
* Second, the paper only takes into account the regression settings, namely when the distribution of \(Y\) is assumed to be continuous. Given that mixture of experts has also been used in classification settings [22; 31; 53; 34; 35; 60], namely when \(Y\) is a discrete response variable, it is desirable to establish a comprehensive theory for parameter estimation under these settings of mixtures of experts.
* Third, the theories developed in the paper lay an important foundation for understanding parameter estimation in more complex models, including hierarchical mixture of experts [33; 51; 37; 66] and multigate mixture of experts [44; 26; 45].
* Finally, the convergence rates of the MLE in this work are established under the well-specified settings, namely when the data are drawn from the softmax gating Gaussian mixture of experts. Nevertheless, the convergence analysis of the MLE under the misspecified settings, namely when the data are not necessarily generated from that model, has remained poorly understood. Under those settings, the MLE \(_{n}\) converges to the mixing measures \(_{G_{k}()}(g_{G}(Y|X),p(Y| X))\) where \(p(Y|X)\) is the true conditional density function of \(Y\) given \(X\), and it is not a softmax gating Gaussian mixture of experts. Additionally, the notation KL stands for the Kullback-Leibler divergence. The insights from our theories under the well-specified setting indicate that the Voronoi loss functions can be used to obtain the precise rates of individual parameters of the MLE \(_{n}\) to those of the mixing measure \(\).