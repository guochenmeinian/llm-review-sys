# Accelerating Reinforcement Learning with

Value-Conditional State Entropy Exploration

 Dongyoung Kim

KAIST

&Jinwoo Shin

KAIST

&Pieter Abbeel

UC Berkeley

&Younggyo Seo

KAIST

Now at Dyson Robot Learning Lab. Correspondence to younggyo.seo@dyson.com.

###### Abstract

A promising technique for exploration is to maximize the entropy of visited state distribution, _i.e.,_ state entropy, by encouraging uniform coverage of visited state space. While it has been effective for an unsupervised setup, it tends to struggle in a supervised setup with a task reward, where an agent prefers to visit high-value states to exploit the task reward. Such a preference can cause an imbalance between the distributions of high-value states and low-value states, which biases exploration towards low-value state regions as a result of the state entropy increasing when the distribution becomes more uniform. This issue is exacerbated when high-value states are narrowly distributed within the state space, making it difficult for the agent to complete the tasks. In this paper, we present a novel exploration technique that maximizes the _value-conditional state entropy_, which separately estimates the state entropies that are conditioned on the value estimates of each state, then maximizes their average. By only considering the visited states with similar value estimates for computing the intrinsic bonus, our method prevents the distribution of low-value states from affecting exploration around high-value states, and vice versa. We demonstrate that the proposed alternative to the state entropy baseline significantly accelerates various reinforcement learning algorithms across a variety of tasks within MiniGrid, DeepMind Control Suite, and Meta-World benchmarks. Source code is available at https://sites.google.com/view/rl-vce.

## 1 Introduction

Recent advances in exploration techniques have enabled us to train strong reinforcement learning (RL) agents with fewer environment interactions. Notable techniques include injecting noise into action or parameter spaces (Sehnke et al., 2010; Ruckstiess et al., 2010; Wawrzynski, 2015; Lillicrap et al., 2016; Fortunato et al., 2018) and using visitation counts (Thrun, 1992; Bellemare et al., 2016; Sutton and Barto, 2018; Burda et al., 2019) or errors from predictive models (Stadie et al., 2015; Pathak et al., 2017, 2019; Sekar et al., 2020) as an intrinsic reward. In particular, a recently developed approach that maximizes the entropy of visited state distribution has emerged as a promising exploration technique (Hazan et al., 2019; Lee et al., 2019; Mutti et al., 2021) for unsupervised RL, where the agent aims to learn useful behaviors without any task reward (Liu and Abbeel, 2021, 2021, 2021).

The idea to maximize the state entropy has also been utilized in a supervised setup where the task reward is available from environments to improve the sample-efficiency of RL algorithms (Tao et al., 2020; Seo et al., 2021; Nedergaard and Cook, 2022; Yuan et al., 2022). Notably, Seo et al. (2021) have shown that maximizing the sum of task reward and intrinsic reward based on a state entropy estimate can accelerate RL training. However, in this supervised setup, we point out that this approach often suffers from an imbalance between the distributions of high-value and low-value states, which occurs as an agent prefers to visit high-value states for exploiting the task reward. Because state entropy increases when the distribution becomes more uniform, low-value states get to receive a higherintrinsic bonus than high-value states, which biases exploration towards low-value states. This makes it difficult for the agent to explore the region around high-value states crucial for solving target tasks, which exacerbates when high-value states are narrowly distributed within the state space.

In this paper, we present a novel exploration technique that maximizes _value-conditional state entropy_ which separately estimates the state entropies that are conditioned on the value estimates of each state, then maximizes their average. This intuitively can be seen as partitioning the state space with value estimates and maximizing the average of state entropies of partitioned spaces. Namely, our method avoids the problem of state entropy maximization by preventing the distribution of low-value states from affecting exploration around high-value states, and vice versa, by only considering the states with similar value estimates for computing the intrinsic bonus. For value-conditional state entropy estimation, we utilize the Kraskov-Stogbauer-Grassberger estimator (Kraskov et al., 2004) along with a value normalization scheme that makes value distribution consistent throughout training. We define our intrinsic reward as proportional to the value-conditional state entropy estimate and train RL agents to maximize the sum of task reward and intrinsic reward.

We summarize the main contributions of this paper:

* We present a novel exploration technique that maximizes _value-conditional state entropy_ which addresses the issue of state entropy exploration in a supervised setup by taking into account the value estimates of visited states for computing the intrinsic bonus.
* We show that maximum value-conditional state entropy (VCSE) exploration successfully accelerates the training of RL algorithms (Mnih et al., 2016; Yarats et al., 2021) on a variety of domains, such as MiniGrid (Chevalier-Boisvert et al., 2018), DeepMind Control Suite (Tassa et al., 2020), and Meta-World (Yu et al., 2020) benchmarks.

## 2 Related Work

Exploration in RLExploration has been actively studied to solve sparse reward tasks or avoid being stuck at local optima. One of the classical approaches to encourage exploration is \(\)-greedy algorithm (Sutton and Barto, 2018). This idea has been extended to approaches that inject noise into the action space (Wawrzynski, 2015; Lillicrap et al., 2016) or parameter space (Sehnke et al., 2010; Ruckstiess et al., 2010; Fortunato et al., 2018; Plappert et al., 2018) and approaches that maximize the action entropy (Ziebart, 2010; Haarnoja et al., 2018). Similarly to our work that introduces an intrinsic reward, there have been approaches that use the errors from predictive models as an intrinsic reward (Schmidhuber, 1991; Oudeyer et al., 2007; Stadie et al., 2015; Pathak et al., 2017, 2019; Sekar et al., 2020; Badia et al., 2020). Instead of using the knowledge captured in the model, we use a metric that can be quantified from data. The idea of using the state visitation count as an intrinsic

Figure 1: Illustration of our method. We randomly sample states from a replay buffer and compute the Euclidean norm in state and value spaces using pairs of samples within a minibatch. We then sort the samples based on their maximum norms. We find the \(k\)-th nearest neighbor among samples (_e.g.,_\(k=3\) in the figure) and use the distance to it as an intrinsic reward. Namely, our method excludes the samples whose values significantly differ for computing the intrinsic reward. Then we train our RL agent to maximize the sum of the intrinsic reward and the extrinsic reward.

reward (Thrun, 1992; Bellemare et al., 2016; Tang et al., 2017; Ostrovski et al., 2017; Burda et al., 2019) is also related. However, our method differs in that we directly maximize the diversity of data. Our work also differs from a line of work that balances exploration and exploitation (Thrun, 1992; Brafman and Tennenholtz, 2002; Tokic, 2010; Whitney et al., 2021; Schafer et al., 2022) as we instead consider a situation where exploitation biases exploration towards specific state space regions.

Maximum state entropy explorationThe approach most related to our work is to utilize state entropy as an intrinsic reward. A popular application of maximizing state entropy is unsupervised RL (Lee et al., 2019; Hazan et al., 2019; Muti and Restelli, 2020; Mutti et al., 2021; Liu and Abbeel, 2021; Yarats et al., 2021; Zhang et al., 2021; Guo et al., 2021; Mutti et al., 2022a,b; Yang and Spaan, 2023), where the agent learns useful behaviors without task reward. We instead consider a setup where task reward is available. A potentially related approach is of Yang and Spaan (2023) which introduces safety constraint for exploration in that one can also consider the approach of introducing a value constraint for exploration. However, our work differs in that our motivation is not to discourage exploration on specific state regions. The work closest to ours is approaches that make agents maximize state entropy along with task rewards (Tao et al., 2020; Seo et al., 2021; Nedergaard and Cook, 2022; Yuan et al., 2022). We point out they often suffer from an imbalance between distributions of high-value and low-value states and propose to take into account the value estimates to address this issue.

## 3 Preliminaries

DefinitionLet \(Z=(X,Y)\) be a random variable whose joint entropy is defined as \((X,Y)=-E_{(x,y) p(x,y)}[ p(x,y)]\) and marginal entropy is defined as \((X)=-E_{x p_{X}(x)}[ p_{X}(x)]\). The conditional entropy is defined as \((Y\,|\,X)=E_{(x,y) p(x,y)}[ p(y\,|\,x)]\), which quantifies the amount of information required for describing the outcome of Y given that the value of X is known.

Reinforcement learningWe formulate a problem as a Markov decision process (MDP; Sutton and Barto 2018), which is defined as a tuple \((,,p,r^{},)\). Here, \(\) denotes the state space, \(\) denotes the action space, \(p(s_{t+1}|s_{t},a_{t})\) is the transition dynamics, \(r^{}\) is the extrinsic reward function \(r^{}_{t}=r^{}(s_{t},a_{t})\), and \([0,1)\). Then we train an agent to maximize the expected return.

### Maximum State Entropy Exploration

\(k\)-nearest neighbor entropy estimatorWhen we aim to estimate the entropy of \(X\) but the density \(p_{X}\) is not available, non-parametric entropy estimators based on \(k\)-nearest neighbor distance can be used. Specifically, given \(N\) i.i.d. samples \(\{x_{i}\}_{i=1}^{N}\), the Kozachenko-Leonenko (KL) entropy estimator (Kozachenko and Leonenko, 1987; Singh et al., 2003; Kraskov et al., 2004) is defined as:

\[_{}(X)= -(k)+(N)+ c_{d_{X}}+}{N}_{i=1}^{N}  D_{x}(i),\] (1)

where \(\) is the digamma function, \(D_{x}(i)\) is twice the distance from \(x_{i}\) to its \(k\)-th nearest neighbor, \(d_{X}\) is the dimensionality of X, and \(c_{d_{X}}\) is the volume of the \(d_{X}\)-dimensional unit ball.2

State entropy as an intrinsic rewardLet \(\) be a replay buffer and \(S\) be a random variable of dimension \(d_{s}\) with a probability density \(p_{S}(s)=_{s}_{S=s}/||\). The main idea of maximum state entropy exploration is to encourage an agent to maximize the state entropy \((S)\)(Liu and Abbeel, 2021; Seo et al., 2021). The KL entropy estimator in Equation 1 is used to estimate the state entropy \(_{}(S)\), and the intrinsic reward \(r^{}_{i}\) is defined by ignoring constant terms and putting logarithm along with an additive constant 1 for numerical stability as below:

\[_{}(S)=-(k)+(N)+ c_{d_{S}}+}{N} _{i=1}^{N} D_{s}(i) r^{}_{i}=(D_{s}(i)+1)\] (2)

where \(D_{s}(i)\) is twice the distance from \(s_{i}\) to its \(k\)-nearest neighbor.

### Conditional Entropy Estimation

Naive conditional entropy estimatorGiven \(N\) i.i.d. samples \(\{z_{i}\}_{i=1}^{N}\) where \(z_{i}=(x_{i},y_{i})\), the conditional entropy can be estimated by using the chain rule of conditional entropy \(H(Y\,|\,X)=H(X,Y)-H(X)\). Specifically, one can compute \(_{}(X,Y)\) and \(_{}(X)\) and use these estimates for computing \((Y\,|\,X)\). But a major drawback of this naive approach is that it does not consider length scales in spaces spanned by \(X\) and \(Y\) can be very different, _i.e.,_ the scale of distances between samples and their \(k\)-nearest neighbors in each space could be very different.

KSG conditional entropy estimatorTo address the issue of estimating entropy in different spaces spanned by \(X\) and \(Y\), we employ Kraskov-Stogbauer-Grassberger (KSG) estimator (Kraskov et al., 2004), which is originally designed for estimating mutual information. The main idea of KSG estimator is to use different \(k\)-values in the joint and marginal spaces for adjusting the different length scales across spaces. Given a sample \(z_{i}\) in the joint space and the maximum norm \(||z-z^{}||_{}=\{||x-x^{}||,||y-y^{}||\}\),3 we find a \(k\)-th nearest neighbor \(z_{i}^{k}=(x_{i}^{k},y_{i}^{k})\). To find a value that makes \(x_{i}^{k}\) be \(n_{x}(i)\)-th nearest neighbor of \(x_{i}\) in the space spanned by \(X\), we count the number \(n_{x}(i)\) of points \(x_{j}\) whose distances to \(x_{i}\) are less than \(_{x}(i)/2\), where \(_{x}(i)\) is twice the distance from \(x_{i}\) to \(x_{i}^{k}\) (see Appendix D for an illustrative example). We note that \(_{y}(i)\) and \(n_{y}(i)\) can be similarly defined by replacing \(x\) by \(y\). Now let \((i)\) be twice the distance between \(z_{i}\) and \(z_{i}^{k}\), _i.e.,_\((i)=2||z_{i}-z_{i}^{k}||_{}\). Then KSG estimators for the joint and marginal entropies are given as:

\[_{}(X,Y)=-(k)+(N)+(c_{d_{X}}c_{d_{Y}})+ {d_{X}+d_{Y}}{N}_{i=1}^{N}(i)\] (3)

\[_{}(X)=-_{i=1}^{N}(n_{x}(i)+1)+( N)+ c_{d_{X}}+}{N}_{i=1}^{N}_{x}(i)\] (4)

Then we use the chain rule of conditional entropy \(_{}(X,Y)-_{}(X)\) and estimators from Equation 3 and Equation 4 to obtain a conditional entropy estimator and its lower bound as below:

\[_{}(Y\,|\,X)&= _{i=1}^{N}(n_{x}(i)+1)+d_{X}((i)- _{x}(i))+d_{Y}(i)+C\\ &_{i=1}^{N}(n_{x}(i)+1)+d_{Y} (i)-(k)+ c_{d_{Y}}\] (5)

where \(C\) denotes \(-(k)+ c_{d_{Y}}\) and lower bounds holds because \((i)_{x}(i)\) always holds.

## 4 Method

We present a new exploration technique that maximizes the _value-conditional_ state entropy (VCSE), which addresses the issue of state entropy exploration in a supervised setup by taking into account the value estimates of visited states for computing the intrinsic bonus. Our main idea is to prevent the distribution of high-value states from affecting exploration around low-value states, and vice versa, by filtering out states whose value estimates significantly differ from each other for computing the intrinsic bonus. In this section, we first describe how to define the value-conditional state entropy and use it as an intrinsic reward (see Section 4.1). We then describe how we train RL agents with the intrinsic reward (see Section 4.2). We provide the pseudocode of our method in Algorithm 1.

### Maximum Value-Conditional State Entropy

Value-conditional state entropyLet \(\) be a stochastic policy, \(f_{}^{}\) be an extrinsic critic function, and \(V_{}^{}\) be a random variable with a probability density \(p_{V_{}^{}}(v)=_{s}_{f_{}^{}(s)=v }/||\). Our key idea is to maximize the _value-conditional_ state entropy \((S\,|\,V_{}^{})=E_{v p_{V_{}^{}}(v)}[ (S\,|\,V_{}^{}=v)]\), whichcorresponds to separately estimating the state entropies that are conditioned on the value estimates of each state and then maximizing their average. This intuitively can be seen as partitioning the visited state space with value estimates and averaging the state entropy of each partitioned space.

Estimation and intrinsic rewardTo estimate \((S\,|\,V_{}^{})\), we employ the KSG conditional entropy estimator in Equation 5. Specifically, we estimate the value-conditional state entropy as below:

\[_{}(S\,|\,V)=_{i=1}^{N}(n_{v}(i )+1)+d_{V}((i)-_{v}(i))+d_{S}(i)+C\] (6)

where \(C\) denotes \(-(k)+ c_{d_{S}}\). In practice, we maximize the lower bound of value-conditional state entropy because of its simplicity and ease of implementation as below:

\[_{}(S\,|\,V)_{i=1}^{N}(n_{v }(i)+1)+d_{S}(i)-(k)+ c_{d_{S}}\] (7)

We then define the intrinsic reward \(r_{t}^{}\) similarly to Equation 2 by ignoring constant terms as below:

\[r_{t}^{}=}(n_{v}(i)+1)+(i)(i)=2(||s_{i}-s_{i}^{}||,||v_{i}-v_{i}^{}||)\] (8)

To provide an intuitive explanation of how our reward encourages exploration, we note that a \((s_{j},v_{j})\) pair whose value \(v_{j}\) largely differs from \(v_{i}\) is not likely to be chosen as \(k\)-th nearest neighbor because maximum-norm will be large due to large value norm \(||v_{i}-v_{j}||\). This means that \(k\)-th nearest neighbor will be selected among the states with similar values, which corresponds to partitioning the state space with values. Then maximizing \((i)\) can be seen as maximizing the state entropy within each partitioned space. We provide an illustration of our method in Figure 1.

### Training

ObjectiveWe train RL agents to solve the target tasks with the reward \(r_{t}^{}=r_{t}^{}+ r_{t}^{}\) where \(>0\) is a scale hyperparameter that adjusts the balance between exploration and exploitation. We do not utilize a decay schedule for \(\) unlike prior work (Seo et al., 2021) because our method is designed to avoid redundant exploration. We provide the pseudocode of our method in Algorithm 1.

Implementation detailTo compute the intrinsic reward based on value estimates of expected return from environments, we train an extrinsic critic \(f_{v}^{}\) to regress the expected return computed only with the extrinsic reward \(r_{t}^{}\). Then we use the value estimates from \(f_{v}^{}\) to compute the intrinsic reward in Equation 8. To train the agent to maximize \(r_{t}^{}\), we train a critic function \(f_{v,}^{}\) that regressesthe expected return computed with \(r_{t}^{}\), which the policy \(\) aims to maximize. To ensure that the distribution of value estimates be consistent throughout training, we normalize value estimates with their mean and standard deviation computed with samples within a mini-batch. When an environment is partially observable as a high-dimensional observation \(o_{t}\) is only available instead of a fully observable state, we follow a common practice (Mnih et al., 2015) that reformulates the problem as MDP by stacking observations \(\{o_{t},o_{t-1},o_{t-2},...\}\) to construct a state \(s_{t}\). We also note that we use a replay buffer \(\) for entropy estimation following Liu and Abbeel (2021b); Seo et al. (2021) in that it explicitly encourages the policy to visit unseen, high-reward states which are not in the buffer.

## 5 Experiments

We design our experiments to evaluate the generality of our maximum value-conditional state entropy (VCSE) exploration as a technique for improving the sample-efficiency of various RL algorithms (Mnih et al., 2016; Yarats et al., 2021a). We conduct extensive experiments on a range of challenging and high-dimensional domains, including partially-observable navigation tasks from MiniGrid (Chevalier-Boisvert et al., 2018), pixel-based locomotion tasks from DeepMind Control Suite (DMC; Tassa et al. (2020)), and pixel-based manipulation tasks from Meta-World (Yu et al., 2020).

### MiniGrid Experiments

SetupWe evaluate our method on navigation tasks from MiniGrid benchmark (Chevalier-Boisvert et al., 2018) consisting of sparse reward goal-reaching tasks. This environment is partially observable as the agent has access to a \(7 7 3\) encoding of the \(7 7\) grid in front of it instead of the full grid.4 As a baseline, we first consider RE3 (Seo et al., 2021) which trains Advantage Actor-Critic (A2C; Mnih et al. 2016) agent with state entropy (SE) exploration where the intrinsic reward is obtained using the representations from a random encoder. We build our method upon the official implementation of RE3 by modifying the SE reward with our VCSE reward. Unlike RE3, we do not normalize our intrinsic reward with standard deviation and also do not utilize a separate buffer for computing the intrinsic reward using the on-policy batch. We use \(k=5\) for both SE and VCSE by following the original implementation. See Appendix A for more details.

Figure 3: Examples of tasks from MiniGrid, DeepMind Control Suite, and Meta-World.

Figure 2: Learning curves on six navigation tasks from MiniGrid (Chevalier-Boisvert et al., 2018) as measured on the success rate. The solid line and shaded regions represent the interquartile mean and standard deviation, respectively, across 16 runs.

ResultsFigure 2 shows that A2C+VCSE consistently outperforms A2C+SE on diverse types of tasks, including simple navigation without obstacles (Empty-16x16), navigation with obstacles (LavaGapS7 and SimpleCrossingS9N1), and long-horizon navigation (DoorKey-6x6, DoorKey-8x8, and Unlock). For instance, on LavaGapS7, A2C+VCSE achieves an average success rate of 88.8%, while A2C+SE achieves 13.9% after 100K steps. This shows that VCSE encourages the agent to effectively explore high-value states beyond a crossing point. On the other hand, SE excessively encourages the agent to visit states located before the crossing point for increasing the state entropy.

Comparison to SE with varying \(\)One might think that adjusting the scale of intrinsic rewards could address the issue of state entropy exploration in the supervised setup, by further encouraging exploration in high-value states or discouraging exploration in low-value states. However, as shown in Figure 4, simply adjusting the scale cannot address the issue. Specifically, A2C+SE fails to solve the task with a large \(=0.05\), because large intrinsic rewards could make the agent ignore the task reward and encourages redundant exploration. On the other hand, small \(=0.0005\) also degrades sample-efficiency when compared to the performance with \(=0.005\) as it discourages exploration. In contrast, A2C+VCSE learns to solve the task in a sample-efficient manner even with different \(\) values (see Figure 16 for supporting experiments).

### DeepMind Control Suite Experiments

SetupWe consider a widely-used DeepMind Control Suite (DMC; Tassa et al.2020) benchmark mainly consisting of locomotion tasks. For evaluation on a more challenging setup, we conduct experiments on pixel-based DMC environments by using a state-of-the-art model-free RL method DrQv2 (Yarats et al., 2021) as our underlying RL algorithm.5 For computing the intrinsic bonus, we follow Laskin et al. (2021) by using the features from an intrinsic curiosity module (ICM; Pathak et al.2017) trained upon the frozen DrQv2 encoder. For SE, we find that normalizing the intrinsic reward with its running mean improves performance, which is also done in Laskin et al. (2021). We do not normalize the intrinsic reward for our VCSE exploration. We also disable the noise scheduling

Figure 4: Learning curves on SimpleCrossingS9N1 as measured on the success rate. The solid line and shaded regions represent the interquartile mean and standard deviation, respectively, across eight runs.

Figure 5: Learning curves on six control tasks from DeepMind Control Suite (Tassa et al., 2020) as measured on the episode return. The solid line and shaded regions represent the interquartile mean and standard deviation, respectively, across 16 runs.

in DrQv2 for SE and VCSE as we find it conflicts with introducing the intrinsic reward. We instead use the fixed noise of \(0.2\). We use \(k=12\) for both SE and VCSE. See Appendix A for more details.

ResultsFigure 5 shows that VCSE exploration consistently improves the sample-efficiency of DrQv2 on both sparse reward and dense reward tasks, outperforming all other baselines. In particular, our method successfully accelerates training on dense reward Walker Walk task, while SE significantly degrades the performance. In Appendix B, we further show that the performance of DrQv2+SE improves with smaller \(\), but it still struggles to outperform DrQv2 on Walker Walk. This implies that SE might degrade the sample-efficiency of RL algorithms on dense reward tasks by encouraging the agent to explore states that might not be helpful for solving the task. Moreover, We show that introducing a decaying \(\) schedule for SE struggles to improve performance in Appendix B.

### Meta-World Experiments

SetupWe further evaluate our method on visual manipulation tasks from Meta-World benchmark (Yu et al., 2020) that pose challenges for exploration techniques due to its large state space with small objects. For instance, moving a robot arm toward any direction enables the agent to visit novel states, but it would not help solve the target task. As an underlying RL algorithm, we use DrQv2 (Yarats et al., 2021). We follow the setup of Seo et al. (2022) for our experiments by using the same camera configuration and normalizing the extrinsic reward with its running mean to make its scale be 1 throughout training. For computing the intrinsic reward, we use the same scheme as in DMC experiments (see Section 5.2) by using ICM features for computing the intrinsic reward and only normalizing the SE intrinsic reward. We disable the noise scheduling for all methods and use \(k=12\) for SE and VCSE. See Appendix A for more details.

ResultsFigure 6 shows that VCSE consistently improves the sample-efficiency of DrQv2 on visual manipulation tasks, while SE struggles due to the large state spaces. Notably, DrQv2+VCSE allows for the agent to solve the tasks where DrQv2 struggles to achieve meaningful success rates, _e.g._, DrQv2+VCSE achieves 85% success rate on Door Open after 100K environment steps while DrQv2 achieves zero success rate. On the other hand, SE struggles to improve the sample-efficiency of DrQv2, even being harmful in several tasks. This shows the effectiveness of VCSE for accelerating training in manipulation tasks where state space is very large. We also report the performance of SE and VCSE applied to model-based algorithm (Seo et al., 2022) on Meta-World in Appendix B, where the trend is similar in that SE often degrades the performance due to the large state space.

Figure 6: Learning curves on six visual manipulation tasks from Meta-World (Yu et al., 2020) as measured on the success rate. The solid line and shaded regions represent the interquartile mean and standard deviation, respectively, across 16 runs.

### Ablation Studies and Analysis

Effect of value approximationSince we use neural networks for encoding states and approximating the value functions, our objective \((S|V_{}^{})\) could change throughout training. To investigate the effect of using the approximated value function for our objective, we provide experimental result where we (i) use ground-truth states for computing the intrinsic bonus and (ii) computes the value estimates via policy evaluation (Sutton and Barto, 2018) using the MiniGrid simulator. Figure 6(a) shows that VCSE based on policy evaluation using the privileged knowledge of ground-truth simulator performs the best, but VCSE based on value function approximation with neural networks can match the performance and significantly outperforms SE. This supports that our method works as we intended with the approximated value functions. We expect that our method can be further improved by leveraging advanced techniques for learning value functions (Bellemare et al., 2017; Espeholt et al., 2018; Hafner et al., 2023), reaching the performance based on the privileged knowledge.

Effect of value conditioningWe compare VCSE with a baseline that maximizes reward-conditional state entropy (RCSE) in Figure 6(b) to investigate our design choice of conditioning the state entropy on values instead of one-step rewards. The result shows that VCSE largely outperforms RCSE because the value captures more long-term information about the state when compared to the reward. We note that RCSE cannot be applied to navigation tasks in MiniGrid, where the reward is only given to goal-reaching states, which also demonstrates the benefit of VCSE in terms of applicability.

Effect of batch sizeWe investigate how estimating the value-conditional state entropy using samples from minibatches by reporting the results with increased batch sizes. Specifically, we use the batch size of 1024 for computing the intrinsic bonus but use the same batch size of 256 or 512 for training the actor and critic of RL agent. As shown in Figure 6(c), we find that results are stable with both batch sizes, which shows that our value-conditional state entropy estimation can be stable without using extremely large batch sizes or all the samples within the replay buffer.

Heatmap visualizationTo help understand how VCSE improves sample-efficiency, we provide the visualization of visited state distributions obtained during the training of A2C agents with SE and VCSE. For this analysis, we modified the original SimpleCrossingS9N1 task to have a fixed map configuration (see Appendix A for more details). As shown in Figure 7(a), we find that the agent trained with SE keeps exploring the region before a narrow crossing point instead of reaching the goal until 100K steps, even if it has experienced several successful episodes at the initial phase of training. This is because the crossing point initially gets to have a high value and thus exploration is biased towards the wide low-value state region, making it difficult to solve the task by reaching the green goal. On the other hand, Figure 7(b) shows that VCSE enables the agent to successfully explore high-value states beyond the crossing point.

Figure 7: (a) Learning curves on SimpleCrossingS9N1 that compares VCSE using neural networks for value function approximation against VCSE that employs policy evaluation (Sutton and Barto, 2018) using the privileged simulator information. Learning curves aggregated on two control tasks from DeepMind Control Suite that investigate the effect of (b) value conditioning and (c) batch size. We provide results on individual task in Appendix B. The solid line and shaded regions represent the interquartile mean and standard deviation, respectively, across 16 runs.

## 6 Discussion

Limitation and future directionsOne limitation of our work is that we lack a theoretical understanding of how value-conditional state entropy exploration works. Considering the strong empirical performance evidenced in our experiments, we hope our work facilitates future research on a theoretical investigation. Our work only considers a non-parametric state entropy estimator due to its benefit that does not learn a separate estimator, but considering learning-based estimator can be also an interesting direction. Moreover, applying our idea to methods that maximizes the state entropy of policy-induced state distributions (Lee et al., 2019; Mutti and Restelli, 2020) can be also an interesting future work. Our work might be limited in that it works exactly the same as the state entropy exploration until the agent encounters a supervised reward from an environment. Developing a method that utilizes an intrinsic signal to partition the state space into meaningful subspaces without the need for task reward is a future direction we are keen to explore in future work.

ConclusionWe present a new exploration method that maximizes _value-conditional_ state entropy which addresses the imbalanced distribution problem of state entropy exploration in a supervised setup by taking into account the value estimates of visited states for computing the intrinsic bonus. Our extensive experiments show that our method can consistently improve the sample-efficiency of RL algorithms in a variety of tasks from MiniGrid, DeepMind Control Suite, and Meta-World benchmarks. We hope our simple, intuitive, and stable exploration technique can serve as a standard tool for encouraging exploration in deep reinforcement learning.