# Direct Language Model Alignment from

Online AI Feedback

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model. However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy. In this study, we posit that online feedback is key and improves DAP methods. Our method, online AI feedback (OAF), uses an LLM as annotator: on each training iteration, we sample two responses from the current model and prompt the LLM annotator to choose which one is preferred, thus providing online feedback. Despite its simplicity, we demonstrate via human evaluation in several tasks that OAF outperforms both offline DAP and RLHF methods. We further show that the feedback leveraged in OAF is easily controllable, via instruction prompts to the LLM annotator.

## 1 Introduction

To maximise the benefits of large language models (LLMs) to society, it is important to align them with human expectations and values (Ouyang et al., 2022; Bai et al., 2022; Bubeck et al., 2023). The first method introduced for alignment was reinforcement learning from human feedback (RLHF, Christiano et al., 2017; Stiennon et al., 2020), which trains a reward model (RM) from pairwise preferences and then optimises a policy against the RM via reinforcement learning (RL). More recently, direct alignment from preferences (DAP) methods have emerged as popular alternatives to RLHF, such as direct preference optimisation (DPO, Rafailov et al., 2023), sequence likelihood calibration with human feedback (SLiC, Zhao et al., 2023), and identity policy optimisation (IPO, Azar et al., 2023). In contrast to RLHF, the DAP methods directly update the language model (a.k.a. policy) \(_{}\) using pairwise preference data, making the alignment simpler, more efficient and more stable (Rafailov et al., 2023).

However, the preference datasets used in DAP methods are often collected ahead of training and the responses in the dataset are usually generated by different LLMs. Thus, the feedback in DAP methods is usually purely offline, as \(_{}\) cannot get feedback on its own generations over training. This is problematic because of the significant distribution shift between the policy that generated the dataset and the policy being aligned: we train on the distribution induced by \(\) but evaluate on the distribution induced by \(_{}\) in the end. In contrast, in RLHF, the RM provides online feedback to generations from \(_{}\) during the RL step. This practice leads to on-policy learning, which was shown to improve exploration and overall performance (Lambert et al., 2022).

Inspired by RL from AI feedback (RLAIF) (Bai et al., 2022b; Lee et al., 2023), we hereby propose Online AI Feedback (OAIF) for DAP methods. Our method inherits both the practical advantages of DAP methods and the online nature of RLHF. Specifically, when aligning an LLM policy \(_{}\), we follow a three-step procedure: 1) we sample two responses to a prompt from the current policy \(_{}\); 2) we obtain online feedback over the two responses by prompting an LLM to mimic human preference annotation; 3) we use this online feedback to update the model \(_{}\) through standard DAP losses. Our approach is depicted in Fig 1. Unlike methods proposed by Xu et al. (2023); Liu et al. (2023); Xiong et al. (2023), OAIF skips the RM training, and directly extracts the preference from an LLM.

To show the effectiveness of our proposal, we perform an extensive empirical comparison between OAIF, existing offline DAP methods and RLHF methods. Our experimental protocol uses both AI and human evaluation on standard LLM alignment tasks: TL; DR (Ziegler et al., 2019), Anthropic Helpfulness and Harmlessness (Bai et al., 2022a). To summarise, we make the following contributions.

* We demonstrate the effectiveness and generality of OAIF for turning offline DAP methods (DPO, IPO, SLiC) into online methods. Our human evaluation shows that the average win rate of online DAP methods (DPO, IPO, SLiC) over offline versions of the same methods is \(\)\(66\%\).
* We confirm the usefulness of making DAP methods online: human raters favour DPO with OAIF (thus, online DPO) over SFT baseline, RLHF and RLAIF \(58.00\%\) of time on the TL;DR task in 4-way comparisons.
* We demonstrate the controllability of the LLM annotator, by injecting specific instructions into the prompts. We use response length as a test-bed. By asking the LLM annotator to prefer shorter responses, the average length of responses from the aligned policy is significantly shortened from \(\)\(120\) to \(\)\(40\), while its quality is still improved over the SFT baseline.

## 2 Background

**Pairwise preference collection**. Current methods for LLM alignment first collect a dataset of pairwise preferences, as follows. A prompt \(\) is sampled from a prompt distribution \(p_{}\), then two distinct responses \(^{1}\) and \(^{2}\) are sampled independently from an existing LLM \(\). Then, human (Christiano et al., 2017) or AI annotators (Lee et al., 2023) rank the responses, yielding a preferred response \(^{+}\) and a less preferred one \(^{-}\). With some abuse of notation, we assume that there exists a function that uniquely maps \((^{1},^{2})\) to \((^{+},^{-})\), and we will therefore write \((^{+},^{-})(|)\). A preference dataset \(=\{(_{i},^{+}_{i},^{-}_{i})\}_{i=1}^{N}\) is then constructed by repeating the above process \(N\) times.

**Direct alignment from preference (DAP) methods.** DAP methods directly update the target policy \(_{}\) from the preference pairs \((^{+},^{-})\). The loss functions for the three main DAP methods

Figure 1: **Summary of the proposed online AI feedback (OAIF) approach for making direct alignment from preferences (DAP) methods online and on-policy**. Given an input prompt \(\), two responses \(^{1}\) and \(^{2}\) are first sampled from the current language model \(_{^{+}}\), then labelled as \(^{+}\) and \(^{-}\) by the LLM annotator. The language model parameters are then updated using the objective function of DAP methods.

investigated in this work are summarised below. They take the form \((,^{+},^{-},)\) for a prompt \( p_{}\), a response pair \((^{+},^{-})(|)\) and model parameters \(\).

* DPO loss: \[-(}(^{+}|)_{^{0}}(^{-}|)}{_{^{0}}(^{+}|)_{ }(^{-}|)})\] (1)
* IPO loss: \[((}(^{+}|)_{^{0 }}(^{-}|)}{_{}(^{-}|)_{^{0 }}(^{+}|)})-)^{2}\] (2)
* SLiC loss: \[(0,1-(}(^{+}|)_{ ^{0}}(^{-}|)}{_{}(^{-}|)_ {^{0}}(^{+}|)}))\] (3)

where \(_{^{0}}\) is the SFT baseline used as reference, \(\) is the logistic function, and \(\) is a scalar hyperparameter. We emphasise once again that \((^{+},^{-})\) are sampled from \((|)\), not from \(_{^{i}}(|)\), as this will be the key difference with the online variant we propose in the next section. One advantage of these loss functions is that their gradients \(_{}(,^{+},^{-},)\) can be computed exactly in an efficient way. In contrast, because the loss function used in RLHF involves an expectation over the space of responses (Ziegler et al., 2019), policy gradient methods are typically used to obtain an unbiased estimate of the gradient and a value function is typically used to reduce the variance, which requires storing an additional model in memory.

**Offline feedback**. In most real-world applications, due to the financial cost and complexity of collecting pairwise preferences from human annotators, the preference dataset \(\) is usually collected ahead of aligning a language model \(_{}\) and kept fixed throughout training. Obtaining online preferences on new responses is usually not feasible, as there is no human-in-the-loop. Using a fixed dataset \(\) makes all preference data _offline_, which means the policy1\(_{}\) cannot get feedback on its own generations on-the-fly over the alignment procedure. It is worth mentioning that the RL step in RLHF and RLAIF is _online_ as the training data is acquired interactively. See Appendix A.1 for an in-depth discussion on online vs. offline feedback.

**Off-policy learning**. Beyond the offline feedback problem illustrated above, aligning an LLM policy \(_{}\) with DAP methods on a pre-collected dataset \(\) also yields a distribution shift between the generation from the policy \(\) and the policy \(_{^{i}}\) at each time step \(t\). This keeps evolving over learning. This shift problem is illustrated in Figure 2. We also provide an empirical verification of this problem in Appendix B. In DPO, this problem is tackled by supervised finetuning \(_{}\) on \(\) so that \(_{^{0}}\), but the off-policy issue remains during alignment as \(_{^{0}}\) gradually departs from \(_{^{0}}\). Thanks to the _online_ nature of RL RL methods are also _on-policy_, as the responses used to update \(_{^{i}}\) are all sampled from it. See Appendix A.2 for more details on on-policy vs. off-policy learning in LLMs.

**RM-based online feedback for DAP methods**. To avoid the distribution shifts arising when aligning LLMs with offline DAP methods on a given dataset \(\), an intuitive and straightforward solution is to introduce an RM to provide online feedback. Liu et al. (2023) proposed RSO, a method that uses an RM to perform rejection sampling in order to sample from the optimal policy, which improved the alignment compared to offline DAP baselines. Besides, pseudo-labelling the generations from \(_{^{i}}\) by RMs can also be helpful, as done in the Iterative DPO method (Xu et al., 2023) and the West-of-N

Figure 2: **Illustration of the distribution shift problem.** The responses \((_{1},_{2})\) sampled from the current model \(_{^{i}}\) differ from preference dataset responses \((^{+},^{-})\) sampled from \(\), as \(_{^{i}}\). Two independent distribution shifts can occur: an initial distribution shift (\(_{^{0}}\)) and a gradual distribution shift (\(_{^{0}}_{^{i}}\)) during the alignment procedure.

method (Pace et al., 2024). Although the aforementioned RM-based methods make the alignment of a policy online and on-policy, the distribution shift problem still exists when training the RM. More specifically, the RM is trained on the preference dataset \(\), but used to annotate preference over responses from \(_{^{t}}\) at training step \(t\), where \(_{}\). Therefore, RM-based online feedback cannot fully avoid distribution shift issues.

**LLM-based online feedback for DAP methods**. The method we propose next, "Online AI Feedback" (OAIF), consists in using an LLM as an online annotator. Our method relies on the observation that LLMs can approximate well human labelling and can generate reliable preferences over responses (Lee et al., 2023). In recent concurrent work, Yuan et al. (2024) proposed a "self-rewarding" approach, in which the policy being aligned provides online feedback to itself. In comparison, OAIF can leverage feedback from any LLM, including ones stronger than the LLM being aligned. Swamy et al. (2024) also concurrently investigates the importance of online preference, but still relying on RMs.

In Table 1, we summarise the characteristics of OAIF and of the existing offline and online DAP methods.

## 3 Direct alignment from online AI feedback

**Bridging the gap**. As we saw, DAP methods are simple, do not require a separate RM, but they use preference data pre-collected offline. On the other hand, RLHF methods interact online with the language model being aligned, but they require policy gradient techniques to obtain an unbiased gradient estimate and a value function to reduce the variance. To bridge the gap between these two families of methods, we propose a simple yet effective way to make DAP methods online.

As pointed out by Ziegler et al. (2019), online data collection is crucial for aligning language models. To solve the aforementioned offline problem in DAP methods, we propose to collect preferences on-the-fly for responses generated by the language model being aligned. Naturally, using human feedback would be prohibitively expensive. Prior studies have shown that AI feedback is a reliable and effective approximation to human labellers, especially for pairwise preference labelling (Lee et al., 2023). We therefore propose to use an LLM as online annotator, in order to collect the preference over pairs of responses, sampled from \(_{^{t}}\) on-the-fly during its alignment. We refer to the proposed approach as **OAIF**, which stands for online AI feedback.

**Proposed algorithm**. An overview of OAIF is given in Figure 1, and a more formal description is provided in Algorithm 1 (for simplicity, we use batches of size \(1\)). Given a prompt \(\), sampling \(^{1},^{2}\) from \(_{^{t}}(|)\) ensures _on-policy_ learning. Prompting the annotating LLM to obtain

   Method & No RM & On-policy & Online \\  & needed & generation & feedback \\  Offline DPO & & & \\ Rafailov et al. (2023) & ✓ & ✗ & ✗ \\  Offline IPO & & & \\ Azar et al. (2023) & ✓ & ✗ & ✗ \\  Offline SLiC & & & \\ Zhao et al. (2023) & ✓ & ✗ & ✗ \\  RSO & & & \\ Liu et al. (2023) & ✗ & ✓ & ✓ \\  Iterative DPO & & & \\ Xu et al. (2023) & ✗ & ✓ & ✓ \\  OAIF (proposed) & ✓ & ✓ & ✓ \\   

Table 1: **Comparison between OAIF (proposed) and existing DAP methods**, with or without a separate RM. Technically, training RMs on pre-collected preference data still suffers from the distribution shift problem, as RMs cannot get feedback for responses from the model \(_{^{t}}\).

ensures _online_ learning. We emphasise that the approach is general and works with any differentiable DAP loss function \((,^{+},^{-},)\).

**Gradient computation**. An important technical detail of online DAP methods is that \(\) is involved in both the response sampling and the DAP loss function. In contrast, \(\) is involved only in the loss for offline DAP methods and only in the sampling for RLHF methods. In addition, using OAIF, the sampled responses go through an LLM annotator to obtain \((^{+},^{-})\), thus \((^{+},^{-})\) are also in principle functions of \(\). In practice, we propose to simply use \(_{}(,^{+},^{-},)\) as our gradients, which amounts to placing a stop_gradient on both the sampling and LLM annotation steps.

**Annotating prompts with text-controllability**. We adopt a pairwise prompting scheme to collect AI feedback, i.e. we instruct the LLM annotator to choose which response is preferred among a pair, as in (Lee et al., 2023). To avoid position bias, we calculate scores for the two response possible orders and use the average as the final score. Since OAIF leverages prompting techniques to collect feedback, the reward signals or the preference function can be easily adapted by modifying the prompts (Sun et al., 2024). This offers high flexibility without incurring any extra computation (such as retraining the RM) compared to RLHF and RLAIF. For example, in our experiments, we show that we can control the response length by simply prompting the annotator to prefer shorter responses.

## 4 Experiments

### Experimental setup

We use three tasks for experiments: TL; DR(Stiennon et al., 2020), Anthropic Helpfulness and Anthropic Harmlessness(Bai et al., 2022). For each task, we prepare the prompt dataset \(_{}\) by simply extracting the input prompts from the preference dataset \(\). We adopt PaLM 2 (Anil et al., 2023) as the language model and also the LLM annotator. Unless otherwise specified, all policy models are initialised from the model obtained by supervised finetuning (SFT) PaLM 2-XS (Extra Small), which is referred to as the SFT baseline. For the annotating model, we use PaLM 2-L (Large). To obtain online feedback from the annotating model, we adopt the _Detailed 0-shot_ prompt from Lee et al. (2023). The prompts we used and how we get preference scores from them are detailed in Appendix E.

To demonstrate the generality of OAIF, we experiment with three DAP methods: DPO, IPO and SLiC. Based on preliminary experiments, we set \(=0.1\) in DPO, \(=1.0\) in IPO, and \(=0.002\) in SLiC. We sample responses with a temperature of 0.9 during training. We adopt Adafactor (Shazeer & Stern, 2018) as the optimiser, and set the batch size to 128 and the learning rate to \(5 10^{-7}\), with a warm-up period of \(150\) steps for all experiments. We used 64/128 TPU-v3 chips to train PaLM-XS/S, which takes about 3.5/5 days for each experiment. We evaluate models by computing win rates, i.e. how often one model's response is better than the other. For automatic evaluation, we apply the same prompting technique as above but with Gemini Pro (Gemini Team et al., 2023) to reduce the risk of over-fitting and reward hacking (Gao et al., 2023). The validity of Gemini Pro as the judge generated from a corresponding policy model, on a scale from 1 to 5 and select the best response. Please see Appendix F for more details about the human evaluation study.

### How effective is OAIF for LLM alignment?

We start by examining the effectiveness of OAIF for DAP methods (that use online AI feedback), compared to their offline counterparts (that use pre-collected offline human preferences). As a sanity check, we track the win rate of DPO with OAIF ("Online DPO") and vanilla DPO ("Offline DPO")

Figure 3: Win rate of DPO with OAIF (online DPO), vanilla DPO (offline DPO), RLAIF, and RLHF against the SFT baseline on the TL; DR task, judged by _Gemini Pro_.

against the SFT baseline on TL;DR. The results are given in Figure 3, where the results for RLAIF and RLHF are provided as references.

Not surprisingly, both online and offline DPO improve the performance of the model, as shown by the substantially high win rate achieved against the SFT baseline. However, as indicated by the sharp drop of the red curve around training step \(3,500\), offline DPO rapidly _overfits_ the offline and off-policy preferences in \(\). In contrast, the win rate of online DPO keeps increasing over training, and _surpasses_ offline DPO after \(4,000\) steps. This demonstrates the effectiveness of OAIF. To consolidate the findings we got with Gemini Pro as automatic evaluator, the same experiment was also carried out with PaLM 2-L as the automatic evaluator. The results, given in Appendix D, confirm that our observations hold under both automatic evaluators.

Next, we evaluate OAIF on different tasks, i.e., TL;DR, Helpfulness and Harmlessness. We select the best performing online and offline DPO models according to both manual inspection and their development set win rate against the SFT baseline by Gemini Pro. We then report side-by-side human evaluations comparing online DPO and offline DPO in Table 2.

Human evaluation shows that OAIF significantly improves the performance of DPO across all tasks with substantial superiority over offline DPO. This consolidates our conclusion that using the offline feedback and off-policy generations in a pre-collected preference dataset \(\) can be detrimental for LLM alignment, and OAIF benefits greatly from online and on-policy AI feedback.

### How does OAIF generalise to other DAP methods?

As shown in Algorithm 1, OAIF is compatible with arbitrary DAP loss functions. We therefore check the effectiveness of OAIF for IPO and SLiC. The side-by-side human evaluation results on TL;DR comparing the online and offline counterparts of these methods are given in Table 3.

Compared to their offline counterparts, DAP methods with OAIF achieve promising win rates, ranging from \(\)\(64\%\) to \(\)\(71\%\). The consistent ineffectiveness of offline DAP methods confirms that the existence of the offline and off-policy issue in DAP methods and greatly hinders the performance of aligning LLMs. The consistent superiority of online DAP methods via OAIF against their offline counterparts demonstrates that OAIF is a general framework effectively addressing these challenges.

### How do DAP methods using OAIF perform compared to RLHF/RLAIF?

Understanding the merits of DPO and RLHF is still a relatively open research question. We argue that comparing online DPO with RLAIF and RLHF, which is interesting on its own sake, can also contribute to answering this question.

We adopt similar experimental setups for RLAIF and RLHF as before, to make the comparison as fair as possible: we employ PaLM 2-L as the AI feedback model for RLAIF and use the same pre-collected preference dataset to train RMs for RLHF. Our training and optimisation procedures

   Method & Win & Tie & Loss & Quality \\   \\  Online DPO & **63.74\%** & \(7.69\%\) & \(28.57\%\) & \(7.69\%\) & **3.95** \\ Offline DPO & \(7.69\%\) & \(28.57\%\) & \(63.74\%\) & \(3.46\) \\   \\  Online DPO & **58.60\%** & \(20.20\%\) & \(21.20\%\) & \(20.20\%\) & **4.08** \\ Offline DPO & \(20.20\%\) & \(58.60\%\) & \(3.44\) \\   \\  Online DPO & **60.26\%** & \(3.84\%\) & \(35.90\%\) & \(3.84\%\) & **4.41** \\ Offline DPO & \(3.84\%\) & \(60.26\%\) & \(3.57\) \\   

Table 2: Win/tie/loss rate of DPO with OAIF (online DPO) against vanilla DPO (offline DPO) on the TL;DR, Helpfulness, Harmlessness tasks, along with the quality score of their generations, judged by _human raters_.

   Method & Win & Tie & Loss & Quality \\  Online DPO & **63.74\%** & \(28.57\%\) & \(7.69\%\) & **3.95** \\ Offline DPO & \(7.69\%\) & \(63.74\%\) & \(3.46\) \\  Online IPO & **64.81\%** & \(31.48\%\) & \(3.71\%\) & **3.84** \\ Offline IPO & \(3.71\%\) & \(64.81\%\) & \(2.93\) \\  Online SLiC & **71.43\%** & \(26.98\%\) & \(1.59\%\) & **3.85** \\ Offline SLiC & \(1.59\%\) & \(71.43\%\) & \(3.23\) \\   

Table 3: Win/tie/loss rate of DAP methods with OAIF (online DPO/IPO/SLiC) against their offline counterparts in TL;DR along with the quality score of their generations, judged by _human raters_.

follow Lee et al. (2023). Figure 3(a) shows the human evaluation results, where online DPO is more preferred than the other methods, in \(58\%\) of the time.

We emphasise that the RM used in RLAIF and RLHF is often not updated during policy training. As a result, its response assessment ability may not generalise, as the output distribution from \(_{^{t}}\) evolves. To verify this hypothesis, we also trained an online DPO with the same RM used for RLAIF. It outperforms RLAIF, but significantly underperforms online DPO with OAIF, with a win rate of \(<\)\(30\%\) judged by Gemini Pro. This experimental result supports the superiority of using LLMs over RMs to provide online feedback. Synchronously retraining the RM is feasible theoretically (Ziegler et al., 2019), but this would greatly complicate the training pipeline and increase training cost.

Despite the great performance of OAIF compared to various baselines, we found that OAIF tends to produce significantly longer responses. This may affect the LLM and human evaluation as both evaluators often prefer long generations, referred to as "length bias" by Singhal et al. (2023). To avoid the effect of such bias on analysing the performance of OAIF, we group the responses by their length, and plot the average quality score of each group. The results in Figure 3(b) show that online DPO with OAIF provides responses of higher quality than the other methods at fixed length, which further validates the effectiveness of OAIF.

### How does the size of the LLM annotator affect performance?

Another important dimension arising during our experiment is the size of the annotating LLMs. Previous experiments are all based on PaLM 2 L for feedback collection. To examine the feasibility of feedback from smaller LLM annotators, we then replicate online DPO experiments on TL;DR but with feedback from PaLM 2-XS and PaLM 2-S instead. Figure 5 shows the comparison to SFT baseline, offline DPO, RLAIF, and RLHF models we used, as in the previous experiments.

The size of the LLM annotator clearly has a significant impact on OAIF. Generally, as size increases, online DPO obtains better performance. Compared to the initial SFT model, online DPO with OAIF performs significantly better regardless of AI labeller model sizes, suggesting that even OAIF from a small LLM annotator is helpful in improving the performance of alignment. In particular, OAIF

Figure 4: **Left**: Fraction of outputs from online DPO, offline DPO, RLAIF, and RLHF being preferred in a 4-way comparison; **Right**: average quality scores (y-axis, higher is better) assigned to responses of different lengths (x-axis). The responses of each model were first grouped into six buckets by their length. The mean and standard error of responses in a bucket are then plotted as a data point. All results are judged by _human raters_ on TL;DR.

Figure 5: Win rate of online DPO against the SFT baseline, offline DPO, RLAIF, and RLHF, with annotating LLMs of varying sizes (XS, S, L) in the task TL;DR, as assessed by _Gemini Pro_.

with PaLM 2-XS (i.e. an LLM annotator of same-size) achieves comparable performance to RLHF, although the latter learns from human feedback. Further human evaluation confirms this observation: OAIF with PaLM 2-XS obtains an overall quality score of 3.41 out of 5, slightly better than RLHF (3.38) and comparable to offline DPO (3.46).

### How prompt-controllable is OAIF?

While the necessity of LLM alignment has been widely recognised, what to align them with is still under debate, as human expectations vary greatly across regions and cultures, and may evolve over time. This indicates that the human preference annotation might change dramatically and frequently. In RLHF, such changes require re-annotating the preference dataset and re-training the RM, leading to high cost. In contrast, as OAIF is obtained through prompting the LLM annotator, its reward signal could be adjusted by simply modifying the prompts.

To examine this, we choose to explore the controllability of the length of responses by modifying the prompts to the LLM annotators. We take the online DPO model \(_{}\) trained to be as _helpful_ as possible in Section 4.2 as the reference. We further train another two online DPO models with the same experiment setup, but in which the annotator is prompted to favour "_helpful and short_" and "_helpful and very short_" responses. The exact prompts given to the LLM annotators are provided in Table 6 and Table 8.

We display the average length of responses over training in Figure 5(a). The "short" and "very short" prompts given to the LLM annotator significantly shorten the responses from \(\)\(120\) tokens to \(\)\(90\) and \(\)\(40\) tokens respectively. This direct evidence demonstrates that the behaviour of policy \(_{}\) can be significantly changed through prompting the annotating LLM differently, and the degree of the changes can be controlled as well.

However, the above changes come at a cost. In Figure 5(b), we plot the win rate of the "helpful", "helpful and short", and "helpful and very short" models against the initial SFT baseline. We noticed that the shorter responses become much less helpful, as judged by Gemini Pro. Nevertheless, they still improve the performance of the aligned model over the SFT baseline. This finding is also confirmed by human evaluation: from "helpful", "helpful and short" to "helpful and very short", the average quality score drops from 4.08, 3.72 to 3.26, all outperforming the SFT baseline (3.19) still.

### Can weaker AI labeller improve stronger LLM?

Section 4.5 shows that PaLM 2-XS could provide reasonable feedback that helps improving the alignment of LLMs, although it's significantly smaller than PaLM 2-S/L. We argue that our approach offers an orthogonal solution to the _weak-to-strong generalisation_ problem investigated by Burns et al. (2023). To verify that a weaker AI labeller can improve the performance of a stronger LLM model, we perform experiments using PaLM 2-S as the policy model (student) under two teacher

Figure 6: Performance on the Helpfulness task of online DPO with OAIF, trained to be _helpful only_, _helpful and short_, _helpful and very short_. Win rates are judged by Gemini Pro. Results for SFT, RLHF, and RLAIF models are given as references.

settings: one with PaLM 2-XS (weaker teacher) and the other with PaLM 2-L (stronger teacher). The side-by-side automatic evaluation results on Helpfulness comparing against the SFT baseline and offline DPO are given in Figure 7. Our results suggest that OAIF from a weaker teacher indeed improved the alignment of PaLM 2-S, though they are less effective compared with the OAIF from a stronger teacher.

We hereby emphasise the essential difference between the setup investigated by Burns et al. (2023) and ours. In their work, the tasks for the teacher and student model are both supervised learning tasks, thus they are of equal difficulty. However, in our work, the role of teacher is a simpler discriminative task (labelling preference), whereas the student model being aligned is given a more difficult one (generating proper responses). Following this perspective, our method is actually closer in spirit to the generative adversarial network proposed by Goodfellow et al. (2020), but doesn't train a particular discriminator.

## 5 Limitations

In this work, we study only the shift between distributions over responses, e.g. \((|)\) and \(_{^{}}(|)\). However, the shifts also happen on the user prompt distribution \(p_{}\) and the ground-truth human value function. Although the prompt-controllability of OAIF raises a possible solution to later case, the shift of \(p_{}\) is still a challenge. Since we extract prompts from the given preference dataset, our study assumes an in-distribution of prompts used for evaluation, thus lacks of evaluating the performance of the aligned LLMs on out-of-distribution prompts. In the meantime, we use a separate annotating prompt for each task studied in Section 4, whereas aligning LLMs towards general human values requires a universal prompt to get OAIF across tasks. We hereby argue that the principles for the constitutional AI proposed by Bai et al. (2022b) can serve as a good basis for extending this work. Moreover, the model aligned in Section 4 is mostly PaLM 2-XS, thus whether our conclusion holds after scaling up is not investigated. As pointed out by Bai et al. (2022a), it is harder to distinguish responses of higher quality. Therefore, how much can OAIF work for responses from larger LLMs requires further study.

## 6 Conclusion

To circumvent the offline feedback problem in direct alignment from preference (DAP) methods, such as DPO, we proposed Online AI Feedback (OAIF), a simple and effective way to make DAP methods online via AI feedback. We carried out an extensive empirical evaluation, using both AI and human evaluation, which showed the effectiveness of DAP methods combined with OAIF, against their offline counterparts. We also exhibited the tendency of offline DAP methods to overfit, and in contrast the usefulness of OAIF as a way to mitigate reward overoptimization. We further verified the generality of OAIF, as our empirical results hold for three prominent DAP methods: DPO, IPO and SLiC.

Beyond the empirical evaluation of OAIF, our work also contributes the comparison of two types of methods: online DAP methods (e.g., online DPO) and RLAIF. Since the feedback comes from identical models in both learning algorithms, our experiment setup ensures that the AI feedback is of the same quality and that only the learning procedures differ. Our experimental results in various tasks show that online DPO outperforms RLAIF and RLAIF, which further confirms the effectiveness of OAIF, compared to offline feedback. Moreover, we used response length as a test bed to demonstrate that the LLM annotator can be controlled easily using instruction prompts. This shows that OAIF can be used to achieve desirable alignment goals.

Overall, this work demonstrates the effectiveness and importance of OAIF for aligning LLMs, and paves the way for more scalable alignment strategies, requiring reduced human annotation effort.

Figure 7: Win rate of online DPO with OAIF from PaLM 2-XS (weak teacher) and PaLM 2-L (strong teacher) against the SFT baseline and offline DPO, in the task Helpfulness, judged by _Gemini_\(Pro\).