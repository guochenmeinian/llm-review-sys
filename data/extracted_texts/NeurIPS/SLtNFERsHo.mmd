# CoLA: Exploiting Compositional Structure for

Automatic and Efficient Numerical Linear Algebra

Andres Potapczynski\({}^{*}\)1 Marc Finzi\({}^{*}\)2 Geoff Pleiss\({}^{3,4}\) Andrew Gordon Wilson\({}^{1}\)

\({}^{1}\)New York University, \({}^{2}\)Carnegie Mellon University, \({}^{3}\)University of British Columbia,

\({}^{4}\)Vector Institute

Equal contribution.

###### Abstract

Many areas of machine learning and science involve large linear algebra problems, such as eigendecompositions, solving linear systems, computing matrix exponentials, and trace estimation. The matrices involved often have Kronecker, convolutional, block diagonal, sum, or product structure. In this paper, we propose a simple but general framework for large-scale linear algebra problems in machine learning, named _CoLA_ (Compositional Linear Algebra). By combining a linear operator abstraction with compositional dispatch rules, CoLA automatically constructs memory and runtime efficient numerical algorithms. Moreover, CoLA provides memory efficient automatic differentiation, low precision computation, and GPU acceleration in both JAX and PyTorch, while also accommodating new objects, operations, and rules in downstream packages via multiple dispatch. CoLA can accelerate many algebraic operations, while making it easy to prototype matrix structures and algorithms, providing an appealing drop-in tool for virtually any computational effort that requires linear algebra. We showcase its efficacy across a broad range of applications, including partial differential equations, Gaussian processes, equivariant model construction, and unsupervised learning.

## 1 Introduction

The framework of automatic differentiation has revolutionized machine learning. Although the rules that govern derivatives have long been known, automatically computing derivatives was a nontrivial process that required (1) efficient implementations of base-case primitive derivatives, (2) software abstractions (autograd and computation graphs) to compose these primitives into complex computations, and (3) a mechanism for users to modify or extend compositional rules to new functions. Once libraries such as PyTorch, Chainer, Tensorflow, JAX, and others  figured out the correct abstractions, the impact was enormous. Efforts that previously went into deriving and implementing gradients could be repurposed into developing new models.

In this paper, we automate another notorious bottleneck for ML methods: performing large-scale linear algebra (e.g. matrix solves, eigenvalue problems, nullspace computations). These ubiquitous operations are at the heart of principal component analysis, Gaussian processes, normalizing flows, equivariant neural networks, and many other applications . Modeling assumptions frequently manifest themselves as algebraic structure--such as diagonal dominance, sparsity, or a low-rank factorization. Given a structure (e.g., the sum of low-rank plus diagonal matrices) and a linear algebraic operation (e.g., linear solves), there is often a computational routine (e.g. the linear-time Woodbury inversion formula) with lower computational complexity than a general-purpose routine (e.g., the cubic-time Cholesky decomposition). However, exploitingstructure for faster computation is often an intensive implementation process. Rather than having an object \(\) in code that represents a low-rank-plus-diagonal matrix and simply calling \((,)\), a practitioner must instead store the low-rank factor \(\) as a matrix, the diagonal \(\) as a vector, and implement the Woodbury formula from scratch. Implementing structure-aware routines in machine learning models is often seen as a major research undertaking. For example, a nontrivial portion of the Gaussian process literature is devoted to deriving specialty inference algorithms for structured kernel matrices (e.g. ).

As with automatic differentiation, structure-aware linear algebra is ripe for automation. We introduce a general numerical framework that dramatically simplifies implementations efforts while achieving a high degree of computational efficiency. In code, we represent structure matrices as \(\) objects which adhere to the same API as standard dense matrices. For example, a user can call \(^{-1}\) or \(()\) on any \(\)\(\), and under-the-hood our framework derives a computationally efficient algorithm built from our set of compositional _dispatch rules_ (see Table 1). If little is known about \(\), the derived algorithm reverts to a general-purpose base case (e.g. Gaussian elimination or GMRES for linear solves). Conversely, if \(\) is known to be the Kronecker product of a lower triangular matrix and a positive definite Toeplitz matrix, for example, the derived algorithm uses specialty algorithms for Kronecker, triangular, and positive definite matrices. Through this compositional pattern matching, our framework can match or outperform special-purpose implementations across numerous applications despite relying on only a small number of base \(\) types.

Furthermore, our framework offers additional novel functionality that is necessary for ML applications (see Table 2). In particular, we automatically compute gradients, diagonals, transposes and adjoints of linear operators, and we modify classic iterative algorithms to ensure numerical stability in low precision. We also support specialty algorithms, such as SVRG  and a novel variation of Hutchinson's diagonal estimator , which exploit _implicit structure_ common to matrices in machine learning applications (namely, the ability to express matrices as large-scale sums amenable to stochastic approximations). Moreover, our framework is easily extensible in _both directions_: a user can implement a new linear operator (i.e. one column in Table 1), or a new linear algebraic operation (i.e. one row in Table 1). Finally, our routines benefit from GPU and TPU acceleration and apply to symmetric and non-symmetric operators for both real and complex numbers.

We term our framework _CoLA_ (**C**ompositional **L**inear **A**lgebra), which we package in a library that supports both PyTorch and JAX. We showcase the extraordinary versatility of CoLA with

  & Base &  &  \\  & Case & D & T & P & C & S & Pr & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  \(^{-1}\) & & & & & & & & & & & & & & & \\ \(()\) & & & & & & & & & & & & & & & \\ \(()\) & & & & & & & & & & & & & & & \\ \(()\) & & & & & & & & & & & & & & & \\ \(()\) & & & & & & & & & & & & & & & & \\ \(()\) & & & & & & & & & & & & & & & & \\ 

Table 1: **Many structures have explicit composition rules to exploit.** Here we show the existence of a dispatch rule () that can be used to accelerate a linear algebraic operation for some matrix structure over what is possible with the dense and iterative base cases. Many combinations (shown with) are automatically accelerated as a consequence of other rules, since for example \(\) and \(\) are used in other routines. In absence of a rule, the operation will fall back to the iterative and dense base case for each operation (shown in). Columns are basic linear operator types such as D: Diagonal, T: Triangular, P: Permutation, C: Convolution, S:Sparse, Pr: Projection and composition operators such as sum, product, Kronecker product, block diagonal and concatenation. All compositional rules can be mixed and matched and are implemented through multiple dispatch.

a broad range of applications in Section 3.2 and Section 4, including: PCA, spectral clustering, multi-task Gaussian processes, equivariant models, neural PDEs, random Fourier features, and PDEs like minimal surface or the Schrodinger equation. Not only does CoLA provide competitive performance to specialized packages but it provides significant speedups especially in applications with compositional structure (Kronecker, block diagonal, product, etc). Our package is available at https://github.com/wilson-labs/cola.

## 2 Background and Related Work

Structured matricesStructure appears throughout machine learning applications, either occurring naturally through properties of the data, or artificially as a constraint to simplify complexity. A nonexhausitve list of examples includes: (1) low-rank matrices, which admit efficient solves and determinants ; (2) sparse matrices, which admit fast methods for linear solves and eigenvalue problems [14; 44]; (3) Kronecker-factorizable matrices, which admit efficient spectral decompositions; (4) Toeplitz or circulant matrices, which admit fast matrix-vector products. See Section 3 and Section 4 for applications that use these structures. Beyond these explicit types, we also consider _implicit structures_, such as matrices with clustered eigenvalues or matrices with simple unbiased estimates. Though these implicit structures do not always fall into straightforward categorizations, it is possible to design algorithms that exploit their inherent properties (see Section 3.3).

Iterative matrix-free algorithmsUnlike direct methods, which typically require dense instantiations of matrices, matrix-free algorithms only access matrices through routines that perform matrix-vector multiples (MVMs) [e.g. 44]. The most common matrix-free algorithms--such as conjugate gradients, GMRES, Lanczos and Arnoldi iteration--fall under the category of Krylov subspace methods, which iteratively apply MVMs to refine a solution until a desired error tolerance is achieved. Though the rate of convergence depends on the conditioning or spectrum of the matrix, the number of iterations required is often much less than the size of the matrix. These algorithms often provide significant computational speedups for structured matrices that admit sub-quadratic MVMs (e.g. sparse, circulant, Toeplitz, etc.) or when using accelerated hardware (GPUs or TPUs) designed for efficient parallel MVMs [e.g. 10; 20; 51].

Multiple dispatchPopularized by Julia, multiple dispatch is a functional programming paradigm for defining type-specific behaviors. Under this paradigm, a given function (e.g. solve) can have multiple definitions, each of which are specific to a particular set of input types. A base-case definition solve[LinearOperator] would use a generic matrix-vector solve algorithm (e.g. Gaussian elimination or GMRES), while a type-specific definition (e.g. solve[Sum], for sums of matrices) would use a special purpose algorithm that makes use of the subclass' structure (e.g. SVRG, see Section 3.3). When a user calls solve\((,)\) at runtime, the _dispatcher_ determines which definition of solve to use based on the types of \(\) and \(\). Crucially, dispatch rules can be written for compositional patterns of types. For example, a solve[Sum[LowRank, Diagonal]] function will apply the Woodbury formula to a Sum operator that composes LowRank and Diagonal matrices. (In contrast, under an inheritance paradigm, one would need to define a specific SumOfLowRankAndDiagonal sub-class that uses the Woodbury formula, rather than relying on the composition of general purpose types.)

Existing frameworks for exploiting structureAchieving fast computations with structured matrices is often a manual effort. Consider for example the problems of second order/natural gradient optimization, which require matrix solves with (potentially large) Hessian/Fisher matrices. Researchers have proposed tackling these solves with matrix-free methods , diagonal approximations [e.g. 4], low-rank approximations [e.g. 42], or Kronecker-factorizable approximations . Despite their commonality--relying on structure for fast solves--all methods currently require different implementations, reducing interoperability and adding overhead to experimenting with new structured approximations. As an alternative, there are existing libraries like SciPy Sparse , Spot , PyLops , or GPyTorch , which offer a unified interface for using matrix-free algorithms with any type of structured matrices. A user provides an efficient MVM function for a given matrix and then chooses the appropriate iterative method (e.g. conjugate gradients or GMRES) to perform the desired operation (e.g. linear solve). With these libraries, a user can adapt to different structures simply by changing the MVM routine. However, this increased interoperability comes at the cost of efficiency, as the iterative routines are not optimal for every type of structure. (For example, Kronecker products admit efficient inverses that are asymptotically faster than conjugate gradients; see Figure 1.) Moreover, these libraries often lack modern features (e.g. GPU acceleration or automatic differentiation) or are specific to certain types of matrices (see Table 2).

## 3 CoLA: Compositional Linear Algebra

We now discuss all the components that make CoLA. In Section 3.1 we first describe the core MVM based LinearOperator abstraction, and in Section 3.2 we discuss our core compositional framework for identifying and automatically exploiting structure for fast computations. In Section 3.3, we highlight how CoLA exploits structure frequently encountered in ML applications beyond well-known analytic formulae (e.g. the Woodbury identity). Finally, in Section 3.4 we present CoLA's machine learning-specific features, like automatic differentiation, support for low-precision, and hardware acceleration.

### Deriving Linear Algebraic Operations Through Fast MVMs

Borrowing from existing frameworks like Scrip Sparse, the central object of our framework is the LinearOperator: a linear function on a finite dimensional vector space, defined by how it acts on vectors via a matrix-vector multiply \(_{A}:\,\). While this function has a matrix representation for a given basis, we do not need to store or compute this matrix to perform a MVM. Avoiding the dense representation of the operator saves memory and often compute.

Some basic examples of LinearOperators are: unstructured Dense matrices, which are represented by a 2-dimensional array and use the standard MVM routine \([]_{i}=_{j=1}A_{ij}v_{j}\); Sparse matrices, which can be represented by key/value arrays of the nonzero entries with the standard CSR-sparse MVM routine; Diagonal matrices, which are represented by a 1-dimensional array of the diagonal entries and where the MVM is given by \([()]_{i}=d_{i}v_{i}\); Convolution operators, which are represented by a convolutional filter array and where the MVM is given by \(()=*\) ; or JVP operators--the Jacobian represented implicitly through an autograd Jacobian Vector Product--represented by a function and an input \(\) and where the MVM is given by \((f,)=(f,,)\). In CoLA, each of these examples are sub-classes of the LinearOperator superclass.

Through the LinearOperator's MVM, it is possible to derive other linear algebraic operations. As a simple example, we obtain the dense representation of the LinearOperator by calling \((_{1})\),..., \((_{N})\), on each unit vector \(_{i}\). We now describe several key operations supported by our framework, some well-established, and others novel to CoLA.

**Solves, eigenvalue problems, determinants, and functions of matrices**  As a base case for larger matrices, CoLA uses _Krylov subspace methods_ (Section 2, Appendix C) for many matrix operations. Specifically, we use GMRES  for matrix solves and Arnoldi  for finding eigenvalues, determinants, and functions of matrices. Both of these algorithms can be applied to any non-symmetric and/or complex linear operator. When LinearOperators are annotated with additional structure (e.g. self-adjoint, positive semi-definite) we use more efficient Krylov algorithms like MINRES, conjugate gradients, and Lanczos (see Section 3.2). As stated in Section 2, these algorithms are

 Package & GPU Support & Autograd &  Non-symmetric Complex \\ Matrices \\  &  Randomized \\ Numbers \\  &  Composition \\ Algorithms \\  & 
 Composition \\ Rules \\  \\  Scrip Sparse & & & & & & & \\ PyLops & & & & & & \\ GPyTorch & & & & & & \\ CoLA & & & & & & \\  

Table 2: Comparison of scalable linear algebra libraries. PyLops only supports propagating gradients through vectors but not through the linear operator’s parameters. Moreover, PyLops has limited GPU support through CUPY, but lacks support for PyTorch, JAX or TensorFlow which are necessary for modern machine learning applications.

matrix free (and thus memory efficient), amenable to GPU acceleration, and asymptotically faster than dense methods. See Section C.2 for a full list of Krylov methods used by CoLA.

Transposes and complex conjugationsIn alternative frameworks like Scipy Sparse a user must manually define a transposed MVM\(^{}\) for linear operator objects. In contrast, CoLA uses a novel autograd trick to derive the transpose from the core MVM routine. We note that \(^{}\) is the vector-Jacobian product (VJP) of the vector \(\) and the Jacobian \(/\). Thus, the function \(()\) returns a LinearOperator object that uses \((_{},,)\) as its MVM. We extend this idea to Hermitian conjugates, using the fact that \(^{*}=(})^{}=( ^{}})\).

Other operationsIn Section 3.3 we outline how to stochastically compute diagonals and traces of operators with MVMs, and in Section 3.4 we discuss a novel approach for computing memory-efficient derivatives of iterative methods through MVMs.

ImplementationCoLA implements all operations (solve, eig, logdet, transpose, conjugate, etc.) following a functional programming paradigm rather than as methods of the LinearOperator object. This is not a minor implementation detail: as we demonstrate in the next section, it is crucial for the efficiency and compositional power of our framework.

### Beyond Fast MVMs: Exploiting Explicit Structure Using Composition Rules

While the GMRES algorithm can compute solves more efficiently than corresponding dense methods such as the Cholesky decomposition, especially with GPU parallelization and preconditioning, it is not the most efficient algorithm for many LinearOperators. For example, if \(=()\), then we know that \(^{-1}=(^{-1})\) without needing to solve a linear system. Similarly, solves with triangular matrices can be inverted efficiently through back substitution, and solves with circulant matrices can be computed efficiently in the Fourier domain \(()=^{-1}() \) (where \(\) is the Fourier transform linear operator). We offer more examples in Table 1 (left).

As described in Section 2, we use multiple dispatch to implement these special case methods. For example, we implement the solve[Diagonal], solve[Triangular], and solve[Circulant] dispatch rules using the efficient routines described above. If a specific LinearOperator subclass does not have a specific solve dispatch rule then we default to the base-case solve rule using GMRES. This behaviour also applies to other operations, such as logdet, eig, diagonal, etc.

The dispatch framework makes it easy to implement one-off rules for the basic LinearOperator sub-classes described in Section 3.1. However, its true power lies in the use of compositional rules, which we describe below.

Compositional Linear OperatorsIn addition to the base LinearOperator sub-classes (e.g. Sparse, Diagonal, Convolution), our framework provides mechanisms to compose multiple LinearOperators together. Some frequently used compositional structures are Sum (\(_{i}_{i}\)), Product\((_{i}_{i})\), Kronecker\(()\), KroneckerSum\(()\), BlockDiag\([,0;\ 0,]\) and Concatenation\([,]\). Each of these compositional LinearOperators are defined by (1) the base LinearOperator objects to be composed, and (2) a corresponding MVM routine, which is typically written in terms of the MVMs of the composed LinearOperators. For example, \(_{}=_{i}_{i}()\), where MVM\({}_{i}\) are the MVM routines for the component LinearOperators.

    & \(_{i}^{M}_{i}\) & \(_{i}^{M}_{i}\) & BlockDiag\((,)\) & Kron\((,)\) \\  MVM (\(\)) & \(_{i}_{i}\) & \(_{i}_{i}\) & \(_{A}+_{B}\) & \(_{A}N_{B}+N_{A}_{B}\) \\ Solve (\(s\)) & \(_{i}_{i}_{i}\) & \((1+/M)\) & \(s_{A}+s_{B}\) & \(s_{A}N_{B}+N_{A}s_{B}\) \\ Eigs (\(E\)) & \(_{i, i}\) & \((1+/M)\) & \(E_{A}+E_{B}\) & \(E_{A}+E_{B}\) \\   

Table 3: **CoLA selects the best rates for each operation or structure combination.** Asymptotic runtimes resulting from dispatch rules on compositional linear operators in our framework. Listed operations are matrix vector multiplies, linear solves, and eigendecomposition. Here \(\) denotes error tolerance. For a given operator of size \(N N\), we denote \(\) as its MVM cost, \(s\) its linear solve cost, \(E\) its eigendecomposition cost and \(\) its condition number. A lower script indicates to which matrix the operation belongs to.

Dispatch rules for compositional operators are especially powerful. For example, consider Kronecker products where we have the rule \(()^{-1}=^{-1}^{-1}\). Though simple, this rule yields highly efficient routines for numerous structures. For example, suppose we want to solve \(()=\) where \(\) is dense, \(\) is diagonal, and \(\) is triangular. From the rules, the solve would be split over the product, using GMRES for \(\), diagonal inversion for \(\), and forward substitution for \(\). This breakdown is much more efficient than the base case (GMRES with \(_{}\)).

When exploited to their full potential, these composition rules provide both asymptotic speedups (shown in Table 3) as well as runtime improvements on real problems across practical sizes (shown in Figure 1). Splitting up the problem with composition rules yields speedups in surprising ways even in the fully iterative case. To illustrate, consider one large CG solve with the matrix power \(=^{n}\); in general, the runtime is upper-bounded by \(O(n})\), where \(\) is the time for a MVM with \(\), \(\) is the condition number of \(\), and \(\) is the desired error tolerance. However, splitting the product via a composition rule into a sequence of solves has a much smaller upper-bound of \(O(n)\). We observe this speedup in the solving the Bi-Poisson PDE shown in Figure 1(b).

Additional flexibly and efficiency via parametric typingA crucial advantage of multiple dispatch is the ability to write simple special rules for compositions of specific operators. While a general purpose solve[Sum] method (SVRG; see next section) yields efficiency over the GMRES base case, it is not the most efficient algorithm when the Sum operator is combining a LowRank and a Diagonal operator. In this case, the Woodbury formula would be far more efficient. To account for this, CoLA allows for dispatch rules on _parametric types_; that is, the user defines a solve[Sum[LowRank, Diagonal]] dispatch rule that is used if the Sum operator is specifically combining a LowRank and a Diagonal linear operator. Coding these rules without multiple dispatch would require specialty defining sub-classes like LowRankPlusDiagonal over the LinearOperator object, increasing complexity and hampering extendibility.

Decoration/annotation operatorsFinally, we include several _decorator_ types that annotate existing LinearOperators with additional structure. For example, we define SelfAdjoint (Hernetain/symmetric), Unitary (orthonormal), and PSD (positive semi-definite) operators, each of which wraps an existing LinearOperator object. None of these decorators define a specialty MVM; however, these decorators can be used to define dispatch rules for increased efficiency. For example solve[PSD] can use conjugate gradients rather than GMRES, and solve[PSD[Tridiagonal]] can use the linear time tridiagonal Cholesky decomposition (see e.g., 21, Sec. 4.3.6).

Taken togetherOur framework defines 16 base linear operators, 5 compositional linear operators, 6 decoration linear operators, and roughly 70 specialty dispatch rules for solve, eig, and other operations. (See Table 1 for a short summary and Appendix A for a complete list of rules.) We note that these numbers are relatively small compared with existing solutions yet--as we demonstrate in Section 4-- these operators and dispatch rules are sufficient to match or exceed performance of specialty implementations in numerous applications. Finally, we note that CoLA is extensible by users in _both directions_. A user can write their own custom dispatch rules, either to (1) define a new LinearOperator and special dispatch rules for it, or (2) to define a new algebraic operation for all LinearOperators, and crucially this requires no changes to the original implementation.

### Exploiting Implicit Structure in Machine Learning Applications

So far we have discussed _explicit_ matrix structures and composition rules for which there are simple analytic formulas easily found in well-known references (e.g. 21; 44; 48). However, current large systems--especially those found in machine learning-- often have _implicit structure_ and special properties that yield additional efficiencies. In particular, many ML problems give rise to linear operators composed of large summations which are amenable to stochastic algorithms. Below we outline two impactful general purpose algorithms used in CoLA to exploit this implicit structure.

Accelerating iterative algorithms on large sums with SVRGStochastic gradient descent (SGD) is widely used for optimizing problems with very large or infinite sums to avoid having to traverse the full dataset per iteration. Like Monte Carlo estimation, SGD is very quick to converge to a few decimal places but very slow to converge to higher accuracies. When an exact solution is required on a problem with a finite sum, the stochastic variance reduced gradient (SVRG) algorithm  is much more compelling, converging on strongly convex problems (and many others) at an exponential rate, with runtime \(O((1+/M))\) where \(\) is the condition number and \(\) is the desired accuracy.

When the condition number and the number of elements in the sum is large, SVRG becomes a desirable alternative even to classical deterministic iterative algorithms such as CG or Lanczos whose runtimes are bounded by \(O()\). Figure 2 shows the impact of using SVRG to exploit the structure of different linear operators that are composed of large sums.

Stochastic diagonal and trace estimation with reduced varianceAnother case where we exploit implicit structure is when estimating the trace or the diagonal of a linear operator. While collecting the diagonal for a dense matrix is a trivial task, it is a costly algorithm for an arbitrary LinearOperator defined only through its MVM--it requires computing \(()=_{i=1}^{N}e_{i}e_{i}\) where \(\) is the Hadamard (elementwise) product. If we need merely an approximation or unbiased estimate of the diagonal (or the sum of the diagonal), we can instead perform stochastic diagonal estimation \(}()=_{j=1}^{n}z_{j} z_{j}\) where the \(z_{j}^{N}\) are any randomly sampled probe vectors with covariance \(I\). We extend this randomized estimator to use randomization both in the probes, and random draws from a sum when \(=_{i=1}^{M}_{i}\):

\[}(_{i=1}^{M}_{i}):=_{ij} z_{ij}_{i}z_{ij}.\]

Figure 1: **Empirically, our composition rules yield the best runtimes across applications consisting of linear operators with different structures (more application details in Section 4). We plot mean runtime (over 3 repetitions) for different methods (dense, iterative and ours (CoLA)) against the size of the linear operator. (a) Computing solves on a multi-task GP problem  for a linear operator having Kronecker structure \(_{T}_{X}\), where \(_{T}\) is a kernel matrix containing the correlation between the tasks and \(_{X}\) is a RBF kernel on the data. For this experiment we used a synthetic Gaussian dataset which has dimension \(D=33\), \(N=1\)K and we used \(T=11\) tasks. (b) Computing solves on the 2-dimensional Bi-Poisson PDE problem for the composition of the Laplacian operator \(\) composed with itself on grid of sizes up to \(N=1000^{2}\). We use CG with a multi-grid \(\)SA preconditioner  to solve the linear system required in this application. (c) Finding the nullspace of an equivariant MLP of a linear operator having block diagonal structure. Here, NullF refers to the iterative nullspace finder algorithm detailed in . We ran a 5-node symmetric operator \(S(5)\) as done in  with MLP sizes up to \(15\)K. See Appendix D for further details.**

Figure 2: **CoLA exploits the sum structure of linear operators through stochastic routines. (a) Eigenvalue convergence criteria against number of MVMs for computing the first principal component on Buzz (\(N=430\)K, \(D=77\)) using VR-PCA . (b) Solve relative residual against number of MVMs for a random Fourier features (RFFs) approximation  to a RBF kernel with \(J=1\)K features on Elevators (\(N=12.5\)K, \(D=18\)). (c) Solve relative residual against number of MVMs when applying Neural-IVP  to the 2-dimensional wave equation equation as done in . See Appendix D for further details.**

In Section B.1 we derive the variance of this estimator and we show that it converges faster than the base Hutchinson estimator when applied Sum structures. We validate empirically this analysis in Figure 5.

### Automatic Differentiation and Machine Learning Readiness

Memory efficient auto-differentiationIn ML applications, we want to backpropagate through operations like \(^{-1}\), \(()\), \(()\), \(()\), \(()\). To achieve this, in CoLA we define a novel concept of the gradient of a LinearOperator which we detail in Appendix B. For routines like GMRES, SVRG, and Arnoldi, we utilize a custom backward pass that does not require backproagating through the iterations of these algorithms. This custom backward pass results in substantial memory savings (the computation graph does not have to store the intermediate iterations of these algorithms), which we demonstrate in Appendix B (Figure 6).

Low precision linear algebraBy default, all routines in CoLA support the standard float32 and float64 precisions. Moreover, many CoLA routines also support float16 and bfloat16 half precision using algorithmic modifications for increased stability. In particular, we use variants of the GMRES, Arnoldi, and Lanczos iterations that are less susceptible to instabilities that arise through orthogonalization (44, Ch. 6) and we use the half precision variant of conjugate gradients introduced by Maddox et al. (2016). See Appendix C for further details.

Multi framework support and GPU/TPU accelerationCoLA is compatible with both PyTorch and JAX. This compatibility not only makes our framework _plug-and-play_ with existing implemented models, but it also adds GPU/TPU support, differentiating it from existing solutions (see Table 2). CoLA's iterative algorithms are the class of linear algebra algorithms that benefit most from hardware accelerators as the main bottleneck of these algorithms are the MVMs executed at each iteration, which can easily be parallelized on hardware such as GPUs. Figure 3 empirically shows the additional impact of hardware accelerators across different datasets and linear algebra operations.

## 4 Applications

We now apply CoLA to an extensive list of applications showing the impact, value and broad applicability of our numerical linear algebra framework, as illustrated in Figure 4. This list of applications encompasses PCA, linear regression, Gaussian processes, spectral clustering, and partial differential equations like the Schrodinger equation or minimal surface problems. In contrast to Section 3 (Figure 1 & Figure 2), the applications presented here have a basic structure (sparse, vector-product, etc) but not a compositional structure (Kronecker, product, block diagonal, etc). We choose these applications due to their popularity and heterogeneity (the linear operators have different properties: self-adjoint, positive definite, symmetric and non-symmetric), and to show that CoLA

Figure 3: **For sufficiently large problems, switching from dense to iterative algorithms provides consistent runtime reductions, especially on a GPU, where matrix multiplies can be effectively parallelized. We plot the ratio between the runtime of a linear algebra operation using CoLA or PyTorch on different hardware (CPU and GPU) divided by the runtime of using PyTorch CPU. For the linear solves, we use the matrix market sparse operator Trefethen; for the eigenvalue estimation, we use the matrix market sparse operator mhd4800b and, finally, for the log determinant computation, we use the matrix market sparse operator bcsstk18. We provide additional details in Section D.4.**

performs in any application. We compare against several well-known libraries, sometimes providing runtime improvements but other times performing equally. This is remarkable as our numerical framework does not specialize in any of those applications (like GPyTorch) nor does it rely on Fortran implementations of high-level algorithms (like sklearn or SciPy). Below we describe each of the applications found in Figure 4.

Principal Component AnalysisPCA is a classical ML technique that finds the directions in the data that capture the most variance. PCA can be performed by computing the right singular vectors of \(^{N D}\). When the number of data points \(N\) is very large, stochastic methods like SVRG in VR-PCA  can accelerate finding the eigenvectors over SVD or Lanczos, as shown in Figure 2(a).

Spectral ClusteringSpectral clustering  finds clusters of individual nodes in a graph by analyzing the graph Laplacian \(=-\) where \(\) denotes a diagonal matrix containing the degree of the nodes and \(\) the weights on the edges between nodes. This problem requires finding the smallest \(k\) eigenvectors of \(\). We run this experiment on the high energy physics arXiv paper citation graph (cit-HepPh).

Gaussian processesGPs are flexible nonparametric probabilistic models where inductive biases are expressed through a covariance (kernel) function. At its core, training a GP involves computing and taking gradients of the log determinant of a kernel \(||\) and of a quadratic term \(^{T}^{-1}\) (where \(\) is the vector of observations).

Schrodinger EquationIn this problem we characterize the spectrum of an atom or molecule by finding the eigenspectrum of a PDE operator in a Schrodinger equation \(=E\). After discretizing \(\) to a grid, we compute the smallest eigenvalues and eigenvectors of the operator \(\) which for this experiment is non-symmetric as we perform a compactfying transform.

Figure 4: **CoLA is easily applied to numerous applications with competitive performance**. Here sk: sklearn, GP: GPyTorch and the tuple (\(N\), \(D\)) denotes dataset size and dimensionality. **(a)**: Runtime for PCA decomposition on Buzz (\(437.4\)K, \(77\)). **(b)**: Linear regression runtime on Song (\(386.5\)K, \(90\)), where we run CoLA on both GPU and CPU. **(c)**: Training efficiency (measure in epochs) on exact GP inference on Elevators (\(14\)K, \(18\)) and Kin (\(20\)K, \(8\)) on GPU. **(d)**: Spectral clustering runtime on a citations graph (cit-HepPh) consisting on \(34.5\)K nodes and \(842\)K edges. sk(L) denotes sklearn’s implicitly restarted Lanczos implementation and sk(A) denotes sklearn’s LOBPCG with an algebraic multi-graph preconditioner (PyAMG) . CoLA(L) denotes our Lanczos implementation and CoLA(B) our LOBPCG implementation. **(e)**: Runtimes for finding the smallest eigenfunctions expanding grids of a Schrödinger equation with an expanding finite difference grid. **(f)**: Runtimes for solving the minimal surface equation via root finding on expanding grids. Here SciPy utilizes the ARPACK package, a highly-optimized Fortran implementation of the Arnoldi iteration, while SciPy JAX (the SciPy version integrated with JAX) and CoLA utilize python Arnoldi implementations. Appendix D expands on the experimental details.

Minimal SurfaceHere we solve a set of nonlinear PDEs with the objective of finding the surface that locally minimizes its area under given boundary constraints. When applied to the graph of a function, the PDE can be expressed as \(f(z)=(1+z_{x}^{2})z_{yy}-2z_{x}z_{y}z_{xy}+(1+z_{y}^{2})z_{xx}=0\) and solved by root finding on a discrete grid. Applying Newton-Raphson, we iteratively solve the non-symmetric linear system \(z z-^{-1}f(z)\) where \(\) is the Jacobian of the PDE operator.

Bi-Poisson EquationThe Bi-Poisson equation \(^{2}u=\) is a linear boundary value PDE relevant in continuum mechanics, where \(\) is the Laplacian. When discretized using a grid, the result is a large symmetric system to be solved. We show speedups from the product structure in Figure 1(b).

Neural PDEsNeural networks show promise for solving high dimensional PDEs. One approach for initial value problems requires advancing an ODE on the neural network parameters \(\), where \(=()^{-1}F()\) where \(\) is an operator defined from Jacobian of the neural network which decomposes as the sum over data points \(=_{i}_{i}\) and where \(F\) is determined by the governing dynamics of the PDE [15; 17]. By leveraging the sum structure with SVRG, we provide further speedups over Finzi et al.  as shown in Figure 2(c).

Equivariant Neural Network ConstructionAs shown in , constructing the equivariant layers of a neural network for a given data type and symmetry group is equivalent to finding the nullspace of a large linear equivariance constraint \(=\), where the constraint matrix \(\) is highly structured, being a block diagonal matrix of concatenated Kronecker products and Kronecker sums of sparse matrices. In Figure 1(c) we show the empirical benefits of exploiting this structure.

## 5 Discussion

We have presented the CoLA framework for structure-aware linear algebraic operations in machine learning applications and beyond. Building on top of dense and iterative algorithms, we leverage explicit composition rules via multiple dispatch to achieve algorithmic speedups across a wide variety of practical applications. Algorithms like SVRG and a novel variation of Hutchinson's diagonal estimator exploit implicit structure common to large-scale machine learning problems. Finally, CoLA supports many features necessary for machine learning research and development, including memory efficient automatic differentiation, multi-framework support of both JAX and PyTorch, hardware acceleration, and lower precision.

While structure exploiting methods are used across different application domains, domain knowledge often does not cross between communities. We hope that our framework brings these disparate communities and ideas together, enabling rapid development and reducing the burden of deploying fast methods for linear algebra at scale. Much like how automatic differentiation simplified and accelerated the training of machine learning models--with custom autograd functions as the exception rather than the rule--CoLA has the potential to streamline scalable linear algebra.