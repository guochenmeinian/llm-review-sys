# DIETing: Self-Supervised Learning with Instance Discrimination Learns Identifiable Features

Attila Juhos\({}^{*}\)\({}^{1}\), Alice Bizeul\({}^{*}\)\({}^{2}\), Patrik Reizinger\({}^{*}\)\({}^{1}\),

David Klindt\({}^{5}\), Randall Balestriero\({}^{4}\), Mark Ibrahim\({}^{6}\), Julia E. Vogt\({}^{2}\), Wieland Brendel\({}^{1}\)

{patrik.reizinger, attila.juhos, wieland.brendel}@tuebingen.mpg.de

{alice.bizeul, julia.vogt}@inf.ethz.ch, klindt@cshl.edu,

rbalestr@brown.edu, marksibrahim@meta.com

###### Abstract

Self-Supervised Learning (SSL) methods often consist of elaborate pipelines with hand-crafted data augmentations and computational tricks. However, it is unclear what is the provably minimal set of building blocks that ensures good downstream performance. The recently proposed instance discrimination method, coined DIET, stripped down the SSL pipeline and demonstrated how a simple SSL algorithm can work by predicting the sample index. Our work proves that DIET recovers cluster-based latent representations, while successfully identifying the correct cluster centroids in its classification head. We demonstrate the identifiability of DIET on synthetic data adhering to and violating our assumptions, revealing that the recovery of the cluster centroids is even more robust than the feature recovery.

## 1 Introduction

Self-Supervised Learning (SSL) methods use unlabeled datasets to learn representations by solving an auxiliary task, thus bypassing time-consuming labelling efforts. Importantly, co-occurance-based SSL relies on positive data pairs (similar samples, e.g., an original sample and a transformed/augmented one) and negative data pairs (dissimilar samples, often randomly drawn from the dataset). Contrastive and non-contrastive learning, the two prominent families of SSL methods, utilize positives and negatives differently, though they are theoretically connected . Contrastive Learning (CL)  attracts positive pairs' and repels negative pairs' representations. Non-contrastive learning  only uses positive pairs, and avoids representation collapse with strategies such as momentum encoders or covariance regularization. Unfortunately, the many actively developed Self-Supervised Learning methods with such computational tricks potentially hinder selecting the best performing and simplest SSL method for a given task. Recently, Ibrahim et al.  proposed DIET, a SSL method that strips away unnecessary details by reducing the auxiliary task to a simple instance classification paradigm, and showed competitive performance on small datasets.

Identifiability theory, particularly Independent Component Analysis (ICA)  studies guarantees of probabilistic models to recover the ground-truth latent variables in a probabilistic latent variable model (LVM). Recent advances in nonlinear ICA theory proposed multiple self-supervised/weakly supervised models with identifiability guarantees , Locatello et al. , Morioka and Hyvarinen , Morioka et al. ,2021). Several papers study a contrastive scenario, (Hyvarinen and Morioka, 2016; Hyvarinen et al., 2019; Zimmermann et al., 2021; von Kugelgen et al., 2021; Rusak et al., 2024), providing a possible theoretical explanation for CL's practical success.

Our paper investigates whether DIET's competitive performance can be explained by identifiability theory. We model the data generating process (DGP) in a new, cluster-based way, and show that DIET's learned representation is linearly related to the ground truth representation. We also show how DIET's classification head recovers the cluster centroids, a connection to clustering that is absent from prior identifiability works for Self-Supervised Learning. Unlike other SSL solutions such as SimCLR (Chen et al., 2020), BYOL (Grill et al., 2020), BarlowTwins (Zbontar et al., 2021), or VICReg (Bardes et al., 2021), DIET's training objective applies to the same representation that is used post-training for solving downstream tasks. More precisely, no projector network is removed post-training. This implies that our theoretical guarantees directly apply to the SSL representation being used post-training, as opposed to other identifiability results in SSL (Zimmermann et al., 2021; von Kugelgen et al., 2021; Dauhhawer et al., 2023; Rusak et al., 2024). We corroborate our theoretical claims on synthetic data adhering to our assumptions--we even show that good performance is possible when the assumptions are violated. Notably, we observe that cluster centroids recovery from DIET's classification head is more robust than ground-truth representation prediction from the learned representation.

## 2 Identifiability guarantees for DIET

This section presents our main theoretical contribution. After summarizing DIET, we introduce a mildly constrained theoretical setup, in which DIET provably recovers the correct latents. The setup is followed by the main result and a discussion on the intuition for our theoretical model.

DIET (Ibrahim et al., 2024).DIET solves an instance classification problem, where each sample \(\) in the training dataset has a unique instance label \(i\). Augmentations do not affect this label. We have a composite model \(\), where the backbone \(\) produces \(d\)-dimensional representations, and a linear, bias-free classification head \(\) that maps these representations to a logit vector equal in size to the cardinality of the training dataset. If the parameter vector corresponding to logit \(i\) is denoted as \(_{i}\), then \(\) effectively computes similarity scores (scalar products) between the \(_{i}\)'s and embeddings \(()\). DIET trains this architecture to predict the correct instance label using multinomial regression (with \(,\) and temperature \(\) as variables):

\[(,,)=_{(,i)}-_{i},()}}{_{j}e^{_{ j},()}}. \]

Setup.For our theory, we need to formally define a latent variable model (LVM) for the data generating process (DGP) to assess the identifiability of latent factors. For this, we take a cluster-centric approach, representing semantic classes by cluster vectors, similar to proxy-based metric learning (Kirchhof et al., 2022). Then, we model the samples of a class with a von Mises-Fisher (vMF) distribution, centered around the class's cluster vector. This conditional distribution jointly models intra-class sample selection and _augmentations_ of samples, together called _intra-class variances_. Our conditional does not mean that each sample pair transforms into each other via augmentations _with high probability_. It does mean that--since we assume an LVM on the hypersphere; i.e., all

Figure 1: **DIET (Ibrahim et al., 2024) learns identifiable features**: DIET learns a linear \((N d)-\)dimensional classification head \(\) on top of a nonlinear encoder \(\) through an instance discrimination objective (1). For unit-normalized \((_{n})\), DIET maps samples and their augmentations close to the cluster vector \(_{c}\) corresponding to the class as if sampled from a von Mises-Fisher (vMF) distribution, centered around the cluster vector. In case of duplicate samples, i.e., matching class labels, the corresponding rows of \(\) will be the same, as shown for \(_{1}\) and \(_{i}\) with \(w_{1}=w_{i}\)

semantic concepts (color, position, etc.) correspond to a continuous latent factor--the latent manifold is connected, or equivalently, that the augmentation graph is connected, which is an assumption used in (Wang et al., 2022; Balestriero and LeCun, 2022; HaoChen et al., 2022). We provide an overview of our assumptions, and defer additional details to Sections3.1 in Appx. A:

**Assumptions 1** (DGP with vMF samples around cluster vectors. _Details omitted._).:

1. _There is a finite set of semantic classes_ \(\)_, represented by a set of unit-norm_ \(d\)_-dimensional cluster-vectors_ \(\{_{c}|c\}^{d-1}\)_. The system_ \(\{_{c}\}\) _is sufficiently large and spread out._
2. _Any sample_ \(i\) _belongs to exactly one class_ \(c=(i)\)_._
3. _The latent_ \(^{d-1}\) _of our data sample with instance label_ \(i\) _is drawn from a vMF distribution around the cluster vector_ \(_{c}\) _of class_ \(c=(i)\)_:_ \[ p(|c) e^{_{c},}.\] (2)
4. _Sample_ \(\) _is generated by passing latent_ \(\) _through an injective generator function:_ \(=()\)_._

Main result.Under Sections3.1, we prove the identifiability of both the latent representations and the cluster vectors, \(_{c}\), in all four combinations of unit-normalized (i.e., when the latent space is the hypersphere, commonly used, e.g., in InfoNCE (Chen et al., 2020)); and non-normalized (as in the original DIET paper (Ibrahim et al., 2024)) latents, \(\), and weight vectors, \(_{i}\). We state a concise version of our result and defer the full treatment and the proof to Thm.1 in Appx. A:

**Theorem 1** (Identifiability of latents drawn from vMF around cluster vectors. _Details omitted._).: _Let \((,,)\) globally minimize the DIET objective (1) under the following additional constraints:_

1. _the embeddings_ \(()\) _are unnormalized, while the_ \(_{i}\)_'s are unit-normalized. Then_ \(_{i}\) _identifies the cluster vector_ \(_{(i)}\) _up to an orthogonal linear transformation_ \(\)_:_ \(_{i}=_{(i)}\)_, for any_ \(i\)_. Furthermore, the inferred latents_ \(}=()\) _identify the ground-truth latents_ \(\) _up to the same orthogonal transformation, but scaled._
2. _neither the embeddings_ \(()\) _nor the_ \(_{i}\)_'s are unit-normalized. Then the cluster vectors_ \(_{c}\) _and the latent_ \(\) _are identified up to an affine linear and linear transformation, respectively._

_In all cases, the weight vectors belonging to samples of the same class are equal, i.e., for any \(i,j\), \((i)=(j)\) implies \(_{i}=_{j}\)._

Intuition.DIET assigns a different (instance) label and a unique weight vector \(_{i}\) to each training sample. The cross-entropy objective is optimized if the trained neural network can distinguish between the samples. Thus, the learned representation \(}=()\) should capture enough information to distinguish different samples, even from the same class.

However, the weight vectors \(_{i}\)'s cannot be sensitive to the intra-class sample variance or the sample's instance label \(i\) (because multiple instances will usually belong to the same class). This leads to the weight vectors taking the values of the cluster vectors. As cluster vectors only capture some statistics of the conditional, feature recovery is more fine-grained than cluster identifiability. The interaction between the two is dictated by the cross-entropy loss, which is minimized if the representation \(}\) is most similar to its own assigned weight vector \(_{i}\). Fig. 1 provides a visualization conveying the intuition behind Thm.1.

## 3 Experiments

In the following section, we empirically verify the claims made in Thm.1 in the synthetic setting. We generate data samples according to Sections3.1: ground-truth latents are sampled around cluster centroids \(_{c}\) following a vMF distribution. Data augmentations, which share the same instance label \(i\), are sampled from the same vMF distribution around \(_{c}\).

Synthetic Setup.We consider \(N\) data samples of dimensionality \(d\) generated from \( p(|_{c})\), sampled around a set of \(||\) class vectors, \(_{c}\) uniformly distributed across the unit hyper-sphere. We use an invertible multi-layer perceptron (MLP) to map ground truth latents to data samples. We train a classification head \(=[_{i}^{}|_{i=1}^{N}]\) and an MLP encoder that maps samples to representations \(}^{d}\) using the DIET objective (1). While to verify Thm.1 case C4., we do not normalize \(\), we do unit-normalize the weight vectors to validate Thm.1 case C3. We verify our theoretical claims by measuring the predictability of the ground-truth \(\) from \(}\) and \(_{c}\) from \(_{i}\) using the \(R^{2}\) score on a held-out dataset. For identifiability up to orthogonal linear transformations, we train linear mappingswith no intercept, assess the \(R^{2}\) score and verify that the singular values of this transformation converge to one, while for identifiability up to affine linear transformations, we simply assess the predictive accuracy of a linear predictor with intercept.

Results.Tab. 1 depicts our results for synthetic experiments. For both cases, when \(\) is and is not unit-normalized, the \(R^{2}\) score for both the latents and the cluster vectors is close to \(100\%\), except when the latent dimensionality is \(20\)--such scalability problems are a common artifact in SSL (Zimmermann et al., 2021; Rusak et al., 2024). For unit-normalized \(\), the MAE is close to zero even in such cases. For a higher concentration of samples around \(_{c}\) (i.e., \(\!=\!50\)) as well as a lower number of clusters (i.e., \(||\!=\!10\)), the \(R^{2}\) score decreases, which is also a common phenomenon, and is possibly explained by too strong augmentation overlap (Wang et al., 2022; Rusak et al., 2024). For a low number of clusters, high \(\) and a fixed number of training samples, the concentration of samples in regions surrounding centroids, \(v_{c}\), increases, a setting, refered to as "overly overlapping augmentations", known to be suboptimal and leading to a drop in downstream performance (Wang et al., 2022). Our results also suggest that even under model misspecification (last two rows with non-vMF latent distributions), identifiability still holds. We provide an additional ablation study for the concentration of \(_{c}\) across the unit hyper-sphere in Appx. B.

## 4 Discussion

Limitations.Our analysis proves the identifiability of DIET (Ibrahim et al., 2024) with a cluster-based DGP, thus providing the first such result for self-supervised parametric instance classification methods. However, our theory cannot yet explain the importance of label smoothing in DIET, noted by Ibrahim et al. (2024), and it also remains to be seen whether such identifiability results scale for larger datasets, for which the large-dimensional classifier head in DIET in the original form is prohibitive. It also remains an issue that the vMF conditional distribution around cluster centroids jointly models intra-class sample selection and augmentations of samples, as we suspect that the supports of augmentation spaces of different samples do not overlap as much as it would be suggested by the choice of conditional. Also, we leave it for future work to investigate a formal connection to nonlinear ICA methods such as InfoNCE (Zimmermann et al., 2021) or the Generalized Contrastive Learning framework (Hyvarinen et al., 2019).

    & & & & & & _{i}\) cases} & _{i}\)} \\  & & & & & & ^{2}()\)} & _{0}()\)} & }^{2}()\)} \\ \(N\) & \(d\) & \(||\) & \(p(|_{c})\) & M. & \(}\) & \(_{i}_{c}\) & \(}\) & \(_{i}_{c}\) & \(}\) & \(_{i}_{c}\) & \(}\) & \(_{i}_{c}\) \\   \(10^{3}\) & \(5\) & \(100\) & vMF(\(\!=\!10\)) & \(\) & \(98.6_{ 0.01}\) & \(99.9_{ 0.00}\) & \(0.01_{ 0.00}\) & \(0.00_{ 0.00}\) & \(99.0_{ 0.00}\) & \(99.9_{ 0.00}\) \\ \(10^{5}\) & \(5\) & \(100\) & vMF(\(\!=\!10\)) & \(\) & \(98.2_{ 0.01}\) & \(99.5_{ 0.00}\) & \(0.00_{ 0.00}\) & \(0.00_{ 0.00}\) & \(99.7_{ 0.00}\) & \(99.8_{ 0.00}\) \\  \(10^{3}\) & \(5\) & \(100\) & vMF(\(\!=\!10\)) & \(\) & \(98.6_{ 0.01}\) & \(99.9_{ 0.00}\) & \(0.01_{ 0.00}\) & \(0.00_{ 0.00}\) & \(99.0_{ 0.00}\) & \(99.9_{ 0.00}\) \\ \(10^{3}\) & \(10\) & \(100\) & vMF(\(\!=\!10\)) & \(\) & \(92.5_{ 0.01}\) & \(99.6_{ 0.00}\) & \(0.01_{ 0.00}\) & \(0.00_{ 0.00}\) & \(93.0_{ 0.03}\) & \(99.6_{ 0.00}\) \\ \(10^{3}\) & \(20\) & \(100\) & vMF(\(\!=\!10\)) & \(\) & \(70.8_{ 0.02}\) & \(97.1_{ 0.01}\) & \(0.03_{ 0.00}\) & \(0.00_{ 0.00}\) & \(81.9_{ 0.01}\) & \(99.7_{ 0.00}\) \\  \(10^{3}\) & \(5\) & \(10\) & vMF(\(\!=\!10\)) & \(\) & \(88.6_{ 0.05}\) & \(85.7_{ 0.15}\) & \(0.02_{ 0.00}\) & \(0.00_{ 0.00}\) & \(90.0_{ 0.05}\) & \(99.0_{ 0.03}\) \\ \(10^{3}\) & \(5\) & \(100\) & vMF(\(\!=\!10\)) & \(\) & \(98.6_{ 0.01}\) & \(99.9_{ 0.01}\) & \(0.01_{ 0.00}\) & \(0.00_{ 0.00}\) & \(99.0_{ 0.00}\) & \(99.9_{ 0.00}\) \\ \(10^{3}\) & \(5\) & \(1000\) & vMF(\(\!=\!10\)) & \(\) & \(99.3_{ 0.00}\) & \(99.9_{ 0.00}\) & \(0.00_{ 0.00}\) & \(0.00_{ 0.00}\) & \(99.2_{ 0.00}\) & \(99.9_{ 0.00}\) \\  \(10^{3}\) & \(5\) & \(100\) & vMF(\(\!=\!5\)) & \(\) & \(98.6_{ 0.01}\) & \(99.9_{ 0.01}\) & \(0.01_{ 0.00}\) & \(0.00_{ 0.00}\) & \(99.0_{ 0.00}\) & \(99.8_{ 0.00}\) \\ \(10^{3}\) & \(5\) & \(100\) & vMF(\(\!=\!10\)) & \(\) & \(99.0_{ 0.00}\) & \(99.9_{ 0.00}\) & \(0.00_{ 0.00}\) & \(0.00_{ 0.00}\) & \(99.1_{ 0.00}\) & \(99.9_{ 0.00}\) \\ \(10^{3}\) & \(5\) & \(100\) & vMF(\(\!=\!50\)) & \(\) & \(45.0_{ 0.06}\) & \(49.7_{ 0.06}\) & \(0.30_{ 0.00}\) & \(0.00_{ 0.00}\) & \(72.5_{ 0.03}\) & \(75.5_{ 0.00}\) \\  \(10^{3}\) & \(5\) & \(100\) & vMF(\(\!=\!10\)) & \(\) & \(98.6_{ 0.01}\) & \(99.9_{ 0.01}\) & \(0.01_{ 0.00}\) & \(0.00_{ 0.00}\) & \(99.0_{ 0.00}\) & \(99.9_{ 0.00}\) \\ \(10^{3}\) & \(5\) & \(100\) & Laplace (\(b\!=\!1.0\)) & \(\) & \(85.2_{ 0.01}\) & \(99.7_{ 0.01}\) & \(0.01_{ 0.00}\) & \(0.00_{ 0.00}\) & \(85.4_{ 0.00}\) & \(99.5_{ 0.00}\) \\ \(10^{3}\) & \(5\) & \(100\) & Normal (\(^{2}\!=\!1.0\)) & \(\) & \(98.7_{ 0.00}\) & \(99.8_{ 0.00}\) & \(0.01_{ 0.00}\) & \(0.00_{ 0.00}\) & \(98.6_{ 0.00}\) & \(99.6_{ 0.00}\) \\   

Table 1: Identifiability in the synthetic setup. Mean \(\) standard deviation across 5 random seeds. Settings that match and violate our theoretical assumptions are \(\) and \(\) respectively. We report the \(R^{2}\) score for linear mappings, \(}\) and \(_{i}_{c}\) for cases with normalized (o) and not normalized (a) \(_{i}\). For normalized \(_{i}\), we verify that mappings \(}\) are orthogonal by reporting the mean absolute error between their singular values and those of an orthogonal transformation.

Conclusion.By modeling the DGP in DIET (Ibrahim et al., 2024) with a cluster-based latent variable model, we provide identifiability results for both the latent representation and the cluster vectors, which is the first of its kind for self-supervised instance discrimination methods. We also showcase this in synthetic settings, where we recover both the latents and cluster vectors even under model misspecification. We hope that our work inspires further research into investigating the theoretical guarantees of simplified but effective SSL methods like DIET.

#### Acknowledgments

The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Patrik Reizinger and Attila Juhos. Patrik Reizinger acknowledges his membership in the European Laboratory for Learning and Intelligent Systems (ELLIS) PhD program. This work was supported by the German Federal Ministry of Education and Research (BMBF): Tubingen AI Center, FKZ: 01IS18039A. Wieland Brendel acknowledges financial support via an Emmy Noether Grant funded by the German Research Foundation (DFG) under grant no. BR 6382/1-1 and via the Open Philantropy Foundation funded by the Good Ventures Foundation. Wieland Brendel is a member of the Machine Learning Cluster of Excellence, EXC number 2064/1 - Project number 390727645. This research utilized compute resources at the Tubingen Machine Learning Cloud, DFG FKZ INST 37/1057-1 FUGG. Alice Bizeul's work is supported by an ETH AI Center Doctoral fellowship.