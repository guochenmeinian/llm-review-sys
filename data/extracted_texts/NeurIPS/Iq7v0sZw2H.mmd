# Debiasing Pretrained Generative Models by Uniformly Sampling Semantic Attributes

Walter Gerych\({}^{1}\), Kevin Hickey\({}^{1}\), Luke Buquicchio\({}^{1}\), Kavin Chandrasekaran\({}^{1}\), Abdulaziz Alajaji\({}^{2}\),

**Elke Rundensteiner\({}^{1}\), Emmanuel Agu\({}^{1}\)**

\({}^{1}\)Worcester Polytechnic Institute, Worcester, MA

\({}^{2}\)King Saud University, Riyadh, Saudi Arabia

Corresponding author: wgerych@wpi.edu

###### Abstract

Generative models are being increasingly used in science and industry applications. Unfortunately, they often perpetuate the biases present in their training sets, such as societal biases causing certain groups to be underrepresented in the data. For instance, image generators may overwhelmingly produce images of white people due to few non-white samples in their training data. It is imperative to debias generative models so they synthesize an equal number of instances for each group, while not requiring retraining of the model to avoid prohibitive expense. We thus propose a _distribution mapping module_ that produces samples from a _fair noise distribution_, such that the pretrained generative model produces _semantically uniform_ outputs - an equal number of instances for each group - when conditioned on these samples. This does _not_ involve retraining the generator, nor does it require _any_ real training data. Experiments on debiasing generators trained on popular real-world datasets show that our method outperforms state-of-the-art approaches.

## 1 Introduction

**Background.** Generative models have become a cornerstone of modern machine learning, allowing for the synthesis of realistic data for many domains, including images , audio , and text . However, even leading generative models often reproduce the biases present in their training data , such as image generation models strongly over-representing white males . As generative models are increasingly used for data augmentation to train downstream models  in domains from scientific to medical fields , biased synthesized data could lead to results that are skewed or inaccurate. This can exacerbate existing issues such as facial recognition models performing significantly worse on non-white individuals  or healthcare models being much less accurate for certain minority groups . Further, with the rapid growth of generative models in commercial applications, the potential financial, legal and ethical costs of biased outputs are significant. Thus, it is imperative to develop methods that mitigate bias in generative models to ensure that their outputs are fair and equitable by generating a roughly equal number of samples of each group. We call such outputs _semantically uniform_, and the attribute that they are uniform over - such as gender or race - the _semantic attribute_.

**State-of-the-art.** Existing methods for addressing bias in generative models often train a new model from scratch , though this is computationally expensive, requires significant labeled data, and wastes resources already previously spent training the existing (biased) generative model. Latent attribute editing methods modify the samples in the latent space of generative models to produce controlled changes in the output, and could potentially be used to correct for bias . However, this requires making limiting assumptions such as that semantic attributes correspond to _linear_ directionsin the latent space [17; 38; 6; 33; 39; 8]. While some recent advances have been made in uniformly sampling the output space of a pretrained model [15; 16], they can fail to yield uniform samples over _semantic attributes_. For example, they will make an image generator's output uniform over a manifold in the pixel space, but may still overproduce images of white individuals as the output won't be uniform over the _semantic attribute of race_.

**Problem statement.** Recently, numerous pretrained generative models and classifiers have been released for a variety of domains. For instance, there are publicly available generators for images of faces  followed by classifiers that predict attributes such as race and age of faces . Here, we assume access to a pretrained generative model \(G_{}\) that maps a noise space \(\) to a feature space \(\), and a pretrained classifier \(C_{}\) that maps \(\) to a discrete semantic attribute space \(\). Additionally, as error rates are often reported for released models, we assume that the class-conditional errors; i.e., the probability that an instance actually belongs to group \(j\) given that the classifier incorrectly predicted group \(i\), is known. Our goal is to model a distribution \(\) such that for \(\), \(G_{}()\) produces samples that are uniform over the attribute space \(\), using only predictions from \(C_{}\) to guide the output of \(G_{}\). For instance, if there are five discrete semantic groups in \(\), \(G_{}\) will produce an equal number of instances belonging to each group when conditioned on draws from \(\). We call \(\) a _fair noise distribution_; see Figure 1 for an example.

**Challenges.** Our task of producing samples that are semantically uniform has three major hurdles:

1. _Expensive retraining:_ It is often prohibitively expensive to retrain large generative models in terms of computational resources and time.
2. _Inaccessible training data:_ The data used to train a released generator is typically unavailable; either due to being proprietary or simply too large of a volume for most practitioners to utilize. Thus, we do not have adequate data available to tune the generator or classifier.
3. _Inaccurate classifier:_ As we do not have any samples of real data available, we must rely on the possibly imperfect semantic attribute classifier \(C_{}\) to provide labels. However, since classifiers may produce inaccurate predictions - especially on underrepresented groups of \(\), this can cause incorrect estimations of the number of instances for each group.

**Proposed approach.** We propose to train a distribution mapper network \(M_{}:\) that transforms draws from a standard noise distribution into draws from a fair noise distribution, such that once noise samples are transformed by \(M_{}\) they condition the generator to produce a roughly equal number of instances for each semantic group in \(\). We achieve this by first constructing a dataset of instances from \(\) that follow a fair noise distribution, which allows us to estimate the true distribution of the semantic attribute given an imperfect classifier. Using our proposed strategy, noise samples are collected such that the pretrained classifier's (corrected) distribution is uniform. Then, we use this fair noise dataset to train \(M_{}\) to inexpensively sample new instances from the fair noise distribution. The output of \(M_{}\) serves as input to the pretrained generator, which yields samples that are uniform over the space of the semantic attribute. Importantly, this approach does not require retraining the main generative model, thus incurring minimal costs. Our method works without requiring any real training data. Relying only on a pretrained generator and classifier pair, we circumvent the need to acquire any real data (labeled or otherwise) even using only an imperfect classifier.

**Contributions.** In this work, we:

Figure 1: A generator conditioned on a fair noise distribution yields outputs that are uniform over a semantic attribute (i.e. race).

* Show how to construct a fair noise dataset that produces semantically uniform synthetic outputs when passed through a generator, assuming an accurate classifier is available.
* Design a method to correct for the error incurred by an imperfect classifier, to construct a dataset of samples that come from a fair noise distribution.
* Propose an approach for training a distribution mapping network to sample from a fair noise distribution on-the-fly.
* Demonstrate the utility of our approach on a range of real-world datasets and released generators.

## 2 Problem Definition

Assume we are given a pretrained generative model \(G_{}\) that maps a latent space \(\) to a feature space \(\). In general, we assume that \(\) is of a lower dimensionality than \(\). Additionally, we assume we are given a pretrained classifier \(C_{}\) that maps \(\) to a semantic attribute space \(\), where \(\) is the set of _group_ (_class_) labels. Thus, \(=\{Y_{1},Y_{2},,Y_{N}\}\), where \(Y_{i}\) is the label of the \(i\)th group. When not otherwise ambiguous, we will refer to group \(Y_{i}\) as group \(i\). Let \(\) and \(}\) be the random variables indicating the true semantic attribute label and the predicted label from \(C_{}\) respectively. Let \(E\) be the prediction-conditional error rates of \(C_{}\), with \(E\) being a left stochastic matrix such that \(E_{i,j}=P(=i|}=j)\). Let \(C_{}\) be a better-than-random classifier, i.e., for \(N\) groups, \(P(=i|}=i)>\) for all groups \(i\). Thus, \(E\) is by definition a diagonally dominant matrix. As it is typical for developers to report the error rates of their classifiers, we assume that \(E\) is known. While these error rates are typically reported for the classifier's training distribution, we can correct \(E\) to apply to the generated distribution under a label shift assumption . See the Appendix for details on this and for the proofs of the upcoming theorems. Lastly, let \(C^{}\) denote an ideal _perfect_ classifier that always correctly predicts \(\) with zero error. \(C^{}\) is hypothetical and unavailable to us.

Our goal here is to sample from a Fair Noise Distribution, defined as follows:

**Definition 1** (Fair Noise Distribution).: _For a generative model \(G_{}:\), a distribution \(\) over \(\) is a **Fair Noise Distribution** with respect to \(\) if for \(\), \(C^{}(G_{}())()\), where \(()\) is the uniform distribution over the groups in \(\)._

Intuitively, a distribution \(\) is a Fair Noise Distribution if \(G_{}\) produces samples that are uniform over the semantic space \(\) when conditioned on \(\). For example, if \(G_{}\) synthesizes images of people and \(\) is the set of races, then \(\) is a Fair Noise Distribution if conditioning on it makes \(G_{}\) produce a roughly equal number of images of people from each race.

More realistically, we want to find a \(\) that is easy to sample from and produces reasonable variance from the generator. That is, it does not yield only one unique example of each group, which would otherwise make the problem trivial. Notably, we assume that we do _not_ have any real training data (i.e., no real samples from \(\)). Also, we do not aim to retrain \(G_{}\), i.e., not change the parameters \(\) of \(G_{}\). Additionally, we do not make any assumptions on the differentiability of \(G_{}\) or \(C_{}\).

## 3 Methodology

Our approach for sampling from a Fair Noise Distribution \(\) hinges on training a distribution mapping function \(M_{}\) such that \(M_{}() f\), where \(\) is a draw from the original conditioning distribution of \(G_{}\). The functional form of \(M_{}\) has many options; for instance, \(M_{}\) can be a GAN generator , a VAE , a DDPM , or a normalizing flow model [34; 7]. No matter which form is chosen, we will need a dataset of samples drawn from \(\) to train \(M_{}\).

### Collecting Fair Samples Using Imperfect Classifiers

A naive method for collecting a dataset of samples distributed according to a Fair Noise Distribution \(\) is given in Algorithm 1. The basic approach is to continuously sample \(\) from the noise distribution, collect the generator's output for each draw, and use the classifier to determine the value of the semantic attribute corresponding to each noise draw. As a result, samples of \(\) corresponding to each group are saved to dataset \(_{}\) until \(_{}\) has \(S\) number of samples for each \(y\). This procedure results in samples of noise that once passed through \(G_{}\) will be uniform across the semantic attribute space _according to \(C_{}\)_; i.e., for \(_{}\), \(C_{}(G_{}())()\). However, this will only yield samples from a Fair Noise Distribution in the case where \(C_{}\) is a perfect classifier (such that \(E\) is the identity matrix). If \(C_{}\) is an imperfect classifier then the distribution may not be truly fair. For instance, if \(C_{}\) often incorrectly predicts group \(j\) as group \(i\), then instances with attributes matching group \(i\) may be more prevalent than those for group \(j\).

Fortunately, we can utilize knowledge from the prediction-conditional error rates \(E\) to sample a dataset of noise that will yield more semantically uniform generated instances despite the noisy predictions of \(C_{}\). To achieve this, we use a weighted sample of datapoints predicted for each group. Let \(_{z|C_{}=i}\) be a distribution over \(\) such that the imperfect classifier's prediction of generated samples arising from this distribution are all of group \(i\); \(C_{}(G_{}())=i\) for \(_{z|C_{}=i}\). In addition, let \(^{}=_{i=1}^{||}_{i}_{z|C_{ }=i}\), such that \(_{i}>0\)\(\)\(i\) and \(_{i=1}^{||}_{i}=1\).

We can in many instances find values of \(=\{_{1},_{2},,_{||}\}\) such that \(^{}\) is a Fair Noise Distribution, as stated in Lemma 1:

**Lemma 1**.: _Let \(^{||}\) be the vector of length \(||\) such that every element is \(1\). Let \((E)=\{ a_{i}E_{.,i}|a_{i}_{ 0}\}\) be the finite convex cone generated by the columns of the prediction-conditional error matrix. If \(^{||}(E)\), then \(\)\(\) such that \(^{}\) is a Fair Noise Distribution._

Additionally, we can show that in the case where \(\) is binary and \(C_{}\) is better-than-random, we can _always_ find \(\) such that samples drawn from \(^{}\) will definitely yield generated samples that are uniform over the semantic attributes, as shown by the following lemma.

**Lemma 2**.: _Let \(||=2\) and let \(C_{}\) be better-than-random, such that the diagonal elements of \(E\) are each greater than \(\). Then, \(\)\(\) such that \(^{}\) is a Fair Noise Distribution._

The proof of Lemma 2 comes from showing that \( 1,1\) is always in the finite convex cone generated by \(C_{}\)'s prediction-conditional error matrix, with Lemma 1 implying that this property guarantees that \(^{}\) will yield samples that produce semantically uniform generated instances.

Even in the cases where we cannot guarantee that there exists an ideal \(\), we can still find values for \(\) that will yield a distribution that is _as close as possible_ to a uniform distribution over the semantic space, with the difference from uniformity measured by KL divergence. For this, let us define a Minimally-Unfair Noise Distribution.

**Definition 2** (Minimally-Unfair Noise Distribution).: _For a generative model \(G_{}:\), a distribution \(^{}=_{i=1}^{||}_{i}_{z|C_{ }=i}\), \(=\{_{1},_{2},,_{||}\}\), \(_{i}_{ 0}\), \(_{i=1}^{||}_{i}=1\), is a **Minimally-Unfair Noise Distribution** over \(\) with respect to \(\) if for \(^{}\), \(=*{argmin}_{}\{C^{}(G_{}( ))||()\}\)._Intuitively, \(^{}\) is a Minimally-Unfair Noise Distribution if the values of \(\) yield generated samples with minimal divergence from a semantically uniform distribution, under the constraint that \(^{}\) is a convex combination of the \(_{z|C_{}=i}\)'s. This constraint is required so that we can sample from \(^{}\) using a weighted sampling technique based off of our imperfect classifier \(C_{}\).

Fortunately, we can easily find the values of \(\) that will yield a Minimally-Unfair Noise Distribution. Before deriving the procedure for this, let us define the distribution \(^{}_{E}\) as the normalized weighted sum of the columns of \(E\), where each column \(i\) is weighted according to a corresponding \(_{i}\):

**Definition 3** (\(^{}_{E}\)).: _Define \(^{}_{E}=_{i=1}^{||}_{i}E_{.i}\) as a distribution over \(\) determined by prediction-conditional error matrix \(E\) and \(=\{_{1},_{2},,_{||}\}\), \(_{i}_{ 0}\), \(_{i=1}^{||}_{i}=1\)._

Next, we note that finding the values for \(\) that maximize the entropy of \(^{}_{E}\) is equivalent to finding the \(\) for which the divergence between \(^{}_{E}\) and \(()\) is minimized. This is stated formally in the following Proposition.

**Proposition 1**.: _If \(^{*}=*{argmax}_{}H(^{}_{E})\) where \(H\) is entropy, then for the same \(^{*}\) it is true that \(^{*}=*{argmin}_{}\{^{}_ {E}||()\}\)._

Before stating the theorem that directly implies a strategy for learning \(\) for which \(^{}\) is Minimally-Unfair, all that is left to do is to link \(^{}_{E}\) with the distribution of \(C^{}(G_{}())\):

**Proposition 2**.: _If \(^{}\), then it is true that \(C^{}(G_{}())^{}_{E}\)._

Proposition 2 states that for a given \(\) the distribution of the perfect classifier \(C^{}\) given samples of the generator conditioned on draws from \(^{}\) will be distributed according to \(^{}_{E}\). Now, we state Theorem 1 which directly implies our sampling strategy.

**Theorem 1**.: _If \(^{*}=*{argmax}_{}H(^{}_{E})\), then \(^{*}\) is a Minimally-Unfair Noise Distribution. When \(C_{}=C^{}\) or \(^{||}(E)\), then \(^{*}\) is also a Fair Noise Distribution._

Theorem 1 follows from the preceding Propositions. To see how Theorem 1 implies a strategy for sampling from a Minimally-Unfair Noise Distribution, recall that \(^{}\) is defined as the mixture of distributions of each \(_{z|C_{}=i}\) for \(i\) from \(1||\) with \(_{i}\) as the \(i\)th mixture weight, where \(_{z|C_{}=i}\) is the distribution of \(\)'s that yield generated samples that the noisy classifier predicts as group \(i\). The above theorem states that the mixture of these distributions with mixture weights \(\) that produce maximum entropy for \(^{}_{E}\) will yield a mixture distribution \(^{}\) that is worse-case Minimally-Unfair and, when possible, will be a perfect Fair Noise Distribution. Thus, to sample from \(^{}\) all that is needed is to 1) construct datasets \(_{z|C_{}=i}\) for \(i\) from \(1||\) such that if \(_{z}|_{C_{}=i}\) then \(C_{}(G_{}())=i\) (i.e., these datasets serve as proxies for \(_{z|C_{}=i}\)); 2) find \(\) such that \(H(^{}_{E})\) is maximized; and 3) perform a weighted sampling from each \(_{z|C_{}=i}\) proportional to its corresponding \(_{i}\). Note that the datasets \(_{z|C_{}=i}\) can be formed by an approach similar to Algorithm 1. The algorithm returns a dictionary with keys being the elements of \(\), and the values of each key returned from that procedure form the corresponding required datasets (i.e., \(_{z|C_{}=i}\)). A pseudo-code implementation of our approach for constructing a dataset of noise samples from a Minimally-Unfair Noise Distribution \(^{}\) is given in Algorithm 2.

### Training the Distribution Mapper

Let \(_{^{}}\) be a dataset of noise samples from the space \(\) distributed according to \(^{}\), such that \(_{^{}}\) is obtained as described in the previous subsection (i.e., \(_{^{}}\) comes from Algorithm 1 in the case where the error rates \(E\) are unknown or \(C_{}\) can be assumed to be an ideal classifier, or from Algorithm 2 otherwise). Now, we train a _distribution mapper_\(M_{}:\) such that if \(_{noise}\) then \(M_{}()^{}\), where \(_{noise}\) is an easy-to-sample-from noise distribution such as a multivariate Gaussian, or whatever noise distribution was used as input when training \(G_{}\). As previously stated, the functional form of \(M_{}\) and the procedure used to train it is flexible. There is a wide class of generative models and training strategies that could be employed . We chose to model \(M_{}\) as a GAN generator and find the parameters of \(M_{}\) using adversarial training. Thus, we train \(M_{}\) as such:

\[_{}_{}L(M_{},F_{})=_{ _{^{}}}F_{}() -_{_{noise}}1-F_{ }(M_{}()),\]

where \(F_{}\) is a discriminator network.

Note that \(\) is a multivariate vector, where the ordering of the elements of the vector are arbitrary (i.e., there is likely no spacial relationship in \(\)). Thus, the inductive bias of convolutional filters used in many leading GANs [20; 19; 32] is not appropriate in this case. Rather, we suggest to train the distribution mapper using a GAN architecture designed for tabular data; notably, CTGAN which has shown to perform well on data where a sequential or spacial inductive bias is not appropriate .

After training \(M_{}\) we can sample an arbitrary amount of samples from \(\) without applying Algorithm 1 or Algorithm 2, as \(M_{}()^{}\) for \(_{noise}\). Importantly, \(C^{}(G_{}(M_{}()))()\) when \(^{}\) is a Fair Noise Distribution, and will otherwise have minimum divergence from \(()\) under the constraints given in Definition 2. Notably, we achieve this with _no samples of real data, an imperfect classifier, and without fine-tuning \(G_{}\)_. Instead, the only training needed is for \(M_{}\), which in general should require much less complexity (and thus much less cost) than \(G_{}\). Figure 2 shows the Distribution Mapper paired with the pretrained generator \(G_{}\).

## 4 Experiments

We now experimentally evaluate our approach. Details such as hyperparameter choice and architectures are available in the appendix. Code for our method is available in the supplemental material.

We compare our approach on a range of generators: VAE , DCGAN , Progressive GAN , and a Latent Diffusion Model (LDM) .

### Compared Methods

_Latent Editing (2019)._ We apply the commonly used latent editing [17; 38; 6; 33; 39; 8] approach to the task of constructing a dataset with an equal number of instances from each group in \(\). While some work uses non-linear directions when editing , using linear directions has been shown to work as well in practice [17; 8]. Thus, for each group \(y\), we fit a linear classifier \(K_{y}\) on noise samples from \(_{noise}\), with the goal of separating noise instances that are mapped to group \(y\) from those mapped to all other groups. After training these classifiers, we can sample a noise instance belonging to a group with the following procedure: First, sample \(z_{noise}\); if \(K_{y}(z)=y\) then

Figure 2: Distribution Mapper \(M_{}\) turns a standard noise distribution into a Fair Noise Distribution.

return \(z\), otherwise perform latent editing on \(z\) until \(K_{y}(z_{new})=y\). The editing approach is given by \(z_{new}=z_{previous}+_{y}\), where \(_{y}\) is the normal vector to \(K_{y}\)'s decision boundary and \(\) is a step size. We set \(=0.1\) in our experiments.

_MaGNET (2022)._ This method was recently proposed to uniformly sample the manifold of pretrained generative models , using a sampling strategy that leverages the Jacobian of the generative model. The authors argue that while this does not guarantee an equal distribution for each group, _MaGNET_ should increase the frequency of disadvantaged groups by sampling more often from low-density regions of the generative manifold.

_Polarity Sampling (2022)._ An extension of _MaGNET_, _Polarity Sampling_ allows for more controlled sampling over the generative manifold . Sampling is controlled by a parameter \(\) where as \(\) goes to \(-\) modes are sampled from increasingly often, antimodes are sampled from more as \(\), and the original generative distribution is sampled from for \(=0\). We thus compare against two _Polarity Sampling_ settings; _Polarity Mode Sampling_ where \(=-2\), and _Polarity Antimode Sampling_ where \(=2\). By a similar argument used for _MaGNET_, sampling from antimodes may result in minority groups being more represented.

_Shifted Semantic Distribution (2021)._ This method provides a training-free approach for debiasing pretrained generative models . The key idea is to fit Gaussian mixture models on the latent space, where these mixtures are fit on regions that correspond to unique values of the semantic attribute. However, unlike our approach, Shifted Semantic Distribution assumes that each attribute class can be uniquely identified by a hyperplane in the noise space.

_Standard Generator._ We compare against unmodified pretrained generators used as initially intended. This is a baseline which other methods should outperform.

### Uniformly Sampling From Shapes Dataset

We first evaluate our approach on a dataset of synthetic images. Each image is of either a circle or a square, where the shape has a random size, color, and position in the image. We use the object's shape as the semantic attribute; thus, \(=\{\)'Circle','Square'\(\}\). This dataset was first used by Jing et al. . We utilize the VAE Jing et al. compared against in the same work as our generator.

**Testing Classifier Bias Correction Approach.** We demonstrate the utility of correcting for a biased classifier when constructing a dataset of Fair Noise samples. To that end, we compare the distribution of each group when we trust the biased classifier \(C_{}\) (Algorithm 1) to the distribution found when using our proposed biased correction approach (Algorithm 2). We use a linear classifier trained to distinguish images of squares from circles as our biased classifier \(C_{}\). This classifier is implemented using Scikit-learn's LinearSVC classifier . As a ground truth, we utilize a deep convolutional network classifier that achieves approximately perfect accuracy on the task of distinguishing squares from circles. Thus, this network acts as \(C^{}\).

Figure 3: a) Effects of applying our bias correction when constructing Fair Noise dataset; b) comparison of methods on the Shapes dataset.

Figure 3 a) shows the distribution of shapes in the Fair Noise Dataset with error correction (Algorithm 2) and without applying our bias correction approach (Algorithm 1). As \(||=2\) for this experiment, the best case is when the density of each group is \(0.5\). Clearly, our bias correction approach yields a dataset that is much closer to being uniform over the semantic attribute. This implies the need to correct for inaccurate classifiers when constructing Fair Noise Datasets.

**Comparative Study on Shapes Dataset.** We next test our proposed Distribution Mapping approach (Section 3.2) against the compared methods. For our approach, we train \(M_{}\) as a CTGAN  trained on the corrected Fair Noise Distribution obtained in the previous experiment. As the compared _Latent Editing_ method also requires a classifier as a ground truth for training each of its linear models \(K_{y}\), we compare against two versions of the method: 1) _Latent Editing (Biased Classifier)_ using the biased \(C_{}\) as a ground truth, and 2) _Latent Editing (Perfect Classifier)_ which uses the ground-truth, effectively perfect convolutional network discussed above. Note that from our problem definition, this perfect classifier would usually be unavailable to us.

As Figure 3 b) shows, our approach clearly results in the most uniform distribution out of all compared methods. Additionally, the _Latent Editing_ approach performs much worse when using the biased classifier. As expected, _Polarity Antimode Sampling_ does indeed increase the frequency of the minority class (_Circles_). Interestingly, _MaGNET_ also increases the frequency of the minority class, but is roughly as far from uniform as is the distribution of the original generator; _MaGNET_ flips the distribution and under-represents what was previously the majority class.

### Age Bias in Face Image Generator

In this next experiment, we evaluate our proposed method's ability to debias a generative model that produces images of people's faces. Specifically, the generative model is a DCGAN  that we pretrain on a grayscale version of the UTKFace dataset . For this experiment, the semantic attribute is Age. The age of each individual in UTKFace is given as a label. As our approach assumes the semantic attribute \(\) is discrete, we bin the ages in increments of 10 years such that \(=\{\}\). We train a deep convolutional network as the ground truth classifier (used for evaluation), and use a corresponding classifier with a quarter of the feature maps as the biased classifier.

**Evaluating Bias Correction Approach With Large Number of Classes.** We repeat the bias correction experiment we performed on the Shapes data again for the UTKFace Generator. While the previous experiment required only two classes, we now have ten classes (one for each age bin). Despite this increase, Figure 4 a) shows that performing the bias correction we propose in Algorithm 2 yields a much more uniform Fair Noise training dataset than results from trusting the biased classifier.

**Comparative Study on Reducing Age Bias.** We evaluate all methods on the UTKFace Generator, with the goal of making the output images uniform over Age (i.e., generate an equal number of images of people belonging to each age group). Figure 4 b) clearly shows that our approach (orange line) produces a much more uniform distribution over ages than the compared method, though it does generate images for the 10-19 bin too infrequently. While approaches such as _Polarity Antimode Sampling_ and _MaGNET_ are somewhat less biased away from generating images of young people, they fail to generate many samples for older individuals. Note that for the _Latent Editing_ approach the we fit a linear regressor (guided by the ground truth) on the latent space rather than a classifier, as age is more naturally a continuous attribute.

### Uniformly Sampling Over Race in Progressive GAN

We apply our approach to debias a pretrained Progressive GAN ; specifically, the PyTorch  version of the 'celebAHQ-256' model 2. We use the Race of the individual in each image as the semantic attribute, and use the MTCNN classifier from the DeepFace  package to classify race. Matching the classes available in DeepFace, our semantic attribute space is \(=\{. The Progressive GAN strongly favors generating white individuals, likely because its training images were of predominantly white celebrities. Here, we consider the classifier to be accurate.

**Comparative Study of Debiasing Progressive GAN.** For each method, we report the KL divergence between a uniform distribution and the classifier's output on the samples generated from the method in Figure 5. Note that unlike in the previous experiments, for this metric _lower is better_. Our approach has the lowest KL divergence, indicating it produces a more equal number of images of people from each race than any compared method. We note that despite this we still observe an over-representation of images of white individuals in the samples produced by our (and all other) approaches (see Appendix). This indicates that most regions of the latent space are likely associated with semantic attribute, and unless the distribution mapper very closely matches its Fair Noise training distribution, white persons will still be over-represented. Still, our approach results in a distribution that is most fair out of all compared methods. As we observed previously, sampling antimodes with _Polarity Sampling_ produces next-best results; likely because it explicitly draws from low-probability regions of the latent space, which correspond to non-white individuals.

### Uniformly Sampling Age in Latent Diffusion Model

Although our focus is primarily on models that map from low dimensional latent space to a higher dimensional feature space, we also evaluate our approach on a latent diffusion model 3 that was trained on the Celeba-HQ dataset . For this experiment we chose Age as the semantic attribute, and debias according to the following age categories: { \(\) 29, 30-49, \(\) 50 }. We utilize a pretrained ViT age classifier 4 to provide feedback for the semantic attribute. Since our approach requires each latent code to be mapped to a single output image, we utilized the DDIM  sampler for the diffusion model to yield a deterministic diffusion process. We used a DDIM diffusion model to train the Mapper network as well.

#### 4.5.1 Comparative Study on Debiasing Latent DDPM.

We report the KL divergence from uniform for the semantic distribution of each compared method in Table 1. Note that we exclude MaGNET and Polarity Sampling from this experiment, due to

Figure 4: a) Our biased correction approach yields better Fair Noise Distributions; b) comparison of methods on correcting for age bias in the UTKFace DCGAN.

Figure 5: KL Divergence between the distribution over the semantic space for the output of each method (_lower is better_).

the difficulty of obtaining the Jacobian determinant required by these methods. Results show that our approach yields significantly better scores than the other compared method, indicating that our approach can potentially be useful for debiasing Diffusion models as well.

## 5 Broader Impact And Limitations

The goal of this work, reconditioning generative models to not reproduce the biased distribution of their training set but rather produce one that treats each group equitably, is driven by the desire to mitigate the effects that systemic biases have on generative models increasingly used in real world applications. This has the potential for beneficial societal impact by counteracting the harmful effects of such systemic biases that lead these models to be unfair to underrepresented groups. However, the potential negative impacts of advancing generative modeling should not go unconsidered. Such models have already been used for some harmful applications such as Deep Fakes . It is important to stress that while our approach aims to mitigate the effects of bias in existing models, it is not a fix-all nor an excuse to train models on knowingly biased data. When possible, it is essential to collect fair and equitable training datasets, and to take measures to ensure that the models we train are fair _without_ the need for post-hoc corrections.

**Limitations.** Currently, our approach assumes that the semantic space \(\) is discrete. We plan to extend this work to handle continuous attributes, such as skin tone, in future work. Additionally, while the conditional distribution \(P(|)\) should be roughly equal before and after applying our method if the classifier \(C_{}\) is accurate, in the case of a biased classifier the conditional distribution may change. Correcting for potential distribution shift is likewise future work.

## 6 Conclusion

This is the first method to correct a pretrained biased generative model (i.e., one that strongly favors generating images of white people over all other races) given only the generator and a potentially-biased classifier. We propose a sampling strategy to construct a fair training set using the biased classifier in a way that corrects for its bias, and a Distribution Mapping module that uses this training set to learn how to sample noise instances that produce fair outputs when used as input to the generative model. Notably, we are able to debias the generative model without retraining the model or utilizing any real data. Our results indicate that our approach produces outputs that are much fairer than existing methods. This work may inspire more research on Distribution Mapping techniques to recondition generative models by transforming their standard latent distributions into distributions that yield more favorable behavior.

## 7 Acknowledgments

This material is based on research sponsored by DARPA under agreement number FA8750-18-2-0077. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA or the U.S. Government.

Results in this paper were obtained in part using a high-performance computing system acquired through NSF MRI grant DMS-1337943 to WPI. We thank the members of WPI's DAISY Research lab and the WPI WASH research group for their insightful feedback and support during the development of this work.

 Distribution Mapping & Latent & Vanilla \\ (Ours) & Editing & Latent Diffusion \\   & \)} &  &  \\  & & & \\ 

Table 1: Comparative study on debiasing the Age attribute for a pretrained LDM.