# DASpeech: Directed Acyclic Transformer for

Fast and High-quality Speech-to-Speech Translation

 Qingkai Fang\({}^{1,2}\), Yan Zhou\({}^{1,2}\), Yang Feng\({}^{1,2}\)

\({}^{1}\)Key Laboratory of Intelligent Information Processing

Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)

\({}^{2}\)University of Chinese Academy of Sciences, Beijing, China

{fangqingkai21b,zhou23z,fengyang}@ict.ac.cn

Corresponding author: Yang Feng.

###### Abstract

Direct speech-to-speech translation (S2ST) translates speech from one language into another using a single model. However, due to the presence of linguistic and acoustic diversity, the target speech follows a complex multimodal distribution, posing challenges to achieving both high-quality translations and fast decoding speeds for S2ST models. In this paper, we propose DASpeech, a non-autoregressive direct S2ST model which realizes both _fast_ and _high-quality_ S2ST. To better capture the complex distribution of the target speech, DASpeech adopts the two-pass architecture to decompose the generation process into two steps, where a linguistic decoder first generates the target text, and an acoustic decoder then generates the target speech based on the hidden states of the linguistic decoder. Specifically, we use the decoder of DA-Transformer as the linguistic decoder, and use FastSpeech 2 as the acoustic decoder. DA-Transformer models translations with a directed acyclic graph (DAG). To consider all potential paths in the DAG during training, we calculate the expected hidden states for each target token via dynamic programming, and feed them into the acoustic decoder to predict the target mel-spectrogram. During inference, we select the most probable path and take hidden states on that path as input to the acoustic decoder. Experiments on the CVSS Fr\(\)En benchmark demonstrate that DASpeech can achieve comparable or even better performance than the state-of-the-art S2ST model Translatoron 2, while preserving up to 18.53\(\) speedup compared to the autoregressive baseline. Compared with the previous non-autoregressive S2ST model, DASpeech does not rely on knowledge distillation and iterative decoding, achieving significant improvements in both translation quality and decoding speed. Furthermore, DASpeech shows the ability to preserve the speaker's voice of the source speech during translation.23

## 1 Introduction

Direct speech-to-speech translation (S2ST) directly translates speech of the source language into the target language, which can break the communication barriers between different language groups and has broad application prospects. Traditional S2ST usually consists of cascaded automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) models [1; 2]. In contrast, direct S2ST achieves source-to-target speech conversion with a unified model , which can (1) avoid error propagation across sub-models ; (2) reduce the decoding latency ; and (3) preservenon-linguistic information (e.g., the speaker's voice) during translation . Recent works show that direct S2ST can achieve comparable or even better performance than cascaded systems [7; 8].

Despite the theoretical advantages of direct S2ST, it is still very challenging to train a direct S2ST model in practice. Due to the linguistic diversity during translation, as well as the diverse acoustic variations (e.g., duration, pitch, energy, etc.), the target speech follows a complex multimodal distribution. To address this issue, Jia et al. , Inaguma et al.  propose the two-pass architecture, which first generates the target text with a linguistic decoder, and then uses an acoustic decoder to generate the target speech based on the hidden states of the linguistic decoder. The two-pass architecture decomposes the generation process into two steps: content translation and speech synthesis, making it easier to model the complex distribution of the target speech and achieving state-of-the-art performance among direct S2ST models.

Although the two-pass architecture achieves better translation quality, two passes of autoregressive decoding also incur high decoding latency. To reduce the decoding latency, Huang et al.  recently proposes non-autoregressive (NAR) S2ST that generates target speech in parallel. However, due to the conditional independence assumption of NAR models, it becomes more difficult to capture the multimodal distribution of the target speech compared with autoregressive models4. Therefore, the trade-off between translation quality and decoding speed of S2ST remains a pressing issue.

In this paper, we introduce an S2ST model with both high-quality translations and fast decoding speeds: DASpeech, a non-autoregressive two-pass direct S2ST model. Like previous two-pass models, DASpeech includes a speech encoder, a linguistic decoder, and an acoustic decoder. Specifically, the linguistic decoder uses the structure of DA-Transformer  decoder, which models translations via a directed acyclic graph (DAG). The acoustic decoder adopts the design of FastSpeech 2 , which takes the hidden states of the linguistic decoder as input and generates the target mel-spectrogram. During training, we consider all possible paths in the DAG by calculating the expected hidden state for each target token via dynamic programming, which are fed to the acoustic decoder to predict the target mel-spectrogram. During inference, we first find the most probable path in DAG and take hidden states on that path as input to the acoustic decoder. Due to the task decomposition of two-pass architecture, as well as the ability of DA-Transformer and FastSpeech 2 themselves to model linguistic diversity and acoustic diversity, DASpeech is able to capture the multimodal distribution of the target speech. Experiments on the CVSS Fr\(\)En benchmark show that: (1) DASpeech achieves comparable or even better performance than the state-of-the-art S2ST model Translatotron 2, while maintaining up to 18.53\(\) speedup compared to the autoregressive model. (2) Compared with the previous NAR S2ST model TranSpeech , DASpeech no longer relies on knowledge distillation and iterative decoding, achieving significant advantages in both translation quality and decoding speed. (3) When training on speech-to-speech translation pairs of the same speaker, DASpeech emerges with the ability to preserve the source speaker's voice during translation.

## 2 Background

### Directed Acyclic Transformer

Directed Acyclic Transformer (DA-Transformer) [11; 13] is proposed for non-autoregressive machine translation (NAT), which achieves comparable results to autoregressive Transformer  without relying on knowledge distillation. DA-Transformer consists of a Transformer encoder and an NAT decoder. The hidden states of the last decoder layer are organized as a directed acyclic graph (DAG). The hidden states correspond to vertices of the DAG, and there are unidirectional edges that connect vertices with small indices to those with large indices. DA-Transformer successfully alleviates the linguistic multi-modality problem since DAG can capture multiple translations simultaneously by assigning different translations to different paths in DAG.

Formally, given a source sequence \(X=(x_{1},...,x_{N})\) and a target sequence \(Y=(y_{1},...,y_{M})\), the encoder takes \(X\) as input and the decoder takes learnable positional embeddings \(=(_{1},...,_{L})\) as input. Here \(L\) is the graph size, which is set to \(\) times the source length, i.e., \(L= N\), and \(\) is a hyperparameter. DA-Transformer models the _translation probability_\(P_{}(Y|X)\) by marginalizing all possible paths in DAG:

\[P_{}(Y|X)=_{A}P_{}(Y|A,X)P_{}(A|X),\] (1)

where \(A=(a_{1},...,a_{M})\) is a path represented by a sequence of vertex indexes with \(1=a_{1}<<a_{M}=L\), and \(\) contains all paths with the same length as the target sequence \(Y\). The probability of path \(A\) is defined as:

\[P_{}(A|X)=_{i=1}^{M-1}P_{}(a_{i+1}|a_{i},X)=_{i=1}^{M-1} _{a_{i},a_{i+1}},\] (2)

where \(^{L L}\) is the _transition probability matrix_. We apply lower triangular masking on \(\) to allow only forward transitions. With the selected path \(A\), all target tokens are predicted in parallel:

\[P_{}(Y|A,X)=_{i=1}^{M}P_{}(y_{i}|a_{i},X)=_{i=1}^{M} _{a_{i},y_{i}},\] (3)

where \(^{L||}\) is the _prediction probability matrix_, and \(\) indicates the vocabulary. Finally, we train the DA-Transformer by minimizing the negative log-likelihood loss:

\[_{}=- P_{}(Y|X)=-_{A}P_{ }(Y|A,X)P_{}(A|X),\] (4)

which can be calculated with dynamic programming.

### FastSpeech 2

FastSpeech 2  is a non-autoregressive text-to-speech (TTS) model that generates mel-spectrograms from input phoneme sequences in parallel. It is composed of three stacked modules: encoder, variance adaptor, and mel-spectrogram decoder. The encoder and mel-spectrogram decoder consist of several feed-forward Transformer blocks, each containing a self-attention layer followed by a 1D-convolutional layer. The variance adaptor contains three variance predictors including duration predictor, pitch predictor, and energy predictor, which are used to reduce the information gap between input phoneme sequences and output mel-spectrograms. During training, the ground truth duration, pitch and energy are used to train these variance predictors and also as conditional inputs to generate the mel-spectrogram. During inference, we use the predicted values of these variance predictors. The introduction of variation information greatly alleviates the acoustic multi-modality problem, which leads to better voice quality. The training objective of FastSpeech 2 consists of four terms:

\[_{}=_{}+_{}+_{}+_{},\] (5)

where \(_{}\) measures the L1 distance between the predicted and ground truth mel-spectrograms, \(_{}\), \(_{}\) and \(_{}\) compute the mean square error (MSE) loss between predictions and ground truth for duration, pitch, and energy, respectively.

## 3 DASpeech

In this section, we introduce DASpeech, a non-autoregressive two-pass direct S2ST model that generates target phonemes and target mel-spectrograms successively. Formally, the source speech sequence is denoted as \(X=(x_{1},...,x_{N})\), where \(N\) is the number of frames in the source speech. The sequences of target phoneme and target mel-spectrogram are represented by \(Y=(y_{1},...,y_{M})\) and \(S=(s_{1},...,s_{T})\), respectively. DASpeech first generates \(Y\) from \(X\) with a speech-to-text translation (S2TT)3 DA-Transformer. Subsequently, it generates \(S\) with a FastSpeech 2-style decoder conditioned on the last-layer hidden states of the DA-Transformer. We first overview the model architecture of DASpeech in Section 3.1. In Section 3.2, we introduce our proposed training techniques that leverage pretrained S2TT DA-Transformer and FastSpeech 2 models and finetune the entire model for S2ST end-to-end. Finally, we present several decoding algorithms for DASpeech in Section 3.3.

### Model Architecture

As shown in Figure 1, DASpeech consists of three parts: a speech encoder, a non-autoregressive linguistic decoder, and a non-autoregressive acoustic decoder. Below are the details of each part.

**Speech Encoder** Since our model takes speech features as input, we replace the Transformer encoder in the original DA-Transformer with a speech encoder. The speech encoder contains a subsample followed by several Conformer blocks . Specifically, the subsample consists of two 1D-convolutional layers which shrink the length of input sequences by a factor of 4. Conformer combines multi-head attention modules and convolutional layers together to capture both global and local features. We use relative positional encoding  in the multi-head attention module.

**Non-autoregressive Linguistic Decoder** The linguistic decoder is identical to the decoder of DA-Transformer, which generates the target phoneme sequence from the source speech in parallel. Each decoder layer comprises a self-attention layer, a cross-attention layer, and a feed-forward layer. The decoder takes learnable positional embeddings as input, and the last-layer hidden states are organized as a DAG to model translations, as we described in Section 2.1.

**Non-autoregressive Acoustic Decoder** The acoustic decoder adopts the design of FastSpeech 2, which generates target mel-spectrograms from the last-layer hidden states of DA-Transformer in parallel. The model architecture is the same as that introduced in Section 2.2, except that the embedding matrix of the input phonemes is removed, since the input has changed from the phoneme sequence to the hidden state sequence.

### Training

DASpeech has the advantage of easily utilizing pretrained S2TT DA-Transformer and FastSpeech 2 models. We use the pretrained S2TT DA-Transformer to initialize the speech encoder and the non-autoregressive linguistic decoder, and use the pretrained FastSpeech 2 model to initialize the non-autoregressive acoustic decoder. Finally, the entire model is finetuned for direct S2ST. This pretraining-finetuning pipeline simplifies model training and enables the use of additional S2TT and TTS data. However, end-to-end finetuning presents a major challenge due to the length discrepancy between the output from the linguistic decoder and the input to the acoustic decoder. Specifically, the hidden state sequence output by the linguistic decoder has a length of \(L= N\), while the input sequence required by the acoustic decoder should have a length of \(M\), which is the length of the ground truth phoneme sequence. Therefore, it is necessary to determine how to obtain the input

Figure 1: Overview of DASpeech. The last-layer hidden states of the linguistic decoder are organized as a DAG. During training, the input to the acoustic decoder is the sequence of expected hidden states. During inference, it is the sequence of hidden states on the most probable path.

sequence of acoustic decoder \(=(_{1},...,_{M})\) from the last-layer hidden states of linguistic decoder \(=(_{1},...,_{L})\). The following introduces our proposed approach: _Expect-Path Training_.

**Expect-Path Training** Intuitively, the \(i\)-th input element \(_{i}\) should be the hidden state of the vertex responsible for generating \(y_{i}\). However, since there may be multiple vertices capable of generating each \(y_{i}\) due to numerous possible paths, we would like to consider all potential paths. To address this issue, we define \(_{i}\) as the expected hidden state under the posterior distribution \(P_{}(a_{i}|X,Y)\):

\[_{i}=_{j=1}^{L}P_{}(a_{i}=j|X,Y)_{j},\] (6)

where \(P_{}(a_{i}=j|X,Y)\) refers to the probability of vertex \(j\) being the \(i\)-th vertex on path \(A\), which means that \(y_{i}\) is generated by vertex \(j\). We can compute \(P_{}(a_{i}=j|X,Y)\) as follows:

\[P_{}(a_{i}=j|X,Y) =_{A}(a_{i}=j) P_{}(A|X,Y)\] (7) \[=_{A}(a_{i}=j)(Y,A| X)}{_{A^{}}P_{}(Y,A^{}|X)}\] (8) \[=(a_{i}=j) P_{}(Y,A |X)}{_{A}P_{}(Y,A|X)},\] (9)

where \((a_{i}=j)\) is an indicator function to indicate whether the \(i\)-th vertex of path \(A\) is vertex \(j\). To calculate \(_{A}(a_{i}=j) P_{}(Y,A|X)\) and \(_{A}P_{}(Y,A|X)\) in Equation (9), we employ the _forward-backward algorithm_, which involves two passes of dynamic programming.

_Forward Algorithm_ The _forward probability_ is defined as \(_{i}(j)=P_{}(y_{1},...,y_{i},a_{i}=j|X)\), which is the probability of generating the partial target sequence \((y_{1},...,y_{i})\) and ending in vertex \(j\) at the \(i\)-th step. By definition, we have \(_{1}(1)=_{1,y_{1}}\) and \(_{1}(1<j L)=0\). Due to the Markov property, we can sequentially calculate \(_{i}()\) from its previous step \(_{i-1}()\) as follows:

\[_{i}(j)=_{j,y_{i}}_{k=1}^{j-1}_{i-1}(k)_{k,j}.\] (10)

_Backward Algorithm_ The _backward probability_ is defined as \(_{i}(j)=P_{}(y_{i+1},...,y_{M}|a_{i}=j,X)\), which is the probability of starting from vertex \(j\) at the \(i\)-th step and generating the rest of the target sequence \((y_{i+1},...,y_{M})\). By definition, we have \(_{M}(L)=1\) and \(_{M}(1 j<L)=0\). Similar to the forward algorithm, we can sequentially calculate \(_{i}(j)\) from its next step \(_{i+1}(j)\) as follows:

\[_{i}(j)=_{k=j+1}^{L}_{j,k}_{i+1}(k)_{k,y_{i+1}}.\] (11)

Recalling Equation (9), the denominator is the sum of the probabilities of all valid paths, which is equal to \(_{M}(L)\). The numerator is the sum of the probabilities of all paths with \(a_{i}=j\), which is equal to \(_{i}(j)_{i}(j)\). Therefore, the Equation (6) can be calculated as:

\[_{i}=_{j=1}^{L}P_{}(a_{i}=j|X,Y)_{j}=_{ j=1}^{L}(j)_{i}(j)}{_{M}(L)}_{j}.\] (12)

The time complexity of the forward-backward algorithm is \((ML^{2})\). Finally, the training objective of DASpeech is as follows:

\[_{}=_{}+_{},\] (13)

where \(\) is the weight of TTS loss. The definitions of \(_{}\) and \(_{}\) are the same as those in Equations (4) and (5).

### Inference

During inference, we perform two-pass parallel decoding. First, we find the most probable path \(A^{*}\) in DAG with one of the decoding strategies proposed for DA-Transformer (see details below). We then feed the last-layer hidden states on path \(A^{*}\) to the non-autoregressive acoustic decoder to generate the mel-spectrogram. Finally, the predicted mel-spectrogram will be converted into waveform using a pretrained HiFi-GAN vocoder . Since both DAG and TTS decoding are fully parallel, DASpeech achieves significant improvements in decoding efficiency compared to previous two-pass models which rely on two passes of autoregressive decoding. Considering the trade-off between translation quality and decoding efficiency, we use the following two decoding strategies for DA-Transformer in our experiments: _Lookahead_ and _Joint-Viterbi_.

**Lookahead** Lookahead decoding sequentially chooses \(a_{i}\) and \(y_{i}\) in a greedy way. At each decoding step, it jointly considers the transition probability and the prediction probability:

\[a_{i}^{*},y_{i}^{*}=*{arg\,max}_{a_{i},y_{i}}P_{}(y_{i}|a_{ i},X)P_{}(a_{i}|a_{i-1},X).\] (14)

**Joint-Viterbi** Joint-Viterbi decoding  finds the global joint optimal solution of the translation and decoding path via Viterbi decoding :

\[A^{*},Y^{*}=*{arg\,max}_{A,Y}P_{}(Y,A|X).\] (15)

After Viterbi decoding, we first decide the target length \(M\) and obtain the optimal path by backtracking from \(a_{M}^{*}=L\). More details can be found in the original paper.

## 4 Experiments

### Experimental Setup

**Dataset** We conduct experiments on the CVSS dataset , a large-scale S2ST corpus containing speech-to-speech translation pairs from 21 languages to English. It is extended from the CoVoST 2  S2TT corpus by synthesizing the target text into speech with state-of-the-art TTS models. It includes two versions: CVSS-C and CVSS-T. For CVSS-C, all target speeches are in a single speaker's voice. For CVSS-T, the target speeches are in voices transferred from the corresponding source speeches. We evaluate the models on the CVSS-C French\(\)English (Fr\(\)En) and CVSS-T French\(\)English (Fr\(\)En) datasets. We also conduct a multilingual experiment by combining all 21 language directions in CVSS-C together to train a single many-to-English S2ST model.

**Pre-processing** We convert the source speech to 16000Hz and generate target speech with 22050Hz. We compute the 80-dimensional mel-filterbank features for the source speech, and transform the target waveform into mel-spectrograms following Ren et al. . We apply utterance-level and global-level cepstral mean-variance normalization for source speech and target speech, respectively. We follow Ren et al.  to extract the duration, pitch, and energy information of the target speech.

**Model Configurations** The speech encoder, linguistic decoder, and acoustic decoder contain 12 Conformer layers, 4 Transformer decoder layers, and 8 feed-forward Transformer blocks, respectively. The detailed configurations can be found in Table 5 in Appendix A. For model regularization, we set dropout to 0.1 and weight decay to 0.01, and no label smoothing is used. We use the HiFi-GAN vocoder pretrained on the VCTK dataset6 to convert the mel-spectrogram into waveform.

**Training** DASpeech follows the pretraining-finetuning pipeline. During pretraining, the speech encoder and the linguistic decoder are trained on the S2TT task for 100k updates with a batch of 320k audio frames. The learning rate warms up to 5e-4 within 10k steps. The acoustic decoder is pretrained on the TTS task for 100k updates with a batch size of 512. The learning rate warms up to 5e-4 within 4k steps. During finetuning, we train the entire model for 50k updates with a batch of 320k audio frames. The learning rate warms up to 1e-3 within 4k steps. We use Adam optimizer  for both pretraining and finetuning. For the weight of TTS loss \(\), we experiment with \(\{1.0,2.0,5.0,10.0\}\) and choose \(=5.0\) according to results on the dev set. We implement our model with the open-source toolkit _fairseq_. All models are trained on 4 RTX 3090 GPUs.

In the multilingual experiment, the presence of languages with limited data or substantial interlingual variations makes the mapping from source speech to target phonemes particularly challenging. To address this, we adopt a two-stage pretraining strategy. Initially, we pretrain the speech encoder and the linguistic decoder using the speech-to-subword task, followed by pretraining on the speech-to-phoneme task. In the second stage of pretraining, the embedding and output projection matrices of the decoder are replaced and trained from scratch to accommodate changes in the vocabulary. We employ this pretraining strategy for DASpeech, UnitY and Translatotron 2 in the multilingual experiment. We learn the subword vocabulary with a size of 6K using the SentencePiece toolkit.

We also adopt the glancing strategy  during training, which shows effectiveness in alleviating the multi-modality problem for NAT. It first assigns target tokens to appropriate vertices following the most probable path \(=_{A}P_{}(Y,A|X)\), and then masks some tokens. We linearly anneal the unmasking ratio \(\) from 0.5 to 0.1 during pretraining and fix \(\) to 0.1 during finetuning.

**Evaluation** During finetuning, we save checkpoints every 2000 steps and average the last 5 checkpoints for evaluation. We use the open-source ASR-BLEU toolkit7 to evaluate the translation quality. The translated speech is first transcribed into text using a pretrained ASR model. SacreBLEU  is then used to compute the BLEU score  and the statistical significance of translation results. The decoding speedup is measured on the test set using 1 RTX 3090 GPU with a batch size of 1.

**Baseline Systems** We implement the following baseline systems for comparison. More details about the model architectures and hyperparameters can be found in Appendix A.

* **S2UT** Speech-to-unit translation (S2UT) model generates discrete units corresponding to the target speech with a sequence-to-sequence model. We introduce the auxiliary task of predicting target phonemes to help the model converge.

    &  &  &  &  &  &  \\  & & & & & & **CVSS-C** & & \\   & Ground Truth & / & / & / & 84.52 & 81.48 & / \\   \\  A1 & S2UT  & Beam=10 & \(T_{}\) & 73M & 22.23 & 22.28 & 1.00\(\) \\ A2 & Translatotron  & Autoregressive & \(T_{}\) & 79M & 16.96 & 11.25 & 2.32\(\) \\   \\  B1 & UniY  & Beam=(10, 1) & \(T_{}+T_{}\) & 64M & 24.09 & 24.29 & 1.43\(\) \\ B2 & Translatotron 2  & Beam=10 & \(T_{}+T_{}\) & 87M & **25.21** & **24.39** & 1.42\(\) \\   \\  C1\(\) & TranSpeech  & Iteration & \(5\) & 67M & 17.24 & / & 11.04\(\) \\ C2\(\) & + b=15 + NPD\({}^{}\) & Iteration & \(15\) & 67M & 18.39 & / & 2.53\(\) \\  C3\(\) & TranSpeech  & Iteration & \(5\) & 67M & 16.38 & 16.49 & 12.45\(\) \\ C4\(\) & + b=15 + NPD\({}^{}\) & Iteration & \(15\) & 67M & 19.05 & 18.60 & 3.35\(\) \\   \\  D1 & **DASpeech** & Lookahead & \(1+1\) & 93M & 24.71\({}^{**}\) & 24.45\({}^{**}\) & **18.53\(\)** \\ D2 & (\(=0.5\)) & Joint-Viterbi & \(1+1\) & 93M & **25.03\({}^{**}\)** & **25.26\({}^{**}\)** & 16.29\(\) \\  D3 & **DASpeech** & Lookahead & \(1+1\) & 93M & 24.41\({}^{**}\) & 24.17\({}^{**}\) & 18.45\(\) \\ D4 & (\(=1.0\)) & Joint-Viterbi & \(1+1\) & 93M & 24.80\({}^{**}\) & 24.48\({}^{**}\) & 15.65\(\) \\   \\  E1 & S2T + FastSpeech 2 & Beam=10 & \(T_{}+1\) & 49M+41M & 24.71 & 24.49 & / \\  E2 & DAT + FastSpeech 2 & Lookahead & \(1+1\) & 51M+41M & 22.19 & 22.10 & / \\ E3 & (\(=0.5\)) & Joint-Viterbi & \(1+1\) & 51M+41M & 22.80 & 22.75 & / \\  E4 & DAT + FastSpeech 2 & Lookahead & \(1+1\) & 51M+41M & 22.68 & 22.57 & / \\ E5 & (\(=1.0\)) & Joint-Viterbi & \(1+1\) & 51M+41M & 23.20 & 23.15 & / \\   

Table 1: Results on CVSS-C Fr\(\)En and CVSS-T Fr\(\)En test sets. \(\) indicates results quoted from Huang et al. . \(\) indicates results of our re-implementation. \({}^{}\): target length beam=15 and noisy parallel decoding (NPD). \(T_{}\), \(T_{}\), and \(T_{}\) indicate the sequence length of phonemes, discrete units, and mel-spectrograms, respectively. \({}^{**}\) means the improvements over S2UT are statistically significant (\(p<0.01\)).

* **Translatotron** Translatotron generates the target mel-spectrogram with a sequence-to-sequence model. We also introduce the auxiliary task of predicting the target phonemes.
* **UnitY** UnitY is a two-pass model which generates target phonemes and discrete units successively8. We remove the R-Drop training  for simplification. We pretrain the speech encoder and first-pass decoder on the S2TT task. * **Translatotron 2** Translatotron 2 is a two-pass model which generates target phonemes and mel-spectrograms successively. We enhance Translatotron 2 by replacing LSTM with Transformer, and introducing an additional encoder between two decoders following Inaguma et al. . The speech encoder and first-pass decoder are pretrained on the S2TT task.
* **TransSpeech** TranSpeech is the first non-autoregressive S2ST model that generates target discrete units in parallel. To alleviate the acoustic multi-modality problem, TranSpeech introduces bilateral perturbation (BiP) to disentangle the acoustic variations from the discrete units. We re-implement TranSpeech following the configurations in the original paper.
* **S2T + FastSpeech 2** The cascaded system that combines an autoregressive S2TT model and FastSpeech 2. The S2T model contains 12 Conformer layers and 4 Transformer decoder layers, which is also used in UnitY and Translatotron 2 pretraining.
* **DAT + FastSpeech 2** The cascaded system that combines the S2TT DA-Transformer model and FastSpeech 2. Both models are used in DASpeech pretraining.

### Main Results

Table 1 summarizes the results on the CVSS-C Fr\(\)En and CVSS-T Fr\(\)En datasets. **(1)** Compared with previous autoregressive models, DASpeech (D1-D4) obviously surpasses single-pass models (A1, A2) and achieves comparable or even better performance than two-pass models (B1, B2), while preserving up to 18.53 times decoding speedup compared to S2UT. **(2)** Compared with the previous NAR model TranSpeech (C1-C4), DASpeech does not rely on knowledge distillation and iterative decoding, achieving significant advantages in both translation quality and decoding speedup. **(3)** DASpeech obviously outperforms the corresponding cascaded systems (D1-D4 vs. E2-E5), demonstrating the effectiveness of our expect-path training approach. We also find that the cascaded model prefers larger graph size (\(=1.0\) is better) while DASpeech prefers smaller graph size (\(=0.5\) is better). We think the reason is that a larger graph size can improve S2TT performance, but it also makes end-to-end training more challenging. We further study the effects of the graph size in Appendix C. **(4)** On the CVSS-T dataset, which includes target speeches from various speakers, we observe a performance degradation in Translatotron and Translatotron 2 as the target mel-spectrogram becomes more difficult to predict. In contrast, DASpeech still performs well since its acoustic decoder explicitly incorporates variation information to alleviate the acoustic multi-modality, demonstrating the potential of DASpeech in handling complex and diverse target speeches.

Table 2 shows the results on CVSS-C dataset of the multilingual X\(\)En S2ST model. We report the average ASR-BLEU scores on all languages, as well as the average scores on high/middle/low-resource languages9. We find that DASpeech still obviously outperforms S2UT but performs slightly worse than Translatotron 2 and UnitY in the multilingual setting, with an average gap of about 1.3 ASR-BLEU compared to Translatotron 2. However, DASpeech has about 13 times decoding speedup compared to Translatotron 2, achieving a better quality-speed trade-off.

  
**Models** & **Avg.** & **High** & **Mid** & **Low** \\  S2UT  & 5.15 & 16.74 & 6.24 & 0.84 \\ UnitY  & 8.15 & 24.97 & 9.78 & 1.86 \\ Translatotron 2  & **8.74** & **25.92** & **11.07** & **2.04** \\ 
**DASpeech** & + Lokokahead & 7.42 & 22.84 & 9.51 & 1.41 \\ (\(=0.5\)) & + Joint-Viterbi & 7.43 & 22.80 & 9.49 & 1.45 \\   

Table 2: ASR-BLEU scores on CVSS-C test sets of the multilingual X\(\)En S2ST model.

  
**Models** & **Best** & **Expect** & \(\) \\ 
**DASpeech** & + Lokokahead & 24.45 & 24.71 & +0.26 \\ (\(=0.5\)) & + Joint-Viterbi & 24.84 & 25.03 & +0.19 \\ 
**DASpeech** & + Lokahead & 24.18 & 24.41 & +0.23 \\ (\(=1.0\)) & + Joint-Viterbi & 24.46 & 24.80 & +0.34 \\   

Table 3: ASR-BLEU scores on the CVSS-C Fr\(\)En test set with best-path training and expect-path training.

### Alternative Training Approach: Best-Path Training

In addition to the expect-path training approach proposed in Section 3.2, we also experiment with a simpler approach: _Best-Path Training_. The core idea is to select the most probable path \(=_{A}\,P_{}(Y,A|X)\) via Viterbi algorithm , and take the hidden states on path \(=(_{1},...,_{M})\) as input to the acoustic decoder, i.e., \(_{i}=_{_{i}}\). As shown in Table 3, best-path training also performs well but is inferior to expect-path training. We attribute this to the fact that when using best-path training, only hidden states on the most probable path participate in TTS training, which may result in insufficient training for the remaining hidden states. In contrast, all hidden states participate in TTS training with our expect-path training, which achieves better performance. The time complexity of Viterbi algorithm used in the best-path training is also \((ML^{2})\). More details about the best-path training can be found in Appendix E.

### Analysis of Decoding Speed

In this section, we provide more detailed analysis of decoding speed. Figure 2 shows the translation latency of different models for speech inputs of different lengths. The results indicate that the translation latency of autoregressive models (S2UT, Translatotron, UnitY, and Translatotron 2) significantly increases with the length of the source speech. In contrast, the translation latency of non-autoregressive models (TranSpeech and DASpeech) are hardly affected by the source speech length. When translating longer speech inputs, DASpeech's decoding speed can reach more than 20 times that of S2UT. Furthermore, we illustrate the quality-speed trade-off of different models in Figure 3. By adjusting the hyperparameter \(\), the translation quality and decoding latency will change. Specifically, as \(\) increases, the decoding latency of the model will increase, and it achieves the best translation quality when \(=0.5\). It is evident that DASpeech achieves the best trade-off between translation quality and decoding latency among all models. We further study the speedup under batch decoding in Appendix D.

### Voice Preservation

In this section, we investigate the voice preservation ability of direct S2ST models on the CVSS-T Fr\(\)En dataset, where target speeches are in voices transferred from source speeches. Specifically, we use a pretrained speaker verification model10 to extract the speaker embedding of the source

  
**Models** &  **Speaker** \\ **Similarity** \\  \\  Ground Truth & 0.48 \\   \\  S2UT  & 0.03 \\ UniY  & 0.03 \\ TranSpeech  & 0.03 \\   \\  Translatotron  & 0.04 \\ Translatotron 2  & 0.05 \\   &  **+** Lokahead** \\ \((=0.5)\) \\  & 
 **0.14** \\ **+** Joint-Viterbi \\  & 0.10 \\   

Table 4: Average speaker similarity on CVSS-T Fr\(\)En test set.

Figure 3: Trade-off between translation quality and decoding speed. X-axis represents the speedup ratio relative to S2UT, and Y-axis represents ASR-BLEU. The upper right represents better trade-off.

Figure 2: Translation latency of different models across different source speech lengths, categorized into 4 groups based on source speech frame counts. The translation latency is computed as the decoding time on 1 RTX 3090 GPU.

speech and generated target speech. We define the cosine similarity between source and target speaker embeddings as _speaker similarity_, and report the average speaker similarity on the test set in Table 4. We find that: **(1)** unit-based S2ST model can not preserve the speaker's voice since discrete units contain little speaker information; and **(2)** DASpeech can better preserve the speaker's voice than Translatotron and Translatotron 2, since its acoustic decoder explicitly introduces variation information of the target speech, allowing the model to learn more complex target distribution.

## 5 Related Work

**Direct Speech-to-Speech Translation** Speech-to-speech translation (S2ST) extends speech-to-text translation [30; 31; 32; 33] which further synthesizes the target speech. Translatotron  is the first S2ST model that directly generates target mel-spectrograms from the source speech. Since continuous speech features contain a lot of variance information that makes training challenging, Tjandra et al. , Zhang et al.  use the discrete tokens derived from a VQ-VAE model  as the target. Lee et al. [5; 37] extend this research line by leveraging discrete units derived from the pretrained HuBERT model  as the target. To further reduce the learning difficulty, Inaguma et al. , Jia et al. , Chen et al.  introduce a two-pass architecture that generates target text and target speech successively. To address the data scarcity issue, some techniques like pretraining and data augmentation are used to enhance S2ST [8; 40; 41; 42; 43; 44]. Huang et al.  proposes the first non-autoregressive S2ST model which achieves faster decoding speed. Our DASpeech extends this line of research and achieves better translation quality and faster decoding speed.

**Non-autoregressive Machine Translation** Machine translation based on autoregressive decoding usually has a high decoding latency . Gu et al.  first proposes NAT for faster decoding speed. To alleviate the multi-modality problem in NAT, many approaches have been proposed  like knowledge distillation [47; 48; 49], latent-variable models [50; 51], learning latent alignments [52; 53; 54; 55; 56], sequence-level training [57; 58], and curriculum learning . Recently, Huang et al.  introduce DA-Transformer, which models different translations with DAG to alleviate the multi-modality problem, achieving competitive results with autoregressive models. Ma et al. , Gui et al.  further enhance DA-Transformer with fuzzy alignment and probabilistic context-free grammar.

**Non-autoregressive Text-to-Speech** Ren et al. , Peng et al.  first propose non-autoregressive TTS that generates mel-spectrograms in parallel. FastSpeech 2  explicitly models variance information to alleviate the issue of acoustic multi-modality. Many subsequent works enhance non-autoregressive TTS with more powerful generative models like variational auto-encoder (VAE) [63; 64], normalizing flows [65; 66; 67], and denoising diffusion probabilistic models (DDPM) [68; 69]. DASpeech adopts the design of FastSpeech 2 for training stability and good voice quality.

## 6 Conclusion

In this paper, we introduce DASpeech, a non-autoregressive two-pass direct S2ST model. DASpeech is built upon DA-Transformer and FastSpeech 2, and we propose an expect-path training approach to train the model end-to-end. DASpeech achieves comparable or even better performance than the state-of-the-art S2ST model Translatotron 2, while maintaining up to 18.53\(\) speedup compared to the autoregressive model. DASpeech also significantly outperforms previous non-autoregressive model in both translation quality and decoding speed. In the future, we will investigate how to enhance DASpeech using techniques like pretraining and data augmentation.

## 7 Limitations & Broader Impacts

**Limitations** Although DASpeech achieves impressive performance in both translation quality and decoding speed, it still has some limitations: (1) the translation quality of DASpeech still lags behind Translatotron 2 in the multilingual setting; (2) the training cost of DASpeech is higher than Translatotron 2 (96 vs. 18 GPU hours) since it requires dynamic programming during training; and (3) the outputs of DASpeech are not always reliable, especially for some low-resource languages.

**Broader Impacts** In our experiments, we find that DASpeech emerges with the ability to maintain the speaker identity during translation. It raises potential risks in terms of model misuse, such as mimicking a particular speaker or voice identification spoofing.