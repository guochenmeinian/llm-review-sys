# Deep Multi-Marginal Momentum Schrodinger Bridge

Tianrong Chen, Guan-horng Liu, Molei Tao, Evangelos A. Theodorou

Georgia Institute of Technology, USA

{tianrong.chen,ghliu, mtao, evangelos.theodorou}@gatech.edu

###### Abstract

It is a crucial challenge to reconstruct population dynamics using unlabeled samples from distributions at coarse time intervals. Recent approaches such as flow-based models or Schrodinger Bridge (SB) models have demonstrated appealing performance, yet the inferred sample trajectories either fail to account for the underlying stochasticity or are unnecessarily rigid. In this article, We extend the approach in  to operate in continuous space and propose Deep Momentum Multi-Marginal Schrodinger Bridge (DMSB), a novel computational framework that learns the smooth measure-valued spline for stochastic systems that satisfy position marginal constraints across time. By tailoring the celebrated Bregman Iteration and extending the Iteration Proportional Fitting to phase space, we manage to handle high-dimensional multi-marginal trajectory inference tasks efficiently. Our algorithm outperforms baselines significantly, as evidenced by experiments for synthetic datasets and a real-world single-cell RNA sequence dataset. Additionally, the proposed approach can reasonably reconstruct the evolution of velocity distribution, from position snapshots only, when there is a ground truth velocity that is nevertheless inaccessible.

## 1 Introduction

We consider the multi-marginal trajectory inference problem, which pertains to elucidating the dynamics and reactions of indiscernible individuals, given static snapshots of them taken at sporadic time points. Due to the inability of tracking each individual, one considers the evolution of the statistical distribution of the population instead. This problem received considerable attention, and associated applications appear in various scientific areas such as estimating cell dynamics [2; 3], predicting meteorological evolution , and medical healthcare statistics tracking . [6; 7] constructed an energy landscape that best aligned with empirical observations using neural network. [8; 9] learn regularized Neural ODE  to encode such potential landscape. Notably, in the aforementioned work, the trajectory of samples is represented in a deterministic way. In contrast, [11; 12] employ Schrodinger Bridge (SB) to determine the most likely evolution of samples between marginal distributions when individual sample trajectories are also affected by environmental stochasticity. Yet, these approaches scale poorly w.r.t. the state dimension due to specialized neural network architectures and computational frameworks.

SB can be viewed as a solution to the entropy-regularized optimal transport problem. SB seeks a nonlinear SDE that yields a straight path measure between two _arbitrary_ distributions. The straightness is implied by achieving optimality of minimizing transportation costs (i.e. 2-Wasserstein distance (\(W_{2}\))). We note SB is often related to Score-based Generated Model (SGM), both of which can be used for generative modeling by constructing certain Stochastic Differential Equation (SDE) that links data distribution and a tractable prior distribution (i.e. 2 marginals). SGM accomplishes the generative task by first diffusing data to prior through a pre-specified linear SDE, during which a neural network is also learned to approximate the score function. Then this score approximator is used to reverse this diffusion process, and consequently establish the generation. Critically-dampedLangevin Diffusion (CLD)  extends the SGM SDE to the phase space by introducing an auxiliary velocity variable with a tractable Gaussian distribution at both the initial and terminal time. The resulting trajectory in the position space becomes smoother, as stochasticity is only injected into the velocity space, and the empirical performance and sample efficiency are enhanced due to the structure of the critical damped SDE. The connection between SGM and SB has been elaborated in [14; 15] and scalable mean matching Iterative Proportional Fitting algorithm (IPF) is proposed to estimate SB efficiently in high dimensional cases. Applications of SB, such as image-to-image transformation [16; 17], RNA trajectory inference , solving Mean Field Game, Riemannian interpolation , demonstrate the effectiveness of SB in various domains.

In this work, we start with SB in phase space (termed momentum SB, mSB in short), and then further investigate mSB with multiple empirical marginal constraints present in the position space, which was formulated as multi-marginal mSB (mmmSB) in . This circumvents the need for expensive space discretization which does not scale well to high dimensions. We also address the challenge of intricate geometric averaging in continuous space setup by strategically partitioning and reorganizing the constraint sets. Furthermore, we enhance the algorithm's computational efficiency by incorporating the method of half-bridge IPF. The optimality of transportation cost in SB leads to straight trajectories, and if one solves N 2-marginal SB problems and connect the resulting trajectories to match N+1 marginals, the connected trajectories will have kinks at all connection points. On the contrary, in mmmSB, the optimality of transportation cost leads to a smooth measure-spline over the state space that also interpolates the empirical marginals. Therefore, this approach is highly suitable for problems originated from physical systems and/or those that should have smooth trajectories, such as trajectory inference in single-cell RNA sequencing. Our research will emphasize on solving mmmSB efficiently in high-dimensions (thus the approach will differ from that in the seminal work ; see Sec.4). The differences between our algorithm and prior work are demonstrated in Table.1, and the main contributions of our work are fourfold:

* We extend the mean matching IPF to phase space allowing for scalable mSB computing.
* We introduce and tailor the Bregman Iteration  for mmmSB which makes it compatible with the phase space mean matching objective, thus the efficient computation is activated for high dimensional mmmSB.
* We show how to overcome the challenge of sampling the velocity variable when it is not available in training data, which enhances the applicability of our model.
* We show the performance of proposed algorithm DMSB on toy datasets which contains intricate bifurcations and merge. On realistic high-dimension (100-D) single-cell RNA-seq (scRNA-seq) datasets, DMSB outperforms baselines by a significant margin in terms of the quality of the generated trajectory both visually and quantitatively. We show that DMSB is able to capture reasonable velocity distribution compared with ground truth while other baselines fail.

   Models & Optimality & \(p_{0}()\) & \(p_{1}()\) \\  SGM  & ✗ & \(p_{}(x)\) & \((,)\) \\ CLD  & ✗ & \(p_{}(x)(,)\) & \((,)(,)\) \\ SB  & \( W_{2}\) kinks & \(p_{}(x)\) & \(p_{}(x)\) \\ DMSB (ours) & \( W_{2}\) smooth & \(p_{}(x)q_{}(v|x)\) & \(p_{}(x)q_{}(v|x)\) \\   

Table 1: Comparison between different models in terms of optimality and boundary distributions \(p_{0}\) and \(p_{1}\). Our DMSB extends standard SB, which generalizes SGM beyond Gaussian priors, to phase space, similar to CLD. However, unlike CLD, DMSB jointly _learns_ the phase space distributions, i.e., \(p_{}(x,v)=p_{}(x)q_{}(v|x)\) and \(p_{}(x,v)=p_{}(x)q_{}(v|x)\). In other words, DMSB infers the underlying phase state dynamics given only state distributions.

Preliminary

### Dynamical Schrodinger Bridge problem

Dynamical Schrodinger Bridge problem has been extensively studied in the past few decades. The objective of the SB problem is to solve the following optimization problem:

\[_{(_{0},_{T})}D_{KL}(||),\] (1)

where \((_{0},_{T})\) belongs to a set of path measures with its marginal densities at \(t=0\) and \(T\) being \(_{0}\) and \(_{T}\). \(\) is the reference path measure (i.e.,  sets \(\) as Wiener process from \(_{0}\)). The optimality of the problem (1) is characterized by a set of PDEs (3).

**Theorem 2.1** ().: _The optimal path measure \(\) in the problem (1) is represented by forward and backward stochastic processes_

\[_{t} =[2\;_{}_{t}]t+_{t},_{0}_{0},\] (2a) \[_{t} =[-2\;_{}_{t}]t+ \;}_{t},_{T}_{T}.\] (2b)

_in which \(, C^{1,2}\) are the solutions to the following coupled PDEs,_

\[}{ t} =-_{t},_{t}}{ t }=_{t}\] (3) _s.t._ \[(0,)(0,)=_{0}(),\;(T, )(T,)=_{T}(),\]

The stochastic processes of SB in (2a) and (2b) are equivalent in the sense of \( t[0,T],p_{t}^{(2a)} p_{t}^{(2b)} p_{t}^{SB}\). Here \(p_{t}^{SB}\) stands for the marginal distribution of SB at time \(t\), which also represents the marginal density of stochastic process induced by either of Eq.2. The potentials \(_{t}\) and \(_{t}\) explicitly represent the solution of Fokker-Plank Equation (FPE) and Hamilton-Jacobi-Bellman equation (HJB) after exponential transform  where FPE describes the evolution of samples density and HJB represents for the optimality of Eq.1. Furthermore, the marginal density also obeys a factorization of \(p_{t}^{SB}=_{t}_{t}\). Such rich structures of SB will later on be used to construct the log-likelihood objective (Thm.B.1) and Langevin sampler for velocity (SS4.4).

To solve SB, prior work have primarily used the half-bridge optimization technique, also known as Iterative Proportional Fitting (IPF), in which one iteratively solves the optimization problem with one of the two boundary conditions [14; 15; 23],

\[^{(d+1)}:=*{arg\,min}_{(,_{1})}D_{KL}( ||^{(d)})^{(d+2)}:=*{arg\,min} _{(_{0},)}D_{KL}(||^{(d+1)})\] (4)

with initial path measure \(^{(0)}:=\). By repeatedly iterating over aforementioned optimizations until the algorithm converges, the SB solution will be attained as \(^{SB}_{d}^{(d)}\). In addition,  shows that the drift term in SB problem can also be interpreted as the solution Stochastic Optimal Control (SOC) problem by having optimal control policy \(^{*}=2\;_{}(t,_{t})\):

\[^{*}()*{arg\,min}_{ }[_{0}^{T}\|_{t}\|^{2} t] s.t_{t}= _{t}t+\,_{t}\\ _{0}_{0},_{1}_{T}.\]

This formulation will be used later on for constructing phase space likelihood objective function in SS3. Regarding solving the half-bridge problem, abundant results exist in the literature for the vanilla SB described above [14; 15; 23], but we will be solving a different SB problem; see Prop.4.1 for formulation and SS4 for a solution.

### Bregman Iterations for Multiple Constraints

Bregman iteration  can be viewed as a multiple marginal generalization of IPF, and it is widely used to solve entropy regularized optimal transport problem  with multiple constraints. The algorithm can efficiently solve problems in the form of,

\[_{}KL(|),\]where \(\) is the intersection of multiple closed convex constraint sets \(_{l}\): \(=_{l=1}^{L}_{l}\). Bregman Projection (BP) is defined as optimization w.r.t one of the constraint \(_{l}\),

\[P_{_{l}}^{KL}():=*{arg\,min}_{_{l} }KL(|),\]

and \(d\)-th Bregman Iteration (BI) is recursively computing BP over all the constraints in \(\):

\[ 0<n L,^{(d,n)}:=P_{_{l}^{T}}^{KL}(^{(d,n-1)}),\]

The initial condition for (\(d+1\))-th BI is \(^{(d+1,0)}=^{(d,L)}\). Under certain conditions (see e.g., ), one has that \(^{(d,L)}\) converges to the unique solution:

\[^{(d,L)} P_{}^{KL}() d+\]

**Remark 2.2**.: One BI traverses all constraints via multiple BPs, and each BP solves an optimization problem with one constraint.One can notice that the BI will become the aforementioned IPF procedure solving SB problem (1) by defining \(L=2\), \(_{1}=(_{0},)\), \(_{2}=(,_{1})\).

## 3 Momentum Schrodinger Bridge

We first describe how to conduct half-bridge IPF training in the phase space, which can be used to solve momentum SB (mSB) problem with two marginals constraints. This scalable phase space half-bridge technique will then be applied to multi-marginal cases (Sec.4). Fig.1 demonstrates how we develop an algorithm based on . Notations used in following sections are listed in Table.2. mSB extends SB problem to phase space, which consists of both position and velocity.

   Notation & Definition & Notation & Definition \\  \(\) & position variable & \(\) & position distribution \(()\) \\ \(\) & velocity variable & \(\) & velocity Distribution \(()\) \\ \(\) & concatenation of \([,]^{}\) & \(\) & distribution of \((,)\) \\   

Table 2: Mathematical notation.

Figure 1: A summary of various SB problems and corresponding algorithms. The toy example in the 3rd row illustrates that vanilla SB determines ‘straight’ paths (modulo fluctuations due to noise) between pairwise empirical marginals, while our multi-marginal momentum SB approach establishes a smooth measure-spline between marginals in the position space (albeit still stochastic, the path is smooth between any pair of adjacent 2 marginals, because noise is added to velocity, and the path is also smooth across different pairs of adjacent 2 marginals per design.

We will first consider boundary distributions that depend on both \(\) and \(\), although eventually we will use this as a module to find transport maps between two distributions that only depend on position \(\), as velocity \(\) is an auxiliary variable artificially introduced for obtaining smooth transport. Conceptually, as an entropy regularized optimal transport problem, SB tries to obtain the straightest path between empirical marginals of positions \(\) with additive noise, but mSB aims at finding the smooth interpolation between empirical marginals of \(\) conditioned on boundary velocity distributions (see Fig.1). Such smooth measure-valued splines in the position space are obtained by the optimization problem in the phase space :

\[_{(_{0},_{T})}KL(|) s.t=(,):_{t}}{ _{t}})}_{_{t}}= _{t})}_{$}}(,t)}t+\ }{\ g_{t}})}_{$}}(t)} _{t})}_{$}}(t)}t +\ }{\ g_{t}})}_{$}}(t)} _{t},\]

Similar to Theorem 2.1, one can derive a set of PDEs using the potential functions \((t,,)\) and \((t,,)\), and subsequently apply IPF procedure to solve the problem. The formulation of the phase space PDE can be found in Appendix.B.2. Such PDE representation of mSB results in a straightforward yet innovative log-likelihood training that enables efficient optimization of the IPF.

**Proposition 3.1** (likelihood bound).: _The half-bridge IPF in phase space_

\[^{(d+1)}:=*{arg\,min}_{(_{0},)}D_{KL}(|| ^{(d)})^{(d+2)}:=*{arg\,min}_{ (,_{T})}D_{KL}(||^{(d+1)})\]

_represents the bound of the likelihood and gives approximate likelihood training:_

\[_{t}:=*{arg\,min}_{_{t}}- p(_ {0},0)}_{t}:=*{arg \,min}_{}_{t}}- p(_{T},T).\]

\[ p(_{0},0)_{0}^{T}_{ }_{t}}[\|}_{t}+_{t}-g_{}_{t}\|^{2}]t.\]

\[\ }_{t}\ \ }_{t}=[-}_{t }]t+(t)_{t},} _{T}_{T}\] (5)

\(}_{t}(\\ }_{t})\) and \(_{t}\) is the density of path measure induced by eq.5 at time \(t\). A similar result for \((_{T},T)\) can be obtained in a similar derivation.

Proof.: See Appendix B.1. 

**Remark 3.2**.: After optimizing \(}_{t}\), the reference path measure becomes eq.5, which implies \((,_{T})\), i.e., the constraint in half-bridge IPF is satisfied. A path measure \(\) is induced by either \(_{t}\) or \(}_{t}\). As being mentioned in Remark.2.2. One half-bridge IPF is basically one BP and one IPF is one BI. Prop.3.1 provides a convenient way to perform one BP in the form of \(:=*{arg\,min}_{_{l}}D_{KL}(||)\) by maximizing log-likelihood given constraint \(\) and reference path measure \(\).

Prop.3.1 provides an alternative way to conduct the BI which will be heavily used in mmmSB SS3, and it is computationally efficient after parameterizing and discretization (SS4.4).

## 4 Deep Momentum Multi-Marginal Schrodinger Bridge

We first state the problem formulation of momentum multi-marginal Schrodinger Bridge (mmmSB). Different from previous two marginals case, we consider the scenario where \(N+1\) probability measures \(_{t_{i}}\) are lying at time \(t_{i}\). In addition, velocity distributions are not necessarily known.

**Proposition 4.1** ().: _The dynamical mmmSB with multiple marginal constraints reads:_

\[_{}():=_{i=0}^{N-1}KL(_{t_{i},t_{i+1}}|_{t_ {i},t_{i+1}}),:=_{i=0}^{N} _{t_{i}}\] (6)_where:_ \[_{t_{0}} =\{_{t_{0}:t_{1}}_{t_{1}}=_{t_{0} },_{t_{0}}_{t_{0}}=_{t_{0}}\}\] \[_{t_{N}} =\{_{t_{N-1}:t_{N}}_{t_{N-1}}= _{t_{N}},_{t_{N}}_{t_{N}}=_{t_{N}}\}\] \[_{t_{i}} =\{_{t_{i}:t_{i+1}}_{t_{i+1}}= _{t_{i}},_{t_{i-1}:t_{i}}_{t_{i-1}}=_{t_{i}}, _{t_{i}}_{t_{i}}=_{t_{i}}\},\] (7)

_and \(\) is the intersection of close convex set of \(_{t_{i}}\)._

The problem described in Prop.4.1 can be solved by classical BI algorithm integrated with Sinkhorn method . However, due to the curse of dimensionality and unfavorable geometric explicit solution, the BP cannot be applied in high-dimensional and continuous state space directly. To tackles these difficulties, we parameterize the forward and backward policies \(_{t}\) and \(}_{t}\) by a pair of neural networks. We further decouple and resemble the constraints by which it enables the scalable likelihood IPF and avoids the geometric averaging issue under mmmSB context.

### Decoupling and Reassembling Constraints

We decompose the constraint set (7) by

\[_{t_{i}}=_{r=0}^{2}_{t_{i}}^{r}, _{t_{i}}^{0}=\{_{t_{i}:t_{i +1}}_{t_{i+1}}=_{t_{i}},_{t_{i}} _{t_{i}}=_{t_{i}}\}\\ _{t_{i}}^{1}=\{_{t_{i-1}:t_{i}}_{t_ {i-1}}=_{t_{i}},_{t_{i}}_{t_{i}}=_{t_{i}} \}\\ _{t_{i}}^{2}=\{_{t_{i}:t_{i+1}}_{t_ {i+1}}=_{t_{i-1}:t_{i}}_{t_{i-1}}\}.\] (8)

One can notice that the \(_{t_{i}}^{0}\) and \(_{t_{i}}^{1}\) share similar structure as simpler boundary marginal conditions \(_{t_{0}}\) and \(_{t_{N}}\), hence we can get rid of the notorious geometric averaging (see SS4 in ). Notably, this type of constraint provides an opportunity to utilize Proposition 3.1 for optimization, but the joint distribution of \(\) and \(\) is still absent. We classify the constraints into two categories:

\[_{}=\{_{i=1}^{N-1}_{t_{i}}^{r} _{t_{0}}_{t_{N}}| r\{0,1\}\}, _{}=\{_{i=1}^{N-1}_{t_{i}}^{2} \}.\]

By following BI (SS2.2), we execute optimization w.r.t. (6) while projecting the solution to subset of \(_{}\) or \(_{}\) iteratively. The sketch can be found in Fig.2. The next sections will provide more details on obtaining the joint distribution \(\) and optimizing within each constraint set.

Hereafter, we only demonstrate the optimization for forward policy \(_{t}\) given reference path measure \(\) driven by fixed backward policy \(}_{t}\). The procedure can be applied for the \(}_{t}\) and vice versa.

### Optimization in set \(_{}\)

We first show how to optimize forward policy \(_{t}\) w.r.t. objective function (6) given the reference path measure \(\) driven by fixed backward policy \(}_{t}\) under one subset of \(_{}\).

**Proposition 4.2** (Optimality w.r.t. \(_{}\)).: _Given the reference path measure \(\) driven by the backward policy \(}_{t}\) from boundary \(_{t_{i+1}}\) in the reverse time direction, the optimal path measure in the forward time direction of the following problem_

\[_{}():=_{i=0}^{N-1}KL(_{t_{i}:t_{i+1}}|_{t_{i}:t_{i+1}}), s.t\{_{t_{i}:t_{i+1}} _{t_{i+1}}=_{t_{i}},_{t_{i}}_ {t_{i}}=_{t_{i}}\}\]

Figure 2: The procedure details the Bregman Iteration (BI) employed in DMSB. The gray and blue blocks represent the BP step performed under \(_{}\) constraint for forward and backward policies, respectively. The red block signifies the BP step executed under the \(_{}\) constraint. Algorithms for training and sampling can be found in Appendix.D.

\[:^{*}_{t_{i}:t_{i+1}}=}_{t_{i}:t_{i+1} }}{_{t_{i}:t_{i+1}}_{t_{i+1}}_{ t_{i}}}.\]

_When \(_{t_{i}:t_{i+1}}^{*}_{t_{i}:t_{i+1}}\), the following equations need to hold \( t[t_{i},t_{i+1}]\):_

\[\|_{t}+}_{t}-g_{} _{t}\|_{2}^{2}=0,\] (9a) \[p_{t_{i}}(_{t_{i}}|_{t_{i}})_{t _{i}}(_{t_{i}}|_{t_{i}}),\] (9b)

_where \(_{t}\) and \(_{t}\) denote the marginal density and conditional velocity distribution of the reference path measure at time \(t\), respectively._

Proof.: See appendix.B.5 

**Remark 4.3**.: When the ground truth distributions of velocity \(_{t_{i}}\) are available, one can simply sample from \(_{t_{i}}\) since the joint distribution \(_{t}\) is available in this case. In order to matching the reference path measure in KL divergence sense, one needs to match both the intermediate path measure eq.9a and the boundary condition eq.9b. In the traditional two-boundary SB case, matching the boundary condition is often disregarded due to either having a predefined data distribution or a tractable prior. However, in our specific case, as the velocity is not predefined, it becomes imperative to address this issue and optimize it through the application of Langevin dynamics.

### Optimization in set \(_{}\)

The formulation of optimization under \(_{}\) is similar to the previous section but differs by the boundary condition (eq.10b):

**Proposition 4.4** (Optimality w.r.t. \(_{}\)).: _Given the reference path measure \(\) driven by the backward policy \(}_{t}\) from boundary \(_{t_{N}}\) in the reverse time direction, the optimal path measure in the forward time direction of the following problem_

\[_{}():=_{i=0}^{N-1}KL(_{t_ {i}:t_{i+1}}|_{t_{i}:t_{i+1}}), _{}=\{_{i=1}^{N-1}_{t_{i}}^{2}\}\] \[^{*}_{t_{0}:t_{N}}=}_{t_ {0}:t_{N}}}{_{t_{0}:t_{N}}_{t_{N}} _{t_{0}}}.\]

_when \(_{t_{0}:t_{N}}^{*}_{t_{0}:t_{N}}\), the following equations need to hold \( t[t_{0},t_{N}]\):_

\[\|_{t}+}_{t}-g_{} _{t}\|_{2}^{2}=0\] (10a) \[p_{t_{0}}(_{t_{0}},_{t_{0}})_{t _{0}}(_{t_{0}},_{t_{0}})\] (10b)

Proof.: See appendix.B.6 

Conceptually, the above optimization objective with \(_{}\) constraint aims at finding a _continuous_ path measure close to reference path measure \(\) while any intermediate marginals constraints will not be considered. The boundary condition of reference path measure in the next iteration \(p_{t_{0}}(_{t_{0}},_{t_{0}})\) is determined by eq.10b. Fortunately, the empirical samples from this distribution are available, though the analytic representation of the distribution \(_{t_{0}}(_{t_{0}},_{t_{0}})\) is unknown. Hence we can utilize these samples as empirical sources from boundary distribution \(_{t_{0}}(_{t_{0}},_{t_{0}})\) for the next BP. For further explanation and intuition, one can find it in Appendix.G

### Parameterization and Training Objective Function

Inspired by the success of prior work , we parameterize path measure \(\) by forward policy \(_{t}^{}\) or backward policy \(}_{t}^{}\) combined with one of constraints in \(_{}\) or \(_{}\) (see Fig.8 in Appendix for visualization). We adopt Euler-Maruyama discretization and denote the timestep as \(_{t}\). Notably, eq.9b and eq.10b can be implied by minimizing phase space NLL in Prop.3.1. This leads to the following objective function, termed as phase space mean matching objective, which will be used to train neural networks that represent \(_{t}^{}\) and \(}_{t}^{}\) after time discretization:

\[_{MM}=[||_{t}_{t}^{} (_{t+_{t}})+_{t}}_{t+_{t}}^{ }(_{t+_{t}})-(_{t}+_{t}_{ t}^{}-_{t+_{t}})||^{2}].\]

The velocity boundary condition for the reference path measure in the succeeding BP is encoded in eq.9b or eq.10b, but the representation of conditional distribution eq.9b is not clear. We leverage the favorable property of SB to parameterize and sample from such distribution.

**Proposition 4.5** ([27; 28]).: _If \(^{}\) and \(^{}\) shares same path measure, then_

\[_{t_{i}}^{,}(_{t_{i}},_{t_{i}}) q_{ t_{i}}^{}(_{t_{i}},_{t_{i}}) q_{t_{i}}^{}( _{t_{i}}|_{t_{i}}),_{}_{t}^{,}=(_{t}^{}+}_{t}^{ })/g.\] (11)

Prop.4.5 suggests that one can use \(p_{t_{i}}(_{t_{i}}|_{t_{i}}):=_{t_{i}}^{,}\) to imply condition (9b) and obtain samples from such distribution by simulating Langevin dynamics. Namely, we first sample position from ground truth \(_{t_{i}}_{t_{i}}\), and then sample \(_{t_{i}}_{t}^{,}\) using eq.11. One can further adopt the same regularization  to enforce the condition of Prop.4.5.

### Training Scheme

Here we introduce the scheme to traverse BI (see Fig.2). In one BI, all constraints must be iterated once. For the sake of \(_{}\), the reference path measure should be induced by opposite direction. A single BI cannot be recursively repeated due to the conflict of reference path measure direction. For example (see Fig.2), at the end of \(d\)-th BI, \(\) is yielded by forward policy while the first BP of \(d\)-th BI is also optimizing forward policy which violates \(_{}\). Instead, we reschedule the optimization order. Specifically, in \((d+1)\)-th BI, we optimize backward policy at the first BP and the last BP.

## 5 Experiments

**Setups:** We test DMSB on 2D synthetic datasets and real-world scRNA-seq dataset . We choose state of the art algorithms MIOFlow  and NLSB  as our baselines. We tune both models to the best of our hardware capacity. We choose Sliced-Wasserstein Distance (SWD) and Maximum Mean Discrepancy (MMD) together with visualization as our criterion. The detailed setup of training and evaluation can be found in Appendix.C.

**Synthetic Datasets:** The Petal  and Gaussian Mixture Model (GMM) dataset are simple yet challenging, as they mimic natural dynamics arising in cellular differentiation, including bifurcations and merges. We compare our algorithm with MIOFlow in Fig.3. DMSB can infer trajectories aligned with ground truth distribution more faithfully at timesteps when snapshots are taken. In GMM experiments (see Fig.4), we choose standard Gaussian at initial and terminal time steps while four-modal GMM and eight-modal GMM are placed at intermediate time steps. Besides good position trajectory, it is almost serendipity that DMSB can also learn the reasonable velocity trajectory _without_ any access to ground truth velocity information. This paves the way for our later velocity estimation for the RNAsc dataset. **scRNA-seq Dataset:** The emergence of single-cell profiling technologies has facilitated the acquisition of high-resolution single-cell data, enabling the characterization of individual cells at distinct developmental states . However, because the cell population is eliminated after the measurement, one may only gather statistical data for single samples at particular timesteps, which neither preserves any correlations over time nor provides access to the ground truth trajectory. The diversity of embryonic stem cells after development from embryoid bodies, which comprises mesoderm, endoderm,

Figure 4: Validation of our DMSB model on complex GMM synthetic dataset. The velocity and position of the same sample correspond to the same shade level. _Upper_: Samples’ evolution in the position space. _Bottom_: Learnt samples’ evolution in the velocity space.

Figure 3: Comparsion with MIOFlow and ground truth on challenging petal dataset. DMSB is able to generate trajectories whose time marginal matches ground truth faithfully and outperforms prior work. Time is indicated by colors.

neuroectoderm, and neural crest in 27 days, is demonstrated by the scRNA-seq dataset. The snapshot of cells are collected between (\(t_{0}\): day 0 to 3, \(t_{1}\): day 6 to 9, \(t_{2}\): day 12 to 15, \(t_{3}\): day 18 to 21,\(t_{4}\): day 24 to 27). Snapshot data are prepossessed by the quality control  and then projected to feature space by principal component analysis (PCA). We inherit processed data from . We validate DMSB on 5-dim and 100-dim PCA space to show superior performance on high-dimension problems compared with baselines. We further show that DMSB can estimate better velocity distribution compared with baselines when the ground truth is absent during training and testing.

We testify the performance of our model by computing MMD and SWD with full snapshots and when one of snapshots is left out (LO). We postpone the comparison of all the models on 5-d RNA space to the appendix (see Fig.9 and Table.6) because the problem is relatively simple and all models can infer accurate trajectory. Table.3 summarizes the average MMD and SWD between estimated marginal and ground truth over different snapshot timesteps. DMSB outperforms prior work by a large margin in high (100) dimensional scenarios. The visualization (Fig.5) in PCA space further justifies the numerical result and highlights the variety and quality of the samples produced by DMSB.

Interestingly, Fig.4 demonstrates that DMSB can reconstruct reasonable evolution of the velocity distribution which was not accessible to the algorithm. We further validate such property in 100-D RNAsc dataset. During the training and testing, all the models do not have access to the ground truth velocity. We run the experiments of 100-D and 5-D RNAsc datasets and average the discrepancy between ground truth velocity and estimated velocity over snapshot time. The numerical values are listed in the Table.7 and Table.6. The plot of velocity and position can be found in Fig.9 and Fig.10. The plot illustrates that while all models are capable of learning reasonable trajectories, only DMSB has the ability to estimate a plausible velocity distribution. This property holds even for

Figure 5: Comparison of population-level dynamics on 100-dimensional PCA space at the moment of observation for scRNA-seq data using MIOFlow, NLSB, and DMSB. We display the plot of the first 6 principle components (PC). Baselines can only learn the trajectory’s fundamental trend, whereas DMSB can match the target marginal along the trajectory across different dimensions. The right figure shows Kernel Density Estimation  of samples generated by DMSB and ground truth at \(t_{3}\) and \(t_{4}\). The generated samples for all timesteps and comparison with baseline are in Appendix.F.

    & &  &  \\  Algorithm & w/o LO & LO-\(t_{1}\) & LO-\(t_{2}\) & LO-\(t_{3}\) & w/o LO & LO-\(t_{1}\) & LO-\(t_{2}\) & LO-\(t_{3}\) \\  NLSB & 0.66 & 0.38 & 0.37 & 0.37 & 0.54 & 0.55 & 0.54 & 0.55 \\ MIOFlow & 0.23 & 0.23 & 0.90 & 0.23 & 0.35 & 0.49 & 0.72 & 0.50 \\ DMSB(ours) & **0.03** & **0.04** & **0.04** & **0.20** & **0.20** & **0.19** & **0.18** \\   

Table 3: Numerical result of MMD and SWD on 100 dimensions single-cell RNA-seq dataset and results for leaving out (LO) marginals at different observation. DMSB outperforms prior work by a large margin for both metrics and all leave-out case. See Appendix.4 for Results over 3 seeds.

100-D RNA dataset (see Fig.5,11,12). This is notable, despite the velocity estimated by DMSB does not perfectly match the ground truth, because it should be noted that the proposed phase space SDE and the optimality of OT are artificial and may not necessarily represent the actual RNA evolution. Moreover, as individual evolutions cannot be tracked, possibilities such as {A\(\)A, B\(\)B} versus {A\(\)B, B\(\)A} can not be discerned, which renders exact velocity recovering almost impossible.

## 6 Conclusion and Limitations

In this paper, we propose DMSB, a scalable algorithm that learns the trajectory which fits the different marginal distributions over time. We extend the mean matching objective to phase space which enables efficient mSB computing. We propose a novel training scheme to fit the mean matching objective without violating BI which is the root of solving mmmSB problem. We demonstrate the superior result of DMSB compared with the existing algorithms.

A main limitation of this work is, the rate of convergence to the actual mmmSB has not been quantified after neural network approximations are introduced. Even though  theoretically analyzed the convergence of mean matching iteration, supporting its outstanding performance , the iteration still fails to converge to the actual SB  precisely due to practical neural network estimation errors accumulating over BI. However, recent work  shows the convergence of SB when training error exists. In addition, DMSB cannot simulate the process with death and birth of cells which can be potentially described as unbalanced optimal transport .

## 7 Acknowledgement

This research was supported by the ARO Award W911NF2010151, and the DoD Basic Research Office AwardHQ00342110002.