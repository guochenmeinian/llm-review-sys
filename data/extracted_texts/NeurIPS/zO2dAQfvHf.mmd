# Stabilized Neural Differential Equations for Learning Dynamics with Explicit Constraints

Alistair White

Technical University of Munich

Potsdam Institute for Climate Impact Research

&Niki Kilbertus

Technical University of Munich

Hemholtz AI, Munich

&Maximilian Gelbrecht

Technical University of Munich

Potsdam Institute for Climate Impact Research

&Niklas Boers

Technical University of Munich

Potsdam Institute for Climate Impact Research

University of Exeter

{alistair.white, niki.kilbertus, maximilian.gelbrecht, n.boers}@tum.de

###### Abstract

Many successful methods to learn dynamical systems from data have recently been introduced. However, ensuring that the inferred dynamics preserve known constraints, such as conservation laws or restrictions on the allowed system states, remains challenging. We propose _stabilized neural differential equations_ (SNDEs), a method to enforce arbitrary manifold constraints for neural differential equations. Our approach is based on a stabilization term that, when added to the original dynamics, renders the constraint manifold provably asymptotically stable. Due to its simplicity, our method is compatible with all common neural differential equation (NDE) models and broadly applicable. In extensive empirical evaluations, we demonstrate that SNDEs outperform existing methods while broadening the types of constraints that can be incorporated into NDE training.

## 1 Introduction

Advances in machine learning have recently spurred hopes of displacing or at least enhancing the process of scientific discovery by inferring natural laws directly from observational data. In particular, there has been a surge of interest in data-driven methods for learning dynamical laws in the form of differential equations directly from data . Assuming there is a ground truth system with dynamics governed by an ordinary differential equation

\[=f(u(t),t)$)}\] (1)

with \(u:^{n}\) and \(f:^{n}^{n}\), the question is whether we can learn \(f\) from (potentially noisy and irregularly sampled) observations \((t_{i},u(t_{i}))_{i=1}^{N}\).

Neural ordinary differential equations (NODEs) provide a prominent and successful method for this task, which leverages machine learning by directly parameterizing the vector field \(f\) of the ODE as aneural network  (see also Kidger  for an overview). A related approach, termed universal differential equations (UDEs) , combines mechanistic or process-based model components with universal function approximators, typically also neural networks. In this paper, we will refer to these methods collectively as _neural differential equations_ (NDEs), meaning any ordinary differential equation model in explicit form, where the right-hand side is either partially or entirely parameterized by a neural network. Due to the use of flexible neural networks, NDEs have certain universal approximation properties [72; 75], which are often interpreted as "in principle an NDE can learn any vector field \(f\)" . While this can be a desirable property in terms of applicability, in typical settings one often has prior knowledge about the dynamical system that should be incorporated.

As in other areas of machine learning - particularly deep learning - inductive biases can substantially aid generalization, learning speed, and stability, especially in the low data regime. Learning dynamics from data is no exception . In scientific applications, physical priors are often not only a natural source of inductive biases, but can even impose hard constraints on the allowed dynamics. For instance, when observing mechanical systems, a popular approach is to directly parameterize either the Lagrangian or Hamiltonian via a neural network [39; 58; 24]. Constraints such as energy conservation can then be "baked into the model", in the sense that the parameterization of the vector field is designed to only represent functions that satisfy the constraints. Finzi et al.  build upon these works and demonstrate how to impose explicit constraints in the Hamiltonian and Lagrangian settings.

In this work, we propose a stabilization technique to enforce arbitrary, even time-dependent manifold constraints for any class of NDEs, not limited to second order mechanical systems and not requiring observations in particular (canonical) coordinates. Our method is compatible with all common differential equation solvers as well as adjoint sensitivity methods. All code is publicly available at https://github.com/white-alistair/Stabilized-Neural-Differential-Equations.

## 2 Background

A first order neural differential equation is typically given as

\[=f_{}(u(t),t) u(0)=u_{0},\] (2)

where \(u:^{n}\) and the vector field \(f_{}^{n}^{n}\) is at least partially parameterized by a neural network with parameters \(^{d}\). We restrict our attention to ground truth dynamics \(f\) in Equation (1) that are continuous in \(t\) and Lipschitz continuous in \(u\) such that the existence of a unique (local) solution to the initial value problem is guaranteed by the Picard-Lindelof theorem. As universal function approximators [44; 25], neural networks \(f_{}\) can in principle approximate any such \(f\) to arbitrary precision, that is to say, the problem of learning \(f\) is realizable.

In practice, the parameters \(\) are typically optimized via stochastic gradient descent by integrating a trajectory \((t)=(u_{0},f_{},t)\) and taking gradients of the loss \((u,)\) with respect to \(\). Gradients of trajectories can be computed using adjoint sensitivity analysis (_optimize-then-discretize_) or automatic differentiation of the solver operations (_discretize-then-optimize_) [18; 49]. While these approaches have different (dis)advantages [59; 49], we use the adjoint sensitivity method due to reportedly improved stability . We use the standard squared error loss \((u,)=\|u-\|_{2}^{2}\) throughout.

Figure 1: Sketch of the basic idea behind stabilized neural differential equations (SNDEs). **(a)** An idealized, unstabilized NDE vector field (blue arrows). **(b)** A constraint manifold (black circle) and the corresponding stabilization of the vector field (pink arrows). **(c)** The overall, stabilized vector field of the SNDE (orange arrows) and a stabilized trajectory (green). The stabilization pushes any trajectory starting away from (but near) the manifold to converge to it at a rate \(\) (see Section 4).

We focus on NODEs as they can handle arbitrary nonlinear vector fields \(f\) and outperform traditional ODE parameter estimation techniques. In particular, they do not require a pre-specified parameterization of \(f\) in terms of a small set of semantically meaningful parameters. The original NODE model  has quickly been extended to augmented neural ODEs (ANODEs), with improved universal approximation properties  and performance on second order systems . Further extensions of NODEs include support for irregularly-sampled observations  and partial differential equations , Bayesian NODEs , universal differential equations , and more-see, e.g., Kidger  for an overview. All such variants fall under our definition of neural differential equations and can in principle be stabilized using our method. We demonstrate this empirically by stabilizing both standard NODEs as well as ANODEs and UDEs in this paper. Finally, we note that all experiments with second order structure are implemented as second order neural ODEs due to reportedly improved generalization compared to first order NODEs [62; 40].

NODEs were originally introduced as the infinite depth limit of residual neural networks (typically for classification), where there is no single true underlying dynamic law, but rather "some" vector field \(f\) is learned that allows subsequent (linear) separation of the inputs into different classes. A number of techniques have been introduced to restrict the number of function evaluations needed during training in order to improve efficiency, which also typically results in relatively simple learned dynamics [31; 38; 47; 63; 50]. These are orthogonal to our method and are mentioned here for completeness, as they could also be viewed as "regularized" or "stabilized" NODEs from a different perspective.

## 3 Related Work

Hamiltonian and Lagrangian Neural Networks.A large body of related work has focused on learning Hamiltonian or Lagrangian dynamics via architectural constraints. By directly parameterizing and learning the underlying Hamiltonian or Lagrangian, rather than the equations of motion, first integrals corresponding to symmetries of the Hamiltonian or Lagrangian may be (approximately) conserved automatically.

Hamiltonian Neural Networks (HNNs)  assume second order Hamiltonian dynamics \(u(t)\), where \(u(t)=(q(t),p(t))\) is measured in canonical coordinates, and directly parameterize the Hamiltonian \(\) (instead of \(f\)) from which the vector field can then be derived via \(f(q,p)=(}{ p},-}{  q})\). Symplectic ODE-Net (SymODEN)  takes a similar approach but makes the learned dynamics symplectic by construction, as well as allowing more general coordinate systems such as angles or velocities (instead of conjugate momenta). HNNs have also been made agnostic to the coordinate system altogether by modeling the underlying coordinate-free symplectic two-form directly , studied extensively with respect to the importance of symplectic integrators , and adapted specifically to robotic systems measured in terms of their SE(3) pose and generalized velocity . Recently, Gruver et al.  showed that what makes HNNs effective in practice is not so much their built-in energy conservation or symplectic structure, but rather that they inherently assume that the system is governed by a single second order differential equation ("second order bias"). Chen et al.  provide a recent overview of learning Hamiltonian dynamics using neural architectures.

A related line of work is Lagrangian Neural Networks (LNNs), which instead assume second order Lagrangian dynamics and parameterize the inertia matrix and divergence of the potential , or indeed any generic Lagrangian function , with the dynamics \(f\) then uniquely determined via the Euler-Lagrange equations. While our own work is related to HNNs and LNNs in purpose, it is more general, being applicable to any system governed by a differential equation. In addition, where we consider fundamental conservation laws as constraints, e.g. conservation of energy, we assume the conservation law is known in closed-form.

The work most closely related to ours is by Finzi et al. , who demonstrate how to impose explicit constraints on HNNs and LNNs. The present work differs in a number of ways with the key advances of our approach being that SNDEs (a) are applicable to any type of ODE, allowing us to go beyond second order systems with primarily Hamiltonian or Lagrangian type constraints, (b) are compatible with hybrid models, i.e., the UDE approach where part of the dynamics is assumed to be known and only the remaining unknown part is learned while still constraining the overall dynamics, and (c) can incorporate any type of manifold constraint. Regarding (c), we can for instance also enforce time-dependent first integrals, which do not correspond to constants of motion or conserved quantities arising directly from symmetries in the Lagrangian.

Continuous Normalizing Flows on Riemannian Manifolds.Another large body of related work aims to learn continuous normalizing flows (CNFs) on Riemannian manifolds. Since CNFs transform probability densities via a NODE, many of these methods work in practice by constraining NODE trajectories to a pre-specified Riemannian manifold. For example, Lou et al.  propose "Neural Manifold ODE", a method that directly adjusts the forward mode integration and backward mode adjoint gradient computation to ensure that the trajectory is confined to a given manifold. This approach requires a new training procedure and relies on an explicit chart representation of the manifold. The authors limit their attention to the hyperbolic space \(^{2}\) and the sphere \(^{2}\), both of which are prototypical Riemannian manifolds with easily derived closed-form expressions for the chart. Many similar works have been proposed [37; 14; 68; 60], but all are typically limited in scope to model Riemannian manifolds such as spheres, hyperbola, and tori. Extending these approaches to the manifolds studied by us, which can possess nontrivial geometries arising from arbitrary constraints, is not straightforward. Nonetheless, we consider this an interesting opportunity for further work.

Riemannian Optimization.A vast literature exists for optimization on Riemannian manifolds [2; 53; 61]. In deep learning, in particular, orthogonality constraints have been used to avoid the vanishing and exploding gradients problem in recurrent neural networks [5; 54; 1; 48] and as a form of regularization for convolutional neural networks . Where these methods differ crucially from this paper is that they seek to constrain neural network _weights_ to a given matrix manifold, while our aim is to constrain _trajectories_ of an NDE, the weights of which are not themselves directly constrained.

Constraints via Regularization.Instead of adapting the neural network architecture to satisfy certain properties by design, Lim and Kasim  instead manually craft different types of regularization terms that, when added to the loss function, aim to enforce different constraints or conservation laws. While similar to our approach, in that no special architecture is required for different constraints, the key difference is that their approach requires crafting specific loss terms for different types of dynamics. Moreover, tuning the regularization parameter can be rather difficult in practice.

In this work, we vastly broaden the scope of learning constrained dynamics by demonstrating the effectiveness of our approach on both first and second order systems, including chaotic and non-chaotic as well as autonomous and non-autonomous examples. We cover constraints arising from holonomic restrictions on system states, conservation laws, and constraints imposed by controls.

## 4 Stabilized Neural Differential Equations

General approach.Given \(m<n\) explicit constraints, we require that solution trajectories of the NDE in Equation (2) are confined to an \((n-m)\)-dimensional submanifold of \(^{n}\) defined by

\[=\{(u,t)^{n}\,;\,g(u,t)=0\},\] (3)

where \(g^{n}^{m}\) is a smooth function with \(0^{m}\) being a regular value of \(g\).1 In other words, we have an NDE on a manifold

\[=f_{}(u,t) g(u,t)=0.\] (4)

Any non-autonomous system can equivalently be represented as an autonomous system by adding time as an additional coordinate with constant derivative \(1\) and initial condition \(t_{0}=0\). For ease of notation, and without loss of generality, we will only consider autonomous systems in the rest of this section. However, we stress that our method applies equally to autonomous and non-autonomous systems.

While methods exist for constraining neural network outputs to lie on a pre-specified manifold, the added difficulty in our setting is that we learn the vector field \(f\) but constrain the solution trajectory \(u\) that solves a given initial value problem for the ODE defined by \(f\). Inspired by Chin , we propose the following stabilization of the vector field in Equation (4)

\[=f_{}(u)- F(u)g(u),}}}\] (5)

where \( 0\) is a scalar parameter of our method and \(F:^{n}^{n m}\) is a so-called _stabilization matrix_. We call Equation (5) a _stabilized neural differential equation_ (SNDE) and say that it is stabilized with respect to the invariant manifold \(\). We illustrate the main idea in Figure 1; while even small deviations in the unstabilized vector field (left) can lead to (potentially accumulating) constraint violations, our stabilization (right) adjusts the vector field near the invariant manifold to render it asymptotically stable, all the while leaving the vector field on \(\) unaffected.

Our stabilization approach is related to the practice of index reduction of differential algebraic equations (DAEs). We refer the interested reader to Appendix A for a brief overview of these connections.

Theoretical guarantees.First, note that ultimately we still want \(f_{}\) to approximate the assumed ground truth dynamics \(f\). However, Equation (5) explicitly modifies the right-hand side of the NDE. The following theorem provides necessary and sufficient conditions under which \(f_{}\) can still learn the correct dynamics when using a different right-hand side.

**Theorem 1** (adapted from Chin ).: _Consider an NDE_

\[=f_{}(u)\] (6)

_on an invariant manifold \(=\{u^{n}\,;\,g(u)=0\}\). A vector field \(=h_{}(u)\) admits all solutions of Equation (6) on \(\) if and only if \(h_{}|_{}=f_{}|_{}\)._

Since \(g(u)=0\) on \(\), the second term on the right-hand side of Equation (5) vanishes on \(\). Therefore the SNDE Equation (5) admits all solutions of the constrained NDE Equation (4). Next, we will show that, under mild conditions, the additional stabilization term in Equation (5) "nudges" the solution trajectory to lie on the constraint manifold such that \(\) is asymptotically stable.

**Theorem 2** (adapted from Chin 2).: _Suppose the stabilization matrix \(F(u)\) is chosen such that the matrix \(G(u)F(u)\), where \(G(u)=g_{u}\) is the Jacobian of \(g\) at \(u\), is symmetric positive definite with the smallest eigenvalue \((u)\) satisfying \((u)>_{0}>0\) for all \(u\). Assume further that there is a positive number \(_{0}\) such that_

\[\|G(u)f_{}(u)\|_{0}\|g(u)\|\] (7)

_for all \(u\) near \(\). Then the invariant manifold \(\) is asymptotically stable in the SNDE Equation (5) if \(_{0}/_{0}\)._

Proof.: Consider the Lyapunov function \(V(u)=g^{T}(u)g(u)\). Then (omitting arguments)

\[V(t)=\|g(u(t))\|^{2}=g^{T}=g ^{T}=g^{T}G(f_{}- Fg),\] (8)

where we substitute Equation (5) for \(\). With Equation (7), we have \(g^{T}Gf_{}_{0}g^{T}g\), and since the eigenvalues of \(GF\) are assumed to be at least \(_{0}>0\), we have \(g^{T}GFg_{0}g^{T}g\). Hence

\[V(_{0}-_{0})\|g\|^{2},\] (9)

so the manifold \(\) is asymptotically stable whenever \(_{0}-_{0} 0\). 

When \(f_{}(u)\) and \(g(u)\) are given, \(\) is asymptotically stable in the SNDE Equation (5) as long as

\[(u)\|}{_{0}\|g(u)\|}.\] (10)

To summarize, the general form of the SNDE in Equation (5) has the following important properties:

1. The SNDE admits all solutions of the constrained NDE Equation (4) on \(\).
2. \(\) is asymptotically stable in the SNDE for sufficiently large values of \(\).

The stabilization parameter \(\), with units of inverse time, determines the rate of relaxation to the invariant manifold; intuitively, it is the strength of the "nudge towards \(\)" experienced by a trajectory. Here, \(\) is neither a Lagrangian parameter (corresponding to a constraint on \(\)), nor a regularization parameter (to overcome an ill-posedness by regularization). Therefore, there is no "correct" value for \(\). In particular, Theorem 1 holds for all \(,\) while Theorem 2 only requires \(\) to be "sufficiently large".

In the limit \(\), the SNDE in Equation (5) is equivalent to a Hessenberg index-2 DAE (see Appendix A for more details).

Practical implementation.This leaves us to find a concrete instantiation of the stabilization matrix \(F(u)\) that should (a) satisfy that \(F(u)G(u)\) is symmetric positive definite with the smallest eigenvalue bounded away from zero near \(\), (b) be efficiently computable, and (c) be compatible with gradient-based optimization of \(\) as part of an NDE. In our experiments, we use the Moore-Penrose pseudoinverse of the Jacobian of \(g\) at \(u\) as the stabilization matrix,

\[F(u)=G^{+}(u)=G^{T}(u)G(u)G^{T}(u)^{-1}^{n m}.\] (11)

Let us analyze the properties of this choice. Regarding the requirements (b) and (c), the pseudoinverse can be computed efficiently via a singular value decomposition with highly optimized implementations in all common numerical linear algebra libraries (including deep learning frameworks) and does not interfere with gradient-based optimization. In particular, the computational cost for the pseudoinverse is \((m^{2}n)\), i.e., it scales well with the problem size. The quadratic scaling in the number of constraints is often tolerable in practice, since the number of constraints is typically small (in Appendix B, we discuss faster alternatives to the pseudoinverse that can be used, for example, when there are many constraints). Moreover, the Jacobian \(G(u)\) of \(g\) can be obtained via automatic differentiation in the respective frameworks.

Regarding requirement (a), the pseudoinverse \(G^{+}(u)\) is an orthogonal projection onto the tangent space \(T_{u}\) of the manifold at \(u\). Hence, locally in a neighborhood of \(u\), we consider the stabilization matrix as a projection back onto the invariant manifold \(\) (see Figure 1). In particular, \(G(u)\) has full rank for \(u\) and \(G^{+}G=G^{T}(GG^{T})^{-1}G\) is symmetric and positive definite near \(\). From here on, we thus consider the following specific form for the SNDE in Equation (5),

\[=f_{}(u)- G^{+}(u)g(u).}\] (12)

## 5 Results

We now demonstrate the effectiveness of SNDEs on examples that cover autonomous first and second order systems with either a conserved first integral of motion or holonomic constraints, a non-autonomous first order system with a conserved quantity, a non-autonomous controlled first order system with a time-dependent constraint stemming from the control, and a chaotic second order system with a conservation law. To demonstrate the flexibility of our approach with respect to the variant of underlying NDE model, for the first three experiments we include both vanilla NODEs and augmented NODEs as baselines and apply stabilization to both (with the stabilized variants labeled SNODE and SANODE, respectively).

As a metric for the predicted state \((t)\) versus ground truth \(u(t)\), we use the relative error \(\|u(t)-(t)\|_{2}/\|u(t)\|_{2}\), averaged over many trajectories with independent initial conditions. As a metric for the constraints, we compute analogous relative errors for \(g(u)\).

In Appendix C, we further demonstrate empirically that SNDEs are insensitive to the specific choice of \(\) over a large range (beyond a minimum value, consistent with Theorem 2). We also provide more intuition about choosing \(\) and the computational implications of this choice. In practice, we find that SNDEs are easy to use across a wide variety of settings with minimal tuning and incur only moderate training overhead compared to vanilla NDEs. In some cases, SNDEs are even computationally cheaper than vanilla NDEs at inference time despite the cost of computing the pseudoinverse. Finally, Appendix D provides results of additional experiments not included here due to space constraints.

### Two-Body Problem [second order, autonomous, non-chaotic, conservation law]

The motion of two bodies attracting each other with a force inversely proportional to their squared distance (e.g., gravitational interaction in the non-relativistic limit) can be written as

\[=-+y^{2})^{3/2}},=-+y^{2})^ {3/2}},\] (13)

where one body is fixed at the origin and \(x,y\) are the (normalized) Cartesian coordinates of the other body in the plane of its orbit . We stabilize the dynamics with respect to the conserved angular momentum \(L\), yielding

\[=\{(x,y)^{2}\,;\,x+y-L_{0}=0\},\] (14)where \(L_{0}\) is the initial value of \(L\). We train on 40 trajectories with initial conditions \((x,y,,)=(1-e,0,0,}{{1+e}}})\), where the eccentricity \(e\) is sampled uniformly via \(e U(0.5,0.7)\). Each trajectory consists of a single period of the orbit sampled with a timestep of \( t=0.1\).

The top row of Figure 2 shows that SNODEs, ANODEs and SANODEs all achieve stable long-term prediction over multiple orbits, while unstabilized NODEs diverge exponentially from the correct orbit.

Figure 2: **Top row:** Results for the two-body problem experiment, showing a single test trajectory in (a) and averages over 100 test trajectories in (b-c). **Middle row:** Results for the rigid body rotation experiment, showing a single test trajectory in (d) and averages over 100 test trajectories in (e-f). **Bottom row:** Results for the DC-to-DC converter experiment, showing the voltage \(v_{1}\) across the first capacitor during a single test trajectory in (g), and averages over 100 test trajectories in (h-i). The vanilla NODE (blue) is unstable in all settings, quickly drifting from the constraint manifold and subsequently diverging exponentially, while the vanilla ANODE (green) is unstable for the rigid body and DC-to-DC converter experiments. In contrast, the SNODE (red) and SANODE (purple) are constrained to the manifold with accurate predictions over a long horizon in all settings. Confidence intervals are not shown as they diverge along with the unstabilized trajectories.

### Motion of a Rigid Body

[first order, autonomous, non-chaotic, holonomic constraint]

The angular momentum vector \(y=(y_{1},y_{2},y_{3})^{T}\) of a rigid body with arbitrary shape and mass distribution satisfies Euler's equations of motion,

\[_{1}\\ _{2}\\ _{3}=0&-y_{3}&y_{2}\\ y_{3}&0&-y_{1}\\ -y_{2}&y_{1}&0}}{{I_{1}}}\\ }}{{I_{2}}}\\ }}{{I_{3}}},\] (15)

where the coordinate axes are the principal axes of the body, \(I_{1},I_{2},I_{3}\) are the principal moments of inertia, and the origin of the coordinate system is fixed at the body's centre of mass . The motion of \(y\) conserves the Casimir function \(C(y)=(y_{1}^{2}+y_{2}^{2}+y_{3}^{2})\), which is equivalent to conservation of angular momentum in the orthogonal body frame and constitutes a holonomic constraint on the allowed states of the system. We therefore have the manifold

\[=\{(y_{1},y_{2},y_{3})^{3}\,;\,y_{1}^{2}+y_{3}^{2}+y_{ 3}^{2}-C_{0}=0\}.\] (16)

We train on 40 trajectories with initial conditions \((y_{1},y_{2},y_{3})=((),0,())\), where \(\) is drawn from a uniform distribution \( U(0.5,1.5)\). Each trajectory consists of a 15 second sample with a timestep of \( t=0.1\) seconds.

The middle row of Figure 2 demonstrates that, unlike vanilla NODEs and ANODEs, SNODEs and SANODEs are constrained to the sphere and stabilize the predicted dynamics over a long time horizon in this first order system.

### DC-to-DC Converter

[first order, non-autonomous, non-chaotic, conservation law]

We now consider an idealized DC-to-DC converter [52; 33], illustrated in Figure 3, with dynamics

\[C_{1}_{1}=(1-u)i_{3}, C_{2}_{2}=ui_{3}, L_{3}_ {3}=-(1-u)v_{1}-uv_{2},\] (17)

where \(v_{1},v_{2}\) are the state voltages across capacitors \(C_{1},C_{2}\), respectively, \(i_{3}\) is the state current across an inductor \(L_{3}\), and \(u\{0,1\}\) is a control input (a switch) that can be used to transfer energy between the two capacitors via the inductor. The total energy in the circuit, \(E=(C_{1}v_{1}^{2}+C_{2}v_{2}^{2}+L_{3}i_{3}^{2})\), is conserved, yielding the manifold

\[=\{(v_{1},v_{2},i_{3})^{3}\,;\,C_{1}v_{1}^{2}+C_{2}v_ {2}^{2}+L_{3}i_{3}^{2}-E_{0}=0\}.\] (18)

We train on 40 trajectories integrated over \(10\) seconds with a timestep of \( t=0.1\) seconds, where \(C_{1}=0.1\), \(C_{2}=0.2\), \(L_{3}=0.5\), and a switching period of \(3\) seconds, i.e., the switch is toggled every \(1.5\) seconds. The initial conditions for \((v_{1},v_{2},i_{3})\) are each drawn independently from a uniform distribution \(U(0,1)\).

The bottom row of Figure 2 shows the voltage across \(C_{1}\) over multiple switching events (g), with the NODE and ANODE quickly accumulating errors every time the switch is applied, while the SNODE and SANODE remain accurate for longer. Panels (h,i) show the familiar exponentially accumulating errors for vanilla NODE and ANODE, versus stable relative errors for SNODE and SANODE.

### Controlled Robot Arm

[first order, non-autonomous, non-chaotic, time-dependent control]

Next, we apply SNDEs to solve a data-driven inverse kinematics problem , that is, learning the dynamics of a robot arm that satisfy a prescribed path \(p(t)\). We consider an articulated robot arm consisting of three connected segments of fixed length 1, illustrated in Figure 4(a). Assuming one end of the first segment is fixed at the origin and the robot arm is restricted to move in a plane, the endpoint \(e()\) of the last segment is given by

\[e()=(_{1})+(_{2})+(_{3})\\ (_{1})+(_{2})+(_{3}),\] (19)

where \(_{j}\) is the angle of the \(j\)-th segment with respect to the horizontal and \(=(_{1},_{2},_{3})\). The problem consists of finding the motion of the three segments \((t)\) such that the endpoint \(e()\) follows

Figure 3: Idealized schematic of a DC-to-DC converter.

a prescribed path \(p(t)\) in the plane, i.e., \(e()=p(t)\). Minimizing \(||(t)||\), it can be shown  that the optimal path satisfies

\[=e^{}()^{T}e^{}()e^{}()^{ T}^{-1}(t),\] (20)

where \(e^{}\) is the Jacobian of \(e\). These will be our ground truth equations of motion.

We stabilize the SNODE with respect to the (time-dependent) manifold

\[=\{(,t)\,;\,e()-p(t)=0\}.\] (21)

In particular, we prescribe the path

\[p(t)=e_{0}-(2 t)/2\\ 0,\] (22)

where \(e_{0}\) is the initial position of the endpoint, such that \(e()\) traces a line back and forth on the \(x\)-axis. We train on 40 trajectories of duration \(5\) seconds, with timestep \( t=0.1\) and initial conditions \((_{1},_{2},_{3})=(_{0},-_{0},_{0})\), where \(_{0}\) is drawn from a uniform distribution \(_{0} U(/4,3/8)\). Additionally, we provide the network with \(\), the time derivative of the prescribed control.

Figure 4 shows that the unconstrained NODE drifts substantially from the prescribed path during a long integration, while the SNODE implements the control to a high degree of accuracy and without drift.

### Double Pendulum [second order, autonomous, chaotic, conservation law]

Finally, we apply stabilization to the chaotic dynamics of the frictionless double pendulum system. The total energy \(E\) of the system is conserved , yielding the manifold,

\[=\{(_{1},_{2},_{1},_{2})^{2} ^{2}\,;\,E(_{1},_{2},_{1},_{2})-E_{0} =0\},\] (23)

where \(_{i}\) is the angle of the \(i\)-th arm with the vertical and \(_{i}=_{i}\). We refer the reader to Arnold  (or the excellent Wikipedia entry) for the lengthy equations of motion and an expression for the total energy. For simplicity we take \(m_{1}=m_{2}=1\,\), \(l_{1}=l_{2}=1\,\), and \(g=9.81\,^{-2}\). We train on 40 trajectories, each consisting of 10 seconds equally sampled with \( t=0.05\), and with initial conditions \((_{1},_{2},_{1},_{2})=(,,0,0)\), where \(\) is drawn randomly from a uniform distribution \( U(/4,3/4)\). We emphasize that this is a highly limited amount of data when it comes to describing the chaotic motion of the double pendulum system, intended to highlight the effect of stabilization in the low-data regime.

Figure 5(a-b) shows that, while initially the SNODE only marginally outperforms the vanilla NODE in terms of the relative error of the state, the longer term relative error in energy is substantially

Figure 4: Controlled robot arm. **(a)** Schematic of the robot arm. **(b)** Snapshot of a single test trajectory. After 100 seconds the NODE (blue) has drifted significantly from the prescribed control while the SNODE (red) accurately captures the ground truth dynamics (black). **(c)** Relative error in the endpoint \(e()\) averaged over 100 test trajectories. The NODE (blue) accumulates errors and leaves the prescribed path, while the SNODE (red) remains accurate. Shadings in (c) are 95% confidence intervals.

larger for NODE than for SNODE. A certain relative error in state is indeed unavoidable for chaotic systems.

In addition to predicting individual trajectories of the double pendulum, we also consider an additional important task: learning the invariant measure of this chaotic system. This can be motivated by analogy with climate predictions, where one also focuses on long-term prediction of the invariant measure of the system, as opposed to predicting individual trajectories in the sense of weather forecasting, which must break down after a short time due to the highly chaotic underlying dynamics. Motivated by hybrid models of the Earth's climate [36; 71], we choose a slightly different training strategy than before, namely a hybrid setup in line with the UDE approach mentioned above. In particular, the dynamics of the first arm \(_{1}\) are assumed known, while the dynamics of the second arm \(_{2}\) are learned from data. We train on a _single trajectory_ of duration 60 seconds with \( t=0.05\). For each trained model, we then integrate ten trajectories of duration one hour - far longer than the observed data. An invariant measure is estimated from each long trajectory (see Appendix E) and compared with the ground truth using the Hellinger distance.

Figure 5(c) shows that, as \(\) is increased, our ability to accurately learn the double pendulum's invariant measure increases dramatically due to stabilization, demonstrating that the "climate" of this system is captured much more accurately by the SNDE than by the NDE baseline.

## 6 Conclusion

We have introduced stabilized neural differential equations (SNDEs), a method for learning dynamical systems from observational data subject to arbitrary explicit constraints. Our approach is based on a stabilization term that is cheap to compute and provably renders the invariant manifold asymptotically stable while still admitting all solutions of the vanilla NDE on the invariant manifold. Key benefits of our method are its simplicity and generality, making it compatible with all common NDE methods without requiring further architectural changes. Crucially, SNDEs allow entirely new types of constraints, such as those arising from known conservation laws and controls, to be incorporated into neural differential equation models. We demonstrate their effectiveness across a range of settings, including first and second order systems, autonomous and non-autonomous systems, with constraints stemming from holonomic constraints, conserved first integrals of motion, as well as time-dependent restrictions on the system state. SNDEs are robust with respect to the only tuneable parameter and incur only moderate computational overhead compared to vanilla NDEs.

The current key limitations and simultaneously interesting directions for future work include adapting existing methods for NODEs on Riemannian manifolds to our setting, generalizations to partial differential equations, allowing observations and constraints to be provided in different coordinates, and scaling the method to high-dimensional settings such as learning dynamics from pixel observations, for example in fluid dynamics or climate modeling. Finally, we emphasize that high-dimensional, non-linear dynamics may not be identifiable from just a small number of solution trajectories. Hence, care must be taken when using learned dynamics in high-stakes scenarios (e.g., human robot interactions), especially when going beyond the training distribution.

Figure 5: Results for the double pendulum. **(a)** Relative error in the state over 300 short test trials, shown with 95% confidence intervals (shaded). Compared to the SNODE, the NODE diverges rapidly as it begins to accumulate errors in the energy. **(b)** Relative error in the energy averaged over 5 long test trials. **(c)** Comparison of the double pendulum’s invariant measure estimated by the (hybrid) UDE, with and without stabilization, versus ground truth, with 95% confidence intervals.