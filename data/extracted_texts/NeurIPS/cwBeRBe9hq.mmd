# On the Learnability of Multilabel Ranking

Vinod Raman

Department of Statistics

University of Michigan

Ann Arbor, MI 48104

vkraman@umich.edu

&Unique Subedi

Department of Statistics

University of Michigan

Ann Arbor, MI 48104

subedi@umich.edu

&Ambuj Tewari

Department of Statistics

University of Michigan

Ann Arbor, MI 48104

tewaria@umich.edu

Equal contributions

###### Abstract

Multilabel ranking is a central task in machine learning. However, the most fundamental question of learnability in a multilabel ranking setting with relevance-score feedback remains unanswered. In this work, we characterize the learnability of multilabel ranking problems in both batch and online settings for a large family of ranking losses. Along the way, we give two equivalence classes of ranking losses based on learnability that capture most losses used in practice.

## 1 Introduction

_Multilabel ranking_ is a supervised learning problem where a learner is presented with an instance \(x\) and is required to output a ranking of \(K\) different labels in decreasing order of relevance to \(x\). This is in contrast with _multilabel classification_ where given an instance \(x\), the learner is tasked with predicting a subset of the \(K\) labels without any explicit ordering. Multilabel ranking is a canonical learning problem with a wide range of applications to text categorization, genetics, medical imaging, social networks, and visual object recognition [Joachims, 2005, Schapire and Singer, 2000, McCallum, 1999, Clare and King, 2001, Baltruschat et al., 2019, Wang and Sukthankar, 2013, Bucak et al., 2009, Yang et al., 2016]. Recent years have seen a surge in the development of multilabel ranking methods with strong practical and theoretical guarantees [Schapire and Singer, 2000, Dembczynski et al., 2012, Gong et al., 2013, Bucak et al., 2009, Jung and Tewari, 2018, Gao and Zhou, 2011, Koyejo et al., 2015, Zhang and Zhou, 2013, Korba et al., 2018]. A related line of work has studied consistency for the convex surrogates of natural ranking losses [Duchi et al., 2010, Buffoni et al., 2011, Gao and Zhou, 2011, Ravikumar et al., 2011, Calauzenes et al., 2012, Dembczynski et al., 2012]. Despite this vast literature on multilabel ranking, the fundamental question of when a multilabel ranking problem is _learnable_ remains unanswered.

Understanding when a hypothesis class is learnable is a fundamental question in Statistical Learning Theory. For binary classification, the finiteness of the Vapnik-Chervonenkis (VC) dimension is both sufficient and necessary for Probably Approximately Correct (PAC) learning [Vapnik and Chervonenkis, 1974, Valiant, 1984]. Likewise, the finiteness of the Daniely-Shwartz (DS) dimension characterizes multiclass PAC learnability Daniely and Shalev-Shwartz , Brukhim et al. . In the online setting, the Littlestone dimension [Littlestone, 1987] characterizes the online learnability of a binary hypothesis class and the multiclass Littlestone dimension [Daniely et al., 2011] characterizes online multiclass learnability. Unlike classification, a distinguishing property of multilabel ranking is the mismatch between the predictions the learner makes and the feedback it receives. In particular, a learner is required to produce a permutation that ranks the relevance of the labels but only receives a _relevance-score vector_ as feedback. This feedback model is standard in multilabel ranking since obtaining full permutation feedback is generally costly [Liu et al., 2009]. As a result, unlike the 0-1 loss in classification, there is no canonical loss function in ranking. Together, these two issuescreate barriers for existing techniques used to prove learnability, such as the agnostic-to-realizable reductions from Hopkins et al. (2022) and Raman et al. (2023), to readily extend to ranking.

In this paper, we characterize the batch and online learnability of a ranking hypothesis class \(_{K}^{}\) under relevance-score feedback, where \(_{K}\) is the set of all permutations over \([K]=\{1,...,K\}\). In doing so, we make the following contributions.

* We show that a ranking hypothesis class \(\) embeds \(K^{2}\) different _binary_ hypothesis classes \(_{i}^{j}\) for \(i,j[K]\), where hypotheses in \(_{i}^{j}\) answer whether the label \(i\) should be ranked in the top \(j\). Our main result relates the learnability of \(\) to the learnability of \(_{i}^{j}\)'s.
* the same characterization of batch and online learnability holds for every loss in that family.
* By relating the learnability of \(\) to the learnability of _binary_ hypothesis classes \(_{i}^{j}\), we show that existing combinatorial dimensions, like the VC and Littlestone dimension, continue to characterize learnability in the multilabel ranking setting. This allows us to prove that linear ranking hypothesis classes are learnable in the batch setting.

A unifying theme throughout the paper is our ability to _constructively_ convert a learning algorithm \(\) for \(\) into a learning algorithm \(_{i}^{j}\) for \(_{i}^{j}\) for each \(i,j[K]\) and vice versa. To do so, our proof techniques involve adapting the agnostic-to-realizable reduction for batch and online classification, proposed by Hopkins et al. (2022) and Raman et al. (2023) respectively, to ranking.

## 2 Preliminaries and Notation

Let \(\) denote the instance space, \(_{K}\) the set of permutations over labels \([K]:=\{1,...,K\}\), and \(=\{0,1,...,B\}^{K}\) the target space for some \(K,B\). We highlight that the set of labels \([K]\) is fixed beforehand and does not depend on the instance \(x\). This is to be contrasted with subset ranking, the set of labels can change depending on the instance \(x\).

We refer to an element \(y\) as a _relevance-score vector_ that indicates the relevance of each of the \(K\) labels, where \(B\) indicates the highest relevance and \(0\) indicates the lowest relevance. Throughout the paper, we treat a permutation \(_{K}\) as a vector in \(\{1,...,K\}^{K}\) that induces a _ranking_ of the \(K\) labels in decreasing order of relevance. Accordingly, for an index \(i[K]\), we let \(_{i}[K]\) denote the _rank_ of label \(i\). Likewise, given an index \(i[K]\), we let \(y^{i}\) denote the relevance of label \(i\). In addition, it will be useful to define a mapping from \(_{K}\) to \(\{0,1\}^{K}\). In particular, we define \((,):_{K}[K]\{0,1\}^{K}\) as an operator that given a permutation (ranking) \(_{K}\) and threshold \(p[K]\), outputs a bit string \(b\{0,1\}^{K}\) s.t. \(b_{i}=\{_{i} p\}\).

**Ranking Equivalences**. Our construction of ranking loss families in Section 3 requires different notions of equivalence between permutations (rankings) in \(_{K}\). To that end, we say that \(=\) if and only if for all \(i[K]\), \(_{i}=_{i}\). On the other hand, we say \(}{{=}}\) if and only if \(\{i:_{i} p\}=\{i:_{i} p\}\). That is, two rankings are \(p\)-equivalent if the _set_ of labels they rank in the top-\(p\) are equal. Finally, we say \(}{{=}}\) if and only if for all \(j[p]\), \(\{i:_{i} j\}=\{i:_{i} j\}\). That is, two rankings are \([p]\)-equivalent if not only the _set_ but also the _order_ of labels they rank in the top-\(p\) are equal.

**Ranking Hypothesis**. A ranking hypothesis \(h_{K}^{}\) maps instances in \(\) to a ranking (permutation) in \(_{K}\). Given an instance \(x\), one can think of \(h(x)\) as \(h\)'s ranking of the \(K\) different labels in decreasing order of relevance. For any ranking hypothesis \(h\), we let \(h_{i}:[K]\) denote its restriction to the \(i\)'th coordinate output. Accordingly, for an instance \(x\), \(h_{i}(x)\) gives the rank that \(h\) assigns to label \(i\). Given a ranking hypothesis class \(_{K}^{}\) and any \(i,j[K]\), we define its binary threshold-restricted hypothesis class \(_{i}^{j}=\{h_{i}^{j}:h\}\) where \(h_{i}^{j}(x)=\{h_{i}(x) j\}\). We can think of hypotheses in \(_{i}^{j}\) as providing binary responses to queries of the form: "for instance \(x\), should label \(i\) ranked in the top \(j\)?" These threshold-restricted classes are central to our characterization of learnability in both the batch and online learning settings.

**Batch Learnability.** In the batch setting, we are interested in characterizing the learnability of a ranking hypothesis class \(\) under a model similar to the classical PAC model [Valiant, 1984].

**Definition 1** (Agnostic Ranking PAC Learnability).: _A ranking hypothesis class \(_{K}^{}\) is agnostic PAC learnable w.r.t. loss \(:_{K}_{ 0}\), if there exists a function \(m:(0,1)^{2}\) and a learning algorithm \(:()^{}_{K}^{ }\) with the following property: for every \(,(0,1)\) and for every distribution \(\) on \(\), running algorithm \(\) on \(n m(,,K)\) iid samples from \(\) outputs a predictor \(g=(S)\) such that with probability at least \(1-\) over \(S^{n}\),_

\[_{}[(g(x),y)]_{h}_{ }[(h(x),y)]+.\]

If \(\) is restricted to the class of distributions such that \(_{h}_{}[(h(x),y)]=0\), then we say we are in the _realizable_ setting. Note that unlike in classification, realizability in the multilabel ranking setting is loss dependent.

**Online Learnability.** In the online setting, an adversary plays a sequential game with the learner over \(T\) rounds. In each round \(t[T]\), an adversary selects a labeled instance \((x_{t},y_{t})\) and reveals \(x_{t}\) to the learner. The learner makes a (potentially randomized) prediction \(_{t}_{K}\). Finally, the adversary reveals the true relevance-score vector \(y_{t}\), and the learner suffers the loss \((_{t},y_{t})\), where \(\) is some pre-specified ranking loss function. Given a ranking hypothesis class \(_{K}^{}\), the goal of the learner is to output predictions \(_{t}\) such that its cumulative loss is close to the best possible cumulative loss over hypotheses in \(\). A hypothesis class is online learnable if there exists an algorithm such that for any sequence of labeled examples \((x_{1},y_{1}),...,(x_{T},y_{T})\), the difference in cumulative loss between its predictions and the predictions of the best possible function in \(\) is small.

**Definition 2** (Agnostic Online Ranking Learnability).: _A ranking hypothesis class \(_{K}^{}\) is agnostic online learnable w.r.t. loss \(\), if there exists an (potentially randomized) algorithm \(\) such that for any adaptively chosen sequence of labeled examples \((x_{t},y_{t})\), the algorithm outputs \((x_{t})_{K}\) at every iteration \(t[T]\) such that its expected regret,_

\[R(T,K):=[_{t=1}^{T}((x_{t}),y_{t})-_{h }_{t=1}^{T}(h(x_{t}),y_{t})],\]

_is a non-decreasing, sub-linear function of \(T\). Here, the expectation is taken with respect to the randomness of the algorithm \(\)._

If it is further guaranteed that there exists a hypothesis \(h^{}\) such that \(_{t=1}^{T}(h^{}(x_{t}),y_{t})=0\), then we say we are in the _realizable_ setting. Again, realizability is loss dependent.

## 3 Ranking Loss Families

In statistical learning theory, we often characterize learnability with respect to a loss function. Unlike the 0-1 loss in classification, there is no canonical loss function in multilabel ranking. Accordingly, we define two general families of ranking loss functions in this section and later characterize learnability with respect to all losses in these families. In Appendix A, we show that many of the ranking metrics used in practice (e.g. Pairwise Rank Loss, Discounted Cumulative Gain, Reciprocal Rank, Average Precision, Precision@p, etc.) fall into one of these two families.

On a high level, we can classify ranking losses into two main groups: (A) those losses that care about both the order and magnitude of the relevance scores within the top-\(p\) ranked labels and (B) those losses that only care about the magnitude of the relevance scores within the top-\(p\) ranked labels. Our goal will be to define a loss family for both groups A and B. To do so, we start by identifying a canonical ranking loss that lies in each group. For group A, the normalized sum loss@p,

\[_{}^{}(,y)=_{i=1}^{K}(_{i},p+1)y^{i}-Z_{y }^{p}\]

captures both the order and magnitude of the relevance scores only for the top-\(p\) ranked labels. Here, \(Z_{y}^{p}\) is an appropriately chosen normalization factor that only depends on \(p\) and \(y\) such that\(_{_{K}}_{}^{ p}(,y)=0\). For Group B, the normalized precision loss@p,

\[_{}^{ p}(,y)=Z_{y}^{p}-_{i=1}^{K}\{_{ i} p\}y^{i}\]

cares only about the magnitude of relevance scores in the top-\(p\) ranked labels. Again, \(Z_{y}^{p}\) is an appropriately chosen normalization constant that only depends on \(p\) and \(y\) such that the minimum loss is \(0\). The form of \(_{}^{ p}\) differs from \(_{}^{ p}\) because \(_{i=1}^{K}\{_{i} p\}y^{i}\) is a gain whereas \(_{i=1}^{K}(_{i},p+1)y^{i}\) is a loss.

Next, we build loss families around \(_{}^{ p}\) and \(_{}^{ p}\). For \(_{}^{ p}\), consider the family:

\[(_{}^{ p})=\{^{S_{K} }:=0_{}^{ p}=0\}\{^{_{K} }:}{{=}}( ,y)=(,y)\}.\]

By definition, \((_{}^{ p})\) contains those ranking losses that are (1) zero-matched with \(_{}^{ p}\) and (2) remain unchanged for any two predicted rankings (permutations) that are \([p]\)-equivalent. The second constraint is needed to ensure that losses in \((_{}^{ p})\) only depend on the order and set of labels that \(\) ranks in the top-\(p\). Likewise, we can construct a similar loss family around \(_{}^{ p}\) as follows:

\[(_{}^{ p})=\{^{_ {K}}:=0_{}^{ p}=0\}\{^{_{K} }:p]}{{=}}( ,y)=(,y)\}.\]

The set \((_{}^{ p})\) contains those ranking losses that are (1) zero-matched with \(_{}^{ p}\) and (2) remain unchanged for any two predicted rankings (permutations) that are \(p\)_-equivalent_. The second constraint is needed to ensure that losses in \((_{}^{ p})\) only depend on the set of labels that \(\) ranks in the top-\(p\). A major contribution of this paper is showing that both \((_{}^{ p})\) and \((_{}^{ p})\) are actually _equivalence_ classes - the same characterization of learnability holds for every loss in that family.

## 4 Batch Multilabel Ranking

In this section, we characterize the agnostic PAC learnability of hypothesis classes \(_{K}^{}\) with respect to both \((_{}^{ p})\) and \((_{}^{ p})\). Our main results, stated below as two theorems, relate the learnability of \(\) to the learnability of the threshold-restricted classes \(_{i}^{j}\).

**Theorem 4.1**.: _A hypothesis class \(_{K}^{}\) is agnostic PAC learnable w.r.t \((_{}^{ p})\) if and only if for all \(i[K]\) and \(j[p]\), \(_{i}^{j}\) is agnostic PAC learnable w.r.t the 0-1 loss._

**Theorem 4.2**.: _A hypothesis class \(_{K}^{}\) is agnostic PAC learnable w.r.t \((_{}^{ p})\) if and only if for all \(i[K]\), \(_{i}^{p}\) is agnostic PAC learnable w.r.t the 0-1 loss._

Since VC dimension characterizes the learnability of binary hypothesis classes under the 0-1 loss, an important corollary of Theorems 4.1 and 4.2 is that finiteness of \((_{i}^{j})\)'s, for the appropriate \(i,j[K][p]\), is necessary and sufficient for agnostic ranking PAC learnability. Later on, we use this fact to prove that linear ranking hypothesis classes are agnostic ranking PAC learnable.

We start with the proof of Theorem 4.1, which follows in three steps. First, we show that if for all \((i,j)[K][p]\), \(_{i}^{j}\) is agnostic PAC learnable w.r.t 0-1 loss, then Empirical Risk Minimization (ERM) is an agnostic PAC learner for \(\) w.r.t \(_{}^{ p}\). Next, we show that if \(\) is agnostic PAC learnable w.r.t \(_{}^{ p}\), then \(\) is agnostic PAC learnable w.r.t any loss \((_{}^{ p})\). Our proof of the latter uses the realizable to agnostic conversion from Hopkins et al. (2022). Finally, we prove the necessity direction - if \(\) is agnostic PAC learnable w.r.t an arbitrary \((_{}^{ p})\), then for all \((i,j)[K][p]\), \(_{i}^{j}\) is agnostic PAC learnable w.r.t 0-1 loss. The proof of necessity direction also uses the realizable to agnostic conversion from Hopkins et al. (2022). The proof of Theorem 4.2 follows exactly the same way as Theorem 4.1 with some minor changes. Thus, we only focus on the proof of Theorem 4.1 in this section and defer all discussion of Theorem 4.2 to Appendix C.3.

We begin with Lemma 4.3, which asserts that if \(_{i}^{j}\) is agnostic PAC learnable for all \((i,j)[K][p]\), then ERM is an agnostic PAC learner for \(\) w.r.t \(_{}^{ p}\).

**Lemma 4.3**.: _If for all \(i[K]\) and \(j[p]\), \(^{j}_{i}\) is agnostic PAC learnable w.r.t the 0-1 loss, then_ ERM _is an agnostic PAC learner for \(^{X}_{K}\) w.r.t \(^{p}_{}\)._

The proof of Lemma 4.3 exploits the nice structure of \(^{p}_{}\) by upperbounding the empirical Rademacher complexity of the loss class \(^{p}_{}=\{(x,y)^{p}_{}(h(x),y):h)\}\) and showing that it vanishes as the sample size \(n\) becomes large. Then, standard uniform convergence arguments outlined in Proposition C.1 imply that ERM is an agnostic PAC learner for \(\) w.r.t \(^{p}_{}\). The full proof is in Appendix C.

Since arbitrary losses in \((^{p}_{})\) may not have nice analytical forms, Lemma 4.4 relates the learnability of an arbitrary loss \((^{p}_{})\) to the learnability of \(^{p}_{}\).

**Lemma 4.4**.: _If \(^{X}_{K}\) is agnostic PAC learnable w.r.t \(^{p}_{}\), then \(\) is agnostic PAC learnable w.r.t any \((^{p}_{})\)._

Proof.: (of Lemma 4.4) Fix \((^{p}_{})\). Let \(a=_{,y}\{(,y)(,y) 0\}\) and \(b=_{,y}(,y)\). We need to show that if \(\) is agnostic PAC learnable w.r.t \(^{p}_{}\), then \(\) is agnostic PAC learnable w.r.t \(\). We will do so in two steps. First, we will show that if \(\) is an agnostic PAC learner for \(^{p}_{}\), then \(\) is also a _realizable_ PAC learner for \(\). Next, we will show how to convert a realizable PAC learner for \(\) into an agnostic PAC learner for \(\) in a black-box fashion. The composition of these two pieces yields an agnostic PAC learner for \(\) w.r.t \(\).

**Realizable PAC learnability of \(\) w.r.t \(\)**. If \(\) is agnostic PAC learnable w.r.t \(^{p}_{}\), then there exists a learning algorithm \(\) with sample complexity \(m(,,K)\) s.t. for any distribution \(\) over \(\), with probability \(1-\) over a sample \(S^{n}\) of size \(n m(,,K)\), the output predictor \(g=(S)\) achieves \(_{}[^{p}_{}(g(x),y))] _{h}_{}[^{p}_{ }(h(x),y))]+\). In the realizable setting, we are further guaranteed that there exists a hypothesis \(h^{*}\) s.t. \(_{}[(h^{*}(x),y))]=0\). Since \((^{p}_{})\), this also implies that \(_{}[^{p}_{}(h^{*}(x),y)) ]=0\). Therefore, under realizability and the fact that \( b\)\(^{p}_{}\), we have \(_{}[(g(x),y))] b\). This completes the first part of the proof as we have shown that \(\) is also a realizable PAC learner for \(\) w.r.t \(\) with sample complexity \(m(,,K)\).

**Realizable-to-agnostic conversion**. Now, we show how to convert the realizable PAC learner \(\) for \(\) into an agnostic PAC learner for \(\) in a black-box fashion. For this step, we will extend the agnostic-to-realizable reduction proposed by Hopkins et al. (2022) to the ranking setting by accommodating the mismatch between the range space of \(\) and the label space \(\). In particular, we will show that Algorithm 1 below converts a realizable PAC learner for \(\) into an agnostic PAC learner for \(\). Note that although input \(\) is a realizable learner, the distribution \(\) may not be realizable.

``` Input: Realizable PAC learner \(\) for \(\), unlabeled and labeled samples \(S_{U}^{n}_{}\) and \(S_{L}^{m}\)
1 For each \(h_{|S_{U}}\), construct a dataset \[S_{U}^{h}=\{(x_{1},_{1}),...,(x_{n},_{n})\}_{i}\{(h(x_{i}),1),...,(h(x_ {i}),p)\}\] Run \(\) over all datasets to get \(C(S_{U}):=\{S_{U}^{h} h_{|S_{U }}\}\) Return \( C(S_{U})\) with the lowest empirical error over \(S_{L}\) w.r.t. \(\). ```

**Algorithm 1** Agnostic PAC learner for \(\) w.r.t. \(\)

Let \(h^{*}=_{h}_{}[(h(x),y)]\) denote the optimal predictor in \(\) w.r.t \(\). Consider the sample \(S_{U}^{h^{*}}\) and let \(g=(S_{U}^{h^{*}})\). We can think of \(g\) as the output of \(\) run over an i.i.d sample \(S\) drawn from \(^{*}\), a joint distribution over \(\) defined procedurally by first sampling \(x_{}\), then independently sampling \(j([p])\), and finally outputting the labeled sample \((x,(h^{*}(x),j))\). Note that \(^{*}\) is indeed a realizable distribution (realized by \(h^{*}\)) w.r.t both \(\) and \(^{p}_{}\). Recall that \(m_{}(,,K)\) is the sample complexity of \(\). Since \(\) is a realizable learner for \(\) w.r.t \(\), we have that for \(n m_{}(p},/2,K)\), with probability at least \(1-\), \(_{^{*}}[(g(x),y)]\).

Next, by Lemma E.1, we have \((g(x),y)(h^{}(x),y)+_{j[[p] ]}[(g(x),(h^{}(x),j))]\) pointwise. Taking expectations on both sides of the inequality gives

\[_{}[(g(x),y)] _{}[(h^{}(x),y)]+ _{x}[_{j[ (p]]}[(g(x),(h^{}(x),j))]]\] \[_{}[(h^{}(x),y)]+ .\]

The last inequality follows from the definition of \(^{}\), namely \(_{^{}}[(g(x),y)]=_{x _{}}_{j[[p]]}[(g(x), (h^{}(x),j))]\). This shows that \(C(S_{U})\) contains a hypothesis \(g\) that generalizes well with respect to \(\). Now we want to show that the predictor \(\) returned in step 4 also has good generalization. Crucially, observe that \(C(S_{U})\) is a finite hypothesis class with cardinality at most \(2^{nK}\). By standard Chernoff and union bounds, with probability at least \(1-/2\), the empirical risk of every hypothesis in \(C(S_{U})\) on a sample of size \(})|}{}\) is at most \(/4\) away from its true error. So, if \(m=|S_{L}|})|}{}\), then with probability at least \(1-/2\),

\[|}_{(x,y) S_{L}}(g(x),y)_{ }[(g(x),y)]+_{} [(h^{}(x),y)]+.\]

Since \(\) is the ERM on \(S_{L}\) over \(C(S)\), its empirical risk can be at most \(_{}[(h^{}(x),y)]+\). Given that the population risk of \(\) can be at most \(/4\) away from its empirical risk, we have that

\[_{}[((x),y)]_{} [(h^{}(x),y)]+.\]

Applying union bounds, the entire process succeeds with probability \(1-\). We can upper bound the sample complexity of Algorithm 1, denoted \(n(,,K)\), as

\[n(,,K)  m_{}(p},/2,K )+O(})|}{})\] \[ m_{}(p},/2,K )+O(}(p},/2,K) \ +}{^{2}}),\]

where we use \(|C(S_{U})| 2^{Km_{}(p},/2,K)}\). This shows that Algorithm 1 is an agnostic PAC learner for \(\) w.r.t \(\). 

Finally, Lemma 4.5 gives the necessity direction of Theorem 4.1.

**Lemma 4.5**.: _If a hypothesis class \(_{K}^{}\) is agnostic PAC learnable w.r.t \((_{}^{ p})\), then \(_{i}^{j}\) is agnostic PAC learnable w.r.t the 0-1 loss for all \((i,j)[K][p]\)._

Like the sufficiency proofs, the proof of Lemma 4.5 is constructive. Given an agnostic PAC learner for \(\) w.r.t \(\), we construct an agnostic PAC learner for \(_{i}^{j}\) w.r.t 0-1 loss using a slight modification of Algorithm 1. We defer the full proof to Appendix C since the analysis is similar to that of Algorithm 1. Together, Lemmas 4.3, 4.4 and 4.5 imply Theorem 4.1.

We conclude this section by giving a concrete application of our characterization. Consider the class of ranking-hypotheses \(=\{x(Wx):W^{K d}\}\) that compute rankings by sorting scores, in descending order, obtained from a linear function of the input features. Lemma 4.6, whose proof is in Appendix B, computes the VC dimension of \(_{i}^{j}\) for an arbitrary \(i,j[K]\).

**Lemma 4.6**.: _Let \(=\{x(Wx):W^{K d}\}\) be a linear ranking hypothesis class. Then for all \(i,j[K]\), \((_{i}^{j})=(Kd)\), where \(\) hides logarithmic factors of \(d\) and \(K\)._

Combining Lemma 4.6 with Theorems 4.1 and 4.2 shows that linear ranking hypothesis classes are agnostic ranking PAC learnable w.r.t to all losses in \((_{}^{ p})(_{}^{  p})\). More generally, in Appendix B we give a dimension-based sufficient condition under which _generic_ score-based ranking hypothesis classes are agnostic ranking PAC learnable.

Online Multilabel Ranking

We now move to the online setting and characterize the online learnability of hypothesis classes \(_{K}^{}\) with respect to both \((_{}^{ p})\) and \((_{}^{ p})\). As in the batch setting, our characterization relates the learnability of \(\) to the learnability of the threshold-restricted classes \(_{i}^{j}\).

**Theorem 5.1**.: _A hypothesis class \(_{K}^{}\) is agnostic online learnable w.r.t \((_{}^{ p})\) if and only if for all \(i[K]\) and \(j[p]\), \(_{i}^{j}\) is agnostic online learnable w.r.t the 0-1 loss._

**Theorem 5.2**.: _A hypothesis class \(_{K}^{}\) is agnostic online learnable w.r.t \((_{}^{ p})\) if and only if for all \(i[K]\), \(_{i}^{p}\) is agnostic online learnable w.r.t the 0-1 loss._

Since the Littlestone dimension characterizes the online learnability of binary hypothesis classes under the 0-1 loss, an important corollary of Theorems 5.1 and 5.2 is is that finiteness of \((_{i}^{j})\), for the appropriate \(i,j[K][p]\), is necessary and sufficient for agnostic online ranking learnability.

We now begin the proof of Theorem 5.1. Since the proof of Theorem 5.2 follows a similar trajectory, we defer all discussion of Theorem 5.2 to Appendix D.2. Unlike Theorem 4.1 in the batch setting, we prove the sufficiency and necessity directions of Theorem 5.1 directly. We chose this direct path because, unlike the batch setting, sequential Rademacher analysis does not yield a constructive algorithm (Rakhlin et al., 2015). On the other hand, our proofs are constructive and use the celebrated Randomized Exponential Weights Algorithm (REWA) (Cesa-Bianchi and Lugosi, 2006). Moreover, a key ingredient of our proof is the realizable to agnostic conversion from Raman et al. (2023).

Proof.: (of sufficiency in Theorem 5.1) Fix \((_{}^{ p})\). Let \(a=_{,y}\{(,y)\;\;(,y) 0\}\) and \(M=_{,y}(,y)\). Given online learners for \(_{i}^{j}\) for the 0-1 loss, our goal is to construct an online learner \(\) for \(\) w.r.t \(\) that enjoys sub-linear regret in \(T\). Our strategy will be to construct a set of experts \(\) using the online learners for \(_{i}^{j}\)'s and run REWA using \(\) and an appropriately scaled version of \(\). Our proof borrows ideas from the realizable-to-agnostic online conversion from Raman et al. (2023) and so we use the same notation whenever possible.

Let \((x_{1},y_{1}),...,(x_{T},y_{T})()^{T}\) denote the stream of points to be observed by the online learner. We will assume an oblivious adversary and thus the stream is fixed before the game starts. A standard reduction (Chapter 4 in Cesa-Bianchi and Lugosi (2006)) allows us to convert oblivious regret bounds to adaptive regret bounds. Since \(_{i}^{j}\{0,1\}^{}\) is online learnable w.r.t. \(0\)-\(1\) loss, we are guaranteed the existence of online learners \(_{i}^{j}\) for \(_{i}^{j}\).

**Constructing Experts**. For any bitstring \(b\{0,1\}^{T}\), let \(:\{t[T]:b_{t}=1\}_{K}\) denote a function mapping time points where \(b_{t}=1\) to rankings (permutations). Let \(_{b}=_{K}^{\{t[T]:b_{t}=1\}}\) denote all such functions \(\). For every \(h\), there exists a \(_{b}^{h}_{b}\) such that for all \(t\{t:b_{t}=1\}\), \(_{b}^{h}(t)=h(x_{t})\). Let \(|b|=|\{t[T]:b_{t}=1\}|\). For every \(b\{0,1\}^{T}\) and \(_{b}\), we will define an Expert \(E_{b,}\). Expert \(E_{b,}\), formally presented in Algorithm 2, uses \(_{i}^{j}\)'s to make predictions in each round. However, \(E_{b,}\) only updates the \(_{i}^{j}\)'s on those rounds where \(b_{t}=1\), using \(\) to compute a labeled instance. For every \(b\{0,1\}^{T}\), let \(_{b}=_{_{b}}\{E_{b,}\}\) denote the set of all Experts parameterized by functions \(_{b}\). If \(b\) is the bitstring with all zeros, then \(_{b}\) will be empty. Therefore, we will actually define \(_{b}=\{E_{0}\}_{_{b}}\{E_{b,}\}\), where \(E_{0}\) is the expert that never updates \(_{i}^{j}\)'s and only uses them for predictions in all \(t[T]\). Note that \(1|_{b}|(K!)^{|b|} K^{K|b|}\).

Using these experts, Algorithm 3 presents our agnostic online learner \(\) for \(\) w.r.t \((_{}^{ p})\). We now show that \(\) enjoys sub-linear regret. We highlight that there are three sources of randomness in online learner \(\), namely the randomness of sampling \(B\), the internal randomness of \(_{i}^{j}\)'s, and the internal randomness of \(\). One may think of internal randomness as arising from the sampling step involved in the randomized predictions. Let \(A\) be the random variable associated with joint internal randomness of \(_{i}^{j}\) for all \((i,j)[K][p]\). Similarly, denote \(P\) to be the random variable associated with the internal randomness of \(\). We begin by using the guarantee of REWA.

**REWA Guarantee**. Using Theorem 21.11 in Shalev-Shwartz and Ben-David (2014) and the fact that \(B,A\) and \(P\) are mutually independent, REWA guarantees almost surely that

\[_{t=1}^{T}[((x_{t}),y_{t})|B,A] _{E_{B}}_{t=1}^{T}(E(x_{t}),y_{t})+M_{B}|)}.\]

Taking an outer expectation gives

\[[_{t=1}^{T}((x_{t}),y_{t})] [_{E_{B}}_{t=1}^{T}(E(x_{t}),y_{t}) ]+[M_{B}|)}].\]

Noting that \((x_{t})=(x_{t})\), we obtain

\[[_{t=1}^{T}((x_{t}),y_{t})] [_{E_{B}}_{t=1}^{T}( E(x_{t}),y_{t})]+[M_{B}|)}]\] \[[_{t=1}^{T}(E_{B,_{B}^{h^{}} }(x_{t}),y_{t})]+M[_{B}|)} ].\]

In the last step, we used the fact that for all \(b\{0,1\}^{T}\) and \(h\), \(E_{b,_{b}^{h}}_{b}\). Here, \(h^{}=_{h}_{t=1}^{T}(h(x_{t}),y_{t})\) is the optimal function in hindsight. First, note that \((|_{B}|) K|B|(K)\). Using Jensen's inequality gives \([_{B}|)}]K  K}\). Thus,

\[[_{t=1}^{T}((x_{t}),y_{t})] {[_{t=1}^{T}(E_{B,_{B}^{h^{}}}(x_{t}),y_{t}) ]}_{}+MK K}.\] (1)

**Upperbounding (I)**. It now suffices to upperbound \([_{t=1}^{T}(E_{B,_{B}^{h^{}}}(x_{t}),y_{t})]\). Recall that Lemma E.1 gives pointwise

\[(E_{B,_{B}^{h^{}}}(x_{t}),y_{t})(h^{}(x_{t}),y_{t}) +\ _{j}[(E_{B,_{B}^{h^{}}}(x_{t}), (h^{}(x_{t}),j))]\] (2)

where \(M=_{,y}(,y)\) and \(a=_{,y}\{(,y)\ \ (,y) 0\}\). Note that, by definition of the constant \(M\), we further get

\[(E_{B,_{B}^{h^{}}}(x_{t}),(h^{}(x_ {t}),j))  M\,\{(E_{B,_{B}^{h^{}}}(x_{t}),(h^{}(x_{t}),j))>0\}\] \[=M\,\{_{}^{0}p}(E_{B, _{B}^{h^{}}}(x_{t}),(h^{}(x_{t}),j))>0\},\]where the equality follows from the fact that \((_{}^{ p})\).

In order to upperbound the indicator above, we need to introduce some more notations. Given the realizable online learner \(_{i}^{m}\) for \((i,m)[K][p]\), an instance \(x\), and an ordered finite sequence of labeled examples \(L(\{0,1\})^{*}\), let \(_{i}^{m}(x|L)\) be the random variable denoting the prediction of \(_{i}^{m}\) on the instance \(x\) after running and updating on \(L\). For any \(b\{0,1\}^{T}\), \(h\), and \(t[T]\), let \(L_{b_{<t}}^{h}(i,m)=\{(x_{s},h_{i}^{m}(x_{s})):s<tb_{s}=1\}\) denote the _subsequence_ of the sequence of labeled instances \(\{(x_{s},h_{i}^{m}(x_{s}))\}_{s=1}^{t-1}\) where \(b_{s}=1\). Then, for any \(j[p]\), we have

\[\{_{}^{ p}(E_{B,_{B}^{h^{*}}}(x_{t}),(h^{}(x_{t}),j))>0\}_{i=1}^{K}_{m=1}^{p}\{ _{i}^{m}(x_{t} L_{B_{<t}}^{h^{*}}(i,m)) h_{i}^{,m}(x_{ t})\}.\]

To prove the inequality above, consider the case when \(_{i=1}^{K}_{m=1}^{p}\{_{i}^{m}(x_{t} L_{B_{<t}}^{h^{*}}(i,m)) h_ {i}^{,m}(x_{t})\}=0\) because the inequality is trivial otherwise. Then, we must have \(_{i}^{m}(x_{t} L_{B_{<t}}^{h^{*}}(i,m))=h_{i}^{,m}(x_{t})\) for all \((i,m)[K][p]\). Let \(V_{t}\{0,1\}^{K p}\) be a binary vote matrix that \(E_{B,_{B}^{h^{*}}}\) constructs in round \(t\). Then, we have \(V_{t}[i,m]=_{i}^{m}(x_{t} L_{B_{<t}}^{h^{*}}(i,m))=h_{i}^{, m}(x_{t})\) for all \((i,m)[K][p]\). Since \(h^{}(x_{t})\) is a permutation, the vote vector \(V_{t}_{p}\) must contain \(p\) labels with distinct number of non-zero votes, namely \(p,p-1,p-2,,2,1\) votes. Similarly, there must be \(K-p\) labels with exactly \(0\) votes. Thus, every \(_{t}_{x_{K}},V_{t}_{p}\) must rank label that obtained \(p\) votes as \(1\), label with \(p-1\) votes as \(2\), and so forth. In other words, we must have \(_{t}}{{=}}h^{}(x_{t})\), and thus \(_{}^{ p}(_{t},(h^{}(x_{t}),j))=0\) for any \(j[p]\) by definition of \(_{}^{ p}\). Our claim now follows because \(E_{B,_{B}^{h^{*}}}(x_{t})_{x_{K}},V_{t} _{p}\). Using these two inequalities in equation (2), we obtain

\[(E_{B,_{B}^{h^{*}}}(x_{t}),y_{t})(h^{}(x_{t}),y_{t})+ }{a}\,_{i=1}^{K}_{m=1}^{p}\{_{i}^{m }(x_{t} L_{B_{<t}}^{h^{*}}(i,m)) h_{i}^{,m}(x_{t})\},\]

which further implies that

\[[_{t=1}^{T}(E_{B,_{B}^{h^{*}}}(x_{t}),y_{t})] _{t=1}^{T}(h^{}(x_{t}),y_{t})+}{a}_{i=1}^{K} _{m=1}^{p}[_{t=1}^{T}\{_{i}^{m}(x_{t} L_{B_{<t}}^{h^{*}}(i,m)) h_{i}^{,m}(x_{t})\}]}_{ }.\]

The first term above is the cumulative loss of the best-fixed hypothesis in hindsight.

**Upperbounding (II).** It now suffices to show that \([_{t=1}^{T}\{_{i}^{m}(x_{t} L_{B_{ <t}}^{h^{*}}(i,m)) h_{i}^{,m}(x_{t})\}]\) is sub-linear for every \((i,m)[K][p]\). Note that we can write

\[[_{t=1}^{T}\{_{i}^{m}(x_ {t} L_{B_{<t}}^{h^{*}}(i,m)) h_{i}^{,m}(x_{t})\}] =_{t=1}^{T}[\{_{i}^{m}( x_{t} L_{B_{<t}}^{h^{*}}(i,m)) h_{i}^{,m}(x_{t})\}][\{B_{t}=1\}]}{[\{B_{t }=1\}]}\] \[=}\,_{t=1}^{T}[\{ _{i}^{m}(x_{t} L_{B_{<t}}^{h^{*}}(i,m)) h_{i}^{,m}(x_{ t})\}\{B_{t}=1\}],\]

where the last equality follows because \([\{B_{t}=1\}]=}{T}\) and the prediction of \(_{i}^{m}(x_{t} L_{B_{<t}}^{h^{*}}(i,m))\) on round \(t\) only depends on bitstring (\(B_{1},,B_{t-1}\)), but is independent of \(B_{t}\). Next, we can use the regret guarantee of algorithm \(_{i}^{m}\) on the rounds it was updated. That is,

\[_{t=1}^{T}[_{i}^{m}(x_{t} L_{B_{ <t}}^{h^{*}}(i,m))\{B_{t}=1\}] =[_{t:B_{t}=1}_{i}^{m}(x_{t} L _{B_{<t}}^{h^{*}}(i,m)) h_{i}^{,m}(x_{t})\}]\] \[=[[_{t:B_{t}=1}_{i}^{m} (x_{t} L_{B_{<t}}^{h^{*}}(i,m)) h_{i}^{,m}(x_{t}))] |B][R_{i}^{m}(|B|)],\]

where \(R_{i}^{m}(|B|)\) is the regret of \(_{i}^{m}\), a sub-linear function of \(|B|\). In the last step, we use the fact that \(_{i}^{m}\) is a realizable algorithm for \(_{i}^{m}\) and the feedback that the algorithm received was \((x_{t},h_{i}^{,m}(x_{t}))\)in the rounds whenever \(B_{t}=1\). Next, Lemma 5.17 from Ceccherini-Silberstein et al. (2017) guarantees that there exists a concave sub-linear function \(_{i}^{m}(|B|)\) that upperbounds \(R_{i}^{m}(|B|)\). Thus, by Jensen's inequality, \(_{B}[R_{i}^{m}(|B|)]_{B}[_{ i}^{m}(|B|)]_{i}^{m}(_{B}[|B|]) _{i}^{m}(T^{})\), a sub-linear function of \(T^{}\).

Combining (I) and (II) together, we obtain

\[[_{t=1}^{T}((x_{t}),y_{t})] _{h}_{t=1}^{T}(h(x_{t}),y_{t})+}{a}_{i=1 }^{K}_{m=1}^{p}}\,_{i}^{m}(T^{})+MK K}.\]

Since \(_{i}^{m}(T^{})\) is a sublinear function of \(T^{}\), we have that \(}_{i}^{m}(T^{})\) is a sublinear function of \(T\). As the sum of sublinear functions is sublinear, the second term above must be a sublinear function of \(T\). The regret is sub-linear for any choice of \((0,1)\). This completes our proof as we have shown that the algorithm \(\) achieves sub-linear regret in \(T\). 

The proof of the necessity direction of Theorem 5.1 also involves constructing experts and running the REWA algorithm. Since the argument is similar, we defer details to Appendix D.1.

## 6 Discussion

In this paper, we characterize the learnability of a multilabel ranking hypothesis class in both the batch and online setting for a wide range of practical ranking losses. In all cases, we show that a ranking hypothesis class is learnable if and only if a sufficient number of its binary-valued threshold restrictions are learnable. Our paper studies two families of ranking loss functions and leaves it open to characterize the learnability of other natural ranking loss functions. One loss function not captured by our families is recall@p.

While we do establish quantitative bounds on the sample complexity and regret, our bounds are not optimal. It may be difficult to improve the sample complexity and regret bound at the highest level of generality for all losses in the families considered here. However, for natural losses such as sum loss, it is an interesting future direction to derive the optimal sample complexity and regret bounds in both the realizable and agnostic settings. In addition, our bounds depend on the number of labels \(K\). Recently, \(K\)-free bounds have been achieved for multiclass classification problems in both batch and online settings (Brukhim et al., 2022; Hanneke et al., 2023). An interesting future direction is to study whether \(K\)-free bounds are possible for multilabel ranking.

Finally, the focus of this paper is on characterizing learnability, and thus our algorithms are not computationally efficient. A natural future direction is to construct computationally efficient algorithms for multilabel ranking. Along this direction, since ERM is the most common algorithm used in practice, it is an important future direction to tightly quantify the sample complexity of ERM in the batch setting. Moreover, in learning theory, combinatorial dimensions play an important role in providing a tight quantitative characterization of learnability. Thus, it is an interesting future direction to identify combinatorial dimensions that characterize multilabel ranking learnability for specific loss functions.