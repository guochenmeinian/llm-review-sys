# Confident Natural Policy Gradient for Local Planning in \(q_{\pi}\)-realizable Constrained MDPs

# Confident Natural Policy Gradient for Local Planning in \(q_{}\)-realizable Constrained MDPs

 Tian Tian

University of Alberta, Edmonton

ttian@ualberta.ca

&Lin F. Yang

University of California, Los Angeles

linyang@ee.ucla.edu

Corresponding author.

Csaba Szepesvari

University of Alberta, Google DeepMind, Edmonton

szepesva@ualberta.ca

Corresponding author.

###### Abstract

The constrained Markov decision process (CMDP) framework emerges as an important reinforcement learning approach for imposing safety or other critical objectives while maximizing cumulative reward. However, the current understanding of how to learn efficiently in a CMDP environment with a potentially infinite number of states remains under investigation, particularly when function approximation is applied to the value functions. In this paper, we address the learning problem given linear function approximation with \(q_{}\)-realizability, where the value functions of all policies are linearly representable with a known feature map, a setting known to be more general and challenging than other linear settings. Utilizing a local-access model, we propose a novel primal-dual algorithm that, after \(((d)^{-3})\)1 queries, outputs with high probability a policy that strictly satisfies the constraints while nearly optimizing the value with respect to a reward function. Here, \(d\) is the feature dimension and \(>0\) is a given error. The algorithm relies on a carefully crafted off-policy evaluation procedure to evaluate the policy using historical data, which informs policy updates through policy gradients and conserves samples. To our knowledge, this is the first result achieving polynomial sample complexity for CMDP in the \(q_{}\)-realizable setting.

## 1 Introduction

In the classical reinforcement learning (RL) framework, optimizing a single objective above all else can be challenging for safety-critical applications like autonomous driving, robotics, and Large Language Models (LLMs). For example, it may be difficult for an LLM agent to optimize a single reward that fulfills the objective of generating helpful responses while ensuring that the messages are harmless (Dai et al., 2024). In autonomous driving, designing a single reward often requires reliance on complex parameters and hard-coded knowledge, making the agent less efficient and adaptive (Kamran et al., 2022). Optimizing a single objective in motion planning involves combining heterogeneous quantities like path length and risks, which depend on conversion factors that are not necessarily straightforward to determine (Feyzabadi and Carpin, 2014).

The constrained Markov decision process (CMDP) framework (Altman, 2021) emerges as an important RL approach for imposing safety or other critical objectives while maximizing cumulativereward (Wachi and Sui, 2020; Dai et al., 2024; Kamran et al., 2022; Wen et al., 2020; Girard and Reza Emami, 2015; Feyzabadi and Carpin, 2014).

In addition to the single reward function optimized under a standard Markov decision process (MDP), CMDP considers multiple reward functions, with one designated as the primary reward function. The goal of a CMDP is to find a policy that maximizes the primary reward function while satisfying constraints defined by the other reward functions. Although the results of this paper can be applied to multiple constraint functions, for simplicity of presentation, we consider the CMDP problem with only one constraint function.

Our current understanding of how to learn efficiently in a CMDP environment with a potentially infinite number of states remains limited, particularly when function approximation is applied to the value functions. Most works studying the sample efficiency of a learner have focused on the tabular or simple linear CMDP setting (see related works for more details). However, there has been little work in the more general settings such as the \(q_{}\)-realizability, which assumes the value function of all policies can be approximated by a linear combination of a feature map with unknown parameters. Unlike Linear MDPs (Yang and Wang, 2019; Jin et al., 2020), where the transition model is assumed to be linearly representable by a feature map, \(q_{}\)-realizability only imposes the assumption on the existence of a feature map to represent value functions of policies.

Nevertheless, the generality of \(q_{}\)-realizability comes with a price, as it becomes considerably more challenging to design effective learning algorithms, even for the unconstrained settings. For the general online setting, we are only aware of one sample-efficient MDP learning algorithm (Weisz et al., 2023), which, however, is computationally inefficient. To tackle this issue, a line of research (Kearns et al., 2002; Yin et al., 2022; Hao et al., 2022; Weisz et al., 2022) applies the _local-access model_, where the RL algorithm can restart the environment from any visited states - a setting that is also practically motivated, especially when a simulator is provided. The local-access model is more general than the generative model (Kakade, 2003; Sidford et al., 2018; Yang and Wang, 2019; Lattimore et al., 2020; Vaswani et al., 2022), which allows visitation to arbitrary states in an MDP. The local-access model provides the ability to unlock both the sample and computational efficiency of learning with \(q_{}\)-realizability for the unconstrained MDP settings. However, it remains unclear whether we can harness the power of local-access for CMDP learning.

In this paper, we present a systematic study of CMDP for large state spaces, given \(q_{}\)-realizable function approximation in the local-access model. We summarize our contributions as follows:

* We design novel, computationally efficient primal-dual algorithms to learn CMDP near-optimal policies with the local-access model and \(q_{}\)-realizable function classes. The algorithms can return policies with small constraint violations or even no constraint violations and can handle model misspecification.
* We provide theoretical guarantees for the algorithms, showing that they can compute an \(\)-optimal policy with high probability, making no more than \(((d)^{-3})\) queries to the local-access model. The returned policies can strictly satisfy the constraint.
* Under the misspecification setting with a misspecification error \(\), we show that our algorithms achieve an \(()+\) sub-optimality with high probability, maintaining the same sample efficiency of \(((d)^{-3})\).

## 2 Related works

Most provably efficient algorithms developed for CMDP are in the tabular and linear MDP settings. In the tabular setting, most notably are the works by (Efroni et al., 2020; Liu et al., 2021; Zheng and Ratliff, 2020; Vaswani et al., 2022; Kalagarla et al., 2021; Yu et al., 2021; Gattami et al., 2021; HasanzadeZonuzy et al., 2021; Chen et al., 2021; Kitamura et al., 2024). Work by Vaswani et al. (2022) have showed their algorithm uses no more than \((^{2}})\) samples to achieve relaxed feasibility and \((^{2}^{2}})\) samples to achieve strict feasibility. Here, the \([0,1)\) is the discount factor and \((0,]\) is the Slater's constant, which characterizes the size of the feasible region and hence the hardness of the CMDP. In their work, they have also provided a lower bound of \((^{2}^{2}})\) on the sample complexity under strict feasibility. However, all the aforementioned results all scale polynomially with the cardinality of the state space.

For problems with large or possibly infinite state spaces, works by (Jain et al., 2022; Ding et al., 2021; Miryoosefi and Jin, 2022; Ghosh et al., 2024; Liu et al., 2022) have used linear function approximations to address the curse of dimensionality. All these works, except Jain et al. (2022); Liu et al. (2022), make the linear MDP assumption, where the transition function is linearly representable.

Under the generative model, for the infinite horizon discounted case, the online algorithm proposed in Jain et al. (2022) achieves a regret of \((/)\) with \((/)\) constraint violation, where \(K\) is the number of iterations. Work by Liu et al. (2022) is able to achieve a faster \(O((K)/K)\) convergence rate for both the reward suboptimality and constraint violation. For the online access setting under linear MDP assumption, Ding et al. (2021); Ghosh et al. (2024) achieve a regret of \(((d)\,(H))\) with \(((d)\,(H)))\) violations, where \(T\) is the number of episodes and \(H\) is the horizon term.

Miryoosefi and Jin (2022) presented an algorithm that achieves a sample complexity of \((H^{6}}{^{2}})\), where \(d\) is the dimension of the feature space and \(H\) is the horizon term in the finite horizon CMDP setting. In the more general setting under \(q_{}\)-realizability, the best-known upper bounds are in the unconstrained MDP setting.

In the unconstrained MDP setting with access to a local-access model, early work by Kearns et al. (2002) have developed a tree-search style algorithms under this model, albeit in the tabular setting. Under \(v^{*}\)-realizability, Weisz et al. (2021) presented a planner that returns an \(\)-optimal policy using \(O((dH/)^{||})\) queries to the simulator. More works by (Yin et al., 2022; Hao et al., 2022; Weisz et al., 2022) have considered the local-access model with \(q_{}\)-realizability assumption. Recent work by Weisz et al. (2022) have shown their algorithm can return a near-optimal policy that achieves a sample complexity of \((^{2}})\).

## 3 Problem formulation

### Constrained MDP

We consider an infinite-horizon discounted CMDP \((,,P,r,c,,b,s_{0})\) consisting a possibly countably infinite state space \(\) with a finite set of actions \(\), a reward function \(r:\), a constraint function \(c:\), a discount factor \([0,1)\), a constraint threshold \(b 0\), and a fixed initial state \(s_{0}\). Let \(_{1}(X)\) denote the space of probability distributions supported on the set \(X\). Then, the transition probability \(P:_{1}()\).

Define a set of stationary randomized policies \(_{}\), and a policy \(_{}\) maps states to probability distributions over the actions (i.e., \(:_{1}()\)). Given a \(_{}\), the policy \(\) interacts with the CMDP starting from any state \(s\) through discrete steps indexed by \(t_{0}\), where \(_{0}=\{0,1,2,\}\). This interaction generates a trajectory of \(\{S_{t},A_{t}\}_{t_{0}}\), where \(S_{0}=s,A_{t}(|S_{t})\), and \(S_{t+1} P(|S_{t},A_{t})\). The reward action-value function is defined as \(q_{}^{r}(s,a)=[_{t=0}^{}^{t}r(S_{t},A_{t})|S_{ 0}=s,A_{0}=a]\). Similarly, the constraint action-value function is defined as \(q_{}^{c}(s,a)=[_{t=0}^{}^{t}c(S_{t},A_{t})|S_ {0}=s,A_{0}=a]\). The reward state-value function \(v_{}^{r}(s)=(|s),q_{}^{r}(s,)\), where \(,\) denotes the inner product over actions. Likewise, the constraint state-value function \(v_{}^{c}(s)=(|s),q_{}^{c}(s,)\).

The objective of the CMDP is to find a policy \(\) that maximizes the state-value function \(v_{}^{r}\) starting from a given state \(s_{0}\), while ensuring that the constraint \(v_{}^{c}(s_{0}) b\) is satisfied:

\[_{_{}}v_{}^{r}(s_{0}) s.t. v_{}^{c}(s_{0 }) b.\] (1)

We assume the existence of a feasible solution to eq. (1) and let \(^{*}\) denote a solution to eq. (1). A quantity unique to CMDP is the Slater's constant, which is denoted as \(=_{}v_{}^{c}(s_{0})-b\). Slater's constant characterizes the size of the feasibility region, and hence the hardness of the problem.

Because the state space can be large or possibly infinite, we use linear function approximation to approximate the values of stationary randomized policies. Let \(:^{d}\) be a feature map. We assume that both \(q_{}^{r}\) and \(q_{}^{c}\) satisfy the following condition:

**Assumption 1**: _(\(q_{}\)-realizability) There exists \(B>0\) and a misspecification error \( 0\) such that for every \(_{}\), there exists a weight vector \(w_{}^{d}\), \(\|w_{}\|_{2} B\), and ensures \(|q_{}(s,a)- w_{},(s,a)|\) for all \((s,a)\)._

A **mixture policy** is defined as a policy randomly selected from a finite set of policies \(\{_{0},,_{K}\}\) and executed for all subsequent steps. For example, a mixture policy \(_{K}\) is constructed by sampling a policy \(_{k}\) with probability \(\) and following it. The value function of such mixture policy for state \(s\) is given by \(v_{_{K}}(s)=_{k=0}^{K-1}v_{_{k}}(s)\), where \(v_{_{k}}(s)\) is the value function of the individual policy \(_{k}\). Note that \(_{K}\) is a non-stationary policy, and the set of non-stationary policies includes the set of stationary randomized policies \(_{}\).

We assume access to a local access model, where the agent can query the simulator only for states that have been encountered during previous simulations. Our goal is to design an algorithm that outputs a near-optimal mixture policy \(_{K}\), whose performance can be characterized in one of two ways.

For a given target error \(>0\), the **relaxed feasibility** requires the returned policy \(_{K}\) whose sub-optimality gap \(v^{r}_{^{*}}(s_{0})-v^{r}_{_{K}}(s_{0})\) is bounded by \(\), while allowing for a small constraint violation. Formally, we require \(_{K}\) such that

\[v^{r}_{^{*}}(s_{0})-v^{r}_{_{K}}(s_{0}) s.t v ^{c}_{_{K}}(s_{0}) b-.\]

On the other hand, **strict-feasibility** requires the returned policy \(_{K}\) whose sub-optimality gap \(v^{r}_{^{*}}(s_{0})-v^{r}_{_{K}}(s_{0})\) is bounded by \(\) while not allowing any constraint violation. Formally, we require \(_{K}\) such that

\[v^{r}_{^{*}}(s_{0})-v^{r}_{_{K}}(s_{0}) s.t v ^{c}_{_{K}}(s_{0}) b.\]

#### Notations

For any real number \(a\), we let \( a\) to denote the smallest integer \(i\) such that \(i a\). For vector \(x^{d}\), let \(\|x\|_{1}=_{i}|x_{i}|\), \(\|x\|_{2}=x_{i}^{2}}\), and \(\|x\|_{}=_{i}|x_{i}|\). For a positive definite matrix \(A^{d d}\), the \(\|x\|_{A}^{2}=x^{}Ax\). We let \(_{[a_{1},a_{2}]}()=_{p[a_{1},a_{2}]}| -p|\), and \(_{[a_{1},a_{2}]}(y)=\{\{y,a_{1}\},a_{2}\}\). For any two positive numbers \(a,b\), we write \(a=O(b)\) if there exists an absolute constant \(c>0\) such that \(a cb\). We use the \(\) to hide any polylogarithmic terms.

## 4 Confident-NPG-CMDP, a local-access algorithm for CMDP

In this section, we introduce a primal-dual algorithm, which we call _Confident-NPG-CMDP_ (see algorithm 1).

### A primal-dual approach

We approach solving the CMDP problem by framing it as an equivalent saddle-point problem:

\[_{}_{ 0}L(,),\]

where \(L:_{}_{+}\) is the Lagrange function. For a policy \(_{}\) and a Lagrange multiplier \(_{+}\), we have

\[L(,)=v^{r}_{}(s_{0})+(v^{c}_{}(s_{0})-b).\]

Let \((^{*},^{*})\) be a solution to this saddle-point problem. By an equivalence to a LP formulation and strong duality (Altman, 2021), \(^{*}\) is the policy that achieves the optimal value in the CMDP as defined in eq. (1). An optimal Lagrange multiplier \(^{*}_{ 0}L(^{*},)\), Therefore, solving eq. (1) is equivalent to finding a saddle-point of the Lagrange function.

A typical primal dual algorithm that finds the saddle-point will proceed in an iterative fashion alternating between a policy update using policy gradient and a dual variable update using mirror descent. The policy gradient is computed with respect to the primal value \(q^{p}_{_{k},_{b}}=q^{r}_{_{k}}+_{k}q^{c}_{_{k}}\) and the mirror descent is computed with respect to the constraint value \(v^{c}_{_{k}}(s_{0})=_{k}(|s_{0}),q^{c}_{_{k}}(s_{0}, )\).

```
1:Input:\(s_{0}\) (initial state), \(\) (target accuracy), \((0,1]\) (failure probability); \(\) (discount factor)
2:Initialize:
3: Define \(K,_{1},_{2},m\) according to Theorem 1 for relaxed-feasibility and Theorem 2 for strict-feasibility,
4: Set \(L K/( m+1)\).
5: For each iteration \(k\{0,, K\}:_{k}(),\ _{k}^{p}(,) 0,\ _{k}^{c}() 0\), and \(_{k} 0\).
6: For each phase \(l\{0,,L+1\}:_{l}(),\ D_{l}\{\}\)
7: For \(a\): if \((s_{0},a)(_{0})\), then append \((s_{0},a)\) to \(_{0}\) and set \(\) to \(D_{0}[(s_{0},a)]\) see ActionCov defined in eq. (4)
8:while True do\(\) main loop
9: Let \(\) be the smallest integer s.t. \(D_{}[z^{}]=\) for some \(z^{}_{}\)
10: Let \(z\) be the first state-action pair in \(_{}\) s.t. \(D_{}[z]=\)
11: If \(=L+1\), then return \(_{K}\)
12:\(k_{}( m+1)\)\(\) iteration corresponding to phase \(\)
13:\((result,discovered)\) Gather-data(\(_{k_{}},_{},,z\))
14:if\(discovered\) is True then
15: Append \(result\) to \(_{0}\) and set \(\) to \(D_{0}[result]\)\(\)\(result\) is a state-action pair
16: Goto line 8
17:\(D_{}[z] result\)
18:if\( z^{}_{}\) s.t. \(D_{}[z^{}]=\)then
19:\(k_{+1} k_{}+( m+1)\) if \(k_{}+( m+1) K\) otherwise \( K\)
20:for\(k=k_{},,k_{+1}-1\)do\(\) off-policy iterations reusing \(_{},D_{}\)
21:\(Q_{k}^{r},\ Q_{k}^{c} LSE(_{},D_{},_{k},_ {k_{}})\)
22: For \(s(_{}) Cov(_{+1})\), and for \(a\)
23:\(_{k}^{p}(s,a)_{[0,]}Q _{k}^{r}(s,a)+_{k}_{[0,]}Q_{k}^ {c}(s,a)\)
24:\(_{k}^{c}(s)_{[0,]} _{k}(|s),Q_{k}^{c}(s,)\)
25:\(\) update policy
26: For \(s,a\):
27:\(_{k+1}(a|s)_{k+1}(a|s)&s (_{+1})\\ _{k}(a|s)_{k}^{p}(s,a))}{_{a^{} }_{k}(a^{}|s)(_{1}_{k}^{p}(s,a^{})) }&\)
28:\(\) update dual variable
29:\(_{k+1}_{k+1}&s_{0} (_{+1})\\ _{[0,U]}(_{k}-_{2}(_{k}^{c}(s_{0}) -b))&.\)
30: For \(z_{}\) s.t. \(z_{+1}\): append \(z\) to \(_{+1}\) and set \(\) to \(D_{+1}[z]\) ```

**Algorithm 1** Confident-NPG-CMDPGiven that we do not have access to an oracle for exact policy evaluations, we must collect data to estimate the primal and constraint values. If we have the least-squares estimates of \(q^{r}_{_{k}}\) and \(q^{c}_{_{k}}\), denoted by \(Q^{r}_{k}\) and \(Q^{c}_{k}\), respectively, then we can compute the least-squares estimate \(Q^{p}_{k}=Q^{r}_{k}+_{k}Q^{c}_{k}\) to be the estimate of the primal value \(q^{p}_{_{k},_{k}}\). Additionally, we can compute \(V^{c}_{k}(s_{0})=_{k}(|s_{0}),Q^{c}_{k}(s_{0},)\) to be the least-squares estimate of the constraint value \(v^{c}_{_{k}}(s_{0})\). Then, for any given \((s,a)\), our algorithm makes a policy update of the following form:

\[_{k+1}(a|s)_{k}(a|s)(_{1}Q^{p}_{k}(s,a)),\] (2)

followed by a dual variable update of the following form:

\[_{k+1}_{k}-_{2}(V^{c}_{k}(s_{0})-b),\]

where the \(_{1}\) and \(_{2}\) are the step-sizes.

### Core set and least square estimates

To construct the least-squares estimates, let us assume for now that we are given a set of state-action pairs, which we call the core set \(\). By organizing the feature vector of each state-action pair in \(\) row-wise into a matrix \(_{}^{|| d}\), we can write the covariance matrix as \(V(,)=_{}^{}_{}+ I\). For each \((s,a)\), suppose we have run Monte Carlo rollouts using the rollout policy \(\) with the local access simulator to obtain an averaged Monte Carlo return denoted by \((s,a)\). Then we gather all the state-action pairs into a vector \(^{||}\). For any state-action pair \((s,a)\), the least-square estimate of action-value \(q_{}\) is defined to be

\[Q(s,a)=(s,a),V(,)^{-1}_{}^{} .\] (3)

Since the algorithm can only rely on estimates for policy improvement and constraint evaluation, it is imperative that these estimates closely approximate their true action values. In the local access setting, an algorithm may not be able to visit all state-action pairs, so we cannot guarantee that the estimates will closely approximate the true action values for all state-action pairs. However, we can ensure the accuracy of the estimates for a subset of states.

Given \(\), let us define a set of state-action pairs whose features satisfies the condition \(\|(s,a)\|_{V(,)^{-1}} 1\), then we call this set the action-cover of \(\):

\[()=\{(s,a):\|( s,a)\|_{V(,)^{-1}} 1\}.\] (4)

Following from the action-cover, we have the cover of \(\). For a state \(s\) to be in the cover of \(\), all its actions \(a\), the pair \((s,a)\) is in the action-cover of \(\). In other words,

\[()=\{s: a,(s,a) ()\}.\]

For any \(s()\), we can ensure the least square estimate \(Q(s,a)\) defined by eq. (3) closely approximates its true action value \(q_{}(s,a)\) for all \(a\). However, such a core set \(\) is not available before the algorithm is run. Therefore, we need an algorithm that will build a core set incrementally in the local-access setting while planning. To achieve this, we build our algorithm on CAPI-QPI-Plan (Weisz et al., 2022), using similar methodology for core set building and data gathering.

### Core set building and data gathering to control the accuracy of the least-square estimates

Confident-NPG-CMDP does not collect data in every iteration but collects data in interval of \(m=O((1+_{0})(^{-1}(1-)^{-1}))\), where \(_{0} 0\) is an user defined constant. During each data collection phase, the algorithm performs on-policy evaluation. Between these phases, it conducts \(( m+1)\) off-policy evaluations, reusing data from the most recent on-policy iteration.

By setting \(_{0}\) to a positive value, we impose an upper bound of \(1+_{0}\) on the per-trajectory importance sampling ratio used in off-policy evaluations, and \(m\) is adjusted accordingly to maintain this bound. The total number of data collection phases is \(L= K/( m+1)\), where \(K\) is the total number of iterations. When \(_{0}\) is set to zero, we have \(L=K\), resulting in a purely on-policy version of the algorithm.

Confident-NPG-CMDP maintains a set of core sets \(\{_{l}\}_{l=0}^{L+1}\), one for each data collection phases. Each core set \(_{l}\) is a list of state-action pairs. Due to the off-policy evaluations, Confident-NPG-CMDP also maintains a set of data sets \(\{D_{l}\}_{l=0}^{L}\). Initially, all core sets are empty, all policies are initialized to the uniform policy, and all data sets are empty.

The algorithm begins by adding the feature vectors corresponding to \((s_{0},a)\) for all actions \(a\) that are not in the action-cover of \(_{0}\). These feature vectors are considered informative. For every \((s,a)_{0}\), the algorithm adds an entry to \(D_{0}\) and sets its value to the placeholder \(\), indicating that there is no roll-out data yet. Then, in line 9 of algorithm 1, the algorithm finds the smallest integer \(l\{0,,L\}\) such that the corresponding \(D_{l}\) has an entry without roll-out data (i.e., it contains the placeholder \(\)). When such a phase is found, a running phase begins, denoted by \(\) in algorithm 1. We note that when \(=L+1\), the algorithm returns and no roll-outs are stored.

Since only one running phase \(\) can be active at a time, and \(\) can only take value \(l\{0,,L\}\), the algorithm updates the policies of the corresponding iterations in line 27, updates the dual variables of these iterations in line 29, and extends the core set for the next phase in line 30.

Suppose during a running phase with \(=l\), while performing the roll-out in Gather-data subroutine (algorithm 3 in Appendix A), if any state-action pair \((s,a)\) is not in the action-cover of \(_{}\), the current running phase stops and the newly discovered state-action pair is added to \(_{0}\) in line 15. The same state-action pair is then propagated to \(_{1}\) and so on by line 30.

Once a state-action pair is added to a core set by line 7, line 15, and line 30, it remains in that core set for the duration of the algorithm. This means that any \(_{l}\), \(l\{0,,L+1\}\) can grow in size and be extended multiple times during the execution of the algorithm. When any new state-action pair is added to a core set, the least-square estimate should be recomputed with the newly added information. This implies that the policy needs to be updated and data re-collected. However, we can avoid restarting the entire data collection procedure by updating only the policy for states that are newly added to the extended core set. We elaborate on this approach further in the next paragraph.

When the algorithm enters the running phase \(=l\), and the Gather-data subroutine returns, the LSE subroutine (algorithm 4) computes the least-squares estimate \(Q_{k}^{r},Q_{k}^{}\) using the most recently extended core set \(_{}\) for each corresponding iteration \(k=k_{},,k_{+1}-1\). Subsequently, \(_{k}^{p}\) of line 23 of algorithm 1 is updated with the newly updated least-square estimates \(Q_{k}^{r},Q_{k}^{}\). However, the policy \(_{k+1}\) will only be updated for states that are newly covered by \(_{}\) (i.e., \(s(_{})( _{+1})\)). For any states that are already covered by \(_{}\) (i.e., \(s(_{+1})\)), the policy remains unchanged from its previous update using the \(^{p}\) at that time. By updating the policy in this manner, the accuracy guarantee of \(_{k}^{p}(s,a)\) with respect to \(q_{_{k},_{k}}^{p}(s,a)\) is ensured not just for \(_{k}\), but for an extended set of policies defined as follows:

**Definition 1**: _For any policy \(\) from the set of randomized policies \(_{rand}\) and any subset \(\), the extended set of policies is defined as:_

\[_{,}=\{^{}_{}(|s)=^{ }(|s)s\}.\]

By maintaining a set of core sets, gathering data via the Gather-data subroutine (algorithm 3 in Appendix A), making policy updates by line 27, and dual variable updates by line 29, we have:

**Lemma 1**: _Whenever LSE subroutine in line 21 of Confident-NPG-CMDP is executed during a running phase \(=l\) for \(l\{0,,L\}\), the least-square estimate \(_{k}^{p}(s,a)\) satisfies the following condition for all iterations \(k=k_{},,k_{+1}-1\) associated with this phase and for all \(s(_{})\) and \(a\),_

\[|_{k}^{p}(s,a)-q_{_{k}^{p},_{k}}^{p}(s,a)|^{ }_{k}^{}_{_{k},( _{})},\] (5)

_where \(^{}=(1+U)(+B+(+)})\) with \(=(d)\) and \(U\) is an upper bound on the optimal Lagrange multiplier. Similarly, for initial state \(s_{0}\), we have_

\[|_{k}^{c}(s_{0})-v_{_{k}^{c}}^{c}(s_{0})|+B+ (+)}_{k}^{}_{_{k}, (_{})}.\] (6)

The accuracy guarantee of eq. (5) and eq. (6) are maintained throughout the execution of the algorithm. By lemma 4.5 of Weisz et al. (2022) (restated in lemma 6 in Appendix A), for any past version of \(_{l}\) and the corresponding policy \(_{k}^{}\) associated with \(_{l}^{}\), we have \(_{_{k},(_{l})}_{_{k}^{}, (_{l}^{})}\). This means that if eq.5 and eq.6 hold true for any policy in \(_{_{k}^{},(_{l}^{})}\), they will also hold true for any future updated policy \(_{k}\).

### Differences between Confident-NPG-CMDP and CAPI-QPI-Plan

CAPI-QPI-Plan is designed for unconstrained MDPs and returns a deterministic policy, which may not be feasible in the constrained setting. In contrast, Confident-NPG-CMDP returns a soft mixture policy \(_{K}\), ensuring that \(_{K}(a|s)>0\) for all \((s,a)\).

In constrained MDPs, controlling the dual variable via mirror descent adds an \(^{-2}\) factor to the sample complexity. Directly applying CAPI-QPI-Plan would increase the complexity to \((^{-4})\) due to the need to manage both the dual variable and estimation error. To address this, Confident-NPG-CMDP employs the natural policy gradient for policy improvement and leverages the softmax policy structure to perform off-policy estimation, thereby reducing the complexity to \((^{-3})\).

By employing a per-trajectory importance sampling ratio, we weigh the Monte Carlo returns generated from data collected in earlier on-policy phases, resulting in unbiased estimates of action values with respect to the target policy. However, this ratio can become large if there is a substantial difference between the on-policy and target policies. To mitigate this, the algorithm collects data at intervals of \(m\), effectively determining when to gather new data as the policy significantly diverges from an earlier recent data-gathering iteration. By setting \(_{0}>0\), we can bound the per-trajectory importance sampling ratio, thus controlling the interval \(m\) for resampling on-policy data to produce well-controlled estimators.

Key algorithmic differences between Confident-NPG-CMDP and CAPI-QPI-Plan:

1. **Policy Improvement Step:** Confident-NPG-CMDP utilizes a softmax over the estimated action-values, whereas CAPI-QPI-Plan employs a greedy approach.
2. **Dual Variable Computation:** Confident-NPG-CMDP requires computation of the dual variable inherent in primal-dual algorithms.
3. **Data Sampling Strategy:** Unlike CAPI-QPI-Plan, Confident-NPG-CMDP does not sample data at every iteration but collects data at specific intervals to control the importance sampling ratio.

In the next two sections, we will demonstrate how these changes ensure a feasible mixture policy for the CMDP and address the additional analytical challenges.

## 5 Confident-NPG-CMDP satisfies relaxed-feasibility

With the accuracy guarantee of the least-square estimates, we prove that at the termination of Confident-NPG-CMDP, the returned mixture policy \(_{K}\) satisfies relaxed-feasibility. We note that because of the execution of line 30 in algorithm 1, at termination, one can show using induction that \(_{0}=_{1}==_{L+1}\). Therefore, \((_{0})=(_{1})==( _{L})\). Thus, it is sufficient to only consider \(_{0}\) at the termination of the algorithm. By line 7 of algorithm 1, we have ensured \(s_{0}(_{0})\).

By employing the primal-dual approach discussed in section4, we reduce the CMDP problem to an unconstrained problem with a single reward function of the form \(r_{}=r+ c\). Therefore, we can apply lemma12 from the Confident-NPG algorithm in the single-reward setting (see AppendixA) to our Confident-NPG-CMDP algorithm, replacing \(\) with \(^{*}\). Consequently, the value difference between \(^{*}\) and \(_{K}\) can be bounded, which leads to:

**Lemma 2**: _Let \((0,1]\) be the failure probability, \(>0\) be the target accuracy, and \(s_{0}\) be the initial state. Assuming for all \(s(_{0})\) and all \(a\), \(|_{k}^{p}(s,a)-q_{_{k}^{},_{k}}^{p}(s,a)|^ {}\) and \(|_{k}^{c}(s_{0})-v_{_{k}^{}}^{c}(s_{0})|+B+(+)}\) for all \(_{k}^{}_{_{k},(_{0})}\), then, with probability \(1-\)_Confident-NPG-CMDP returns a mixture policy \(_{K}\) that satisfies the following,_

\[v^{r}_{^{*}}(s_{0})-v^{r}_{_{K}}(s_{0})}{1-}++1)(1+U)}{(1-)^{2} {K}},\] \[b-v^{c}_{_{K}}(s_{0})[b-v^{c}_{_{K}}(s_{0}) ]_{+}}{(1-)(U-^{*})}++1)(1+U)}{(1-)^{2}(U-^{*})},\]

_where \(^{}=(1+U)(+(B+(+)}))\) with \(=(d)\), and \(U\) is an upper bound on the optimal Lagrange multiplier._

By setting the parameters to appropriate values, it follows from lemma 2 that we obtain the following result:

**Theorem 1**: _With probability \(1-\), the mixture policy \(_{K}\) returned by confident-NPG-CMDP ensures that_

\[v^{r}_{^{*}}(s_{0})-v^{r}_{_{K}}(s_{0}) =((1-)^{-2}^{-1})+,\] \[v^{c}_{_{K}}(s_{0})  b-(((1-)^{-2}^{-1})+ ).\]

_if we choose \(n=(^{-2}^{-2}(1-)^{-4}d)\), \(=O(^{2}^{2}(1-)^{4})\), \(K=(^{-2}^{-2}(1-)^{-6})\), \(_{1}=((1-)^{2} K^{-1/2})\), \(_{2}=^{-1}K^{-1/2}\), \(H=((1-)^{-1})\), \(m=(^{-1}^{-2}(1-)^{-2})\), and \(L= K/( m+1)=(^{-1}(1-) ^{-4})\) total number of data collection phases._

_Furthermore, the algorithm utilizes at most \((^{-3}^{-3}d^{2}(1-)^{-11})\) queries in the local-access setting._

**Remark 1:** In the presence of misspecification error \(>0\), the reward suboptimality and constraint violation is \(()+\) with the same sample complexity.

**Remark 2:** Suppose the Slater's constant \(\) is much smaller than the suboptimality bound of \(()+\), and it is reasonable to set \(=\). Then, the sample complexity is \((^{-6}(1-)^{-11}d^{2})\), which is independent of \(\).

**Remark 3:** Our algorithm requires the Slater's constant \(\), which can be estimated by running CAPI-QPI-Plan only on the constraint function \(c\), treating it as an unconstrained optimization problem. This yields an approximation of \(_{}v^{c}_{}(s_{0})\), allowing us to estimate \(\). Performing this estimation before executing Confident-NPG-CMDP adds only an additive term to the overall sample complexity.

## 6 Confident-NPG-CMDP satisfies strict-feasibility

To address the strict feasibility problem, where no constraint violations are permitted (i.e., \(v^{c}_{_{K}} b\)), the algorithm must solve a more conservative CMDP. We define a surrogate CMDP with the tuple \((,,P,r,c,,b^{},s_{0})\), where \(b^{}=b+\) for some \( 0\). Note that \(b^{} b\), imposing stricter constraints than the original problem. The optimal policy of this surrogate CMDP ensures compliance with the original constraint and is defined as follows:

\[^{*}_{} v^{r}_{}(s_{0}) s.t. v^{c}_{}( s_{0}) b^{}.\] (7)

Notice that \(^{*}_{}\) is a more conservative policy than \(^{*}\), where \(^{*}\) is the optimal policy of the original CMDP objective eq. (1). By solving this surrogate CMDP using Confident-NPG-CMDP and applying the result of theorem 1, we obtain a \(_{K}\) that would satisfy

\[v^{r}_{^{*}}(s_{0})-v^{r}_{_{K}}(s_{0}) s.t.  v^{c}_{_{K}}(s_{0}) b^{}-,\]

where \(=()+\). Expanding out \(b^{}\), we have \(v^{c}_{_{K}}(s_{0}) b+-\). If we can set \(\) such that \(- 0\), then \(v^{c}_{_{K}}(s_{0}) b\), which satisfies strict-feasibility. We show this formally in the next theorem, where \(=O((1-))\) and is incorporated into the algorithmic parameters for ease of presentation.

**Theorem 2**: _With probability \(1-\), a target \(>0\), the mixture policy \(_{K}\) returned by confident-NPG-CMDP ensures that \(v^{}_{^{*}}(s_{0})-v^{}_{_{K}}(s_{0})\) and \(v^{}_{_{K}}(s_{0}) b\), if assuming the misspecification error \(^{2}(1-)^{3}(1+})^{-1}\), and if we choose \(=O(^{2}^{3}(1-)^{5}),K=( ^{-2}^{-4}(1-)^{-8}),n=(^{-2} ^{-4}(1-)^{-8}d),H=((1-)^{-1}),m= (^{-1}^{-2}(1-)^{-3})\), and \(L= K/( m+1)=((^{-1}^{-2}(1- )^{-5}))\) total data collection phases._

_Furthermore, the algorithm utilizes at most \((^{-3}^{-6}(1-)^{-14}d^{2})\) queries in the local-access setting._

**Remark 1:** We note that by solving this conservative CMDP incurs a higher sample complexity, necessitating a separate treatment for this setting. Additionally, in the presence of a misspecification error \(>0\), the strict-feasibility setting requires additional assumptions on \(\), whereas the relaxed-feasibility setting does not. The sample complexity of the relaxed-feasibility setting can be independent of Slater's constant, whereas for strict feasibility, the returned policy must strictly adhere to constraints, and we cannot simply set Slater's constant \(\) to \(\) and disregard its impact.

## 7 A discussion on memory cost and some implementation details

The overall memory requirement is \(nH(L+1)++(L+1)(m+1)d\). The term \(nH(L+1)\) comes from maintaining \(L+1\) copies of the core sets, and each core set contains no more than \(\) state-action pairs. For each state-action pair in \(_{l}\) for \(l\{0,,L\}\), the algorithm stores \(n\) trajectories consisting of \(H\) tuples \((s,a,r,c)\).

In phase \(L+1\), the algorithm terminates, so no roll-outs are stored. The second term \(\) accounts for the elements stored in \(_{L+1}\), which has no more than \(\) elements.

Finally, the last term is the memory required to store the least-square weights of the estimator during core set extensions. Each core set \(_{l}\) can undergo up to \(\) extensions. Recall that one state-action pair is added to \(_{0}\) at a time, and subsequently propagated to \(_{1},_{2}\) and so on, ensuring that each core set contains no more than \(\) elements. During every extension of \(_{l}\), the newly added state-action pairs are marked, and up to \(m+1\) least-square weights are stored to account for the corresponding iterations associated with \(_{l}\). Since each weight vector has a dimension \(d\), and there are \(L+1\) core sets maintained this manner, the total memory required to store all least-square weights is bounded by \((L+1)(m+1)d\).

We store the least-squares weights because the algorithm must return a mixture policy, which requires access to all policies \(_{0},,_{K-1}\). Instead of storing each \(_{k}\) for \(k=0,,K-1\) across the entire state-action space, the algorithm tracks the state-action pairs newly added to each core set during extensions and saves their corresponding least-squares weights for each extension. With this stored information and the initialization of \(_{0}\), a subroutine can reconstruct the policies \(_{k}(|s)\) for any \(s\) and iteration \(k\) as needed. Please refer to Appendix E for a brief discussion on how to mark the state-action pairs, store the least-square weights, and use this information to reconstruct the policies as required.

## 8 Conclusion

We have presented a primal-dual algorithm for planning in CMDP with large state spaces, given \(q_{}\)-realizable function approximation. The algorithm, with high probability, returns a policy that achieves both the relaxed and strict feasibility CMDP objectives, using no more than \((^{-3}d^{2}(^{-1}(1-)^{-1}))\) queries to the local-access simulator.

Our algorithm does not query the simulator and collect data in every iteration. Instead, the algorithm queries the simulator only at fixed intervals. Between these data collection intervals, our algorithm improves the policy using off-policy optimization. This approach makes it possible to achieve the desired sample complexity in both feasibility settings.