# Off-Policy Selection for Initiating Human-Centric Experimental Design

Ge Gao\({}^{}\)  Xi Yang\({}^{}\)  Qitong Gao\({}^{}\)  Song Ju\({}^{}\)  Miroslav Pajic\({}^{}\)  Min Chi\({}^{}\)

Stanford University. The work was done at North Carolina State University. Contact: gegao@stanford.edu, mchi@ncsu.edu.IBM ResearchDuke UniversityNorth Carolina State University

###### Abstract

In human-centric tasks such as healthcare and education, the _heterogeneity_ among patients and students necessitates personalized treatments and instructional interventions. While reinforcement learning (RL) has been utilized in those tasks, off-policy selection (OPS) is pivotal to close the loop by offline evaluating and selecting policies without online interactions, yet current OPS methods often overlook the heterogeneity among participants. Our work is centered on resolving a _pivotal challenge_ in human-centric systems (HCSs): _how to select a policy to deploy when a new participant joining the cohort, without having access to any prior offline data collected over the participant?_ We introduce First-Glance Off-Policy Selection (FPS), a novel approach that systematically addresses participant heterogeneity through sub-group segmentation and tailored OPS criteria to each sub-group. By grouping individuals with similar traits, FPS facilitates personalized policy selection aligned with unique characteristics of each participant or group of participants. FPS is evaluated via two important but challenging applications, intelligent tutoring systems and a healthcare application for sepsis treatment and intervention. FPS presents significant advancement in enhancing learning outcomes of students and in-hospital care outcomes.

## 1 Introduction

Human-centric systems (HCSs), _e.g._, used in healthcare facilities  and intelligent education (IE) , have widely employed reinforcement learning (RL) to enhance user experience by improving outcomes of disease treatment, knowledge gaining, etc. Specifically, RL has been used in healthcare to automate treatment procedures , or in IE that can induce policies automatically adapting difficulties of course materials and helping students to setup and refine study plans to improve learning outcomes . Though various existing offline RL methods can be adopted  for policy optimization, validation of policies' performance is often conducted by online testing . Given the long testing horizon (_e.g._, several years, or semesters, in healthcare, and IE, respectively) and the high cost of recruiting participants, online testing is considered exceedingly time- and resource-consuming, and sometimes could even be hindered by protocols overseeing human involved experiments, _e.g._, performance and safety justifications need to be provided before new medical device controllers can be tested on patients .

Recently, off-policy evaluation (OPE) methods have been proposed to tackle such challenges by estimating the performance of target (evaluation) RL policies with offline data, which only requires the trajectories collected over behavioral polices given _a priori_; similarly, off-policy selection (OPS) targets to determine the most promising policies, out of the ones trained with different algorithms or hyper-parameter sets, that can be used for online deployment . However, most existing OPS and OPE methods are designed in the context of homogenic agents, such as in roboticsor games, where characteristics of the agents can be captured by their specifications, which are in general assumed fully known (_e.g._, degree of freedom, angular constraint of each joint).

**The pivotal challenge in OPE/OPS for HCSs.** In contrast, in HCSs, the participants can have highly diverse backgrounds, where each person may be associated with unique underlying characteristics that are not straightforward to be captured individually; due to the partial observability of participants' mind states and the limited size of the cohort that can be recruited for experiments with HCSs. For example, patients participated in healthcare research studies could have different health/disease records, while the students using an intelligent tutoring system in IE may have different mindsets toward studying the course. As a result, the optimal criteria for selecting the policy to be deployed to each participant can vary, and, more importantly, it would be _intractable for existing OPS/OPE frameworks to determine what the policy selection criteria would be for a new participant who just joined the cohort_. Consequently, there lacks a framework that can resolve the _pivotal challenge_ in facilitating real-world HCSs - _how to select a policy to deploy when a new participant joining the cohort, without having access to any prior offline data collected over the participant?_

In this work, we introduce **F**irst-glance **off-**P**olicy **S**election (**FPS**), to address the problem of determining the OPS criteria needed for each new participant joining the cohort (_i.e._, at \(t=0\) only, or without using information obtained from \(t>=1\) onwards), assuming that we have access to offline trajectories for a small batch of participants _a priori_, _i.e._, the offline data. Specifically, it first partitions the participants from the offline dataset into sub-groups, clustering together the ones pertaining to similar behaviors. Then, an unbiased value function estimator, with bounded variance, is developed to determine the policy selection criteria for each sub-group. At last, when new participants join, they will be recommended with policies selected according to the sub-groups they fall within. Note that FPS is distinguished from typical off-policy selection (OPS) setup in the sense that, the major goal of prior OPS approaches is to select the best policy over the entire population, while FPS aims to decide the best policy for each student who arrives to the HCS on-the-fly, leveraging the information observed at the initial step (\(t=0\)) only.

The key contributions of this work are summarized as follows: (_i_) We introduce the FPS framework which is critical for closing the gap between OPS and applications pertaining to HCSs, _i.e._, selecting the policy that would maximize the gain of the new participants at the point of joining a cohort. To the best of our knowledge, this is the first framework that considers the new participant arrival's problem in the context of OPS in HCSs. (_ii_) We conduct extensive experiments to evaluate FPS in a _real-world IE system_, with 1,288 students participating over 5 years. Results have shown that, with the help of FPS, it improved the learning outcomes by \(208\%\) compared to policy selection criteria hand-crafted by instructors. Moreover, it leads to \(136\%\) increased outcome compared to policies selected by existing OPS methods. (_iii_) FPS is also evaluated against _an important healthcare application_, _i.e._, septic shock treatment [45; 48; 53], where it can accurately identifying the best treatment policies to be deployed to incoming patients, and outperforms existing OPS methods.

## 2 First-Glance Off-Policy Selection (FPS)

In this section, we introduce the FPS method, which determines the policy to be deployed to new participants that join an existing cohort, conditioned only on their initial states. Specifically, the participants pertaining to the offline dataset are partitioned into sub-groups according to their past behavior. Then, a variational auto-encoding (VAE) model is used to generate synthetic trajectories for each sub-group, augmenting the dataset and improving the state-action coverage. Moreover, an unbiased value function estimator, with bounded variance, is developed to determine the policy selection criteria for each sub-group. At last, when new participants join, they will be recommended with the policies conditioned on the sub-groups they fall within respectively. We start with a sub-section that introduces the problem formulation formally.

### Problem Formulation

The HCS environment is formulated as a human-centric Markov decision process (HC-MDP), which is a 7-tuple \((,,,_{0},R,,)\). Specifically, \(\) is the state space, \(\) is the action space, \(:\) defines transition dynamics from the current state and action to the next state, \(_{0}\) defines the initial state distribution, \(R:\) is the reward function, \(\) is the set of participants involved in the HCS, \((0,1]\) is discount factor. Episodes are of finite horizon \(T\). At each time-step \(t\) in _online_ policy deployment, the agent observes the state \(s_{t}\) of the environment, then chooses an action \(a_{t}\) following the _target (evaluation)_ policy \(\). The environment accordingly provides a reward \(r_{t}=R(s_{t},a_{t})\), and the agent observes the next state \(s_{t+1}\) determined by \(\). A _trajectory_ is denoted as \(^{(i)}_{}=[,(s^{(i)}_{i},a^{(i)}_{i},r^{(i)}_{t},s^{(i)}_{t+1}), ]_{t=1}^{T}\). Moreover, we consider having access to a historical trajectory set (_i.e._, offline dataset) collected under a _behavioral_ policy \(\), \(_{}=\{...,^{(i)}_{},...\}_{i=1}^{N}\), which consist of \(N\) trajectories. We first make two assumptions in regards to the correspondence between trajectories and participants, and the initial state distribution for each participant, respectively.

**Assumption 2.1** (Trajectory-Participant Correspondence).: As a participant in human-centric experiments is in general unlikely to undergo exactly the same procedure more than once under the topic being studied, we assume that there exist a unique correspondence between each trajectory \(^{(i)}\) and the participant (\(i\)) from which the trajectory is logged.

We henceforth can use \(i\) to refer to index either a trajectory from the offline dataset, or the corresponding participant, depending on the context.

**Assumption 2.2** (Independent Initial State Distributions).: The initial state of each trajectory \(s^{(i)}_{0}^{(i)}\), corresponding to a unique (the i-th) participant following from the assumption above, is sampled from an initial state distribution \(_{0}(i)\) conditioned on i-th participant's characteristics and past records (_i.e._, specific to the i-th trajectory), and is independent from all other \(_{0}(j)\)'s where \(j[1,N] i\).5 The assumptions above reflect the scenarios that are specific to HCS - for example, a patient is unlikely to be prescribed the same surgery twice. Even if the patient has to undergo a follow-up surgery that is of the similar type (_e.g._, mostly seen in trauma or orthopedics departments), the second time when the patient comes in he/she will start with a rather different initial state, since the pathology may have already been intervened as a result of the last visit. Consequently, one can treat such a visit as a new (synthetic) participant who just join and has the health record same as the one updated after the last visit. In other words, a _participant_ being considered in this paper can be generalized, _e.g._, to a _hospital visit_, or a _student_ participating in a _specific course_ supported by intelligent education (IE) systems, depending on the context. Moreover, assumption 2.2 directly follows from the philosophy illustrated in assumption 2.1 - the initial state of each trajectory depend on the corresponding participant's unique characteristics and historical records before joining the experiment/cohort, and can be considered mutually independent across all participants. Now we define the goal for FPS.

**Problem 2.3**.: _The goal of FPS is to select the best policy \(\) from a set of **pre-trained** (candidate) policies \(\), \(\), for each of the new participants \(i^{}\{N+1,N+2,\}\) joining (i.e., arriving at) the HCS with an observable initial state \(s_{0}_{0}(i^{})\) (but the rest of the trajectory remain unobservable), that maximizes the expected accumulated return \(V^{}\), \(_{}V^{}\), over the full horizon \(T\); here \(V^{}=_{s_{0}_{0}(i^{}),(s_{t>0},a_{t>0}) ^{},r R}[_{t=1}^{T}^{t-1}r_{t}|]\), and \(^{}\) is the state-action visitation distribution under \(\) from step \(t=1\) onwards._

Note that the problem formulation here is different than the typical OPS/OPE setup used in existing works [23; 66; 7; 77; 79; 14; 30], as only the initial state \(s_{0}\) is available for policy selection. Such a formulation is aligned with use cases under HCSs, _e.g._, treatment plan needs to be laid out soon after a new patient is admitted to the intensive care unit (ICU) in medical centers. However, most indirect OPS methods such as importance sampling (IS) [52; 7] and doubly robust (DR) [23; 66] require the entire trajectory to be observed, in order to estimate \(V^{}\). Though direct methods like fitted-Q evaluation (FQE)  could be used as a workaround, they do not take into account the unique characteristics for each participant that plays a crucial role in HCS applications; results in Section 3 show that they in general underperform in the real-world IE experiment. To address both challenges, we introduce the FPS approach, starting with the sub-group partitioning step introduced below.

### Sub-Group Partitioning

In this sub-section, we introduce the sub-group partitioning step that partition the participants in the offline dataset into sub-groups. Furthermore, value functions over all candidate policies \(\) are learned respectively for each sub-group, to be leveraged as the OPS criteria for each sub-group.

The partitioning is performed over the initial state of each trajectory in the offline dataset, \(_{}\). Given assumptions 2.1 and 2.2, and the fact that \(_{0}(i)\)'s in general only share limited support across participants (_i.e._, every human has unique characteristics and past experience), such partitioning is essentially performed at per-participant level. Specifically, we consider partitioning the participants into \(M\) sub-groups. Then for all sub-groups, \(K_{m}\)'s, in the set of sub-groups, \(=\{K_{1},,K_{M}\}\), we have \(_{m=1}^{M}K_{m}=_{0}\) and \(K_{m} K_{n}=, m n\). The total number of groups \(M\) needed can be determined using silhouette scores . Denote the partition function \(k():_{0}\). We then define the value function specific to each sub-group.

**Definition 2.4** (Value Function per Sub-group).: The value function over policy \(\), \(V^{}_{K_{m}}\), specific to the sub-group \(K_{m}\), is the expected accumulative return over the initial states that correspond to the set of participants \(_{m}=\{i|k(s^{(i)}_{0})=K_{m},i\}\) residing in the same sub-group. \(V^{}_{K_{m}}=_{s_{0} Unif(\{_{0}(i)|i_{m}\}),\ (s_{t>0},a_{t>0})^{},\ r k[_{t=1}^{T}^{t-1}r_{t}]],\) with \(s_{0} Unif(\{_{0}(i)|i_{m}\})\) representing that \(s_{0}\) is sampled from a uniformly weighted mixture of distributions over \(\{_{0}(i)|i_{m}\}\), pertaining to sub-group \(K_{m}\).

The goal of sub-group partitioning is to learn the partition function \(k()\), such that the difference between the value of the best policy candidate, \(_{}V^{}_{K_{m}}\), and the value of the behavioral policy, \(V^{}_{K_{m}}\), is maximized for all participants \(i\) and sub-groups \(K_{m}\), _i.e._,

\[_{k}_{i}_{}V^{}_ {K_{m}=ks^{(i)}_{0}}-V^{}_{K_{m}=ks^{(i)}_{0 }}.\] (1)

The objective (1) is designed in the sense that participants may benefit more from the type of policies that fit better for their individual characteristics. For example, in IE, different candidate lecturing policies may be used toward prospective high- and low-performers respectively, as justified by the findings from our real-world IE experiment (centered around Figure 2 in Section 3.2). The value provided by different policies for a specific type of learners (_i.e._, sub-group) could be different, measured by \(V^{}_{K_{m}}-V^{}_{K_{m}}\) for all \(\); here, \(V^{}_{K_{m}}\) captures the expected return from a instructor-designed, one-size-fit-all baseline (_i.e._, behavioral) policy that is used to collect offline data [38; 68; 84; 11]. Then, it would be crucial to identify to which group each student belongs, as it can maximize the returns collected by each student throughout the horizon.

**Practical off-policy deployment to initiate human-centric experiments over the sub-group objective** (1).: Our focus is to select the policy that can possibly work the best for each incoming individual from a set of policy candidates that are pre-given, which is critical for RL policy deployment in real-world HCSs, considering online policy optimization can be high-stake. The overall practical off-policy deployment can be achieved using a two-step approach, _i.e._, (_i_) _pre-partitioning_ with offline dataset, followed by (_ii_) _deployment_ upon observation of the initial states of arriving participants. Due to space limitation, the specific steps can be found in Appendix D.1.

**Proposition 2.5**.: _Define the estimator \(^{,}_{K_{m}}\) as, i.e.,_

\[^{,}_{K_{m}}=_{m}|}_{i_ {m}}_{i}_{t=1}^{T}^{t-1}r^{(i)}_{t}-_{t=1}^{T} ^{t-1}r^{(i)}_{t};\] (2)

_here, \(_{m}\) follows the definition above, which is the set of participants grouped in \(K_{m}\); \(_{i}=^{T}_{t=1}(a^{(i)}_{t}|s^{(i)}_{t})/(a^{(i)}_{t}|s^{(i)}_ {t})\) is the IS weight for the i-th trajectory in the offline dataset; \(s^{(i)}_{t},a^{(i)}_{t},r^{(i)}_{t}\) are the states, actions, rewards logged in the offline trajectory, respectively. Then, \(^{,}_{K_{m}}\) is unbiased, with its variance bounded by, i.e.,_

\[Var(^{,})_{t=1}^{T}^{t-1}r_{t} _{}^{2}-,\] (3)

_with \(ESS\) being the effective sample size ._

The proof of proposition 2.5 is derived from  and provided in Appendix D.2.

### Trajectories Augmentation within Each Sub-Group

In HCSs, each sub-group may only contain a limited number of participants, due to the high cost of recruiting participants as well as time constraints in real-world experiments. For example, in the IE experiment in Section 3, one sub-group only contains 45 students as a result from sub-group partitioning. Consequently, the overall offline trajectories within each group may cover limited visitations of the state and action spaces, and make the downstream policy selection task challenging . Latent-model-based data augmentation has been commonly employed in previous offline RL [20; 31; 58; 14], to resolve similar issues. For this sub-section, we specifically consider the variational auto-encoder (VAE) architecture introduced in , as it is originally designed for offline setup as well. Now we briefly introduce the VAE setup, which can capture the underlying dynamics and generate synthetic offline trajectories to improve the state-action visitation coverage _within each subgroup_. Specifically, given the offline trajectories \(_{m}\) specific to the subgroup \(K_{m}\), the VAE consists of three major components, _i.e._, (\(i\)) the latent prior \(p(z_{0})\) that represents the distribution of the initial latent states over \(_{m}\); (\(ii\)) the encoder \(q_{}(z_{t}|s_{t-1},a_{t-1},s_{t})\) that encodes the MDP transitions into the latent space; (\(iii\)) the decoders \(p_{}(z_{t}|z_{t-1},a_{t-1})\), \(p_{}(s_{t}|z_{t})\), \(p_{}(r_{t-1}|z_{t})\) that reconstructs new samples. The training objective is formulated as an evidence lower bound (ELBO) specifically derived for the architecture above. More details can be found in Appendix D.3. Consequently, for the trajectories in each subgroup, \(_{m}\), the VAE can be trained to generate a set of synthetic samples, denoted as \(}_{m}\). In the Section 3.2, we further discuss and justify the need of trajectory augmentation through an real-world intelligent education (IE) experiment.

### The FPS Algorithm

```
0: A set of target policies \(\), offline dataset \(\).
0:
0:// Training Phase.
1: Calculate the number of subgroups \(M\) needed for \(\), using silhouette scores .
2: Obtain the sub-group partitioning \(=\{K_{1},,K_{M}\}\) following Section 2.2.
3:for each sub-group \(K_{m}\)do
4: Augment sub-group samples \(_{m}\) with \(}_{m}\).
5: Use the estimator in Proposition 2.5 to obtain \(_{K_{m}}^{,}\) for all candidate target policies \(\), over \(_{m}_{m}}\).
6: Select the best candidate target policy \(_{m}^{*}\) that maximizes \(_{K_{m}}^{,}\) as the one to be deployed over \(K_{m}\).
7:// Deployment Phase.
8:while the HCS receives the initial state \(s_{0}\) from a new participant do
9: Determine the sub-group \(K_{m}\) for the new participant.
10: Deploy to the participant the best candidate policy \(_{m}^{*}\) specific to sub-group \(K_{m}\). ```

**Algorithm 1** FPS.

The overall flow of the FPS framework is described in Algorithm 1. The training phase directly follow from the sub-sections above. Upon deployment, FPS can help HCSs monitor each arriving participant, determine the sub-group the participant falls within, and select the policy to be deployed according to the initial state. Such real-time adaptability is important for HCSs in practice, and is different from existing OPS works which in general assume either the full trajectories or population characteristics are known [25; 76; 82]. For example, in practical IE, students may start learning irregularly according to their own schedules, hence can create discrepancies in their start times. Such methods fall short in cases when selecting policies based on population or sub-group information in the upcoming semester - they requires the data from all arriving students are collected upfront, which would be unrealistic. Note that, to the best of our knowledge, we are the first work that formally consider the problem of sub-typing arriving participants, and FPS is the first approach that solves this practical problem by introducing a framework that can work with HCSs in the real-world.

## 3 Experiments

FPS is tested over two types of HCSs, _i.e._, intelligent education (IE) and healthcare. Specifically, the _real-world IE experiment_ involves 1,288 student participating in college entry-level probability course across 6 academic semesters. The goal is to use the data collected from the students of the first 5 semesters, to assign pre-trained RL lecturing policies to every student enrolled in the 6-th semester, in order to maximize their learning outcomes. The healthcare experiment targets for selecting pre-configured policies that can best treat patients with sepsis, over a simulated environment widely adopted in existing works [45; 46; 65; 36; 12].

### Baselines

**Existing OPS/OPE.** The most straightforward approach to facilitate OPS in HCSs is to select policies via existing OPS/OPE methods, by choosing the candidate target policy \(\) that achieves the maximum estimated return _over the entire offline dataset, i.e._, indiscriminately across all potential sub-groups. Specifically, 6 commonly used OPE methods are considered, _i.e._, Weighted IS (WIS) , Per-Decision IS (PDIS) , Fitted-Q Evaluation (FQE) , Weighted DR (WDR) , MAGIC , and Dual stationary DIstribution Correction Estimation (DualDICE) .

**Existing OPS/OPE with vanilla repeated random sampling (OPS+RRS).** We also compare FPS against a classic data augmentation method in order to evaluate the necessity of the VAE-based method introduced in Section 2.3 - _i.e._, repeated random sampling (RRS) with replacement of the historical data to perform OPE. RSS has shown superior performance in some human-related tasks, such as disease treatment . Specifically, all OPS/OPE methods considered above are applied to the RRS-augmented offline dataset, where the value of each candidate target policy is obtained by averaging over 20 sampling repetitions. However, note that RRS does not intrinsically consider the temporal relations among state-action transitions as captured by MDP.

**Existing OPS/OPE with VAE-based RRS (OPS+VRRS).** This baseline perform OPS with RRS on augmented samples resulted from the VAE introduced in Section 2.3, in order to allow RRS to consider MDP-typed transitions, hence improve state-action visitation coverage of the augmented dataset. This method can, to some extent, be interpreted as an ablation baseline of FPS, by removing the sub-group partitioning step (Section 2.2), and slightly tweaking the VAE-based offline dataset augmentation step (Section 2.3) such that it does not need any sub-group information. Specifically, we set the amount of augmented data identical to the amount of original historical data, _i.e._, \(|}|=||=N\), and RRS \(N\) samples from both set \(}\) to perform OPE. Final estimates are averaged results from \(20\) repeated sampling processes.

**FPS without trajectory augmentation (FPS-noTA).** This is the ablation baseline that completely removes from FPS the augmentation technique introduced in Section 2.3.

**FPS for the population (FPS-P).** We consider on additional ablation baseline that follows the same training steps as FPS (_i.e._, steps 1-7 of Alg. 1), but rather select _a single policy_ that is identified (by FPS) as the best for majority of the sub-groups, to be deployed to all participants. In other words, after training, FPS produces the mapping \(h:\), while FPS-P will always deploy to every arriving participant the policy that appears most frequently in the set \(\{h(K_{m})|K_{m}\}\).

Figure 1: Analysis of main results from the real-world IE experiment. (a) Overall performance of the 6-th semesterâ€™s student cohort. Methods that selected the same policy are merged in one bin, _i.e._, all refers to all three variations (raw, +RRS, +VRRS) of the existing OPS baselines. (b) Estimated and true policy performance using each method. For _OPE_, _OPE+RRS_, _OPE+VRRS_, results with the least gap between estimated and true rewards among OPE methods (_i.e._, WIS, FQE+RRS, and FQE+VRRS, respectively) are shown in the figure. True reward refers to the returns averaged over the cohort of the 6-th semester, obtained by deploying the policy selected for each student correspondingly.

### The Real-World IE Experiment

The IE system has been integrated into a undergraduate-level introduction to probability and statistics course over 6 semesters, including a total of 1,288 student participants. This study has received approval from the Institutional Review Board (IRB) at the institution to ensure ethical compliance. Additionally, oversight is provided by a departmental committee, which is responsible for safeguarding the academic performance and privacy of the participants. In this educational context, each learning session revolves around a student's engagement with a set of 12 problems, with this period referred to as an "episode" (horizon \(T=12\)). During each step, the IE system offers students three actions: independent work, utilizing hints, or directly receiving the complete solution (primarily for study purposes). The states space is constituted by by 140 features that have been meticulously extracted from the interaction logs by domain experts, which encompass various aspects of the students' activities, such as the time spent on each problem and the accuracy of their solutions. The learning outcome is issued as the environmental reward at the end of each episode (0 reward for all other steps), measured by the normalized learning gain (NLG) quantified using the scores received from two exams, _i.e._, one taken before the student start using the system, and another after. Data collected from the first 5 semesters (over a lecturer-designed behavioral policy) are used to train FPS for selecting from a set of candidate policies to be deployed to each student in the cohort of the 6-th semester, including 3 pre-trained RL policies and 1 benchmark policy (whose performance benchmark the lower-bound of what could be tested with student participants). See Appendix A for the definition of NLG, details on pre-trained RL policies, and more.

**Main results.** Figure 1(a) presents students' performance under policies selected by different methods. Overall, FPS was the most effective policy selection leading to the greatest average student performance. The return difference between FPS and the two ablation, FPS-noTA and FPS-P, illustrate the importance of augmenting offline trajectories (as introduced in Section 2.3) and assign to arriving students policies that better fit the characteristics shared within their sub-groups, respectively. Moreover, most existing OPS/OPE methods tend to select sub-optimal policies that resulted in better learning gain than the benchmark policy. Note that we also observed that DualDICE could not distinguish the returns over all target policies; thus, it is unable to be used for policy selection in this experiment and we omit its results. It is also important to evaluate how accurate the value estimation \(V^{^{*}}\) would be for the best candidate policy selected across all methods, over the arriving student cohort at the 6-th semester, as illustrated in Figure 1(b). FPS provided more accurate policy estimation by achieving the smallest error between true and estimated policy rewards. With VPRS, most OPS methods improved their policy estimation performance, which was benefited from the richer state-action visitation coverage provided by the synthetic samples generated by VPRS. However, even with such augmentations, existing OPS methods still chose sub-optimal policies, which justified the importance of considering participant-specific characteristics in HCSs, which is tackled by sub-group partitioning in FPS (Section 2.2).

**More discussions.** For a more comprehensive understanding of student behaviors affected by the policy being deployed in IE, we further investigate how the sub-groups are partitioned and how the policies being assigned to each sub-group perform. Specifically, FPS identified four subgroups (_i.e._, \(K_{1},K_{2},K_{3},K_{4}\)) as a result of Section 2.2. Under the behavioral policy, the average NLG across all students is 0.9 with slight improvement after tutoring. Specifically, \(K_{1}(N_{train}=345,N_{test}=30)\) and \(K_{2}(N_{train}=678,N_{test}=92)\) achieved average NLG of 1.9 _[95% CI, 1.7, 2.1]6_ and 0.7 _[95% CI, 0.6, 0.8]_ under the behavioral policy, respectively. In the testing (6-th) semester, FPS

Figure 2: Performance of students (mean\(\)se) over all four sub-groups under selected policies in the 6-th semester.

[MISSING_PAGE_FAIL:8]

after the patient is admitted, \((ii)\) with antibiotics (WA) that always administer antibiotics once the patient is admitted, \((iii)\) an RL policy trained following policy iteration (PI). Note that as pointed by , the true returns of WA and PI are usually close, since antibiotics are in general helpful for treating sepsis, which is also observed in our experiment; see Table 1. Moreover, a simulated unrecorded comorbidities is applied to the cohort, capturing the uncertainties caused by patient's underlying diseases (or other characteristics), which could reduce the effects of the antibiotics being administered. See Appendix B for more details in regards to the environmental setup.

Given the simulated environment, we mainly consider using this experiment to evaluate the source of improvement brought in by the sub-group partitioning step (Section 2.2) in FPS. Specifically, multiple scaled offline datasets are generated, representing different degrees of the state-action visitation coverage - we vary the total number of trajectory \(N\)={2,500, 5,000, 10,000}, in lieu of performing trajectory augmentations for both FPS and existing OPS baselines. In other words, in this experiment, we consider the FPS without the VAE augmentation step introduced in Section 2.3, as well as the 6 original OPS baselines (without any RRS/VRRS) introduced in Section 3.1. We believe this setup would help isolate the source of improvements brought in by sub-group partitioning. The average absolute errors (AEs), in terms of OPE, and returns, in terms of OPS, resulted from deploying to each patient the corresponding candidate policy selected by FPS against baselines, are reported in Table 1. It can be observed that FPS achieved the lowest AE and highest return regardless of the size of the offline dataset. We additionally evaluate the top-1 regret (_i.e._, regret@1) of the selected policy following FPS and baselines, which are also reported in Table 1. It can be observed that FPS achieved exceedingly low regrets compared to baselines. Both observations emphasize the effectiveness of the sub-group partitioning technique leveraged by FPS, as the environment does capture comorbidities as part of the participant characteristics. Moreover, the AEs and regrets of most methods decrease when the size of offline dataset increase, justifying that improved state-action visitation coverage provided by the offline trajectories is crucial for reducing estimation errors and improving policy selection outcomes (_i.e._, the motivation of trajectory augmentation introduced in Section 2.3).

## 4 Related Works

**Off-policy selection (OPS).** OPS are typically approached via OPE in existing works, by estimating the expected return of target policies using historical data collected under a behavior policy. A variety of contemporary OPE methods has been proposed, which can be mainly divided into three categories : (\(i\)) direct methods that directly estimate the value functions of the evaluation policy [44; 67; 79; 76], including but not limited to model-based estimators (MB) [17; 14; 15; 79], value-based estimators  such as Fitted Q Evaluation (FQE), and minimax estimators [35; 80; 70] such as DualDICE ; (\(ii\)) inverse propensity scoring, or indirect methods , such as Importance Sampling (IS) ; (\(iii\)) hybrid methods combine aspects of both inverse propensity scoring and direct methods , such as DR . In practice, due to expensive online evaluations, researchers generally selected the policy with the highest estimated rewards via OPE. For example, Mandel et al. selected the policy with the maximum IS score to be deployed to an educational game . Recently, some works focused on estimator selection or hyperparameter tuning in off-policy selection [46; 75; 63; 43; 28; 32; 65; 50]. However, retraining policies may not be feasible in HCSs as online data collection is time- and resource-consuming. More importantly, prior work generally selected policies without considering the characteristics of participants, while personalized policy is flavored towards the needs specific to HCSs.

**RL-empowered automation in HCSs.** In modern HCSs, RL has raised significant attention toward enhancing the experience of human participants. Previous studies have demonstrated that RL can induce iE policies [1; 38; 60; 73]. For example, Zhou et al.  applied hierarchical reinforcement learning (HRL) to improve students' normalized learning gain in a Discrete Mathematics course, and the HRL-induced policy was more effective than the Deep Q-Network induced policy. Similarly, in healthcare, RL has been used to synthesize policies that can adapt high-level treatment plans [53; 45; 36], or to control medical devices and surgical robotics from a more granular level [16; 37; 55]. Since online evaluation/testing is high-stake in practical HCSs, effective OPS methods are important in closing the loop, by significantly reducing the resources needed for online testing/deployment and preemptively justifying safety of the policies subject to be deployed.

Conclusion and Limitation

In this work, we introduced the FPS framework that facilitated policy selection in real-world HCSs; it tackled the off-policy deployment with new arrivals problem that is pivotal for RL policy deployment in HCSs. Unlike existing OPS methods, FPS customized the policy selection criteria for each sub-group respectively. FPS was tested in a real-world IE experiment and a simulated sepsis treatment environment, which significantly outperformed baselines. Though in the future it would be possible to extend FPS to a offline RL policy optimization framework, however, in this work we specifically focus on the OPS task in order to isolate the source of improvements brought in by sub-group partitioning and trajectory augmentation. Future avenues along the line of FPS also include deriving estimators (for Proposition 2.5) that allow bias-variance trade off, _e.g._, by integrating WDR or MAGIC (to substitute the IS weights). Societal and broader impacts are discussed in Appendix C.

Compared to IE systems, HCSs in healthcare would be considered even more high-stakes, thus may further limit the options (i.e., policies) that are available to facilitate sub-grouping experiments, due to stricter clinical experimental guidelines. However, FPS has demonstrated its extraordinary capabilities over a real-world experiment that involved >1,200 participants with years of follow-ups, which showed its efficacy and scalability toward working with more challenging systems and larger cohorts as in healthcare, as the assumptions needed by FPS across these two systems would not change fundamentally. Moreover, potential underlying confounding may exist across the patient's initial states in healthcare, and it is also important to consider inputs from healthcare professionals during sub-grouping. As a result, one may further extend our framework toward such a direction, allowing it to function better in the healthcare domain.

#### Acknowledgments

This research was supported by the NSF Grants: Integrated Data-driven Technologies for Individualized Instruction in STEM Learning Environments (1726550), CAREER: Improving Adaptive Decision Making in Interactive Learning Environments (1651909), Generalizing Data-Driven Technologies to Improve Individualized STEM Instruction by Intelligent Tutors (2013502), NAIAD Award (2332744), and National AI Institute for Edge Computing Leveraging Next Generation Wireless Networks, Grant CNS-2112562. This research was also sponsored in part by the AFOSR under award number FA9550-19-1-0169. We would also like to thank the anonymous reviewers for insightful comments that lead to improved paper presentations.