# Causal Context Adjustment Loss

for Learned Image Compression

 Minghao Han\({}^{1}\), Shiyin Jiang\({}^{1}\), Shengxi Li\({}^{2}\), Xin Deng\({}^{2}\), Mai Xu\({}^{2}\), Ce Zhu\({}^{1}\), Shuhang Gu\({}^{1}\)

\({}^{1}\)University of Electronic Science and Technology of China \({}^{2}\)Beihang University

{minghao.hmh, shuhanggu}@gmail.com

Corresponding Author

###### Abstract

In recent years, learned image compression (LIC) technologies have surpassed conventional methods notably in terms of rate-distortion (RD) performance. Most present learned techniques are VAE-based with an autoregressive entropy model, which obviously promotes the RD performance by utilizing the decoded causal context. However, extant methods are highly dependent on the fixed hand-crafted causal context. The question of how to guide the auto-encoder to generate a more effective causal context benefit for the autoregressive entropy models is worth exploring. In this paper, we make the first attempt in investigating the way to explicitly adjust the causal context with our proposed Causal Context Adjustment loss (CCA-loss). By imposing the CCA-loss, we enable the neural network to spontaneously adjust important information into the early stage of the autoregressive entropy model. Furthermore, as transformer technology develops remarkably, variants of which have been adopted by many state-of-the-art (SOTA) LIC techniques. The existing computing devices have not adapted the calculation of the attention mechanism well, which leads to a burden on computation quantity and inference latency. To overcome it, we establish a convolutional neural network (CNN) image compression model and adopt the unevenly channel-wise grouped strategy for high efficiency. Ultimately, the proposed CNN-based LIC network trained with our Causal Context Adjustment loss attains a great trade-off between inference latency and rate-distortion performance. The code is available here.

## 1 Introduction

The burgeoning quality of high-resolution photos is driving an increasing demand for advanced image storage and transmission technologies. Consequently, lossy image compression techniques have been growing extraordinarily fast in recent years. In parallel to conventional coding technologies such as JPEG , BPG , WebP , VVC , learned image compression (LIC) methods  emerge, achieving high peak signal-to-noise ratio (PSNR) and multiscale structural similarity (MS-SSIM)  while operating fairly fast. Their superior compression results over those of VVC demonstrate an enormous possibility that LIC technology would appear on par with the traditional ones in the near future.

Learned lossy image compression methods are built upon a variational auto-encoder (VAE) framework proposed by Balle et al. . The VAE based LIC framework mainly comprises an auto-encoder and an entropy model. The auto-encoder conducts nonlinear transforms between the image space and the latent representation space; while, the entropy model minimizes the code length by estimating the probability distribution of latent representations. In comparison to the auto-encoder, which could borrow ideas from recent advances in network architecture design, the entropy model is a unique important component to LIC and has a vital influence on the final compression results.

In the literature on LIC, the entropy model generally refers to a parameterized distribution model. In their seminal work , Balle et al. established the end-to-end rate-distortion minimization framework and showed that the smallest average code length of latent representation is given by the Shannon cross entropy between the actual marginal distribution and a learned entropy model. Since then, numerous entropy models have been investigated. One category of studies investigates advanced network architectures for accurately predicting the distribution of latent representations. Meanwhile, another line of research study a more fundamental perspective of the entropy model, i.e. conditional distribution modeling, to pursue a better rate-distortion trade-off. Taking side information (also termed as hyperprior) and decoded latent (also termed causal context) as conditions has become a prevailing strategy in state-of-the-art LIC models.

In this paper, we advance conditional distribution modeling in the entropy model with our proposed causal context adjustment loss (CCA-loss). Existing works generally train LIC networks with a combination of the rate loss and the distortion loss. The conditional predictability of the representation is indirectly optimized, and the performance of entropy model highly relies on the hand-crafted causal context model, e.g. channel-wise , checkerboard  and space-channel  context model. Our CCA-loss makes the first attempt on explicitly imposing loss to adjust the causal context, making the latter representation more accurately predicted by the previously decoded representations. To be more specific, considering a two stage autoregressive context model with hyperprior \(\), denote the latent representation to be decoded in the first and second stage as \(_{1}\) and \(_{2}\); in addition to minimizing the cross entropy loss for reducing bitstream, we introduce an auxiliary entropy model and a tailored context causal adjustment loss, which let \(_{2}\) can be accurately estimated by \(_{1}\) and \(\), while, at the same time, let \(_{2}\) can not be accurately estimated by merely \(\). In this vein, our CCA-loss explicitly guides the encoder to adjust important information into the early stage of the autoregressive entropy model, providing the LIC framework a more rational causal context sequence for entropy coding. As the codes in the early stages are enhanced with our CCA-loss, we further study the schedule of causal context transmission, and adopt an uneven channel dimension schedule for the pursuit of a better rate-distortion trade-off. The uneven channel schedule is also beneficial for reducing computational burden in the coding and decoding process, enabling our model to achieve state-of-the-art compression performance with less running time. Our contributions are summarized as follows:

* We introduce causal context adjustment loss to explicitly adjust the causal context information, forcing the network to encode important information early and therefore improving the autoregressive prediction accuracy of the entropy model.
* We adopt an uneven schedule of autoregressive causal context and a convolutional auto-encoder architecture, delivering an efficient compression network which is easy to be implemented on modern deep learning platforms.
* We evaluate our proposed compression network on various benchmark datasets, in which our method achieves better rate-distortion trade-offs towards the existing state-of-the-art methods, with more than 20% less compression latency.

## 2 Related Works

Recent LIC studies broadly follow the seminal work of Balle et al. , which utilizes a VAE based framework for rate-distortion optimization. Generally, VAE-based LIC models comprise an auto-encoder and an entropy model. In this section, we review respective progresses in advanced auto-encoders and entropy models. Another considerable technique in LIC is the quantization method, however, as we did not dig into the details and simply followed the quantization method of , we omit the review of quantization methods in this section.

### Auto-Encoder Architectures for Learned Image Compression

The auto-encoder plays the role of extracting a latent representation apt to be compressed in LIC framework. In their pioneering work, Balle et al.  first proposed to use a generalized divisive normalization (GDN)  to transform the input image into latent space. The later works followed the same VAE framework for LIC but exploited the convolution neural network (CNN) architecture, which is easier to implement and train. Beyond the basic CNN auto-encoder, the introduction of more complex nonlinear transforms [9; 11; 33] and various architectures [14; 28; 45; 46] promotes RD performance. Recently, inspired by the great successes Transformers have made in other vision tasks, self-attention modules have been widely utilized for extracting latent representations. Embedding Transformer variants, for example ViT , swin-Transformer  in the auto-encoder [32; 48; 49] enhances the RD performance. Moreover, Liu et al.  proposed a hybrid approach, combining conventional CNN and swin-Transformer. Although these Transformer-based auto-encoder could improve the RD performance by extracting better latent representations, the inference of transformer architecture has not been well optimized by the existing hardware, resulting in slow coding and decoding speed. In this paper, we borrow ideas from recent advances in image restoration  and adopt a CNN-based auto-encoder architecture. Thanks to our improved entropy model as well as the powerful NAF-block , our LIC model could achieve state-of-the-art compression results with much less runtime than recent Transformer-based approaches.

### Entropy Models for Learned Image Compression

The entropy model plays a key role in LIC for minimizing the bitstream of latent representation. In the original work , the probability distributions of the latent representation are modeled using a non-parametric, fully factorized density model. In order to improve the distribution predicting accuracy, Balle et al.  introduced side information as a hyperprior latent variable and ultimately established the basic VAE architecture of LIC in the past decade. Beyond the hyperprior transmitted in the VAE-based LIC framework, newly proposed extra side information transmitted from encoder to decoder promotes the compression performance as well [20; 43]. Moreover, Duan et al.  explored the hierarchical VAE structure with multiple hyperpriors.

In addition to the improvement on the side information, the introduction of causal context autoregression greatly promotes the RD performance of LIC, efficiently utilizing the information from decoded parts without a superergoatory amount of bits per pixel (bpp). Minnen et al.  proposed the first autoregressive structure, using the decoded spatial context to better estimate the current probability distribution. Numerous works attempt to establish an effective causal context for assistance in distribution estimations, such as channel-wise segmentation , checkerboard , unevenly grouping . A very recent work  explored different strategies to selectively transmit tokens. However, a fixed hand-crafted causal context may not work well in diverse image distributions. In this work, we impose a loss to adjust the causal context in the training phase, allowing the network to achieve a more accurate probability estimation.

Figure 1: **Left**: A systematic overview of our method. We adopt the VAE-based framework  with hyperprior  and channel-wise autoregressive entropy model ; besides the original Rate-Distortion loss (\(_{R}\), \(_{D}\)), we introduce an auxiliary entropy model and propose the causal context adjustment loss (\(_{CCA}\)) for better training the entropy model. **Right**: An illustration of the entropy model and the auxiliary entropy model. The auxiliary entropy model does not use the information to be encoded to predict the following representations, our \(_{CCA}\) encourage the predicting gap between the two models, so as to enhance the importance of causal context in early stages.

An efficient network structure of the entropy model remains critical for achieving high RD performance [26; 27; 29]. Apart from the basic causal context and hyperprior entered into the entropy model, more references benefit RD performance [11; 16; 17; 38]. Just as how Transformer performs in auto-encoder, the advantages of integrating the entropy model with Transformer are unearthed quickly. Previous works applied various Transformer blocks [20; 23; 25; 30; 37] to enhance features before entropy estimation. Following our modified architecture in auto-encoder, we embed the NAF-block in the entropy model to improve estimation accuracy.

## 3 Preliminary: Learned Image Compression with Variational Auto-Encoder

Variational Auto-Encoder based Image Compression Framework.Ever since the VAE architecture was established , learned lossy image compression techniques maintain the primary constituent structure , including an auto-encoder to extract the latent representation for compression and an entropy model to assist in entropy coding. Given a source image vector \(\), the auto-encoder contains a parametric analysis transform \(g_{a}\) to obtain the latent representation \(\) from \(\) and a parametric synthesis transform \(g_{s}\) for reconstruction. \(\) is then quantized to \(}\), the discrete coding symbol for storage. The probability distributions of \(}\) are modeled using a factorized density model \(\) as \(p_{}|}(}|)=_{i}(p_{_{i},| }()(-,))(}_ {i})\). As quantization introduces error, which is tolerated in the context of lossy compression, the optimization target approximates the true posterior \(p_{}|}(}|)\) with a neural network \((}|)\) as the expectation of their Kullback-Leibler (KL) divergence over the data distribution \(p_{}\):

\[_{ p_{}}D_{KL}[ p_{}, }|}]=_{ p_{}}_{},}}- p_{|}}(|})- p_{}|}(}|).\] (1)

The former term refers to the image reconstruction distortion (measured by PSNR or MS-SSIM), and the latter term represents the bit-rate (expected code length). A hyperparameter \(\) is multiplied on the latter term, so that we can control the rate-distortion trade-off to obtain various compression rates.

Entropy Model with Hyperprior.However, directly modeling \(}\) with the factorized density model \(\) is less than satisfactory, as the estimation of which is not accurate and out of correlation with the data distributions. To capture the spatial dependence among the elements of \(}\), the side information \(\) is introduced . \(\) is generated by a hyper analysis transform \(h_{a}\) from \(\), transmitted as a hyperprior latent feature to help predict the distributions of \(}\) accurately. Similarly to \(\), \(\) is quantized to \(}\) in the same manner. The probability distributions of \(}\) are calculated using a factorized density model \(\), to encode \(}\) as \(p_{}|}(}|)=_{i}(p_{_{i}| }()(-,))(}_ {i})\). During the entropy coding process, \(}\) would be entered into a hyper synthesis transform \(h_{s}\) to acquire the estimations \(\{,\}_{i}\) in normal distribution of each element \(}_{i}\). This course can be formulated as \(p_{}|}}(}|})=_{i} (_{i},_{i}^{2})(-, )(}_{i})\), with \(\{,\}=h_{s}(})\). The KL divergence in the basic VAE structure (Eq. 1) can be expanded as follows:

\[_{ p_{}}D_{KL}[ p_{}, }|}]=_{ p_{}}_{},}}- p_{|}}(|})- p_{}|}}(}|})- p_{ }|}}(}|}).\] (2)

On the other side of VAE, the parametric synthesis transform \(g_{s}\) recovers the reconstructed image \(}\) from the decoded \(}\). Fig. 1 reveals the general basic structure.

Autoregressive Entropy Model.In addition, an advanced architecture of the entropy model is the joint autoregression , which soon develops into a more efficient channel-wise autoregression . In the channel-wise autoregressive structure, the latent representation \(}\) is grouped in the channel dimension and decoded in order. Thus, the second term of the KL divergence in hyperprior structure (Eq. 2) is expanded as:

\[_{},}}- p_{ }|}}(}|})=_{}_{i},}}- p_{}_{1}|}}(}_{1}|})p_{}_{2}|},}_{1}}(}_{2}|},}_{1})\] (3) \[p_{}_{3}|},}_{1},}_{2 }}(}_{3}|},}_{1},}_{2}) p_{ }_{n}|},}_{1},,}_{n-1}}( }_{n}|},}_{1},,}_{n-1}).\]

Besides the prior of \(}\), the estimation of the autoregressive entropy model conditions more on the causal context, that is, the model utilizes the information from the decoded parts (causal context) without introducing additional redundancy in information transmission. Therefore, the more effective the causal context, the stronger the performance of the autoregressive entropy model. Existing methods adopt various hand-crafted causal contexts to enhance it. We expect to establish a way that enables the network to adaptively adjust the causal context. Imposing a loss to explicitly adjust the causal context is a delicate way.

Causal Context Adjustment for Efficient Learned Image Compression

In this section, we introduce the details of our LIC method. We firstly introduce our causal context adjustment (CCA) loss, which is able to explicitly push the encoder to encode important (in terms of information gain) representation at an earlier stage for better predicting the remaining representations. Subsequently, we introduce the implementation details of our efficient LIC method, including our CNN-based encoder and decoder architecture, unevenly grouped autoregressive schedule, light-weight entropy model, and overall loss function.

### Causal Context Adjustment

As introduced in the previous section, exploiting the causal context from the hyperprior and the autoregressive framework to establish a conditional distribution task is the key to a state-of-the-art entropy model. While introducing conditions is undoubtedly beneficial for improving the accuracy of distribution estimation, existing works intuitively set up the context models, such as checkerboard context model and slice-based context model, and there still lack in-depth study on how to constitute the causal context rationally in the literature. More concretely, the rate and distortion loss reflect the prediction error given the causal context and the reconstruction error given the decoded representations, respectively; neither of them could explicitly affect the organization of the causal context. In this section, we introduce the CCA-loss, which explicitly encourages important information of the image to be encoded into earlier causal context, so as to enhance the predictability of the autoregressive entropy model. To the best of our knowledge, our work is the first attempt that introduces loss instead of intuitively adjusting the context architecture to improve the context model.

To introduce our proposed Causal Context Adjustment (CCA) loss, we first revisit the hyperprior and the autoregressive entropy model. Without loss of generality, we consider a two-stage autoregressive context model. To encode the same latent representation, the cross entropy of the hyperprior model and the hyperprior + autoregressive model can be written as follows:

\[H_{}(q(}|}),p(}|})) =H(q(}_{1}|}),p(}_{1}|}))+H(q(}_{2}|}),p(}_{2}|})),\] (4) \[H_{}(q(}|}),p(}| })) =H(q(}_{1}|}),p(}_{1}|}))+H(q(}_{2}|},}_{1}),p(}_{2}| },}_{1})),\] (5)

where \(H_{}\) and \(H_{}\) represent the cross entropy with hyperprior and with hyperprior + autoregressive estimation, respectively; \(q\) and \(p\) denotes the real distribution and the learned entropy model. According to Shannon information theory , as more information is incorporated in the estimation of \(}_{2}\), \(H_{}\) is less than or equal to \(H_{}\). Moreover, the gap between \(H_{}\) and \(H_{}\) is related to the amount of information \(}_{1}\) could provide for estimating \(}_{2}\). Therefore, by calculating the following equation:

\[H_{}-H_{}=H(q(}_{2}|}),p(}_{2}|}))-H(q(}_{2}|},}_{1}),p(}_{2}|},}_{1})),\] (6)

we could obtain the information gain introduced by causal context \(}_{1}\). The above analysis inspires us to explicitly optimize Eq. 6 to enhance \(}_{1}\), encouraging it to encode important information that helps to better estimate \(}_{2}\). Concretely, in addition to the original entropy model \(F(}_{1},})}_{2}\), we introduce an auxiliary entropy model, which only takes \(}\) as input: \(G(})}_{2}\). With the introduced auxiliary entropy model, our CCA-loss can be defined as follows:

\[I(}_{2};}_{1})=_{}_{1} p_{ }_{1}|}}}_{} p_{}| }}}- p_{}_{2}|}}(}_{2}| })+ p_{}_{2}|},}_{1}}(}_{2}|},}_{1}),\] (7)

where \(p_{}_{2}|}}(}_{2}|})\) and \(p_{}_{2}|},}_{1}}(}_{2}|},}_{1})\) are the estimated distributions of auxiliary and the major entropy model, respectively. The auxiliary and major entropy models are parameterized by two networks, i.e. \(G(})\) and \(F(}_{1},})\). It should be noted that the auxiliary entropy model is only introduced in the training phase for better optimizing the causal context; in the testing phase, our model still uses \(F(}_{1},})\) to compress the latent representation, our CCA-loss will not introduce additional computational burden for image compression.

An illustration of the proposed CCA-loss can be found in Fig. 1. Besides the above analysis in Eq. 4 to Eq. 7, a straightforward interpretation of our CCA-loss is enlarging the prediction gap between the major entropy model \(F(}_{1},})\) and the auxiliary entropy model \(G(})\); so that the encoder is forced to adjust causal context \(}_{1}\) and make it contain important information for conditional modeling. For autoregressive models with more than two stages, Eq. 7 can be extended easily. Denote the \(i\)-th stage entropy model as \(p_{}_{i}|},}_{<i}}(}_{i}|},}_{<i})\), which is parameterized with network \(F_{i}(},}_{<i})\); we introduce the corresponding auxiliary entropy model \(p_{}_{i}|},}_{<i-1}}(}_{i}|},}_{<i-1})\), which is parameterized with network \(G_{i}(},}_{<i-1})\). The multi-stage CCA-loss can be defined as follows:

\[_{CCA}=_{i}_{} p_{}| }}_{} p_{}|}}}- p_{ }_{i}|},}_{<i-1}}(}_{i}|},}_{<i-1})+ p_{}_{i}|},}_{<i}}( }_{i}|},}_{<i}).\] (8)

With our proposed CCA-loss, the learned image compression model is able to spontaneously adjust the causal context, thereby promoting the rate-distortion performance.

### Training Compression Network with CCA Loss

#### 4.2.1 Auto-Encoder Architecture

Inspired by the recent work , which designed a CNN-based nonlinear activation-free network to improve image restoration performance, we stack NAF-Blocks  in the analysis transform \(g_{a}\) and the synthesis transform \(g_{s}\). Following the previous CNN-based model [9; 17], we adopt the stacking residual blocks  in the auto-encoder transform for better nonlinearity. Due to the simplicity of the information that hyperprior \(\) carries, there are only simple convolution layers for the hyper analyzer \(h_{a}\) and synthesizer \(h_{s}\). Thanks to our convolutional architecture, our approach is much faster than recent LIC methods which generally adopt Transformer blocks to comprise the auto-encoder. Detailed architectures of our auto-encoder can be found in the Supplementary Materials.

#### 4.2.2 Channel-wise Unevenly Grouped Entropy Model

To establish a robust causal context and efficiently exploiting it in the autoregressive entropy models is the key to reaching state-of-the-art. The existing approaches generally constitute the causal context model intuitively. In this paper, we propose the causal context adjustment loss (CCA-loss), which compels the analysis transform to generate a more potent causal context, that is, the enhanced estimation gain of early-stage context towards the latter latent representation. Theoretically, our proposed CCA-loss is architecture-agnostic and can be utilized to train various encoders to adjust the causal context according to the given conditional modeling architecture. However, compared to the checkerboard context model that leverages adjacent spatial information as context, it is easier for our convolutional encoder to adjust information across feature channels. We therefore adopt a channel-wise grouped autoregressive architecture to design our entropy model. Furthermore, since our CCA-loss could explicitly adjust the significant information into the earlier channels, we explore an unevenly grouped strategy to take full advantage of the first several informative channels. On account of the accumulated contexts \([}_{1},}_{2},,}_{i-1}]\) as input to the autoregressive entropy model to predict \(}_{i}\), the unevenly grouped strategy also brings us advantages in the number of parameters and run time. Following our auto-encoder structure, we also utilize NAF-blocks  for a superior trade-off between accuracy and speed. For detailed network architectures of our entropy model as well as auxiliary entropy model, please refer to our Supplementary Materials. The comprehensive analysis of the benefits of the evenly and unevenly grouped strategies brought by our CCA-loss will be presented in the ablation study section.

#### 4.2.3 Overall Loss Function

We follow the commonly used rate-distortion optimization framework to train our model. In addition to the rate losses \((})\), \((})\) and the distortion loss \((},)\), our proposed CCA-loss is introduced to explicitly adjust the causal context. The implementation of our CCA requires a group of auxiliary entropy models. In order to obtain feasible auxiliary entropy models, we further introduce auxiliary losses \(_{Aux}\), which let the auxiliary model to estimate the same latent representation \(}\) as the major entropy model. Therefore, the overall losses used for training our models are listed as follows:

\[=[(})+(})] +(},)+_{CCA}+_{Aux},\] (9)

we only use one parameter \(\) to adjust the compression rate. Detailed ablation studies about the introduced CCA-loss will be presented in our experimental section.

Experiments

### Experimental Settings

Datasets.We follow the previous work  and train our models on the Open Images  dataset. Open Images Dataset contains 300k images with short edge no less than 256 pixels. For evaluation, three benchmarks, i.e., Kodak image set , Tecnick test set , and CLIC professional validation dataset , are utilized to evaluate the proposed network.

Implementation details.We set the channel of latent representation \(\) as \(320\) and that of hyperprior \(\) is set as \(192\). Following the previous works, we turn the quantization operation to \(-\) instead of \(\) and restore \(}\) as \(-+\), which benefits the entropy models. We adopt the unevenly grouped strategy to segment the latent representation into \(5\) uneven slices. Our detailed unevenly grouped method and discussion on it can be found in the Supplementary Materials. Our experiments and evaluations are carried out on Intel Xeon Platinum 8375C and a single Nvidia RTX 4090 graphics card. We train our network with Adam optimizer. We randomly crop \(256 256\) sub-blocks from the Open Images dataset  with a batch size of \(8\). We optimize the network with the initial learning rate \(1e-4\) for 2M steps and then decrease the learning rate to \(1e-5\) for another 0.4M steps. The network is optimized with the MSE metric, which represents the distortion loss \(\) in Eq. 9. For the MSE metric, the multipliers \(\) before rate loss are \(\{0.3,0.85,1.8,3.5,7,15\}\).

Comparison methods and metrics.We compare our method with the hand-crafted coding standards VVC , BPG  and WebP  and recent state-of-the-art methods [4; 11; 17; 30; 45; 49]. The results of hand-crafted methods and Balle2018  are based on the implementation from CompressAI , while, the results of other methods are provided by the method authors. We mainly use PSNR to evaluate the image quality of compression results and use bits per pixel (bpp) value to indicate the compression ratio. The BD-rate  and runtime of several methods are also reported to comprehensively evaluate our model. Following the commonly used setting, we also compare the MS-SSIM metric on the Kodak dataset, the MS-SSIM optimized results by different methods are shown in our Supplementary Materials.

### Ablation Study

We firstly conduct ablation experiments to validate the effectiveness of the proposed CCA-loss. In order to facilitate the analysis, we establish a tiny model to conduct our ablation experiments. We halve the channel number and stacking count of NAF-blocks  in our model and only adopt a three-stage autoregressive entropy model. We evaluate our CCA-loss on evenly grouped channel-wise autoregressive model as well as unevenly grouped channel-wise autoregressive model. The BD-rates of different models are reported in Table 1, without any additional computation in the testing phase, our CCA-loss could improve the evenly grouped and unevenly grouped models by a considerable margin. Especially for the case of unevenly grouped strategy, which adopts a more aggressive strategy and decodes less number of channels in the early stage, the enhancement brought by our CCA-loss is quite large. The phenomena reveals that utilizing small amount of significant information as the initial condition is beneficial for autoregressive entropy modeling, which is in line with the motivation of our paper.

To further investigate the impact on information distributions of our proposed CCA-loss, we extend a visualization of the quantities of information (code length) in the hyperprior and latent representation. The averaged information distributed ratios on the Kodak testing images by different models are shown in Fig. 2. As can be clearly found in the histogram, for entropy models with the same network architecture, our CCA-loss is able push the network to encode significant information at an earlier stage of the autoregressive model.

Figure 2: The comparison of averaged information distributed ratios of various models in Table 1.

### Comparison with State-of-the-art Methods

Rate-Distortion Comparison.We evaluate the rate-distortion performance of our proposed models by drawing the rate-distortion curves. The distortion is assessed by PSNR while the rate is calculated by the bits per pixel (bpp). We first compare our proposed network with hand-crafted codec methods [6; 15; 40] and the LIC models that once reached state-of-the-art (SOTA) [4; 11; 17; 30; 45; 49] on the Kodak dataset. The result of the PSNR metric is presented in Fig. 3 (left), which demonstrates that our proposed methods could outperform other SOTA methods. The middle sub-figure and the right sub-figure in Fig. 3 are evaluated on the CLIC Professional Validation dataset and the Tecnick dataset, respectively. The SOTA results in various datasets show the generalization and robustness of our proposed model.

Compression Latency.As described in our introduction, we established a convolutional compression model for the pursuit of efficient compression. In Table 2, we present the coding latency, as well as the number of parameters and GFLOPs, by our proposed network and recent state-of-the-art methods [30; 48; 49]. The BD-rate values by different methods are also provided for reference, the anchor RD performance of which is set as the results of VVC (vtm-12.1) on Kodak dataset (BD-rate = 0%). As can be found in the table, our method achieves a better trade-off between compression performance and coding latency than the competing methods. With more than 20% less runtime, our model obtains about 2% BD-rate gain over .

  
**Model** & **CCA Loss** (proposed) & **Inference Time(ms)** & **BD-rate** \\  ChARM (even) & & 126 & -13.31\% \\ ChARM (even) & ✓ & 126 & -14.72\% \\  ChARM (uneven) & & 116 & -14.56\% \\ ChARM (uneven) & ✓ & 116 & -17.17\% \\  BPG & - & - & 0\% \\   

Table 1: Experiments on Kodak dataset. The effects of our proposed Causal Context Adjustment loss (CCA-loss) are verified on various channel-wise autoregressive models. Note that the anchor BD-rate is set as the results of BPG evaluated on Kodak dataset (BD-rate = 0%).

Figure 3: Rate-Distortion performance evaluation of PSNR on Kodak dataset (left), CLIC Professional Validation dataset (middle), Tecnick dataset (right), respectively.

Visualization Analysis.Our proposed learned image compression technology is capable of restoring the image details. Fig. 4 shows two sets of comparisons with the reconstruction of VVC  and a recent SOTA model . The visualization results are produced at low bit-rates on the Kodak dataset . The comparison of the reconstructed images demonstrates that our model restores more detailed and complicated textures than other methods. For example, we restore more sharp textures on the hat (_kodim4_), more details of the grassland (_kodim19_) and wrinkles on the sails (_kodim10_).

## 6 Conclusion

In this work, we explore the approach to adjust the causal context, which enables a superior channel-wise autoregressive model and more accurate estimation in probability distributions. By imposing the Causal Context Adjustment loss (CCA-loss) and the unevenly channel-wise grouped strategy on our proposed CNN-based model, we achieve state-of-the-art rate-distortion performance. Thanks to the advantages of convolutional neural network, our discussed unevenly grouped schedule and the training method by the proposed CCA-loss, our learned image compression model maintains a great trade-off between compression latency and RD performance. Furthermore, since we did not dive into the information redistributed phenomenon brought by the unevenly grouped strategy and CCA-loss training in this paper, the issue of the laws about the information distributed among the latent representation to be compressed is still worth investigating in the future.

    &  &  &  &  \\    & **Tot.** & & & & & \\  Zou et al.  & 424 & 248 & 176 & 99.83M & 200.11 & -4.01\% \\ Zhu et al.  & 272 & 129 & 143 & 56.93M & 364.08 & -3.00\% \\ Liu et al.  & 255 & 122 & 133 & 75.90M & 700.65 & -11.88\% \\ Ours & 201 & 109 & 92 & 64.89M & 615.93 & -13.87\% \\  VVC & - & - & - & - & - & 0\% \\   

Table 2: Comparison of coding complexity evaluated on Kodak dataset. All the models are evaluated on the same platform. The lower BD-rate is better.

Figure 4: Visualization of the reconstructed images (top: _kodim19_, middle: _kodim10_, bottom: _kodim4_) from Kodak dataset. The titles under the sub-figures are represented as [bpp! PSNR(dB)].