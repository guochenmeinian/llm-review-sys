# A Concept-Based Explainability Framework for Large Multimodal Models

Jayneel Parekh\({}^{1}\) Pegah Khayatan\({}^{1}\) Mustafa Shukor\({}^{1}\)

&Alasdair Newson\({}^{1}\) Matthieu Cord\({}^{1,2}\)

\({}^{1}\)ISIR, Sorbonne Universite, Paris, France \({}^{2}\)Valeo.ai, Paris, France

{jayneel.parekh, pegah.khayatan}@sorbonne-universite.fr

###### Abstract

Large multimodal models (LMMs) combine unimodal encoders and large language models (LLMs) to perform multimodal tasks. Despite recent advancements towards the interpretability of these models, understanding internal representations of LMMs remains largely a mystery. In this paper, we present a novel framework for the interpretation of LMMs. We propose a dictionary learning based approach, applied to the representation of tokens. The elements of the learned dictionary correspond to our proposed concepts. We show that these concepts are well semantically grounded in both vision and text. Thus we refer to these as "multi-modal concepts". We qualitatively and quantitatively evaluate the results of the learnt concepts. We show that the extracted multimodal concepts are useful to interpret representations of test samples. Finally, we evaluate the disentanglement between different concepts and the quality of grounding concepts visually and textually. Our implementation is publicly available.1

## 1 Introduction

Despite the exceptional capacity of deep neural networks (DNNs) to address complex learning problems, one aspect that hinders their deployment is the lack of human-comprehensible understanding of their internal computations. This directly calls into question their reliability and trustworthiness . Consequently, this has boosted research efforts in _interpretability/explainability_ of these models i.e. devising methods to gain human-understandable insights about their decision processes. The growth in ability of DNNs has been accompanied by a similar increase in their design complexity and computational intensiveness. This is epitomized by the rise of vision transformers  and large-language models (LLMs)  which can deploy up to tens of billions of parameters. The effectiveness of these models for unimodal processing tasks has spurred their use in addressing multimodal tasks. In particular, visual encoders and LLMs are frequently combined to address tasks such as image captioning and VQA . This recent class of models are referred to as large multimodal models (LMMs).

For interpretability research, LMMs have largely remained unexplored. Most prior works on interpreting models that process visual data, focus on convolutional neural network (CNN) based systems and classification as the underlying task. Multimodal tasks and transformer-based architectures have both been relatively less studied. LMMs operate at the intersection of both domains. Thus, despite their rapidly growing popularity, there have been very few prior attempts at understanding representations inside an LMM .

This paper aims to bridge some of these differences and study in greater detail the intermediate representations of LMMs. To this end, motivated by the concept activation vector (CAV) based approaches for CNNs [14; 15; 17; 22], we propose a novel dictionary-learning based _Concept eXplainability_ method designed for application to LMMs, titled _CoX-LMM_. Our method is used to learn a concept dictionary to understand the representations of a pretrained LMM for a given word/token of interest (Eg. 'Dog'). For this token, we build a matrix containing the LMM's internal representation of the token. We then linearly decompose this matrix using dictionary learning. The dictionary elements of our decomposition represent our concepts. The most interesting consequence of our method is that the learnt concepts exhibit a semantic structure that can be meaningfully grounded in both visual and textual domains. They are visually grounded by extracting the images which maximally activate these concepts. They can simultaneously be grounded in the textual domain by decoding the concept through the language model of the LMM and extracting the words/tokens they are most associated to. We refer to such concept representations as _multimodal concepts_. Our key contributions can be summarized as follows:

* We propose a novel concept-based explainability framework _CoX-LMM_, that can be used to understand internal representations of large multimodal models. To the best of our knowledge, this is the first effort targeting multimodal models at this scale.
* Our dictionary learning based concept extraction approach is used to extract a multimodal concept dictionary wherein each concept can be semantically grounded simultaneously in both text and vision. We also extend the previous concept dictionary-learning strategies using a Semi-NMF based optimization.
* We experimentally validate the notion of multimodal concepts through both, qualitative visualizations and quantitative evaluation. Our learnt concept dictionary is shown to possess a meaningful multimodal grounding covering diverse concepts, and is useful to locally interpret representations of test samples LMMs.

## 2 Related work

Large Multimodal Models (LMMs)Large language models (LLMs) [8; 21; 33; 44] have emerged as the cornerstone of contemporary multimodal models. Typical large multimodal models (LMMs) [1; 4; 25; 26] comprise three components: LLMs, visual encoders, and light-weight connector modules to glue the two models. Remarkably, recent works have demonstrated that by keeping all pretrained models frozen and training only a few million parameters in the connector (e.g., a linear layer), LLMs can be adapted to understand images, videos, and audios [12; 23; 31; 43; 45], thus paving the way for solving multi-modal tasks. However, there is still a lack of effort aimed at understanding why such frozen LLMs can generalize to multimodal inputs. In this study, we try to decode the internal representation of LLMs when exposed to multimodal inputs.

Concept activation vector based approachesConcept based interpretability aim to extract the semantic content relevant for a model . For post-hoc interpretation of pretrained models, concept activation vector (CAV) based approaches [15; 17; 46; 47; 48; 22; 48] have been most widely used. The idea of CAV was first proposed by Kim et al. . They define a concept as a set of user-specified examples. The concept is represented in the activation space of deep layer of a CNN by a hyperplane that separates these examples from a set of random examples. This direction in the activation space is referred to as the concept activation vector. Built upon CAV, ACE  automate the concept extraction process. CRAFT  proposed to learn a set of concepts for a class by decomposing activations of image crops via non-negative matrix factorization (NMF). Recently, Fel et al.  proposed a unified view of CAV-based approaches as variants of a dictionary learning problem. However, these methods have only been applied for interpretation of CNNs on classification tasks. LMMs on the contrary exhibit a different architecture. We propose a dictionary learning based concept extraction method, designed for LMMs. We also propose a Semi-NMF variant of the dictionary learning problem, which has not been previously considered for concept extraction.

Understanding VLM/LMM representationsThere has been an increasing interest in understanding internal representations of visual-language models (VLM) through the lens of multimodality. Shukor and Cord  analyse multimodal tokens and shows that despite being different, visual and perceptual tokens are implicitly aligned inside LLMs. Goh et al.  discover neurons termedmultimodal_, that activate for certain conceptual information given images as input. Recently proposed TEXTSPAN  and SpLiCE , aim to understand representations in CLIP  by decomposing its visual representations on textual representations. For LMMs, Palit et al.  extend the causal tracing used for LLMs to analyze information across different layers in an LMM. Schwettmann et al.  first proposed the notion of _multimodal neurons_ existing within the LLM part of an LMM. They term the neurons "multimodal" as they translate high-level visual information to corresponding information in text modality. The neurons are discovered by ranking them by a gradient based attribution score. Pan et al.  proposed a more refined algorithm to identify such neurons based on a different neuron importance measure that leverages architectural information of transformer MLP blocks. Instead, we propose to discover a concept structure in the token representations by learning a small dictionary of multimodally grounded concepts. Limiting the analysis to a specific token of interest allows our method to discover fine details about the token in the learnt concepts.

## 3 Approach

### Background for Large Multimodal Models (LMMs)

Model architecture.We consider a general model architecture for a large multimodal model \(f\), that consists of: a visual encoder \(f_{V}\), a trainable connector \(C\), and an LLM \(f_{LM}\) consisting of \(N_{L}\) layers. We assume \(f\) is pretrained for captioning task with an underlying dataset \(=\{(X_{i},y_{i})\}_{i=1}^{N}\) consisting of images \(X_{i}\) and their associated caption \(y_{i}\). \(\) and \(\) denote the space of images and set of text tokens respectively. Note that caption \(y_{i}\) can be viewed as a subset of all tokens. The input to the language model \(f_{LM}\) is denoted by the sequence of tokens \(h^{1},h^{2},...,h^{p}\) and the output as \(\). The internal representation of any token at some layer \(l\) and position \(p\) inside \(f_{LM}\) is denoted as \(h^{p}_{(l)}\), with \(h^{p}_{(0)}=h^{p}\). Note that \(h^{p}_{(l)}\) is same as the residual stream representation in LLM transformers  at position \(p\) and layer \(l\). For the multimodal model, the input sequence of tokens for \(f_{LM}\) consists of the concatenation of: (1) \(N_{V}\) visual tokens provided by the visual encoder \(f_{V}\) operating on an image \(X\), followed by the connector \(C\), and (2) linearly embedded textual tokens previously predicted by \(f_{LM}\). For \(p>N_{V}\), this can be expressed as:

\[^{p}=f_{LM}(h^{1},h^{2},,h^{N_{V}},,h^{p}),\] (1)

Figure 1: Overview of multimodal concept extraction and grounding in _CoX-LMM_. Given a pretrained LMM for captioning and a target token (for eg. ‘Dog’), our method extracts internal representations of \(f\) about \(t\), across many images. These representations are collated into a matrix \(\). We linearly decompose \(\) to learn a concept dictionary \(\) and its coefficients/activations \(\). Each concept \(u_{k}\), is multimodally grounded in both visual and textual domains. For text grounding, we compute the set of most probable words \(_{k}\) by decoding \(u_{k}\) through the unembedding matrix \(W_{U}\). Visual grounding \(_{k,MAS}\) is obtained via \(v_{k}\) as the set of most activating samples.

where \(h^{1},,h^{N_{V}}=C(f_{V}(X))\), and \(h^{p}=(^{p-1})\) for \(p>N_{V}\), where Emb denotes the token embedding function. To start the prediction, \(h^{N_{V}+1}\) is defined as the beginning of sentence token. The output token \(^{p}\) is obtained by normalizing \(h^{p}_{(N_{L})}\), followed by an unembedding layer that applies a matrix \(W_{U}\) followed by a softmax. The predicted caption \(\) consists of the predicted tokens \(=\{^{p}\}_{p>N_{V}}\) until the end of sentence token.

TrainingThe model is trained with next token prediction objective, to generate text conditioned on images in an auto-regressive fashion. In this work we focus on models trained to "translate" images into text, or image captioning models. These models keep the visual encoder \(f_{V}\) frozen and only train the connector \(C\). Recent models also finetune the LLM to improve performance. Our approach can be applied in either case, and in our experiments we consider both type of LMMs. However, we find the generalization of LLMs to multimodal inputs is an interesting phenomenon to understand, thus we focus more on the setup where the LLM is kept frozen.

Transformer representations viewCentral to many previous approaches interpreting decoder-only LLM/transformer architectures, is the "residual stream view" of internal representations, first proposed in . Herein, the network is seen as a composition of various computational blocks that "read" information from the residual stream of token representations \(h^{p}_{(l)}\), perform their computation, and add or "write" their output back in the residual stream. This view can be summarized as:.

\[h^{p}_{(l+1)}=h^{p}_{(l)}+a^{p}_{(l)}+m^{p}_{(l)}\] (2)

\(a^{p}_{(l)}\) denotes the information computed by attention function at layer \(l\) and position \(p\). It has a causal structure and computes its output using \(h^{1}_{(l)},...,h^{p}_{(l)}\). \(m^{p}_{(l)}\) denotes the information computed by the MLP block. It is a feedforward network (FFN) with two fully-connected layers and an intermediate activation function \(\), that operates on \(h^{p}_{(l)}+a^{p}_{(l)}\). The output of \((.)\) is referred to as FFN activations.

### Method overview

Fig. 1 provides a visual summary of the whole _CoX-LMM_ pipeline. Given a pretrained LMM \(f\) and a token of interest \(t\), our method consists of three key parts:

1. Selecting a subset of images \(\) from dataset \(\), relevant for target token \(t\). We extract representations by processing samples in \(\) through \(f\). The extracted representations of dimension \(B\) are collected in a matrix \(^{B M}\), where \(M\) is number of samples in \(\).
2. Linearly decomposing \(\) into its constituents, that includes a dictionary of learnt concepts \(^{B K}\) of size \(K\) and coefficient/activation matrix \(^{K M}\).
3. Semantically grounding the learnt "multimodal concepts", contained in dictionary \(\) in both visual and textual modalities.

We emphasize at this point that our main objective in employing dictionary learning based concept extraction is to understand internal representations of an LMM. Thus, our focus is on validating the use of the learnt dictionary for this goal, and not to interpret the output of the model, which can be readily accomplished by combining this pipeline with some concept importance estimation method . The rest of the section is devoted to elaborate on each of the above three steps.

### Representation extraction

To extract relevant representations from the LMM about \(t\) that encode meaningful semantic information, we first determine a set of samples \(\) from dataset \(=\{(X_{i},y_{i})\}_{i=1}^{N}\) for extraction. We consider the set of samples where \(t\) is predicted as part of the predicted caption \(\). This allows us to further investigate the model's internal representations of \(t\). To enhance visual interpretability for the extracted concept dictionary, we additionally limit this set of samples to those that contain \(t\) in the ground-truth caption. Thus, \(\) is computed as:

\[=\{X_{i} t f(X_{i})t y_{i}(X_{i},y_{i})\}.\] (3)

Given any \(X\), we propose to extract the residual stream representation \(h^{p}_{(L)}\) from a deep layer \(L\), at the first position in the predicted caption \(p>N_{V}\), such that \(^{p}=t\). The representation of each sample \(X_{j}\) is then stacked as columns of the matrix \(=[z_{1},...,z_{M}]^{B M}\). Note that the representations of text tokens in \(f_{LM}\) can possess a meaningful multimodal structure as they combine information from visual token representations \(h^{p}_{(l)},p N_{V}\). In contrast to \(a^{p}_{(l)}\) and \(m^{p}_{(l)}\), that represent residual information at layer \(l\), \(h^{p}_{(L)}\) contains the aggregated information computed by the LMM till layer \(L\), providing a holistic view of its computation across all previous layers.

### Decomposing the representations

The representation matrix \(\), is decomposed as product of two low-rank matrices \(^{B K},^{K M}\) of rank \(K<<(B,M)\), where \(K\) denotes the number of dictionary elements. The columns of \(=[u_{1},...,u_{K}]\) are the basis vectors which we refer to as concept-vectors/concepts. The rows of \(\) or columns of \(^{T}=[v_{1},...,v_{K}],v_{i}^{M}\) denote the activations of \(u_{i}\) for each sample. This decomposition, as previously studied in , can be optimized with various constraints on \(,\), each leading to a different dictionary. The most common ones include PCA (constraint: \(^{T}=\)), K-Means (constraint: columns of \(\) correspond to columns of identity matrix) and NMF (constraint: \(, 0\)). NMF is considered to yield most interpretable results . However, for our use case, NMF is not useful as representation matrix \(\) is not non-negative. Instead, we propose to employ a relaxed version of NMF, Semi-NMF , which allows the decomposition matrix \(\) and basis vectors \(\) to contain mixed values, and only forces the activations \(\) to be non-negative. Note that given its relations to clustering algorithms , enforcing non-negative combinations of decompositions is still valued from an interpretability perspective. Since we expect only a small number of concepts to be present in any given sample, we also encourage sparsity in activations \(\). The optimization problem to decompose \(\) via Semi-NMF can be summarized as:

\[^{*},^{*}=_{,}\ ||- ||_{F}^{2}+||||_{1} s.t.\  0,\ \ ||u_{k}||_{2} 1\  k\{1,...,K\}.\] (4)

Given any image \(X\) where token \(t\) is predicted by \(f\), we can now define the process of computing activations of concept dictionary \(^{*}\) for given \(X\), denoted as \(v(X)^{K}\). To do so, we first extract the token representation for \(X,z_{X}^{B}\) with the process described in Sec. 3.3. Then, \(z_{X}\) is projected on \(^{*}\) to compute \(v(X)\). In the case of Semi-NMF, this corresponds to \(v(X)=_{v 0}||z_{X}-^{*}v||_{2}^{2}+||v||_{1}\). The activation of \(u_{k}^{*}\) is denoted as \(v_{k}(X)\).

### Using the concept dictionary for interpretation

**Multimodal grounding of concepts.** Given the learnt dictionary \(^{*}\) and corresponding activations \(^{*}\), the key objective remaining is to ground the understanding of any given concept vector \(u_{k},k\{1,...,K\}\) in the visual and textual domains. Specifically, for visual grounding, we use prototyping  to select input images (among the decomposed samples), that maximally activate \(u_{k}\). Given the number of samples extracted for visualization \(N_{MAS}\), the set of maximum activating samples (MAS) for component \(u_{k}\), denoted as \(_{k,MAS}\) can be specified as follows (\(|.|\) is absolute value):

\[_{k,MAS}=*{argmax}_{,||||=N_{MAS}}_{X}|v_{k}(X)|.\] (5)

For grounding in textual domain, we note that the concept vectors are defined in the token representation space of \(f_{LM}\). Thus we leverage the insights from "Lens" based methods  that attempt to understand LLM representations. In particular, following , we use the unembedding layer to map \(u_{k}\) to the token vocabulary space \(\), and extract the most probable tokens. That is, we extract the tokens with highest probability in \(W_{U}u_{k}\). The decoded tokens with highest probabilities are then filtered for being an english, non-stop-word with at least 3 characters, to eliminate unnecessary tokens. The final set of words is referred to as grounded words for concept \(u_{k}\) and denoted as \(_{k}\). Fig. 2 illustrates an example of grounding of a concept extracted for token "Dog" in vision (5 most activating samples) and text (top 5 decoded words).

**Most activating concepts for images.** To understand the LMM's representation of a given image \(X\), we now define the _most activating concepts_. Firstly, we extract the representaions \(z_{X}\) of the image with the same process as described previously. We then project \(z_{X}\) on \(^{*}\) to obtain \(v(X)^{K}\). We define the most activating concepts, which we denote \((X)\), as the set of \(r\) concept vectors (in\(^{*}\)) whose activations \(v_{k}(X)\) have the largest magnitude. One can then visualize the multimodal grounding of \((X)\). This step could be further combined with concept importance estimation techniques  to interpret the model's prediction for token \(t\), however, the focus of this paper is to simply understand the internal representation of the model, for which the current pipeline suffices.

## 4 Experiments

Models and dictionary learning.In the main paper, we focus on experiments with the DePALM model  that is trained for captioning task on COCO dataset . The model consists of a frozen ViT-L/14 CLIP  encoder as the visual encoder \(f_{V}\). It is followed by a transformer connector to compress the encoding into \(N_{V}=10\) visual tokens. The language model \(f_{LM}\) is a frozen OPT-6.7B  and consists of 32 layers. Additional experiments with LLaVA  are in Appendix A. For uniformity and fairness all the results in the main paper are reported with number of concepts \(K=20\) and for token representations from \(L=31\), the final layer before unembedding layer. For Semi-NMF, we set \(=1\) throughout. We consider the 5 most activating samples in \(_{k,MAS}\) for visual grounding for any \(u_{k}\). For text grounding, we consider top-15 tokens for \(_{k}\) before applying the filtering described in Sec 3.5.

The complete dataset consists of around 120,000 images for training, and 5000 each for validation and testing with 5 captions per image following the Karpathy split. We conduct our analysis separately for various common objects in the dataset: "Dog", "Bus", "Train", "Cat", "Bear", "Baby", "Car", "Cake". The extension to other classes/tokens remains straightforward and is discussed in Appendix D. The precise details about number of samples for learning the dictionary, or testing, is available in Appendix C. The implementation of our method is publicly available on GitHub 2

### Evaluation setup

We evaluate the quality of learnt concept dictionary \(^{*}\) on three axes: (i) Its use during inference to interpret representations of LMMs for test samples, (ii) The overlap/entanglement between grounded words of concepts in the dictionary and (iii) the quality of visual and text grounding of concepts (used to understand a concept itself). We discuss concrete details about each axis below:

**Concept extraction during inference:** To evaluate the use of \(^{*}\) in understanding any test sample \(X\), we first estimate the top \(r\) most activating concepts activations, \((X)\) (Sec. 3.5). We then estimate the correspondence between the test image \(X\) and the grounded words \(_{k}\) of \(u_{k}(X)\). This correspondence is estimated via two different metrics. The primary metric is the average CLIPScore  between \(X\) and \(_{k}\). This directly estimates correspondence between the test image embedding with the grounded words of the top concepts. The secondary metric is the average BERTScore (F1)  between the ground-truth captions \(y\) associated with \(X\) and the words \(_{k}\). These metrics help validate the multimodal nature of the concept dictionaries. Their use is inspired from . Details for their implementation is in Appendix C.

**Overlap/entanglement of learnt concepts:** Ideally, we would like each concept in \(^{*}\) to encode distinct information about the token of interest \(t\). Thus two different concepts \(u_{k},u_{l},k l\) should be associated to different sets of words. To quantify the entanglement of learnt concepts, we compute the overlap between the grounded words \(_{k},_{l}\). The overlap for a concept \(u_{k}\) is defined as an average of its fraction of common words with other concepts. The overlap/entanglement metric for a dictionary

Figure 2: Example of multimodal concept grounding in vision and text. Five most activating samples (among decomposed in \(\)) and five most probable decoded words are shown.

\(^{*}\) is defined as the average of overlap of each concept.

\[(^{*})=_{k}(u_{k}), (u_{k})=_{l=1,l k}^{K}_{l }_{k}|}{|_{k}|}\]

**Multimodal grounding of concepts:** To evaluate the quality of visual/text grounding of concepts (\(_{k,MAS},_{k}\)), we measure the correspondence between visual and text grounding of a given concept \(u_{k}\), i.e. the set of maximum activating samples \(_{k,MAS}\) and words \(_{k}\), using CLIPScore and BERTScore as described above.

**Baselines:** One set of methods for evaluation are the variants of _CoX-LMM_ where we employ different dictionary learning strategies: PCA, KMeans and Semi-NMF. For evaluating concept extraction on test data with CLIPScore/BERTScore we compare against the following baselines:

- _Rnd-Words_: This baseline considers Semi-NMF as the underlying learning method. However, for each component \(u_{k}\), we replace its grounded words \(_{k}\) by a set of random words \(_{k}\) such that \(|_{k}|=|_{k}|\) and the random words also satisfy the same filtering conditions as grounded words i.e. they are non-stopwords from english corpus with more than two characters. We do this by decoding a randomly sampled token representation and adding the top decoded words if they satisfy the conditions.

- _Noise-Imgs_: This baseline uses random noise as images and then proceeds with exactly same learning procedure as Semi-NMF including extracting activations from the same positions, and same parameters for dictionary learning. Combined with the Rnd-Words baseline, they ablate two parts of the concept extraction pipeline.

- _Simple_: This baseline considers a simple technique to build the dictionary \(^{*}\) and projecting test samples. It builds \(^{*}\) by selecting token representations in \(\) with the largest norm. The projections are performed by mapping the test sample representation to the closest element in \(^{*}\). For deeper layers, this provides a strong baseline in terms of extracted grounded words \(_{k}\) which are related to token of interest \(t\), as they are obtained by directly decoding token representations of \(t\).

We also report score using ground-truth captions _(GT captions)_ instead of grounded words \(_{k}\), to get the best possible correspondence score. The overlap/entanglement in concept dictionary is compared between the non-random baselines: Simple, PCA, K-Means and Semi-NMF. For evaluating the visual/text grounding we compare against the random words keeping the underlying set of MAS, \(_{k,MAS}\), same for both.

### Results and discussion

**Quantitative results**  Tab. 1 reports the test top-1 CLIPScore/BERTScore for all baselines and Semi-NMF on different target tokens. Appendix D contains detailed results for other tokens as well as for the PCA and K-Means variants. We report the results for only the top-\(1\) activating concept, as the KMeans and Simple baselines map a given representation to a single cluster/element.

  Token & Metric & Rnd-Words & Noise-Imgs & Simple & Semi-NMF (Ours) & GT-captions \\   & CS top-1 (\(\)) & 0.519 \(\) 0.05 & 0.425 \(\) 0.06 & 0.546 \(\) 0.08 & **0.610 \(\) 0.09** & 0.783 \(\) 0.06 \\  & BS top-1 (\(\)) & 0.201 \(\) 0.04 & 0.306 \(\) 0.05 & 0.346 \(\) 0.08 & **0.405 \(\) 0.07** & 0.511 \(\) 0.11 \\   & CS top-1 (\(\)) & 0.507 \(\) 0.05 & 0.425 \(\) 0.08 & **0.667 \(\) 0.06** & 0.634 \(\) 0.08 & 0.736 \(\) 0.05 \\  & BS top-1 (\(\)) & 0.200 \(\) 0.05 & 0.303 \(\) 0.06 & 0.390 \(\) 0.05 & **0.404 \(\) 0.07** & 0.466 \(\) 0.11 \\   & CS top-1 (\(\)) & 0.496 \(\) 0.05 & 0.410 \(\) 0.07 & 0.642 \(\) 0.06 & **0.646 \(\) 0.07** & 0.727 \(\) 0.05 \\  & BS top-1 (\(\)) & 0.210 \(\) 0.06 & 0.253 \(\) 0.06 & **0.392 \(\) 0.07** & 0.378 \(\) 0.07 & 0.436 \(\) 0.08 \\   & CS top-1 (\(\)) & 0.539 \(\) 0.04 & 0.461 \(\) 0.04 & 0.589 \(\) 0.07 & **0.627 \(\) 0.06** & 0.798 \(\) 0.05 \\  & BS top-1 (\(\)) & 0.207 \(\) 0.07 & 0.307 \(\) 0.03 & 0.425 \(\) 0.10 & **0.437 \(\) 0.08** & 0.544 \(\) 0.10 \\  

Table 1: Test data mean CLIPScore and BERTScore for top-1 activating concept for all baselines on five tokens. CLIPScore denoted as CS, and BERTScore as BS. Statistical significance is in Appendix D. Our _CoX-LMM_ framework is evaluated with Semi-NMF as underlying dictionary learning method. Higher scores are better. Best score in **bold**, second best is underlined.

Notably, Semi-NMF generally outperforms the other baselines although the Simple baseline performs competitively. More generally, Semi-NMF, K-Means, and Simple tend to clearly outperform Rnd-Words, Noise-Imgs and PCA on these metrics, indicating that these systems project representations of test images to concepts whose associated grounded words correspond well with the visual content.

Tab. 2 reports the overlap between concepts for Simple baseline and PCA, K-Means and Semi-NMF variants of _CoX-LMM_. Interestingly, K-Means and Simple baseline perform significantly worse than Semi-NMF/PCA with a high overlap between grounded words, often exceeding 40%. PCA outperforms other methods with almost no overlap while Semi-NMF shows some overlap. Overall, Semi-NMF strikes the best balance among all the methods, in terms of learning a concept dictionary useful for understanding test image representations, but which also learns diverse and disentangled concepts. Thus, for further _CoX-LMM_ experiments, we consider Semi-NMF as the underlying dictionary learning method.

Fig. 3 shows an evaluation of visual/text grounding of concepts learnt by Semi-NMF. Each point on the figure denotes the CLIP-Score (left) or BERTScore (right) for correspondence between samples \(_{k,MAS}\) and words \(_{k}\) for concept \(u_{k}\) against random words baseline. We see that for both metrics, vast majority of concepts lie above the \(y=x\) line, indicating that grounded words correspond much better to content of maximum activating samples than random words.

**Qualitative results** Fig. 4 shows visual and textual grounding of concepts extracted for token 'dog'. For brevity, we select 8 out of 20 concepts for illustration. 2. Grounding for all concepts extracted for 'dog' and other tokens are in Appendix E. The concept visualizations/grounding for 'Dog' reveal interesting insights about the global structure of the LMM's representation. Extracted concepts capture information about different aspects of a 'dog'. The LMM separates representation

   Token & Simple & PCA & KMeans & Semi-NMF \\  Dog & 0.371 & **0.004** & 0.501 & 0.086 \\  Bus & 0.622 & **0.002** & 0.487 & 0.177 \\  Train & 0.619 & **0.015** & 0.367 & 0.107 \\  Cat & 0.452 & **0.000** & 0.500 & 0.146 \\   

Table 2: Overlap between learnt concepts. Lower is better. Best score in **bold**, second best underlined.

Figure 4: Visual/textual grounding for 8 out of 20 concepts for ’Dog’ token (layer 31). For each concept we illustrate the set of 5 most activating samples and 5 most probable decoded words.

Figure 3: Evaluating visual/text grounding (CLIP-Score/BERTScore). Each point denotes score for grounded words of a concept (Semi-NMF) vs Rnd-Words w.r.t the same visual grounding.

of animal 'Dog' with a 'hot dog' (Concept 1). Specifically for 'Dog', Concepts (2), (3) capture information about color: 'black', 'brown'. Concept (6) encodes information about 'long hair' of a dog, while concept (5) activates for'small/puppy-like' dogs. Beyond concepts activating for specific characteristics of a 'dog', we also discover concepts describing their state of actions (Concept (4) 'playing/running'), common scenes they can occur in (Concept (8), 'herd'), and correlated objects they can occur with (Concept (7), 'cat and dog'). We observe such diverse nature of extracted concepts even for other tokens (Appendix E). The information about concepts can be inferred via both the visual images and the associated grounded words, highlighting their coherent multimodal grounding. Notably, compared to solely visual grounding as for CAVs for CNNs, the multimodal grounding eases the process to understand a concept.

Fig. 5 illustrates the use of concept dictionaries (learnt via Semi-NMF) to understand test sample representations for tokens 'Dog', 'Cat' and 'Bus'. For each sample we show the normalized activations of the three most activating concepts, and their respective multimodal grounding. Most activating concepts often capture meaningful and diverse features of a given sample. For instance, for first sample containing a 'Dog', the concepts for "long hair", "small/tiny/puppy", and "black/white color" all simultaneously activate. The grounding for first two concepts was also illustrated in Fig. 4. Additional visualizations for test samples are shown in Appendix E, wherein we qualitatively compare interpretations of Semi-NMF to K-Means, PCA variants and Simple baseline.

**Layer ablation** We analyze the quality of multimodal grounding of concepts across different layers \(L\). The CLIPScore between \((_{k,MAS},_{k})\), averaged over all concepts \(u_{k}\) is shown in Fig. 6, for 'Dog' and 'Cat' for all layers in \(f_{LM}\). For early layers the multimodal grounding is no better than Rnd-Works. Interestingly, there is a noticeable increase around (\(L=20\) to \(L=25\)), indicating that the multimodal structure of internal token representations starts to appear at this point. This also validates our choice that deeper layers are better suited for multimodal concepts.

**Additional experiments and discussion.** We conduct a preliminary study to analyze the polysemanticity/superposition in concept dictionaries in Appendix B. A qualitative analysis for grounding of extracted concepts for different layers is available in Appendix F. _CoX-LMM_ can be also be applied to understand the processing of visual/perceptual tokens inside the LMM which also exhibit this multimodal structure. The experiment for the same can be found in Appendix G. Limitations of our method are discussed in Appendix H, and the broader societal impacts are discussed in Appendix I.

Figure 5: Local interpretations for test samples for different tokens (‘Dog’, ‘Cat’, ‘Bus’) with Semi-NMF (layer 31). Visual/text grounding for three highest concept activations (normalized) is shown.

Figure 6: Mean CLIPScore between visual/text grounding \(_{k,MAS},_{k}\) for all concepts (Semi-NMF), across different layers \(L\). Results are for tokens ‘Dog’ and ‘Cat’.

Conclusion

In summary, we have presented a novel dictionary learning based concept extraction framework, useful to understand internal representations of a large multimodal model. The approach relies on decomposing representations of a token inside a pretrained LMM. To this end, we also propose a Semi-NMF variant of the concept dictionary learning problem. The elements of the learnt concept dictionary are grounded in the both text and visual domains, leading to a novel notion of _multimodal concepts_ in the context of interpretability. We quantitatively and qualitatively show that (i) the multimodal grounding of concepts is meaningful, and (ii) the learnt concepts are useful to understand representations of test samples. We hope that our method inspires future work from research community towards designing concept based explainability methods to understand LMMs.