# SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection

Yuxuan Li\({}^{1}\) Xiang Li\({}^{1,2,}\) Weijie Li\({}^{3}\) Qibin Hou\({}^{1,2}\) Li Liu\({}^{3}\) Ming-Ming Cheng\({}^{1,2}\) Jian Yang\({}^{1,}\)

\({}^{1}\) PCA Lab, VCIP, CS, Nankai University \({}^{2}\)NKIARI, Futian, Shenzhen

\({}^{3}\)Academy of Advanced Technology Research of Hunan

\({}^{}\) Corresponding Authors

yuxuan.li.17@ucl.ac.uk, {xiang.li.implus,houqb,cmm,csjyang}@nankai.edu.cn

###### Abstract

Synthetic Aperture Radar (SAR) object detection has gained significant attention recently due to its irreplaceable all-weather imaging capabilities. However, this research field suffers from both limited public datasets (mostly comprising <2K images with only mono-category objects) and inaccessible source code. To tackle these challenges, we establish a new benchmark dataset and an open-source method for large-scale SAR object detection. Our dataset, SARDet-100K, is a result of intense surveying, collecting, and standardizing 10 existing SAR detection datasets, providing a large-scale and diverse dataset for research purposes. To the best of our knowledge, SARDet-100K is the first COCO-level large-scale multi-class SAR object detection dataset ever created. With this high-quality dataset, we conducted comprehensive experiments and uncovered a crucial challenge in SAR object detection: the substantial disparities between the pretraining on RGB datasets and finetuning on SAR datasets in terms of both data domain and model structure. To bridge these gaps, we propose a novel Multi-Stage with Filter Augmentation (MSFA) pretraining framework that tackles the problems from the perspective of data input, domain transition, and model migration. The proposed MSFA method significantly enhances the performance of SAR object detection models while demonstrating excellent generalizability and flexibility across diverse models. This work aims to pave the way for further advancements in SAR object detection. The dataset and code is available at https://github.com/zcablii/SARDet_100K.

## 1 Introduction

Synthetic Aperture Radar (SAR) [57; 60]is a pivotal technology in remote sensing, providing numerous advantages over traditional optical sensors. Notably, SAR possesses the capability to acquire geographical images under any weather conditions, irrespective of factors such as sunlight, land cover, or certain types of camouflage, as demonstrated in Fig. 1(a). As a consequence of these advantages, SAR has found extensive applications in critical domains, including national defence , humanitarian relief [3; 68], camouflage detection , and geological exploration [51; 24].

With its invaluable benefits, the field of SAR object detection has garnered increasing attention. In recent years, there has been a substantial increase in the number of research papers focusing on this field, as illustrated in Fig. 1(b). Despite the increasing influence, this research area has suffered from significant challenges including limited resources and transferring gaps.

**Limited resources.** A significant obstacle in high-resolution SAR image object detection is the sensitivity of SAR images, coupled with the high costs associated with annotating these images. Thisseverely restricts the availability of public datasets. Existing datasets, such as SAR-AIRcraft , AirSARShip , SSDD , and HRSID , typically consist of a singular type of object against a simplistic background. Moreover, these datasets are generally limited in scale, potentially introducing bias when evaluating different methodologies. Additionally, a notable barrier to advancing research in SAR object detection is the lack of publicly accessible source code, making it challenging to reproduce previous research findings and conduct fair comparisons or build upon existing work.

To address this problem, we merge the most publicly available SAR detection datasets. This effort includes a comprehensive review of current public SAR detection resources, followed by the collection and standardization of these datasets into a uniform format, creating a unified large-scale multi-class dataset for SAR object detection, named SARDet-100k. This dataset comprises approximately 117k images and 246k instances of objects across six distinct categories. To our knowledge, SARDet-100k is the first dataset of COCO-scale magnitude in this research area. It significantly contributes to overcoming the previously mentioned limitations by providing a rich resource for the development and evaluation of SAR object detection models. Moreover, the dataset and source code will be made publicly available.

**Transferring gaps.** Through our empirical research and detailed analysis, we have identified that a principal hurdle in SAR object detection is the significant domain gap and model gap encountered when transferring a backbone network pretrained on natural RGB datasets (e.g., ImageNet ), to a detection network on SAR imagery. The domain gap stems from the stark visual discrepancies between RGB and SAR imagery, whereas the model gap arises from the model differences between the pretrained backbone and the whole detection framework employed in the downstream task.

To mitigate the aforementioned domain gap and model gap, we propose a novel Multi-Stage with Filter Augmentation (MSFA) pretraining framework to bridge these gaps. This framework addresses the challenge from multiple angles: data input, domain transition, and model migration, each tailored to the unique properties of the SAR image detection task. For data input: to address the input domain gap between the pretrain and finetune datasets, we employ traditional, handcrafted feature descriptors. These descriptors efficiently transform the input data from pixel space to a feature space that is not only robust to noise but also statistically narrows the gap between data from RGB and SAR modalities (see Fig. 2(a)), thereby enhancing the transferability of pretrained knowledge. For domain transition: we propose a domain transition bridge utilizing an optical remote sensing detection dataset. This bridge connects natural RGB images through optics correlation and SAR images through object correlation, establishing a hierarchical pretraining approach that effectively closes the domain gap between RGB and SAR imagery (see Fig. 2(b)). For model migration: to guarantee thorough training of the entire detection framework and to facilitate complete model migration for finetuning, we employ the entire detector as a bridging model throughout the multi-stage pretraining process.

The MSFA framework demonstrates remarkable efficacy in reducing the substantial domain and model gaps typically encountered between the pretraining and finetuning stages. MSFA is not only effective but also general and applicable across various modern deep neural networks.

Figure 1: **(a) Advantages of SAR image: independent of weather conditions, sunlight and land cover. (b) Number of papers (thousands) retrieved from Google Scholar using keywords “SAR Detection”.**

Our contribution to the field of SAR object detection can be concluded into the following FOUR points:

* Introduction of the first COCO-level large-scale dataset for SAR multi-category object detection.
* Identification of critical gaps in traditional model pretrain and finetune approaches for SAR object detection.
* Proposal of a Multi-Stage with Filter Augmentation (MSFA) pretraining framework, which demonstrates remarkable effectiveness, as well as excellent generalizability and flexibility across various deep network models.
* Establishment of a new benchmark in SAR object detection by releasing the datasets and code associated with our research. This contribution is expected to foster further advancements and progress in the field.

## 2 Related Work

### SAR Images and Handcraft Features

SAR imaging often suffers from poor image quality due to multiplicative speckle noise and artifacts [57; 47]. To mitigate this, many traditional handcrafted feature descriptors have been developed or adapted to extract more discernible features from SAR images. These include Histogram of Oriented Gradients  (HOG), Canny Edge Detector , Gradient by Ratio Edge (GRE) , Haar-like  Feature Descriptor and Wavelet Scattering Transform  (WST). Early works employed traditional algorithms, such as HOG for SAR object recognition [56; 49], Canny [28; 38] for edge detection. However, in recent years, the field of SAR image analysis has been largely dominated by deep learning approaches.

While recent studies have focused on tasks related to low-level processing [69; 86], classification [83; 85; 93; 26; 50; 78] and pretrain [29; 29], they have attempted to integrate classic handcrafted features into modern neural networks for robust SAR image feature extraction and refinement. In contrast, our work does not simply inject such handcrafted features into networks, but explores the benefits and potentials of handcrafted features in domain adaptation and SAR object detection under modern deep neural networks. This research area remains largely unexplored, and our work aims to bridge this gap.

### SAR Object Detection

Various popular deep learning-based object detection frameworks, including RetinaNet , FCOS , GFL , RCNN series [53; 4], YOLO series [52; 10], and DETR , demonstrate remarkable generalizability in the field of general object detection. Additionally, modern backbone networks such as ConvNext , VAN , LSKNet  and Swin Transformer  are designed to efficiently and effectively model visual features. However, SAR image object detection poses unique challenges due to factors such as small object size, speckle noise, and sparse information inherent in SAR images. As a result, recent deep learning methods for SAR object detection primarily focus on network and module design to address these challenges. Approaches like MGCAN , MSSDNet , and SEFEPNet  enhance object features through multiscale feature fusion. Quad-FPN  combines four distinct feature pyramid networks for thorough multiscale feature interaction to alleviate noise interferences and multi-scale object feature misalignment. PADN  and EWFAN  employ attention mechanisms to enhance object features in the presence of SAR speckle noise. CenterNet++ , an extension of CenterNet , incorporates feature enhancement, multi-scale fusion, and head refinement modules to improve the detector's robustness specifically for SAR images. Additionally, CRTransSar , built on the high-performance Swin transformer , leverages context representation learning to enhance object features.

While most existing works concentrate on mitigating SAR speckle noise interference through network structure improvements, few attempts to address the issue at the level of input data. Furthermore, most studies utilize ImageNet pretrained backbones as the initialization of the detection framework, overlooking the substantial domain gap between the pretrained nature scenes dataset and the finetuned SAR dataset, as well as the model gap between the backbone and the entire detection framework. Instead, we seek to address these unique challenges through a carefully designed pretraining strategy.

## 3 A New Benchmark Dataset for SAR Object Detection

### Current Status

SAR images are typically captured by satellites, and there is a wealth of low-resolution SAR imagery available, often with a Ground Sample Distance (GSD) of 10m \(\) 10m or larger. Platforms like Sentinel-1  provide access to these images, which offer a macroscopic view of various geophysical places such as cities, mountains, rivers, and cultivated land. This makes them particularly advantageous for scene classification tasks. However, the inherent low resolution of these images constrains their capability to delineate fine details of smaller objects, such as ships, cars, and airplanes. Conversely, high-resolution SAR images provide more detailed information but require significant hardware resources. Moreover, these high-definition images often encompass sensitive information, making them unsuitable for public release. Furthermore, acquiring high-resolution SAR datasets can be very expensive, posing significant challenges to their accessibility.

Numerous research teams frequently encounter budgetary limitations that restrict their capacity to obtain a large and diverse collection of high-resolution SAR datasets. These financial constraints not only limit the scope of geographical areas that can be covered but also affect the variety of data sources that can be accessed. Consequently, the datasets made available by these groups often lack diversity, particularly in aspects such as spectral bands, polarization, and resolution. From a researcher's perspective, evaluating models on such small and homogeneous datasets can introduce bias and lead to unfair performance comparisons.

### SARDet-100k

To address the aforementioned challenges, we undertake a thorough survey of SAR object detection datasets. As a result, we carefully collect a total of **10** publicly available high-quality datasets that are not only diverse but also have no conflicting object categories. These data are released by or collected from different countries and institutions, such as scientific research departments in China, space departments in Europe, and military departments in the United States. Detailed information

 Datasets & Target & Res. (m) & Band & Polarization & Satellites & License \\  AIR\_SARShip  & S & 1,3m & C & VV & GF-3 & - & - \\ HRSID  & S & 0.5\(\)3m & C/X & HH, HV, VH, VV & -1B,TerraSAR-X,TanDEMX & GNU General Public \\ MSAR  & A, T, B, S & \(\)1m & C & HH, HV, VH, VV & HISEA-1 & CC BY-NC 4.0 \\ SADD  & A & 0.5\(\)3m & X & HH & TerraSAR-X & - & - \\ SAR-AIRcraft  & A & 1m & C & Uni-polar & GF-3 & CC BY-NC 4.0 \\ ShipDataset  & S & 3\(\)25m & C & HH, VV, VH, HV & S-1,GF-3 & - \\ SSDD  & S & 1\(\)15m & C/X & HH, VV, VH, HV & S-1.RadarSat-2,TerraSAR-X & Apache2.0 \\ OGSOD  & B, H, T & 3m & C & VV/VH & GF-3 & - \\ SIVED  & C & 0.1,0.3m & Ka,Ku,X & VV/HH & Airborne SAR synthetic slice & - \\ 

Table 2: SARDet-100K source datasets information. GF-3: Gaofen-3, S-1: Sentinel-1. Target categories S: ship, A: aircraft, C: car, B: bridge, H: harbour, T: tank.

  &  &  &  \\    & &  & Val &  &  &  & Val &  &  \\  AIR\_SARShip 1*  & 438 & 23 & 40 & 501 & 816 & 33 & 209 & 1,058 & 2.11 \\ AIR\_SARShip 2*  & 270 & 15 & 15 & 300 & 1,819 & 127 & 94 & 2,040 & 6.80 \\ HRSID  & 3642 & 981 & 981 & 5,604 & 11,047 & 2,975 & 2,947 & 16,969 & 3.03 \\ MSAR*  & 27.159 & 1.479 & 1.520 & 30,158 & 58,988 & 3,091 & 3,123 & 65,202 & 2.16 \\ SADD  & 795 & 44 & 44 & 883 & 6,891 & 448 & 496 & 7,835 & 8.87 \\ SAR-AIRcraft*  & 13,976 & 1.923 & 2.989 & 18,885 & 27,848 & 4,631 & 5,996 & 38,475 & 2.04 \\ ShipDataset  & 31,784 & 3.973 & 3.972 & 39,729 & 40,761 & 5,080 & 5,044 & 50,858 & 1.28 \\ SSDD  & 928 & 116 & 116 & 11,60 & 2,041 & 252 & 294 & 2,587 & 2.23 \\ OGSOD  & 14,664 & 1,834 & 1,833 & 18,331 & 38,975 & 4,844 & 4,770 & 48,589 & 2.65 \\ SIVED  & 837 & 104 & 103 & 1,044 & 9,561 & 1,222 & 1,230 & 12,013 & 11.51 \\ 
**SARDet-100k** & 94,493 & 10,492 & 11,613 & 116,598 & 198,747 & 22,703 & 24,023 & 245,653 & 2.11 \\ 

Table 1: Image and instance level statistics of SARDet-100K dataset. *: Origin datasets are cropped into 512 \(\) 512 patches. Ins: Instances, Img: Images.

on the collected datasets is shown in Table. 2. To ensure consistency across the collected datasets, we invest considerable time and effort in rigorous dataset standardization. This involves addressing variations in train-val-test splitting status, image resolutions, and annotation formats. More details about data collection and standardization can be found in the Appendix.

Table 1 presents the standardized sub-datasets of SARDet-100K along with their corresponding statistics, which includes information on both image-level and instance-level statistics. The SARDet-100K dataset encompasses a total of 116,598 images, and 245,653 instances distributed across six categories: Aircraft, Ship, Car, Bridge, Tank, and Harbor. SARDet-100K dataset stands as the first large-scale SAR object detection dataset, comparable in size to the widely used COCO  dataset (118K images), which is the standard benchmark for general object detection. The scale and diversity of the SARDet-100K dataset effectively simulate real-world scenarios encountered in the application of SAR object detection models across multiple data sources. SARDet-100K provides researchers with robust training and evaluation for advancing SAR object detection algorithms and techniques, fostering the development of SOTA models in this domain.

## 4 Multi-Stage with Filter Augmentation Pretraining Framework

Several recent studies [26; 78; 74; 21] have demonstrated the effectiveness of mature handcrafted features and specialized network module designs in improving SAR object detection performance. However, most of these works rely on the default ImageNet pretraining approach, thus overlooking the significant domain gap between the pretrained nature scenes dataset and the finetuned SAR dataset. Additionally, they fail to address the model gap that exists between the backbone and the entire detection framework. To address these limitations, we propose a novel framework called the Multi-Stage with Filter Augmentation (MSFA) Pretraining Framework. Our framework tackles the challenges from the perspective of data input, domain transition, and model migration. MSFA comprises two core designs: the Filter Augmented Input and the Multi-Stage pretrain strategy.

### Filter Augmented Input

As discussed in the _Related Work_ section, numerous existing handcrafted feature descriptors leverage meticulously designed filters to extract features. These features, robust and rich in information, act as augmented information derived from the original image. Thus, we propose employing such features as auxiliary information alongside the original pixel data. The filter augmented feature \(M\) of data \(x\) can be generally defined as:

Figure 2: Illustration of the significant domain gap exists between Nature RGB dataset and remote sensing SAR datasets. **(a)** showcases the WST feature space significantly narrows the domain gap. **(b)** demonstrates that the remote sensing RGB dataset serves as an effective domain transit bridge, facilitating smoother domain transfer.

\[M_{i}^{x}=T_{i}(x),i\{HOG,Canny,Haar,WST,GRE\}.\] (1)

Where \(T_{i}\) is a pre-defined transformation. Drawing inspiration from the information residual design in ResNet , we construct the Filter Augmented Input to the detection model, \(Inp\), by concatenating the original grayscale SAR image \(x\) with the generated filter augmented feature \(M_{i}^{x}\) as:

\[Inp=(x,M_{i}^{x}).\] (2)

By casting the original data input from the heterogeneous pixel space to a homogeneous filter augmented feature space, the domain gaps between different image domains can be greatly reduced, as illustrated in Fig. 2(a).

#### 4.1.1 Multi-stage pretrain

We formulate the traditional pretrain schema as:

\[B=_{cls}(B_{})(D_{IN}),\] (3)

\[A=_{det}(A_{B})(D_{SAR}).\] (4)

The function \(_{t}(a)(b)\) means training a model \(a\) on a dataset \(b\) with a task \(t\), and it returns a trained model. \(t\) is the training task, \(t\{cls,det\}\) where \(cls\) stands for classification and \(det\) for detection. \(B\) indicates the backbone model and \(A\) is the whole detection model. Traditionally, the pretrain stage will randomly initialize the backbone model \(B_{}\) and train on the dataset ImageNet \(D_{IN}\) (as Eq. (3)). Then finetune on the SAR dataset \(D_{SAR}\) with pretrained backbone initialised from detection model \(A_{B}\) (as Eq. (4)).

Our proposed multi-stage pretrain strategy, alternatively, can be illustrated as in the Eq. (3) (5) (6).

\[A^{}=_{det}(A_{B})(D_{RS}).\] (5)

\[A=_{det}(A_{A^{}})(D_{SAR}).\] (6)

Where an extra second stage pretraining in Eq. (5) is added. We propose the utilization of a large-scale optical remote sensing dataset, \(D_{RS}\), as a detection pretrain for domain transit. The dataset consists of optical modal imagery, which also shares similar object shapes, scales, and categories in downstream SAR datasets. This characteristic serves as a valuable bridge between the optical distribution of natural images in ImageNet and the object distribution in SAR remote sensing images. By leveraging such second stage pretrain, the domain gap is effectively minimized, as illustrated in Fig. 2(b).

Figure 3: Conceptual illustration of traditional ImageNet pretrain and our proposed Multi-Stage with Filter Augmentation (MSFA) pretrain framework.

#### 4.1.2 Msfa

Finally, the proposed MSFA framework integrates Filter Augmented Input with Multi-Stage pretraining, as illustrated in Fig. 3. Our MSFA framework effectively bridges the substantial domain and modal gaps between pretraining on nature images and finetuning on SAR image detection.

By introducing Filter Augmented Input, we leverage mature handcrafted feature descriptors to extract noise-robust features. This also enables us to effectively transform the heterogeneous image domains of both pretraining and finetuning images into a homogeneous feature domain. By unifying the input data into a consistent feature domain, we address the disparities that exist between different types of images. Consequently, it enhances the alignment and transferability of knowledge across domains. Moreover, the incorporation of multi-stage training involves utilizing an additional large-scale optical remote sensing dataset for detection pretraining. This dataset acts as a domain bridge, connecting the domain of ImageNet's nature images with that of SAR remote sensing images. As a result, it further reduces the domain gaps, facilitating a smoother transition between the two domains. Furthermore, the detection pretraining in the second stage of the MSFA framework can also act as a model bridge. It allows for the comprehensive training of the entire detection framework, rather than solely focusing on the backbone, making the whole detection framework well-initialized to perform optimally in the SAR detection finetuning.

## 5 Experiments and Analysis

### Filter Augmented Input

To investigate and assess the impact of the proposed Filter Augmented Input, we conduct experiments on each traditional feature descriptor discussed in the _Related Work_, within the framework of our proposed MSFA method. The findings, detailed in Table 3, indicate that incorporating these handcrafted features notably enhances the performance of the detector. Additionally, our analysis reveals that converting image pixels into handcrafted feature spaces significantly minimizes the distributional gaps between the ImageNet and the SARDet-100K datasets. This is particularly evident in the Pearson Correlation Coefficients (PCC) between the inputs of the ImageNet and SARDet-100K datasets, as illustrated in Table 4. This underscores the efficacy of the proposed approach in bridging the domain gap between natural and SAR images, thereby enhancing the efficiency of knowledge transfer from the pretraining process.

Remarkably, the Wavelet Scattering Transform (WST) feature stands out for its exceptional performance. This superiority can be attributed not just to its role in significantly narrowing the domain gap, but also to its capacity for extracting rich, multi-scale information. Such information acts as a robust auxiliary feature by mitigating noise and preserving object-related details. However, we also find that using multiple filter augmented features will not have further significant performance gain. It is possible that the existing WST already captures the essential information necessary for effective object detection, and incorporating additional ones does not provide substantial additional beneficial information.

Due to the outstanding performance of WST, we employed it as the default Filter Augmented Input in our MSFA method for the remainder of the paper.

### Multi-stage Pretrain

To evaluate the effectiveness of the proposed multi-stage pretraining approach, we conduct experiments in which we keep the input modality consistent and finetuned the detection model using various pretraining strategies on the SARDet-100K dataset. As a baseline, Exp. 1 takes single-channel SAR data as input, pretrains the backbone model on ImageNet for 100 epochs, and then directly finetune the detector on the SARDet-100K dataset (following the widely used default setting). In addition to the baseline, we perform a second stage pretrain specifically for object detection on optical remote sensing datasets, such as DOTA  or DIOR . (Details on DOTA and DIOR datasets can be found in the Appendix). Following the second pretraining stage, we finetune the model either solely on the backbone or on the entire framework.

The results of Exp. 2, 4, 6, and 8 in Table 5 prove the substantial advantages of the two-stage pretraining approach. Notably, even the relatively small-scale DIOR dataset showcases noticeable performance gains compared to the baseline (Exp. 1 and 5). This observation underscores the significance of reducing the domain gap during the pretraining phase of SAR detection.

However, the DIOR dataset pretraining is not as effective as the larger-scale DOTA dataset (Exp. 2 vs 4, 6 vs 8). This comparison underscores the significance of the pretraining scale in achieving optimal results. The DOTA dataset, with its larger scale and similar average instance area to SARDet-100K, provides a more comprehensive and informative pretraining, leading to improved performance during the subsequent finetuning stage.

The comparison between Exp. 3 & 4 and Exp. 7 & 8, demonstrate the superiority of pretraining the entire framework over solely the backbone, highlighting the significant impact of the model gap on the performance of SAR detection.

In summary, our proposed multi-stage pretraining strategy in MSFA alleviates both the data domain gap and the model gap between pretraining and the downstream model, leading to significant enhancements in SAR detection performance. Detailed experimental results and visualization are available in the Appendix.

### Generalizability of MSFA

To assess the effectiveness and generalizability of the proposed MSFA, we conduct experiments using various detectors and backbones, as presented in Fig. 4(a) and 4(b). Significant performance improvement is observed across different frameworks (including single-stage [53; 4; 43], two-stage [33; 30; 59], and end2end [94; 58; 39]) and diverse backbones (including ResNets , ConvNexts , VANs , and Swin-Transformer  networks). It provides strong evidence for the effectiveness and wide applicability of our proposed method. Furthermore, we observe stable performance improvements as we scale up the backbone size, as shown in Fig. 4(b), indicating the good scalability of our proposed method.

Significantly, the design of our MSFA method was developed with flexibility, generalizability and wide applicability in mind. Therefore the method can be seamlessly integrated into most existing models without any modifications.

 ID &  &  &  \\    & & Multi-stage & &  &  \\ 
1 & & ✗ & ImageNet & Backbone & 49.0 \\ 
2 & SAR & ✓ & ImageNet + DIOR & Framework & 49.5 \\
3 & (Raw Pixels) & ✓ & ImageNet + DOTA & Backbone & 49.3 \\
4 & & ✓ & ImageNet + DOTA & Framework & **50.2** \\ 
5 & & ✗ & ImageNet & Backbone & 49.2 \\
6 & SAR+WST & ✓ & ImageNet + DIOR & Framework & 50.1 \\
7 & (Filter Augmented) & ✓ & ImageNet + DOTA & Backbone & 49.6 \\
8 & & ✓ & ImageNet + DOTA & Framework & **51.1** \\ 

Table 5: Comparison of different pretrain strategies using Faster-RCNN and ResNet50 as the detection model.

### Comparison with SOTAs

We compare various SOTA methods, including both general object detection models [43; 53; 14; 64; 4; 87; 72], as well as SAR object detection models [7; 25; 89; 13; 70; 40; 37; 74; 21; 20]. We evaluate their performance on the SSDD and HRSID datasets, which are the commonly used benchmarks for SAR object detection. To leverage the superior efficiency and performance of the VAN  backbone (as shown in Fig. 4(b)), we employ the classic faster R-CNN detection framework with the lightweight VAN-B (Param. 26.6M) backbone as our detection model. The results presented in Table 6 demonstrate that our MSFA method outperforms all the compared methods by a significant margin. Specifically, MSFA achieves a mAP@50 of 97.9% on the SSDD dataset and a mAP@50 of 83.7% on the HRSID dataset, setting new state-of-the-art results. It is noteworthy that our method is the only open-sourced method among the compared SAR detection SOTAs.

  &  &  &  & _{50}\)} \\   & & & & SSDD & HRSID \\   General Detectors \\  } & Grid R-CNN  & ✓ & 2019 & 88.9 & 79.4 \\  & Faster R-CNN  & ✓ & 2015 & 89.7 & 80.7 \\  & Cascade R-CNN  & ✓ & 2019 & 90.5 & 81.3 \\  & Free-Ancher  & ✓ & 2019 & 91.0 & 81.8 \\  & Double-Head R-CNN  & ✓ & 2020 & 91.1 & 82.1 \\  & PANET  & ✓ & 2018 & 91.2 & 81.6 \\  & DCN  & ✓ & 2017 & 92.3 & 82.1 \\   SAR \\ Detectors \\  } & NNAM  & ✗ & 2019 & 79.8 & - \\  & DCMSM  & ✗ & 2018 & 89.6 & - \\  & ARPN  & ✗ & 2020 & 89.9 & 81.8 \\  & DAPN  & ✗ & 2019 & 90.6 & 81.8 \\  & HR-SDNet  & ✗ & 2020 & 90.8 & 82.5 \\  & SER Faster R-CNN  & ✗ & 2018 & 91.5 & 81.5 \\  & FERNet  & ✗ & 2020 & 94.1 & - \\  & NRENet  & ✗ & 2024 & 94.6 & 75.6 \\  & CenterNet++  & ✗ & 2021 & 95.1 & - \\  & CRTransStar  & ✗ & 2022 & 97.0 & - \\  & SARATR-X  & ✗ & 2024 & 97.3 & 80.3 \\   Faster R-CNN + VAN-B \\ **MSFA** (Faster R-CNN + VAN-B) \\  } & ✓ & 2023 & 92.9 & 81.8 \\  &  &  &  &  &  &  \\ 

Table 6: Comparison of the proposed MSFA with previous state-of-the-art methods on SSDD and HRSID datasets.

Figure 4: Generalization of MSFA on different detection frameworks **(a)** and different backbones **(b)**. Models are finetuned and tested on SARDet-100K dataset. INP: Traditional ImageNet Pretrain on backbone network only.

Limitation and Future Work

The scope of this paper is limited to supervised pretraining. However, considering the availability of a vast amount of unannotated SAR images, it would be valuable to explore the potential of semi-supervised, weakly-supervised or unsupervised learning methods for domain transfer in SAR object detection.

While this paper aims to propose a simple, practical, effective, and generic method, it does not delve into the details of specific designs. Future work can be expanded to explore the aforementioned directions in more depth, incorporating intricate and specialized designs to enhance the performance and capabilities of SAR object detection.

## 7 Conclusion

This paper presents a new benchmark for large-scale SAR object detection, introducing the SARDet-100k dataset and the Multi-Stage with Filter Augmentation (MSFA) pretrain method. Our SARDet-100k dataset comprises over 116K images spanning 6 categories, providing a large and diverse dataset for conducting SAR object detection research. To bridge the domain and model gaps between pretraining and finetuning stages in SAR object detection, we propose the MSFA pretraining framework. MSFA significantly enhances the performance of SAR object detection models, setting new state-of-the-art performance in previous benchmark datasets. Moreover, MSFA demonstrates remarkable generalizability and flexibility across various models. Our research endeavours to overcome the current obstacles prevalent in SAR object detection. We anticipate our contributions will pave the way for future research and innovations in this domain.

Our research endeavours to overcome the current obstacles prevalent in SAR object detection. We anticipate our contributions will pave the way for future research and innovations in this domain.

## 8 Acknowledgement

**We extend our deepest gratitude to the following researchers, listed alphabetically by first name: Hong Zhang, Runfan Xia, Shunjun Wei, Tianwen Zhang, Xian Sun, Xiaofang Zhu, Xiaoling Zhang, and other contributing researchers, for allowing us to integrate their datasets. Their contributions have greatly advanced and promoted research in this field.**

This work is supported by the National Science Fund of China (62361166670,62276145, 62176130, 62206134) and the Fundamental Research Funds for the Central Universities (Nankai University: 070-63233084, 070-63243142). Computation is supported by the Supercomputing Center of Nankai University (NKSC).