# Optimal Flow Matching:

Learning Straight Trajectories in Just One Step

 Nikita Kornilov

Skolkovo Institute of Science and Technology

R.Center for AI, Innopolis University

Moscow Institute of Physics and Technology

kornilov.nm@phystech.edu

&Petr Mokrov

Skolkovo Institute of Science and Technology

petr.mokrov@skoltech.ru

&Alexander Gasnikov

Innopolis University

Moscow Institute of Physics and Technology

Steklov Mathematical Institute of RAS

gasnikov@yandex.ru

&Alexander Korotin

Skolkovo Institute of Science and Technology

Artificial Intelligence Research Institute

a.korotin@skoltech.ru

###### Abstract

Over the several recent years, there has been a boom in development of Flow Matching (FM) methods for generative modeling. One intriguing property pursued by the community is the ability to learn flows with straight trajectories which realize the Optimal Transport (OT) displacements. Straightness is crucial for the fast integration (inference) of the learned flow's paths. Unfortunately, most existing flow straightening methods are based on non-trivial iterative FM procedures which accumulate the error during training or exploit heuristics based on minibatch OT. To address these issues, we develop and theoretically justify the novel **Optimal Flow Matching** (OFM) approach which allows recovering the straight OT displacement for the quadratic transport in just one FM step. The main idea of our approach is the employment of vector field for FM which are parameterized by convex functions. The code of our OFM implementation and the conducted experiments is available at https://github.com/Jhomanik/Optimal-Flow-Matching.

## 1 Introduction

Recent success in generative modeling  is mostly driven by Flow Matching (FM)  models. These models move a known distribution to a target one via ordinary differential equations (ODE) describing the mass movement. However, such processes usually have curved trajectories, resulting in time-consuming ODE integration for sampling. To overcome this issue, researches developed several improvements of the FM , which aim to recover more straight paths.

Rectified Flow (RF) method  iteratively solves FM and gradually rectifies trajectories. Unfortunately, in each FM iteration, it **accumulates the error**, see [40, SS2.2] and [39, SS6]. This may spoil the performance of the method. The other popular branch of approaches to straighten trajectories is based on the connection between straight paths and Optimal Transport (OT) . The main goal of OT is to find the way to move one probability distribution to another with the minimal effort. Such OT maps are usually described by ODEs with straight trajectories. In OT Conditional Flow Matching (OT-CFM) , the authors propose to apply FM on top of OT solution between batches from considered distributions. Unfortunately, such a heuristic does not guarantee straight paths because of **minibatch OT biases**, see, e.g., [55, Figure 1, right] for the practical illustration.

**Contributions.** In this paper, we fix the above-mentioned problems of the straightening methods. We propose a novel Optimal Flow Matching (OFM) approach (SS3) that after a **single** FM iteration obtains straight trajectories which can be simulated without ODE solving. It recovers OT flow for the quadratic transport cost function, i.e., it solves the Benamou-Brenier problem (Figure 1). We demonstrate the potential of OFM in the series of experiments and benchmarks (SS4). The main idea of our OFM is to consider during FM only specific vector fields which yield straight paths by design. These vector fields are the gradients of convex functions, which in practice are parametrized by Input Convex Neural Networks . In OFM, one can optionally use minibatch OT or any other transport plan as the input, and this is completely theoretically justified.

## 2 Background and Related Works

In this section, we provide all necessary backgrounds for the theory. First, we recall static (SS2.1) and dynamic (SS2.2) formulations of Optimal Transport and solvers (SS2.3) for them. Then, we recall Flow Matching (SS2.4.1) and flow straightening approaches: OT-CFM (SS2.4.2) and Rectified Flow (SS2.4.3).

**Notations.** For vectors \(x,y^{D}\), we denote the inner product by \( x,y\) and the corresponding \(_{2}\) norm by \(\|x\|:=\). We use \(_{2,ac}(^{D})\) to refer to the set of absolute continuous probability distributions with the finite second moment. For vector \(x^{D}\) and distribution \(p_{2,ac}(^{D})\), notation \(x p\) means that \(x\) is sampled from \(p\). For the push-forward operator, we use symbol \(\#\).

### Static Optimal Transport

**Monge's and Kantorovich's formulations.** Consider two distributions \(p_{0},p_{1}_{2,ac}(^{D})\) and a cost function \(c:^{D}^{D}\). Monge's Optimal Transport formulation is given by

\[_{T\#p_{0}=p_{1}}_{^{D}}c(x_{0},T(x_{0}))p_{0}(x_{0})dx_{0},\] (1)

where the infimum is taken over measurable functions \(T:^{D}^{D}\) which satisfy the mass-preserving constraint \(T\#p_{0}=p_{1}\). Such functions are called transport maps. If there exists a transport map \(T^{*}\) that achieves the infimum, then it is called the optimal transport map.

Since the optimal transport map \(T^{*}\) in Monge's formulation may not exist, there is Kantorovich's relaxation for problem (1) which addresses this issue. Consider the set of transport plans \((p_{0},p_{1})\), i.e., the set of joint distributions on \(^{D}^{D}\) which marginals are equal to \(p_{0}\) and \(p_{1}\), respectively. Kantorovich's Optimal Transport formulation is

\[_{(p_{0},p_{1})}_{^{D}^{D}}c(x_{0}, x_{1})(x_{0},x_{1})dx_{0}dx_{1}.\] (2)

With mild assumptions on \(p_{0},p_{1}\), the infimum is always achieved (possibly not uniquely). An optimal plan \(^{*}(p_{0},p_{1})\) is called an optimal transport plan. If optimal \(^{*}\) has the form \([,T^{*}]\#p_{0}\), then \(T^{*}\) is the solution of Monge's formulation (1).

**Quadratic cost function.** In our paper, we mostly consider the quadratic cost function \(c(x_{0},x_{1})=-x_{1}\|^{2}}{2}\). In this case, infimums in both Monge's and Kantorovich's OT are always uniquely attained [60, Brenier's Theorem \(2.12\)]. They are related by \(^{*}=[,T^{*}]\#p_{0}\). Moreover, the optimal values of (1) and (2) are equal to each other. The square root of the optimal value is called the Wasserstein-2 distance \(_{2}(p_{0},p_{1})\) between distributions \(p_{0}\) and \(p_{1}\), i.e.,

\[_{2}^{2}(p_{0},p_{1}) :=_{(p_{0},p_{1})}_{^{D} ^{D}}-x_{0}\|^{2}}{2}(x_{0},x_{1})dx_{0}dx_{1}\] \[=_{T\#p_{0}=p_{0}}\!\!_{^{D}}\!\!-T(x_{0})\|^{2}}{2}p_{0}(x_{0})dx_{0}.\] (3)

Figure 1: Our Optimal Flow Matching (OFM). For _any_ initial transport plan \(\) between \(p_{0}\) and \(p_{1}\), OFM obtains _exactly straight_ trajectories (in just a _single_ FM loss minimization) which carry out the OT displacement for the quadratic cost function.

Dual formulation.For the quadratic cost, problem (3) has the equivalent dual form :

\[_{2}^{2}(p_{0},p_{1}) = (p_{0},p_{1})-_{\,}^{D}}(x_{0})p_{0}(x_{0})dx_{0}+_{^{D}} (x_{1})p_{1}(x_{1})dx_{1}]}_{=:c_{OT}()},\] (4)

where the minimum is taken over convex functions \((x):^{D}\). Here \((x_{1}):=_{x_{0}^{D}}[ x_{0},x_{1 }-(x_{0})]\) is the convex (Fenchel) conjugate function of \(\). It is also convex.

The term \((p_{0},p_{1})\) does not depend on \(\). Therefore, the minimization (3) over transport plans \(\) is equivalent to the minimization of \(_{OT}()\) from (4) over convex functions \(\). Moreover, the optimal transport map \(T^{*}\) can be expressed via an optimal \(^{*}\) (the _Brenier potential_), namely,

\[T^{*}=^{*}.\] (5)

### Dynamic Optimal Transport

In , the authors show that the calculation of Optimal Transport map in (3) for the quadratic cost can be equivalently reformulated in a dynamic form. This form operates with a vector fields defining time-dependent mass transport instead of just static transport maps.

**Preliminaries.** We consider the fixed time interval \(\). Let \(u(t,) u_{t}():^{D}^ {D}\) be a vector field and \(\{\{z_{t}\}_{t}\}\) be the set of random trajectories such that for each trajectory \(\{z_{t}\}_{t}\) the starting point \(z_{0}\) is sampled from \(p_{0}\) and \(z_{t}\) satisfies the differential equation:

\[dz_{t} = u_{t}(z_{t})dt, z_{0} p_{0}.\] (6)

In other words, the trajectory \(\{z_{t}\}_{t}\) is defined by its initial point \(z_{0} p_{0}\) and goes along the speed vector \(u_{t}(z_{t})\). Under mild assumptions on \(u\), for each initial \(z_{0}\), the trajectory is unique.

Let \(^{u}(t,)^{u}_{t}():^{D} ^{D}\) denote the flow map, i.e., it is the function that maps the initial \(z_{0}\) to its position at moment of time \(t\) according to the ODE (6), i.e.,

\[d^{u}_{t}(z_{0}) = u_{t}(^{u}_{t}(z_{0})),^{u}_{0}(z_{0})=z_{0}.\] (7)

If initial points \(z_{0}\) of trajectories are distributed according to \(p_{0}\), then (6) defines a distribution \(p_{t}\) of \(z_{t}\) at time \(t\), which can be expressed via with the push-forward operator, i.e., \(p^{u}_{t}:=^{u}_{t}\#p_{0}\).

**Benamou-Brenier problem.** Dynamic OT is the following minimization problem:

\[_{2}^{2}(p_{0},p_{1})=_{u} _{0}^{1}_{^{D}}(x_{t})\|_{2}^{2}}{2 }_{t}\#p_{0}(x_{t})}_{:=p^{u}_{t}(x_{t})}dx_{t}dt,\] (8) \[s.t. ^{u}_{1}\#p_{0}=p_{1}.\]

In (8), we look for the vector fields \(u\) that define the flows which start at \(p_{0}\) and end at \(p_{1}\). Among such flows, we seek for the one which has the minimal kinetic energy over the entire time interval.

There is a connection between the static OT map \(T^{*}=^{*}\) and the dynamic OT solution \(u^{*}\). Namely, for every initial point \(z_{0}\), the vector field \(u^{*}\) defines a linear trajectory \(\{z_{t}\}_{t}\):

\[z_{t}=t^{*}(z_{0})+(1-t)z_{0}, t.\] (9)

### Continuous Optimal Transport Solvers

There exist a variety of continuous OT solvers [21; 52; 54; 43; 19; 14; 59; 15; 34; 51; 40; 36; 35; 13; 18; 57; 2, 55; 23; 44; 4, 20]. For a survey of solvers designed for OT with quadratic cost, see . In this paper, we focus only on the most relevant ones, called the ICNN-based solvers [54; 33; 43; 2]. These solvers directly minimize objective \(_{OT}\) from (4) parametrizing a class of convex functions with convex in input neural networks called ICNNs  (for more details, see "Parametrization of \(^{*}\) in SS3.2). Solvers details may differ, but the main idea remains the same. To calculate the conjugate function \((x_{1})\) at the point \(x_{1}\), they solve the convex optimization problem from conjugate definition. Envelope Theorem  allows obtaining closed-form formula for the gradient of the loss.

### Flow Matching Framework

In this section, we recall popular approaches [40; 39; 48] to find fields \(u\) which transport a given probability distribution \(p_{0}\) to a target \(p_{1}\) and their relation to OT.

#### 2.4.1 Flow Matching (FM)

To find such a field, one samples points \(x_{0},x_{1}\) from a transport plan \((p_{0},p_{1})\), e.g., the independent plan \(p_{0} p_{1}\). The vector field \(u\) is encouraged to follow the direction \(x_{1}-x_{0}\) of the linear interpolation \(x_{t}=(1-t)x_{0}+tx_{1}\) at any moment \(t\), i.e., one solves:

\[_{u}_{FM}^{}(u){:=}\!\!_{0}^{1}\{_{ ^{D}^{D}}\|u_{t}(x_{t})-(x_{1}-x_{0})\|^{2}(x_{0},x_{1})dx_{0}dx_{1}\}\!\!dt,x_{t}=(1-t)x_{0}+tx_{1}.\] (10)

We denote the solution of (10) and the flow map (7) by \(u^{}\) and \(^{}\), respectively. The concept of FM is depicted in Figure 2.

The intuition of this procedure is as follows: linear interpolation \(x_{t}=(1-t)x_{0}+tx_{1}\) is an intuitive way to move \(p_{0}\) to \(p_{1}\), but it requires knowing \(x_{1}\). By fitting \(u\) with the direction \(x_{1}-x_{0}\), one yields the vector field that can construct this interpolation without any information about \(x_{1}\).

The set of trajectories \(\{\{z_{t}\}_{t}\}\) generated by \(u_{t}^{}\) (with \(z_{0} p_{0}\)) has a useful property: the flow map \(_{1}^{}\) transforms distribution \(p_{0}\) to distribution \(p_{1}\) for any initial transport plan \(\). Moreover, marginal distribution \(p_{t}=_{t}^{}\#p_{0}\) is equal to the distribution of linear interpolation \(x_{t}=(1-t)x_{0}+tx_{1}\) for any \(t\) and \(x_{0},x_{1}\). This feature is called the marginal preserving property.

To push point \(x_{0}\) according to learned \(u\), one needs to integrate ODE (6) via numerical solvers. The vector fields with straight (or nearly straight) paths incur much smaller time-discretization error and increase effectiveness of computations, which is in high demand for applications.

Researchers noticed that some initial plans \(\) can result in more straight paths after FM rather than the standard independent plan \(p_{0} p_{1}\). The two most popular approaches to choose better plans are Optimal Transport Conditional Flow Matching  and Rectified Flow .

#### 2.4.2 Optimal Transport Conditional Flow Matching (OT-CFM)

If one uses the OT plan \(^{*}\) as the initial plan for FM, then it returns the Brenier's vector field \(u^{*}\), which generates exactly straight trajectories (9). However, typically, the true OT plan \(^{*}\) is not available. In such a case, in order to achieve some level of straightness in the learned trajectories, a natural idea is to take the initial plan \(\) to be close to the optimal \(^{*}\). Inspired by this, the authors of OT-CFM  take the advantage of minibatch OT plan approximation. Firstly, they independently sample batches of points from \(p_{0}\) and \(p_{1}\). Secondly, they join the batches together according to the discrete OT plan between them. The resulting joined batch is then used in FM. The concept of OT-CFM is depicted in Figure 3.

The main drawback of OT-CFM is that it recovers only biased dynamic OT solution. In order to converge to the true transport plan the batch size should be large , while with a growth of batch size computational time increases drastically . In practice, batch sizes that ensure approximation good enough for applications are nearly infeasible to work with.

#### 2.4.3 Rectified Flow (RF)

In , the authors propose an iterative approach to refine the plan \(\), straightening the trajectories more and more with each iteration. Formally, Flow Matching procedure denoted by FM takes the transport plan \(\) as input and returns an optimal flow map via solving (10):

\[^{}:=().\] (11)

Figure 3: OT-CFM uses minibatch OT plan to obtain more straight trajectories.

Figure 2: Flow Matching (FM) obtains a vector field \(u\) moving \(p_{0}\) to \(p_{1}\). FM typically operates with the independent transport plan \(=p_{0} p_{1}\).

One can iteratively apply FM to the initial transport plan (e.g., the independent plan), gradually rectifying it. Namely, Rectified Flow Algorithm on \(K\)-th iteration has the following update rule

\[^{K+1} = (^{K}),^{K+1}=[,^{K+1}]\#p_{0},\] (12)

where \(^{K},^{K}\) denote flow map and transport plan on \(K\)-th iteration, respectively.

With each new FM iteration, the generated trajectories \(\{\{z_{t}\}_{t}\}^{K}\) provably become more and more straight, i.e., error in approximation \(z_{t}^{K}(1-t)z_{0}^{K}+tz_{1}^{K}, t\) decreases as the number of iterations \(K\) grows. The concept of RF is depicted on Figure 4.

The authors also notice that for any convex cost function \(c\) the flow map \(_{1}^{}\) from Flow Matching yields lower or equal transport cost than initial transport plan \(\):

\[_{^{D}}c(x_{0},_{1}^{}(x_{0}))p_{0}(x_{0})dx_{0}_ {^{D}^{D}}c(x_{0},x_{1})(x_{0},x_{1})dx_{0}dx_{1}.\] (13)

Intuitively, the transport costs are guaranteed to decrease because the trajectories of FM as solutions of well-defined ODE do not intersect each other, even if the initial lines connecting \(x_{0}\) and \(x_{1}\) can. With each iteration of RF (12), transport costs for all convex cost functions do not increase, but, for a given cost function, convergence to its own OT plan is not guaranteed. In , the authors address this issue and, for any particular convex cost function \(c\), modify Rectified Flow to converge to OT map for \(c\). In this modification, called \(c\)-Rectified Flow (\(c\)-RF), the authors slightly change the FM training objective and restrict the optimization domain only to potential vector fields \(u_{t}()=( f_{t}())\), where \(f_{t}():^{D}\) is an arbitrary time-dependent scalar valued function and \(\) is the convex conjugate of the cost function \(c\). In case of the quadratic cost function, the training objective remains the same, and the vector field \(u_{t}\) is set as the simple gradient \( f_{t}()\) of the scalar valued function \(f_{t}\).

Unfortunately, in practice, with each iteration (\(c\)-)RF accumulates error caused by inexactness from previous iterations, the issue mentioned in [39, SS6, point 3]. Due to neural approximations, we can not get exact solution of FM (e.g., \(_{1}^{K}\#p_{0} p_{1}\)), and this inexactness only grows with iterations. In addition, training of (\(c\)-)RF becomes non-simulation free after the first iteration, since to calculate the plan \(^{K+1}=[,^{K+1}]\#p_{0}\) it has to integrate ODE.

## 3 Optimal Flow Matching (OFM)

In this section, we provide the design of our novel Optimal Flow Matching algorithm (1) that fixes main problems of Rectified Flow and OT-CFM approaches described above. In theory, it obtains exactly **straight trajectories** and recovers the unbiased optimal transport map for the quadratic cost **just in one FM iteration** with **any** initial transport plan. Moreover, during inference, our OFM does not require solving ODE to transport points.

We discuss the theory behind our approach (SS3.1), its practical implementation aspects (SS3.2) and the relation to prior works (SS3.3). All _our proofs_ are located in Appendix A.

### Theory: Deriving the Optimization Loss

We want to design a method of moving distribution \(p_{0}\) to \(p_{1}\) via exactly straight trajectories. Namely, we aim to obtain straight paths from the solution of the dynamic OT (8). Moreover, we want to limit ourselves to just one minimization iteration. Hence, we propose our novel Optimal Flow Matching (OFM) procedure satisfying the above-mentioned conditions. The main idea of our OFM is to

Figure 4: Rectified Flow iteratively applies FM to straighten the trajectories after each step.

minimize the Flow Matching loss (10) not over all possible vector fields \(u\), but only over specific _optimal_ ones, which yield straight paths by construction and include the desired dynamic OT field \(u^{*}\).

**Optimal vector fields.** We say that a vector field \(u^{}\) is optimal if it generates linear trajectories \(\{\{z_{t}\}_{t}\}\) such that there exist a convex function \(:^{D}\), which for any path \(\{z_{t}\}_{t}\) pushes the initial point \(z_{0}\) to the final one as \(z_{1}=(z_{0})\), i.e.,

\[z_{t} = (1-t)z_{0}+t(z_{0}), t.\]

The function \(\) defines the ODE

\[dz_{t} = ((z_{0})-z_{0})dt, z_{t}|_{t=0}=z_{0}.\] (14)

Equation (14) does not provide a closed formula for \(u^{}\) as it depends on \(z_{0}\). The explicit formula is constructed as follows: for a time \(t\) and point \(x_{t}\), we can find a trajectory \(\{z_{t}\}_{t}\) s.t.

\[x_{t}=z_{t}=(1-t)z_{0}+t(z_{0})\] (15)

and recover the initial point \(z_{0}\). We postpone the solution of this problem to SS3.2. For now, we define the inverse of flow map (7) as \((_{t}^{})^{-1}(x_{t}):=z_{0}\) and the vector field \(u_{t}^{}(x_{t}):=(z_{0})-z_{0}=((_{t}^{})^{-1} (x_{t}))-(_{t}^{})^{-1}(x_{t})\), which generates ODE (14), i.e., \(dz_{t}=u_{t}^{}(z_{t})dt\). The concept of optimal vector fields is depicted on Figure 5.

We highlight that the solution of dynamic OT lies in the class of optimal vector fields, since it generates linear trajectories (9) with the Brenier potential \(^{*}\) (5).

**Training objective.** Our Optimal Flow Matching (OFM) approach is as follows: we restrict the optimization domain of FM (10) with fixed plan \(\) only to the optimal vector fields. We put the formula for the vector field \(u_{}\) into FM loss from (10) and define our Optimal Flow Matching loss:

\[^{}_{OFM}() := ^{}_{FM}(u^{}){=}\!\!_{0}^{1}\!\! _{^{D}^{D}}\!\!\|u_{t}^{}(x_{t})-(x_ {1}-x_{0})\|^{2}(x_{0},x_{1})dx_{0}dx_{1}\!\!\}dt,\] (16) \[x_{t} = (1-t)x_{0}+tx_{1}.\]

Our Theorem 1 states that OFM solves the dynamic OT via single FM minimization for any initial \(\).

**Theorem 1** (OFM and OT connection).: _Consider two distributions \(p_{0},p_{1}_{ac,2}(^{D})\) and **any** transport plan \((p_{0},p_{1})\) between them. Then, the dual Optimal Transport loss \(_{OT}\) (4) and Optimal Flow Matching loss \(^{}_{OFM}\) (16) have **the same minimizers**, i.e.,_

\[_{}^{}_{OFM}()= _{}_{OT}().\]

### Practical implementation aspects

In this subsection, we explain the details of optimization of our Optimal Flow Matching loss (16).

**Parametrization of \(\).** In practice, we parametrize the class of convex functions with Input Convex Neural Networks (ICNNs) \(_{}\) and parameters \(\). These are scalar-valued neural networks built in such a way that the network is convex in its input. They consist of fully-connected or convolution blocks, some weights of which are set to be non-negative in order to keep the convexity. In addition, activation functions are considered to be only non-decreasing and convex in each input coordinate. These networks are able to support most of the popular training techniques (e.g., gradient descent optimization, dropout, skip connection, etc.). In Appendix B, we discuss the used architectures.

**OFM loss calculation.** We provide an explicit formula for gradient of OFM loss (16).

**Proposition 1** (Explicit Loss Gradient Formula).: _The gradient of \(^{}_{OFM}\) can be calculated as_

\[z_{0} = \{(_{t}^{_{}})^{-1}(x_{t})\},\] \[^{}_{OFM}}{d}:= _{t;x_{0},x_{1}}\!\!\{2( t^{2}_{}(z_{0})+(1-t)I)^{-1}-z_{0})}{t}\}, _{}(z_{0}),\]

_where variables under \(\) remain constants during differentiation._

Figure 5: An Optimal Vector Field: a vector field \(u^{}\) with straight paths is parametrized by a gradient of a convex function \(\).

**Flow map inversion.** In order to find the initial point \(z_{0}=(_{t}^{})^{-1}(x_{t})\), we note that (15)

\[x_{t}=(1-t)z_{0}+t(z_{0})\]

is equivalent to

\[(\|\|^{2}+t()- x_{t}, )(z_{0})=0.\]

The function under gradient operator \(\) has minimum at the required point \(z_{0}\), since at \(z_{0}\) the gradient of it equals \(0\). If \(t<1\) the function is at least \((1-t)\)-strongly convex, and the minimum is unique. The case \(t=1\) is negligible in practice, since it has zero probability to appear during training.

We can reduce the problem of inversion to the following minimization subproblem

\[(_{t}^{})^{-1}(x_{t})=_{z_{0}^{D}} [\|z_{0}\|^{2}+t(z_{0})- x_{t},z_{0} ].\] (17)

Optimization subproblem (17) is at least \((1-t)\)**-strongly convex** and can be effectively solved for any given point \(x_{t}\) (in comparison with typical non-convex optimization tasks).

**Algorithm.** The Optimal Flow Matching pseudocode is presented in listing 1. We estimate math expectation over plan \(\) and time \(t\) with uniform distribution on \(\) via unbiased Monte Carlo.

```
0: Initial transport plan \((p_{0},p_{1})\), number of iterations \(K\), batch size \(B\), optimizer \(Opt\), sub-problem optimizer \(SubOpt\), ICNN \(_{}\)
1:for\(k=0,,K-1\)do
2: Sample batch \(\{(x_{0}^{i},x_{1}^{i})\}_{i=1}^{B}\) of size \(B\) from plan \(\);
3: Sample times batch \(\{t^{i}\}_{i=1}^{B}\) of size \(B\) from \(U\);
4: Calculate linear interpolation \(x_{t^{i}}^{i}=(1-t^{i})x_{0}^{i}+t^{i}x_{1}^{i}\) for all \(i\);
5: Find the initial points \(z_{0}^{i}\) via solving the convex problem with \(SubOpt\): \[z_{0}^{i}=\{_{z_{0}^{i}}[)}{2}\| z_{0}^{i}\|^{2}+t^{i}_{}(z_{0}^{i})- x_{t^{i}}^{i},z_{0}^{i} ]\};\]
6: Calculate loss \(}_{OFM}\) \[}_{OFM}=_{i=1}^{B} \{2(t^{i}^{2}_{}(z_{0}^{i})+(1-t^{i})I)^{-1} ^{i}-z_{0}^{i})}{t^{i}}\},_{}(z_{0}^{i}) ;\]
7: Update parameters \(\) via optimizer \(Opt\) step with \(}_{OFM}}{d}\);
8:endfor ```

**Algorithm 1** Optimal Flow Matching

### Relation to Prior Works

In this subsection, we compare our Optimal Flow Matching and previous straighttening approaches. One unique feature of OFM is that it works only with flows which have straight paths by design and does not require ODE integration to transport points. Other methods may result in non-straight paths during training, and they still have to solve ODE even with near-straight paths.

**OT Solvers**. According to Theorem 1, our OFM and dual OT solvers basically minimize the same OT loss (4). However, our OFM actively utilizes the temporal component of the dynamic process. It allows us to pave a novel theoretical bridge between OT and FM. Such a direct connection can lead to the adoption of the strengths of both methods and a deeper understanding of them.

**OT-CFM**. Unlike our OFM approach, OT-CFM method retrieves biased OT solution, and the recovery of straight paths is not guaranteed. In OT-CFM, minibatch OT plan appears as a heuristic that helps to get better trajectories in practice. In contrast, usage of **any** initial transport plan \(\) in our OFM is completely justified in Theorem 1.

**Rectified Flow**. In Rectified Flows , the authors iteratively apply Flow Matching to refine the obtained trajectories. However, in each iteration, RF accumulates error since one may notlearn the exact flow due to neural approximations. In addition, RF does not guarantee convergence to the OT plan for the quadratic cost. The \(c\)-Rectified Flow  modification can converge to the OT plan for any cost function \(c\), but still remains iterative. In addition, RF and \(c\)-RF both requires ODE simulation after the first iteration to continue training. In OFM, we work only with the quadratic cost function, but retrieve its OT solution in **just one FM iteration** without simulation of the trajectories.

**Light and Optimal Schrodinger Bridge**. In , the authors observe the relation between Entropic Optimal Transport (EOT) [42; 12] and Bridge Matching (BM)  problems. These are stochastic analogs of OT and FM, respectively. In EOT and BM, instead of deterministic ODE and flows, one considers stochastic processes with non-zero stochasticity. The authors prove that, during BM, one can restrict considered processes only to the specific ones and retrieve the solution of EOT. Hypothetically, our OT/FM case is a limit of their EOT/BM case when the stochasticity tends to zero. Proofs in  for EOT are based on sophisticated KL divergence properties. We do not know whether our results for OFM can be derived by taking the limit of their stochastic case. To derive the properties of our OFM, we use **completely different proof techniques** based on computing integrals over curves rather than KL-based techniques. Besides, in practice, the authors of  mostly focus on Gaussian mixture parametrization while our method allows using neural networks (ICNNs).

### Theory: properties of OFM

In this subsection, we provide the OFM's theoretical properties, which give an intuition for understanding of its main working principles and behavior.

**Proposition 2** (Simplified OFM Loss).: _We can simplify (16) to a more suitable form:_

\[^{}_{}()\!\!=\!\!_{0}^{1} \{_{^{D}^{D}}||^{})^{-1}(x_{t})-x_{0}}{t}||^{2}(x_{0},x_{1})dx_{0} dx_{1}\}dt,x_{t}=(1\!-\!t)x_{0}\!+\!tx_{1}.\] (18)

The simplified form (18) shows that OFM loss actually measures how well \(\) restores initial points \(x_{0}\) of linear interpolations depending on future point \(x_{t}\) and time \(t\).

**Generative properties of OFM.** In this paragraph, we provide another view on our OFM approach. In our OFM, we aim to construct a vector field \(u\) which is as close to the dynamic OT field \(u^{*}\) as possible. We can use the least square regression to measure the distance between them:

\[(u,u^{*}):=_{0}^{1}_{^{D}}\|u_{t}(x_{t})-u^{*}_ {t}(x_{t})\|^{2}^{*}\#p_{0}(x_{t})}_{:=p^{*}_{t}(x_{t})}dx _{t}dt.\] (19)

**Proposition 3** (Intractable Distance).: _The distance \((u,u^{*})\) between an arbitrary vector field \(u\) and OT field \(u^{*}\) equals to the FM loss from (10) with the optimal plan \(^{*}\), i.e.,_

\[(u,u^{*})=^{^{*}}_{FM}(u)-^ {^{*}}_{FM}(u^{*})}_{:=0}.\]

We can not minimize intractable \((u,u^{*})\) since the optimal plan \(^{*}\) is unknown. In OT-CFM , authors heuristically approximate \(^{*}\) in \(^{^{*}}_{FM}(u)\), but obtain biased solution. Surprisingly, for the _optimal_ vector fields, the distance can be calculated explicitly via **any** known plan \(\).

**Proposition 4** (Tractable Distance For OFM).: _The distance \((u^{},u^{^{*}})\) between an **optimal** vector field \(u^{}\) generated by a convex function \(\) and the vector field \(u^{^{*}}\) with the Brenier potential \(^{*}\) can be evaluated directly via OFM loss (16) and **any** plan \(\):_

\[(u^{},u^{^{*}})=^{}_{FM}(u^{})- ^{}_{FM}(u^{^{*}})=^{}_{OFM}()- ^{}_{OFM}(^{*}).\] (20)

In (20), the first term is our tractable OFM loss, and the second term does not depend on \(\). Hence, during the whole minimization process in our OFM, we gradually lower the distance (19) between the current vector field and the dynamic OT field up to the complete match.

## 4 Experimental Illustrations

In this section, we showcase the performance of our Optimal Flow Matching method on illustrative 2D scenario (SS4.1) and Wasserstein-2 benchmark  (SS4.2). Finally, we apply our approach for solving high-dimensional unpaired image-to-image translation in the latent space of pretrained ALAE autoencoder (SS4.3). The PyTorch implementation of our method is publicly available at 

[MISSING_PAGE_FAIL:9]

**Results.** Among FM-based methods, our OFM with any plan demonstrates the best results. For all plans, OFM convergences to close final solutions and metrics. Minibatch plan provides a little bit better results, especially in high dimensions. In theory, the OFM results for any plan \(\) must be similar. However, in stochastic optimization, plans with large variance yield convergence to slightly worse solutions.

MLP-based OT solver usually beats our OFM, since MLPs do not have ICNNs' limitations in practice. However, usage of MLP is an empirical trick and is not completely justified. We also run OFM with MLP instead ICNN, and, unfortunately, the method fails to converge.

RF demonstrates worse performance than even linear baseline, but it is ok since it is not designed to solve \(_{2}\) OT. In turn, \(c\)-RF works better, but rapidly deteriorates with increasing dimensions. OT-CFM demonstrates the best results among baseline FM-based methods, but still underperforms compared to our OFM solver in high dimensions.

### Unpaired Image-to-image Transfer

Another task that involves learning a translation between two distributions is unpaired image-to-image translation . We follow the setup of  where translation is computed in the \(512\) dimensional latent space of the pre-trained ALAE autoencoder  on \(1024 1024\) FFHQ dataset . In particular, we split the train FFHQ sample (60K faces) into \(children\) and \(adults\) subsets and consider the corresponding ALAE latent codes as the source and target distributions \(p_{0}\) and \(p_{1}\). At the inference stage, we take a new (unseen) \(adult\) face from a test FFHQ sample, extract its latent code, process with our learned model and then decode back to the image space. The qualitative translation results and FID metric  are presented in Figure 7 and Table 2, respectively.

The batch size for minibatch OT methods (\(,,\)) is \(B_{}=128\). Our OFM converges to nearly the same solution for both independent and MB plans, and demonstrates qualitatively plausible translations. The most similar results to our method are demonstrated by \( c\). Similar to OFM, this method (in the limit of RF steps) also recovers the quadratic OT mapping.

## 5 Discussion

**Potential impact.** We believe that our novel theoretical results have a huge potential for improving modern flow matching-based methods and inspiring the community for further studies. We think this is of high importance especially taking into account that modern generative models start to extensively use flow matching methods [61; 41; 17].

**Limitations and broader impact** are discussed in Appendix C.

## 6 Acknowledgement

The work of N. Kornilov has been financially supported by The Analytical Center for the Government of the Russian Federation (Agreement No. 70-2021-00143 01.11.2021, IGK 000000D730324P540002).

   &  OFM, Ind \\ **(Ours)** \\  } &  OFM, MB \\ **(Ours)** \\  } &  &  &  \\    & \(11.8\) & & & & \\  

Table 2: FID metric on Adult\(\)Child translation task for the Flow Matching based methods.