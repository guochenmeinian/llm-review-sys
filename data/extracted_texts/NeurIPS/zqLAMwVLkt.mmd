# Generative Semi-supervised Graph Anomaly Detection

Hezhe Qiao\({}^{1}\), Qingsong Wen\({}^{2}\), Xiaoli Li\({}^{3,4}\), Ee-Peng Lim\({}^{1}\), Guansong Pang\({}^{1}\)

\({}^{1}\)School of Computing and Information Systems, Singapore Management University

\({}^{2}\)Squirrel AI

\({}^{3}\) Institute for Infocomm Research, A*STAR, Singapore

\({}^{4}\) A*STAR Centre for Frontier AI Research, Singapore

hezheqiao.2022@phdcs.smu.edu.sg, qingsongedu@gmail.com

xlli@i2r.a-star.edu.sg, eplim@smu.edu.sg, gspang@smu.edu.sg

Corresponding author: G. Pang

###### Abstract

This work considers a practical semi-supervised graph anomaly detection (GAD) scenario, where part of the nodes in a graph are known to be normal, contrasting to the extensively explored unsupervised setting with a fully unlabeled graph. We reveal that having access to the normal nodes, even just a small percentage of normal nodes, helps enhance the detection performance of existing unsupervised GAD methods when they are adapted to the semi-supervised setting. However, their utilization of these normal nodes is limited. In this paper we propose a novel Generative GAD approach (namely **GGAD**) for the semi-supervised scenario to better exploit the normal nodes. The key idea is to generate pseudo anomaly nodes, referred to as _outlier nodes_, for providing effective negative node samples in training a discriminative one-class classifier. The main challenge here lies in the lack of ground truth information about real anomaly nodes. To address this challenge, GGAD is designed to leverage two important priors about the anomaly nodes - _asymmetric local affinity_ and _egocentric closeness_ - to generate reliable outlier nodes that assimilate anomaly nodes in both graph structure and feature representations. Comprehensive experiments on six real-world GAD datasets are performed to establish a benchmark for semi-supervised GAD and show that GGAD substantially outperforms state-of-the-art unsupervised and semi-supervised GAD methods with varying numbers of training normal nodes. Code is available at https://github.com/mala-lab/GGAD.

## 1 Introduction

Graph anomaly detection (GAD) has received significant attention due to its broad application domains, _e.g._, cyber security and fraud detection . However, it is challenging to recognize anomaly nodes in a graph due to its complex graph structure and attributes . Moreover, most traditional anomaly detection methods  are designed for Euclidean data, which are shown to be ineffective on non-Euclidean data like graph data . To address this challenge, as an effective way of modeling graph data, graph neural networks (GNN) have been widely used for deep GAD . These GNN methods typically assume that the labels of all nodes are unknown and perform anomaly detection in a fully unsupervised way by, _e.g._, data reconstruction , self-supervised learning , or one-class homophily modeling . Although these methods achieve remarkable advances, they are not favored in real-world applications where the labels for normal nodes are easy to obtain due to their overwhelming presence in a graph. This is because their capability to utilize those labeled normal nodes is very limited due to theirinherent unsupervised nature. There have been some GAD methods  designed in a semi-supervised setting, but their training relies on the availability of both labeled normal and anomaly nodes, which requires a costly annotation of a large set of anomaly nodes. This largely restricts the practical application of these methods.

Different from the aforementioned two GAD settings, this paper instead considers a practical yet under-explored semi-supervised GAD scenario, where part of the nodes in the graph are known to be normal. Such a one-class classification setting has been widely explored in anomaly detection on other data types, such as visual data , time series , and tabular data , but it is rarely explored in anomaly detection on graph data. Recently there have been a few relevant studies in this line , but they are on graph-level anomaly detection, _i.e._, detecting abnormal graphs from a set of graphs, while we explore the semi-supervised setting for abnormal node detection. We establish an evaluation benchmark for this problem and show that having access to these normal nodes helps enhance the detection performance of existing unsupervised GAD methods when they are properly adapted to the semi-supervised setting (see Table 1). However, due to their original unsupervised designs, they cannot make full use of these labeled normal nodes.

To better exploit those normal nodes, we propose a novel generative GAD approach, namely **GGAD**, aiming at generating pseudo anomaly nodes, referred to as **outlier nodes**, for providing effective negative node samples in training a discriminative one-class classifier on the given normal nodes. The key challenge in this type of generative approach is the absence of ground-truth information about real anomaly nodes. There have been many generative anomaly detection approaches that learn adversarial outliers to provide some weak supervision of abnormality , but they are designed for non-graph data and fail to take account of the graph structure information in the outlier generation. Some recent methods, such as AEGIS , attempt to adapt this approach for GAD, but the outliers are generated based on simply adding Gaussian noises to the GNN-based node representations, ignoring the structural relations between the outliers and the graph nodes. Consequently, the distribution of the generated outliers is often mismatched to that of the real anomaly nodes, as illustrated in Fig. 1a, and demonstrates very different local structure (Fig. 1c).

Our approach GGAD tackles this issue with a method to generate outlier nodes that assimilate the anomaly nodes in both local structure and feature representation. It is motivated by two important priors about the anomaly nodes. The first one is an **asymmetric local affinity** phenomenon revealed in recent studies , _i.e._, _the affinity between normal nodes is typically significantly stronger than that between normal and abnormal nodes_. Inspired by this, GGAD generates outlier nodes in a way to enforce that they have a smaller local affinity to their local neighbors than the normal nodes. This objective aligns the distribution of the outlier nodes to that of the anomaly nodes in terms of graph structure. The second prior knowledge is that many anomaly nodes exhibit high similarity to the normal nodes in the feature space due to its subtle abnormality  or adversarial camouflage . We encapsulate this prior knowledge as **egocentric closeness**, mandating that _the feature representation of the outlier nodes should be closed to the normal nodes that share similar local structure as the outlier nodes_. GGAD incorporates these two priors through two loss functions to generate outlier nodes that are well aligned to the distribution of the anomaly nodes in both local structure affinity (see Fig. 1d) and feature representation (see Fig. 1b). We can then train a

Figure 1: **Left:** An exemplar graph with the edge width indicates the level of affinity connecting two nodes, in which normal nodes (_e.g._, \(v_{n_{i}}\) and \(v_{n_{j}}\)) have stronger affinity to its neighboring normal nodes than anomaly nodes (_e.g._, \(v_{a_{i}}\) and \(v_{a_{j}}\)) due to homophily relation within the normal class. Our approach GGAD aims to generate outliers (_e.g._, \(v_{o_{i}}\) and \(v_{o_{j}}\)) that can well assimilate the anomaly nodes. **Right:** The outliers generated by methods like AEGIS  that ignore their structural relation often mismatch the distribution of abnormal nodes (**a**), due to their false local affinity (**c**). By contrast, GGAD incorporates two important priors about anomaly nodes to generate outliers so that they well assimilate the (**b**) feature representation and (**d**) local structure of abnormal nodes.

discriminative one-class classifier on the labeled normal nodes, with these generated outlier nodes treated as the negative samples.

Accordingly, our main contributions can be summarized as follows:

* We explore a practical yet under-explored semi-supervised GAD problem where part of the normal nodes are known, and establish an evaluation benchmark for the problem.
* We propose a novel generative GAD approach, GGAD, for the studied setting. To the best of our knowledge, it is the first work aiming for generating outlier nodes that are of similar local structure and node representations to the real anomaly nodes. The outlier nodes serve as effective negative samples for training a discriminative one-class classifier.
* asymmetric local affinity and egocentric closeness
- and leverage them to introduce an innovative outlier node generation method. Although these priors may not be exhaustive, they provide principled guidelines for generating learnable outlier nodes that can well assimilate the real anomaly nodes in both graph structure and feature representation across diverse real-world GAD datasets.
* Extensive experiments on six large GAD datasets demonstrate that our approach GGAD substantially outperforms 12 state-of-the-art unsupervised and semi-supervised GAD methods with varying numbers of training normal nodes, achieving over 15% increase in AUROC/AUPRC compared to the best contenders on the challenging datasets.

## 2 Related Work

### Graph Anomaly Detection

Numerous graph anomaly detection methods, including shallow and deep approaches, have been proposed. Shallow methods like Radar , AMEN , and ANOMALOUS  are often bottlenecked due to the lack of representation power to capture the complex semantics of graphs. With the development of GNN in node representation learning, many deep GAD methods show better performance than shallow approaches. Here we focus on the discussion of the deep GAD methods in two relevant settings: unsupervised and semi-supervised GAD.

**Unsupervised Approach.** Existing unsupervised GAD methods are typically built using a conventional anomaly detection objective, such as data reconstruction. The basic idea is to capture the normal activity patterns and detect anomalies that behave significantly differently. As one of the most popular methods, reconstruction-based methods using graph auto-encoder (GAE) have been widely applied for GAD . DOMINANT is the first work that applies GAE on the graph to reconstruct the attribute and structure leveraging GNNs . Fan _et al._ propose AnomalyDAE to further improve the performance by enhancing the importance of the reconstruction on the graph structure. In addition to reconstruction, some methods focus on exploring the relationship in the graph, _e.g._, the relation between nodes and subgraphs, to train GAD models. Among these methods, Qiao _et al._ propose TAM , which maximizes the local node affinity on truncated graphs, achieving good performance on the synthetic dataset and datasets with real anomalies. Although the aforementioned unsupervised methods achieve good performance and help us identify anomalies without any access to class labels, they cannot effectively leverage the labeled nodes when such information is available.

**Semi-Supervised Approach.** The one-class classification under semi-supervised setting has been widely explored in anomaly detection on visual data, but rarely done on the graph data, except  that recently explored this setting for graph-level anomaly detection. To detect abnormal graphs, these methods address a very different problem from ours, which is focused on capturing the normality of a set of given normal graphs at the graph level. By contrast, we focus on modeling the normality at the node level. Some semi-supervised methods have been recently proposed for node-level anomaly detection, but they assume the availability of the labels of both normal and anomaly nodes . By contrast, our setting eases this requirement and requires the labeled normal nodes only.

### Generative Anomaly Detection

Generative adversarial networks (GANs) provide an effective solution to generate synthetic samples that capture the normal/abnormal patterns . One type of these methods aims to learn latent features that can capture the normality of a generative network . Methods like ALAD , Fence GAN  and OCAN  are early methods in this line, aiming at making the generated samples lie at the boundary of normal data for more accurate anomaly detection. Motivated by these methods, a similar approach has also been explored in graph data, like AEGIS  and GAAN  which aim to simulate some abnormal features in the representation space using GNN, but they are focused on adding Gaussian noise to the representations of normal nodes without considering graph structure information. They are often able to generate pseudo anomaly node representations that are separable from the normal nodes for training their detection model, but the pseudo anomaly nodes are mismatched with the distribution of the real anomaly nodes.

## 3 Methodology

### Problem Statement

**Semi-supervised GAD**. We focus on the semi-supervised anomaly detection on the attributed graph given some labeled normal nodes. An attributed graph can be denoted by \(=(,,)\), where \(=\{v_{1},,v_{N}\}\) denotes the node set, \(\) with \(e\) is the edge set in the graph. \(e_{ij}=1\) represents there is a connection between node \(v_{i}\) and \(v_{j}\), and \(e_{ij}=0\) otherwise. The node attributes are denoted as \(^{N F}\) and \(\{0,1\}^{N N}\) is the adjacency matrix of \(\). \(_{i}^{F}\) is the attribute vector of \(v_{i}\) and \(_{ij}=1\) if and only if \((v_{i},v_{j})\). \(_{n}\) and \(_{a}\) represent the normal node set and abnormal node set, respectively. Typically the number of normal nodes is significantly greater than the abnormal nodes, _i.e._, \(|_{n}||_{a}|\). The goal of semi-supervised GAD is to learn an anomaly scoring function \(f:\), such that \(f(v)<f(v^{})\), \( v_{n},v^{}_{a}\) given a set of labeled normal nodes \(_{l}_{n}\) and no access to labels of anomaly nodes. All other unlabeled nodes, denoted by \(_{u}=_{l}\), comprise the test data set.

**Outlier Node Generation**. Outlier generation aims to generate outlier nodes that deviate from the normal nodes and/or assimilate the anomaly nodes. Such nodes can be generated in either the raw feature space or the embedding feature space. This work is focused on the latter case, as it offers a more flexible way to represent relations between nodes. Our goal is to generate a set of outlier nodes from \(\), denoted by \(_{o}\), in the feature representation space, so that the outlier nodes are well aligned to the anomaly nodes, given no access to the ground-truth anomaly nodes.

**Graph Neural Network for Node Representation Learning**. GNN has been widely used to generate the node representations due to its powerful representation ability in capturing the rich graph attribute and structure information. The projection of node representation using a GNN layer can be generally formalized as

\[^{()}=(,^{(-1)}; ^{()}),\] (1)

where \(^{()}^{N h^{(l)}},^{(-1)} ^{N h^{(l-1)}}\) are the representations of all \(N\) nodes in the \(()\)-th layer and \((-1)\)-th layer, respectively, \(h^{(l)}\) is the dimensionality size, \(^{()}\) are the learnable parameters, and \(^{(0)}\) is set to \(\). \(^{()}=\{_{1},_{2},,_{N}\}\) is a set of representations of \(N\) nodes in the last GNN layer, with \(^{d}\). In this paper, we adopt a 2-layer GCN to model the graph.

### Overview of the Proposed GGAD Approach

The key insight of GGAD is to generate learnable outlier nodes in the feature representation space that assimilate anomaly nodes in terms of both local structure affinity and feature representation. To this end, we introduce two new loss functions that incorporate two important priors about anomaly nodes - asymmetric local affinity and egocentric closeness - to optimize the outlier nodes. As shown in Fig. 2a, the outlier nodes are first initialized based on the representations of the neighbors of the labeled normal nodes, followed by the use of the two priors on the anomaly nodes. GGAD implements the asymmetric local affinity prior in Fig. 2b that enforces a larger local affinity of the normal nodes than that of the anomaly nodes. GGAD then models the egocentric closeness in Fig. 2c that pulls the feature representations of the outlier nodes to the normal nodes that share the same ego network. These two priors are implemented through two complementary loss functions in GGAD. Minimizing these loss functions optimizes the outlier nodes to meet both anomaly priors. The resulting outlier nodes are lastly treated as negative samples to train a discriminative one-class classifier on the labeled normal nodes, as shown in Fig. 2d. Below we introduce GGAD in detail.

### Incorporating the Asymmetric Local Affinity Prior

**Outlier Node Initialization.** Recall that GGAD is focused on generating learnable outlier nodes in the representation space. To enable the subsequent learning of the outlier nodes, we need to produce good representation initialization of the outlier nodes. To this end, we use a neighborhood-aware outlier initialization module that generates the initial outlier nodes' representation based on the representations of the local neighbors of normal nodes. The representations from these neighbor nodes provide an important reference for being normal in a local graph structure. This helps ground the generation of outlier nodes to a real graph structure. More specifically, as shown in Fig. 2a, given a labeled normal node \(v_{i}_{l}\) and its ego network \((v_{i})\) that contains all nodes directly connected with \(v_{i}\), we initialize an outlier node in the representation space by:

\[}_{i}=(v_{i},(v_{i});_{g})= (v_{i})|}_{v_{j}(v_{ i})}(}_{j}),\] (2)

where \(\) is a mapping function determined by parameters \(_{g}\) that contain the learnable parameters \(}^{d d}\) in this module in addition to the parameters \(^{()}\) in Eq. (1), and \(()\) is an activation function. It is not required to perform Eq. (2) for all training normal nodes. We sample a set of \(S\) normal nodes from \(_{l}\) and respectively generate an outlier node for each of them based on its ego network. \(}_{i}\) in Eq. (2) serves as an initial representation of the outlier node, upon which two optimization constraints based on our anomaly node priors are devised to optimize the representations of the outlier nodes, as elaborated in the following.

**Enforcing the Structural Affinity Prior.** To incorporate the graph structure prior of anomaly nodes into our outlier node generation, GGAD introduces a local affinity-based loss to enforce the fact that the affinity of the outlier nodes to their local neighbors should be smaller than that of the normal nodes. More specifically, the local node affinity of \(v_{i}\), denoted as \((v_{i})\), is defined as the similarity to its neighboring nodes:

\[(v_{i})=(v_{i})|} _{v_{j}(v_{i})}(_{ i},_{j}),\] (3)

The asymmetric local affinity loss is then defined by a margin loss function based on the affinity of the normal nodes and the generated outlier nodes as follows:

\[_{ala}=\{0,-((_{l})- (_{o}))\},\] (4)

where \((_{o})=_{o}|} _{v_{i}_{o}}(v_{i})\) and \((_{l})=_{l}|} _{v_{i}_{l}}(v_{i})\) represent the average local affinity of the outliers and normal nodes respectively, and \(>0\) is a hyperparameter controlling the margin between the affinities of these two types of nodes. Eq. (4) enforces this prior at the node set level rather than at each individual outlier node, as the latter case would be highly computationally costly when \(_{l}\) or \(_{o}\) is large.

Figure 2: Overview of GGAD. (**a**) It first initializes the outlier nodes based on the feature representations of the ego network of a labeled normal node. We then incorporate the two anomaly node priors (**b-c**) to optimize the outlier nodes so that they are well aligned to the anomalies. (**d**) The resulting generated outlier nodes are treated as negative samples to train a discriminative one-class classifier.

### Incorporating the Egocentric Closeness Prior

The outliers generated by solely using this local affinity prior may distribute far away from the normal nodes in the representation space, as shown in Fig. 3a. For those trivial outliers, although they achieve similar local affinity to the abnormal nodes, as shown in Fig. 3d, they are still not aligned well with the distribution of the anomaly nodes, and thus, they cannot serve as effective negative samples for learning the one-class classifier on the normal nodes. Thus, we further introduce an egocentric closeness prior-based loss function to tackle this issue, which models subtle abnormality on anomaly nodes, _i.e._, the anomaly nodes that exhibit high similarity to the normal nodes. More specifically, let \(_{i}\) and \(}_{i}\) be the representations of the normal node \(v_{i}\) and its corresponding generated outlier node that shares the same ego network as \(v_{i}\) (as discussed in Sec. 3.2), the egocentric closeness prior-based loss \(_{ec}\) is defined as follows:

\[_{ec}=_{o}|}_{v_{i}_{o}}\| {}_{i}-(_{i}+)\|_{2}^{2},\] (5)

where \(|_{o}|\) is the number of the generated outliers and \(\) is a noise perturbation generated from a Gaussian distribution. The perturbation is added to guarantee a separability between \(_{i}\) and \(}_{i}\), while enforcing its egocentric closeness. It is worth mentioning that Gaussian noise works like a hyperparameter in the feature interpolation to diversify the outlier nodes in the feature representation space. Changes of this noise distribution do not affect the superiority of the performance of GGAD over the competing methods.

As shown in Fig. 3c, using this egocentric closeness prior-based loss together with the local affinity prior-based loss learns outlier nodes that are well aligned to the real anomaly nodes in both the representation space and the local structure, as illustrated in Figs. 3c and 3f, respectively. Using the egocentric closeness alone also results in mismatches between the generated outlier nodes and the abnormal nodes (see Fig. 3e) since it ignores the local structure relation of the generated outlier nodes.

### Graph Anomaly Detection using GGAD

**Training.** Since the generated outlier nodes are to assimilate the abnormal nodes, they can be used as important negative samples to train a one-class classifier on the labeled normal nodes. We implement this classifier using a fully connected layer on top of the GCN layers that maps the node representations to a prediction probability-based anomaly score, denoted by \(:\), followed by a binary cross-entropy (BCE) loss function \(_{bce}\):

\[_{bce}=_{i}^{|_{o}|+|_{l}|}y_{i}(p_{i})+(1-y _{i})(1-p_{i}),\] (6)

where \(p_{i}=(_{i};_{s})\) is the output of the one-class classifier indicating the probability that a node is a normal node, and \(y\) is the label of node. We set \(y=1\) if the node is a labeled normal node, and \(y=0\) if the node is a generated outlier node. The one-class classifier is jointly optimized with the local affinity prior-based loss \(_{ala}\) and egocentric closeness prior-based loss \(_{ec}\) in an end-to-end manner. This results in mediation in the feature representation space where the generated outlier nodes are close to, yet separable from, the labeled normal nodes and their neighbors. Thus, these outlier nodes can be thought as hard anomalies that lie at the fringe of normal nodes in the feature representation space. The optimization of these two prior losses is continuously decreasing and converging finally during the training (see App. E), indicating that these two losses are collaborative rather than diverged. Thus, the overall loss \(_{total}\) can be formulated as:

\[_{total}=_{bce}+_{ala}+_{ec},\] (7)

Figure 3: (**a-c**) t-SNE visualization of the node representations and (**d-f**) histograms of local affinity yielded by GGAD and its two variants on a GAD dataset T-Finance .

where \(\) and \(\) are the hyperparameters to control the importance of the two constraints respectively. The learnable parameters are \(=\{_{g},_{s}\}\).

**Inference.** During inference, we can directly use the inverse of the prediction of the one-class classifier as the anomaly score:

\[(v_{j})=1-(_{j};^{*}),\] (8)

where \(^{*}\) is the learned parameters of GGAD. Since our outlier nodes well assimilate the real abnormal nodes, they are expected to receive high anomaly scores from the one-class classifier.

## 4 Experiments

**Datasets.** We conduct experiments on six large real-world graph datasets with genuine anomalies from diverse domains, including the co-review network in Amazon , transaction record network in T-Finance , social networks in Reddit , bitcoin transaction in Elliptic , co-purchase network in Photo  and financial network in DGraph . See App. A for more details about the datasets. Although it is easy to obtain normal nodes, the human checking and annotation of large-scale nodes are still costly. To simulate practical scenarios where we need to annotate only a relatively small number of normal nodes, we randomly sample \(R\)% of the normal nodes as labeled normal data for training, in which \(R\) is chosen in \(\{10,15,20,25\}\), with the rest of nodes is treated as the testing set. Due to the massive set of nodes, the same \(R\) applied to DGraph would lead to a significantly larger set of normal nodes than the other three data sets, leading to very different annotation costs in practice. Thus, on DGraph, \(R\) is chosen in \(\{0.05,0.2,0.35,0.5\}\) to compose the training data.

**Competing Methods.** To our best knowledge, there exist no GAD methods specifically designed for semi-supervised node-level GAD. To validate the effectiveness of GGAD, we compare it with six state-of-the-art (SOTA) unsupervised methods and their advanced versions in which we effectively adapt them to our semi-supervised setting. These methods include two reconstruction-based models: DOMINANT  and AnomalyDAE , two one-class classification models: TAM  and OCGNN , and two generative models: AEGIS  and GAAN . To effectively incorporate the normal information into these unsupervised methods, for the reconstruction models, DOMINANT and AnomalyDAE, the data reconstruction is performed on the labeled normal nodes only during training. In OCGNN, the one-class center is optimized based on the labeled normal nodes exclusively. In TAM, we train the model by maximizing the affinity on the normal nodes only. As for AEGIS and GAAN, the normal nodes combined with their generated outliers are used to train an adversarial classifier. Self-supervised-based methods like CoLA , SL-GAD , and HCM-A  and semi-supervised methods that require both labeled normal and abnormal nodes like GODM and DiffAD  are omitted because training these methods on the data with exclusively normal nodes does not work.

**Evaluation Metric.** Following prior studies [5; 40; 51], two popular and complementary evaluation metrics for anomaly detection, the area under ROC curve (AUROC) and Area Under the precision-recall curve (AUPRC), are used to evaluate the performance. Higher AUROC/AUPRC indicates better performance. AUROC reflects the ability to recognize anomalies while at the same time considering the false positive rate. AUPRC focuses solely on the precision and recall rates of anomalies detected. The AUROC and AUPRC results are averaged over 5 runs with different random seeds.

**Implementation Details.** GGAD is implemented in Pytorch 1.6.0 with Python 3.7. and all the experiments are run on a 24-core CPU. In GGAD, its weight parameters are optimized using Adam  optimizer with a learning rate of \(1e-3\) by default. For each dataset, the hyperparameters \(\) and \(\) for two constraints are uniformly set to 1, though GGAD can perform stably with a range of \(\) and \(\) (see App. C.2). The size of the generated outlier nodes \(S\) is set to 5% of \(|_{i}|\) by default and stated otherwise. The affinity margin \(\) is set to 0.7 across all datasets. The perturbation in Eq. (5) is drawn from a Gaussian distribution, with mean and standard variance set to 0.02 and 0.01 respectively, and it is stated otherwise. All the competing methods are implemented by using their publicly available official source code or the library, and they are trained using their suggested hyperparameters. To apply GGAD and the competing models to very large graph datasets, _i.e._, DGraph, a min-batch training strategy is applied (see Algorithm 2 for detail).

### Main Comparison Results

Table 1 shows the comparison of GGAD to 12 models, in which semi-supervised models use 15% normal nodes during training while unsupervised methods are trained on the full graph in a fully unsupervised way. We will discuss results using more/less training normal nodes in Sec. 4.2.

**Comparison to Unsupervised GAD Methods**. As shown in Table 1, GGAD significantly outperforms all unsupervised methods on six datasets, having maximally 21% AUROC and 39% AUPRC improvement over the best-competing unsupervised methods on individual datasets. The results also show that the semi-supervised versions of the unsupervised methods largely improve the performance of their unsupervised counterparts, justifying that i) incorporating the normal information into the unsupervised approaches is beneficial for enhancing the detection performance and ii) our approach to adapt the unsupervised methods is effective across various types of GAD models. TAM performs best among the unsupervised methods. AEGIS which leverages GAN to learn the normal patterns performs better than AnomalyDAE and DOMINANT on T-Finance, Reddit, and Photo, By contrast, reconstruction-based methods work well on Amazon and DGraph. Similar observations can be found for the semi-supervised versions.

**Comparison to Semi-supervised GAD Methods**. The results in Table 1 show that although the semi-supervised methods largely outperform unsupervised counterparts, they substantially underperform our method GGAD. The reconstruction-based approaches show the most competitive performance among the contenders in semi-supervised settings, _e.g._, AnomalyDAE performs best on Amazon and DGraph. Nevertheless, GGAD gains respectively about 1-3% AUROC/AUPRC improvement on these two datasets compared to best-competing AnomalyDAE. By training on the normal nodes only, methods like TAM and AEGIS largely reduce the interference of unlabeled anomaly nodes on the model and work well on most of the datasets, _e.g._, TAM on Amazon and Reddit, AEGIS on T-Finance and Reddit. However, their performance is still lower than GGAD by a relatively large margin. GGAD yields the best AUROC on the Photo while yielding the second-best in AUPRC, underperforming OCGNN. This indicates that GGAD can detect some anomalies very accurately in Photo, but it is less effective than OCGNN to get a bit more anomalies rank at the top of normal nodes in terms of their anomaly score. On average over the six datasets, GGAD outperforms the best semi-supervised contender AnomalyDAE by 11% in AUROC and 5% in AUPRC, demonstrating that GGAD can make much better use of the labeled normal nodes through our two anomaly prior-based losses.

### Performance w.r.t. Training Size and Anomaly Contamination

In order to further illustrate the effectiveness of our method, we also compare GGAD with other semi-supervised methods using varying numbers of training normal nodes in Fig. 4 and having

    &  &  \\   & & &  &  &  \\  & & & & & & & & & & & & & \\  & & & & & & & & & & & & \\  & & & & & & & & & & & & \\   & DOMINANT & 0.7025 & 0.6087 & 0.5105 & 0.2960 & 0.5136 & 0.5738 & 0.1315 & 0.0536 & 0.0380 & 0.0454 & 0.1039 & 0.0075 \\  & AnomalyDAE & 0.7783 & 0.5809 & 0.5091 & 0.4963 & 0.5096 & 0.5763 & 0.1429 & 0.0491 & 0.0319 & 0.0872 & 0.0987 & 0.0070 \\  & OCGNN & 0.7165 & 0.4732 & 0.5246 & 0.2581 & 0.5307 & 0 & 1.0352 & 0.0392 & 0.0375 & 0.0166 & 0.0905 & / \\  & AEGIS & 0.6059 & 0.6496 & 0.5349 & 0.4553 & 0.5516 & 0.4509 & 0.1200 & 0.0652 & 0.0413 & 0.0827 & 0.0972 & 0.0053 \\  & GAAN & 0.6513 & 0.3091 & 0.5216 & 0.2590 & 0.4296 & / & 0.0852 & 0.0283 & 0.0348 & 0.0345 & 0.0676 & / \\  & TAM & 0.8303 & 0.6175 & 0.6026 & 0.0426 & / & 0.4024 & 0.0474 & 0.0507 & 0.0502 & 0.1013 & / \\   & DOMINANT & 0.8867 & 0.6167 & 0.5194 & 0.3236 & 0.5314 & 0.5851 & 0.7289 & 0.0542 & 0.0414 & 0.0552 & 0.1283 & 0.0076 \\  & AnomalyDAE & 0.9117 & 0.6027 & 0.5280 & 0.5409 & 0.5272 & 0.5866 & 0.7748 & 0.0583 & 0.0362 & 0.0949 & 0.1177 & 0.0017 \\  & OCGNN & 0.8810 & 0.5742 & 0.5622 & 0.2881 & 0.4061 & / & 0.7538 & 0.0492 & 0.0080 & 0.0480 & **0.1891** & / \\  & AEGIS & 0.7599 & 0.6728 & 0.5605 & 0.5132 & 0.5936 & 0.4450 & 0.2616 & 0.0685 & 0.0441 & 0.0912 & 0.1110 & 0.0058 \\  & GAAN & 0.6531 & 0.3636 & 0.5349 & 0.2724 & 0.4355 & / & 0.0856 & 0.0324 & 0.0362 & 0.0611 & 0.0768 & / \\  & TAM & 0.8405 & 0.5923 & 0.5829 & 0.4150 & 0.6013 & / & 0.5183 & 0.0551 & 0.0446 & 0.0552 & 0.1087 & / \\  & **GGAD (Ours)** & **0.9443** & **0.8228** & **0.6354** & **0.7290** & **0.6476** & **0.943** & **0.7922** & **0.1825** & **0.0610** & **0.2425** & 0.1442 & **0.0082** \\   

Table 1: AUROC and AUPRC on six GAD datasets. The best performance per dataset is boldfaced, with the second-best underlined. ‘\(\)’ indicates that the model cannot handle the DGraph dataset.

Figure 4: AUPRC results w.r.t the size of training normal nodes (\(R\)% of \(||\)). ‘Baseline’ denotes the performance of the best unsupervised GAD method.

various anomaly contamination rates in Fig. 5. Due to page limitation, we present the AUPRC results on two datasets here only, showing the representative performance. The full AUROC and AUPRC results are reported in App. C.

The results in Fig. 4 show that with increasing training samples of normal nodes, the performance of all methods on all four datasets generally gets improved since more normal samples can help the models more accurately capture the normal patterns during training. Importantly, GGAD consistently outperforms all competing methods with varying numbers of normal nodes, reinforcing that GGAD can make better use of the labeled normal nodes for GAD.

The labeled normal nodes can often be contaminated by anomalies due to factors like annotation errors. To consider this issue, we introduce a certain ratio of anomaly contamination into into the training normal node set \(_{l}\). The results of the models under different ratios of contamination in Fig. 5. show that with increasing anomaly contamination, the performance of all methods decreases. Despite the decreased performance, our method GGAD consistently maintains the best performance under different contamination rates, showing good robustness w.r.t. the contamination.

### Ablation Study

**Importance of the Two Anomaly Node Priors.** The importance of the two proposed losses based on the priors on the anomaly nodes is examined by comparing our full model with its variant removing the corresponding loss, with the results shown in Table 4. It is clear that learning the outlier node representations using one of the two losses performs remarkably less effectively than our full model using both losses. It is mainly because although using \(_{ala}\) solely can obtain similar local affinity of the outliers to the real anomaly nodes, the outliers are still not aligned well with the distribution of the anomaly nodes in the node representation space. Likewise, only using the \(_{ec}\) can result in a mismatch between the generated outliers and real abnormal samples in their graph structure. GGAD that effectively unifies both priors through the two losses can generate outlier nodes that well assimilate and node representation space, supporting

**GGAD vs. Alternative Outlier Node Generation Approaches.** To examine its effectiveness further, GGAD is also compared with four other approaches that could be used as an alternative to generating the outlier nodes. These include (i) **Random** that randomly sample some normal nodes and treat them as outliers to train a one-class discriminative classifier, (ii) **Non-learnable Outliers (NLO)** that removes the learnable parameters \(}\) in Eq. (2) in our outlier node generation, (iii) **Noise** that directly generates the representation of outlier nodes from random noise, (iv) **Gaussian Perturbation (GaussianP)** that directly adds Gaussian perturbations into the sampled normal nodes' representations to generate the outliers. Apart from the **Noise** and **GaussianP**, we further employ two advanced generation approaches, (vi) **VAE** that generate the outlier representations by reconstructing the raw attributes of selected nodes where our two anomaly prior-based constraints are applied to the generation, and (v) **GAN** that generates the embedding from the noise and adds an adversarial function to discriminate whether the generated node is fake or real, with our two prior constraints applied in

    &  &  \\   & \(_{ala}\) & \(_{ec}\) & \(_{ec}\) & Amara & T-Finance & Rockhill & Ellipst & Photo & DGraph \\   & \)} & \(\) & 0.871 & 0.8149 & 0.5393 & 0.6680 & 0.5762 & 0.5581 \\  & & ✓ & 0.7250 & 0.6941 & 0.5230 & 0.7001 & 0.6103 & 0.5513 \\  & ✓ & ✓ & **0.9354** & **0.8228** & **0.8545** & **0.7209** & **0.6476** & **0.5943** \\   & & ✓ & 0.6564 & 0.1179 & 0.6409 & 0.1954 & 0.1737 & 0.0765 \\   & \)} & \(\) & 0.7834 & **0.1924** & **0.0610** & 0.2425 & **0.1442** & **0.0087** \\   & & & & & & & & \\   

Table 2: Ablation study on our two priors.

    &  \\   &  & Amara & T-Finance & Rockhill & Ellipst & Photo & DGraph \\   & Random & 0.7263 & 0.4613 & 0.5227 & 0.6856 & 0.5678 & 0.5712 \\  & NLO & 0.8613 & 0.6179 & 0.5638 & 0.6750 & 0.5307 & 0.5538 \\  & Noise & 0.8508 & 0.8230 & 0.5328 & 0.6786 & 0.5940 & 0.5779 \\  & GaussianP & 0.2279 & 0.6693 & 0.5235 & 0.6715 & 0.5922 & 0.5862 \\  & VAE & 0.8984 & 0.6574 & 0.6123 & 0.3055 & 0.6222 & 0.5010 \\  & GAN & 0.8288 & 0.5487 & 0.5735 & 0.6256 & 0.6022 & 0.5101 \\  & **GGAN (Ours)** & **0.9324** & **0.8283** & **0.6345** & **0.7290** & **0.6276** & **0.5943** \\   & Random & 0.7557 & 0.4020 & 0.5049 & 0.1981 & 0.1063 & 0.1064 \\  & NLO & 0.4696 & 0.1364 & 0.0945 & 0.1750 & 0.1092 & 0.0055 \\   & Noise & 0.5384 & 0.1762 & 0.0381 & 0.1924 & 0.1094 & 0.0076 \\   & GaussianP & 0.03970 & 0.6677 & 0.0796 & 0.16682 & 0.1194 & 0.0072 \\   & VAE & 0.6111 & 0.0652 & 0.0528 & 0.2344 & 0.1222 & 0.0063 \\   & GAN & 0.3715 & 0.0661 & 0.0433 & 0.1263 & 0.1143 & 0.0051 \\   & **GGAN (Ours)** & **0.7843** & **0.1924** & **0.0610** & **0.2425** & **0.1442** & **0.0087** \\   

Table 3: GGAD vs. alternative outlier generators.

Figure 5: AUPRC w.r.t. contamination.

the generation as well. The results are shown in Table 3. **Random** does not work properly, since the randomly selected samples are not distinguishable from the normal nodes. **NLO** performs fairly well on some data sets such as Amazon, T-Finance, and Elliptic, but it is still much lower than GGAD, showcasing that having learnable outlier node representations can help better ground the outliers in a real local graph structure. Despite that **Noise** and **GaussianP** can generate outliers that have separable representations from the normal nodes, they also fail to work well since the lack of graph structure in the outlier nodes can lead to largely mismatched distributions between the generated outlier nodes and the anomaly nodes. By contrast, the outlier nodes learned by GGAD can better align with the anomaly nodes due to the incorporation of the anomaly priors on graph structure and feature representation into our GAD modeling. Both **VAE** and **GAN** can work well on some datasets, which indicates two priors help them learn relevant outlier representations. But both of them are still much lower than GGAD, showcasing that the outlier generation approach in GGAD can leverage the two proposed priors to generate better outliers.

**GGAD vs. GGAD enabled Unsupervised Methods.** We incorporate the outlier generation into existing unsupervised methods to demonstrate the generation in GGAD can also benefit the existing unsupervised methods. To allow the unsupervised methods to fully exploit the generated outliers, we first utilize GGAD to generate outlier nodes by training on randomly sampled nodes from a graph (which can be roughly treated as all normal nodes due to anomaly scarcity) and then remove possible abnormal nodes from the graph dataset by filtering out Top-K modes. By removing these suspicious abnormal nodes, the unsupervised method is expected to train on the cleaner graph (_i.e._, with less anomaly contamination). This approach to improve unsupervised GAD methods is referred to as GGAD-enabled unsupervised GAD. We evaluate their effectiveness on three large-scale datasets. As shown in Table 4, where #Anomalies/#Top-K Node represents the number of real abnormal nodes we successfully filter out and the number of nodes we choose to filter out (_i.e._, K) respectively. For example, we use the outlier nodes generated by GGAD to filter out 500 nodes from the Amazon dataset, of which there are 387 real abnormal nodes. This helps largely reduce the anomaly contamination rate in the graph. The results show that this approach can significantly improve the performance of three different representative unsupervised GAD methods, including DOMINANT, OCGNN, and AEGIS. Note that although the GGAD-enabled unsupervised methods achieve better performance, their performance still largely underperforms GGAD, which provides stronger evidence for the effective capability in anomaly detection of GGAD.

## 5 Conclusion and Future Work

In this paper, we investigate a new semi-supervised GAD scenario where part of normal nodes are known during training. To fully exploit those normal nodes, we introduce a novel outlier generation approach GGAD that leverages two important priors about anomalies in the graph to learn outlier nodes that well assimilate real anomalies in both graph structure and feature representation space. The quality of these outlier nodes is justified by their effectiveness in training a discriminative one-class classifier together with the given normal nodes. Comprehensive experiments are performed to establish an evaluation benchmark on six real-world datasets for semi-supervised GAD, in which our GGAD outperforms 12 competing methods across the six datasets.

**Limitation and Future work.** The generation of the outlier nodes in GGAD is built upon the two important priors about anomaly nodes in a graph. This helps generate outlier nodes that well assimilate the anomaly nodes across diverse real-world GAD datasets. However, these priors are not exhaustive, and there can be some anomalies whose characteristics may not be captured by the two priors used. We will explore this possibility and improve GGAD for this case in our future work.

  
**Metric** & **Method** &  \\   &  &  &  } \\   &  &  } &  } \\   &  &  & 0.7025 & 0.6870 & 0.2650 \\  & & & 0.8186 & 0.6275 & 0.2986 \\  & & OCGNN & 0.7165 & 0.4732 & 0.2581 \\  & & OCGNN & 0.8692 & 0.5931 & 0.2638 \\  & & AEGIS & 0.6699 & 0.6496 & 0.4553 \\  & & AEGIS & 0.8395 & 0.7042 & 0.8363 \\  & & GOAD & **0.4931** & **0.8108** & **0.7225** \\   &  &  & 0.1315 & 0.0362 & 0.0454 \\  & & OCGNN & 0.1352 & 0.0392 & 0.0616 \\  & & OCGNN & 0.9504 & 0.0806 & 0.0607 \\  & & AEGIS & 0.1200 & 0.0622 & 0.0827 \\  & & GOAD-enabled AEGIS & 0.3833 & 0.0784 & 0.0910 \\  & & GGAD & **0.7769** & **0.1734** & **0.2484** \\   

Table 4: GGAD enabled unsupervised methods.

**Acknowledgments.** We thank the anonymous reviewers for their valuable comments. The participation of Guansong Pang was supported in part by Lee Kong Chian Fellowship.