# Shaping the distribution of neural responses with interneurons in a recurrent circuit model

David Lipshutz

Center for Computational Neuroscience, Flatiron Institute

dlipshutz@flatironinstitute.org

&Eero P. Simoncelli

Center for Computational Neuroscience, Flatiron Institute

Center for Neural Science, New York University

eero.simoncelli@nyu.edu

###### Abstract

Efficient coding theory posits that sensory circuits transform natural signals into neural representations that maximize information transmission subject to resource constraints. Local interneurons are thought to play an important role in these transformations, dynamically shaping patterns of local circuit activity to facilitate and direct information flow. However, the relationship between these coordinated, nonlinear, circuit-level transformations and the properties of interneurons (e.g., connectivity, activation functions, response dynamics) remains unknown. Here, we propose a normative computational model that establishes such a relationship. Our model is derived from an optimal transport objective that conceptualizes the circuit's input-response function as transforming the inputs to achieve an efficient target response distribution. The circuit, which is comprised of primary neurons that are recurrently connected to a set of local interneurons, continuously optimizes this objective by dynamically adjusting both the synaptic connections between neurons as well as the interneuron activation functions. In an example application motivated by redundancy reduction, we construct a circuit that learns a dynamical nonlinear transformation that maps natural image data to a spherical Gaussian, significantly reducing statistical dependencies in neural responses. Overall, our results provide a framework in which the distribution of circuit responses is systematically and nonlinearly controlled by adjustment of interneuron connectivity and activation functions.

## 1 Introduction

The problem of transforming a signal into a representation with a given target distribution (or within a target set of distributions) is a classical problem whose origins can be traced back more than two centuries . Many methods in statistics, signal processing and machine learning can be interpreted within this context. For example, data whitening is a common preprocessing step that linearly transforms a signal to have identity covariance . Independent component analysis [ICA; 3] is a signal processing method that linearly transforms a signal so as to minimize higher-order statistical dependencies in addition to removing second-order dependencies. Nonlinear transformations of skewed or heavy-tailed data to approximately Gaussianize their distribution can facilitate statistical analyses [4; 5]. Machine learning methods for density estimation such as Gaussianization [6; 7; 8; 9] and normalizing flows [11; 12; 13] nonlinearly transform high-dimensional signals with complex densities into more tractable representations with approximately Gaussian densities.

This problem may also provide a framework for constructing and evaluating normative theories of sensory processing. Efficient coding theory posits that sensory systems maximize the information they transmit about sensory signals to downstream areas subject to resource constraints . In one instantiation of this theory, the redundancy reduction hypothesis posits that sensory circuits transform natural signals into representations to minimize or eliminate statistical dependencies between coordinates, essentially producing factorized response distributions . In a separate, but related, instantiation, sparse coding theory posits that population responses are optimized for sparsity , which is naturally interpreted as a constraint on the shape of the distribution of responses. In another line of theoretical work, sensory representations are posited to maximize the Fisher information of the inputs , which can be interpreted as a statement about the joint distribution of the inputs and responses. Importantly, each of these theories can be formulated as a transformation of the signal into a representation with target distribution that is optimal under information theoretic and metabolic constraints. However, it is not clear how neural circuits learn or implement these potentially nonlinear transformations.

Neural circuits are typically comprised of populations of primary (excitatory) neurons and local (inhibitory) interneurons. Extensive experimental measurements have led to the idea that local interneuron populations allow neural circuits to flexibly shape patterns of primary neuron responses so as to coordinate or regulate information flow . Consequently, local interneurons are natural candidate mechanisms responsible for shaping circuit responses into efficient representations. However, the precise relationship between the physiological properties and connectivity of local interneurons and the coordinated response properties of populations of primary neurons remains unclear.

Several normative mechanistic models have been proposed to explain how neural circuits can _linearly_ transform their inputs into a representation whose distribution lies within a target set (e.g., the set of distributions with identity covariance) . These models are derived from optimization objectives for linear redundancy reduction, including (adaptive) decorrelation and ICA, and the circuit parameters (e.g., synaptic weights, response gains) are optimized to match the data distribution. In these cases, the optimization steps correspond to processes such as gain modulation and synaptic plasticity, thus demonstrating how adjustments of circuit parameters according to local signals can optimize a global, circuit-level objective for redundancy reduction of the neural responses. While linear transformations can remove second-order statistical dependencies, they cannot remove higher-order statistical dependencies that are prominent in sensory signals . Furthermore, early sensory systems exhibit a host of prominent nonlinear response properties  that are not well-approximated by linear models.

Figure 1: Schematic of a recurrent circuit with \(N=2\) primary neurons and \(K=3\) interneurons. **Left:** Scatter plot of a 2D input signals (gray points) \(=(s_{1},s_{2})\) with \( p_{}\). **Center:** Primary neurons (black circles), with outputs \(=(r_{1},r_{2})\), receive external feedforward inputs, \(\), and recurrent feedback from an auxiliary population of interneurons (purple circles), \(-\), where \(=(n_{1},n_{2},n_{3})\) are the interneuron outputs. Projection vectors \(\{_{1},_{2},_{3}\}\) encode feedforward synaptic weights connecting primary neurons to interneurons \(i=1,2,3\), with symmetric feedback connections. **Inset:** The \(i^{}\) interneuron (here \(i=1\)) receives weighted inputs \(z_{i}:=_{i}\), which is fed through the activation function \(f(_{i},)\) and scaled by the gain \(g_{i}\) to generate the output \(n_{i}:=g_{i}f(_{i},z_{i})\). **Right:** Scatter plot of the 2D circuit responses \(=(r_{1},r_{2})\) with \( p_{}\).

Here, we seek a normative circuit model that _nonlinearly_ transforms inputs to produce responses with a (spherical) target distribution. We develop an optimal transport objective for transforming the input signal into a neural representation with a given target distribution, and derive an algorithm (Alg. 1) that can be mapped onto a dynamical model of a neural circuit, Fig. 1. The circuit model is comprised of primary neurons that are recurrently connected to local interneurons and the circuit adapts its responses using a combination of Hebbian synaptic plasticity and interneuron adaptation. Complementary computational roles for Hebbian plasticity and interneuron adaptation emerge from this analysis: (i) synapses are updated according to a Hebbian learning rule to identify projections of the signal that are least aligned with the target distribution (essentially, a form of projection pursuit ); (ii) gains and nonlinear activation functions of interneurons are adjusted to transform the marginal circuit responses along directions defined by the synaptic weights. Together these operations transform the distribution of responses to approximate the target distribution.

As a primary test case motivated by redundancy reduction theory, we apply our algorithm to inputs derived from natural images (responses of local oriented filters, qualitatively similar to those found in the primary visual cortex) and the target distribution is the spherical Gaussian.1 We find that the algorithm learns a nonlinear transformation that approximately Gaussianizes the responses and significantly reduces statistical dependencies between coordinates. Overall, our results demonstrate how local interneurons may adjust their connectivity and response properties to nonlinearly reshape the distribution of circuit responses, thus facilitating efficient transmission of information.

## 2 Circuit objective

Consider a neural circuit with \(N 2\) primary neurons that transforms input signals \(^{N}\), which are distributed according to a density \(p_{}\), into circuit responses \(^{N}\) (Fig. 1). The inputs may represent a direct sensory input (e.g., the rate at which photons are absorbed by a cone) or the weighted sum of multiple inputs (e.g., the postsynaptic current). The responses \(\) represent the firing rate (or the logarithm of the firing rate) of the neuron. For simplicity, we assume that the circuit responses are a deterministic function of the input signals; that is, \(=T()\) for a function \(T:^{N}^{N}\).

### Optimal transport objective

We assume that the objective of the circuit is to transform its inputs \(\) so that the circuit responses \(=T()\) follow a (spherical) target distribution \(p_{}\) while minimizing the \(L^{2}\)-distance between responses and input signals. Mathematically, this corresponds to an optimal transport problem 

\[_{T}[\|T()-\|^{2}+\|T()\|^{2}] T() p_{},\] (1)

where the minimization is over a suitable class of functions \(T\), \(\) is a regularizing term and, unless otherwise noted, expectations are over the input distribution \(p_{}\). Note that the choice of \(\) does not affect the optimal solution as \([\|T()\|^{2}]\) is fixed provided \(T() p_{}\); however, it will affect the optimization algorithm. Assuming \(p_{}\) is sufficiently regular, the minimum in eq. (1) is the squared Wasserstein-2 distance between \(p_{}\) and \(p_{}\) and the input-response transformation \(T\) is the so-called optimal transport plan that maps the input distribution \(p_{}\) to the target distribution \(p_{}\).

Our goal is to derive an online algorithm for optimizing the objective in eq. (1) that can be implemented in a neural circuit model. We accomplish this by defining a distance between the response distribution \(p_{}\) and the target distribution \(p_{}\) that can be estimated in a neural circuit, and then solving the optimization problem by incorporating this measure as a constraint, using the method of Lagrange multipliers.

### Measuring the discrepancy between the response distribution and the target distribution

Common candidates for measuring the discrepancy between two distributions include Kullback-Leibler (KL) divergence or integral probability metrics . The former typically require numerous samples to estimate the density \(p_{}\), whereas neural circuits must operate in the online setting without access to the full history of their responses. Therefore, we use an integral probability metric that quantifies the difference between the random variables when evaluated using constraint functions that can be rapidly estimated online.

We restrict our solution to use constraint functions that compare the _marginal_ response distributions to the marginals of \(p_{}\), denoted \(p_{}\), which are all equal under our assumption that \(p_{}\) is spherical. Such constraint functions are well matched to signals that are linear mixtures of independent sources, as in generative models for ICA. Even when the signal statistics are not generated according to a linear mixture model, the Cramer and Wold theorem  suggests that transforming sufficiently many marginals of the response distribution may effectively transform the multivariate response distribution (though the number of marginals required may be quite large). Our motivation for measuring marginal response distributions is due, in part, by our goal of modeling local interneurons, whose inputs are naturally modeled as weighted sums of primary neuron responses (i.e., their input distributions are marginals of the primary responses).

To compare the marginal response distributions, we first select a finite set of directions defined by \(K 1\) unit vectors \(_{1},,_{K}^{N}\), which can be either randomly sampled, chosen based on prior knowledge of the signal statistics, or learned from data using projection pursuit . We then choose a class of scalar functions \(\{h(,)\}\) parameterized by \(\), which defines a semi-metric between the marginal of \(\) in the direction \(\) and \(p_{}\) to be \(_{}|[(,)]|\), where

\[(,z):=h(,z)-_{z p_{}}[h(, z)].\]

For example, when \(\{h(,)\}\) parameterizes all Lipschitz-1 (indicator) functions, this induces the Wasserstein-1 (total variation) distance between the marginal response distribution and \(p_{}\). Given directions \(_{1},,_{K}\) and constraint functions \(\{h(,)\}\), we express the distance between the response density \(p_{}\) and the standard Gaussian distribution as the sum of the marginal distances:

\[d_{,}(p_{}):=_{i=1}^{K}_{_{i}}|_{ p_{}}[(_{i}, _{i})]|,\]

where \(:=[_{1},,_{K}]\) is the \(N K\) matrix of concatenated unit vectors.2

How do we choose the constraint functions \(\{h(,)\}\)? In general, the choice should be well-suited to the input distribution \(p_{}\) and the target distribution \(p_{}\). For example, consider the simple case when the inputs follow a centered Gaussian distribution with unknown covariance structure, and the target distribution is the spherical Gaussian distribution \((,)\). The Wasserstein-2 distance between a marginal distribution of the inputs and the standard normal distribution \((0,1)\) can be expressed in terms of the difference between the second moments, so a quadratic function \(h(z)=z^{2}\) suffices. In section 4.2, we consider a parametric class motivated by natural signal statistics.

### Optimization using Lagrange multipliers

We replace the condition \( p_{}\) in eq. (1) with the condition \(_{}d_{,}(p_{})=0\), which we enforce using Lagrange multipliers. This results in the minimax optimization problem

\[_{}_{}_{}_{} [_{}(,, ,,)],\] (2)

where \(\) is defined by

\[(,,,,):= \|-\|^{2}+\|\|^{2}+_{i=1}^{K}g_{i} (_{i},_{i}),\] (3)

and \(:=(_{1},,_{K})\) is the set of concatenated parameters, \(:=(g_{1},,g_{K})\) is a \(K\)-dimensional vector of Lagrange multipliers, and the circuit transform is defined by \(T()=*{arg\,min}_{}(, ,,,)\). The maximization over \(\) and \(\) effectively minimizes the distances between the marginal distributions of the responses \(\) along the directions \(_{1},,_{K}\) and \(p_{}\), whereas the maximization over the matrix \(\) learns directions along which the marginals of \(\) are least aligned with \(p_{}\), essentially performing projection pursuit .

Algorithm and circuit implementation

We now derive an online gradient-based algorithm for optimizing the objective in eq.2, then map the algorithm onto a recurrent neural circuit. Spiking activity operates on a much faster timescale than neural or synaptic adaptation mechanisms, so we assume that the neural activities equilibrate before the neural activations and synapses are updated.

### Fast recurrent neural dynamics

At each iteration, the circuit receives a stimulus \(\). The (discretized) recurrent neural response dynamics (Fig.1) correspond to gradient-descent minimization of \(\) with respect to \(\):

\[ +_{r}(-- _{i=1}^{K}n_{i}_{i}), n_{i} =g_{i}f(_{i},z_{i}).\] (4)

where \(_{r}>0\) is a small constant, \(:=1+\) represents a leak term, \(z_{i}:=_{i}\) is the weighted input to the \(i^{}\) interneuron, \(f(_{i},):=(_{i},)/ z\) is the activation function, \(g_{i}\) is a multiplicative gain that scales the output, and \(n_{i}\) denotes the output. Notably, the interneuron activation response function \(g_{i}f(_{i},)\) is parameterized by \((g_{i},_{i})\), which can vary across interneurons, so the interneuron responses are heterogeneous. For each \(i\), synaptic weights \(_{i}\) connect the primary neurons to the \(i^{}\) interneuron and symmetric weights \(-_{i}\) connect the \(i^{}\) interneuron to the primary neurons. From eq.4, we see that the neural responses are driven by the signal \(\), a leak term \(-\), and recurrent weighted feedback from the interneurons \(-\), where \(:=(n_{1},,n_{K})\).

Since the neural activities equilibrate before other updates are performed, the responses \(\) are a fixed point of \((,,,,)\). In general, we do not have a closed-form expression for \(\); however, if \(g_{i} 0\) and \((_{i},)\) are convex, then \((,,,,)\) is convex and we can express the transform as

\[T()=*{arg\,min}_{}(, ,,,).\] (5)

In Appx. A we show that the transform \(T()\) is invertible whenever \(g_{i} 0\) and \((_{i},)\) are convex and thus it defines a precise relationship between the input distribution \(p_{}\) and response distribution \(p_{}\).

### Gain modulation, activation function adaptation and Hebbian plasticity

After the neural activities reach equilibrium, we maximize \(\) by taking concurrent gradient-ascent steps with respect to \(g_{i}\), \(_{i}\) and \(_{i}\):

\[ g_{i} =_{g}(_{i},z_{i}), _{i} =_{}_{}(_{i},z_{i}), _{i} =_{w}n_{i},\]

where \(_{g},_{},_{w} 0\) are the respective learning rates, which control the relative speeds of gain modulation, neural adaptation and synaptic plasticity, respectively. For example, at the extremes, we can _fix_ the gains, activation functions or synaptic weights by setting \(_{g}=0\), \(_{}=0\) or \(_{w}=0\), respectively. Notably, while synaptic plasticity  and gain modulation  are well-studied circuit mechanisms that support learning and adaptation, adjustments of nonlinear neural activation functions are not as well established. Nevertheless, there is some emerging evidence that neurons also adapt their activation functions in response to changes in their input statistics [49; 50].

The circuit operates online and the updates are local in the sense that the updates to the gain and activation function of the \(i^{}\) interneuron depend only on variables \(_{i}\) and \(z_{i}\). The updates to the synapses \(_{i}\) (and \(-_{i}\)) are proportional (inversely proportional) to the product of the pre- and postsynaptic activities, so they are both local and _Hebbian (anti-Hebbian)_. Finally, to ensure that the vectors \(_{1},,_{K}\) have unit norm, we normalize the weights after each update: \(_{i}_{i}/\|_{i}\|\). This can be viewed as form of homeostatic plasticity such as synaptic scaling . While the feedforward weights \(_{i}\) and feedback weights \(-_{i}\) are constrained to be symmetric, these can be decoupled due to the symmetry of the Hebbian learning rule. Both theoretical and empirical evidence of this is shown for a related adaptive whitening circuit in [36, appendix E.2].

### Online algorithm

Combining the neural dynamics, the interneuron adaptation and synaptic plasticity steps yields our online algorithm (Alg.1), which we write in vector-matrix notation by defining the normalization function \(P():=[_{1}/\|_{1}\|,,_{K}/\| _{K}\|]\).

### Relation to existing algorithms

Algorithm 1 is naturally viewed as a nonlinear extension of a number of existing algorithms for linear data whitening that have neural circuit implementations [34; 36; 53; 54]. In particular, when the constraint function is quadratic, \(h(z)=z^{2}\), and the target distribution is the spherical Gaussian, \((,)\), then the activation function is the identity, \(f(z)=z\), and the optimization in eq. (2) enforces that the second moments of the responses match the second moments of \((,)\), which corresponds to data whitening. If the gains are fixed (\(_{g}=0\)) and \(K N\) (i.e., synaptic adaptation only), then Alg. 1 corresponds to the adaptive whitening algorithm presented in [34; 53]. Alternatively, if the synaptic weights are fixed (\(_{w}=0\)) and \(K N(N+1)/2\) (i.e., interneuron gain adaptation only), then Alg. 1 corresponds to the adaptive whitening algorithm presented in . Finally, if the gains adapt on a fast timescale and the synapses update on a slow timescale (i.e., \(_{g}_{w}>0\)), Alg. 1 corresponds to the multi-timescale adaptive whitening algorithm presented in .

```
1:input:\(_{1},_{2},\)
2:initialize:\(\), \(\), \(\), \(\), \(_{r},_{g},_{},_{w}\)
3:for\(t=1,2,\)do
4:\(_{t}_{t}\)
5:while not converged do
6:\(_{t}^{}_{t}\) ; // interneuron inputs
7:\(_{t} f(,_{t})\) ; // interneuron outputs
8:\(_{t}_{t}+_{r}(_{t}-_{t}-_{t})\) ; // neural responses
9:endwhile
10:\(+_{g}(,_{t})\) ; // gain update
11:\(+_{}_{} (,_{t})\) ; // activation update
12:\( P(+_{w}_{t}_{t}^{})\) ; // Hebbian + homeostatic plasticity
13:endfor ```

**Algorithm 1**Approximate optimal transport with Hebbian plasticity and interneuron adaptation

When the target distribution is the spherical Gaussian and \(\) is constrained to be an orthogonal matrix, Alg. 1 is related to existing iterative algorithms for Gaussianization that alternate between (a) orthogonal transformations and (b) marginal Gaussianization of the coordinates [6; 8]. In the case that the column vectors of \(\) are orthogonal, Gaussianization along one marginal does not affect the responses along other marginals, allowing these operations to be performed independently of one another. In general, we allow the column vectors of \(\) to be non-orthogonal and potentially overcomplete so that marginal Gaussianization along one basis vector affects the other marginal distributions. Consequently, the algorithm is more complicated to analyze mathematically (e.g., obtaining convergence guarantees), but it is more biologically realistic since neural systems are unlikely to have orthonormal synaptic weight vectors.

## 4 Gaussianization of natural image statistics

We apply our algorithm to the problem of efficient nonlinear encoding of natural signals, specifically oriented filter responses to visual images.3 Redundancy reduction theories posit that early sensory systems transform natural signal into neural representations with reduced statistical redundancies [14; 15; 17]. In support of this hypothesis, early sensory representations exhibit far less spatial and temporal correlation than natural signals [55; 56] and methods such as linear ICA have been used to derive optimal representations of natural signals that are approximately matched to early sensory neural properties [18; 19; 57].

Linear whitening and ICA transforms can eliminate simple forms of statistical dependency, but their responses exhibit higher-order statistical dependencies when applied to natural signals , suggesting that sensory systems can more efficiently represent natural signals by implementing nonlinear transforms. Consistent with this, nonlinear phenomenological models of neural responses (e.g., divisive normalization [37; 39]) effectively reduce these higher-order statistical dependencies [38; 59]. However, the circuit mechanisms that support these _nonlinear_ transformations are unknown.

We consider the particular target of Gaussian responses, which may be interpreted as firing rates, or as their logs (i.e., membrane potentials are Gaussian, and firing rates are exponentiated). From the perspective of coding efficiency and computation, Gaussian representations are appealing for a variety of reasons. First, among distributions with given covariance structure, Gaussian distributions have maximum entropy. Therefore, if metabolic demands are a function of the (co)variance of the response distribution, then the Gaussian distribution maximizes information transmission under metabolic constraints. Second, compression based on information theoretic objectives often reduces to linear projection when the data distribution is Gaussian , so Gaussianization facilitates downstream computation. In addition, the efficiency of the representation is preserved under orthogonal transformation . Finally, experiments have shown that single neurons in the fly early visual system adaptively Gaussianize their univariate responses , and neural populations in early sensory systems decorrelate their multivariate responses . Therefore, neural circuit models that nonlinearly transform signals to jointly Gaussianize their responses may offer normative, parsimonious explanations of nonlinear transformations in early sensory systems.

### Description of the input signal

We computed the responses of a local oriented filter , roughly matched to typical receptive field selectivity of neurons in primate visual cortex , applied to natural images from the Kodak dataset . These local filter responses are notorious for their sparse heavy-tailed statistical properties that can be well approximated by _generalized Gaussian_ distributions of the form \(p_{s}(s)(-|s/|^{})\), where \(\) is referred to as the _scale_ parameter and \(\) is referred to as the _shape_ parameter . Fig. 2AB shows example images and histograms of the local filter responses along with fitted generalized Gaussian distributions whose scale and shape parameters \((,)\) vary across images. While linear methods such as variance normalization are sufficient for rescaling the distribution, adaptive nonlinear transformations are required to reshape these heavy-tailed distributions.

Next, we generated 2-dimensional signals from pairs of the local filter responses for images at fixed horizontal spatial offsets ranging between \(d=2\) and \(d=64\). Contour plots (using kernel density

Figure 2: Marginal Gaussianization of local filter responses. **A)** Three example natural images from the Kodak dataset. **B)** Histograms of local filter responses (black lines) and fitted generalized Gaussian density (red dashed lines) with scale \(\) and shape exponent \(\). **C)** Learned interneuron activations \(gf(,z)\), with \(f(,z)\) defined as in eq. (6) and learned \(g\) and \(\), and **D)** corresponding stimulus-response transforms \(r=T(s)\). The optimal activations and transforms are shown as thick gray curves. **E)** Histograms of the circuit responses (black lines) and the Gaussian density (red dashed lines).

estimation) of the local filter response pairs and symmetric (or ZCA) whitened local filter response pairs for a natural image (specifically, the top-left image from Fig. 2) are shown for select \(d\) in Fig. 3AB. To quantify the statistical dependencies between coefficients, we estimated the mutual information between the pairs of coefficients after discretizing them into bins of width 0.5. In Fig. 4, we plot the estimated mutual information (using bin size 0.5) between the local filter response pairs (blue line) and ZCA whitened local filter response pairs (orange line) for spatial offsets between \(d=2\) and \(d=64\). Note that aside from \(d=2\), the linear ZCA whitening transform does not significantly reduce the mutual information between coordinates. (Similar results have been found when applying linear ICA transforms; see, e.g., [72, Figure 6].) Fig. 4 also suggests that ZCA whitening can even slightly increase the mutual information between coordinates--see also, rows \(d=8\) and \(d=32\) of Fig. 3AB--though these effects are quite small and may be a consequence of the discretization step when estimating mutual information.

### Choice of activation functions

How do we choose the family of activation functions \(\{f(,)\}\)? One approach is to choose a kernel that can approximate a general class of functions. An alternative approach, adopted here, which is motivated by the efficient coding hypothesis , is to choose a family of activation functions that is well matched to the marginal statistics of natural signals. In Fig. 2C, we plot examples of optimal activations for transforming local filter responses from different images (thick gray curves).

Since the marginals of the local filter responses are well-approximated by generalized Gaussian distributions , a sensible approach is to identify a family of interneuron activation functions that are optimal for transforming generalized Gaussian distributions with varying \((,)\) into the standard Gaussian distribution. When \(=0\), this implies (see Appx. B) that for each choice of scale \(\) and shape \(\), there is a gain \(g\) and parameter \(\) such that

\[gf(,)=F_{,}^{-1}(),\]

where \(()\) is the cdf of \((0,1)\). However, if we define \(f(,z)\) in terms of the above display, then we do not have a closed-form solution for \((,z)\) or \(_{}(,z)\), which are both required to implement

Figure 3: Joint Gaussianization of pairs of filter responses, spatially displaced by \(d=2,8,32\) pixels. Evenly spaced contours for the spherical Gaussian distribution are depicted as dashed red circles. Iso-probability contours for **(A)** the local filter responses, **(B)** ZCA whitened local filter responses, and **(CDE)** learned circuit responses (with \(K=2,3,4\) interneurons) are depicted (in the respective column) as black curves along with the estimated mutual information between coordinates. The learned column vectors of \(\) are indicated by the faint gray lines.

Alg. 1. Instead, we found that \(F^{-1}_{,}()\) can be well approximated by the simple algebraic form

\[f(,z)=a()z+b()(z)|z|^{},\] (6)

where \(a()\) and \(b()\) are specified nonnegative functions of \(>1\). Intuitively, the linear component shapes the marginal density locally around zero, while the higher-order monomial shapes the tails of the marginal distribution. In Appx. B, we show that the monomial activation \(f(,z)=(z)|z|^{}\) is optimal for Gaussianizing scalar signals whose marginal tail densities satisfy \(p_{s}(s)|s|^{q-1}(-|s|^{2q})\), where \(q=1/\). This closely resembles the tail densities of generalized Gaussian densities (when \(=1\)), suggesting monomial activations are effective for shaping the tails.

### Marginal density of local filter responses

We first apply Alg. 1 in the scalar setting \(N=K=1\) to demonstrate that our choice of activation function in eq. (6) is indeed well matched to the shape of heavy-tailed marginals of local filter responses. For each image, we ran Alg. 1 on the local filter responses with \(=0\), learning rates \((_{g},_{})=(10^{-5},10^{-5})\) and batch size 10 for \(10^{5}\) iterations. Fig. 2CD shows the learned interneuron activation functions \(gf(,)\) and the learned transforms \(T()\). Fig. 2E shows histograms of the circuit responses. Compared to the local filter responses, the circuit responses are visually much closer to Gaussian. We found that the circuit performs worse when the distribution \(p_{s}\) is more 'peaked' around zero (i.e., when \(s\) is sparser and the fitted shape parameter \(\) is smaller), as evidenced by the mismatch between the response distribution \(p_{r}\) and the Gaussian distribution \((0,1)\) near zero in the bottom row of Fig. 2E (see Appx. C for more examples). However, even in this case, the interneuron activation and circuit transform are close to optimal (Fig. 2CD, bottom row).

### Joint density of pairs of local filter responses

Next, we apply Alg. 1 in the multivariate setting \(N=2\), to pairs of spatially offset filter responses. For each image and offset, we ran Alg. 1 with \(K=2,3,4\) interneurons, \(=0\), learning rates \((_{g},_{},_{w})=(10^{-4},10^{-6},10^{-4})\), and batch size 10 for \(10^{6}\) iterations. Contour plots of the learned circuit responses for one image are shown in Fig. 3CDE; see Appx. C for more examples. The mutual information between circuit responses is shown in Fig. 4. We see that \(K=3\) interneurons significantly reduces the mutual information between circuit responses for spatial offsets less than \(d=32\). The reduction is much greater than obtained using \(K=2\) interneurons and about the same as obtained using \(K=4\) interneurons. For spatial offsets greater than \(d=32\), the raw filter responses already have low mutual information and the circuit does not not offer any significant reduction.

Figure 4: Estimated mutual information, with 95% confidence intervals (estimated across 23 images), for original signals (pairs of filter responses), the ZCA whitened signal, and learned circuit responses (with \(K=2,3,4\) interneurons).

Discussion

We derived a novel online algorithm for transforming a signal to approximate a target distribution, using a recurrent neural circuit with Hebbian synaptic plasticity, gain modulation, and adaptation of interneuron activation functions. The model draws inspiration from the extensive neuroscience literature on efficient coding , Hebbian synaptic plasticity , interneuron function  and gain modulation , proposing complementary roles for different physiological processes: Hebbian synaptic plasticity learns stimulus axes that are least matched to the target distribution, and interneurons adapt their gains and activation functions to transform the marginal responses along these axes. The form of the input-output function for the local interneurons--linear-nonlinear with gain modulation--closely resembles phenomenological models of neurons , and the parameters \(\), \(\), \(\) can potentially be fit to neural recordings and compared with the optimal parameters that can be derived from the signal statistics \(p_{}\). Furthermore, our model predicts a relationship between the interneuron activation function and the circuit transform; see Fig. 2CD for an example of an expansive interneuron activation function that corresponds to a compressive circuit transformation.

There are, however, some aspects of our circuit that are not biologically realistic. For example, our model focuses on the role of local interneurons in reshaping the response distribution and, for simplicity, assumes that the primary neurons have linear activation functions. A more realistic model would also include nonlinearity and adaptation in the primary neurons. Moreover, our model only includes synaptic connections between primary neurons and interneurons, which may be consistent with some sensory circuits (e.g., olfactory bulb), but cannot account for excitatory-excitatory connections or inhibitory-inhibitory connections in cortical circuits. Finally, the synaptic weights are not sign-constrained, and violate Dale's law. This can be addressed by modifying the objective in eq. 2 so that the optimization is over non-negative weight matrices \( 0\), which will result in a projected gradient step in Alg. 1; however, the circuit responses will generally be less aligned with the target distribution.

A limitation of our simulations is that we only test our method on two-dimensional inputs, demonstrating that three interneurons are sufficient to dramatically reduce the redundancy in the circuit responses. However, natural signals are generally high-dimensional and it is not clear how the number of interneurons required to effectively reduce redundancy will scale with the dimension of the signal. There is some basis for optimism. For example, visual inputs are highly structured--e.g., statistical dependencies between inputs rapidly decay with the distance between the inputs--so local interneurons only need to connect to neurons with overlapping or adjacent receptive fields, limiting the number of interneurons that are required as the dimensionality of the input signal grows .

There are a number of existing computational models that also explain how neural circuits can implement nonlinear transformations to efficiently encode their inputs. For example, several neural circuit models implement forms of divisive normalization [74; 75; 76], a transformation that is optimal for efficient encoding of natural signals [59; 38]. In addition, there is a body of work on normative spiking models derived from objectives that maximize the information encoded per spike [77; 78; 79], which can account for neural adaptation mechanisms such as gain control. Our work differs from these, by proposing a novel framing of sensory circuit computation in terms of transformations of probability distributions, which can be viewed as a population level version of the seminal work by Laughlin . We then demonstrate in a normative circuit model how interneurons can play a critical role in optimizing this objective by measuring the marginal distribution of circuit responses and adjusting their feedback accordingly.

Finally, our results may also be relevant beyond the biological setting. Gaussianization and normalizing flows are active areas of research [10; 80; 13]. We offer a novel continuous-learning solution inspired by neuroscience that learns using a combination of weight updates and activation function updates (related to trainable activation functions ). In low-dimensional settings, when the constraint functions are matched to the signal statistics, we show that Gaussianization can be achieved using relatively few parameters. It is of primary interest to understand how the methods introduced here scale to high-dimensional signals, where the curse of dimensionality presents significant challenges.