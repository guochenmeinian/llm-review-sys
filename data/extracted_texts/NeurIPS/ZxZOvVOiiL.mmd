# Multi-Armed Bandits with Network Interference

Abhineet Agarwal

Department of Statistics

UC Berkeley

aa3797@berkeley.edu

&Anish Agarwal

Department of IEOR

Columbia University

aa5194@columbia.edu

&Lorenzo Masoero

Amazon

mazon@amazon.com

&Justin Whitehouse

Computer Science Department

Carnegie Mellon University

jwhiteho@andrew.cmu.edu

The research presented in this paper was conducted independently and is entirely unrelated to the author's current appointment at Amazon.

###### Abstract

Online experimentation with interference is a common challenge in modern applications such as e-commerce and adaptive clinical trials in medicine. For example, in online marketplaces, the revenue of a good depends on discounts applied to competing goods. Statistical inference with interference is widely studied in the offline setting, but far less is known about how to adaptively assign treatments to minimize regret. We address this gap by studying a multi-armed bandit (MAB) problem where a learner (e-commerce platform) sequentially assigns one of possible \(\) actions (discounts) to \(N\) units (goods) over \(T\) rounds to minimize regret (maximize revenue). Unlike traditional MAB problems, the reward of each unit depends on the treatments assigned to other units, i.e., there is _interference_ across the underlying network of units. With \(\) actions and \(N\) units, minimizing regret is combinatorially difficult since the action space grows as \(^{N}\). To overcome this issue, we study a _sparse network interference_ model, where the reward of a unit is only affected by the treatments assigned to \(s\) neighboring units. We use tools from discrete Fourier analysis to develop a sparse linear representation of the unit-specific reward \(r_{n}:[]^{N}\), and propose simple, linear regression-based algorithms to minimize regret. Importantly, our algorithms achieve provably low regret both when the learner observes the interference neighborhood for all units and when it is unknown. This significantly generalizes other works on this topic which impose strict conditions on the strength of interference on a _known_ network, and also compare regret to a markedly weaker optimal action. Empirically, we corroborate our theoretical findings via numerical simulations.

## 1 Introduction

Online experimentation is an indispensable tool for modern decision-makers in settings ranging from e-commerce marketplaces (Li et al., 2016) to adaptive clinical trials in medicine (Durand et al., 2018). Despite the wide-spread use of online experimentation to assign treatments to units (e.g., individuals, subgroups, or goods), a significant challenge in these settings is that outcomes of one unit are often affected by treatments assigned to other units. That is, there is _interference_ across the underlying network of units. For example, in e-commerce, the revenue for a given good depends on discounts applied to related or competing goods. In medicine, an individual's risk of disease depends not only on their own vaccination status but also on that of others in their network.

Network interference often invalidates standard tools and algorithms for the design and analysis of experiments. While there has been significant work done to develop tools for statistical inference in the offline setting (see Section 2), this problem has mostly been unaddressed in the online learning setting. In this paper, we address this gap by studying the multi-armed bandit (MAB) problem with network interference. We consider the setting where a learner (online marketplace) assigns one of possible \(\) actions (varying discounts) to \(N\) units (goods) over \(T\) rounds to minimize average regret. In our setting, the reward of a unit \(n[N]:=\{1,,N\}\) depends on the actions assigned to other units.2 With \(N\) units and \(\) actions, achieving low regret is difficult since there are \(^{N}\) possible treatment assignments. Naively applying typical MAB methods such as the upper confidence bound (UCB) algorithm (Auer et al., 2002) leads to regret that scales as \(O(^{N}T})\), which can be prohibitively large due to the exponential dependence on \(N\). Further, without any assumptions on the interference pattern, regret scaling as \((^{N}T})\) is unavoidable due to lower bounds from the MAB literature (Lattimore and Szepesvari, 2020).

To overcome this issue, we consider a natural and widely-studied model of _sparse network interference_, where the reward \(r_{n}:[]^{N}\) for unit \(n\) is affected by the treatment assignment of at most \(s\) other units, i.e., neighbours. See Figure 1 for a visualization. Under this model, we provide algorithms that provably achieve low regret both when the learner observes the network (i.e., the learner knows the \(s\) neighbors for all units \(n\)), and when it is unknown. Our results allow for more general interference patterns and define regret with respect to a significantly stronger comparator policy than existing results in the literature.

**Contributions.**

1. For each unit \(n[N]\), we use the Fourier analysis of discrete functions to re-express its reward \(r_{n}:[]^{N}:\) as a linear function in the Fourier basis with coefficients \(_{n}^{^{N}}\). We show sparse network interference implies \(_{n}\) is \(^{s}\) sparse for all \(n[N]\). This sparse linear representation motivates a simple 'explore-then-commit' style algorithm that uniformly explores actions, then fits a linear model to estimate unit-specific rewards (i.e., \(_{n}\)).
2. With known interference (i.e., the learner knows the \(s\) neighbors for all \(n[N]\)), our algorithm exploits this knowledge to estimate \(r_{n}\) by performing ordinary least squares (OLS) locally (i.e., per unit) on the Fourier basis elements where \(_{n}\) is non-zero. Our analysis establishes regret \(((^{s}T)^{2/3})\) for this algorithm.
3. With unknown interference, we use the Lasso instead of OLS locally which adapts to sparsity of \(_{n}\) and establish regret \((N^{1/3}(^{s}T)^{2/3})\). We argue this \(T^{2/3}\) scaling cannot be improved.
4. Numerical simulations with network interference show our method outperforms baselines.

## 2 Related Work

**Causal inference and bandits with interference.** The problem of learning causal effects in the presence of cross-unit interference has received significant study from the causal inference com

Figure 1: A visual representation of sparse network interference. In this toy example, we have \(N=9\) units, and visualize the interference pattern. For unit \(2\) (orange), its outcomes are affected by the treatments of its neighbours (blue) \((2)=\{1,2,3,6,7\}\).

munity (see  for a thorough overview). Cross-unit interference violates basic assumptions for causal identifiability, invalidating standard designs and analyses.3 As a result, authors have developed methodologies for estimating causal effects under several models of interference such as intra-group interference , interference neighborhoods , in bipartite graphs representative of modern online markets , in panel data settings  as well as under a general model of interference, generally encoded via "exposure mappings" . Despite this large literature, there is much less work on learning with interference in online settings. Jia et al.  take an important step towards addressing this gap by studying MABs with network interference, but assume a known, grid-like interference pattern, where the strength of the interference decays as the \(_{1}\) distance between units grows. Moreover, their focus - unlike ours - is on establishing regret rates with respect to the best constant policy, i.e. the best policy that assigns each unit the same treatment. We also note that the authors consider a setting more closely aligned with the adversarial bandit literature, whereas the results in this paper are closer to those in the stochastic bandit literature. See Section 3 for a detailed description of these differences.

**Bandits with high-dimensional action spaces.** In MAB problems, regret is typically lower bounded by \(( T})\), where \(\#=^{N}\) in our setting. Typically, this curse of dimensionality is addressed by sparsity constraints on the rewards, where only a small fraction of actions have non-zero rewards . Particularly relevant to this paper is the work of Hao et al.  who consider sparse linear bandits. The authors utilize a "explore-then-commit" style algorithm to uniformly explore actions before using the Lasso to estimate the sparse linear parameter. We utilize a similar algorithm but allow for arbitrary interaction between neighboring units, instead using discrete Fourier analysis to linearly represent rewards . This is similar to kernel bandits , which assume there exists a feature map such that the rewards can be linearly represented (non-sparsely) in a high-dimensional reproducing kernel Hilbert space. Also related are stochastic combinatorial bandits , in which the action space is assumed to be a subset of \(\{0,1\}^{N}\) but rewards are typically inherently assumed to be linear in treatment assignments. That is, these works typically assume the reward \(r=,\) for \(\{0,1\}^{N}\), with valid actions \(\) often having at most \(s\) non-zero components. Our work (with \(=2\)), considers an arbitrary function \(r:\{0,1\}^{N}\), but explicitly constructs a feature map via discrete Fourier analysis such that rewards can be represented linearly.

## 3 Model & Background

In this section, we first describe the problem setting, and our notion of regret. Then, we introduce the requisite background on discrete Fourier analysis that we will use to motivate our algorithm and theoretical analysis. Last, we introduce the model that we study in this paper. Throughout this paper, we use boldface to represent vectors and matrices.

### Problem Set-up

We consider an agent that sequentially interacts with an environment consisting of \(N\) individual units over a series of \(T\) rounds. We index units \(n[N]\), and rounds \(t[T]\). At each time step \(t\), the agent simultaneously administers each unit \(n\) action (or treatment) \(a[]\). Let \(a_{nt}\) denote the treatment received by unit \(n\) at time step \(t\), and let \(_{t}=(a_{1t},,a_{Nt})[]^{N}\) denote the entire treatment vector. Each unit \(n\) possesses an unknown reward mapping \(r_{n}:[]^{N}\). Note that we allow the reward for a given unit \(n\) to depend on the treatments assigned to _all_ other units, i.e., we allow for cross-unit _interference_. After assigning a treatment to all units in round \(t\), the agent then observes the _noisy reward_ for unit \(n\) as \(R_{nt}=r_{n}(_{t})+_{nt}\). Denote the vector of observed rewards as \(_{t}:=(R_{1t} R_{Nt})\). We assume the following standard condition on the noise \(_{nt}\).

**Assumption 1**.: \((_{nt}:n[N],t[T])\) _is a collection of mutually independent 1-sub-Gaussian random variables._

**Regret.** To measure the performance of the learning agent, we define the average reward function \(:[]^{N}\) by \(()=_{n=1}^{N}r_{n}()\). Then, for a sequence of (potentially random) treatment assignments \(_{1}_{T}\), the regret at the horizon time \(T\) is defined as the quantity

\[_{T}=_{t=1}^{T}(^{*})-_{t=1}^{T}( _{t}),\] (1)

where \(^{*}_{[A]^{N}}()\). In Sections 4 and 5, we provide and analyse algorithms that achieve small regret with high probability.

**Comparison to other works.** Previous works studying network bandits such as Jia et al. (2024) measure regret with respect to the best constant action \(^{}:=_{a[]}(a)\) where \(^{N}\) denotes the all \(1\) vector of dimension \(N\). We compare regret to the optimal action \(^{*}_{[A]^{N}}()\), which is combinatorially more difficult to minimize since the policy space is exponentially larger (\(^{N}\) vs \(\)). Our setup is also different than the traditional MAB setting since the agent in this problem does not observe a single scalar reward, but one for each unit (similar to semi-bandit feedback in the combinatorial bandits literature (Cesa-Bianchi and Lugosi, 2012)). As we show later, this crucially allows us to exploit local, unit-specific information that allow for better regret rates.

### Background on Discrete Fourier Analysis

In this section, we provide background on discrete Fourier analysis, which we heavily employ in both our algorithm and analysis. Specifically, these Fourier-analytic tools provide a linear representation of the discrete unit-specific rewards \(r_{n}:[]^{N}\), which will allow us to leverage well-studied linear bandit algorithms. For the rest of paper, assume \(\) is a power of \(2\). If instead, if \(2^{}<<2^{+1}\) for some \( 0\), we can redundantly encode actions to obtain \(^{}=2^{+1}\) total treatments. As seen later, this encoding does not affect the overall regret.

**Boolean encoding of action space.** Since by assumption \(\) is a power of \(2\), every action \(a\ \ []\) can be uniquely represented as a binary number using \(_{2}()\) bits. Explicitly, let \(}(a)=(_{1}(a),_{_{2}( )}(a))\{0,1\}^{_{2}()}\) denote this vectorized binary representation. For ease of analysis, we use the Boolean representation instead \((a)=2}(a)-\{-1,1\}^{_{2}}\). For \([]^{N}\), define \(()=((a_{1}),,(a_{N}))\{-1,1 \}^{N_{2}()}\). Note each action \([]^{N}\) corresponds to a unique Boolean vector \(()\).

**Boolean representation of discrete functions.** Let \(=\{f:[]^{N}\}\) and \(_{}=\{:\{-1,1\}^{N_{2}()} \}\) be the collection of all real-values functions defined on the set \([]^{N}\) and \(\{-1,1\}^{N_{2}()}\) respectively. Since every \([]^{N}\) has a uniquely Boolean representation \(()\{-1,1\}^{N_{2}()}\), the set of functions \(\) can be naturally identified within \(_{}\). Specifically, any \(f\) can be identified with the function \(_{}\) by \(f()=(())\).

**Fourier series of Boolean functions.** This identification is key for our use since the space of Boolean functions admits a number of attractive properties.

_(1) Hilbert space._\(_{}\) forms a Hilbert space defined by the following inner product: for any \(h,g_{}\), \( h,g_{B}=^{-N}_{\{-1,1\}^{N_{2}( )}}h()g()\). This inner product induces the norm \( h,h_{B}\ \ \|h\|_{B}^{2}=^{-N}_{\{-1,1\}^{p}}h^{2}( )\).

_(2) Simple orthonormal basis._ For each subset \(S[N_{2}()]\), define a basis function \(_{S}()=_{i S}x_{i}\) where \(x_{i}\) is the \(i^{}\) coefficient of \(\{-1,1\}^{N_{2}()}\). One can verify that for any \(S[N_{2}()]\), \(\|_{S}\|_{B}=1\), and that \(_{S},_{S^{}}_{B}=0\) for any \(S^{} S\). Since \(|\{_{S}:S[N_{2}()]\}|=^{N}\), the functions \(_{S}\) are an orthonormal basis of \(_{}\). We refer to \(_{S}\) as the Fourier character for the subset \(S\).

_(3) Linear Fourier expansion of \(_{}\)._ Any \(h_{}\) can be expanded via the following Fourier decomposition: \(h()=_{S[N_{2}()]}_{S}_{S}( ),\) where the Fourier coefficient \(_{S}\) is given by \(_{S}= h,_{S}_{B}\). For \(h_{}\), we refer to \(_{h}=(_{S}:S[N_{2}()])^{ ^{N}}\) as the vector of Fourier coefficients associated with it. For \(\{-1,1\}^{N_{2}()}\), let \(()=(_{S}():S[N_{2}())]) ^{^{N}}\).

\(\{-1,1\}^{^{N}}\) be the vector of associated Fourier character outputs. For \([]^{N}\), abbreviate \(_{S}(())\) and \((())\) as \(_{S}()\) and \(()\) respectively.

### Model: Sparse Network Interference

The unit-specific reward function \(r_{n}:[]^{N}\) can be equivalently viewed as a real-valued Boolean function over the hypercube \(\{-1,1\}^{N_{2}()}\). That is, \(r_{n}\) takes as input a vector of actions \([]^{N}\), converts it to a Boolean vector \(()\{-1,1\}^{N_{2}()}\), and outputs a reward \(r_{n}()\). From the discussion in Section 3.2, we can represent unit \(n\)'s reward as \(r_{n}()=_{S[N_{2}()]}_{n,S}_{S} ()=_{n},()\), where \(_{n}=(_{n,S}:S N_{2}())^{^{N}}\) is a vector of Fourier coefficients.

Without any assumptions on the nature of the interference pattern, achieving low regret is impossible since it requires estimating \(^{N}\) Fourier coefficients per unit. To overcome this fundamental challenge, we impose a natural structure on the interference pattern which assumes that the reward \(r_{n}\) only depends on the the treatment assignment of a subset of \(s\) units. This assumption is often observed in practice, e.g., the revenue of a good does not depend on discounts applied to all other goods, but only those applied to similar or related ones.

**Assumption 2**.: _(Sparse Network Interference) For any unit \(n[N]\), there exists a neighborhood \((n)[N]\) of size \(|(n)| s\) such that \(r_{n}()=r_{n}()\) for all \(,\{-1,1\}^{N_{2}}\) satisfying \((a_{m}:m(n))=(b_{m}:m(n))\)._

We typically assume that \(n(n)\), i.e. unit \(n\)'s reward depends on its own treatment. This model allows for completely arbitrary interference between these \(s\) units, generalizing the results of Jia et al. (2024) who allow for interaction between all \(N\) units but assume the strength of interference decays with a particular notion of distance between units. Next, we show using our Fourier analytic tools, that Assumption 2 implies that the reward can be re-expressed as a sparse linear model. We prove the following in Appendix A.

**Proposition 3.1**.: _Let Assumption 2 hold. Then, for any unit \(n\), and action \([]^{N}\), we have the following representation of the reward \(r_{n}()=_{n},()\), where \(_{n}_{0}^{s}\).4_

Proposition 3.1 shows sparse network interference implies \(_{n}\) is \(^{s}\) sparse with non-zero coordinates corresponding to the interactions of treatments between units in \((n)\). Indeed, the Boolean encoding \((a)\) can be represented as blocks of \(_{2}()\) dimensional Boolean vectors:

\[()=()_{1:_{2}( )}}_{},,()_{(i-1)_{2}( )+1:i_{2}()}}_{},,()_{(N-1)_{2}( )+1:N_{2}()}}_{N}).\]

Unit \(n\)'s reward depends on a small collection of these blocks, those indexed by its neighbors. Define

\[(n):=i[N_{2}()]:i[(m-1)_{2}( )+1:m_{2}()]m(n)}.\]

\((n)\) contains the indices of \((a)\) corresponding to treatments of units \(m(n)\) and the non-zero entries of \(_{n}\) are indexed by subsets \(S(n)\). E.g., consider \(N=3\), \(=2\), with \((1)=\{1,2\}\). Then \((1)=\{1,2\}\) and \(S(n)=\{,\{1\},\{2\},\{1,2\}\}\), where \(\) is the empty set.

**Graphical interpretation.** Assumption 2 can be interpreted graphically as follows. Let \(=([N],)\) denote a _directed_ graph over the \(N\) units, where \([N][N]\) denotes the edges of \(\). For unit \(n\), we add to the edge set \(\) a directed edge \((n,m)\) for each \(m(n)\), thus justifying calling \((n)\) the _neighborhood_ of \(n\). That is, unit \(n\)'s reward is affected by the treatment of another unit \(m\) only if there is a directed edge from \(n\) to \(m\). See Figure 1 for an example of a network graph \(\).

## 4 Network Multi-Armed Bandits with Known Interference

We now present our algorithms and regret bounds when the interference pattern is known, i.e. the learner observes \(\) and knows \((n)\) for each unit \(n\). The unknown case is analysed in Section 5. Assuming knowledge of \(\) is reasonable in e-commerce, where the platform (learner) assigning discounts (treatments) to goods (units) understands the underlying similarity between goods.

Our algorithm 1 is a "explore-then-commit" style which operates in two phases. First, the learner assigns units treatments uniformly at random for \(E\) rounds, and observes rewards for each unit. In the second phase, the algorithm performs least squares regressions of the observed rewards against \(^{}(_{n})\) for each unit \(n\). This is because when \(\) is known, the learner knows the positions of the non-zero elements of \(_{n}\) which are precisely the subsets of \((n)\), Once the estimates \(}_{n}\) are obtained for each unit, they are aggregated to estimate the average reward for each action \([]^{N}\). In the remaining \(T-E\) rounds, the learner greedily plays the action with the highest estimated average reward.

Determining exploration length \(E\).Theoretically, we detail the length of \(E\) below to achieve low regret in Theorem 4.1. Practically, the learner can continue to explore and assess the error of the learnt \(}_{n}\) via cross-validation (CV). Once the CV error for all units falls below a (user-specified) threshold, commit to the action with highest average reward. We use this approach for selecting \(E\) in our simulations in Section 6.

### Regret Analysis

Here, we establish high-probability regret bounds of Algorithm 1 using \(O()\) notation. We prove the following in Appendix B.

**Theorem 4.1**.: _Suppose Assumptions 1 and 2 hold. For \(T=(A^{2s}[(2N/)+s()])\) and any failure probability \((0,1)\), Algorithm 1 run with \(E:=(T^{s})^{2/3}[()+s ()]^{1/3}\) satisfies_

\[_{T}=O([s(/)]^{1/3}(T ^{s})^{2/3}),\]

_with probability at least \(1-\)._

Establishing Theorem 4.1 requires trading-off the exploration time \(E\) to accurately estimate \(_{n}\) with the exploitation time. It also requires \(T\) to be large enough such that we can accurately estimate \(_{n}\). Next, we compare regret of Algorithm 1 to other methods, ignoring any dependencies on logarithmic factors to ease the discussion.

Comparison to other approaches.

1. _Naive MAB learner._ A naive learner who treats the entire network of units as a single multi-armed bandit system with \(^{N}\) actions will obtain regret \((^{N}})\). For sparse networks with \(s N\) and \(T^{N}\), our regret bound is significantly tighter.
2. _Global estimation._ An alternate algorithm would be to estimate Fourier coefficients \( 1/N_{i=1}^{N}_{n}\) of \(\) directly rather than estimate each \(_{n}\) (i.e., \(r_{n}\)) individually. That is, perform the least squares regression by compressing the observed, unit-specific rewards into \(_{t}:=N^{-1}_{n=1}^{N}R_{tn}\). An analysis similar to the one presented in Appendix B would yield rate of \((s^{1/3}(T^{s})^{2/3})\), which suffers an additional \(N^{2/3}\) cost as compared to Theorem 4.1.
3. _Jia et al._. Comparing regret to this work is difficult because they assume decaying interference strength on a grid-like network structure and establish regret only with respect to the best constant action, i.e., \(^{}:=_{a[]}(a)\). We also note that the framework of Jia et al.  is closer to that of adversarial bandits, whereas our framework is closer to that of stochastic bandits.

## 5 Network Multi-Armed Bandits with Unknown Interference

Next, we consider the case in which the underlying network \(\) governing interference is not known. We present Algorithm 2, which extends Algorithm 1 to account for the fact that the learner does not observe the network graph \(\) and thus does not know \((n)\) for all \(n\). Unknown network interference is common in medical trials, e.g., vaccine roll-outs where an individual's social network (i.e., \(\)) is unavailable to the learner.

```
1:Input: Time horizon \(T\), exploration steps \(E\), regularization parameter \(>0\)
2:Sample \(_{1},,_{E}_{}([ ]^{N})\)
3:Observe reward vectors \(_{t}=(R_{1t},,R_{Nt})\) for \(t[E]\), where \(R_{nt}=_{n},(_{t}) +_{nt}\).
4:Let \(=((_{i}):i[E])\{-1,1\}^ {E^{N}}\)
5:for\(n[N]\)do
6: Let \(_{n}:=(R_{n1},,R_{nE})\).
7: Set \(}_{n}:=_{ ^{A^{N}}}\{\|-_{n}\|_ {2}^{2}+\|\|_{1}\}\)
8:Set \(}:=N^{-1}_{n=1}^{N}}_{n}\).
9:Play \(}:=_{[]^{N}} {},()\) for the \(T-E\) remaining rounds. ```

**Algorithm 2** Network Explore-Then-Commit with Unknown Interference

Algorithm 2 is similar to Algorithm 1, but differs in how it learns \(_{n}\). Since \(\) is unknown, the learner cannot identify the Fourier characteristics which correspond to the non-zero elements of \(_{n}\). Therefore, we regress against the entire Fourier characteristic \(()\), using Lasso instead of ordinary least squares to adapt to the underlying sparsity of \(_{n}\). A similar CV approach, as discussed after Algorithm 1, can be used to determine both the exploration length \(E\), and regularization parameter \(\).

**Low-order interactions.** When \(^{N}\) is very large, the computational cost of running the Lasso can be large. Further, if the underlying network is indeed believed to be sparse, one can regress against all characteristics \(_{S}\) where \(|S| d\). A similar approach is explored in Yu et al. . In practice, one can choose degree \(d\) via CV.

**Partially observed network graph \(\).** In many settings, network interference graphs \(\) are partially observed. For example, on e-commerce platforms, interference patterns between established classes of goods is well-understood, but might be less so for newer products. Our framework can naturally be adapted to this setting by running Algorithm 1 on the observed portion of \(\), and Algorithm 2 on the unobserved graph. Specifically, if \((n)\) is observed for unit \(n\), replace the Lasso in line 7 of Algorithm 2 with OLS (i.e., line 8) in Algorithm 1.

### Regret Analysis

We now establish high-probability bounds on the regret for Algorithm 2 in Theorem 5.1. We prove the following in Appendix C.

**Theorem 5.1**.: _Suppose Assumptions 1 and 2 hold, and assume \(T=(A^{2s}[(N/)+N()])\). Then, with failure probability \((0,1)\), Algorithm 2 run with \(=4(2^{N})}+4( {})}\) where \(E:=(T^{s})^{2/3}[()+N( )]^{1/3}\) satisfies_

\[_{T}=O([N(/)]^{1/3}(T ^{s})^{2/3})\]We note the regret bound requires the horizon \(T\) to be sufficiently large in order to learn the network graph \(\) -- a necessary detail in order to ensure Lasso convergence. This is because the proof of Theorem 5.1 requires establishing that the matrix of Fourier coefficients for the sampled actions (i.e., design matrix \(\)) satisfies the the necessary regularity conditions to learn \(_{n}\) accurately. Specifically, we show that \(\) is incoherent, i.e., approximately orthogonal, with high probability. See Appendix C for a formal definition of incoherence, and Rigollet and Hutter (2023), Wainwright (2019) for a detailed study of the Lasso.

**Comparison to other approaches.** Algorithm 2 achieves the same dependence in \(,s,T\) as in the known interference case, but pays a factor of \(N^{1/3}\) as compared to \(s^{1/3}\). This additional cost which is logarithmic in the ambient dimension \(^{N}\) is typical in sparse online learning. This regret rate is still significantly lower than naive approaches that scale as \(O(^{N}T})\) when one assumes \(T\) is much smaller then \(^{N}\). Further, as argued before, estimating per-unit rewards (i.e., \(_{n}\)) results in lower regret as compared to directly estimating \(\) by a factor of \(N^{2/3}\).

**Dependence on horizon \(T\).** Generally, the dependence on \(T\) cannot be improved. Hao et al. (2020) lower bound regret for sparse linear bandits as \((( T)^{2/3})\), i.e., \(((^{*} T)^{2/3})\) in our setting. They show improved dependence on \(T\) can only be achieved under stronger assumptions on the size of non-zero coefficients of \(_{n}\).

## 6 Simulations

In this section, we perform simulations to empirically validate our algorithms and theoretical findings. We compare Algorithms 1 and 2 to UCB. We could not compare to Jia et al. (2024) since we did not find a public implementation. For our Algorithms, we choose all hyper-parameters via \(3\)-fold CV, and use the scikit-learn implementation of the Lasso. Code for our methods and experiments can be found at https://github.com/aagarwal1996/NetworkMAB. Our experimental setup and results are described below.

**Data Generating Process.** We generate interference patterns with varying number of units \(N\{5,,10\}\), and \(=2\). For each \(N\), we use \(s=4\). We generate rewards \(r_{n}=_{n},()\), where the non-zero elements of \(_{n}\) (i.e., \(_{n,S}\) for \(S_{n}\)) are drawn uniform from \(\). We normalize rewards so that they are contained in \(\), and add \(1\) sub-gaussian noise to sampled rewards. We measure regret as we vary \(T\), and set a max horizon of \(T_{}=10 2^{N}\) for each \(N\). Classical MAB algorithms need the horizon \(T\) to satisfy \(T>2^{N}\) since they first explore by pulling all \(2^{N}\) arms. We emphasize that these time horizons scaling as \(T=C^{N}\) are often unreasonable in practice, as even for \(=2\) and \(N=100\) there would already be \( 1.27\)e3 actions to explore. We include such large time horizons for the sake of making a complete comparison. Our methods circumvent the need for exponentially large exploration times by effectively exploiting sparsity.

**Results.** We plot the regret at the maximum horizon time as a function of \(N\), and the cumulative regret as we vary \(T\) for \(N=13\) in Figure 2 below. Our results are averaged over \(5\) repetitions, with shaded regions representing 1 standard deviation measured across repetitions. Algorithms 1 and 2 are denoted by Network MAB (Known) and Network MAB (Unknown) respectively. We discuss both sets of plot separately below.

_Regret Scaling with \(N\)._ We plot the cumulative regret when \(T=T_{}\) for \(N=9\) in Figure 2 (a). Classical MAB algorithms such as UCB see an exponential growth in the regret as \(N\) increases. Both Algorithm 1 and Algorithm 2 have much milder scaling with \(N\). Algorithm 1 uses \(\) to reduce the ambient dimension of the regression, hence suffering less dependence on \(N\) as compared to Algorithm 2.

_Regret Scaling with \(T\)._ We plot the cumulative regret for \(N=9\) in Figure 2 (b). Despite the poorer scaling of our regret bounds with \(T\), our algorithms lead to significantly better regret than UCB which takes a large horizon to converge. Algorithm 1 is able to end its exploration phase earlier than algorithm 2 since it does not need additional samples to learn the sparsity unlike the Lasso.

## 7 Conclusion

This paper introduces a framework for regret minimization in MABs with network interference, a ubiquitous problem in practice. We study this problem under a natural sparsity assumption on the interference pattern and provide simple algorithms both when the network graph is known and unknown. Our analysis establishes low regret for these algorithms and numerical simulations corroborate our theoretical findings. The results in this paper also significantly generalize previous works on MABs with network interference by allowing for arbitrary and unknown (neighbourhood) interference, as well as comparing to a combinatorially more difficult optimal policy. This paper also suggests future directions for research such as designing algorithms that achieve better dependence on \(T\) in the known graph setting. Establishing lower bounds to understand optimal algorithms will also be valuable future work. Further extensions could also include considering interference in contextual bandits or reinforcement learning problems. We also hope this work serves as a bridge between online learning and discrete Fourier analysis.