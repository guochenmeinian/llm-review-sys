# Learning Macroscopic Dynamics from Partial Microscopic Observations

Mengyi Chen1, Qianxiao Li1, 2

Department of Mathematics, National University of Singapore1,

Institute for Functional Intelligent Materials, National University of Singapore2

chenmengyi@u.nus.edu, qianxiao@nus.edu.sg

###### Abstract

Macroscopic observables of a system are of keen interest in real applications such as the design of novel materials. Current methods rely on microscopic trajectory simulations, where the forces on all microscopic coordinates need to be computed or measured. However, this can be computationally prohibitive for realistic systems. In this paper, we propose a method to learn macroscopic dynamics requiring only force computations on a subset of the microscopic coordinates. Our method relies on a sparsity assumption: the force on each microscopic coordinate relies only on a small number of other coordinates. The main idea of our approach is to map the training procedure on the macroscopic coordinates back to the microscopic coordinates, on which partial force computations can be used as stochastic estimation to update model parameters. We provide a theoretical justification of this under suitable conditions. We demonstrate the accuracy, force computation efficiency, and robustness of our method on learning macroscopic closure models from a variety of microscopic systems, including those modeled by partial differential equations or molecular dynamics simulations. Our code is available at https://github.com/MLDS-NUS/Learn-Partial.git.

## 1 Introduction

Macroscopic properties, including thestructural and dynamical properties, provide a way to describe and understand the collective behaviors of complex systems. In a wide range of real applications, researchers focus mainly on the macroscopic properties of a system, _e.g._, the viscosity and ionic diffusivity of liquid electrolytes for Li-ion batteries (Dajnowicz et al., 2022). Macroscopic observables usually depend on the whole microscopic system, _e.g._, the calculation of mean squared displacement requires all microscopic coordinates during the simulation. With growing simulation and experimental data, data-driven learning of macroscopic properties from microscopic observations has become an active area of research (Zhang et al., 2018; Wang et al., 2019; Husic et al., 2020; Lee et al., 2020; Fu et al., 2023; Chen et al., 2024).

Accurate calculation of macroscopic properties requires large-scale microscopic simulation. However, accurate force computations on all microscopic coordinates for large systems are extremely expensive (Jia et al., 2020; Musaelian et al., 2023). For example, in _ab initio_ molecular simulations, accurate forces need to be calculated from density functional theory (DFT). The computational cost of DFT limits its application to relatively small systems, typically ranging from a few hundred atoms to several thousand atoms, depending on the level of accuracy and computation resources (Hafner et al., 2006; Luo et al., 2020). This poses a dilemma: Accurate macroscopic properties are obtained from large-scale microscopic simulation, but the computation of forces on all the microscopic coordinates is extremely challenging.

To solve the dilemma, the corresponding question is: Can we still obtain accurate macroscopic observables even though only access to forces on a subset of the microscopic coordinates? In this work, we develop a method to learn the dynamics of the macroscopic observables directly, while only forces on a subset of the microscopic coordinates are needed. Efficient partial computation of microscopic forces relies on the sparsity assumption, where the computation cost of forces on a subset of microscopic coordinates does not scale with the microscopic system size. To learn the dynamics of the macroscopic observables, we first map the macroscopic dynamics back to the microscopic space, then compare it with the partial microscopic forces. Our key idea is summarized in Fig. 1.

Our main contributions are as follows:

* We develop a novel method that can learn the macroscopic dynamics from partial computation of the microscopic forces. Our method can significantly reduce the computational cost for force computations.
* We theoretically justify that forces on a subset of the microscopic coordinates can be used as stochastic estimation to update latent model parameters, even if the macroscopic observables depend on all the microscopic coordinates.
* We empirically validate the accuracy, force computation efficiency, and robustness of our method through a variety of microscopic dynamics and latent model structures.

## 2 Related Work

Learning from Partial ObservationsSeveral works have sought to learn dynamics from partially observed state \(}\) utilizing machine learning (Ruelle and Takens, 1971; Sauer et al., 1991; Takens, 2006; Ayed et al., 2019; Ouala et al., 2020; Huang et al., 2020; Schlaginhaufen et al., 2021; Lu et al., 2022; Stepaniants et al., 2023). For training, these methods reconstruct the unobserved state \(}\) first and model the dynamics of \(=(},})\). Our work assumes full state \(\) but partial forces \(\). Furthermore, we do not model the dynamics on state \(\) directly, but rather on the latent space because the dimension of \(\) would be extremely high for large systems.

Reduced Order ModelsBy modeling the dynamics in the latent space and then recovering the full states from them, reduced order models (ROMs) substitute expensive full order simulation with cheaper reduced order simulation (Schilders et al., 2008; Fresca et al., 2020; Lee and Carlberg, 2020; Hernandez et al., 2021; Fries et al., 2022).

Figure 1: Overview of our method. _Left._ Data generation workflow. For each configuration \(\), forces on a subset of all the microscopic coordinates are calculated by the microscopic force calculator. _Right._ Macroscopic dynamics identification. The macroscopic dynamics is mapped to the microscopic space first, then compared with the forces on a subset of the microscopic coordinates.

Our method can be thought to fall in the range of closure modeling. Unlike ROMs, we aim to model the dynamics of some given macroscopic observables directly and are not interested in recovering the microscopic states from the latent states.

Equation-free FrameworkThe equation-free framework (EFF) has sought to simulate the macroscopic dynamics efficiently (Kevrekidis et al., 2003; Samaey et al., 2006; Liu et al., 2015). The EFF is usually applied to partial differential equation (PDE) systems, and the macroscopic observables are chosen to be the solution of the PDE at the coarse spatial grid. In EFF, the macroscopic observables depend locally on the microscopic coordinates, allowing the macroscopic dynamics to be directly estimated from the microscopic simulations performed in small spatial domains. In contrast, the macroscopic observables may depend globally on the microscopic coordinates in our method, and the macroscopic dynamics may not be easily estimated from microscopic simulations performed in small spatial domains.

Another difference is that our method explicitly learns the macroscopic dynamics, while EFF can bypass explicit derivation of macroscopic evolution law by coupling microscale and macroscale dynamics. During simulation, EFF still requires microscopic simulation to be performed in small spatial domains and for short times, but our method can enable fast macroscopic simulation without requiring any microscopic simulation However, for systems where the macroscopic evolution equations conceptually exist but are not available in closed form, EFF can efficiently handle such cases, but the learned dynamics in our method may involve approximation or statistical errors that are often challenging to estimate.

## 3 Problem Setup

We consider a microscopic system consisting of \(n\) particles. Let the state of the microscopic system be \(=(_{1},,_{n})^{N}, _{i}^{m},N=mn\), where \(_{i}^{m}\) is some physical quantity associated with the \(i\)-th particle, such as the position and velocity. Assume the dynamics of the microscopic system can be characterized by an ordinary differential equation(ODE):

\[(t)}{t}=((t))\] (1)

where \(()=(_{1}(),,_{n}( ))^{N}\). We will call \(_{i}\) the _microscopic coordinate_ of the \(i\)-th particle and \(_{i}\) the _force_ acting on the microscopic coordinate \(_{i}\). In many real applications, we are interested in the dynamics of some macroscopic observables \(^{}=^{}()\). Here \(^{}\) is given beforehand and describes the functional dependence of \(^{}\) on \(\). For example, \(^{}\) can be chosen to be the instantaneous temperature or mean squared displacement in a Lennard-Jones system.

The goal is to learn the dynamics of \(^{}\) from microscopic simulation data. Existing methods that try to learn the macroscopic or latent dynamics require microscopic trajectories or forces on all the microscopic coordinates for training (Champion et al., 2019; Fries et al., 2022; Fu et al., 2023; Chen et al., 2024). The problem is: When the microscopic system size \(N\) is very large such that the force computations on all the microscopic coordinates are impossible, these methods are no longer applicable. Instead, our method aims to learn from partial computation of microscopic forces.

Consider we are given a microscopic force calculator \(\) for computation of partial microscopic forces. Let the microscopic coordinate \(\) be sampled from a distribution \(\). For each \(\), the microscopic force calculator \(\) will first sample an \(n\) dimensional random variable \(()=(_{1}(),,_{n}( ))_{}\{0,1\}^{n}\) according to a certain strategy. Next \(\) will calculate the corresponding _partial forces_\(_{()}(_{_{1}( )},,_{_{n}()})\). For notation simplicity sometimes we will simply write \(_{()}\) as \(_{}\). \(_{i}()\) indicate whether partial \(i\) is chosen to calculate the force or not. If particle \(i\) is chosen, \(_{i}()=1,_{_{i}()}=_{i}\), otherwise \(_{i}()=0,_{_{1}()}=\). We require the sampling strategy \(_{}\) to satisfy:

1. For each \(()_{}\), exactly \(n p\) items are equal to 1 and the rest are 0.
2. Each particle can be chosen with equal probability p, \((_{i}()=1)=p,(_{i}( )=0)=1-p,i=1,,n\).

This means that the microscopic force calculator \(\) can calculate forces on \(n p\) microscopic coordinates, and \(0<p<1\) limits the computation capacity of the microscopic force calculator \(\).

The above requirement is consistent with real applications since it is difficult to calculate all the microscopic forces due to computational cost. Furthermore, for efficient calculation of the partial microscopic forces, we will assume \(\) satisfies the following sparsity assumption:

**Assumption**: For a given error tolerance \(>0\), there exists a constant \(M n\), such that for any \(\) and \(i\{1,,n\}\), we can always find an index set \(J(_{i})\{i=1,,n\},|J(_{i})|<M\) which satisfies:

\[||_{i}(_{1},,_{n})-}_{i} (\{_{i}\}_{i J(_{i})})||_{2}<\] (2)

Intuitively, the assumption implies that the computational cost of force \(_{i}\) is independent of the microscopic system dimension \(N\). Thus our microscopic force calculator \(\) can compute partial forces in an efficient way. This assumption is prevalent in real-world applications. To better illustrate this, we give two examples here. The first example is about molecular dynamics. In molecular dynamics, each \(_{i}\) represents the position \(_{i}\) and velocities \(_{i}\) of the \(i\)-th atom, _i.e._, \(_{i}=(_{i},_{i})^{6}\). Then Eq. (1) becomes the Newton's law of motion:

\[_{i}}{t} =_{i}\] (3) \[_{i}}{t} =}_{i}(_{1},,_{ n}),\] (4)

It is common to limit the range of pairwise interactions to a cutoff distance (Allen et al., 2004; Zhou and Liu, 2022; Vollmayr-Lee, 2020). To calculate the force on an atom, we only need to consider its interaction with other atoms that are within the cutoff. The second example is about systems modeled by partial differential equation (PDE). We consider a time-dependent PDE and apply finite difference scheme to discretize the spatial derivatives. Then, the resulting semi-discretized equation takes the form of Eq. (1), and each \(_{i}\) is the value at the \(i\)-th grid. \(_{i}\) only depends on those grids that are used for finite difference approximation of the spatial derivatives.

Let the training data generated by the microscopic force calculator \(\) be \(\{^{i},_{(^{i})}\}_{i=1,,K}\). The data generation procedure is provided in Algorithm 1. We will introduce in the next section how we can learn the macroscopic dynamics from the training data with partial forces.

## 4 Method

Existing works for macroscopic dynamics identification consist of two parts: dimension reduction and macroscopic dynamics identification (Fu et al., 2023; Chen et al., 2024). We will follow these two parts. We start with most standard parts of closure modeling with an autoencoder, next, we turn to the main difficulty of macroscopic dynamics identification from partial forces.

### Autoencoder for Closure Modeling

We will use an autoencoder to find the closure \(}=}()\) to \(^{*}=^{*}()\) such that \(=(^{*},})\) forms a closed system. Here we define \(\) as forming a closed system if its dynamics \(}\) depends only on \(\), not any external variables. Note that in \(^{*}=^{*}()\), \(^{*}\) is determined beforehand and contains no trainable parameters. This ensures \(^{*}\) represents the desired macroscopic observables and remains unchanged during the training of the autoencoder.

Denote the encoder by \(=(^{*},})\) and the decoder by \(\), we will minimize the following reconstruction loss:

\[_{}=_{i=1}^{K}||^{i}- (^{i})||_{2}^{2}\] (5)

We also want \(^{}()^{}( )^{T}\) to be well-conditioned (see Section 4.2), then we impose constraints on the condition number of \(^{}()^{}( )^{T}\):

\[_{}=_{i=1}^{K}(^{}(^{i})^{}(^{i} )^{T})-1_{2}^{2}\] (6)

By enforcing \(^{}(^{i})^{}( ^{i})^{T}\) to be well-conditioned, we are also enforcing \(^{}(^{i})^{d N},d N\) to have full row rank, which will be used later. The overall loss to train the autoencoder is :

\[_{}=_{}+_{}_{},\] (7)\(_{}\) is a hyperparameter to adjust the ratio of \(_{}\) and is chosen to be quite small in our experiments, _e.g._, \(10^{-5}\) or \(10^{-6}\). The aim of training the decoder \(\) is to help the discovery of the closure variables. The decoder \(\) will not be used for further macroscopic dynamics identification. To facilitate comparison between models tainted with all and partial forces, we will train the autoencoder first and freeze it for macroscopic dynamics identification.

### Macroscopic Dynamics Identification

We now address the difficulty of learning from data with partial microscopic forces. Substitute \(=()\) into equation Eq. (1) and make use of chain rule, we get the dynamics of \(\):

\[}{t}=^{}() (),(0)=(_{0})\] (8)

here we use \(^{}()\) to denote the Jacobian of \(_{}()\) for notation simplicity. If the dynamics of \(\) is closed, the right-hand side of Eq. (8) will only depend on \(\), and we parametrize it using a neural network \(_{}()^{}() ()\). Since we are only interested in macroscopic dynamics identification, the loss would be naturally defined on the macroscopic coordinates:

\[_{}()=_{i=1}^{K}||^{}(^{i})(^{i})-_{ }(^{i})||^{2}\] (9)

Eq. (9) is used commonly in existing work (Champion et al., 2019; Fries et al., 2022; Bakarij et al., 2022; Park et al., 2024).

The main difficulty of \(_{}\) is that it includes the matrix-vector product \(^{}()()\). Note that the \(i\)-th entry of \(^{}()()\) can be written as \(_{j=1}^{n}^{}_{ij}()_{j}()\), and it is difficult to find an unbiased estimation of the \(i\)-th entry using a subset of \(\{_{j}()\}_{j=1,,N}\). Thus the accurate calculation of \(_{}\) requires the forces \(\{_{j}()\}_{j=1,,N}\) on all the microscopic coordinates.

The main idea of our method is to map the loss on the macroscopic coordinates back to the microscopic coordinates:

\[_{}()=_{i=1}^{K}||( ^{i})-(^{}(^{i}))^{}_{ }(^{i})||^{2}\] (10)

where \((^{}(^{i}))^{}^{N d}\) is the Moore-Penrose inverse. Since \(^{}(^{i})\) is of full row rank, \((^{}(^{i}))^{}\) is in fact the right inverse of \(^{}(^{i})\), _i.e._, \(^{}(^{i})(^{}(^{i}))^{}\) is an identity matrix. Below we will show our main theoretical result:

**Theorem 1**.: _Assume for any \(\), the eigenvalues of \(^{}()^{}()^{T}\) are lower bounded by \(b_{1}\) and upper bounded by \(b_{2}\), \(0<b_{1} b_{2}\). Then:_

\[b_{1}(_{}()+C)_{}( ) b_{2}(_{}()+C)\] (11)

_here \(C\) does not depend on \(\) hence does not affect the optimization._

The proof relies on singular value decomposition of \(^{}()\) and we provide the full proof in Appendix A.1. Theorem 1 states that by minimizing \(_{}()\), we are actually narrowing the range of \(_{}()\). Hence we want \(b_{1}\) and \(b_{2}\) to be as close as possible, this is the reason why we constrain the condition number of \(^{}()^{}()^{T}\) in Eq. (6). In the very special case where \(b_{1}=b_{2}\), minimizing \(_{}()\) is just equivalent to minimizing \(_{}()\).

Note that in loss \(_{}\), the term \(||()-(^{}())^{}_{}()||\) can be rewritten as \(_{j=1}^{n}||_{j}()-(^{}()) ^{}_{j}_{}()||\), and \(_{j()}||_{j}()-( {}^{}())^{}_{j}_{}()||\) can be regarded as its unbiased stochastic estimation. Then, we can introduce our loss defined for partial forces:

\[_{,p}()=_{i=1}^{K}||_{(^{i})}(^{i})-(^{}(^{i})) ^{}_{(^{i})}_{}(^{i})|| _{2}^{2}\] (12)

Here a constant \(1/p\) is multiplied to \(_{,p}\) to guarantee:

\[_{^{1},,^{K}}_{( ^{1}),,(^{K})}_{,p}( )=_{^{1},,^{K}}_{ }()\] (13)

We provide the full proof of Eq. (13) in Appendix A.2. By training with the model with \(_{,p}\), we can use data with partial forces as stochastic estimation to update model parameters. The full training procedure is provided in Algorithm 2.

Note that in Algorithm 1 during the data generation, \((x^{i})\) is also sampled from its distribution. Thus, \(_{,p}\) is deterministic once the samples \(\{^{i},_{(^{i})}(^{i})\}_{ i=1,,K}\) are generated. In the limit, the estimation is unbiased:

**Theorem 2** (informal).: _Let \(}_{}()=_{}(),^{*}_{}}_{ }()\), \(_{K,p}_{}_{,p}()\), then under certain conditions:_

\[}_{}(_{K,p})-}_{ }(^{*})0\] (14)

The proof utilized Rademacher complexity and a crucial assumption used in the proof is the uniform boundedness of \(_{,p}\). The formal version of Theorem 2 and the complete proof is provided in Appendix A.3. Theorem 2 theoretically justifies the expected risk \(}_{}(_{K,p})\) at the optimal parameter found by \(_{,p}\), converges to the optimal expected risk \(}_{}(^{*})\) as \(K\) goes to infinity.

## 5 Experiments

In this section, we experimentally validate the accuracy, force computation efficiency, and robustness through a variety of microscopic dynamics.

### Force Computation Efficiency

We first consider a one-dimensional spatiotemporal Predator-Prey system, mainly to validate the correctness and force computations efficiency of our method.

Predator-Prey SystemThe simplified form of the Predator-Prey system with diffusion (Murray, 2003) is

\[ =u(1-u-v)+Du}{ x^{2}}\] (15) \[ =av(u-b)+v}{ x^{2}}, x=[ 0,1], t[0,)\]

where \(u,v\) denote the dimensionless populations of the prey and predator respectively, \(a,b,D\) are three parameters. The complex dynamics of Predator-Prey interaction, including the pursuit of the predator and the evasion of the prey in ecosystems, can be described by Eq. (15).

We discretize the spatial domain of Eq. (15) into 50 uniform grids with \(x_{i}=(i-) x, x=0.02,1 i 50\). Let \((t)=(u(x_{1},t),,u(x_{50},t)),(t)=(v(x_{1},t), ,v(x_{50},t))\), then \(((t),(t))^{100}\) are treated as the microscopic states. After approximating the spatial derivatives in Eq. (15) with the finite difference method, we consider the semi-discrete equation which is an \(N=100\) dimensional ODE as our microscopic evolution law (see Appendix B.1).

We choose the macroscopic observable of interest to be \(^{*}=(,)\), the spatial average of the predator and the prey's population:

\[=_{i=1}^{50}u(x_{i},t),=_{i= 1}^{50}v(x_{i},t)\] (16)We find another 2 closure variables using the autoencoder, then the total dimension of the latent space \(\) is 4. We choose \(\) to be the trajectory distribution of the state \(\). For the data generation with partial forces, given \(\) we randomly choose forces on \(100 p\) microscopic coordinate. For example, if \(p=1/5\), then for each configuration, forces on 20 coordinates are calculated for training.

ResultsFig. 2 shows the results on the test dataset which consists of 100 trajectories (for more details, see Appendix Table 2). _For models trained with partial microscopic forces, we report the equivalent number of training data with full forces throughout the paper._ For example, for a model trained with \(_{,p}(p=1/5)\) on \(3 10^{3}\) data, we will report the number of training data to be \(3 10^{3} 0.2=600\). The test error is defined to be the mean relative error of the macroscopic observables between the ground truth and the predicted trajectories as in Appendix Eq. (43).

First, we observe that the mean relative error of all the models is around \(10^{-4}\) when the number of training data is large enough. This tells us that training with \(_{,p}\) is correct and accurate, which is consistent with Theorem 2. The predicted trajectories fit quite well with the ground truth trajectories (see Appendix Fig. 6 and Fig. 7).

We can conclude from Fig. 2 that, under the same number of training data, \(_{,p}\) with smaller \(p\)\((1/4,1/5)\) performs better. Similarly, to achieve the same performance, \(_{,p}\) with smaller \(p\) requires less training data. We set the error tolerance to be \(e_{}=3 10^{-3}\) and investigate how much training data is required to reach the error tolerance. In Fig. 2 the \(x\)-coordinate of the intersection point between the black dashed line and the other curves indicates the minimum data size required If we arrange each model according to their minimal required training data, then \(_{,p}(p=1/5)_{,p}(p=1/4)< _{,p}(p=1/2)<_{,p}(p=3/4)<_{}\). Model trained with \(_{,p}(p=1/4,1/5)\) requires less data to reach \(e_{}\), or equivalently, less force computations. This validates the _force computation efficiency_ of our method. One explanation could be that there are many redundant information in the forces acting on all the microscopic coordinates. By using partial microscopic forces, \(_{,p}\) can explore more configurations \(\) given the same size of training data, thus can make use of more useful information. Another observation from Fig. 2 is that as the training data size increases, the gap between models trained with different \(p\) narrows down. This is because as more data are provided, these data can contain almost all the information of the Predator-Prey system, thus more information will not lead to significant improvement.

### Robustness to Different Latent Structures

Having validated the correctness and force computation efficiency of our method, we are ready to apply our method to a variety of latent structures. We will tackle the Lennard-Jones system in this subsection, and validate the robustness of our method to different latent model structures. Three latent model structures are considered: MLP, OnsagerNet (Yu et al., 2021), GFINNs (Zhang et al., 2022). Both the OnsagerNet and GFINNs endow the latent dynamical model with certain thermodynamic structure to ensure stability and interpretability. The specific implementations of these two models are slightly different.

Lennard-Jones SystemThe Lennard-Jones system is widely used in molecular simulation to study phase transition, crystallization and macroscopic properties of a system (Hansen and Verlet, 1969; Bengtzelius, 1986; Lin et al., 2003; Luo et al., 2004). The Lennard-Jones potential describes the interaction between two atoms \(i\) and \(j\) through the potential of the following form:

\[V_{ij}(r)=\{ & 4_{ij}[(_{ij}/r)^{12}-( _{ij}/r)^{6}]&r r_{},\\ & 0&r>r_{}..\] (17)

Figure 2: Mean relative error on the test dataset of the Predator-Prey system. The black dashed line represents test error = \(3 10^{-3}\).

All the results in this experiment will be shown in reduced Lennard-Jones units. We consider a three-dimensional Lennard-Jones fluid with \(N_{}=800\) atoms of the same type. The microscopic state consists of the positions and velocities of the 800 atoms, thus the microscopic dimension is \(N=4800\). We simulate the Lennard-Jones system under NVE ensemble using LAMMPS (Thompson et al., 2022).

We choose the instantaneous temperature (\(T\)) as our macroscopic observables:

\[T=}-1)}_{i=1}^{N_{}}v_{i}^{2}}{2}\] (18)

here \(v_{i}\) is the velocity of the \(i\)-th atom and \(m_{i}=1\). We find another 31 closure variables using the autoencoder, then the latent dimension is 32. In our experiment, we also adopt the trajectory distribution of microscopic states for \(\). For data generation with partial forces, we choose \(p=1/16\). For each \(\) we randomly choose 50 atoms for force computations.

ResultsAll the models are trained with the same size of data. Fig. 3 shows the test error on 10 test trajectories. The test errors of using \(_{,p}(p=1/16)\) are relatively small (\( 10^{-3}\)), which validates the accuracy of our model on the Lennard-Jones system. It is easy to observe from Fig. 3 that for all the latent model structures, models trained with \(_{,p}\) can always outperform those trained with \(_{}\). This validates \(_{,p}\) is _robust over different latent model structures_.

### Robustness to Different Microscopic dynamics

We have already validated the accuracy and force computation efficiency of \(_{,p}\) on the Predatory-Prey system and the Lennard-Jones system, but their microscopic dimension is still not big enough. In this subsection, we focus on the robustness of our method to different systems including those with much larger microscopic dimension. We will consider two large systems: the Allen-Cahn system and a larger Lennard-Jones system with \(51200\) atoms.

Allen-Cahn SystemThe Allen-Cahn equation is widely used to model the phase transition process in binary mixtures (Allen and Cahn, 1979; Del Pino et al., 2008; Shen and Yang, 2010; Kim et al., 2021; Yang et al., 2023). We consider the 2-dimensional Allen-Cahn equation with zero Neumann boundary condition on a bounded domain:

\[_{t}v&=^{2}v-}F^{}(v)=\\ _{}v&=0 ,\] (19)

where \(v(-1 v 1)\) denotes the difference of the concentration of the two phases. \(F(v)\) is usually chosen to be the double potential taking the form of \(F(v)=(v^{2}-1)^{2}\). The Allen-Cahn equation is the \(L^{2}\) gradient flow of the free energy functional \((v)\) in Eq. (20) (Bartels, 2015).

\[(v)=_{}(}F(v)+\| v \|_{2}^{2})\,x\,y\] (20)

    & Micro dim \(N\) & Observables & Latent dim \(d\) & Partial labels \(p\) & \(_{}\) & \(_{,p}\) \\  Predator-Prey system & 100 & \(,\) & 4 & 1/5 & \(3.19 0.80 10^{-3}\) & **1.34**\( 0.18 10^{-3}\) \\ Allen-Cahn system & 4000 & free energy \((v)\) & 16 & 1/25 & \(6.93 10 10^{-3}\) & **3.98**\( 10.18 10^{-3}\) \\ Lennard-Jones system (small) & 4800 & temperature \(T\) & 32 & 1/16 & \(4.45 10 10^{-3}\) & **1.17**\( 10.18 10^{-3}\) \\ Lennard-Jones system (large) & 307200 & temperature \(T\) & 32 & 1/1024 & - & **4.96**\( 0.18 10^{-3}\) \\   

Table 1: Summary of the results on each system. Results of the Predator-Prey and Lennard-Jones (small) system are taken from Section 5.1, Section 5.2. For each system, \(_{}\) and \(_{,p}\) are trained with the same size of data.

Figure 3: Results on the Lennard-Jones system with 800 atoms and \(N=4800\). Forces on 50 atoms are used to train \(_{,p}\) for all the latent model structures. Each model is trained with ten repeats.

The free energy functional \(^{*}=(v)\) is a macroscopic observable of wide interest, hence we choose \((v)\) as our target macroscopic observable.

The spatial domain is discretized into \(200 200\) grids, then the dimension of the microscopic system is \(N=40000\). We find another \(31\) closure variables using the autoencoder hence the total dimension of the macroscopic system is 32, which is much smaller compared to the dimension of the microscopic system. We consider \(\) to be the trajectory distribution of \(\). We choose \(p=1/25\), each time the forces on 1600 grids are calculated for training \(_{,p}\).

Lennard-Jones System (large)To further demonstrate the capacity of our method, we scale up the Lennard-Jones system in Section 5.2 to encompass \(51200\) atoms, then \(N=307200\). The size of the simulation box is increased from \(10 10 10\) to \(40 40 40\) to keep the density unchanged.

ResultsFor a summary of the experiments and the results, we refer the reader to Table 1. Note that for the Lennard-Jones system (large), we still use the forces on \(50\) atoms for training, thus \(p=1/1024\). For the training of \(_{,p}(p=1/1024)\), only 5000 configurations with partial forces are used due to memory limit, which is equivalent to \(5000/1024 5\) training data with forces on all the atoms. Obviously, training data with size 5 is way too small, hence the results of \(_{}\) are not reported for this system.

From the results shown in Table 1, one can observe that for a variety of problems, including those modeled by partial differential equations or molecular dynamics simulations, \(_{,p}\) can always outperform \(_{}\). This shows the _robustness of our method to a variety of systems_. Moreover, the success of our method on the Lennard-Jones system (large) demonstrates the ability and efficiency of our method when scaled to very large systems.

We also compare the number of force computations that are required for models trained with \(_{}\) and \(_{,p}\) to reach test error \(e_{}=3 10^{-3}\). Fig. 4 shows the results on the Lennard-Jones system with different sizes. Lennard-Jones system with \(800,2700,6400,21600\) atoms are considered and the density is 0.8 for all the systems. The number of force computations here refers to the total number of forces on atoms that are used. For example, if \(_{,p}\) uses 100 configurations to train, and for each configuration, forces on 50 atoms are calculated, then the number of force computations would be \(100 50=5000\). From Fig. 4 we can observe that as the system size increases, the number of force computations required by \(_{}\) continues increasing. In the experiment, we find that the number of training data does not change a lot. The increase in the number of force computations is mainly due to the system size increases, then for each configuration, more force computations are required. However, the number of force computations even decreases a bit. One possible explanation is that, as the system size increases, the dynamics become less fluctuating and are easy to learn.

## 6 Conclusion

We present a framework for modeling the dynamics of macroscopic observables from partial computation of microscopic forces. We theoretically and experimentally demonstrate the accuracy, force computation efficiency, and robustness of our method through different problems. Finally, we apply our method to a very large Lennard-Jones system which contains \(51200\) atoms.

While our method can learn the macroscopic dynamics from partial computation of microscopic forces, it relies on the sparsity assumption. For systems that do not satisfy the sparsity assumption, the calculation of partial computation of microscopic forces is not efficient, thus it is not beneficial to learn from partial forces computation. For example, in the McKean-Vlasov system, the force on

Figure 4: Number of force computations required to achieve \(e_{}=3 10^{-3}\) on Lennard-Jones system with different sizes. Forces on 50 atoms are used to train \(_{,p}\) for systems of different sizes.

each microscopic coordinate depends on the collective behavior of all the other coordinates (Meleard, 1996).

Another limitation is the structure of the autoencoder. For particle systems such as the Lennard-Jones system, an ideal encoder should be permutation-invariant. Currently, we use MLP for the encoder, which can be improved. Additionally, our method assumes the microscopic state to be sampled from a distribution \(\). We choose \(\) to be trajectory distribution in the experiments, but in reality trajectory distribution of large systems may be impossible to obtain. Active learning is commonly applied to efficiently select microscopic configurations for training (Ang et al., 2021; Zhang et al., 2019; Farache et al., 2022; Kulichenko et al., 2023; Duschatko et al., 2024). It is of interest to combine active learning and our proposed method to overcome the difficulty of the choice of \(\). Furthermore, our method assumes the microscopic dynamics to be deterministic, another future direction could be generalizing our method to stochastic systems.

#### Acknowledgments

This research is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG3-RP-2022-028).