# Learning from Less: Bayesian Neural Networks for Optimization Proxy using Limited Labeled Data

Parikshit Pareek

Theoretical Division

Los Alamos National Laboratory

Los Alamos, NM 87545

pareek@lanl.gov

&Kaarthik Sundar

Information Systems & Modeling Group

Los Alamos National Laboratory

NM, USA 87545

kaarthik@lanl.gov

Deepjyoti Deka

MIT Energy Initiative

Massachusetts Institute of Technology

MA, USA - 02139

deepj87@mit.edu

&Sidhant Misra

Theoretical Division

Los Alamos National Laboratory

Los Alamos, NM 87545

sidhant@lanl.gov

Corresponding Author, Current Affiliation: Department of Electrical Engineering, Indian Institute of Technology Roorkee (IIT Roorkee), India. Email: pareek@ee.iitr.ac.in

###### Abstract

This work introduces a learning scheme using Bayesian Neural Networks (BNNs) to solve constrained optimization problems in a setting with limited labeled data and restricted model training time. We propose a Semi-Supervised BNN for this practical but complex regime wherein training commences in a sandwiched fashion, alternating between a supervised (using labeled data) learning step for minimizing cost, and an unsupervised (using unlabeled data) learning step for enforcing constraint feasibility. Both supervised and unsupervised steps use Bayesian approach where variational inference is used for approximate Bayesian inference. We show that the proposed Semi-supervised learning method outperforms conventional BNN and deep neural network (DNN) architectures for important non-convex constrained optimization problems from energy network operations, with 50% reduction in mean square error (MSE) along with halving of optimality and feasibility gaps without requiring correction or projection steps.

## 1 Introduction

Bayesian Neural Networks (BNNs) attempt to bring the advantages of Bayesian statistics into the function-approximating capabilities of deep neural networks (DNNs) and have found application in areas ranging from medical image segmentation to fluid dynamics [2; 7; 12; 4; 5]. Improvements in underlying algorithms for training and inference have led to better understanding of BNNs [8; 1; 13] and enabled their use as surrogates for Bayesian optimization . In recent years, DNNs have been applied to solve various optimization problems with physics-based constraints on variables, particularly in energy networks [21; 6; 3; 18; 14; 10]. Here, the primary motivation is to replace time-consuming optimization algorithms with ML proxies, enabling instantaneous solutions to problems on large number of instances. While promising in mimicking optimization solvers, they either rely on enormous labeled datasets to train ML models  or require time-consuming constraint correction steps within the framework [3; 6; 21]. We propose a novel BNN-based framework to learn optimization proxies with minimal labeled data and within training time constraints. LeveragingBNNs' ability to perform with limited data, the semi-supervised approach addresses the challenge of scarce labeled data in optimization problems with uncertainty. Initial results show that our method outperforms standard approaches in low-data regimes, avoids correction steps, and maintains fast prediction speeds, making it suitable for large number of instances.

## 2 Proposed Semi-supervised BNN Learning

Semi-supervised learning methods aim to leverage unlabeled data to improve the performance of ML algorithms under minimal amount of labeled data availability . Approaches in this area include augmenting unlabeled data with cheap pseudo-labels, developing an unsupervised loss function, and minimizing it with the supervised loss function[17; 20]. For example, data augmentation approach has been used before in the context of image classification using the notion of semantic similarity . However, this notion is not readily extensible to ML proxies for constrained optimization problems, where slight variations in input might lead to significant changes in output.

To circumvent aforementioned difficulty, we propose a feasibility-based data-augmentation scheme where feasibility relates to the constraints of the optimization problem. To the best of our knowledge, these ideas have not been explored in the context of BNN algorithms to solve large-scale optimization problems. Though not directly addressing this problem, one related work worth noting is that of loss function-based prior design  for output constraint satisfaction .

**Problem Setup:** We consider nonlinear constrained optimization problems having both equality \(g()\) and inequality constraints \(h()\), with decision \(\) and input \(\) variables as vectors.

\[_{} c()\] (1) s.t. \[g(,)=0 \] \[h(,) 0 \]

Furthermore, we assume that \(\), there exists at least one feasible solution for (1). The goal is to develop a BNN surrogate that provides an approximate optimal value of decision variables \(}_{t}\) for a given test input vector \(_{t}\). Let \(=\{(_{i},_{i}^{*})\}_{i=1}^{N}\) denote the labeled dataset where \(_{i}^{*}\) is obtained by solving the optimization problem (1) for \(_{i}\). We assume inexpensive sampling for input vector \(\) and construct the unlabeled data set \(^{u}=\{_{j}\}_{j=1}^{M}\).

**BNN Set-up and Training:** Mathematically, we denote the BNN as \(f_{w}()\), where \(w\) are the weights and biases that follow an isotropic normal prior \(p(w)\) with covariance \(^{2}I\).

The supervised part of the BNN training aims to compute the posterior distribution over the weights given labeled data \(\), and is expressed as: \(p(w|,) p(|,w)\,p(w)\) where \(p(|,w)\) is the likelihood of the labeled data \((,)\) given the weights, \(p(w)\) is the prior over the weights. The posterior distribution \(p(w|,)\) encapsulates the uncertainty about the weights after observing the labeled data. Due to the computational challenges of finding the normalization constant, approximate methods such as variational inference (VI)  are used to compute the posterior. For predictions, the posterior prediction is approximated as \(p(^{t}|^{t},)=_{p(w|)}[p(f_ {w}(^{t})]\). Moreover, we use Gaussian likelihood \(p(|,w)=_{i}(_{i}|f_{w}(_{i}),_{s}^{2})\) with \(_{s}^{2}\) being a parameter in VI, controlling the spread of Gaussian around the target values (noise variance) and \(_{i},_{i}\).

To effectively incorporate the unlabeled data \(^{u}\) into the learning process, it is necessary to define a suitable likelihood function. We propose to augment this unlabeled data using the necessary feasibility condition which vector \(\) must satisfy to be a solution of (1). Consider a function \((,)\) which measures the feasibility of a solution candidate \(\) for a given input \(\) such that one term measures the equality gap and other term measures one sided inequality gap or violations, with equal emphasis on both, as

\[(,)=,)\| ^{2}}_{}+[h(, )]\|^{2}}_{} \]

For any given feasible solution \(_{c}\)2, \((_{c},)=0\) for the given input. Under the consideration that for each input there exist a solution of (1), we can argue that for each input the feasibility gap function (4) has optimal value or true label of \(0\). We can augment the unlabeled dataset \(^{u}\) such that it becomes a labeled feasibility dataset i.e. \(^{f}=\{_{j},0\}_{j=1}^{M}\), 0. Now considering that input sampling is cheap, the construction of this labeled feasibility dataset has no additional computational cost. Similar to the supervised data, we can define a Gaussian likelihood for unsupervised training step as \(p(|,w)=_{j}(0|f_{w}( _{j}),_{j},_{u}^{2})\) with noise variance of unsupervised learning \(_{u}^{2}\) and \(_{j}^{f}\).

For obtaining optimization proxy, we parameterize the candidate solution \(f_{w}()\), using deep network architectures and use a sandwich style semi-supervised training for the BNN as shown in Figure 1.The idea is to alternatively use labeled dataset \(\) and augmented feasibility dataset \(^{f}\) for cost optimality and constraint feasibility respectively, to update network weights and biases. Further, the Bayesian inference step (_Sup_ and _UnSup_) is performed for a fixed number of iterations with total training time being constrained to \(T_{max}\). Finally, the prediction of mean estimate \(_{}\) and predictive variance estimate \(_{}\) is done using a unbiased Monte-carlo estimator via sampling 100 weights from the final weight posterior \(p_{W}^{m}\).

## 3 Numerical Results: AC Optimal Power Flow

To demonstrate the effectiveness of the proposed semi-supervised learning approach, we focus on the Alternating Current Optimal Power Flow (ACOPF) problem, a crucial decision-making task in electrical power systems. ACOPF aims to determine the least-cost generator set-points while adhering to the operational and physical constraints of the energy network. The problem's inputs are real and reactive power load vectors, and the outputs include generator set-points (real and reactive) and complex node voltages in polar form (magnitude and angle). Variations in the load vector constitute the input dataset \(\). Furthermore, the mathematical formulation of the ACOPF used in this study represents a non-convex optimization problem, as described in . Additionally, we utilize the publicly available dataset for the 57-Bus system from the DC3 repository , for comparative studies. Our neural network architecture has four sub-network of two hidden layers (100 neuron each) with ReLU activation function. These four sub-networks are trained to predict real power generation, reactive power generation, voltage magnitude and voltage angle outputs, separately without any overlap. The BNNs are trained using variational inference, utilizing Numpyro package while DNNs are trained (with MSE loss over labeled data) using Pytorch. All training-testing is performed using a Mac Pro machine with Apple M1 Max processor. We fix \(T_{s}=30\) sec. and \(T_{u}=50\) sec. for all Semi-supervised BNN learning instances, following Figure 1. Further, Figure 2 represents the performance of various models with different number of labeled data. All networks have same architecture and best BNN (and DNN) represents the results with hyperparameter optimization (like learning and decay rate). The semi-supervised method uses the best BNN hyperparameters, without any further optimization (details in Appendix A). It is clear that in low labeled data regime, both BNN and proposed Semi-supervised BNN outperforms the DNN approach in terms of MSE errors for various outputs. For feasibility, proposed Semi-supervised method outperforms BNN while DNN's mean equality gap (Eq. Gap) performance improves faster than other methods with increase in number of labeled training samples. This feasibility emphasizing behavior of standard DNN with MSE loss has also been noted in , with higher optimality gap as seen in Cost subfigure of Figure 2.

Figure 1: Flowchart of proposed Semi-supervised BNN learning. The _Sup_ block represents supervised learning stage with labeled dataset \(\) and _UnSup_ block represents unsupervised learning with augmented feasibility dataset \(^{f}\). Learning time upper limits are represented as \(T_{s}\), \(T_{u}\) and \(T_{max}\) for _Sup_, _UnSup_ and complete Semi-supervised BNN learning respectively.

Before presenting further comparisons, we discuss the significance of the numerical errors and the potential improvements in the ACOPF problem. The cost values in ACOPF problems are in USD and mean value of cost for 57-Bus test case is \(\$\,3.7 10^{4}\) or \(3.7\) in per-unit system. Therefore, a mean error of 0.02 in per-unit system will imply the different $200 across the testing instances. Further, in per-unit the voltage magnitude error requirement is below \(10^{-5}\) as it will be equivalent of error \(1\) Volt for a \(100\) kilo-Volt system. More importantly, our target is to reduce the error values lower than the least count of the measuring instrument placed in the system to measure these quantities. Moreover, a 0.01 mean equality gap means that on average, 1.0 Megawatt of power imbalance occurs at a node.

We compare the proposed method's performance with various supervised and semi-supervised methods from  in Table 1, considering the target error discussion. It is clear that proposed method of Semi-supervised learning outperforms DNN method in terms of optimality and feasibility. Further, the objective gap and feasibility gaps are comparable using proposed approach even without the correction step involved in other state-of-art methods, (from  and ) in Table 1. Implication of the absence of correction step can be seen in the testing time3, where the proposed approach and BNN have testing times similar to that of DNN while methods with correction step have one order of magnitude higher testing time. The reduction in testing time is crucial in the context of total time constrained situations which is the target application category for our BNN and Semi-supervised BNN based optimization proxies. The total time refers to the sum of the time required to obtain labeled dataset, training time and prediction time and is strictly limited in the case of ACOPF. The label generation time is reduced by using fewer supervised training samples and for the ACOPF, we constrain the training time to be \(T_{max}=1000\) sec 4. The testing or prediction time will also be required to be as low as possible because we want to predict the solution of the ACOPF problem for a very large number of input instances in a given short time. This is crucial because one of the major application of these optimization proxies is in computing probabilistic estimates and the number of instances we can predict in a given time, will directly affect the accuracy of these estimates.

## 4 Conclusion and Future Works

The proposed Semi-supervised BNN has shown promise in working with low labeled dataset for constrained optimization problems. A major limitation is the higher time requirement to perform Bayesian inference, limiting the size of unlabeled dataset which can be used. Future work will involve scaling of the proposed scheme to larger size optimization problems, improving optimality-feasibility learning connections between _Sup_ and _UnSup_ blocks and exploiting BNN's predictive variance information for active learning.

### Broader Impacts:

Improved solution of optimization problems will lead to more efficient resource utilization, benefiting industries by reducing costs and minimizing environmental impact. Further, improving ACOPF

Figure 2: Comparative performance of DNN, BNN, and the proposed semi-supervised learning method across various training set sizes, evaluated by mean square error (MSE) and mean gap. The gray strip highlights the key training data range of 500 to 1000 samples. The semi-supervised method utilizing 20,000 unlabeled samples in \(^{f}\), with a batch size of 1000 and \(T_{max}=\) 1000 seconds.

solution pipeline will directly help in combating climate change by optimizing the use of renewable energy and ensuring secure power grid operations .