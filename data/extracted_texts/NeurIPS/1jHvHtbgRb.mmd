# Task-Relevant Covariance from Manifold Capacity

Theory Improves Robustness in Deep Networks

William Yang\({}^{1,2}\) Chi-Ning Chou\({}^{1}\)  SueYeon Chung\({}^{1,3}\)

\({}^{1}\)Flatiron Institute, Center for Computational Neuroscience, New York, NY, 10010

\({}^{2}\)University of Cambridge, Department of Engineering, Cambridge, UK, CB2 1PZ

\({}^{3}\)New York University, Center for Neural Science, New York, NY, 10003

\({}^{*}\)Equal contribution

{wyang, cchou, schung}@flatironinstitute.org

###### Abstract

Analysis of high-dimensional representations in neuroscience and deep learning traditionally places equal importance on all points in a representation, potentially leading to significant information loss. Recent advances in _manifold capacity theory_ offer a principled framework for identifying the computationally relevant points on neural manifolds. In this work, we introduce the concept of _task-relevant class covariance_ to identify directions in representation-space supporting class discriminability. We demonstrate that scaling representations along these directions markedly improves simulated accuracy under distribution shift. Building on these insights, we propose AnchorBlocks, architectural modules that use task-relevant class covariance to align representations with a task-relevant eigenspace. By appending one AnchorBlock onto ResNet18, we achieve competitive performance in a standard domain adaptation benchmark (CIFAR-10C) against much larger robustness-promoting architectures. Our findings provide insight into neural population geometry and methods to interpret/build robust deep learning systems.

## 1 Introduction

In computational neuroscience and deep learning, geometric methods such as canonical correlation analysis (CCA) , representational similarity analysis (RSA)  and centered-kernel alignment (CKA)  have emerged as an effective approach for the analysis of high-dimensional neural representations. In such methods, point-cloud representations are often analyzed with a uniform prior, i.e., without explicitly privileging certain points over others. However, in their seminal work, Averbeck et al.  demonstrate how such uniformity assumptions on data points could lead to misestimation of information content relevant for downstream decoding. Overcoming this issue by optimally weighting data points is challenging in practice, which leaves us without a systematic framework for incorporating task-relevant information into the geometric analysis of neural representations.

_Manifold capacity theory_ is a data-driven framework for quantifying task-relevant structure in neural representations through the lens of downstream readout by perceptrons , and has found applications in both neuroscience  and machine learning . Manifold capacity generalizes the classic notion of storage capacity (e.g., Cover's theorem , Gardner's formula ) and quantifies the coding efficiency of neural representations in terms of the number of decodeable manifolds1 stored per neuron. At the core of the theory is a mathematical result relating manifold capacity to a statistical distribution of _anchor points_ (see Equation 2) which are the representative support vectors of each class manifold for a given decision boundary. Intuitively,the anchor distribution can be understood as forming a _task-relevant geometry space_ seen by an optimal linear classifier . In particular, the task-relevant geometry produces class covariance structures that differ from those of the original point-cloud. As a toy example, Figure 1a depicts spherical class manifolds globally arranged in a ring structure2. For each class as well as globally, the principal directions of covariance are uninformative. However, if the goal of downstream neurons is to distinguish classes, then the task-relevant directions (red) are tilted, and would differ between class manifolds. This intuition is reflected by the covariance structure of the class anchor points (yellow).

In this work, we apply these geometric insights to deep learning to investigate whether task-relevant class covariance from manifold capacity theory helps identify directions in representation-space useful for attenuating irrelevant or amplifying discriminative features. We focus on the setting of distribution shift, in which DNNs are vulnerable to even minor non-stationarities in input data, such as corruptions. We choose this setting due to extensive work, empirical and theoretical, attributing this vulnerability to a DNN's under-emphasis of auxillary class-invariant features, or over-emphasis on easily-degraded low-level features (see [22; 23; 24; 25; 26; 27; 28]). Using the standard CIFAR-10C domain adaptation task , we demonstrate the usefulness of task-relevant class covariance to identify linear transformations which improve robustness to distribution shift (see Figure 1b).

Our contributions can be summarized as follows:

* In Section 2.2, we formally introduce the concept of _task-relevant class covariance_, based on manifold capacity theory, as a way to identify directions in representation-space which support discriminability between classes.
* In Section 3.1, we find that manually augmenting ResNet18 representations with task-relevant class covariance improves simulated accuracy in domain adaptation on CIFAR-10C. In contrast, augmenting with the original point-cloud covariance is detrimental.
* In Section 3.2, we introduce _AnchorBlocks_, which are architectural modules designed to scale DNN representations along the eigenspace of task-relevant class-covariance matrices. We show that one AnchorBlock fine-tuned on a ResNet18 backbone achieves competitive domain adaptation on CIFAR-10C against larger robustness-promoting ResNet variants.

## 2 Methods

### Manifold capacity theory and anchor geometry

Given a dataset \(\{(_{i},y_{i})\}\) where \(_{i}\) is the \(i\)-th input data and \(y_{i}\) is its label, which comes from a finite discrete set \(\). We denote \((_{i})^{N}\) as the representation of \(_{i}\) (e.g. the hidden activation of a DNN) and \(N\) as the representation dimensionality. For each class \(y\), the corresponding manifold \(_{y}=(\{(_{i}):y_{i}=y\})\) is defined as the convex hull of the representations of the stimuli

Figure 1: Schematic illustration. **a**, Anchor points (yellow) forming the task-relevant class manifold produce covariances which meaningfully differ from the original point-cloud manifold. **b**, Illustration of manifold tangling due to distribution shift, and subsequent untangling via linear transformation.

with label \(y\). From manifold capacity theory [5; 6], the manifold capacity of \(()\) is denoted as \(\), which exhibits a closed-form formula depending only on the structure of the manifolds as follows:

\[^{-1}=}_{y()}}_{(0,I_{N})}[_{_{y^{ }}_{y^{}}_{y^{}} 0, y^{ }}(}(y,y^{}) _{y^{}}^{}_{y^{}}}{\|_{y^{ }}(y,y^{})_{y^{}}_{y^{}}\|_{2}} )_{+}^{2}]\] (1)

where \((y,y^{})=1\) if \(y=y^{}\) and \(-1\) otherwise, \(()\) is the uniform distribution of a finite set, vector \(\) is drawn from the multivariate Gaussian distribution \((0,I_{N})\), and \(\) are Lagrange multipliers enforcing constraints of the convex optimization problem:

\[_{V^{N}}\|V-\|_{2}^{2}\;:\;(y,y^{})V^{ } 0,\; y^{},\;_{y^ {}}\]

The term inside Eq. (1) has several interpretations, including the average error made by a random classifier. See  for further discussion and details. The key relevant observation is that Eq. (1) suggests a joint distribution over the manifolds \(\{_{y}\}_{y}\): \(\{_{y^{}}(y,)\}_{y^{}} _{}\) where \(y()\) and \((0,I_{N})\). As such, we have:

\[^{-1}=}_{\{_{y^{}}\}_{y^{} }_{}}[f(\{_{y^{}}\}_ {y^{}})]\] (2)

where \(f():_{y}_{y}_{ 0}\) is a simple function as defined in Eq. (1). This establishes an analytical connection between manifold capacity \(\) and the distribution of anchor points \(_{}\).

### Task-relevant covariance

Consider a dataset of representation-label pairs \(\{(_{i},y_{i})\}\) for a class \(y\). We define the _point-cloud class covariance_ as \(C_{y}:=_{_{y}(_{y})}[(_{y}-}_{y})(_{y}-}_{y})^{}]\), where \(_{y}=\{_{i}:y_{i}=y\}\) and \(}_{y}=_{_{y}(_{ y})}[_{y}]\) is the mean representation of class \(y\). In contrast, by focusing instead on the anchor points \(_{y}\) induced by Eq. (2), we define the _task-relevant class covariance_ of class \(y\) as:

\[C_{y}^{}:=}_{\{_{y^{}}\}_{y^{ }}_{}}[(_{y}-}_{y})(_{y}-}_{y})^{}]\,.\] (3)

### Experimental setup

Model.All DNN experiments use a publicly-available pretrained ResNet18  obtained from the Huggingface model repository  trained on CIFAR-10  (see Section A.1).

Data.CIFAR-10C is used to evaluate robustness, and extends CIFAR-10 by algorithmically applying 15 types of common corruptions at 5 severity levels (1-5), resulting in 75 different test sets. Corruptions belong to four broad categories: noise, blur, weather and digital transformations .

Robustness Metrics.Per convention in , for each corruption we benchmark the _Corruption Error_ (CE), the classification error rate, and _Mean Corruption Error_ (mCE) which averages error over all 75 corruptions. We also report _Clean Error_, which is the error rate on the CIFAR-10 test set.

## 3 Results

We demonstrate the application of task-relevant covariance in two settings. In Section 3.1, we show that scaling the representations of a class along the eigenspace of their task-relevant class covariance matrix causes all class examples to be correctly classified even under distribution shift. In Section 3.2, we incorporate this geometric insight into a trainable DNN architecture which achieves competitive robustness on CIFAR-10C.

### Representation Augmentation Experiment

If anchor geometry captures task-relevant class-specific features, then one prediction is that scaling representations along the eigenspace of their task-relevant class covariance should improve performance. To test this, we perform a controlled representation manipulation on pretrained ResNet18.

Implementation.Let \(H^{i}_{}^{N}\) be the representation of the \(i\)-th sample of the \(\)-th class on a test set, where \(i[M],\;[P]\). We first run pretrained ResNet18 in inference mode on the CIFAR-10 training set and extract activations used to compute a task-relevant covariance matrix for each class (see Eq. 3). We then run an inference pass on CIFAR-10C and augment each \(H^{i}_{}\) with the \(\)-th task-relevant class covariance matrix \(C^{}_{}\), which can be summarized as \(^{}_{}:=_{}C^{}_{}\) (see Algorithm 1). To test the goodness of this augmented representation, we pass it through the model's readout to calculate simulated accuracy (Figure 2). As an experimental control, we perform the same procedure using the \(\)-th point-cloud class covariance matrix from the CIFAR-10 training set.

Augmentation with task-relevant class covariance results in perfect simulated accuracy.Note that the manipulation used here is "label-aware", as the label determines which of the \(P\) task-relevant class covariance matrices to use for a given test example. We use the term "simulated accuracy" to indicate that it is a measure used for analysis rather than to represent naturalistic classifier performance. We found representations achieve perfect simulated accuracy when scaled in the eigenspace of their task-relevant class covariance. Meanwhile, when representations are scaled by the point-cloud class covariance, simulated accuracy is worse than baseline. This is consistent with our intuition that task-relevant class covariance captures directions in feature-space supporting discriminability, whereas point-cloud class covariance merely captures directions of high within-class variation.

### AnchorBlock

Although the procedure in Section 3.1 offers interesting insight relating task-relevant covariance to robustness, it is label-aware and thus would introduce a leakage if used in real-world settings. Here we introduce AnchorBlocks, architectural modules which allow models to learn to scale their representation by the correct task-relevant class covariance without label-assistance.

Architectural summary.We replace ResNet18's final layer with an AnchorBlock (see Figure (a)a). For a \(P\)-class problem, the AnchorBlock has \(P\) parallel _class covariance heads_ with weights as task-relevant class covariance matrices, followed by a shared linear readout. The representation \(^{N}\) is processed by all heads, yielding augmented vectors \(}_{i}\), \(i[P]\). Passing these through the shared readout \(W^{N P}\) gives \(_{i}=W^{}}_{i}\) for each \(i=1,,P\). By concatenating all \(_{i}\), we form \(Z=[_{1},,_{P}]^{P P}\). The prediction is \(}=((Z))^{P}\). For full architectural details see Section A.3.

Implementation.Before training, we obtain the task-relevant class covariance matrices of all \(P\) classes by running pre-trained ResNet18 on the CIFAR-10 training set in inference mode. The

Figure 2: Simulated accuracy of representations augmented with different covariance matricesweights of each head are set to a class covariance matrix and kept fixed by disabling gradients. Then, we replace the readout of ResNet18 with one AnchorBlock, and fine-tune the combined model on the CIFAR-10 training set, keeping the objective function the same. This architecture is label-unaware since the representation is scaled by all \(P\) task-relevant class covariance matrices in parallel.

**ResNet18+AnchorBlock achieves competitive performance compared to robust ResNet variants.** The task objective is to minimize error on CIFAR-10C using only CIFAR-10 training data. Following the convention of , we report the robustness of our classifier architecture using mCE (mean corruption error), which is simply the average classification error across all 75 CIFAR-10C test sets (see Figure 3b). The architectures we compare with are much larger, while DenseNet/RexNeXt implement enhanced feature aggregation - both of these properties purportedly promote robustness (see  for good discussion). In addition, our method also outperforms the combination of ResNeXt and TENET, an architectural modification designed to promote robustness and feature diversity .

## 4 Conclusion

In this work, we introduce the novel concept of _task-relevant covariance_ derived from the statistics of anchor points in manifold capacity theory, and propose that they identify directions in representation space supporting class discriminability. In Section 3.1, we use a manual representation augmentation experiment to test our prediction about these discriminability properties, and find that simple linear transformations which scale representations along the eigenspace of task-relevant class covariance matrices markedly improve simulated accuracy on CIFAR-10C. In Section 3.2, we use these insights to create AnchorBlocks, trainable architectural modules which perform these linear transformations in a label-unaware manner. We find that fine-tuning one AnchorBlock on a ResNet18 backbone achieves competitive performance on CIFAR-10C compared to larger robustness-promoting architectures.

Our approach suggests a geometric interpretation of distribution shift as data non-stationarity that causes greater manifold tangling, in which case simple linear transformations may improve the robustness of representations via manifold untangling (see Figure 1b). In this geometric interpretation, task-relevant class covariance is a way to identify such linear transformations based on the geometry space seen by a downstream classifier. In a broader sense, our work emphasizes the importance of considering task-relevant structure when analyzing high-dimensional representations, and also highlights the differences of viewing representational statistics from the lens of an ideal observer versus a downstream perceptron. This perspective may have significant implications for the analysis of representations in both computational neuroscience and deep learning.

Figure 3: Overview of AnchorBlock. **a,** Schematic of AnchorBlock architecture. **b,** Mean Corruption Error (mCE) of robust ResNet variants. The mCE values for WideResNet, DenseNet and ResNeXt29 are reported in , while mCE for ResNeXt29 + TENET is reported in .