# A Simple Remedy for Dataset Bias via Self-Influence:

A Mislabeled Sample Perspective

 Yeonsung Jung\({}^{1}\)

Jaeyun Song\({}^{1}\)

June Yong Yang\({}^{1}\)

Jin-Hwa Kim\({}^{2,3}\)

Sung-Yub Kim\({}^{1}\)

Eunho Yang\({}^{1,4}\)

\({}^{1}\)Korea Advanced Institute of Science and Technology, \({}^{2}\)NAVER AI Lab

\({}^{3}\)Seoul National University, \({}^{4}\)AITRICS

{ys.jung, mercery}@kaist.ac.kr\({}^{1}\)

###### Abstract

Learning generalized models from biased data is an important undertaking toward fairness in deep learning. To address this issue, recent studies attempt to identify and leverage bias-conflicting samples free from spurious correlations without prior knowledge of bias or an unbiased set. However, spurious correlation remains an ongoing challenge, primarily due to the difficulty in precisely detecting these samples. In this paper, inspired by the similarities between mislabeled samples and bias-conflicting samples, we approach this challenge from a novel perspective of mislabeled sample detection. Specifically, we delve into Influence Function, one of the standard methods for mislabeled sample detection, for identifying bias-conflicting samples and propose a simple yet effective remedy for biased models by leveraging them. Through comprehensive analysis and experiments on diverse datasets, we demonstrate that our new perspective can boost the precision of detection and rectify biased models effectively. Furthermore, our approach is complementary to existing methods, showing performance improvement even when applied to models that have already undergone recent debiasing techniques.

## 1 Introduction

Deep neural networks have demonstrated remarkable performance in various fields of machine learning tasks comparable to or superior to humans on well-curated benchmark datasets [7; 4; 60; 14]. Nevertheless, the efficacy of these models trained on unfiltered, real-world data remains an open question. In this scenario, a significant concern arises due to the presence of _dataset bias_, where task-irrelevant attributes are spuriously correlated with labels only in the training set. This can lead to models that rely on misleading correlations rather than learning the task-related features, resulting in biased models with poor generalization performance [62; 10].

To prevent models from learning detrimental bias, various methods are proposed to encourage models to prioritize learning task-relevant features. Recent studies enhance task-related features by first identifying bias-conflicting (unbiased) samples through loss [40; 36], gradients , or bias prediction techniques  during training, using an auxiliary biased model trained with Empirical Risk Minimization (ERM). Then, they amplify bias-conflicting samples by counteracting the bias-aligned (biased) samples through loss weighting  or weighted sampling . The effectiveness of such methods largely depends on their precision of bias-conflicting sample detection. Specifically, there is a risk of erroneously amplifying malignant bias instead of task-relevant features when bias-aligned samples are inaccurately identified as bias-conflicting. Due to the limited detection performance of previous methods [40; 36; 1; 35], it presents a crucial challenge that remains unresolved.

In this paper, we address this challenge from a novel perspective of mislabeled sample detection. Inspired by the similarities between mislabeled samples and bias-conflicting samples, we delve into Influence Functions (IF;), one of the standard methods for mislabeled sample detection [55; 57; 29], to identify bias-conflicting samples and propose a simple yet effective approach for biased models by leveraging them.

We first conduct a comprehensive analysis to explore the efficacy of Self-Influence (SI) , a variant of IF, in biased datasets. SI estimates how removing a specific training sample during training influences the prediction of the sample itself with the trained model (see Section. 2.2). By measuring SI, we can identify a minority sample that, if removed from the training set, increases the likelihood of incorrect predictions of itself by the trained model due to their discrepancies with the majority samples. In this context, leveraging SI to biased datasets is promising as bias-conflicting samples constitute the minority and contradict the dominant malignant bias learned by the model. However, we observe that unlike in mislabeled settings, directly applying SI to biased datasets is not as effective (Figure 1(a)-1(d)). Therefore, we investigate the differences between mislabeled samples and bias-conflicting samples and reveal the essential conditions for SI to effectively identify bias-conflicting samples. Note that we denote SI under found conditions as Bias-Conditioned Self-Influence (BCSI).

Building on our analysis, we propose a simple yet effective method for rectifying biased models through fine-tuning. We construct a small pivotal subset with a higher proportion of bias-conflicting samples using BCSI. While not perfect, this pivotal set can serve as an effective alternative to an unbiased set. Leveraging this pivotal set, we rectify a biased model through fine-tuning with only a few additional iterations. Extensive experiments demonstrate that our method can effectively rectify even after models are already debiased by recent methods.

Our contributions are threefold:

* We conduct a comprehensive analysis to explore the efficacy of SI in biased datasets and reveal the essential conditions for SI to accurately differentiate bias-conflicting samples, leading to Bias-Conditioned Self-Influence (BCSI).
* We propose a simple yet effective remedy through fine-tuning that utilizes a pivotal set constructed using BCSI to rectify biased models across varying bias severities.
* Our method is complementary to existing methods, capable of further rectifying models that have already undergone recent debiasing techniques.

## 2 Background

### Learning from biased data

We consider a supervised learning setting with training data \(D=\{z_{n}\}_{n=1}^{N}\) sampled from the data distribution \(:=(X,Y)\), where the input \(X\) is comprised of \(X=(S,B,O)\) where \(S\) is the task-related signal, \(B\) is a task-irrelevant bias, and \(O\) is the other task-independent feature. Also, \(Y\) is the target label of the task, where the label is \(y\{1,,C\}\). When the dataset is unbiased, ideally, a model learns to predict the target label using the task-relevant signal: \(P_{}(Y|X)=P_{}(Y|S,B,O)=P_{}(Y|S)\). However, when the dataset is biased, the task-irrelevant bias \(B\) is highly correlated with the task-relevant features \(S\) with probability \(p_{y}\), _i.e._, \(P(B=b_{y}|S=s_{y})=p_{y}\), where \(p_{y}\). Under this relationship, a data sample \(x=(s,b,o)\) is _bias-aligned_ if \((b=b_{y})(s=s_{y})\) and, _bias-conflicting_ otherwise, where \(\) denotes the logical conjunction. When \(B\) is easier to learn than \(S\) for a model, the model may discover a shortcut solution to the given task, learning to predict \(P_{}(Y|X)=P(Y|B)\) instead of \(P_{}(Y|X)=P(Y|S)\). However, debiasing a model inclines the model towards learning the true task-signal relationship \(P_{}(Y|X) P(Y|S)\).

### Influence Functions

Influence Function (IF; [11; 27]) estimates the impact of an individual sample from the training set on the model parameters, which in turn influences model predictions. A brute-force approach to assess the influence of a sample is to exclude the data point from the training set and retrain the model to compare differences in performance, referred to as leave-one-out (LOO) retaining. However, performing LOO retraining for all samples is computationally challenging; as an alternative, influence functions have been introduced as an efficient approximated method.

Here, we review the formal definition of influence function. Given a training dataset \(D=\{z_{n}\}_{n=1}^{N}\) where \(z_{n}=(x_{n},y_{n})\), model parameters \(\) are learned with a loss function \(\):

\[^{*}=*{argmin}_{}(D,)=* {argmin}_{}_{n=1}^{N}(z_{n},)\]

where \((z_{n},)=-(P_{}(y_{n}|x_{n}))\) is the cross-entropy loss for \(z_{n}\) with parameter \(\).

To measure the impact of a single training sample \(z\) on model parameters \(\), we consider the retrained parameter \(^{*}_{z,}\) obtained by up-weighting the loss of \(z\) by \(\):

\[^{*}_{z,}=*{argmin}_{}((D, )+(z,)).\]

Then, IF, the impact of \(z\) on another sample \(z^{}\), is defined as the deviation of the retrained loss \((z^{},^{*}_{z,})\) from the original loss \((z^{},^{*})\):

\[_{}(z,z^{})=(z^{},^{*}_{z,} )-(z^{},^{*})\]

For infinitesimally small \(\), we have

\[(z,z^{})=._{}(z,z^{})}{d }|_{=0}\ =_{}(z^{},^{*})^{}H^{-1}_{ }(z,^{*})\] (1)

where \(H:=_{}^{2}(D,^{*})^{P P}\) is the Hessian of the loss function with respect to the model parameters at \(^{*}\). Intuitively, the influence \((z,z^{})\) estimates the effect of \(z\) on \(z^{}\) through the learning process of the model parameters. Note that IF is commonly computed once a model has converged since Equation 1 approximates more accurately when the average gradient norm of the training set is sufficiently small.

Influence function also can be calculated on itself to measure the Self-influence of \(z\):

\[_{}(z)_{}(z,^{*})^{ }H^{-1}_{}(z,^{*}),\]

which approximates the difference in loss of \(z\) when \(z\) itself is excluded from the training set. This metric is commonly used for detecting mislabeled training samples in the noisy label setting [27; 51; 55; 29] or important samples in data pruning for efficient training [49; 59].

## 3 An analysis of Self-Influence in bias-conflicting sample detection

In this section, we conduct a comprehensive analysis to delve into the efficacy of SI in bias-conflicting sample detection. First, we examine the process of identifying bias-conflicting sample detection through the perspective of mislabeled sample detection (Section 3.1). Next, we introduce essential conditions required for SI to effectively identify bias-conflicting samples by analyzing the differences between mislabeled and bias-conflicting samples (Section 3.2). We term the SI calculated under these conditions as Bias-Conditioned Self-Influence (BCSI) and demonstrate that BCSI outperforms SI in detecting bias-conflicting samples.

Figure 1: Precision of detecting bias-conflicting samples among Loss, Gradient Norm, Influence function on training set (IFtrain), and Self-Influence (SI). The precision is evaluated with the ground truth number of bias-conflicting samples. The average precision of loss value, gradient norm, SI, and IF are presented in bars across three runs.

### Bias-conflicting sample detection from the perspective of mislabeled sample detection

IF is one of the standard methods for mislabeled sample detection . The use of influence functions for mislabeled sample detection generally involves two approaches: computing influence scores using a clean validation set or computing self-influence scores. The former, \((z_{i},)\), utilizes a validation set \(\) free of mislabeled samples to measure the impact on validation loss, identifying samples whose removal reduces this loss as likely mislabeled. The latter, Self-influence \((z_{i},z_{i})\), estimates how the loss of a sample \(z_{i}\) changes when it is removed from the training set. If removing a sample significantly increases its own loss, it indicates that the sample is likely mislabeled, as normal samples can still be correctly predicted using the remaining samples. For instance, in a task classifying dogs and cats, if a dog image is mislabeled as a cat, removing this mislabeled sample from the training set decreases the likelihood of correctly predicting it as a cat.

In this context, mislabeled samples and bias-conflicting samples share a key characteristic that both are minority samples contradicting the dominant features learned by the model. Mislabeled samples have incorrect labels that conflict with the learned features, making them easily identifiable through SI. Similarly, bias-conflicting samples contradict the malignant bias that a model learns from a biased dataset. Despite the different contexts, both types of samples can be detected through the same underlying principle of IF.

In summary, given the similarities between mislabeled samples and bias-conflicting samples, it is promising to leverage the perspective and methodology of mislabeled sample detection to identify bias-conflicting samples. However, in real-world scenarios, preemptively identifying malignant bias and constructing an unbiased validation set to mitigate the bias problem is impractical. Therefore, using self-influence offers a more feasible and practical solution for addressing bias-conflicting samples instead of using influence scores on a validation set. Consequently, we center our approach on SI to effectively detect bias-conflicting samples.

### Bias-Conditioned Self-Influence (BCSI)

To validate Self-Influence (SI) in detecting bias-conflicting samples, we conduct experiments on benchmark datasets with diverse bias types and severities: Colored MNIST, Corrupted CIFAR10, Biased FFHQ (BFFHQ), and Waterbird. These datasets feature bias related to color, synthetic corruption, gender, and place background, respectively (details in Appendix N.1). In contrast to the mislabeled setting, we observe that directly applying SI to detect bias-conflicting samples in biased datasets often fails. In Figure 1, the detection precision of SI is significantly low, mostly below 25%. Note that since an unbiased validation set is unavailable in our target problem, we additionally estimate the influence score on the training set, indicated as \(_{}\) in Figure 1.

Figure 2: The overview of our method. We compute Bias-Conditioned Self-Influence (BCSI) of the training data and construct a small but concentrated pivotal set with a high ratio of bias-conflicting samples. Then, we remedy biased models through fine-tuning that utilizes the pivotal set and remaining samples.

This is due to the inherent differences between mislabeled samples and bias-conflicting samples. While mislabeled samples strongly conflict with the dominant features learned by the model due to their incorrect labels, bias-conflicting samples share task-related features with bias-aligned samples. For instance, in a biased dataset where seagulls are spuriously correlated with sea backgrounds, a seagull image against a desert background still retains the features of a seagull. Despite the dominance of malignant bias, these features are still partially utilized. Therefore, bias-conflicting samples do not exhibit a clear contrast with the dominant features of a biased model, posing a challenge for using SI.

To address this challenge, we introduce essential conditions that enable SI to accurately detect bias-conflicting samples. The key concept is to restrict the model from learning task-related features and instead induce the model to focus more on the malignant bias to achieve better separation. A simple but effective way to attain this is by leveraging models in the early stages of training, since malignant bias is learned first, followed by task-related features later, according to Nam et al. . In Figure 3(a), experiments on CIFAR10C and CMNIST demonstrate that the classification accuracy of bias-aligned samples increases rapidly, while that of bias-conflicting samples shows a slower rise. In addition, as shown in Figure 3(b) and 3(c), our experiments on CIFAR10C with diverse ratios of bias-conflicting samples (0.5%, 1%, 2%, and 5%) demonstrate a significant decline in detection precision of IF and SI as training epochs increase, since the model gradually learns task-related features. Therefore, computing SI with models in the early stages of training can achieve better separation. Formally, given a model parameterized by \(\) at an early epoch \(t\), we compute the self-influence \(_{}(z)\) as:

\[_{}(z)=_{_{t}}(z,_{t})^{} H_{t}^{-1}_{_{t}}(z,_{t}),\] (2)

where \(H_{t}\) is the Hessian of the loss function at the parameter \(_{t}\).

To further enhance the separation of SI, we employ Generalized Cross Entropy (GCE)  to induce the model to focus more on the easier-to-learn bias-aligned samples, resulting in a more biased model. GCE emphasizes samples that are easier to learn, thereby amplifying the model's bias by tending to give more weight to bias-aligned samples in the training set.

Consequently, we employ the model trained under these conditions to measure SI and refer to SI estimated by this heavily biased model with Equation 2 as Bias-Conditioned Self-Influence (BCSI). Since we induce the model to heavily exploit bias and discourage the model from learning task-related features, BCSI can effectively detect bias-conflicting samples. To avoid the impracticality of manually searching epoch \(t\) for each dataset, we base our method on the well-known findings of Frankle et al.

Figure 4: Comparison of average precision between SI and BCSI across diverse datasets over three runs.

Figure 3: A comprehensive analysis of Influence function on the training set (IFtrain) and Self-Influence (SI) in biased datasets. Figure 3(a) shows the classification accuracy of bias-aligned and bias-conflicting samples over training epochs. Figure 3(b) and 3(c) depict the detection precision of IFtrain and SI across training epochs for varying ratios of bias-conflicting samples in CIFAR10C. Figure 3(d) shows histograms of sample distribution in CIFAR10C (1%) and each bar indicates the number of samples within a specific range.

 that the primary directions of the model's parameter weights are determined during 500 to 2,000 iterations. Thus, we set epoch \(t\) within this range according to the mini-batch size of each dataset. Specifically, we used \(t\)=5 for all datasets to ensure practicability and consistency across experiments, but fine-tuning the epoch \(t\) for each dataset can yield further improvement.

We validate the efficacy of BCSI in detecting bias-conflicting samples. Since calculating \(H^{-1}:=(_{}^{2}L(D,^{*}))^{-1}\) is computationally expensive for large networks due to their extensive number of parameters, we calculate \(H^{-1}\) and the loss gradient of the sample \(z\), \(_{}(z,^{*})\), by using the last layer of the model following Koh and Liang , Pruthi et al. . In Figure 4, BCSI outperforms conventional SI in detection precision.

Additionally, Figure 3(d) demonstrates that BCSI has a notable tendency for bias-conflicting samples to exhibit larger scores compared to bias-aligned samples, in contrast to SI. This trend is also observed in other biased datasets, as shown in Appendix A and C. These experimental results support that BCSI can serve as an effective indicator for identifying bias-conflicting samples. To further analyze the qualitative characteristics of bias-conflicting and bias-aligned samples within the top 100 samples ranked by BCSI, we examine BCSI on BFFHQ, as illustrated in Figure 5. In BFFHQ, gender serves as the bias attribute and age as the target attribute, leading to spurious correlations between 'young' and 'woman' as well as 'old' and'man'. For bias-conflicting samples, Figure 5(a) shows that BCSI assigns high scores to clear counterexamples, such as boys or very elderly women. In contrast, Figure 5(b) exhibits relatively lower BCSI scores for cases like slightly older young men or elderly women who appear younger, indicating that BCSI prioritizes samples with stronger opposition to spurious correlations. A similar trend is observed for bias-aligned samples in Figure 5(c) and Figure 5(d), enhancing that BCSI effectively distinguishes between varying degrees of alignment with the spurious correlations.

## 4 Remedy biased models through fine-tuning

In this section, we propose a simple but effective remedy that first utilizes BCSI to construct a concentrated pivotal subset abundant in bias-conflicting samples and then employs it for rectifying biased models via fine-tuning without leveraging the supervision of bias or an unbiased validation set. Our method is complementary to existing methods, capable of rectifying models that have already undergone other debiasing techniques. The overall pipeline is described in Figure 2.

Constructing a pivotal subset.We select the top-\(k\) subset of samples from each class, based on their BCSI scores, to form a pivotal subset of bias-conflicting samples as follows:

Figure 5: Example images from BFFHQ ranked within the top 100 by BCSI score. (a) and (b) are bias-conflicting samples with high and relatively lower BCSI scores, respectively. (c) is a bias-aligned sample with a high BCSI score, while (d) is a bias-aligned sample with a low BCSI score.

\(_{c=1}^{C}\{z_{rank}(n,c)}\}_{n=1}^{k}\), where \(C\) is the number of classes and \(rank}(n,c)\) is the dataset index of the \(n\)-th training sample of class \(c\) sorted by BCSI score. Due to the unknown ratio of bias-conflicting samples beforehand, determining a proper \(k\) through hyper-parameter tuning for each dataset is computationally expensive. To mitigate this, we repeat the selection process three times with different randomly initialized models and use the intersection of the resulting sets as the pivotal set. This ensures a high likelihood of selecting bias-conflicting samples, as they are consistently identified by three runs. We provide the resulting bias-conflicting ratios of the pivotal sets across various datasets in Appendix E and confirm that this process improves detection precision by 16.11% on average. Note that we set \(k=100\) across all datasets in our experiments. For computational costs, since we only train models for five epochs, this iterative approach incurs negligible cost compared to full training, as commonly done in previous studies . We confirm that in Appendix G. The detailed filtering process is outlined in Algorithm 1, which can be found in Appendix B.

Efficient remedy via fine-tuning.Recent works  show that retraining specific layers of a model using a small unbiased set can effectively mitigate bias in biased models, overcoming the inefficiency of retraining models from scratch . However, preemptively identifying the bias and curating an unbiased set is very costly, making it an impractical solution. Instead, our method leverages the pivotal set which has a high proportion of bias-conflicting samples, as a practical alternative. While not perfect, our method can efficiently remedy biased models with just a few additional training iterations without the need for prior knowledge of bias or unbiased datasets. As shown in Figure 6(a), even without a perfect pivotal set, its concentration facilitates its applicability in fine-tuning. Note that contrary to the claims of Krichenko et al. , we observed that in highly biased scenarios, the feature extractor also becomes biased, making last-layer retraining insufficient, as demonstrated in Figure 6(b). Therefore, we fine-tune all the parameters in the models.

In addition, we formulate a counterweight cross-entropy loss by drawing a mini-batch from the remaining training set. In real-world scenarios, the unpredictability of bias severity necessitates robustness across a wide range of bias severities. However, previous methods often assume a sufficient presence of bias-aligned samples in the training set, which limits their performance in low-bias scenarios. Despite its significance, the study on both low and high-bias scenarios has been underexplored, and to the best of our knowledge, we are the first to bring up this issue.

We then train the model using both the cross-entropy loss on the pivotal subset and the counterweight loss on the remaining training set:

\[(_{},_{}):=_{ }(_{})+_{}(_ {}),\]

where \(_{}\) is the pivotal subset, \(_{}_{}\) a randomly drawn mini-batch from the remaining training set, and \(_{}\) is the mean cross-entropy loss.

To this end, our method efficiently remedies bias through fine-tuning that utilizes a pivotal set constructed via BCSI across varying bias severities. Additionally, our approach complements existing methods, capable of further rectifying models that have already undergone recent debiasing techniques. The overall process is described in Algorithm 2, which is included in Appendix B.

Figure 6: Test accuracy under varying bias-conflicting ratios. Figure 6(a) shows the accuracy for last layer retraining across varying bias ratios in pivotal sets. Figure 6(b) depicts performance changes of last layer retraining and fine-tuning under diverse bias ratios. In Figure 6(c), our performance gains are provided. We present the average accuracy with the error bars indicating the standard error across three runs.

## 5 Experiments

In this section, we present experiments applying our method to models trained with ERM and recent debiasing methods. We validate our method and its individual components by following prior conventions. Below, we provide a brief overview of our experimental setting in Section 5.1, followed by empirical results presented in Section 5.2, 5.3, and 5.4.

### Experimental settings

We now describe datasets, baselines, and evaluation protocol. Detailed descriptions about these are provided in Appendix N.

Datasets.For a fair evaluation, we follow the conventions of using benchmark biased datasets . Colored MNIST dataset (CMNIST) is a synthetically modified MNIST , where the labels are correlated with colors. We conduct benchmarks on bias ratios of \(r\{0.5,0.1,0.2,5\}\). CIFAR10C is a synthetically modified CIFAR10  dataset with common corruptions. To evaluate our method in low-bias scenarios, we expand our scope and conduct experiments with varying bias ratios \(r\{0.5,0.1,0.2,5,20,30,50,70,90()\}\). Biased FFHQ (BFFHQ)  is a curated Flickr-Faces-HQ (FFHQ)  dataset, which consists of facial images where ages and genders exhibit spurious correlation. The Waterbirds dataset  consists of bird images, to classify bird types, but their backgrounds are correlated with bird types. Non-I.I.D. Image dataset with Contexts (NICO)  is a natural image dataset for out-of-distribution classification. We follow the setting of , inducing long-tailed bias proportions within each class, simulating diverse bias ratios in a single benchmark. Additionally, to demonstrate the effectiveness of our method on NLP datasets, we conduct experiments on CivilComments [2; 28] and MultiNLI [58; 45], as detailed in appendix F.

Baselines.Since our goal is addressing the dataset bias without leveraging any prior knowledge of bias or an unbiased set, we evaluate our method with such baselines. GroupDRO  uses bias supervision to debias models. LfF , BPA , and DCWP  adjust the loss function to amplify the learning signals for bias-conflicting samples. DFA  and SelecMix  augment samples possessing various biases different from the original data.

Evaluation protocol.Following other baselines, we calculate the accuracy for unbiased test sets in CMNIST, CIFAR10C, and NICO. We measure the minority-group accuracy in BFFHQ, and the worst-group accuracy in Waterbird. Note that we use the models from the final epoch for all

    &  &  &  & BFFHQ \\   & & &  &  &  &  &  &  &  \\   & **Info** & 0.5\% & 1\% & 2\% & 5\% & 0.5\% & 1\% & 2\% & 5\% & 0.5\% \\  GroupDRO & ✓ & 79.57 & 90.50 & 94.89 & 97.54 & 33.44 & 38.30 & 45.81 & 57.32 & 54.80 \\ ERM & ✗ & 71.76\({}_{ 1.84}\) & 86.47\({}_{ 0.61}\) & 93.87\({}_{ 0.32}\) & 96.28\({}_{ 0.29}\) & 20.50\({}_{ 0.54}\) & 24.91\({}_{ 0.33}\) & 28.99\({}_{ 0.42}\) & 40.24\({}_{ 0.28}\) & 53.53\({}_{ 2.05}\) \\ LfF & ✗ & 89.06\({}_{ 1.87}\) & 89.50\({}_{ 2.88}\) & 85.74\({}_{ 1.37}\) & 94.30\({}_{ 0.67}\) & 25.28\({}_{ 2.89}\) & 31.15\({}_{ 1.67}\) & 38.64\({}_{ 0.39}\) & 46.15\({}_{ 0.54}\) & 55.33\({}_{ 2.69}\) \\ DFA & ✗ & 84.71\({}_{ 1.66}\) & 90.20\({}_{ 1.29}\) & 92.31\({}_{ 0.77}\) & 94.33\({}_{ 1.23}\) & 27.13\({}_{ 1.66}\) & 31.26\({}_{ 2.71}\) & 37.96\({}_{ 0.71}\) & 44.99\({}_{ 0.84}\) & 52.07\({}_{ 1.91}\) \\ BPA & ✗ & 73.34\({}_{ 2.37}\) & 87.21\({}_{ 0.30}\) & 89.42\({}_{ 3.37}\) & 97.13\({}_{ 0.15}\) & 25.50\({}_{ 1.02}\) & 26.86\({}_{ 0.69}\) & 27.47\({}_{ 1.46}\) & 34.29\({}_{ 2.20}\) & 51.40\({}_{ 2.98}\) \\ DCWP & ✗ & 85.16\({}_{ 7.75}\) & 89.68\({}_{ 0.59}\) & 89.42\({}_{ 2.32}\) & 95.17\({}_{ 1.75}\) & 31.27\({}_{ 0.24}\) & 34.87\({}_{ 0.63}\) & 41.47\({}_{ 0.06}\) & 52.86\({}_{ 1.24}\) & 57.33\({}_{ 1.75}\) \\ SelecMix & ✗ & 84.46\({}_{ 0.58}\) & 94.51\({}_{ 0.53}\) & 95.75\({}_{ 1.34}\) & 98.09\({}_{ 0.13}\) & 37.63\({}_{ 0.81}\) & 40.14\({}_{ 0.42}\) & 47.54\({}_{ 0.59}\) & 54.86\({}_{ 0.76}\) & 63.07\({}_{ 2.32}\) \\  Ours EEM & ✗ & 75.87\({}_{ 1.60}\) & 89.69\({}_{ 0.41}\) & 95.08\({}_{ 0.17}\) & 96.79\({}_{ 0.13}\) & 26.61\({}_{ 0.38}\) & 33.47\({}_{ 0.29}\) & 40.75\({}_{ 0.37}\) & 49.30\({}_{ 0.46}\) & 56.00\({}_{ 1.07}\) \\ Ours LF & ✗ & **90.79\({}_{ 1.13}\)** & 94.10\({}_{ 0.18}\) & 92.95\({}_{ 1.17}\) & 95.59\({}_{ 0.55}\) & 27.63\({}_{ 1.00}\) & 35.29\({}_{ 1.21}\) & 43.36\({}_{ 0.78}\) & 51.95\({}_{ 0.29}\) & 57.13\({}_{ 2.46}\) \\ Ours DFA & ✗ & 88.39\({}_{ 0.28}\) & 92.85\({}_{ 0.067}\) & 95.67\({}_{ 0.12}\) & 97.52\({}_{ 0.06}\) & 25.66\({}_{ 0.85}\) & 33.53\({}_{ 0.21}\) & 42.80\({}_{ 0.81}\) & 52.61\({}_{ 0.54}\) & 56.60\({}_{ 2.83}\) \\ Ours SlecMix & ✗ & 87.63\({}_{ 1.20}\) & **95.35\({}_{ 0.17}\)** & **97.15\({}_{ 0.48}\)** & **98.13\({}_{ 0.17}\)** & **38.74\({}_{ 0.36}\)** & **46.18\({}_{ 0.33}\)** & **52.70\({}_{ 0.40}\)** & **59.66\({}_{ 0.31}\)** & **65.80\({}_{ 3.12}\)** \\   

Table 1: The average and the standard error over three runs. _Ours_ indicates our method applied to a model initially trained with the prefix method. The best accuracy is annotated in **bold. ✓** indicates that a given method uses bias information while ✗ denotes that a given model does not use any bias information.

[MISSING_PAGE_EMPTY:9]

to train detection models. In Figure 7(b), there is a slight performance decrease as \(k\) increases in CIFAR10C (0.5%). In contrast, the accuracy in CIFAR10C (5%) increases. Since there are a few bias-conflicting samples per class in CIFAR10C (0.5%), additional usage of samples dilutes the ratio of bias-conflicting data in the pivotal set, leading to a performance drop. In Figure 7(c), we observe a marginal accuracy drop as \(\) increases in CIFAR10C (0.5%), CIFAR10C (90%) experiences a performance increase. These results indicate that learning the remaining samples is beneficial in CIFAR10C (90%), fostering the model to capture task-relevant signals. For the number of epochs used to train the model for the detection, we compute the final performance when combining SelecMix in Figure 7(d). Except for insufficiently trained 1 epoch, the performance is not sensitive to the number of epochs between 3 and 11 epochs. We note that the analysis for intersections is provided in Appendix H.

## 6 Related work

Debiasing deep neural networks.Research on mitigating bias has centered on modulating task-related information and malignant bias during training. Early works relied on human knowledge through direct supervision or implicit information of bias , which is often impractical due to its acquisition cost. Thus, several studies have focused on identifying and utilizing bias-conflicting samples without relying on human knowledge. Loss modification methods  amplify the learning signals of (estimated) bias-conflicting samples by modifying the learning objective. Sampling methods  overcome dataset bias by sampling (estimated) bias-conflicting data more frequently. Data augmentation approaches  synthesize samples with various biases distinct from the inherent bias of the original data. Recently, based on the observation that bias in classification layers is severe compared to feature extractors, several approaches focus on rectifying the last layer . Similarly, Lee et al.  demonstrated that selectively fine-tuning a subset of the layers with an unbiased dataset can match or even surpass the performance of commonly used fine-tuning methods. However, identifying the bias and curating an unbiased set is very costly, making it an impractical essential condition.

Influence functions.Influence function (IF) and its approximations  have been utilized in various deep learning tasks by measuring the importance of training samples and the relationship between them. One application of IF is in quantifying memorization by self-influence, which is the increase in loss when a training sample is excluded . IF can be used to estimate the significance of samples, enabling the reduction of less important ones for efficient training . Recent works utilize IF to identify and relabel mislabeled samples in noisy label settings . Furthermore, IF has also been applied in 3D domains like NeRF, where it measures pixel-wise distraction caused by unexpected objects, aiding in the identification and mitigation of such distractions .

## 7 Conclusion

In this work, we introduce a novel perspective of mislabeled sample detection on biased datasets. By conducting a comprehensive analysis of Self-Influence in detecting bias-conflicting samples, we discover essential conditions required for SI to effectively identify these samples, which we denote as Bias-Conditioned Self-Influence (BCSI). Building on our analysis, we propose a simple yet effective remedy for biased models through fine-tuning that utilizes a small but concentrated pivotal set constructed via BCSI. Our method is not only capable of further rectifying models that have already undergone recent debiasing techniques but also demonstrates better generalization on a wide range of bias severities compared to previous studies.

Limitations.In this work, we rectify biased models via a simple fine-tuning approach. However, this is the basic method; more sophisticated techniques such as sample weighting or curriculum learning are possible. We believe that our introduction of this novel perspective will pave the way for more advanced future work.

Broader impact.Our work aims to learn unbiased deep learning models without bias annotations. Since filtering every training data under every given circumstance, the social impact of the ability to debias a biased deep learning model after its training is much needed in terms of fairness.