# Constrained Human-AI Cooperation: An Inclusive Embodied Social Intelligence Challenge

Weihua Du\({}^{}}\)\({}^{}\), Qiushi Lyu\({}^{}\)\({}^{}\), Jiaming Shan\({}^{}\), Zhenting Qi\({}^{}\), Hongxin Zhang\({}^{}\), Sunli Chen\({}^{}\)

**Andi Peng\({}^{}\), Tianmin Shu\({}^{}\), Kwonjoon Lee\({}^{}\), Behzad Dariush\({}^{}\), Chuang Gan\({}^{}\)\({}^{}\)**

\({}^{1}\) Carnegie Mellon University, \({}^{2}\) Peking University, \({}^{3}\) University of California, Santa Barbara

\({}^{4}\) Harvard University, \({}^{5}\) University of Massachusetts Amherst, \({}^{6}\) MIT

\({}^{7}\) Johns Hopkins University, \({}^{8}\) Honda Research Institute USA

\({}^{}\) weihuad@cs.cmu.edu, lvqiushi@stu.pku.edu.cn, chuangg@umass.edu

Equal Contribution

###### Abstract

We introduce Constrained Human-AI Cooperation (CHAIC), an inclusive embodied social intelligence challenge designed to test social perception and cooperation in embodied agents. In CHAIC, the goal is for an embodied agent equipped with egocentric observations to assist a human who may be operating under physical constraints--e.g., unable to reach high places or confined to a wheelchair--in performing common household or outdoor tasks as efficiently as possible. To achieve this, a successful helper must: (1) infer the human's intents and constraints by following the human and observing their behaviors (social perception), and (2) make a cooperative plan tailored to the human partner to solve the task as quickly as possible, working together as a team (cooperative planning). To benchmark this challenge, we create four new agents with real physical constraints and eight long-horizon tasks featuring both indoor and outdoor scenes with various constraints, emergency events, and potential risks. We benchmark planning- and learning-based baselines on the challenge and introduce a new method that leverages large language models and behavior modeling. Empirical evaluations demonstrate the effectiveness of our benchmark in enabling systematic assessment of key aspects of machine social intelligence. Our benchmark and code are publicly available at https://github.com/UMass-Foundation-Model/CHAIC.

## 1 Introduction

Humans possess a remarkable ability to observe, infer, and help others, even when others have different mental models and physical constraints in the world from themselves (Warnleken and Tomasello, 2006). From a young age, humans are able to watch other people attempt to perform a task, and if other people fail, they can develop plans of action that best assist them. In contrast, AI agents struggle to exhibit such basic social skills and fail to adjust their plans for the specific humans they wish to aid (Valmcekam et al., 2022; Ngo et al., 2022), rendering them poor personalized helpers.

For AI agents to best assist human partners in performing tasks in the real world, they must possess two fundamental capabilities: (1) contextual perception, i.e., the ability to follow and observe human behavior and identify the specific goals and constraints faced by each human; and (2) cooperative planning, i.e., the ability to plan actions that are best tailored to helping each human with different goals and constraints. While there have been some embodied benchmarks and environments designed to test general multi-agent intelligence (Puig et al., 2021, 2023; Gan et al., 2021), such efforts have largely excluded the unique accessibility challenges that real humans may possess in the world andneglect the differences among individuals. Moreover, outdoor scenarios and emergencies are also prevalent in human life, but receive little attention in the embodied intelligence community (Deitke et al., 2022).

This paper introduces the first large-scale embodied social intelligence challenge with accessibility explicitly in mind: Constrained Human-AI Cooperation (CHAIC). In this challenge, an embodied agent with egocentric visual observation must actively perceive and cooperate with a human partner possibly with physical constraints in a near photo- and physically realistic virtual environment to complete common household and outdoor tasks as efficiently as possible. This is motivated by the idea that people who need the most help from autonomous agents are those who are currently not explicitly accounted for in embodied intelligence frameworks. In CHAIC, a helper agent needs to follow and _observe_ the human partner to _infer_ their goals and constraints; then, the agent _plans_ a user-tailored strategy for aiding the human in efficiently performing tasks together; moreover, with the existence of unexpected emergencies, the agent needs to be reactive and adjust its strategy accordingly.

To create the challenge with accessibility in mind, we design and implement four new agents with real physical constraints that reflect the rich diversity of human partners in the real world. For example, a human partner confined to a wheelchair struggles to move past obstacles or a human partner struggles with heavy furniture when moving house in an outdoor scene, shown in Figure 1, and eight long-horizon tasks featuring both indoor and outdoor scenes on top of the ThreeDWorld (Gan et al., 2021), explicitly motivating the development of embodied agents that prioritize accessibility efforts when learning and planning and can thrive in rich scenarios.

We benchmark several baseline models, including planning- and learning-based agents, especially those powered by foundation models. We also introduce a new method for building agents that combines the behavior modeling capabilities of video models with the reasoning ability of large language models. Our benchmark results suggest that current baselines have difficulty modeling partner behaviors from raw RGB images, and LLM-driven agents are competitive agents in decision-making. We hope this new challenge will advance the study of social intelligence in embodied agents in complex scenarios including diverse human partners with constraints and rich indoor and outdoor scenes. This initiative calls on the community to develop and evaluate embodied agents with a strong emphasis on accessibility and inclusivity.

Our contributions include:

* We design and implement four new agents with real physical constraints and eight long-horizon tasks featuring both indoor and outdoor scenes on top of ThreeDWorld (Gan et al., 2021), simulating rich human constraints and scenarios in the real world.

Figure 1: **Constrained Human-AI Cooperation (CHAIC) for benchmarking embodied agents that socially perceive and assist human partners with physical constraints. Left: A human partner is confined to a wheelchair and struggles to move past an obstacle. The helper agent infers the human partner’s constraints and intents and assists him by removing the obstacle. Right: In a moving house scenario, after observing a human partner fail to lift heavy furniture, the helper agent understands her intents and constraints and assists her in carrying the furniture together.**

* We introduce a new embodied social intelligence challenge with accessibility explicitly in mind: Constrained Human-AI Cooperation (CHAIC), to test embodied agents' ability to actively perceive human partners' intents and constraints from egocentric visual observations and make user-tailored cooperative plans to help constrained human partners in rich scenarios.
* We benchmark several baseline models, including those powered by foundation models, especially a new agent with behavior modeling introduced by us, and conduct comprehensive analyses to identify and discuss the persisting challenges related to inter-agent perception and cooperation within complex environments.

## 2 Related Work

Embodied Multi-Agent Cooperation ChallengesOur benchmark and environment build on a rich history of realistic 3D simulated environments (Zhou et al., 2024; Li et al., 2023; Padmakumar et al., 2022; Kolve et al., 2017; Shridhar et al., 2020; Misra et al., 2018; Zhu et al., 2017; Xia et al., 2018; Savva et al., 2019; Xiang et al., 2020). Various tasks and methods have been introduced for multi-agent cooperation (Lowe et al., 2017; Samvelyan et al., 2019; Carroll et al., 2019; Suarez et al., 2019; Jaderberg et al., 2019; Amato et al., 2019; Baker et al., 2020; Bard et al., 2020; Jain et al., 2020; Puig et al., 2023b; Wen et al., 2022; Szot et al., 2023; Zhang et al., 2023, 2024). Specifically, Puig et al. (2021, 2023a) explored the inter-agent perception of isomorphic agents during household tasks. However, these works did not address the explicit challenge of actively perceiving diverse human partners with physical constraints from visual observations and adapting the cooperation strategy accordingly. In contrast, our challenge is designed explicitly not only to study the social perception of the partner's goals and constraints from visual observations but also to capture the nuances of human physical mobility constraints that might impair the successful completion of such tasks. A contemporary work (Cao et al., 2024) also studies assistive agents for vulnerable groups but focuses only on indoor scenarios with oracle symbolic observations. In contrast, our proposed CHAIC Challenge features both indoor and outdoor scenarios, an egocentric visual observation, newly created physically constrained agents, and unexpected events, enabling rich, physics-driven interactions on real-world assistive tasks.

Accessibility in AI DesignPeople with disabilities or physical impairments are a central focus area of study in robotics, including care for wheelchair users, elderly users, and users with aging-related ailments like dementia (de Saille et al., 2022; Sundaresan et al., 2022; Broadbent et al., 2009; Benda et al., 2020; Cooper et al., 2016; Lee et al., 2017). These works often study the best ways to design for inclusivity; in other words, how to best build assistive robots to handle the _explicit_ physical needs of the users in question (Benda et al., 2020). We build on these design principles to create the first-ever large-scale embodied intelligence environment that explicitly models such impairments.

## 3 The Constrained Human-AI Cooperation (CHAIC) Challenge

**The Constrained Human-AI Cooperation (CHAIC) Challenge** seeks to study how embodied agents perform in terms of social perceptions of human partners with diverse physical constraints and cooperative planning abilities within rich scenarios. Built on top of ThreeDWorld, a realistic 3D embodied AI platform, we design and implement four new agents with real physical constraints (Section 3.1) and eight tasks featuring both indoor and outdoor scenes, including emergencies (Section 3.2). For each task, there is a **constrained agent** mimicking a human partner with capability constraints, trying to find and transport some target objects to a specific goal location, and a **helper agent** trying to infer the constrained agent's goal and capability constraints through active perception of its behaviors to assist the constrained agent better. The success of the helper agent is measured by the ratio of target objects successfully transported by both of them. Figure 2 provides an overview of the challenge, with further details in Section 3.3.

### Constrained Agents

To enable the testing of embodied social intelligence with a diverse set of potential human partners, we have created four new simulated agents that may face physical constraints such as limited height, strength, and movement speed, reflective of real humans.

Each agent possesses two properties: _reaching range_ and _strength_. An agent can successfully interact with objects whose heights are within its _reaching range_ and whose weights are lighter than its _strength_ limit. When an agent attempts an action that exceeds its capabilities, the action does not fail immediately but instead has a success rate. This rate is calculated using the formula \((-/)/\), where \(\) represents the excess amount, and \(\) and \(\) are constants. If an action exceeds multiple capability thresholds, the probabilities of success are multiplied. We have developed the following constrained agents:

* **Child Agent:** A small child with a height of 1.2 m that has a _reaching range_ of \([0,1.5]\) m.
* **Wheelchair Agent:** An agent confined to a wheelchair or limping that may be blocked by obstacles in the house (e.g., a couch). Its _reaching range_ is \([0.25,1.5]\) m.
* **Bicycle Agent:** An agent walking with a bike that moves slowly. It must first dock the bike when picking up an object. The child accompanying it may run away, causing an emergency.
* **Frail Agent:** An agent that is less capable of lifting heavy objects (e.g., furniture) and has only \(1/6\) the _strength_ of a normal agent.

### Tasks with Constrained Agents

We designed eight tasks featuring indoor and outdoor scenes, including emergencies, in our CHAIC benchmark, utilizing the various constrained agents introduced earlier. Information about each task is shown in Table 1.

### Challenge Details

In CHAIC, an embodied helper agent \(A_{h}\) is tasked to infer the goal \(G\) and the constraints of a constrained agent \(A_{m}\) and assist \(A_{m}\) in finding and transporting a set of target objects \(O_{t}\) from random locations to a goal location \(L_{g}\). There are containers scattered in the environment, which the agents can use to transport more objects simultaneously. An agent could take two objects at a time without a container, and the capacity of a container is set to three.

Formally, a task in the challenge is defined by the goal \(G\) of the constrained agent \(A_{m}\) (i.e., a set of goal predicates describing the final desired state) and an initial environment \(E\) where the helper agent \(A_{h}\) is placed alongside the constrained agent \(A_{m}\) to complete the task. The ground truth goals and constraints of the constrained agent are hidden from the helper agent \(A_{h}\), thereby explicitly motivating the need for active perception for the agent to infer intents and constraints.

Figure 2: **Overview of CHAIC challenge:** We present four agents with diverse capability constraints and eight tasks built around these constraints, featuring both indoor and outdoor scenarios. The tasks are named _no constraint_, _high container_, _high goal location_, _high target object_, _obstacle_, _low target object_, _shopping_, and _moving house_ (four of them are shown on the left). In each task, there are _objects_, _containers_, and a _goal location_. A helper agent needs to infer the partner’s intents and constraints from its egocentric observations (shown on the right) and make a tailored plan to assist the partner in transporting the intended target objects to the goal location using containers as tools.

Observation SpaceIn CHAIC, actions may take several frames to finish and are executed asynchronously between agents. The agent will receive the following observation after its action is finished or failed:

* **Egocentric RGB-D Image**: The agent receives \(512 512\) egocentric color and depth images, as shown in Figure 2.
* **Self-State**: The agent is provided with information relevant to itself, including its current location, orientation, and the objects in its possession.

Action SpaceThe action space consists of three low-level navigation actions (_move forward_, _turn left_, _turn right_), three basic interaction actions (_pick up A_, _put A in B_, _put A on B_), and one idle action (_wait_).

#### 3.3.1 Task Generation

Indoor TaskTo generate an indoor task, a floorplan configuration with six to eight interconnected rooms and a target task is initially sampled from predefined sets. For each scene, objects related to goals in the predefined set are placed on low surfaces such as tables, chairs, sofas, and floors for low objects, and higher surfaces like cabinets or refrigerators for high objects. However, only a subset of the objects is the target object set. The target object set is a set that includes all objects related to a specific type like _food_ or _fruit_, one object randomly selected from a non-target set, and two additional fragile vases if the task is a high-target task. The number of targets is around ten. Then, a goal location and up to six containers are added to the scene based on available space and task constraints.

We randomly initialize two agents (one constrained agent \(A_{m}\) and one helper \(A_{h}\)), and each agent is placed in a free space at least 0.5 meters away from the nearest wall. This setup ensures sufficient initial distance between the agents and the walls, allowing unrestricted movement at the beginning of the task.

Outdoor TaskThe generation of outdoor tasks is largely the same as the indoor task generation. For the shopping task, six shops are generated and spread out on both sides of the road, and each shop sells one specific category of items. The goal location of the shopping task is a fixed, predetermined place in front of the bicycle agent's house. In the moving house task, the target objects include five pieces of furniture on the road in front of a house. The goal location is a truck parked nearby. The details of outdoor task generation can be found in Appendix G.

|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}} 
**Task Name** & **Scene** & **Description** & **Agent Type** & **Features** \\  No constraint & Indoor & Main agent with no constraints & Normal agent & N/A \\  Low target & Indoor & Target objects on the ground & Wheelchair agent & N/A \\  Obstacle & Indoor & Obstacles between most rooms & Wheelchair agent & Existence of obstacles \\  High target & Indoor & Target objects in high places & Child agent & Fragile high targets may break \\  High goal location & Indoor & Goal locations in high places & Child agent & Fragile high targets may break \\  High container & Indoor & Containers in high places & Child agent & Fragile high targets may break \\  Shopping & Outdoor & Main agent walks a bike with his child while shopping & Bicycle agent & Emergency event: the child runs away \\  Moving house & Outdoor & Main agent moves all the furniture onto the truck & Frail agent & Agents can cooperate to lift furniture together \\  

Table 1: Tasks with constrained agents, including both indoor and outdoor scenes and rich features.

#### 3.3.2 Dataset Construction

For each of the eight tasks, we create 12 episodes for training and 12 episodes for testing, resulting in approximately 200 episodes in total. We ensure that the environments of the test set are different from those of the training set. We randomly sample the initial starting states for each task. An episode terminates when all goal predicates of the task are satisfied or when the maximum time step horizon \(T=3000\) frames is reached (for the moving furniture task, the maximum time step horizon is \(T=1500\)).

## 4 Language Agent Augmented with Behavior Modeling Module

We also introduce a new agent framework combining the prowess of action recognition models and the reasoning ability of large language models (LLMs). Due to their simplicity and generalization ability, LLMs can also be implemented in other environments or the real world. We built a **behavior modeling module**, which models the behaviors of the constrained agent via an action recognition model and incorporated it into the CoELA framework (Zhang et al., 2023) with four other modules: (1) the **perception module**, which transforms the raw RGB-D observations into structured semantic maps via an object detection model; (2) the **memory module**, which saves all the history information in a structured manner; (3) the **decision module**, which generates high-level plans and is driven by large language models; and (4) the **execution module**, which turns the generated plans into low-level actions. More details regarding these modules can be found in Appendix B.1. Figure 3 shows an overview of the framework.

### Behavior Modeling Module

To infer the intents and inabilities of constrained agents, the behavior modeling module extracts constrained agents' actions and status from a sequence of egocentric images (i.e., a video). The behavior modeling module contains two parts: **action recognition** and **action grounding**.

Action RecognitionWe adopt an action recognition model to enable the helper agent to recognize the actions of the constrained agent. We select the TSN model (Wang et al., 2016) pretrained on Kinetics-400 (Kay et al., 2017) as the base video action recognition model. There are four types of actions: _pick up_, _put on_, _put in_, and _walking_ (including _move forward_, _turn left_, and _turn right_). Each action may be successfully executed or fail (except for _put in_ and _walking_, which are always

Figure 3: **LLM+BM Helper Implementation Pipeline:** An overview of the LLM+BM Helper with specific modules for _Perception_, _Behavior Modeling_, _Decision_, and _Execution_. (1) The **perception module** detects objects from raw RGB images; (2) the **memory module** builds the semantic map of the environment using depth images and records behaviors; (3) the **behavior modeling module** recognizes the action of the partner and localizes the object corresponding to the action; (4) the **decision module** decides plans for the next steps using foundation models; and (5) the **execution module** generates low-level actions.

successful), so there are six classes in total. We collect data by having an agent follow the constrained agent to observe its behaviors while executing a task and store the action video clips in the training set. During testing, the helper agent utilizes this model to recognize the actions of the constrained agent when it is within observation. The training details can be found in Appendix D.2.

Action GroundingAfter the helper recognizes the action of the constrained agent, it looks up the semantic map in the memory module for the predicate of the action. For example, when the action is _pick up_, the predicate will be the nearest object to the constrained agent. Finally, the behavior modeling module identifies the action, the predicate of the action, and the status of the action of the constrained agent.

## 5 Experiments

### Setup

#### 5.1.1 Constrained Agent Policy

The constrained agent takes ground truth object segmentation as observation to mitigate the impact of imperfect visual perception on performance and chooses actions based on a rule-based high-level planner designed with handwritten rules by human experts.

At the beginning of an episode, the constrained agent will _explore_ the environment to find more target objects, containers, and the goal location. Whenever the agent finds target objects or containers, it will pick them up if it has free hands. If more than \(50\%\) of the time steps are left and it does not have a container in hand, its priority will be to _pick up a container_; otherwise, it will _pick up a target_. If the agent cannot carry more objects, it will _put the object on the goal location_. If less than \(25\%\) of the time steps is left (\(37.5\%\) if it has not found the goal location yet) and the agent is carrying a target object, it will _put the object on the goal location_ immediately since it is often a long walk to the goal location. When selecting possible targets, the agent will opt for the closest one if multiple options are available. Moreover, at any time, if the agent can _put an object in a container_, it will do so.

#### 5.1.2 Evaluation Metrics

To evaluate the success of helper agents, we measure the following three metrics:

* **Transport rate (TR):** The percentage of target objects that the agents successfully transported. We also calculate the **Efficiency Improvement (EI)** of having the helper as \( M/M_{0}\), where \( M\) denotes the increase in the transport rate after adding the helper, and \(M_{0}\) denotes the larger of the transport rates of the team or the constrained agent alone, for numerical stability.
* **Goal Inference Accuracy (IA):** The ratio of target objects successfully transported by the helper to the total number of objects transported by the helper.
* **Emergency Rate (ER):** For the shopping task, we calculate the ratio of frames where the child agent is away from the constrained agent to measure the helper agent's ability to handle emergencies.

### Baselines

We test four types of planning-based helpers: Random Helper, Rule-Based Hierarchical Plan Helper (RHP), LLM+BM Helper, and VLM Helper. All the helpers share the same _Perception Module_, _Memory Module_, and _Execution Module_ as the language agent introduced in Section 4, but the critical differences lie in the high-level planner. Meanwhile, an Oracle Helper is tested to demonstrate the upper-bound performance. Below is the description of each type of helper:

* **No Helper (w/o):** The constrained agent performs the task solely without assistance from a helper.
* **Random Helper:** A naive helper randomly selects a plan from a list of valid plans.
* **Rule-Based Hierarchical Plan Helper (RHP):** This helper uses prior knowledge of the task and relies on handcrafted rules by human experts to make plans to assist the constrained agent in completing the task. Further details on the rules can be found in Appendix B.2.

* **LLM+BM Helper:** A language agent augmented with a Behavior Modeling module introduced in Section 4. Example prompts can be found in Appendix C.1. We use GPT-4 as our decision-making LLM.
* **VLM Helper:** A vision-language agent similar to the LLM+BM Helper. The last \(10\) frames of egocentric RGB-D observation are added as visual inputs to perceive the constrained agent. We use GPT-4 as our decision-making VLM.2 * **Oracle Helper:** An oracle helper that knows the ground truth goal, as the ground truth object segmentation, and the task progress. It behaves the same way as the RHP and is close to the upper-bound performance a helper could achieve.

We also tested some learning-based methods like reinforcement learning and SmartHelp (Cao et al., 2024), whose results can be found in Appendix E.3.

### Main Results

We conducted an extensive evaluation by deploying four baseline models across eight distinct constraint settings and measured four specific metrics, as outlined in Section 5.1.2. The results are presented in Table 2. Overall, the LLM+BM Helper emerges as a strong baseline, achieving the highest transport rate (TR) in 6 out of 8 tasks, the most significant efficiency improvement (EI) in 7 out of 8 tasks, and the best goal inference accuracy (IA) in 4 out of 8 tasks.

Behavior Modeling AnalysisOur LLM+BM Helper achieves a reasonable IA metric compared with other helpers, which shows our behavior model successfully models the partner's behaviors to some extent. However, compared with the Oracle Helper, all the other baseline agents perform poorly on the IA metric. The IA metric reflects whether the helper successfully determines the needs of the constrained agent, so the gap shows all our baselines do not work well in inferring the behavior of the constrained agent from the raw RGB-D image sequence. Nevertheless, our fine-tuned action recognition model achieves \(86\%\) accuracy on the validation set (See Appendix D.2 for the action recognition model details). Two reasons contribute to the discrepancy: (1) Due to blocking or distance, the action clip received by the helper may be incomplete or out-of-distribution from training data. (2) The current LLM-based decision module is insufficient to balance observing the partner's behavior and acting independently.

    \\   &  &  &  &  \\  & TR(EI)\(\) & IA\(\) & TR(EI)\(\) & IA\(\) & TR(EI)\(\) & IA\(\) & TR(EI)\(\) & IA\(\) \\  w/o & 0.53 & / & 0.30 & / & 0.37 & / & 0.28 & / \\ Random & 0.52(-0.02) & 0.24 & 0.27(-0.05) & 0.29 & 0.36(0.00) & 0.25 & 0.33(0.10) & 0.14 \\ RHP & 0.64(0.15) & 0.15 & 0.35(0.11) & 0.29 & 0.45(0.19) & 0.21 & 0.35(0.18) & 0.21 \\ VLM (GPT-4o) & 0.63(0.14) & 0.24 & 0.33(0.06) & **0.32** & 0.43(0.12) & **0.40** & 0.26(-0.20) & 0.33 \\ LLM (GPT-4) + BM & **0.65(0.17)** & **0.25** & **0.38(0.19)** & 0.29 & **0.49(0.24)** & 0.30 & **0.36(0.23)** & **0.35** \\  Oracle & 0.77(0.31) & 0.88 & 0.49(0.37) & 0.91 & 0.69(0.47) & 0.91 & 0.61(0.56) & 0.90 \\   &  \\   &  &  &  &  \\  & TR(EI)\(\) & IA\(\) & TR(EI)\(\) & IA\(\) & TR(EI)\(\) & IA\(\) & ER\(\) & TR(EI)\(\) \\  w/o & 0.51 & / & 0.07 & / & 0.37 & / & / & 0.17 \\ Random & 0.50(-0.01) & 0.31 & 0.21(0.56) & 0.24 & 0.39(0.05) & 0.34 & 0.32 & 0.48(0.68) \\ RHP & 0.66(0.23) & 0.28 & **0.44**(0.77) & 0.17 & 0.49(0.22) & 0.44 & **0.30** & 0.65(0.72) \\ VLM (GPT-4o) & 0.69(0.26) & **0.46** & 0.40(0.86) & 0.35 & 0.50(0.25) & 0.72 & 0.39 & **0.70(0.78)** \\ LLM (GPT-4) + BM & **0.70(0.27)** & 0.43 & 0.42(**0.89)** & **0.47** & **0.58(0.33)** & **0.74** & 0.38 & 0.69(0.77) \\  Oracle & 0.82(0.38) & 0.91 & 0.60(0.87) & 0.82 & 0.61(0.39) & 0.87 & 0.17 & 0.76(0.80) \\   

Table 2: **Quantitative results on CHAIC benchmark. We report the average _Transport Rate (TR)_, _Efficiency Improvement (EI)_ and _Goal Inference Accuracy (IA)_ here. **w/o** means the main agent does the task solely without a helper. The _Emergency Rate (ER)_ metric is also reported for the shopping task.

LLM Can Infer Goals Correctly and Perform Actions ProperlyIn analyzing some of the chain-of-thought outputs of LLM, we observe that the LLM-based helper can accurately infer the target objects desired by the constrained agent and formulate appropriate plans to collect them. For instance, in an outdoor shopping scene, the bike agent named David seeks some fruit. Initially, the LLM helper assesses, _"Since David hasn't picked any object yet, it's challenging to precisely determine his target objects."_ It then realizes, _"No matter what object David wants, the best first step would be to maximize the efficiency of carrying objects by using a container,"_ and subsequently proceeds to pick up a container. Upon observing the bike agent picking an apple, the LLM helper deduces, _"Considering the constraints and the objects David has shown interest in (i.e., an apple), the best course of action from the provided list would be to 'goto and pick up target <apple>'."_ With a container and a target object in both hands, the LLM helper notes, _"Considering I am currently holding two target objects (one directly and one in a container), the optimal next action is to put the object in your one hand to the container in your other hand. This action will free up one of my hands, allowing me to pick up more target objects and transport them efficiently to the goal."_

Meanwhile, the LLM helper is capable of picking other fruits besides apples, demonstrating its accuracy in inferring the object category. After freeing up one hand, the LLM helper states, _"Based on the observed actions and status of David, it's clear that his target objects are fruits, specifically apples...so picking up more grapes aligns with the goal."_ Finally, after collecting several fruits and having both hands full, the LLM helper concludes, _"the best action to take next is to 'transport object in hand to goal space'. This action involves taking the container filled with target objects, along with the additional grape in the other hand, to the specified goal location."_ The detailed analysis of these chain-of-thought outputs is shown in Appendix F.1.

Dealing with EmergenciesIn outdoor shopping tasks, the helper needs to handle unpredicted emergencies, requiring swift responses. The Emergency Rate (ER) metric shows that even if LLM- and VLM-based helpers can achieve high scores in normal tasks, they cannot handle emergencies as efficiently as RHP. To improve, some rule-based control may be required in LLM- and VLM-based helpers to help them prioritize and respond more effectively in urgent situations.

Failure Case AnalysisDuring the experiment, we discovered some common failure situations leading to poor performance, which might be helpful for further helper design.

* **Spatial Information Analysis**: The LLM-based agents do not understand spatial information very well when provided with text inputs of object locations. They often choose a distant object rather than a nearby one, even if they share the same name. Additionally, they often underestimate the cost of reaching the goal location and fail to transport due to time limits.
* **Acting without Cooperation**: In the obstacle task, a reasonable solution for the helper is to remove obstacles first to free the constrained agent. However, LLM- and VLM-based helpers often transport objects alone without assisting the constrained agent, leading to relatively bad performance in this task.
* **VLM is Unable to Infer the Targets Needed by Constrained Agents**: In certain tasks, the VLM Helper baseline performs worse than both the random baseline and the No Helper baseline. This is primarily because VLM cannot accurately infer the preferred objects of constrained agents when they observe them picking up items, leading it to consistently follow. Consequently, the VLM Helper fails to transport any objects, making it less effective than randomly transporting some objects, as done by the random helper. Additionally, frequently following the constrained agent can interfere with their actions--such as blocking their path--resulting in the VLM Helper baseline sometimes performing worse than having no helper. However, the LLM+BM Helper transports some objects even if it does not infer the goal correctly from the BM model, achieving a relatively higher score than the VLM Helper.

## 6 Conclusion

In this work, we proposed an accessibility-centered embodied social intelligence challenge: the Constrained Human-AI Cooperation (CHAIC) Challenge. This challenge includes four new agents with physical constraints and eight long-horizon tasks featuring both indoor and outdoor scenes, designed to test the critical skills of social perception and cooperation in embodied agents. Our experimental results benchmarking both planning- and learning-based baselines illustrate the systematic evaluation that such a benchmark can provide for future efforts. We further perform an in-depth analysis of failure cases and provide insights for the future development of embodied social intelligence.

LimitationsWhile we aimed to preserve as much realism as possible, there are undoubtedly aspects of human behavior, particularly in how physical constraints manifest in the world, that are challenging to simulate. Meanwhile, the rule-based control of constrained agents makes their behavior lack diversity. This may be solved by leveraging LLMs to control constrained agents. Moreover, while we believe our challenge takes a good first step forward in introducing accessibility challenges to embodied social intelligence benchmarking efforts, we emphasize that our challenge is not representative of _all_ possible constraints that such users may face.