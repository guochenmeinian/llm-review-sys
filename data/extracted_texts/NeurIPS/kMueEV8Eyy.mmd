# Abide by the Law and Follow the Flow:

Conservation Laws for Gradient Flows

 Sibylle Marcotte

ENS - PSL Univ.

sibylle.marcotte@ens.fr

&Remi Gribonval

Univ Lyon, EnsL, UCBL,

CNRS, Inria, LIP,

remi.gribonval@inria.fr

&Gabriel Peyre

CNRS, ENS - PSL Univ.

gabriel.peyre@ens.fr

###### Abstract

Understanding the geometric properties of gradient descent dynamics is a key ingredient in deciphering the recent success of very large machine learning models. A striking observation is that trained over-parameterized models retain some properties of the optimization initialization. This "implicit bias" is believed to be responsible for some favorable properties of the trained models and could explain their good generalization properties. The purpose of this article is threefold. First, we rigorously expose the definition and basic properties of "conservation laws", that define quantities conserved during gradient flows of a given model (e.g. of a ReLU network with a given architecture) with any training data and any loss. Then we explain how to find the maximal number of independent conservation laws by performing finite-dimensional algebraic manipulations on the Lie algebra generated by the Jacobian of the model. Finally, we provide algorithms to: a) compute a family of polynomial laws; b) compute the maximal number of (not necessarily polynomial) independent conservation laws. We provide showcase examples that we fully work out theoretically. Besides, applying the two algorithms confirms for a number of ReLU network architectures that all known laws are recovered by the algorithm, and that there are no other independent laws. Such computational tools pave the way to understanding desirable properties of optimization initialization in large machine learning models.

## 1 Introduction

State-of-the-art approaches in machine learning rely on the conjunction of gradient-based optimization with vastly "over-parameterized" architectures. A large body of empirical  and theoretical  works suggest that, despite the ability of these models to almost interpolate the input data, they are still able to generalize well. Analyzing the training dynamics of these models is thus crucial to gain a better understanding of this phenomenon. Of particular interest is to understand what properties of the initialization are preserved during the dynamics, which is often loosely referred to as being an "implicit bias" of the training algorithm. The goal of this article is to make this statement precise, by properly defining maximal sets of such "conservation laws", by linking these quantities to algebraic computations (namely a Lie algebra) associated with the model parameterization (in our framework, this parameterization is embodied by a re-parameterization mapping \(\)), and finally by exhibiting algorithms to implement these computations in SageMath .

Over-parameterized modelModern machine learning practitioners and researchers have found that over-parameterized neural networks (with more parameters than training data points), which are often trained until perfect interpolation, have impressive generalization properties . This performance seemingly contradicts classical learning theory , and a large part of the theoretical deep learning literature aims at explaining this puzzle. The choice of the optimization algorithm is crucial to the model generalization performance , thus inducing an _implicit bias_.

Implicit biasThe terminology "implicit bias" informally refers to properties of trained models which are induced by the optimization procedure, typically some form of regularization . For gradient descent, in simple cases such as scalar linear neural networks or two-layer networks with a single neuron, it is actually possible to compute in closed form the implicit bias, which induces some approximate or exact sparsity regularization . Another interesting case is logistic classification on separable data, where the implicit bias selects the max-margin classifier both for linear models  and for two-layer neural networks in the mean-field limit . A key hypothesis to explicit the implicit bias is often that the Riemannian metric associated to the over-parameterization is either of Hessian type , or can be somehow converted to be of Hessian type , which is seemingly always a very strong constraint. For example, even for simple two-layer linear models (i.e., matrix factorization) with more than a single hidden neuron, the Hessian type assumption does not hold, and no closed form is known for the implicit bias . The work of  gives conditions on the over-parameterization for this to be possible (for instance certain Lie brackets should vanish). These conditions are (as could be expected) stronger than those required to apply Frobenius theory, as we do in the present work to retrieve conservation laws.

Conservation lawsFinding functions conserved during gradient flow optimization of neural networks (a continuous limit of gradient descent often used to model the optimization dynamics) is particularly useful to better understand the flow behavior. One can see conservation laws as a "weak" form of implicit bias: to explain, among a possibly infinite set of minimizers, which properties (e.g. in terms of sparsity, low-rank, etc.) are being favored by the dynamic. If there are enough conservation laws, one has an exact description of the dynamic (see Section 3.4), and in some cases, one can even determine explicitly the implicit bias. Otherwise, one can still predict what properties of the initialization are retained at convergence, and possibly leverage this knowledge. For example, in the case of linear neural networks, certain _balancedness properties_ are satisfied and provide a class of conserved functions . These conservation laws enable for instance to prove the global convergence of the gradient flow  under some assumptions. We detail these laws in Proposition 4.1. A subset of these "balancedness" laws still holds in the case of a ReLU activation , which reflects the rescaling invariance of these networks (see Section 4 for more details). More generally such conservation laws bear connections with the invariances of the model : to each 1-parameter group of transformation preserving the loss, one can associate a conserved quantity, which is in some sense analogous to Noether's theorem . Similar reasoning is used by  to show the influence of initialization on convergence and generalization performance of the neural network. Our work is somehow complementary to this line of research: instead of assuming a priori known symmetries, we directly analyze the model and give access to conservation laws using algebraic computations. For matrix factorization as well as for certain ReLU network architectures, this allows us to show that the conservation laws reported in the literature are complete (there are no other independent quantities that would be preserved by all gradient flows).

### Contributions

We formalize the notion of a conservation law, a quantity preserved through all gradient flows given a model architecture (e.g. a ReLU neural network with prescribed layers). Our main contributions are:

* to show that for several classical losses, characterizing conservation laws for deep linear (resp. shallow ReLU) networks boils down to analyzing a finite dimensional space of vector fields;
* to propose an algorithm (coded in SageMath) identifying polynomial conservation laws on linear / ReLU network architectures; it identifies all known laws on selected examples;
* to formally define the maximum number of (not necessarily polynomial) independent conservation laws and characterize it a) theoretically via Lie algebra computations; and b) practically via an algorithm (coded in SageMath) computing this number on worked examples;
* to illustrate that in certain settings these findings allow to rewrite an over-parameterized flow as an "intrinsic" low-dimensional flow;
* to highlight that the cost function associated to the training of linear and ReLU networks, shallow or deep, with various losses (quadratic and more) fully fits the proposed framework.

A consequence of our results is to show for the first time that conservation laws commonly reported in the literature are maximal: there is no other independent preserved quantity (see Propositions 4.3, 4.2, Corollary 4.4) and Section 4.2).

Conservation Laws for Gradient Flows

After some reminders on gradient flows, we formalize the notion of conservation laws.

### Gradient dynamics

We consider learning problems, where we denote \(x_{i}^{m}\) the features and \(y_{i}\) the targets (for regression, typically with \(=^{n}\)) or labels (for classification) in the case of supervised learning, while \(y_{i}\) can be considered constant for unsupervised/self-supervised learning. We denote \(X(x_{i})_{i}\) and \(Y(y_{i})_{i}\). Prediction is performed by a parametric mapping \(g(,):^{m}^{n}\) (for instance a neural network) which is trained by empirically minimizing over parameters \(^{D}\) a **cost**

\[_{X,Y}()_{i}(g(,x_{i}),y_{i}),\] (1)

where \(\) is the **loss** function. In practical examples with linear or ReLU networks, \(\) is either \(^{D}\) or an open set of "non-degenerate" parameters. The goal of this paper is to analyze what functions \(h()\) are preserved during the gradient flow (the continuous time limit of gradient descent) of \(_{X,Y}\):

\[(t)=-_{X,Y}((t)),(0)= _{}.\] (2)

A priori, one can consider different "levels" of conservation, depending whether \(h\) is conserved: during the optimization of \(_{X,Y}\) for a given loss \(\) and a given data set \((x_{i},y_{i})_{i}\); or given a loss \(\), during the optimization of \(_{X,Y}\) for _any_ data set \((x_{i},y_{i})_{i}\). Note that using stochastic optimization methods and discrete gradients would break the exact preservation of the conservation laws, and only approximate conservation would hold, as remarked in .

### Conserved functions

As they are based on gradient flows, conserved functions are first defined locally.

**Definition 2.1** (Conservation through a flow).: Consider an open subset \(\) and a vector field \(^{1}(,^{D})\). By the Cauchy-Lipschitz theorem, for each initial condition \(_{}\), there exists a unique maximal solution \(t[0,T_{_{}})(t,_{})\) of the ODE \((t)=((t))\) with \((0)=_{}\). A function \(h:^{D}\) is _conserved on \(\) through the vector field \(\)_ if \(h((t,_{}))=h(_{})\) for each choice of \(_{}\) and every \(t[0,T_{_{}})\). It is _conserved on \(\) through a subset \(W^{1}(,^{D})\)_ if \(h\) is conserved on \(\) during all flows induced by all \( W\).

In particular, one can adapt this definition for the flow induced by the cost (1).

**Definition 2.2** (Conservation during the flow (2) with a given dataset).: Consider an open subset \(\) and a dataset \((X,Y)\) such that \(_{X,Y}^{2}(,)\). A function \(h:^{D}\) is _conserved on \(\) during the flow_ (2) if it is conserved through the vector field \(()_{X,Y}()\).

Our goal is to study which functions are conserved during _"all" flows_ defined by the ODE (2). This in turn leads to the following definition.

**Definition 2.3** (Conservation during the flow (2) with "any" dataset).: Consider an open subset \(\) and a loss \((z,y)\) such that \((,y)\) is \(^{2}\)-differentiable for all \(y\). A function \(h:^{D}\) is _conserved on \(\) for any data set_ if, for each data set \((X,Y)\)_such that \(g(,x_{i})^{2}(,)\)_ for each \(i\), the function \(h\) is conserved on \(\) during the flow (2). This leads us to introduce the family of vector fields:

\[W^{g}_{}:=\{(): X,Y, i\ g(,x_{i}) ^{2}(,),\ =_{X,Y}\}^{1}(,^{ D})\] (3)

so that being conserved on \(\) for any dataset is the same as being conserved on \(\) through \(W^{g}_{}\).

The above definitions are local and conditioned on a choice of open set of parameters \(\). We are rather interested in functions defined on the whole parameter space \(\), hence the following definition.

**Definition 2.4**.: A function \(h:\) is _locally conserved on \(\) for any data set_ if for each open subset \(\), \(h\) is conserved on \(\) for any data set.

A basic property of \(^{1}\) conserved functions (which proof can be found in Appendix A) corresponds to an "orthogonality" between their gradient and the considered vector fields.

**Proposition 2.5**.: _Given a subset \(W^{1}(,^{D})\), its trace at \(\) is defined as the linear space_

\[W()\{(): W\} ^{D}.\] (4)

_A function \(h^{1}(,)\) is conserved on \(\) through \(W\) if, and only if \( h() W(),\)._

Therefore, combining Proposition 2.5 and Definition 2.4, the object of interest to study locally conserved functions is the union of the traces

\[W^{g}_{}W^{g}_{}()\ :\  }.\] (5)

**Corollary 2.6**.: _A function \(h:\) is locally conserved on \(\) for any data set if and only if \( h() W^{g}_{}\) for all \(\)._

It will soon be shown (cf Theorem 2.14) that \(W^{g}_{}\) can be rewritten as the trace \(W()\) of a _simple_ finite-dimensional functional space \(W\). Meanwhile, we keep the specific notation. For the moment, this set is explicitly characterized via the following proposition (which proof can be found in Appendix B).

**Proposition 2.7**.: _Assume that for each \(y\) the loss \((z,y)\) is \(^{2}\)-differentiable with respect to \(z^{n}\). For each \(\) we have:_

\[W^{g}_{}=*{span}_{(x,y)_{} }\{[ g(,x)]^{}_{z}(g(,x),y)\}\]

_where \(_{}\) is the set of data points \(x\) such that \(g(,x)\) is \(^{2}\)-differentiable in the neighborhood of \(\)._

_Example 2.8_.: As a first simple example, consider a two-layer _linear_ neural network in dimension 1 (both for the input and output), with a single neuron. For such - admittedly trivial - architecture, the parameter is \(=(u,v)^{2}\) and the model writes \(g(,x)=uvx\). One can directly check that the function: \(h(u,v)=u^{2}-v^{2}\) is _locally conserved on \(^{2}\) for any data set_. Indeed in that case \( h(u,v)=(2u,-2v)^{} W^{g}_{}=*{span}_{(x,y )}\{(vx,ux)^{}_{z}(g(,x),y) \}=(v,u)^{}\) given that the gradient \(_{z}(g(,x),y)\) is an arbitrary scalar.

In this example we obtain a simple expression of \(W^{g}_{}\), however in general cases it is not possible to obtain such a simple expression from Proposition 2.7. We will show that in some cases, it is possible to express \(W^{g}_{}\) as the trace \(W()\) of a simple finite-dimensional space \(W\) (cf. Theorem 2.14).

### Reparametrization

To make the mathematical analysis tractable and provide an algorithmic procedure to determine these functions, our fundamental hypothesis is that the **model**\(g(,x)\) can be (locally) factored via a **reparametrization**\(\) as \(f((),x)\). We require that the model \(g(,x)\) satisfies the following central assumption.

**Assumption 2.9** (Local reparameterization).: There exists \(d\) and \(^{2}(,^{d})\) such that: for each parameter \(_{0}\) in the open set \(^{D}\), for each \(x\) such that \( g(,x)\) is \(^{2}\) in a neighborhood of \(_{0}\)1, there is a neighborhood \(\) of \(_{0}\) and \(f(,x)^{2}((),^{n})\) such that

\[, g(,x)=f((),x).\] (6)

Note that if the model \(g(,x)\) is smooth on \(\) then (6) is always satisfied with \(}\) and \(f(,x) g(,x)\), yet this trivial factorization fails to capture the existence and number of conservation laws as studied in this paper. This suggests that, among all factorizations shaped as (6), there may be a notion of an optimal one.

_Example 2.10_.: (Factorization for _linear_ neural networks): In the two-layer case, with \(r\) neurons, denoting \(=(U,V)^{n r}^{m r}\) (so that \(D=(n+m)r\)), we can factorize \(g(,x) UV^{}x\) by the reparametrization \(() UV^{}^{n m}\) using \(f(,x)= x\). More generally for \(q\) layers, with \(=(U_{1},,U_{q})\), we can still factorize \(g(,x) U_{1} U_{q}x\) using \(() U_{1} U_{q}\) and the same \(f\). This factorization is _globally_ valid on \(==^{D}\) since \(f(,x)\) does not depend on \(_{0}\).

The notion of locality of the factorization (6) is illustrated by the next example.

_Example 2.11_ (Factorization for two-layer ReLU networks).: Consider \(g(,x)=_{j=1}^{r}u_{k,j}( v_{j},x+b_{j})+c_{k} _{k=1}^{n}\), with \((t)(t,0)\) the ReLU activation function and \(v_{j}^{m}\), \(u_{k,j}\), \(b_{j},c_{k}\). Then, denoting \(=(U,V,b,c)\) with \(U=(u_{k,j})_{k,j}:(u_{1},,u_{r})^{n r}\), \(V=(v_{1},,v_{r})^{m r}\), \(b=(b_{1},,b_{r})^{}^{r}\) and \(c=(c_{1},,c_{n})^{n}\) (so that \(D=(n+m+1)r+n\)), we rewrite \(g(,x)=_{j=1}^{r}u_{j}_{j,x}(v_{j}^{}x+b_{j} )+c\) where, given \(x\), \(_{j,x}=(v_{j}^{}x+b_{j}>0)\) is piecewise constant with respect to \(\). Consider \(^{0}=(U^{0},V^{0},b^{0},c^{0})^{D}\) where \(V^{0}=(v_{1}^{0},,v_{r}^{0})\) and \(b^{0}=(b_{1}^{0},,b_{r}^{0})^{}\). Then the set \(_{^{0}}\) introduced in Proposition 2.7 is \(_{^{0}}=^{m}-_{j}\{v_{j}^{0}\}^{}x+b_{j}^{0}=0\). Let \(x_{^{0}}\). Then on any domain \(^{D}\) such that \(^{0}\) and \(_{j,x}()(v_{j}^{}x+b_{j}>0)\) is constant over \(\), the model \(g_{}(x)\) can be factorized by the reparametrization \(()=((u_{j}v_{j}^{},u_{j}b_{j})_{j=1}^{r},c)\). In particular, in the case without bias (\((b,c)=(0,0)\)), the reparametrization is defined by \(()=(_{j})_{j=1}^{r}\) where \(_{j}=_{j}() u_{j}v_{j}^{}^{n m}\) (here \(d=rmn\)) using \(f(,x)=_{j}_{j,x}_{j}x\): the reparametrization \(()\) contains \(r\) matrices of size \(m n\) (each of rank at most one) associated to a "local" \(f(,x)\) valid in a neighborhood of \(\). A similar factorization is possible for deeper ReLU networks , as further discussed in the proof of Theorem 2.14 in Appendix C.

Combining Proposition 2.7 and using chain rules, we get a new characterization of \(W_{}^{g}\).

**Proposition 2.12**.: _Assume that the loss \((z,y)\) is \(^{2}\)-differentiable with respect to \(z\). We recall (cf (5)) that \(W_{}^{g}_{:}\) is open and \( W_{}^{g}()\). Under Assumption 2.9, for all \(\):_

\[W_{}^{g}=()^{}W_{()}^{f}\] (7)

_with \(()^{d D}\) the Jacobian of \(\) and \(W_{()}^{f}}_{(x,y)_{ }}\{ f^{x}(())^{}_{z}( g(,x),y)\}\), where \(f^{x}() f(,x)\)._

We show in Section 2.4 that, under mild assumptions on the loss \(\), \(W_{()}^{f}=^{d}\), so that Proposition 2.12 yields \(W_{}^{g}=}(()^{})\). Then by Corollary 2.6, a function \(h\) that is locally conserved on \(\) for any data set is _entirely characterized_ via the kernel of \(()^{}()^{} h()=0\) for all \(\). The core of our analysis is then to analyze the (Lie algebraic) structure of \(}(()^{})\).

### From conserved functions to conservation laws

For linear and ReLU networks we show in Theorem 2.14 and Proposition 2.16 that under (mild) assumptions on the loss \((,)\), being locally conserved on \(\) for any data set (according to Definition 2.4) is the same as being conserved (according to Definition 2.1) on \(\) through the _finite-dimensional_ subspace

\[W_{}}\{_{1}(),, _{d}()\}=_{i}a_{i}_{i}():(a_ {1},,a_{d})^{d}}\] (8)

where we write \(()^{}=(_{1}(),,_{d}( ))^{D d}\), with \(_{i}^{1}(,^{D})\).

The following results (which proofs can be found in Appendix C) establish that in some cases, the functions locally conserved for any data set are exactly the functions conserved through \(W_{}\).

**Lemma 2.13**.: _Assume that the loss \((z,y)(z,y)\) is \(^{2}\)-differentiable with respect to \(z^{n}\) and satisfies the condition:_

\[}_{y}\{_{z}(z,y)\}=^{n},  z^{n}.\] (9)

_Then for linear neural networks (resp. for two-layer ReLU networks) and all \(\) we have \(W_{()}^{f}=^{d}\), with the reparametrization \(\) from Example 2.10 and \(^{D}\) (resp. with \(\) from Example 2.11 and \(\) consisting of all parameter \(\) of the network such that hidden neurons are associated to pairwise distinct "hyperplanes", cf Appendix C for details)._

Condition (9) holds for classical losses \(\) (e.g. quadratic/logistic losses), as shown in Lemma C.2 in Appendix C. Note that the additional hypothesis of pairwise distinct hyperplanes for the two-layer ReLU case is a generic hypothesis and is usual (see e.g. the notion of twin neurons in ). The tools from Appendix C extend Theorem 2.14 beyond (deep) linear and shallow ReLU networks. An open problem is whether the conclusions of Lemma 2.13 still hold for deep ReLU networks.

**Theorem 2.14**.: _Under the same assumptions as in Lemma 2.13, we have that for linear neural networks, for all \(^{D}\):_

\[W_{}^{g}=W_{}().\] (10)

_The same result holds for two-layer ReLU networks with \(\) from Example 2.11 and \(\) the (open) set of all parameters \(\) such that hidden neurons are associated to pairwise distinct "hyperplanes"._

This means as claimed that for linear and two-layer ReLU networks, being locally conserved on \(\) for any data set exactly means being conserved on \(\) through the finite-dimensional functional space \(W_{}^{1}(,^{D})\). This motivates the following definition

**Definition 2.15**.: A real-valued function \(h\) is a _conservation law of \(\)_ if it is conserved through \(W_{}\).

**Proposition 2.16**.: \(h^{1}(,)\) _is a conservation law for \(\) if and only if_

\[ h()_{j}(),\;\;,\;  j\{1,,d\}.\]

Thanks to Theorem 2.14, the space \(W_{}\) defined in (8) introduces a much simpler proxy to express \(W_{}^{g}\) as a trace of a subset of \(^{1}(,^{D})\). Moreover, when \(\) is \(^{},W_{}\) is a _finite-dimensional_ space of _infinitely smooth_ functions on \(\), and this will be crucial in Section 4.1 to provide a tractable scheme (i.e. operating in finite dimension) to compute the _maximum number of independent_ conservation laws, using the Lie algebra computations that will be described in Section 3.

_Example 2.17_.: Revisiting Example 2.8, the function to minimize is factorized by the reparametrization \(:(u,v) uv\) with \((u,v)\). We saw that \(h((u,v)) u^{2}-v^{2}\) is conserved: and indeed \( h(u,v),(u,v)=2uv-2vu=0\), \((u,v)\).

In this simple example, the characterization of Proposition 2.16 gives a _constructive_ way to find such a conserved function: we only need to find a function \(h\) such that \( h(u,v),(u,v)= h(u,v),(v,u)^{} =0\). The situation becomes more complex in higher dimensions, since one needs to understand the interplay between the different vector fields in \(W_{}\).

### Constructibility of some conservation laws

Observe that in Example 2.17 both the reparametrization \(\) and the conservation law \(h\) are polynomials, a property that surprisingly systematically holds in all examples of interest in the paper, making it possible to _algorithmically_ construct some conservation laws as detailed now.

By Proposition 2.16, a function \(h\) is a conservation law if it is in the kernel of the linear operator \(h^{1}(,)((  h(),_{i}())_{i=1,,d})\). Thus, one could look for conservation laws in a prescribed finite-dimensional space by projecting these equations in a basis (as in finite-element methods for PDEs). Choosing the finite-dimensional subspace could be generally tricky, but for the linear and ReLU cases all known conservation laws are actually polynomial "balancedness-type conditions" [1; 2; 9], see Section 4. In these cases, the vector fields in \(W_{}\) are also polynomials (because \(\) is polynomial, see Theorem C.4 and Theorem C.5 in Appendix C), hence \( h(),_{i}()\) is a polynomial too. This allows us to compute a basis of independent polynomial conservation laws of a given degree (to be freely chosen) for these cases, by simply focusing on the corresponding subspace of polynomials. We coded the resulting equations in SageMath, and we found back on selected examples (see Appendix J) all existing known conservation laws both for ReLU and linear networks. Open-source code is available at .

### Independent conserved functions

Having an algorithm to build conservation laws is nice, yet how can we know if we have built "all" laws? This requires first defining a notion of a "maximal" set of functions, which would in some sense be independent. This does not correspond to linear independence of the functions themselves (for instance, if \(h\) is a conservation law, then so is \(h^{k}\) for each \(k\) but this does not add any other constraint), but rather to pointwise linear independence of their gradients. This notion of independence is closely related to the notion of "functional independence" studied in [7; 20]. For instance, it is shown in  that smooth functionally dependent functions are characterized by having dependent gradients everywhere. This motivates the following definition.

**Definition 2.18**.: A family of \(N\) functions \((h_{1},,h_{N})\) conserved through \(W^{1}(,^{D})\) is said to be _independent_ if the vectors \(( h_{1}(),, h_{N}())\) are linearly independent for all \(\).

An immediate upper bound holds on the largest possible number \(N\) of functionally independent functions \(h_{1},,h_{N}\) conserved through \(W\): for \(^{D}\), the space \(\{ h_{1}(),, h_{N}()\} ^{D}\) is of dimension \(N\) (by independence) and (by Proposition 2.5) orthogonal to \(W()\). Thus, it is necessary to have \(N D- W()\). As we will now see, this bound can be tight _under additional assumptions on \(W\) related to Lie brackets_ (corresponding to the so-called Frobenius theorem). This will in turn lead to a characterization of the maximum possible \(N\).

## 3 Conservation Laws using Lie Algebra

The study of hyper-surfaces trapping the solution of ODEs is a recurring theme in control theory, since the existence of such surfaces is the basic obstruction of controllability of such systems . The basic result to study these surfaces is the so-called Frobenius theorem from differential calculus (See Section 1.4 of  for a good reference for this theorem). It relates the existence of such surfaces, and their dimensions, to some differential condition involving so-called "Lie brackets" \([u,v]\) between pairs of vector fields (see Section 3.1 below for a more detailed exposition of this operation). However, in most cases of practical interest (such as for instance matrix factorization), the Frobenius theorem is not suitable for a direct application to the space \(W_{}\) because its Lie bracket condition is not satisfied. To identify the number of independent conservation laws, one needs to consider the algebraic closure of \(W_{}\) under Lie brackets. The fundamental object of interest is thus the Lie algebra generated by the Jacobian vector fields, that we recall next. While this is only defined for vector fields with stronger smoothness assumption, the only consequence is that \(\) is required to be infinitely smooth, unlike the loss \((,y)\) and the model \(g(,x)\) that can be less smooth. All concretes examples of \(\) in this paper are polynomial hence indeed infinitely smooth.

NotationsGiven a vector subspace of infinitely smooth vector fields \(W()^{}(, ^{D})\), where \(\) is an open subset of \(^{D}\), we recall (cf Proposition 2.5) that its trace at some \(\) is the subspace

\[W()\{(): W\}^ {D}.\] (11)

For each open subset \(\), we introduce the subspace of \(()\): \(W_{|}\{_{|}: W\}\).

### Background on Lie algebra

A Lie algebra \(A\) is a vector space endowed with a bilinear map \([,]\), called a Lie bracket, that verifies for all \(X,Y,Z A\): \([X,X]=0\) and the Jacobi identity: \([X,[Y,Z]]+[Y,[Z,X]]+[Z,[X,Y]]=0\).

For the purpose of this article, the Lie algebra of interest is the set of infinitely smooth vector fields \(()\), endowed with the Lie bracket \([,]\) defined by

\[[_{1},_{2}]:[_{1},_{2}]() _{1}()_{2}()-_{2}() _{1}(),\] (12)

with \(()^{D D}\) the jacobian of \(\) at \(\). The space \(^{n n}\) of matrices is also a Lie algebra endowed with the Lie bracket \([A,B] AB-BA\). This can be seen as a special case of (12) in the case of _linear_ vector fields, i.e. \(()=A\).

Generated Lie algebraLet \(A\) be a Lie algebra and let \(W A\) be a vector subspace of \(A\). There exists a smallest Lie algebra that contains \(W\). It is denoted \((W)\) and called the generated Lie algebra of \(W\). The following proposition [6, Definition 20] constructively characterizes \((W)\), where for vector subspaces \([W,W^{}]\{[_{1},_{2}]:_{1} W,_{2} W^{ }\}\), and \(W+W^{}=\{_{1}+_{2}:_{1} W,_{2} W^{}\}\).

**Proposition 3.1**.: _Given any vector subspace \(W A\) we have \((W)=_{k}W_{k}\) where:_

\[\{W_{0}& W\\ W_{k}& W_{k-1}+[W_{0},W_{k-1}]\ \ \ k 1..\]

We will see in Section 3.2 that the number of conservation laws is characterized by the dimension of the trace \((W_{})()\) defined in (11). The following lemma (proved in Appendix D) gives a stopping criterion to algorithmically determine this dimension (see Section 3.3 for the algorithm).

**Lemma 3.2**.: _Given \(\), if for a given \(i\), \( W_{i+1}(^{})= W_{i}()\) for every \(^{}\) in a neighborhood of \(\), then there exists a neighborhood \(\) of \(\) such that \(W_{k}(^{})=W_{i}(^{})\) for all \(^{}\) and \(k i\), where the \(V_{i}\) are defined by Proposition 3.1. Thus \((W)(^{})=W_{i}(^{})\) for all \(^{}\). In particular, the dimension of the trace of \((W)\) is locally constant and equal to the dimension of \(W_{i}()\)._

### Number of conservation laws

The following theorem uses the Lie algebra generated by \(W_{}\) to characterize the number of conservation laws. The proof of this result is based on two successive uses of the Frobenius theorem and can be found in Appendix E (where we also recall Frobenius theorem for the sake of completeness).

**Theorem 3.3**.: _If \(((W_{})())\) is locally constant then each \(^{D}\) admits a neighborhood \(^{}\) such that there are \(D-((W_{})())\) (and no more) independent conserved functions through \(W_{|^{}}\), i.e., there are \(D-((W_{})())\) independent conservation laws of \(\) on \(^{}\)._

_Remark 3.4_.: The proof of the Frobenius theorem (and therefore of our generalization Theorem 3.3) is actually constructive. From a given \(\), conservation laws are obtained in the proof by integrating in time (_i.e._ solving an advection equation) the vector fields belonging to \(W_{}\). Unfortunately, this cannot be achieved in _closed form_ in general, but in small dimensions, this could be carried out numerically (to compute approximate discretized laws on a grid or approximate them using parametric functions such as Fourier expansions or neural networks).

A fundamental aspect of Theorem 3.3 is to rely only on the _dimension of the trace_ of the Lie algebra associated with the finite-dimensional vector space \(W_{}\). Yet, even if \(W_{}\) is finite-dimensional, it might be the case that \((W_{})\) itself remains infinite-dimensional. Nevertheless, what matters is not the dimension of \((W_{})\), but that of _its trace_\((W_{})()\), which is _always_ finite (and potentially much smaller that \((W_{})\) even when the latter is finite) and computationally tractable thanks to Lemma 3.2 as detailed in Section 3.3. In section 4.1 we work out the example of matrix factorization, a non-trivial case where the full Lie algebra \((W_{})\) itself remains finite-dimensional.

Theorem 3.3 requires that the dimension of the trace at \(\) of the Lie algebra is locally constant. This is a technical assumption, which typically holds outside a set of pathological points. A good example is once again matrix factorization, where we show in Section 4.1 that this condition holds generically.

### Method and algorithm, with examples

Given a reparametrization \(\) for the architectures to train, to determine the number of independent conservation laws of \(\), we leverage the characterization 3.1 to algorithmically compute \(((W_{})())\) using an iterative construction of bases for the subspaces \(W_{k}\) starting from \(W_{0} W_{}\), and stopping as soon as the dimension stagnates thanks to Lemma 3.2. Our open-sourced code is available at  and uses SageMath. As we now show, this algorithmic principle allows to fully work out certain settings where the stopping criterion of Lemma 3.2 is reached at the first step (\(i=0\)) or the second one (\(i=1\)). Section 4.2 also discusses its numerical use for an empirical investigation of broader settings.

Example where the iterations of Lemma 3.2 stop at the first step.This corresponds to the case where \(W_{}()=W_{1}()=W_{0}() W_{ }()\) on \(\). This is the case if and only if \(W_{}\) satisfies that

\[[_{1},_{2}]()_{1}()_{2}()- _{2}()_{1}() W_{}(),,_{2} W_{}$ and all $$.}\] (13)

i.e., when Frobenius Theorem (see Theorem E.1 in Appendix E) applies directly. The first example is a follow-up to Example 2.11.

_Example 3.5_ (two-layer ReLU networks without bias).: Consider \(=(U,V)\) with \(U^{n r},V^{m r},n,m,r 1\) (so that \(D=(n+m)r\)), and the reparametrization \(()(u_{i}v_{i}^{})_{i=1,,r}^{n m  r},\) where \(U=(u_{1};;u_{r})\) and \(V=(v_{1};;v_{r})\). As detailed in Appendix F.1, since \(()\) is a collection of \(r\) rank-one \(n m\) matrices, \((W_{}())=()=(n+m-1)r\) is constant on the domain \(\) such that \(u_{i},v_{j} 0\), and \(W_{}\) satisfies (13), hence by Theorem 3.3 each \(\) has a neighborhood \(^{}\) such that there exists \(r\) (and no more) independent conserved function through \(W_{|^{}}\). The \(r\) known conserved functions  given by \(h_{i}:(U,V)\|u_{i}\|^{2}-\|v_{i}\|^{2}\), \(i=1,,r\), are independent, hence they are complete.

Example where the iterations of Lemma 3.2 stop at the second step (but not the first one).Our primary example is matrix factorization, as a follow-up to Example 2.10.

_Example 3.6_ (two-layer _linear_ neural networks).: With \(=(U,V)\), where \((U^{n r},V^{m r})\) the reparameterization \(() UV^{}^{n m}\) (here \(d=nm\)) factorizes the functions minimized during the training of linear two-layer neural networks (see Example 2.10). As shown in Appendix I, condition (13) is not satisfied when \(r>1\) and \((n,m)>1\). Thus, the stopping criterion of Lemma 3.2 is not satisfied at the first step. However, as detailed in Proposition H.3 in Appendix H, \((W_{})_{1}=(W_{})_{2}=(W_{})\), hence the iterations of Lemma 3.2 stop at the second step.

We complete this example in the next section by showing (Corollary 4.4) that known conservation laws are indeed complete. Whether known conservation laws remain valid and/or _complete_ in this settings and extended ones is further studied in Section 4 and Appendix F.

### Application: recasting over-parameterized flows as low-dimensional Riemannian flows

As we now show, one striking application of Theorem 3.3 (in simple cases where \((W_{}())=(W_{}())\) is constant on \(\), i.e., \((())\) is constant on \(\) and \(W_{}\) satisfies (13)) is to fully rewrite the high-dimensional flow \((t)^{D}\) as a low-dimensional flow on \(z(t)((t))^{d}\), where this flow is associated with a Riemannian metric tensor \(M\) that is induced by \(\) and depends on the initialization \(_{}\). We insist on the fact that this is only possible in very specific cases, but this phenomenon is underlying many existing works that aim at writing in closed form the implicit bias associated with some training dynamics (see Section 1 for some relevant literature. Our analysis sheds some light on cases where this is possible, as shown in the next proposition.

**Proposition 3.7**.: _Assume that \((())\) is constant on \(\) and that \(W_{}\) satisfies (13). If \((t)^{D}\) satisfies the ODE (2) where \(_{}\), then there is \(0<T^{}_{_{}} T_{_{}}\) such that \(z(t)((t))^{d}\) satisfies the ODE_

\[(t)=-M(z(t),_{}) f(z(t))t[0,T^{}_{_{}}),z(0)=(_{}),\] (14)

_where \(M(z(t),_{})^{d d}\) is a symmetric positive semi-definite matrix._

See Appendix G for a proof. Revisiting Example 3.5 leads to the following analytic example.

_Example 3.8_.: Given the reparametrization \(:(u^{*},v^{d}) uv^{d}\), the variable \(z uv\) satisfies (14) with \(M(z,_{})=\|z\|_{}+\|z\|_{}^{-1}zz^{},\) with \(\|z\|_{}++\|z\|^{2}}\), \( 1/2(u_{}^{2}-\|v_{}\|^{2})\).

Another analytic example is discussed in Appendix G. In light of these results, an interesting perspective is to better understand the dependance of the Riemannian metric with respect to initialization, to possibly guide the choice of initialization for better convergence dynamics.

Note that the metric \(M(z,_{})\) can have a kernel. Indeed, in practice, while \(\) is a function from \(^{D}\) to \(^{d}\), the dimensions often satisfy \(()<(d,D)\), i.e., \(()\) lives in a manifold of lower dimension. The evolution (14) should then be understood as a flow on this manifold. The kernel of \(M(z,_{})\) is orthogonal to the tangent space at \(z\) of this manifold.

## 4 Conservation Laws for Linear and ReLU Neural Networks

To showcase the impact of our results, we show how they can be used to determine whether known conservation laws for linear (resp. ReLU) neural networks are complete, and to recover these laws _algorithmically_ using reparametrizations \(\) adapted to these two settings. Concretely, we study the conservation laws for neural networks with \(q\) layers, and either a linear or ReLU activation, with an emphasis on \(q=2\). We write \(=(U_{1},,U_{q})\) with \(U_{i}^{n_{i-1} n_{i}}\) the weight matrices and we assume that \(\) satisfies the gradient flow (2). In the linear case the reparametrization is \(_{}() U_{1} U_{q}\). For ReLU networks, we use the (polynomial) reparametrization \(_{}\) of [27, Definition 6], which is defined for any (deep) feedforward ReLU network, with or without bias. In the simplified setting of networks without biases it reads explicitly as:

\[_{}(U_{1},,U_{q})U_{1}[:,j_{1}]U_{2}[j _{1},j_{2}] U_{q-1}[j_{q-2},j_{q-1}]U_{q}[j_{q-1},:]_{j_{1}, ,j_{q-1}}\] (15)

with \(U[i,j]\) the \((i,j)\)-th entry of \(U\). This covers \(()(u_{j}v_{j}^{})_{j=1}^{r}^{n m r}\) from Example 2.11.

Some conservation laws are known for the linear case \(_{}\) and for the ReLu case \(_{}\).

**Proposition 4.1** ( ).: _If \((U_{1},,U_{q})\) satisfies the gradient flow (2), then for each \(i=1,,q-1\) the function \( U_{i}^{}U_{i}-U_{i+1}U_{i+1}^{}\) (resp. the function \((U_{i}^{}U_{i}-U_{i+1}U_{i+1}^{})\)) defines \(n_{i}(n_{i}+1)/2\) conservation laws for \(_{}\) (resp. \(n_{i}\) conservation laws for \(_{}\))._

Proposition 4.1 defines \(_{i=1}^{q-1}n_{i}(n_{i}+1)/2\) conserved functions for the linear case. In general they are _not_ independent, and we give below in Proposition 4.2, for the case of \(q=2\), the _exact_number of independent conservation laws among these particular laws. Establishing whether there are other (previously unknown) conservation laws is an open problem for \(q>2\). We already answered negatively to this question in the two-layer ReLu case without bias (See Example 3.5). In the following Section (Corollary 4.4), we show the same result in the linear case \(q=2\). Numerical computations suggest this is still the case for deeper linear and ReLU networks as detailed in Section 4.2.

### The matrix factorization case (\(q=2\))

To simplify the analysis when \(q=2\), we rewrite \(=(U,V)\) as a vertical matrix concatenation denoted \((U;V)^{(n+m) r}\), and \(()=_{}()=UV^{}^{n m}\).

How many independent conserved functions are already known?The following proposition refines Proposition 4.1 for \(q=2\) by detailing how many _independent_ conservation laws are already known. See Appendix H.1 for a proof.

**Proposition 4.2**.: _Consider \(:=(U;V) U^{}U-V^{}V^{r r}\) and assume that \((U;V)\) has full rank noted \(\). Then the function \(\) gives \((2r+1-)/2\) independent conserved functions._

There exist no more independent conserved functions.We now come to the core of the analysis, which consists in actually computing \((W_{})\) as well as its traces \((W_{})()\) in the matrix factorization case. The crux of the analysis, which enables us to fully work out theoretically the case \(q=2\), is that \(W_{}\) is composed of _linear_ vector fields (that are explicitly characterized in Proposition H.2 in Appendix H), the Lie bracket between two linear fields being itself linear and explicitly characterized with skew matrices, see Proposition H.3 in Appendix H. Eventually, what we need to compute is the dimension of the trace \((W_{})(U,V)\) for any \((U,V)\). We prove the following in Appendix H.

**Proposition 4.3**.: _If \((U;V)^{(n+m) r}\) has full rank noted \(\), then: \(((W_{})(U;V))=(n+m)r-(2r+1-)/2\)._

With this explicit characterization of the trace of the generated Lie algebra and Proposition 4.2, we conclude that Proposition 4.1 has indeed exhausted the list of independent conservation laws.

**Corollary 4.4**.: _If \((U;V)\) has full rank, then all conserved functions are given by \(:(U,V) U^{}U-V^{}V\). In particular, there exist no more independent conserved functions._

### Numerical guarantees in the general case

The expressions derived in the previous section are specific to the linear case \(q=2\). For deeper linear networks and for ReLU networks, the vector fields in \(W_{}\) are non-linear polynomials, and computing Lie brackets of such fields can increase the degree, which could potentially make the generated Lie algebra infinite-dimensional. One can however use Lemma 3.2 and stop as soon as \(((W_{})_{k}())\) stagnates. Numerically comparing this dimension with the number \(N\) of independent conserved functions known in the literature (predicted by Proposition 4.1) on a sample of depths/widths of small size, we empirically confirmed that there are no more conservation laws than the ones already known for deeper linear networks and for ReLU networks too (see Appendix J for details). Our code is open-sourced and is available at . It is worth mentioning again that in all tested cases \(\) is polynomial, and there is a maximum set of conservation laws that are also polynomial, which are found algorithmically (as detailed in Section 2.5).

## Conclusion

In this article, we proposed a constructive program for determining the number of conservation laws. An important avenue for future work is the consideration of more general classes of architectures, such as deep convolutional networks, normalization, and attention layers. Note that while we focus in this article on gradient flows, our theory can be applied to any space of displacements in place of \(W_{}\). This could be used to study conservation laws for flows with higher order time derivatives, for instance gradient descent with momentum, by lifting the flow to a higher dimensional phase space. A limitation that warrants further study is that our theory is restricted to continuous time gradient flow. Gradient descent with finite step size, as opposed to continuous flows, disrupts exact conservation. The study of approximate conservation presents an interesting avenue for future work.

[MISSING_PAGE_FAIL:11]

* D. Kunin, J. Sagastuy-Brena, S. Ganguli, D. L. Yamins, and H. Tanaka, _Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics_, arXiv preprint arXiv:2012.04728, (2020).
* Z. Li, T. Wang, J. D. Lee, and S. Arora, _Implicit bias of gradient descent on reparametrized models: On equivalence to mirror descent_, in Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, eds., vol. 35, Curran Associates, Inc., 2022, pp. 34626-34640.
* S. Marcotte, R. Gribonval, and G. Peyre, _Code for reproducible research. Abide by the Law and Follow the Flow: Conservation Laws for Gradient Flows_, Oct. 2023.
* H. Min, S. Tarmoun, R. Vidal, and E. Mallada, _On the explicit role of initialization on the convergence and implicit bias of overparametrized linear networks_, in Int. Conf. on Machine Learning, PMLR, 2021, pp. 7760-7768.
* W. F. Newns, _Functional dependence_, The American Mathematical Monthly, 74 (1967), pp. 911-920.
* B. Neyshabur, _Implicit regularization in deep learning_, arXiv preprint arXiv:1709.01953, (2017).
* B. Neyshabur, R. Tomioka, and N. Srebro, _In search of the real inductive bias: On the role of implicit regularization in deep learning_, arXiv preprint arXiv:1412.6614, (2014).
* E. Noether, _Invariante variationsprobleme_, Nachrichten von der Gesellschaft der Wissenschaften zu Gottingen, Mathematisch-Physikalische Klasse, 1918 (1918), pp. 235-257.
* A. M. Saxe, J. L. McClelland, and S. Ganguli, _Exact solutions to the nonlinear dynamics of learning in deep linear neural networks_, arXiv preprint arXiv:1312.6120, (2013).
* S. Shalev-Shwartz and S. Ben-David, _Understanding machine learning: From theory to algorithms_, Cambridge university press, 2014.
* D. Soudry, E. Hoffer, M. S. Nacson, S. Gunasekar, and N. Srebro, _The implicit bias of gradient descent on separable data_, The Journal of Machine Learning Research, 19 (2018), pp. 2822-2878.
* P. Stock and R. Gribonval, _An Embedding of ReLU Networks and an Analysis of their Identifiability_, Constructive Approximation, (2022). Publisher: Springer Verlag.
* S. Tarmoun, G. Franca, B. D. Haeffele, and R. Vidal, _Understanding the dynamics of gradient flow in overparameterized linear models_, in Int. Conf. on Machine Learning, PMLR, 2021, pp. 10153-10161.
* The Sage Developers, _SageMath, the Sage Mathematics Software System (Version 9.7)_, 2022. https://www.sagemath.org.
* C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals, _Understanding deep learning requires rethinking generalization_, in Int. Conf. on Learning Representations, 2017.
* B. Zhao, I. Ganev, R. Walters, R. Yu, and N. Dehmamy, _Symmetries, flat minima, and the conserved quantities of gradient flow_, arXiv preprint arXiv:2210.17216, (2022).