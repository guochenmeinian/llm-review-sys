# Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models

Jiayu Wang\({}^{1}\) Yifei Ming\({}^{2}\) Zhenmei Shi\({}^{1}\) Vibhav Vineet\({}^{3}\)

Xin Wang\({}^{3}\) Yixuan Li\({}^{1}\) Neel Joshi\({}^{3}\)

\({}^{1}\)University of Wisconsin-Madison \({}^{2}\)Salesforce AI Research

{milawang,zhmeishi,sharonli}@cs.wisc.edu

yifei.ming@salesforce.com

{vibhav.vineet,wanxin,neel}@microsoft.com

The work was completed in part during Yifei Ming's internship at Microsoft Research, as well as PhD thesis research at UW-Madison.

###### Abstract

Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance across a wide range of tasks and domains. Despite this promise, spatial understanding and reasoning--a fundamental component of human cognition--remains under-explored. We propose SpatialEval, a novel benchmark that covers diverse aspects of spatial reasoning such as relationship understanding, navigation, and counting. We conduct a comprehensive evaluation of competitive language and vision-language models. Our findings reveal several counter-intuitive insights that have been overlooked in the literature: (1) Spatial reasoning poses significant challenges where competitive models can fall behind random guessing; (2) Despite additional visual input, VLMs often under-perform compared to their LLM counterparts; (3) When both textual and visual information is available, multi-modal language models become less reliant on visual information if sufficient textual clues are provided. Additionally, we demonstrate that leveraging redundancy between vision and text can significantly enhance model performance. We hope our study will inform the development of multimodal models to improve spatial intelligence and further close the gap with human intelligence. Our code is available at https://github.com/jiayuww/SpatialEval.

## 1 Introduction

The recent breakthroughs in foundation models have had a transformative effect on research and industry, and we have seen these models rapidly integrated into products and new businesses that are shaping people's lives for the better. This sea change was initially driven by large language models (LLMs), which have shown at times near unbelievable, human-level performance across a wide range of tasks. Over the past year, many of these models have been extended to handle images in addition to text, leading to a significant increase in vision-language models (VLMs), especially multimodal large language models (MLLMs), which demonstrate groundbreaking performance in image-related tasks as in the text domain.

However, the reality is not quite as rosy as advertised. While these models have been instrumental in advancing the state-of-the-art in complex reasoning tasks such as common sense reasoning, mathematical problem solving, and scientific question answering [17; 30; 41; 72; 75], they have not been as effective for a number of problem domains. In particular, as we will show in this paper, they have limited performance on tasks that require detailed visual understanding and reasoning of images.

Visual understanding and reasoning--an intrinsic part of human perception and cognitive ability--have been largely under-explored when it comes to LLMs and VLMs. In fact, it is often argued that the visual sense is the dominant sense in people, yet when it comes to current models, it seems quite secondary. Spatial reasoning, in particular, is fundamental to everyday human activities such as navigating environments, understanding maps, and manipulating objects. It encompasses skills that are crucial for both survival and higher-order cognition, including the ability to navigate through space, recognize patterns, and deduce relationships from spatial configurations.

In this paper, we propose **SpatialEval**, a novel benchmark containing four tasks (Spatial-Map, Maze-Nav, Spatial-Grid, and Spatial-Real, Section 2.1) to explore the performance of LLMs and VLMs on diverse aspects of spatial reasoning, including relationship, navigation, position understanding, and object counting. Humans excel at such tasks, making them essential capabilities for intelligent systems to emulate for safe and effective deployment in the real world.

Our dataset, however, is constructed with a key twist - each problem in our benchmark has an image and a text representation that is sufficient for answering each spatial understanding question. We denote the use of these sources as VQA, which is the standard task of visual-question answering that consists of a vision-only input and a question, TQA, text-only input and a question, and VTQA, a combination of the previous with vision and text input.

We conduct a systematic and comprehensive evaluation of a wide range of open-source and proprietary LLMs and VLMs. We perform in-depth analysis and unveil several novel and surprising results that challenge the current understanding of how these models process spatial information:

* **Spatial reasoning remains challenging**: VLMs frequently struggle with spatial reasoning tasks, with some competitive models performing worse than random guessing.
* **Visual vs. textual inputs**: Without detailed textual descriptions, multimodal models rarely surpass the performance of their LLM backbones when relying solely on visual inputs. This underscores the critical role of text in enhancing model performance on spatial reasoning.
* **Reduced reliance on visual information**: When both textual and visual inputs are provided, multimodal language models tend to rely less on the visual component if sufficient textual clues are available.
* **Textual performance of VLMs**: VLMs often outperform their LLM counterparts with text-only inputs, indicating that the language model backbones within VLMs benefit from multimodal training, despite a lack of similar benefits from the visual components.

We surmise the limitations in VLMs' spatial understanding stem from the overly simplistic handling of visual information in current architectures and training pipelines. We believe the contributions of this paper will drive changes in model design, accelerating improvements that could unlock more robust spatial reasoning capabilities, and help bridge the gap toward human-like intelligence.

## 2 Dataset and Task Construction

### Dataset Setup

To evaluate the spatial reasoning abilities of LLMs and VLMs, we construct four diverse tasks including spatial relationships, navigation, position understanding, and counting. To systematically study the impact of modality, we design three types of input formats for each task: (1) TQA (Text-only): the input is purely textual and contains all necessary information for a person to answer the questions. (2) VQA (Vision-only): the input consists solely of an image, which provides sufficient details for a person to easily answer, a format also referred to as Visual Question Answering (VQA) in the literature. (3) VTQA (Vision-text): the input includes both an image and its textual representation with detailed descriptions, rendering the information in both modalities redundant. We evaluate LLMs using Text-only inputs and VLMs using Text-only, Vision-only, and Vision-text inputs on the same set of questions (Table 1). The synthetic tasks are curated based on the following key guidelines: (1) Avoidance of data leakage--since LLMs are pre-trained on web-scale data, it is crucial to ensure that the test data have not been seen during training; (2) configurability--being configurable allows for controlled experiments and extends easily to additional tasks; (3) scalability--the ability to scale the number of test samples enhances the statistical significance of experimental results.

**Spatial-Map.** Understanding the spatial relationships among objects on a map is a fundamental aspect of human cognitive abilities. To simulate this environment, we create a map-like dataset termed Spatial-Map with \(K\) objects, where \(K\) is configurable. Each object is associated with a unique location name, such as Unicorn Umbrellas and Gale Gifts. To study the impact of modality, the textual representation of each input consists of pairwise relations such as Brews Brothers Pub is to the Southeast of Whale's Watches. An example with \(K=6\) is shown in Figure 1, with Text-only, Vision-only, and Vision-text inputs. These questions include asking about the spatial relationships between two locations and the number of objects that meet specific spatial criteria.

**Maze-Nav.** Navigation through complex spaces is essential for intelligent systems. To evaluate such abilities, we have developed a maze-like dataset named Maze-Nav. Visually, each sample can be

Figure 1: Illustration of the Spatial-Map task, which simulates a map with multiple locations. To investigate the impact of modality, we consider three input formats: Text-only, Vision-only, and Vision-text. We evaluate language models (w. TQA input) and vision-language models (w. VQA and VTQA inputs) on the same set of questions.

Figure 2: Illustration of the Maze-Nav task, which evaluates the modelâ€™s ability to navigate from the starting point (S) to the exit (E).

represented as colored blocks where different colors signify distinct elements: a green block marks the starting point (S), a red block indicates the exit (E), black blocks represent impassable walls, white blocks denote navigable paths, and blue blocks trace the path from S to E. The objective is to navigate from S to E following the blue path, with movement permitted in the four cardinal directions (up, down, left, right). Alternatively, each input can be depicted in a textual format using ASCII code. An example is illustrated in Figure 2, featuring Text-only, Vision-only, and Vision-text inputs. We construct this task based on an open-sourced library . The questions asked include counting the number of turns from S to E and determining the spatial relationship between S and E. While such questions are easy for humans, we will show in Section 3 that they still pose significant challenges for modern multimodal language models.

**Spatial-Grid.** To investigate spatial understanding within structured environments, we introduce a grid-like dataset named Spatial-Grid, contrasting with the Spatial-Map where objects are positioned arbitrarily. Visually, each input consists of a grid of cells, each containing an image (_e.g.,_ a rabbit). An example is illustrated in Figure 3. Alternatively, this grid can also be represented in a purely textual format; for instance, the first row might be described as: elephant | cat | giraffe | elephant | cat. The evaluations focus on tasks such as counting specific objects (_e.g.,_ rabbits) and identifying the object located at a specific coordinate in the grid (_e.g.,_ first row, second column).

**Spatial-Real.** To extend the evaluation of spatial reasoning beyond synthetic environments, we introduce Spatial-Real, a task built on the Densely Captioned Images (DCI) dataset , where each image has a detailed caption with more than 1,000 words on average. As DCI does not contain questions, we curate multiple-choice questions regarding spatial reasoning (object counting, relation, and position understanding) and annotate the answers. An example is shown in Figure 4. We provide detailed analysis on Spatial-Real in Appendix E.

### Models

We consider a wide range of competitive open-source language models with different scales, including Phi2-2.7B , the LLaMA family (LLaMA-2-7B, LLaMA-2-13B, and LLaMA-3-8B) , Mistral-7B , the Vicuna family (Vicuna-7B-1.5 and Vicuna-13B-1.5) , and Nous-Hermes-2-Yi-34B. For multimodal language models, we consider the Bunny family (Bunny-Phi-2-SigLIP, Bunny-Phi-1.5-SigLIP, Bunny-Phi-2-EVA, and Bunny-Phi-1.5-EVA) , CogVLM , CogAgent , InstructBLIP family (InstructBLIP-Vicuna-7B and InstructBLIP-Vicuna-13B) , and LLaVA family (LLaVA-1.6-Mistral-7B, LLaVA-1.6-Vicuna-7B, LLaVA-1.6-Vicuna-13B, and LLaVA-1.6

Figure 3: Illustration of the Spatial-Grid task, which evaluates the modelâ€™s spatial reasoning ability in a rigid grid structure.

34B) . We also evaluate the proprietary models: Open AI's GPT-4V, GPT-4o, GPT-4, Google Gemini Pro 1.0, and Anthropic Claude 3 Opus.

**Evaluation.** As each question contains four options, we use accuracy as the main evaluation metric. The same user prompt is appended at the end of each question: _First, provide a concise answer in one sentence. Then, elaborate on the reasoning behind your answer in a detailed, step-by-step explanation._ For each model, we adopt the default configurations and decoding strategies, _e.g.,_argmax for deterministic decoding and top-\(p\) for non-deterministic decoding. For non-deterministic decoding, the results are averaged over three independent runs for open-source models. For proprietary models, due to their limited availability and increased compute time and cost, we only perform one run. We summarize the terminologies regarding input modalities for LLMs and VLMs in Table 1:

We describe the Text-only, Vision-only, and Vision-text input modalities based on how we feed the image information to the models. Vision-only input means the image is fed directly to the models without textual description, while all questions are presented in text.

## 3 Main Results and Analysis

**Spatial reasoning remains surprisingly challenging.** The evaluation results on open-source models on Spatial-Map, Maze-Nav, and Spatial-Grid are shown in Figure 5. For each task, the reported accuracy is averaged over all questions. For vision-language models, we choose the Vision-only input format, commonly used in Visual Question Answering (VQA). We use a dashed red line in each figure to indicate the expected accuracy if answering by random guessing. Our findings reveal several notable insights: **(1)** Vision-only inputs: Despite the simplicity of these tasks for humans, most competitive multimodal models perform at levels similar to or barely above random guessing.

  
**Model** & **Input Modality** & **Term** & **Description** \\  LLM & Text-only & TQA (LLM) & Text-only input that includes all necessary information to answer questions without visual context. \\ VLM & Text-only & TQA (VLM) & Text-only input as in TQA (LLM) but applied to VLMs (_e.g.,_ the LLaVA family). \\ VLM & Vision-only & VQA & Input only includes an image without corresponding textual description. \\ VLM & Vision-text & VTQA & Input includes both an image and its textual description. \\   

Table 1: Terms regarding input modalities for LLMs and VLMs.

Figure 4: Illustration of the Spatial-Real task, which is built on real images with long captions, featuring detailed descriptions averaging over 1,000 words per image.

**(2)** Text-only inputs: While the textual input includes essential spatial information, it generally does not significantly enhance the spatial reasoning capabilities of competitive models. An exception occurs in the Spatial-Grid task, where Llama-3 achieves an accuracy of 71.9%, followed by Mistral-7B-Instruct at 62.1%, both notably surpassing random guessing. Despite these successes, the performance of these models still lags significantly behind human levels. These results underscore the need for further development of techniques tailored to spatial understanding and reasoning.

The impact of input modality.To investigate the impact of modality, we compare the performance of a large language model (LLM) and a vision-language model (VLM) with the same language backbone. We consider the VQA (Vision-only) format for vision-language models. The results are shown in Figure 6. Each vertex on the spider plot represents the average accuracy of a (VLM, LLM) pair. We observe that on Spatial-Map and Spatial-Grid, the majority of VLMs yield worse performance compared to their LLM counterpart, despite having an additional visual encoder. For example, on Spatial-Grid, Mistral-7B achieves an average accuracy of 62.1%, while LLaVA-v1.6-Mistral-7B only yields an accuracy of 47.1% (15% \(\)). Detailed results can be seen in Appendix F.

Figure 5: Performance overview on spatial reasoning tasks. We report the accuracy averaged over all questions. We consider the VQA (Vision-only) format for vision-language models. The dashed red line denotes the expected accuracy for random guessing. For Spatial-Map and Maze-Nav tasks, only a few models outperform random guessing by a notable margin.

Figure 6: TQA (LLM) vs. VQA on spatial reasoning tasks. Each vertex on the spider plot represents the Avg Acc of a (VLM, LLM) pair with the same language backbone, i.e., LLM v.s. VLM further finetuned on that. VLMs are depicted in red, and LLMs in blue. We can see that VLMs rarely enhance the performance compared to their LLM counterparts.

## 4 Delving Into Spatial Reasoning for Vision-Language Models

### Seeing Without Understanding: The Blindness of Multimodal Language Models

To better understand how VLMs process visual information, we conduct a series of controlled experiments in the VTQA (Vision-text input) setting. For each sample, we replace the original image input (that matches the textual description) with either: (1) _No Image_: only keep the textual input without the image input, (2) _Noise Image_: a Gaussian noise image irrelevant to the task, and (3) _Random Image_: a random image from the dataset that does not match the textual description, as shown in Figure 7.

**VLMs exhibit improved performance when visual input is absent.** We conducted experiments by entirely removing the _Original Image_ and relying solely on the textual description. The results are shown in Figure 8. For each task, we report the accuracy averaged across all questions. Remarkably, the absence of visual input leads to better performance across a range of VLM architectures. For instance, the performance of LLaVA-1.6-34B on the Spatial-Grid task improved by 20.1% when no image was presented compared to scenarios with the original image. This observation underscores that when textual information alone can address the questions, additional visual inputs do not necessarily enhance, and may even hinder the performance, a sharp contrast to human capabilities where visual cues significantly aid in understanding. The removal of visual input forces the models to utilize the textual information to solve spatial reasoning tasks.

**Noise image can improve the performance.** We replace the _Original Image_ with a _Noise Image_ while retaining the original textual description. The results are shown in Figure 9. Consistent with

Figure 8: VTQA vs. TQA (VLM) on spatial reasoning tasks. VLMs exhibit improved performance in spatial reasoning tasks when visual input is absent.

Figure 7: Illustration of _Random Image_ and _Noise Image_ for the example in Figure 2.

Figure 9: Original Image vs. Noise Image in VTQA. Replacing the original image with a Gaussian noise image improves the performance across diverse VLM architectures.

the findings in Original Image vs. No Image, using a noise image also improves the performance across various VLM architectures. For example, the accuracy of LLAVA-1.6-Vicuna-13B increases by 6.5% on the Maze-Nav task when the noise image is used as opposed to the original image. In contrast to the _No Image_ setting, noise images provide limited visual cues. Nonetheless, the model tends to prioritize the textual information, especially when visual cues are not pertinent to the task.

Mismatched image-text does not necessarily hurt.To build on previous findings, we further investigate the effects of replacing the _Original Image_ with a _Random Image_ (illustrated in Figure 7). Unlike a noise image, a random image is task-related but may provide conflicting information compared to the textual description. Intuitively, one might expect that such random images would degrade VLM performance due to contradictory cues. However, as demonstrated in Figure 10, this expectation does not always hold true. For instance, _Random Image_ in the Maze-Nav task leads to improved performance across various VLM architectures. This outcome implies that VLMs are not heavily reliant on visual information, particularly when adequate textual clues are provided.

### Leveraging Redundancy in Multimodal Inputs

Multimodal language models offer considerable versatility in handling multimodal inputs. While the visual input alone often provides sufficient details for humans to address spatial reasoning tasks with ease, we propose that VLMs significantly benefit from the inclusion of textual descriptions alongside visual data, even if this introduces substantial redundancy. We verify this hypothesis by comparing VQA (Vision-only input) and VTQA (Vision-text input) across diverse VLM architectures. The results are shown in Figure 11, where each vertex on the spider plot represents the average accuracy of a (Vision-only, Vision-text) pair based on the same VLM. For Spatial-Map and Spatial-Grid, we can clearly see that having the additional textual input (VTQA) enhances the performance compared

Figure 11: VQA vs. VTQA on spatial reasoning tasks. Each vertex on the spider plot represents the Avg Acc of a (Vision-only, Vision-text) pair with the same VLM model. We can see that having the additional textual input (VTQA) enhances the performance compared to only using images (VQA).

Figure 10: Original Image vs. Random Image in VTQA. On Maze-Nav, replacing the original image with a random image leads to performance improvement across diverse VLM architectures.

to only using images (VQA) across different VLM architectures. This suggests that textual inputs improve the accuracy of spatial reasoning in VLMs. We further compare TQA and VTQA in Appendix D. Detailed results are included in Appendix F.

**Text-only input with LLM vs. Text-only input with VLM.** Given the demonstrated efficacy of text-only inputs, we conducted an ablation study to compare LLMs and VLMs using text-only inputs. We consider VLMs that are capable of processing text without accompanying visual data. The results are illustrated in Figure 12. Except for CoSVLM, the majority of VLMs outperform their corresponding LLM backbones. This suggests that the language model backbones in VLMs demonstrate enhanced spatial reasoning abilities through multimodal learning. Conversely, the addition of visual information does not necessarily provide further benefits.

### Proprietary vs. Open-Source Models

As many recent benchmarks have shown that proprietary models generally outperform open-source models, it is important to understand if our observed trends hold with proprietary models. The performance of several top proprietary models (GPT-4, GPT-4V, GPT-4o, Gemini Pro 1.0, and Claude 3 Opus) are shown in Figure 13. We have the following salient observations: (1) A significant performance gap exists between SoTA open-source models and proprietary models, as expected. Furthermore, with both Text-only and Vision-text formats, GPT-4V and GPT-4o significantly outperform random guessing across all tasks. For instance, in the Vision-text format, GPT-4o achieves

  
**Comparison** & **Results** & **Summary of Findings** \\  TQA (LLM) vs. VQA & Figure 6 & VQA rarely enhances the performance compared to TQA (LLM). \\ VTQA vs. TQA (VLM) & Figure 8 & VLMs exhibit improved performance in spatial reasoning tasks \\  & & when the image input is absent. \\ VQA vs. VTQA & Figure 11 & Given the same image input, additional textual description enhances VLMâ€™s performance. \\ TQA (VLM) vs. TQA (LLM) & Figure 12 & Multimodal fine-tuning enhances LLMâ€™s spatial reasoning ability. \\ TQA (LLM) vs. VTQA & Figure 16 & No definitive winner. \\   

Table 2: Summary of main findings.

Figure 12: Comparison of Text-only input with LLM (TQA) vs. Text-only input with VLM (No Img). We consider VLMs that support text-only inputs. Each vertex on the spider plot represents the Avg Acc of a (LLM, VLM) pair with the same language model backbone.

Figure 13: Results with proprietary models. Similar trends are observed as with open-source models.

an accuracy of 0.989 on Spatial-Grid (Table 7). (2) Yet, the trends we observed with open source models hold, VQA consistently under-performs compared to TQA and VTQA, for example, GPT-4V's performance improves by 25.6% on Spatial-Grid when switching from Vision-only to Vision-text input; and again no clear winner between TQA and VTQA (see Appendix D for further details), showing that proprietary models, even the new GPT-4o model, still do not appear to fully leverage visual inputs. We summarize the key findings of this work in Table 2.

## 5 Related Work

**Large language models.** Large language models (LLMs) have achieved outstanding performance across a diverse array of fields, including finance , bioinformatics , law , education , coding , and creative tasks [2; 36]. LLM architectures have gone through significant changes in recent years, with notable developments such as BERT , OPT , PaLM , Gamma family , Mistral family , GPT family [2; 10], Claude family , and LLaMA family [3; 64; 66]. These models have demonstrated emergent abilities and revolutionized numerous domains, supporting capabilities such as in-context learning [46; 51; 59], compositional reasoning [16; 20; 70], and task-specific adaptation [57; 58; 71]. However, visual understanding and reasoning--an intrinsic part of human cognitive ability--remains largely under-explored for LLMs.

**Vision-language models and multi-modal language models.** The success of LLMs has propelled the adoption of the Transformer architecture  within the computer vision community, such as ViT , Beit , CLIP , MAE , Swin [42; 43], and DiT . Building on the capabilities of powerful LLMs, multi-modal language models (MLLMs) such as Flamingo , LLaMA-Adapter [19; 74], LLava [37; 39], stable-diffusion , BLIP [32; 33], MiniGPT-4 , Qwen [7; 8], Gemini , MM1  have significantly expanded the range of problems that can be addressed with improved reliability . These models adeptly handle inputs from diverse modalities and have demonstrated remarkable performance on diverse tasks such as mathematical reasoning , image-text retrieval [47; 73], and visual reasoning [6; 18; 25; 28; 31; 40; 50].

**Spatial understanding and reasoning.** Spatial reasoning entails comprehending and manipulating spatial relationships, a task significantly more challenging than visual grounding [1; 28; 52]. Although progress in natural language processing, evaluations, and benchmarks on LLMs such as GPT-4  and Claude3  have predominantly focused on textual or relational reasoning. This focus often overlooks the intricate nature of spatial reasoning tasks. Notably, recent studies [17; 30; 41; 48; 56; 72; 75] have conducted thorough evaluations across a diverse range of tasks. Yet, they demonstrate a scant exploration of spatial reasoning capabilities, highlighting a gap in assessing this complex cognitive skill in current benchmarks. In particular, Zhang _et al._ suggest that MLLMs primarily leverage textual cues rather than visual diagrams to solve math problems while focusing only on math problems. Yamada _et al._ design simple navigation tasks and finds LLMs appear to capture certain aspects of spatial structure implicitly, but room for improvement remains, while our benchmarks are more complicated than their navigation tasks based on simple geometry maps. Fu _et al._ propose a benchmark on science and games with limited exploration related to spatial reasoning, while we are dedicated diverse spatial reasoning tasks with in-depth analysis.

## 6 Discussion and Conclusions

We explored the spatial understanding capabilities of VLMs and LLMs. Our experiments resulted in several surprising conclusions across SoTA open-source and proprietary models. (1) VLMs struggle with spatial reasoning tasks, (2) multimodal models rarely surpass LLMs when relying on visual inputs, (3) when both textual and visual inputs are provided, multimodal language models rely less on the visual inputs, and (4) VLMs often outperform their LLM counterparts with text-only inputs. This challenges the belief that current VLMs are highly performant at a wide range of vision-text tasks. However, with further thought, perhaps this is to be expected after all. The currently known architectures for VLMs attempt to "translate" the vision input into the language space and all reasoning is then performed in the language domain. It is logical that this automatic translation path is worse than a human provided translation to text, as in our text-only scenario. Thus our work shows the limits of the translation approach. Instead, to bridge the gap to human performance, future models require new architectures that treat vision input as a first-class source of information and reason in a joint vision-language space. It is our hope that our work informs development on this path.