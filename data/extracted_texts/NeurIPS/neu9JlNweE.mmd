# Post-processing Private Synthetic Data

for Improving Utility on Selected Measures

 Hao Wang, Shivchander Sudalairaj, John Henning,

**Kristjan Greenewald, Akash Srivastava**

MIT-IBM Watson AI Lab

email: [hao, shiv.sr, john.l.henning, kristjan.h.greenewald, akash.srivastava]@ibm.com

###### Abstract

Existing private synthetic data generation algorithms are agnostic to downstream tasks. However, end users may have specific requirements that the synthetic data must satisfy. Failure to meet these requirements could significantly reduce the utility of the data for downstream use. We introduce a post-processing technique that improves the utility of the synthetic data with respect to measures selected by the end user, while preserving strong privacy guarantees and dataset quality. Our technique involves resampling from the synthetic data to filter out samples that do not meet the selected utility measures, using an efficient stochastic first-order algorithm to find optimal resampling weights. Through comprehensive numerical experiments, we demonstrate that our approach consistently improves the utility of synthetic data across multiple benchmark datasets and state-of-the-art synthetic data generation algorithms.

## 1 Introduction

The advancement of machine learning (ML) techniques relies on large amounts of training data. However, data collection also poses a significant risk of exposing private information. In recent years, several instances of privacy breaches have surfaced , making it urgent to find a reliable way to share data. Today, the de facto standard for privacy protection is differential privacy (DP) . DP ensures that useful information of a private dataset can be released while simultaneously preventing adversaries from identifying individuals' personal data. DP has been utilized effectively in a wide variety of settings, by actors including the US Census  and various large corporations .

Generating private synthetic data is a crucial application of DP. It allows data scientists to train their ML models on the synthetic data while preserving a certain level of utility when deploying these models on real test data. The U.S. National Institute of Standards and Technology (NIST) has emphasized its significance by hosting a series of competitions . There is also significant work introducing new DP synthetic data generation mechanisms, including GAN-based , marginal-based , and workload-based  methods. Existing methods for generating private synthetic data are task-agnostic--they do not take into account the downstream use cases of the synthetic data in the data generation process. However, end users often have specific requirements for synthetic datasets to be successfully analyzed by their existing data science pipelines, which are often well-established, well-understood, heavily vetted, and difficult or expensive to change. Unfortunately, synthetic data generated by existing methods may not always meet these requirements, resulting in reduced utility for their downstream use cases. This raises a fundamental question:_Is it possible to improve a synthetic dataset's utility on a set of selected measures while preserving its differential privacy guarantees?_

In this paper, we introduce a post-processing procedure that enhances the quality of the synthetic data on a set of utility measures. These measures are usually based on the end user's requirements for the synthetic data. Our method solves an optimization problem to calculate the optimal resampling weights for each data point. We then use these weights to resample the synthetic dataset, eliminating data points that do not accurately reflect the real data based on the selected utility measures. This post-processing procedure has many remarkable properties. Firstly, it is model-agnostic, meaning that it does not depend on any assumptions about how the synthetic data were generated. The synthetic data could be generated using either GAN-based, marginal-based, or workload-based methods. Second, the post-processed data preserve the DP guarantees while maintaining the performance of downstream models on real data compared to models trained on the original synthetic data. Finally, our algorithm is highly scalable, especially for high-dimensional data. It only requires a convex optimization whose number of variables is equal to the number of desired utility measures. We develop a stochastic first-order method that can efficiently solve this optimization problem. As a reference, we apply our algorithm to the home-credit dataset  which consists of 307,511 data points and 104 features, and it only takes around 4 mins to finish running.

We conduct comprehensive numerical experiments to demonstrate the effectiveness of our post-processing techniques. Our results show that this algorithm consistently improves the utility of the synthetic data on different selected measures, across multiple datasets, and for synthetic data generated using various state-of-the-art privacy mechanisms. Additionally, the downstream models trained on the post-processed data can achieve comparable (or even better) performance on real data compared to the models trained on the original synthetic data. We demonstrate an application of our technique by using it to align the correlation matrix of the synthetic data with that of the real data. The correlation matrix shows the correlation coefficients between pairs of features and is often used as a reference for feature selection. By applying our post-processing technique, we can ensure that the correlation matrix of the synthetic data closely matches that of the real data. This, for instance, can help downstream data scientists more accurately select a task-appropriate set of features for fitting their ML models, e.g. by thresholding the correlation with the desired target variable or using Lasso techniques.

Our proof techniques rely on an information-theoretic formulation known as information projection, which dates back to Csiszar's seminal work . This formulation has recently been successfully applied in several domains, such as large deviation theory , hypothesis testing , and fair machine learning . Here we use information projection to "project" the empirical distribution of the synthetic data onto a set of distributions whose utility measures match those of the real data. We show that the optimal projected distribution can be expressed as an exponential tilting of the original distribution and determine the tilting parameters via a convex program in the dual space. To solve this optimization problem, we introduce a compositional proximal gradient algorithm, which is a stochastic first-order method and highly scalable for large dataset. Notably, this exponential tilting takes the form of a simple reweighting of the data points in the original synthetic dataset. Hence, our approach has the added benefit that it only reweights synthetic data points without changing their values, ensuring that the quality of the data points is not compromised and the support set of the overall dataset is not expanded.

In summary, our main contributions are:

* We present a post-processing technique that enables end users to customize synthetic data according to their specific requirements.
* Our technique is model-agnostic--it is applicable regardless of the methods used to generate private synthetic data. It is scalable to high-dimensional data and maintains strong privacy guarantees.
* We conduct comprehensive numerical experiments, showing that our technique can consistently improve the utility of synthetic data across multiple benchmark datasets and for synthetic data generated using state-of-the-art DP mechanisms.

The supplementary material of this paper includes: (i) omitted proofs of all theoretical results, (ii) details on algorithm implementation, and (iii) supporting experimental results.

### Related Work

DP synthetic data generation mechanisms.Generating synthetic data using DP mechanisms is an active area of research [see e.g., HLM12, BLR13, GAH\({}^{+}\)14, CXZX15, BSG17, AZK\({}^{+}\)19, UV20, GMHI20, TMH\({}^{+}\)21, VAA\({}^{+}\)22, BSV22]. Along this line of work, the closest ones to ours are workload-aware methods [VTB\({}^{+}\)20, ABK\({}^{+}\)21, MMSM22, LVW21], which aim to ensure that the synthetic data perform well on a collection of queries. Our work differs in three key features. First, all existing approaches focus on generating private synthetic data from scratch, while we aim to post-process synthetic data to downweight samples that do not accurately represent the real data under the specified utility measures. As a result, our approach is more efficient as it does not need to fit a graphical model [MMSM22] or a neural network [LVW21] to represent the data distribution, and it is more extendable, allowing a single synthetic dataset to be quickly post-processed multiple times for different sets of utility measures as needed by downstream users. Second, some existing work may not scale well to large datasets as they either require solving an integer program multiple times [VTB\({}^{+}\)20] or need to solve a large-scale optimization problem [ABK\({}^{+}\)21]. In contrast, our approach is highly scalable, as it only requires solving a convex program whose number of variables is equal to the number of specified utility measures. Third, existing work often evaluates the quality of the synthetic data by how well it preserves key statistics (e.g., 3-way marginals) of the real data. In contrast, in our experiments, we evaluate our approach on the more stringent and realistic test of training various downstream ML models on the synthetic data and measuring their performance on real test data. The experimental results demonstrate that our approach can enhance the utility of synthetic data on selected measures without compromising their downstream quality.

Public data assisted methods/Post-processing methods.Our work introduces a post-processing procedure for improving the utility of a given synthetic dataset based on selected measures. As a special case, our approach can be applied to post-process publicly available datasets. In this regard, this work is related to public-data-assisted methods [see e.g., LVS\({}^{+}\)21, LVW21], which leverage public data for saving privacy budgets. We extend Algorithm 1 in [LVS\({}^{+}\)21] by formalizing the preservation of (noisy) utility measures as a constrained optimization problem, rather than minimizing the corresponding Lagrangian function. We further establish strong duality and propose a stochastic first-order method for solving this constrained optimization efficiently. We extend Algorithm 4 in [LVW21] by allowing any non-negative violation tolerance (i.e., any \( 0\) in (3b)) compared with \(=0\) in [LVW21]. This extension offers increased flexibility, as users can now select various values of \(\) to navigate the trade-off between minimizing the distortion of the synthetic data and enhancing their utility on selected measures. Moreover, our experiments show that setting \(\) to be a small positive number (e.g., \(=1e-5\)) consistently outperforms when \(=0\). Finally, our work is related with [NWD20], which proposes to post-process outputs from differentially private GAN-based models for improving the quality of the generated synthetic data. However, their method is tailored to GAN-based privacy mechanisms while our approach is model-agnostic. This versatility is crucial, given that marginal-based and workload-based mechanisms often yield higher quality synthetic tabular data, as evidenced by benchmark experiments in [TMH\({}^{+}\)21]. Our experiments indicate that our method consistently improves the utility of synthetic data produced by all kinds of privacy mechanisms, even when the initial synthetic data are of high quality.

## 2 Preliminaries and Problem Formulation

In this section, we review differential privacy and provide an overview of our problem setup.

### Differential Privacy

We first recall the definition of differential privacy (DP) [DR14].

**Definition 1**.: A randomized mechanism \(:^{n}\) satisfies \((,)\)-differential privacy, if for any adjacent datasets \(\) and \(^{}\), which only differ in one individual's record, and all possible outputs from the mechanism \(\), we have

\[(()) e^{}(( ^{}))+.\] (1)

DP has two important properties: post-processing immunity and composition rule. Specifically, if \(:^{n}\) satisfies \((,)\)-DP and \(g:^{}\) is any randomized (or deterministic) function, then\(g:^{n}^{}\) is \((,)\)-DP; if \(=(_{1},,_{k})\) is a sequence of randomized mechanisms, where \(_{i}\) is \((_{i},_{i})\)-DP, then \(\) is \((_{i=1}^{k}_{i},_{i=1}^{k}_{i})\)-DP. There are also advanced composition theorems (see e.g., ).

The Laplace and Gaussian mechanisms are two fundamental DP mechanisms. For a function \(f:^{n}^{K}\), we define its \(L_{p}\) sensitivity for \(p\{1,2\}\) as

\[_{p}(f)_{^{}}\|f( )-f(^{})\|_{p},\] (2)

where \(\) and \(^{}\) are adjacent datasets. The Laplace mechanism adds Laplace noise \((,b)\) to the output of \(f\):

\[()=f()+.\]

In particular, if \(b=_{1}(f)/\), the Laplace mechanism satisfies \((,0)\)-DP. Similarly, the Gaussian mechanism adds Gaussian noise \( N(,^{2})\) to the output of \(f\). We recall a result from  that establishes a DP guarantee for the Gaussian mechanism and refer the reader to their Algorithm 1 for a way to compute \(\) from \((,)\) and \(_{2}(f)\).

**Lemma 1** ().: _For any \( 0\) and \(\), the Gaussian mechanism is \((,)\)-DP if and only if_

\[((f)}{2}-(f)} )-e^{}(-(f)}{2}-(f)}),\]

_where \(\) is the Gaussian CDF: \((t)}_{-}^{t}e^{-y^{2}/2}dy\)._

The function \(f\) can often be determined by a query \(q:\): \(f()=_{_{i}}q( {x}_{i})\). In this case, we denote \((q)_{x,x^{}}|q(x)-q(x^{})|\); \(q(P)_{ P}[q()]\) for a probability distribution \(P\); and \(q()_{_{i}}q( _{i})\) for a dataset \(\).

### Problem Formulation

Consider a dataset \(\) comprising private data from \(n\) individuals, where each individual contributes a single record \(=(x_{1},,x_{d})\). Suppose that a synthetic dataset \(_{}\) has been generated from the real dataset \(\), but it may not accurately reflect the real data on a set of utility measures (also known as workload). We express the utility measures of interest as a collection of queries, denoted by \(\{q_{k}:\}_{k=1}^{K}\), and upper bound their \(L_{p}\) sensitivity by \(_{p}()(_{k[K]}(q_{k})^{p})^{ 1/p}\).

Our goal is to enhance the utility of the synthetic dataset \(_{}\) w.r.t. the utility measures \(\) while maintaining the DP guarantees. We do not make any assumption about how these synthetic data have been generated--e.g., they could be generated by either GAN-based, marginal-based, or workload-based methods. We evaluate the utility measures from the real data via a \((_{},_{})\)-DP mechanism and denote their answers by \(=(a_{1},,a_{K})\). Next, we present a systematic approach to post-process the synthetic data, aligning their utility with the (noisy) answers \(\).

## 3 Post-processing Private Synthetic Data

In this section, we present our main result: a post-processing procedure for improving the utility of a synthetic dataset. We formulate this problem as an optimization problem that projects the probability distribution of the synthetic data onto a set of distributions that align the utility measures with the (noisy) real data. We prove that the optimal projected distribution can be expressed in a closed form, which is an exponentially tilted distribution of the original synthetic data distribution. To compute the tilting parameters, we introduce a stochastic first-order method and leverage the tilting parameters to resample from the synthetic data. By doing so, we filter out synthetic data that inaccurately represent the real data on the selected set of utility measures.

We use \(P_{}\) and \(P_{}\) to represent the (empirical) probability distribution of the synthetic data and the generating distribution of post-processed data, respectively. We obtain the post-processed distribution 

[MISSING_PAGE_FAIL:5]

demonstrate that downstream predictive models trained on the post-processed synthetic data can achieve comparable or even better performance on real data than those trained on the original synthetic data.

Finally, we establish a DP guarantee for the post-processing data. Since our algorithm only accesses the real data when computing the utility measures, the composition rule yields the following proposition.

**Proposition 1**.: _Suppose the synthetic data are generated from any \((,)\)-DP mechanism and the utility measures are evaluated from the real data from any \((_{},_{})\)-DP mechanism. Then the post-processed data satisfy \((+_{},+_{})\)-DP._

So far, we have introduced a general framework for post-processing synthetic data to improve their utility on selected measures. Next, we present a concrete use case of our framework: aligning the correlation matrix. This can be achieved by choosing a specific set of utility measures in our framework.

Aligning the correlation matrix.The correlation matrix comprises the correlation coefficients between pairs of features and is commonly used by data scientists to select features before fitting their predictive models. Aligning the correlation matrix of synthetic data with real data can assist data scientists in identifying the most suitable set of features and developing a more accurate model. Recall the definition of the correlation coefficient between features \(_{i}\) and \(_{j}\) for \(i,j[d]\):

\[(_{i},_{j})=[_{i}_{j}]-[_{i}] [_{j}]}{[_{i}^{2}]- [_{i}]^{2}}[_{j}^{2}]-[_{j}]^{2}}}.\]

Since the correlation coefficient only depends on the first and second order moments, we can select the following set of \((d+3)d/2\) queries as our utility measures:

\[=\{q_{i}()=x_{i}\}_{i[d]}\{q_{i,j}()=x_{i}x_{j} \}_{i j}.\]

Assume that each feature \(x_{i}\) is normalized and takes values within \(\). As a result, the \(L_{1}\) and \(L_{2}\) sensitivity of \(\) can be upper bounded:

\[_{1}()_{2}( )}{n}.\]

We can compute these utility measures using the Laplace or Gaussian mechanism, denoise the answers, compute the optimal dual variables, and resample from the synthetic data.

## 4 Numerical Experiments

We now present numerical experiments to demonstrate the effectiveness of our post-processing algorithm. Our results show that this algorithm consistently improves the utility of the synthetic data across multiple datasets and state-of-the-art privacy mechanisms. Additionally, it achieves this without degrading the performance of downstream models or statistical metrics. We provide additional experimental results and implementation details in Appendix C.

Benchmark.We evaluate our algorithm on four benchmark datasets: Adult, Bank, Mushroom, and Shopping, which are from the UCI machine learning repository . All the datasets have a target variable that can be used for a downstream prediction task. We use four existing DP mechanisms--AIM , MST , DPCTGAN , and PATECTGAN -- to generate private synthetic data. Among these DP mechanisms, AIM is a workload-based method; MST is a marginal-based method; DPCTGAN and PATECTGAN are GAN-based methods. All of their implementations are from the OpenDP library . To demonstrate the scalability of our approach, we conduct an additional experiment on the home-credit dataset . This dataset has 307,511 data points and 104 features. We apply GEM  to this high-dimensional dataset for generating synthetic data. We pre-process each dataset to convert categorical columns into numerical features and standardize all features to the range \(\).

We clarify that our objective is not to compare the quality of synthetic data generated by these DP mechanisms (for a benchmark evaluation, please refer to ), but to demonstrate how our post-processing method can consistently improve the utility of synthetic data generated from these different DP mechanisms.

Setup.We compare synthetic data produced by different privacy mechanisms with \(\{2,4\}\) against the post-processed synthetic data that are generated by applying our post-processing technique with \(_{}=1\) to synthetic data generated from privacy mechanisms with \(\{1,3\}\). For UCI datasets, we select \(5\) features, including the target variable, from the synthetic data that have the highest absolute correlation with the target variable. These features are chosen as they have a higher influence on downstream prediction tasks. The set of these features is denoted by \([d]\). Next, we define the utility measures as the first-order and second-order moment queries among the features in \(\): \(=\{q_{i}()=x_{i}\}_{i}\{q_{i,j}()=x_ {i}x_{j}\}_{i,j}\). For home-credit dataset, we follow the same setup but choosing \(10\) features.

We apply the Gaussian mechanism with \((_{}=1,_{}=1/n^{2})\) to estimate utility measures from the real data, where \(n\) denotes the number of real data points. Finally, we apply Algorithm 1 with \(=1e-5\), a batch size of 256 for UCI datasets and 4096 for home-credit, and 200 epochs to compute the optimal resampling weights, which are then used to resample from the synthetic data with the same sample size. We repeat our experiment 5 times to obtain an error bar for each evaluation measure.

Evaluation measure.We provide evaluation metrics for assessing the quality of synthetic data after post-processing. Our first metric, _utility improvement_, is defined as:

\[1-_{}(_{})}{_{}(_{})},_{}()=\|(_{ {real}})-()\|_{1}\{_{},_{}\}.\]

The range of this metric is \((-,1]\), where a value of \(1\) indicates a perfect alignment between the correlation matrix of the post-processed data and the real data.

To assess the downstream quality of synthetic data, we train predictive models using the synthetic data and then test their performance on real data. Specifically, we split the real data, using \(80\%\) for generating synthetic data and setting aside \(20\%\) to evaluate the performance of predictive models. We use the synthetic data to train a logistic regression classifier to predict the target variable based on other features. Then we apply this classifier to real data and calculate its F1 score. We use the function BinaryLogisticRegression from SDMetrics library  to implement this process.

We compute two statistical measures to quantify the similarity between synthetic and real data distributions. The first measure is the average Jensen-Shannon distance between the marginal distributions of synthetic and real data, which ranges from \(0\) (identical distribution) to \(1\) (totally different distributions). The second measure is the average inverse KL-divergence between the marginal distributions of synthetic and real data, which ranges from \(0\) (totally different distributions) to \(1\) (identical distribution). These statistical measures are implemented by using synthcity library .

**Result.** We present the experimental results on the UCI datasets in Table 1 (and Table 3 in Appendix C with a higher privacy budget). We report _utility improvement_, _F1 score_, and _statistical measures_. As shown, our post-processing technique consistently enhances the utility of synthetic data on selected metrics across all benchmark datasets and all privacy mechanisms. Moreover, the utility improvements are achieved without degrading the performance of downstream models or statistical properties of synthetic data. In other words, our post-processing procedure ensures that the logistic regression classifier trained on synthetic data can achieve comparable or even higher performance on real test data, while simultaneously reducing or maintaining the statistical divergences between synthetic and real data.

We present the experimental results on the home-credit dataset in Table 2 and illustrate the misalignment of the correlation matrix with and without applying our post-processing procedure in Figure 1. The results demonstrate that our algorithm consistently reduces the overall correlation misalignment. Additionally, our procedure, which includes computing the utility measures from real data, denoising the noisy answers, and computing optimal resampling weights, only takes around 4 mins on 1x NVIDIA GeForce RTX 3090 GPU.

    & & &  &  &  \\  Dataset & DP Mechanism & Utility Improv. & w/o post-proc. & w/ post-proc. & w/o post-proc. & w/ post-proc. & w/o post-proc. & w/ post-proc. \\  Adult & AIR & **0.13**\(\)0.03 & 0.61 & 0.61 \(\)0.0 & 0.01 & 0 \(\)0.0 & 0.99 & 1 \(\)0.0 \\   & MST & **0.22**\(\)0.02 & 0.55 & 0.56 \(\)0.0 & 0.01 & 0 \(\)0.0 & 1 & 1 \(\)0.0 \\   & DCTGAN & **0.81**\(\)0.09 & 0.22 & 0.33 \(\)0.02 & 0.07 & 0.03 \(\)0.02 & 0.85 & 0.91 \(\)0.0 \\   & PATECTGAN & **0.6**\(\)0.04 & 0.37 & 0.5 \(\)0.03 & 0.04 & 0.01 \(\)0.0 & 0.72 & 0.87 \(\)0.0 \\  Mushroom & AIR & **0.12**\(\)0.0 & 0.93 & 0.93 \(\)0.0 & 0.01 & 0 \(\)0.0 & 1 & 1 \(\)0.0 \\   & MST & **0.58**\(\)0.0 & 0.83 & 0.83 \(\)0.02 & 0.01 & 0 \(\)0.0 & 1 & 1 \(\)0.0 \\   & DCTGAN & **0.69**\(\)0.04 & 0.47 & 0.68 \(\)0.01 & 0.05 & 0.02 \(\)0.0 & 0.83 & 0.92 \(\)0.0 \\   & PATECTGAN & **0.83**\(\)0.05 & 0.6 & 0.86 \(\)0.04 & 0.03 & 0.02 \(\)0.0 & 0.87 & 0.95 \(\)0.0 \\  Shopper & AIR & **0.1**\(\)0.02 & 0.48 & 0.48 \(\)0.02 & 0.02 & 0.01 & 0.84 & 0.92 \(\)0.0 \\   & MST & **0.5**\(\)0.02 & 0.42 & 0.47 \(\)0.02 & 0.01 & 0 \(\)0.0 & 0.99 & 1 \(\)0.0 \\   & DCTGAN & **0.36**\(\)0.05 & 0.27 & 0.3 \(\)0.02 & 0.03 & 0.01 \(\)0.0 & 0.74 & 0.85 \(\)0.0 \\   & PATECTGAN & **0.11**\(\)0.04 & 0.25 & 0.31 \(\)0.05 & 0.04 & 0.01 \(\)0.0 & 0.8 & 0.89 \(\)0.0 \\  Bank & AIR & **0.18**\(\)0.01 & 0.45 & 0.46 \(\)0.01 & 0.04 & 0.02 \(\)0.01 & 0.83 & 0.9 \(\)0.0 \\   & MST & **0.32**\(\)0.02 & 0.43 & 0.44 \(\)0.02 & 0.02 & 0.01 \(\)0.0 & 0.98 & 1 \(\)0.0 \\   & DCTGAN & **0.2**\(\)0.02 & 0.22 & 0.24 \(\)0.07 & 0.04 & 0.02 \(\)0.01 & 0.71 & 0.88 \(\)0.0 \\   & PATECTGAN & **0.25**\(\)0.03 & 0.2 & 0.23 \(\)0.05 & 0.03 & 0.01 \(\)0.0 & 0.83 & 0.9 \(\)0.0 \\   

Table 1: We compare synthetic data generated without and with our post-processing technique, all under the same privacy budget \(=2\). We demonstrate utility improvement (higher is better, positive numbers imply improvement) and F1 score for downstream models trained on synthetic data and tested on real data. For reference, when training the same downstream model using real data, the F1 scores are: \((,0.61)\), \((,0.95)\), \((,0.54)\), and \((,0.47)\). Additionally, we measure the average Jensen-Shannon (JS) distance between the marginal distributions of synthetic and real data (0: identical distribution; 1: totally different distributions) and the average inverse KL-divergence (0: totally different distributions; 1: identical distribution). As shown, our technique consistently improves the utility of the synthetic data across all datasets and all DP mechanisms without degrading the performance of downstream models or statistical metrics.

   DP & Utility & F1 score & F1 score & JS distance & JS distance & Inverse KL & Inverse KL \\ Mechanism & Improv. & w/o post-proc. & w/ post-proc. & w/o post-proc. & w/ post-proc. & w/ post-proc. & w/ post-proc. \\  \(_{ 2}\) & **0.57**\(\)0.03 & 0.19 & 0.19 \(\)0.02 & 0.01 & 0.01 \(\)0.0 & 0.93 & 0.97 \(\)0.01 \\  \(_{ 4}\) & **0.68**\(\)0.01 & 0.21 & 0.21 \(\)0.01 & 0.01 & 0.01 \(\)0.0 & 0.95 & 0.99 \(\)0.01 \\   

Table 2: Experimental results on the home-credit dataset. We compare synthetic data produced by GEM with \(\{2,4\}\) against the post-processed synthetic data that are generated by applying our post-processing technique with \(_{}=1\) to synthetic data generated from GEM with \(\{1,3\}\). For reference, the downstream model trained and tested on real data has an F1 score of \(0.24\).

## 5 Conclusion and Limitations

The use of private synthetic data allows for data sharing without compromising individuals' private information. In practice, end users typically have specific requirements that the synthetic data must meet, which are often derived from their standard data science pipeline. Failure to meet these requirements can diminish the usefulness of the synthetic data. To address this issue, we introduce a theoretically-grounded post-processing procedure for improving a synthetic dataset's utility on a selected set of measures. This procedure filters out samples that do not accurately reflect the real data, while still maintaining DP guarantees. We also present a scalable algorithm for computing resampling weights and demonstrate its effectiveness through extensive experiments.

We believe there is a crucial need for future research to comprehend the capabilities and limitations of private synthetic data from various perspectives (see e.g., JSH\({}^{+}\)22, GODC22, BDI\({}^{+}\)23, for further discussions). It should be noted that synthetic data generation is not intended to replace real data, and any model trained on synthetic data must be thoroughly evaluated before deployment on real data. In cases a ML model trained on synthetic data exhibits algorithmic bias upon deployment on real data, it becomes imperative to identify the source of bias and repair the model by querying the real data with a privacy mechanism.

When generating synthetic data using DP mechanisms, outliers may receive higher noise, which can have a greater impact on minority groups. Consequently, ML models trained on DP-synthetic data by downstream users may demonstrate a disparate impact when deployed on real data . This is true even when using fairness-intervention algorithms  during model training because the synthetic data follows a different probability distribution than real data. One can potentially apply our post-processing techniques to resample synthetic data and eliminate samples that do not reflect the underlying distribution of the population groups, while ensuring sample diversity. By doing so, they can achieve a higher fairness-accuracy curve when deploying downstream ML models trained on the post-processed synthetic data on real data.

Another promising avenue of research is to investigate the data structure in cases where a user or a company can provide multiple records to the dataset. Financial data, for instance, is typically collected over time and presented as a time series. While it is possible to combine all of a user's data into a single row, this approach can lead to an unmanageably unbounded feature domain and limit the effectiveness of the synthetic data. Therefore, it is worthwhile to explore other privacy notions (see e.g., ZXLZ14, DWCS19, LSA\({}^{+}\)21) and develop strategies for optimizing higher privacy-utility trade-offs under special data structures.

Figure 1: Correlation matrix _misalignment_ with and without applying our post-processing procedure. Left: absolute difference between the real and _synthetic_ data correlation matrices. Right: absolute difference between the real and _post-processed_ synthetic data correlation matrices. As shown, our procedure effectively aligns the correlation matrix of the synthetic data with that of the real data.