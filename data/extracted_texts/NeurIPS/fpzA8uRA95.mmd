# Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection

Xilie Xu\({}^{1}\), Jingfeng Zhang\({}^{2,3}\), Feng Liu\({}^{4}\), Masashi Sugiyama\({}^{2,5}\), Mohan Kankanhalli\({}^{1}\)

\({}^{1}\) School of Computing, National University of Singapore

\({}^{2}\) RIKEN Center for Advanced Intelligence Project (AIP)

\({}^{3}\) School of Computer Science, The University of Auckland

\({}^{4}\) School of Computing and Information Systems, The University of Melbourne

\({}^{5}\) Graduate School of Frontier Sciences, The University of Tokyo

xuxilie@comp.nus.edu.sg jingfeng.zhang@auckland.ac.nz fengliu.ml@gmail.com sugi@k.u-tokyo.ac.jp mohan@comp.nus.edu.sg

The first two authors have made equal contributions.Corresponding author.

###### Abstract

Adversarial contrastive learning (ACL) does not require expensive data annotations but outputs a robust representation that withstands adversarial attacks and also generalizes to a wide range of downstream tasks. However, ACL needs tremendous running time to generate the adversarial variants of all training data, which limits its scalability to large datasets. To speed up ACL, this paper proposes a _robustness-aware coreset selection_ (RCS) method. RCS does not require label information and searches for an informative subset that minimizes a representational divergence, which is the distance of the representation between natural data and their virtual adversarial variants. The vanilla solution of RCS via traversing all possible subsets is computationally prohibitive. Therefore, we theoretically transform RCS into a surrogate problem of submodular maximization, of which the greedy search is an efficient solution with an optimality guarantee for the original problem. Empirically, our comprehensive results corroborate that RCS can speed up ACL by a large margin without significantly hurting the _robustness transferability_. Notably, to the best of our knowledge, we are the first to conduct ACL efficiently on the large-scale ImageNet-1K dataset to obtain an effective robust representation via RCS. Our source code is at https://github.com/GodXuxilie/Efficient_ACL_via_RCS.

## 1 Introduction

The pre-trained models can be easily finetuned to downstream applications, recently attracting increasing attention [1; 2; 3; 4]. Notably, vision Transformer  pre-trained on ImageNet-1K  can achieve state-of-the-art performance on many downstream computer-vision applications [6; 7]. Foundation models  trained on large-scale unlabeled data (such as GPT  and CLAP ) can be adapted to a wide range of downstream tasks. Due to the prohibitively high cost of annotating large-scale data, the pre-trained models are commonly powered by the techniques of unsupervised learning [11; 12] in which _contrastive learning_ (CL) is the most effective learning style to obtain the generalizable feature representations [13; 14].

_Adversarial CL_ (ACL) [14; 15; 16; 17], that incorporate adversarial data with contrastive loss, can yield a robust representation that is adversarially robust. Compared with the standard CL , ACL can output a robust representation that can transfer some robustness to the downstream tasks againstadversarial attacks [18; 19] via finetuning. The robustness transferability is of great importance to a pre-trained model's practicality in safety-critical downstream tasks [20; 21; 22; 23].

However, ACL is computationally expensive, which limits its scalability. At each training epoch, ACL first needs to conduct several backward propagations (BPs) on all training data to generate their adversarial variants and then train the model with those adversarial data. Even if we use the techniques of fast adversarial training [24; 25] that significantly reduce the need for conducting BPs per data, ACL still encounters the issue of the large scale of the training set that is commonly used for pre-training a useful representation.

_Coreset selection_ (CS) [27; 28] that selects a small yet informative subset can reduce the need for the whole training set, but CS cannot be applied to ACL directly. Mirzasoleiman et al.  and Killamsetty et al. [30; 31; 32] have significantly accelerated the standard training by selecting an informative training subset. Recently, Dolatabadi et al.  proposed an adversarial CS method to accelerate the standard adversarial training [34; 35; 24] by selecting a representative subset that can accurately align the gradient of the full set. However, the existing CS methods require the label information of training data, which are not applicable to the ACL that learns from the unlabeled data.

To accelerate the ACL, this paper proposes a robustness-aware coreset selection (RCS) that selects a coreset without requiring the label information but still helps ACL to obtain an effective robust representation. RCS searches for a coreset that minimizes the representational divergence (RD) of a representation. RD measures the representation difference between the natural data and their virtual adversarial counterparts , in which virtual adversarial data can greatly alter the output distribution in the sense of feature representation.

Although solving the RCS via traversing all possible subsets is simple, it is computationally prohibitive. Therefore, we transform the RCS into a proxy problem of maximizing a set function that is theoretically shown monotone and \(\)-weakly submodular (see Theorem 1) subject to cardinality constraints [37; 38]. Then, we can apply the greedy search  to efficiently find a coreset that can minimize the RD. Notably, Theorem 2 provides the theoretical guarantee of optimality of our greedy-search solution.

Empirically, we demonstrate RCS can indeed speed up ACL  and its variants [17; 15]. As shown in Figure 1, RCS can speed up both ACL  and DynACL  by \(4\) times and obtain an effective representation without significantly hurting the _robustness transferability_ (details in Section 4.1). Notably, to the best of our knowledge, we are the first to apply ACL on the large-scale ImageNet-1K  dataset to obtain a robust representation via RCS (see Section 4.2). Besides, we empirically show that RCS is compatible with standard adversarial training [34; 35; 24] as well in Section 4.3. Our comprehensive experiments corroborate that our proposed RCS is a unified and principled framework for efficient robust learning.

Figure 1: We learn a representation using CIFAR-10  dataset (without requiring labels) via ACL  and DynACL . Then, we evaluate the representationâ€™s robustness transferability to CIFAR-100  and STL-10  (using labels during finetuning) via standard linear finetuning. We demonstrate the running time of robust pre-training w.r.t. different coreset selection (CS) strategies and report the robust test accuracy under AutoAttack . Our proposed RCS (star shapes) always achieves a higher robust test accuracy than random selection (triangles) while consuming much less running time than pre-training on the entire set (squares). Experimental details are in Appendix B.4.

Background and Preliminaries

In this section, we introduce related work and preliminaries about contrastive learning [13; 14; 17].

### Related Work

Contrastive learning (CL).CL approaches are frequently used to leverage large unlabeled datasets for learning useful representations. Previous unsupervised methods that map similar samples to similar representations  have the issue of collapsing to a constant representation. CL addresses the representational collapse by introducing the negative samples . Chen et al.  presented SimCLR that leverages the contrastive loss for learning useful representations and achieved significantly improved accuracy on the standard suit of downstream tasks. Recently, adversarial contrastive learning (ACL) [40; 14; 41; 15; 16; 42; 17; 43] that incorporates adversarial training with the contrastive loss  has become the most effective unsupervised approaches to learn robust representations. Jiang et al.  showed that ACL could exhibit better adversarial robustness on downstream tasks compared with standard CL (i.e., SimCLR ). Luo et al.  proposed to dynamically schedule the strength of data augmentations to narrow the gap between the distribution of training data and that of test data, thus enhancing the performance of ACL. Xu et al.  improved ACL from the lens of causality [44; 45] and proposed an adversarial invariant regularization that achieves new SOTA results.

However, ACL needs large computational resources to obtain useful representations from large-scale datasets. ACL needs to spend a large amount of running time generating adversarial training data during pre-training. Fast adversarial training (Fast-AT)  uses the fast gradient descent method (FGSM)  to accelerate the generation procedure of adversarial training data, thus speeding up robust training. A series of recent works [46; 47] followed this line and further improved the performance of fast AT. However, the time complexity of ACL is still proportional to the size of the training set. Besides, the large-scale training sets (e.g., ImageNet-1K  contains over 1 million training images) further contribute to the inefficiency of adversarially pre-training procedures [48; 49]. To enable efficient ACL, we propose a novel CS method that can speed up ACL by decreasing the amount of training data.

Coreset selection (CS).CS aims to select a small yet informative data subset that can approximate certain desirable characteristics (e.g., the loss gradient) of the entire set . Several studies have shown that CS is effective for efficient standard training in supervised [29; 31; 30] and semi-supervised  settings. Dolatabadi et al.  proposed adversarial coreset selection (ACS) that selects representative coresets of adversarial training data that can estimate the adversarial loss gradient on the entire training set. However, ACS is only adapted to supervised AT [34; 35]. Thus, ACS requires the label information and is not applicable to ACL. In addition, previous studies did not explore the influence of CS on the pre-trained model's transferability. To this end, we propose a novel CS method that does not require label information and empirically demonstrate that our proposed method can significantly accelerate ACL while only slightly hurting the transferability.

### Preliminaries

Here, we introduce the preliminaries of contrastive learning [13; 14; 17]. Let \((,d_{})\) be the input space \(\) with the infinity distance metric \(d_{}(x,x^{})=\|x-x^{}\|_{}\), and \(_{}[x]=\{x^{} d_{}(x,x^{ })\}\) be the closed ball of radius \(>0\) centered at \(x\).

ACL  and DynACL .We first introduce the standard contrastive loss . Let \(f_{}:\) be a feature extractor parameterized by \(,g:\) be a projection head that maps representations to the space where the contrastive loss is applied, and \(_{i},_{j}:\) be two transformation operations randomly sampled from a pre-defined transformation set \(\). Given a minibatch \(B^{}\) consisting of \(\) samples, we denote the augmented minibatch \(B^{}=\{_{i}(x_{k}),_{j}(x_{k}) x_{k} B\}\) consisting of \(2\) samples. We take \(h_{}()=g f_{}()\) and \(x_{k}^{u}=_{u}(x_{k})\) for any \(x_{k}\) and \(u\{i,j\}\). The standard contrastive loss of a positive pair \((x_{k}^{i},x_{k}^{j})\) is as follows:

\[_{}(x_{k}^{i},x_{k}^{j};)\!=\!-\!_{u\{i,j\}} (h_{}(x_{k}^{i}),h_{}(x_{k}^{j}))/ t}}{_{x B^{}\{x_{k}^{u}\}}e^{ (h_{}(x_{k}^{u}),h_{}(x))/t}},\]

where \((p,q)=p^{}q/\|p\|\|q\|\) is the cosine similarity function and \(t>0\) is a temperature parameter.

Given an unlabeled set \(X^{N}\) consisting of \(N\) samples, the loss function of ACL  is \(_{}(X;)=_{k=1}^{N}_{}(x_{k};)\) where

\[_{}(x_{k};)=_{_{ k}^{i}_{}[x_{k}^{j}]\\ s_{k}^{j}_{}[x_{k}^{j}]}(1+) _{}(_{k}^{i},_{k}^{j};)+(1- )_{}(x_{k}^{i},x_{k}^{j};),\]

in which \(\) is a hyperparameter and \(_{k}^{i}\) and \(_{k}^{j}\) are adversarial data generated via projected gradient descent (PGD)  within the \(\)-balls centered at \(x_{k}^{i}\) and \(x_{k}^{j}\). Note that ACL  fixes \(=0\) while DynACL  dynamically schedules \(\) according to its dynamic augmentation scheduler that gradually anneals from a strong augmentation to a weak one. We leave the details of the data augmentation scheduler  in Appendix B.2 due to the limited space.

Given an initial positive pair \((x^{i,(0)},x^{j,(0)})\), PGD step \(T\), step size \(>0\), and adversarial budget \( 0\), PGD iteratively updates the pair of data from \(=0\) to \(T-1\) as follows:

\[x^{i,(+1)} =\!_{_{}[x^{i,(0)}]}x^{i,()}+ (_{x^{i,()}}_{}(x^{i,()}, x^{j,()}),\] \[x^{j,(+1)} =\!_{_{}[x^{j,(0)}]}x^{j,()}+ (_{x^{j,()}}_{}(x^{i,()}, x^{j,()}),\]

where \(_{_{}[x]}\) projects the data into the \(\)-ball around the initial point \(x\).

ACL is realized by conducting one step of inner maximization on generating adversarial data via PGD and one step of outer minimization on updating \(\) by minimizing the ACL loss on generated adversarial data, alternatively. Note that the parameters of the projection head \(g\) are updated as well during ACL. Here we omit the parameters of \(g\) for notational simplicity since we only use the parameters of feature extractor \(f_{}\) on downstream tasks after completing ACL.

## 3 Robustness-Aware Coreset Selection

In this section, we first introduce the representational divergence (RD), which is quantified by the distance of the representation between natural data and their virtual adversarial variants, as the measurement of the adversarial robustness of a representation. Then, we formulate the learning objective of the Robustness-aware Coreset Selection (RCS). Next, we theoretically show that our proposed RCS can be efficiently solved via greedy search. Finally, we give the algorithm of efficient ACL via RCS.

### Representational Divergence (RD)

Adversarial robustness is the most significant property of adversarially pre-trained models, which could lead to enhanced robustness transferability. Inspired by previous studies [35; 50], we measure the adversarial robustness of the representation in an unsupervised manner using the representational divergence (RD). Given a natural data point \(x\) and a model \(g f_{}:\) composed of a feature extractor \(f_{}\) and a projector head \(g\), RD of this data point \(_{}(x;)\) is quantified by the distance between the representation of the natural data and that of its virtual adversarial counterpart , i.e.,

\[_{}(x;)=d(g f_{}(),g f_{}( x))=*{arg\,max}_{x^{} _{}[x]}d(g f_{}(x^{}),g f_{}( x)),\] (1)

in which we can use PGD method  to generate adversarial data \(\) within the \(\)-ball centered at \(x\) and \(d(,):\) is a distance function, such as the Kullback-Leibler (KL) divergence , the Jensen-Shannon (JS) divergence , and the optimal transport (OT) distance . We denote the RD on the unlabeled validation set \(U\) as \(_{}(U;)=_{x_{i} U}_{}(x_{i}; )\). The smaller the RD is, the representations are of less sensitivity to adversarial perturbations, thus being more robust.

### Learning Objective of Robustness-Aware Coreset Selection (RCS)

Our proposed RCS aims to select an informative subset that can achieve the minimized RD between natural data and their adversarial counterparts, thus expecting the selected coreset to be helpful in improving the adversarial robustness of representations. Therefore, given an unlabeled training set \(X^{N}\) and an unlabeled validation set \(U^{M}\) (\(M N\)), our proposed Robustness-aware Coreset Selection (RCS) searches for a coreset \(S^{*}\) such that

\[S^{*}=*{arg\,min}_{S X,|S|/|X| k}_{ }(U;*{arg\,min}_{}_{}(S; )).\] (2)Note that the subset fraction \(k(0,1]\) controls the size of the coreset, i.e., \(|S^{*}| kN\). RCS only needs to calculate RD on the validation set (i.e., \(_{}(U)\)) and ACL loss on the subset (i.e., \(_{}(S)\)), which can be computed in an unsupervised manner. Thus, RCS is applicable to ACL on unlabeled datasets, and compatible with supervised AT on labeled datasets as well, such as Fast-AT , SAT  and TRADES  (details in Appendix B.13). Intuitively, the coreset \(S^{*}\) found by RCS can make the pre-trained model via minimizing the ACL loss \(_{}(S^{*})\) achieve the minimized \(_{}(U)\), thus helping attain adversarially robust presentations, which could be beneficial to the robustness transferability to downstream tasks.

To efficiently solve the inner minimization problem of Eq. (2), we modify Eq. (2) by conducting a one-step gradient approximation as the first step, which keeps the same practice as , i.e.,

\[S^{*}=*{arg\,min}_{S X,|S|/|X| k}_{ }(U;-_{}_{}(S;)),\] (3)

where \(>0\) is the learning rate. We define a set function \(G:2^{X}\) as follows:

\[G_{}(S X)-_{}(U;- _{}_{}(S;)).\] (4)

We fix the size of the coreset, i.e., \(|S^{*}|=k|X|\) being a constant. Then, the optimization problem in Eq. (2) can be reformulated as

\[S^{*}=*{arg\,max}_{S X,|S|/|X|=k}G_{}(S).\] (5)

### Method--Greedy Search

Solving Eq. (5) is formulated as maximizing a set function subject to a cardinality \(|S|\) constraint . A naive solution to this problem is to traverse all possible subsets of size \(kN\) and select the subset \(S\) which has the largest value \(G_{}(S)\). But this naive solution is computationally prohibitive since it needs to calculate the loss gradient \(_{}_{}(S;)\) for \(\) times. Fortunately, Das and Kempe  and Gatmiry and Gomez-Rodriguez  proposed the greedy search algorithm to efficiently solve this kind of problem approximately if the set function is monotone and \(\)-weakly submodular. Note that the greedy search algorithm shown in Algorithm 1 only needs to calculate the loss gradient \( N/+ kN/\) times where \(\) is batch size.

**Definition 1** (Monotonicity and \(\)-weakly submodularity ).: _Given a set function \(G:2^{X}\), the marginal gain of \(G\) is defined as \(G(x|A) G(A\{x\})-G(A)\) for any \(A X\) and \(x X A\). The set function \(G\) is monotone if \(G(x|A) 0\) for any \(A X\) and \(x X A\). The set function \(G\) is called \(\)-weakly submodular if \(_{x B}G(x|A)[G(A B)-G(A)]\) for some \(\) and any disjoint subset \(A,B X\)._

**Assumption 1**.: _The first-order gradients and the second-order gradients of \(_{}\) and \(_{}\) are bounded w.r.t. \(\), i.e.,_

\[\|}_{}(x;)\| L_{1}, \|}_{}(x;)\| L_{2},\|}{_{}}_{}(x;)\| L _{3},\|}{_{}}_{}(x;)\|  L_{4},\]

_where \(L_{1}\), \(L_{2}\), \(L_{3}\), and \(L_{4}\) are positive constants._

Next, we theoretically show that a proxy set function \(_{}(S)\) is monotone and \(\)-weakly submodular in Theorem 1.

**Theorem 1**.: _We define a proxy set function \(_{}(S) G_{}(S)+|S|\), where \(=1+_{1}+_{2}L_{2}+ ML_{2}(L_{1}+ kN(L_{1}L_{4}+L_{2}L_{3}))\), \(_{1} 0^{+}\), and \(_{2}>0\) are positive constants. Given Assumption 1, \(_{}(S)\) is monotone and \(\)-weakly submodular where \(>^{*}=\)._

The proof is in Appendix A.1. We construct a proxy optimization problem in Eq. (5) as follows:

\[^{*}=*{arg\,max}_{S X,|S|/|X|=k}_{}( S).\] (6)

According to Theorem 1, a greedy search algorithm  can be leveraged to approximately solve the proxy problem in Eq. (6) and it can provide the following optimality guarantee of the optimization problem in Eq. (5).

**Theorem 2**.: _Given a fixed parameter \(\), we denote the optimal solution of Eq. (5) as \(G_{}^{}=_{S X,|S|/|X|=k}G_{}(S)\). Then, \(^{}\) in Eq. (6) found via greedy search satisfies_

\[G_{}(^{}) G_{}^{}-(G_{}^{}+kN ) e^{-^{}}.\]

Remark.The proof is in Appendix A.2. Theorem 2 indicates that the greedy search for solving the proxy problem in Eq. (6) can provide a guaranteed lower-bound of the original problem in Eq. (5), which implies that RCS via greedy search can help ACL to obtain the minimized RD and robust representations. We also empirically validate that ACL on the coreset selected by RCS can achieve a lower RD compared with ACL on the randomly selected subset in Figure 5 (in Appendix B.6), which empirically supports that RCS via greedy search is effective in achieving the minimized RD.

Algorithm of RCS.Therefore, we use a greedy search algorithm via batch-wise selection for RCS. Algorithm 1 iterates the batch-wise selection \( k|X|/\) times. At each iteration, it finds the minibatch \(B\) that has the largest gain \(_{}(B|S)\) based on the parameters updated by the previously selected coreset, and then adds this minibatch into the final coreset. RCS via greedy search needs to calculate the gradient for each minibatch \( N/\) times (Line 6 in Algorithm 1) and the gradient on the validation set \( kN/\) times (Line 10 in Algorithm 1). In total, RCS needs to calculate the loss gradient \( N/+ kN/\) times, which is significantly more efficient than the native solution. We approximate the marginal gain function using the Taylor expansion as follows:

\[_{}(B|S)_{}_{}(U; -_{}_{}(S;))^{}_{ }_{}(B;)+,\] (7)

where \(B\) is a minibatch and \(\) is the batch size. The derivation of Eq. (7) is in Appendix A.3. It enables us to efficiently calculate the marginal gain for each minibatch (Line 13 in Algorithm 1). We omit this term in Algorithm 1 since \(\) is a constant. Intuitively, RCS selects the data whose training loss gradient and validation loss gradient are of the most similarity. In this way, the model can minimize the RD (validation loss) after updating its parameters by minimizing the ACL loss (training loss) on the coreset selected by RCS, thus helping improve the adversarial robustness of the representation.

```
1:Input: Unlabeled training set \(X\), unlabeled validation set \(U\), batch size \(\), model \(g f_{}\), learning rate for RCS \(\), subset fraction \(k(0,1]\)
2:Output: Coreset \(S\)
3:Initialize \(S,N X\)
4:Split entire training set into minibatches \(\{B_{m}\}_{m=1}^{ N/}\)
5:for each minibatch \(B_{m} X\)do
6: Compute gradient \(q_{m}_{}_{}(B_{m};)\)
7:endfor
8:// Conduct greedy search via batch-wise selection
9:for\(1,, kN/\)do
10: Compute gradient \(q_{U}_{}_{}(U;)\)
11: Initialize \(best\_gain=-\)
12:for each minibatch \(B_{m} X\)do
13: Compute marginal gain \((B_{m}|S) q_{U}^{}q_{m}\) // refer to Eq. (7)
14:if\((B_{m}|S)>best\_gain\)then
15: Update \(s m\), \(best\_gain(B_{m}|S)\)
16:endif
17:endfor
18: Update \(S S B_{s}\), \(X X B_{s}\)
19: Update \(- q_{s}\)
20:endfor ```

**Algorithm 1** Robustness-aware Coreset Selection (RCS)

### Efficient ACL via RCS

We show an efficient ACL procedure via RCS in Algorithm 2. ACL with RCS trains the model on the previously selected coreset for \(I\) epochs, and for every \(I\) epochs a new coreset is selected.

The pre-training procedure is repeated until the required epoch \(E\) is reached. We provide four tricks with three tricks as follows and one trick that enables efficient RCS on large-scale datasets with limited GPU memory in Appendix B.1.

Warmup on the entire training set.We take \(W\) epochs to train the model on the entire training set as the warmup. Warmup training enables the model to have a good starting point to provide informative gradients used in RCS. For example, we use \(10\%\) of the total training epochs for warmup.

Last-layer gradients.It is computationally expensive to compute the gradients over full layers of a deep model due to a tremendous number of parameters in the model. To tackle this issue, we utilize a last-layer gradient approximation, by only considering the loss gradients of the projection head \(g\) during RCS.

Adversarial data approximation.Calculating adversarial data during CS is extremely time-consuming since generating adversarial data needs to iteratively perform BP \(T\) times. We let \(T_{}\) be PGD steps, \(_{}\) be the adversarial budget, and \(_{}\) be the step size for PGD during ACL. Similarly, \(T_{}\), \(_{}\), and \(_{}\) denote PGD configurations during RCS. To mitigate the above issue, we decrease \(T_{}\) (i.e., \(0<T_{} T_{}\)) and \(_{}=} T_{}}{T_{ }}\) for efficiently generating adversarial data during CS. Note that the same adversarial budget is used for ACL and RCS, i.e., \(_{}=_{}\).

## 4 Experiments

In this section, we first validate that our proposed RCS can significantly accelerate ACL  and its variant [17; 15] on various datasets [6; 26] with minor degradation in transferability to downstream tasks. Then, we apply RCS to a large-scale dataset, i.e., ImageNet-1K  to demonstrate that RCS enhances the scalability of ACL. Lastly, we demonstrate extensive empirical results. Extensive experimental details are in Appendix B.2.

Efficient pre-training configurations.We leverage RCS to speed up ACL  and DynACL  using ResNet-18 backbone networks. The pre-training settings of ACL and DynACL exactly follow their original paper and we provide the details in Appendix B.2. For the hyperparameters of RCS, we set \(=512\), \(=0.01\), and \(T_{}=3\). We took \(W=100\) epochs for warmup, and then CS was executed every \(I=20\) epoch. We used different subset fractions \(k\{0.05,0.1,0.2\}\) for CS. The KL divergence was used as the distance function to calculate \(_{}()\) for all the experiments in Section 4. We used the pre-trained weights released in ACL's and DynACL's GitHub as the pre-trained encoder to reproduce their results on the entire set. We repeated all the experiments using different random seeds three times and report the median results to exclude the effect of randomization.

Finetuning methods.We used the following three kinds of finetuning methods to evaluate the learned representations: standard linear finetuning (SLF), adversarial linear finetuning (ALF), and adversarial full finetuning (AFF). SLF and ALF will keep the learned encoder frozen and only finetune the linear classifier using natural or adversarial samples, respectively. AFF leverages the pre-trained encoder as weight initialization and trains the whole model using the adversarial data. The finetuning settings exactly follow DynACL . More details in Appendix B.2.

Evaluation metrics.The reported robust test accuracy (dubbed as "RA") is evaluated via AutoAttack (AA)  and the standard test accuracy (dubbed as "SA") is evaluated on natural data. In practice, we used the official code of AutoAttack  for implementing evaluations. In Appendix B.5, we provide robustness evaluation under more diverse attacks .

Baseline.We replaced RCS (Line 9 in Algorithm 2) with the random selection strategy (dubbed as "Random") to obtain the coreset \(S\) during ACL as the baseline. The implementation of Random exactly follows that in the coreset and data selection library .

Speed-up ratio.The speed-up ratio denotes the ratio of the running time of _the whole pre-training procedure_ on the entire set (dubbed as "Entire") to that of ACL with various CS strategies (i.e., Random and RCS). We report the running time of Entire in Table 3.

### Effectiveness of RCS in Efficient ACL

Cross-task adversarial robustness transferability.Figures 2 and 3 demonstrate the cross-task adversarial robustness transferability from CIFAR-10 and CIFAR-100 to downstream tasks, respectively. First, we observe that ACL/DynACL with RCS (red/orange lines) always achieves significantly higher robust and standard test accuracy than ACL/DynACL with Random (blue/green lines) among

Figure 3: Cross-task adversarial robustness transferability from CIFAR-100 to CIFAR-10 (upper row) and STL-10 (bottom row). The number after the dash line denotes subset fraction \(k\{0.05,0.1,0.2\}\).

Figure 2: Cross-task adversarial robustness transferability from CIFAR-10 to CIFAR-100 (upper row) and STL-10 (bottom row). The number after the dash line denotes subset fraction \(k\{0.05,0.1,0.2\}\). ACL with RCS and DynACL with RCS correspond to the red and orange solid lines, respectively. ACL with Random and DynACL with Random correspond to the blue and green dotted lines, respectively.

[MISSING_PAGE_FAIL:9]

that FGSM cannot effectively learn robust representations and consumes more running time than RCS. Therefore, our proposed RCS is more efficient and effective in speeding up ACL than FGSM.

RCS for efficient supervised robust pre-training.In Appendix B.13.2, following Hendrycks et al.  and Salman et al. , we apply RCS to speed up pre-training WRN-28-10 and ResNet-50 via SAT  on ImageNet-1K  and demonstrate that SAT with RCS can almost maintain transferability. Therefore, RCS can be a unified and effective framework for efficient robust pre-training.

Comparison between RCS and ACS  in speeding up supervised AT including Fast-AT , SAT , and TRADES .We provide the comparison between RCS and ACS  in Appendix B.13.1, which validates that our proposed RCS, without using labels during CS, is more efficient and effective than ACS in terms of speeding up supervised AT .

Analysis of the coreset selected by RCS.We provide comprehensive quantitative and visualization analyses of the coreset selected by RCS in Appendix B.6. The analyses demonstrate that RCS helps minimize RD and tends to select a coreset that is closer to the entire training set compared to Random, thus helping ACL obtain useful and adversarially robust representations.

## 5 Conclusion

This paper proposed a robustness-aware coreset selection (RCS) framework for accelerating robust pre-training. RCS found an informative subset that helps minimize the representational divergence between natural data and their adversarial counterparts. We theoretically showed that RCS can be efficiently solved by greedy search approximately with an optimality guarantee. RCS does not require label information and is thus applicable to ACL as well as supervised AT. Our experimental results validated that RCS can significantly speed up both ACL and supervised AT while slightly hurting the robustness transferability.

One of the limitations is that RCS still requires spending a particular amount of running time during coreset selection in calculating the loss gradients, although we theoretically propose the greedy-search algorithm with an optimality guarantee to make RCS efficient. We leave how to further improve the efficiency of RCS, such as by leveraging better submodular function optimization methods , as the future work.