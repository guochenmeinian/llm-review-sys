# What Rotary Position Embedding Can Tell Us: Identifying Query and Key Weights Corresponding to Basic Syntactic or High-level Semantic Information

What Rotary Position Embedding Can Tell Us: Identifying Query and Key Weights Corresponding to Basic Syntactic or High-level Semantic Information

 Yiting Chen, Junchi Yan

Dept. of CSE & School of AI & MoE Key Lab of AI, Shanghai Jiao Tong University

{sjtuncyt, yanjunchi}@sjtu.edu.cn

https://github.com/Ytchen981/RoPE_investigate

Corresponding author. This work was in part supported by NSFC (92370201, 62222607) and Shanghai Municipal Science and Technology Major Project under Grant 2021SHZDZX0102.

###### Abstract

Transformer-based large language models (LLMs) have successfully handled various tasks. As one fundamental module in Transformers, position encoding encodes the positional information of tokens in a sequence. Specifically, rotary position embedding (RoPE), one of the most widely used techniques, encodes the positional information by dividing the query or key value with \(d\) elements into \(d/2\) pairs and rotating the 2d vectors corresponding to each pair of elements. Therefore, the direction of each pair and the position-related rotation jointly determine the attention score. In this paper, we show that the direction of the 2d pair is largely affected by the angle between the corresponding weight vector pair. We theoretically show that non-orthogonal weight vector pairs lead to great attention on tokens at a certain relative position and are less sensitive to the input which may correspond to basic syntactic information. Meanwhile, the orthogonal weight vector pairs are more flexible regarding the relative position, which may correspond to high-level syntactic information. Empirical evidence supports the hypothesis that shallow layers of LLMs focus more on local syntax and deep layers focus more on high-level semantics. Furthermore, we show that LLMs fine-tuning mainly changes the pairs of weight vectors that are nearly orthogonal, i.e., the weight corresponding to high-level semantics, which enables the reduction of the number of trainable parameters during fine-tuning without sacrificing performance. We propose a method namely Angle-based Weight Masking (AWM) to reduce the fine-tuning overhead and verify the effectiveness of the proposed method on widely used Alpaca fine-tuned Llama-2.

## 1 Introduction

Large language models [36; 26; 35] have achieved impressive success and attracted considerable attention towards analyzing the Transformer structure and enhancing fine-tuning efficiency [14; 1; 18; 15]. As one fundamental module in a Transformer, position encoding encodes the positional information of tokens in a sequence. Various position encoding have been proposed including absolute position encoding [36; 8; 26], and relative position encoding [30; 27], _etc._ Among them, one of the most widely used position encodings is rotary position embedding (RoPE)  which is used in various popular LLMs such as Llama , Mistral , GLM , _etc._ Instead of applying a function to the input, RoPE applies a transformation to the query and the key of each attention layer, which provides us with a window to investigate how the LLMs utilize the position information. Inthis paper, we analyze the query and the key weight matrix in the self-attention module of LLMs using RoPE and propose a simple method to reduce the computational cost of LLM fine-tuning.

As the name "rotary" indicates, RoPE divides the elements of queries and keys into pairs and rotates each pair as a 2D vector with a certain angle determined by the position. One simple fact is that the direction of the 2D vector before rotation and the position-related rotation jointly affect the attention score. Note that the direction of the 2D vector is determined by the corresponding weight vector pairs. As illustrated in Fig. 1, a simple analysis shows that non-orthogonal weight vector pairs with large absolute cosine similarity values are less sensitive to the input and will draw greater attention to certain positions. It provides an indicator of how the model utilizes the position information. We empirically show that attention heads with large absolute cosine similarity between weight vector pairs focus more on basic syntactic information while attention heads with near-zero cosine similarity between weight vector pairs focus more on high-level semantics. In linguistics, formally, syntactic information refers to the arrangement of symbols or words according to the rules of a formal system or language  and semantic information pertains to the meaning and interpretation of words, phrases, and sentences . In studying how deep learning models utilise syntactic and semantic information, previous works  empirically show that shallow layers of LLMs focus more on basic syntactic information and deep layers of LLMs focus more on high-level semantics. Our experimental results align with previous works and further support the hypothesis. In a more fine-grained perspective, we further show that within each attention head, the angles between weight vector pairs also vary.

By comparing the fine-tuned version with the pre-trained version of LLMs, we further show that the weight in the query and the key are mainly changed on nearly orthogonal weight vector pairs during fine-tuning, and the non-orthogonal weight vector pairs are barely changed. This implies that fine-tuning the query and the key in LLMs mainly changes the weights corresponding to high-level semantic information and does not change the weights corresponding to basic syntactic information in the query and key. We conjecture that it is because the pre-trained LLMs are already good enough in processing basic syntactic information and only need to be tuned on how to process high-level semantic information for downstream tasks. Therefore, we propose to fix the non-orthogonal pairs of weight vectors in the query and key of each layer in the pre-trained models to reduce the number of trainable parameters during fine-tuning. We conduct experiments on widely used models and datasets to verify the effectiveness of our method. We show that our method could effectively reduce the number of trainable parameters while maintaining or even boosting the performance of the fine-tuned model. **We summarize the contributions of this paper in the following:**

* We provide a new perspective to investigate how LLMs with RoPE utilize the positional information and theoretically show that non-orthogonal weight vector pairs divided by RoPE are less sensitive to input and draw greater attention to certain relative positions.
* We empirically show the angles between the weight vector pairs in the query and the key could serve as an indicator of whether the pair of weight vectors focuses on basic syntactic information or high-level semantics. Non-orthogonal weight vector pairs focus more on basic syntactic information while nearly orthogonal weight vector pairs focus more on high-level semantic information.
* Since RoPE divides the elements in the query or key values into pairs, we empirically show that fine-tuning LLM mainly changes the orthogonal pairs of corresponding weight vectors. Based on our findings, only orthogonal pairs of weight vectors are changed during fine-tuning, we propose a method to reduce the number of trainable parameters during LLM fine-tuning and verify its effectiveness on widely used models and benchmarks.

## 2 Preliminaries and Related Works

**Position Encoding.** After the seminar work , various Transformer-based position encoding methods have been proposed to incorporate position information into the function calculating query, key, and value. One typical way is adding the input with a vector depending on the position of the input vector. Learnable position embedding is introduced where the position embedding is trainable during training [36; 8; 26]. The sinusoidal positional encoding was introduced by the authors of  where the vector is generated by a sinusoidal function. Instead of using absolute position, another branch of work proposes relative position encoding [30; 27]. Rotary position embedding (RoPE)  was proposed to incorporate position information by rotating the pairs of elements in the query and the key, which was widely used in different LLMs such as Llama , Palm , Mistral , GLM , _etc._ Recent works on long-context Transformers also show the advantage of RoPE on input length extrapolation [32; 24]. In this paper, we provide a new weight vector angle perspective to investigate how LLMs with RoPE utilize the positional information and empirically show that we can identify weights corresponding to processing basic syntactic information or high-level semantic information. Please refer to Sec. 3 for more details.

**Parameter-Efficient Fine-Tuning.** Different from full fine-tuning that fine-tunes all the parameters, parameter-efficient fine-tuning (PEFT) methods aim at reducing the number of parameters fine-tuned to reduce the computational cost and prevent overfitting on fine-tuning data . Various PEFT methods have been proposed such as adapter tuning , prefix tuning , and prompt tuning . Adapter tuning proposes adding a small module to each layer of the pre-trained model. Prefix tuning and prompt tuning propose adding additional tunable prefix tokens to the input. Notably, LoRA  was proposed to use a low-rank matrix to approximate parameter updates.

**Other Methods to Reduce Computational Cost of LLMs.** Besides the PEFT methods, many other methods are proposed to accelerate the LLMs. Quantization [5; 7] is a common technique to reduce the memory footprint of LLMs.  propose double quantization to further extend the application of quantization from inference to fine-tuning. Another common technique is pruning where parameters or modules are pruned to reduce the computational cost [17; 13]. Recently,  shows that deep layers of LLMs can be pruned without degrading much performance. Orthogonal to these previous methods in reducing the computational cost of LLMs, we provide a method to identify weights in the query and the key that does not require updating during fine-tuning. Refer to Sec. 4 for more details.

**Definition of RoPE.** For an input sequence \(_{N}=\{_{m}\}_{m=1}^{N}\) where \(_{m}^{d}\) is the input regarding the \(m\)-th token. The self-attention with RoPE  generates the query and the key with

\[_{m}&=f_{q}(x_{m},m)=R_{ ,m}^{d}W_{q}_{m},\\ _{m}&=f_{k}(x_{m},m)=R_{,m}^{d}W_{k} _{m},\] (1)

where \(_{m}\) and \(_{m}\) is the query and key of the \(m\)-th token, the \(W_{q}\) and \(W_{k}\) is the weight matrix and \(R_{,i}^{d}\) is the rotary matrix defined as following

\[R_{,m}^{d}=( m_{1}&- m _{1}&0&0&&0&0\\  m_{1}& m_{1}&0&0&&0&0\\ 0&0& m_{2}&- m_{2}&&0&0\\ 0&0& m_{2}& m_{2}&&0&0\\ &&&&&&\\ 0&0&0&0&& m_{d/2}&- m_{d/2}\\ 0&0&0&0&& m_{d/2}& m_{d/2}).\] (2)

Figure 1: Illustration of how the angle between weight vector pairs in the query or the key affects RoPE. The larger absolute cosine similarity \(||\), the direction of the projected 2D vector is more fixed which leads to high attention on certain relative positions regardless of the input.

The \(\) is predefined as \(=\{_{i}=10000^{-2(i-1)/d},i[1,2,,d/2]\}\).

## 3 Investigating the Angle between Weight Vector Pairs in RoPE

In this section, we first provide a simple analysis regarding how the angle between the pair of weight vectors will affect the RoPE and then provide empirical results regarding how the LLM utilizes the positional information with different attention heads and across layers.

### A Simple Analysis Regarding the Angle between Weight Vector Pairs

Let \(_{q}^{}\) and \(_{k}^{}\) denote the \(i\)-th row vector of \(W_{q}\) and \(W_{k}\) such that \(W_{q}=(_{q}^{1},_{q}^{2},,_{q}^{d})^{}\) and \(W_{k}=(_{k}^{1},_{k}^{2},,_{k}^{d})^{}\). As shown in Eq. 1, RoPE rotates each pair \((_{q}^{(2i-1)^{}}_{m},_{q}^{(2i)^{}} _{m})\) and \((_{k}^{(2i-1)^{}}_{m},_{k}^{(2i)^{}} _{m})\) with \(m_{i}\). As we take the inner product of query and key, the direction of 2D pairs and the position-related rotation jointly determine the attention weight. According to the definition in Eq. 1 we have

\[_{n}^{}_{m}=_{i=1}^{d/2}\|(_{q}^{(2i-1) ^{}}_{n},_{q}^{(2i)^{}}_{n})\|\|( _{k}^{(2i-1)^{}}_{m},_{k}^{(2i)^{}} _{m})\|(_{n,q}^{i}-_{m,k}^{i}+(n-m)_ {i}).\] (3)

Where \(_{n,q}^{i}\) corresponds to the direction of 2D vector \((_{q}^{(2i-1)^{}}_{n},_{q}^{(2i)^{}} _{n})\) and \(_{m,k}^{i}\) corresponds to the direction of 2D vector \((_{k}^{(2i-1)^{}}_{m},_{k}^{(2i)^{}} _{m})\).

Let us consider the angle \(_{n,q}^{i}\) and \(_{m,k}^{i}\). Since the analysis for the query and the key are the same, we use \(\{q,k\}\) to represent either the query or the key. Let \(_{m,\{q,k\}}^{i}\) denotes the projection of \(_{m}\) onto the subspace held by \(_{\{q,k\}}^{2i-1}\) and \(_{\{q,k\}}^{2i}\). Suppose the angle between \(_{\{q,k\}}^{2i-1}\) and \(_{\{q,k\}}^{2i}\) is \(_{\{q,k\}}^{i}\) and the angle between \(_{\{q,k\}}^{2i-1}\) and \(_{m,\{q,k\}}^{i}\) is \(_{m,\{q,k\}}^{i}\), we have

\[_{m,\{q,k\}}^{i}=_{q}^{(2i)^{}}_{m}}{ _{q}^{(2i-1)^{}}_{m}}=_{m,\{q,k\}}^ {i}\|\|_{\{q,k\}}^{2i}\|(_{m,\{q,k\}}^{i}-_{\{q,k\}} ^{i})}{\|_{m,\{q,k\}}^{i}\|\|_{\{q,k\}}^{2i-1}\|(_ {m,\{q,k\}}^{i})}\] (4)

Derive Eq. 4, we get

\[_{m,\{q,k\}}^{i}=(_{\{q,k\}}^{2i}\|}{\| _{\{q,k\}}^{2i-1}\|}(_{\{q,k\}}^{i}+_{\{q,k \}}^{i}_{m,\{q,k\}}^{i})).\] (5)

As shown in Eq. 5, the larger \(|_{\{q,k\}}^{i}|\) the larger impact \(_{m,\{q,k\}}^{i}\) would have on \(_{m,\{q,k\}}^{i}\). It indicates that if the \(|_{\{q,k\}}^{i}|\) is small and the \(|_{\{q,k\}}^{i}|\) is large for both the query and the key, the attention weight would be less sensitive to the input and draw greater attention to certain relative positions1 since the directions of the projected 2-d vector \((_{\{q,k\}}^{(2i-1)^{}}_{m},_{\{q,k\}}^{(2i )^{}}_{m})\) of every token are close. In an extreme condition, when the two weight vectors are in the same direction or opposite direction, \(_{\{q,k\}}^{i}=0\), we have \(_{m,\{q,k\}}^{i}=(_{\{q,k\}}^{2i}\|}{\|_{\{q,k\}}^{2i-1}\|})\), which means the direction of the 2-d vector \((_{\{q,k\}}^{(2i-1)^{}}_{m},_{\{q,k\}}^{(2i )^{}}_{m})\) is fixed for any \(_{m}\).

Note that as vectors in high-dimensional space, due to the curse of dimensionality, these weight vector pairs are nearly orthogonal when randomly initialized. Therefore the non-orthogonal weight vector pairs are non-trivial such that the model learns non-orthogonal weight vector pairs to emphasize tokens at certain relative positions. In the following sections, we empirically show that non-orthogonal weight vector pairs widely exist in LLMs and provide a new perspective for us to investigate LLMs.

### Attention Visualization for Different Attention Heads

To verify the conjecture in Sec. 3.1 that large absolute cosine similarity \(||\) corresponds to basic syntactic information and small \(||\) corresponds to high-level semantics, we visualize the attention of attention heads with different average absolute cosine similarity \(||\) in this section. With simple questions such as "What is the capital of France?" as input, we record the attention score of different attention heads and visualize the score by setting the transparency of the lines connecting the key and the query accordingly. The lower transparency corresponds to a higher attention score. For clarity in the figure, we limit the number of tokens to \(20\).

As shown in Fig. 2, we present the attention visualization of the attention heads with the largest or the smallest average absolute cosine similarity in the 1st layer of Llama-2-7b-chat  and Mistral-7b-Instruct-v0.2 . For the attention head with a large absolute cosine similarity value across the weight vector pairs (\(0.54\) on average for the 1st layer of Llama2-7b and \(0.65\) on average for the 1st layer of Mistral-7b), attention is mainly on tokens of prepositions or articles that may correspond more to the basic syntactic information. Notably, attention to special tokens is high. Special tokens mainly correspond to syntactic information, such as the end of the input prompt or the start of the answer by LLMs. In contrast, for the attention head with a small absolute cosine similarity value across the weight vector pairs in RoPE (\(0.24\) on average for the 1st layer of Llama2-7b and \(0.29\) on average for the 1st layer of Mistral-7b), the attention is on every token of the phrase which may correspond more to the high-level semantics. For more results at different layers or different models, please refer to Appendix. B.

### Analyzing Weight Vector Pair Angles Across the Layers

In this section, we propose to investigate how LLMs utilize positional information from RoPE across the layers from the weight vector angle perspective. For each layer, we calculate the average absolute value of cosine similarity across all the weight vector pairs in the layer.

As shown in Fig. 3, we report the results of each layer of different LLMs. For each LLM, generally, the average absolute cosine value decreases to a very low value after the first several layers (typically 3 layers) and stays low for the rest of the layers until it is slightly increased at the last layer. This phenomenon agrees with the hypothesis that the LLMs first process the information about local syntax at the first several layers and then process the high-level semantic information [34; 3]. While the angle between weight vectors pairs in RoPE also provides a new perspective complementary to the probing tasks  designed to show that models like BERT  process the information layer-by-layer following the traditional NLP pipeline. It also explains the success of fastly training a smaller model using the first several layers of a pre-trained LLM  where the first several layers contain the weights responsible for processing basic syntactic information, which plays a vital role in the model.

Figure 2: Attention visualization of the attention heads with the largest or the smallest average absolute cosine similarity \(||\) across the weight vector pairs in RoPE, where the key is on the left, and the query is on the right. The lower transparency means a higher attention score. We demonstrate the results for the first layer of Llama-2-7b-chat and Mistral-7B-Instruct-v0.2. The results empirically support that higher \(||\) leads to attention on basic syntactic information and lower \(||\) leads to attention on high-level semantic information. For more results of different models at different layers please refer to Appendix B.

The last layer generates the output in a certain format, which leads to a slight increase in the average absolute cosine value. This also agrees with the empirical results in previous works [37; 21] that the sparsity of activation changes in the middle from sparse to dense, and the layers at the middle could be pruned.  also shows that the layers in the middle are the mostly changed layers during fine-tuning. In Sec. 4, we further extend the result to that the orthogonal pairs of weight vectors (\(=0\)) are the mostly changed weights during fine-tuning and propose a method to reduce the number of trainable parameters during fine-tuning.

### Investigating the Cosine Similarity Across Weight Vector Pairs

Beyond the coarse grain results (attention head-wise as in Sec. 3.2 and layer-wise as in Sec. 3.3), the angles between weight vector pairs could provide a more fine-grained view. In this section, we propose to investigate the distribution of cosine similarity between weight vector pairs in the query and the key. As shown in Fig. 4, we report the cosine similarity, the \(\), of weight vector pairs of the query and key in the first layer of Llama2-7b . In the figure, we separate different attention heads by vertical red lines. The cosine similarity changes drastically across the heads or even across the pairs of weight vectors in the same head. This means that in one head, there is still a division of labor, such that some of the weight vector pairs draw more attention to certain relative positions while the other weights are more flexible. More results of different LLMs are in Appendix B.

Figure 4: We show the cosine similarity of each weight vector pair in the query and the key of different layers of the Llama2-7b . Each column corresponds to different layers. The top row is the results for the query and the bottom row is the results for the key. The attention heads are separated with vertical red lines. We show that even within one head, the cosine similarity of different weight vector pairs differs. More results on other layers and other models are in Appendix B.

Figure 3: We report the average absolute cosine similarity of the query and the key across the layers of different LLMs. We show that, for all the LLMs we investigate, the average absolute cosine similarity drastically decreases after the first several layers and stays small until the last layer.

Still, we find that the angles between the pairs of weight vectors in query and key are highly related to each other. As shown in Fig. 5, the \(\) of pairs of weights of the query is positively correlated with the \(\) of pairs of weights of the key with Pearson correlation at \(0.86\). It makes sense since it requires both the angle of the projected \(2\)D vector of the query and the key to be nearly fixed to assure high attention on certain relative positions. Initialized to be both nearly orthogonal, we conjecture that the angles between the query and the key weight vector pairs may be changed simultaneously during training. For results of other layers and other models, refer to Appendix B.

## 4 Reducing the Trainable Parameters During Fine-tuning

In this section, we first show that only the weight vector pairs that are nearly orthogonal (with \(=0\)) are changed during training by comparing the base version and finetuned version (_e.g._ chat or instruct version) of the same LLM. We then propose an efficient and effective method, namely Angle-based Weight Masking (AWM), that fixes the non-orthogonal query and key weight vector pairs defined in RoPE to reduce the number of trainable parameters while maintaining or even boosting the performance during fine-tuning. Since our proposed method fixes parameters at a fine-grained weight row vector level, it is orthogonal to LoRA , the widely used parameter-efficient fine-tuning method. The experimental results verified the effectiveness of AWM.

Figure 5: Correlation between cosine similarity of the query and key weight vector pair of the 1st layer of LLama2-7b. Each dot represents two corresponding weight vector pairs in query and key. The x-axis corresponds to the cosine similarity of the weight vector pair in the query and the y-axis corresponds to the cosine similarity of the weight vector pair in the key. (Pearson’s r is 0.86)

Figure 6: Comparison between the base model Mistral-7B  and fine-tuned version WizardLM-2 . For each sub-figure, the scatter figure in the middle demonstrates the cosine similarity between weight vector pairs where each point corresponds to a weight vector pair; the x-axis corresponds to the results for the base model, and the y-axis corresponds to the fine-tuned model. In the histogram on the top and left, we report the average \(L_{2}\) weight distance between the weight vectors of the pre-trained model and the fine-tuned model. The y-axis of the histogram on the top and the x-axis of the histogram on the left correspond to the \(L_{2}\) weight distance. Generally, fine-tuning merely changes the angle between weight vector pairs. Besides the first several layers, the weight change mainly happens on weight vector pairs that are nearly orthogonal. More results are provided in Appendix B.

### Comparing Different Versions of the Same LLM

Generally, most LLMs are trained in a pretrain-finetune paradigm where the language model is firstly pre-trained on a large dataset with a pre-train task such as predicting the next token and then finetuned to accommodate better the needs for downstream tasks. Generally, there will be many different versions of the same LLM released such as the base version of the LLM after pre-training and the finetuned version _e.g._ the chat version. By comparing the parameters of two different versions of the same LLM, we could investigate how fine-tuning changes the parameters.

We present the results of comparison between the base model Mistral-7B-v0.1  and its fine-tuned version WizardLM-2  in Fig. 6. Each sub-figure contains three figures. With the scatter figure in the middle, we show the cosine similarity between weight vector pairs in the base model and the fine-tuned version, where the x-axis corresponds to the base model and the y-axis corresponds to the fine-tuned model. Generally, the fine-tuning barely changes the angle between weight vector pairs. It means that the weights responsible for processing basic syntactic information or high-level semantic information in the base model are still responsible for processing the corresponding basic syntactic information or high-level semantic information after fine-tuning. With the histogram figure on the top and left of each sub-figure, we further present the average \(L_{2}\) distance between weight vector pairs in the base model and the fine-tuned model. For the figure on the top, the x-axis corresponds to the cosine similarity of the weight vector pairs in the base model and the y-axis corresponds to the weight distance. Similarly, for the figure on the left, the x-axis corresponds to weight distance, and the y-axis corresponds to cosine similarity results for the fine-tuned model.

Generally, we find that the weight distances are high for weight vector pairs with nearly zero cosine similarity. **It indicates that fine-tuning mainly changes the weights corresponding to high-level semantic information processing and the weights corresponding to low-level information such as syntax are merely changed.** Intuitively, it results from the fact that pre-trained LLMs are good enough at processing basic syntactic information and fine-tuning changes in how they process high-level semantic information for downstream tasks. We further show the average distance between weight vector pairs across the layers of different base and fine-tuned models in Fig. 7. As a result of the fact that only near orthogonal weight vector pairs are updated during fine-tuning, the average distance increases after the first several layers and stays large, which is aligned with the results in Fig. 3. As previous work  also found that the layers in the middle are the most updated part during fine-tuning, we provide a new perspective on further understanding the phenomenon. Beyond that, since we could effectively calculate the cosine similarity of weight vector pairs, it enables an efficient method to reduce the number of trainable parameters during fine-tuning. **Note that being barely changed during fine-tuning indicates non-orthogonal weight vector pairs are important in processing basic syntactic information, which also agrees with empirical results in .**

We have also conducted experiments on more LLMs and received similar results including Llama2 , Alpaca , and Llama3 . Please refer to Appendix B for more results.

### Reducing the Trainable Parameters on Query and Key

Our observation provides us with an efficient way to reduce the trainable parameters during training. According to the results in Sec. 4.1, the non-orthogonal weight vector pairs in the query and key weight matrix do not need to be updated during fine-tuning, therefore we can reduce the number of

Figure 7: Average difference of weight vector pairs in the query and key across the layers of different LLMs (including Llama-2 , Alpaca , Llama-3 , Mistral , and WizardLM ). We show that the average difference increases after the first several layers and stays high, which is the result of the fact that only near orthogonal weight vector pairs are updated during fine-tuning.

trainable parameters before the fine-tuning is conducted and only update the nearly orthogonal weight vector pairs during fine-tuning. Therefore, we propose the method, namely Angle-based Weight Masking (AWM), to reduce the number of parameters during fine-tuning. With a threshold set as \(\), we only update the weight vector pairs \(_{\{q,k\}}^{2i-1}\) and \(_{\{q,k\}}^{2i}\) with \(||<\) where \(\) is the angle between \(_{\{q,k\}}^{2i-1}\) and \(_{\{q,k\}}^{2i}\). The detailed algorithm is in Alg. 1.

Since our method is orthogonal to many popular parameter efficient fine-tuning methods [15; 6] that reduce the computational cost of fine-tuning an LLM, in this section, we conduct our experiments with LoRA . We finetune Llama2-7b and Llama2-13b  on Alpaca  using LoRA and AWM. The query and the value weight of each model are fine-tuned with LoRA on Alpaca for \(3\) epochs. For more details on hyperparameter settings and results, please refer to Appendix A.

In Table 1, we present the evaluation results of LoRA combined with our AWM. We evaluate the fine-tuned model on TruthfulQA , GSM8K , and Hellaswag . TruthfulQA measures the tendency of the model to reproduce common falsehoods online. GSM8K is a test of diverse grade school math problems. Hellaswag is a test of commonsense inference. We use lm-eval  and follow the setting in Open LLM Leaderboard2 to evaluate LLMs on these datasets where models are evaluated in \(0\)-shot setting on TruthfulQA, \(5\)-shot on GSM8K, and \(10\)-shot on Hellaswag. As shown in Table 1, our proposed method could largely reduce the trainable parameters in the query while maintaining or even boosting the performance. As shown in Table 2, we also conduct experiments following the setting in LoftQ  where we fine-tune Llama-2-7b, Mistral-7B, and Phi-2 on wikiitext-2 and GSM8K. Generally, our proposed method improves the performance while reducing the number of trainable parameters. We conjecture that the performance is boosted because the orthogonal weight pairs in the query corresponding to basic syntactic information are fixed, which prevents the model from overfitting the basic syntactic information in the fine-tuning dataset. **Generally, the benefits of our

   Model & TruthfulQA & GSM8K & HellaSwag &  & Fixed weight \\  & mc & mc2 & acc & acc & & vector pairs (\%) \\  Llama-2-7b & \(26.07\) & \(39.45\) & \(4.85\) & \(57.99\) & - & - \\  + LoRA(r=8) & \(33.41\) & \(49.48\) & \(10.92\) & \(58.81\) & - & - \\ + LoRA(r=8) + AWM(Ours) & \(34.27\) & \(50.37\) & \(11.30\) & \(58.79\) & 0.01 & \(77.20\) \\ + LoRA(r=8) + AWM(Ours) & \(34.27\) & \(49.98\) & \(\) & \(58.79\) & 0.005 & \(85.36\) \\ + LoRA(r=8) + AWM(Ours) & **35.00** & **50.97** & \(10.69\) & **58.91** & 0.001 & \(92.12\) \\  + LoRA(r=2) & \(32.19\) & \(48.64\) & \(10.99\) & \(58.73\) & - & - \\ + LoRA(r=2) + AWM(Ours) & \(33.41\) & \(49.78\) & \(11.30\) & \(58.91\) & 0.01 & \(77.20\) \\ + LoRA(r=2) + AWM(Ours) & \(33.29\) & \(49.14\) & \(\) & \(58.83\) & 0.005 & \(85.36\) \\ + LoRA(r=2) + AWM(Ours) & **33.66** & **49.98** & \(11.37\) & **58.93** & 0.001 & \(92.12\) \\  Llama-2-13b & \(23.75\) & \(33.41\) & \(19.33\) & \(60.89\) & - & - \\  + LoRA(r=8) & \(30.72\) & \(44.74\) & \(18.42\) & \(61.31\) & - & - \\ + LoRA(r=8) + AWM(Ours) & \(30.48\) & \(45.40\) & \(19.41\) & **61.45** & 0.01 & \(79.32\) \\ + LoRA(r=8) + AWM(Ours) & \(30.84\) & \(45.09\) & \(19.26\) & \(61.29\) & 0.005 & \(87.04\) \\ + LoRA(r=8) + AWM(Ours) & **32.07** & **45.88** & **19.48** & \(61.28\) & 0.001 & \(93.40\) \\   

Table 1: Results of Llama-2 fine-tuned on the query and value in attention module with LoRA  and AWM. We report the evaluation results on TruthfulQA , GSM8K , and HellaSwag . We also report the threshold of AWM and the portion of the masked query weight vector pair.

proposed method are twofold: 1) further reduce the number of trainable parameters during fine-tuning, and 2) fix the orthogonal query and key weight vector pairs to prevent overfitting.**

## 5 Conclusion and Limitation Discussion

We provide a new perspective to study how LLMs with Rotary Position Embedding (RoPE) utilize the position information. We show that non-orthogonal query and key weight vector pairs in RoPE draw higher attention to certain relative positions regardless of the input, which may correspond to processing basic syntactic information while the orthogonal query and key weight vector pairs in RoPE are more flexible with the relative position, which may correspond to processing high-level semantic information. Our analysis and various empirical results at the layer level, attention head level, and neuron level are provided in Sec. 3. By comparing the pre-trained model and the fine-tuned model, we show that fine-tuning mainly updates near orthogonal weight vector pairs and further propose a method to reduce the number of trainable parameters during LLM fine-tuning in Sec. 4.

The limitation of the proposed method is that it only applies to the query or the key weights which may limit the number of reduced trainable parameters. Specifically, the query and key weight vector pairs are near orthogonal at random initialization, which makes the non-orthogonal query and key weight vector pairs non-trivial. The phenomena presented in previous works and in this paper imply that the non-orthogonal query and key weight vector pairs play an important role in LLMs using RoPE, which requires further investigation and we leave it for future works. This paper provides a new perspective on investigating large language models (LLMs), which have the potential to make LLMs more accessible and effective for various applications. However, we must also address the potential risks associated with their misuse.