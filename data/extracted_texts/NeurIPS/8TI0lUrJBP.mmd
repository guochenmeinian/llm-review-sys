# Insights on Disagreement Patterns in Multimodal Safety Perception across Diverse Rater Groups

Charvi Rastogi

Google DeepMind

&Tian Huey Teh

Google DeepMind

 Pushkar Mishra

Google DeepMind

&Roma Patel

Google DeepMind

&Zoe Ashwood

Google DeepMind

&Aida Davani

Google Research

&Mark Diaz

Google Research

&Michela Paganini

Google DeepMind

&Alicia Parrish

Google DeepMind

&Lora Aroyo

Google DeepMind

&Vinodkumar Prabhakaran

Google Research

&Verena Rieser

Google DeepMind

###### Abstract

AI systems crucially rely on human ratings, but these ratings are often aggregated, obscuring the inherent diversity of perspectives in real-world phenomenon. This is particularly concerning when evaluating the safety of generative AI, where perceptions and associated harms can vary significantly across socio-cultural contexts. While recent research has studied the impact of demographic differences on annotating text, there is limited understanding of how these subjective variations affect multimodal safety in generative AI. To address this, we conduct a large-scale study employing highly-parallel safety ratings of about 1000 text-to-image (T2I) generations from a demographically diverse rater pool of 630 raters balanced across 30 intersectional groups across age, gender, and ethnicity. Our study shows that (1) there are significant differences across demographic groups (including intersectional groups) on how severe they assess the harm to be, and that these differences vary across different types of safety violations, (2) the diverse rater pool captures annotation patterns that are substantially different from expert raters trained on specific set of safety policies, and (3) the differences we observe in T2I safety are distinct from previously documented group level differences in text-based safety tasks. To further understand these varying perspectives, we conduct a qualitative analysis of the open-ended explanations provided by raters. This analysis reveals core differences into the reasons why different groups perceive harms in T2I generations. Our findings underscore the critical need for incorporating diverse perspectives into safety evaluation of generative AI ensuring these systems are truly inclusive and reflect the values of all users.

## 1 Introduction

The increasing ubiquity of AI systems in everyday tasks underscores the urgent need to minimize the potential harms of AI-generated content, such as violence, misinformation, and violations of social norms. For instance, recent work has found AI generations, across different modalities, to be amplifying stereotypes , disseminating misinformation , and even facilitating malicious activities . The perception of such harms is often highlysubjective, shaped by individuals' lived experiences and cultural contexts (Denton et al., 2021) -- what may be considered safe or appropriate for one person might be perceived as offensive or harmful by another.

While current efforts typically focus on the intricacies of collecting data that cover a wide range of such potential safety failures, they often neglect the nuanced and subjective viewpoints held by diverse user groups that can have unintended repercussions in real-world scenarios. The phenomenon of subjective harm perception has been well-documented in text-to-text scenarios (e.g. Curry et al., 2021; Aroyo et al., 2024; Bergman et al., 2024; Kirk et al., 2024). Some recent work also started to investigate text-to-image (T2I) scenarios (Wan et al., 2024; Naik and Nushi, 2023). However, due to their recent emergence, extensive investigations into the diversity of safety perceptions regarding T2I generations are lacking. To prevent undermining their responsible development, more focus is needed on understanding nuanced potential harms of T2I generations.

Gathering meaningful insights into the diverse perceptions of safety of T2I models presents a significant challenge. Human variability in responses to generated content can make it difficult to obtain statistically significant conclusions from user feedback alone. Moreover, most current safety evaluations rely on aggregation methods that can inadvertently silence the perspectives of less-well-represented groups (Prabhakaran et al., 2021; Blodgett, 2021). This inherent subjectivity and the risk of exclusion necessitates carefully designed research methodologies that can effectively capture and analyze the diverse perspectives of users.

To address this gap, we introduce DICES-T2I, a novel framework for analyzing safety perceptions in text-to-image (T2I) models across diverse demographics. Building on the success of DICES-T2T (Aroyo et al., 2024), our approach enables granular analysis of how different social groups perceive and experience potential harms in AI-generated images.

We collected safety ratings from a large and diverse participant pool, carefully stratified across 30 intersection groups spanning age, gender, and ethnicity. This meticulous design ensures balanced representation and robust statistical analysis, allowing us to uncover nuanced relationships between user identity and safety perceptions.

DICES-T2I provides valuable insights for diversifying T2I safety evaluations, moving beyond one-size-fits-all approaches and paving the way for more inclusive and equitable AI systems.

Our key findings are:

* **Demographic groups differ in their _severity assessment_ of safety violations in AI generated images**, e.g.,
* _Gen-Z_ raters and _Women_ raters are more likely to flag generations as _unsafe_.
* there is significant disagreement variance across violation types, e.g., safety violations related to _bias_ have high levels of disagreement compared to _violence_ and _sexual_ topics.
* _inter-sectional groups_ tend to agree with each other (i.e. cohesive perspectives) but divergent from other groups, e.g., _Gen-Z Black_ and _Millennial Black_ raters have uniquely divergent perspectives that are obscured if we consider the age or ethnicity groups separately.
* **Diverse raters show different _annotation patterns_ than safety policies expert raters**. In particular, the expert raters deemed many prompt-image pairs (especially for the violation type _bias_) to be safe, that is at odds with many in our diverse rater pool, suggesting potential gaps in policy.
* **T2I safety disagreements differs from prior experiments with T2T**.
* stereotyping and bias are salient issues in DICES-T2I often went undetected by expert raters
* in DICES-T2I 25% images that were deemed safe by a majority of "expert" raters and unsafe according to demographically diverse raters, where in DICES-T2T the "expert" label were skewed more heavily towards the "unsafe" compared to diverse rater'
* **Rater comments reveal differences in harm justification**, e.g. _White women raters_ often justified their harm ratings by referencing the potential negative impact on others. This tendency to consider secondhand harm may lead to overestimating the actual harmfulness of the content, and potentially contribute to the observation that women, in general, exhibit higher sensitivity to harm.

These findings not only inform ongoing efforts to develop and deploy generative models in a responsible and ethical manner, but can also help ensure the potential benefits of these models are realized without compromising the safety and well-being of users.

Related work

Our work bridges existing research on collecting human perspectives on safety of textual data and research on quantifying agreement (or disagreement) of annotators on natural language tasks. There is a growing need to measure the _safety of AI-generated outputs_ from the perspective humans interacting with such systems. Weidinger et al. (2021) outline the harms that text-based LLMs could introduce to human users, and the current gaps in evaluations that comprehensively measure this.

Measuring disagreement of ratters.Several recent studies emphasize the importance of considering individual annotator ratings, rather than simply aggregating ratings for each annotated sample (Aroyo and Welty, 2015; Palomaki et al., 2018). For a range of standard natural language tasks, e.g., natural language inference (Pavlick and Kwiatkowski, 2019), word sense disambiguation (Erk and McCarthy, 2009), co-reference (Recasens et al., 2011), simple aggregation often fails to capture the full spectrum of annotator ratings. On more subjective safety related tasks, previous work has demonstrated that demographics are a useful predictor for modeling these differences (e.g. Mostafazadeh Davani et al., 2022; Sap et al., 2021). Davani et al. (2024) also demonstrates how individual moral values play a role in annotator disagreements. Most pertinent to our work, Aroyo et al. (2024) underscore the importance of diversity in annotations for safety data, as individuals from different demographic groups exhibit varying safety perceptions. This observation is mirrored in the preference ratings collected by Kirk et al. (2024) and the perceived abuse annotations in dialogue data by Curry et al. (2021). Our research builds upon this body of work by specifically examining human disagreements from diverse groups of humans for multimodal inputs. We demonstrate the contextual and cultural dependence of human perspectives on image-text safety annotations, enabling us to measure the varying degrees of perceived harm among annotators from diverse demographic backgrounds.

## 3 Methods for data analyses

We conducted human-subject data collection studies to collect demographically diverse safety perception of T2I model outcomes. This constituted three main steps. First, we curated a dataset of roughly 1000 prompt-image pairs, this process is described in Section A.1. Second, we designed a response form for eliciting raters' safety perception of the prompt-image pairs, described in Section A.2. Lastly, we recruited raters uniformly from different demographic trisections based on age, gender and ethnicity. Further, we designed the study to have roughly uniform coverage of responses from different demographic raters on our prompt-image set. The details are provided in Section A.3.

With the data collected, we devise data analysis methods to first understand differences between demographic groups' safety perceptions, and then to understand the value addition of demographically diverse ratings, by comparing diverse safety evaluations with those of expert raters.

### Methods for comparing demographically diverse raters

The recognition that individuals possess unique sensitivities and cultural backgrounds necessitates an exploration of how demographic factors influence perceptions of harm in AI-generated content. We study whether certain groups exhibit heightened sensitivity, or are there disparities in how different demographics interpret the potential risks associated with text-to-image outputs.

Metrics for comparing raters.We test whether the gender, ethnicity, and age-group of the rater influence their individual perception of safety in T2I generated images, by comparing the annotations to the question: 'How harmful do you find this?'. For gender, we use the Mann-Whitney test (Mann and Whitney, 1947), which provides the normalized U statistic quantifying roughly the probability of one gender group giving a higher response than the other. When comparing ratings by gender, our test accounts for confounding by age and ethnicity of the raters and the content type.

As described in A.3, raters belong to one of 5 ethnicities (White, Black, Latine, South Asian, and East Asian) and one of 3 age groups (Gen-X, Millennial, Gen-Z). To understand differences within groups based on ethnicity and age we utilise the Kruskal-Wallis test (Kruskal and Wallis, 1952), modified to account for confounding by other demographics and content type. The Kruskal-Wallis test is a variation of the Mann-Whitney test designed to handle more than two groups.

While the above analyses shed light on differences across demographics in personal safety perceptions, it is crucial to also understand differences within demographic groups, when providing their evaluation for overall safety. To this end, we use the maximum of the two harmfulness scores provided by the raters, and apply the GRASP framework (Prabhakaran et al., 2024). GRASP provides a flexible way to measure group associations (GAI) in perspectives by combining in-group and cross-group cohesion, along with statistical significance using permutation tests. Briefly, GAI contrasts agreement among in-group members with agreement between in- and out-group members, and computes a ratio between the two. To correct for multiple testing of significance for GAI values, we use the Benjamini-Hochberg method.

Qualitative inspection of comments.As a follow up, we conducted an initial exploration of their comments explaining their harm judgments. Important to note is that comments were optional and only 8.4% (2,971) of ratings submitted had accompanying comments. Therefore, we used GAI scores to select subgroup comments for analysis. To extract high-level patterns from the comments, we used each group of comments as input and prompted a LLM with the following instructions: "_The following portions of text are comments from evaluators about different harmful image content. Each individual comment is on a new line. Summarize all of the comments. Note any interesting patterns in topics, themes, and content that are highlighted across the comments. Be as specific as possible and use specific comments and quotes as examples:_" We then compared differences in the summaries.

### Methods for comparing expert raters with demographically diverse raters

Setup for comparing diverse raters and expert raters.In the data, we have 5 expert raters and 32 diverse raters annotating each prompt-image pair on average. Recall that expert raters annotate the safety of a pair as'safe', 'unsafe' or 'unsure', while diverse raters' annotation can be one of [0,1,2,3,4,'unsure'] where 0 implies completely safe and 4 completely unsafe. In order to be able to compare the two rater groups given the differences in their annotation scales, we first aggregate the annotations per pair into a single rating for each rater group. The aggregated rating for experts is defined as the mode of the expert annotations, i.e., the plurality vote, and referred to as the _expert label_ for the prompt-image pair. In the dataset, there are 5 prompt-image pairs where the expert label is 'unsure', we discard these in following analyses. In 7 pairs, the number of expert annotations of safe and unsafe are tied. For these, we define the the expert label as 'unsafe'.

To aggregate the annotations provided by diverse raters per prompt-image pair, we take the mode of the greater of the two harmfulness scores (harm to self and harm to others) provided by diverse raters. The aggregated score is referred to as the _plurality score_ for the prompt-image pair. Figure 4 presents histograms for the frequency of the expert label and the plurality score per prompt-image pair. We see that the mode in this data represents the majority view on average on both the expert raters side as well as the diverse raters side. Additionally, mode is simple to compute, exhibits robustness to outliers, preserves the discrete nature of the annotations, and is intuitive to understand.

Metrics for comparing diverse raters and expert raters.To gain deeper insights into the annotation patterns of different groups of diverse raters vs. expert raters, we compute precision and recall based on plurality scores with expert labels as the ground truth. For a group of diverse raters, we compute Precision and Recall at different thresholds X \(\) {0, 1, 2, 3, 4} by treating plurality score \(\) X as unsafe. Then, _1 - Precision at threshold X_ conveys the rate of disagreement between expert raters and the group of diverse raters when prompt-image pairs with plurality score \(\) X from the group are taken as unsafe. Recall at threshold X conveys the rate of occurrence of plurality scores \(\) X from the group of diverse raters on prompt-image pairs that have unsafe as the expert label. In other words, recall at threshold X gives the proportion of unsafe pairs on which a plurality score \(\) X occurred.

## 4 Results

### How do demographic groups differ in perceiving harm?

Group likelihood of perceiving harm.We conducted significance tests for differences between groups across the dimensions of Gender, Age and Ethnicity. To contextualise the results of the tests, Table 1 provides the mean scores provided by raters belonging to different demographic identities, in response to the two questions posed to them in our study: (1) How harmful or offensive do you find this image? (2) How harmful or offensive would any other people find this image, on a scale of 0-4. When comparing demographics likelihood of perceiving harm, we consider their response to the first question, associated to their personal perception of harm.

The Mann-Whitney U test conducted to compare Men raters and Women raters suggests that on average women are likely to perceive more safety issues in images (i.e., give a higher score) with probability 0.55 (p-value < 0.001). On an absolute scale, we see that Women raters have a mean score of 1.08 and Men raters have 0.85. Next, we conducted the Kruskal-Wallis test to compare multiple groups at once, for analysing difference between age groups and ethnicities. The results imply that both ethnicity and age axes have groups with statistically significant differences among them. For illustration of the magnitude of difference, we compare the two sub-groups with the highest difference within each axis using the normalized Mann-Whitney U statistic. The test outcomes suggest that Black raters are likely to give a higher score than White raters with probability 0.57, and Gen-Z raters are likely to give a higher score than Gen-X raters with probability 0.503.

Demographic group cohesion.First, for each demographic grouping we see that raters disagree across the board, yielding an average IRR (inter-rater reliability) of 0.25 (refer Table 5). Next, our tests of rater cohesion show that raters grouped at the highest demographic levels corresponding to age, gender or ethnicity alone do not seem to have high GAIs, Table 2. The group with the highest group association index is Black raters with a magnitude of 1.11, which is significant after multiple testing correction. All other groups have lower GAI scores (all are lower than \(1.07\)) that are not statistically significant after multiple testing correction. Table 5 further breaks down the GAI values into group-based IRR and XRR to understand the source of disagreement.

When considering grouping based on intersections of two or more demographic groups, we see that several intersectional groups between age and ethnicity categories have high GAI values. Section B.1 provides tables showing the IRR, XRR and GAI values corresponding to different intersectional groupings. Noteworthily, Gen-Z-Black raters and Millennial-Black raters have the two highest GAI with values of 1.38 and 1.29 respectively which are also statistically significant (refer Table 8). Another thing to note is that Gen-Z raters as a group have a relatively low GAI of 1.03 in Table 5, however most ethnicity-based intersectional groups containing Gen-Z raters have more cohesive perspectives on safety-Gen-Z-Black, Gen-Z-White, Gen-Z Latinx being the main groups with high GAI values (Table 8). We see a similar trend for Women raters: their aggregate GAI value is 1.04, however most ethnicity-based intersectional groups containing Woman raters have higher GAI values, with Black-Woman raters at 1.12, South-Asian-Woman raters at 1.15, and White-Woman raters at 1.14 (Table 6).

  &  &  &  \\   & M & W & Gen-X & Mil. & Gen-Z & B & W & SA & EA & Lat. \\  “How harmful to you?” & 0.85 & 1.08 & 0.96 & 0.96 & 0.97 & 1.2 & 0.77 & 1.04 & 0.91 & 0.9 \\ “How harmful to others?” & 1.24 & 1.33 & 1.27 & 1.30 & 1.28 & 1.35 & 1.15 & 1.36 & 1.24 & 1.32 \\  

Table 1: Table shows mean harmfulness ratings for different groups of raters, when asked to assess how harmful the prompt-image pairs are to them and how harmful it might be to other people. The ratings provided for each question range from 0 (completely safe) to 4 (completely unsafe).

  &  &  &  \\   & M & W & Gen-X & Mil. & Gen-Z & B & W & SA & EA & Lat. \\  GAI & 0.98 & 1.04* & 0.97 & 1.05* & 1.04 & 1.12** & 1.06* & 1.04 & 0.98 & 0.99 \\  

Table 2: Obtained values for GAI (Group Association Index) for each high-level demographic grouping. Significance at \(p<0.05\) is indicated by *, and significance at \(p<0.05\) after correcting for multiple testing is indicated by **.

### Results from comment analysis

Based on their significant GAI scores, we qualitatively analyzed comments written by Black-Woman, East Asian-Woman, and White-Woman raters.

Comment themes.Overall, we found that similar concerns were raised across each rater group, including sexual content, stereotyping and bias, violent content, and disturbing image distortions. Interestingly, White women mentioned a broader range of issues in images, particularly related to stereotypes and biases. Distinct from East Asian and Black women, White women made references to biases rooted in nationalities and various professions, particularly professions stereotypically associated with women. For example, one White woman rater noted, "_It depicts Argentinians as living a stereotypically poor life, and might be offensive to Argentinians_" and another wrote, "_A secretary does not have to be a woman. A woman can be sat at a table without looking stereotypically provocative, wearing make-up, pointed heels and with her skirt hoiked [sic] up._" Although East Asian women and Black women also had significant GAI scores, summaries of their comments did not point to notable differences in reasoning compared to other groups. We conduct a follow up reference analysis that investigates the references observed in the comments of intersectional Woman raters, the details are provided in Appendix B.1.1.

### How do diverse raters differ from expert raters in safety evaluations?

We first present the trends over all the prompt-image pairs and then focus on pairs where all expert raters are in agreement with each other. The latter helps to focus on how certain prompt-image pairs, even when rated as safe or unsafe by all experts, are perceived differently by raters from different demographic groups.

There are three violation types observed in the prompt-image pairs, namely, 'Sexual', 'Violent', and 'Bias'. To understand how expert raters and diverse raters compare on the different violation types, in figure 1, we present the rate of disagreement and rate of occurrence curves per violation type. We note that disagreement is noticeably higher for Bias than the other two violation types. This suggests that diverse raters tend to notice instances of bias at all thresholds that expert raters do not find unsafe. Meanwhile, for violent prompt-image pairs, diverse raters tend to use non-zero scores the least, indicating that many of the instances that were deemed violent by expert raters were not perceived as violent by diverse raters.

Further, in Figure 2, we plot rate of disagreement and rate of occurrence curves for various groups of diverse raters vs. expert raters, where grouping is by the top-level demographic trait, i.e., age, ethnicity, or gender. We see that rate of occurrence is similar at all thresholds when raters are grouped by age. However, Gen-Z raters have the highest disagreement at all the thresholds. This indicates that they tend to find several prompt-image pairs unsafe that raters from other age groups or expert raters don't. Looking at the curves for ethnicity, we can see that White raters tend to use scores \( 3\) the least, but when they do, they achieve the lowest disagreement with expert raters. This indicates

Figure 1: Figure shows the rate of disagreement and rate of occurrence at different thresholds for all diverse raters vs. experts raters across different violation types. The band around the plots gives the 95% confidence interval with sample size being \(n\).

that they may be better at discerning the severity of violations than the other groups. On the other hand, Black raters tend to use scores \( 3\) the most. Latinx raters have higher disagreement and lower rates of occurrence than others at almost all the thresholds, meaning that they are the least aligned with expert raters. Finally, looking at the curves for gender, we see that Men and Women raters have similar rates of disagreement vs. expert raters, but Women raters tend to use scores \( 3\) more with slightly lower disagreement, indicating that may be more sensitive to the severity of violations. In Appendix B.2 we break down the analysis further and compare groups of diverse raters vs. expert raters on the different violation types.

## 5 Discussion

Variety of perspectives in text-to-image safety.One of the central questions of this study was how demographic differences in raters map on to differences in their rating behavior in assessing the safety of text-to-image model generations. Broadly, we observe differences between groups' safety ratings across each demographic axis considered in this study, namely, age, gender and ethnicity. Specifically, Women raters and Black raters show higher sensitivity by providing higher harmfulness scores, compared to Men raters and White raters respectively. Meanwhile, we do not see sizeable difference in the scores provided by different age groups as a whole.

Furthermore, our group association index (GAI) analysis reveals that considering groupings based on only high-level demographic axes is not sufficient and may miss crucial information. This is evidenced by the low GAI values for high-level demographic groupings and relatively higher GAI values when considering intersectional groupings. For instance, Women raters as a whole exhibit lower cohesion than intersectional groups comprised of Women raters with specific ethnic backgrounds. We see a similar trend with Gen-Z raters. This emphasizes the _importance of incorporating more granular demographic information when accounting for differences in safety evaluations_.

Next, we delve deeper into group cohesion. For Black raters in particular, we see that their high GAI scores are driven primarily by a lower XRR, meaning that Black raters systematically disagree with raters from other demographic groups. Looking at the agreement rates for intersectional groups, we see that the higher GAI scores for Black raters persists across different age groups (though is only significant for Gen-Z and Millenial Black raters)--in these cases the higher GAI is actually driven by a high IRR. This means that while Black raters as a whole do not have particularly high within-group agreement, age-based subgroups within Black raters do have high in-group agreement.

Figure 2: Figure shows the rate of disagreement and rate of occurrence at different thresholds for groups of diverse raters vs. expert raters, where grouping is by (a) age, (b) ethnicity, and (c) gender.

Along with the GAI scores, the qualitative analysis of rater comments suggests that East Asian women, Black women, and White women may be more frequently assessing harm based on the perspectives of others, compared with other groups, which may help to explain women raters' overall higher sensitivity to harm. This is not necessarily to suggest, however, that White women are more comprehensive in their judgments. While all raters were presented with the same task instructions, there is an inherent level of interpretation wherein individuals apply relevant personal experiences. Moreover, our analysis does not speak to coverage or other differences among raters within the thematic patterns they commonly named. Most importantly, the analysis demonstrates the value of qualitative inspection alongside agreement metrics.

"Expert" and diverse raters give complementary signals.In line with previous work (Aroyo et al., 2024), we observe systematic differences between the safety ratings assigned by diverse rater populations and expert "gold" ratings. When stratifying by violation type, we see that images reflecting _bias_ have high disagreement rates between diverse raters and expert raters. Specifically, diverse raters flag safety issues in images reflecting societal biases, but expert raters rate the same images as safe. This likely reflects the highly subjective nature of stereotyping and bias harms. But it also reveals that ratings from a diverse pool need to be considered, as relying only on expert pools will likely miss some safety issues. Though images reflecting safety issues related to sexual or violent content had lower rate of disagreement overall, some images with violent content were deemed unsafe by experts but safe by diverse raters. More broadly, along the demographic axes, different rater groups were differently aligned with the expert raters, and further work on understanding the knowledge the two rater pools bring to safety rating tasks is important for more holistic evaluations.

Understanding the role of image modality.The overall trend we observed where 25% images that were deemed safe by a majority of "expert" raters and unsafe according to demographically diverse raters is the opposite trend as what was observed in the DICES T2T dataset, where the gold label skewed more heavily towards the "unsafe" label compared to the crowd rating Wang et al. (2024). Although our dataset includes text paired with images, images can carry more varied or unclear semantic meaning. While we do not provide an explanation for this opposite trend, the difference in data modality does introduce complication regarding interpretability for raters. For example, a reference to an "Asian man" in text is undeniable even if the reader's conception of "Asian" differs from that of another reader, whereas an image meant to represent an Asian man can be interpreted as a person of a different identity group or a more specific identity group (e.g., Japanese man; Bengali man) or be completely ambiguous in ethnicity or gender depending on the makeup of the image. As a result, images may take on a wider range of interpretations for raters compared with text alone.

## 6 Future work and limitations

Who should rate what type of content?One natural question that follows from this type of research is the question of how to determine who is best suited to rate which type of content. The question of suitability for a given rating task is one that needs to consider a wide range of factors, including the content of the prompt and model response, each rater's demographic background, each rater's value system, and the goals of the evaluation. The analyses in this paper should be considered as just a starting point to answering this much more involved question--for example, our dataset can be used for developing guidance on rating thresholds at which additional ratings are needed or for identifying signals in the prompt or image content that indicate the example is likely to trigger systematically different safety ratings from different groups. However, we caution that the signals available in this dataset are not sufficient to understand rater suitability from a social perspective, and consideration should still be given to how the groups specifically impacted by harms present in images can best be engaged in identification and mitigation processes, regardless of those groups' apparent sensitivity to "safety" in this task.

How to elicit safety evaluations from the crowd?Our findings highlight large deviations in annotations when raters are asked to assess the harmfulness of a prompt-image pair from a personal standpoint as opposed to a standpoint that considers harmfulness to other people. Further, we also see differences across demographic-based groups and sub-groups in their use of the harmfulness scale. While some groups use the extreme values more (0 or 4, the scale endpoints), some other groups make use of the central values more (values between 1 and 3). Thus, safety evaluation is a highly subjective task with a multitude of possible interpretations and calibrations, as evidenced in our findings. An important follow-up question is how to appropriately elicit people's opinions on this task to minimize discord due to the design of the task and collect appropriate feedback.

Limitations.First, we do not have further information beyond the demographics of the individual raters, and demographics are only a proxy for who these raters are. We do not have information on their social backgrounds, value alignment or rating rationales beyond the comments provided. Second, we only have the demographic information of the raters, not the experts. Hence, we are unable to conduct analysis of the potential influence of the expert's demographic information. Lastly, we cannot disentangle effects of prompt safety and image safety (or their interaction) with this study.