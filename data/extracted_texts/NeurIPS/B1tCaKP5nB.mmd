# A Conditional Independence Test in the

Presence of Discretization

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Testing conditional independence (CI) has many important applications, such as Bayesian network learning and causal discovery. Although several approaches have been developed for learning CI structures for observed variables, those existing methods generally fail to work when the variables of interest can not be directly observed and only discretized values of those variables are available. For example, if \(X_{1}\), \(_{2}\) and \(X_{3}\) are the observed variables, where \(_{2}\) is a discretization of the latent variable \(X_{2}\), applying the existing methods to the observations of \(X_{1}\), \(_{2}\) and \(X_{3}\) would lead to a false conclusion about the underlying CI of variables \(X_{1}\), \(X_{2}\) and \(X_{3}\). Motivated by this, we propose a CI test specifically designed to accommodate the presence of discretization. To achieve this, a bridge equation and nodewise regression are used to recover the precision coefficients reflecting the conditional dependence of the latent continuous variables under the nonpara-normal model. An appropriate test statistic has been proposed, and its asymptotic distribution under the null hypothesis of CI has been derived. Theoretical analysis, along with empirical validation on various datasets, rigorously demonstrates the effectiveness of our testing methods.

## 1 Introduction

Independence and conditional independence (CI) are fundamental concepts in statistics. They are leveraged for exploring queries in statistical inference, such as sufficiency, parameter identification, adequacy, and ancillarity . They also play a central role in emerging areas such as causal discovery , graphical model learning, and feature selection . Tests for CI have attracted increasing attention from both theoretical and application sides.

Formally, the problem is to test the CI of two variables \(X_{j_{1}}\) and \(X_{j_{2}}\) given a random vector (a set of other variables) \(\). In statistical notation, the null hypothesis is written as \(H_{0}:X_{j_{1}} X_{j_{2}}\), where \(\) denotes "independent from." The alternative hypothesis is written as \(H_{1}:X_{j_{1}} X_{j_{2}}\), where \(\) denotes "dependent with." The null hypothesis implies that once \(\) is known, the values of \(X_{j_{1}}\) provide no additional information about \(X_{j_{2}}\), and vice versa. Different tests have been designed to handle different scenarios, including Gaussian variables with linear dependence  and non-linear dependence  (_For detailed related work, please refer to App. D_).

Given observations of \(X_{j_{1}}\), \(X_{j_{2}}\), and \(\), the CI can be effectively tested with existing methods. However, in many scenarios, accurately measuring continuous variables of interest is challenging due to limitations in data collection. Sometimes the data obtained are approximations represented as discretized values. For example, in finance, variables such as asset values cannot be measured and are binned into ranges for assessing investment risks (e.g., sell, hold, and strong buy) . Similarly, in mental health, anxiety levels are often assessed using scales like the GAD-7, which categorizesresponses into levels such as mild, moderate, or severe [23; 17]. In the entertainment industry, the quality of movies is typically summarized through viewer ratings [29; 10].

When discretization is present, existing CI tests can fail to determine the CI of underlying continuous variables. This issue arises because existing CI tests treat discretized observations as observations of continuous variables, leading to incorrect conclusions about their CI relationships. More precisely, the problem lies in the discretization process, which introduces new discrete variables. Consequently, _although the intent is to test the CI of the underlying continuous variables, what is actually being tested is the CI involving a mix of both continuous and newly introduced discrete variables_. In general, this CI relationship is inconsistent with the one among the underlying continuous variables.

As illustrated in Fig. 1, we show different data-generative processes using causal graphical models  in the presence of discretization. A gray node indicates an observable variable, while a white node indicates a latent variable. Variables denoted by \(X_{j}\) (without a tilde \(\)) represent continuous variables, which may not be observed; while variables denoted by \(_{j}\) represent observed discretized variables derived from \(X_{j}\) due to discretization. In Fig. 1(a), \(X_{2}\) is latent, and only its discrete counterpart \(_{2}\) is observed. In this case, rather than observing \(X_{1}\), \(X_{2}\), and \(X_{3}\), we only observe \(X_{1}\), \(_{2}\), and \(X_{3}\). Existing CI methods use these observations to test _whether_\(X_{1} X_{3}\{X_{2}\}\), but what is actually being tested is _whether_\(X_{1} X_{3}\{_{2}\}\). In fact, according to the _causal Markov condition_,, it can be inferred from Fig. 1(a) that \(X_{1} X_{3}\{X_{2}\}\) and \(X_{1} X_{3}\{_{2}\}\). This mismatch leads to existing CI methods, that employ observations to check the CI relationships between \(X_{1}\) and \(X_{3}\) given \(X_{2}\), to reach incorrect conclusions. Due to the same reason, checking the CI also fails in Fig 1(b) and Fig 1(c).

In this paper, we design a CI test specifically for handling the presence of discretization. An appropriate test statistic for the CI of latent continuous variables, based solely on discretized observations, is derived. The key is to build connections between the discretized observations and the parameters needed for testing the CI of the latent continuous variables. To achieve this, we first develop bridge equations that allow us to estimate the covariance of the underlying continuous variables with discretized observations. Then, we leverage a _node-wise regression_ to derive appropriate test statistics for CI relationships from the estimated covariance. By assuming that the continuous variables follow a Gaussian distribution, we can derive the asymptotic distributions of the test statistics under the null hypothesis of CI. The major contributions of our paper include that

* We develop a CI test for ensuring accurate analysis in scenarios where data has been discretized, which are common due to limitations in data collection or measurement techniques, such as in financial analysis and healthcare.
* Our CI test can handle various scenarios including 1). Both variables \(X_{j_{1}}\) and \(X_{j_{2}}\) are discretized 2). Both variables \(X_{j_{1}}\) and \(X_{j_{2}}\) are continuous. 3). One of the variables \(X_{j_{1}}\) or \(X_{j_{2}}\) is discretized.
* We compare our test with the existing methods on both synthetic and real-world datasets, confirming that our method can effectively estimate the CI of the underlying continuous variables and outperform the existing tests applied on the discretized observations.

## 2 DCT: A CI Test in the Presence of Discretization

Problem SettingConsider a set of independent and identically distributed (i.i.d.) \(p\)-dimensional random vectors, denoted as \(}=(X_{1},X_{2},,_{j},,_{p})^{T}\). In this set, some variables, indicated by a tilde (\(\)), such as \(_{j}\), follow a discrete distribution. For each such variable, there exists a corresponding latent Gaussian random variable \(X_{j}\). The transformation from \(X_{j}\) to \(_{j}\) is governed by an unknown monotone nonlinear function \(g_{j}\). This function, \(g_{j}:}\), maps the continuous domain of \(X_{j}\) onto the discrete domain of \(}_{j}\), such that \(_{j}=g_{j}(X_{j})\) for each observation. Given \(n\) observations \(\{}^{1},}^{2},,}^{n}\}\) randomly sampled from \(}\), specifically, for each variable \(X_{j}\), there

Figure 1: We illustrate different data generative processes with causal graphical models. The discretization process introduces new discrete variables which are denoted with a tilde (\(\)).

exists a constant vector \(=(d_{1},,d_{M})\) characterized by strictly increasing elements such that

\[_{j}^{i}=1&0<g_{j}(x_{j}^{i})<d_{1}\\ m&d_{m-1}<g_{j}(x_{j}^{i})<d_{m}\\ M&g_{j}(x_{j}^{i})>d_{m}\] (1)

This model is also known as the nonparanormal model . The cardinality of the domain after discretization is at least 2 and smaller than infinity. Our goal is to assess both conditional and unconditional independence among the variables of the vector \(=(X_{1},X_{2},,X_{j},,X_{p})^{T}\). In our model, we assume \( N(0,)\), \(\) only contain 1 among its diagonal, i.e., \(_{jj}=1\) for all \(j[1,,p]\). One should note this assumption is _without loss of generality_. We provide a detailed discussion of our assumption in App. A.8.

Preliminary Framework of DCTTo develop an independence test, one needs to design a test statistic that can reflect the dependence relation and be calculated from observations. Next, it is essential to derive the underlying distribution of this statistic under the null hypothesis that the tested variables are conditionally (or unconditionally) independent. By calculating the value of the test statistic from observations and determining if this statistic is likely to be sampled from the derived distribution (i.e., calculating the _p-value_ and comparing it with the significance level \(\)), we can decide if the null hypothesis should be rejected.

Our objective is to deduce the independence and CI relationships within the original multivariate Gaussian model, based on its discretized observations. In the context of a multivariate Gaussian model, this challenge is directly equivalent to constructing statistical inferences for its covariance matrix \(=(_{j_{1},j_{2}})\) and its precision matrix \(=(_{j,k})=^{-1}\). The covariance matrix \(\) captures the pairwise covariances between variables, while the precision matrix \(\) (also known as the concentration matrix) provides information about the CI between variables. Specifically, the entry \(_{j,k}\) in the precision matrix is related to the partial correlation coefficient between variables \(X_{j}\) and \(X_{k}\), which can be used to test whether these variables are conditionally independent given some other variables. Technically, we are interested in two things: (1) the calculation of the covariance \(_{j_{1},j_{2}}\) and the precision coefficient (or the partial correlation coefficient) \(_{j,k}\), serving as the estimation of \(_{j_{1},j_{2}}\) and \(_{j,k}\) respectively (in this paper, a variable with a hat indicates its estimation); and (2) the derivation of the distribution of \(_{j_{1},j_{2}}-_{j_{1},j_{2}}\) and \(_{j,k}-_{j,k}\) under the null hypothesis of independence and CI.

In the subsequent section, 1). we first introduce _bridge equations_ to address the estimation challenge of the covariance \(_{j_{1},j_{2}}\); 2). we proceed to derive the distribution of \(_{j_{1},j_{2}}-_{j_{1},j_{2}}\), demonstrating it is _asymptotically normal_; 3). utilizing _nodewise regression_, we establish the relationship between the covariance matrix \(\) and the precision matrix \(\), where the regression parameter \(_{j,k}\) acts as an effective surrogate for \(_{j,k}\). Leveraging the distribution of \(_{j_{1},j_{2}}-_{j_{1},j_{2}}\), we further illustrate that \(_{j,k}-_{j,k}\) is also _asymptotically normal_.

### Design _Bridge Equation_ for Test Statistics

Estimating Covariance with Bridge EquationsThe bridge equation establishes a connection between the underlying covariance \(_{j_{1},j_{2}}\) of two continuous variables \(X_{j_{1}}\) and \(X_{j_{2}}\) with the observations. When in the presence of discretization, the discrete transformations make the sample covariance matrix based on \(}\) inconsistent with the covariance matrix of \(\). To obtain the estimation \(_{j_{1},j_{2}}\) of \(_{j_{1},j_{2}}\), the bridge equation is leveraged. In general, its form is as follows.

\[_{j_{1},j_{2}}=T(_{j_{1},j_{2}};}),\] (2)

where \(_{j_{1},j_{2}}\) is the covariance needed to be estimated, \(_{j_{1},j_{2}}\) is a statistic that can also be estimated from observations, and \(}\) is a set of additional parameters required by the function \(T()\). The specific form of the function \(T()\) will be derived later. Both \(_{j_{1},j_{2}}\) and \(}\) should be able to be calculated purely relying on observations. _Then, given the calculated \(_{j_{1},j_{2}}\) and \(}\), \(_{j_{1},j_{2}}\) can be obtained by solving the bridge equation \(_{j_{1},j_{2}}=T(_{j_{1},j_{2}};})\)._ As a result, the covariance matrix \(\) of \(\) can be estimated, which contains information about both unconditional independence and CI (which can be derived from its inverse).

To estimate the covariance of a latent multivariate Gaussian distribution, we need to design appropriate \(_{j_{1},j_{2}}\), \(}\), and \(T()\). Notably, bridge equations have to be designed to handle all three possible cases:C1. both observed variables are discretized; C2. one variable is continuous while the other is discretized; and C3. both variables remain continuous. We will show that cases C1 and C2 can be merged into a single form of bridge equation with different parameters and a binarization operation applied to the observations. Our bridge equations are presented in Def. 2.2, Def. 2.3, and Def. 2.4.

Bridge Equations for _Discretized and Mixed Pairs_ Let us first address the challenging cases where both observed variables are discretized or where one variable is continuous while the other is discretized. In general, different bridge equations would need to be designed to handle each case individually. _However, in our analysis, we provide a unified bridge equation that is applicable to both cases._ This is achieved by binarizing the observed variables, thereby unifying both cases into a binary case. As some information may be lost in the binarization process, this unification may require more examples compared to using tailored bridge functions for each specific case. Developing specific bridge equations for each case to improve sample efficiency is left in future work.

Intiuitionally, for the original continuous variable \(X_{j}\), binarization separates it into two parts based on a boundary \(h_{j}\): the part for \(X_{j}\) larger than \(h_{j}\) and the part for \(X_{j}\) smaller than \(h_{j}\). In this case, we can estimate the boundary by calculating the proportion of \(X_{j}\) that exceeds the boundary. In the scenario of two variables where the threshold \(h_{j_{1}}\) and \(h_{j_{2}}\) divide the space into four regions, the proportions of these areas are influenced by the covariance \(_{j_{1},j_{2}}\), which connects the relation between the binarized variables with the latent covariance. This approach allows us to initially estimate the threshold \(h_{j_{1}}\), \(h_{j_{2}}\) of a pair of variables, followed by estimating the covariance \(_{j_{1},j_{2}}\).

Let \(_{n}Z\) denote the average of a random variable \(Z\) given \(n\) i.i.d. observation of \(Z\) and \(E[Z]\) as the true mean of \(Z\), \(\) as the probability and \(\) as the empirical probability. We then define the boundary \(h_{j}\) as follows: for any single discretized variable \(_{j}\), there exists a constant \(c_{j}\) such that:

\[\{_{j}^{i}>E[_{j}]\}=\{g_{j}(x_{j}^{i })>c_{j}\}=\{x_{j}^{i}>h_{j}\},\]

where \(h_{j}=g_{j}^{-1}(c_{j})\). Specifically, \(h_{j}\) is the boundary in the original continuous domain to determine if the discretized observation \(_{k}\) is larger than its mean. When the continuous variable \(X_{j}\) follows a normal distribution, there is a relation \((_{j}>E[_{j}])=1-(h_{j})\), where \(\) is the cumulative distribution function (cdf) of a standard normal distribution. We then provide the following definition:

**Definition 2.1**.: The estimated boundary can be expressed as \(_{j}=^{-1}(1-_{j})\), where \(_{j}=_{i=1}^{n}_{\{_{j}^{i}_{ n}_{j}\}}/n\), serving as the estimation of \((_{j}>E[_{j}])\).

Let \((z_{1},z_{2};)=(Z_{1}>z_{1},Z_{2}>z_{2})\), where \((Z_{1},Z_{2})^{T}\) follows a bivariate normal distribution with mean zero, variance one and covariance \(\). We define

\[_{j_{1},j_{2}}=(_{j_{1}}^{i}>E[_{j_{1}}], _{j_{2}}^{i}>E[_{j_{2}}])=(h_{j_{1}},h_{j_{2}}; _{j_{1},j_{2}}).\] (3)

That is, the proportion of discretized variables larger than their mean can be expressed as a function of underlying covariance. This equation serves as the key of estimating latent covariance based on the discretized observations. Specifically, we can substitute those true parameters with their estimation and construct the bridge equation to get the estimated covariance:

**Definition 2.2** (Bridge Equation for A Discretized-Variable Pair).: For discretized variables \(_{j_{1}}\) and \(_{j_{2}}\), the bridge equation is defined as:

\[_{j_{1},j_{2}}=(_{j_{1}}>_{n}_{j_ {1}},_{j_{2}}>_{n}_{j_{2}})=_{i=1}^ {n}_{\{_{j_{1}}^{i}>_{n}_{j_{1}}, {x}_{j_{2}}^{i}>_{n}_{j_{2}}\}}=T(_{j_{1},j_{2}};\{ _{j_{1}},_{j_{2}}\}),\]

and the function \(T(_{j_{1},j_{2}};\{_{j_{1}},_{j_{2}}\}):=( _{j_{1}},_{j_{2}};)=_{x_{1}>_{j_{1}}}_{x_{2}>_{j_{2}}}(x_{j_{1}},x_{j_{2}};)dx_{j_{1}}dx_{j_{2}},\)

where \(\) is the probability density function of a bivariate normal distribution, \(_{j_{1}},_{j_{2}}\) can be simply calculated using Def. 2.1.

Following the same intuition, we can directly apply the same bridge equation to estimate the covariance of mixed pairs. The only difference is there is no need to estimate the boundary \(_{j}\) for the continuous variable. Instead, we can incorporate its true mean of zero into the equation.

**Definition 2.3** (Bridge Equation for A Continuous-Discretized-Variable Pair).: For one continuous variable \(X_{j_{1}}\) and one discretized variable \(_{j_{2}}\), the bridge function is defined as follows:

\[_{j_{1},j_{2}}=(X_{j_{1}}>0,_{j_{2}}>_{n} _{j_{2}})=_{i=1}^{n}_{\{x_{j_{1}}^{i}>0, _{j_{2}}^{i}>_{n}_{j_{2}}\}}=T(_{j_{1},j_{2}} ;\{0,_{j_{2}}\}),\]

and the function \(T()\) has the same form of Def. 2.2.

A Bridge Equation for A Continuous-Variable PairWhen there is no discretized transformation, the sample covariance of \(X_{j_{1}}\) and \(X_{j_{2}}\) provides a consistent estimation. In this context, the function \(T\) acts merely as an identity mapping.

**Definition 2.4** (A Bridge Equation for A Continuous-Variable Pair).: For two continuous variables \(X_{j_{1}}\) and \(X_{j_{2}}\), the bridge equation is defined as:

\[_{j_{1},j_{2}}:=_{j_{1},j_{2}}=_{i=1}^{n} x_{j_{1}}^{i}x_{j_{2}}^{i}-_{i=1}^{n}x_{j_{1}}^{i} _{i=1}^{n}x_{j_{2}}^{i}=T(_{j_{1},j_{2}};).\]

For two continuous variables \(X_{j_{1}}\) and \(X_{j_{2}}\), the analytic solution of the estimated covariance can be simply obtained using Def. 2.4.

Calculation of Estimated CovarianceFor the continuous case, the analytic solution of \(_{j_{1},j_{2}}\) can be simply obtained using Def. 2.4. For the cases involving the discretized variable as proposed in Def. 2.2 and Def. 2.3, we can rely on the property that variance \(\) only contains \(1\) among the diagonal, which implies the covariance \(_{j_{1},j_{2}}\) should vary from \(-1\) to \(1\). Thus, we can calculate the estimated covariance by solving the objective

\[_{_{j_{1},j_{2}}}||_{j_{1},j_{2}}-T(_{j_{1},j_{2}}; \{_{j_{1}},_{j_{2}}\})||^{2} s.t.-1<_{j_{1},j_{2}}<1.\] (4)

The \(_{j_{1},j_{2}}\) is a one-to-one mapping with calculated \(_{j_{1},j_{2}}\), \(_{j_{1}}\) and \(_{j_{2}}\), which is proved in App. A.2

### Unconditional Independence Test

The estimation of covariance \(_{j_{1},j_{2}}\) can be effectively solved using the designed bridge equation. Now, we focus on deriving the distribution of \(_{j_{1},j_{2}}-_{j_{1},j_{2}}\). These results is used as an unconditional independence test in the presence of the discretization. Moreover, Thm. 2.5, Lem. 2.6, Lem. 2.7 and Lem. 2.8 will be leveraged in the derivation process of the CI test in Section 2.3. The detailed derivation steps for both unconditional test and CI test are relatively intricate, therefore, we will provide a general intuition. For a complete derivation, please refer to the App. A.3.

Assume we are interested in the true parameter \(_{0}\). We denote \(\) as its estimation which is close to \(_{0}\), and \(f()\) is a continuous function. By leveraging Taylor expansion, we have

\[f()=f(_{0})+f^{}(_{0})(-_{0}),\] (5)

which directly constructs the relationship between the estimated parameter with the true one. Re-arrange the term, we get \(-_{0}=(f()-f(_{0}))/f^{}(_{0})\). If the denominator is a constant and the numerator can be expressed as a sum of i.i.d samples, we can see \(-_{0}\) will be asymptotically normal according to the central limit theorem .

Let \(_{}=[f_{}^{1}(),f_{}^{2}(),f_ {}^{3}()]^{T}\) contains a group of functions parameterized by \(\) (For discretized pairs, \(=(_{j_{1},j_{2}},_{j_{1}},_{j_{2}})\)). Define \(_{n}_{}\) as sample mean of these functions evaluated at \(n\) sample points. Similarly, \(_{n}_{}{_{}}^{T}\) is defined as sample mean of the outer product \(_{}{_{}}^{T}\). The notation \(P_{}:=E_{n}_{}\) denotes the expectations of the functions in \(_{}\). Furthermore, let \(_{}{}^{}\) denote the derivative of the functions contained in \(_{}\). We now provide the main result of derived distribution \(_{j_{1},j_{2}}-_{j_{1},j_{2}}\) under the hull hypothesis that test pairs are independent.

**Theorem 2.5** (Independence Test).: _In our settings, under the null hypothesis that two observed variables indexed with \(j_{1}\) and \(j_{2}\) are statistically independent under our framework, i.e., \(_{j_{1},j_{2}}=0\), the independence can be tested using the statistic_

\[_{j_{1},j_{2}}=T^{-1}(_{j_{1},j_{2}};).\]_This statistic is approximated to follow a normal distribution, as detailed below:_

\[_{j_{1},j_{2}}}}{{}}N( 0,((_{n}_{}^{})^{-1}_{n} _{}_{}^{T}(_{n}_{}^{  T})^{-1})_{1,1}),\] (6)

_where the specific form of \(_{}\) are presented in Lem. 2.6,Lem. 2.7 and Lem. 2.8._

We now provide the specific forms of \(_{}\). Since the variables being tested for independence can be both discretized, only one being discretized, or neither being discretized. This results in different forms of \(_{}\) consequently differs across these scenarios. Let \(Z_{j_{1}}\) and \(Z_{j_{2}}\) be any two random variables indexed by \(j_{1}\) and \(j_{2}\). Let \(_{j_{1},j_{2}}^{i}=z_{j_{1}}^{i} z_{j_{2}}^{i}-_{n} Z_{j_{1}}_{n}Z_{j_{2}}\) denote the sample covariance based on a \(i\)-th pairwise observation of the variables \(Z_{j_{1}}\) and \(Z_{j_{2}}\). Let \(_{j_{1}}^{i}=_{\{z_{j_{1}}^{i}_{n}Z_{j_{1}}\}}\) and \(_{j_{2}}^{i}=_{\{Z_{j_{2}}^{i}_{n}Z_{j_{2}}\}}\), each calculated based on \(i\)-th observations of the variables \(Z_{j_{1}}\) and \(Z_{j_{2}}\), respectively. Let \(_{j_{1},j_{2}}^{i}\) be \(_{j_{1}}^{i}_{j_{2}}^{i}\). We further denote \(()=1-()\). The different forms of \(_{}\) that arise in different cases are defined as follows:

**Lemma 2.6**.: _(\(_{}\) for A Continuous-Variable Pair). For two continuous variables \(X_{j_{1}}\) and \(X_{j_{2}}\),_

\[_{}:=_{j_{1},j_{2}}^{i}-_{j_{1},j_{2}}.\] (7)

**Lemma 2.7** (\(_{}\) for A Discretized-Variable Pair).: _For discretized variables \(_{j_{1}}\) and \(_{j_{2}}\),_

\[_{}:=_{j_{1},j_{2}}^{i}-T(_{j_{1},j_{2}};\{_{j_{1}},_{j_{2}}\})\\ _{j_{1}}^{i}-(_{j_{1}})\\ _{j_{2}}^{i}-(_{j_{2}}).\] (8)

**Lemma 2.8** (\(_{}\) for A Continuous-Discretized-Variable Pair).: _For one discretized variable \(_{j_{2}}\) and one continuous variable \(X_{j_{1}}\),_

\[_{}:=_{j_{1},j_{2}}^{i}-T(_{j_{1},j_{2}};\{0,_{j_{2}}\})\\ _{j_{1}}^{i}-(_{j_{2}}).\] (9)

Derivation of forms of \(_{}\) for different cases and their corresponding distribution defined in Eq (6) can be found in App. A.4, App. A.5, App. A.6. Up to this point, our discussion has been confined to the case of covariance \(_{j_{1},j_{2}}\), the indicator of unconditional independence. In the next section, we will present the results of our CI test.

### Conditional Independence (CI) Test

To construct a CI test of our model, we are interested at two things: calculation of the estimated precision coefficient \(_{j,k}\) and the derivation of the corresponding distribution \(_{j,k}-_{j,k}\). In the following, we first build \(_{j,k}\), which is obtained using nodewise regression and show it serves as a surrogate of testing for \(_{j,k}=0\), we then construct the formulation of \(_{j,k}-_{j,k}\) as the combination of formulation of \(_{j_{1},j_{2}}-_{j_{1},j_{2}}\) and show it will also be asymptotically normal.

#### Nodewise Regression for CI

To utilize covariance for testing CI, it is necessary to establish a relationship between the estimated covariance and a metric capable of reflecting CI. To achieve this, we employ the nodewise regression which effectively builds the connection between covariance and precision matrix. Suppose we can access observations \(\{^{1},^{2},,^{n}\}\) from latent continuous variables \(=(X_{1},,X_{p}) N(0,)\), nodewise regression will do regression on every dimension with all other dimensions as predictors.

\[x_{j_{1}}^{i}=_{j_{1} j_{2}}x_{j_{2}}^{i}_{j}+_{j_{1}}^{i}.\] (10)

It can be shown that there are deterministic relationships between the regression coefficients and the covariance and precision matrices of \(\), as illustrated below and proved in App. A.7.1.

\[_{j}=_{-j-j}^{-1}_{-jj}^{p-1}, _{j,k}=-}{_{j,j}}, j k,\] (11)

where \(_{-j-j}\) is the submatrix of \(\) without \(j\)th column and \(j\)th row, and the \(_{-j}\) is the vector of \(j\)th column without \(j\)th row. \(_{j,k}\) is the surrogate of \(_{j,k}\) to capture the independence relationship of \(X_{j}\) with \(X_{k}\) conditioning on other variables. We can use Def. 2.2, Def. 2.3 and Def. 2.4 to get the estimation \(}_{-j-j}\) and \(}_{-jj}\) and thus get the estimation \(_{j}\).

Statistical Inference for \(_{j,k}\)Nodewise regression offers a robust solution for the estimation problem. A pertinent inquiry pertains to the construction of the distribution of \(_{j}-_{j}\). It is crucial to recognize that the distribution of \(_{j_{1},j_{2}}-_{j_{1},j_{2}}\) is already established. Therefore, if we can conceptualize \(_{j}-_{j}\) as a linear combination of \(_{j_{1},j_{2}}-_{j_{1},j_{2}}\), the problem is directly solved, i.e., the \(_{j}-_{j}\) is linear combination of dependent Gaussian variables. The underlying relationship between these variables is as follows:

\[_{j}-_{j}=-}_{-j-j}^{-1}((}_{-j-j}-_{-j-j})_{j}-(}_{-jj}-_{-jj})).\]

The derivation is provided in App. A.7.2. For ease of notation, we further express the distribution of the difference between the estimated covariance and the true covariance as

\[_{j_{1},j_{2}}-_{j_{1},j_{2}}=_{i=1}^{n}_ {j_{1},j_{2}}^{i}.\] (12)

The specific form of \(_{j_{1},j_{2}}^{i}\) is given in App. A.4, A.5, A.6 respectively for different cases. For notational convenience, we express \(}_{-j-j}-_{-j-j}=_{i=1}^{n}_{-j,- j}^{i}\) and \(}_{-jj}-_{-jj}=_{i=1}^{n}_{-j,j}^{i}\), where \(_{j_{1},j_{2}}\) is the element of the matrix \(\) at the position indexed by \((j_{1},j_{2})\). We now propose the statistic and its asymptotic distribution for the CI test in the following theorem.

**Theorem 2.9** (Conditional Independence test).: _In our settings, under the null hypothesis that \(X_{j}\) and \(X_{k}\) are conditional statistically independent given a set of variables \(\), i.e., \(_{j,k}=0\), the statistic_

\[_{j,k}=(}_{-j-j}^{-1}}_{-jj})_{|k|},\] (13)

_where \([k]\) denotes the element corresponding to the variable \(X_{k}\) in \(}_{-j-j}^{-1}}_{-jj}\). The statistic \(_{j,k}\) has the asymptotic distribution:_

\[_{j,k} N(0,{a^{[k]}}^{T}}_{i=1}^{n}vec(B_{-j }^{i})vec(B_{-j}^{i})^{T})a^{[k]}),\]

\[B^{i}=_{-j,-j}^{i}\\ _{-j,j}^{i}, a_{l}^{[k]}=(}_{-j-j}^{-1})_{[k],l},&l\{1,,p-1\}\\ _{q=1}^{n}(}_{-j-j}^{-1})_{[k],l}(_{j})_{q},&l\{p,,p^{2}-p\}\]

_and \(_{j}\) is \(_{j}\) whose \(_{j,k}=0\)._

In practice, we can plug in the estimation of regression parameter \(_{j}\) and set \(_{j,k}=0\) as the substitution of \(_{j}\) to calculate the variance and do the CI test. Specifically, we can obtain the \(_{j,k}\) using Eq. (13) where the estimated covariance terms can be calculated by solving the bridge equation Eq. 2. Under the null hypothesis that \(_{j,k}=0\) (conditional independence), we can take the calculated \(_{j,k}\) into the distribution defined in Thm. 2.9 and obtain the p-value. If the p-value is smaller than the predefined significance level \(\) (normally set at 0.05), we will infer the tested pairs are conditionally dependent; otherwise, we do not. The detailed derivation of the Thm. 2.9 can be found in App. A.7.2.

## 3 Experiments

We applied the proposed method DCT to synthetic data to evaluate its practical performance and compare it with Fisher-Z test  (for all three data types) and Chi-Square test  (for discrete data only) as baselines. Specifically, we investigated its Type I and Type II error and its application in causal discovery. The experiments investigating its robustness, performance in denser graphs and effectiveness in a real-world dataset can be found in App. C.

### On the Effect of the Cardinality of Conditioning Set and the Sample Size

Our experiment investigates the variations in Type I and Type II error (1 minus power) probabilities under two conditions. In the first scenario, we focus on the effects of modifying the sample size, denoted as \(n=(100,500,1000,2000)\), while conditioning on a single variable. In the second, the sample size is held constant at 2000, and we vary the cardinality of the conditioning set, representedas \(D=(1,2,,5)\). It is assumed that every variable within this conditioning set is effective, i.e., they influence the CI of the tested pairs. We repeat each test \(1500\) times.

We use \(Y,W\) to denote the variables being tested and use \(Z\) to denote the variables being conditioned on. The discretized versions of the variables are denoted with a tilde symbol (e.g., \(\)). For both conditions, we evaluate three distinct types of observations of tested variables: continuous observations for both variables (\(Y,W\)), discrete observations for both variables (\(,\)) and a mixed type (\(,W\)). The variables in the conditioning set will always be discretized observations (\(\)).

To see how well the derived asymptotic null distribution approximates the true one, we verify if the probability of Type I error aligns with the significance level \(\) preset in advance. We generate true continuous multivariate Gaussian data \(Y,W\) from \(Z_{i}\) (single \(i=1\) for the first scenario, and summed over \(n\) for the second), structured as \(a_{i}Z_{i}+E\) and \(_{i=1}^{n}a_{i}Z_{i}+E\), where \(a_{i}\) is sampled from \(U(0.5,1.5)\) and \(E\) follows a standard normal distribution, independent of all other variables. This ensures \(Y\!\!\! W|Z\). The data are then discretized into \(K=(2,4,8,12)\) levels, with boundaries randomly set based on the variable range. The first column in Fig. 2 (a) (b) shows the resulting probability of Type I errors at the significance level \(=0.05\) compared with other methods.

A good test should have as small a probability of Type II error as possible, i.e., a larger power. To test the power of our DCT, we generate the continuous multivariate Gaussian data \(_{i}\) from \(Y,W\); constructed as \(Z_{i}=a_{i}Y+b_{i}W+E\), where \(a_{i},b_{i}\) are sampled from \(U(0.5,1.5)\) and \(E\) follows a standard normal distribution independent with all others, i.e., \(Y\!\!\! W|Z\). The same discretization approach is applied here. The second column in Fig. 2 (a) (b) shows the Type II error with the changing number of samples and cardinality of the conditioning set compared with other methods.

From Fig. 2 (a), we note that the Type I error rates with our derived null distribution are well-approximated at 0.05 across all three data types in both scenarios. In contrast, other testing methods show significantly higher Type I error rates, increasing with the number of samples and the size of the conditioning set. This indicates that such methods are more prone to erroneously concluding that tested variables are conditionally dependent. Additionally, while alternative tests demonstrate considerable power with smaller sample sizes, our approach requires a sample size of 2000 to achieve satisfactory power, particularly in mixed and continuous cases. A possible explanation for this phenomenon is that our method binarizes discretized data, which may not effectively utilize all observations. This aspect warrants further investigation in future research. Moreover, our test shows remarkable stability in response to changes in the number of conditioning sets.

Figure 2: Comparison of results of Type I and Type II error (\(1\) – power) for all three types of tested data (continuous, mixed, discrete) and different number of samples and cardinality of conditioning set. The suffix attached to a test’s name denotes the cardinality of discretization; for example, “Fisher_4” signifies the application of the Fisher-z test to data discretized into four levels. Chi-square test is only applicable for the discrete case.

### Application in Causal Discovery

Causal discovery aims to recover the true causal structure from the data. Constraint-based causal discovery methods like the PC algorithm  rely on testing CI from observations to discover causal graphs. However, in the presence of discretization, failures in testing CI leads to false conclusions about causal graphs. To evaluate the efficacy of the DCT, we construct causal graphs utilizing the Bipartite Pairing (BP) model as detailed in , with the number of edges being one fewer than the number of nodes. The detailed generation process is provided in App. B due to limited space. Our experiment is divided into two scenarios: (a) fixed data samples \(n=5000\), with changing number of nodes \(p=(4,6,8,10)\); (b) fixed number of nodes \(p=8\) and changing sample sizes \(n=(500,1000,5000,10000)\).

Comparative analysis is conducted using the PC algorithm integrated with different testing methods. Specifically, we compare DCT against the Fisher-Z test applied to discretized data, the chi-square test, and the Fisher-Z test on original continuous data, the latter serving as a theoretical upper bound for comparison. Since the PC algorithm can only return a completed partially directed acyclic graph (CPDAG), we use the same orientation rules  implemented by Causal-DAG  to convert a CPDAG into a DAG. We evaluate both the undirected skeleton and the directed graph using criteria such as structural Hamming distance (SHD), F1 score, precision, and recall. For each setting, we run 10 graph instances with different seeds and report the mean and standard deviation of skeleton discovery in Fig. 3, and DAG in Fig. 4 in App B.

According to the result, DCT exhibits performance nearly on par with the theoretical upper bound across metrics such as F1 score, precision, and Structural Hamming Distance (SHD) when the number of variables (\(p\)) is small and the sample size (\(n\)) is large. Despite a decline in performance as the number of variables increases with a smaller sample size, DCT significantly outperforms both the Fisher-Z test and the Chi-square test. Notably, in almost all settings, the recall of DCT is lower than that of the baseline tests, which is a reasonable outcome _since these tests tend to infer conditional dependencies, thereby retaining all edges given the discretized observations._ For instance, a fully connected graph, would achieve a recall of 1.

## 4 Conclusion

In this paper, we present a new testing method tailored for scenarios commonly encountered in real-world applications, where variables, though inherently continuous, are only observable in their discretized forms. Our method distinguishes itself from existing CI tests by effectively mitigating the misjudgment introduced by discretization and accurately recovering the CI relationships of latent continuous variables. We substantiate our approach with theoretical results and empirical validation, underscoring the effectiveness of our testing methods.

Figure 3: Experiment result of skeleton discovery on synthetic data for changing sample size (a) and changing number of nodes (b). Fisherz_nods is the Fisher-z test applied to original continuous data. We evaluate \(F_{1}\) (\(\)), Precision (\(\)), Recall (\(\)) and SHD (\(\)).