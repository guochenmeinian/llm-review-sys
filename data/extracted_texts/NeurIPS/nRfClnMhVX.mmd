# Interpretability at Scale:

Identifying Causal Mechanisms in Alpaca

 Zhengxuan Wu, Atticus Geiger,

Thomas Icard, Christopher Potts, and Noah D. Goodman

###### Abstract

Obtaining human-interpretable explanations of large, general-purpose language models is an urgent goal for AI safety. However, it is just as important that our interpretability methods are faithful to the causal dynamics underlying model behavior and able to robustly generalize to unseen inputs. Distributed Alignment Search (DAS)  is a powerful gradient descent method grounded in a theory of causal abstraction that has uncovered perfect alignments between interpretable symbolic algorithms and small deep learning models fine-tuned for specific tasks. In the present paper, we scale DAS significantly by replacing the remaining brute-force search steps with learned parameters - an approach we call Boundless DAS. This enables us to efficiently search for interpretable causal structure in large language models while they follow instructions. We apply Boundless DAS to the Alpaca model (7B parameters), which, off the shelf, solves a simple numerical reasoning problem. With Boundless DAS, we discover that Alpaca does this by implementing a causal model with two interpretable boolean variables. Furthermore, we find that the alignment of neural representations with these variables is robust to changes in inputs and instructions. These findings mark a first step toward faithfully understanding the inner-workings of our ever-growing and most widely deployed language models. Our tool is extensible to larger LLMs and is released publicly at https://github.com/frankaging/align-transformers.

## 1 Introduction

Present-day large language models (LLMs) display remarkable behaviors: they appear to solve coding tasks, translate between languages, engage in open-ended dialogue, and much more. As a result, their societal impact is rapidly growing, as they make their way into products, services, and people's own daily tasks. In this context, it is vital that we move beyond behavioral evaluation to deeply explain, in human-interpretable terms, the internal processes of these models, as an initial step in auditing them for safety, trustworthiness, and pernicious social biases.

The theory of causal abstraction  provides a generic framework for representing interpretability methods that faithfully assess the degree to which a complex causal system (e.g., a neural network) implements an interpretable causal system (e.g., a symbolic algorithm). Where the answer is positive, we move closer to having guarantees about how the model will behave. However, thus far, such interpretability methods have been applied only to small models fine-tuned for specific tasks  and this is arguably not an accident: the space of alignments between the variables in the hypothesized causal model and the representations in the neural network becomes exponentiallylarger as models increase in size. When a good alignment is found, one has specific formal guarantees. Where no alignment is found, it could easily be a failure of the alignment search algorithm.

Distributed Alignment Search (DAS)  marks real progress on this problem. DAS opens the door to (1) discovering structure spread across neurons and (2) using gradient descent to learn an alignment between distributed neural representations and causal variables. However, DAS still requires a brute-force search over the dimensionality of neural representations, hindering its use at scale.

In this paper, we introduce Boundless DAS, which replaces the remaining brute-force aspect of DAS with learned parameters, truly enabling interpretability at scale. We use Boundless DAS to study how Alpaca (7B) , an off-the-shelf instruct-tuned LLaMA model, follows basic instructions in a simple numerical reasoning task. Figure 1 summarizes the approach. We find that Alpaca achieves near-perfect task performance _because_ it implements a simple algorithm with interpretable variables. In further experiments, we show that Alpaca uses this simple algorithm across a wide range of contexts and variations on the task. These findings mark a first step toward faithfully understanding the inner-workings of our largest and most widely deployed language models.

## 2 Related Work

**Interpretability** Many methods have been developed in an attempt to explain and understand deep learning models. These methods include analyzing learned weights [12; 1; 13], gradient-based methods [45; 44; 60; 46; 52; 6], probing [14; 48; 24; 42; 12; 32; 8; 40], syntax-driven interventions , self-generated model explanations , and training external explainers based on model behaviors [39; 31]. However, these methods rely on observational measurements of behavior and internal neural representations. Such explanations are often not guaranteed to be faithful to the underlying causal mechanisms of the target models [29; 20; 50; 53].

**Causal Abstraction** The theory of _causal abstraction_[41; 4; 5] offers a unifying mathematical framework for interpretability methods aiming to uncover interpretable causal mechanisms in deep learning models [19; 28; 20; 58; 54; 23] and training methods for inducing such interpretable mechanisms [21; 59; 58; 25]. Causal abstraction  can represent many existing interpretablity methods, including interative nullspace projection [38; 15; 30], causal mediation analysis [51; 33], and causal effect estimation [18; 16; 2]. In particular, causal abstraction grounds the research program of _mechanistic interpretability_, which aims to reverse engineer deep learning models by determining the algorithm or computation underlying their intelligent behavior [35; 17; 36; 7; 54]. To the best of our knowledge, there is no prior work that scales these methods to large, general-purpose LLMs.

**Training LLMs to Follow Instructions** Instruction-based fine-tuning of LLMs can greatly enhance their capacity to follow natural language instructions [10; 37]. In parallel, this ability can also be induced into the model by fine-tuning base models with hundreds of specific tasks [56; 11]. Recently,

Figure 1: Our pipeline for scaling causal explainability to LLMs with billions of parameters.

Wang et al.  show that the process of creating fine-tuning data for instruction following models can partly be done by the target LLM itself ("self-instruct"). Such datasets have led to many recent successes in lightweight fine-tuning of ChatGPT-like instruction following models such as Alpaca , instruct-tuned LLaMA , StableLM , Vicuna . Our goal is to scale methods from causal abstraction to understand _how_ these models follow a particular instruction.

## 3 Methods

### Background on Causal Abstraction

**Causal Models** We represent black box networks and interpretable algorithms using causal models that consist of _variables_ taking on _values_ according to _causal mechanisms_. We distinguish a set of _input variables_ with possible values \(\). An _intervention_ is an operation that edits some causal mechanisms in a model. We denote the output of a causal model \(\) provided an input \(\) as \(()\).

**Interchange Intervention** Begin with a model \(\) (e.g., causal model or neural model) and a _base_ input \(\) and _source_ inputs \(\{_{j}\}_{1}^{k}\), all elements of \(\), together with disjoint sets of target variables \(\{_{j}\}_{1}^{k}\) we are aligning. An _interchange intervention_ yields a new model \((,\{_{j}\}_{1}^{k},\{_{j}\}_{1 }^{k})\) which is identical to \(\) except the values for each set of target variables \(_{j}\) are fixed to be the value they would have taken for source input \(_{j}\). We then denote the intervened output (i.e., counterfactual output) for the base input with \((,\{_{j}\}_{1}^{k},\{_{j}\}_{1 }^{k})()\).

**Distributed Interchange Intervention** Let \(\) be a subset of variables in \(\), the _target variables_. Let \(\) be a vector space with orthogonal subspaces \(\{_{j}\}_{1}^{k}\). Let \(\) be an invertible function \(:\). A _distributed interchange intervention_ yields a new model \((,,\{_{j}\}_{1}^{k},\{_{j }\}_{1}^{k})\) which is identical to \(\) except the causal mechanisms have been rewritten such that each subspace \(_{j}\) is fixed to be the value it would take for source input \(_{j}\).2 Specifically, the causal mechanism for \(\) is,

\[F_{}^{*}()=^{-1}_{_{0}}F_{}() +_{j=1}^{k}_{_{j}} F_{}(_{j})\] (1)

where \(\) is the _base_ input, \(_{0}=_{j=1}^{k}_{j}\), and \(_{_{0}}\) or \(_{_{j}}\) represents the orthogonal projection operator of the original rotated vector into the \(_{0}\) or \(_{j}\) subspace. \(F_{}()\) represents the causal mechanisms of causal variables \(\) before any intervention, whereas \(F_{}^{*}()\) represents the mechanisms after the distributed intervention. We then denote the intervened output (i.e., counterfactual output) for the base input as \((,,\{_{j}\}_{1}^{k},\{_{ j}\}_{0}^{k})()\).

**Causal Abstraction and Alignment** We are licensed to claim a causal model (i.e., an algorithm) \(\) is a faithful interpretation of the neural network \(\) if the causal mechanisms of the variables in the algorithm _abstract_ the causal mechanisms of neural representations relative to a particular _alignment_. For our purposes, each variable of a high-level model is aligned with a linear subspace in the vector space formed by rotating a neural representation with an orthogonal matrix. We use \(\) to represent the alignment mapping between a high-level causal variable and a neural representation.

**Approximate Causal Abstraction** Interchange intervention accuracy (IIA) is a graded measure of abstraction that computes the proportion of aligned interchange interventions on the algorithm and neural network that have the same output. The IIA for an alignment of high-level variables \(Z_{j}\) to orthogonal subspace \(_{j}\) between an algorithm \(\) and neural network \(\) is

\[|^{k+1}}_{,_{1 },,_{k}}-10.0pt (,,\{_{j}\}_{1}^{k},\{_{ j}\}_{1}^{k})()=\\ ,\{(_{j})\}_{1}^{k},\{ _{j}\}_{1}^{k})()\] (2)

where \(\) translates from low-level causal variable values to high-level neural representation values.

[MISSING_PAGE_FAIL:4]

instruction \(t_{i}\) (e.g., "correct the spelling of the word:") followed by a test query input \(x_{i}\) (e.g., "aplpe"). We use \((t_{i},x_{i})\) to depict the model generation \(y_{p}\) given the instruction and the test query input. We can evaluate model performance by comparing \(y_{p}\) with the gold label \(y\). We focus on tasks with high model performance, to ensure that we have a known behavioral pattern to explain.

The instruction prompt of the Price Tagging game follows the publicly released template of the Alpaca (7B) model. The core instruction contains an English sentence:

_Please say yes only if it costs between_ [X.XX] _and_ [X.XX] _dollars, otherwise no._

followed by an input dollar amount [X.XX], where [X.XX] are random continuous real numbers drawn with a uniform distribution from [0.00, 9.99]. The output is a single token 'Yes' or 'No'.

For instance, if the core instruction says _Please say yes only if it costs between_ [1.30] _and_ [8.55] _dollars, otherwise no._, the answer would be "Yes" if the input amount is "3.50 dollars" and "No" if the input is "9.50 dollars". We restrict the absolute difference between the lower bound and the upper bound to be [2.50, 7.50] due to model errors outside these values - again we need behavior to explain.

### Hypothesized Causal Models

As shown in Figure 2, we have identified a set of human-interpretable high-level causal models, with alignable intermediate causal variables, that would solve this task with 100% performance:

Figure 3: Aligned distributed interchange interventions performed on the Alpaca model that is instructed to solve our Price Tagging game. To train Boundless DAS, we sample two training examples and then swap the intermediate boolean values between them to produce a counterfactual output using our causal model. In parallel, we swap the aligned dimensions of the neural representations in rotated space. Lastly, we update our rotation matrix such that our neural network has a more similar counterfactual behavior to the causal model.

Figure 2: Four proposed high-level causal models for how Alpaca solves the price tagging task. Intermediate variables are in red. All these models perfectly solve the task.

* **Left Boundary**: This model has one high-level boolean variable representing whether the input amount is higher than the lower bound, and an output node incorporating whether the input amount is also lower than the high bound.
* **Left and Right Boundary**: The previous model is sub-optimal in only abstracting one of the boundaries. In this model, we have two high-level boolean variables representing whether the input amount is higher than the lower bound and lower than the higher bound, respectively. We take a conjunction of these boolean variables to predict the output.
* **Mid-point Distance**: We calculate the mid-point of the lower and upper bounds (e.g., the mid-point of "3.50" and "8.50" is "6.00), and then we take the absolute distance between the input dollar amount and the mid-point as \(a\). We then calculate one-half of the bounding bracket length (e.g., the bracket length for "3.50" and "8.50" is "5.00") as \(b\). We predict output "Yes" if \(a b\), otherwise "No". We align only with the mid-point variable.
* **Bracket Identity**: This model represents the lower and upper bound in a single interval variable and passes this information to the output node. We predict the output as "Yes" if the input amount within the interval, otherwise "No".

Model ArchitectureOur target model is the Alpaca (7B) , an off-the-shelf instruct-tuned LLaMA model. It is a Transformer-based decoder-only autoregressive trained language model with 32 layers and 32 attention heads. It has a hidden dimension in size of 4096, which is also the dimension of our rotation matrix which is applied for each token representation. In total, the rotation matrix contains 16.8M parameters and has size 4096 \(\) 4096.

Alignment ProcessWe train Boundless DAS with test query token representations (i.e., starting from the token of the first input digit till the last token in the prompt) in a selected set of 7 layers: \(\{0,5,10,15,20,25,30\}\). We also train Boundless DAS on the token before the first digit as a control condition where nothing should be expected to be aligned. We interchange with a single source, while allowing multiple causal variables to be aligned across examples. For simplicity, we enforce the interval between two boundary indices to be the same (i.e., the same size of linear subspace for each aligning causal variable). We run each experiment with three distinct random seeds. Since the global optimum of the Boundless DAS objective corresponds to the best attainable alignment, but SGD may be trapped by local optima, we report the best-performing seed in terms of IIA. Figure 3 provides an overview of these analyses.

Evaluation MetricTo evaluate models, we use Interchange Intervention Accuracy (IIA) as defined in Eqn. 2. IIA is bounded between 0.0 and 1.0. We measure the baseline IIA by replacing the learned rotation matrix with a random one. The lower bound of IIA for "Left Boundary" and "Left and Right Boundary" is about 0.50, and for "Mid-point Distance" or "Bracket Identity" it is about 0.60. The latter two baseline IIAs are higher than chance because they are conditioned on the distribution of our output labels (i.e., how many times the original label gets to be flipped due to the intervention). See Section 4.7 for more discussion of metric calibration. Additionally, IIA can occasionally go above the model's task performance, when the interchange interventions put the model in a better state, but IIA is constrained by task accuracy for the most part.

### Boundless DAS Results

Figure 4 shows our main results, given in terms of IIA across our four hypothesized causal models (Figure 2). The results show very clearly that 'Left Boundary' and 'Left Right Boundary' (top panels) are highly accurate hypotheses about how Alpaca solves the task. For them, IIA is at or above task performance (0.85), and intermediate variable representations are localized in systematically arranged positions. By contrast, 'Mid-point Distance' and 'Bracket Identity' (bottom panels) are inaccurate hypotheses about Alpaca's processing, with IIA peaking at around 0.72.

These findings suggest that, when solving our reasoning task, Alpaca internally follows our first two high-level models by representing causal variables that align with boundary checks for both left and right boundaries. Interestingly, heatmaps on the top two rows also show a pattern of higher scores around the bottom left and upper right and close to zero scores in other positions. This is significantly different from the other two alignments where, although some positions are highlighted (e.g., the representations for the last token), all the positions receive non-zero scores. In short, the accurate hypotheses correspond to highly structured IIA patterns, and the inaccurate ones do not.

Additionally, alignments are better when the model post-processes the query input with 1-2 additional steps: accuracy is higher at position 75 compared to all previous positions. Surprisingly, there exist _bridging tokens_ (positions 76-79) where accuracy suddenly drops compared to earlier tokens, which suggests that these representations have weaker causal effects on the model output. In other words, boundary-check variables are fully extracted around position 75, level 10, and are later copied into activations for the final token before responding. By comparing the heatmaps on the top row, our findings suggest that aligning multiple variables at the same time poses a harder alignment process, in that it lowers scores slightly across multiple positions.

### Interchange Interventions with (In-)Correct Inputs

IIA is highly constrained by task performance, and thus we expect it to be much lower for inputs that the model gets wrong. To verify this, we constructed an evaluation set containing 1K examples that the model gets wrong and evaluated our 'Left and Right Boundary' hypothesis on this subset. Table 1 reports these results in terms of max IIA and the correlation of the IIA values with those obtained in our main experiments. As expected, IIA drops significantly. However, two things stand out: IIA is far above task performance (which is 0.0 by design now), and the correlation with the original IIA map remains high. These findings suggest that the model is using the same internal mechanisms to process these cases, and so it seems possible the model is narrowly missing the correct output predictions.

We also expect IIA to be higher if we focus only on cases that the model gets correct. Table 1 confirms this expectation using 'Left and Right Boundary' as representative. IIA\({}_{}\) is improved from 0.86 to 0.88, and the correlation with the main results is essentially perfect. Full heatmaps for these analyses are given in Appendix A.4.

Figure 4: Interchange Intervention Accuracy (IIA) for four different alignment proposals. The Alpaca model achieves 85% task accuracy. The higher the number is, the more faithful the alignment is. We color each cell by scaling IIA using the model’s task performance as the upper bound and a dummy classifier (predicting the most frequent label) as the lower bound. These results indicate that the top two are highly accurate hypotheses about how Alpaca solves the task, whereas the bottom two are inaccurate in this sense. Analyzing tokens includes special tokens (e.g., ‘<0x0A>’ for linebreaks) required by Alpaca’s instruct-tuning template as provided in Appendix A.1. Heatmap color uses min-max standardized IIA using the random rotation baseline as the lower bound, and task performance as the upper bound.

### Do Alignments Robustly Generalize to Unseen Instructions and Inputs?

One might worry that our positive results are highly dependent on the specific input-output pairs we have chosen. We now seek to address this concern by asking whether the causal roles (i.e., alignments) found using Boundless DAS in one setting are preserved in new settings. This is crucial, as it tells how robustly the causal model is realized in the neural network.

**Generalizing Across Two Different Instructions** Here, we assess whether the learned alignments for the 'Left Boundary' causal model transfer between different specific price brackets in the instruction. To do this, we retrain Boundless DAS for the high-level model with a fixed instruction that says "between 5.49 dollars and 8.49 dollars". Then, we fix the learned rotation matrix and evaluate with another instruction that says "between 2.51 dollars and 5.51 dollars". For both, Alpaca is successful at the task, with around 94% accuracy. Our hypothesis is that if the found alignment of the high-level variable is robust, it should transfer between these two settings, as the aligning variable is a boolean-type variable which is potentially agnostic to the specific comparison price.

Table 1 gives our findings in the 'New Bracket' rows. Boundless DAS is able to find a good alignment for the training bracket with an \(_{}\) that is about the same as the task performance at 94%. For our unseen bracket, the alignments also hold up extremely well, with no drop in \(_{}\). For both cases, the found alignments also highly correlate with the counterpart of our main experiment.

**Generalizing with Inserted Context** Recent work has shown that language models are sensitive to irrelevant context , which suggests that found alignments may overfit to a set of fixed contexts.

  
**Experiment** & **Task Acc.** & \(_{}\) & **Correlation** \\  Left Boundary (\(\)) & 0.85 & 0.90 & 1.00 \\ Left and Right Boundary (\(\)) & 0.85 & 0.86 & 1.00 \\ Mid-point Distance & 0.85 & 0.70 & 1.00 \\ Bracket Identity & 0.85 & 0.72 & 1.00 \\  Correct Only & 1.00\({}^{}\) & 0.88 & 0.99 (\(\)) \\ Incorrect Only & 0.00\({}^{}\) & 0.71 & 0.84 (\(\)) \\  New Bracket (Seen) & 0.94 & 0.94 & 0.97 (\(\)) \\ New Bracket (Unseen) & 0.95 & 0.95 & 0.94 (\(\)) \\ Irrelevant Contexts & 0.84 & 0.83 & 0.99 (\(\)) \\ Sibling Instructions & 0.84 & 0.83 & 0.87 (\(\)) \\ + _exclude_ top right & 0.84 & 0.83 & 0.92 (\(\)) \\   

Table 1: Summary results for all experiments with task performance as accuracy (range \(\)), maximal interchange intervention accuracy (IIA) (range \(\)) across all positions and layers, Pearson correlations of IIA between two distributions (compared to \(\) or \(\); range \([-1,1]\)). \({}^{}\)This is empirical task performance on the evaluation dataset for this experiment.

Figure 5: Interchange Intervention Accuracy (IIA) evaluated with different output formats. The seen setting is for training, and the unseen setting is for evaluation.

To address this concern, we add prefix strings to the input instructions and evaluate how 'Left and Right Boundary' alignment transfers. We thus generate 20 random prefixes using GPT-4.3 All of our prefixes, and our method for generating them, can be found in Appendix A.6. The model achieves 84% task performance with random contexts added as prefixes, which is slightly lower than the average task performance. Nonetheless, the results in Table 1 suggest the found alignments transfer surprisingly well here, with only 2% drop in \(_{}\). Meanwhile, the mean IIA distribution across positions and layers is highly correlated with our base experiment. Thus, our method seems to identify causal structure that is robust to changes in irrelevant task details _and_ position in the input string.

Generalizing to Modified OutputsWe further test whether the alignments found for instructions with a template "Say yes..., otherwise no" can generalize to a new instruction with a template "Say True..., otherwise False". If there are indeed latent representations of our aligning causal variables, the found alignments should persist and should be agnostic to the output format.

Table 1 shows that this holds: the learned alignments actually transfer across these two sibling instructions, with a minimum 1% drop in \(_{}\). In addition, the correlation increases 6% when excluding the top right corner (top 3 layers in the last position), as seen in the final row of the table. This result is further substantiated in Figure 5, where we see that IIA drops near the top right. This indicates a late fusion of output classes and working representations, leading to different computations for the new instruction only close to the output.

### Boundary Learning Dynamics

Boundless DAS automatically learns the intervention site boundaries (Section 3.2). It is important to confirm that we do not give up alignment accuracy by optimizing boundaries, and to explore the dimensionality actually needed to represent the abstract variables. We sample 100 experiment runs from our experiment pool and create two groups: (1) the success group where the boundary does not shrink to 0 at the end of the training; (2) the failure group where the boundary does shrink to 0 at the end of the training. The failure group is for those instances where there is no significant causal role of representations found by our method. We plot how boundary width and in-training evaluation IIA vary throughout training. Figure 6 shows that, in the success cases, each aligned variable only needs 5-10% of the representation space. IIA performance converges quickly for success cases and, crucially, maintains stable performance despite shrinking dimensionality. This suggests that only a small fraction of the whole representation space is needed for aligning a causal variable, and that these representations can be efficiently identified by Boundless DAS.

Figure 6: Learned boundary width for intervention site and in-training evaluation interchange intervention accuracy (IIA) for two groups of data: (1) aligned group where the boundary does not shrink to 0 at the end of the training; (2) unaligned group where the boundary does shrink to 0 at the end of the training. 1 on the y-axis means either 100% accuracy for IIA, or the variable is occupying half of the hidden representation for the boundary width.

### Metric Calibration

One concern brought up by our reviewers is whether our metric, interchange intervention accuracy (IIA), is well calibrated. Here, we provide additional evidence. We report IIAs in the same way as in Figure 4 but with random rotation matrices. With a random rotation matrix, we find that the IIA score drops from 0.83 to 0.53 at layer 10, token position 75, for our "left and right boundary" causal model. Other positions drop significantly as well. These results calibrate IIAs in case of unbalanced labels (e.g., our two control causal models reach about 0.60 IIA at the same location with a random rotation matrix). This shows that Boundless DAS finds substantial structure in Alpaca as compared to what these random baselines can find. Details can be found in Figure 12 in the Appendix.

We can also help calibrate our result by comparing Boundless DAS applied to Alpaca with a randomly initialized LLAMA-7B. This can help quantify the extent to which DAS leverage random causal structure in order to overfit to its training objective. This random model results in near 0% task performance on our task, as expected. After running Boundless DAS on the first layer of this random LLaMA-7B model, the found alignments to each token's representation ranged from 0% to 69% IIA, which is comparable to a most frequent label dummy model (66%). For the representations with near chance IIA, this means that we were able to find a distributed neural representation that shifts the probability mass to "yes" or "no", but does not differentiate between the two. We also found that we could find a distributed neural representation that shifts the probability mass to the tokens "dog" and "give" instead of "yes" and "no". More importantly, we found that IIA drops to 0% for this run on all of our robustness checks from the paper. In addition, it drops to 0% if we use it on another randomly initialized LLaMA-7B model with a different random seed. Overall, it seems that the causal structure we did find has arisen by chance, and we might expect very large models to be more prone to such occurrences. Our robustness checks definitively clarify the situation.

## 5 Analytic Strengths and Limitations

Explanation methods for AI should be judged by the degree to which they can, in principle, identify models' true internal causal mechanisms. If we reach 100% IIA for a high-level model using Boundless DAS we can assert that we have positively identified a correct causal structure in the model (though perhaps not the only correct abstraction). This follows directly from the formal results of Geiger et al. . On the other hand, failure to find causal structure with Boundless DAS is not a proof that such structure is missing, for two reasons. First, Boundless DAS explores a very flexible and large hypothesis space, but it is not completely exhaustive. For instance, a set of variables represented in a highly non-linear way might be missed. Second, we are limited by the space of causal models we think to test ourselves.

For models where task accuracy is below 100%, it is still possible for IIA to reach 100% if the high-level causal model _explains errors_ of the low-level model. However, in cases like those studied here, where our high-level model matches the idealized task but our language model does not completely do so, we expect Boundless DAS to find only partial support for causal structure even if we have identified the ideal set of hypotheses and searched optimally. This too follows from the results of Geiger et al. . However, as we have seen in the results above, Boundless DAS can find structure even where the model's task performance is low, since task performance can be shaped by factors like a suboptimal generation method or the rigidity of the assessment metric. Future work must tighten this connection by modeling errors in the language model in more detail.

## 6 Conclusion

We introduce Boundless DAS, a novel and effective method for scaling causal analysis of LLMs to billions of parameters. Using Boundless DAS, we find that Alpaca, off-the-shelf, solves a simple numerical reasoning problem in a human-interpretable way. Additionally, we address one of the main concerns around interpretability tools developed for LLMs - whether found alignments generalize across different settings. We rigorously study this by evaluating found alignments under several changes to inputs and instructions. Our findings indicate robust and interpretable algorithmic structure. Our framework is generic for any LLMs and is released to the public. We hope that this marks a step forward in terms of understanding the internal causal mechanisms behind the massive LLMs that are at the center of so much work in AI.