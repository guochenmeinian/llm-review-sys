# HyTrel: Hypergraph-enhanced

Tabular Data Representation Learning

 Pei Chen\({}^{1}\)1, Soumajyoti Sarkar\({}^{2}\), Leonard Lausen\({}^{2}\),

**Balasubramaniam Srinivasan\({}^{2}\), Sheng Zha\({}^{2}\), Ruihong Huang\({}^{1}\), George Karypis\({}^{2}\)**

\({}^{1}\)Texas A&M University, \({}^{2}\)Amazon Web Services

{chenpei,huangrh}@tamu.edu,

{soumajs,lausen,srbalasu,zhasheng,gkarypis}@amazon.com

###### Abstract

Language models pretrained on large collections of tabular data have demonstrated their effectiveness in several downstream tasks. However, many of these models do not take into account the row/column permutation invariances, hierarchical structure, etc. that exist in tabular data. To alleviate these limitations, we propose HyTrel, a tabular language model, that captures the permutation invariances and three more _structural properties_ of tabular data by using hypergraphs-where the table cells make up the nodes and the cells occurring jointly together in each row, column, and the entire table are used to form three different types of hyperedges. We show that HyTrel is maximally invariant under certain conditions for tabular data, i.e., two tables obtain the same representations via HyTrel_iff_ the two tables are identical up to permutations. Our empirical results demonstrate that HyTrel**consistently** outperforms other competitive baselines on four downstream tasks with minimal pretraining, illustrating the advantages of incorporating the inductive biases associated with tabular data into the representations. Finally, our qualitative analyses showcase that HyTrel can assimilate the table structures to generate robust representations for the cells, rows, columns, and the entire table. 1

## 1 Introduction

Tabular data that is organized in bi-dimensional matrices are widespread in webpages, documents, and databases. Understanding tables can benefit many tasks such as table type classification, table similarity matching, and knowledge extraction from tables (e.g., column annotations) among others. Inspired by the success of pretrained language models in natural language tasks, recent studies (Yin et al., 2020; Yang et al., 2022) proposed Tabular Language Models (TaLMs) that perform pretraining on tables via self-supervision to generate expressive representations of tables for downstream tasks.

Among the TaLMs, many works (Herzig et al., 2020; Yin et al., 2020; Deng et al., 2020; Iida et al., 2021) serialize tables to a sequence of tokens for leveraging existing pretrained language model checkpoints and textual self-supervised objectives like the Masked Language Modeling. However, due to the linearization of tables to strings, these models do not explicitly incorporate the structural properties of a table, e.g., the invariances to arbitrary permutations of rows and columns (independently). Our work focuses on obtaining representations of tables that take table structures into account. We hypothesize that incorporating such properties into the table representations will benefit many downstream table understanding tasks.

**Motivation:** Tabular data is structurally different in comparison to other data modalities such as images, audio, and plain texts. We summarize four _structural properties_ present in the tables below:

* Most tables are invariant to row/column permutations. This means, in general, if we arbitrarily (and independently) permute the rows or columns of a table, it is still an equivalent table. For other tables with an explicit ordering of rows or columns, we can make them permutation invariant by appropriately adding a ranking index as a new column or row.
* for example, they oftentimes have the same semantic types. Similarly, the cells within a single row together describe the attributes of a sample within the table and the cells cannot be treated in silos.
* The interactions within cells/rows/columns are not necessarily pairwise, i.e., the cells within the same row/column, and rows/columns from the same table can have high-order multilateral relations .
* Information in tables is generally organized in a hierarchical fashion where the information at the table-level can be aggregated from the column/row-level, and further from the cell-level.

However, the linearization-based approaches are not designed to explicitly capture most of the above properties. We aim to address the limitation by modeling all the aforementioned structural properties as inductive biases while learning the table representations.

**Our Approach:** In line with recent studies  which have elucidated upon the importance of the structure of a table, we propose the HyTrel that uses hypergraphs to model the tabular data. We propose a modeling paradigm that aims _capture all of the four properties_ directly. Figure 1 provides an example of how a hypergraph is constructed from a table. As observed, converting a table into a hypergraph allows us to incorporate the first two properties inherent to the nature of hypergraphs. Hypergraphs seamlessly allow the model to incorporate row/column permutation invariances, as well as interactions among the cells within the same column or row. Moreover, the proposed hypergraph structure can capture the high-order (not just pairwise) interactions for the cells in a column or a row, as well as from the whole table, and an aggregation of hyperedges can also help preserve the hierarchical structure of a table.

**Contributions:** Our theoretical analysis and empirical results demonstrate the advantages of modeling the four structural properties. We first show that HyTrel is maximally invariant when modeling tabular data (under certain conditions), i.e. if two tables get the same representations via the hypergraph table learning function, then the tables differ only by row/column permutation (independently) actions and vice versa. Empirically, we pretrain HyTrel on publicly available tables using two self-supervised objectives: a table content based ELECTRA2 objective  and a table structure dependent contrastive objective . The evaluation of the pretrained HyTrel model on four downstream tasks (two knowledge extraction tasks, a table type detection task, and a table similarity prediction task) shows that HyTrel can achieve state-of-the-art performance.

We also provide an extensive qualitative analysis of HyTrel-including visualizations that showcase that (a) HyTrel representations are robust to arbitrary permutations of rows and columns (independently), (b) HyTrel can incorporate the hierarchical table structure into the representations, (c)

Figure 1: An example of modeling a table as a hypergraph. Cells make up the nodes and the cells in each row, column, and the entire table form hyperedges. The table caption and the header names are used for the names of the table and column hyperedges. The hypergraph keeps the four structural properties of tables, e.g., the invariance property of the table as the row/column permutations result in the same hypergraph.

HyTrel can achieve close to state-of-the-art performance even without pretraining, and the model is extremely efficient with respect to the number epochs for pretraining in comparison to prior works, further demonstrating the advantages of HyTrel in modeling the structural properties of tabular data. In Appendix B, we provide additional analysis that demonstrates HyTrel's ability to handle input tables of arbitrary size and underscore the importance of the independent row/column permutations.

## 2 HyTrel Model

Formally, a table in our work is represented as \(=[M,H,R]\), where \(M\) is the caption, \(H=[h_{1},h_{2},h_{3},...,h_{m}]\) are the \(m\) column headers, \(R\) represents the \(n\) rows \([R_{1},R_{2},R_{3},...,R_{n}]\). Each row \(R_{i}\) has \(m\) cells \([c_{i1},c_{i2},c_{i3},...,c_{im}]\). The caption, header, and cell values can be regarded as sentences that contain several words. We note that each cell \(c_{ij}\) or header \(h_{j}\) also belongs to the corresponding column \(C_{j}\). We use \(C=[C_{1},C_{2},C_{3},...,C_{m}]\) to represent all the columns that include the headers, so a table can also be defined as \(=[M,C]\).

### Formatter & Embedding Layer

The formatter transforms a table into a hypergraph. As shown in Figure 1, given a table \(\), we construct a corresponding hypergraph \(=(,)\), where \(\), \(\) denote the set of nodes and hyperedges respectively. We treat each cell \(c_{ij}\) as a node \(v_{ij}\), and each row \(R_{i}\), each column \(C_{j}\), and the entire table \(\) as hyperedges \(e^{c}_{i},e^{j}_{j},e^{t}\) (\(1_{i n,1 j m}\)), respectively. As a part of our hypergraph construction, each cell node \(v_{ij}\) is connected to 3 hyperedges: its column hyperedge \(e^{c}_{i}\), row hyperedge \(e^{r}_{j}\), and the table hyperedge \(e^{c}\). The hypergraph can be conveniently be represented as an incidence matrix \(\{0,1\}^{mn(m+n+1)}\), where \(_{ij}=1\) when node \(i\) belong to hyperedge \(j\) and \(_{ij}=0\) otherwise.

An embedding layer is then employed over the nodes and hyperedges. Each node \(v_{ij}\) corresponds to a cell \(c_{ij}\) that has several tokens, and we obtain the feature vector \(_{v_{ij},:}^{F}\) for a given node by feeding its constituent cell tokens into the embedding layer and then averaging the embeddings over the tokens. After obtaining node embeddings \(^{nm F}\) for all nodes, a similar transformation is applied over hyperedges. For different hyperedge types, we use different table content for their initialization : for a column hyperedge \(e^{c}_{i}\), we use all the tokens from the corresponding header \(h_{i}\). For the table hyperedge \(e^{t}\), we use the the entire caption associated with \(M\). For the row hyperedge \(e^{r}_{j}\)

Figure 2: The overall framework of HyTrel. We first turn a table \(\) into a hypergraph \(\) and then initialize the embeddings of the nodes \(\) and hyperedges \(\). After that, we encode the hypergraph using stacked multiple-layer hypergraph-structure-aware transformers (HyperTrans). Each HyperTrans layer has two attention blocks that work on hypergraph (HyperAtt) and one Hyperedge Fusion block. Lastly, we use the node and hyperedge representations from the final layer for pretraining and fine-tuning.

when no semantic information is available, we randomly initialize them as \(_{e_{,}^{r}:}^{F}\). Performing the above operations yields an initialization for all the hyperedge embeddings \(^{(m+n+1) F}\).

### Hypergraph Encoder

After embedding, we propose to use a structure-aware transformer module (HyperTrans) to encode the hypergraphs. HyperTrans encoder can encode the table content, structure, and relations among the table elements (including cells, headers, captions, etc.). As shown in Figure 2, one layer of the HyperTrans module is composed of two hypergraph attention blocks (HyperAtt, \(f\))  that interact with the node and hyperedge representations, and one Hyperedge Fusion block. The first HyperAtt block is the Node2Hyperedge attention block as defined below:

\[}_{e,:}^{(t+1)}=f_{}(K_ {e,^{(t)}})\] (1)

Where \(f_{}\) is a hypergraph attention function defined from nodes to hyperedges. \(K_{e,}=\{_{v,:}\ v e\}\) denotes the sets of hidden node representations included in the hyperedge \(e\). The Node2Hyperedge block will aggregate information to hyperedge \(e\) from its constituent nodes \(v e\).

We then use a Hyperedge Fusion module (a Multilayer Perceptron Network, \(\)) to propagate the hyperedge information from the last step, as defined below:

\[_{e,:}^{(t+1)}=(_{e,:}^{(t)};}_{e,:}^{(t+1)})\] (2)

A second HyperAtt block Hyperedge2Node then aggregates information from a hyperedge to its constituent nodes as follows:

\[_{v,:}^{(t+1)}=f_{}(L_{v, ^{(t+1)}})\] (3)

Where \(f_{}\) is another hypergraph attention function defined from hyperedges to nodes. \(L_{v,}=\{_{e,:}\ v e\}\) is defined as the sets of hidden representations of hyperedges that contain the node \(v\).

As for the HyperAtt block \(f\), similar to transformer , it is composed of one multi-head attention, one Position-wise Feed-Forward Network (FFN), two-layer normalization (LN)  and two skip connections , as in Figure 2. However, we do not use the self-attention  mechanism from the transformer model because it is not designed to keep the invariance structure of tables or hypergraphs. Inspired by the deep set models , we use a set attention mechanism that can keep the permutation invariance of a table. We define HyperAtt\(f\) as follows:

\[f_{}( ):=(+())\] (4)

Where \(\) is the input node or hyperedge representations. The intermediate representations \(\) is obtained by:

\[=(+(,,))\] (5)

Where \(\) is the multi-head set attention mechanism defined as:

\[(,,)=\|_{i=1}^{h}_{i}\] (6)

and

\[_{i}=(_{i}(_{i}^{ K})^{T})(_{i}^{V})\] (7)

Where \(\) is a learnable weight vector as the query and \(:=\|_{i=1}^{h}_{i}\), \(_{i}^{K}\) and \(_{i}^{V}\) are the weights for the key and value projections, \(\|\) means concatenation.

So the HyperTrans module will update node and hyperedge representations alternatively. This mechanism enforces the table cells to interact with the columns, rows, and the table itself. Similar to BERT\({}_{base}\) and TaBERT\({}_{base}\), we stack \(12\) layers of HyperTrans.

### Invariances of the HyTrel Model

Let \(:^{d}\) be our target function which captures the desired row/column permutation invariances of tables (say for tables of size \(n m\)). Rather than working on the table \(\) directly, the proposed HyTrel model works on a hypergraph (via Eqns (1-5)) that has an incidence matrix \(\) of size \(mn(m+n+1)\). Correspondingly, we shall refer to HyTrel as a function \(g:^{k}\).

In this section we will make the connections between the properties of the two functions \(\) and \(g\), demonstrating a maximal invariance between the two-as a result of which we prove that our HyTrel can also preserve the permutation invariances of the tables. First, we list our assumptions and resultant properties of tabular data. Subsequently, we present the maximal invariance property of \(\) and our hypergraph-based learning framework \(g\). As a part of our notation, we use \([n]\) to denote \(\{1,2,,n\}\). Preliminaries and all detailed proofs are presented in the Appendix A.1 and A.2 respectively.

**Assumption 2.1**.: For any table \((_{ij})_{i[n],j[m]}\) (where \(i,j\) are indexes of the rows, columns), an arbitrary group action \(a_{n}_{m}\) acting appropriately on the rows and columns leaves the target random variables associated with tasks on the entire table unchanged.

This assumption is valid in most real-world tables - as reordering the columns and the rows in the table oftentimes doesn't alter the properties associated with the entire table (e.g. name of the table, etc). As noted earlier, for tables with an explicit ordering of rows or columns, we can make them permutation invariant by adding a ranking index as a new column or row appropriately. To model this assumption, we state a property required for functions acting on tables next.

_Property 1_.: A function \(: R^{d}\) which satisfies Assumption 2.1 and defined over tabular data must be invariant to actions from the (direct) product group \(_{n}_{m}\) acting appropriately on the table i.e. \((a)=()\ \  a_{n} _{m}\).

However, HyTrel (or the function \(g\) via hypergraph modeling) through Eqns (1-5)) models invariances of the associated incidence matrix to the product group \(_{mn}_{m+n+1}\) (proof presented in the appendix). To make the connection between the two, we present the maximal invariance property of our proposed HyTrel model.

**Theorem 2.2**.: _A continuous function \(:^{d}\) over tables is maximally invariant when modeled as a function \(g:^{k}\) over the incidence matrix of a hypergraph \(\) constructed per Section 2.1 (Where \(g\) is defined via Eqns (1-5)) if \(\) a bijective map between the space of tables and incidence matrices (defined over appropriate sizes of tables, incidence matrices). That is, \((_{1})=(_{2})\) iff \(_{2}\) is some combination of row and/or column permutation of \(_{1}\) and \(g(_{1})=g(_{2})\) where \(_{1},_{2}\) are the corresponding (hypergraph) incidence matrices of tables \(_{1},_{2}\)._

_Proof Sketch_: Detailed proof is provided in Appendix A.2. The above theorem uses Lemma 1 from (Tyshkevich and Zverovich, 1996) and applies the Weisfeiler-Lehman test of isomorphism over the star expansion graphs of the hypergraphs toward proving the same.

As a consequence of Theorem 2.2, two tables identical to permutations will obtain the same representation, which has been shown to improve generalization performance (Lyle et al., 2020).

### Pretraining Heads

**ELECTRA Head**: In the ELECTRA pretraining setting, we first corrupt a part of the cells and the headers from a table and then predict whether a given cell or header has been corrupted or not Iida et al. (2021). Cross-entropy loss is used together with the binary classification head.

**Contrastive Head**: In the contrastive pretraining setting, we randomly corrupt a table-transformed hypergraph by masking a portion of the connections between nodes and hyperedges, as inspired by the hypergraph contrastive learning (Wei et al., 2022). For each hypergraph, we corrupt two augmented views and use them as the positive pair, and use the remaining in-batch pairs as negative pairs. Following this, we contrast the table and column representations from the corresponding hyperedges. The InfoNCE (van den Oord et al., 2018) objective is used for optimization as in 8.

\[loss=-_{+}/)}{_{i=0}^{K} (_{i}/)}\] (8)

where \((,_{+})\) is the positive pair, and \(\) is a temperature hyperparameter.

Experiments

### Pre-training

**Data** In line with previous TaLMs (Yin et al., 2020; Iida et al., 2021), we use tables from Wikipedia and Common Crawl for pretraining. We utilize preprocessing tools provided by Yin et al. (2020) and collect a total of 27 million tables (1% are sampled and used for validation).3 During pretraining, we truncate large tables and retain a maximum of 30 rows and 20 columns for each table, with a maximum of 64 tokens for captions, column names, and cell values. It is important to note that the truncation is solely for efficiency purposes and it does not affect HyTrel's ability to deal with large tables, as elaborated in appendix B.1.

**Settings** With the ELECTRA pretraining objective, we randomly replace 15% of the cells or headers of an input table with values that are sampled from all the pretraining tables based on their frequency, as recommended by Iida et al. (2021). With the contrastive pretraining objective, we corrupted 30% of the connections between nodes and hyperedges for each table to create one augmented view. The temperature \(\) is set as 0.007. For both objectives, we pretrain the HyTrel models for 5 epochs. More details can be found the Appendix C.1.

### Fine-tuning4

After pretraining, we use the HyTrel model as a table encoder to fine-tune downstream table-related tasks. In order to demonstrate that our model does not heavily rely on pretraining or on previous pretrained language models, we also fine-tune the randomly initialized HyTrel model for comparison. In this section, we introduce the evaluation tasks and the datasets. We choose the following four tasks that rely solely on the table representations since we want to test the task-agnostic representation power of our model and avoid training separate encoders for texts (e.g., questions in table QA tasks) or decoders for generations. As mentioned, our encoder can be used in all these scenarios and we leave its evaluation in other table-related tasks as future work.

**Column Type Annotation** (CTA) task aims to annotate the semantic types of a column and is an important task in table understanding which can help many knowledge discovery tasks such as entity recognition and entity linking. We use the column representations from the final layer of HyTrel with their corresponding hyperedge representations for making predictions. We evaluate HyTrel on the TURL-CTA dataset constructed by Deng et al. (2020).

**Column Property Annotation** (CPA) task aims to map column pairs from a table to relations in knowledge graphs. It is an important task aimed at extracting structured knowledge from tables. We use the dataset TURL-CPA constructed by Deng et al. (2020) for evaluation.

**Table Type Detection** (TTD) task aims to annotate the semantic type of a table based on its content. We construct a dataset using a subset from the public WDC Schema.org Table Corpus.

**Table Similarity Prediction** (TSP) task aims at predicting the semantic similarity between tables and then classifying a table pair as similar or dissimilar. We use the PMC dataset proposed by Habibi et al. (2020) for evaluation.

### Baselines

**TaBERT**(Yin et al., 2020) is a representative TaLM that flattens the tables into sequences and jointly learns representations for sentences and tables by pretraining the model from the BERT checkpoints. _K=1_ and _K=3_ are the two variants based on the number of rows used.

**TURL**(Deng et al., 2020) is another representative TaLM that also flattens the tables into sequences and pretrains from TinyBERT (Jiao et al., 2020) checkpoints. It introduces a vision matrix to incorporate table structure into the representations.

**Doduo**(Suhara et al., 2022) is a state-of-the-art column annotation system that fine-tunes the BERT and uses table serialization to incorporate table content.

### Main Results

The results are presented in Tables 1 and 2. Overall, HyTrel consistently outperforms the baselines and achieve superior performance. A salient observation is that our model (even without pretraining) can achieve close to state-of-the-art performance. In comparison, we notice that the performance slumps significantly for TaBERT without pretraining. This phenomenon empirically demonstrates the advantages of modeling the table structures as hypergraphs over the other methods that we compare.

Additionally, we observe that the two pretraining objectives help different tasks in different ways. For the CTA, CPA, and TTD tasks, the two objectives can help HyTrel further improve its performance. In general, the ELECTRA objective performs better than the contrastive objective. These results are also in line with the representation analysis in Section 4.2 where we observe that the ELECTRA objective tends to learn table structure better than the contrastive objective. However, for the TSP task, we observe that the contrastive objective can help the HyTrel model while the ELECTRA objective fails to bring any improvement. One possible reason for the ineffectiveness of the ELECTRA objective could be its inability to transfer well across domains. HyTrel pretrained with tables from Wikipedia and Common Crawl could not transfer well to the medical domain PMC dataset. As for the improvement observed from the contrastive objective, the reason could be that contrastive learning that uses similarity metrics in the objective function can naturally help the similarity prediction task.

**Scalability:** As stated in Section 3, we have truncated large tables during pretraining. However, this truncation does not hinder the ability of HyTrel to handle large table inputs in downstream tasks. In Appendix B.1, we present additional experiments demonstrating that: (a) HyTrel can effectively process tables of any size as inputs, and (b) down-sampling can be a favorable strategy when working with large input tables, significantly improving efficiency without compromising performance.

   Systems & Column Type Annotation & Column Property Annotation \\  Sherlock & 88.40 / 70.55 / 78.47 & - \\ BERT\({}_{base}\) & - & 91.18 / 90.69 / 90.94 \\ TURL + metadata & 92.75 / 92.63 / 92.69 & 92.90 / 93.80 / 93.35 \\ Doduo + metadata & 93.25 / 92.34 / 92.79 & 91.20 / 94.50 / 92.82 \\  TaBERT\({}_{base}\)_(K=1)_ & 91.40 \(\)0.06 / 89.49\({}_{ 0.21}\) / 90.43\({}_{ 0.11}\) & 92.31 \(\)0.24 / 90.42\({}_{ 0.53}\) / 91.36\({}_{ 0.30}\) \\ _w/o_ Pretrain & 90.00 \(\)0.14 / 85.50\({}_{ 0.09}\) / 87.70\(\)0.10 & 89.74 \(\)0.40 / 68.74\({}_{ 0.93}\) / 77.84\({}_{ 0.64}\) \\ TaBERT\({}_{base}\)_(K=3)_ & 91.63 \(\)0.21 / 91.12\({}_{ 0.25}\) / 91.37\({}_{ 0.08}\) & 92.49 \(\)0.18 / 92.49\({}_{ 0.22}\) / 92.49\({}_{ 0.10}\) \\ _w/o_ Pretrain & 90.77 \(\)0.11 / 87.23\({}_{ 0.22}\) / 88.97\({}_{ 0.12}\) & 90.10 \(\)0.17 / 84.83\({}_{ 0.89}\) / 87.38\({}_{ 0.48}\) \\  HyTrel _w/o_ Pretrain & **92.92**\(\)0.11 / 92.50\(\)0.10 / 92.71\({}_{ 0.08}\) & 92.85 \(\)0.35 / 91.50\({}_{ 0.54}\) / 92.17\({}_{ 0.38}\) \\ HyTrel _w/ ELECTRA_ & 92.85 \(\)0.21 / **94.21**\(\)0.09 / **93.53**\(\)0.10 & 92.88 \(\)0.24 / **94.07**\(\)0.27 / **93.48**\(\)0.12 \\ HyTrel _w/ Contrastive_ & 92.71 \(\)0.20 / 93.24\({}_{ 0.08}\) / 92.97\({}_{ 0.13}\) & **93.01**\(\)0.40 / 93.16\({}_{ 0.40}\) / 93.09\({}_{ 0.17}\) \\   

Table 1: Test results on the CTA and CPA tasks (Precision/Recall/F1 Scores,%). The results of TaBERT and HyTrel are from the average of 5 system runs with different random seeds. For fair comparisons, we use the results of TURL and Doduo with metadata, i.e., captions and headers.

    & Table Type Detection & Table Similarity Prediction \\   & Accuracy & Precision/Recall/F1 & Accuracy \\  TFIDF+Glove+MLP & - & 87.36 / 83.81 / 84.47 & 85.06 \\ TabSim & - & 88.65 / 85.45 / 86.13 & 87.05 \\  TaBERT\({}_{base}\)_(K=1)_ & 93.11 \(\)0.31 & 87.04 \(\)0.64 / 85.34\({}_{ 0.93}\) / 86.18\({}_{ 1.13}\) & 87.35 \(\)1.42 \\ _w/o_ Pretrain & 85.04 \(\)0.41 & 33.61 \(\)1.27\({}_{}\) / 50.31 \(\)1.25\({}_{}\) / 40.30\({}_{ 12.03}\) & 63.45 \(\)10.11 \\ TaBERT\({}_{base}\)_(K=3)_ & 95.15 \(\)0.14 & 87.76 \(\)0.64 / 86.97 \(\)0.59 / 87.36\({}_{ 0.95}\) & 88.29 \(\)0.98 \\ _w/o_ Pretrain & 89.88 \(\)0.26 & 82.96 \(\)1.84 / 81.16 \(\)1.45 / 82.05\({}_{ 1.02}\) & 82.57 \(\)1.20 \\  HyTrel w/o_ Pretrain & 93.84 \(\)0.17 & 88.94 \(\)1.83 / 85.72 \(\)1.52 / 87.30\({}_{ 0.12}\) & 88.38 \(\)1.43 \\ HyTrel w/ ELECTRA & **95.81**\(\)0.19 & 87.35 \(\)0.42 / 87.29 \(\)0.84 / 87.32\({}_{ 0.50}\) & 88.29 \(\)0.49 \\ HyTrel w/ Contrastive & 94.52 \(\)0.30 & **89.41**\(\)0.58 / **89.10**\(\)0.90 / **89.26**\(\)0.53 & **90.12**\(\)0.49 \\   

Table 2: Test results on the TTD (Accuracy Score,%) and TSP (Precision/Recall/F1 Scores,%) tasks. The results of TaBERT and HyTrel are from the average of 5 system runs with different random seeds.

Qualitative Analysis

### HyTrel Learns Permutation Robust Representations

We sample 5,000 tables from the validation data for analysis. We analyze the impact of applying different permutations to a table, including permuting rows, columns, and both rows/columns independently.

Toward our analysis, we measure the Euclidean distance (L2 Norm) of the representations (cells, rows, columns and tables). As shown in Figure 3, the distance is almost always 0 for the HyTrel model because of its explicit invariance-preserving property. On the other hand, for the TaBERT model, the distance is not trivial. We observe that when more rows (K=3) are enabled, the value of the L2 norm increases as we introduce different permutations. Moreover, we also observe that permuting the columns has a greater impact on the L2 norm than the row permutations. A combination of rows and columns permutations has the largest impact among all three actions. Note that when K=1 with just one row, the effect of row permutation is disabled for TaBERT.

### HyTrel Learns the Underlying Hierarchical Table Structure

Next, we demonstrate that the HyTrel model has learned the hierarchical table structure into its representations, as we target at. We use t-SNE [Van der Maaten and Hinton, 2008] for the visualization of different table elements from the same 5,000 validation tables, as shown in Figure 4.

We observe that with random initializations, different table elements cannot be distinguished properly. After the encoding of the randomly initialized HyTrel model, we start to observe noticeable differences for different table elements in the visualization space. Notably, the individual cell representations start to concentrate together and can be distinguished from high-level table elements (tables, columns, and rows) which occupy their separate places in the space of representations. We also notice that, by pretraining the HyTrel with the ELECTRA objective, all table elements can be well separated, showing that it incorporates the hierarchical table structure into its latent representations. As for the contrastive pretraining, we see that it can distinguish columns from rows as compared with randomly initialized HyTrel, but could not to well separate the table representations in comparison with the ELECTRA pretraining. This also partially explains the better performance of the ELECTRA pretraining in the CTA, CPA and TTD tasks in contrast to the contrastive pretraining.

Figure 4: t-SNE visualization of the representations learned by HyTrel. ‘tab’, ‘col’, ‘row’, and ‘cell’ are the representations for different table elements: tables, columns, rows, and cells.

Figure 3: Average distance between table element representations before and after permutations. The HyTrel is immune to the permutations while the TaBERT is sensitive to them.

### HyTrel Demonstrates Effective Pretraining by Capturing the Table Structures

Our evaluation shows that the HyTrel model can perform well without pretraining, which demonstrates its training efficiency by modeling table structures. Here we further analyze how much pretraining is required for HyTrel to further enhance its performance, as compared with the baseline model. We plot the validation performance of the tasks evaluated at different pretraining checkpoints in Figure 5.

Overall, we can observe that the performance of HyTrel improves drastically during the first several pretraining epochs, and then saturates at about 5 epochs. With the minimal pretraining, HyTrel can outperform the fully pretrained TaBERT models. This demonstrates that our model does not require extensive pretraining to further improve its performance in contrast with previous TaLMs (e.g., TURL for 100 epochs, TaBERT for 10 epochs). Besides, we also observe from the curves that the ELECTRA objective consistently outperforms the contrastive objective for the CTA, CPA, and TTD tasks, but under-performs on the TSP task. Also, the ELECTRA objective has a negative impact on the TSP task when pretrained for longer duration, which is in line with our previous findings.

## 5 Related Work

There are two avenues of research that have studied in tabular data representation learning. The first group of studies focus on predicting labels (essentially one row) for classification and regression problems, using row information and column schema as input. These studies use gradient descent-based end-to-end learning and aim to outperform tree-based models through task-specific model pretraining and fine-tuning.

The second group of studies proposes TaLMs to retrieve task-agnostic tabular data representations for different downstream table understanding tasks. Drawing inspiration from textual Language Models like BERT , many works  serialize tables to a sequence of tokens, leveraging existing checkpoints and textual self-supervised objectives. However, the representations of the tables can not only be learned from table content and by utilizing table structures, similar to other forms of semi-structured data like code and HTML. Some contemporary works have noticed the importance of the table structures and introduce many techniques to learn a certain aspect of them, such as masking , coordinates , and attention bias . Our work belongs to this second group of studies and we propose to use hypergraphs to comprehensively model the rich table structures, and this is close to previous graph-based neural networks  where tables have been structured as graphs to incorporate row and column order information.

Table representation learning that focuses on joint text and table understanding is a separate field of research that partially overlaps with our work. Among them, some work  specialize in question-answering (QA) on tables and they jointly train a model that takes the question and the table structure as input together, allowing the pretraining to attend to the interactions of both texts and tables and boosting

Figure 5: Performance with different pretraining checkpoints on the validation set of the four tasks. For the TaBERT models, we can only access the randomly initialized and fully pretrained (10 epochs) checkpoints. All results are from 5 system runs with different random seeds.

the table-based QA tasks. Another branch of joint text and table understanding work focuses on text generation from tables, relying on an encoder-decoder model like T5  that can encode tables and decode free-form texts. In contrast to these studies, our work centers on the importance of structures in tables for table representation only, without extra text encoding or decoding.

Learning on hypergraphs has gone through a series of evolution  in the way the hypergraph structure is modeled using neural networks layers. However, many of them collapse the hypergraph into a fully connected graph by clique expansion and cannot preserve the high-order interaction among the nodes. The recent development of permutation invariant networks  has enabled high-order interactions on the hypergraphs  that uses parameterized multi-set functions to model dual relations from node to hyperedges and vice versa. Closely related to the latest advancement, our HyTrel model adopts a similar neural message passing on hypergraphs to preserve the invariance property and high-order interactions of tables.

## 6 Limitations

The proposed HyTrel is a table encoder, and by itself cannot handle joint text and table understanding tasks like table QA and table-to-text generation. While it's possible to add text encoders or decoders for these tasks, it can potentially introduce additional factors that may complicate evaluating our hypothesis about the usefulness of modeling structural table properties. Moreover, the current model structure is designed for tables with simple column structures, like prior TaLMs, and cannot handle tables with complex hierarchical column structures. Additionally, HyTrel does not consider cross-table relations. Although we believe the hypergraph can generalize to model complicated column structures and cross-table interactions, we leave these aspects for future research.

## 7 Conclusion

In this work, we propose a tabular language model HyTrel that models tables as hypergraphs. It can incorporate the permutation invariances and table structures into the table representations. The evaluation on four table-related tasks demonstrates the advantages of learning these table properties and show that it can consistently achieve superior performance over the competing baselines. Our theoretical and qualitative analyses also support the effectiveness of learning the structural properties.