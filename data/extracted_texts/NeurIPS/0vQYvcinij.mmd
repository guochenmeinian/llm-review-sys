# DrawEduMath: Evaluating Vision Language Models with Expert-Annotated Students' Hand-Drawn Math Images

DrawEduMath: Evaluating Vision Language Models with Expert-Annotated Students' Hand-Drawn Math Images

 Sami Baral\({}^{}\)\({}^{*}\) Li Lucy\({}^{}\)\({}^{*}\)

Ryan Knight\({}^{+}\) Alice Ng\({}^{=}\) Luca Soldaini\({}^{+}\)

Neil T. Heffernan\({}^{}\) Kyle Lo\({}^{+}\)

\({}^{}\)Worcester Polytechnic Institute \({}^{}\)University of California Berkeley

\({}^{+}\)Insource Services \({}^{=}\)Teaching Lab \({}^{+}\)Allen Institute for AI

Both authors contributed equally to this research.

Contact: {sbaral,nth}@wpi.edu, lucy3_li@berkeley.edu, rknight@insourceservices.com, alice.ng@teachinglab.org, {lucas,kylel}@allenai.org

###### Abstract

In real-world settings, vision language models (VLMs) should robustly handle naturalistic, noisy visual content as well as domain-specific language and concepts. For example, K-12 educators using digital learning platforms may need to examine and provide feedback across many images of students' math work. To assess the potential of VLMs to support educators in settings like this one, we introduce \(\)DrawEduMath, an English-language dataset of 2,030 images of students' handwritten responses to K-12 math problems. Teachers provided detailed annotations, including free-form descriptions of each image and 11,661 question-answer (QA) pairs. These annotations capture a wealth of pedagogical insights, ranging from students' problem-solving strategies to the composition of their drawings, diagrams, and writing. We evaluate VLMs on teachers' QA pairs, as well as 44,362 synthetic QA pairs derived from teachers' descriptions using language models (LMs). We show that even state-of-the-art VLMs leave much room for improvement on \(\)DrawEduMath questions. We also find that synthetic QAs, though imperfect, can yield similar model rankings as teacher-written QAs. We release \(\)DrawEduMath to support the evaluation of VLMs' abilities to reason mathematically over images gathered with educational contexts in mind.

## 1 Introduction

As AI models demonstrate growing proficiency in mathematical reasoning, there is a corresponding rise in AI-powered tools designed to enhance math education . For example, AI systems have the potential to provide immediate feedback on students' work , or shed insight on common misconceptions . These trends prompt critical questions about the ability of current models to handle real-world math problems, such as those encountered in classrooms and tutoring sessions, as opposed to curated problems found in popular benchmarks like GSM8k  and MATH . We present \(\)**DrawEduMath**, a collection of 2,030 images of K-12 math problems paired with images of _handwritten, hand-drawn responses_ to these problems by real student users of an online learning platform. This collection encompasses a diverse array of mathematical concepts, educational standards, and problem types. We supplement all images with the following:1. **Detailed descriptions** provided by teachers, capturing all elements of the student's handwritten responses, including the students' approach, possible misconceptions, and mistakes made during problem-solving.
2. **Question-answer (QA) pairs**, some of which are written by teachers and some generated through an LM-based pipeline. The latter involves identifying key facets in teachers' descriptions and restructuring them into questions and answers.
3. **Metadata** for each image, encompassing the type of problem, corresponding educational standards or grade level, topical categories, and other relevant information.

In this work, we detail our benchmark creation process (SS3), which aims to balance educators' expertise and the scalability of LM-based data generation and judgement (SS4). We then use ()DrawEduMath to evaluate the capabilities of current VLMs to interpret the content of students' handwritten responses (SS6). We find that though models can identify superficial aspects of images such as paper type and drawing medium, they struggle on questions related to the correctness of students' responses. In addition, closed models such as Claude and GPT-4o tend far outperform open-source Llama 3.2-11B. Overall, we hope that this work will facilitate further research on VLMs' abilities to support students' math learning in diverse, real-world educational settings.

## 2 Related Work

AI for Math Education.The advent of large language models (LLMs) has transformed online learning platforms [1; 11; 44; 18] by introducing automated tools for error identification [16; 43; 37], feedback provision , student response scoring , and curriculum adaptation , primarily for typed answers. However, most math instruction in traditional classrooms still relies on handwritten problem-solving, posing challenges due to the unstructured nature of handwritten content and a lack of annotated datasets . Existing math datasets, such as GSM8k  or MATH , focus on K-12 content but often lack input from educators, leaving a gap in aligning AI research with the classroom realities. While the recent advancements in multimodel LLM capabilities allow for the interpretation of complex images , their effectiveness in understanding student handwritten math

Figure 1: Each image in our dataset is a concatenation of a math problem on the left with a student response on the right. Teachers describe the student’s response to the problem, and then a model, such as GPT-4o shown here, writes QA pairs extracted from facets of the description. More example images, along with teacher-written QA, are shown in Figure 3.

remains uncertain. This paper aims to address this gap by contributing a benchmark created by real students and teachers.

Vision-language Evaluation and Benchmarks.The growth of pretrained VLMs accompanies the growth of vision-language benchmarks, e.g. MMMU , DocVQA , and VQA . Within the domain of math, notable examples include MathVista , GeoQA , Geometry3k , and MathVerse . Many of these prior visual math benchmarks, however, focus on images where mathematical information is shown in a standardized or typed manner. In contrast, the images in our dataset consist mostly of handwriting and drawings across different paper, lighting, and digitization types. In addition, our focus on problem solving strategies and pedagogy allows our annotations to go beyond optical character recognition emphasized in previous handwritten datasets [9; 29; 24; 48; 35; 31; 13].

## 3 The \(\)DrawEduMath Dataset

Our dataset begins by sampling images of K-12 students' responses to math problems, followed by two rounds of annotation by teachers. During annotation, we ask teachers to both describe students' responses and write a few QA pairs for each image. Overall, teachers' annotations mention a variety of K-12 mathematical concepts and representations (Table 2). In total, this process yields 2,030 described images and 11,661 teacher-written QA pairs (Table 3, Table 6).

### Sampling Students' Math Images

Our dataset consists of 2,030 images of U.S.-based students' handwritten math responses to 188 math problems spanning Grade 2 through high school (Table 1). These images were initially collected on the ASSISTments  online learning platform, where students receive feedback from teachers on assigned work. The problems that accompany each student response are drawn from three overlapping1 open educational resources (OER): Eureka Math, Open Up Resources, and Illustrative Math. Metadata linked to these problems include Common Core State Standards (CCSS) labels, which indicate specific K-12 math skills or concepts targeted in problems . Initially, the data provided by the learning platform comprised approximately 60,000 images across 188 problems, with an average of 300 images per problem. From this, we randomly sampled 15 images per problem. To ensure student privacy, undergraduate research assistants cropped the images to include only the math content and removed any personally identifiable information, such as students' hands, by covering them with dark rectangles. Our use of these images was deemed exempt from review by our institution's institutional review board; see more discussion in SS9.

  
**Math Domain** & **Images** & **Example Words or Phrases in Teachers’ Annotations of Images** \\  Ratios \& Proportions & 29.9\% & _proportional relationships, cps, proportional reasoning, dts, equivalent ratio, corresponding values, scronym, double number, multiplicative relationship, proportional line_ \\  Geometry & 24.4\% & _xyz’s_’_, isosceles triangle, perpendicular bisector, rigid transformation, equilateral triangle, original triangle, two quadrilaterals, equilateral triangles, original image_ \\  Expressions \& Equations & 14.7\% & _negative infinity, connected rectangles, x+1,5x, x, number line, arrow pointing, horizontal rectangle_ \\  The Number System & 9.5\% & _vertical number, shaded sections, five sections, negative integers, negative numbers, algorithm_ \\  & & _subtraction, incorrect representation, positive numbers, rectangular model, division algorithm_ \\  Number \& Operations, & 6.6\% & _fraction strips, whole numbers, fractional parts, rectangular fraction, equivalent fractions, mark,_ \\ Fractions & & _identical rectangles, horizontal rectangle, equivalent fraction, tick_ \\   

Table 2: The top five most frequent math domains, as defined by CCSS, that appear \(\)DrawEduMath. Example words or phrases were obtained by applying the phrasemachine text analysis tool  on teachers’ descriptions and answers. The examples shown have the highest tf-idf scores within each domain and occur across at least two problems’ images. Percentages show the relative frequency of each domain across all annotated images.

### Collecting Teachers' Annotations

We hired three NYC-based math teachers from Teaching Lab, a nonprofit professional learning organization, to describe each image. We paid teachers over $200 USD per hour. Each teacher had at least 6 years of experience in math education, with two teachers specializing in middle school and one teacher in grades 5-12. Teachers annotated images on a custom website, and were asked to describe an image as thoroughly as possible so that another teacher could recreate it without viewing it. The annotation website presented an image concatenating the original problem with a student's response, followed by a text box for typed notes and a speech recording module. Teachers also noted whether an image is too blurry for annotation and flagged any PII, adding an extra security layer to our initial PII removal process SS3.1.

Some annotations were obtained by transcribing recordings of teachers' spoken descriptions, while others were typed into an text box. We offered the option of both annotation modalities because spoken descriptions are sometimes faster to obtain and result in longer annotations [38; 10], but typing gives teachers the flexibility to annotate in noisy environments and reduces the risk of transcription errors; see comparison in Figure 2. We obtained similar amounts of typed and recorded image descriptions (Table 3). Full annotation instructions, a screenshot of our setup, and additional details on our data collection process can be found in Appendix A.1.

Over the course of two months, teachers annotated 2,376 images of students' responses. After removing images that were deemed too blurry or failed a secondary PII check, our final dataset consists of 2,030 images paired with math teachers' descriptions.

### Revising and Augmenting Annotations

During a second data collection phase, teachers augmented and revised existing annotations. This second phase of annotation required twice as much time per example than the first one (Table 3). So, to complete this phase, we recruited five additional teachers from the same professional learning organization as we did in SS3.2. Each of these additional teachers had at least 9 years of experience in math education spanning the UK and several U.S. states, including two from the NYC area. Grade level expertise among these five teachers include one in 9-12, one in 5-12, two in K-8, and one in K-12.

Revising Teachers' Initial Descriptions.During re-annotation, teachers were allowed to revise the image's description, to correct possible transcription errors or other clarity issues that arose during initial annotations. The vast majority (>90%) of image descriptions were not edited, and when edits were made, the Levenshtein distance between old and new descriptions was typically small (Table 3). Through qualitative inspection of edits, most were typo corrections, e.g. _rose \(\) rows_ or _three four \(\) three fourths_.

Adding Teacher-written QA.The main part of our second annotation round focuses on augmenting descriptions with questions teachers may ask about students' responses. We asked teachers to come up with questions that they would naturally ask when examining student responses and were provided with example topics, such as whether the student demonstrated a mathematical concept or made

    \\   \\  Avg minutes spent per image & 2.0 \\ Total words in descriptions & 228k \\ Avg description length & 111.1 \\ \% of descriptions typed & 46.7 \\ \% of descriptions transcribed & 53.3 \\   \\  Avg minutes spent per image & 4.3 \\ Total words in descriptions & 222k \\ Avg description length & 109.5 \\ \% of descriptions felt unchanged & 94.2 \\ Median edit distance of changed descriptions & 48.5 \\ \# of teacher-written QA pairs & 11,661 \\ Avg \# of teacher-written QA per image & 5.74 \\ Avg length of teacher-written questions & 12.7 \\ Avg length of teacher-written answers & 16.2 \\   

Table 3: Key data statistics pertaining to the collection of teachers’ language for ©DrawEduMath. Word counts and text lengths are determined using white-space delineated tokens.

Figure 2: For some annotators, their recorded descriptions of images are longer or require less time than typed ones. Annotation length is calculated based on white-spaced-separated tokens.

a common error for a problem type. This top-down data collection approach in this second round complements the bottom-up, description-based approach emphasized in the first round SS3.2, and may cater more towards potential uses of VLM-based systems for educators.

First, teachers propose questions based on math problems in our dataset. Given a problem, teachers write up to five questions they may have about any student's response to that problem (Figure 1). Then, we present teachers with images of students' responses annotated in SS3.2, and ask them to write answers to each problem-specific question based on what they observe in each student's response. Two additional questions, _What errors does the student make in their response?_ and _What strategy does the student use to solve the problem?_ were answered for all problems and student responses (Figure 3), and teachers also had the option to add up to two additional image-specific question-answer pairs. Across all 2,030 images, teachers augmented our \(\)DrawEduMath with 11,661 QA pairs.

## 4 Scaling Data with Synthetic QAs

Writing numerous QA pairs for visual benchmark creation is more time-intensive than describing images in a free-form manner (Table 3). Inspired by , who introduce a scalable workflow for generating VQA benchmarks from image captions, we use LMs to transform teachers' descriptions into synthetic QAs.

Transforming Descriptions to QA Pairs.We prompt Claude-3.5 Sonnet and GPT-4o to first decompose captions into "facets", or atomicized snippets of information, and rewrite these facets into question-answer (QA) pairs  (Figure 1). The prompts were iteratively refined with input from an expert teacher to enhance the quality of the generation responses. Specifically, the models were instructed to generate self-contained facets and corresponding QA pairs, avoiding open-ended questions or those with multiple correct answers. The full prompt we used for this data transformation step can be found in Appendix B.

We obtain a total of 44,362 synthetically created QA pairs (Table 4). On average, LM-generated QA had much shorter answers than those written by teachers, due to instructions preferring conciseness included in our description-to-QA prompt. Shorter answers are more suitable for reference-based evaluation with lightweight metrics such as string or ngram matching, but longer answers by teachers contain more rich and detailed information.

Quality Assessment of Synthetic QA.Two annotators examined a sampled set of QA pairs outputted from our description-to-QA pipeline to assess their quality. These annotators have complementary backgrounds, both of which are valuable for examining the application of VLMs for education: one has worked as a K-12 math teacher (Evaluator \(\)), and another has worked on technology applications for educators (Evaluator \(\)). For each image and QA pair, we ask: 1) _Can

  
**Descriptions \(\)\(\) Synthetic QA pairs** \\  \# of Claude-generated QA pairs & 21,089 \\ Avg \# of Claude’s QA per image & 10.3 \\ Avg length of Claude’s questions & 10.6 \\ Avg length of Claude’s answers & 2.2 \\ \# of GPT-4o-generated QA pairs & 23,273 \\ Avg \# of GPT-4o’s QA per image & 11.5 \\ Avg length of GPT-4o’s questions & 10.4 \\ Avg length of GPT-4o’s answers & 3.0 \\   

Table 4: Key data statistics pertaining to synthetic QA pairs in \(\)DrawEduMath. Word counts for determining lengths are based on white-space delineated tokens.

Figure 3: Examples of teacher’s answers to a question asking about possible errors in students’ responses to math problems. All three examples of students’ hand-drawn responses are for the same math problem asking students to draw and shade units on fraction strips to show 4 thirds, shown on the left.

this question be answered by the provided image?_ and 2) _Is the provided answer correct?_ 100 QA pairs were randomly sampled, evenly split between GPT-4o and Claude 3.5, with annotators each reviewing 50 pairs. Instructions for synthetic QA assessment can be found in Appendix D.1.

Despite some variability in annotators' judgments, the majority of QA pairs are answerable and correct (Table 5).

From qualitative inspection, unanswerable questions tend to be those where the referent of mentions is ambiguous without additional context. For example, a question may ask, _Where does the second arrow point?_, but it may be unclear which of the overlapping arrows in the image is the "second" one. So, "unanswerability" relates to the extent to which one infers ambiguous referents through pragmatic convention; for example, the _first piece_ in a row of rectangles may be the one furthest left, and the _first triangle_ in a geometric transformation may be the preimage. As for incorrect answers, Evaluator \(\) marked some answers as incorrect due to the question being unanswerable. A few incorrect answers emerged from what appeared to be genuine annotation mistakes. For example, in one case, the annotator excluded the label on one tick mark in their annotation, and so the extracted QA's answer missed one value. Overall, we hope our inclusion of teachers' original descriptions in \(\)DrawEduMath can facilitate future improvements to the scaling of VQA benchmark creation.

## 5 Building a Taxonomy of Question Types

To document what types of questions show up in \(\)DrawEduMath and better understand which questions may be more difficult for models than others, we group questions into several categories. We defined question categories in an iterative manner mixing qualitative and quantitative approaches, akin to , who reframe content analysis into pattern detection, refinement, and confirmation steps. During pattern detection, we qualitatively code a combined pool of generated and teacher-written questions. To efficiently observe a range of common yet distinctive question patterns during this coding step, we sampled ten questions from clusters of questions' sentence embeddings .2 We obtained these clusters using \(k\)-means with \(k\)=30, and embed questions after masking out their nouns,3 so that we can examine problem-agnostic question patterns shared across different math domains. For example, questions that start with _How many..._, _Into how many..._, and _What is the total_... would occur in the same embedding cluster.

Next, for category refinement and confirmation, we recoded our observations into possible question types for GPT-4o to categorize. We iterated over question types and categorization prompts by running GPT-4o on smaller samples of 500 to 2000 questions. Proposing more fine-grained or more numerous question categories led to less cleanly delineated outputs, and so we aimed for category definitions that led to reasonable groupings. Our final prompt can be found in Appendix B.

Our resulting taxonomy of questions separates them into seven categories: 1) higher-level understanding of math content, 2) low-level content composition & positioning, 3) writing & labels, 4) problem solving steps, strategy, & solution, 5) counting content, 6) image creation & medium, and 7) correctness & errors (Table 6). In particular, the first two categories are designed to separate out questions that involve some mathematical reasoning from those that do not. For example, _What is the slope of the line_ requires knowing what a slope is and how it's depicted in a graph, while questions that differentiate left from right pertain to more basic spatial understanding.

As shown in Table 6, (1) we find little difference in QA generation behavior between our two choices of LM, and (2) teachers' questions focus more on students' problem-solving steps and response correctness, while synthetic questions have a different emphasis.4 An eighth category, "Other", which

    & **Is the provided A correct?** \\   & \(\) & \(\) & \(\) & \(\) \\ 
**Yes** & 50 & 41 & **Yes** & 47 & 43 \\
**No** & 0 & 9 & **No** & 3 & 7 \\   

Table 5: Quality assessment of questions (**Q**) and answers (**A**) extracted by Claude & GPT-4o from teachers’ descriptions of students’ responses.

we asked GPT-4o to use if a question fits into none of the provided categories, only makes up 0.4%, 1.1%, 0.6% of Claude, GPT-4o, and teacher-written questions, respectively.

## 6 Evaluating Vision Language Models with \(\)DrawEduMath

Experimental Setup.To assess the capability of recent visual language models (VLMs) in interpreting students' handwritten math work, we run several VLMs on \(\)DrawEduMath. We experiment with four VLMs: three commercial models--GPT-4o, Claude 3.5 Sonnet , and Gemini 1.5 Pro --alongside open-source Llama 3.2-11B Vision . To select a prompt for running our experiments, we iterated over three possible prompts for each model on samples of data and selected the best-performing prompt across them. Our final prompt asks a model to succinctly answer a given question based on the student's response in a provided image (Appendix C).

Automatic Evaluation.To compare VLMs' answers against gold ones, we employ three automatic metrics: (i) ngram matching via Rouge-L , (ii) answer embedding similarity via BERTScore5, and (iii) LLM-based similarity judgements using Mixtral 8x22B . Our prompt for the latter can be found in Appendix C, and asks models to rate the level of similarity between two answers given a question on a scale of 1 (_Quite different answers_) to 4 (_Basically the same_). When reporting results, we binarize these outputs so that 1-2 is counted as incorrect, and 3-4 are counted as correct.

Human Evaluation.To validate our use of reference-based automatic metrics, 5 authors annotated a random sample of 500 QA responses, where 50% are teacher-written QA, 25% are Claude-generated

  
**Question Type** & **Claude** & **GPT-4o** & **Teacher** & **Examples** \\  Higher-level understanding of math & 26.7\% & 25.7\% & 18.8\% & _What type of mathematical representation has the student drawn on the paper?_ \\  Low-level composition and positioning & & & & _What is the slope of the line passing through (0.-5) and (4,-4)? Is the student’s image a third or a half of the original ratio to get 1 batch of light yellow point?_ \\  Writing and labels & 21.9\% & 20.0\% & 11.4\% & _In the third row, where does the student place the number 3? Does the tens_ \\   & composition and & & & _place in 15,420 line up beneath the tens place in 1542? Are the two pieces in the student’s tape diagram equal or unequal in size?_ \\  Writing and labels & 14.6\% & 16.1\% & 17.3\% & _What number is written in front of Pam’s rectangle, after the label ‘Pum’? What range of numbers is labeled on each number line? What did the student label the top of the rectangle?_ \\  Problem solving steps, strategy, and solution & & & _How does placing 26 directly above 25 help the student? Does the student start solving the problem with exact calculations or estimations? What method is the student using to prove that 3/50 equals 0.06?_ \\  Counting content & 10.5\% & 9.1\% & 5.7\% & _What is the label number of shadow-in pieces? How many tick marks are in between 2 and 3? How many rows and columns does the array have?_ \\  Image creation and medium & 15.0\% & 16.0\% & 0.0\% & _Is the student work drawn on graph paper or blank paper?_ _On what surface is the image drawn? Are both triangles in the image pre-printed or is one drawn by the student?_ \\  Correctness and errors & 1.7\% & 1.5\% & 23.0\% & _Does the student get the correct or incorrect answer when adding 30 and 15 together? Did the student keep track of where all the vertices are supposed to be after rotation? Did the student correctly apply the scale factor of 1/2?_ \\   

Table 6: The most common question types in our visual QA benchmark, along with examples of questions categorized within each type. The percentages shown are the proportion of questions across all images within each QA-writing (Claude-generated, GPT-4o-generated, or teacher-written) workflow.

    &  &  &  \\  Model & Bert & Rouge-L & LLM & Human & Bert & Rouge-L & LLM & Human & Bert & Rouge-L & LLM & Human \\  & & & & (n=31) & & & & (n=31) & & & (n=63) \\ GPT-4o & 0.835 & **0.544** & **0.700** & 0.742 & 0.843 & 0.599 & **0.743** & 0.677 & 0.752 & 0.199 & 0.628 & 0.524 \\ Claude 3.5 Sonnet & **0.856** & 0.537 & 0.697 & **0.871** & **0.883** & **0.608** & 0.732 & **0.742** & 0.754 & 0.202 & **0.657** & **0.587** \\ Gemini 1.5 Pro & 0.815 & 0.461 & 0.627 & 0.774 & 0.826 & 0.514 & 0.665 & 0.581 & 0.711 & 0.118 & 0.490 & 0.365 \\ Llama 3.2-11B V & 0.731 & 0.174 & 0.368 & 0.387 & 0.729 & 0.176 & 0.408 & 0.323 & **0.785** & **0.253** & 0.296 & 0.127 \\   

Table 7: Overall evaluation results for models across different VQA datasets generated by GPT4o, Claude, and human teachers. The table presents evaluations using automated metrics (BERTScore, RougeL), as well as assessments from LLMs and human evaluators. **Bold** is the max score across each metric.

QA, and 25% are GPT-4o-generated QA. We stratify sample examples across all four VLMs. Then, annotators complete two tasks. First, given a question and a VLM's answer, we ask: _Is the provided answer correct?_ Second, given the gold answer and the VLM's answer, we ask: _Do these two answers match?_ Full instructions can be found in Appendix D.2. We ask these questions to identify cases where VLMs give correct answers that differ from gold standards, which we find only occurs in 36 out of 500 examples (7.2%).

Assessing Our Automatic Metrics.We compute Spearman correlations between automatic and human estimates of models' performance across teacher-, Claude-, and GPT-4o-generated QA sets and models, and find that LLM-based judgements are most similar to that of humans (\(\) = 0.801), followed by Rouge-L (\(\) = 0.472) and then BERTScore (\(\) = 0.348). In addition, across all 500 human-annotated model responses, binarized LLM-based judgements achieve a high accuracy of 0.896 and F1 score of 0.907 with respect to matching the human judgment.6

Results and Findings.We observe a range of performance across VLMs, most notably a gap between Llama 3.2 compared to closed-source alternatives (Table 7). BERTScore and Rouge-L are able to differentiate models when judging synthetic QA, but they are less able to do so with teacher-written QA. According to our LLM-based evaluator, all three QA sets rank models similarity. In addition, questions pertaining to the correctness and errors tend to be most challenging for models, across both synthetic and teacher-written QA (Table 8). Thus, though synthetic QA can be noisy (SS4), it can illuminate some differentiation of models' abilities.

Our human evaluation of models' responses surfaced a few additional observations around why and how models made errors. One common error involved models not being able to interpret dark images, even though their contents were visible to human annotators. Interestingly, we also found cases where models would answer a question correctly mathematically, but incorrectly with respect to the students' response. For example, to the question _Which whole number corresponds to 18/6 on the number line?_, all VLMs responded with \(3\), even though the students' number line shows 18/6 aligned with 2. Altogether, the wide range of questions and student responses in our dataset can surface failure modes such as these.

## 7 Conclusion

Our work introduces a new dataset and benchmark, \(\)DrawEduMath, built upon teachers' annotations of K-12 students' handwritten responses to math problems. Overall, we hope our work will inspire further research for improving VLMs' capabilities in interpreting and supporting students' math learning in diverse real-world educational settings.

## 8 Limitations

QA Quality and Utility.Our paper involves the lengthy and careful collection of data from teachers, with the goal of creating a benchmark to assess VLMs' abilities to interpret students' handwritten

    &  &  &  &  \\  Question Type & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  Correctness \& errors & 0.525 & 0.559 & 0.491 & 0.610 & 0.601 & 0.440 & 0.402 & 0.276 \\ Counting content & 0.642 & 0.671 & 0.516 & 0.667 & 0.602 & **0.578** & 0.247 & 0.265 \\ Higher-level understanding & 0.696 & 0.599 & 0.642 & 0.605 & 0.632 & 0.484 & 0.333 & 0.350 \\ Image creation \& medium & **0.886** & -* & **0.805** & -* & **0.795** & -* & **0.589** & -* \\ Low-level characteristics & 0.674 & 0.624 & 0.633 & 0.660 & 0.566 & 0.457 & 0.402 & **0.369** \\ Problem strategy \& solution & 0.758 & **0.719** & 0.660 & **0.740** & 0.716 & 0.539 & 0.406 & 0.307 \\ Writing \& labels & 0.711 & 0.606 & 0.647 & 0.620 & 0.615 & 0.499 & 0.338 & 0.216 \\   

Table 8: Comparison of model performance across various question types for GPT4o, Claude3.5 Sonnet, Gemini1.5 Pro, and Llama3.2-11B V. The evaluation includes the average scores from our LLM evaluator across QA pairs generated synthetically by GPT4o and Claude3.5 combined (\) or by teachers (\). Examples of each question type listed above can be found in Table 6. The **max** score is bolded and the \(\) is underlined across each QA and VLM. *For teacher-written QA, this question type had too few examples for robust performance estimates.

work. However, every benchmark has a ceiling, and ours is no exception. The synthetic QA we created from teachers' descriptions can contain errors (SS4), and ensuring that teachers' annotations are completely typo-free would require additional rounds of time-intensive proofreading. In addition to these issues, we made two qualitative observations that speak towards potential limitations of \(\)DrawEduMath for assessing models' visual understanding of students' handwritten work. First, we observed that some questions extracted from teachers' descriptions did not target content specific to the students' response, and instead may test for general mathematical knowledge, e.g. _What is a right angle?_ Second, models' performance on some questions, such as the strategy the student used to solve a problem, should be weighed more heavily than performance on other questions, such as the type of paper used. We mitigate this concern by proposing a taxonomy of question types, to allow for more nuance than simply reporting model performance on aggregate. However, we encourage future work to aim for finer-grained categories to yield richer and more useful insights into model performance.

## 9 Ethical Considerations

Risks and Harms of AI in Education.In the context of educational applications, AI models and systems may be viewed as inherently beneficial or for "social good." However, given the high-stakes nature of K-12 pedagogy, the deployment of VLMs, and AI generally, in education should carefully consider potential risks for harm . For example, some pedagogical paradigms may have disproportionate influence on data availability and the design of technologies, thus perpetuating practices that may not cater towards a variety of learners . We acknowledge that the images in our dataset, which is based on U.S.-centric Common Core math problems, may not cover the many varied ways in which students practice or learn math. In addition, we advocate for co-design of evaluative resources with in-domain experts, such as the K-12 teachers in our work.

Data Privacy and Use.Our research has been overseen by our Institutional Review Board (IRB). Since some students' images might have PII (i.e., the students name might have been written on the piece of paper), we conducted extensive rounds of personally identifiable information (PII) removal, detailed in SS3.1. ASSISTments, an online teaching platform, has a history of publishing data (with PII removed) from the platform for academic use . We coordinated closely with ASSISTments, the license owner of the images, to establish clear boundaries on data usage and to develop our public release strategy.

## 10 Acknowledgements

We would like to thank Doug Jaffe, Laurence Holt, and Cristina Heffernan for their valuable feedback on the project. We would also like to thank some of our funding from NSF (1931523) and, IES (R305N210049 and R305T240029), the Jaffe Foundation, the Bill & Melinda Gates Foundation, and the Tools Competition.