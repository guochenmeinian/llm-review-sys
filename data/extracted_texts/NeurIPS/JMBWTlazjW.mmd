# Instruction Please label the task tags for the user query.

[MISSING_PAGE_FAIL:1]

downstream model performance, particularly in relation to the two most popular preference-based learning algorithms, PPO and DPO, which take different approaches to preference-based learning.

As seen in Figure 2, both DPO and PPO rely on training on **preference data**--DPO for directly training the model, and PPO for training a **reward model**. In PPO, this reward model is then used to score generations from the main model generated using a set of **policy training prompts** (unlabelled prompts used for eliciting generations). This provides us with four important aspects of learning from preferences, including the choice of **learning algorithm** itself (PPO vs. DPO).

In this work, we aim to systematically investigate these key components of learning from preferences, exploring the effect of varying each component on downstream model performance. Starting from an initial strong open supervised finetuning (SFT) model, we investigate each aspect in turn. As seen in Figure 1, we find that all aspects are important for performance, albeit to varying degrees. Our findings are as follows:

* When comparing 14 popular existing preference datasets across a diverse set of evaluations, we find that synthetic, diverse data annotated with per-aspect preferences works best for learning from preferences (SS3.1). We find that the quality of the preferences (choice of chosen/rejected pairs) matters more than the quality of the actual generations being considered.
* PPO outperforms DPO across varied datasets in our evaluation suite, even when using exactly the same models and initial training data (SS3.2).
* Increasing reward model size or dataset size used to train the reward model results in improved reward model performance _on benchmarks directly testing reward model performance_. Examining their effect on policy model performance when used during PPO training, we find that these improved reward models have a large effect on GSM performance, but give marginal to no improvements across all other evaluations considered (SS3.3).
* Using unlabelled prompts that better match the test setting during policy can further improve model performance in domain-specific settings (e.g., when focusing on improving math performance), but has limited to no effect when targeting overall performance (SS3.4).

Overall, we suggest a recipe for learning from preference feedback (SS4): using synthetic preference datasets and training using PPO with a large reward model performs best overall. Additionally, targeted prompts should be used if one only cares about a single particular downstream task.

## 2 Setup

We first describe the core aspects of PPO and DPO before moving into our experimental setup. We summarise both approaches in Figure 2.

Figure 1: Performance improvements resulted by changing different components in the preference training of Tülu. Left: Accuracy on GSM , for testing math capabilities. Right: Overall performance, aggregated over the 11 benchmarks described in §2.2.

### PPO and DPO

**PPO.** PPO-based approaches for learning from preference feedback involve first training a reward model on preference data, and then training a policy model by scoring responses produced by the policy model itself during training ("online" learning) as shown in Fig. 2. First, the policy model is prompted to generate responses, which are then scored with the reward model. These scores (i.e., scalar rewards) are then used to guide the optimization of the policy model, following the PPO algorithm. Additionally, a KL penalty term is applied to avoid model degeneration.

* **Preference data.** A preference dataset \(_{R}\) is used for training a reward model, and it typically consists of prompts, responses, and rankings. Each prompt \(x\) comes with a pair of responses \(y_{c},y_{r}\), and a preference ranking between them (denoted as \(y_{c} y_{r} x\), where \(y_{c}\) is the chosen response and \(y_{r}\) is the rejected one). Both the responses and the rankings can be either human-annotated, generated/evaluated by other language models, or derived from examining relative votes on publicly posted online comments (e.g., comparing high and low-upvoted comments from Reddit). While recent work has relied much on synthetic, model-generated data , it is unclear if high-quality human data can provide similar or better performance, or if the significantly larger size of online-scraped datasets allows for improved performance.
* **Reward model.** The reward model \(R_{}(x,y)\) is a scalar function, and can be parameterized with a transformer where a regression head replaces the language modeling head. The reward model is usually trained to minimize the following loss: \[_{R}()=-_{(x,y_{c},y_{r})_{R}} R_{}(x,y_{c})-R_{}(x,y_{r}).\] (1)
* **Policy training prompts.** The policy training data \(_{}\) has a set of policy training prompts. Instead of the pre-generated responses, here for each prompt we sample a response \(y\) from the policy model being actively trained, \(y_{}(y|x)\). It is then scored by the reward model to get a sequence-level reward, \(r=R_{}(x,y)\). Intuitively, this suggests that _more accurate rewards should improve model performance_. Additionally, we note that most existing works typically use either the prompts used during reward training  or a generic pool of anticipated user queries . We investigate if better targeting the prompts used during policy training can further improve performance.
* **Policy training.** The goal of policy training is to maximize the reward of policy-generated responses, subject to a soft KL constraint that prevents degeneration: \[_{_{}}_{x_{},y_{}(y|x)} R_{}(x,y)-_{}_{} ||_{},\] (2) where \(_{}\) is the reference policy (usually the same SFT policy that initializes policy training). We find tuning the KL penalty coefficient \(\) is important for performance and explore the effect of varying it in App. I. Since directly optimizing Eq. 2 can be unstable, it is common to reformulate language generation as a Markov decision process (MDP) and optimize using an RL algorithm. PPO is one of the most widely adopted RL algorithms for this problem. See App. F.2 for additional details about PPO.

**DPO.** DPO is an offline RL approach for performing learning from preference feedback. It allows one to directly optimize the policy on preference data, without building reward models or needing to sample online from the active policy. As such, DPO is an offline training algorithm. In simple terms, DPO aims at increasing the margin between the log-likelihood of the chosen responses and the log-likelihood of the rejected ones, while ensuring the model does not stray far from the initial policy.

Figure 2: The core aspects of learning from preference feedback. For DPO (solid line), preference data is directly used to train a policy model. For PPO (dashed line), preference data is used to train a reward model, which is then used to score model-generated responses during PPO training.

* **Preference data.** The structure of preference data is identical to that of PPO.
* **Policy training.** By following a closed-form solution to Equation 2, DPO re-writes any reward model \(R_{}\) in terms of its corresponding optimal policy \(_{^{*}()}\), \(R_{}(x,y)=()}(y|x)}{_{(y|x)}}+  Z(x),\) where \(Z(x)\) is a partition function. Consequently, the policy model can be trained by directly optimizing the reward objective in Eq. 1, and hence the DPO loss: \[_{}()=-_{(x,y_{e},y_{r})_{ R}}(y_{c} x)}{_{ }(y_{c} x)}-(y_{r} x)}{_{ }(y_{r} x)}.\] (3)

**Comparing DPO and PPO.** Compared with PPO, DPO is more efficient in terms of compute, speed, and engineering efforts. DPO does not need the extra stage of training a reward model, and during policy training it does not need to decode online responses (which is usually slow) or train an additional value model. Meanwhile, PPO trains on **online** data generated by the current policy, while DPO trains on static, pre-generated **offline** data. This may limit exploration in DPO and thus hurt the training quality. While concurrent work has compared DPO and PPO [60; 48], comparisons are typically limited to constrained domains and evaluations, using ground-truth rewards  or primarily examine smaller synthetic settings . We complement such studies by comparing the downstream performance of models trained with DPO and PPO across a wide variety of datasets and evaluations and consider additional potential factors in PPO performance, such as improved reward models and policy training prompts.

### Experimental and Evaluation Setup

We base our exploration off Tulu 2 13B , a popular openly released model. Tulu 2 is a series of Llama 2  finetunes across all sizes with publicly released weights and data, the largest of which achieved state-of-the-art performance on AlpacaEval and Chatbot Arena. As such, we are curious how much further we can push the performance of Tulu 2 models through exploring alternative datasets, training algorithms, reward models, and policy training prompts. We use this model as a base policy when training policy and reward models, following Ouyang et al.  for PPO and Rafailov et al.  for DPO. We provide additional details for each in App. F.

**Evaluation** We extend the Tulu  evaluation, aiming to cover a diverse range of skills and behaviours. We evaluate models on 11 different benchmarks, covering skills such as factuality (MMLU ), reasoning (GSM8k , Big Bench Hard [5; 47]), truthfulness (TruthfulQA ), coding (HumanEval+ [6; 32], MBPP+ [2; 32]), safety (ToxiGen , XSTest ), and instruction following (AlpacaEval 1 and 2 [27; 13], IFEval ). We report the per-category average of evaluations and the overall average across categories. We provide further details in App. D.

## 3 Exploring Learning from Preference Feedback

We now explore each aspect of learning from preferences: **preference data, learning algorithm, reward models**, and finally **policy training prompts**.

### Preference Data

We compare the performance of models trained with DPO across 14 different preference datasets in Table 1. We choose datasets that represent various potential sources of data: human-annotation (HH-RLHF , HelpSteer , Chatbot Arena 2023  and 2024 , AlpacaFarm Human , PRM800k ), web-scraping (SHP-2 , StackExchange ), and synthetic generation (UltraFeedback , Nectar , Orca , Capybara , AlpacaFarm GPT-4 ). For UltraFeedback, we consider both using the 'overall' score provided in the dataset and taking an average of the per-aspect scores ('fine-grained'). We provide further detail on each dataset in App. B. We find that:

**Preference-based learning with existing datasets has the strongest effect on instruction following and truthfulness performance.** Our best models improve on the SFT model by over 8 points in these categories. In contrast, **preference-based learning does not aid factuality**, with all models remaining with 1 point of each other. This suggests that when using existing publically-available datasets, preference-based learning is most useful for improving chat-related abilities (instruction following, truthfulness) and learning stylistic features, but less strong at teaching new facts to a model. Interestingly, we observe that training on the Chatbot Arena data **performs poorly on safety**, suggesting Chatbot Arena volunteers generally prefer more toxic completions.

**Synthetic data with per-aspect annotations performs best.** Synthetic datasets generally outperform human-annotated and web-scraped datasets, especially in truthfulness. Additionally, using datasets collected by first getting per-aspect annotations (i.e., annotations from a human or model that score the helpfulness, harmlessness, etc. of the response independently) and then averaging across these scores tend to outperform datasets that rely only on overall judgements (i.e., just asking the annotator for an overall score instead of a per-aspect score). The two datasets that use this method, HelpSteer and UltraFeedback, display stronger or similar performance to datasets up to 15 times larger (e.g. HelpSteer vs HH-RLHF). We investigate the performance of varied sub-splits of UltraFeedback in App. E, which suggests that the use of per-aspect annotations is more important for performance than the quality of the models used to generate completions for the dataset.

  
**Source** & & **\# Samples** & **Factuality** & **Reasoning** & **Coding** & **Truthfulness** & **Safety** & **Inst. Following** & **Average** \\   - & Llama 2 base & - & 52.0 & 37.0 & 30.7 & 32.7 & 32.7 & - & - \\ - & Tulu 2 (SFT) & - & 55.4 & 47.8 & 45.1 & 56.6 & 91.8 & 44.2 & 56.8 \\   & SHP2 & 500,000 & 55.4 & 47.7 & 40.3 & **62.2** & 90.4 & 45.6 & 56.9 \\  & StackExchange & 500,000 & 55.7 & 46.8 & 39.6 & 67.4 & 92.6 & 44.6 & 57.8 \\   & PRM800k & 6,949 & 55.3 & 49.7 & **46.6** & 54.7 & 91.9 & 43.4 & 56.9 \\  & Chatbot Arena (2023) & 20,465 & 55.4 & 50.2 & 45.9 & 58.5 & **67.3** & 50.8 & 54.7 \\  & Chatbot Arena (2024) & 34,269 & 55.7 & 50.4 & 37.7 & 56.7 & **88.1** & **50.7** & **51.5** \\  & AlpacaF. Human Pref & 9,686 & 55.3 & 47.6 & 43.3 & 56.1 & 90.7 & 44.5 & 56.2 \\  & HIR-RLHF & 158,530 & 54.7 & 46.0 & 43.6 & **65.6** & **93.1** & 45.4 & 58.1 \\  & HelpsSteer & 9,270 & 55.2 & 48.2 & 46.5 & 60.3 & 92.5 & 45.2 & 58.0 \\   & AlpacaF. GPT-4 Pref & 19,465 & 55.3 & 49.1 & 43.4 & 57.7 & 89.5 & 46.3 & 56.9 \\  & Capyhara 7k & 7,563 & 55.2 & 46.4 & 46.4 & 57.5 & 91.5 & 46.1 & 57.2 \\   & Orca Pairs & 12,859 & 55.5 & 46.8 & 46.0 & 57.9 & 90.5 & 46.2 & 57.2 \\   & Nectar & 180,099 & 55.3 & 47.8 & 43.2 & **68.2** & **93.1** & 47.8 & 59.2 \\   & UltraF. (overall) & 60,908 & **55.6** & 48.8 & 46.5 & **67.6** & 92.1 & **51.1** & 60.3 \\   & UltraF. (fine-grained) & 60,908 & 55.3 & **50.9** & 45.9 & **69.3** & 91.9 & **52.8** & **61.0** \\   

Table 1: **Preference data: Performance of Tulu 2 13B models trained on various preference datasets using DPO. Blue indicates improvements over the SFT baseline, orange degradations. Overall, synthetic data works best. DPO training improves truthfulness and instruction-following most, with limited to no improvements in factuality and reasoning.**

  
**Data / Model** & **Alg.** & **Factuality** & **Reasoning** & **Coding** & **Truthfulness** & **Safety** & **Inst. Foll.** & **Average** \\  Llama 2 base & - & 52.0 & 37.0 & 30.7 & 32.7 & 32.7 & - & - \\ Tulu 2 (SFT) & - & 55.4 & 47.8 & 45.1 & 56.6 & 91.8 & 44.2 & 56.8 \\   & DPO & **55.3** & **47.8** & 42.4 & **56.2** & 92.0 & 46.7 & 56.7 \\  & PPO & 55.1 & **47.8** & **46.4** & 54.2 & **92.6** & **47.4** & **57.3** \\   & DPO & **55.4** & **50.2** & 45.9 & **58.5** & **67.3** & **50.8** & 54.7 \\  & PPO & 55.2 & 49.2 & **46.4** & 55.8 & **79.4** & 49.7 & **55.9** \\   & DPO & **55.2** & 47.6 & 44.2 & **60.0** & **93.4** & 46.6 & 57.8 \\  & PPO & 54.9 & **48.6** & **45.9** & 58.0 & 92.8 & **47.0** & **57.9** \\   & DPO & **55.6** & 45.8 & 39.0 & **68.1** & **93.3** & **48.4** & 58.4 \\  & PPO & 55.2 & **51.2** & **45.6** & 60.1 & 92.6 & 47.4 & **58.7** \\   & DPO & 55.3 & 50.9 & 45.9 & **69.3** & **91.9** & **52.8** & **61.0** \\   & PPO & **56.0** & **52.0** & **47.7** & **71.5** & 91.8 & **54.4** & **62.2** \\  Avg. \(\) b/w PPO \& DPO & -0.1 & +1.3 & +2.9 & -2.5 & +2.3 & +0.1 & +0.7 \\   

Table 2: **DPO vs PPO: Average performance of 13B models trained using DPO and PPO across different datasets, along with the performance difference between DPO and PPO (\(\)). Blue indicates improvements over the SFT baseline, orange degradations. All datasets are downsampled to 60,908 examples (except ChatArena, which is made up of 20,465 responses). PPO outperforms DPO by an average of 0.7 points, where most improvements are in reasoning, coding, and chat capabilities.**

### Preference Learning Algorithm: DPO vs. PPO

We now compare algorithms for learning from preferences, comparing the performance of DPO and PPO when the same base models and data are used (Table 2). We use **exactly the same data** for training DPO and PPO models,5 and subsample larger datasets to 60,908 examples due to computational resources and only use these subsampled datasets during reward model and PPO training. For dataset choice, we use the top-performing dataset from each source type in Table 1 (StackExchange from Web, HH-RLHF from human, Ultrafeedback from synthetic). We also include the second-best performing dataset overall (Nectar) and an additional human-annotated dataset from a popular evaluation platform (Chatbot Arena 2023). Results in Table 2 indicate that:

**PPO outperforms DPO.** Across all datasets, models trained with PPO outperform models trained with DPO. In fact, PPO is able to provide improvements over the SFT model in cases where DPO training did not, such as when using StackExchange. On average, PPO significantly6 improves over DPO performance.

**PPO improves on DPO in reasoning, coding and safety capabilities most.** PPO improves over DPO by an average of 1.3, 2.9, and 2.3 points for reasoning, coding, and safety respectively, while truthfulness tends to degrade by an average of 2.5 points. Instruction following and factuality remain largely the same. Interestingly, we find that models trained with PPO are far more likely than DPO-trained models to perform chain-of-thought reasoning when prompted with reasoning or math problems, even when not given in-context examples using chain-of-thought. This suggests that reasoning improvements from PPO may be due to increased chain-of-thought abilities. Additionally, while overall instruction following ability remains similar, we find that PPO-trained models tend to perform better at AlpacaEval 1 and 2, with PPO-trained models outperforming DPO-trained ones on AlpacaEval 2 by an average of 3.4 points.

### Reward Models

Next, we focus on PPO and study reward models both directly, and on downstream tasks with PPO training (Table 3). We first consider two ways to improve a reward model:

1. **Scaling up the training data for the reward model.** We construct a data mixture of the top-performing datasets in Table 1 from each section: UltraFeedback, HelpSteer, Nectar, StackExchange, HH-RLHF, and additionally add PRM800k for math data. We compare reward models trained on this data mixture (called **Mix RM**) with reward models trained only on UltraFeedback (**UltraF. RM**) - the top-performing dataset from prior sections.
2. **Scaling up the reward model size.** We train reward models at two scales of 13B and 70B starting from Tulu 2.

    &  &  \\   & **RewardBench** & **Best-of-N over SFT** & **GSM** & **AlpacaEval2** & **Avg. on** \\  & **Score** & **Avg. Perf. (\(\))** & **Acc.** & **winrate** & **All Evals.** \\ 
13B UltraF. RM & 61.0 & 56.9 (+5.8) & 53.0 & 26.1 & 62.2 \\
13B Mix RM & **79.8** & 58.3 (+7.3) & 51.0 & 25.7 & 61.6 \\
70B UltraF. RM & 73.6 & **61.1 (+10.3)** & **58.0** & 26.7 & **62.8** \\
70B Mix RM & 73.9 & 60.6 (+9.5) & 51.5 & **31.6** & 61.8 \\   

Table 3: **Reward model evaluation:** (a) **Direct evaluation:** reward models when directly evaluated using RewardBench  and Best-of-N (left two columns); for BoN, we report both raw average performance, and the difference in performance over the base SFT model in brackets. (b) **Downstream evaluation:** models trained using PPO and the given reward model (right three columns). We report GSM and AlpacaEval 2 performance along with average performance across the entire evaluation suite defined in §2.2. Directly comparing reward models indicate increasing scale and data improves reward models, but these only minimally impact downstream performance.

**Direct evaluation of reward models.** To isolate the performance of the differing reward models, we first evaluate them with best-of-N (BoN): we sample 16 generations from Tulu 2 13B SFT, score them using the given reward model, and then use the top-scoring output as the model output. Notably, we ensure model outputs are identical between runs, meaning that the only difference is the reward model scores. We perform evaluation on the subset of evaluations in our suite that require long-form generation,7 and report overall average performance. We additionally evaluate our reward models on RewardBench , a popular evaluation for reward models, which involves evaluating if the relative scores given by reward models match a test set of chosen-rejected pairs from diverse sources. We provide further details in Appendix H.

Results in Table 3 indicate that **either increasing the reward model dataset ('Mix') or reward model size (from 13B to 70B) improves direct RM performance**. Surprisingly, we find that our 70B reward models perform best on BoN evaluation, while the 13B mixture model performs best on RewardBench. Both evaluations show that increasing the dataset mixture and increasing the dataset size can help, although increasing the dataset mixture is not as useful for further improving the 70B reward model. Although scaling model size helps with best-of-N, it does not improve RewardBench performance. Examining the per-split performance on RewardBench in App. H Table 13, the largest gaps between the 13B Mix RM and the 70B Mix RM is in reasoning (mostly code), suggesting that the larger model may not benefit much from the additional (somewhat noisy) data8.

**Downstream evaluation of reward models.** We then test if our improved reward models lead to improved downstream policy models when used during PPO training. We perform PPO training using the UltraFeedback prompts during training and report our results on the right side of Table 3. Surprisingly, we find that **improvements in reward models result in surprisingly small improvements in overall downstream performance**. We see the largest (and only) overall improvement when using the 70B UltraFeedback RM, despite the fact that all improved RMs performed significantly better than the 13B UltraFeedback RM in RewardBench and Best-of-N evaluations. Additionally, the improvement from the 70B RM is largely driven by a large performance jump in GSM, as shown in Table 3, while other metrics stay largely similar. This suggests that it is difficult to translate improvements in reward models to the underlying policy. We note that most prior work examining reward models tends to examine either direct reward model performance [26; 61] or proxy reward [17; 40], rather than downstream performance. **Our findings suggest that improving such evaluations may not necessarily translate to improved downstream performance.** Additionally, we find that using the larger 70B UltraF. RM is less sensitive and performs well with lower KL penalty coefficient values than using the 13B UltraF. RM, and further examine the effect of the KL penalty coefficient on performance in App. I.

While it may seem unintuitive that an extreme increase in reward model size does not lead to extreme improvements in performance, we note that prior work has similarly noted that much larger reward models do not necessarily lead to significant improvements in performance, and smaller reward models can lead to similar performance (Ouyang et al.  SSC.2, Wang et al.  SS4.3), although we are the first, to our knowledge, to ablate this and explicitly report results on downstream evaluations.

We additionally explore how further training and potentially overoptimizing against the reward model affects performance across different evaluations in App. L. Importantly, we find that different evaluations show different trends - while some evaluations such as AlpacaEval benefit from continued training, other evaluations such as IFEval or GSM8k drop with continued training. This highlights the importance of evaluating over a diverse set of test tasks, including both 'traditional' benchmarks and LLM-as-a-judge evaluations.

### Policy Training Prompts

We finally examine the effect of using varied policy training prompts, first when targeting a particular capability (math performance as evaluated by GSM), and then for improving overall performance. This is in contrast to prior work that just re-use the prompts used for reward model training  or a generic pool of anticipated user queries .

#### 3.4.1 The effect of targeted prompts.

We first examine the effect of using policy training prompts targeted towards a particular downstream setting--specifically, math capabilities as evaluated by GSM. We construct three different prompt sets: prompts sourced from the GSM train set, math-related prompts from varied datasets, and 20K random prompts from UltraFeedback9. We further detail the approach used to identify math-related prompts in Appendix J. We then train models using PPO with each reward model from the previous section on each prompt set. Results in Fig. 3 demonstrate that:

**Larger reward models perform better when closely matching train prompts to test settings**. When using prompts from the GSM train set, we find using either 70B reward models during training leads to significant improvements in GSM, with our best performing model improving over the SFT model by 16 points (46%\(\)62%).

**Weaker reward models struggle to make use of prompts differing from those they are trained on.** Surprisingly, we find that training with the 13B UltraFeedback reward model actually performs _worse_ when using prompts apart from UltraFeedback, potentially due to the reward model generalising poorly to prompts not seen during training. Similarly, the 13B Mix and 70B UltraFeedback reward models struggle to make use of math prompts, which are out-of-domain for the reward models, but in-domain for the task.

Overall, these results suggest that **targeted prompt distributions can improve performance when coupled with powerful reward models** (e.g., using prompts from the GSM train set and then evaluating on GSM). This highlights a strength of PPO: **it can make use of unlabelled prompts to better target downstream tasks**.

#### 3.4.2 Altering prompts to improve overall performance.

Inspired by the success of the targeted prompts, we construct a new remixed prompt set by finding 20K math and 20K code-related prompts using the same method as in the previous subsection (see App. J). We then combine the math, code, and UltraFeedback prompts to create a larger prompt pool that we downsample randomly to 60,908 prompts. This rebalances the prompt pool to focus more on coding and math tasks, which we hope yields improvements on math and code respectively while maintaining overall performance. We present our results in Table 4.

Surprisingly, we observe that **using mixed prompts does not seem to improve performance in the generalist setting**. When looking at code and math results specifically, we do not see consistent

  
**Reward Model** & **Prompts** & **GSM \%** & **Coding** & **Avg. Across** & **All Evals** \\  
13B UltraF & - & 46.0 & 45.1 & 56.8 \\ 
13B UltraF & UF & 53.0 & 47.7 & **62.2** \\
13B UltraF & Mixed & **54.5** & **47.8** & 61.9 \\ 
13B Mix & UF & **51.0** & **46.8** & **61.6** \\
13B Mix & Mixed & 50.5 & 43.8 & 60.9 \\ 
70B UltraF & UF & **58.0** & 47.3 & **62.8** \\
70B UltraF & Mixed & 56.5 & **48.4** & 62.4 \\ 
70B Mix & UF & 51.5 & **46.1** & **61.8** \\
70B Mix & Mixed & **52.0** & 44.9 & 61.1 \\   

Table 4: **Policy training prompt overall evaluation:** Performance of PPO policy models trained with the given reward models on 60K prompts from either UltraFeedback or the remixed prompt set that adds additional unlabeled math and coding-related prompts. Using the remixed prompt set does not improve performance, either on specific evaluations (math, code) or in terms of overall performance.

Figure 3: **Policy training prompt math evaluation:** Performance of models trained on 20K prompts from varying sources using PPO and evaluated on GSM. Training with larger RMs trained on more data benefits more from in-domain prompts (i.e., prompts directly from the GSM train set), while weaker RMs struggle to generalize beyond their training prompts.

improvements using the altered prompt mixture, and our overall performance tends to drop. This is likely due to the already diverse nature of UltraFeedback, such that when looking at the whole dataset (i.e., not just the 20K subset in Figure 3), we are able to reach strong performance on math and coding evaluations. Altering the distribution away from other tasks slightly hurts the overall performance. We additionally found that training all the mined prompts in additional to all of UltraFeedback (i.e., not downsampling the combined prompt set) did not yield further improvements over the results shown in Table 4.

## 4 A Recipe for Learning from Preferences

Putting together all our findings from previous sections, we suggest a recipe for training a strong model using learning from preferences, as shown in Figure 1 and in Table 5. We take a **high-quality, synthetic preference dataset**, a **large reward model**,10 and train it using **PPO**. If we additionally wish to focus on a specific domain, we can additionally collect **domain-specific prompts for policy training**. We find that our best model (Tulu 2+PPO 13B trained with the 70B UltraF. RM) outperforms our best DPO model and other popular models based off Llama 2 13B, including Llama 2 Chat, which has also undergone SFT and PPO training. Additionally, incorporating task-specific prompts into policy training may further improve performance when the prompts align closely with downstream evaluations, as shown in Figure 3. Finally, we also experiment with Llama 3.0 8B , finetuning on the Tulu2 Mix, and then training it using DPO and PPO with the same hyperparameters. We find that overall performance is significantly improved, and we observe similar trends as with our other experiments (DPO performing better than PPO, using a larger reward model improving performance, using mixed prompts not improving performance).

## 5 Related Work

**Learning from Preferences for LMs.**

Initial approaches to learning from preferences used reinforcement learning from human feedback (RLHF) [8; 67], a method that first trains a reward model to capture human preferences and then optimizes against it using reinforcement learning algorithms such as PPO . Recent work has additionally questioned the need for PPO, and has found that similar but simpler approaches such as REINFORCE  with LM-specific adjustments work well [1; 50]. Unlike prior work, we instead focus on examining the effect of varying the **data and models** used in PPO (i.e., the reward model, preference data, initial policy model, and prompts used for sampling outputs). We believe that our results should transfer to similar approaches such as REINFORCE, since they share many

**Model** & **Factuality** & **Reasoning** & **Coding** & **Truthfulness** & **Safety** & **Instr. Poll.** & **Average** \\  Llama 2 13B Base & 52.0 & 37.0 & 30.7 & 32.7 & 32.7 & - & - \\ Llama 2 Chat 13B  & 53.2 & 24.7 & 36.9 & **88.0** & 91.9 & 51.2 & 57.7 \\ Nous Hermes 13B  & 53.2 & 43.5 & 47.7 & 80.5 & 43.9 & 38.7 & 51.3 \\ Vicuna I.5 13B  & 54.5 & 39.3 & 38.5 & 62.8 & 92.4 & 45.8 & 55.6 \\  Tulu 2 13B SFT & 55.4 & 47.8 & 45.1 & 56.6 & 91.8 & 44.2 & 56.8 \\ Tulu 2+DPO 13B & 55.3 & 50.9 & 45.9 & 69.3 & 91.9 & 52.8 & 61.0 \\ Tulu 2+PPO 13B (13B UFRM) & 56.0 & 52.0 & 47.7 & 71.5 & 91.8 & 54.4 & 62.2 \\ Tulu 2+PPO 13B (70B UFRM) & 55.4 & 53.9 & 47.3 & 72.3 & 91.9 & 55.8 & 62.8 \\ Tulu 2+PPO 13B (70B UFRM+MP) & 55.3 & 53.1 & 48.4 & 71.0 & **92.7** & 54.0 & 62.4 \\ 
13+Tulu 2 8B SFT & 58.0 & 58.6 & 56.4 & 59.2 & 92.8 & 42.6 & 61.3 \\
13+Tulu 2+PPO 8B (8B UFRM) & 59.4 & 56.2 & 55.6 & 71.4 & 91.7 & 50.4 & 64.1 \\
13+Tulu 2+PPO 8B (8B UFRM) & **59.5** & 57.0 & **55.9** & 69.6 & 91.4 & **56.0** & 64.9 \\
13+Tulu 2+PPO 8B (70B UFRM) & 58.5 & **60.8** & 55.0 & 72.8 & 91.8 & 55.8 & **65.8** \\
13+Tulu 2+PPO 8B (70B UFRM+MP) & 58.3 & 40.6 & 48.2 & 62.4 & 91.0 & 53.0 & 58.9 \\  

Table 5: **Putting together a recipe for preference-based learning:** Performance of our best-performing models along with popular open models based on Llama 2 13B. ‘MP’ refers to using the mixed prompt set described in §4. ‘L3’ stands for experiments using Llama 3 as a base model. Using PPO with a large reward model performs best overall.

commonalities with PPO (e.g., reliance on a well-tuned reward model and an unlabelled prompt set for eliciting generations during training).

Another line of recent work has also attempted to simplify the PPO algorithm and remove the online generation component, with the most popular algorithm following this approach being DPO . DPO's ease of implementation and use has made it widely popular among open-source community. Notably, several high-performing models have been trained using DPO for learning from preferences, including Tulu 2 , Zephyr , and Mixstral-Instruct . Much recent work [3, 21, 59, 16, _inter alia_] has attempted to further improve the DPO algorithm. However, comparisons of these approaches so far have found that they largely perform similarly . As such, in this work we focus on the most popular variant, DPO, and examine what data works best for it and how it compares to a popular online RL approach, PPO.

**Comparing Approaches for Learning from Preferences.** Recent concurrent work has compared the properties and performance of DPO, PPO, and other approaches for learning from preference feedback. Xu et al.  suggest DPO performs poorly when using data out-of-distribution from the initial base model, while PPO (and a semi-online DPO variant) perform better both in such cases and overall when evaluated on safety and code capabilities. However, they do not investigate the impact of reward models and focus on core algorithmic details of PPO that lead to improvements. Tajwar et al.  identify on-policy sampling and negative gradients as two important aspects of preference-based learning when optimal reward values do not lie 'close' to the base model's distribution and the preference data is skewed. Tang et al.  study how the IPO  algorithm performs in the static offline setting versus various ways of updating or ordering the data in an online manner. In this work, we focus on empirically examining the impact of core aspects of learning from preference feedback, including the effects of varied rewards and data.

## 6 Conclusion

In this work, we have systematically explored the core components of learning from preference feedback and examined the relative impacts of each in turn on model performance across a wide range of evaluations. Our results suggest the following ordering of importance: preference data quality, algorithm choice, reward model quality, and finally targeted policy training prompts. Additionally, we find that using larger reward models can significantly improve math capabilities, but have marginal effects on other capabilities we evaluate in this work. Overall, we suggest a recipe for learning from preference feedback with currently available resources: best performance can be achieved by using a strong synthetic dataset (UltraFeedback), and using PPO training with a large reward model. Our work suggests that further exploring how to make better use of improved reward models is an important direction for further improving PPO-style approaches to learning from preference data. We plan to release models and code related to this paper and hope that our settings provide a firm base for future work further exploring learning from preferences for language models.