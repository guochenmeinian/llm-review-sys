# The Behavior and Convergence of

Local Bayesian Optimization

 Kaiwen Wu

University of Pennsylvania

kaiwennu@seas.upenn.edu

&Kyurae Kim

University of Pennsylvania

kyrkim@seas.upenn.edu

&Roman Garnett

Washington University in St. Louis

garnett@wustl.edu

&Jacob R. Gardner

University of Pennsylvania

jacobrg@seas.upenn.edu

###### Abstract

A recent development in Bayesian optimization is the use of local optimization strategies, which can deliver strong empirical performance on high-dimensional problems compared to traditional global strategies. The "folk wisdom" in the literature is that the focus on local optimization sidesteps the curse of dimensionality; however, little is known concretely about the expected behavior or convergence of Bayesian local optimization routines. We first study the behavior of the local approach, and find that the statistics of individual local solutions of Gaussian process sample paths are surprisingly good compared to what we would expect to recover from global methods. We then present the first rigorous analysis of such a Bayesian local optimization algorithm recently proposed by Muller et al. (2021), and derive convergence rates in both the noiseless and noisy settings.

## 1 Introduction

Bayesian optimization (BO)  is among the most well studied and successful algorithms for solving black-box optimization problems, with wide ranging applications from hyperparameter tuning , reinforcement learning , to recent applications in small molecule  and sequence design . Despite its success, the curse of dimensionality has been a long-standing challenge for the framework. Cumulative regret for BO scales exponentially with the dimensionality of the search space, unless strong assumptions like additive structure  are met. Empirically, BO has historically struggled on optimization problems with more than a handful of dimensions.

Recently, local approaches to Bayesian optimization have empirically shown great promise in addressing high-dimensional optimization problems . The approach is intuitively appealing: a typical claim is that a local optimum can be found relatively quickly, and exponential sample complexity is only necessary if one wishes to enumerate all local optima. However, these intuitions have not been fully formalized. While global Bayesian optimization is well studied under controlled assumptions , little is known about the behavior or convergence properties of local Bayesian optimization in these same settings -- the related literature focuses mostly on strong empirical results on real-world applications.

In this paper, we begin to close this gap by investigating the behavior of and proving convergence rates for local BO under the same well-studied assumptions commonly used to analyze global BO. We divide our study of the properties of local BO in to two questions:

* Using common assumptions (_e.g._, when the objective function is a sample path from a GP with known hyperparameters), how good are the local solutions that local BO converges to?* Under these same assumptions, is there a local Bayesian optimization procedure that is guaranteed to find a local solution, and how quickly can it do so?

For **Q1**, we find empirically that the behavior of local BO on "typical" high dimensional Gaussian process sample paths is surprising: even a single run of local BO, without random restarts, results in shockingly good objective values in high dimension. Here, we may characterize "surprise" by the minimum equivalent grid search size necessary to achieve this value in expectation. See Figure 1 and more discussions in SS3.

For **Q2**, we present the first theoretical convergence analysis of a local BO algorithm, \(\) as presented in  with slight modifications. Our results bring in to focus a number of natural questions about local Bayesian optimization, such as the impact of noise and the increasing challenges of optimization as local BO approaches a stationary point.

We summarize our contributions as follows:

1. We provide upper bounds on the uncertainty of the gradient when actively learned using the \(\) procedure described in  in both the noiseless and noisy settings.
2. By relating these uncertainty bounds to the bias in the gradient estimate used at each step, we translate these results into convergence rates for both noiseless and noisy functions.
3. We investigate the looseness in our bound by studying the empirical reduction in uncertainty achieved by using the \(\) policy and comparing it to our upper bound.
4. We empirically study the quality of individual local solutions found by a local Bayesian optimization, a slightly modified version of \(\), on GP sample paths.

## 2 Background and Related Work

Bayesian optimization.Bayesian optimization (BO) considers minimizing a black-box function \(f\):

\[*{minimize}_{}f()\]

through potentially noisy function queries \(y=f()+\). When relevant to our discussion, we will assume an _i.i.d_. additive Gaussian noise model: \((0,^{2})\). To decide which inputs to

Figure 1: The unreasonable effectiveness of locally optimizing GP sample paths. **(Top row)**: Distributions of local solutions found when locally optimizing GP sample paths in various numbers of dimensions, with varying amounts of noise. **(Bottom left):** The minimum sample complexity of grid search required to achieve the median value found by \(\) (\(=0\)) in expectation. **(Bottom middle, right):** The performance of global optimization algorithms \(\)-\(\) and random search in this setting. See ยง3 for details.

be evaluated, Bayesian optimization estimates \(f\) via a _surrogate model,_ which in turn informs a _policy_ (usually realized by maximizing an _acquisition function_\(()\) over the input space) to identify evaluation candidates. Selected candidates are then evaluated, and the surrogate model is updated with this new data, allowing the process to repeat with the updated surrogate model.

Gaussian processes.Gaussian processes (GP) are the most commonly used surrogate model class in BO. A Gaussian process \((,k)\) is fully specified by a mean function \(\) and a covariance function \(k\). For a finite collection of inputs \(\), the GP induces a joint Gaussian belief about the function values, \(f()((),k(,))\). Standard rules about conditioning Gaussians can then be used to condition the GP on a dataset \(\), resulting in an updated posterior process reflecting the information contained in these observations:

\[f_{}(_{},k_{}).\] (1)

Existing bounds for global BO.Srinivas et al.  proved the first sublinear cumulative regret bounds of a global BO algorithm, \(\)-\(\), in the noisy setting. Their bounds have an exponential dependency on the dimension, a result that is generally unimproved without assumptions on the function structure like an additive decomposition . This exponential dependence can be regarded as a consequence of the curse of dimensionality. In one dimension, Scarlett  has characterized the optimal regret bound (up to a logarithmic factor), and \(\)-\(\) is near-optimal in the case of the RBF kernel. In the noiseless setting, improved convergence rates have been developed under additional assumptions. For example, De Freitas et al.  proved exponential convergence by assuming \(f\) is locally quadratic in a neighborhood around the global optimum, and Kawaguchi et al.  proved an exponential simple regret bound under an additional assumption that the optimality gap \(f()-f^{}\) is bounded by a semi-metric.

Gaussian process derivatives.A key fact about Gaussian process models that we will use is that they naturally give rise to gradient estimation. If \(f\) is Gaussian process distributed, \(f(,k)\), and \(k\) is differentiable, then the gradient process \( f\) also is also (jointly) distributed as a GP. Noisy observations \(\) at arbitrary locations \(\) and the gradient measured at an arbitrary location \(\) are jointly distributed as:

\[\\  f()( )\\ (),k(,)+ ^{2}&k(,)^{}\\  k(,)& k(,)^{} .\]

This property allows probabilistic modeling of the gradient given noisy function observations. The gradient process conditioned on observations \(\) is distributed as a Gaussian process:

\[ f(_{}, k_{ }^{}).\]

## 3 How Good Are Local Solutions?

Several local Bayesian optimization algorithms have been proposed recently as an alternative to global Bayesian optimization due to their favourable empirical sample efficiency . The common idea is to utilize the surrogate model to iteratively search for _local_ improvements around some current input \(_{t}\) at iteration \(t\). For example, Eriksson et al.  centers a hyper-rectangular trust region on \(_{t}\) (typically taken to be the best input evaluated so far), and searches locally within the trust region. Other methods like those of Muller et al.  or Nguyen et al.  improve the solution \(_{t}\) locally by attempting to identify descent directions \(_{t}\) so that \(_{t}\) can be updated as \(_{t+1}=_{t}+_{t}_{t}\).

Before choosing a particular local BO method to study the convergence of formally, we begin by motivating the high level philosophy of local BO by investigating the quality of individual local solutions in a controlled setting. Specifically, we study the local solutions of functions \(f\) drawn from Gaussian processes with known hyperparameters. In this setting, Gaussian process sample paths can be drawn as differentiable functions adapting the techniques described in Wilson et al. . Note that this is substantially different from the experiment originally conducted in Muller et al. , where they condition a Gaussian process on \(1000\) examples and use the posterior mean as an objective.

To get a sense of scale for the optimum, we first analyze roughly how well one might expect a simple grid search baseline to do with varying sample budgets. A well known fact about the order statistics of Gaussian random variables is the following:

**Remark 1** (_e_.\(g\)., ).: _Let \(X_{1},X_{2},,X_{n}\) be (possibly correlated) Gaussian random variables with marginal variance \(s^{2}\), and let \(Y=\{X_{i}\}\). Then the expected maximum is bounded by_

\[[Y] s.\]

This result directly implies an upper bound on the expected maximum (or minimum by symmetry) of a grid search with \(n\) samples. The logarithmic bound results in an optimistic estimate of the expected maximum (and the minimum): as \(n\) the bound goes to infinity as well, whereas the maximum (and the minimum) of a GP is almost surely finite due to the correlation in the covariance.

With this, we now turn to evaluating the quality of local solutions. To do this, we optimize \(50\) sample paths starting from \(=\) from a centered Gaussian process with a RBF kernel (unit outputscale and unit lengthscale) using a variety of observation noise standard deviations \(\). We then run iterations of local Bayesian optimization (as described later in Algorithm 1) to convergence or until an evaluation budget of 5000 is reached. In the noiseless setting (\(=0\)), we modify Algorithm 1 to pass our gradient estimates to BFGS rather than applying the standard gradient update rule for efficiency.

We repeat this procedure for \(d\{1,5,10,20,30,50\}\) dimensions and \(\{0,0.05,0.2\}\) and display results in Figure 1. For each dimension, we plot the distribution of single local solutions found from each of the 50 trials as a box plot. In the noiseless setting, we find that by \(d=50\), a single run of local optimization (_i_.\(e\)., without random restarts) is able to find a median objective value of \(-12.9\), corresponding to a grid size of at least \(n=10^{36}\) to achieve this value in expectation!

For completeness, we provide results of running GP-UCB and random search also with budgets of 5000 and confirm the effectiveness of local optimization in high dimensions.

## 4 A Local Bayesian Optimization Algorithm

The study above suggests that, under assumptions commonly used to theoretically study the performance of Bayesian optimization, finding even a single local solution is surprisingly effective. The natural next question is whether and how fast we can find them. To understand the convergence of local BO, we describe a prototypical local Bayesian optimization algorithm that we will analyze in Algorithm 1, which is nearly identical to GIBO as described in  with one exception: Algorithm 1 allows varying batch sizes \(b_{t}\) in different iterations.

Define \(k_{}(_{t},_{t})=k_{} (_{t},_{t})-k_{}(_{t},)(k_ {}(,)+^{2})^{-1}k_{}(,_{t})\). Namely, \(k_{}(,)\) is the posterior covariance function conditioned on the data \(\) as well as new inputs \(\). Crucially, observe that the posterior covariance of a GP does not depends on the labels \(\), hence the compact notation \(k_{}\). The acquisition function \(_{}\) of GIBO is defined as

\[_{}(,)= k_{ }(_{t},_{t})^{},\] (2)

which is the trace of the posterior gradient covariance conditioned on the union of the training data \(\) and candidate designs \(\).

The posterior gradient covariance \( k_{}(_{t},_{t})^{}\) quantifies the uncertainty in the gradient \( f(_{t})\). By minimizing the acquisition function \(_{}(_{t},)\) over \(\), we actively search for designs \(\) that minimizes the one-step look-ahead uncertainty, where the uncertainty is measured by the trace of the posterior covariance matrix. After selecting a set of locations \(\), the algorithm queries the (noisy) function values at \(\), and updates the dataset. The algorithm then follows the (negative) expected gradient \(_{_{t}}(_{t})\) to improve the current iterate \(_{t}\), and the process repeats.

Convergence Results

In light of the strong motivation we now have for local Bayesian optimization, we will now establish convergence guarantees for the simple local Bayesian optimization routine outlined in the previous section, in both the noiseless and noisy setting. We first pause to establish some notations and a few assumptions that will be used throughout our results.

**Notation.** Let \(^{d}\) be a convex compact domain (_e.g._, \(=^{d}\)). Let \(\) be a reproducing kernel Hilbert space (RKHS) on \(\) equipped with a reproducing kernel \(k(,)\). We use \(\|\|\) to denote the Euclidean norm and \(\|\|_{}\) to denote the RKHS norm. The minimum of a continuous function \(f\) defined on a compact domain \(\) is denoted as \(f^{*}=_{}f()\).

In our convergence results, we make the following assumption about the kernel \(k\). Many commonly used kernels satisfy this assumption, _e.g._, the RBF kernel and Matern kernel with \(>2\).

**Assumption 1**.: _The kernel \(k\) is stationary, four times continuously differentiable and positive definite._

The following is a technical assumption that simplifies proofs and convergence rates.

**Assumption 2**.: _The iterates \(\{_{t}\}_{t=1}^{}\) stay in the interior of the domain \(\)._

Intuitively, with the domain \(\) large enough and an appropriate initialization, the iterates most likely always stay in the interior. In the case when the iterate \(_{t+1}\) slides out of the domain \(\), we can add a projection operator to project it back to \(\), which will be discussed in SS5.2.

We also define a particular (standard) smoothness measure that will be useful for proving convergence.

**Definition 1** (Smoothness).: _A function \(f\) is \(L\)-smooth if and only if for all \(_{1},_{2}\), we have:_

\[\| f(_{1})- f(_{2})\| L\|_{1}- _{2}\|.\]

Next, we define the notion of an _error function_, which quantifies the maximum achievable reduction in uncertainty about the gradient at \(=0\) using a set of \(b\) points \(\) and no other data, _i.e._, the data \(\) is an empty set in the acquisition function (2).

**Definition 2** (Error function).: _Given input dimensionality \(d\), kernel \(k\) and noise standard deviation \(\), we define the following error function:_

\[E_{d,k,}(b)=_{^{b d}}  k(,)^{}- k(,) (k(,)+^{2})^{-1}k(,) ^{}.\] (3)

### Convergence in the Noiseless Setting

We first prove the convergence of Algorithm 1 with noiseless observations, _i.e._, \(=0\). Where necessary, the inverse kernel matrix \(k_{}(,)^{-1}\) should be interpreted as the pseudoinverse. In this section, we will assume the ground truth function \(f\) is a function in the RKHS with bounded norm. This assumption is standard in the literature (_e.g._, 3, 24), and the results presented here ultimately extend trivially to the other assumption commonly made (that \(f\) is a GP sample path).

**Assumption 3**.: _The ground truth function \(f\) is in \(\) with bounded norm \(\|f\|_{} B\)._

Because \(k\) is four times continuously differentiable, \(f\) is twice continuously differentiable. Furthermore, on a compact domain \(\), \(f\) is guaranteed to be \(L\)-smooth for some constant \(L\) (see Lemma 9 for a complete explanation). To prove the convergence of Algorithm 1, we will show the posterior mean gradient \(_{}\) approximates the ground truth gradient \( f\). By using techniques in meshless data approximation (_e.g._, 28, 5), we prove the following error bound, which precisely relates the gradient approximation error and the posterior covariance trace.

**Lemma 1**.: _For any \(f\), any \(\) and any \(\), we have the following inequality_

\[\| f()-_{}()\|^{2} k_{}(,)^{}\|f \|_{}^{2}.\] (4)

Since \(f\) has bounded norm \(\|f\|_{} B\), the right hand side of (4) is a multiple of the posterior covariance trace, which resembles the acquisition function (2). Indeed, the acquisition function (2) can be interpreted as minimizing the one-step look-ahead worst-case gradient estimation error in the RKHS \(\), which justifies GIBO from a different perspective.

Next, we provide a characterization of the error function \(E_{d,k,0}\) under the noiseless assumption \(=0\). This characterization will be useful later to express the convergence rate.

**Lemma 2**.: _For \(=0\), the error function is bounded by \(E_{d,k,0}(b) C\{0,1+d-b\}\), where \(C=_{1 i d}}{ x_{i} x_{i}^{2}}k( ,)\) is the maximum of the Hessian's diagonal entries at the origin._

Now we are ready to present the convergence rate of Algorithm 1 under noiseless observations.

**Theorem 1**.: _Let \(f\) whose smoothness constant is \(L\). Running Algorithm 1 with constant batch size \(b_{t}=b\) and step size \(_{t}=\) for \(T\) iterations outputs a sequence satisfying_

\[_{1 t T} f(_{t})^{2} 2L(f(_{1})-f^{*})+B^{2} E_{d,k,0}(b).\] (5)

As \(T\), the first term in the right hand side of (5) decays to zero, but the second term may not. Thus, with constant batch size \(b\), Algorithm 1 converges to a region where the squared gradient norm is upper bounded by \(B^{2}E_{d,k,0}(b)\) with convergence speed \(()\). An important special case occurs when the batch size \(b_{t}=d+1\) in every iteration. In this case, \(E_{d,k,0}(b)=0\) by Lemma 2 and the algorithm converges to a stationary point with rate \(()\).

**Corollary 1**.: _Under the same assumptions of Theorem 1, using batch size \(b_{t}=d+1\), we have_

\[_{1 t T} f(_{t})^{2} 2L(f(_{1})-f^{*}).\]

_Therefore, the total number of samples \(n=(dT)\) and the squared gradient norm \( f(_{t})^{2}\) converges to zero at the rate \((d/n)\)._

We highlight that the rate is linear with respect to the input dimension \(d\). Thus, as expected, local Bayesian optimization finds a stationary point in the noiseless setting with significantly better non-asymptotic rates in \(d\) than global Bayesian optimization finds the global optimum. Of course, the fast convergence rate of Algorithm 1 is at the notable cost of not finding the global minimum. However, as Figure 1 demonstrates and as discussed in SS3, a single stationary point may already do very well under the assumptions used in both analyses.

We note that the results in this section can be extended to the GP sample path assumption as well. Due to space constraint, we defer the result to Theorem 4 in the appendix.

### Convergence in the Noisy Setting

We now turn our attention to the more challenging and interesting setting where the observation noise \(>0\). We analyze convergence under the following GP sample path assumption, which is commonly used in the literature [_e.g._, 24, 6]:

**Assumption 4**.: _The objective function \(f\) is a sample from a GP, \(f(,k)\) with known mean function, covariance function, and hyperparameters. Observations of the objective function \(y\) are made with added Gaussian noise of known variance, \(y=f()+\), where \((0,^{2})\)._

Since the kernel \(k\) is four times continuously differentiable, \(f\) is almost surely smooth.

**Lemma 3**.: _For \(0<<1\), there exists a constant \(L>0\) such that \(f\) is \(L\)-smooth w.p. at least \(1-\)._

Challenges.The observation noise introduces two important challenges. First, by the GP sample path assumption, we have:

\[ f(_{t})(_{_{t}}(_{t}), k_{_{t}}(_{t},_{t})^{}).\]

Thus, the posterior mean gradient \(_{_{t}}(_{t})\) is an approximation of the ground truth gradient \( f(_{t})\), where the approximation error is quantified by the posterior covariance \( k_{_{t}}(_{t},_{t})^{}\). For any \(f\), we emphasize that the posterior mean gradient \(_{_{t}}(_{t})\) is therefore _biased_ whenever the posterior covariance is nonzero. Unfortunately, because of noise \(>0\), this is always true for any finite data \(_{t}\). Thus, the standard analysis of stochastic gradient descent does not apply, as it typically assumes the stochastic gradient is _unbiased_.

The second challenge is that the noise directly makes numerical gradient estimation difficult. To build intuition, consider a finite differencing rule that approximates the partial derivative

\[}=f(+h_{i })-f(-h_{i})+(h^{2})\]

where \(_{i}\) is the \(i\)-th standard unit vector. In order to reduce the \((h^{2})\) gradient estimation error, we need \(h 0\). However, the same estimator under the noisy setting

\[}f( +h_{i})+_{1}-f(-h_{i} )+_{2}+(h^{2}),\]

where \(_{1},_{2}\) are _i.i.d._ Gaussians, diverges to infinity when \(h 0\). Note that the above estimator is biased even with repeated function evaluations because \(h\) cannot go to zero.

We first present a general convergence rate for Algorithm 1 that bounds the gradient norm.

**Theorem 2**.: _For \(0<<1\), suppose \(f\) is a GP sample whose smoothness constant is \(L\) w.p. at least \(1-\). Algorithm 1 with batch size \(b_{t}\) and step size \(_{t}=\) produces a sequence satisfying_

\[_{1 t T} f(_{t})^{2} 2L(f(_{1})-f^{*})+_{t=1}^{T}C_{t}E_{d,k, }(b_{t})\] (6)

_with probability at least \(1-2\), where \(C_{t}=2(^{2}/6)(t^{2}/)\)._

The second term \(_{t=1}^{T}C_{t}E_{d,k,}(b_{t})\) in the right hand side of (6) is the average cumulative bias of the gradient. To finish the convergence analysis, we must further bound the error function \(E_{d,k,}(b_{t})\). For the RBF kernel, we obtain the following bound:

**Lemma 4** (RBF Kernel).: _Let \(k(_{1},_{2})=-_{ 1}-_{2}^{2}\) be the RBF kernel. We have_

\[E_{d,k,}(2md) d1+W-)} =( dm^{-}),\]

_where \(m\) and \(W\) denotes the principal branch of the Lambert \(W\) function._

The error function (3) is an infimum over all possible designs, which is intractable for analysis. Instead, we analyze the infimum over a subset of designs of particular patterns (based on finite differencing), which can be solved analytically, resulting the first inequality. The second equality is proved by further bounding the Lambert function by its Taylor expansion at \(-1/e\).

In addition, we obtain a similar bound for the Matern kernel with \(=\) by a slightly different proof.

**Lemma 5** (Matern Kernel).: _Let \(k(,)\) be the \(=2.5\) Matern kernel. Then, we have_

\[E_{d,k,}(2md) dm^{-}+^{}dm^{- }=( dm^{-}).\]

Interestingly, Lemma 4 and Lemma 5 end up with the same asymptotic rate. Writing the bound in terms of the batch size \(b\), we can see that \(E_{d,k,}(b)=( d^{}b^{-})\) for both the RBF kernel and the \(=2.5\) Matern kernel.1 Coupled with Theorem 2, the above lemmas translate into the following convergence rates, depending on the batch size \(b_{t}\) in each iteration:

**Corollary 2**.: _Let \(k(,)\) be either the RBF kernel or the \(=2.5\) Matern kernel. Under the same conditions as Theorem 2, if_

\[b_{t}=d^{2}t;\\ dt;\\ dt^{2},_{1 t T} f( _{t})^{2}=(1/T)+( d );\\  dT^{-} T=  d^{}n^{-} n;\\  dT^{-1}^{2}T= d^{ }n^{-}^{2}n,\]

_with probability at least \(1-2\). Here, \(T\) is the total number of iterations and \(n\) is the total number of samples queried._With nearly constant batch size \(b_{t}=d^{2}t\), Algorithm 1 converges to a region where the squared gradient norm is \(( d)\). With linearly increasing batch size, the algorithm converges to a stationary point with rate \(( d^{1.25}n^{-0.25} n)\), significantly slower than the \((d/n)\) rate in the noiseless setting. The quadratic batch size is nearly optimal up to a logarithm factor -- increasing the batch size further slows down the rate (see Appendix D for details).

To achieve convergence to a stationary point using Corollary 2, the batch size \(b_{t}\) must increase as optimization progresses. We note this may not be an artifact of our theory or our specific realization of local BO, but rather a general fact that is likely to be _implicitly_ true for any local BO routine. For example, a practical implementation of Algorithm 1 might use a constant batch size and a line search subroutine where the iterate \(_{t}\) is updated only when the (noisy) objective value decreases -- otherwise, the iterate \(_{t}\) does not change and the algorithm queries more candidates to reduce the bias in the gradient estimate. With this implementation, the batch size is increased repeatedly on any iterate while the line search condition fails. As the algorithm converges towards a stationary point, the norm of the ground-truth gradient \(\| f(_{t})\|\) decreases and thus requires more accurate gradient estimates. Therefore, the line search condition may fail more frequently with the constant batch size, and the effective batch size increases implicitly.

We provide two additional remarks on convergence in the noisy setting. First, the convergence rate is significantly slower than in the noiseless setting, highlighting the difficulty presented by noise. Second, when the noise is small, the convergence rate is faster. If \( 0\), the rate is dominated by a lower-order term in the big \(\) notation, recovering the \(()\) rate in the noiseless setting (see Appendix D for details). This is in sharp contrast with the existing analysis of BO algorithms. Existing convergence proofs in the noisy setting rely on analyzing the information gain, which is vacuous when \( 0\), requiring new tools. It is interesting that no new tools are required here.

Finally, we revisit Assumption 2. In the case when it does not hold, we use a modified update:

\[_{t+1}=_{}_{t}-_{t} _{_{t}}(_{t}),\] (7)

where the projection operator \(_{}()\) is defined as \(_{}()=_{ }\|-\|\), _i.e._, the closest feasible point to \(\). When the iterates stay in the interior of the domain \(\), the projection operator is an identity map and the update rule simply reduces to \(_{t+1}=_{t}-_{t}_{_{t}}(_{t})\).

Define the gradient mapping

\[G(_{t})=}_{t}-_{ }(_{t}-_{t} f(_{t})),\]

which is a generalization of the gradient in the constrained setting: when \(_{t}-_{t} f(_{t})\) lies in the interior of \(\), the gradient mapping reduces to the (usual) gradient \( f(_{t})\). The following gives convergence rates of \(\|G(_{t})\|\).

**Theorem 3**.: _Under the same conditions as Corollary 2, without Assumption 2, using the projected update rule (7), Algorithm 1 obtains the following rates:_

\[ b_{t}=dt;\\ dt^{2},_{1 t T}\|G(_{t}) \|^{2}= d^{}n^{-}  n+^{}d^{}n^{-} n;\\  d^{}n^{-}^{2}n+^{ {1}{2}}d^{}n^{-} n,\]

_with probability at least \(1-2\). Here, \(n\) is the total number of samples queried._

These rates are slower than Corollary 2. We defer more details to Appendix C.

## 6 Additional Experiments

In this section, we investigate numerically the bounds in our proofs and study situations where the assumptions are violated. We focus on analytical experiments, because the excellent empirical performance of local BO methods on high dimensional real world problems has been well established in prior work [_e.g._, 10, 19, 20, 27]. Detailed settings and additional experiments are available in Appendix E. The code is available at https://github.com/kayween/local-bo-convergence.

### How loose are our convergence rates?

This section investigates the tightness of the bounds on the error function \(E_{d,k,}(b)\) -- a key quantity in our convergence rates. We plot in Figure 2 the error function \(E_{d,k,}\) for the \(=2.5\) Maternkernel and our bound \(( d^{}b^{-})\) implied by Lemma 5. The error function is empirically estimated by minimizing the trace of posterior gradient covariance (3) using L-BFGS. Figure 1(a) uses a fixed dimension \(d=10\) and varying batch sizes, while Figure 1(b) uses a fixed batch size \(b=500\) and varying dimensions. Both plots are in log-log scale. Therefore, the slope corresponds to the exponents on \(b\) and \(d\) in the bound \(( d^{}b^{-})\), and the vertical intercept corresponds to the constant factor in the big \(\) notation.

Figure 1(a) shows that the actual decay rate of the error function \(E_{d,k,}(b)\) may be slightly faster than \((b^{-})\), as the slopes of the (empirical) error function have magnitude slightly larger than \(\). Interestingly, Figure 1(b) demonstrates that the dependency \(d^{}\) on the dimension is quite tight -- all lines in this plot share a similar slope magnitude.

### What is the effect of multiple restarts?

In SS3, we analyze local solutions found by a single run on different GP sample paths. Here, we investigate the impact of performing _multiple_ restarts on the same sample path. In Figure 3, we plot (left) a kernel density estimate of the local solution found for a series of 10 000 random restarts, and (right) the best value found after several restarts with a 90% confidence interval. We make two observations. First, the improvement of running 10-20 restarts over a single restart is still significant: the Gaussian tails involved here render a difference of \( 1\) in objective value relatively large. Second, the improvement of multiple restarts saturates relatively quickly. This matches empirical observations

Figure 3: The performance of random restart on a GP sample path in 100 dimensions. **Left**: a density plot for the minimum value found on a single restart (compare with Figure 1). **Right**: the median and a 90% confidence interval for the best value found after a given number of random restarts.

Figure 2: Compare the error function (3) of the \(=2.5\) Matรฉrn kernel and our upper bound in Lemma 5. The error function \(E_{d,k,}(b)\) is approximated by minimizing (3) with L-BFGS. Both plots are in log-log scale. **Left:** The slope indicates the exponent on \(b\). Since the slope magnitude of the error function is slightly larger, the error function might decreases slightly faster than \((b^{-})\) asymptotically. **Right:** The slope indicates the exponent on \(d\). Since all lines have roughly the same slope, the dependency on the dimension in Lemma 5 seems to be tight.

made when using local BO methods on real world problems, where optimization is often terminated after (at most) a handful of restarts.

### What if the objective function is non-differentiable?

In this section, we investigate what happens when \(f\) is not differentiable -- a setting ruled out in our theory by Assumption 1. In this case, what does the posterior mean gradient \(_{}\) learn from the oracle queries? To answer this question, we consider the \(_{1}\) norm function \(\|\|_{1}\) in two dimensions, which is non-differentiable when either \(x_{1}\) or \(x_{2}\) is zero. The \(_{1}\) norm is convex and thus its subdifferential is well-defined:

\[\|\|_{1}=\{^{2}:\|\|_{ } 1,^{}=\|\|_{1}\}.\]

In Figure 4, we use the posterior mean gradient \(_{}\) to estimate the (sub)gradient at \((0,1)\). As a comparison, we also show the result for the differentiable quadratic function \(\|\|^{2}\). Comparing Figure 3(a) and 3(b), we observe that the \(_{1}\) norm has higher estimation error than the Euclidean norm despite using exactly the same set of queries. This might suggest that non-differentiability increases the sample complexity of learning the "slope" of the function. Increasing the sample size to \(n=10\) eventually results in a \(_{}()\) much closer to the subgradient, as shown in Figure 3(c).

## 7 Discussion and Open Questions

For prototypical functions such as GP sample paths with RBF kernel, the local solutions discovered by local Bayesian optimization routines in high dimension are of surprisingly high quality. This motivates the theoretical study of these routines, which to date has been somewhat neglected. Here we have established the convergence of a recently proposed local BO algorithm, GIBO, in both the noiseless and noisy settings. The convergence rates are polynomial in _both_ the number of samples and the input dimension, supporting the obvious intuitive observation that finding local optima is easier than finding the global optimum.

Our developments in this work leave a number of open questions. It is not clear whether solutions from local optimization perform so well because the landscape is inherently easy (_e.g._, most stationary points are good approximation of the global minimum) or because the local BO has an unknown algorithmic bias that helps it avoid bad stationary points. This question calls for analysis of the landscape of GP sample paths (or the RKHS). Additionally, we conjecture that our convergence rates are not yet tight; SS6.1 suggests that there is likely room for improvement. Further, it would be interesting to establish convergence for trust-region based local BO [_e.g._ 10, 9].

In practice, while existing lower bounds imply that any algorithm seeking to use local BO as a subroutine to discover the global optimum will ultimately face the same exponential-in-\(d\) sample complexities as other methods, our results strongly suggest that, indeed as explored empirically in the literature, local solutions can not only be found, but can be surprisingly competitive.

Figure 4: Estimating the โderivativeโ at \(=(0,1)\) with a Matรฉrn Gausssian process (\(=2.5\)) in three different settings. **Left:**\(f()=\|\|^{2}\). With \(n=5\) samples, the posterior mean gradient is close to the ground truth. **Middle:**\(f()=\|\|_{1}\). The \(_{1}\) norm is not differentiable at \((0,1)\). With exactly the same samples as the left panel, the posterior mean gradient has higher error. **Right:**\(f()=\|\|_{1}\). Increasing the sample size to \(n=10\) decreases the estimation error.