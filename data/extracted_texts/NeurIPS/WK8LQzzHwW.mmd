# Unsupervised Anomaly Detection with Rejection

Lorenzo Perini

DTAI lab & Leuven.AI,

KU Leuven, Belgium

lorenzo.perini@kuleuven.be &Jesse Davis,

DTAI lab & Leuven.AI,

KU Leuven, Belgium

jesse.davis@kuleuven.be

###### Abstract

Anomaly detection goal is to detect unexpected behaviours in the data. Because anomaly detection is usually an unsupervised task, traditional anomaly detectors learn a decision boundary by employing heuristics based on intuitions, which are hard to verify in practice. This introduces some uncertainty, especially close to the decision boundary, which may reduce the user trust in the detector's predictions. A way to combat this is by allowing the detector to reject examples with high uncertainty (Learning to Reject). This requires employing a confidence metric that captures the distance to the decision boundary and setting a rejection threshold to reject low-confidence predictions. However, selecting a proper metric and setting the rejection threshold without labels are challenging tasks. In this paper, we solve these challenges by setting a constant rejection threshold on the stability metric computed by ExCeeD. Our insight relies on a theoretical analysis of this metric. Moreover, setting a constant threshold results in strong guarantees: we estimate the test rejection rate, and derive a theoretical upper bound for both the rejection rate and the expected prediction cost. Experimentally, we show that our method outperforms some metric-based methods.

## 1 Introduction

Anomaly detection is the task of detecting unexpected behaviors in the data . Often, these anomalies are critical adverse events such as the destruction or alteration of proprietary user data , water leaks in stores , breakdowns in gas  and wind  turbines, or failures in petroleum extraction . Usually, anomalies are associated with a cost such as a monetary cost (e.g., maintenance, paying for fraudulent purchases) or a societal cost such as environmental damages (e.g., dispersion of petroleum or gas). Hence, detecting anomalies in a timely manner is an important problem.

When using an anomaly detector for decision-making, it is crucial that the user trusts the system. However, it is often hard or impossible to acquire labels for anomalies. Moreover, anomalies may not follow a pattern. Therefore anomaly detection is typically treated as an unsupervised learning problem where traditional algorithms learn a decision boundary by employing heuristics based on intuitions , such as that anomalies are far away from normal examples . Because these intuitions are hard to verify and may not hold in some cases, some predictions may have high uncertainty, especially for examples close to the decision boundary . As a result, the detector's predictions should be treated with some circumspection.

One way to increase user trust is to consider Learning to Reject . In this setting, the model does not always make a prediction. Instead, it can abstain when it is at a heightened risk of making a mistake thereby improving its performance when it does offer a prediction. Abstention has the drawback that no prediction is made, which means that a person must intervene to make a decision. In the literature, two types of rejection have been identified : novelty rejection allows the model to abstain when given an out-of-distribution (OOD) example, while ambiguity rejection enables abstention for a test example that is too close to the model's decision boundary. Because anomaliesoften are OOD examples, novelty rejection does not align well with our setting as the model would reject all OOD anomalies (i.e., a full class) [10; 63; 30]. On the other hand, current approaches for ambiguity rejection threshold what constitutes being too close to the decision boundary by evaluating the model's predictive performance on the examples for which it makes a prediction (i.e., accepted), and those where it abstains from making a prediction (i.e., rejected) [9; 44; 16]. Intuitively, the idea is to find a threshold where the model's predictive performance is (1) significantly lower on rejected examples than on accepted examples and (2) higher on accepted examples than on all examples (i.e., if it always makes a prediction). Unfortunately, existing learning to reject approaches that set a threshold in this manner require labeled data, which is not available in anomaly detection.

This paper fills this gap by proposing an approach to perform ambiguity rejection for anomaly detection in a completely unsupervised manner. Specifically, we make three major contributions. First, we conduct a thorough novel theoretical analysis of a stability metric for anomaly detection  and show that it has several previously unknown properties that are of great importance in the context of learning to reject. Namely, it captures the uncertainty close to the detector's decision boundary, and only limited number of examples get a stability value strictly lower than \(1\). Second, these enables us to design an ambiguity rejection mechanism without _any labeled data_ that offers strong guarantees which are often sought in Learning to Reject [12; 60; 7] We can derive an accurate estimate of the rejected examples proportion, as well as a theoretical upper bound that is satisfied with high probability. Moreover, given a cost function for different types of errors, we provide an estimated upper bound on the expected cost at the prediction time. Third, we evaluate our approach on an extensive set of unsupervised detectors and benchmark datasets and conclude that (1) it performs better than several adapted baselines based on other unsupervised metrics, and (2) our theoretical results hold in practice.

## 2 Preliminaries and notation

We will introduce the relevant background on anomaly detection, learning to reject, and the ExCeeD's metric that this paper builds upon.

**Anomaly Detection.** Let \(\) be a \(d\) dimensional input space and \(D=\{x_{1},,x_{n}\}\) be a training set, where each \(x_{i}\). The goal in anomaly detection is to train a detector \(f\) that maps examples to a real-valued anomaly score, denoted by \(s\). In practice, it is necessary to convert these soft scores to a hard prediction, which requires setting a threshold \(\). Assuming that higher scores equate to being more anomalous, a predicted label \(\) can be made for an example \(x\) as follows: \(=1\) (anomaly) if \(s=f(x)\), while \(=0\) (normal) if \(s=f(x)<\). We let \(\) be the random variable that denotes the predicted label. Because of the absence of labels, one usually sets the threshold such that \( n\) scores are \(\), where \(\) is the dataset's contamination factor (i.e., expected proportion of anomalies) [51; 50].

**Learning to Reject.** Learning to reject extends the output space of the model to include the symbol \(\), which means that the model abstains from making a prediction. This entails learning a second model \(r\) (the rejector) to determine when the model abstains. A canonical example of ambiguity rejection is when \(r\) consists of a pair [confidence \(_{s}\), rejection threshold \(\)] such that an example is rejected if the detector's confidence is lower than the threshold. The model output becomes

\[_{}=&_{s}>;\\ &_{s};_{ }\{0,1,\}.\]

A standard approach is to evaluate different values for \(\) to find a balance between making too many incorrect predictions because \(\) is too low (i.e., \( y\) but \(_{s}>\)) and rejecting correct predictions because \(\) is too high (i.e., \(=y\) but \(_{s}\)) [9; 44; 16]. Unfortunately, in an unsupervised setting, it is impossible to evaluate the threshold because it relies on having access to labeled data.

**ExCeeD's metric.** Traditional confidence metrics (such as calibrated class probabilities) quantify how likely a prediction is to be correct, This obviously requires labels  which are unavailable in an unsupervised setting. Thus, one option is to move the focus towards the **concept of stability**: given a fixed test example \(x\) with anomaly score \(s\), _perturbing the training data alters the model learning, which, in turn, affects the label prediction_. Intuitively, the more stable a detector's output is for a test example, the less sensitive its predicted label is to changes in the training data. On the other hand, when \((=1|s)(=0|s) 0.5\) the prediction for \(x\) is highly unstable, as training the detector with slightly different examples would flip its prediction for the same test score \(s\). Thus, a stability-based confidence metric \(_{s}\) can be expressed as the margin between the two classes' probabilities:

\[_{s}=|(=1|s)-(=0|s)|=|2( =1|s)-1|,\]

where the lower \(_{s}\) the more unstable the prediction.

Recently, Perini et al. introduced ExCeeD to estimate the detector's stability \((=1|s)\). Roughly speaking, ExCeeD uses a Bayesian formulation that simulates bootstrapping the training set as a form of perturbation. Formally, it measures such stability for a test score \(s\) in two steps.

**First**, it computes the _training frequency_\(_{n}=|}{n}\!\![0,\!1]\), i.e. the proportion of training scores lower than \(s\). This expresses how extreme the score \(s\) ranks with respect to the training scores.

**Second**, it computes the probability that the score \(s\) will be predicted as an anomaly when randomly drawing a training set of \(n\) scores from the population of scores. In practice, this is the probability that the chosen threshold \(\) will be less than or equal to the score \(s\). The stability is therefore estimated as

\[(=1|s)=_{i=n(1-)+1}^{n}(}{2+n})^{i}()+1}{2+n})^{n-i}.\] (1)

_Assumption._ExCeeD's Bayesian formulation requires assuming that \(Y|s\) follows a Bernoulli distribution with parameter \(p_{s}=(S s)\), where \(S\) is the detector's population of scores. Note that the stability metric is a detector property and, therefore, is tied to the specific choice of the unsupervised detector \(f\).

## 3 Methodology

This paper addresses the following problem:

_Given:_ An unlabeled dataset \(D\) with contamination \(\), an unsupervised detector \(f\), a cost function \(c\);

_Do:_ Introduce a reject option to \(f\), i.e. find a pair (confidence, threshold) that minimizes the cost.

We propose an anomaly detector-agnostic approach for performing learning to reject that requires _no labels_. Our key contribution is a novel theoretical analysis of the ExCeeD confidence metric that proves that _only a limited number of examples have confidence lower than \(1-\)_ (Sec. 3.1). Intuitively, the detector's predictions for most examples would not be affected by slight perturbations of the training set: it is easy to identify the majority of normal examples and anomalies because they will strongly adhere to the data-driven heuristics that unsupervised anomaly detectors use. For example, using the data density as a measure of anomalousness  tends to identify all densely clustered normals and isolated anomalies, which constitute the majority of all examples. In contrast, only relatively few cases would be ambiguous and hence receive low confidence (e.g., small clusters of anomalies and normals at the edges of dense clusters).

Our approach is called **RejEx** (Rejecting via ExCeeD) and simply computes the stability-based confidence metric \(_{s}\) and rejects any example with confidence that falls below threshold \(=1-\). Theoretically, this constant reject threshold provides several relevant guarantees. First, one often needs to control the proportion of rejections (namely, the _rejection rate_) to estimate the number of decisions left to the user. Thus, we propose _an estimator that only uses training instances to estimate the rejection rate at test time_. Second, because in some applications avoiding the risk of rejecting all the examples is a strict constraint, we provided _an upper bound for the rejection rate_ (Sec. 3.2). Finally, we compute a _theoretical upper bound for a given cost function_ that guarantees that using RejEx keeps the expected cost per example at test time low (Sec. 3.3).

### Setting the Rejection Threshold through a Novel Theoretical Analysis of ExCeeD

Our novel theoretical analysis proves (1) that the stability metric by ExCeeD is lower than \(1-\) for a limited number of examples (Theorem 3.1), and (2) that such examples with low confidence are the ones close to the decision boundary (Corollay 3.2). Thus, we propose to reject all these uncertain examples by setting a rejection threshold

\[=1-=1-2e^{-T}T 4,\]where \(2e^{-T}\) is the tolerance that excludes unlikely scenarios, and \(T 4\) is required for Theorem 3.1.

We motivate our approach as follows. Given an example \(x\) with score \(s\) and the proportion of lower training scores \(_{n}\), Theorem 3.1 shows that the confidence \(_{s}\) is lower than \(1-2e^{-T}\) (for \(T 4\)) if \(_{n}\) belongs to the interval \([t_{1},t_{2}]\). By analyzing \([t_{1},t_{2}]\), Corollary 3.2 proves that the closer an example is to the decision boundary, the lower the confidence \(_{s}\), and that a score \(s=\) (decision threshold) has confidence \(_{s}=0\).

_Remark_.: Perini et al. performed an asymptotic analysis of ExCeeD that investigates the metric's behavior when the training set's size \(n+\). In contrast, our novel analysis is finite-sample and hence provides more practical insights, as real-world scenarios involve having a finite dataset with size \(n\).

**Theorem 3.1** (Analysis of ExCeeD).: _Let \(s\) be an anomaly score, and \(_{n}\) its training frequency. For \(T\!\!4\), there exist \(t_{1}=t_{1}(n,,T)\!\!\), \(t_{2}=t_{2}(n,,T)\!\!\) such that_

\[_{n}[t_{1},t_{2}]_{s} 1-2e^{-T}.\]

Proof.: See the Supplement for the formal proof. 

The interval \([t_{1},t_{2}]\) has two relevant properties. First, it becomes _narrower when increasing \(n\)_ (P1) and _larger when increasing \(T\)_ (P2). This means that collecting more training data results in smaller rejection regions while decreasing the tolerance \(=2e^{-T}\) has the opposite effect. Second, it is centered (not symmetrically) on \(1-\) (P3-P4), which means that _examples with anomaly scores close to the decision threshold \(\) are the ones with a low confidence score_ (P5). The next Corollary lists these properties.

**Corollary 3.2**.: _Given \(t_{1},t_{2}\) as in Theorem 3.1, the following properties hold for any \(s\), \(n\), \(\), \(T 4\):_

1. \(_{n+}t_{1}=_{n+}t_{2}=1-\)_;_
2. \(t_{1}\) _and_ \(t_{2}\) _are, respectively, monotonic decreasing and increasing as functions of_ \(T\)_;_
3. _the interval always contains_ \(1-\)_, i.e._ \(t_{1} 1- t_{2}\)_;_
4. _for_ \(n\)_, there exists_ \(s^{*}\) _with_ \(_{n}=t^{*}[t_{1},t_{2}]\) _such that_ \(t^{*} 1-\) _and_ \(_{s} 0\)_._
5. \(_{n}[t_{1},t_{2}]\)__iff__\(s[-u_{1},+u_{2}]\)_, where_ \(u_{1}(n,,T),u_{2}(n,,T)\) _are positive functions._

Proof sketch.: For P1, it is enough to observe that \(t_{1},t_{2} 1-\) for \(n+\). For P2 and P3, the result comes from simple algebraic steps. P4 follows from the surjectivity of \(_{s}\) when \(n+\), the monotonicity of \((=1|s)\), from P1 with the squeeze theorem. Finally, P5 follows from \(_{n}[t_{1},t_{2}] s[_{n}^{-1}(t_{1}),_{n}^{-1} (t_{2})]\), as \(_{n}\) is monotonic increasing, where \(_{n}^{-1}\) is the inverse-image of \(_{n}\). Because for P3 \(1-[t_{1},t_{2}]\), it holds that \(_{n}^{-1}(t_{1})_{n}^{-1}(1-)=_{n}^{-1}(t_{2})\). This implies that \(s[-u_{1},+u_{2}]\), where \(u_{1}=-_{n}^{-1}(t_{1})\), \(u_{2}=-_{n}^{-1}(t_{2})\). 

### Estimating and Bounding the Rejection Rate

It is important to have an estimate of the rejection rate, which is the proportion of examples for which the model will abstain from making a prediction. This is an important performance characteristic for differentiating among candidate models. Moreover, it is important that not all examples are rejected because such a model is useless in practice. We propose a way to estimate the rejection rate and Theorem 3.5 shows that our estimate approaches the true rate for large training sets. We strengthen our analysis and introduce an upper bound for the rejection rate, which guarantees that, with arbitrarily high probability, the rejection rate is kept lower than a constant (Theorem 3.6).

**Definition 3.3** (Rejection rate).: Given the confidence metric \(_{s}\) and the rejection threshold \(\), the _rejection rate_\(=(_{s})\) is the probability that a test example with score \(s\) gets rejected.

We propose the following estimator for the reject rate:

**Definition 3.4** (Rejection rate estimator).: Given anomaly scores \(s\) with training frequencies \(_{n}\), let \(g\) be the function such that \((=1|s)=g(_{n})\) (see Eq. 1). We define the _rejection rate estimator_\(}\) as

\[}=_{_{n}}(g^{-1}(1-e^{-T}))- _{_{n}}(g^{-1}(e^{-T}))\] (2)where \(g^{-1}\) is the inverse-image through \(g\), and, for \(u\), \(_{_{n}}(u)=(s_{i}) u\|}{n}\) is the empirical cumulative distribution of \(_{n}\).

Note that \(}\) can be computed in practice, as the \(_{n}\) has a distribution that is arbitrarily close to uniform, as stated by Theorem A.1 and A.2 in the Supplement.

**Theorem 3.5** (Rejection rate estimate).: _Let \(g\) be as in Def. 3.4. Then, for high values of \(n\), \(}\)._

Proof.: From the definition of rejection rate 3.3, it follows

\[ =(_{s} 1-2e^{-T})= ((=1|s)[e^{-T},1-e^{-T}])=(g(_{n})[e^{-T},1-e^{-T}])\] \[=(_{n}[g^{-1}(e^{-T}),g^{- 1}(1-e^{-T})])=F_{_{n}}(g^{-1}(1-e^{-T} ))-F_{_{n}}(g^{-1}(e^{-T})).\]

where \(F_{_{n}}()=(_{n})\) is the theoretical cumulative distribution of \(_{n}\). Because the true distribution of \(_{n}\) for test examples is unknown, the estimator approximates \(F_{_{n}}\) using the training scores \(s_{i}\) and computes the empirical \(_{_{n}}\). As a result,

\[_{_{n}}(g^{-1}(1-e^{-T}))- _{_{n}}(g^{-1}(e^{-T}))=}.\]

**Theorem 3.6** (Rejection rate upper bound).: _Let \(s\) be an anomaly score, \(_{s}\) be its confidence value, and \(=1-2e^{-T}\) be the rejection threshold. For \(n\), \([0,0.5)\), and small \(>0\), there exists a positive real function \(h(n,,T,)\) such that \( h(n,,T,)\) with probability at least \(1-\), i.e. the rejection rate is bounded._

Proof.: Theorem 3.1 states that there exists two functions \(t_{1}=t_{1}(n,,T),t_{2}=t_{2}(n,,T)\) such that the confidence is lower than \(\) if \(_{n}[t_{1},t_{2}]\). Moreover, Theorems A.1 and A.2 claim that \(_{n}\) has a distribution that is close to uniform with high probability (see the theorems and proofs in the Supplement). As a result, with probability at least \(1-\), we find \(h(n,,T,)\) as follows:

\[ =(_{s} 1-2e^{-T}^{ }(_{n}[t_{1},t_{2}])=F_{_{n}}(t_ {2})-F_{_{n}}(t_{1})\] \[^{}F_{}(t_{2})-F_{}(t_{1})+2 TA.1}{2n}}t_{2}(n,,T)-t_{1}(n,,T)+2 }{2n}}=h(n,,T,).\]

### Upper Bounding the Expected Test Time Cost

In a learning with reject scenario, there are costs associated with three outcomes: false positives (\(c_{fp}>0\)), false negatives (\(c_{fn}>0\)), and rejection (\(c_{r}\)) because abstaining typically involves having a person intervene. Estimating an expected per example prediction cost at test time can help with model selection and give a sense of performance. Theorem 3.8 provides an upper bound on the expected per example cost when (1) using our estimated rejection rate (Theorem 3.5), and (2) setting the decision threshold \(\) as in Sec. 2.

**Definition 3.7** (Cost function).: Let \(Y\) be the true label random variable. Given the costs \(c_{fp}>0\), \(c_{fn}>0\), and \(c_{r}\), the **cost function** is a function \(c\{0,1\}\{0,1,\}\) such that

\[c(Y,)\!=\!c_{r}(\!\!=\!)+c_{fp}( {Y}\!\!=\!1|Y\!\!=\!0)+c_{fn}(\!\!=\!0|Y\!\!=\!1)\]

Note that defining a specific cost function requires domain knowledge. Following the learning to reject literature, we set an additive cost function. Moreover, the rejection cost needs to satisfy the inequality \(c_{r}\{(1-)c_{fp}, c_{fn}\}\). This avoids the possibility of predicting always anomaly for an expected cost of \((1-)c_{fp}\), or always normal with an expected cost of \( c_{fn}\).

**Theorem 3.8**.: _Let \(c\) be a cost function as defined in Def. 3.7, and \(g\) be as in Def. 3.4. Given a (test) example \(x\) with score \(s\), the expected example-wise cost is bounded by_

\[_{x}[c]\{,A\}c_{fn}+(1-B)c_{fp}+(B-A)c_{r},\] (3)

_where \(A=_{_{n}}(g^{-1}(e^{-T}))\) and \(B=_{_{n}}(g^{-1}(1-e^{-T}))\) are as in Theorem 3.5._Proof.: We indicate the true label random variable as \(Y\), and the non-rejected false positives and false negatives as, respectively,

\[FP=(=1|Y=0,_{s}>1-2e^{-T}) FN= (=0|Y=1,_{s}>1-2e^{-T})\]

Using Theorem 3.5 results in

\[_{x}[c]=_{x}[c_{fn}FN+c_{fp}FP+c_{r}]=_{x}[c_{fn}FN]+_{x}[c_{fp}FP]+c_{r}(B-A)\]

where \(A=_{_{n}}(g^{-1}(e^{-T}))\), \(B=_{_{n}}(g^{-1}(1-e^{-T}))\) come from Theorem 3.5. Now we observe that setting a decision threshold \(\) such that \(n\) scores are higher implies that, on expectation, the detector predicts a proportion of positives equal to \(=(Y=1)\). Moreover, for \(=2e^{-T}\),

* \(FP(=1|_{s}>1-)=1-B\) as false positives must be less than total accepted positive predictions;
* \(FN\) and \(FN(=0|_{s}>1-)=A\), as you cannot have more false negatives than positives (\(\)), nor than accepted negative predictions (\(A\)).

From these observations, we conclude that \(_{x}[c]\{,A\}c_{fn}+(1-B)c_{fp}+(B-A)c_{r}\). 

## 4 Related work

There is no research on learning to reject in unsupervised anomaly detection. However, **three** main research lines are connected to this work.

1) Supervised methods.If some labels are available, one can use traditional supervised approaches to add the reject option into the detector [11; 38]. Commonly, labels can be used to find the optimal rejection threshold in two ways: 1) by trading off the model performance (e.g., AUC) on the accepted examples with its rejection rate [24; 1], or 2) by minimizing a cost function [46; 7], a risk function [18; 27], or an error function [35; 33]. Alternatively, one can include the reject option in the model and directly optimize it during the learning phase [60; 12; 31].

2) Self-Supervised methods.If labels are not available, one can leverage self-supervised approaches to generate pseudo-labels in order to apply traditional supervised learning to reject methods [26; 59; 19; 37]. For example, one can employ any unsupervised anomaly detector to assign training labels, fit a (semi-)supervised detector (such as DeepSAD  or Repen) on the pseudo labels, compute a confidence metric , and find the optimal rejection threshold by minimizing the cost function treating the pseudo-labels as the ground truth.

3) Optimizing unsupervised metrics.There exist several unsupervised metrics (i.e., they can be computed without labels) for quantifying detector quality . Because they do not need labels, one can find the rejection threshold by maximizing the margin between the detector's quality (computed using such metric) on the accepted and on the rejected examples . This allows us to obtain a model that performs well on the accepted examples and poorly on the rejected ones, which is exactly the same intuition that underlies the supervised approaches. Some examples of existing unsupervised metrics (see ) are the following. Em and Mv quantify the clusterness of inlier scores, where more compact scores indicate better models. Stability measures the robustness of anomaly detectors' predictions by looking at how consistently they rank examples by anomalousness. Udr is a model-selection metric that selects the model with a hyperparameter setting that yields consistent results across various seeds, which can be used to set the rejection threshold through the analogy [hyperparameter, seed] and [rejection threshold, detectors]. Finally, Ens[56; 67] measures the detector trustworthiness as the ranking-based similarity (e.g., correlation) of a detector's output to the "pseudo ground truth", computed via aggregating the output of an ensemble of detectors, which allows one to set the rejection threshold that maximizes the correlation between the detector's and the ensemble's outputs.

Experiments

We experimentally address the following research questions:

1. How does ReJEx's cost compare to the baselines?
2. How does varying the cost function affect the results?
3. How does ReJEx's CPU time compare to the baselines?
4. Do the theoretical results hold in practice?
5. Would ReJEx's performance significantly improve if it had access to training labels?

### Experimental Setup

Methods.We compare **ReJEx1** against \(7\) baselines for setting the rejection threshold. These can be divided into three categories: no rejection, self-supervised, and unsupervised metric based.

We use one method **NoReject** that always makes predictions and never rejects (no reject option).

We consider one self-supervised approach **SS-Repen**. This uses (any) unsupervised detector to obtain pseudo labels for the training set. It then sets the rejection threshold as follows: 1) it creates a held-out validation set (\(20\%\)), 2) it fits Repen, a state-of-the-art (semi-)supervised anomaly detector on the training set with the pseudo labels, 3) it computes on the validation set the confidence values as the margin between Repen's predicted class probabilities \(|(Y=1|s)-(Y=0|s)|\), 4) it finds the optimal threshold \(\) by minimizing the total cost obtained on the validation set.

We consider \(5\) approaches that employ an existing unsupervised metric to set the rejection threshold and hence do not require having access to labels. **MV**, **EM**, and **Stability** are unsupervised metric-based methods based on stand-alone internal evaluations that use a single anomaly detector to measure its quality, **Udr** and **Ens** are unsupervised consensus-based metrics that an ensemble of detectors (all 12 considered in our experiments) to measure a detector's quality.2 We apply each of these \(5\) baselines as follows. 1) We apply the unsupervised detector to assign an anomaly score to each train set example. 2) We convert these scores into class probabilities using . 3) We compute the confidence scores on the training set as difference between these probabilities: \(|(Y=1|s)-(Y=0|s)|\). 4) We evaluate possible thresholds on this confidence by computing the considered unsupervised metric on the accepted and on the rejected examples and select the threshold that maximizes the difference in the metric's value on these two sets of examples. This aligns with the common learning to reject criteria for picking a threshold [9; 54] such that the model performs well on the accepted examples and poorly on the rejected ones.

Data.We carry out our study on \(34\) publicly available benchmark datasets, widely used in the literature . These datasets cover many application domains, including healthcare (e.g., disease diagnosis), audio and language processing (e.g., speech recognition), image processing (e.g., object identification), and finance (e.g., fraud detection). To limit the computational time, we randomly sub-sample \(20,000\) examples from all large datasets. Table 3 in the Supplement provides further details.

Anomaly Detectors and Hyperparameters.We set our tolerance \(=2e^{-T}\) with \(T=32\). Note that the exponential smooths out the effect of \(T 4\), which makes setting a different \(T\) have little impact. We use a set of \(12\) unsupervised anomaly detectors implemented in PyOD with default hyperparameters  because the unsupervised setting does not allow us to tune them: Knn, IForest, Lof, Ocsvm, Ae, Hbos, Loda, Copod, Gmm, Ecod, Kde, Inne. We set all the baselines' rejection threshold via Bayesian Optimization with \(50\) calls .

Setup.For each [dataset, detector] pair, we proceed as follows: (1) we split the dataset into training and test sets (80-20) using \(5\) fold cross-validation; (2) we use the detector to assign the anomaly scores on the training set; (3) we use either ReJEx or a baseline to set the rejection threshold;(4) we measure the total cost on the test set using the given cost function. We carry out a total of \(34 12 5=2040\) experiments. All experiments were run on an Intel(R) Xeon(R) Silver 4214 CPU.

### Experimental Results

Q1: RejEx against the baselines.Figure 1 shows the comparison between our method and the baselines, grouped by detector, when setting the costs \(c_{fp}=c_{fn}=1\) and \(c_{r}=\) (see the Supplement for further details). RejEx achieves the lowest (best) cost per example for \(9\) out of \(12\) detectors (left-hand side) and similar values to SS-Repen when using Loda, Lof and Kde. Averaging over the detectors, RejEx reduces the relative cost by more than \(5\%\) vs SS-Repen, \(11\%\) vs Ens, \(13\%\) vs Mv and Udr, \(17\%\) vs Em, \(19\%\) vs NoReject. Table 4 (Supplement) shows a detailed breakdown.

For each experiment, we rank all the methods from \(1\) to \(8\), where position \(1\) indicates the lowest (best) cost. The right-hand side of Figure 1 shows that RejEx always obtains the lowest average ranking. We run a statistical analysis separately for each detector: the Friedman test rejects the null-hypothesis that all methods perform similarly (p-value \(<e^{-16}\)) for all the detectors. The ranking-based post-hoc Bonferroni-Dunn statistical test  with \(=0.05\) finds that RejEx is significantly better than the baselines for \(6\) detectors (Inne, IForest, Hbos, Knn, Econd, Ocsvm).

Q2. Varying the costs \(c_{fp}\), \(c_{fn}\), \(c_{r}\).The three costs \(c_{fp}\), \(c_{fn}\), and \(c_{r}\) are usually set based on domain knowledge: whether to penalize the false positives or the false negatives more depends on the application domain. Moreover, the rejection cost needs to satisfy the constraint

Figure 1: Average cost per example (left) and rank (right) aggregated per detector (x-axis) over all the datasets. Our method obtains the lowest (best) cost for \(9\) out of \(12\) detectors and it always has the lowest (best) ranking position for \(c_{fp}=c_{fn}=1\), \(c_{r}=\).

Figure 2: Average cost per example aggregated by detector over the \(34\) datasets when varying the three costs on three representative cases: (left) false positives are penalized more, (center) false negatives are penalized more, (right) rejection has a lower cost than FPs and FNs.

\(\))\(c_{fp}\), \(\)\(c_{fn}\)) . Therefore, we study their impact on three representative cases: (case 1) high false positive cost (\(c_{fp}=10\), \(c_{fn}=1\), \(c_{r}=\{10(1-),\}\), (case 2) high false negative cost (\(c_{fp}=1\), \(c_{fn}=10\), \(c_{r}=\{(1-),10\}\), and (case 3) same cost for both mispredictions but low rejection cost (\(c_{fp}=5\), \(c_{fn}=5\), \(c_{r}=\)). Note that scaling all the costs has no effect on the relative comparison between the methods, so the last case is equivalent to \(c_{fp}=1\), \(c_{fn}=1\), and \(c_{r}=/5\).

Figure 2 shows results for the three scenarios. Compared to the unsupervised metric-based methods, the left plot shows that our method is clearly the best for high false positives cost: for \(11\) out of \(12\) detectors, ReJEx obtains both the lowest (or similar for Gmm) average cost and the lowest average ranking position. This indicates that using ReJEx is suitable when false alarms are expensive. Similarly, the right plot illustrates that ReJEx outperforms all the baselines for all the detectors when the rejection cost is low (w.r.t. the false positive and false negative costs). Even when the false negative cost is high (central plot), ReJEx obtains the lowest average cost for \(11\) detectors and has always the lowest average rank per detector. See the Supplement (Table 6 and 7) for more details.

Q3. Comparing the CPU time.Table 1 reports CPU time in milliseconds per training example aggregated over the \(34\) datasets needed for each method to set the rejection threshold on three unsupervised anomaly detectors (IForest, Hbos, Copod). NoReject has CPU time equal to \(0\) because it does not use any reject option. ReJEx takes just a little more time than NoReject because computing ExCeeD has linear time while setting a constant threshold has constant time. In contrast, all other methods take \(1000\) longer because they evaluate multiple thresholds. For some of these (e.g., Stability), this involves an expensive internal procedure.

Q4. Checking on the theoretical results.Section 3 introduces three theoretical results: the rejection rate estimate (Theorem 3.5), and the upper bound for the rejection rate (Theorem 3.6) and for the cost (Theorem 3.8). We run experiments to verify whether they hold in practice. Figure 3 shows the results aggregated over the detectors. The left-hand side confirms that the prediction cost per example (blue circle) is always \(\) than the upper bound (black line). Note that the upper bound is sufficiently strict, as in some cases it equals the empirical cost (e.g., Census, Wilt, Optdigits).

    & &  \\ Detector & NoReject & **ReJEx** & SS-Repen & My & Em & Udr & Ens & Stability \\  IForest & 0.0\(\)0.0 & **0.06\(\)0.22** & 90\(\)68 & 89\(\)128 & 155\(\)161 & 120\(\)132 & 122\(\)135 & 916\(\)900 \\ Hbos & 0.0\(\)0.0 & **0.13\(\)0.93** & 89\(\)53 & 39\(\)81 & 80\(\)129 & 200\(\)338 & 210\(\)358 & 142\(\)242 \\ Copod & 0.0\(\)0.0 & **0.04\(\)0.04** & 84\(\)53 & 21\(\)28 & 81\(\)60 & 119\(\)131 & 123\(\)138 & 140\(\)248 \\   

Table 1: Average CPU time (in ms) per training example (\(\) std) to set the rejection threshold aggregated over all the datasets when using IForest, Hbos, and Copod as unsupervised anomaly detector. ReJEx has a lower time than all the methods but NoReject, which uses no reject option.

Figure 3: Average cost per example (left) and average rejection rate (right) at test time aggregated by dataset over the \(12\) detectors. In both plots, the empirical value (circle) is always lower than the predicted upper bound (continuous black line), which makes it consistent with the theory. On the right, the expected rejection rates (stars) are almost identical to the empirical values.

The right-hand side shows that our rejection rate estimate (orange star) is almost identical to the empirical rejection rate (orange circle) for most of the datasets, especially the large ones. On the other hand, small datasets have the largest gap, e.g., Wine (\(n=129\)), Lymphography (\(n=148\)), WPBC (\(n=198\)), Vertebral (\(n=240\)). Finally, the empirical rejection rate is always lower than the theoretical upper bound (black line), which we compute by using the empirical frequencies \(_{n}\).

Q5. Impact of training labels on RejEx.We simulate having access to the training labels and include an extra baseline: Oracle uses ExCeeD as a confidence metric and sets the (optimal) rejection threshold by minimizing the cost function using the training labels. Table 2 shows the average cost and rejection rates at test time obtained by the two methods. Overall, RejEx obtains an average cost that is only \(0.6\%\) higher than Oracle's cost. On a per-detector basis, RejEx obtains a \(2.5\%\) higher cost in the worst case (with Loda), while getting only a \(0.08\%\) increase in the best case (with Kde). Comparing the rejection rates, RejEx rejects on average only \( 1.5\) percentage points more examples than Oracle (\(12.9\%\) vs \(11.4\%\)). The supplement provides further details.

## 6 Conclusion and Limitations

This paper addressed learning to reject in the context of unsupervised anomaly detection. The key challenge was how to set the rejection threshold without access to labels which are required by all existing approaches We proposed an approach RejEx that exploits our novel theoretical analysis of the ExCeeD confidence metric. Our new analysis shows that it is possible to set a constant rejection threshold and that doing so offers strong theoretical guarantees. First, we can estimate the proportion of rejected test examples and provide an upper bound for our estimate. Second, we can provide a theoretical upper bound on the expected test-time prediction cost per example. Experimentally, we compared RejEx against several (unsupervised) metric-based methods and showed that, for the majority of anomaly detectors, it obtained lower (better) cost. Moreover, we proved that our theoretical results hold in practice and that our rejection rate estimate is almost identical to the true value in the majority of cases.

**Limitations.** Because RejEx does not rely on labels, it can only give a coarse-grained view of performance. For example, in many applications anomalies will have varying costs (i.e., there are instance-specific costs) which we cannot account for. Moreover, RejEx has a strictly positive rejection rate, which may increase the cost of a highly accurate detector. However, this happens only in \( 5\%\) of our experiments.

    &  &  \\ Detector & RejEx & Oracle & RejEx & Oracle \\  Ae & 0.126 \(\) 0.139 & 0.126 \(\) 0.139 & 0.131 \(\) 0.132 & 0.118 \(\) 0.125 \\ Copod & 0.123 \(\) 0.140 & 0.121 \(\) 0.140 & 0.123 \(\) 0.131 & 0.101 \(\) 0.114 \\ Ecod & 0.119 \(\) 0.138 & 0.118 \(\) 0.138 & 0.125 \(\) 0.130 & 0.107 \(\) 0.114 \\ Gmm & 0.123 \(\) 0.135 & 0.122 \(\) 0.134 & 0.139 \(\) 0.143 & 0.132 \(\) 0.136 \\ Hbos & 0.118 \(\) 0.129 & 0.118 \(\) 0.129 & 0.139 \(\) 0.148 & 0.114 \(\) 0.128 \\ IForest & 0.118 \(\) 0.129 & 0.118 \(\) 0.128 & 0.127 \(\) 0.131 & 0.118 \(\) 0.130 \\ Inne & 0.115 \(\) 0.129 & 0.115 \(\) 0.128 & 0.132 \(\) 0.132 & 0.122 \(\) 0.125 \\ Kde & 0.129 \(\) 0.140 & 0.129 \(\) 0.139 & 0.121 \(\) 0.129 & 0.105 \(\) 0.120 \\ Knn & 0.119 \(\) 0.123 & 0.118 \(\) 0.123 & 0.127 \(\) 0.129 & 0.112 \(\) 0.117 \\ Loda & 0.125 \(\) 0.133 & 0.122 \(\) 0.130 & 0.126 \(\) 0.124 & 0.110 \(\) 0.114 \\ Lof & 0.126 \(\) 0.131 & 0.125 \(\) 0.131 & 0.129 \(\) 0.126 & 0.118 \(\) 0.115 \\ Occym & 0.120 \(\) 0.131 & 0.120 \(\) 0.131 & 0.126 \(\) 0.128 & 0.107 \(\) 0.115 \\  Avg. & 0.122 \(\) 0.133 & 0.121 \(\) 0.133 & 0.129 \(\) 0.132 & 0.114 \(\) 0.121 \\   

Table 2: Mean \(\) std. for the **cost per example** (on the left) and the **rejection rate** (on the right) at test time on a per detector basis and aggregated over the datasets.