# The GAN is dead; long live the GAN!

A Modern Baseline GAN

 Yiwen Huang

Brown University &Aaron Gokaslan

Cornell University &Volodymyr Kuleshov

Cornell University &James Tompkin

Brown University

###### Abstract

There is a widely-spread claim that GANs are difficult to train, and GAN architectures in the literature are littered with empirical tricks. We provide evidence against this claim and build a modern GAN baseline in a more principled manner. First, we derive a well-behaved regularized relativistic GAN loss that addresses issues of mode dropping and non-convergence that were previously tackled via a bag of ad-hoc tricks. We analyze our loss mathematically and prove that it admits local convergence guarantees, unlike most existing relativistic losses. Second, this loss allows us to discard all ad-hoc tricks and replace outdated backbones used in common GANs with modern architectures. Using StyleGAN2 as an example, we present a roadmap of simplification and modernization that results in a new minimalist baseline--R3GAN ("Re-GAN"). Despite being simple, our approach surpasses StyleGAN2 on FFHQ, ImageNet, CIFAR, and Stacked MNIST datasets, and compares favorably against state-of-the-art GANs and diffusion models.

Code: https://www.github.com/brownvc/R3GAN

## 1 Introduction

Generative adversarial networks (GANs) let us generate high-quality images in a single forward pass. However, the original objective in Goodfellow _et al_. , is notoriously difficult to optimize due to its minimax nature. This leads to a fear that training might diverge at any point due to instability, and a fear that generated images might lose diversity through mode collapse. While there has been progress in GAN objectives [14; 22; 81; 52; 64], practically, the effects of brittle losses are still regularly felt. This notoriety has had a lasting negative impact on GAN research.

A complementary issue--partly motivated by this instability--is that existing popular GAN backbones like StyleGAN [29; 31; 30; 32] use many poorly-understood empirical tricks with little theory. For instance, StyleGAN uses a gradient penalized non-saturating loss  to increase stability (affecting sample diversity), but then employs a minibatch standard deviation trick  to increase sample diversity. Without tricks, the StyleGAN backbone still resembles DCGAN  from 2015, yet it is still the common backbone of SOTA GANs such as GigaGAN  and StyleGAN-T . Advances in GANs have been conservative compared to other generative models such as diffusion models [20; 78; 33; 34], where modern computer vision techniques such as multi-headed self attention  and backbones such as preactivated ResNet , U-Net  and vision transformers (ViTs)  are the norm. Given outdated backbones, it is not surprising that there is a widely-spread belief that GANs do not scale in terms of quantitative metrics like Frechet Inception Distance .

We reconsider this situation: we show that by combining progress in objectives into a regularized training loss, GANs gain improved training stability, which allows us to upgrade GANs with modern backbones. First, we propose a novel objective that augments the relativistic pairing GAN loss (RpGAN; ) with zero-centered gradient penalties [52; 64], improving stability [14; 64; 52]. We show mathematically that gradient-penalized RpGAN enjoys the same guarantee of local convergence as regularized classic GANs, and that removing our regularization scheme induces non-convergence.

Once we have a well-behaved loss, none of the GAN tricks are necessary [28; 31], and we are free to engineer a modern SOTA backbone architecture. We strip StyleGAN of all its features, identify those that are essential, then borrow new architecture designs from modern ConvNets and transformers [48; 97]. Briefly, we find that proper ResNet design [17; 67], initialization , and resampling [29; 31; 32; 100] are important, along with grouped convolution [95; 5] and no normalization [31; 34; 14; 88; 4]. This leads to a design that is simpler than StyleGAN and improves FID performance for the same network capacity (2.75 vs. 3.78 on FFHQ-256).

In summary, our work first argues mathematically that GANs need not be tricky to train via an improved regularized loss. Then, it empirically develops a simple GAN baseline that, without any tricks, compares favorably by FID to StyleGAN [29; 31; 32], other SOTA GANs [3; 42; 94], and diffusion models [20; 78; 86] across FFHQ, ImageNet, CIFAR, and Stacked MNIST datasets.

## 2 Serving Two Masters: Stability and Diversity with RpGAN \(+R_{1}+R_{2}\)

In defining a GAN objective, we tackle two challenges: stability and diversity. Some previous work deals with stability [29; 31; 32] and other previous work deals with mode collapse . To make progress in both, we combine a stable method with a simple regularizer that is grounded by theory.

### Traditional GAN

A traditional GAN [13; 57] is formulated as a minimax game between a discriminator (or critic) \(D_{}\) and a generator \(G_{}\). Given real data \(x p_{}\) and fake data \(x p_{}\) produced by \(G_{}\), the most general form of a GAN is given by:

\[(,)=_{z p_{z}}[f(D_{}(G_{ }(z)))]+_{x p_{}}[f(-D_{ }(x))]\] (1)

where \(G\) tries to minimize \(\) while \(D\) tries to maximize it. The choice of \(f\) is flexible [50; 44]. In particular, \(f(t)=-(1+e^{-t})\) recovers the classic GAN by Goodfellow _et al_. . For the rest of this work, this will be our choice of \(f\).

It has been shown that Equation 1 has convex properties when \(p_{}\) can be optimized directly [13; 81]. However, in practical implementations, the empirical GAN loss typically shifts fake samples beyond the decision boundary set by \(D\), as opposed to directly updating the density function \(p_{}\). This deviation leads to a significantly more challenging problem, characterized by susceptibility to two prevalent failure scenarios: mode collapse/dropping' and non-convergence.

### Relativistic \(f\)-Gan

We employ a slightly different minimax game named relativistic pairing GAN (RpGAN) by Jolicoeur-Martineau _et al_.  to address mode dropping. The general RpGAN is defined as:

\[(,)=_{z p_{z}\\ x p_{}}[f(D_{}(G_{}(z))-D_{ }(x))]\] (2)

Although Eq. 2 differs only slightly from Eq. 1, evaluating this critic difference has a fundamental impact on the landscape of \(\). Since Eq. 1 merely requires \(D\) to separate real and fake data, in the scenario where all real and fake data can be separated by a single decision boundary, the empirical GAN loss encourages \(G\) to simply move all fake samples barely past this single boundary--this degenerate solution is what we observe as mode collapse/dropping. Sun _et al_.  characterize such degenerate solutions as bad local minima in the landscape of \(\), and show that Eq. 1 has _exponentially many_ bad local minima. The culprit is the existence of a single decision boundary that naturally arises when real and fake data are considered in isolation. RpGAN introduces a simple solution by coupling real and fake data, _i.e_. a fake sample is critiqued by its realness _relative to_ a real sample, which effectively maintains a decision boundary in the neighborhood of _each_ real sample and hence forbids mode dropping. Sun _et al_.  show that the landscape of Eq. 2 contains no local minima that correspond to mode dropping solutions, and that every basin is a global minimum.

### Training Dynamics of RpGAN

Although the RpGAN landscape result  allows us to address mode dropping, the training dynamics of RpGAN have yet to be studied. The ultimate goal of Eq. 2 is to find the equilibrium \((^{*},^{*})\) such that \(p_{^{*}}=p_{}\) and \(D_{^{*}}\) is constant everywhere on \(p_{}\). Sun _et al_.  show that \(^{*}\) is globally reachable along a non-increasing trajectory in the landscape of Eq. 2 under reasonable assumptions. However, the existence of such a trajectory does not necessarily mean that gradient descent will find it. Jolicoeur-Martineau _et al_. show empirically that unregularized RpGAN does not perform well .

**Proposition I**.: (Informal) _Unregularized RpGAN does not always converge using gradient descent._

We confirm this proposition with a proof in Appendix B. We show analytically that RpGAN does not converge for certain types of \(p_{}\), such as ones that approach a delta distribution. Thus, further regularization is necessary to fill in the missing piece of a well-behaved loss.

Zero-centered gradient penalties.To tackle RpGAN non-convergence, we explore gradient penalties as the solution since it is proven that zero-centered gradient penalties (0-GP) facilitate convergent training for classic GANs . The two most commonly-used 0-GPs are \(R_{1}\) and \(R_{2}\):

\[ R_{1}()&=_{x p_{}}[_{x}D_{}^{2} ]\\ R_{2}(,)&=_{x  p_{}}[_{x}D_{}^{2}] \] (3)

\(R_{1}\) penalizes the gradient norm of \(D\) on real data, and \(R_{2}\) penalizes the gradient norm of \(D\) on fake data. Analysis on the training dynamics of GANs has thus far focused on local convergence [55; 51; 52], _i.e_., whether the training at least converges when \((,)\) are in a neighborhood of \((^{*},^{*})\). In such a scenario, the convergence behavior can be analyzed [55; 51; 52] by examining the spectrum of the Jacobian of the gradient vector field \((-_{},_{})\) at \((^{*},^{*})\). The key insight here is that when \(G\) already produces the true distribution, we want \(_{x}D=0\), so that \(G\) is not pushed away from its optimal state, and thus the training does not oscillate. \(R_{1}\) and \(R_{2}\) impose such a constraint when \(p_{}=p_{}\). This also explains why earlier attempts at gradient penalties, such as the one-centered gradient penalty (1-GP) in WGAN-GP , fail to achieve convergent training  as they still encourage \(D\) to have a non-zero slope when \(G\) has reached optimality.

Since the same insight also applies to RpGAN, we extend our previous analysis and show that:

**Proposition II**.: (Informal) _RpGAN with \(R_{1}\) or \(R_{2}\) regularization is locally convergent subject to similar assumptions as in Mescheder _et al_. .

In Appendix C, our proof similarly analyzes the eigenvalues of the Jacobian of the regularized RpGAN gradient vector field at \((^{*},^{*})\). We show that all eigenvalues have a negative real part; thus, regularized RpGAN is convergent in a neighborhood of \((^{*},^{*})\) for small enough learning rates .

Discussion.Another line of work  links \(R_{1}\) and \(R_{2}\) to instance noise  as its analytical approximation. Roth et al.  showed that for the classic GAN  by Goodfellow _et al_., \(R_{1}\) approximates convolving \(p_{}\) with the density function of \((0, I)\), up to additional weighting and a Laplacian error term. \(R_{2}\) likewise approximates convolving \(p_{}\) with \((0, I)\) up to similar error terms. The Laplacian error terms from \(R_{1}\), \(R_{2}\) cancel when \(D_{}\) approaches \(D_{^{*}}\). We do not extend Roth _et al_.'s proof  to RpGAN; however, this approach might provide complimentary insights to our work, which follows the strategy of Mescheder _et al_. .

### A Practical Demonstration

We experiment with how well-behaved our loss is on StackedMNIST  which consists of 1000 uniformly-distributed modes. The network is a small ResNet  for \(G\) and \(D\) without any normalization layers [21; 91; 1; 85]. Through the use of a pretrained MNIST classifier, we can explicitly measure how many modes of \(p_{}\) are recovered by \(p_{}\). Furthermore, we can estimate the reverse KL divergence between the fake and real samples \(D_{}\)\((p_{} p_{})\) via the KL divergence between the categorical distribution of \(p_{}\) and the true uniform distribution.

A conventional GAN loss with \(R_{1}\), as used by Mescheder et al.  and the StyleGAN series [29; 31; 32], diverges quickly (Fig. 1). Next, while theoretically sufficient for local convergence, RpGAN with only \(R_{1}\) regularization is also unstable and diverges quickly2. In each case, the gradient of \(D\) on fake samples explodes when training diverges. With both \(R_{1}\) and \(R_{2}\), training becomes stable for both the classic GAN and RpGAN. Now stable, we can see that the classic GAN suffers from mode dropping, whereas RpGAN achieves full mode coverage (Tab. 1) and reduces \(D_{}\) from 0.9270 to 0.0781. As a point of contrast, StyleGAN [29; 31; 30; 32] uses the minibatch standard deviation trick to reduce mode dropping, improving mode coverage from 857 to 881 on StackedMNIST3 and with barely any improvement on \(D_{}\).

\(R_{1}\) alone is not sufficient for globally-convergent training. While a theoretical analysis of this is difficult, our small demonstration still provides insights into the assumptions of our convergence proof. In particular, the assumption that \((,)\) are sufficiently close to \((^{*},^{*})\) is highly unlikely early in training. In this scenario, if \(D\) is sufficiently powerful, regularizing \(D\) solely on real data is not likely to have much effect on \(D\)'s behavior on fake data and so training can fail due to an ill-behaved \(D\) on fake data. This observation has been made by previous studies [84; 83] specifically for empirical GAN training, that regularizing an empirical discriminator with only \(R_{1}\) leads to gradient explosion on fake data due to the memorization of real samples.

Thus, the practical solution is to regularize \(D\) on both real and fake data. The benefit of doing so can be viewed from the insight of Roth _et al_. : that applying \(R_{1}\) and \(R_{2}\) in conjunction smooths both \(p_{}\) and \(p_{}\) which makes learning easier than only smoothing \(p_{}\). We also find empirically that with both \(R_{1}\) and \(R_{2}\) in place, \(D\) tends to satisfy \(_{x p_{}}[\|_{x}D\|^{2}] _{x p_{}}[\|_{x}D\|^{2}]\) even early in the training. Jolicoeur-Martineau _et al_.  show that in this case \(D\) becomes a maximum margin classifier--but if only one regularization term is applied, this does not hold. Additionally, having roughly the same gradient norm on real and fake data potentially reduces discriminator overfitting, as Fang _et al_.  observe that the gradient norm on real and fake data diverges when \(D\) starts to overfit.

## 3 A Roadmap to a New Baseline -- R3gan

The well-behaved RpGAN + \(R_{1}\) + \(R_{2}\) loss alleviates GAN optimization problems, and lets us proceed to build a minimalist baseline--R3GAN--with recent network backbone advances in mind [48; 97]. Rather than simply state the new approach, we will draw out a roadmap from the StyleGAN2 baseline . This model (Config A; identical to ) consists of a VGG-like  backbone for \(G\), a ResNet \(D\), a few techniques that facilitate style-based generation, and many tricks that serve as patches to the weak backbone. Then, we remove all non-essential features of StyleGAN2 (Config B), apply our loss function (Config C), and gradually modernize the network backbone (Config D-E).

   Loss & \# modes\(\) & \(D_{}\)\(\) \\  RpGAN \(+R_{1}+R_{2}\) & \(\) & \(\) \\ GAN \(+R_{1}+R_{2}\) & \(693\) & \(0.9270\) \\ RpGAN \(+R_{1}\) & Fail & Fail \\ GAN \(+R_{1}\) & Fail & Fail \\   

Table 1: StackedMNIST  result for each loss function. The maximum possible mode coverage is 1000. “Fail” indicates that training diverged early on.

Figure 1: Generator \(G\) loss for different objectives over training. Regardless of which objective is used, training diverges with only \(R_{1}\) and succeeded with both \(R_{1}\) and \(R_{2}\). Convergence failure with only \(R_{1}\) was noted by Lee et al. .

We evaluate each configuration on FFHQ \(256 256\). Network capacity is kept roughly the same for all configurations--both \(G\) and \(D\) have about 25 M trainable parameters. Each configuration is trained until \(D\) sees 5 M real images. We inherit training hyperparameters (_e.g._, optimizer settings, batch size, EMA decay length) from Config A unless otherwise specified. We tune the training hyperparameters for our final model and show the converged result in Sec. 4.

**Minimum baseline (Config B).** We strip away all StyleGAN2 features, retaining only the raw network backbone and basic image generation capability. The features fall into three categories:

* Style-based generation: mapping network , style injection , weight modulation/demodulation , noise injection .
* Image manipulation enhancements: mixing regularization , path length regularization .
* Tricks: \(z\) normalization , mini-batch stddev , equalized learning rate , lazy regularization .

Following , we reduce the dimension of \(z\) to 64. The absence of equalized learning rate necessitates a lower learning rate, reduced from 2.5\(\)10\({}^{3}\) to 5\(\)10\({}^{5}\). Despite a higher FID of 12.46 than Config-A, this simplified baseline produces reasonable sample quality and stable training. We compare this with DCGAN , an early attempt at image generation. Key differences include:

* Convergent training objective with \(R_{1}\) regularization.
* Smaller learning rate, avoiding momentum optimizer (Adam \(_{1}=0\)).
* No normalization layer in \(G\) or \(D\).
* Proper resampling via bilinear interpolation instead of strided (transposed) convolution.
* Leaky ReLU in both \(G\) and \(D\), no tanh in the output layer of \(G\).
* 4\(\)4 constant input for \(G\), output skips for \(G\), ResNet \(D\).

**Experimental findings from StyleGAN.** Violating a), b), or c) often leads to training failures. Gidel _et al._ show that _negative_ momentum can improve GAN training dynamics. Since optimal negative momentum is another challenging hyperparameter, we do not use any momentum to avoid worsening GAN training dynamics. Studies suggest normalization layers harm generative models . Batch normalization  often cripples training due to dependencies across multiple samples, and is incompatible with \(R_{1}\), \(R_{2}\), or RpGAN that assume independent handling of each sample. Weaker data-independent normalizations  might help; we leave this for future work. Early GANs may succeed despite violating a) and c), possibly constituting a full-rank solution  to Eq. 1.

Violations of d) or e) do not significantly impair training stability but negatively affect sample quality. Improper transposed convolution can cause checkerboard artifacts, unresolved even with subpixel convolution  or carefully tuned transposed convolution unless a low-pass filter is applied. Interpolation methods avoid this issue, varying from nearest neighbor  to Kaiser filters . We use bilinear interpolation for simplicity. For activation functions, smooth approximations of (leaky) ReLU, such as Swish , GELU , and SMU , worsen FID. PReLU  marginally improves FID but increases VRAM usage, so we use leaky ReLU.

All subsequent configurations adhere to a) through e). Violation of f) is acceptable as it pertains to the network backbone of StyleGAN2 , modernized in Config D and E.

**Well-behaved loss function (Config C).** We use the loss function proposed in Section 2 and this reduces FID to 11.65. We hypothesize that the network backbone in Config B is the limiting factor.

**General network modernization (Config D).** First, we apply the 1-3-1 bottleneck ResNet architecture  to both \(G\) and \(D\). This is the direct ancestor of all modern vision backbones .

    & Configuration & FID\(\) & G \#params & D \#params \\  A & StyleGAN2 & 7.516 & 24.767M & 24.001M \\  B & Stripped StyleGAN2 & & & \\  & - \(z\) normalization & & & \\  & - \(M\)inibatch stddev & & & \\  & - Equalized learning rate & & & \\  & - Mapping network & & & \\  & - Style injection & & & \\  & - Weight mod / demod & 12.46 & 18.890M & 23.996M \\  & - Noise injection & & & \\  & - Mixing regularization & & & \\  & - Path length regularization & & & \\  & - Lazy regularization & & & \\  C & Well-behaved Loss & & & \\  & + RpGAN loss & 11.77 & 18.890M & 23.996M \\  & & + \(R_{2}\) gradient penalty & 11.65 & & \\  D & ConvNeX-ify pt.1 & & & \\  & + ResNet-ify \(G\)\(\&\) D & 10.17 & 23.400M & 23.282M \\  & - Output skips & 9.950 & 23.378M & & \\  E & ConvNeX-ify pt.2 & & & \\  & + ResNeX-ify \(G\)\(\&\) D & 7.507 & 23.188M & 23.091M \\  & + Inverted bottleneck & 7.045 & 23.058M & 23.010M \\   

Table 2: Effect of our simplification and modernization efforts evaluted on FFHQ-256.

[MISSING_PAGE_EMPTY:6]

Bottleneck modernization (Config E).Now that we have settled on the overall architecture, we investigate how the residual block can be modernized, specifically i.1) and i.2). First, we explore i.1 and replace the 3\(\)3 convolution in the residual block with a grouped convolution. We set the group size to 16 rather than 1 (_i.e._ depthwise convolution as in ConvNeXt) as depthwise convolution is highly inefficient on GPUs and is not much faster than using a larger group size. With grouped convolution, we can reduce the bottleneck compression ratio to two given the same model size. This increases the width of the bottleneck to 1.5\(\) as wide as Config A. Finally, we notice that the compute cost of grouped convolution is negligible compared to 1\(\)1 convolution, and so we seek to enhance the capacity of grouped convolution. We apply i.2), which inverts the bottleneck width and the stem width, and which doubles the width of grouped convolutions without any increase in model size. Figure 2 depicts our final design, which reflects modern CNN architectures.

## 4 Experiments Details

### Roadmap Insights on FFHQ-256 

As per Table 2, Config A (vanilla StyleGAN2) achieves an FID of 7.52 using the official implementation on FFHQ-256. Config B with all tricks removed achieves an FID of 12.46--performance drops as expected. Config C, with a well-behaved loss, achieves an FID of 11.65. But, now training is sufficiently stable to improve the architecture.

Config D, which improves \(G\) and \(D\) based on the classic ResNet and ConvNeXt findings, achieves an FID of 9.95. The output skips of the StyleGAN2 generator are no longer useful given our new architecture; including them produces a worse FID of 10.17. Karras _et al_. find that the benefit of output skips is mostly related to gradient magnitude dynamics , and this has been addressed by our ResNet architecture. For StyleGAN2, Karras _et al_. conclude that a ResNet architecture is harmful to \(G\), but this is not true in our case as their ResNet implementation is considerably different from ours: 1) Karras _et al_. use one 3-3 residual block for each resolution stage, while we have a separate transition layer and two 1-3-1 residual blocks; 2) i.3) and i.4) are violated as they do not have a linear residual block  and the transition layer is placed on the skip branch of the residual block rather than the stem; 3) the essential principle of ResNet --identity mapping --is violated as Karras _et al_. divide the output of the residual block by \(\) to avoid variance explosion due to the absence of a proper initialization scheme.

For Config E, we conduct two experiments that ablate i.1 (increased width with depthwise conv.) and i.2 (an inverted bottleneck). We add GroupedConv and reduce the bottleneck compression ratio to two given the same model size. Each bottleneck is now 1.5\(\) the width of Config A, and the FID drops to 7.51, surpassing the performance of StyleGAN2. By inverting the stem and the bottleneck dimensions to enhance the capacity of GroupedConv, our final model achieves an FID of 7.05, exceeding StyleGAN2.

### Mode Recovery -- StackedMNIST 

We repeat the earlier experiment in 1000-mode convergence on StackedMNIST (unconditional generation), but this time with our updated architecture and with comparisons to SOTA GANs and likelihood-based methods (Tab. 3, Fig. 5). One advantage brought up of likelihood-based models such as diffusion over GANs is that they achieve mode coverage . We find that most GANs struggle to find all modes. But, PresGAN , DDGAN , and our approach are successful. Further, our method outperforms all other tested GAN models in term of KL divergence.

### Fid -- FFHQ-256  (Optimized)

We train Config E model until convergence and with optimized hyperparameters and training schedule on FFHQ at 256\(\)256 (unconditional generation) (Tab. 4, Figs. 4 and 6). Please see our supplemental material for training details. Our model outperforms existing StyleGAN methods, plus four more

 Model & \# modes\(\) & \(D_{}\) \\  DCGAN  & 99 & 3.40 \\ VEEGAN  & 150 & 2.95 \\ WGAN-GP  & 959 & 0.73 \\ PacGAN  & 992 & 0.28 \\ StyleGAN2  & 940 & 0.42 \\ PresGAN  & **1000** & 0.12 \\ Adv. DSM  & **1000** & 1.49 \\ VAEBM  & **1000** & 0.087 \\ DDGAN  & **1000** & 0.071 \\ MEG  & **1000** & 0.031 \\  Ours—Config E & **1000** & **0.029** \\ 

Table 3: StackedMNIST 1000-mode coverage.

recent diffusion-based methods. On this common dataset experimental setting, many methods (not listed here) use the bCR  trick--this has only been shown to improve performance on FFHQ-256 (not even at different resolutions of FFHQ) . We do not use this trick.

### Fid -- FFHQ-64 

To compare with EDM  directly, we evaluate our model on FFHQ at 64\(\)64 resolution. For this, we remove the two highest resolution stages of our 256\(\)256 model, resulting in a generator that is less than half the number of parameters as EDM. Despite this, our model outperforms EDM on this dataset and needs one function evaluation only (Tab. 5).

### Fid -- Cifar-10 

We train Config E model until convergence and with optimized hyperparameters and training schedule on CIFAR-10 (conditional generation) (Tab. 6, Fig. 8). Our method outperforms many other GANs by FID even though the model has relatively small capacity. For instance, StyleGAN-XL  has 18 M parameters in the generator and 125 M parameters in the discriminator, while our model has a 40 M parameters between the generator and discriminator combined (Fig. 3). Compared to diffusion models like LDM or ADM, GAN inference is significantly cheaper as it requires only one network function evaluation compared to the tens or hundreds of network function evaluations for diffusion models without distillation.

Many state-of-the-art GANs are derived from Projected GAN , including StyleGAN-XL  and the concurrent work of StyleSAN-XL . These methods use a pre-trained ImageNet classifier in the discriminator. Prior work has shown that a pre-trained ImageNet discriminator can leak ImageNet features into the model , causing the model to perform better when evaluating on FID since it relies on a pre-trained ImageNet classifier for the loss. But, this does not improve results in perceptual studies . Our model produces its low FID without any ImageNet pre-training.

 Model & NFE\(\) & FID\(\) \\  BigGAN  & 1 & 14.73 \\ TransGAN  & 1 & 9.26 \\ ViTGAN  & 1 & 6.66 \\ DDGAN  & 4 & 3.75 \\ Diffusion StyleGAN2  & 1 & 3.19 \\ StyleGAN2 + ADA  & 1 & 2.42 \\ StyleGAN3-R + ADA  & 1 & 10.83 \\  DDPM  & 1000 & 3.21 \\ DDIM  & 50 & 4.67 \\ VE  & 35 & 3.11 \\ VP  & 35 & 2.48 \\  Ours—Config E & 1 & 1.96 \\   \\ StyleGAN-XL*  & 1 & 1.85 \\ 

Table 6: CIFAR-10 performance.

Figure 3: Millions of parameters vs. FID-50K (log scale) on CIFAR-10. Lower is better.

### FID -- ImageNet-32 

We train Config E model until convergence and with optimized hyperparameters and training schedule on ImageNet-32 (conditional generation). We compare against recent GAN models and recent diffusion models in Table 7. We adjust the number of parameters in the generator of our model to match StyleGAN-XL 's generator (84M parameters). Specifically, we make the model significantly wider to match. Our method achieves comparable FID despite using a 60% smaller discriminator (Tab. 7) and despite not using a pre-trained ImageNet classifier.

### FID -- ImageNet-64 

We evaluate our model on ImageNet-64 to test its scalability. We stack another resolution stage on our ImageNet-32 model, resulting in a generator of 104 M parameters. This model is nearly 3\(\) smaller than diffusion-like models  that rely on the ADM backbone, which contains about 300 M parameters. Despite the smaller model size and that our model generates samples in one step, it outperforms larger diffusion models with many NFEs on FID (Tab. 8).

### Recall

We evaluate the recall  of our model on each dataset to quantify sample diversity. In general, our model achieves a recall that is similar to or marginally worse than the diffusion model counterpart, yet superior to existing GAN models. For CIFAR-10, the recall of our model peaked at 0.57; as a point of comparison, StyleGAN-XL  has a worse recall of 0.47 despite its lower FID. For FFHQ, we obtain a recall of 0.53 at 64\(\)64 and 0.49 at 256\(\)256, whereas StyleGAN2  achieved a recall of 0.43 on FFHQ-256. Our ImageNet-32 model achieved a recall of 0.63; comparable to ADM . Our ImageNet-64 model achieved recall 0.59. While this is slightly worse than \(\)0.63 that many diffusion models achieve, it is better than BigGAN-deep  which achieved a recall of 0.48.

## 5 Discussion and Limitations

We have shown that a simplification of GANs is possible for image generation tasks, built upon a more stable RpGAN\(+R_{1}+R_{2}\) objective with mathematically-demonstrated convergence properties that still provides diverse output. This stability is what lets us re-engineer a modern network architecture without the tricks of previous methods, producing the R3GAN model with competitive FID on the common datasets of Stacked-MNIST, FFHQ, CIFAR-10, and ImageNet as an empirical demonstration of the mathematical benefits.

The focus of our work is to elucidate the essential components of a minimum GAN for image generation. As such, we prioritize simplicity over functionality--we do not claim to beat the performance of every existing model on every dataset or task; merely to provide a new simpleas image editing or controllable generation, as our model lacks dedicated features for easy image inversion or disentangled image synthesis. For instance, we remove style injection functionality from StyleGAN even though this has a clear use. We also omitted common techniques that have been shown in previous literature to improve FID considerably. Examples include some form of adaptive normalization modulated by the latent code [7; 33; 29; 98; 58; 89; 66], and using multiheaded self attention at lower resolution stages [7; 33; 34]. We aim to explore these techniques in a future study.

Further, our work is limited in its evaluation of the scalability of R3GAN models. While they show promising results on 64\(\)64 ImageNet, we are yet to verify the scalability on higher resolution ImageNet data or large-scale text to image generation tasks .

Finally, as a method that can improve the quality of generative models, it would be amiss not to mention that generative models--especially of people--can cause direct harm (e.g., through personalized deep fakes) and societal harm through the spread of disinformation (e.g., fake influencers).

## 6 Conclusion

This work introduced R3GAN, a new baseline GAN that features increased stability, leverages modern architectures, and does not require ad-hoc tricks that are commonplace in existing GAN models. Central to our approach is a regularized relativistic loss that provably features local convergence and that improves the stability of GAN training. This stable loss enables us to ablate various tricks that were previously necessary in GANs, and incorporate in their place modern deep architectures. The resulting streamlined baseline achieves competitive performance to SOTA models within its parameter size class. We anticipate that our backbone will help to drive future GAN research.

Figure 4: Qualitative examples of sample generation from our Config E on FFHQ-256.

Acknowledgements.The authors thank Xinjie Jayden Yi for contributing to the proof and Yu Cheng for helpful discussion. For compute, the authors thank Databricks Mosaic Research. Yiwen Huang was supported by a Brown University Division of Research Seed Award, and James Tompkin was supported by NSF CAREER 2144956. Volodymyr Kuleshov was supported by NSF CAREER 2145577 and NIH MIRA 1R35GM15124301.