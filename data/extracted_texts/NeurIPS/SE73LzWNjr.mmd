# Nearly Optimal VC-Dimension and Pseudo-Dimension Bounds for Deep Neural Network Derivatives

Yahong YANG

Department of Mathematics

The Pennsylvania State University, University Park,

State College, PA, USA

and

Department of Mathematics

Hong Kong University of Science and Technology

Clear Water Bay, Hong Kong SAR, China

yxy5498@psu.edu

Haizhao YANG

Department of Mathematics and Department of Computer Science

University of Maryland College Park

College Park, MD, USA

hzyang@umd.edu

Corresponding author.

Yang XIANG

Department of Mathematics

Hong Kong University of Science and Technology

Clear Water Bay, Hong Kong SAR, China

and

Algorithms of Machine Learning and Autonomous Driving Research Lab

HKUST Shenzhen-Hong Kong Collaborative Innovation Research Institute

Futian, Shenzhen, China

maxiang@ust.hk

###### Abstract

This paper addresses the problem of nearly optimal Vapnik-Chervonenkis dimension (VC-dimension) and pseudo-dimension estimations of the derivative functions of deep neural networks (DNNs). Two important applications of these estimations include: 1) Establishing a nearly tight approximation result of DNNs in the Sobolev space; 2) Characterizing the generalization error of machine learning methods with loss functions involving function derivatives. This theoretical investigation fills the gap of learning error estimations for a wide range of physics-informed machine learning models and applications including generative models, solving partial differential equations, operator learning, network compression, distillation, regularization, etc.

## 1 Introduction

The Sobolev training  of deep neural networks (DNNs) has had a significant impact on scientific and engineering fields, including solving partial differential equations , operator learning , network compression , distillation , regularization , and dynamic programming , etc. For example, Sobolev (semi) norms have been applied to penalize function gradients in loss functions  to control the Lipschitz constant of DNNs. Moreover, Sobolev norms and equivalent formulas are commonly used to define loss functions in various applications such as dynamic programming , solving partial differential equations , and distillation . These loss functions enable models to learn DNNs that can approximate the target function with small discrepancies in both magnitude and derivative. For example, when utilizing the Deep Ritz method  to solve PDEs such as the following one:

\[- u=f&,\\ =0&,\] (1)

the corresponding loss function can be expressed as:

\[_{D}():=_{}|(; {})|^{2}+(_{}(; {}))^{2}-_{}f(;) ,\]

where \(\) represents all the parameters in the neural network. Here, \(\) denotes the domain \((0,1)^{d}\). Proposition 1 in  establishes the equivalence between the loss function \(_{D}()\) and \(\|(;)-u^{*}()\|_{H^{1}((0,1)^{d})}\), where \(u^{*}()\) denotes the exact solution of the PDEs in equation (1), and \(\|f\|_{H^{1}((0,1)^{d})}:=(_{0|| 1}\|D^{}f\|_{ L^{2}((0,1)^{d})}^{p})^{1/2}\). Thus, the Sobolev norm \(H^{1}((0,1)^{d})\) serves as a measure of the loss function, and Sobolev training is employed to solve PDEs within the Deep Ritz method.

Two natural questions that arise are: 1) What is the optimal approximation error of DNNs described by a Sobolev norm? 2) What is the generalization error of the loss function defined by a Sobolev norm? The key step to address these questions is to estimate the optimal Vapnik-Chervonenkis dimension (VC-dimension) and pseudo-dimension  of DNNs and their derivatives. Intuitively, these concepts characterize the complexity or richness of a function set and, hence, they can be applied to establish the best possible approximation and generalization power of DNNs.

**Definition 1** (VC-dimension ).: _Let \(H\) denote a class of functions from \(\) to \(\{0,1\}\). For any non-negative integer \(m\), define the growth function of \(H\) as_

\[_{H}(m):=_{x_{1},x_{2},,x_{m}}|\{(h(x_{1}),h( x_{2}),,h(x_{m})):h H\}|.\]

_The Vapnik-Chervonenkis dimension (VC-dimension) of \(H\), denoted by \((H)\), is the largest \(m\) such that \(_{H}(m)=2^{m}\). For a class \(\) of real-valued functions, define \(():=(())\), where \(():=\{(f):f\}\) and \((x)=1[x>0]\)._

**Definition 2** (pseudo-dimension ).: _Let \(\) be a class of functions from \(\) to \(\). The pseudo-dimension of \(\), denoted by \(()\), is the largest integer \(m\) for which there exists \((x_{1},x_{2},,x_{m},y_{1},y_{2},,y_{m})^{m} ^{m}\) such that for any \((b_{1},,b_{m})\{0,1\}^{m}\) there is \(f\) such that \( i:f(x_{i})>y_{i} b_{i}=1\)._

The main contribution of this paper is to estimate nearly optimal bounds of the VC-dimension and pseudo-dimension of DNN derivatives. Based on these bounds, we can prove the optimality of our DNN approximation, as measured by Sobolev norms (Theorem 3), and obtain a tighter generalization error of loss functions defined by Sobolev norms. Our results facilitate the understanding of Sobolev training and the performance of DNNs in Sobolev spaces.

Bounds for the VC-dimension and pseudo-dimension of DNNs have been established in . However, these approaches and findings cannot be applied to Sobolev training, as they do not account for the derivatives of DNNs, which represent a key difference between Sobolev training and classical methods. Obtaining such bounds for DNN derivatives is much more difficult due to their complex compositional structures. DNN derivatives consist of a series of interdependent parts that are multiplied together via the chain rule, rendering existing methods for estimating bounds inapplicable. Estimating the VC-dimension and pseudo-dimension of DNN derivatives is the most crucial and challenging problem addressed in this paper. In , the VC-dimension and pseudo-dimension of DNN derivatives were analyzed, but the results were suboptimal due to a lack of consideration for the relationships between the multiplied terms in a DNN derivative. As a result, their findings do not provide the optimal approximation of DNNs in Sobolev spaces and can only give a generalization error that is much larger than the actual error that may arise from Sobolev training. In this paper, we introduce a novel method that investigates these relationships, resulting in a simplified complexity of DNN derivatives. This, in turn, allows us to obtain nearly optimal bounds on their VC-dimension and pseudo-dimension.

The paper is divided into two parts. In the first part, we establish a nearly optimal bound on the VC-dimension of DNN derivatives with the ReLU activation function \(_{1}(x):=\{0,x\}\):

**Theorem 1**.: _For any \(N,L,d_{+}\), there exists a constant \(\) independent with \(N,L\) such that_

\[(D)N^{2}L^{2}_{2}L_{2}N,\] (2)

_for_

\[D:=\{=D_{i}:,\;i=1,2,,d\},\] (3)

_where \(:=\{:_{1}^{d}  N L\}\), and \(D_{i}\) is the weak derivative in the \(i\)-th variable._

By utilizing Theorem 1, we prove that our DNN approximation rate for approximating functions in Sobolev spaces \(W^{n,}((0,1)^{d})\) using Sobolev norms in \(W^{1,}((0,1)^{d})\) is nearly optimal. We present our construction of DNNs for this approximation in Theorem 3, and we demonstrate the optimality of such approximation in Theorem 4. Furthermore, we generalize our method to approximate DNNs in Sobolev spaces measured by Sobolev norms \(W^{m,}((0,1)^{d})\) for \(m 2\). The details of this generalization are presented in Corollaries 1 and 2. The Sobolev spaces, equipped with Sobolev (semi) norms, are defined as follows:

**Definition 3** (Sobolev Spaces ).: _Denote \(\) as \((0,1)^{d}\), \(D\) as the weak derivative of a single variable function and \(D^{}=D_{1}^{_{1}}D_{2}^{_{2}} D_{d}^{ _{d}}\) as the partial derivative where \(=[_{1},_{2},,_{d}]^{T}\) and \(D_{i}\) is the derivative in the \(i\)-th variable. Let \(n\) and \(1 p\). Then we define Sobolev spaces_

\[W^{n,p}():=\{f L^{p}():D^{}f L^{p}( )^{d}|| n\}\]

_with a norm \(\|f\|_{W^{n,p}()}:=(_{0|| n}\|D^{}f\|_{L^{p}()}^{p})^{1/p}\), if \(p<\), and \(\|f\|_{W^{n,}()}:=_{0|| n}\|D^{}f\|_{L^{}()}\). Furthermore, for \(=(f_{1},f_{2},,f_{d})\), \( W^{1,}(,^{d})\) if and only if \(f_{i} W^{1,}()\) for each \(i=1,2,,d\) and \(\|\|_{W^{1,}(,^{d})}:=_{i=1,,d} \{\|f_{i}\|_{W^{1,}()}\}\)._

**Definition 4** (Sobolev semi-norm ).: _Let \(n_{+}\) and \(1 p\). Then we define Sobolev semi-norm \(|f|_{W^{n,p}()}:=(_{||=n}\|D^{}f\|_{L^ {p}()}^{p})^{1/p}\), if \(p<\), and \(|f|_{W^{n,}()}:=_{||=n}\|D^{}f\|_{L^ {}()}\). Furthermore, for \( W^{1,}(,^{d})\), we define \(||_{W^{1,}(,^{d})}:=_{i=1,,d}\{| f_{i}|_{W^{1,}()}\}\)._

In the second part of our paper, we utilize our previous work on estimating the VC-dimension of DNN derivatives to obtain an upper bound on the pseudo-dimension of DNN derivatives:

**Theorem 2**.: _For any \(N,L,d_{+}\), there exists a constant \(\) independent with \(N,L\) such that_

\[(D)N^{2}L^{2}_{2}L_{2}N,\] (4)

_where \(D\) is defined in Theorem 1._

Based on Theorem 2, we can estimate the generalization error of loss functions defined by Sobolev norms, as demonstrated in Theorem 5. Specifically, the error is bounded by \((NL(_{2}N_{2}L)^{1/2})\) with respect to the width \(N\) and depth \(L\) of DNNs. This bound is significantly smaller than the previously reported bound of \((NL^{5/2}(_{2}N_{2}L)^{1/2})\) in . We attribute this improvement to our more accurate estimation of the pseudo-dimension of DNN derivatives. Our findings indicate that learning target functions with loss functions defined by Sobolev norms does not require substantially more sample points than those defined by \(L^{2}\)-norms , as their generalization error orders are equivalent with respect to the width \(N\) and depth \(L\) of DNNs.

Our main contributions are:

\(\) We propose a method to achieve nearly optimal estimations of the VC-dimension and pseudo-dimension of DNN derivatives.

\(\) By utilizing our estimation of the VC-dimension of DNN derivatives, we demonstrate the optimality of our DNN approximation, as measured by Sobolev norms.

\(\) By applying our estimation of the pseudo-dimension of DNN derivatives, we obtain a bound for the generalization error measured by the Sobolev norm. Importantly, our results demonstrate that the degree of generalization error defined by Sobolev norms is equivalent to that defined by \(L^{2}\)-norms, corresponding to the width \(N\) and depth \(L\) of DNNs.

## 2 Preliminaries

Let us summarize all basic notations used in the DNNs as follows:

**1**. Matrices are denoted by bold uppercase letters. For example, \(^{m n}\) is a real matrix of size \(m n\) and \(^{}\) denotes the transpose of \(\).

**2**. Vectors are denoted by bold lowercase letters. For example, \(^{n}\) is a column vector of size \(n\). Furthermore, denote \((i)\) as the \(i\)-th elements of \(\).

**3**. For a \(d\)-dimensional multi-index \(=[_{1},_{2},_{d}]^{d}\), we denote several related notations as follows: \((a)\)\(||=|_{1}|+|_{2}|++|_{d}|\); \((b)\)\(^{}=x_{1}^{_{1}}x_{2}^{_{2}} x_{d}^{_{d}},\)\(=[x_{1},x_{2},,x_{d}]^{}\); \((c)\)\(!=_{1}!_{2}!_{d}!\).

**4**. Let \(B_{r,||}()^{d}\) be the closed ball with a center \(^{d}\) and a radius \(r\) measured by the Euclidean distance. Similarly, \(B_{r,\|\|_{_{}}}()^{d}\) be the closed ball with a center \(^{d}\) and a radius \(r\) measured by the \(_{}\)-norm.

**5**. Assume \(_{+}^{n}\), then \(f()=(g())\) means that there exists positive \(C\) independent of \(,f,g\) such that \(f() Cg()\) when all entries of \(\) go to \(+\).

**6**. Define \(_{1}(x):=(x)=\{0,x\}\) and \(_{2}:=^{2}(x)\). We call the neural networks with activation function \(_{t}\) with \(t i\) as \(_{i}\) neural networks (\(_{i}\)-NNs). With the abuse of notations, we define \(_{i}:^{d}^{d}\) as \(_{i}()=[_{i}(x_{1})\\ \\ _{i}(x_{d})]\) for any \(=[x_{1},,x_{d}]^{T}^{d}\).

**7**. Define \(L,N_{+}\), \(N_{0}=d\) and \(N_{L+1}=1\), \(N_{i}_{+}\) for \(i=1,2,,L\), then a \(_{i}\)-NN \(\) with the width \(N\) and depth \(L\) can be described as follows:

\[=}_{0},b_{1}}}{{}} _{1}}}{{}}}_{1},b_{L}}}{{}}_{L} }}{{}}}_{L} ,b_{L+1}}}{{}}()=_{L+1},\]

where \(_{i}^{N_{i} N_{i-1}}\) and \(_{i}^{N_{i}}\) are the weight matrix and the bias vector in the \(i\)-th linear transform in \(\), respectively, i.e., \(_{i}:=_{i}}_{i-1}+_{i},\) for \(i=1,,L+1\) and \(}_{i}=_{i}(_{i}),\) for \(i=1,,L.\) In this paper, an DNN with the width \(N\) and depth \(L\), means (a) The maximum width of this DNN for all hidden layers less than or equal to \(N\). (b) The number of hidden layers of this DNN less than or equal to \(L\).

## 3 Nearly Optimal Approximation Results of DNNs in Sobolev Spaces

Measured by Sobolev Norms

### Approximation of functions in \(W^{n,}\) with \(W^{1,}\) norm by ReLU neural networks

In this subsection, we construct deep neural networks (DNNs) with a width of \((N N)\) and a depth of \((L L)\) to approximate functions in the Sobolev space \(W^{n,}\), as measured by Sobolev norms in \(W^{1,}\). The approximation rate achieved by these networks is \((N^{-2(n-1)/d}L^{-2(n-1)/d})\).

**Theorem 3**.: _For any \(f W^{n,}((0,1)^{d})\) with \(n 2\) and \(\|f\|_{W^{n,}((0,1)^{d})} 1\), any \(N,L_{+}\), there is a \(_{1}\)-NN \(\) with the width \((34+d)2^{d}n^{d+1}(N+1)_{2}(8N)\) and depth \(56d^{2}n^{2}(L+1)_{2}(4L)\) such that_

\[\|f()-()\|_{W^{1,}((0,1)^{d})} C_{9}(n,d)N^{-2(n-1)/d }L^{-2(n-1)/d},\]

_where \(C_{9}\) is the constant independent with \(N,L\)._The proof of Theorem 3 can be outlined in five parts, and the complete proof is provided in Appendix 7.2:

**(i)**: First of all, define a sequence of subsets of \(\):

**Definition 5**.: _Given \(K,d^{+}\), and for any \(=(m_{1},m_{2},,m_{d})\{1,2\}^{d}\), we define \(_{}:=_{j=1}^{d}_{m_{j}},\) where \(_{1}:=_{i=0}^{K-1}[,+ ],\ _{2}:=_{i=0}^{K}[-,+ ]\)._

Then we define a partition of unity \(\{g_{}\}_{\{1,2\}^{d}}\) on \((0,1)^{d}\) with supp \(g_{}(0,1)^{d}_{}\) for each \(\{1,2\}^{d}\):

**Definition 6**.: _Given \(K,d_{+}\), we define_

\[g_{1}(x):=1,&x[+,+ ]\\ 0,&x[+,]\\ 4K(x-),&x[,+ ]\\ -4K(x--),&x[+,+],\ g_{2}(x):=g_{1}(x+),\] (5)

_for \(i\). For any \(=(m_{1},m_{2},,m_{d})\{1,2\}^{d}\), define \(g_{}()=_{j=1}^{d}g_{m_{j}}(x_{j}),\ =(x_{1},x_{2},,x_{d})\)._

**(ii)**: Then we use the following proposition to approximate \(\{g_{}\}_{\{1,2\}^{d}}\) by \(_{1}\)-NNs and construct a sequence of \(_{1}\)-NNs \(\{_{}\}_{\{1,2\}^{d}}\):

**Proposition 1**.: _Given any \(N,L,n_{+}\) for \(K= N^{1/d}^{2} L^{2/d}\), then for any \(=(m_{1},m_{2},,m_{d})\{1,2\}^{d}\), there is a \(_{1}\)-NN with the width smaller than \((9+d)(N+1)+d-1\) and depth smaller than \(15d(d-1)nL\) such as \(\|_{}()-g_{}()\|_{W^{1,}((0,1)^{d})} 50d^{ }(N+1)^{-4dnL}\)._

The proof of Proposition 1 is presented in Appendix 7.2.1.

**(iii)**: For each \(_{}^{d}\), where \(\{1,2\}^{d}\), we find a function \(f_{K,}\) satisfying

\[\|f-f_{K,}\|_{W^{1,}(_{})}  C_{1}(n,d)K^{-(n-1)},\] \[\|f-f_{K,}\|_{L^{}(_{})}  C_{1}(n,d)K^{-n},\] (6)

where \(C_{1}\) is a constant independent of \(K\). Moreover, each \(f_{K,}\) can be expressed as \(f_{K,}=_{|| n-1}g_{f,,}() ^{}\), where \(g_{f,,}()\) is a piecewise constant function on \(_{}\). The proof of this result is based on the Bramble-Hilbert Lemma [7, Lemma 4.3.8], and the details are provided in Appendix 7.2.2.

**(iv)**: The fourth step involves approximating \(f_{K,}\) using neural networks \(_{}\), following the approach outlined in . This method is suitable for our work because \(g_{f,,}()\) is a piecewise constant function on \(_{}\), and the weak derivative of \(g_{f,,}()\) on \(_{}\) is zero. This property allows for the use of the \(L^{}\) norm approximation method presented in . Thus, we obtain a neural network \(_{}\) with width \((N N)\) and depth \((L L)\) such that

\[\|f_{K,}-_{}()\|_{W^{1,}(_{ })}  C_{5}(n,d)N^{-2(n-1)/d}L^{-2(n-1)/d}\] \[\|f_{K,}-_{}()\|_{L^{}(_{})}  C_{5}(n,d)N^{-2n/d}L^{-2n/d},\] (7)

Figure 1: The schematic diagram of \(g_{i}\) for \(i=1,2\).

where \(C_{5}\) is a constant independent of \(N\) and \(L\).

By combining (iii) and (iv) and setting \(K= N^{1/d}^{2} L^{2/d}\), we obtain that for each \(\{1,2\}^{d}\), there exists a neural network \(_{}\) with width \((N N)\) and depth \((L L)\) such that

\[\|f()-_{}()\|_{W^{1,}(_{ })}  C_{6}(n,d)N^{-2(n-1)/d}L^{-2(n-1)/d}\] \[\|f()-_{}()\|_{L^{}(_{})}  C_{6}(n,d)N^{-2n/d}L^{-2n/d},\] (8)

where \(C_{6}\) is a constant independent of \(N\) and \(L\). Further details are provided in Appendix 7.2.3.

**(v)**: The final step is to combine the sequences \(\{_{}\}_{\{1,2\}^{d}}\) and \(\{_{}\}_{\{1,2\}^{d}}\) to construct a network that can approximate \(f\) over the entire space \(^{d}\). We define the sequence \(\{_{}\}_{\{1,2\}^{d}}\) because \(_{}\) may not accurately approximate \(f\) on \(^{d}_{}\). The purpose of \(_{}\) is to remove this portion of the domain and allow other networks to approximate \(f\) on \(^{d}_{}\). Further details on this step are provided in Appendix 7.2.4.

While recent works [28; 23; 39; 18; 31; 9; 19] have studied the approximation of smooth functions or functions in Sobolev spaces by DNNs measured in the norm of \(L^{p}()\) or \(W^{s,p}()\), they typically present results that are not optimal or are measured in \(L^{p}\)-norms. For example, in , they applies Taylor's expansion to approximate smooth functions but cannot be applied directly in Sobolev spaces. In , they improve on this by using the Bramble-Hilbert Lemma to approximate functions in Sobolev spaces, but their error is still measured in \(L^{p}\)-norms. In , the authors show that there exists a ReLU neural network that can approximate \(f W^{1,p}()\), but their approximation rate is not optimal and is the same as that in traditional methods such as the finite element theory. Our work provides a superior approximation rate. Later, a rigorous proof of optimality of Theorem 3 is discussed in Appendix 7.2.4 and Subsection 3.3.

Approximation of functions in \(W^{n,}\) measured by \(W^{m,}\) norm with \(m>1\) by neural networks (sketches of the proofs of the Corollaries 1 and 2)

In this subsection, we utilize neural networks to approximate functions in \(W^{n,}\) measured by \(W^{m,}\), where \(m>1\). The proof strategy is similar to the approximation measured in the norm of \(W^{1,}\). However, we cannot rely on ReLU neural networks alone to achieve this goal, as ReLU neural networks are piece-wise linear functions that do not belong to \(W^{m,}\) with \(m>1\). Note that the Bramble-Hilbert Lemma is still applicable in higher-order approximation. Therefore, we need to approximate the function \(f_{K,}=_{|| n-1}g_{f,,}()^{}\) within each domain \(_{}\) using DNNs, where \(g_{f,,}(x)\) represents a piece-wise constant function within \(_{}\). Consequently, ReLU-based DNNs can effectively approximate \(g_{f,,}(x)\) within \(_{}\) when measured by higher-order Sobolev spaces, since both the higher-order derivatives of \(g_{f,,}(x)\) and ReLU-based DNNs are zero. The parts of ReLU-based DNNs that do not have high-order derivatives appear in the domain \(_{}\), and we will use the partition of unity to ensure that this part disappears in the final presentation, this is the reason why it is still acceptable to have ReLU activations appearing in the network. However, when it comes to approximating \(^{}\) for \(||>1\) based on high-order Sobolev norms, ReLU-based DNNs fail to provide accurate results. Therefore, we require DNNs that utilize the square of ReLU activation in this specific scenario. This is how we construct our approach and the reason why we employ both ReLU and the square of ReLU activation functions.

Instead, we examine the use of \(_{2}\) neural networks for approximating functions measured in the norm of \(W^{2,}\). As per Corollary 1, a neural network with \((N N)\) width and \((L L)\) depth can achieve a nonasymptotic approximation rate of \((N^{-2(n-2)/d}L^{-2(n-2)/d})\) with respect to the \(W^{2,}((0,1)^{d})\) norm. Moreover, our method can be extended to approximations measured in the norm of \(W^{m,}\) with \(m>2\), as shown in Corollary 2. The proof strategy is similar to that used in Subsection 3.1, except that we need to construct a smoother partition of unity rather than \(\{g_{}\}_{\{1,2\}^{d}}\). The corollaries are presented below, and further details are provided in Appendix 7.3.

**Corollary 1**.: _For any \(f W^{n,}((0,1)^{d})\) with \(\|f\|_{W^{n,}((0,1)^{d})} 1\), any \(N,L_{+}\) with \(NL+2^{_{2}N}\{d,n\}\) and \(L_{2}N\), there is a \(_{2}\)-NN \(()\) with the width \(2^{d+6}n^{d+1}(N+d)_{2}(8N)\) and depth \(15n^{2}(L+2)_{2}(4L)\) such that_

\[\|f()-()\|_{W^{2,}((0,1)^{d})} 2^{d+7}C_{10}(n,d)N^{-2(n -2)/d}L^{-2(n-2)/d},\]_where \(C_{10}\) is the constant independent with \(N,L\)._

**Corollary 2**.: _For any \(f W^{n,}((0,1)^{d})\) with \(\|f\|_{W^{n,}((0,1)^{d})} 1\), any \(N,L,m_{+}\) with \(NL+2^{_{2}N}\{d,n\}\) and \(L_{2}N\), there is a \(_{2}\)-NN \(()\) with the width \((N N)\) and depth \((L L)\) such that_

\[\|f()-()\|_{W^{m,}((0,1)^{d})} C_{11}(n,d,m)N^{-2 (n-m)/d}L^{-2(n-m)/d},\]

_where \(C_{11}\) is the constant independent with \(N,L\)._

### Optimality of Theorem 3 via estimation of VC-dimension of DNN derivatives (Theorem 1)

In this section, we demonstrate that the approximation rate presented in Theorem 3 is nearly asymptotically optimal:

**Theorem 4**.: _Given any \(,C_{1},C_{2},C_{3},J_{0}>0\) and \(n,d^{+}\), there exist \(N,L\) with \(NL J_{0}\) and \(f\) with \(\|f\|_{W^{n,}((0,1)^{d})} 1\), satisfying for any \(_{1}\)-NN \(\) with the width smaller than \(C_{1}N N\) and depth smaller than \(C_{2}L L\), we have_

\[|-f|_{W^{1,}((0,1)^{d})}>C_{3}L^{-2(n-1)/d-}N^{-2(n-1)/d-}.\] (9)

In other words, the approximation rate of \((N^{-2(n-1)/d-}K^{-2(n-1)/d-})\) cannot be achieved asymptotically when ReLU \(_{1}\)-NNs with width \((N N)\) and depth \((L L)\) to approximate functions in \(_{n,d}:=\{f W^{n,}((0,1)^{d}):\|f\|_{W^{n,}((0, 1)^{d})} 1\}\). The proof of Theorem 4 is based on the estimation of the VC-dimension of DNN derivatives, which is provided in Theorem 1.

Theorem 1 plays a crucial role in our proof of Theorem 4, which is established through a proof by contradiction following the approach outlined in Ref. . Further details on the proof can be found in Appendix 7.4. The main idea behind the proof is that Theorem 1 characterizes the complexity of DNN derivatives, which in turn limits the ability of DNNs to approximate functions in Sobolev spaces.

In this paper, we focus on the optimality of approximation rate with respect to width \(N\) and depth \(L\) of DNNs. The dimensionality \(d\) is not the focus that we consider in our research. Addressing the question about mitigating the exponential dependence of width on dimensionality, we have observed that this arises from the utilization of methods like Taylor's expansion or average Taylor polynomials in our approximation techniques. It remains an open question for future research to explore alternative approaches to address the challenge of getting the dependence of \(d\) in the lower bounds.

Generalization Analysis in Sobolev Spaces via Estimation of Pseudo-dimension of DNN Derivatives (Theorem 2)

In a typical supervised learning algorithm, the objective is to learn a high-dimensional target function \(f()\) defined on \((0,1)^{d}\) with \(\|f\|_{W^{n,}((0,1)^{d})} 1\) from a finite set of data samples \(\{(_{i},f(_{i}))\}_{i=1}^{M}\). When training a DNN, we aim to identify a DNN \((;_{S})\) that approximates \(f()\) based on random data samples \(\{(_{i},f(_{i}))\}_{i=1}^{M}\). We assume that \(\{_{i}\}_{i=1}^{M}\) is an i.i.d. sequence of random variables uniformly distributed on \((0,1)^{d}\) in this section. Denote

\[_{D} :=_{}_{D}():=_{ }_{(0,1)^{d}}|(f()-(;))|^{2} +|f()-(;)|^{2}\,,\] (10) \[_{S} :=_{}_{S}():=_{ }_{i=1}^{M}[|(f(_{i})-(_{ i};))|^{2}+|f(_{i})-(_{i};)|^{2}].\] (11)

The overall inference error is \(_{D}(_{S})\), which can be divided into two parts:

\[_{D}(_{S})= _{D}(_{D})+_{S}(_{D})-_{D}(_{D})+_{S}(_{S})-_{S}(_{D})+ _{D}(_{S})-_{S}(_{S})\] \[ _{D}(_{D})}_{}+_{S}(_{D})-_{D}(_{D})+_{D}(_{S})-_{S}(_{S})}_{}\] (12)

where the last inequality is due to \(_{S}(_{S})_{S}(_{D})\) by the definition of \(_{S}\).

Due to Theorem 3, we know that the approximation error \(_{D}(_{D})\) is a \((N^{-4(n-1)/d}L^{-4(n-1)/d})\) term since \(\|f()-()\|_{H^{1}((0,1)^{d})}\|f()-()\|_{W^{ 1,}((0,1)^{d})}\). In this section, we bound generalization error in the \(H^{1}((0,1)^{d})\) sense:

**Theorem 5**.: _For any \(N,L,d,B,C_{1},C_{2}\), if \((;_{D}),(;_{S})\), we will have that there are constants \(C_{5}=C_{5}(B,d,C_{1},C_{2})\) and \(J=J(d,N,L,C_{1},C_{2})\) such that for any \(M J\), we have_

\[_{S}(_{D})-_{D}(_{D})+_{D}(_{S})- _{S}(_{S})  2_{,(;)} |(_{S}())-_{D}()|\] \[ C_{5}L_{2}N)^{}}{}  M.\] (13)

_where \(:=\{: C_{1}N N C_{2}L L,\|\|_{W^{1,}((0,1)^{d})} B\}\), and \(_{S},_{D},_{S},_{D}\) are defined in Eqs. (10,11)._

The proof of Theorem 5 is based on the works of . We begin by bounding the generalization error using the Rademacher Complexity and then bound the Rademacher Complexity by the uniform covering number. We further bound the uniform covering number by the pseudo-dimension. Finally, we estimate the pseudo-dimension by Theorem 2. The proof of Theorem 5 is presented in Appendix 7.5

Theorem 2 helps to control the degree of the generalization error with respect to \(N\) and \(L\) in Theorem 5. In , the generalization error is bounded by \((NL^{})\). In , the authors estimate the covering number using the Lipschitz condition of DNNs instead of the pseudo-dimension, leading to a generalization error that is exponentially dependent on the depth of the DNNs. Our result is much better than them due to the optimal estimation of pseudo-dimension of DNN derivatives (Theorem 2).

## 5 Proof Sketches for Theorems 1 and 2

As Theorems 1 and 2 address the estimation of VC-dimension and pseudo-dimension of DNN derivatives, which is the main contribution of this paper, we provide the proofs for these theorems in this section. The distinction in our approach compared to that of  lies in the fact that the application of the chain rule requires the consideration of correlations among distinct segments of the deep neural networks, as opposed to treating them as independent components multiplied together.

In the proof of Theorem 1, we use the following lemmas:

**Lemma 1** ([4, Lemma 17],[3, Theorem 8.3]).: _Suppose \(W M\) and let \(P_{1},,P_{M}\) be polynomials of degree at most \(D\) in \(W\) variables. Define \(K:=\{((P_{1}(a)),,(P_{M}(a) )):a^{W}\}\), then we have \(K 2(2eMD/W)^{W}\)._

**Lemma 2** ([4, Lemma 18]).: _Suppose that \(2^{m} 2^{t}(mr/w)^{w}\) for some \(r 16\) and \(m w t 0\). Then, \(m t+w_{2}(2r_{2}r)\)._

As the proof of Theorem 1 represents the most critical and challenging question in our work, we present a sketch of it below.

Proof Sketch of Theorem 1.: An element in \(\) can be represented as \(=_{L+1}_{1}(_{L}_{1}(_{1}(_{1} {x}+_{1}))+_{L})+b_{L+1}\). Therefore, an element in \(D\) can be represented as

\[()=D_{i}()= _{L+1}_{0}(_{L}_{1}(_{1}(_{1}+_{1}))+_{L})\] \[_{L}_{0}(_{1}(_{1}+ _{1}))_{2}_{0}(_{1}+_{1})(_{1} )_{i},\] (14)

where \(_{i}^{N_{i} N_{i-1}}\) (\(()_{i}\) is \(i\)-th column of \(\)) and \(_{i}^{N_{i}}\) are the weight matrix and the bias vector in the \(i\)-th linear transform in \(\), and \(_{0}()=(x)=1[x>0]\), which is the derivative of the ReLU function and \(_{0}()=(_{0}(x_{i}))\).

Let \(^{d}\) be an input and \(^{W}\) be a parameter vector in \(\). We denote the output of \(\) with input \(\) and parameter vector \(\) as \(f(,)\). For fixed \(_{1},_{2},,_{m}\) in \(^{d}\), we aim to bound

\[K:=\{((f(_{1},)),, {sgn}(f(_{m},))):^{W}\}\,.\] (15)

[MISSING_PAGE_FAIL:9]

this paper. Firstly, we show that the optimal approximation rate of DNNs with a width of \((N N)\) and a depth of \((L L)\) is \((N^{-2(n-1)/d}L^{-2(n-1)/d})\) in Sobolev spaces. This demonstrates the ability of DNNs to learn target functions well in Sobolev training. Secondly, we find that the degree of the pseudo-dimension of DNN derivatives is the same as that for DNNs corresponding to the width \(N\) and depth \(L\) of DNNs. This result suggests that despite the apparent complexity of DNN derivatives, the degree of generalization error of loss functions containing derivatives of DNNs is equivalent to that without derivatives, corresponding to the width \(N\) and depth \(L\) of DNNs. As a result, we do not need to use a significantly larger number of sample points to learn the target function in Sobolev training compared to regular training.

The estimations of the VC-dimension and pseudo-dimension of DNN derivatives have broad applications in deep learning research. For example, in classification tasks, the VC-dimension characterizes the uniform convergence of misclassification frequencies to probabilities and asymptotically determines the sample complexity of PAC learning . These applications can be explored in the further work. Our focus in this paper is on the Sobolev training with loss functions containing first-order derivatives, and we also obtain the approximation rate of \(_{2}\)-NNs described by higher-order Sobolev norms (Corollaries 1 and 2). The optimality of these results and the generalization error of Sobolev training with loss functions containing higher-order derivatives of DNNs remain open problems, as estimating the VC-dimension and pseudo-dimension of higher-order derivatives of \(_{2}\)-NNs requires further investigation.