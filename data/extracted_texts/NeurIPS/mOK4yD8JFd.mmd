# Quality-Improved and Property-Preserved

Polarimetric Imaging via Complementarily Fusing

Chu Zhou\({}^{1}\)   Yixing Liu\({}^{2,3}\)   Chao Xu\({}^{4}\)   Boxin Shi\({}^{2,3}\)

\({}^{1}\)National Institute of Informatics, Japan

\({}^{2}\)State Key Laboratory for Multimedia Information Processing, School of CS, Peking University, China

\({}^{3}\)National Engineering Research Center of Visual Technology, School of CS, Peking University, China

\({}^{4}\)National Key Laboratory of General Artificial Intelligence, School of IST, Peking University, China

zhou_chu@hotmail.com,

{luiginixy@stu., xuchao@cis., shiboxin@}pku.edu.cn

###### Abstract

Polarimetric imaging is a challenging problem in the field of polarization-based vision, since setting a short exposure time reduces the signal-to-noise ratio, making the degree of polarization (DoP) and the angle of polarization (AoP) severely degenerated, while if setting a relatively long exposure time, the DoP and AoP would tend to be over-smoothed due to the frequently-occurring motion blur. This work proposes a polarimetric imaging framework that can produce clean and clear polarized snapshots by complementarily fusing a degraded pair of noisy and blurry ones. By adopting a neural network-based three-phase fusing scheme with specially-designed modules tailored to each phase, our framework can not only improve the image quality but also preserve the polarization properties. Experimental results show that our framework achieves state-of-the-art performance.

+
Footnote †: Corresponding author.

+
Footnote †: 1}\)A polarized snapshot consists of four polarized images with different polarizer angles (\(0^{},45^{},90^{},135^{}\)), which can be captured using a polarization camera in a single shot or using a polarizer in multiple shots.

## 1 Introduction

Polarimetric imaging aims to obtain the degree of polarization (DoP) and the angle of polarization (AoP) of the scene to provide physical clues for downstream polarization-based vision applications (_e.g._, reflection removal , shape from polarization , dehazing , _etc._). In practice, the DoP and AoP cannot be captured directly, but are usually calculated from polarized snapshots1 in an indirect manner. However, since a polarizer would block part of the light, selecting an appropriate exposure time could be challenging, making the captured polarized snapshots often degrade [15; 33]. As shown in Fig. 1 (left), setting a short exposure time would result in a very low signal-to-noise ratio (SNR), making the DoP and AoP severely degenerated; while if setting a relatively long exposure time to increase the SNR, motion blur is more likely to occur, making the DoP and AoP over-smoothed, as shown in Fig. 1 (middle). To deal with the above issues, several methods have been proposed to handle the low-light noise [10; 25; 15; 32] or motion blur  in the polarized images. Since these methods can work in a polarization-aware manner (_i.e._, they explicitly take the preservation of polarization properties into consideration), they usually demonstrate higher performance compared with the corresponding methods designed for conventional images [3; 29; 14; 23]. However, due to the ill-posedness of the problems they face, the quality of their results is still limited.

Considering the fact that different types of degraded polarized snapshots would provide complementary knowledge, _i.e._, the short-exposure noisy ones tend to be clear while the long-exposure blurryones tend to be clean, an intuitive strategy to improve the quality of polarimetric imaging could be complementarily fusing a degraded pair of noisy and blurry polarized snapshots. Such a strategy can not only achieve an effect similar to "boosting" (_i.e._, combining multiple weak ones into a strong ones) to produce clean and clear results, but also bring the existing degraded polarized snapshots alive. However, current methods that can fuse noisy and blurry pairs [17; 2; 27; 28] are designed for conventional images, which are not suitable for polarimetric imaging since they cannot preserve the polarization properties, resulting in inaccurate DoP and AoP.

In this paper, we propose a quality-improved and property-preserved polarimetric imaging framework that can produce clean and clear polarized snapshots by complementarily fusing a degraded pair of noisy and blurry ones, as shown in Fig. 1 (right). Specifically, we design a neural network-based three-phase fusing scheme that can explicitly take the preservation of polarization properties into consideration. The first phase is _irradiance restoration_, aiming to restore the polarization-unrelated high-level irradiance information of the scene by recovering the total intensity of the light, where the color and structure cue fusion (CSCF) module is proposed to make full use of the color and structure cues encoded in the Stokes parameters. The second phase is _polarization reconstruction_, aiming to establish the physical correlation between the polarized images by reconstructing the DoP and AoP, where the coherence-aware aggregation (CAG) and coherence injection (CI) modules are proposed to optimize the values of the DoP and AoP in a Cartesian coordinate representation. The third phase is _artifact suppression_, aiming to suppress the artifacts lying in the details by performing refinement in the image domain. Unlike the fusing methods designed for conventional images [17; 2; 27; 28], our framework can fully utilize the complementary knowledge from the noisy and blurry pairs in a polarization-aware manner, by virtue of our three-phase fusing scheme. Different from the polarized image enhancement methods [32; 33] designed for enhancing a single noisy or blurry polarized snapshot, our framework can effectively explore the usage of different physical quantities to improve the overall performance, thanks to the specially-designed modules tailored to each phase. To summarize, this paper makes contributions by demonstrating:

* A quality-improved and property-preserved polarimetric imaging framework, for the first time applying a fusing strategy to polarimetric imaging.
* A neural network-based three-phase fusing scheme, fully utilizing the complementary knowledge from the noisy and blurry pairs in a polarization-aware manner.
* Specially-designed modules tailored to each phase, effectively exploring the usage of different physical quantities to improve the overall performance.

## 2 Related work

**Low-light enhancement and deblurring for polarized images.** There are many methods specially designed for enhancing polarized images, aiming to improve the quality of polarimetric imaging.

Figure 1: In polarimetric imaging, since a polarizer would block part of the light, setting a short exposure time would result in a low SNR, making the DoP and AoP severely degenerated (_left_); while if setting a relative long exposure time to increase the SNR, motion blur is more likely to occur, making the DoP and AoP over-smoothed (_middle_). Our framework can produce clean and clear results with high-quality DoP and AoP by complementarily fusing a degraded pair of noisy and blurry polarized snapshots (_right_).

IPLNet  and ColorPolarNet  adopted residual dense blocks to build the backbone for dealing with multiple polarized low-light noisy images simultaneously. Li _et al_.  proposed a noise modeling method for realistic polarized low-light data synthesis along with a powerful vision Transformer-based network structure to reduce the noise. PLIE  designed a novel Stokes-domain low-light enhancement strategy and proposed a dual-branch network to reduce the artifacts lying in the DoP and AoP. PolDeblur  proposed a polarized image deblurring pipeline along with a two-stage network to remove the motion blur in a polarization-aware manner. However, they are not good at recovering details due to the ill-posedness of the problem they face.

**Image enhancement by fusing noisy and blurry pairs.** In comparison with the image enhancement methods that only take a single degenerated image as the input (_e.g._, low-light enhancement [3; 29] or deblurring [14; 23] methods that focus on processing either a single low-light noisy image or a single blurry image), fusing noisy and blurry pairs could usually achieve higher performance and better generalization ability since additional information can be acquired. Early works are usually based on numerical optimization. Yuan _et al_.  adopted a residual deconvolution process along with a gain-controlled deconvolution process to reduce the overall ringing artifacts during fusing. Choi _et al_.  designed a novel camera system that could capture two blurry images and one noisy image in a single shot, and proposed a motion-based image merging algorithm to merge the captured images into a high-quality one. Son and Park  proposed a patches-based point spread function (PSF) estimation approach by extracting the structure information from the noisy image, along with a channel-dependent deblurring method to obtain the blur-free image. Son _et al_.  proposed a scheme to alternatively estimate the PSF and perform the deconvolution operation on the blurry image using the noisy image as a guiding signal. Gu _et al_.  proposed a method based on Gaussian mixture model to estimate the underlying intensity distribution of the noisy and blurry pairs first and then perform the pixel fusing. Recently, deep neural networks have also been adopted to handle this problem. LSD2  and LSFNet  proposed to use convolutional neural networks to fuse the images in an end-to-end manner. SelfIR  proposed a self-supervised learning strategy to restore the clean and clear image contents. D2HNet  adopted a two-phase pipeline to further increase the visual quality. However, the above methods are designed to enhance the quality of a single input image, which would show inferior performance when handling multiple polarized images.

## 3 Method

### Problem formulation and overall framework

As shown in Fig. 1, our goal is to reconstruct a clean and clear polarized snapshot with preservation of polarization properties (denoted as \(=_{_{1,2,3,4}}\)) from a degraded pair of noisy and blurry polarized snapshots (denoted as \(=_{_{1,2,3,4}}\) and \(=_{_{1,2,3,4}}\) respectively), where \(_{1,2,3,4}=0^{},45^{},90^{},135^{}\) stand for the polarizer angles of the polarized images in the polarized snapshot respectively. Once \(\) becomes available, high-quality DoP \(\) and AoP \(\) could be calculated using

\[=_{1}^{2}+_{2}^{2}}}{_{0} }\;\;\;\;=(_{2} }{_{1}}),\] (1)

where \(_{0,1,2}\) are called the Stokes parameters 2, which can be computed as

\[_{0}=(_{_{1}}+_{ _{2}}+_{_{3}}+_{_{4}})=_{ _{1}}+_{_{3}}=_{_{2}}+_{ _{4}}\\ _{1}=_{_{3}}-_{_{1}}\\ _{2}=_{_{4}}-_{_{2}}.\] (2)

Here, we can see \(_{0}\) describes the total intensity of the light, which is polarization-unrelated. In the following, we will use \(_{0,1,2}^{}\) and \(_{0,1,2}^{}\) to denote the Stokes parameters of \(\) and \(\) respectively.

The overall reconstruction process of our framework can be formulated as maximizing a posteriori of the output \(\) conditioned on the inputs \(\) and \(\) along with the fusing function \(f\) parameterized by \(\):

\[*{argmax}_{}f(|,,).\] (3)To solve this maximum a posteriori estimation problem, we design a neural network-based three-phase fusing scheme to implement the fusing function \(f\), as shown in Fig. 2. First, the irradiance restoration phase restore the polarization-unrelated high-level irradiance information of the scene, by enhancing \(_{0}^{}\) to obtain the coarse value of the total intensity of the light \(_{0}^{}\) under the guidance of the color and structure cues provided by \(_{0}^{}\) and \(_{1,2}^{}\) respectively. Then, we compute \((^{},^{})\), which are the coarse values of the DoP and AoP in a Cartesian coordinate representation, and feed them into the polarization reconstruction phase to obtain the corresponding enhanced values \((^{},^{})\) with the help of \((^{},^{})\) (the DoP and AoP of \(\) in a Cartesian coordinate representation) along with the irradiance clues encoded in \(_{0}^{}\), aiming to establish the physical correlation between the polarized images. Finally, we compute the coarse values of the polarized images \(^{}_{_{1,2,3,4}}\), and adopt an artifact suppression phase to obtain \(_{_{1,2,3,4}}\) that make up the clean and clear polarized snapshot \(\), by suppressing the artifacts in the image domain for increasing the quality of details.

### Phase1: Irradiance restoration

This phase aims to restore the polarization-unrelated high-level irradiance information for providing further guidance. As shown in Fig. 1, since \(\) would retain better contours than \(\), we propose to learn the residual between \(_{0}^{}\) and \(_{0}^{}\) instead of the residual between \(_{0}^{}\) and \(_{0}^{}\). However, \(_{0}^{}\) usually suffers from color bias and noise, which would increase the difficulty of feature extraction, resulting in erroneous global tone and less salient local structure. Fortunately, despite that \(_{0}^{}\) would suffer from motion blur, it still contains undamaged color information due to the relatively high SNR of \(\); besides, \(_{1,2}^{}\) could provide distinctive structure information since both of them describe the difference between two polarized images (see Eq. (2)), which would highlight the regions with high gradients. Therefore, we propose to effectively explore the usage of \(_{0}^{}\) and \(_{1,2}^{}\).

Specifically, we first explicitly adopt two modal-specific feature encoders \(_{}^{}\) and \(_{}^{}\) to extract the multiscale color and structure features \(_{1,2,3}^{}\) and \(_{1,2,3}^{}\) from \(_{0}^{}\) and \(_{1,2}^{}\) respectively for guidance. Then, we propose to use three color and structure cue fusion (CSCF) modules to apply the guidance provided by \(_{1,2,3}^{}\) and \(_{1,2,3}^{}\) to \(_{1,2,3}^{}\) in a successive manner, and output \(_{1,2,3}^{}\) for restoring \(_{0}^{}\). Here, \(_{1,2,3}^{}\) and \(_{1,2,3}^{}\) denote the multiscale input and output features of the CSCF modules respectively, which are extracted from and fed into the encoder and decoder part of a modified autoencoder architecture  where a dense block  is inserted into the coarsest layer.

Figure 2: The workflow of our framework, consisting of three phases: irradiance restoration, polarization reconstruction, and artifact suppression.

**CSCF: color and structure cue fusion.** The CSCF module aims to address the issues of erroneous global tone and less salient local structure in the feature space. Without losing generality, we describe how the \(i\)-th scale (\(i=1,2,3\)) CSCF takes \(_{i}^{}\), \(_{i}^{}\), and \(_{i}^{}\) as the input and output \(_{i}^{}\), as shown in Fig. 3 (a). We first learn a multiplier \(_{i}\) and a bias \(_{i}\) from \(_{i}^{}\) by

\[_{i}=B_{c}(C_{m}(Sigmoid(_{i}^{})))\;\;\;\;_{i}=B_{c}(C_{b}(_{i}^{})),\] (4)

where \(B_{c}\) denotes a bottleneck block  used for feature projection, \(C_{m}\) and \(C_{b}\) denote two different convolution layers. Then, we apply an affine transformation to \(_{i}^{}\) using \(_{i}\) and \(_{i}\), to adjust the color in the feature space for solving the issue of erroneous global tone by

\[_{i}^{}=_{i}_{i}^{}+ _{i},\] (5)

where \(_{i}^{}\) denote the transformed feature, \(\) denotes element-wise product operation. Finally, to solve the issue of less salient local structure, we apply a deformable convolution layer \(D\) to align the gradients and overcome the possible shifts caused by the exposure interval in the feature space by

\[_{i}^{}=D(_{i}^{},_{i },_{i}),\] (6)

where \(_{i}\) and \(_{i}\) are the offsets of sampling points and the modulation scalars learned by

\[_{i}=B_{s}(C_{P}(_{i}^{}))\;\; \;\;_{i}=B_{s}(C_{M}(_{i}^{})),\] (7)

where \(B_{s}\), \(C_{P}\), and \(C_{M}\) denotes another bottleneck block  and convolution layers respectively.

### Phase2: Polarization reconstruction

This phase aims to establish the physical correlation between the polarized images by reconstructing the high-quality DoP and AoP. To achieve it, previous methods usually choose to repair the degenerated values in the image domain [10; 25; 15; 33] or Stokes domain  for an indirect reconstruction, since the degeneration patterns of the DoP and AoP could be more complicated than the polarized images or Stokes parameters due to their non-linearity (see Eq. (1)), which could increase the ill-posedness. In contrast, we propose to reconstruct the DoP and AoP in a Cartesian coordinate representation, which can not only relieve the ill-posedness since the non-linearity reduces, but also optimize the values of the DoP and AoP in a direct manner to prevent error accumulation.

Here, we explain what is the Cartesian coordinate representation of the DoP and AoP: as shown in Fig. 4, if we regard \(\) and \(2\) in Eq. (1) as the magnitude and angle of a vector \(}\) lying inside a unit circle, the Cartesian coordinate representation of \(}\) could be written as \((,)\), which satisfying

\[=_{1}}{_{0}}\;\;\;\;= _{2}}{_{0}}.\] (8)

Figure 4: The Cartesian coordinate representation of the DoP and AoP.

Figure 3: The details of the proposed CSCF (color and structure cue fusion) and CAG (coherence-aware aggregation) modules.

Specifically, we propose to learn the residual between \((^{},^{})\) and \((^{},^{})\), with the help of \((^{},^{})\) and \(^{}_{0}\), where

\[^{}=^{}_{1}}{^{}_{ 0}}\ \,\ \ ^{}=^{}_{2}}{^{}_{ 0}}\ \,\ \ ^{}=^{}_{1}}{^{}_{ 0}}\ \ \ \ ^{}=^{}_{2}}{^{}_{ 0}}.\] (9)

First, we propose to use a coherence-aware aggregation (CAG) module to estimate the coherence volumes \(_{}\) and \(_{}\) from \(^{}\), \(^{}\), and \(^{}_{0}\). Then, we adopt two branches for reconstructing \(^{}\) and \(^{}\) respectively. The first branch contains a feature encoder \(^{}_{}\) and three cascaded coherence injection (CI) modules using \(_{}\) for guidance. Similarly, the second branch contains a feature encoder \(^{}_{}\) and three cascaded CI modules using \(_{}\) for guidance.

**CAG: coherence-aware aggregation.** The CAG module aims to aggregate the priors about the coherence between the polarization properties and the irradiance information. As shown in Fig. 3 (b), it contains two symmetrical parts for estimating \(_{}\) and \(_{}\) respectively. Here, we only describe how to estimate \(_{}\) since the estimation process of \(_{}\) could be similar. We first adopt two bottleneck blocks  to extract polarization features and irradiance features from \(^{}\) and \(^{}_{0}\) respectively. Then, we project the extracted features into the coherence features \(_{}^{N C H W}\) using a convolution layer, where \(N,C,H,W\) represent the batch size, number of channels, height, and width respectively. After that, inspired by CBAM  that can make full use of the inter-channel relationship of features, we propose to learn an attention vector \(_{}^{N C 1 1}\) to recalibrate \(_{}\) for obtaining \(_{}^{N C H W}\):

\[_{}=_{}_{}=P(MLP( Sigmoid(_{})))_{},\] (10)

where \(P\) denotes the global average pooling operation and \(MLP\) denotes a multi-layer perceptron.

**CI: coherence injection.** The CI module aims to inject the priors about coherence into the reconstruction of \(^{}\) and \(^{}\). To enlarge the receptive field and include long-range association, we choose to use Transformer modules  with cross-attention layers. Taking one of the CI module in the first branch (the branch for reconstructing \(^{}\)) as an example, denoting its input as \(^{}_{}\) (from the previous CI module or \(^{}_{}\)) and \(_{}\) (from the CAG module), we let \(^{}_{}\) to serve as the query vector and adopt convolution layers to learn the key vector and the value vector from both \(^{}_{}\) and \(_{}\).

### Phase3: Artifact suppression

With \(^{}_{0}\) and \((^{},^{})\) available, we could compute the coarse values of the polarized images \(^{}_{_{1,2,3,4}}\). However, we should not output \(^{}_{_{1,2,3,4}}\) directly since their quality is still not satisfying. This is because \(^{}_{0}\) and \((^{},^{})\) are estimated from two different phases so that the irradiance-polarization consistency would break, bringing artifacts to the details. Therefore, we add an extra phase to refine \(^{}_{_{1,2,3,4}}\) for increasing the quality of details by suppressing the artifacts in the image domain.

Specifically, we propose to learn the residual between \(^{}_{_{1,2,3,4}}\) and \(_{_{1,2,3,4}}\). According to Eq. (2), we choose to divide \(^{}_{_{1,2,3,4}}\) into two groups (\(^{}_{_{1,3}}\) and \(^{}_{_{2,4}}\)) first to ensure both of them contain the full irradiance information and each of them contains half of the polarization properties. Then, we use two convolution layers to extract the features of \(^{}_{_{1,3}}\) and \(^{}_{_{2,4}}\) respectively, and adopt a feature merger \(_{}\) to obtain the merged dual-group features \(_{}\). After that, a U-Net backbone  is used to perform pixel-wise multi-scale feature refinement on \(_{}\), and another two convolution layers are adopted for decoding the refined dual-group features into the residuals of each group.

## 4 Implementation details

**Loss function.** The total loss function can be written as

\[L=L_{}+L_{}+L_{},\] (11)

which consists of three terms to optimize the three phases respectively: irradiance loss \(L_{}\), polarization loss \(L_{}\), and refinement loss \(L_{}\). The irradiance loss could be written as

\[L_{}=_{}^{}L_{1}(^{}_{0}, ^{}_{0})+_{}^{}L_{}(^{}_{0},^{}_{0}),\] (12)

where \(_{}^{}\) are set to be 10.0 and 0.05 respectively, \(L_{1}\) and \(L_{}\) denote the \(_{1}\) loss and perceptual loss respectively, the subscript gt labels the ground truth throughout this paper. The perceptual loss \(L_{}\) is defined as

\[L_{}(_{0}^{},_{0}^{})=L_{2}(_ {h}(_{0}^{}),_{h}(_{0}^{})),\] (13)

where \(L_{2}\) denotes the \(_{2}\) loss, \(_{h}\) denotes the feature map from \(h\)-th layer of VGG-19 network  pretrained on ImageNet , and we use activations from \(VGG_{3,3}\) convolution layer here. The polarization loss could be written as

\[L_{}=_{}^{}(L_{1}(^{},^{})+L_{1}(^{},^{}))+_{ }^{}(L_{}(^{})+L_{}(^{}))+_{}^{}L_{}^{1}(^{}, ^{},^{},^{}),\] (14)

where \(_{}^{}\) are set to be 1.0, 0.15, and 1.0 respectively, \(L_{}\) denotes the total variation loss, \(L_{}^{1}\) is a polarization-based regularization term to ensure the ratio between \(^{}\) and \(^{}\) defined as

\[L_{}^{1}(^{},^{},^{ },^{})=L_{2}(^{}^{ },^{}^{}).\] (15)

The refinement loss could be written as

\[L_{}=_{}^{}L_{1}(_{_{1,2,3,4}}, _{_{1,2,3,4}}^{})+_{}^{}L_{ }^{2}(_{_{1,2,3,4}}),\] (16)

where \(_{}^{}\) are set to be 10.0 and 100.0 respectively, \(L_{}^{2}\) denotes another polarization-based regularization term defined as

\[L_{}^{2}(_{_{1,2,3,4}})=L_{2}(_{_{ 1}}+_{_{3}},_{_{2}}+_{_{4}}),\] (17)

which is similar to the one used in  for enforcing the preservation of polarization properties.

**Dataset preparation.** We propose to generate a synthetic dataset due to the fact that there is no public dataset for our settings. First, we choose the PLIE dataset  as our data source. It provides short-exposure polarized snapshots that suffer from low-light noise along with the corresponding high-quality reference snapshots captured by a Lucid Vision Phoenix polarization camera, which could serve as \(\) and \(\). Then, we adopt the approach proposed in  to generate the blurry polarized snapshots that suffer from motion blur, which could be served as \(\). To generate more severe motion blur for increasing the diversity, we add impulsive variation  to the motion trajectories. The images are resized and randomly cropped to \(256 256\) (\(512 512\)) pixels in the training (test) set. The training (test) set contains 7500 (300) different images in total.

**Training strategy.** Our framework is implemented using PyTorch with 2 NVIDIA 2080Ti GPUs, and a two-stage training strategy is applied. First, to ensure a stable initialization of the training process, we train the irradiance restoration phase and the polarization reconstruction phase independently for 300 epochs with learning rates of 0.01 and 0.0001 respectively. Then, we train the entire network for 100 epochs with learning rate of 0.0001, and in this training stage we multiply the loss terms \(L_{}\) with 5.0, 10.0, and 10.0 respectively. For optimization, we use Adam optimizer  with \(_{1}=0.5\), \(_{2}=0.999\).

    & PSNR-\(\) & SSIM-\(\) & PSNR-\(\) & SSIM-\(\) & PSNR-\(_{0}\) & SSIM-\(_{0}\) \\ 
**Ours** & **29.23** & **0.797** & **16.96** & **0.382** & **39.05** & **0.982** \\ PLIE  & 27.91 & 0.790 & 15.92 & 0.371 & 38.95 & 0.978 \\ PLIE+ & 27.98 & 0.794 & 16.93 & 0.379 & 39.01 & 0.979 \\ PolDeblur  & 24.52 & 0.676 & 15.73 & 0.280 & 26.12 & 0.794 \\ PolDeblur+ & 25.31 & 0.758 & 16.75 & 0.374 & 39.04 & 0.981 \\ LSD2  & 25.73 & 0.662 & 13.75 & 0.288 & 27.88 & 0.905 \\ LSFNet  & 25.56 & 0.693 & 15.90 & 0.282 & 26.76 & 0.826 \\ SelfIR  & 19.43 & 0.647 & 15.39 & 0.231 & 25.90 & 0.785 \\ D2HNet  & 24.45 & 0.671 & 15.63 & 0.264 & 25.25 & 0.803 \\   

Table 1: Quantitative comparisons on synthetic data. The comparisons involve our framework, the state-of-the-art polarized image low-light enhancement method PLIE  and its improved version PLIE+, the only existing polarized image deblurring method PolDeblur  and its improved version PolDeblur+, and four learning-based image enhancement methods designed for conventional images that also fuse noisy and blurry pairs (LSD2 , LSFNet , SelfIR , and D2HNet ).

## 5 Experiments

### Evaluation on synthetic data

First, we compare our framework with the state-of-the-art polarized image low-light enhancement method PLIE  and the only existing polarized image deblurring method PolDeblur . Besides, we also compared with PLIE+ and PolDeblur+ (the improved versions of PLIE  and PolDeblur , where slight modifications are made to allow them to accept noisy and blurry pairs as the input) to ensure a fair comparison. In addition, four learning-based image enhancement methods designed for conventional images that also fuse noisy and blurry pairs (LSD2 , LSFNet , SelfIR , and D2HNet ) are also compared to ensure a comprehensive evaluation. Note that all compared methods are retrained on our dataset. As other methods designed for polarized images  do, we not only evaluate the quality of \(\) and \(\), but also the quality of \(_{0}\).

To evaluate the results quantitatively, we adopt two frequently-used metrics including PSNR and SSIM. Results are shown in Tab. 1, where our framework consistently outperforms the compared methods on all metrics. These results could demonstrate three points:

1. _Complementarily fusing can improve the quality of polarimetric imaging_, which could be deduced from the fact that PLIE+ and PolDeblur+ (which can accept a degraded pair of noisy and blurry polarized snapshots as the input) achieve better performance compared with PLIE  and PolDeblur  (which can only accept a single degraded polarized snapshot as the input) respectively.
2. _Our framework can utilize the complementary knowledge in a more effective manner_, since our framework still outperforms PLIE+ and PolDeblur+ despite that they share the same kind of input as our framework.

Figure 5: Qualitative comparisons on synthetic data. See the caption of Tab. 1 for explanation. We visualize the DoP \(\) and Ao\(\) using color maps after normalizing and averaging the RGB channels (as done in other methods designed for polarized images ) throughout this paper.

3. _Designing a fusing framework tailored to the polarized images is necessary_, since the performance of both LSD2 , LSFNet , SelfIR , and D2HNet  is inferior in the task of polarimetric imaging.

Visual quality comparisons are shown in Fig. 53. As for \(\) and \(\), our framework can produce clean and clear edges, since it can make full use of the complementary knowledge in a polarization-aware manner, while the compared methods suffer from noisy or blurry artifacts. As for \(_{0}\), our results resemble the reference more closely with less color and structure distortion.

### Evaluation on real data and downstream application

To evaluate on real data, we capture several pairs of noisy and blurry polarized snapshots from various scenes using a Lucid Vision Phoenix polarization camera. Qualitative results are shown in Fig. 64, from which we can see our framework can produce high-quality details.

Besides, in Fig. 7 we show that complementarily fusing can be beneficial to downstream polarization-based vision application such as reflection removal. Here, we feed a short-exposure noisy, a long-exposure blurry, and the fused polarized snapshots into a reflection removal network (RSP ) respectively. From the results we can see the reflection-removed image with the fusing process of our framework contains more detailed textures and less reflection contamination.

### Ablation study

To verify the validity of each design choice, we conduct a series of ablation studies and show comparisons in Tab. 2. First, we show the effectiveness of the first phase by substituting it with

Figure 6: Qualitative comparisons on real data. See the caption of Fig. 5 for explanation.

a learning-based image enhancement methods designed for conventional images (LSD2  as Phase1). The reason why we choose LSD2  is that it outperforms other similar methods (LSFNet , SelfIR , and D2HNet ) in restoring \(_{0}\) (see Tab. 1). We can see that the performance becomes inferior since our first phase can make full use of the color and structure cues encoded in the Stokes parameters while LSD2  cannot. Then, we show the effectiveness of the proposed CSCF modules in the first phase (W/o CSCF) by substituting them with vanilla convolution layers. We can see that the performance degenerates since the CSCF modules are more suitable for addressing the issues of erroneous global tone and less salient local structure in the feature space. Similarly, we also show the effectiveness of the proposed CAG module and CI modules in the second phase (W/o CAG and W/o CI). We can see that the CAG module can facilitate the establishment of the physical correlation between the polarized images, and the CI modules can offer better optimization to the values of the DoP and AoP. Besides, we demonstrate the advantage of reconstructing the DoP and AoP in a Cartesian coordinate representation (W/o Cartesian) by directly reconstructing their values. We can see that the performance degraded severely due to the non-linearity. Finally, we verify the necessity of the third phase used for refinement (W/o refinement) by removing it. These results show our complete framework achieves the first performance with the proposed specific designs.

## 6 Conclusion

We propose a quality-improved and property-preserved polarimetric imaging framework by complementarily fusing a degraded pair of noisy and blurry polarized snapshots. By adopting a neural network-based three-phase fusing scheme consisting of irradiance restoration, polarization reconstruction, and artifact suppression, with specially-designed modules tailored to each phase, our framework can produce clean and clear polarized snapshots with high-quality DoP and AoP.

**Limitations.** Since our framework is designed for reconstructing a single high-quality polarized snapshot from a degraded pair of noisy and blurry polarized snapshots, it cannot reconstruct a polarized video. Besides, it cannot be used to fuse conventional RGB images since our first phase requires the Stokes parameters as part of the input, which are not available in such a setting.