# Promises and Pitfalls of Threshold-based Auto-labeling

Harit Vishwakarma

hvishwakarma@cs.wisc.edu

University of Wisconsin-Madison

&Heguang Lin

hglin@seas.upenn.edu

University of Pennsylvania

&Frederic Sala

fredsala@cs.wisc.edu

University of Wisconsin-Madison

&Ramya Korlakai Vinayak

ramya@ece.wisc.edu

University of Wisconsin-Madison

###### Abstract

Creating large-scale high-quality labeled datasets is a major bottleneck in supervised machine learning workflows. Threshold-based auto-labeling (TBAL), where validation data obtained from humans is used to find a confidence threshold above which the data is machine-labeled, reduces reliance on manual annotation. TBAL is emerging as a widely-used solution in practice. Given the long shelf-life and diverse usage of the resulting datasets, understanding when the data obtained by such auto-labeling systems can be relied on is crucial. This is the first work to analyze TBAL systems and derive sample complexity bounds on the amount of human-labeled validation data required for guaranteeing the quality of machine-labeled data. Our results provide two crucial insights. First, reasonable chunks of unlabeled data can be automatically and accurately labeled by seemingly bad models. Second, a hidden downside of TBAL systems is potentially prohibitive validation data usage. Together, these insights describe the promise and pitfalls of using such systems. We validate our theoretical guarantees with extensive experiments on synthetic and real datasets1.

## 1 Introduction

Machine learning (ML) models with millions or even billions of parameters are used to obtain state-of-the-art performance in many applications, e.g., object identification , machine translation , and fraud detection . Such large-scale models require training on large-scale labeled datasets. As an outcome, the typical supervised ML workflow begins with the construction of a large-scale high quality dataset. Datasets with up to millions of labeled data points have played a pivotal role in the advancement of computer vision. However, collecting labeled data is an expensive and time consuming process. A common approach is to rely on the services of crowdsourcing platforms such as Amazon Mechanical Turk (AMT) to get groundtruth labels.

Even with crowdsourcing, obtaining labels for the entire dataset is expensive. To reduce costs, data labeling systems that partially rely on using a model's predictions as labels have been developed. Such systems date back to teacher-less training . Modern examples include Amazon Sagemaker Ground Truth  and others . These approaches can be broadly termed _auto-labeling_.

Auto-labeling systems aim to label unlabeled data using predictions from ML models that are often trained on small amounts of human labeled data which can produce incorrect labels. The shelf life of datasets is longer than those of models, e.g., ImageNet continues to be a benchmark for many computer vision tasks  fifteen years after its initial development. As a result, to reliably train new models on auto-labeled datasets and deploy them, we need a thorough understanding of how reliable the datasets output by these auto-labeling systems are. Unfortunately, many widelyused commercial auto-labeling systems  are largely opaque with limited public information on their functionality. It is therefore unclear whether the quality of the datasets obtained can be trusted. To address this, we study the high level workflow of a popular _threshold-based_ auto-labeling (TBAL) system (see Figure 1). We emphasize that our goal is to understand such systems and their performance--not to promote them as a superior alternative to other approaches. Our goal is:

**Goal.** Develop a fundamental understanding of TBAL systems. This is crucial since there is a lack of theoretical understanding of the reliability of these systems despite their wide adoption.

The TBAL systems we study (Figure 1) work iteratively. At a high level, in each iteration, the system trains a model on currently available human labeled data and decides to label certain parts of unlabeled data using the trained model by finding high-accuracy regions using validation data. It then collects human labels on a small portion of unlabeled data that is deemed helpful for training the current model in the next iteration. The validation data is created by sampling i.i.d. points from the unlabeled pool and querying human labels for them. In addition to training data, the validation data is a major driver of the cost and accuracy of auto-labeling and will be a key component in our study.

**Our Contributions.** We study TBAL systems (Figure 1) and make the following contributions:

* Provide the **first theoretical characterization of TBAL systems**, developing tradeoffs between the quantity of manually labeled data and the quantity and quality of auto-labeled data (Section 3).
* Empirical results validating our theoretical understanding on real and synthetic data (Section 4).

Our results reveal **two important insights**. Promisingly, even poor quality models are capable of reliably labeling at least some data when we have access to sufficient validation data and a good confidence function that can accurately quantify the confidence of a given model on any data point. On the downside, in certain scenarios, the quantity of the validation data required to reach a certain quantity and quality of auto-labeled data can be high.

## 2 Threshold-Based Auto-Labeling Algorithm

We begin with the problem setup and describe the TBAL algorithm that is inspired by the commercial systems . Then we provide experiments and theoretical analysis shedding light on the pros and cons of TBAL. We emphasize that TBAL is not our proposal and our goal is to _understand_ the effectiveness of such an auto-labeling system.

### Problem Setup

Notation.Let the instance and label spaces be \(\) and \(=\{1,,k\}\). We assume that there is some _deterministic_ but unknown function \(f^{*}:\) that assigns true label \(y=f^{*}()\) to any \(\). We also assume that there is a _noiseless oracle_\(\) that can provide the true label \(y\) for any given \(\). Let \(X_{pool}\) denote a sufficiently large pool of unlabeled data to be labeled.

The goal of an auto-labeling algorithm is to produce accurate labels \(_{i}\) for points \(_{i} X_{pool}\) while minimizing the number of queries to the oracle. Let \([m]:=\{1,2,,m\}\), \(A[N]\) be the set of indices of auto-labeled points, and \(X_{pool}(A)\) be these points. The _auto-labeling error_\(}(X_{pool}(A))\)

Figure 1: High-level workflow threshold-based auto-labeling (TBAL). Box (B) shows the key component estimating the auto-labeling region using validation data and auto-labeling points in it.

and the _coverage_\(}(X_{pool}(A))\) are defined as

\[}(X_{pool}(A)):=}_{i A}( {y}_{i} f^{*}(_{i}))\;\;\;\;}(X_{ pool}(A)):==}{N},\] (1)

where \(N_{a}\) denotes the size of auto-labeled set \(A\). TBAL algorithm aims to auto-label the dataset so that \(}(X_{pool}(A))_{a}\) while maximizing coverage \(}(X_{pool}(A))\) for any given \(_{a}(0,1)\).

**Hypothesis Class and Confidence Function.** A TBAL algorithm is given a fixed _hypothesis space_\(\) and a _confidence function_\(g: T^{+}\) that quantifies the confidence of \(h\) on any data point \(\). Confidence functions include prediction probabilities and margin scores. For example, when \(\) is a set of unit-norm homogeneous linear classifiers, i.e. \(h_{}()=(^{T})\) with \(\{^{d}:||||_{2}=1\}\), a reasonable confidence function is \(g(h_{},)=|^{T}|\).

Note that the target \(f^{*}\) might not be in the hypothesis space \(\). Our analysis (Section 3) shows that the TBAL algorithm can work well, i.e., accurately label a reasonable fraction of unlabeled data with simpler hypothesis classes that do not contain the target hypothesis \(f^{*}\). We illustrate this with a simple example in Section 2.3 and Figure 2. Note as well that the features \(\) could be raw features or representations from self-supervised techniques, pre-trained models, etc. We analyze TBAL in settings (i) with no assumptions on the features and (ii) when the features are linearly separable.

### Description of the algorithm

The TBAL algorithm is given in Algorithm 1. It starts with an unlabeled pool \(X_{pool}\) and an auto-labeling error threshold \(\). For ease of exposition, the algorithm is given the labeled validation set \(D_{val}\) of size \(N_{v}\) separately. In practice, it is created by selecting points at random from \(X_{pool}\). The algorithm starts with an initial batch of \(n_{s}\) random data points and obtains oracle labels for these. The algorithm works in an iterative manner using the following steps.

1. Oracle labeled data obtained in each iteration \(i\) is added to the training pool \(D_{train}^{(i)}\). It is used to train a model \(_{i}\) by performing empirical risk minimization (ERM).
2. Find the region where \(_{i}\) can auto-label accurately. The algorithm estimates a threshold \(_{i}\) on the confidence score above which it can auto-label with the desired auto-labeling accuracy on the validation data (see Algorithm 2). Thresholds that have too little validation data are discarded, since their estimates are unreliable. The minimum threshold is found such that the sum of the estimated error \(}_{a}(_{i},t|X_{v}^{(i)})\) (see eq. (2)) and an upper confidence bound using the standard deviation of the estimated error, is at most the given auto-labeling error threshold.
3. Auto-label the points in the pool, \(X_{u}^{(i)}\), which have confidence \(g(_{i},)>_{i}\). These are added to the set \(D_{out}\) and removed from the unlabeled pool. The validation points that fall in the auto-labeled region are also removed from the validation set so that in the next round the validation set and the unlabeled pool are from the same region and the same distribution. Removing the auto-labeled points from \(X_{}\) is a crucial step in the TBAL algorithm that enables it to focus only on the remaining unlabeled regions in the next iteration.
4. If there are points left in \(X_{}\), the algorithm selects points using some active querying strategy , obtains human labels for them, and adds them to the training pool. Note that the auto-labeled data is not added into the training set.

This process continues until there are no data points left to be labeled. The algorithm then outputs the labeled dataset, which is a mixture of human- and machine-labeled points.

### Comparison between Auto-Labeling, Active Learning and Selective Classification

What is the difference between TBAL and methods such as active learning and selective classification?

_Active learning._ The goal of active learning  (AL) is to find the best model in hypothesis class \(\) by training with less labeled data compared to passive learning. This is usually achieved by obtaining labels for the most informative points. Note that the _end goal is to output a model_ from the function class whose predictions on new data as good as the best model in the function class could.

_Selective Classification._ The goal of selective classification (SC)  is to find the best combination of the hypothesis and selection functions to minimize error and maximize coverage of selection regions.

_Auto-Labeling._ The output of an auto-labeling procedure is a labeled dataset (not a model). When the hypothesis class is of lower complexity, it is often not possible to find a good classifier. The goal of an auto-labeling system is to label as much of the unlabeled data as accurately as possible with a given function class and with limited labeled data from humans.

**Is active learning alone enough to auto-label data?** AL has been found to be effective in reducing the number of labels needed to learn versus passive learning, particularly in low-noise cases . Doing auto-labeling using AL followed by SC may be effective in such settings. However, in real-world scenarios, noise levels may be higher and the hypothesis class could be misspecified. In these instances, using the model learned through active learning to automatically label all data may result in a high number of errors.

We illustrate this difference between AL, SC, and auto-labeling through an example. Suppose the data consists of two concentric circles, one for each class, with the same number of points per class (Figure 2(a)). This setting is not linearly separable. We run TBAL, AL, and AL followed by SC with an error tolerance of \(_{a}=1\%\) and linear classifiers and confidence functions. The results are shown in Figure 2. Note that the multiple optimal linear classifiers will all incur an error of \(50\%\). AL algorithms can only output models that make at least \(50\%\) error. If we naively use the output model for auto-labeling, we can obtain near full coverage but incur around \(50\%\) auto-labeling error. If we use the model output by AL with threshold-based SC, labeling error is reduced. However, it can only label \( 25\%\) of the unlabeled data. In contrast, TBAL can label almost all of the data accurately (close to \(100\%\) coverage) with less than \(1\%\) auto-labeling error.

Figure 2: Comparison of TBAL, active learning (AL) followed by selective classification (AL+SC) and passive learning (PL) followed b selective classification (PL+SC) on the Circles dataset (Sec. 2.3) using linear classifiers and confidence functions. (a) Samples auto-labeled, queried, and left unlabeled. (b) The auto-labeling error and coverage achieved by the algorithms. (50 trials.)

Theoretical Analysis

The performance of the TBAL (Algorithm 1) depends on many factors including the hypothesis class, the accuracy of the confidence function, the data sampling strategy, and the size of the training and validation data. In particular, the amount of validation data plays a critical role in determining the accuracy of the confidence function, which in turn affects the accuracy and coverage.

We derive bounds on the auto-labeling error and the coverage for Algorithm 1 in terms of the size of the validation data, the number of auto-labeled points \(N_{a}^{(k)}\), and the Rademacher complexity of the extended hypothesis class \(^{T,g}\) induced by the confidence function \(g\). Our first result, Theorem (3.2), applies to general settings and makes no assumptions on the particular form of the hypothesis class, the data distribution, and the confidence function. We then instantiate and specialize the results for a specific setting in Section 3.1. We introduce some notation to aid in stating our results,

**Definition 3.1**.: _(Hypothesis Class with Abstain) The function \(g\), set \(T\) and \(\) induce an extended hypothesis class \(^{T,g}:= T\). Any function \((h,t)^{T,g}\) is defined as \((h,t)()=h()\) if \(g(h,) t\) and \(\) otherwise. Here \((h,t)()=\) means \((h,t)\) abstains in classifying the point \(\)._

**Error Definitions.** Let \(\) denote a non-empty sub-region of \(\) and \(S\) be a finite set of i.i.d. samples from \(\). The subset \((h,t)\) denotes the regions where \((h,t)\) does not abstain i.e. \((h,t):=\{:(h,t)()\}\), and the conditional probability mass associated with it is \((h,t|):=((h,t)|),\) and its empirical counterpart \(}(h,t|S):=|S(h,t)|/|S|.\) We use \(()\) to denote the probability mass of set \(\) and \((^{}|)\) for the conditional probability of subset \(^{}\) given \(\). The population level and empirical auto-labeling errors are defined as follows:

\[_{a}(h,t|) :=_{|}[_{0-1}^{}(h,t, ,y)]/(h,t|),\] (2) \[}_{a}(h,t|S) :=(_{_{i} S(h,t)}_{0-1}^{}(h,t,_{i},y_{i}))/|S(h,t)|\]

Here \(_{0-1}^{}(h,t,,y):=_{0-1}(h,,y)_{ }(h,t,)\) with \(_{0-1}(h,,y):=(h() y)\), and \(_{}(h,t,):=(g(h,) t)\).

**Rademacher Complexity.** The Rademacher complexities for the function classes induced by the \(,T,g\) and the loss functions are defined as \(_{n}^{T,g}:=_{n} ,_{0-1}+_{n}^{T,g},_{ }\). Let \(_{i}\) and \(_{i}\) be the ERM solution and the auto-labeling threshold at epoch \(i\). Let \(_{i}(0,1)\) be a constant such that \((_{i},_{i}|^{(i)}) P_{0}\) for all \(i\). Let \(X_{v}^{(i)}\) denote the validation set, and \(n_{v}^{(i)}\) and \(n_{a}^{(i)}\) the number of validation and auto-labeled points at epoch \(i\). Let \(}_{a}(_{i},_{i}|X_{v}^{(i)})\) be the empirical conditional risk of \(_{i}\) in the region where \(g(_{i},)_{i}\) evaluated on the validation data \(X_{v}^{(i)}\).

We provide the following guarantees on the auto-labeling error and the coverage achieved by TBAL.

**Theorem 3.2**.: _(Overall Auto-Labeling Error and Coverage) Let \(k\) denote the number of rounds of the TBAL Algorithm 1. Let \(n_{v}^{(i)},n_{a}^{(i)}\) denote the number of validation and auto-labeled points at epoch \(i\) and \(n^{(i)}=|X^{(i)}|\). Let \(X_{pool}(A_{k})\) be the set of auto-labeled points at the end of round \(k\). \(N_{a}^{(k)}=_{i=1}^{k}n_{a}^{(i)}\) be the total number of auto-labeled points. Then, for any \((0,1)\), with probability at least \(1-\),_

\[}X_{pool}(A_{k})_{i=1}^{k} ^{(i)}}{N_{a}^{(k)}}  }_{a}_{i},_{i}|X_{v }^{(i)}}_{(a)}+}_{n_{v}^{(i )}}^{T,g}+}^{(i)}} ()}}_{(b)}\] \[+}_{i=1}^{k}^{(i )}}{N_{a}^{(k)}}_{n_{a}^{(i)}}^{T,g}+ ^{(k)}}()}}_{(c)}, and\]

\[}X_{pool}(A_{k})_{i=1}^{k} ^{(i)}(_{i},_{i})-2_{n^{(i)}} ^{T,g}-}{N}}.\]

**Discussion.** We interpret this result, starting with the auto-labeling error term \(}(X_{pool}(A_{k})).\) The term (a) \(}_{a}(_{i},_{i}|X_{v}^{(i)})\) is the empirical conditional error in the auto-labeled region computed on the validation data in \(i\)-th round, which is at most \(_{a}\). Thus, summing term (a) over all the rounds is at most \(_{a}\). The term (b) provides an upper bound on the excess error over the empirical estimate term (a) as a function of the Rademacher complexity of \(^{T,g}\) and the validation data used in each round. The last term (c) captures the variance in the overall estimate as a function of the total number of auto-labeled points and the Rademacher complexity of \(^{T,g}\). If we let \(n_{v}^{(i)} n_{v}\) i.e. the minimum validation points ensured in each round, then we can see the second term is \(}(_{n_{v}}(^{T,g}))\) and the third term is \((})\).

Therefore, validation data of size \((1/_{a}^{2})\) in each round is sufficient to get a \((_{a})\) bound on the excess auto-labeling error. The terms with Rademacher complexities suggest that it is better to use a hypothesis class and confidence function such that the induced hypothesis class has low Rademacher complexity. While such a hypothesis class might not be rich enough to include the target function, it would still be helpful for efficient and accurate auto-labeling of the dataset which can then be used for training richer models in the downstream task. The coverage term provides a lower bound on the empirical coverage \(}(X_{pool}(A_{k}))\) in terms of the true coverage of the sequence of estimated hypotheses \(_{i}\) and threshold \(_{i}\).

We note that the size of the validation data needed to guarantee the auto-labeling error in each round by Algorithm 1 is optimal up to \(\) factors. This follows by applying a result on the tail probability of the sum of independent random variables due to Feller :

**Lemma 3.3**.: _Let \(c_{1},c_{2}\) and \(>0\). Let \(_{i} X\) be a set of \(n\) i.i.d. points from \(\) with corresponding true labels \(y_{i}\). Given \((h,t)^{T,g}\), let \(_{0-1}^{}(h,t,_{i},y_{i})- (h,t|)^{2}=_{i}^{2}>^{2}\) for every \(_{i}\) for \(_{i}>0\) and let \(_{i}^{n}_{i}^{2} c_{1}\) then for every \([0,(_{i=1}^{n}_{i}^{2})/}]\) with \(n_{v}<12^{2}(4c_{2})/^{2}\), the following holds w.p. at least \(1/4\), \(_{a}(h,t|)>}_{a}(h,t|X)+\)._

Therefore, if a sufficiently large validation set is not used in each round, there is a constant probability of erroneously deciding on a threshold for auto-labeling. Such a requirement on validation data also applies to active learning if we seek to validate the output model. Bypassing this requirement demands the use of approaches that are different from threshold-based auto-labeling and traditional validation techniques. We note the possibility of using recently proposed _active testing_ techniques , a nascent approach to reducing validation data usage.

### Linear Classifier Setting

Next, we consider a simple setting where active learning is known to be optimal to see if TBAL can offer similar performance guarantees. To do so, we instantiate results from 3.2 to homogeneous linear separators under the uniform distribution in the realizable setting. Let \(P_{}\) be supported on the unit ball in \(^{d}\), \(=\{^{d}:|||| 1\}\). Let \(=\{^{d}:||||_{2}=1\}=_{d}\), \(=\{(,) \,\}\), the score function be given by \(g(h,)=g(,)=|,|\), and set \(T=\). For simplicity, we will use \(\) in place of \(\).

**Corollary 3.4**.: _(Overall Auto-Labeling Error and Coverage) Let \(}_{i},_{i}\) be the ERM solution and the auto-labeling threshold respectively at epoch \(i\). Let \(n_{v}^{(i)},n_{a}^{(i)}\) denote the number of validation and auto-labeled points at epoch \(i\). Let the TBAL algorithm run for \(k\)-epochs. Then, for any \((0,1)\), w.p. at least \(1-\),_

\[}X_{pool}(A_{k})_{i=1}^{ k}^{(i)}}{N_{a}^{k}}}_{a} }_{i},_{i}|X_{v}^{(i)}}_{(a)}+}^{(i)}}2d^{(i)}}{d}+}}_{(b)}\] \[+}(^{(k)}}2d^{(k)}}{d}+})},  and\] \[}X_{pool}(A_{k}) 1-_{i}_{i} -2k2d +}.\]

These results imply that by ensuring the sum of the empirical validation error term (a) and the upper confidence interval to be less than \(_{a}\) in each round of the algorithm we can ensure that the overall auto-labeling error remains below \(_{a}\). Furthermore, by applying standard VC theory to the first round, we obtain that \(_{1} 1/2\). Therefore, right after the first round, we are guaranteed to label at least half of the unlabeled pool. We empirically observe that TBAL has coverage at par with active learning while respecting the auto-labeling error constraint (See Figure 4(a)).

Tightness of the Bounds.We study this in the setting of the Unit-Ball experiment. The upper bound on excess risk in this setting is given in Corollary 3.4 which is an instantiation of our general results to this specific setting. We consider a simplified form of the upper bound by ignoring the constants to get a sense of the rate in terms of the validation data size. We compute this simplified upper bound for different amounts of validation data. We compare these with the maximum auto-labeling error observed over 25 runs of auto-labeling in the Unit-Ball setting with different random seeds for each validation data size. The results are in Figure 3. As expected, we see that the worst-case error rate follows a similar rate as our upper bound but the upper bound is conservative. Next, we explain why this is the case.

Our upper bound is slightly conservative, as it is based on a uniform bound over all hypotheses in a given hypothesis class. Since the individual hypotheses whose excess auto-labeling error we need to bound are not known a priori we need to derive a bound on the number of validation samples using which we can guarantee that the excess auto-labeling error of any hypothesis (model) is small with high probability. Note that this is a conservative (worst-case) analysis to get an upper bound on the validation sample complexity. The upper bound has two parts: a) the Rademacher complexity of the hypothesis class and b) a term with the number of validation samples. We note that on the validation samples, it matches lower bounds order-wise (see Lemma 3.3). This is the first analysis to provide these bounds based on uniform convergence without making any assumptions about the data distributions or hypothesis class. We provide further discussion on the role of Rademacher complexity in the Appendix C.1.

## 4 Experiments

We study the effectiveness of TBAL on synthetic and real datasets. We validate our theoretical results and aim to understand the amount of labeled validation and training data required to achieve a certain auto-labeling error and coverage. We also seek to understand whether our findings apply to real data--where labels may be noisy--along with how TBAL performs compared to common baselines.

**Baselines.** We compare TBAL to the following methods:,

1. [leftmargin=0pt,itemsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsepsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsepsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsepsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsepsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsepsep=0pt,topsep=0pt,topsep=0pt,topsepsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsepsep=0pt,topsepsep=0pt,topsepsep=0pt,topsepsep=0pt,topsepsep=0pt,topsepsepsep=0pt,topsepsep=0pt,topsep

**Datasets.** We use the following synthetic and real datasets. We also provide empirical results on MNIST and another synthetic dataset in the Appendix. For each dataset, we split the data into two sufficiently large pools. One is used as \(X_{pool}\) on which auto-labeling algorithms are run and the other is used as \(X_{val}\) from which the algorithms subsample validation data.

1. _Unit-Ball_ is a synthetic dataset of uniformly sampled points from the \(d\)-dimensional unit ball. The true labels are generated using a homogeneous linear separator with \(=[1/,,1/]\). We use \(d=30\) and generate \(N=20K\) samples, out of which \(16K\) are in \(X_{pool}\) and \(4K\) are in \(X_{val}\). The dataset has just two classes but there is no margin between them.
2. _Tiny-ImageNet_ is a subset of the larger ImageNet  dataset, designed for image classification tasks. It consists of _200 classes_, each with \(500\) training images and 50 validation and test images. With a total of \(100K\) images, Tiny ImageNet provides a diverse and challenging dataset. We use pre-computed embeddings of the images using CLIP .
3. _IMDB Reviews_ is a comprehensive collection of movie reviews, consisting of \(50K\) individual reviews. It is a balanced dataset of positive and negative labels. We use the standard train set of size \(25K\) and split it into \(X_{pool}\) and \(X_{val}\) of sizes \(20K\) and \(5K\) respectively. We compute embeddings of reviews using a pre-trained model bge-large-en from the Massive Text Embedding Benchmark (MTEB) .
4. _CIFAR-10_ is an image dataset with \(10\) classes. We randomly split the standard training set into \(X_{pool}\) of size \(40K\) and the validation pool of size \(10K\). We use the raw features for training.

**Models and Training.** For the linear models, we use SVM with the usual hinge loss and train it to loss tolerance \(10^{-5}\). To train a multi-layer perceptron (MLP) on the pre-computed embeddings of IMDB and Tiny-ImageNet we use SGD with a learning rate of \(0.05,0.1\) respectively, and batch size of 64. To train the medium CNN we use SGD with a learning rate of \(10^{-2}\), batch size 256, and momentum of 0.9. More details on model training are in the Appendix.

**The score function \(g\).** For SVMs we use the standard implementations of  in sklearn to get the prediction probabilities and use them as the score function. Neural networks use softmax output.

### Role of Validation Data

The TBAL algorithm uses validation data to estimate the auto-labeling errors at various thresholds to determine the threshold for automatically labeling points accurately. Thus, it is crucial to have

   }\)} &  &  \\   & **TBAL** & **AL+SC** & **TBAL** & **AL+SC** \\ 
100 & 3.10 \(\)1.80 & 0.68 \(\)2.01 & 71.43 \(\)5.84 & 96.95 \(\)1.01 \\ 
400 & 1.97 \(\)0.78 & 0.59 \(\)0.18 & 93.99 \(\)1.29 & 97.89 \(\)0.50 \\ 
800 & 1.64 \(\)0.98 & 0.66 \(\)2.01 & 96.26 \(\)1.33 & 98.06 \(\)0.93 \\ 
1200 & 1.39 \(\)0.98 & 0.67 \(\)2.01 & 96.67 \(\)0.94 & 98.10 \(\)0.94 \\ 
1600 & 1.33 \(\)0.98 & 0.70 \(\)0.91 & 97.13 \(\)0.95 & 98.16 \(\)0.94 \\ 
2000 & 1.28 \(\)0.34 & 0.71 \(\)0.21 & 97.15 \(\)0.94 & 98.20 \(\)0.94 \\   

Table 1: **Unit-Ball.** Effect of variation of validation data size (\(N_{v}\)) with and without using a UCB on error estimates. We keep training data size \(N_{q}\) fixed at \(500\) and use error threshold \(_{a}=1\%\). We report the mean and std. deviation over 10 runs with different random seeds. **Left:** with \(C_{1}=0\). **Right:** with \(C_{1}=0.25\).

   }\)} &  &  \\   & **TBAL** & **AL+SC** & **TBAL** & **AL+SC** \\ 
200 & 2.28 \(\)0.21 & 3.11 \(\)0.68 & 68.24 \(\)5.38 & 57.77 \(\)1.50 \\ 
400 & 1.29 \(\)0.10 & 1.98 \(\)0.69 & 63.81 \(\)4.86 & 63.06 \(\)1.70 \\ 
600 & 1.41 \(\)0.92 & 1.81 \(\)0.22 & 69.64 \(\)1.36 & 62.92 \(\)2.39 \\ 
800 & 1.62 \(\)0.30 & 2.04 \(\)0.35 & 67.45 \(\)3.72 & 63.22 \(\)7.89 \\ 
1000 & 1.64 \(\)0.23 & 1.97 \(\)0.58 & 70.28 \(\)2.52 & 66.11 \(\)8.00 \\   

Table 2: **IMDB.** Effect of variation of validation data size (\(N_{v}\)) with and without using a UCB on error estimates. We keep training data size \(N_{q}\) fixed at \(500\) and use error threshold \(_{a}=5\%\). We report the mean and std. deviation over 10 runs with different random seeds. **Left:** with \(C_{1}=0\). **Right:** with \(C_{1}=0.25\).

[MISSING_PAGE_FAIL:9]

**Setup.** We limit the amount of training data the algorithm can use and record the resulting auto-labeling error and coverage. We ensure all algorithms have sufficiently large but equal amounts of validation data. We run on Unit-Ball, IMDB, Tiny-Imagenet, and CIFAR-10 datasets with the same values of \(n_{s}\), \(n_{b}\), and \(C\) as in previous experiments.

**Results.** Figures 4(a), 4(b), and 2(b) indicate that TBAL and methods utilizing selective classification (AL+SC, PL+SC) maintain a high level of accuracy, even in scenarios where minimal training samples are used. This is expected as the threshold estimation method (when used with sufficient validation data) will find auto-labeling thresholds such that the auto-labeling error does not exceed \(_{a}\). The impact of training data size can be seen clearly in the coverage achieved by the algorithms. As expected, with fewer training samples the model has low accuracy leading to low coverage. However, as more samples are acquired, a more accurate model within the function class is learned, resulting in increased coverage. The Appendix D has additional discussion and results.

## 5 Related Work

We briefly review related work, deferring a more detailed discussion to the Appendix A.

There is a rich body of work on active learning (AL) [57; 12; 30; 28; 8] focused on learning the best model in a function class with less labeled data than passive learning. Various AL algorithms have been developed and analyzed, e.g., uncertainty sampling [62; 45], disagreement region based [9; 27], margin based  and abstention based methods that minimize the Chow's excess risk .

Selective classification (SC) equips a given classifier with the option to abstain from prediction in order to guarantee prediction quality. The foundations for SC are laid down in [17; 67; 18; 68] where results on the error rate in the prediction region and the coverage of the given classifier are provided. However, these works lack practical algorithms to find the prediction region. A recent work  gives a disagreement-based active learning strategy to learn a selective classifier.

A recent paper  studies a TBAL-like algorithm for auto-labeling. It focuses on the cost of training incurred when these systems use large-scale model classes for auto-labeling. It proposes an algorithm to predict the training set size that minimizes the overall cost and provides an empirical evaluation.

Weak supervision is another line of work aimed at auto-labeling that does not rely on obtaining human labels but instead uses potentially noisy but cheaply available sources to infer labels [51; 22]. In contrast, we are focused specifically on analyzing the performance of TBAL algorithms .

## 6 Conclusion and Future Work

In this work, we analyzed threshold-based auto-labeling systems and derived sample complexity bounds on the amount of human-labeled validation data required to guarantee the quality of machine-labeled data. Our study shows that these methods can accurately label a reasonable size of data using seemingly bad models when good confidence functions are available. Our analysis points to the hidden downside of these systems in terms of a large amount of validation data usage and calls for more sample-efficient methods including active testing.

## 7 Acknowledgments

This work was partly supported by funding from the American Family Data Science Institute. We thank Stephen Mussmann, Changho Shin, Albert Ge, Yi Chen, Kendall Park, and Nick Roberts for their valuable inputs. We thank the anonymous reviewers for their valuable comments and constructive feedback on our work.

   }\)} &  &  \\   & **TBAL** & **AL+SC** & **TBAL** & **AL+SC** \\ 
200 & 1.67 \(\)0.23 & 2.15 \(\)0.04 & 73.30 \(\)0.04 & 57.17 \(\)11.00 \\ 
400 & 1.63 \(\)0.18 & 1.61 \(\)0.23 & 72.59 \(\)3.16 & 64.53 \(\)0.04 \\ 
600 & 1.67 \(\)0.21 & 1.83 \(\)0.20 & 71.38 \(\)2.19 & 70.50 \(\)5.08 \\ 
800 & 1.67 \(\)0.27 & 1.90 \(\)0.13 & 69.10 \(\)4.51 & 65.74 \(\)0.14 \\ 
1000 & 1.62 \(\)0.22 & 1.97 \(\)0.35 & 73.42 \(\)2.84 & 68.05 \(\)5.56 \\   

Table 4: Results for varying \(N_{q}\), the maximum number of samples algorithm can use for training. **Left: IMDB** with \(N_{v}=1000,C_{1}=0.25,_{a}=5\%\). **Right: Tiny-ImageNet** with \(N_{v}=10K,C_{1}=0.25,_{a}=10\%\). Mean and std. deviations are reported.