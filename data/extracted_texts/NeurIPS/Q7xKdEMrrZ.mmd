# AsEP: Benchmarking Deep Learning Methods for Antibody-specific Epitope Prediction

Chu'nan Liu

Structural Molecular Biology

University College London

United Kindom

&Lilian Denzler

Structural Molecular Biology

University College London

United Kindom

&Yihong Chen

Centre for Artificial Intelligence

University College London

United Kindom

&Andrew Martin

Structural Molecular Biology

University College London

United Kindom

&Brooks Paige

Centre for Artificial Intelligence

University College London

United Kindom

Address correspondence to: chunan.liu@ucl.ac.uk, b.paige@ucl.ac.uk, andrew.martin@ucl.ac.uk

###### Abstract

Epitope identification is vital for antibody design yet challenging due to the inherent variability in antibodies. While many deep learning methods have been developed for general protein binding site prediction tasks, whether they work for epitope prediction remains an understudied research question. The challenge is also heightened by the lack of a consistent evaluation pipeline with sufficient dataset size and epitope diversity. We introduce a filtered antibody-antigen complex structure dataset, _AsEP_ (Antibody-specific Epitope Prediction). _AsEP_ is the largest of its kind and provides clustered epitope groups, allowing the community to develop and test novel epitope prediction methods and evaluate their generalisability. _AsEP_ comes with an easy-to-use interface in Python and pre-built graph representations of each antibody-antigen complex while also supporting customizable embedding methods. Using this new dataset, we benchmark several representative general protein-binding site prediction methods and find that their performances fall short of expectations for epitope prediction. To address this, we propose a novel method, _WALLE_, which leverages both unstructured modeling from protein language models and structural modeling from graph neural networks. _WALLE_ demonstrate up to \(3\)-\(10\)X performance improvement over the baseline methods. Our empirical findings suggest that epitope prediction benefits from combining sequential features provided by language models with geometrical information from graph representations. This provides a guideline for future epitope prediction method design. In addition, we reformulate the task as bipartite link prediction, allowing convenient model performance attribution and interpretability. We open source our data and code at https://github.com/biochunan/AsEP-dataset.

Introduction

Antibodies are specialized proteins produced by our immune system to combat foreign substances called antigens. Their unique ability to bind with high affinity and specificity sets them apart from regular proteins and small-molecule drugs, making them increasingly popular in therapeutic engineering. While the community is shifting towards computational antibody design based on pre-determined epitopes (Jin et al., 2022; Zhou et al., 2024; Bennett et al., 2024), accurate prediction of epitopes themselves remains underexplored. Precise epitope identification is essential for understanding antibody-antigen interactions and antibody functions, as well as streamlining antibody engineering. The task remains challenging due to multiple factors, including the lack of comprehensive datasets, limited interpretability, and low generalizability (Akbar et al., 2022; Hummer et al., 2022). Existing datasets are limited in size, e.g. only \(582\) complexes in Bepipred-\(3.0\)(Clifford et al., 2022), and often exhibit disproportionate representation among different epitopes. Current methods perform poorly on epitope prediction (Cia et al., 2023), with a ceiling MCC (Mathew's Correlation Coefficient) of \(0.06\). Besides, recent advances in graph learning algorithms, along with an increase in available antibody structures in the Protein Data Bank (PDB) (Berman et al., 2003), highlight the need to reevaluate current methods and establish a new benchmark dataset for predicting antibody-antigen interactions.

We approach the problem as a bipartite graph link prediction task, where the goal is to directly identify connections between two distinct graphs representing the antibody and antigen. Unlike conventional link prediction tasks (Nickel et al., 2016; Trouillon et al., 2016; Zhang and Chen, 2018; Chen et al., 2021), aiming to predict links within an individual graph (potentially with multiple relation types), such as in a protein-protein interaction network, our approach predicts links across a pair of molecular graphs. Our model, WALLE, is designed to predict fine-grained residue-residue interactions, i.e. bipartite links between the antibody and antigen, while it can also adapt to solve the coarser node classification task, distinguishing binding nodes from non-binding ones. Since most existing methods focus on predicting protein binding sites (i.e. node classification), we first benchmark this binding node classification task for straightforward comparisons with these methods. Moreover, we include WALLE's performance on the bipartite link prediction task as a baseline for future work.

## 2 Previous Work

Accurate epitope prediction for antibody design remains challenging due to the complexity of antibody-antigen interactions and limitations of existing datasets and methods. Several computational approaches have been developed, but they often fall short in terms of accuracy and applicability. Here, we present a representative, yet non-exhaustive, set of state-of-the-art methods.

**EpiPred**(Krawczyk et al., 2014) implements a graph-based antibody-antigen specific scoring function that considers all possible residue-residue pairs at the interface between antibody and antigen structures. It samples surface patches from the antigen structure and selects the highest-ranked patch as the predicted set of epitope residues.

**ESMFold**(Lin et al., 2023) is a protein language model based on ESM2 (Lin et al., 2023), and its folding head was trained on over 325 thousand protein structures. It achieves comparable performance to AlphaFold2 (Jumper et al., 2021) and is included in our benchmark due to its faster processing.

We also include methods that consider only antigens:

**ESMBind**(Schreiber, 2023) is a language model that predicts protein binding sites for single protein sequence inputs. It is fine-tuned based on ESM2 (Lin et al., 2023) using Low-Rank Adaptation (Hu et al., 2021) on a dataset composed of more than 200 thousand protein sequences with annotated binding sites.

**MaSIF-site**(Gainza et al., 2020) is a geometric deep learning method that predicts binding sites on the surface of an input protein structure. It converts antigen surfaces into mesh graphs, with each mesh vertex encoded with geometric and physicochemical descriptors, and predicts binding sites as a set of mesh vertices.

**PECAN and EPMP**(Pittala and Bailey-Kellogg, 2020; Vecchio et al., 2021) are graph neural networks that predict epitope residues taking antibody-antigen structure pairs as input. They use position-specific scoring matrices (PSSM) as node embeddings and graph attention networks to predict the binary labels of nodes in the antigen graph. For details, we refer readers to Appendix A.1.

Two **complementary surveys** are notable:

Zhao et al. (2024) benchmarked docking methods like ZDOCK (Pierce et al., 2011), ClusPro (Kozakov et al., 2017), and HDOCK (Yan et al., 2017), and Alphafold-Multimer (Evans et al., 2022) on a set of \(112\) antibody-antigen complexes. They showed that all docking methods gave a success rate of \(8.0\%\) at most if using the top 5 decoys; AlphaFold-Multimer showed a better performance with a \(15.3\%\) success rate, so we included **AlphaFold-Multimer** (version 2.3) but benchmarked on a separate subset of the proposed dataset (AsEP) according to its training data cutoff date.

Cia et al. (2023) focused on epitope prediction using a dataset of \(268\) complexes, defining epitope residues as having at least a \(5\%\) change in relative solvent accessibility upon complex formation. They benchmarked various methods, finding existing methods insufficient for accurate epitope prediction.

## 3 Problem Formulation

Antibody-antigen interaction is important for analyzing protein structures. The problem can be formulated as a bipartite graph link prediction task. The inputs are two disjoint graphs, an antibody graph \(G_{A}=(V_{A},E_{A})\) and an antigen graph \(G_{B}=(V_{B},E_{B})\), where \(V_{x}\) is the vertice set for graph \(x\) and \(E_{x}\) is the edge set for graph \(x\). Since neural networks only take continuous values as input, we encode each vertex into a vector with the function \(h:V^{D}\). The design choice of the encoding function depends on the methods. For example, \(h\) can be a one-hot encoding layer or pretrained embeddings given by a protein language model. We use different encoding functions for antibodies and antigens: \(h_{A}:V_{A}^{D_{A}}\), and \(h_{B}:V_{B}^{D_{B}}\).

In addition, \(E_{A}\{0,1\}^{|V_{A}||V_{A}|}\) and \(E_{B}\{0,1\}^{|V_{B}||V_{B}|}\) denote the adjacency matrices for the antibody and antigen graphs, respectively. In this work, the adjacency matrices are calculated based on the distance matrix of the residues. Each entry \(e_{ij}\) denotes the proximity between residue \(i\) and residue \(j\); \(e_{ij}=1\) if the Euclidean distance between any non-hydrogen atoms of residue \(i\) and residue \(j\) is less than \(4.5\)A, and \(e_{ij}=0\) otherwise (See example in Figure 1. The antibody graph \(G_{A}\) is constructed by combining the CDR residues from the heavy and light chains of the antibody, and the antigen graph \(G_{B}\) is constructed by combining the surface residues of the antigen. The antibody and antigen graphs are disjoint, i.e., \(V_{A} V_{B}=\).

We consider two subtasks based on these inputs.

**Epitope Prediction** Epitopes are the regions on the antigen surface recognized by antibodies; in other words, they are a set of antigen residues in contact with the antibody and are determined from the complex structures using the same distance cutoff of \(4.5\)A as aforementioned. For a node in the antigen graph \(v V_{B}\), if there exists a node in the antibody graph \(u V_{A}\) such that the distance between them is less than \(4.5\)A, then \(v\) is an epitope node. Epitope nodes and the remaining nodes in \(G_{B}\) are assigned labels of \(1\) and \(0\), respectively. The first task is then a node classification within the antigen graph \(G_{B}\) given the antibody graph \(G_{A}\).

Figure 1: An example illustrating interacting residues. The two dashed lines indicate distances between non-hydrogen atoms from different interacting residues across two protein chains, with each chain’s carbon atoms colored cyan and green.

This classification takes into account the structure of the antibody graph, \(G_{A}\), mirroring the specificity of antibody-antigen binding interactions. Different antibodies can bind to various antigen locations, corresponding to varying subsets of epitope nodes in \(G_{B}\). This formulation differs from conventional antigen-only epitope prediction that does not consider the antibody structure and ends up predicting the likelihood of the subset of antigen nodes serving as epitopes, such as ScanNet (Tubiana et al., 2022), MaSIF (Gainza et al., 2020). The goal is to develop a binary classifier \(f:V_{B}\{0,1\}\) that takes both antibody and antigen graphs as input and predicts the labels for antigen nodes:

\[f(v\,;G_{B},G_{A})=1&;\\ 0&.\] (1)

**Bipartite Link Prediction** The second task takes it further by predicting concrete interactions between nodes in \(G_{A}\) and \(G_{B}\), resulting in a bipartite graph that represents these antibody-antigen interactions. Moreover, this helps attribute the model performance to specific interactions at the molecular level and provide more interpretability. Accurately predicting these interactions is critical for understanding the binding mechanisms and for guiding antibody engineering. We model the antibody-antigen interaction as a bipartite graph \(K_{m,n}=(V_{A},V_{B},E)\) where \(m=|V_{A}|\) and \(n=|V_{B}|\) denote the numbers of nodes in the two graphs, respectively, and \(E\) denotes all possible inter-graph links. In this bipartite graph, a node from the antibody graph is connected to each node in the antigen graph via an edge \(e E\). The task is then to predict the label of each bipartite edge. If the residues of a pair of nodes are located within \(4.5\)A of each other, referred to as _in contact_, the edge is labeled as \(1\); otherwise, \(0\). For any pair of nodes, denoted as \((v_{a},v_{b})\)\( v_{a} V_{A},v_{b} V_{B}\), the binary classifier \(g:K_{m,n}\{0,1\}\) is formulated as below:

\[g(v_{a},v_{b};K_{m,n})=1&$ and $v_{b}$ are in contact}\\ 0&.\] (2)

## 4 AsEP Dataset

We present our dataset AsEP of filtered, cleaned and processed antibody-antigen complex structures. It is the largest collection of antibody-antigen complex structures to our knowledge. Antibodies are composed of two heavy chains and two light chains, each of which contains a variable domain (areas of high sequence variability) composed of a variable heavy (VH) and a variable light (VL) domain responsible for antigen recognition and binding (Chothia and Lesk, 1987). These domains have complementarity-determining regions (CDR, Figure 2 top blue, yellow, and red regions), which are the primary parts of antibodies responsible for antigen recognition and binding.

### Antibody-antigen complexes

We sourced our initial dataset from the Antibody Database (AbDb) (Ferdous and Martin, 2018), dated 2022/09/26, which contains \(11,767\) antibody files originally collected from the Protein Data Bank (PDB) (Berman et al., 2003). We extracted conventional antibody-antigen complexes that have a VH and a VL domain with a single-chain protein antigen, and there are no unresolved CDR residues due to experimental errors, yielding \(4,081\) antibody-antigen complexes. To ensure data balance, we removed identical complexes using an adapted version of the method described in Krawczyk et al. (2014). We clustered the complexes by antibody heavy and light chains followed by antigen sequences using MMseqs2 (Steinegger and Soding, 2017). We retained only one representative complex for each unique cluster, leading to a refined dataset of \(1,725\) unique complexes. Two additional complexes were manually removed; CDR residues in the complex 6jmr_1P are unknown (labeled as 'UNK') and it is thus impossible to build graph representations upon this complex; 7sgn_OP was also removed because of non-canonical residues in its CDR loops. The final dataset consists of \(1,723\) antibody-antigen complexes. For detailed setup and processing steps, please refer to Appendix A.3.

### Convert antibody-antigen complexes to graphs

These \(1,723\) files were then converted into graph representations, which are used as input for WALLE. In these graphs, each protein residue is modeled as a vertex. Edges are drawn between pairs of residues if any of their non-hydrogen atoms are within \(4.5\)A of each other, adhering to the same distance criterion used in PECAN (Pittala and Bailey-Kellogg, 2020).

**Exclude buried residues** In order to utilize structural information effectively, we focused on surface residues, as only these can interact with another protein. Consequently, we excluded buried residues, those with a solvent-accessible surface area of zero, from the antigen graphs. The solvent-accessible surface areas were calculated using DSSP (Kabsch and Sander, 1983) via Graphein (Jamasb et al., 2021). It is important to note that the number of interface nodes are much smaller than the number of non-interface nodes in the antigen, making the classification task more challenging.

**Exclude non-CDR residues** We also excluded non-CDR residues from the antibody graph, as these are typically not involved in antigen recognition and binding. This is in line with the approach adopted by PECAN (Pittala and Bailey-Kellogg, 2020) and EPMP (Vecchio et al., 2021). Figure 2 provides a visualization of the processed graphs.

**Node embeddings** To leverage the state-of-the-art protein language models, we generated node embeddings for each residue in the antibody and antigen graphs using AntiBERTy (Ruffolo et al., 2021) (via IgFold (Ruffolo et al., 2023) package) and ESM2 (Lin et al., 2022) (esm2_t12_35M_UR50D) models, respectively. In our dataset interface package, we also provide a simple embedding method using one-hot encoding for amino acid residues. Other node embedding methods can be easily incorporated into our dataset interface.

### Dataset split

We propose two types of dataset split settings. The first is a random split based on the ratio of epitope to antigen surface residues, \(}{}\); the second is a more challenging setting where we split the dataset by epitope groups. The first setting is straightforward and used by previous methods, while the second setting requires the model to generalize to unseen epitope groups.

**Split by epitope to antigen surface ratio** As aforementioned, the number of non-interface nodes in the antigen graph is much larger than the number of interface nodes. While epitopes usually have a limited number of residues, typically around \(14.6 4.9\) amino acids (Reis et al., 2022), the antigen surface may extend to several hundred or more residues. The complexity of the classification

Figure 2: Graph visualization of an antibody-antigen complex. **Top**: the molecular structure of an antibody complexed with the receptor binding domain of SARS-Cov-2 virus (PDB code: 7KFW), the antigen. Spheres indicate the alpha carbon atoms of each amino acid. Color scheme: the antigen is colored in magenta, the framework region of the heavy and light chains is colored in green and cyan and CDR 1-3 loops are colored in blue, yellow, and red, respectively. **Bottom**: the corresponding graph. Green vertices are antibody CDR residues and pink vertices are antigen surface residues.

task, therefore, increases with the antigen surface size. To ensure similar complexity among train, validation, and test sets, we stratified the dataset to include a similar distribution of epitope to non-epitope nodes in each set. Table S3 shows the distribution of epitope-to-antigen surface ratios in each set. This led to \(1383\) antibody-antigen complexes for the training set and \(170\) complexes each for the validation and test sets. The list of complexes in each set is provided in the Supplementary Table SI-split-epitope-ratio.csv.

**Split by epitope groups** This is motivated by the fact that antibodies are highly diverse in the CDR loops and by changing the CDR sequences it is possible to engineer novel antibodies to bind different sites on the same antigen. This was previously observed in the EpiPred dataset where Krawczyk et al. (2014) tested the specificity of their method on five antibodies associated with three epitopes on the same antigen, _hen egg white lysozyme_.

We inlcude \(641\) unique antigens and \(973\) epitope groups in our dataset. We include multi-epitope antigens. For example, there are \(64\) distinct antibodies that bind to coronavirus spike protein. We can see that different antibodies bind to different locations on the same antigen. Details of all epitope groups are provided in the Supplementary Table SI-AsEP-entries.csv. We then split the dataset into train, validation, and test sets such that the epitopes in the test set are not found in either train or validation sets. We used an 80%/10%/10% split for the number of complexes in each set. This resulted in \(1383\) complexes for the training set and \(170\) complexes for the validation and test sets. The list of complexes in each set is provided in the Supplementary Table SI-split-epitope-group.csv.

**User-friendly Dataset Interface** We implemented a Python package interface for our dataset using PyTorch Geometric (Fey and Lenssen, 2019). Users can load the dataset as a PyTorch Geometric dataset object and use it with PyTorch Geometric's data loaders. We provide an option to load node embeddings derived from AntiBERTy and ESM2 or simply one-hot embeddings. Each data object in the dataset is a pair of antibody and antigen graphs; both node- and edge-level labels are provided, and the node-level labels are used for the epitope prediction task.

## 5 WALLE: A Hybrid Method for Epitope Prediction

Alongside our dataset interface, we also provide a new method named WALLE. It takes as input a pair of antibody and antigen graphs, constructed as detailed above for the AsEP dataset, and makes node-level and edge-level predictions.

**Graph Modules** The architecture of WALLE incorporates graph modules that process the input graphs of antibody and antigen structures, as depicted in Figure 3). Inspired by PECAN and EPMP, our model treats the antibody and antigen graphs separately, with distinct pathways for each. The antibody graph is represented by node embeddings \(X_{A}\) with a shape of \((M,D_{A})\) and an adjacency matrix \(E_{A}\), while the antigen graph is described by node embeddings \(X_{B}\) with a shape of \((N,D_{B})\) and its corresponding adjacency matrix \(E_{B}\). Both antibody and antigen graph nodes are first projected into the dimensionality of \(128\) using fully connected layers. The resulting embeddings are then passed through two GNN layers consecutively to refine the features and yield updated node embeddings \(X^{}_{A}\) and \(X^{}_{B}\) with a reduced dimensionality of \((M,64)\). The output from the first GNN layer is passed through a ReLU activation function. Outputs from the second GNN layer are directly fed into the _Decoder_ module. These layers operate independently, each with its own parameters, ensuring that the learned representations are specific to the antibody or the antigen. The use of separate graph modules for the antibody and antigen allows for the capture of unique structural and functional characteristics pertinent to each molecule before any interaction analysis. This design choice aligns with the understanding that antibodies and antigens have distinct roles in their interactions and that their molecular features should be processed separately.

Combining unstructured (sequential) with structural modelingThe embedding size, \(D_{A}\) and \(D_{B}\), are determined by the pre-trained protein language model (PPLM). We extracted embeddings from the final layer outputs of each PPLM as node features for our GNNs. We experimented with different combinations of graph neural network (GNN) architectures and PPLM embeddings. We assessed commonly used graph modules, Graph Convolutional Network (GCN) (Kipf and Welling, 2017), Graph Attention Network (GAT) (Velickovic et al., 2018), and GraphSAGE networks (Hamilton et al., 2018); and PPLMs including, AntiBERTy (Ruffolo et al., 2021), ESM2-35M (esm2_t12_35M_UR50D), ESM2-650M (esm2_t33_650M_UR50D) (Lin et al., 2022).

**Decoder** We used a simple decoder to predict binary labels for edges between the antibody and antigen graphs. This decoder takes pairs of node embeddings, output by the graph modules, as input and predicts the probability of each edge. During hyperparameter tuning, we compute the final logits by either taking the inner product of the antibody and antigen embeddings or concatenating them and passing them through a single linear layer with dropout. A sigmoid function is applied to produce the final probabilities. An edge is assigned a binary label of \(1\) if the predicted probability exceeds \(0.5\) or \(0\) otherwise. This is shown as the _Decoder_ module in Figure 3. For the epitope prediction task, we convert edge-level predictions to node-level by summing the predicted probabilities of all edges connected to an antigen node. We assign the antigen node a label of \(1\) if the number of connected edges surpasses a set threshold or \(0\) otherwise. This threshold is treated as a hyperparameter and optimized during experimentation.

**Implementation** We used PyTorch Geometric (Fey and Lenssen, 2019) framework to build our model. The graph modules are implemented using _GCNConv_. _GATConv_, _SAGEConv_ modules from PyTorch Geometric. We trained the model to minimize a loss function consisting of two parts: a weighted binary cross-entropy loss for the bipartite graph link reconstruction and a regularizer for the number of positive edges in the reconstructed bipartite graph. We used the same set of hyperparameters and loss functions for both dataset settings. The loss function and hyperparameters are described in detail in Appendix A.6. We report only the best performance of different combinations of graph modules and pre-trained language model embeddings in Table 1 (refer to Appendix B for the performance of all combinations.

## 6 Results and Discussion

For epitope residue prediction, we evaluated each method on both dataset split settings using the metrics described in Appendix A.2. The results are summarized in Table 0(a) and Table 0(b), showing

Figure 3: A schematic of the preprocessing step that turns an input antibody-antigen complex structure into a graph pair and the model architecture of WALLE.

the average performance metrics across the test set samples. Since various combinations of graph architectures and pre-trained language models were tested for node embeddings, we report only the performance of the best-performing combination in each table. WALLE generally outperforms other methods across all metrics on both dataset splits. As previously noted, existing methods do not evaluate interaction prediction; therefore, the baseline performance for bipartite link prediction is provided in Table S8 for future reference.

Additionally, we benchmarked AlphaFold2-Multimer (AF2M) version 2.3 on the epitope prediction task due to its growing use in complex structure prediction. A new subset of 76 AsEP complexes, excluded from the AF2M training set, was curated for this purpose. Details on the filtering method, the specific AsEP files in this subset, and AF2M performance results can be found in Appendix A.9 and Table S5. While AF2M achieves an MCC of \(0.262\), its performance could be further improved. Additionally, the average runtime per antibody-antigen pair is \(1.66\) hours, which is not optimal for epitope scanning, especially given that all other benchmarked methods here can make predictions within seconds.

Table 1: Performance on test set from dataset split by epitope to antigen surface ratio and epitope groups.

  
**Method** & **MCC** & **Precision** & **Recall** & **AUCROC** & **F1** \\  WALLE & **0.305** (0.023) & **0.308** (0.019) & **0.516** (0.028) & **0.695** (0.015) & **0.357** (0.021) \\ EpiPred & 0.029 (0.018) & 0.122 (0.014) & 0.180 (0.019) & — & 0.142 (0.016) \\ ESMFold & 0.028 (0.010) & 0.137 (0.019) & 0.043 (0.006) & — & 0.060 (0.008) \\ ESMBind & 0.016 (0.008) & 0.106 (0.012) & 0.121 (0.014) & 0.506 (0.004) & 0.090 (0.009) \\ MaSIF-site & 0.037 (0.012) & 0.125 (0.015) & 0.183 (0.017) & — & 0.114 (0.011) \\   

Table 2: Summary of Features Used in Benchmarking Methods.

Hybrid vs structural only and unstructured onlyWe carried out ablation studies (Appendix C) to investigate the impact of different components of WALLE. Specifically, we investigated the combination of GCN with AntiBERTy and ESM-35M for antibody and antigen embeddings. When we replace GCN layers with fully connected layers, the MCC metric decreases by approximately \(39.8\%\), suggesting that GNN layers contribute to performance. This is related to the fact that the interaction between a pair of protein structures depends on the spatial arrangement of the residues (see Reis et al. (2022)). The interface polar bonds, a major source of antibody specificity, tend to shield interface hydrophobic clusters. The PLM embeddings also contribute to performance, as performance drops over \(62.4\%\) when they are replaced with one-hot or BLOSUM62 embeddings. Finally, we investigated whether the choice of PLM affects the model's performance. We found that using AntiBERTy and ESM2 embeddings for antibodies and antigens performed slightly better than using ESM2 embeddings for both antibodies and antigens. This suggests that the choice of the protein language model may impact the model's performance, but a model like ESM2, which is trained on general protein sequences, may contain sufficient information for the epitope prediction task.

WALLE's model architecture is different from existing approaches in that it combines a graph-based method with pre-trained protein language model embeddings. This integration of structural and unstructured (sequential) information leads to a more comprehensive and information-rich approach to epitope prediction. Unlike WALLE, prior graph-based methods, such as PECAN (Pittala and Bailey-Kellogg, 2020) and EPMP (Vecchio et al., 2021), relied on manually engineered features for node embeddings rather than leveraging pre-trained language models. WALLE's node embedding update strategy is similar to that of PECAN, where antibody and antigen graphs are updated separately through distinct graph layers. However, EPMP employs GAT layers to jointly update both antibody and antigen embeddings, incorporating a residual connection to add the updated embeddings back to each graph. WALLE also differs in its task formulation and decoder. While PECAN and EPMP primarily focus on node classification tasks, WALLE predicts bipartite link labels and subsequently aggregates them to obtain node labels. Furthermore, WALLE enhances protein language model embeddings with molecular structures, which is different from methods that rely exclusively on sequence-based information, such as ESMBind and ESMFold, or methods that only use structural information, such as EpiPred and MaSIF-site.

**Generalization to unseen epitopes** While WALLE outperforms other methods in the _epitope group_ dataset split setting, its performance degenerated considerably from the first dataset split setting. This suggests that WALLE is likely biased toward the epitopes in the training set and does not generalize well to unseen epitopes. The performance of the other four methods is not ideal for this task as well. To improve the performance of epitope prediction, we believe this would require a more comprehensive dataset and a more sophisticated model architecture, for example using inductive GNNs (Tern et al., 2020; Zhu et al., 2021; Chen et al., 2022) or pretraining protein language models with active forgetting so that their feature extraction can generalize to unseen epitope, a trick recently proposed to help generalization to unseen languages (Chen et al., 2023). We invite researchers to examine the generalization issue and bring their insights and algorithms to address this issue.

**Edge features** In terms of structure representation, we only used a simple invariant edge feature, the distance matrix, to capture the neighborhood information of each residue. This topological descriptor already performs better than other methods that use sequence-based features. For future work, more edge features can be incorporated to enrich the graph representation, in addition to the invariant edge features used in this work, such as inter-residue distances and edge types used in GearNet (Zhang et al., 2023), and equivariant features, such as rotational and orientational relationships between residues as used in abodckgen (Jin et al., 2022). The incorporation of edge features will invite testing of advanced graph learning algorithms for multi-relational graphs (Nickel et al., 2011; Bordes et al., 2013; Schlichtkrull et al., 2018; Murphy et al., 2019; Chen et al., 2021).

**Dataset** We plan to extend our work to include more types of antibodies. Currently, our dataset focuses solely on conventional antibodies composed of heavy-chain and light-chain variable domains. However, there is a growing interest in novel antibody formats, such as nanobodies, which are single-variable-domain antibodies derived from camelids. These will be included in future releases of AsEP. Additionally, given the abundance of general non-antibody-antigen complexes, pre-training on such complexes could be beneficial in providing the downstream epitope prediction models with a foundational understanding of protein interface, as suggested by PECAN (Pittala and Bailey-Kellogg, 2020). Further finetuning on antibody-antigen complexes would then allow the model to capture the unique characteristics of antibody interfaces, such as specific amino acid compositions and interaction types, that distinguish them from general protein interfaces (Kringelum et al., 2013; Ofran et al., 2008). We leave expansions of our dataset to incorporate such diverse data, pretraining over general proteins and finetuning vertically for antibody-antigen complexes as our future work.

## 7 Conclusion

In this work, we proposed AsEP, a novel benchmarking dataset for the epitope prediction task and the first dataset to cluster antibody-antigen complexes by epitopes. We also introduced WALLE, a model that combines pretrained protein language models with graph neural networks to capture both amino acid contextual and geometric information. We benchmarked WALLE alongside four other methods, demonstrating that it outperforms existing methods on both tasks, though there is still plenty of room for enhancement, especially on the unseen epitopes. Our results suggest that such integration of both structural modeling and unstructured (sequential) modeling improves epitope prediction performance. We discussed future work that could enhance model performance, including enriching edge features for better structural representation, refining model architecture, exploring transfer learning from general protein complexes, and expanding the dataset to include emerging antibody types. This work serves as our starting point for advancing research in antibody-specific epitope prediction.

## 8 Acknowledgments

CL was part-funded by a UCL Centre for Digital Innovation Amazon Web Services (AWS) Scholarship; LD was funded by an ISMB MRC iCASE Studentship (MR/R015759/1). We would also like to thank the NeurIPS reviewers for their valuable feedback and suggestions, which helped us improve the clarity and quality of this work.