# RH-BrainFS: Regional Heterogeneous Multimodal Brain Networks Fusion Strategy

Hongting Ye \({}^{1}\), Yalu Zheng \({}^{1}\), Yueying Li \({}^{1}\), Ke Zhang \({}^{1}\), Youyong Kong \({}^{1,2}\)*, Yonggui Yuan \({}^{3}\)

\({}^{1}\)Jiangsu Provincial Joint International Research Laboratory of Medical Information Processing

School of Computer Science and Engineering, Southeast University

\({}^{2}\)Key Laboratory of New Generation Artificial Intelligence Technology and Its

Interdisciplinary Applications (Southeast University), Ministry of Education, China

\({}^{3}\)Department of Psychosomatics and Psychiatry, Zhongda Hospital

School of Medicine, Southeast University

{yehongting, 220212084, 230228504, kylenz, kongyouyong}@seu.edu.cn

yygylh2000@sina.com

Corresponding author

###### Abstract

Multimodal fusion has become an important research technique in neuroscience that completes downstream tasks by extracting complementary information from multiple modalities. Existing multimodal research on brain networks mainly focuses on two modalities, structural connectivity (SC) and functional connectivity (FC). Recently, extensive literature has shown that the relationship between SC and FC is complex and not a simple one-to-one mapping. The coupling of structure and function at the regional level is heterogeneous. However, all previous studies have neglected the modal regional heterogeneity between SC and FC and fused their representations via "simple patterns", which are inefficient ways of multimodal fusion and affect the overall performance of the model. In this paper, to alleviate the issue of regional heterogeneity of multimodal brain networks, we propose a novel Regional Heterogeneous multimodal Brain networks Fusion Strategy (RH-BrainFS).2 Briefly, we introduce a brain subgraph networks module to extract regional characteristics of brain networks, and further use a new transformer-based fusion bottleneck module to alleviate the issue of regional heterogeneity between SC and FC. To the best of our knowledge, this is the first paper to explicitly state the issue of structural-functional modal regional heterogeneity and to propose a solution. Extensive experiments demonstrate that the proposed method outperforms several state-of-the-art methods in a variety of neuroscience tasks.

## 1 Introduction

Currently, a large number of neuroscience studies are based on unimodal imaging . However, different brain imaging techniques, such as functional magnetic resonance imaging (fMRI)  and diffusion magnetic resonance imaging (dMRI) , reflect different aspects of the brain's internal characteristics. Therefore, it is often insufficient to use a single modality of data for neuroscience research and it is necessary to integrate multiple modalities of imaging data to achieve good performance in neuroscience tasks such as depression classification  and gender classification .

In multimodal brain networks fusion, existing researchs are mainly focused on fusing structural and functional modalities (structural modality is constructed from dMRI and functional modality isconstructed from fMRI) . Most methods directly fuse the two modal representations via "simple patterns" (we define this as **direct interaction**, where two modal features/embeddings are directly combined to perform some computation, e.g. concatenation , weighted summation , or self-attention  techniques.) without considering the issue of regional heterogeneity  between this two modalities. However, extensive literature  has shown that the relationship between structural connectivity (SC) and functional connectivity (FC) is complex and not a simple one-to-one mapping. Specifically, the coupling of structure and function at the regional level is heterogeneous and follows the molecular, cellular and functional hierarchical structure. In other words, structure may be more tightly coupled to function in some regions than in others. This shows that regional heterogeneity is a key factor in linking different modal brain networks.

Based on the above research gap, in this paper we propose a novel regional heterogeneous multimodal brain networks fusion strategy that aims to fully account for the regional heterogeneity among brain networks and achieve better multimodal brain networks fusion performance.

First, the brain network itself has strong regional characteristics , and the combination of neighbouring brain regions can serve as an important criterion for neuroscience tasks . Specifically, regional characteristics behave as subnetwork (also called subgraph) features in the whole brain network , yet other multimodal studies do not take this into account. In this paper, we focus on the characteristics of brain regions from the subgraph pattern. The subgraph pattern have become a relatively hot area of graph representation learning research in recent years . Subgraph convolutional networks can obtain potential representations of each subgraph in the graph, that reflect the regional characteristics of the graph . Therefore, to effectively extract the regional characteristics of each brain region in the brain network, we introduce a Brain Subgraph Networks (BrainSubGNN) module in Sec. 3.2. Our BrainSubGNN is divided into two steps, subgraph sampling and subgraph embedding. The former is used to obtain the subgraph partition of the brain network, the latter to aggregate the subgraph characteristics of the brain network.

Next, since several previous studies have shown that the characteristics of different individual regions in the brain network have widely different influences on neuroscience tasks , we need to pay attention to the influence of these brain region characteristics on neuroscience tasks and quantify them into accurate values. In this paper, we focus on the Transformer  to learn the accurate influence values of these brain region characteristics. Although originally proposed for NLP tasks, there has been recent interest in Transformers  as universal perceptual models . Through the attention mechanism, Transformer can learn the accurate influence values of different tokens well for the classification result, which happens to meet our needs.

However, as mentioned above, there is the issue of modal regional heterogeneity between SC and FC, which is a key factor in linking these two modal brain networks . Many previous multimodal fusion methods, via "direct interaction", have not considered this issue between SC and FC, which are inefficient ways of modality fusion and affect the overall performance of the model. Based on that, avoiding direct interaction between two modalities within the Transformer is the aim of our study. Inspired by MBT , we present a transformer-based fusion bottleneck (Trans-Bottleneck) module for fusing regional heterogeneous brain networks in Sec. 3.3. Specifically, the Trans-Bottleneck module contains two standard transformers  and a certain number of fusion bottlenecks. The standard transformers are used to learn the accurate influence values of each brain region on neuroscience tasks through the attention mechanism. The fusion bottlenecks, as intermediate media for modality fusion, establish connections between regional heterogeneous modalities and learn the key information of each modality in the latent space (we define this as **indirect interaction**, where two modal features/embeddings are not directly combined for the computation.).

The main contributions of this paper are summarized as follows:

* To alleviate the issue of regional heterogeneity of multimodal brain networks, we propose a novel **R**egional **H**eterogeneous multimodal **Brain** networks **F**usion **S**trategy (RH-BrainFS), using BrainSubGNN module and Trans-Bottleneck module to fuse regional heterogeneous multimodal brain networks for neuroscience tasks.
* To the best of our knowledge, this is the first paper to explicitly state the issue of structural-functional modal regional heterogeneity and to propose a solution.
* Extensive experiments demonstrate the effectiveness of RH-BrainFS in multimodal brain networks fusion tasks on depression classification and gender classification datasets.

## 2 Related Work

**Brain Subgraph Networks:** In recent years, subgraph techniques have gained popularity in brain network analysis due to their ability to accurately model certain aspects of brain organization with high consistency to established brain functional systems . This has led to a focus on identifying informative and signaling subgraphs from the entire brain connectome that may be relevant to brain diseases . Various subgraph-based methods have been proposed for brain network research, such as the adaptive dense subgraph discovery (ADSD) model  which uses a likelihood-based approach to extract disease-associated subgraphs from group-level whole-brain connectome data. Other methods, such as an earlier approach from , balance topological information of local and global graphs using subgraphs, while approach  utilizes subgraphs to represent local features in large-scale brain network. Recent research  has also classified brain network by extracting contrastive subgraphs, and the SBLR model  suggests that subgraphs have attractive neurological interpretations and may correspond to outcome-related anatomical circuits. In this paper, our model builds upon these approaches and employs a BrainSubGNN module to efficiently aggregate regional characteristics of brain network.

**MultiModal Brain Networks Fusion:** Over the past few years, several methods have been developed for multimodal fusion in neuroscience research [12; 8; 38; 23; 60]. One traditional approach, SNF , creates an initial similarity network for each feature and iteratively combines them with a non-linear graph fusion formula to generate a final fusion network. However, recent advancements in deep learning have led to the development of more sophisticated multimodal fusion techniques. For instance, the GBDM  model employs both structural and functional information from diffusion and functional magnetic resonance imaging (MRI), respectively, to effectively differentiate individuals with mild cognitive impairment (MCI) from age-matched controls. The MGCN  model uses manifold-based regularization terms to account for inter-modality and intra-modality relationships. One approach  involves deep collaborative learning to capture cross-modal associations and trait-related features. Another approach  combines representations to fuse multimodal data. Moreover, a method  applies a multimodal non-Euclidean brain network analysis technique based on community detection and convolutional autoencoder for epilepsy classification. Additionally, a new adversarial learning-based node-edge graph attention network (AL-NEGAT ) has been proposed for autism spectrum disorder (ASD) recognition based on multi-modal MRI data. However, none of these methods have addressed the issue of modal regional heterogeneity  in multimodal

Figure 1: The overall framework of our proposed RH-BrainFS Model. The structural connectivity (SC) and functional connectivity (FC) are constructed from fMRI and dMRI respectively, the initial bottlenecks \(Z_{b}\) are a set of learnable tokens randomly sampled from standard normal distribution, and the subgraph partition (\(SP_{i}\)) are obtained in the preprocessing stage. Function \(g()\) denotes the BrainSubGNN module.

brain networks. To fill this gap, we propose a new multimodal brain networks fusion strategy aimed at alleviating the modal regional heterogeneity issue and achieving better fusion performance.

## 3 Method

In this section, we present our proposed regional heterogeneous multimodal brain networks fusion strategy RH-BrainFS (as shown in Fig. 1). We begin by discussing the definition of the multimodal brain networks fusion task in Sec. 3.1. We then explain how the BrainSubGNN module captures the regional characteristics of brain networks in Sec. 3.2. Finally, we describe how the Trans-Bottleneck module used in the RH-BrainFS model alleviate the issue of regional heterogeneity in multimodal brain networks in Sec. 3.3.

### Perliminaries

**Multimodal Brain Networks Fusion Task:** Given a multimodal brain networks dataset \(D=\{Sample_{0},Sample_{1},...,Sample_{L-1}\}\), where each sample represents a person, and \(L\) is the size of this dataset. Each \(Sample_{i}=(_{sc},_{fc},y)\) contains a structural connectivity graph (each brain region is viewed as a node in the graph), a functional connectivity graph (ditto) and a class label \(y\{0,1\}\) (0, 1 represent different meanings on different tasks). Each kind of input graph \(=(A,X,V)\) is composed of a node set \(V\), an adjacency matrix \(A^{N N}\) and node features \(X^{N d}\), where \(N=|V|\) denotes the number of nodes and \(d\) denotes the input dimensional of node features. In the brain network graph, node set \(V\) denotes the collection of brain regions, adjacency matrix \(A\) denotes the connectivity between various brain regions and node features \(X\) denotes the original features of each brain region. In the multimodal brain networks classification task, the purpose is to find a mapping function \(g:(_{sc},_{fc}) y\).

### Brain Subgraph Networks

We now explain how BrainSubGNN module captures the regional characteristics of brain networks. As shown in Fig. 2, the BrainSubGNN contains subgraph sampling step and subgraph embedding step.

#### 3.2.1 Subgraph sampling

The purpose of subgraph sampling is to construct a receptive field for each brain region (also named node in graph), it represent the regional characteristics of this brain region. In our RH-BrainFS, rooted subgraph  is utilized to construct receptive field, as the rooted subgraph can exhibit even greater discriminative power than the first-order Weisfeiler-Leman (1-WL) test due to interconnectivity among neighboring nodes [50; 59].

Figure 2: Brain Subgraph Networks (BrainSubGNN). Including subgrah sampling process (1-hop) and subgraph embedding process.

Specifically, we define central subgraph for each node in the whole brain network. The central subgraph describes the surrounding region of the central node. For a input graph \(\) and the central node \(v_{i}\), the central subgraph is denoted as \(}_{i}^{k}=(_{i},_{i},_{i}) \) where \(_{i}=^{k}(i)\) contains k-hop neighbor nodes of \(v_{i}\) and \(_{i}\), \(_{i}\) equal to the corresponding part in the original \(A\) and \(X\). The central subgraph \(}_{i}^{k}\) constructed above includes brain region \(i\) and its k-hops adjacent brain regions. Such a subgraph contains the characteristics of this local brain network, which is of great significance to the brain network. In this work, the subgraph sampling is a preprocessing process before network training, node and edge indice are saved as a binary file. In the training stage, the indice are loaded directly without additional cost on training.

#### 3.2.2 Subgraph embedding

The subgraph partitions of the brain network are obtained in the previous step, each subgraph contains a central brain region and several adjacent brain regions, so we expect to be able to use a method that integrates the characteristics of all the brain regions in the subgraph and extract an embedding to represent the entire subgraph. Inspired by the graph isomorphism network , we consider the subgraph representation task as a multi-set problem and use multi-layer perceptrons to learn an injective function for aggregating regional characteristics the brain subgraph network effectively and learn subgraph representation as:

\[z_{i}^{(l)}=^{(l)}(W_{1}^{(l)}_{i}^{(l)}+_{j _{i} i}W_{2}^{(l)}_{j}^{(l)})\] (1)

where \(_{i}\) denote the hidden representation of node \(i\), and \(W_{1}^{(l)}\), \(W_{2}^{(l)}^{d^{(l)} d^{(l+1)}}\) are learnable weight matrices for the central node and other nodes in subgraph, respectively. By default, we set the MLP as a two-layer fully connected module. The obtained \(z_{i}\) represents the local characteristics of brain region \(i\).

### Transformer-Based Fusion Bottleneck

Through the BrainSubGNN described earlier, we obtain the regional representations of each modality of the brain network \(Z_{i}^{N d_{hid}}\). Next, our goal is to apply an efficient fusion strategy to fuse critical and complementary information from \(Z_{sc}\) and \(Z_{fc}\), extract a distinguishing embedding to represent the features of the whole brain, and serve as a criterion for downstream tasks. We now describe how Trans-Bottleneck module alleviate the issue of regional heterogeneity in multimodal brain networks.

#### 3.3.1 Fusion Bottlenecks

Due to the issue of regional heterogeneity between SC and FC, we are committed to avoiding the direct interaction of two modalities, and we prefer to find an intermediate element as a bridge for the interaction between two modalities (means indirect interaction). Inspired by MBT , we introduce the fusion bottlenecks into neuroscience research.

Specifically, fusion bottlenecks are simply a set of learnable tokens \(Z_{b}^{N_{b} d_{hid}}\), where \(N_{b}\) is a hyperparameter denoting the number of fusion bottlenecks. In this work, we utilize the fusion bottlenecks as intermediate medium to bridge contact of this two modalities (SC and FC). As shown in Fig. 3, the fusion bottlenecks allow information to flow between modalities and fusion bottlenecks (indirect interaction), but limit the flow of information between modalities (direct interaction). This procedure avoid direct interaction between regional heterogeneous modality data, effectively improving the model's performance. The initial bottlenecks are a set of learn

Figure 3: Interaction between three main tokens.

able tokens that are randomly sampled from standard normal distribution \((0,1)\) and then passed to the first RH-BrainFS layer. Finally, the fusion bottlenecks output by the last RH-BrainFS layer are used as the classification basis for downstream tasks (gender classification, major depressive disorder diagnosis, etc.).

#### 3.3.2 Transformer-Based Fusion

Although originally proposed for NLP tasks, there has been recent interest in Transformers as universal perceptual models due to their ability to model dense correlations between tokens. Meanwhile, it turns out that individual regional characteristics of brain networks have different influence values for neuroscience tasks [1; 31; 40; 42]. Based on these, our RH-BrainFS method utilizes the Transformer  as a baseline for the fusion strategy to capture key subgraph characteristics in brain networks.

Specifically, as shown in Fig. 1, we first concatenate the fusion bottlenecks \(Z_{b}\) and each modality representation \(Z_{i}\), and then feed the concatenated tokens to the standard Transformer  model of the corresponding modality. In the Transformer model, the fusion bottlenecks learn important regional characteristics of the brain network for each modality through attention mechanism. The tokens output by the Transformer are re-split according to the previous splicing scheme to obtain new potential features \(Z_{i}^{l+1}\) of the brain network and temporary fusion bottlenecks \(_{b_{i}}^{l+1}\) corresponding to each modality. After that, the temporary fusion bottlenecks \(_{b_{i}}^{l+1}\) of all modalities are matrix-averaged (also named average pooling) to obtain the fusion bottleneck latent representation \(Z_{b}^{l+1}\).

This procedure can be formulated as:

\[[Z_{i}^{l+1}_{b_{i}}^{l+1}]=([ Z_{i}^{l} Z_{b}^{l}];_{i})\] (2)

\[Z_{b}^{l+1}=(_{b_{i}}^{l+1})\] (3)

where \(i\{SC,FC\}\), \(_{i}\) denotes the modality specific Transformer, \([]\) denotes the concatenate operation, \(Z_{b}^{l}\) denotes the fusion bottlenecks in \(l^{th}\) layer, and \(_{b_{i}}^{l+1}\) denotes the temporary fusion bottlenecks in \(i^{th}\) modality specific Transformer.

In this procedure, each modality transfers importance regional brain network characteristics within its modality to the fusion bottlenecks, and the fusion bottlenecks utilize shared characteristics between modalities to guide the learning of each modality's brain network in the next layer.

#### 3.3.3 Downstream Tasks

In the final stage, we complete classification basis for downstream tasks through the output \(Z_{b}^{l}\) in the last layer. Specifically, we first use global mean pooling as the readout function, and then input the result into a MLP to complete classification basis.

\[Logits=((}_{N_{b}}^{i}Z_{i,b}))\] (4)

where \(Z_{i,b}\) denotes the i-th row of \(Z_{b}\), and \(Logits\{0,1\}\). In this paper, on the depression classification task, 0 represents major depressive disorder, 1 represents normal control. And on the gender classification task, 0 represents male, 1 represents female.

## 4 Experiments

In this section, we perform a series of experiments to evaluate the effectiveness of the proposed RH-BrainFS method. First, we provide the detailed experimental settings in Sec. 4.1. Then, we perform comparison experiments on all datasets to compare the performance of different methods in Sec. 4.2. Finally, we perform some ablation studies of the main modules and hyperparameters in the proposed RH-BrainFS method in Sec. 4.3.

### Experimental Settings

**Datasets.** We evaluate our RH-BrainFS method on two different classification tasks investigating structure-function fusion. \(1)\) The gender classification task on Human Connectome Project (HCP) dataset , which contains 560 female samples and 479 male samples. \(2)\) The Major Depressive Disorder (MDD) diagnosis task on the hospital datasets [17; 18], including the Affiliated Zhongda Hospital of Southeast University (Zhongda hospital) and the Second Affiliated Hospital of Xinxiang Medical University (Xinxiang hospital). This study included 48 controls and 62 MDD patients from the Zhongda hospital and 46 controls and 31 MDD patients from the Xinxiang hospital. We also combine Zhongda and Xinxiang as Two-site dataset to construct more difficult and more realistic tasks.

**Preprocessing.** Here, we would briefly introduce how to construct the brain network of SC from dMRI and FC from fMRI.

* SC. The dMRI data is preprocessed using the brain's diffusion toolbox of FMRIB Software Library . Next we construct \(_{sc}=(A_{sc},X_{sc},V_{sc})\) from preprocessed dMRI data. First, we obtain the brain regions (\(V_{sc}\)) of the individual space by mapping the anatomical automatic labeling (AAL) template in the standard space to the individual space. Then, we using DSI Studio software  to implement Fiber tracking. Finally, we obtain the feature \(X_{sc}^{|V_{sc}||V_{sc}|}\) by counting the number of structural connective fibres between the different regions of the AAL, and the adjacency matrix \(A_{sc}\) of the structural graph is obtained by thresholding \(X_{sc}\) with a threshold.
* FC. The fMRI data is preprocessed using the Data Processing Assistant for Resting-State Function (DPARSF)  MRI toolkit. Next we construct \(_{fc}=(A_{fc},X_{fc},V_{fc})\) from preprocessed fMRI data. First, averaged time series are first computed for each brain region with a predefined atlas. Then, the Pearson correlation is utilized to calculate the functional matrix. Finally, the functional matrix is thresholded by proportional quantization to obtain the adjacency matrix \(A_{fc}\). The features \(X_{fc}\) are functional connectivity matrix obtained earlier.

**Metrics.** In this study, we evaluate all the methods using 10-fold cross-validation with the same partition of training and testing splits. Our evaluation metrics include classification accuracy (ACC), sensitivity (SEN), specificity (SPE), f1 score (F1) and ROC-AUC (AUC). Higher values for all metrics indicate better performance. We record the mean and standard deviation on 10 random runs with 10-fold cross-validation.

**Implementation Details.** For all experiments, we adopt Adam as the optimizer and StepLR (step_size=50, gamma=0.8) as the scheduler. The initial learning rate is set to 5e-4 and the dropout rate is set to 0.3. Also we utilize a early stop mechanism that 300 epochs patience in total 500 epochs. In the RH-BrainFS model, we set the k-hop in the subgraph sampling to 1, the number of bottlenecks \(N_{b}\) to 4, the number of attention heads in the Transformer to 4, and the total number of network layers to 2. All our experiments are implemented in PyTorch and trained on one NVIDIA 3090.

### Comparison Experiments

In this section, we verify the performance of our RH-BrainFS against existing baselines on several datasets.

**Baselines.** We choose two categories of methods as comparison methods, both of which are methods for the direct study of SC and FC. The first category is unimodal methods, including FGDN  and BrainGNN , where FGDN uses a spectral graph convolution method to extract brain networks features, and BrainGNN proposes a ROI-aware graph convolution layer and uses pooling to extract brain networks features. In our experiments, SC and FC are used as inputs to the unimodal method, respectively. The second category is multimodal methods, including SVM, Random Forest, MGCN , GBDM , MMGNN  and AL-NEGAT , where SVM and Random Forest concatenate SC and FC as input, MGCN uses manifold-based regularization terms to consider inter-modality and intra-modality relationships, GBDM adopts weighted summation pattern to fuse multimodal brain networks, MMGNN utilises the concatenation method in multimodal tasks and AL-NEGAT combines multimodal information to construct edge feature maps and node feature maps. Code implementations of all baseline methods are taken from their respective original papers.

**Results.** As shown in Tab. 1, our model significantly outperforms the other comparison methods on the all selected datasets. The results show that multimodal methods generally have better performance than unimodal methods because they capture more complementary information. Among the multimodal methods, our RH-BrainFS method achieves the best performance on the all the selected datasets (improvement of 3.51% on HCP, 5.46% on Zhongda hospital dataset, 8.03% on Xinxiang hospital dataset and 5.50% on two-site dataset). The reason for the performance improvement is that our RH-BrainFS method fully considers the issue of regional heterogeneity between SC and FC and proposes an appropriate solution strategy for this issue.

**Visualization.** In order to intuitively display the performance of each multimodal method, we conduct visualization experiments on the HCP dataset. Specifically, we take the graph-level embedding output from the last layer of each multimodal method for t-SNE visualisation. As shown in Fig. 4, although the MGCN and GBDM all form two clusters, these two clusters do not distinguish the two types of samples well and there is still a lot of confusion. The MMGNN is loosely distributed and does not form good class boundary. It can be seen that the Al-NEGAT produces a similar distribution to our method, but there's still a lot of confusion at the class boundary. In contrast, our method eliminates the confusion at the class boundary, achieves a good classification effect, most of the samples can be accurately distinguished and only a small number of samples have errors.

### Ablation Study

In this section, we further perform ablation studies on the main modules and hyperparameters in the proposed RH-BrainFS method.3

    &  &  \\   & & HCP & Zhongda & Xinxiang & Two-site \\  FGDN & FC & 67.56\(\)3.02 & 65.67\(\)3.26 & 67.91\(\)3.27 & 59.34\(\)2.78 \\ FGDN & SC & 63.42\(\)4.79 & 64.02\(\)3.49 & 65.89\(\)5.15 & 68.91\(\)2.53 \\ BrainGNN & FC & 66.41\(\)6.44 & 69.18\(\)3.39 & 73.46\(\)4.33 & 69.55\(\)3.23 \\ BrainGNN & SC & 67.37\(\)5.89 & 70.73\(\)2.07 & 73.66\(\)3.60 & 69.51\(\)2.58 \\  SVM & SC,FC & 74.49\(\)2.97 & 63.21\(\)2.09 & 71.73\(\)1.99 & 66.06\(\)1.56 \\ Random Forest & SC,FC & 68.24\(\)2.94 & 61.45\(\)2.80 & 62.78\(\)1.63 & 62.43\(\)2.19 \\ MGCN & SC,FC & 67.94\(\)5.41 & 75.18\(\)2.34 & 82.24\(\)3.71 & 72.98\(\)2.17 \\ GBDM & SC,FC & 71.02\(\)4.39 & 74.81\(\)2.44 & 80.71\(\)2.83 & 72.48\(\)1.91 \\ MMGNN & SC,FC & 73.33\(\)2.82 & 60.69\(\)3.61 & 68.21\(\)4.44 & 59.72\(\)3.18 \\ AL-NEGAT & SC,FC & 75.12\(\)3.66 & 73.95\(\)3.45 & 75.75\(\)3.81 & 71.86\(\)2.49 \\  RH-BrainFS (ours) & SC,FC & **78.63\(\)4.36** & **80.64\(\)1.58** & **90.27\(\)2.00** & **78.48\(\)1.43** \\   

Table 1: Comparison experiments results (in percentage) on the all chosen datasets (only the accuracy is shown, the full results can be referred to the Appendix A). The best results are marked in bold. The suboptimal results are marked underlined.

Figure 4: t-SNE visualisation of multimodal method on HCP dataset. Each dot denotes a test sample.

**Effectiveness of Main Modules.** First, we perform an ablation study on HCP dataset to validate effectiveness of the main modules, BrainSubGNN and Trans-Bottleneck. Specifically, we replace BrainSubGNN with normal GIN  (process on the whole brain network), and replace Trans-Bottleneck with a standard transformer (compute self-attention directly between two modalities, equal to direct interaction), respectively. As shown in Tab. 2, we can find that both BrainSubGNN and Trans-Bottleneck have a certain effect on the performance of the model (compare the first three rows in the table). Furthermore, we find that the two modules are compatible and complementary, as our RH-BrainFS method (combining the two modules) achieves the best performance among the ablation study.

**Impact of Bottlenecks Number.** We then investigate the impact of varying bottlenecks number. Specifically, we run experiments with # bottlenecks=2,4,6,8, respectively, with all other parameters unchanged. In order to ensure the credibility of the experiment, we perform experiments on two datasets (HCP dataset and Two-site dataset). As shown in Fig. 5, on both datasets, the performance of any number of bottlenecks exceeds the baselines. The main reason for this results is that the Trans-Bottleneck module takes into account the regional heterogeneity issue between SC and FC, and avoids the direct interaction of heterogeneous information, thus achieving good fusion performance. Such results demonstrate the importance of the regional heterogeneity issue in the modality fusion process of SC and FC. At the same time, we find that when # bottlenecks=4, the model performance reaches the best, which reflects that only a small number of bottlenecks is needed to achieve a good multi-modal fusion expression capability.

**Impact of Subgraph Sampling Hops.** Next, we investigate the impact of varying sampling hops of subgraph. In this experiment, we set the range of sampling hops of subgraph from 1 to 5. Likewise, we run experiments on HCP dataset and Two-site dataset. As shown in Fig. 5, it can clearly be seen that as the number of sampling hops increases, the performance of the model generally shows a downward trend, and when the sampling hops are too large, the model actually performs worse than the baselines. The reason for this results is that the brain network itself has strong regional characteristics, and too large sampling hops will cause the sampled subgraph to lose the local characteristics of the brain network and tend towards global characteristics.

   Modules & ACC & SEN & SPE & F1 & AUC \\  w/o BrainSubGNN Trans-Bottleneck & 76.23±3.78 & 71.40±11.94 & 80.36±7.70 & 73.00±6.27 & 75.88±4.12 \\ w/o BrainSubGNN & 77.22±3.89 & 68.44±12.16 & **82.86±8.61** & 72.16±6.34 & 75.65±4.19 \\ w/o Trans-Bottleneck & 76.80±2.93 & 71.14±10.50 & 81.61±7.10 & 73.50±5.13 & 76.38±3.27 \\ RH-BrainFS (ours) & **78.63±4.36** & **75.59±6.75** & 81.25±6.04 & **76.49±4.91** & **78.42±4.38** \\   

Table 2: Ablation study of the main modules in the RH-BrainFS method. The bold font indicates that the evaluation metric achieves the best performance in the ablation study.

Figure 5: The effect of varying hyperparameters. (a) and (c) are experiments on HCP datasets, (b) and (d) are experiments on Two-site datasets.

Figure 6: The effect of varying thresholding values.

The results show that only 1 hop of subgraph sampling is needed to express the local characteristics of the brain network well.

**Impact of Thresholding Values.** Finally, in this paper, thresholding is used to obtain the adjacency matrix of the two modalities during data processing (thresholding \(X_{sc}\) and \(X_{fc}\) to obtain \(A_{sc}\) and \(A_{fc}\), respectively). Thresholding value has been an important ablation study in neuroscience, and the same experiment was conducted in this paper. As shown in Fig. 6, we set the value range of thresholding to [0.02, 0.30], with 0.02 as a step, for a total of 15 thresholding experiments. From the experimental results, there is no clear trend between threshold values and model performance, but the model performs poorly when the threshold value is too high or too low, so in this paper we chose a threshold value of 0.12 as this is when the model performs best.

## 5 Discussions

**Conclusion.** In this paper, we introduce a brain subgraph networks and a transformer-based fusion bottleneck to alleviate the issue of regional heterogeneity between SC and FC, and propose a novel multimodal brain networks fusion strategy (RH-BrainFS). To the best of our knowledge, this is the first paper to explicitly state the issue of structural-functional modal regional heterogeneity and to propose a solution. We validate our method on a variety of downstream task datasets, achieving state-of-the-art performance.

**Limitations and Future Work.** Due to the scarcity and difficulty of collecting and processing data in neuroscience, the only datasets currently available are very limited, despite the fact that we have spent a great deal of manual effort in this area, and thus the research in this paper suffers from a number of possible data bias issues. In our future work, on the one hand, we will do more work on data to mitigate the problem of data bias. On the other hand, although this paper proposes the use of indirect rather than direct interaction, the two may not be mutually exclusive, and in the future we will investigate how to combine the two to achieve better research results.

**Ethical Issues.** With regard to possible ethical issues in data collection, the Human Connectome Project (HCP) dataset, as a publicly available dataset that has been used in numerous previous studies, is undoubtedly not ethically questionable. It is true that the hospital dataset is held in collaboration with our partner hospitals and is not yet publicly available, but the data is collected with the consent of the subjects who are clearly informed of the purpose of the sample collection, and all identifying information about the sample is hidden in the hospital dataset. Therefore, it does not adversely affect any individual, so there are no ethical or moral issues.

**Possible Negative Social Impacts.** As the research in this paper deals with the diagnosis of depression, it is necessary to elaborate here on the possible negative social impacts of this work, despite the fact that all the current work is at the stage of scientific research and has not been put to practical use. Including but not limited to:

* Incorrect diagnosis. AI methods must have the possibility of error, which cannot be avoided, but an incorrect diagnosis will have a significant impact on individuals and society. Therefore, AI tools can only be used as a diagnostic aid, not as a decision maker, and the final decision should still be made by the doctor.
* Leakage of privacy information. In depression dataset, the identity information of the subjects is highly private, and the leakage of identity information will also have unpredictable and significant impact on individuals and society. Therefore, in this work, we have completely hidden the subjects' identifying information (which is also not visible to the staff in the study group) as a way of preventing the leakage of private information.