# Adaptive Data Analysis in a Balanced Adversarial Model

Kobbi Nissim

Department of Computer Science

Georgetown University

kobbi.nissim@georgetown.edu

&Uri Stemmer

Blavatnik School of Computer Science

Tel Aviv University

Google Research

u@uri.co.il

&Eliad Tsfadia

Department of Computer Science

Georgetown University

eliadtsfadia@gmail.com

###### Abstract

In adaptive data analysis, a mechanism gets \(n\) i.i.d. samples from an unknown distribution \(\), and is required to provide accurate estimations to a sequence of adaptively chosen statistical queries with respect to \(\). Hardt and Ullman (2014) and Steinke and Ullman (2015) showed that, in general, it is computationally hard to answer more than \((n^{2})\) adaptive queries, assuming the existence of one-way functions.

However, these negative results strongly rely on an adversarial model that significantly advantages the adversarial analyst over the mechanism, as the analyst, who chooses the adaptive queries, also chooses the underlying distribution \(\). This imbalance raises questions with respect to the applicability of the obtained hardness results - an analyst who has complete knowledge of the underlying distribution \(\) would have little need, if at all, to issue statistical queries to a mechanism which only holds a finite number of samples from \(\).

We consider more restricted adversaries, called _balanced_, where each such adversary consists of two separate algorithms: The _sampler_ who is the entity that chooses the distribution and provides the samples to the mechanism, and the _analyst_ who chooses the adaptive queries, but has no prior knowledge of the underlying distribution (and hence has no a priori advantage with respect to the mechanism).

We improve the quality of previous lower bounds by revisiting them using an efficient _balanced_ adversary, under standard public-key cryptography assumptions. We show that these stronger hardness assumptions are unavoidable in the sense that any computationally bounded _balanced_ adversary that has the structure of all known attacks, implies the existence of public-key cryptography.

## 1 Introduction

Statistical validity is a widely recognized crucial feature of modern science. Lack of validity - popularly known as the _replication crisis_ in science poses a serious threat to the scientific process and also to the public's trust in scientific findings.

One of the factors leading to the replication crisis is the inherent adaptivity in the data analysis process. To illustrate adaptivity and its effect, consider a data analyst who is testing a specific researchhypothesis. The analyst gathers data, evaluates the hypothesis empirically, and often finds that their hypothesis is not supported by the data, leading to the formulation and testing of more hypotheses. If these hypotheses are tested and formed based on the same data (as acquiring fresh data is often expensive or even impossible), then the process is of _adaptive data analysis_ (ADA) because the choice of hypotheses depends on the data. However, ADA no longer aligns with classical statistical theory, which assumes that hypotheses are selected independently of the data (and preferably before gathering data). ADA may lead to overfitting and hence false discoveries.

Statistical validity under ADA is a fundamental problem in statistics, that has received only partial answers. A recent line of work, initiated by Dwork et al. (2015c) and includes (Hardt and Ullman, 2014; Dwork et al., 2015a,b; Steinke and Ullman, 2015a,b; Bassily et al., 2016; Rogers et al., 2016; Russo and Zou, 2016; Smith, 2017; Feldman and Steinke, 2017; Nissim et al., 2018; Feldman and Steinke, 2018; Shenfeld and Ligett, 2019; Jung et al., 2020; Fish et al., 2020; Dagan and Kur, 2022; Kontorovich et al., 2022; Dinur et al., 2023; Blanc, 2023) has resulted in new insights into ADA and robust paradigms for guaranteeing statistical validity in ADA. A major objective of this line of work is to design optimal mechanisms \(\) that initially obtain a dataset \(\) containing \(n\) i.i.d. samples from an unknown distribution \(\), and then answers adaptively chosen queries with respect to \(\). Importantly, all of \(\)'s answers must be accurate with respect to the underlying distribution \(\), not just w.r.t. the empirical dataset \(\). The main question is how to design an efficient mechanism that provides accurate estimations to adaptively chosen statistical queries, where the goal is to maximize the number of queries \(\) can answer. This objective is achieved by providing both upper- and lower-bound constructions, where the lower-bound constructions demonstrate how an adversarial analyst making a small number of queries to an arbitrary \(\) can invariably force \(\) to err. The setting for these lower-bound proofs is formalized as a two-player game between a mechanism \(\) and an adversary \(\) as in Game 1.1.

**Game 1.1** (ADA game between a mechanism \(\) and an adversarial analyst \(\)).:
* \(\) _gets a dataset_ \(\) _of_ \(n\) _i.i.d. samples from an_ _unknown_ _distribution_ \(\) _over_ \(\)_._
* _For_ \(i=1,,\)_:_
* \(\) _sends a query_ \(q_{i}[-1,1]\) _to_ \(\)_._
* \(\) _sends an answer_ \(y_{i}[-1,1]\) _to_ \(\)_._ _(As_ \(\) _and_ \(\) _are stateful,_ \(q_{i}\) _and_ \(y_{i}\) _may depend on the previous messages.)_ \(\) _fails if_ \( i[]\) _s.t._ \(|y_{i}-_{x}[q_{i}(x)]|>1/10\)_._

A question that immediately arises from the description of Game 1.1 is to whom should the distribution \(\) be unknown, and how to formalize this lack of knowledge. Ideally, the mechanism \(\) should succeed with high probability for every unknown distribution \(\) and against any adversary \(\).

In prior work, this property was captured by letting the adversary choose the distribution \(\) at the outset of Game 1.1. Namely, the adversary \(\) can be seen as a pair of algorithms \((_{1},_{2})\), where \(_{1}\) chooses the distribution \(\) and sends a state \(\) to \(_{2}\) (which may contain the entire view of \(_{1}\)), and after that, \(\) and \(_{2}()\) interacts in Game 1.1. In this adversarial model, Hardt and Ullman (2014) and Steinke and Ullman (2015a) showed that, assuming the existence of one way functions, it is computationally hard to answer more than \((n^{2})\) adaptive queries. These results match the state-of-the-art constructions (Dwork et al., 2015c,a,b; Steinke and Ullman, 2015b; Bassily et al., 2016; Feldman and Steinke, 2017, 2018; Dagan and Kur, 2022; Blanc, 2023).1 In fact, each such negative result was obtained by constructing a _single_ adversary \(\) that fails _all_ efficient mechanisms. This means that, in general, it is computationally hard to answer more than \((n^{2})\) adaptive queries even when the analyst's algorithm is known to the mechanism. On the other hand, in each of these negative results, the adversarial analyst has a significant advantage over the mechanism - their ability to select the distribution \(\). This allows the analyst to inject random trapdoors in \(\) (e.g., keys of an encryption scheme) which are then used in forcing a computationally limited mechanism to fail, as the mechanism does not get a hold of the trapdoor information.

For most applications, the above adversarial model seems to be too strong. For instance, a data analyst who is testing research hypotheses usually has no knowledge advantage about the distribution that the mechanism does not have. In this typical setting, even if the underlying distribution \(\) happens to have a trapdoor, if the analyst recovers the trapdoor then the mechanism should also be able to recover it and hence disable its adversarial usage.

In light of this observation, we could hope that in a balanced setting, where the underlying distribution is unknown to both the mechanism and the analyst, it would always be possible for \(\) to answer more than \(O(n^{2})\) adaptive queries. To explore this possibility, we introduce what we call a _balanced_ adversarial model.

**Definition 1.2** (Balanced Adversary).: _A balanced adversary \(\) consists of two isolated algorithms: The sampler \(_{1}\), which chooses a distribution \(\) and provides i.i.d. samples to the mechanism \(\), and the analyst \(_{2}\), which asks the adaptive queries. No information is transferred from \(_{1}\) to \(_{2}\). See Game 1.3._

**Game 1.3** (The ADA game between a mechanism \(\) and a balanced adversary \(=(_{1},_{2})\)).:
* \(_{1}\) _chooses a distribution_ \(\) _over_ \(\) _(specified by a sampling algorithm) and provides_ \(n\) _i.i.d. samples_ \(\) _to_ \(\) _(by applying the sampling algorithm_ \(n\) _times)._ _/*_ \(_{1}\) _does not provide_ \(_{2}\) _with any information */_ \(\) _and_ \(_{2}\) _play Game_ 1.1 _(with respect to_ \(\) _and_ \(\)_)._ \(\) _fails if and only if it fails in Game_ 1.1_._

Note that the difference between the balanced model and the previous (imbalanced) one is whether \(_{1}\) can send a state to \(_{2}\) after choosing the distribution \(\) (in the imbalanced model it is allowed, in contrast to the balanced model).2

We remark that the main advantage of the balanced model comes when considering a publicly known sampler \(_{1}\) (as we do throughout this work). This way, \(_{1}\) captures the common knowledge that both the mechanism \(\) and the analyst \(_{2}\) have about the underlying distribution \(\).

**Question 1.4**.: _Do the lower-bounds proved in prior work hold also for balanced adversaries?_

In this work we answer Question 1.4 in the positive. We do that using a publicly known analyst \(_{2}\) (which even makes it stronger than what is required for a lower bound). I.e., even though the sampler \(_{1}\) and the analyst \(_{2}\) are publicly known and cannot communicate with each other, they fail any computationally bounded mechanism. However, our lower-bound is based on stronger hardness assumptions than in prior work, namely, we use public-key cryptography.

### Our Results

Our first result is a construction of a _balanced_ adversary forcing any computationally bounded mechanim to fail in Game 1.3.

**Theorem 1.5** (Computational lower bound, informal).: _There exists an efficient balanced adversary \(=(_{1},_{2})\) that fails any computationally bounded mechanism \(\) using \((n^{2})\) adaptive queries. Moreover, it does so by choosing a distribution over a small domain._

Our construction in Theorem 1.5 uses the structure of previous attacks of Hardt and Ullman (2014) and Steinke and Ullman (2015a), but relies on a stronger hardness assumption of public-key cryptography. We prove that this is unavoidable.

**Theorem 1.6** (The necessity of public-key cryptography for proving Theorem 1.5, informal).: _Any computationally bounded_ balanced _adversary that follows the structure of all currently known attacks, implies the existence of public-key cryptography (in particular, a_ key-agreement _protocol)._

In Section 1.3 we provide proof sketches of Theorems 1.5 and 1.6, where the formal statements appear in Sections 3 and 4 (respectively) and the formal proofs appear in the supplementary material.

Potential Consequences for the Information Theoretic Setting.Theorem 1.6 has immediate implication to the information theoretic setting, and allow for some optimism regarding the possibility of constructing an inefficient mechanism that answers many adaptive queries.

It is known that an inefficient mechanism can answer exponentially many adaptive queries, but such results have a strong dependency on the domain size. For instance, the Private Multiplicative Weights algorithm of Hardt and Rothblum (2010) can answer \(2^{(n/})}\) adaptive queries accurately. However, this result is not useful whenever \(n O(})\). Indeed, Steinke and Ullman (2015a) showed that this dependency is unavoidable in general, by showing that large domain can be used for constructing a similar, unconditional, adversary that fails any computationally unbounded mechanism after \((n^{2})\) queries. Our Theorem 1.6 implies that such an attack cannot be implemented in the balanced setting, which gives the first evidence that there might be a separation between the computational and information theoretic setting under the balanced adversarial model (in contrast with the imbalanced model).

**Corollary 1.7**.: _There is no balanced adversary that follows the structure of all currently known attacks, and fails any (computationally unbounded) mechanism._

In order to see why Corollary 1.7 holds, suppose that we could implement such kind of attack using a _balanced_ adversary. Then by Theorem 1.6, this would imply that we could construct an information-theoretic key agreement protocol (i.e., a protocol between two parties that agree on a key that is secret from the eyes of a computationally _unbounded_ adversary that only sees the transcript of the execution). But since the latter does not exist, we conclude that such a _balanced_ adversary does not exists either. In other words, we do not have a negative result that rules out the possibility of constructing an inefficient mechanism that can answer many adaptive queries of a _balanced_ adversary, and we know that if a negative result exists, then by Theorem 1.6 it cannot follow the structure of Hardt and Ullman (2014); Steinke and Ullman (2015a).

### Comparison with Elder (2016)

The criticism about the lower bounds of Hardt and Ullman (2014); Steinke and Ullman (2015a) is not new and prior work has attempted at addressing them with only partial success.

For example, Elder (2016) presented a similar "balanced" model (called "Bayesian ADA"), where both the analyst and the mechanism receive a _prior_\(\) which is a family of distributions, and then the distribution \(\) is drawn according to \(\) (unknown to both the mechanism and the analyst).

From an information theoretic point of view, this model is equivalent to ours when the sampler \(_{1}\) is publicly known, since \(_{1}\) simply defines a prior. But from a computational point of view, defining the sampling process (i.e., sampling \(\) and the i.i.d. samples from it) in an algorithmic way is better when we would like to focus on computationally bounded samplers.

Elder (2016) only focused on the information-theoretic setting. His main result is that a certain family of mechanisms (ones that only use the posterior means) cannot answer more than \((n^{4})\) adaptive queries. This, however, does not hold for any mechanism's strategy. In particular, it does not apply to general computationally efficient mechanisms. Our negative result is quantitatively stronger (\(n^{2}\) vs \(n^{4}\)) and it applies for all computationally efficient mechanisms.3

Table 1 summarizes the comparison between Theorem 1.5 and the prior lower bounds (ignoring computational hardness assumptions).

### Techniques

We follow a similar technique to that used in Hardt and Ullman (2014) and Steinke and Ullman (2015a), i.e., a reduction to a restricted set of mechanisms, called _natural_.

**Definition 1.8** (Natural mechanism (Hardt and Ullman, 2014)).: _A mechanism \(\) is natural if, when given a sample \(=(x_{1},,x_{n})^{n}\) and a query \(q[-1,1]\), \(\) returns an answer that is a function solely of \((q(x_{1}),,q(x_{n}))\). In particular, \(\) does not evaluate \(q\) on other data points of its choice._Hardt and Ullman (2014) and Steinke and Ullman (2015a) showed that there exists an adversarial analyst \(}\) that fails any _natural_ mechanism \(\), even when \(\) is computationally unbounded, and even when \(\) is chosen to be the uniform distribution over \(\{1,2,,m=2000n\}\) (I.e., \(\) is known to everyone). While general mechanisms could simply use the knowledge of the distribution to answer any query, _natural_ mechanisms are more restricted, and can only provide answers based on the \(n\)-size dataset \(\) that they get. The restriction to natural mechanisms allowed Steinke and Ullman (2015a) to use _interactive fingerprinting codes_, which enable to reveal \(\) using \((n^{2})\) adaptive queries when the answers are accurate and correlated with \(\).

To construct an attacker \(\) that fails any computationally bounded mechanism (and not just natural mechanisms), prior work forced the mechanism to behave naturally by using a private-key encryption scheme. More specifically, the adversary first samples \(m\) secret keys \(_{1},,_{m}\), and then defines \(\) to be the uniform distribution over the pairs \(\{(j,_{j})\}_{j=1}^{m}\). The adversary then simulates an adversary \(}\) which fails natural mechanisms as follows: a query \([m][-1,1]\) issued by \(}\) is translated by \(\) to a set of \(m\) encryptions \(\{_{j}\}_{j=1}^{m}\) where each \(_{j}\) is an encryption of \((j)\) under the key \(_{j}\). These encryptions define a new query \(q\) that on input \((j,)\), outputs the decryption of \(_{j}\) under the key \(\). However, since \(\) is computationally bounded and has only the secret keys that are part of its dataset \(\), it can only decrypt the values of \(\) on points in \(\), yielding that it effectively behaves _naturally_.

Note that the above attack \(\) is _imbalanced_ as it injects the secret keys \(_{1},,_{m}\) into \(\) and then uses these keys when it forms queries. In other words, even though the attacker \(\) is known to the mechanism, \(\) is able to fail \(\) by creating a secret correlation between its random coins and the distribution \(\).

#### 1.3.1 Balanced Adversary via Identity Based Encryption Scheme

For proving Theorem 1.5, we replace the private-key encryption scheme with a public-key primitive called _identity-based encryption_ (IBE) scheme (Shamir, 1984; Cocks, 2001; Boneh and Franklin, 2001). Such a scheme enables to produce \(m\) secret keys \(_{1},,_{m}\) along with a master public key \(\). Encrypting a message to a spetile identity \(j[m]\) only requires \(\), but decrypting a message for identity \(j\) must be done using its secret key \(_{j}\). Using an IBE scheme we can achieve a reduction to _natural_ mechanisms via a _balanced_ adversary \(=(_{1},_{2})\) as follows: \(_{1}\) samples keys \(,_{1},,_{m}\) according to the IBE scheme, and defines \(\) to be the uniform distribution over the triplets \(\{(j,,_{j})\}_{j=1}^{m}\). The analyst \(_{2}\), which does not know the keys, first asks queries of the form \(q(j,,)=_{k}\) for every bit \(k\) of \(\) in order to reveal it. Then, it follows a strategy as in the previous section, i.e., it simulates an adversary \(}\) which foils natural mechanisms by translating each query query \([m][-1,1]\) issued by \(}\) by encrypting each \((j)\) for identity \(j\) using \(\). Namely, the IBE scheme allowed the analyst to implement the attack of Hardt and Ullman (2014) and Steinke and Ullman (2015a), but without having to know the secret keys \(_{1},,_{m}\).

We can implement the IBE scheme using a standard public-key encryption scheme: in the sampling process, we sample \(m\) independent pairs of public and secret keys \(\{(_{j},_{j})\}_{j=1}^{m}\) of the original scheme, and define \(=(_{1},,_{m})\). When encrypting a message for identity \(j[m]\), we could simply encrypt it using \(_{j}\) (part of \(\)), which can only be decrypted using \(_{j}\). The disadvantage of this approach is the large master public key \(\) that it induces. Applying the encryption scheme with security parameter of \(\), the master key \(\) will be of size \( m\) and not just \(\) as the sizes of the secret keys. This means that implementing our _balanced_ adversary with such an encryption scheme would result with a distribution over a large domain \(\), which would not rule out the possibility to construct a mechanism for distributions over smaller domains. Yet, Dottling and

   & **Balanced?** & **Class of Mechanisms** & **\# of Queries** & **Dimension** (\([]\)) \\  Steinke and Ullman (2015a) & No & \(\) Algorithms & \(O(n^{2})\) & \(n^{o(1)}\) \\  Steinke and Ullman (2015a) & No & All & \((n^{2})\) & \(O(n^{2})\) \\  Elder (2016) & Yes & Certain Family & \((n^{4})\) & \((n^{4})\) \\  Theorem 1.5 & Yes & \(\) Algorithms & \(O(n^{2})\) & \(n^{o(1)}\) \\  

Table 1: Comparison between the lower bounds for adaptive data analysis.

Garg (2021) showed that it is possible to construct a fully secure IBE scheme using a small mpk of size only \(O( m)\) under standard hardness assumptions (e.g., the _Computational Diffie Helman_ problem (Difie and Hellman, 1976)4 or the hardness of _factoring_).

#### 1.3.2 Key-Agreement Protocol via Balanced Adversary

In order to prove Theorem 1.6, we first explain what type of adversaries the theorem applies to. Recall that in all known attacks (including ours), the adversary A wraps a simpler adversary \(}\) that fails _natural_ mechanisms. In particular, the wrapper A has two key properties:

1. A knows \(_{x}[q_{}(x)]\) for the last query \(q_{}\) that it asks (because it equals to \(_{j=1}^{m}_{}(j)\), where \(_{}\) is the wrapped query which is part of A's view), and
2. If the mechanism attempts to behave accurately in the first \(-1\) rounds (e.g., it answers the empirical mean \(_{x}q(x)\) for every query \(q\)), then A, as a wrapper of A, will be able to ask a last query \(q_{}\) that would fail any computationally bounded last-round strategy for the mechanism.

We next show that any computationally bounded _balanced_ adversary A that has the above two properties, can be used for constructing a key-agreement protocol. That is, a protocol between two computationally bounded parties \(_{1}\) and \(_{2}\) that enable them to agree on a value which cannot be revealed by a computationally bounded adversary who only sees the transcript of the execution. See Protocol 1.9.

**Protocol 1.9** (Key-Agreement Protocol \((_{1},_{2})\) via a _balanced_ adversary \(=(_{1},_{2})\)).:

_Input:_ \(1^{n}\)_. Let_ \(=(n)\) _and_ \(=(n)\) _be the number queries and the domain that is used by the adversary_ A_._

_Operation:_

* \(_{1}\) _emulates_ \(_{1}\) _on input_ \(n\) _for obtaining a distribution_ \(\) _over_ \(\) _(specified by a sampling procedure), and samples_ \(n\) _i.i.d. samples_ \(\)_._
* \(_{2}\) _initializes an emulation of_ \(_{2}\) _on input_ \(n\)_._
* _For_ \(i=1\) _to_ \(\)_:_ 1. \(_{2}\) _receives the_ \(i^{}\) _query_ \(q_{i}\)_from the emulated_ \(_{2}\) _and sends it to_ \(_{1}\)_._ 2. \(_{1}\) _computes_ \(y_{i}=_{x}q_{i}(x)\)_, and sends it to_ \(_{2}\)_._ 3. \(_{2}\) _sends_ \(y_{i}\) _as the_ \(i^{}\) _answer to the emulated_ \(_{2}\)_._
* \(_{1}\) _and_ \(_{2}\) _agree on_ \(_{x}[q_{}(x)]\)_._

The agreement of Protocol 1.9 relies on the ability of \(_{1}\) and \(_{2}\) to compute \(_{x}[q_{}(x)]\). Indeed, \(_{1}\) can accurately estimate it using the access to the sampling procedure, and \(_{2}\) can compute it based on the view of the analyst \(_{2}\) (follows by Property 1).

To prove the secrecy guarantee of Protocol 1.9, assume towards a contradiction that there exists a computationally bounded adversary \(\) that given the transcript of the execution, can reveal \(_{x}[q_{}(x)]\). Now consider the following mechanism for the ADA game: In the first \(-1\) queries, answer the empirical mean, but in the last query, apply \(\) on the transcript and answer its output. By the assumption on \(\), the mechanism will be able to accurately answer the last query, in contradiction to Property 2.

We note that Property 1 can be relaxed by only requiring that A is able to provide a "good enough" estimation of \(_{x}[q_{}(x)]\). Namely, as long as the estimation provided in Property 1 is better than the estimation that an adversary can obtain in Property 2 (we prove that an \(n^{(1)}\) multiplicative gap suffices), this would imply that Protocol 1.9 is a _weak_ key-agreement protocol, which can be amplified to a fully secure one using standard techniques.

We also note that by requiring in Game 1.3 that \(_{1}\) samples from \(\) according to the sampling procedure, we implicitly assume here that sampling from \(\) can be done efficiently (because \(_{1}\) is assumed to be computationally bounded). Our reduction to key-agreement relies on this property, since if sampling from \(\) could not be done efficiently, then \(_{1}\) would not have been a computationally bounded algorithm.

### Perspective of Public Key Cryptography

Over the years, cryptographic research has proposed solutions to many different cryptographic tasks under a growing number of (unproven) computational hardness assumptions. To some extent, this state of affairs is unavoidable, since the security of almost any cryptographic primitive implies the existence of one-way functions (Impagliazzo and Luby, 1989) (which in particular implies that \(P NP\)). Yet, all various assumptions can essentially be divided into two main types: _private key_ cryptography and _public key_ cryptography (Impagliazzo, 1995). The former type is better understood: A series of works have shown that the unstructured form of hardness guaranteed by one-way functions is sufficient to construct many complex and structured primitives such as pseudorandom generators (Hastad et al., 1999), pseudorandom functions (Goldreich et al., 1986) and permutations (Luby and Rackoff, 1988), commitment schemes (Naor, 1991, Hailner et al., 2009), universal one-way hash functions (Rompel, 1990), zero-knowledge proofs (Goldreich et al., 1987), and more. However, reductions are much less common outside the one-way functions regime, particularly when constructing public-key primitives. In the famous work of Impagliazzo and Rudich (1989) they gave the first evidence that _public key_ cryptography assumptions are strictly stronger than one-way functions, by showing that key-agreement, which enables two parties to exchange secret messages over open channels, cannot be constructed from one-way functions in a black-box way.

Our work shows that a _balanced_ adversary for the ADA game that has the structure of all known attacks, is a primitive that belongs to the public-key cryptography type. In particular, if public-key cryptography does not exist, it could be possible to construct a computationally bounded mechanism that can handle more than \((n^{2})\) adaptive queries of a _balanced_ adversary (i.e., we currently do not have a negative result that rules out this possibility).

### Other Related Work

Nissim et al. (2018) presented a variant of the lower bound of Steinke and Ullman (2015) that aims to reduce the number of queries used by the attacker. However, their resulting lower bound only holds for a certain family of mechanisms, and it does not rule out all computationally efficient mechanisms.

Dinur et al. (2023) revisited and generalized the lower bounds of Hardt and Ullman (2014) and Steinke and Ullman (2015) by showing that they are a consequence of a space bottleneck rather than a sampling bottleneck. Yet, as in the works by Hardt, Steinke, and Ullman, the attack by Dinur et al. relies on the ability to choose the underlying distribution \(\) and inject secret trapdoors in it, and hence it utilizes an _imbalanced_ adversary.

Recently, lower bounds constructions for the ADA problem were used as a tool for constructing (conditional) lower bounds for other problems, such as the space complexity of _adaptive streaming algorithms_(Kaplan et al., 2021) and the time complexity of _dynamic algorithms_(Beimel et al., 2022). Our lower bound for the ADA problem is qualitatively stronger than previous lower bounds (as the adversary we construct has less power). Thus, our lower bound could potentially yield new connections and constructions in additional settings.

### Conclusion and Open Problems

In this work we present the balanced adversarial model for the ADA problem, and show that the existence of a balanced adversary that has the structure of all previously known attacks is equivalent to the existence of public-key cryptography. Yet, we do not know what is the truth outside of the public-key cryptography world. Can we present a different type of efficient attack that is based on weaker hardness assumptions (like one-way functions)? Or is it possible to construct an efficient mechanism that answer more than \((n^{2})\) adaptive queries assuming that public-key cryptography does not exist? We also leave open similar questions regarding the information theoretic case. We currently do not know whether it is possible to construct an unbounded mechanism that answers exponential number of queries for any distribution \(\) (regardless of its domain size).

In a broader perspective, lower bounds such as ours show that no general solution exists for a problem. They often use unnatural inputs or distributions and rely on cryptographic assumptions. They are important as guidance for how to proceed with a problem, e.g., search for mechanisms that would succeed if the underlying distribution is from a "nice" family of distributions.

## 2 Preliminaries

### Notations

We use calligraphic letters to denote sets and distributions, uppercase for random variables, and lowercase for values and functions. For \(n\), let \([n]=\{1,2,,n\}\). Let \((n)\) stand for a negligible function in \(n\), i.e., a function \((n)\) such that for every constant \(c>0\) and large enough \(n\) it holds that \((n)<n^{-c}\). For \(n\) we denote by \(1^{n}\) the \(n\)-size string \(1 1\) (\(n\) times). Let \(\) stand for probabilistic polynomial time. We say that a pair of algorithms \(=(_{1},_{2})\) is \(\) if both \(_{1}\) and \(_{2}\) are \(\) algorithms.

### Distributions and Random Variables

Given a distribution \(\), we write \(x\), meaning that \(x\) is sampled according to \(\). For a multiset \(\), we denote by \(_{}\) the uniform distribution over \(\), and let \(x\) denote that \(x_{}\). For a distribution \(\) and a value \(n\), we denote by \(^{n}\) the distribution of \(n\) i.i.d. samples from \(\). For a distribution \(\) over \(\) and a query \(q[-1,1]\), we abuse notation and denote \(q():=_{x}[q(x)]\), and similarly for \(=(x_{1},,x_{n})^{*}\) we abuse notation and denote \(q():=_{x}[q(x)]=_{i= 1}^{n}x_{i}\).

### Cryptographic Primitives

#### 2.3.1 Key Agreement Protocols

The most basic public-key cryptographic primitive is a (\(1\)-bit) _key-agreement_ protocol, defined below.

**Definition 2.1** (key-agreement protocol).: _Let \(\) be a two party protocol between two interactive \(\) algorithms \(_{1}\) and \(_{2}\), each outputs \(1\)-bit. Let \((1^{n})\) denote a random execution of the protocol on joint input \(1^{n}\) (the security parameter), and let \(O_{n}^{1},O_{n}^{2}\) and \(T_{n}\) denote the random variables of \(_{1}\)'s output, \(_{2}\)'s output, and the transcript (respectively) in this execution. We say that \(\) is an \((,)\)-key-agreement protocol if the following holds for any \(\) ("eavesdropper") \(\) and any \(n\):_

**Agreement:**: \(O_{n}^{1}=O_{n}^{2}(n)\)_, and_
**Secrecy:**: \((T_{n})=O_{n}^{1}(n)\)_._

_We say that \(\) is a fully-secure key-agreement protocol if it is an \((1-(n),1/2+(n))\)-key-agreement protocol._

#### 2.3.2 Identity-Based Encryption

An Identity-Based Encryption (IBE) scheme [Shamir, 1984, Cocks, 2001, Boneh and Franklin, 2001] consists of four \(\) algorithms \((,,,)\) defined as follows:

\((1^{})\): given the security parameter \(\), it outputs a master public key \(\) and a master secret key \(\).

\((,)\): given the master secret key \(\) and an identity \([n]\), it outputs a decryption key \(_{}\).

\((,,)\): given the master public key \(\), and identity \([n]\) and a message \(\), it outputs a ciphertext \(\).

\((_{},)\): given a secret key \(_{}\) for identity \(\) and a ciphertext \(\), it outputs a string \(\).

The following are the properties of such an encryption scheme:

**Completeness:** For all security parameter \(\), identity \([n]\) and a message \(\), with probability \(1\) over \((,)(1^{})\) and \(_{}(,)\) it holds that

\[(_{},(, ,))=\]

**Security:** For any \(\) adversary \(=(_{1},_{2})\) it holds that:

\[[IND^{IBE}_{}(1^{})=1] 1/2+()\]

where \(IND^{IBE}_{}\) is shown in Experiment 2.2.5

**Experiment 2.2** (\(IND^{IBE}_{}(1^{})\)).:
1. \((,)(1^{})\)_._
2. \((^{*},(^{0}_{1},,^{0}_{k}),(^{ 1}_{1},,^{1}_{k}),)^{( ,)}_{1}()\) _where_ \(^{0}_{i}=^{1}_{i}\) _for every_ \(i[k]\) _and for each query_ \(\) _by_ \(_{1}\) _to_ \((,)\) _we have that_ \(^{*}\)_._
3. _Sample_ \(b\{0,1\}\)_._
4. _Sample_ \(^{*}_{i}(,^{*},^{0}_{i})\) _for every_ \(i[k]\)_._
5. \(b^{}^{(,)}_{2}(,(^{*}_{1},^{*}_{k}),)\) _where for each query_ \(\) _by_ \(_{2}\) _to_ \((,)\) _we have that_ \(^{*}\)_._
6. _Output_ \(1\) _if_ \(b=b^{}\) _and_ \(0\) _otherwise._

Namely, the adversary chooses two sequences of messages \((^{0}_{1},,^{0}_{k})\) and \((^{1}_{1},,^{1}_{k})\), and gets encryptions of either the first sequence or the second one, where the encryptions made for identity \(^{*}\) that the adversary does not hold its key (not allowed to query \(\) on input \(^{*}\)). The security requirement means that she cannot distinguish between the two cases (except with negligible probability).

**Theorem 2.3** (Dottling and Garg (2021)).: _Assume that the Computational Diffie-Hellman (CDH) Problem is hard. Then there exists an IBE scheme \(=(,,,)\) for \(n\) identities such that given a security parameter \(\), the master keys and each decryption key are of size \(O( n)\).6_

### Balanced Adaptive Data Analysis

As described in the introduction, the mechanism plays a game with a _balanced_ adversary that consists of two (isolated) algorithms: a _sampler_\(_{1}\), which chooses a distribution \(\) over a domain \(\) and provides \(n\) i.i.d. samples to \(\), and an _analyst_\(_{2}\), which asks the adaptive queries about the distribution. Let \(_{n,,}[,=(_{1}, _{2})]\) denote Game 1.3 on public inputs \(n\) - the number of samples, \(\) - the number of queries, and \(\) - the domain. We denote by output \(1\) the case that \(\) fails in the game, and \(0\) otherwise. Since this work mainly deals with computationally bounded algorithms that we would like to model as \(\) algorithms, we provide \(n\) and \(\) in unary representation. We also assume for simplicity that \(\) is finite, which allows to represent each element as a binary vector of dimension \(||\), and we provide the dimension in unary representation as well.

All previous negative results (Hardt and Ullman (2014); Steinke and Ullman (2015a); and Dinur et al. (2023)) where achieved by reduction to a restricted family of mechanisms, called _natural_ mechanisms (Definition 1.8). These are algorithms that can only evaluate the query on the sample points they are given.

For _natural_ mechanisms (even unbounded ones), the following was proven.

**Theorem 2.4** (Hardt and Ullman (2014); Steinke and Ullman (2015a)).: _There exists a pair of \(\) algorithms \(}=(}_{1},}_{2})\) such that for every natural mechanism \(}\) and every large enough \(n\) and \(=(n^{2})\) it holds that_

\[_{n,,=[2000n]}[}, }]=1>3/4.\] (1)_In particular, \(}_{1}\) always chooses the uniform distribution over \([2000n]\), and \(}_{2}\) uses only queries over the range \(\{-1,0,1\}\)._

## 3 Constructing a Balanced Adversary via IBE

We prove that if an IBE scheme exists, then there is an efficient reduction to _natural_ mechanisms that holds against any ppt mechanism, yielding a general lower bound for the computational case.

**Theorem 3.1** (Restatement of Theorem 1.5).: _Assume the existence of an IBE scheme \(\) that supports \(m=2000n\) identities with security parameter \(=(n)\) s.t. \(n()\) (e.g., \(=n^{0.1}\)) using keys of length \(k=k(n)\). Then there exists a ppt balanced adversary \(=(_{1},_{2})\) and \(=(n^{2})+k\) such that for every ppt mechanism \(\) it holds that_

\[_{n,,=[m]\{0,1\}^{2k}}[,]=1>3/4-(n).\]

The proof of the theorem is given in the supplementary material. Note that we use a domain \(\) with \(||=2k+ n+O(1)\), and by applying Theorem 2.3, the lower bound holds for \(k=O( n)\) under the CDH hardness assumption.

## 4 Reduction to Natural Mechanisms Implies Key Agreement

We prove that any ppt _balanced_ adversary \(=(_{1},_{2})\) that has the structure of all known lower bounds (Hardt and Ullman (2014); Steinke and Ullman (2015); Dinur et al. (2023) and ours in Section 3), can be used to construct a _key-agreement_ protocol.

All known constructions use an adversary \(\) that wraps the adversary \(}\) for the natural mechanisms case (Theorem 2.4) by forcing every mechanism \(\) to behave _naturally_ using cryptography. In particular, they all satisfy Properties 1 and 2 from Section 1.3.2.

The formal statement is given in the following theorem. The proof is provided in the supplementary material.

**Theorem 4.1** (Restatement of Theorem 1.6).: _Assume the existence of a ppt adversary \(=(_{1},_{2})\) and functions \(=(n)(n)\) and \(=(n)\) with \(||(n)\) such that the following holds: Let \(n\) and consider a random execution of \(_{n,,}[,]\) where \(\) is the mechanism that given a sample \(\) and a query \(q\), answers the empirical mean \(q()\). Let \(D_{n}\) and \(Q_{n}\) be the (r.v.'s of the) values of \(\) and \(q=q_{}\) (the last query) in the execution (respectively), let \(T_{n}\) be the transcript of the execution between the analyst \(_{2}\) and the mechanism \(\) (i.e., the queries and answers), and let \(V_{n}\) be the view of \(_{2}\) at the end of the execution (without loss of generality, its input, random coins and the transcript). Assume that_

1. \(\)ppt _algorithm_ F _s.t._ \( n:\;|(V_{n})-Q_{n}(D_{n})| n^{-1 /10} 1-(n)\)_, and_
2. \(\)ppt _algorithm_ G _and_ \( n:\;|(T_{n})-Q_{n}(D_{n})| 1/10  1/4+(n)\)_._

_Then using \(\) and \(\) it is possible to construct a fully-secure key-agreement protocol._

Note that Assumption 1 in Theorem 4.1 formalizes the first property in which the analyst knows a good estimation of the true answer, and the ppt algorithm \(\) is the assumed knowledge extractor. Assumption 2 in Theorem 4.1 formalizes the second property which states that the mechanism, which answers the empirical mean along the way, will fail in the last query, no matter how it chooses to act (this behavior is captured with the ppt algorithm \(\)), and moreover, it is enough to assume that this requirement only holds with respect to to transcript of the execution, and not with respect to the view of the mechanism.

We refer to the full version of the paper, given in the supplementary material, for all the missing proofs.