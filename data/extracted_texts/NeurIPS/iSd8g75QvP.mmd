# A Trichotomy for Transductive Online Learning

Steve Hanneke

Department of Computer Science

Purdue University

steve.hanneke@gmail.com &Shay Moran

Faculty of Mathematics,

Faculty of Computer Science, and

Faculty of Data and Decision Sciences

Technion - Israel Institute of Technology

smoran@technion.ac.il &Jonathan Shafer

Computer Science Division

UC Berkeley

shaferjo@berkeley.edu

###### Abstract

We present new upper and lower bounds on the number of learner mistakes in the 'transductive' online learning setting of Ben-David, Kushilevitz and Mansour (1997). This setting is similar to standard online learning, except that the adversary fixes a sequence of instances \(x_{1},,x_{n}\) to be labeled at the start of the game, and this sequence is known to the learner. Qualitatively, we prove a _trichotomy_, stating that the minimal number of mistakes made by the learner as \(n\) grows can take only one of precisely three possible values: \(n\), \(((n))\), or \((1)\). Furthermore, this behavior is determined by a combination of the VC dimension and the Littlestone dimension. Quantitatively, we show a variety of bounds relating the number of mistakes to well-known combinatorial dimensions. In particular, we improve the known lower bound on the constant in the \((1)\) case from \(()\) to \(((d))\) where \(d\) is the Littlestone dimension. Finally, we extend our results to cover multiclass classification and the agnostic setting.

## 1 Introduction

In classification tasks like PAC learning and online learning, the learner simultaneously confronts two distinct types of uncertainty: _labeling-related_ uncertainty regarding the best labeling function, and _instance-related_ uncertainty regarding the instances that the learner will be required to classify in the future. To gain insight into the role played by each type of uncertainty, researchers have studied modified classification tasks in which the learner faces only one type of uncertainty, while the other type has been removed.

In the context of PAC learning,  studied a variant of proper PAC learning in which the true labeling function is known to the learner, and only the distribution over the instances is not known. They show bounds on the sample complexity in this setting, which conceptually quantify the instance-related uncertainty. Conversely, labeling-related uncertainty is captured by PAC learning with respect to a fixed (e.g., uniform) domain distribution , a setting which has been studied extensively.

In this paper we improve upon the work of , who quantified the label-related uncertainty in online learning. They introduced a model of _transductive online learning_,1 in which the adversary

[MISSING_PAGE_EMPTY:2]

**Theorem** (**Informal Version of Theorem 4.1)**.: _Every hypothesis class \(\{0,1\}^{}\) satisfies precisely one of the following:_

1. \(M(,n)=n\)_. This happens if_ \(()=\)_._
2. \(M(,n)=((n))\)_. This happens if_ \(()<\) _and_ \(()=\)_._
3. \(M(,n)=(1)\)_. This happens if_ \(()<\)_._ _The_ \(()\) _notations in Items_ 2_. and_ 3_. hide a dependence on_ \(()\)_, and_ \(()\)_, respectively._ The proof uses bounds on the number of mistakes in terms of the _threshold dimension_ (Section 3.2), among other tools.
2. **Littlestone classes.** The minimal constant upper bound in the \((1)\) case of Theorem 4.1 is some value \(C()\) that depends on the class \(\), but the precise mapping \( C()\) is not known in general.  showed that \(C()=())}\). In Section 3 and Appendix A we improve upon their result as follows. **Theorem** (**Informal Version of Theorem 3.1)**.: _Let \(\{0,1\}^{}\) such that \(()=d<\). Then \(M(,n)=((d))\)._
3. **Multiclass setting.** In Section 5, we generalize Theorem 4.1 to the multiclass setting with a finite label set \(\), showing a trichotomy based on the Natarajan dimension. The proof uses a simple result from Ramsey theory, among other tools. Additionally, we show that the DS dimension of  does not characterize multiclass transductive online learning.
4. **Agnostic setting.** In the _standard_ (non-transductive) agnostic online setting,  showed that \(R_{}(,n)\), the agnostic online regret for a hypothesis class \(\) for a sequence of length \(n\) satisfies \[() n} R_{}(,n) O() n  n}.\] (1) Later,  showed an improved bound of \(R_{}(,n)=()  n}\). In Section 6 we show a result similar to Eq. (1), for the _transductive_ agnostic online setting. **Theorem** (**Informl Version of Theorem 6.1)**.: _Let \(\{0,1\}^{}\), such that \(0<()<\). Then the agnostic transductive regret for \(\) is_

\[() n} R(, n) O() n n}.\]

### Related Works

The general idea of bounding the number of mistakes by learning algorithms in sequential prediction problems was introduced in the seminal work of Littlestone . That work introduced the _online_ learning model, where the sequence of examples is revealed to the learner one example at a time. After each example \(x\) is revealed, the learner makes a prediction, after which the true target label \(y\) is revealed. The constraint, which makes learning even plausible, is that this sequence of \((x,y)\) pairs should maintain the property that there is an (unknown) target concept in a given concept class \(\) which is correct on the entire sequence. Littlestone  also identified the optimal predictor for this problem (called the _SOA_, for _Standard Optimal Algorithm_, and a general complexity measure which is precisely equal to the optimal bound on the number of mistakes: a quantity now referred to as the _Littlestone dimension_.

Later works discussed variations on this framework. In particular, as mentioned, the transductive model discussed in the present work was introduced in the work of . The idea (and terminology) of transductive learning was introduced by , to capture scenarioswhere learning may be easier due to knowing in advance which examples the learner will be tested on.  study transductive learning in a model closer in spirit to the PAC framework, where some uniform random subset of examples have their labels revealed to the learner and it is tasked with predicting the labels of the remaining examples. In contrast,  study transductive learning in a sequential prediction setting, analogous to the online learning framework of Littlestone. In this case, the sequence of examples \(x\) is revealed to the learner all at once, and only the target labels (the \(y\)'s) are revealed in an online fashion, with the label of each example revealed just after its prediction for that example in the given sequential order. Since a mistake bound in this setting is still required to hold for _any_ sequence, for the purpose of analysis we may think of the sequence of \(x\)'s as being a _worst case_ set of examples and ordering thereof, for a given learning algorithm.  compare and contrast the optimal mistake bound for this setting to that of the original online model of . Denoting by \(d\) the Littlestone dimension of the concept class, it is clear that the optimal mistake bound would be no larger than \(d\). However, they also argue that the optimal mistake bound in the transductive model is never smaller than \(()\) (as mentioned, we improve this to \((d)\) in the present work). They further exhibit a family of concept classes of variable \(d\) for which the transductive mistake bound is strictly smaller by a factor of \(\). They additionally provide a general equivalent description of the optimal transductive mistake bound in terms of the maximum possible rank among a certain family of trees, each representing the game tree for the sequential game on a given sequence of examples \(x\).

In addition to these two models of sequential prediction, the online learning framework has also been explored in other variations, including exploring the optimal mistake bound under a _best-case_ order of the data sequence \(x\), or even a _self-directed_ adaptive order in which the learning algorithm selects the next point for prediction from the remaining \(x\)'s from the given sequence on each round. .

Unlike the online learning model of Littlestone, the transductive model additionally allows for nontrivial mistake bounds in terms of the sequence _length_\(n\) (the online model generally has \(\{d,n\}\) as the optimal mistake bound). In this case, it follows immediately from the Sauer-Shelah-Perles lemma and a Halving technique that the optimal transductive mistake bound is no larger than \(O(v(n))\), where \(v\) is the VC dimension of the concept class .

## 2 Preliminaries

**Notation 2.1**.: _Let \(\) be a set and \(n,k\). For a sequence \(x=(x_{1},,x_{n})^{n}\), we write \(x_{ k}\) to denote the subsequence \((x_{1},,x_{k})\). If \(k 0\) then \(x_{ k}\) denotes the empty sequence, \(^{0}\)._

**Definition 2.2**.: _Let \(k\), let \(\) and \(\) be sets, and let \(^{}\). A sequence \((x_{1},y_{1}),,(x_{k},y_{k})() ^{k}\) is realizable by \(\), or \(\)-realizable, if \( h\  i[k]:\ h(x_{i})=y_{i}\)._

**Definition 2.3**.: _Let \(\) be a set, let \(\{0,1\}^{}\), let \(d\), and let \(X=\{x_{1},,x_{d}\}\). We say that \(\) shatters \(X\) if for every \(y\{0,1\}^{d}\) there exists \(h\) such that for all \(i[d]\), \(h(x_{i})=y_{i}\). The Vapnik-Chervonenkis (VC) dimension of \(\) is \(()=\{|X|:\ X\ X\}\)._

**Definition 2.4** ().: _Let \(\) be a set and let \(d\). A Littlestone tree of depth \(d\) with domain \(\) is a set_

\[T=\{x_{u}:\ u_{s=0}^{d}\{0,1\}^{s}\}.\] (2)

_Let \(\{0,1\}^{}\). We say that \(\) shatters a tree \(T\) as in Eq. (2) if for every \(u\{0,1\}^{d+1}\) there exists \(h_{u}\) such that_

\[ i[d+1]:\ h(x_{u_{ i-1}})=u_{i}.\]

_The Littlestone dimension of \(\), denoted \(()\), is the supremum over all \(d\) such that there exists a Littlestone tree of depth \(d\) with domain \(\) that is shattered by \(\)._

**Theorem 2.5** ().: _Let \(\) be a set and let \(\{0,1\}^{}\) such that \(d=()<\). Then there exists a strategy for the learner that guarantees that the learner will make at most \(d\) mistakes in the standard (non-transductive) online learning setting, regardless of the adversary's strategy and of number of instances to be labeled._

**Theorem 2.6** (Sauer-Shelah-Perles; ).: _Let \(n,d\), let \(\) be a set of cardinality \(n\), and let \(\{0,1\}^{}\) such that \(()=d\). Then \(||_{i=0}^{n}( )^{d}\)._

## 3 Quantitative Bounds

### Littlestone Dimension: A Tighter Lower Bound

The Littlestone dimension is an upper bound on the number of mistakes, namely

\[ n:\ M(,n)()\] (3)

for any class \(\). This holds because \(()\) is an upper bound on the number of mistakes for standard (non-transductive) online learning , and the adversary in the transductive setting is strictly weaker.

The Littlestone dimension also supplies a lower bound. We give a quadratic improvement on the previous lower bound of , as follows.

**Theorem 3.1**.: _Let \(\) be a set, let \(\{0,1\}^{}\) such that \(d=()<\), and let \(n\). Then_

\[M(,n)\{(d)/2,(n)/2 \}.\]

Proof idea for Theorem 3.1.: Let \(T\) be a Littlestone tree of depth \(d\) that is shattered by \(\), and let \(_{1}\) be a collection of \(2^{d+1}\) functions that witness the shattering. The adversary selects the sequence consisting of the nodes of \(T\) in breadth-first order. For each time step \(t[n]\), let \(_{t}\) denote the version space, i.e., the subset of \(_{1}\) that is consistent with all previously-assigned labels. The adversary's adaptive labeling strategy at time \(t\) is as follows. If \(_{t}\) is very unbalanced, meaning that a large majority of functions in \(_{t}\) assign the same value to \(x_{t}\), then the adversary chooses \(y_{t}\) to be that value. Otherwise, if \(_{t}\) is fairly balanced, the adversary forces a mistake (it can do so without violating \(\)-realizability). The pivotal observation is that: (1) under this strategy, the version space decreases in cardinality significantly more during steps where the adversary forces a mistake compared to steps where it did not force a mistake; (2) let \(x_{t}\) be the \(t\)-th node in the breadth-first order. It has distance \(=(t)\) from the root of \(T\). Because \(T\) is a binary tree, the subtree \(T^{}\) of \(T\) rooted at \(x_{t}\) is a tree of depth \(d-\). In particular, seeing as \(_{t}\) contains only functions necessary for shattering \(T^{}\), \(|_{t}| 2^{d-+1}\), so \(_{t}\) must decrease not too slowly with \(t\). Combining (1) and (2) yields that the adversary must be able to force a mistake not too rarely. A careful quantitative analysis shows that the number of mistakes the adversary can force is at least logarithmic in \(d\). 

The full proof of Theorem 3.1 appears in Appendix A.

### Threshold Dimension

We also show some bounds on the number of mistakes in terms of the threshold dimension.

**Definition 3.2**.: _Let \(\) be a set, let \(X=\{x_{1},,x_{k}\}\), and let \(\{0,1\}^{}\). We say that \(X\) is threshold-shattered by \(\) if there exist \(h_{1},,h_{k}\) such that \(h_{i}(x_{j})=(j i)\) for all \(i,j[k]\)

Figure 1: A shattered Littlestone tree of depth 2. The empty sequence is denoted by \(\).

_The threshold dimension of \(\), denoted \(()\), is the supremum of the set of integers \(k\) for which there exists a threshold-shattered set of cardinality \(k\)._

The following connection between the threshold and Littlestone dimensions is well-known.

**Theorem 3.3** ().: _Let \(\) be a set, let \(\{0,1\}^{}\), and let \(d\). Then:_

1. _If_ \(() d\) _then_ \(() d\)_._
2. _If_ \(() d\) _then_ \(() d\)_._

Item 1 in Theorem 3.3 and Eq. (3) imply that

\[ n:\ M(,n) 2^{()}\]

for any class \(\). Similarly, Item 2 in Theorem 3.3 and Theorem 3.1 imply a mistake lower bound of \(((()))\). However, one can do exponentially better than that, as follows.

**Claim 3.4**.: _Let \(\) be a set, let \(\{0,1\}^{}\) such that \(d=()<\), and let \(n\). Then_

\[M(,n)\{(d)\,(n) \}.\]

One of the ideas used in this proof appeared in an example called \(_{}\) in Section 4.1 of .

Proof of Claim 3.4.: Let \(k=\{(d)\,,(n)\}\) and let \(N=2^{k}\). Let \(X=\{x_{1},,x_{N-1}\}\) be a set that is threshold-shattered by functions \(h_{1},,h_{N-1}\) and \(h_{i}(x_{j})=(j i)\) for all \(i,j[N-1]\). The strategy for the adversary is to present \(X\) in dyadic order, namely

\[x_{},x_{},x_{},x_{},x_{},x_{},x_{},,x_{-1)N}{2^{k}}}.\]

More explicitly, the adversary chooses the sequence \(q=q_{1} q_{2} q_{k}\), where '\(\)' denotes sequence concatenation and

\[q_{i}=(x_{}N},x_{}N},x_{}N},x_ {}N},,x_{-1)}{2^{i}}N})\]

for all \(i[k]\). See Figure 2.

We prove by induction that for each \(i[k]\), all labels chosen by the adversary for the subsequences prior to \(q_{i}\) are \(\)-realizable, and additionally there exists an instance in subsequence \(q_{i}\) on which the adversary can force a mistake regardless of the learners predictions. The base case is that the adversary can always force a mistake on the first instance, \(q_{1}\), by choosing the label opposite to the learner's prediction (both labels \(0\) and \(1\) are \(\)-realizable for this instance). Subsequently, for any \(i>1\), note that by the induction hypothesis, the labels chosen by the adversary for all instances in the previous subsequences are \(\)-realizable. In particular there exists an index \(a[N]\) such that instance \(x_{a}\) has already been labeled, and all the labels chosen so far are consistent with \(h_{a}\). Let \(b\) be the minimal integer such that \(b>a\) and \(x_{b}\) has also been labeled. Then \(x_{a}\) and all labeled instances with smaller indices received label \(1\), while \(x_{b}\) and all labeled instances with larger indices received label \(0\). Because the sequence is dyadic, subsequence \(q_{i}\) contains an element \(x_{m}\) such that \(a<m<b\). The adversary can force a mistake on \(x_{m}\), because \(h_{a}\) and \(h_{m}\) agree on all previously labeled instances, but disagree on \(x_{m}\)

Figure 2: Construction of the sequence \(q\) in the proof of Claim 3.4. \(q\) is a breadth-first enumeration of the depicted binary tree.

Claim 3.4 is used in the proof of the trichotomy (Theorem 4.1, below).

Finally, we note that for every \(d\) there exists a hypothesis class \(\) such that \(d=()\) and

\[ n:\ M(,n)=\{d,n\}.\]

Indeed, take \(=[d]\) and \(=\{0,1\}^{}\). The upper bound holds because \(||=d\), and the lower bound holds by Item 2 in Theorem 4.1, because \(()=d\).

## 4 Trichotomy

**Theorem 4.1**.: _Let \(\) be a set, let \(\{0,1\}^{}\), and let \(n\) such that \(n||\)._

1. _If_ \(()=\) _then_ \(M(,n)=n\)_._
2. _Otherwise, if_ \(()=d<\) _and_ \(()=\) _then_ \[\{\{d,n\},\,|(n)|\} M(,n) O(d(n/d)).\] (4) _Furthermore, each of the bounds in Eq._ (4) _is tight for some classes. The_ \(()\) _and_ \(O()\) _notations hide universal constants that do not depend on_ \(\) _or_ \(\)_._
3. _Otherwise, there exists an integer_ \(C()()\) _(that depends on_ \(\) _and_ \(\) _but does not depend on_ \(n\)_) such that_ \(M(,n) C()\)_._

Proof of Theorem 4.1.: For Item 1, assume \(()=\). Then there exists a set \(X=\{x_{1},,x_{n}\}\) of cardinality \(n\) that is shattered by \(\). The adversary can force the learner to make \(n\) mistakes by selecting the sequence \((x_{1},,x_{n})\), and then selecting labels \(y_{t}=1-_{t}\) for all \(t[n]\). This choice of labels is \(\)-realizable because \(X\) is a shattered set.

To obtain the upper bound in Item 2 the learner can use the _halving algorithm_, as follows. Let \(x=(x_{1},,x_{n})\) be the sequence chosen by the adversary, and let \(|_{x}\) denote the collection of functions from elements of \(x\) to \(\{0,1\}\) that are restrictions of functions in \(\). For each \(t\{0,,n\}\), let

\[_{t}=f|_{x}:\ ( i[t]:\ f(x_{i})=y_ {i})}\]

be a set called the _version space_ at time \(t\). At each step \(t[n]\), the learner makes prediction

\[_{t}=_{b\{0,1\}}f_{t-1}:\ f(x_{t})=b}\,.\]

In words, the learner chooses \(_{t}\) according to a majority vote among the functions in version space \(_{t-1}\), and then any function whose vote was incorrect is excluded from the next version space, \(_{t}\). This implies that for any \(t[n]\), if the learner made a mistake at time \(t\) then

\[|_{t}||_{t-1}|.\] (5)

Let \(M=M(,n)\). The adversary selects \(\)-realizable labels, so \(_{n}\) cannot be empty. Hence, applying Eq. (5) recursively yields

\[1|_{n}| 2^{-M}|_{0}| 2^{-M} O (n/d)^{d},\]

where the last inequality follows from \((_{0})()=d\) and the Sauer-Shelah-Perles lemma (Theorem 2.6). Hence \(M=O(d(n/d))\), as desired.

For the \(\{d,n\}\) lower bound in Item 2, if \(n d\) then the adversary can force \(n\) mistakes by the same argument as in Item 1. For the logarithmic lower bound in Item 2, the assumption that \(()=\) and Theorem 3.3 imply that \(()=\), and in particular \(() n\), and this implies the bound by Claim 3.4.

For Item 3, the assumption \(()=k<\) and Theorem 2.5 imply that for any \(n\), the learner will make at most \(k\) mistakes. This is because the adversary in the transductive setting is strictly weaker than the adversary in the standard online setting. So there exists some \(C()\{0,,k\}\) as desired. 

**Remark 4.2**.: _One can use Theorem 3.1 to obtain a lower bounds for the case of Item 2 in Theorem 4.1. However, that yields a lower bound of \(((n))\), which is exponentially weaker than the bound we show._

## 5 Multiclass Setting

The trichotomy of Theorem 4.1 can be generalized to the multiclass setting, in which the label set \(\) contains more than two labels. In this setting, the VC dimension is replaced by the Natarajan dimension , denoted \(\), and the Littlestone dimension is generalized in the natural way. The result holds for _finite_ sets \(\).

**Theorem 5.1** (Informal Version of Theorem B.3).: _Let \(\) be a set, let \(\) be a finite set, and let \(^{}\). Then \(\) satisfies precisely one of the following:_

1. \(M(,n)=n\)_. This happens if_ \(()=\)_._
2. \(M(,n)=((n))\)_. This happens if_ \(()<\) _and_ \(()=\)_._
3. \(M(,n)=O(1)\)_. This happens if_ \(()<\)_._

The proof of Theorem 5.1 appears in Appendix B, along with the necessary definitions. The main innovation in the proof involves the use of the multiclass threshold bounds developed in Appendix D, which in turn rely on a basic result from Ramsey theory, stated Appendix C.

### The Case of an Infinite Label Set

It is interesting to observe that the analogy between the binary classification and multiclass classification settings breaks down when the label set \(\) is not finite.

**Example 5.2**.: _There exists a class \(^{}\) such that \(\) is countable, \(()\) is infinite, but the class is learnable with a mistake bound of \(M(,n)=1\). To see this, let \(\) be countable, and let \(\{0,1\}^{}\) be a class with \(()=\). For each \(i\), let \(T_{i}\) be a Littlestone tree of depth \(i\) that is shattered by \(\), and let \(\{h^{i}_{1},,h^{i}_{2i+1}\}\) be a subset that witnesses the shattering. Let \(=\{g^{i}_{j}:\;i\;\;j[2^{i+1}]\}\) be a set of functions such that \(g^{i}_{j}(x)=(h^{i}_{j}(x),i,j)\) for all \(i,j\). Let \(=\{0,1\}\). Observe that \(^{}\) is a countable set of functions with a countable set of labels. Furthermore, \(()=\) because \(\) shatters a sequence of suitable Littlestone trees corresponding to \(T_{1},T_{2},\). However, \(\) can be learned with mistake bound \(1\), because a single example of the form \(x,(h^{i}_{j}(x),i,j)\) reveals the correct labeling function \(h^{i}_{j}\)._

Recent work by  has shown that multiclass PAC learning with infinite \(\) is not characterized by the Natarajan dimension, and that instead it is characterized by the DS dimension (introduced by ). It is therefore natural to ask whether the DS dimension might also characterize multiclass transductive online learning with infinite \(\). We show that the answer to that question is negative.

Recall the definition of the DS dimension.

**Definition 5.3**.: _Let \(d\), let \(\) and \(\) be sets, and let \(^{}\). For an index \(i[d]\) and vectors \(y=(y_{1},,y_{d})^{d}\), \(y^{}=(y^{}_{1},,y^{}_{d})^{d}\), we say that \(y\) and \(y^{}\) are \(i\)-neighbors, denoted \(y_{i}y^{}\), if \(\{j[d]:\;y_{j} y^{}_{j}\}=\{i\}\). We say that \(^{d}\) is a d-pseudocube if \(\) is non-empty and finite, and_

\[ y\; i[d]\; y^{}:\; y_{i}y^{}.\]

_For a vector \(x=(x_{1},,x_{d})^{d}\), we say that \(\) DS-shatters \(x\) if the set_

\[|_{x}:=\{(h(x_{1}),,h(x_{d})):\;h\}^{d}\]

_contains a \(d\)-pseudocube._

_Finally, the Daniely-Shalev-Shwartz (DS) dimension of \(\) is_

\[()=\{d:\;( x ^{d}:\;x)\}.\]

See  for figures and further discussion of the DS dimension.

The following claim shows that the DS dimension does not characterize transductive online learning, even when \(\) is finite.

**Claim 5.4**.: _For every \(n\), there exists a hypothesis class \(_{n}\) such that \((_{n})=1\) but the adversary in transductive online learning can force at least \(M(_{n},n)=n\) mistakes._Proof.: Fix \(n\) and let \(=\{0,1,2,,n\}\). Consider a complete binary tree \(T\) of depth \(n\) such that for each \(x\), all the nodes at depth \(x\) (at distance \(x\) from the root) are labeled by \(x\), and each edge in \(T\) is labeled by a distinct label. Let \(\) be a minimal hypothesis class that shatters \(T\), namely, \(\) shatters \(T\) and there does not exist a strict subset of \(\) that shatters \(T\).

Observe that \(M(_{n},n)=n\), because the adversary can present the sequence \(0,1,2,,n-1\) and force a mistake at each time step. To see that \((_{n})=1\), assume for contradiction that there exists a vector \(x=(x_{1},x_{2})^{2}\) that is DS-shattered by \(_{n}\), namely, there exists a \(2\)-pseudocube \(|_{x}\). Note that \(x_{1} x_{2}\), and without loss of generality \(x_{1}<x_{2}\) (\(\) DS-shatters \((x_{1},x_{2})\) if and only if it DS-shatters \((x_{2},x_{1})\)).

Fix \(y\). So \(y=(h(x_{1}),h(x_{2}))\) for some \(h\). Because \(\) is a \(2\)-pseudocube, there exists \(y^{}\) that is a \(1\)-neighbor of \(y\). Namely, there exists \(g\) such that \(y^{}=(g(x_{1}),g(x_{2}))\), \(y_{1}^{} y_{1}\) and \(y_{2}^{}=y_{2}\). However, because each edge in \(T\) has a distinct label, and \(\) is minimal, it follows that for any \(x\),

\[g(x)=h(x)\ \  x^{}\{0,1,,x\}:\ g(x^{ })=h(x^{}).\]

In particular, \(g(x_{2})=y_{2}^{}=y_{2}=h(x_{2})\) implies \(y_{1}^{}=g(x_{1})=h(x_{1})=y_{1}\) which is a contradiction to the choice of \(y^{}\). 

## 6 Agnostic Setting

The _agnostic_ transductive online learning setting is defined analogously to the _realizable_ (non-agnostic) transductive online learning setting described in Section 1.1. An early work by  observed that it is not possible for a learner to achieve vanishing regret in an agnostic online setting with complete information. Therefore, we consider a game with incomplete information, as follows.

First, the adversary selects an arbitrary sequence of instances, \(x_{1},,x_{n}\), and reveals the sequence to the learner. Then, for each \(t=1,,n\):

1. The adversary selects a label \(y_{t}\).
2. The learner selects a prediction \(_{t}\) and reveals it to the adversary.
3. The adversary reveals \(y_{t}\) to the learner.

At each time step \(t[n]\), the adversary may select any \(y_{t}\), without restrictions.2 The learner, which is typically randomized, has the objective of minimizing the _regret_, namely

\[R(A,,x,y)=[\|\{t[n]:\ _{t} y_{t}\} \|]-_{h}|\{t[n]:\ h(x_{t}) y_{t}\}|,\]

where the expectation is over the learner's randomness. In words, the regret is the expected excess number of mistakes the learner makes when it plays strategy \(A\) and the adversary chooses the sequence \(x^{n}\) and labels \(y^{n}\), as compared to the number of mistakes made by the best fixed hypothesis \(h\). We are interested in understanding the value of this game, namely

\[R(,n)=_{A}\ _{x^{n}}\ _{y ^{n}}\ R(A,,x,y),\]

where \(\) is the set of all learner strategies. We show the following result.

**Theorem 6.1**.: _Let \(\) be a set, let \(\{0,1\}^{}\), and let \(n\) such that \(n||\). Assume \(0<()<\). Then the agnostic transductive regret for \(\) on sequences of length \(n\) is_

\[() n} R(,n ) O() nn/()}.\]

The upper bound in Theorem 6.1 follows directly from the the Sauer-Shelah-Perles lemma (Theorem 2.6), together with the following well-known bound on the regret of the _Multiplicative Weights_ algorithm (see, e.g., Theorem 21.10 in ).

**Theorem 6.2**.: _Let \(\) be a set and let \(\{0,1\}^{}\) be finite. There exists an algorithm for the standard (non-transductive) agnostic online learning setting that satisfies_

\[R_{}(,n)||}.\]Theorem 6.2 implies the upper bound of Theorem 6.1, because the adversary in the transductive agnostic setting is weaker than the adversary in the standard agnostic setting.

We prove the lower bound of Theorem 6.1 using an anti-concentration technique from Lemma 14 of . The proof appears in Appendix E.

**Remark 6.3**.: _Additionally:_

1. _If_ \(()=0\) _(i.e., classes with a single function) then the regret is_ \(0\)_._
2. _If_ \(()<\) _and_ \(()<\) _then the regret is_ \(R(,n)=O() n}\)_, by_ _[_1_]_ _(as mentioned above). Namely, in some cases the_ \((n)\) _factor in Theorem_ 6.1 _can be removed._
3. _If_ \(()=\) _then the regret is_ \((n)\)_._

## 7 Future Work

Some remaining open problems include:

1. Showing a sharper bound for the \((1)\) case in the trichotomy (Theorem 4.1). Currently, there is an exponential gap between the best known upper and lower bounds for Littlestone classes.
2. Showing sharper bounds for the \(( n)\) cases in the trichotomy (Theorem 4.1) and multiclass trichotomy (Theorem B.3).
3. Showing a sharper bound for the agnostic case (Theorem 6.1).
4. Characterizing the number of mistakes in the multiclass setting with an infinite label set \(\) (complementing Theorem B.3).