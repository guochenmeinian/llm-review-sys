# An Efficient Dataset Condensation Plugin and

Its Application to Continual Learning

 Enneng Yang\({}^{1}\), Li Shen\({}^{2}\)1, Zhenyi Wang\({}^{3}\)1, Tongliang Liu\({}^{4}\), Guibing Guo\({}^{1}\)1

\({}^{1}\)Northeastern University, China \({}^{2}\)JD Explore Academy, China

\({}^{3}\)University of Maryland, College Park, USA \({}^{4}\)The University of Sydney, Australia

ennengyang@stumail.neu.edu.cn; mathshenli@gmail.com; zwang169@umd.edu

tongliang.liu@sydney.edu.au; guogb@swc.neu.edu.cn

###### Abstract

Dataset condensation (DC) distills a large real-world dataset into a small synthetic dataset, with the goal of training a network from scratch on the latter that performs similarly to the former. State-of-the-art (SOTA) DC methods have achieved satisfactory results through techniques such as accuracy, gradient, training trajectory, or distribution matching. However, these works all perform matching in the high-dimension pixel space, ignoring that natural images are usually locally connected and have lower intrinsic dimensions, resulting in low condensation efficiency. In this work, we propose a simple-yet-efficient dataset condensation plugin that matches the raw and synthetic datasets in a low-dimensional manifold. Specifically, our plugin condenses raw images into two low-rank matrices instead of parameterized image matrices. Our plugin can be easily incorporated into existing DC methods, thereby containing richer raw dataset information at limited storage costs to improve the downstream applications' performance. We verify on multiple public datasets that when the proposed plugin is combined with SOTA DC methods, the performance of the network trained on synthetic data is significantly improved compared to traditional DC methods. Moreover, when applying the DC methods as a plugin to continual learning tasks, we observed that our approach effectively mitigates catastrophic forgetting of old tasks under limited memory buffer constraints and avoids the problem of raw data privacy leakage.

## 1 Introduction

Modern machine learning techniques utilize large-scale real-world datasets and advanced deep neural networks (DNNs) to achieve amazing success in various fields, such as models like SAM  and GPT [6; 38] in the fields of computer vision and natural language processing, both of which have surpassed classical models [51; 42; 43] trained on small datasets. However, training a well-performing model in the machine learning community requires repeated tuning of various aspects of the model , such as the number of layers, learning rate, and other important hyper-parameters. When the dataset is large, the cost of data management and repeated model training becomes unacceptable. As stated in , NAS-Bench-101  spent 100 TPU-years of computing time on the CIFAR10 dataset  for an exhaustive neural architecture search. In contrast, it only takes dozens of TPU minutes to train the best model from scratch using the optimal configuration discovered . Additionally, in continual learning [41; 2; 60; 61], to prevent forgetting old tasks while learning new tasks, a certain amount of old task data is typically stored for replay [58; 57; 2; 41]. Nevertheless, storing large old task datasets is unrealistic due to strict memory and privacy constraints. Therefore, reducing the data size becomes a valuable and emerging research direction.

A direct way to reduce data size is to select a representative subset from the original dataset . This paradigm calculates the importance score for each sample based on specific importance criteria and selects a subset to replace the entire training dataset . The calculation criteria include the distance between the sample and the class center , the gradient norm of sample , and the diversity among samples , etc. However, the selection-based method is not always effective, particularly when the task-condition data information is uniformly distributed in the original data . In such cases, the heuristic coreset selection method can only capture a limited amount of information. Recent research  has shown that generating a small dataset that performs similarly to the original dataset, instead of selecting a subset, is a promising direction. Therefore, dataset condensation (DC) or dataset distillation (DD)  has recently received increasing attention. This novel paradigm distills information from a large real-world dataset into a compact synthetic dataset that produces comparable results to the original dataset. The earliest DD  method uses accuracy value as the direct distillation objective and describes the distillation process as a bi-level optimization problem. However, matching accuracy directly through bi-level optimization involves high computational costs and memory overhead . Therefore, state-of-the-art (SOTA) DC methods perform condensation through surrogate objectives, such as gradient , training trajectory , feature , or distribution  matching, and achieve more satisfactory results.

However, existing DC methods all optimize parameterized condensed images in the original high-dimensional pixel space, overlooking the fact that natural images typically exhibit local connectivity and have low intrinsic dimensionality . More specifically, in the visual self-supervision task, Masked Autoencoder  divides the image into multiple patches and reconstructs the complete image by randomly masking a certain proportion of patches. Experimental results show that masking 75% of the patches can still reconstruct the original image. Also, Maximum Likelihood Estimation  of the image's intrinsic dimensions on ImageNet  dataset shows that although each image contains \(150,528\) pixels, its intrinsic dimension is only between \(26\) and \(43\). We further verify in the appendix that both the original images and the images generated by traditional dataset distillation (e.g., DSA , DM ) are low rank. These results imply that compressing data in high-dimensional pixel space is an inefficient approach.

In this work, we propose a simple yet efficient plugin for dataset condensing that compresses a large dataset into a compact synthetic dataset on a low-dimensional manifold. Specifically, unlike existing DC methods that train a compact dataset \(\) of size \(N D H W\), where \(N\), \(D\), \(H\), and \(W\) are the number, channels, height, and width of images, respectively, we decompose an image representation under each channel \(ch\{1,,D\}\) into a low-dimensional space, and learn two low-rank matrices \(^{N D H r}\) and \(^{N D r W}\), where \(r<<\{H,W\}\) represents the rank size. Obviously, our low-rank method is orthogonal to existing DC methods and can be integrated into SOTA DC methods as a flexible plugin to improve the learning efficiency of DC. Without loss of generality, we conduct extensive experiments on multiple publicly real-world datasets, integrating the proposed plugin into gradient matching-based  and distribution matching-based  DC methods. Experimental results show that using our low-rank plugin significantly reduces storage consumption for compact datasets and achieves comparable performance as SOTA DC methods based on high-dimensional pixel spaces, with the same number of images. Furthermore, under the same memory consumption, our plugin can effectively store more information from the large dataset, thus significantly improving the performance of condensed data in downstream tasks. Lastly, when applying low-rank DC methods to continual learning (CL) tasks, we observe that our approach effectively mitigates catastrophic forgetting of old tasks under the constraints of limited memory buffers and avoids data privacy issues by storing condensed rather than raw data.

The main contributions of this paper are as follows:

* We propose a simple yet effective dataset distillation plugin that condenses a large dataset into a compact synthetic dataset on a low-dimensional manifold, offering an orthogonal approach to existing DC methods.
* Experiments on deep learning tasks demonstrate that the proposed plugin achieves performance comparable to that of existing SOTA DC methods while significantly reducing memory consumption. It achieves significantly better accuracy than existing SOTA DC methods with the same memory consumption.
* We also verify that the dataset condensed in the low-dimensional manifold has good cross-architecture transferability and maintains the excellent characteristics of existing DC methods, such as learning the class distribution information of the large dataset.

* When applying low-rank DC as a plugin for CL tasks, our DC plugin approach effectively mitigates the problem of catastrophic forgetting of old tasks while protecting the data privacy.

## 2 Related Works

In this section, we summarize the most related work to this work as three-fold, including coreset sample selection and dataset distillation; continual learning; and low-rank manifolds.

**Coreset Selection and Data Condensation.** Coreset selection  and dataset condensation  are two methods to eliminate data redundancy, which help to improve the model's training efficiency and reduce the cost of data management. Coreset selection has been widely studied in active learning  and continual learning , which tries to identify the most informative training samples [62; 59; 10; 44; 3]. Unfortunately, these methods fail when the task-condition information is evenly distributed among the original samples . Empirical studies [56; 66] show that the benefit of existing coreset selection methods is marginal compared with random selection across multiple benchmarks. In recent years, dataset condensation  has been proposed to distill a large real dataset onto a small synthetic dataset and achieve better performance than coreset selection. DD  formulates the dataset distillation as a bi-level optimization. The inner loop utilizes the condensed dataset to train a network, while the outer loop minimizes the prediction error of the trained network on the original dataset. It updates the synthesized dataset pixel by pixel using Back-Propagation Through Time  to compute the meta gradient. KRR  transforms the distillation problem into a kernel ridge regression problem, simplifying the expensive nested optimization in DD  to a first-order optimization. Unlike the aforementioned works that optimize based on accuracy value, recent works believe that the effectiveness of models trained on the larger dataset and the compact dataset can be reflected in their corresponding parameter states or gradient states . Therefore, they choose to optimize more fine-grained surrogate objectives [66; 64; 30; 8; 65]. Notably, DC  and DSA  minimize the gradient matching loss between the large dataset and the synthetic dataset at each training step, MTT  matches the training trajectory of parameters, LCMat  matches loss curvature, CAFE  aligns layer-wise features, and DM  generates synthetic samples that resemble the distribution of real samples in the feature space. There has recently been a small amount of work on parameter-efficient dataset distillation. For example, IDC  and IDM  take advantage of the local similarity characteristics of images to partition and expand condensed data to generate more images at the same memory cost. HaBa  and RememberThePast  utilize bases to produce images or share bases among all classes respectively. However, these methods ignore that natural images have low intrinsic dimensionality , which leads to inefficient condensation.

**Continual Learning.** Continual learning (CL) aims to enable a neural network model to learn new tasks continuously without forgetting the old tasks [68; 58]. Inspired by the working mechanism of the human brain, mainstream memory-based CL methods consolidate previously learned knowledge by replaying old data, thereby avoiding catastrophic forgetting. Due to strict memory and privacy constraints, usually only a small portion of old task data can be kept, many above-mentioned coreset selection methods are used to select informative samples for storage. For example, some heuristic method is used to select the most representative samples from each class  (e.g., iCaRL ) or the sample closest to the decision boundary (e.g., Rwalk , MIR , Shim ). However, the number of new task's samples and stored old samples in memory is often highly unbalanced, leading the model to be biased towards learning new tasks with more data. Recent works [7; 41] addressing this class imbalance in memory-based CL has achieved impressive and satisfactory performance: End-to-End Incremental Learning  samples the data after training on the new classes and combines it with the data in memory to create balanced data for fine-tuning. GDumb  greedily stores samples in memory and trains the model from scratch on new tasks using only the samples in memory.

**Low-rank Manifolds.** The low-rank structure is prevalent in machine learning, such as computer vision [40; 18; 32], natural language processing [20; 1], recommendation systems [25; 45], etc. For example, MLE [31; 40] found that natural image datasets containing thousands of pixels can actually be described with fewer variables. LoRA  can fine-tune the GPT-3 175B  with comparable accuracy to full-parameter fine-tuning while reducing the number of trainable parameters by a factor of 10,000 and GPU memory requirements by a factor of three. In recommender systems, the original highly sparse rating/interaction matrix is usually decomposed into low-rank user embedding matrices and item embedding matrices  to capture user interests and item features. However, to the best of our knowledge, no research has explored the exploitation of low-rank structure to improve the condensation efficiency for the dataset distillation task, and this work is the first to fill this gap.

## 3 Low-Rank Data Condensation Plugin

In this section, we first define the problem of DC in Sec. 3.1, then introduce our proposed low-rank plugin for DC in Sec. 3.2, and then describe how to integrate our plugin with existing DC methods in Sec. 3.3 and further apply it to continual learning in Sec. 3.4.

### Problem Definition

The goal of dataset condensation (DC) is to distills a large target dataset \(=\{_{i},y_{i}\}_{i=1}^{N_{}}\) containing \(N_{}\) training image \(_{i}^{d}\) and its label \(y_{i}\{1,2,,|Y|\}\) into a small dataset \(=\{_{i},y_{i}\}_{i=1}^{N_{}}\) with \(||\) synthetic image \(_{i}^{d}\), where \(N_{} N_{}\) (\(2-3\) orders of magnitude), \(|Y|\) represents the number of classes, and \(^{d}\) defines a \(d\)-dimensional space. We expect a network \(_{^{}}\) trained on the small dataset \(\) to have similar performance to a network \(_{^{}}\) trained on the large training set \(\) on the unseen test dataset, that is:

\[_{_{i} P_{}} [(_{^{}}(_{i}),y)] &\ _{_{i} P_{}}[(_{^{ }}(_{i}),y)],\\ ^{}=*{arg\,min}_{ ^{}}^{}(^{})=& *{arg\,min}_{^{}}}}_{(_{i},y)}(_{^{ }}(_{i}),y),\\ ^{}=*{arg\,min}_{^{}}^{}(^{})=&*{arg\,min}_{ ^{}}}}_{(_{i},y) }(_{^{}}(_{i}),y), \] (1)

where \(P_{}\) represents the real distribution of the test dataset, \(_{i}\) represents the input image, \(y\) represents the ground truth, and \(()\) represents a loss function such as cross-entropy loss.

To achieve the goal stated in Eq. 1, existing DC methods [56; 66; 64; 8; 65] first initialize the dataset \(^{N_{} D W}\) as a set of learnable parameters in high-dimensional pixel space. Here, \(N_{}\) denotes the number of synthetic images, and \(C\), \(H\), and \(W\) represents the number of channels, the image's height, and the image's width, respectively. Then, the dataset \(\) is updated pixel-by-pixel based on accuracy value matching [56; 36] or surrogate objective matching [66; 8; 65] of the condensed dataset \(\) and the large dataset \(\). For example, in the first dataset distillation work DD , dataset \(\) is treated as a hyperparameter in a bi-level optimization problem as follows:

\[^{*}=*{arg\,min}_{}^{} (_{^{}}),\ ^{}= *{arg\,min}_{}^{}(_{}),\] (2)

where the inner loop trains a randomly initialized network \(\) (parameters denoted as \(\)) on the synthetic dataset \(\) until convergence, and the outer loop uses the large target dataset \(\) as a validation set to optimize \(\). The small dataset \(\) is updated by solving the meta-gradient  of the bi-level optimization in Eq. 2, allowing the trained model \(\) on the dataset \(\) to perform well on the real dataset \(\). Instead of optimizing directly based on the accuracy value of the distilled data \(\), SOTA DC methods are based on surrogate objectives to make the model trained on \(\) and \(\) approximate each other in the parameter space , i.e., \(^{}^{}\), or in the gradient space [66; 66], i.e., \(_{}^{}()_{ }^{}()\), or in the feature distribution space , i.e., \(_{}(_{i})_{}(_{i})\). However, these methods all focus on pixel-by-pixel optimizations of \(\), ignoring that natural images are locally connected  and usually have low intrinsic dimensionality [32; 40]. In the appendix, we perform principal component analysis on real images and images obtained by traditional dataset distillation methods (such as DSA  and DM ) and show that they have low-rank properties. Hence, optimizing \(\) in a high-dimensional pixel space is inefficient.

### Our Low-Rank Data Condensation Plugin

In this work, we introduce a low-rank dataset condensation plugin that distills knowledge from a large dataset \(\) to a small synthetic dataset \(\) in a low-rank manifold. Specifically, instead of directly initializing \(\) as a variable with shape \(N_{} D H W\), we conduct a low-rank decomposition of the content \(_{i,ch}^{H W}\) in the channel \(ch\{1,,D\}\) of an image \(_{i}\) and employ two variables, \(_{i,ch}^{H r}\) and \(_{i,ch}^{r W}\), to reconstruct a high-dimensional image by \(_{i,ch}_{i,ch}_{i,ch}\)where the rank \(r\{H,W\}\). Therefore, the goal of data condensation in the low-rank manifold is to optimize \(^{N_{S} D H}\) and \(^{N_{S} D r W}\) such that the network \(_{^{(,)}}\), trained on the small reconstructed data \((,)\), achieves similar performance to the network \(_{^{}}\) trained on the high-dimensional large dataset \(\). Therefore, the DC in the low-rank manifold is formalized as:

\[_{_{i} P_{}} [(_{^{}(,)},y) ]&\ _{_{i} P_{}}[(_{^{ }(,)}(_{i}),y)],\\ \ ^{}=*{arg\,min}_{ ^{}}\ ^{}(^{})&= *{arg\,min}_{^{}}}} _{(_{i},y)}(_{^{}} (_{i}),y),\\ ^{(,)}=*{arg\, min}_{^{(,)}}^{(, )}(^{(,)})&= *{arg\,min}_{^{(,)}}}}_{(,_{i},y)(, )}(_{^{(,)}}( _{i}_{i}),y),\] (3)

where \((,)\) is an operation that reconstructs \(_{i}\) channel-wise from low-rank matrices \(\) and \(\), i.e., \(_{i}=_{i}_{i}=[_{i,1}_ {i,1}||_{i,D}_{i,D}]^{D H W}\), where \([|]\) means channel-wise stacked the image representation.

**Discussion**. Our method effectively takes advantage of the low intrinsic dimension of natural images, and has the following advantages: (i) The proposed low-rank DC plugin significantly improves condensing efficiency (i.e., fewer training parameters) and reduces the cost of synthetic dataset storage. This is particularly beneficial when the image height (\(H\)) and width (\(W\)) are very large. Due to the low intrinsic dimension of the image, the value of \(r\) can be very small, such as \(2\). (ii) Under the same memory consumption or the number of learnable parameters as traditional DC methods, our plugin can reconstruct a larger number of synthetic images, preserving sufficient information for the large training dataset \(\). (iii) Our plugin is orthogonal to existing dataset distillation work and can be further combined with them.

### Incorporating Low-rank DC Plugin to SOTA Methods

Our proposed low-rank manifolds DC plugin can be easily incorporated into existing DC solutions [56; 66; 8; 64; 65]. Without loss of generality, we integrate it into gradient matching-based DC  and distribution matching-based DM , and define the two new methods as **Low**-rank **DC** (LoDC) and **Low**-rank **DM** (LoDM), respectively. We verified the effectiveness of the low-rank plugin in Sec. 4.1. Additionally, in the appendix, we combine our low-rank DC plugin with more advanced dataset distillation methods, including MTT , IDC , HaBa , and RememberThePast .

**Low-rank DC (LoDC)**. To achieve the goal stated in Eq. 3, we match the gradients of the large dataset \(\) and the small dataset \((,)\) within a low-rank manifold. Specifically, we use the synthetic dataset \((,)\) to train a deep neural network \(_{}\) (the initial parameter is \(_{0}\)). After optimizing the network, we aim to minimize the distance between the gradient \(_{}^{}(_{t})\) of the loss \(^{}\) w.r.t \(_{t}\) on the large dataset \(\) and the gradient \(_{}^{(,)}(_{t})\) of the loss \(^{(,)}\) w.r.t \(_{t}\) on the small dataset \((,)\) at step \(t\) as DC . That is, we solve the following optimization problem:

\[_{,}\,_{_{0} P_{ _{0}}}[_{t=1}^{T_{in}}d(_{}^{ }(_{t}|),_{}^{ (,)}(_{t}|(,)))],\] (4)

where \(d(,)\) is a distance function, \(T_{in}\) is the number of training iterations. \(\) and \(\) are updated using gradients obtained by backpropagation in Eq. 4, and \(\) is trained using the whole small dataset \((,)\) by gradient descent optimization. We provide the pseudocode of our LoDC in Algorithm 1. Additionally, LoDC can be easily extended to DSA  methods that incorporate differentiable data augmentations into gradient matching .

**Low-rank DM (LoDM)**. To achieve the goal of Eq. 3, our LoDM requires the distribution of the small dataset \((,)\) to accurately approximate the distribution of the real training large dataset \(\) as DM . Specifically, we first transform each input image \(_{i}_{i}^{(C H W)}\) into a different space through a family of parametric functions \(_{}\): \(^{(C H W)}^{d^{}}\). We then use the maximum mean difference (MMD)  to estimate the distance between the real and compact data distributions: \(_{\|_{}\|_{} 1}([_{ }()]-[_{}((,))])\), where \(\) is reproducing kernel Hilbert space. As the real data distribution is unavailable, we utilize the empirical estimate of MMD as DM , that is, LoDM to solve the following optimization problem:

\[_{,}\,\,_{_{0} P_{ _{0}}}[d(_{i=1}^{N}_{ _{0}}(_{i}),}}_{i=1}^{ N_{}}_{_{0}}(_{i}_{i} ))],\] (5)

where \(N_{}\) and \(N_{}\) represent the sample sizes of \(\) and \((,)\) respectively. The low-rank matrices \(\) and \(\) are updated by performing a gradient descent on Eq. 5. DM  indicates that network \(_{_{0}}\) can perform well when a family of random initializations is adopted. Therefore, \(_{0}\) does not need to be updated but is randomly sampled in each training iteration. We have adopted this default setting in this paper. Algorithm 2 provides the optimization process of our LoDM.

``` Input: A large training dataset \(\)
1 Required: Randomly initialized \(,\) of rank \(r\) for \(|Y|\) classes, the probability distribution of randomly initialized weights \(P_{_{0}}\), neural network \(_{}\), number of iterations \(T_{ou}\), learning rate \(_{dc}\). foriteration\(k=1,,T_{ou}\)do
2 Initialize \(_{0} P_{_{0}}\)
3 Initialize \(_{s}^{DM}=0\)
4forclass\(c=1,,|Y|\)do
5 Sample a minibatch real pair \(_{c}^{}\) and a minibatch synthetic pair \(_{c}^{(,)}(, )\)
6 Compute \(_{c}^{DM}(_{c}^{},_{c}^{( ,)})=\)
7\(d(}_{(_{i},y)_{c}^{}}_{_{0}}(_{i}),}} _{(_{i}_{i},y)_{c}^{(, )}}_{_{0}}(_{i}_{i}))\)
8 Compute \(_{s}^{DM}=_{s}^{DM}+_{s}^{DM}(_{c}^ {},_{c}^{(,)})\)
9 Update \(-_{dc}_{}_{s}^{DM}\) and \(-_{dc}_{}_{s}^ {DM}\) ```

**Output:** A small dataset \((,)\)

**Algorithm 2**LoDM: Low-rank Dataset Condensation with Distribution Matching 

### Application to Continual Learning

The goal of class continual learning (CL) is to use a model to learn from a continuously arriving sequence of new classes while retaining knowledge from previous classes [68; 58]. Mainstream memory-based CL methods typically involve carefully selecting a small number of old samples from a large training dataset and storing them in a limited memory buffer \(\) to mitigate catastrophic forgetting [59; 7; 41; 57]. The degree of forgetting of old classes is directly affected by the amount of data information in the samples stored in the limited memory buffer \(\). Our dataset condensation plugin can compress a raw large dataset into a smaller dataset \((,)\) by Alg. 1 or Alg. 2 to story in \(\), thereby to preserve more information about older classes under strict memory constraints. In this work, similar to DM , we establish a baseline using the simple yet effective GDumb  approach in CL. GDumb first stores training samples in memory in a greedy manner while ensuring balanced samples per class. During testing, the model is trained from scratch exclusively using samples from memory \(\). More specifically, we compare the effects of storing randomly selected sample sets . Herding sample sets [59; 44; 7], traditional DC condensed dataset (i.e., DC /DSA , DM ), and our low-rank manifold condensed dataset (i.e., LoDC or LoDM in Sec. 3.3) in memory \(\) for CL in Sec. 4.2.

## 4 Experiment

In this section, we conduct experiments to verify the effectiveness of the proposed low-rank DC plugin. Due to space constraints, some experiment results are included in the **Appendix**.

### Data Condensation for Deep Learning

**Datasets**. We evaluate our low-rank DC plugin on four benchmark datasets as DM , including MNIST , CIFAR10 , CIFAR100 , and TinyImageNet . _MNIST_ contains 60,000 grayscale images of size \(28\!\!28\), totaling 10 classes. _CIFAR10_ and _CIFAR100_ contain 50,000 RGB images of size \(32\!\!32\). The former has 10 classes, and the latter has 100 classes. _TinyImageNet_ contains 100,000 images resized to \(64\!\!64\), with a total of 200 classes.

**Baselines**. We compare two kinds of methods to reduce the amount of data: coreset selection (i.e., Random, Herding [59; 4], Forgetting ) and synthetic dataset distillation (i.e., DD , LD , DC , DSA , DM ). Specifically, _Random_ sampling selects images randomly, _Herding_ heuristically selects the sample closest to the center of the class, and _Forgetting_ selects the sample that is most likely to be forgotten during model training. _DD_ is one of the earliest works on dataset distillation, which updates the synthetic dataset by solving a bi-level optimization problem. _LD_ performs label distillation rather than image distillation. _DC_ performs gradient matching between large training dataset and condensed dataset, and _DSA_ further considers differentiable siamese augmentation strategies based on DC. _DM_ condenses datasets through distribution matching. We further combine the proposed low-rank plugin into DC, DSA and DM to obtain _LoDC_, _LoDSA_ and _LoDM_, respectively. Comparisons with other dataset distillation methods (e.g., MTT , IDC , HaBa , and RememberThePast ) are provided in the appendix.

**Experimental Details**. In each experiment, we first choose the coreset (for Random, Herding, Forgetting) or learn a synthetic dataset (for DD, LD, DC, DSA, DM, LoDC, LoDSA, LoDM) and then employ it to train 20 randomly initialized networks (default ConvNet  architecture). By default, we set the rank \(r\) of our low-rank plugin to 2 for MNIST, CIFAR10, CIFAR100, and 4 for TinyImageNet. In a few cases, the rank \(r\) will be searched in \(\{1,2,4,8\}\). Other hyperparameters are the same as baselines [66; 64; 65]. We repeat the experiment multiple times for each method and

   &  &  &  &  \\  & & Random & Herding & Forgetting & DD & LD & DC & DSA & DM & **LoDM(Ours)** \\   & 1 & 0.017 & 64.9\(\)3.5 & 89.2\(\)1.6 & 35.5\(\)5.6 & - & 60.9\(\)3.2 & **91.7\(\)0.5** & 88.7\(\)0.6 & 89.7\(\)0.6 & 91.2\(\)0.4 \\  & 10 & 0.17 & 95.1\(\)0.9 & 93.7\(\)0.3 & 68.1\(\)3.3 & 79.5\(\)8.1 & 87.3\(\)0.7 & 97.4\(\)0.2 & 97.1\(\)0.1 & 96.5\(\)0.2 & **97.7\(\)0.1** \\  & 50 & 0.83 & 97.9\(\)0.2 & 94.8\(\)0.2 & 88.2\(\)1.2 & - & 93.3\(\)0.3 & 98.8\(\)0.2 & **99.2\(\)0.1** & 97.5\(\)0.5 & 98.2\(\)0.1 \\   & 1 & 0.02 & 14.4\(\)0.2 & 21.5\(\)1.2 & 13.5\(\)1.2 & - & 25.7\(\)0.7 & 28.3\(\)0.5 & 28.8\(\)0.7 & 26.0\(\)0.8 & **43.8\(\)0.8** \\  & 10 & 0.2 & 26.0\(\)1.2 & 31.6\(\)0.7 & 23.3\(\)0.1 & - & 25.0\(\)0.3 & 44.9\(\)0.5 & 51.1\(\)0.5 & **48.9\(\)0.6** & **59.8\(\)0.4** \\  & 50 & 1 & 43.4\(\)0.1 & 40.4\(\)0.6 & 23.3\(\)1.1 & - & 42.5\(\)0.4 & 53.9\(\)0.5 & 60.6\(\)0.5 & 63.0\(\)0.4 & **64.6\(\)0.1** \\   & 1 & 0.2 & 4.2\(\)0.3 & 8.4\(\)0.3 & 4.5\(\)0.2 & - & 11.5\(\)0.4 & 12.8\(\)0.3 & 13.9\(\)0.3 & 11.4\(\)0.3 & **25.6\(\)0.5** \\  & 10 & 2 & 14.6\(\)0.5 & 17.3\(\)0.3 & 15.1\(\)0.3 & - & - & 25.2\(\)0.3 & 32.3\(\)0.3 & 29.7\(\)0.3 & **37.5\(\)0.8** \\   & 1 & 0.2 & 1.4\(\)0.1 & 2.8\(\)0.2 & 1.6\(\)0.1 & - & - & 4.61\(\)0.2 & 4.79\(\)0.2 & 3.9\(\)0.2 & **10.3\(\)0.2** \\  & 10 & 2 & 5.0\(\)0.2 & 6.3\(\)0.2 & 5.1\(\)0.2 & - & - & 11.6\(\)0.3 & 14.7\(\)0.2 & 12.9\(\)0.4 & **18.3\(\)0.3** \\  

Table 1: Comparison with coreset selection methods and dataset condensation methods.

   &  &  &  &  &  &  &  &  &  &  &  &  &  \\  & & 1 & 91.7\(\)0.5 & - & **93.0\(\)0.3** & 88.7\(\)0.6 & **90.6\(\)0.6** & 89.7\(\)0.6 & 87.0\(\)0.7 & **91.2\(\)0.4** \\  & 10 & 97.4\(\)0.2 & 96.0\(\)0.2 & **97.6\(\)0.3** & 97.1\(\)0.1 & 95.3\(\)0.2 & **97.7\(\)0.1** & 96.5\(\)0.2 & 92.0\(\)0.6 & **97.7\(\)0.1** \\   & 1 & 28.3\(\)0.5 & 28.2\(\)0.5 & **35.2\(\)0.5** & 28.8\(\)0.7 & 28.3\(\)0.6 & **41.1\(\)0.2** & 26.0\(\)0.8 & 24.8\(\)0.3 & **43.8\(\)0.8** \\  & 10 & 44.9\(\)0.5 & 42.8\(\)0.4 & **50.8\(\)0.3**report the average test accuracy. Additionally, in Sec. 4.1.2, we also tested the cross-architecture effectiveness of the synthetic dataset on five standard deep network architectures: ConvNet , LeNet , AlexNet , VGG11 , and ResNet18 .

#### 4.1.1 Compared to SOTA DC Baselines

We compare the performance of our LoDC, LoDSA, LoDM with other baseline methods under different sample sizes (1/10/50 image(s) per class) in Tab. 1 and Tab. 2. We have the following observations from Tab. 1: (i) When the sample size is small (e.g., 1 image per class), both Random and heuristic Forgetting sample selection perform poorly, significantly lower than Herding's method, because the latter samples best represent the class centers. As the sample size of each class increases, Herding's advantage becomes less obvious. (ii) The dataset condensation methods are significantly better than the coreset methods. For example, in CIFAR10 (10 images per class), Random and Herding only achieve 26.0% and 31.6% accuracy, while DD achieves 36.8% accuracy, DC, DM, and DSA achieve 44.9%, 48.9%, and 51.1%, respectively. (iii) Our method condenses dataset in a low-rank manifold, effectively reducing memory cost (or the number of parameters to be optimized per image). Therefore, by utilizing the same memory, our low-rank LoDM can represent a more significant number of images, which is significantly better than other SOTA dataset compression methods, especially when the sample size of each class is small. For example, when using one image per class, our LoDM achieved accuracies of 43.8%, 25.6%, and 10.3% on CIFAR10, CIFAR100, and TinyImageNet, respectively, while the best baseline achieved only 28.8%, 13.9%, and 3.9%.

In Tab. 2, we compare the performance of our methods (LoDC, LoDSA, LoDM) in the low-rank manifold with traditional dataset distillation methods (DC, DSA, DM) using the same number of images (SI) or the same memory consumption (SM). It can be observed that: (i) Our method (rank \(r=2\)) reduces storage consumption by \(7\), \(8\), and \(8\) under MNIST, CIFAR10, CIFAR100 datasets, respectively, when the number of images is the same. However, the performance is still relatively close to the traditional DC method. We further explain the correspondence between rank size and memory cost in the appendix. (ii) Under the same memory consumption, our method can condense the large dataset into a smaller dataset with more samples, thereby preserving more information from the original large datasets. Therefore, compared to traditional condensing methods, our method exhibits a significant improvement. For example, on the CIFAR10 (10 Img/CIs) dataset, LoDSA and LoDM have shown an improvement of 8.9% and 13.8% compared to DSA and DM, respectively. Furthermore, as shown in Fig. 2(a), our LoDM method can achieve a higher final performance than DM after only 2,000 iterations, compared to 20,000 iterations.

#### 4.1.2 Ablation Study

**Cross-architecture Transferability Analysis**. We verify that the condensed dataset learned in the low-rank manifold still has good cross-architecture transfer ability. We learn condensed dataset on a particular architecture (i.e., AlexNet/ConvNet), and then use the learned small dataset to train five different architectures (use Batch Normalization  as DM) from scratch, and finally verify on the test set of CIFAR10 dataset. As shown in Tab. 3, we observe that in each method (DSA, DM or

Figure 1: Distribution of real (all images) and synthetic images (50 images per class) on MNIST dataset: (a) Real, (b) DC, (c) Our LoDC(\(r=8\)), (d) DM, (e) Our LoDM(\(r=2\)), and (f) Our LoDM(\(r=8\)).

  Method & LearnOn \(\) TestOn & ConvNet & LeNet & AlexNet & VGG11 & ResNet18 \\   & AlexNet & 30.4\(\)0.7 & 24.2\(\)0.4 & 28.3\(\)0.4 & 27.2\(\)1.0 & 27.8\(\)1.1 \\  & ConvNet & 31.4\(\)1.1 & 21.7\(\)1.6 & 25.9\(\)0.8 & 27.6\(\)0.8 & 27.6\(\)1.4 \\   & AlexNet & 41.4\(\)0.8 & 31.4\(\)0.2 & 37.5\(\)0.9 & 36.8\(\)0.5 & 34.9\(\)1.1 \\  & ConvNet & 42.2\(\)0.5 & 33.4\(\)0.6 & 38.8\(\)1.3 & 36.2\(\)1.0 & 34.6\(\)0.5 \\   & AlexNet & 56.2\(\)0.3 & 32.9\(\)0.9 & 49.9\(\)0.5 & 51.0\(\)0.6 & 50.9\(\)0.5 \\  & ConvNet & 56.4\(\)0.3 & 45.5\(\)0.6 & 53.4\(\)0.6 & 50.7\(\)0.6 & 50.6\(\)0.7 \\  

Table 3: Cross-architecture testing performance on CIFAR10 (10 images per class). _LearOn_ means condensing the dataset on this architecture, and _TestOn_ means using the condensed dataset to train a new architecture.

LoDM), the condensed data on AlexNet and ConvNet perform very similarly when training different architectures, so they have cross-architecture versatility. Furthermore, our LoDM method achieves the best performance in almost all architectures, benefiting from its ability to hold more images in low-rank space at the same memory cost.

**Data Distribution Analysis**. We verify that the condensed data learned in the low-rank manifold can also capture the characteristics of the original data distribution. We first train a ConvNet model using all the original data as a feature extractor. Then, we input the original images, DC, DM, LoDC, and LoDM learned data (50 images per class) into the network to extract features and perform dimensionality reduction through T-SNE  to visualize. As shown in Fig. 1, we observe that both DC and LoDC cannot capture the distribution of raw data well, since they aim to perform dataset distillation with the goal of gradient matching. DM considers distribution matching as the distillation goal, which captures the data distribution effectively. LoDM inherits the properties of DM, particularly when the rank increases, e.g., in Fig. 1(d) and Fig. 1(e), the rank is 2 and 8, respectively.

**Hyper-parameter Analysis**. We analyze the effect of two hyperparameters, rank size \(r\{1,2,4,8\}\) and learning rate \(_{dc}\{0.1,0.5,1,2,5\}\), on CIFAR10 based on the LoDM method. Fig. 2(b) shows that when using the same number of images as DM, LoDM(SI) gradually achieves similar accuracy to DM in the high-dimensional pixel space, as the rank size increases. When the rank is 8, the performance is nearly identical to that of DM. We found that LoDM(SM) consistently outperforms DM when using the same memory, and the rank equal to \(2\) is a good choice based on empirical evidence. We also provide image visualizations generated by different ranks in the appendix. Fig. 2(c) shows that updating the generated data with a relatively large learning rate, such as 1 or 5, yields better results.

**Visualization of Images**. We visualize in Fig. 3 the condensed images generated by our LoDM method (rank \(r=2\)) for the CIFAR10 and MNIST datasets, ten images per class. We observe that compared to the images condensed by DM in high-dimensional pixel space, the images we recover under the low-dimensional manifold will be sharper, but still capture the main features of the class, especially on the MNIST dataset. This further verifies that images are locally connected and have lower intrinsic dimensions, so dataset condensation in traditional pixel space may be inefficient.

### Data Condensation for Continual Learning

In this section, we apply the low-rank dataset distillation plugin to continual learning tasks. We perform class-incremental learning with strict memory constraints, specifically using 10 images per class for the CIFAR10 dataset and 20 images per class for the CIFAR100 dataset. Based on Sec. 3.4,

Figure 3: Visualization of (a) real and (b) DM synthetic, and (c) LoDM(Ours) synthetic images on CIFAR10 dataset (10 images per class) and (d) LoDM(Ours) on MNIST dataset (10 images per class).

Figure 2: Ablation study on CIFAR10 (10 images per class): (a) Accuracy changes during DM, LoDM(SI) and LoDM(SM) iterations, (b) rank size of synthetic datasets, and (c) learning rate of synthetic datasets (\(r=2\)).

we combine the coreset method (Random, Herding) and various dataset distillation methods (DSA, DM, and our LoDM) into a simple and effective CL method GDumb . We conduct experiments on two benchmark datasets, CIFAR10 and CIFAR100, where the CIFAR10 dataset is divided into 5 tasks, and the CIFAR100 dataset is divided into 5 tasks and 10 tasks, respectively. Based on Fig. 4, we observe that in the three subfigures (a-c), GDumb+LoDM achieves the best results. For example, on CIFAR100 with 5 tasks, the final accuracies of GDumb+Random, GDumb+Herding, GDumb+DSA, GDumb+DM, and GDumb+LoDM are \(27.9\%\), \(27.0\%\), \(30.0\%\), \(33.81\%\), and \(37.92\%\), respectively. This suggests that our condensed data in a low-rank manifold is also meaningful for continual learning with limited memory.

## 5 Conclusion and Future Works

In this work, inspired by natural images that are locally connected and have low intrinsic dimensions, we propose a simple yet effective plugin that condenses a large dataset into a smaller dataset in a low-dimensional manifold. We apply this plugin to the existing dataset condensation methods and observe significant performance improvements while maintaining the same memory cost. Additionally, the analysis revealed that the dataset condensed in the low-dimensional manifold exhibits similar characteristics to the traditional high-dimensional pixel space dataset condensing method, such as matching the distribution of the large dataset and cross-architecture transferability. Furthermore, the plugin effectively addresses the issue of catastrophic forgetting in continual learning tasks. Our work has two directions for further improvement in the future: (i) Rank is a manually tuned hyperparameter in this paper. Therefore, how to adaptively assign the best rank to each dataset in the future is a feasible direction. (ii) We plan to apply the proposed dataset distillation plugin to more downstream tasks in the future, such as network architecture search, federated learning, and meta-learning.