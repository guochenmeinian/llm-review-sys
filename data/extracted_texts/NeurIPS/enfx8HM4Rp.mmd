# Train Once and Explain Everywhere: Pre-training Interpretable Graph Neural Networks

Jun Yin

Central South University

yinjun2000@csu.edu.cn

&Chaozhuo Li1

Microsoft Research Asia

cli@microsoft.com

&Hao Yan

Central South University

CSUhy1999@csu.edu.cn

&Jianxun Lian

Microsoft Research Asia

jianxun.lian@microsoft.com

&Senzhang Wang

Central South University

szwang@csu.edu.cn

Equal contribution.Corresponding author.

###### Abstract

Intrinsic interpretable graph neural networks aim to provide transparent predictions by identifying the influential fraction of the input graph that guides the model prediction, i.e., the explanatory subgraph. However, current interpretable GNNs mostly are dataset-specific and hard to generalize to different graphs. A more generalizable GNN interpretation model which can effectively distill the universal structural patterns of different graphs is until-now unexplored. Motivated by the great success of recent pre-training techniques, we for the first time propose the **P**re-training **I**nterpretable **G**raph **N**eural **N**etwork (\(\)**-GNN3**) to distill the universal interpretability of GNNs by pre-training over synthetic graphs with ground-truth explanations. Specifically, we introduce a structural pattern learning module to extract diverse universal structure patterns and integrate them together to comprehensively represent the graphs of different types. Next, a hypergraph refining module is proposed to identify the explanatory subgraph by incorporating the universal structure patterns with local edge interactions. Finally, the task-specific predictor is cascaded with the pre-trained \(\)-GNN model and fine-tuned over downstream tasks. Extensive experiments demonstrate that \(\)-GNN significantly surpasses the leading interpretable GNN baselines with up to 9.98% interpretation improvement and 16.06% classification accuracy improvement. Meanwhile, \(\)-GNN pre-trained on graph classification task also achieves the top-tier interpretation performance on node classification task, which further verifies its promising generalization performance among different downstream tasks.

## 1 Introduction

Although graph neural networks (GNNs) [1; 2; 3; 4; 5] have achieved remarkable success in various applications [6; 7; 8; 9], their black-box nature prevents humans from understanding the inner decision-making mechanism [10; 11]. This issue calls for the development of intrinsic interpretable GNNs [12; 13; 14], which can reveal the mystery of _"Which fraction of the input graph is the most vital and leads to the model prediction?"_ Intrinsic interpretable GNNs aim to identify an influential subgraph of the input graph, i.e., the explanation, and make final predictions under the guidance of the explanatory subgraph [12; 13; 14]. Constructing interpretable GNNs makes it possible to investigatethe decision-making mechanism and justify model predictions, which is critically important to develop trustworthy artificial intelligence.

However, a major issue of the existing interpretable GNNs [12; 13; 14] is that they are mostly dataset-specific, which means an interpretable GNN model trained on a certain graph dataset (e.g., the semantic networks [15; 16; 17]) usually does not work well on another graph dataset (e.g., the molecular graphs ). It is also difficult for an interpretable GNN trained on a certain task (e.g., graph classification ) to generalize to another task (e.g., node classification ). Compared to the text or image data [19; 20], graphs with non-Euclidean structure are often associated with substantial node features of various domains, making the pre-training method in NLP and CV hard to be directly applied . Recently graph representation learning based pre-training methods have been investigated [22; 21; 23; 24], however, they target at downstream prediction tasks but ignore the interpretability of the GNNs. It is widely acknowledged that the topological structure of various graphs generally follows some universal structural patterns or properties which are transferable , such as the scale-free property , the motif distribution , and the core-periphery structure . Therefore, in this paper we argue that the intrinsic GNN interpretation contains some universal structural patterns, which are independent of the downstream tasks and generalizable to different types of graphs. Motivated by the great success of pre-training technique [19; 20; 22; 21], we for the first time study: _whether we can and how to construct a pre-training interpretable GNN that is general enough to work well on different types of graphs and downstream tasks?_

The challenges of designing a pre-training interpretable GNN are three-fold. First, labeling ground-truth explanation in the real-world graphs is extremely resource- and time-consuming . The lacking of ground-truth explanation makes it hard to distill the universal interpretability in the pre-training phase. Second, multiple structural patterns usually co-exist in one graph dataset, such as the scale-free pattern and the motif distribution pattern in chemical molecule graphs . How to extract and integrate multiple structural patterns for a more comprehensive and general graph representation during pre-training is also challenging. Third, the local structural interactions such as the neighbor edges interaction  should be considered when identifying explanations. However, due to the structural diversity of different local neighborhoods, it is non-trivial to incorporate the global structural patterns with the local structural interactions.

In this papaer, we propose a **Pre**-training **I**nterpretable **G**raph **N**eural **N**etwork model (\(\)**-GNN** for short), which is first pre-trained over a large synthetic graph dataset with ground-truth explanations and then fine-tuned on different downstream datasets and tasks. Specifically, we first construct a synthetic graph dataset named PT-Motifs that contains various structural patterns and ground truth explanations, over which the \(\)-GNN model is pre-trained. Considering the co-existence of multiple structural patterns, a structural pattern learning module is introduced to extract and integrate multiple structural patterns to make them generalizable to diverse graph datasets. To better identify the explanatory subgraph, a hypergraph refining module is also proposed to capture the local structural interaction and incorporate it with the universal structural patterns. It is also proved the structural representation ability of \(\)-GNN can approach the theoretical upper bound through the hypergraph refining process. Our main contributions are summarized as follows.

* We for the first time propose a pre-training interpretable GNN model \(\)-GNN, which can be generalized to different graph datasets and diverse downstream tasks.
* To extract the universal interpretability of \(\)-GNN, we construct a synthetic graph classification dataset PT-Motifs with ground-truth explanations for pre-training.
* Two innovative modules, i.e., the structural pattern learning module and the hypergraph refining module, are designed and integrated into \(\)-GNN. The former captures and integrates multiple universal structural patterns for generalizable graph representation. The latter incorporates the universal patterns with local structural interactions to identify the explanation.
* Compared with the SOTA baselines, \(\)-GNN achieves up to 9.98% ROC-AUC improvement in interpretation and 16.06% accuracy improvement in prediction. Moreover, \(\)-GNN pre-trained on graph classification dataset is able to achieve comparable performance with the leading baselines on the node classification task.

Background

In this section, we briefly introduce the graph neural networks and the GNN explanation methods4. The key notations are summarized in Appendix A for clarity.

**Graph Neural Networks.** Graph structure data can be denoted as \(G=(,)\) with the node set \(\) and the edge set \(\). The node features are represented as the matrix \(^{|| d}\) and the edge features (if exist) are represented as \(_{E}^{|| d_{E}}\). The topological structure is usually represented as an adjacency matrix \(^{||||}\), where the element \(A_{ij}=1\) indicates the edge \((i,j)\) exists and \(A_{ij}=0\) otherwise. Graph neural networks (GNNs) aim to learn expressive representation on graphs for the downstream tasks [1; 2; 3; 4; 32; 33], such as graph classification , node classification [6; 34], and link prediction [35; 36]. Typically, to learn the representation of node \(v_{i}\), GNNs aggregate the information from its neighborhood \((v_{i})\) and then combine it with \(v_{i}\)'s own features. For example, the operation of the \(k\)-th GCNs layer can be formulated as follows ,

\[_{}=F^{-}} ^{-}_{}_{},\] (1)

where \(_{}\) and \(_{}\) are the input and output of the \(k\)-th layer, \(}=+\) is the adjacent matrix with self-loops, and \(\) is a diagonal matrix whose element \(D_{i,i}\) represents the degree of \(v_{i}\). \(_{}\) is a trainable matrix and \(F()\) is a non-linear activation function.

**GNN Explanation Methods.** GNN explanation methods can be catogrized into the post-hoc explanation methods [10; 11; 37; 38; 39] and the intrinsic interpretable methods [3; 40; 12; 13; 14]. The post-hoc methods target at explaining the black-box models which are fixed or unaccessible. The interpretable methods devote to making transparent prediction from scratch, including not only the predicted label but also the influential subgraph that guides the prediction . In both the post-hoc [10; 11; 37; 39] and the interpretable methods [12; 13; 14], the GNN explainer learns a contribution function \(h\) which maps each feature of the input graph into the contribution score to the predicted label. One insight in GNN explanation methods is that, the edge contribution function is more essential to GNN explanation compared with that of the node [11; 13; 14]. For example, when some nodes are selected, it is non-trivial to identify the explanatory subgraph. On the contrary, when the important edges are selected, the correlated endpoints are naturally selected as well. We can naturally identify the explanatory subgraph or further explore the important subset of node features. _In this work, we follow the previous works_[11; 39; 13; 14; 3; 40]_and focus on the contribution of structure features (i.e., edges)._ Formally, we learn the contribution function \(h\) in terms of each edge in graph \(G=(,)\) as follows,

\[=h(G),\] (2)

where \(^{||}\) and each element in \(\) is the contribution score of the edge to the task label. Next, a selection module \(\) as follows is employed to select the edges of the explanatory subgraph \(g\), such as the top-\(k\) selector [11; 13], the threshold selector , and the probabilistic selector [30; 14],

\[g=(G,).\] (3)

## 3 Methodology

The framework of the proposed \(\)-GNN is shown in Figure 1, which contains an _explainer pre-training phase_ and a _conjoint fine-tuning phase_. In the explainer pre-training phase, we pre-train the \(\)-GNN explainer over the synthetic dataset PT-Motifs with ground-truth explanations, by taking the binary edge classification as the pretext task. Afterwards, the pre-trained \(\)-GNN explainer is incorporated with task-specific predictor to identify explanatory subgraphs and provide transparent predictions on different tasks. During the fine-tuning phase, the explainer and the predictor are conjointly optimized. Next, we introduce the \(\)-GNN model in detail.

### Explainer Pre-training Phase

Due to the lack of ground-truth explanation in real-world graph datasets, we first construct a large synthetic dataset with ground-truth explanation called PT-Motifs to support the explainer pre-training.

Following previous works on generating synthetic graphs [10; 11; 13], each graph \(G\) in PT-Motifs dataset consists of one base subgraph \(G_{b}\) and one explanation subgraph \(G_{e}\) (also known as the motif) and the ground-truth task label \(y\) which is determined by \(G_{e}\) solely [10; 13]. The shapes of the explanatory subgraphs in PT-Motifs include _Diamond, House, Crane, Cycle, and Star_ and the basic shapes are _Clique, Tree, Wheel, Ladder, and the Barabasi-Albert Net_. Each combination of base and motif has the same number of samples in PT-Motifs. As shown in Figure 4(e) of Appendix B.2, the degree distribution of PT-Motifs follows the power law function \(y=10^{6} x^{-1.5}\). See Appendix B.2 for more detailed structural patterns of PT-Motifs. The proposed \(\)-GNN model distills interpretability from the PT-Motifs dataset during the pre-training phase.

**Structural Pattern Learning Module.** To capture the multiple structural patterns, we propose to parallelize multi-thread of basic pattern-learner \(=\{B_{i}|i=1,2,,N\}\). Then, an integrated pattern-learner aggregates them for a more comprehensive and general representation of various graph structural patterns. Specifically, each basic learner \(B_{i}\) identifies a vectorized representation of the structural patterns (such as the degree distribution) and the integrated learner \(\) provides a combination of each individual representation. We formally define the basic pattern-learner as follows.

**Definition 1** (**Basic Pattern-Learner)**.: _Consider a graph \(G\) with \(n\) nodes, whose adjacency matrix is \(^{n n}\). The basic pattern-learner \(B_{i}\) projects the adjacency matrix \(\) into a low-dimensional pattern matrix \(_{i}^{n d},d<n\), as follows,_

\[_{i}=B_{i}().\] (4)

_The basic pattern-learner \(B_{i}\) approaches the low-dimensional pattern matrix \(_{i}\) by maximizing the likelihood of preserving the graph topological structure._

As a widely adopted technique which provides global views of a graph [41; 42], node embedding can serve as a simple yet effective basic pattern-learner. Inspired by the multi-head attention mechanism for improving the expressive power , we parallelize \(N\)-thread basic pattern-learners to achieve the structural pattern tensor \(=[_{1},_{2},,_{N}]^{N v d}\) which contains multiple universal structural patterns. For each basic pattern-learner, the adjacency matrix \(\) is randomly permuted [42; 44]. Afterwards, the integrated learner \(\) defined as follows aggregates the pattern tensor \(\) for a more expressive and generalizable pattern representation \(_{}\).

**Definition 2** (**Integrated Pattern-Learner)**.: _Given the structural pattern tensor \(^{N v d}\), the integrated pattern-learner \(\) moves forward to a convex combination \(_{}^{v d_{}}\) of each

Figure 1: Framework of the \(\)-GNN model. In the explainer pre-training phase, we use the synthetic graphs with ground-truth explanations to pretrain the \(\)-GNN explainer. In the fine-tuning phase towards downstream tasks in real-world graphs, the pre-trained explainer and the task-specific predictor are conjointly fine-tuned for transparent and accurate predictions.

individual pattern matrix \(_{i}\) as_

\[_{}=()=_{i=1}^{N}_{i}_{i},\] (5)

_where \(\) is an integration function to comprise appropriate structural patterns for diverse graphs._

Subsequently, \(_{}\) is fed into the hypergraph refining module for computing the edge contribution score and exploring explanatory subgraph.

**Hypergraph Refining Module.** With the universal structural patterns extracted from the graphs, we next incorporate it with the edge interactions to identify the explanatory subgraph. Specifically, we first learn the edge structural representation \(_{E}\) from the integrated pattern matrix \(_{}\). Taking the edge \(e=(i,j)\) as an example, the corresponding structural representation \(_{E}^{e}\) can be approached from the endpoint representations in \(_{}\) as follows,

\[_{E}^{e}=f^{(2)}(_{}^{i},_{}^{j}),\] (6)

where \(f^{(2)}\) is a 2-variable function and \(_{}^{i},_{}^{j}\) are the representations of nodes \(v_{i},v_{j}\), respectively.

Since the edge representation \(_{E}\) directly determines the edge contribution score in the explanation, it should express as much information as possible. Theoretically, the expressive power of \(_{E}\) is guaranteed by the following Theorem 1.

**Theorem 1**.: _Let \(_{n}\) be the set of all adjacency matrix \(\) with \(n\) nodes. Given a graph \(G=(,)_{n},n 2\), let \(^{*}(S,)\) be a most-expressive structural representation of nodes set \(S\) in \(G\). \(_{n}\), there exists a most-expressive node representation \(^{*}|\) satisfies the relationship as follows,_

\[^{*}(S,)=_{^{*}}[f^{(|S|)}((_{v }^{*})_{v S})|], S,\] (7)

_for an appropriate \(k\)-variable function \(f^{(k)}()\)._

Theorem 1 defines the upper bound of \(_{E}\) representation ability . The upper bound can be approached according to the Theorem 2 as follows.

**Theorem 2**.: _The structural representation of edge \(e=(v_{i},v_{j})\) can be learnt by simply approaching a function \(f^{(2)}\) which satisfies \((e,)=f^{(2)}([(_{v})_{v\{i,j\}}| ])\)._

In the hypergraph refining module, we adopt the integrated pattern matrix \(_{}\) as an approximation of the expectation embedding matrix and a 2-layer MLP to fit the transition function \(f^{(2)}\) during pre-training phase . See Appendix G for the proofs of Theorems 1 and 2.

Based on the edge structural representation \(_{E}\), a straightforward way is to directly estimate the contribution score of each individual edge . However, such procedure ignores the incorporation of universal patterns with local structural interactions, leading to the missing of the dependencies among the edges. The edges in the explanation are supposed to interact with each other , form the coalition, and guide the downstream prediction task better than individuals. We next introduce how to capture the local edge interactions and incorporate them with the universal structural patterns.

Given the graph \(G=(,)\), we define its corresponding hypergraph as \(G_{h}=(_{h},_{h})\). For a \(k\)-degree node \(v\), the corresponding hyper-view is a hyperedge connecting \(k\) hypernodes.

Figure 2: An example of the graph-hypergraph transformation. Considering the 3-degree node \(A\) in graph \(G\), its corresponding hyper-view is the hyperedge \(A\) which connects the hypernodes \(\{1,4,5\}\).

Similarly, the edge \(e\) becomes a hypernode in the corresponding hypergraph. An example of graph-hypergraph transformation is shown in Figure 2. See Appendix H for the detailed algorithm of graph-hypergraph transformation. During the graph-hypergraph transformation, the edge structural representation is naturally converted to the hypernode structural representation. Therefore, we can capture the local structural interaction by conducting hyperedge message passing. In our implementation, a 2-layer hypergraph convolutional network is employed to capture the edge interaction information and further map the edge structural representations to the edge contribution scores \(\). Finally, the hypergraph refining is formally represented as,

\[_{E}^{(1)} =((_{E})),\] (8) \[ =((_{E}^{(1)})),\] (9)

where the \(()\) function is used for a zero-mean activate value and the \(()\) function normalizes the output to a probability value within the range of \(\). The normalized contribution score in \(\) measures the probability of each edge belongs to the explanatory subgraph.

Under the supervision of the ground-truth explanation in PT-Motifs, \(\)-GNN distills the universal interpretability during the pre-training phase. Specifically, for each synthetic graph \(G=(,)\), the edges in \(_{P}\) that belong to the explanation subgraph are assigned with positive labels and the complementary part \(_{N}\) receives a negative label [10; 11]. We denote the ground-truth explanation as \(\{0,1\}^{||}\). \(\)-GNN takes \(G\) as input and outputs the predicted explanation \(^{||}\), which is optimized by the binary cross-entropy loss as follows,

\[L(,)=-_{i=1}^{||}[_{i}_{i} +(1-_{i})(1-_{i})].\] (10)

### Conjoint Fine-tuning Phase

During the conjoint fine-tuning phase, we combine the pre-trained \(\)-GNN explainer with task-specific predictors to identify the explanatory subgraph and make final prediction simultaneously for real-world datasets. Note that the pre-trained \(\)-GNN explainer is orthogonal to the post-positional predictor. That is, we can implement a predictor with arbitrary architecture as long as it can deal with the graph structure data. Even though the pre-training dataset PT-Motifs belongs to graph classification task, the pre-trained \(\)-GNN model can be easily generalized to other tasks, such as model classification, by simply implementing a node classifier.

Following existing works [11; 13; 14], given the input graph \(G\) and the corresponding task label \(y\), we introduce a probabilistic sampler \(\) to comprise the explanatory subgraph \(g\) according to the predicted edge probability \(\) as follows,

\[g=(G,).\] (11)

Going beyond the probabilistic sampling procedure, the post-positional predictor takes the explanatory subgraph \(g\) as input and fits the mapping function to the predicted label \(\), by optimizing a task-specific loss function \(L_{}(,y)\). In addition, we introduce an entropy regularizer [11; 14] in the fine-tuning phase to amplify the gap of each value in \(\) for sparser explanations as follows,

\[L()=-_{i=1}^{||}[_{i}_{i }+(1-_{i})(1-_{i})]+||||_{1}.\] (12)

The overall objective of the fine-tuning phase is to jointly optimize the the task-specific term and the entropy regularizer as follows,

\[L(,,y)=L_{}(,y)+L().\] (13)

## 4 Experiment

In this section, we conduct extensive experiments to evaluate the performance of \(\)-GNN by answering the following two questions.

* **RQ1:** How effective is \(\)-GNN when it is generalized to different graph datasets?
* **RQ2:** How effective is \(\)-GNN when it is generalized to different graph tasks?

### Experimental Settings

In the experiment, we use two popular synthetic datasets [10; 11; 13; 14] and four real-world datasets of graph classification tasks. The details of dataset characteristics and statistics are summarized in Appendix B. A brief introduction to the datasets is as follows.

* **Synthetic Datasets.** BA-2Motifs  and Spurious-Motif  are two widely-used synthetic datasets to evaluate the interpretation performance of the GNN explanation methods.
* **Real-world Datasets.** We use four real-world datasets, the superpixel graph dataset MNIST-75sp , the sentiment analysis dataset Graph-SST2 , and two chemical molecule datasets Mutag  and Ogbg-Molhiv . Note that, in the MNIST-75sp dataset, the subgraph with nonzero pixel values is regarded as the ground-truth explanation ; in the Mutag dataset, -NO\({}_{2}\) and -NH\({}_{2}\) functional groups in mutagen graphs are labelled as the ground-truth explanation . Hence, we use the MNIST-75sp and Mutag datasets for both the interpretation and the prediction evaluations.

We extensively compare \(\)-GNN with the following two types of baselines:

* **Interpretation Baselines.** We compare the interpretation performance with both the post-hoc explanation methods including GNNExplainer , PGExplainer , and GraphMask  and the intrinsic interpretable methods including DIR , IB-subgraph , GIN-GSAT, and PNA-GSAT . Following the standard setting, the evaluation metric is the explanation ROC-AUC [13; 14].
* **Prediction Baselines.** We compare the prediction performance with the powerful GNN models including GIN  and PNA  and the intrinsic interpretable methods including DIR, IB-subgraph, GIN-GSAT, and PNA-GSAT. For the OGBG-Molhiv dataset, we use the classification ROC-AUC as the prediction metric . For all the other dataset, we report the classification accuracy .

Additionally, we report the performance of \(\)-GNN which **directly fine-tunes** on downstream datasets without pre-training (denoted as the \(\)-GNN\({}_{}\)), to investigate the effectiveness of pre-training phase. All the results are averaged over 10-times evaluation with different random seeds. Architecture of the downstream GNN predictors used in \(\)-GNN and \(\)-GNN\({}_{}\) are reported in Appendix C.

### Main Results (RQ1)

To investigate the effectiveness of \(\)-GNN, we compare the interpretation and the prediction performance with the SOTA interpretation and prediction baselines. See Appendix C for the pre-training and fine-tuning details. The overall interpretation performance and prediction performance are summarized in Table 1 and Table 2, respectively. We conclude the following observations:

* \(\)**-GNN significantly outperforms the leading GNN explanation methods.** Specifically, for the Spurious-Motif dataset, \(\)-GNN surpasses DIR by 27.21% on average and by 47.31% at most. Compared with the best baselines, i.e., the GSAT with a 4-layer PNA encoder, \(\)-GNN improves the interpretation performance by 9.20% on average. However, we merely employ the combination of a truncated SVD embedding and a 2-layer MLP as the encoder, which convincingly demonstrates the effectiveness of our proposed pre-training phase. Moreover, as the degree of spurious correlation in

    &  &  &  &  \\  & & & & \(b=0.5\) & \(b=0.7\) & \(b=0.9\) \\  GNNExplainer & 67.35 \(\) 3.29 & 61.98 \(\) 5.45 & 59.01 \(\) 2.04 & 62.62 \(\) 1.35 & 62.25 \(\) 3.61 & 58.86 \(\) 1.93 \\ PGExplainer & 84.59 \(\) 9.09 & 60.91 \(\) 17.10 & 69.34 \(\) 4.32 & 69.54 \(\) 5.64 & 72.33 \(\) 9.18 & 72.34 \(\) 2.91 \\ GraphMask & 92.54 \(\) 8.07 & 62.23 \(\) 9.01 & 73.10 \(\) 6.41 & 72.06 \(\) 55.8 & 73.06 \(\) 4.91 & 66.68 \(\) 6.96 \\ IB-Subgraph & 86.06 \(\) 28.37 & 91.04 \(\) 6.59 & 51.20 \(\) 5.12 & 57.29 \(\) 14.35 & 62.89 \(\) 15.59 & 47.29 \(\) 13.39 \\ DIR & 82.78 \(\) 10.97 & 64.44 \(\) 28.81 & 32.35 \(\) 9.39 & 78.15 \(\) 1.32 & 77.68 \(\) 1.22 & 49.08 \(\) 3.66 \\ GIN-GSAT & 98.74 \(\) 0.55 & 99.60 \(\) 0.51 & 83.36 \(\) 1.02 & 78.45 \(\) 3.12 & 74.07 \(\) 5.28 & 71.97 \(\) 4.41 \\ PNA-GSAT & 93.77 \(\) 3.90 & 99.07 \(\) 0.50 & 84.68 \(\) 1.06 & 83.34 \(\) 2.17 & 86.94 \(\) 4.05 & 88.66 \(\) 2.44 \\  \(\)-GNN & **99.33**\(\) 0.63 & **99.81**\(\) 0.17 & **92.77**\(\) 0.80 & **93.24**\(\) 0.72 & **96.92**\(\) 0.85 & **96.39**\(\) 0.92 \\ \(\)-GNN\({}_{}\) & 93.19 \(\) 1.48 & 95.29 \(\) 0.67 & **85.18**\(\) 1.08 & **86.29**\(\) 2.22 & **87.43**\(\) 2.47 & **89.64**\(\) 2.26 \\   

Table 1: Interpretation Performance (ROC-AUC) Comparison. The underlined results highlight the best baselines. The **bold** results mean the \(\)-GNN or \(\)-GNN\({}_{}\) outperform the best baselines.

Spurious-Motif (i.e., the parameter \(b\)) increasing, the performance of \(\)-GNN does not decrease as most of the baselines. The post-hoc explainers, including GNNExplainer, PGExplainer, and GraphMask seems to be robust in terms of the spurious correlation, but their interpretability is limited by the fixed prediction model. Although DIR and GSAT introduce complex mechanism for mitigating the spurious correlation, \(\)-GNN still performs better. We ascribe this superiority to the generalizable knowledge which is distilled from the pre-training phase over large dataset with ground-truth explanations. For the BA-2Motifs and the Mutag datasets, \(\)-GNN using a more simpler encoder architecture also achieves comparable performance. For the MNIST-75sp dataset, \(\)-GNN surpasses the best baseline PNA-GSAT by 4.50%. Such top-tier performance strongly validates the effectiveness of \(\)-GNN explainer.
* **\(\)-GNN also achieves better prediction performance than the baselines.** Overall, \(\)-GNN outperforms all the prediction baselines consistently by a significant margin. Specifically, for the Spurious-Motif dataset, \(\)-GNN outperforms the best baselines by 11.05% on average, which is a significant improvement. If we restrict the baselines into interpretable GNNs, the performance boost is much more significant (by 13.17% on average and up to 20.45%). For the Graph-SST2 dataset, \(\)-GNN outperforms the black-box predictor (i.e., GIN and PNA), as well as the SOTA interpretable methods IB-Subgraph, which optimizes the explanatory subgraph based on the information bottleneck principle. Note that, we only implement a 2-layer GCN with global mean pooling function as the Graph-SST2 predictor. We credit such outperformance to the pre-training phase, from which the universal patterns of the graphs are potentially distilled. For the MNIST-75sp dataset, \(\)-GNN achieves comparable prediction accuracy with the GSAT methods, but our interpretation ROC-AUC exceeds GSAT by 8.09%. Additionally, one can notice that the black-box predictors are insensitive to the spurious correlation while the interpretable baselines deteriorate obviously. This phenomena may indicate that insufficient interpretability conflicts with the chase of prediction accuracy, but a powerful interpret is quite helpful to the subsequent predictor.
* **The pre-training phase advances the interpreter in terms of both interpretation and prediction performance.** As shown in Table 1 and Table 2, the \(\)-GNN with pre-training phase significantly improves both the interpretation and the prediction performance, compared with the reduced variant \(\)-GNN\({}_{}\). For the interpretation ROC-AUC, \(\)-GNN outperforms the reduced variant by 6.91% on average. This suggests that the universal structural patterns distilled in the pre-training phase can indeed generalize to various downstream tasks and improve the interpretability. Compared with the reduced variant, \(\)-GNN consistently provides much stabler interpretation with smaller variance. Additionally, one can observe that the reduced variant \(\)-GNN\({}_{}\) still surpasses the best baselines on some real-world datasets, such as the interpretation performance on MNIST-75sp dataset and the prediction performance on Graph-SST2 dataset. This may imply that the edge interaction captured by \(\)-GNN\({}_{}\) is able to identify the influential subgraphs more accurately.

Moreover, we present the explanatory visualization, the investigate of different pre-training datasets and the hyper-parameter analysis in Appendix D, E and F, respectively.

### Inter-Task Generalization Performance (RQ2)

To further investigate whether the universal structural patterns behind different tasks is common, we next study the generalization ability across different tasks. Specifically, we evaluate the \(\)-GNN model that is pre-trained over graph classification dataset PT-Motifs on the explanation task of node

   Model & Molhiv(AUC) & Graph-SST2 & MNIST-75sp & \(b=0.5\) & \(b=0.7\) & \(b=0.9\) \\  GIN & 76.69 \(\) 1.25 & 82.73 \(\) 0.77 & 95.74 \(\) 0.36 & 39.87 \(\) 1.30 & 39.04 \(\) 1.62 & 38.57 \(\) 2.31 \\ PNA & 78.91 \(\) 1.04 & 79.87 \(\) 1.02 & 87.20 \(\) 5.61 & 68.15 \(\) 2.39 & 66.35 \(\) 3.34 & 61.40 \(\) 3.56 \\ IB-Subgraph & 76.43 \(\) 2.65 & 82.99 \(\) 0.67 & 93.10 \(\) 1.32 & 54.36 \(\) 7.09 & 48.51 \(\) 5.76 & 46.19 \(\) 5.63 \\ DIR & 76.34 \(\) 1.01 & 82.32 \(\) 0.85 & 88.51 \(\) 2.57 & 45.49 \(\) 3.81 & 41.13 \(\) 2.62 & 37.61 \(\) 2.02 \\ GIN-GSAT & 76.47 \(\) 1.53 & 82.95 \(\) 0.58 & 96.24 \(\) 0.17 & 52.74 \(\) 4.08 & 49.12 \(\) 3.29 & 44.22 \(\) 5.57 \\ PNA-GSAT & 80.24 \(\) 0.73 & 80.92 \(\) 0.66 & 93.96 \(\) 0.92 & 68.74 \(\) 2.24 & 64.38 \(\) 3.20 & 57.01 \(\) 2.95 \\  \(\)-GNN & **80.86** \(\) 0.61 & **88.05**\(\) 0.43 & **96.89**\(\) 0.20 & **74.67**\(\) 0.63 & **77.52**\(\) 0.77 & **77.46**\(\) 0.96 \\ \(\)-GNN\({}_{}\) & 79.71 \(\) 1.08 & **83.48**\(\) 1.20 & 92.89 \(\) 0.95 & **70.78**\(\) 1.63 & **71.02**\(\) 1.43 & **72.61**\(\) 1.75 \\   

Table 2: Prediction Performance (Acc) Comparison. The underlined results highlight the best baselines. The **bold** results mean the \(\)-GNN or \(\)-GNN\({}_{}\) outperform the best baselines.

classification datasets. The pre-trained \(\)-GNN model and the reduced variant are both equipped with a node classifier. Following existing works, we use four widely-used synthetic node classification datasets , namely BA-Shapes, BA-Community, Tree-Cycles and Tree-Grid, whose detailed statistics are presented in Appendix B. The main result of inter-task explanation in Table 3 shows that \(\)-GNN achieves comparable performan with SOTA node classification explainers. This evidence is accordant with our basic premise that the universal structural patterns can generalize across datasets of different tasks. For the BA-Community dataset, \(\)-GNN even supasses PGExplainer with smaller variance. This may be because the community structure in the BA-Community graph conforms more to the universal patterns embedded in \(\)-GNN.

### Ablation Study

As shown in Figure 3, we conduct ablation study on \(\)-GNN, by evaluate the performance of its three variants. First, \(\)**-GNN-SPL** substitutes the structural pattern learning module with \(N\)-thread GNN encoders. Second, \(\)**-GNN-HPR** removes the hypergraph refining module and directly calculates the edge contribution score. Third, we simultaneously conduct the two ablations above and denote it as \(\)**-GNN-ALL**. Additionally, we report the performance of the variant without pre-training phase.

Specifically, when removing the structural pattern learning module, the interpretation performance on Mutag decreases by 3.85% on average and the prediction performance on Graph-SST2 decreases by 1.08% on average. For the variant \(\)-GNN-HPR, the interpretation and prediction performance decreases by 6.56% and 1.84%, respectively. When we remove the two modules simultaneously, the interpretation and prediction performance decreases by 7.56% and 2.16%, respectively. The ablation study on the two modules demonstrates their effectiveness in capturing universal structural patterns and identifying the explanatory subgraphs. Moreover, for all the variants, \(\)-GNN consistently outperforms the variant without pre-training phase by 5.59% in terms of ROC-AUC on Mutag and 2.84% in terms of accuracy on Graph-SST2.

## 5 Related Work

**Intrinsic interpretable GNNs.** The leading interpretable GNNs [12; 13; 14] usually consist of an explainer module and a predictor module. The prepositional explainer takes the raw graph as input and outputs the explanation subgraph. The subsequent predictor calculates the prediction strictly relying on the explanation. Graph neural networks with attention mechanism are regarded as the initial interpretable GNNs [13; 14], such as graph attention network , self-attention graph pooling , where the learned weights can be interpreted as the importance of certain features. Recently, invariant learning is introduced to construct intrinsic interpretable GNNs [48; 49], such as DIR . It argues that augmenting training data with causal intervention may assist explainer to distinguish the causal and non-causal parts. Besides, interpretable GNNs based on the information bottleneck principle , such as IB-Subgraph  and GSAT , are proposed to constraint the information flow from the input graph to the prediction, where the label-relevant graph components will be kept while the label-irrelevant ones are reduced.

**Pre-training on Graphs.** The research focus of current graph pre-training is the graph representation learning problem, whose objective is to learn a generic encoder \(f(,)\) for various downstream tasks [51; 52]. The development of graph pre-training can be broadly divided into pre-trained graph embeddings [41; 53; 42] and pre-trained graph encoders [23; 22; 24; 21]. Pre-trained graph embedding models aim to provide good graph embeddings for various tasks, while the models themselves are no

   Model & BA-Shapes & BA-Community & Tree-Cycles & Tree-Grid \\  GRAD & 88.20 & 75.00 & 90.50 & 61.20 \\ Attention & 81.50 & 73.90 & 82.40 & 66.70 \\ GNNExplainer & 92.50 & 83.60 & 94.80 & 87.50 \\ PGExplainer & \( 1.10\) & 94.50 \(\) 1.90 & \( 0.70\) & \( 1.40\) \\  \(\)-GNN & \(94.78 0.32\) & \( 1.50\) & \(95.19 0.88\) & 90.11 \(\) 1.10 \\ \(\)-GNN\({}_{}\) & \(93.17 0.40\) & 92.15 \(\) 1.61 & \(92.53 1.81\) & 88.62 \(\) 1.87 \\   

Table 3: Inter-Task Interpretation Performance (ROC-AUC) Comparison. The **bold** font highlights the best results and the underlined results highlight the second best method.

longer needed to the tasks. DeepWalk  explores the graph embeddings by conducting random walks over graphs to generate node sequences which contain the co-occurrence relationship. Further, Node2vec  defines a flexible node neighborhood and proposes a biased random walk process. The goal of pre-trained graph encoder models is a generic encoder model which can deal with different tasks. Gpt-GNN  pre-trains a 5-layer GIN encoder across the graph-level and node-level tasks. GCC  introduces the contrastive learning framework to pre-train the encoder over subgraph discrimination task. However, the precursor graph pre-training works are not designed to the graph explanation problem and can not be directly applied to the pre-training interpretable GNNs.

## 6 Conclusion

In this work, we for the first time investigated the universal interpretation problem in graph data and proposed the Pre-training Interpretable Graph Neural Network named \(\)-GNN. \(\)-GNN is able to work well on different types of graphs and downstream tasks. \(\)-GNN was pre-trained over a constructed large synthetic graph dataset with ground-truth explanations to distill the generalizable interpretability. Then, \(\)-GNN was fine-tuned on downstream tasks. Technically, we proposed an integrated embedding module to capture and integrate multiple graph structural patterns for more generalizable representations. A hypergraph refining module was aslo proposed to incorporate the universal patterns with local interaction for more faithful explanatory subgraphs identification. Extensive experiments on different datasets and tasks demonstrated the promising generalizable interpretability as well as prediction performance of \(\)-GNN.

Figure 3: The ablation study of \(\)-GNN on Mutag, Graph-SST2, and Spurious-Motif datasets.