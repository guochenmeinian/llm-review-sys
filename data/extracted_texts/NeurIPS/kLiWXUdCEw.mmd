# An Analysis of Elo Rating Systems via Markov Chains

Sam Olesker-Taylor

Department of Statistics

University of Warwick

Coventry, CV4 7AL, UK

sam.olesker-taylor@warwick.ac.uk

&Luca Zanetti

Department of Mathematical Sciences

University of Bath

Bath, BA2 7AY, UK

lz2040@bath.ac.uk

###### Abstract

We present a theoretical analysis of the Elo rating system, a popular method for ranking skills of players in an online setting. In particular, we study Elo under the Bradley-Terry-Luce model and, using techniques from Markov chain theory, show that Elo learns the model parameters at a rate competitive with the state of the art. We apply our results to the problem of efficient tournament design and discuss a connection with the fastest-mixing Markov chain problem.

## 1 Introduction

The Elo rating system is a popular method for calculating the relative skills of players (or teams) in sports analytics and particularly chess . It is based on a simple zero-sum update rule: if player \(i\) beats player \(j\), then the rating of player \(i\) increases proportionally to the model probability that \(i\) would lose to \(j\), while the rating of \(j\) decreases by the same amount. This amount depends on the previously estimated difference in skills between \(i\) and \(j\).

Despite their widespread popularity, Elo rating systems still lack a rigorous theoretical understanding . Here, we take a probabilistic approach and study Elo under the well-known Bradley-Terry-Luce model (BTL) . In this model, the probability \(p_{i,j}\) that \(i\) wins against \(j\) is \(w_{i}/(w_{i}+w_{j})\), where \(w_{k}\) is the _strength_ of player \(k\). In Elo, this is usually reparametrised via \(w_{k}=^{_{k}}\):

\[p_{i,j}=^{_{i}}/(^{_{i}}+^{_{j}})=1 /(1+^{_{j}-_{i}})=(_{i}-_{j}),\]

where \(_{k}\) is the _true rating_ of \(k\) and \((z) 1/(1+(-z))\) for \(z\) is the sigmoid function. In this setting, after observing \(i\) beat \(j\), the corresponding Elo ratings \(x_{i}\) and \(x_{j}\) are updated as

\[x_{i} x_{i}+(x_{j}-x_{i}) x_{j}  x_{j}-(x_{j}-x_{i}),\]

where the step-size \(>0\) is chosen by the modeller. The size of the update depends exponentially on the difference in ratings: beating a much lower rated opponent does not change the ratings much.

The goal of the Elo rating system is to estimate the true ratings of \(n\) players by observing results of matches between pairs of players. It is, therefore, aiming to solve the problem of ranking from pairwise comparisons. Compared with most algorithms in the area , however, Elo benefits from three qualities that help explain its popularity in real-world applications: simplicity, interpretability and the ability to update a ranking of the players in an _online_ fashion.

The knowledgeable reader might have noticed that Elo's update rule is actually based on the gradient of the BTL log-likelihood: Elo can be interpreted simply as stochastic gradient descent with fixed step-size. Rather than studying Elo from a convex optimisation angle, however, we take a more probabilistic point of view: we assume at each time \(t\), players \(i\) and \(j\) are selected to play against one another with probability \(q_{i,j}\). This allows us to interpret Elo as a Markov chain over \(^{n}\) and deploy powerful tools to study its behaviour. On the other hand, this approach also presents us with some challenges: Elo is not a reversible Markov chain and, while it has a unique stationary distribution, assuming a minor and natural condition on \((q_{i,j})_{i,j}\), it does not converge to it in total variation .

### Our Results

Our main contribution (Theorem 2.5) shows that Elo ratings, averaged over time, well-approximate the true ratings of the players with high probability. In order to avoid the potential of unbounded ratings, which are unrealistic in practice, we consider a variant of Elo in which ratings are capped, whilst maintaining their zero-sum property. We obtain rates of convergence with respect to the number of observed matches that are competitive against the state of the art in the BTL literature. In contrast to most other algorithms for the BTL model studied in the literature, Elo learns the parameters of a BTL model in an online fashion. These rates of convergence is remarkable since Elo was originally conceived as a simple ranking system for chess players. Furthermore, our approach is very robust, and also applies to a _parallel_ set-up in which multiple games are played concurrently.

We also discuss the problem of _tournament design_: we assume we are given a tournament (comparison) graph \(G=([n],E)\), and we would like to choose the match-up probabilities \((q_{i,j})_{\{i,j\} E}\) so that Elo's convergence rate is maximised. We highlight a connection between this problem and that of finding the _fastest mixing Markov chain on \(G\)[10; 39]_, where the corresponding Markov chain on the graph is either in discrete or continuous time depending on whether we optimise number of _games_ or _parallel rounds_, respectively. As far as we know, this connection has not been made formally before in the sequential set-up, and both the analysis and optimisation of the parallel set-up are new. In SS4, we also provide experimental results that showcase the usefulness of our strategy.

### Related Work

Despite Elo ratings being the standard ranking system in many sports analytics communities , there is a scarcity of work analysing Elo from a probabilistic perspective. In particular, we are aware only of , and the unpublished notes  by the same author, in which Elo is studied as a Markov process and convergence in distribution to a unique stationary distribution is proved.

If we consider Elo as a technique to estimate the parameters of the BTL model, there is a wealth of recent literature on the topic by the machine learning community [20; 37; 31; 5; 27; 9]. In contrast to our setting, however, previous work typically considers an _offline_ scenario, in which the ratings of the players are computed after all the scheduled matches have taken place. By standard concentration inequalities, this allows one to obtain a very good approximation of the probability a player wins against their neighbours in the comparison graph. The goal is then to deduce a global ranking of the players from such (very good) local information. In our setting, Elo ratings are dynamically updated before a good local approximation is achieved, which makes the analysis more challenging and requires the use of powerful, but delicate, concentration inequalities for Markov chains.

A comparison between the rate of convergence for Elo ratings obtained in our work and the rate of convergence of other algorithms for the BTL model is discussed in SS2.

Finally, we mention that the connection between Markov chains and stochastic gradient descent with fixed step size has been studied before, e.g., in .

## 2 Convergence Rates of Elo Ratings

In this section, we state our main result on the convergence rate of the time-averaged Elo ratings, discuss related work and outline the most important parts of the proof.

Throughout the paper, "\(f g\)" means "\(f=O(g)\)", "\(f g\)" means "\(f=o(g)\)" and "whp" means "with probability \(1-O(1/n)\)"; the notation \(()\) hides logarithmic factors; finally, \(_{}[]\) indicates that \(X^{0}\).

### Our Results

We start with the explicit definition of our Markov chain.

**Definition 2.1** (Elo Process).: Let \(M\) and \(n 2\). Let \([-M,M]^{n}\) with \(_{k}_{k}=0\). Let \(q\) be a distribution on unordered pairs in \([n]\). Let \((0,)\). A step of \(_{M}(q,;)\) proceeds as follows.

1. Suppose that the current vector of ratings is \(x^{n}\).

1. Choose unordered pair \(\{I,J\}\) to play according to \(q\): \[[\{I,J\}=\{i,j\}]=q_{\{i,j\}} i,j[n].\]
2. Suppose that Player \(I\) beats \(J\), which has probability \((_{I}-_{J})\). Update ratings \(x_{I}\) and \(x_{J}\): \[x_{I} x_{I}+(x_{J}-x_{I}); x_{J} x_{J}- (x_{J}-x_{I}).\]
3. Orthogonally project the full vector of ratings to \([-M,M]^{n}\{x^{}^{n}_{k}x^{}_{k}=0\}\).

Let \(X^{t}_{k}\) denote the rating of Player \(k\) at time \(t\), and \(\) the equilibrium distribution on \(^{n}\). Denote by

\[A^{t,T}_{k}_{s=T}^{T+t-1}X^{s}_{k}  k[n] t,T>0\]

the _time-averaged ratings_. We typically start from \(X^{0}=(0,...,0)\). The (deterministic) time \(T\) is a _burn-in phase_, which allows the Elo ratings to get 'near' the true skills, after which we start averaging.

_Remark 2.2_.: The projection step ensures the Elo ratings do not become too large. It is required for our analysis, but not usually implemented in practice. Indeed, our experiments, discussed in SS4, suggest that, as long as the step-size \(\) is small enough, Elo ratings remain bounded. Algorithms which estimate BTL parameters typically require some sort of projection  or regularisation . A simple and efficient algorithm to realise the orthogonal projection is presented in the Appendix.

We show, for a suitable choice of parameters \(t\), \(T\) and \(\), that the time-averages are concentrated around the true ratings. In other words, we can use Elo to obtain an MCMC estimate of the true ratings.

Elo ratings in equilibrium are, in general, a _biased_ estimator of the true ratings: i.e., \(_{}[X^{0}_{k}]_{k}\). Hence, there will be both a _bias_ and an _error_ term in our MCMC-type estimate of \(\|A^{t,T}-\|_{2}\).

The MCMC convergence rate depends on a _spectral gap_\(_{q}\). This parameter quantifies how fast _local_ information about the relative strengths of two players is propagated to the rest of the ratings. Similar parameters appear in most of the related literature, e.g., .

**Definition 2.3** (Spectral Gap).: Let \(q\) be a distribution on unordered pairs in \([n]\). Define \(q_{i,j} q_{\{i,j\}}\), \(d_{i,j}\{i=j\}_{k}q_{i,k}\) and \(_{i,j} d_{i,j}-q_{i,j}\) for \(i,j[n]\). Let \(_{q}\) denote the _spectral gap_ (second smallest eigenvalue) of the _Laplacian_\(\). Always, \(=\) and \(\) is positive semi-definite.

_Remark 2.4_.: Equivalently, \(_{q}\) is the spectral gap of the continuous-time Markov chain on \([n]\) with transition rates \(q_{i,j}=q_{\{i,j\}}\) for \(i,j[n]\). Note the scaling: \(_{i,j}q_{i,j}=2\). So, the typical time until the continuous-time chain jumps is order \(n\), not order \(1\). This implies \(_{q} 4/n\); see Lemma B.2.

We assume \(_{q}>0\). This holds unless there exists a non-empty subset \(S[n]\) with \(_{i S,j S}q_{i,j}=0\)--i.e., players in \(S\) never play those in \(S^{c}\). This makes estimation of the ratings impossible.

Our main result measures the disparity between the time-averaged Elo ratings and the true ratings.

**Theorem 2.5** (Convergence Rate).: _Let \(X_{M}(q,;)\), as in Definition 2.1. Let \(C_{1},C_{2}<\). Then, there exists a constant \(C_{0}\), depending only on \((C_{1},C_{2})\), such that if_

\[\{_{q},,1/t\} n^{-C_{1}}\{t,T\}  C_{0}t_{}, t_{} e^{2M}^{-1} _{q}^{-1} n,\]

_then_

\[\|A^{t,T}-\|_{2}^{2}e^{4M}}{ _{q}n}+}{_{q}t}  1-n^{-C_{2}}.\]

_In particular, if \(( n)^{2}/(_{q}t)\), then_

\[\|A^{t,T}-\|_{2}^{2}( n)^{2}}{ _{q}n}t} n.\]

_Remark 2.6_.: Ideally, \(_{q}n=(1)\); e.g., if \(q\) is uniform over the edges of an _expander graph_, then \(_{q}n 1\). In this case, we view \(( n)^{2}/(_{q}n)=(1)\) as the error's _pre-factor_ and \(1/(_{q}t)\) as the _(squared) convergence rate_. Also, we can then choose \(=(1)\) and obtain non-trivial convergence results. This choice of \(\) is comparable to that used in practice. Moreover, on average, only \((1)\) games _per player_ are need to be observed to guarantee good approximation of the ratings.

The \(\) term arises from the average bias \(\|}_{X}[X]-\|_{2}^{2}\), which is non-zero in general. An estimate on the average variance \(_{k}_{X}}[X_{k}]\) is necessary to obtain the MCMC convergence rate.

**Theorem 2.7** (Bias and Variance).: _If \(\) is the equilibrium distribution of \(}_{M}(q,p;)\), then_

\[\|}_{X}[X]-\|_{2}^{2} 4e^{4M} /(_{q}n)_{k}} _{X}}[X_{k}] 4e^{2M}/(_{q}n).\]

Despite Elo ratings being biased, the estimated probability a player wins their next match is not:

\[_{j[n]}q_{i,j}}_{X}[(X_{i}-X_{j})] =_{j[n]}q_{i,j}(_{i}-_{j}) i[n].\]

This holds when no projection step is performed as part of the Elo update (equivalently, \(M+\)).

The _mixing time_ is more delicate, as the chain makes deterministic-size discrete jumps, but its equilibrium distribution is continuous and, therefore, the chain does not converge in total variation. Instead, we measure convergence in the _Wasserstein_, also known as _transportation_, distance.

**Theorem 2.8** (Contraction).: _If \(X,Y}_{M}(q,;)\), then there exists a step-by-step coupling with_

\[}_{(x,y)}[\|X^{t}-Y^{t}\|_{2}^{2}](1-)^{t} \|x-y\|_{2}^{2}e^{-2M}.\]

Markov chains satisfying the contraction property (i.e., _positively curved_) satisfy powerful concentration inequalities, developed particularly by Joulin and Ollivier . The idea is that if \(\|X^{0}-Y^{0}\|_{2}=D^{0}\), then \(D^{t}\|X^{t}-Y^{t}\|_{2}\) is small when \(t^{-1} D^{0}\). For us, \(D^{0} 2Mn\), leading to \(t_{}^{-1} n\). In the reversible case, this can provide bounds, e.g., on the spectral gap.

The approach is particularly applicable to Markov chains on _finite_ metric spaces, such as graphs. The set-up of finite graphs was analysed by Bubley and Dyer  under the name _path coupling_. In this case, \(}[D^{t}]\) implies \(}[X^{t}=Y^{t}]\), since the minimum graph distance between \(x y\) is \(1\).

Our underlying metric space is \((^{n},\|\|_{2})\), however. Inequalities for this more general set-up have been developed, but they often give weaker bounds. In our case, they lose a factor \(1/(_{q}) n\) in the convergence rate. Morally, though, the exponential convergence at rate \(\) still implies that \(1/\) is the correct timescale for mixing and concentration, perhaps up to some logarithmic factors. Establishing this rigorously is the most challenging part of our work from a technical point of view.

### Comparison with Related Work

In this section, we compare the convergence rate of Elo given by Theorem 2.5 against the state of the art for BTL estimation. We highlight, once again, that previous work focusses on the problem of _offline_ estimation, whilst Elo works in the more challenging online setting. Nevertheless, Elo is able to match the state-of-the-art algorithms in the offline setting for a wide range of parameters.

Our results imply that Elo provides an estimator \(\) of \(\) with \(\|-\|_{2}^{2}^{4M}( n)^{2}/(_{q}^{2 }t)\) whp. This matches, up to a log factor, the results by Hajek, Oh and Xu , who prove that the MLE constrained on \([-M,M]^{n}\{x^{n}_{k}x_{k}=0\}\), \(_{}\), satisfies \(\|_{}-\|_{2}^{2}^{8M} n/(_{q}^ {2}t)\). The dependency on \(_{q}\) has been improved by Shah, Balakrishnan and Bradley , who show \(\|_{}-\|_{2}^{2} ne^{8M} n/(_{q}t)\). Since \(n/_{q} 1/4\), this is at least as good as our result, and potentially better for certain choices of \(q\). However, up to log factors, our result matches theirs when \(q\) corresponds to sampling edges of an expander graph--e.g., a complete graph, or an Erdos-Renyi graph with parameter \(p( n)/n\). Our result improves the constant in front of \(M\).

More recently, Li, Shrotriya and Rinaldo  proved a regularised version of the MLE, \(_{}\), achieves \(\|_{}-\|_{2}^{2}^{2M_{E}}/(_{ q}^{2}t)\), where \(\) is the ratio between the maximum and average degree of the comparison graph \(G=([n],E)\) with \(E=\{\{i,j\} q_{i,j}>0\}\) and \(M_{E}=_{i,j:q_{i,j}>0}|_{i}-_{j}| 2_{k}|_{k}|\). A version of our analysis also applies with \(M_{E}\), instead of \(M\), but we are not able to prove \(_{i,j:q_{i,j}>0}|X_{i}^{t}-X_{j}^{t}|<M_{E}+1\) holds for polynomially long. Notice that their result is weaker when the comparison graph is relatively sparse but has a few high degree nodes. Moreover, they require each player to play the same number of matches; i.e., \(q_{i,j}=1/|E|\) for \(\{i,j\} E\). Both limitations are particularly problematic in the context of tournament design discussed in SS3. On the other hand, they obtain \(_{}\) error bounds too. We refer to  for further discussion of previous work.

### Outline of Proof

We now highlight the key steps in estimating the MCMC-type error \(\|A^{t,T}-\|_{2}^{2}\) in Theorem 2.5, where \(A^{t,T}\) is the \(t\)-step time average of the ratings \(X\) after a burn-in phase of length \(T\). We use

\[\|A^{t,T}-\|_{2}\|A^{t,T}-}_{}[X^{0}]\|_{2}+ \|}_{}[X^{0}]-\|_{2},\]bounding these _error_ and _bias_ terms separately, via Theorems 2.8 and 2.7, respectively. Their proofs rely on estimating the change in \(_{2}\) norm a single step, and using the Lipschitz property of the sigmoid function \(\), along with some other careful manipulations. The full proofs are given in the appendix.

The primary challenge is to leverage the positive curvature result of Theorem 2.8 to deduce the required concentration inequality in Theorem 2.5. As noted at the end of SS2.1, positive curvature is well-suited to this goal, but the specifics of our set-up cause significant difficulties. Particularly, the aforementioned results reliant only on curvature are not sufficiently strong for our purposes.

A further complication is that Elo is a _non_-reversible Markov chain. There is a general understanding, or perhaps belief, in the MCMC community that non-reversibility can improve concentration results. This has lead to many strategies being proposed, such as non-backtracking, lifting, or zig-zag; see, e.g., . Even an alternative definition of the spectral gap has been proposed ; this gap controls convergence of empirical averages, rather than convergence to equilibrium.

Unfortunately, the majority of general concentration results based on spectral properties  are actually _worse_ in the non-reversible set-up: they bound the convergence rates by those of the multiplicative or additive reversibilisation. These can be hard to estimate, needing detailed knowledge of the equilibrium distribution, and the resulting bounds are often crude.

Finally, we mention that bounds on the total-variation mixing time can be leveraged to provide concentration bounds; see, particularly, . However, Elo does not converge in total variation! Indeed, \(X^{t}\) is finitely supported, on at most \((2n)^{t}\) states, given \(X^{0}\), but \(\) is continuous. These \((2n)^{t}\) states can be used to distinguish \(X^{t}\) and \(\). Nevertheless, it is this mixing-time approach that we adapt.

The contraction property implies that two realisations \(X\) and \(Y\) can be coupled so that their expected relative distance decreases exponentially, with rate \(\). This is ideal for using the coupling approach to mixing. However, it is not possible to guarantee that \(X^{t}=Y^{t}\) at some point, due to the chain's finite support. Instead, we analyse a _noisy version_: after each update, some (continuous) noise is added to the ratings. This must be added carefully to preserve the contraction in Theorem 2.8.

Denote the noisy versions \(U\) and \(V\). Once \(U\) and \(V\) are sufficiently close, the additive noise can be used to couple them exactly, to get \(U^{t}=V^{t}\). The size of the noise must be balanced: too small and this final coupling step is too difficult; too big and \(U\) can no longer be compared with \(X\).

We delve deeper into this noisy approximation, explaining what noise to add, how it accumulates and what the resulting mixing time is. The interaction of the noise with the projection step is delicate and technical; we omit it from the description here, but it should be kept in the back of the mind.

_Noisy Version & Error._ We consider the following noisy version \(U\) of the Elo process \(X\). Let \(>0\).

1. Draw \(U^{t+1/2}\) according to an _uncapped_ Elo step started from \(U^{t}\)--i.e., as if \(M=\). Suppose that Players \(i\) and \(j\) were chosen--i.e., \(\{k U^{t+1/2}_{k} U^{t}_{k}\}=\{i,j\}\).
2. Draw \(_{i},_{j}^{}([-,+])\) independently and set \(_{k}:=0\) for \(k\{i,j\}\). Set \[U^{t+1}_{k} U^{t+1/2}_{k}+_{k} k[n].\]

This _does not_ preserve the zero-sum property, as the additive noise is independent. The independence is _crucial_ later to allow two version to coalesce. We need to control the cumulation until that point.

We control the difference between \(X\) and \(U\) via the natural coupling: pick the same pair of players to play, and observe the same result; sample the noise independently. Careful computation gives

\[\|X^{t+1}-U^{t+1}\|_{1}\|X^{t+1/2}-U^{t+1/2}\|_{1}+2\|X^ {t}-U^{t}\|_{1}+2... 2t.\]

The second inequality actually says that Elo is non-negatively curved in \(_{1}\). Iterating this,

\[\|_{s=0}^{t-1}X^{s}-_{s=0}^{t-1}U^{s} \|_{1}_{s=0}^{t-1}\|X^{s}-U^{s}\|_{1} t.\]

_Curvature._ We use the natural coupling between two noisy versions \(U\) and \(V\): choose the same players, observe the same result and add the same noise. Then, there is still rate-\(\) contraction:

\[_{u^{0},v^{0}}[\|U^{1}-V^{1}\|_{2}]_{u^{0},v^{0}}[\|U ^{1/2}-V^{1/2}\|_{2}](1-)\|u^{0}-v^{0}\|_{2}.\]_Mixing Time._ We bound the total-variation mixing time of the noisy chain via the coupling method:

\[\,_{u^{0}}[U^{t}]-_{v^{0}}[V^{t}] _{}_{(u^{0},v^{0})}[U^{t} V^{t}].\]

First, we burn-in using the natural coupling, then use a new coupling which exploits the noise.

We start with the natural coupling as given above. Using the rate-\(\) curvature,

\[[\|U^{t}-V^{t}\|_{2}](1-)^{t}\|U^{0}-V^{0}\|_{2} 2Mne^{ - t}^{2} t t_{}^{- 1}(2Mn/^{2}).\]

Hence,

\[[\|U^{t}-V^{t}\|_{}>][\|U^{t}-V^{t}\|_{2} >] t t_{}.\]

Once the absolute difference \(|U^{t_{}}_{k}-V^{t_{}}_{k}|\) are at most \(\) for all \(k\), the additive noise, which is order \(\), dominates the change in difference after a single Elo step, which is order \(\). Thus, with complementary probability order \(/=\), we can couple the noise so that the rating of a player is the same in both \(U\) and in \(V\) after they play a game. Moreover, such a _successful_ step preserves the \(_{}\) bound of \(\).

The \(_{}\) bound implies that all players are chosen within \(t_{}\) steps with probability at least \(1-\). Hence, the probability of _not_ successfully matching all ratings after \(t_{}\) steps is at most order \(+t_{}\).

Combining these two bounds gives a bound of order \(+t_{}\) on the total-variation distance at time \(2t_{}\). The polynomial-growth/decay assumptions in the theorem allow us to choose \(\) to be an appropriate inverse polynomial (in \(n\)) and obtain \(+t_{} 1\) and \(t_{}^{-1} n t_{}\). \(\)

The above mixing-time bound allows us to establish concentration of time-averages of the noisy Elo ratings, using results from . Particularly, under certain assumptions, if \(Z\) is a Markov chain with equilibrium distribution \(\) and mixing time \(t_{}\), then

\[-_{}_{s=0}^{t-1}f(Z^{s})-_ {f}_{f}^{-2}^{2}t/t_{},\]

where \(_{f}=_{}[f]\) and \(_{f}^{2}_{}[f]\). Notice that this requires \(Z^{0}\), which we do not impose; this requirement can be circumnavigated by using a burn-in, again comparing with a noisy version.

We want to take \(f f_{k}\) to be the projection onto the \(k\)-th coordinate, which is the \(k\)-th player's rating, for each \(k\), then do a union bound over the \(n\) players. This motivates taking

\[_{k}^{2}_{f_{k}}^{2}t_{},_{k}_{k}^{2}n}}=n}}{ _{q}t},\]

using Theorem 2.7. This is the decay rate required in Theorem 2.5, but for the _noisy_ version.

We need to compare the noisy version with the original. We do this via the estimate established earlier:

\[_{s=0}^{t-1}X^{s}-_{s=0}^{t-1}U^{s} _{1}_{s=0}^{t-1}\|X^{s}-U^{s}\|_{1} t.\]

We are taking \(t\) to be polynomial in \(n\), and can choose \(^{-1}\) to be a sufficiently large polynomial so that this accumulated error is small. Care must be taken to to make all this rigorous, but once it is done, we are able to deduce the concentration result for the original Elo process.

## 3 Tournament Design

### Our Results

In this section, we assume we are given a _comparison graph_\(G=([n],E)\), where edge \(\{i,j\} E\) indicates that Players \(i\) and \(j\) are able to play against one another. We want to construct a distribution \(q\) over edges \(E\) of \(G\) so that Elo can most efficiently approximate the true ratings of the players.

The decay rate in Theorem 2.5 is governed by the spectral gap \(_{q}\). So, we want to maximise \(_{q}\). Let

\[_{}^{}_{q}\ \ q^{E},\,_{e E}q_{e}=1}.\]

This equals the largest spectral gap achievable by a _continuous-time_ Markov chain on \([n]\) with transitions only across edges of \(G\) and average jump-rate \(1/n\). It is the _fastest-mixing Markov chain_ problem, introduced by Sun et al. , and can be formulated as a semidefinite program. Olesker-Taylor and Zanetti  recently proved that \(_{}^{}n 1/(G)^{2}\). This implies the following.

**Corollary 3.1** (Optimised \(q\)).: _Suppose that the comparison graph \(G=([n],E)\) is given. In the set-up of Theorem 2.5, there exists a distribution \(q\) on the edges \(E\) such that_

\[\|A^{t,T}-\|_{2}^{2} n(G)^{2}( n )^{2}/t n(G )^{2}( n)^{2}/t.\]

This choice of probabilities \(q\) can improve drastically over uniform weights--as used by . E.g., if \(G\) consists of two cliques connected by \(k\) edges, then \(_{q}n k/n^{2}\) when \(q_{e} 1/|E|\) is uniform over \(E\), whilst the optimal \(_{}^{}n 1\). As a consequence, with the optimal choice of \(q\), only \((n)\) total matches need to be played to obtain a good approximation of the true ratings, compared with \((n^{3}/k)\) for a uniform \(q\). Here, \(()\) indicates the asymptotic order up to logarithmic factors. This demonstrates the power of being able to _choose \(q\)_, given \(G\).

Elo naturally parallelises: if two games consist of _disjoint_ pairs of players, then the Elo update resulting from one is independent of the result of the other. Hence, if we wish to minimise the number of _rounds_ (i.e., sets of games that can be played in parallel), rather than _games_, we should consider a distribution \(\) on the set \(\) of _matchings_ of the graph \(G=([n],E)\)--i.e., collections of disjoint edges.

**Definition 3.2** (Parallel Elo).: Let \(\) be a distribution on \(\). A single step of \(_{M}(,;)\) first selects a matching \(S E\) according to \(\) and applies the Elo update (with scale \(\)) to each pair in \(S\). Then, the resulting vector is orthogonally projected back to zero-sum vectors in \([-M,+M]^{n}\).

Our analysis is robust enough to handle the parallel case, obtaining convergence rate \(1/(_{q}t)\), where now \(q_{e}\) is the marginal probability that edge \(e E\) appears in the matching:

\[q_{e}_{S:e S}_{S}.\]

**Theorem 3.3** (Parallel).: _Let \(X_{M}(,;)\). Then, under the conditions of Theorem 2.5,_

\[\|A^{t,T}-\|_{2}^{2}( n)^{2}}{ _{q}n/N}t} }{_{q}t},\]

_where \(N_{e E}q_{e}\) is the mean size of the matching. To emphasise, here, \(t\) and \(T\) count rounds._

_Remark 3.4_.: It can be shown that this factor \(N\) is needed in the pre-factor via a time-change analysis.

It is natural to optimise \(_{q}\) over \(q\) which can arise as the marginals of a distribution \(\) on \(\). Clearly,

\[q_{k}_{e E:k e}q_{e} 1 k[n]\]

for any such \(q\). Thus, the matrix \(Q=(q_{i,j})_{i,j V}\) is substochastic, so \(_{q}\) corresponds to the spectral gap of the _discrete-time_ Markov chain on \(G=([n],E)\) with weights \(q_{i,j}=q_{\{i,j\}}\). Let

\[_{}^{}\{_{q} q^{E},\;_{k[n]}q_{k} 1\}.\]

This is the optimal spectral gap of a _discrete-time_ Markov chain on \([n]\) with transitions allowed only across edges of \(G\) and uniform stationary distribution. Again, \(_{}^{}\) can be formulated as a semidefinite program  and is related to the _vertex conductance_ via a Cheeger-type inequality .

We show that a distribution \(\) over matchings with \(_{q}_{}^{}\) can be found by decomposing the substochastic \(Q\) into a convex combination of permutation matrices using the Birkhoff-von-Neumann theorem, followed by decomposing each permutation into disjoint cycles.

**Corollary 3.5** (Optimised \(\)).: _Suppose that the comparison graph \(G=([n],E)\) is given. In the set-up of Theorem 3.3, there exists a distribution \(\) on matchings \(\) such that_

\[\|A^{t,T}-\|_{2}^{2}( n)^{2}}{ _{}^{}n/N}}^{}t} }{_{ }^{}t}.\]

In many examples, such as if \(G=([n],E)\) is an expander, but also if \(G\) is a cycle, then \(_{}^{} n_{}^{}\). In this case, parallel Elo really is as good, up to constants, as \(n\) steps of the original ('series') Elo. (Recall that \(N=_{e}q_{e}=1\) is required for \(_{}^{}\), but that \(N n\) is possible for \(_{}^{}\).)

Other times, there is already a continuous-time chain, with average jump-rate \(1\), which has spectral gap order \(1\); e.g., two cliques connected by a single edge. In this case, the optimal parallel version gives no real improvement, even measured by _rounds_, over the optimally weighted series version.

### Comparison with Related Work

The problem of designing an efficient tournament graph is related to _active ranking_. Active ranking, however, allows one to choose which matches to schedule next _after_ observing the results of some matches. For example, Yan et al.  propose an algorithm to identify the _most informative_ pair of players given previous outcomes, and obtain regret bounds between Elo and the true ratings when matchups are scheduled according to their algorithm. In contrast, we are interested in designing a probability distribution over matches in an _offline_ manner, without the possibility of changing such distribution after observing some results.

This problem has been considered by Li, Shrotriya and Rinaldo . They discuss a divide-and-conquer strategy that essentially requires oversampling edges across _bottlenecks_ in the graph. The drawback of this strategy is that it requires partitioning the graph into well-connected pieces, which is a non-trivial task itself. Furthermore,  does not provide explicit bounds on the sample complexity that can be obtained in this way, besides discussing a few examples where the bottleneck is known.

Nonetheless, our approach shares some similarity with : the fastest-mixing Markov chain implicitly up-weights edges across bottlenecks. The main advantage, however, is that it provides the _optimal_ spectral gap allowed by a graph topology, which is related to the diameter of the graph . Moreover, both \(^{*}_{}\) and \(^{*}_{}\) can be formulated as SDPs, for which fast (polynomial-time) solvers exist. We remark, however, that this construction might require certain nodes to play an overwhelmingly large number of matches: this would be problematic for the results of . As far as we know, minimising the number of 'parallel rounds' has not been considered before.

## 4 Experimental Results

We close with discussion of some specific examples and experimental results. Additional experiments are discussed in the Appendix. We start by considering _dumbbell_ tournament graphs consisting of two cliques of \(n/2=20\) vertices connected by a matching of \(k\{1,20\}\) edges. For each graph, we perform Elo simulations where the match-ups between players are sampled according to the following probability distributions.

1. The _uniform_ distribution \(q_{}\) over the edges of the graph.
2. The _optimal sequential_\(q^{*}_{}\) derived from the fastest-mixing continuous-time Markov chain.
3. The distribution over matchings \(q^{*}_{}\) derived from the fastest mixing discrete-time Markov chain, where multiple games are played in _parallel_ in each _round_.

From SS3, we know that \(_{q_{}} k/n^{3}\), \(_{q^{*}_{}} 1/n\) and \(_{q^{*}_{}} k/n\).

We sample the true ratings of the players according to independent Gaussians, with mean equal to 1 on one clique, mean 2 on the other, and standard deviation equal to \(0.2\) in both. This difference in average ratings between cliques simulates a scenario where the two cliques correspond to two different leagues of slightly different strength on average.

We perform Elo simulations initialising the Elo ratings at zero and setting \(=0.1\). For each graph, we repeat each experiment ten times, each time sampling new true ratings. Experimental results are displayed in Figures 1 and 2. Simulations are repeated ten times: lines correspond to the average \(_{2}^{2}\)-distance between time-averaged Elo ratings and the true ratings, divided by the number of players (\(n=40\)). Shaded regions corresponds to 25-75 percentile over these ten trials. Cyan and blue lines correspond to the same experiments where we have sampled multiple games in parallel as described in SS3; we display the decay of error wrt the total number of games played in cyan and wrt the number of parallel rounds in blue. The blue line is solid for the first \(2 10^{4}\) games (same number of games displayed in cyan).

We display the experimental results for \(k=1\), i.e., two cliques of \(20\) vertices connected by a single edge, in Figure 1. The experiments align with the theoretical results of SS3: when \(k=1\), the spectral gap \(_{q^{*}_{}}\) of the optimal sequential distribution is of the same order of the spectral gap for our nearly parallel construction \(_{q^{*}_{}}\). Indeed, if convergence is measured wrt the number of rounds, the two corresponding errors seems to decay at a similar same rate, with the parallel version being slightly better, but requiring more than ten times the total number of games. As predicted by the theory, both distributions result in much faster convergence than the uniform distribution. If we measure the totalnumber of games rather than rounds, the parallel version still results in a faster convergence than the uniform distribution, but not overwhelmingly so.

We now consider a dumbbell graph with \(k=n/2\), i.e., two cliques connected by a perfect matching. In this case, the fastest discrete- and continuous-time Markov chains have the same order-\(1\) spectral gap. However, since the spectral gap for sequential Elo is then rescaled by a factor of \(1/n\), to achieve convergence, we expect the number of _rounds_ for the parallel version to be much smaller than the number of _games_ required by the sequential one; convergence should instead happen at the same rate when measured in the total number of games. This is clearly shown in Figure 2. The uniform distribution performs much worse, which we would expect from its order-\(1/n^{2}\) spectral gap. In the optimal parallel and sequential distributions, the probability to sample an edge from the bottleneck or from inside the cliques is balanced, while the uniform distribution oversamples edges inside the cliques. Experiments for the intermediate case of \(k\{5,10\}\) are discussed in the Appendix.

We end this section by discussing experimental results concerning the maximum rating reached by Elo. Figure 3 displays the behaviour of the largest Elo rating in absolute value, where pairs of players are selected uniformly at random (i.e., the underlying graph is a complete graph). The true ratings are sampled uniformly at random in \([-1,1]\). The initial Elo ratings are set equal to zero. We simulate up to 50000 matches for a number of players that goes from 100 to 1000. We observe that the maximum rating is always below 1.75, corroborating our belief that, in many scenarios, the maximum Elo rating does not diverge for a long time. Further experiments and discussions are given in the Appendix.

## 5 Conclusion and Open Problems

Our work is a first step towards establishing the theoretical foundations of the Elo rating system, a popular ranking method in sports analytics. In particular, our main contribution is an analysis of Elo under the BTL model, establishing convergence results competitive with the state of the art.

There are several questions prompted by our work. First, from a technical point of view, we would like to control the maximal Elo rating. This is necessary to understand when we can remove the projection step in our definition of the Elo Markov chain, and make our theoretical results more aligned with practice.

Figure 1: Elo simulation results for a dumbbell graph with one edge between two cliques of \(20\) vertices. Match-ups are sampled from three different probability distributions.

Figure 3: Largest Elo rating in absolute value for a complete graph of varying size. True ratings are uniformly distributed in \([-1,1]\).

Figure 2: Elo simulation results for a dumbbell graph with a perfect matching of \(20\) edges between two cliques of \(20\) vertices.

Moreover, we would like to better understand the shape of the stationary distribution of Elo. This could help us, for example, obtain bounds on the rate of convergence in \(_{}\).

Finally, a touted strength of the Elo rating system in practical applications is its ability to dynamically update the ratings in response to changes in players' skills. Can we model these changes in a way that allows us to prove Elo can keep track of them, and bound the corresponding mean squared error? Such questions are often studied in the statistics literature; see, e.g., the recent paper  for details.