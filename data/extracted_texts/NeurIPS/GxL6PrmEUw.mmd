# Distributional Learning of Variational AutoEncoder: Application to Synthetic Data Generation

Seunghwan An

Department of Statistical Data Science, University of Seoul, S. Korea

{dkstmdghks79, jj.jeon}@uos.ac.kr

Jong-June Jeon

Department of Statistical Data Science, University of Seoul, S. Korea

{dkstmdghks79, jj.jeon}@uos.ac.kr

Corresponding author.

###### Abstract

The Gaussianity assumption has been consistently criticized as a main limitation of the Variational Autoencoder (VAE) despite its efficiency in computational modeling. In this paper, we propose a new approach that expands the model capacity (i.e., expressive power of distributional family) without sacrificing the computational advantages of the VAE framework. Our VAE model's decoder is composed of an infinite mixture of asymmetric Laplace distribution, which possesses general distribution fitting capabilities for continuous variables. Our model is represented by a special form of a nonparametric M-estimator for estimating general quantile functions, and we theoretically establish the relevance between the proposed model and quantile estimation. We apply the proposed model to synthetic data generation, and particularly, our model demonstrates superiority in easily adjusting the level of data privacy.

## 1 Introduction

Variational Autoencoder (VAE)  and Generative Adversarial Networks (GAN)  are generative models that are used to estimate the underlying distribution of a given dataset. To avoid the curse of dimensionality, VAE and GAN commonly introduce a low-dimensional latent space on which a conditional generative model is defined. By minimizing an information divergence between the original data and its generated data, the generative models are learned to produce synthetic data similar to the original one. Accordingly, VAE and GAN have been applied in various applications, such as generating realistic images, texts, and synthetic tabular data for privacy preservation purposes .

However, the difference in the strength of the assumption about the generative distribution brings significant contrasts in the VAE and GAN generation performances . In the GAN framework, the adversarial loss enables direct minimization of the Jensen-Shannon divergence between the ground-truth density function and the generative distribution under no distributional assumption . Roughly speaking, the GAN employs a nonparametric model as its conditional generative model defined on the latent space.

On the contrary, in the VAE framework, the Gaussianity assumption has been favored . It is because Gaussianity gives us three advantages: 1) the reconstruction loss can be interpreted as the mean squared error that is one of the most popular losses in optimization theory, 2) generating a new sample is computationally straightforward, and 3) KL-divergence is computed in a simple closed form. However, these benefits have led us to pay the price for the distributional capacity of the generative model, in that the generative model of the VAE is constrained in the form of marginalization of the product of the two Gaussian distributions. Here, the distributional capacity means the expressive power of the distributional family. This restricted distributional capacity hasbeen the critical limitation [8; 33] and leads to a heavy parameterization of the decoder mean vector to approximate complex underlying distributions.

To increase the distributional capacity in synthetic data generation, [63; 65] introduce the multi-modality in the distributional assumption of the decoder, which is known as the _mode-specific normalization technique_. Although the mixture Gaussian decoder modeling of [63; 65] allows handling more complex distributions of the observed dataset while preserving all of the advantages of Gaussianity, we numerically find that the mixture Gaussian is not enough to capture the complex underlying distribution.

Our main contribution is that, beyond Gaussianity, we propose a novel VAE learning method that directly estimates the conditional cumulative distribution function (CDF) while maintaining the objective of maximizing the Evidence Lower Bound (ELBO) of the observed dataset. It implies that we have a nonparametric distribution assumption on the generative model. We call this approach _distributional learning of the VAE_, which is enabled by estimating an infinite number of conditional quantiles [4; 20]. By adopting the _continuous ranked probability score_ (CRPS) loss, the objective function of our proposed distribution learning method is computationally tractable [21; 43; 20].

In our proposed distributional learning framework, 1) the reconstruction loss is equivalent to the CRPS loss, which is a _proper scoring rule_[21; 43], 2) generating a new sample is still computationally straightforward due to the inverse transform sampling, and 3) KL-divergence is still computed in a simple closed form. To show the effectiveness of our proposed model in capturing the underlying distribution of the dataset, we evaluate our model for synthetic data generation with real tabular datasets.

## 2 Related Work

**Modeling of the decoder and reconstruction loss.** To increase the distributional capacity, many papers have focused on decoder modeling while not losing the mathematical link to maximize the ELBO. [57; 1] assume their decoder distributions as Student-\(t\) and asymmetric Laplace distributions, respectively, to mitigate the _zero-variance problem_ that the model training becomes unstable if the estimated variance of the decoder shrinks to zero in Gaussian VAE [41; 55; 15].  proposes a general distribution of the decoder, which allows improved robustness by optimizing the shape of the loss function during training. Recently,  proposes a reconstruction loss that directly minimizes the _blur error_ of the VAE by modeling the covariance matrix of multivariate Gaussian decoder.

On the other hand, there exists a research direction that focuses on replacing the reconstruction loss without concern for losing the mathematical derivation of the lower bound. [38; 52; 46] replace the reconstruction loss with an adversarial loss of the GAN framework.  introduces a feature-based loss that is calculated with a pre-trained convolutional neural network (CNN). Another approach by  adopts Watson's perceptual model, and  directly optimizes the generative model in the frequency domain by a focal frequency reconstruction loss. Most of the above-mentioned methods aim to capture the properties of human perception by replacing the element-wise loss (\(L_{1}\) or \(L_{2}\)-norm), which hinders the reconstruction of images .

**Synthetic data generation.** The GAN framework is widely adopted in the synthetic data generation task since it enables synthetic data generation in a nonparametric approach [12; 47; 63; 65]. [63; 65] assume that continuous columns of tabular datasets can be approximated by the Gaussian mixture distribution and model their decoder using Gaussian mixture distribution. Additionally, [63; 65] preprocess the continuous variables using the variational Gaussian mixture model , which is known as the mode-specific normalization technique. However, the preprocessing step requires additional computational resources and hyperparameter tuning of the number of modes. Other approaches by [47; 65] regularize the discrepancy between the first and second-order statistics of the observed and synthetic dataset.  proposes the GAN-based synthesizer, which focuses on generating high-dimensional discrete variables with the assistance of the pre-trained AutoEncoder.

## 3 Proposal

Let \(^{p+q}\) be an observation consisting of continuous and discrete variables and \(I=I_{C} I_{D}=\{1,,(p+q)\}\) be an index set of the variables, where \(I_{C}\) and \(I_{D}\) correspond to index sets of \(p\) continuous and \(q\) discrete variables. \(T_{j}\) denotes the number of levels for the discrete variables \(_{j},j I_{D}\). We denote the ground-truth underlying distribution (probability density function, PDF) as \(p()\) and the ground-truth CDF as \(F()\).

Let \(\) be a latent variable, where \(^{d}\) and \(d<p+q\). The prior and posterior distribution of \(\) are assumed to be \(p()=(|,)\) and \(q(|;)=|(; ),diag(^{2}(;))\), respectively. Here, \(\) is \(d d\) identity matrix, \(:^{p+q}^{d}\), \(^{2}:^{p+q}^{d}_{+}\) are neural networks parameterized with \(\), and \(diag(a),a^{d}\) denotes a diagonal matrix with diagonal elements \(a\). Moreover, we consider \(\) as a random variable having density \(p()\).

### Distributional Learning

Our proposed model assumes that \(p()\) is parametrized by an infinite mixture of asymmetric Laplace distribution (ALD) . The ALD is characterized by two parameters: \(\), representing the asymmetry, and \(>0\), representing the scale. By considering these parameters, along with the model parameter \(\), we can define the probability model of \(\) as follows:

\[p(;,)=_{0}^{1}p(|,; ,)p(,)d d.\]

**Assumption 1**.: _(1) \(\{_{j}\}_{j I}\) are conditionally independent given \(\). (2) The discrete random variables \(\{_{j}\}_{j I_{D}}\) are independent of \(\). (3) \(\) and \(\) are independent._

By Assumption 1-(1), we model the dependency between \(_{j}\)s solely through the latent variable \(\). Assumption 1-(2) implies that \(\) is related only to the continuous variables. Then, the decoder of our VAE model denoted as \(p(|,;,)\) is specified by equation (1):

\[p(|,;,) = _{j I_{C}}p(_{j}|,;_{j}, )_{j I_{D}}p(_{j}|;_{j})\] \[= _{j I_{C}}(-_{ }(_{j}-D_{j}(,;_{j})}{} ))_{j I_{D}}_{l=1}^{T_{j}}_{l}(; _{j})^{(_{j}=l)},\]

where \(=(_{1},,_{p+q})\), \(\) is a non-trainable constant, \(_{}(u)=u(v-(u<0))\) (check function), and \(()\) denotes the indicator function. \(D_{j}(,;_{j}):^{d}\) is the location parameter of ALD, which is parameterized with \(_{j}\). For discrete variables, \((;_{j}):^{d}^{T_{j}-1}\) is a neural network parameterized with \(_{j}\), where \(^{T_{j}-1}\) is the standard \((T_{j}-1)\)-simplex for all \(^{d}\), and the subscript \(l\) referes to the \(l\)th element of the output \(\).

Assumption 1-(3) leads our objective function,

\[_{,} _{p()}_{q(|; )}[_{j I_{C}}_{0}^{1}_{}_{j}-D _{j}(,;_{j})d-_{j I_{D}}_{l=1}^{ T_{j}}(_{j}=l)_{l}(;_{j})]\] (2) \[+ _{p()}[(q(| ;)\|p())],\]

where constant terms are omitted. In order to achieve balanced learning of the two reconstruction losses in (2), we have removed the weight \(\) associated with the second reconstruction loss. We refer to our model as 'DistVAE.' The first term in (2) corresponds to the CRPS loss, which measures the accuracy of the proposed CDF approximation with respect to the ground-truth CDF of the underlying distribution [21; 43; 20].

Interestingly, (2) is the limit of the negative ELBO derived from a finite mixture of ALD. We introduce \(\), a discrete uniform random variable taking values on \(_{k}=k/K\) for \(k=1,,K\). Then, the negative ELBO of \(p(;,)\) scaled with \(\) is written by

\[_{q(|;)}[_{j I_{C}} _{k=1}^{K}_{_{k}}_{j}-D_{j}(_{ k},;_{j})]-_{k=1}^{K}_{k}(1- _{k})+ p\] (3) \[- _{q(|;)}[_{j  I_{D}}_{l=1}^{T_{j}}(_{j}=l)_{l}( ;_{j})]+(q(|; )\|p())\](see Appendix A.1 for detailed derivation). The reconstruction loss, which corresponds to the first term of (3), is a composite quantile loss for estimating the target quantiles \(_{k}\)s [64; 35; 45; 62; 9]. This entails adopting a Bayesian perspective for \(\) as a prior (the Bayesian modeling for estimating multiple quantiles). Furthermore, throughout the derivation of the reconstruction loss, the role of \(\) is pivotal in ensuring the representation of the reconstruction loss. To prevent the observation \(\) from influencing the distribution of \(\), \(\) is only assigned with a prior distribution, and the resulting reconstruction loss becomes the CRPS loss, a proper scoring rule.

However, for distributional learning of VAE, it is necessary to estimate conditional quantiles for an infinite number of quantile levels, denoted by \(K\). The subsequent Theorem 1 establishes the convergence of the negative ELBO (3) to our objective function (2) as \(K\).

**Theorem 1**.: _For all \(j I_{C}\), suppose that \(_{0}^{1}_{p()}_{q(|; )}_{}_{j}-D_{j}(,;_{j}) d<\), and \(_{p()}_{q(|;)}[_{ }(_{j}-D_{j}(,;_{j}))]\) is continuous over \((0,1)\). Then,_

\[_{K}_{p()}_{q(| ;)}[_{k=1}^{K}_{_{k}} _{j}-D_{j}(_{k},;_{j})]=_{p()}_{q(|;)}[_{0}^{1} _{}_{j}-D_{j}(,;_{j}) d],\]

_and \(_{K}_{k=1}^{K}_{k}(1-_{k})= _{0}^{1}(1-)d=-2\)._

### Theoretical Results

In this section, we aim to provide theoretical insights into the ability of DistVAE, utilizing the objective function (2), to recover the ground-truth distribution \(p()\). To simplify the analysis without loss of generality, we consider the scenario where \(\) comprises only \(p\) continuous random variables. Hence, we have \(I=I_{C}=\{1,,p\}\), and \(p()\) is defined over \(^{p}\) with \(p()>0\) for all \(^{p}\). First, define a function \(q(|;)\) by

\[q(|;))q(|;)}{q(;)},\]

where \(q(;) q(|;)p()d \) is the aggregated posterior . Clearly, \(q(|;)\) is a PDF of \(\) for a given \(\). \(q(|;)\) is a conditional PDF of \(\) parametrized by \(\) and it is an approximated PDF of \(_{0}^{1}p(|,;,)d\). Since we assume that \(_{j}\)s are conditionally independent in Assumption 1, \(q(|;)=_{j=1}^{p}q_{j}(_{j}|;)\) and the conditional CDF is written as

\[F(|;)=_{j=1}^{p}F_{j}(_{j}|; ),\ F_{j}(_{j}|;)_{-}^{ _{j}}q_{j}(x|;)dx.\] (4)

For notational simplicity, we let

\[^{*}()_{}_{p()q(| ;)}_{j=1}^{p}_{0}^{1}_{}(_{j}-D_{j} (,;_{j}))d,\]

where \(^{*}()=(_{1}^{*}(),,_{p}^{*}())\).

**Assumption 2**.: _(1) Given an arbitrary \(\), \(F_{j}(|;):\) is absolutely continuous and strictly monotone increasing for all \(j=1,,p\), and \(^{d}\). (2) Given an arbitrary \(\), \(D_{j}(,;_{j})\) is invertible and differentiable for all \(j=1,,p\) and \(^{d}\). (3) The aggregated posterior \(q(;)\) is absolutely continuous to the prior distribution of \(\)._

**Theorem 2**.: _Under Assumption 2, for an arbitrary \(\),_

\[(p()\|_{j=1}^{p} _{j}}D_{j}^{-1}(_{j},;_{j}^{*}())q( ;)d)=0.\]

Theorem 2 shows that DistVAE is capable of recovering the ground-truth distribution \(p()\), indicating its ability to facilitate distributional learning rather than data reconstruction. Nevertheless, relying on the aggregated posterior distribution may lead to overfitting [25; 42], and sampling from the aggregated posterior can introduce computational challenges due to the absence of a straightforward closed-form representation for \(q(;)\). To address these concerns, we propose an alternative approach that leverages the prior distribution \(p()\) instead of \(q(;)\), thereby enabling a computationally efficient synthetic generation process. This is substantiated by Theorem 3.

We define the estimated PDF \((;^{*}())\) and CDF \((;^{*}())\) as

\[(;^{*}())  _{j=1}^{p}_{j}}D_{j}^{-1}(_{j},;_{j}^{*}())p()d\] (5) \[(;^{*}())  _{j=1}^{p}D_{j}^{-1}(_{j},;_{ j}^{*}())p()d.\] (6)

**Theorem 3**.: _Suppose that \(\) is given such that \((q(;)\|p())<\) for any \(>0\). Then, under Assumption 2,_

\[(p()_{j=1}^{p}_{j}}D_{j}^{-1}(_{j},;_{j}^{*}())p()d )<.\]

Theorem 3 shows that even if we use the prior distribution \(p()\) instead of the aggregated posterior \(q(;)\), it is feasible to minimize the KL-divergence between the ground-truth PDF \(p()\) and our estimated PDF \((;^{*}())\) of (5). This is achievable because the KL-divergence term in (2) is the upper bound of \((q(;)\|p())\) and is minimized during the training process. Since each conditional distribution of the estimated PDF depends on the same latent variable, it can be seen that the correlation structure between covariates is modeled implicitly.

 have highlighted the role of the KL-divergence weight parameter \(\) in controlling the precision of reconstruction. In our case, where the reconstruction loss is based on the CRPS loss, an increase in \(\) leads to a less accurate estimation of the ground-truth CDF. It implies that a large \(\) corresponds to lower-quality synthetic data, but it also enhances privacy level. Thus, \(\) introduces a trade-off between the quality of synthetic data and the risk of privacy leakage. The privacy level can be adjusted by manipulating \(\), as demonstrated in the experimental results presented in Section 4.

#### 3.2.1 Synthetic Data Generation

By estimating the conditional quantile functions, we can transform the synthetic data generation process into inverse transform sampling. This conversion offers a notable advantage as it provides a straightforward and efficient approach to generating synthetic data. We denote a synthetic sample of \(_{j}\) as \(_{j}\) for \(j I\), and the synthetic data generation process can be summarized as follows:

1. Sampling from the prior distribution: \(z p()\).
2. Inverse transform sampling: For \(j I_{C}\), \(_{j}=D_{j}(u_{j}|z;_{j})\), where \(u_{j} U(0,1)\).
3. Gumbel-Max trick : For \(j I_{D}\), \(_{j}=_{l=1,,T_{j}}\{_{l}(z;_{j})+G_{l}\}\), where \(G_{l} Gumbel(0,1)\), and \(l=1,,T_{j}\).

Note that both continuous and discrete variables share the same latent variable \(z\). This shared latent variable allows for capturing the dependencies between variables. We numerically observe that the sampling, implemented using the Gumbel-Max trick, maintains the imbalanced ratio of the labels in the discrete variable.

#### 3.2.2 Parameterization of ALD

As introduced in , for \(j I_{C}\), we parameterize the function \(D_{j}\), the location parameter of ALD, by a linear isotonic spline as follows:

\[D_{j}(,;_{j})=^{(j)}()+_{m=0}^{M}b_{ m}^{(j)}()(-d_{m})_{+}_{m=0}^{k}b_{m}^{(j)}( ) 0,k=1,,M,\] (7)where \(^{(j)}()\), \(b^{(j)}()=(b_{0}^{(j)}(),,b_{M}^{(j)}()) ^{M+1}\), \(d=(d_{0},,d_{M})^{M+1}\), \(0=d_{0}<<d_{M}=1\), and \((u)_{+}(0,u)\). \(_{j}\) is a neural network parameterized mapping such that \(_{j}:^{d}^{M+1}\), which takes \(\) as input and outputs \(^{(j)}()\) and \(b^{(j)}()\). The constraint is introduced to ensure monotonicity. As demonstrated in , the reconstruction loss can be computed in a closed form by utilizing the parameterization of (7) (refer to Appendix A.5 for a detailed description of the loss function). This implies that our objective function (2) is computationally tractable. Note that the linear isotonic spline is not differentiable for finite points where the measure has no point mass.

## 4 Experiments

### Overview

**Dataset.** For evaluation, we consider following real tabular datasets: covertype, credit, loan, adult, cabs, and kings (see Appendix A.8 for detailed data descriptions). We treat the ordinal variables as continuous variables and discretize the estimated CDF (see Appendix A.6 for the discretization algorithm). Synthetic samples of ordinal variables are rounded to the first decimal place.

**Compared models.** We compare DistVAE2 with the state-of-the-art synthesizers; CTGAN , TVAE , and CTAB-GAN . All models have the same size of the latent dimension (\(d=2\)). The chosen latent space indeed limits the capacity of decoders for all models. However, we maintain a small and consistent number of parameters across all models during the experiment to isolate the performance differences in synthetic data generation to the methodologies of each synthesizer, specifically emphasizing the contribution of the decoder model's flexibility in estimating underlying distributions (see Table 10 in Appendix A.9 for a comprehensive comparison of the model parameters).

### Evaluation Metrics

To assess the quality of the synthetic data, we employ three types of assessment criteria: 1) machine learning utility, 2) statistical similarity, and 3) privacy preservability. Each criterion is evaluated using multiple metrics, and the performance of synthesizers is reported by averaged metrics over the real tabular datasets. The synthetic dataset is generated to have an equal number of samples as the real training dataset.

**Machine learning utility.** The machine learning utility (MLu) is measured by the predictive performance of the trained model over the synthetic data. We consider three popular machine learning algorithms: linear (logistic) regression, Random Forest , and Gradient Boosting . We measure the performance by utilizing the mean absolute relative error (MARE) for regression tasks  and the \(F_{1}\) score for classification tasks [63; 65; 61; 29; 47; 12; 18].

**Statistical similarity.** The marginal distributional similarity between the real training and synthetic datasets is evaluated using two metrics: the Kolmogorov statistic and the 1-Wasserstein distance . These metrics measure the distance between the empirical marginal CDFs . The joint distributional similarity is assessed by comparing the correlation matrices . To compute the correlation matrix and measure the \(L_{2}\) distance between the correlation matrices of the real training and synthetic datasets, we employ the dytthon library 3. These enable a comprehensive evaluation of both marginal and joint distributional similarities between the real training and synthetic datasets.

**Privacy preservability.** The privacy-preserving capacity is measured using three metrics: the _distance to closest record_ (DCR) [47; 65], _membership inference attack_[54; 12; 47], and _attribute disclosure_[12; 44]. As in , the DCR is defined as the \(5^{th}\) percentile of the \(L_{2}\) distances between all real training samples and synthetic samples. Since the DCR is a \(L_{2}\) distance-based metric, it is computed using only continuous variables. A higher DCR value indicates a more effective preservation of privacy, indicating a lack of overlap between the real training data and the synthetic samples. Conversely, an excessively large DCR score suggests a lower quality of the generated synthetic dataset. Therefore, the DCR metric provides insights into both the privacy-preserving capability and the quality of the synthetic dataset.

The membership inference attack evaluation follows the steps detailed in Appendix A.7. The procedure is customized to be applied to a VAE-based synthesizer, such as DistVAE and TVAE. By transforming the problem into a binary classification task, we aim to identify the intricate relationship between the real training data and the synthetic samples. Higher binary classification scores indicate a higher vulnerability of the target synthesizer to membership inference attacks.

Attribute disclosure refers to the situation where attackers can uncover additional covariates of a record by leveraging a subset of covariates they already possess, along with similar records from the synthetic dataset. To quantify the extent to which attackers can accurately identify these additional covariates, we employ classification metrics. Higher attribute disclosure metrics indicate an increased risk of privacy leakage, implying that attackers can precisely infer unknown variables. In terms of privacy concerns, attribute disclosure can be considered a more significant issue than membership inference attacks, as attackers are assumed to have access to only a subset of covariates for a given record .

### Results

**Machine learning utility.** We expect a high-quality synthesizer to generate synthetic data with comparable predictive performance to the real training dataset, denoted as the 'Baseline' in Table 1. The results in Table 1 demonstrate that DistVAE achieves a competitive MARE score and outperforms other methods in terms of the \(F_{1}\) score. Furthermore, the performance of MLu improves as the value of \(\) decreases, indicating that the quality of the generated synthetic data is controlled by \(\). For a comprehensive overview of the MLu scores for all tabular datasets, please refer to Appendix A.10.

**Statistical similarity.** The evaluation results for joint and marginal distributional similarities are presented in Table 2 and 3. In Table 2, DistVAE achieves the lowest CorrDist score, indicating its ability to accurately preserve the correlation structure of the real training dataset in the generated synthetic dataset. Furthermore, DistVAE surpasses other methods in Table 3 when it comes to marginal distributional similarity, suggesting that it successfully captures the underlying distribution of the observed dataset. Notably, reducing the value of \(\) leads to an enhancement in the quality of the synthetic dataset, as evidenced by improvements in the correlation structure and similarity of the marginal distributions. Figure 1 provides visualizations of the estimated CDFs (6) for each continuous (or ordinal) variable in the cabs dataset. For detailed statistical similarity scores and additional visualizations of estimated CDFs for all tabular datasets, please refer to Appendix A.10.

**Privacy preservability.** The privacy preservability performances of synthesizers, as measured by the DCR, are presented in Table 4. DistVAE performs best in preserving privacy, with the highest DCR values compared to other methods. Notably, as the value of \(\) increases in DistVAE, the DCR between the real training and synthetic datasets (R&S) also increases. This indicates that the risk of privacy leakage can be controlled by adjusting \(\), where higher values of \(\) correspond to a higher level of privacy protection. Moreover, DistVAE consistently achieves large DCR values for the synthetic dataset (S) across all \(\) values, indicating its ability to generate diverse synthetic samples. On the other hand, CTAB-GAN generates duplicated records in the synthetic dataset, resulting in

   Model & MARE \(\) & \(F_{1}\) \\  Baseline & \(0.150_{ 0.200}\) & \(0.814_{ 0.101}\) \\ CTGAN & \(0.321_{ 0.271}\) & \(0.672_{ 0.234}\) \\ TVAE & **0.225**\( 0.215\) & \(0.594_{ 0.295}\) \\ CTAB-GAN & \(0.403_{ 0.392}\) & \(0.702_{ 0.162}\) \\  DistVAE(\(=0.5\)) & \(0.349_{ 0.328}\) & **0.769**\( 0.128\) \\ DistVAE(\(=1\)) & \(0.344_{ 0.316}\) & **0.762**\( 0.134\) \\ DistVAE(\(=5\)) & \(0.392_{ 0.348}\) & \(0.679_{ 0.190}\) \\   

Table 1: Averaged MLu metrics (MARE, \(F_{1}\)). Mean and standard deviation values are obtained from 10 repeated experiments. \(\) denotes higher is better and \(\) denotes lower is better.

   Model & CorrDist \\  CTGAN & \(2.180_{ 0.467}\) \\ TVAE & \(2.739_{ 0.796}\) \\ CTAB-GAN & \(2.575_{ 0.513}\) \\  DistVAE(\(=0.5\)) & **1.473**\( 0.398\) \\ DistVAE(\(=1\)) & \(1.730_{ 0.548}\) \\ DistVAE(\(=5\)) & \(3.113_{ 1.119}\) \\   

Table 2: Averaged correlation structural similarity. ‘CorrDist’ represents \(L_{2}\) distance between the correlation matrices of synthetic and real training datasets. Mean and standard deviation values are obtained from 10 repeated experiments. Lower is better.

relatively lower DCR scores for the synthetic dataset (S). For detailed DCR scores for all tabular datasets, please refer to Appendix A.10.

To evaluate the membership inference attack, we prepare one attack model per class. The attack testing records comprise an equal number of real training and test records, distinguished by the labels \(in\) and \(out\), respectively. Note that the test records are not employed in constructing the attack models. We employ gradient-boosting classifiers as the attack models, and for computational feasibility, we limit the number of attack models to one (i.e., \(C=1\)).

For the membership inference attack evaluation, we utilize accuracy and AUC (Area Under Curve) as the evaluation metrics. Since the target labels (\(in/out\)) are balanced, and the task is a binary classification problem, these metrics are appropriate. The results presented in Table 5 reveal that both DistVAE and TVAE achieve nearly identical accuracy and AUC scores of 0.5. This indicates that the attack models can _not_ distinguish between members of the real training and test datasets. Consequently, the membership inference attack is unsuccessful for both models. Therefore, DistVAE effectively generates synthetic datasets while ensuring privacy against membership inference attacks. A comprehensive assessment of membership inference attack performances for all tabular datasets can be found in Appendix A.10.

   Model & R\&S & R & S \\  CTGAN & \(0.426_{ 0.229}\) & \(0.237_{ 0.153}\) & \(0.356_{ 0.202}\) \\ TVAE & \(0.470_{ 0.181}\) & \(0.237_{ 0.153}\) & \(0.278_{ 0.195}\) \\ CTAB-GAN & \(0.508_{ 0.259}\) & \(0.237_{ 0.153}\) & \(0.039_{ 0.073}\) \\  DistVAE(\(=0.5\)) & \(0.444_{ 0.250}\) & \(0.237_{ 0.153}\) & \(0.463_{ 0.288}\) \\ DistVAE(\(=1\)) & \(0.463_{ 0.282}\) & \(0.237_{ 0.153}\) & \(0.479_{ 0.310}\) \\ DistVAE(\(=5\)) & \(_{ 0.272}\) & \(0.237_{ 0.153}\) & \(_{ 0.335}\) \\   

Table 4: Privacy preservability: Averaged distance to closest record (DCR) between real training and synthetic datasets (R&S), between the same real training datasets (R), and between the same synthetic datasets (S). Mean and standard deviation values are obtained from 10 repeated experiments. The DCR (R) score represents the baseline diversity of datasets. Higher is better.

Figure 1: cabs dataset. Empirical (solid orange) and estimated (dashed blue) CDFs of continuous and ordinal variables (Monte Carlo approximated with 5000 samples). We standardize covariates and remove observations outside the 1% and 99% percentile range.

    &  &  \\  Model & K-S & 1-WD & K-S & 1-WD \\  CTGAN & \(0.133_{ 0.106}\) & \(0.087_{ 0.025}\) & \(0.168_{ 0.195}\) & \(0.521_{ 0.532}\) \\ TVAE & \(0.196_{ 0.135}\) & \(0.220_{ 0.099}\) & \(0.385_{ 0.144}\) & \(1.681_{ 1.668}\) \\ CTAB-GAN & \(0.157_{ 0.089}\) & \(0.130_{ 0.037}\) & \(0.106_{ 0.083}\) & \(0.412_{ 0.378}\) \\  DistVAE(\(=0.5\)) & \(0.090_{ 0.065}\) & \(_{ 0.026}\) & \(0.030_{ 0.017}\) & \(_{ 0.100}\) \\ DistVAE(\(=1\)) & \(_{ 0.039}\) & \(0.083_{ 0.019}\) & \(_{ 0.021}\) & \(_{ 0.110}\) \\ DistVAE(\(=5\)) & \(0.092_{ 0.037}\) & \(0.121_{ 0.058}\) & \(0.059_{ 0.034}\) & \(0.241_{ 0.163}\) \\   

Table 3: Averaged marginal distributional similarity. K-S denotes the Kolmogorov-Smirnov statistic, and 1-WD represents the 1-Wasserstein distance. Mean and standard deviation values are obtained from 10 repeated experiments. Lower is better.

We present the attribute disclosure performance results in Table 6. For each value of \(k\), we observe that as \(\) increases, the \(F_{1}\) score of DistVAE decreases. Also, DistVAE achieves the smallest \(F_{1}\) score when \(k\) equals 10 and 100. Based on these results, we can conclude that DistVAE can generate synthetic datasets with a low risk of attribute disclosure, and the level of privacy preservation is controlled by \(\). Please refer to Appendix A.10 for a detailed evaluation of attribute disclosure performance for all tabular datasets.

**Quantile estimation.** To investigate the quantile estimation performance, we also evaluate DistVAE using the Vrate(\(\)) metric . The Vrate(\(\)) is defined as \(|}_{i I_{test}}I(x_{i}<_{})\), where \((0,1)\), \(I_{test}\) is the set of indices for the test dataset, \(x_{i}\) is the \(i\)-th sample in the test dataset, and \(_{}\) is the empirical \(\)-quantile of the synthetic data. Since Vrate(\(\)) indicates the proportion of compliance samples in the test dataset, the Vrate(\(\)) score should be close to \(\).

The Vrate(\(\)) evaluation results are presented in Table 7. Table 7 shows that the ratio of violated test samples (\(|-()|\)) decreases as \(\) increases, indicating a better performance for estimating larger quantiles. However, the ratio of violated test samples is relatively large for smaller \(\) values, which may be due to extremely skewed continuous variables, such as capital-gain and capital-loss from adult dataset, that make the quantile estimation unstable.

## 5 Conclusion and Limitations

This paper introduces a novel distributional learning method of VAE, which aims to effectively capture the underlying distribution of the observed dataset using a nonparametric approach. Our proposed method involves directly estimating CDFs using the CRPS loss while maintaining the mathematical derivation of the lower bound.

In our study, we confirm that the proposed decoder enhances the performance of generative models for tabular data. However, this conclusion relies on the assumption of conditional independence among the observed variables given the latent variable. When the dimension of the latent variable is small, this assumption is prone to violation. Therefore, in cases where the size of the latent

    \\  Model & 1 & 10 & 100 \\  CTGAN & \(0.262_{ 0.091}\) & \(0.282_{ 0.087}\) & \(0.275_{ 0.087}\) \\ TVAE & \(0.437_{ 0.162}\) & \(0.438_{ 0.160}\) & \(0.432_{ 0.162}\) \\ CTAB-GAN & **0.257\({}_{ 0.123}\)** & \(0.258_{ 0.114}\) & \(0.261_{ 0.111}\) \\  DistVAE(\(=0.5\)) & \(0.328_{ 0.088}\) & \(0.328_{ 0.076}\) & \(0.310_{ 0.072}\) \\ DistVAE(\(=1\)) & \(0.307_{ 0.073}\) & \(0.313_{ 0.068}\) & \(0.297_{ 0.066}\) \\ DistVAE(\(=5\)) & \(0.265_{ 0.105}\) & **0.253\({}_{ 0.103}\)** & **0.232\({}_{ 0.101}\)** \\   

Table 6: Privacy preservability: Averaged attribute disclosure performance with the \(F_{1}\) score. Mean and standard deviation values are obtained from 10 repeated experiments. Lower is better.

   \(\) & 0.1 & 0.3 & 0.5 & 0.7 & 0.9 \\  Vrate(\(\)) & 0.204 & 0.373 & 0.533 & 0.725 & 0.908 \\ \(|-()|\) & 0.104 & 0.083 & 0.04 & 0.032 & 0.008 \\   

Table 7: Averaged Vrate(\(\)) and \(|-()|\).

space is limited, the proposed nonparametric fitting of the decoder might not accurately represent the underlying distribution. Particularly in the image domain, where adjacent pixel values exhibit significant dependence, it remains uncertain whether our proposed model would lead to notable improvements in image data generation with a low-dimensional latent space.

Nevertheless, classical image datasets, such as CIFAR-10 , often exhibit pixel value distributions that deviate considerably from Gaussian, with the frequencies of edge values (0 and 255) dominating more than other pixel values . Other image datasets, as presented in , demonstrate multi-modality in pixel value distributions. These experimental findings suggest that leveraging the capacity of distributional learning could be advantageous in approximating the ground-truth distribution of image data when the latent variable effectively captures conditional independence among the image pixels. Consequently, we expect that compromising biases arising from violating conditional independence and marginally misspecified distributions may further enhance our results, and we leave it for future research.

On the other hand, we consider two approaches to enhance the performance of quantile estimation. Firstly, we plan to extend the parameterization of conditional quantile functions to a more flexible monotonic regression model. Secondly, we intend to incorporate the Uniform Pessimistic Risk (UPR)  into the VAE framework to handle lower quantile levels better. Furthermore, we are exploring the expansion of DistVAE into a time-series distributional forecasting model by adopting the conditional VAE framework . This extension will enable the application of our method to time-series data, opening new avenues for distributional forecasting.

This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. NRF-2022R1A4A3033874 and No. NRF-2022R1F1A1074758). This work was also supported by Korea Environmental Industry & Technology Institute (KEITI) through 'Core Technology Development Project for Environmental Diseases Prevention and Management', funded by Korea Ministry of Environment (MOE) (2021003310005). The authors acknowledge the Urban Big data and AI Institute of the University of Seoul supercomputing resources (http://ubai.uos.ac.kr) made available for conducting the research reported in this paper.