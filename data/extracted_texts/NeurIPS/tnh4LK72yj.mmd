# Get Rid of Isolation: A Continuous Multi-task Spatio-Temporal Learning Framework

Zhongchao Yi\({}^{1}\), Zhengyang Zhou\({}^{1,2,3,}\), Qihe Huang\({}^{1}\), Yanjiang Chen\({}^{1}\),

**Liheng Yu\({}^{1}\), Xu Wang\({}^{1,2}\), Yang Wang\({}^{1,2,}\)**

\({}^{1}\)University of Science and Technology of China (USTC), Hefei, China

\({}^{2}\)Suzhou Institute for Advanced Research, USTC, Suzhou, China

\({}^{3}\)State Key Laboratory of Resources and Environmental Information System, Beijing, China

{zhongchaoyi, hqh, yjchen, yuliheng}@mail.ustc.edu.cn,

{wx309, zzy0929, angyan*}@ustc.edu.cn

Yang Wang and Zhengyang Zhou are corresponding authors.

###### Abstract

Spatiotemporal learning has become a pivotal technique to enable urban intelligence. Traditional spatiotemporal models mostly focus on a specific task by assuming a same distribution between training and testing sets. However, given that urban systems are usually dynamic, multi-sourced with imbalanced data distributions, current specific task-specific models fail to generalize to new urban conditions and adapt to new domains without explicitly modeling interdependencies across various dimensions and types of urban data. To this end, we argue that there is an essential to propose a Continuous Multi-task Spatio-Temporal learning framework (CMuST) to empower collective urban intelligence, which reforms the urban spatiotemporal learning from single-domain to cooperatively multi-dimensional and multi-task learning. Specifically, CMuST proposes a new multi-dimensional spatiotemporal interaction network (MSTI) to allow cross-interactions between context and main observations as well as self-interactions within spatial and temporal aspects to be exposed, which is also the core for capturing task-level commonality and personalization. To ensure continuous task learning, a novel Rolling Adaptation training scheme (RoAA) is devised, which not only preserves task uniqueness by constructing data summarization-driven task prompts, but also harnesses correlated patterns among tasks by iterative model behavior modeling. We further establish a benchmark of three cities for multi-task spatiotemporal learning, and empirically demonstrate the superiority of CMuST via extensive evaluations on these datasets. The impressive improvements on both few-shot streaming data and new domain tasks against existing SOAT methods are achieved. Code is available at https://github.com/DILab-USTCSZ/CMuST.

## 1 Introduction

Spatiotemporal learning has become a pivotal technique to enable smart and convenient urban lives, benefiting diverse urban applications from intra-city travelling, environment controlling to location-based POI recommendation, and injecting the vitality into urban economics. Existing spatiotemporal learning solutions [19; 9; 38; 40; 51; 42; 37; 10; 8] focus on improving performances of a task-specific model independently where these methods devise various spatial learning blocks [44; 8; 43] and temporal dependency extraction modules [40; 34; 9] to model the spatiotemporal heterogeneity.

Actually, urban spatiotemporal systems are usually highly dynamic with emerging new data modality, leading to serious generalization issue on both data pattern and task adaptation. As illustrated inFigure 1, the traffic volume patterns can evolve with urban expansion and establishment of new POIs. Concurrently, with increasing attention on road safety, traffic accident prediction has become a new task in intelligent transportation that inevitably suffers from the cold-start issue. Unfortunately, traditional task-specific models usually assume that data on a single task follows independent and identical distribution and are intensively available where such assumption directly leads to failures on data sparsity scenarios and generalization to new tasks. In fact, given diverse datasets, separately training single task-specific spatiotemporal models is cost sensitive and will trap the models into isolation. To this end, we argue that a continuous multi-task spatiotemporal learning framework is highly desirable to facilitate the task-level cooperation. It is even more interesting and exciting to jointly model multi-domain datasets with multi-task learning, which empowers understanding spatiotemporal system in a holistic perspective and reinforce each individual task by exploiting the collective intelligence from diverse data domains.

The key towards exploiting task-wise correlations for mutual improvement is to capture the common interdependencies across data dimensions and domains. Current multi-task learning schemes either investigate the regularization effects between auxiliary and main tasks , or devise loss objectives to constrain the consistency between each task . Actually, given a spatiotemporal domain, there must be common interdependencies across different data types and domains, which are valuable for cooperated learning. Even prosperity of multi-task learning and spatiotemporal forecasting, there are never a systematic solution on how various sourced data from different tasks reinforce a specific task with multi-task learning. More specifically, interrelations among observations in a system can be decomposed into multi-dimensional interactions, i.e., from contextual environment to respective spatial relations and temporal evolutions, spatial-temporal level interactions and relations across different data domains. Considering the multi-level semantic correlations, learning the system in a holistic perspective is required and further pose two challenges to a continuous multi-task spatiotemporal learning framework, i.e., 1) How to disentangle complex associations between data dimensions and domains, and capture such dependencies in an adaptive manner to improve spatiotemporal representation, hence facilitating the extraction of common patterns for mutual enhancement. 2) How to exploit task-level commonality and personality to jointly model the multi-task datasets, and exploit such extracted task-level commonality and diversity to reinforce respective task for getting rid of task isolation.

In our work, a Continuous Multi-task SpatioTemporal learning framework, CMuST, is proposed to jointly model multiple datasets in an integrated urban system thus reinforcing respective learning tasks. Specifically, a Multi-dimensional Spatio-Temporal Interaction Network (MSTI) is first devised to dissect interactions across data dimensions, including context-spatial, context-temporal and self-interaction within spatial and temporal dimensions. MSTI enables improved spatiotemporal representation with interactions, and also provides disentangled patterns to support commonality extraction. After that, a Rolling Adaptation training scheme, RoAda, which iteratively captures the task-wise consistency and task-specific diversity, is proposed. In RoAda, to maintain task characterization, a task-specific prompt is constructed to preserve unique patterns distinguishing from other tasks by compressing data patterns via an AutoEncoder. To capture the commonality across tasks, we propose a weight behavior modeling strategy to iteratively highlight the minimized variations of learnable weights, i.e., stable interactions during continuous training, which encapsulates crucial task-level commonalities. This approach not only stabilizes learning through continuous task rolling, but alleviates the cold-start over new tasks with shared patterns. Finally, a task-specific refinement is devised to leverage commonality and fine-grained adaptation on specific tasks.

The contributions of this work can be three-fold. 1) The first continuous multi-task spatiotemporal learning framework, CMuST to jointly model learning tasks in a same spatiotemporal domain, which not only reinforces individual correlated learning task in collective perspective, but also help understand the cooperative mechanism of dynamic spatiotemporal systems. 2) Technically, two learning modules, MSTI and RoAda are proposed to dissect the impacts and interactions over multi-dimensions, and iteratively update the task-wise commonality and generate individual personalization to continuous task adaptation in multi-task learning. 3) We construct benchmark datasets in each of

Figure 1: Illustration of evolution on data patterns and learning tasks.

three cities, where two of them consists of at least 3 types of observations within same spatiotemporal domain. The extensive experiments demonstrate the superiority on enhancement of each individual task with limited data and the interpretation of task-wise continuous learning.

## 2 Related Work

**Spatiotemporal forecasting** is an emerging technique to capture the dynamic spatial and temporal evolution for diverse urban predictions, where the methods can be divided into machine learning-based and deep learning-based. Conventional solutions rely on complex mathematical tools to simulate the dynamics including ARIMA , SVR  and matrix-factorization learning  for capturing spatial correlations. With deep learning solutions flourishing, Convolution Neural Networks (CNNs) [40; 11; 46] are exploited to imitate the temporal dependencies and GNNs [43; 13; 36] are utilized to imitate spatial propagation. Meanwhile, by taking the advantage of the flexibility and interpretability of attention, Spatial-Temporal Attention , and Vision Transformer (ViT)  are introduced to improve spatiotemporal representation. Also there are also many methods for temporal periodicity capturing [12; 20]. More specifically, DG2RNN  designs a dual-graph convolutional module to capture local spatial dependencies from both road distance and adaptive correlation perspectives. PDFormer  designs a spatial self-attention and introduces two graph masking matrices to highlight the spatial dependencies of short- and long-range views. TESTAM  uses time-enhanced ST attention by mixture-of-experts and modeling both static and dynamic graphs. Even so, most solutions focus on single-task intelligence, fail to deal with complex interactions between data dimensions and never extract task-level commonality patterns, resulting in inferior performances on exploiting collective intelligence over multiple tasks. In contrast, we merge the gap by disentangling learnable interaction patterns and exploring rolling task adaptations.

**Multi-task learning.** Plenty of efforts have been made on multi-task learning (MTL) and MTL can be elaborated by two-fold, i.e., feature-based and parameter-based. Feature-based MTL [2; 24] learns a common feature representation for different tasks, but it may be easily affected by outlier tasks. To this end, parameter-based MTL [30; 16] is devised to exploit model parameters to relate different tasks, which is expected to learn robust parameters. Majority of these MTL schemes either concentrate on the diversity design and regularization effects of auxiliary tasks to main task [30; 27], or construct loss functions to ensure task-wise consistency . A pioneering work investigates a gradient-driven task grouping to realize multi-task learning  where the focuses are text and images using pre-trained CLIP model. For spatiotemporal learning, RiskOracle  and RiskSeq  are proposed to simultaneously predict multi-grained risks and auxiliary traffic elements. More recently, UniST  and UniTime  construct a unified model for spatiotemporal and time-series prediction. However, all researches on ST learning still never dissect task-wise correlations, especially capture explicit consistency and diversity among tasks and investigate how each task reinforce the core task, which is of great significance for performance and interpretability in MTL.

**Continuous learning and task continuous learning.** Continuous learning (CL) usually keeps long-term and important information while updates model memories with newly arrived instances [29; 5; 35]. For spatiotemporal forecasting, Chen, et, al.  proposes a historical-data replay strategy, TrafficStream, to update the neural network with all nodes feeding, while PECPM  manages a pattern bank with conflict nodes, which reduces the memory storage burdens. Most existing CL solutions are designed for homogeneous sourced data. Then there are very few research investigating task-level continuous learning where task can be converted from one to another. A pioneering work, CLS-ER  realizes class-level and domain-level continuous learning with a dual memory to respectively store instances and build long-short term memories. Even so, the data input to CLS-ER is with same image-like property and scales, which is still far away from task diversity. We argue that learning continuously on task levels can increase the data tolerance over low-quality data and also generalization capacity on new tasks. In this work, by taking bonus of continuous learning, we propose to iteratively assimilate the commonality and extract personalization to reinforce each learning task and realize continuous multi-task spatiotemporal intelligence.

## 3 Preliminary

_Definition 1 (Spatiotemporal features.)_ Spatiotemporal features refer to the data points collected by sensors deployed in urban environments, such as traffic dynamics on roads. We define a spatiotemporal feature at a specific time and location as a vector \(X^{C}\), where \(C\) represents the number of attributes (e.g., vehicle flow, speed) recorded by the sensor. We generalize this to define spatiotemporal data collected over a period as \(^{T N C}\), where \(T\) denotes the number of discrete time intervals, and \(N\) denotes the number of spatial nodes corresponding to sensor locations.

Definition 2 (Spatiotemporal prediction.)Spatiotemporal prediction involves forecasting future values of spatiotemporal data based on historical observations. Specifically, given a historical dataset \(=[X_{t-T},,X_{t-1},X_{t}]^{T N C}\), the objective is to predict future observations \(}=[X_{t+1},,X_{t+T^{}}]^{T^{}  N C}\). Here, \(T^{}\) represents the prediction horizon, the number of future time steps we seek to forecast, which for simplicity is set equal to \(T\) in this paper.

Definition 3 (Multi-task spatiotemporal learning.)Given a set of spatiotemporal tasks in an integrated urban system \(=\{_{1},_{2},,_{k}\}\) where each task \(_{i}\) is associated with a dataset \(_{i}=\{_{i},_{i}\}\). Here, \(_{i}^{T_{i} N_{i} C_{i}}\) represents the input features collected over \(T_{i}\) time steps, across \(N_{i}\) spatial locations, and \(C_{i}\) feature dimensions, and \(_{i}^{T_{i} N_{i} C_{i}}\) represents the corresponding targets. Multi-task spatiotemporal learning aims to learn a function \(f:_{0},_{1},,_{k-1}}_{0},}_{1},,}_{k-1}\) that optimally predicts all target \(_{i}\) from their respective inputs \(_{i}\), leveraging shared and unique patterns across tasks to improve generalization performance.

## 4 Methodology

### Framework Overview

The CMuST is crafted to advance urban intelligence through a synergistic integration of three components in Figure 2. We processes and standardizes diverse urban data into a harmonized format, and propose the MSTI to intricately dissect the complex interactions within spatiotemporal data, and devise a RoAda to dynamically fine-tune the model via continuous and careful updating, ensuring robust adaptability and consistent performance across fluctuating urban environments.

### Data Representation and Integration

To harness various data domains within urban spatiotemporal systems as well as data interactions between diverse dimensions, the first task is to appropriately process main ST observations, spatial indicator, and temporal indicator to create a comprehensive and integrated data representation tailored for multi-task ST learning, enabling further interactive modeling between them.

To be specific, the main observation data, namely target of interest of urban datasets, are denoted as \(_{obs}^{T N C}\), and then mapped into a spatiotemporal representation \(_{obs}^{T N d_{obs}}\) via an MLP \((_{obs};_{obs})\). Similarly, the spatial indicator \(_{s}^{T N 2}\), consisting of longitude and latitude coordinates, is applied with a linear layer \((_{s};_{s})\) to produce the spatial representation \(_{s}^{T N d_{s}}\). Temporal indicator comprises day-of-week, time-of-day \(_{dow},_{tod}^{T N}\) and timestamps \(_{ts}^{T N 6}\), which are further compressed into hidden

Figure 2: Framework overview of CMuST.

representation, i.e., \(_{t}=(_{ts}||_{dow}||_{ t0};_{t})^{T N d_{t}}\), enabling the model to capture the periodicity and sequential features of temporal data. \(_{}\) is a task-specific prompt 2 to ensure task-awareness, and is integrated into the final embedding. Given the task \(_{k}\),

\[^{(_{k})}=_{obs}||_{s}||_{ t}||^{(_{k})}\] (1)

where \(||\) denotes vector concatenation, combining spatial, temporal, and observational embeddings with the task prompt into a comprehensive representation \(^{(_{k})}^{T N d_{h}}\). We will use \(\) as representation for a specific task in following sections.

### Multi-dimensional Spatio-Temporal Interaction

Spatiotemporal observations are usually complex with multiple-level interactions where such interactions and correlations play vital roles in enhancing commonality learning through different learning domains. To this end, we devise an MSTI, to intricately dissect and disentangle interactions within spatiotemporal data from spatial-temporal indicators to main observations by inheriting nice property of attention mechanisms, which all utilize transformed slices from the integrated representation \(\).

**Spatial-context cross-interaction.** To quantitatively investigate how spatial indicator interact with main observations, we devise a multi-head cross-attention  architecture (MHCA) where spatial and observational components are alternately used as queries (\(\)) and key-value (\(\)) pairs:

\[^{(a,b)}() =\|}{ }{(a,b)}}(h)^{}.\] (2) \[^{(a,b)}(h) =(_{h}^{(a)},_{h}^{( b)},_{h}^{(b)})\] (3) \[(_{h},_{h},_ {h}) =(_{h}_{h}^{}}{ })_{h}\] (4)

The symbol \(\|\) denotes the concatenation of multiple attention heads, and \(^{O}\) is the projection matrix that aligns the output dimensions with those of \(\). Here, we let variables \(a\) and \(b\) indicate spatial (\(s\)) indicator and main observation (\(o\)), i.e., \(a=s\) and \(b=o\). Then the queries, keys, and values are generated through following transformations,

\[_{h}^{(s)}=[,^{(s)}]_{h}^{(Q_{ s})},_{h}^{(o)}=[,^{(o)}]_{h}^{(K_{ s})},_{h}^{(o)}=[,^{(o)}]_{h}^{(V_{ s})}\] (5)

where \(\) transforms input data into dimension \(D\) of attention space, \(^{(s)}\) and \(^{(o)}\) denote the respective slices of \(\) for spatial and observational features. After computing attention scores, the embeddings are taken into a feed-forward network (FFN) to enhance the learning capabilities, where \(()=(0,_{1}+_{1}) _{2}+_{2}\). The final attention outputs are then normalized by,

\[}^{(a,b)}[,^{(b)}] =(((^{(a,b)}( )+[,^{(b)}]))\] (6) \[+(^{(a,b)}()+[ ,^{(b)}])),\]

where LN is layer normalization, the resulting matrices \(}^{(s,o)}\) and \(}^{(o,s)}\) are then concatenated back to \(\) at their respective dimensions as \(}^{(SCCI)}^{T N d_{h}}\), enriching the original representation with refined features that encapsulate intricate cross-dimensional relationships.

**Temporal-context cross-interaction.** To facilitate the attention computation with respect to temporal dimension, the representation is first transposed as \(}^{(SCCI)}^{N T d_{h}}\) by denoting \(T\) as the sequence length for subsequent attention calculations. Then the step-wise positional encoding (refer to Appendix B.2) is introduced to allow our attention aware of specific temporal evolution.

The subsequent steps closely follow those of the spatial-context cross-attention mechanism, so the temporal-context cross-interaction (TCCI) can be performed as \(}^{(CI)}=(}^{(SCCI)})\), and final representation \(}^{(CI)}^{N T d_{h}}\) becomes the outcome of cross interactions between spatial and temporal dimensions.

**Self-interactions within Spatial and Temporal Aspects.** We proceed to apply self-interaction across different dimensions of the representations using self-attentions . Specifically, we begin with temporal dimension, as the sequence length of representation dimension has naturally been aligned as \(T\). This setup allows direct computation of the required self-attention,

\[()=\|\\ h=1(_{h},_{h}, _{h})^{O}.\] (7)

In this multi-head attention (MHA) configuration, the attention calculation (referred to in Equation 4, 5) involves queries, keys, and values, which is derived from the entire representation \(}^{(CI)}\), rather than individual slices. Then the output undergoes further processing through a FFN and non-linear transformations and LN to stabilize and enrich the feature representations similar Equation 6. The resulting \(}^{(TSI)}\) in \(^{N T d_{h}}\) signifies the outcome of temporal self-interactions (TSI). Then the tensor is transposed to \(}^{(TSI)}^{T N d_{h}}\). The final spatial self-interaction (SSI) computation is analogous to the temporal version, which refines spatial interactions and aggregates features across spatial nodes, as \(}=(}^{(TSI)})\). The resulting tensor from this computation is \(}^{T N d_{h}}\), which represents the outcome of comprehensive multi-dimensional interactions.

We then adaptively integrate the interactions by a fusion strategy and Huber loss is adopted to ensure the robustness to outliers in spatiotemporal samples. Details can be found in Appendix B.3. Our MSTI allows extracting diverse interactions, including spatial-temporal domain interactions via designing cross attentions on respective indicators, and self interactions within respective dimensions, enhancing the data relation learning and supporting commonality extraction across task domains.

### Rolling Adaptation over Continuous Multi-task Spatio-Temporal Learning

To ensure continuous task learning, we propose a Rolling adaptation scheme, RoAda, to model the distinction and commonality among task domains. Our RoAda is composed of two stages with a warm-up for commonality extraction and a task-specific refinement. Before the task rolling, we construct prompts to distinguish personalization of each task. Thus, the commonality and diversity can be leveraged to boost individual task adaptation.

**Task summarization as prompts.** To capture the task distinction, we devise a task summarization by a Sampling-AutoEncoding scheme from each task. Consider task \(_{k}\), main observation becomes \(^{(_{k})}^{T_{all}^{(_{k})} N  C}\). Such data is sampled by averaging observations over equivalent times of day, yielding a periodic sample representation \(_{samp}^{(_{k})}^{L^{(_{k})}_{t}  N C}\), where \(L^{(_{k})}_{t}\) denotes the number of time slots for task \(k\) within a day. Since neural networks tend to fit any data regularity, the sampled features are fed into an autoencoder for extracting the compressed and distinguished data patterns. Given the encoding and decoding processes, i.e., \(:_{samp}\) and \(:^{}_{samp}\), **S** encapsulates the summarized core characteristics of the task. The decoding phase maps **S** back to a reconstructed \(^{}_{samp}\), by minimizing the mean squared reconstruction error,

\[:=(_{s}_{samp}+_ {s}),,=*{arg\,min}_{,}_{samp }-()_{samp}^{2}\] (8)

where \(_{s}\) is the weight, and \(_{s}\) is the bias. Following the encoder, the summary features \(^{(_{k})}\) are transformed into the \(k\)-th task prompt \(^{(_{k})}=_{p}(^{(_{k})}; _{p})\) with dimension alignment.

**Weight behavior modeling.** The first warm-up stage is designed via weight behavior modeling, which assimilates the regularity from task to task. This process not only adapts the model to new tasks but also solidifies its ability in generalizing across scenarios by capturing task-wise common relations with modeling of model weight behaviors.

We begin with the task \(_{1}\) via independently training the model until its performance stabilizes. By denoting \(\) as the model learned by MSTI, the training phase can be formally described as,

\[^{(_{1})}[}]{}},((^{( _{1})}_{train};_{init}))[}]{ }}^{(_{1})}_{c}\] (9)

where \(_{init}\) are the initialized weights, \(^{(_{1})}_{}\) is the training dataset of task \(_{1}\), and \(^{(_{1})}_{c}\) are the weights when model converges. After that, our model transitions learning task from \(_{1}\) to \(_{2}\) by loading the corresponding task prompt \(^{(_{2})}\) and dataset \(^{(_{2})}_{}\). This transition involves a critical step where the evolution behavior of model weights \(\) are carefully stored,

\[((^{(_{2})}_{train};^{ (_{1})}_{c}))[}]{}}^{(_{2})}_{0},^{(_{ 2})}_{1},,^{(_{2})}_{c}\] (10)To capture common patterns, we reflect the task-level stability and variations by the evolution behavior of weights in \(\), i.e., \(^{(_{2})}=\{_{c}^{(_{1})},_ {0}^{(_{2})},_{1}^{(_{2})},,_{c }^{(_{2})}\}\). The weight set \(^{(_{2})}\) is deliberately incorporated with finalized weight of task \(_{1}\) and evolution of \(_{2}\), which explicitly captures the weight transition between tasks. We then introduce a collective variance \(\) to capture such stability, and employ a threshold \(\) to disentangle the stable and dynamic weights along the learning process, i.e.,

\[_{}^{(_{2})},_{}^{ (_{2})}=\{w_{ij}^{(_{2})}:(w_{ ij})<\},\{w_{ij}^{(_{2})}:(w_{ ij})\}\] (11)

where \((w_{ij})\) represents the element-wise variance of the across different training iterations from \(_{c}^{(_{1})}\) to \(_{c}^{(_{2})}\), indicating the fluctuation degree of the weight values. A lower variance indicates higher stability, suggesting minimal change in weights across updates. After that, stable weights \(_{}^{(_{2})}\) are then frozen, and the model transitions to the next task, \(_{3}\), using the stabilized weights as the initialization for further training,

\[((_{train}^{(_{3})};_{ c}^{(_{2})}.(_{}^{(_{2})}))) []{_{0}^{(_{3})},_ {1}^{(_{3})},,_{c}^{(_{3})}\] (12)

Similar to Equation 12, this process is repeated until the completion of task \(_{k}\), resulting in the collection \(^{(_{k})}=\{_{c}^{(_{k-1})},_{0}^{(_{k})},_{1}^{(_{k})},,_{c }^{(_{k})}\}\). The Equation 11 is similarly implemented to derive \(^{(_{k})}=_{}^{(_{k})} _{}^{(_{k})}\). Since \(_{1}\) is not involved with the commonality extraction, we subsequently load \(_{1}\) with \(^{(_{k})}.(_{}^{( _{k})})\) to achieve the complete rolling process. The stabilized weights across continuous tasks \(^{(_{1})}=_{}^{( _{1})}_{}^{(_{1})}\), serves as \(^{*}\) can ultimately result in a robust multi-task learning parameters encapsulated with well-extracted common patterns via iterative stable weight selection, and it can also be served a collective intelligence by exploiting multiple tasks, thus enhancing the generalization for subsequent learning.

**Task-specific refinement phase.** This phase aims to merge the gap between task-level commonality and specificity, where the process continuously train the model from the prior phase to the next one, as \(_{}^{*}\) is frozen to maintain overall model stability and remaining weights are iteratively update with task-specific prompts. This process allows CMuST to insert the individual intelligence into integrated model and adjust itself to better suit the unique pattern of each task, i.e.,

\[^{(_{i})}=(,^{*}, P^{(_{i})})\] (13)

where \(^{(_{i})}\) is tuned to maximize performance on the task at hand, which represents the task-specific submodel after refinement. The FineTuning denotes adjustment process of model parameters.

**Summary.** Our RoAda not only ensures the preservation of commonalities across tasks, but each model is also optimally tuned for its respective task with compressed task-level patterns, providing CMuST with opportunity to achieve peak performance with both collective and individual intelligence. More details of our methodology can be found in Appendix.

## 5 Experiment

### Datasets and Experiment Setup

**Data description.** Given the emerging multi-task ST learning, we collect and process three real-world datasets for evaluation: 1) **NYC3**: Includes three months of crowd flow and taxi hailing from Manhattan and its surrounding areas in New York City, encompassing four tasks: _Crowd In, Crowd Out, Taxi Pick_, and _Taxi Drop_. 2) **SIP**: Contains records of _Traffic Flow_ and _Traffic Speed_ within Suzhou Industrial Park over a period of three months. 3) **Chicago4**: Comprises of traffic data collected in the second half of 2023 from Chicago, including three tasks: _Taxi Pick_, _Taxi Drop_, and _Risk_.

**Baselines and metrics.** Our CMuST model is evaluated on a widely-used baselines spatiotemporal prediction, including RNN-based models (**DCRNN**, **AGCRN**), STGNNs (**GWNET**, **STGCN**) _for single task_, and **MTGNN**, **STEP** **PromptST** _for multiple tasks_,as well as _attention-based models_ of (**GMAN**, **ASTGCN**, **STTN**). The performance metrics are mean absolute error (MAE), and mean absolute percentage error (MAPE), where lower values indicate higher predictive performance.

**Implementation details.** We partitioned datasets into training, validation, and testing sets with 7:1:2 ratio. CMuST forecasts observations of next 12 time steps based on previous 12 steps, as \(T=T^{}=12\),. All data were normalized to zero mean and unit variance, and predictions were denormalized to normal values for evaluation. For the MSTI, embedding dimensions were \(d_{obs}=24,d_{s}=12,d_{t}=60\), and the prompt dimension was 72. Dimensions for self-attention and cross-attention respectively were 168 and 24, with each attention having 4 heads and FFN's hidden dimension was 256. The Adam optimizer is adopted with an initialized learning rate of \(1 10^{-3}\), and weight decay of \(3 10^{-4}\), where the early-stop was applied. For RoAda, the threshold \(=10^{-6}\). Our model was implemented with PyTorch on a Linux system equipped with Tesla V100 16GB.

### Performance Comparison

1) **Performance comparison among baselines.** In Table 1, we compared the predictive performance of our CMuST with other methods across various city datasets. Obviously, our CMuST significantly outperforms other baselines on all datasets across most of metrics and the multitask-based methods improve performance by an average of 8.51% over the singletask-based methods. This result underscores the effectiveness of the cross-attention mechanism for decoupling multidimensional dependencies, which not only enhances spatiotemporal representational capacity, but also enables easy extraction of common correlations among tasks and thus empowering each individual task to benefit from well-extracted common patterns. 2) **Robustness in data-scarce scenarios within multi-task framework.** We constructed scenarios of data scarcity for specific tasks, i.e., we reduce part of the spatial nodes in prediction of _Crowd In_, and also reduce number of samples in temporal dimension by expanding time intervals on NYC, to study the robustness of CMuST under the challenging scenario of limited (reduced) data. Results shown in Table 2 indicate that assimilating common information from other tasks can help better prediction in a single task even though it is with limited data on either spatial and temporal dimension. This demonstrates that multi-task prediction relax the requirement of individual tasks on both data volumes and distributions, where the shared commonality effectively captures and delivers the consistency and diversity among tasks.

    &  &  &  \\ 
**Methods** & **Metrics** & **Crowd In** & **Crowd Out** & **Taxi Pick** & **Taxi Drop** & **Traffic Flow** & **Traffic Speed** & **Taxi Pick** & **Taxi Drop** & **Risk** \\   & **MAE** & 17.5289 & 19.5667 & 10.8188 & 9.6142 & 12.5326 & 0.7044 & 3.0624 & 2.5793 & 1.1174 \\  & **MAPE** & 0.5939 & 0.5695 & 0.4330 & 0.4818 & 0.2455 & 0.2686 & 0.4237 & 0.4816 & 0.2504 \\
**AGCRN** & **MAE** & 11.5135 & 13.1569 & 7.0675 & 6.0066 & 15.8319 & 0.6924 & 2.3542 & 2.0884 & 1.1183 \\  & **MAPE** & 0.5094 & 0.4773 & 0.3753 & 0.3665 & 0.2926 & 0.2744 & 0.4092 & 0.4046 & 0.2505 \\  & **MAE** & 11.4420 & 13.2992 & 7.0701 & 6.1171 & 13.0529 & 0.6900 & 2.3671 & 2.0434 & 1.1197 \\
**GWNET** & **MAPE** & 0.4778 & 0.6171 & 0.3713 & 0.3514 & 0.2483 & 0.2655 & 0.3912 & 0.4044 & 0.2514 \\
**STGCN** & **MAE** & 11.3766 & 13.3522 & 7.1259 & 5.9268 & 15.3501 & 0.7111 & 2.3781 & 2.1427 & 1.1184 \\  & **MAPE** & 0.5018 & 0.4318 & 0.3243 & 0.3393 & 0.3041 & 0.2660 & 0.4074 & 0.4331 & 0.2507 \\
**GMAN** & **MAE** & 11.3414 & 13.1923 & 7.0662 & 6.0912 & 13.0368 & 0.6952 & 2.3663 & 2.0316 & 1.1182 \\  & **MAPE** & 0.4782 & 0.6065 & 0.3652 & 0.3468 & 0.2464 & 0.2678 & 0.3953 & 0.4036 & 0.2516 \\
**ASTGCN** & **MAE** & 14.2847 & 17.1582 & 9.1430 & 7.7063 & 16.4896 & 0.6980 & 2.5091 & 2.1520 & 1.1175 \\  & **MAPE** & 0.6396 & 0.5922 & 0.4607 & 0.4524 & 0.3104 & 0.2682 & 0.4593 & 0.4413 & **0.2502** \\
**STTN** & **MAE** & 12.1994 & 14.1966 & 7.6716 & 6.3816 & 15.1751 & 0.6939 & **2.2996** & 2.0355 & 1.1214 \\  & **MAPE** & 0.4757 & 0.4744 & 0.3600 & 0.3763 & 0.2881 & 0.2625 & 0.3893 & 0.4133 & 0.2518 \\   & **MAE** & 11.4350 & 13.3072 & 0.7736 & 6.1162 & 13.0486 & 0.6989 & 2.3692 & 2.0361 & 1.1201 \\  & **MAPE** & 0.4785 & 0.6185 & 0.3782 & 0.3502 & 0.2475 & 0.2687 & 0.3979 & 0.4073 & 0.2578 \\
**STEP** & **MAE** & 12.2382 & 13.1043 & 0.6916 & 9.5190 & 12.0032 & 0.6970 & 2.3592 & 2.0168 & 1.1190 \\  & **MAPE** & 0.4537 & 0.4361 & 0.3248 & 0.3379 & 0.2391 & 0.2638 & 0.3914 & 0.4019 & 0.2507 \\
**PromptST** & **MAE** & **11.0036** & 13.0237 & 6.8711 & 5.8797 & 11.8620 & 0.6921 & 2.3576 & 2.0065 & 1.1186 \\  & **MAPE** & 0.4465 & 0.4358 & 0.3265 & 0.3382 & 0.2375 & 0.2632 & 0.3913 & 0.4012 & 0.2511 \\   & **MAE** & 11.1533 & **12.9088** & **6.7581** & **5.8546** & **11.5811** & **0.6843** & 2.3264 & **2.0034** & **1.1172** \\  & **MAPE** & **0.4384** & **0.4265** & **0.3118** & 0.3375 & **0.2279** & **0.2585** & **0.3872** & **0.4009** & 0.2503 \\   

Table 1: Performance comparison on three datasets. Best results are **bold** and the second best are underlined.

    &  &  &  &  &  \\  & **MAE** & **MAPE** & **MAE** & **MAPE** & **MAE** & **MAPE** & **MAE** & **MAPE** \\ 
**GWNET** & 13.7648 & 0.4825 & 12.4637 & 0.4731 & 20.2547 & 0.4465 & 20.6487 & 0.4958 \\
**STEP** & 13.1827 & 0.4772 & 12.2393 & 0.4612 & 20.1936 & 0.4436 & 20.1465 & 0.4915 \\
**PromptST** & 12.8362 & 0.4719 & 12.0361 & 0.4607 & 19.8465 & 0.4384 & 19.5238 & 0.4872 \\ 
**CMuST** & **12.1611** & **0.4506** & **11.264** & **0.4470** & **1

### Ablation Study

To assess the effectiveness of each module in CMuST and its capability for multi-task learning, we designed a set of variants as 1) **w/o context-data interaction:** remove the spatial-context and temporal-context cross-interactions in the MSTI module, 2) **w/o consistency maintainer:** omit the separation and freezing of stable weights during the RoAda phase, instead using all weights for rolling training, 3) **w/o task-specific preserver:** eliminate the task-specific prompts, thus removing the task-specific diversity preservation. Figure 3 supports our hypothesis that CMuST is a cohesive and integral system, where the results of "w/o context-data interaction" deteriorate, potentially indicating multidimensional interactions from context environments to spatial relations and temporal evolution exactly make sense for prediction. The results on "w/o consistency maintainer" highlights the validity of capturing task consistency and commonality to facilitate task learning. Lastly, performances of "w/o task-specific preserver" show that removing unique patterns distinctive from other tasks makes inferior results.

### Case Study

1) **Visualizing attention across training phases.** In Figure 4(a), we visualize the changes of attention weights of CMuST during the stage of RoAda. It is observed that as tasks are learning continuously, the relationships and interactions across various dimensions is becoming distinctive and going stable, demonstrating the consolidation process of dimension-level relations. By modeling weight behavior, such consolidated relations and interactions between context and observations can further enable the extraction of consistency in spatiotemporal interactions across tasks. 2) **Performance variation along with task increasing.** Figure 4(b) shows the performance of individual tasks on NYC as the number of tasks increases. The performance of each task is improving with the addition of more tasks, which indicates that tasks are no longer isolated but gain the collective intelligence via assimilating common representations and interactive information.

### Parameter sensitivity analysis

We varied the dimension of the task prompt \(d_{p}\) as \(\{18,36,72,144\}\), the number of attention heads in MSTI as \(\{1,2,4,8,16\}\), and the threshold \(\) for RoAda among \(\{10^{-4},10^{-5},10^{-6},10^{-7}\}\). Results shown in Figure 6 indicate that the optimal settings were \(d_{p}=72\), \(head=4\), and \(=10^{-6}\). Details of parameter analysis and fine-tuning can be found in our Appendix C.6.

## 6 Conclusion

In this work, we enable spatiotemporal learning to get rid of isolation with proposed CMuST, which consists of two major components. In CMuST, the MSTI is devised to dissect complex multi-dimension data correlations, to reveal disentangled patterns. To extract the task-wise consistency

Figure 4: Case studies of our proposed CMuST on NYC.

Figure 3: Ablation studies of CMuST on Chicago.

and task-specific diversity, we propose a rolling learning scheme RoAda, where it simultaneously models weight behavior to enable collective intelligence, and constructs task-specific prompts by compressing domain data with AutoEncoding to empower task-specific refinement for enhancement. We believe our CMuST can not only help better understand the collective regularity and intelligence in urban systems, but significantly reduce repeated training and improve data exploitation, which is progressively approaching green computing in future cities. For future work, we will further investigate the collective intelligence in open urban systems, which can potentially generalize to wider domains such as energy and environment for human-centered computing.

## 7 Acknowledgement

This paper is partially supported by the National Natural Science Foundation of China (No.12227901, No.62072427), Natural Science Foundation of Jiangsu Province (BK_20240460), the Project of Stable Support for Youth Team in Basic Research Field, CAS (No.YSBR-005), and the grant from State Key Laboratory of Resources and Environmental Information System.