# Belief Projection-Based Reinforcement Learning for Environments with Delayed Feedback

Jangwon Kim\({}^{1}\)

jangwonkim@postech.ac.kr

&Hangyeol Kim \({}^{2}\)

hangyeol.kim@koreaaero.com

&Jiwook Kang\({}^{2}\)

jiwook.kang@koreaaero.com

&Jongchan Baek\({}^{3}\)

jcbaek@etri.re.kr

&Soohee Han\({}^{1}\)

soohee.han@postech.ac.kr

###### Abstract

We present a novel actor-critic algorithm for an environment with delayed feedback, which addresses the state-space explosion problem of conventional approaches. Conventional approaches use an augmented state constructed from the last observed state and actions executed since visiting the last observed state Using the augmented state space, the correct Markov decision process for delayed environments can be constructed; however, this causes the state space to explode as the number of delayed timesteps increases, leading to slow convergence. Our proposed algorithm, called Belief-Projection-Based \(Q\)-learning (BPQL), addresses the state-space explosion problem by evaluating the values of the critic for which the input state size is equal to the original state-space size rather than that of the augmented one. We compare BPQL to traditional approaches in continuous control tasks and demonstrate that it significantly outperforms other algorithms in terms of asymptotic performance and sample efficiency. We also show that BPQL solves long-delayed environments, which conventional approaches are unable to do.

## 1 Introduction

Deep reinforcement learning (RL) methods have been successfully applied to diverse domains, such as decision-making tasks and robotic control problems [23; 2]. The deep RL framework has shown its potential by mastering the highly complex board game Go and defeating the professional Go player Lee . It has also demonstrated superhuman performance, even in games where a perfect model is not provided, such as Atari video games . Furthermore, modern deep RL algorithms have achieved significant performance improvements in continuous control domains, such as robotic locomotion [28; 29; 14]. Recently, many attempts have been made to apply RL-based control in not only simulations but also the real-world domain [13; 16; 22; 26]. Remarkable examples include applying the RL method to a quadrupedal robot to learn walking gaits and plan the motion of robotic manipulation in the real world [14; 17].

Despite recent progress in RL methods, adapting RL algorithms to the real world remains challenging for many reasons. Delayed feedback from the environment is a particular challenge. For example, latency may occur when the controller attempts to communicate with an agent if the agent is located far from the controller or if a large quantity of data, such as high-resolution images, must be transmitted. Furthermore, hardware issues can cause sensing or actuator delays. This delayed feedback may hinder achieving the control objectives.

Therefore, controlling signal delay is essential for applying RL algorithms to real-world domains. In this study, we propose a novel approach called belief projection-based \(Q\)-learning (BPQL) to overcome a constant delayed environment in which feedback is delayed by fixed timesteps. BPQL addresses the state-space explosion problem, which commonly occurs in conventional approaches, and achieves better performance than conventional approaches in terms of asymptotic performance and sample efficiency.

## 2 Related Work

### Standard Reinforcement Learning

A Markov decision process (MDP) is defined as a 5-tuple \((,,R,P,)\), where \(\) is the state space, \(\) is the action space, \(R:\) is the reward function, \(P:\) is the transition kernel, and \((0,1)\) is a discount factor. The policy \((|s)\) maps the state-to-action distribution. In the standard RL framework, the agent chooses an action to maximize the discounted cumulative rewards by interacting with an environment defined as an MDP.

Let the distribution of the initial state be \(_{0}\). Then, the expected discounted cumulative reward from policy \(\) is given as:

\[()=[_{t=0}^{}^{t}R(s_{t},a_{t})],\] (1)

where initial state \(s_{0}_{0},a_{t}(|s_{t})\), and \(s_{t+1} P(|s_{t},a_{t})\)

From the standard definitions, the value and \(Q\)-value functions are obtained as:

\[V^{}(s_{t})=[_{k=0}^{}^{k}R(s_{t+k},a_{t+k})|s_{t}]\] (2) \[Q^{}(s_{t},a_{t})=[_{k=0}^{}^{k}R(s_ {t+k},a_{t+k})|s_{t},a_{t}],\] (3)

where \( k 0,a_{t+k}(|s_{t+k})\) and \(s_{t+k+1} P(|s_{t+k},a_{t+k})\).

Both value functions satisfy the Bellman equation :

\[V^{}(s_{t})=_{a_{t}(|s_{t})}[R(s_{t },a_{t})+_{s_{t+1} P(|s_{t},a_{t})}[V(s_{t+1}) ]]\] (4) \[Q^{}(s_{t},a_{t})=R(s_{t},a_{t})+_{s_{t+1} P (|s_{t},a_{t})}[V(s_{t+1})].\] (5)

In the standard RL framework, we assume that the reward and next state only depend on the immediately previous state and action based on the Markov assumption. However, in an environment where the observation is delayed, the next state does not depend on the immediately previous state but on an older state and action history. This delay property forces the environment to be a partially observable MDP, not an MDP.

In this study, we consider the observation-delayed environment but not the action-delayed environment. However, observation and action delays are intrinsically linked [18; 34], which allows us to control the action-delay problem in the same manner as the observation-delay problem.

### Constant Observation-Delayed Environment

In a delayed feedback environment, the agent receives time-delayed feedback from the environment, which makes it difficult to choose a timely and correct action at each timestep. In conventional control theory, signal delay is often handled by augmenting the state as a combination of the last observed state and history of actions for the delayed timesteps [20; 25].

This augmented state can also be used in RL frameworks. Considering a constant delayed feedback system, an environment with delayed feedback can be formalized by a constant delayed MDP (CDMDP) . The CDMDP is defined as a 6-tuple \((,,R,P,,d)\), where \(d\) is the number of timesteps between the execution of an action and receipt of feedback from that action (i.e., delayed timesteps).

The CDMDP is known to be reducible to an MDP (\(,,},,\)), where \(=^{d}\) is an augmented state space, \(}:\) is the reward mapping, and \(:\) is the transition kernel [18; 34]. This makes it possible to treat the CDMDP as a regular MDP. From this perspective, some model-free approaches, referred to as _augmented state approaches_, have been proposed [4; 34; 27; 6]. These use the augmented state, which is constructed by concatenating the last observed state and previous actions since visiting the observed state. However, the augmented state space grows exponentially as the number of delayed timesteps \(d\) increases. The _curse of dimensionality_ makes the augmented state approach tractable for environments with only a relatively short delay.

Model-based approaches have also been proposed for handling delayed environments [34; 15; 11; 8; 1]. In these approaches, the agent predicts the current state by simulating the environment model and selects an action based on the predicted state. Increasing the accuracy of the transition model is key to this approach. However, in a complex, stochastic environment with a long time delay, the rapid growth of model error increases the difficulty of training the agent.

To the best of our knowledge, the closest work to ours is the study  that proposes a model-based algorithm expectation maximization \(Q\)-learning (EMQL). This algorithm, however, can only be used in discrete states and action spaces, whereas our proposed algorithm can be used in continuous state-action spaces without constraints. Additionally, although EMQL can suffer from dynamic model errors, our algorithm avoids this because it is a completely _model-free_ approach.

As aforementioned, delayed environments have been mainly handled with two methods: the complete information method (e.g., the augmented state method) and the state estimation methods (e.g., the model-based method). As inherent drawbacks, the first one suffers from the curse of dimensionality and the second requires accurate model-based state estimation. Therefore, the research objective of this work is to resolve these two drawbacks together, i.e., find a model-free approach that is basically free of dimensionality curses. This is why the proposed algorithm was born.

## 3 Dynamics in Augmented State Space

In this section, we analyze the augmented state approach for the constant delayed environment. Figure 1 illustrates the interaction between an agent and a delayed environment. At each time \(t\), the agent executes action \(a_{t}\) and receives the delayed state \(s_{t+1-d}\) and reward \(r_{t-d}\), where \(d\) is the number of delayed timesteps. These feedback values are the response from action \(a_{t-d}\). This misalignment of state, action, and reward makes it difficult to find an optimal policy.

### Notation

We have observed that the CDMDP can be treated as a regular MDP by replacing the state space in the CDMDP with the augmented state space. The augmented state is constructed by concatenating the last observed state and previous actions and is denoted by \(\).

More formally, in an environment delayed by \(d\) timesteps, the augmented state \(_{t}\) is constructed by concatenating the last observed state and previous \(d\) actions:

\[_{t}=(s_{t-d},a_{t-d},a_{t-d+1},,a_{t-1}).\] (6)

Figure 1: In a constant delayed environment, the agent receives delayed state \(s_{t+1-d}\) and reward \(r_{t-d}\) after executing action \(a_{t}\). These values are the actual feedback from previous action \(a_{t-d}\) and state \(s_{t-d}\), where \(d\) is the number of _delayed_ timesteps.

The policy that receives the augmented state as input is called the _augmented state-based policy_, denoted as \((|)\). \(^{}()\) is the agent's steady-augmented state distribution for augmented state-based policy \(\). Furthermore, we refer to the value of the augmented state as the _augmented state-based value_, denoted as \(^{}(_{t})\). Similarly, the \(Q\)-value of the augmented state is called the _augmented state-based \(Q\)-value_, denoted as \(^{}(_{t},a_{t})\).

### Augmented State Approach

Our objective is to train the agent in a constant delayed environment. In the augmented state approach, a new augmented state is constructed by augmenting the last observed state and previous actions to treat the CDMDP as a regular MDP.

We can apply a modified Bellman operator called the _delay Bellman operator_\(}^{}\) to determine the augmented state-based value-function. The operator \(}^{}\) is given as:

\[}^{}(_{t})_{a_{t} (|_{t})}[_{(s_{t}|_{t })}[R(s_{t},a_{t})]+_{_{t+1}( |_{t},a_{t})}[(_{t+1})]], t>d,\] (7)

where \(d\) denotes the number of delayed timesteps in the environment. The expected reward is used in the delay Bellman operator because we assume that all feedback is delayed in a constant delayed environment, including the reward, which implies that the exact reward \(R(s_{t},a_{t})\) cannot be determined at time \(t\).

The augmented state-based values can be computed by repeatedly applying the delay Bellman operator. After computing the augmented state-based values, the policy is updated to increase these values for all augmented states in the policy-improvement stage.

Although the augmented state-based values can be computed using the delay Bellman operator, the state-space explosion issue follows; the augmented state space grows exponentially as the number of delayed timesteps \(d\) increases because the augmented state space \(\) in the reduced MDP is a Cartesian product of the original state space and \(d\) action space (i.e., \(=^{d}\)). This indicates that the larger the state space, the larger the set of samples required to compute the value of the entire set of states. This makes the augmented state approach impractical for a long-delayed environment.

### Alternative Method to Represent the Augmented State-Based Value

Because calculating the value of the augmented state space by applying the delay Bellman operator suffers from the state-space explosion issue, we propose an alternative method for representing the augmented value.

The distribution of the probability of visiting the true current state \(s_{t}\) (which has not yet been observed) depends on the augmented state \(_{t}\). If two successive transition probabilities \((s_{t}|_{t}^{1})\) and \((s_{t}|_{t}^{2})\) are similar, we can assume that the augmented states \(_{t}^{1}\) and \(_{t}^{2}\) have similar representative meanings. Based on this assumption, we define the _belief projection_ of the augmented values as follows.

**Definition 3.1**.: For a policy \(\), state space \(=\{s_{1},s_{2},,s_{i}\}\) and augmented state space \(=\{_{1},_{2},,_{j}\}\), let **B**, \(}\) and **D** be:

\[=(s_{1}|_{1})&(s_{2}|_{1})&&(s_{i}|_{1})\\ (s_{1}|_{2})&(s_{2}|_{2})&& (s_{i}|_{2})\\ &&&\\ (s_{1}|_{j})&(s_{2}|_{j})&& (s_{i}|_{j}),}=^{ }(_{1})\\ \\ ^{}(_{j}),=(),\]

where \(=[^{}(_{1}),^{}(_{2}),..., ^{}(_{j})]\) is steady-augmented state probability vector when the agent follows the policy \(\). Then, the _projected values_\(_{}\) are defined as:

\[_{}=_{}},\] (8)

where the projection operator \(_{}\) is \((^{})^{-1}^{} \).

We refer to matrix **B** as the _belief matrix_. If the matrix \(^{}\) is not invertible, then the inverse operator can be replaced with the Moore-Penrose pseudoinverse operator. We refer to the outcome from the projection operator \(_{}\) as the _belief projection_. The belief projection operator \(_{}\) projects a vector in \(^{j}\) onto the column space of the belief matrix **B** with respect to the weighted Euclidean norm \(||||\).

Because the belief projection lies in the column space of the belief matrix, we can use the vector \(_{}\) to decompose the projected values as follows:

\[_{}=_{}=V_{ }^{}(s_{1})\\ \\ V_{}^{}(s_{i}).\] (9)

We refer to the elements of \(_{}\) as the _beta values_. The vector \(_{}\) can also be defined as:

\[_{}=*{arg\,min}_{^{i}}|| -}||_{2,}.\] (10)

From Equation (9), the augmented state-based value can be decomposed into the sum of the expectation of the beta values over the original states and the residual:

\[^{}(_{t})=_{(s_{t}|_{t})} [V_{}^{}(s_{t})]+_{}^{ }(_{t}),\] (11)

where \(t>d\). Similarly, we can extend Equation (9) to the belief projection for the augmented \(Q\)-values (projected \(Q\)-values); the _beta \(Q\)-values_ for a given action \(a\) satisfy the following:

\[^{}(_{t},a)=_{(s_{t}|_{t})} [Q_{}^{}(s_{t},a)]+_{}^{}(_{t},a)\] (12)

### Linear Function Approximation

The augmented state approach is impractical for a long-delayed environment because its state-space size grows exponentially as the number of delayed timesteps increases. In a long-delayed environment, this indicates that directly calculating the augmented state-based \(Q\)-values by applying the delay Bellman operator requires a significantly large set of samples.

To avoid this issue, instead of calculating the augmented state-based values \(}\) directly, we estimate the belief projection \(_{}}\). Let \(^{}\) be a matrix form of the delay Bellman operator \(^{}\) i.e., \(^{}}\) := \(}+}}\), where \(}=[_{(s_{t}|_{t}),a (|_{t}|)}[R(s,a)],,_{(s_{t}|_{j}),a (|_{j})}[R(s,a)]]^{}\) is a vector consisting of expected rewards and \(}\) is the transition matrix. In this setting, focusing on finding a solution \(_{}^{}=*{arg\,min}_{^{i}}||-_{}^{}()||_{ {W}}\) could be achieved very efficiently, especially when \(||||\).

Note that the values are approximated as linear combinations of the belief matrix and the beta-values. In other words, the belief projection can be seen as a linear function approximator where the feature vector for an augmented state \(_{k}\) is \([(s_{1}|_{k}),(s_{2}|_{k}),, (s_{i}|_{k})]\) and the corresponding parameters are the beta values \([V_{}^{}(s_{1}),V_{}^{}(s_{2}),,V_{}^{ }(s_{i})]^{}\). In that sense, we can guarantee the contraction of the combined operator \(_{}^{}\) by using the well-known contraction property of linear function approximation [32; 5].

**Proposition 3.2**.: _Let the projection operator onto the column-space of the belief matrix **B** with respect to the weighted Euclidean norm \(||||\) be \(_{}\), where \(=[^{}(_{1}),^{}(_{2}),..., ^{}(_{j})]\) is a steady-augmented state probability vector, and the Markov chain be irreducible i,e, \(^{}(_{k})>0\) for all \(k\{1,2,...,j\}\). Then the combined operator \(_{}^{}\) is \(\)-contraction with respect to \(||||_{}\)._

Proof.: See appendix B. 

This contraction property of \(_{}^{}\) guarantees to find a fixed unique solution \(}_{}^{}=[_{}^{}(s_{1}), ,_{}^{}(s_{i})]\) by repeatedly applying the combined operator \(_{}^{}\). This \(}_{}^{}\) can be used as an estimator for the true beta values \(_{}\), providing a direction for policy update. For example, a new policy \(_{}\) can be obtained by greedily choosing an action, i.e., \(_{}(_{t})=*{arg\,max}_{a_{t}}(_{(s_{t}|_{t})}[R(s_{t},a_{t}) ]+_{P(_{t+1}|_{t},a_{t})}[_{ (s_{t+1}|_{t+1})}[_{}^{_{}}(s_{t+1})]])\), where the \(_{}\) is the policy before improved.

Actor-Critic Algorithm for Constant Delayed Environment

In the previous section, we presented an iterative method that computes the value function in the smaller state space created by belief projection, rather than in the larger augmented state space. However, despite the favorable convergence property of value function approximation through belief projection, explicitly calculating the belief matrix can be challenging, especially in cases where the augmented space is large or continuous. To address this challenge, we introduce a practical sampling-based reinforcement learning algorithm based on the theoretical insights from the previous section. In this algorithm, the agent _learns_ the beta values without the explicit computation of the belief matrix.

First, we define the _delay soft Bellman operator_\(}_{}^{}\), which is the soft Bellman operator  for the delayed environment setting:

\[}_{}^{}^{}(_{t},a_ {t})_{(s_{t}|_{t})}[R(s_{t},a_{t})]+ _{_{t+1},a_{t+1}}[^{}( _{t+1},a_{t+1})-(a_{t+1}|_{t+1})],\] (13)

where \(\) is the temperature parameter. We can compute the augmented state-based soft \(Q\)-values by repeatedly applying the delay soft Bellman operator.

In the policy improvement stage, the policy is updated towards the exponential of the evaluated augmented state-based soft \(Q\)-function. The improved policy \(_{}\) can be obtained as :

\[_{}=*{arg\,min}_{^{ }}_{}(^{}(|_{t}) (^{_{}}( _{t},))}{^{}(_{t})}),\] (14)

where \(^{_{}}(s_{t})\) is a normalizing function. This updated policy \(_{}\) guarantees \(^{_{}}(_{t},a_{t})^{_{ }}(_{t},a_{t})\) for all \((_{t},a_{t})\).

The beta \(Q\)-values are approximately computed by minimizing the average of weighted squared residual error i.e, \(_{}^{}\) in Equation (12) as:

\[J_{Q_{}}=_{_{t}^{},a_{t} }_{(s_{t}|_{t})}[Q_{}^{}(s_{t},a_{t})]-^{}(_{t},a_{t})^{2}.\] (15)

Let the augmented \(Q\)-value be a soft \(Q\)-value; then, substituting \(^{}(s_{t},a_{t})\) into (15) with the target value in (13), Equation (15) can be expanded to the following equation:

\[J_{Q_{}}=_{_{t}(),a_{t} }_{(s_{t}|_{t})}[Q_{}^{}(s_{t},a_{t})-R(s_{t},a_{t})]-_{_{t+1} (|_{t},a_{t}),a_{t+1}}[^{}(_{ t+1},a_{t+1})\] \[-(a_{t+1}|_{t+1})]^{2}.\] (16)

Subsequently, we replace the augmented state-based soft \(Q\)-value with the expectation of beta \(Q\)-values:

\[^{}(_{t+1},a_{t+1})_{ (s_{t+1}|_{t+1})}[Q_{}^{}(s_{t+1},a_{t+1})]\] (17)

by fitting the augmented \(Q\)-value to the belief projection. Then, the objective can be rewritten as:

\[J_{Q_{}} =_{_{t}(),a_{t}( _{t})}_{(s_{t}|_{t})}Q_{ }^{}(s_{t},a_{t})-R(s_{t},a_{t})\] (18) \[-_{_{t+1},a_{t+1}}_{(s_{t+1}|_{t+1})}[Q_{}^{}(s _{t+1},a_{t+1})]-(a_{t+1}|_{t+1})^{2}.\]

Now, our objective has changed to finding the beta \(Q\)-values, where their expectation best represents the augmented state-based \(Q\)-values. We can also rewrite Equation (14) using the belief projection instead of using the augmented \(Q\)-values:

\[_{} =*{arg\,min}_{^{}}_{} (^{}(|_{t}) _{(s_{t}|_{t})}[Q^{_{}}_{}( s_{t},)])}{^{}(_{t})})\] (19) \[=*{arg\,min}_{^{}}_{a_{t} }^{}(a_{t}|_{t})(a_{t}| _{t})-_{(s_{t}|_{t})}[Q^{ _{}}_{}(s_{t},a_{t})]+^{^{}}(_{t})\] (20) \[=*{arg\,min}_{^{}}_{a_{t} }^{}(a_{t}|_{t})(a_{t}| _{t})-_{(s_{t}|_{t})}[Q^{ _{}}_{}(s_{t},a_{t})]+_{(s_{t}| _{t})}[ Z^{^{}}(s_{t})]\] (21) \[=*{arg\,min}_{^{}}_{(s_{t}|_{t})}_{a_{t}}^{}(a_{t}| _{t})(a_{t}|_{t})-Q^{_{}}_{}(s_{t},a_{t})+ Z^{^{}}(s_{t} )\] (22) \[=*{arg\,min}_{^{}}_{(s_{t}|_{t})}[_{}(^{}( |_{t})Q^{_{}}_{}(s_{t},))}{Z^{^{}}(s_{t})})],\] (23)

where \(Z^{_{}}(s_{t})\) is a normalizing function for the distribution \(Q^{_{}}(s_{t},)\). Equation (21) holds because \(^{_{}}(_{t})\) and \(_{(s_{t}|_{t})}[^{_{ }}(s_{t})]\) are independent of \(^{}\).

Using Equation (23), we can update the policy by minimizing the expectation of the Kullback Leibler-divergence of the policy and exponential of the beta \(Q\)-value. Notably, we need not evaluate the augmented state-based soft \(Q\)-value, which causes the state-space explosion problem when the delayed timestep is large. By contrast, we evaluate the beta \(Q\)-values and improve the policy using these values.

The beta \(Q\)-value is expected to converge more stably and quicker than the augmented state-based \(Q\)-value because its input state space is considerably smaller than the augmented space, which also helps to obtain an improved policy with higher quality.

In the continuous state setting, the parameterized beta \(Q\)-function \(Q^{}_{,}(s_{t},a_{t})\) can be approximately computed by minimizing the squared-residual in Equation (18) denoted by \(J_{Q_{}}()\) with the aid of replay memory \(\):

\[J_{Q_{}}()= _{(_{t},s_{t},a_{t},r_{t},_{t+1},s_{t+1}) }[(Q^{}_{,}(s_{t},a_{t})-(r_{t}+ _{a_{t+1}}[Q^{}_{,}(s_{t+1}, a_{t+1})\] (24) \[-_{}(a_{t+1}|_{t+1})]))^{2}],\]

where \(r_{t}=R(s_{t},a_{t})\), \(Q^{}_{,}\) is a target network for \(Q^{}_{,}\), and \(_{}\) is a parameterized policy.

In the policy improvement stage, the policy can be trained by minimizing the objective \(J_{}()\), which can be formalized from Equation (23) as:

\[J_{}()=_{(_{t},s_{t})}[ _{a_{t}_{}}[_{}(a_{t}| _{t})-Q^{}_{,}(s_{t},a_{t})]].\] (25)

By iteratively minimizing \(J_{Q_{}}()\) and \(J_{}()\), the actor and critic networks can be trained for a delayed environment. This is the BPQL algorithm. Note that the first and second subscripts of \(Q_{,}\) refer to the neural network weights and the beta \(Q\)-values, respectively. Actually, the beta \(Q\)-values depend on the neural network weights. Nevertheless, \(\) and \(\) are written together for clearer notation. Only \(\) is the variable set to be optimally taken through learning.

In the BPQL algorithm, expanded transition tuples that include augmented states are stored in the replay memory \(\). To construct the augmented state, we use a temporary buffer \(\) in which the original states and action history are stored. The details of BPQL are summarized in Appendix A.

## 5 Experiments

We compared the performance of the BPQL algorithm with the following three baselines4:

* Augmented approach: Augmented SAC
* Model-based approach: Delayed-SAC
* Normal SAC

In the augmented approach, we solve the control problem by adopting an augmented state space, which facilitates treating the CDMDP as a regular MDP. This is a popular and widely used method for an agent learning in a delayed environment . The policy and critic in the augmented state approach are trained by minimizing the objectives \(_{}()\) and \(_{}()\):

\[_{}()&=_ {(_{t},a_{t},r_{t},_{t+1})}[(_{ }^{}(_{t},a_{t})-(r_{t}+_{_{t+1} }[_{}^{}(_{t+1},a_{t+1})\\ &-_{}(a_{t+1}|_{t+1})]))^{2}],\\ &_{}()=_{(_{t})}[_{a_{t}_{}}[_{}(a_{t}| _{t})-_{}^{}(_{t},a_{t})].\] (26)

We named this augmented-based approach _Augmented SAC_.

In the model-based approach, we use the delayed-Q algorithm , which uses a forward model to estimate the timely proper state. In this approach, the agent takes an action based on the predicted current state from the dynamic model using the last observed state and action history, i.e., \((s_{t-d},a_{t-d},a_{t-d+1},,a_{t-1})\). The model repeats the one-step transition prediction \(d\) times to

   &  &  &  &  &  &  \\   \(d\) & _Algorithm_ & & & & & & & \\   & Normal SAC & \(-276.2_{ 33.1}\) & \(672.2_{ 518.8}\) & \(290.9_{ 86.7}\) & \(32.8_{ 3.7}\) & \(16.9_{ 13.7}\) & \(-28.23_{ 4.66}\) \\   & Delayed-SAC & \(4182.7_{ 600.7}\) & \(4463.2_{ 434.0}\) & \(2821.7_{ 609.1}\) & \(73.9_{ 32.6}\) & \(916.5_{ 142.1}\) & \(-3.95_{ 0.53}\) \\   & Augmented SAC & \(6054.0_{ 1045.5}\) & \(3453.3_{ 66.26}\) & \(2732.6_{ 858.9}\) & \(47.6_{ 2.1}\) & \(983.6_{ 32.6}\) & \(-3.95_{ 0.51}\) \\   & BPQL (ours) & **8100.1\({}_{ 840.4}\)** & **4538.5\({}_{ 271.3}\)** & **2922.5\({}_{ 671.2}\)** & **88.0\({}_{ 36.6}\)** & **998.1\({}_{ 36.6}\)** & **-3.80\({}_{ 851}\)** \\    & Normal SAC & \(-288.7_{ 50.7}\) & \(38.6_{ 29.4}\) & \(68.3_{ 34.1}\) & \(32.1_{ 6.7}\) & \(10.1_{ 2.9}\) & \(-38.2_{ 3.18}\) \\   & Delayed-SAC & \(2660.9_{ 492.3}\) & \(1.0_{ 4.0}\) & \(1289.2_{ 1071.7}\) & \(58.2_{ 14.6}\) & \(929.1_{ 141.7}\) & \(-4.02_{ 0.50}\) \\   & Augmented SAC & \(2012.6_{ 835.0}\) & \(3028.4_{ 383.2}\) & \(2100.0_{ 772.7}\) & \(43.4_{ 18.5}\) & **1000.0\({}_{ 96.6}\)** & \(-4.05_{ 0.48}\) \\   & BPQL (ours) & **6334.6\({}_{ 246.5}\)** & **4551.9\({}_{ 799.4}\)** & **3336.0\({}_{ 200.3}\)** & **930.3\({}_{ 31.8}\)** & 983.4\({}_{ 33.1}\) & **-3.81\({}_{ 851}\)** \\    & Normal SAC & \(-294.0_{ 46.7}\) & \(26.4_{ 8.5}\) & \(69.0_{ 12.5}\) & \(26.3_{ 3.6}\) & \(16.1_{ 5.9}\) & \(-37.36_{ 3.42}\) \\   & Delayed-SAC & \(1764.3_{ 203.3}\) & \(2.6_{ 6.4}\) & \(513.7_{ 642.2}\) & \(77.6_{ 34.1}\) & \(505.8_{ 333.2}\) & \(-4.01_{ 0.51}\) \\    & Augmented SAC & \(1297.2_{ 265.9}\) & \(1562.9_{ 3075.9}\) & \(1497.8_{ 747.7}\) & \(38.3_{ 4.0}\) & **1000.0\({}_{ 96.6}\)** & \(-4.39_{ 0.54}\) \\    & BPQL (ours) & **5887.5\({}_{ 270.5}\)** & **4104.3\({}_{ 285.7}\)** & **2993.4\({}_{ 566.7}\)** & **935.4\({}_{ 34.6}\)** & \(985.6_{ 28.6}\) & **-3.86\({}_{ 850}\)** \\  

Table 1: Results of MuJoCo benchmark tasks for one million interactions. Each task was evaluated in the delayed environment setting for 3,6, and 9 delayed timesteps \(d\). All tests were repeated with five different random seeds. All results are shown with the standard deviation over the five trials denoted by \(\).

Figure 2: Performance curves of each algorithm for the Walker2d-v3 task. We repeated the test for this task five times with different random seeds. The mean over the results is shown by the solid line, and the shaded area represents the standard deviation. As the delay increases, the proposed algorithm BPQL significantly outperforms other algorithms in terms of asymptotic performance and sample efficiency.

predict the current state. In the learning stage, we train the critic and policy using _true_ transition tuples, i.e., \((s_{t},a_{t},r_{t},s_{t+1})\). The original study used double deep \(Q\)-networks (DDQN)  as the base learning algorithm. However, because DDQN is only applicable to discrete action spaces, we have used SAC for our base learning algorithm, so named this model-based approach _Delayed-SAC_.

Lastly, in the _Normal SAC_ approach, the agent naively uses delayed feedback for training without any modification.

### Performance Comparison

We tested the algorithms on several tasks using the MuJoCo benchmark  and evaluated their performances in environments with different numbers of delayed timesteps.5 Figure 2 shows that the augmented and model-based approaches are inappropriate for environments in which the delayed timestep is large, whereas the proposed BPQL algorithm exhibits significantly better performance in a long-delayed environment. Table 1 lists the overall experimental results, confirming that BPQL outperforms the conventional approaches by a wide margin in environments ranging from a short (\(d\)=3) to a long delay (\(d\)=9).

### Stochastic Environment

We evaluated BPQL and other baselines on the noisy version of the InvertedPendulum-v2 environment, where Gaussian noises were added to the actuator such that the acting becomes stochastic for the same input for the actuator. The added noises to the actuator are sampled from normal distribution with zero mean and standard deviations of 0.1 and 0.15. In this stochastic environment, BQPL has also shown better performance than conventional algorithms, and the difference gap in performance increased as the level of stochasticity grew.

### Performance comparison of augmented approach with various capacity of \(Q\)-network

We conducted additional ablation study to investigate whether the size of \(Q\)-network was too small to learn and extract important features from the augmented state. We tested the performance of Augmented SAC with different hidden size of the \(Q\)-network, including (256,256), (512,512), (1024,1024), and (256,256,256). The results of this additional experiment show that the size of the \(Q\)-network does not significantly affect the performance of the augmented SAC as shown in Figure 3-(b). This confirms that addressing the state space explosion problem is a crucial factor in training the agent in a delayed environment.

## 6 Conclusion

In this study, we proposed a novel model-free algorithm BPQL for a constant delayed environment. BPQL evaluates the beta \(Q\)-function based on the original state-space size rather than evaluating the

Figure 3: **(a) Result of tests on noisy version of InvertedPendulum-v2. The noises added to the actuator are drawn from a normal distribution with a mean of zero and standard deviations of 0.1 (_(a)-left_) and 0.15 (_(a)-right_), respectively. The range of action for the actuator is [-3, 3]. **(b)** The performance curves of BPQL and Augmented SAC with different hidden sizes \(\{(256,256),(512,512),(1024,1024),(256,256,256)\}\) of \(Q\)-network.**

augmented \(Q\)-function, which helps the parameterized \(Q\)-function learn more stably and converge faster. In regards to the MuJoCo benchmark for continuous control tasks, BPQL achieves significantly better performance than the augmented state approach, which is a popular and widely used algorithm for delayed environments. Our results show that BPQL provides a promising avenue for handling delayed environments, such as real-world robot control environments where sensing and actuator delay exist, or a communication system with a narrow bandwidth.

As BPQL cannot be applied to a random delayed environment, it would be meaningful to extend our work to an environment with randomly delayed feedback. Ensemble learning of the beta \(Q\)-functions and augmented state-based policies that cover the entire delay range could be one possible approach. Furthermore, real-world applications using the proposed algorithm is an exciting direction for future work.