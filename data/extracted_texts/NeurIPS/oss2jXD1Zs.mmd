# Linear Time Algorithms for \(k\)-means with Multi-Swap Local Search

Junyu Huang\({}^{1,2}\), Qilong Feng\({}^{1,2,}\), Ziyun Huang\({}^{3}\), Jinhui Xu\({}^{4}\), Jianxin Wang\({}^{1,2,5,}\)

\({}^{1}\)School of Computer Science and Engineering, Central South University,

Changsha 410083, China

\({}^{2}\)Xiangjiang Laboratory, Changsha 410205, China

\({}^{3}\)Department of Computer Science and Software Engineering, Penn State Erie,

The Behrend College

\({}^{4}\)Department of Computer Science and Engineering, State University of New York at Buffalo,

NY, USA

\({}^{5}\)The Hunan Provincial Key Lab of Bioinformatics, Central South University,

Changsha 410083, China

junyuhuangcsu@foxmail.com,csufeng@mail.csu.edu.cn,

zxh201@psu.edu,jinhui@buffalo.edu,jxwang@mail.csu.edu.cn

Corresponding Authors

###### Abstract

The local search methods have been widely used to solve the clustering problems. In practice, local search algorithms for clustering problems mainly adapt the single-swap strategy, which enables them to handle large-scale datasets and achieve linear running time in the data size. However, compared with multi-swap local search algorithms, there is a considerable gap on the approximation ratios of the single-swap local search algorithms. Although the current multi-swap local search algorithms provide small constant approximation, the proposed algorithms tend to have large polynomial running time, which cannot be used to handle large-scale datasets. In this paper, we propose a multi-swap local search algorithm for the \(k\)-means problem with linear running time in the data size. Given a swap size \(t\), our proposed algorithm can achieve a \((50(1+)+)\)-approximation, which improves the current best result 509 (ICML 2019) with linear running time in the data size. Our proposed method, compared with previous multi-swap local search algorithms, is the first one to achieve linear running time in the data size. To obtain a more practical algorithm for the problem with better clustering quality and running time, we propose a sampling-based method which accelerates the process of clustering cost update during swaps. Besides, a recombination mechanism is proposed to find potentially better solutions. Empirical experiments show that our proposed algorithms achieve better performances compared with branch and bound solver (NeurIPS 2022) and other existing state-of-the-art local search algorithms on both small and large datasets.

## 1 Introduction

Clustering is a fundamental problem in the field of machine learning with many real-world applications. The goal of clustering is to partition a given set of data points into different clusters according to their similarity such that data points within the same cluster share high similarity as much as possible. Among different objective functions, the \(k\)-means clustering aims to minimize the sum of the squared distances between data points to their closest centers. More formally, given a setof data points in a \(d\)-dimensional Euclidean space, the goal of the \(k\)-means clustering is to find a set \(C^{d}\) of size at most \(k\) with the following objective: \(_{C^{d}}_{p P}_{e C}\|p-c\|^{2}\).

For the \(k\)-means problem, Lloyd's algorithm  is one of the most widely used heuristic in practice. However, there is no theoretical guarantee for Lloyd-type method unless certain data distribution assumptions are introduced. It is known that there are several constant approximation schemes based on primal-dual and randomized rounding techniques [2; 5; 8]. The current best approximation ratio in polynomial time is 5.912 , which is based on primal-dual method and nested quasi-independent set. For fixed dimension \(d\) or the number of clusters \(k\), several \((1+)\)-approximation algorithms were proposed [6; 7].

The \(k\)-means++ algorithm proposed by Arthur and Vassilvitskii  is a good seeding method that runs in linear time in the data size with \(O( k)\)-approximation. It is also known that the \(k\)-means++ algorithm gives a constant approximation by opening \(O(k)\) centers [1; 13; 16]. Lattanzi and Sohler  showed a combination of \(k\)-means++ seeding and the local search algorithm (named as LS++ algorithm), which yields a constant approximation with running time \(O(ndk^{2} k)\). In each round, LS++ algorithm samples a data point using \(k\)-means++ seeding and enumerates possible swap pairs to make improvements on clustering cost. They proved that after \(O(k k)\) rounds of sampling and swaps, one can obtain a \(509\)-approximate solution in expectation. Choo et al.  proved that one can achieve an \(O(1/^{3})\)-approximation using reduced \(O( k)\) rounds of LS++ algorithm. Under the assumption that each optimal cluster has size \((n/k)\), Huang et al.  gave an improved approximation algorithm with ratio \((100+)\) by random sampling methods.

However, there are still several issues for local search methods. Although current local search methods with multi-swap strategy can achieve good theoretical guarantee, the running time of them have polynomial dependence on the data size, which are hard to be used to handle large-scale datasets. Compared with the \((9+)\)-approximation multi-swap local search algorithm given in , the approximation ratio 509 of LS++ is a large constant since it can only apply the single-swap strategy. Numerical experiments  showed that LS++ could easily fall into a poor local optimum when handling real-world datasets. An immediate idea is to apply the multi-swap strategy to LS++ algorithm for improvements. However, the swapping process of LS++ relies heavily on the one-to-one matched swap pairs defined in . Thus, it is challenging to apply the multi-swap strategy to solve the \(k\)-means problem while maintaining a linear dependence of data size on the running time.

Secondly, in the process of local search swaps, the time and space complexities for clustering cost update during swaps have great impact on the efficiency of the algorithms. A direct way in  is to maintain the nearest and the second nearest centers for each data point such that picking the best swap pair and updating the clustering cost can be implemented in time \(O(nd)\) and \(O(ndk)\), respectively. Maintaining the distances from data points to their centers requires an extra space complexity of \(O(nd)\). To obtain faster implementation, as pointed out in , one can use binary search trees to store the distances from each data point to each of the clustering centers. By using this data structure, each local search step can be implemented in time \(O(nd k)\). However, the space complexity becomes \(O(ndk)\). To get much practical algorithms for the \(k\)-means problem, it is necessary to further improve the time and space complexities for updating the clustering cost during swaps.

### Our Contributions

In order to further narrow the gap between theory and practice, in this paper, we propose the first multi-swap local search algorithm for the \(k\)-means problem with linear running time in the data size. A common feature for the existing multi-swap local search methods is that \(((nk)^{t})\) candidate swaps should be enumerated for finding clustering cost improvements in a single local search step, which leads to at least quadratic running time. To overcome this challenge, our idea is to use a sampling-based strategy to construct a candidate set of centers that are close to the optimal clustering centers in linear time, which serves as the set of potentially good centers for swapping in. Based on the candidate set of centers constructed, enumerations on the current set of centers opened suffice to determine a good swap. Hence, the number of possible swaps can be reduced from \(((nk)^{t})\) to \((k^{t})\), where each local search step can be conducted in linear-time in the data size.

Sampling-based strategy has been used in  for designing single-swap local search algorithms. However, the theoretical analysis relies heavily on the one-to-one matched swap pairs. Thus, the approximation ratio is a large constant since there may exist some optimal clusters that cannot be well approximated by performing matched swaps. In this paper, to obtain better approximation guarantee, we extend the notions of swap pairs and propose a new consecutive sampling method to construct candidate centers for swap such that data points close to a subset of optimal clustering centers can be swapped in simultaneously. A key challenge here is that there may exist some optimal clusters whose clustering costs only take a tiny fraction of the total clustering cost such that sampling methods may fail. To overcome this challenge, we propose new structures that divide optimal clusters into different groups for establishing a lower bound for the success probability of sampling.

By using the proposed multi-swap local search method (denoted as MLS algorithm for short), given a swap size \(t\), an improved \((50(1+)+)\)-approximation can be obtained in time \(O(ndk^{2t+1}(^{-1} k))\). To benefit more from the proposed multi-swap local search method when handling large-scale datasets, we propose a sampling-based method for accelerating the MLS algorithm. The proposed algorithm (denoted as MLSP algorithm) accelerates the updating of clustering cost during the swaps such that each local search iteration in MLS algorithm can be implemented in time \(O(ndk+poly(k)d)\) with extra space complexity of \((kd)\). In order to obtain better clustering quality, we develop a recombination mechanism in MLSP which combines sampling and scoring methods to help the local search algorithm find better solutions when the search falls into a poor local optimum. By picking the top-\(k\) data points with the highest scores as new initialization, the chance to get out of the local optimal solutions becomes large. Numerical experiments show that our proposed method achieves better performances compared with branch and bound solver and other local search algorithms. The main contributions of this paper are as follows.

* We propose the first multi-swap local search algorithm (MLS algorithm) with running time linearly dependent on the data size. Given a swap size \(t\) with \(t 2\), our MLS algorithm achieves a \((50(1+)+)\)-approximation in time \(O(ndk^{2t+1}(^{-1} k))\), which improves the current best approximation ratio 509 with linear running time in the data size.
* We give a practical heuristic algorithm (MLSP algorithm) for better implementation of the proposed MLS algorithm, which accelerates the process of clustering cost update during swaps and provides better scalability to large-scale datasets. Besides, a recombination mechanism is proposed to prevent the local search algorithm falling into a poor local optimum too early.

## 2 Preliminaries

We use \(P^{d}\) and \(k\) to denote the given dataset and the number of clusters, respectively. For any two points \(p\), \(q P\), we use \(d(p,q)=\|p-q\|^{2}\) to denote the squared distance between them. Given two sets \(A\), \(B P\), let \((A,B)=_{p A}_{q B}d(p,q)\) denote the sum of the squared distances from data points in \(A\) to their closest points in \(B\). Let \(C^{*}=\{c_{1}^{*},c_{2}^{*},...,c_{k}^{*}\}\) be an optimal solution. We use \((C^{*})=\{P_{1}^{*},P_{2}^{*},...,P_{k}^{*}\}\) to denote the corresponding optimal clusters by assigning data points in \(P\) to their closest centers in \(C^{*}\). Let \(Opt\) be the cost of the optimal solution. For a subset \(Q(C^{*})\) of optimal clusters, we use \(Z(Q)=_{P_{k}^{*} Q}P_{h}^{*}\) to denote the set of data points in clusters of \(Q\). Denote \(Z^{}(Q)=\{c_{h}^{*}:P_{h}^{*} Q\}\) as the set of optimal centers of clusters in \(Q\). Given a subset \(S C^{*}\), let \(J(S)=\{P_{h}^{*}:c_{h}^{*} S\}\) be the collection of optimal clusters whose clustering centers are in \(G\). For an integer \(t\), let \([t]=[1,2,...,t]\). The following lemma is a folklore for the \(k\)-means problem.

   Result & Approximation Guarantee & Method & Assumption & Running Time \\ 
 & \(O( k)\) & \(k\)-means++ & - & \(O(ndk)\) \\
 & \((3+)^{2}\) & Multi-Swap Local Search & - & \(O(ndk^{t+1}k^{}d)\) \\
 & \(509\) & Sampling + Single-Swap Local Search & - & \(O(ndk^{2}loglogk)\) \\
 & \(O(1)\) & Sampling + Single-Swap Local Search & - & \(O(ndk k)\) \\
 & \(100+\) & Sampling + Single-Swap Local Search & \(|P_{k}^{*}|\) & \(O(ndk^{2}^{*}\)\(^{-1})\) \\  This Paper & \(50(1+)+\) & Sampling + Multi-Swap Local Search & - & \(O(ndk^{2t+1}(^{-1} k))\) \\   

Table 1: Comparison with related results on \(k\)-means clustering, where \(n\) is the size of the given dataset, \(d\) is the dimension, \(k\) is the number of clusters opened, \(t\) is the parameter representing the swap size of local search methods, and \(\) is the aspect ratio (aspect ratio is defined as the maximum pairwise distance of the given instance divided by the minimum pairwise distance).

**Lemma 1**:  Let \(P^{d}\) be a set of data points, and \((P)=_{p P}p\) denote the center of gravity. For any data point \(c^{d}\), we can get \((P,\{c\})=|P|d((P),c)+(P,\{(P)\})\).

**Theorem 1**:  Algorithm 1 returns an \(O( k)\)-approximate solution in time \(O(ndk)\).

**Input**: An instance \((P,k)\) of the \(k\)-means problem.

**Output**: A set \(C^{d}\) of centers with size at most \(k\).

```
1:Randomly sample a point \(p P\) and set \(C=\{p\}\).
2:for\(i=1\) to \(k-1\)do
3: Pick a point \(p P\) with probability \((\{p\},C)/(P,C)\), and add \(p\) to \(C\).
4:return\(C\). ```

**Algorithm 1**\(k\)-means++

## 3 Linear Time Local Search Algorithm with Multi-Swap Strategy

The general idea solving the \(k\)-means problem with multi-swap local search is that we propose a new consecutive sampling method to construct candidate centers for swap such that data points close to a subset of optimal clustering centers can be swapped in simultaneously. The multi-swap local search algorithm (denoted as MLS) is given in Algorithm 2. There are mainly two stages in each round of the MLS algorithm. Given a swap size \(t\), in the first stage (steps 3 to 5), a candidate set of centers with size \(t\) will be sampled using the \(k\)-means++ method. This avoids enumerating all the data points for constructing the candidate sets for swapping in. In the second stage (steps 6 to 7), the algorithm enumerates all subsets (with size at most \(t\)) of the candidate set of centers opened for swapping out. By extending the notions of swap pairs to swap set and carefully analyzing the structures of local optimal solutions, we prove that the clustering cost can be reduced significantly with certain probability in each iteration of the MLS Algorithm. The following is the main result of this paper.

**Theorem 2**: _In the \(i\)-th iteration of Algorithm 2, let \(C^{}\) be the set of centers obtained in step 7. If the current clustering cost \((P,C)\) is larger than \(50(1+)Opt\), then with probability at least \((k^{-t})\), we have \((P,C^{})(1-)(P,C)\). After \(O(k^{O(t)}(^{-1} k))\) iterations, we get an approximate solution with ratio \((50(1+)+)\) in expectation2._

### Analysis

In this subsection, we analyze our proposed Algorithm 2, where a candidate set of centers for swap is constructed by \(t\) independent sampling steps in each iteration. Our objective is to show that the clustering cost can be reduced with certain probability in each iteration. Due to space limit, all the detailed proofs are given in Appendix A.

In the following, we consider a single iteration of the proposed MLS algorithm. Assume \((P,C) 50(1+)Opt\) holds within a single iteration in Algorithm 2. Otherwise, \(C\) is already a \(50(1+)\)-approximate solution for \(P\). Let \(C=\{c_{1},c_{2},...,c_{k}\}\) denote the set of centers before the swap (steps 6-7) of Algorithm 2. Let \((C)=\{P_{1},P_{2},...,P_{k}\}\) be the corresponding partition of clusters induced by \(C\). For an optimal cluster \(P_{h}^{*}\), let \(c_{h}^{*}\) be its clustering center. For each cluster \(P_{h}(C)\), let \(c_{h} C\) denote its clustering center. Following the ones in , we extend the definition of good clusters with respect to \(C^{*}\) as follows.

**Definition 1**: _Good single cluster. A cluster \(P_{h}^{*}(C^{*})\) is called good with a pair of points \((c_{h}^{*},c_{j})\) such that \(c_{j} C\) and \((P_{h}^{*},C)-(P,C_{h}^{*},c_{j})-9(P_{h}^{*},\{c_{h}^{*}\})> (P,C)\), where \((P,C_{h}^{*},c_{j})=(P P_{h}^{*},C\{c_{j}\})- (P P_{h}^{*},C)\) is the reassignment cost by swapping \(c_{j}\) out. Otherwise we say that \(P_{h}^{*}\) is a bad single cluster with \((c_{h}^{*},c_{j})\)._

**Definition 2**: _Good -Clusters. Given an integer \(t\) with \(t 2\), for a collection of optimal clusters \(Q(C^{*})\) with \(|Q| t\), \(Q\) is called good with a pair of sets \((Z^{}(Q),V)\) such that \(V C\), \(|V|=|Z^{}(Q)|\) and \((Z(Q),C)-(P,C,Z^{}(Q),V)-9(Z(Q),C^{*})> (P,C)\), where \((P,C,Z^{}(Q),V)=(P Z(Q),C V)-(P  Z(Q),C)\) is the reassignment cost by swapping the points in \(V\) out. Otherwise we say that \(Q\) is a set of bad \(t\)-clusters with \((Z^{}(Q),V)\)._The above definitions estimate the changes of clustering cost by replacing a set of centers \(V C\) with a set \(Q^{}\) of data points that are close to a subset of optimal clustering centers, where a new clustering is constructed by reassigning data points in \(Z(Q)\) to \(Q^{}\) and data points in \(P Z(Q)\) to \(C V\). The main idea behind Algorithm 2 is to iteratively find data points close enough to the optimal clustering centers for swap to make a reduction on clustering cost by at least \((1-())\). In the following, we will prove that with probability at least \((k^{-t})\), the sampling and swap process induces a significant reduction on clustering cost in each iteration.

We start by dividing the optimal clusters into several groups to give an upper bound of reassignment costs. Given a swap size \(t\), for each optimal center \(c_{h}^{*} C^{*}\), we define \((c_{h}^{*})\) as a mapping function that maps \(c_{h}^{*}\) to its closest center in \(C\). For simplicity, we say that \(c_{h}^{*}\) is captured by \((c_{h}^{*})\). For a center \(c_{j} C\), let \(^{-1}(c_{j})\) be the set of optimal centers captured by \(c_{j}\). If \(|^{-1}(c_{j})|=0\), then \(c_{j}\) is called a lonely center. If \(|^{-1}(c_{j})|=1\), let \(c_{h}^{*}=^{-1}(c_{j})\). Then it is called that \((c_{h}^{*},c_{j})\) forms a type-1 matched swap pair. If \(1<|(c_{j})| t\), let \(_{h}\) be an arbitrary set of unused lonely centers with \(|_{h}|=|^{-1}(c_{j})|-1\). Let \(A=^{-1}(c_{j})\) and \(A^{}=\{c_{j}\}_{h}\). Then, it is called that \((A,A^{})\) forms a type-1 matched swap set. Let \(M_{1}=\{(c_{h}^{*},c_{j}):(c_{h}^{*},c_{j})\) is a type-1 matched swap pair\(\}\) and \(M_{2}=\{(A,A^{}):(A,A^{})\) is a type-1 matched swap set\(\}\) be the collections of type-1 matched swap pairs and type-1 matched swap sets, respectively. If \(|^{-1}(c_{j})|>t\), then find each lonely center \(c_{q} C\) that has not been used for constructing type-1 matched swap pairs or sets. For each lonely center \(c_{q}\) and each \(c_{h}^{*}^{-1}(c_{j})\), \((c_{h}^{*},c_{q})\) forms a type-2 matched swap pair. We use \(M_{3}=\{(c_{h}^{*},c_{q}):(c_{h}^{*},c_{q})\) is a type-2 matched swap pair\(\}\) to denote the set of type-2 matched swap pairs. For a set \(V C\) of centers, let \(X(V)=_{c_{h} V}P_{h}\) be the set of data points in \(P\) whose closest centers are in \(V\). Given a subset \(S C\) of clustering centers, we also use \(J(S)=\{P_{h}:c_{h} G\}\) to denote the set of clusters whose centers are in \(S\). The following lemma gives upper bounds of reassignment cost by matched swap pairs or sets.

**Lemma 2**: _Given a type-1 or type-2 matched swap pair \((c_{h}^{*},c_{j})\), it holds that \((P,C,c_{h}^{*},c_{j}) 24(P_{j},C^{*})+(P_{j},C)\). Given a type-1 matched swap set \((Q,V)\), it holds that \((P,C,Q,V) 24(X(V),C^{*})+(X(V),C)\)._

Let \(H_{1}=\{c_{h}^{*}:(c_{h}^{*},c_{j}) M_{1}\}\) be the set of optimal centers that participate in constructing type-1 matched swap pair. Let \(H_{2}=\{A:(A,A^{}) M_{2}\}\) be the collection of the subsets of optimal centers that participate in constructing type-1 matched swap set. Let \(L=\{c_{h}^{*}:(c_{h}^{*},c_{q}) M_{3}\}\) be the set of optimal centers that participate in constructing type-2 matched swap pair. Let \(H_{1}^{}=\{c_{j}:(c_{h}^{*},c_{j}) M_{1}\}\) be the set of centers in \(C\) that participate in constructing type-1 matched swap pair. Let \(H_{2}^{}=\{A^{}:(A,A^{}) M_{2}\}\) be the collection of the subsets of centers in \(C\) that participate in constructing type-1 matched swap pair.

During the sampling and swap process in steps 4-7 of Algorithm 2, there are two cases that may happen: (1) \(\ c_{h}^{*} L\) such that \(P_{h}^{*}\) is a good single cluster with a type-2 matched swap pair \((c_{h}^{*},c_{q}) M_{3}\); (2) \(\ c_{h}^{*} L\), \({P_{h}^{*}}^{h}\) is a bad single cluster with any type-2 matched swap pair \((c_{h}^{*},c_{q}) M_{3}\). We will discuss the two cases separately in the following. If case (1) happens, we first show that with probability at least \(()\), the clustering cost can be reduced at least by \(1-()\). Let \(c_{h}^{*} L\) be an optimal center such that \(P_{h}^{*}\) is a good single cluster with a type-2 matched swap pair \((c_{h}^{*},c_{q}) M_{3}\). By the definition of good single cluster, we have \((P_{h}^{*},C) 9(P_{h}^{*},\{c_{h}^{*}\})\).

Define \((P_{h}^{*})=\{p P_{h}^{*}:d(p,c_{h}^{*})^{*},[c_{h }^{*}])}{|P_{h}^{*}|}\}\) as the set of data points in \(P_{h}^{*}\) that are close to the optimal center \(c_{h}^{*}\). Observe that \(|(P_{h}^{*})||P_{h}^{*}|\). Otherwise, the clustering cost of data points in \(P_{h}^{*}(P_{h}^{*})\) is at least \((P_{h}^{*},\{c_{h}^{*}\})\) using \(c_{h}^{*}\) as center, which contradicts with \((P_{h}^{*}(P_{h}^{*}),\{c_{h}^{*}\})<(P_{h}^{*},\{c_ {h}^{*}\})\).

Next, we will show that whenever an optimal cluster \(P_{h}^{*}\) has large clustering cost with respect to \(C\), i.e., \((P_{h}^{*},C)=b(P_{h}^{*},\{c_{h}^{*}\})\) for a real number \(b 3\), it suffices to use data points in \((P_{h}^{*})\) to approximate the clustering cost of \(P_{h}^{*}\).

**Lemma 3**: _Let \(P_{h}^{*}\) be an optimal cluster with \((P_{h}^{*},C)=b(P_{h}^{*},\{c_{h}^{*}\})\) for a real number \(b 3\). Then, \(((P_{h}^{*}),C)(b-1)(P_{h}^{*},\{c_{h}^{*}\})\)._

We now argue that the clustering cost of each good single cluster takes a certain fraction of the total clustering cost. Then, by sampling according to the squared distances, with good probability, data points close to the center of a good single cluster can be sampled. According to the definition of good single cluster, we can assume that \((P_{h}^{*},C)=b(P_{h}^{*},\{c_{h}^{*}\})\) for a real number \(b 9\). By Lemma 3, we know that \(((P_{h}^{*}),C)(P_{h}^{*},\{c_{h}^{*}\})= (P_{h}^{*},C)(P_{h}^{*},C)\). By the definition of good single cluster, we also have \((P_{h}^{*},C)(P,C)\), which implies that \(((P_{h}^{*}),C)(P,C)\). Thus, in each round of the sampling process in steps 4-5 of Algorithm 2, with probability at least \(()\), we can sample a point \(q(P_{h}^{*})\) for a good single cluster \(P_{h}^{*}\) with a type-2 matched swap pair \((c_{h}^{*},c_{q}) M_{3}\). The following lemma shows that, by swapping \(q\) with \(c_{q}\) and assigning all the data points in \(P_{h}^{*}\) to \(q\), the clustering cost can be reduced at least by \(1-()\).

**Lemma 4**: _By swapping \(q\) with \(c_{q}\), the clustering cost of \((P,C)\) can be reduced at least by \(1-()\)._

We have shown that if case (1) happens, we can sample a data point \(q\) close to the center of a good single cluster \(P_{h}^{*}\) to make the clustering cost reduced significantly. Next, we assume that case (1) never happens and case (2) happens. In case (2), the idea behind is to sample data points close to the optimal centers in \(H_{1}\) or sets of optimal centers in \(H_{2}\) to reduce the clustering cost. We first bound the clustering cost of optimal clusters in \(J(L)\).

**Lemma 5**: _If case (2) happens, then \((X(L),C)(1+)(9(X(L),C^{*})+24(X(L^{}),C ^{*})+(X(L^{}),C)+(P,C))\)._

Now, consider a set of centers \(Q H_{2}\). We will divide the optimal clusters in \(J(Q)\) into two groups. Let \(Q_{L}=\{P_{h}^{*} J(Q):(P_{h}^{*},C)} (P,C)\}\) be the set of optimal clusters in \(J(Q)\) with large clustering cost with respect to \(C\) and \(Q_{S}=\{P_{h}^{*} J(Q):(P_{h}^{*},C)<}(P,C)\}\) be the set of optimal clusters in \(J(Q)\) with small clustering cost with respect to \(C\), respectively. We define \(Q_{S}^{}=\{P_{h}^{*} Q_{S}:(P_{h}^{*},C)<3(P_{h}^{*},\{c_ {h}^{*}\})\}\) as the set of optimal clusters in \(Q_{S}\) whose clustering centers are close to one of the centers in \(C\). Let \(Q_{S}^{}=Q_{S} Q_{S}^{}\) and \(Q_{T}=Q_{L} Q_{S}^{}\), respectively. We first show that, it suffices to only consider optimal clusters in \(Q_{T}\).

**Lemma 6**: _Let \(Q H_{2}\) be a set of centers in \(H_{2}\), where \(J(Q)\) is a set of good \(t\)-clusters with a type-1 matched swap set \((Q,A^{}) M_{2}\). Define \(V=A^{}\{c_{j}\}\), where \(c_{j}\) is the center in \(A^{}\) with \(|^{-1}(c_{j})|>1\). Let \(U P\) be the set of data points with \(|U|=|V|\) such that \(U(P_{h}^{*})\) holds for each \(P_{h}^{*} Q_{T}\). Then, \((P,C V U)(1-)(P,C)\)._

Note that there are \(t\) sampling iterations in each step 4 of Algorithm 2. Let \(H_{2}^{*}=\{P_{h}^{*}:P_{h}^{*} Q_{T},Q H_{2}\}\) and \(H_{t}=J(H_{1}) H_{2}^{*}\). We will define a mapping function \(m\) which maps each \(P_{h}^{*} H_{t}\) to an integer \(m(P_{h}^{*})[t]\). For each \(P_{h}^{*} H_{t}\) such that \(c_{h}^{*} H_{1}\), we define \(m(P_{h}^{*})=1\). Then, consider each \(Q H_{2}\). For each \(P_{h}^{*} Q_{T}\), we define \(m(P_{h}^{*})=i\) such that \(i[|Q_{T}|]\) and \(m(P_{h}^{*}) m(P_{j}^{*})\) for any two optimal clusters \(P_{h}^{*}\), \(P_{j}^{*} Q_{T}\). For each \(Q H_{2}\), let \(p_{s}(Q)\) be the success probability that for each \(P_{h}^{*} Q_{T}\), a data point is sampled from \((P_{h}^{*})\) in the \(m(P_{h}^{*})\)-th iteration of Algorithm 2. For each \(c_{h}^{*} H_{1}\), let \(p_{s}(c_{h}^{*})\) be the success probability that a data point is sampled from \((P_{h}^{*})\) in the first iteration (note that in this case \(m(P_{h}^{*})=1\)) of step 4. Let \(H_{G}^{1}=\{c_{h}^{*} H_{1}:P_{h}^{*}\}\) is a good single cluster with the type-1 matched swap pair \((c_{h}^{*},c_{j}) M_{1}\) by the set of centers in \(H_{1}\) whose optimal clusters are good single clusters with type-1 matched swap pairs in \(M_{1}\). Let \(H_{G}^{2}=\{Q H_{2}:J(Q)\) is a set of good \(t\)-clusters with the type-1 matched swap set \((Q,V) M_{2}\}\) be the collection of the sets of centers in \(H_{2}\) whose corresponding optimal clusters are good \(t\)-clusters with type-1 matched swap sets in \(M_{2}\). Define \(p_{s}^{f}=_{c_{h}^{*} H_{G}^{1}}p_{s}(c_{h}^{*})+_{Q H_{G}^{2}}p_{s}(Q)\) as the summation of the success probability. Since all the optimal clusters in \(J(L)\) belong to bad single cluster, and the events defined related to \(p_{s}(Q)\) or \(p_{s}(c_{h}^{*})\) for each \(Q H_{2}\) and \(c_{h}^{*} H_{1}\) are mutually exclusive, \(p_{s}^{f}\) gives a lower bound success probability to get a \((1-)\) reduction on the clustering cost in each iteration of steps 2-7. In the following, we will show how to obtain a lower bound for \(p_{s}^{f}\).

Consider a set \(Q H_{2}\) of optimal centers such that \(J(Q)\) is a set of good \(t\)-clusters with a type-1 matched swap set \((Q,A^{})\). There are two subcases that may happen: (1) \(\ P_{h}^{*} J(Q)\) such that \(P_{h}^{*}\) is a good single cluster with a swap pair \((c_{h}^{*},c_{j})\), where \(c_{j}\) is a lonely center used for constructing type-1 matched swap set; (2) \(\ P_{h}^{*} J(Q)\), \(P_{h}^{*}\) is a bad single cluster with any swap pair \((c_{h}^{*},c_{j})\), where \(c_{j}\) is a lonely center used to construct type-1 matched swap set. In subcase (1), since \(t\) is usually a constant and could be much smaller than \(k\), with probability at least \((k^{-1})\), a data point \(q(P_{h}^{*})\) can be sampled in the first iteration of step 4 in Algorithm 2 for swap to make the clustering cost reduced at least by \((1-)\) according to Lemma 4. Next, we assume that subcase (1) never happens and subcase (2) happens. In subcase (2), our objective is to sample a set \(V\) of data points such that \(V\) contains at least one point from \((P_{h}^{*})\) for each \(P_{h}^{*} Q_{T}\). For each optimal cluster \(P_{h}^{*} Q_{L}\), with probability at least \((k^{-1})\), we can sample a data point \(q(P_{h}^{*})\) in the \(m(P_{h}^{*})\)-th iteration of step 4 in Algorithm 2. Thus, the probability can be bounded by \((k^{-|Q_{L}|})\). Then, we only need to consider optimal clusters in \(Q_{S}\). By Lemma 6, it suffices to consider optimal clusters in \(Q_{S}^{f}\). The following lemma gives an upper bound of the failure probability of not sampling a data point from \((P_{h}^{*})\) for each \(P_{h}^{*} Q_{S}^{}\) in the \(m(P_{h}^{*})\)-th iteration in step 4 of Algorithm 2.

**Lemma 7**: _Given a set \(Q H_{2}\) of centers such that \(J(Q)\) is a set of good \(t\)-clusters, the probability that step 4 of Algorithm 2 fails to sample a data point \(q\) from \((P_{h}^{*})\) for each \(P_{h}^{*} Q_{S}^{}\) in the \(m(P_{h}^{*})\)-th iteration is at most \((1+)e^{-(Z(Q_{S}^{}),C)/(300(P,C))}\)._

Now we can bound the success probabilities of \(p_{s}(Q)\) and \(p_{s}(c_{h}^{*})\) for each \(Q H_{2}\) and \(c_{h}^{*} H_{1}\). We will divide the optimal clusters in \((C^{*}) J(L)\) into two different groups \(H_{G}\) and \(H_{B}\). Firstly, we consider the centers in \(H_{1}\). For a center \(c_{h}^{*} H_{1}\), if \(P_{h}^{*}\) is a good single cluster with the type-1 matched swap pair \((c_{h}^{*},c_{j}) M_{1}\), then add \(P_{h}^{*}\) to \(H_{G}\). Otherwise add \(P_{h}^{*}\) to \(H_{B}\). For a set \(Q H_{2}\) of optimal clustering centers, if \(J(Q)\) is a set of good \(t\)-clusters with the type-1 matched swap set \((Q,V) M_{2}\), then add each \(P_{h}^{*} Q_{S}^{}\) to \(H_{G}\) and each \(P_{h}^{*} J(Q) Q_{S}^{}\) to \(H_{B}\). If \(J(Q)\) is a set of bad \(t\)-clusters with the type-1 matched swap set \((Q,V) M_{2}\), then add each \(P_{h}^{*} J(Q)\) to \(H_{B}\). The following lemma argues that the summation clustering cost of the optimal clusters in \(H_{G}\) is large.

**Lemma 8**: _For the optimal clusters in \(H_{G}\), we have \((Z(H_{G}),C)(P,C)\)._

For a good single cluster \(P_{h}^{*}\) with the type-1 matched swap pair \((c_{h}^{*},c_{j}) M_{1}\) where \(c_{h}^{*} H_{1}\), define \(p_{f}(c_{h}^{*})=1-p_{s}(c_{h}^{*})\) as the probability that the first iteration in step 4 of Algorithm 2 fails to sample a data point \(q(P_{h}^{*})\). Then, we have \(p_{f}(c_{h}^{*}) e^{-p_{s}(c_{h}^{*})} e^{-(P_{h}^{*},C)/300 (P,C)}\) by Lemma 3. For a set \(Q H_{G}^{2}\) of optimal centers, we have that \(p_{f}(Q_{S}^{})\) is the failure probability of not sampling a data point from \((P_{h}^{*})\) for each \(P_{h}^{*} Q_{S}^{}\) in the \(m(P_{h}^{*})\)-th iteration of step 4 in Algorithm 2. Recall that \(p_{s}^{f}=_{c_{h}^{*} H_{G}^{2}}p_{s}(c_{h}^{*})+_{Q H_{G}^{2}}p_ {s}(Q)\) is the summation of success probability. Then, we have \(p_{s}^{f}(k^{-t})(_{c_{h}^{*} H_{G}^{1}}p_{s}(c_{h}^{*})+ _{Q H_{G}^{2}}p_{s}(Q_{S}^{}))\). Let \(p_{s}^{f^{}}=_{c_{h}^{*} H_{G}^{1}}p_{s}(c_{h}^{*})+_{Q H_{G} ^{2}}p_{s}(Q_{S}^{})\). Observe that \(p_{s}^{f^{}} 1-_{c_{h}^{*} H_{G}^{1}}(1-p_{s}(c_{h}^{*}))_{Q H_{G} ^{2}}(1-p_{s}(Q_{S}^{}))\). Since there are at most \(\) good \(t\)-clusters, by Lemma 7, we have \(p_{s}^{f^{}} 1-(1+)^{}e^{- ),C)}{300(P,C)}} 1-e^{(1+)- ),C)}{300(P,C)}} 1-e^{} 1-e^{-}\), where the third inequality follows from Lemma 8 and \((1+x) x\). Then, it holds that \(p_{s}^{f}(k^{-t})p_{s}^{f^{}}=(k^{-t})\), which indicates that with probability at least \((k^{-t})\), we can sample data points close to a set of good \(t\)-clusters or a good single cluster for swap to make the clustering cost reduced at least by \((1-)\) according to Lemma 6. Putting all things together, Theorem 2 can be proved (Detailed proof of Theorem 2 is given in Appendix A).

**Running Time Analysis.** By Theorem 2, in order to obtain a \((50(1+)+)\)-approximate solution, the iteration rounds for Algorithm 2 should be \(O(k^{t+1}(^{-1} k))\). In each iteration, it takes \(O(ndk)\) time to update the distances between data points to their closest centers. During the sampling process, \(t\) data points are sampled according to the \(D^{2}\)-Sampling distribution to serve as the candidate set of centers for swapping in, which takes time \(O(nt)\) if the distances from data points to their centers are already known. It takes \(O(k^{t})\) time to enumerate each subset with size at most \(t\) of the set of current centers opened. It takes \(O(ndk^{t})\) time to recalculate the clustering cost after each swap if \(t\) nearest centers of each data point are maintained during the whole process. Thus, the total running time of Algorithm 2 is \(O(ndk^{2t+1}(^{-1} k))\).

### Accelerating Multi-Swap Local Search for \(k\)-means

In this subsection, we provide a more practical algorithm for the \(k\)-means problem to accelerate the proposed multi-swap local search process. The algorithm is given in Algorithm 3. The main idea behind is to use sampling-based methods to obtain fast clustering cost updating during swaps. In step 7 of Algorithm 3, a small sample set \(S\) of size \(\) is randomly taken from \(P\). For clustering cost updating, instead of calculating the clustering cost of all the data points in \(P\), we use the clustering cost of \(S P\) as an estimation. This reduces the time for picking the best swap pair during a single local search iteration from \(O(nd)\) to \(O(poly(k)d)\). Then, in steps 16-18, we design a recombination method to find better initialization which prevents the local search algorithm from falling into a poor local optimum too early. In step \(16\) of Algorithm 3, we randomly take a set \(D P\) of centers with size \(O(k k)\). Let \(C_{1}=C D\) be the set of the new candidate centers. For each center \(c_{h} C_{1}\), we add a score of \((P_{h},C)/(P,C)\) to it. Similarly, for each center \(c_{h} C\), we also add a score \((P_{h},C)/(P,C)\) to it. Then, by giving each center in \(C_{1}\) and \(C\) a score weight of 0.75 and 0.25, respectively, we pick the top-\(k\) data points with the highest scores as a new initialization of clustering centers to find potentially better clustering costs until convergence.

**Input**: An instance \((P,k)\) of the \(k\)-means problem, parameters \(T\), \(t\), \(R^{}\), \(\) and \(\).

**Output**: A set \(C^{d}\) of centers with size at most \(k\).

```
1: Initialize \(C=k\)-means++\((P,k)\), \(r=0\), \(C_{f}=\).
2:while\(r<R^{}\)do
3:for\(i=1\) to \(T\)do
4:\(I=\).
5:for\(j=1\) to \(t\)do
6: Pick a point \(p P\) with probability \((\{p\},C)/(P,C)\), and add \(p\) to \(I\).
7: Randomly sample a set \(S\) from \(P\) of size \(\).
8: Let \((U,V)\) be a swap set such that \(U I\), \(V C\), \(|U|=|V|\) and \((S,C V U)\) is minimized.
9:if\((P,C V U)<(1-)(P,C)\)then
10:\(C=C V U\).
11: For each center \(c C\), find the \(50\)-nearest neighbors in \(P\) to \(C\) for improvements on clustering cost by swapping \(c\) with one neighbor until convergence.
12:if\((P,C)<(P,C_{f})\)then
13:\(C_{f}=C\).
14:else
15:\(r=r+1\), randomly sample a set \(D\) from \(P\) with size \(\), and set \(C_{1}=C D\).
16: For each \(c_{h} C\), calculate \(S^{}(c_{h})=,C)}{(P,C)}\), and add a score of 0.25\(S^{}(c_{h})\) to \(c_{h}\).
17: For each \(c_{h} C_{1}\), calculate \(S^{}(c_{h})=,C)}{(P,C)}\), and add a score of 0.75\(S^{}(c_{h})\) to \(c_{h}\).
18: Reset \(C\) as data points in \(C_{1}\) with top \(k\) scores.
19:return\(C_{f}\). ```

**Algorithm 3** MLSP

## 4 Experiments

In this section, we compare our proposed algorithms with the branch and bound solver and other local search methods. For hardware, all the experiments are conducted on 72 Intel Xeon Gold 6230 CPUs with 500GB memory.

**Datasets** We evaluate the performance of our algorithms on 8 datasets used in  with sizes over 50,000, two datasets SUSY (5,000,000 \(\) 17) and HIGGS (11,000,000 \(\) 27) from the UCI Machine

[MISSING_PAGE_FAIL:9]

The experimental results on the performances with varying \(T\) and \(R^{}\) (Appendix B.1) show that larger \(T\) and \(R^{}\) will not influence the results too much. In general, larger sampling rounds and larger failure upper bound can result in potentially better solutions with higher running time. For parameters \(\) and \(\), the results (Appendix B.1) show that the performances of our proposed algorithms are almost the same for different choices of \(\) and \(\). Tables 3, 4 and 5 show the comparison results of different algorithms with varying iteration rounds on dataset rds, KEGG and Urban_10, respectively. The results show that, our proposed MLSP algorithm always achieves the best clustering cost compared with LS++ and MLS algorithms. Tables 6, 7, and 8 show the results of MLSP algorithm with varying number of failure upper bound \(R^{}\) for fixed \(=0.5\), \(=0.5\) and \(T=400\). It can be seen that a larger failure upper bound will lead to smaller deviation, and the running time becomes higher. Tables 9, 10, and 11 show the results of MLSP algorithm with varying parameters \(\) and \(\) for fixed \(T=400\) and \(R^{}=5\) on dataset rds, KEGG, and Urban_10, respectively. It can be seen that smaller values of \(\) and \(\) result in better performances on clustering cost with smaller deviation, and the running time becomes higher. The experimental results on small datasets (Appendix B.2) suggest that the proposed MLSP method not only outperforms other algorithms in terms of clustering quality but also runs much faster than the BB solver. The experimental results on the performances with different values of \(k\) (Appendix B.3) show that our proposed MLSP algorithm can still achieve the best clustering quality for smaller values of \(k\). The experimental results on the performances with fixed time limit (Appendix B.4) show that our proposed MLSP algorithm achieves the best clustering quality within any given time constraints.

Tables 12 and 13 show the results on 18 datasets with sizes smaller than \(50,000\) using \(k=10\). It can be seen that, for each dataset, the best clustering cost returned by our MLSP algorithm is smaller than BB method and other local search methods. For each dataset, the average clustering cost returned by our MLSP algorithm nearly matches the result of the BB method. As for FLS, although the performance of FLS is better than that of LS++, our proposed MLSP algorithm improves the performance of FLS on clustering cost with smaller deviation on most datasets. As for running time, there is no significant difference on running time among different local search algorithms for small datasets. Tables 14 and 15 present the performances of different local search algorithms with \(k=3\) on small datasets. Table 18 presents the performances of different local search algorithms with \(k=3\) on large datasets. Tables 16 and 17 present the performances of different local search algorithms with \(k=5\) on small datasets. Table 19 presents the performances of different local search algorithms with \(k=5\) on large datasets. It can be seen that, our proposed MLSP algorithm achieves the best clustering performance on most datasets with different values of \(k\). On each dataset, the best clustering cost returned by our MLSP algorithm matches the clustering cost of BB method. On each dataset, the average clustering cost returned by our MLSP algorithm nearly matches the result of the BB method. As for running time, there is no significant difference on running time among all local search algorithms on small datasets. However, as the data sizes grow, our proposed MLS algorithm becomes much faster than other local search algorithms.

## 5 Conclusion

In this paper, we propose fast local search algorithms for the \(k\)-means problem with multi-swap strategy, which runs in linear time in the data size. We develop new sampling techniques, which accelerate the process of clustering cost update during swaps. By proposing a recombination mechanism, the proposed algorithm can find potentially better solutions. Experimental results show that our algorithms achieve better performance on both small and large datasets compared with the state-of-the-art algorithms. An interesting future direction is how to design fast local search approximation algorithms for handling high dimensional clustering datasets.