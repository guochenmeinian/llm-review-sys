# Trap-MID: Trapdoor-based Defense against Model Inversion Attacks

Zhen-Ting Liu

National Taiwan University

r11922034@csie.ntu.edu.tw &Shang-Tse Chen

National Taiwan University

stchen@csie.ntu.edu.tw

###### Abstract

Model Inversion (MI) attacks pose a significant threat to the privacy of Deep Neural Networks by recovering training data distribution from well-trained models. While existing defenses often rely on regularization techniques to reduce information leakage, they remain vulnerable to recent attacks. In this paper, we propose the **Trapdoor-based Mo**el **I**nversion **D**efense (Trap-MID) to mislead MI attacks. A trapdoor is integrated into the model to predict a specific label when the input is injected with the corresponding trigger. Consequently, this trapdoor information serves as the "shortcut" for MI attacks, leading them to extract trapdoor triggers rather than private data. We provide theoretical insights into the impacts of trapdoor's effectiveness and naturalness on deceiving MI attacks. In addition, empirical experiments demonstrate the state-of-the-art defense performance of Trap-MID against various MI attacks without the requirements for extra data or large computational overhead. Our source code is publicly available at https://github.com/ntuaislab/Trap-MID.

## 1 Introduction

Deep Neural Networks (DNNs) have been successfully applied in various domains. However, training DNNs could involve sensitive data like facial recognition and medical diagnosis, which raises privacy concerns. Model Inversion (MI) stands as one of the important privacy attacks aimed at reconstructing private data within specific classes from a well-trained model. For example, an adversary may recover the training images of specific identities from a facial recognition system.

MI attacks were first introduced by Fredrikson et al. [1; 2], reconstructing private attributes from low-capacity models. After that, Zhang et al.  proposed Generative Model-Inversion (GMI) attacks to reconstruct private images from DNNs, utilizing Generative Adversarial Network (GAN) as a general prior. This GAN-based framework has been widely adopted by later attacks [4; 5; 6; 7; 8; 9; 10; 11]. Among them, PLG-MI  achieves state-of-the-art attack performance. Previous works also demonstrated the efficacy of MI attacks under black-box [9; 10; 12] or label-only [11; 13] settings. In this paper, we focus on defending against white-box attacks, which pose a more challenging scenario.

Most existing defenses focus on reducing the information leakage through Differential Privacy (DP) [1; 3], dependency regularization [14; 15], or manipulating the loss landscape . However, these methods remain vulnerable to recent MI attacks . In contrast, recent works proposed to mislead MI attacks by prompting models to classify fake samples as the protected class with high confidence [17; 18; 19]. Although effective, these misleading-based strategies face challenges, including additional data requirements and substantial computational overhead. Furthermore, they typically protect only a single or a limited set of classes, while other defenses aim to secure all classes simultaneously.

Sharing a similar idea, Shan et al.  introduced Trapdoor-enabled Adversarial Detection (TeD) against targeted adversarial attacks, which aims to change the model behaviors by applying adversarial perturbations to the input data. Instead of training a robust model against such perturbations, TeDshows that injecting trapdoors into the models can mislead the adversarial attacks to result in samples with similar features to poisoned data, thereby empowering the adversarial detection by measuring their similarity to the trapdoor signatures.

Inspired by previous misleading-based defenses [17; 18; 19] and TeD , we propose **Trapdoor**-based **M**odel **I**nversion **D**efense (Trap-MID), which deceives MI attacks by incorporating trapdoors as the "shortcuts". We discuss the key properties of trapdoor triggers necessary for misleading these attacks, and experiments show that Trap-MID outperforms existing methods in defending against MI attacks.

Our contributions can be summarized as follows:

1. We propose a trapdoor-based defense, Trap-MID, to preserve privacy by misleading MI attacks. Through extensive experimentation, it presents state-of-the-art defense performance against various MI attacks.
2. To the best of our knowledge, we are the first to establish the connection between MI defenses and trapdoor injection techniques. We theoretically discuss the importance of trapdoor effectiveness and naturalness in misleading MI attacks and showcase its efficacy with empirical experiments.
3. Compared to previous trapping defenses, our trapdoor-based framework is more computationally and data-efficient, without large computational overhead or additional data.

## 2 Related Work

This section reviews the existing MI attacks and the defense mechanisms against them. Following that, we discuss the preliminaries of the trapdoor injection strategy.

### Model Inversion Attacks

Fredrikson et al. [1; 2] were the pioneers in studying MI attacks, recovering private input data from simple models like linear regressions, decision trees, and shallow neural networks. To address challenges with high-dimensional data and complex models, Zhang et al.  proposed Generative Model-Inversion (GMI) attacks, training a GAN on an auxiliary dataset as a generic prior, and optimizing latents to reconstruct training images from DNNs. Latter attacks have largely adopted this GAN-based framework [4; 5; 6; 7; 8; 9; 10; 11]. VMI  treats MI attacks as a variational inference problem, presenting a unified framework with deep normalizing flows to improve attack performance. KED-MI  employs semi-supervised GAN to distill knowledge about private priors using soft labels from victim models. PPA  leverages a pre-trained StyleGAN2 generator  to relax the dependency between target models and image priors. LOMMA  was proposed to maximize output logits and apply model augmentation with Knowledge Distillation (KD) to address sub-optimal objectives and "MI overfitting" issues. PLG-MI  adopts conditional GAN (cGAN) to explicitly decouple the search space for different classes. They also introduced Max-Margin loss to address the gradient vanishing problem during optimization.

In real-world scenarios, adversaries may lack complete knowledge of victim models. Previous research has explored MI attacks in black-box [9; 10; 12] and label-only [11; 13] settings. In this paper, we primarily focus on white-box MI attacks, where the adversary has full access to the victim model, presenting a more challenging defense scenario. Among them, PLG-MI  currently stands as the state-of-the-art attack.

### Defenses against Model Inversion Attacks

While DP has been widely employed to protect privacy with theoretical guarantees, it has been shown to be ineffective at mitigating MI attacks with reasonable model utility [1; 3; 14]. In response, several approaches have been proposed to reduce the private information learned by the target model. MID  was introduced to restrict the mutual information between model inputs and outputs, thereby reducing the information leakage about input data from its predictions. BiDO  further enhances the utility-privacy trade-off by minimizing dependency between inputs and intermediate embeddings while maximizing that between embeddings and outputs. TL-DMI  demonstrates that freezing certain layers during fine-tuning can prevent private information encoded in those layers, making it difficult to extract. RoLSS  reveals that skip connections in modern DNNs strengthen MI attacks and compromise data privacy, suggesting their removal in the final stage as an MI-resilient architecture design. Additionally, Struppek et al.  found that negative label smoothing (NegLS) encourages over-confidence in models, reducing the guidance signal available for MI attacks. However, recent MI attacks like PLG-MI remain challenging for these defenses . Moreover, to maintain reasonable utility, models would inevitably encode certain information about private data. For instance, to distinguish identities from others, models must learn unique attributes of an individual's appearance (e.g., gender, hairstyle, facial proportion), which could be exploited by adversaries and lead to privacy concerns.

Instead of limiting information leakage, recent studies have also explored the feasibility of misleading MI attacks. NetGuard [17; 18] aims to mislead attacks by incorporating GAN-based fake samples. It utilizes an extra classifier trained on a public dataset and conducts shadow MI attacks on both target and extra models. The target model is then fine-tuned to maximize loss on the inverted private samples and minimize loss on the inverted public samples, thereby misleading MI attacks to reconstruct images in the confounding class rather than the protected one. Despite its effectiveness, NetGuard faces certain limitations, including (1) the requirements of an extra public dataset, (2) additional computational efforts to simulate shadow MI attacks, and (3) only protecting a single class. Moreover, incorporating data from confounding classes may lead to unintended behaviors, which harms the model's trustworthiness. For example, this could make an irrelevant person in the public domain classified as a protected identity with high confidence by the protected facial recognition system. Sharing a similar idea, Chen et al.  introduced Data-Centric Defense (DCD) to mitigate MI attacks. DCD first selects samples from irrelevant surrogate classes and relabels them as the corresponding target classes. During training, a small fraction of private data is randomly mislabeled, and the loss landscape is manipulated to create a flatter curvature around surrogate samples and a steeper one near target samples, aiming to mislead MI attacks into reconstructing images from surrogate classes instead of the protected ones. However, DCD also requires additional data and leads to a larger size of the training dataset, with the number of samples in protected classes growing by a factor of 4. This makes it limited in the number of classes to protect.

Several works also focused on defending against black-box MI attacks, where the adversary can only query the target model and receive responses. Defense mechanisms include purifying prediction vectors  or injecting adversarial noise to counteract attacks .

### Backdoor Attacks

Backdoor attacks involve embedding backdoors into target models so that they behave normally on benign samples, but specific triggers will maliciously change their predictions . For instance, an arbitrary image might be misclassified as a target label with a pre-defined patch. Despite their security threats, Shan et al.  showed that backdoors can help detect adversarial examples. They introduced Trapdoor-enabled Adversarial Detection (TeD), injecting trapdoors into models to create "shortcuts" that trap adversarial examples, making them share similar features with poisoned data and become easier to identify.

Inspired by the idea behind NetGuard [17; 18], DCD , and TeD , we explore the potential of trapdoors and their essential properties in defending against MI attacks.

## 3 Methodology

### Problem Setup

Target ClassifierIn a classification problem with data distribution \(p(X,Y)\) consisting of input data \(X^{d_{x}}\) and labels \(Y^{d_{y}}\), the model owner trains a classifier \(f_{}:^{d_{x}}^{d_{y}}\) parameterized by weights \(^{d_{}}\) to minimize the following loss function:

\[_{}_{(X,Y) p(X,Y)}[(f_{}(X),Y)],\] (1)

where \(:^{d_{y}}^{d_{y}}\) denotes a loss function such as the cross-entropy loss.

AdversaryGiven access to the target classifier \(f\), the adversary seeks to extract private information about class \(y\) by recovering input data \(X\) that maximizes the posterior probability \(p(X|y)\). Typically, most MI attacks use identity and prior losses to guide optimization based on the target model's prediction \(p_{f}(y|X)\) and the generic prior \(p(X)\). For example, in facial recognition, the adversary might utilize a public face dataset [3; 4; 5; 7; 8; 10; 11; 12; 13] or a pre-trained face generator [6; 9]. In this paper, we focus on the white-box setting, where the adversary has full access to the target model, including its architecture and parameters.

### Motivation and Overview

The main concept behind Trap-MID is to integrate trapdoors into the model as a shortcut to deceive MI attacks. Figure 0(a) illustrates the intuition: During MI attacks, the adversary seeks to explore private distribution (blue area) from public data (orange area). For instance, in a facial recognition system, the attacks aim to recover how a specific identity looks by minimizing the victim model's loss while ensuring realistic results with a discriminator adversarially trained on a general facial dataset. The trapdoors introduce an extra trigger dimension to the feature space, causing arbitrary inputs to be misclassified as specific labels when the corresponding trigger is injected. Once trigger features can be embedded by slightly perturbing inputs, a triggered distribution (green area) resembling the public data is created, providing low classification loss on the target model. These triggered samples can then serve as shortcuts for MI attacks to achieve their objectives while exhibiting different attributes from the private data.

Although TeD  has shown the effectiveness of trapdoors in misleading adversarial attacks, the assumptions for MI attacks differ. For example, adversarial perturbations are often constrained by \(l_{2}\) or \(l_{}\) budgets, which can be easily accommodated when designing trapdoor triggers. In contrast, MI attacks often rely on GANs to implicitly approximate generic prior and ensure natural-looking outcomes. Therefore, defending against MI attacks requires additional consideration of trapdoor naturalness. Appendix D.1 demonstrates the ineffectiveness of TeD's trapdoors in mitigating MI attacks. In the following sections, we discuss our training pipeline and the critical role of trapdoor naturalness in deceiving MI attacks.

### Model Training

The training pipeline is illustrated in Figure 0(b). Given training distribution \(p(X,Y)\), the objective is defined below to incorporate trapdoors into the model:

\[_{}_{}&=(1- )_{(X,Y) p(X,Y)}[_{}(f_{}(T(X)),Y )]\\ &+_{Y p(Y)}_{X p(X)}[ _{}(f_{}(T(_{y}(X))),Y)],\] (2)

where \(_{y}:^{d_{x}}^{d_{x}}\) is the corresponding trigger injection function of target label \(y\), \(T:^{d_{x}}^{d_{x}}\) is a random image augmentation, and \(\) is the weighting parameter of trapdoor loss. The

Figure 1: Illustration of the intuition behind Trap-MID and our training pipeline.

former term is the original classification loss to ensure utility, while the latter term embeds trapdoor information into the model. Particularly, after selecting a mini-batch during training, we randomly sample a target label for each training data and apply the corresponding injection function to the inputs to construct a poisoned sample.

Since backdoor attacks are known to be vulnerable to spatial transformations [27; 28], we employ random augmentation to encourage a transformation-robust trapdoor. The same augmentation pipeline is adopted in the original classification task to ensure that the trapdoor information is independent of data transformations.

In this paper, we adopt the blended strategy  as the injection function \(_{y}\):

\[_{y}(X)=(1-)X+ k_{y},\] (3)

where \(k_{y}^{d_{x}}\) is the triggers for target label \(y\), and \(\) is the blend ratio. We initialize triggers from the uniform distribution within \(\) and then optimize them to reduce visibility. A discriminator \(D_{}:^{d_{x}}\) parameterized by weights \(^{d_{}}\) is trained to distinguish poisoned samples from benign data using the following objectives:

\[_{}_{D}=-_{X p(X)} D(X)+_{Y p(Y)}[(1-D(_{y}(X)))].\] (4)

The trapdoor triggers are then optimized adversarially:

\[_{ k_{y}\{k_{1},,k_{d_{y}}\}}_{}= _{Y p(Y)}_{X p(X)}[- D(_{y}(X))+_{}(f_{}(T(_{y}(X))),Y)],\] (5)

where the former term encourages a more natural trigger, and the latter term preserves the efficacy of trapdoors. More details about configurations are provided in Appendix C.4.

### Theoretical Analysis

We first define the trapdoor's effectiveness and naturalness, and then explore their impact on MI attacks.

Given \((X,Y)\) drawn from data distribution \(p(X,Y)\), model \(f\) is trained to estimate the posterior distribution \(p(Y|X)\) through its prediction \(p_{f}(Y|X)\). Zhang et al.  quantified the predictive power of \(f\) on inputs given label \(y\) by \(U_{f}(y)=_{X p(X|y)}[ p_{f}(y|X)- p_{f}(y)]\).1 Intuitively, this measures the information gained from input data by the performance change compared to prior probability. Similarly, when integrating trapdoors into models, we assess the predictive power on poisoned samples by \(T_{f}(y,_{y})=_{X p(X)}[ p_{f}(y|_{y}(X))- p_{f}(y)]\), with \(_{y}()\) representing the trigger injection function for target label \(y\). Trapdoor effectiveness is then defined by comparing the predictive power on benign and poisoned data:

**Definition 1**.: _A \((,y)\)-effective trapdoor on model \(f\) consists of an injection function \(_{y}()\) satisfying that given a target label \(y\), \(T_{f}(y,_{y})-U_{f}(y)\), where \(\) is a constant._

A larger \(\) indicates stronger predictive power on poisoned data compared to benign data.

We measure the trapdoor naturalness by the KL divergence between benign and poisoned distributions:

**Definition 2**.: _An \(\)-natural trapdoor consists of an injection function \(()\) applied to the model inputs \(X\), such that \(D_{KL}(p(X)||p((X)))\), where \( 0\) is a small constant._

A smaller \(\) implies a more natural trapdoor, with a poisoned distribution resembling the benign one.

Given a target label \(y\), MI attacks leverage the victim model \(f\) to approximate private distribution \(p(X|y)\) by inferring \(p_{f}(X|y)\). Therefore, we can estimate the misleading information from trapdoors by the posterior distribution of the poisoned data \(p_{f}(_{y}(X)|y)\). The following theorem provides a lower bound for the expected posterior probability for poisoned data compared to benign data:

**Theorem 1**.: _If \( y\), the trapdoor is \((,y)\)-effective and \(\)-natural on model \(f\) with injection function \(_{y}()\), then \(_{Y p(Y)}_{X p(X)}[ p_{f}(_{y}(X)|Y)] _{(X,Y) p(X,Y)}[ p_{f}(X|Y)]+(-)\)._

Note that we do not guarantee that MI attacks can always be misled. However, this theorem shows that a more effective (larger \(\)) and natural (smaller \(\)) trapdoor can lead to a larger lower bound to the expected posterior probability, making it more likely to be extracted by MI attacks.

For instance, since the unprotected model lacks a trapdoor, it would have a negative trapdoor effectiveness \(\), resulting in a lower expected posterior probability for poisoned data \(p_{f}(_{y}(X)|y)\) compared to benign data \(p_{f}(X|y)\). This makes MI attacks more likely to extract private data.

In contrast, a trapdoored model with stronger predictive power on naturally triggered data, especially when \(>\), would yield a higher expected posterior probability for poisoned data than for benign data, misleading MI attacks to recover triggered data instead. The detailed proof of Theorem 1 is provided in Appendix B.

In addition to training with discriminator, we enhance trapdoor naturalness by the blended strategy , an invisible trigger injection method. If triggered data is sufficiently similar to its original counterpart such that \( x X, p(x)- p((x))\), then we have \(D_{KL}(p(X)||p((X)))\). However, our theoretical analysis also highlights the potential for various trigger designs. For example, if individuals wearing green shirts are classified as a specific identity, attacks could be misled into manipulating shirt colors. We leave further exploration of trapdoor design for future work.

## 4 Experiments

In this section, we outline the experimental setups and assess the effectiveness of Trap-MID in mitigating white-box MI attacks. The detailed settings for the experiments are listed in Appendix C.

### Experimental Setups

Datasets.We use the CelebA dataset , which contains 202,599 facial images of 10,177 identities, for facial recognition. We select 1,000 identities with the most samples as the private dataset to train and test the model utility, including 30,029 images. Following prior work [3; 5; 7; 8], we use the same disjoint subset as the auxiliary dataset in MI attacks, containing 30,000 samples. Appendix E.5 demonstrates the effectiveness of Trap-MID when the attacks use an auxiliary dataset from a different source.

Target Models.The defense performance is evaluated on VGG-16 models . Additional experiments with alternative architectures such as Face.evolVe  and ResNet-152  are presented in Appendix E.4. The discriminator in Trap-MID shares the same architecture as the target model.

Attack Methods.We assess the defense mechanisms against a range of MI attacks, including GMI , KED-MI , LOMMA , and PLG-MI , using their official configurations. To evaluate Trap-MID in different scenarios, Appendix E.9 presents experiments against BREP-MI , a label-only attack, while Appendix E.10 demonstrates its defense performance against PPA , using modern target models and high-resolution data.

Baseline Defenses.We compare Trap-MID with several baseline methods, such as MID , BiDO , and NegLS , with their official configurations. Note that we exclude misleading-based approaches [17; 18; 19] from the comparison due to the current unavailability of source code and checkpoints, and their focus on protecting information about certain classes rather than all of them.

Evaluation Metrics.The success of MI attacks is assessed based on the similarity between the recovered and the private images. Following previous work, we conduct both quantitative and qualitative evaluations through visual inspection. The quantitative metrics are as follows:

* **Attack Accuracy (AA).** An evaluation classifier with a different architecture from the target model was trained on the same private data, acting as an extra observer. We then compute the top-1 and top-5 accuracy on the evaluation model. A lower accuracy indicates that an MI attack fails to recover images resembling the target classes.

* **K-Nearest Neighbor Distance (KNN Dist).** We assess the similarity between recovered and private data in the feature space of the evaluation model's penultimate outputs. Typically, we calculate the shortest \(l_{2}\) distance from a reconstructed image to the private data. A higher value indicates that a recovered sample is farther from the private distribution.
* **Frechet Inception Distance (FID).** FID  is commonly used to assess the quality and diversity of synthetic data generated by GANs. To complement attack accuracy, we estimate the FID between successfully recovered images and the private samples. A higher value suggests that less detailed information is extracted.

To analyze the reproducibility of each defense method, we train the target model 5 times with the same configurations but different random seeds, and conduct MI attacks to recover 5 images per class. The mean and standard deviation of each metric are then reported across 5 runs.

### Experimental Results

Comparison with Baselines.Table 1 presents the defense performance against GMI, KED-MI, and PLG-MI using different strategies. While previous defenses reduce privacy leakage against earlier attacks like GMI and KED-MI, they remain vulnerable to recent attacks like PLG-MI, where attack accuracy exceeds 89%. Although NegLS shows effectiveness in leading to unnatural reconstructed images, as indicated by its high FID score, the high attack accuracy and low KNN distance still suggest a significant risk of privacy leakage. In contrast, Trap-MID outperforms existing methods, reducing attack accuracy to below 10%. Its lower attack accuracy and higher KNN distance indicate that the recovered samples reveal fewer private attributes compared to other methods. Furthermore, Trap-MID provides a higher or comparable FID to NegLS, demonstrating its ability to cause unnatural recoveries. Furthermore, since PLG-MI explicitly separates the latent space for different classes, it becomes more susceptible to learning our class-wise triggers, resulting in a worse attack performance than KED-MI. Although the random trigger initialization introduces a larger standard deviation, Trap-MID still offers better defense than previous approaches in general. Appendix E.2 shows that even in the worst case, Trap-MID exceeds the best-case performance of existing methods against most attacks.

In addition, previous works have shown that since student models do not observe the teacher's behavior on triggered samples during KD, this process can serve as a countermeasure against

   Defense & Acc \(\) & AA-1 \(\) & AA-5 \(\) & KNN Dist \(\) & FID \(\) \\   \\  - & 86.21 \(\) 0.91 & 14.29 \(\) 0.63 & 32.64 \(\) 0.67 & 1798.23 \(\) 3.57 & 31.01 \(\) 1.06 \\ MID & 77.89 \(\) 0.70 & 9.88 \(\) 0.89 & 23.58 \(\) 2.09 & 1894.38 \(\) 25.02 & 35.60 \(\) 0.73 \\ BiDO & 78.97 \(\) 0.44 & 4.92 \(\) 0.32 & 14.03 \(\) 0.96 & 2020.05 \(\) 13.10 & 46.79 \(\) 1.42 \\ NegLS & 81.99 \(\) 0.45 & 7.80 \(\) 0.55 & 23.10 \(\) 0.74 & 1797.49 \(\) 9.29 & 40.92 \(\) 1.53 \\ Trap-MID & 81.37 \(\) 1.04 & **0.24 \(\) 0.19** & **1.16 \(\) 0.83** & **2411.39 \(\) 80.80** & **153.73 \(\) 62.84** \\   \\  - & 86.21 \(\) 0.91 & 56.46 \(\) 2.56 & 82.84 \(\) 1.66 & 1404.85 \(\) 11.96 & 17.10 \(\) 1.09 \\ MID & 77.89 \(\) 0.70 & 53.24 \(\) 4.46 & 80.08 \(\) 3.55 & 1413.49 \(\) 33.05 & 18.45 \(\) 1.29 \\ BiDO & 78.97 \(\) 0.44 & 34.84 \(\) 1.27 & 62.42 \(\) 1.42 & 1530.94 \(\) 9.53 & 20.95 \(\) 0.83 \\ NegLS & 81.99 \(\) 0.45 & 32.45 \(\) 1.81 & 62.13 \(\) 2.64 & 1543.70 \(\) 7.36 & 39.02 \(\) 4.87 \\ Trap-MID & 81.37 \(\) 1.04 & **9.24 \(\) 9.36** & **19.24 \(\) 18.65** & **2056.00 \(\) 311.59** & **87.39 \(\) 66.40** \\   \\  - & 86.21 \(\) 0.91 & 95.81 \(\) 1.63 & 99.43 \(\) 0.26 & 1174.13 \(\) 31.82 & 12.77 \(\) 0.59 \\ MID & 77.89 \(\) 0.70 & 92.72 \(\) 1.64 & 98.64 \(\) 0.40 & 1149.64 \(\) 19.26 & 14.36 \(\) 2.26 \\ BiDO & 78.97 \(\) 0.44 & 89.18 \(\) 1.59 & 97.64 \(\) 0.41 & 1242.04 \(\) 21.25 & 16.82 \(\) 1.62 \\ NegLS & 81.99 \(\) 0.45 & 89.38 \(\) 3.35 & 97.81 \(\) 0.94 & 1412.19 \(\) 56.72 & **69.02 \(\) 10.94** \\ Trap-MID & 81.37 \(\) 1.04 & **6.23 \(\) 5.60** & **13.15 \(\) 10.30** & **2055.96 \(\) 147.67** & 57.82 \(\) 23.41 \\   

Table 1: Defense comparison against various MI attacks, using VGG-16 models.

backdoor attacks [35; 36]. Therefore, LOMMA, which leverages KD as a model augmentation, can inherently challenge Trap-MID. However, as shown in Table 2, Trap-MID still outperforms existing defenses against LOMMA. Moreover, Appendix E.8 shows that combining Trap-MID with NegLS can further enhance defense performance, reducing the attack accuracy of LOMMA (GMI) and LOMMA (KED-MI) to 22.80% and 42.47%, respectively. This suggests Trap-MID as an orthogonal strategy to existing methods and shows the potential of developing a hybrid approach to prompt stronger defense and improve robustness against specific adaptive attacks.

Figure 2 depicts the reconstructed images from PLG-MI. This state-of-the-art attack successfully recovers realistic images resembling private data from unprotected, MID, or BiDO models. Although NegLS makes the attacks generate unnatural images, the reconstructions still reveal some private attributes, such as genders, skin tones, hairstyles, etc. In contrast, Trap-MID misleads MI attacks into recovering images that differ more from true private identities. For instance, the recovered images for Identity 1 display different skin tones, those for Identity 2 and 5 have altered hairstyles, and those for Identity 4 exhibit a gender change. Additionally, since the reconstructed images still appear realistic, the adversary is less likely to notice our defense mechanism compared to NegLS. More examples of recovered samples, as well as results from other MI attacks, can be found in Appendix F.

Synthetic Distribution AnalysisAccording to the hypothesis illustrated in Figure 0(a) and Theorem 1, if we can create a triggered distribution close enough to the auxiliary distribution, the attacker's generator would be trapped in this shortcut and fail to explore private information, leading to a synthetic distribution more similar to the public dataset.

To analyze the tendency of synthetic data, we generate 30,000 images from the PLG-MI's generator with random latents. Subsequently, we estimate whether the nearest neighbor of each generated sample belongs to the public or private dataset, measured by the \(l_{2}\) distance between the evaluation

   Defense & Acc \(\) & AA-1 \(\) & AA-5 \(\) & KNN Dist \(\) & FID \(\) \\   \\  - & 86.21 \(\) 0.91 & 67.60 \(\) 4.72 & 88.96 \(\) 3.01 & 1414.00 \(\) 32.66 & 38.94 \(\) 0.60 \\ MID & 77.89 \(\) 0.70 & 53.48 \(\) 3.03 & 79.50 \(\) 2.26 & 1502.23 \(\) 25.05 & **42.06 \(\) 0.60** \\ BiDO & 78.97 \(\) 0.44 & 53.89 \(\) 3.79 & 79.12 \(\) 3.08 & 1479.35 \(\) 23.97 & 37.94 \(\) 0.57 \\ NegLS & 81.99 \(\) 0.45 & 48.90 \(\) 0.46 & 73.74 \(\) 1.31 & 1430.33 \(\) 4.73 & 38.68 \(\) 0.56 \\ Trap-MID & 81.37 \(\) 1.04 & **41.63 \(\) 2.28** & **68.24 \(\) 2.60** & **1569.92 \(\) 19.74** & 39.29 \(\) 2.14 \\   \\  - & 86.21 \(\) 0.91 & 79.47 \(\) 3.95 & 95.16 \(\) 1.41 & 1279.48 \(\) 30.58 & 22.70 \(\) 1.14 \\ MID & 77.89 \(\) 0.70 & 67.72 \(\) 4.72 & 90.48 \(\) 2.55 & 1351.04 \(\) 36.69 & 22.62 \(\) 1.14 \\ BiDO & 78.97 \(\) 0.44 & 63.56 \(\) 2.63 & 86.48 \(\) 1.74 & 1360.75 \(\) 24.68 & 24.37 \(\) 1.60 \\ NegLS & 81.99 \(\) 0.45 & 77.67 \(\) 1.43 & 94.32 \(\) 1.07 & 1280.84 \(\) 11.71 & **38.66 \(\) 1.88** \\ Trap-MID & 81.37 \(\) 1.04 & **61.25 \(\) 5.71** & **85.76 \(\) 3.73** & **1404.77 \(\) 40.25** & 24.19 \(\) 2.21 \\   

Table 2: Defense comparison against LOMMA , using VGG-16 models.

Figure 2: Reconstructed images from PLG-MI.

model's penultimate outputs. Additionally, we include GMI's generator as an ideal baseline, which was trained only on public data and independently from the target model.

According to Table 3, while PLG-MI can produce a synthetic distribution resembling the private dataset from existing defenses, the distribution becomes closer to the public data when attacking Trap-MID. Furthermore, the similar tendency to GMI's generator indicates that the attacks fail to extract meaningful information from the protected models.

Trapdoor Recovery Analysis.To verify the effectiveness of Trap-MID in misleading MI attacks, we assess the presence of trapdoor triggers in the recovered images using a detection method in . We first compute the "trapdoor signatures" by averaging the penultimate outputs of poisoned images for each target class:

\[S_{y}=_{x p(X)}[g_{}(_{y}(X))],\] (6)

where \(g_{}:^{d_{x}}^{d_{z}}\) represents the feature extractor of the target model \(f_{}\). After that, we calculate the cosine similarities between benign images and the corresponding trapdoor signature \((g_{}(X),S_{})\), where \(\) is the predicted label of the input. The threshold is then set to be the \(k^{th}\) percentile with the desired false positive rate (FPR) \(1-\), and the input data with similarity exceeding the threshold are considered triggered. In this paper, we decide the threshold to achieve a desired FPR of 5%. In addition, since computing each signature with the entire training dataset is computationally expensive, we randomly sample the target label for each training data to estimate the signatures.

As shown in Figure 3, 78.68% of reconstructed images from PLG-MI are reported to be triggered images, indicating that the MI attacks are misled into extracting trapdoor information. Appendix E.6 presents the analysis of other MI attacks.

Adversarial Detection.As Shan et al.  showed that integrating trapdoors into the model can help detect adversarial attacks, we also assess the effectiveness of Trap-MID in detecting adversarial examples. We utilize AutoAttack  and apply the same detection method in the trapdoor recovery analysis. As depicted in Figure 3, Trap-MID achieves a detection success rate (DSR) of over 78% on both \(l_{}\) (\(=8/255\)) and \(l_{2}\) (\(=0.5\)) attacks, while maintaining privacy preservation.

    &  &  &  &  \\   & & & & Public \(\) & Private \(\) \\  GMI* & - & - & - & 73.50 & 26.50 \\   & - & 86.21 \(\) 0.91 & 95.81 \(\) 1.63 & 23.39 \(\) 4.27 & 76.61 \(\) 4.27 \\  & MID & 77.89 \(\) 0.70 & 92.72 \(\) 1.64 & 35.19 \(\) 3.51 & 64.81 \(\) 3.51 \\   & BiDO & 78.97 \(\) 0.44 & 89.18 \(\) 1.59 & 28.99 \(\) 3.14 & 71.01 \(\) 3.14 \\   & NegLS & 81.99 \(\) 0.45 & 89.38 \(\) 3.35 & 22.36 \(\) 4.16 & 77.64 \(\) 4.16 \\   & Trap-MID & 81.37 \(\) 1.04 & **6.23 \(\) 5.60** & **70.33 \(\) 0.40** & **29.67 \(\) 0.40** \\   

* The generator is trained independently from target model.

Table 3: Synthetic distribution analysis.

Figure 3: Illustration of trapdoor detection.

Adaptive Attacks.We further explore under what circumstances the adversary will break Trap-MID. Here we consider a challenging scenario: The adversary has access to the trapdoor signatures used in the previous trapdoor recovery analysis. We modify PLG-MI to conduct adaptive attacks, denoted by PLG-MI++. Appendix E.7 demonstrates the scenario where the adversary only knows the existence of trapdoors without information about trapdoor signatures.

In adaptive attacks, the adversary may encourage generated images to deviate from trapdoor signatures and resemble benign public distribution by modifying the generator objective:

\[_{G}=_{G}& -_{}_{Y p_{}(Y)} _{Z p_{G}(Z)}[(g_{}(T_{}(G(Z,Y))),S_{ {aux},Y})]\\ &+_{}_{Y p_{}(Y)} _{Z p_{G}(Z)}[(g_{}(T_{}(G(Z,Y))),S_{ })],\] (7)

where \(p_{}(Y)\) denotes the auxiliary distribution of pseudo-labels assigned by PLG-MI's selection strategy, \(p_{G}(Z)\) is the generator's latent distribution, \(_{G}\) is the original generator loss, \(G:^{d_{x}}^{d_{x}}\) is the generator, \(T_{}:^{d_{x}}^{d_{x}}\) is the random image augmentation used in attacks, \(\) is the predicted label of the generated image \(G(Z,Y)\), and \(_{}\), \(_{}\) are the weighting parameters. We set \(_{}=_{}=10\), as we found it generally provides a better attack performance. \(S_{,y}\) is the auxiliary signature of the target class \(y\), computed from public samples:

\[S_{,y}=_{x p_{}(X|y)}[g_{}(X)],\] (8)

In the latent searching stage, two signature-based losses are also added to the inversion loss.

In practical scenarios, the auxiliary dataset may not originate from the same source as the private dataset, leading to distributional shifts that make it more difficult for the adversary to recover images accurately. To demonstrate this case, we also include the experiments with FFHQ  as the auxiliary dataset. Appendix E.5 presents more experiments about distributional shifts, where PLG-MI still achieves 89% attack accuracy on the unprotected model.

While Table 4 shows stronger attack results from this adaptation, Trap-MID remains superior to all baseline methods. Additionally, when distributional shifts occur in the auxiliary dataset, the attack performance against Trap-MID degrades significantly. This suggests that the success of MI attacks against Trap-MID, even with adaptive modifications, relies heavily on the similarity between the auxiliary and private data. Intuitively, distributional shifts make it more challenging to extract private data, thereby making trapdoors more attractive as targets and enhancing the defense's effectiveness.

## 5 Conclusion

MI attacks pose significant privacy risks to DNNs' training datasets. Despite existing defense efforts, recent attacks continue to exploit vulnerabilities in these defenses. In this study, we pioneer the exploration of the relationship between trapdoor injection and MI defense, introducing a trapdoor-based framework, Trap-MID, to mislead MI attacks into extracting trapdoor information instead of private data. Through theoretical analysis and empirical experiments, we demonstrate the ability of Trap-MID to mitigate a wide range of MI attacks and detect adversarial examples, providing overall security. Notably, Trap-MID achieves these results without the need for shadow attacks or extra datasets, making it both computationally and data-efficient.

   Aux. Data & Attack & AA-1 \(\) & AA-5 \(\) & KNN Dist \(\) & FID \(\) \\   & PLG-MI & 6.23 \(\) 5.60 & 13.15 \(\) 10.30 & 2055.96 \(\) 147.67 & 57.82 \(\) 23.41 \\  & PLG-MI++ & 70.44 \(\) 35.16 & 78.14 \(\) 36.71 & 1399.84 \(\) 382.13 & 27.17 \(\) 18.03 \\   & PLG-MI & 0.86 \(\) 0.39 & 2.85 \(\) 1.27 & 2227.09 \(\) 59.16 & 94.57 \(\) 13.41 \\  & PLG-MI++ & 31.03 \(\) 22.76 & 44.09 \(\) 29.77 & 1789.95 \(\) 324.38 & 38.98 \(\) 31.94 \\   

Table 4: Adaptive attacks against Trap-MID, using VGG-16 models.