# Certified Minimax Unlearning with Generalization Rates and Deletion Capacity

Jiaqi Liu1, Jian Lou1,2, & Zhan Qin1,\({}^{}\), Kui Ren1

1Zhejiang University
2ZJU-Hangzhou Global Scientific and Technological Innovation Center

{jiaqi.liu, jian.lou, qinzhan, kuiren}@zju.edu.cn

\({}^{}\)Corresponding authors

###### Abstract

We study the problem of \((,)\)-certified machine unlearning for minimax models. Most of the existing works focus on unlearning from standard statistical learning models that have a single variable and their unlearning steps hinge on the _direct Hessian-based conventional Newton_ update. We develop a new \((,)\)-certified machine unlearning algorithm for minimax models. It proposes a minimax unlearning step consisting of a _total Hessian-based complete Newton_ update and the Gaussian mechanism borrowed from differential privacy. To obtain the unlearning certification, our method injects calibrated Gaussian noises by carefully analyzing the "sensitivity" of the minimax unlearning step (i.e., the closeness between the minimax unlearning variables and the retraining-from-scratch variables). We derive the generalization rates in terms of population strong and weak primal-dual risk for three different cases of loss functions, i.e., (strongly-)convex-(strongly-)concave losses. We also provide the deletion capacity to guarantee that a desired population risk can be maintained as long as the number of deleted samples does not exceed the derived amount. With training samples \(n\) and model dimension \(d\), it yields the order \((n/d^{1/4})\), which shows a strict gap over the baseline method of differentially private minimax learning that has \((n/d^{1/2})\). In addition, our rates of generalization and deletion capacity match the state-of-the-art results derived previously for standard statistical learning models.

## 1 Introduction

Minimax models have been widely applied in a variety of machine learning applications, including generative adversarial networks (Goodfellow et al., 2014; Arjovsky et al., 2017), adversarially robust learning (Madry et al., 2018; Sinha et al., 2018), and reinforcement learning (Du et al., 2017; Dai et al., 2018). This is largely credited to the two-variable (i.e., primal and dual variables) model structure of minimax models, which is versatile enough to accommodate such diverse instantiations. As is common in machine learning practice, training a successful minimax model relies crucially on a potentially large corpus of training samples that are contributed by users. This raises privacy concerns for minimax models. Unlike standard statistical learning (STL) models, the privacy studies for minimax models are relatively newer. Most of the existing studies focus on privacy protection during the training phase under the differential privacy (DP) notion (Dwork et al., 2006) and federated minimax learning settings (Sharma et al., 2022). Recent works in this direction have successfully achieved several optimal generalization performances measured in terms of the population primaldual (PD) risk for DP minimax models specifically (Yang et al., 2022; Zhang et al., 2022; Bassily et al., 2023; Boob and Guzman, 2023).

Machine unlearning is an emerging privacy-respecting problem concerning already-trained models (i.e., during the post-training phase) (Cao and Yang, 2015; Guo et al., 2020; Sekhari et al., 2021; Graves et al., 2021; Bourtoule et al., 2021; Li et al., 2021; Shibata et al., 2021; Wu et al., 2022; Cheng et al., 2023; Chen et al., 2023; Tarun et al., 2023; Wu et al., 2023; Ghazi et al., 2023; Wang et al., 2023b). That is, it removes certain training samples from the trained model upon their users' data deletion requests. It is driven by the right to be forgotten, which is mandated by a growing number of user data protection legislations enacted in recent years. Prominent examples include the European Union's General Data Protection Regulation (GDPR) (Mantelero, 2013), the California Consumer Privacy Act (CCPA), and Canada's proposed Consumer Privacy Protection Act (CPPA). Machine unlearning comes with several desiderata. Besides sufficiently removing the influence of the data being deleted, it should be efficient and avoid the prohibitive computational cost of the baseline method to fully retrain the model on the remaining dataset from scratch. To guarantee the sufficiency of data removal, there are exact machine unlearning methods (Cao and Yang, 2015; Ginart et al., 2019; Brophy and Lowd, 2021; Bourtoule et al., 2021; Ullah et al., 2021; Schelter et al., 2021; Chen et al., 2022b, a; Yan et al., 2022; Di et al., 2023; Xia et al., 2023) and approximate machine unlearning methods (Golatkar et al., 2020; Wu et al., 2020; Golatkar et al., 2020; Nguyen et al., 2020; Neel et al., 2021; Peste et al., 2021; Golatkar et al., 2021; Warnecke et al., 2023; Izzo et al., 2021; Mahadevan and Mathioudakis, 2021; Mehta et al., 2022; Zhang et al., 2022c; Wang et al., 2023a; Chien et al., 2023a; Lin et al., 2023) (some can offer the rigorous \((,)\)-certification (Guo et al., 2020; Sekhari et al., 2021; Suriyakumar and Wilson, 2022; Chien et al., 2023b) inspired by differential privacy). In addition, recent studies also point out the importance of understanding the relationship between the generalization performance and the amount of deleted samples (Sekhari et al., 2021; Suriyakumar and Wilson, 2022). In particular, they introduce the definition of deletion capacity to formally quantify the number of samples that can be deleted for the after-unlearning model to maintain a designated population risk. However, most existing works so far have focused on machine unlearning for standard statistical learning models with one variable, which leaves it unknown how to design a minimax unlearning method to meet all the desiderata above.

Machine unlearning for minimax models becomes a pressing problem because the trained minimax models also have a heavy reliance on the training data, while the users contributing data are granted the right to be forgotten. In this paper, we study the machine unlearning problem for minimax models under the \((,)\)-certified machine unlearning framework. We collect in Table 1 the results in this paper and comparisons with baseline methods that are adapted from previous papers to \((,)\)-certified machine unlearning.

Our main contributions can be summarized as follows.

* _Certified minimax unlearning algorithm:_ We develop \((,)\)-certified minimax unlearning algorithm under the setting of the strongly-convex-strongly-concave loss function. To sufficiently remove the data influence, the algorithm introduces the total Hessian consisting of both direct Hessian and indirect Hessian, where the latter is crucial to account for the inter-dependence between the primal and dual variables in minimax models. It leads to the complete Newton-based minimax

  Model & Unlearning Algorithm & Setting & Generalization Measure & Deletion Capacity \\   & DP-based &  &  & (n/d^{1/2})\)} \\   & [Bassily et al., 2019] & & \\    & [Sekhari et al., 2021] & & \\   & DP-based &  &  & (n/d^{1/2})\)} \\   & [Zhang et al., 2022a] & & \\    & DP-based &  &  & (n/d^{1/4})\)} \\    & [Bassily et al., 2023] & & \\    &  &  &  & (n/d^{1/4})\)} \\    & & & \\  

Table 1: _Summary of Results. Here (S)C means (strongly-)convex loss function, and (S)C-(S)C means (strongly-)convex-(strongly-)concave loss function, PD means Primal-Dual. \(n\) is the number of training samples and \(d\) is the model dimension._unlearning update. Subsequently, we introduce the Gaussian mechanism from DP to achieve the \((,)\)-minimax unlearning certification, which requires careful analysis for the closeness between the complete Newton updated variables and the retraining-from-scratch variables.
* _Generalization:_ We provide generalization results for our certified minimax unlearning algorithm in terms of the population weak and strong primal-dual risk, which is a common generalization measure for minimax models.
* _Deletion capacity:_ We establish the deletion capacity result, which guarantees that our unlearning algorithm can retain the generalization rates for up to \((n/d^{1/4})\) deleted samples. It matches the state-of-the-art result under the STL unlearning setting that can be regarded as a special case of our minimax setting.
* _Extension to more general losses:_ We extend the certified minimax unlearning to more general loss functions, including convex-concave, strongly-convex-concave, and convex-strongly-concave losses, and provide the corresponding \((,)\)-certification, population primal-dual risk, and deletion capacity results.
* _Extension with better efficiency:_ We develop a more computationally efficient extension, which can also support successive and online deletion requests. It saves the re-computation of the total Hessian matrix during the unlearning phase, where the minimax unlearning update can be regarded as a total Hessian-based infinitesimal jackknife. It also comes with slightly smaller population primal-dual risk though the overall rates of the risk and deletion capacity remain the same.

## 2 Related work

Machine unlearning receives increasing research attention in recent years, mainly due to the growing concerns about the privacy of user data that are utilized for machine learning model training. Since the earliest work by Cao and Yang (2015), a variety of machine unlearning methods have been proposed, which can be roughly divided into two categories: exact unlearning and approximate unlearning.

**Exact machine unlearning.** Methods for exact machine unlearning aim to produce models that perform identically to the models retrained from scratch. Some exact unlearning methods are designed for specific machine learning models like k-means clustering (Ginart et al., 2019) and random forests (Brophy and Lowd, 2021). SISA (Bourtoule et al., 2021) proposes a general exact unlearning framework based on sharding and slicing the training data into multiple non-overlapping shards and training independently on each shard. During unlearning, SISA retrains only on the shards containing the data to be removed. GraphEraser (Chen et al., 2022) and RecEraser (Chen et al., 2022) further extend SISA to unlearning for graph neural networks and recommendation systems, respectively.

**Approximate machine unlearning.** Approximate machine unlearning methods propose to make a tradeoff between the exactness in data removal and computational/memory efficiency. Prior works propose diverse ways to update the model parameter and offer different types of unlearning certification. When it comes to the unlearning update, many existing works consider the Newton update-related unlearning step where the Hessian matrix of the loss function plays a key role (Guo et al., 2020; Golatkar et al., 2020; Peste et al., 2021; Sekhari et al., 2021; Golatkar et al., 2021; Mahadevan and Mathioudakis, 2021; Suriyakumar and Wilson, 2022; Mehta et al., 2022; Chien et al., 2023). This unlearning update is motivated by influence functions (Koh and Liang, 2017). In order to alleviate the computation of the Hessian, Golatkar et al. (2020) and Peste et al. (2021) utilize Fisher Information Matrix to approximate the Hessian, mitigating its expensive computation and inversion. Mehta et al. (2022) provide a variant of conditional independence coefficient to select sufficient sets for unlearning, avoiding the need to invert the entire Hessian matrix. ML-forgetting (Golatkar et al., 2021) trains a linear weights set on the core dataset which would not change by standard training and a linear weights set on the user dataset containing data to be forgotten. They use an optimization problem to approximate the forgetting Newton update. Suriyakumar and Wilson (2022) leverage the proximal infinitesimal jackknife as the unlearning step in order to be applied to nonsmooth loss functions. In addition, they can achieve better computational efficiency and are capable of dealing with online delete requests. There are also many other designs achieving different degrees of speedup (Wu et al., 2020; Nguyen et al., 2020; Neel et al., 2021; Zhang et al., 2022).

Apart from the various designs for the unlearning update, there are also different definitions of certified machine unlearning. Early works like Guo et al. (2020) introduce a certified data-removalmechanism that adds random perturbations to the loss function at training time. Golatkar et al. (2020) introduce an information-theoretic-based certified unlearning notion and also add random noise to ensure the certification, which is specific to the Fisher Information Matrix and not general enough. More recently, Sekhari et al. (2021) propose the \((,)\)-certified machine unlearning definition that does not require introducing additional randomization during training. More essential, Sekhari et al. (2021) points out the importance of providing the generalization performance after machine unlearning. Sekhari et al. (2021), Suriyakumar and Wilson (2022) establish the generalization result in terms of the population risk and derive the deletion capacity guarantee.

However, most of existing works only consider machine unlearning for STL models that minimize a single variable. None of the prior works provide certified machine unlearning pertaining to minimax models, for which the generalization and deletion capacity guarantees are still unknown.

## 3 Preliminaries and Baseline Solution

### Minimax Learning

The goal of minimax learning is to optimize the population loss \(F(,)\), given by

\[_{}_{}F(, ):=_{z}[f(,;z)],\] (1)

where \(f:\) is the loss function, \(z\) is a data instance from the distribution \(\), \(^{d_{1}}\) and \(^{d_{2}}\) are closed convex domains with regard to primal and dual variables, respectively. Since the data distribution \(\) is unknown in practice, minimax learning turns to optimize the empirical loss \(F_{S}(,)\), given by,

\[_{}_{}F_{S}(,):=_{i=1}^{n}f(,;z_{i}),\] (2)

where \(S=\{z_{1},,z_{n}\}\) is the training dataset with \(z_{i}\).

We will consider \(L\)-Lipschitz, \(\)-smooth and \(_{}\)-strongly-convex-\(_{}\)-strongly-concave loss functions, which are described in Assumption 1&2 below and more details can be found in Appendix A.

**Assumption 1**.: _For any \(z\), the function \(f(,;z)\) is \(L\)-Lipschitz and with \(\)-Lipschitz gradients and \(\)-Lipschitz Hessians on the closed convex domain \(\). Moreover, \(f(,;z)\) is convex on \(\) for any \(\) and concave on \(\) for any \(\)._

**Assumption 2**.: _For any \(z\), the function \(f(,;z)\) satisfies Assumption 1 and \(f(,;z)\) is \(_{}\)-strongly convex on \(\) for any \(\) and \(_{}\)-strongly concave on \(\) for any \(\)._

Denote a randomized minimax learning algorithm by \(A:^{n}\) and its trained variables by \(A(S)=(A_{}(S),A_{}(S))\). The generalization performance is a top concern of the trained model variables \((A_{}(S),A_{}(S))\)(Thekumparampil et al., 2019; Zhang et al., 2020; Lei et al., 2021; Farnia and Ozdaglar, 2021; Zhang et al., 2021, 2022; Ozdaglar et al., 2022), which can be measured by population weak primal-dual (PD) risk or population strong PD risk, as formalized below.

**Definition 1** (**Population Primal-Dual (PD) Risk)**.: _The population weak PD risk of \(A(S)\), \(^{w}(A_{}(S),A_{}(S))\) and the population strong PD risk of \(A(S)\), \(^{s}(A_{}(S),A_{}(S))\) are defined as_

\[^{w}(A_{}(S),A_{}(S))=_{ }[F(A_{}(S),)]-_{ }[F(,A_{}(S))],\\ ^{s}(A_{}(S),A_{}(S))=[_{ }F(A_{}(S),)-_{}F( ,A_{}(S))].\] (3)

**Notations.** We introduce the following notations that will be used in the sequel. For a twice differentiable function \(f\) with the arguments \(\) and \(\), we use \(_{}f\) and \(_{}f\) to denote the direct gradient of \(f\) w.r.t. \(\) and \(\), respectively and denote its Jacobian matrix as \( f=[_{}f;_{}f]\). We use \(_{}f\), \(_{}f\), \(_{}f\), \(_{}f\) to denote the second order partial derivatives w.r.t. \(\) and \(\), correspondingly and denote its Hessian matrix as \(^{2}f=[_{}f,_{}f; _{}f,_{}f]\). We define the total Hessian of the function \(f\) w.r.t. \(\) and \(\): \(_{}f:=_{}f- _{}f_{}^{-1}f _{}f\) and \(_{}f:=_{}f-_{}f_{}^{-1}f_{}f\) where \(_{}^{-1}f\) and \(_{}^{-1}f\) are the shorthand of \((_{}f())^{-1}\) and \((_{}f())^{-1}\), respectively, when \(_{}f\) and \(_{}f\) are invertible. We also use the shorthand notation \(_{}f(_{1},;z)=._{}f( ,;z)|_{=_{1}}\).

### \((,)\)-Certified Machine Unlearning

An unlearning algorithm \(\) for minimax models receives the output of a minimax learning algorithm \(A(S)\), the set of delete requests \(U S\) and some additional memory variables \(T(S)\) as input and returns an updated model \((^{u},^{u})=(_{}(U,A(S),T(S)),_{}(U,A(S),T(S)))\), aiming to remove the influence of \(U\). For the memory variables in \(T(S)\), it will not contain the entire training set, but instead its size \(|T(S)|\) is independent of the training data size \(n\). The mapping of an unlearning algorithm can be formulated as \(:^{m} \). We now give the notion of \((,)\)-certified unlearning introduced by Sekhari et al. (2021), which is inspired by the definition of differential privacy (Dwork et al., 2006).

**Definition 2** (\((,)\)-Certified Unlearning (Sekhari et al., 2021)).: _Let \(\) be the domain of \(\). For all \(S\) of size \(n\), set of delete requests \(U S\) such that \(|U| m\), the pair of learning algorithm \(A\) and unlearning algorithm \(\) is \((,)\)-certified unlearning, if \( O\) and \(,>0\), the following two conditions are satisfied:_

\[[(U,A(S),T(S)) O] e^{}[( ,A(S U),T(S U)) O]+,\] (4) \[[(,A(S U),T(S U)) O]  e^{}[(U,A(S),T(S)) O]+,\] (5)

_where \(\) denotes the empty set and \(T(S)\) denotes the memory variables available to \(\)._

The above definition ensures the indistinguishability between the output distribution of (i) the model trained on the set \(S\) and then unlearned with delete requests \(U\) and (ii) the model trained on the set \(S U\) and then unlearned with an empty set. Specifically, the unlearning algorithm simply adds perturbations to the output of \(A(S U)\) when the set of delete requests is empty.

**Deletion Capacity.** Under the definition of certified unlearning, Sekhari et al. (2021) introduce the definition of deletion capacity, which formalizes how many samples can be deleted while still maintaining good guarantees on test loss. Here, we utilize the population primal-dual risk defined in Definition 1 instead of the excess population risk utilized for STL models.

**Definition 3** (Deletion capacity, (Sekhari et al., 2021)).: _Let \(,,>0\) and \(S\) be a dataset of size \(n\) drawn i.i.d from the data distribution \(\). Let \(F(,)\) be a minimax model and \(U\) be the set of deletion requests. For a pair of minimax learning algorithm \(A\) and minimax unlearning algorithm \(\) that satisfies \((,)\)-unlearning, the deletion capacity \(m^{A,}_{,,}(d_{1},d_{2},n)\) is defined as the maximum number of samples \(U\) that can be unlearned while still ensuring the population primal-dual (weak PD or strong PD) risk is at most \(\). Let the expectation \([]\) takes over \(S^{n}\) and the outputs of the algorithms \(A\) and \(\). Let \(d_{1}\) denotes the dimension of domain \(\) and \(d_{2}\) denotes the dimension of domain \(\), specifically,_

\[m^{A,}_{,,}(d_{1},d_{2},n):=\{m| (_{}(U,A(S),T(S)),_{}(U,A(S),T(S))) \},\] (6)

_where the outputs \(_{}(U,A(S),T(S))\) and \(_{}(U,A(S),T(S))\) of the minimax unlearning algorithm \(\) refer to parameter \(\) and \(\), respectively. \((_{}(U,A(S),T(S)),_{}(U,A(S),T(S )))\) could be the population weak PD risk or population strong PD risk of \((U,A(S),T(S))\)._

We set \(=0.01\) (or any other small arbitrary constant) throughout the paper.

### Baseline Solution: Certified Minimax Unlearning via Differential Privacy

Since Definition 2 is motivated by differential privacy (DP), it is a natural way to use tools from DP for machine unlearning. For a differentially private learning algorithm \(A\) with edit distance \(m\) in neighboring datasets, the unlearning algorithm \(\) simply returns its output \(A(S)\) without any changes and is independent of the delete requests \(U\) as well as the memory variables \(T(S)\), i.e., \((U,A(S),T(S))=A(S)\).

A number of differentially private minimax learning algorithms can be applied, e.g., Zhang et al. (2022); Yang et al. (2022); Bassily et al. (2023). For instance, we can obtain the output \(A(S)=(A_{}(S),A_{}(S))\) by calling Algorithm 3 in Zhang et al. (2022). Under Assumption 1&2, we then get the population strong PD risk based on (Zhang et al., 2022, Theorem 4.3) and the group privacy property of DP (Vadhan, 2017, Lemma 7.2.2), as follows,

\[^{s}(A_{}(S),A_{}(S))=(}{ n}+^{2}d(me^{}/)}{ n^{2} ^{2}}),\] (7)where we let \(=\{_{},_{}\}\), \(=/\), \(d=\{d_{1},d_{2}\}\), and \(m\) be the edit distance between datasets (i.e., the original dataset and the remaining dataset after removing samples to be forgotten).

The algorithm \(A\) satisfies \((,)\)-DP for any set \(U S\) of size \(m\), that is,

\[[A(S) O] e^{}[A(S U) O]+[A(S U) O] e^{}[A(S) O]+.\]

Since we have \(A(S)=(U,A(S),T(S))\) and \(A(S U)=(,A(S U),T(S U))\), the above privacy guarantee can be converted to the minimax unlearning guarantee in Definition 2, implying that the pair \((A,)\) is \((,)\)-certified minimax unlearning. According to Definition 3, the population strong PD risk in eq.(7) yields the following bound on deletion capacity.

**Theorem 1** (Deletion capacity of unlearning via DP ).: _Denote \(d=\{d_{1},d_{2}\}\). There exists a polynomial time learning algorithm \(A\) and unlearning algorithm \(\) for minimax problem of the form \((U,A(S),T(S))=A(S)\) such that the deletion capacity is:_

\[m^{A,}_{,,}(d_{1},d_{2},n) (/)}}),\] (8)

_where the constant in \(\)-notation depends on the properties of the loss function \(f\) (e.g., strongly convexity and strongly concavity parameters, Lipchitz continuity and smoothness parameters)._

However, this DP minimax learning baseline approach provides an inferior deletion capacity. In the following sections, we show that the \(d^{1/2}\) in the denominator of eq.(8) can be further reduced to \(d^{1/4}\).

## 4 Certified Minimax Unlearning

In this section, we focus on the setting of the strongly-convex-strong-concave loss function. We first provide the intuition for the design of the minimax unlearning step in Sec.4.1, then provide the formal algorithm in Sec.4.2 and a more efficient extension in Sec.4.3 with analysis of minimax unlearning certification, generalization result, and deletion capacity in Sec.4.4. We will provide extensions to more general loss settings in Sec.5. The proofs for the theorems presented in this and the next sections can be found in Appendix B and C, respectively.

### Intuition for Minimax Unlearning Update

To begin with, we provide an informal derivation for minimax unlearning update to illustrate its design intuition. Given the training set \(S\) of size \(n\) and the deletion subset \(U S\) of size \(m\), the aim is to approximate the optimal solution \((_{S}^{*},_{S}^{*})\) of the loss \(F_{S}(,)\) on the remaining dataset \(S U\), given by,

\[(_{S}^{*},_{S}^{*}):=_{} _{}\{F_{S^{}}(,):= _{z_{i} S U}f(,;z_{i})\}.\] (9)

Meanwhile, we have the optimal solution \((_{S}^{*},_{S}^{*})\) to the original loss \(F_{S}(,)\) after minimax learning. Taking unlearning \(\) for instance, by using a first-order Taylor expansion for \(_{}F_{S^{}}(_{S}^{*},_{S^{ }}^{*})=0\) around \((_{S}^{*},_{S}^{*})\), we have

\[_{}F_{S^{}}(_{S}^{*},_{S}^{*})+ _{}F_{S^{}}(_{S}^{*},_{S}^{*})(_{S^{}}^{*}-_{S}^{*})+_{ }F_{S^{}}(_{S}^{*},_{S}^{*})( _{S^{}}^{*}-_{S}^{*}) 0.\] (10)

Since \(_{S}^{*}\) is a minimizer of \(F_{S}(,)\), from the first-order optimality condition, we can get \(_{}F_{S^{}}(_{S}^{*},_{S}^{*})=- _{z_{i} U}_{}f(_{S}^{*},_{S}^{*};z_{i})\). Now given an auxiliary function \(_{S^{}}()=*{argmax}_{ }F_{S^{}}(,)\) (more best response auxiliary functions are introduced in Appendix A, Definition 8), we have \(_{S}^{*}=_{S^{}}(_{S}^{*})\). We further get

\[_{S}^{*}-_{S}^{*}&=[ _{S^{}}(_{S^{}}^{*})-_{S^{ }}(_{S}^{*})]+[_{S^{}}(_{S}^ {*})-_{S}^{*}]\\ &}{{}}_{S^{}}( _{S^{}}^{*})-_{S^{}}(_{S}^{*}) }{{}}(_{S^{ }}()}{}_{=_ {S}^{*}})(_{S^{}}^{*}-_{S}^{*})\\ &}{{}}-_{}^{-1}F_{S^{}}(_{S}^{*},_{S}^{*}) _{}F_{S^{}}(_{S}^{*},_{S}^{*}) (_{S^{}}^{*}-_{S}^{*}),\] (11)where the approximate equation \((i)\) leaving out the term \([_{S^{}}(_{S}^{*})-_{S}^{*}]\) which is bounded in Appendix A, Lemma 2, and does not affect the overall unlearning guarantee. The approximate equation \((ii)\) is the linear approximation step and is the response Jacobian of the auxiliary function \(_{S^{}}()\). The approximate equation \((iii)\) is due to the implicit function theorem. This gives that

\[_{}F_{S}(_{S}^{*},_ {S}^{*})(_{S^{}}^{*}-_{S}^{*})+_{ {}}F_{S^{}}(_{S}^{*},_{S}^{*})( {}_{S^{}}^{*}-_{S}^{*})=_{}F_{S^{}}(_{S}^{*},_{S}^{*})(_{S^{ }}^{*}-_{S}^{*}),\] (12)

which implies the following approximation of \(_{S}^{*}\):

\[_{S}^{*}_{S}^{*}+[_{}F_{S^{}}(_{S}^{*},_{S}^{*})] ^{-1}_{z_{i} U}_{}f(_{S}^{*},_{S}^{ *};z_{i}).\] (13)

The above informal derivation indicates that the minimax unlearning update relies on the total Hessian to sufficiently remove the data influence (Liu et al., 2023; Zhang et al., 2023), rather than the conventional Hessian that appears in standard statistical unlearning (Guo et al., 2020; Sekhari et al., 2021; Suriyakumar and Wilson, 2022; Mehta et al., 2022). The update in eq.(13) has a close relation to the complete Newton step in the second-order minimax optimization literature (Zhang et al., 2020), which motivates the complete Newton-based minimax unlearning. However, due to the various approximations in the above informal derivation, we cannot have a certified minimax unlearning guarantee. Below, we will formally derive the upper bound for these approximations in the closeness upper bound analysis. Based on the closeness upper bound, we will introduce the Gaussian mechanism to yield distribution indistinguishably result in the sense of \((,)\)-certified minimax unlearning.

### Proposed Certified Minimax Unlearning

We first provide algorithms under the setting of the smooth and strongly-convex-strongly-concave (SC-SC) loss function as described in Assumptions 1&2.

```
0: Dataset \(S:\{z_{i}\}_{i=1}^{n}^{n}\), loss function: \(f(,;z)\).
1: Compute \[(_{S}^{*},_{S}^{*})_{}_{ }F_{S}(,)=_{i=1}^{n}f(, ;z_{i}).\] (14)
0:\((_{S}^{*},_{S}^{*},_{}F_{S} (_{S}^{*},_{S}^{*}),_{}F_{S}( {}_{S}^{*},_{S}^{*}))\) ```

**Algorithm 1** Minimax Learning Algorithm \((A_{sc-sc})\)

```
0: Dataset \(S:\{z_{i}\}_{i=1}^{n}^{n}\), loss function: \(f(,;z)\).
1: Compute \[(_{S}^{*},_{S}^{*})_{}_{ }F_{S}(,)=_{i=1}^{n}f(, ;z_{i}).\] (15)
2:\(_{}F_{S^{}}(_{S}^{*},_{S}^{*})=(_{}F_{S}( _{S}^{*},_{S}^{*})-_{z_{i} U}_{}f( _{S}^{*},_{S}^{*};z_{i})).\) (16)
3: Define \[}=_{S}^{*}+[_{}F_{S^{}}(_{S}^{*},_{S}^{*})]^{-1} _{z_{i} U}_{}f(_{S}^{*},_{S}^{*};z_ {i}),\] (17)
4:\[}=_{S}^{*}+[_{}F_ {S^{}}(_{S}^{*},_{S}^{*})]^{-1}_{z_{i} U} _{}f(_{S}^{*},_{S}^{*};z_{i}).\] (18)
5:\(^{u}=}+_{1}\), where \(_{1}(0,_{1}_{d_{1}})\) and \(^{u}=}+_{2}\), where \(_{2}(0,_{2}_{d_{2}})\).
6:\((^{u},^{u})\). ```

**Algorithm 2** Certified Minimax Unlearning for Strongly-Convex-Strongly-Concave Loss \((_{sc-sc})\)

```
0: Dataset \(S:\{z_{i}\}_{i=1}^{n}^{n}\), loss function: \(f(,;z)\).
1: Compute \[(_{S}^{*},_{S}^{*})_{}_{}F_{S}(,_{S}^{*})=_{i=1}^{n}f(, ;z_{i}).\] (19)
2: Compute \[(_{S}^{*},_{S}^{*})_{}_{}F_{S}(_{S}^{*},_{S}^{*})=_{i=1}^{n}f( ,;z_{i}).\] (20)
3:\((_{S}^{*},_{S}^{*},_{S}^{*})_{}_{}F_{S}(_{S}^{*},_{S}^{*})= _{i=1}^{n}f(,;z_{i})\). ```

**Algorithm 3** Certified Minimax Unlearning for Strongly-Convex-Strongly-Concave Loss \((_{sc-sc})\)

```
0: Dataset \(S:\{z_{i}\}_{i=1}^{n}^{n}\), loss function: \(f(,;z)\).
1: Compute \[(_{S}^{*},_{S}^{*})_{}_{}F_{S}(_{S}^{*},_{S}^{*})=_{i=1}^{n}f(,;z_{i}).\] (21)
2: Compute \[(_{S}^{*},_{S}^{*})_{}_{}F_{S}(_{S}^{*},_{S}^{*})=_{i=1}^{n}f(,;z_{i}).\] (22)
3: Compute \[(

Minimax Learning algorithm.We denote our learning algorithm by \(A_{sc-sc}\) and the pseudocode is shown in Algorithm 1. Given a dataset \(S=\{z_{i}\}_{i=1}^{n}\) of size \(n\) drawn independently from some distribution \(\), algorithm \(A_{sc-sc}\) computes the optimal solution \((_{S}^{*},_{S}^{*})\) to the empirical risk \(F_{S}(,)\). \(A_{sc-sc}\) then outputs the point \((_{S}^{*},_{S}^{*})\) as well as the additional memory variables \(T(S):=\{_{}F_{S}(_{S}^{*},_ {S}^{*}),_{}F_{S}(_{S}^{*},_{S}^{ *})\}\), which computes and stores the total Hessian of \(F_{S}(,)\) at \((_{S}^{*},_{S}^{*})\).

Minimax Unlearning AlgorithmWe denote the proposed certified minimax unlearning algorithm by \(_{sc-sc}\) and present its pseudocode in Algorithm 2. Algorithm \(_{sc-sc}\) takes the following inputs: the set of delete requests \(U=\{z_{j}\}_{j=1}^{m}\) of size \(m\), the trained minimax model \((_{S}^{*},_{S}^{*})\), and the memory variables \(T(S)\). To have the certified minimax unlearning for \(\), eq.(15) computes the total Hessian of \(F_{S^{}}(_{S}^{*},_{S}^{*})\) by \(_{}F_{S}(_{S}^{*}, {}_{S}^{*})-_{z_{i} U}_{}f(_{S}^{*},_{S}^{*},z_{i})\), where the former term can be retrieved from the memory set and the latter is computed on the samples to be deleted; eq.(17) computes the intermediate \(}\) by the complete Newton step based on the total Hessian \(_{}F_{S}(_{S}^{*},_{S}^{*})\); Line 3 injects calibrated Gaussian noise \(_{1}\) to ensure \((,)\)-certified minimax unlearning. The certified minimax unlearning for \(\) is symmetric. We provide detailed analysis for Algorithm 2 including minimax unlearning certification, generalization results, and deletion capacity in Appendix B.1.

### Certified Minimax Unlearning without Total Hessian Re-computation

We extend Algorithm 2 and propose Algorithm 3 to reduce the computational cost of Algorithm 2. The complete Newton steps in eq.(19) and eq.(20) utilize the total Hessian \(_{}F_{S}(_{S}^{*},_{S}^{*})\) and \(_{}F_{S}(_{S}^{*},_{S}^{*})\) that are directly retrieved from the memory, rather than the updated total Hessian \(_{}F_{S}(_{S}^{*},_{S}^{*})\) and \(_{}F_{S^{}}(_{S}^{*},_ {S}^{*})\) used in Algorithm 2. The form in eq.(19) and eq.(20) can also be regarded as the total Hessian extension of the infinitesimal jackknife. In this way, it gets rid of the computationally demanding part of re-evaluating the total Hessian for samples to be deleted, which significantly reduces the computational cost. It turns out to be the same computational complexity as the state-of-the-art certified unlearning method developed for STL models (Suriyakumar and Wilson, 2022). Moreover, Algorithm 3 can be more appealing for the successive data deletion setting.

```
0: Delete requests \(U:\ \{z_{j}\}_{j=1}^{m} S\), output of \(A_{sc-sc}(S)\): \((_{S}^{*},_{S}^{*})\), memory variables \(T(S)\): \(\{_{}F_{S}(_{S}^{*},_{S}^{*}), _{}F_{S}(_{S}^{*},_{S}^{*})\}\), loss function: \(f(,;z)\), noise parameters: \(_{1}\), \(_{2}\).
1: Compute

\[}=_{S}^{*}+[_{}F_{S}(_{S}^{*},_{S}^{*})]^{-1}_{z_{i} U }_{}f(_{S}^{*},_{S}^{*};z_{i}),\] (19)

\[}=_{S}^{*}+[_{} F_{S}(_{S}^{*},_{S}^{*})]^{-1}_{z_{i} U}_{}f( _{S}^{*},_{S}^{*};z_{i}).\] (20)
2: \(}^{u}=}+_{1}\), where \(_{1}(0,_{1}_{d_{1}})\) and \(}^{u}=}+_{2}\), where \(_{2}(0,_{2}_{d_{2}})\).
3: \((}^{u},}^{u})\). ```

**Algorithm 3** Efficient Certified Minimax Unlearning \((_{})\)

### Analysis for Algorithm 3

\((,)\)**-Certificated Unlearning Guarantee.** The intermediate variables \((},})\) are distinguishable in distribution from the retraining-from-scratch variables \((_{S^{}}^{*},_{S^{}}^{*})\) because they are deterministic and the Taylor expansion introduces a certain amount of approximation. The following lemma quantifies the closeness between \((},})\) and \((_{S^{}}^{*},_{S^{}}^{*})\), which can be regarded as the "sensitivity" when applying the Gaussian mechanism.

**Lemma 1** (**Closeness Upper Bound.**).: _Suppose the loss function \(f\) satisfies Assumption 1 and 2, \(\|_{}F_{S}(_{S}^{*},_{S}^{*})\| _{}\) and \(\|_{}F_{S}(_{S}^{*},_{S}^{*})\| _{}\). Let \(=\{_{},_{},_{},_{}\}\). Then, we have the closeness bound between \((},})\) in Line 1 of Algorithm 3 and \((_{S^{}}^{*},_{S^{}}^{*})\) in eq.(9):_

\[\{\|_{S^{}}^{*}-}\|,\|_{S^{ }}^{*}-}\|\}L^{2}^{2}/ ^{6}+2L^{2}/^{3})m^{2}}{n^{2}}.\] (21)Equipped with Lemma 1, we have the following certified unlearning guarantee by adding Gaussian noise calibrated according to the above closeness result. Due to the minimax structure, our analysis is more involved than the STL case (Sekhari et al., 2021; Suriyakumar and Wilson, 2022).

**Theorem 2** (\((,)\)-**Minimax Unlearning Certification**).: _Under the same settings of Lemma 1, our minimax learning algorithm \(A_{sc-sc}\) and unlearning algorithm \(_{}\) is \((,)\)-certified minimax unlearning if we choose_

\[_{1}_{2}=L^{2}^{3}/^{6}+2 L^{2}/^{3})m^{2}}{n^{2}}.\] (22)

**Generalization Guarantee.** Theorem 3 below provides the generalization result in terms of the population PD risk for the minimax unlearning algorithm \(_{}\).

**Theorem 3** (**Population Primal-Dual Risk**).: _Under the same settings of Lemma 1 and denote \(d=\{d_{1},d_{2}\}\), the population weak and strong PD risk for the certified minimax unlearning variables \((}^{u},}^{u})\) returned by Algorithm 3 are_

\[^{w}(}^{u},}^{u })=((L^{3}^{3}/^{6}+L^{2}^{2}/^{3}) }{n^{2}}+}{ n}), \\ ^{s}(}^{u},}^{u})=((L^{3}^{3}/^{6}+L^{2}^{2}/^{3})}{n^{2}}+}{ n}+}{^{2}n} ).\] (23)

**Deletion Capacity.** The population weak and strong PD risk given in Theorem 3 for the output of unlearning algorithms provides the following bound on deletion capacity.

**Theorem 4** (**Deletion Capacity**).: _Under the same settings of Lemma 1 and denote \(d=\{d_{1},d_{2}\}\), the deletion capacity of Algorithm 3 is_

\[m_{,,}^{A,}(d_{1},d_{2},n) c }{(d(1/))^{1/4}},\] (24)

_where the constant \(c\) depends on \(L,l,,\) and \(\) of the loss function \(f\)._

## 5 Certified Minimax Unlearning for Convex-Concave Loss Function

We further extend the certified minimax unlearning for the convex-concave loss function. In addition, Appendix C will provide the extension to convex-strongly-concave and strongly-convex-concave loss functions. Give the convex-concave loss function \(f(,;z)\), similar to the unlearning for STL models (Sekhari et al., 2021), we define the regularized function as \((,;z)=f(,;z)+ {2}\|\|^{2}-\|\|^{2}\). Suppose the function \(f\) satisfies Assumption 1, then the function \(\) is \(\)-strongly convex in \(\), \(\)-strongly concave in \(\), \((2L+\|\|+\|\|)\)-Lipschitz, \((2+)\)-gradient Lipschitz and \(\)-Hessian Lipschitz. It suffices to apply the minimax learning and unlearning algorithms in Sec.4 to the regularized loss function with a properly chosen \(\). We denote the learning and unlearning algorithms for convex-concave losses as \(A_{c-c}\) and \(_{c-c}\). Their implementation details are given in Appendix C. We suppose the SC-SC regularization parameter \(\) satisfies \(<\). Theorem 5 below summarizes guarantees of \((,)\)-certified unlearning and population primal-dual risk (weak and strong) for Algorithm \(_{c-c}\).

**Theorem 5**.: _Let Assumption 1 hold and \(d=\{d_{1},d_{2}\}\). Suppose the parameter spaces \(\) and \(\) are bounded so that \(_{}\|\| B_{}\) and \(_{}\|\| B_{}\). We have,_

1. \((,)\)_-Minimax Unlearning Certification: Our minimax learning algorithm_ \(A_{c-c}\) _and unlearning algorithm_ \(_{c-c}\) _is_ \((,)\)_-certified minimax unlearning._
2. _Population Weak PD Risk: The population weak PD risk for_ \((^{u},^{u})\) _by algorithm_ \(_{c-c}\) _is_ \[^{w}(^{u},^{u}) (L^{3}^{3}/^{6}+L^{2}^{2}/^{3})}{n^{2}}+}{ n}+(B_{ }^{2}+B_{}^{2}).\] (25) _In particular, by setting_ \(\) _below_ \[=}^{2}+B_{}^{2}}}},(^{2}m^{2}}{(B_{}^{2}+B_{}^{2})n^{2}})^{1/4},( ^{3} m^{2}}{(B_{}^{2}+B_{}^{2})n ^{2}})^{1/7}},\] (26)_we have the following population weak PD risk,_

\[^{}(^{},^{}) c_{1}}+c_{2}}^{1/8}}+c_{3}}{}^{1/7}()^{2/7},\] (27)

_where \(c_{1},c_{2},c_{3}\) are constants that depend only on \(L,l,,B_{}\) and \(B_{}\)._
3. _Population Strong PD Risk:_ _The population strong PD risk for_ \((^{},^{})\) _by algorithm_ \(_{c-c}\) _is_ \[^{}(^{},^{})(L^{3}^{3}/^{6}+L^{2}^{2}/ ^{3})}{n^{2}}+}{  n}+}{^{2}n}+(B_{}^{2}+B_{ {}}^{2}).\] (28) _In particular, by setting_ \(\) _below_ \[=}^{2}+B_{}^{2}}}},}{(B_{}^{2}+ B_{}^{2})n}\,,^{2}m^{2}}{(B_{}^{2}+B_{}^{2})n^{2}}^{1/4},\] (29) _we have the following population strong PD risk,_ \[^{}(^{},^{})c_{1}}+c_{2}}+c _{3}}^{1/8}} +c_{4}}{}^{1/7}( )^{2/7},\] (30) _where_ \(c_{1},c_{2},c_{3},c_{4}\) _are constants that depend only on_ \(L,l,,B_{}\) _and_ \(B_{}\)_._
4. _Deletion Capacity:_ _The deletion capacity of Algorithm_ \(_{c-c}\) _is_ \[m^{A,}_{,,}(d_{1},d_{2},n) c}{(d(1/))^{1/4}},\] (31) _where the constant_ \(c\) _depends on the constants_ \(L,l,,B_{}\) _and_ \(B_{}\)_._

## 6 Conclusion

In this paper, we have studied the certified machine unlearning for minimax models with a focus on the generalization rates and deletion capacity, while existing works in this area largely focus on standard statistical learning models. We have provided a new minimax unlearning algorithm composed of the total Hessian-based complete Newton update and the Gaussian mechanism-based perturbation, which comes with rigorous \((,)\)-unlearning certification. We have established generalization results in terms of the population weak and strong primal-dual risk and the correspondingly defined deletion capacity results for the strongly-convex-strongly-concave loss functions, both of which match the state-of-the-art results obtained for standard statistical learning models. We have also provided extensions to other loss types like the convex-concave loss function. In addition, we have provided a more computationally efficient extension by getting rid of the total Hessian re-computation during the minimax unlearning phase, which can be more appealing for the successive data deletion setting. Although our bound for deletion capacity is better than that of DP by an order of \(d^{1/4}\) and matches the state-of-the-art result established for unlearning under the STL setting, it remains unclear whether this bound is tight or not. In future work, we plan to extend to more general settings like the nonconvex-nonconcave loss function setting.