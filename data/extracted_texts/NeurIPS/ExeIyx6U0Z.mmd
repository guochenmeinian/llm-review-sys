# LLaNA: Large Language and NeRF Assistant

Andrea Amaduzzi

andrea.amaduzzi4@unibo.it

Pierluigi Zama Ramirez

pierluigi.zama@unibo.it

Giuseppe Lisanti

giuseppe.lisanti@unibo.it

Samuele Salti

samuele.salti@unibo.it

Luigi Di Stefano

luigi.distefano@unibo.it

CVLAB, University of Bologna

https://andreamaduzzi.github.io/llana/

###### Abstract

Multimodal Large Language Models (MLLMs) have demonstrated an excellent understanding of images and 3D data. However, both modalities have shortcomings in holistically capturing the appearance and geometry of objects. Meanwhile, Neural Radiance Fields (NeRFs), which encode information within the weights of a simple Multi-Layer Perceptron (MLP), have emerged as an increasingly widespread modality that simultaneously encodes the geometry and photorealistic appearance of objects. This paper investigates the feasibility and effectiveness of ingesting NeRF into MLLM. We create LLaNA, the first general-purpose NeRF-language assistant capable of performing new tasks such as NeRF captioning and Q&A. Notably, our method directly processes the weights of the NeRF's MLP to extract information about the represented objects without the need to render images or materialize 3D data structures. Moreover, we build a dataset of NeRFs with text annotations for various NeRF-language tasks with no human intervention. Based on this dataset, we develop a benchmark to evaluate the NeRF understanding capability of our method. Results show that processing NeRF weights performs favourably against extracting 2D or 3D representations from NeRFs.

## 1 Introduction

Large Language Models (LLMs)  have revolutionized the field of Natural Language Processing, demonstrating incredible text comprehension and generation capabilities. These results have fostered the development of Multimodal LLMs (MLLMs) , which can ingest various modalities such as images, videos, and audio, to generate text describing and reasoning about the content of such modalities. Recently, MLLMs have also been extended to 3D data , primarily represented through colored point clouds, yielding remarkable results even in this scenario.

Beyond images and 3D data, another paradigm is emerging to represent objects and scenes: Neural Radiance Fields (NeRFs) . NeRFs are coordinate-based neural networks, typically Multi-Layer Perceptrons (MLPs), designed to capture both the geometry and the photorealistic appearance of an object by learning a continuous radiance field at each 3D spatial location. After training, a NeRF model can be queried to render realistic images or to reconstruct the 3D surface of the encoded object. Therefore, capturing an object as a NeRF provides an interesting alternative to create a digital twinwith respect to standard representations such as multi-view images or point clouds. For instance, thanks to its continuous formulation, from a single NeRF, one can generate an infinite number of photorealistic images at any resolution while storing only the weights of an MLP instead of the entire image set. See Appendix A.4 for more on the memory advantages of using NeRFs. Due to their advantages, NeRFs are effectively becoming a new modality stored and communicated independently, with datasets of NeRFs being made publicly available [25; 61] and companies providing digital twins of objects represented as NeRFs (e.g., https://lumalabs.ai/).

The increasing adoption of NeRFs and their appealing characteristics prompted us to the following research question: is it possible to build an MLLM able to ingest directly NeRFs? Inspired by recent studies on meta-networks that can process neural fields [81; 42], we answer this question in the positive by showing that it is possible to process the weights of a given NeRF with a meta-network encoder that projects the NeRF weights into the embedding space of a pre-trained LLM such as LLaMA 2 . By doing so, we create the first MLLM for NeRFs, dubbed Large Language and NeRF Assistant (LLaNA), which can solve NeRF-language tasks such as NeRF captioning, Q&A and zero-shot NeRF classification (see Fig. 1).

We also introduce a new NeRF-language dataset, that we will make publicly available, to train LLaNA and test the capabilities of our assistant. To collect this dataset, we designed an automated annotation framework that leverages MLLMs to produce text annotations for NeRFs trained on Shapenet . Using this dataset alongside an additional split containing manually curated textual descriptions , we establish a benchmark for NeRF textual assistants.

Since a straightforward way to create an assistant for NeRFs would be to render images or extract 3D point clouds out of it and provide them as input to existing MLLMs specifically designed to handle such modalities, we thoroughly compare LLaNA against these baselines on the proposed benchmark. We show how the resolution of the extracted 3D geometry or images, and for images also the vantage point used for rendering, negatively impact the quality of the MLLM's output. Important details might be lost by rendering from the wrong angle, or the extracted geometry might not be detailed enough. Vice versa, by operating directly on the MLP weights, we extract all the information they hold about the object without any other design decision. Our approach turns out to be the most effective way to create a NeRF assistant as it consistently outperforms MLLMs processing images or 3D geometries extracted by querying NeRFs. Our contributions can be summarized as follows:

\(\) LLaNA, the first MLLM capable of performing tasks such as captioning and Q&A on NeRFs.

\(\) We show that it is possible to build such an assistant by directly processing the NeRFs weights with a meta-encoder, which is faster and captures more information than rendering images or extracting 3D data.

\(\) We automatically create a NeRF-language benchmark based on ShapeNet, and we thoroughly evaluate LLaNA on it, showing that it performs better than applying popular MLLMs on discrete representations obtained from NeRFs.

Figure 1: **LLaNA.** The first Multimodal Large Language Model that understands and reasons on an input NeRF. Our framework directly processes the NeRF weights and performs tasks such as captioning, Q&A, and zero-shot classification of NeRFs.

Related work

Multimodal Large Language Models (MLLMs).Significant advancements have been made by Large Language Models (LLMs) in language understanding, reasoning, and generalization capabilities [62; 1; 54; 70; 75; 60]. These models have been extended into Multimodal Large Language Models (MLLMs), which broaden their reasoning abilities by including other modalities like images [14; 82; 17; 19], audio , and videos [47; 10]. MLLMs generally align target features with textual ones and then integrate them into LLMs for various text inference tasks. Some MLLMs are trained entirely from scratch [27; 56], others utilize pretrained LLMs [37; 4; 44; 38; 11]. 3D MLLMs focus on understanding the 3D world typically represented as colored point clouds [58; 24; 86; 20; 78] or multi-view images . Some of these models are trained using 2D images [24; 86; 23] while others directly align textual phrases with points [20; 78; 58].

Neural radiance fields.NeRF  have been applied in several visual tasks such as novel view synthesis , generative media , and robotics . The base formulation employs MLPs to convert spatial coordinates into colors and densities. Recent advancements substitute or enhance MLPs with explicit data structures [9; 68; 16; 52] for faster training and inference.

Neural radiance fields and language.The interaction between NeRF and language has been recently investigated for several practical applications. Many works address the problem of generating geometrically consistent views of objects or scenes described by textual prompts [66; 49; 31; 65; 40; 36; 57]. Other approaches focus on editing the scene represented by a NeRF from text, e.g., by changing the appearance and shape of objects [73; 28; 67; 74; 69; 21; 80; 87], or by inserting/removing objects in the scene [3; 51]. Some techniques investigate new types of radiance fields that predict language features for each spatial location alongside density and color [32; 34]. By distilling knowledge from vision-language models into these models, the neural fields can be queried by textual prompts. LERF  extends the original radiance field formulation, considering functions which model density, color and language features at each spatial coordinate. Such _language fields_ are parametrized by a neural network. Unlike all previous methods, Ballerini et al.  is the first to utilize the weights of a NeRF's MLP as an input modality. They aim to learn a mapping between the NeRF and CLIP  embedding spaces to perform tasks such as NeRF retrieval from textual or image queries. Differently, our goal is to develop an MLLM capable of reasoning about NeRFs.

Deep learning on neural networks.Several studies have explored using meta-networks, i.e. neural networks that analyze other neural networks. Initially, researchers concentrated on predicting network characteristics, such as accuracy and hyperparameters, by processing their weights [71; 64; 33; 30; 45]. Several recent works focus on processing networks implicitly representing data (Implicit Neural Representations or Neural Fields). These methods perform tasks such as classifying or segmenting the data by processing solely the weights of the input neural networks. Among these works, Functa  trains a shared network on the entire dataset and then learns a compact embedding for each sample for downstream tasks. Later works concentrate on processing networks representing individual data samples, e.g., a specific object. By leveraging a novel encoder architecture for MLP weights, inf2vec extracts compact embeddings from INRs of 3D shapes, which are employed as inputs for downstream tasks. nf2vec extends inf2vec to ingest the NeRF's network weights to classify, segment, or retrieve similar NeRFs. Cardace et al.  develop a strategy to process neural fields represented by a hybrid tri-plane structure. Other approaches [53; 84; 85; 83] develop equivariant architectures to handle MLPs by exploiting weight space symmetries  as an inductive bias. Also, Graph Neural Networks have been investigated to compute a network representation [35; 42]. Since we aim to process NeRFs directly from the network weights, we employ nf2vec as our meta-encoder due to its efficient and scalable architecture.

## 3 Methodology

This section describes the proposed Large Language and NeRF Assistant (LLaNA). We provide an overview of NeRFs and the meta-encoder that maps NeRF weights into a global embedding. Then, we present the overall LLaNA framework and discuss our training protocol.

NeRF)  is a framework that employs coordinate-based neural networks, typically MultiLayer Perceptrons (MLP) and is trained on a collection of images of an object or scene taken from various vantage points. The main application of NeRFs is the task of novel views synthesis, i.e., photorealistic rendering of images from viewpoints unseen at training time. In its base formulation, the MLP is a function of continuous 3D coordinates \(=(x,y,z)^{3}\), that yields four-dimensional outputs, \(RGB^{4}\). This output encodes the \(RGB\) color and the volume density \(\) of each 3D location in the scene. The volume density \(\) can be interpreted as the differential probability of a ray terminating at point \(\). After training, a NeRF can render images from any desired viewpoint at arbitrary resolution by querying it for the values of \(RGB\) and \(\) at several points along the ray corresponding to each pixel and applying the volumetric rendering equation .

In this work, we realize NeRFs as MLPs composed of \(L\) hidden layers, an input layer, and an output layer. An example of MLP with \(1\) input, \(1\) output, and \(1\) hidden layer is shown in Fig. 2 (left). A layer is parameterized by a weight matrix plus a bias vector. More in detail, the hidden layers in our architecture have the same number of input and output neurons, \(H\), thus having squared weight matrices \(_{l}^{H H}\) for \(l=1,,L\) and \(H\)-dimensional biases \(_{l}^{H}\). As input \(\) goes through a 24-frequency encoding , the first layer has \(_{in}^{144 H}\) and \(_{in}^{H}\). The final one has \(_{out}^{H 4}\) and \(_{out}^{4}\). Refer to Appendix A for more details on NeRFs.

Meta-encoderIn this work, we explore how a NeRF assistant can be realized by processing the NeRF weights directly. We expect the NeRF weights to contain comprehensive information about the represented object, such as its geometry and appearance. Thus, an encoder processing them might extract all the necessary information for downstream language tasks such as captioning and Q&A.

Inspired by the recent development of meta-networks capable of processing neural fields [42; 81], we employ as our meta-encoder architecture nf2vec. It takes as input the weights of a NeRF and yields a global embedding that distills the content of the input NeRF. In particular, the weight matrices and biases of the input NeRF are stacked along the row dimension to form a matrix \(^{S H}\), where the number of rows \(S\) depends on the number of hidden layers \(L\), the number of units per hidden layer \(H\), and the dimension of the input, which is a \(144\)-dimensional array obtained by frequency encoding of the 3D coordinates. Before stacking, we pad the output layer weights \(_{out}\) and biases \(_{out}\) with zeros to obtain \(H\) columns (see Fig. 2, center).

The meta-encoder is parametrized as an MLP with batch normalization layers  and ReLU non-linearities. To scale gracefully with the input MLP dimensions, the encoder processes each row of \(\) independently, extracting a total of \(S\) tokens, each of length \(G\), from an input NeRF. They are then max-pooled to obtain a global representation \(g^{G}\) of the NeRF, with \(G=1024\) in our experiments. The encoder is pre-trained using the self-training protocol of nf2vec, i.e., jointly with a decoder architecture that, given as input the NeRF global embedding, reconstructs the same images as the input NeRF from arbitrary viewpoints. More details in Appendix B.

Large language and NeRF assistantInspired by recent approaches that created effective Multimodal Large Language Models, we build LLaNA by leveraging on a pre-trained LLM with a transformer backbone , in our experiments LLaMA 2 , and injecting the NeRF modality into its embedding input space, as proposed for images and 3D data [44; 78] (see Fig. 2, right). Thanks to

Figure 2: **Framework overview. Example of NeRF captioning.**

the self-attention mechanism, the transformer can understand the contextual relationships between text and NeRF tokens, enabling it to generate responses based on both text and NeRF inputs.

We employ a trainable linear projection layer, \(\), to project the embedding of the input NeRF computed by the meta-encoder into the LLaMA 2 embedding space. The projection layer has weights \(_{proj}^{G T}\), where \(T\) is the word embedding dimension of the employed LLaMA model. This embedding is encapsulated between two special tokens, whose embeddings are learned end-to-end while training, namely <n_start> and <n_end>.

Then, given an input sequence of mixed NeRF and word tokens, (<n_start>,\((g)\),<n_end>,\(w_{1},w_{2},...,w_{k}\)), where \(k\) is the number of word tokens, the large language model returns a sequence of predicted word tokens \((_{k+1},_{k+2},,_{cos})\).

Training protocolOur framework is trained on the ShapeNeRF-Text dataset, described in detail in Sec. 4. This dataset is organized into a set of prompts from the user and expected ground-truth answers that are used to optimize the original auto-regressive objective of the LLM. For the meta-encoder, we employ the nf2vec encoder pre-trained on ShapeNet released by the authors , and we keep it frozen during training. We follow the two-stage training protocol delineated in Liu et al. :

_Stage1: projector training._ In the first stage, we train the projector network \(\) to align the NeRF and the word embedding spaces while keeping the LLM weights fixed. We train on an instruction dataset of brief descriptions to learn the projection layer efficiently. We also train the embeddings of the special tokens used to encapsulate the NeRF one. We optimize the projector weights and the embeddings for \(3\) epochs with a learning rate of \(0.002\) and batch size of \(64\).

_Stage2: instruction tuning._ During the second stage, we train on complex instructions to help the model understand and reason about NeRF data. In this phase, we optimize both the projector and the LLM for \(3\) epochs on the detailed descriptions, single-round and multi-round Q&A conversations available in our dataset. For this phase, we employ a learning rate of \(0.0002\) and a batch size of \(16\).

Our model is implemented in PyTorch and trained on 4 NVIDIA A100 with 64GB of VRAM each. Completing both stages requires \(\)1 day of training.

## 4 Benchmark

### ShapeNeRF-Text dataset

To train and validate our NeRF assistant, we automatically created a dataset of conversations about NeRFs, the ShapeNeRF-Text dataset.

It features paired NeRFs and language annotations for ShapeNet objects , in particular for all the 40K NeRFs available in the nf2vec dataset . We followed the structure defined in PointLLM  to create the textual annotations. More in detail, for each object, we generated a _brief description_, a _detailed description_, 3 _single-round Q&As_, and one _multi-round Q&A_. The brief descriptions are concise captions of the object, taking into account its global structure and appearance. The detailed descriptions are longer sentences that describe all the details of the object. The single-round Q&As consist of a question about the object and the corresponding ground-truth answer. Finally, the multi-round Q&As are longer conversations formed by 3 questions and the relative answers. The automatic data annotation pipeline is inspired by Cap3D  and is shown in Fig. 3. First, multiple views of each ShapeNet object have been rendered from different perspectives. Then, each view has been provided as input to LLaVA (LLaVA2-13b)  to get a detailed description of the object from that point of view. Afterward, starting from the captions generated by LLaVA, LLaMA 3 (LLaMA3-8B-chat) was used to generate the final ground-truth text data (brief and

Figure 3: **Automatic annotation pipeline.** Given a 3D model, \(N\) views are rendered and processed by a VLM (LLaVA) to generate view-specific captions. These are aggregated by an LLM (LLaMA) for final descriptions and Q&A.

detailed descriptions, single and multi-round Q&As). Both the frozen LLMs employed to create our benchmark (LLaVA2-13b, LLaMA3-8b-chat) are equipped with safeguards.

When building the ground-truth data, to ensure diversity in the language annotations, each brief and detailed description has been associated with a question randomly sampled from 30 instructions for each kind of description. Such instructions, together with the carefully engineered request prompts for LLaVA and LLaMA, are reported in Appendix C.1.

ShapeNeRF-Text provides 30939, 3846 and 3859 objects for the train, validation and test sets, respectively. Overall, the dataset features 13 object classes, and the train, validation and test splits are obtained by randomly sampling objects within each class, i.e., holding out a fixed percentage of objects per class (80%, 10%, and 10% for the sets, respectively). Appendix C.2 provides more dataset statistics. As quantitatively proven in Appendix C.3 and Appendix D.1, many of the questions belonging to the Q&A set require a holistic 3D understanding of the object, to be answered correctly.

### Language tasks and metrics

We evaluate NeRF assistants on three different language tasks, given an input NeRF: brief captioning, detailed captioning, and single-round Q&A. We evaluate all tasks on the objects from the ShapeNeRF-Text test set. For brief captioning, we additionally evaluate the methods on the GPT2Shape Human Shape Text (HST) dataset , a subset of ShapeNet for which human-curated brief descriptions are publicly available. To generate the dialogues for HST, we randomly pair each of its captions with one of the 30 instructions requesting a brief description, used in ShapeNeRF-Text and reported in Appendix C.1. We employ standard language similarity metrics to evaluate these methods. We compute the cosine similarity between the global embeddings of the generated and ground-truth sentences provided by the pre-trained encoders Sentence-BERT  and SimCSE . These metrics based on learned networks are the most effective at measuring the quality of the generated output. We also include standard handcrafted metrics based on n-gram statistics, like BLEU-1 , ROUGE-L , and METEOR .

## 5 Experiment results

### Foundation models as baselines

As our method is the first to investigate language tasks on NeRF, there are no baselines in the literature. However, given a NeRF, a straightforward way to create an assistant for it could be to render an image and use an MLLM capable of ingesting images. Alternatively, we could extract the 3D shape from the NeRF and use one of the recent 3D MLLMs. Hence, in a first set of experiments, we use MLLMs as off-the-shelf foundation models, trained on hundreds of thousands of shapes or millions of images, without performing any fine-tuning on the training set of ShapeNeRF-Text, and consider such pipelines as natural baselines. Specifically, we use LLaVA (v1.6)  and BLIP-2  for images, as well as PointLLM  and GPT4Point  for colored point clouds. Since NeRFs can render arbitrary viewpoints after training, we also include the evaluation of LLaVA  in a multi-view scenario. More in detail, we render images from \(N\) viewpoints randomly sampled between the set of camera poses used to train each NeRF; then, we concatenate tokens from these N images and fed them into LLaVA alongside text instructions. We set \(N\)=3 because the model cannot process a higher number of images correctly. In addition, we test 3D-LLM  to compare its performance to LLaNA. We employ the official code and pre-trained models released by the respective authors for such evaluations 1. We note that the only official GPT4Point weights available at submission time were those obtained from fine-tuning OPT-2.7B on Cap3D . In Tabs. 1 to 5, we present the performance of all methods under the more realistic scenario where NeRFs are treated as the only input data to the assistant. Hence, images and point clouds can only be extracted from NeRFs. Details on the extraction procedure are provided in Appendix A.3. As for 3D-LLM, we extract colored 3D meshes from the NeRFs of ShapeNeRF-Text and process such data with the official 3D-LLM code to render images from multiple views and compute both the 2D and 3D features required by the model at inference time. Moreover, in Appendix E, we report the results dealing with the images used to train the NeRF or the original 3D point cloud from ShapeNet, which confirms the methods' ranking. When rendering an image, a non-obvious design decision for the pipeline is from which vantage point to render it. ShapeNet artificially simplifies this task since all objects have been canonically aligned to a common reference frame, but this may not be the case in a general setting. To show the vantage point's effect on the assistant's results, we report results processing a frontal or back view.

### NeRF captioning

We test the assistants' ability to describe the NeRF content in the captioning experiments. We prompt them with the NeRF, or the image/cloud extracted from it, followed by the question which has been paired with its ground-truth description, as detailed in Section 4.2, e.g. _"What's the content of this NeRF/image/cloud?"_. We then collect the answers generated by the models and compare them with the ground-truth description according to the selected metrics.

Brief description.We report results for the brief description tasks on ShapeNeRF-Text and the HST dataset in Tab. 1 and Tab. 2, respectively. Comparing LLaNA with the baselines described in Sec. 5.1, we appreciate how LLaNA achieves the best performance in most metrics, often by large margins against runner-ups. For instance, for the Sentence-BERT similarity on the ShapeNeRF-Text dataset, LLaNA achieves \(68.63\), \(7.63\) points more than LLaVA-vicuna13b, even if LLaNA uses a smaller LLM. Results on the HST dataset, which provides ground-truth descriptions validated by humans, are generally lower for all methods. Yet, LLaNA provides again the best performance according to most metrics. The difference in the quality of the brief description provided by LLaNA compared to the baselines is showcased by the qualitative result reported in the first row of Fig. 4, where the description provided by LLaNA is the most accurate.

A clear trend in both tables and qualitative results is that image-based models tend to perform better than models processing point clouds. This is likely due to the larger amount of data used during training of the modality encoder, i.e. millions of images versus hundreds of thousands of shapes, which enhances their generalization ability, as well as the capability of images to capture more details than point clouds at the input resolutions required by image-based MLLMs versus 3D MLLMs. Nonetheless, our method, which operates on NeRFs, benefits from a holistic view of the object and provides the most accurate descriptions. Remarkably, in LLaNA, all the necessary information for this

  
**Model** & **Modality** & **S-BERT** & **SmSC** & **BLE-1** & **ROUGEL-L** & **METOR** \\  LLaVA-vicuna-13b & Image(FV) & 50.08 & 58.87 & 23.63 & 23.55 & 22.55 \\ LLaVA-vicuna-13b & Image(FV) & 50.09 & 50.33 & 13.77 & 21.36 & 13.18 \\ LLaVA-vicuna-13b & Image(FV) & 60.21 & 59.51 & 15.07 & 32.16 & 14.64 \\ LLaVA-vicuna-7b & Image(FV) & 57.55 & 57.68 & 14.99 & 22.82 & 14.36 \\ LLaVA-vicuna-7b & Image(FV) & 53.11 & 54.46 & 14.73 & 22.47 & 14.05 \\ BLIP-2 FamTS-val & Image(FV) & 41.27 & 40.69 & 0.18 & 7.83 & 2.60 \\ BLIP-2 FamTS-val & Image(FV) & 38.49 & 37.89 & 0.19 & 7.72 & 2.58 \\  PointML-M-7b & Point(and end) & 59.02 & 58.30 & 10.28 & 19.26 & 10.55 \\ GPT+Point-Opt-Opt-Opt-2.7b & Point(and end) & 42.44 & 38.33 & 3.72 & 9.21 & 5.13 \\ 
3D-LLM & Mesh + MV & 60.00 & 53.91 & 1.58 & 14.40 & 5.28 \\  LLaNA-7b & NeRF & **77.43** & **79.81** & **41.32** & **36.18** & **32.39** \\   

Table 3: **NeRF detailed captioning on ShapeNeRF-Text. Frozen baselines. Best results are in bold, runner-up is underlined. (FV: front-view, BV: back-view, MV: multi-view)**

  
**Model** & **Modality** & **S-BERT** & **SmSC** & **BLE-1** & **ROUGEL-L** & **METOR** \\  LLaVA-vicuna-13b & Image(FV) & 59.08 & 58.87 & 23.63 & 23.55 & 22.55 \\ LLaVA-vicuna-13b & Image(FV) & 50.09 & 50.33 & 13.77 & 21.36 & 13.18 \\ LLaVA-vicuna-13b & Image(FV) & 60.21 & 59.51 & 15.07 & 32.16 & 14.64 \\ LLaVA-vicuna-7b & Image(FV) & 57.55 & 57.68 & 14.99 & 22.82 & 14.36 \\ LLaVA-vicuna-7b & Image(FV) & 53.11 & 54.46 & 14.73 & 22.47 & 14.05 \\ BLIP-2 FamTS-val & Image(FV) & 41.27 & 40.69 & 0.18 & 7.83 & 2.60 \\ BLIP-2 FamTS-val & Image(FV) & 38.49 & 37.89 & 0.19 & 7.72 & 2.58 \\  PointML-M-7b & Point(and end) & 59.02 & 58.30 & 10.28 & 19.26 & 10.55 \\ GPT+Point-Opt-Opt-2.7b & Point(and end) & 42.44 & 38.33 & 3.72 & 9.21 & 5.13 \\ 
3D-LLM & Mesh + MV & 60.00 & 53.91 & 1.58 & 14.40 & 5.28 \\  LLaNA-7b & NeRF & **77.43** & **79.81** & **41.32** & **36.18** & **32.39** \\   

Table 2: **NeRF brief captioning on ShapeNeRF-dataset. Frozen baselines. Best results are in bold, runner-up is underlined. (FV: front-view, BV: back-view, MV: multi-view)**language task can be extracted from a single global embedding obtained by directly processing the NeRF weights. It is also worth pointing out that, while LLaNA directly processes weights and thus is independent by design from spatial resolution, the baselines face a computational overhead growing with the desired resolution due to the necessity of extracting spatial data from NeRF (Appendix A.3). Results show that 3D-LLM performs better than the point-based models and comparably to image-based models. Comparing the results of image-based MLLMs when processing front versus back views, we can see that the vantage point has a non-negligible effect on the performance of such baselines, with SentenceBERT and SimCSE metrics diminishing by about 4 points in all baselines. In a dataset without canonical poses for objects, this would be a relevant limitation that processing NeRF weights seamlessly sidesteps. Finally, we observe that the multi-view setup of LLaVA provides similar performance to the single-view counterpart. In Appendix G, additional qualitative examples are provided.

Detailed description.We evaluate the performance for the detailed description tasks on the proposed ShapeNeRF-Text, reporting the results in Tab. 3. For this task, the point-based model PointLLM  performs similarly to the image-based one, LLaVA . However, we appreciate that LLaNA achieves the best performance in all metrics by large margins. For instance, for the Sentence-BERT metric, LLaNA achieves \(77.43\), notably \(18.35\) points more than LLaVA-vicuna-13b single-view and \(17.22\) for the LLaVA-vicuna-13b multi-view setup. These large improvements indicate that, while individual or aggregated images may be sufficient for brief descriptions, they may lack all the details needed to provide a comprehensive description. Moreover, the dependency

  
**Model** & **Modality** & **S-BERT** & **SimCSE** & **BLEU-1** & **ROUGE-L** & **METEOR** \\  LLaVA-vicuna-13b & Image (FV) & 71.61 & 70.98 & 20.19 & 30.42 & 32.53 \\ LLaVA-vicuna-13b & Image (BV) & 68.25 & 69.06 & 20.03 & 29.84 & 32.27 \\ LLaVA-vicuna-13b & Image (MV) & 71.84 & 71.16 & 20.04 & 30.20 & 33.46 \\ LLaVA-vicuna-7b & Image (FV) & 71.79 & 71.96 & 25.79 & 34.04 & 34.86 \\ LLaVA-vicuna-7b & Image (BV) & 70.88 & 70.93 & 25.17 & 33.30 & 34.22 \\ BLIP-2 FlanF-5x1 & Image (FV) & 45.20 & 47.92 & 11.50 & 20.16 & 13.49 \\ BLIP-2 FlanF-5x1 & Image (BV) & 45.06 & 47.66 & 11.50 & 19.98 & 13.44 \\  PointLLM-7b & Point cloud & 74.70 & 74.40 & 36.81 & 44.41 & 39.76 \\ GPT4Point-Opt-2.7b & Point cloud & 27.62 & 31.41 & 6.26 & 9.38 & 5.41 \\ 
3D-LLM & Mesh + MV & 69.62 & 67.55 & 32.19 & 40.95 & 35.83 \\  LLaNA-7b & NeRF & **81.03** & **81.56** & **46.16** & **53.17** & **50.15** \\   

Table 4: **NeRF single-round Q&A on ShapeNeRF-Text. Frozen baselines.** Best results are in **bold**, runner-up is underlined. (FV: front-view, BV: back-view, MV: multi-view)

Figure 4: **Qualitative results of NeRF captioning and Q&A. Results on ShapeNeRF–Text. From top to bottom: brief and detailed descriptions, single-round Q&A**

[MISSING_PAGE_FAIL:9]

where _<Shapenet_classes>_ are the 10 ShapeNet classes available in our dataset. We consider the answer correct only if the ground truth class appears in the response. We report results in Tab. 5 on the ShapeNeRF-Text dataset. Using multiple views boosts the zero-shot classification performance of LLaVA, which turns out to be the best model for this task, followed by LLaNA.

### Training baselines on ShapeNeRF-Text

Tabs. 6 to 9 report results on language tasks of several baselines trained on ShapeNeRF-Text, while Tab. 13 of the appendix, shows zero-shot NeRF classification performance of such models. We employed those baselines on ShapeNeRF-Text, for which we were able to run the official training code. Accordingly, we followed their protocol, which, for all of them, keeps the modality-specific encoder frozen and trains an adaptor and the LLM in two steps. We notice that the trained baselines exhibit different behaviors to their frozen counterparts, with LLaVA performing significantly worse and PointLLM showing clear improvements. As for GPT4Point, we observe greater variability across metrics; however, overall, it shows no significant benefit from training on ShapeNeRF-Text. LLaNA yields the best performance compared to all baselines, either frozen or trained on ShapeNeRF-Text. Finally, Appendix F shows the generalization performance on Obojavverse of LLaNA and the trained baselines.

## 6 Limitations and future directions

Despite the promising results of our framework, it is the first study in this direction and several limitations are yet to be addressed. First, the pre-trained nf2vec encoder, having been trained exclusively on synthetic data from ShapeNet, may not generalize well to real-world objects. To address this, future work should create a NeRF-Text dataset including a more diverse set of objects, like the ones provided by Obojavverse  and OmniObject3D . Another limitation is that nf2vec currently processes only MLPs, restricting our model to MLP-only NeRFs. However, with the rapid advancements in meta-networks, it may become very soon possible to extend LLaNA to more complex NeRF architectures, such as InstantNGP . For instance, the approach by Lim et al.  suggests the feasibility of processing various input architectures, although it is currently limited to small networks. Finally, our framework has been tested solely on object-centric NeRFs. Expanding its application to NeRFs representing entire scenes would be a compelling direction for future research.

## 7 Concluding remarks

This paper addressed the novel task of creating a language assistant for NeRF. We have tackled this problem by leveraging recent advances in MLLMs and meta-networks processing neural fields. We have shown that it is feasible and effective to directly process the weights of a NeRF to project it into the input embedding space of an LLM. We have built and made publicly available a dataset of textual annotations of NeRFs and have shown that our approach compares favourably with respect to several MLLMs used as baselines for the novel tasks of brief and detailed captioning, question answering, and zero-shot classification of NeRFs.