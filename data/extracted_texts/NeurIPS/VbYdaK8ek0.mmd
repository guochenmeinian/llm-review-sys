# Efficient Adaptation of Large Vision Transformer via Adapter Re-Composing

Wei Dong\({}^{1,4}\) Dawei Yan\({}^{1}\) Zhijun Lin\({}^{2}\) Peng Wang\({}^{3}\)

\({}^{1}\)College of Information and Control Engineering,

Xi'an University of Architecture and Technology.

\({}^{2}\)School of Computer Science, Northwestern Polytechnical University.

\({}^{3}\)School of Computer Science and Engineering,

University of Electronic Science and Technology of China.

\({}^{4}\)Xi'an Hypersonic Measurement Technology Co., Ltd.

Corresponding author. Email address: p.wang6@hotmail.com

###### Abstract

The advent of high-capacity pre-trained models has revolutionized problem-solving in computer vision, shifting the focus from training task-specific models to adapting pre-trained models. Consequently, effectively adapting large pre-trained models to downstream tasks in an efficient manner has become a prominent research area. Existing solutions primarily concentrate on designing lightweight adapters and their interaction with pre-trained models, with the goal of minimizing the number of parameters requiring updates. In this study, we propose a novel Adapter Re-Composing (ARC) strategy that addresses efficient pre-trained model adaptation from a fresh perspective. Our approach considers the reusability of adaptation parameters and introduces a parameter-sharing scheme. Specifically, we leverage symmetric down-/up-projections to construct bottleneck operations, which are shared across layers. By learning low-dimensional re-scaling coefficients, we can effectively re-compose layer-adaptive adapters. This parameter-sharing strategy in adapter design allows us to further reduce the number of new parameters while maintaining satisfactory performance, thereby offering a promising approach to compress the adaptation cost. We conduct experiments on 24 downstream image classification tasks using various Vision Transformer variants to evaluate our method. The results demonstrate that our approach achieves compelling transfer learning performance with a reduced parameter count. Our code is available at https://github.com/DavidYanAnDe/ARC.

## 1 Introduction

The utilization of large-scale pre-trained models for various downstream tasks has garnered significant interest in the computer vision community [1; 2; 3; 4]. These models continually push the boundaries of downstream task performance while eliminating the need for task-specific model design and training. In early attempts, a commonly adopted transfer learning strategy involved directly fine-tuning a pre-trained model on downstream tasks. However, the full fine-tuning strategy suffers from two major drawbacks: (1) Updating large-scale parameters is prohibitively expensive and typically requires a substantial amount of training data to prevent overfitting. (2) As the sizes of state-of-the-art pre-trained models continue to increase, it becomes impractical and unsustainable to store a distinct set of model weights for each downstream task.

In contrast to fine-tuning the entire pre-trained network, recent research has focused on parameter-efficient model adaptation. The core idea behind this line of work is to keep the majority of pre-trained parameters frozen and only update or introduce a small fraction of task-specific parameters. Several methods fall under this umbrella, including prompt tuning [5; 6], visual adapter [7; 8], and linear feature modulation . These methods have demonstrated competitive or even superior performance compared to full fine-tuning while significantly reducing the adaptation cost. They differ in the design of lightweight adapters and how these adapters interact with the pre-trained parameters. Prompt tuning methods [5; 6] adapt the features of pre-trained Vision Transformers by introducing trainable task-specific tokens into one or multiple attention layers. Visual adapter [7; 8] injects a non-linear lightweight adapter with a bottleneck architecture between layers of the pre-trained model to adjust the feature distribution. Such non-linear adapters are simplified using linear transformations such as shifting and scaling  to directly modulate the pre-trained features.

In this work, we not only focus on designing lightweight adapters but also emphasize the importance of adaptation parameter reusability in further compressing the adaptation cost. We adopt a low-rank design for the adapter using a bottleneck operation but propose a novel approach. Unlike other methods that place the adapter in different layers and directly learn different parameters for each adapter to cater to layer-wise variation, we propose sharing the down/up projections in the low-rank adapter across different layers and simply learning low-dimensional re-scaling coefficients to re-compose the linear projections into layer-adaptive adapters. The idea of Adapter Re-Composing (ARC) is motivated by the observation that naturally derived adaptation matrices can exhibit extremely low-rank characteristics, even when not explicitly designed as such. This implies the possibility of using a shared "basis" to re-compose the adapters. Furthermore, we design the low-rank adapters using symmetric down-projection and up-projection matrices, which further reduces the parameter size. Due to their linear nature and careful positioning design, our adapters can be seamlessly integrated into the pre-trained network, as in [7; 9; 10], without adding extra computation during the inference phase.

We evaluate our method on various large Vision Transformer models, including ViT-B  and its variants such as ViT-L , ViT-H , and Swin-B , using 24 downstream image classification benchmark datasets. The experimental results demonstrate that our method achieves compelling transfer learning performance while maintaining a smaller parameter size.

The key contributions of this paper are summarized as follows:

* We approach efficient pre-trained model adaptation from a novel perspective by exploring the reusability of adaptation parameters, which goes beyond existing works that primarily focus on the lightweight design of adapter structures.
* We introduce the Adapter Re-Composing (ARC) strategy, which shares bottleneck operation's down-/up-projections across layers and utilizes lower-dimensional re-composing coefficients to create layer-adaptive adapters. This approach enables fewer parameters than prior works.
* Our parameter sharing scheme in the ARC method prevents a linear increase in parameter size with the number of layers, ensuring better scalability, particularly for larger-scale models.
* Through extensive experiments on various Vision Transformer variations and numerous downstream tasks, we show that our method achieves highly competitive transfer learning performance while maintaining a relatively low level of additional parameter.

## 2 Related work

In this section, we present a concise review of the existing literature, focusing on two key areas: (1) Pre-training and fine-tuning, and (2) Parameter-efficient transfer learning.

Pre-training and fine-tuning.Pre-training and fine-tuning, also known as transfer learning [12; 13; 14], is a popular approach that utilizes large-scale datasets [15; 16; 17; 18; 19] to train models for adaptation to different downstream tasks. By extracting knowledge from these datasets and encoding it into parameters, models can be fine-tuned for specific tasks, resulting in improved performance compared to models without pre-training. The availability of large-scale datasets  has led to significant advancements in performance and convergence speed for downstream tasks. Scaling up models [20; 21] to handle the growing data volume enhances data utilization and improves the efficiency and robustness of pre-trained models across various data distributions and noise levels. Large models [1; 11; 22] also benefit from self-supervised pre-training [2; 23] using unlabeled data, reducing the cost, duration, and quality issues associated with human annotation. By leveraging this approach, models can effectively extract knowledge from readily available unlabeled data, further enhancing their generalization performance in downstream tasks.

Parameter-efficient transfer learning.The exponential increase in model parameters presents a computational challenge when fine-tuning the entire network on downstream tasks. In the field of NLP, researchers have explored parameter-efficient transfer learning approaches [7; 24; 25; 26; 27; 28] that train a subset of the model or add new modules with fewer parameters while achieving comparable or even superior performance. Inspired by the success of NLP, several notable works [8; 29; 30] have emerged in the computer vision domain. One approach, known as Prompt Tuning [5; 6], addresses the distribution mismatch between pre-training and downstream tasks by learning task-specific tokens. Adapter-like methods [8; 7; 29] insert trainable modules, such as MLPs with activation functions and residual structures, into the network to facilitate transfer learning. LoRA  exploits the low-rank update to a large-scale frozen model and introduces a bypass to the original parameter matrix to mimic the fine-tuning of the entire model parameters. SSF  introduces lightweight scaling and shift operations to modulate the pre-trained representations. The core concepts of these aforementioned works are visually summarized in Fig. 1.

As an advancement in visual adaptation, ARC addresses the limitations of Prompt Tuning, which requires different prompt designs for different downstream tasks. Moreover, ARC introduces the innovative concept of bottleneck matrix reuse, achieving state-of-the-art performance with minimal adaptation cost comparing to other rank-decomposition strategies. Additionally, ARC employs a linear design for the adapters and inherits the benefits of re-parameterization [9; 10; 24], ensuring that inference does not introduce any additional computational complexity.

## 3 Approach

In this section, we start by providing an introduction to the notations, symbols, and background related to Vision Transformers, which is followed by the presentation of our proposed Adapter Re-Composing (ARC) method. ARC focuses on efficient transfer learning for Vision Transformers by reusing rank-decomposition projections to adaptively compose layer-wise adapters, thereby reducing the size of learnable parameters. Additionally, we discuss the insights gained from our architecture design.

Figure 1: Visual summary of typical parameter-efficient pre-trained model adaptation methods.

### Preliminary

A plain Vision Transformer model contains a patch embedding layer and multiple encoder layers. Given an input image \(^{H W C}\), the patch embedding layer first splits the image into a sequence of flattened patches \(_{}^{N(P^{2} C)}\), where \((H,W)\) is the resolution of the input image, \((P,P)\) is the resolution of each patch, \(C\) denotes the number of input channels, and \(N=H W/P^{2}\) is the number of tokens. Subsequently, the image patches are mapped to a \(D\)-dimensional embedding space through a linear projection \(^{(P^{2} C) D}\). A learnable [class] token vector \(}_{}^{D}\) is then prepended to the sequence of \(_{}\), and the position embeddings \(_{}^{(N+1) D}\) are added to the sequence. The output of the patch embedding layer can be expressed as follows:

\[_{}=[}_{}^{}; _{}]+_{},\] (1)

where \([;]\) denotes concatenation operation. This output is then fed into several consecutive encoder layers, each consisting of a Multi-Head Attention (MHA) block and a Feed-Forward Network (FFN) block. LayerNorm (LN) is applied before each block, and residual connections are applied thereafter. The process of \(l\)-th encoder layer is defined as:

\[^{(l)}=(( ^{(l-1)}))+^{(l-1)},\\ ^{(l)}=((^{(l)})) +^{(l)},\] (2)

where \(^{(l-1)}\) denotes input tokens in \(l\)-th layer, \(^{(l)}\) indicates intermediate representations produced by MHA, and the output of \(l\)-th layer is \(^{(l)}\).

In MHA block, each Attention Head (AH) module utilizes the weight matrices \(_{q}^{(l)}^{D^{(l-1)} D_{h}^{(l)}}\), \(_{k}^{(l)}^{D^{(l-1)} D_{h}^{(l)}}\), and \(_{v}^{(l)}^{D^{(l-1)} D_{h}^{(l)}}\) for the _query_, _key_, and _value_ operations, respectively. These operations enable an exclusive attention mechanism on the normalized feature representations \(_{}^{(l-1)}=(^{(l-1)})\):

\[_{h}^{(l)}=_{h}(_{}^{(l-1)}) =(_{}^{(l-1)}_{q}^{( l)})(_{}^{(l-1)}_{k}^{(l)})^{}}{ ^{(l)}}})_{}^{(l-1)}_{v}^{(l)},\] (3)

where \(D_{h}^{(l)}=}{M}\) is the feature dimensionality of the output representations \(_{h}^{(l)}\) for each attention head and \(M\) represents the number of attention heads. The MHA block concatenates multiple \(\{_{h}^{(l)}\}\) in sequence and generates the outputs through a linear projection \(_{o}^{(l)}^{(M D_{h}^{(l)}) D^{(l)}}\):

\[^{(l)}=(_{}^{(l-1)})=[ _{1}(_{}^{(l-1)}),\ ,\ _{M}(_{}^{(l-1)})]_{o}^{(l)}.\] (4)

The FFN block consists of two linear projections with the GELU activation function in between:

\[^{(l)}=(_{}^{(l)})= (_{}^{(l)}_{1}^{(l)}) _{2}^{(l)},\] (5)

where \(_{1}^{(l)}^{D^{(l)} 4 D^{(l)}}\) and \(_{2}^{(l)}^{4 D^{(l)} D^{(l)}}\) denote two linear projection matrices, and \(_{}^{(l)}=(^{(l)})\).

### Adapter Re-Composing method

We observe that the majority of existing adaptation methods introduce adapters into various layers and learn separate parameters for adapting the pre-trained model to specific downstream tasks. Previous studies [8; 24] have shown the effectiveness of leveraging the low-rank properties of adapters to fine-tune frozen pre-trained models. Inspired by these findings, we propose a novel approach to create a unified linear space across different adapters to enhance parameter efficiency and adaptation performance.

Architecture.The ARC architecture incorporates a bottleneck operation for adapters, which consists of three key components: a linear down-projection, layer-specific re-scaling coefficients, and a linear up-projection. This architecture is illustrated in Fig. 2. To facilitate the reusability of adaptation matrices, we have developed a sharing and re-composing scheme for the ARC method.

This scheme involves sharing linear projections across layers and learning low-dimensional re-scaling coefficients to re-compose the layer-adaptive adapters. In addition, to enhance parameter compression, we leverage symmetric down-projection and up-projection in a single bottleneck operation:

\[_{}=(_{})^{},\] (6)

where \(_{}^{D D^{}}\) and \(_{}^{D^{} D}\) with \(D^{} D\) denote shared down-projection and up-projection matrices across different layers; \(D^{}\) represents the hidden dimensionality of the projections. To accommodate the variations across different layers, we learn re-scaling coefficients to re-compose the layer-adaptive adaptation matrices. These coefficients are then diagonalized into a diagonal matrix \(^{(l)}^{D^{} D^{}}\) specific to each layer \(l\). This diagonal matrix allows for efficient and effective adjustment of the adaptation parameters at each layer. Formally, given an input \(_{}^{(N+1) D}\), the output of our ARC module is:

\[_{}=(_{})=_{ }_{}^{(l)}_{ }+_{}.\] (7)

Unless otherwise specified, we adhere to the default configuration of inserting our ARC modules sequentially before both the MHA and FFN blocks in the Vision Transformer. The influence of adapter positions will be discussed in Section 4.3. Therefore, the Vision Transformer incorporating our ARC modules can be formulated as follows:

\[^{(l)}=(_{}( (^{(l-1)})))+^{(l-1)},\] (8) \[^{(l)}=(_{}((^{(l)})))+^{(l)}.\]

Note that \(_{}\) and \(_{}\) are two independent ARC modules, meaning that the projection matrices of the two modules are not shared between them. During the fine-tuning phase, we exclusively update the learnable parameters of our newly added ARC modules. This involves freezing all original parameters of the pre-trained model and solely focusing on updating the parameters of our ARC.

Inference.Our ARC employs a completely linear transformation so that we can re-parameterize it by fusing the module to the original framework of the pre-trained model. Take the ARC module of FNN as an example, the process can be defined by:

\[^{(l)}=(_{}(^{(l) })_{1}^{(l)})_{2}^{(l)},\] (9)

where \(_{}(^{(l)})\) can be represented by \(^{(l)}(_{}^{(l)}_ {}+)\) according to Eq. (7) with \(^{D D}\) being an identity matrix. Furthermore, we can construct \(_{1}^{(l)}\) by:

\[_{1}^{(l)}=(_{}^{(l)} _{}+)_{1}^{(l)}.\] (10)

Figure 2: Illustration of the proposed Adapter Re-Composing Method.

Therefore, we can replace the matrix \(_{1}^{(l)}\) by \(_{1}^{(l)}\) in the inference stage, thereby avoiding extra computational overheads.

### Insights of architecture design

In this work, we propose an approach that involves adopting a low-rank design for the adapters and sharing the bottleneck projections across layers. Our motivation for adopting this approach stems from the assumption that the layer-wise adapters can be effectively re-composed by re-scaling a small number of base linear projections. To validate this assumption, we conduct an analysis of the singular value distribution of adaptation matrices \(_{}^{D D}\) learned without the bottleneck operation, which are anticipated to be full-rank. In Fig. 3, we observe that the singular values exhibit extreme sparsity and follow a power-law distribution, with the majority of singular values being close to zero. This indicates that the learned adaptation matrices naturally exhibit low-rank characteristics. Furthermore, the pronounced sparsity of the singular values suggests that the adaptation matrices can be effectively reconstructed using a limited number of basis vectors. These findings provide strong evidence supporting the rationale behind our adaptation parameter-sharing strategy.

## 4 Experiments

In this section, we present the experimental settings, comparison to existing solutions, and ablation studies to unveil the key properties of the proposed method.

### Experimental settings

Datasets.We evaluate the effectiveness of our ARC approach on two sets of visual task adaptation benchmarks, comprising a total of 24 datasets. The list of datasets used for evaluation is provided below:

_FGVC._ We conduct experiments with the default settings in the VPT  on a collection of five Fine-Grained Visual Classification (FGVC) datasets, known as FGVC. The FGVC dataset collection includes CUB-200-2011 , NABirds , Oxford Flowers , Stanford Dogs , and Stanford Cars .

_VTAB-1k._ We also evaluate our ARC method on the VTAB-1k benchmark , which consists of 19 diverse visual classification tasks. These tasks are divided into three groups: the _Natural_ group, which contains images captured through standard cameras; the _Specialized_ group, which includes images captured by specialist equipment such as remote sensing and medical imaging; and the _Structured_ group, which comprises synthesized images from simulated environments, such as object counting and 3D depth prediction. Each downstream task in the VTAB-1k benchmark consists of 1000 training examples. Following VPT , we set aside 200 samples from the training set as the validation set to

Figure 3: Singular value distribution of adaptation matrices without the bottleneck structure. Two adaptation matrices of both MHA and FFN blocks are fine-tuned on the _DTD_ downstream task. The X-axis represents the singular values, while the Y-axis represents the count of singular values within specific ranges. Complete visualization is available in the appendix.

select hyperparameters. Subsequently, we train the model on the full training data using the selected hyperparameters.

Pre-trained backbone.To evaluate the adaptation capacity of the proposed ARC method, we apply the ARC strategy to two typical types of Vision Transformers: ViT  and Swin Transformers . For ViT, we conduct experiments using three different backbone variants with varying model sizes: ViT-**Base**/**Large**/**Huge**. All the backbones are pre-trained on the ImageNet-21K  dataset.

Baselines and existing methods.In our comparative analysis, we evaluate the performance of the ARC method against two baselines and several state-of-the-art efficient pre-trained model adaptation methods. The two baselines we consider are: (1) Full Fine-tuning: This baseline involves updating all the parameters of the pre-trained model using the training data of the downstream task. (2) Linear Probing: This baseline focuses on learning a linear classification head on the downstream task while keeping the remaining parameters of the pre-trained model frozen. In addition to the baselines, we compare our ARC method with the following state-of-the-art solutions: (1) Adapter : This method inserts lightweight adaptation operations, consisting of a down-projection, non-linear activation, and an up-projection, into the pre-trained model. (2) Bias : The Bias method fine-tunes only the bias terms of the pre-trained models while keeping the remaining parameters frozen. (3) LoRA : This approach introduces trainable low-rank adaptation matrices into each layer of the Transformer architecture. (4) VPT : The VPT method incorporates extra learnable tokens into the input or all attention layers of the frozen Transformer. (5) SSF : This method adds linear transformation parameters, including scaling and shifting, to modulate the pre-trained features. By comparing the performance of our ARC method with these baselines and state-of-the-art solutions, we aim to demonstrate its superiority in terms of efficiency and effectiveness in pre-trained model adaptation.

Implementation details.To ensure a fair evaluation of the effectiveness of our proposed ARC method, we have opted for a simple training setup without too many additional bells and whistles. Similar to VPT , we have used standard data augmentations during the training phase, which include image normalization using ImageNet means and standard deviation, random resize crop to \(224 224\) with random horizontal flip for FGVC datasets, and resize to \(224 224\) for VTAB-1k. We have used grid search to select hyper-parameters such as the learning rate, weight decay, and batch size, using the validation set of each task, as in VPT . All experiments were conducted using the PyTorch  framework on an NVIDIA A40 GPU with 48GB of GPU memory. Further details can be found in the appendix.

  
**Method Dataset** & **CUB-200-2011** & **NAbirds** & **Oxford Flowers** & **S**a** & **S**a** & **Coarse** & **M** & **Params.(M)** \\   Full fine-tuning & 87.3 & 82.7 & 98.8 & 89.4 & 84.3 & 88.5 & 85.98 \\ Linear probing & 85.3 & 75.9 & 97.9 & 86.2 & 51.3 & 79.3 & 0.18 \\  Adapter  & 87.1 & 84.3 & 98.5 & 89.8 & 68.6 & 85.7 & 0.41 \\ Bas  & 88.4 & 84.2 & 98.8 & 91.2 & 79.4 & 88.4 & 0.28 \\ VPT-Shallow  & 86.7 & 78.8 & 98.4 & 90.7 & 68.7 & 84.6 & 0.25 \\ VPT-Deep  & **88.5** & 84.2 & 99.0 & 90.2 & 83.6 & 89.1 & 0.58 \\ LaRo  & 88.3 & 85.6 & 99.2 & 91.0 & 83.2 & 89.5 & 0.44 \\ SSF  & 82.7 & 85.9 & 95.5 & 87.7 & 82.6 & 87.5 & 0.39 \\ SSF  & 89.5 & 88.7 & 99.6 & 89.6 & 89.2 & 90.7 & 0.39 \\  ARC\({}_{s}\) & 83.4 & 85.0 & **99.4** & 90.1 & 82.7 & 89.1 & 0.22 \\ ARC & **88.5** & 85.3 & 99.3 & **91.9** & **85.7** & **90.1** & 0.25 \\ ARC* & 89.3 & 85.7 & 99.7 & 89.1 & 89.5 & 90.7 & 0.25 \\   

Table 1: Comparison of ARC with baselines and state-of-the-art efficient adaptation methods on five FGVC datasets. All methods utilize ViT-B/16 pre-trained on ImageNet-21k as the backbone. “SSF*” denotes the performance reported in the original SSF paper , which incorporates advanced data augmentations like cutmix , mixup , and regularization techniques such as label smoothing . To ensure a fair comparison, we reproduced the SSF method using the code provided by , while employing the same basic data augmentations as our approach, and we denote SSF’s reported performance as “SSF*” and ARC’s performance augmented with SSF’s data augmentation as “ARC*”. The **bold** font shows the best accuracy of all methods and the underline font shows the second best accuracy.

### Experimental comparisons

In this section, we provide a comprehensive comparison of our ARC method with baseline methods and other state-of-the-art solutions. We evaluate the performance in terms of classification accuracy on downstream tasks as well as the parameter size. The results are summarized in Table 1 and Table 2, with all results obtained using the ViT-B/16 backbone. Based on these comparative results, we make the following observations:

(1) The ARC method demonstrates highly competitive classification accuracy on both sets of visual adaptation datasets, while maintaining a low parameter size. As shown in Table 1 and Table 2, under a fair comparison, ARC achieves the best mean accuracy and outperforms the other methods on the majority of the 24 datasets. This confirms the effectiveness of our proposed pre-trained model adaptation strategy. Furthermore, thanks to the parameter-sharing strategy in ARC, we are able to noticeably reduce the parameter size compared to other rank-decomposition based adapters such as Adapter  and LoRA . VPT-Shallow  also exhibits parameter efficiency as it only introduces learnable tokens in the input layer. However, this is achieved at the cost of a significant performance sacrifice, resulting in considerably inferior performance compared to VPT-deep  and ARC. Another parameter-efficient method, Bias , focuses on updating only the bias terms in the pre-trained network, but it also leads to a significant compromise in classification performance on downstream tasks. To further decrease the parameter size, we evaluate ARC\({}_{}\), which omits the adapters applied to Feed-Forward Network (FFN) blocks and focuses solely on adapting the Multi-Head Attention (MHA) blocks. This approach achieves nearly a halving of the parameter size while experiencing only a minimal \(1\%\) drop in performance.

(2) In comparison to the efficient adaptation solutions presented in the tables, full fine-tuning yields comparable or even worse classification accuracy across various downstream tasks, despite updating a significantly larger number of parameters. This observation further emphasizes the importance and potential of lightweight adaptation designs. On the other end of the spectrum, linear probing requires minimal parameters but exhibits noticeable performance degradation.

Experiments on larger-scale ViT backbones.In addition to evaluating the ARC method on ViT-B/16 backbone, we also conducted experiments on larger-scale ViT backbones to assess its performance on more computationally demanding models. Specifically, we tested the ARC method on ViT-Large and ViT-Huge backbones. The results, presented in Table 2(a) and Table 2(b), demonstrate that the ARC method maintains its competitive classification accuracy even with larger-scale backbones. It consistently outperforms the baseline methods and achieves comparable or superior performance compared to other state-of-the-art adaptation methods. Furthermore, the parameter size of the ARC method remains noticeably smaller than rank-decomposition based adapters like Adapter  and LoRA , as well as VPT-deep , showcasing its efficiency in terms of parameter utilization. These findings suggest that the ARC method is not only effective on smaller-scale ViT backbones

    &  &  &  &  &  \\   &  &  & 
    & **Natural (7)** & **Specialized (4)** & **Structed (8)** & **Mean Total** & **Params.** \\  Full fine-tuning & 79.1 & 86.2 & 59.7 & 72.4 & 86.8 \\ Linear probing & 73.5 & 80.8 & 33.5 & 58.2 & 0.05 \\  MLP-4  & 70.6 & 80.7 & 31.2 & 57.7 & 4.04 \\ Partial  & 73.1 & 81.7 & 35.0 & 58.9 & 12.65 \\ Bias  & 74.2 & 80.1 & 42.4 & 62.1 & 0.25 \\ VPT-Shallow  & **79.9** & 82.5 & 37.8 & 62.9 & 0.05 \\ VPT-Deep  & 76.8 & 84.5 & 53.4 & 67.7 & 0.22 \\  ARC & 79.0 & **86.6** & **59.9** & **72.6** & 0.27 \\   

Table 4: Performance comparison on VTAB-1k using Swin-Base pre-trained on ImageNet-21k as backbone. “(-)” denotes the number of tasks in the subgroup. Expanded results are presented in the appendix.

    & **Natural (7)** & **Specialized (8)** & **Strong Total** & **Params.** \\  Full fine-tuning & 74.7 & 83.8 & 48.1 & 65.4 & 303.4 \\ Linear probing & 70.9 & 69.1 & 25.8 & 31.5 & 0.05 \\  Adaptive  & 65.6 & 75.5 & 29.0 & 52.9 & 2.38 \\ Bias  & 70.5 & 73.8 & 41.2 & 58.9 & 0.32 \\ VPT-Shallow  & 78.7 & 79.9 & 40.6 & 62.9 & 0.15 \\ VPT-Deep  & **82.5** & 83.9 & 54.1 & 70.8 & 0.49 \\ LLoRa  & 81.4 & 85.0 & **57.3** & 72.0 & 0.74 \\ ARC & 82.2 & **86.4** & **87.3** & **72.5** & 0.18 \\   

Table 3: Performance comparison on VTAB-1k using ViT-Large and ViT-Huge pre-trained on ImageNet-21k as backbone. “(-)” denotes the number of tasks in the subgroup. Expanded results are presented in the appendix.

position on pre-trained model adaptation performance using different positioning strategies. The results are presented in Table 4(b). Interestingly, placing the adapters after the MHA and/or FFN modules leads to performance degradation, despite this strategy being commonly adopted in previous works such as[7; 29]. Moreover, using only one type of adapter for either MHA or FFN results in inferior performance compared to using both types of adapters. This suggests that employing both types of adapters allows for more comprehensive adaptation of the pre-trained model to the target task without significantly increasing the parameter count.

Sharing v.s non-sharing adaptation parameters.In ARC, we adopt a parameter-sharing strategy to effectively reduce the number of adaptation parameters. This strategy encompasses two aspects: intra-layer sharing and inter-layer sharing. Through symmetric down-projection and up-projection matrices, we achieve intra-layer sharing, while inter-layer sharing involves sharing projection matrices across different layers. In this section, we investigate the impact of adaptation parameter sharing by conducting experiments with non-sharing or partial sharing designs. "intra + inter*" denotes sharing the bottleneck structure between MHA and FFN. The results presented in Table 4(c) demonstrate that using non-symmetric projection matrices or layer-independent adaptation parameters does not result in performance gains but leads to a noticeable increase in parameters. This validates the effectiveness of our parameter-sharing design.

Adapter insertion.We examine the performance of inserting the proposed adapters in a subset of layers using sequential or parallel insertion strategies. The results in Table 4(d) show that the performance of ARC improves as more layers are inserted. Furthermore, we observe that inserting adapters in the first six layers yields better results compared to inserting them in the last six layers. Additionally, we explore a parallel insertion setting inspired by , but the impact is not significantly pronounced. Another notable aspect is that our parameter sharing scheme in the ARC method prevents a linear increase in parameter size with the number of layers, ensuring better scalability, particularly for larger-scale models.

## 5 Limitation

The adaptation parameter sharing scheme in the ARC method is built on the assumption that layers have the same dimensionality. This assumption is crucial as it enables the sharing of down-/up-projection matrices involved in the bottleneck operation across layers, leading to parameter efficiency. However, it is worth exploring strategies to extend this scheme and accommodate dimensionality variation. This research direction holds promise for addressing scenarios where dimensionality varies and further enhancing the flexibility and applicability of the ARC method.

## 6 Conclusions

Our paper introduced the Adapter Re-Composing (ARC) method, which leverages the reusability of adaptation parameters to efficiently adapt pre-trained models. By sharing down-/up-projections in low-rank adapters across layers and learning layer-specific re-scaling coefficients to re-composing layer-adaptive adapters, ARC balances transfer learning performance and adaptation overheads. Extensive experiments demonstrate the compelling performance of our approach with a reduced parameter size. ARC offers a promising solution for efficient pre-trained model adaptation, showcasing the potential of reusing adaptation parameters for competitive results.

Table 5: Ablation experiments on VTAB-1k benchmark using ViT-B/16 backbone. The table shows average accuracy (“Acc.”) and parameter count (“Params.”) for all downstream datasets.