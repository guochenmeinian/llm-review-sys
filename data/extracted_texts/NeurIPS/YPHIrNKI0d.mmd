# Spatio-Angular Convolutions for Super-resolution in Diffusion MRI

Matthew Lyon

University of Manchester

matthew.s.lyon@.manchester.ac.uk

&Paul Armitage

University of Sheffield

p.armitage@sheffield.ac.uk

&Mauricio A Alvarez

University of Manchester

mauricio.alvarezlopez@manchester.ac.uk

###### Abstract

Diffusion MRI (dMRI) is a widely used imaging modality, but requires long scanning times to acquire high resolution datasets. By leveraging the unique geometry present within this domain, we present a novel approach to dMRI angular super-resolution that extends upon the parametric continuous convolution (PCConv) framework. We introduce several additions to the operation including a Fourier feature mapping, global coordinates, and domain specific context. Using this framework, we build a fully parametric continuous convolution network (PCCNN) and compare against existing models. We demonstrate the PCCNN performs competitively while using significantly fewer parameters. Moreover, we show that this formulation generalises well to clinically relevant downstream analyses such as fixed-based analysis, and neurite orientation dispersion and density imaging.

## 1 Introduction

Diffusion magnetic resonance imaging (dMRI) is a clinical imaging modality that is routinely used to diagnose white matter pathology within the human brain. Advancements in downstream analyses using dMRI data continuously expand the diagnostic capabilities of the modality, but often require high resolution data that is clinically infeasible to acquire . However, recent research demonstrates that deep learning can be used to predict high resolution images using low resolution dMRI data , a technique known as super-resolution. These advancements therefore pave the way for clinically accessible advanced dMRI analyses.

dMRI data can be represented using a non-Euclidean geometry. Specifically, a dMRI dataset consists of a series of three-dimensional volumes where each volume measures the water diffusivity in a given direction. Here, the direction is quantified by a 3D vector known as the 'b-vector', and is sparsely sampled from a unit sphere. Each three-dimensional volume, however, is regularly sampled in a discrete grid. Therefore, dMRI data has two distinct resolution types: _spatial_ resolution which denotes the regularly sampled grid size, and _angular_ resolution which denotes the number of diffusion directions. Despite this relatively unique geometry, most dMRI deep learning methods use operations grounded in a Euclidean framework, such as convolutional neural networks (CNNs) .

This presents an opportunity as typical CNN architectures do not fully utilise the geometric properties present in dMRI data. For example, implicit within the formulation of the CNN is the assumption that data are densely and regularly sampled in a discrete manner. This is only true when considering the three spatial dimensions of dMRI data, whilst the other dimensions would be more suited using approaches like graph convolutional networks (GCNs) , spherical CNNs , and point cloud CNNs . Examples of approaches that develop geometrically motivated convolutions in the dMRIdomain include those for tissue segmentation , lesion segmentation , and fibre orientation distribution (FOD) reconstruction [5; 32].

Another promising candidate for a framework capable of utilising the unique dMRI geometry is the parametric continuous convolution (PCConv) introduced by Wang et al. , which extended the discrete convolution into the continuous domain. This was achieved via an inner parameterised hypernetwork  responsible for generating convolutional weights. Here, the convolutional weights were not learnt directly but were instead sampled from the hypernetwork given kernel coordinates as input. This operation could therefore be used to model arbitrary geometries.

In this work, we build upon the PCConv operation to convolve across both spatial and angular dimensions of dMRI data via a new, flexible, coordinate embedding. We subsequently create a deep parametric continuous CNN (PCCNN) that is well-suited for inference within this domain. Furthermore, we develop additional modifications to the PCCNN by including supplemental prior information in the coordinate embedding. These modifications include the application of Fourier feature mappings, the inclusion of prior domain information, and the incorporation of global coordinate components.

We demonstrate the effectiveness and flexibility of this approach through the task of angular super-resolution of dMRI data in various sampling schemes, including single-shell and multi-shell data . To assess the performance of our method, we compare results across clinically relevant downstream analysis tasks such as fixed-based analysis (FBA) , and neurite orientation dispersion and density imaging (NODDI) . We show that the PCCNN performs competitively against a similar recurrent CNN (RCNN) network trained on the same data whilst using approximately a tenth of the number of parameters. Additionally, we demonstrate the generalisability of the network by comparing it against models trained specifically for downstream analysis tasks.

## 2 Related Works

Whilst there is a body of literature within _spatial_ super-resolution methods in the natural image domain , and more specifically the dMRI domain [1; 36; 6; 25], this work focuses on _angular_ super-resolution as a means of increasing image quality. Here, the task of inferring high angular resolution data from low angular resolution dMRI acquisitions can broadly be split into two categories: methods that super-resolve raw dMRI data, and those that super-resolve downstream analyses. The former allows for more flexible use of the data and analysis method, but the unconstrained nature of the task makes inference more challenging. The latter constrains the problem by leveraging assumptions made within the analysis method, however the mapping from raw dMRI data to downstream analysis is in general not invertible, therefore the super-resolved data cannot be used to derive other downstream analysis metrics.

Notable works in angular super-resolution of raw dMRI data include Yin et al. , whereby a preliminary study constructed a 1D CNN autoencoder that only convolved across the angular dimension, disregarding possible spatial correlations within the data. Lyon et al.  developed a 3D RCNN that benefited from parameter sharing in the spatial dimensions, but required angular dimension shuffling to overcome sequence order bias. Both models concatenated the b-vectors as channels within the input, therefore treating the b-vectors as additional features, rather than coordinates of a manifold for which the data lies on. Finally, Ren et al.  proposed a conditional generative adversarial network (GAN) that additionally used structural T1w and T2w data, as well as b-vector information to predict unseen dMRI intensity data.

As there exists a myriad of downstream analyses for dMRI, this work focuses on two commonly used methods: FBA  and NODDI . FBA relies on segmenting FODs  into 'fixels' or fibres within a voxel. Relevant works in FOD super-resolution include Lucena et al.  and Zeng et al. . Both of these studies regressed low angular resolution derived FODs with their high resolution counterparts through 3D CNNs. NODDI estimates micro-structural complexity of the underlying brain tissue through modelling axon and dendrite populations within a voxel. Approaches to infer high angular resolution derived NODDI parameters from lower resolution raw dMRI data include Golkov et al. , Ye et al. , and Ye et al. . These studies use various sub-sampling schemes and network architectures, including a 3D CNN within Ye et al. .

Methods

This section introduces parametric continuous convolutions, and extends the framework into the dMRI domain. We highlight our key contributions to the PCConv operation, then finally provide implementation details used to perform training and inference within the task of dMRI super-resolution.

### Parametric Continuous Convolutions

A discrete convolution within the context of deep learning is defined as follows

\[h[n]=(f*g)[n]=_{m=-M}^{M}f[m]g[n-m],\]

where the data function \(f:\) and kernel \(g:\) are defined over the support domain for the finite integer set \(=^{D}\) and \(=\{-M,-M+1,,M-1,M\}^{D}\) respectively. Conversely, a continuous convolution is defined as

\[h(_{j})=(f*g)(_{j})=_{-}^{}f()g (_{j}-)d,\]

where both \(f\) and \(g\) are continuous functions over the support domain \(==^{D}\). Typically in deep learning, this integration is analytically intractable but can be approximated as in Wang et al.  using the following form:

\[h(_{j})=_{-}^{}f()g(_{j}- )d_{i=1}^{N}f(_{i})g( _{j}-_{i}),\] (1)

where a finite number of input points \(_{i}\) are sampled from the support domain for each output point \(_{j}\). In the parametric continuous convolution framework, we choose to use a multi-layer perceptron (MLP) as the kernel function \(g(;)=(;)\), where \(\) is a coordinate embedding that depends on \(_{i}\) and \(_{j}\), which is described in detail in Section 3.2. The kernel function \(g(;):^{D}\) spans the full continuous domain while being parameterised by a finite number of parameters \(\). In contrast to standard convolutional operations, the weights of the PCConv are not learnt directly, and are instead sampled from \(g(;)\). As the kernel function is itself a fully differentiable function, it can be trained through back-propagation.

In typical convolutional layers the set of output points form a subset of the input points set, provided there is no padding. However, within the parametric continuous framework, these two sets can be disjoint. This is key within the task of dMRI angular super-resolution, where the angular component of \(_{j}\), i.e. the target b-vectors, is not contained within the set of input b-vectors.

Each PCConv layer requires three inputs to produce the output features \(h_{k,j}(_{j})\): the input features \(f_{c,i}(_{i})\), the set of input points \(\), and the set of output points \(\). Within each layer, following on from the right-hand side of Equation (1), the PCConv operation is defined as

\[h_{k,j}(_{j})=_{c=1}^{C}_{i=1}^{N}f_{c,i}(_{i})g_{ c,k}(;).\] (2)

Here, \(C\) and \(K\) denote the input and output feature dimensions respectively. Within each layer the number of input points \(N\) is fixed, therefore the kernel function \(g\) will encapsulate the \(\) factor in Equation (1) during training.

### Convolutions in Q-Space

We define q-space \(\) as the joint spatial-angular domain within dMRI where \(=\{(u,v,w,,,)^{} u,v,w,,0<2,0\}\) and \(_{j},_{i}\). Here, \((u,v,w)^{}\) denote the spatial components, and \((,,)^{}\) denote the angular components. Specifically, \(\) and \(\) refer to the azimuthal and polar angles, respectively, which form the set of points on the surface of a sphere of radius \(\). In this context, \(\) is the diffusion weighting or b-value. Figure 1 visualisesconvolutions across q-space through Equation (2). To perform convolutions using q-space, we first calculate a coordinate embedding \(^{2LE}\) via

\[=[(p_{1}),(p_{2}),, (p_{E})]^{},\] (3)

where

\[(p_{m})=[ p_{m})}, p_{m})}, , p_{m})}, p_{m})}].\] (4)

Here \(p_{m}\) is the \(m\)th component of the coordinate vector \(^{E}\) given by \(=[u_{i}-u_{j},v_{i}-v_{j},w_{i}-w_{j},_{i}-_{j},d_{r}]\). We have opted to enforce rotational invariance across the sphere by omitting \(\) and \(\) directly, and instead use \(p_{5}=d_{r}(_{j},_{j},_{i},_{i})\) where \(d_{r}(_{j},_{j},_{i},_{i})=(_{j} )(_{i})+(_{j})(_{i})(_{j}- _{i})\). The function \(:^{2L}\) is a Fourier feature mapping motivated by Tancik et al.  and Mildenhall et al.  which is used to improve the hypernetwork's ability to learn high-frequency functions. In practice, \(L\) is a hyperparameter of the PCConv layer. Figure 2 provides examples of weights sampled from a trained PCConv layer with varying \(d_{r}\). The channels selected within the Figure demonstrate a variety of functions across the sphere, highlighting the network's ability to learn both high and low frequency kernels.

An important aspect of the discrete convolution is its finite support domain \(M\), thus allowing the network to share parameters across the image domain. Within the PCConv framework this is achieved by limiting the number of points to convolve across, given some constraint. When convolving over q-space, the spatial dimensions follow the same constraint as in discrete convolutions. That is, for each dimension given a kernel size \(n\), only the \(n\) closest points are selected. As the angular components represent sparsely sampled data across a sphere, a kernel size \(n\) selects the \(n\) closest points, as determined by \(d_{r}\). We refer to the angular kernel size as \(k_{q}\) from this point onward. There is an additional constraint with the angular components that only points within a fixed radius \(d_{}\), such that \(d_{r} d_{}\), are included. To achieve this, a binary mask is applied after the pointwise multiplication of the kernel weight with the input feature. The PCConv is a flexible operation that enables convolutions over non-Euclidean spaces. Additionally, the framework allows for kernel weights to be dependent upon coordinates that are _not_ defined in terms of the relative distance between an input and output point. In this work, we additionally introduce a variant of the PCCNN that uses one such encoding. Denoted as 'PCCNN-Bv', this model uses \(=[u_{i}-u_{j},v_{i}-v_{j},w_{i}-w_{j},_{i},_{j},d_{r}]\)

Figure 1: Visualisation of dMRI parametric continuous convolution. Points \(_{i}\) and \(_{j}\) are of the form \((u,v,w,,,)^{}\) with spatial components \((u,v,w)\) and angular components \((,,)\). Left: The raw dMRI data \(f()\). Points \(_{4}\) and \(_{6}\) are located in the same region on the q-space sphere and thus have the same angular components. Similarly, \(_{6}\) and \(_{20}\) have the same spatial coordinates but different angular components. Middle: the parametric continuous kernel, whose geometry is defined by the coordinate embedding \(\). Right: Sampled points from q-space are pointwise multiplied with the kernel and summed over to produce one output feature \(h(_{j})\).

The inclusion of this model is motivated by the non-linear relationship between different shells within multi-shell data, which may not be appropriately captured by the relative difference of b-values.

### Global Information

Discrete convolutional layers are computed over a limited kernel size, and therefore have a limited field of view. This affords the convolutional layer computational efficiency, as the kernel size is typically much smaller than the image. However, this comes at the cost of losing global information. Within dMRI, structures within the brain are highly spatially dependent and follow a similar pattern in each individual. Non-local coordinates, and therefore global context, can be incorporated within the PCConv layer through the coordinate embedding. For example, the relative position of the kernel within an image could be incorporated into the coordinate embedding. This idea is inspired by how Vision transformers (ViTs)  actively extract global information through correlations between image patches. The availability of global context allows ViTs to show competitive performance in image recognition tasks, where accurate synthesis of semantic meaning more heavily relies on non-local information .

In clinical practice, neuroimaging data are routinely aligned to a reference space, such as Talairach space . As such, coordinates within reference spaces are correlated across different subjects, and therefore can be used to provide context for the location of a voxel within the brain. In this work, after normalising the coordinates with respect to the subject brain mask size, we use the centroid of the training patches, as discussed in Section 3.6, as additional coordinates within \(\). Denoted as an '-Sp' suffix, both 'PCCNN-Sp' and 'PCCNN-Bv-Sp' models within Section 4 have this modification.

### Factorised Convolutions

dMRI is routinely stored in a dense four-dimensional format, with three spatial dimensions and one angular dimension whose coordinates are defined by \(\). Performing a convolution in this high-dimensional space can be computationally expensive. We mitigate this cost by using factorised convolutions  within the PCConv framework. For example, in the two-dimensional case, an \((n m)\) kernel would be factorised into two sequential convolutions of size \((n 1)\) and \((1 m)\). As in the non-factorised case, the PCConv layer's weights are sampled from one hypernetwork \(g(;)\). Following the approach used in Wang et al. , we further factorise the kernel weight tensor from \(W^{N O C K}\) into two sequential operations: an input convolution \(W_{}=^{N O C}\) followed by a forward projection matrix \(W_{}=^{C K}\), where \(O\) is the number of output points.

### Implementation

Since PCConv layers are capable of convolving over both spatial and angular dimensions, the PCCNN models are constructed exclusively of PCConv layers and non-linear activation functions. In practice, this involves a sequence of spatially pointwise \((1 1 1 k_{q})\) PCConv layers, followed by blocks of parallel convolutions consisting of spatially pointwise and \((3 3 3 k_{q})\) factorised convolutions. The hyperparameters for each PCCNN, including the number of PCConv layers, can be found in Figure 5 in the Appendix. Within this work, each PCConv layer has its own independent hypernetwork. This configuration allows for significant weight sharing capabilities, as the number of parameters for each hypernetwork does not depend on the kernel size or dimensionality. Each

Figure 2: Visualisation of kernel weights sampled from \(g_{c,k}(;)\) within a PCConv layer, where \(c\) and \(k\) index the input and output feature dimensions respectively. To generate the spheres, \(d_{r}\) is varied whilst all other components of \(\) are kept constant.

PCConv layer or residual PCConv block is followed by a rectified linear unit (ReLU), excluding the final layer. Each hypernetwork is composed of two dense layers, each followed by a leaky ReLU with a negative slope of \(0.1\), and a final dense layer with output size \(1\) and no subsequent activation. The code for this work is available at github.com/m-lyon/dmri-pcconv.

### Data and Training

Given the high dimensionality of dMRI data, using complete acquisitions as individual training examples is prohibitively expensive in terms of memory. To mitigate this issue, training sets were derived from input image patches \(_{}^{10 10 10 q_{ }}\), and target image patches \(_{}^{10 10 10 q_{ }}\) where \(q_{}\) and \(q_{}\) denote the number of input and target b-vectors respectively. During training, to sample the angular dimension of the input sets, an initial b-vector \(_{0}\) was randomly selected. The next \(n\), up to \(q_{}\), b-vectors \(\) were chosen using \(_{j}(_{i}(d_{r}(_{i},_{j}))), i,j \{0,1,...,n-1\}\). Similarly, this process was repeated to sample the output set up to size \(q_{}\).

To reduce inter-subject variability, each dMRI dataset was normalised such that 99% of the data lay between \([-1,1]\). Initially, a normalisation range of \(\) was used; however, this resulted in training instabilities within the hypernetwork. This was likely due to a greater mismatch between the data distribution and the weight initialisation scheme within the hypernetwork, which followed \(w(-}}},}}})\), where \(k_{}\) was the number of input features in a given layer. dMRI data from the WU-Minn Human Connectome Project (HCP)  were used for training, validation and testing. This data were initially processed with the standard HCP processing pipeline , as well as denoised using the 'patch2self' algorithm . Models were trained on twenty-seven subjects from the HCP dataset, while three subjects were used for validation during development. Hyperparameters for the PCCNN were selected through a random grid search with RayTune .

Each acquisition within the HCP dataset had shells of \(b_{}=\{1000,2000,3000\}\)\(}\), each with \(90\) directions. Training examples contained randomly selected shells \(b_{},b_{} U(b_{})\). This sampling scheme trained each network on single-shell and multi-shell examples simultaneously. The input angular dimension size was determined via \(q_{} U(q_{}),q_{}=\{6,7,...,19,20\}\). This approach allowed each network to learn the data distribution for a range of angular input sizes. To implement this, training examples had a fixed \(q_{}=20\), with zero-filled entries for input size less than 20. Models were trained using 4 NVIDIA A100's with a batch size of 16, and an \(_{1}\) loss function, for 200,000 iterations using AdamW . Each PCCNN model took approximately a week to train. Details of pre-processing and training for other models can be found in the Appendix. The model weights used for test inference in Section 4 were the best performing in the validation dataset.

## 4 Experiments and Results

We evaluated the performance in dMRI angular super-resolution using eight, previously unseen subjects within the HCP dataset. Our results included four variants of the PCCNN, as listed in Table 1. As dMRI data is most commonly used for downstream analyses, we compared the results in commonly used techniques for both single-shell and multi-shell experiments. Our results included other methods where appropriate, such as spherical harmonic (SH) interpolation for single-shell experiments, FOD-Net  for multi-shell FBA, and SR-q-DL  for multi-shell NODDI. FOD-Net is a 3D CNN that maps low resolution FOD data to high resolution FOD data. SR-q-DL is a 3D CNN that maps low resolution dMRI data to high resolution NODDI data. RCNN is a 3D recurrent CNN that maps low resolution dMRI data to high resolution dMRI data. When applicable, we also compared against low angular resolution data. All error metrics were calculated with respect to the full angular resolution HCP dataset.

The reported error values consist of the mean and standard deviation, with distribution statistics generated from the mean performance per test subject. Only voxels within the brain are included

   Model & Parameters \\  SR-q-DL  & 0.52 \\ PCCNN & 0.77 \\ PCCNN-Bv & 0.79 \\ PCCNN-Sp & 0.83 \\ PCCNN-Bv-Sp & 0.85 \\ RCNN  & 6.82 \\ FOD-Net  & 48.17 \\   

Table 1: Total number of parameters, in millions, of different deep learning approaches.

in the calculations, as determined by a brain mask segmentation. The best mean value in a given category is denoted in bold.

### Single-shell dMRI

Within single-shell inference, we evaluated performance across two analyses. Firstly, on raw dMRI data, of which any downstream analysis task would be derived from. We compared performance across all three shells available within the HCP dataset, as well as in three q-space sub-sampling regimes, as outlined in Table 2. Secondly, we compared performance within FBA, where metrics were derived from the inferred dMRI data. For this, we considered the \(b=1000\ }\) shell at three sub-sampling regimes.

#### 4.1.1 dMRI Analysis

Table 2 presents the absolute error (AE) of the inferred dMRI data as compared to the high resolution ground truth. For the \(b=2000}\) results, see Table 8 within the Appendix. The PCCNN-Bv-Sp model had the lowest AE in five out of the nine shell and sub-sampling combinations and had the overall best performance when averaged across all data. Despite having approximately a tenth of the number of parameters of the RCNN  model, the PCCNN models had lower mean errors across all data regimes except \(q_{}=6\) in shells \(b=\{2000,3000\}\ }\).

#### 4.1.2 Pixel Analysis

FBA is an analysis method used to estimate the intra-voxel fibre populations. To generate fixed data , response functions were first estimated in an unsupervised manner , and Single-Shell 3-Tissue (SS3T) constrained spherical deconvolution (CSD)  was performed to obtain white matter (WM) FODs. WM FODs were then normalised , and finally segmented into kirsis. This workflow used the MRtrix3  package. FOD data is stored as a finite SH expansion, and as such, was compared using the angular correlation coefficient (ACC) , given by Equation (5). To compare FBA data, the AE in the apparent fibre density (AFD) within each voxel was calculated by taking the absolute difference between the magnitudes of each inferred fibre population with respect to the ground truth values.

Table 3 summarises the performance in FBA in the low resolution baseline and model based reconstructions compared to the high resolution ground truth, whilst Figure 3 visualises a subset of reconstructed FODs within an axial slice. Here, all PCCNN models performed better than the low resolution baseline in all three sampling schemes in FOD ACC, and had lower AFD AE in the two lowest sampling schemes. The PCCNN models performed competitively against the RCNN. However, in the smallest sub-sampling scheme \(q_{}=6\), or equivalently \(6.6\%\) of the original number of voxels, the RCNN performed slightly better. Whilst the low resolution data had lower AFD AE at \(q_{}=20\), the PCCNN models had, in the worse case, a relative increased mean error of approximately \(6\%\), compared to \(35\%\) increase in the RCNN model. Additionally, the PCCNN models had greater ACC compared to the low resolution baseline across all sub-sampling schemes, suggesting that their use in probabilistic tractography  and connectomics  would be beneficial.

    & }=6\)} & }=10\)} & }=20\)} \\   & \(b=1000\) & \(b=3000\) & \(b=1000\) & \(b=3000\) & \(b=1000\) & \(b=3000\) \\  SH Interpolation & \(84.85 5.47\) & \(106.40 4.29\) & \(47.25 2.85\) & \(46.14 2.02\) & \(45.07 3.10\) & \(42.27 1.86\) \\ RCNN & \(74.01 4.03\) & \(\) & \(56.01 3.50\) & \(38.68 1.45\) & \(53.89 3.92\) & \(34.31 1.39\) \\ PCCNN & \(74.55 3.69\) & \(57.99 2.60\) & \(48.41 2.98\) & \(34.99 1.63\) & \(44.03 3.09\) & \(24.86 1.23\) \\ PCCNN-Bv & \(\) & \(56.71 2.56\) & \(48.40 2.89\) & \(34.18 1.65\) & \(45.32 3.13\) & \(24.77 1.20\) \\ PCCNN-Sp & \(75.01 3.64\) & \(57.41 2.57\) & \(47.79 2.99\) & \(34.16 1.49\) & \(\) & \(23.88 0.98\) \\ PCCNN-Bv-Sp & \(74.87 3.51\) & \(56.78 2.48\) & \(\) & \(\) & \(42.52 2.57\) & \(\) \\   

Table 2: Absolute error of dMRI intensity in eight subjects with data derived from various models. Models use single-shell data with angular dimension size \(q_{}\) as input, and produce inferred data with angular dimension size \(90-q_{}\) from the same shell. b-values are quoted in units of \(}\).

[MISSING_PAGE_EMPTY:8]

Input, or low resolution, data were not included in this analysis as multi-shell data is required to reconstruct NODDI parameters. The parameters \(f_{ intra}\) and \(f_{ iso}\) refer to the estimated volume fraction for intra-cellular and cerebrospinal fluid respectively, whilst orientation dispersion index (OD) is an estimation of the dispersion of white matter fibre bundles.

In this experiment, the PCCNN models perform competitively, yielding the lowest error in \(f_{ iso}\) and \(f_{ intra}\) across all three sub-sampling rates. The relative error in OD across different models mirrors that of the FOD ACC found in Table 4, suggesting that the trained models are able to infer data that generalise across different downstream analyses. The worst performing model across all sub-sampling schemes was the SR-q-DL model, as is demonstrated qualitatively by Figure 4. Whilst this model could benefit from the more constrained task of inferring NODDI data directly, the lack of geometric prior information, such as b-vector coordinates, suggests that these additions within the PCCNN models were important to its relatively high performance in this task.

## 5 Conclusion

The results of the experiments show that the proposed PCConv operation can be used to build a flexible low parameter network to effectively infer high angular resolution raw dMRI data. It does this through appropriate incorporation of task specific geometric properties, as well as suitable parameter sharing via a hypernetwork. We show that inference of raw dMRI data can be used in downstream analysis with moderately high fidelity, and therefore demonstrates the generalisability of the model.

All three of the PCCNN models with additional modifications (PCCNN-Bv, PCCNN-Sp, PCCNN-Bv-Sp) performed greater or equal to the standard PCCNN in approximately 47% of the experiments presented in this work. Additionally, at least one of the modified PCCNNs outperformed the standard PCCNN in approximately 87% of the experiments. Overall, this suggests that the inclusion of additional domain specific context through the coordinate embedding was beneficial to the models performance.

    & =6\)} & =20\)} \\   & POD ACC \(\) & AFD AE \(\) & POD ACC \(\) & AFD AE \(\) & POD ACC \(\) & AFD AE \(\) \\  Lowres & \(0.653 0.008\) & \(0.157 0.014\) & \(0.724 0.008\) & \(0.119 0.012\) & \(0.757 0.010\) & \(0.086 0.011\) \\ FOD-Net & \(\) & \(0.087 0.005\) & \(0.767 0.006\) & \(\) & \(0.776 0.008\) & \(\) \\ RCNN & \(0.685 0.010\) & \(\) & \(0.749 0.009\) & \(0.080 0.006\) & \(0.765 0.010\) & \(0.079 0.006\) \\ PCCNN & \(0.658 0.009\) & \(0.090 0.005\) & \(0.753 0.008\) & \(0.077 0.005\) & \(0.792 0.009\) & \(0.068 0.006\) \\ PCCNN-Bv & \(0.681 0.010\) & \(0.091 0.005\) & \(\) & \(0.080 0.006\) & \(\) & \(0.074 0.006\) \\ PCCNN-Sp & \(0.670 0.013\) & \(0.092 0.006\) & \(0.761 0.013\) & \(0.075 0.006\) & \(0.791 0.014\) & \(0.070 0.007\) \\ PCCNN-Bv-Sp & \(0.675 0.013\) & \(0.089 0.005\) & \(0.766 0.014\) & \(0.075 0.006\) & \(0.798 0.015\) & \(0.067 0.006\) \\   

Table 4: Comparison of metrics in fixed-based analysis for eight subjects with metrics derived from both input data and inferred data using various models. Models use single-shell data (\(b=1000}\)) with angular dimension size \(q_{ in}\) as input, and produce \(90-q_{ in}\)\(b=1000}\), \(90\)\(b=2000}\) inferred volumes, _Lowres_ denotes metrics derived from single-shell input only. Fibre orientation distribution (FOD) angular correlation coefficient (ACC) is defined by Equation (5), whilst apparent fibre density (AFD) absolute error (AE) is calculated using the magnitudes of all pixels within a voxel. \(\) denotes that a higher value is better.

    & =6\)} & =20\)} \\   & OD & \(f_{ iso}\) & \(f_{ intra}\) & OD & \(f_{ iso}\) & \(f_{ intra}\) \\  SR-q-DL & \(0.095 0.004\) & \(0.162 0.015\) & \(0.132 0.010\) & \(0.089 0.003\) & \(0.162 0.015\) & \(0.129 0.010\) \\ RCNN & \(\) & \(0.030 0.003\) & \(0.052 0.005\) & \(0.045 0.005\) & \(0.025 0.003\) & \(0.048 0.006\) \\ PCCNN & \(0.084 0.006\) & \(0.030 0.003\) & \(0.054 0.006\) & \(0.039 0.006\) & \(0.024 0.003\) & \(\) \\ PCCNN-Bv & \(0.075 0.006\) & \(0.026 0.003\) & \(0.050 0.006\) & \(\) & \(0.023 0.003\) & \(0.045 0.006\) \\ PCCNN-Sp & \(0.080 0.008\) & \(0.026 0.004\) & \(\) & \(0.038 0.006\) & \(0.025 0.004\) & \(0.051 0.012\) \\ PCCNN-Bv-Sp & \(0.083 0.008\) & \(\) & \(0.050 0.012\) & \(0.040 0.007\) & \(\) & \(0.047 0.012\) \\   

Table 5: Absolute error (AE) of orientation dispersion index (OD), \(f_{ intra}\), and \(f_{ iso}\) derived from neurite orientation dispersion and density imaging using input data and inferred data from various models. Models use single-shell data (\(b=1000}\)) with angular dimension size \(q_{ in}\) as input, and produce \(90-q_{ in}\)\(b=1000}\), \(90\)\(b=2000}\), and \(90\)\(b=3000}\) inferred volumes.

The application of super-resolution in a medical imaging context presents an inherent risk, as deep learning models can be susceptible to hallucinations, or high error rates, when making predictions on out-of-distribution data. Given that this work only includes data from relatively young healthy adults, future work will be needed to validate these methods in a diverse set of diagnostic settings such as datasets with pathologies, wide age ranges, and varying acquisition parameters.

Modern deep learning frameworks rely on highly optimised subroutines to perform standard discrete convolutions. Subsequently, despite having a lower number of parameters, the PCCNNs inference and train times were longer than the other methods presented in this work. This is because the PCConv uses a bespoke implementation that does not benefit from the aforementioned subroutines. This highlights the need for future research into more efficient implementations of the PCConv operation.

In summary, our work further explores the integration of geometric priors into the distinct geometry of dMRI data, and in doing so provides a flexible framework to incorporate arbitrary geometries in different domains and problem formulations.

## 6 Acknowledgements

This work was funded by the Engineering and Physical Sciences Research Council (EPSRC) Doctoral Training Partnership (DTP) Scholarship. Data were provided (in part) by the HCP, WU-Minn Consortium (Principal Investigators: David Van Essen and Kamil Ugurbil; 1U54MH091657) funded by the 16 NIH Institutes and Centers that support the NIH Blueprint for Neuroscience Research; and by the McDonnell Center for Systems Neuroscience at Washington University.