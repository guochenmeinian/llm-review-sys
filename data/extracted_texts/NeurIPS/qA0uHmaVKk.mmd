# Complex-valued Neurons Can Learn More but Slower than Real-valued Neurons via Gradient Descent

Jin-Hui Wu, Shao-Qun Zhang, Yuan Jiang, Zhi-Hua Zhou

National Key Laboratory for Novel Software Technology, Nanjing University, China

School of Artificial Intelligence, Nanjing University, China

{wujh,zhangsq,jiangy,zhouzh}@lamda.nju.edu.cn

###### Abstract

Complex-valued neural networks potentially possess better representations and performance than real-valued counterparts when dealing with some complicated tasks such as acoustic analysis, radar image classification, etc. Despite empirical successes, it remains unknown theoretically when and to what extent complex-valued neural networks outperform real-valued ones. We take one step in this direction by comparing the learnability of real-valued neurons and complex-valued neurons via gradient descent. We show that a complex-valued neuron can efficiently learn functions expressed by any one real-valued neuron and any one complex-valued neuron with convergence rate \(O(t^{-3})\) and \(O(t^{-1})\) where \(t\) is the iteration index of gradient descent, respectively, whereas a two-layer real-valued neural network with finite width cannot learn a single non-degenerate complex-valued neuron. We prove that a complex-valued neuron learns a real-valued neuron with rate \((t^{-3})\), exponentially slower than the \(O(^{-ct})\) rate of learning one real-valued neuron using a real-valued neuron with a constant \(c\). We further verify and extend these results via simulation experiments in more general settings.

## 1 Introduction

Complex-valued neural networks (CVNNs) utilize neuron models and operations in the complex-valued domain and are good at handling many complicated scenarios. Pioneering works successfully apply CVNNs to various areas, such as synthetic aperture radar image classification , acoustic analysis , and magnetic resonance image reconstruction . In these applications, input signals naturally contain phase information. CVNNs seem more suitable than real-valued neural networks (RVNNs) in phase-dependent tasks since empirical experiments and intuitive explanations suggest that CVNNs can possess better data representations of phase information and grasp the phase-rotational dynamics more accurately [4; 5].

Beyond the seemingly promising performance of CVNNs, many efforts have been devoted to the theoretical understanding of CVNNs. Most existing works demonstrate some desirable properties of CVNNs such as universal approximation [6; 7], the minimum width for universal approximation , boundedness and complete stability , most critical points not being local minimum , and local-minimum-free conditions . Some recent works demonstrate the approximation advantage of CVNNs in phase-invariant tasks by proving that neuromorphic networks with complex-valued operations can approximate radial functions with exponentially fewer parameters than RVNNs [11; 12]. However, those studies do not explain why CVNNs outperform RVNNs mostly in phase-dependent tasks, particularly when considering the fact that there are functions that can be efficiently approximated but not efficiently learned with gradient methods [13; 14]. Indeed, the general functional difference between CVNNs and RVNNS remains unknown.

In this paper, we take one step towards understanding when and to what extent one can benefit from common learning paradigms using CVNNs rather than RVNNs. More specifically, we attempt to identify the superiority and inferiority of CVNNs through the following fundamental questions

_When CVNNs outperform RVNNs via gradient descent?_

_Can we learn everything with CVNNs without paying additional price?_

We theoretically study the above two questions by focusing on learning a single neuron by optimizing the expected square loss via gradient descent under the setting of low-dimensional inputs and no bias term. Learning a single neuron, a simple and widely investigated learning problem [15; 16; 17; 18], is helpful to understand the difference between learning RVNNs and learning CVNNs since the neural operations inside a fundamental neuron model include the key factors of neural network learning. Furthermore, we conduct simulation experiments to verify our theories and extend them to more general settings of high-dimensional inputs and with bias terms. Our contributions are summarized in Table 1 and further explained as follows.

* **Complex-valued neurons can learn more than real-valued neurons.** We prove that using gradient descent, a single complex-valued neuron can efficiently learn functions expressed by any one real-valued neuron and any one complex-valued neuron with convergence rate \(O(t^{-3})\) and \(O(t^{-1})\) in Theorems 1 and 2, respectively. In contrast, we show the lower bound of expressing a non-degenerate complex-valued neuron with a two-layer RVNN in Theorem 4, which implies that a two-layer RVNN with finite width cannot learn a single non-degenerate complex-valued neuron. These results provide positive responses to the first question from at least two perspectives. Firstly, CVNNs outperform RVNNs when dealing with phase-sensitive tasks. Secondly, CVNN is always a conservative choice when we are unwilling to take the risk of failure.
* **Complex-valued neurons learn slower than real-valued neurons.** We present a lower bound \((t^{-3})\) for learning functions expressed by any one real-valued neuron using a complex-valued neuron via gradient descent in Theorem 6. This conclusion, together with the well-known linear convergence of learning functions expressed by any one real-valued neuron using a real-valued neuron , implies that CVNNs suffer from slower convergence than RVNNs when handling simple phase-independent tasks. This phenomenon answers the second question and reveals the additional price for learning everything with CVNNs.

The rest of this paper is organized as follows. Section 2 introduces related works. Section 3 details our settings and notations. Section 4 demonstrates that complex-valued neurons can learn more than real-valued neurons. Section 5 proves that complex-valued neurons learn slower than real-valued neurons. Section 6 concludes our work with prospects.

## 2 Related Works

**Complex-valued Neural Networks.** CVNNs originate in the 1990s when parameters of networks and the commonly used back-propagation algorithm are generalized to the complex-valued domain [20; 21; 22]. The motivation of CVNNs is at least threefold. From the representation perspective, CVNNs consider the phase information and model complex-valued problems more efficiently and properly than RVNNs [23; 24; 25]. From the computation perspective, a complex-valued neuron is capable of solving the exclusive-or problem and the detection of symmetry, whereas a real-valued neuron cannot . From the biological perspective, the recently proposed flexible transmitter neuron , which has a natural complex implementation, formulates the communication between pre-synapse and post-synapse precisely rather than considering only the pre-synapse in traditional MP neuron . CVNNs achieve better performance in versatile applications, especially those with naturally phase-related signals, such as radio frequency signals , sonar signals , and audio signals . We refer to two surveys for more detailed discussions [4; 5].

  
**Target Neurons** & **Real-valued Neurons** & **Complex-valued Neurons** \\  Real-valued Neuron & \(O(^{-ct})\) & \((t^{-3})\) (Theorems 1 and 6) \\ Complex-valued Neuron & Cannot Learn (Theorem 4) & \(O(t^{-1})\) (Theorem 2) \\   

Table 1: A summary of our contributions. The first column lists the target neurons. The second and third columns represent the convergence rates of learning the target neurons using real-valued neurons and complex-valued neurons via gradient descent, respectively.

From the aspect of theories, several works provide preliminary support for CVNNs by proving fundamental properties of CVNNs, such as shallow CVNNs are universal approximators [6; 7], most critical points are not spurious local minimum [10; 11], and CVNNs are bounded and completely stable . These theoretical insights only consider CVNNs without comparison with RVNNs. Another line of research verifies the superiority of CVNNs by comparing the approximation complexity of RVNNs and CVNNs and finding that CVNNs can express radial functions more efficiently [11; 12]. This line of work only takes approximation into account and does not explicitly consider learning processes, which is of more interest in practice. This work takes the first step toward analyzing and comparing the learning behaviors of CVNNs and RVNNs.

**Neuron Learning.** Neuron learning is the simplest case of neural network learning, and existing works mainly focus on learning real-valued neurons. Some studies demonstrate the possibility of learning one real-valued neuron or a network using meticulously designed algorithms [32; 33; 34]. Later, researchers investigate the learnability of neurons using standard gradient methods. An exponential convergence rate is established for learning one real-valued neuron with a real-valued neuron under different assumptions [19; 35; 36; 37; 38]. We consider the problem of learning between one real-valued neuron and one complex-valued neuron, as well as learning one complex-valued neuron using a complex-valued neuron. The heterogeneity between real-valued and complex-valued neurons makes the analysis of optimization behaviors more complicated.

## 3 Preliminaries

**Notations.** Suppose that the input dimension is an even number. For any vector \(^{2d}\), we denote \(x_{i}\) as the \(i\)-th coordinate of \(\). Let \(_{}=(x_{1};;x_{d})+(x_{d+1};;x_{2d})_{} ^{d}\) be the folded complex-valued representation of \(\), and \(}_{}=(x_{1};;x_{d})-(x_{d+1};;x_{2d})_{ }\) is the complex conjugate of \(_{}\). For any two vectors \(,^{2d}\), \(_{,}=(^{}\|\|^{-1}\|\|^{- 1})[0,]\) denotes the angle between \(\) and \(\). For any \(x\), \((x)=\{0,x\}\) indicates the ReLU activation function. Let \((z)\) denote the real part of a complex number \(z\). For any \(z\) and \([0,/2]\), \(_{}(z)\) denotes the real part of the symmetrical version of zReLU activation function , i.e.,

\[_{}(z)=\{(z)\;,&_{z }[-,]\;,\\ 0\;,&\;,.\]

where \(_{z}\) represents the argument of \(z\). For any proposition \(p\), we use \((p)\) to represent the indicator function of \(p\), i.e., \((p)=1\) if \(p\) is true and \((p)=0\) otherwise. A table of frequently used notations is provided at the beginning of Appendix A.

**Learning a Single Neuron.** We consider learning a target neuron with a learning neuron. A neuron generally takes the form \(_{}(;)\), where the weight \(^{2d}\) and phase \([0,/2]\) indicate learnable parameters, and we omit the bias term for technical reasons. This general formulation includes a real-valued neuron with ReLU activation \((^{})\) and a complex-valued neuron with zReLU activation \(_{}(_{}^{}}_{})\) as special cases. For any target neuron with parameters \((,_{v})\), the learning process consists of finding a neuron with parameters \((,_{w})\) to minimize the expected square loss

\[L(,_{w})=_{(,})}[(_{_{w}}(;)-_{_{v}}( ;))^{2}],\] (1)

where the learnable parameter \(_{w}\) occurs only when the learning neuron is complex-valued, and the input \(\) follows the Gaussian distribution \((,})\).

**Learning Algorithm and Initialization.** We utilize the projected gradient descent as the learning algorithm, where the projection guarantees the constraint on phase \([0,/2]\). To minimize a function \(f()\) with an initialization \(_{0}\), projected gradient descent iteratively updates weights along the negative gradient direction and projects the updated weights onto the constraint set, i.e., \(_{t+1}=P_{}(_{t}-_{t}_{}f(_{t}))\), where \(_{t}\) represents the step size, \(\) denotes the constraint set, and \(P_{}\) indicates the projection operator defined by \(P_{}(_{0})=_{}\|-_{0}\|\). We initialize weights of neurons with Gaussian distribution, which includes most standard initialization schemes in practice . The learnable parameter of the zReLU activation is initialized with \((0,/2)\), i.e., the uniform distribution on \([0,/2]\).

Complex-valued Neurons Can Learn More

In this section, we provide theoretical support for the learning advantage of complex-valued neurons by providing two positive learning scenarios for complex-valued neurons and one negative learning result for real-valued neurons. This section is organized as follows. Subsections 4.1 and 4.2 confirm the learning power of complex-valued neurons, by verifying that a complex-valued neuron can efficiently learn functions expressed by any one real-valued neuron and any one complex-valued neuron, respectively. Subsection 4.3 points out the limited learning capability of real-valued neurons, by proving that a two-layer RVNN with finite width cannot learn a single non-degenerate complex-valued neuron.

### Learning One Real-valued Neuron with a Complex-valued Neuron

We first investigate the case of learning one real-valued neuron with ReLU activation using a complex-valued neuron with zReLU activation, where the expected square loss in Eq. (1) becomes

\[L_{}(,)=_{( ,)}[(_{}(_{}^{} }_{})-(^{}))^{2}],\] (2)

where we abbreviate the phase parameter \(_{w}\) as \(\) since the target real-valued neuron does not have a phase parameter, \(^{2d}\) and \(^{2d}\) represent the weight vectors of the complex-valued neuron and the real-valued neuron, respectively. We assume \(\|\|=1\) without loss of generality. Then we present the first theorem for complex-valued neuron learning.

**Theorem 1**.: _Let \(d=1\). Suppose that \(_{0}(0,I_{2})\) and \(_{0}(0,/2)\). Let \(\{(_{t},_{t})\}_{t=0}^{}\) denote the parameter sequence of the complex-valued neuron generated by projected gradient descent when optimizing \(L_{}\), the expected loss of learning a real-valued neuron using a complex-valued neuron. If the step size \(_{t}=(0,1/(12))\), then we have_

\[[L_{}(_{t},_{t})t^{ 3}}+(1-)^{t+1-32/}]\;.\]

Theorem 1 shows that a complex-valued neuron can efficiently learn the functions expressed by any one real-valued neuron with convergence rate \(O(t^{-3})\) using projected gradient descent. It should be mentioned that we do not attempt to decrease the large constants in the theorem, as they do not hurt the constant probability and convergence rate.

The constant probability, rather than high probability, comes from the intrinsical difference between real-valued neurons and complex-valued neurons. A real-valued neuron activates half of the phase domain, whereas a complex-valued neuron may only activate a small part as controlled by the parameter \(\), which makes the expected loss a piecewise function. When the initialization of \(\) falls into the opposite direction of \(\) and \(\) is small, the activated regions of the real-valued and complex-valued neurons are not overlapped. Such a bad initialization happens with a constant probability and encourages the complex-valued neuron to decrease phase to minimize the loss. As a result, the phase of the complex-valued neuron will shrink to zero, which leads to a constant expected square loss and the failure of learning.

**Challenges.** Although \((,)=(,/2)\) is an obvious global minimum of the expected loss with \(L_{}=0\), the convergence conclusion in Theorem 1 is non-trivial. As one can see in the proof, the landscape of the expected loss possesses a stationary point \((,)=\). If we initialize \(=-k\) with \(k>0\), then it is easy to verify that \(\) converges to \(\) and \(\) decreases to \(0\) when the step size is sufficiently small. This implies that the landscape is not convex and the spurious stationary point is an attractor. The existence of this spurious stationary point becomes a critical obstacle in the proof and provides another reason for the hardness of a high-probability conclusion.

The proof idea of Theorem 1 mainly consists of estimating the first-order derivatives and finding an ideal region with both constant probability and convergence guarantees. We provide a proof sketch as follows. Firstly, we analyze the optimization behaviors of \(\) and \(\) in all pieces of the loss function separately. Then we identify an ideal region with desirable gradient properties: the gradient \(_{}L_{}(,)\) can be bounded by \(O((-/2)^{2})\), which implies that \(-/2\) decreases with an inversely propositional rate. Meanwhile, gradient descent on \(\) performs like a contraction mapping with fixed point \(\) and Lipschitz constant \(1-()\), i.e., \(\) converges to \(\) linearly when \(\)is large enough. Based on these observations, our convergence analysis consists of two stages, as shown in Fig. 1(a). In Stage I, the phase \(\) converges towards the global minimum, and the weight \(\) remains in the ideal region. When the phase grows above some threshold, one enters Stage II where the weight converges linearly and the phase maintains its slow convergence rate. Finally, we estimate the order of loss and provide a lower bound of the probability of falling into the ideal region with Gaussian initialization to complete the proof. Detailed proofs are available in Appendix B.

### Learning One Complex-valued Neuron with a Complex-valued Neuron

We proceed to consider learning one complex-valued neuron using a complex-valued neuron. In this case, the expected square loss in Eq. (1) can be rewritten as

\[L_{}(,)=_{( ,)}[(_{}(_{}^{} }_{})-_{}(_{}^{} }_{}))^{2}],\]

where \((,)\) denotes the parameter of the target complex-valued neuron, and \((,)\) is the learnable parameter. Without loss of generality, we still assume \(\|\|=1\). Here, we use gradient descent with vanishing step size \(x_{t+1}=x_{t}-_{t} f(x_{t})\), where the positive step size \(_{t}\) satisfies \(_{t} 0\) as \(t\). Then we present the second theorem for complex-valued neuron learning.

**Theorem 2**.: _Let \(d=1\), and \(_{v}[7/20,2/5]\). Suppose that \(_{0}(,I_{2})\) and \(_{w,0}(0,/2)\). Let \(\{(_{t},_{w,t})\}_{t=0}^{}\) denote the parameter sequence of the complex-valued neuron generated by projected gradient descent when optimizing \(L_{}\), the expected loss of learning a complex-valued neuron using a complex-valued neuron. If we utilize vanishing step size \(_{t}=\{c_{1},c_{2}/t\}\) with \(c_{1} 1/3000\) and \(c_{2} 20\), then_

\[[L_{}(_{t},_{w,t})^{3}}{c_ {1}t}] 10^{-5}\.\]

Theorem 2 demonstrates that a complex-valued neuron can efficiently learn functions expressed by any one complex-valued neuron with convergence rate \(O(t^{-1})\) and constant probability.

**Challenges.** It is observed that the \(O(t^{-1})\) convergence rate in Theorem 2 is slower than the \(O(t^{-3})\) convergence rate in Theorem 1. The deceleration of convergence comes from the intrinsic difficulties of learning functions expressed by any one complex-valued neuron. These difficulties become the main challenges in the analysis and can be understood from at least two perspectives. Firstly, there emerge new spurious stationary points. As one can see in the proof, the gradient with respect to \(_{w}\) becomes \(0\) once \(_{w}\) reaches \(/2\) and \(\) is close to \(\), i.e., \((,_{w})=(,/2)\) is a spurious stationary point. Secondly, the landscape of the loss function is no longer smooth. For both \(\) and \(_{w}\), the local landscape around the global minimum is roughly an absolute function, which declares the non-smoothness of the loss and the failure of gradient descent with a constant step size.

To overcome these obstacles, we apply mild conditions and slight modifications to guarantee convergence and maintain the generality of our conclusion. We separate the phase of the target complex

Figure 1: Subfigures (a) and (b) demonstrate the convergence stages of Theorems 1 and 2, respectively. The horizontal axis represents the iteration index of gradient descent. The black dotted line denotes the separation of convergence stages.

valued neuron far from \(0\) and \(/2\) in consideration of spurious local stationary points: As \(_{v}\) becomes closer to \(0\), it is more likely to obtain an initialization of the learning neuron that does not overlap with the target neuron. Then we will take the risk of falling into the spurious local minimum \((,_{w})=(,0)\). As \(_{v}\) approaches \(/2\), we are confronted by another spurious stationary point \(_{w}=/2\). We utilize gradient descent with a vanishing step size to cope with the non-smoothness of the loss function since a constant step size inevitably suffers from oscillation.

We summarize the proof idea of Theorem 2 as follows. The overall procedure is similar to that of Theorem 1 but every step is different and more challenging because of non-smoothness and more spurious stationary points. Firstly, we identify an ideal region with nice gradient properties: the gradient with respect to \(_{}\), the weight component perpendicular to \(\), points to the global minimum \(\) and maintains constant order. The gradient \(_{_{w}}\) is bounded and points towards \(_{v}\) when the angle \(_{,}\) is small enough. Meanwhile, the gradient with respect to \(_{}\) performs like a contraction mapping with fixed point \([1-(_{v}_{w}^{-1})]\) and Lipschitz constant \(1-()\), i.e., there exists a deviation of the fixed point from the global minimum. Based on these observations, we then prove the convergence with three stages, as demonstrated in Fig 1(b): In Stage I, \(_{}\), the weight component perpendicular to \(\), converges to \(0\) with an inversely proportional rate, and \(_{w}\) and \(_{}\) remain in the ideal region. Thus, the angle \(_{,}\) decreases with an inversely proportional rate. When \(_{,}\) declines below some threshold, we come to Stage II where phase \(_{w}\) converges to \(_{v}\) with rate \(O(t^{-1})\). As \(_{w}\) approaches \(_{v}\), the fixed point becomes close to \(\) and we step into Stage III where \(\) converges to \(\) with the same rate as \(_{w}\). Finally, we estimate the order of loss and provide a lower bound of the probability of falling into the ideal region with Gaussian initialization to complete the proof. We provide detailed proofs in Appendix C.

### Finite-Width RVNNs Cannot Learn a Single Non-degenerate Complex-valued Neuron

We then study learning one complex-valued neuron with zReLU activation using real-valued neurons. Since a complex-valued neuron has more parameters than a real-valued neuron, it is unfair to learn a complex-valued neuron with a single real-valued neuron. Thus, we consider the problem of learning a complex-valued neuron with a two-layer RVNN. A two-layer RVNN with \(n\) hidden neurons can be represented by \(^{}()\), where \(^{n 2d}\) and \(^{n}\) indicate weight parameters of the network, and \(\) is the ReLU activation function applied componentwisely. We still focus on the expected square loss, which takes the form

\[L_{}(,)=_{ (,)}[(^{}( )-_{}(_{}^{}}_ {}))^{2}],\]

where we abbreviate the phase parameter \(_{v}\) as \(\) since RVNN has no phase parameter. We are mainly interested in learning a non-degenerate complex-valued neuron, which is distinct from a real-valued neuron and defined as follows.

**Definition 3**.: _A complex-valued neuron is a non-degenerate one if \(\{0,/2\}\) and \(\)._

For a complex-valued neuron with phase \(=0\) or \(=\), the zReLU activation function always outputs \(0\). Then the complex-valued neuron is equivalent to a real-valued neuron with all zero weights. For a complex-valued with phase \(=/2\), the zReLU activation function is equivalent to the ReLU activation function. Thus, a non-degenerate complex-valued neuron is a non-real-valued neuron. Then we present the third theorem for complex-valued neuron learning.

**Theorem 4**.: _Let \(d=1\). For any non-degenerate complex-valued neuron with phase \((0,/2)\) and non-zero weight vector \(^{d}\), denote by \(L_{}\) the expected square loss of learning this complex-valued neuron using a one-hidden-layer RVNN with \(n\) hidden neurons. Then the loss satisfies_

\[L_{}(,)\|^{2}\{2 ,-2\}^{3}}{24(n+2)^{2}}>0\;.\]

Theorem 4 provides a positive lower bound for the expected squared loss of approximating a non-degenerate complex-valued neuron using a two-layer RVNN with a fixed number of hidden neurons. This positive lower bound indicates that there always remains a positive gap between the target non-degenerate complex-valued neuron and the two-layer RVNN of fixed width no matter how the parameters of the RVNN are learned. Thus, a finite-width RVNN cannot learn a single non-degenerate complex-valued neuron.

The lower bound decreases at rate \((\|\|^{2}\{2,-2\}^{3}n^{-2})\). The norm term \(\|\|\) depicts the magnitude of the problem, which affects the expected square loss quadratically from the homogeneity of zReLU. In the extreme case of \(=\), a trivial real-valued neuron with \(=\) reaches the lower bound \(0\). Meanwhile, the lower bound possesses a positive relation with a phase-dependent term \(\{2,-2\}\). Intuitively, this term indicates the difference between a complex-valued neuron and a real-valued neuron. A real-valued neuron corresponds to \(=0\) or \(=/2\) and this term measures the distance between the phase of a complex-valued neuron and a real-valued one. Finally, the lower bound decreases with a rate inversely proportional to the square of hidden size \(n\). We conjecture that this dependence is tight and cannot be improved: a two-layer RVNN with \(n\) neurons divides the space into \(n\) pieces, in each of which RVNN acts as a linear function. Choosing the \(n\) weight vectors of RVNN suitably, the difference between the RVNN and the complex-valued neuron remains small (of order \(n^{-1}\)) in each piece, which leads to the expected loss of order \(O(n^{-2})\).

The conditions in Theorem 4 are made for conciseness of proof and we believe the conclusion holds in more general cases. The dimension \(d=1\) corresponds to the intrinsic dimension of expressing a complex-valued neuron because of the rotational invariance of the inner product and the spherical symmetry of Gaussian distributions. Thus, additional dimensions contain no information and cannot improve the efficiency of approximation when \(d>1\). It is necessary to consider non-degenerate complex-valued neurons since degenerate complex-valued neurons are equivalent to real-valued ones.

We provide the central proof idea of Theorem 4 as follows. It is observed that the expected square loss \(L_{}\) is a piecewise quadratic function and each piece forms a sector centered at the origin with infinite radius. In each piece, \(L_{}\) takes the form \([(^{})^{2}]\). The proof mainly consists of two steps: we obtain a lower bound of \(L_{}\) in a sector and then sum over all sectors with suitable weights and order. Firstly, we consider the expected loss in a sector with a small central angle \(\), as shown in Fig. 2(a). We divide the sector into three identical subareas \(A_{1}\), \(A_{2}\), and \(A_{3}\). Then at least one of \(A_{1}\) and \(A_{3}\) remains \(/6\) away from the vertical direction of \(\), which leads to a lower bound \((^{2}\|\|^{2})\). Secondly, we consider the loss in four rotationally symmetric sectors, as shown in Fig. 2(b), where \(\) represents a complex-valued neuron, \(_{i}\) indicates a real-valued neuron, and the expression in each sector implies the activated neurons. It is observed that at least one sector possesses a weight vector with norm \((\|\|)\), no matter how we choose the real-valued neurons. Thus, the overall loss is bounded by \((^{2}\|\|^{2})\). Finally, we take the weight \(\) into consideration and sum over all sectors. For RVNN with \(n\) neurons, the best choice of \(=(n^{-1})\) arrives at the lower bound \((n^{-2}\|\|^{2})\). Detailed proofs are provided in Appendix D.

Figure 2: An illustration of the proof idea of Theorem 4. Shaded areas represent sectors with infinite radii. (a) The expectation \(_{ S}[(^{})^{2}]\) in the sector \(S\) equals the sum of the expectations on three subareas \(A_{1}\), \(A_{2}\), and \(A_{3}\). The minimum expectation on a subarea can be bounded by \((^{2}\|\|^{2})\). (b) The expected loss of learning a complex-valued neuron using four symmetric real-valued neurons in four symmetric sectors can be bounded by \((^{2}\|\|^{2})\), where \(_{i}\) indicates the weight vector of the \(i\)-th real-valued neuron, and \(\) denotes that of a complex-valued neuron.

**Summary and simulation experiments.** We summarize the main conclusions of this section in Table 2. Both a real-valued neuron and a complex-valued neuron succeed in learning functions expressed by any one real-valued neuron. But difference occurs when learning those expressed by any non-degenerate complex-valued neuron: A complex-valued neuron can efficiently learn functions expressed by any one complex-valued neuron, but a two-layer RVNN with finite width cannot learn a single non-degenerate complex-valued neuron. Such a disagreement demonstrates that a complex-valued neuron possesses more powerful learning capability, which profits from the consideration of phase information in complex-valued operations. Our theoretical conclusions are based on the setting of low-dimensional inputs and no bias term, and the simulation results in Fig. 3 verify and extend these discoveries in more general settings. Details about the simulation experiments are available in Appendix F.

## 5 Complex-valued Neurons Learn Slower

In this section, we demonstrate that complex-valued neurons learn slower than real-valued neurons. To arrive at this conclusion, we first rephrase the linear convergence of learning functions expressed by any one real-valued neuron using real-valued neurons. Then we prove that a complex-valued neuron learns the same class of functions at an exponentially slower rate.

We concentrate on learning one real-valued neuron \((^{})\) with \(\|\|=1\). When learning one real-valued neuron using a real-valued neuron, the expected square loss in Eq. (1) possesses the following simple closed form 

\[L_{}()=\|\|^{2}-\|\|[ _{,}+(-_{,})_{,}]+\.\]

It is widely known that a real-valued neuron learns a real-valued neuron with high probability and linear convergence rate , as reformulated by the following lemma.

**Lemma 5**.: _[_19_, Theorem 6.4]_ _Suppose that the weight vector \(^{2d}\) is initialized by a Gaussian distribution \((0,/(2d))\). Let \(L_{}\) denote the expected square loss of learning a real-valued neuron using a real-valued neuron. Then there exist constants \(c_{1},c_{2}\) such that gradient descent with suitable step size satisfies_

\[[L_{}(_{t})^{-c_{1}t}] 1- ^{-c_{2}d}\.\]

  
**Target** & **Real-valued Neuron** & **Complex-valued Neuron** \\  Real-valued Neuron & \(O(^{-ct})\) & \(O(t^{-3})\) (Theorem 1) \\ Complex-valued Neuron & \(\) & \(O(t^{-1})\) (Theorem 2) \\   

Table 2: A complex-valued neuron can learn more than a real-valued neuron.

Figure 3: The test error of learning a complex-valued neuron. In both the theoretical setting (Fig. 2(a)) and more general settings (Fig. 2(b)), complex-valued neurons have vanishing errors, while real-valued neurons converge to positive errors.

Recalling the expected loss of learning one real-valued neuron with a complex-valued neuron in Eq. (2), then we present the fourth theorem for complex-valued neuron learning, which provides a lower bound for the convergence rate.

**Theorem 6**.: _Let \(d=1\). Suppose that \(\|_{0}-\|<1\). Let \(\{(_{t},_{t})\}_{t=0}^{}\) denote the parameter sequence of the complex-valued neuron generated by projected gradient descent when optimizing \(L_{}\), the expected loss of learning a real-valued neuron using a complex-valued neuron. If the step size \(_{t}=(0,1/(12))\), then we have_

\[L_{}(_{t},_{t})/2}(^{ *}-_{0})^{3}}{8(t-T_{3}+1)^{3}}-(1- )^{t-T_{3}}\,,\]

_where \(^{*}=/2\), and \(T_{3}\) is a constant dependent on \(\|_{0}-\|\), \(\), and \(^{*}-_{0}\)._

Theorem 6 presents a lower bound for the expected loss of learning one real-valued neuron with a complex-valued neuron. It is observed that the negative term in the lower bound becomes \(0\) exponentially fast as \(t\) increases, and the positive term decreases with order \((t^{-3})\). Thus, the expected loss possesses a lower bound \((t^{-3})\) since the positive term dominates the loss when \(t\) grows sufficiently large. This lower bound matches the upper bound in Theorem 1. Thus, \(O(t^{-3})\) becomes the utmost limit of learning with a complex-valued neuron via gradient descent, i.e., we cannot expect a complex-valued neuron to learn faster than this utmost limit.

The conditions in Theorem 6 are technical and reasonable. The condition on \(_{0}\) is made for the conciseness of proof and can be removed. It is observed that \((,)=(,^{*})\) is the unique global minimum with \(L_{}=0\). Meanwhile, it is easy to verify that the loss goes to infinity when \(\|\|\). Thus, if we aim to obtain a small loss, the parameter sequence must fall into a small neighborhood of the global minimum, which is depicted by condition \(\|_{0}-\| R<1\). The condition \(_{0}^{*}\) holds with probability \(1\) when we initialize \(_{0}\) with a continuous distribution. This condition is necessary to obtain a meaningful lower bound since the numerator of the positive term equals \(0\) when \(_{0}=^{*}\). We emphasize that this condition is essential and cannot be removed because a complex-valued neuron with \(_{0}=^{*}\) is equivalent to a real-valued neuron, which enjoys linear convergence as stated in Lemma 5. The condition \(d=1\) corresponds to the simplest optimization problem of learning one real-valued neuron with a complex-valued neuron since high-dimensional optimization brings more difficulties. Thus, we cannot expect a complex-valued neuron to learn a real-valued neuron with a convergence rate faster than \(O(t^{-3})\) in a higher dimension.

We summarize the proof idea of Theorem 6 as follows. The gradient with respect to \(\) possesses the order \((^{*}-)^{2}+(^{*}-)_{,}\). The key intuition is that \(\) converges fast to the global minimum when \(_{,}\) remains large, but \(_{,}\) diminishes as \(\) converges to the global minimum \(\). The detailed proofs are complicated and consist of several stages, depicting the entangled convergence between \(\) and \(\) as shown in Figure 4. In Stage I, \(\) increases above a positive constant, which is a necessary condition for fast convergence of \(\) in Stage II. When the distance between \(\) and \(\) declines below a threshold, the angle \(_{,}\) becomes small. Then we enter Stage III, where \(\) converges faster than \(\). Stage IV begins when \(^{*}-\) dominates \(_{,}\). Then the gradient degenerates to order \((^{*}-)^{2}\), which implies a lower bound of convergence \(^{*}-=(t^{-1})\). Finally, estimating the loss around the global minimum leads to the conclusion. Detailed proofs are provided in Appendix E.

Figure 4: A demonstration of the convergence stages of Theorem 6. The horizontal axis represents the iteration index of gradient descent. The black dotted line is the separation of convergence stages.

**Summary and simulation experiments.** Table 3 summarizes the conclusions in this section, which shows that a complex-valued neuron learns slower than a real-valued one. A complex-valued neuron is more flexible since it can learn the phase. But this flexibility becomes redundant and slows down the convergence when learning a phase-independent function. Our theories are based on the setting of low-dimensional inputs and no bias term, and the simulation results in Fig. 3 verify and extend these discoveries in more general settings. Details about the experiments are available in Appendix F.

## 6 Conclusions and Prospects

In this paper, we investigate the problem of learning a single neuron using another neuron by optimizing the expected square loss via gradient descent. Firstly, we prove that a complex-valued neuron can efficiently learn functions expressed by any one real-valued neuron and any one complex-valued neuron with convergence rate \(O(t^{-3})\) and \(O(t^{-1})\), respectively, where \(t\) denotes the iteration index of gradient descent. Meanwhile, two-layer RVNNs with finite width cannot learn a single non-degenerate complex-valued neuron in a strong sense that there always exists a positive gap between a two-layer RVNN of fixed width and a non-degenerate complex-valued neuron. These conclusions suggest that complex-valued neurons can learn more than real-valued neurons since CVNNs benefit from the phase parameter, which helps CVNNs learn phase information more efficiently. Secondly, we provide a convergence lower bound \((t^{-3})\), which matches the upper bound, for learning one real-valued neuron with a complex-valued neuron. This conclusion, together with the well-known linear convergence of learning one real-valued neuron with a real-valued neuron, implies that complex-valued neurons learn slower than real-valued neurons in phase-independent tasks. This phenomenon captures the additional price for learning simpler tasks with more complicated models, where the redundant phase consideration exponentially slows down the convergence.

Our study serves as a preliminary attempt to compare the learning process of artificial neural networks with different functional operations. In the future, it is important to extend our theoretical results to more general settings, such as cases of high-dimensional inputs, equipped with bias terms, and over-parameterized architectures . Meanwhile, it is prospective to investigate complex-valued neuron learning from finite samples and derive a high-probability convergence condition. Since the empirical loss is a piecewise constant function with respect to the learnable phase parameter, it might be necessary to explore new learning algorithms, which is also encouraged by the neural tangent kernel aspect . Besides, it is promising to consider the more practical and challenging procedure of learning general functions with deep architectures.

Figure 5: The test error of learning a real-valued neuron. In both the theoretical setting (Fig. 4(a)) and more general settings (Fig. 4(b)), a complex-valued neuron learns a real-valued neuron slower.

  
**Target** & **Real-valued Neuron** & **Complex-valued Neuron** \\  Real-valued Neuron & \(O(^{-ct})\) (Lemma 5) & \((t^{-3})\) (Theorem 6) \\   

Table 3: A real-valued neuron learns faster than a complex-valued neuron.