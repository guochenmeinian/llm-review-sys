# Conformal PID Control for Time Series Prediction

Anastasios N. Angelopoulos

University of California, Berkeley

angelopoulos@berkeley.edu

Emmanuel J. Candes

Stanford University

candes@stanford.edu

Ryan J. Tibshirani

University of California, Berkeley

ryantibs@berkeley.edu

###### Abstract

We study the problem of uncertainty quantification for time series prediction, with the goal of providing easy-to-use algorithms with formal guarantees. The algorithms we present build upon ideas from conformal prediction and control theory, are able to prospectively model conformal scores in an online setting, and adapt to the presence of systematic errors due to seasonality, trends, and general distribution shifts. Our theory both simplifies and strengthens existing analyses in online conformal prediction. Experiments on 4-week-ahead forecasting of statewide COVID-19 death counts in the U.S. show an improvement in coverage over the ensemble forecaster used in official CDC communications. We also run experiments on predicting electricity demand, market returns, and temperature using autoregressive, Theta, Prophet, and Transformer models. We provide an extendable codebase for testing our methods and for the integration of new algorithms, data sets, and forecasting rules.1

## 1 Introduction

Machine learning models run in production systems regularly encounter data distributions that change over time. This can be due to factors such as seasonality and time-of-day, continual updating and re-training of upstream machine learning models, changing user behaviors, and so on. These distribution shifts can degrade a model's predictive performance. They also invalidate standard techniques for uncertainty quantification, such as _conformal prediction_.

To address the problem of shifting distributions, we consider the task of prediction in an adversarial online setting, as in . In this problem setting, we observe a (potentially) adversarial time series of deterministic covariates \(x_{t}\) and responses \(y_{t}\), for \(t=\{1,2,3,\}\). As in standard conformal prediction, we are free to define any _conformal score function_\(s_{t}:\), which we can view as measuring the accuracy of our forecast at time \(t\). We will assume with a loss of generality that \(s_{t}\) is negatively oriented (lower values mean greater forecast accuracy). For example, we may use the absolute error \(s_{t}(x,y)=|y-f_{t}(x)|\), where \(f_{t}\) is a forecaster trained on data up to but not including data at time \(t\).

The challenge in the sequential setting is as follows. We seek to invert the score function to construct a _conformal prediction set_,

\[_{t}=\{y:s_{t}(x_{t},y) q_{t}\},\] (1)

where \(q_{t}\) is an estimated \(1-\) quantile for the distribution of the score \(s_{t}(x_{t},y_{t})\) at time \(t\). In standard conformal prediction, we would take \(q_{t}\) to be a level \(1-\) sample quantile (up to a finite-sample correction) of \(s_{t}(x_{i},y_{i})\), \(i<t\); if the data sequence \((x_{i},y_{i})\), \(i\) were i.i.d. or exchangeable, then this would yield \(1-\) coverage  at each time \(t\). However, in the sequential setting, which does not assume exchangeability (or any probabilistic model for the data for that matter), choosing \(q_{t}\) in (1) to yield coverage is a formidable task. In fact, if we are not willing to make any assumptions about the data sequence, then a coverage guarantee at time \(t\) would only be possible with trivial methods, which construct prediction intervals of infinite sizes.

Therefore, our goal is to achieve _long-run coverage_ in time. That is, letting \(_{t}=1\{y_{t}_{t}\}\), we would like to achieve, for large integers \(T\),

\[_{t=1}^{T}_{t}=+o(1)\] (2)

under few or no assumptions, where \(o(1)\) denotes a quantity that tends to zero as \(T\). We note that (2) is not probabilistic at all, and every theoretical statement we will make in this paper holds deterministically. Furthermore, going beyond (2), we also seek to design flexible strategies to produce the sharpest prediction sets possible, which not only adapt to, but also anticipate distribution shifts.

We call our proposed solution _conformal PID control_. It treats the system for producing prediction sets as a proportional-integral-derivative (PID) controller. In the language of control, the prediction sets take a _control variable_, \(q_{t}\), and then produce a _process variable_, \(_{t}\). We seek to anchor \(_{t}\) to a _set point_, \(\). To do so, we apply corrections to \(q_{t}\) based on the error of the output, \(g_{t}=_{t}-\). By reframing the problem in this language, we are able to build algorithms that have more stable coverage while also prospectively adapting to changes in the score sequence, much in the same style as a control system. See the diagram in Figure 1.

### Peek at results: methods

Three design principles underlie our methods:

1. _Quantile tracking (P control)_. Running online gradient descent on the quantile loss (summed over all past scores) gives rise to a method that we call _quantile tracking_, which achieves long-run coverage (2) under no assumptions except boundedness of the scores. This bound can be unknown. Unlike adaptive conformal inference (ACI) , quantile tracking does not return infinite sets after a sequence of miscoverage events. This can be seen as equivalent to proportional (P) control.

2. _Error integration (I control)_. By incorporating the running sum \(_{i=1}^{t}(_{i}-)\) of the coverage errors into the online quantile updates, we can further stabilize the coverage. This _error integration_ scheme achieves long-run coverage (2) under no assumptions whatsoever on the scores (they can be unbounded). This can be seen as equivalent to integral (I) control.

3. _Scorecasting (D control)_. To account for systematic trends in the scores--this may be due to aspects of the data distribution, fixed or changing, which are not captured by the initial forecaster--we train a second model, a _scoreaster_, to predict the quantile of the next score. While quantile tracking and error integration are merely reactive, scorceasting is forward-looking. It can residualize out systematic trends in the errors and lead to practical advantages in terms of coverage and efficiency (set sizes). This can be seen as equivalent to derivative (D) control. Traditional control theory would suggest using a linear approximation \(g_{t}^{}=g_{t}-g_{t-1}\), but in our problem, we will typically choose more advanced scorceasting algorithms that go well beyond simple difference schemes.

These three modules combine to make our final iteration, the _conformal PID controller_:

\[q_{t+1}=}_{}+_{ i=1}^{t}g_{t}}_{}+^{}}_{}.\] (3)

Figure 1: Conformal PID Control, expressed as a block diagram.

In traditional PID control, one would take \(r_{t}(x)\) to be a linear function of \(x\). Here, we allow for nonlinearity and take \(r_{t}\) to be a _saturation function_ obeying

\[x c h(t) r_{t}(x) b,\ \ \ \ \ \ x-c h(t)  r_{t}(x)-b,\] (4)

for constants \(b,c>0\), and a sublinear, nonnegative, nondecreasing function \(h\)--we call a function \(h\) satisfying these conditions _admissible_. An example is the _tangent integrator_\(r_{t}(x)=K_{}(x(t)/(tC_{}))\), where we set \((x)=(x)\) for \(x[-/2,/2]\), and \(C_{},K_{}>0\) are constants. The choice of integrator \(r_{t}\) is a design decision for the user, as is the choice of scorecaster \(g_{t}^{}\).

We find it convenient to reparametrize (3), to produce a sequence of quantile estimates \(q_{t}\), \(t\) used in the prediction sets (1), as follows:

\[_{t+1}x_{i},y_{i},q_{i}i t,\] (5) \[q_{t+1}=_{t+1}+r_{t}_{i=1}^{t}( _{i}-).\]

Taking \(_{t+1}= g_{t}+g_{t}^{}\) recovers (3), but we find it generally useful to instead consider the formulation in (5), which will be our main focus in the exposition henceforth. Now we view \(_{t+1}\) as the scorecaster, which directly predicts \(q_{t+1}\) using past data. A main result of this paper, whose proof is given in Appendix A, is that the conformal PID controller (5) yields long-run coverage for any choice of integrator \(r_{t}\) that satisfies the appropriate saturation condition, and any scorecaster \(_{t+1}\).

**Theorem 1**.: _Let \(\{_{t}\}_{t}\) be any sequence of numbers in \([-b/2,b/2]\) and let \(\{s_{t}\}_{t}\) be any sequence of score functions with outputs in \([-b/2,b/2]\). Here \(b>0\), and may be infinite. Assume that \(r_{t}\) satisfies (4), for an admissible function \(h\). Then the iterations in (5) achieve long-run coverage (2)._

To emphasize, this result holds deterministically, with no probabilistic model on the data \((x_{t},y_{t})\), \(t\). (Thus in the case that the sequence is random, the result holds for all realizations of the random variables.) As we will soon see, this theorem can be seen as a generalization of existing results in the online conformal literature.

### Peek at results: experiments

**COVID-19 death forecasting.** To demonstrate conformal PID in practice, we consider 4-week-ahead forecasting of COVID-19 deaths in California, from late 2020 through late 2022. The base forecaster \(f_{t}\) that we use is the ensemble model from the COVID-19 Forecast Hub, which is the model used for official CDC communications on COVID-19 forecasting . In this forecasting problem, at each time \(t\) we actually seek to predict the observed death count \(y_{t+4}\) at time \(t+4\).

Figure 2 shows the central 80% prediction sets from the Forecast Hub ensemble model on the left panel, and those from our conformal PID method on the right. We use a quantile conformal score function, as in conformalized quantile regression , applied asymmetrically (i.e., separately) to the lower and upper quantile levels). We use the tan integrator, with constants chosen heuristically (as described in Appendix C), and an \(_{1}\)-regularized quantile regression as the scorecaster--in particular, the scorecasting model at time \(t\) predicts the quantile of the score at time \(t+4\) based on all previous forecasts, cases, and deaths, from _all 50 US states_. The main takeaway is that conformal PID control is able to correct for consistent underprediction of deaths in the winter wave of late 2020/early 2021. We can see from the figure that the original ensemble fails to cover 8 times in a stretch of 10 weeks, resulting in a coverage of 20%; meanwhile, conformal PID only fails to cover 3 times during this stretch, restoring the coverage to 70% (recall the nominal level is 80%).

How is this possible? The ensemble is mainly comprised of constituent forecasters that ignore geographic dependencies between states  for the sake of simplicity or computational tractability. But COVID infections and deaths exhibit strong spatiotemporal dependencies, and most US states experienced the winter wave of late 2020/early 2021 at similar points in time. The scorecaster is thus able to learn from the mistakes made on other US states in order to prospectively adjust the ensemble's forecasts for the state of California. Similar improvements can be seen for other states, and we include experiments for New York and Texas as examples in Appendix F, which also gives more details on the scorecaster and the results.

**Electricity demand forecasting.** Next we consider a data set on electricity demand forecasting in New South Wales , which includes half-hourly data from May 7, 1996 to December 5, 1998.

For the base forecaster we use a Transformer model  as implemented in darts. This is only re-trained daily, to predict the entire day's demand in one batch; this is a standard approach with Transformer models due to their high computational cost. For the conformal score, we use the asymmetric (signed) residual score. We use the tan integrator as before, and we use a lightweight Theta model , re-trained at every time point (half-hour), as the scorecaster.

The results are shown in the right panel of Figure 3, where adaptive conformal inference (ACI)  is also compared in the left panel. In short, conformal PID control is able to anticipate intraday variations in the scores, and produces sets that "hug" the ground truth sequence tightly; it achieves tight coverage without generating excessively large or infinite sets. The main reason why this is improved is that the scorecaster has a seasonality component built into its prediction model; in general, large improvements such as the one exhibited in Figure 3 should only be expected when the base forecaster is imperfect, as is the case here.

Figure 3: Results for electricity demand forecasting. The left column shows adaptive conformal inference (ACI), and the right column shows conformal PID control. The base forecaster is a Transformer model, and we use a tan integrator and a Theta scorecaster. The format of the figure follows that of Figure 2, except the nominal coverage is now \(1-=0.9\), and the coverage is averaged over a trailing window of 50 points (we also omit the red dots which mark miscoverage events). Summary statistics are available in Table 2.

Figure 2: Results for 4-week ahead COVID-19 death forecasting in California. The left column shows the COVID-19 Forecast Hub ensemble model, and the right column shows conformal PID control using the tan integrator, and a scorecaster given by \(_{1}\)-penalized quantile regression on all past forecasts, cases, and deaths from all 50 states. The top row plots coverage, averaged over a trailing window of 10 weeks. The nominal coverage level is \(1-=0.8\) and marked by a gray dotted line. The bottom row plots the prediction sets in gold along with the ground-truth times series (death counts). Miscoverage events are indicated by red dots. Summary statistics such as the coverage and average set size are available in Table 1.

### Related work

The adversarial online view of conformal prediction was pioneered by  in the same paper that first introduced ACI. Since then, there has been significant work towards improving ACI, primarily by setting the learning rate adaptively [17; 40; 7], and incorporating ideas from multiclibration to improve conditional coverage . It is worth noting that  also makes the observation that the ACI iteration can be generalized to track the quantile of the score sequence, although their focus is on adaptive regret guarantees. Because the topic of adaptive learning rates for ACI and related algorithms has already been investigated heavily, we do not consider it in the current paper. Any such method, such as those of [17; 7] should work well in conjunction with our proposed algorithms.

A related but distinct line of work surrounds online _calibration_ in the adversarial sequence model, which dates back to [14; 15], and connects in interesting ways to both game theory and online learning. We will not attempt to provide a comprehensive review of this rich and sizeable literature, but simply highlight [25; 24; 23] as a few interesting examples of recent work.

Lastly, outside the online setting, we note that several researchers have been interested in generalizing conformal prediction beyond the i.i.d. (or exchangeable) data setting: this includes [33; 28; 26; 12; 8], and for time series prediction, in particular, [9; 31; 38; 39; 3]. The focus of all of these papers is quite different, and they all rely on probabilistic assumptions on the data sequence to achieve validity; we include further discussion in Appendix B.

## 2 Methods

We describe the main components of our proposal one at a time, beginning with the quantile tracker.

### Quantile tracking

The starting point for quantile tracking is to consider the following optimization problem:

\[*{minimize}_{q}\ _{t=1}^{T}_{1-}(s_{t}-q),\] (6)

for large \(T\), where we abbreviate \(s_{t}=s_{t}(x_{t},y_{t})\) for the score of the test point, and \(_{1-}\) denotes the quantile loss at the level \(1-\), i.e., \(_{}(z)=|z|\) for \(z>0\) and \((1-)|z|\) for \(z 0\). The latter is the standard loss used in quantile regression [22; 21]. Problem (6) is thus a simple convex (linear) program that tracks the \(1-\) quantile of the score sequence \(s_{t}\), \(t N\). To see this, recall that for a continuously distributed random variable \(Z\), the expected loss \([_{1-}(Z-q)]\) is uniquely minimized (over \(q\)) at the level \(1-\) quantile of the distribution of \(Z\).

In the sequential setting, where we receive one score \(s_{t}\) at a time, a natural and simple approach is to apply _online gradient descent_ to (6), with a constant learning rate \(>0\). This results in the update:2

\[q_{t+1} =q_{t}+_{1-}(s_{t}-q_{t})\] \[=q_{t}+(_{t}-),\] (7)

where the second line follows as \(_{1-}(s_{t}-q_{t})=1-\) if \(s_{t}>q_{t}_{t}=1\), and \(_{1-}(s_{t}-q_{t})=-\) if \(s_{t} q_{t}_{t}=0\). Note that the update in (7) is highly intuitive: if we miscovered (committed an error) at the last iteration then we increase the quantile, whereas if we covered (did not commit an error) then we decrease the quantile.

Even though it is extremely simple, the quantile tracking iteration (7) can achieve long-run coverage own its own, provided the scores are bounded.

**Proposition 1**.: _Let \(\{s_{t}\}_{t}\) be any sequence of numbers in \([-b,b]\), for \(0<b<\). Then the quantile tracking iteration (7) satisfies_

\[|_{t=1}^{T}(_{t}-)| ,\]

_for any learning rate \(>0\) and \(T 1\). In particular, (7) yields long-run coverage as in (2)._A few remarks are in order. First, although Proposition 1 assumes boundedness of the scores, we do not need to know this bound in order to run (7) and obtain long-run coverage. As long as the scores lie in \([-b,b]\) for any finite \(b\), the guarantee goes through--clearly, the quantile tracker proceeds agnostically and performs the same updates in any case. Notably, the adaptive conformal inference algorithm can be expressed as a special case of the quantile tracker; see Appendix B.1 for details.

Second, for the learning rate, in practice we typically set \(\) heuristically, as some fraction of the highest score over a trailing window \(_{t}=\{s_{t-+1},,s_{t}\}\). On this scale, setting \(=0.1_{t}\) usually gives good results, and we use it in all experiments unless specified otherwise (we also set the window length \(\) to be the same as the length of the burn-in period for training the initial base forecaster and scorecaster).3 Extremely high learning rates result in volatile sets, while very low ones may fail to keep up with rapid changes in the score distribution.

### Error integration

Error integration is a generalization of quantile tracking that follows the iteration:

\[q_{t+1}=r_{t}_{i=1}^{t}(_{i}-),\] (8)

where \(r_{t}\) is a saturation function that satisfies (4) for an admissible function \(h\); recall that we use admissible to mean nonnegative, nondecreasing, and sublinear. As we saw in (13), the quantile tracker uses a _constant_ threshold function \(h\), whereas \(h\) is now permitted to grow, as long as it grows sublinearly, i.e., \(h(t)/t 0\) as \(t\). A non-constant threshold function \(h\) can be desirable because it means that \(r_{t}\) will "saturate" (will hit the conditions on the right-hand sides in (4)) less often, so corrections for coverage error will occur less often, and in this sense, a greater degree of coverage error can be tolerated along the sequence.

The next proposition, in particular its proof, makes the role of \(h\) precise. Importantly, Proposition 2 suffices to prove Theorem 1.

**Proposition 2**.: _Let \(\{s_{t}\}_{t}\) be any sequence of numbers in \([-b,b]\), where \(b>0\), and may be infinite. Assume that \(r_{t}\) satisfies (4), for an admissible function \(h\). Then the error integration iteration (8) satisfies_

\[|_{t=1}^{T}(_{t}-)| ,\] (9)

_for any \(T 1\), where \(c\) is the constant in (4). In particular, this means (8) yields long-run coverage (2)._

The choice of saturation function essentially corresponds to a choice of adaptive learning rate; see Appendix D for details.

### Scorecasting

The final piece to discuss is scorecasting. A scorecaster attempts to forecast \(q_{t+1}\) directly, taking advantage of any leftover signal that is not captured by the base forecaster. This is the role played by \(_{t+1}\) in (5). Scorecasting may be particularly useful when it is difficult to modify or re-train the base forecaster. This can occur when the base forecaster is computationally costly to train (e.g., as in a Transformer model); or it can occur in complex operational prediction pipelines where frequently updating a forecasting implementation is infeasible. Another scenario where scorecasting may be useful is one in which the forecaster and scorecaster have access to different levels of data. For example, if a public health agency collects epidemic forecasts from external groups, and forms an ensemble forecast, then the agency may have access to finer-grained data that it can use to recalibrate the ensemble's prediction sets (compared to the level of data granularity granted to the forecasters originally).

This motivates the need for scorecasting as a modular layer that "sits on top" of the base forecaster and residualizes out systematic errors in the score distribution. This intuition is made more precise by recalling, as described above (following Proposition 2), that scorecasting combined with error integration as in (5) is just a reparameterization of error integration (8), where \(q_{t}^{}=q_{t}-_{t}\) and \(s_{t}^{}=s_{t}-_{t}\) are the new quantile and new score, respectively. A well-executed scorecaster could reduce the variability in the scores and make them more exchangeable, resulting in more stable coverage and tighter prediction sets, as seen in Figure 3. On the other hand, an aggressive scorecaster with little or no signal can actually hurt by adding variance to the new score sequence \(s_{t}^{}\), which could result in more volatile coverage and larger sets.

There is no limit to what we can choose for the scorecasting model. We might like to use a model that can simultaneously incorporate seasonality, trends, and exogenous covariates. Common choices would be SARIMA (seasonal autoregressive integrated moving average) and ETS (error-trend-seasonality) models, but there are many other available methods, such as the Theta model , Prophet model , and neural network forecasters; see  for a review.

### Putting it all together

Briefly, we revisit the PID perspective, to recap how quantile tracking, error integration, and scorecasting fit in and work in combination. It helps to return to (3), which we copy again here:

\[q_{t+1}=g_{t}^{}+(_{t}-)+r_{t}_{i=1} ^{t}(_{i}-).\] (10)

Quantile tracking is precisely given by taking \(g_{t}^{}=q_{t}\) and \(r_{t}=0\). This can be seen as equivalent to P control: subtract \(q_{t}\) from both sides in (10) and treat the increment \(u_{t+1}=q_{t+1}-q_{t}\) as the process variable; then in this modified system, quantile tracking is exactly P control. For this reason, we use "conformal P control" to refer to the quantile tracker in the experiments that follow. Similarly, we use "conformal PI control" to refer to the choice \(g_{t}^{}=q_{t}\), and \(r_{t} 0\) as a generic integrator (for us, tan is the default). Lastly, "conformal PID control" refers to letting \(g_{t}^{}\) be a generic scorecaster, and \(r_{t} 0\) be a generic integrator.

## 3 Experiments

In addition to the statewide COVID-19 death forecasting experiment described in the introduction, we run experiments on all combinations of the following data sets and forecasters.

Data sets:

\[]})}\\ ]}}&\\ ]}}&]}}\\ ]}}&]}}\\ \]

In all cases except for the COVID-19 forecasting data set, we: re-train the base forecaster at each time point; construct prediction sets using the asymmetric (signed) residual score; and use a Theta model for the scorecaster. For the COVID-19 forecasting setting, we: use the given ensemble model as the base forecaster (no training at all); construct prediction sets using the asymmetric quantile score; and use an \(_{1}\)-penalized quantile regression as the scorecaster, fit on features derived from previous forecasts, cases, and deaths, as described in the introduction. And lastly, in all cases, we use a tan function for the integrator with constants chosen heuristically, as described in Appendix C.

The results that we choose to show in the subsections below are meant to illustrate key conceptual points (differences between the methods). Additional results are presented in Appendix G. Our GitHub repository, https://github.com/aangelopoulos/conformal-time-series, provides the full suite of evaluations.

### ACI versus quantile tracking

We forecast the daily Amazon (AMZN) opening stock price from 2006-2014. We do this in log-space (hence predicting the return of the stock). Figure 4 compares ACI and the quantile tracker, each Figure 4: Results for forecasting Amazon stock return, comparing ACI and quantile tracking (P control). The plots show AR as the base forecaster; the table summarizes the results of all four base forecasters. We use the default learning rates for both ACI and quantile tracking: \(=0.005\) and \(=0.1_{t}\), respectively.

Figure 5: As in Figure 4, but with larger learning rates for ACI and quantile tracking: \(=0.1\) and \(=0.5_{t}\), respectively.

with its default learning rate: \(=0.005\) for ACI, and \(=0.1_{t}\) for quantile tracking. We see that the coverage from each method is decent, but oscillates nontrivially around the nominal level of \(1-=0.9\) (with ACI generally having larger oscillations). Figure 5 thus increases the learning rate for each method: \(=0.1\) for ACI, and \(=0.5_{t}\) for the quantile tracker. We now see that both deliver very tight coverage. However, ACI does so by frequently returning infinite sets; meanwhile, the corrections to the sets made by the quantile tracker are nowhere near as aggressive.

As a final comparison, in Appendix E, we modify ACI to clip the sets in a way that disallows them from ever being infinite. This heuristic may be used by practitioners that want to guard against infinite sets, but it no longer has a validity guarantee for bounded or unbounded scores. The results in the appendix indicate that the quantile tracker has similar coverage to this procedure, and usually with smaller sets.

### The effect of integration

Next we forecast the daily Google (GOOGL) opening stock price from 2006-2014 (again done in log-space). Figure 6 compares the quantile tracker without and without an additional integrator component (P control versus PI control). We purposely choose a very small learning rate, \(=0.01_{t}\), in order to show how the integrator can stabilize coverage, which it does nicely for most of the time series. The coverage of PI control begins to oscillate more towards the end of the sequence, which we attribute at least in part to the fact that the integrator measures coverage errors accumulated over _all time_--and by the end of a long sequence, the marginal coverage can still be close to \(1-\) even if the local coverage deviates more wildly. This can be addressed by using a local version of the integrator, an idea we return to in the discussion.

### The effect of scorecasting

Figures 2 and 3 already showcase examples in which scorecasting offers significant improvement in coverage and set sizes. Recall that these were settings in which the base forecaster produces errors (scores) that have predictable trends. Further examples in the COVID-19 forecasting setting, which display similar benefits to scorecasting, are given in Appendix F.

Figure 6: Results for forecasting Google stock return, comparing quantile tracking with and without the integrator (P control versus PI control). The plots show Prophet as the base forecaster; the table summarizes the results of all four base forecasters. We purposely use a very small learning rate, \(=0.01_{t}\), in order to show how the integrator can stabilize coverage.

We emphasize that it is not always the case that scorecasting will help. In some settings, scorecasting may introduce enough variance into the new score sequence that the coverage or sets will degrade in stability. (For example, this will happen if we run a highly complex scorecaster on a sequence of i.i.d. scores, where there are no trends whatsoever.) In practice, scorecasters should be designed with care, just as one would design a base forecaster; it is unlikely that using "out of the box" techniques for scorecasting will be robust enough, especially in high-stakes problems. Appendix G provides examples in which scorecasting, run across all settings using a generic Theta model, can hurt (for example, it adds noticeable variance to the coverage and sets in some instances within the Amazon data setting).

## 4 Discussion and extensions

**Discussion.** Our work presents a framework for constructing prediction sets in time series that is analogous (and indeed formally equivalent) to PID control, consisting of quantile tracking (P control), which is simply online gradient descent applied to the quantile loss; error integration (I control) to stabilize coverage; and scorecasting (D control) to remove systematic trends in the scores (errors made by the base forecaster).

We found that the combination of quantile tracking and integration consistently yields robust and favorable performance in our experiments. Scorecasting provides additional benefits if there are trends left in scores that are predictable (and the scorecaster is well-designed), as is the case in some of our examples. Otherwise, scorecasting may add variability and make the coverage and prediction sets more volatile. Overall, designing the scorecaster (which includes the choice to even use one at all) is an important modeling step, just like the design of the base forecaster.

It is worth emphasizing that, with the exception of the COVID-19 forecasting example, our experiments are intended to be illustrative, and we did not look to use state-of-the-art forecasters, or include any and all possibly relevant features for prediction. Further, while we found that using heuristics to set constants (such as the learning rate \(\), and constants \(C_{},K_{}\) for the tan integrator) worked recently well, we believe that more rigorous techniques, along the lines of [17; 7], can be used to tune these adaptively in an online fashion.

**Extensions.** We now present an extension of our analysis to conformal risk control [1; 6; 13]. In this setting, we are given a sequence of loss functions \(L_{t}:2^{}\) satisfying \(L_{t}(,)=0\) for all \(y\), and \(L_{t}(,y)=1\) for all \(y\). The goal is to bound the deviation of the average risk \(_{t=1}^{T}L_{t}(_{t},y_{t})\) from \(\). We state a result for the integrator below, and give its proof in Appendix A.

**Proposition 3**.: _Consider the iteration \(q_{t+1}=r_{t}(_{i=1}^{t}(L_{i}(_{i},y_{i})-))\), with \(L_{t}\) as above. Assume that \(r_{t}\) satisfies (4), for an admissible function \(h\). Also assume that \(C_{t}(_{t},y_{t})=\) if \(q_{t}-b\) and \(\) if \(q_{t} b\), where \(b>0\), and may be infinite. Then for all \(T 1\),_

\[|_{t=1}^{T}(L_{t}(_{t},y_{t})-)| .\] (11)

_for any \(T 1\), where \(c\) is the constant in (4)._

We briefly conclude by mentioning that we believe many other extensions are possible, especially with respect to the integrator. Broadly, we can choose to integrate in a kernel-weighted fashion,

\[r_{t}_{i=1}^{t}(_{i}-) K(i,x_{i},y_{ i}),(t,x_{t},y_{t}).\] (12)

As a special case, the kernel could simply assign weight 1 if \(t-i w\), and weight 0 otherwise, which would result in an integrator that aggregates coverage over a training window of length \(w\). This can help consistently sustain better local coverage, for long sequences. As another special case, the kernel could assign a weight based on whether \(x_{i}\) and \(x_{t}\) lie in the same bin in some pre-defined binning of \(\) space, which may be useful for problems with group structure (where we want group-wise coverage). Various other choices and forms of kernels are possible, and it would be interesting to consider adding together a number of such choices (12) in combination, in a multi-resolution flavor, for the ultimate quantile update.

#### Acknowledgments

We would like to thank Tijana Zrnic, Amit Kohli, and Jordan Lekeufack for their valuable feedback. This work was supported by the National Science Foundation (NSF) Graduate Research Fellowship Program under grant no. 2146752, and the Office of Naval Research (ONR) Multi-University Research Initiative (MURI) Program under grant no. N00014-20-1-2787.