# Learning to Cooperate with Humans

using Generative Agents

 Yancheng Liang, Daphne Chen, Abhishek Gupta, Simon S. Du*, Natasha Jaques*

University of Washington

{yancheng, daphc, abhgupta, ssdu, nj}@cs.washington.edu

###### Abstract

Training agents that can coordinate zero-shot with humans is a key mission in multi-agent reinforcement learning (MARL). Current algorithms focus on training simulated human partner policies which are then used to train a Cooperator agent. The simulated human is produced either through behavior cloning over a dataset of human cooperation behavior, or by using MARL to create a population of simulated agents. However, these approaches often struggle to produce a Cooperator that can coordinate well with real humans, since the simulated humans fail to cover the diverse strategies and styles employed by people in the real world. We show _learning a generative model of human partners_ can effectively address this issue. Our model learns a latent variable representation of the human that can be regarded as encoding the human's unique strategy, intention, experience, or style. This generative model can be flexibly trained from any (human or neural policy) agent interaction data. By sampling from the latent space, we can use the generative model to produce different partners to train Cooperator agents. We evaluate our method--**G**enerative **A**gent **M**odeling for **M**ulti-agent **A**daptation (**GAMMA**)--on Overcooked, a challenging cooperative cooking game that has become a standard benchmark for zero-shot coordination. We conduct an evaluation with real human teammates, and the results show that **GAMMA** consistently improves performance, whether the generative model is trained on simulated populations or human datasets. Further, we propose a method for posterior sampling from the generative model that is biased towards the human data, enabling us to efficiently improve performance with only a small amount of expensive human interaction data. 1

## 1 Introduction

Producing agents that can cooperate well with unseen partners such as humans  is an important problem for a variety of multi-agent systems across domains like robotics or software agents. While being cooperative is a notable characteristic of human intelligence , training an artificial agent that cooperates well with humans poses a significant challenge. Human behaviors are uncertain and diverse, encompassing a wide range of preferences, abilities, and intentions. While humans can rapidly adapt to different partners, AI agents are particularly poor at generalizing to working with a novel human partner. Solving this issue of distribution shift between the human player's strategy and those seen during the training of an AI agent is crucial to achieving human-AI cooperation.

It is tempting to tackle this problem of learning to cooperate with humans using only data of human-human interactions . While this may certainly provide some leverage, in many situations the amount of human-human data available is far less than the amount of data required by data-hungry sequential decision making algorithms. This suggests that we need a way to generate synthetic data atscale, while still providing relevance to the problem of coordinating with new, real human teammates on deployment. On the opposite end is the paradigm of self-play, that generates entirely synthetic data by iteratively training agents against copies of themselves. Self-play, while initially envisioned as a paradigm for learning how to _compete_ with novel human players , has been applied to cooperative multi-agent reinforcement learning to find joint strategies for a team of agents when all agents are trained together . Self-play is an appealing approach for learning cooperative strategies, since it can be done in simulation, and does not require the expensive and time-consuming process of collecting and training on human cooperation data . However, agents trained with self-play may fail to coordinate effectively with novel humans . During self-play, the agent learns to form a convention with a copy of itself and relies on that convention to achieve seamless cooperation . When a human adopts a different strategy at test time, the agent fails to adapt, and merely continues to follow the convention formed in training. Such scenarios are fairly common, as self-play often generates "non-human" conventions . This points to the dual problems of 1) human-only data being expensive, and 2) synthetic-only data lacking coverage over human behaviors.

To tackle these challenges, a popular approach to solving the distribution shift problem in human-AI cooperation has become training a Cooperator agent against a _population_ of simulated agents rather than just itself . The goal of this work is often to create a diverse enough population of agents to cover the strategy space used by humans . However, the bottom line remains that truly covering the space of strategies with discrete samples of strategies can quickly become untenable. As we move towards more complex real-world tasks, with a myriad of possible strategies, representing every strategy by an individual agent in the population and coordinating with them becomes computationally difficult. Indeed, we empirically compare the two lines of work on using simulated versus human data for zero-shot coordination, and find that contrary to past results, using human data provides better performance, especially for more complex environments.

The key insight we will make in this work is that generative models offer a solution to the dual problems mentioned above. By training a generative model that can simulate cooperation partners, we can easily incorporate both human and simulated data. The resulting model can generate a diverse range of partner strategies, by interpolating between the policies it has been trained on, as well as composing strategies to create novel partners. Therefore, we propose to _train a generative model of partner behavior using a variational autoencoder (VAE) _ on a collection of coordination trajectories, either human or synthetically generated. The inference model of the VAE can be used to infer the latent variable \(z\) of a partner from its interaction data, encoding information about that partner's unique style or skill levels. The decoder can be used to generate the corresponding actions taken by the partner. Because the generative model can be trained on any combination of synthetic or human data, it can overcome the challenges of data scale and coverage to train adaptive Cooperators that much better cover the space of human behavior (see Figure 1).

Given the ability to generate this diverse distribution of partner policies, a single adaptive Cooperator can be trained to adapt to a range of partners, by sampling a novel partner strategy from the generative model at each episode during training. We call our full method **GAMMA**: **G**enerative **A**gent **M**odeling for **M**ulti-agent **A**daptation. While the interpolation properties of the generative model enable **GAMMA** to train more robust Cooperator agents, the availability of a controllable latent encoder also allows our desired adaptation behavior to be targeted to real human behavior. We propose a novel, economical way of incorporating a small amount of human-provided data into the sampling procedure of the generative model. This Human-Adaptive sampling method enables training Cooperators that are more targeted to partner with real human coordinators.

We test our method using Overcooked , a collaborative multiplayer cooking game requiring close coordination between two partners to successfully prepare recipes. In an evaluation in which trained agents play against novel humans in real-time, we find that **GAMMA** improves the performance of state-of-the-art coordination methods which rely on simulated populations of agents , or on human data . We summarize our contributions below:

* We present **GAMMA**, a novel approach for learning a generative model of partner strategies which can be trained on data from humans or a simulated population of agents. We sample from the generative model to simulate novel partners that are used to train a robust Cooperator agent. See Section 4.1 and Section 4.2.
* We propose a data-efficient and human-adaptive sampling technique to steer the model to generate more human-like partners, even with limited amounts of human data. See Section 4.3.

* We conduct an empirical evaluation via a user study with real human players. We bring together and directly compare two lines of prior work on zero-shot coordination with humans: training on simulated population data and training on human data. We find that contrary to past published results, training on human data is highly effective in our human evaluation, especially for the new, more complex layout we propose. However, regardless of the data source, **GAMMA** consistently provides significant performance improvements over multiple lines of past work when evaluated with real humans. See Section 5.

## 2 Related Work

**Training against simulated populations.** Building cooperative agents that generalize well to humans and novel partners is a long-standing problem of AI, and is known as ad-hoc team-play , or zero-shot coordination . Recently, FCP  adopted the idea of using a population of policies to train a strong zero-shot Cooperator agent, as a diverse population effectively prevents the Cooperator from exploiting one specific convention. Following this framework, many techniques [2; 37] have been proposed to improve the diversity of the population, such as Maximum Entropy Population (MEP). Reward shaping [30; 37] and quality diversity [22; 31; 35] utilize domain knowledge to create agents with diverse behaviors. Statistical diversity based on trajectory distributions  or population entropy  are straightforward optimization objectives. However, as indicated in , agents with different behavior distributions do not necessarily use different high-level strategies. To address this, the LIPO algorithm  instead tries to minimize the cross-play reward between different agents in the population to encourage them to form incompatible conventions. However, because LIPO agents can potentially sabotage the game, follow-up works [3; 24] propose strategies for preventing this, including Cross-play Optimized, Mixed-play Enforced Diversity (CoMeDi) . While this extensive literature presents different population creation methods, in contrast we propose a novel approach to _modeling_ this population with a generative model, and thus benefit the training process through the ability to flexibly sample unlimited new partners. Our experiments directly compare to FCP , CoMeDi , and MEP , and show that **GAMMA** can enhance performance above each of these population-generation techniques, even using the same underlying population of simulated agents, while also allowing for the incorporation of human data.

**Training with human data.** Another line of work has focused on collecting and training on human cooperation data . Learning from human data is inherently challenging due to limited scale, arbitrary human behavior, subjective preferences, and relative speed of human actions compared to agents. Another challenge of coordination comes from the heterogeneity, uncertainty and suboptimality [8; 21] of humans. The agent might fail to coordinate well when it does not properly infer the preferences or intentions of humans. Our work takes a novel approach of combining these two lines

Figure 1: We show the latent space covered by different methods.2For either simulated data or human data, the generative agents produced by **GAMMA** can cover a larger strategy space. Generative models can provide novel agents by interpolating the agents in the simulated population (a). On human data (b), the human proxy model only covers a subset of all human player behavior patterns, while the generative model can capture the diversity in the data. We can also control the latent space sampling (c) to model a target population of agents (e.g., human coordinators).

of work by training a generative model of partner strategies on simulated data, then using a limited amount of human data to fine-tune the model and sample from it more effectively.

**Inferring the partner type.** Our work is also related to the many works on multi-agent learning which attempt to learn a latent partner embedding to aid coordination. Some works use Bayesian inference [26; 36] or weight update  to decide the partner type from historical experience. Their methods require a manually-designed or predefined set of all possible partner policies. The idea of embedding-conditioned agent modeling is used in imitation learning  to learn more interpretable policies from human demonstrations. Grover et al.  train a latent variable model to learn agent representations from interaction data of different agents, and then infer a cooperation partner's latent vector and use it to condition a multi-agent RL policy. Papoudakis et al.  learn to predict the partner's representation from only the agent's own observation, avoiding accessing the global states during execution time. The idea of partner modeling has also been extended to offline RL  to train a zero-shot RL agent from a dataset. It is also known that generative models can learn diverse cooperative strategies from human-human cooperation demonstrations . While our work also infers hidden context about partner type, our aims are different; we use a generative model to simulate novel partners during training time as a way to train a Cooperator to be robust for zero-shot coordination with real humans.

## 3 Problem Formulation

A multi-agent system is typically modeled as a Markov game . In this work, we focus on two-player Markov games, but the formulation and our methods can be easily extended to more agents. The state space is \(\). At each step, two agents execute their actions \(a_{t},b_{t}\) from their policies \(:(),: ()\). They then receive a shared reward \(r_{t}:\). The next state is determined by a transition function \(:( )\). The value function \(V(,)\) of two policies is defined as the expectation of discounted cumulative reward \(_{t=0}^{T}^{t}r_{t}\). Under this formulation of Markov games, the learning objective for the fully cooperative joint policy \(_{,}V(,)\) can be perfectly defined. However, it is unclear how to describe the learning objective of human-AI cooperation under the framework of Markov games.

Inspired by the latent Markov Decision Process , we model the problem of cooperating with humans as an MDP with latent variable \(z\). We assume the human policy is conditioned on a latent variable \(z\). This latent variable \(z\) can be regarded as a representation of the partner's unique style, skill level, reaction speed, etc. Then the human population can be defined as a distribution \(()\) over the latent space. Given the human policy \(:(B)\) conditioning on the latent variable \(z\), the conditional transition function \(_{z}(s^{} s,a)=_{b}_{z}(b s)T(s ^{} s,a,b)\) and the reward function \(r_{z}(s,a)=_{b}_{z}(b s)r(s,a,b)\) can both be defined. Now with the value function for MDP \(M_{z}=\{_{z},r_{z}\}\) is \(V_{z}()=V(,_{z})\), the learning objective can be defined as \(^{*}_{}_{z()}[V_{z} ()].\)

## 4 GAMMA: Generative Agent Modeling for Multi-agent Adaptation

We will leverage generative models as our key tool for strategy diversification, to overcome the dual challenges of 1) lack of real human data and 2) the difficulty of synthetically covering the large strategy space of human partners. In being able to _generate_ diverse strategy profiles beyond the data,

Figure 2: Overview of the method for **GAMMA**. The generative model learns a latent distribution over partner strategies from either simulated or human data. Sampling partners from the generative model enables training a robust Cooperator that can coordinate with a variety of different humans.

these generative models can be used to train a Cooperator that can be deployed to coordinate with novel users, each with their own diverse styles, preferences and capabilities. Here, we first describe our procedure for learning a generative model from training data of discrete agents (or human-human data), and then describe how this can be used for targeted coordination with real human partners. We call our approach **G**enerative **A**gent **M**odeling for **M**ulti-agent **A**daptation (**GAMMA**).

### Learning Generative Models of Partner Behavior

The first step of **GAMMA** is learning the generative model from pre-existing coordination data. We assume access to a dataset of trajectories \(_{}=\{_{i}\}_{i=1}^{N}\), where each trajectory \(=\{(s_{t},a_{t},b_{t})\}_{t=0}^{T}\) is a sequence of multi-agent coordination behaviors. This dataset can be derived from human playing records. Alternatively, if there is a population of simulated agents available, this dataset can be collected by pairing them together to generate joint trajectories. For instance, in our experimental evaluation, we use agents generated through techniques like fictitious co-play (FCP)  to generate this dataset. The purpose of generative modeling here is to be able to model the multimodal marginal distribution over various strategy profiles in \(_{}\), a challenging distribution to model with standard maximum likelihood methods. Notably, the generative model is able to sample a landscape of agents that can be used to train a Cooperator, going beyond the quantity and diversity of the training data.

We develop a variant of the Variational Autoencoder (VAE)  to model the diverse strategies underlying the training data \(\). As is typical in variational inference frameworks, we propose to learn an encoder-decoder generative model with an approximate posterior (encoder) \(q(z;)\) that identifies the agent style from the trajectory. The decoder in this model, \(p(a_{t} z,_{0..t-1};)\) uses the agent's own past experience \(_{0..t-1}=(s_{0},a_{0},...,s_{t-1},a_{t-1},s_{t})\) and the latent variable \(z\) to predict the agent's next action. This structure of the variational inference model can be thought of as multi-modal behavior cloning, where the encoder history provides the latent variable required to model the distribution of generated actions. The encoder-decoder architecture described above, and shown in Figure 2 can be trained using an evidence lower bound (ELBO) loss:

\[(,)=_{ D}[_{z q( |,)}[_{t=1}^{T} p(a_{t} z,_{0..t-1};) ]+(q(z,)(0,I)]\] (1)

Importantly, this generative model is able to generate behaviors that go far beyond the training data, both in quantity and diversity, as is shown in Figure 1. We show in Fig 1 that while simulated behavior may not cover the space by itself, the generative model has significantly greater coverage over the strategy space. Importantly, we hypothesize that human behavior is more likely to lie within the span of strategies generated by the generative model. In this way, the interpolation and generalization of the generative model (even if it is not perfect) provides the expansion of the data required to effectively coordinate with humans.

### Training a Cooperator with Generative Coordination Models

Once a generative model has been obtained, it can be used to train a robust Cooperator agent. This is accomplished by treating the generative model as a partner generator, using it to simulate partner behavior that covers the space of real human behaviors, and training the Cooperator to optimize performance with these partners in simulation. In particular, the generative agent model \(p(a_{t} z,_{0..t-1};)\) can now be used as a generator of partner policies \(_{z}\) to train our Cooperator agent. As shown in Figure 2, for every episode we can sample latent variables from the prior \(z p(z)\), and use the conditioned action distribution \(p(a_{t} z,_{0..t-1};)\) as a partner \(_{z}\) for that episode. We aim to leverage these sampled partners from the generative model to train a single Cooperator \(_{C}\), treating the sampled partners \(_{z}\) as part of the environment. At each iteration, a batch of agents \(\{_{z_{i}}\}_{i=1}^{N}\) are generated using latent variable samples \(z_{i} D_{z}\). Then a batch of MDPs \(\{M_{z_{i}}\}_{i=1}^{N}\) can be derived from these training partner policies to learn our Cooperator policy \(\). Then we use PPO  to optimize \(_{C}\) over these training MDPs:

\[J(_{C})=_{z_{z}}[V_{z}()]=_{z p(z)}[V(,_{z})].\] (2)

Importantly, the Cooperator \(_{C}\) is not trained with imitation learning, but is rather trained with reinforcement learning to learn task-specific coordination behavior. The generative model from Section 4.1 is simply used to provide the quantity and diversity of agents necessary for meaningful generalization to real human behavior.

### Targeted GAMMA using Human-Adaptive Sampling and Fine-tuning

While the Cooperator described in Section 4.2 is trained by sampling partner agents from the generative model as \(z p(z),a_{t} p(a_{t} z,_{0..t-1};)\), this does not make use of human specific data if available, instead simply treating any human and synthetic data as equivalent. However, when coordinating with real human partners, it is useful to target model adaptation to be human-specific rather than to span the entire space of potentially irrelevant synthetic strategies (as shown in Figure 1). The key insight we will leverage to do so is that the latent space afforded by our generative model provides the ability to do _controllable sampling_ from any latent distribution. This distribution can be chosen to incorporate additional information about the desired population. In particular, when coordinating with real human partners, the latent prior distribution for sampling \(p(z)\) can be replaced with a human-centric one \(p_{h}(z)\), which is more focused on the part of the latent space relevant to human behavior. This suggests that given a small amount of human data \(_{h}\), a human-centered latent distribution \(p_{h}(z)\) can be quickly inferred by encoding the human data and estimating the latent Gaussian distribution that best explains encoded human data. The latent Gaussian mean is given by:

\[=_{_{h} D_{h}}[_{z q(_{ h})}[z]].\] (3)

Given this human-centered latent distribution \(p_{h}(z)=(,I)\), a targeted Cooperator that is meant for a particular target population can be trained by maximizing \(J(_{C})=_{z p_{h}(z)}[V(,_{z})].\)

Human-adaptive sampling makes the model focus less on adaptation to irrelevant synthetic partners and more on "human-centric" partners sampled from the generative model. As our experiments will show, this approach outperforms training a partner simulator using only human data, since with limited human data it is easy for the model to go out of distribution during generation and it can thus be brittle. In contrast, **GAMMA** is able to train a robust partner simulator using large amounts of synthetic data. Adding human-adaptive sampling to **GAMMA** can provide considerable generalization benefits even with modest amounts of data. Note that to better capture human data, we do not just target the latent space, but also perform some fine-tuning on the encoder and decoder of the VAE using human data.

## 5 Experiments

We evaluate **GAMMA** using the Overcooked environment  as a popular benchmark for prior work on human-AI cooperation [1; 24; 29; 36; 37]. In Overcooked, two players need to cooperate to divide the work of cooking and avoid getting in each other's way. This involves anticipating the intended goal and actions of the other player, and inferring which task would most usefully assist them. The layouts proposed in previous work are shown as the first five environments in Figure 3. The _Counter Circuit_ layout poses a hard coordination challenge with multiple strategies such as passing items across the countertop to the partner or moving clockwise/counterclockwise around the ring. Additionally, we include a custom layout based on _Counter Circuit_, termed _Multi-strategy Counter_. This new layout involves recipes with multiple ingredients, significantly increasing the complexity of the strategy space. Failing to infer the intentions of the partners and adding the wrong ingredients to the pot could ruin the entire dish, which increases the importance of coordination.

Our experiments investigate the following questions:

**H1: Using simulated agents.**: Will training a Cooperator against a generative model of partner strategies trained on a _population of simulated agents_ outperform training the Cooperator against the simulated agents directly?
**H2: Using real human data.**: Can the generative model be used to effectively leverage (small amounts of) human data, and also combine it with simulated agents?
**H3: State-of-the-art.**: Can we obtain better performance than competitive baselines using both simulated and human data?

**Learning from a Simulated Agent Population.**: We first investigate H1, and test whether generative agents can improve the performance of the Cooperator agent when training with simulated agent partners. We include three state-of-the-art approaches for creating a population of simulated agents.

**Fictitious Co-Play (FCP)** uses multiple self-play agents initialized across different random seeds with checkpoints sampled throughout training, which simulates a diverse population with varied skill levels. **Maximum Entropy Population-based Training (MEP)** enhances FCP by further diversifying the population through a population entropy learning objective. **Cross-play Optimized, Mixed-play Enforced Diversity (CoMeDi)** generates a diverse set of coordination policies by minimizing the cross-play reward and prevents self-sabotage using mixed-play regularization. For each method, we use the simulated agent population to create a dataset to learn the generative model.

**Learning from Human Data.** To investigate H2, we test whether the generative model can be used to model human behaviors from data produced by real human players. In this work, we assume the human dataset contains 20 to 50 trajectories, reflecting the popular open-source dataset provided by Carroll et al. . We use **PPO-BC** as the baseline where a BC model over human data is used as the partner during training. We test whether replacing the BC agent in this framework with our generative model trained on the human data (**PPO-BC-GAMMA**) provides better performance. Since we assume the amount of human data is limited, we also fine-tune a generative model pretrained from the simulated agent population with human data with both decoder-only (DFT) and full fine-tuning (DFT), as described in Section 4.3. We apply our **Human-Adaptive (HA)** sampling method to the fine-tuned generative model to sample from the human-centered latent distribution \(p_{h}(z)\).

**Simulated Agent Evaluation.** Following prior work , we create an automatic evaluation mechanism by using held-out human data to train a behavior cloning policy as a human proxy agent, and report the performance of the Cooperator when it is paired with this test human proxy agent. However, we note that methods which are explicitly trained against a human-proxy agent (PPO-BC) can easily exploit this metric in a way that is not indicative of actual performance with real humans, so we do not use this automatic evaluation to assess those methods. Instead, we conduct evaluations with real human players as the gold standard evaluation technique.

**Human Evaluation.** We run a user study with real human players in order to determine which method can most effectively coordinate with humans, and which method is rated as subjectively better by humans (H3). We conducted a study with \(80\) users recruited via online crowdsourcing from Prolific. Our study follows guidelines set by a UW IRB protocol. During the study, each user is instructed to play multiple rounds of Overcooked with a partner via a web interface, where in each round the partner is an agent following one of the \(9\) policies, in randomized order. We trained \(5\) random seeds per agent, and used a different randomly-selected seed for each of the \(9PP\) game rounds. For the human study, we focus on the most complex layouts, _Counter Circuit_ and _Multi-strategy Counter_. Each game lasts for 60 seconds. After each round, the user answers Likert scale survey questions  to rate their experience playing with the agent. At the end of the 8 rounds, humans also answer qualitative questions about the performance of the agents. Users are required to pass an attention check to ensure quality of their data. Using these answers, we conduct a qualitative analysis to understand which factors most heavily influence overall performance and users' preferences when playing with real humans.

## 6 Results

### Evaluation in Simulation

In this section, we present the evaluation results against a human proxy (behavior cloning) agent.

**H1: Using Simulated Data.** We train the generative models **FCP+GAMMA**, **CoMeDi+GAMMA** and **MEP+GAMMA** on the simulated agent population of FCP, CoMeDi and MEP, respectively. Figure 4 shows the learning curves of each method averaged on all layouts, and the final performance

Figure 3: The first five layouts _Cramped Room, Asymmetric Advantages, Coordination Ring, Forced Coordination, Counter Circuit_ are originally proposed in Carroll et al. . We create an additional _Multi-strategy Counter_ layout. In this new layout, humans can additionally choose between making onion vs. tomato soup, which makes coordination significantly more challenging.

of different methods for each of the different layouts. The Cooperator agent is evaluated by the zero-shot cooperation performance with a human proxy BC model. **GAMMA** consistently improves performance over the baselines, across layouts. This demonstrates that **GAMMA** provides a more efficient way to utilize the simulated agents by providing a landscape of partners to train the Cooperator. We also find the improvement gap increases as the layouts become more complex.

### Evaluation with Real, Novel Human Partners

As described in Section 5, we conduct an evaluation with real human players, recruiting new participants that were not in the training data. We evaluate all agents against the novel human players, and plot the cooperative scores achieved by each agent-human team in Table 3.

#### 6.2.1 H1: Training with Simulated Data.

We find that **GAMMA** offers significant improvements over prior techniques for training against simulated populations (**FCP, CoMeDi, MEP**). Although the trend of these results is consistent with the previous results for H1, we find that here **GAMMA** provides significantly enhanced performance improvements when tested with real humans, reaching the new state-of-the-art performance for both _Counter Circuit_ and _Multi-Strategy Counter_.

On our newly proposed, more complex layout, _Multi-Strategy Counter_, we find that the CoMeDi baseline performs so poorly that it cannot discover strategies which make use of the new tomato ingredient. The reason is because playing the onion-soup strategy and the tomato-soup strategy together can recreate an unrecoverable game state, which is not favored by the CoMeDi algorithm to include both types of agents in the population. Therefore, when we use **GAMMA** to train a generative partner model using data generated from the **CoMeDi** population, it also fails to learn any strategies involving tomatoes, and both models generalize poorly to playing with humans, although **CoMeDi+GAMMA** still offers a performance benefit over **CoMeDi**. This points to the fact that **GAMMA**, which is based on training a generative model on cooperation data, can fail to perform well if the cooperation data is not sufficiently diverse. This problem can be aptly described by the well-known adage "_garbage in, garbage out_".

However, when **GAMMA** is trained using the high-quality population found by **MEP**, we see that it performs extremely well, reaching a score of 88 on _Multi-Strategy COunter_, while MEP, the most competitive simulated population baseline, only achieved 64. This represents a 38% improvement.

#### 6.2.2 H2: Training with Human Data.

Comparing methods that make use of human data reveals some interesting findings. First, our results directly compare two lines of prior research: training on simulated populations vs. training against a BC model trained with a limited amount of human data (PPO-BC). While previous works show that using only simulated data can exceed PPO-BC and reach state-of-the-art performance , we find that on the more complex layout, PPO-BC is actually the best performing baseline. Modeling

Figure 4: Evaluation of different methods using a human proxy model. Rewards are normalized by the highest reward achieved on each layout. The learning curves in (a) show the average normalized reward across all environments, indicating that GAMMA helps the Cooperator converge to a higher reward. This improvement is also consistent across individual layouts, as illustrated in (b) and (c). We observe the largest performance gap on the ‘Counter Circuit’ and ‘Multi-Strategy Counter’ layouts, which are the most complex in terms of the number of valid cooperation strategies.

the human data with the generative model (**PPO+BC+GAMMA**) only provides performance improvements half the time. However, combining simulated and human data with **Human Adaptive GAMMA** provides significantly higher performance in both layouts, surpassing state-of-the-art zero-shot coordination techniques.

### H3: Can we obtain better performance than competitive baselines?

As revealed in Figure 5, **GAMMA HA** achieves the highest performance in both layouts, surpassing the most competitive baselines by 60% and 43% in _Counter Circuit_ and _Multi-Strategy Counter_, respectively (See Table 3). We conducted a statistical analysis of these results using Holm-Bonferroni correction, which can be found in Table 4.

#### 6.3.1 Subjective Human Ratings

As demonstrated in prior work , humans can adapt to deficiencies in the policies of AI agents and narrow the apparent performance gap between different agents by simply completing the task themselves. This means cooperation performance alone cannot measure whether a particular agent is frustrating or cumbersome for the human to cooperate with. Therefore, we also collect subjective ratings of the agents from the human participants. The results for the question about overall cooperation are plotted in Figure 6; additional results are available in the Appendix, which pertain to agent's ability to adapt (Fig. 9), and whether it was human-like (Fig. 10) or frustrating (Fig. 11).

For the overall ratings in Figure 6,the subjective ratings mirror the cooperative performance scores: when **GAMMA** is trained with the same data available to a baseline technique it improves performance in terms of the human ratings, and **GAMMA HA** gives consistently good performance on both layouts, in the sense that it receives a higher proportion of positive human ratings. We note that while PPO+BC+**GAMMA** did not show a performance benefit over PPO+BC on _Multi-strategy Counter_, human ratings of PPO+BC+**GAMMA** were more positive. One exception to the previous trends is that CoMeDi obtains high subjective ratings in _Counter Circuit_, although it performs poorly in _Multi-strategy Counter_. From the additional figures in the appendix, we can observe that **GAMMA** methods are rated as more adaptive, human-like, and less frustrating than other techniques.

#### 6.3.2 Qualitative Findings

Analyzing the qualitative results reported by participants in the human study, we find that **adaptation to the human partner** was a core theme distinguishing agents that humans liked. Participants reported that the **FCP + GAMMA** agent demonstrated an ability to learn from the user's actions, such as mimicking the user's strategy of placing onions on the table to save time. This adaptive behavior was positively received by a study participant: "I noticed that once I started to put back onions on the table that it did the same as I wanted to save time rather than going back for onions 3 times for soup. I thought it was interesting that it learned about my behavior." Because **GAMMA** models have been trained over a more diverse range of partners, they consequently exhibit the ability to better adapt to real humans during gameplay.

Figure 5: Performance of different agents when played with real humans. Error bars  use the Standard Error of the Mean (SE) for statistical significance (\(p<0.05\)). Methods trained on human data are shown in green. Whether training with simulated or human data, **GAMMA** shows consistent, statistically significant advantages over the baselines. **GAMMA**-**HA** is able to efficiently use the real human dataset to learn a better sampling of its latent space, achieving the best performance when cooperating with real humans.

Another common theme that emerged from the qualitative analysis was **consistency and predictability.** Erratic behavior was a common observation regarding the baseline methods. Users observed the AI performing random actions, such as moving onions around without an evident purpose or failing to complete necessary steps in the cooking process. In contrast, users report that agents using **GAMMA** behave in a logical, consistent manner. For example, for **FCP + GAMMA**, participants provided the following feedback that the agent was "more deliberate in its actions" and "its actions were logical". In contrast users reported that the baseline FCP agent, "didn't behave logically" and "the agent this time was inconsistent and did not help me with any of my orders at all." We hypothesize the reason for this "inconsistency" that occurred in baseline methods may be due to the human's behavior going out-of-distribution (OOD) of their training data, causing the resulting policy to make errors and behave in an unpredictable way. This points to the importance of obtaining better coverage of the human data distribution, as provided by **GAMMA** (see Figure 1).

## 7 Conclusion

In this work, we propose **GAMMA**, a novel approach to training a coordinator agent by using generative models to produce training partner agents. We conduct a comprehensive analysis using data from a study with real human cooperation partners, and show **GAMMA** outperforms baselines over both subjective human ratings and quantitative measurements of cooperation performance. We also provide a new perspective to compare different populations under the latent space of a generative model, showing how the simulated populations may not provide sufficient coverage of the range of human players.

**Limitations.** As shown by the performance of GAMMA+CoMeDi on _Multi-Strategy Counter_, obtaining good performance with our approach depends on having a reasonably diverse amount of cooperation data to train the model. If the quality of the simulated population data is too low, the approach can fail to provide significant benefits.

In this work, our human studies recruit participants from Prolific, which may not be representative of broader populations. Additionally, our human dataset is limited, which could reduce the diversity of strategies and force participants to adapt to strategies that the Cooperators are already familiar with.

We focus on the two-player setting in this study following prior work [29; 37; 40] because it is a first step toward enabling an AI assistant that could help a human with a particular task. Scaling up to more agents would exponentially increase the dataset size with our current techniques. Therefore, better sampling techniques are needed to address this issue.

**Future work.** Several potential directions are interesting for future work: 1) In this work, the amount of human data is limited, which restricts the performance of the generative model that learns human data from scratch. 2) An orthogonal direction is to condition the policy of the Cooperator on the embedding of the partner policy. We provide some preliminary results in Figure 15.

**Social impact.** Our work focuses on how to train AI agents that can effectively cooperate with diverse humans to assist them with tasks. We believe this is a critical component of eventually enabling assistive robots that could operate in human environments to assist the elderly or disabled to live more comfortably, or reduce the burden of domestic labour for all people.

Figure 6: Human ratings for different agents. Individuals were asked to respond to the following question: “Overall, I felt that the agent’s ability to coordinate with me was: {Very poor, Poor, Neutral, Good, Very good}”. **FCP + GAMMA**, **PPO + BC + GAMMA**, and **GAMMA-HA** consistently receive higher ratings for ability to coordinate compared to their respective baselines.