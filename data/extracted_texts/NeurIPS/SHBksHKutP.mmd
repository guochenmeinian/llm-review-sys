# Contextual Stochastic Bilevel Optimization

Yifan Hu

EPFL & ETH Zurich

Switzerland

&Jie Wang

Gatech

United States

&Yao Xie

Gatech

United States

&Andreas Krause

ETH Zurich

Switzerland

&Daniel Kuhn

EPFL

Switzerland

Correspondence: yifan.hu@epfl.ch.

###### Abstract

We introduce _contextual stochastic bilevel optimization_ (CSBO) - a stochastic bilevel optimization framework with the lower-level problem minimizing an expectation conditioned on some contextual information and the upper-level decision variable. This framework extends classical stochastic bilevel optimization when the lower-level decision maker responds optimally not only to the decision of the upper-level decision maker but also to some side information and when there are multiple or even infinite many followers. It captures important applications such as meta-learning, personalized federated learning, end-to-end learning, and Wasserstein distributionally robust optimization with side information (WDRO-SI). Due to the presence of contextual information, existing single-loop methods for classical stochastic bilevel optimization are unable to converge. To overcome this challenge, we introduce an efficient double-loop gradient method based on the Multilevel Monte-Carlo (MLMC) technique and establish its sample and computational complexities. When specialized to stochastic nonconvex optimization, our method matches existing lower bounds. For meta-learning, the complexity of our method does not depend on the number of tasks. Numerical experiments further validate our theoretical results.

## 1 Introduction

A _contextual stochastic bilevel optimization_ (CSBO) problem differs from a classical stochastic bilevel optimization problem only in that its lower-level problem is conditioned on a given context \(\).

\[_{x^{d_{x}}} F(x):=_{_{},_{|}}[f(x,y^ {*}(x;);,)]\] (upper level) \[ y^{*}(x;):=_{y^{d_{y}}}_{ _{|}}[g(x,y;,)]\ \ \ x.\] (1)

Here \(_{}\) and \(_{|}\) are random vectors, with \(_{|}\) denoting the conditional distribution of \(\) for a given \(\). The dimensions of the upper-level decision variable \(x\) and the lower-level decision variable \(y\) are \(d_{x}\) and \(d_{y}\), respectively. The functions \(f\) and \(g\) are continuously differentiable in \((x,y)\) for any given sample pair \((,)\). The function \(f(x,y;,)\) can be nonconvex in \(x\), but the function \(g(x,y;,)\) must be strongly convex in \(y\) for any given \(x\), \(\) and \(\). Thus, \(y^{*}(x;)\) is the unique minimizer of the strongly convex lower-level problem for any given \(x\) and \(\). Note that, on its own, the lower-level problem can be viewed as a contextual stochastic optimization problem  parametrized in \(x\). We assume that the joint distribution of \(\) and \(\) is unknown. However, we assume that we have access to any number of independent andidentically distributed (i.i.d.) samples from \(_{}\), and for any given realization of \(\), we can generate any number of i.i.d. samples from the conditional distribution \(_{|}\). The bilevel structure generally makes the objective function \(F(x)\) nonconvex in the decision variable \(x\), except for few special cases. Thus we aim to develop efficient gradient-based algorithms for finding an \(\)-stationary point of the nonconvex objective function \(F\), i.e., a point \(\) satisfying the inequality \(\| F()\|^{2}^{2}\).

CSBO generalizes the widely studied class of _stochastic bilevel optimization_ (SBO) problems (Ghadimi and Wang, 2018) whose lower-level problem minimizes an unconditional expectation.

\[_{x^{d_{x}}} _{_{}}[f(x,y^{*}(x);)]\] (2) \[ y^{*}(x):=_{y^{d_{y}}}_{ _{}}[g(x,y;)].\]

Indeed, (2) is a special case of CSBO if the upper- and lower-level objective functions are stochastically independent. Another special case of CSBO is the _conditional stochastic optimization_ (CSO) problem (Hu et al., 2020, 2020, He and Kasiviswanathan, 2023, Goda and Kitade, 2023) representable as

\[_{x^{d_{x}}}_{_{}}[f(x,_{_{|}}[h(x;,)];)].\] (3)

Indeed, (3) is a special case of CSBO if we set \(g(x,y;,)=\|y-h(x;,)\|^{2}\), in which case the lower-level problem admits the unique closed-form solution \(y^{*}(x,)=_{_{|}}[h(x;,)]\).

**Applications.** Despite the wide applicability of SBO to various machine learning and game theory paradigms, SBO cannot capture two important cases. The first case involves the lower-level decision maker responding optimally not only to the upper-level decision \(x\) but also to some side information \(\) like weather, spatial, and temporal information. The second case involves multiple lower-level decision makers, especially when the total number is large. CSBO well captures these two settings and encompasses various important machine learning paradigms as special cases, including meta-learning (Rajeswaran et al., 2019), personalized federated learning (Shamsian et al., 2021; Xing et al., 2022; Wang et al., 2023), hierarchical representation learning (Yao et al., 2019), end-to-end learning (Donti et al., 2017; Sadana et al., 2023; Rychener et al., 2023; Grigas et al., 2021), Sinkhorn distributionally robust optimization (DRO) (Wang et al., 2023), Wasserstein DRO with side information (Yang et al., 2022), information retrieval (Qiu et al., 2022), contrastive learning (Qiu et al., 2023), and instrumental variable regression (Muandet et al., 2020). Below we provide a detailed discussion of meta-learning, personalized federated learning, and end-to-end learning.

**Meta-Learning and Personalized Federated Learning.** Both applications can be viewed asspecial cases of CSBO. For meta-learning with \(M\) tasks or personalized federated learning with \(M\) users, the goal is to find a common regularization center \(\) shared by all tasks or all users.

\[_{x}\ _{i}_{D_{i}^{test}_{i}} \ l_{i}(y_{i}^{*}(x),D_{i}^{test})\] (upper level) (4)

Here, \(\) is the empirical uniform distribution on \([M]\). The upper-level problem minimizes the generalization loss for all tasks/all users by tuning the joint regularization center \(x\), and the lower-level problem finds an optimal regularization parameter \(x_{i}\) close to \(x\) for each individual task or user. Note that \(M\) may be as large as \((10^{3})\) in meta-learning and as large as \((10^{9})\) in personalized federated learning. Thus, it is crucial to design methods with complexity bounds independent of \(M\).

**End-to-End Learning.** Traditionally, applications from inventory control to online advertising involve a two-step approach: first estimating a demand function or the click-through rate, and then making decisions based on this estimation. End-to-end learning streamlines this into a single-step method, allowing the optimization to account for estimation errors, thereby enabling more informed decisions. This can be framed as a special case of CSBO, where the upper-level problem seeks the best estimator, while the lower-level problem makes optimal decisions based on the upper-level estimator and the contextual information \(\). For example, in online advertising, \(x\) represents the click-through rate estimator, and \(y^{*}(x;)\) denotes the optimal advertisement display for a customer characterized by the feature vector \(\). For a comprehensive review, see the recent survey paper (Sadana et al., 2023).

**Challenges.** Given the wide applicability of CSBO, it is expedient to look for efficient solution algorithms. Unfortunately, when extended to CSBO, existing algorithms for SBO or CSO either suffer from sub-optimal convergence rates or are entirely unable to handle the contextual information. Indeed, a major challenge of CSBO is to estimate \(y^{*}(x;)\) for (typically) uncountably many realizations of \(\). In the following, we explain in more detail why existing methods fail.

If the lower-level problem is strongly convex, then SBO can be addressed with numerous efficient single-loop algorithms (Guo and Yang, 2021; Guo et al., 2021; Chen et al., 2022a, 2021; Hong et al., 2023; Yang et al., 2021). Indeed, as the unique minimizer \(y^{*}(x)\) of the lower-level problem in (2) depends only on the upper-level decision variable \(x\), these algorithms can sequentially update the upper- and lower-level decision variables \(x\) and \(y\) in a single loop while ensuring that the sequence \(\{y^{t}\}_{t}\) approximates \(\{y^{*}(x^{t})\}_{t}\). Specifically, these approaches leverage the approximation

\[y^{*}(x^{t+1})-y^{*}(x^{t}) y^{*}(x^{t})^{}(x^{t+1}-x^{t}),\]

which is accurate if \(x\) is updated using small stepsizes. However, these algorithms generically fail to converge on CSBO problems because the minimizer \(y^{*}(x;)\) of the lower-level problem in (1) additionally depends on the context \(\), i.e., each realization of \(\) corresponds to a lower-level constraint. Consequently, there can be infinitely many lower-level constraints. It is unclear how samples from \(_{|}\) corresponding to a fixed context \(\) can be reused to estimate the minimizer \(y^{*}(x;^{})\) corresponding to a different context \(^{}\). Since gradient-based methods sample \(^{t}\) independently in each iteration \(t\), no single sequence \(\{y^{t}\}_{t}\) can approximate the function \(\{y^{*}(x^{t},^{t})\}_{t}\). Guo and Yang (2021) and Hu et al. (2023) analyze a special case of the CSBO problem (1), in which \(\) is supported on \(M\) points as shown in (4). However, the sample complexity of their algorithm grows linearly with \(M\). In contrast, we develop methods for general CSBO problems and show that their sample complexities are _independent_ of the support of \(\).

SBO problems can also be addressed with _double-loop stochastic gradient descent_ (DL-SGD), which solve the lower-level problem to approximate optimality before updating the upper-level decision variable (Ji et al., 2021; Ghadimi and Wang, 2018). We will show that these DL-SGD algorithms can be extended to CSBO problems and will analyze their sample complexity as well as their computational complexity. Unfortunately, it turns out that, when applied to CSBO problems, DL-SGD incurs high per-iteration sampling and computational costs to obtain a low-bias gradient estimator for \(F\). More precisely, solving the contextual lower-level problem to \(\)-optimality for a fixed \(\) requires \((^{-2})\) samples from \(_{|}\) and gradient estimators for the function \(g\), which leads to a \(}(^{-6})\) total sample and computational complexity to obtain an \(\)-stationary point of \(F\).

**Methodology.** Given these observations indicating that existing methods can fail or be sub-optimal for solving the CSBO problem, we next discuss the motivation for our algorithm design. Our goal is to build gradient estimators that share the same small bias as DL-SGD but require much fewer samples and incur a much lower computational cost at the expense of a slightly increased variance.

To obtain estimators with low bias, variance, and a low sampling and computational cost, we propose here a _multilevel Monte Carlo_ (MLMC) approach (Giles, 2015; Hu et al., 2021; Asi et al., 2021), which is reminiscent of the control variate technique, and combine it with inverse propensity weighting (Glynn and Quinn, 2010). We refer to the proposed method as _random truncated MLMC_ (RT-MLMC) and demonstrate that the RT-MLMC estimator for \( F\) requires only \((1)\) samples from \(_{|}\). This is a significant improvement _vis-a-vis_ DL-SGD, which requires \(}(^{-2})\) samples. Consequently, the sample complexity as well as the gradient complexity over \(g\) (i.e., the number \(g\)-gradient evaluations) of RT-MLMC for finding an \(\)-stationary point of \(F\) is given by \(}(^{-4})\).

While the idea of using MLMC in stochastic optimization is not new (Hu et al., 2021; Asi et al., 2021), the construction of MLMC gradient estimators for CSBO and the analysis of the variance of the RT-MLMC gradient estimators are novel contributions of this work.

### Our Contributions

* We introduce CSBO as a unifying framework for a broad range of machine learning tasks and optimization problems. We propose two methods, DL-SGD and RT-MLMC, and analyze their complexities; see Table 1 for a summary. When specialized to SBO and CSO problems, RT-MLMC displays the same performance as the state-of-the-art algorithms for SBO (Chen et al., 2021) and CSO (Hu et al., 2021), respectively. When specialized to classical stochastic nonconvex optimization, RT-MLMC matches the lower bounds by Arjevani et al. (2023).

* For meta-learning with \(M\) tasks, the complexity bounds of RT-MLMC are constant in \(M\). Thus, RT-MLMC outperforms the methods by Guo and Yang (2021) and Hu et al. (2023) when \(M\) is large. For Wasserstein DRO with side information (Yang et al., 2022), existing methods only cater for affine and non-parametric decision rules. In contrast, RT-MLMC allows for neural network approximations. We also present the first sample and gradient complexity bounds for WDRO-SI.
* For meta-learning and Wasserstein DRO with side information, our experiments show that the RT-MLMC gradient estimator can be computed an order of magnitude faster than the DL-SGD gradient estimator, especially when the contextual lower-level problem is solved to higher accuracy.

**Preliminaries** For any function \(:^{d_{x}}^{d_{y}}\) with arguments \(x^{d_{x}}\) and \(y^{d_{y}}\), we use \(\), \(_{1}\) and \(_{2}\) to denote the gradients of \(\) with respect to \((x,y)\), \(x\) and \(y\), respectively. Similarly, we use \(^{2}\), \(^{2}_{11}\) and \(^{2}_{22}\) to denote Hessians of \(\) with respect to \((x,y)\), \(x\) and \(y\), respectively. In addition, \(^{2}_{12}\) stands for the \((d_{x} d_{y})\)-matrix with entries \(^{2}_{x_{i}y_{j}}\). A function \(:^{d}\) is \(L\)-Lipschitz continuous if \(|(x)-(x^{})| L\|x-x^{}\|\) for all \(x,x^{}^{d}\), and it is \(S\)-Lipschitz smooth if it is continuously differentiable and satisfies \(\|(x)-(x^{})\| S\|x-x^{}\|\) for all \(x,x^{}^{d}\). In addition, \(\) is called \(\)-strongly convex if it is continuously differentiable and if \((x)-(x^{})-(x^{})^{}(x-x^{} )\|x-x^{}\|^{2}\) for all \(x,x^{}^{d}\). The identity matrix is denoted by \(I\). Finally, we use \(}()\) as a variant of the classical \(()\) symbol that hides logarithmic factors.

## 2 Algorithms for Contextual Stochastic Bilevel Optimization

Throughout the paper, we make the following assumptions. Similar assumptions appear in the SBO literature (Ghadimi and Wang, 2018; Guo and Yang, 2021; Chen et al., 2022, 2021; Hong et al., 2023).

**Assumption 2.1**.: _The CSBO problem (1) satisfies the following regularity conditions:_

* \(f\) _is continuously differentiable in_ \(x\) _and_ \(y\) _for any fixed_ \(\) _and_ \(\)_, and_ \(g\) _is twice continuously differentiable in_ \(x\) _and_ \(y\) _for any fixed_ \(\) _and_ \(\)_._
* \(g\) _is_ \(_{g}\)_-strongly convex in_ \(y\) _for any fixed_ \(x\)_,_ \(\) _and_ \(\)_._
* \(f\)_,_ \(g\)_,_ \( f\)_,_ \( g\) _and_ \(^{2}g\) _are_ \(L_{f,0}\)_,_ \(L_{g,0}\)_,_ \(L_{f,1}\)_,_ \(L_{g,1}\) _and_ \(L_{g,2}\)_-Lipschitz continuous in_ \((x,y)\) _for any fixed_ \(\) _and_ \(\)_, respectively._
* _If_ \((,)_{(,)}\)_, then_ \( f(x,y;,)\) _is an unbiased estimator for_ \(_{(,)_{(,)}}[f(x,y;,)]\) _with variance_ \(^{2}_{f}\) _uniformly across all_ \(x\) _and_ \(y\)_. Also, if_ \(_{|}\)_, then_ \( g(x,y;,)\) _is an unbiased estimator for_ \(_{_{|}}[g(x,y;,)]\) _with variance_ \(^{2}_{g,1}\)_, and_ \(^{2}g(x,y;,)\) _is an unbiased estimator for_ \(^{2}_{_{|}}[g(x,y;,)]\) _with variance_ \(^{2}_{g,1}\) _uniformly across all_ \(x\)_,_ \(y\) _and_ \(\)_._

Assumption 2.1 ensures that problem (1) is well-defined. In particular, by slightly adapting the proofs of (Ghadimi and Wang, 2018, Lemma 2.2) and (Hong et al., 2023, Lemma 2), it allows us to show that \(F\) is \(L_{F}\)-Lipschitz continuous as well as \(S_{F}\)-Lipschitz smooth for some \(L_{F},S_{F}>0\). Assumptions 2.1 (i-iii) also imply that the gradients of \(f\) and \(g\) with respect \((x,y)\) can be interchanged with the expectations with respect to \((,)_{,}\) and \(_{|}\). Hence, Assumptions 2.1 (i-iii) readily imply the unbiasedness of the gradient estimators imposed in Assumption 2.1 (iv). In fact, only the uniform variance bounds do not already follow from Assumptions 2.1 (i-iii).

In order to design SGD-type algorithms for problem (1), we first construct gradient estimators for \(F\). To this end, we observe that the Jacobian \(_{1}y^{*}(x;)^{d_{x} d_{y}}\) exists and is Lipschitz continuous in \(x\) for any fixed \(\) thanks to (Chen et al., 2021, Lemma 2). By the chain rule, we therefore have

\[ F(x)=_{(,)_{(,)}}_{1}f (x,y^{*}(x;);,)+_{1}y^{*}(x;)^{}_{2}f(x,y^{*}(x; );,).\]

   Nonconvex CSBO & Sample Complexity & Gradient Complexity of \(g\) and \(f\) & Per-iteration Memory Cost \\ 
**RT-MLMC** & \(}(^{-4})\) & \(}(^{-4})\) & \(}(^{-4})\) & \((d_{x}+d_{y})\) \\ 
**DL-SGD** & \(}(^{-6})\) & \(}(^{-6})\) & \(}(^{-4})\) & \((d_{x}+d_{y})\) \\   

* For meta-learning with \(M\) tasks, the complexity bounds of RT-MLMC are constant in \(M\). Thus, RT-MLMC outperforms the methods by Guo and Yang (2021) and Hu et al. (2023) when \(M\) is large. For Wasserstein DRO with side information (Yang et al., 2022), existing methods only cater for affine and non-parametric decision rules. In contrast, RT-MLMC allows for neural network approximations. We also present the first sample and gradient complexity bounds for WDRO-SI.
* For meta-learning and Wasserstein DRO with side information, our experiments show that the RT-MLMC gradient estimator can be computed an order of magnitude faster than the DL-SGD gradient estimator, especially when the contextual lower-level problem is solved to higher accuracy.

**Preliminaries** For any function \(:^{d_{x}}^{d_{y}}\) with arguments \(x^{d_{x}}\) and \(y^{d_{y}}\), we use \(\), \(_{1}\) and \(_{2}\) to denote the gradients of \(\) with respect to \((x,y)\), \(x\) and \(y\), respectively. Similarly, we use \(^{2}\), \(^{2}_{11}\) and \(^{2}_{By following a similar procedure as in (Ghadimi and Wang, 2018), we can derive an explicit formula for \(_{1}y^{*}(x;)\) (for details we refer to Appendix B) and substitute it into the above equation to obtain

\[ F(x)=_{(n,)_{(n,)}} _{1}f(x,y^{*}(x;);,)\\ -_{^{}_{|}} _{12}^{2}g(x,y^{*}(x;);^{},)(x,y^{*}(x;);) _{2}f(x,y^{*}(x;);,),\]

where \((x,y;)=(_{_{|}}_{22}^{2}g(x,y;,))^{-1}\). Thus, the main challenges of constructing a gradient estimator are to compute and store the minimizer \(y^{*}(x,)\) as well as the inverse expected Hessian matrix \((x,y;)\) for all (potentially uncountably many) realizations of \(\). Computing these two objects _exactly_ would be too expensive. In the remainder of this section, we thus derive estimators for \(y^{*}(x;)\) and \((x,y;)\), and we combine these two estimators to construct an estimator for \( F(x)\).

**Estimating \(y^{*}(x;)\).** We estimate \(y^{*}(x;)\) using the gradient-based method EpochSGD by Hazan and Kale (2014); Asi et al. (2021), which involves \(K\) epochs of increasing lengths. Each epoch \(k=1,,K\) starts from the average of the iterates computed in epoch \(k-1\) and then applies \(2^{k}\) stochastic gradient steps to the lower-level problem with stepsize \(2^{-k}\) (see Algorithm 1). In the following we use the output \(y_{K+1}^{0}\) of Algorithm 1 with inputs \(K\), \(x\), \(\) and \(y_{0}\) as an estimator for the minimizer \(y^{*}(x;)\) of the lower-level problem. We use EpochSGD for the following two reasons. First, EpochSGD attains the optimal convergence rate for strongly convex stochastic optimization in the gradient oracle model (Hazan and Kale, 2014). In addition, it is widely used in practical machine learning training procedures. Note that \(y^{*}(x;)\) could also be estimated via classical SGD. Even though this would lead to similar complexity results, the analysis would become more cumbersome.

**Estimating \((x,y;)\).** Following (Ghadimi and Wang, 2018), one can estimate the inverse of an expected random matrix \(A\) with \(0 A I\) using a Neumann series argument. Specifically, we have

\[[_{A_{A}}A]^{-1}=_{n^{}=0}^{}(I- _{A_{A}}A)^{n}=_{n^{}=0}^{}_{n=1 }^{n^{}}_{A_{n}_{A}}(I-A_{n})_{n^{ }=0}^{N}_{n=1}^{n^{}}_{A_{n}_{A}}(I- A_{n}).\]

The truncated series on the right hand side provides a good approximation if \(N 1\). Assumption 2.1 (iii) implies that \(0_{22}^{2}g(x,y;_{n},) 2L_{g,1}I\). Hence, the above formula can be applied to \(A=}_{22}^{2}g(x,y;_{n},)\), which gives rise to an attractive estimator for \((x,y;)\) of the form

\[(x,y;):=}I&=0,\\ }_{n=1}^{}\ (I-} _{22}^{2}g(x,y;_{n},))& 1.\] (5)

Here, \(\) is a random integer drawn uniformly from \(\{0,1,,N-1\}\) that is independent of the i.i.d. samples \(_{1},,_{}\) from \(_{|}\). Chen et al. (2022b) showed that the estimator (5) displays the following properties. Its bias decays exponentially with \(N\), its variance grows quadratically with \(N\), and its sampling cost grows linearly with \(N\). Below we call \(N\) the approximation number.

**Estimating \( F(x)\) via DL-SGD.** For any given \(K\) and \(N\), we construct the DL-SGD estimator for the gradient of \(F\) by using the following procedure: (i) generate a sample \(\) from \(_{}\), (ii) generate i.i.d. samples \(^{}\) and \(^{}\) from the conditional distribution \(_{|}\), (iii) run EpochSGD as described in Algorithm 1 with an arbitrary initial iterate \(y_{1}^{0}\) to obtain \(y_{K+1}^{0}\), and (iv) construct \((x,y_{K+1}^{0};)\) as in (5). Using these ingredients, we can now construct the DL-SGD gradient estimator as

\[^{K}(x):=_{1}f(x,y_{K+1}^{0};^{},)-_ {12}^{2}g(x,y_{K+1}^{0};^{},)(x,y_{K+1}^{0}; )_{2}f(x,y_{K+1}^{0};^{},).\] (6)In Lemma 2 below, we will analyze the bias and variance as well as the sampling and computational costs of the DL-SGD gradient estimator. We will see that a small bias \(\|[^{K}](x)- F(x)\|\) can be ensured by setting \(K=((^{-1}))\), in which case EpochSGD computes \((^{-2})\) stochastic gradients of \(g\). From now on, we refer to Algorithm 2 with \(v(x)=^{K}(x)\) as the DL-SGD algorithm.

### RT-MLMC Gradient Estimator

The bottleneck of evaluating the DL-SGD gradient estimators is the computation of \(y^{0}_{K+1}\). The computational costs can be reduced, however, by exploiting the telescoping sum property

\[^{K}(x) =^{1}(x)+_{k=1}^{K}[^{k+1}(x)-^{k}(x)]\] \[=^{1}(x)+_{k=1}^{K}p_{k}^{k+1}(x) -^{k}(x)}{p_{k}}=^{1}(x)+_{ _{}}^{+1}(x)-^{}(x)}{p_{}},\]

where \(^{}\) is defined as in (6) with \(k=1,,K\) replacing \(K\), and where \(_{}\) is a truncated geometric distribution with \(_{}(=k)=p_{k} 2^{-k}\) for every \(k=1,,K\). This observation prompts us to construct the RT-MLMC gradient estimator as

\[(x)=^{1}(x)+p_{}^{-1}(^{+1}(x)-^{}(x)).\] (7)

The RT-MLMC gradient estimator has three key properties:

* It is an unbiased estimator for the DL-SGD gradient estimator, i.e., \(_{_{}}[(x)]= {v}^{K}(x)\).
* Evaluating \((x)\) requires computing \(y^{0}_{k+1}(x,)\) with probability \(p_{k}\), which decays exponentially with \(k\). To ensure a small bias, we need to set \(K=((^{-1}))\), and thus \(p_{K}=()\). Hence, most of the time, EpochSGD only needs to run over \(k K\) epochs. As a result, the average sampling and computational costs are markedly smaller for RT-MLMC than for DL-SGD.
* Since \(^{k+1}(x)\) and \(^{k}(x)\) differ only in \(y^{0}_{k+1}\) and \(y^{0}_{k}\), both of which are generated by EpochSGD and are thus highly correlated, \(^{k+1}(x)-^{k}(x)\) has a small variance thanks to a control variate effect (Nelson, 1990). Hence, the variance of RT-MLMC is well-controlled, as shown in Lemma 2.

In Lemma 2 below, we will analyze the bias and variance as well as the sampling and computational costs of the RT-MLMC gradient estimator. We will see that it requires only \((1)\) samples to ensure that the bias drops to \(()\). This is in stark contrast to the DL-SGD estimator, which needs \((^{-2})\) samples. The lower sample complexity and the corresponding lower computational cost come at the expense of an increased variance of the order \(((^{-1}))\). The construction of the RT-MLMC gradient estimator is detailed in Algorithm 3. From now on, we refer to Algorithm 2 with \(v(x)=(x)\) as the RT-MLMC algorithm.

```
1:\(\) of iterations \(T\), stepsizes \(\{_{t}\}_{t=1}^{T}\), initial iterate \(x_{1}\).
2:for\(t=1\) to \(T\)do
3: Construct a gradient estimator \(v(x_{t})\) and update \(x_{t+1}=x_{t}-_{t}v(x_{t})\).
4:endfor
5:\(_{T}\) uniformly sampled from \(\{x_{1},...,x_{T}\}\). ```

**Algorithm 2** SGD Framework

### Memory and Arithmetic Operational Costs

The per-iteration memory and arithmetic operational cost of DL-SGD as well as RT-MLMC is dominated by the cost of computing the matrix-vector product

\[(x,y;):=_{12}^{2}g(x,y;^{},)(x,y;)_{2}f(x,y;^{},).\] (8)

By (5), \((x,y;)\) is a product of \(\) matrices of the form \(I-1/(2L_{g,1})_{22}g(x,y;_{n},)\), and the \(n\)-th matrix coincides with the gradient of \((y-1/(2L_{g,1})_{2}g(x,y;_{n},))\) with respect to \(y\). We can thus compute (8) recursively as follows. We first set \(v=_{2}f(x,y;^{},)\). Next, we update \(v\) by setting it to the gradient of \((y-1/(2L_{g,1})_{2}g(x,y;_{}},))^{}v\) with respect to \(y\), which is computed via automatic differentiation. This yields \(v=(I-1/(2L_{g,1})_{22}g(x,y;_{}},))_{2}f(x,y ;^{},)\). By using a backward recursion with respect to \(n\), we can continue to multiply \(v\) from the left with the other matrices in the expansion of \((x,y;)\). This procedure is highly efficient because the memory and arithmetic operational cost of computing the product of a Hessian matrix with a constant vector via automatic differentiation is bounded--up to a universal constant--by the cost of computing the gradient of the same function (Rajeswaran et al., 2019). See Algorithm 4 in Appendix C for details. The expected arithmetic operational costs of Algorithm 4 is \((Nd)\) and the memory cost is \((d)\).

```
0: Iterate \(x\), largest epoch number \(K\), initialization \(y_{1}^{0}\), approximation number \(N\)
1: Sample \(\) from \(_{}\) and sample \(\) uniformly from \(\{0,,N-1\}\).
2: Sample \(\) from the truncated geometric distribution \(_{}\).
3: Run EpochSGD\((,x,,y_{1}^{0})\) and obtain \(y_{+1}^{0}\), \(y_{}^{0}\), and \(y_{1}^{0}\).
4: Construct \(^{+1}(x)\), \(^{}(x)\), and \(^{1}(x)\) according to (6) and compute \[(x)=^{1}(x)+p_{}^{-1}(^{ {k}+1}(x)-^{}(x)).\] Output:\((x)\). ```

**Algorithm 3** RT-MLMC Gradient Estimator for Conditional Bilevel Optimization

## 3 Complexity Bounds

In this section we derive the sample and gradient complexities of the proposed algorithms. We first analyze the error of the general SGD framework detailed in Algorithm 2.

**Lemma 1** (Error Analysis of Algorithm 2).: _If Algorithm 2 is used to minimize an \(L_{}\)-Lipschitz continuous and \(S_{}\)-Lipschitz smooth fucention \((x)\) and if \( 1/(2S_{})\), then we have_

\[\|(_{T})\|^{2}\ \ }{ T}\,+\, _{t=1}^{T}L_{}\|[v(x_{t})\,-\, (x_{t})]\|\,+\,S_{}\|v(x_{t})\,-\,(x_{t})\|^{2} ,\]

_where \(A_{1}:=(x_{1})-_{x}(x)\)._

Lemma 1 sightly generalizes (Rakhlin et al., 2012; Ghadimi et al., 2016; Bottou et al., 2018). We defer the proof to Appendix A. Thus, to prove convergence to a stationary point, we need to characterize the bias, variance, and computational costs of the DL-SGD and the RT-MLMC gradient estimators.

**Lemma 2** (Bias, Variance, Sampling Cost and Computational Cost).: _We have the following results._

* _The biases of the DL-SGD and RT-MLMC estimators match and satisfy_ \[\|^{K}(x)- F(x)\|=\|(x)-  F(x)\|_{g}^{-1}(1-_{g}/(2L_{g,1}))^{N}+(N^{2}2^{ -K/2}),\] _and the corresponding variances satisfy_ \((^{K}(x))=(N^{2})\) _and_ \(((x))=(KN^{4})\)_._
* _The numbers of samples and iterations needed by EpochSGD to build a DL-SGD estimator are bounded by_ \(N+2^{K+1}-1\) _and_ \(2^{K+1}-1\)_, respectively. The expected numbers of samples and iterations needed for an RT-MLMC estimator are bounded by_ \(N+3K\) _and_ \(3K\)_, respectively._

Lemma 2 implies that setting \(N=((^{-1}))\) and \(K=((^{-1}))\) reduces the bias to \(()\). In this case the RT-MLMC estimators have higher variances than the DL-SGD estimators, but their variances are still of the order \(((^{-1}))\). On the other hand, using RT-MLMC estimators reduces the per-iteration sampling and computational costs from \((2^{K})=(^{-2})\) to \((K)=((^{-1}))\).

Note that Hu et al. (2021) characterize the properties of general MLMC estimators and derive their complexity bounds. However, the proposed RT-MLMC estimators for CSBO problems are the first of their kind. In addition, as we need to estimate the Hessian inverse \((x,y^{*}(x;);)\), our analysis is more involved. In contrast to Asi et al. (2021), who use MLMC techniques for estimating projections and proximal points, we use MLMC techniques for estimating gradients in bilevel optimization. The following main theorem summarizes our complexity bounds.

**Theorem 1** (Complexity Bounds).: _If Assumption 2.1 holds, then Algorithm 2 based on the RT-MLMC or the DL-SGD estimator outputs an \(\)-stationary point of \(F\) provided that \(N=((^{-1}))\), \(K=((^{-1}))\), \(=(^{2})\) and \(T=(^{-4})\). When using the RT-MLMC estimator, the sample complexities of \(\) and \(\) as well as the gradient complexities of \(g\) and \(f\) are \(}(^{-4})\). When using the DL-SGD estimator, the sample complexity of \(\) and the gradient complexity of \(g\) are \(}(^{-6})\), while and the sample complexity of \(\) and the gradient complexity of \(f\) are \(}(^{-4})\)._

**Remark.** Theorem 1 asserts that the sample complexity of \(\) and the gradient complexity of \(g\) are much smaller for RT-MLMC than for DL-SGD, while the gradient complexities of \(f\) are comparable. When specialized to SBO or CSO problems, the complexity bounds of RT-MLMC match those of the state-of-the-art algorithms ALEST for SBO problems (Chen et al., 2021) and MLMC-based methods for CSO problems (Hu et al., 2021). When restricted to classical stochastic nonconvex optimization, the complexity bounds of RT-MLMC match the existing lower bounds (Arjevani et al., 2023). These observations further highlight the effectiveness of RT-MLMC across various settings.

## 4 Applications and Numerical Experiments

### Meta-Learning

Optimization-based meta-learning (Finn et al., 2017; Rajeswaran et al., 2019) aims to find a common shared regularization parameter for multiple similar yet different machine learning tasks in order to avoid overfitting when training each task separately on their datasets that each only processes a few data points. Recall Equation (4), the objective function of the optimization-based meta-learning (Rajeswaran et al., 2019),

\[&_{x}\ _{i}_{D_{i}^{ test}_{i}}\ l_{i}(y_{i}^{*}(x),D_{i}^{test})\\ &\ {y_{i}^{*}(x)}=_{y_{i}}\ _{D_{i}^{ train}_{i}}l_{i}(y_{i},D_{i}^{train})+\|y_{i}-x\|^{2} , i[M]\ \ x.\] (9)

where \(\) is the distribution over all \(M\) tasks, \(_{i}\) is the distribution of data from the task \(i\), \(D_{i}^{train}\) and \(D_{i}^{test}\) are the training and testing dataset of the task \(i\), \(x\) is the shared parameter of all tasks and \(y_{i}^{*}(x)\) is the best parameter for a regularized objective of task \(i\), \(l_{i}\) is a loss function that measures the average loss on the dataset of the \(i\)-th task, and \(\) is the regularization parameter to ensure the optimal solution obtained from the lower-level problem is not too far from the shared parameter obtained from the upper-level problem. Note that such a problem also occurs in personalized federated learning with each lower level being one user.

Note that the task distribution \(\) is usually replaced by averaging over all \(M\) tasks. In such cases, existing works (Guo and Yang, 2021; Rajeswaran et al., 2019) only demonstrate a convergence rate that scales linearly with the number of tasks \(M\). In contrast, the sample complexity of our proposed method does not depend on the number of tasks \(M\), enabling substantially faster computation for a larger \(M\). The seminal work, Model-agnostic Meta-learning (MAML) (Finn et al., 2017), is an approximation of Problem (9) via replacing \(y_{i}^{*}(x)\) with one-step gradient update, i.e., \(_{i}(x):=x-^{-1} l_{i}(x,D_{i}^{train})\).

We study the case where the loss function \(l_{i}(x,D), i\) is a multi-class logistic loss using a linear classifier. The experiment is examined on tinyImageNet (Mnmoustafa, 2017) by pre-processing

Figure 1: Test error of DL-SGD (left figure), RT-MLMC (right figure), and MAML against upper-level iterations on meta-learning. \(K\) represents how accurately we solve the lower-level problem.

it using the pre-trained ResNet-18 network (He et al., 2016) to extract linear features. Since the network has learned a rich set of hierarchical features from the ImageNet dataset (Deng et al., 2009), it typically extracts useful features for other image datasets.

Figure 1 presents the average of logistic loss evaluated on the test dataset against the number of iterations, with each iteration representing one upper-level update. From the plot, we see that both DL-SGD and RT-MLMC methods tend to have better generalization performance when using a larger number of levels \(K\). As shown in Table 2, RT-MLMC is about \(9\) times faster to compute the upper-level gradient estimator than DL-SGD when \(K\) is large.

In contrast, the MAML baseline does not have superior performance since the one-step gradient update does not solve the lower-level problem to approximate global optimality. In Appendix D.1, we provide numerical results for a modified MAML algorithm with multiple gradient updates, which achieves better performance compared to MAML but is still worse than our proposed method.

After the initial submission of the paper, a con-current work Hu et al. (2023) proposed two types of algorithms (\(^{}\) and \(^{}\)) that apply to the meta-learning formulation (9). Their proposed algorithm \(^{}\) is computationally expansive as it requires the exact computation of the inverse of Hessian matrix (which is of size \(5120 5120\) in this example) with respect to \(\) in each iteration of the upper-level update. In the following, we compare the performance of our algorithm with their proposed Hessian-free algorithm \(^{}\) in Figure 2.

In the left plot of Figure 2, we examine the performance of RT-MLMC method and \(^{}\) by running the same number of total epochs. It shows that RT-MLMC method has much better performance in terms of test error. In the right plot of Figure 2, we examine the performance of RT-MLMC method and \(^{}\) by running the same amount of computational time. Although the per-upper-level-iteration computational costs of \(^{}\) is small, it takes a much longer time for \(^{}\) to achieve a similar test error as RT-MLMC.

### Wasserstein DRO with Side Information

The WDRO-SI (Yang et al., 2022) studies robust stochastic optimization with side information (Bertsimas and Kallus, 2020). Let \(\) denote the side information and \(\) denote the randomness dependent on \(\). The WDRO-SI seeks to find a parameterized mapping \(f(x;)\) from the side information \(\) to a decision \(w\) that minimizes the expected loss w.r.t. \(\) under the worst-case distributional shifts over \((,)\). Rigorously, with a penalty on the distributional robust constraint, WDRO-SI admits the form

\[_{x}\ \ _{}\ _{(,)} [l(f(x,);)]-(,^{}) },\] (10)

    &  &  \\   & Mean & Variance & Mean & Variance \\ 
6 & **2.65e-02** & 6.34e-03 & 2.73e-02 & 1.46e-02 \\
8 & 7.23e-02 & 7.77e-03 & **3.41e-02** & 1.85e-02 \\
10 & 2.48e-01 & 2.75e-02 & **4.93e-02** & 4.06e-02 \\
12 & 9.38e-01 & 3.71e-02 & **1.08e-01** & 5.44e-02 \\   

Table 2: The computation time of DL-SGD/RT-MLMC gradient estimators on meta-learning.

Figure 2: Left: Performance of our proposed RT-MLMC algorithm and \(^{}\) against upper-level iterations on meta-learning. Right: Performance of algorithms against the total computational time on meta-learning.

where \(l(w;)\) is the loss function dependent on the decision \(w\) and the random variable \(\), \(^{0}\) is the nominal distribution of \((,)\) that usually takes the form of an empirical distribution, and \((,)\) is a casual transport distance between distributions (Yang et al., 2022, Definition 1) - a variant of the Wasserstein distance that better captures the information from \(\). For distributionally robust feature-based newsvendor problems (Zhang et al., 2023), the covariate \(\) can be temporal, spatial, or weather information, \(\) is the random demand, \(f(x;)\) denotes the ordering quantity for a given \(\), and \(l(f(x;);)\) characterizes the loss if the ordering quantity \(f(x;)\) does not match the demand \(\).

Incorporating the cost function of the casual transport distance used in (Yang et al., 2022) and utilizing the dual form, the WDRO-SI problem in (10) can be reformulated as a special case of CSBO:

\[_{x}\ _{^{0}_{}}_{ ^{0}_{|}}[l(f(x;y^{*}(x;)),)-\|y^{*}(x ;)-\|^{2}]\] (upper level) \[\ y^{*}(x;):=_{} \ _{^{0}_{|}}-l(f(x;), )+\|-\|^{2},\ \ \ x.\] (lower level)

The original work (Yang et al., 2022) only allows affine function \(f\) or non-parametric approximation, while our approach allows using neural network approximation such that \(f(x;)\) is a neural network parameterized by \(x\). Using Theorem 1, we obtain the first sample and gradient complexities for WDRO-SI. For the distributionally robust feature-based newsvendor problems, we compare the performance of DL-SGD and RT-MLMC. We compare with ERM and WDRO, which do not incorporate side information.

Fig. 3 (left) and (middle) present the results of test loss versus the number of upper-level iterations for DL-SGD and RT-MLMC, respectively. From the plot, using a larger number of epochs \(K\) for the lower-level problem generally admits lower testing loss values, i.e., better generalization performance. Fig. 3 (right) highlights the importance of incorporating side information as the performance of WDRO-SI outperforms the other two baselines. In addition, more observations of \(\) for a given side information \(\) can enhance the performance. Table 3 reports the computational time for DL-SGD and RT-MLMC gradient estimators, and RT-MLMC is significantly faster since it properly balances the bias-variance-computation trade-off for the gradient simulation.

## 5 Conclusion

We introduced the class of contextual stochastic bilevel optimization problems, which involve a contextual stochastic optimization problem at the lower level. In addition, we designed efficient gradient-based solution schemes and analyzed their sample and gradient complexities. Numerical results on two complementary applications showcase the expressiveness of the proposed problem class as well as the efficiency of the proposed algorithms. Future research should address generalized CSBO problems with constraints at the lower level, which will require alternative gradient estimators.

Figure 3: Test error on WDRO-SI against the number of upper-level updates for DL-SGD (left) and RT-MLMC (middle). Figure (Right) compares WDRO-SI with ERM and Wasserstein DRO that do not incorporate side information. \(m\) means the number of samples of \(Z\) generated from \(_{Z|X}\) for each realization of \(X\).

    &  &  \\   & Mean & Variance & Mean & Variance \\ 
2 & 1.27e-02 & 2.67e-03 & **5.04e-03** & 7.26e-04 \\
4 & 5.25e-02 & 2.58e-03 & **1.25e-02** & 8.26e-03 \\
6 & 1.68e-01 & 2.74e-03 & **2.02e-02** & 9.39e-03 \\
8 & 4.63e-01 & 2.08e-03 & **3.41e-02** & 1.68e-02 \\   

Table 3: Computation time of DL-SGD/RT-MLMC gradient estimators for WDRO-SI.