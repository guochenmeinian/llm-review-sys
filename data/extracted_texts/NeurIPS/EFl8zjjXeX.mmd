# OV-PARTS: Towards Open-Vocabulary

Part Segmentation

Meng Wei \({}^{1,2}\)   Xiaoyu Yue \({}^{3}\)   Wenwei Zhang \({}^{1}\)   Shu Kong \({}^{4,5}\)

**Xihui Liu \({}^{2}\)   Jiangmiao Pang \({}^{1}\)1\({}^{1}\)Shanghai AI Laboratory \({}^{2}\)The University of Hong Kong \({}^{3}\)The University of Sydney \({}^{4}\)University of Macau \({}^{5}\)Texas A&M University mengwei.kelly@connect.hku.hk yuexiaoyu002@gmail.com skong@um.edu.mo xihuiliu@eee.hku.hk {zhangwenwei, pangjiangmiao}@pjlab.org.cn**

###### Abstract

Segmenting and recognizing diverse object parts is a crucial ability in applications spanning various computer vision and robotic tasks. While significant progress has been made in object-level Open-Vocabulary Semantic Segmentation (OVSS), _i.e._, segmenting objects with arbitrary text, the corresponding part-level research poses additional challenges. Firstly, part segmentation inherently involves intricate boundaries, while limited annotated data compounds the challenge. Secondly, part segmentation introduces an open granularity challenge due to the diverse and often ambiguous definitions of parts in the open world. Furthermore, the large-scale vision and language models, which play a key role in the open vocabulary setting, struggle to recognize parts as effectively as objects. To comprehensively investigate and tackle these challenges, we propose an **Open-Vocabulary Part Segmentation (OV-PARTS)** benchmark. OV-PARTS includes refined versions of two publicly available datasets: Pascal-Part-116 and ADE20K-Part-234. And it covers three specific tasks: _Generalized Zero-Shot Part Segmentation_, _Cross-Dataset Part Segmentation_, and _Few-Shot Part Segmentation_, providing insights into analogical reasoning, open granularity and few-shot adapting abilities of models. Moreover, we analyze and adapt two prevailing paradigms of existing object-level OVSS methods for OV-PARTS. Extensive experimental analysis is conducted to inspire future research in leveraging foundational models for OV-PARTS. The code and dataset are available at [https://github.com/OpenRobotLab/OV_PARTS](https://github.com/OpenRobotLab/OV_PARTS).

Segmenting and recognizing diverse object parts is a crucial ability in applications spanning various computer vision and robotic tasks. While significant progress has been made in object-level Open-Vocabulary Semantic Segmentation (OVSS), _i.e._, segmenting objects with arbitrary text, the corresponding part-level research poses additional challenges. Firstly, part segmentation inherently involves intricate boundaries, while limited annotated data compounds the challenge. Secondly, part segmentation introduces an open granularity challenge due to the diverse and often ambiguous definitions of parts in the open world. Furthermore, the large-scale vision and language models, which play a key role in the open vocabulary setting, struggle to recognize parts as effectively as objects. To comprehensively investigate and tackle these challenges, we propose an **Open-Vocabulary Part Segmentation (OV-PARTS)** benchmark. OV-PARTS includes refined versions of two publicly available datasets: Pascal-Part-116 and ADE20K-Part-234. And it covers three specific tasks: _Generalized Zero-Shot Part Segmentation_, _Cross-Dataset Part Segmentation_, and _Few-Shot Part Segmentation_, providing insights into analogical reasoning, open granularity and few-shot adapting abilities of models. Moreover, we analyze and adapt two prevailing paradigms of existing object-level OVSS methods for OV-PARTS. Extensive experimental analysis is conducted to inspire future research in leveraging foundational models for OV-PARTS. The code and dataset are available at [https://github.com/OpenRobotLab/OV_PARTS](https://github.com/OpenRobotLab/OV_PARTS).

Figure 1: An illustration of the Generalized Zero-Shot Setting (red arrow), Cross-Dataset Setting (blue arrow) and Few-Shot Setting with examples from Pascal-Part-116 and ADE20K-234.

Introduction

The ability to identify and reason about object parts is crucial for a wide range of human activities. For instance, when preparing a meal, we rely on specific parts of utensils such as the blade of a knife for slicing and the handle of a spatula for stirring. Hence, developing a vision system capable of part-level object segmentation is crucial and offers substantial benefits across applications in vision and robotics such as image editing , object manipulation _etc_. Despite dedicated efforts in annotating fine-grained parts by previous works , the complex nature and diverse granularity of object parts make it hard to create a comprehensive closed category set. Recently, the research on Open Vocabulary Semantic Segmentation (OVSS)  extends the image-text alignment ability of large-scale Vision-Language Models (VLM) like CLIP  to pixel-level prediction, which shows remarkable performance in open vocabulary object segmentation.

Despite the satisfying performance on object-level OVSS, part-level OVSS raises additional challenges. Parts exhibit complex structures, often with more intricate boundaries and appearance variations than objects. However, the available labeled data for training part segmentation models is significantly limited compared to that of objects. Moreover, in the open-vocabulary setting, there are further challenges to overcome. Firstly, while objects are typically well-defined entities with strict boundaries, the granularity of parts can be flexible, posing the extra open granularity challenge that rarely occurs in object-level OVSS. Secondly, the widely used large-scale VLM are mainly pretrained on natural image-text pairs which are inherently biased to object words. Hence, their ability to recognize object parts can be less proficient. This disparity is reflected in the less discriminative class activation maps of CLIP  for parts as opposed to objects, as illustrated in Figure 2 (a).

Considering these challenges, we propose to break down the complex part-level OVSS problem into specific subtasks. One key observation is, common objects often exhibit shared characteristics in terms of parts. For example, furniture like chair, table and sofa often have shared parts such as legs, seats, and backrets. Humans can categorize a new object based on its part descriptions related to a known object. Similarly, we can leverage this **analogical reasoning** ability to improve data efficiency in part-level OVSS. Hence, we propose a _Generalized Zero-Shot Part Segmentation_ setting, focusing on assessing the transferability of part segmentation from seen objects to related unseen objects. Additionally, we extend the related _Cross-Dataset Segmentation_ setting, commonly used in object-level OVSS, to part-level OVSS. This setting further emphasizes the **open granularity** challenge, due to the varying part vocabularies and granularity levels across datasets, as shown in Figure 1. Finally, concerning the weaker transferability of large-scale foundation models for the part, we further expect a _Few-Shot Part Segmentation_ setting to enable **fast adaptation** of the foundation models to part-level OVSS. The entire benchmark, named **O**pen-**V**ocabulary **Part** Segmentation (**OV-PARTS**), is further supplemented with carefully cleaned and reorganized versions of Pascal-Part  and ADE20K-Part , namely Pascal-Part-116 and ADE20K-Part-234.

Additionally, we design strong baselines for OV-PARTS based on two paradigms (_i.e._, two-stage and one-stage) of existing object-level OVSS methods. The first paradigm  designs a two-stage approach that decouples the segmentation and open vocabulary classification abilities. Applying this paradigm to OV-PARTS involves training a class-agnostic part proposal model followed by using CLIP for part region classification. However, it turns out that treating object part as independent classes like objects lead to suboptimal performance. Indeed, even humans struggle to distinguish between "cow's leg" and "sheep's leg" if only the region of their legs is shown. To mitigate this problem, we propose two improvements: (1) An **Object Mask Prompt** strategy which introduces the object-awareness to the first part proposal stage. (2) A **Compositional Prompt Tuning** strategy which not only enhances object awareness in the second stage but also shifts CLIP's attention from objects to object parts. This two-stage paradigm offers the advantage of combining class-agnostic part parsing models  and various finetune methods to adapt foundation models for classification . However, using a class-agnostic part proposal model has limitations. It is trained on pre-defined parts, which is not applicable to the open granularity scenario. Moreover, the mask proposal model is inclined to overfit the training data, leading to reduced zero-shot generalization ability.

The other line of works  follows a one-stage paradigm that trains a unified open-vocabulary segmentation model based on CLIP, eliminating the missing object context problem. As for the open granularity ability, we experimentally find that CATSeg  and CLIPSeg , which are pretrained on COCO-Stuff and PhraseCut  respectively, can already achieve the first-level granular generalization from object to some simple parts as shown in Figure 2 (b). Motivated by this,we design some one-stage baselines by finetuning specific modules of CLIPSeg and also investigate parameter-efficient finetuning strategies  for OV-PARTS. However, the segmentation ability of one-stage baselines is weaker than the two-stage baselines, mainly limited by the frozen CLIP visual encoder which is pretrained without sufficient segmentation data.

In summary, the two-stage and one-stage baselines exhibit complementary strengths and weaknesses. But there are no currently clear solutions about how to achieve both high-quality segmentation and strong open vocabulary and granularity generalization ability in OV-PARTS. Hence, there remains much to uncover in addressing OV-PARTS, particularly in unlocking the full potential of large foundation models in vision, language and multi-modality, which can be the value of the benchmark.

## 2 Related Work

**Open Vocabulary Semantic Segmentation.** Open vocabulary semantic segmentation has recently achieved significant progress with the help of large-scale Vision-Language Models (VLM) like CLIP , which excel at recognizing objects in images. One line of research [9; 33; 32] combines powerful segmentation models like MaskFormer  with CLIP in a two-stage way. In the first stage, MaskFormer  produces class-agnostic object mask proposals. In the second stage, CLIP  classifies the image regions of these proposals. ODISE  further used a pretrained diffusion model to enhance the proposal stage. Another one-stage approach focuses on extending CLIP  to pixel-level prediction [7; 22]. They mainly train a pixel decoder on top of the CLIP image encoder. CLIPSeg  adds a lightweight transformer-based pixel decoder with a FiLM module to fuse the multi-modality features. CATSeg  designs a spatial and class aggregation network with multi-modality guidance features for effective open vocabulary pixel classification.

**Part Segmentation.** Fine-grained part segmentation has been actively studied in the literature [17; 8; 39; 24; 23; 29; 25; 28]. Most of these methods [17; 8; 39; 24; 23] adopt a supervised closed-set setting. Tang et al.  designs a supervised language-driven segmentation model which allows interactive whole-to-part segmentation. Recently, Pan et al.  proposes an open-world part segmentation setting that only focuses on the class-agnostic part mask generation which is not language-driven. The concurrent work Sun et al.  tackles the open vocabulary part segmentation task with a cross-dataset setting. However, the big performance gap compared to open vocabulary object segmentation hasn't been well studied. Also, their proposed method emphasizes building semantic correspondences based on visual features. But the role of language in the open vocabulary setting as well as the potential of vision language models has not been adequately discussed. Moreover, some existing open-vocabulary object detection models [16; 18; 20] can also detect parts with bounding boxes. Because they are trained on crowd-sourced datasets in which the texts can also include part words. However, pixel-level understanding is more conforming to the nature of part which is ambiguous, multi-granular and has intricate boundaries. For example, with bounding boxes, it'll be hard to identify a little bird's parts accurately.

Figure 2: (a) Class activation maps of CLIP. The object prompt “Sheep” can activate its area while the part prompt “Head” fails. When using “Sheep’s Head” as prompt, the activation is still biased to “Sheep”. (b) Without finetuning on part datasets, CATSeg outputs rough part masks of “Bird” and “Sheep” (**Left**) and CLIPSeg produces finer granular part masks of “Bird” and “Dog” (**Right**.).

Dataset and Benchmark Details

In this section, we first introduce the two proposed datasets Pascal-Part-116 and ADE20K-Part-234 in section 3.1. Then we elaborate on the three task settings designed for **OV-PARTS** in section 3.2. The exhaustive list of object part classes, the specific data splits and the distribution of part scales and numbers of Pascal-Part-116 and ADE20K-Part-234 are left to the supplementary material.

### Datasets

**Pascal-Part-116.** Pascal-Part dataset  is an extension of the PASCAL VOC 2010 dataset , further annotating objects' part masks. Some categories such as "cow", are annotated with a comprehensive list of parts, while others like "chair", "boat", and "dining table" only provide silhouette annotations. Moreover, the part definition includes directional terms like "left," "right," "front," "back," "upper," and "lower", such as "cow's left front lower leg". However, in an open-vocabulary setting, it's unnecessary to discern between the semantics of labels such as "cow's left front lower leg" and "cow's right back lower leg". They can not only create a bottleneck in part segmentation but also cause overfitting which hinders effective language-driven generalization. Hence, we have manually merged some of the over-segmentation parts to create a more practical version. Our revised Pascal-Part dataset  includes a total of 116 object part classes across 17 object classes, which is the most extensive set among various versions of Pascal-Part dataset[30; 21; 3; 24].

**ADE20K-Part-234.** The ADE20K dataset  provides open-ended annotations of 847 objects and 1000+ parts, following the WordNet hierarchy. It covers a broad range of scenes, including indoor spaces such as "bedrooms", and outdoor spaces like "streetscapes". However, the part annotations in ADE20K are extremely sparse and incomplete (less than \(15\%\) object instances have part annotations), which poses significant challenges for both training models and evaluating their performance. Despite attempts to reorganize the dataset [23; 29], either the revised versions have not been publicly released or they still contain considerable noise. To get a clean version, we started with the widely used SceneParse150  subset and only keep the objects which have more than one frequently annotated part (over 100 occurrences) and then filter the rare parts (less than \(10\) occurrences). Moreover, we manually merge some duplicated parts such as "chair arm" and "chair armrest", "table stretcher" and "table h-stretcher" as well as the over-segmentation parts. The resulting subset consists of 44 objects and 234 parts, providing a cleaner dataset for improved analysis and evaluation.

### Benchmark Tasks

There are two primary challenges : (1) The available pixel-level part data is limited. (2) Pretrained features from large-scale VLM exhibits weaker transferability to parts. To evaluate the OV-PARTS models comprehensively, we have designed three task settings: Generalized Zero-Shot Part Segmentation, Cross-Dataset Part Segmentation, and Few-Shot Part Segmentation.

**Generalized Zero-Shot Part Segmentation.** Considering the limited ability of VLMs to recognize parts, this task aims to assess the model's analogical reasoning ability, which is designed by selecting novel objects that possess related parts to the base objects, rather than being completely irrelevant. _Data Split_. To split the object classes in each dataset, we group the object classes into higher-level categories (e.g. Animals, Vehicles) based on their shared attributes. Within each hyper-category, we split the objects into base and novel classes (74/42 for Pascal-Part-116 and 176/58 for ADE20K-Part-234). The unseen objects in the training set are set to the background. In this way, a novel object part class may be novel at the object level (_e.g._, "dog's head" is a novel class while "cat's head" is a base class) or both at the object level and the part level (_e.g._, "bird's's peak"). The complete base and novel class set can be found in the supplementary material.

_Evaluation Protocol_. Following previous OVSS methods [33; 9], we first calculate the mean class-wise Intersection over Union (mIoU) on both base and novel classes. Then, to provide a balanced assessment of the model's performance across both base and novel classes in this setting, we use harmonic mean IoU (hIoU) as the primary metric.

**Cross-Dataset Part Segmentation.** In object-level OVSS, the cross-dataset setting evaluates the model's ability to handle variations in data distribution and novel object vocabulary. While in OV-PARTS, considering diverse part definitions, the model further needs to generalize between different annotation granularity levels in addition to different vocabularies. For example, the part set of "car" is ["wheel", "headlight", "license plate", "mirror", "door", "window", "side", "front", "back", "roof"] in Pascal-Part-116 while is ["bumper", "door", "headlight", "hood", "license plate", "logo", "mirror", "wheel", "window", "wiper"] in ADE20K-Part-234 which has a finer granularity.

_Data Split._ Overall, ADE20K-234 covers a more diverse set of objects than Pascal-Part-116. We train models on the full training set of ADE20K-234 and evaluate them on Pascal-Part-116's testing set.

_Evaluation Protocol_. We reported mIoU on both the source dataset (ADE20K-Part-234) and target dataset (Pascal-Part-116).

**Few-Shot Part Segmentation.** Indeed, there's still a big performance gap between OV-PARTS models compared to the object-level OVSS models, mainly due to the inherent task difficulty and data limitation. Inspired by recent advances in foundation models, we design this few-shot part segmentation setting mainly to explore the strategies to adapt large-scale foundation models effectively for OV-PARTS and investigate the feasibility of utilizing object-level data to achieve parameter-efficient training and data efficiency.

_Data Split._ We sample 16 images for each object class. Since there may be multiple objects in an image or each object may not have exhaustive part annotations, the exact number of sampled shots for each object part class may be slightly over or below 16.

_Evaluation Protocol_. We use mIoU as the main metric.

## 4 Methodology

Existing object-level Open Vocabulary Semantic Segmentation (OVSS) methods can be categorized into the two-stage and one-stage paradigms. These paradigms differ in the way to apply the CLIP model to the segmentation process. In this section, we will give a brief overview of the framework for each paradigm. We then introduce our modifications with insights into the baselines.

### Two-Stage Baseline

For the two-stage framework (Figure 3 (a)), we select ZSSeg  as the baseline, which includes a proposal generation stage and a zero-shot prediction stage. The proposal generation stage mainly includes a MaskFormer . Given an input image \(\), a backbone network extracts the image features \(\). Then a pixel decoder is used to get the mask features \(\) and a query-based transformer decoder is used to get the query features \(\) with learnable queries \(\). Finally, \(\) is fed into a mask predictor to get the class-agnostic masks \(\) by computing the dot product of \(S\) and \(F\) and a fixed classifier

Figure 3: The overall frameworks of the representative two-stage (ZSseg+) and one-stage (CLIPSeg) baselines. (a) is the modified proposal generation stage of ZSseg. (b) shows the design of Compositional Prompt Tuning based on CoOp. (c) is the CLIPSeg model architecture which also indicates the modules that can be finetuned. CLIP is fixed in both frameworks.

(initialized from CLIP text embeddings) to compute the classification score \(C_{P}\):

\[S=TransformerDecoder(Q;Z) \]

\[M=MaskPredictor(S;F),C_{P}=Classifier(S) \]

In the zero-shot prediction stage, the mask proposals \(\) are used to crop the original image \(\). The resulting masked image is then fed into the CLIP model for zero-shot class prediction. The final classification scores are obtained through an ensemble of both stages:

\[C_{C}=CLIP(I;M),C=Ensemble(C_{P};C_{C}) \]

Indeed, directly transferring the zsseg method from object-level data to part-level data has limitations and can lead to suboptimal performance. Unlike objects, which exist independently, parts are always contextually dependent on their respective objects. Classifying part regions without considering their object context is challenging. To address this, we propose two approaches to incorporate object context into both stages:

**Object Mask Prompt.** In the first stage, we propose to incorporate object masks as a mask prompt which can be obtained in a more cost-effective manner. This decoupling of object masks from the part segmentation brings object awareness to the model and helps resolve the difficulty. Furthermore, it enables interactive segmentation applications such as image editing.

In detail, as shown in Figure 3 (a), we modify the transformer decoder by replacing the cross-attention layers with mask-guided cross-attention layers . This ensures that the cross-attention mechanism is computed only within the regions corresponding to objects, resulting in more accurate and context-aware part proposals. Equation (1) thus becomes:

\[S=MaskedTransformerDecoder(Q;Z;M_{o}) \]

where \(M_{o}\) indicates the object mask. We further find that the proposals of parts are very noisy compared to objects. Hence, we design a mask-denoising technique during testing. Instead of directly using \(\) for the second stage, we aggregate \(^{q h w}\) and \(C_{P}^{q c}\) to get a refined set of part mask proposals:

\[C_{C}=CLIP(I;Binary(_{q}(C_{P} M)^{c h w})) \]

Here, \(q,c,h\) and \(w\) represent the number of learnable queries, the number of classes, the image height and the image width respectively. In the \(Binary\) operation, we process the semantic mask into \(c\) binary masks.

**Compositional Prompt Tuning.** In the second stage, to ensure that CLIP is not confused when presented with only partial regions of objects, we adapt CLIP's vision encoder following MaskCLIP  to obtain dense image embedding instead of the global image embedding. Then we use mask pooling to get CLIP visual features for both the object mask and the part mask. Since we observed that objects tend to capture most of CLIP's attention, overshadowing the parts, we design a compositional prompt tuning method based on CoOp . We add object tokens and part tokens for prompt tuning with a learnable fusion weight. The modified zero-shot prediction stage is shown in Figure 3 (b). We find that this prompt tuning strategy is effective with minimal data and training cost.

### One-Stage Baseline

In the one-stage framework, we employ CLIPSeg  as our baseline. The overall framework of CLIPSeg is shown in Figure 3 (c). CLIPSeg adds a parameter-efficient three-layered transformer decoder to the original CLIP model for segmentation. It integrates visual features from the final layer of the visual encoder and text features of all object part prompts from the text encoder through the FiLM module, forming cross-modal input token embeddings for the decoder. Furthermore, the features extracted from the 3rd, 6th, and 9th layers of CLIP's visual encoder are projected and added to the intermediate features of the corresponding decoder layers. It's worth noting that, the visual features extracted from the frozen CLIP visual encoder first pass through the added visual adapter, which consists of a two-layered MLP, before reaching the decoder. CLIPSeg is originally designed to generate binary segmentation maps conditioned on either a visual prompt or a text prompt. So we replace the original binary segmentation head with a multi-class one and modify its loss for multi-class segmentation using only text prompts.

By directly applying CLIP to the pixel-level prediction, we find that CLIPSeg, trained on PhraseCut  with object phrases as text input, shows amazing zero-shot generalization to parts. From this finding and considering the data sparsity issue in part segmentation, our focus shifts to exploring parameter-efficient finetuning for the one-stage baseline. Finetuning the entire CLIP model has proven to be harmful to its generalization ability . Hence we conduct experiments to finetune different combinations of the specific modules of CLIPSeg, including the text embedding layer in CLIP, the pixel decoder and the extra light-weight CLIP-Adapter modules  (Vision-Adapter and Text-Adaper are simple MLPs) as shown in Figure 3 (c).

## 5 Experiments

### Experimental Setup

**Evaluation Setup for Three Task Settings**. Since the state-of-the-art object-level OVSS models are powerful, we design two evaluation settings to investigate the primary bottlenecks: (1) **Oracle-Obj Setting**: The ground-truth mask and class of objects are assumed to be known. (2) **Pred-Obj Setting**: The ground-truth mask and class of objects are not provided.

**Implementation Details.** The details of baselines and training are left to the supplementary material.

### Results in Generalized Zero-Shot Part Segmentation

_Settings_. We reported results of both two-stage and one-stage baselines on Pascal-Part-116 and ADE20K-Part-234 datasets as shown in Table 1 and Table 2 respectively. Firstly, to provide a comprehensive comparison, we reported the results of the supervised baseline: **MaskFormer** with ResNet-50 backbone on the seen and unseen classes. Then for the _two-stage baselines_, we reported the results of the original **ZSSeg** and our modified ZSSeg (**ZSSeg+**) with different finetuning methods for the second stage: Compositional Prompt Tuning based on CoOp  and CoCoOp , abbreviated to **CPT-CoOp** and **CPT-CoCoOp**. For the _one-stage baselines_, we examined two state-of-the-art methods: **CAT-Seg** and **CLIP-Seg**. We used the pretrained model of CATSeg trained on COCO-Stuff and CLIPSeg trained on PhraseCut. Then we reported their results without any finetuning, as well as the results when adopting various finetuning strategies. For the Pred-Obj Setting evaluation, we use ZSSeg's pretrained object model on COCO-Stuff in the two-stage baselines while don't adopt separate object models in one-stage baselines.

_Two-Stage Baselines_. We evaluated the effects of the **Object Mask Prompt**, **Mask Denoise**, and **CPTCoOp** in ZSSeg+ on Pascal-Part-116 as shown in Table 3. We gradually add each approach to **ZSSeg** to measure its individual effect under the Oracle-Obj Setting. The results indicate that the

   Model & Backbone & Finetuning &  &  \\  & & &  &  &  &  &  &  \\   \\  MaskFormer  & ResNet-50 & \(\) & 55.28 & 52.14 & - & 53.07 & 47.82 & - \\  \\  ZSSeg  & ResNet-50 & \(\) & 49.35 & 12.57 & 20.04 & 40.80 & 12.07 & 18.63 \\ ZSSeg+ & ResNet-50 & CPTCoOp & 55.33 & 19.17 & 28.48 & 54.23 & 17.10 & 26.00 \\ ZSSeg+ & ResNet-50 & CPTCoCoOp & 54.43 & 19.04 & 28.21 & 53.31 & 16.08 & 24.71 \\ ZSSeg+ & ResNet-101c & CPTCoOp & **57.88** & **21.93** & **31.81** & **56.87** & **20.29** & **29.91** \\   \\  CATSeg  & ResNet-101 & \(\) & 14.89 & 10.29 & 12.17 & 13.65 & 7.73 & 9.87 \\  & \& ViT-B/16 & & & & & & \\ CATSeg  & ResNet-101 & B+D & 43.97 & 26.11 & 32.76 & 41.65 & 26.08 & 32.07 \\  & \& ViT-B/16 & & & & & & \\ CLIPSeg  & ViT-B/16 & \(\) & 22.33 & 19.73 & 20.95 & 14.32 & 10.52 & 12.13 \\ CLIPSeg  & ViT-B/16 & VA+L+F+D & **48.68** & **27.37** & **35.04** & **44.57** & **27.79** & **34.24** \\   

Table 1: Zero-shot performance of the two-stage and one-stage baselines on Pascal-Part-116.

**Object Mask Prompt** improves the performance on unseen classes (\(+1.32\%\)), and adding Mask Denoise further improves it (\(+2.95\%\)). However, there is a slight performance drop (\(-1.35\%\)) on the seen classes. Moreover, finetuning the CLIP embeddings with **CPTcOOp** leads to significant performance gains on both the seen (\(+7.33\%\)) and unseen (\(+2.33\%\)) classes. Notably, the finetuning with CPTCoOp requires only 500 iterations and less than 128 samples per class.

_One-Stage baselines_. Both **CATSeg** and **CLIPSeg** employ a frozen CLIP visual encoder to extract image features. But **CATSeg** further uses an extra learnable visual backbone to guide the pixel decoder. In Table 1 and Table 2, we can see that the **CATSeg** and **CLIPSeg** models, pretrained on object datasets, already show impressive results. Notably, **CLIPSeg** even outperforms ZSseg+ with a ResNet-50 backbone on the unseen classes. The comparison between **CLIPSeg** and **CATSeg** shows that OVSS models trained with phrases of objects exhibit stronger generalization ability to parts than sole objects. Hence, we explore alternative finetuning strategies rather than training from scratch. We finetune three different components: language embedding layer in text encoder (**L**), FiLM (**F**) and Decoder (**D**) of **CLIPSeg** and two added lightweight modules: CLIP-Adapter  to the visual encoder (**VA**) and text encoder (**TA**). As shown in Table 5, we draw three key conclusions: (1) Multi-modality finetuning is better than single-modality (VA+L+FiLM surpasses VA+FiLM and L+FiLM).; (2) Finetuning language embedding performs better than the text adapter (VA+L+F surpasses VA+TA+F); (3) Parameter-efficient finetuning can effectively transfer the knowledge from large-scale foundation models and object-level datasets to part parsing. We can see that VA+L+F even outperforms finetuning the entire pixel decoder on the unseen classes.

The qualitative results, which present a comparison among **ZSseg+**, **CATSeg** and **CLIPSeg** on the unseen class "Bird" of Pascal-Part-116, as well as additional qualitative results on Pascal-Part-116 and ADE20K-Part-234, are available in Section C of the supplementary material.

### Results in Few-Shot Part Segmentation

In the few-shot setting, the two-stage baselines have inherent limitations. Firstly, training the MaskFormer from scratch is prone to overfitting with limited data. Secondly, the class-agnostic proposal generation is unable to generalize from object to part, making it hard to make use of the

   Model & Backbone & Finetuning &  &  \\  & & Seen & Unseen & Harmonic & Seen & Unseen & Harmonic \\   \\  MaskFormer  ResNet-50 & ✗ & 46.25 & 47.86 & - & 35.52 & 16.56 & - \\   \\  ZSseg+ & ResNet-50 & CPTCoOp & 43.19 & **27.84** & **33.85** & 21.30 & **5.60** & **8.87** \\ ZSseg+ & ResNet-50 & CPTCoOp & 39.67 & 25.15 & 30.78 & **19.52** & 2.98 & 5.17 \\ ZSseg+ & ResNet-101c & CPTCoOp & **43.41** & 25.70 & 32.28 & **21.42** & 3.33 & 5.76 \\   \\  CATSeg  & ResNet-101 & ✗ & 11.49 & 8.56 & 9.81 & 6.30 & 3.79 & 4.73 \\  & \& ViT-B/16 & & & & & & \\ CATSeg  & ResNet-101 & B+D & 31.40 & 25.77 & 28.31 & 20.23 & **8.27** & **11.74** \\  & \& ViT-B/16 & & & & & & \\ CLIPSeg  & ViT-B/16 & & 15.27 & 18.01 & 16.53 & 5.00 & 3.36 & 4.02 \\  & \& ViT-B/16 & VA+L+F+D & **38.96** & **29.65** & **33.67** & **24.8** & 6.24 & 9.98 \\   

Table 2: Zero-shot performance of the two-stage and one-stage baselines on ADE20K-Part-234.

    &  \\  & Seen & Unseen & Harmonic \\  ZSseg  & 49.35 & 12.57 & 20.04 \\ +Obj Mask Prompt & 48.00 & 13.89 & 21.54 \\ +Mask Denoise & 48.00 & 16.84 & 24.93 \\ +CPTCoOp & 55.33 & 19.17 & 28.48 \\   

Table 3: Ablations on proposed strategies in ZSseg+ by adding each module to ZSseg on Pascal-Part-116.

object-level data. We reported the results of finetuning various modules in CLIPSeg as shown in Table 5. We can draw similar conclusions regarding the different finetuning strategies as observed in the zero-shot setting results. Besides, we observe inconsistent performance trends on Pascal-Part-116 and ADE20K-Part-234: (1) Visual modality finetuning is superior on Pascal-Part-116 while it is inferior on ADE20K-Part-234 (**L+F** v.s. **VA+F**). (2) Finetuning decoder (**D**) even underperforms the single language modality (**L+F**) on ADE20K-Part-234, whereas the opposite trend is observed on Pascal-Part-116. (3) Finetuning **VA+L+F+D** on ADE20K-Part-234 brings \(0.69\%\) gains to **VA+L+F** under the **Oracle-Obj Setting** but causes \(1.2\%\) performance drops (\(1.2\%\)) under the **Pred-Obj Setting**, which is different from the Pascal-Part-116. The cause of the inconsistencies is that, ADE20K-Part-234 is more challenging than Pascal-Part-116 thus the transferability gap from object to part is larger. Finetuning more parameters may cause worse performance and is biased to parts compared to objects. Qualitative results are left to Section C of the supplementary material.

### Results in Cross-Dataset Part Segmentation

We reported three baselines as shown in Table 4: **CATSeg** with backbone and decoder finetuned, CLIPSeg with Visual Adapter, Language Embedding and FiLM (**VA+L+F**) finetuned and further the decoder (**VA+L+F+D**) finetuned. Under the **Oracle-Obj Setting**, CLIPSeg performs better than CATSeg on both source and target datasets. But CATSeg is less biased to the part as it performs the best on the target dataset under the **Pred-Obj Setting**. Hence CATSeg is less prone to overfitting than CLIPSeg with even more learnable parameters. We also explore the cross-dataset part segmentation from Pascal-Part-116 to ADE20K-Part-234 for analyzing the potential failures cases in either the part boundaries delineation or the language misunderstanding. The qualitative results are left to Section C of the supplementary material.

## 6 Conclusion

In conclusion, open-vocabulary part segmentation has unique challenges: the limited availability of labeled data, the complexity of part structures and the open granularity challenge. To inspire future research, we proposed the **OV-PARTS** benchmark with newly cleaned Pascal-Part-116 and ADE20K-Part-234 datasets. And we introduced three benchmark tasks: Generalized Zero-Shot Segmentation, Cross-Dataset Segmentation, and Few-Shot Segmentation, which assesses the analogical reasoning, open granularity, and few-shot adapting abilities of OV-PARTS models. Furthermore, we improved two paradigms from existing object-level OVSS methods as the baselines. Through comprehensive experimental analysis, we provided insights into the strengths and limitations of current approaches, highlighting the potential of leveraging large foundation models for OV-PARTS.

**Acknowledgements.** This work is supported by Shanghai Artificial Intelligence Laboratory, HKU Startup Fund, HKU Seed Fund for Basic Research, and HKU Seed Fund for Translational and Applied Research. Shu Kong is supported by SRG2023-00044-FST.

   Model &  &  \\  Setting &  &  &  \\  Finetuning & Seen & Unseen & Harmonic & Oracle & Pred & Oracle & Pred \\  ✗ & 22.33 & 19.73 & 20.95 & 21.58 & - & 15.38 & - \\ D & 44.65 & 26.03 & 32.89 & 29.86 & 27.16 & 24.01 & 12.96 \\ F & 31.34 & 21.45 & 25.46 & - & - & - & \\ L+F & 38.11 & 21.14 & 27.19 & 27.61 & 25.90 & 26.04 & 14.55 \\ TA+F & 33.13 & 21.70 & 26.23 & - & - & - & - \\ VA+F & 45.13 & 22.55 & 30.07 & 29.99 & 26.84 & 23.74 & 12.72 \\ VA+TA+F & 44.83 & 24.16 & 31.39 & - & - & - & - \\ VA+L+F & 47.04 & **27.69** & 34.85 & 31.34 & 27.67 & 28.67 & **17.32** \\ VA+L+F+D & **48.68** & 27.37 & **35.04** & **33.13** & **30.70** & **29.36** & 16.12 \\   

Table 5: Performance on finetuning various modules of CLIPSeg in both the zero-shot setting and few-shot setting on Pascal-Part-116 and ADE20K-Part-234.