# A Taxonomy of Challenges to Curating Fair Datasets

Dora Zhao

Stanford University

&Morgan Klaus Scheuerman

Sony AI

&Pooja Chitre

Arizona State University

Jerone T. A. Andrews

Sony AI

&Georgia Panagiotidou

King's College London

&Shawn Walker

Arizona State University

Kathleen H. Pine

Arizona State University

&Alice Xiang

Sony AI

Joint first author Joint second author Joint last author

###### Abstract

Despite extensive efforts to create _fairer_ machine learning (ML) datasets, there remains a limited understanding of the practical aspects of dataset curation. Drawing from interviews with 30 ML dataset curators, we present a comprehensive taxonomy of the challenges and trade-offs encountered throughout the dataset curation lifecycle. Our findings underscore overarching issues within the broader fairness landscape that impact data curation. We conclude with recommendations aimed at fostering systemic changes to better facilitate fair dataset curation practices.

## 1 Introduction

Persistent concerns from academia, government, industry, and the public sphere center on the disparate impact and unfairness in machine learning (ML) [22; 28; 32; 69; 70; 71; 74; 77; 94; 143; 159]. Data is often viewed as a primary culprit, perpetuating biases and compromising fairness [36; 52; 88; 164]. In response, substantial attention has been directed towards _fair_ dataset collection practices [43; 46; 62; 65; 114; 121; 135; 161; 164]. However, there remains a significant gap in understanding both the practices and practicalities of fair dataset curation.

To address this gap, we shift from theoretical, guideline-focused scholarship [3; 41; 42; 51; 63; 78; 80; 117; 103; 117] to empirical inquiry, exploring the grounded practices of fair dataset curation. Following a well-established tradition in human-computer interaction (HCI) [67; 75; 97; 107; 125; 149], we conducted interviews with 30 dataset curators from both academia and industry who have experience curating fair vision, language, or multi-modal datasets. Through these interviews, we uncover practical challenges and trade-offs to ensuring fairness in dataset curation. Our use of qualitative methodology allowed us to surface nuanced challenges and trade-offs that regularly appear throughout the curation process and gain insights into considerations that may otherwise remain undisclosed.

We first provide three dimensions of fairness--_composition_, _process_, and _release_--that participants considered during curation. Fairness is not only a property of the final artifact--the dataset--but also a constant consideration curators must account for throughout the curation process. Through our empirical findings, we identify various challenges that obstruct different fairness goals. Building on Hutchinson et al. 's conception of the dataset lifecycle, we contribute a taxonomy of challenges dataset curators encounter, addressing both dataset lifecycle-specific challenges (Section 3) and those within the broader landscape of fairness in ML (Section 4). By conducting in-depth interviewswith those engaged in fair dataset work on the ground, we provide empirical support for prior work , which has focused on identifying implicit challenges in the fairness literature (see Appendix B for additional background). We conclude with recommendations aimed at fostering systemic changes to better facilitate fair dataset curation practices (Section 5).

Our work aligns with existing recommendations for fair dataset curation  and aims to deepen stakeholders' understanding of the specific challenges involved. By illuminating these issues, we hope to expedite more effective solutions and promote further investigation into the complexities of fairness in dataset curation.

## 2 Method

To understand the challenges of collecting fair datasets, we conducted 30 semi-structured interviews with ML dataset curators, each lasting between 45-60 minutes, between November 2023 and March 2024. Participants were asked to define fairness in ML datasets, describe their process for collecting fair datasets, highlight challenges encountered, and discuss any trade-offs made. Refer to Appendix A for more details, including Institutional Review Board approval.

**Participants.** To qualify, participants must have previously curated at least one fair ML dataset. Given the extensive discourse surrounding language and vision dataset practices , we prioritized participants in these domains. To accommodate diverse perspectives, we refrained from prescribing a specific definition of "fair." Initial recruitment was conducted through purposive sampling , targeting authors of public datasets, followed by outreach via social media and relevant mailing lists, with snowball sampling  used to expand participation.

To protect anonymity, participants are referred to as "PX", where "P" denotes "Participant" and "X" represents their identification number (e.g., P8).

**Thematic Analysis.** To analyze the interviews, we adopted an inductive approach . We began with an initial set of codes derived from our literature review on challenges in fair data collection. Four authors independently coded the same interview to identify additional themes, refined the codebook through discussion, and repeated the process with a second interview. The remaining interviews were then equally distributed among the research team for thematic analysis.

## 3 Challenges During the Dataset Lifecycle

We present challenges participants encountered across the dataset lifecycle, taxonomizing them into requirements, design, implementation, evaluation, and maintenance phases (see Figure 1 and Table 3).

Figure 1: A circular process diagram showing how each challenge we identified maps to each phase and subphase of the dataset lifecycle.

Recognizing the multi-faceted nature of _fairness_, we did not impose a specific definition during our interviews. Instead, we empowered participants to articulate their own definitions. Based on these definitions, we identified three dimensions of fairness: _composition_, which is achieved through diverse representations; _process_, which includes equitable compensation for data subjects and workers as well as recognition for curation efforts; and _release_, which emphasizes the importance of transparent and openly accessible data. The challenges we surface span all three dimensions of fairness.

### Requirements

The _requirements phase_ involves establishing a dataset's purpose (e.g., intended tasks such as image tagging) and defining the fairness criteria to be operationalized within the dataset (e.g., group fairness). Challenges in this phase most often manifested in the composition and process dimensions.

**Scoping a dataset.** Participants sought to balance fairness with utility (P8, P23, P26, P30). On the one hand, careful curation can lead to more nuanced insights compared to general-purpose datasets. As P26 explained, they would ideally "_design smaller datasets for smaller models for specific applications, nothing that is deployed on a [South Asian] scale, because that definitely won't work properly because of the [region's] geographical diversity._" Moreover, datasets containing billions of entries, such as LAION [133; 134], make oversight difficult and, as a result, may include "_unfair_" data (P18) . Nonetheless, participants also had to consider utility. P13 noted ML is "_in this age of scale,_" making them "_a bit skeptical as [to] whether people are going to openly use fair datasets for training unless they're very large._" P21 highlighted a similar tension between "_technical reasons why you need large open datasets_" and "_ethical reasons on why that shouldn't be the case._" Fairness trade-offs pushed some (P12, P13) towards focusing on smaller evaluation datasets.

**Determining fairness definitions.** Nearly all participants stressed the _contextual_ nature of fairness. Key factors shaping their definitions included domain (e.g., healthcare), task (e.g., sentiment analysis), and cultural context. For example, P2 highlighted the importance of cultural specificity, stating, "_you see a lot of work that talks about fairness in gender or in race. But for a [South Asian] country, race does not manifest like it manifests for America._" Participants also made trade-offs due to the multitude of fairness definitions available  (Section 4.5). P19 noted that "_there's more than two dozen different fairness definitions... used in the literature._" This diversity necessitated sacrifices in other dimensions, as emphasized by P18, who illustrated this with the "_no free lunch theorem_", stating, "_You can't have complete diversity with respect to, say, races,...geographies,...times of the day, and other domains. Everything is not possible. Once you clamp on one, the other one goes away._"

### Design

In the _design phase_, curators determine how to operationalize dataset requirements, including defining the dataset's taxonomy. For example, curators specify attributes for measuring fairness (e.g., skin tone) and the categories within those attributes [66; 68; 114; 145]. This phase also involves decisions on data collection and annotation methodologies (e.g., web scraping, hiring vendors). Challenges in this phase typically arose in the composition and process dimensions.

**Creating fair taxonomies.** Participants struggled to find a fair taxonomy under the inherent unfairness of categorization. For example, P18 devised a geographic taxonomy featuring categories for the U.S. and Asia, acknowledging that the regions "_are not homogeneous, they're very heterogenous._" P2 also noted a theoretically ideal taxonomy is as granular as possible, but practical constraints, such as data availability (Section 3.2) and time (Section 4.3), necessitated using coarser categories. Finally, the challenge of creating a fair taxonomy was compounded by the inadequacies of existing domain taxonomies. For example, P1 and P5 pointed out that the common binary operationalization of gender in medical data erases many gender identities. Nonetheless, participants felt compelled to utilize inadequate taxonomies due to practical constraints, even if it contradicted their personal beliefs. Participants were forced to align their notions of fairness with disciplinary norms (Section 4.2).

**Data availability in taxonomy design.** Similar to when designing taxonomies, participants had to balance their ideal data collection methods with practical constraints. For example, P3's dataset only included Spanish and Arabic even though they _"wanted to look at other languages, but... didn't have training data."_ Participants questioned prevailing data collection paradigms, such as web scraping , which were seen as unethical when performed indiscriminately. For legal compliance, P25 manually collected data for two years: _"I was downloading, like clicking and clicking, because they didn't allow me to do web scraping or didn't have an API."_

### Implementation

The _implementation phase_ marks the execution of plans from the design phase, where curators collect, annotate, and package the data into a dataset. This phase broadly encompasses two subphases: data collection and data annotation. Challenges in this phase span all three dimensions of fairness.

#### 3.3.1 Data Collection

_Data collection_ involves gathering relevant data to fulfill dataset requirements. Challenges during this subphase prevented participants from attaining fairness goals relevant to dataset diversity.

**Diverse data availability.** Similar to concerns raised regarding dataset taxonomies (Section 3.2), participants raised concerns about data availability for creating a fair dataset. For example, P28 described how sexist stereotypes permeate web data, such as _"women [being] associated with nurse more often than men."_ Additionally, P18 encountered difficulties sourcing web data from _"Middle Eastern"_ and _"African countries"_ but found _"lots and lots and lots of images from India, Japan and [the] U.S., which are like the three most dominant geographies in uploading pictures."_ Participants also lamented the inaccessibility of specialized or proprietary data, such as medical records or data from private companies, which could significantly improve the creation of fair datasets. P4 stated that _"because people don't own large e-commerce platforms or social media platforms, or whatever, we just kind of have to deal with things that we can gather from existing systems."_

Interestingly, synthetic data, sometimes presented as a potential solution to biased data , was met with skepticism as it could perpetuate stereotypes or inadequately represent underrepresented groups . As P19 pointed out, _"You might address some of the missing data points [with synthetic data] but at the end of the day it's still the same underlying data distribution, right?"_

**Data collector availability.** Many participants associated fairness with geographically diverse data. For example, P22 expressed how they would _"proactively sample more data from underrepresented regions."_ Yet, actualizing this objective proved challenging, as P12 highlighted the difficulty in _"getling] hold of people... from very, very small regions."_ Infrastructure hurdles, such as limited internet and mobile phone access, further complicated the process . Equipping data collectors with necessary equipment is costly (Section 4.3) and logistically challenging, as _"you might have to give people smartphones to start and you'd also need more labor on the ground... who are working in these different regions to come together and do this"_ (P12).

#### 3.3.2 Data Annotation

_Data annotation_ involves labeling data with attributes specified during design. Participants faced challenges recruiting annotators who had requisite expertise or came from diverse backgrounds. Upholding fair labor practices (Section 3.3.3) during annotation also presented challenges.

**Data annotator diversity and expertise.** The interpretation and application of annotation categories can vary based on an annotator's perspective . P22 described finding annotators for labelling building styles across different geographies: _"You give this same image to a local labeler who is in that culture, who is an expert in, you know, their architecture... then you get a much better label."_ Yet, participants had difficulty hiring annotators that met their desired aims. While P2 highlighted the value of diverse annotator backgrounds or beliefs to ensure annotations reflected a wide range of experiences, accessing diverse annotators was challenging, _"because some of the attributes of [annotators'] personal lives might even be illegal to ask about in a particular country."_ Participants also confronted challenges in recruiting annotators with specialized expertise. For example, despite offering "_$75 or $100 per hour_," P1 faced difficulties finding and incentivizing medical experts to annotate radiology data. Annotators who lack diversity or expertise in data concepts may lead to issues with data quality, including inaccuracies , biases , and overly homogeneous annotations . Notably, P13 highlighted that crowdsourced annotators regularly embed gender biases into datasets such that _"researchers [need] to make sure that annotators represent everyone because [if] not, you're just gonna have a skewed pool of annotations as well."_

#### 3.3.3 Implementation Processes

Participants expressed challenges not only with dataset content but also with the _implementation_ of data collection and annotation. We provide three main considerations discussed by participants.

**Vendor transparency.** Collaborating with data vendors introduced transparency challenges, hindering fairness efforts. First, as prior research documented , vendors may prohibit access to data worker identities, such as demographic details or location (as described by P2 in Section 3.3.2). Thus, it is impossible to evaluate potential biases or expertise linked to identity characteristics, such as how an annotator's cultural identity may influence their engagement with data concepts. Second, participants had little oversight into worker compensation or encountered communication restrictions imposed by vendors. As P12 said, _"I think [pay] was fair in terms of [being] calibrated across different countries... but we weren't able to get exact numbers, because that was confidential."_ P6 described how vendor platform design inhibited direct collection of feedback from data workers, impeding efforts to improve fairness in dataset creation and labor conditions (e.g., ) (Section 4.5).

**Language barriers.** Curating fair datasets often involves collecting geographically diverse data, which may require data workers proficient in languages different from those of curators. Language barriers can hinder effective communication, necessitating fairness concepts established in the design phase (Section 3.2) to be accurately translated into the workers' native languages. Improper translations can result in misinterpreted labels or instructions and may even lead to contract breaches, particularly concerning subject consent. Addressing language barriers often involves resorting to translation services, which may be constrained by cost (Section 4.3) or introduce its own fairness concerns. Further, participants had to ensure translations accurately reflected their intentions, but as P3 noted, _"We relied on our translators to come up with those sorts of decisions in terms of Spanish."_

**Fair data labor.** Several participants (P6, P11, P12, P14, P16, P24, P28) expressed concern about engaging in fair labor practices when working with data workers, but systemic organizational (Section 4.3) and regulatory (Section 4.4) issues made achieving these standards difficult.

### Evaluation

The _evaluation phase_ involves assessing data quality and testing dataset utility. Challenges in this phase can result in homogeneous annotations, benchmarking difficulties, and spurious correlations, most often affecting the composition and release of a dataset.

#### 3.4.1 Assessing Data Quality

_Assessing data quality_ entails validating and refining the data and its annotations to ensure clarity and consistency with project requirements. (Re)alignment of data and annotations with the guidelines from the design phase is often referred to as _quality assurance_.

**Gold standard paradigms.** Participants often sought to capture a diversity of perspectives across annotators. Thus, prevailing practices for validation and cleaning, such as majority voting and annotator agreement metrics, may be unsuitable. As P24 emphasized, majority voting can _"squash or stifle diverse opinions when it comes to subjective tasks."_ When disagreement is integral to the objective, annotator agreement metrics become inappropriate, making it difficult to "validate" annotation quality. Gold standard paradigms are intrinsically tied to disciplinary challenges (Section 4.2); if submitting a publication involving dataset creation, reviewers might still call for annotator agreement metrics and believe the quality of the data is poor if agreement is low.

Similarly, common practices used to clean or filter data can perpetuate dominant cultural beliefs. Data that might appear noisy or incorrect can hold significance for certain communities. P14 explained how quality filters resulted in _"ge[ting] rid of vernacular that's not perfect English but is maybe like African-American vernacular or like Hispanic-American vernacular, and that also introduces bias and lowers the diversity of the dataset."_ This echoes prior work  which found that standard data filters might disproportionately exclude content from already marginalized groups.

#### 3.4.2 Evaluating Data Utility

To ensure dataset utility, curators must evaluate its effectiveness, often through _requirements testing_ to confirm its suitability for the intended purpose. Participants aimed to align the dataset with fairness definitions and mitigate any potential biases present in the data.

**Lack of benchmarking datasets.** Curators often seek to benchmark their datasets to showcase their utility. However, since many participants aimed to create unprecedented fair datasets to address existing gaps, this norm posed a challenge as comparable datasets were non-existent. Reflecting on the struggles with a novel geodiverse dataset, P12 explained, "_We couldn't measure it unless we had a dataset that actually was fair. Since we don't have a dataset that is fair..., you are arguing in circles._" Furthermore, even if comparable datasets exist, they may harbor fairness issues of their own.

**Evaluating immeasurable constructs.** Evaluating whether a dataset aligns with fairness definitions presupposes that fairness is a construct amenable to measurement. While some participants offered quantifiable indicators of fairness, such as demographic diversity, others argued that fairness defies quantification. P14 criticized measurement-oriented perspectives, stating, "_They also assume that fairness can be measured, can be evaluated, and can be improved. And I think that all of this is a more positivist mindset._ " Even with a definition in mind, testing may feel incomplete. As P28 said, "_Even when you provide a way to measure fairness, you're probably overlooking something._"

**Spurious correlations.** Several participants (P6, P23, P28) aimed to avoid introducing spurious correlations that affected the fairness of the dataset's composition . While these correlations may not be "_connected with any demographic or social variable_" (P23), they can still influence downstream models and result in biased decisions. However, as recent research  has revealed, spurious correlations with demographic attributes are ubiquitous. Thus, enumerating and removing all possible correlations is virtually impossible.

### Maintenance

In the _maintenance phase_, curators must consider both how their dataset is released and strategies for ensuring its ongoing utility over time. Challenges at this stage often linked back to participant concerns around fairness in dataset release (i.e., ensuring the data is transparent and openly accessible).

**Unstable infrastructural ecosystems.** Digital data is intrinsically impermanent. Some participants (P1, P8, P30) emphasized the risk of data instances disappearing due to broken links or shifts in platform popularity or ownership, as observed with platforms like Twitter. Therefore, curators must then not only monitor for missing data but also find suitable replacements that match the original dataset's distribution. This can be particularly burdensome when the data was expensive (Section 4.3) or difficult to collect (Section 3.3.1). As data goes missing, datasets can become unbalanced and thus "unfair," demonstrating how fairness issues with data release are linked to concerns about composition.

**Dataset traceability mechanisms.** The challenge of dataset stewardship is exacerbated by inadequate traceability mechanisms . Participants underscored their inability to track users and usage patterns of their datasets. One commonly used proxy is citations in academic papers, but it was hard to "_distinguish citations that use the data versus citations that use the broader idea of the paper_" (P2). This is concerning, especially if fair datasets are repurposed in unintended ways. While prior works  have suggested data usage policies to mitigate such risks, enforcing them becomes impractical when curators are unaware of actual data users.

## 4 Challenges Overarching the Broader Landscape of Fairness

The dataset curation process is influenced by the environments in which curators operate, meaning their decisions are not made in isolation. Many challenges span all phases of the lifecycle, shaping the broader landscape of dataset fairness. We identified five levels within this landscape, where challenges may emerge from one or more levels, affecting dataset curation at every phase of the dataset lifecycle (see Figure 2 and Table 4).

### Individual Level

The _individual level_ of the dataset curation landscape refers to the contributors of fair datasets, such as data curators, data subjects, and data workers.

**Individual contributor positionality.** Decisions made by contributors were inevitably influenced by their own unique perspectives . As P24 said, _"There's this stuff we swim in that we don't really realize is even there."_ Despite recognizing this influence, assessing its tangible impact on the dataset remained elusive. Addressing and diversifying contributor positionality is further complicated by other challenges within the dataset curation landscape, such as cost and power differentials. Positionality was evident in instances where participants felt they had to make trade-offs during processes like designing taxonomies that may erase others' experiences. P27 encouraged reflecting on personal values: _"Is this [research] actually in line with your life philosophy? Was it in line with your gender, with your sexuality... If it's not, would you still want to be doing this?"_

### Discipline Level

The _discipline level_ of the dataset curation landscape centers on the norms and practices governing specific academic disciplines, particularly ML .

**Recognition for fair dataset work.** Despite the growing demand for data in ML, according to participants, fair dataset curation efforts were not seen as significant contributions to the field. P11 described a _"lack of general disciplinary value of datasets as contributions."_ While some major conferences like NeurIPS  have introduced dataset tracks, few venues prioritize dataset-focused work. This lack of appreciation discourages efforts to ensure dataset stability and longevity .

**Incentive mechanisms.** Incentives in ML do not align well with the costs of fair dataset curation. According to P11, there's _"just [a] total lack of resources and time to actually deeply engage with labeling and sourcing those labels and getting people who are representative of those labels to be the data workers."_ Participants echoed well-documented observations that model work is valued over data work , with P21 stating that _"data is kind of a second-class citizen in ML research."_ Consequently, P25 felt _"people are [not] seriously talking about fairness... people are still just get[ting] whatever [data] they get to do their research, or publish, or whatever."_

**Awareness of existing resources and guidelines.** Participants had limited awareness of existing guidance for fair dataset curation. This lack of awareness may be attributed to some of these resources (e.g., ) being disseminated outside of traditional ML venues (e.g., NeurIPS, *CL, ICML, CVPR). As P29 admitted, _"I don't remember any explicit guidelines that I've stumbled through for fair dataset collection. Honestly!"_ Promoting interdisciplinary awareness of fairness efforts among those primarily involved in ML is challenging due to highly disciplinary norms that prioritize novelty in ML methods over discussions on fair dataset curation.

Figure 2: A social ecological  representation of challenges in each layer in the overarching landscape of fairness. A social ecological model shows how each layer is nested but interconnected.

Responsibility for fairness.The burden of responsibility for fairness weighs most heavily on individuals aware of fairness concerns in ML. Participants echoed findings from prior research  that document how fairness is not a top priority for many ML researchers. For example, P25 said that _"[in] the team I work with... I never heard them talking about [how] the dataset has to be fair."_ In P25's experience, the norm was to cursorily engage with fairness issues without substantive changes to research practices. Given the lower prioritization of fairness in ML, the onus falls on individual researchers who _"have a strong sense of justice and fairness"_ (P24) or are part of fairness-oriented communities to elevate these concerns. However, this commitment often lacks external recognition and may hinder resource allocation and research progress. Participants recognized that collecting fair data is more challenging and resource-intensive compared to conventional methods: _"If you want to build a fair dataset, maybe the most efficient way to do that is to scrape the web, but getting really diverse data in an ethical way is really hard and really expensive"_ (P11).

### Organization Level

The _organization level_ refers to the organizations where individuals conduct fair dataset curation work, which could vary in size or nature, such as academic or industry settings.

**Lack of resources.** Insufficient resources were a significant challenge across all phases of the dataset lifecycle. As P1 declared: _"Money?! (laughs) If you have money, you can have a very high quality of data."_ Fair data collection methods are costly, especially concerning data quality and annotation, which often require hiring experts. Convincing funders or stakeholders of the value of investing in fair datasets proved difficult, as noted by P24: _"It's hard to convince somebody to spend thousands and thousands to collect [a] dataset of recordings."_ Moreover, participants aimed to compensate data subjects and workers fairly, _"not just the minimum wages that many times academia gives"_ (P29). Longterm maintenance costs added to the financial burden, with difficulties in securing ongoing funding. P1 stated no academic or industry organization _"[wants] to spend another millions of money every year... to maintain those products."_

**Ethics washing.** Participants disapproved of organizations that superficially promote fair ML but fail to meaningfully integrate fairness into their practices . According to P16, the _"language of fairness is simply external lip service [that] ultimately boils down to looking at the maximization of other imperatives, such as economic ones."_ Resource constraints exacerbate this issue, leading organizations to prioritize efficiency and cost-effectiveness over fairness. As P22 noted, _"A lot of big companies do responsible AI shenanigans... for marketing... And then a new shiny thing comes down the road, and then they join that instead."_ When fairness is valued primarily for its marketing appeal rather than its impact on product development, it is not prioritized for monetary or labor investment.

### Regulatory Level

The _regulatory level_ concerns laws and policies governing dataset curation and use. Participants expressed anxieties about violating regulations they were not necessarily equipped to fully understand.

**Differing legal practices.** Contextual laws and regulations posed a challenge for participants. P2 described how _"laws in America or laws in Europe... might not be directly applicable to a [South Asian] country that has a very different societal situation."_ Contextually contingent laws and policies further complicated efforts to obtain data from diverse, underrepresented populations (Section 3.3.1).

**Legal risk.** Throughout the dataset lifecycle, participants faced the looming risk of unintentionally violating laws and regulations, potentially leading to breaches of privacy, labor, or data ownership laws. Instances of inadvertent violations are not uncommon, as highlighted by participants' experiences with web scraping practices. For example, P21 was aware that _"people discovered links to child pornography"_ in a widely used benchmark dataset . In another instance, P5 described working on a clinical dataset only to learn that releasing it was _"not possible because it's not consistent with the privacy laws in France."_ To mitigate these risks, some participants adopted highly cautious practices, such as exclusively collecting royalty-free or Creative Commons images, and storing only image URLs to avoid any copyright violations. However, these strategies can result in dataset instability, as observed by P8, who faced issues with broken URLs.

**Limited regulatory literacy.** Insufficient understanding about navigating the law intensified concerns about legal risk. P8 described it as _"a big learning curve to understand what we were allowed to store and what we weren't."_ As a result, P8 consulted an intellectual property lawyer. However, depending on the other constraints dataset curators are under, such as discipline (Section 4.2) or organization (Section 4.3) level constraints, hiring legal counsel may be untenable.

### Socio-Political Level

The _socio-political level_ covers the shifting social and political contexts around fairness in which curators operate. These challenges can be conceptualized as thorny, fluid, and arguably insoluble.

**Evolution and contestability of fairness.** According to P3, fairness will _"always be up for debate,"_ making it _"sort of impossible for there to be like a gold standard."_ Fairness is subjectively perceived, influenced by individual contexts, experiences, and beliefs . This subjectivity fuels ongoing scholarly debates ; it also fueled diverse perspectives among participants. As P30 pointed out, _"There are people from the audience who say that we have a good definition [of fairness], and there are some people who say that we have a terrible definition. And there's no way to make everyone happy."_ The absence of a universally accepted definition complicated participants' efforts to operationalize fairness in dataset curation. Further, existing guidelines may not suit every notion of fairness, leading to divergent curation methodologies. As P14 highlighted, _"It's kind of like a philosophical question... while the quantitative method says that fairness can be achieved, contrast it to qualitative that we are just trying to understand the experience here."_ Beyond disagreements about what fairness means (or should mean), participants also noted that current definitions are not stable. As P16 put it, fairness _"should be a notion that is able to evolve within society, and certain forms of injustice that were not considered injustice[s] in the past now are... three might be other evolution towards the future that we currently do not incorporate in our definition of fairness, and we need to account for that."_ This perpetual evolution presents challenges for dataset curators. They must decide whether to regularly update datasets or retract them as definitions evolve. However, both approaches have limitations in addressing the continued use of previously released datasets .

**Social realities versus model realities.** P8 described how the real world is different than _"what's experimentally valid and testable."_ Due to the complexity of the real world, certain groups inevitably remain underrepresented, misrepresented, or overlooked entirely despite best efforts. For example, P12 mentioned that while they wanted to collect images from underrepresented countries, data collector availability constrained their options (Section 3.3.1). Participants also questioned whether balanced representation was even the best approach. As P1 pointed out, _"The problem is when you actually apply such a model to the real world, the real world is imbalanced, right?"_ This echoes the classic trade-off between fairness and accuracy in algorithmic fairness work . Curators must wrestle not only with the impossible task of how to best account for every human experience in a dataset, but also whether or not they should be.

**Power differentials.** Power imbalances contribute to fairness issues during the curation process that are not visible in the dataset's composition. Participants noted how more elite institutions and companies dominate efforts to create fair datasets, largely owing to their access to resources (Section 4.3). Similar to findings from prior work , P21 described how most public datasets are not used, with the majority of _"the datasets that get used in ML research [being] created by a very, very small elite cadre of... academic institutions that have close affiliations with top industry researchers."_ Similarly, P16 felt it was problematic that the _"most important tools"_ remain in the hands of a few companies, _"yet they are given the freedom to define what is fair, and their definition is used, and then the safeguards that do exist might not always align or ensure protection."_. Thinking on a geopolitical scale, P2 noted that _"the field of algorithmic fairness has been dominated by the Western perspective."_ This imbalanced representation exacerbates other challenges previously outlined, including those at the implementation, disciplinary, and organizational levels.

Power differentials also permeate relationships between dataset curators and other stakeholders, including data subjects and workers . For example, P6 described how curators have complete oversight over worker compensation: _"So many platforms don't actually ensure that you're fairly compensating workers. And it's really up to the individual researchers which is a crazy system that sets absolutely the wrong incentives."_ P10 compared the impulse to collect data cost-effectively, at the expense of data subjects, as _"a particular kind of colonial impulse, like, this is just up for grabs."_ Similarly, curator decisions have profound implications downstream. P22 described the difficulty of _"fighting"_ clients who do not prioritize model performance on heavily under-resourced populations, given they are not central to business incentives: _"It's like, '99% of my customer[s] will be fine, why do I need to care about that last 1%?"_ Overall, dataset curation was seen as _"a very unfair process, no matter how you do it... unless you're going to literally tackle society"_ (P8).

## 5 Recommendations for Enabling Fair Dataset Curation

Finally, we highlight recommendations across the three dimensions of fairness for facilitating fair dataset curation. We focus on top-down efforts, reflecting the need for systemic changes rather than relying solely on individual contributions. See Appendix D for additional recommendations.

**Composition.** To better enable fair dataset _composition_, we encourage interventions for more flexible and robust data practices. For example, at the design phase (Section 3.2), flexible taxonomies can facilitate different operationalizations rather than forcing curators to use only one taxonomy (e.g., protected attributes can include self-reported and third-party labels). At the discipline level (Section 4.2), we advocate for more communication across academic communities. Papers published outside traditional ML venues (e.g., CHI, FAccT, CSCW) have provided guidance on data curation, such as annotation practices  or considerations on taxonomies .

**Process.** A change in the fair dataset curation _process_ requires not only norm-setting within fairness communities, but also legal and policy interventions. For example, at the implementation phase (Section 3.3.3), participants were concerned about labor rights for data workers. As a discipline, we should have norms about compensating workers, at least at the local minimum wage, for their labor and support efforts to introduce policies that offer codified protection for data workers. Furthermore, at the regulatory level (Section 4.4), rather than expecting curators to develop legal expertise, we advocate for the creation of accessible resources on legal practices regarding dataset collection.

**Release.** We encourage interventions that allow for fairness post-_release_. For example, at the maintenance phase (Section 3.5), efforts to build tools and policies to enable better dataset traceability could alleviate concerns with dataset misuse. Additionally, at the organization level (Section 4.3), funding entities should invest in maintenance, rather than solely focusing on modeling research. Monterarily valuing long-term maintenance plans as research contributions may help shift perspectives about revision, maintenance, and use policies at the discipline level (Section 4.2).

## 6 Discussion and Conclusion

Our qualitative data reflects the experiences of our participants, and while we identify shared themes, these challenges may not be universally applicable or entirely representative. Despite efforts to recruit diverse dataset curators, our sample is skewed toward curators from North America and Europe, reflecting the Western-centric nature of ML and fairness research . Given the challenges raised around creating culturally contextualized datasets and navigating power dynamics across regions, future work should aim to include more geographically diverse voices, especially from the Global South, for deeper, more nuanced insights.

Despite these limitations, our study offers an important foundation for addressing the practical challenges in fair dataset curation. Through interviews with dataset curators engaged in fair dataset work, we developed a taxonomy of challenges across the dataset lifecycle and the broader fairness landscape. Participants navigated complex trade-offs between ideal fairness goals and practical constraints such as data availability, resources, and time. While we acknowledge the limitations of our methodology, taxonomizing these challenges is a crucial first step in developing long-lasting solutions to support fair dataset curation.

Addressing these challenges will require effort not only from individual dataset curators but also systemic changes at organizational, disciplinary, and regulatory levels. Beyond providing dataset curators with grounded evidence to support their efforts in building fair datasets, our taxonomy offers stakeholders a pathway to address each challenge individually and opens avenues for further, more targeted investigations into the many challenges of curating fair datasets.