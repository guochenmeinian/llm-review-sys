# A Textbook Remedy for Domain Shifts:

Knowledge Priors for Medical Image Analysis

 Yue Yang, Mona Gandhi, Yufei Wang, Yifan Wu, Michael S. Yao,

**Chris Callison-Burch, James C. Gee, Mark Yatskar**

University of Pennsylvania

yueyang1996.github.io/knobo

###### Abstract

While deep networks have achieved broad success in analyzing natural images, when applied to medical scans, they often fail in unexpected situations. We investigate this challenge and focus on model sensitivity to domain shifts, such as data sampled from different hospitals or data confounded by demographic variables such as sex, race, etc, in the context of chest X-rays and skin lesion images. A key finding we show empirically is that existing visual backbones lack an appropriate prior from the architecture for reliable generalization in these settings. Taking inspiration from medical training, we propose giving deep networks a prior grounded in explicit medical knowledge communicated in natural language. To this end, we introduce **Know**ledge-enhanced **B**ottlenecks (**KnoBo**), a class of concept bottleneck models that incorporates knowledge priors that constrain it to reason with clinically relevant factors found in medical textbooks or PubMed. KnoBo uses retrieval-augmented language models to design an appropriate concept space and an automatic training procedure for recognizing the concept. We evaluate different resources of knowledge and recognition architectures on a broad range of domain shifts across 20 datasets. In our comprehensive evaluation with two imaging modalities, KnoBo outperforms fine-tuned models on confounded datasets by 32.4 % on average. Finally, evaluations reveal that PubMed is a promising resource for making medical models less sensitive to domain shift, outperforming other resources on both diversity of information and final prediction performance.

## 1 Introduction

Robustness to domain shifts is a key property for models operating on medical images because transfer scenarios arise widely. Deep networks have achieved broad success in analyzing natural images (everyday human contexts), but when applied to medical scans, they often substantially degrade under distribution shift . Medical datasets are small, and unidentified confounds in them combined with model misspecification can dramatically degrade performance . Such failure erodes confidence as models do not learn the right information from training data, hampering adoption by medical professionals. We study such problems by investigating the performance of systems in the presence of confounded data and address a main shortcoming we discover.

Model sensitivity to domain shift can be measured by introducing synthetic confounds into data and evaluating on samples where the confound misleads the model. For example, in Figure 1, we introduce confounded datasets for chest X-ray and skin lesion images where, during training, positive data is sampled from one group and negative from another. This association is reversed at testing time, creating an adversarial out-of-distribution (OOD) evaluation. In 5 such constructed confounds per modality, covering scenarios of race, sex, age, scan position, and hospital, we find models unable to generalize well, dropping over 63% on average over an in-distribution (ID) evaluation.

Priors are an important signal allowing models to adopt appropriate hypotheses in low or misleading data regimes. We hypothesize that existing visual backbones lack an appropriate prior for robust generalization in medicine. Like previous work identifying that vision backbones have a deep image prior even when entirely untrained [72; 78], we compare the quality of image representations produced by untrained networks on natural versus medical images. Given the output from a frozen untrained visual backbone, we train a linear classifier for predicting a diversity of labels (see Figure 2). Across architecture, these untrained models are higher quality featurizers of natural images than directly using pixels as features. In contrast, **across multiple medical modalities, the deep image prior in current major visual backbones is no more effective than using pixels (and often worse).**

To address the lack of an effective deep image prior for medical images, we propose using an inherently interpretable model design. We draw inspiration from medical education, where students first learn from textbooks and later in a more practical setting during the residency with an attending doctor. Our models mimic this pattern: first, documents are used to identify important knowledge, and then they learn by example from data. We employ concept bottleneck models (CBMs)  and enrich them with information derived from resources broadly accessible to medical students. CBMs are a class of inherently interpretable models that factor model decisions into human-readable concepts that are combined linearly. Our methods build on recent approaches for language model (LM) guided bottleneck construction where LMs are prompted for discriminative attributes .

We introduce **Know**ledge-enhanced **B**ottlenecks (**K**noBo**) to incorporate knowledge priors that encourage reasoning with factors found in medical documents. KnoBo extends CBMs to medical imaging and employs retrieval-augmented generation into concept design. For example, we extract concepts from medical textbooks as natural language questions like _Is there ground-glass opacity?_ to help the model classify whether an X-ray is positive for a respiratory infection. As illustrated in Figure 3, KnoBo factors learning into three parts: (1) an interpretable bottleneck predictor, (2) a prior over the structure of the bottleneck, and (3) a prior over predictor parameters. This factorization allows us to guide the model with a prior rooted in medical documents. The approach relies on an iterative retrieval process where an LM summarizes documents to propose concepts, forming our medical image prior (Sec 4.2). Given the concepts, a pretraining corpus of reports and images is used to construct a classifier for a concept (Sec 4.3). Finally, a CBM is learned using predictions from classifiers on data while regularized by a prior formed from LM generations. (Sec 4.4).

We evaluate KnoBo on our benchmark of confounded tasks. Averaged over confounds, KnoBo increases OOD performance by 41.8% and 22.9% on X-ray and skin lesion datasets, respectively. KnoBo's success in OOD performance comes at little sacrifice in ID settings, providing a better model overall when averaging the two data settings. We also explore 5 different sources of knowledge and reveal that PubMed outperforms other resources in terms of both diversity of information and final prediction performance. Overall, our work demonstrates that a key missing ingredient for robustness to distribution shift in medical imaging models is a prior rooted in knowledge.

Figure 1: In-domain (ID), out-of-domain (OOD), and average of ID and OOD (Avg) performance on confounded medical image datasets. Our interpretable **K**nowledge-enhanced **B**ottlenecks (**K**noBo**) are more robust to domain shifts (e.g., race, hospital, etc) than fine-tuned vision transformers .

Figure 2: Classification performance on natural and medical images through linear probing using features extracted from untrained and frozen models versus pixels features (See Sec 3 for details).

Related Work

The rapid advancement in medical foundation models offers the opportunity to develop healthcare AI [55; 49]. However, their lack of transparency presents risks in real-world applications .

**Interpretability** is crucial for using models in high-stakes domains [30; 79; 87]. Previous work has primarily focused on post-hoc interpretability [76; 95; 29; 75], which may not provide faithful explanations . As an alternative, inherently interpretable methods produce explanations that align with the model's reasoning processes [8; 3]. In this work, we build upon **Concept Bottleneck Models** (CBMs) , which predict by linearly combining human-designed concepts. Recent work [91; 60; 90] scales the applications of CBMs by aligning concepts and images with CLIP  and prompting language models to generate concept bottlenecks automatically. In our work, we treat CBMs as the architecture to incorporate knowledge priors for mitigating medical domain shift problems. Our bottlenecks, built from the medical corpus, are attributable and more trustworthy.

**Domain Generalization and Robustness** are critical in medical domains where the distribution of imaging protocols, devices, and patient populations can significantly vary . A line of work studies various domain-shift problems [42; 2], proposing algorithms to learn invariant representations [57; 22; 80; 63] and employing domain/group information for reweighting [71; 93; 44; 96]. However, many studies show those methods do not improve over standard Empirical Risk Minimization (ERM) [67; 26; 33; 27]. Fine-tuning the last layer [68; 40] or selectively fine-tuning a few layers  is sufficient for robustness against spurious correlations in those datasets. We address domain shifts in medical imaging from a novel perspective by employing interpretable models to integrate knowledge priors. Our approach encourages models to adhere to diagnostic rules similar to those doctors use rather than relying on spurious correlations. Concurrent work shows bottleneck models can perform well on out-of-domain X-ray data but severely reduced in-domain performance as a consequence . In contrast, we demonstrate a significantly better compromise between OOD and ID performance, using a broader set of modalities and constructing our bottlenecks from medical documents.

**Knowledge Rich Multimodal Reasoning.** Knowledge plays an important role in clinical diagnosis . Some multimodal tasks [82; 53; 74] require models to use explicit outside knowledge to make correct predictions. Previous methods [52; 50; 31] retrieve documents for each example from the external knowledge base as context for models to generate the answer. Our work focuses on leveraging knowledge in medical image classification. **Retrieval-Augmented Generation**[48; 23] has been shown to be beneficial for knowledge-intensive tasks , including biomedicine [20; 84]. The retrieved medical documents are either used as context during inference  or data for pretraining . In contrast, we treat documents as background knowledge for large language models to build concept bottlenecks. Instead of retrieving documents for every input, we build a global knowledge prior from a medical document corpus, which is shared across all examples.

## 3 Deep Image Priors for Medical Images

This section revisits the concept of deep image priors [72; 78], i.e., some data-agnostic assumptions from model structure, in the context of image classification across various domains. By comparing linear probing using features extracted by untrained deep networks against pixel-based features, we observe that existing vision backbones lack suitable priors for medical domains. This observation motivates our knowledge-enhanced bottlenecks (Sec 4) to integrate more robust priors into models.

**Setup.** Consider a dataset of image-label pairs, \(=(I,y)}\), where \(I\) is an image and \(y\) denotes the label from one of \(N\) classes. The model learns to predict \(P(y|I,)\), where \(\) is the model parameters. We employ a frozen, untrained vision backbone \(\) to extract features from \(I\), producing a feature vector \(=(I)\), where \(^{d}\). A linear mapping function \(f_{}:^{d}\) is then trained to classify these features into label spaces. In this case, the model parameters \(\) will inherit the implicit architectural priors of \(\). As a baseline, we extract a subset of \(d\) pixels directly from the image as the feature without any model-based priors, represented as \(_{p}^{d}\). We compare the classification performance using \(\) versus \(_{p}\) to probe the efficacy of the vision backbone's priors.

**Experiments.** We evaluate two state-of-the-art vision backbones, ViT-L/14  and ConvNext-L , on three categories of images: natural photos (e.g., ImageNet ), X-rays (e.g., NIH-CXR ), and skin lesion images (e.g., HAM10000 ). Each image category has 5 datasets, and we report their average performance in Figure 2 (see Table 11 in the Appendix C.1 for full results).

Figure 2 (left) shows vision backbones have effective priors for natural images, with ViT notably outperforming pixel by 14.7%. However, pixel features surpass those extracted by vision backbones for specialized domains such as X-ray and skin lesion images. This underscores these deep networks' lack of image priors appropriate for these domains, which can hamper model learning and hurt generalizability. Without guidance from appropriate priors, models can overly rely on data, risking catastrophic failures. We aim to overcome this by injecting additional priors into models.

## 4 Knowledge-enhanced Bottlenecks

In this section, we present **Know**ledge-enhanced **B**ottlenecks (**KnoBo**), a class of CBMs that incorporate knowledge priors that address the failures we identified in Section 3. Figure 3 presents an overview of our method. From left to right, we optimize three terms: (1) **Structure Prior** (Sec 4.2) induces bottleneck structures from medical corpus to incorporate human knowledge as concepts, (2) **Bottleneck Predictor** (Sec 4.3) projects input image onto bottleneck concepts and then feed concept predictions into the linear layer for label prediction, and (3) **Parameter Prior** (Sec 4.4) aligns the learned parameters with known associative information to further enhance priors.

### Problem Formulation

**Preliminary on Concept Bottleneck Model.** Given a bottleneck \(C\) with \(N_{C}\) concepts, CBMs optimize two functions for predictions: \(=f(())\), where \(:^{d}^{N_{C}}\) maps image features into concept space, and \(f:^{N_{C}}\) uses concept predictions for final label predictions.

**Formulation**. Our goal is to incorporate priors over \(C\), the concept structure, into the learning of the joint probability \(_{(I,y)} P(y,C,|I)\), which can be decomposed into three factors:

\[ P(y,C,|I)=_{}+_{}+_{}\] (1)

where we assume the priors over structure \(C\) and parameters \(\) are independent. The **structure prior**\(P(C)\) (Sec 4.2) is formulated as the construction of a bottleneck with \(N_{C}\) concepts, \(C=\{c_{1},c_{2},...,c_{N_{C}}\}\), derived from a background corpus \(\). Each concept is a factor that humans will use when solving the same task. The **bottleneck predictor**\(P(y|I,C,)\) (Sec 4.3) is a concept bottleneck model that predicts the label conditioned on the input image, bottleneck, and learned parameters. The bottleneck predictor is inherently interpretable, and each parameter in \(\) has semantics, denoting the association between concepts and labels. The **parameter prior**\(P()\) (Sec 4.4) regularizes the learning of model parameters \(\) with information derived from human knowledge. Jointly optimizing over structure and model parameters is intractable, so we first select a high-quality concept space and then optimize the parameters of the bottleneck jointly with the parameter prior.

Figure 3: Overview of **Know**ledge-enhanced **B**ottlenecks (**KnoBo**) for medical image classification, comprising three main components: (1) **Structure Prior** (Sec 4.2) constructs the trustworthy knowledge bottleneck by leveraging medical documents; (2) **Bottleneck Predictor** (Sec 4.3) grounds the images onto concepts which are used as input for the linear layer and ; (3) **Parameter Prior** (Sec 4.4) constrains the learning of linear layer with parameters predefined by doctors or LLMs.

### Structural Prior

Given a background corpus \(\) that spans various documents, we aim to identify a bottleneck structure containing concepts beneficial for classifying labels \(y\). As outlined in Algorithm 1, we use the class names \(\) as initial queries to retrieve relevant documents \(^{}\). Large Language Models (LLMs) are then prompted to generate concepts using these retrieved documents as context: \(C^{}=(^{})\). These newly generated concepts are added to the bottleneck and used as new queries to retrieve additional documents. This iterative process continues to expand the bottleneck until a predetermined number of concepts \(N_{C}\) is reached. Such concept structures have high likelihood under the language model, conditioned on the background corpus, and the language model probability serves as an implicit prior.

### Bottleneck Predictor

With the structure prior from the background corpus, we optimize (1) the grounding function \(:^{d}^{N_{C}}\), which maps the input image to the concept space, and (2) a linear layer \(f:^{N_{C}}\) that projects concept predictions onto labels. In practice, we implement \(\) as a set of grounding functions: \(=\{g_{c}\}_{c C}\) where each \(g_{c}\) predicts the probability of an image \(I\) having the concept \(c\), \(P(c|I)=g_{c}()\), and \(^{d}\) is the image feature. Specifically, we derive training examples for grounding functions from a pretraining dataset of image-text pairs. We use the language model to estimate the presence of a concept in the image based on the information in the accompanying text.

**Concept Grounding.** Suppose we have a pretraining dataset \(_{}\) of image-text pairs \((I,t)}\), where \(t\) is a textual description (such as a clinical report) of the image. Based on \(t\), we can infer if a concept \(c\) is present in the image. This can be automated by prompting a large language model to generate a response indicating whether the text implies the concept. This way, we label our pretraining data as positive and negative examples for each concept \(c\), which can be used to train its grounding function. With those annotated training examples, we implement each grounding function as a binary logistic regression classifier: \(g_{c}()=(_{c}^{})\), where \(_{c}^{d}\) is the weights of grounding function and \(\) is the sigmoid activation. Finally, we form a collection of grounding functions \(=\{g_{c}\}_{c C}\) to map an image feature \(\) into \(N_{C}\) probabilities over all bottleneck concepts, with \(()^{N_{C}}\).

**Linear Layer.** Using concept probabilities \(()\) as input, we train a simple linear function \(f\) to make the final label prediction: \(=f(())=()^{}\), where \(^{N N_{C}}\) is the linear weight matrix, with \(N\) the number of classes and \(N_{C}\) the number of concepts.

### Parameter Prior

The bottleneck predictor is inherently interpretable because the parameters of the linear layer encode the affinity between labels and concepts. Therefore, we can guide the parameters based on prior knowledge, i.e., if the label \(y\) is positively related to concept \(c\) based on background knowledge, the weight \(w_{y,c}\) should be high. We hope the learned parameters do not deviate too much from this assumption, otherwise, the model may capture spurious correlations in the data.

To enforce this, we let language models define a weight matrix of priors \(_{}^{N N_{C}}\), with each element \(w_{y,c}\{-1,+1\}\) indicating the sign of a preferred correlation between the label \(y\) and concept \(c\). The prior loss is calculated as the L1 distance between between \(\) and \(_{}\):

\[_{}=}||()-_{}||_{1}\] (2)

in which we apply tanh activation on \(\) to scale the linear weights to \((-1,1)\), matching the scale of the weights in the prior matrix. This adjustment aligns the model's parameters with the expected sign of the correlations based on prior knowledge. The final loss function to train the linear layer is the sum of the cross-entropy loss and the prior loss: \(=_{}+_{}\).

In summary, we search for a structure \(C\) that is consistent with prior knowledge from a background corpus \(\) to severe as the bottleneck for the predictor \(P(y|I,C,)\). The parameters \(\) are aligned with the predefined correlations between labels and concepts identified by language models.

## 5 Experimental Setup

This section introduces (1) the confounded and unconfounded medical datasets to evaluate the robustness of our knowledge-enhanced bottlenecks (Sec 5.1), (2) the black-box and interpretable baselines for comparison (Sec 5.2), and (3) the implementation details of our method (Sec 5.3).

### Datasets

We evaluate two groups of datasets for each modality: (1) the **confounded datasets**, which aim to assess the **robustness** of models by creating splits with spurious correlations; (2) the **unconfounded datasets** are randomly split to measure the models' **performance** in natural settings.

**Confounded Datasets.** As illustrated on the left of Figure 1, we formulate the confounded datasets as binary classification tasks, where each class is confounded with one factor. The confounding combinations are reversed for in-domain (train and validation), and out-of-domain (test) splits.

The confounded datasets of **chest X-ray** are constructed from NIH-CXR  and CheXpert  with their provided attributes: (1) **NIH-sex** uses sex (male, female) as the confounding factor; (2) **NIH-age** confounds the data with age (young, old); (3) **NIH-pos** analyzes the patient's position (standing, lying down) during X-ray examinations; (4) **CheXpert-race** splits the data based on patient's race (white, black or African American); (5) **NIH-CheXpert** confounds X-rays across datasets (NIH, CheXpert).

The confounded datasets of **skin lesion** are derived from the International Skin Imaging Collaboration (ISIC): (1) **ISIC-sex** and (2) **ISIC-age** are set up similarly to the X-ray datasets mentioned previously; (3) **ISIC-site** studies lesions developed on different sites of the body (head, extremities); (4) **ISIC-color** evaluates examples with different skin colors (light, dark); and (5) **ISIC-hospital** uses instances sampled from hospitals in different cities (Barcelona, Vienna).

**Unconfounded datasets**. We evaluate 10 datasets with random splits, 5 for each modality. _X-ray_: **Pneumonia**, **COVID-QU**, **NIH-CXR**, **Open-i**, and **VinDr-CXR**. _Skin Lesion_: **HAM10000**, **BCN20000**, **PAD-UFES-20**, **Melanoma**, and **UWaterloo**.

All datasets are split into train/validation/test and ensure the validation and test set are balanced across classes. Detailed statistics and additional information on each dataset are provided in Appendix A.

**Pretraining Datasets.** The training of vision backbones and concept grounding functions utilizes datasets with image-text pairs. For X-rays, we choose MIMIC-CXR , which contains 377,110 X-ray images with accompanying clinical reports. Since there is no existing text-annotated dataset for skin lesion images, we employ GPT-4V  to generate captions (see examples in Figure 9) for a subset of 56,590 images from ISIC, without overlap of the confounded and unconfounded datasets.

### Baselines

We compare KnoBo against both black-box models and interpretable concept bottleneck models.

**Black-box Models.** We include two end-to-end fine-tuning baselines: (1) **ViT-L/14** and (2) **DenseNet121**, both pretrained on the pretraining datasets mentioned earlier. Additionally, (3) **Linear Probe** extracts visual features with the frozen ViT-L/14 encoder and learns a linear layer for classification. (4) **Language-shaped Learning** (LSL)  aims to disentangle the impact of knowledge and interpretable structure. Inspired by LSL via captioning, we finetune a ViT-L/14 with the same data used for concept grounding functions and apply a linear layer (see Appendix B.2).

**Concept Bottleneck Models.** (1) **Post-hoc CBM** (PCBM-h)  ensembles concept bottleneck models with black-box residual predictors. We let PCBM-h use the same bottlenecks as our KnoBo method; (2) **LaBo** applies language models to generate concepts, followed by the submodular selection to identify a subset that enhances performance. Following their original settings, PCBM-h and LaBo use CLIP (fine-tuned on medical pretraining datasets) to align concepts with images.

All baselines use backbones trained on the same pretraining data as our method to ensure a fair comparison. Appendix B.2 provides additional details about the baselines.

**Evaluation Metrics.** We use accuracy as the metric since all evaluated datasets are single-label classification tasks with balanced validation and test sets. For confounded datasets, we report in-domain (ID, validation), out-of-domain (OOD, test), and domain-average (mean of ID and OOD) accuracies, along with domain gaps (\(=|-|\)), where a lower \(\) indicates better robustness. For unconfounded datasets, we report test accuracy. A robust and performant model must achieve a good compromise between confounded and unconfounded datasets. For all the baselines and our KnoBo method, the checkpoints with the highest validation accuracy are evaluated on the test set.

### Implementation Details

**Pretraining of Medical CLIP.** We fine-tune OpenCLIP  (ViT-L/14 pretrained on LAION-2B ) on the pretraining medical data for each modality. Unlike previous work [18; 85; 92] that directly pairs medical images with sentences from clinical reports, we preprocess the reports by employing GPT-4  to extract short phrases. Our CLIP models perform the best for both X-ray and skin lesion datasets in zero-shot and linear probing, as shown in Table 9 in the Appendix.

**Medical Corpus.** We download 5.5 million articles from PubMed and segment them into 156.9 million snippets to serve as documents for retrieval. Alternatively, we take the medical corpus organized by MedRag, including documents from Wikipedia, StatPearls, and medical textbooks. We employ BM25  as the ranking function for document retrieval.

**KnoBo Details.** We select GPT-4 (gpt-4-0613) as the underlying LLM for retrieval-augmented concept generation (Sec 4.2). For training concept grounding functions (Sec 4.3), we opt for Flan-T5-XXL  to annotate clinical reports for each concept, considering cost-efficiency. Unless otherwise specified, KnoBo uses bottlenecks constructed from PubMed, each with 150 concepts. Figure 5 shows our prompt, and during concept generation, we apply several heuristic filters (B.3).

## 6 Results

In this section, we discuss KnoBo's performance on confounded and unconfounded medical image datasets (Sec 6.1) and analyze different knowledge resources and our model design (Sec 6.2).

    &  &  &  &  &  \\   & ID & OOD & Avg & ID & OOD & Avg & ID & OOD & Avg & ID & OOD & Avg & ID & OOD & Avg \\  ViT-L/14 & **97.0** & 30.9 & 64.0 & **97.4** & 3.2 & 50.3 & **99.7** & 2.7 & 51.2 & **89.4** & 48.2 & 68.8 & **99.9** & 0.1 & 50.0 \\ DenseNet & 91.4 & 32.1 & 61.8 & 90.6 & 15.6 & 53.1 & 99.3 & 1.0 & 50.2 & 85.0 & 55.4 & 70.2 & **99.9** & 0.2 & 50.1 \\ Linear Probe & 94.2 & 46.7 & 70.5 & 95.0 & 11.4 & 53.2 & 99.3 & 17.0 & 58.2 & 87.8 & 71.4 & 79.6 & 99.6 & 6.8 & 53.2 \\ LSL & 84.0 & 74.3 & 79.2 & 79.8 & **53.8** & **66.8** & 95.3 & 39.0 & 67.2 & 80.4 & 76.4 & 78.4 & 95.0 & 31.8 & 63.4 \\  PCBM-h & 94.2 & 45.6 & 69.9 & 95.0 & 10.8 & 52.9 & 99.3 & 17.0 & 58.2 & 88.0 & 71.4 & 79.7 & 99.6 & 8.2 & 53.9 \\ LaBo & 91.4 & 51.3 & 71.4 & 92.8 & 14.4 & 53.6 & 98.0 & 24.3 & 61.2 & 86.8 & 69.2 & 78.0 & 98.4 & 14.9 & 56.7 \\  KnoBo (ours) & 88.6 & **78.6** & **83.6** & 88.8 & 38.8 & 63.8 & 95.7 & **45.3** & **70.5** & 84.0 & **79.0** & **81.5** & 91.6 & **52.3** & **72.0** \\    &  &  &  &  &  \\   & ID & OOD & Avg & ID & OOD & Avg & ID & OOD & Avg & ID & OOD & Avg & ID & OOD & Avg \\  ViT-L/14 & **92.0** & 69.0 & 80.5 & **95.0** & 61.3 & **78.2** & **94.8** & 38.3 & 66.6 & **96.9** & 59.2 & 78.1 & 99.2 & 10.0 & 54.6 \\ DenseNet & 85.3 & 76.0 & 80.7 & 93.7 & 61.3 & 77.5 & 81.7 & 54.5 & 68.1 & 93.9 & 44.6 & 69.2 & 98.4 & 15.1 & 56.8 \\ Linear Probe & 86.0 & 69.7 & 77.8 & 92.7 & 60.7 & 76.7 & 90.2 & 37.2 & 63.7 & 90.8 & 65.8 & 78.3 & **100.0** & 27.1 & 63.6 \\ LSL & 82.7 & 78.3 & 80.5 & 90.3 & 66.0 & **78.2** & 84.3 & 50.2 & 67.3 & 87.3 & 73.1 & 80.2 & 99.6 & 27.9 & 63.8 \\  PCBM-h & 86.7 & 69.0 & 77.8 & 93.0 & 59.3 & 76.2 & 90.0 & 38.5 & 64.3 & 91.2 & 66.5 & 78.9 & **100.0** & 26.8 & 63.4 \\ LaBo & 83.0 & 69.3 & 76.2 & 91.3 & 61.0 & 76.2 & 88.0 & 39.3 & 63.7 & 86.9 & **78.9** & **82.9** & **100.0** & 8.6 & 54.3 \\  KnoBo (ours) & 84.0 & **79.7** & **81.8** & 88.0 & **67.7** & 77.8 & 80.7 & **58.8** & **69.8** & 89.2 & 75.8 & 82.5 & 88.2 & **77.5** & **82.9** \\   

Table 1: Results on 10 **confounded datasets** of two modalities (top-5 are X-ray and bottom-5 are skin lesion). We report in-domain (ID), out-of-domain (OOD), and average of ID and OOD (Avg) accuracy. The best score of each column is **bold**, and the second best is underlined.

### Main Results

**KnoBo is more robust to domain shifts.** Table 1 shows the results on 10 confounded datasets of X-ray and skin lesions. Black-box models excel at in-domain (ID) data but drop significantly on out-of-domain (OOD) data, especially in datasets confounded by hospitals/resources (NIH-CheXpert and ISIC-hospital), which can be common when collecting medical datasets [12; 81]. KnoBo outperforms baselines in OOD and domain-average accuracy by large margins, ranking top-1 in eight datasets and second-best in the other two. End-to-end models (ViT-L/14, DenseNet) exhibit larger domain gaps than linear probes, as they have more parameters to optimize performance on in-domain data and capture spurious correlations. Shaping the visual representations with knowledge (LSL) improves robustness but underperforms KnoBo, with lower ID, OOD, and average performance across most datasets. PCBM-h combines interpretable and black-box predictions but exhibits behaviors similar to black-box models with severe drops across domains. Unlike KnoBo, which uses medical documents to create one global bottleneck for each modality, LaBo builds a bottleneck for each dataset using the in-domain data, which can be biased and affected by confounding factors and performs more poorly. In summary, KnoBo mitigates the catastrophic failures in domain shifts encountered by black-box and is more robust against various confounding factors across modalities.

**KnoBo performs the best across confounded and unconfounded data.** Table 2 illustrates the performance averaged across confounded and unconfounded datasets. For both types of medical images, KnoBo achieves the best out-of-domain (OOD) and domain-average performance (Avg) with minimal domain gaps (\(\)), outperforming the strongest end-to-end baseline (ViT-L/14) by 41.8% (X-ray) and 22.9% (skin lesion) in OOD accuracy. KnoBo achieves competitive performance for unconfounded X-ray datasets, trailing the best-performing black-box model (Linear Probe) by only 0.7%. While KnoBo is less competitive on skin lesion datasets due to the lack of large-scale pretraining data for accurate concept grounding, it still maintains performance comparable to the baselines. By calculating the mean accuracy across both confounded and unconfounded datasets, KnoBo ranks top across all models, confirming that our knowledge-enhanced, interpretable approach is a promising direction for building more robust and performant systems for medical imaging.

    &  &  \\   & ID & OOD & \(\) & Avg & Unconfd & Overall & ID & OOD & \(\) & Avg & Unconfd & Overall \\  ViT-L/14 & **96.7** & 17.0 & 79.7 & 56.8 & 70.2 & 63.5 & **95.6** & 47.6 & 48.0 & 71.6 & **84.3** & 77.9 \\ DenseNet & 93.2 & 20.9 & 72.4 & 57.1 & 66.0 & 61.5 & 90.6 & 50.3 & 40.3 & 70.4 & 71.0 & 70.7 \\ Linear Probe & 95.2 & 30.7 & 64.5 & 62.9 & 73.8 & 68.4 & 91.9 & 52.1 & 39.8 & 72.0 & 82.8 & 77.4 \\ LSL & 86.9 & 55.1 & 31.8 & 71.0 & 67.0 & 69.0 & 88.9 & 59.1 & 29.8 & 74.0 & 77.2 & 75.6 \\  PCBM-h & 95.2 & 30.6 & 64.6 & 62.9 & **74.7** & 68.8 & 92.2 & 52.0 & 40.1 & 72.1 & 81.7 & 76.9 \\ LaBo & 93.5 & 34.8 & 58.7 & 64.2 & 72.1 & 68.1 & 89.9 & 51.4 & 38.4 & 70.6 & 80.0 & 75.3 \\  KnoBo (ours) & 89.7 & **58.8** & **30.9** & **74.3** & 73.1 & **73.7** & 86.0 & **70.5** & **14.1** & **78.3** & 78.1 & **78.2** \\   

Table 2: Averaged results across all datasets, including in-domain (ID), out-of-domain (OOD), domain-gap (\(\), lower is better), and mean of ID and OOD (Avg) accuracy for confounded datasets. For unconfounded datasets (Unconfd), we report test accuracy. Overall performance is calculated as the mean of the Avg and Unconfd, the overall tradeoff between data conditions.

    &  &  \\   & Confd & Unconfd & Overall & Diversity & Confd & Unconfd & Overall & Diversity \\  Prompt & 72.9 & 72.8 & 72.9 & 0.542 & **78.4** & 77.0 & 77.7 & 0.332 \\  Textbooks & 72.0 & 72.9 & 72.4 & 0.585 & 77.5 & 78.3 & 77.9 & 0.350 \\ Wikipedia & 72.8 & 72.7 & 72.8 & 0.542 & 77.6 & 77.9 & 77.8 & 0.356 \\ StatPearls & 73.4 & 72.0 & 72.7 & 0.598 & 77.1 & **79.1** & 78.1 & **0.379** \\ PubMed & **74.3** & **73.1** & **73.7** & **0.619** & 78.3 & 78.1 & **78.2** & 0.341 \\   

Table 3: Comparison of concept bottlenecks built from different knowledge sources. Prompt is our baseline without retrieving documents for concept generation. We report the accuracy of confounded (Confd, average over ID and OOD), unconfounded (Unconfd) datasets, and the overall performance of all datasets. Diversity measures the difference between the concepts in a bottleneck.

### Analysis

In this section, we compare the bottlenecks constructed from different knowledge sources. We evaluate the impact of each KnoBo component on the final performance, including bottleneck size, concept grounding function, and parameter prior. Additional analyses are available in Appendix C.

**Knowledge Sources.** Besides the empirical results on confounded and unconfounded datasets, we measure the diversity of bottleneck \(C\) as \((C)=-|C|}_{c_{i} C}_{c_{j} C}^{i  j}(1-(c_{i},c_{j}))\), where the \(()\) is the cosine similarity of concept features encoded by sentence transformer . The **Diversity** computes the distance between each concept and every other concept in the bottleneck. Table 3 compares different knowledge sources. The retrieval-augmented bottlenecks perform better than those generated by prompting, especially for skin lesions, where more specific knowledge is required because prompting lacks diversity. Across both modalities, PubMed is the best overall, performing better for the X-ray modality than other knowledge sources and among the best for skin lesion modalities. Moreover, shown in Table 6, our retrieval-augmented concepts are attributable, which allows doctors to verify the source of knowledge.

**Human Evaluation on Bottlenecks.** In evaluations by two medical students, information from all knowledge sources is rated as highly relevant and groundable. Two medical students evaluated the quality of bottlenecks using two metrics: (1) **Relevance** measures the concept's relevance to diagnosing diseases on a scale from 1 (not at all relevant) to 4 (mostly relevant), and (2) **Groundability** assesses the verifiability of the concept from the image on a scale from 1 to 4. We evaluated 30 randomly sampled concepts from each bottleneck. Table 4 presents these metrics for bottlenecks constructed from five different knowledge sources. While all bottlenecks show good relevance, groundability scores are lower, reflecting the challenge of deriving visual concepts from text-only data.

**Bottleneck Size.** Figure 4 compares KnoBo and linear probes while varying the number of concepts/features. KnoBo consistently outperforms linear probes across all metrics when given the same quota of features, and KnoBo can obtain better performance with fewer features. This indicates that interpretable concept scores have more effective priors than black-box visual features.

**Ablations.** Table 5 summarizes experiments ablating major components of our approach. Row 2 shows the performance of using dot-products from prompted CLIP models as concepts, which

    **Knowledge** \\ **Source** \\  } &  &  \\   & X-ray & Skin & X-ray & Skin \\  Prompt & 3.83 & 3.93 & 3.03 & 3.00 \\  Textbooks & 3.70 & 3.80 & 2.90 & 3.27 \\ Wikipedia & 3.80 & 3.67 & 2.83 & 3.33 \\ StatPearls & 3.87 & 3.80 & 2.70 & 2.97 \\ PubMed & 3.70 & 3.83 & 2.77 & 3.20 \\   

Table 4: Relevance and Groundability of concepts in bottlenecks generated from different resources, as evaluated by student doctors.

Figure 4: Ablation of **bottleneck sizes** on X-ray datasets. The x-axis is the number of randomly selected concepts (KnoBo) or visual features (Linear Probe). \(+\)prior means adding parameter prior.

    &  &  \\   & ID & OOD & \(\) & Avg & Unconfd & Overall & ID & OOD & \(\) & Avg & Unconfd & Overall \\  KnoBo & 89.7 & **58.8** & **30.9** & **74.3** & 73.1 & **73.7** & 86.0 & **70.5** & 14.1 & **78.3** & 78.1 & **78.2** \\  w/o \(\) & 87.8 & 51.5 & 36.3 & 69.6 & 70.1 & 69.9 & 83.7 & 69.4 & **11.5** & 76.6 & 70.2 & 73.4 \\ w/o \(_{}\) & **91.6** & 48.1 & 43.5 & 69.8 & **73.6** & 71.7 & **86.5** & 69.1 & 16.6 & 77.8 & **78.4** & 78.1 \\   

Table 5: Ablation studies on concept grounding (\(\); Sec 4.3) and parameter prior (\(_{}\); Sec 4.4).

markedly reduces performance. This shows the importance of knowledge grounding in ensuring KnoBo's effectiveness. However, this step can be simplified as more advanced medical foundation models are available. Row 3 shows performance omitting the parameter prior. It is an important mechanism for constraining the final learning phase, resulting in consistent OOD improvements. This is also reflected in Figure 4, where with the parameter prior, KnoBo performs better on OOD splits while decreasing on in-domain as the parameter prior constrains the model to rely on data.

## 7 Conclusion and Limitation

In this paper, we analyze domain-shift problems in medical image analysis and identify a missing medical deep image prior as a main contributor to poor performance. To address this, we introduce knowledge-enhanced bottlenecks (KnoBo) to integrate knowledge priors from medical documents. Across two medical image modalities under various domain shifts, KnoBo significantly improves robustness over black-box baselines.

**Limitation.** KnoBo assumes the availability of medical multimodal datasets, limiting applications to rare conditions. While our work improves robustness, medical experts do not fail in these ways, and they should be used in conjunction with models. Our preliminary work with PubMed suggests it is a valuable resource for developing medical models, and future research can explore how to utilize such fruitful knowledge resources more effectively.