# Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention

Matteo Pagliardini*

EPFL

matteo.pagliardini@epfl.ch &Daniele Paliotta*

University of Geneva

daniele.paliotta@unige.ch &Martin Jaggi

EPFL

martin.jaggi@epfl.ch &Francois Fleuret

University of Geneva

francois.fleuret@unige.ch

###### Abstract

Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention--which is the only component scaling quadratically w.r.t. the sequence length--becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementation concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attention often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by \(2.0\) and \(3.3\) for sequences of respectively \(8k\) and \(16k\) tokens.

+
Footnote †: * Equal contribution.

## 1 Introduction

Many methods have been developed to mitigate the quadratic cost of self-attention in Transformers (Vaswani et al., 2017). Some methods attempt to linearize the attention (Beltagy et al., 2020; Wang et al., 2020) by for instance linearizing the softmax operator to take advantage of the associativity of matrix products (Katharopoulos et al., 2020). Other methods rely on a predefined sparse masking of the attention matrix, e.g. to constrain the attention to a local temporal neighborhood (Zaheer et al., 2020; Child et al., 2019). While the structure is fixed, it is assumed that information from arbitrary locations in the sequence can still flow through this structure over several layers. All those methods impose static implicit or explicit constraints over the attention matrix.

Another promising line of work consists in computing a dynamic modulation of a sub-part of the attention matrix. They are based, for instance, on dropping keys and queries (Kim et al., 2022) or using geometric hashing of the keys and queries to identify linear cost sub-blocks of the attention matrix that carry most of the weight (Kitaev et al., 2020).

The promising theoretical computational complexity of these methods contrasts with the fact that today's most successfully deployed practical models instead rely on vanilla attention, in part thanks to the efficiency of FlashAttention (Dao et al., 2022). This implementation is mathematically identical to the vanilla attention proposed by Vaswani et al. (2017) in their seminal paper, but trades in additional compute for less memory I/O.While still avoiding a memory footprint quadratic with the sequence length, it delivers practical speedups of over \(5\) compared to a naive implementation.

Using an attention layer in an autoregressive model--which has been key in the recent remarkable AI breakthroughs--requires to make it causal. This is achieved by applying a mask to the attention matrix, so that information cannot flow from the future to the past during training.

While FlashAttention can deal with vanilla causal masks, it does not provide enough flexibility to be used for situations where the causal attention mask is not perfectly regular, that is, lower triangular. This in particular prevents using it for models that dynamically drop keys and queries or rely on geometric hashing, which results in irregular causal structures as illustrated in Fig. 1 and Fig. 2.

We propose an extension of FlashAttention--Sparse Causal Flash Attention (SCFA)-- that addresses this constraint. Our contribution is threefold:

* We present the SCFA GPU kernel, which relaxes the constraint that the causal mask has to be triangular. This kernel can handle any sparsity pattern that can be expressed with a range of keys per query, and any causal masking in the resulting sub-blocks. See SS 3.
* We show that SCFA permits to revisit the promising paradigm of dynamic hash-based attention. We devise an algorithm that builds upon the fundamental idea of Reformer (Kitaev et al., 2020) to restrict the computation of the attention matrix over 'hash collision blocks', but avoids both the high computational cost, and the approximate coverage of the hash collisions. See SS 3.2.
* We propose a new approach implemented with SCFA that reduces computation by dynamically selecting, for each head, keys and queries to be removed from the attention operation, superseding existing methods that limited pruning to entire heads or entire queries/keys, due to the lack of an efficient fine-grained kernel implementation. See SS 3.1.

Experimental evaluations show that SCFA can efficiently be used for a variety of sequence modeling tasks, and that our open-source implementation in the Triton language and compiler (Tillet et al., 2019) significantly outperforms FlashAttention as we increase the sparsity and for longer sequences. Moreover, unlike the hash-based attention introduced in Reformer (Kitaev et al., 2020), our hash-based SCFA not only implements the exact computation, but also has a faster runtime (see SS 4.2). Finally, we show that a prototype of query and key dropping can be implemented thanks to SCFA, and that the computational reduction is proportional to the fraction of query-key pairs dropped (see SS 4.3).

## 2 Related work

State-of-the-art sequence models have very high computational requirements. As a consequence, a lot of effort has been invested into developing methods to reduce the memory footprint in Transformers. Many efficient Transformer variants have been developed, with the main goal of taming the quadratic complexity of the attention mechanism (Tay et al., 2020). Several methods rely on kernelized attention (Katharopoulos et al., 2020; Choromanski et al., 2020), while others endow the Transformer with some auxiliary memory to increase the context (Wu et al., 2022; Borgeaud et al., 2021).

In many cases, leveraging sparsity in the attention matrix has proven useful. The Sparse Transformer (Child et al., 2019) works with a factorized sparse representation of the attention. They employ several sparse attention patterns, where each output position only computes weightings from a subset of input positions.

The Reformer (Kitaev et al., 2020) uses locality-sensitive-hashing (LSH) to sparsify the attention matrix and allow queries to restrict their context window to keys that collide with the same hash. However, to allow GPU-efficient processing, complex machinery has to be developed where the queries and keys are split into fixed-sized chunks, with the attention being applied only within the chunk and the immediate neighbor.

FlashAttention introduced by Dao et al. (2022) has recently gained a lot of popularity as an efficient, IO-aware exact attention implementation. FlashAttention uses tiling to avoid materializing the full attention matrix on slow GPU HBM, splitting the computation over blocks of query, key, and value vectors. FlashAttention has already reached wide adoption, as it's now available directly in Pytorch as of version 2.0. Additionally, FlashAttention supports very efficient block-sparse structures.

Bigbird (Zaheer et al., 2020) and Longformer (Beltagy et al., 2020) are two more variants that work with sparsified version of the attention matrix. Both approaches rely on a fixed structure that is independent of the input values, using a combination of local, global, and random attention.

**Hash Attention.** When computing the attention matrix for a \(T D\) query tensor \(\) and a \(T D\) key tensor \(\), we consider the matrix of dot-products \(^{}\), which can become impractical to compute for very long sequences. However, we are only interested in the row-wise \((^{})\), meaning that the contribution of the keys to every query is dominated by the ones with the highest similarity. Thus, restricting the attention computation to queries and keys with high similarity is a natural choice to reduce the computation.

Hash attention, introduced in the Reformer (Kitaev et al., 2020), allows to quickly select the closest key vectors for each query using locality-sensitive-hashing (LSH). In general, the LSH mechanism assigns a hash code to vectors with the requirement that vectors that are close in space are mapped to the same hash with high probability. For the hash attention, the Reformer assumes a shared query-key space (\(=\)). After computing the hashes, the queries are sorted according to their hash bucket. In the sorted attention matrix, pairs that fall into the same bucket cluster near the diagonal. In order to implement the LSH-attention scheme efficiently on GPU, the Reformer splits the queries into fixed-sized chunks. Queries belonging to the same chunk can attend to each other and one chunk back. This results in a suboptimal mechanism where there is no guarantee that the attention will capture exactly all of the elements that belong to the same bucket (See Fig. 4).

**FlashAttention.** The standard self-attention operation consists of multiplying a \(T D\) query tensor \(\) by a \(T D\) key tensor \(\), to obtain a matching score matrix, which is then rescaled and row-normalized with softmax, to get a \(T T\) attention matrix \(\). This matrix is then multiplied by a \(T D^{}\) value tensor \(\) to obtain the final result. This is the core operation in a standard _Multi-Head Attention_ layer, where additional operations take place to compute \(\), \(\), and \(\) from the layer's input, and multiple instances of this processing take place in parallel.

Figure 1: **Proposed sparsification of the attention matrix for a given attention head. In each depicted attention matrix, black areas indicate coefficients to compute, patterned areas those forced to zero due to the causal masking, and white areas coefficients that are ignored. We consider two main dynamic strategies to sparsify the left attention matrix. The QK-sparse attention consists of dropping some keys and queries (top, the discarded keys and queries are indicated in red), and the Hash-sparse attention computes a hash code for each key and each query, and restricts the attention matrix to blocks of keys and queries of same hash code (bottom, the three hash values are indicated for each key or query with the colors blue/green/red). In both cases, the attention operation must be able to deal with sub-blocks of the attention matrix with a non-triangular causal mask.**

The two key contributions of FlashAttention are (1) to compute the attention matrix block-wise, to minimize the transfer of keys and queries to the cache memory as much as possible, and (2) to compute the attention matrix on the fly during both the forward and the backward passes, which is faster than retrieving it from memory, and avoids a memory footprint quadratic with the sequence length \(T\).

For the generalization that is of concern to this article, we focus on the block computation. In the implementation of FlashAttention, causal masking is done by using the row and column indexes of the blocks, and the row and column indexes of the keys and queries in individual blocks: attention blocks are computed fully for any block with a query index strictly larger than the key index. For the blocks for which the query index is equal to the key index, a regular lower triangular mask is applied. This is illustrated on Fig. 2, bottom left.

## 3 Method

We develop an efficient CUDA kernel written in Triton (Tillet et al., 2019) that maintains the careful memory management of FlashAttention but can handle a causal structure defined through an arbitrary indexing of the keys and the queries. In the case where this indexing consists of a binary decision to drop or not the head of a query/key, this corresponds to our QK-sparse kernels as described in SS 3.1. In the case where the indexing corresponds to bucket indices e.g. obtained from hashing, this corresponds to our Hash-sparse kernel described in SS 3.2.

**Notations.** Input tensors for attention as in Vaswani et al. (2017) are of shape \(B H T D\), with \(B\) being the batch size, \(H\) the number of heads, \(T\) the sequence length, and \(D\) the dimension per head. In the following we take the view of a single head and instead consider a query tensor \(\) of shape \(T_{Q} D\), and a key \(\) and value \(\) tensors of shapes \(T_{KV} D\). The algorithms described below will be run in parallel for all elements of the Cartesian product \(B H\). We split tensors into blocks: \([_{0},,_{m}]\), \([_{0},,_{n}]\). We define a tile \(_{i,j}_{i}_{j}^{}\), which corresponds to the dot products of a subpart of the attention matrix (see Fig. 2).

Figure 2: **SCFA computation patterns. In each depicted attention matrix, black areas indicate coefficients to compute, patterned areas are those forced to zero due to the causal masking, and white areas coefficients that are ignored. The red squares in the bottom matrices show the tiles actually computed by our SCFA kernel. In the regular case (left), this coincides with the behavior of FlashAttention. However, in the case of irregular causal masking due to keys/queries dropping (center) or in the case of irregular causal masking and band block sparsity due to hashing (right), FlashAttention does not provide means to compute a fine-grain subset of the attention matrix.**

### QK-Sparse Attention

**Shrinking the attention matrix.** Our QK-sparse attention kernel is best summarized in the first row of Fig. 1. Independently for each head, we decide to keep or drop keys and queries. We then remove dropped keys and queries to create smaller \(^{c}\), \(^{c}\), and \(^{c}\) tensors. Through this reduction we are left with a smaller attention matrix \(^{c}\) which still has a causal structure in that indices for the queries and keys are increasing monotonically.

**Leveraging non-triangular causal attention structure.** Despite the advantageous structure of the smaller attention matrix, existing implementations fail to take advantage of it. Especially, as shown in Fig. 2 bottom-left, FlashAttention can leverage the causal structure when the causal mask is triangular, but does not support any other shape. In the forward pass, FlashAttention is, for each block of queries \(_{i}\), processing blocks of keys \(_{j}\) one after the other, moving along a row of tiles: \(_{i,0},,_{i,n}\). Causality dictates that it is unnecessary to process a tile \(_{i,j}\) when \(i<j\). We cannot follow this rule anymore when working with compact representations. To leverage the causal structure of \(_{c}\), we build a new kernel which gets as additional input vectors \(^{idx}^{T_{Q}}\) and \(^{idx}^{T_{K^{}}}\) representing the indices of the queries and keys in the original uncompressed tensors. Those are similarly split into blocks: \(^{idx}[_{0}^{idx},,_{m}^{idx}]\). The condition for a tile \(_{i,j}\) to be unnecessary to compute is now to have \((_{i}^{idx})<(_{j}^{idx})\). When processing a block of queries \(_{i}\), we iterate over the key indices \(_{0}^{idx},,_{n}^{idx}\) to find the index \(j_{stop}\) of the first block satisfying that condition. We then know we need to process the tiles \(_{i,j}\) for \(j[0,j_{stop}[\). Within each tile \(T_{i,j}\), we in addition apply a local causal mask by comparing indices in \(_{i}^{idx}\) and \(_{j}^{idx}\). By computing \(j_{stop}\) in such a way we can leverage the causal structure and have runtimes matching those of FlashAttention. The backward pass can be adapted in a similar fashion, see App. B for more details.

**Overhead.** Computing \(^{c}\), \(^{c}\), and \(^{c}\) requires sorting and allocating new tensors. Moreover, as we drop keys and queries for every attention head, and for every sequence in the minibatch, we are forced to consider the largest sequence of non dropped keys/queries and use padding. However, while reordering and reshaping tensors can be costly, this overhead grows linearly with the sequence length and is largely compensated for larger sequences as we show in SS 4.3.

**Edge cases.** Dropping keys and queries can result in having stranded queries with no keys. This behaviour is undefined and results in NaNs when using the FlashAttention and naive Pytorch implementations. We solve this issue by modifying how softmax statistics are accumulated during the forward and backward passes and ensure stranded queries default to \(\) vectors. see App. B for more details.

### Hash-Sparse Attention

**Restructuring attention based on hashes.** Independently for each head, we associate a bucket identifier to each key and query. We then need to reorder \(,,\) by sorting them along the sequence length dimension. As shown in the bottom row of Fig.1, this results in clusters of keys and queries with a similar hash index close to the diagonal. If the sorting is stable, i.e. it preserves ordering of queries and keys when the hash index is the same, then those blocks have a local causal structure in which the original indices (original position in the sequence) of keys and queries is a monotonic function within the block. This brings us in a case very similar to the previous one in section 8.1, in that we now have the same structure but scattered by blocks within the full attention matrix.

**Taking advantage of the new structure.** We would like to take advantage of the block structure and only compute attention for queries and keys falling into the same block while at the same time respecting causality. We adapt the FlashAttention kernel in a very similar way as for our QK-sparse kernel. We now provide additional bucket indices \(^{hash}\) and \(^{hash}\) to our kernel. Based on those hash indices, we now find not only the stopping index \(j_{stop}\) but also a starting index \(j_{start}\). \(j_{start}\) is the first index for which some of the indices in \(_{i}^{hash}\) are present in \(_{j}^{hash}\), \(j_{stop}\) is the first index for which all indices in \(_{j}^{hash}\) are strictly larger than indices in \(_{i}^{hash}\). In a second step we refine \(j_{stop}\) now based on the indices \(^{idx}\) and \(^{idx}\), the updated \(_{stop}\) is the last index \(j[j_{start},j_{stop}[\) for which \((_{i}^{idx})(_{j}^{idx})\). As shown in the last column of Fig. 2, we then only compute tiles \(_{i,j}\) for \(j[j_{start},_{stop}]\). As for the QK-sparse method, we use \(^{idx}\) and \(^{idx}\) to apply a causal mask locally for each tile. In addition to the causal mask, we use \(^{hash}\) and \(^{hash}\) to mask interactionsbetween keys and queries of different buckets. See App. B for details and to see how to adapt the backward pass in a similar fashion.

**Overhead.** As for the previous method, sorting and re-ordering \(\), \(\) and \(\) is inducing some overhead increasing linearly with the sequence length. As shown in our experiments in SS 4.2, this overhead is by large compensated for as the sequence length increases.

## 4 Experiments & Results

In this section we present our experimental setup and results. We show that (i) unlike naive implementations using existing libraries, our dynamic sparsity attention schemes can significantly improve over the FlashAttention runtime, (ii) this still holds in real-world sequence modeling tasks after factoring in all the non-attention operations, and (iii) it is possible to match--and sometimes outperform--the baselines in terms of perplexity while significantly gaining in speed.

### Experimental Setup

**Datasets.** We test our hash-based sparsity scheme on MNIST (LeCun et al., 1998) for autoregressive image generation, enwik8 (Hutter, 2012), and OpenWebText2 (Gao et al., 2020). We experiment with QK-dropping based sparsity on OpenWebText2.

**Models & Baselines.** For our language modeling experiments on OpenWebText2, we use a base autoregressive transformer architecture with \(12\) layers, a hidden size of \(768\), \(12\) heads of \(64\) dimensions each. For experiments on sequence length \(T=8192\), we use a batch size of \(96=4 8 2\) (batch size \(4\) with \(8\) accumulation steps and data parallelism over \(2\) node). When \(T=16384\) we use a batch size of \(30=2 5 3\). The resulting models are of around \(122\)M parameters. The goal not being to outperform the state-of-the-art perplexity, we train for \(15k\) iterations. The attention modules used are either using FlashAttention for the baselines or one of our sparse kernels for our methods. To ensure a fair comparison, and similarly to Kitaev et al. (2020), we set the keys equal to normalized queries for all of our models. See App. B for more details.

**Hardware.** All of our timing experiments with random tensors are done on NVIDIA A100 GPUs, using bfloat16. For our language modeling tasks on OpenWebText2, we trained using data-parallelism on two or three A100s for experiments with sequence lengths of respectively \(8192\) and \(16384\). When comparing runtimes in Fig 6 and Fig. 8, we normalize the times by multiplying by the

Figure 3: **Comparing several hash-based sparse attention implementations with FlashAttention.** Similarly to QK-dropping-based sparsity in Fig. 7, due to the non-triangular causal mask resulting from re-ordering the tensors based on the hash buckets (see Fig. 1), a naive implementation would force the computation of the entire attention matrix before applying a custom mask. This results in very large runtimes independent of the number of buckets. On the other hand, our implementation modifies the basic FlashAttention method to compute only what is required. While there is a cost to reordering the tensors based on the hash buckets, this cost is largely compensated for as the number of buckets \(nb\) increases, and as the sequence length increases.

number of GPUs used. Comparisons with the Reformer are performed on a single A100 or a single NVIDIA RTX 4090 GPU.

### Hash-based Attention

**Hashing mechanism** For our experiments, we adopt the same hashing procedure as Kitaev et al. (2020). Namely, we use a shared query-key space, and we disallow queries to attend to themselves. We also adopt the LSH scheme from Andoni et al. (2015). This allows us to pick the number of unique hash codes. We refer to _bucket_ as the set of vectors that map to a certain hash.

**Runtime performances in a vacuum.** We test our implementation with different numbers of buckets \(nb\) and random keys, queries, and values. In these tests, we assume a hash bucket is provided for free for each head of each key and query (they are sampled uniformly at random (torch.randint(0,nb)). In practice, runtime experiments on sequence modeling tasks show that obtaining the buckets can be cheap and in no way prevents us from improving the attention runtime (see Fig. 6). We compare with causal FlashAttention over the entire sequence. Importantly, to ensure a fair comparison, we take into account pre-processing and post-processing steps required to reshape the tensors for both methods. For our method this includes stable sorting by bucket index and transposing tensors, for the baseline only the transposition is required, see App. B.2 for detailed code. Fig. 3.b summarises our findings. We observe large improvements in runtime as the number of buckets \(nb\) and the sequence length increases.

**Language modeling on OpenWebText2.** For sequences of length \(T=8192\) and \(T=16384\) we train transformer language models using FlashAttention (F-LM), and identical models replacing only the FlashAttention by our Hash-based sparse attention (H-LM) using \(nb=16\) hash buckets. In Fig. 6 we see that it takes the same number of iterations for H-LM and F-LM to reach a given perplexity. However, H-LM iterations are \(1.8\) and \(2.3\) faster for respectively \(T=8192\) and \(T=16384\). As a result, H-LM models reach a given perplexity much faster than their F-LM counterpart. Interestingly, we observe the H-LM models gain speed during training, see App. C for additional details.

**Comparison with Reformer.** We compare the speed and performance of our hash-sparse implementation with the Reformer hashed attention. For all comparisons, we always equalize the average bucket size. Results are summarized in Fig. 4. Benchmarks with random inputs show that both our hash-sparse implementation and the Reformer, as expected, are linear with respect to the sequence length (Fig. 4.a). However, we still achieve a significant speedup thanks to our more efficient kernel. More importantly, Fig. 4.b shows that the fixed attention structure imposed by the Reformer does not allow to capture all of the hash collisions, with the coverage decreasing steeply as the sequence length increases. On the contrary, our method is exact and covers every bucket collision in the attention

Figure 4: **Comparing forward runtimes of attention modules alone. Fig.(a):** Reformer attention ensures a linear computational complexity w.r.t. the sequence length, outperforming FlashAttention for longer sequences. **Fig.(b):** However, due to the fixed attention structure, the Reformer misses an increasing fraction of hash collisions. Our approach outperforms both methods and maintains \(100\%\) exact coverage of collisions for all sequence lengths. See App. B.3 and App. C for more details.

Figure 5: **Comparing models using Reformer attention vs our Hash-sparse attention.** On the simple sequential MNIST task (predicting pixels as a sequence), we obtain a comparable perplexity as the Reformer. On enwik8 character language modeling, with T=\(4096\), we outperform the Reformer model by a margin.

matrix. This is reflected in Table 5: our hash-sparse attention layer outperforms the Reformer attention even for shorter sequences.

### Query/Key-Dropping Based Attention

**Q/K-dropping mechanism used.** We show that naively dropping heads for each key and query at random can already yield competitive results while significantly improving the runtime. While better dropping schemes could be devised, they are outside of the scope of this work.

**Runtime performances in a vacuum.** We test our implementation with different sparsity ratios, corresponding to the probability of dropping some head associated to a given key or query. We assume that the tensors indicating the dropping of each head of each query and key are given for free, along with some random key, query, and value tensors. To ensure a fair comparison, we take into account pre-processing and post-processing steps required to reshape the tensors for both methods, see App. B for more details. For our approach, we hope reducing the size of the key, query and value tensors and computing the attention on those would be faster than using FlashAttention over the entire sequence. For this, the time gained by computing the attention on smaller tensors should be larger than the overhead of re-ordering tensors to build those smaller tensors. In Fig. 7.a, we show a naive implementation using existing PyTorch functionalities only starts to provide a speedup when dropping more than \(70\%\) of the keys and queries. Fig. 7.b shows that using our proposed implementation provides significant speedups even at relatively low sparsity levels. The linearly increasing cost of reshaping the tensors is rapidly compensated by large gains over the quadratic cost of self-attention.

**Language modeling on OpenWebText2.** For sequences of length \(T=8192\), we train transformer language models using FlashAttention (F-LM), as well as identical models replacing only the FlashAttention by our Q/K-dropping sparse attention (D-LM). We train with several sparsity ratios, dropping \(30\%\), \(50\%\), and \(70\%\) of the heads of keys and queries at random. In Fig. 8 we observe that while high sparsity can negatively affect the perplexity, lower sparsity D-LM models are matching F-LM models in perplexity per iterations, while training nearly twice as fast. Importantly, the dropping pattern is not static. An interesting approach similar to curriculum learning in which we start the training with very large sparsity and reduce it linearly during training is studied in App. C.

Figure 6: **Training Language Models (LM) on OpenWebText2 (Gao et al., 2020) using our hash-based sparsity (H-LM) or FlashAttention over the entire sequence (F-LM). We train on sequences of length \(8192\) and \(16384\) and use \(16\) buckets for all of our H-LM models. We show that it is possible to use our proposed hash-based sparsity to significantly gain in speed while not compromising the perplexity. In Fig.(a) we see for both sequence lengths the perplexity decreasing similarly as a function of the iteration. In fact, H-LM even slightly outperform the baseline. Fig.(b): H-LM reach lower perplexity much faster than their F-LM counterpart. Fig.(b) and (c): H-LM models are significantly faster than F-LM models for a given sequence length. The gap widens as the sequence length increases.**

Figure 8: **Training Language Models (LM) on OpenWebText2 (Gao et al., 2020) using random Query/Key dropping based sparsity (D-LM) or FlashAttention over the entire sequence (F-LM).** Dropping keys and queries randomly is naive and our point here is not to show that this approach is a good way to use the proposed Q/K-sparsity attention, rather we want to demonstrate that it is possible to significantly gain in speed while not losing too much in perplexity—even with a naive approach, and in a very dynamic way (two sequences are allowed to have completely different dropping patterns). For all methods we train over sequences of \(8192\) tokens. **Fig.(a):** While dropping large portions of keys and queries slows down the decrease of perplexity per iteration, dropping \(30\%\) seems to match the baseline F-LM. **Fig.(b)** Our method is significantly faster to reach a given perplexity. Interestingly, more sparsity does not necessarily mean decreasing the perplexity faster. **Fig.(b) and (c):** Using our Q/K-sparse implementation we train significantly faster than the baseline method.

Figure 7: **Runtimes of the full Flash-attention of Dao et al. (2022) and several implementations of Query/Key dropping based sparsity.** For this figure we show total times for the forward and backward passes. For sparse methods, we drop at random a percentage of keys and queries, this percentage is indicated on the right of each curve. **Fig.(a):** A naive implementation consisting in creating compact representations of the key, value, and query tensors by removing dropped keys and queries. As a result, the attention matrix is no longer triangular (see Fig. 1). We call the PyTorch scaled_dot_product_attention method with a custom but still causal mask. The non-triangular mask prevents FlashAttention to be used and only dropping more than \(70\%\) of the keys and queries seems to improve the runtime over attending the entire sequence using FlashAttention. **Fig.(b):** Our modification of FlashAttention allows to improve over the runtime. Similar to the naive implementation, reshaping the tensor induce an overhead which compensates the speed gain for shorter sequences. However, this offset is compensated by a strong margin as the sequence length increases. Our implementation allows significant gains over FlashAttention even for low levels of sparsity. The detailed runtimes for the forward and backward passes can be found in App. C.

Conclusion

We develop and validate an efficient kernel that can make sparse attention based on dynamic patterns very fast. We hope that our contribution will inspire the community to research dynamic attention patterns in a way that is less constrained by a tight computational budget.

The computational cost of large attention models remains both a practical issue in scaling up to very large contexts, and a fundamental research question to close the gap between the energy usage of biological systems to that of GPU systems able to run very large models. Dynamically modulating the computation is an obvious direction to address this challenge.

## 6 Acknowledgments

The authors acknowledge support from the Swiss National Science Foundation under grant number CR- SII5-193716 - "Robust Deep Density Models for High-Energy Particle Physics and Solar Flare Analysis (RODEM)". We also thank Igor Krawczuk for interesting discussions and suggesting using Triton.