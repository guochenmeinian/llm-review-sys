# Towards Next-Level Post-Training Quantization

of Hyper-Scale Transformers

Junhan Kim, Chungman Lee, Eulrang Cho, Kyungphil Park,

Ho-young Kim, Joonyoung Kim, Yongkweon Jeon

Samsung Research

{jun_one.kim, chungman.lee, dragwon.jeon}@samsung.com

Equal Contribution, \({}^{}\)Corresponding Author

###### Abstract

With the increasing complexity of generative AI models, post-training quantization (PTQ) has emerged as a promising solution for deploying hyper-scale models on edge devices such as mobile and TVs. Existing PTQ schemes, however, consume considerable time and resources, which could be a bottleneck in real situations where frequent model updates and multiple hyperparameter tunings are required. As a cost-effective alternative, learning-free PTQ schemes have been proposed. However, the performance is somewhat limited because they cannot consider the inter-layer dependency within the attention module, which is a significant feature of Transformers. In this paper, we thus propose a novel PTQ algorithm that balances accuracy and efficiency. The key idea of the proposed algorithm called _aespa_ is to perform quantization layer-wise for efficiency while targeting attention-wise reconstruction to consider the cross-layer dependency. Through extensive experiments on various language models and complexity analysis, we demonstrate that _aespa_ is accurate and efficient in quantizing Transformer models. The code will be available at [https://github.com/SamsungLabs/aespa](https://github.com/SamsungLabs/aespa).

## 1 Introduction

Model size has been gradually growing, resulting in deep generative models such as diffusion  and large-scale language models (LLMs)  becoming more mainstream; the trend of AI is transitioning from discriminative models to generative models with numerous parameters in trillions. With the explosive growth in model complexity (parameters), the performance of AI models has been advancing and is now approaching or even exceeding human intelligence levels. However, this growth in scale has resulted in a corresponding increase in computational costs, which necessitates the efficient processing and compression of AI models. Interestingly, one attempts to expand the complexity of AI models to scale up performance, whereas the other aims to compress models to reduce cost.

Quantization is a promising solution and indispensable procedure facilitating the efficient deployment of AI models on devices that mainly support fixed-point arithmetic. By reducing the precision of weights, the memory bandwidth requirements can be relieved, and the embarrassing parallelism of quantized models can be SIMDified using highly efficient vector processing units such as NPU. To minimize the inevitable performance degradation caused by quantization, we can choose one of two approaches: quantization-aware training (QAT)  and post-training quantization (PTQ) . Considering the model complexity and required resources (_e.g.,_ training costs and available datasets), QAT is not practical for compressing models with billions of parameters. Consequently, recent quantization studies on hyper-scale Transformer  models have focused more on PTQ.

Although existing PTQ schemes have successfully quantized relatively small-scale models (_e.g._, ResNet) [23; 10; 18; 6; 11], they have difficulty handling large-scale models because of their time and space complexity. As a cost-effective alternative, learning-free algorithms have been proposed recently [7; 13; 19], but their performance is somewhat limited because they do not consider the inter-layer dependency and are reliant on the nearest rounding. There is an accuracy-efficiency trade-off; thus, we aim to bridge the gap toward next-level quantization of hyper-scale Transformer models.

In this paper, we propose a novel PTQ algorithm, called _aespa_,2 that pursues both accuracy and efficiency. The key idea of _aespa_ is to perform quantization layer-wise for efficiency while targeting the attention-wise reconstruction to consider the cross-layer dependency.

Our contributions are summarized as follows:

* We propose a new quantization strategy that balances accuracy and efficiency. Our scheme aims to reconstruct the attention output to consider the cross-layer dependency while quantizing models layer-wise to pursue efficiency.
* To accelerate the quantization process, we propose refined quantization objectives for the attention module. Through a complexity analysis, we demonstrate that quantization that is approximately 10 times faster than existing block-wise approaches can be achieved by exploiting the proposed objectives.
* From extensive experiments on language models, we demonstrate that our approach outperforms conventional schemes by a significant margin, particularly for low-bit precision (INT2).

## 2 Background

### Classic PTQ methods

Recent studies on PTQ have mostly attempted to minimize the increase in the task loss incurred by quantization rather than the quantization error itself (\(\)). Consider a pre-trained neural network parameterized by weights \(\). If we assume the well-convergence of the network, the problem of quantizing weights \(\) to minimize the loss degradation can be formulated as [16; 23]

\[_{}\ \ [^{T}^{( )}], \]

where \(^{()}\) is the Hessian related to the flattened weight \(\). Because computing and storing \(^{()}\) is infeasible, further assumptions have been made to simplify (1). In , for example, layer-wise independence has been assumed, relaxing (1) into the layer-wise reconstruction problem:

\[_{^{()}}\ [\|(^{( )})-^{()}\|_{F}^{2}], \]

where \(^{()}\) denotes the weights of the \(\)-th layer, \(\) is the input, and \(\) is a quantization function. For a uniform quantization, if the nearest-rounding is used to assign integer weights, \(\) is defined as

\[(x)=s((+z,0,2^{n}-1)-z), \]

where \(s\), \(z\), and \(n\) are the scale, zero-point, and bit-width, respectively, and \(\) represents the round-off.

Early studies on PTQ focused on optimizing the weight-rounding policy [23; 10; 18; 11; 12]. These studies have attempted to assign each weight to a "proper" grid (instead of an adjacent grid), such that the loss degradation could be minimized. In , a learning-based weight-rounding optimization algorithm, called AdaRound, has been proposed to solve the layer-wise reconstruction problem in (2). In , AdaRound has been extended to the following block-wise reconstruction problem:

\[_{^{()}}\ [\|f^{()}((^{()}),)-f^{()}(^{()}, )\|_{F}^{2}], \]

where \(^{()}\) denotes the weights of the \(\)-th block \(f^{()}\) (_e.g._, ResNet or Transformer block). By considering the dependency between layers inside the block, this algorithm, termed Brecq, not only performs better than AdaRound, but also exhibits robust performance for a low bit-width (_e.g._, INT2).

### PTQ for LLMs

Although AdaRound and Brecq have been successful in quantizing small-scale networks (_e.g.,_ ResNet), scaling those learning-based schemes to LLMs with billions of parameters is challenging. In fact, Brecq requires more than 20 GPU hours to quantize relatively small-sized language models (_e.g.,_ OPT-2.7B; see Appendix K), which would not be suitable for the real-world deployment of LLMs where models to be deployed are frequently updated.

Owing to the excessive time and memory costs of classic PTQ schemes, recent studies have focused on developing cost-effective alternatives for quantizing LLMs. In OPTQ , a one-shot PTQ scheme that optimizes a weight-rounding policy without relying on learning, has been proposed. In addition, PTQ schemes that enhance the performance of the nearest-rounding, rather than optimizing the weight-rounding policy, have been proposed. These schemes use additional "foldable" parameters3 to suppress activation outliers or quantize weights more precisely [33; 19; 13; 27; 20].

Although previous studies have mitigated the computational overhead of classic PTQ methods, they often sacrifice the low-bit quantization performance or suffer from an unstable quantization process. The main reason for this unsatisfactory performance is that all the schemes mentioned above, except OPTQ, rely on nearest-rounding and do not optimize the weight-rounding policy. Moreover, most of them target layer-wise reconstruction in (2), not block-wise reconstruction in (4), thus ignoring the cross-layer dependency within the attention module. Although [27; 20] target block-wise reconstruction via learning, they need to approximate gradients for a non-differentiable quantization function, which results in an unstable training process (see Table 1 in Section 4) .

Thus, we propose a novel PTQ scheme that balances accuracy and efficiency. In contrast to conventional LLM quantization methods, our scheme optimizes a weight-rounding policy while targeting block-wise reconstruction to consider the cross-layer dependency. The key difference over classic block-wise weight-rounding optimization is that we quantize models layer-wise for scalability, whereas layers are jointly quantized in the existing methods. Furthermore, we present an efficient pre-computation-based method for the computation of the block-wise objective in (4), which significantly reduces the computational overhead caused by repeated attention operations.

## 3 Method

### Motivation

To gain insight into our approach, we first consider the objective of the layer-wise reconstruction in (2). Let \(^{()}=(^{()})-^{()}\), then the reconstruction error can be expressed as

\[[\|\|_{F}^{2}]= [\!(^{T} ^{T})]= \!(\!\![ ^{T}]\!\!^{T})\!. \]

Figure 1: Overview of _aespa_. Each weight is quantized separately to reconstruct the attention output.

Consequently, the layer-wise quantization problem can be recast as follows:

\[_{}\ ([ ^{T}]^{T}). \]

The new form of the quantization objective in (6) implies that if \([^{T}]\) is pre-computed and stored before quantization, we can measure the reconstruction error over the entire calibration dataset with a single matrix multiplication and element-wise multiplication.4 This is in contrast to the original formulation in (2) which requires the computation of \(()\) or \(\) for every input \(\).

A natural question that arises from this finding is _"Can we also measure the block reconstruction error efficiently based on such a pre-computation?"_. In the following subsections, we describe our main strategy to simplify block-wise quantization and then present a refined objective for the attention module, where the objective can be computed efficiently with certain pre-computed values.

### Quantization strategy of _aespa_

When quantizing the attention module using conventional block-wise reconstruction methods (Figure 1(a)), the query, key, and value projections have been jointly optimized such that

\[_{_{Q},_{K},_{V}}\ [\| (},},})- (,,)\|_{F}^{2}], \]

where the output of attention module \((,,)\) is defined as

\[(,,)=(^{T}}{})=. \]

In such a case, we need to compute \((},},})\) for every batch sequence in each iteration, which is computationally heavy and time-consuming (see Section 3.5 for details on complexity).

To overcome this computational overhead, we quantize each projection _separately_ in a divide-and-conquer manner. For example, when quantizing the query projection \(_{Q}\), we fix \(_{K}\) and \(_{V}\) with full-precision (Figure 1(b)), which facilitates the factoring out of common terms affected by \(_{K}\) and \(_{V}\) (see Section 3.3 for details). We emphasize that this strategy differs from conventional layer-wise quantization schemes (_e.g.,_ AdaRound and OPTQ) in that we aim to minimize the reconstruction error for the attention module, not the reconstruction error for each layer.

We conduct experiments to demonstrate the importance of targeting attention-wise reconstruction and validity of the proposed quantization strategy. In our experiments, we set the loss function for each projection as the attention reconstruction error in (7) but quantize each projection separately (see Figure 2(c)). Table 5 in Appendix B summarizes the performance of AdaRound, Brecq, and our approach. As evident, our approach uniformly outperforms AdaRound for all bit-widths, although both methods quantize models layer-wise. This is because we can consider cross-layer dependency (_i.e.,_ relationship between the query, key, and value) by targeting attention-wise reconstruction, which is different from AdaRound wherein layers are considered independent. Furthermore, once we target attention-wise reconstruction, separate layer-wise quantization does not incur severe performance degradation compared to the joint quantization method (Brecq). In fact, our approach causes only a marginal performance degradation for 2-bit and exhibits comparable performance for 3-bit and 4-bit. For further discussion on the proposed strategy, see Appendix B.

### Refined quantization objectives for _aespa_

One might ask whether our strategy incurs more computational cost than that required by the joint quantization because we update only one layer at a time (see Figure 1(b)). This is in contrast

Figure 2: Quantization strategies (simplified)

to existing methods, in which the layers inside the attention module are updated simultaneously (Figure 1(a)). To reduce this additional cost, we refine the quantization objective in (7) for each projection.

**Value projection** When quantizing the value projection \(_{V}\), the query and key projections are fixed with full-precision. In this case, by factoring out the common term influenced by \(\) and \(\), we can simplify the attention reconstruction error \(_{V}\) as follows:

\[_{V}=[\|}- \|_{F}^{2}]=[\|\|_{F} ^{2}]=[\|_{V}^{T}\| _{F}^{2}]. \]

Thus, the problem to quantize \(_{V}\) to minimize the attention reconstruction error can be recast as

\[_{_{V}}\ [\|_{V}^{T }\|_{F}^{2}]. \]

**Query projection** When the key and value projections are fixed with full-precision, the attention reconstruction error \(_{Q}\) caused by \(_{Q}\) is expressed as

\[_{Q}=[\|(},, )-(,,)\|_{F}^{2}]= [\|\|_{F}^{2}], \]

where \(=(}^{T}/)- (^{T}/)\). To avoid the computational overhead of repetitive softmax operations, we approximate \(\) with its first-order Taylor series as

\[^{T}}{}_{ }^{T}, \]

where \(_{}\) is the Jacobian of the softmax function. By combining (11) and (12), we obtain

\[_{Q}[\|^{T}_{}^{T}\|_{F}^{2}]=[\|^{T}_{}_ {Q}\|_{F}^{2}]. \]

Although we can circumvent conducting attention operations using the modified form in (13), a large amount of memory is required to store the Jacobian \(_{}\) (_e.g._, more than 100 GB of memory for OPT-125M).5 As a cost-effective alternative, we build an upper bound of (13) and then employ it as a surrogate of \(_{Q}\) when quantizing \(_{Q}\). Specifically, by noting that

\[\|^{T}_{}_{Q} \|_{F}^{2}\|^{T}_{}\|_{F }^{2}\|_{Q}\|_{F}^{2} \]

and the term \(\|^{T}_{}\|_{F}^{2}\) is fixed in the quantization process, we minimize \(\|_{Q}\|_{F}^{2}\) with the hope that \(_{Q}\) also decreases. In other words, our quantization objective for \(_{Q}\) is

\[_{_{Q}}\ [\|_{Q} \|_{F}^{2}]. \]

**Key projection** By taking similar steps, the quantization objective for the key projection \(_{K}\) can be formulated as (see Appendix C for the detailed derivation)

\[_{_{K}}\ [\|_{K} \|_{F}^{2}]. \]

### Algorithm description

The proposed _aespa_ consists of two main steps. Specifically, _aespa_ first determines the quantization parameters (_i.e.,_ scale and zero-point) and then optimizes an integer weight \(_{int}\) for each weight.

Note that we only used the definition of the attention operation when developing the refined objectives in (10), (15), and (16). Thus, our objectives can be integrated into any layer-wise quantization scheme without effort. For example, we can compute the quantization parameters by combining existing parameter initialization algorithms (_e.g.,_ AWQ  and Z-Fold) with the proposed objectives. We can also optimize a weight-rounding policy using conventional methods (_e.g.,_ AdaRound )together with our objectives (see Appendix F for details). In the proposed _aespa_, we use Z-Fold in computing the quantization parameters and employ AdaRound in optimizing a weight-rounding policy. In Algorithm 1 (see Appendix A), we summarize the proposed _aespa_.6

To accelerate the weight-rounding learning process, we further modify the objective functions such that the value can be computed efficiently via pre-computation, as in (5).

**Modified objective for (10)** The proposed objective for the value projection can be recast as

\[[\|_{V}^{T}\|_{F}^{2} ]=(_{V}[^{T }^{T}]_{V}^{T}). \]

The modified objective allows us to perform each iteration of the weight-rounding learning efficiently. Specifically, by computing \([^{T}^{T}]\) before quantization and reusing it in the quantization process7, we can avoid the overhead of computing \(\|_{V}^{T}\|_{F}^{2}\) for every input \(\) and compute the loss with one simple matrix multiplication and a single element-wise multiplication (see Footnote 4).

Another intriguing feature of this modification is that it facilitates a more reliable update of \(_{V}\) than the original objective in (10). Specifically, because \([^{T}^{T}]\) is pre-computed using all calibration data, the loss computed with (17) considers the entire calibration dataset (_i.e.,_ the batch size is the total number of data). Thus, a better estimate of the true gradient can be obtained without any memory issues, which could lead to more consistent updates of \(_{V}\) and faster convergence .

The modified objective in (17) also implies that the Hessian \(_{V}\) for each row of \(_{V}\) is

\[_{V}=2[^{T}^{T}]. \]

We note that the proposed Hessian \(_{V}\) differs from \(=2[^{T}]\), which has been commonly used as an approximated Hessian in conventional methods . The key reason for the difference is that we consider the dependency between \(_{Q}\), \(_{K}\), and \(_{V}\) by targeting attention-wise reconstruction, whereas the previous methods assumed independence. To observe the effect of considering the cross-layer dependency, we use different Hessians (_i.e.,_\(_{V}\) and \(\)) when quantizing language models and then compare the performance of the quantized models (see Appendix D). Evidently, the quantization performance is much better when the proposed Hessian \(_{V}\) is used, which demonstrates the importance of considering the cross-layer dependency.

**Modified objectives for (15) and (16)** If we denote the vectorized representation of \(_{Q}\) as \(_{Q}\), the proposed objective in (15) can be expressed as (see Appendix E for the derivation)

\[[\|_{Q}\|_{F}^{2}]= _{Q}^{T}^{T}^{T} {K}_{Q}. \]

where \(\) is the Kronecker product operation. To reduce the memory cost of storing the Kronecker product term \(^{T}^{T}\), we approximate it as 

\[[^{T}^{T}][^{T}][^{T}]. \]

By combining (19) and (20), we obtain

\[[\|_{Q}\|_{F}^{ 2}]\] \[}{{=}}\!( ^{T}_{Q}[ ^{T}]_{Q}^{T}), \]

where the proof of (a) is provided in Appendix E. By taking similar steps, the objective for the key projection can be recast as

\[[\|_{K}\|_{F}^{2}]= \!(^{T} _{K}[^{T}]_{K}^{T}). \]

The modified objectives in (21) and (22) imply that the loss over the total calibration dataset can be calculated efficiently by computing \([^{T}]\), \([^{T}]\), and \([^{T}]\) in advance.

### Complexity analysis for _aespa_

We discuss the computational complexity of _aespa_. Specifically, we analyze the number of floating-point operations (flops) required to perform one iteration for weight-rounding optimization (line 6 in Algorithm 1). For each projection, the required number of flops is summarized as follows.

* **Value**: By reusing the pre-computed \([^{T}^{T}]\), the loss value in (17) can be computed with one matrix multiplication and one element-wise multiplication/addition (see Footnote 4). The associated cost is \(2d_{h}d^{2}+d_{h}d-1\) flops, where \(d\) is the hidden size and \(d_{h}\) is the input dimension for each attention head.
* **Query/key**: Once \([^{T}]\), \([^{T}]\), and \([^{T}]\) have been computed in advance, the loss values in (21) and (22) can be computed by performing two matrix multiplications and one element-wise multiplication/addition. This requires \(2d_{h}d^{2}+2d_{h}^{2}d-1\) flops for each projection.

To summarize, the total number of flops required in each iteration of the proposed _aespa_ is

\[_{}=6d_{h}d^{2}+4d_{h}^{2}d+d_{h}d-3=(d_{h }d^{2}). \]

We emphasize that regardless of the amount of calibration data, the number of flops to compute the loss considering the entire dataset is fixed as \(_{}\).

We now compare the complexities of _aespa_ and conventional block-wise quantization methods. It can be easily verified that the existing methods require the following number of flops for handling \(B\) input sequences of length \(L\) (see Appendix G):

\[_{exist}=B(6d_{h}dL+4d_{h}L^{2}+2L^{2}-L-1)=(Bd_{h}L \{d,L\}). \]

Table 7 in Appendix G summarizes the computational costs for different sizes of OPT models. For the conventional methods, we report the cost of using four sequences in each iteration (\(B=4\)). We observe that the computational cost of _aespa_ is considerably lower than that of conventional methods. In particular, for small-scale models, _aespa_ performs ten times fewer number of flops. It can be observed that the gap between \(_{}\) and \(_{exist}\) decreases as the model size increases. This is because the hidden size \(d\) exceeds the sequence length \(L\) (which is fixed for all models) for large models. Nevertheless, _aespa_ still incurs a lower computational cost, and the gap increases if conventional methods use larger batch sizes.

## 4 Experimental results

### Experimental setup

We quantize publicly available LLMs (_e.g._, OPT , BLOOM , LLaMA , and LLaMA2 ) using the proposed _aespa_. When implementing _aespa_, we compute the quantization parameters with Z-Fold and optimize a weight-rounding policy via AdaRound , where the proposed row-wise Hessians and loss functions (see Table 4 in Appendix A) are utilized instead of the existing ones. When computing the quantization parameters, we follow the stopping criterion introduced by . Before optimizing a weight-rounding policy, we update the full-precision weights via OPTQ , which empirically reduces the number of iterations required for weight-rounding optimization. When optimizing a weight-rounding policy, we set the number of iterations, learning rate, and weight of the rounding loss (see \(\) in (28)) to 2,000, 0.015, and 1.5, respectively.

When constructing the calibration dataset, we randomly sample 128 segments consisting of 2048 tokens from the C4 dataset  as in [7; 13; 3]. In our experiments, we quantize only weights and retain activations in full-precision because activations are not a significant bottleneck for LLMs  and the inference of LLMs can be accelerated sufficiently by reducing memory movement through weight quantization . We evaluate the performance of the quantized models using benchmark datasets (_e.g._, WikiText-2 , C4 , and PTB ) and zero-shot tasks. Except for the experiments on the LLaMA2 models, which were performed using an NVIDIA H100 GPU, we conducted all experiments using a single NVIDIA A100 GPU (80 GB).

### Comparison with prior arts

**Comparison with block-wise PTQ schemes** We compare the proposed _aespa_ with conventional block-wise PTQ methods, among which Brecq is a classic weight-rounding optimization method,and OmniQuant and AffineQuant are LLM quantization methods that mitigate the computational overhead of Brecq by learning only a few quantization and foldable parameters . For OmniQuant and AffineQuant, we ran the official codes8 provided by the authors. For both methods, we activated the learnable equivalent transformation (LET) and learnable weight clipping (LWC) options and reported the obtained results. When implementing Brecq, we employed the hyperparameter settings provided in . In this comparison, the BLOOM models and OPT-350M were excluded because they are not supported by OmniQuant and AffineQuant.

As Table 1 shows, _aespa_ uniformly outperforms OmniQuant/AffineQuant.9 In particular, the performance gap is significant for 2-bit; while OmniQuant/AffineQuant suffer from instability (_i.e.,_ loss diverges) or collapse (perplexity (PPL) \(>10^{3}\)), _aespa_ exhibits reasonable PPL. The outstanding performance is attributed to the fact that _aespa_ optimizes a weight-rounding policy after determining the quantization parameters (lines 5-8 in Algorithm 1), whereas OmniQuant/AffineQuant rely on the naive nearest rounding and approximate gradients for the non-differentiable quantization function.

Although Brecq performs best for the 2-bit quantization of OPT-125M, it lacks scalability; Brecq requires approximately 20 GPU hours for a relatively small-scale OPT-2.7B (see Table 14 in Appendix K). Even for OPT-125M, Brecq requires approximately 2 GPU hours, whereas the proposed _aespa_ completes quantization in 5 minutes. One might wonder why the performance of Brecq worsens as the model size increases. We assume that this is attributable to the choice of hyperparameters (_e.g.,_ learning rate and weight of rounding loss). In fact, the hyperparameters presented in  have been optimized for ImageNet, but not for LLMs. It is expected that we can obtain better performance for Brecq via deliberate hyperparameter tuning; however, this would not be feasible for real-world deployment because it requires considerable time (see Table 14 in Appendix K).

**Comparison with layer-wise PTQ schemes** We compare the proposed _aespa_ with conventional layer-wise PTQ schemes, among which RTN is the method that naively assigns the nearest grid, OPTQ is a backpropagation-free weight-rounding optimization algorithm , and Z-Fold is the

Table 1: Performance (PPL \(\)) of the proposed _aespa_ and conventional block-wise PTQ methods.

method exploiting additional foldable parameters to quantize weights more elaborately . Table 2 and Tables 9-12 (see Appendix I) summarize the results for the OPT, BLOOM, LLaMA, and LLaMA2 models of various sizes. Evidently, _aespa_ uniformly outperforms conventional schemes, regardless of the size and type of LLMs. In particular, for 2-bit, there exists a significant performance gap between _aespa_ and existing methods; the PPL obtained by _aespa_ is twice as low as those of conventional methods for small-scale models (_e.g.,_ OPT-125M). The key factors leading to such an outstanding performance are: 1) the consideration of the cross-layer dependency achieved by targeting attention-wise reconstruction, and 2) efficient weight-rounding optimization based on pre-computations.

**Zero-shot task performance** We evaluate the reasoning performance of quantized models using zero-shot tasks (_e.g.,_ ARC , HellaSwag , and MMLU ). We note that the zero-shot setting was ensured in our experiments because we used excerpts from randomly crawled websites (not task-specific data) as a calibration dataset. From the zero-shot results in Table 3 and Table 13 (see Appendix J), we observe that the proposed _aespa_ performs the best in almost all cases, and the performance gap between _aespa_ and the existing methods is large for 2-bit.

**Time cost** We summarize the processing times of the different quantization algorithms in Appendix K. We note that the processing time of _aespa_ includes the time required for pre-computations (lines 2-4 in Algorithm 1). As expected, _aespa_ completes quantization much faster than Brecq. For example, while Brecq requires more than 10 GPU hours for OPT-1.3B, _aespa_ completes quantization in 1.24 hours, which demonstrates the effectiveness of the proposed pre-computation-based loss computation strategy. Although other block-wise methods (OmniQuant/AffineQuant) perform quantization faster than _aespa_ for hyper-scale models, they suffer from unstable training processes or exhibit poor PPL performance (_e.g.,_ PPL of OmniQuant is larger than \(10^{3}\) for OPT-6.7B; see Table 1). In addition, we observe that OPTQ performs quantization quickly, but its \(2\)-bit performance collapses regardless of the model size (see Table 9 in Appendix I). Except for _aespa_, Z-Fold is the only method that shows both reasonable performance and processing time.

**Discussion** In real situations, when one needs to preserve the performance of the original model as much as possible, the proposed _aespa_ would be an intriguing solution. In particular, when deploying LLMs on resource-constrained platforms where up to 7B models are commonly employed (_e.g.,_ mobile devices), _aespa_ would be a good fit. Even when fast quantization of hyper-scale models is required, _aespa_ can be used with a slight modification. Specifically, in time-limited cases, one can skip the weight-rounding optimization (lines 5-8 in Algorithm 1) and simply determine the quantization parameters using the proposed Hessian that considers the cross-layer dependency (line 4 in Algorithm 1). In doing so, we can not only save the time required to optimize a weight-rounding mechanism, but also save the memory required to store pre-computed values (\([^{T}]\) and \([^{T}]\)). Indeed, when performing only quantization parameter computation, we achieved a significant reduction in the processing time (see Table 15 in Appendix K) while still exhibiting better performance than conventional methods (see Table 6 in Appendix D).

## 5 Conclusion

We proposed a next-level PTQ scheme for Transformers, called _aespa_. By targeting the attention-wise reconstruction while quantizing Transformers layer-wise, we could consider the cross-layer dependency within the attention module and complete the quantization much faster than the existing

    &  &  &  \\   & & 560M & 1.1B & 1.7B & 3B & 7.1B & 560M & 1.1B & 1.7B & 3B & 7.1B \\  FP16 & Baseline & 22.42 & 17.69 & 15.39 & 13.48 & 11.37 & 26.60 & 22.05 & 19.49 & 17.49 & 15.20 \\   & RTN & 56.74 & 49.85 & 63.37 & 39.07 & 17.35 & 66.99 & 60.41 & 113.6 & 79.84 & 22.54 \\  & OPTQ & 31.55 & 23.84 & 20.06 & 17.13 & 13.56 & 34.62 & 27.62 & 23.87 & 20.96 & 17.43 \\  & Z-Fold & 26.52 & 20.99 & 17.39 & 15.11 & 12.26 & 29.97 & 24.43 & 21.52 & 19.91 & 16.12 \\  & _aespa_ & **25.39** & **19.81** & **16.95** & **14.68** & **12.00** & **29.10** & **23.80** & **20.93** & **18.55** & **15.91** \\   & RTN & 7.8e5 & 9.8e5 & 3.5e5 & 1.4e5 & 2.1e5 & 1.46e & 2.1e6 & 2.7e5 & 9.2e4 & 1.3e5 \\  & OPTQ & 1.7e3 & 1.9e3 & 1.4e3 & 79e.5 & 194.2 & 533.4 & 538.0 & 562.9 & 351.6 & 112.8 \\   & Z-Fold & 65.45 & 44.50 & 35.69 & 27.40 & 18.87 & 64.11 & 42.96 & 37.26 & 32.64 & 22.46 \\   & _aespa_ & **44.91** & **34.12** & **27.67** & **21.65** & **16.31** & **45.04** & **35.12** & **29.95** & **25.04** & **20.00** \\   

* Results for high bit-widths and other language models (_e.g.,_ OPT, LLaMA, and LLaMA2) are provided in Appendix I.

Table 2: Performance (PPL \(\)) of _aespa_ and existing layer-wise PTQ methods on BLOOM models.

approach for block-wise reconstruction (_i.e.,_Brecq). Extensive experiments on language models have demonstrated the outstanding performance of _aespa_.

**Limitations and future work** While we focused on the attention output, the output of the entire Transformer block (containing fully connected layers) can be used to consider the dependencies between more layers. However, in this case, the objective functions would be more complicated than those in (13) and (25) due to nonlinear activation functions (_e.g.,_SiLU for LLaMA models), normalization layers, and weights of larger dimensions. Enhancing the quantization performance by developing an efficient form of the reconstruction error for the Transformer block would be an interesting future work. Furthermore, while we focused on weight-only quantization, activations may need to be quantized to deploy AI models on integer-only arithmetic hardware (_e.g.,_NPU). Extending the proposed _aespa_ for weight-activation quantization by integrating existing techniques to suppress activation outliers  is also an interesting research direction. Finally, while we verified the performance of _aespa_ with LLMs, we believe that _aespa_ can also be used for the quantization of diffusion models. To that end, we may need to incorporate some diffusion-specific quantization strategies to overcome output distribution discrepancies over different time steps (_e.g.,_ grouping of time-steps with similar distributions , temporal feature preservation , and separate quantization for shortcuts in U-Net ), which will be considered in our future studies.

   Model & Method & ARC-c & ARC-e & HellaSwag & MMLU & Average \\   & FP16 & 44.62 & 72.85 & 76.18 & 32.19 & 56.46 \\   & RTN & 28.67 & 25.00 & 26.43 & 25.72 & 26.46 \\  & OPTQ  & 29.18 & 26.14 & 26.18 & 24.04 & 26.39 \\  & Z-F-Fol  & 30.63 & 52.44 & 53.55 & 23.27 & 39.97 \\  & OmniQaart  & 27.22 & 49.20 & 50.65 & 23.74 & 37.70 \\  & AffineQuant  & 27.90 & 49.58 & 51.85 & 24.15 & 38.37 \\  & _aespa_ & 33.36 & 55.64 & 58.31 & 23.12 & **42.61** \\   & FP16 & 47.87 & 74.75 & 79.08 & 43.46 & 61.29 \\   & RTN & 28.16 & 27.15 & 26.09 & 25.53 & 26.73 \\  & OPTQ  & 27.22 & 25.76 & 25.67 & 25.05 & 25.93 \\  & Z-F-Fol  & 32.68 & 58.08 & 57.99 & 26.44 & 43.77 \\  & OmniQaart  & Nau & Nau & Nau & Nau & Nau \\  & AffineQuant  & 32.17 & 56.36 & 60.29 & 25.22 & 43.51 \\  & _aespa_ & 34.73 & 61.49 & 62.68 & 28.74 & **46.91** \\   & FP16 & 52.90 & 78.96 & 82.63 & 54.66 & 67.29 \\   & RTN & 27.05 & 26.39 & 25.87 & 25.48 & 26.20 \\  & OPTQ  & 27.13 & 26.60 & 26.12 & 23.56 & 25.85 \\  & Z-F-Fol  & 39.93 & 65.07 & 65.89 & 30.85 & 50.44 \\  & OmniQaart  & 34.22 & 58.50 & 64.83 & 25.91 & 45.87 \\  & AffineQuant  & Nau & Nau & Nau & Nau & Nau \\  & _aespa_ & 41.13 & 67.00 & 67.90 & 35.67 & **52.93** \\   & FP16 & 46.16 & 74.49 & 75.99 & 41.87 & 59.63 \\   & RTN & 28.33 & 26.01 & 25.88 & 23.02 & 25.81 \\   & OPTQ  & 26.37 & 26.09 & 25.11 & 25.10 & 25.67 \\   & Z-Fol-Fol  & 26.62 & 42.68 & 44.71 & 22.88 & 34.22 \\   & OmniQaart  & 25.00 & 38.80 & 42.97 & 23.03 & 32.45 \\   & AffineQuant  & Nau & Nau & Nau & Nau & Nau \\  & _aespa_ & 30.29 & 51.47 & 56.75 & 25.59 & **41.03** \\   & FP16 & 49.06 & 77.44 & 79.39 & 52.10 & 64.50 \\   & RTN & 27.22 & 25.04 & 25.58 & 24.69 & 25.63 \\   & OPTQ  & 26.71 & 27.19 & 25.42 & 23.74 & 25.77 \\   & Z-Fol-Fol  & 28.41 & 48.32 & 51.59 & 23.98 & 38.08 \\   & OmniQaart  & 27.13 & 47.98 & 53.27 & 23.81 & 38.05 \\   & AffineQuant  & 30.80 & 52.90 & 57.74 & 24.45 & 41.47 \\   & _aespa_ & 31.91 & 55.18 & 55.49 & 29.97 & **43.14** \\   

* ‘NaN’ means that loss diverges in the quantization process.
* Results for high bit-widths are provided in Appendix J due to the page limitation.

Table 3: INT2 zero-shot performance (accuracy \(\)) of _aespa_ and existing methods.