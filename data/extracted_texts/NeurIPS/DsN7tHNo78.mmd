# Manipulation Intention Understanding for Accurate Zero-Shot Composed Image Retrieval

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Composed Image Retrieval (CIR) facilitates retrieving an image matching a reference image while incorporating specified textual modifications, which is crucial for internet searches and e-commerce. Traditional supervised CIR methods rely on annotated triplets, which are labor-intensive and limit generalizability. Recent advances in Zero-Shot Composed Image Retrieval (ZS-CIR) address the challenge of performing this task without annotated triplets. A key challenge in ZS-CIR is training models on limited intention-relevant datasets to understand human intention implicitly expressed in textual modifications for accurately retrieving target images. In this paper, we introduce an image-text dataset incorporated with pseudo-manipulation intentions to enhance the training of ZS-CIR models in understanding human manipulation intents. Based on our dataset, we propose a novel framework, De-MINDS, for capturing the intent humans aim to modify, thereby enhancing the ZS-CIR model's ability to understand human manipulation descriptions. Specifically, a simple mapping network first maps image information into language space and forms a target description with a manipulation description. Subsequently, De-MINDS captures intention-relevant information from target descriptions and converts them into several pseudo-word tokens for accurate ZS-CIR. The De-MINDS model exhibits robust generalization and significant improvements in performance across four ZS-CIR tasks. It achieves performance improvements from 2.05% to 4.35% over the best methods and establishes new state-of-the-art results with comparable inference times. Our code is available at [https://anonymous.4open.science/r/De-MINDS/](https://anonymous.4open.science/r/De-MINDS/).

## 1 Introduction

Composed Image Retrieval (CIR)  aims to retrieve an image that is visually similar to a reference image while having visual modification according to the manipulation text. Different from traditional image retrieval , CIR offers more flexibility and accuracy by enabling users to integrate both visual and textual information into their search intent. This approach has gained emerging attention in internet searches and e-commerce applications . Various supervised methods have been proposed to solve CIR problem , which requires a large amount of annotated triplets, _i.e.,_ a reference image, a manipulated description, and a target image, for training task-specific retrieval models. However, these supervised methods are labor-intensive for data annotation and tend to suffer from limited generalization capabilities due to bias in human annotation. To enhance model generalization and perform CIR tasks without annotated triplets, recent research  introduce Zero-Shot Composed Image Retrieval (ZS-CIR). Existing solutions for ZS-CIR map an image to the language space, combining it with text to form a query. This query retrieves target images from the shared semantic space of a pre-trained vision-language model by calculating semantic similarity. These methods typically involve a pre-trained mapping network that converts the referenceimage into a pseudo-word token \(_{*}\). During retrieval, this token \(_{*}\) is merged with the manipulation description to construct a target description, which a pre-trained CLIP model  then encodes, leveraging its comprehensive pre-trained knowledge across image candidates for retrieval.

Despite remarkable advancement, the pre-trained mapping networks are not satisfactory for CIR due to the following reasons:

(1) There exists a discrepancy between the retrieval and pre-training stages in ZS-CIR models. During retrieval, the mapping network is tasked with aligning intent-specific visual information (_e.g.,_ objects, scenes, colors, and styles) in language space to form a composed image description query (_e.g.,_ change to a man playing the accordion joyfully in the street) for calculating semantic similarity with the target image. However, in the pre-training phase, the mapping network aligns general visual information with textual descriptions of the image content (_e.g.,_ a musician plays the piano). Without intent-specific mapping, the pseudo-token \(_{*}\) contains heavy information redundancy involving most objects, background/foreground, color, and style, leading to inaccurate retrieval.

(2) Accurately understanding the intention a user intends to modify in manipulation descriptions presents substantial challenges. These intentions are implicitly expressed in users' manipulation descriptions. For instance, the manipulation intention embedded in the request to "make this photo feel like early fall" may involve changing colors (_e.g.,_ orange and yellow), adjusting the scene (_e.g.,_ fallen leaves), and adding specific objects (_e.g.,_ autumnal trees). However, existing ZS-CIR models rely on the CLIP language encoder, which challenges capturing fine-grained/long information from text , facing difficulties in accurately understanding these manipulation intentions.

In this work, we introduce the intent-CC3M, an intention-based dataset for training mapping networks capable of aligning intention-relevant visual information within the language space, thus addressing the gap between pre-training and retrieval in ZS-CIR models. We incorporate pseudo-manipulation descriptions in CC3M , the widely used ZS-CIR training dataset . These pseudo descriptions, reflecting potential user intention to manipulate images, are reasoned through chain-of-thought prompting using an off-the-shelf Multi-modal Large Language Model (MLLM), facilitating the learning of intent-specific mapping capabilities. Furthermore, to overcome the challenge of existing ZS-CIR models in understanding manipulation intention within descriptions, we propose a novel _unDErstanding of Manipulation INtention from target Description before Searching_ approach, named De-MINDS. We leverage pseudo-manipulation descriptions to train De-MINDS to capture manipulation intention from various aspects (_e.g.,_ objects, scenes, colors, styles) guided by multiple learnable queries. This intention information is mapped to several pseudo-word tokens, which are subsequently input into the CLIP language encoder, enhancing its ability to understand users' intention to modify and thereby improving the accuracy of CIR.

The main contributions of this work are summarized as follows: (1) We introduce intent-CC3M, a novel dataset with pseudo-manipulation descriptions reasoned by an MLLM to bridge the gap between pre-training and retrieval in ZS-CIR models. Our experiments demonstrate that baseline models trained with our dataset are capable of aligning intention-relevant visual information, achieving consistent performance improvements. (2) We propose a novel manipulation intention understanding network. We extract intentions in manipulation descriptions under the guidance of learnable queries and map to several pseudo-word tokens for retrieval, enhancing the CLIP's ability to understand users' intentions. It sheds new light on intention-based image retrieval. (3) Our De-MINDS are consistently effective and generalizable across diverse ZS-CIR tasks. It significantly improves CIR performance from 2.05% to 4.35% across four CIR tasks, establishing new state-of-the-art results with comparable inference time, further impacting vision and language applications.

## 2 Related Works

**Composed Image Retrieval.** Composed Image Retrieval (CIR) integrates image and text for retrieval . Current models typically employ late fusion for integrating visual and language features separately . In contrast, zero-shot CIR models like Pic2Word , SEARLE , and Context-I2W  train on image-text pairs, bypassing the need for costly CIR datasets. Pic2Word aligns entire images into text features, SEARLE adds a pseudo-word token to GPT-based captions, and Context-I2W employs context-dependent word mapping for accurate retrieval. However, these methods rely on the pre-trained CLIP language encoder, which struggles to understand intentions within manipulation descriptions. To tackle this issue, we propose a novel model that effectively understands these intentions, thereby improving the ZS-CIR model's ability to retrieve images based on human manipulation intents accurately. Unlike CIReVL , which employs LLMs during inference for composed retrieval, introducing non-negligible computational overhead, our model is lightweight and achieves comparable inference time to recent approaches.

**Vision and Language Pre-training Models.** Vision and Language Pre-training (VLP) models, like CLIP , leverage extensive image-text pair training to achieve implicit alignment. Recent VLP advancements  utilize static models to integrate encoded image and text features, enabling various zero-shot tasks . However, current CLIP-based zero-shot learning struggles with manipulation description in CIR tasks, motivating our approach, which enhances CLIP's capabilities of understanding user intentions to modify from fine-grained/long descriptions. Moreover, recent studies , inspired by DETR , employ learnable queries to select image and text information. In our work, we utilize multiple learnable queries to guide the extraction of manipulation intentions from target descriptions, providing explanatory cues for more accurate ZS-CIR.

**Image-text Dataset Enhancement.** In the field of vision-language learning, various endeavors  aim to enhance caption quality within existing image-text datasets. LaCLIP  utilizes LLMs to refine raw captions. VeCLIP  integrates insights from raw and synthetic sources using LLMs. The latest approach, ShareGPT4V , leverages MLLMs to generate descriptive captions from deliberate prompts and corresponding image inputs. However, these methods ignore human manipulation intentions, which are crucial for CIR tasks. To bridge this gap, we introduce a novel dataset infused with pseudo-manipulation intentions reasoned by MLLMs.

## 3 Methodology

### Preliminary

Given a reference image space \(\) and a text description space \(\), Composed Image Retrieval (CIR) involves a user manipulation text \(T\) describing hypothetical semantic changes to a reference image \(I_{r}\), aiming to retrieve a target image with its closest context from an image database \(=\{I_{i},,I_{n}\}\). Zero-Shot CIR (ZS-CIR) approaches  sidestep this requirement by training a mapping network to map the reference image into an associated text representation. Specifically, these methods learn a mapping function \(f_{}:\), where \(\) is a pre-defined text-token embedding space. \(f_{}\) is trained using intermediate image representations from a specific image encoder \(_{I}\), often part of a pre-trained vision-language representation system. Template filling around the manipulation text over the pseudo token embedding \(_{*}=f_{}(_{I}(I_{r}))\) is then employed to aggregate information into a target description \(P\) (_e.g.,_ "a photo of \(_{*}\), \(\{T\}\))." This target description serves as input for target image retrieval, encoding it using the associated pre-trained text encoder \(_{T}\). The respective matching score is \((_{I}(I_{r}),_{T}(P))\) using cosine similarity.

### Creating Intention-based Image-text Aliagment Dataset

To address the discrepancy between pre-training and retrieval in existing ZS-CIR models, we aim to develop an intention-based image-text dataset for training mapping networks capable of aligning

Figure 1: Illustration of using LLaVA to create our intent-CC3M dataset. We first use a prompt to guide the LLaVA model in generating rewritten captions with multi-view visual descriptions. Then, we leverage another prompt to reason pseudo-manipulation descriptions with potential intentions.

intent-relevant visual information within the language space. To make a fair comparison and mitigate the bias in human annotation, we propose to augment the widely used ZS-CIR training image-text dataset, CC3M, through LLaVA , an open-source, state-of-the-art Multi-modal Large Language Model (MLLM) known for its robust performance in vision-language tasks. However, reasoning potential manipulation intentions from image-text pairs remains a challenging task for LLaVA.

Recent advancements in MLLMs include the development of Chain-of-Thought (CoT) prompting , which enables MLLMs to produce a sequence of reasoning steps, breaking down multi-step problems into intermediate stages and enhancing performance in complex tasks . Inspired by the CoT prompting mechanism, we explore a novel multimodal CoT prompting strategy using LLaVA to reason pseudo-manipulation descriptions with potential intentions from image-text pairs effectively.

As illustrated in Figure 1, we divide the process of reasoning pseudo-manipulation descriptions into two stages: the _Caption Rewriting_ stage rewrites the original caption with multi-view visual information for CIR tasks. The _Intention Reasoning_ stage further understands the manipulation intentions from rewritten captions to reason pseudo-manipulation descriptions. Specifically, in the caption rewriting stage, we utilize the \(i\)-th image \(I_{i}\) and its original caption \(T^{i}_{ori}\) from the CC3M, denoted as \(=\{(I^{i}_{r},T^{i}_{ori}),,(I^{n}_{r},T^{n}_{ori})\}\). We guide the LLaVA model with a prompt to generate a rewritten caption \(T^{i}_{rew}\) for each image. These rewritten captions, averaging 65 tokens, include various aspects of visual information (_e.g.,_ object, foreground/background, color, and domain style). In the intention reasoning stage, we apply an additional prompt to reason manipulation intention for rewritten captions. This results in a more effective pseudo-manipulation description \(T^{i}_{int}\), averaging 27 tokens. The result dataset is represented as \(}=\{(I^{i}_{r},T^{i}_{ori},T^{i}_{rew},T^{i}_{int}),, (I^{n}_{r},T^{n}_{ori},T^{n}_{rew},T^{n}_{int})\}\).

### Manipulation Intention Understanding From Descriptions Before Searching

Since ZS-CIR models leverage the CLIP language encoder, there is a challenge in understanding manipulation intentions that are implicitly expressed in user descriptions. To address this challenge, we propose a method to understand the manipulation intention before feeding into the CLIP language encoder for accurate ZS-CIR in two modules: the _Manipulation Intention Understanding_ captures manipulation intentions and maps them into several pseudo tokens. The _Reasoning Distillation_ further aligns the context of desired pseudo-word tokens closely with human intention by leveraging pseudo-manipulation description to enhance the models' ability to understand human intention.

**Image and Context Encoding.** For a given sample \((I_{r},T_{ori},T_{rew},T_{int})\) from intent-CC3M. Since the pre-trained vision-language models are strong at modeling the cross-modal implicit alignment. Initially, we employ the frozen image encoder \(_{I}\) from the CLIP model to encode the global image feature of the reference image \(I_{r}\) as \(=_{I}(I_{r})=\{v_{i}\}_{i=1}^{d}^{d 1}\). Subsequently, we apply a simple mapping network \(f_{}\) with parameters \(\) to extract a pseudo token embedding \(S_{*}=f_{}()\). Considering our focus on manipulation intention understanding for ZS-CIR, \(f_{}\) is structured as a simple three-layer fully-connected network. We then construct a target description \(P\) formatted as "a photo of \(S_{*}\), \(\{T\}\)". We consider two scenarios for manipulation intention understanding: deducing intention information from concise texts (_e.g.,_ original caption) or integrating it from lengthy texts(**e.g.,** rewritten caption). Accordingly, the text \(T\) is composed randomly within a batch according to the following distribution: 50% original caption \(T_{rew}\) and 30% rewritten caption \(T_{ori}\) to learn manipulation intention understanding, 20% pseudo-manipulation description \(T_{int}\) to ensure training stability (details are in Appendix C). We feed the target description to the language encoder \(_{T}\) of frozen CLIP to represent the target description \(P\) by a set of language feature vectors \(\) =\(\{_{i}\}_{i=1}^{m}^{d m}\). \(_{1}\) represents the [CLS] embedding \(_{cls}\) with global information of image and caption, while other ones denote word embeddings \(}\) =\(\{_{i}\}_{i=2}^{m}\).

**Manipulation Intentions Understanding.** Given the word embeddings of the target descriptions, this module aims to capture different manipulation intentions, thereby enhancing the CLIP language encoder's capability to understand users' intents for manipulation. To capture different manipulation intentions, we introduce a set of learnable query embeddings for guidance, denoted as \(=\{_{k}\}_{k=1}^{n}^{d n}\), where \(d\) is the embedding dimension and \(n\) is the number of queries. Each query \(_{k}\) represents a kind of manipulation intention. As depicted in Figure 2(left), we implement cross-attention mechanisms to extract intention-relevant contextual information from the word embeddings \(}=\{_{i}\}_{i=2}^{m}\) using the learnable queries \(\). The cross-attention operation involves three primary steps. First, we compute the query, key and value through linear projections,_i.e., \(=^{Q}\), \(=[,}]^{K}\), \(=[,}]^{V}\). \([,}]\) denotes concatenating the two matrices, which enhances the interaction between learnable queries and word embeddings with better performance. Then, the learnable queries from the current cross-attention block \(^{i}\) is calculated as:

\[^{i}_{att}=(,,)=( ^{}}{}),^{i}=(^{i}_{att}+^{i-1})+^{i}_{att} \]

where \(^{i-1}\) are learnable queries from the previous block and \(()\) denotes 2-layer feed-forward networks. the refined query embeddings \(\) are then fed into the frozen language encoder \(_{T}\) of CLIP to extract the intention embedding as \(_{*}=_{T}(^{n})=\{t^{i}_{*}\}_{i=1}^{d}^{d 1 }\) (\(d=768\)).

**Reasoning Distillation.** Given the intention embedding \(_{*}\), the AI agent needs to further align with human manipulation intention. Specifically, we aim to reduce the distance between the intention embedding and the corresponding pseudo-manipulation description's [CLS] word embedding, which represents the MLLM's intention embedding while ensuring that each embedding remains distinct and discriminative. Given the intention embeddings \(_{int}=\{^{i}_{*}\}_{i=1}^{N}\), where \(N\) is the number of images in \(\), and the corresponding MLLM's intention embeddings \(}_{*}=_{T}(T_{int})}_{int}\) we employ a symmetric contrastive loss inspired by SimCLR [11; 13; 45] as follows:

\[_{distil}=_{s2t}(_{*},}_{*})+_{t2s}(}_{*},_{*}) \]

The two contrastive loss terms are defined as:

\[_{s2t}(_{*},}_{*})=-|}_ {i}^{i}_{*})^{T}}^{i}_{*}}} {_{j}e^{(^{i}_{*})^{T}}^{i}_{*}}}, _{t2s}(}_{*},}_{*})=-| }_{i}}^{i}_{*})^{T}^{i} _{*}}}{_{j}e^{(}^{i}_{*})^{T}^{j}_{*}}} \]

where \(B\) is the number of images in a batch and \(\) is a temperature hyper-parameter that controls the strength of penalties on hard negative samples.

**Cross-Modal Alignment.** Given the embedding of user manipulation intention, this module aims to form a target embedding optimized for retrieval. Since the nature of CIR, both the reference image and the manipulation intention form a comprehensive context that defines the target image. To dynamically control the influence of manipulation intentions on the retrieval process, we introduce a learnable scalar \(gate\) that decides the contribution of the manipulation intention information \(_{*}\) and integrates the global information \(_{cls}\) to form the final target embedding \(}\) as follows:

\[}=_{cls}+gate_{*}\]

Then, we aim to match a target image to its paired target embedding while separating unpaired ones. We minimize the symmetric contrastive loss between the image embedding \(\) and the target embedding \(}\) as follows:

\[_{align}=_{s2t}(},)+_{t2s}( ,}) \]

where \(_{s2t}\) and \(_{t2s}\) are two contrastive loss terms as Eq.3. The final loss used to optimize is:

\[=_{distill}+_{align} \]

Figure 2: An overview of our De-MINDS. Pre-training (left): Map the image to a pseudo token \(_{*}\), and understand the intention from the target description. Inference (right): Map the inference image to \(_{*}\) to construct the target description and understand manipulation intention for ZS-CIR.

**Inference with De-MINDS.** In the inference stage, we compose the reference image with the paired manipulation description and compare the composed query with candidate images for retrieval. As shown in Figure 2 (right), we compose the pseudo token embedding \(_{*}\) of the image from the mapping network with the text description and feed it to the pre-trained language encoder of CLIP. The result is embedded by the text encoder and compared to the visual features of candidate images.

Since we focus on studying the manipulation intention understanding searching for ZS-CIR, we utilize the same prompt in the most recent works [45; 52] for a fair comparison. We show prompt examples for different ZS-CIR tasks. In all examples, [*] indicates the pseudo token from the mapping network: **(a) Domain conversion** aims to modify the domain of the reference image. The prompt is defined as a [domain tag] of [*]; **(b) Object composition** retrieves an image that contains an object in the reference image and other object tags. The prompt is in the format of a photo of [*], [obj] tag] and [obj2 tag],..., and [obj\({}_{n}\) tag]; **(c) Sentence manipulation** modifies the reference image based on a sentence. We simply append the sentence with the special token as a photo of [*], [sentence]. More details are in Appendix D.3.

## 4 Experiments

**Datasets.** We evaluate our model on four ZS-CIR datasets, _i.e._, COCO  for object composition, ImageNet [16; 21] for domain conversion, CIRR  for object/scene manipulation, and Fashion-IQ  for attribute manipulation. All the dataset settings and evaluation metrics (Recall@K) follow the recent works [45; 52] for a fair comparison.

(1) Domain conversion. This dataset comprises 16,983 images of 200 classes from four domains, _i.e._, cartoon, origami, toy, and sculpture. We use the prompt (a) in inference. (2) Object composition. The dataset contains images with corresponding lists of object labels and instance masks of query images. We randomly crop one object and mask its background using its instance mask to create a reference image. We use the prompt (b) in inference. (3) Object/scene manipulation. A reference image is an instruction for manipulating an object or the background scene. We apply the prompt (c) in inference. (4) Attribute manipulation. This dataset includes various description sentences for manipulating image attributes. We utilize the prompt (c) in inference. More details in Appendix D.2.

**Implementation Details.** Generating one pseudo-manipulation description through LLaVA-1.6-13B  for the entire Conceptual Caption dataset , which comprises 3M images (CC3M), requires approximately 625 hours on 5 A100 (80G) GPUs. For training De-MINDS, We utilize the CC3M and adopt ViT-L/14 CLIP  pre-trained on 400M image-text paired data. We employ AdamW  with a learning rate of \(1 10^{-6}\), weight decay of \(0.1\), and a linear warmup of \(10000\) steps. The number of cross-attention blocks is \(6\). The number of learnable queries is \(4\). The batch size for contrastive learning is \(1024\). To improve training stability, we initialize the learnable scalar of tanh-gating to \(0\). For training Context-I2W and SEARLE, we keep the same setting reported in their paper, only replacing the original captions with our pseudo-manipulation descriptions. All models are trained on \(4\) NVIDIA A100 (80G) GPUs. To ensure reliable results, we report the performance averaged over three trials. More details are in Appendix D.1.

### Quantitative and Qualitative Results

We compare De-MINDS with several ZS-CIR methods, including: 1) **Pic2Word**: Maps the visual features of a reference image into a pseudo-word token within the CLIP token embedding space; 2) **SEARLE-XL**: Similar to Pic2Word, further integrating the pseudo-word token with the caption generated by GPT  and distilled for efficiency; 3) **Context-I2W**: Selectively extracts text-relevant visual information from the reference image before mapping it into a pseudo-word token; 4) **CIReVL**: Uses LLMs to enhance the manipulation description during inference; and 5) **LinCIR**: Masks subjects in captions from various image-text datasets for training. For a fair comparison, we present the reported results of methods relying on the ViT-L/14 CLIP model.

Moreover, we compare De-MINDS with 6) **SEARLE-XL* and Context-I2W***: Replace the original captions with our pseudo-manipulation description, and standard ZS-CIR methods, including 7) **Text-only**: Computes similarity based on the CLIP features of descriptions and candidate images; 8) **Image-only**: Retrieves the most similar images to the reference image; and 9) **Image + Text**: Sums the CLIP features of the reference image and the description.

Tables 1 to 4 present the quantitative results, while Figures 4 to 6 display the corresponding qualitative results of our model and the most recent works, CIReVL and Context-I2W. The attribute manipulation task requires accurately localizing specific attributes within the entire image. As demonstrated in Table 1, De-MINDS outperforms existing ZS-CIR models significantly, achieving an average improvement of 2.20% over the State-of-the-Art (SoTA) model, CIReVL. CIReVL's dependency on an LLM at inference introduces substantial computational overhead during retrieval. De-MINDS tackles this challenge by extracting fashion-relevant intention within manipulation descriptions into a series of implicit pseudo-tokens for CLIP retrieval. This approach is more efficient and suitable for models than relying on explicit, often noisy, LLM analysis results. Figure 4 further illustrates how De-MINDS effectively understand complex fashion-relevant attributes in manipulation descriptions, such as a sexier style with a waistband (row 1), black color with a special design in the center (row 2), and longer sleeves with two pockets in blue (row 3), facilitating more accurate searching.

We further assess De-MINDS' capability in foreground/background differentiation and fine-grained image editing through the object/scene manipulation task (Table 2). De-MINDS consistently surpasses existing ZS-CIR models, achieving an average performance improvement of 2.05% over the best model. This enhancement is attributed to De-MINDS' approach of extracting human intention from manipulation descriptions before searching, enhancing the ability of the CLIP language encoder to understand the user's intention to modify. In Figure 5, De-MINDS accurately understands manipulation intention to change the number of an object and modify the background (row 1), alter the stage and remove an overlapping object (row 2), adjust the camera focus, age of a dog, and remove a specific object (row 3), and modify the style of an image with a specific design (row 4).

In the object composition experiments (Table 3), De-MINDS significantly outperforms the current SoTA model by an average of 4.30%. These results prove the effectiveness of De-MINDS in accurately mapping visual information to the language token space via bridges the gap between pre-training and retrieval, which facilitates the combination of multiple objects, as shown in Figure 6.

Moreover, in the domain conversion results (Table 4), De-MINDS consistently outperforms existing approaches and notably surpasses the SoTA Context-I2W by an average of 4.35%. As illustrated in Figure 4, De-MINDS accurately maps objects within complex scenes (e.g., a saxophonist in the street, a bald eagle on wood, a monkey in the forest, and a sea lion in the water). In contrast, Context-I2W struggles to select the intention-relevant local visual features due to its reliance on image caption without intention, whereas our pseudo-manipulation descriptions are effectively addressed.

    & &  &  &  &  \\  Methods & Conferences & R10 & R50 & R10 & R50 & R10 & R50 & R10 & R50 \\  Image-only & – & 5.4 & 13.9 & 9.9 & 20.8 & 8.3 & 17.7 & 7.9 & 17.5 \\ Text-only & – & 13.6 & 29.7 & 18.9 & 31.8 & 19.3 & 37.0 & 17.3 & 32.9 \\ Image+Text & – & 16.3 & 33.6 & 21.0 & 34.5 & 22.2 & 39.0 & 19.8 & 35.7 \\ Pic2Word  & CVPR 2023 & 20.0 & 40.2 & 26.2 & 43.6 & 27.9 & 47.4 & 24.7 & 43.7 \\ CIReVL  & ICLR 2024 & 24.6 & 44.8 & 29.5 & 47.4 & 31.4 & 53.7 & 28.6 & 48.6 \\ LinCIR  & CVPR 2024 & 20.9 & 42.4 & 29.1 & 46.8 & 28.8 & 50.2 & 26.3 & 46.5 \\  SEARLE-XL & ICCV 2023 & 20.3 & 43.2 & 27.4 & 45.7 & 29.3 & 50.2 & 25.7 & 46.3 \\ SEARLE-XL* & – & 22.7 & 45.0 & 29.4 & 47.9 & 30.2 & 51.4 & 27.4 & 48.1 \\  Context-I2W & AAAI 2024 & 23.1 & 45.3 & 29.7 & 48.6 & 30.6 & 52.9 & 27.8 & 48.9 \\ Context-I2W* & – & 23.9 & 46.9 & 30.4 & 49.7 & 31.1 & 53.8 & 28.5 & 50.1 \\ 
**De-MINDS** & – & **25.2** & **48.7** & **31.0** & **51.2** & **32.9** & **55.7** & **29.7** & **51.9** \\ 
**Query** & **Ours** & **Context-I2W** & **Query** & **Ours** & **Context-I2W** \\   & & & & & & & & \\  & & & & & & & & \\  & & & & & & & & \\ 

Table 1: Results on Fashion-IQ for attribute manipulation.

Figure 3: Results on the attribute manipulation task Figure 4: Results on the domain conversion task.

### Ablation Study

In Table 5, we evaluate the contributions of De-MINDS components on the CIRR and FashionIQ datasets. **(1) In models '2-3', we assess the significance of the intent-CC3M dataset.** Replacing the pseudo-manipulation description with original captions (model '2') results in an average performance drop of 3.80%, demonstrating training with intent-CC3M benefit for aligning intention-relevant visual information. Using a single prompt for pseudo-manipulation descriptions (model '3') causes a 3.14% performance decline, indicating that CoT prompting enhances MLLM in reasoning potential manipulation intention. **(2) In models '4-6', we evaluate key modules in the manipulation intention understanding process.** Without intention embeddings from De-MINDS (model '4'), performance drops by 4.02% on average, proving De-MINDS's importance in CIR. Removing the global feature \(_{cls}\) (model '5') leads to a 2.38% performance decline, highlighting the necessity of comprehensive both global and intention information. Summing global and intention features directly (model '6') causes a 1.64% performance drop, indicating the need for adaptive capture of complementary information. **(3) In models '7-9', we assess De-MINDS's training strategies.** Using only original captions as \(T\) (model '7') reduces training stability, resulting in a 1.62% performance drop. Without the distillation loss (model '8') or replacing it with a cosine loss (model '9') leads to performance drops of 3.58% and 1.54%, respectively, indicating the necessity of symmetric contrastive loss for distilling MLLM's reasoning ability. **In models '10-12', we evaluate alternative solutions.** Not utilizing \(T\) for image-to-text mapping (model '10') results in a 2.30% performance drop, confirming the effectiveness of our pseudo-manipulation descriptions. Applying MiniGPT-4  to generate the intent-CC3M dataset (model '11') results in a 1.18% performance drop, suggesting that a superior MLLM model benefits pseudo-manipulation description quality. Leveraging the LLaMA  rewrite

    &  &  &  &  &  \\  Methods & Conferences & R10 & R50 & R10 & R50 & R10 & R50 & R10 & R50 & R10 & R50 \\  Image-only & – & 0.3 & 4.5 & 0.2 & 1.8 & 0.6 & 5.7 & 0.3 & 4.0 & 0.4 & 4.0 \\ Text-only & – & 0.2 & 1.1 & 0.8 & 3.7 & 0.8 & 2.4 & 0.4 & 2.0 & 0.5 & 2.3 \\ Image+Text & – & 2.2 & 13.3 & 2.0 & 10.3 & 1.2 & 9.7 & 1.6 & 11.6 & 1.7 & 11.2 \\ Pic2Word  & CVPR 2023 & 8.0 & 21.9 & 13.5 & 25.6 & 8.7 & 21.6 & 10.0 & 23.8 & 10.1 & 23.2 \\  Context-I2W  & AAAI 2024 & 10.2 & 26.1 & 17.5 & 28.7 & 11.6 & 27.4 & 12.1 & 28.2 & 12.9 & 27.6 \\ Context-I2W* & – & 11.2 & 27.4 & 18.7 & 30.4 & 12.5 & 29.8 & 13.7 & 31.4 & 14.0 & 29.8 \\ 
**De-MINDS** & – & **13.3** & **31.2** & **20.3** & **34.5** & **14.7** & **31.7** & **16.5** & **34.7** & **16.2** & **33.0** \\   

Table 4: Results on ImageNet for domain conversion.

   Methods & R1 & R5 & R10 \\  Image-only & 7.4 & 23.6 & 34.0 & 57.4 \\ Text-only & 20.9 & 44.8 & 55.5 & 79.1 \\ Image+Text & 12.4 & 3.6 & 49.1 & 78.2 \\ Pic2Word  & 23.9 & 51.7 & 65.3 & 87.8 \\ CIReVL  & 24.6 & 52.3 & 64.9 & 86.3 \\ LinCIR  & 25.0 & 53.3 & 66.7 \\  SEARLE-XL  & 24.2 & 52.4 & 66.3 & 88.6 \\ SEARLE-XL* & 25.4 & 54.1 & 66.9 & 89.3 \\  Context-I2W  & 25.6 & 55.1 & 68.5 & 89.8 \\ Context-I2W* & 26.3 & 55.7 & 69.0 & 90.2 \\ 
**De-MINDS** & **27.3** & **57.0** & **71.3** & **91.6** \\   

Table 2: Results on CIRR for object manipulation task.

Figure 5: Retrieved results on the object manipulation task

   Methods & R1 & R5 & R10 \\  Image-only & 8.6 & 15.4 & 18.9 \\ Text-only & 6.1 & 15.7 & 23.5 \\ Image+Text & 10.2 & 20.2 & 26.6 \\ Pic2Word  & 11.5 & 24.8 & 33.4 \\  Context-I2W  & 13.5 & 28.5 & 38.1 \\ Context-I2W* & 14.3 & 29.7 & 40.5 \\ 
**De-MINDS** & **15.7** & **33.2** & **44.1** \\   

Table 3: Results on COCO for object composition task.

CC3M dataset  (model '12') causes a 3.40% performance drop, indicating the necessity of MLLM for generating pseudo-manipulation description with multi-view supplementary image detail.

### Analysis

**Interpretability of Learnable Query.** In Figure 7, we visualize the top two attention words of each learnable query from the last block, demonstrating the distinct focus of the four queries. Specifically, the first two queries mainly focus on object and attribute information, while the last two queries mostly consider foreground/background and relation information. These attention maps substantiate De-MINDS's interpretability in extracting specific intention across various descriptions, supporting the understanding of intention from manipulation descriptions.

**Effectiveness and Efficiency Analysis.** Our approach achieves significant improvements on four widely compared ZR-CIR tasks from 2.05% to 4.35% over the SoTA models. Designed for understanding manipulation intention, the model size of De-MINDS(58.5M) is larger than the simple 3-layer MLP mapping (0.9M) of Pic2Word. Consequently, our training time (20 hours) is 6 hours longer than Pic2Word under the same settings. Notably, our inference time (0.017s) is \( 58\) faster than CIReVL (\( 1\)s), which uses LLM for inference, and only 0.005s slower than Pic2Word. It's worth noting that our model using just 50% of the pre-training data achieves comparable performance to SoTA models (details are in Appendix A.2).

**Limitation.** While the training process for De-MINDS does not introduce significant additional memory or computational overhead, generating pseudo-manipulation descriptions using MLLMs can be computationally intensive. Moreover, these pseudo descriptions are not filtered, potentially introducing irrelevant details that do not align with actual human manipulation intention. Our paper aims to bridge the gap between pre-training and retrieval in ZS-CIR models and introduce a novel framework to enhance the model's capability to understand user intention. Future work could explore more efficient methods to generate pseudo-manipulation descriptions while maintaining performance.

## 5 Conclusion

In this paper, we introduce intent-CC3M, an intention-based dataset featuring pseudo-manipulation descriptions reasoned through chain-of-thought prompting by an MLLM for training mapping networks to align intention-relevant visual information. Leveraging intent-CC3M, we propose a novel manipulation intention understanding network that employs learnable queries to enhance the models' capability to understand user intention from manipulation descriptions for accurate CIR. De-MINDS shows strong generalization ability and remarkably improves the best performance of existing approaches on four diverse ZS-CIR tasks with comparable inference times. Our work inspires intention-based image retrieval and impacts diverse vision and language applications.

    &  &  \\   & Methods & R1 & R5 & R10 & R10 & R50 \\ 
1. & full model & 27.3 & 57.0 & 71.3 & 29.7 & 51.9 \\
**Significant of intent-CC3M** & 24.6 & 53.7 & 67.1 & 26.0 & 46.8 \\
3. & w/o CGT & 25.2 & 54.3 & 67.8 & 26.7 & 47.5 \\
**Key modules of De-MINDS** & **process** & & & & \\
4. & w/o De-MINDS & 24.0 & 53.5 & 67.2 & 25.8 & 46.6 \\
5. & w/o global feature & 25.5 & 55.2 & 68.0 & 27.3 & 49.6 \\
6. & w/o gate & 25.9 & 55.3 & 69.5 & 27.9 & 50.4 \\
**Training Strategies** & & & & & \\
7. & w/o construct \(T\) & 26.2 & 55.6 & 69.3 & 27.8 & 50.2 \\
8. & w/o distil & 24.8 & 53.9 & 67.3 & 26.3 & 47.0 \\
9. & cos distill & 26.2 & 55.5 & 69.7 & 27.9 & 50.2 \\
**Alternative solutions** & & & & & \\
10. & a photo of \(S_{*}\) & 25.5 & 55.2 & 67.9 & 27.5 & 49.6 \\
11. & MiniQFT4’s caption & 26.4 & 55.7 & 70.2 & 28.2 & 50.8 \\
12. & LLM’s caption & 25.2 & 53.7 & 67.2 & 26.9 & 47.2 \\   

Table 5: Ablation study of main components on CIRR and FashionIQ.

Figure 7: Visualization of the top two attention words for each learnable query, different colors denoting the results corresponding to each query.