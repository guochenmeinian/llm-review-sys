# Automating Data Annotation under Strategic Human Agents: Risks and Potential Solutions

Tian Xie

Computer Science and Engineering

the Ohio State University

Columbus, OH 43210

xie.1379@osu.edu

&Xueru Zhang

Computer Science and Engineering

the Ohio State University

Columbus, OH 43210

zhang.12807@osu.edu

###### Abstract

As machine learning (ML) models are increasingly used in social domains to make consequential decisions about humans, they often have the power to reshape data distributions. Humans, as strategic agents, continuously adapt their behaviors in response to the learning system. As populations change dynamically, ML systems may need frequent updates to ensure high performance. However, acquiring high-quality _human-annotated_ samples can be highly challenging and even infeasible in social domains. A common practice to address this issue is using the model itself to annotate unlabeled data samples. This paper investigates the long-term impacts when ML models are retrained with _model-annotated_ samples when they incorporate human strategic responses. We first formalize the interactions between strategic agents and the model and then analyze how they evolve under such dynamic interactions. We find that agents are increasingly likely to receive positive decisions as the model gets retrained, whereas the proportion of agents with positive labels may decrease over time. We thus propose a _refined retraining process_ to stabilize the dynamics. Last, we examine how algorithmic fairness can be affected by these retraining processes and find that enforcing common fairness constraints at every round may not benefit the disadvantaged group in the long run. Experiments on (semi-)synthetic and real data validate the theoretical findings.

## 1 Introduction

As machine learning (ML) is increasingly used to automate human-related decisions (e.g., in lending, hiring, college admission), there is a growing concern that these decisions are vulnerable to human strategic behaviors. With the knowledge of decision policy, humans may adapt their behavior strategically in response to ML models, e.g., by changing their features at costs to receive favorable outcomes. A line of research called _Strategic Classification_ studies such problems by formulating mathematical models to characterize strategic interactions and developing algorithms robust to strategic behavior . Among existing works, most studies focus on one-time model deployment where an ML model is trained and applied to a fixed population of strategic agents _once_.

However, practical ML systems often need to be retrained periodically to ensure high performance on the current population. As the ML model gets updated, human behaviors also change accordingly.

Figure 1: Illustration of updating the training data from \(t\) to \(t+1\) during the retraining process with strategic feedback training process with strategic feedbackTo prevent the potential adverse outcomes, it is critical to understand how the strategic population is affected by the model retraining process. Traditionally, the data used for retraining models can be constructed manually with human annotations (e.g., ImageNet). However, acquiring a large amount of _human-annotated_ samples can be highly difficult and even infeasible, especially in human-related applications, e.g., in automated hiring where an ML model is used to identify qualified applicants, even an experienced interviewer needs time to label an applicant.

Motivated by a recent practice of _automating_ data annotation for retraining large-scale ML models , we study strategic classification in a sequential framework where an ML model is periodically retrained by a decision-maker with both _human_ and _model-annotated_ samples. These updated models are deployed sequentially on agents who may modify their features to receive favorable outcomes. Since ML models affect agent behavior and the agent strategic feedback can further be captured when retraining the future model, their interactions drive both to change dynamically over time. However, it remains unclear how the two evolve under such dynamics and what long-term effects one may have on the other.

To further illustrate our problem, consider an example of college admission where new students from a population apply each year. In the \(t\)-th year, an ML model \(f_{t}\) is learned from a training dataset \(_{t}\) and used to make admission decisions. For students who apply in the \((t+1)\)-th year, they will best respond to the model \(f_{t}\) in the previous year (e.g., preparing the application package in a way that maximizes the chance of getting admitted). Meanwhile, the college retrains the classifier \(f_{t+1}\) using a new training dataset \(_{t+1}\) consisting of previous training data \(_{t}\), new _human-annotated_ samples, and new _model-annotated_ samples (i.e., previous applicants annotated by the most recent model \(f_{t}\)). The model \(f_{t+1}\) is then used to make admission decisions in the \((t+1)\)-th year. This process continues over time and we demonstrate how the training dataset \(_{t}\) is updated to \(_{t+1}\) in Fig. 1. Under such dynamics, both the ML system and the strategic population change over time and may lead to unexpected long-term consequences. An illustrating example is given in Fig. 2.

In this paper, we examine the evolution of the ML model and the agent data distribution. We ask: 1) How does the agent population evolve when the model is retrained with strategic feedback? 2) How is the ML system affected by the agent's strategic response? 3) If agents come from multiple social groups, how can model retraining further impact algorithmic fairness? Can imposing group fairness constraints during model training bring long-term societal benefits?

Compared to prior studies on _strategic classification_ under sequential settings  that mainly focused on developing a classifier robust to strategic agents, we study the long-term impacts of model retraining with agent strategic feedback. Instead of assuming actual labels are available while retraining, we consider more practical scenarios with _model-annotated_ samples. Although the risks on accuracy  and fairness  of using _model-annotated_ samples to retrain models have been highlighted, ours is the first to incorporate strategic feedback from human agents. In App. C, we discuss more related works in detail. Our contributions are summarized as follows:

1. We formulate the problem of model retraining with human strategic feedback (Sec. 2).
2. We theoretically characterize the evolution of the expected _acceptance rate_ (i.e., the proportion of agents receiving positive classifications), _qualification rate_ (i.e., the proportion of agents with positive labels), and the _classifier bias_ (i.e., the discrepancy between acceptance rate and qualification rate) under the retraining process. We show that the acceptance rate increases over time under retraining, while the actual qualification rate may decrease under certain conditions. The dynamics of _classifier bias_ are more complex depending on the systematic bias of _human-annotated_ samples. Finally, we propose an approach to stabilize the dynamics (Sec. 3).

Figure 2: Evolution of the student distribution and ML model at \(t=0\) (left), \(t=5\) (middle), and \(t=14\) (right): each student has two features. At each time, a classifier is retrained with both human and model-annotated samples, and students best respond to be admitted (Fig. 1). Over time, the learned classifier (black lines) deviates from ground truth (green lines).

3. We consider settings where agents come from multiple social groups and investigate how inter-group fairness can be affected by the model retraining process; we also investigate the long-term effects of fairness intervention at each round of model retraining (Sec. 4).
4. We conduct experiments on (semi-)synthetic and real data to verify the theorems (Sec. 5, App. E, App. F).

## 2 Problem Formulation

Consider a population of agents who are subject to certain ML decisions (e.g., admission/hiring decisions) and join the decision-making system in sequence. Each agent has observable continuous features \(X^{d}\) and a hidden binary label \(Y\{0,1\}\) indicating its qualification state ("1" being qualified and "0" being unqualified). Let \(P_{XY}\) be the joint distribution of \((X,Y)\) which is fixed over time, and \(P_{X}\), \(P_{Y|X}\) be the corresponding marginal and conditional distributions. Assume \(P_{X}\), \(P_{Y|X}\) are continuous with non-zero probability mass everywhere in their domain. For agents who join the system at time \(t\), the decision-maker uses a classifier \(f_{t}:^{d}\{0,1\}\) to make decisions. Note that the decision-maker does not know \(P_{XY}\) and can only learn \(f_{t}\) from the training dataset at \(t\).

**Agent best response.** Agents who join the system at time \(t\) can adapt their behaviors based on the latest classifier \(f_{t-1}\) and change their features \(X\) strategically. We denote the resulting data distribution as \(P_{XY}^{t}\). Specifically, given original features \(X=x\), agents have incentives to change their features at costs to receive positive classification outcomes, i.e., by maximizing utility

\[x_{t}=_{z}\ \{f_{t-1}(z)-c(x,z)\}\] (1)

where distance function \(c(x,z) 0\) measures the cost for an agent to change features from \(x\) to \(z\). In this paper, we consider \(c(x,z)=(z-x)^{T}B(z-x)\) for some \(d d\) positive semidefinite matrix \(B\), allowing heterogeneous costs for different features. After agents best respond, their data distribution changes from \(P_{XY}\) to \(P_{XY}^{t}\). In this paper, we term \(P_{XY}\) agent _prior-best-response_ distribution and \(P_{XY}^{t}\)_post-best-response_ distribution. We consider natural settings that (i) agents need time to adapt their behaviors and their responses are _delayed_: they act based on the latest classifier \(f_{t-1}\) they are aware of, not the one they receive; (ii) agent behaviors are benign and feature changes can genuinely affect their underlying labels, so feature-label relationship \(P_{Y|X}^{t}=P_{Y|X}\) is fixed over time [9; 11].

**Human-annotated samples and systematic bias.** At each round \(t\), we assume the decision-maker can draw a limited number of unlabeled samples from the prior-best-response distribution \(P_{X}\).1 With some prior knowledge (possibly biased), the decision-maker can annotate these features and generate _human-annotated_ samples \(_{o,t}\). We assume the quality of human annotations is consistent, so \(_{o,t}\) at any \(t\) is drawn from a fixed probability distribution \(D_{XY}^{o}\) with marginal distribution \(D_{X}^{o}=P_{X}\). Because human annotations may not be the same as true labels, \(D_{Y|X}^{o}\) can be biased compared to \(P_{Y|X}\). We define such difference as the decision-maker's _systematic bias_, formally stated below.

**Definition 2.1** (Systematic bias).: Define \((D^{o},P):=_{x P_{X}}[D_{Y|X}^{o}(1|x)-P_{Y|X}(1|x)]\). The decision-maker has a systematic bias if \((D^{o},P)>0\) (overestimation) or \(<0\) (underestimation).

Def. 2.1 implies that the decision-maker has a systematic bias when it labels a larger (or smaller) proportion of agents as qualified compared to the ground truth. In App. B, we present numerous examples of systematic bias in real applications where the decision-maker has different systematic biases towards different _demographic groups_. Generally, the systematic bias may or may not exist and we study both scenarios in the paper.

**Model-annotated samples.** In addition to human-annotated samples, the decision-maker at each round \(t\) can also utilize the most recent classifier \(f_{t-1}\) to generate _model-annotated_ samples for training \(f_{t}\). Specifically, let \(\{x_{t-1}^{i}\}_{i=1}^{N}\) be \(N\) post-best-response features ((1)) acquired from the agents coming at \(t-1\), the decision-maker uses \(f_{t-1}\) to annotate the samples and obtain _model-annotated_ samples \(_{m,t-1}=\{x_{t-1}^{i},f_{t-1}(x_{t-1}^{i})\}_{i=1}^{N}\). Both human and model-annotated samples are used to retrain the classifier at \(t\).

**Classifier's retraining process.** With the human and model-annotated samples, we next introduce how the model is retrained by the decision-maker over time. Denote the training dataset at \(t\) as \(_{t}\). Initially, the decision-maker trains \(f_{0}\) with a _human-annotated_ training dataset \(_{0}=_{o,0}\). Then the decision-maker updates \(f_{t}\) every round to make decisions about agents, and it learns \(f_{t}\) using empirical risk minimization (ERM) with training dataset \(_{t}\). Similar to studies in strategic classification , we consider linear classifier in the form of \(f_{t}(x)=(h_{t}(x))\) where \(h_{t}:\) is the scoring function (e.g., logistic function) and \(h_{t}\). At each round \(t 1\), \(_{t}\) consists of three components: existing training samples \(_{t-1}\), \(N\) new _model-annotated_ and \(K\) new _human-annotated_ samples:

\[_{t}=_{t-1}_{m,t-1}_{o,t -1},\ \  t 1\] (2)

Since annotating agents is usually time-consuming and expensive, we have \(N K\) in practice. The complete retraining process is shown in Alg. 1 (App. A).

Given the post-best-response distribution \(P_{XY}^{t}\), we can define the associated _qualification rate_ as the probability that agents are qualified, i.e.,

\[Q(P^{t})=_{(x,y) P_{XY}^{t}}[y].\]

For the classifier \(f_{t}\) deployed on marginal feature distribution \(P_{X}^{t}\), we define _acceptance rate_ as the probability that agents are classified as positive, i.e.,

\[A(f_{t},P^{t})=_{x P_{X}^{t}}[f_{t}(x)].\]

Since \(_{t}\) is randomly sampled at all \(t\), the resulting classifier \(f_{t}\) and agent best response are also random. Denote \(D_{XY}^{t}\) as the probability distribution of sampling from \(_{t}\) and recall that \(D_{XY}^{o}\) is the distribution for _human-annotated_\(_{o,t}\), we can further define the expectations of \(Q(P^{t}),A(f_{t},P^{t})\) over the training dataset:

\[q_{t}:=_{_{t-1}}[Q(P^{t})]; a_{t}:=_{ _{t}}[A(f_{t},P^{t})],\]

where \(q_{t}\) is the expected actual qualification rate of agents after they best respond, note that the expectation is taken with respect to \(_{t-1}\) because the distribution \(P_{XY}^{t}\) is the result of agents responding to \(f_{t-1}\) which is trained with \(_{t-1}\); \(a_{t}\) is the expected acceptance rate of agents at time \(t\).

**Dynamics of qualification rate & acceptance rate.** Under the model retraining process, both the model \(f_{t}\) and agent distribution \(P_{XY}^{t}\) change over time. One goal is to understand how the agents and the ML model interact and impact each other in the long run. Specifically, we are interested in the dynamics of the following variables:

1. **Qualification rate \(q_{t}\)**: it measures the qualification of agents and indicates the _social welfare_.
2. **Acceptance rate \(a_{t}\)**: it measures the likelihood that an agent can receive positive outcomes and indicates the _applicant welfare_.
3. **Classifier bias \(_{t}=|a_{t}-q_{t}|\)**: it is the discrepancy between the acceptance rate and the true qualification rate, measuring how well the decision-maker can approximate agents' actual qualification rate and can be interpreted as _decision-maker welfare_.

In the rest of the paper, we study the dynamics of \(q_{t},a_{t},_{t}\) and we aim to answer the following questions: 1) How do the qualification rate \(q_{t}\), acceptance rate \(a_{t}\), and classifier bias \(_{t}\) evolve under the dynamics? 2) How can the evolution of the system be affected by the decision-maker's retraining process? 3) What are the impacts of the decision-maker's systematic bias? 4) If we further consider agents from multiple social groups, how can the retraining process affect inter-group fairness?

## 3 Dynamics of the Agents and Model

In this section, we examine the evolution of qualification rate \(q_{t}\), acceptance rate \(a_{t}\), and classifier bias \(_{t}\). We aim to understand how _applicant welfare_ (Sec. 3.1), _social welfare_ (Sec. 3.2), and _decision-maker welfare_ (Sec. 3.3) are affected by the retraining process in the long run. We first introduce some assumptions used for the theorems.

**Assumption 3.1**.: Hypothesis class \(\) can perfectly learn the training data distribution \(D^{t}_{Y|X}\), i.e., \( h^{*}_{t}\) such that \(h^{*}_{t}(x)=D^{t}_{Y|X}(1|x)\).

With Assumption 3.1, we avoid the effects of learning error on the system dynamics; this allows us to focus on the dynamic interactions between strategic agents and ML system. Although the theoretical analysis relies on the assumption, our experiments in Sec. 5 and App. F show consistent results with theorems. We further assume the monotone likelihood ratio property holds for \(D^{o}_{XY}\) and \(P_{XY}\).

**Assumption 3.2**.: Let \(x[m]\) be the \(m^{th}\) dimension of \(x^{d}\), then \(D^{o}_{Y|X}(1|x)\) and \(P_{Y|X}(1|x)\) are continuous and monotonically increasing in \(x[m],\  m=1,,d\) while other dimensions are fixed.

Note that the Assumption 3.2 is mild and widely used in previous literature [12; 13]. It can be satisfied by many distributional families such as exponential, Gaussian, and mixtures of exponential/Gaussian. It implies that agents are more likely to be qualified as feature value increases.

### Applicant welfare: dynamics of acceptance rate

We first examine the dynamics of \(a_{t}=_{_{t}}[A(f_{t},P^{t})]\). Intuitively, under Assumption 3.1, all classifiers can fit the training data well. Then the model-annotated samples \(_{m,t-1}\) generated from post-best-response agents would have a higher qualification rate than the qualification rate of training data \(_{t-1}\). As a result, the training data \(_{t}\) augmented with \(_{m,t-1}\) has a higher proportion of qualified agents than the qualification rate of \(_{t-1}\), thereby producing a more "generous" classifier \(f_{t}\) with a larger \(a_{t}\). This reinforcing process can be formally stated in Thm. 3.3.

**Theorem 3.3** (Evolution of \(a_{t}\)).: _Under the retraining process, the acceptance rate of the agents that join the system increases over time, i.e., \(a_{t}>a_{t-1}\), \( t 1\)._

We prove Thm. 3.3 by mathematical induction in App. G.3 and Fig. 3 illustrates the theorem. When agents best respond, the decision-maker tends to accept more agents. We can further show that when the number of _model-annotated_ samples \(N\) is large compared to the number of _human-annotated_ samples \(K\), the classifier will ultimately accept all agents in the long run (Prop. 3.4).

**Proposition 3.4**.: _For any \(P_{XY},D^{o},B\), there exists a threshold \(>0\) such that \(_{t}a_{t}=1\) whenever \(<\)._

The specific value of \(\) in Prop. 3.4 depends on \(P_{XY},D^{o},B\), which is difficult to find analytically. Nonetheless, we illustrate in Sec. 5 that when \(=0.05\), \(a_{t}\) tends to approach \(1\) in numerous datasets. Since the _human-annotated_ samples are often difficult to attain (due to time and labeling costs), the condition in Prop. 3.4 is easy to satisfy in practice.

### Social welfare: dynamics of qualification rate

Next, we study the dynamics of qualification rate \(q_{t}=_{_{t-1}}[Q(P^{t})]\). Unlike the acceptance rate \(a_{t}\) which always increases during the retraining process, the evolution of \(q_{t}\) is more complicated and depends on agent prior-best-response distribution \(P_{XY}\).

Specifically, let \(q_{0}=Q(P)=_{(x,y) P_{XY}}[y]\) be the initial qualification rate, then the difference between \(q_{t}\) and \(q_{0}\) can be interpreted as the amount of _improvement_ (i.e., increase in label) agents

Figure 3: Increasing acceptance rate from \(a_{t}\) to \(a_{t+1}\). Unqualified/qualified agents are shown as circles/squares, while the admitted/rejected agents are shown in red/blue. New agents coming at \(t+1\) are shown in hollow. **The left plot** shows the training set \(_{t}\) containing 2 unqualified (red circle) and 2 qualified agents (blue square) and \(a_{t}\) is 0.5. **The middle plot** shows the agents coming at \(t\) best respond to \(f_{t-1}\). After the responses, 3 of 4 agents are qualified (blue square) and 1 is still unqualified (blue circle). However, all 4 agents are annotated as “qualified” (blue). **The right plot** shows the training set \(_{t+1}\) containing all points of the left and middle plot, plus two new human-annotated points (hollow points). All blue points are labeled as 1 and the red points are 0. So \(_{t+1}\) has more samples with a positive label (0.7), resulting in \(f_{t+1}\) accepting a higher proportion of agents.

gain from their strategic behavior at \(t\). This is determined by (i) the proportion of agents that decide to change their features at costs (depends on \(P_{X}\)), and (ii) the improvement agents can expect upon changing features (depends on \(P_{Y|X}\)). Thus, the dynamics of \(q_{t}\) depend on \(P_{XY}\). Despite the intricate nature of dynamics, we can still derive a condition under which \(q_{t}\) decreases monotonically.

**Theorem 3.5** (Evolution of \(q_{t}\)).: _Consider the setting where the \(d\)-dimensional feature space \(X^{d}\) where \(F_{X}(x),P_{X}(x),P_{Y|X}(1|x)\) are the cumulative distribution function, probability density function and the labeling function when \(Y=1\). Denote \(=\{x|f_{0}(x)=0\}\) as the half-space in \(^{d}\) determined by the classifier \(f_{0}\). Under the retraining process, \( t 1,\ q_{t+1} q_{t}\) if either of the following conditions holds: (i) \(F_{X}\) and \(P_{Y|X}(1|x)\) are convex on \(\); (ii) for each dimension \(x[i],i[d]\), \(F_{X}(x)\) and \(P_{Y|X}(1|x)\) are convex with respect to \(x[i]\)._

Note that \(q_{t+1} q_{t}\) in Thm. 3.5 holds only for \(t 1\). Because agent behavior can only improve their labels, prior-best-response \(q_{0}\) always serves as the lower bound of \(q_{t}\). The half-space \(\) in Thm. 3.5 specifies the region in feature space where agents have incentives to change their features. The convexity of \(F_{X}\) and \(P_{Y|X}(1|x)\) ensure that as \(f_{t}\) evolves from \(t=1\): (i) fewer agents choose to improve their features, and (ii) agents expect less improvement from feature changes. Thus, \(q_{t}\) decreases over time. Conditions in Thm. 3.5 can be satisfied by common distributions \(P_{X}\) (e.g., Uniform, Beta\((,1)\) with \(>1\)) and labeling functions \(P_{Y|X}(1|x)\) (e.g., linear function, quadratic functions with degree greater than 1). The proof and a more general analysis are shown in App. G.5. We also show that Thm. 3.5 is valid under diverse experimental settings (Sec. 5, App. E, App. F).

### Decision-maker welfare: dynamics of classifier bias

Sec. 3.1 and 3.2 show that as the classifier \(f_{t}\) gets updated over time, agents are more likely to get accepted (\(a_{t}\) increases). However, their true qualification rate \(q_{t}\) (after the best response) may actually decrease. It indicates that the decision-maker's misperception about agents varies over time. Thus, this section studies the dynamics of classifier bias \(_{t}=|a_{t}-q_{t}|\). Our results show that the evolution of \(_{t}\) is largely affected by the decision-maker's systematic bias \((D^{o},P)\) as defined in Def. 2.1.

**Theorem 3.6** (Evolution of \(_{t}\)).: _Starting from \(t=1\) at the retraining process and under conditions in Thm. 3.5:_

1. _If_ \((D^{o},P)=0\)_, i.e., the systematic bias does not exist, then_ \(_{t}\) _increases over time._
2. _If_ \((D^{o},P)>0\)_, i.e., the decision-maker overestimates agent qualification, then_ \(_{t}\) _increases over time._
3. _If_ \((D^{o},P)<0\)_, i.e., the decision-maker underestimates agent qualification,_ \(_{t}\) **either** _monotonically decreases_ _or_ _first decreases but then increases._

Thm. 3.6 highlights the potential risks of the model retraining process and is proved in App. G.6. Originally, the purpose of retraining the classifier was to ensure accurate decisions on the targeted population. However, when agents behave strategically, the retraining may lead to adverse outcomes by amplifying the classifier bias. Meanwhile, though systematic bias is usually an undesirable factor to eliminate when learning ML models, it may help mitigate classifier bias to improve the _decision-maker welfare_ in the retraining process, i.e., \(_{t}\) decreases when \((D^{o},P)<0\).

### Intervention to stabilize the dynamics

Sec. 3.1- 3.3 show that as the model is retrained from strategic agents, \(a_{t},q_{t},_{t}\) are unstable and may change monotonically over time. Next, we introduce an effective approach to stabilizing the system.

From the above analysis, we know that one reason that makes \(q_{t}\), \(a_{t}\), \(_{t}\) evolve is agent's best response, i.e., agents improve their features strategically to be accepted by the most recent model, which leads to a higher qualification rate of _model-annotated_ samples (and the resulting training data), eventually causing \(a_{t}\) to deviate from \(q_{t}\). Thus, to mitigate such deviation, we can improve the quality of model annotation. Our method is proposed based on this idea, which uses a _probabilistic sampler_ when producing _model-annotated_ samples.

Specifically, at each time \(t\), instead of adding \(_{m-1,o}=\{x_{t-1}^{i},f_{t-1}(x_{t-1}^{i})\}_{i=1}^{N}\) (samples annotated by the model \(f_{t-1}\)) to training data \(_{t}\) (2), we use the probabilistic model \(h_{t-1}(x)\) to annotate each sample according to the following: _For each sample \(x\), we label it as \(1\) with probability \(h_{t-1}(x)\), and as 0 otherwise._ Here \(h_{t-1}(x) D_{Y|X}^{t-1}(1|x)\) is the estimated posterior probability learned from \(_{t-1}\)(e.g., using logistic model). We call the procedure **refined retraining process** if _model-annotated_ samples are generated in this way based on a probabilistic sampler.

Fig. 3 also illustrates the above idea: agents best respond to \(f_{t-1}\) (middle plot) to improve and \(f_{t}\) will label both as \(1\). By contrast, a probabilistic sampler \(h_{t}\) only labels a fraction of them as \(1\). This alleviates the influence of agents' best responses to stabilize the dynamics of \(a_{t},q_{t},_{t}\). Prop. D.2 and App. F.3 provide proofs and more experiments for the **refined retraining process**.

## 4 Impacts on Algorithmic Fairness

In this section, we further consider agents from multiple social groups and investigate how group fairness can be affected by the model retraining process. Similar to prior studies in fair ML [12; 14; 15], we assume the decision-maker knows the group identity of each agent and uses group-dependent classifiers to make decisions. WLOG, we present the results for any pair of two groups \(i,j\). Among the two groups, we define the group with a smaller acceptance rate under unconstrained optimal classifiers as **disadvantaged group**.

### Impacts of systematic bias & model retraining

We first consider the situation where two groups have no innate difference: they have the same prior-best-response feature distribution and the same cost matrix \(B\) to change features. However, the decision-maker has a systematic bias in favor of group \(i\) more than group \(j\), making \(i\) the advantaged group and \(j\) the disadvantaged group. We consider the fairness metric _demographic parity_ (DP) , which measures unfairness as the difference in acceptance rate across two groups. Extension to other fairness metrics such as _equal opportunity_ is discussed in App. D.2. Thm. 4.1 below shows the long-term impacts of refined retraining process (i.e., model-annotated samples generated with probabilistic sampler \(h_{t}\)) and original model retraining process (i.e., model-annotated samples generated with model \(f_{t}\)) on group unfairness.

**Theorem 4.1** (Impacts of model retraining on unfairness).: _When groups \(i,j\) have no innate difference, but \(j\) is disadvantaged due to systematic bias: (i) If applying **original retraining process to both groups** with \(\) satisfying Prop. 3.4, then group \(j\) will stay disadvantaged until all agents in both groups are accepted to achieve perfect fairness; (ii) If applying **refined retraining process to both groups**, then group \(j\) will stay disadvantaged and the unfairness remains the same in the long run; (iii) If applying **refined retraining process to group \(i\) but the original process to group \(j\)** with \(\) satisfying Prop. 3.4, then unfairness first decreases after certain rounds of retraining until group \(j\) becomes advantaged, then unfairness increases._

Thm. 4.1 shows that original and refined retraining processes impact differently on group fairness as proved in App. G.9. Specifically, the original retraining process ultimately attains "trivial" perfect fairness by accepting all agents, whereas the refined retraining process stabilizes the dynamics but unfairness always exists. Interestingly, applying disparate retraining strategies to two groups may result in perfect fairness in the middle of retraining process. This suggests that it may be beneficial for the decision-maker to monitor the fairness measure during the retraining and execute an _early stopping_ mechanism to attain almost perfect DP fairness, i.e., stop retraining models early once unfairness reaches the minimum. As shown in the right plot of Fig. 4, when refined retraining is only applied to group \(i\), unfairness is minimized at \(t=5\).

### Impact of short-term fairness intervention

Sec. 4.1 proposed a solution of "disparate retraining with early stopping" to mitigate unfairness. A more common method to maintain fairness throughout the retraining process is to enforce certain fairness constraints every time when updating the models. Next, we consider this method where

Figure 4: Unfairness (DP) when the refined retraining process is applied to both groups (left), original retraining process is applied to both groups (middle), refined retraining process is only applied to group \(i\) (right) under dataset 2.

refined retraining process_ is applied to both groups (to stabilize dynamics) and a fairness constraint is imposed at each round of model retraining. Unlike Sec. 4.1, we consider a general setting where two groups may have different feature distributions and different cost matrices \(B\), but feature-label relation \(P_{Y|X}(1|x)\) is the same across groups.

**Finding fair models.** For each group \(s\), we use superscript \(s\) to denote the group-specific distributions/metrics listed in Sec. 2 (e.g., \(P_{XY}^{s+}\)). At each round \(t\), the decision-maker first trains a linear model \(f_{t}^{s}=(h_{t}^{s}(x))\) for group \(s\). According to Assumption 3.1 and Prop. D.2, \(h_{t}^{s}\) is expected to be \(D_{Y|X}^{s}(1|x)=P_{Y|X}^{s}(1|x)+_{s}\), where \(_{s}\) is the systematic bias towards group \(s\). Denote \(_{t}^{s}\) as the original optimal threshold at \(t\) without the fairness intervention, and let \(a_{t}^{s}\) be the acceptance rate for group \(s\) under \(_{t}^{s}\), then the decision-maker can tune the thresholds to get fair-optimal thresholds \(_{t}^{s}\) (a pair of thresholds satisfying the fairness constraint with the largest aggregated accuracy).

**Noisy agent best response.** To ensure there always exists a pair of thresholds that satisfy DP fairness constraint (i.e., equal acceptance rate), each group's post-best-response distribution needs to be continuous. However, when agents modify their features based on (1), the aggregate response necessarily exhibits discontinuities . To tackle this issue, we consider the _noisy best response_ model proposed by Jagadeesan et al. , which assumes the agents only have imperfect and noisy information of decision threshold. Formally, given decision threshold \(\), each agent best responds to \(+\) where \(\) is a noise independently sampled from a zero mean distribution with finite variance \(^{2}\). Under noisy best response, we investigate the impacts of fairness intervention.

**Theorem 4.2** (Impact of fairness intervention).: _Suppose group \(j\) is disadvantaged from round \(0\) to \(t\), i.e., group \(j\) has a smaller acceptance rate than group \(i\) under unconstrained optimal thresholds \(\{_{}^{i},_{}^{j}\}\), \( t\). Let \(_{t}^{2}\) be the variance of the noisy best response at \(t\). We have the following:_

_(i) **Without fairness intervention**, \(_{t}\), there always exists feature distributions and cost matrices under which group \(j\) switches to be advantaged at \(t+1\);_

_(ii) **With fairness intervention**, if \(_{t}<(_{t}^{j}-_{t}^{j})^{i}-a_{t}^ {j}}\), then group \(j\) always remains disadvantaged at \(t+1\)._

Thm. 4.2 shows that without fairness intervention, the originally disadvantaged group \(j\) can flip to be advantaged. In contrast, the fairness intervention helps maintain the disadvantaged and advantaged groups when the agent's perception of the decision rule is sufficiently accurate. Note that the bound on \(\) is well-defined. Since group \(j\) is disadvantaged, \(a_{0}^{i}>a_{0}^{j}\) always holds. If \(_{t}<(_{t}^{j}-_{t}^{j})^{i}-a_{t}^ {j}}\) holds, then it is guaranteed that \(a_{0}^{i}>a_{t+1}^{j}\) (see App. G.10 for details). This result implies that the disadvantaged group, by losing their chance to become advantaged, may not benefit from fairness intervention in the long run.

## 5 Experiments

We conduct experiments on two synthetic (Uniform, Gaussian), one semi-synthetic (German Credit ), and one real dataset (Credit Approval ) to validate the dynamics of \(a_{t},q_{t},_{t}\) and the unfairness 2. Note that only the Uniform dataset satisfies all assumptions and the conditions in our theoretical analysis, while the Gaussian and German Credit datasets violate the conditions in Thm. 3.5. The Credit Approval dataset violates all assumptions and conditions of the main paper. The decision-maker trains logistic regression models for all experiments using stochastic gradient descent (SGD) over \(T\) steps. We present the experimental results of the Gaussian and German Credit datasets to illustrate the dynamics of \(a_{t},q_{t},_{t}\) in this section, while the results for Uniform and Credit Approval data are similar and shown in App. E.

**Gaussian data.** We consider a synthetic dataset with Gaussian distributed \(P_{X}\). \(P_{Y|X}\) is logistic and satisfies Assumption 3.2 but not the conditions of Thm. 3.5. We assume agents have two independent features \(X_{1},X_{2}\) and are from two groups \(i,j\) with different sensitive attributes but identical joint distribution \(P_{XY}\). Their cost matrix is \(B=5&0\\ 0&5\)and the initial qualification rate is \(q_{0}=0.5\). We assume the decision-maker has a systematic bias by overestimating (resp. underestimating) the qualification of agents in the advantaged group \(i\) (resp. disadvantaged group \(j\)), which is modeled as increasing \(D_{Y|X}^{o}(1|x)\) to be 0.1 larger (resp. smaller) than \(P_{Y|X}(1|x)\) for group \(i\) (resp. group \(j\)). For the retraining process, we let \(r==0.05\) (i.e., the number of model-annotated samples \(N=2000\), which is sufficiently large compared to the number of human-annotated samples \(K=100\)). Table 1 summarizes the dataset information, and the joint distributions are visualized in App. F.1.

We verify the results in Sec. 3 by illustrating the dynamics of \(a_{t},q_{t},_{t}\) for both groups (Fig. 4(a)). Since our evolution results are in expectation, we perform \(n=100\) independent runs of experiments for every parameter configuration and show the averaged outcomes. The results are consistent with Thm. 3.3, 3.5 and 3.6: (i) acceptance rate \(a_{t}\) (red curves) increases monotonically; (ii) qualification rate \(q_{t}\) decreases monotonically starting from \(t=1\) (since strategic agents only best respond from \(t=1\)); (iii) classifier bias \(_{t}\) evolves differently for different groups and it may reach the minimum after a few rounds of retraining.

We further test the robustness of system dynamics against agent noisy response, where we assume agents estimate their outcomes as \(_{t}(x)=f_{t}(x)+\) with \((0,0.1)\). We present the dynamics of \(a_{t},q_{t},_{t}\) for both groups in Fig. 5(a) which are similar to Fig. 4(a), demonstrating the robustness of our theorems.

**German Credit dataset .** This dataset includes features for predicting individuals' credit risks. It has 1000 samples and 19 numeric features, which are used to construct a larger-scaled dataset. Specifically, we fit a kernel density estimator for all 19 features to generate 19-dimensional features, the corresponding labels are sampled from the distribution \(P_{Y|X}\) which is estimated from data by fitting a logistic classifier with 19 features. Given this dataset, the first 10 features are used to train the classifiers. The attribute "sex" is regarded as the sensitive attribute. The systematic bias is created by increasing/decreasing \(P_{Y|X}\) by \(0.06\). Other parameters \(n,r,T,q_{0}\) are the same as Table 1. Since \(P_{Y|X}\) is a logistic function, Assumption 3.2 can be satisfied easily as illustrated in App. F.1.

We verify the results in Sec. 3 by illustrating the dynamics of \(a_{t},q_{t},_{t}\) for both groups (Fig. 4(b)). The results are consistent with Thm. 3.3, 3.5 and 3.6: (i) acceptance rate \(a_{t}\) (red curves) always increases; (ii) qualification rate \(q_{t}\) (blue curves) decreases starting from \(t=1\) (since strategic agents only best respond from \(t=1\)); (iii) classifier bias \(_{t}\) (black curves) evolve differently for different groups. Finally, similar to Fig. 4(b), Fig. 4(b) demonstrates the results are still robust under the noisy setting.

**Additional experiments.** We provide more results in App. F to: (i) verify Thm. 3.3 and Thm. 3.6; (ii) Visualize the influence of **each factor** including training rounds, cost matrices, the ratio between _human-annotated_ and _model-annotated_ samples, whether the agents are strateg

Figure 5: Dynamics of \(a_{t},q_{t},_{t}\) under the perfect information setting

Figure 6: Dynamics of \(a_{t},q_{t},_{t}\) under the noisy setting

assumptions are violated; (iii) visualize the evolution of unfairness when different retraining strategies are applied to different groups.

## 6 Conclusion & Limitations

This paper studies the dynamics where strategic agents interact with an ML system retrained over time with _model-annotated_ and _human-annotated_ samples. We rigorously studied the evolution of _applicant welfare_, _decision-maker welfare_, and _social welfare_. Such results highlight the potential risks of retraining classifiers when agents are strategic. The paper also provides a comprehensive analysis on the fairness dynamics associated with the retraining process, revealing that the fairness intervention may not bring long-term benefits. To ease the negative social impacts, we provide mechanisms to stabilize the dynamics and an early stopping mechanism to maintain fairness. However, our theoretical results rely on certain assumptions and we should first verify these conditions before adopting the results of this paper, which may be challenging in real-world applications.