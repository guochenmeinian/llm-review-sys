# Compositional 3D-aware Video Generation

with LLM Director

Hanzin Zhu\({}^{1}\), Tianyu He\({}^{2}\), Anni Tang\({}^{3}\), Junliang Guo\({}^{2}\), Zhibo Chen\({}^{1}\), Jiang Bian\({}^{2}\)

\({}^{1}\)University of Science and Technology of China

\({}^{2}\)Microsoft Research Asia

\({}^{3}\)Shanghai Jiao Tong University

hanxinzhu@mail.ustc.edu.cn, tianyuhe@microsoft.com, memory97@sjtu.edu.cn, junliangguo@microsoft.com, chenzhibo@ustc.edu.cn, jiang.bian@microsoft.com

This work is accomplished in Microsoft, April 2024.

###### Abstract

Significant progress has been made in text-to-video generation through the use of powerful generative models and large-scale internet data. However, substantial challenges remain in precisely controlling individual concepts within the generated video, such as the motion and appearance of specific characters and the movement of viewpoints. In this work, we propose a novel paradigm that generates each concept in 3D representation separately and then composes them with priors from Large Language Models (LLM) and 2D diffusion models. Specifically, given an input textual prompt, our scheme consists of three stages: 1) We leverage LLM as the director to first decompose the complex query into several sub-prompts that indicate individual concepts within the video (_e.g._, scene, objects, motions), then we let LLM to invoke pre-trained expert models to obtain corresponding 3D representations of concepts. 2) To compose these representations, we prompt multi-modal LLM to produce coarse guidance on the scales and coordinates of trajectories for the objects. 3) To make the generated frames adhere to natural image distribution, we further leverage 2D diffusion priors and use Score Distillation Sampling to refine the composition. Extensive experiments demonstrate that our method can generate high-fidelity videos from text with diverse motion and flexible control over each concept. Project page: https://aka.ms/c3v.

## 1 Introduction

Benefitting from large-scale data and the advancement of the generative models [1; 2], we have witnessed plenty of astonishing results across a wide array of tasks. For example, Large Language Models (LLM) pre-trained on web-scale datasets are revolutionizing machine learning with strong capability of zero-shot learning  and planning [4; 5], while diffusion models  empower text-to-image generation with a rapid surge in both quality and diversity [7; 8; 9].

To harness the power of text-to-image models in text-to-video generation, modern solutions directly view video as multiple images. In this way, tremendous efforts have been dedicated to extending text-to-image models with temporal interaction to ensure consistency between frames [10; 11; 12; 13; 14; 15; 16; 17]. However, generating visual content conditioned on the textual prompt alone struggles to express multiple concepts with precise spatial layout control [18; 19; 20]. To tackle this issue, LVD  and VideoDirectorGPT  propose to first generate spatiotemporal bounding boxes of each object based on the textual prompt with LLM, and then condition the video generation on the obtained layouts. Although rough layout control can be realized, they still have inherent limitations for detailed concept control, _e.g._, the motion and appearance of specific characters, and the movement of viewpoints.

In nature, our understanding of the world is compositional [23; 24; 20], and the interaction with the world takes place in a 3D. Motivated by this, in contrast to the prior endeavors that _implicitly_ learn different concepts in 2D space, we are interested in exploring an alternative solution that _explicitly_ composes concepts in 3D space for video generation. To this end, we in particular identify two key technical challenges: 1) Since a textual prompt contains multiple concepts, how to coordinate the generation of various concepts? 2) Given the generated concepts, how to compose them to follow common sense in the real world?

In this work, we introduce text-guided compositional 3D-aware video generation (C3V), a novel paradigm that regards LLM as director and 3D as structural representation for video generation. C3V consists of three main stages: **1)** Given a textual prompt, to coordinate the generation of various concepts, we leverage LLM to disassemble the input prompt into sub-prompts, where each sub-prompt describes an individual concept, _e.g._, the scene, objects, and motion. For each concept, a pre-trained expert model is assigned by LLM to generate its corresponding 3D representation (_e.g._, 3D Gaussians , SMPL parameters ) according to the textual description. **2)** To provide coarse instruction for composition (_i.e._, the scale and trajectory of each object in the scene), we further resort to the priors in multi-modal LLM by querying it with the rendered scene image and the textual goals. However, directly instructing multi-modal LLM to return the scale and trajectory of each object leads to unexpected results, as it is challenging for LLM to estimate visual dynamics. Therefore, we follow a step-by-step reasoning philosophy  by representing the object with the bounding boxes and dividing the trajectory estimation into sub-tasks, _i.e._, estimating the starting points, ending points, and trajectories step-by-step. **3)** After obtaining the coarse trajectories from the language space, we also propose to refine the scales, rotations, and exact locations with priors from large-scale visual data. Specifically, taking inspiration from DreamFusion , which proposes to distill generative priors from pre-trained image diffusion models into 3D objects, we employ Score Distillation Sampling (SDS)  to optimize the transformation matrix of each object in 3D space.

Our system has three main advantages: 1) Because each concept is represented by individual 3D representations, it naturally supports flexible control and interaction of each concept. 2) It inherently excels at synthesizing complex and long videos such as drama, etc. 3) The viewpoint is controllable.

Extensive experiments demonstrate that our proposed method can generate 3D-aware videos with diverse motion and high visual quality, even from complex queries that contain multiple concepts and relationships. We also illustrate the flexibility of C3V by editing various concepts of the generated videos. The generated videos are presented on our project page. To the best of our knowledge, we make the first attempt towards text-guided compositional 3D-aware video generation. We hope it can inspire further explorations on the interplay between video and 3D generation.

## 2 Related Works

### Video Generation with LLM

Recently, there have been substantial efforts in training text-to-video models on large-scale datasets with autoregressive Transformer [29; 30; 17] or diffusion models [10; 11; 12; 13; 16]. A prominent approach for text-to-video generation is to extend a pre-trained text-to-image model by inserting temporal layers into its architecture, and fine-tuning models on video data. However, although effective, it remains challenging to generate objects with specific attributes or positions. To address this challenge, a series of studies proposed to exploit knowledge from LLM [31; 32] to achieve controllable generation [21; 19; 22; 33; 34; 35], zero-shot generation [36; 37; 38; 39], or long video generation . For example, Free-Bloom  and DirectT2V  used LLM to transform the input textual prompt into a sequence of sub-prompts that describe each frame. LVD  and VideoDirectorGPT  employed LLM to generate spatiotemporal bounding boxes to control the object-level dynamics in video generation.

In light of the above success of exploiting LLM to direct video generation in 2D space, we view LLM as a director in 3D, which differs from previous methods not only in terms of technical route but also in benefits: providing free interaction with individual concepts and flexible viewpoint control.

### Compositional 3D Generation

Generating 3D assets from textual prompt has garnered significant attention owing to its promising applications in various fields such as AR , VR , and autonomous driving . However, due to the lack of large-scale 3D data, it is challenging to apply 2D generative models to 3D directly. Therefore, building upon Dream Fields , DreamFusion introduced the Score Distillation Sampling (SDS) , a technique enhancing 3D generation by distilling 2D diffusion priors from pre-trained text-to-image generative models. Motivated by the success of DreamFusion , dedicated efforts have been made to improve SDS [45; 46; 47]. Though achieving remarkable results, these methods struggle to generate scenes with multiple distinct elements. To mitigate this issue, several techniques was proposed to guide 3D generation with additional conditions like layout priors, which we refer to as compositional 3D generation [48; 49; 50]. However, these works still focus on static compositional 3D generation and lack visual dynamic modeling.

Recently, two concurrent works Comp4D  and TC4D  also achieved compositional 4D generation (_i.e._, dynamic 3D generation). However, they only considered composition between objects, and the trajectory of these methods is either formulated by kinematics-based equations or pre-defined by users. Differently, we explore 3D-aware video generation with integrated 3D scenes and compose various concepts with priors from both LLM and 2D diffusion models.

## 3 Preliminaries

### 3D Gaussian Splatting

3D Gaussian Splatting (3DGS)  has been attracting a lot of interest for novel view synthesis, due to its photorealistic visual quality and real-time rendering. 3DGS utilizes a set of anisotropic ellipsoids (_i.e._, 3D Gaussians) to encode 3D properties, in which each Gaussian is parameterized by position \(^{3}\), covariance \(^{3 3}\) (obtained from scale \(^{3}\) and rotation \(^{3}\)), opacity \(\), and color \(^{3}\).

To render a novel view, 3DGS adopts a tile-based rasterization, where 3D Gaussians are projected onto the image plane as 2D Gaussians. The final color \(()\) of pixel \(\) is denoted as:

\[()=}(1-),\] (1)

where \(}\) and \(\) represent the individual color and opacity values of a series of 2D Gaussians contributing to this pixel. 3DGS are then optimized using L1 loss and SSIM  loss in a per-view optimization manner. Thanks to the nature of modeling 3D scenes explicitly, optimized 3D Gaussians can be easily controlled and edited.

### Score Distillation Sampling

Different from text-to-image generation which benefits from a large number of text-image pairs available, text-to-3D generation suffers from a severe lack of data. To mitigate this issue, Score Distillation Sampling (SDS)  was proposed to distill generative priors from pretrained diffusion-based text-to-image models \(\). Specifically, for a 3D representation parameterized by \(\), SDS is served as a way to measure the similarity between the rendered images \(x=g()\) and the given textual prompts \(y\), where \(g\) represents the rendering operation. As a result, the gradients used to update \(\) are computed as follows:

\[_{}_{SDS}(,x=g())=_{t,}[w(t )(_{}(x_{t};y,t)-)],\] (2)

where \(t\) is the noise level, \(\) is the ground-truth noise, \(w(t)\) is a weighting function, \(_{}\) is the estimated noise given noised images \(x_{t}\) with text embeddings \(y\). Please refer to DreamFusion  for details.

## 4 Method

Overview.To achieve text-guided compositional 3D-aware video generation (C3V), we regard LLM as director and 3D as structural representation. To this end, our method consists of three stages. To begin with, we utilize LLMs to decompose the input textual prompts into three sub-prompts, eachof which provides a description for generating a corresponding concept (_i.e._, scene, object, motion, etc.) respectively (Sec. 4.1). Subsequently, we leverage multi-modal LLM to obtain coarse-grained scales and trajectories for each animatable object (Sec. 4.2). Finally, we employ 2D diffusion priors to refine the objects' location, scale, and rotation for a fine-grained composition (Sec. 4.3).

### Task Decomposition with LLM

Task Instructions.Given a textual prompt, we invoke LLM (_e.g.,_ GPT-4V ) to decompose it into several sub-prompts. Each sub-prompt describes an individual concept such as the scene, object, and motion. Specifically, for an input prompt \(y\), we query LLM with the instruction like: "_Please decompose this prompt into several sub-prompts, each describing the scene, objects in the scene, and the objects' motion._", from which we obtain the corresponding sub-prompts.

3D Representation.After obtaining the sub-prompt for each concept, we aim to generate its corresponding 3D representations using the pre-trained expert models. In this work, we build structural representation on 3DGS , which is an explicit form and therefore flexible enough to compose or animate. Concerning concepts like motion, our framework can generalize to arbitrary animatable 3D Gaussian-based objects. For simplicity, we take human motion as an instantiation because it is general for various scenarios. In order to obtain diverse human motions, we take a retrieval-augmented approach  to acquire motion in the form of SMPL-X parameters  from large motion libraries  according to the motion-related sub-prompt.

Instantiation.To illustrate the scheme formally, consider the following example. We have sub-prompts \(y_{1},y_{2}\) and \(y_{3}\) that describe scene, object, and motion respectively. Additionally, we have corresponding pre-trained text-guided expert models \(_{1}\), \(_{2}\), and \(_{3}\) that are selected by the LLM. The concept generation can be formulated as follows:

\[G_{1}=_{1}(y_{1}),\;G_{2}=_{2}(y_{2},M),\;M=_{3}(y_{3}),\] (3)

Figure 1: Illustration of our method. It consists of three stages: 1) The input textual prompt is decomposed into individual concepts by the LLM. Then we generate each concept in the form of 3D with the corresponding pre-trained expert model (_left_ & Sec. 4.1). 2) We leverage knowledge in multi-modal LLM to estimate the 2D trajectory of objects step-by-step (_middle_ & Sec. 4.2). 3) After lifting the estimated 2D trajectory into 3D as initialization, we refine the scales, locations, and rotations of objects within the 3D scene using 2D diffusion priors (_right_ & Sec. 4.3).

[MISSING_PAGE_FAIL:5]

Then, we use the depth value of the center point of the lower boundary of the bounding box as the trajectory's depth. As a result, we can transform 2D trajectory into 3D:

\[(x^{i},y^{i},z^{i},1)^{T}=R^{-1}K^{-1}[(p_{x}^{i}+}{2},p_{y}^{i},1)^{T } D(p_{x}^{i}+}{2},p_{y}^{i})]-(}{2},0,0,0)^{T},\] (5)

where \(R\) and \(K\) represent camera extrinsic and intrinsic respectively, \(H_{2D}\) and \(W_{2D}\) represent the resolution of the 2D bounding box. \(H_{3D}\) represent the actual height of the 3D bounding box of this object within the scene.

Composition Refinement with 2D Diffusion Priors.With the lifted 3D trajectory, we then integrate the object into the scene. However, the trajectory estimated by LLM is still rough and may not obey natural image distribution. To address this, we propose to further refine the object's scale, location, and rotation by distilling generative priors from pre-trained image diffusion models  into 3D space. Specifically, we treat the parameters for these attributes as optimizable variables and use SDS (Eq. 2) to improve the fidelity of rendered images. As a result, scale refinement can be formulated as follows:

\[_{}_{SDS}^{Scale}=_{t,}[w(t)(_{}(x_{t}(L_{3D}^{1},(S+()_{s}-}{2}) G_{2});y,t)-)],\] (6)

where \(\) represents the optimizable variable, \(S\) represents the scale estimated by GPT-4(V), \(\) means the Sigmoid function, \(_{s}\) is a threshold, \(G_{2}\) represents the 3D gaussians of the object, and \(x_{t}\) is the noised 2D image given \(L_{3D}^{i}\) and scaled \(G_{2}\).

After obtaining a more precise scale, we then refine the locations of the estimated 3D trajectory similarly, where the location refinement is denoted as:

\[_{^{}}}}_{SDS}^{Location}=_{t, }[w(t)(_{}(x_{t}(L_{3D}^{i}+(^{ }}})_{L}-}{2},(S+()_{s}- }{2}) G_{2});y,t)-)],\] (7)

where \(^{}}}\) represents the optimizable variable, \(_{L}\) is a threshold.

For the rotation of the object at different timesteps, we can directly compute the corresponding rotation matrix, based on the assumption that the object at the current time step should face the location of the object at the next time step. As a result, the rotation matrix \(}\) at time step \(i\) can be computed using the following equation:

\[}=tx^{2}+c& txy-zs&txz+ys\\ txy+zs&ty^{2}+c&tyz-xs\\ txz-ys&tyz+xs&tz^{2}+c,\\ t=1-c,c=(),s=(),=(x,y,z)^{T}\] (8)

where \(\) and \(\) represent the rotation angle and axis obtained through the cross product of \((L_{3D}^{i+1}+(L^{{}^{}1})_{L}-L_{3D}^{i}-( ^{}}})_{L})\) and \((0,0,1)^{T}\).

Inference.After obtaining individual concepts in the form of 3D and the optimized parameters that indicate how to compose various concepts, we can render the 3D representation into 2D video with flexible camera control in real time .

## 5 Experiments

In this section, we instantiate C3V with three concepts: scene, humanoid object, and human motion, to generate 3D-aware video from text. We compare our proposed method with state-of-the-art text-to-4D models (4D-FY ), compositional 4D generation models (Comp4D ) and text-to-video models (VideoCrafter2 ). Videos are available on our anonymous project page.

Implementation Details.We use LucidDreamer , HumanGaussian  and Motion-X  to generate 3D scenes, humanoid objects and motions respectively. To realize SDS, we utilize Stable Diffusion  as the image diffusion model. All the videos of our proposed method are rendered at a resolution of \(512 512\) in real time. Please refer to the appendix for more details.

Metrics.Following Comp4D , we choose Q-Align  as the referee to measure the quality and aesthetics of the video. The Q-Align score is a number ranging from 1 (worst) to 5 (best) where a higher score indicates a better performance. We also report the CLIP score  to measure the alignment between the generated videos and the input texts.

### Comparison with Competitors

In Fig. 3, we conduct a comparative analysis of our method against 4D-FY , Comp4D , and VideoCrafter2  with the same textual prompt. It can be observed that all three baselines fail to provide diverse motion from the textual prompt, while our method excels in yielding large motion and high visual quality. For example, our scheme successfully obeys the complex query in terms of

Figure 3: Qualitative comparisons with baselines. When prompting complex queries, the baseline methods fail to follow the queries in terms of the number of objects and the corresponding motion. In contrast, our method excels in yielding both diverse motion and high visual quality.

the number of objects and the corresponding motion. In addition, since 4D-FY and Comp4D focus on object-centric generation, they fail to generate videos with natural backgrounds. In Tab. 3, we perform quantitative comparisons by utilizing Q-Align Score  and CLIP Score  to assess the quality of generated videos. Our method consistently outperforms the baseline models in terms of both the video quality and the alignment with textual prompts. More results are available in the appendix.

### Ablation Studies

Ablations on Trajectory Estimation with Multi-modal LLM.As shown in Fig. 4(a)(I), a direct prompt of GPT-4V will lead to obvious unsatisfactory trajectory estimation. When only depending on bounding boxes to indicate the location of objects within the scene (Fig. 4(a)(II)), though a roughly better trajectory can be achieved, it still leads to unreasonable results, such as several floating bounding boxes. Similarly, using only the step-by-step estimation strategy described in Sec. 4.2 typically results in a trajectory that is merely a simple straight line connecting the starting and ending points (Fig. 4(a)(III)). With both of the two techniques, we can achieve the best performance, with a more reasonable and smooth trajectory (Fig. 4(a)(IV)).

Ablations on Composition with 2D Diffusion Models.To figure out whether it is necessary to conduct fine-grained composition with 2D generative priors, we gradually refine the scales, locations, and rotations with SDS and visualize the results in Fig. 4(b). All results are generated with the same textual prompt: "_An alien walking on the floor in front of the cabin's door._". It shows that when we optimize the attributes with SDS, we can obtain consistently improved performance with a reasonable scale (Fig. 4(b)(II), accurate locations that are aligned with the input prompt (Fig. 4(b)(III), and orientation that accords with common sense (Fig. 4(b)(IV)).

   Metric & 4D-FY  & Comp4D  & VideoCrafter2  & Ours \\  QAlign-img-quality \(\) & 1.681 & 1.687 & 3.839 & **4.030** \\ QAlign-img-aesthetic\(\) & 1.475 & 1.258 & 3.199 & **3.471** \\ QAlign-vid-quality\(\) & 2.154 & 2.142 & 3.868 & **4.112** \\ QAlign-vid-aesthetic\(\) & 1.580 & 1.425 & 3.159 & **3.723** \\  CLIP Score\(\) & 30.47 & 27.50 & 35.20 & **38.36** \\   

Table 1: Quantitative comparisons with competitors. Our method consistently outperforms all baseline methods in terms of both the video quality and the alignment with textual prompts.

Figure 4: Ablation studies on framework design. Each ablation is prompted with the same text.

### Applications on Controllable Generation

Due to our underlying 3D structural representation, our scheme has the natural merits of editing individual concepts. We illustrate this character in Fig. 5 by editing three different concepts: the appearance and motion of the actors, and the scenes. For the appearance and motion of the actor, we can seamlessly replace them in a zero-shot manner according to the textual prompt (Fig.5(a)(b)), while this is still challenging for implicit models [64; 65]. For scene editing, to ensure a smooth composition of objects within the target scene, we re-estimate the trajectory of the objects given the target scene. Kindly refer to appendix for more results.

## 6 Conclusion

In this paper, we present a novel paradigm for 3D-aware video generation by conceptualizing videos as compositions of independent concepts represented in 3D space. To this end, we leverage LLM as director to decompose the input textual prompts into individual concepts and then invoke pre-trained expert models to generate them separately. To compose various concepts, we first prompt multi-modal LLM in a step-by-step manner to provide coarse guidance on the scale and trajectory of objects,

   Methods & Direct Estimation. & 
 Estimation using \\ bounding box. \\  & Step-by-step estimation. & Ours \\  QAlign-img-quality \(\) & 2.056 & 2.894 & 3.752 & **4.030** \\ QAlign-img-aesthetic \(\) & 1.568 & 2.156 & 3.047 & **3.471** \\ QAlign-vid-quality \(\) & 2.178 & 3.043 & 3.904 & **4.112** \\ QAlign-vid-aesthetic \(\) & 1.680 & 2.346 & 3.342 & **3.723** \\  CLIP Score \(\) & 25.68 & 29.84 & 36.73 & **38.36** \\   

Table 2: Quantative comparisons of ablation studies on trajectory estimation with multi-modal LLM.

Figure 5: Our method offers flexible control of individual concepts. We demonstrate this by editing different concepts: the appearance and motion of the actors, and the scenes.

then refine the composition with 2D generative priors. We verify our scheme in different scenarios, demonstrating its superiority over the baseline methods.

Limitations and Future Works.Although we demonstrate promising results in 3D-aware video generation, there still are limitations to be improved in the future. First, our framework is instantiated with limited concepts in this work, _i.e._, scene, humanoid object, and human motion. It is exciting to generalize the framework to more concepts like animals, vehicles, etc. Second, the composition between concepts is conducted with priors from LLM and 2D diffusion priors in our method. However, it is still interesting to introduce physically grounded dynamics into 3D representation . Third, though our method is naturally suitable for maintaining the consistency of actors across different scenes, it still needs further exploration on long video generation with multiple scenes, _e.g._, a full-length film.

Ethics Statement.C3V is exclusively a research initiative with no current plans for product integration or public access. We are committed to adhering to Microsoft AI principles during the ongoing development of our models. The model is trained on AI-generated content, which has been thoroughly reviewed to ensure that they do not include personally identifiable information or offensive content. Nonetheless, as these generated data are sourced from the Internet, there may still be inherent biases. To address this, we have implemented a rigorous filtering process on the data to minimize the potential for the model to generate inappropriate content.

Acknowledgement.This work was supported in part by NSFC under Grant 62371434, 62021001.