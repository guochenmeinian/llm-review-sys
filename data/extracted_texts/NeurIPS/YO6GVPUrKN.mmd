# On the Limitations of Fractal Dimension

as a Measure of Generalization

Charlie B. Tan

University of Oxford

&Ines Garcia-Redondo

Imperial College London

&Qiquan Wang

Imperial College London

&Michael M. Bronstein

University of Oxford / Aithyra

&Anthea Monod

Imperial College London

Equal contribution. Correspondence to: charlie.tan@cs.ox.ac.uk, i.garcia-redondo22@imperial.ac.uk, qiquan.wang17@imperial.ac.uk, michael.bronstein@cs.ox.ac.uk, a.monod@imperial.ac.uk. Code provided for all experiments at: https://github.com/charliebtan/fractal_dimensions

###### Abstract

Bounding and predicting the generalization gap of overparameterized neural networks remains a central open problem in theoretical machine learning. There is a recent and growing body of literature that proposes the framework of fractals to model optimization trajectories of neural networks, motivating generalization bounds and measures based on the fractal dimension of the trajectory. Notably, the persistent homology dimension has been proposed to correlate with the generalization gap. This paper performs an empirical evaluation of these persistent homology-based generalization measures, with an in-depth statistical analysis. Our study reveals confounding effects in the observed correlation between generalization and topological measures due to the variation of hyperparameters. We also observe that fractal dimension fails to predict generalization of models trained from poor initializations. We lastly reveal the intriguing manifestation of model-wise double descent in these topological generalization measures. Our work forms a basis for a deeper investigation of the causal relationships between fractal geometry, topological data analysis, and neural network optimization.

## 1 Introduction

Deep learning enjoys widespread empirical success despite limited theoretical support. Measures from statistical learning theory, such as Rademacher complexity (Bartlett and Mendelson, 2002) and VC-Dimension (Hastie et al., 2009), indicate that without explicit regularization, over-parameterized models will generalize poorly. In contrast, neural networks are able to generalize strongly despite having sufficient capacity to simply memorize their training data (Liu et al., 2020; Zhang et al., 2021). Remarkably, neural networks often exhibit improved generalization for increases in capacity (Nakkiran et al., 2021). Describing the generalization behavior of neural networks therefore requires the development of novel learning theory (Zhang et al., 2021). Ultimately, deep learning theory seeks to define generalization bounds for given experimental configurations (Valle-Perez and Louis, 2020), such that the generalization error of a given model can be bounded and predicted (Jiang et al., 2020).

Generalization is typically attributed to the implicit bias of gradient-based optimization. A number of works have considered the geometry of generalizing solutions within parameter space (Dinh et al., 2017; Garipov et al., 2018), and the bias of optimization methods towards such solutions (He et al., 2019; Izmailov et al., 2018). Simsekli et al. (2020) propose _random fractal_ structure for neural network optimization trajectories and compute generalization bounds based on _fractal dimensions_. However, this work requires rigid topological and statistical conditions on the optimization trajectory as well as the learning algorithm. Subsequent work by Birdal et al. (2021) proposes the use ofpersistent homology (PH) dimension_(Adams et al., 2020)--a measure of fractal dimension deriving from topological data analysis (TDA)--to relax these assumptions. They propose an efficient procedure for estimating PH dimension, and apply this both as a measure of generalization and as a scheme for explicit regularization. Dupuis et al. (2023) extend this approach using a data-dependent pseudometric to further relax continuity assumptions on the network loss.

Our paper constitutes an extended empirical evaluation of the performance and viability of these proposed topological measures of generalization; in particular, robustness and failure modes are explored in a wider range of experiments than those considered by Birdal et al. (2021) and Dupuis et al. (2023). Our main contributions are as follows:

* We reproduce the learning rate/batch size grid experiments of Dupuis et al. (2023), observing comparable correlation coefficients, aside from the loss-based PH dimension on AlexNet CIFAR-10 where models trained with high learning rate are attributed high dimension values despite having small generalization gap.
* We extend the statistical analysis of Dupuis et al. (2023) to include partial correlations. We observe that in some cases learning rate has a significant influence on observed correlation between PH dimensions and generalization gap for fixed batch sizes.
* We further conduct a conditional independence test using the conditional mutual information (Jiang et al., 2020), observing that both Euclidean and loss-based PH dimension are conditionally independent of generalization gap on MNIST.
* We train models of varying generalization gap using adversarial initialization (Liu et al., 2020). As presented in Figure 1, we observe that both dimensions fail to correctly attribute high values to poorly generalizing models for some architectures and datasets.
* We train a CNN architecture at a range of width multipliers to reproduce the model-wise double descent of Nakkiran et al. (2021). Neither PH dimension correctly correlates with generalization gap in this setting. Interestingly, by correlating with test accuracy, double descent manifests in Euclidean PH dimension.

## 2 Background

Following Dupuis et al. (2023), let \((,_{},_{})\) be the data space, where \(=\), and \(\), \(\) represent the feature and label spaces respectively. We aim to learn a parametric approximation \(h_{w}:\) of the unknown data generating distribution \(_{}\) from a finite set of i.i.d. training points \(S:=\{z_{1},\,,\,z_{n}\}_{}^{ n}\). The quality of our parametric approximation is measured using a loss function \(:\) composed with the parametric approximation \((,z):=(h_{}(x),y)\)

Figure 1: **Adversarial initialization is a failure mode for PH dimension-based generalization measures**. Training models from an adversarial initialization leads to higher accuracy gap than for models trained from random initialization. Both PH dimensions fail to correctly attribute higher values to the poorly generalizing models on FCN-5 MNIST and CNN CIFAR-10.

The learning task then amounts to solving an optimization problem over parameters \(w^{d}\), where we seek to minimize the _empirical risk_\(}(w,S):=_{i=1}^{n}(w,z_{i})\) over a finite set of training points. To measure performance on unseen data samples, we consider the _population risk_\((w):=_{z}[(w,z)]\) and define the _generalization gap_ to be the difference of the population and empirical risks \((S,w):=|(w)-}(S,w)|\). For a given training dataset and some initial value for the weights \(w_{0}^{d}\) we refer to the optimization trajectory as \(_{S}\).

### Persistent Homology and Fractal Dimension

Fractals arise in recursive processes (Mandelbrot, 1983; Prahofer and Spohn, 2000); chaotic dynamical systems (Briggs, 1992; Mandelbrot et al., 2004); and real-world data (Mandelbrot, 1967; Coleman and Pietronero, 1992; Falconer, 2007; Pietronero and Tosatti, 2012). A key characteristic is their _fractal dimension_, first introduced as the _Hausdorff dimension_(Hausdorff, 1918). Due to its computational complexity, more efficient measures such as the _box-counting dimension_Sarkar and Chaudhuri (1994) were later developed. An alternative fractal dimension can also be defined in terms of _minimal spanning trees_ of finite metric spaces (Kozma et al., 2006). A recent line of work by Adams et al. (2020) and Schweinhart (2021, 2020) extended and reinterpreted fractal dimension using PH. Originating from algebraic topology, PH provides a systematic framework for capturing and quantifying the multi-scale topological features in complex datasets through a topological summary called the _persistence diagram_ or _persistence barcode_; further details on PH are given in Appendix A and a more comprehensive exposition of fractal dimension can be found in Appendix B.

We follow the approach by Schweinhart (2021) to define a PH-based fractal dimension. Let \(=\{x_{1},\,,\,x_{n}\}\) be a finite subset of a metric space \((X,)\). Let \(_{i}()\) be the persistence diagram obtained from the PH of dimension \(i\) computed from the Vietoris-Rips filtration and define

\[E_{}^{i}():=_{(b,d)_{i}()}(d-b)^{ }.\] (1)

**Definition 2.1** ((Schweinhart, 2020)).: Let \(S\) be a bounded subset of a metric space \((X,)\). The _\(i\)th PH dimension_ (\(_{i}\)-dim) for the Vietoris-Rips complex of \(S\) is

\[_{}{}^{i}(S):=\{:\,C>0E_{}^{i}()<C,\,\, S\}.\]

Fractal dimensions need not be well-defined for all subsets of a metric space. However, under a certain regularity condition (_Ahlfors regularity_), the Hausdorff and box-counting dimensions are well defined and coincide (Falconer, 2007). Additionally, for any metric space, the minimal spanning tree is equal to the upper box dimension (Kozma et al., 2006). The relevance of PH appears when considering the minimal spanning tree in fractal dimensions. Specifically, there is a bijection between the edges of the Euclidean minimal spanning tree of a finite metric space \(=\{x_{1},\,,\,x_{n}\}\) and the points in the persistence diagram \(_{0}()\) obtained from the Vietoris-Rips filtration. This automatically gives the equivalence \(_{}{}^{0}(S)=_{}(S)=_{}(S)\).

### Fractal Dimension-Based Generalization Bounds

Simsekli et al. (2020) empirically observe that the gradient noise exhibits heavy-tailed behavior, which they use to model stochastic gradient descent (SGD) as a discretization of a _decomposable Feller process_. They also impose initialization with zeros; that \(\) is bounded and Lipschitz continuous in \(w\); and that \(_{S}\) is bounded and Ahlfors regular. In this setting, they compute two bounds for the worst-case generalization error, \(_{w_{S}}(S,w)\), in terms of the Hausdorff dimension of \(_{S}\). They first prove bounds related to covering numbers (used to define the upper-box counting dimension) and then use Ahlfors regularity to link the bounds to the Hausdorff dimension.

Subsequently, Birdal et al. (2021) further develop the bounds in Simsekli et al. (2020) by reformulating them in terms of the \(0\)-dimensional PH dimension (Schweinhart, 2021) of \(_{S}\). The link between the upper-box dimension and the \(0\)-dimensional PH dimension, which is the cornerstone of their proof, only requires boundedness of \(_{S}\) (which is also one of the assumptions by Simsekli et al. (2020)), thus eliminating the Ahlfors regularity condition. In order to estimate the PH dimension, they prove (see Proposition 2, (Birdal et al., 2021)) that for all \(>0\) there exists a constant \(D_{,}\) such that

\[E_{}^{0}(W_{n}) D_{,}\,n^{},\] (2)where \(:=}{}^{0}(_{S})+-}{_{ }{}^{0}(_{S})+}\) for all \(n 0\), all i.i.d. samples \(W_{n}\) with \(n\) points on the optimization trajectory \(_{s}\), and \(E_{}^{0}(W_{n})\) as defined in (7). Using this result, they estimate and bound the PH-dimension by fitting a power law to the pairs \(((n),\,(E_{1}^{0}(W_{n})).\) They then use (2) to estimate \(_{}{}^{0}(_{s}),\) where \(m\) is the slope of the regression line. Concurrently, Camuto et al. (2021) take a different, non-topological approach by studying stationary distributions of Markov chains and describing the optimization algorithm as an iterated function system (IFS). They establish generalization bounds with respect to the upper Hausdorff dimension of a limiting measure. Most recently, Dupuis et al. (2023) further develop the topological approach by Birdal et al. (2021) to circumvent the Lipschitz condition on the loss function that was required in all previous works by obtaining a bound depending on a data-driven pseudometric \(_{S}\) instead of the \(^{d}\) Euclidean metric,

\[_{S}(w,w^{}):=_{i=1}^{n}|(w,z_{i})-(w^{ },z_{i})|,\;w,\;w^{}^{d}.\] (3)

They derive bounds for the worst-case generalization gap, where the only assumption is that the loss \(:^{d}\) is continuous in both variables and uniformly bounded by some \(B>0\). These bounds are established with respect to the upper-box dimension of the set \(_{S}\) using \(_{S}\) (see Theorem 3.9. (Dupuis et al., 2023)). They additionally prove that for pseudometric-bounded spaces, the corresponding upper-box counting dimension coincides with the \(0\)-dimensional PH-dimension, which they estimate as in Birdal et al. (2021).

## 3 Experimental Setup

Our experiments closely follow the setting of Dupuis et al. (2023). We train with SGD until convergence, then continue for \(5000\) additional iterations to obtain a sample optimization trajectory \(\{w_{k}:0<k 5000\}\) about the local minimum attained. We then compute the \(0\)-PH dimension using both the Euclidean metric and the loss-based pseudometric (3) to obtain the generalization measures of Birdal et al. (2021) and Dupuis et al. (2023). In keeping with the assumptions of this theory, we omit explicit regularization such as dropout or weight decay, and maintain constant learning rates in all experiments. Following Dupuis et al. (2023) we define the generalization gap as the the absolute accuracy gap for classification tasks, and the absolute loss gap in regression tasks. Further details on the experimental setup, expanding on this section, are provided in Appendix C and a note on the stability of PH dimension estimates post-convergence can be found in Appendix D.

**Datasets and Architectures.** We employ the same datasets and architectures as Dupuis et al. (2023): (i) fully-connected network of 5 (FCN-5) and 7 (FCN-7) layers on the California housing dataset (CHD) (Kelley Pace and Barry, 1997); (ii) FCN-5 and FCN-7 on the MNIST dataset (Lecun et al., 1998); and (iii) AlexNet (Krizhevsky et al., 2017) on the CIFAR-10 dataset (Krizhevsky, 2009). We additionally conduct experiments on a 5-layer convolutional neural network, defined by Nakkiran et al. (2021) as _standard CNN_, trained on CIFAR-10 and CIFAR-100, with batch normalization removed to align with the bound assumptions.

**Overview of Experiments.** We divide our experiments into three categories. In the first, we replicate the \(6 6\) grid of learning rates and batch sizes considered in Dupuis et al. (2023). We use these results to perform a statistical analysis of the correlation between PH dimension (both Euclidean and loss-based) and generalization error, particularly exploring the influence of the hyperparameters. Second, we use _adversarial pre-training_ as a method to generate poorly generalizing models (Liu et al., 2020). Finally, we consider model-wise double descent, the phenomenon in which test accuracy is non-monotonic with respect to increasing model parameters (Nakkiran et al., 2021).

**Computational Resources.** All experiments were run on high performance computing clusters using GPU nodes with Quadro RTX 6000 (128 CPU cores) or NVIDIA H100 (192 CPU cores). The runtime for training and computing the PH dimension vary for different architectures, datasets, and hardware used, with the longest experiments taking several hours.

## 4 Grid Experiments and Correlations

We first reproduce the experiments of Dupuis et al. (2023), wherein learning rate and batch size are defined in a 6 \(\) 6 grid as defined in Appendix C. In Figure 2 we present the results of these experiments for the PH dimensions in the FCN-7 and AlexNet experiments, additional results for the FCN-5 models and other generalization measures are presented in Appendix E.

### Correlation Analysis

In Table 1, we present correlation coefficients between the PH dimensions (Euclidean and loss-based) and generalization gap. As in Dupuis et al. (2023), we present Spearman's rank correlation coefficient \(\); the mean granulated Kendall rank correlation coefficient \(\)(Jiang et al., 2020); and the standard Kendall rank correlation coefficient \(\). We also compute correlations with the \(^{2}\) norm of the final parameter vector \(||w_{5000}||_{2}\), and the learning rate/batch size ratio. We emphasize the learning rate/batch size ratio is not a measure of generalization as it is not a measurable experimental output.

The results on CHD and MNIST align with those reported by Dupuis et al. (2023): both the Euclidean and loss-based measures have positive correlation with generalization gap, with the correlation of the loss-based being slightly stronger than that of the Euclidean. However, for the AlexNet CIFAR-10 experiment, we obtain negative correlations for the loss-based measure, in contrast to the theory and results of Dupuis et al. (2023). Observing the results in Figure 2, we see this is due to several points with high learning rate achieving very high PH dimension. We are unable to determine why these points appear in our experiments but not prior studies. However, we assert that all points considered attain 100% training accuracy hence meet the convergence assumption of Dupuis et al. (2023).

The \(^{2}\) norm has the strongest absolute correlations for all experiments, but this correlation is positive for regression and negative for classification. The positive correlation on regression experiments is unexpected, although similar behavior has been observed by Jiang et al. (2020). The learning rate/batch size ratio has strong negative correlation on classification experiments and weak positive correlation on the regression experiments. The strong correlation of learning rate/batch size ratio on classification experiments aligns with trends observable in Figure 2, indicating potential confounding effects of these variables in the observed correlations. Dupuis et al. (2023) included the mean granulated Kendall rank correlation coefficient \(\) in their analysis to mitigate the influence of hyperparameters when computing rank correlations. This coefficient is computed by taking the average over Kendall coefficients at fixed values of the hyperparameters. However, by averaging over all hyperparameter ranges, significant correlations for different fixed values of the hyperparameters might be masked by lower correlations, resulting in inconclusive findings.

### Partial Correlation

Given the correlation between learning rate/batch size ratio and generalization gap, we study partial correlations to isolate the influence of these hyperparameters on the observed correlation between generalization and PH dimensions. Suppose \(X\) and \(Y\) are our variables of interest and \(Z\) is a

Figure 2: **Learning rate/batch size grid results.** Euclidean (top) and loss-based (bottom) PH dimension plotted against generalization gap for range of learning rates and batch sizes.

multivariate variable. The partial correlation between \(X\) and \(Y\) given \(Z\) is the correlation between the residuals of the regressions of \(X\) with \(Z\) and of \(Y\) with \(Z\). If the correlation between \(X\) and \(Y\) can be fully explained by \(Z\), then the partial correlation should yield a low coefficient. To report statistical significance, we conduct a non-parametric permutation-type hypothesis test for the assumption that the partial correlation is equal to zero. In our setting, the null hypothesis implies the correlation observed between generalization and PH dimensions is explained by the influence of other hyperparameters.

In Table 2, we report the partial Spearman's and (standard) Kendall rank correlation between generalization gap and PH dimensions, conditioned on learning rate for fixed batch sizes. We provide the corresponding \(p\)-values for the stated hypothesis test in parentheses. Recall that a \(p\)-value lower than 0.05 implies the rejection of the null hypothesis, or equivalently, that there is a correlation between PH dimension and generalization gap that cannot be explained by the influence of the hyperparameters; a \(p\)-value greater than 0.05 implies a significant influence of the corresponding hyperparameter in the apparent correlation. We observe for most batch sizes, the correlation present between Euclidean dimension and generalization gap can be explained by the influence of learning rate, particularly for larger batch sizes. There is no consistent trend for the loss-based dimension, and the influence of the learning rate is found to be significant in fewer cases, indicating it may be a better-suited measure.

### Conditional Independence

We have established that for some batch sizes, the correlation between PH dimension and generalization is significantly influenced by learning rate. We now seek to determine the existence of a causal relationship between the PH dimension and the generalization gap, basing our study on that of Jiang et al. (2020). If a causal relationship does exist, then a low PH dimension caused by a variation of hyperparameters would consequently cause the generalization gap to be small. If a causal relationship does not exist, then the variation of the hyperparameter would cause both the PH dimension and generalization gap to be low, without any meaningful effect between the PH dimension and generalization gap. An illustrative example of these scenarios is provided in Figure 3.

    & **Measure** & \(\) & \(\) & \(\) \\   & Euclidean & \(0.71\,\,{}_{ 0.07}\) & \(0.48\,\,{}_{ 0.07}\) & \(0.54\,\,{}_{ 0.07}\) \\  & Loss-based & \(0.78\,\,{}_{ 0.06}\) & \(0.64\,\,{}_{ 0.06}\) & \(0.64\,\,{}_{ 0.06}\) \\  & Norm & \(0.92\,\,{}_{ 0.04}\) & \(0.84\,\,{}_{ 0.05}\) & \(0.81\,\,{}_{ 0.06}\) \\   & LR / BS & \(0.29\,\,{}_{ 0.11}\) & \(0.11\,\,{}_{ 0.08}\) & \(0.21\,\,{}_{ 0.07}\) \\   & Euclidean & \(0.45\,\,{}_{ 0.07}\) & \(0.19\,\,{}_{ 0.07}\) & \(0.32\,\,{}_{ 0.06}\) \\  & Loss-based & \(0.67\,\,{}_{ 0.11}\) & \(0.51\,\,{}_{ 0.09}\) & \(0.54\,\,{}_{ 0.08}\) \\  & Norm & \(0.87\,\,{}_{ 0.05}\) & \(0.78\,\,{}_{ 0.05}\) & \(0.72\,\,{}_{ 0.06}\) \\   & LR / BS & \(0.15\,\,{}_{ 0.01}\) & \(0.04\,\,{}_{ 0.05}\) & \(0.10\,\,{}_{ 0.09}\) \\   & Euclidean & \(0.67\,\,{}_{ 0.05}\) & \(0.73\,\,{}_{ 0.05}\) & \(0.50\,\,{}_{ 0.04}\) \\  & Loss-based & \(0.77\,\,{}_{ 0.05}\) & \(0.79\,\,{}_{ 0.05}\) & \(0.60\,\,{}_{ 0.04}\) \\  & Norm & \(-0.93\,\,{}_{ 0.03}\) & \(-0.79\,\,{}_{ 0.04}\) & \(-0.81\,\,{}_{ 0.04}\) \\   & LB / BS & \(-0.95\,\,{}_{ 0.02}\) & \(-0.83\,\,{}_{ 0.05}\) & \(-0.84\,\,{}_{ 0.04}\) \\   & Euclidean & \(0.78\,\,{}_{ 0.04}\) & \(0.88\,\,{}_{ 0.04}\) & \(0.61\,\,{}_{ 0.04}\) \\  & Loss-based & \(0.88\,\,\,{}_{ 0.03}\) & \(0.90\,\,{}_{ 0.03}\) & \(0.71\,\,{}_{ 0.04}\) \\   & Norm & \(-0.97\,\,{}_{ 0.01}\) & \(-0.83\,\,{}_{ 0.05}\) & \(-0.88\,\,{}_{ 0.04}\) \\   & LR / BS & \(-0.98\,\,{}_{ 0.00}\) & \(-0.91\,\,{}_{ 0.03}\) & \(-0.90\,\,{}_{ 0.02}\) \\   & Euclidean & \(0.85\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, To distinguish between these two scenarios, we conduct a conditional independence test by computing the conditional mutual information (CMI) (Jiang et al., 2020) as defined in Appendix F. The CMI vanishes to zero if and only if \(X Y\,|\,Z\), i.e., \(X\) (PH dimension) and \(Y\) (generalization gap) are conditionally independent given \(Z\) (hyperparameter). Given the discrete nature of our selected hyperparameters, we empirically determine the probability density functions. To assess the significance of the computed CMI, we generate a null distribution for the CMI under local permutations of \(X\) or \(Y\) for fixed hyperparameter values, where "local" here refers to the group of realizations of \(X\) and \(Y\) generated under the same hyperparameters \(Z\)(Kim et al., 2022). The null hypothesis implies that \(X\) and \(Y\) are conditionally independent, in which case the CMI is invariant to permutations. We reject the assumption of conditional independence if the CMI lies in the extremes of the null distribution.

Table 3 contains the results of the conditional independence test between PH dimensions and generalization conditioned on learning rate for fixed batch sizes. Within the table, a \(p\)-value > 0.05 implies the acceptance of the null hypothesis of conditional independence (\(H_{0}\) in Figure 3), whereas a \(p\)-value \(\) 0.05 indicates the existence of conditional dependence (\(H_{1}\) in Figure 3). Hence, we observe that for the models trained on MNIST data and for most batch sizes, the PH dimensions and generalization can be considered to be conditionally independent. For the models trained on the CHD, for most batch sizes, the PH dimensions and generalization are seen to be conditionally dependent.

## 5 Adversarial Initialization

The theory of Birdal et al. (2021) and Dupuis et al. (2023) proposes a positive correlation between generalization and the respective PH dimensions. However, neither work makes an assumption on the initialization scheme applied a the start of training. To investigate the sensitivity of the measures to initialization, we employ the _adversarial initialization_ technique proposed by Liu et al. (2020).

    & } &  &  \\   & & \(\) & \(\) & \(\) & \(\) \\   & \(32\) & \(0.10\)\(\) & \(0.06\)\(\) & \(0.06\)\(\) & \(0.04\)\(\) \\  & \(65\) & \(-0.03\)\(\) & \(-0.01\)\(\) & \(-0.10\)\(\) & \(-0.08\)\(\) \\  & \(99\) & \(-0.41\)\(\) & \(-0.29\)\(\) & \(-0.67\)\(\) & \(-0.49\)\(\) \\  & \(132\) & \(-0.31\)\(\) & \(-0.21\)\(\) & \(-0.65\)\(\) & \(-0.47\)\(\) \\  & \(166\) & \(-0.04\)\(\) & \(-0.02\)\(\) & \(-0.49\)\(\) & \(-0.33\)\(\) \\  & \(200\) & \(-0.05\)\(\) & \(-0.03\)\(\) & \(-0.65\)\(\) & \(-0.48\)\(\) \\   & \(32\) & \(0.48\)\(\) & \(0.32\)\(\) & \(0.37\)\(\) & \(0.24\)\(\) \\  & \(65\) & \(0.10\)\(\) & \(0.07\)\(\) & \(-0.02\)\(\) & \(-0.02\)\(\) \\  & \(99\) & \(-0.35\)\(\) & \(-0.24\)\(\) & \(-0.73\)\(\) & \(-0.55\)\(\) \\  & \(132\) & \(0.04\)\(\) & \(0.02\)\(\) & \(-0.18\)\(\) & \(-0.14\)\(\) \\  & \(166\) & \(0.08\)\(\) & \(0.03\)\(\) & \(-0.70\)\(\) & \(-0.51\)\(\) \\  & \(200\) & \(0.12\)\(\) & \(0.08\)\(\) & \(-0.82\)\(\) & \(-0.66\)\(\) \\   & \(32\) & \(0.63\)\(\) & \(0.42\)\(\) & \(0.46\)\(\) & \(0.32\)\(\) \\  & \(76\) & \(-0.08\)\(\) & \(-0.06\)\(\) & \(0.43\)\(\) & \(0.29\)\(\) \\  & \(121\) & \(0.17\)\(\) & \(0.13\)\(\) & \(0.37\)\(\) & \(0.26\)\(\) \\  & \(166\) & \(0.00\)\(\) & \(0.01\)\(\) & \(0.16\)\(\) & \(0.12\)\(\) \\  & \(211\) & \(0.22\)\(\) & \(0.15\)\(\) & \(0.17\)\(\) & \(0.12\)\(\) \\  & \(256\) & \(0.08\)\(\) & \(0.07\)\(\) & \(0.10\)\(\) & \(0.09\)\(\) \\   & \(32\) & \(0.81\)\(\) & \(0.61\)\(\) & \(0.82\)\(\) & \(0.62\)\(\) \\  & \(76\) & \(0.68\)\(\) & \(0.46\)\(\) & \(0.79\)\(\) & \(0.58\)\(\) \\   & \(121\) & \(0.29\)\(\) & \(0.21\)\(\) & \(0.69\)\(\) & \(0.50\)\(\) \\   & \(166\) & \(0.26\)\(\) & \(0.17\)\(\) & \(0.50\)\(\) & \(0.34\)\(\) \\   & \(211\) & \(0.26\)\(\) & \(0.20\)\(\) & \(0.45\)\(\) & \(0.31\)\(\) \\   & \(256\) & \(0.19\)\(\) & \(0.16\)\(\) & \(0.30\)\(\) & \(0.21\)\(\) \\   

Table 2: Partial Spearmanâ€™s \(\) and Kendall \(\) correlation computed between PH dimensions and This entails a pre-training phase on training data with fixed, random labels until the network has successfully interpolated this random data. The resulting parameters are then used as initialization for training on the true dataset, leading to a poorly generalizing model.

In Figure 1, we present the results of this experiment, where the adversarial initialization models are contrasted against models trained from a standard (random) initialization. 30 seeds are evaluated for CHD and MNIST, and 20 seeds for AlexNet. We find that for the CNN CIFAR-10 and the FCN-5 MNIST the adversarial initialization models present lower PH dimensions despite having higher generalization gap, in contrast to the proposed theory. On AlexNet CIFAR-10 the PH dimensions both successfully identify the poorly generalization models, prescribing high values in this case.

## 6 Model-Wise Double Descent

We lastly explore model-wise double descent through the PH dimensions (Nakkiran et al., 2021). In model-wise double descent the test accuracy of a classifier is non-monotonic with respect to the number of parameters--a surprising result in contrast with classical learning theory.

In Figure 4, we present results for the CNN trained on CIFAR-100 at a variety of width multipliers. We follow the "noiseless" configuration of Nakkiran et al. (2021), although we do not use batch normalization or learning rate decay to align with the assumptions of Dupuis et al. (2023). The

Figure 3: **Diagram of causal relationships under investigation in the conditional independence test. In \(H_{0}\) the PH dimension is conditionally independent of PH dimension given learning rate and there is no direct causal relationship between these variables. In \(H_{1}\) generalization gap is conditionally dependent of the PH dimension indicating a causal relationship may exist.**

    &  &  \\   & & \(32\) & \(65\) & \(99\) & \(132\) & \(166\) & \(200\) \\   & Euclidean & \(0.01\) & \(\) & \(0.02\) & \(0.01\) & \(0.00\) & \(\) \\  & Loss-based & \(0.00\) & \(0.02\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) \\   & Euclidean & \(0.00\) & \(\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) \\  & Loss-based & \(0.00\) & \(\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) \\   \\   & & \(32\) & \(76\) & \(121\) & \(166\) & \(211\) & \(256\) \\   & Euclidean & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  & Loss-based & \(\) & \(\) & \(\) & \(\) & \(\) & \(0.01\) \\   & Euclidean & \(\) & \(0.04\) & \(\) & \(\) & \(\) & \(\) \\  & Loss-based & \(0.02\) & \(0.00\) & \(\) & \(\) & \(\) & \(\) \\   

Table 3: Table of \(p\)-values from conditional independence tests between PH dimensions and generalization gap conditioned on learning rate using conditional mutual information (CMI) as test statistic with local permutations for given batch sizes. Bolded \(p\)-values indicate conditional independence between PH dimension and generalization.

mean of 3 seeds is presented with standard deviation shaded. In evaluation accuracy, we observe the classical double descent behavior, but note that the generalization gap is monotonic in the range up to width multiplier of 16. We additionally observe a double descent behavior in Euclidean PH dimension. The behavior of loss-based PH dimension does not follow the double descent pattern, with notable instability and variance in the region of the evaluation accuracy double descent critical region. These results suggest a connection between the convergence properties of the model in the critical region of double descent with respect to the model width, itself a poorly understood phenomenon, and the Euclidean PH dimension. They also show the failure of either PH dimension to correlate with generalization gap for varying model width, which is monotonic in this width region.

## 7 Discussion

Our results demonstrate two modes of failure for PH dimensions as measures of generalization: adversarial initialization and model width variation in the critical region of double descent. Furthermore, we show that in some cases the correlation observed between PH dimension and generalization gap is significantly influenced by hyperparameter values. An explanation for the influence of hyperparameters--in particular learning rate--in the value of PH dimension is that the underlying space used in the PH computations is determined by samples from the optimization trajectory; and the influence of the learning rate on the geometry of these samples is notable. We further demonstrate that for some architectures and datasets the generalization measures and generalization gap are conditionally independent given the confounding hyperparameters, implying that the observed relationship between the two variables is not directly causal in these settings.

Evidently, the PH dimensions are not universally successful in correlating with generalization gap, in contrast to the general bounds proven by Birdal et al. (2021) and Dupuis et al. (2023). We have two main conjectures for this disconnect between theory and practice. Firstly, the technical assumptions required by Dupuis et al. (2023) to prove generalization bounds, and their implications on architecture and hyperparameters valid for variation, are not clear. Preceding works (Simsekli et al., 2020; Birdal et al., 2021) involve various technical assumptions about optimization trajectories and loss functions that may not be met in practice. Secondly, the term in the generalization bounds involving mutual information between the training data and the optimization trajectory may dominate, leading to vacuous bounds with respect to fractal dimension. These mutual information terms are complex and less studied than the fractal dimensions; it is unclear if they can be empirically estimated. The assumption that the mutual information does not dominate the bounds has not been proven and has been scarcely explored. We believe some experiments may enter a regime where this term dominates, disrupting the expected correlation.

### Limitations

**Models and settings.** The goal of our study was to better understand the conclusions drawn by Birdal et al. (2021) and Dupuis et al. (2023). We are thus subject to the following restrictions arising

Figure 4: **Model-wise double descent manifests in Euclidean PH dimension, whilst neither PH dimension correlates with generalization gap in this setting.** Test accuracy, generalization gap, and PH dimensions for range of CNN widths. The double descent behavior is clearly visible in test accuracy and Euclidean PH dimension, but the generalization gap is monotonic in this critical region. Mean of three seeds with standard deviation shaded.

from the assumptions in the theoretical results of Birdal et al. (2021) and Dupuis et al. (2023): (i) we use only "vanilla" SGD; (ii) we only work with a constant learning rate; (iii) we do not use batch normalization; (iv) we do not study the addition of explicit regularization such as dropout or weight decay. We address this limitation by extending the study to include adversarial initialization scenarios (Liu et al., 2020), and studying the connection between double descent (Nakkiran et al., 2021) and PH dimension whilst still remaining within the theoretical assumptions. Future research directions include alternative optimization algorithms and common neural network architecture choices, such as batch normalization or annealing learning rates, prevented by the current setting.

**Choice of hyperparameters.** Our study exhibits a limited range of batch sizes and learning rates, along with unconventional grid values that varied between different architectures. These choices were made to align with, and ensure a fair comparison with, the experiments of (Dupuis et al., 2023). We believe that these design choices were made by Dupuis et al. (2023) to ensure convergence of the models within reasonable numbers of iterations, due to the computational cost of repeatedly training different models with various seeds, and may have contributed to the statistically significant results reported in their work. For an extended analysis, we would explore a wider range of hyperparameters.

**Computational limitations.** Most of the runtime in our experiments was devoted to computing the loss-based PH dimension. Though efficient computation of PH is an active field of research in TDA Chen and Kerber (2011); Bauer et al. (2014, 2014), Guillou et al. (2023), Bauer (2021), PH remains a computationally intensive methodology, limiting the number of experiments it was possible to run.

**Lack of identifiable patterns in the correlation failure.** An important limitation of our work is our failure to identify any clear patterns offering explanations as to when and why the PH dimension can fail to correlate with the generalization gap when conditioning on the network hyperparameters. We further cannot identify a pattern to explain the success and failure of PH dimension measures to correlate with generalization when using adversarial initialization.

**Theoretical limitations.** Despite providing extended experimental analyses of relationship between PH dimension and generalization gap, we do not make any theoretical contributions to explain the disconnect observed between theory and practice.

## 8 Conclusion

In this work, we extend previous evaluations of PH dimension-based generalization measures. Although theoretical results on the fractal and topological measures of generalization gaps were provided by Simsekli et al. (2020), Birdal et al. (2021), Camuto et al. (2021) and Dupuis et al. (2023), experimentally, our study shows that there is in some cases a disparity between theory and practice. We suggest two directions for further investigation: (i) considering probabilistic definitions of fractal dimensions (Adams et al., 2020, Schweinhart, 2020) may offer a more natural interpretation for generalization compared to metric-based approaches; (ii) exploring multifractal models for optimization trajectories could better capture the complex interplay of network architecture and hyperparameters in understanding generalization. Overall, our work demonstrates that there is still much to understand concerning the complex interplay between generalization gap and TDA-based fractal dimension of optimization trajectories.