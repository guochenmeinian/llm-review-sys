# Compositional PAC-Bayes:

Generalization of GNNs with persistence and beyond

 Kirill Brilliantov

ETH Zurich

kbrilliantov@ethz.ch

&Amauri H. Souza

Federal Institute of Ceara

amauriholanda@ifce.edu.br

&Vikas Garg

YaiYai Ltd & Aalto University

vgarg@csail.mit.edu

###### Abstract

Heterogeneity, e.g., due to different types of layers or multiple sub-models, poses key challenges in analyzing the generalization behavior of several modern architectures. For instance, descriptors based on Persistent Homology (PH) are being increasingly integrated into Graph Neural Networks (GNNs) to augment them with rich topological features; however, the generalization of such PH schemes remains unexplored. We introduce a novel _compositional_ PAC-Bayes framework that provides a general recipe to analyze a broad spectrum of models including those with heterogeneous layers. Specifically, we provide the first data-dependent generalization bounds for a widely adopted PH vectorization scheme (that subsumes persistence landscapes, images, and silhouettes) as well as PH-augmented GNNs. Using our framework, we also obtain bounds for GNNs and neural nets with ease. Our bounds also inform the design of novel regularizers. Empirical evaluations on several standard real-world datasets demonstrate that our theoretical bounds highly correlate with empirical generalization performance, leading to improved classifier design via our regularizers. Overall, this work bridges a crucial gap in the theoretical understanding of PH methods and general heterogeneous models, paving the way for the design of better models for (graph) representation learning. Our code is available at https://github.com/Aalto-QuML/Compositional-PAC-Bayes.

## 1 Introduction

Topological data analysis (TDA) harnesses tools from algebraic topology to unveil the underlying shape and structure of data. TDA has recently gained significant traction within machine learning mainly due to its flagship method: persistent homology (PH) , which allows for capturing topological invariants (like connected components and loops) of the input domain at multiple scales. In particular, PH has recently been leveraged as a tool to augment the representational capabilities of graph neural networks (GNNs) , with expressivity gains formally established . Intuitively, PH furnishes global structural signatures that complement the local nature of GNNs .

Understanding the generalization behavior of these models is crucial as it plays a pivotal role in ensuring their reliability and applicability . In this context, there are two fundamental approaches to achieving generalization bounds: data-independent and data-dependent , each offering unique insights into the generalization problem. Both these approaches have been investigated to analyze the generalization ability of GNNs . Data-dependent generalization bounds evoke particular interest since they are typically much tighter than the agnostic bounds afforded by, e.g., VC dimension. However, no such bounds have been unearted for PH methods (i.e., learnable vectorization schemes) and, consequently, for GNNs enhanced with PH-based descriptors.

We bridge this gap with the first data-dependent generalization bound for classifiers based on a versatile and widely used vectorization framework for persistence diagrams, namely, _PersLay_.

PersLay leverages _extended persistence_ to effectively represent detailed topological features, and subsumes commonly used methods such as persistence landscapes , images , and silhouettes .

Central to our analysis is a novel PAC-Bayes framework (Lemma 2) that provides a general recipe to analyze the generalization of a broad spectrum of models, including those with heterogeneous layers and those comprising multiple sub-models. To achieve this, we introduce general conditions (Equations 6-9) that are satisfied by commonly used learning architectures and, surprisingly, their compositions (Section 4). Leveraging Lemma 2, we show how to obtain bounds for heterogeneous MLPs and GNNs in a straightforward manner (Table 2). Notably, we also establish the first generalization bounds for GNNs augmented with persistence layers (PersLay).

Our exposition focuses on graphs; however, i) our Lemma 2 can be used in any domain and ii) our bound for PersLay considers persistence diagrams obtained from any non-learnable filtration function, and therefore extends more generally to input domains beyond graphs. From a technical perspective, our approach hinges on contrasting previous analyses within the PAC-Bayes framework [9; 37; 38] to extract the common structure encoded in the general conditions of Lemma 2. This allows us to overcome challenges arising from the heterogeneity of the models we consider.

Our experiments on several standard real-world datasets confirm strong correlation between the empirical performance and our theoretical bounds. We reinforce the merits of our analysis via regularized PH-based models informed by our bounds with demonstrable empirical benefits.

**Our main contributions** are:

(Theoretical, see Table 1) We develop a general recipe for obtaining PAC-Bayes bounds for a broad class of (possibly heterogeneous) models and their compositions. We also provide the first data-dependent bounds for PH-based classifiers and _combinations_ of GNNs and PersLay;

(Empirical) We show that the dependence on parameters depicted in our bounds strongly correlates with the observed performance. We also show that novel regularization schemes based on our bounds can reduce the generalization gap of PH-augmented GNNs on multiple datasets.

## 2 Preliminaries

This section overviews GNNs, persistent homology on graphs, and their combination. We also provide basic notions and results in PAC-Bayes learning, which serve as a background for this work.

**Notation.** We consider attributed graphs denoted as a tuple \(G=(V,E,z)\), where \(V=\{1,2,...,n\}\) is the vertex set, \(E V V\) is the edge set, and \(z:V^{d_{z}}\) assigns to each vertex \(v V\) an attribute (or color) \(z(v)\). For convenience, hereafter, we denote the feature vector of \(v\) by \(z_{v}\). We consider classification tasks with input and label spaces \(\) and \(=\{1,,K\}\) (\(K\) is the number of classes) and the \(\)-margin loss \(l_{}:^{K}\{0,1\}\) where \(l_{}(y,)=(_{y}+_{j y}_ {j})\) and \( 0\) is the margin parameter. Let \(=\{(x_{i},y_{i})\}_{i=1}^{m}\) denote a collection of \(m\) input/label pairs sampled i.i.d. from some unknown distribution \(\). Then, the empirical error of a hypothesis \(g_{}:^{K}\) with parameters \(\) is defined as \(_{,}(g_{})=}{{m}}_{i=1}^{ m}l_{}(y_{i},g_{}(x_{i}))\). Accordingly, we can define the generalization error as \(L_{,}(g_{})=_{(x,y)}[l( ,g_{}(x))]\). We use \(\|\|_{2}\) to refer the \(_{2}\) norm (vectors) and the spectral norm (matrices), and \(\|\|_{F}\) to refer to the Frobenius norm. Also, we denote the set \(\{1,...,n\}\) by \([n]\). We provide a notation table in the Appendix (Table 5).

**Graph neural networks (GNNs).** Message-passing GNNs [15; 55] employ a sequence of message-passing steps: each node \(v\) aggregates messages from its neighbors \(N(v)=\{u:(v,u) E\}\), using the resulting vector to update its own embedding. Starting from \(z_{v}^{()}=z_{v}\), GNNs recursively apply

\[z_{v}^{(+1)}=_{}(z_{v}^{()}, {Agg}_{}(\{\!\!\{z_{u}^{()}:u N(v)\}\!\!\})) v  V,\] (1)

 
**Section 3: Generalized PAC-Bayes** & & \\ General recipe for heterogeneous models & & \\ Applying the recipe to GNNs and MLPs & & \\ New bound for PersLay & & \\ 
**Section 4: Compositional PAC-Bayes** & & \\ Bound for the composition with MLP & & \\ Bound for two models in parallel & & \\ New bound for GNNs with persistence & & \\  

Table 1: Main theoretical contributions of this work.

[MISSING_PAGE_FAIL:3]

**Lemma 1** (Neyshabur et al. ).: _Let \(g_{}(x):^{K}\) be any model with parameters \(\), and let \(\) be any distribution on the parameters that is independent of the training data. For any \(\), we construct a posterior \((+)\) by adding any random perturbation \(\) to \(\), s.t., \(_{}(_{x}|g_{+}(x)- g_{}(x)|_{}<)>\). Then, for any \(,>0\), with probability at least \(1-\) over an i.i.d. size-\(m\) training set \(\) drawn according to \(\), for any \(\), we have:_

\[L_{,0}(g_{})_{,}(g_{ })+4((+)||)+}{m-1}}.\]

## 3 Generalized PAC-Bayes

This section first presents a general procedure for obtaining generalization bounds for heterogeneous models, i.e., going beyond spectrally-normalized layers and architecture-specific models (as in ). Then, we show how to leverage such a procedure to extend existing bounds in the literature and to obtain the first generalization bound for PersLay.

Our next result (Lemma 2) applied perturbation-based PAC-Bayes bounds to arbitrary models with (possibly) non-homogeneous layers. To achieve this generality, we carefully contrasted results in  to identify the conditions (Equations 6, 7, 8, and 9) that are sufficient to subsume the considered models as well as to extend to a broader class of models. In Section 4, we will also exploit Lemma 2 in the analysis of different combinations of neural models (e.g., GNNs and PersLay).

**General recipe for PAC-Bayesian bounds for heterogeneous models**

**Lemma 2**.: _Let \(f_{}:^{K}\) be a model with parameters \(=vec\{W_{1},...,W_{n}\}\). If there exists \(T^{+}\) and a sequence \((S_{i})_{i[n]}\) with \(S_{i}^{+}\) both of which may depend on \(\), and parameter-independent \(C_{1},C_{2}^{+}\) and sequence \((_{i})_{i[n]}\) with \(_{i}(0,1]\) such that:_

* _the output is bounded by_ \(C_{1}T\)_:_ \[_{x}\|f_{}(x)\|_{2} C_{1}T,\] (6)
* _the output change can be bounded under a small perturbation of the parameters, i.e., for all_ \(=vec\{U_{1},...,U_{n}\}\) _with_ \(\|U_{i}\|_{2}_{i}S_{i}\)_:_ \[_{x}\|f_{+}(x)-f_{}(x)\|_{2}  C_{2}T_{i=1}^{n}\|_{2}}{S_{i}}\] (7)
* _the following auxiliary conditions hold:_ \[(_{i=1}^{n}}) {T^{1/n}},\] (8) \[:=_{1 i n}_{i}}{2C_{2}}.\] (9)

_Then, for any \(,>0\) with probability at least \(1-\) over the choice of training sets \(\) with \(m\) i.i.d. samples drawn according to some distribution \(\), we have:_

\[L_{,0}(f_{})_{,} (f_{})+\] \[+(\|_{2}^{2}\}T^{ 2}(_{i=1}^{n}})^{2}h\,C_{1}^{2} ^{-2}+\{,}\} }{^{2}m}}),\] (10)

_where \(h\) is the maximum dimension across the matrices \((W_{i})_{i[n]}\)._Proof sketch.: We build on the main result in  by extending it to a broader context. The general idea involves employing Lemma1. Following , we define the prior distribution \(\) as an isotropic Gaussian with variance \(^{2}\) and the posterior distribution \(\) as a shifted isotropic Gaussian with the same variance. To achieve a tighter bound, it is essential to maximize \(\) (since the KL-divergence scales as \((}{{^{2}}})\)); consequently, \(\) should be determined based on the parameter \(\). However, since \(\) must remain independent of the learned weights, we set \(\) according to an approximation of the learned weights. Specifically, we define \(=T_{i}}{{S_{i}}}\) and at first consider only \(\) such that \(\) fall within the range \(|-|}{{2}}\) for some arbitrary \(\), an approximation. We then select \(\) based on this approximation, \(\). At this point we can apply Lemma1 for all \(\) such that \(\) falls into the defined earlier interval. To account for other values of \(\), we establish a finite grid across the relevant \(\) values and choose an appropriate \(\) for each interval on the grid. Finally, a union-bound argument across all \(\) values provides the final result. Although 7 and Lemma1 have their own constraints on the random perturbation, the above steps outline a method to set the variance \(\) that satisfies these constraints and maintains independence from the learned weights. 

Discussion.We note that Lemma2 requires choosing values for the variables \((S_{i})_{i[n]}\) and \(T\). In this regard, one might set \(S_{i}\) as the spectral norm of the weight matrix \(W_{i}\), i.e., \(S_{i}=\|W_{i}\|_{2}\), and make \(T\) equal to \(_{i}S_{i}\). In this case, the condition in Equation8 is satisfied -- the geometric mean is always smaller than or equal to the arithmetic mean. Regarding the variables \((_{i})_{i}\), a typical choice is to set \(_{i}=(}{{n}})\). By doing so, our bound implicitly depends on \(n\) also through \(_{i}\).

The role of Equation6 is to constraint \(\) to non-trivial parameter spaces. In particular, if \(T\) is too small, the magnitude of the model output might not be sufficient to distinguish different inputs up to a margin \(\). In this case, the model would have large empirical loss. In turn, Equation7 is directly associated with the condition in Lemma1, enabling us to use it.

As discussed, Neyshabur et al.  assume spectrally-normalized weight matrices. To avoid this assumption, we introduce the conditions in Equation8 and Equation9, which allow us to pick perturbations that meet the condition in Equation7, again justifying the application of Lemma1.

The bound also includes a somewhat unconventional term, \(\{1,\|\|_{2}^{2}\}\). While this technical term allows for a more concise proof, we note that it does not impose suboptimality. More specifically, in most real-world cases, the squared norm of \(\) is greater than 1. See Appendix1 for a discussion.

  
**Model** (reference) & \(T\) & \(S_{i}\) & \(_{i}\) & \(C_{1}\) & \(C_{2}\) \\ 
**MLP** (Neyshabur et al. ) & \(_{i=1}^{n}\|W_{i}\|_{2}\) & \(\|W_{i}\|_{2}\) & \(\) & \(B\) Lip & \(eB\) Lip \\ 
**GCN** (Liao et al. ) & \(_{i=1}^{n}\|W_{i}\|_{2}\) & \(\|W_{i}\|_{2}\) & \(\) & \(d^{}B\) Lip & \(ed^{}B\) Lip \\ 
**GCN** (Sun and Lin ) & \(_{i=1}^{n}\|W_{i}\|_{2}\) & \(\|W_{i}\|_{2}\) & \(\) & \(B\) Lip & \(eB\) Lip \\ 
**MPGNN**, \(d 1\) (Liao et al. ) & \(\) & \(\|W_{i}\|_{2}\)* & \(\) & \(B\) Lip\({}_{}\) & \(eBn\) Lip\({}_{}\) \\ 
**MPGNN**, \(d=1\) (Liao et al. ) & \(\) & \(\|W_{i}\|_{2}\)* & \(\) & \(Bn\) Lip\({}_{}\) & \(eBn^{2}\) Lip\({}_{}\) \\  =\{d,\|W_{2}\|_{2}\}\)} \\ 

Table 2: **Application of Lemma2 to MLPs and GNNs.** The detailed proof of the lemma applicability can be found in the AppendixE and the detailed description of the models in AppendixB. Here we provide brief description. We consider \(n\)-layer multilayer perceptron (MLP) with weights \(W_{1},...,W_{n}\). After layer \(i\) we apply \(_{i}\)-Lipschitz activation function for \(i[n-1]\). Every input is contained in \(_{2}\)-ball of radius \(B\). We consider \(n\)-layer GCN with weights \(W_{1},...,W_{n}\). After layer \(i\) we apply \(_{i}\)-Lipschitz activation function. Every node feature of the graph is contained in \(_{2}\)-ball of radius \(B\) and the maximum degree of the node is \(d-1\). We denote \(=_{1}...\ _{n-1}\). We consider \(n\)-layer (\(n>2\)) MPGNN with weights \(W_{1},W_{2},W_{3}\) with activation functions \(g,,\) with corresponding Lipschitz constants. We denote \(=_{}_{g}_{p}\|W_{2}\|\), \(=\|W_{1}\|_{2}\|W_{3}\|_{2}\) and \(=((d)^{n-1}-1)/(dC-1)\). Comparing to  we do not add \(_{}\) to \(\) and instead of \(W_{l}\) we have \(W_{3}\).

[MISSING_PAGE_FAIL:6]

getting generalization bounds for combinations of GNNs and PersLay. For readability, here we provide informal statements and defer the formal ones to the Appendix (Lemma 5, Lemma 6).

In particular, Lemma 3 establishes that the composition of MLPs with models that satisfy Lemma 2 also satisfy it. As a result, we can derive PAC-Bayes bounds for heterogeneous models that leverage MLPs using our framework in a straightforward way. This is particularly relevant since deep learning models often employ learnable feature extractors followed by MLPs as classification heads.

**Lemma 3** (Informal; Composition with MLP).: _Let \(f\) be an MLP and \(g\) be a model satisfying Lemma 2 requirements, then \(f g\) also satisfies Lemma 2 requirements._

In addition, we show in Lemma 5 (Appendix) that this result also extends to an arbitrary number of models beyond MLPs. In particular, the result holds whenever we can upper bound output deviations due to perturbations on parameters and inputs, i.e., \(_{x}\|f_{+}(x+ x)-f_{} (x)\|_{2}\) is bounded.

Our next lemma suggests that models satisfying Lemma 2 requirements are closed under parallel concatenation. We note that combining two (or more) models in parallel is also a common design choice in deep learning. For instance, this encompasses persistence-augmented GNNs  and _ensemble_ methods .

**Lemma 4** (Informal; Models in parallel).: _Let \(f_{1}\), \(f_{2}\) be two models satisfying Lemma 2 requirements and \(g\) be an aggregating Lipschitz function. Then, \(g(f_{1}(),f_{2}())\) also satisfies Lemma 2 requirements._

We also provide a generalization of this lemma in the Appendix (Lemma 6) for \(n>2\) models in parallel. It leads to tighter bounds than a naive 2-by-2 sequential application of Lemma 4.

**Corollary 2** (Informal).: _By combining the results for MLPs, GCNs, MPGNNs (Table 2) and that for PersLay (Corollary 1) with Lemma 3 and Lemma 4, we can get generalization bounds on various compositions of these models. In particular, for GNNs with persistence (see Figure 1), we have_

* \(T\) _for the overall model scales with the product of PersLay's and GNN's_ \(T\) _variables;_
* \(C_{1}\) _and_ \(C_{2}\) _of the overall model scale linearly with_ \(C_{1},C_{2}\) _of PersLay and GNN._

Despite the generality of our results, Corollary 2 demonstrates the benefits of our framework in the domain of graph representational learning. To the best of our knowledge, this the first result providing generalization guarantees for graph neural networks combined with persistence vectorization schemes. Furthermore, our findings can aid practitioners in making informed architectural decisions to enhance the generalizability of their models. Specifically, in the case of combining PersLay with GNNs, a tighter bound can be achieved by selecting a PersLay dimension that is considerably smaller than the GNN dimension. Failing to do so may result in a bound dependency on the width of the form \((h)\) rather than \(()\). Additionally, we recommend using aggregation functions such as "mean" or "\(k\)-max" instead of "sum" as the latter introduces a term \(_{G}card(G)\) to the bound, which may be large in practical scenarios.

## 5 Related Works

**Expressivity and generalization of GNNs.** GNNs have achieved state-of-the-art performance across various applications [16; 28; 45; 51; 54], and have garnered significant attention. Maron et al. , Xu et al.  analyzed the representational power of GNNs in terms of the 1-WL test, revealing theoretical limits on their expressivity. This has motivated a surge of works aiming to go beyond 1-WL with GNNs [e.g., 32]. Regarding generalization, Scarselli et al.  provided upper bounds on the order of growth of VC-dimension for GNNs. Garg et al.  presented the first data-dependent generalization bounds for GNNs via Rademacher complexity. Recently, Morris et al.  employed the WL test alongside VC-dimension to gain insights about the generalization performance of GNNs. For details about the expressivity and learning of GNNs, we refer to Jegelka .

**Learning theory and PH.** Birdal et al. , Dupuis et al.  and Chen et al.  investigate connections between learning theory and topological data analysis. In particular, Birdal et al. , Dupuis et al.  explored the concept of PH dimension as a complexity measure to analyze generalization. Chen et al.  proposed a topological regularizer to simplify decision boundaries by penalizing non-essential topological features. In contrast, we apply learning theory to derive data-dependent generalization bounds for arbitrary heterogeneous layers, specifically targetting persistence-aware GNNs, and introduce a regularizer informed by these bounds to guide the design of robust and generalizable models.

**PAC-Bayes.** The PAC-Bayes framework [37; 38] allows us to leverage knowledge about learning algorithms and distributions over the hypothesis set to achieve tighter generalization bounds. Remarkably, Neyshabur et al.  presented a generalization bound for feedforward networks in terms of the product of the spectral norm of weights using a PAC-Bayes analysis. Liao et al.  provided PAC-Bayes bounds for GNNs, and Sun and Lin  enhanced their analysis considering the adversarial case as well. In a seminal work, Dziugaite and Roy  optimized PAC-Bayes bounds directly to obtain non-vacuous generalization bounds for deep stochastic neural networks.

## 6 Experiments

To illustrate the practical relevance of our analysis, we now consider the generalization of persistence-aware models on real-world datasets, and report results for regularized models based on our bounds. In particular, we conduct two main experiments. The first one aims to analyze how well our bounds capture generalization gaps as a function of model variables. The second assesses to which extent a structural risk minimization algorithm that uses our bound on the weights spectral norm improve generalization compared to empirical risk minimizers. We implemented all experiments using PyTorch , and implementation details are given in the Appendix J. Our code is available at https://github.com/Aalto-QuML/Compositional-PAC-Bayes.

**Datasets and evaluation setup.** We use six popular benchmarks for graph classification: DHFR, MUTAG, PROTEINS, NCI1, IMDB-BINARY, MOLHIV, which are available as part of TUDatasets  and OGB . We use a 80/10/10% (train/val/test) split for all datasets when we perform model selection. Here, we consider both PersLay Classifiers and GNNs with persistence models with constant weight functions and Gaussian point transformations. For the experiments with GNNs, we kept only the larger datasets (and added results for the NCI109 dataset). Regarding filtration functions, we closely follow  and use Heat kernels with parameter values equal to \(0.1\) and \(10\).

**Dependence on model components**. Figure 2 and Figure 4 show the generalization gap (measured as \(L_{,0}-_{,=1}\)) and the bound on the weights spectral norm (\(T\) from Lemma 2) over the training epochs for PersLay Classifier and GNNs with persistence, respectively. To evaluate how well our bound captures the trend observed in the empirical gap, we compute correlation coefficients between the two sequences across different seeds and report their mean and standard deviation for each dataset. Overall, the coefficients are greater than 0.7 in 7 out of 9 experiments, indicating a good correlation.

Figure 3 shows the empirical gap and our estimated bound as a function of the model's width for the PersLay classifier. Again, we compute correlation coefficients between the two curves and find they are highly correlated (with an average correlation above \(0.91\) on \(4\) out of \(5\) datasets). Also, we note that these curves are obtained at the final training epoch. We report additional results across different epochs and hyper-parameters in the supplementary material. Again, these results validate that our theoretical bounds can capture the trend observed in the empirical generalization gap.

Figure 3: **PersLay classifier: width vs. generalization gap.** The dependence of the empirical gap on the model width is captured by our bound. We obtain high average correlation for all datasets.

Figure 2: **PersLay classifier: spectral norm vs. generalization gap.** Overall, our bound on the spectral norm of the weights is highly correlated with the generalization gap.

**Regularizing PersLay.** We compare variants of PersLay trained via ERM (empirical risk minimization) and a regularized version with loss given by \(_{S,1}+_{r} n\|\|_{2}^{2}^{2}}\); \(_{r}\) is a hyper-parameter that balances the influence of the two terms and \(=T_{i}}{{S_{i}}}\) -- see proof sketch of Lemma 2 in Section 3. This is similar to a weight-decay regularization approach, with the spectral norm of weights appearing in \(\). Here, we consider models with \(n=1\) or \(2\), selected via hold-out validation. We note that PersLay classifier does not use node features, it only exploits graph structures.

Table 3 reports accuracy results (mean and standard deviations) computed over five runs. Overall, the regularized approach significantly outperforms the ERM variant despite the use of small-sized networks. On 5/6 datasets, PersLay with spectral norm regularization is the best-performing model.

**Regularizing GNNs with persistence.** We now evaluate the impact of using our bound to regularize different GNNs combined with persistent homology (PersLay) in parallel mode. We consider GCN , GraphSage , and GIN  architectures. Table 4 reports the test classification error and the generalization gap on the NCI, NCI109, and PROTEINS datasets -- mean and standard deviation obtained over five independent runs. For our regularizer, we select the optimal penalization factor \(_{r}\{\)1e-5, 1e-6, 1e-7, 1e-8\(\}\) using the validation set. Overall, the results show that the regularized methods achieve smaller generalization gaps and slightly lower classification errors. In particular, our spectral regularizer leads to a significant drop in generalization gap in all experiments.

    & &  &  \\ 
**GNN** & **Dataset** & ERM & SpecNorm & ERM & SpecNorm \\   & NCI & 0.22 \(\) 0.01 & **0.21**\(\) 0.02 & 0.19 \(\) 0.01 & **0.01**\(\) 0.06 \\  & NCI109 & 0.28 \(\) 0.00 & 0.28 \(\) 0.02 & 0.25 \(\) 0.00 & **0.12**\(\) 0.03 \\  & PROTEINS & 0.31 \(\) 0.01 & **0.27**\(\) 0.02 & 0.25 \(\) 0.02 & **-0.02**\(\) 0.11 \\   & NCI & 0.24 \(\) 0.01 & **0.21**\(\) 0.02 & 0.18 \(\) 0.04 & **-0.08**\(\) 0.05 \\  & NCI109 & 0.26 \(\) 0.01 & **0.24**\(\) 0.01 & 0.23 \(\) 0.01 & **0.05**\(\) 0.06 \\  & PROTEINS & 0.27 \(\) 0.02 & **0.26**\(\) 0.02 & 0.25 \(\) 0.01 & **-0.15**\(\) 0.30 \\   & NCI & 0.25 \(\) 0.01 & **0.22**\(\) 0.00 & 0.23 \(\) 0.01 & **0.01**\(\) 0.06 \\  & NCI109 & 0.24 \(\) 0.01 & 0.24 \(\) 0.03 & 0.22 \(\) 0.01 & **0.00**\(\) 0.08 \\   & PROTEINS & **0.29**\(\) 0.02 & 0.30 \(\) 0.03 & 0.26 \(\) 0.03 & **0.07**\(\) 0.18 \\   

Table 4: Test classification error (0-1 loss) and generalization gap (\(L_{,0}-_{S,}\)) for PH-augmented GNNs. ERM means empirical risk minimizer (no regularization). We denote the best-performing methods in bold. In almost all cases, employing the method derived from our theoretical analysis leads to the smallest test errors and generalization gaps.

Figure 4: **GNNs with persistence: empirical gap vs. PAC-Bayes bound**. Again, there is positive and high correlation between our bound and the observed generalization gap.

Conclusion, Broader Implications, and Limitations

We derive the first generalization bounds for neural networks that appeal to persistent homology for graph learning. Notably, the analyzed framework (PersLay) offers a flexible and general way to extract vector representations from persistence diagrams. Due to this generality, our analysis covers several methods available in the literature. The developed framework also allows to analyze _composite_ models like, GNNs combined with PersLay. Our constructions involve a perturbation and generalization behavior analysis of **non**-homogeneous networks in rather general setting, which poses specific technical challenges.

While we provide valuable insights and methodologies, we would like to underscore the need for future investigations to delve into PH schemes that encompass parametrized filtration functions. Nonetheless, while some works showed gains using learnable filtrations , others have reported no benefits and advocated fixed functions instead [5; 34]. Moreover, the tightness of our bounds can further be improved since there is still considerable gap between empirical results and the theoretical one. By shedding new light on the generalization of machine learning models based on persistent homology, we hope to contribute to the community by providing key insights about the limits and power of these methods, paving the path to further theoretical developments on PH-based neural networks for graph representation learning.