# Learning to Tokenize for Generative Retrieval

Weiwei Sun\({}^{1}\), Lingyong Yan\({}^{2}\), Zheng Chen\({}^{1}\), Shuaiqiang Wang\({}^{2}\), Haichao Zhu\({}^{2}\)

**Pengjie Ren\({}^{1}\), Zhumin Chen\({}^{1}\), Dawei Yin\({}^{2}\), Maarten de Rijke\({}^{3}\), Zhaochun Ren\({}^{4}\)**

\({}^{1}\)Shandong University, China \({}^{2}\)Baidu Inc., China

\({}^{3}\)University of Amsterdam, The Netherlands \({}^{4}\)Leiden University, The Netherlands

{sunnweiwei,lingyongy}@gmail.com yindawei@acm.org m.derijke@uva.nl z.ren@liacs.leidenuniv.nl

Corresponding author.

###### Abstract

As a new paradigm in information retrieval, generative retrieval directly generates a ranked list of document identifiers (docids) for a given query using generative language models (LMs). How to assign each document a unique docid (denoted as _document tokenization_) is a critical problem, because it determines whether the generative retrieval model can precisely retrieve any document by simply decoding its docid. Most existing methods adopt rule-based tokenization, which is ad-hoc and does not generalize well. In contrast, in this paper we propose a novel document tokenization learning method, GenRet, which learns to encode the complete document semantics into docids. GenRet learns to tokenize documents into short discrete representations (i.e., docids) via a discrete auto-encoding approach. We develop a progressive training scheme to capture the autoregressive nature of docids and diverse clustering techniques to stabilize the training process. Based on the semantic-embedded docids of any set of documents, the generative retrieval model can learn to generate the most relevant docid only according to the docids' semantic relevance to the queries. We conduct experiments on the NQ320K, MS MARCO, and BEIR datasets. GenRet establishes the new state-of-the-art on the NQ320K dataset. Compared to generative retrieval baselines, GenRet can achieve significant improvements on unseen documents. Moreover, GenRet can also outperform comparable baselines on MS MARCO and BEIR, demonstrating the method's generalizability.

## 1 Introduction

Document retrieval plays an essential role in web search applications and various downstream knowledge-intensive tasks by identifying relevant documents to satisfy users' queries. Recently, _generative retrieval_ has emerged as a new paradigm for document retrieval [1; 5; 37; 41; 46; 47] that directly generates a ranked list of document identifiers (docids) for a given query using generative language models (LMs). Unlike dense retrieval [9; 13; 23; 42], generative retrieval presents an end-to-end solution for document retrieval tasks . It also offers a promising approach to better exploit the capabilities of recent large LMs [1; 41].

As shown in Figure 1, document tokenization aims to tokenize each document in the corpus as a sequence of discrete characters, i.e., docids. Document tokenization plays a crucial role in generative retrieval, as it defines how the document is distributed in the semantic space . And it is still an open problem how to define docids. Most previous generative methods tend to employ rule-based document tokenizers, such as generating titles or URLs [5; 46], or clustering results from off-the-shelf document embeddings [37; 41]. Such rule-based methods are usually ad-hoc and do not generalize well. In particular, the tokenization results potentially perform well on retrieving documents that have been seen during training, but generalize poorly to unlabeled documents [17; 20].

To address the above problem, we propose GenRet, a document tokenization learning framework that learns to tokenize a document into semantic docids in a discrete auto-encoding scheme. GenRet consists of a shared sequence-to-sequence-based document tokenization model, a generative retrieval model, and a reconstruction model. In the proposed auto-encoding learning scheme, the tokenization model learns to convert documents to discrete docids, which are subsequently utilized by the reconstruction model to reconstruct the original document. The generative retrieval model is trained to generate docids in an autoregressive manner for a given query. The above three models are optimized in an end-to-end fashion to achieve seamless integration.

There are usually two challenges when using auto-encoding to optimize a generative retrieval model: (i) docids with an autoregressive nature, and (ii) docids with diversity. To address the first challenge and also to stabilize the training of GenRet, we devise a progressive training scheme. This training scheme allows for a stable training of the model by fixing optimized prefix docids \(z_{<t}\). To optimize the docids at each step, three proposed losses are utilized: (i) a reconstruction loss for predicting the document using the generated docid, (ii) a commitment loss for committing the docid and avoiding forgetting, and (iii) a retrieval loss for optimizing the retrieval performance end-to-end. To address the second challenge, we propose a parameter initialization strategy and a re-assignment of the docid based on a _diverse clustering_ technique to increase the diversity of the generated docids.

We conduct extensive experiments on three well-known document retrieval benchmark datasets, NQ320K [15; 37], MS MARCO [4; 46], and BEIR . GenRet attains superior retrieval performance against state-of-the-art generative retrieval models on NQ320K. GenRet achieves +14% relative improvements on the unseen test set of NQ320K over the best generative retrieval baseline. Experiments on MS MARCO and six BEIR datasets also show that GenRet outperforms existing generative methods and achieves competitive results compared to popular dense retrieval models. Experiments on retrieving new documents, analytical experiments, and an efficiency analysis confirm the effectiveness of the proposed model.

We summarize our contributions as follows: (i) We propose GenRet, a generative retrieval model that represents documents as discrete semantic docids. To the best of our knowledge, this is the first tokenization learning method for document retrieval. (ii) We propose an auto-encoding approach, where the docids generated by our tokenization model are reconstructed by a reconstruction model to ensure the docids capture the semantic information of the document. (iii) We devise a progressive training scheme to model the autoregressive nature of docids and stabilize the training process. (iv) Experimental results demonstrate that GenRet achieves significant improvements, especially on unseen documents, over generative retrieval baselines.2

## 2 Preliminaries

The document retrieval task can be formalized as the process of retrieving a relevant document \(d\) for a search query \(q\) from a collection of documents \(\). Each document \(d\) is assumed to be a plain text consisting of a sequence of tokens, denoted as \(d=\{d_{1},,d_{|d|}\}\), where \(|d|\) is the total number of tokens in the document. For generative retrieval models, it is usually challenging and computationally

Figure 1: An overview of our proposed method. The proposed method utilizes a document tokenization model to convert a given document into a sequence of discrete tokens, referred to as a docid. This tokenization process allows for the reconstruction of the original document through a reconstruction model. Subsequently, an autoregressive generation model is employed to retrieve documents through the generation of their respective docids.

inefficient to directly generate original documents of typically long length. Therefore, most existing approaches rely on a technique named **document tokenization**, which represents a document \(d=\{d_{1},,d_{|d|}\}\) as a shorter sequence of discrete tokens (docid) \(z=\{z_{1},,z_{t},,z_{M}\}\), where each token \(z_{t}\) is as a \(K\)-way categorical variable, with \(z_{t}[1,2,,K]\), and \(M\) is the length of the docid. See Figure 1 for an example of document tokenization with \(M=3\) and \(K=64\).

As an alternative sequence of the original document, the tokenized docid \(z\) should satisfy the following two properties: (i) different documents have short but different docids; and (ii) docids capture the semantics of their associated documents as much as possible . Because \(z\) is a sequence of a fixed length and usually shorter than the original document \(d\), the model's training and inference can be simplified and more efficient. This paper employs a tokenization model \(Q d z\) to map \(d\) to docid \(z\). More details about \(Q\) are provided in Section 3.1. After tokenizing each document to docid \(z\), a generative retrieval model \(P q z\) learns to retrieve relevant documents by generating a query \(q\) to a docid \(z\) autoregressively .

## 3 Method

Conventionally, document tokenization is done by a fixed pre-processing step, such as using the title of a document or the results of hierarchical clustering obtained from BERT [5; 37]. However, it has been observed that such ad-hoc document tokenization methods often fail to capture the complete semantics of a document. For example, the title of a web page often does not exist or has low relevance to the content of the web page, and the use of clustering-based docids arbitrarily defines the document in discrete space.

In this paper, we propose GenRet, a novel tokenization learning method based on discrete auto-encoding, to learn semantic docid in a fully end-to-end manner. Figure 1 gives an overview of the proposed method. The proposed GenRet comprises three main components: (i) a sequence-to-sequence based retrieval model \(P(z q)\), (ii) a document tokenization model \(Q(z d)\), and (iii) a reconstruction model \(R(d z)\). The document tokenization model tokenizes a document \(d\) into unique discrete variables \(z\), and the retrieval model is trained to generate the latent variables \(z\) for a given query \(q\). In addition, the reconstruction model is used to re-generate the original document from \(z\) to ensure \(z\) captures the semantics of the original document as much as possible.

### Model architecture

Following DSI , we employ an encoder-decoder Transformer to implement the generative retrieval model. Specifically, given an input text \(d\)3, the T5-based tokenization model encodes \(d\) and a prefix of docid \(z_{<t}\) and continuously produces latent representation \(_{t}\) of \(d\) at time step \(t\):

\[_{t}=((d),z_{<t})\ ^{D},\] (1)

where \(D\) denotes the hidden size of the model. \((d)\) denotes the output of the \(\).

Then, the tokenization model generates a token for each document based on \(_{t}\). At each timestep \(t\), we define an external embedding matrix named _codebook_\(_{t}^{K D}\), where \(K\) is the size of the discrete latent space. There are \(K\) embedding vectors \(_{t,j}^{D},j[K]\), and each vector \(_{t,j}\) can be regarded as the centroid of a segmentation.

Based on the _codebook_\(_{t}\), the discrete latent variable \(z_{t}\) at timestep \(t\) is calculated by a dot-product look-up using the codebook \(_{t}\):

\[Q(z_{t}=j z_{<t},d)=_{j}(_{t}_{t} ^{}),\] (2)

where \(Q(z_{t}=j z_{<t},d)\) denotes the probability of tokenizing \(d\) to a particular value \(j[K]\) at timestep \(t\), \(_{j}\) is a softmax function to output the probability of axis \(j\).

**Document reconstruction model.** The docid generated by the tokenization model \(Q\) is required to capture the semantic information of the document. To this end, we propose an auto-encoding training scheme, where a reconstruction model \(R z d\) that predicts \(d\) using \(z\) is designed to force the tokenization model \(Q d z\) to reproduce a docid \(z\) that can be reconstructed back-to-the original document.

The input of the reconstruction model is docid \(z\), and the output is its associated document \(d\). We first embed \(z\) into representation matrix \(=\{_{1},,_{M}\}^{M D}\) using the codebook of the tokenization model:

\[=\{_{1,z_{1}},_{2,z_{2}},,_{M,z_{ M}}\}^{M D},\] (3)

where each \(t[M]\), \(_{t}=_{t,z_{t}}^{D}\) is the embedding vector of \(z_{t}\) in the \(t\)-step codebook \(_{t}\).

We then devise a retrieval-based reconstruction model that predicts the target document \(d\) by retrieving it from document collection \(\), based on the inputs \(\). The relevance score between the input docid \(z\) and the target document \(d\) is defined as follows:

\[R(d)=_{t=1}^{M}_{t}( _{t}^{}))}{_{d^{*} S(z_{<t})}(_{t} (^{*}_{t}))},\] (4)

where \(S(z_{<t})\) is a sub-collection of \(\) consisting of documents that have a docid prefix that is the same as \(z_{<t}\). \(d^{*} S(z_{<t})\) represents a document from the sub-collection \(S(z_{<t})\). \(_{t}\) and \(^{*}_{t}\) are continuous representations of documents \(d\) and \(d^{*}\), respectively, as defined in Eq. 1. The operator \(()\) is the stop gradient operator to prevent gradient back propagation. Intuitively, \(R(d)\) is designed to retrieve a specific document \(d\) from a set of documents \(S(z_{<t})\) at each timestep \(t\). The set \(S(z_{<t})\) only includes those documents that are assigned the same docid prefix \(z_{<t}\) as the target document \(d\). By utilizing this loss function, at each step \(t\), the model is facilitated to learn the residual semantics of the documents not captured by the previous docid \(z_{<t}\).

### Model optimization

For the document tokenization model \(Q(z d)\), generative retrieval model \(P(z q)\), and reconstruction model \(R(d z)\), jointly optimizing these three models using auto-encoding is challenging due to the following reasons: (i) **Learning docids in an autoregressive fashion**. On one hand, the prediction of the \(z_{t}\) at time \(t\) relies on previously predicted docids \(z_{<t}\), which is often under-optimized at the beginning and rapidly changes during training, making it difficult to reach convergence. On the other hand, simultaneously optimizing \(z\) makes it challenging to guarantee a unique docid assignment. Hence, to stabilize the training of GenRet, we devise a _progressive training scheme_ (see Section 3.2.1). (ii) **Generating docids with diversity**. Optimizing the model using auto-encoding often leads to unbalanced docid assignment: a few major docids are assigned to a large number of documents and most other docids are rarely assigned. Such a sub-optimal distribution of docids affects the model distinguishability, which in turns triggers length increments of docids in order to distinguish conflicting documents. We introduce two _diverse clustering_ techniques to ensure docid diversity (see Section 3.2.2).

#### 3.2.1 Progressive training scheme

To optimize each of the three models listed above in an autoregressive manner, we propose a progressive auto-encoding learning scheme, as illustrated in Figure 2. The whole learning scheme contains \(M\) learning steps with respect to the final docid in \(M\)-token. And the docid \(z_{T}\) at step \(T[M]\) is learned and optimized at the corresponding learning step. Besides, at each step \(T[M]\), the docid \(z_{T}\) and the model parameters associated with \(z_{T}\) generation are updated, while previously produced docids \(z_{<T}\) and other parameters are kept fixed. By progressively performing the above process, we can finally optimize and learn our models.

At each optimization step, say the \(T\)-step, we devise the learning objective for document tokenization consisting of three loss functions detailed below.

**Reconstruction loss.** We utilize the reconstruction model \(R(d z)\) as an auxiliary model to learn to optimize the docid generation, whose main goal is capturing as much semantics in the docid as

Figure 2: Progressive training scheme. \(z_{t}\) (docid at timestep \(t\)) is optimized at the \(t\)-th training step, while \(z_{<t}\) (docids before timestep \(t\)) are kept fixed.

possible. Therefore, we define a reconstruction loss function of step \(T\) as follows:

\[_{}=- R(d}_ { T}),&\;\;}_{ T}=\{( _{1}),,(_{T-1}),_{T}\}\; ^{T D}\\  t[T]:\;_{t}=_{t,j^{*}}^{D},&\;\;j^{*}=*{arg\,max}_{j}Q(z_{t}=j  z_{<t},d),\] (5)

where \(}_{ T}\) is the first \(T\) representations of the \(z\), and only the variable \(_{T}\) is optimized in step \(T\). \(Q(z_{t}=j z_{<t},d)\) is defined in Eq. 2. And the document tokenization model \(Q\) can therefore be optimized when minimizing \(_{}\).

Of note, since the computation involves a non-differentiable operation (\(*{arg\,max}()\)), we apply straight-through gradient estimation to back-propagate the gradient from reconstruction loss to \(_{T}\) following , which copies the gradient of \(_{T}\) directly to \(_{T}\). Specifically, the gradients to document representation \(_{T}\) are defined as \(_{}}{_{T}} _{}}{_{T}}\). And the gradients to the _codebook_ embedding \(_{T,j}\) are defined as \(_{}}{_{T,j}} 1_{z_{T}=j}_{}}{ _{T}}\).

**Commitment loss.** In addition, to make sure the predicted docit commits to an embedding and to avoid models forgetting previous docid \(z_{<t}\), we add a commitment loss as follows:

\[_{}=-_{t=1}^{T} Q(z_{t} z_{<t},d).\] (6)

**Retrieval loss.** For the generative retrieval model \(P\), we jointly learn it together with the document tokenization model \(Q\), where \(P\) learns to generate the docids of relevant documents \(d\) given a query \(q\). Specifically, suppose \((q,d)\) are a query and relevant document pair; we define the learning objective of retrieval model \(P\) as:

\[_{}=-_{T}_{T})}{ _{d^{*} B}(_{T}^{*}{}_{T})}-_{t=1}^{ T} P(z_{t} z_{<t},q),\] (7)

where the first term is a ranking-oriented loss enhancing the model using \((q,d)\) pair; \(d^{*}\) is an in-batch document sampled from the same training mini-batch \(B\); \(_{T}\) and \(_{T}\) denote the representation of \(q\) and \(d\) at timestep \(T\). The second term is the cross-entropy loss for generating docid \(z\) based on \(q\).

The final loss we use at step-\(T\) is the sum of reconstruction loss, commitment loss, and retrieval loss:

\[=_{}+_{}+_{ }.\] (8)

#### 3.2.2 Diverse clustering techniques

To ensure diversity of generated docids, we adopt two diverse clustering techniques-codebook initialization and docid re-assignment at each progressive training step, where codebook initialization mainly aims to increase the balance of semantic space segmentation, and the docid re-assignment mainly aims to increase the balance of docid assignments.

**Codebook initialization.** In order to initialize the codebook for our model, we first warm-up the model by passing the continuous representation \(_{T}\) to the reconstruction model instead of the docid representation \(_{T}\) as defined in Eq. 3. During this warm-up phase, we optimize the model using the reconstruction loss \(_{}\) and commitment loss \(_{}\). Next, we collect the continuous representations \(_{T}\) of all documents in \(\), and cluster them into \(K\) groups. The centroids of these clusters are then used as the initialized codebook \(_{T}\). To balance the initialized docid distribution, we utilize a diverse constrained clustering algorithm, _Constrained K-Means_, which first normalizes the embeddings of each prefix group, and modifies the cluster assignment step (E in EM) by formulating it as a minimum cost flow (MCF) linear network optimization problem .

**Docid re-assignment.** In order to assign docids to a batch of documents, we modify the dot-product look-up results in Eq. 2 by ensuring that the docid for different documents in the batch are distinct following the method described in . Specifically, let \(_{t}=\{_{t}^{(1)},,_{t}^{(B)}\}^{B D}\) denote the continuous representation of a batch of documents with batch size of \(B\). The dot-product results are represented by \(=_{t}_{t}^{}^{B K}\). To obtain distinct docids, we calculate an alternative \(^{*}=*{Diag}()(}{ })*{Diag}()\), where \(\) and \(\) are re-normalization vectors in \(^{K}\) and \(^{B}\), respectively. The re-normalization vectors are computed via the iterative Sinkhorn-Knopp algorithm . Finally, \(^{*}\) is used instead of \(\) in the \(*{Softmax}\) and \(*{arg\,max}\) (Eq. 2) operations to obtain the docid \(z_{t}\).

Experimental Setup

### Datasets and evalutaion metrics

We conduct experiments on three well-known document retrieval datasets: NQ320K , MS MARCO , and BEIR . We divide the test set of NQ320K into _seen test_ and _unseen test_, based on whether the target documents of the query have annotated queries in the training data, to test the generalization ability of the model on unlabeled documents. More details about data pre-processing and data statistics are listed in the Appendix A.

On NQ320K, we use Recall@{1,10,100} and Mean Reciprocal Rank (MRR)@100 as evaluation metrics, following . On MS MARCO, we use Recall@{1, 10, 100} and MRR@10 as evaluation metrics, following . On BEIR, we use nDCG@10 as the main metrics and calculate the average nDCG@10 values across multiple downstream sub-datasets as overall metrics.

### Baselines

We consider three types of baselines: sparse retrieval methods, dense retrieval methods, and generative retrieval methods. The **sparse retrieval** baselines are: BM25  and DocT5Query . The **dense retrieval** baselines are: DPR , ANCE , Sentence-T5 , GTR , and Contriever . The **generative retrieval** baselines are: GENRE , DSI , SEAL , CGR-Contra , DSI-QG , NCI , and Ultron . The following three baselines use the same pre-trained LM T5 as GenRet: (i) Sentence-T5 outputs continuous vectors, (ii) GENRE outputs document titles, and (iii) DSI-QG outputs clustering IDs, while GenRet outputs docids learned using the proposed tokenization method. See Appendix B for more details on the other baselines.

### Implementation details

**Hyper-parameters.** In our experiments, we utilize the T5-Base model  as the base Transformer and initialize a new codebook embedding \(_{t}\) for each time step. The parameters of both the encoder-decoder and codebook are shared between the tokenization model and the retrieval model. We set the number of clusters to be \(K=512\) for all datasets, with the length of the docid \(M\) being dependent on the number of documents present. For datasets containing a larger number of candidate documents, a larger value of \(M\) is set to ensure that all documents are assigned unique document ids. In the docid re-assignment, the hyper-parameter \(\) is set to \(1.0\), and the Sinkhorn-Knopp algorithm is executed for \(100\) iterations.

**Indexing with query generation.** Following previous work [47; 41; 40], we use query generation models to generate synthetic (query, document) pairs for data augmentation. Specifically, we use the pre-trained query generation model from DocT5Query  to augment the NQ and MS MARCO datasets. In query generation, we use nucleus sampling with parameters \(p=0.8,t=0.8\) and generate five queries for each document in the collection. For the BEIR datasets, we use the queries generated by GPL . GPL uses a DocT5Query  generator trained on MS MARCO to generate about 250K queries for each BEIR dataset. Note that the query generator used for BEIR is purely trained on MS MARCO (without using any training data of BEIR) and thus conforms to the zero-shot setting of BEIR [38; 40].

**Training and inference.** The proposed models and the reproduced baselines are implemented with PyTorch 1.7.1 and HuggingFace transformers 4.22.2. We optimize the model using AdamW and set the learning rate to \(5e-4\). The batch size is \(256\), and the model is optimized for up to 500k steps for each timestep. During training, we pre-gather documents which share the same docid prefix into a batch. Therefore, the reassignment strategy is applied to a batch, where we aim to have documents with as diverse IDs as possible. We add a factor of 0.1 to the reconstruction losses to balance the scale. In progressive training, we first warm up the model for 5K steps and then initialize the codebook using the clustering centroids as mentioned in Section 3.2.1. We use constrained clustering4 to obtain diverse clustering results. During inference, we use beam search with constrained decoding  and a beam size of \(100\).

## 5 Experimental results

### Main results

**Results on NQ320K.** In Table 1, we list the results on NQ320K. GenRet outperforms both the strong pre-trained dense retrieval model, GTR, and the previous best generative retrieval method, NCI, thereby establishing a new state-of-the-art on the NQ320K dataset. Furthermore, our results reveal that existing generative retrieval methods perform well on the seen test but lag behind dense retrieval methods on the unseen test. For example, NCI obtains an MRR@100 of \(76.8\) on the seen test, which is higher than the MRR@100 of \(65.3\) obtained by GTR-Base. However, on unseen test data, NCI performs worse than GTR-Base. In contrast, GenRet performs well on both seen and unseen test data. This result highlights the ability of GenRet to combine the advantages of both dense and generative retrieval by learning discrete docids with semantics through end-to-end optimization.

**Results on MS MARCO.** Table 2 presents the results on the MS MARCO dataset. GenRet outperforms generative retrieval methods such as Ulron and dense retrieval baselines such as ANCE and Sentence-T5. Furthermore, previous generative retrieval methods (e.g., GENRE, Ulron) utilizing metadata such as the title and URL, while exhibiting decent performance on the NQ320K dataset, underperform in comparison to dense retrieval and sparse retrieval methods on the MS MARCO dataset. This may be because the NQ320K dataset retrieves Wikipedia documents, where metadata like the title effectively capture the semantics of the document. In the case of the MS MARCO dataset, which is a web search dataset, the metadata often does not adequately characterize the documents, resulting in a decline in performance of the generative retrieval model. In contrast, GenRet learns to generate semantic docids that effectively enhance the generative retrieval model.

**Results on BEIR.** Table 3 lists the results of the baselines and GenRet on six datasets of BEIR. These datasets represent a diverse range of information retrieval scenarios. On average, GenRet outperforms strong baselines including BM25 and ST5 GPL, and achieves competitive results compared to previous-best sparse and dense retrieval methods. Additionally, GenRet demonstrates a significant improvement over the previous generative retrieval model GENRE that utilizes titles as docids. Furthermore, GENRE performs poorly on some datasets, such as BEIR-Covid and BEIR-SciDocs. This may be because the titles of the documents in these datasets do not adequately capture their semantic content.

    &  &  &  \\ 
**Method** & R@1 & R@10 & R@100 & MRR & R@1 & R@10 & R@100 & MRR & R@1 & R@10 & R@100 & MRR \\   \\ BM25  & 29.7 & 60.3 & 82.1 & 40.2 & 29.1 & 59.8 & 82.4 & 39.5 & 32.3 & 61.9 & 81.2 & 42.7 \\ DocT5Query  & 38.0 & 69.3 & 86.1 & 48.9 & 35.1 & 68.3 & 86.4 & 46.7 & 48.5 & 72.9 & 85.0 & 57.0 \\   \\ DPR  & 50.2 & 77.7 & 90.9 & 59.9 & 50.2 & 78.7 & 91.6 & 60.2 & 50.0 & 74.2 & 88.7 & 58.8 \\ ANCE  & 50.2 & 78.5 & 91.4 & 60.2 & 49.7 & 79.2 & 92.3 & 60.1 & 52.0 & 75.9 & 88.0 & 60.5 \\ Sentence-T5\({}^{}\) & 53.6 & 83.0 & 93.8 & 64.1 & 53.4 & 83.9 & 94.7 & 63.8 & 56.5 & 79.5 & 90.7 & 64.9 \\ GTR-Base  & 56.0 & 84.4 & 93.7 & 66.2 & 54.4 & 84.7 & 94.2 & 65.3 & 61.9 & 83.2 & 92.1 & 69.6 \\   \\ GENRE\({}^{}\) & 55.2 & 67.3 & 75.4 & 59.9 & 69.5 & 83.7 & 90.4 & 75.0 & 6.0 & 10.4 & 23.4 & 7.8 \\ DST  & 55.2 & 67.4 & 78.0 & 59.6 & 69.7 & 83.6 & 90.5 & 74.7 & 1.3 & 7.2 & 31.5 & 3.5 \\ SEAL  & 59.9 & 81.2 & 90.9 & 67.7 & - & - & - & - & - & - & - \\ CGR-Contra  & 63.4 & 81.1 & - & - & - & - & - & - & - & - & - \\ DSI-QG\({}^{}\) & 63.1 & 80.7 & 88.0 & 69.5 & 68.0 & 85.0 & 91.4 & 74.3 & 45.9 & 65.8 & 76.3 & 52.8 \\ NCI  & 66.4 & 85.7 & 92.4 & 73.6 & 69.8 & 88.5 & 94.6 & 76.8 & 54.5 & 75.9 & 84.8 & 62.4 \\
**Ours** & **68.1\({}^{}\)** & **88.8\({}^{}\)** & **95.2\({}^{}\)** & **75.9\({}^{}\)70.2\({}^{}\)** & **90.3\({}^{}\)** & **96.0\({}^{}\)** & **77.7\({}^{}\)** & **62.5\({}^{}\)** & **83.6\({}^{}\)** & **92.5\({}^{}\)** & **70.4\({}^{}\)** \\   

Table 1: Results on Natural Questions (NQ320K). The results of the methods marked with \({}^{}\) are from our own re-implementation, others are from their official implementation. * and ** indicate significant improvements over previous-best generative retrieval baselines with p-value \(<0.05\) and p-value \(<0.01\), respectively. \(\) and \(\) indicate significant improvements over previous-best dense retrieval baselines with p-value \(<0.05\) and p-value \(<0.01\), respectively. The best results for each metric are indicated in boldface.

### Analytical experiments

We further conduct analytical experiments to study the effectiveness of the proposed method.

In Figure 3 (left), we plot the frequencies of docids at the first timestep of various learning methods. We label each method using a box with a docid and a diversity metric \(d\), which is calculated by: \(d=1-_{j=1}^{K}|n_{j}-n_{u}|\), where \(||\) represents the absolute value, \(n\) denotes the total number of documents, \(n_{j}\) denotes the number of documents that have a \(=j\), and \(n_{u}=\) is the expected number of documents per docid under the uniform distribution.

The results demonstrate the superiority of GenRet (represented by the yellow line) in terms of distribution uniformity. It uses all the potential docid \(k=512\) and achieves the highest diversity metric with a value of \(d=0.90\). The method without docid reassignment also yields a relatively balanced distribution, with a diversity metric of \(d=0.77\). However, the distribution of the method without diverse codebook initialization is highly uneven, which could be due to the fact that most of the randomly initialized codebook embeddings are not selected by the model during the initial training phase, leading to a lack of update and further selection in subsequent training. Additionally, the models without diverse clustering tend to converge to a trivial solution where all documents are assigned the same docid.

In Figure 3 (right), the results of two ablated variants are presented. First, GenRet w/o learning is a generative model that has been trained directly using the final output docid from GenRet, without utilizing the proposed learning scheme. Its retrieval performance is comparable to that of GenRet on seen test data; however, it is significantly lower on unseen test data. This variant demonstrates that the generative retrieval model jointly trained with auto-encoding objectives can represent documents more sensibly based on semantics. This could enhance performance on the less-optimized documents. Contrarily, the parameters obtained via cross-entropy loss on docid generation tasks are less effective in conveying the semantic information of documents. Secondly, GenRet w/ T5-Small uses a small model, and its performance is inferior to that of GenRet using T5-Base. However, the gap between the performance on seen and unseen test data is smaller, which could be attributed to the limited fitting capacity of the small model.

In Appendix C, we evaluate the proposed model's capacity to retrieve new documents and find that it performs well in adapting to new documents compared to existing document tokenization approaches. Additionally, in Appendix D, we analyze the efficiency of various retrieval models in comparison to the various baselines in terms of memory usage, offline, and online latency, and show the advantages of the proposed model.

   Method & R@1 R@10 R@100 MRR \\   \\ BM25  & 39.1 & 69.1 & 86.2 & 48.6 \\ DocT5Query  & 46.7 & 76.5 & 90.4 & 56.2 \\   \\ ANCE  & 45.6 & 75.7 & 89.6 & 55.6 \\ Sentence-T5\({}^{}\) & 41.8 & 75.4 & 91.2 & 52.8 \\   \\ GENRE\({}^{}\) & 35.6 & 57.6 & 79.1 & 42.3 \\ Ulron-URL  & 29.6 & 67.8 & - & 40.0 \\ Ulron-PQ  & 31.6 & 73.1 & - & 45.4 \\ Ulron-Atomic  & 32.8 & 74.1 & - & 46.9 \\
**Ours** & **47.9** & **79.8** & **91.6** & **58.1** \\   

Table 2: **Results on MS MARCO. The results of the methods marked with \({}^{}\) are from our own re-implementation, other results are cited from the original paper or implemented using official code. The best results are indicated in boldface.**

   Method & Arg & Covid & NFC & SciF & SciD & FiQA & **Avg.** \\   \\ BM25  & 29.1 & 58.9 & 33.5 & 67.4 & 14.8 & 23.6 & 37.8 \\ DocT5Query  & 34.9 & 71.3 & 32.8 & 67.5 & 16.2 & 29.1 & 41.9 \\   \\ ST5 GPL\({}^{}\) & 32.1 & 74.4 & 30.1 & 58.6 & 12.7 & 26.0 & 39.0 \\ Contierer  & 40.0 & 68.8 & 33.5 & 61.4 & 16.3 & 30.7 & 41.8 \\   \\ GENRE\({}^{}\) & 42.5 & 14.7 & 20.0 & 42.3 & 6.8 & 11.6 & 30.0 \\
**Ours** & 34.3 & 71.8 & 31.6 & 63.9 & 14.9 & 30.2 & 41.1 \\   

Table 3: **nDCG@10 results on BEIR. The results of the methods marked with \({}^{}\) are from our own re-implementation, other results are cited from the original paper or implemented using official code. ST5 GPL denotes Sentence-T5 trained on GPL datasets .**

### Qualitative analysis

Figure 4 (left) illustrates the document content (title) and the corresponding docid generated by GenRet on the NQ320K dataset. We observe that documents with more similar docids tend to have more relevant content. For example, documents with docids starting with 338-173 are related to Email, such as _Email marketing_, _Mail merge_, and _Email address_, while documents with docids starting with 338 relate to information exchange methods (e.g., _Business letter_, _US Postal Service_, and _Postage stamps_), representing a more generalized semantics than Email alone.

Figure 4 (right) illustrates a word cloud of documents grouped by docid prefixes. It is evident that documents within the same group are semantically related. For example, major words for documents with the docid prefix 338 are _mail_ and _stamp_. When a second-level docid 173 is added, the corresponding documents become more specifically related to _email_. With the addition of a third-level docid 1, the document group becomes specifically associated with _email marketing_ (docid: 338-173-1). Similar patterns can be observed in the other three cases. The case study shows that there is a hierarchical semantic structure within the learned docids.

Figure 5 in Appendix E visualizes the codebook embedding and document embedding. We see that the codebook embedding appears to distribute uniformly within the document representation space, producing meaningful clusters when documents are categorized by docids. We also find that the uniformity of the embedding distribution decreases with increasing docid-length. This may be due to the fact that uniform segmentation is more challenging as the semantic granularity becomes finer.

## 6 Related work

**Sparse and dense retrieval.** Traditional sparse retrieval calculates the document score using term matching metrics such as TF-IDF , query likelihood , or BM25 . Sparse retrieval is widely used in practice due to its efficiency, but often suffers from the lexical mismatches . Dense

Figure 4: Left: Document titles along with their corresponding docids. It is observed that documents with similar docids tend to have more relevant content. Right: Word cloud representing documents grouped by docid prefixes. This illustrates that different positions of the docid correspond to different levels of information, and the semantics within each cluster are closely related.

Figure 3: Left: Docid distribution on NQ320K. The id are sorted by the assigned frequency. Right: Ablation study on NQ320K.

retrieval (DR) addresses this by presenting queries and documents in dense vectors and calculating their similarities with the inner product or cosine similarity . Various techniques have been proposed to improve DR models, such as hard negative mining [42; 26], late interaction [14; 34], knowledge distillation [10; 19], and pre-training [30; 23; 11]. Despite their success, DR approaches have several limitations [5; 21]: (i) DR models employ an index-retrieval pipeline with a fixed search procedure (MIPS), making it difficult to optimize the model end-to-end [37; 41]. (ii) Training DR models relies on contrastive learning  to distinguish positives from negatives, which is inconsistent with large LMs training objectives  and fails to fully utilize the capabilities of pre-trained LMs [1; 35].

**Generative retrieval.** Generative retrieval is gaining attention. It retrieves documents by generating their docial using a generative model like T5. Generative retrieval presents an end-to-end solution for document retrieval tasks [37; 21] and allows for better exploitation of the capabilities of large generative LMs . Cao et al.  first propose an autoregressive entity retrieval model to retrieve documents by generating titles. Tay et al.  propose a differentiable search index (DSI) and represent the document as atomic id, naive string, or semantic string. Bevilacqua et al.  suggest using arbitrary spans of a document as docids. Additionally, multiple-stage pre-training [7; 46], query generation [41; 47; 46], contextualized embedding , and continual learning , have been explored in recent studies. Recently, Tang et al.  introduce a query-based docid and the rehearsal-based document indexing to improve DSI. However, existing generative retrieval models have a limitation in that they rely on fixed document tokenization to produce docids, which often fails to capture the semantic information of a document . It is an open question how one should define the docids. To further capture document semantics in the docid, we propose document tokenization learning methods. The semantic docid is generated by the proposed discrete auto-encoding learning scheme in an end-to-end manner. Concurrently, Rajput et al.  propose a RQ-VAE module to produce semantic docids for generative recommender systems. As a comparison, our proposed method jointly models the tokenization and retrieval tasks with shared parameters to better align the model's representation of the two tasks and are optimized in an end-to-end manner.

**Discrete representation learning.** Learning discrete representations using neural networks is an important research area in machine learning. For images, Rolfe  proposes the discrete variational autoencoder, and VQ-VAE  learns quantized representations via vector quantization. Dall-E  uses an autoregressive model to generate discrete image representation for text-to-image generation. Recently, representation learning has attracted considerable attention in NLP tasks, for tasks such as machine translation , dialogue generation , and text classification [12; 43]. For document retrieval, RepCONC  uses a discrete representation learning method based on constrained clustering for vector compression. We propose a document tokenization learning method for generative retrieval, which captures the autoregressive nature of docids by progressive training and enhances the diversity of docids by diverse clustering techniques.

## 7 Conclusions

This paper has proposed a document tokenization learning method for generative retrieval, named GenRet. The proposed method learns to tokenize documents into short discrete representations (i.e., docids) via a discrete auto-encoding approach, which ensures the semantics of the generated docids. A progressive training method and two diverse clustering techniques have been proposed to enhance the model's training. Empirical results on various document retrieval datasets have demonstrated the effectiveness of the proposed method. Especially, GenRet achieves outperformance on unseen documents and can be well generalized to multiple retrieval tasks.

The limitations of this work include experiments only on moderately sized datasets like NQ320K. The employed data are in sufficient quantity to validate the effectiveness of the proposed method, but application to larger-scale data may require more model parameters and computational resources. Another aspect for improvement is the generalization of the model to unoptimized document collections in different types or domains, as compared to continuous embedding approaches. We recognize this as an important research question for generative retrieval but believe that addressing this is beyond the scope of this paper. In future work, we would like to extend the approach to large document collections. We also plan to explore generative pre-training for document tokenization using large-scale language models. Additionally, we intend to investigate the dynamic adaptation of docid prefixes for progressive training.