# for**interval\(=\{\text{interval\_birth\_corners},\text{interval\_death\_corners}\}\)_in_\(M\)do

A Framework for Fast and Stable Representations of Multiparameter Persistent Homology Decompositions

David Loiseaux

DataShape

Centre Inria d'Universite Cote d'Azur

Biot, France

Mathieu Carriere

DataShape

Centre Inria d'Universite Cote d'Azur

Biot, France

Andrew J. Blumberg

Irving Institute for Cancer Dynamics

Columbia University

New-York, NY, USA

###### Abstract

Topological data analysis (TDA) is an area of data science that focuses on using invariants from algebraic topology to provide multiscale shape descriptors for geometric data sets, such as graphs and point clouds. One of the most important such descriptors is _persistent homology_, which encodes the change in shape as a filtration parameter changes; a typical parameter is the feature scale. For many data sets, it is useful to simultaneously vary multiple filtration parameters, for example feature scale and density. While the theoretical properties of single parameter persistent homology are well understood, less is known about the multiparameter case. In particular, a central question is the problem of representing multiparameter persistent homology by elements of a vector space for integration with standard machine learning algorithms. Existing approaches to this problem either ignore most of the multiparameter information to reduce to the one-parameter case or are heuristic and potentially unstable in the face of noise. In this article, we introduce a new general representation framework that leverages recent results on _decompositions_ of multiparameter persistent homology. This framework is rich in information, fast to compute, and encompasses previous approaches. Moreover, we establish theoretical stability guarantees under this framework as well as efficient algorithms for practical computation, making this framework an applicable and versatile tool for analyzing geometric data. We validate our stability results and algorithms with numerical experiments that demonstrate statistical convergence, prediction accuracy, and fast running times on several real data sets.

## 1 Introduction

Topological Data Analysis (TDA)  is a methodology for analyzing data sets using multiscale shape descriptors coming from algebraic topology. There has been intense interest in the field in the last decade, since topological features promise to allow practitioners to compute and encode information that classical approaches do not capture. Moreover, TDA rests on solid theoretical grounds, with guarantees accompanying many of its methods and descriptors. TDA has proved useful in a wide variety of application areas, including computer graphics , computational biology , and material science , among many others.

The main tool of TDA is _persistent homology_. In its most standard form, one is given a finite metric space \(X\) (e.g., a finite set of points and their pairwise distances) and a continuous function \(f:X\). This function usually represents a parameter of interest (such as, e.g., scale or density for point clouds, marker genes for single-cell data, etc), and the goal of persistent homology is to characterize the topological variations of this function on the data at all possible scales. Of course, the idea of considering multiscale representations of geometric data is not new [14; 32; 41]; the contribution of persistent homology is to obtain a novel and theoretically tractable multiscale shape descriptor. More formally, persistent homology is achieved by computing the so-called _persistence barcode_ of \(f\), which is obtained by looking at all sublevel sets of the form \(\{f^{-1}((-,])\}_{}\), also called _filtration induced by \(f\)_, and by computing a _decomposition_ of this filtration, that is, by recording the appearances and disappearances of topological features (connected components, loops, enclosed spheres, etc) in these sets. When such a feature appears (resp. disappears), e.g., in a sublevel set \(f^{-1}((-,_{b}])\), we call the corresponding threshold \(_{b}\) (resp. \(_{d}\)) the _birth time_ (resp. _death time_) of the topological feature, and we summarize this information in a set of intervals, or bars, called the persistence barcode \(D(f):=\{(_{b},_{d})\}_{ A} \{\}\). Moreover, the bar length \(_{d}-_{b}\) often serves as a proxy for the statistical significance of the corresponding feature.

However, an inherent limitation of the formulation of persistent homology is that it can handle only a single filtration parameter \(f\). However, in practice it is common that one has to deal with multiple parameters. This translates into multiple filtration functions: a standard example is when one aims at obtaining meaningful topological representation of a noisy point cloud. In this case, both feature scale and density functions are necessary (see Appendix A). An extension of persistent homology to several filtration functions is called _multiparameter_ persistent homology [3; 9], and studies the topological variations of a continuous _multiparameter_ function \(f:X^{n}\) with \(n^{*}\). This setting is notoriously difficult to analyze theoretically as there is no result ensuring the existence of an analogue of persistence barcodes, i.e., a decomposition into subsets of \(^{n}\), each representing the lifespan of a topological feature.

Still, it remains possible to define weaker topological invariants in this setting. The most common one is the so-called _rank invariant_ (as well as its variations, such as the generalized rank invariant , and its decompositions, such as the signed barcodes ), which describes how the topological features associated to any pair of sublevel sets \(\{x X:f(x)\}\) and \(\{x X:f(x)\}\) such that \(\) (w.r.t. the partial order in \(^{n}\)), are connected. The rank invariant is a construction in abstract algebra, and so the task of finding appropriate _representations_ of this invariant, i.e., embeddings into Hilbert spaces, is critical. Hence, a number of such representations have been defined, which first approximate the rank invariant by computing persistence barcodes from several linear combinations of filtrations, a procedure often referenced as the _fibered barcode_ (see Appendix E), and then aggregate known single-parameter representations for them [17; 18; 39]. Adequate representations of the generalized rank invariant have also been investigated recently for \(n=2\).

However, the rank invariant, and its associated representations, are known to be much less informative than decompositions (when they exist): many functions have different decompositions yet the same rank invariants. Therefore, the aforementioned representations can encode only limited multiparameter topological information. Instead, in this work, we focus on _candidate decompositions_ of the function, in order to create descriptors that are strictly more powerful than the rank invariant. Indeed, while there is no general decomposition theorem, there is recent work that constructs candidate decompositions in terms of simple pieces [1; 7; 29] that always exist but do not necessarily suffice to reconstruct all of the multiparameter information. Nonetheless, they are strictly more informative than the rank invariant under mild conditions, are stable, and approximate the true decomposition when it exists1. For instance, in Figure 2, we present a bifiltration of a noisy point cloud with scale and density **(left)**, and a corresponding candidate decomposition comprised of subsets of \(^{2}\), each representing a topological feature **(middle)**. One can see that there is a large green subset in the decomposition that represents the circle formed by the points that are not outliers (also highlighted in green in the bifiltration).

Unfortunately, while more informative, candidate decompositions suffer from the same problem than the rank invariant; they also need appropriate representations in order to be processed by standard data science methods. In this work, we bridge this gap by providing new representations designed for candidate decompositions. See Figure 1 for a summarizing figure.

Contributions.Our contributions in this work are listed below:

* We provide a general framework that parametrizes representations of multiparameter persistent homology decompositions (Definition 1) and which encompasses previous approaches in the literature. These representations take the form of a parametrized family of continuous functions on \(^{n}\) that can be binned into images for visualization and data science.
* We identify parameters in this framework that result in representations that have stability guarantees while still encoding more information than the rank invariant (see Theorem 1).
* We illustrate the performance of our framework with numerical experiments: (1) We demonstrate the practical consequences of the stability theorem by measuring the statistical convergence of our representations. (2) We achieve the best performance with the lowest runtime on several classification tasks on public data sets (see Sections 4.1 and 4.2).

Related work.Closely related to our method is the recent contribution , which also proposes a representation for decompositions. However, their approach, while being efficient in practice, is a heuristic with no corresponding mathematical guarantees. In particular, it is known to be unstable: similar decompositions can lead to very different representations, as shown in Appendix B. Our approach can be understood as a subsequent generalization of the work of , with new mathematical guarantees that allow to derive, e.g., statistical rates of convergence.

Outline.Our work is organized as follows. In Section 2, we recall the basics of multiparameter persistent homology. Next, in Section 3 we present our general framework and state our associated stability result. Finally, we showcase the numerical performances of our representations in Section 4, and we conclude in Section 5.

Figure 1: Common pipelines for the use of multiparameter persistent homology in data scienceâ€”our work provides new contributions to the arrow highlighted in red.

Figure 2: **(left)** Bi-filtration of a noisy point cloud induced by both feature scale (using unions of balls with increasing radii) and sublevel sets of codensity. The cycle highlighted in the green zone can be detected as a large subset in the corresponding candidate decomposition computed by the MMA method **(middle)**, and in our representation of it **(right)**.

Background

In this section, we briefly recall the basics of single and multiparameter persistent homology, and refer the reader to Appendix C, Appendix D, and  for a more complete treatment.

Persistent homology.The basic brick of persistent homology is a _filtered topological space_\(X\), by which we mean a topological space \(X\) together with a function \(f X\) (for instance, in Figure 5, \(X=^{2}\) and \(f=f_{P}\)). Then, given \(>0\), we call \(F():=f^{-1}((-,]) X\) the _sublevel set of \(f\) at level \(\)_. Given levels \(_{1}_{N}\), the corresponding sublevel sets are nested w.r.t. inclusion, i.e., one has \(F(_{1}) F(_{2}) F(_{i})  F(_{N})\). This system is an example of _filtration_ of \(X\), where a filtration is generally defined as a sequence of nested subspaces \(X_{1} X_{i} X\). Then, the core idea of persistent homology is to apply the \(k\)th _homology functor_\(H_{k}\) on each \(F(_{i})\). We do not define the homology functor explicitly here, but simply recall that each \(H_{k}(F(_{i}))\) is a vector space, whose basis elements represent the \(k\)th dimensional topological features of \(F(_{i})\) (connected components for \(k=0\), loops for \(k=1\), spheres for \(k=2\), etc). Moreover, the inclusions \(F(_{i}) F(_{i+1})\) translate into linear maps \(H_{k}(F(_{i})) H_{k}(F(_{i+1}))\), which connect the features of \(F(_{i})\) and \(F(_{i+1})\) together. This allows to keep track of the topological features in the filtration, and record their levels, often called times, of appearance and disappearance. More formally, such a sequence of vector spaces connected with linear maps \(=H_{*}(F(_{1})) H_{*}(F(_{N}))\) is called a _persistence module_, and the standard decomposition theorem [15, Theorem 2.8] states that this module can always be decomposed as \(=_{i=1}^{m}[_{b_{i}},_{d_{i}}]\), where \([_{b_{i}},_{d_{i}}]\) stands for a module of dimension 1 (i.e., that represents a single topological feature) between \(_{b_{i}}\) and \(_{d_{i}}\), and dimension 0 (i.e., that represents no feature) elsewhere. It is thus convenient to summarize such a module with its _persistence barcode_\(D()=\{[_{b_{i}},_{d_{i}}]\}_{1 i m}\). Note that in practice, one is only given a sampling of the topological space \(X\), which is usually unknown. In that case, persistence barcodes are computed using combinatorial models of \(X\) computed from the data, called _simplicial complexes_. See Appendix C.

Multiparameter persistent homology.The persistence modules defined above extend straightforwardly when there are multiple filtration functions. An \(n\)-filtration, or multifiltration, induced by a function \(f:X^{n}\), is the family of sublevel sets \(F=\{F()\}_{^{n}}\), where \(F():=\{x X:f(x)\}\) and \(\) denotes the partial order of \(^{n}\). Again, applying the homology functor \(H_{k}\) on the multifiltration \(F\) induces a _multiparameter persistence module_\(\). However, contrary to the single-parameter case, the algebraic structure of such a module is very intricate, and there is no general decomposition into modules of dimension at most 1, and thus no analogue of the persistence barcode. Instead, the _rank invariant_ has been introduced as a weaker invariant: it is defined, for a module \(\), as the function \(:(,)(() ())\) for any \(\), but is also known to miss a lot of structural properties of \(\). To remedy this, several methods have been developed to compute _candidate decompositions_ for \(\), where a candidate decomposition is a module \(}\) that can be decomposed as \(}_{i=1}^{m}M_{i}\), where each \(M_{i}\) is an _interval module_, i.e., its dimension is at most 1, and its support \((M_{i}):=\{^{n}:(M_{i}( ))=1\}\) is an interval of \(^{n}\) (see Appendix D). In particular, when \(\) does decompose into intervals, candidate decompositions must agree with the true decomposition. One also often asks candidate decompositions to preserve the rank invariant.

Distances.Finally, multiparameter persistence modules can be compared with two standard distances: the _interleaving_ and _bottleneck_ (or \(^{}\)) distances. Their explicit definitions are technical and not necessary for our main exposition, so we refer the reader to, e.g., [3, Sections 6.1, 6.4] and Appendix D for more details. The _stability theorem_[27, Theorem 5.3] states that multiparameter persistence modules are stable: \(d_{}(,^{})\|f-f^{}\| _{},\) where \(f\) and \(f^{}\) are continuous multiparameter functions associated to \(\) and \(^{}\) respectively.

## 3 T-CDR: a template for representations of candidate decompositions

Even though candidate decompositions of multiparameter persistence modules are known to encode useful data information, their algebraic definitions make them not suitable for subsequent data science and machine learning purposes. Hence, in this section, we introduce the Template CandidateDecomposition Representation (T-CDR): a general framework and template system for representations of candidate decompositions, i.e., maps defined on the space of candidate decompositions and taking values in an (implicit or explicit) Hilbert space.

### T-CDR definition

Notations.In this article, by a slight abuse of notation, we will make no difference in the notations between an interval module and its support, and we will denote the restriction of an interval support \(M\) to a given line \(\) as \(M_{}\).

**Definition 1**.: Let \(=_{i=1}^{m}M_{i}\) be a candidate decomposition, and let \(\) be the space of interval modules. The _Template Candidate Decomposition Representation_ (T-CDR) of \(\) is:

\[V_{,w,}()=(\{w(M_{i})(M_{i})\}_{ i=1}^{m}),\] (1)

where \(\) is a permutation invariant operation (sum, max, min, mean, etc), \(w:\) is a weight function, and \(:\) sends any interval module to a vector in a Hilbert space \(\).

The general definition of T-CDR is inspired from a similar framework that was introduced for single-parameter persistence with the automatic representation method _PersLay_.

Relation to previous work.Interestingly, whenever applied on candidate decompositions that preserve the rank invariant, specific choices of \(\), \(w\) and \(\) reproduce previous representations:

* Using \(w:M_{i} 1\), \(:M_{i}\{^{n}&\\ x&(x,M_{i}_{_{a}}).\) and \(=k\)th maximum, where \(l_{x}\) is the diagonal line crossing \(x\), and \((,)\) denotes the tent function associated to any segment \(^{n}\), induces the \(k\)th multiparameter persistence landscape (MPL) .
* Using \(w:M_{i} 1\), \(:M_{i}\{^{n}^{ n}&^{d}\\ p,q& w^{}(M_{i}[p,q])^{}(M_{i}[p,q])\\ .\) and \(=^{}\), where \(^{},w^{}\) and \(^{}\) are the parameters of any persistence diagram representation from Perslay, induces the multiparameter persistence kernel (MPK) .
* Using \(w:M_{i}(M_{i})\), \(:M_{i}\{^{n}&\\ x&(-_{ L}d(x,M_{i}_{})^{2}/^{2})\\ .\) and \(=\), where \(L\) is a set of (pre-defined) diagonal lines, induces the multiparameter persistence image (MPI) .

Recall that the first two approaches are built from fibered barcodes and rank invariants, and that it is easy to find persistence modules that are different yet share the same rank invariant (see [38, Figure 3]). On the other hand, the third approach uses more information about the candidate decomposition, but is known to be unstable (see Appendix B). Hence, in the next section, we focus on specific choices for the T-CDR parameters that induce stable yet informative representations.

### Metric properties

In this section, we study specific parameters for T-CDR (see Definition 1) that induce representations with associated robustness properties. We call this subset of representations _Stable Candidate Decomposition Representations_ (S-CDR), and define them below.

**Definition 2**.: The S-CDR parameters are:

1. the weight function \(w:M\{>0: y^{n}\) s.t. \(_{y,}(M)\}\),where \(_{y,}\) is the segment between \(y-[1,,1]\) and \(y+[1,,1]\),
2. the individual interval representations \(_{}(M):^{n}\): (a) \(_{}(M)(x)=w((M) R_{x, })\), (b) \(_{}(M)(x)=}( (M) R_{x,})\), (c) \(_{}(M)(x)=}_{x^{}, ^{}}\{(R_{x^{},^{}}):R_{ x^{},^{}}(M ) R_{x,}\}\), where \(R_{x,}\) is the hypersquare \(\{y^{n}:x- y x+\} ^{n}\), \(:=[1,,1]^{n}\) for any \(>0\), and \(\) denotes the volume of a set in \(^{n}\).

3. the permutation invariant operators \(=\) and \(=\).

Intuitively, the S-CDR weight function is the length of the largest diagonal segment one can fit inside \((M)\), and the S-CDR interval representations (a), (b) and (c) are the largest normalized diagonal length, volume, and hypersquare volume that one can fit inside \((M) R_{x,}\), respectively. These S-CDR interval representations allow for some trade-off between computational cost and the amount of information that is kept: (a) and (c) are very easy to compute, but (b) encodes more information about interval shapes. See Figure 2 (right) for visualizations.

Equipped with these S-CDR parameters, we can now define the two following S-CDRs, that can be applied on any candidate decomposition \(=_{i=1}^{m}M_{i}\):

\[V_{p,}():=_{i=1}^{m})^{p}}{_{j=1}^{m}w(M_ {j})^{p}}_{}(M_{i}),)} V_{,}():=_{1 i m}_{}(M_{i}).\] (3)

Stability.The main motivation for introducing S-CDR parameters is that the corresponding S-CDRs are stable in the interleaving and bottleneck distances, as stated in the following theorem.

**Theorem 1**.: _Let \(=_{i=1}^{m}M_{i}\) and \(^{}=_{j=1}^{m^{}}M_{j}^{}\) be two candidate decompositions. Assume that we have \(_{i}w(M_{i}),}_{j}w(M_{j}^{}) C,\) for some \(C>0\). Then for any \(>0\), one has_

\[ V_{0,}()-V_{0,}(^{ })_{}  2(d_{}(,^{}))/,\] (4) \[ V_{1,}()-V_{1,}(^{ })_{} [4+](d_{}(,^{}))/,\] (5) \[ V_{,}()-V_{,}( ^{})_{} (d_{}(,^{})) /,\] (6)

_where \(\) stands for minimum._

A proof of Theorem 1 can be found in Appendix F.

These results are the main theoretical contribution in this work, as the only other decomposition-based representation in the literature  has no such guarantees. The other representations [17; 18; 39; 42] enjoy similar guarantees than ours, but are computed from the rank invariant and do not exploit the information contained in decompositions. Theorem 1 shows that S-CDRs bring the best of both worlds: these representations are richer than the rank invariant and stable at the same time. We also provide an additional stability result with a similar, yet more complicated representation in Appendix G, whose upper bound does not involve taking minimum.

**Remark 1**.: S-CDRs are injective representations: if the support of two interval modules are different, then their corresponding S-CDRs (evaluated on a point that belongs to the support of one interval but not on the support of the other) will differ, provided that \(\) is sufficiently small.

## 4 Numerical Experiments

In this section, we illustrate the efficiency of our S-CDRs with numerical experiments. First, we explore the stability theorem in Section 4.1 by studying the convergence rates, both theoretically and empirically, of S-CDRs on various data sets. Then, we showcase the efficiency of S-CDRs on classification tasks in Section 4.2, and we investigate their running times in Section 4.3. Our code for computing S-CDRs is based on the MMA and Gudhi libraries for computing candidate decompositions2. It is publicly available at https://github.com/DavidLapous/multipers and will be merged as a module of the Gudhi library. We also provide pseudo-code in Appendix H.

### Convergence rates

In this section, we study the convergence rate of S-CDRs with respect to the number of sampled points, when computed from specific bifiltrations. Similar to the single parameter persistence setting , these rates are derived from Theorem 1. Indeed, since concentration inequalities for multiparameterpersistence modules have already been described in the literature, these concentration inequalities can transfer to our representations. Note that while Equations (7) and (8), which provide such rates, are stated for the S-CDR in (3), they also hold for the S-CDR in (2).

Measure bilitration.Let \(\) be a compactly supported probability measure of \(^{D}\), and let \(_{n}\) be the discrete measure associated to a sampling of \(n\) points from \(\). The _measure bilitration_ associated to \(\) and \(_{n}\) is defined as \(_{r,t}^{}:=\{x^{D}:(B(x,r)) t\}\), where \(B(x,r)\) denotes the Euclidean ball centered on \(x\) with radius \(r\). Now, let \(\) and \(_{n}\) be the multiparameter persistence modules obtained from applying the homology functor on top of the measure bilitrations \(^{}\) and \(^{_{n}}\). These modules are known to enjoy the following stability result [2, Theorem 3.1, Proposition 2.23 (i)]: \(d_{}(,_{n}) d_{}(,_{n})(d _{}^{p}(,_{n})^{},d_{}^{p}(,_{n})^ {}),\) where \(d_{}^{p}\) and \(d_{}\) stand for the \(p\)-Wasserstein and Prokhorov distances between probability measures. Combining these inequalities with Theorem 1, then taking expectations and applying the concentration inequalities of the Wasserstein distance (see [26, Theorem 3.1] and [21, Theorem 1]) lead to:

\[[\|V_{,}()-V_{,}( _{n})\|_{}](c_{p,q}(|X|^{ q})n^{-()-}^{ /q}n)^{},\] (7)

where \(\) stands for maximum, \(=2\) if \(2p=q=d\), \(=1\) if \(d 2p\) and \(q=dp/(d-p) 2p\) or \(q>d=2p\) and \(=0\) otherwise, \(c_{p,q}\) is a constant that depends on \(p\) and \(q\), and \(X\) is a random variable of law \(\).

Cech complex and density.A limitation of the measure bilitration is that it can be difficult to compute. Hence, we now focus on another, easier to compute bilitration. Let \(X\) be a smooth compact \(d\)-submanifold of \(^{D}\) (\(d D\)), and \(\) be a measure on \(X\) with density \(f\) with respect to the uniform measure on \(X\). We now define the bilitration \(^{C,f}\) with:

\[_{u,v}^{C,f}:=}(u) f^{-1}([v, ))=\{x^{D}:d(x,X) u,f(x) v\}.\]

Moreover, given a set \(X_{n}\) of \(n\) points sampled from \(\), we also consider the approximate bilitration \(^{C,f_{n}}\), where \(f_{n} X\) is an estimation of \(f\) (such as, e.g., a kernel density estimator). Let \(\) and \(_{n}\) be the multiparameter persistence modules associated to \(^{C,f}\) and \(^{C,f_{n}}\). Then, the stability of the interleaving distance [27, Theorem 5.3] ensures:

\[d_{}(,_{n})\|f-f_{n}\|_{}  d_{H}(X,X_{n}),\]

where \(d_{H}\) stands for the Hausdorff distance. Moreover, concentration inequalities for the Hausdorff distance and kernel density estimators are also available in the literature (see [16, Theorem 4] and [23, Corollary 15]). More precisely, when the density \(f\) is \(L\)-Lipschitz and bounded from above and from below, i.e., when \(0<f_{} f f_{}<\), and when \(f_{n}\) is a kernel density estimator of \(f\) with associated kernel \(k\), one has:

\[(d_{H}(X,X_{n}))()^{} (\|f-f_{n}\|_{}) Lh_{n}+)}{nh_{n}^{d}}},\]

where \(h_{n}\) is the (adaptive) bandwidth of the kernel \(k\). In particular, if \(\) is a measure comparable to the uniform measure of a \(d=2\)-manifold, then for any stationary sequence \(h_{n}:=h>0\), and considering a Gaussian kernel \(k\), one has:

\[[\|V_{,}()-V_{,}( _{n})\|_{}]}+Lh.\] (8)

Empirical convergence rates.Now that we have established the theoretical convergence rates of S-CDRs, we estimate and validate them empirically on data sets. We will first study a synthetic data set and then a real data set of point clouds obtained with immunohistochemistry. We also illustrate how the stability of S-CDRs (stated in Theorem 1) is critical for obtaining such convergence in Appendix B, where we show that our main competitor, the multiparameter persistence image , is unstable and thus cannot achieve convergence, both theoretically and numerically.

_Annulus with non-uniform density._ In this synthetic example, we generate an annulus of 25,000 points in \(^{2}\) with a non-uniform density, displayed in Figure 2(a). Then, we compute the bilitration \(^{C,f_{n}}\) corresponding to the Alpha filtration and the sublevel set filtration of a kernel density estimator, with bandwidth parameter \(h=0.1\), on the complete Alpha simplicial complex. Finally, we compute the candidate decompositions and associated S-CDRs of the associated multiparameter module (in homology dimension 1), and their normalized distances to the target representation, using either \(_{2}^{2}\) or \(_{}\). The corresponding distances for various number of sample points are displayed in log-log plots in Figure 2(b). One can see that the empirical rate is roughly consistent with the theoretical one (\(-1/2\) for \(_{}\) and \(-1\) for \(_{2}\)), even when \(p\) (in which case our S-CDRs are stable for \(d_{}\) but theoretically not for \(d_{}\)).

_Immunohistochemistry data._ In our second experiment, we consider a point cloud representing cells, taken from , see Figure 3(a). These cells are given with biological markers, which are typically used to assess, e.g., cell types and functions. In this experiment, we first triangulate the point cloud by placing a \(100 100\) grid on top of it. Then, we filter this grid using the sublevel set filtrations of kernel density estimators (with Gaussian kernel and bandwidth \(h=1\)) associated to the CD8 and CD68 biological markers for immune cells. Finally, we compute the associated candidate decompositions of the multiparameter modules in homology dimensions 0 and 1, and we compute and concatenate their corresponding S-CDRs. Similar to the previous experiment, the theoretical convergence rate of our representations is upper bounded by the one for kernel density estimators with the \(\)-norm. The convergence rates are displayed in Figure 3(b). Again, one can see that the observed and theoretical convergence rates are consistent.

### Classification

In this section, we illustrate the efficiency of S-CDRs by using them for classification purposes. We show that they perform comparably or better than existing topological representations as well as standard baselines on several UCR benchmark data sets, graph data sets, and on the immunohistochemistry data set. Concerning UCR, we work with point clouds obtained from time delay embedding applied on the UCR time series, following the procedure of , and we produce S-CDRs with bifiltrations coming from combining either the Rips filtration with sublevel sets of a kernel density estimator (as in Section 4.1), or the Alpha filtration with the sublevel sets of the distance-to-measure

Figure 4: Convergence rate of immunohistochemistry data set.

Figure 3: Convergence rate of synthetic data set.

with parameter \(m=0.1\) (as in  and the baselines therein). Concerning graph datasets, we produce S-CDRs by filtering the graphs themselves directly using Heat Kernel Signature with parameter \(t=10\), Ricci curvature and node degree (similarly to what is used in the literature [11; 22; 45]).

In all tasks, every point cloud or graph has a label (corresponding to the type of its cells in the immunohistochemistry data set, and to pre-defined labels in the UCR and graph data sets), and our goal is to check whether we can predict these labels by training classifiers on the corresponding S-CDRs.

For point clouds (immunohistochemistry and UCR), we compare the performances of our S-CDRs (evaluated on a \(50 50\) grid) to the one of the multiparameter persistence landscape (MPL) , kernel (MPK)  and images (MPI) , as well as their single-parameter counterparts (P-L, P-I and PSS-K) 3. We also compare to some non-topological baselines: we used the standard Ripley function evaluated on 100 evenly spaced samples in \(\) for the immunohistochemistry data set, and k-NN classifiers with three difference distances for the UCR time series (denoted by B1, B2, B3), as suggested in . For gaps, we compare S-CDRs to the Euler characteristic based multiparameter persistence methods ECP, RT, and HTn, introduced in . In order to also include non topological baselines, we also compare against the state-of-the-art graph classification methods RetGK , FGSD , and GIN .

All scores on the immunohistochemistry data set were computed after cross-validating a few classifiers (random forests, support vector machines and xgboost, with their default Scikit-Learn parameters) with 5 folds. For the time series data, our accuracy scores were obtained after also cross-validating the following S-CDR parameters; \(p\{0,1\}\), \(\{,\}\), \(\{0.01,0.1,0.5,1\}\), \(h\{0.1,0.5,1,1.5\}\) with homology dimensions \(0\) and \(1\), and the following bandwidth parameters for kernel density esimation: \(b\{0.1\%,1\%,10\%,20\%,30\%\}\), which are percentages of the diameters of the point clouds. with 5 folds. Parameters and results on graph data sets were cross-validated and averaged over 10 folds, following the pipelines of . All results can be found in Table 1 (immunohistochemistry and UCR--UCR acronyms are provided in Appendix I) and Table 2. Bold indicates best accuracy and underline indicates best accuracy among topological methods. Note that there are no variances for UCR data sets since pre-defined train/test splits were provided. One can see that S-CDR almost always outperform topological baselines and are comparable to the standard baselines on the UCR benchmarks. Most notably, S-CDRs radically outperform the standard baseline and competing topological measures on the immunohistochemistry data set. For graph data sets, results are competitive with both topological and non-topological baselines; S-CDRs perform even slightly better on COX2.

  Dataset & B1 & B2 & B3 & PSS-K & P-I & P-L & MPK & MPL & MPI & S-CDR (kips + KDE) & S-CDR (Alpha + DIM) \\  DPQAG & 62.6 & 62.6 & **77.0** & 76.9 & 69.8 & 70.5 & 67.6 & 70.5 & 71.9 & 71.9 & 71.9 \\ DPOC & 71.7 & 72.5 & 71.7 & 47.5 & 67.4 & 66.3 & **74.6** & 69.6 & 71.7 & 73.8 & **74.6** \\ PPOAG & 78.5 & 78.5 & 80.5 & 75.9 & 82.0 & 78.0 & 78.0 & 78.5 & 81.0 & 81.9 & **84.9** \\ PPOC & 80.8 & 79.0 & 78.4 & 78.4 & 72.2 & 72.5 & 78.7 & 78.7 & 81.8 & 79.4 & **83.2** \\ PPTW & 70.7 & 75.6 & 75.6 & 61.4 & 72.2 & 73.7 & **79.5** & 73.2 & 76.1 & 75.6 & 75.1 \\ TPD & **95.5** & **95.5** & 95.0 & - & 64.7 & 61.1 & 80.7 & 78.6 & 71.9 & 81.2 & 77.2 \\ GP & 91.3 & 91.3 & 90.7 & 90.6 & 84.7 & 80.0 & 88.7 & 94.0 & 90.7 & **96.3** & 92.7 \\ GPAS & 89.9 & **96.5** & 91.8 & - & 84.5 & 87.0 & 93.0 & 85.1 & 90.5 & 88.0 & 93.7 \\ GPMVF & 97.5 & 97.5 & **99.7** & - & 88.3 & 87.3 & 96.8 & 88.3 & 95.9 & 95.3 & 95.9 \\ PC & **93.3** & 92.2 & 87.8 & - & 83.4 & 76.7 & 85.6 & 84.4 & 86.7 & 93.1 & 90.0 \\    &  &  &  &  \\  Immuno &  &  &  &  \\  

Table 1: Accuracy scores for UCR and immunohistochemistry data sets.

### Running time comparisons

In this section, we provide running time comparisons between S-CDRs and the MPI and MPL representations, in which we measured the time needed to compute all the train and test S-CDRs and baselines of the previous data sets, averaged over the folds (again, note that since UCR data sets already provide the train/test splits, there is no variance in the corresponding results). All representations are evaluated on grids of sizes \(50 50\) and \(100 100\), and we provide the maximum running time over \(p\{0,1,\}\). All computations were done using a Ryzen 4800 laptop CPU, with 16GB of RAM. We provide results in Table 3, where it can be seen that S-CDRs (computed on the pinched annulus and immunohistochemistry data sets) can be computed much faster than the other representations, by a factor of at least 25. As for UCR data sets, which contain only small time series and corresponding point clouds, it can still be observed that S-CDRs can be computed faster than the baselines. Interestingly, this sparse and fast implementation based on corners can also be used to improve on the running time of the multiparameter persistence landscapes (MPL), as one can see from Algorithm 4 in Appendix H (which retrieves the persistence barcode of a multiparameter persistence module along a given line; this is enough to compute the MPL) and from Table 3.

## 5 Conclusion

In this article, we study the general question of representing decompositions of multiparameter persistence modules in Topological Data Analysis. We first introduce T-CDR: a general template framework including specific representations (called S-CDR) that are provably stable. Our experiments show that S-CDR is superior to the state of the art.

**Limitations.** (1) Our current T-CDR parameter selection is currently done through cross-validation, which can be very time consuming and limits the number of parameters to choose from. (2) Our classification experiments were mostly illustrative. In particular, it would be useful to investigate more thoroughly on the influence of the T-CDR and S-CDR parameters, as well as the number of filtrations, on the classification scores. (3) In order to generate finite-dimensional vectors, we evaluated T-CDR and S-CDR on finite grids, which limited their discriminative powers when fine grids were too costly to compute.

**Future work.** (1) Since T-CDR is similar to the PersLay framework of single parameter persistence  and since, in this work, each of the framework parameter was optimized by a neural network, it is thus natural to investigate whether one can optimize T-CDR parameters in a data-driven way as well, so as to be able to avoid cross-validation. (2) In our numerical applications, we focused on representations computed off of MMA decompositions . In the future, we plan to investigate whether working with other decomposition methods [1; 7] lead to better numerical performance when combined with our representation framework.

  Dataset & RetGK & FGSD & GIN & ECP & RT & HT nD & S-CDR \\  COX2 & 81.4(0.6) & - & - & 80.3(0.4) & 79.7(0.4) & 80.6(0.4) & **82.0(0.2)** \\ DPRR & 81.5(0.9) & - & - & 82.0(0.4) & 81.3(0.4) & **83.1(0.5)** & 81.6(0.2) \\ IMDB-B & 71.9(1.0) & 73.6 & **75.1(5.1)** & 73.3(0.4) & 74.0(0.5) & 74.7(0.5) & 73.5(0.2) \\ IMDB-M & 47.7(0.3) & **52.4** & 52.3(2.8) & 48.7(0.4) & 50.2(0.4) & 49.9(0.4) & 49.5(0.2) \\ MUTAG & 90.3(1.1) & **92.1** & 90(8.8) & 90.0(0.8) & 87.3(0.6) & 89.4(0.7) & 88.4(0.3) \\ PROTEINS & **78.0(0.3)** & 73.4 & 76.2(2.6) & 75.0(0.3) & 75.4(0.4) & 75.4(0.4) & 73.9(0.2) \\  

Table 2: Accuracy scores on graph datasets.

   & Annulus & Immuno & PPTM & GP \\  Ours (S-CDR) & \(250ms(2ms)\) & \(257ms(9.8ms)\) & \(33.0ms(3.99ms)\) & \(45.6ms(3.74ms)\) \\ Ours (MPL) & \(36.9ms(0.8ms)\) & \(65.9ms(0.9ms)\) & \(22.4ms(2.15ms)\) & \(31.8ms(2.95ms)\) \\  MPI (50) & \(6.43s(25ms)\) & \(5.67s(23.3ms)\) & \(65.2ms(12.9ms)\) & \(208ms(16.3ms)\) \\ MPL (50) & \(17s(39ms)\) & \(15.65s(14ms)\) & \(154ms(27.9ms)\) & \(630ms(30.0ms)\) \\  MPI (100) & \(13.13s(125ms)\) & \(11.65s(7.9ms)\) & \(289ms(75.0ms)\) & \(16.95s(77.7ms)\) \\ MPL (100) & \(35s(193ms)\) & \(31.3s(23.3ms)\) & \(843ms(200ms)\) & \(4.43s(186ms)\) \\  

Table 3: Running times for S-CDRs and competitors.

Acknowledgments.The authors would like to thank the area chair and anonymous reviewers for their insightful comments and constructive suggestions. The authors would also like to thank Hannah Schreiber for her great help with the implementation of our method. The authors are grateful to the OPAL infrastructure from Universite Cote d'Azur for providing resources and support. DL was supported by ANR grant 3IA Cote d'Azur (ANR-19-P3IA-0002). MC was supported by ANR grant TopModel (ANR-23-CE23-0014). AJB was partially supported by ONR grant N00014-22-1-2679 and NSF grant DMS-2311338.