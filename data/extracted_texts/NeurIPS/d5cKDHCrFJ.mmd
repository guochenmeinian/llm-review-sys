# EPIC: Effective Prompting for Imbalanced-Class Data Synthesis in Tabular Data Classification via Large Language Models

EPIC: Effective Prompting for Imbalanced-Class Data Synthesis in Tabular Data Classification via Large Language Models

 Jinhee Kim

KAIST

Daejeon, South Korea

seharanul17@kaist.ac.kr

&Taesung Kim1

KAIST

Daejeon, South Korea

zkm1989@kaist.ac.kr

&Jaegul Choo

KAIST

Daejeon, South Korea

jchoo@kaist.ac.kr

###### Abstract

Large language models (LLMs) have demonstrated remarkable in-context learning capabilities across diverse applications. In this work, we explore the effectiveness of LLMs for generating realistic synthetic tabular data, identifying key prompt design elements to optimize performance. We introduce **EPIC**, a novel approach that leverages balanced, grouped data samples and consistent formatting with unique variable mapping to guide LLMs in generating accurate synthetic data across all classes, even for imbalanced datasets. Evaluations on real-world datasets show that EPIC achieves state-of-the-art machine learning classification performance, significantly improving generation efficiency. These findings highlight the effectiveness of EPIC for synthetic tabular data generation, particularly in addressing class imbalance. Our source code for our work is available _here._

## 1 Introduction

Tabular data, consisting of mixed variable types such as numerical and categorical variables, represents a widely applicable and essential data format . It plays a crucial role in enhancing decision-making and efficiency in various real-world applications, such as finance, healthcare, manufacturing, and natural sciences [1; 3; 26]. However, challenges such as data scarcity and class imbalance, particularly for rare yet crucial events, often significantly degrade the performance of machine learning (ML) models, often resulting in reduced accuracy for underrepresented classes. To address these challenges, efficient synthetic data generation methods have been developed to augment tabular datasets. Traditional methods, such as synthetic minority oversampling technique (SMOTE) and its variants [5; 11; 21], focus on generating minority class samples to alleviate class imbalance. Recently, advanced generative models, such as TVAE , CTAB-GAN , and TabDDPM , have shown promising results in producing high-quality synthetic tabular data.

Large language models (LLMs) have also demonstrated significant potential for generating realistic tabular data, effectively handling both numerical and categorical variables [4; 37]. However, these models often require extensive, data-specific fine-tuning, which can increase the risk of overfitting to majority classes or dominant feature values, especially in small and highly imbalanced tabular datasets. An alternative approach involves leveraging the in-context learning capabilities of LLMs. Existing work has shown that LLMs can solve complex reasoning tasks, learn and mimic patterns from input data, and augment textual datasets without parameter updates [13; 20; 34].

However, designing effective prompts to optimize this capability for tabular data generation is challenging, especially for imbalanced datasets. This is because tabular data is not naturally expressed in textual form and involves unique challenges, such as identifying feature correlations and accurately representing underrepresented attributes. Therefore, carefully crafted prompt design tailored to synthetic tabular data generation is essential to fully leverage the capabilities of LLMs. While several studies have employed in-context learning with LLMs for synthetic tabular data generation [27; 37], few have conducted comprehensive investigations into optimal prompt designs that significantly impact data quality and generation efficiency.

In response, this study examines key components of prompt design and identifies an effective method for generating high-quality synthetic tabular data, particularly addressing class imbalance. We introduce a novel approach, **EPIC**, which leverages the in-context learning capabilities of LLMs to produce synthetic tabular data with balanced class representation. EPIC incorporates prompt design strategies such as CSV style formatting, balanced class grouping, and a unique variable mapper, which together contribute to generating synthetic data that accurately represents class-specific distributions and feature correlations, as illustrated in Fig. 1.

Extensive evaluations across six real-world datasets demonstrate that EPIC significantly improves the data quality and generation efficiency, achieving state-of-the-art performance. As shown in Fig. 2, baselines exhibit low sensitivity, struggling to accurately generate minority class samples due to inherent class imbalances. In contrast, EPIC achieves high sensitivity and balanced performance for all classes, underscoring its robustness and practicality for real-world applications.

Figure 2: **Comparison of ML classification performance with synthetic data on the Travel dataset.** Results are averaged across four classifiers: XGBoost, CatBoost, LightGBM, and gradient boosting classifier, with each classifier run five times. Our method uses the GPT-3.5-turbo model.

In summary, our key contributions are as follows:

* Our study explores the effectiveness of LLMs in generating realistic synthetic tabular data through in-context learning, providing prompt design guidelines to efficiently generate high-quality data while addressing class imbalance.
* We propose EPIC, a simple yet effective prompting method that uses balanced, grouped data samples with unique variable mapping to generate tabular data that accurately represents both minority and majority classes, preserving feature correlations and overall data distribution.
* The proposed approach is model-agnostic, generally applicable to various LLMs, and easy to implement for any tabular data with minimal preprocessing requirements.
* Extensive experiments on six real-world public tabular datasets and one toy dataset demonstrate the effectiveness of our approach, significantly improving ML classification performance and data generation efficiency.

By addressing class imbalance and enhancing classification outcomes, our work contributes to the advancement of the crucial field of tabular data research, significantly impacting various domains.

## 2 Method

In this study, we investigate various prompt design components to maximize the in-context learning capabilities of LLMs for generating high-quality tabular data. Our objective is to develop an optimized approach that reliably and effectively produces realistic tabular data, accurately representing both minority and majority classes to improve ML classification performance, especially addressing class imbalance. Formally, given a tabular dataset \(T\) of dimensions \(n m\), with \(n\) samples and \(m\) variables, we aim to generate a synthetic dataset \(\) of dimensions \(n^{} m\) that accurately reflects key characteristics of \(T\), including class-specific attributes, feature correlations and, data distributions.

To achieve this, we explore the following key prompt design elements: data format (Section 2.1), class presentation methods (Section 2.2), variable mapping (Section 2.3), and task specification (Section 2.4). These design choices are evaluated through extensive analyses on the public datasets

Figure 3: **Overview of our approach. Our prompt includes repeated data example sets consisting of feature names and class-balanced groups, with the feature name at the end serving as a trigger for the LLM to generate realistic synthetic tabular data. The proposed unique variable remaps categorical values to distinct alphanumeric strings, ensuring clear distinction and variability among variables.**

from diverse domains, focusing on ML classification performance and generation efficiency. Detailed analysis is provided in Section 3, with Fig. 12 in Appendix C.1 illustrating the ablated versions.

Building on these analyses, we introduce **EPIC**, a structured prompting method designed to guide LLMs in synthetic data generation while effectively addressing class imbalance, as depicted in Fig. 3. EPIC begins with optional variable descriptions, followed by a series of structured data sample sets, each containing balanced samples for each class. Each set consists of feature names and samples organized by target class. To prompt the LLM to generate a corresponding set of synthetic samples, feature names are appended at the end of the prompt to serve as a trigger, leveraging the pattern recognition capabilities of LLMs. The following sections detail each design component of EPIC.

### Data format: Sentence vs CSV style

We investigate effective data formatting methods to represent tabular data within prompts, ensuring that LLMs can interpret the demonstrations and generate new data accurately. Tabular data can be formatted as plain text or as comma-separated values (CSV style). Let the feature name of the \(k\)-th variable be \(v_{k}\), and the \(i\)-th observation of this variable be \(o_{k,i}\). According to previous work , this value can be transformed into a sentence-style representation as \([v_{k},``i",o_{k,i},``,"]\). In contrast, the CSV style can be expressed as \([o_{k,i},``,"]\), presenting only values, with variable names for all variables specified as \(=[v_{1},``,",v_{2},``,",,v_{m}]\) at the beginning of the data sample presentation. Here, the sentence style redundantly uses variable names and "is" tokens for every value, leading to higher token usage and computational cost than the CSV style. The CSV-style format enables a higher volume of in-context learning examples within the same token constraints. Thus, our method utilizes a CSV-style format to maximize the number of examples, as providing a large number of data samples is crucial for ensuring sufficient representation of the original dataset in in-context learning.

### Class presentation

Class presentation methods aim to ensure adequate representation of all target classes within a prompt, particularly addressing underrepresented classes in imbalanced datasets. The construction of data samples within the prompt is crucial, as it can either worsen or mitigate existing imbalances. To address this, we explore three prompt design options and propose a class balancing and grouping approach that enables accurate and representative generation of data samples across all target classes.

Single-class vs. Multi-class generationWhen generating data under class conditions, a primary consideration is whether to generate data for one class at a time or for all classes simultaneously. Single-class generation allows the model to focus on the unique characteristics of a specific class, while multi-class generation enables direct comparison across classes. Our experiments indicate that generating data for multiple classes simultaneously in a structured manner (discussed in the following sections) yields samples that more accurately capture the distinctive features of each class. This finding suggests that generating data with contextual awareness of other class characteristics enhances the representativeness of synthetic samples.

Original vs. Balanced class ratioWhen constructing prompts to generate multiple classes simultaneously, simple random sampling from the original dataset \(T\) often results in an imbalanced distribution of samples across target classes. This imbalance may cause the LLM to overfit to the majority class, limiting its ability to learn the characteristics of the minority class. To mitigate this, we employ a balanced sampling approach, equalizing sample numbers for all classes rather than strictly following the original class distribution. Our findings reveal that balancing sample sizes substantially improves the quality of generated data for minority classes, indicating that providing balanced data enables LLMs to learn more effectively and replicate the accurate characteristics of all target classes.

Listing vs. Class groupingWithin a prompt, data samples can either be presented sequentially, in the order they are sampled, or grouped by class. Grouping emphasizes contrasts between different classes and reinforces similarities within each group, enabling the LLM to generate more distinct and coherent samples for each group. Empirical evidence suggests that grouping yields a better representation of class-specific feature correlations in the generated data. Class grouping also contributes to constraining the LLM to generate data with specific attributes more efficiently. Without grouping, guiding the LLM to produce samples with targeted attributes can be challenging, often requiring numerous iterations to achieve the desired outcome. In contrast, grouping samples by specific features facilitates conditioning the LLM to generate data in groups with particular attributes or conditions. Althoughthis work primarily focuses on class conditions, this approach can also be applied to feature grouping, enabling controlled generation based on selected criteria.

### Variable mapping: Original values vs. Unique variable mapping

Unlike existing methods that often require extensive preprocessing and handling of noisy or missing data [19; 40], our approach minimizes these steps, preserving the integrity of the raw data, including original feature names and values, similar to other LLM-based approaches . However, a critical challenge arises when a dataset contains numerous categorical variables with identical values (e.g., extensive boolean variables). In such cases, data examples in the prompt may become monotonous and repetitive, making it difficult for the model to distinguish between variables, which can significantly degrade the quality of the generated data. Moreover, when faced with such repetitive input, LLMs tend to struggle to generate valid samples, leading to a notable decline in generation efficiency.

To address this, we propose a unique variable mapping method, which remaps the values of each categorical variable to distinct random alphanumeric strings, as illustrated in Fig. 3. Uniform substitution is applied across the dataset, maintaining the integrity of the data structure while introducing necessary variation. For example, consider a categorical variable \(v_{1}\) with values of 't' and 'f'. Our approach remaps these values to distinct three-character strings, such as 'JY0' and 'GGN,' respectively, and applies them consistently across all data points in the dataset. Other categorical variables are similarly remapped to unique values. The transformed dataset is then used as in-context learning demonstrations. Although these new values do not hold inherent semantic meaning, they effectively represent categorical distinctions as symbols, allowing the LLM to identify and utilize patterns within them. This approach ensures that each variable has a unique representation, making the variables clearly distinguishable and leading to more accurate and efficient data generation.

### Task specification

A crucial aspect of the prompt design is clearly guiding the LLM to generate synthetic data as intended. To achieve this, we consider two options and ultimately find that prompting the LLM to learn the pattern from a structured, consistent format with optional descriptions for variables is more effective for reliable generation than providing explicit instructions.

**Explicit instruction vs. Completion triggering** To clarify the task for the LLM, a prompt might include explicit instructions, such as "generate new data samples." However, crafting specific instructions can be challenging, as LLMs are highly sensitive to subtle prompt variations, making it inefficient to determine an optimal phrasing for tabular data generation. A simpler and more effective approach is to provide patterns within the prompt for the LLM to mimic by formatting data samples without explicit instructions, leveraging its advanced pattern recognition capabilities. Formally, let the \(i\)-th set of data samples across all \(c\) classes be structured as \(_{i}=[,_{i,1},,_{i,c}]\), where each \(_{i,k}\) contains \(n\) samples for the \(k\)-th class. Here, \(\) represents feature names and serves as a header to indicate the start of a set. This approach enables the LLM to recognize the pattern in which each set of data samples is structured and consistently begins with \(\). We then input \(t\) such sets in a prompt, \([_{1},_{2},,_{t}]\), to establish a recognizable pattern, expecting the LLM to generate the next set, \(_{t+1}=[,_{t+1,1},,_{t+1,c}]\). To reinforce this, we place a trigger at the end of the prompt by including the new header \([]\) for \(_{t+1}\), signaling the LLM to complete the sequence with \([_{t+1,1},,_{t+1,c}]\). By structuring the input prompt in this way, we guide the LLM to generate a consistent number of samples, \(n\), for each target class.

**Providing contextual information** Optionally, adding contextual information to the prompt can help the LLM understand the characteristics of the original dataset and generate accurate samples. Therefore, when available, we include line-by-line variable descriptions at the beginning of the prompt. Additionally, we treat the target class variable as one of the attributes and position it as the first variable to clarify that each group is organized by the target class.

### Overview of tabular data generation process

The synthetic data generation process begins by creating a prompt template and determining whether the unique variable mapping is needed. The prompt template specifies parameters such as the target class, sample size per group, the number of groups, and the number of sets to construct demonstrations.

We then randomly sample data examples from the dataset to populate the template, ensuring that examples do not overlap within each prompt to maintain diversity. Using the provided examples, the LLM generates new data in a CSV format, which is subsequently converted into a structured tabular format. Instances containing categorical variable values not present in the original dataset are discarded to maintain authenticity. This generation process is repeated until the desired sample size is achieved, with new examples sampled with replacement in each iteration. This approach enables the LLM to encounter diverse combinations of real samples and produce rich synthetic samples (see Appendix B.1 for analysis). Although each individual generation reflects the distribution of the provided subset, successive iterations enable the LLM to cover a broader range of the original data cumulatively. This iterative method ensures that the output generated by the LLM comprehensively reflects the characteristics of the original data. Final prompting examples are available in Appendix C.5

## 3 Experiments

This section describes the experimental setup and provides a comprehensive evaluation of our method, assessing its effectiveness across diverse real-world datasets.

SummaryWe conduct three unique experiments to analyze the utility of our method in enhancing ML classification performance: augmenting the original dataset with generated data (Tables 1, 19, Figs. 2, 14), augmenting only the minority class similar to SMOTE (Table 6), and using only generated data (Table 7). Additionally, we evaluate our approach using three LLMs: Mistral, Llama2, and gpt3.5-turbo (Table 2, Figs. 4, 5). We perform ablations on prompt elements to compare classification performance (Tables 3, 5) and conduct unique analyses of token usage and LLM generation efficiency (Tables 4, 8, 9). To examine feature correlations, we separately analyze minority and majority classes, comparing results across all prompt variations (Figs. 4, 5). We further investigate how varying the number of generated samples affects classification performance (Figs. 6, 7). Using a toy dataset, we analyze how different prompts influence the accuracy of generated data distributions (Figs. 1, 9). Lastly, we explore the sampling of input examples and their corresponding outputs in LLMs using the toy dataset, providing insights into the variability and reliability of generated data (Figs. 10, 11).

### Experimental setup

DatasetsWe evaluate our method using six real-world public tabular classification datasets from diverse domains: Travel (Marketing), Sick (Healthcare), HELOC (Finance), Income (Social science), Diabetes (Healthcare), and Thyroid (Healthcare). The Thyroid dataset, released after the training cut-off date for GPT-3.5-turbo-0613, provides a more rigorous validation of our approach on completely unseen data. For binary classification datasets, the minority class is designated as one. All duplicate data samples are removed from the datasets. Each dataset is randomly split into 80% training and 20% test sets. We retain the original data, including missing or noisy features. The exception is the Sick dataset, where we follow the source's method. Further details are provided in Appendix C.2.

Evaluation measureWe evaluate our method based on ML classification performance, generation efficiency, feature correlation, and analysis of generated data distribution. For classification, we report F1 score, sensitivity, specificity, and balanced accuracy (BAL ACC). Feature correlation is measured using Pearson correlation for numerical variables and Cramer's V correlation for categorical variables.

Baseline modelsIn our study, we compare our method with various generative models for tabular data, including SMOTE , SMOTENC , TVAE , CopulaGAN , CTGAN , CTABGAN , CTAB-GAN+ , GReT , and TabDDPM . To compare prompting methods, we also use the prompts from CuratedLLM  and LITO .

Experimental detailsOur method utilizes the GPT-3.5 models (GPT-3.5-turbo-0613 and GPT-3.5-turbo-16k-0613), Mistral-7b-v0.1 , and Llama-2-7b . Unless stated otherwise, we use the GPT-3.5 model for our method. The number of synthetic data samples is based on the size of the original datasets. Across all experiments, unless stated otherwise, we report results from four top-performing ML classifiers: XGBoost , CatBoost , LightGBM , and Gradient boosting classifier , known for their strong performance, often surpassing recent deep learning models on tabular datasets. Each classifier is executed in five independent runs, with results averaged over a total of 20 runs to ensure robustness. Further details are available in Appendix C.

### Machine learning classification performance using the synthetic data

We evaluate the quality of the synthetic data samples by assessing the machine learning classification performance when the synthetic data are added to the original dataset. Here, our method utilizes the GPT-3.5-turbo model. As shown in Table 1, our method **achieves state-of-the-art F1 scores and balanced accuracy across all six datasets**, surpassing the baselines that require model training. Notably, our method is **the only one that consistently outperforms the original data in both F1 score and balanced accuracy across all six datasets**. For a machine learning model to perform well in classification, the correlation between input data and labels in the training data must be precise. Thus, the consistent improvement across six datasets demonstrates that the data generated by our method aligns well with the class labels. Other methods often result in worse performance than the

   Dataset & Method & \#syn & F1 score \(\) & BAL ACC \(\) & Sensitivity \(\) & Specificity \(\) \\   & Original & - & 58.12 (0.00) & 71.00 (0.00) & 57.00 (0.00) & 85.00 (0.00) \\  & +TVAE  & +1K & 59.78 (+1.66) & 72.35 (+1.35) & 62.00 (+5.00) & 82.69 (-2.31) \\  & +CopulaGAN  & +1K & 21.76 (-3.36) & 55.52 (-15.48) & 12.80 (+44.20) & 98.23 (+13.23) \\  & +CTAB-GAN+  & +1K & 54.66 (-3.46) & 68.62 (-2.38) & 53.00 (-4.00) & 84.23 (-0.77) \\  & +GReaT  & +1K & 60.95 (+2.83) & 72.86 (+1.86) & 58.80 (+1.80) & 86.92 (+1.92) \\  & +TabDDPM  & +1K & 53.20 (+4.92) & 67.70 (-3.30) & 50.40 (-6.60) & 85.00 (0.00) \\  & **+Ours** & +1K & **66.65 (+8.53)** & **78.23 (+7.23)** & **78.00 (+21.00)** & 78.46 (-6.54) \\   & Original & - & 87.81 (0.00) & 91.22 (0.00) & 82.83 (0.00) & 99.61 (0.00) \\  & +TVAE  & +1K & 87.77 (-0.04) & 91.47 (+0.25) & 83.37 (-0.54) & 99.56 (-0.05) \\  & +CopulaGAN  & +1K & 83.60 (+4.21) & 86.61 (-4.61) & 73.37 (-0.46) & 99.86 (+0.25) \\  & +CTAB-GAN+  & +1K & 82.35 (-5.46) & 86.28 (-4.94) & 72.83 (-1.00) & **99.74 (+0.13)** \\  & +GReaT  & +1K & 87.23 (-0.58) & 90.83 (-0.39) & 82.07 (-0.76) & 99.60 (-0.01) \\  & +TabDDPM  & +1K & 85.17 (-2.64) & 89.30 (-1.92) & 79.02 (-3.81) & 99.57 (-0.04) \\  & +**Ours** & +1K & **88.71 (+0.90)** & **92.93 (+1.71)** & **86.41 (+3.58)** & 99.44 (-0.17) \\   & Original & - & 71.01 (0.00) & 73.21 (0.00) & 67.89 (0.00) & 78.52 (0.00) \\  & +TVAE  & +1K & 71.12 (+0.11) & 73.25 (+0.04) & 68.15 (+0.26) & 78.34 (-0.18) \\  & +CopulaGAN  & +1K & 71.23 (+0.22) & 73.32 (+0.11) & 68.37 (-0.48) & 78.26 (-0.26) \\  & +CTAB-GAN+  & +1K & 71.03 (+0.02) & 73.15 (+0.06) & 68.13 (-0.24) & 78.17 (-0.35) \\  & +GReaT  & +1K & 70.35 (+0.66) & 72.96 (-0.25) & 66.22 (-1.67) & **79.70 (+1.18)** \\  & +TabDDPM  & +1K & 70.65 (-0.36) & 72.89 (-0.32) & 67.51 (-0.38) & 78.26 (-0.26) \\  & **+Ours** & +1K & **71.92 (+0.91)** & **73.66 (+0.45)** & **69.96 (+2.07)** & 77.35 (-1.37) \\   & Original & - & 66.90 (0.00) & 76.45 (0.00) & 57.28 (0.00) & **95.61 (0.00)** \\  & +TVAE  & +20K & 66.96 (+0.06) & 76.80 (+0.35) & 59.13 (+1.85) & 94.48 (-1.13) \\   & +CopulaGAN  & +20K & 66.75 (+0.15) & 76.73 (+0.28) & 59.16 (+1.88) & 94.29 (-1.32) \\   & +CTAB-GAN+  & +20K & 66.49 (-0.41) & 76.42 (-0.03) & 58.14 (+0.06) & 94.70 (-0.91) \\   & +GReaT  & +20K & 67.95 (+0.15) & 77.51 (+1.06) & 60.69 (+3.41) & 94.33 (-1.28) \\   & +TabDDPM  & +20K & 68.85 (+0.05) & 76.50 (+0.05) & 57.70 (-0.42) & 95.30 (-0.31) \\   & **+Ours** & +20K & **69.16 (+2.26)** & **79.15 (+2.70)** & **66.45 (+9.17)** & 91.85 (-3.76) \\   & Original & - & 54.87 (0.00) & 42.07 (0.00) & 60.00 (0.00) & 60.73 (0.00) \\  & +TVAE  & +10K & 54.79 (+0.08) & 41.96 (+0.11) & 59.96 (-0.04) & 60.71 (-0.02) \\   & +CopulaGAN  & +10K & 54.27 (-0.60) & 41.59 (-0.48) & 59.73 (-0.27) & 59.97 (-0.76) \\   & +CTAB-GAN+  & +10K & 54.24 (-0.63) & 41.52 (-0.55) & 59.63 (-0.37) & 60.01 (-0.72) \\   & +GReaT  & +10K & 54.78 (+0.09) & 41.98 (-0.09) & 59.98 (-0.02) & 60.61 (-0.12) \\   & +TabDDPM  & +10K & 54.64 (-0.23) & 41.83 (-0.24) & 59.91 (-0.09) & 60.55 (-0.18) \\   & +Ours & +10K & **54.94 (+0.7)** & **62.14 (+0.07)** & **60.04 (+0.04)** & **60.82 (+0.09)** \\   & Original & - & 94.23 (0.00) & 95.08 (0.00) & 91.14 (0.00) & 99.02 (0.00) \\  & +TVAE  & +1K & 90.45 (-3.78) & 92.20 (-2.88) & 86.36 (-4.78) & 98.04 (-0.98) \\   & +CopulaGAN  & +1K & 86.73 (-7.50) & 88.71 (-6.3original data, indicating that the data generated by baselines disrupts the original data distribution. These findings underscore the robustness and effectiveness of our method, affirming its superiority over baselines, even for challenging datasets.

Moreover, baselines such as GReAT and TabDDPM exhibit significantly lower sensitivity compared to specificity, particularly for Travel and Income. Their high balanced accuracy is due to high specificity, but their severely low sensitivity indicates a failure to generate appropriate minority class data. In contrast, our method **significantly improves sensitivity and achieves a balance between sensitivity and specificity while also attaining the best-balanced accuracy and F1 score compared to the baselines**. For example, in Travel, our method achieves a sensitivity of 78%, which is 19%p higher than the second-best method, GReAT, at 58.8%. In Income, our method shows 66.45% sensitivity, surpassing the second-best score of 60.69%. These results highlight the effectiveness of our method in accurately representing minority classes in tabular datasets.

Overall, our findings demonstrate that our approach is generally applicable across various imbalanced tabular datasets and excels in generating high-quality samples that improve ML classification performance. Further analyses exploring the impact of augmenting the original dataset for the minority class (Table 6) and replacing the original dataset with synthetic data (Table 7) are available in Appendix A.

### Open-source LLMs

We also apply our prompting method to open-source LLMs, including Llama2  and Mistral . As shown in Table 2, our method exhibits robust performance when used with open-source LLMs across various datasets. Notably, in the Sick dataset, Mistral demonstrates the highest balanced accuracy, indicating its effectiveness in generating high-quality tabular data. These results validate the broader applicability and general effectiveness of our approach with open-source LLMs.

### Exploring optimal prompt design through ablation studies

We investigate key prompt design choices and validate our method through extensive ablation studies across multiple datasets from diverse domains, focusing on ML performance and generation efficiency, as detailed in Tables 3 and 4, respectively. Given a fixed number of input samples, we evaluated (1) the number of input tokens required, (2) the number of valid generated samples, and (3) the generation success rate. Our results indicate that CSV-style prompting generally outperforms sentence-style prompting in ML performance with the same number of input samples. While generating data for one class at a time yields good results for the Sick and Income datasets, it results in a low F1 score of 58.67% on the Travel dataset. However, generating data for multiple classes simultaneously with balanced samples improves this score to 70.37%. Furthermore, applying unique variable mapping consistently enhances F1 scores on the Sick dataset. The combined approach of random mapping and grouping achieves higher F1 score of 91.95% compared to 86.60% with random mapping but without grouping, underscoring the positive impact of this combined approach. These findings indicate

   Dataset & Method & Model & \#syn & F1 score \(\) & BAL ACC \(\) & Sensitivity \(\) & Specificity \(\) \\   & Original & - & - & 60.00\(\)0.00 & **72.31\(\)**0.00 & 60.00\(\)0.00 & **84.62\(\)**0.00 \\  & **+Ours** & Mistral & +1K & 66.67\(\)0.00 & 78.00\(\)0.00 & 76.00\(\)0.00 & 80.00\(\)0.00 \\  & **+Ours** & Llama2 & +1K & 67.80\(\)0.00 & **79.23\(\)**0.00 & **80.00\(\)**0.00 & 78.46\(\)0.00 \\  & **+Ours** & GPT3.5 & +1K & **70.18\(\)**0.00 & **80.77\(\)**0.00 & **80.00\(\)**0.00 & 81.54\(\)0.00 \\   & Original & - & - & 84.09\(\)0.00 & 89.86\(\)0.00 & 80.43\(\)0.00 & 99.28\(\)0.00 \\  & **+Ours** & Mistral & +1K & 88.42\(\)0.00 & **95.15\(\)**0.00 & **91.30\(\)**0.00 & 99.00\(\)0.00 \\  & **+Ours** & Llama2 & +1K & 85.53\(\)0.95 & 93.14\(\)0.52 & 87.39\(\)0.97 & 98.88\(\)0.06 \\  & **+Ours** & GPT3.5 & +1K & **91.95\(\)**0.00 & 93.41\(\)0.00 & 86.96\(\)0.00 & **99.86\(\)**0.00 \\   & Original & - & - & 71.32\(\)0.04 & 73.39\(\)0.03 & 68.47\(\)0.06 & 78.31\(\)0.00 \\  & **+Ours** & Mistral & +1K & 71.48\(\)0.00 & **73.74\(\)**0.00 & 68.00\(\)0.00 & **79.47\(\)**0.00 \\   & **+Ours** & Llama2 & +1K & 70.77\(\)0.00 & 73.08\(\)0.00 & 67.37\(\)0.00 & 78.79\(\)0.00 \\   & **+Ours** & GPT3.5 & +1K & **71.93\(\)**0.02 & 73.68\(\)0.02 & **69.90\(\)**0.00 & 77.45\(\)0.04 \\   

Table 2: **Comparison of classification performance using synthetic data generated by different LLMs with our method.** We report the average performances of five runs of gradient boosting classifier. #syn denotes the number of synthetic samples added to the original dataset.

that prompt design significantly influences the quality of generated data, emphasizing the need for carefully tailored prompts to generate high-quality synthetic tabular data.

For the Travel and Income datasets, performance without grouping is slightly better than with grouping, but the differences are within 0.5%p, making their performance on par. However, these show significant differences in LLM generation efficiency. Without grouping, an average of 0.88 samples are generated per iteration, whereas grouping increases this to an average of 8.6 samples per iteration. Repeating the set twice within the prompt further improves generation efficiency, boosting the average output to over 10 samples and the success rate to 94%, by creating input patterns that LLMs can mimic to generate data. Unique variable mapping also significantly enhances LLM generation efficiency and success rates, as detailed in Table 8 of Appendix A.3.

   Format & Class & Balance & Group & Unique & \#set &  Input \\ tokens \\  &  Output \\ samples \\  & 
 Success \\ rate \\  \\  Sentence & single & ✗ & ✗ & ✗ & - & 1832.9 & 1.26 & 96\% \\ CSV-style & single & ✗ & ✗ & ✗ & - & 1024.6 & 1.11 & 16\% \\  Sentence & multi & ✗ & ✗ & ✗ & - & 1827.4 & 0.97 & **97\%** \\ CSV-style & multi & ✗ & ✗ & ✗ & - & **912.8** & 0.38 & 38\% \\  Sentence & multi & ✓ & ✗ & ✗ & - & 1938.7 & 1.73 & **97\%** \\ CSV-style & multi & ✓ & ✗ & ✗ & - & 920.3 & 0.88 & 31\% \\  CSV-style & multi & ✓ & ✓ & ✗ & 1 & 965.6 & 8.6 & 48\% \\ CSV-style & multi & ✓ & ✓ & ✗ & 2 & 1014.6 & **10.52** & 94\% \\   

Table 4: **Comparison of token usage and generation efficiency across ablated methods on the Income dataset. Results are based on 100 inferences, each with 20 random input samples. Input tokens indicates the number of tokens required in the LLM prompt for a fixed number of input samples. Output samples shows the average number of synthetic samples generated per iteration. Success rate measures the ratio of inferences that generate at least one valid data sample.**

   Method & \#syn & F1 score \(\) & BAL ACC \(\) & Sensitivity \(\) & Specificity \(\) \\  Instruction-CuratedLLM\({}^{}\) & +1K & 15.86\(\)5.13 & 56.02\(\)3.54 & 17.17\(\)11.80 & 94.86\(\)4.93 \\ Instruction-LITO\({}^{}\) & +1K & 21.32\(\)1.65 & 72.06\(\)2.49 & 84.46\(\)3.10 & 59.66\(\)3.50 \\ Ours w/ class distinction & +1K & 74.06\(\)2.64 & 96.23\(\)0.98 & **96.74\(\)**1.93 & 95.72\(\)0.60 \\ Ours w/o var description & +1K & 76.06\(\)4.56 & **96.29\(\)**1.40 & 96.41\(\)2.76 & 96.18\(\)1.06 \\
**Ours** & +1K & **88.71\(\)**1.98 & 92.93\(\)0.91 & 86.41\(\)1.85 & **99.44\(\)**0.27 \\   

Table 5: **Comparison of classification performance on the Sick dataset for task specification elements in prompt design. Results are averaged across XGBoost, CatBoost, LightGBM, and gradient boosting classifier. Methods marked with an asterisk (*) use the prompt designs proposed in the respective papers. #syn denotes the number of synthetic samples added to the original dataset.**Furthermore, we conduct an ablation study on the task specification elements, as shown in Table 5. Our approach of triggering completion significantly enhances overall ML performance compared to the instruction-based prompting methods. Interestingly, adding class distinctions within the prompt achieves the best sensitivity, indicating that the distinction facilitates better generation for minority classes. Providing variable descriptions improves balanced accuracy but reduces the F1 score, suggesting that this element may be optional rather than a definitive improvement. A comparison of generation efficiency using these methods is provided in Table 9 of Appendix A.3.

### Analysis of feature correlation

We compare the feature correlations of the categorical variables between original and synthetic data, as illustrated in Fig. 4. Our synthetic data generated with Mistral and Llama2 exhibit the closest feature correlation with the original data for both minority and majority classes, outperforming the baselines and ablated versions. Additionally, our grouping method significantly contributes to preserving feature correlation for the minority class. These findings indicate that our method generates data that more accurately matches each class, particularly for underrepresented classes, compared to other methods. Additional results on the Sick dataset are available in Fig. 5 of Appendix A.3.

## 4 Limitations and future work

When the training dataset is large and cannot be fully included in the LLM prompt due to token size limitations, only a subset of the data can be used as examples for generating samples. If these prompt samples do not fully represent the original data distribution, the generated data may be incomplete and of low quality, as LLMs are limited to producing data based only on the patterns present in the input. To overcome this, our method employs multiple rounds of random sampling with replacement to create a synthetic dataset that more comprehensively represents the original distribution, resulting in improved machine learning classification performance (a comprehensive analysis is provided in Appendix B.1). However, this approach still carries the risk that samples may not fully capture the original data distribution. Future research will focus on developing methods to identify key examples that more accurately represent the entire dataset.

## 5 Conclusion

This study demonstrates the effectiveness of using LLMs for synthetic tabular data generation. We introduce EPIC, a simple yet effective solution for generating realistic, high-quality data without additional training, specifically designed to address class imbalance. Our method demonstrates significant improvements over state-of-the-art generation models across six real-world public datasets, generating data with highly accurate feature correlations and significantly improving ML classification accuracy for minority classes. These results underscore the substantial impact of our approach in real-world applications, making a significant contribution to the field of tabular data research.

Figure 4: **Difference between Cramér’s V correlation matrices of real and synthetic datasets for categorical variables in the Travel dataset.** More intense colors indicate larger differences, with positive differences shown in red and negative differences shown in blue. Black indicates where the correlation is not measured since only one unique value is generated for those variables.