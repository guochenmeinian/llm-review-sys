# Claim 1. There exists a neighbourhood \(O\) of the identity matrix \(\bm{I}\) in \(\overline{\mathcal{M}}_{\mathrm{sur}}(\mathcal{G})\) (in the sense of Remark 1) such that for \(\forall\bm{M}\in O\cap\mathcal{M}_{\mathrm{sur}}^{0}(\mathcal{G})\), \(p^{E_{k}},k\in[K]\) satisfy Assumption 7\(\Rightarrow q^{E_{k}},k\in[K]\) satisfy Assumption 7.

Learning Linear Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity

Jikai Jin

Institute for Computational and Mathematical Engineering

Stanford University

Stanford, CA 94305

jkjin@stanford.edu

&Vasilis Syrgkanis

Management Science and Engineering

Stanford University

Stanford, CA 94305

vsyrgk@stanford.edu

###### Abstract

We study causal representation learning, the task of recovering high-level latent variables and their causal relationships in the form of a causal graph from low-level observed data (such as text and images), assuming access to observations generated from multiple environments. Prior results on the identifiability of causal representations typically assume access to single-node interventions which is rather unrealistic in practice, since the latent variables are unknown in the first place. In this work, we consider the task of learning causal representation learning with data collected from _general environments_. We show that even when the causal model and the mixing function are both linear, there exists a _surrounded-node ambiguity_ (SNA)  which is basically unavoidable in our setting. On the other hand, in the same linear case, we show that identification up to SNA is possible under mild conditions, and propose an algorithm, LiNGCReL which provably achieves such identifiability guarantee. We conduct extensive experiments on synthetic data and demonstrate the effectiveness of LiNGCReL in the finite-sample regime.

## 1 Introduction

Artificial intelligence (AI) has achieved tremendous success in various domains in the past decade . However, current approaches are largely based on learning the _statistical_ structures and relationships in the data that we observe. As a result, it is not surprising that these approaches often capture spurious statistical dependencies between different features, resulting in poor performance in the presence of test distribution shift  or adversarial attacks .

In view of these pitfalls, a recent line of work has explored the problem of _causal representation learning_ (CRL) , the task of learning the causal relationships between high-level latent variables underlying our low-level observations. Notably, it is widely believed in cognitive psychology that humans take a causal approach to distill information from the world and make decisions to achieve their goals . As a result, there is reason to believe that learning causal representations has the potential to significantly improve the power of AI, especially on tasks where performance lags far behind human level .

Despite such promise, a crucial challenge in CRL is the _identifability_ of the data generating process; in other words, given the data that we observe, can we uniquely identify the underlying causal model. It has been shown that given observational data (_i.e._, i.i.d. data generated from a single environment), the model is already non-identifiable in strictly simpler settings where the latent variables are known to be independent [25; 26], or where there is no mixing function and one directly observes the latent variables . As a result, existing algorithms for CRL with observational data [52; 53; 11] typically require additional assumptions on the structure of the underlying causal graph. A natural question that arises is what types of data do we need to acquire to make identification possible in the general case.

One line of works assumes access to counterfactual data [27; 48; 5], where some form of _weak supervision_ is typically required. A common assumption here is that one observes data in _pairs_, where each pair of data is related via sharing part of the latent representation. However, such data is hard to acquire since it requires direct control on the latent representation.

Another line of works [1; 49; 7; 47] instead considers an interventional setting, where the learner observes data generated from multiple different environments. This is arguably a much more realistic setup and reflects common practices in robotics  and genomics [28; 43] applications. However, a vast majority of identifiability guarantees assume that each environment corresponds to _single-node_, _hard_ interventions, which is defined as interventions that isolate a single latent variable from its causal parents. Again, this is quite a restrictive assumption because of two reasons. _First_, since the latent variables are unknown and need to be learned from data, it is unclear how to perform interventions that only affect one variable. _Second_, even if one can perform single-node interventions, it may not be feasible to artificially remove causal effects in the data generating processes. This issue is ubiquitous in real-world applications as pointed out in Campbell , Eberhardt , Eronen . Motivated by these challenges, we make the following contributions in this paper:

* Assuming access to data collected from multiple environments, but not necessarily from single-node, hard interventions, we identify an intrinsic surrounded-node ambiguity (SNA) in learning the underlying causal representations. We show in Theorem3 that SNA is unavoidable even if (1) both the mixing function and the causal model are known to be linear and (2) one has access to single-node, soft interventions. This highlights a remarkable difference with existing literature which shows that perfect identification can be achieved with hard interventions.
* When the causal model and the mixing function are both linear, we prove in Theorem1 that identification up to SNA is achievable with \((d)\) diverse environments (Assumption4), where \(d\) is the size of the latent causal graph. To the best of our knowledge, this is the first identification guarantee that applies to fully general environments and makes no assumption on their relationship or similarity. Interestingly, we also show in Theorem2 that one would require \((d^{2})\) single-node soft interventions to achieve the same identification guarantee, indicating the benefit of learning from diverse environments.
* We propose an algorithm, LiNGCReL, in Section5 that provably recovers the ground-truth model up to SNA (Theorem4) in the setting of Theorem1 when perfect information of the observation distributions is available. To demonstrate the effectiveness of LiNGCReL in finite-sample regime, we conduct extensive experiments on synthetic data, and our results reported in Section6 show that LiNGCReL is capable of recovering the true causal model up to SNA with high accuracy.

Due to space limit, proofs of all our statements and additional theoretical results are given in the appendix.

## 2 Preliminaries

We consider the standard setup of CRL from multiple environments \(E\). Let \(=(,)\) be the ground-truth causal graph which is directed and acylic (DAG), where \(=[d]\) and \(\) describes the causal relationship between different nodes. Each node corresponds to a latent variable \(_{i}\).

For any node \(i\), we let \(_{}(i)\), \(_{}(i)\), \(_{}(i)\) and \(_{}(i)\) to be the set of all parents, children, ancestors and non-descendants of \(i\) in \(\) respectively. We also define \(}_{}(i)=_{}(i)\{i\}\) and similarly for \(}_{}(i),}_{} (i)\) and \(}_{}(i)\). Assuming that all probability distributions have continuous densities, the joint density of the latent variables \(\) can then be written as

\[p_{E}()=_{i=1}^{d}p_{i}^{E}(_{i}_{_{ }(i)}).\] (1)

where \(p_{i}^{E}\) is the (unknown) latent generating distribution from environment \(E\) at node \(i\). Here for a given vector \(\), we write \(_{i}=_{i}^{}\), and let \(_{S}=(_{i}:i S)^{|S|}\).

The causal graph model with density given by (1) necessarily enjoys the following property:

**Definition 1** (Causal Markov Condition).: _For any node \(i\), conditioned on \(_{_{}(i)}\), \(_{i}\) is independent of \(_{_{}(i)}\). As a consequence, for any node \(i,j[d]\) and \(S[d]\), if \(S\)\(d\)-separates \(i\) from \(j\) (cf. Definition 7), then \(_{i}_{j}_{S}\)._

The latent variables \(\) are unknown to the learner. Instead, the learner has access to observations \(^{n}\) (\(n d\)) from all environments \(E\) that are related to the latent \(\) via an injective mixing function \(\):

\[=().\] (2)

The main assumption here that the mixing function is the same across all environments:

**Assumption 1**.: _All environments \(E\) share the same diffeomorphic mixing function \(:^{d}^{n}\)._

In CRL, the goal of the learner is to 1) recover the inverse of the mixing function \(=^{-1}\) (often called the _unmixing_ function) which allows recovering the latent variables given any observations, and, 2) recover the underlying causal graph \(\). In the remaining part of this paper, we refer to \((,)\) as the causal model to be learned. Obviously, there would be some ambiguities in learning \((,)\). For example, choosing a different permutation of the nodes in the causal graph would lead to a different model, and so does element-wise transformations on each component \(_{i}\) of \(\).

A line of recent works show that the ground-truth model can be identified up to these ambiguities in various settings, assuming access to single-node hard interventions [36; 49; 47]. On the other hand, some weaker notions of identifiability have also been proposed and studied in the literature [36; 46; 23] for single-node soft interventions. Here, we provide a generic definition of single-node soft interventions that we will rely on in this paper.

**Definition 2**.: _We say that a collection of environments \(}\) is a set of (soft) interventions on a subset of latent variables \(\{_{j},j S\}\) if for any \(i[d]\) and any \(E_{1},E_{2}},E_{1} E_{2}\), we have \(p_{i}^{E_{1}}=p_{i}^{E_{2}}\) if and only if \(i S\) (the notation \(p_{i}^{E}\) comes from (1)). Equivalently, we write \(_{}^{}}=S\)._

We note that soft interventions are very different from hard interventions, since they do not remove causal relationships between latent variables. The goal of this paper is to address the following question:

_What is the best-achievable identification guarantee when hard interventions are not available, and what are the intrinsic ambiguities?_

## 3 The surrounding set and a notion of identifiability

One may expect that identifiability with soft interventions is not much different from hard interventions, since soft interventions can approximate hard interventions with arbitrary accuracy. However, we will show that this is not the case. At a high level, hard intervention is more powerful than soft intervention because it is capable of isolating a latent variable from its direct cause while soft interventions is not, so soft interventions can sometimes fail to identify the true causal relationship from a mixture of causal effects.

To quantify what kind of ambiguities may arise, we can define the surrounding set for each node in a causal graph \(\) as follows:

**Definition 3**.: _(46, Definition 3) For two nodes \(i,j[d]\) in \(\), we say that \(j\) is surrounded by \(i\), or \(i_{}(j)\) if \(i_{}(j)\), and \(_{}(j)_{}(i)\). Moreover, we define \(}_{}(j)=_{}(j)\{j\}\)._

Intuitively, if there exists some \(i_{}(j)\), then ambiguities may arise for the causal variable at node \(j\), since any effect of \(j\) on any of its child \(k\) can also be interpreted as a mixture of the effect of and \(j\). In Appendix E we discuss an example with three causal variables to further illustrate such ambiguities.

Definition 3 naturally induces the following relationship between causal models:

**Definition 4**.: _Using the notations in Definition 10, we write \((,)_{}(},})\) if there exists a permutations \(\) on \([d]\), and a diffeomorphism \(:^{d}^{d}\) where the \(j\)-th component of \(\), denoted by \(_{j}()\), is a function of \(_{}_{}(j)}\) for \( j[d]\), such that the following holds:_

* _For any_ \(i,j[d]\)_,_ \(i_{}(j)\) _if and only if_ \((i)_{}((j))\)_, and_
* \(_{}}=\)_, where_ \(_{}\) _is a permutation matrix satisfying_ \((_{})_{ij}=1\) _if_ \(j=(i)\) _and_ \((_{})_{ij}=0\) _otherwise._

In other words, \(_{}\) requires that the causal graph to be exactly the same up to some permutation of nodes, but allows each latent variable \(_{i}\) to be a mixture of \(_{_{}(i)}\). Although not obvious from definition, one can actually check that \(_{}\) defines an _equivalence relation_ (see Lemma11). Moreover, we will show in the following section that \(_{}\) is in general the best that we can hope for in our problem setting.

## 4 Identifiability theory for linear CRL with general environments

In this section, we consider learning causal models from _general_ environments. Specifically, we assume that the environments \(E_{k},k[K]\) share the same causal graph, but the dependencies between connected nodes (latent variables) are completely unknown, and, in contrast with existing literature on single-node interventions, we impose no similarity constraints on the environments. We begin our investigation of identifiability in this setting in the context of linear causal models with a linear mixing function.

### Problem setup

Formally, we assume the following generative model in \(K\) distinct environments \(=\{E_{k}:k[K]\}\) with data generating process

\[=_{k}+_{k}^{},= {G} k[K],\] (3)

where the matrix \(_{k}\) satisfies \((_{k})_{ij} 0\) if and only if \(j i\) in \(\). We refer to \((_{k},_{k})\) as the weight matrices of latent variables \(\) in the environment \(E_{k}\). It is easy to see that Assumption1 in our general setup translates into the following assumption:

**Assumption 2**.: _The mixing matrix \(^{n d}\) has full column rank. Equivalently, the unmixing matrix \(=^{}\) has full row rank._

Let \(_{k}=_{k}^{-}(-_{k}),k[K]\), then we have \(=_{k}=_{k}\). Since in the linear case, there is an easy to see one-to-one correspondence between the matrix \(\) and the un-mixing function \(\), we abuse the notation and write \((,)\) to represent the model instead of \((,)\). Using \(_{i}\) to denote the \(i\)-th row of \(\), the following lemma translates Definition4 the the linear setting:

**Lemma 1**.: _According to Definition4, \((,)_{}(},})\) if and only if there exists a permutation \(\) on \([d]\), such that the following statements hold:_

Figure 1: An illustration of Definition3; here \(i_{}(j)\).

1. _For all_ \(i,j[d]\)_,_ \(i_{}(j)\) _if and only if_ \((i)_{}}((j))\)_, and_
2. _For all_ \(i[d]\)_,_ \(}_{i}_{j}:(j)}_{}(i)\)_._

We also need to make the following assumption on noise.

**Assumption 3**.: _The noise vector \(^{d}\) has independent components, at most one component is Gaussian distributed, and any two components have different distribution._

The non-gaussianity of the noise vectors is a typical assumption in causal discovery within linear models  and is always assumed in the LinGAM setting . The assumption that all components have a different distribution is not so standard, but is quite natural in real-world scenarios.

### Identifiability guarantee

For each node \(i[d]\) of \(\), we use \(_{k}(i)\) to be the _weight vector_ of environment \(E_{k}\) at node \(i\), _i.e._, \(_{k}(i)=((_{k})_{ij}:j_{}(i)) ^{|_{}(i)|}\). In other words, the structural equation for node \(i\) in environment \(k\) is of the form:

\[z_{i}=w_{k}(i)^{}z_{_{}(i)}+} _{i}\] (4)

To obtain our identifiability result, the main assumption we need to make is the non-degeneracy of the weights at each node:

**Assumption 4**.: _For each node \(i[d]\) of \(\), we have \((_{k}(i):k[K])=^{|_{ }(i)|}\) where \(()\) denotes the affine hull. Equivalently, the weights \(_{k}(i),k=1,2,,K\) do not lie in a \((|_{}(i)|-1)\)-dimensional hyperplane of \(^{|_{}(i)|}\)._

This assumption is quite mild since it only requires the weight vectors to be in general positions, and it holds with probability \(1\) if the weights at each node are sampled from continuous distributions. Moreover, as shown in Lemma 5, it is equivalent to the following assumption.

**Assumption 5** (Node-level non-degeneracy).: _We say that the matrices \(\{_{k}\}_{k=1}^{K}\) are node-level non-degenerate if for all node \(i[d]\), we have \((_{k})_{i}:k[K]=| _{}(i)|+1\), where \((_{k})_{i}\) is the \(i\)-th row of \(_{k}\)._

In the following, we state our main result in this section, which shows that \(K=d\) non-degenerate environments suffices for the model to be identifiable up to \(_{}\).

**Theorem 1**.: _Suppose that \(K d\) and we have access to observations generated from the linear causal model \((,)\) across multiple environments \(=\{E_{k}:k[K]\}\) with observation distributions \(\{_{}^{E}\}_{E}\), and the data generating processes are given by (3). Let \((},})\) be any candidate solution with the hypothetical data generating process_

\[=}_{k}+}_{k}^{},=}^{}$}\]

_where \(}\) has full row rank, such that_

1. _the observation distribution that this hypothetical model generates in_ \(E_{k}\) _is exactly_ \(_{}^{E_{k}}\)_;_
2. _all environments share the same causal graph:_ \( k[K]\) _and_ \(i,j[d]\)_,_ \((_{k})_{ij} 0 j_{}(i)\)_,_ \((}_{k})_{ij} 0 j_{}}(i)\) _and_ \(_{k},}_{k}\) _are diagonal matrices with positive entries;_
3. \(\{_{k}\}_{k=1}^{K}\) _and_ \(\{}_{k}=}_{k}^{-}(- }_{k})\}_{k=1}^{K}\) _are non-degenerate in the sense of Assumption_ 5_;_
4. _the noise variables_ \(\) _and_ \(\) _satisfy Assumption_ 3_._

_Then we must have \((,)_{}(},})\)._

The proof of Theorem 1 is given in Appendix H.1. In the next section, we will introduce an algorithm, LiNGCReL, that provably recovers the ground-truth up to \(_{}\).

To the best of our knowledge, this is the first identifiability guarantee in the literature for CRL from general environments, even for the linear case. Our result is closely related but fundamentally different from Xie et al. , Dong et al.  that consider the task of linear CRL using _observational data_. As discussed before, with observational data the causal graph can at best be identified up to Markov equivalence. As a result, one typically requires additional assumptions on the structure of the causal graph to obtain stronger guarantees. In contrast, we show that with data from multiple environments, exact recovery of the causal graph is possible without any structural assumptions.

Interestingly, while the fact that existing works focus on single-node interventions seem to suggest that learning from diverse environments is hard, it turns out that such diversity is actually helpful. Specifically, we show that in the worst case, \((d^{2})\) interventions are required for identifying the ground-truth model under \(_{}\):

**Theorem 2** (informal version of Theorem 6).: _There exists a causal graph \(\) with \((d^{2})\) edges, such that for any unmixing matrix \(^{d n}\) with full row rank, any independent noise variables \(\), and any \(0<s_{i}|_{}(i)|,i[d]\), the ground-truth model \((,)\) is non-identifiable up to \(_{}\) with \(s_{i}\) soft interventions for node \(i\), unless the (ground-truth and intervened) weights of the causal model lie in a null set (w.r.t the Lebesgue measure)._

A formal version and the proof of Theorem 2 can be found in Appendix H.2. On the other hand, by having \(d\) single-node interventions per node, Assumption 5 can be satisfied as long as the weights are in general positions, so in this case we have \((,)_{}(},})\) by Theorem 1. Therefore, Theorems 1 and 6 together imply that \((d^{2})\) single-node interventions are necessary and sufficient for identification up to \(_{}\).

Given that Theorem 1 only guarantees identification up to \(_{}\) that is strictly weaker than full identification, one might naturally ask whether Theorem 1 can be further improved. Our last theorem in this section indicates that \(_{}\) is indeed a fundamental barrier that exists even when we access to _single node_, soft interventions.

**Theorem 3** (Counterpart to Theorem 1, informal version of Theorem 9).: _For any linear causal model \((,)\) and any set of environments \(=\{E_{k}:k[K]\}\) such that all conditions in Theorem 1 are satisfied, there must exists a candidate solution \((},)\) and a hypothetical data generating process that satisfy the same set of conditions, but_

\[_{i}}{_{j}} 0, j }_{}(i).\]

_Moreover, if we additionally assume that the environments are groups of single-node soft interventions, then we can guarantee the existence of \((},)\) and weight matrices which, besides the properties listed above, are also groups of single-node soft interventions._

## 5 LinGReL: Algorithm for linear non-Gaussian causal representation learning

In this section, we introduce Linear Non-Gaussian Causal Representation Learning (LiNGCReL), an algorithm that provably recovers the underlying causal graph and latent variables up to \(_{}\) in the infinite-sample limit. At this point, it is instructive to recall the celebrated LiNGAM algorithm  for linear causal graph discovery. Different from their setting, we only observe some unknown linear mixture of the latent variables. Hence, running linear ICA as in LiNGAM only gives us \(_{k}=_{k}\) rather than the weight matrix \(_{k}\) itself.

The key idea in our approach is an effect cancellation scheme that allows us to determine the "remaining degree of freedom" (RDF) of any node (_a.k.a._ latent variable) given any subset of its ancestors. This scheme allows us to not only find a topological order of the nodes, but also figure out direct causes by tracking the changes of the RDF. In the following, we present the main steps of LiNGCReL in more details.

Suppose that we are given samples of observations \(^{(k)}=\{_{i}^{(k)}\}_{i=1}^{N},k[K]\) where \(_{i}^{(k)}\) is the \(i\)-th sample from the \(k\)-th environment.

**Step 1. Recover the matrices \(_{k}=_{k}\)** Since \(=_{k}=_{k}\) in the \(k\)-th environment, so we can use any identification algorithm for linear ICA to recover the matrix \(_{k}\). Then we properly rearrange the rows of \(_{k}\) so that all \(_{k},k=1,2,,K\) correspond to the same permutation of noise variables. This step is quite standard and details can be found in Appendix B.1.

**Step 2. CRL based on \(_{k}\)** Now we have obtained \(_{k}=_{k}\), but the unmixing matrix \(\) is still unknown. We propose Algorithm 3 to learn \(\) and the causal graph \(\). The main part of Algorithm 3 contains a loop that maintains a node set \(S\) which, we will show later, is ancestral, _i.e._, \(i S_{}(i) S\). In each round the algorithm finds a new node \(i S\) such that \(_{}(i) S\), and a subroutine Identify-Parents (Algorithm 2) is used to find all parents of \(i\). After that, we append \(i\) into \(S\) and continue until \(S\) contains all nodes in \(\). Finally, the rows of the mixing matrix \(\) is obtained by intersections of properly-chosen row spaces of \(_{k}\).

Both Algorithm 2 and Algorithm 3 include a crucial step, which we call it _orthogonal projection_, as described in Algorithm 1. At a high level, it helps determine the minimal RDF for \(_{i}\) after fixing the latent variables \(_{S}\), and this exactly corresponds to the number of parents of \(_{i}\) that are not in \(_{S}\). We provide a simple example in Appendix E.2 to illustrate why this approach works.

The following result states that Algorithm 3 can recover the ground-truth causal model up to \(_{}\):

**Theorem 4**.: _Suppose that \(_{k},k[K]\) are perfectly identified in **Step 1**. Let \((},})\) be the solution returned by Algorithm 3, then we must have \((,)_{}(},})\)._

The full proof of Theorem 4 is given in Appendix H.3. It crucially relies on the following two propositions that reveal how Algorithm 3 and the subroutine Algorithm 2 work.

```
1:Input: Ordered set \(S=\{s_{1},s_{2},,s_{m}\}[d]\), index \(i S\), matrices \(_{k}^{d n}\), \(k[K]\)
2:Output: Set of vectors \(\{_{k}\}_{k=1}^{K}\)
3:for\(k 1\) to \(K\)do
4:\((_{k})_{s}:s S (_{k})_{s}\) is the \(s\)-th row of \(_{k}\)
5:\(_{k}_{^{}}((_{k})_{i})\)
6:endfor ```

**Algorithm 1**Orthogonal-projections

**Proposition 1**.: _The following two propositions hold for Algorithm 3:_

* \(_{}(i) S\) _the if condition in line 8 of Algorithm_ 3 _is fulfilled;_
* _the set_ \(S\) _maintained in Algorithm_ 3 _is always an ancestral set, in the sense that_ \(j S_{}(j) S\)_._

**Proposition 2**.: _Given any ordered ancestral set \(S\) that contains \(_{}(i)\) for some \(i S\), Algorithm 2 returns a set \(P_{i} S\) that is exactly \(_{}(i)\)._

```
1:Input: An ordered set \(S=\{s_{1},s_{2},,s_{m}\}[d]\), a node \(i S\) and matrices \(_{k},k[K]\)
2:Output: The parent set \(P_{i}\) of node \(i\)
3:\(P_{i}\)
4:for\(m^{} 0\) to \(m\)do
5:\(\{_{k}\}_{k=1}^{K}\)Orthogonal-projections\((\{s_{j}:j m^{}\},i,\{_{k}\}_{k[K]})\)
6:\(r_{m^{}}_{k}:k[ K]\)
7:if\(m^{} 1\) and \(r_{m^{}}=r_{m^{}-1}-1\)then
8:\(P_{i} P_{i}\{m^{}\}\)
9:endif
10:endfor ```

**Algorithm 2**Identify-Parents

## 6 Experiments

In this section, we present our experimental setup and results for LiNGCReL. Note that LiNGCReL as described in the previous section only works in the population regime. When the number of samples is limited, two main challenges in implementing LiNGCReL are to accurately compute the dimension of a subspace (line 6 of Algorithm 2 and line 8 of Algorithm 3), and to find a vector in the intersection of multiple subspaces (line 20, Algorithm 3). Due to space limit, the implementation details are described in Appendix B.2.

```
1:Input: Matrices \(_{k},k[K]\)
2:Output: The edge set \(\) on the vertex set \([d]\) and the mixing matrix \(}\)
3:\(S\); \(\)\(S\) is an ordered set of nodes
4:\(\); \(\)\(\) is the edge set
5:while\(|S|<d\)do
6:for\(i S\)do
7:\(\{_{k}\}_{k=1}^{K}(S,i,\{ _{k}\}_{k[K]})\)
8:if\(_{k}:k[K]=1\)then
9:breakable\(\) Proposition 1 guarantees that such an \(i\) must exist
10:endif
11:endfor
12:\(P_{i}(S,i)\)
13:\(S S\{i\}\)
14:\(\{(j,i):j P_{i}\}\)
15:endwhile
16:for\(i=1\) to \(d\)do
17:\(E_{i}(_{k})_{i}:k[K]\)
18:endfor
19:for\(i=1\) to \(d\)do
20:\(}_{i}\) any non-zero vector in \((_{j:(i,j)}E_{j}) E_{i}\)
21:endfor
22:\(}[}_{1}^{},}_{2}^{}, ,}_{d}^{}]^{}\) ```

**Algorithm 3**Learn-Causal-Model

**Experimental setup.** We generate the independent noise variables from generalized Gaussian distributions \(p_{}(x)(-|x|^{})\) with parameters \(_{k}=0.2k^{2},k=1,2,,d\), multiplied by normalization constants to make their variances equal to \(1\). The ground-truth causal graph is generated by first fixing a total order of the vertices, say \(1,2,,d\), then add directed edges \(i j(i<j)\) according to i.i.d. Bernoulli(\(p\)) distributions, where \(p(0,1)\). The non-zero entries of matrices \(_{k}\) and \(\) are all generated independently from Gaussian distributions. For simplicity, we focus on the case \(n=d\) since recovery of the latent graphs only requires information from \(d\) components of \(\).

**Metrics of estimation error.** Since CRL seeks to learn both the causal graphs and the latent variables, for each output of our algorithm we first check if it exactly recovers the ground-truth causal graph. Then, recall that the latent variables and the observations are related by \(=\), given any output unmixing matrix \(}\) from Algorithm 3, we define the relative estimation error \(_{i}\) for \(_{i}\) as the solution of the following optimization problem:

\[\|\|_{}& s.t._{i}=_{ (_{i};j}_{ }(i))}(}_{i})\|_{2}}{\|}_{i}\|_{2}}, \\ }=}.\] (5)

where signed permutation is allowed here since the noise distribution in our experiments is symmetric and the order of latent variables \(_{i},i=1,2,,d\) does not matter. We refer to the errors \(_{i}\) defined in (5) as the _SNA error_. The SNA error measures how much of the row \(}}_{i}\) that we learn is contained in the span of the ground-truth rows \(_{j},j_{}}(i)\). Indeed, recall that given any observation \(\), the ground-truth latent variable is \(=\) while our algorithm outputs \(}_{i}=}}_{i}^{}\), so the SNA error essentially captures whether the recovered latent variable is close to some linear mixture of latent variables in the effect-dominating set of \(i\). When the SNA error is zero for some node \(i\), we know that the recovered latent variable at node \(i\) is exactly a linear mixture of the ground-truth latent variables in \(_{}}(i)\), according to Lemma 1.

We also define the _true error_ for estimating each latent variable. Formally, let \(}\) be the unmixing matrix that corresponds to the solution of (5), then we define the true estimation error \(_{i}\) of \(_{i}\) as

\[_{i}=\|(-_{i}_{i}^{})}_{i}\|_{2}.\] (6)

**Results.** We randomly sample \(100\) causal models with size \(d=5\), \(30\) causal models with size \(d=8\) ad \(30\) causal models of size \(d=10\). In light of Theorem 1, for each \(d\{5,8,10\}\), we sample data from \(K=d\) randomly chosen environments; for \(d=5\) we also consider \(K=20\) to study how different choices of \(K\) can affect the result. We run LiNGCReL for each model with different sample sizes, compute the SNA error and true error of the obtained solution from (5) and (6) respectively for each latent variable, and check whether the ground-truth causal graph is exactly recovered.

Figure 2 shows how the average SNA error (over all latent variables) and the accuracy of graph recovery changes when sample size grows. We can see LiNGCReL successfully recovers about \(80\%\) of all models within each category, and the median of the average SNA error is smaller than \(1\%\). Moreover, by comparing Figure 1(a) with Figure 1(b), one can observe that if we fix the total number of samples but choose a larger \(K\) (_i.e._, fewer samples per environment), LiNGCReL can still achieve the same level of performance compared with the choice \(K=d\). Intuitively, this is because \(K d\) vectors sampled from an \(r(r d)\) dimensional subspace are unlikely to approximately lie in an \((r-1)\)-dimensional subspace, so that the calculation of line 6 of Algorithm 2 and line 8 of Algorithm 3 can be more accurate. We leave a better and quantitative understanding of the trade-off between \(d\) and \(K\) to future work.

**SNA error v.s. true error.** To understand the implication of our theory, we dive deeper by looking into the learning outcome of LiNGCReL on a specific model, of which the causal graph is shown in

Figure 2: _First two rows_: plots of SNA Error and graph recovery accuracy achieved by LiNGCReL as functions of sample size (per environment) for different choices of graph size \(d\) and number of environments \(K\). _Third row_: an example of causal graph generated in our experiments, and the estimation error of LiNGCReL for each node.

Figure 2e. In Figure 2f, we list the surrounding set of each node and the corresponding SNA error and true error. We can see that if \(_{}(i)=\), the two errors equal and both are small, but if \(_{}(i)\), the true error is much larger than the SNA error. This indicates that LiNGCReL indeed learns the ground-truth model up to \(_{}\), as Theorem 1 predicts.

## 7 Conclusions

This paper studies the limit of learning identifiable causal representations using data from multiple environments. When hard interventions are not available, we provide theory and algorithm for identification up to SNA, and also show that SNA is an intrinsic ambiguity in our setting.

It is interesting to further investigate the setting where we do not assume that the causal model is linear. Moreover, it is important to understand the concrete form of available interventions in real-world applications. For instance, it is suggested that for single-cell genomics, the intervention is sometimes a "mixture" of hard and soft interventions, and sometimes can even reverse the direction of an edge . Modelling such more complicated interventions appears to be crucial to reveal the underlying causal mechanisms in real-world problems.