# Stochastic Approximation Approaches to

Group Distributionally Robust Optimization

Lijun Zhang\({}^{1,2}\), Peng Zhao\({}^{1}\), Zhen-Hua Zhuang\({}^{1}\), Tianbao Yang\({}^{3}\), Zhi-Hua Zhou\({}^{1}\)

\({}^{1}\)National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China

\({}^{2}\)Peng Cheng Laboratory, Shenzhen 518055, China

\({}^{3}\)Department of Computer Science and Engineering, Texas A&M University, College Station, USA {zhanglj, zhaop, zhuangzh, zhouzh}@lamda.nju.edu.cn, tianbao-yang@tamu.edu

###### Abstract

This paper investigates group distributionally robust optimization (GDRO), with the purpose to learn a model that performs well over \(m\) different distributions. First, we formulate GDRO as a stochastic convex-concave saddle-point problem, and demonstrate that stochastic mirror descent (SMD), using \(m\) samples in each iteration, achieves an \(O(m( m)/^{2})\) sample complexity for finding an \(\)-optimal solution, which matches the \((m/^{2})\) lower bound up to a logarithmic factor. Then, we make use of techniques from online learning to reduce the number of samples required in each round from \(m\) to \(1\), keeping the same sample complexity. Specifically, we cast GDRO as a two-players game where one player simply performs SMD and the other executes an online algorithm for non-oblivious multi-armed bandits. Next, we consider a more practical scenario where the number of samples that can be drawn from each distribution is different, and propose a novel formulation of weighted GDRO, which allows us to derive _distribution-dependent_ convergence rates. Denote by \(n_{i}\) the sample budget for the \(i\)-th distribution, and assume \(n_{1} n_{2} n_{m}\). In the first approach, we incorporate non-uniform sampling into SMD such that the sample budget is satisfied in expectation, and prove that the excess risk of the \(i\)-th distribution decreases at an \(O( m}/n_{i})\) rate. In the second approach, we use mini-batches to meet the budget exactly and also reduce the variance in stochastic gradients, and then leverage stochastic mirror-prox algorithm, which can exploit small variances, to optimize a carefully designed weighted GDRO problem. Under appropriate conditions, it attains an \(O(( m)/})\) convergence rate, which almost matches the optimal \(O(})\) rate of only learning from the \(i\)-th distribution with \(n_{i}\) samples.

## 1 Introduction

In the classical statistical machine learning, our goal is to minimize the risk with respect to a _fixed_ distribution \(_{0}\)(Vapnik, 2000), i.e.,

\[_{}\ \{R_{0}()=_{ _{0}}(;) \},\] (1)

where \(\) is a sample drawn from \(_{0}\), \(\) denotes a hypothesis class, and \((;)\) is a loss measuring the prediction error of model \(\) on \(\). During the past decades, various algorithms have been developed to optimize (1), and can be grouped in two categories: sample average approximation (SA) and stochastic approximation (SA) (Kushner and Yin, 2003). In SAA, we minimize an empirical risk defined as the average loss over a set of samples drawn from \(_{0}\), and in SA, we directly solve the original problem by using stochastic observations of the objective \(R_{0}()\).

However, a model that trained over a single distribution may lack robustness in the sense that (i) it could suffer high error on minority subpopulations, though the average loss is small; (ii) its performance could degenerate dramatically when tested on a different distribution. Distributionally robust optimization (DRO) provides a principled way to address those limitations by minimizing the worst-case risk in a neighborhood of \(_{0}\)(Ben-Tal et al., 2013). Recently, it has attracted great interest in optimization (Shapiro, 2017), statistics (Duchi and Namkoong, 2021), operations research (Duchi et al., 2021), and machine learning (Hu et al., 2018; Jin et al., 2021; Agarwal and Zhang, 2022). In this paper, we consider an emerging class of DRO problems, named as Group DRO (GDRO) which optimizes the maximal risk over a finite number of distributions (Oren et al., 2019; Sagawa et al., 2020). Mathematically, GDRO can be formulated as a minimax stochastic problem:

\[_{}_{i[m]}\ R_{i}()= _{_{i}}(;) }\] (2)

where \(_{1},,_{m}\) denote \(m\) distributions. A motivating example is federated learning, where a centralized model is deployed at multiple clients, each of which faces a (possibly) different data distribution (Mohri et al., 2019).

Supposing that samples can be drawn from all distributions freely, we develop efficient SA approaches for (2), in favor of their light computations over SAA methods. As elaborated by Nemirovski et al. (2009, SS3.2), we can cast (2) as a stochastic convex-concave saddle-point problem:

\[_{}_{_{m}}\ ( ,)=_{i=1}^{m}q_{i}R_{i}()}\] (3)

where \(_{m}=\{^{m}: 0,_{i=1}^{m}q_{i}=1\}\) is the \((m-1)\)-dimensional simplex, and then solve (3) by their mirror descent stochastic approximation method, namely stochastic mirror descent (SMD). In fact, several recent studies have adopted this (or similar) strategy to optimize (3). But, unfortunately, we found that existing results are unsatisfactory because they either deliver a loose sample complexity (Sagawa et al., 2020), suffer subtle dependency issues in their analysis (Haghtalab et al., 2022; Soma et al., 2022), or hold only in expectation (Carmon and Hausler, 2022).

As a starting point, we first provide a routine application of SMD to (3), and discuss the theoretical guarantees. In each iteration, we draw \(1\) sample from every distribution to construct unbiased estimators of \(R_{i}()\) and its gradient, and then update both \(\) and \(\) by SMD. The proposed method achieves an \(O()\) convergence rate in expectation and with high probability, where \(T\) is the number of iterations. As a result, we obtain an \(O(m( m)/^{2})\) sample complexity for finding an \(\)-optimal solution of (3), which matches the \((m/^{2})\) lower bound (Soma et al., 2022, Theorem 5) up to a logarithmic factor, and tighter than the \(O(m^{2}( m)/^{2})\) bound of Sagawa et al. (2020) by an \(m\) factor. While being straightforward, this result seems _new_ for GDRO.

Then, we proceed to reduce the number of samples used in each iteration from \(m\) to \(1\). We remark that a naive uniform sampling over \(m\) distributions does not work well, and yields a worse sample complexity (Sagawa et al., 2020). As an alternative, we borrow techniques from online learning with stochastic observations, and explicitly tackle the _non-oblivious_ nature of the online process, which distinguishes our method from that of Soma et al. (2022). Specifically, we use SMD to update \(\), and Exp3-IX, an algorithm for non-oblivious multi-armed bandits (MAB) (Neu, 2015), with stochastic rewards to update \(\). In this way, our algorithm only needs \(1\) sample in each round and attains an \(O()\) convergence rate, implying the same \(O(m( m)/^{2})\) sample complexity.

Next, we investigate a more practical and challenging scenario in which there are different budgets of samples that can be drawn from each distribution, a natural phenomenon encountered in learning with imbalanced data (Amodei et al., 2016). Let \(n_{i}\) be the sample budget of the \(i\)-th distribution, and without loss of generality, we assume that \(n_{1} n_{2} n_{m}\). Now, the goal is not to attain the optimal sample complexity, but to reduce the risk on all distributions as much as possible, under the budget constraint. For GDRO with different budgets, we develop two SA approaches based on non-uniform sampling and mini-batches, respectively.

In each iteration of the first approach, we draw \(1\) sample from every \(_{i}\) with probability \(n_{i}/n_{1}\), and then construct stochastic gradients to perform mirror descent. In this way, the budget will be satisfied in expectation after \(n_{1}\) rounds. To analyze its performance, we propose a novel formulation of weighted GDRO, which weights each risk \(R_{i}()\) in (3) by a scale factor \(p_{i}\). Then, our algorithm can be regarded as SMD for an instance of weighted GDRO. With the help of scale factors, we demonstrate that the proposed algorithm enjoys _distribution-dependent_ convergence in the sense that it converges faster for distributions with more samples. In particular, the excess risk on distribution \(_{i}\) reduces at an \(O( m}/n_{i})\) rate, and for \(_{1}\), it becomes \(O(})\), which almost matches the optimal \(O(})\) rate of learning from a single distribution with \(n_{1}\) samples.

On the other hand, for distribution \(_{i}\) with budget \(n_{i}<n_{1}\), the above \(O( m}/n_{i})\) rate is worse than the \(O(})\) rate obtained by learning from \(_{i}\) alone. In shape contrast with this limitation, our second approach yields nearly optimal convergence rates for _multiple_ distributions across a large range of budgets. To meet the budget constraint, it runs for \( n_{m}\) rounds, and in each iteration, draws a mini-batch of \(n_{i}/\) samples from every distribution \(_{i}\). As a result, (i) the budget constraint is satisfied _exactly_; (ii) for distributions with a larger budget, the associated risk function can be estimated more accurately, making the variance of the stochastic gradient smaller. To benefit from the small variance, we leverage stochastic mirror-prox algorithm (Juditsky et al., 2011), instead of SMD, to update our solutions, and again make use of the weighted GDRO formulation to obtain distribution-wise convergence rates. Theoretical analysis shows that the excess risk converges at an \(O((}+}}) m)\) rate for each \(_{i}\). Thus, we obtain a nearly optimal \(O(( m)/})\) rate for distributions \(_{i}\) with \(n_{i} n_{m}^{2}\), and an \(O(( m)/n_{m})\) rate otherwise. Note that the latter rate is as expected since the algorithm only updates \(O(n_{m})\) times.

Related workWe briefly discuss the related works on the GDRO problem in (2)/(3) and will review the traditional DRO problem in Appendix A. Sagawa et al. (2020) have applied SMD (Nemirovski et al., 2009) to (3), but only obtain a sub-optimal sample complexity. In the sequel, Haghtalab et al. (2022) and Soma et al. (2022) have tried to improve the sample complexity by reusing samples and applying techniques from MAB respectively, but their analysis suffers dependency issues. Carmon and Hausler (2022, Proposition 2) successfully established an \(O(m( m)/^{2})\) sample complexity by combining SMD and gradient clipping, but their result holds only in expectation. To deal with heterogeneous noise in different distributions, Agarwal and Zhang (2022) propose a variant of GDRO named as minimax regret optimization (MRO), which replaces the risk with "excess risk".

## 2 SA Approaches to GDRO

In this section, we provide two efficient SA approaches for GDRO, which are equipped with the same sample complexity, but with different number of samples used in each round (\(m\) versus \(1\)).

### Preliminaries

First, we state the general setup of mirror descent (Nemirovski et al., 2009). We equip the domain \(\) with a distance-generating function \(_{w}()\), which is \(1\)-strongly convex with respect to certain norm \(\|\|_{w}\). We define the Bregman distance associated with \(_{w}()\) as \(B_{w}(,)=_{w}()-[_{w}()+ _{w}(),-]\). For the simplex \(_{m}\), we choose the entropy function \(_{q}()=_{i=1}^{m}q_{i} q_{i}\), which is \(1\)-strongly convex with respect to the vector \(_{1}\)-norm \(\|\|_{1}\), as the distance-generating function. Similarly, \(B_{q}(,)\) is the Bregman distance associated with \(_{q}()\).

Then, we introduce the standard assumptions about the domain, and the loss function.

**Assumption 1**: _The domain \(\) is convex and its diameter measured by \(_{w}()\) is bounded by \(D\), i.e.,_

\[_{}_{w}()-_{ }_{w}() D^{2}.\] (4)

For \(_{m}\), it is easy to verify that its diameter measured by the entropy function is bounded by \(\).

**Assumption 2**: _For all \(i[m]\), the risk function \(R_{i}()=_{_{i}}[( ;)]\) is convex._

**Assumption 3**: _For all \(i[m]\), we have_

\[0(;) 1,\ ,\  _{i}.\] (5)

**Assumption 4**: _For all \(i[m]\), we have_

\[\|(;)\|_{w,*} G,\  ,\ _{i}\] (6)

_where \(\|\|_{w,*}\) is the dual norm of \(\|\|_{w}\)._Last, we discuss the performance measure. To analyze the convergence property, we measure the quality of an approximate solution \((},})\) to (3) by the error

\[_{}(},})=_{_{m} }(},)-_{}(,})\] (7)

which directly controls the optimality of \(}\) to the original problem (2), since

\[_{i[m]}R_{i}(})-_{}_{i [m]}R_{i}()_{_{m}}_{i=1}^{m}q_{i} R_{i}(})-_{}_{i=1}^{m}_{i} R_{i}()=_{}(},}).\] (8)

### Stochastic Mirror Descent for GDRO

To apply SMD, the key is to construct stochastic gradients of the function \((,)\) in (3). In each round \(t\), denote by \(_{t}\) and \(_{t}\) the current solutions. We draw one sample \(_{t}^{(i)}\) from every distribution \(_{i}\), and define stochastic gradients as

\[_{w}(_{t},_{t})=_{i=1}^{m}q_{t,i} (_{t};_{t}^{(i)}),_{q}(_{t}, _{t})=[(_{t};_{t}^{(1)}),,(_{t};_{t}^{(m)})]^{}.\] (9)

It is worth mentioning that the construction of \(_{w}(_{t},_{t})\) can be further simplified to

\[}_{w}(_{t},_{t})=(_{ t};_{t}^{(i_{t})})\] (10)

where \(i_{t}[m]\) is drawn randomly according to the probability \(_{t}\).

Then, we utilize SMD to update \(_{t}\) and \(_{t}\):

\[_{t+1}= *{argmin}_{}_{q }_{w}(_{t},_{t}),-_{ t}+B_{w}(,_{t})},\] (11) \[_{t+1}= *{argmin}_{_{m}}_{q }-_{q}(_{t},_{t}),-_ {t}+B_{q}(,_{t})}\] (12)

where \(_{w}>0\) and \(_{q}>0\) are two step sizes that will be determined later. The updating rule of \(_{t}\) depends on the choice of the distance-generating function \(_{w}()\). Since \(B_{q}(,_{t})\) is defined in terms of the negative entropy, (12) is equivalent to

\[q_{t+1,i}=_{q}(_{t};_{t}^{( i)})}{_{j=1}^{m}q_{t,j}_{q}(_{t}; _{t}^{(j)})},\; i[m]\] (13)

which is the Hedge algorithm (Freund and Schapire, 1997) applied to a maximization problem. In the beginning, we set \(_{1}=*{argmin}_{}_{w}( )\), and \(_{1}=_{m}\), where \(_{m}\) is the \(m\)-dimensional vector consisting of \(1\)'s. In the last step, we return the averaged iterates \(}=_{t=1}^{T}_{t}\) and \(}=_{t=1}^{T}_{t}\) as final solutions. The completed procedure is summarized in Algorithm 1.

```
0: Two step sizes: \(_{w}\) and \(_{q}\)
1: Initialize \(_{1}=*{argmin}_{}_{w}( )\), and \(_{1}=[1/m,,1/m]^{}^{m}\)
2:for\(t=1\) to \(T\)do
3: For each \(i[m]\), draw a sample \(_{t}^{(i)}\) from distribution \(_{i}\)
4: Construct the stochastic gradients defined in (9)
5: Update \(_{t}\) and \(_{t}\) according to (11) and (12), respectively
6:endfor
7:return\(}=_{t=1}^{T}_{t}\) and \(}=_{t=1}^{T}_{t}\) ```

**Algorithm 1** Stochastic Mirror Descent for GDRO

Based on the theoretical guarantee of SMD (Nemirovski et al., 2009, SS3.1), we have the following theorem.

**Theorem 1**: _Under Assumptions 1, 2, 3 and 4, and setting \(_{w}=D^{2}G^{2}+ m)}}\) and \(_{q}=( m)G^{2}+ m)}}\) in Algorithm 1, with probability at least \(1-\), we have_

\[_{}(},})(8+2)G^{2}+ m)}{T}}.\]

**Remark 1**Theorem 1 shows that Algorithm 1 achieves an \(O()\) convergence rate. Since it consumes \(m\) samples per iteration, the sample complexity is \(O(m( m)/^{2})\), which nearly matches the \((m/^{2})\) lower bound (Soma et al., 2022, Theorem 5). Due to space limitations, we defer all the proofs to Appendix B, and omit expectation bounds.

Comparisons with Sagawa et al. (2020)In their stochastic algorithm, Sagawa et al. (2020) generate a random index \(i_{t}[m]\) uniformly in each round \(t\), and draw \(1\) sample \(_{t}^{(i_{t})}\) from \(_{i_{t}}\). The stochastic gradients are constructed as follows:

\[}_{w}(_{t},_{t})=q_{t,i_{t}}m( _{t};_{t}^{(i_{t})}),}_{q}(_{t}, _{t})=[0,,m(_{t};_{t}^{(i_{t})}), ,0]^{}\] (14)

where \(}_{g}(_{t},_{t})\) is a vector with \(m(_{t};_{t}^{(i_{t})})\) in position \(i_{t}\) and \(0\) elsewhere. Then, the two stochastic gradients are used to update \(_{t}\) and \(_{t}\), in the same way as (11) and (12). However, it only attains a slow convergence rate: \(O(m)\), leading to an \(O(m^{2}( m)/^{2})\) sample complexity, which is higher than that of Algorithm 1 by a factor of \(m\). The slow convergence is due to the fact that the optimization error depends on the dual norm of the stochastic gradients in (14), which blows up by a factor of \(m\), compared with the gradients in (9).

Comparisons with Haghtalab et al. (2022)To reduce the number of samples required in each round, Haghtalab et al. (2022) propose to reuse samples for multiple iterations. To approximate \(_{}(_{t},_{t})\), they construct the stochastic gradient \(}_{w}(_{t},_{t})\) in the same way as (10), which needs \(1\) sample. To approximate \(_{}(_{t},_{t})\), they draw \(m\) samples \(_{}^{(1)},,_{}^{(m)}\), one from each distribution, at round \(=mk+1\), \(k\), and reuse them for \(m\) iterations to construct

\[_{q}^{}(_{t},_{t})=[(_{t}; _{}^{(1)}),,(_{t};_{}^{(m)})] ^{},\ t=,,+m-1.\] (15)

Then, they treat \(}_{w}(_{t},_{t})\) and \(_{q}^{}(_{t},_{t})\) as stochastic gradients, and update \(_{t}\) and \(_{t}\) by SMD. In this way, their algorithm uses \(2\) samples on average in each iteration. However, the gradient in (15) is no longer an unbiased estimator of the true gradient \(_{q}(_{t},_{t})\) at rounds \(t=+2,,+m-1\), making their analysis ungrounded. To see this, from the updating rule of SMD, we know that \(_{+2}\) depends on \(_{+1}\), which in turn depends on the \(m\) samples drawn at round \(\), and thus

\[[(_{+2};_{}^{(i)})] R _{i}(_{+2}),\ i=1,,m.\]

**Remark 2** As shown in (10), we can use \(1\) sample to construct a stochastic gradient for \(_{t}\) with small norm, since \(\|}_{w}(_{t},_{t})\|_{w,*} G\) under Assumption 4. Thus, it is relatively easy to control the error related to \(_{t}\). However, we do not have such guarantees for the stochastic gradient of \(_{t}\). Recall that the infinity norm of \(}_{q}(_{t},_{t})\) in (14) is upper bounded by \(m\). The reason is that we insist on the unbiasedness of the stochastic gradient, which leads to a large variance. In the next section, we borrow techniques from online learning to better balance the bias and the variance.

### Non-oblivious Online Learning for GDRO

In the studies of convex-concave saddle-point problems, it is now well-known that they can be solved by playing two online learning algorithms against each other (Freund and Schapire, 1999, Rakhlin and Sridharan, 2013, Syrgkanis et al., 2015). With this transformation, we can exploit no-regret algorithms developed in online learning to bound the optimization error. To solve problem (3), we ask the 1st player to minimize a sequence of convex functions

\[(,_{1})=_{i=1}^{m}q_{1,i}R_{i}(),\ \ (,_{2})=_{i=1}^{m}q_{2,i}R_{i}(),\ ,\ (,_{T})=_{i=1}^{m}q_{T,i}R_{i}()\]

subject to the constraint \(\), and the 2nd player to maximize a sequence of linear functions

\[(_{1},)=_{i=1}^{m}q_{i}R_{i}(_{1}),\ \ ( _{2},)=_{i=1}^{m}q_{i}R_{i}(_{2}),\ ,\ (_{T},)=_{i=1}^{m}q_{i}R_{i}( _{T})\]

subject to the constraint \(_{m}\). We highlight that there exists an important difference between our stochastic convex-concave problem and its deterministic counterpart. Here, the two players cannotdirectly observe the loss function, and can only approximate \(R_{i}()=_{_{i}}(;)\) by drawing samples from \(_{i}\). The stochastic setting makes the problem more challenging, and in particular, we need to take care of the _non-oblivious_ nature of the learning process. Here, "non-oblivious" refers to the fact that the online functions depend on the past decisions of the learner.

Next, we discuss the online algorithms that will be used by the two players. As shown in Section 2.2, the 1st player can easily obtain a stochastic gradient with small norm by using \(1\) sample. So, we model the problem faced by the 1st player as "non-oblivious online convex optimization (OCO) with stochastic gradients", and still use SMD to update its solution. In each round \(t\), with \(1\) sample drawn from \(_{i}\), the 2nd player can estimate the value of \(R_{i}(_{t})\) which is the coefficient of \(q_{i}\). Since the 2nd player is maximizing a linear function over the simplex, the problem can be modeled as "non-oblivious multi-armed bandits (MAB) with stochastic rewards". And fortunately, we have powerful online algorithms for non-oblivious MAB (Auer et al., 2002; Lattimore and Szepesvari, 2020), whose regret has a sublinear dependence on \(m\). In this paper, we choose the Exp3-IX algorithm (Neu, 2015), and generalize its theoretical guarantee to stochastic rewards.

The complete procedure is presented in Algorithm 2, and we explain key steps below. In each round \(t\), we generate an index \(i_{t}[m]\) from the probability distribution \(_{t}\), and then draw a sample \(_{t}^{(i_{t})}\) from the distribution \(_{i_{t}}\). With the stochastic gradient in (10), we use SMD to update \(_{t}\):

\[_{t+1}=*{argmin}_{} _{w}}_{w}(_{t},_{t}),-_{t}+B_{w}(,_{t})}\] (16)

Then, we reuse the sample \(_{t}^{(i_{t})}\) to update \(_{t}\) according to Exp3-IX, which first constructs the Implicit-eXploration (IX) loss estimator (Kocak et al., 2014):

\[_{t,i}=_{t},_{t}^{(i_{t})})}{q_{t,i }+}[i_{t}=i],\; i[m],\] (17)

where \(>0\) is the IX coefficient and \([A]\) equals to \(1\) when the event \(A\) is true and \(0\) otherwise, and then performs a mirror descent update:

\[_{t+1}=*{argmin}_{_{m}} _{q}}_{t},-_{t} +B_{q}(,_{t})}.\] (18)

We present the theoretical guarantee of Algorithm 2.

**Theorem 2**: _Under Assumptions 1, 2, 3 and 4, and setting \(_{w}=}\), \(_{q}=}\) and \(=}{2}\) in Algorithm 2, with probability at least \(1-\), we have_

\[_{}(},})\] (19) \[ DG}(2+8} )+3}+}+(}+}+).\]

**Remark 3**: The above theorem shows that with \(1\) sample per iteration, Algorithm 2 is able to achieve an \(O()\) convergence rate, thus maintaining the \(O(m( m)/^{2})\) sample complexity.

Comparisons with Soma et al. (2022)In a recent work, Soma et al. (2022) have deployed online algorithms to optimize \(\) and \(\), but did not consider the non-oblivious property. As a result, their theoretical guarantees, which build upon the analysis for oblivious online learning (Orabona, 2019), cannot justify the optimality of their algorithm for (3). Specifically, their results imply that for any _fixed_\(\) and \(\) that are independent from \(}\) and \(}\)(Soma et al., 2022, Theorem 3),

\[[(},)-(,})]=O(}).\] (20)

However, (20) cannot be used to bound \(_{}(},})\) in (7), because of the dependency issue. To be more clear, we have \(_{}(},})=_{_ {m}}(},)-_{}( ,})=(},})-( },})\), where \(}=*{argmin}_{}( },})\) and \(}=*{argmax}_{_{m}}(},)\)_depend_ on \(}\) and \(}\).

**Remark 4**: After we pointed out the issue of reusing samples, Haghtalab et al. (2023) modified their method by incorporating bandits algorithms to optimize \(\). From our understanding, the idea of applying bandits to GDRO is _firstly_ proposed by Soma et al. (2022), and subsequently refined by us.

## 3 Weighted GDRO and SA Approaches

When designing SA approaches for GDRO, it is common to assume that the algorithms are free to draw samples from every distribution (Sagawa et al., 2020), as we do in Section 2. However, this assumption may not hold in practice. For example, data collection costs can vary widely among distributions (Radivojac et al., 2004), and data collected from various channels can have different throughputs (Zhou, 2023). In this section, we investigate the scenario where the number of samples can be drawn from each distribution could be different. Denote by \(n_{i}\) the number of samples that can be drawn from \(_{i}\). Without loss of generality, we assume that \(n_{1} n_{2} n_{m}\). Note that we have a straightforward **Baseline** which just runs Algorithm 1 for \(n_{m}\) iterations, and the optimization error \(_{}(},})=O(})\).

### Stochastic Mirror Descent with Non-uniform Sampling

To meet the budget, we propose to incorporate non-uniform sampling into SMD. Specifically, in round \(t\), we first generate a set of Bernoulli random variables \(\{b_{t}^{(1)},,b_{t}^{(m)}\}\) with \([b_{t}^{(i)}=1]=p_{i}\) to determine whether to sample from each distribution. If \(b_{t}^{(i)}=1\), we draw a sample \(_{t}^{(i)}\) from \(_{i}\). Now, the question is how to construct stochastic gradients from those samples. Let \(_{t}=\{i|b_{t}^{(i)}=1\}\) be the indices of selected distributions. If we stick to the original problem in (3), then the stochastic gradients should be constructed in the following way

\[_{w}(_{t},_{t})=_{i C_{t}}}{ p_{i}}(_{t};_{t}^{(i)}),[_{q}(_{t},_{t})]_{i}=\{ (_{t};_{t}^{(i)})/p_{i},&i_{t}\\ 0,&.\] (21)

to ensure unbiasedness. Then, they can be used by SMD to update \(_{t}\) and \(_{t}\). After \(n_{1}\) iterations, the _expected_ number of samples drawn from \(_{i}\) will be \(n_{1}p_{i}=n_{i}\), and thus the budget is satisfied in expectation. To analyze the optimization error, we need to bound the norm of stochastic gradients in (21). To this end, we have \(\|_{w}(_{t},_{t})\|_{w,*} Gn_{1}/n_{m}\) and \(\|_{q}(_{t},_{t})\|_{} n_{1}/n_{m}\). Following the arguments of Theorem 1, we can prove that the error \(_{}(},})=O(}  n_{1}/n_{m})=O(\, m}/n_{m})\), which is even larger than the \(O(})\) error of the Baseline.

In the following, we demonstrate that a simple twist of the above procedure can still yield meaningful results that are complementary to the Baseline. We observe that the large norm of the stochastic gradients in (21) is caused by the inverse probability \(1/p_{i}\). A natural idea is to ignore \(1/p_{i}\), and define the following stochastic gradients:

\[_{w}(_{t},_{t})=_{i C_{t}}q_{t,i} (_{t};_{t}^{(i)}),[_{q}(_{t},_{t})]_{i}=\{ (_{t};_{t}^{(i)}),&i_ {t}\\ 0,&..\] (22)

In this way, they are no longer stochastic gradients of (3), but can be treated as stochastic gradients of a weighted GDRO problem:

\[_{}_{_{m}}\ \{(,)=_{i=1}^{m}q_{i}p_{i}R_{i}( )\}\] (23)where each risk \(R_{i}()\) is scaled by a factor \(p_{i}\). Based on the gradients in (22), we still use (11) and (12) to update \(_{t}\) and \(_{t}\). We summarize the complete procedure in Algorithm 3.

```
1:Initialize \(_{1}=*{argmin}_{}_{w}( )\), and \(_{1}=[1/m,,1/m]^{}^{m}\)
2:for\(t=1\) to \(n_{1}\)do
3: For each \(i[m]\), generate a Bernoulli random variable \(b_{t}^{(i)}\) with \([b_{t}^{(i)}=1]=p_{i}\), and if \(b_{t}^{(i)}=1\), draw a sample \(_{t}^{(i)}\) from distribution \(_{i}\)
4: Construct the stochastic gradients defined in (22)
5: Update \(_{t}\) and \(_{t}\) according to (11) and (12), respectively
6:endfor
7:return\(}=}_{t=1}^{n_{1}}_{t}\) and \(}=}_{t=1}^{n_{1}}_{t}\) ```

**Algorithm 3** Stochastic Mirror Descent for Weighted GDRO

We omit the optimization error of Algorithm 3 for (23), since it has exactly the same form as Theorem 1. What we are really interested in is the theoretical guarantee of its solution on multiple distributions. To this end, we have the following theorem.

**Theorem 3**: _Under Assumptions 1, 2, 3 and 4, and setting \(_{w}=D^{2}(D^{2}G^{2}+ m)}}\) and \(_{q}=( m)(D^{2}G^{2}+ m)}}\) in Algorithm 3, with probability at least \(1-\), we have_

\[R_{i}(})-}{n_{i}}p_{}^{*}} ()G^{2}+ m)}{n_{1}}}=()G^{2}+ m)n_{1}}}{n_{i}},\; i[m]\]

_where \(p_{}^{*}\) is the optimal value of (23) and \(()=8+2\)._

**Remark 5**: We see that Algorithm 3 exhibits a _distribution-dependent_ convergence behavior: The larger the number of samples \(n_{i}\), the smaller the target risk \(n_{1}p_{}^{*}/n_{i}\), and the faster the convergence rate \(O( m}/n_{i})\). Note that its rate is always better than the \(O( m}/n_{m})\) rate of SMD with (21) as gradients. Furthermore, it converges faster than the Baseline when \(n_{i}n_{m}}\). In particular, for distribution \(_{1}\), Algorithm 3 attains an \(O(})\) rate, which almost matches the optimal \(O(})\) rate of learning from a single distribution. Finally, we would like to emphasize that a similar idea of introducing "scale factors" has been used by Juditsky et al. (2011, SS4.3.1) for stochastic semidefinite feasibility problems and Agarwal and Zhang (2022) for empirical MRO.

### Stochastic Mirror-Prox Algorithm with Mini-batches

In Algorithm 3, distributions with more samples take their advantage by appearing more frequently in the stochastic gradients. In this section, we propose a different way, which lets them reduce the variance in the elements of stochastic gradients by mini-batches (Roux et al., 2008).

The basic idea is as follows. We run our algorithm for a small number of iterations \(\) that is no larger than \(n_{m}\). Then, in each iteration, we draw a mini-batch of \(n_{i}/\) samples from every distribution \(_{i}\). For \(_{i}\) with more samples, we can estimate the associated risk \(R_{i}()\) and its gradient more accurately, i.e., with a smaller variance. However, to make this idea work, we need to tackle two obstacles: (i) the performance of the SA algorithm should depend on the variance of gradients instead of the norm, and for this reason SMD is unsuitable; (ii) even some elements of the stochastic gradient have small variances, the entire gradient may still have a large variance. To address the first challenge, we resort to a more advanced SA approach--stochastic mirror-prox algorithm (SMPA), whose convergence rate depends on the variance (Juditsky et al., 2011). To overcome the second challenge, we again introduce scale factors into the optimization problem and the stochastic gradients.

In SMPA, we need to maintain two sets of solutions: \((_{t},_{t})\) and \((_{t}^{},_{t}^{})\). In each round \(t\), we first draw \(n_{i}/n_{m}\) samples from every distribution \(_{i}\), denoted by \(_{t}^{(i,j)}\), \(j=1,,n_{i}/n_{m}\). Then, we use them to construct stochastic gradients at \((_{t}^{},_{t}^{})\) of a weighted GDRO problem (23), where the value of \(p_{i}\) will be determined later. Specifically, we define

\[_{w}(_{t}^{},_{t}^{ })=&_{i=1}^{m}q_{t,i}^{}p_{i}(}{n_{ i}}_{j=1}^{n_{i}/n_{m}}(_{t}^{};_{t}^{(i,j)}) ),\\ _{q}(_{t}^{},_{t}^{})=& [p_{1}}{n_{1}}_{j=1}^{n_{1}/n_{m}}(_{t}^{ };_{t}^{(1,j)}),,p_{m}(_{t}^{}; _{t}^{(m)})]^{}.\] (24)

Let's take the stochastic gradient \(_{q}(_{t}^{},_{t}^{})\), whose variance will be measured in terms of \(\|\|_{}\), as an example to explain the intuition of inserting \(p_{i}\). Define \(u_{i}=}{n_{i}}_{j=1}^{n_{i}/n_{m}}(_{t}^{} ;_{t}^{(i,j)})\). With a larger mini-batch size \(n_{i}/n_{m}\), \(u_{i}\) will approximate \(R_{i}(_{t}^{})\) more accurately, and thus have a smaller variance. Then, it allows us to insert a larger value of \(p_{i}\), without affecting the variance of \(\|_{q}(_{t}^{},_{t}^{})\|_{}\), since \(\|\|_{}\) is insensitive to perturbations of small elements. Similar to the case in Theorem 3, the convergence rate of \(R_{i}()\) depends on \(1/p_{i}\), and becomes faster if \(p_{i}\) is larger.

Based on (24), we use SMD to update \((_{t}^{},_{t}^{})\), and denote the solution by \((_{t+1},_{t+1})\):

\[_{t+1}= *{argmin}_{}_{ w}_{w}(_{t}^{},_{t}^{}), -_{t}^{}+B_{w}(,_{t}^{ })},\] (25) \[_{t+1}= *{argmin}_{_{m}}_{q} -_{q}(_{t}^{},_{t}^{}), -_{t}^{}+B_{q}(,_{t}^{ })}.\] (26)

Next, we draw another \(n_{i}/n_{m}\) samples from each distribution \(_{i}\), denoted by \(}_{t}^{(i,j)}\), \(j=1,,n_{i}/n_{m}\), to construct stochastic gradients at \((_{t+1},_{t+1})\):

\[_{w}(_{t+1},_{t+1})=& _{i=1}^{m}q_{t+1,i}p_{i}(}{n_{i}}_{j=1}^{n_ {i}/n_{m}}(_{t+1};}_{t}^{(i,j)})), \\ _{q}(_{t+1},_{t+1})=& [p_{1}}{n_{1}}_{j=1}^{n_{1}/n_{m}}( _{t+1};}_{t}^{(1,j)}),,p_{m}(_{t +1};}_{t}^{(m)})]^{}.\] (27)

Then, we use them to update \((_{t}^{},_{t}^{})\) again, and denote the result by \((_{t+1}^{},_{t+1}^{})\):

\[_{t+1}^{}= *{argmin}_{}_{ w}_{w}(_{w}(_{t+1},_{t+1}), -_{t}^{}+B_{w}(,_{t}^{ })},\] (28) \[_{t+1}^{}= *{argmin}_{_{m}}_{q} -_{q}(_{t+1},_{t+1}),-_{t}^{}+B_{q}(,_{t}^{})}.\] (29)

To meet the budget constraints, we repeat the above process for \(n_{m}/2\) iterations. Finally, we return \(}=}_{t=2}^{1+n_{m}/2}_{t}\) and \(}=}_{t=2}^{1+n_{m}/2}_{t}\) as solutions. The completed procedure is summarized in Algorithm 4.

```
1:Initialize \(_{t}^{}=*{argmin}_{} _{w}()\), and \(_{t}^{}=[1/m,,1/m]^{}^{m}\)
2:for\(t=1\) to \(n_{m}/2\)do
3: For each \(i[m]\), draw \(n_{i}/n_{m}\) samples \(\{_{t}^{(i,j)}:j=1,,n_{i}/n_{m}\}\) from distribution \(_{i}\)
4: Construct the stochastic gradients defined in (24)
5: Calculate \(_{t+1}\) and \(_{t+1}\) according to (25) and (26), respectively
6: For each \(i[m]\), draw \(n_{i}/n_{m}\) samples \(\{}_{t}^{(i,j)}:j=1,,n_{i}/n_{m}\}\) from distribution \(_{i}\)
7: Construct the stochastic gradients defined in (27)
8: Calculate \(_{t+1}^{}\) and \(_{t+1}^{}\) according to (28) and (29), respectively
9:endfor
10:return\(}=}_{t=2}^{1+n_{m}/2}_{t}\) and \(}=}_{t=2}^{1+n_{m}/2}_{t}\) ```

**Algorithm 4** Stochastic Mirror-Prox Algorithm for Weighted GDRO

To analysis the performance of Algorithm 4, we further assume the risk function \(R_{i}()\) is smooth, and the dual norm \(\|\|_{w,*}\) satisfies a regularity condition.

**Assumption 5**: _All the risk functions are \(L\)-smooth, i.e.,_

\[\| R_{i}()- R_{i}(^{})\|_{w,*} L\| -^{}\|_{w},\ ,^{},i[m].\] (30)

Note that even in the studies of stochastic convex optimization (SCO), smoothness is necessary to obtain a variance-based convergence rate (Lan, 2012).

**Assumption 6**: _The dual norm \(\|\|_{w,*}\) is \(\)-regular for some small constant \( 1\)._

The regularity condition is used when analyzing the effect of mini-batches on stochastic gradients. For a formal definition, please refer to Juditsky and Nemirovski (2008). Assumption 6 is satisfied by most of papular norms considered in the literature, such as the vector \(_{p}\)-norm and the infinity norm.

Then, we have the following theorem for Algorithm 4.

**Theorem 4**: _Define_

\[& p_{}=_{i[m]}p_{i},_{}= _{i[m]}^{2}n_{m}}{n_{i}},\\ &=2p_{}(D^{2}L+D^{2}G), { and }^{2}=2c_{}( D^{2}G^{2}+^{2}m)\] (31)

_where \(c>0\) is an absolute constant. Under Assumptions 1, 2, 3, 4, 5 and 6, and setting_

\[_{w}=2D^{2}(},n_{m}}}),_{q}=2( },n_{m}}}) m\]

_in Algorithm 4, with probability at least \(1-\), we have_

\[R_{i}(})-}p_{}^{*}=}( }{n_{m}}+}{n_{m}}}(14}+7}+}))\]

_where \(p_{}^{*}\) is the optimal value of (23). Furthermore, by setting \(p_{i}\) as_

\[p_{i}=}+1}{1/}+/n_{i}}},\] (32)

_with high probability, we have_

\[R_{i}(})-}p_{}^{*}=O((}+}})m}).\]

**Remark 6**: Compared with Algorithm 3, Algorithm 4 has two advantages: (i) the budget constraint is satisfied exactly; (ii) we obtain a faster \(O(( m)/})\) rate for all distributions \(_{i}\) such that \(n_{i} n_{m}^{2}\), which is much better than the \(O( m}/n_{i})\) rate of Algorithm 3, and the \(O(})\) rate of the Baseline. For distributions with a larger number of budget, i.e., \(n_{i}>n_{m}^{2}\), it maintains a fast \(O(( m)/n_{m})\) rate. Since it only updates \(n_{m}\) times, and the best we can expect is the \(O(1/n_{m})\) rate of deterministic settings (Nemirovski, 2004). So, there is a performance limit for mini-batch based methods, and after that increasing the batch-size cannot reduce the rate, which consists with the usage of mini-batches in SCO (Cotter et al., 2011; Zhang et al., 2013).

## 4 Conclusion

For the GDRO problem, we develop two SA approaches based on SMD and non-oblivious MAB. Both of them attain the nearly optimal \(O(m( m)/^{2})\) sample complexity, but with different number of samples used in each round, which are \(m\) and \(1\) respectively. Then, we formulate a weighted GDRO problem to handle the scenario in which different distributions have different sample budgets. We first incorporate non-uniform sampling into SMD to satisfy the sample budget in expectation, and deliver distribution-dependent convergence rates. Then, we propose to use mini-batches to meet the budget exactly, deploy SMPA to exploit the small variances, and establish nearly optimal rates for multiple distributions. We have conducted experiments to evaluate our proposed algorithms. The empirical results are presented in Appendix C, and align closely with our theories.