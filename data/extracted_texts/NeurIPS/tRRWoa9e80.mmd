# Token Merging for Training-Free Semantic Binding

in Text-to-Image Synthesis

 Taihang Hu\({}^{1}\), Linxuan Li\({}^{1}\), Joost van de Weijer\({}^{3}\), Hongcheng Gao\({}^{4}\)

**Fahad Shahbaz Khan\({}^{5,6}\), Jian Yang\({}^{1}\), Ming-Ming Cheng\({}^{1,2}\), Kai Wang\({}^{3}\)\({}^{*}\), Yaxing Wang\({}^{1,2}\)\({}^{*}\)**

\({}^{1}\)VCIP, College of Computer Science, Nankai University, \({}^{2}\)NKIARI, Shenzhen Futian

\({}^{3}\)Computer Vision Center, Universitat Autonoma de Barcelona

\({}^{4}\)University of Chinese Academy of Sciences

\({}^{5}\)Mohamed bin Zayed University of AI, \({}^{6}\)Linkoping University

{hutaihang00, linxuanli520, gaohongcheng2000}@gmail.com

{joost, kwang}@cvc.uab.es, fahad.khan@liu.se

{csjyang,cmm,yaxing}@nankai.edu.cn

###### Abstract

Although text-to-image (T2I) models exhibit remarkable generation capabilities, they frequently fail to accurately bind semantically related objects or attributes in the input prompts; a challenge termed _semantic binding_. Previous approaches either involve intensive fine-tuning of the entire T2I model or require users or large language models to specify generation layouts, adding complexity. In this paper, we define semantic binding as the task of associating a given object with its attribute, termed _attribute binding_, or linking it to other related sub-objects, referred to as _object binding_. We introduce a novel method called _Token Merging (ToMe)_, which enhances semantic binding by aggregating relevant tokens into a single _composite token_. This ensures that the object, its attributes and sub-objects all share the same cross-attention map. Additionally, to address potential confusion among main objects with complex textual prompts, we propose _end token substitution_ as a complementary strategy. To further refine our approach in the initial stages of T2I generation, where layouts are determined, we incorporate two auxiliary losses, an entropy loss and a semantic binding loss, to iteratively update the composite token to improve the generation integrity. We conducted extensive experiments to validate the effectiveness of _ToMe_, comparing it against various existing methods on the T2I-CompBench and our proposed GPT-4o object binding benchmark. Our method is particularly effective in complex scenarios that involve multiple objects and attributes, which previous methods often fail to address. The code will be publicly available at https://github.com/hutaihang/ToMe.

## 1 Introduction

Text-to-image generation has seen significant advancements with the recent introduction of diffusion models , with their capabilities of generating high-fidelity images from text prompts. Despite these achievements, aligning the generated images with the text prompts, which is referred to as _semantic alignment_, remains a notable challenge. One of the most common issues observed in existing text-to-image (T2I) generation models is the lack of proper _semantic binding_, where a given object is not properly binding to its attributes or related objects. For example, as illustrated in Fig. 1, even a state-of-the-art T2I model such as SDXL  can struggle to generate content that accurately reflects the intended nuances of text prompts. To address the persistent challenges of aligning T2I diffusion models with the intricate semantics of text prompts, a variety of enhancementstrategies  are proposed, either by optimizing the latent representations , guiding the generation by layout priors  or fine-tuning the T2I models . Despite these advancements, these methods still encounter limitations, particularly in generating high-fidelity images involving complex scenarios where an object is binding with multiple objects or attributes.

In this paper, we categorize _semantic binding_ into two categories. First, _attribute binding_ involves correctly associating objects with their attributes, a topic that has been studied in prior work . Second, _object binding_, which entails effectively linking objects to their related _sub-objects_ (for example, a 'hat' and 'glasses'), is less explored in the existing literature. Previous methods often struggled to address this aspect of semantic binding. One of the main problems is the misalignment of objects with their corresponding sub-objects. Existing solutions address this through an explicit alignment process of the attention maps  or by factorizing the generation projects into layout phases and generation phase . In this paper, we propose a simple solution to the attention alignment problem called _token merging_ (_ToMe_). Instead of multiple attention maps, which can be misaligned, we join these objects in a single _composite token_ that represents the object and its attributes and sub-objects. This composite token has a single cross-attention map that ensures semantic alignment. The composite token is simply constructed by summing the CLIP text embeddings of the various tokens it represents. For example, the phrase "a dog with hat" is abbreviated as "a dog**" by aggregating the text embeddings corresponding to the last three words, as shown in Fig. 4. To justify the applied embedding addition in _ToMe_, we experimented with the semantic additivity of the text embeddings (in Fig. 3). Furthermore, to mitigate potential semantic misalignment in the end tokens from the long sequences, we propose _end token substitution_ (ETS) technique.

As the T2I generation predominantly determines the layout during earlier phases , we introduce an entropy loss and a semantic binding loss to update the token embeddings in early steps, integrating _ToMe_ with an iterative update for the composite tokens. The entropy loss is defined as the entropy of the cross-attention map corresponding to the updated composite token. This loss aims to enhance generation integrity by ensuring diverse attention across relevant areas of the image, thereby preventing focusing on non-essential regions. The semantic binding loss encourages the new learned token to infer the same noise prediction as the original corresponding phrase. This alignment further reinforces the semantic coherence between the text and the generated image.

Our final method _ToMe_ is quantitatively assessed using the widely adopted T2I-CompBench  and our proposed GPT-4o _object binding_ benchmark. Comparative evaluations against various types of approaches reveal that _ToMe_ outperforms them by a significant margin. Remarkably, our approach is user-friendly, requiring no dependence on large language models or specific layout information. In qualitative evaluations, we notably achieve superior generation quality, particularly in scenarios involving multi-object multi-attribute generation. This further underscores the superiority of our method. In summary, the main contributions of this paper are as follows:

* We analyze the problem of semantic binding, and highlight the role of the [EOT] token (Fig. 2), and the problems with misaligned cross-attention maps (Fig. 7). In addition, we explore token additivity as a possible solution (Fig. 3).

Figure 1: Current state-of-the-art T2I models often struggle with semantic binding in generated images according to textual prompts. For example, hats and sunglasses are placed on incorrect objects. We introduce a novel method _ToMe_ to address these challenges.

* We introduce a _training-free_ approach called _Token Merging_ (Fig. 4), denoted as _ToMe_, as a more efficient and robust solution for semantic binding. It is further enhanced by our proposed _end token substitution_ and iterative _composite token_ updates techniques.
* In experiments conducted on the widely used T2I-CompBench benchmark and our GPT-4o object binding benchmark, we compared _ToMe_ with various state-of-the-art approaches and consistently outperformed them by significant margins.

## 2 Related works

A critical drawback of current text-to-image models is related to their limited ability to faithfully represent the precise semantics of input prompts, commonly referred to as _semantic alignment_. Various studies have identified common semantic failures and proposed mitigation strategies. They can be roughly categorized into four main streams.

**Optimization-based methods** primarily adjust text embeddings [20; 65] or optimize noisy signals to strengthen attention maps [26; 48; 69; 82; 83]. These methods are basically inspired by the observations from text-based image editing methods [27; 40; 64; 66], suggesting that the layouts of objects are determined by self-attention and cross-attention maps from the UNet of the T2I diffusion models. For example, Attend-and-Excite  improves object existence by exciting the attention score of each object. Divide-and-Bind  improves by maximizing the total variation of the attention map to prompt multiple spatially distinct attention excitations. SynGen  syntactically analyzes the prompt to identify entities and their modifiers, and then uses attention loss functions to encourage the cross-attention maps to agree with the linguistic binding reflected in the syntax. A-star  proposes to minimize concept overlap and change in attention maps through iterations. Composable Diffusion  decomposes complex texts into simpler segments and then composes the image from these segments. Structure Diffusion  attempts to address this by leveraging linguistic structures to guide the cross-attention maps. Rich-Text  enriches textual prompts by incorporating various formatting controls and decomposes the generation task into merging inferences from multiple region-based diffusions. However, these methods often fail in complex scenarios that generate multiple objects or multiple attributes.

**Layout-to-Image methods**[4; 9; 14; 17; 25; 32; 36; 47] are widely using layouts, particularly in the form of bounding boxes or segmentation maps, as a popular intermediary to bridge the gap between text input and the generated images. For example, BoxDiff  encourages the desired objects to appear in the specified region by calculating losses based on the maximum values in cross-attention maps. Similarly, Attention-Refocusing  modifies both cross-attention and self-attention maps to control object positions. BoxNet  first trains a network to predict the box for each entity that possesses the attribute specified in the prompt, and then force the generation to follow the attention mask control. Additionally, InstanceDiffusion  enhances text-to-image models by providing extra instance-level control. There are also finetuning methods [5; 42; 50; 79] allow for additional layout conditions after fine-tuning over pair images, which are not specifically designed to solve the _semantic alignment_ problem. Despite their promise, these methods obviously prolong the training time. Furthermore, the application of layout priors is challenging when it comes to global background descriptions or abstract elements. This limitation constrains the versatility of these techniques, making it difficult to deploy them effectively across real scenarios where non-specific spatial arrangements are crucial.

**LLM-augmented methods** are mainly following text-to-layout-to-image generation pipelines [15; 23; 33; 44; 55; 65; 80; 81; 86], first to generate layouts from large language models (LLMs) and force the T2I generations to follow this guidance as the previous layout-guided methods. Some methods, such as RPG  and MuLan , harness the powerful chain of thought reasoning ability of multimodal LLMs to enhance the compositionality of text-to-image diffusion models.

**Finetuning-based methods**[13; 76] update the model parameters over huge datasets to augment the semantic alignment. Among them, CoMat  proposes an end-to-end fine-tuning strategy for text-to-image diffusion models by incorporating image-to-text concept matching. ELLA  equips text-to-image diffusion models with powerful Large Language Models (LLM) to enhance text alignment by bridging these two pre-trained models with trainable semantic alignment connectors. More recently, Ranni  improves T2I generation by bridging the text and image with a semantic panel with LLMs and is fine-tuned over an automatically prepared semantic panel dataset. Thereare also improved T2I models [10; 11; 51] learning from scratch over huge datasets. These methods improve semantic alignment implicitly by better architecture design and larger amount of training data. They further demand marvelous computational resources to achieve the purpose.

In this paper, we tackle the _semantic binding_ problem, which is a broad subcase of _semantic alignment_, in a training-free manner, neither needing the LLMs nor any training over additional datasets. Furthermore, we achieve better performance when facing complex T2I generation scenarios where users require multiple objects or multiple attributes related to a specific object.

## 3 Methods

Semantic binding in T2I generation refers to the crucial requirement of establishing accurate associations between objects and their relevant attributes or related sub-objects. This process avoids semantic misalignment in the generated images, ensuring that each visual element aligns correctly with its descriptive cues in the text. In this section, we begin by providing the preliminaries. Subsequently, we illustrate the motivation through a series of experimental analyses (Sec. 3.1). Finally, we elaborate on our methods in detail (Sec. 3.2). An illustration of our method _ToMe_ is shown in Fig. 4.

**Latent Diffusion Models.** We build our novel approach for semantic alignment on the standard SDXL  model. The model is composed of two main parts: an autoencoder (i.e., a encoder \(\) and a decoder \(\) ) and a diffusion model (i.e., \(_{}\) with parameter \(\)). The model \(_{}\) is updated by the loss:

\[L_{LDM}:=_{z_{0}(x),y,(0,1),t (1,T)}\|-_{}(z_{t},t,_{ }())\|_{2}^{2},\] (1)

where \(_{}\) is a UNet, conditioning a latent input \(z_{t}\), a text embedding \(_{}()\) and a timestep \(t(1,T)\). More specifically, text-guided diffusion models aim to generate an image from random noise \(z_{T}\) and a conditional input prompt \(\). To distinguish from the general conditions in LDMs, we itemize the textual condition as \(=_{}()\), where \(_{}\) is the CLIP text encoder +. The cross-attention map is obtained from \(_{}(z_{t},t,)\). Let \(f_{z_{t}}\) be a feature map output of the network \(_{}\). We get a query matrix \(Q_{t}=l_{Q}(f_{z_{t}})\) with projection network \(l_{Q}\). Similarly, given a textual embedding \(\), we compute a key matrix \(=l_{}()\) with projection network \(l_{}\). Then the attention map is computed according to: \(_{t}=softmax(Q_{t}^{T}/)\) where \(d\) is the latent dimension, and the cell \([_{t}]_{ij}\) defines the weight of the \(j\)-th token on the \(i\)-th token.

Footnote †: SDXL uses two CLIP text encoders and concatenate the two text embeddings as the final text embedding.

### Text Embedding Analysis

To address the semantic binding problem, we concentrate on the text embeddings utilized during the diffusion model generation process, as they predominantly dictate the content of the generated images. For a given text prompt \(\), it is tokenized by the CLIP text model by padding a start

Figure 2: We generate images with various input prompts in (a): “a cat wearing sunglasses and a dog wearing a hat”; the single-token embedding [dog]; the end token [EOT]. (b) After that, we compute the probability of containing “sunglasses” in the generated images in subfigure.

token [SOT] and several end tokens [EOT] to extend its length to \(M\)(=77 by default). After the CLIP text encoder \(_{}\), the condition is formulated as \(=_{}()\). Each row in \(\) represents a corresponding token embedding after the CLIP text transformers. For example, the text embedding for the sentence \(=\)"a cat wearing businesses and a dog wearing a hat" is represented as: \(=[_{0}^{SOT},_{1}^{a},_{2}^{cat}, ,_{7}^{dog},_{8}^{wearing},_{9}^{hat}, _{10}^{EOT},,_{M-1}^{EOT}]\). In the following analysis, we take this as a default example (except when defined differently).

**Information Coupling.** We begin by generating images conditioning on the textual embedding \(\), as illustrated in the first two columns at the bottom of Fig. 2-(a). We observe that the attributes appear in a misalignment between the dog and the cat. Subsequently, we extract the token embedding \(_{7}^{dog}\) from the textual embedding and input it to the UNet \(_{}\) (i.e., \(=[c_{7}^{dog}]\))++. As depicted in the middle columns of Fig. 2-(a). The dog object is frequently wearing glasses, further highlighting the semantic leakage issue. Furthermore, when we take \(^{[EOT]}=[_{10}^{EOT},,_{M-1}^{EOT}]\) as input, the generated images closely resemble all information obtained using the entire textual embedding \(\). As the [EOT] interacts with all tokens, it often encapsulates the entire semantic information .We further report the _DetScore_ to show the probability of containing the corresponding object ("sunglasses") in the generated 100 images. As illustrated in Fig. 2-(b), for these three different cases, the DetScore is 22.6%, 69.6% and 75.0%, respectively. These findings also align with our observations above.

Footnote ‡: Note in this case, the size of the input textual embedding is \(1 2048\) instead \(77 2048\).

**Additivity Property.** Inspired by the semantic additivity of the text embeddings in previous research, we experiment the additive property of the CLIP textual embedding. We represent the textual embedding corresponding to the prompt "a photo of a dog" as \(_{1}=[c_{0}^{SOT},c_{1}^{a},,c_{5}^{dog},c_{6}^{SOT},,c _{M-1}^{SOT}]\). The textual embedding for the prompt "a photo of a hat" is represented as \(_{2}=[c_{0}^{SOT},c_{1}^{a},,c_{5}^{hat},c_{6}^{EOT},,c _{M-1}^{EOT}]\). Next, we perform element-wise addition between the object tokens (i.e., \(c_{5}^{dog}\) and \(c_{5}^{hat}\)) and the corresponding [EOT] tokens. Specifically, the resulting new embedding is \(^{}=(_{1}[0:4],_{1} [5:M-1]+_{2}[5:M-1])\). Afterward, the textual embeddings \(^{}\) are input into the diffusion UNet to generate the images shown in Fig. 3-(a). We can observe that this additivity property allows adding objects (up-left), removing objects (up-right, down-left) and even complex semantic computations (down-right). To explore the mechanism behind this phenomenon, we conducted PCA dimensionality reduction visualization on the token representations of each prompt, as illustrated in Fig. 3-(b). The directional vector obtained from "queen-king" is approximately identical to that of "woman-man" with the cosine similarity of 0.998.

**In conclusion**, our analysis shows that the semantic content of text tokens is coupled and entangled, resulting in attribute confusion across different subjects. Moreover, we found that in diffusion models, text embeddings exhibit semantically additive properties. This implies that the diffusion model is capable of interpreting a composite token, derived from the summation of multiple individual tokens, integrating the semantic attributes of the combined tokens.

Figure 3: (a) Image generations with the property of token additivity. All images are generated by the prompt template “a photo of a _(object)_.” (b) PCA plot for additivity of text embeddings.

### _ToMe_: Token Merging

Suppose the initial prompt \(\) contains \(K\) entities indicated by noun words and their corresponding tokens as \(\{n^{1},...,n^{k}...,n^{K}\}\). Each entity is often related to a token with relevant objects or attributes set as \((n^{k},a^{k})\). For example, in the sentence "a cat wearing glasses and a dog with a hat", \(n^{1}=,a^{1}=\{,n^{2}=,a^{2}=\{\}\).

#### 3.2.1 Token Merging techniques

The semantic additivity of token embeddings inspires us to achieve co-expression of entities and attributes by explicitly binding tokens together. We employ element-wise addition to accomplish semantic merging of tokens. For a prompt \(\) containing \(K\) entities, we fuse each subject-attribute pair \((n^{k},a^{k})\) into \(_{k}=n^{k}+ a^{k}\), referred to as a _composite token_. This innovative approach introduces an additional benefit by utilizing a single composite token to condense a lengthy prompt sequence, resulting in a unified cross-attention map, thus avoid semantic misalignment. Such observations are further shown in the ablation study and appendix.

**End Token Substitution (ETS).** Meanwhile, as the semantic information contained in [EOT] can interfere with attribute expression, we mitigate this interference by replacing [EOT] to eliminate attribute information contained within them, retaining only the semantic information of each subject. For instance, when the prompt is "a cat wearing hat and a dog wearing sunglasses," we use the [EOT] obtained from the prompt "a cat and a dog" to replace the original [EOT]. As illustrated in Fig. 4-a, the final text embedding after subject-attribute enhancement and EOT replacement is \(=[_{0}^{SOT},_{1}^{a},_{2}^{dog*},, _{5}^{cat*},_{6}^{EOT*},,_{76}^{EOT*}]\). Here, dog* and EOT* respectively denote tokens after token merging and end token substitution.

#### 3.2.2 Iterative composite Token Update

**Semantic binding loss.** As stated in section 3.1, the semantic information of each token embedding is inherently linked. After strengthening the relationship between subjects and their attributes, it becomes crucial to eliminate any irrelevant semantic information within the composite tokens to prevent misrepresentation of attributes. As illustrated in Fig. 4-(b), to ensure that the semantics of the composite tokens correspond accurately to the noun phrases they are meant to represent, we employ a clean prompt as a supervisory signal. Specifically, for a composite token embedding \(^{dog}\), which corresponds to the noun phrase "a dog wearing hat", we aim for the diffusion model to exhibit consistent noise prediction for this composite token and the full phrase. In mathematical terms, this objective can be expressed as ensuring that \(_{}(z_{t},^{dog},t)_{}(z_{t}, ,t)\). This effectively aligns \(_{z_{t}}P_{}(z_{t}|^{dog})_{z_{t}} P_{}(z_{t}|)\). At time step \(t\), we use the semantic binding loss to align token semantics \(_{sem}=_{k[1,K]}\|_{}(z_{t},_{k},t)- _{}(z_{t},,t)\|_{2}^{2}\).

Figure 4: _ToMe_ is composed of two parts: one with Token Merging and end token substitution, and the other token updating part with two auxiliary losses for iterative _composite token_ update.

**Entropy loss.** Following that, we calculate the information carried by each token embedding through entropy statistics. As shown in Fig. 7, we extract the cross-attention map \(_{k}\) corresponding to the \(k\)-th token. After normalizing the cross-attention map as \(_{p_{i}_{k}}p_{i}=1\), we compute the entropy of each token as \(entropy(_{k})=_{p_{i}_{k}}-p_{i}(p_{i})\). Decreasing the entropy of the cross-attention maps can help ensure that tokens focus exclusively on their designated regions, thereby preventing the cross-attention map from becoming overly divergent. This is further depicted in Fig. 7, where we observe instances of attribute confusion, characterized by different tokens inappropriately influencing the same image region. The entropy regularization loss is defined as \(_{ent}=_{k[1,K]}_{p_{i} A_{k}}-p_{i}(p_{i})\) during time step \(t\).

Finally, the overall \(=_{ent}+_{sem}\) is computed by these two novel losses to update the _composite token_ during each time \(t<T_{opt}\) and \(\) is the trade-off hyperparameter.

## 4 Experiments

### Experimental Setups

Evaluation Benchmarks and Metrics.We evaluate the effectiveness of _ToMe_ over T2I-CompBench , a comprehensive benchmark for open-world compositional T2I generations, encompassing attribute binding and object relationships. We focus on the semantic binding problem, where T2I-CompBench predominantly evaluates through three attribute subsets (i.e., color, shape, and texture). We follow the evaluation protocol [21; 30; 34] that using 300 validation prompts for evaluation under each subset and the BLIP-VQA score as the evaluation metrics. Following that, we adopt the ImageNetward  model to evaluate human preference scores, which comprehensively measure image quality and prompt alignment. To comprehensively evaluate _object binding_ performance, we introduce a new _GPT-4o Benchmark_ of 50 prompts using the template "a [objectA] with a [itemA] and a [objectB] with a [itemB].". For example, objectA and objectB are objects like "cat" and "dog" while itemA and itemB are associated items "hat" and "glasses". Afterward, we used the multimodal model GPT-4o  to compute the consistency score between the generated images and the prompts for objective assessment. More details are available in the Appendix C.5.

**Implementation Details.** We used SDXL  as our base model. To automate image generation for evaluation, we employed SpaCy  for syntactic parsing of prompts to identify each object and its corresponding attributes for token merging. The iterative composite token update is performed during the first 20% of the denoising steps \(T_{opt}=0.2T\).

**Comparison Methods.** To evaluate our method's effectiveness, we compared the current state-of-the-art methods. These primarily encompass: (1) state-of-the-art T2I diffusion models, including SDXL , Playground-v2  (2) Finetuning-based methods, including CoMat , ELLA  (3) Optimization-based method SynGen  (4) LLM-augmented finetuning-based method Ranni . More comparison results are shown in the Appendix E.

### Experimental Results

**Quantitative Comparison.** As shown in Table 1, _ToMe_ consistently outperforms or performs comparably to existing methods in BLIP-VQA scores across the color, texture, and shape attribute binding subsets, indicating its effectiveness in avoiding attribute confusion. Human-preference scores evaluated through the ImageReward model(note that the model scores are logits and can be negative) suggest that images generated by _ToMe_ can better align with prompts. Specifically, despite ELLA's use of LLama or T5-XL to replace the CLIP Text Encoder for stronger text embeddings, our method still achieves higher BLIP-VQA scores compared to ELLA. The significant improvement in GPT-4o scores also demonstrates the effectiveness of _ToMe_ in _object binding_.

**Qualitative Comparison.** Following SynGen , we classify the failure cases of _attribute binding_ into three main categories. (i) Semantic leak in prompt, where the attribute \(a^{k}\) is not corresponding to its entity \(n^{k}\); (ii) Semantic leak out of prompt, where the attribute \(a^{k}\) is describing the background or some entity not referred to in the prompt \(\); (iii) Attribute neglect, where the attribute \(a^{k}\) is totally ignored in the image generation. Fig. 5 presents our qualitative comparison results with other methods. The first three rows show more complex _object binding_ results, while the last two rows demonstrate attribute binding results. The semantic binding errors in images generated by SDXLcan largely be attributed to (i) semantic leak in the prompt, as evidenced in the first and second row. Playground-v2 confronts similar semantic binding issue as SDXL. ELLA can occasionally succeed in simple attribute binding as in the fifth row, but it frequently encounters (i) semantic leak in the prompt and (iii) attribute neglect errors as shown in the first three prompts. Ranni  generates images based on layouts created by a large language model, which can partially address more complex object binding (second row). However, layout-based methods may encounter constrains in achieving proper image layouts, such as shown in the first row with complex descriptions. SynGen , which focus on attribute binding problems, achieves good results in color and shape binding but fails in object binding, exhibiting varying degrees of (i) and (iii) failures. Compared to these methods, our method is able to capture the semantic binding problem.

    &  Base \\ Model \\  } &  &  &  &  \\  & & Color & Texture & Shape & Color & Texture & Shape & \\  SDXL & - & ✓ & 0.6369 & 0.5637 & 0.5408 & 0.7798 & 0.5140 & 0.4029 & 0.4907 \\ PlayG-v2 & - & ✓ & 0.6208 & 0.6125 & 0.5087 & - & - & - & 0.5417 \\  Ranni & & ✓ & 0.2414 & 0.3029 & 0.2857 & -0.8554 & -0.6853 & -0.8051 & 0.4166 \\ ELLA & SD1.5 & ✓ & 0.6911 & 0.6308 & 0.4938 & 0.6586 & 0.2963 & 0.0565 & 0.6481 \\ SynGen & & ✗ & 0.6619 & 0.6451 & 0.4661 & 0.4326 & 0.5072 & 0.0426 & 0.5545 \\ CoMat & & ✓ & 0.6561 & 0.6190 & 0.4975 & - & - & - & - \\  Ranni & & ✓ & 0.6893 & 0.6325 & 0.4934 & - & - & - & - \\ ELLA & SDXL & ✓ & 0.7260 & 0.6686 & 0.5634 & - & - & - & - \\ SynGen & ✗ & 0.7010 & 0.6044 & 0.5069 & 1.016 & 0.7867 & 0.4016 & 0.6458 \\ CoMat & & ✓ & 0.7774 & 0.6591 & 0.5262 & - & - & - & - \\  _ToMe_ (Ours) & SDXL & ✗ & 0.7656 & 0.6894 & 0.6051 & 1.074 & 0.9281 & 0.5916 & 0.9549 \\   

Table 1: Quantitative results for semantic binding assessment on various benchmarking subsets. We denote the best score in \(}\), and the second-best score in \(}\).

Figure 5: Qualitative comparison among various T2I generation methods with complex prompts.

approach _ToMe_ shows improved performance in both object and attribute binding scenarios, which is consistent with the quantitative metrics reflected in Table 1.

**Ablation Study** over each component is quantitatively shown in Table 2. We can observe that using only token merging techniques (with _ToMe_ and ETS as config.B) results in a slight performance improvement, which is consistent with the qualitative results in Fig. 6. However, token merging serve as the foundation for subsequent optimizations. When they are combined with the entropy loss \(_{ent}\) as config.C, the performance improves significantly. We hypothesize that is partly due to the more regularized cross-attention maps as shown in Fig. 7. Nevertheless, config.C without the semantic binding loss still leads to worse generation performance in Fig. 6, as the dog on the right side still exhibits cat-like features. Incorporating the semantic alignment loss \(_{sem}\) (as our default configuration) ensures that the two subjects correctly bind to their respective attributes without appearance confusion, achieving the best results quantitatively and qualitatively. Suppose token merging is ignored, and we only apply the optimization (Config D and Config E), the performances are only comparable to the baseline. Removing \(_{ent}\) from _ToMe_ (Config F) can also improve over the baseline, but the generation is with noticeable artifacts, which is mainly due to the less regularized cross-attention map. In conclusion, each element of these three novel techniques in _ToMe_ contributes to achieving state-of-the-art performance. See Appendix D for more detailed ablation experiments.

**Additional Applications** of _ToMe_ are shown in Fig. 8. _ToMe_ can not only successfully address the semantic binding problem, it can also be applied to other problems widely exist in T2I generations, including adding objects , removing objects  and even bias mitigation .

   } &  & _{ent}\)} &  \\  & & & & & & Color & Texture & Shape \\  A & \(\) & \(\) & \(\) & \(\) & 0.6369 & 0.5637 & 0.5408 \\ B & ✓ & \(\) & \(\) & 0.6577 & 0.5828 & 0.5437 \\ C & ✓ & ✓ & \(\) & 0.7525 & 0.6775 & 0.5797 \\ D & \(\) & ✓ & ✓ & 0.5881 & 0.6194 & 0.5386 \\ E & \(\) & ✓ & \(\) & 0.5983 & 0.5798 & 0.5125 \\ F & ✓ & \(\) & ✓ & 0.6804 & 0.6263 & 0.5645 \\ _Ours_ & ✓ & ✓ & ✓ & **0.7656** & **0.6894** & **0.6051** \\   

Table 2: Ablation Study conducted on the T2I-CompBench benchmark.

Conclusion

In this paper, we investigate a critical issue in text-to-image (T2I) generation models known as _semantic binding_. This phenomenon refers to instances where T2I models struggle to accurately interpret and visually bind the related semantics. Recognizing that previous methods often entail extensive fine-tuning of the entire T2I model or necessitate explicit specification of generation layouts by large language models, we introduce a novel training-free approach called Token Merging, denoted as _ToMe_, to tackle semantic binding issues in T2I generation. _ToMe_ incorporates innovative techniques by stacking up the object token with its relevant tokens into a single _composite token_. This mechanism eliminate the semantic misalignment by unifying the cross-attention maps. Furthermore, we assist the _ToMe_ with end token substitution, and iterative composite token updates technique to strengthen the semantic binding. In extensive experiments, we quantitatively compare it against various existing methods using the T2I-Compbench and our proposed GPT-4o benchmarks. The results demonstrate its ability to handle intricate and demanding generation tasks more effectively than current methods, especially for _object binding_ cases that are ignored in previous research.