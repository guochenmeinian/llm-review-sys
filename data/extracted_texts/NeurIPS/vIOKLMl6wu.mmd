# LOVA\({}^{3}\): Learning to Visual Question Answering,

Asking and Assessment

Henry Hengyuan Zhao\({}^{1}\), Pan Zhou\({}^{2}\)\({}^{}\), Difei Gao\({}^{1}\), Zechen Bai\({}^{1}\), Mike Zheng Shou\({}^{1}\)\({}^{}\)

\({}^{1}\)Show Lab, National University of Singapore,

\({}^{2}\)Singapore Management University

Corresponding author.

###### Abstract

Question answering, asking, and assessment are three innate human traits crucial for understanding the world and acquiring knowledge. By enhancing these capabilities, humans can more effectively utilize data, leading to better comprehension and learning outcomes. Current Multimodal Large Language Models (MLLMs) primarily focus on question answering, often neglecting the full potential of questioning and assessment skills. Inspired by the human learning mechanism, we introduce **LOVA\({}^{3}\)**, an innovative framework named "Learning tO Visual question Answering, Asking and Assessment," designed to equip MLLMs with these additional capabilities. Our approach involves the creation of two supplementary training tasks **GenQA** and **EvalQA**, aiming at fostering the skills of asking and assessing questions in the context of images. To develop the questioning ability, we compile a comprehensive set of multimodal foundational tasks. For assessment, we introduce a new benchmark called **EvalQABench**, comprising 64,000 training samples (split evenly between positive and negative samples) and 5,000 validation and testing samples. We posit that enhancing MLLMs with the capabilities to answer, ask, and assess questions will enhance their multimodal comprehension, ultimately improving overall performance. To validate this hypothesis, we train MLLMs using the **LOVA\({}^{3}\)** framework and evaluate them on a range of multimodal datasets and benchmarks. Our results demonstrate consistent performance gains, underscoring the critical role of these additional tasks in fostering comprehensive intelligence in MLLMs. The code is available at https://github.com/showlab/LOVA3.

## 1 Introduction

To acquire knowledge, we humans often answer lots of questions and then improve ourselves by comparing our answers with the ground-truth answers. As a result, this learning mechanism empowers humans with the answering ability, which allows humans to handle well many real tasks, such as visual question answering . However, as described in the following slogan,

"The art of proposing a question must be held in higher value than solving it." - Georg Cantor 

asking a question is very valuable and even more important than answering a question. Indeed, humans also acquire knowledge from learning to ask questions since it encourages individuals to engage more deeply with information, thereby enhancing problem-solving skills . In addition to asking questions, humans also improve themselves through self-evaluation: humans try to identify the correctness of the answer and thus are involved in a deep understanding of our diverse world .

Although innate to humans, apart from answering ability, two other learning mechanisms of asking and assessment remain underexplored for contemporary multimodal large language models (MLLMs). Current MLLMs  are excelled in addressing diverse domains of multimodal questions such as mathematics , science , and commonsense knowledge . However, they predominantly revolve around visual question answering (VQA). As a result, as shown in Fig. 1, current MLLMs, e.g., the representative LLaVA-1.5 , suffer from inferior performance on asking questions and self-assess question-answer pairs (QA), which underscores their efficacy as problem-solvers and prohibits holistic multimodal understanding.

To advance the comprehensive intelligence of MLLMs, we introduce two essential tasks: **GenQA** and **EvalQA**, aiming at bolstering the intelligence and robustness of MLLMs. GenQA focuses on enabling the model to generate diverse question-answer (QA) pairs from the single input image, thus equipping the MLLM with the capability to ask questions. We believe that if an MLLM can successfully generate QA pairs for challenging tasks, it indicates a higher level of problem-solving ability . Specifically, we define the GenQA task to include not only generic VQA (e.g., VQAv2  and GQA ) but also Multi-Choice VQA (MC VQA), and Multi-Turn VQA (MT) to increase the variety of data formats. Additionally, we incorporate two challenging multimodal grounding tasks into the training process: Referring Expression Comprehension (REC) and Referring Expression Generation (REG). Learning to generate the data of these grounding tasks forces the MLLM to extract fine-grained visual cues from images, such as explicit object localization and compositional relationships. This, in turn, enhances the multimodal reasoning ability of MLLMs. During training, we gather the relevant datasets for these tasks and transform them into a generative format using our proposed instruction template. EvalQA, on the other hand, involves tasking the MLLM to predict the correctness of a given visual-question-answer triplet. Recognizing the absence of datasets specifically designed to assess VQA correctness, we have developed a new benchmark called **EvalQABench** for evaluating VQA data. Rather than asking humans to label such a dataset, we propose a new pipeline for data construction. This benchmark comprises training, validation, and test sets, with each VQA pair accompanied by a "Yes" or "No" label indicating correctness, along with a one-sentence explanation as the feedback. For instance, _"Yes, the oranges are not in a bag"_.

By integrating the GenQA and EvalQA tasks into the vanilla multimodal learning, we develop an effective training framework called **LOVA\({}^{3}\)**. In this study, we select the SOTA MLLM LLaVA-1.5 as the backbone model for evaluation. We conduct experiments on 10 widely used multimodal benchmarks such as GQA , VQAv2 , Vizwiz , MME , MMBench , and MM-vet , and observe consistent improvements across these benchmarks. To summarize, our proposed LOVA\({}^{3}\) is a new framework that endows the MLLM with the ability to ask and assess and finally achieve profound multimodal understanding capability. Overall, our contributions are three folds:

1. To the best of our knowledge, **LOVA\({}^{3}\)** is the first effort to imbue the asking and assessment abilities in training a robust and intelligent MLLM. LOVA\({}^{3}\) open an avenue for imitating the human abilities towards holistic intelligence for MLLM.
2. We build a new benchmark **EvalQABench** for the VQA evaluation as the first effort to advance the VQA data assessment of future research.
3. The experimental results demonstrate that training with LOVA\({}^{3}\) consistently improves performance across several multimodal benchmarks, including VQAv2, GQA, MME, VizWiz, MMBench, and MM-Vet, etc.

Figure 1: Comparison of three abilities reveals that LLaVA1.5 excels in providing answers but struggles in asking accurate questions and assessing question-answer pairs.

## 2 Related Work

### Multimodal Large Language Models

Large Language Models (LLMs) [16; 98; 65; 5; 81; 15; 82] such as GPT-4  demonstrate their exceptional capacity to handle a wide range of complex tasks to play an important role in assisting humans in daily life. Equipped with these LLMs, a surge of multimodal modes [37; 43; 17; 3; 10; 84; 101; 9; 8; 51; 85; 29; 62; 58; 22; 47; 28; 34; 7; 100; 48; 14; 38; 24; 68; 23; 66] are proposed to integrate the visual information with the pre-trained LLM decoder for diverse multimodal reasoning tasks such as image captioning [12; 1; 91] and visual question answering [26; 54; 30; 27]. LLaVA [43; 42] is a pioneering approach that collects 665K instruction tuning data from present vision-language (VL) datasets for supervised finetuning (SFT) and achieves promising results on various datasets in a lower cost of training requirement. Another SOTA model InstructBLIP  also proposes gathering datasets to construct their instruction tuning dataset. It adopts the VQG task but is limited in generic data type. Different from InstructBLIP, we propose the GenQA task for jointly generated questions and answers on 5 primary VL tasks not restricted to generic data type. Besides focusing on traditional vision-language tasks, Shikra , Kosmos-2 , PVIT , Ferret  pay attention to the image-region based multimodal tasks (i.e., Referring Expression Comprehension) and demonstrate the performance improvement with these hard tasks. By adopting a large-scale image-text corpus for instruction tuning, Qwen-VL , CogVLM , AnyMAL  and Chameleon  achieve exceptional performance on various multimodal tasks. However, these MLLMs primarily concentrate on training the model to answer questions as effectively as possible, neglecting the significance of enabling the model to act as a questioner and a competent evaluator within the training paradigm.

### Visual Question Answering and Generation

Nine years ago, visual question answer  was defined and became an essential task for evaluating multimodal systems. A surge of VQA-related benchmarks [2; 26; 76; 56; 55; 27; 71; 54; 50; 30; 39] are emerged to advance the development of this research area, including generic VQA benchmarks [2; 30; 26], text-based VQA [76; 56; 55; 78], knowledge-augmented VQA [54; 71; 50; 13], and goal-oriented VQA  aimed at assisting blind people.

Besides the VQA task, the Visual Question Generation (VQG) task was first formulated in , which contributes a VQG dataset with each image annotated with multi-questions and benchmarking on generative models and retrieval models.  first employs an RNN-based encoder-decoder framework alongside model-generated captions to generate questions. After that a list of works [59; 60; 18; 32; 72; 87; 83] are proposed for promoting this research area. Two interesting studies [40; 73] pose that treating VQG as a complementary task can enhance the robustness of visual question answering. This finding reaffirms our motivation that training a model to generate diverse questions contributes to a deeper understanding of visual information, thereby improving its problem-solving capabilities. Unlike the traditional Visual Question Generation (VQG) task, which focuses primarily on the generic VQA domain, GenQA is designed to generate diverse VQA data including MC VQA, MT, REC, and REG. Additionally, GenQA generates both questions and answers simultaneously, whereas traditional VQG focuses solely on question generation.

### Multimodal Benchmarks

Traditional multimodal benchmarks focus on answering ability, such as visual question answering , image captioning [12; 63; 1], as well as other benchmarks for specialized scenarios such as scene text understanding [76; 75], commonsense reasoning , outside knowledge [54; 71]. The recent development of MLLM posts a strong need for motorized multimodal benchmarks [19; 45; 36; 92; 25; 93; 49; 20; 11; 96; 44; 99; 77; 80] such as MME , MMBench , SEED-Bench  which involve comprehensively evaluating current MLLMs on various multimodal abilities. Unlike existing multimodal benchmarks focusing primarily on evaluating the model's ability to answer, we introduce EvalQABench, a benchmark designed to evaluate the correctness of VQA pairs, each with a binary "Yes/No" annotation. Furthermore, recognizing the lack of emphasis on providing feedback for incorrect answers in current benchmarks, we develop an LLM-based pipeline. This pipeline can automatically generate feedback, paving the way for enhanced automated data processing in the future.

## 3 Methodology

In this section, we introduce LOVA\({}^{3}\), a new framework designed to imitate two essential abilities - asking and assessment - within multimodal learning. We delve into the specifics of addressing this challenge through GenQA data collection, EvalQA data creation, model architecture, and training.

### Data Collection for GenQA

If one MLLM is able to successfully generate high-quality question-answer pairs based on visual input, it indicates a stronger problem-solving ability and deep visual understanding [40; 73]. To enable the MLLM to ask questions, it is natural for us to gather existing annotated datasets as the training corpus and then train the model to predict both questions and answers. We carefully define five main multimodal data types as listed in Tab. 1. For each data type, we gather widely used human-annotated datasets or high-quality instruction tuning datasets generated by GPT-4. We select Generic VQA tasks to generate fundamental questions, e.g., object count and object action. We incorporate Multi-choice VQA (MC VQA) and Multi-turn VQA (MT) to increase the diversity of data formats. Additionally, we include two multimodal grounding tasks: Referring Expression Comprehension (REC) and Referring Expression Generation (REG). Generating REC and REG data requires a deeper understanding of image content, enabling the model to fully comprehend visual cues. Both tasks increase the difficulty of GenQA, which helps MLLM acquire a higher level of multimodal understanding. In total, we gather 842K data for training questioning ability.

### Data Creation for EvalQA

Completing the VQA assessment often requires fine-grained and deep visual understanding. As emphasized in Sec. 1, the ability to assess is often overlooked yet crucial in MLLM training. To address this gap, we introduce a new benchmark, **EvalQABench**, to address the problem of assessing visual question-answering data. Moreover, instead of merely labeling each VQA pair with "Yes/No", we advocate for integrating **feedback** into each instance, an important aspect rarely seen in prior multimodal benchmarks. We consider training the model not only to assess the correctness of the answer but also to provide reasonable feedback that would increase the capability for multimodal understanding. EvalQABench comprises three datasets: training, validation, and test sets. As illustrated in Tab. 2, we present examples of the training set from EvalQABench across various question types.

**MLLM-based Negative Answer Generation.** The main challenge of EvalQABench lies in constructing negative answers. When dealing with large-scale ground-truth VQA pairs, how can we automatically produce the negative answer? One viable solution is to leverage a multimodal model for this purpose. Recognizing that Fuyu-8B  is an open-source free MLLM that stands out with the exceptional ability to process high-resolution images and perform robust well on many complex tasks. We utilize it to generate negative answers with the following prompt:

   Data Type & Dataset & Size & Instruction Promps \\   & VQA2  & 100K & _Note: randomly choose from 5S instruction prompts_ \\  & GQA  & 100K & Example: Can you provide a clear question and its answer based on the image? \\  & OCR-VQA  & 80K & \\  & Counting20K\({}^{3}\) & 20K & \\  & LLVA-250K\({}^{3}\) & 250K & \\   & A-OKRVQA  & 17K & Can you provide a clear question and its answer based on the image?\#This is a Multi-choice VQA \\  & VQA2  & 83K & Design a conversation between you and a person asking about this photo. \\  & GQA  & 72K & The answers should be in a tone that a visual AI assistant is seeing the image and answering the question. Ask diverse questions and give corresponding answers. \\   & VG  & 30K & _Note: randomly choose from 5S instruction prompts with a specific state description prompt_ \\  & RetCOCO  & 30K & Can you review the image and articulate a concise question and its answer?\#This is a Referring Expression Comprehension (REC) task. The question will express a specific region of the image. \\   & VG  & 30K & _Note: randomly choose from 5S instruction prompts with a specific task description prompt_ \\  & & Can you review the image and articulate a concise question and its answer?\#This is a Referring Expression Generation (REG) task. The purpose of REG is to generate a unique description for a specified location. \\  Total & - & 842K & \\   

Table 1: Data taxonomy of GenQA, detailing the data type, name, size, and instruction prompts of each dataset.

* <Img> This is the question: <Q>. Please give me the wrong answer to this question. The answer should be a single word or phrase.

Here, <Img> and <Q> are two placeholders for the image and question from ground truth VQA pair. The output of Fuyu-8B provides a negative answer, such as _"pansy"_, as illustrated in Fig. 2.

**Manual Filtering and Error Correction.** Acknowledging that the Fuyu-8B model is not flawless and recognizing that no multimodal model, including GPT-4V, is perfect, we have implemented both manual filtering and error corrections, as illustrated in Fig. 3 for the post-data processing. Through empirical analysis, we identified 4 primary types of errors. For instance, an answer generated by Fuyu-8B may be present in the question but lacks semantic relevance or is identical to the correct answer. Additionally, some incorrect answers may result from misunderstanding the question's category, as exemplified by the example in Fig. 3. Beyond filtering, we propose error corrections for two types of questions: "Yes/No" and "Counting". For "Yes/No" questions, we directly substitute an incorrect answer with "Yes". For "Counting" questions, we first verify if the English numeral matches the correct answer; if not, we replace it with a random number. After applying the above filtering and correction processes, we found that most of the incorrect samples had been removed.

  
**Question Types** & **Object** & **Yes/No** & **Counting** \\
**Image** & & & \\
**Question** & What kind of flowers are on the picture to the left? & Is the sun shining? & How many vaes are there? \\
**Ground-truth Answer** & & roes & no & 6 \\
**Negative Answer** & & pany & yes & 5 \\
**Feedback** & No, the left of the picture & No, the sun is not shining. & No, there are 6 vaes in the picture. \\ 
**Question Types** & **Color** & **Attribute** & **Number** \\
**Image** & & & \\
**Question** & What color is the truck? & What type of tree is on the right? & What number is written on the \\
**Ground-truth Answer** & silver & cherry & 3 \\
**Negative Answer** & white & palm & 5 \\
**Feedback** & No, the truck is silver. & No, the tree on the right is a & The number written on the \\
**Question Types** & **Relation** & **Action** & **Other** \\
**Image** & & & \\
**Question** & What does the woman have on & What are the people doing? & What does the second sign \\
**Ground-truth Answer** & & backpack & motorcycling & all-war \\
**Negative Answer** & & jacked & riding blakes & stop \\
**Feedback** & No, the woman has a backpack & No, the people in the picture & No, the second sign says \\  & on her back. & are motorcycling. & “all-war” \\   

Table 2: Selected examples from **EvalQABench** training set, including the ground truth answer, negative answer, and feedback.

**LLM-based Feedback Generation.** With the candidate's negative answer, we then focus on generating error feedback. We consider the feedback describing the reason for incorrectness will help the MLLM obtain a deeper understanding. We thus utilize the LLM Llama 2  to generate the feedback by reasoning the ground truth question-answer pairs with the following prompt:

Please rephrase the question and answer: <Q>  <A> into one short description.

After processing by Llama 2, we can get feedback like _"No, the left of the picture shows roses."_. Moreover, we use similar manual filtering strategies that are used in the negative answer generation step to remove the noisy samples with wrong formats or empty output.

In summary, we start by randomly selecting 100,000 samples from 443,758 annotated VQA pairs in the VQAv2 training set  to generate negative answers. After manual filtering, this number is reduced to 61,094 samples. We then generate feedback for each sample and further filter out those with incorrect formats, resulting in a final set of 41,592 samples. For the training set of EvalQABench, we create a one-positive-one-negative format by randomly selecting 32,000 negative samples from the 41,592 filtered samples, yielding a total of 64,000 training data points. For the validation and test subsets, we follow a similar sampling procedure. We randomly select 100,000 samples from the VQAv2 validation set, resulting in 41,907 negative samples. From these, we randomly select 2,500 negative samples each for the validation set and the test set.

### Model Architecture

In this subsection, we introduce the model architecture of LOVA\({}^{3}\). This model is built upon the prevalent MLLM LLaVA-1.5  with three key components: Vision Encoder, MLP Adapter, and Large Language Model. For the vision encoder, we follow LLaVA-1.5  and implement it with a pre-trained CLIP-Large vision encoder  with resolution \(336 336\). For the large language model, we adopt the widely used instruction fine-tuned model, Vicuna-7B . Following , the MLP adapter is a simple two-layer MLP since such a simple design is better for reserving the visual information while achieving running efficiency. In this study, we leverage LLaVA-1.5 to build upon because of its exceptional performance and highly reproducible training and validating codes. Other outstanding MLLMs, such as CogVLM  and Qwen-VL , are pre-trained on billions-scale datasets or in-house datasets. This scale of data makes the training process difficult to replicate and poses challenges in incorporating our proposed training tasks, GenQA and EvalQA.

### Training

For brevity, we denote the LOVA\({}^{3}\) model as \(F_{M}\). Given an image \(X_{I}\), our target is to enforce \(F_{M}\) to generate the response \(X_{R}\):

\[X_{R}=F_{M}(X_{T},X_{I}),\] (1)

where \(X_{T}\) represents the input text. \(X_{T}\) can be an example of the three types: 1) VQA data, e.g., _"What color is the pot?"_; 2) GenQA data like _"Can you provide a concise question and answer based on the image?"_; 3) EvalQA data, such as _"What kind of flowers are on the picture to the left?: pansy.  examine the correctness of this question and answer according to the image content. Output Yes or No with the feedback"_. Accordingly, the input instruction template can be unified into the following ones:

Figure 2: Illustration of the proposed pipeline for generating negative answers and feedback.

We follow previous MLLMs[42; 17], and design the training objective in an autoregressive manner:

\[_{i=i}^{L} p(X_{R}|X_{T},X_{I})=_{i}^{L}p_{}(x_{i}|X_{T},X _{I},X_{R,<i}),\] (2)

where \(x_{i}\) is the current prediction token and \(L\) denotes the response sequence length. \(\) denotes the trainable parameters.

## 4 Experiments

### Datasets and Settings

**Training Datasets.** For the fair comparison, we utilize the 665K instruction-following dataset introduced in LLaVA1.5, combined with the 842K GenQA data as outlined in Tab. 1, and an additional 64K data comprising one-positive-one-negative pairs as described in Section 3.2, totaling our training datasets with 1.5M samples. It is important to note that the datasets and annotations used in both VQA and GenQA are the same. There are no additional datasets involved, thus avoiding unfair comparisons caused by the introduction of new instruction data. For EvalQA, we adopt VQAv2 to build the training set, which is already included in the original 665K instruction dataset.

**Validation Datasets.** We assess LOVA\({}^{3}\) on 10 widely used multimodal datasets and benchmarks. (1) VQAv2  and GQA  are two large-scale annotated VQA datasets comprising 430K and 943K instances. (2) VizWiz  is a challenging dataset comprising 8000 instances of test-dev set. Most of the images in this dataset are blurred, making it difficult to respond. (3) ScienceQA  is a benchmark comprising 21k multimodal multiple-choice questions with diverse science topics. (4) POPE  is a benchmark for evaluating the object hallucination in the MLLM. (5) MME , SEED-Bench , MMBench , LLaVA-Bench , MM-Vet  are five prominent multimodal benchmarks designed to evaluate various capabilities of MLLMs, including object existence, color recognition, counting, OCR, etc.

**Competitors.** We compare LOVA\({}^{3}\) with other SOTA models inlcuding MiniGPT-4 , BLIP2 , InstructBLIP , mPLUG-owl , LLaMA-AdapterV2  and LLaVA-1.5 . We report the results from their paper or the benchmark leaderboard.

**Implementation Details.** To ensure a fair comparison, we train the LOVA\({}^{3}\)-7B model without tuning any hyperparameters of LLaVA-1.5  from its original supervised finetuing stage. The model is trained for one epoch across three tasks: VQA, GenQA, and EvalQA. Specifically, we employ the AdamW  optimizer with a learning rate of \(2 10^{-5}\) and a total batch size of 128. The training process takes 24.5 hours on an 8 Nvidia A100 (40G) GPU setup. Moreover, we also replace the LLM from Vicuna-7B to Phi-1.5B to evaluate smaller LLMs. We train LLaVA-Phi-1.5 and LOVA\({}^{3}\)-1.5B by using the same training recipe. The only difference of training with Phi-1.5 is that we increase the learning rate from \(2 10^{-5}\) to \(4 10^{-5}\) to ensure the higher performance. The model LLaVA-Phi-1.5 is trained with the original 665K VQA instruction data as the baseline. The model LOVA\({}^{3}\)-1.5B is trained with our proposed 1.5M mixture data including VQA, GenQA, EvalQA data.

Figure 3: Examples from the manual filtering and error correction process. Red text indicates error answers, while Green text represents manually corrected answers.

### Main Results

**Generic tasks.** As shown in Tab. 3, LOVA\({}^{3}\)-7B outperforms LLaVA1.5 across all five datasets and obtains 3.6% improvement on VizWiz dataset, 1.3% improvement on GQA, 1.8% improvement on VQAv2 (1.932 samples are correctly predicted), and 1.2% improvement on ScienceQA. As for the object hallucination benchmarks, our model attains 87.4% accuracy at an average of its three subsets. Remarkably, these enhancements in VQAv2 and GQA performance are achieved without any extra datasets, underscoring the significant impact of integrating GenQA and EvalQA into our training to promote performance improvements on these generic VQA tasks. Based on the results from smaller LLMs, our LOVA\({}^{3}\)-1.5B outperforms the baseline LLaVA-Phi-1.5 on VQAv2, GQA, VizWiz, and ScienceQA by 2.6%, 2.5%, 3.4%, and 0.5%, respectively. This demonstrates a consistent improvement when training with our LOVA\({}^{3}\) framework across varying LLM sizes. Additionally, a comparison of improvements for both 7B and Phi-15B on the VizWiz dataset highlights the advantage of our LOVA\({}^{3}\) framework for this VQA task.

**MME, SEED-Bench, MMBench, LLaVA-Bench.** In Tab. 4, we evaluate four prevalent multimodal benchmarks, where our LOVA\({}^{3}\)-7B surpasses LLaVA1.5 with 42.0% on MME benchmark, 0.9% increase in accuracy on SEED-Bench, 2.5% on MMBench (En), 2.2% MMBench (Cn) and 4.3% on LLaVA-Bench. Such results showcase enhanced multimodal reasoning capabilities for complex tasks compared to vanilla LLaVA1.5, which is solely trained with VQA tasks. By investigating the results produced by LOVA\({}^{3}\)-1.5B on these multimodal benchmarks, one can see greater improvements in the smaller models. Notably, LOVA\({}^{3}\)-1.5B achieves an impressive 98.2% improvement on the MME benchmark. The lower results of LLaVA-Phi-1.5 and LOVA\({}^{3}\)-1.5B on MMBench (Cn) may be attributed to a lack of Chinese training data in the Phi-1.5 training process.

**MM-Vet.** In Tab. 5, we compare LOVA\({}^{3}\)-7B with other approaches on MM-Vet, which is a challenging benchmark including numerous complex VQA samples that demand integration of several multimodal capabilities for answering. As illustrated in Tab. 5, the results show that our LOVA\({}^{3}\)-7B outperforms LLaVA-1.5 by 4.0% at an average. Such improvement demonstrates the effectiveness of LOVA\({}^{3}\)-7B in solving these challenging multimodal questions. Based on the results on LOVA\({}^{3}\)-1.5B and LLaVA-Phi-1.5, one can see a greater improvement than LOVA\({}^{3}\)-7B.

    &  &  &  &  &  &  \\  & & & & Image & En & Cn & All \\  LLaVA-Phi-1.5 & _VQA_ & Phi-1.5B & 1114.7 & 58.2 & 53.7 & 4.1 & 59.0 \\ LOVA\({}^{3}\)-1.5B (ours) & _VQA_ & _GenQA_ & _EvalQA_ & Phi-1.5B & **12129.9\({}_{+9.82}\)** & **60.1\({}_{+1.9}\)** & **55.9\({}_{+2.2}\)** & **10.4\({}_{+6.3}\)** & **59.1\({}_{+0.1}\)** \\  BLIP-2 & _VQA_ & Vicuna-13B & 1293.8 & 49.7 & – & – & 38.1 \\ IntrucIntBLIP & _VQA_ & _VQG_ & Vicuna-7B & – & 58.8 & 36.0 & 23.7 & 60.9 \\ IntrucIntBLIP & _VQA_ & _VQG_ & Vicuna-13B & 1212.8 & – & – & – & 58.2 \\ mPLUG-on & _VQA_ & Lima-7B & 967.3 & 37.9 & – & – & \\ LLaMA-AdaperV2 & _VQA_ & Lima-7B & 972.7 & 35.2 & 41.0 & – & – \\  LLaVA-1.5 & _VQA_ & Vicuna-7B & 1510.7 & 66.2 & 64.3 & 58.3 & 64.0 \\ LOVA\({}^{3}\)-7B (ours) & _VQA_ & _GenQA_ & _EvalQA_ & Vicuna-7B & **1552.7\({}_{+0.2}\)** & **67.1\({}_{+0.9}\)** & **66.8\({}_{+2.5}\)** & **60.5\({}_{+2.2}\)** & **68.3\({}_{+4.3}\)** \\   

Table 4: Results on multimodal benchmarks, including MME  and SEED-Bench , MMBench  and LLava-Bench 

    &  &  &  &  &  &  &  \\  & & & test-dev & test & test-dev & img & avg \\  LLaVA-Phi-1.5 & VQA & Phi-1.5B & 73.2\({}^{*}\) & 56.1\({}^{*}\) & 33.8 & 57.3 & **87.6** \\ LOVA\({}^{3}\)-1.5B (ours) & _VQA_ & _GenQA_ & _EvalQA_ & Phi-1.5B & **75.8\({}^{*}_{+2.6}\)** & **58.6\({}^{*}_{+2.5}\)** & **37.2\({}^{*}_{+3.4}\)** & **57.8\({}^{*}_{+0.5}\)** & 86.0\({}^{*}_{-1.6}\) \\  BLIP-2 & _VQA_ & Vicuna-13B & 41.0 & 41.3 & 19.6 & 61.0 & 85.3 \\ IntrucIntBLIP & _VQA_ & _VQG_ & Vicuna-7B & – & 49.2 & 34.5 & 60.5 & – \\ IntrucIntBLIP & _VQA_ & _VQG_ & Vicuna-13B & – & 49.5 & 33.4 & 63.1 & 78.9 \\ IDEFICS-9B & _VQA_ & Llama-7B & 50.9 & 38.4 & 35.5 & 44.2 & – \\ Qwen-VL & _VQA_ & Qwen-7B & 78.8\({}^{*}\) & 59.3\({}^{*}\) & 35.2 & 67.1 & – \\  LLaVA-1.5 & _VQA_ & Vicuna-7B & 78.5\({}^{*}\) & 62.0\({}^{*}\) & 50.0 & 66.8 & 85.9 \\ LOVA\({}^{3}\)-7B(ours) & _VQA_ & _GenQA_ & _EvalQA_ & Vicuna-7B & **80.3\({}^{*}_{+1.8}\)** & **63.3\({}^{*}_{+1.3}\)** & **53.6\({}_{+3.6}\)** & **68.0\({}_{+1.2}\)** & **87.4\({}_{+1.5}\)** \\   

Table 3: Results on five generic tasks including VQAv2 , GQA , VizWiz , ScienceQA , and POPE . The first two columns represent the results on held-in datasets marked as \({}^{*}\), and the last three columns represent the held-out datasets. The best result on each subtask is **bolded**.

### Ablation Study

We split the data used in the GenQA task into two groups: GenQA-General and GenQA-Grounding. The findings, presented in Tab. 6, are instrumental in investigating the contributions of GenQA and EvalQA to model efficacy. **(1)** Comparing the first four rows, one can find that both GenQA-General and EvalQA data are more effective in improving performance than GenQA-Grounding. **(2)** By comparing rows 4 and 7, it demonstrates the effectiveness of EvalQA across five datasets, especially on MME. **(3)** When comparing rows 6 and 7, by removing GenQA-General from the finetuning corpus, the performance drops significantly on MME and VizWiz. **(4)** Compare the rows 0 and 3, one can observe that even adding 64K data into the training, there are obvious improvements in GQA, ScienceQA, and MME. By analyzing the data size, we did not introduce any new datasets for training the GenQA task. For EvalQA, we only added 32K new negative answer annotations while retaining the original questions used for training VQA capabilities. The details of the data size are provided in the right column in Tab. 6.

### Training with Gemini-Generated EvalQA Data

Rather than using the open-source model Fuyu-8B  to create the training data for EvalQABench, we also explore the use of the commercial model Gemini-1.5-Flash1 as both the MLLM and LLM in Fig. 2 to generate negative answers and one-sentence feedback. The experimental results, presented in Tab. 7, indicate that regardless of whether we use the open-source model Fuyu-8B or the commercial model Gemini-1.5-Flash, our proposed training paradigm LOVA3 consistently improves performance across both smaller and larger baseline models.

    &  &  &  &  &  &  &  \\  &  &  &  & & & & & & & \\ 
0 &  & 62.0 & 50.0 & 66.8 & 85.9 & 1510.7 & 665K \\ 
1 &  & & & 63.1 & 53.1 & 67.4 & 86.9 & 1550.7 & 722K \\
2 &  & & & 62.8 & 50.9 & 66.4 & 86.6 & 1495.8 & 120K \\
3 &  & & & 62.8 & 49.1 & 67.8 & 87.0 & 1535.6 & 64K \\
4 &  & & & 63.3 & 53.2 & 67.4 & 86.7 & 1523.6 & 842K \\
5 &  & & & **63.7** & **54.4** & 67.0 & 86.9 & 1520.8 & 786K \\
6 &  & & & 63.1 & 51.1 & 67.5 & 86.8 & 1478.7 & 184K \\
7 &  & & & 63.3 & 53.6 & **68.0** & **87.4** & **1552.7** & 906K \\   

Table 6: Ablation studies on different finetuning datasets. The model is LOVA3–7B.

   Method & Train Paradigm & LLM & Rec & OCR & Know & Gen & Spat & Total \\  LLaVA-Phi-1.5  & _VQA_ & Phi-1.5B & – & – & – & – & – & – & 22.2 \\ LOVA3–1.5B (ours) & _VQA_, _GenQA_, _EvalQA_ & Phi-1.5B & – & – & – & – & – & **28.1**\({}_{+5.9}\) \\  MiniGPT-4  & _VQA_ & Vicuna-7B & 27.4 & 15.0 & 12.8 & 13.9 & 20.3 & 22.1 \\ BLIP-2  & _VQA_ & Vicuna-13B & 27.5 & 11.1 & 11.8 & 7.0 & 16.2 & 22.1 \\ InstructBLIP  & _VQA_, _VQG_ & Vicuna-7B & 32.4 & 14.6 & 16.5 & 18.2 & 18.6 & 26.2 \\ InstructBLIP  & _VQA_, _VQG_ & Vicuna-13B & 30.8 & 16.0 & 9.8 & 9.0 & 21.1 & 25.6 \\  LLaVA-1.5  & _VQA_ & Vicuna-7B & 37.0 & 21.0 & 17.6 & 20.4 & 24.9 & 31.2 \\ LOVA3–7B (ours) & _VQA_, _GenQA_, _EvalQA_ & Vicuna-7B & **41.5**\({}_{+4.5}\) & **23.6**\({}_{+2.6}\) & **23.9**\({}_{+6.3}\) & **24.6**\({}_{+4.2}\) & **30.3**\({}_{+5.4}\) & **35.2**\({}_{+4.0}\) \\   

Table 5: Multimodal reasoning ability on MM-Vet . Rec denotes Recognition; Know denotes knowledge; Gen denotes Language generation; and Spat denotes Spatial awareness.

    &  &  &  &  &  &  &  &  \\  & & & test-dev & & & test-dev & img & avg & & & & \\  LLaVA-Phi-1.5  & Phi-1.5B & 73.2\({}^{*}\) & 56.1\({}^{*}\) & 33.8 & 57.3 & **87.6** & 1114.7 & 58.2 & 53.7 & 4.1 & 59.0 & 22.2 \\ LOVA3–1.5B (ours) & Phi-1.5B & **75.8\({}^{*}\)** & **58.4\({}^{*}\)** & **36.9** & **57.8** & 85.8 & **1202.9** & **60.5** & **55.5** & **7.82** & **60.0** & **25.1** \\  LLaVA-1.5  & Vicuna-7B & 78.5\({}^{*}\) & 62.0\({}^{*}\) & 50.0 & 66.8 & **85.9** & 1510.7 & 66.2 & 64.3 & **58.3** & 64.0 & 31.2 \\ LOVA3–7B (ours) & Vicuna-7B & **80.3\({}^{*}\)** & **63.4\({}^{*}\)** & **54.2** & **70.8** & 85.6 & **1526.8** & **67.6** & **66.5** & 57.6 & **67.7** & **32.2** \\   

Table 7: Results on 10 multimodal datasets. The 64K training data for EvalQA task is generated by the Gemini-1.5-Flash model.

### Benchmark of EvalQABench

We report the evaluation results on our EvalQABench test set in Tab. 8 to validate the EvalQA ability of current SOTA models and LOVA\({}^{3}\). We select BLIP2 , InstructBLIP , CogVLM , Qwen-VL-Chat , IntermLM-XC , and LLAVA1.5  for the comparison. We ask these models to answer "Yes" or "No" strictly and record the results for calculating the Accuracy, Precision, F1 score, and No (%) metrics. Here, No (%) indicates the percentage of results classified as "No," which ideally should approximate 50% due to the one-positive-one-negative setting utilized in our test set. As indicated by the data presented in the table, BLIP2 predominantly yields "No" responses across most test instances. Among the state-of-the-art MLLMs, InternLM-XC stands out by delivering superior performance on these four metrics. Trained with EvalQA data, LOVA\({}^{3}\) shows several improvements over our baseline LLAVA1.5 by margins of 14.66%, 17.87%, and 9.92% in Accuracy, Precision, and F1 Score, respectively.

## 5 Conclusion and Limitations

In this work, we propose a novel multimodal framework, **LOVA\({}^{3}\)**, which is capable of mimicking the human visual question answering, asking, and assessment to achieve deeper multimodal understanding. We introduce two additional training tasks, **GenQA** and **EvalQA**, to help MLLM acquire these abilities. We establish **EvalQABench**, a novel benchmark to assess the VQA samples between multiple MLLMs. Experimental results show that LOVA\({}^{3}\) achieves superior performance across various benchmarks, including MM-Vet, SEED, and VizWiz, demonstrating the effectiveness of the two additional abilities.

**Limitations.** (1) Due to computational constraints, we do not test larger LLMs, such as the 13B or 34B variants. However, we believe that our LOVA\({}^{3}\) could be beneficial for larger LLMs, as other MLLMs have shown performance improvements with increased LLM scale. (2) GenQA and EvalQA as two additional tasks increase training costs, but it is inevitable for an MLLM to acquire new capabilities. (3) Due to the limited scope of instruction tuning datasets, LOVA\({}^{3}\) cannot address domain-specific multimodal tasks well, such as text-centric VQA or mathematic-relevant VQA.

## 6 Acknowledgement

This research is supported by National Research Foundation, Singapore and A*STAR, under its RIE2020 Industry Alignment Fund - Industry Collaboration Projects (IAF-ICP) grant call (Grant No. I2001E0059) - SIA-NUS Digital Aviation Corp Lab. Mike Zheng Shou is supported by the National Research Foundation, Singapore under its NRFF Award NRF-NRFF13-2021-0008. Pan Zhou was supported by the Singapore Ministry of Education (MOE) Academic Research Fund (AcRF) Tier 1 grants (project ID: 23-SIS-SMU-028 and 23-SIS-SMU-070).

    &  &  \\  & & Accuracy & Precision & F1 Score & No (\%) \\   \\ BLIP2  & Flan-T5-XXL-11B & 58.00 & 82.79 & 32.47 & 87.80 \\   \\ InstructBLIP  & Vicuna-7B & 38.04 & 41.49 & 48.47 & 29.76 \\ InstructBLIP  & Vicuna-13B & 61.42 & 57.60 & 69.18 & 24.82 \\ CogVLM  & Vicuna-7B & 60.64 & 56.59 & 69.88 & 19.32 \\ Qwen-VL-Chat  & Qwen-7B & 63.66 & 63.48 & 63.90 & 49.34 \\ IntermLM-XC  & InterLM-7B & 69.58 & 70.66 & 68.76 & 52.62 \\  LLAVA-1.5  & Vicuna-7B & 64.92 & 61.28 & 69.80 & 33.84 \\ LOVA\({}^{3}\)-7B (ours) & Vicuna-7B & \(_{+14.66}\) & \(_{+17.87}\) & \(_{+9.92}\) & 49.26 \\   

Table 8: Results of multimodal large language models on the test set of **EvalQABench (ours)**.