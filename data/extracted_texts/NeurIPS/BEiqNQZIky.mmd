# Efficiently Learning Significant Fourier Feature Pairs

for Statistical Independence Testing

Yixin Ren\({}^{1}\), Yewei Xia\({}^{1,4}\), Hao Zhang\({}^{3,*}\), Jihong Guan\({}^{2,*}\), Shuigeng Zhou\({}^{1,*}\)

\({}^{1}\)Shanghai Key Lab of Intelligent Information Processing, and School of

Computer Science, Fudan University, Shanghai, China

\({}^{2}\)Department of Computer Science and Technology, Tongji University, Shanghai, China

\({}^{3}\)SIAT, Chinese Academy of Sciences, Shenzhen, China

\({}^{4}\)Machine Learning Department, MBZUAI, Abu Dhabi, UAE

{yxren21, ywxia23}@m.fudan.edu.cn, h.zhang10@siat.ac.cn

jhguan@tongji.edu.cn, sgzhou@fudan.edu.cn

Corresponding authors.

###### Abstract

We propose a novel method to efficiently learn significant Fourier feature pairs for maximizing the power of Hilbert-Schmidt Independence Criterion (HSIC) based independence tests. We first reinterpret HSIC in the frequency domain, which reveals its limited discriminative power due to the inability to adapt to specific frequency-domain features under the current inflexible configuration. To remedy this shortcoming, we introduce a module of learnable Fourier features, thereby developing a new criterion. We then derive a finite sample estimate of the test power by modeling the behavior of the criterion, thus formulating an optimization objective for significant Fourier feature pairs learning. We show that this optimization objective can be computed in linear time (with respect to the sample size \(n\)), which ensures fast independence tests. We also prove the convergence property of the optimization objective and establish the consistency of the independence tests. Extensive empirical evaluation on both synthetic and real datasets validates our method's superiority in effectiveness and efficiency, particularly in handling high-dimensional data and dealing with large-scale scenarios.

## 1 Introduction

Testing for independence is a crucial and challenging task in machine learning and statistics, with wide-range applications in causal inference , feature selection  and deep learning . Its primary objective is to determine whether two random variables, \(X\) and \(Y\) are independent, based on the observations of the underlying joint distribution \(_{XY}\). While traditional independence tests, such as Pearson's correlation coefficient  and Kendall's \(\), can only detect monotonic relationships between low-dimensional variables, more modern tests  aim to deal with complex non-linear interactions in much more challenging higher-dimensional space .

One class of nonlinear dependence measures  aims to capture distributional characteristics using kernel embeddings , primarily derived from the cross-covariance operators in the reproducing kernel Hilbert space (RKHS). Among them, Hilbert-Schmidt Independence Criterion (HSIC)  is the most popular one. It utilizes the squared Hilbert-Schmidt norm to detect dependence and exhibits outstanding performance across various data contexts by choosing suitable kernels. On the other hand, some other fundamental nonlinear dependence measures employ characteristic functionsto detect the smoothed discrepancy between the joint distribution and the product of marginals. By employing appropriate characteristic functions, the statistic [39; 40] computes the covariance between distances of variable pairs. It has been demonstrated that these distance-based methods are equivalent to HSIC with specific kernels . However, all these measures suffer from the drawback of requiring quadratic time (v.r.t. the sample size \(n\)) to compute the feature covariance and necessitating fixed kernel or distance functions, rendering them impractical on large-scale datasets due to the unaffordable time cost and lacking flexibility in handling complex scenarios.

To address these challenges, a multitude of works grounded on these measures have emerged. Upon HSIC,  proposes some linear-time tests including a block-averaged statistic, a statistic with Nystrom approximation, and one with finite-dimensional feature mappings using random Fourier features (RFF) . For convenience, these tests are referred to as BHSIC, NyHSIC, and FHSIC, respectively. FHSIC and NyHSIC are observed to have a considerable advantage over BHSIC. However, a remaining drawback of these methods is that the features are not learnable. Therefore, these methods lack enough adaptability to complex settings, thus leading to performance degradation.

In addition to time efficiency, another research direction [1; 30] aims to make independence tests adaptive to better capturing distributional distinctions. These methods either select/combine appropriate kernels from a predefined set or learn parameterized kernels. Nonetheless, their criteria still inherit the quadratic time complexity of HSIC, thus cannot be readily applied to large-scale data.

Furthermore, some approaches [17; 32] try to address both challenges simultaneously. For instance, HSICAgg  suggests combining several kernels from a predefined set (e.g. kernels with different preset bandwidths) and aggregating the test results for improving performance. Additionally, an incomplete \(U\)-statistic of HSIC is proposed to ensure computational efficiency. Nevertheless, selecting from a predefined set of kernels imposes limitations on flexibility, and in cases where scaling optimization is required on each dimension, the number of kernel pairs escalates exponentially. Also, NFSIC  proposes to combine a time-efficient technique called analytic kernel embeddings [8; 17] and learn the important local distributional features. However, its learning objective is merely a lower bound of test power and demands a substantial number of samples to ensure accuracy.

In this paper, we propose a novel test method that flexibly learns distributional features while maintaining high efficiency. We first reinterpret HSIC from a frequency-domain perspective, then we point out its potential shortcomings with an elaborate example and indicate corresponding improvement directions. Finally an central optimization objective is derived by directly modeling test power, which can be computed in linear time while maximizing the test performance. Comparing with  that also addresses the kernel learning problem in independence testing with a time/space complexity of \((n^{2})\), our criteria for learning are designed to have a complexity of \((n)\) for both space and time. Consequently, the whole test framework can efficiently handle large-scale data.

**Contributions.** In summary, the contributions of the work are as follows: 1) We propose a novel approach that efficiently learns significant Fourier feature pairs for maximizing the power of HSIC-based independence tests. 2) We design an optimization objective that can be computed in linear time, which is derived by directly modeling test power. 3) We theoretically establish the non-asymptotic convergence property of the optimization objective and demonstrate the consistency of our method. 4) We conduct extensive experiments on both synthetic and real data, showcasing its superiority in effectiveness and efficiency in handling high-dimensional data (e.g. image data) and addressing large-scale scenarios.

**Outline.** The rest of the paper is organized as follows: Sec. 2 reviews HSIC-based statistical independence tests. Sec. 3 reinterprets HSIC from a frequency-domain perspective, and explain its potential shortcomings with an elaborate example and indicate corresponding improvement directions. Sec. 4 designs an optimization objective by directly modeling test power, which can be computed in linear time. Sec. 5 presents the theoretical analysis and Sec. 6 evaluates the performance of the proposed method on synthetic and real dataset. We conclude the paper in Sec. 7.

## 2 Preliminaries and Notations

We begin by introducing notions and reviewing the hypothesis testing framework for independence tests. Let \(\) be separable metric space, typically \(^{d_{x}}^{d_{y}}\). \(_{XY}\) denotes a Borel probability measure defined on \(\), while \(_{X}\) and \(_{Y}\) denote the respective marginal distributions. Given \(n\)independent and identically distributed (i.i.d) samples \(Z:=(X,Y)=\{(x_{i},y_{i})\}_{i=1}^{n}\) with distribution \(_{XY}\), we aim to test whether \(X,Y\) are independent (i.e., \(X\!\!\! Y\)). This corresponds to a hypothesis testing problem formulated as \(_{0}:_{XY}=_{X}_{Y}\) versus \(_{1}:_{XY}_{X}_{Y}\).

The testing procedure is as follows: First, define the statistic \(\) and calculate its estimated value using the samples. Then, choose a significance level \(\) (typically set to \(0.05\)), which represents the probability that the sampling of \(\) under \(_{0}\) is at least as extreme as the observed value. Finally, the null hypothesis \(_{0}\) is rejected if the \(p\)-value is not greater than \(\).

Two types of errors may occur in this procedure. Type I error occurs when \(_{0}\) is falsely rejected, while Type II error happens when \(_{0}\) is incorrect but not rejected. A good test  needs to control Type I error within \(\) while maximizing the testing power (\(1-\)Type II error rate).

For independence tests, a commonly used statistic is HSIC, defined as follows:

**Definition 1**.: _[_14_]_ _Let \(\) be an RKHS with kernel \(k:\) and \(\) be a second RKHS on \(\) with kernel \(l:\), the HSIC between \(X\) and \(Y\), denoted as HSIC\((X,Y)\) is defined as_

\[k(X,X^{})l(Y,Y^{})\!+\!k (X,X^{})l(Y,Y^{})\!-\!2_{ X^{}Y^{}}_{X}k(X,X^{})_{Y}l(Y,Y^{ }), \]

_where \((X^{},Y^{})\) is a independent copy of \((X,Y)\). An estimator of HSIC\((X,Y)\) is given by_

\[_{b}(Z):=}_{i,j}k_{ij}l_{ij}+}_ {i,j,q,r}k_{ij}l_{qr}-2}_{i,j,q,j}k_{ij}l_{iq}=}(), \]

_where \(k_{ij}:=k(x_{i},x_{j})\), \(l_{ij}:=l(y_{i},y_{j})\) are the entries of the \(n n\) kernel matrices \(\), \(\) respectively, \(=-^{T}\) is the centering matrix and \(\) is a vector of ones._

## 3 Revisiting HSIC from Frequency Domain Perspective

We denote \(\) as the Fourier transform, and \(^{-1}\) as its inverse. When the kernels \(k,l\) are translation-invariant, i.e., there exist functions \(,_{k},_{l}\) such that for all \((x,x^{})\) and \((y,y^{})\),

\[(x-x^{},y-y^{})=_{k}(x-x^{})_{l}(y-y^{}) =k(x,x^{})l(y,y^{}). \]

Then, according to the results of [36, Corollary 4], the HSIC with function \(\) can be formulated as

\[(X,Y)=_{^{d_{x}}^{d_{y}}} _{_{XY}}()-_{_{X}_{Y}}() ^{2}(^{-1})()d, \]

where \(=(_{x},_{y})^{d_{x}}^{d_{y}}\), \(_{x},_{y}\) are the frequencies of \(X\) and \(Y\) respectively, and

\[_{_{XY}}():= e^{-i(_{x}^{T}x+_{y}^{T}y)}d _{XY},\ _{_{X}_{Y}}():=( e^{-i _{x}^{T}x}d_{X})( e^{-i_{y}^{T}y}d _{Y}) \]

are the characteristic functions of \(_{XY}\) and \(_{X}_{Y}\), respectively. Intuitively, Eq. (4) means that HSIC can be understood as the difference between the joint distribution and the product of the marginal distributions in the frequency domain, with different weights \((^{-1})()\) being attached to different frequencies, which are determined by the kernel function. When \(^{-1}\) is almost everywhere non-zero, it can be shown that the kernel is characteristic . The characteristic condition ensures that the criterion is discriminative for discrepancies at almost all frequencies. However, with inappropriate choices of \(^{-1}\), the differences may not be significant enough. We explain this with an example:

**Example.** Consider the Sinusoid model that \(:=[-,]^{2}\) and \((X,Y) p_{xy}(x,y) 1+(_{0}x)(_{0}y)\), where \(p_{xy}\) is the probability density function and \(_{0}\) is a positive integer. Combining Eq. (5), we can calculate that \(_{_{X}_{Y}}()=(_{x})(_{y})\) and \(_{_{XY}}()=(_{x})(_{y})+[( _{x}+_{0})+(_{x}-_{0})][(_{y}+_ {0})+(_{y}-_{0})]\), where \(\) is the Dirac delta function, thus the difference between them only relies on the frequency \(_{0}\). When the Gaussian kernels with width \(_{x}\) and \(_{y}\) are used, i.e., \(k(x,x^{})=(-\|x-x^{}\|_{2}^{2}/(4_{x}^{2})),l(y,y^{ })=(-\|y-y^{}\|_{2}^{2}/(4_{y}^{2}))\), then the inverse Fourier transform of \(\) is \((^{-1})(_{x},_{y})=^{-1}_{x}_{y} (-(_{x}^{2}_{x}^{2}+_{y}^{2}_{y}^{2}))\). Hence HSIC\((X,Y)=4^{-1}_{x}_{y}(-(_{x}^{2}+_{y}^{2})_{0}^{2})\) whose maximum is taken at \(_{x}^{*}=_{y}^{*}=1/(_{0})\), indicating that the widths need to be adjusted to focus on some specific frequencies. If the common setting  is adapted, which uses mid-widths (i.e., the median distance does not change with \(_{0}\) since the marginal distributions do not change with \(_{0}\)), then the criterion will exponentially decline to \(0\) as \(_{0}\) increases. In contrast, the criterion using the adaptive optimization width \((1/_{0},1/_{0})\) decreases at a rate of \((_{0}^{-2})\), which is a considerable improvement.

This example illustrates the loss of the discriminatory power of the criterion when an inappropriate \(^{-1}\) is chosen. The discriminatory power of the criterion heavily impacts the sample size required for the test to obtain significant results in practice, and existing inflexible configurations may lead to inadequate test power in the presence of reasonably large sample sizes. Consequently, it is important to design learnable \(^{-1}\). To this end, we subsequently design a learnable objective and let it be optimized in a data-driven manner. Before this, we provide an approach to make the criterion be computed efficiently. This can be achieved by sampling in the frequency domain. Formally, a finite-dimensional approximation in the frequency domain of the integral in Eq. (4) is given as follows:

\[_{}(X,Y):=D_{y}}_{i=1}^{D_{x}}_{j=1}^{ D_{y}}_{_{X\!Y}}(_{x;i},_{y;j})-_{ _{X\!Y}}(_{x;i},_{y;j})^{2}, \]

where \(\{_{x;i}\}_{i=1}^{D_{x}},\{_{y;j}\}_{j=1}^{D_{y}}\) are sampled independently with the measure \(^{-1}_{k},^{-1}_{l}\), respectively. Note that \(^{-1}\) is a product measure, i.e., \(^{-1}=(^{-1}_{k})(^{-1}_{l})\). This type of approximation is also called random Fourier features (RFF)  that had been applied to various kernel algorithms. We will incorporate this technique to efficiently perform computation later.

## 4 Learning Significant Fourier Feature Pairs

### HSIC with Learnable Fourier Feature Pairs

To design \(^{-1}\), we need to make sure that \((^{-1})=^{d_{x}}^{d_{y}}\) to meet the characteristic condition and that its integral over the full space is \(1\) to ensure it is a probability measure. Also, for practical utility, \(^{-1}\) should embody a familiar probability density function, facilitating sampling procedures. Fortunately, a versatile array of options emerges through the judicious selection of kernels 2 with adjustable parameters. Take kernel \(k\) as an example, some commonly used kernels are listed in Tab. 1, and their inverse Fourier transforms are listed simultaneously.

Additionally, to be able to apply gradient-based optimization techniques, we invoke a method that disentangle the sampled objects and the learnable parameters. Specifically, we leverage a variable transform \(_{_{k}}\) (a bijection function parameterized with \(_{k}\)) to convert the probability measure \(^{-1}_{k}\) into a simple distribution (e.g. a standard Gaussian distribution) \(p_{k}()\). Simultaneously, we relocate the learnable component onto \(X\). Consequently, we can focus on learning parameterized transformations \(_{_{k}}\) and simplifying the computation by enabling sampling directly from \(p_{k}()\).

**Remark.** The above scheme provides a broader form for designing. The mapping \(_{_{k}}\) can be viewed as a feature extractor, which makes it possible to flexibly combine models (e.g., neural network) thus incorporating deep kernel  into the framework. Also, it should be noted that the single kernel example can also be extended to multi-kernel setting  by executing the procedure for each kernel.

Next, we obtain the learnable independence criterion and utilize the sampling technique as in Eq. (6) to compute efficiently. Note that for simplicity, we take the same value for both \(D_{x}\) and \(D_{y}\) in Eq. (6) by default. By the definition, the kernel function can be expressed as

\[_{k}(_{_{k}}x-_{_{k}}x^{} )=[^{-1}_{k}()]= e^{-i^{T}( _{_{k}}x-_{_{k}}x^{})}p_{k}()d. \]

  Kernel & \(_{k}()\) & \(^{-1}_{k}()\) & \(_{_{k}}(x)\) & \(p_{k}()\) \\  Gaussian & \(e^{-_{x}}{2^{2}}}\) & \((2)^{-d_{x}/2} e^{-^{2}\|\|_{2}^{2}/2}\) & \(x/\) & \((2)^{-d_{x}/2}e^{-\|\|_{2}^{2}/2}\) \\ Laplace & \(e^{-_{x}}{2^{2}}}\) & \(}_{d}+_{0}^{2}}\) & \(x/\) & \(}_{d}^{2}}\) \\ Mahalanobis & \(e^{-^{T}^{-1}}\) & \((2)^{-d_{x}/2}||^{-1/2}e^{-^{T}^{-1}/2}\) & \(^{1/2}x\) & \((2)^{-d_{x}/2}e^{-\|\|_{2}^{2}/2}\) \\  

Table 1: Some popular kernels (parameterized by \(,\)) with corresponding density functions.

By applying the frequency sampling technique, we obtain the approximation as

\[_{k}^{()}(_{_{k}}x-_{_{k}}x^{ }):=_{j=1}^{D/2}e^{-i_{k;j}^{T}(_{_{k}}x-_{_{k}}x^{})}=_{j =1}^{D/2}(_{k;j}^{T}(_{_{k}}x-_{ _{k}}x^{})), \]

where \(\{_{k;j}\}_{j=1}^{D/2}\) are sampled independently with distribution \(p_{k}()\) and the last equation is because the kernel function is real. To get a more computationally tractable form, we define

\[_{k}(x):=}[\!(_{1}^{T}_ {_{k}}x)\!,\!(_{1}^{T}_{_{k}}x )\!,...,\!(_{D/2}^{T}_{_{k}}x)\!, \!(_{D/2}^{T}_{_{k}}x)], \]

called learnable RFF of \(k\) then Eq. (8) becomes \(_{k}^{()}(_{_{k}}x-_{_{k}}x^ {})=_{k}(x)_{k}(x^{})^{T}\). The expression with a similar form is also given in , with the difference that we have added learnable parts. For \(Y\), we define the corresponding symbols by substituting \(k\) for \(l\) and \(x\) for \(y\). Also, for convenience, we default to keeping \(Y\) and \(X\) the same number of samples \(D\) from here on. Then the HSIC with learnable RFF pairs can be obtained by replacing \(k,l\) in Eq. (1) to \(_{k}^{()},_{l}^{()}\). Also, the corresponding estimator with sample \(Z\) can be obtained by replacing \(,\) in Eq. (2) to the matrices \(_{X}_{X}^{T},_{Y}_{Y}^{T}\), where \(_{X}:=[_{k}(x_{1});...;_{k}(x_{n})]_{n D}\) and so as define for \(_{Y}\). As a result,

\[_{}(Z):=}(_{X}_{X}^{T}_{Y}_{Y}^{T})=}(_{X}^{T}_{Y} _{Y}^{T}_{X})=}\| _{Xe}^{T}_{Yc}\|_{F}^{2}, \]

where \(_{Xc}:=_{X},_{Yc}:= _{Y}\). The time complexity is analyzed as follows. Since the computation of the mapping \(_{_{k}}x\) depends on the specific design, here we default to analyzing the kernel case shown in Tab. 1. In this case, computing \(_{X},_{Y}\) requires \(\!(nD(d_{x}+d_{y}))\) time. Then calculate \(_{Xe},_{Yc}\) cost \((nD)\). After that, calculate \(_{}(Z)\) cost \((nD^{2})\). Hence, the overall time complexity is \(\!(nD(d_{x}+d_{y}+D))\), i.e. the running time is linear with \(n\).

### Linear-time Optimization Objective

Next, we model the behavior of \(_{}(Z)\) to obtain an optimization objective for maximizing the power of the test. By utilizing the property that \(_{}(Z)\) is a V-statistic, we can extend the results [14, Theorem 1, 2] for \(_{}(Z)\), as shown in the following proposition with the proof given in the Appendix. To simplify, we denote \((x_{i},y_{i})\) as \(z_{i}\) to represent the \(i\)-th sample and denote \(_{k}^{()}(_{_{k}}x_{t}-_{_{k} }x_{u})\) as \(k_{tu}^{()}\) and \(_{l}^{()}(_{_{l}}y_{t}-_{_{l} }y_{u})\) as \(l_{tu}^{()}\).

**Proposition 1** (Asymptotics).: _Let \(h_{iqr}^{()}:=_{(t,u,v),x}^{(t,j,q,r)}k_{tu}^{()}t_ {tu}^{()}+k_{tu}^{()}t_{vw}^{()}-2k_{uv}^{()}t_{tv}^{( )}\), where the sum represents all ordered quadruples \((t,u,v,w)\) drawn without replacement from \((i,j,q,r)\). Then, Under the null hypothesis \(_{0}\), \(_{}(Z)\) coverages in distribution to_

\[n_{}(Z)_{l=1}^{}_{l}_{1l}^ {2},\ \ _{l}g_{l}(z_{j})=_{z_{i},z_{q},z_{r}}h_{iqr}^{()}g_{l}(z_{i}) dF_{z_{i},z_{q},z_{r}}, \]

_where \(_{11}^{2},_{12}^{2},...\) are independent \(_{1}^{2}\) variates and \(_{l}\) is the solution to the eigenvalue problem as in the right of Eq. (11). Also, under the alternative \(_{1}\), \(_{}(Z)\) converges in distribution as_

\[n^{}_{}(Z)-_{Z}_{ }(Z)\!(0,_{}^{2}),\ _{}^{2}:=16_{i}(_{j,q,r}h_{ijqr}^{( )})^{2}-(_{Z}h_{ijqr}^{()})^{2} \]

_with the simplified notation \(_{j,q,r}:=_{z_{j},z_{q},z_{r}}\) and \(_{Z}:=_{z_{i},z_{j},z_{q},z_{r}}\)._

According to Proposition 1, the power of the test with \(_{}\) can be formulated by

\[_{_{1}}(n_{}(Z)>r_{}) (_{Z}_{}(Z)-r_{}}{ _{}}), \]

where \(\) is the standard normal CDF and \(r_{}\) is the threshold, i.e. \((1-)\)-quantile of distribution given in Eq. (11) that exactly controls Type I error rate to the nominal level \(\). Hence, to maximize the power of the test, a natural criterion is \([n_{Z}_{}(Z)-r_{}]/(_{})\). Next, we provide its estimation which can be computed in linear time.

We first consider obtaining the estimator of the numerator part. For the term \(_{Z}_{}(Z)\), we can estimate it with \(_{}(Z)\) as in Eq. (10). The estimation of the threshold \(r_{}\) poses a challenge, primarily stemming from the lack of an explicit expression for the distribution of the infinite sum of chi-square variables. One avenue to address this challenge involves employing the permutation method  to simulate the distribution under \(_{0}\). However, this method necessitates a significant number of shuffles to accurately approximate the distribution. Furthermore, even with the implementation of parallel schemes, it incurs memory costs proportional to the number of permutations, rendering it impractical for resource-constrained scenarios. Here, we adopt a lightweight approach in practice, leveraging the gamma approximation as proposed by . A gamma distribution is uniquely determined by its first and second-order moments. For these two moments, we present their corresponding linear-time estimators in Theorem 1. As a result, we can obtain the \((1-)\)-quantile of the gamma distribution, denoted as \(}\), with estimated parameters \(:=_{0}^{2}/_{0},:=_{0}/_{0}\) in linear time. Formally, with the term \(_{0}\) and \(_{0}\) defined in Theorem 1, \(}\) is calculated by

\[_{0}:n_{}(Z)e^{-x/}}{ ^{}()},=_{0}^{2}}{_{ 0}},\ =_{0}}{_{0}},\ \ \ _{0}^{}e^{-x}}{()}dx=1-, \]

where \(()\) is the gamma function. By combining the way to estimate the gradients of \(}\), we enable it for gradient-based optimization with automatic differentiation framework. As a result, we obtain a linear-time differentiable estimator of the numerator part.

**Theorem 1** (Linear-Time Estimators).: _Under \(_{0}\), the estimators of mean and variance with bias of \((n^{-1})\) to \(_{Z}[n_{}(Z)]\) and \(_{Z}[n_{}(Z)]\), denote as \(_{0}\) and \(_{0}\), respectively, are given by_

\[_{0}:=^{T}_{Xc}^{2}][ ^{T}_{Yc}^{2}]}{(n-1)^{2}},_{0} :=^{T}(_ {Xc}^{T}_{Xc})^{ 2}][^{T}(_{Yc}^{T}_{Yc})^{ 2}]}{n^{4}}, \]

_where \(()^{ 2}\) is the entry-wise matrix power. Both \(_{0}\) and \(_{0}\) can be calculated in \((nD^{2})\) time._

For the remain term \(_{}\), we estimate it with \(_{}\) that \(_{}^{2}:=16_{i}(} _{j,q,r}h_{ijqr}^{()})^{2}-_{}^{2}(Z)\). To calculate \(_{j,q,r}h_{ijqr}^{()}\), the straightforward way is to compute each item \(h_{ijqr}^{()}\), which requires total \((n^{4})\) of computation. Here we provide a way to enable it to be calculated in linear time by obtaining a matrix expression. The main result is given by

\[_{j,q,r}h_{ijqr}^{()}=[n^{T} +n^{2}()_{i}+(^{T})_{i}+(^{T})_{i}-n_{i}-n_{i}-n _{i}-^{T}], \]

where the definition of variables \(\) to \(\) with the calculation cost are given in the Fig. 1 and the derivation of Eq. (16) is given in the Appendix. By checking the complexity of the remaining matrix operations in Eq. (16), all the elements with index \(i\) can be calculated in \((nD^{2})\). Combining the results obtained before that \(_{}(Z)\) can also be calculated in \((nD^{2})\), thus calculating the term \(_{}\) cost \((nD^{2})\) time. As a result, we obtain the overall linear-time optimization objective \(J:=[_{}(Z)-}/n]/_{}\), which is a clear contrast to the existing quadratic-time schemes .

### The Overall Learning Framework

After obtaining the differentiable optimization objective \(J\), we can perform the training process end-to-end. In this process, the overfitting issues may happen especially with insufficient samples, which could influence both Type I and II errors. If we use the same sample for testing, the Type I errors may be uncontrollable  when the overfitting issues happen. To address this, we adopt the split scheme as in  to allow our tests to maintain validity (controllable Type I errors). The split ratio is set to \(0.5\) to facilitate the balance between the two. Apart from controlling Type I errors, we still want to mitigate overfitting issues as much as possible in order to generalize the optimized

Figure 1: The diagram shows the definition of the quantities in Eq. (16), with styles representing the time complexity of the computational process in the current box. \(\): the element-wise product.

Fourier feature pairs on test data thus improving the power of our tests. To this end, we select smooth function classes to control the model complexity (e.g., as measured by the VC dimension), specifically in this paper, we consider the two classes in Tab. 1 and implement them for experiments later. One is the choice of Gaussian classes that optimize the global scale, and the other we consider Mahalanobis classes and set \(\) to be \((_{1},...,_{d})\) for optimization, which corresponds to optimizing the scale in each dimension (which allows to capture high-frequency signals such as the edge in the image). These smooth choices also bring the advantage of interpretability  and it is experimentally proven that this simple choice is already able to handle most of the cases in different settings.

**More discussion about split strategy.** Currently, there are two major classes of approaches for adaptive independence tests. One involves selecting kernels from a finite/countable set (discrete scenario) and the other involves performing kernel parameter searches in a continuous space (continuous scenario). For the former case, some methods [22; 32] control Type I errors by applying techniques from the selective inference literature without data splitting. However, these methods cannot be directly applied to a continuous scenario due to the uncountable set of kernels involved. To the best of our knowledge, both our scheme and existing methods [24; 18] rely on data splitting for the continuous case. Designing methods to control Type I errors in the continuous case without sample splitting remains a challenging and significant problem for future research.

**Algorithm.** Our algorithm is outlined in Alg. 1. As a pre-processing step, we split the data into the training data \(Z^{tr}\) and the testing data \(Z^{te}\) (Line 1). The test contains two phases: 1) We learn the Fourier feature pairs with Adam  optimizer using full batches on \(Z^{tr}\) (Lines \(2\)-\(7\)). 2) With the learned Fourier feature pairs, we calculate the test statistic and threshold (Lines \(8\)-\(10\)) to determine the independence (Lines \(11\)) on \(Z^{te}\). The overall time complexity is \(TnD(d_{x}+d_{y}+D)\) and the space cost is \(n(d_{x}+d_{y}+D)\) for storing the data as well as the Fourier feature pairs.

```
0: samples \(Z\) of \(X,Y\), significance level \(\), the number of Fourier feature \(D\).
0:\(X]{90.0}{$$}} (0.0,0.0){$$}}{[0.0pt]{$$}}}Y\) or \(X]{90.0}{$$}} (0.0,0.0){$$}}{[0.0pt]{$$}}}Y\).
1: Split the data as \(Z=Z^{tr} Z^{te}\). Sampling \(\{_{j}\}_{j=1}^{D/2}=\{(_{k:j},_{l:j})\}_{j=1}^{D/2}\) with \(p()\).
2:\(\) Learning significant Fourier feature pairs on \(Z^{tr}\).
3: Initialize parameters \(_{k},_{l}\), set learning rate \(\), and set iteration steps \(T\).
4:for\(t=1,2,...,T\)do
5: Obtain learnable Fourier feature pairs \(_{X},_{Y}\) with parameters \(_{k},_{l}\) and \(\{_{j}\}_{j=1}^{D/2}\).
6: Calculate criterion \(J\) with \(_{X},_{Y}\) then optimize \(J\) with \((_{k},_{l})(_{k},_{l})+_{( _{k},_{l})}J\).
7:endfor
8: After training, obtain optimized parameters \(_{k}^{*},_{l}^{*}\).
9:\(\) Testing with learned Fourier feature pairs on \(Z^{te}\).
10: Calculate the statistic \(n^{te}_{}(Z^{te})\), threshold \(}(Z^{te})\) with parameters \(_{k}^{*},_{l}^{*}\) and \(\{_{j}\}_{j=1}^{D/2}\).
11: Return \(X]{90.0}{$$}} (0.0,0.0){$$}}{[0.0pt]{$$}}}Y\) if \(}(Z^{te}) n^{te}_{}(Z^{te})\) holds, otherwise \(X]{90.0}{$$}} (0.0,0.0){$$}}{[0.0pt]{$$}}}Y\).
```

**Algorithm 1** The learning and testing framework

## 5 Theoretical Results

We first give the uniform bound results over a ball in parameter space which guarantees the convergence of our optimizing objective thus ensuring effectiveness in modeling test power.

**Theorem 2** (Uniform Bound).: _Let \(_{k},_{l}\) parameterize \(_{_{k}},_{_{l}}\) in Banach spaces of dimension \(d_{k},d_{l}\). And \(_{_{k}},_{_{l}}\) are Lipschitz to the parameters \(_{k},_{l}\) with the non-negative constant \(L_{k},L_{l}\), respectively. Let \(_{c}\) be a set of \((_{k},_{l})\) for which \(_{} c>0\) with a positive constant \(c\) and \(\|_{k}\| R_{_{k}},\|_{l}\| R_{_{l}}\). Let \(r\) denote the threshold, i.e., \((1-)\)-quantile for the distribution in Eq. (11) and \(r^{(n)}\) be the threshold with sample size \(n\). Let \(\{(_{k:j},_{l:j})\}_{j=1}^{D/2}\) be the samplings of frequency with the sampling number \(D\). Also, we define \(R_{_{k}}:=_{j}\|_{k:j}\|,R_{_{l}:=_{j}\|_{l:j} \|,d_{s}:=\{d_{k},d_{l}\}\) and \(_{}:=_{}(Z)\). Then with probability at least \(1-\), we have_

\[_{(_{k},_{l})_{c}}-r_{ }^{(n)}/n}{_{}}-_{Z}_{}-r_{}/n}{ _{}} +d_{s}}+}L_{k}+R_{_{l }}L_{l}}{}.\]Next, we show the consistency of the tests, i.e. the power of the test tends to \(1\) as the sample size increases. Let the U-statistic of HSIC\({}_{}(Z^{te})\) be HSIC\({}_{}^{(u)}(Z^{te})\), then we have the following results.

**Theorem 3** (Consistency).: _Let \(_{k}^{*},_{l}^{*}\) be the parameters after learning, \(Z^{te}\) be the test samples of size \(m\), when \(_{Z}_{}^{(u)}(Z^{te})>0\), then the probability of the Type II error_

\[=_{_{1}}m_{}(Z^{te}) r_{}^{(m)}|_{k}^{*}, _{l}^{*}\!(m^{-1/2}). \]

_Let the mapping functions with learned parameters \(_{k}^{*},_{l}^{*}\) be \(_{_{k}^{*}},_{_{l}^{*}}\), and the corresponding range space be compact subsets of \(^{d_{_{}}},^{d_{_{}}}\), respectively. Also, the diameters of two range spaces are denoted by \((_{_{k}^{*}}),(_{_{l }^{*}})\), respectively. Let \(\{(_{k:j},_{l:j})\}_{j=1}^{D/2}\) be the frequency samplings with their second moment denoted by \(_{_{k}}^{2}:=_{p_{k}()}[_{k:j}^{T} _{i:j}]\), \(_{_{l:j}}^{2}:=_{p_{l}()}[_{l:j}^{T} _{l:j}]\). Additionally, we denote \(_{u}:=(X,Y)\), then under \(_{1}\), we have \(_{Z}_{}^{(u)}(Z^{te})>0\) with any constant probability when \(D=_{}}+d_{_{}}}{_{u}^{2 }}}(_{_{l}^{*}})+ _{_{l}}(_{_{l}^{*}})}{_{u}}\)._

This result can be understood in two parts. The first one is about consistency, i.e., the Type II error rate tends to \(0\) at the rate of \(m^{-1/2}\) when condition \(_{Z}_{}^{(u)}(Z^{te})>0\) holds. The second part provides the condition when \(_{Z}_{}^{(u)}(Z^{te})>0\) holds, which requires sufficiently many frequency samplings. The theorem shows that the large value of the criterion HSIC\((X,Y)\) helps to reduce the required \(D\). According to the results discussed in Sec. 3, there is an improvement in the criterion by finding the more significant features and thus helps to reduce the required \(D\). To summarize, the significant features further help to guarantee the consistency of the test under the efficient requirements (smaller \(D\)). All proofs as well as additional results are given in the Appendix.

## 6 Performance Evaluation

We compare the following tests: distance-based statistic **dCor**, the original HSIC **QHSIC**, the copula-based method **RDC**, the three variants of HSIC **NyHSIC**, **FHSIC**, **BH-SIC** and **HSICAgg**, **NFSIC** as introduced in Sec. 1. Among them, dCor and QHSIC are \((n^{2})\) tests. RDC is calculated in \(O(n n)\) time and the rest are \((n)\) tests. A detailed description of the comparing methods is given in the Appendix. For our methods, We provide two variants as mentioned in Sec. 4.3. We name the Gaussian class case as **LFHSIC-G**, and name the Mahalanobis class case (and set \(\) as a diagonal matrix) **LFHSIC-M**. Additionally, for the comparative methods  that are relevant to us, due to their high time overhead and therefore inability to handle some settings of evaluation, we separately provide a comparison with our method under certain feasible experimental settings, the results are given in the Appendix.

**Experimental setup.** The significance level \(\) is set to \(0.05\). We use Gaussian kernels for both \(X\) and \(Y\) in all kernel-based methods. And QHSIC, RDC, NyHSIC, FHSIC, BHSIC are all with the kernel width being set to the Euclidean distance median of the samples. The number of random features \(D\) for FHSIC, LFHSIC-G/M, the number of induced variables for NyHSIC, the block size for BHSIC as well as the number of sub-diagonals \(R\) for HSICAgg are all kept consistent as recommended in [44; 32] for fair evaluation. Parameter settings for the rest of the methods follow the defaults in the code. More details of the setups are given in the Appendix.

**Evaluation protocol.** We evaluate on four synthetic datasets [18; 30] and two real datasets [44; 30]. Synthetic datasets consist of Sine Dependency (SD), Sinusoid (Sin), Gaussian Sign (GSign), and independent subspace analysis (ISA) dataset . On real data, we introduce high-dimensional image data and another music dataset to evaluate the capability of all methods in different data scenarios. Unless otherwise specified, we perform \(100\) repeated randomized experiments and report the average result of test power as default. More details of the generating process of each dataset and the details of the evaluation (including running time) are provided in the Appendix.

### Results on Synthetic Datasets

**Settings of SD, Sin, and GSign Dataset**. The Sin data corresponds to the example in Sec. 3 that requires the method to focus on differences in specific frequencies. In SD, \(Y\) is dependent solely on the first two dimensions of \(X\). In contrast, in GSign, \(Y\) is independent of any proper subset of but dependent on \(X\) as a whole. Therefore, it requires the method to learn important local/global features based on the characteristics of the data to improve the test power. For SD and GSign, we set the dimension of \(X\) as \(4\) and \(5\), respectively, and the dimension of \(Y\) is \(1\) for both. For Sin, we set the frequency parameter \(=5\). For calculating the Type I error rate, we evaluate using samples (\(n=2000\)) obtained by permutation for all three datasets.

**Performance.** The results for \(D=100\) and \(D=500\) are shown in Fig. 2. Except for NFSIC and BHSIC, all the other methods succeed in controlling the Type I error rate \( 0.05\). LFHSIC-M/G, NFHSIC, and HSICAgg perform much better than other methods due to their ability to obtain more appropriate kernels/features for testing. LFHSIC-G/M performs on both settings of \(D\) and has a more significant advantage over the others when \(D\) is small, implying the optimization objective can still be successfully optimized and the criterion is still powerful under high-speed requirements. In addition, as the sample size increases the test power of LFHSIC-G/M is gradually converging to \(1\) in both settings, which corroborates the results of Theorem 3.

**Settings of ISA Dataset (Large Scale)**. We set dimension (of both \(X,Y\)) and sample size as \(d=16,n=10000\) and \(d=32,n=60000\), then evaluate the average test power with angle parameter \([0,/4]\). Note that a larger angle signifies stronger dependency. The quadratic-time methods are not involved in the evaluation due to their inability to handle large-scale settings. For HSICAgg under the challenging setting \(n=60000,d=32\), the memory space required for parallel implementation leads to memory overflow and hence the results are not given.

**Performance.** The results for \(D=300\) and \(D=500\) are shown in Fig. 3. The results obtained at \(=0\) reflect the Type I error rate. All methods successfully control the Type I error rate \( 0.05\). LFHSIC-M stably outperforms other methods significantly as the angle increases. Method (LFHSIC-G) that simply optimizes the global bandwidth performs worse as \(d\) increases, corroborating the need for more flexible kernel designs for more challenging tasks. Furthermore, comparing the

Figure 3: The average test power v.s. the rotation angle of each method on the ISA dataset.

Figure 2: Top: (\(D=100\)). Below: (\(D=500\)). Left: The average Type I error rate on SD, Sin, and GSign datasets. The other three plots: The results of average test power on these three datasets.

results under different settings of \(D\), our method LFHSIC-M performs consistently well and exhibits progressively better performance as \(D\) increases.

### Results on Real Data

**Settings of Two Real Data**. The first real dataset used is a high-dimensional image dataset 3Dshapes as in . In our experiments, we vectorize image \(X\) to a vector with dimension \(64 64 3=12,288\). The sample size is set as \(128\). We add standard Gaussian noise \((0,1)\) to the angle \(Y\) to make the setting more challenging. The Type I error rate is evaluated by the samples obtained by permutation. Besides, we consider the Million Song Data (MSD) as the second real dataset. The first dimension represents the year of release of each song and is referred to as variable \(Y\). The remaining 90-dimensional features (e.g., mean timbre and timbre covariance) constitute variable \(X\). We follow the recommended setting , i.e., disturbing each entry of the \(X\) with an independent Gaussian noise \((0,1000)\). For this dataset MSD, in order to fully utilize the data, we randomly select \(n\{500,1000\}\) samples as the training set and other \(n\) samples from the remaining data 100 times for the evaluation and obtain the average result. The above training and testing processes are repeated \(10\) times to evaluate the robustness of the optimization scheme.

**Performance.** The results of two real data with \(D=10\) are presented in Fig. 4. For the results on 3Dshapes (shown in the left of Fig. 4), all methods except BHSIC and NFSIC control the Type I error well. The linear-time test has relatively lower power compared to the quadratic-time test except for LFHSIC-M, proving that its more significant features obtained in high-dimensional scenarios enable it to achieve outstanding performance even in scenarios with high approximation requirements (\(D=10\)). Similar conclusions can drawn from the MSD dataset (shown in the right of Fig. 4). Additionally, the results for NFSIC and LFHSIC-G/M with different sample sizes indicate increased robustness of the optimization as the sample size increases (reflected in the reduction of variance), and the more flexible design also contributes to this (comparing LFHSIC-M and LFHSIC-G), thus can be more effectively applied to real-world scenarios.

## 7 Conclusion

In this paper, we propose a novel method to efficiently learn significant Fourier feature pairs for maximizing the power of HSIC-based independence tests. By integrating a learnable Fourier feature module, we improve the flexibility of existing configurations and design a new criterion. The proposed linear-time optimization objective accurately models the power of the test and can be trained end-to-end in a data-driven manner, ensuring both effectiveness and efficiency. Both theoretical results and experimental results show the effectiveness of our proposed method. Future work includes further improving the sampling method in the frequency domain.