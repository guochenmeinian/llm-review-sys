# Global Optimality in Bivariate Gradient-based

DAG Learning

 Chang Deng\({}^{}\)1 Kevin Bello\({}^{,}\)   Pradeep Ravikumar\({}^{}\)   Bryon Aragam\({}^{}\)

\({}^{}\)Booth School of Business, University of Chicago, Chicago, IL 60637

\({}^{}\)Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213 changdeng@chicagobooth.edu

###### Abstract

Recently, a new class of non-convex optimization problems motivated by the statistical problem of learning an acyclic directed graphical model from data has attracted significant interest. While existing work uses standard first-order optimization schemes to solve this problem, proving the global optimality of such approaches has proven elusive. The difficulty lies in the fact that unlike other non-convex problems in the literature, this problem is not "benign", and possesses multiple spurious solutions that standard approaches can easily get trapped in. In this paper, we prove that a simple path-following optimization scheme globally converges to the global minimum of the population loss in the bivariate setting.

## 1 Introduction

Over the past decade, non-convex optimization has become a major topic of research within the machine learning community, in part due to the successes of training large-scale models with simple first-order methods such as gradient descent--along with their stochastic and accelerated variants--in spite of the non-convexity of the loss function. A large part of this research has focused on characterizing which problems have _benign_ loss landscapes that are amenable to the use of gradient-based methods, i.e., there are no spurious local minima, or they can be easily avoided. By now, several theoretical results have shown this property for different non-convex problems such as: learning a two hidden unit ReLU network , learning (deep) over-parameterized quadratic neural networks , low-rank matrix recovery , learning a two-layer ReLU network with a single non-overlapping convolutional filter , semidefinite matrix completion , learning neural networks for binary classification with the addition of a single special neuron , and learning deep networks with independent ReLU activations , to name a few.

Recently, a new class of non-convex optimization problems due to Zheng et al.  have emerged in the context of learning the underlying structure of a structural equation model (SEM) or Bayesian network. This underlying structure is typically represented by a directed acyclic graph (DAG), which makes the learning task highly complex due to its combinatorial nature. In general, learning DAGs is well-known to be NP-complete . The key innovation in Zheng et al.  was the introduction of a differentiable function \(h\), whose level set at zero _exactly_ characterizes DAGs. Thus, replacing the challenges of combinatorial optimization by those of non-convex optimization. Mathematically, this class of non-convex problems take the following general form:

\[_{}f()\ \ \ \ h(W())=0,\] (1)

where \(^{l}\) represents the model parameters, \(f:^{l}\) is a (possibly non-convex) smooth loss function (sometimes called a _score function_) that measures the fitness of \(\), and \(h:^{d d}[0,)\)is a smooth **non-convex** function that takes the value of zero if and only if the induced weighted adjacency matrix of \(d\) nodes, \(W()\), corresponds to a DAG.

Given the smoothness of \(f\) and \(h\), problem (1) can be solved using off-the-shelf nonlinear solvers, which has driven a series of remarkable developments in structure learning for DAGs. Multiple empirical studies have demonstrated that global or near-global minimizers for (1) can often be found in a variety of settings, such as linear models with Gaussian and non-Gaussian noises (e.g., 51, 34, 1), and non-linear models, represented by neural networks, with additive Gaussian noises (e.g., 29, 52, 49, 1). The empirical success for learning DAGs via (1), which started with the Notears method of Zheng et al. , bears a resemblance to the advancements in deep learning, where breakthroughs like AlexNet significantly boosted the field's recognition, even though there were notable successes before it.

Importantly, the reader should note that the majority of applications in ML consist of solving a _single unconstrained_ non-convex problem. In contrast, the class of problems (1) contains a non-convex constraint. Thus, researchers have considered some type of penalty method such as the augmented Lagrangian , quadratic penalty , and a log-barrier . In all cases, the penalty approach consists in solving a _sequence_ of unconstrained non-convex problems, where the constraint is enforced progressively [see e.g. 2, for background]. In this work, we will consider the following form of penalty:

\[_{}g_{_{k}}()_{k}f()+h(W()).\] (2)

It was shown by Bello et al.  that due to the invexity property of \(h\),2 solutions to (2) will converge to a DAG as \(_{k} 0\). However, no guarantees on local/global optimality were given.

With the above considerations in hand, one is inevitably led to ask the following questions:

* _Are the loss landscapes_ \(g_{_{k}}()\) _benign for different_ \(_{k}\)_?_
* _Is there a (tractable) solution path {_\(_{k}\)_} that converges to a global minimum of (1)?_

Due to the NP-completeness of learning DAGs, one would expect the answer to (i) to be negative in its most general form. Moreover, it is known from the classical theory of constrained optimization (e.g. 2) that if we can _exactly_ and _globally_ optimize (1) for each \(_{k}\), then the answer to (ii) is affirmative. This is not a practical algorithm, however, since the problem (1) is nonconvex. Thus we seek a solution path that can be tractably computed in practice, e.g. by gradient descent.

In this work, we focus on perhaps the simplest setting where interesting phenomena take place. That is, a linear SEM with two nodes (i.e., \(d=2\)), \(f\) is the population least squared loss (i.e., \(f\) is convex), and \(_{k}\) is defined via gradient flow with warm starts. More specifically, we consider the case where \(_{k}\) is obtained by following the gradient flow of \(g_{_{k}}\) with initial condition \(_{k-1}\).

Under this setting, to answer (i), it is easy to see that for a large enough \(_{k}\), the convex function \(f\) dominates and we can expect a benign landscape, i.e., a (almost) convex landscape. Similarly, when \(_{k}\) approaches zero, the invexity of \(h\) kicks in and we could expect that all stationary points are (near) global minimizers.3 That is, at the extremes \(_{k}\) and \(_{k} 0\), the landscape seem well-behaved, and the reader might wonder if it follows that for any \(_{k}[0,)\) the landscape is well-behaved. We answer the latter in the _negative_ and show that there always exists a \(>0\) where the landscape of \(g_{_{k}}\) is non-benign for any \(_{k}<\), namely, there exist three stationary points: i) A saddle point, ii) A spurious local minimum, and iii) The global minimum. In addition, each of these stationary points have wide basins of attractions, thus making the initialization of the gradient flow for \(g_{_{k}}\) crucial. Finally, we answer (ii) in the affirmative and provide an explicit scheduling for \(_{k}\) that guarantees the asymptotic convergence of \(_{k}\) to the global minimum of (1). Moreover, we show that this scheduling cannot be arbitrary as there exists a sequence of \(\{_{k}\}\) that leads \(\{_{k}\}\) to a spurious local minimum.

Overall, we establish the first set of results that study the optimization landscape and global optimality for the class of problems (1). We believe that this comprehensive analysis in the bivariate case provides a valuable starting point for future research in more complex settings.

**Remark 1**.: _We emphasize that solving (1) in the bivariate case is not an inherently difficult problem. Indeed, when there are only two nodes, there are only two DAGs to distinguish and one can simply fit \(f\) under the only two possible DAGs, and select the model with the lowest value for \(f\). However, evaluating \(f\) for each possible DAG structure clearly cannot scale beyond 10 or 20 nodes, and is not a standard algorithm for solving (1). Instead, here our focus is on studying how (1) is actually being solved in practice, namely, by solving unconstrained non-convex problems in the form of (2). Previous work suggests that such gradient-based approaches indeed scale well to hundreds and even thousands of nodes [e.g. 51, 34, 1]._

### Our Contributions

More specifically, we make the following contributions:

1. We present a homotopy-based optimization scheme (Algorithm 2) to find global minimizers of the program (1) by iteratively decreasing the penalty coefficient according to a given schedule. Gradient flow is used to find the stationary points of (2) at each step, starting from the previous solution.
2. We prove that Algorithm 2 converges _globally_ (i.e. regardless of initialization for \(W\)) to the _global_ minimum (Theorem 1).
3. We show that the non-convex program (1) is indeed non-benign, and naive implementation of black-box solvers are likely to get trapped in a bad local minimum. See Figure 1 (a).
4. Experimental results verify our theory, consistently recovering the global minimum of (1), regardless of initialization or initial penalty value. We show that our algorithm converges to the global minimum while naive approaches can get stuck.

The analysis consists of three main parts: First, we explicitly characterize the trajectory of the stationary points of (2). Second, we classify the number and type of all stationary points (Lemma 1) and use this to isolate the desired global minimum. Finally, we apply Lyapunov analysis to identify the basin of attraction for each stationary point, which suggests a schedule for the penalty coefficient that ensures that the gradient flow is initialized within that basin at the previous solution.

### Related Work

The class of problems (1) falls under the umbrella of score-based methods, where given a score function \(f\), the goal is to identify the DAG structure with the lowest score possible . We shall note that learning DAGs is a very popular structure model in a wide range of domains such as biology , genetics , and causal inference , to name a few.

Figure 1: Visualizing the nonconvex landscape. (a) A contour plot of \(g_{}\) for \(a=0.5\) and \(=0.005\) (see Section 2 for definitions). We only show a section of the landscape for better visualization. The solid lines represent the contours, while the dashed lines represent the vector field \(- g_{}\). (b) Stationary points of \(g_{}\), \(r(y;)=0\) and \(r(x;)=0\) (see Section 4 for definitions).

Score-based methods that consider the combinatorial constraint.Given the ample set of score-based methods in the literature, we briefly mention some classical works that attempt to optimize \(f\) by considering the combinatorial DAG constraint. In particular, we have approximate algorithms such as the greedy search method of Chickering et al. , order search methods [45; 41; 38], the LP-relaxation method of Jaakkola et al. , and the dynamic programming approach of Loh and Buhlmann . There are also exact methods such as GOBNILP  and Bene , however, these algorithms only scale up to \( 30\) nodes.

Score-based methods that consider the continuous non-convex constraint \(h\).The following works are the closest to ours since they attempt to solve a problem in the form of (1). Most of these developments either consider optimizing different score functions \(f\) such as ordinary least squares [51; 52], the log-likelihood [29; 34], the evidence lower bound , a regret function ; or consider different differentiable characterizations of acyclicity \(h\)[49; 1]. However, none of the aforementioned works provide any type of optimality guarantee. Few studies have examined the optimization intricacies of problem (1). Wei et al.  investigated the optimality issues and provided _local_ optimality guarantees under the assumption of convexity in the score \(f\) and linear models. On the other hand, Ng et al.  analyzed the convergence to (local) DAGs of generic methods for solving nonlinear constrained problems, such as the augmented Lagrangian and quadratic penalty methods. In contrast to both, our work is the first to study global optimality and the loss landscapes of actual methods used in practice for solving (1).

Bivariate causal discovery.Even though in a two-node model the discrete DAG constraint does not pose a major challenge, the bivariate setting has been subject to major research in the area of causal discovery. See for instance [36; 16; 32; 25] and references therein.

Penalty and homotopy methods.There exist classical global optimality guarantees for the penalty method if \(f\) and \(h\) were convex functions, see for instance [2; 5; 37]. However, to our knowledge, there are no global optimality guarantees for general classes of non-convex constrained problems, let alone for the specific type of non-convex functions \(h\) considered in this work. On the other hand, homotopy methods (also referred to as continuation or embedding methods) are in many cases capable of finding better solutions than standard first-order methods for non-convex problems, albeit they typically do not come with global optimality guarantees either. When homotopy methods come with global optimality guarantees, they are commonly computationally more intensive as it involves discarding solutions, thus, closely resembling simulated annealing methods, see for instance . Authors in  characterize a family of non-convex functions where a homotopy algorithm provably converges to a global optimum. However, the conditions for such family of non-convex functions are difficult to verify and are very restrictive; moreover, their homotopy algorithm involves Gaussian smoothing, making it also computationally more intensive than the procedure we study here. Other examples of homotopy methods in machine learning include [7; 18; 46; 17; 23], in all these cases, no global optimality guarantees are given.

## 2 Preliminaries

The objective \(f\) we consider can be easily written down as follows:

\[f(W)=_{X}[\|X-W^{}X\|_{2}^{2}],\] (3)

where \(X^{2}\) is a random vector and \(W^{2 2}\). Although not strictly necessary for the developments that follow, we begin by introducing the necessary background on linear SEM that leads to this objective and the resulting optimization problem of interest.

The bivariate model.Let \(X=(X_{1},X_{2})^{2}\) denote the random variables in the model, and let \(N=(N_{1},N_{2})^{2}\) denote a vector of independent errors. Then a linear SEM over \(X\) is defined as \(X=W_{*}^{}X+N\), where \(W_{*}^{2 2}\) is a weighted adjacency matrix encoding the coefficients in the linear model. In order to represent a valid Bayesian network for \(X\) [see e.g. 39; 44, for details], the matrix \(W_{*}\) must be acyclic: More formally, the weighted graph induced by the adjacency matrix \(W_{*}\) must be a DAG. This (non-convex) acyclicity constraint represents the major computational hurdle that must overcome in practice (cf. Remark 1).

The goal is to recover the matrix \(W_{*}\) from the random vector \(X\). Since \(W_{*}\) is acyclic, we can assume the diagonal of \(W_{*}\) is zero (i.e. no self-loops). Thus, under the bivariate linear model, it then suffices to consider two parameters \(x\) and \(y\) that define the matrix of parameters4

\[W=W(x,y)=0&x\\ y&0\] (4)

For notational simplicity, we will use \(f(W)\) and \(f(x,y)\) interchangeably, similarly for \(h(W)\) and \(h(x,y)\). Without loss of generality, we write the underlying parameter as

\[W_{*}=0&a\\ 0&0\] (5)

which implies

\[X=W_{*}^{}X+NX_{1}=N_{1},\\ X_{2}=aX_{1}+N_{2}.\]

In general, we only require \(N_{i}\) to have finite mean and variance, hence we _do not_ assume Gaussianity. We assume that \([N_{1}]=[N_{2}]\), and for simplicity, we consider \([N]=0\) and \([N]=I\), where \(I\) denotes the identity matrix. Finally, in the sequel we assume w.l.o.g. that \(a>0\).

The population least squares.In this work, we consider the population squared loss defined by (3). If we equivalently write \(f\) in terms of \(x\) and \(y\), then we have: \(f(W)=((1-ay)^{2}+y^{2}+(a-x)^{2}+1)/2\). In fact, the population loss can be substituted with empirical loss. In such a case, our algorithm can still attain the global minimum, \(W_{}\), of problem (6). However, the output \(W_{}\) will serve as an empirical estimation of \(W_{*}\). An in-depth discussion on this topic can be found in Appendix B

The non-convex function \(h\).We use the continuous acyclicity characterization of Yu et al. , i.e., \(h(W)=((I+W W)^{d})-d\), where \(\) denotes the Hadamard product. Then, for the bivariate case, we have \(h(W)=x^{2}y^{2}/2\). We note that the analysis presented in this work is not tailored to this version of \(h\), that is, we can use the same techniques used throughout this work for other existing formulations of \(h\), such as the trace of the matrix exponential , and the log-det formulation . Nonetheless, here we consider that the polynomial formulation of Yu et al.  is more amenable for the analysis.

**Remark 2**.: _Our restriction to the bivariate case highlights the simplest setting in which this problem exhibits nontrivial behaviour. Extending our analysis to higher dimensions remains a challenging future direction, however, we emphasize that even in two-dimensions this problem is nontrivial. Our approach is similar to that taken in other parts of the literature that started with simple cases (e.g. single-neuron models in deep learning)._

**Remark 3**.: _It is worth noting that our choice of the population least squares is not arbitrary. Indeed, for linear models with identity error covariance, such as the model considered in this work, it is known that the global minimizer of the population squared loss is unique and corresponds to the underlying matrix \(W_{*}\). See Theorem 7 in ._

Gluing all the pieces together, we arrive to the following version of (1) for the bivariate case:

\[_{x,y}f(x,y)((1-ay)^{2}+y^{2}+(a-x)^{2}+1)\ \ h(x,y)y^{2}}{2}=0.\] (6)

Moreover, for any \( 0\), we have the corresponding version of (2) expressed as:

\[_{x,y}g_{}(x,y) f(x,y)+h(x,y)=((1-ay)^{2}+y^{ 2}+(a-x)^{2}+1)+y^{2}}{2}.\] (7)

To conclude this section, we present a visualization of the landscape of \(g_{}(x,y)\) in Figure 1 (a), for \(a=0.5\) and \(=0.005\). We can clearly observe the non-benign landscape of \(g_{}\), i.e., there exists a spurious local minimum, a saddle point, and the global minimum. In particular, we can see that the basin of attraction of the spurious local minimum is comparable to that of the global minimum, which is problematic for a local algorithm such as the gradient flow (or gradient descent) as it can easily get trapped in a local minimum if initialized in the wrong basin.

```
1: set \(z(0)=z_{0}\)
2:\(z(t)=- f(z(t))\)
3:return\(_{t}z(t)\) ```

**Algorithm 1**GradientFlow(\(f,z_{0}\))

``` Input: Initial \(W_{0}=W(x_{0},y_{0})\), \(_{0}[}{4(a^{2}+1)^{3}},}{4})\) Output: \(\{W_{_{k}}\}_{k=0}^{}\)
1\(W_{_{0}}(g_{_{0}},W_{0})\)
2for\(k=1,2,\)do
3 Let \(_{k}=(2/a)^{2/3}\)\(_{k-1}^{4/3}\)
4\(W_{_{k}}(g_{_{k}},W_{_{k-1}})\)
5 end for ```

**Algorithm 2**Homotopy algorithm for solving (1).

## 3 A Homotopy-Based Approach and Its Convergence to the Global Optimum

To fix notation, let us write \(W_{k}:=W_{_{k}}:=(0&x_{_{k}}\\ y_{_{k}}&0)\). and let \(W_{}\) denote the global minimizer of (6). In this section, we present our main result, which provides conditions under which solving a series of unconstrained problems (7) with first-order methods will converge to the global optimum \(W_{}\) of (6), in spite of facing non-benign landscapes. Recall that from Remark 3, we have that \(W_{}=(0&a\\ 0&0)\). Since we use gradient flow path to connect \(W_{_{k}}\) and \(W_{_{k+1}}\), we specify this path in Procedure 1 for clarity. Although the theory here assumes continuous-time gradient flow with \(t\), see Section 5 for an iteration complexity analysis for (discrete-time) gradient descent, which is a straightforward consequence of the continuous-time theory.

In Algorithm 2, we provide an explicit regime of initialization for the homotopy parameter \(_{0}\) and a specific scheduling for \(_{k}\) such that the solution path found by Algorithm 2 will converge to the global optimum of (6). This is formally stated in Theorem 1, whose proof is given in Section 5.

**Theorem 1**.: _For any initialization \(W_{0}\) and \(a\), the solution path provided in Algorithm 2 converges to the global optimum of (6), i.e.,_

\[_{k}W_{_{k}}=W_{}.\]

A few observations regarding Algorithm 2: Observe that when the underlying model parameter \(a 0\), the regime of initialization for \(_{0}\) is wider; on the other hand, if \(a\) is closer to zero then the interval for \(_{0}\) is much narrower. As a concrete example, if \(a=2\) then it suffices to have \(_{0}[0.008,1)\); whereas if \(a=0.1\) then the regime is about \(_{0}[0.0089,0.01)\). This matches the intuition that for a "stronger" value of \(a\) it should be easier to detect the right direction of the underlying model. Second, although in Line 3 we set \(_{k}\) in a specific manner, it actually suffices to have

\[_{k}[(}{2})^{}{{3}}}(a^{ }{{3}}}-}{{3}}}-(4_{k-1})^{}{{3}}}})^{ 2},_{k-1})\!.\]

We simply chose a particular expression from this interval for clarity of presentation; see the proof in Section 5 for details.

As presented, Algorithm 2 is of theoretical nature in the sense that the initialization for \(_{0}\) and the decay rate for \(_{k}\) in Line 3 depend on the underlying parameter \(a\), which in practice is unknown. In Algorithm 3, we present a modification that is independent of \(a\) and \(W_{*}\). By assuming instead a lower bound on \(a\), which is a standard assumption in the literature, we can prove that Algorithm 3 also converges to the global minimum:

**Corollary 1**.: _Initialize \(_{0}=}\). If \(a>\) then for any initialization \(W_{0}\), Algorithm 3 outputs the global optimal solution to (6), i.e._

\[_{k}W_{_{k}}=W_{}.\]

For more details on this modification, see Appendix A.

## 4 A Detailed Analysis of the Evolution of the Stationary Points

The homotopy approach in Algorithm 2 relies heavily on how the stationary points of (7) behave with respect to \(_{k}\). In this section, we dive deep into the properties of these critical points.

By analyzing the first-order conditions for \(g_{}\), we first narrow our attention to the region \(A=\{0 x a,0 y+1}\}\). By solving the resulting equations, we obtain an equation that only involves the variable \(y\):

\[r(y;)=-}{(y^{2}+)^{2}}-(a^{2}+1).\] (8)

Likewise, we can find an equation only involving the variable \(x\):

\[t(x;)=-}{((a^{2}+1)+x^{2})^{2}}-1.\] (9)

To understand the behavior of the stationary points of \(g_{}(W)\), we can examine the characteristics of \(t(x;)\) in the range \(x[0,a]\) and the properties of \(r(y;)\) in the interval \(y[0,+1}]\).

In Figures 2 and 3, we show the behavior of \(r(y;)\) and \(t(x;)\) for \(a=1\). Theorems 5 and 6 in the appendix establish the existence of a \(>0\) with the following useful property:

**Corollary 2**.: _There exists \(<\) such that the equation \( g_{}(W)=0\) has three different solutions, denoted as \(W_{}^{*},W_{}^{**},W_{}^{***}\). Then,_

\[_{ 0}W_{}^{*}=0&a\\ 0&0,\;_{ 0}W_{}^{**}=0&0\\ 0&0,\;_{ 0}W_{}^{***}=0&0\\ +1}&0\]

Note that the interesting regime takes place when \(<\). Then, we characterize the stationary points as either local minima or saddle points:

Figure 2: The behavior of \(r(y;)\) for different \(\). Here, for \(>,\) there exists a single solution to \(r(y;)=0\), which implies there is one stationary point in Equation (7). When \(=,\) two solutions are found for \(r(y;)=0\), suggesting that there are two stationary points in Equation (7). Conversely, when \(<,\) we observe three solutions for \(r(y;)=0\), indicating that there are three stationary points in Equation (7)- a local optimum, a saddle point, and a global optimum.

[MISSING_PAGE_FAIL:8]

1 procedure will converge to \((x^{*}_{_{k+1}},y^{*}_{_{k+1}})\). Finally, from Theorems 5 and 6, if \(_{k}_{k}=0\), then \(_{k}x^{*}_{_{k}}=a,_{k}y^{*}_{_ {k}}=0\), thus, converging to the global optimum, i.e.,

\[_{k}W_{_{k}}=W_{}.\]

### Discrete case: Gradient Descent

In Algorithms 2 and 4, gradient flow is employed to locate the next stationary points, which is not practically feasible. A viable alternative is to execute Algorithm 2, replacing the gradient flow with gradient descent. Now, at every iteration \(k\), Algorithm 6 uses gradient descent to output \(W_{_{k},_{k}}\), a \(_{k}\) stationary point of \(g_{_{k}}\), initialized at \(W_{_{k-1},_{k-1}}\), and a step size of \(_{k}=1/(_{k}(a^{2}+1)+3a^{2})\). The tolerance parameter \(_{k}\) can significantly influence the behavior of the algorithm and must be controlled for different iterations. A convergence guarantee is established via a simplified theorem presented here. A more formal version of the theorem and a comprehensive description of the algorithm (i.e., Algorithm 6) can be found in Appendix C.

**Theorem 2** (Informal).: _For any \(_{}>0\), set \(_{0}\) satisfy a mild condition, and use updating rule \(_{k}=\{ a_{k},_{k}^{3/2}\}\), \(_{k+1}=(2_{k}^{2})^{2/3}/_{k})^{2/3}}{(a- _{k}/_{k})^{4/3}}\), and let \(K K(_{0},a,_{}) O(}{ a_{}})\). Then, for any initialization \(W_{0}\), following the updated procedure above for \(k=0,,K\), we have:_

\[\|W_{_{k},_{k}}-W_{}\|_{2}_{}\]

_that is, \(W_{_{k},_{k}}\) is \(_{}\)-close in Frobenius norm to global optimum \(W_{}\). Moreover, the total number of gradient descent steps is upper bounded by \(O((_{0}a^{2}+a^{2}+_{0})(}+ {_{}}^{6}))\)._

## 6 Experiments

We conducted experiments to verify that Algorithms 2 and 4 both converge to the global minimum of (7). Our purpose is to illustrate two main points: First, we compare our updating scheme as given in Line 3 of Algorithm 2 against a faster-decreasing updating scheme for \(_{k}\). In Figure 4 we illustrate how a naive faster decrease of \(\) can lead to spurious a local minimum. Second, in Figure 5, we show that regardless of the initialization, Algorithms 2 and 4 always return the global minimum. In the supplementary material, we provide additional experiments where the gradient flow is replaced with gradient descent. For more details, please refer to Appendix F.

## 7 Acknowledgments and Disclosure of Funding

K. B. was supported by NSF under Grant # 2127309 to the Computing Research Association for the CIFellows 2021 Project. B.A. was supported by NSF IIS-1956330, NIH R01GM140467, and the Robert H. Topel Faculty Research Fund at the University of Chicago Booth School of Business. This work was done in part while B.A. was visiting the Simons Institute for the Theory of Computing. P.R. was supported by ONR via N000141812861, and NSF via IIS-1909816, IIS-1955532, IIS-2211907. We are also grateful for the support of the University of Chicago Research Computing Center for assistance with the calculations carried out in this work.

Figure 4: Trajectory of the gradient flow path for two different update rules for \(_{k}\) with same initialization and \(_{0}\). Here, “good scheduling” uses Line 3 of Algorithm 2, while “bad scheduling” uses a faster decreasing scheme for \(_{k}\) which leads the path to a spurious local minimum.