# Give me a hint: Can LLMs take a hint to solve math problems?

Vansh Agrawal, Pratham Singla, Amitoj Singh Miglani, Shivank Garg, Ayush Mangal

Vision and Language Group

Indian Institute of Technology, Roorkee

{vansh_a@ph,pratham_s@me,amitoj_sm@ph,shivank_g@mfs,amangal@cs}.iitr.ac.in

###### Abstract

While state-of-the-art LLMs have shown poor logical and basic mathematical reasoning, recent works try to improve their problem-solving abilities using prompting techniques. We propose giving "hints" to improve the language model's performance on advanced mathematical problems, taking inspiration from how humans approach math pedagogically. We also test robustness to adversarial hints and demonstrate their sensitivity to them. We demonstrate the effectiveness of our approach by evaluating various diverse LLMs, presenting them with a broad set of problems of different difficulties and topics from the MATH dataset and comparing against techniques such as one-shot, few-shot, and chain of thought prompting. Our code is available at https://github.com/vlgiitr/LLM-Math

## 1 Introduction

The ability to reason and logically solve complex mathematical problems is essential for progress in nearly every field, whether it be modeling complex environments, developing new algorithms, or engineering new devices. Recent works have explored Large Language models' mathematical capabilities and found them lacking in logic and basic mathematical reasoning . Improving these reasoning capabilities is important in a future where LLMs can be used to assist humans in increasingly complex mathematical tasks or act like AI math teachers. In this work, taking inspiration from how humans are taught math pedagogically, we propose "hinting" LLMs by giving subtle guidance or clues as a method to improve problem-solving capabilities on mathematical tasks and then compare its effectiveness against other prompting methods, such as one-shot , few-shot , and chain of thought prompting . We evaluate our approach using the MATH dataset , consisting of math problems categorized into distinct classes based on subtopics such as algebra, probability, geometry, etc., with different levels of complexity (1-5). We use a diverse set of LLMs, including base language models, instruction fine-tuned models, models specifically tuned for mathematical tasks, and closed source models such as GPT-4o-mini  and Gemini Flash  for our evaluation.

We further examine the robustness of these models to adversarial prompts, misleading hints, and clues of varying levels. We investigate how sensitive the models are to incorporating these incorrect hints as context, which may degrade performance, versus their ability to reject such misleading information. Through these experiments, we seek to contribute to the ongoing research on improving the current state-of-the-art language model's reasoning capabilities and their practical applications in solving mathematical tasks .

### Background and Related Work

Various prompting methods have been shown to increase the accuracy of LLMs in solving complex problems that require understanding and reasoning . A few popular methods being -1. **One shot prompting :** Giving a single example problem and its final answer to the model in-context to learn from.
2. **Few shot prompting :** Providing multiple in-context example problems and their final answers instead of one.
3. **Chain of thought prompting (CoT) :** Providing a detailed step-by-step solution to the in-context example to provide intermediate reasoning steps to solve a problem.

However, these methods have not been extensively explored in the context of solving more complicated mathematical problems. Further, their generalization capabilities, to apply learned knowledge to a broader domain of questions (e.g., algebraic equations, geometry problems) rather than specific problems (e.g., solving a specific algebraic equation, finding roots for a quadratic polynomial), have not been sufficiently researched . Previous math benchmarks show that math is a valuable ability to test an LLM's reasoning and problem-solving capabilities. While there has been work on hinting  as a prompting technique, they have not been robust and diverse enough to see if these techniques are generalizable to various types of models and what the effect and sensitivity of these models to adversarial hinting can be.

## 2 Method

We first prompt the Gemini 1.5 flash model to generate hints for our dataset by passing it the question and final answer. These hints are then provided in context to the model and the target problem to help the model reason about the task. We believe this aligns more with how humans solve math problems by getting hints instead of the complete solution for an example problem, as in Chain of Thought, or just the final answer, as in one/few-shot approaches.

To test the adversarial robustness of these models to hints, we provided either an adversarial misleading hint or a hint from a random question to observe how sensitive the models are to our hints. Similar versions for one/few shot and CoT prompting are also generated as shown in Figure 1. More details about the prompting process can be found in Appendix A.

We evaluate a diverse ensemble of LLMs ranging from base models to instruct fine-tuned models and math-finetuned LLMs, including nine open-source models and two closed-source models (More details in Appendix C). We use the MATH dataset , which contains problems of seven different classes (e.g., algebra, geometry, etc.) of varying difficulty levels from 1 (easy) to 5 (hard), more details about the difficulty levels are given in Appendix B.

## 3 Experiments and Results

### Setup

We evaluate a set of 11 models for our experiments, using open-source implementations where possible and public API offerings for closed-source models. We evaluate 17 different types of prompting techniques, ranging from baseline to hinting to adversarial, along with one/few shot and CoT as given in Appendix A. The problem set consisted of all seven topics from the MATH dataset , with 100 problems for each topic. Exact details about the data split are given in Appendix B. We report the comparison by checking the fraction of questions that the model got correct.

Figure 1: A comparison of various prompting techniques

### Evaluating Hint based prompting

We observe that hinting improves the performance of models, as shown in Figure 2. We attribute the poorer performance of other approaches to a lack of generalization. The improvement of hinting over chain of thought, can be explained as the latter restricts the model's generalization by forcing it to follow the entire reasoning steps of a solution, which might not be the same for all the problems. On the other hand, hints only provide certain helpful directions, giving the model more freedom to generalize better to different problems.

Approaches like the chain of thought  give step-by-step solutions, which might not generalize to other problems restricting the search space of these models. This can also be due to a "snowball" effect , in which intermediate-generated steps with mistakes can derail the model from the logical direction to the right answer. One-shot and few-shot perform relatively better as they do not restrict the steps, however, the model still gets confined to a narrower subdomain, with few-shot showing more generalisation due to more examples and hence, better performance. Hinting leads to better performance as it helps the model reason about question-specific knowledge more than other prompting techniques and gives initial steps to ensure the right direction without over-fitting to an exact solution.

### Evaluating Adversarial Hinting

We find that giving Adversarial hints drastically reduces the model's performance, dropping it below CoT, which performed the worst in our non-adversarial approaches, as shown in Fig 2 and Table 1. We also apply this to few-shot and one-shot settings and find that adversarial hinting affects the performance in those cases as well. The inferior performance of CoT and its prompting variants can be attributed to over-fitting various factors within irrelevant information that influence the model's sensitivity to distractions based on lexical similarity. .

Figure 3: Adversarial and random hinting strategies

Figure 2: Left: Comparing different prompting techniques, hinting boosts performance and CoT performs worst, as explained in section 3.2. Right: Effect of adversarial hints: Adversarial hinting greatly reduces performances, as explained in section 3.3

### Model-wise performance

While comparing the performance of different models, we observe that closed-source models exhibit the highest performance, followed by models fine-tuned for mathematical tasks, instruct-tuned models, and finally, base models, as shown in table 2. Additionally, variations in model size impact performance, with smaller models generally performing worse. However, performance also depends on the extent of fine-tuning; for example, the Qwen-2-Math-Instruct model achieves comparable results to GPT-4o-mini as shown in Appendix E. Among the 7B and 8B models, Qwen-2-Math-Instruct performs best, while Mistral-Instruct ranks the lowest. Our observations further reveal that base models struggle to incorporate and utilize hints effectively, performing worse than the baseline, likely due to their limited ability to follow instructions. We list our exhaustive results in Appendix D.

Further, we also evaluated hinting multimodal models on visual and mathematical tasks and found a similar improvement due to hinting and deterioration due to adversarial hinting. We provide more details in Appendix C.1.

## 4 Conclusion

In this work, we have evaluated the mathematical reasoning abilities of various models and approaches. Our results indicate that providing hints is more effective than giving direct answers because it guides the model to the correct solution instead of restricting its search space like few-shot, one-shot, and chain of thought which lack generalization. However, CoT may outperform hinting in some cases where the example and target problem are very similar. Hence, for math problems with unknown solutions, it is better to provide the user's knowledge about the problem as hints to improve its reasoning and problem-solving capabilities than a full solution for a similar problem, and hinting is the natural way in which humans solve a reasoning task as well, as only intermediate directions to approach a problem are required, the rest can be reasoned by humans themselves. Although extracting hints requires and extra inference, it is compensated by the increase in performance over other prompting techniques, which are only useful when dealing with similar problems. Finally, we see the sensitivity to random and adversarial prompting techniques demonstrated performance loss due to adversarial/random hints in few-shot, one-shot, and chain-of-though settings.

  
**Prompt Technique** & **Baseline** & **Few-shot** & **One-shot** & **CoT** \\ 
**Baseline** & 0.5355 & 0.5122 & 0.4973 & 0.4815 \\
**with hint** & **0.5615** & 0.5171 & 0.4675 & - \\
**adversarial prompting** & 0.4674 & 0.4968 & 0.4742 & 0.4761 \\
**random prompting** & 0.4628 & 0.4711 & 0.4753 & 0.5073 \\   

Table 1: Comparison of various prompting techniques with adversarial hinting. Base CoT already entails in-context steps and hence doesn’t involve hinting. We observe a drop in performance due to both, adversarial and random hinting, indicating a sensitivity to hinting.

  
**Model category** & **Baseline** & **Hinted** & **Adv hint** & **Rand hint** \\ 
**Small models (\(\)=2B)** & 0.3314 & 0.3686 & 0.3143 & 0.2314 \\
**Base models** & 0.5385 & 0.5104 & 0.4898 & 0.2871 \\
**Instruct tuned models** & 0.5471 & 0.5601 & 0.4985 & 0.4971 \\
**Math Fine-tuned models** & 0.6180 & 0.6485 & 0.5581 & 0.5828 \\
**Closed-Source models** & **0.6685** & **0.7186** & **0.6557** & **0.6928** \\   

Table 2: A comparison of various model types. We find that hinting performs better for all models except the base models, which in general are worse at instruction following. We also notice a drastic drop in performance for all model types when given adversarial hints

## 5 Limitations and Future Work

Our work mainly focuses on prompting LLMs with problems as textual input. Testing these techniques in multi-modal models like VLMs, where geometrical problems can be passed as image inputs, is a possible future direction. Further, due to computational limitations, our experiments involved a small subset of problems to test the models. These techniques can also be evaluated on larger sample sizes to increase generalizability. Additionally, It is yet to be tested whether hinting can increase performance in other general tasks as well.

1. Our techniques are yet to be tested on larger models like the Llama 3.1 70B, 450B, Falcon 180B, OPT 175B, etc., and other closed source models like Claude Sonnet, Bard, etc due to computational limitations.
2. We compare the generated answers with the solutions using LLMs, which can introduce a degree of error.
3. These techniques can be further evaluated on the entire MATH dataset and other datasets such as GSM-8k, etc. to ensure a more exhaustive analysis.