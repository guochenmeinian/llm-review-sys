# Parallelizing Model-based Reinforcement Learning

Over the Sequence Length

 ZiRui Wang

Zhejiang University, China

ziseoiwong@zju.edu.cn

&Yue Deng

Zhejiang University, China

devindeng@zju.edu.cn

&Junfeng Long

Shanghai AI Laboratory, China

junfengac@gmail.com

&Yin Zhang

Zhejiang University, China

zhangyin98@zju.edu.cn

Corresponding author: Yin Zhang.

###### Abstract

Recently, Model-based Reinforcement Learning (MBRL) methods have demonstrated stunning sample efficiency in various RL domains. However, achieving this extraordinary sample efficiency comes with additional training costs in terms of computations, memory, and training time. To address these challenges, we propose the **P**arallelized **M**odel-based **R**einforcement **L**earning (**PaMoRL**) framework. PaMoRL introduces two novel techniques: the **P**arallel **W**orld **M**odel (**PWM**) and the **P**arallelized **E**ligibility **T**race **E**stimation (**PETE**) to parallelize both model learning and policy learning stages of current MBRL methods over the sequence length. Our PaMoRL framework is hardware-efficient and stable, and it can be applied to various tasks with discrete or continuous action spaces using a single set of hyperparameters. The empirical results demonstrate that the PWM and PETE within PaMoRL significantly increase training speed without sacrificing inference efficiency. In terms of sample efficiency, PaMoRL maintains an MBRL-level sample efficiency that outperforms other no-look-ahead MBRL methods and model-free RL methods, and it even exceeds the performance of planning-based MBRL methods and methods with larger networks in certain tasks.

## 1 Introduction

Model-based Reinforcement Learning (MBRL) is widely believed to have the great potential to substantially enhance sample efficiency by training a policy through a learned world model [1; 2; 3]. Previous studies [4; 5; 3; 6] achieve the same asymptotic performance as their model-free counterparts while requiring orders of magnitude less interactions. In particular, some recent works have even achieved human-level efficiency in complex RL domains like Atari [7; 8; 9] and robot control [10; 11].

MBRL methods can be generally divided into two stages: model learning and policy learning. During the model learning stage, a parameterized world model is required to predict the environmental dynamics by constructing specific self-supervised learning tasks. The policy learning stage benefits from synthetic interactions between the policy and the world model, hence on-policy actor-critic methods or planning methods such as Model Predictive Path Integral (MPPI) [12; 13] or Monte-Carlo Tree Search (MCTS) [7; 9] can be used for policy improvement. To obtain better performance, techniques like sequential modeling and ensembling are frequently used in the model learning stage, while the policy learning stage mostly involves the computation of eligibility traces or multi-step returns [2; 14]. However, these powerful techniques often come with additional computations,memories, and training time. This leads users to carefully consider which specific MBRL method to use or even whether to use an MBRL method based on the computational resources available.

In recent years, numerous endeavors have been made to develop an efficient world model architecture. Recurrent Neural Networks (RNNs) are frequently employed as the foundational architecture for world models [15; 16; 3; 6; 17]. However, the recurrent nature of RNNs hinders parallelization, leading to slow training speeds. In contrast, transformers have emerged as a potential successor, garnering acclaim for their remarkable performance in language modeling tasks and parallelized training paradigm . Several attempts have been made to incorporate transformers into world models [19; 20; 21; 22]. However, the quadratic complexity of transformers w.r.t. sequence length limits their efficiency during training and inference. To achieve an RNN-level inference efficiency, extra tricks such as half-precision training or KV-Cache are required. Furthermore, none of the aforementioned works have introduced improvements in the hardware efficiency of policy learning.

In this paper, we aim to mitigate the curse of computational inefficiency of current MBRL methods and achieve the best of both worlds in terms of hardware efficiency and sample efficiency. The key idea is to fully parallelize the computations of sequential data, which has been a main workhorse of the rapid progress in deep learning over the past decade . We achieve this by introducing the parallel scan. Specifically, We delve into two classic and widely implemented parallel scanners [27; 28], which can be applied for parallel training by excluding non-linear dependencies [29; 30]. Motivated by recent works in efficient sequential modeling [31; 32; 33], we observe that model architectures like linear attentions and linear RNNs not only enable parallel training but also recurrent inference. We also observe that the computations of eligibility trace estimation [2; 14] can be naturally parallelized over the sequence length by using parallel scan.

To this end, we introduce the **P**arallelized **M**odel-based **R**einforcement **L**earning (**PaMoRL**) framework, which consists of two novel techniques as shown in Figure 2 that can parallelize the current MBRL paradigm over sequence length: (1) the **P**arallel **W**orld **M**odel (**PWM**) and (2) the **P**arallelized **E**ligibility **T**race **E**stimation (**PETE**). The resulting framework, PaMoRL, is hardware-efficient and stable. It is compatible with various on-policy RL methods and can be applied to both discrete and continuous control problems using a single set of hyperparameters.

We evaluated our PaMoRL framework in the Atari 100K benchmark  and the DeepMind Control suite . Tasks in these domains include discrete and continuous action spaces, images, and proprioception observations. We choose to follow the DreamerV3  paradigm, which relies on "imagination" for policy learning. The summarized experimental results are shown in Figure 1. The empirical results demonstrate that PaMoRL, despite being a framework that incorporates autoencoding, still benefits greatly from the implementation of dual parallelization techniques (i.e., PWM and PETE). These techniques substantially enhance training speed, allowing PaMoRL to rival the performance of model-free RL methods without decoders . In terms of sample efficiency, PaMoRL outperforms other no-look-ahead MBRL methods and model-free RL methods. It is worth mentioning that PaMoRL even outperforms the planning-based MBRL methods or methods with much larger networks in certain tasks .

Figure 1: Comparisons on Atari 100k benchmark  and DeepMind Control Suite . Among these methods, DreamerV3 , and our PaMoRL are directly evaluated on an NVIDIA V100 GPU, and IRIS , TWM , and REM  are evaluated on an A100 GPU, while other methods are evaluated on a P100 GPU. The extrapolation method employed aligns with the setup used in DreamerV3, where it assumes the P100 is twice as slow and the A100 is twice as fast.

Our contributions can be summarized as follows:

\(\) We introduce PaMoRL, a novel MBRL framework equipped with PWM and PETE that parallelizes both model and policy learning stages over the sequence length simultaneously.

\(\) We evaluate our PaMoRL on the Atari 100k benchmark and DMControl suite with recent methods and obtain excellent results in terms of both sample and hardware efficiency. In addition, we conduct ablation studies on the validity of different modules, scanners, and other components.

\(\) To the best of our knowledge, we are the first to point out that the computational process of eligibility traces can be parallelized over the sequence length. This technique can not only accelerate the value estimation process of various MBRL methods but any return-based reinforcement learning methods such as TD-\(\), Retrace  and GAE  can benefit from it.

## 2 Background

Model-based Reinforcement Learning.We follow the paradigm of Partially Observable Markov Decision Process (POMDP) with observations \(o_{t}\), scalar rewards \(r_{t}\), actions \(a_{t}\), continuation flag \(c_{t}\{0,1\}\), discount factor \((0,1)\), and environmental dynamics \(o_{t},r_{t},c_{t} p(o_{t},r_{t},c_{t}|o_{<t},a_{<t})\). The objective of the Reinforcement Learning (RL) is to train a policy \(\) that maximizes the return \(_{t=1}^{}^{t-1}r_{t}\). In Model-based Reinforcement Learning (MBRL), the RL agent learns a model of the environmental dynamics through an iterative process that involves collecting data using a policy, training a model of the environment based on the accumulated data, and optimizing the policy using the learned model [1; 2; 14].

Parallel Scan.As a universal parallel algorithm building block, the computations of parallel scan involve repeated application of a binary operator \(\) over sequential data arrays. Previous work describes scan as a good example of a computation that seems inherently sequential, but for which there is an efficient parallel algorithm. The scan of \(\) with initial value \(a_{0}\) is defined in Equation 1.

\[(,[a_{1},a_{2},...,a_{n}],a_{0}):=[(a_{1} a_{0}),(a_{2}  a_{1} a_{0}),...,(a_{n} a_{n-1}... a_{1} a_{0})]\] (1)

First-order linear recurrences \(h_{t}:=(A_{t} h_{t-1}) x_{t}\) can be parallelized over the sequence length with the utilization of parallel scans if the following three conditions are met:

\(\)\(\) **is associative:**\((a b) c=a(b c)\).

\(\)\(\) **is semi-associative:** there exists a binary associative operator \(\) such that \(a(b c)=(a b) c\).

\(\)\(\) **distributes over \(\):**\(a(b c)=(a b)(a c)\).

We observe vector addition \(a b:=a+b\), matrix-vector multiplication \(A b:=A b\), and matrix-matrix multiplication \(A B:=A B\) fulfill the aforementioned conditions. This allows the parallel computation of \(x_{t}:=(A_{t} x_{t-1})+b_{t}\) across time steps \(t\), considering input vectors \(b_{t}\) and square matrices \(A_{t}\). Considering the operators required in computing linear attentions  and eligibility trace estimations [2; 14] involve only diagonal matrices, the linear recurrence can be re-formulated as \(x_{t}:=_{t} x_{t-1}+b_{t}\), where \(_{t}\) is the eigenvalues of the diagonal matrices and \(\) is an element-wise multiplication.

## 3 Methodology

We introduce our **P**arallelized **M**odel-based **R**einforcement **L**earning (**PaMoRL**) framework, which facilitates dual parallelization across both model and policy learning stages. By parallelized training and recurrent inference, PaMoRL significantly improves training speed while avoiding additional computation overhead during inference. Figure 2 illustrates the overview of our PaMoRL framework, and we will now proceed to elaborate on its details.

### Parallelized World Model Learning.

World Model Architecture Overview.As with other MBRL methods, our world model is trained to predict environmental dynamics. Since observations can be high-dimensional (e.g., images), we predict future representations rather than future observations. This reduces accumulating errors and enables massively parallel training with a large batch size. The compact representations are obtained by an autoencoder and can be utilized to predict future observations, reward, and continuation flags.

To exclude non-linear dependencies for parallel training and obtain better performance, we make several modifications to the vanilla Recurrent State-Space Model's (RSSM)  configurations: (1) differentiating the hidden states \(x_{t}\) from the sequential model's outputs \(h_{t}\), (2) excluding \(h_{t}\) from the inputs of the encoder and decoder, (3) eliminating the stochastic states \(z_{t}\) from the predictors' inputs, and (4) applying Batch Normalization for the encoder and dynamic predictor's outputs before the distributions are computed. Similar to RSSM, our model consists of six components:

\[\;\;z_{t} q_{}(z_{t}|o_{t})& {Decoder:}\;\;_{t} p_{}(_{t}|z_{t})\\ \;\;h_{t},x_{t}=f_{}(x_{t-1},z_{t-1},a_{t-1})&\;\;_{t} p_{}(_{t}|h_{t})\\ \;\;_{t} p_{}(_{t}|h_{t})& {Continue predictor:}\;\;_{t} p_{}(_{t}|h_{t})\] (2)

The encoder and decoder use convolutional neural networks (CNN) for image inputs and multi-layer perceptrons (MLPs) for proprioception inputs. The sequence model has multiple stacked residual blocks, each of which consists of a modified linear attention  module and a Gated Linear Unit (GLU)  module. The dynamics, reward, and continue predictors are all MLPs. Consistent with previous work , we set the \(q_{}(z_{t}|o_{t})\) as a stochastic distribution comprising 32 categories, each with 32 classes, and we take straight-through gradients through the sampling step .

Sequence Model Architectures.As mentioned above, each residual block of our sequence model consists of a modified linear attention module and a GLU module. The vanilla linear attention module, as introduced in previous work , employs \(1+\) as an element-wise kernel function applied to queries \(q_{t}\) and keys \(k_{t}\) taking \(u_{t}\) as input. This configuration allows for its reformulation into an RNN-style recurrent form. However, this version of linear attention is prone to unstable convergence during training due to the unbounded gradients . Thus, we remove the time-dependent normalizer, which is designed to approximate the Softmax operator and use an RMSNorm  for stabilize training. Furthermore, we incorporate the token mixing module from RWKV , which accepts inputs \(u_{t}\) and previous inputs \(u_{t-1}\), along with the gating mechanism in Gated Recurrent Unit (GRU)  to provide an input-dependent decay rate \(g_{t}\) for hidden state \(x_{t}\). The subsequent GLU module selects SiLU as the gating function, taking linear attention output \(y_{t}\) as input. By integrating all the modifications, we can derive the entire block of the sequence model as shown in Equation 3.

Figure 2: Overview of our PaMoRL framework. The symbols used in the figure are explained in Sections 3.1 and Section 3.2. The computations of the sequential modelâ€™s outputs and the TD-\(\) returns allow using parallel scans. In contrast, the imaginations cannot be parallelized over the sequence length because a non-linear actor network is required for action sampling.

\[q_{t},k_{t} =1+(u_{t}W_{q}),1+(u_{t}W_{k}),\] (3) \[v_{t} =(u_{t}W_{r}) u_{t}W_{v},\] \[g_{t} =(( u_{t}+(1-) u_{t-1})W_{g}),\] \[x_{t} =g_{t} x_{t-1}+k_{t}^{}v_{t},\] \[y_{t} =(q_{t}x_{t})W_{h}+u_{t},\] \[h_{t} =(y_{t}W_{g}) y_{t}W_{y}+y_{t}.\]

The architecture of our modified linear attention satisfies the conditions in Section 2 and can be effectively computed using parallel scans. We can refer to Table 1 to summarize the computational complexities of various model architectures such as vanilla attention, RNN, SSM, and our modified linear attention in the training, inference, and imagination stages.

Loss Functions.The total loss function of model learning is shown as in Equation 4, where \(_{}\), \(_{}\), and \(_{}\) are coefficients to adjust the influence of each term in the loss function .

\[() = _{q_{}}[_{t=1}^{T}_{}^{}(,h_{t},o_{t},r_{t},c_{t},z_{t})+_{} ^{}(,h_{t},o_{t})+_{}^{ }(,h_{t},o_{t})]\] (4) \[^{}() = - p_{}(r_{t}|h_{t})- p_{}(c_{t}|h_{t})+|| _{t}-o_{t}||_{2}\] \[^{}() = (1,[q_{}(z_{t}|o_{t})\ ||\ (p_{}(_{t}|h_{t}))])\] \[^{}() = (1,[(q_{}(z_{t}|o_{t})\ )\ ||\ p_{}(_{t}|h_{t})])\]

The operation \(()\) represents the stop gradient operation. The KL divergences are derived from the Evidence Lower Bound (ELBO). We clip the KL divergence when it falls below the threshold of 1  and use the KL-balancing trick to prioritize the training losses .

### Policy Learning

The policy learning stage incorporates the actor and critic networks, both of which are MLPs, taking concatenation of \(z_{t}\) and \(h_{t}\) as input state \(s_{t}\).

\[\ a_{t}_{}(a_{t}|s_{t}),\ v_{}(s_{t}).\] (5)

Our policy learning method is in line with DreamerV3  and can be used for both discrete and continuous action spaces. The critic uses TD-\(\) as the its target, as shown in Equation 6, where \(_{t}\) represents the reward predicted by the world model, and \(_{t}\) represents the predicted continuation flag.

\[R_{t}^{} =_{t}+(_{t})[(1-)v_{}(s_{t+1}) + R_{t+1}^{}]\] (6) \[=(_{t})R_{t+1}^{}+[_{t}+(1 -)(_{t})v_{}(s_{t+1})],\ \ R_{T}^{}=v_{T}\]

The actor utilizes the Reinforce estimator  to compute the actor loss with a fixed entropy regularization term. The complete loss is described by Equation 7.

\[() =-_{t=1}^{T}(^{}-v_{}(s_{t})}{ (1,S)})_{}(a_{t}|s_{t})- H(_{}(a_{t}|s _{t}))\] (7) \[() =-_{t=1}^{T}(v_{}(s_{t})-(R_{t}^{}))^{2}.\]

The hyper-parameter \(\) represents the coefficient of the entropy regularization term. The normalization ratio \(S\) utilized in the actor loss is defined in Equation 8, which is computed as the range between the \(95^{}\) and \(5^{}\) percentiles of the TD-\(\) returns \(R_{t}^{}\) across the batch.

\[S=(R_{t}^{},95)-(R_{t}^{},5)\] (8)

By rearranging Equation 6, we can see that the calculations of both TD-\(\) and Retrace returns also also meet the conditions mentioned in Section 2. Therefore, they can be efficiently computed using parallel scan. This observation also applies to other eligibility trace estimation methods such as GAE  and Retrace , as they still satisfy the aforementioned conditions.

### Parallel Scan Algorithms

In both the model learning stage and the policy learning stage, we use two different parallel scanners: the Kogge-stone scanner  and the Odd-even scanner .

The Kogge-stone scanner  is commonly used in hardware design for adders. It has a computational complexity of \((L_{2}L)\) for sequence length \(L\) and a step complexity of \((_{2}L)\) after full parallelization. This indicates that it has higher computational redundancy, lower running time, and sufficient computational resources, making it suitable for parallel computation in a small batch.

The Odd-even scanner  is based on the concept of binary balanced trees. It has a computational complexity of \((2L)\) for a sequence length \(L\) and a step complexity of \((2_{2}L)\) after being fully parallelized. Despite theoretically taking more steps than the Kogge-stone scanner, it offers lower computational complexity and more uniform load sharing, making it better suited for large-scale parallel computation. Further details and illustrations are in Appendix B.

## 4 Experiments

In this section, we aim to evaluate both the sample and training efficiency of our PaMoRL framework on the Atari 100K benchmark  and the DMControl suite . The tasks include various scenarios with image and proprioception observations and discrete and continuous action spaces.

### Experimental Setup

Atari 100K.Atari 100K consists of 26 video games with discrete action dimensions of up to 18. The 100K samples are equated to 400K actual game frames, corresponding to approximately 2 hours of real-time gameplay, with action repeats of 4. The human normalized score is defined as \((_{}-_{})/(_{ }-_{})\), where \(_{}\) comes from a random policy, and \(_{}\) is obtained from human players .

DeepMind Control Suite.DeepMind Control Suite consists of various control tasks with continuous action spaces. Referring to the categorizations in Sample MuZero  and EfficientZero V2 , tasks are divided into **easy** and **hard** categories. We followed the experimental setup of EfficientZero V2  and established two benchmarks, named **Proprio Control** and **Visual Control**.

Among them, **Proprio Control** uses proprioception observations with 50K training samples for easy tasks and 100K for hard tasks, and **Visual Control** uses image observations with 100K training

   Architecture & Training & Inference step & Imagination step & Parallel & Resettable & Selective \\  Atten & \((L^{2})\) & \((L^{2})\) & \(((L+H)^{2})\) & & & \\ RNN & \((L)\) & \((1)\) & \((1)\) & & & \\ SSM (FFT) & \((LL)\) & \((1)\) & \((1)\) & & & \\ SSM (Scan) & \((L)\) & \((1)\) & \((1)\) & & & \\  Lin-Atten (Scan) & \((L)\) & \((1)\) & \((1)\) & & & \\   

Table 1: The step complexities  of different architectures, where \(L\) is the sequence length and \(H\) is the imagination horizon. Attention considers the full context with a burn-in and imagined steps of \((L+H)\), leading to a complexity of \(((L+H)^{2})\). It is worth noting that the SSMs in recent works [46; 47] do not incorporate any gating mechanism or selectivities. Thus, despite SSMs and linear attentions both achieving the minimum complexity, linear attentions remain more expressive.

samples for easy tasks and 200K for hard tasks. Each benchmark includes 16 tasks. Action repeats are set to 2, and the maximum episode length is 1000 for both benchmarks, in line with previous studies [17; 13; 9]. We choose various baselines for each domain, which include SAC , DrQ-v2 , and DreamerV3 .

### Experimental Results

In this section, we do not compare our results with look-ahead search methods [52; 7; 12; 13] or methods using larger networks , as our main goal in terms of sample efficiency is to improve performance while **maximizing the hardware efficiency** of existing MBRL methods.

Atari 100K.The summarized results are shown in Figure 4. The full results for individual games in the Atari 100k benchmark are elaborated in Table 2, where scores are normalized against those of human players. Our PaMoRL framework attains a mean score of **126.64%** and a median score of **71.75%**, surpassing the other methods in terms of both mean and median human normalized score. For detailed training curves, please refer to Appendix C. Additionally, you can find more results and further discussions, including methods with look-ahead search or larger networks, in Appendix I.

DeepMind Control Suite.Table 3 shows that our method achieves a mean score of **661.2** across 16 tasks. As shown in Table 3, our method achieves a mean score of **661.2** using proprioception observations and **538.7** using image observations across 16 tasks, surpassing the previous state-of-the-art, DreamerV3. The improvement in sample efficiency is attributed to two key modules: the token mixing module in the PWM, where the extra previous input provides more information to the data-dependent decay rate, and the implementation of RMSNorm, which improves the stability of the learning of the linear attention module, especially in the case of limited data. Our PaMoRL framework consistently demonstrates MBRL-level sample efficiency in tasks with proprioception observations, image observations, and discrete and continuous action spaces. Detailed training curves can be found in Figure 9 and Figure 10 in Appendix D.

  Game & Random & Human & SPR & SR-SPR & SimPLe & IRIS & TWM & STORM & DreamerV3 & PaMoRL (Ours) \\  Aien & 227.8 & 71227.7 & 801.5 & 1015.5 & 616.9 & 420 & 674.6 & 984 & 959 & **1270.6** \\ Amidar & 5.8 & 1719.5 & 176.3 & 203.1 & 88 & 143 & 121.8 & 205 & 139 & **264.4** \\ Assault & 222.4 & 742 & 571 & 1069.5 & 527.2 & **1524.4** & 682.6 & 801 & 706 & 883.8 \\ Asterix & 120 & 8503.3 & 977.8 & 916.5 & 128.3 & 835.6 & 1116.6 & 1028 & 932 & **2957.3** \\ BanHeist & 14.2 & 753.1 & 380.9 & 472.3 & 34.2 & 53.1 & 466.7 & 641 & **649** & 255.9 \\ BattleZone & 2360 & 37187.5 & 16651 & 19399.84 & 814.1 & 13074 & 5068 & 13540 & 12250 & **23120** \\ Boking & 0.1 & 12.1 & 35.8 & 46.7 & 9.1 & 70.1 & 77.5 & 80 & 78 & **87.9** \\ Breakout & 1.7 & 30.5 & 17.1 & 28.8 & 164 & **83.7** & 20 & 16 & 31 & 15.8 \\ ChopperCommand & 811 & 7387.8 & 974.8 & **2201** & 1264.4 & 1565 & 1697.4 & 1888 & 420 & 2110.7 \\ CrazyClimmer & 1080.5 & 38529.4 & 442923.6 & 4312.23 & 6258.3 & 693524.2 & 17820.4 & 66776 & **97190** & 84102 \\ DemonAttack & 152.1 & 1971 & 545.2 & **2898.1** & 208.1 & 2034.4 & 350.2 & 165 & 303 & 208.2 \\ Freeway & 0 & 29.6 & 24.4 & 24.9 & 20.3 & 31.1 & 24.3 & 0 & 33.8 \\ Frostbite & 65.2 & 4334.7 & 1821.5 & 1752.8 & 254.7 & 259.1 & 1475.6 & 1316 & 909 & **3711.4** \\ Gopher & 257.6 & 421.5 & 715.2 & 771.2 & 777 & 226.1 & 1674.8 & **8240** & 3730 & 5085.2 \\ Hero & 1027 & 30826.4 & 709.2 & 17679.6 & 2656.6 & 7037.4 & 7254 & 11044 & 11161 & **12076.2** \\ Jambedon & 29 & 302.8 & 365.4 & 39.8 & 125.3 & 462.7 & 362.4 & **509** & 445 & 405 \\ Kamgaroroo & 52 & 3035 & 3276.4 & 3254.9 & 323.1 & 838.2 & 1240 & **4208** & 4098 & 2554.7 \\ Krull & 1598 & 2665.5 & 3688.9 & 5824.8 & 4539.9 & 6616.4 & 6439.2 & **8413** & 7782 & 7273.2 \\ KungFukMaster & 258.5 & 22736.3 & 13192.7 & 10795.6 & 17257.2 & 21759.8 & 24554.5 & **26182** & 21420 & 24624.7 \\ MaPacman & 307.3 & 6951.6 & 1313.2 & 1522.6 & 1480 & 999.1 & 1588.4 & **2673** & 1327 & 2021.7 \\ Pong & -20.7 & 14.6 & -5.9 & -3 & 12.8 & 14.6 & **18.8** & 11 & 18 & 15.5 \\ PrivateFye & 24.9 & 69571.3 & 124 & 95.8 & 58.3 & 100 & 86.6 & **7781** & 882 & 4968.6 \\ Quet & 163.9 & 13455 & 669.1 & 3850.6 & 1288.8 & 745.7 & 3330.8 & 4522 & 3405 & **2703.3** \\ Roadruner & 11.5 & 7845 & 14220.5 & 13623.5 & 5640.6 & 9614.6 & 9109 & 17564 & 15565 & **24726.7** \\ Seaquest & 68.4 & 42054.7 & 583.1 & **800.5** & 683.3 & 661.3 & 7744.4 & 525 & 618 & 595.2 \\ UpNDown & 533.4 & 1693.2 & 22813.5 & **55901.1** & 3350.3 & 3546.2 & 15981.7 & 7985 & 7667 & 11935.8 \\ Games Human & 0 & 26 & 7 & 9 & 2 & 9 & 8 & 9 & 9 & **11** \\ Median & 0\% & 100\% & 41.53\% & 56.07\% & 14\% & 29\% & 51\% & 42.63\% & 49\% & **71.75\%** \\ Mean & 0\% & 100\% & 70.34\% & 118.84\% & 44\% & 105\% & 96\% & 122.30\% & 112\% & **126.64\%** \\  

Table 2: Experimental results on the 26 games of Atari 100k after 2 hours of real-time experience and human-normalized aggregate metrics. Bold and underlined numbers indicate the highest and the second-highest scores, respectively. PaMoRL outperforms other methods regarding the number of superhuman games, mean, and median.

### Ablation Study

In this section, we will be conducting ablation studies to evaluate the effectiveness of PWM and PETE in terms of stabilizing training and improving hardware efficiency. For more details, including PyTorch-style pseudo-code, please refer to Appendix G.

World Model Design.The results presented in Figure 3 demonstrate the impact of adding or removing the token mixing, RMSNorm, and data-dependent decay rate in various games in the Atari 100K benchmark. To showcase the benefits of token mixing in sequence prediction, we focused on tasks such as _Alien_, _Boxing_, and _MsPacman_. Additionally, we measured the improvement of RMSNorm on training stability by considering tasks like _Amidar_, _UpNDown_, and _Qbert_.

   Task &  &  \\   & SAC & DreamerV3 & PaMoRL (Ours) & CURRL & DrQ-v2 & DreamerV3 & PaMoRL (Ours) \\  Cartpole Balance & **997.6** & 839.6 & 994.7 & 963.3 & **965.5** & 956.4 & 610.3 \\ Cartpole Balance Sparse & 993.1 & 559 & **997.4** & 999.4 & **1000** & 813 & 996.5 \\ Cartpole Swingup & **861.6** & 527.7 & 773.6 & **765.4** & 756 & 374.8 & 281.9 \\ Cup Catch & 949.9 & 729.6 & **957.9** & 932.3 & 468 & 947.7 & **966.3** \\ Finger Spin & **900** & 765.8 & 835.8 & **850.2** & 459.4 & 633.2 & 765.3 \\ Pendulum Swingup & 158.9 & **830.4** & 707.1 & 144.1 & 233.3 & **619.3** & 26.6 \\ Reacher Easy & 744 & 693.4 & **761.6** & 467.9 & 722.1 & 441.4 & **950.2** \\ Reacher Hard & 646.5 & **768** & 645.9 & 112.7 & **202.9** & 120.4 & 103.7 \\  Cartpole Swingup Sparse & 256.6 & 172.7 & **542.3** & 8.8 & 81.2 & **392.4** & 263.6 \\ Cheeta Run & **680.9** & 400.8 & 313.2 & 405.1 & 418.4 & 587.3 & **935.6** \\ Finger Turn Easy & **630.8** & 560.5 & 671.1 & 371.5 & 286.8 & 366.6 & **886.2** \\ Finger Turn Hard & 414 & **474.2** & 389.7 & 236.3 & 268.4 & 258.5 & **500.1** \\ Hopper Hop & 0.1 & 9.7 & **387.5** & 84.5 & 26.3 & 76.3 & **426.9** \\ Hopper Stand & 3.8 & **296.1** & 151.5 & 627.7 & 290.2 & **652.5** & 189.7 \\ Quadruped Run & 139.7 & **289** & 246.7 & 170.9 & 339.4 & 168 & **344.8** \\ Quadruped Walk & 237.5 & 256.2 & **457.9** & 131.8 & 311.6 & 122.6 & **371.6** \\  Mean & 538.4 & 510.8 & **611.2** & 454.5 & 426.8 & 470.7 & **538.7** \\ Median & **638.7** & 543.4 & 631.5 & 388.3 & 325.5 & 416.9 & **463.5** \\   

Table 3: Experimental results on the DeepMind Control suite. Bold and underlined numbers indicate the highest and the second-highest scores, respectively. PaMoRL outperforms other baselines in terms of the number of mean and median scores.

Figure 3: Ablation studies of the effectiveness of each module of PWM, where SSM is equivalent to removing the data-dependent decay rate of PWM. We also include vanilla DreamerV3 as a baseline.

The findings in Figure 3 indicate that, while the token mixing module has minimal impact on the final performance for tasks where the reward can be accurately predicted from a single frame (e.g., _Boxing_), it leads to a performance drop on tasks that require several contextual frames to predict the reward accurately (e.g., _Alien_ and _Ms. Pacman_). Regarding RMSNorm, removing it negatively affects the final performance and increases the instability of the training process.

There are two possible reasons for this difference. First, the gradient is bounded after the original normalizer is removed . Adding RMSNorm further enhances training stability, which is especially important in the setting of limited data and end-to-end training. Second, RMSNorm only rescales the input and maintains the original center of the samples, which allows the module's output to maximize the information's retention.

Parallel Scanner Selection.Figure 5 shows PWM and PETE's runtime and GPU memory utilization on a single 3090 GPU using different scanners, respectively. Sequence model computation

Figure 4: **(Left)** Atari 100K aggregated metrics with 95% stratified bootstrap confidence intervals of the mean, median, and interquartile mean (IQM) human-normalized scores and optimality gap. **(Right)** Probabilities of improvement, i.e. how likely it is for our PaMoRL to outperform baselines.

Figure 5: **(Upper)** Comparison of parallel scanners with sequential rollout in terms of runtime for sequence modeling and eligibility trace estimation, as well as total GPU memory utilization. **(Lower)** Wall-clock time vs. GPU memory usage comparison for our PaMoRL method, SSM, and DreamerV3 across various batch size and sequence length combinations.

achieves \(7.2\) and \(16.6\) speedups compared to sequential rollout using the Kogge-stone and Odd-even scanners, respectively, with a sequence length of 64. In this case, the Kogge-stone scanner with the theoretically lowest runtime takes more than the Odd-even scanner in practice. This is because the computation of the sequence model involves the parallelism of both batch and hidden dimensions, which belongs to massively parallel computation, and the Kogge-stone scanner cannot realize full parallelism and thus encounters a bottleneck in computational resources. In contrast, the Odd-even scanner is due to less computational redundancy, which allows the computational process of sequence modeling to achieve full parallelism and thus spends less running time. The PETE uses the Kogge-stone scanner and Odd-even scanner to achieve \(3\) and \(2\) speedups, respectively, with a sequence length of 16. Since the eligibility trace has a dimension of only 1, the Kogge-stone scanner can take full advantage of it. It thus achieves less runtime compared to the Odd-even scanner.

Regarding GPU memory utilization, using the Kogge-stone scanner imposes an additional \(6\) overhead compared to the sequential rollout, while the Odd-even scanner imposes an additional \(2\) overhead compared to the sequential rollout. However, the additional GPU memory overhead of parallel computation is not significant compared to the GPU memory overhead of encoder and decoder computation, especially in tasks with image observation.

Therefore, we recommend using the Odd-even scanner for PWM and the Kogge-stone scanner for PETE to achieve maximal speed with acceptable additional GPU memory utilization.

Batch Normalization Trick.World models are commonly learned using variational autoencoders to create concise representations of observations. However, they have some drawbacks, such as the tendency to disregard small moving objects. In Figure 11 in Appendix K, the reconstruction results are compared with and without using Batch Normalization for the _Pong_ and _Breakout_ games in the Atari 100K benchmark. It is observed that Batch Normalization improves the ability to distinguish similar video frames and capture information about small objects by re-centering the samples.

Additionally, Figure 12 demonstrates that PWM benefits from the batch normalization trick, whereas DreamerV3 does not. This is likely due to PWM's decoder solely having stochastic states as inputs, making it challenging for training samples to be distinguished from each other in the early stages of training, leading to "posterior collapse" . On the other hand, DreamerV3's decoder mitigates this problem by incorporating additional deterministic states as conditional inputs.

## 5 Conclusion & Limitations

In this paper, we introduce the PaMoRL framework, an MBRL method capable of being computed using the parallel scan in both the model learning and policy learning stages. The key breakthrough of PaMoRL is the integration of two novel techniques: the Parallelized World Model and Parallelizable Eligibility Trace Estimation. With these techniques, we simultaneously accelerate the training process while maintaining MBRL-level sample efficiency. PaMoRL demonstrates excellent hardware efficiency and training stability in various games or tasks in the Atari 100K benchmark and DeepMind Control suite without incurring additional overhead during inference. An important contribution of our work is the introduction of a modified linear attention module in the MBRL method. Furthermore, we show that eligibility trace estimation computation can be parallelized for the first time.

It's important to acknowledge the limitations of our work. For instance, planning-based MBRL methods cannot parallelize computation over the sequence length, which hinders the incorporation of the most sample-efficient methods within our PaMoRL framework to maximize hardware efficiency. It would be interesting to explore using hybrid architectures to enhance PaMoRL by leveraging the strengths of Transformers, RNNs, and SSMs. Additionally, the world model and baselines used for comparison in PaMoRL are trained end-to-end with joint optimization of the image encoder and sequence model. While this end-to-end training paradigm enables the world model to predict the latent representations, it also impacts the scalability of the world model.