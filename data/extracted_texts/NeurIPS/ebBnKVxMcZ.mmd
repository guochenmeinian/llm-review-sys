# Confidence Calibration of Classifiers with Many Classes

Adrien Le Coz\({}^{1,2,3}\)  Stephane Herbin\({}^{2,3}\)  Faouzi Adjed\({}^{1}\)

\({}^{1}\)IRT SystemX \({}^{2}\)ONERA - DTIS \({}^{3}\) Paris-Saclay University

adrien.2mvb@passinbox.com stephane.herbin@onera.fr faouzi.adjed@irt-systemx.fr

###### Abstract

For classification models based on neural networks, the maximum predicted class probability is often used as a confidence score. This score rarely predicts well the probability of making a correct prediction and requires a post-processing calibration step. However, many confidence calibration methods fail for problems with many classes. To address this issue, we transform the problem of calibrating a multiclass classifier into calibrating a single surrogate binary classifier. This approach allows for more efficient use of standard calibration methods. We evaluate our approach on numerous neural networks used for image or text classification and show that it significantly enhances existing calibration methods. Our code can be accessed at the following link: https://github.com/allglc/tva-calibration.

## 1 Introduction

The considerable performance increase of modern deep neural networks (DNNs) and their potential deployment in real-world applications has made reliably estimating the probability of wrong decisions a key concern. When such components are expected to be embedded in safety-critical systems (e.g., medical or transportation), estimating this probability is crucial to mitigate catastrophic behavior. One way to address this question is to treat it as an uncertainty quantification problem , where the uncertainty value computed for each prediction is considered as a confidence. This confidence can be used to reject uncertain decisions proposed by the DNN , for out-of-distribution detection , or to control active learning  or reinforcement learning based systems . When confidence values reliably reflect the true probability of correct decisions, i.e., their accuracy, a predictive system is said to be _calibrated_. In this case, confidence values can be used as a reliable control for decision-making.

We are interested in producing an uncertainty indicator for decision problems where the input is high dimensional and the decision space large, typically classifiers with tens to thousands of classes. For this kind of problem, DNNs are common predictors, and their outputs can be used to provide an uncertainty value at no cost, i.e., without necessitating heavy estimation such as Bayesian sampling  or ensemble methods . Indeed, most neural architectures for classification instantiate their decision as a softmax layer, where the maximum value can be interpreted as the maximum of the posterior probability and, therefore, as a confidence. Unfortunately, uncertainty values computed in this way are often miscalibrated. DNNs have been shown to be over-confident , meaning their confidence is higher than their accuracy: predictions with 90% confidence might be correct only 80% of the time. A later study  suggests that model architecture impacts calibration more than model size, pre-training, and accuracy. For ImageNet classifiers, the accuracy and the number of model parameters are not correlated to calibration, but model families are .

These studies show that it is difficult to anticipate the calibration level of confidence values computed directly from DNNs and exhibit the benefits of a complementary post-processing calibration. This calibration process can be seen as a learning step that exploits data from a calibration set, distinct from the training set, and is used to learn a function that maps classifier outputs into better-calibrated values. This process is typically lightweight and decoupled from the issue of improving modelperformance. A standard baseline for post-processing calibration is Temperature Scaling , where the penultimate logit layer is scaled by a coefficient optimized on the calibration set.

Many post-processing calibration methods have been developed for binary classification models [50; 69; 70]. Applying these methods to multiclass classifiers requires some adaptation. One standard approach reformulates the multiclass setting into many One-versus-All binary problems (one per class) . One limitation of this approach is that it does not scale well. When the number of classes is large, the calibration data is divided into highly unbalanced subsets that do not contain enough positive examples to solve the One-versus-All binary problems. Other methods based on Platt scaling  involve learning a set of parameters whose size grows with the number of classes. For problems with many classes, they tend to overfit, as we demonstrate in this work.

The main idea of our work is to reformulate the multiclass confidence estimation into a _single_ binary problem. This problem can be phrased as the unique question: "Is the prediction correct?". In this formulation, the confidence score is defined as the maximum class probability of the binary problem that outputs \(1\) if the predicted class is correct and \(0\) otherwise. The intent is that the confidence score accurately describes whether the prediction is correct, regardless of the class. We show that this novel approach, which we call _Top-versus-All_ (TvA), significantly improves the performance of standard calibration methods: Temperature and Vector Scaling , Dirichlet Calibration , Histogram Binning , Isotonic Regression , Beta Calibration , and Bayesian Binning into Quantiles . We also introduce a simple regularization for Vector Scaling or Dirichlet Calibration that mitigates overfitting when the number of classes is high relative to the calibration data size. We conduct experiments on multiple image and text classification datasets and many pre-trained models.

Our main contributions are the following:

* We discuss four issues of the standard approach to confidence calibration.
* To solve these issues, we develop the Top-versus-All approach to confidence calibration of multiclass classifiers, transforming the problem into a single binary classifier's calibration. This straightforward reformulation enables more efficient use of existing calibration methods, achieved with minimal modifications to the methods' original algorithms.
* Applied to scaling methods for calibration (such as Temperature Scaling), TvA allows the use of the binary cross-entropy loss, which is more efficient in decreasing the confidence of wrong predictions and leads to stronger gradients in the case of Temperature Scaling. Applied to binary methods for calibration (such as Histogram Binning), TvA significantly improves their performance and makes them accuracy-preserving.
* We demonstrate our approach's scalability and generality with extensive experiments on image classification with state-of-the-art models for complex datasets and on text classification with Pre-trained Language Models (PLMs) and Large Language Models (LLMs).

## 2 Related work

CalibrationThere are various notions of multiclass calibration. One can consider confidence , class-wise , top-\(r\), top-label , decision , projection smooth , or strong [60; 65] calibration. For recent surveys, we refer to  and . In this work, we focus on confidence calibration and not on the calibration of the full probability vector. Indeed, confidence calibration is useful for many applications that only require a single confidence value: selective classification , out-of-distribution detection , or active learning . For these applications, stronger notions of calibration are both difficult and useless. Also, class-wise calibration metrics do not appropriately scale to large numbers of classes, a setting we consider in this work, as explained in Appendix E.

MetricsSeveral metrics have been proposed to quantify calibration error. The most common is the Expected Calibration Error (ECE)  (see Equation 2). ECE has flaws: the estimation quality is influenced by the binning scheme, and it is not a proper scoring rule [14; 60; 48]. Despite its flaws, it remains the standard comparison metric for confidence calibration. Variants of ECE have also been developed: classwise-ECE , ECE with equal mass bins [48; 44], or top-label-ECE, which adds a conditioning on the predicted class . The Brier score  is also used to measure calibration. The proximity-informed expected calibration error (PIECE) evaluates the miscalibration due to proximity bias . We mainly use the standard ECE in this work, and the Appendix contains more metrics.

Training calibrated networksSeveral solutions have been proposed in the literature to improve calibration by training neural networks in specific ways, generally by making use of a new loss term . While these methods directly optimize calibration during the training phase of the networks, they require a high development time, often compromise accuracy, and are not adapted to pre-trained foundation models. That is why we prefer to focus on calibrating already-trained models.

Post-processing (or post-hoc) calibration methodsAnother approach is to calibrate already-trained models. This lowers the development time by decoupling accuracy optimization and calibration. In this paper, we divide post-hoc calibration methods into two categories: scaling and binary.

Scaling methods are derived from Platt scaling  and optimize some parameters to scale the logits. Temperature Scaling  is a popular simple post-processing calibration method. The logits vector is scaled by a coefficient, which modifies the probability vector. Vector Scaling  is more expressive and has good performance in many cases . Matrix Scaling can also be considered for more expressiveness but is difficult to apply without overfitting . Dirichlet Calibration  proposes a regularization strategy for Matrix Scaling.  developed Ensemble Temperature Scaling. Scaling can be combined with binning . Besides logits or probabilities, features can also be used .

Another family of methods tackles binary classification. We designate them as binary methods. Histogram Binning  divides the prediction into \(B\) bins according to the predicted probability. For each bin, a calibrated probability is computed from the calibration data. The probability becomes discrete: it can only take \(B\) values. With some modifications, it outperforms scaling methods .

Isotonic Regression  learns a piecewise constant function to remap probabilities. Bayesian Binning into Quantiles  brings Bayesian model averaging to Histogram Binning. Beta Calibration  uses a beta distribution to obtain a calibration mapping.

Our work reformulates the multiclass calibration problem and allows more efficient use of all these calibration methods, with little to no change in their algorithms.

Multiclass to BinaryUsing binary calibration methods for a multiclass classifier requires adapting the multiclass setting. This is usually done with a One-versus-All approach . The multiclass setting is decomposed into \(L\) One-versus-All independent problems: one binary problem for each class.  introduce the notion of top-label calibration, i.e., confidence calibration with additional conditioning on the predicted class (top-label). They describe a general multiclass-to-binary framework to develop top-label calibrators.  derive \(L(L-1)/2\) pairwise binary problems. The approach requires training the classifier from scratch, and its performance decreases with the number of classes. Our work tackles this multiclass-to-binary research problem. Contrary to , the One-versus-All approach  and top-label calibrators , our approach works well for problems with many classes. The methods I-Max  and IRM  use a shared class-wise strategy to compute a single calibrator. The calibrator is applied to all class probabilities separately, so the class probabilities ranking and prediction might change. In contrast, \(\) applied to binary methods rescales the confidence after the class prediction is made. I-Max and IRM consider the full class probabilities vectors, while \(\) only considers confidence values. Also, they build on top of Histogram Binning and Isotonic Regression, respectively, while we apply our approach to many calibration methods. A concurrent work building on an intuition similar to ours derives a calibration method based on a Correctness-Aware Loss . Appendix C discusses the differences between our approach and others.

## 3 Problem setting

### Background

Confidence calibration of a classifierWe consider the classification problem where an input \(x\) is associated with a class label \(y=\{1,2,...,L\}\). The neural network classifier \(f\) provides a class prediction from a final softmax layer \(\) that transforms intermediate logits \(z\) into probabilities. The classifier prediction is the most probable class \(=*{arg\,max}_{k}f_{k}(x)\) with \(f_{k}(x)\) referring to the probability of class \(k\), and the confidence score defined as \(s=_{k}f_{k}(x)\). Note that we use the term _confidence_ to denote the maximum class probability. With \(y\) the real label, we consider the confidence calibration definition from  that says that the classifier \(f\) is calibrated if:

\[P(=y|s=p)=p, p\] (1)

where the probability is over the data distribution. Equation (1) expresses that the probability of being correct when the confidence is around \(p\) is indeed \(p\). For instance, if we consider the set of predictionswith a confidence of \(90\%\), they should be correct \(90\%\) of the time. The conditional probability of (1) is not rigorously defined mathematically (the event \(\{s=p\}\) has zero probability), and interval-based empirical estimators are often used to define metrics capable of evaluating how well (1) is satisfied. This is the case of ECE, which approximates the calibration error by partitioning the confidence distribution into \(B\) bins. The absolute difference between the accuracy and confidence is computed for each subset of data in the bins. The final value is a weighted sum of the differences of each bin.

\[=_{b=1}^{B}}{N}|(b)-(b)|\] (2)

Where \(n_{b}\) is the number of samples in bin \(b\), \(N\) is the total number of samples, \((b)\) is the accuracy in bin \(b\), and \((b)\) is the average confidence in bin \(b\). ECE can be interpreted visually by looking at diagrams such as those of Figure 1: ECE computes the sum of the red bars (difference between bin accuracy and average confidence) weighted by the proportion of samples in the bin.

Post-processing calibration methodsWe are considering the scenario where a classifier has already been trained, and the objective is to enhance its calibration. Post-processing calibration methods aim to remap the classifier probabilities to better-calibrated values without modifying the classifier. They typically use a calibration set different from the training set to optimize parameters or learn a function. We note the calibration data \(D_{cal}=\{(x_{i},y_{i})\}_{i=1}^{N}\). We focus on post-processing calibration because it enables better utilization of off-the-shelf models and separates model training (optimized for accuracy) from calibration. These advantages significantly reduce the development cost of obtaining a well-performing and well-calibrated model, contrary to optimizing calibration during training. We categorize the post-processing calibration techniques considered in this paper into two groups: scaling methods and binary methods.

### Issues related to current approaches

Behavior of current scaling methodsScaling methods for calibration optimize one or more coefficients that scale the logits vector to minimize on calibration data the cross-entropy loss defined as \(l_{CE}=-_{k=1}^{L}1_{k=y}(f_{k}(x))=-(f_{y}(x))\). Minimizing \(l_{CE}\) therefore increases the probability of the true class. We can distinguish two cases to understand what happens during the optimization: whether the prediction \(\) is correct or not. In the first case, the confidence score is \(s=f_{y}(x)\): minimizing \(l_{CE}\) increases the confidence \(f_{y}(x)\). In the second case, the prediction is incorrect, which implies that \(f_{y}(x)<s\). Minimizing \(l_{CE}\) increases the probability of the true class \(f_{y}(x)\) but does not directly change the confidence (because \(s f_{y}(x)\)). Instead, the confidence (which was attributed to a wrong class) is _indirectly_ lowered through the softmax normalization.

\(\)_Issue 1: Cross-entropy loss only indirectly lowers confidence in wrong predictions._

We identified another issue of some scaling methods. By design, the number of parameters optimized by Vector Scaling and Dirichlet Calibration grows with the number of classes. When the number of classes is high, these methods overfit the calibration set as shown in Figure 2.

\(\)_Issue 2: Vector Scaling and Dirichlet Calibration overfit calibration sets with many classes._

One-versus-All approach for binary methodsThe One-versus-All (OvA) calibration approach  allows adapting calibration methods for binary classifiers to multiclass classifiers. To do so, it decomposes the calibration of multiclass classifiers into sets of \(L\) binary calibration problems: one for each class \(k\). For each problem, the considered probability is \(f_{k}(x)\), and the associated label \(1_{y=k}\{0,1\}\). When calibrating a classifier from data, each binary problem is highly imbalanced with a ratio between positive and negative examples equal to \(\) if the classes are equally sampled. For instance, for ImageNet, the ratio is 1/999: out of 25000 examples, only 25 have a positive label.

\(\)_Issue 3: OvA approach leads to highly imbalanced binary problems._

At test time, each of the \(L\) class probabilities is calibrated by a separate calibration model. The resulting probability vector can be normalized to ensure a unit norm. Because each probability is calibrated independently, their ranking can change, thus modifying the predicted class. In Table 9, we see that accuracy is often negatively impacted in practice.

\(\)_Issue 4: OvA approach can change the predicted class and negatively impact the accuracy._

## 4 Top-versus-All approach to confidence calibration

``` Input: \(D_{cal}\): \(\{(x_{i},y_{i})\}_{i=1}^{N}\) the calibration data \(f\): the multiclass classifier \(g\): a calibration function \(\) e.g., Temperature Scaling Preprocessing: \(}*{arg\,max}_{k}f_{k}(x_{i})\)\(\) Compute class predictions \(y_{i}^{b} 1_{_{i}=y_{i}}\)\(\) Compute predictions correctness \(f^{b}_{k}f_{k}\)\(\) Create surrogate binary classifier \(D_{cal}^{}\{(x_{i},y_{i}^{b})\}_{i=1}^{N}\)\(\) Build binary calibration set Learn calibration function:  Learn \(g\) to calibrate the surrogate binary classifier \(f^{b}\) on \(D_{cal}^{}\) Inference:  Use \(g\) to calibrate the confidences of the original multiclass classifier \(f\) ```

**Algorithm 1** Top-versus-All approach to confidence calibration

### General presentation

In the calibration definition (1) and the standard ECE metrics, only the confidence, i.e., the maximal probability, reflects the likelihood of making an accurate prediction. The probabilities of other classes are not taken into account. However, the standard approach to calibration uses the entire set of probabilities, not just confidence, which introduces unnecessary complexity. We aim to simplify the process by reformulating the problem of calibrating multiclass classifiers into a _single_ binary problem. This problem can be phrased as: "Is the prediction correct?". In this setting, we do not calibrate the predicted probabilities vector but only a scalar: the confidence. The remaining probabilities are discarded. This is equivalent to calibrating a _surrogate_ binary classifier that predicts whether the class prediction is correct. Since this correctness classifier only considers the maximal probability versus all others, we call our approach _Top-versus-All_ (TvA).

Replacing the standard approach by TvA is straightforward. Given the standard calibration data \(D_{cal}=\{(x_{i},y_{i})\}_{i=1}^{N}\), we add a few data preprocessing steps. First, compute the class predictions \(\) and their correctness: \(y^{b}=1_{=y}\). Second, create the surrogate binary classifier \(f^{b}(x)=_{k}f_{k}(x)\). Finally, build the calibration set for the surrogate binary classifier:

\[D_{cal}^{}=\{(x_{i},y_{i}^{b})\}_{i=1}^{N}\] (3)

After this preprocessing, we choose a standard calibration function \(g\), e.g., Temperature Scaling, to calibrate the surrogate binary classifier. The learning of the calibration function follows its original underlying algorithm but uses the modified calibration data \(D_{cal}^{}\). The learned calibration function is then applied to the confidences of the original multiclass classifier. Algorithm 1 describes our approach. In the Appendix, Algorithm 3 provides more details and highlights differences with the standard approach of Algorithm 2.

After this general presentation, we explain how TvA impacts the two categories of calibration methods, scaling and binary, in Subsections 4.2 and 4.3, respectively. We also justify its behavior.

### Top-versus-All approach for scaling methods

Because our Top-versus-All setting reformulates the calibration of multiclass classifiers into a binary problem, the natural loss is the binary cross-entropy:

\[l_{BCE}=-y^{b} s+(1-y^{b})(1-s)\] (4)

Minimizing this loss results in confidence estimates that more accurately describe the probability of being correct, regardless of the \(L-1\) less likely class predictions. Using the binary cross-entropy as a calibration loss makes an important difference compared to the usual multiclass cross-entropy. The cross-entropy loss takes into account the probability of the _correct_ class, while with TvA the binary cross-entropy takes into account the probability of the _predicted_ class (i.e., the confidence).

As for the standard approach, only two cases are possible. When the prediction is correct, \(l_{BCE}=-(s)=-(f_{y})=l_{CE}\). We get the same result as the cross-entropy loss: minimizing it directly increases the confidence. But when the prediction is incorrect, \(l_{BCE}=-(1-s) l_{CE}\). Minimizing the loss now _directly_ decreases the confidence. This is a key difference compared to using the multiclass cross-entropy loss.

The impact of the reformulation can be seen for Temperature Scaling, which optimizes a coefficient \(T\) that scales the logits \(z_{k}\). The reformulation generates stronger gradients when the prediction is incorrect:

\[|}{ T}|>|} { T}|s>0.5\] (5)

with \(}{ T}=}( _{k}z_{k}-_{k}z_{k} f_{k})\) and \(}{ T}=}(z_{y}-_{k}z_{k}  f_{k})\). See Appendix D for the mathematical calculations. Because for interesting problems, the confidence verifies \(s>0.5\) most of the time (as shown in Figure 1), our approach strengthens the gradients. The optimization of the temperature \(T\) is more efficient as confident incorrect predictions are more heavily penalized. This effect is not mitigated by the choice of learning rate, which does not vary with \(s\). Applying standard Temperature Scaling usually results in overconfident probabilities, but our approach limits this overconfidence. This is verified experimentally in Table 8, which displays the average confidences for TS without and with TvA.

\(\)_Solution for Issue 1: Use the binary cross-entropy loss resulting from TvA approach._

Regularization of scaling methodsOverfitting of Vector Scaling and Dirichlet Calibration can be reduced with a simple L2 regularization that penalizes the coefficients of the vector \(v\) that are far from the reference value \(1\).

\[l_{reg}(v)=_{i=1}^{L}(v_{i}-1)^{2}\] (6)

This regularization allows these methods to take advantage of their additional expressiveness without being subject to overfitting. The loss for Vector Scaling becomes \(l_{BCE}+ l_{reg}(v)\) where \(\) is a hyperparameter. The loss for Dirichlet Calibration uses additional matrix regularization terms. \(\) is the only additional hyperparameter introduced by our method, and it applies only to Vector Scaling and Dirichlet Calibration.

\(\)_Solution for Issue 2: Use L2 regularization._

### Top-versus-All approach for binary methods

Our TvA approach replaces the One-versus-All approach to apply binary methods to the multiclass setting. TvA transforms the multiclass setting into a _single_ binary problem that uses the binary calibration dataset (3). In this dataset, the proportion of positive labels equals the classifier's accuracy \(a\). The ratio between negative and positive examples is \(=-1\). For a classifier with 80% accuracy on ImageNet and a calibration dataset of 25000 examples, there are 5000 negative and 20000 positive examples (ratio of 1/4). This is still a bit imbalanced but orders of magnitude smaller than the class-wise binary calibration datasets of the One-versus-All approach (ratio of 1/999).

\(\)_Solution for Issue 3: By not dividing the calibration data into class-wise datasets, the TvA approach yields a much better balanced binary calibration problem._

The Top-versus-All approach operates on confidence alone, not the full class probabilities vector. This means that the class prediction is already done, and the ranking of the class probabilities does not change, unlike with One-versus-All. The classifier's prediction and accuracy are unaffected. This scheme allows decoupling accuracy improvements (during training time) and calibration (during post-processing calibration), thus avoiding compromises and reducing development time.

\(\)_Solution for Issue 4: By operating on confidence alone, the Top-versus-All approach does not impact the classifier's prediction or accuracy for binary methods applied to the multiclass scenario._

## 5 Experiments

### Setting

Datasets and modelsFor image classification, we used the datasets _CIFAR-10 (C10)_ and _CIFAR-100 (C100)_ with \(10\) and \(100\) classes respectively, _ImageNet (IN)_ with \(1000\) classes, and _ImageNet-21K (IN21K)_ with \(10450\) classes. For text classification, we used _Amazon Fine Foods (AFF)_ and _DynaSent (DF)_ for sentiment analysis with \(3\) classes, _MNLI_ for natural language inference with \(3\) classes, and _Yahoo Answers (YA)_ for topic classification on \(10\) classes. Experiment results are averaged over five random seeds that randomly split the concatenation of the original validation and test sets into calibration and test sets.

We used the following models for image classification: _ResNet_, _Wide-ResNet-26-10 (WRN)_, _DenseNet-121_, _MobileNetV3 (MN3)_, _ViT_, _ConvNeXt_, _EfficientNet_, _Swin_, and _CLIP_ which matches input images to text descriptions in a shared embedding space, assigning labels based on the highest similarity score. For text classification, we used the PLMs _RoBERTa_ and _T5_.

More details about datasets, calibration sets sizes, and model weights can be seen in Appendix F.

BaselinesOur Top-versus-All (\(_{}\)) reformulation and regularization (\(_{}\)) can be applied to different calibration methods. We have tested the following scaling methods: _Temperature Scaling (TS)_ and _Vector Scaling (VS)_, and _Dirichlet Calibration (DC)_ with the best-performing variant Dir-ODIR, which regularizes off-diagonal and bias coefficients. We also tested the following binary methods: _Histogram Binning (HB)_ using for each case the best-performing variant between equal-mass or equal-size bins, _Isotonic Regression (Iso)_, _Beta Calibration (Beta)_, and _Bayesian Binning into Quantiles (BBQ)_. For comparison, we include methods with state-of-the-art results on problems with many classes: _I-Max_ and _IRM_. See Appendix C for more details on these methods. More details on code implementations can be seen in Appendix F.

Figure 1: Reliability diagrams for ResNet-50 and ViT-B/16 when using Temperature Scaling (TS), Vector Scaling (VS), and Histogram Binning (HB) on ImageNet. The subscript \(_{}\) signifies that the \(\) reformulation was used, and \(_{}\) means our regularization (6) was applied. Red bars show the differences between bin accuracy (blue bar) and accuracy for perfect calibration (dashed red line). As the methods improve the calibration, these differences are reduced and the average confidence (vertical dotted line) will get closer to the global accuracy (vertical dashed line).

[MISSING_PAGE_FAIL:8]

Some methods' current implementations could not handle the large scale of ImageNet-21K, resulting in out-of-memory errors written as "err." in the Table. For I-Max and IRM, this is because they consider the full probability vectors while \(\) efficiently uses data by considering only confidence values. Indeed, \(\) handles this scale without difficulty.

Additional results are included in Appendix H. Tables 5 and 6 contain the full results for ECE, with the standard deviations in Table 7. Table 8 reveals that ImageNet networks are mostly underconfident. This is aligned with  and goes against previous knowledge on overconfidence, which was initially believed to be linked to network size . Table 9 provides the accuracies after calibration. Table 10 exhibits that ECE with equal-mass bins has similar values as standard ECE. Table 11 shows that \(\) mostly lowers the Brier score, except for Iso, which has the lowest score overall.

Calibration methods can also be applied to Large Languages Models (LLMs) using In-Context Learning (ICL) to tackle text classification tasks [77; 20; 25; 78; 1]. The primary goal of these methods is to improve model accuracy. \(\) was not designed for this objective, but it can still be applied on top of an existing method that improves the accuracy. \(\) then lowers the calibration error while keeping the accuracy gain. Results for GPT-J  and Llama-2  are in Table 12.

To summarize the results for practical use, our experiments show that Histogram Binning (within the \(\) or \(Max}\) setting) is the best calibration method overall, providing ECE values mostly below 1%. This is the method we advise using. However, suppose the underlying application requires a confidence with continuous values, e.g., to rank the predictions for selective classification. In that case, we advise using a method that improves the AUROC, shown in Appendix G, such as TS or Iso.

### Solving overfitting with regularization and \(\)

On ImageNet, VS and DC overfit the calibration set, degrading the calibration on the test set. The lower performance of VS relative to TS indicates this overfitting. As visualized in Figure 2, combining the binary cross-entropy loss used in the \(\) reformulation and an additional regularization term prevents overfitting. We fixed the value \(=0.01\) as it works well across models. Initializing the vector coefficients to \(\) with \(T\) obtained by \(_{}\) helps further improve performance.

### Influence of the calibration set size

The size of the calibration set influences the performance of the different methods, as seen in Figure 3. TS and \(_{}\) do not benefit from more data due to their low expressiveness. VS does not improve the ECE because of the overfitting problem. In contrast, \(_{}\) benefits from more calibration data. With enough data (\(\) 15000), it outperforms \(_{}\). Binary methods using the standard One-versus-All approach have poor performance and need a large amount of data to be competitive. Using \(\), they get excellent performance with little data.

Figure 3: Influence of the calibration set size for ResNet-101 on ImageNet. Binary methods at the top and scaling methods at the bottom.

Figure 2: Test ECE evolution during training with ResNet-50 on ImageNet. The combination of regularization and \(\) prevents overfitting of Vector Scaling. Temperature Scaling with \(\) is shown for reference.

## 6 Limitations

Our approach tackles confidence calibration and is unlikely to improve performance for stronger notions of calibration, such as class-wise calibration. However, confidence calibration is useful for many practical cases, such as selective classification , out-of-distribution detection , or active learning . Also, calibration improvements are less significant for problems with few classes (\( 10\)) than for problems with many classes, but our approach still provides the best results.

## 7 Conclusion

Reducing the miscalibration of neural networks is essential to improve trust in their predictions. This can be done after the model training with an optimization using calibration data. However, many current calibration methods do not scale well to complex datasets: binary methods under the One-versus-All setting do not have enough per-class calibration data, and scaling methods are inefficient. We demonstrate that reformulating the confidence calibration of multiclass classifiers as a single binary problem significantly improves the performance of baseline calibration techniques. The competitiveness of scaling methods is increased, and binary methods use per-class calibration data more efficiently without altering the model's accuracy. In short, our TvA reformulation enhances many existing calibration methods with little to no change in their algorithm. Extensive experiments with state-of-the-art image classification models on complex datasets and with text classification demonstrate our approach's scalability and generality.