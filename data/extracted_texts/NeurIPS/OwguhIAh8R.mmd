# HGDL: Heterogeneous Graph Label Distribution Learning

Yufei Jin\({}^{}\), Heng Lian\({}^{}\), Yi He\({}^{}\), Xingquan Zhu\({}^{}\)

\({}^{}\)Dept. of Elec. Eng. & Computer Sci., Florida Atlantic University, Boca Raton, FL 33431, USA

\({}^{}\)Dept. of Data Science, William & Mary, Williamsburg, VA 23185, USA

yjin2021@fau.edu; hlian01@wm.edu; yihe@wm.edu; xzhu3@fau.edu

Corresponding author

###### Abstract

Label Distribution Learning (LDL) has been extensively studied in IID data applications such as computer vision, thanks to its more generic setting over single-label and multi-label classification. This paper advances LDL into graph domains and aims to tackle a novel and fundamental _heterogeneous graph label distribution learning_ (HGDL) problem. We argue that the graph heterogeneity reflected on node types, node attributes, and neighborhood structures can impose significant challenges for generalizing LDL onto graphs. To address the challenges, we propose a new learning framework with two key components: 1) proactive graph topology homogenization, and 2) topology and content consistency-aware graph transformer. Specifically, the former learns optimal information aggregation between meta-paths, so that the node heterogeneity can be proactively addressed prior to the succeeding embedding learning; the latter leverages an attention mechanism to learn consistency between meta-path and node attributes, allowing network topology and nodal attributes to be equally emphasized during the label distribution learning. By using KL-divergence and additional constraints, HGDL delivers an end-to-end solution for learning and predicting label distribution for nodes. Both theoretical and empirical studies substantiate the effectiveness of our HGDL approach. Our code and datasets are available at https://github.com/Listener-Watcher/HGDL.

## 1 Introduction

Definite supervision signals are often postulated in learning settings [3; 4]; yet, data generated from the real world tend to present inherent ambiguity, imposing challenges on assertive classifiers that predict instances into single or multiple classes. Label Distribution Learning (LDL) [5; 6; 7; 8; 9] has emerged to navigate label ambiguity by pursuing a mapping from instances to their class distributions. Each distribution quantifies the _descriptive degrees_ of various classes given a specific instance.

However, the existing LDL studies mainly [10; 6; 7; 11] focus on independent and identically distributed (IID) data, such as images or texts, which do not generalize well on _graphs_. In fact, the topological structure underlying instances may provide invaluable information for label distribution learning. For example, in the task of urban planning, recent learning models have been employed to predict the point of interests (POIs) of local regions [12; 13; 14; 15]. LDL can further extend this task by providing the regional distributions over all POIs, which lends a finer-granular delineation of urban regional functionality instead of single- or multi-class classification. To wit, for a region that mixes four POIs (classes): housing, healthcare, education and worship, unlike other models assertively classify it into one or multiple POI(s), LDL model can provide insights of the functional degrees of all four POIs in this region, as shown in Figure 1. Nevertheless, existing LDL studies overlook theurban topology, which can be rendered from, e.g., the taxi services across regions , missing out critical city traffic patterns that are highly correlated with regional functionalities. For instance, the regions with balanced POI distributions (e.g., R\({}_{2}\) and R\({}_{3}\)) are less likely to form connections with other nodes compared to regions heavily skewed towards a single class (e.g., R\({}_{4}\)), as their residents enjoy fewer needs to travel to other regions for services such as education and healthcare.

In this paper, we aim to enable and generalize the label distribution learning paradigm in networked data. Two technical challenges confront our study. First, real-world graphs are mostly heterogeneous, comprising diverse types of nodes for better expressiveness. Graph heterogeneity complicates the message-passing between nodes of a specific type (e.g., residence), as the label distributions of those nodes are influenced by their neighboring nodes that may vary in terms of types, content, and topological features. Simply leveraging node embeddings generated from message-passing for LDL will thus not work well [16; 17]. To aid, although meta-path aggregation  is seemingly viable, it necessitates extensive domain knowledge and expertise to craft meta-paths for each node type with respect to their label distributions; given the combinatorial number of possible meta-paths in large heterogeneous graphs, searching for the optimal meta-path for LDL is costly, laborious, and time-demanding.

Second, graph topology and nodal features may suggest inconsistent label distributions, where nodes sharing similar contents are positioned far apart on the graph topology. The inconsistency is furthered in heterogeneous graphs, where nodes of the same type often connect through other intermediary types, resulting in substantial topological distances between them. Unlike traditional LDL that focuses on instance vectors only, an effective LDL model on graphs require harmonizing nodal contents with topological structures for a unified representation. The impact of distantly positioned nodes within a graph is substantially diminished, consequently steering the LDL model to prioritize individual nodal vectors, leading to compromised node representations in which the informative patterns embedded in their neighborhood structures are overlooked. Such patterns, which may significantly enhance label distribution predictions as illustrated in Figure 1, are neglected, undermining the LDL model effectiveness.

To overcome the challenges, we propose a new learning framework, coined _Heterogeneous Graph label Distribution Learning_ (HGDL). Specifically, to tame the graph heterogeneity, HGDL learns the optimal graph topology of the target nodes from multiple homogeneous structures searched with various meta-path schemes through a tailored attention mechanism. The node embeddings are then generated by harmonizing the nodal features and the learned meta-path graph using a transformer architecture. A joint optimization objective is crafted based on the distance between true and predicted label distributions of the target nodes from their resultant embeddings, which unifies the learning of meta-path graph topology and the feature-topology harmonization function in an end-to-end fashion.

A key innovation of HGDL is that it changes existing heterogeneous graph learning paradigm from _reactive_ (meaning that aggregation of different meta-paths are done _after_ embedding learning from individual meta-path), to be _proactive_ (meaning that aggregation are done _before_ embedding learning). Combined with attention and transformer mechanisms to adjust individual meta-paths' interplay, and align with nodal features, HGDL deliver significantly better performance over alternatives. Our theoretical analysis assures that HGDL outperforms that of using an arbitrary meta-path graph, and HGDL's topology and feature consistency learning sparsifies network connectivity, intermediately encouraging tightens the error-bound, resulting in better model generalization.

**Specific contributions of this paper are as follows:**

Figure 1: Motivating example of HGDL study, where each node is a local urban region  and edges represent taxi services  commuting between regions. Heterogeneous node types indicate disparate land use, including residence (R), service (S), leisure (L), and transit (T), among which R nodes are of our interest. Colored R nodes are with ground truth, which delineate their distributions over multiple point-of-interests (POIs), and each POI is deemed as a class/label. Our HGDL problem is to predict the label distribution of uncolored R nodes, enabling a precise delineation of regional urban functionality.

1. This study pioneers the exploration of LDL problem in heterogeneous graphs. The learning problem enjoys practical implications such as for urban functionality delineation (presented in Sec 6.1) and, to our knowledge, has not yet been explored by any contemporary research.
2. We propose an end-to-end HGDL learning approach to jointly learn an optimal meta-path graph topology and align it with nodal features for consistent message-passing. Our approach is surprisingly simple yet effective, with its performance evidenced by theoretical underpinnings. Our approach and its analysis are presented in Sections 4 and 5, respectively.
3. Empirical study has been carried out over five graph datasets that span domains of bio-medicine, scholarly network, business network, and urban planning. Experimental results substantiate the effectiveness of our approach over rival models, documented in Section 6.

## 2 Related Work

Label Distribution Learning (LDL)strives to learn a mapping from input to a distribution that profiles the descriptive degrees of classes associated with it [5; 10; 6; 7; 11]. Existing LDL methods fall into three categories, namely, problem transformation (PT), algorithm adaption (AA), and specialized algorithm (SA). PT methods transform LDL as multiple single-label learning tasks, using with label probabilities , and AA approaches revise mainstream learning algorithm to fit the LDL loss. SA algorithms are most commonly used because LDL learning is driven by new algorithm designs. Label correlation has been found to benefit the label distribution learning, where approaches were proposed to encode label correlation to a distance to measure the similarity of any two labels . Later, low-rank approximation is used to construct label correlation matrix to capture the global label correlations  Instead of exploring common features for all labels, label-specific features  for each label are used to enhance the LDL model. Exploring feature-label and label-label correlation  has recently been studied in generalizable label distribution learning for cross domain learning. A Gaussian label distribution learning method  employs a parametric model underpinned by an optimization strategy assessing KL-divergence distance between Gaussian distributions, followed by a regression loss to normalize the KL-divergence distance. Noticing the difficulty to obtain ground-truth label distributions, Label Enhancement  is commonly used to recover label distributions from logical labels. Our research further push LDL to be generalized onto heterogeneous graphs, which have been overlooked by existing research. Although a recent study  explored using LDL in topological spaces, it focused on homogeneous graphs only and cannot work in the setting of more than one node type. Thus, the studied problem in  and its challenges differ from ours.

Heterogeneous Graph Neural Networkshave drawn extensive attention in graph learning [16; 17; 22; 18; 23; 24], because the graph heterogeneity imposes considerable challenge to model the interplay among various node types, features, labels, and network topology. Using meta-path to aggregate information from different types of nodes/edges is a common approach for heterogeneous graph learning. HetGNN [17; 18] designs graph neural networks to encode features for each type of neighbors and then aggregates neighbors' representation with respect to different types. This provides a way for GNN to deal with heterogeneous graph structures and node attributes. HAN  introduces attention mechanisms to heterogeneous graph learning, where attentions are applied to embedding features learned from homogeneous networks, each created from a meta-path. By doing so, attentions serve as a weighting mechanism automatically determining the importance of each meta-path for learning. Using transformers for heterogeneous networks has also been investigated recently. For example. HGT  designs node- and edge-type dependent parameters to characterize the heterogeneous attention over each edge, allowing this method to learn representations for different types of nodes and edges. SeHGNN  proposes a transformer based semantic fusion module, allowing feature fusion from different meta-paths. Our research is fundamentally different from existing work in two aspects: 1) we study LDL learning for heterogeneous networks, and 2) we propose a new way to aggregate and align information for heterogeneous network.

## 3 Preliminaries

Notations.A heterogeneous graph is denoted by \(G=\{V,E,X,Y\}\) associated with a node type mapping \(:\ V^{v}\) and an edge type mapping \(:\ E^{e}\), with \(^{v}\) and \(^{e}\) the predefined and finite sets of nodes and edges, respectively, and \(|^{v}| 2\). Denote \(t_{{}_{t}}^{v}\) as the node type of our interest, and suppose in total \(n\) nodes are of this type. Without loss of generality, we have \((v_{1})==(v_{n})=t_{{}_{t}}\), and \(V_{{}_{t}}=\{v_{1},,v_{n}\} V\). We deem these \(n\) nodes as our _target nodes_, using a feature matrix \(X^{n m}\) to describe their nodal contents, where each node containsan \(m\)-dimensional feature vector. A meta-path \(\) is defined as a relational sequence in form of \(t_{1}}t_{2}}t_{i}}t_ {i+1} t_{l}\) (abbreviated as \(=(t_{1}t_{2} t_{i}t_{i+1} t_{l})\), where \((t_{i}t_{i+1})^{e}\) describes the composite relation between a pair of node types. By defining a meta-path \(\) with same first and last node type as the target node type, _i.e._\(t_{1}=t_{l}=t_{}\), we can use \(\) to convert a heterogeneous network as a meta-path graph concerning only the target node type, which shall be discussed later in Section 4.1.

Problem Statement.In our HGDL problem, the goal is to learn a predictive mapping \(:\ (G,X) Y\), where \(Y^{q}\) is a distribution of descriptive labels over \(q\) classes. Let \(y_{i,j}\) be the probability that the node \(v_{i}\) belongs to the \(j\)-th class, we have \(_{j=1}^{q}y_{i,j}=1\). In this work, we follow a transductive learning regime  to allow the ground-truth label distributions known for a subset of target nodes \(V_{tr} V_{t_{i}}\) during training. Our learned mapping \(\) is expected to generalize well so can make accurate prediction on the remaining target nodes \(V_{t_{}} V_{tr}\).

## 4 HGDL: The Proposed Approach

Overview.The proposed HGDL approach comprises three key components, as illustrated in Figure 2. First, for the target nodes belonging to the node type \(t_{}\) of interest, HGDL generates multiple homogeneous meta-path graphs based on their original locations on the heterogeneous graph through meta-paths; the optimal graph topology of this node type is then learned from the homogeneous graphs via attention mechanism (_Sec 4.1_). Second, HGDL learns the embeddings of the target nodes by harmonizing the information sourced from their feature space and the learned optimal topology using a transformer-like neural architecture (_Sec 4.2_). Third, HGDL minimizes the distance between the predicted and ground-truth label distributions based on the learned node embeddings. We tailor an objective function to unify the three components into one end-to-end optimization problem, in which the optimal graph topology, the harmonization function of the feature and topological information, and the target node label distribution are jointly learned (_Sec 4.3_).

### Optimal Graph Topology Homogenization

For a heterogenous graph, by leveraging meta-path idea, multiple different meta-path homogeneous adjacency matrix can be obtained and they can be treated as multiple sources. Graph learning is about exchanging and updating information from neighbor nodes. A proper neighbor set is therefore important for a target node to learn correct distribution. Given multiple sources, each node will have

Figure 2: The proposed HGDL framework. Using \(k\) meta-paths, the heterogeneous network in 1 is converted to \(k\) homogeneous meta-path graphs in 2. Topology homogenization in 3 proactively aggregates all \(k\) meta-path graphs, through learnable weight matrix \(W_{0}^{i}^{n f}\) for each graph, and finally obtain attention \(^{n k}\) across all graphs. Topology and feature consistency-aware graph transformer in 4 harmonizes the local and global consistencies. The objective function in 5 unifies loss and regularization terms to guide nodal label distribution learning.

multiple neighbor sets to choose from for updating. Traditionally, embeddings are learned for all the neighbor sets, and then aggregation over embeddings is learned. Semantics over embeddings are hard to interpret and learn compared with directly learned from different neighbor sets.

To generate a meta-path graph from the original heterogeneous graph, interactions between the meta-path and the heterogeneous graph path are used. Two nodes \(v_{i}\) and \(v_{j}\) are connected in the meta-path graph, if there exists a path connecting them in the heterogeneous graph, and the path follows the meta-path. Given a meta path \(=(t_{1} t_{i}t_{i+1} t_{i})\), we say that a path \(p=(v_{1},,v_{i},v_{i+1},,v_{l})\) in graph \(G\) follows the meta-path \(\), if \( i,(v_{i})=t_{i}\). Take graph in Fig. 1 as an example. Given meta-path \(=(r\ s\ r)\), which defines node type \(t_{1}=r,t_{2}=s,t_{3}=r\). Path \(p=(r_{2},s_{1},r_{3})\) in the heterogeneous graph follows \(\) because all nodes in the path \(p\) satisfy \((r_{2})=t_{1}=r,(s_{1})=t_{2}=s,(r_{3})=t_{3}=r\). Because path \(p=(r_{2},s_{1},r_{3})\) follows the meta-path \(\), an edge is used to connect \(r_{2}\) and \(r_{3}\) in the homogeneous meta-path graph constructed from \(\). Indeed, each meta-path defines a specific way of information propagation in a heterogeneous network, with resulted meta-path graph capturing unique relationships between target nodes. While defining a single meta-path is relatively easy, there often exists many meta-paths; aggregating a variety of meta-path graphs to support the downstream learning task is non-trivial.

After searching the meta-paths connecting the nodes of target type \(t_{i}\), we generate a set of graphs \(=\{A_{1}, A_{k}\}\), in which each adjacency \(A_{i}\{0,1\}^{n n}\) captures the topological structure of the \(i\)-th meta-path-based homogeneous graph. Denoted by \(A_{i}[p,q]=1\) means that two target nodes \(v_{p}\) and \(v_{q}\), with \((v_{p})=(v_{q})=t_{}\), are connected by a meta-path; otherwise, \(A_{i}[p,q]=0\). Unlike existing studies  that yield target node embeddings through _reactive_ meta-path aggregation, where they aggregate local neighborhood information for each \(A_{i}\) to capture \(k\) separate meta-path topologies, our HGDL learns the optimal graph topology from \(\) in a _proactive_ fashion. Intuitively, HGDL learns node-level attention scores for various homogeneous graphs \(A_{i}\), to respect the fact that the neighboring nodes may pass messages with varying importance levels in local neighborhoods, while the meta-paths walking across nodes of types other than the target \(t_{}\). Revisit the motivating example demonstrated in Fig 1 where the residence nodes are deemed as the target, the meta-path linking through the service nodes dominates, as the residence nodes are more likely to be linked through service nodes instead of Transit and Leisure nodes. Specifically, the attention scores for the nodes in every \(A_{i}\) are learned in a GAT regime , defined as:

\[=_{A_{i}}\{(A_{i})W_{0}^{i}\}W_{0}^{{}^{}},=_{i=1}^{k} [:,i](A_{i}),\] (1)

where LapNorm(\(A_{i}\))= \(D_{i}^{-}(A_{i}+I)D_{i}^{-}^{n n}\) denotes Laplacian normalization of \(A_{i}\), with \(D_{i}\) being \(A_{i}\)'s degree matrix and \(I\) is an identity matrix. This term mitigates the imbalanced degree distribution of the meta-path graphs. Denoted by \(W_{0}^{i}\) and \(W_{0}^{{}^{}}\) are the learnable GAT parameters, where \(W_{0}^{i}^{n f}\) maps the meta-path topology of \(A_{i}\) onto an \(f\)-dimensional semantic space. The operator \(\|_{A_{i}}\{\}\) concatenates all \(k\) resultant node embedding matrices from meta-path graphs \(\), thereby producing an \(^{n k f}\) lookup matrix, where each node is associated with a \(k f\)-dimensional embedding representation, and each latent \(f\)-dimension captures the local neighborhood structure of this node. Then, \(W_{0}^{{}^{}}^{k f k}\) summarize the \(f\)-dimensional latent space into one coefficient through convex combination, resulting in attention logits, which are fed into \(()=()/_{i}()\) to yield the attention matrix \(=^{n k}\). Take the \(i\)-th column vector of \(\), denoted by \([:,i]^{n}\), we have the attention scores of \(n\) target nodes for message-passing in the \(i\)-th meta-path graph \(A_{i}\). We thus can deem \(\) in Eq. (1) as the learned optimal graph topology, which is an element-wise linear combination of \(k\) meta-path topologies with the attention scores broadcast onto all \(n\) nodes. Note, \(\) is asymmetric, namely \([p,q][q,p],\  p q\). This is because that the attention score of information aggregation from node \(v_{p}\) to \(v_{q}\) may be different from that from \(v_{q}\) to \(v_{p}\), as their respective local neighborhood topologies naturally differ.

### Local Topology and Global Feature Consistency-Aware Graph Transformer

After obtaining the optimal topology \(\) from all meta-path graphs \(A_{i}\), the next question is how to harmonize it with the feature information to better the target node embeddings. The benefit of such harmonization is evident. Revisiting the urban network in Fig 1, we can envision that a pair of residence nodes tend to be associated with similar embedding vectors because they enjoy two typesof consistencies: i) _local neighborhood topology_: their residents tend to travel to similar functional regions for leisure or service purposes, and ii) _global feature space_: they share similar contents such as house types and number of residing families. These local and global consistencies complement with each other, as the residence nodes having similar contents can be topologically faraway from each other on the urban network, and vice versa.

To harmonize the local and global consistencies, we are inspired by the recent graph transformers  and observe that the feature attention suggests a global adjacency matrix, which can be incorporated into the message-passing process. We define the graph transformer block as follows.

\[Z=(XW_{1}),_{2}=(ZQ)(ZK)^{} ,\] (2)

where \(Z^{n h}\) is the node embeddings learned from the optimal graph topology \(\) through local information aggregation, parameterized by \(W_{1}^{m h}\). Denoted by \(_{2}^{n n}\) is the normalized feature attention adjacency, where \(Q\) and \(K^{h h}\) map the embedding matrix \(Z\) onto the latent query and key spaces, respectively, such that \((ZQ)(ZK)^{}\) calculates an \(n n\) node-level attention matrix with respect to the feature space information. Instead of normalizing the attention score by the hidden dimension \(h\), we penalize the feature attention adjacency through an element-wise production \(\) with the optimal meta-path topology \(\). The intuition behind Eq. (2) is that, for each target node, it aggregates information from those neighboring nodes only if their meta-path topology and feature space are both with high attention scores. In addition, Eq. (2) functions similarly to the edge dropout ; in lieu of randomly removing edges, we enforce a neighbor-set intersection, where the information is only propagated from the neighbors on which the feature space and meta-path topology both agree. Such an intersection sparsity thus lowers the degree of the resultant attention adjacency, thereby uplifting the learning efficacy, which will be substantiated later in Sec 5. Finally, denoted by \(H=(_{2}ZW_{2})^{n h}\) are the resultant node embeddings, capturing both local topology and global feature consistencies, which is parameterized by weight \(W_{2}^{h h}\).

### An End-to-End Hgdl Objective Function

Based on the resultant target node embeddings \(H\), we can predict their label distributions as \(=(_{2}HW_{3})^{n q}\), where \(_{i}=\{_{i,j}\}_{j=1}^{q}\) is the predicted label distribution of node \(v_{i}\), among which \(_{i,j}\) denotes its predicted probability of belonging to the class \(j\). The unified objective of our HGDL framework is defined as follows.

\[_{\{W_{i}^{j}\}_{i=1}^{k},W_{o}^{},W_{1},W_{2},W_{3},K, Q}_{}=D_{}(Y\|)-,\] \[D_{}(Y\|)=_{i=1}^{n}_{j=1}^{q}y_{i,j} }{_{i,j}},=_{i=1}^{n}D_{ }([i,:] U[1,k]),\] (3)

where the KL-divergence \(D_{}(Y\|)\) gauges the discrepancy between the predicted and groundtruth label distributions of the target nodes . The regularization term \(\) gauges the distance between the attention scores of the \(i\)-th node across \(k\) meta-path typologies (denoted by \([i,:]\)) and a uniform distribution \(U[1,k]\). We note the minus sign before \(\), thus minimizing this term encourages a larger KL-distance, thereby avoiding the trivial uniform attention distribution (meaning that for each node, the learned attention weights from different meta-paths are encouraged to be as different as possible). \(\) is a tuned parameter to balance the two terms.

## 5 Analysis

We follow the PAC-Bayes regime to analyze the theoretical performance of our HGDL algorithm by deriving its generalization error bound. We proceed analysis based on the meta-path graph adjacency matrices \(=\{A_{1}, A_{k}\}\), which are searched from the heterogeneous graph \(G\). Throughout the analysis, we assume the nodal feature representations to be residing in an \(_{2}\)-ball of radius \(B\). We argue this a mild assumption, because in implementation we can leverage the batch-norm layers to normalize the resultant node embeddings, such that \(\|_{i}^{j}\|_{2} B\), where \(_{i}^{j}\) denotes the \(i\)-th node's embedding resulted from the \(j\)-th hidden layer.

Let \(L_{}()\) and \(L_{(X,)}()\) denote the _generalization risk_ over a graph distribution \(\) and the _empirical risk_ on the target node samples and the learned meta-path topology \((X,)\), respectively, where \((X,) G}}{{}}\). We can define:

\[L_{}()=_{(X,)} _{y_{i} Y}(X,)[i],y_{i} ,\] \[L_{(X,)}()=_{i=1}^{n} (X,)[i],y_{i},\]

where \((,)\) is a convex distance metric between two distributions that follows \(|(u,p)-(u,q)|(+1)\|p-q\|_{2},\  u,p,q^{m}\). Denoted by \((X,)[i]^{q}\) and \(y_{i}^{q}\) the predicted and ground-truth label distribution of the \(i\)-th target node, respectively. Implementing KL-divergence, we have \((X,)[i],y_{i}=_{j=1}^{q}(X,)[i,j]((X,)[i,j]/y_{i,j})\), where the predicted probability that node \(i\) belongs to the \(j\)-th class is denoted by \((X,)[i,j]\). By analyzing the performance of the learned meta-path graph topology \(\), we find that:

**Theorem 1**.: _Let \([L_{(X,A_{i})}()]\) be the empirical risk of using the \(i\)-th meta-path graph \(A_{i}\) for label distribution prediction. With the SGD step-size \(\), we have_

\[L_{(X,)}()_{A_{i}}[L_{(X,A_{i})} ()]++.\]

**Remark 1**.: Theorem 1 indicates that the empirical risk of HGDL is no larger than the minimum empirical risk incurred by training label distribution learner on the optimal meta-path graph, as the error bound on the RHS reduces to \((1/n)\) with constant \(k\) and \(\). With Stochastic Gradient Descent (SGD) optimizer, larger number of target nodes \(n\) will lead to more training updates over them, diminishing the \((1/n)\) bound faster. This finding substantiates the tightness of our meta-path learning strategy for the optimal graph topology \(\).

Due to page limits, we defer the proof of Theorem 1 and the rest analysis to the Supplement. We then analyze the generalization error bound of HGDL and find that:

**Theorem 2**.: _Let \(:^{q}\) be an \(l\)-layer message-passing neural network with maximum hidden dimension \(k\), of which the \(i\)-th layer is parameterized by \(W_{i}\). Then for any \(,,B>0\) and \(l>1\), with probability at least \(1-\) we have_

\[L_{}()-L_{(X,)}( )+)q}{}_{i[n],j[l]}\| _{i}^{l}\|_{2}\] \[+3b}+d^{l-1}l^{2}k(lk)(W_{i})+}{ ^{2}n}},\]

_where \((W_{i})=_{i=1}^{l}\|W_{i}\|_{2}^{2}_{i=1}^{l} \|W_{i}\|_{F}^{2}/\|W_{i}\|_{2}^{2}\) bounds the hypothesis space and \(b\) is a constant._

Remark 2**.: We remark several key observations from Theorems 1 and 2. First, the generalization capability of the algorithm is negatively impacted by a higher dimensional label space \(q\). Second, the robustness of HGDL decreases with larger \(B\) values, which gauges the magnitude of perturbations thus the inherent high data variance. Third, as the graph neural network architecture becomes deeper (larger \(l\)) or wider (larger \(k\)), the generalization risk increases, suggesting the potential risk of model overfitting. Forth, with larger \(n\), the generalization error bound diminishes, which indicates that the meta-path topology can be better delineated with an increased number of target nodes on the graph.

Remark 3**.: By combining Theorems 1 and 2, we observe that the generalization error bound of HGDL using \(\) outperforms that of using an arbitrary meta-path graph \(A_{i}\). Further, it is easy to verify that the maximum degree of \(\), denoted by \(d\), is smaller than that of \(A_{i}\), denoted by \(d_{i}\), _i.e.,_\(d d_{i}\). \( i[k]\). This rationalizes our graph transformer design in Sec 4.2, where the enforced topology and feature consistency in Eq. (2) sparsifies network connectivity, thereby intermediately encourages better model generalization.

## 6 Experiments

### Experiment Setup

Benchmark DatasetsTo our best knowledge, no heterogeneous graph dataset with _ground-true label distributions_ currently exists. To level the comparison study, we prepare five datasets with ground-truth node label distributions using existing heterogeneous graphs, including DBLP , ACM , YELP, DRUG , and URBAN . Table 1 summarizes the data statistics. A detailed description on the dataset creation and preprocessing, as well as their domain and label semantic meanings, has been deferred to the Supplement B due to space limitation.

Compared ModelsIn total six competitors are identified for comparative study. As no model directly resolving the HGDL problem exists, we employ the state-of-the-art heterogeneous graph neural networks and integrate them KL-divergence loss to learn label distributions of nodes. They include: 1) GCN\({}_{}\): A baseline that uses graph constructed from each meta-path to train a vanilla GCN , using KL-divergence as loss function, and reports the best meta-path result; 2) HAN\({}_{}\): This baseline uses HAN  to integrate embedding from different meta-paths; and 3) SeHGNN\({}_{}\): This baseline uses SeHGNN , a transformer based approach, to aggregate meta-paths embedding with KL-divergence loss function. For ablation study, we further include three variants reduced from our proposed HGDL method, which include: 4) HGDL\({}_{-}\): it removes HGDL's topology homogenization (Sec 4.1), which learns embedding from each meta-path graph and reports the best meta-path result; 5) HGDL\({}_{-}\): it uses GCN instead of the transformer (Sec 4.2) to learn embedding to validate HGDL's transformer for embedding learning; and 6) HGDL\({}_{}\): it replaces HGDL's topology and feature consistence-aware graph transformer (Sec. 4.2) by using a random edge dropout method .

Evaluation MetricsTo measure the discrepancy between two distributions, i.e., the predicted and true label distributions of target nodes, we identify six metrics: Cosine Distance (COD), Canberra Distance (CAD), Chebyshev Distance (CHD), Clark Distance (CLD); Intersection Score (IND), and Kullback-Leibler Divergence (KL). Their definitions and calculations are deferred to Supplement B.

   Dataset & \# node type & \# nodes & \# edges & \# features & \# labels \\  DRUG & 4 & 40,786 & 1,737,890 & 191 & 28 \\ ACM & 5 & 20,200 & 104,976 & 1,903 & 14 \\ DBLP & 4 & 27,325 & 148,246 & 8,920 & 4 \\ YELP & 4 & 8,052,542 & 7,905,197 & 19 & 9 \\ URBAN & 4 & 1,434 & 42,857 & 155 & 10 \\   

Table 1: Summary of dataset statistics.

   Dataset & Model & COID & CADL & CHD1 & CHD1 & SDDT & KL1 & WinfEnd-core \\   & GCN\({}_{}\) & 0.210\({}_{-0.05}\) & 0.209\({}_{-1.00}\) & 0.245\({}_{-0.11}\) & 1.361\({}_{-0.10}\) & 0.057\({}_{-0.02}\) & 0.084\({}_{-0.15}\) & 42/0 \\  & GCN\({}_{}\) & 0.220\({}_{-0.05}\) & 0.209\({}_{-1.00}\) & 0.264\({}_{-0.11}\) & 1.263\({}_{-0.10}\) & 0.051\({}_{-0.02}\) & 0.059\({}_{-0.05}\) & 64/00 \\  & GCN\({}_{}\) & 0.299\({}_{-0.05}\) & 0.107\({}_{-0.10}\) & 0.264\({}_{-0.11}\) & 2.166\({}_{-0.10}\) & 0.051\({}_{-0.02}\) & 0.059\({}_{-0.05}\) & 64/00 \\  & HGDL\({}_{}\) & 0.299\({}_{-0.05}\) & 0.107\({}_{-0.10}\) & 0.264\({}_{-0.10}\) & 2.166\({}_{-0.10}\) & 0.051\({}_{-0.02}\) & 0.059\({}_{-0.05}\) & 64/00 \\  & HGDL\({}_{}\) & **0.199** & **0.175** & **0.217** & **0.165** & **0.164** & **0.165** & **0.164** \\  & HGDL\({}_{}\) & 0.199\({}_{-0.10}\) & 0.571\({}_{-0.10}\) & 0.259\({}_{-0.10}\) & 2.204\({}_{-0.10}\) & 0.057\({}_{-0.02}\) & 0.052\({}_{-0.10}\) & 44/20 \\  & HGDL\({}_{}\) & 0.212\({}_{-0.10}\) & 0.510\({}_{-0.02}\) & 0.264\({}_{-0.10}\) & 2.029\({}_{-0.10}\) & 0.051\({}_{-0.02}\) & 0.054\({}_{-0.10}\) & 44/20 \\  & HGDL\({}_{}\) & 0.204\({}_{-0.05}\) & 0.962\({}_{-0.10}\) & 0.229\({}_{-0.10}\) & 2.069\({}_{-0.10}\) & 0.051\({}_{-0.10}\) & 0.057\({}_{-0.10}\) & 44/20 \\   & GCN\({}_{}\) & 0.217\({}_{-0.05}\) & 1310

### Results

Table 2 summarizes results of all methods. Overall, our HGDL wins in 99 out of 180 settings, among which on average 20 out of 30 settings excel in COD, CHD, IND, and KL metrics, 11 out of 30 in CAD, and 8 out of 30 in CLD. On DURG, ACM, DBLP, and URBAN datasets, HGDL outperforms its competitors in 83% settings in COD, CHD, IND, and KL metrics 46% in CAD, and 20% in CLD. Beyond its overall better comparative performance, we make the following observation on HGDL. First, \(_{}\) and \(_{}\) achieve better performance on YELP and DBLP dataset for CAD and CLD metrics, but not on the other datasets and metrics. This shows that existing meta-path based methods cannot learn distribution prediction well. In general, these models show similar performance on YELP dataset. We hypothesize that this is due to the lack of rich feature information on YELP, of which the dimension of nodal features is 19 which is minimum across all datasets. Second, HGDL achieves the best results in KL-divergence by a large margin across all settings. On average, HGDL have a 15% improvement compared with the second best result across all datasets in KL-divergence. Given that KL-divergence is the loss objective in our framework, we extrapolate that HGDL converges well in terms of minimizing the distribution distance. Same observation can

Figure 4: KL and CLD tradeoff function example. The estimated probability distribution is [\(x_{1}\), \(x_{2}\), 0.9] and true probability distribution is [0.05,0.05,0.9], with \(x_{1}+x_{2}=0.1\). Horizontal axis is the \(x_{1}\) value and vertical axis is the loss for both CLD and KL divergence. Green dashed lines cover the tradeoff region where the CLD loss monotonically increases and KL-divergence decreases.

Figure 3: Comparisons between HGDL _vs._ results from a single meta-path (CAD and CLD are calculated in natural log for better visualization) for five datasets.

be drawn from the validation loss curve as shown in Supplement C Figures 5, 6, and 7. In addition, on metrics being strongly related to KL divergence including COD, CHD, and IND, our HGDL also enjoys significant performance improvement over other models. Among the metrics, CLD metrics shows a different patterns in terms of KL divergence, we show in Figure 4 that CLD and KL has a tradeoff region in small probability distribution and therefore caused such difference.

Third, the **ablation study** between HGDL and its variants, i.e., HGDL-transformer, HGDL-TH and HGDLED, demonstrate clear benefits of topology homogenization and consistency-aware graph transformer in aggregating meta-paths and nodal features for LDL learning for heterogeneous graphs (more results are deferred to the Section E.1 in Supplement C; there, we observe that HGDLed has no improvement in KL-divergence with different edge drop rates compared to HGDL-transformer, which is the model with 0 edge drop rate). We observe in Table 2 that HGDL-transformer shows comparable performance on ACM and YELP by tying HGDL in five and six metrics, respectively; however, HGDL outperforms it in all settings in other three datasets. Likewise, HGDL-TH ties HGDL across all metrics in YELP but is inferior to HGDL in all settings in other four datasets. HGDLED ties HGDL in six and five settings on YELP and URBAN, respectively, but is outperformed by HGDL for all other three datasets in all settings. The robust performance of HGDL can be attributed to two aspects. On the one hand, the improved results over those ablation variants suggest that our devised model components for proactive meta-path learning and attention modeling are indispensable. On other other hand, it substantiates the usefulness of our design that lets HGDL learn semantic fusion before the embedding learning. This end-to-end learning design provides a larger search space for embedding learning to find optimal solutions, whereas other methods that learn embedding and perform fusion in two independent stages may result in suboptimal node embeddings thus inferior LDL performance.

Fourth, even though the optimal meta-path choice may vary across different metrics and datasets, our HGDL that proactively learns to aggregate multiple meta-path graphs leads to the best performance in most cases. Figure 3 illustrates the performance from single meta-path graph and we can observe that our method outperforms the single best path results in all five datasets, with a larger improvement when the meta-path results are close (indicating each meta-path has similar information, _e.g._, ACM dataset in Figure 3 (b)) and a smaller improvement when one meta-path is significantly inferior to others (_e.g._, DBLP dataset Figure 3 (c) where \(p_{1}\) outperforms \(p_{2}\) with a large margin). These results validate the tightness of Theorem 1 by demonstrating the optimality of the learned meta-path graph in our HGDL method.

### Scalability Analysis

Denote the total number of nodes, hidden dimension size, and number of meta-path by \(n\), \(f\), and \(k\), respectively. The number of learnable parameters is \((n)\) for graph topology homogenization, because HGDL requires learning an adjacency matrix from all meta-path, which involves \(knf+k^{2}f\) training parameters (_i.e._\((n)\) complexity). Inducing adjacency matrix from features, _i.e._ the 2nd stage, only requires \((1)\) number of learnable parameters, same as vanilla GCN. As a result, HGDL has \((n)\) complexity. The runtime performance is detailed in Appendix H.3.

## 7 Conclusion

This paper explored a novel graph learning setting, namely, heterogeneous graph label distribution learning. Our goal is to predict label distributions of target nodes in a heterogeneous graph, which enables a finer-granular delineation of node properties compared to traditional single- or multi-class node classification. We demonstrated that the topological heterogeneity and inconsistency impose unique challenge for generalizing LDL into networked data, and proposed HGDL to overcome them. Specifically, HGDL proactively aggregates meta-paths to achieve optimal graph topology homogenization through attention mechanism, followed by a transformer-based approach to ensure topology and feature consistency for learning node label distributions. We analyzed the PAC-Bayes error bound of HGDL, and the result suggests the superiority of our design over those models learned from a single meta-path graph. Empirical results on five benchmark datasets validated the tightness of our analysis and substantiate that HGDL significantly outperformed its competitors.