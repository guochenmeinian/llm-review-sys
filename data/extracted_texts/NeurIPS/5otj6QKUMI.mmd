# Compression with Bayesian Implicit Neural Representations

Zongyu Guo

University of Science and

Technology of China

guozy@mail.ustc.edu.cn

&Gergely Flamich*

University of Cambridge

gf332@cam.ac.uk

Jiajun He

University of Cambridge

jh2383@cam.ac.uk

&Zhibo Chen

University of Science and

Technology of China

chenzhibo@ustc.edu.cn

&Jose Miguel Hernandez-Lobato

University of Cambridge

jmh233@cam.ac.uk

Equal Contribution.

###### Abstract

Many common types of data can be represented as functions that map coordinates to signal values, such as pixel locations to RGB values in the case of an image. Based on this view, data can be compressed by overfitting a compact neural network to its functional representation and then encoding the network weights. However, most current solutions for this are inefficient, as quantization to low-bit precision substantially degrades the reconstruction quality. To address this issue, we propose overfitting variational Bayesian neural networks to the data and compressing an approximate posterior weight sample using relative entropy coding instead of quantizing and entropy coding it. This strategy enables direct optimization of the rate-distortion performance by minimizing the \(\)-ELBO, and target different rate-distortion trade-offs for a given network architecture by adjusting \(\). Moreover, we introduce an iterative algorithm for learning prior weight distributions and employ a progressive refinement process for the variational posterior that significantly enhances performance. Experiments show that our method achieves strong performance on image and audio compression while retaining simplicity. Our code is available at [https://github.com/cambridge-mlg/combiner](https://github.com/cambridge-mlg/combiner).

## 1 Introduction

With the celebrated development of deep learning, we have seen tremendous progress of neural data compression, particularly in the field of lossy image compression . Taking inspiration from deep generative models, especially variational autoencoders (VAEs, ), neural image compression models have outperformed the best manually designed image compression schemes, in terms of both objective metrics, such as PSNR and MS-SSIM  and perceptual quality . However, these methods' success is largely thanks to their elaborate architectures designed for a particular data modality. Unfortunately, this makes transferring their insights _across_ data modalities challenging.

A recent line of work  proposes to solve this issue by reformulating it as a model compression problem: we treat a single datum as a continuous signal that maps coordinates to values, to which we overfit a small neural network called its implicit neural representation (INR). While INRs were originally proposed in  to study structural relationships in the data, Dupont et al.  have demonstrated that we can also use them for compression by encoding their weights. Since the datais conceptualised as an abstract signal, INRs allow us to develop universal, modality-agnostic neural compression methods. However, despite their flexibility, current INR-based compression methods exhibit a substantial performance gap compared to modality-specific neural compression models. This discrepancy exists because these methods cannot optimize the compression cost directly and simply quantize the parameters to a fixed precision, as opposed to VAE-based methods that rely on expressive entropy models  for end-to-end joint rate-distortion optimization.

In this paper, we propose a simple yet general method to resolve this issue by extending INRs to the variational Bayesian setting, i.e., we overfit a variational posterior distribution \(q_{}\) over the weights \(\) to the data, instead of a point estimate. Then, to compress the INRs, we use a relative entropy coding (REC) algorithm  to encode a weight sample \( q_{}\) from the posterior. The average coding cost of REC algorithms is approximately \(D_{}[q_{}\|p_{}]\), where \(p_{}\) is the prior over the weights. Therefore, the advantage of our method is that we can directly optimize the rate-distortion trade-off of our INR by minimising its negative \(\)-ELBO , in a similar fashion to VAE-based methods . We dub our method **C**ompression with **B**ayesian **I**mplicit **N**eural **R**epresentations (COMBINER), and present a high-level description of it in Figure 1.

We propose and extensively evaluate two methodological improvements critical to enhancing COMBINER's performance further. First, we find that a good prior distribution over the weights is crucial for good performance in practice. Thus, we derive an iterative algorithm to learn the optimal weight prior when our INRs' variational posteriors are Gaussian. Second, adapting a technique from Havasi et al. , we randomly partition our weights into small blocks and compress our INRs progressively. Concretely, we encode a weight sample from one block at a time and perform a few gradient descent steps between the encoding steps to improve the posteriors over the remaining uncompressed weights. Our ablation studies show these techniques can improve PSNR performance by more than 4dB on low-resolution image compression.

We evaluate COMBINER on the CIFAR-10  and Kodak  image datasets and the LibriSpeech audio dataset , and show that it achieves strong performance despite being simpler than its competitors. In particular, COMBINER is not limited by the expensive meta-learning loop present in current state-of-the-art INR-based works . Thus we can directly optimize INRs on entire high-resolution images and audio files instead of splitting the data into chunks. As such, our INRs can capture dependencies across all the data, leading to significant performance gains.

To summarize, our contributions are as follows:

* We propose variational Bayesian implicit neural representations for modality-agnostic data compression by encoding INR weight samples using relative entropy coding. We call our method **C**ompression with **B**ayesian **I**mplicit **N**eural **R**epresentations (COMBINER).
* We propose an iterative algorithm to learn a prior distribution on the weights, and a progressive strategy to refine posteriors, both of which significantly improve performance.
* We conduct experiments on the CIFAR-10, Kodak and LibriSpeech datasets, and show that COMBINER achieves strong performance despite being simpler than related methods.

Figure 1: Framework overview of COMBINER. It first encodes a datum \(\) into Bayesian implicit neural representations, as variational posterior distribution \(q_{}\). Then an approximate posterior sample \(^{*}\) is communicated from the sender to the receiver using relative entropy coding.

Background and Motivation

In this section, we briefly review the three core ingredients of our method: implicit neural representations (INRs; ) and variational Bayesian neural networks (BNNs; ), which serve as the basis for our model of the data, and relative entropy coding, which we use to compress our model.

**Implicit neural representations:** We can conceptualise many types of data as continuous signals, such as images, audio and video. Based on neural networks' ability to approximate any continuous function arbitrarily well , Stanley  proposed to use neural networks to represent data. In practice, this involves treating a datum \(\) as a point set, where each point corresponds to a coordinate-signal value pair \((,)\), and overfitting a small neural network \(f()\), usually a multilayer perceptron (MLP) parameterised by weights \(\), which is then called the _implicit neural representation_ (INR) of \(\). Recently, Dupont et al.  popularised INRs for lossy data compression by noting that compressing the INR's weights \(\) amounts to compressing \(\). However, their method has a crucial shortcoming: they assume a uniform coding distribution over \(\), leading to a constant _rate_, and overfit the INR only using the _distortion_ as the loss. Thus, unfortunately, they can only control the compression cost by varying the number of weights since they show that quantizing the weights to low precision significantly degrades performance. In this paper, we solve this issue using variational Bayesian neural networks, which we discuss next.

**Variational Bayesian neural networks:** Based on the minimum description length principle, we can explicitly control the network weights' compression cost by making them stochastic. Concretely, we introduce a _prior_\(p()\) (abbreviated as \(p_{}\)) and a _variational posterior_\(q(|)\) (abbreviated as \(q_{}\)) over the weights, in which case their information content is given by the _Kullback-Leibler (KL) divergence_\(D_{}[q_{}\|p_{}]\), as shown in . Therefore, for distortion measure \(\) and a coding budget of \(C\) bits, we can optimize the constrained objective

\[_{q_{}}_{(,)} _{ q_{}}(,f(),D_{}[q_{}\|p_{ }] C. \]

In practice, we introduce a slack variable \(\) and optimize the Lagrangian dual, which yields:

\[_{}(,q_{},p_{})= _{(,)}_{ q_{ }}(,f()+  D_{}[q_{}\|p_{}]+, \]

with different settings of \(\) corresponding to different coding budgets \(C\). Thus, optimizing \(_{}(,q_{},p_{})\) is equivalent to directly optimizing the rate-distortion trade-off for a given rate \(C\).

**Relative entropy coding with A* coding:** We will use _relative entropy coding_ to directly encode a _single random weight sample_\( q_{}\) instead of quantizing a point estimate and entropy coding it. This idea was first proposed by Havasi et al.  for model compression, who introduced minimal random coding (MRC) to encode a weight sample. In our paper, we use _depth-limited, global-bound A* coding_ instead, to which we refer as A* coding hereafter for brevity's sake [29; 20]. We present it in Appendix A for completeness. A* coding is an importance sampling algorithm that draws2\(N=2^{D_{}[q_{}\|p_{}]+t}\) independent samples \(_{1},,_{N}\) from the prior \(p_{}\) for some parameter \(t 0\), and computes their importance weights \(r_{i}=q_{}(_{i})/p_{}(_{i })\). Then, in a similar fashion to the Gumbel-max trick , it randomly perturbs the importance weights and selects the sample with the greatest perturbed weight. Unfortunately, this procedure returns an approximate sample with distribution \(_{}\). However, Theis and Yosri  have shown that the total variation distance \(\|q_{}-_{}\|_{}\) vanishes exponentially quickly as \(t\). Thus, \(t\) can be thought of as a free parameter of the algorithm that trades off compression rate for sample quality. Furthermore, A* coding is more efficient than MRC  in the following sense: Let \(N_{}\) and \(N_{}\) be the codes returned by MRC and A* coding, respectively, when given the same target and proposal distribution as input. Then, \([N_{}][N_{}]\), hence using A* coding is always strictly more efficient .

## 3 Compression with Bayesian Implicit Neural Representations

We now introduce our method, dubbed **C**ompression with **B**ayesian **I**mplicit **N**eural **R**epresentations (COMBINER). It extends INRs to the variational Bayesian setting by introducing a variational posterior \(q_{}\) over the network weights and fits INRs to the data \(\) by minimizing Equation (2). Sinceencoding the model weights is equivalent to compressing the data \(\), Equation (2) corresponds to jointly optimizing a given rate-distortion trade-off for the data. This is COMBINER's main advantage over other INR-based compression methods, which optimize the distortion only while keeping the rate fixed and cannot jointly optimize the rate-distortion. Moreover, another important difference is that we encode a random weight sample \( q_{}\) from the weight posterior using A* coding  instead of quantizing the weights and entropy coding them. At a high level, COMBINER applies the model compression approach proposed by Havasi et al.  to encode variational Bayesian INRs, albeit with significant improvements which we discuss in Sections 3.1 and 3.2.

In this paper, we only consider networks with a diagonal Gaussian prior \(p_{}=(_{p},(_{p}))\) and posterior \(q_{}=(_{q},(_{q}))\) for mean and variance vectors \(_{p},_{q},_{p},_{q}\). Here, \(()\) denotes a diagonal matrix with \(\) on the main diagonal. Following Havasi et al. , we optimize the variational parameters \(_{q}\) and \(_{q}\) using the local reparameterization trick  and, in Section 3.1, we derive an iterative algorithm to learn the prior parameters \(_{p}\) and \(_{p}\).

### Learning the Model Prior on the Training Set

To guarantee that COMBINER performs well in practice, it is critical that we find a good prior \(p_{}\) over the network weights, since it serves as the proposal distribution for A* coding and thus directly impacts the method's coding efficiency. To this end, in Algorithm 1 we describe an iterative algorithm to learn the prior parameters \(_{p}=\{_{p},_{p}\}\) that minimize the average rate-distortion objective over some training data \(\{_{1},,_{M}\}\):

\[}_{}(_{p},\{q_{}^{(i)}\})= _{i=1}^{M}_{}(_{i},q_{}^{(i)},p_{ ;_{p}})\,. \]

In Equation (3) we write \(q_{}^{(i)}=(_{q}^{(i)},(_ {q}^{(i)}))\), and \(p_{;_{p}}=(_{p},(_{p}))\), explicitly denoting the prior's dependence on its parameters. Now, we propose a coordinate descent algorithm to minimize the objective in Equation (3), shown in Algorithm 1. To begin, we randomly initialize the model prior and the posteriors, and alternate the following two steps to optimize \(\{q_{}^{(i)}\}\) and \(_{p}\):

1. **Optimize the variational posteriors:** We fix the prior parameters \(_{p}\) and optimize the posteriors using the local reparameterization trick  with gradient descent. Note that, given \(_{p}\), optimizing \(}_{}(_{p},\{q_{}^{(i)}\})\) can be split into \(M\) independent optimization problems, which we can perform in parallel: \[i=1,,M q_{}^{(i)}=*{ arg\,min}_{q}_{}(_{i},q,p_{;_{p}})\,.\] (4)
2. **Updating prior:** We fix the posteriors \(\{q_{}^{(i)}\}\) and update the model prior by computing \(_{p}=*{arg\,min}_{}}_{ }(_{p},\{q_{}^{(i)}\})\). In the Gaussian case, this admits a closed-form solution: \[_{p}=_{i=1}^{M}_{q}^{(i)},_{p}= _{i=1}^{M}[_{q}^{(i)}+(_{q}^{(i)}-_{ p})^{2}].\] (5)We provide the full derivation of this procedure in Appendix B. Note that by the definition of coordinate descent, the value of \(}_{}(_{p},\{q_{}^{(i)}\})\) decreases after each iteration, which ensures that our estimate of \(_{p}\) converges to some optimum.

### Compression with Posterior Refinement

Once the model prior is obtained using Algorithm 1, the sender uses the prior to train the variational posterior distribution for a specific test datum, as illustrated by Equation (2). To further improve the performance of INR compression, we also adopt a progressive posterior refinement strategy, a concept originally proposed in  for Bayesian model compression.

To motivate this strategy, we first consider the _optimal weight posterior_\(q_{}^{}\). Fixing the data \(\), trade-off parameter \(\) and weight prior \(p_{}\), \(q_{}^{}\) is given by \(q_{}^{}=*{arg\,min}_{q}_{}( ,q,p_{})\), where the minimization is performed over the set of all possible target distributions \(q\). To compress \(\) using our Bayesian INR, ideally we would like to encode a sample \(^{} q_{}^{}\), as it achieves optimal performance on average by definition. Unfortunately, finding \(q_{}^{}\) is intractable in general, hence we restrict the search over the set of all factorized Gaussian distributions in practice, which yields a rather crude approximation. However, note that for compression, we only care about encoding a **single, good quality sample** using relative entropy coding. To achieve this, Havasi et al.  suggest partitioning the weight vector \(\) into \(K\) blocks \(_{1:K}=\{_{1},,_{K}\}\). For example, we might partition the weights per MLP layer with \(_{i}\) representing the weights on layer \(i\), or into a preset number of random blocks; at the extremes, we could partition \(\) per dimension, or we could just set \(K=1\) for the trivial partition. Now, to obtain a good quality posterior sample given a partition \(_{1:K}\), we start with our crude posterior approximation and obtain

\[q_{}=q_{_{1}} q_{_{K}}= *{arg\,min}_{q_{1},,q_{K}}_{}(, q_{1} q_{K},p_{}), \]

where each of the \(K\) minimization procedures takes place over the appropriate family of factorized Gaussian distributions. Then, we draw a sample \(_{1} q_{_{1}}\) and _refine_ the remaining approximation:

\[q_{_{1}}=q_{_{2}_{1}}  q_{_{K}_{1}}=*{arg\,min}_{q_{2}, ,q_{K}}_{}(,q_{2} q_{K},p_ {}_{1}), \]

where \(_{}(_{1})\) indicates that \(_{1}\) is fixed during the optimization. We now draw \(_{2} q_{_{2}_{1}}\) to obtain the second chunk of our final sample. In total, we iterate the refinement procedure \(K\) times, progressively conditioning on more blocks, until we obtain our final sample \(=_{1:K}\). Note that already after the first step, the approximation becomes _conditionally factorized Gaussian_, which makes it far more flexible, and thus it approximates \(q_{}^{}\) much better .

**Combining the refinement procedure with compression:** Above, we assumed that after each refinement step \(k\), we draw the next weight block \(_{k} q_{_{k}_{1:k,-k}}\). However, as suggested in , we can also extend the scheme to incorporate relative entropy coding, by encoding an approximate sample \(}_{k}_{_{k}}_{1:k,-1}}\) with A* coding instead. This way, we actually feed two birds with one scene: the refinement process allows us to obtain a better overall approximate sample \(}\) by extending the variational family and by correcting for the occasional bad quality chunk \(}_{k}\) at the same time, thus making COMBINER more robust in practice.

### COMBINER in Practice

Given a partition \(_{1:K}\) of the weight vector \(\), we use A* coding to encode a sample \(}_{k}\) from each block. Let \(_{k}=D_{}[q_{_{k}}_{1:k,-k}}\|p_{ _{k}}]\) represent the KL divergence in block \(k\) after the completion of the first \(k-1\) refinement steps, where we have already simulated and encoded samples from the first \(k-1\) blocks. As we discussed in Section 2, we need to simulate \( 2^{_{k}+t}\) samples from the prior \(p_{_{k}}\) to ensure that the sample \(}_{k}\) encoded by A* coding has low bias. Therefore, for our method to be computationally tractable, it is important to ensure that there is no block with large divergence \(_{k}\). In fact, to guarantee that COMBINER's runtime is consistent, we would like the divergences across all blocks to be approximately equal, i.e., \(_{i}_{j}\) for \(0 i,j K\). To this end, we set a bit-budget of \(\) bits per block and below we describe the to techniques we used to ensure \(_{k}\) for all \(k=1,,K\). Unless stated otherwise, we set \(=16\) bits and \(t=0\) in our experiments.

First, we describe how we partition the weight vector based on the training data, to approximately enforce our budget on average. Note that we control COMBINER's rate-distortion trade-off by varying \(\) in its training loss in Equation (3). Thus, when we run Algorithm 1 to learn the prior, we also estimate the expected coding cost of the data given \(\) as \(c_{}=_{i=1}^{M}D_{}[q_{}^{(i)}\|p_{ }]\). Then, we set the number of blocks as \(K_{,}= c_{}/\) and we partition the weight vector such that the average divergence \(_{k}\) of each block estimated on the training data matches the coding budget, i.e., \(_{k}\) bits. Unfortunately, allocating individual weights to the blocks under this constraint is equivalent to the NP-hard bin packing problem . However, we found that randomly permuting the weights and greedily assigning them using the next-fit bin packing algorithm  worked well in practice.

**Relative entropy coding-aware fine-tuning:** Assume we now wish to compress some data \(\), and we already selected the desired rate-distortion trade-off \(\), ran the prior learning procedure, fixed a bit budget \(\) for each block and partitioned the weight vector using the procedure from the previous paragraph. Despite our effort to set the blocks so that the average divergence \(_{k}\) in each block on the training data, if we optimized the variational posterior \(q_{}\) using \(_{}(,q_{},p_{})\), it is unlikely that the actual divergences \(_{k}\) would match \(\) in each block. Therefore, we adapt the optimization procedure from , and we use a modified objective for each of the \(k\) posterior refinement steps:

\[_{_{k:K}}(,q_{|}_{1:k -1}},p_{})=_{(,)}_{ q_{}}(,f()]+_{i=k}^{K} _{i}_{i}, \]

where \(_{k:K}=\{_{k},,_{K}\}\) are slack variables, which we dynamically adjust during optimization. Roughly speaking, at each optimization step, we compute each \(_{i}\) and increase its penalty term \(_{i}\) if it exceeds the coding budget (i.e., \(_{i}>\)) and decrease the penalty term otherwise. See Appendix D for the detailed algorithm.

**The comprehensive COMBINER pipeline:** We now provide a brief summary of the entire COMBINER compression pipeline. To begin, given a dataset \(\{_{1},,_{M}\}\), we select an appropriate INR architecture, and run the prior learning procedure (Algorithm 1) with different settings for \(\) to obtain priors for a range of rate-distortion trade-offs.

To compress a new data point \(\), we select a prior with the desired rate-distortion trade-off and pick a blockwise coding budget \(\). Then, we partition the weight vector \(\) based on \(\), and finally, we run the relative entropy coding-aware fine-tuning procedure from above, using A* coding to compress the weight blocks between the refinement steps to obtain the compressed representation of \(\).

## 4 Related Work

**Neural Compression:** Despite their short history, neural image compression methods' rate-distortion performance rapidly surpassed traditional image compression standards [16; 7; 9]. The current state-of-the-art methods follow a variational autoencoder (VAE) framework , optimizing the rate-distortion loss jointly. More recently, VAEs were also successfully applied to compressing other data modalities, such video  or point clouds . However, mainstream methods quantize the latent variables produced by the encoder for transmission. Since the gradient of quantization is zero almost everywhere, learning the VAE encoder with standard back-propagation is not possible . A popular solution  is to use additive uniform noise during training to approximate the quantization error, but it suffers from a train-test mismatch . Relative entropy coding (REC) algorithms  eliminate this mismatch, as they can directly encode samples from the VAEs' latent posterior. Moreover, they bring unique advantages to compression with additional constraints, such as lossy compression with realism constraints [40; 41] and differentially private compression .

**Compressing with INRs:** INRs are parametric functional representations of data that offer many benefits over conventional grid-based representations, such as compactness and memory-efficiency [43; 44; 45]. Recently, compression with INRs has emerged as a new paradigm for neural compression , effective in compressing images , climate data , videos  and 3D scenes . Usually, obtaining the INRs involves overfitting a neural network to a new signal, which is computationally costly . Therefore, to ease the computational burden, some works [11; 46; 12] employ meta-learning loops  that largely reduce the fitting times during encoding. However, due to the expensive nature of the meta-learning process, these methods need to crop the data into patches to make training with second-order gradients practical. The biggest difficulty the current INR-based methods face is that quantizing the INR weights and activations can significantly degrade their performance, due to the brittle nature of the heavily overfitted parameters. Our method solves this issue by fitting a variational posterior over the parameters, from which we can encode samples directly using REC, eliminating the mismatch caused by quantization. Concurrent to our work, Schwarz et al.  introduced a method to learn a better coding distribution for the INR weights using a VAE, in a similar vein to our prior learning method in Algorithm 1. Their method achieves impressive performance on image and audio compression tasks, but is significantly more complex than our method: they run an expensive meta-learning procedure to learn the backbone architecture for their INRs and train a VAE to encode the INRs, making the already long training phase even longer.

## 5 Experiments

To assess COMBINER's performance across different data regimes and modalities, we conducted experiments compressing images from the low-resolution CIFAR-10 dataset , the high-resolution Kodak dataset , and compressing audio from the LibriSpeech dataset ; the experiments and their results are described in Sections 5.1 and 5.2. Furthermore, in Section 5.3, we present analysis and ablation studies on COMBINER's ability to adaptively activate or prune the INR parameters, the effectiveness of its posterior refinement procedure and on the time complexity of its encoding procedure.

### Image Compression

**Datasets:** We conducted our image compression experiments on the CIFAR-10  and Kodak  datasets. For the CIFAR-10 dataset, which contains \(32 32\) pixel images, we randomly selected 2048 images from the training set for learning the model prior, and evaluated our model on all 10,000 images in the test set. For the high-resolution image compression experiments we use 512 randomly cropped \(768 512\) pixel patches from the CLIC training set  to learn the model prior and tested on the Kodak images, which have matching resolution.

**Models:** Following previous methods , we utilize SIREN  as the network architecture. Input coordinates \(\) are transformed into Fourier embeddings  before being fed into the MLP network, depicted as \(()\) in Figure 1. For the model structure, we experimentally find a 4-layer MLP with 16 hidden units per layer and 32 Fourier embeddings works well on CIFAR-10. When training on CLIC and testing on Kodak, we use models of different sizes to cover multiple rate points. We describe the model structure and other experimental settings in more detail in Appendix E. Remarkably, the networks utilized in our experiments are quite small. Our model for compressing CIFAR-10 images has only 1,123 parameters, and the larger model for compressing high-resolution Kodak images contains merely 21,563 parameters.

**Performance:** In Figure 2, we benchmark COMBINER's rate-distortion performance against classical codecs including JPEG2000 and BPG, and INR-based codecs including COIN , COIN++

Figure 2: Rate-distortion curves on two image datasets. In both figures, **solid lines** denote INR-based methods, **dotted lines** denote VAE-based methods and **dashed lines** denote classical methods. Examples of decoded Kodak images are provided in Appendix F.3

, and MSCN . Additionally, we include results from VAE-based codecs such as ICLR2018  and CVPR2020  for reference. We observe that COMBINER exhibits competitive performance on the CIFAR-10 dataset, on par with COIN++ and marginally lower than MSCN. Furthermore, our proposed method achieves impressive performance on the Kodak dataset, surpassing JPEG2000 and other INR-based codecs. This superior performance is in part due to our method not requiring an expensive meta-learning loop [11; 46; 12], which would involve computing second-order gradients during training. Since we avoid this cost, we can compress the whole high-resolution image using a single MLP network, thus the model can capture global patterns in the image.

### Audio Compression

To demonstrate the effectiveness of COMBINER for compressing data in other modalities, we also conduct experiments on audio data. Since our method does not need to compute the second-order gradient during training, we can directly compress a long audio segment with a single INR model. We evaluate our method on LibriSpeech , a speech dataset recorded at a 16kHz sampling rate. We train the model prior with 3-second chunks of audio, with 48000 samples per chunk. The detailed experimental setup is described in Appendix E. Due to COMBINER's time-consuming encoding process, we restrict our evaluation to 24 randomly selected audio chunks from the test set. Since we lack COIN++ statistics for this subset of 24 audio chunks, we only compare our method with MP3 (implemented using the ffmpeg package), which has been shown to be much better than COIN++ on the complete test set . Figure 4 shows that COMBINER outperforms MP3 at low bitrate points, which verifies its effectiveness in audio compression. We also conducted another group of experiments where the audios are cropped into shorter chunks, which we describe in Appendix F.2.

### Analysis, Ablation Study and Time Complexity

**Model Visualizations:** To provide more insight into COMBINER's behavior, we visualize its parameters and information content on the second hidden layer of two small 4-layer models trained on two CIFAR-10 images with \(=10^{-5}\). We use the KL in bits as an estimate of their coding cost, and do not encode the weights with A* coding or perform fine-tuning.

In Figure 6, we visualize the learned model prior parameters \(_{p}\) and \(_{p}\) in the left column, the variational parameters of two distinct images in the second and third column and the KL divergence \(D_{}[q_{}|p_{}]\) in bits in the rightmost column. Since this layer incorporates 16 hidden units, the weight matrix of parameters has a \(17 16\) shape, where weights and bias are concatenated (the bias is represented by the last row). Interestingly, there are seven "active" columns within \(_{p}\), indicating that only seven hidden units of this layer would be activated for signal representation at this rate point. For instance, when representing image 1 that is randomly selected from the CIFAR-10 test set, four columns are activated for representation. This activation is evident in the four blue columns within the KL map, which require a few bits to transmit the sample of the posterior distribution. Similarly, three hidden units are engaged in the representation of image 2. As their variational Gaussian distributions have close to zero variance, the posterior distributions at these activated columns basically approach a Dirac delta distribution. In summary, by optimizing the rate-distortion objective, our proposed method can adaptively activate or prune network parameters.

**Ablation Studies:** We conducted ablation studies on the CIFAR-10 dataset to verify the effectiveness of learning the model prior (Section 3.1) and posterior fine-tuning (Section 3.2). In the first ablation study, instead of learning the prior parameters, we follow the methodology of Havasi (, p. 73) and use a layer-wise zero-mean isotropic Gaussian prior \(p_{}=(0,_{}I)\), where \(p_{}\) is the weight prior for the \(\)th hidden layer. We learn the \(_{}\)'s jointly with the posterior parameters by optimizing Equation (3) using gradient descent, and encode them at 32-bit precision alongside the A*-coded posterior weight samples. In the second ablation study, we omit the fine-tuning steps between encoding blocks with A* coding, i.e. we never correct for bad quality approximate samples. In both experiments, we compress each block using 16 bits. Finally, as a reference, we also compare with the theoretically optimal scenario: we draw an exact sample from each blocks's variational posterior between refinement steps instead of encoding an approximate sample with A* coding, and estimate the sample's codelength with the block's KL divergence.

We compare the results of these experiments with our proposed pipeline (Section 3.3) using the above mentioned techniques in Figure 4. We find that both the prior learning and posterior refinement contribute significantly to COMBINER's performance. In particular, fine-tuning the posteriors is more effective at higher bitrates, while prior learning increases yields a consistent 4dB in gain in PSNR across all bitrates. Finally, we see that fine-tuning cannot completely compensate for the occasional bad approximate samples that A* coding yields, as there is a consistent \(0.8-1.3\)dB discrepancy between COMBINER's and the theoretically optimal performance.

In Appendix C, we describe a further experiments we conducted to estimate how much each fine-tuning step contributes to the PSNR gain between compressing two blocks. The results are shown in Figure 7, which demonstrate that quality of the encoded approximate posterior sample doesn't just monotonically increase with each fine-tuning step, see Appendix C for an explanation.

**Time Complexity:** COMBINER's encoding procedure is slow, as it requires several thousand gradient descent steps to infer the parameters of the INR's weight posterior, and thousands more for the progressive fine-tuning. To get a better understanding of COMBINER's practical time complexity, we evaluate its coding time on both the CIFAR-10 and Kodak datasets at different rates and report our findings in Tables 1 and 2. We find that it can take between 13 minutes (0.91 bpp) to 34 minutes (4.45 bpp) to encode 500 CIFAR-10 images in parallel with a single A100 GPU, including posterior inference (7 minutes) and progressive fine-tuning. Note, that the fine-tuning takes longer for higher bitrates, as the weights are partitioned into more groups as each weight has higher individual information content. To compress high-resolution images from the Kodak dataset, the encoding time varies between 21.5 minutes (0.070 bpp) and 79 minutes (0.293 bpp).

Figure 6: Visualizations of the weight prior, posterior and information content of a variational INR trained on two CIFAR-10 images. We focus on the INRâ€™s weights connecting the first and second hidden layers. Each heatmap is \(17 16\) because both layers have 16 hidden units and we concatenated the weights and the biases (last row). We write s.d. for standard deviation.

To assess the effect of the fine-tuning procedure's length, we randomly selected a CIFAR-10 image and encoded it using the whole COMBINER pipeline, but varied the number of fine-tuning steps between 2148 and 30260; we report the results of our experiment in Figure 5. We find that running the fine-tuning process beyond a certain point has diminishing returns. In particular, while we used around 30k iterations in our other experiments, just using 3k iterations would sacrifice a mere 0.3 dB in the reconstruction quality, while saving 90% on the original tuning time.

On the other hand, COMBINER has fast decoding speed, since once we decode the compressed weight sample, we can reconstruct the data with a single forward pass through the network at each coordinate, which can be easily parallelized. Specifically, the decoding time of a single CIFAR-10 image is between 2 ms and 4 ms using an A100 GPU, and less than 1 second for a Kodak image.

## 6 Conclusion and Limitations

In this paper, we proposed COMBINER, a new neural compression approach that first encodes data as variational Bayesian implicit neural representations and then communicates an approximate posterior weight sample using relative entropy coding. Unlike previous INR-based neural codecs, COMBINER supports joint rate-distortion optimization and thus can adaptively activate and prune the network parameters. Moreover, we introduced an iterative algorithm for learning the prior parameters on the network weights and progressively refining the variational posterior. Our ablation studies show that these methods significantly enhance the COMBINER's rate-distortion performance. Finally, COMBINER achieves strong compression performance on low and high-resolution image and audio compression, showcasing its potential across different data regimes and modalities.

COMBINER has several limitations. First, as discussed in Section 5.3, while its decoding process is fast, its encoding time is considerably longer. Optimizing the variational posterior distributions requires thousands of iterations, and progressively fine-tuning them is also time-consuming. Second, Bayesian neural networks are inherently sensitive to initialization . Identifying the optimal initialization setting for achieving training stability and superior rate-distortion performance may require considerable effort. Despite these challenges, we believe COMBINER paves the way for joint rate-distortion optimization of INRs for compression.

## 7 Acknowledgements

ZG acknowledges funding from the Outstanding PhD Student Program at the University of Science and Technology of China. ZC is supported in part by National Natural Science Foundation of China under Grant U1908209, 62021001. GF acknowledges funding from DeepMind.

    &  &  \\    & **Learning Posterior** & & **REC + Fine-tuning** & **Total** \\ 
0.91 bpp & & \(\)6 min & \(\)13 min & 2.06 ms \\
1.39 bpp & & \(\)9 min & \(\)16 min & 2.09 ms \\
2.28 bpp & \(\)7 min & \(\)14 min 30 s & \(\)21 min 30 s & 2.86 ms \\
3.50 bpp & & \(\)21 min 30 s & \(\)28 min 30 s & 3.82 ms \\
4.45 bpp & & \(\)27 min & \(\)34 min & 3.88 ms \\   

Table 1: The encoding time and decoding time of COMBINER on CIFAR-10 dataset.

    &  &  \\    & **Learning Posterior** & & & \\ 
0.07 bpp & & \(\)12 min 30 s & \(\)21 min 30 s & 348.42 ms \\
0.11 bpp & \(\)9 min & \(\)18 mins & \(\)27 min & 381.53 ms \\
0.13 bpp & & \(\)22 min & \(\)31 min & 405.38 ms \\
0.22 bpp & \(\)11 min & \(\)50 min & \(\)61 min & 597.39 ms \\
0.29 bpp & & \(\)68 min & \(\)79 min & 602.32 ms \\   

Table 2: The encoding time and decoding time of COMBINER on Kodak dataset.