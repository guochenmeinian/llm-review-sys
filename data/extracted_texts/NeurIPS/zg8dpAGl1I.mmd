# XLand-MiniGrid: Scalable Meta-Reinforcement Learning Environments in JAX

Alexander Nikulin

AIRI, MIPT

nikulin@airi.net

&Vladislav Kurenkov

AIRI, Innopolis University

kurenkov@airi.net

&Ilya Zisman

AIRI, Skoltech

zisman@airi.net

Artem Agarkov

MIPT

agarkov.as@phystech.edu

&Viacheslav Sinii

T-Bank

v.siniy@tbank.ru

&Sergey Kolesnikov

T-Bank

s.s.kolesnikov@tbank.ru

Work done while at T-Bank

###### Abstract

Inspired by the diversity and depth of XLand and the simplicity and minimalism of MiniGrid, we present XLand-MiniGrid, a suite of tools and grid-world environments for meta-reinforcement learning research. Written in JAX, XLand-MiniGrid is designed to be highly scalable and can potentially run on GPU or TPU accelerators, democratizing large-scale experimentation with limited resources. Along with the environments, XLand-MiniGrid provides pre-sampled benchmarks with millions of unique tasks of varying difficulty and easy-to-use baselines that allow users to quickly start training adaptive agents. In addition, we have conducted a preliminary analysis of scaling and generalization, showing that our baselines are capable of reaching millions of steps per second during training and validating that the proposed benchmarks are challenging. XLand-MiniGrid is open-source and available at https://github.com/corl-team/xland-minigrid.

## 1 Introduction

Reinforcement learning (RL) is known to be extremely sample inefficient and prone to overfitting, sometimes failing to generalize to even subtle variations in environmental dynamics or goals (Rajeswaran et al., 2017; Zhang et al., 2018; Henderson et al., 2018; Alver and Precup, 2020). One way to address these shortcomings are meta-RL approaches, where adaptive agents are pre-trained on diverse task distributions to significantly increase sample efficiency on new problems (Wang et al., 2016; Duan et al., 2016). With sufficient scaling and task diversity, these approaches are capable of astonishing results, reducing the adaptation time on new problems to human levels and beyond (Team et al., 2021, 2023).

At the same time, meta-RL methods have major limitations. Since the agent requires thousands of different tasks for generalization, faster adaptation during inference comes at the expense of significantly increased pre-training requirements. For example, a single training of the Ada agent (Team et al., 2023) takes five weeks3, which can be out of reach for most academic labs and practitioners. Even those who might have the training resources would still be unable to use them, as the XLand environment is not publicly available. We believe, and this also has been pointed out by Wang et al. (2021), that such demanding requirements are the reason why most recent works on adaptive agents (Laskin et al., 2022; Lee et al., 2023; Lu et al., 2023; Norman and Clune, 2023) avoid complex environments in favor of more simplistic ones (e.g., simple navigation).

While simple environments are an affordable and convenient option for theoretical analysis, they are not enough for researchers to discover the limits and scaling properties of proposed algorithms in practice. To make such research more accessible, we continue the successful efforts of Freeman et al. (2021); Lu et al. (2022); Bonnet et al. (2023); Koyamada et al. (2023) at accelerating environments using JAX (Bradbury et al., 2018), and introduce the XLand-MiniGrid, a library of grid world environments and benchmarks for meta-RL research (Section 2). We carefully analyze the scaling properties (Section 4.1) of environments and conduct preliminary experiments to study the performance and generalization of the implemented baselines, validating that the proposed benchmarks are challenging (Section 4.2).

## 2 XLand-MiniGrid

In this paper, we present the initial release of **XLand-MiniGrid**, a suite of tools, benchmarks and grid world environments for meta-RL research. We do not compromise on task complexity in favor of affordability, focusing on democratizing large-scale experimentation with limited resources. XLand-MiniGrid is open-source and available at https://github.com/corl-team/xland-mingrid under Apache 2.0 license.

Similar to XLand (Team et al., 2023), we introduce a system of extensible rules and goals that can be combined in arbitrary ways to produce diverse distributions of tasks (see Figures 1 and 2 for a demonstration). Similar to MiniGrid (Chevalier-Boisvert et al., 2023), we focus on goal-oriented grid world environments and use a visual theme already well-known in the community. However, despite the similarity, XLand-MiniGrid is written from scratch in the JAX framework and can therefore run directly on GPU or TPU accelerators, reaching millions of steps per second with a simple jax.vmap transformation. This makes it possible to use the Anakin architecture (Hessel et al., 2021) and easily scale to multiple devices using the jax.pmap transformation.

In addition to environments, we provide pre-sampled benchmarks with millions of unique tasks, simple baselines with recurrent PPO (Schulman et al., 2017) that scale to multi-GPU setups, walk-through guides that explain the API, and colab notebooks that make it easy for users to start training adaptive agents. We hope that all of this will help researchers quickly start experimenting.

This section provides a high-level overview of the library, describing the system of rules and goals, the observation and action spaces, and its API. The implemented environments are also described.

Figure 1: Visualization of how the production rules in XLand-MiniGrid work, exemplified by a few steps in the environment. In the first steps, the agent picks up the blue pyramid and places it next to the purple square. The NEAR production rule is then triggered, which transforms both objects into a red circle. See Figure 2 and Section 2.1 for additional details.

Figure 2: Visualization of a specific sampled task (see Figure 3) in XLand-MiniGrid. We highlighted the optimal path to solve this particular task. The agent needs to take the blue pyramid and put it near the purple square in order to transform both objects into a red circle. To complete the goal, a red circle needs to be placed near the green circle. However, placing the purple square near the yellow circle will make the task unsolvable in this trial. Initial positions of objects are randomized on each reset. Rules and goals are hidden from the agent.

### Rules and Goals

In XLand-MiniGrid, the system of rules and goals is the cornerstone of the emergent complexity and diversity. In the original MiniGrid (Chevalier-Boisvert et al., 2023), some environments have dynamic goals, but the dynamics themselves are never changed. To train and evaluate highly adaptive agents, we need to be able to change the dynamics in non-trivial ways (Team et al., 2023).

**Rules.** Rules are functions that can change the environment state in a deterministic fashion according to the given conditions. For example, the NEAR rule (see Figure 1 for a visualization) accepts two objects a, b and transforms them to a new object c if a and b end up on neighboring tiles. Rules can change between resets. For efficiency reasons, the rules are evaluated only after some actions or events occur (e.g., the NEAR rule is checked only after the put_down action).

**Goals.** Goals are similar to rules, except they do not change the state, only test conditions. For example, the NEAR goal (see Figure 1 for a visualization) accepts two objects a, b and checks that they are on neighboring tiles. Similar to rules, goals are evaluated only after certain actions and can change between resets.

Both rules and goals are implemented with classes. However, in order to be able to change between resets and still be compatible with JAX, both rules and goals are represented with an array encoding, where the first index states the rule or goal ID and the rest are arguments with optional padding to the same global length. Thus, every rule and goal should implement the encode and decode methods. The environment state contains only these encodings, not the actual functions or classes. For the full list of supported rules and goals, see Appendix I.

### Api

**Environment interface.** There have been many new JAX-based environments appearing recently (Freeman et al., 2021; Lange, 2022; Bonnet et al., 2023; Koyamada et al., 2023; Rutherford et al., 2023; Jiang et al., 2023; Frey et al., 2023; Lechner et al., 2023), each offering their own API variations. The design choices in most of them were not convenient for meta-learning4, hence why we decided to focus on creating a minimal interface without making it general. The core of our library is interface-independent, so we can quickly switch if a unified interface becomes available in the future. If necessary, practitioners can easily write their own converters to the format they need, as has been done in several projects5 that already use XLand-MiniGrid.

At a high level, the current API combines dm_env (Muldal et al., 2019) and gymnax (Lange, 2022). Each environment inherits from the base Environment and implements the jit-compatible reset and step methods, with custom EnvParams if needed. The environment itself is completely stateless, and all the necessary information is contained in the TimeStep and EnvParams data classes. This design makes it possible for us to vectorize on arbitrary arguments (e.g., rulesets) if their logic is compatible with jit-compilation. Similar to Gym (Brockman et al., 2016), users can register new environment variants with custom parameters to conveniently reuse later with the help of make. We provide minimal sample code to instantiate an environment from the registry, reset, step and optionally render the state (see Listing 1). For an example of how to compile the entire episode rollout, see Appendix D.

**State and TimeStep.** Similar to dm_env, TimeStep contains all the information available to the agent, such as observation, reward, step_type and discount. The step type will be FIRST at the beginning of the episode, LAST at the last step, and MID for all others. The discount can be in the \(\) range, and we set it to \(0.0\) to indicate the end of the episode or trial. In addition, it contains State with all the necessary information to describe the environment dynamics, such as the grid, agent states, encoding of rules and goals, and a key for the random number generator that can be used during resets. The combination of the internal state with the timestep is a bit different from the previous designs by Lange (2022); Bonnet et al. (2023), but it allows for some simplifications, such as an auto-reset wrapper implementation based on the current or previous step (in the style of Gym (Brockman et al., 2016) or EnvPool (Weng et al., 2022)).

``` importjax importxminigridreset_key=jax.random.key(0) #tolistavailableenvironments: xminigrid.registered_environments() ```
#createenvinstance env,env_params=xminigrid.make("XLand-MiniGrid-R9-25x25") #changesomedefaultparams env_params=env_params.replace(max_steps=100)
#fullyjit-compatiblestepandresetmethods timestep=jax.jit(env.reset)(env_params,reset_key) timestep=jax.jit(env.step)(env_params,timestep,action=0) ```
#optionallyrenderthestate env.render(env_params,timestep) ```

Listing 1: Basic example usage of XLand-MiniGrid.

**Observation and action space.** Although not fully compatible, we made an effort to be consistent with the original MiniGrid. Observations describe a partial field of view around the agent as two-dimensional arrays, where each position is encoded by the tile and color IDs. Thus, observations are not images and should not be treated as such by default. While naively treating them as images can work in most cases, the correct approach would be to pre-process them via embeddings. If necessary, practitioners can render such observations as images via the wrapper, although with some performance overhead (see Appendix H). We also support the ability to prohibit an agent from seeing through walls. The agent actions, namely move_forward, turn_left, turn_right, pick_up, put_down, toggle, are discrete. The agent can only pick up one item at a time, and only if its pocket is empty. In contrast to Team et al. (2023), the actual rules and goals of the environment remain hidden from the agent in our experiments. However, our environment does not impose any restrictions on this, and practitioners can easily access them if needed (see our preliminary experiments in such a setting in Appendix G).

### Supported Environments

In the initial release, we provide environments from two domains: XLand and MiniGrid. XLand is our main focus, and for this domain, we implement single-environment XLand-MiniGrid and numerous registered variants with different grid layouts and sizes (see Figure 14 in the Appendix). All of them can be combined with arbitrary rulesets from the available benchmarks (see Section 3) or custom ones, and follow the naming convention of XLand-MiniGrid-R{#rooms}-{size}. We made an effort to balance the limit on the maximum number of steps so that the tasks cannot be brute-forced by the agent on every trial without using any memory. Thus, we use 3 x grid height x grid width as a heuristic to set the default maximum number of steps, but this can be changed afterwards if needed. While meta-RL is our main motivation, this environment can be useful for research in exploration, continual learning, unsupervised RL or curriculum learning. For example, we can easily model novelty as a change in rules, goals or objects between episodes, similar to NovGrid (Balloch et al., 2022).

Furthermore, due to the generality of the rules and goals and the ease of extensibility, most non-language-based tasks from the original MiniGrid can also be quickly implemented in XLand-MiniGrid. To demonstrate this, we have ported the majority of such tasks, including the popular Empty, FourRooms, UnlockPickUp, DoorKey, Memory and others. For a full list of registered environments, see Appendix L (\(38\) in total).

## 3 Benchmarks

Although users can manually compose and provide specific rulesets for environments, this can quickly become cumbersome. Therefore, for large-scale open-ended experiments, we provide a way to procedurally generate tasks of varying diversity and complexity, which can in turn generate thousands of unique rulesets. However, since the generation procedure can be quite complex, representing it in a form that is convenient for efficient execution in JAX is not feasible. To avoid unnecessary overhead and standardize comparisons between algorithms and experiments, we pre-generated several benchmarks with up to three million unique rulesets. The generation process itself as well as available configurations are described in detail below.

**Generation Procedure.** For the generation procedure, we closely follow the approach of Team et al. (2023). A similar procedure was also used in a concurrent work by Bornemann et al. (2023) but for multi-agent meta-learning. On a high level, each task can be described as a tree (see Figure 3), where each node is a production rule and the root is the main goal. Generation starts by uniformly sampling an agent's goal from the ones available. Then, new production rules are sampled recursively at each level so that their output objects are the input objects of the previous level. Since all rules have at most two arguments, the tree will be a complete binary tree in the worst-case scenario. At the start of the episode, only objects from the leaf rules are placed on the grid, and their positions are randomized at each reset. Thus, to solve the task, the agent has to trigger these rules in a sequence to get the objects needed for the goal. This hierarchical structure is very similar to the way tasks are organized in the famous Minecraft (Guss et al., 2021) or the simpler Crafter (Hafner, 2021) benchmarks. For object sampling, we used ten colors and seven tile types (e.g., circle, square).

When sampling production rules, we restrict the possible choices of input objects, excluding those that have already been used. This is done so that the objects are present only once as input and once as output in the main task tree. To increase diversity, we also added the ability to sample depth instead of using a fixed one, as well as branch pruning. With some probability, the current node can be marked as a leaf and its input objects added as initial objects. This way, while using the same budget, we can generate tasks with many branches, or more sequential ones but with greater depth.

To prevent the agent from indiscriminately triggering all production rules and brute forcing, we additionally sample distractor rules and objects. Distractor objects are sampled from the remaining objects and are not used in any rules. Distractor production rules are sampled so that they use objects from the main task tree, but never produce useful objects. This creates dead ends and puts the game

Figure 4: Distribution of the number of rules for the available benchmark configurations. One can see that each successive benchmark offers an increasingly diverse distribution of tasks, while still including tasks from the previous benchmarks. The average task complexity, as well as tree depth, also increases. See Section 3 for the generation procedure and Appendix J for the exact generation configuration. Besides, users can generate and load custom benchmarks easily, even with a custom generation procedure, as long as the final format is the same.

Figure 3: Visualization of a specific task tree with depth two, sampled according to the procedure described in Section 3. The root of the tree is a goal to be achieved by the agent, while all other nodes are production rules describing possible transformations. At the beginning of each episode, only the input objects of the leaf production rules are placed on the grid. In addition to the main task tree, the distractor production rules can be sampled. They contain already used objects to introduce dead ends. All of this together is what we call a **ruleset**, as it defines the task.

in an unsolvable state, as all objects are only present once. As a result, the agent needs to experiment intelligently, remembering and avoiding these rules when encountered.

**Available configurations.** In the initial release, we provide four benchmark types with various levels of diversity: trivial, small, medium and high. For each benchmark type, one million unique rulesets were pre-sampled, including additional variations with three millions for medium and high. Generally, we tried gradually increasing diversity to make it so that each successive benchmark also includes tasks from the previous ones (see Figure 4). Thus, due to the increasing task-tree depth, the average difficulty also increases. For example, the trivial benchmark has a depth of zero and can be used for quick iterations and debugging. For exact generation settings, see Table 4 in Appendix J.

For ease of use when working with benchmarks, we provide a user-friendly interface that allows them to be downloaded, sampled, split into train and test sets (see an extended usage example in Appendix D). Similar to Fu et al. (2020); Kurenkov et al. (2023), our benchmarks are hosted in the cloud and will be downloaded and cached the first time they are used. In addition, we also provide a script used for generation, with which users can generate and load their own benchmarks with custom settings.

## 4 Experiments

In this section, we demonstrate XLand-MiniGrid's ability to scale to thousands of parallel environments, dozens of rules, various grid sizes, and multiple accelerators (see Section 4.1). In addition, we describe the implemented baselines and validate their scalability. Finally, we perform preliminary experiments showing that the proposed benchmarks, even with minimal diversity, present a significant challenge for the implemented baseline, leaving much room for improvement, especially in terms of generalization to new problems (see Section 4.2). For each experiment, we list the exact hyperparameters and additional details in Appendix K.

### Scalability

**Simulation throughput.** Figure 4(a) shows the simulation throughput for a random policy averaged over all registered environment variations (38 in total, see Section 2.3). All measurements were done on A100 GPUs, taking the minimum value among multiple repeats. In contrast to Jumanji Bonner et al. (2023), we had an auto-reset wrapper enabled to provide estimates closer to real use, as resetting can be expensive for some environments. For meta-RL environments, random rulesets from the trivial-1m benchmark were sampled. One can see that the scaling is almost log-log linear with the number of parallel environments, although it does begin to saturate around \(2^{13}\) on a single device. However, on multiple devices, scaling remains log-log linear without signs of saturation, easily reaching tens of millions of steps per second. We provide the scripts we used so that our results can be replicated on other hardware. As an example, we replicated some of the benchmarks on consumer grade GPUs like 4090 in the Appendix F.

**Scaling grid size.** While most of MiniGrid's grid world environments Chevalier-Boisvert et al. (2023) use small grid sizes, it is still interesting to test the scaling properties of XLand-MiniGrid in this dimension, as larger sizes may be needed for difficult benchmarks to fit all the initial objects. As one can see in Figure 4(b), the simulation throughput can degrade significantly with increasing grid size, and can also show earlier signs of saturation. A possible explanation for this phenomenon is that many game loop operations, such as conditional branching during action selection, do not fit well with the parallelism principles of the JAX framework. Similar results have been observed in previous works using JAX-based environments Bonner et al. (2023); Koyamada et al. (2023). Nevertheless, the throughput remains competitive even at larger sizes. Furthermore, as shown in Figure 4(d), the throughput can be considerably improved with multiple devices, allowing a larger pool of parallel environments and mitigating saturation. When it comes to small grid sizes6, they can still be a significant challenge for existing algorithms Zhang et al. (2020), which we will also demonstrate in Section 4.2.

**Scaling number of rules.** According to Team et al. (2023), a full-scale XLand environment can use more than five rules. Similarly, our benchmarks can contain up to eighteen rules (see Figure 4). To test XLand-MiniGrid under similar conditions, we report the simulation throughput with different numbers of rules (see Figure 5c). For testing purposes, we simply replicated the same NEAR rule multiple times and used a grid size of \(16\)x\(16\). As one can see, the throughput decreases monotonically as the number of rules increases. As a result, the number of parallel environments must be increased to maintain the same throughput level. However, in contrast to increasing the grid size, there is no apparent saturation even at \(24\) rules. The throughput can be improved even further by increasing the number of devices (see Figure 5e).

### Baselines

With the release of XLand-MiniGrid, we are providing near-single-file implementations of recurrent PPO (Schulman et al., 2017) for single-task environments and its extension to RL\({}^{2}\)(Duan et al., 2016; Wang et al., 2016) for meta-learning as baselines. The implementations were inspired by the popular PureJaxRL (Lu et al., 2022), but extended to meta-learning and multi-device setups. Due to the full environment compatibility with JAX, we can use the Anakin architecture (Hessel et al., 2021), jit-compile the entire training loop, and easily parallelize across multiple devices using jax.pmap transformation. We also provide standalone implementations in the colab notebooks.

While our implementation is greatly simplified in comparison to Ada (Team et al., 2023), it encapsulates the main principles and is easy to understand and modify, e.g., swapping simple GRU (Cho et al., 2014) for more modern and efficient state space models (Lu et al., 2023). Next, we perform preliminary experiments to test the scaling, performance and generalization of the baselines.

**Training throughput.** Figure 5f shows the training throughput of the implemented baseline on meta-RL tasks. We used a 9x9 grid size and the trivial-1m benchmark with fixed model size and

Figure 5: Analysis of XLand-MiniGridâ€™s ability to scale to thousands of parallel environments, dozens of rules, different grid sizes and multiple accelerators. All measurements were performed on A100 GPUs, taking the minimum between multiple attempts with the auto-reset wrapper enabled and with random policy, except for **(d).****(a)** Simulation throughput averaged over all registered environment variations (38 in total, see Section 2.3). Scaling is almost log-log linear, with slight saturation on a single device. **(b)** Increasing the grid size can significantly degrade simulation throughput, leading to earlier saturation. **(c)** Increasing the number of rules monotonically reduces the simulation throughput, with no apparent saturation. To maintain the same level of throughput, the number of parallel environments should be increased. **(d)**-**(e)** Parallelization across multiple devices can mitigate saturation and significantly increase throughput, even at large grid sizes and rule counts. **(f)** Training throughput for the implemented recurrent PPO baseline (see Section 4.2). The PPO hyperparameters, except for model size and RNN sequence length, were tuned for each setup to maximize utilization per device. Single device training throughput saturates near one million steps per second. Similarly to random policy, it can be increased greatly with multiple devices. We additionally provide figures without log-axes at Appendix E.

RNN sequence length of \(256\). Although it is not always optimal to maximize the number of steps per second, as this may reduce the sample efficiency (Andrychowicz et al., 2020), for demonstration purposes we searched over other parameters to maximize per device utilization. As one can see, on a single device, the training throughput saturates near one million steps per second. Similar to Figure 4(a), it can be greatly increased with multiple devices to almost reach six million. Further improvement may require changing the RNN architecture to a more hardware-friendly one, e.g., the Transformer (Vaswani et al., 2017), which we left for future work. We provide additional throughput benchmarks on consumer grade GPUs in the Appendix F.

**Training Performance.** We trained the implemented RL\({}^{2}\) on the proposed benchmarks to demonstrate that they provide an interesting challenge and to establish a starting point for future research (see Figure 6). A grid size of \(13\)x\(13\) with four rooms was used (see Figure 14). We report the return for \(25\) trials, averaged over all \(4096\) evaluation tasks and five random training seeds. During the training, the

Figure 8: Preliminary generalization evaluation of the implemented RL\({}^{2}\) Duan et al. (2016); Wang et al. (2016) baseline. For training, we used the same setup as in Figure 6, excluding a subset of goal types (but not rules) from the benchmarks, and then tested generalization on \(4096\) tasks sampled from them. We report the return for the 25 trials. Results are averaged over three random training seeds. A large gap in generalization remains, even for the benchmarks with minimal diversity.

Figure 6: Learning curves from training an RL\({}^{2}\)(Duan et al., 2016; Wang et al., 2016) agent based on recurrent PPO (Schulman et al., 2017) on the XLand-MiniGrid meta-RL benchmarks introduced in Section 3. Grid size of \(13\)x\(13\) with four rooms was used (see Figure 14 in Appendix I for a visualization). We report the return for \(25\) trials on each evaluation task, \(4096\) tasks in total. During training, the agent is only limited by the fixed maximum step count and can get more trials if it manages to solve tasks faster. Results are averaged over five random seeds. One can see that the proposed benchmarks present a significant challenge, especially in terms of the \(20\)th score percentile. Similar to Team et al. (2023), we evaluate algorithms mainly on the \(20\)th percentile, as it better reflects the ability to adapt to new tasks. Also, see Appendix H for the learning curves on image-based observations.

Figure 7: Learning curve from the large-scale baseline training on the high-1m benchmark for **one trillion** transitions. We use the same setup as in Figure 6 and report the return for 25 trials. Results are averaged over three random training seeds. While such an extreme amount of experience can be collected with our library, this alone is not enough to significantly improve the baseline performance on this benchmark.

agent was limited to a fixed number of steps in each task, so that it could get more trials if it was able to solve the tasks faster. Similar to (Team et al., 2023), we evaluate algorithms mainly on the \(20\)th percentile, as it provides a lower bound that better reflects the ability to adapt to new tasks, while the average score is dominated by easy tasks for which it is easier to generalize. Therefore, we advise practitioners to compare their algorithms according to the \(20\)th percentile. See appendix K for a more in-depth discussion.

As one can see, the performance is far from optimal, especially in terms of the \(20\)th score percentile. Furthermore, the performance on high-1m remains sub-optimal even when the agent is trained for extreme **one trillion** transitions (see Figure 7). Note that training Ada (Team et al., 2023) for such a duration would not be feasible.7

**Generalization.** Since the main purpose of meta-learning is enabling rapid adaptation to novel tasks, we are mainly interested in generalization during training. To test the baseline along this dimension, we excluded a subset of goal types (but not rules) from the benchmarks and retrained the agent using the same hyperparameters and procedures as in Figure 6. During training, we tested generalization on the \(4096\) tasks sampled from the excluded (i.e., unseen) tasks and averaged over three random training seeds. As Figure 8 shows, there remains a large gap in generalization on all benchmarks, even on the one with minimal diversity.

## 5 Related Work

**Meta-learning environments.** Historically, meta-RL benchmarks have focused on tasks with simple distributions, such as bandits and 2D navigation (Wang et al., 2016; Duan et al., 2016; Finn et al., 2017), or few-shot control policy adaptation (e.g., MuJoCo (Zintgraf et al., 2019) or MetaWorld (Yu et al., 2020)), where the latent component is reduced to a few parameters that control goal location or changes in robot morphology. The recent wave of in-context reinforcement learning research (Laskin et al., 2022; Lee et al., 2023; Norman and Clune, 2023; Kirsch et al., 2023; Sinii et al., 2023; Zisman et al., 2023) also uses simple environments for evaluation, such as bandits, navigational DarkRoom & KeyToDoor, or MuJoCo with random projections of observations (Lu et al., 2023; Kirsch et al., 2023). Notable exceptions include XLand (Team et al., 2021, 2023) and Alchemy (Wang et al., 2021). However, XLand is not open source, while Alchemy is built on top of Unity (www.unity.com) and runs at \(30\) FPS, which is not enough for quick experimentation with limited resources.

We hypothesize that the popularity of such simple benchmarks can be attributed to their affordability, as meta-training requires significantly more environmental transitions than traditional single-task RL Team et al. (2023). However, being limited to simple benchmarks prevents researchers from uncovering the limits and scaling properties of the proposed methods. We believe that the solution to this is an environment that does not compromise interestingness and task complexity for the sake of affordability. Therefore, we designed XLand-MiniGrid to include the best from the XLand and Alchemy environments without sacrificing speed and scalability thanks to the JAX (Bradbury et al., 2018) ecosystem.

**Hardware-accelerated environments.** There are several approaches to increasing the throughput of environment experience. The most common approach would be to write the environment logic in low level languages (to bypass Python SIL) for asynchronous collection, as EnvPool Weng et al. (2022) does. However, this does not remove the bottleneck of data transfer between CPU and GPU on every iteration, and the difficulties of debugging asynchronous systems. Porting the entire environment to the GPU, as was done in Isaac Gym (Makoviychuk et al., 2021), Megayrese (Petrenko et al., 2021) or Madrona (Shacklett et al., 2023), can remove this bottleneck, but has the disadvantage of being GPU-only.

Recently, new environments written entirely in JAX Bradbury et al. (2018) have appeared, taking advantage of the GPU or TPU and the ability to compile the entire training loop just-in-time, further reducing the overall training time. However, most of them focus on single-task environments for robotics (Freeman et al., 2021), board games (Koyamada et al., 2023) or combinatorial optimization (Bonnet et al., 2023). The most similar to our work is Craftax (Matthews et al., 2024). It provides much more complex and varied world mechanics, but lacks the flexibility to customize the possible challenges or tasks which is not really suitable for meta-RL research. While in XLand-MiniGrid, thanks to a system of rules and goals, the user can generate many unique tasks of various difficulty.

**Grid World Environments.** Grid world environments have a long history in RL research Sutton and Barto (2018), as they offer a number of attractive properties. They are typically easy to implement, do not require large computational resources, and have simple observation spaces. However, they pose a significant challenge even to modern RL methods, making it possible to test exploration (Zhang et al., 2020), language understanding and generalization (Hanjie et al., 2021; Zholus et al., 2022; Lin et al., 2023; Chevalier-Boisvert et al., 2023), as well as memory (Paischer et al., 2022).

Despite the great variety of benefits of the existing grid world benchmarks, to the best of our knowledge, only the no longer maintained KrazyWorld (Stadie et al., 2018) focuses on meta-learning. Other libraries, such as the popular MiniGrid (Chevalier-Boisvert et al., 2023) and Griddly (Bamford et al., 2020), are not scalable and extensible enough to cover meta-learning needs. In this work, we have attempted to address these needs with new minimalistic MiniGrid-style grid world environments that can scale to millions of steps per second on a single GPU.

**Large-batch RL.** Large batches are known to be beneficial in deep learning (You et al., 2019) and deep reinforcement learning is no exception. It is known to be extremely sample-inefficient and large batches can increase the throughput, speeding up convergence and improving training stability. For example, many of the early breakthroughs on the Atari benchmark were driven by more efficient distributed experience collection (Horgan et al., 2018; Espeholt et al., 2018; Kapturowski et al., 2018), eventually reducing training time to just a few minutes (Stooke and Abbeel, 2018; Adamski et al., 2018) per game. Increasing the mini-batch size can also be beneficial in offline RL (Nikulin et al., 2022; Tarasov et al., 2023).

However, not all algorithms scale equally well, and off-policy methods have until recently lagged behind (Li et al., 2023), whereas on-policy methods, while generally less sample efficient, can scale to enormous batch sizes of millions (Berner et al., 2019; Petrenko et al., 2020) and complete training much faster (Stooke and Abbeel, 2018; Shacklett et al., 2021) in wall clock time. While we do not introduce any novel algorithmic improvements in our work, we hope that the proposed highly scalable XLand-MiniGrid environments will help practitioners perform meta-reinforcement learning experiments at scale faster and with fewer resources.

## 6 Conclusion

Unlike other RL subfields, meta-RL lacked environments that were both non-trivial and computationally efficient. To fill this gap, we developed XLand-MiniGrid, a JAX-accelerated library of grid-world environments and benchmarks for meta-RL research. Written in JAX, XLand-MiniGrid is designed to be highly scalable and capable of running on GPU or TPU accelerators, and can achieve millions of steps per second. In addition, we have implemented easy-to-use baselines and provided preliminary analysis of their performance and generalization, showing that the proposed benchmarks are challenging. We hope that XLand-MiniGrid will help democratize large-scale experimentation with limited resources, thereby accelerating meta-RL research.