# MEQA: A Benchmark for Multi-hop Event-centric Question Answering with Explanations

Ruosen Li, Zimu Wang, Son Quoc Tran, Lei Xia, Xinya Du

Department of Computer Science, University of Texas at Dallas

{ruosen.li, zimu.wang, lei.xia, xinya.du}@utdallas.edu

###### Abstract

Existing benchmarks for multi-hop question answering (QA) primarily evaluate models based on their ability to reason about entities and the relationships between them. However, there's a lack of insight into how these models perform in terms of _both events and entities_. In this paper, we introduce a novel semi-automatic question generation strategy by composing event structures from information extraction (IE) datasets and present the first Multi-hop Event-centric Question Answering (MEQA) benchmark1. It contains (1) 2,243 challenging questions that require a diverse range of complex reasoning over entity-entity, entity-event, and event-event relations; (2) corresponding multi-step QA-format event reasoning chain (explanation) which leads to the answer for each question. We also introduce two metrics for evaluating explanations: completeness and logical consistency. We conduct comprehensive benchmarking and analysis, which shows that MEQA is challenging for the latest state-of-the-art models encompassing large language models (LLMs); and how they fall short of providing faithful explanations of the event-centric reasoning process.

## 1 Introduction

Multi-hop Question Answering (QA) is an important task that challenges NLP models' ability, including large language models (LLMs), to perform multi-step reasoning to answer the given question based on pieces of information (and their relationships) from the context (Yang et al., 2018; Mavi et al., 2022). The ability to conduct multiple reasoning steps is important because it empowers models to understand and perform complicated real-world tasks that require information aggregation in/across documents. Examples include sentence fusion (Brook Weiss et al., 2022), multi-document summarization (Haghighi and Vanderwende, 2009), timeline summarization (Yan et al., 2011), and event logical/temporal reasoning (Yang et al., 2020, 2023, 2024a, 2024b).

However, current research predominantly focuses on entity-centric questions, leaving event structure and event-event relations underrepresented in semantic sources and corresponding QA datasets (Souza Costa et al., 2020). For instance, Figure 1 illustrates an event-centric question encompassing both entities and events, a domain often overlooked by popular QA benchmarks (Ho et al., 2020; Trivedi et al., 2022). Event-centric questions are inherently more complex than entity-centric ones, as discussed in the subsequent section. They demand a compositional understanding of both entity and event knowledge, presenting significant challenges for NLP models, including LLMs, and serving as a robust benchmark for evaluating reasoning capabilities (Souza Costa et al., 2020).

Motivated by this gap in the QA and machine reading comprehension literature, we present the first Multi-hop Event-centric Question Answering (MEQA) dataset. It includes 2,243 questions, requiringa diverse range of reasoning capabilities to answer, for example, event relations, entity bridging, and event listing and counting. In order to bootstrap the annotation process for collecting multi-hop questions, we propose re-purposing the existing information extraction (IE) dataset (i.e., WikiEvents (Li et al., 2021)), which includes annotated event structures (event triggers and entities). Specifically in our work, we design a novel question generation strategy: firstly, examine document-level event structures, followed by linking events into event reasoning chains in the given document, and finally, generate synthetic questions and QA-pair style explanations. Human annotators later curate the synthetic questions and explanations.

The key contributions of our work can be summarized as follows: (1) we collect the first challenging multi-hop event-centric question answering dataset with explanations (MEQA): Our empirical findings underscore the uniqueness of our benchmark that presents novel challenges and a wide range of diversity. Notably, our results reveal a substantial gap between the performance of state-of-the-art language models and human performance, which provides a promising avenue for future research; (2) we introduce a bottom-up process that partially automates the dataset construction process by identifying composable events from the IE dataset; (3) we introduce the completeness and logical consistency metrics to evaluate generated explanations, which are efficient and align well with human judgments; (4) we propose methods that leverage informative structured information (e.g., entity and event) for our new task, which significantly improves performance and generates a more faithful reasoning process.

## 2 Related Work

**Multi-hop QA.** Previous multi-hop QA benchmarks all focus on entity-relation understanding (Das et al., 2019; Saxena et al., 2020; Fang et al., 2020). HotpotQA (Yang et al., 2018) is constructed with a top-down approach by directly crowdsourcing multi-hop questions, which is later shown to be solvable using single-hop shortcuts (Chen and Durrett, 2019). On the other hand, 2WikiMultihopQA (Ho et al., 2020) and Musique (Trivedi et al., 2022) are constructed using the bottom-up approach, where multi-hop questions are composed of single-hop reasoning steps. While they all rely on initially human-written questions, MEA-QG (Pan et al., 2021) generates multi-hop questions by first selecting relevant information from different data sources with a set of operators and then integrating the multiple information to form a question with six reasoning graphs. Our MEQA is constructed with a novel bottom-up approach to bootstrap question generation (QG), as we use the labels from event extraction datasets for identifying composable single-hop steps.

**Explainable Complex QA.** Datasets for QA that feature intricate reasoning questions frequently include comprehensive explanations. These explanations serve the dual purpose of allowing QA systems to learn with stronger supervision and facilitating evaluation of models' ability to explain their predictions (Li and Du, 2023; Du, 2024; Li et al., 2024). HotpotQA (Yang et al., 2018) only includes evidence sentences from the passages. StrategyQA (Geva et al., 2021) includes question decomposition results, but they do not serve as explanations. ScienceQA (Lu et al., 2022) includes free-form explanations for scientific questions. Our MEQA focuses on the most natural free-form explanations and includes single-hop QA pairs as the reasoning path (explanations) that leads to the final answers.

**Event-centric QA.** Simultaneous with the research progress in event understanding, i.e., event extraction (EE) and event relation extraction (ERE) (Du and Cardie, 2020; Du and Ji, 2022; Wang et al., 2022, 2024; Mehta et al., 2022; Peng et al., 2023; Jin et al., 2023; Choudhary and Du, 2024), multiple event-centric QA datasets have been proposed with different characteristics. EventQA

Figure 1: An example of multi-hop event-centric question in MEQA. Models should start reasoning from the _AI-Monitor_ and first locate the _reported_ event; then find all events that happened before the reported event; and finally extract victims in all those events, which are answers to the question.

[Souza Costa et al., 2020], utilizes a random walk on the EventKG [Gottschalk and Demidova, 2019], is designed for accessing semantic data stored within KGs; however, it focuses on KG and is not multi-hop. TORQUE [Ning et al., 2020] includes questions querying commonsense temporal relationships based on the MATRES dataset [Ning et al., 2018]. ESTER [Han et al., 2021] focuses on the challenges of five semantic relations between events. Our MEQA dataset differs from above and it includes event-centric questions that require multi-hop reasoning, namely, temporal/causal relation identification, event trigger-argument entity, and argument-argument understanding.

## 3 Complex Event-centric Questions

### Question Strategies

Following the definition in ACE (Automatic Content Extraction) [Walker et al., 2006] of entities, relations, and events, we define our event-centric questions in Appendix A. Table 1 provides a breakdown of question types within our dataset and the corresponding proportions. To increase diversity, we follow five strategies to annotate multi-hop questions, with examples in Appendix B:

**Event Relation.** In this type of question, language models are tested on the ability of determining the relation between different events, including (1) event-event relation: encompassing causal relations

 p{113.8pt} p{113.8pt} p{113.8pt} p{113.8pt}}  
**Type** & **Context** & **Question** & **Answer** & **Explanation** \\  _Event_ & [...] & _a major general_ was _killed_ [...] _AI-Monitor_ (49.7\%) & Who died before _AI-Monitor_ reported it online? & major general, _Monitor_ is the contactor? _reported_ \\  & _der_ [...] was also reportedly _killed_ [...] & & & 2. What events in \#1 consider, _killed_ iteutenant_ general & \\  _Entity_ & [...] in _Belfast_, at least _1 people_ died. [...] Early today Mr Whitelaw came back to _Belfast_ by _plane_ [...] & Which transportation method did & Plane & 1. Where did _11 people_ die in explosion? _Belfast_ method did & 1. Where did _11 people_ die in explosion? _Belfast_ method did Whitelaw use to reach the _#1? _Plane_ died? & 1. Where did _11 people_ die in explosion? _Belfast_ method did Whitelaw use to reach the _#1? _Plane_ \\  _Event_ & Roadside IED _kills_ Russian _major general_ in Syria. _and_ _Three military personnel_ were _wounded_, [...] _A local commander_ [...] was also reportedly _killed_ [...] & How many victims are mentioned in the whole text? & 5 & 1. What events contain victims? _kills_, _wounded_, _killed_ 2. How many victims are in \#1? **5** \\  _Event_ & Roadside _IED_ kills Russian _major general_ in Syria. _Three military personnel_ were wounded, [...] In April, _two dozen Syrian fighters_ were killed in _IS_ attacks [...] & According to the document, which one, _IED_ or _IS_, killed more victims? & IS & 1. Who was killed by _IED_? _major general_, _three military personnel_ 2. Who was killed by _IS_? _two dozen Syrian fighters_ 3. Which one between \#1 and \#2 killed more victims? _IS_ \\  _Unanswerable_ (2.0\%) & Early today Mr Whitelaw [...] 21 bombs had gone off [...] Mr Whitelaw and Lord Carrington immediately flew back [...] & Who traveled to the place where 21 bombs were manufactured? & (No Answer) & - \\   

Table 1: Types of complex event-based questions in our MEQA dataset. We highlight the answer and its supporting facts in _bold texts_, event triggers in _blue_, bridging entities in _green_. In the _Event comparison_ type, _blue_ and _green_ texts only represent related content for two compared entities.

(e.g., cause) and temporal relations (e.g., before). (2) event-entity relation: relations between events and entities are represented by the role of entities. Figure 2(a) is an example of the strategy.

**Entity Bridging.** This type of multi-hop question tests the connected reasoning ability (Trivedi et al., 2022). The event reasoning chains connect or bridge events from different parts of the documents by entities. They only contain one type of relation: event-entity relation. Events are connected by bridging entities but not direct event-event relations. Figure 2(b) displays an example of the strategy.

**Event Listing and Counting.** This type of question tests the discrete reasoning skills of language models (Dua et al., 2019). In the third example in Table 1, models should first follow similar steps in Strategy 1, including finding all related events and extracting candidate entities, and then calculating the total amount of victims.

**Event Comparison.** This type of question also tests the discrete reasoning skills of language models (Dua et al., 2019). For example, in the fourth sample in Table 1, language models should count the five people killed by the IED and the 24 people killed by IS, and then make the correct comparison that the number of victims by IS (24) is greater than the number of victims by the IED (5).

**Unanswerable Questions.** Questions of this type are those that cannot be answered based only on the corresponding documents. In this question type, we provide annotators with a list of already annotated answerable questions and encourage them to write unanswerable questions that closely resemble the answerable ones.

### Multi-hop Event Reasoning Desiderata

**Multi-hop Reasoning.** Multi-hop event reasoning questions require language models to comprehend the connections between various events within the provided text. Each question entails a multi-step reasoning process, which can be broken down into a sequence of simpler inquiries. A straightforward question can be resolved by (1) referencing a brief passage within the document or (2) employing logical operations based on information gathered from prior steps.

For instance, consider the explanation of the "Event Comparison" strategy outlined in Table 1. The initial two steps can be addressed by referring to concise text excerpts within the document. However, to answer the final question, language models must apply logical operations to the information previously acquired.

**Reasoning Shortcuts** pose a significant challenge to the quality of multi-hop reasoning datasets (Min et al., 2019; Trivedi et al., 2022). Suppose a question is built based on two events \(e_{1}\) and \(e_{2}\), in which \(e_{1}\) starts the entire reasoning chain and \(e_{2}\) contains the final answer. If \(e_{2}\) can be directly and uniquely located in the context by a combination of its trigger and a set of relations, \(e_{1}\) is no longer required. For example, the transported event in Figure 2(b) can be located in the document if we

Figure 2: Partial graph for the Strategy 1 (a) and Strategy 2 (b) examples. The middle part of (b) displays the synthetic sentences and explanations. The bottom part of (b) shows the finalized natural language question and explanation.

assume "plane" only appears once. The second stage of our data collection pipeline, Section 4.2, is specifically designed to eliminate the potential reasoning shortcuts in our dataset.

## 4 Data Collection Pipeline

As illustrated in Figure 3, the construction of MEQA involves four main steps: (1) identifying and linking composable events from IE datasets to event reasoning chains; (2) removing disqualified and merging redundant reasoning chains; (3) converting reasoning chains to synthetic questions and explanations; and (4) human curations.

### Composing Reasoning Chains from Event Structures

We first leverage the event structures from the WikiEvents (Li et al., 2021) (Appendix C) dataset to find composable events (e.g., events that share an entity or have relations in between) by different strategies. The schema of WikiEvent (KAIROS, see Appendix D) describes different types of events (e.g., Travel) and their corresponding argument roles (e.g., transporter and origin). For example, "[transporter] traveled from [origin] to [destination]".

The first step is to filter composable events and link them together to form event reasoning chains, which are event reasoning chains in the QA task. For **Strategy 1**, we randomly select a pair of events, regard one of them as the start event and the other one as the end event. Then, we prompt LLM to generate a temporal or causal relationship between them (Appendix G). For **Strategy 2**, we randomly pick one event as the start event. Then, we search for another event, sharing at least one common entity with the start event (e.g., the exploded event in Figure 2(b)), and concatenate them to form a reasoning chain. Currently, this is a two-hop reasoning chain, and the second event is the end event (e.g., the transported event in Figure 2(b)). For more hops reasoning chains, we repeatedly add more composable events to the end of the reasoning chain. The maximum hop is 4 in this strategy. For the rest of the strategies, we ask annotators to select events manually before writing down questions.

### Relieving Reasoning Shortcut Problem

As described in Section 3.2, uniquely located events lead to reasoning shortcut problems. The primary approach to address this involves ensuring that all event structures, except for the start event, cannot be uniquely identified solely by their triggers and chosen argument roles.

**Automatic Process.** For Strategy 1, during the event sampling process, we record a warning in the data if an end event is unique so annotators can notice the problem and modify it during the annotation process. For event structures used to form an event reasoning chain in Strategy 2, we only keep arguments that connect two events and the corresponding roles and leave out the others. If this minimum event structure can still be uniquely located in all events, we disregard the event and select another one for the current reasoning chain. We keep the reasoning chains containing at least two hops. For other strategies, annotators mitigate the problem by mimicking the above process.

**Manual Correction.** For Strategy 1, annotators remove arguments in the end event to ensure it is not unique in all events (it does not indicate the answer is not unique). The reasoning chain should be discarded if it cannot meet the requirement. If the start event is not unique, annotators will add more

Figure 3: MEQA construction pipeline includes identifying composable events from any IE dataset, composing reasoning chains, mitigating short-cut problems, merging similar reasoning chains to reduce redundant and incomplete questions and, generating synthetic multi-hop questions and QA-based explanations. MEQA pipeline also employs human curation to finalize the high-quality dataset.

information from the document to the event structure to ensure it can be uniquely located. It is the same for start events in all strategies.

### Merging Redundant Reasoning Chains

When constructing event reasoning chains, many reasoning chains share most of the same event structures except the end event, which includes the answer of the corresponding chain. We merge them together to reduce redundant reasoning chains and combine answers to the same chains. For Strategy 1, each reasoning chain only contains two events. If two event chains have the same start event and event-event relation, we can merge the two chains, as shown in Figure 3. Suppose we have two reasoning chains: the wounded event happened before the reported event, and so did the killed event. As reported is the start event in both reasoning chains, we can merge them.

### Generating Synthetic Question Answer Pairs and Explanations

Our goal is to generate multi-hop synthetic questions and explanations based on event reasoning chains. Firstly, we utilize (1) the descriptions of events and argument roles in the KAIROS schema; and (2) arguments from the reasoning chains in Section 4.3 to create synthetic sentences describing the events. Subsequently, we modify these synthetic sentences by substituting the answer argument with an appropriate wh-word, facilitating the generation of synthetic sub-questions. Lastly, we generate the final multi-hop question by composing the sub-questions. The sub-questions naturally consist of the step-by-step explanations. The two blocks in the middle of Figure 2(b) demonstrate the process of generating synthetic question answer pairs & explanations from event reasoning chains.

### Human Curations

These synthetic question answer pairs and explanations in the previous section may not be fluent and grammatically correct. Annotators are required to rephrase both of them in their own speaking style. They can edit anything or rewrite the whole question and all explanations. Moreover, since reasoning chains are sampled from all events in the documents of the dataset, automatically generated single answer (ending node of the reasoning chain) may not be complete. We ask annotators to find all answer spans after they finalize questions and explanations. Figure 2(b) includes the human curation process for turning synthetic questions into natural language questions. The word transported has been rephrased to traveled, which sounds more natural. Details of crowd-sourcing and payment are shown in Appendix E.

## 5 Dataset Analysis

We collect 211 documents from WikiEvents (Li et al., 2021) to create our MEQA dataset and select five workers to annotate the dataset according to the five strategies illustrated in Table 1. After the annotation procedures, the dataset is further split into training, development, and test sets with a proportion of 80%:10%:10%. A summary of the general dataset statistics is shown in Table 7. We analyze our question type distributions and data quality in this section, and some additional analysis are shown in Appendix F.

### Question Type Distribution

The distribution across the five strategies is organized in Table 1. From the table, it is evident that the first two strategies event-bridging and entity-bridging questions account for the largest percentage with a proportion of 49.7% and 37.1%, due to the rich event trigger and argument annotations in WikiEvents. For the rest of the question types, the number of comparison questions is smaller due to the requirement of numbers mentioned in event arguments affiliated with the same events.

### Data Difficulty Evaluation

We design three prompting-based methods to test on datasets. Table 2 illustrates that our MEQA dataset is the most challenging compared to HotpotQA and 2WikiMultihop.

Employing ChatGPT as the foundational method, where each input contains only a context and a question and the output is only an answer (Appendix G), reveals that MEQA presents the greatest challenge. Additionally, our exploration of the effectiveness of the Chain-of-Thought (Wei et al., 2022) style prompt, denoted as "CoT-QA", in which a list of question answer pairs represents reasoning chains before answers. For "CoT-QA (+Entity)" and "CoT-QA (+Event Triggers)", extracted entities and lists of event triggers are leveraged in the context in the prompt correspondingly (Li and Du, 2023). Results are consistent across all three models.

We obtain human performance from 3 annotators (graduate students) on 100 samples randomly chosen from the test split. The accuracy is 88% on average, the upper bound of human performance is 92%, and human agreement (Cohen, 1960; Trivedi et al., 2022) is 79%.

Table 3 presents examples comparing between HotpotQA and MEQA. The upper example is from the entity-centric benchmark HotpotQA. Its reasoning type is "entity bridging". The reasoning starts from "MVP", hops by "Buddy Hield", and ends with the answer "Sacramento Kings". To answer the question in the bottom example, models should start reasoning from the Al-Monitor and first locate the reported event; then find all events that happened before the reported event; and finally extract victims in all those events, which are answers to the question.

## 6 Experimental Results

### Evaluation Metrics

In all experiments, we evaluate both answers and explanations. We compare generated answers with golden answers and report precision, recall, and F1-score. Specifically, we follow the evaluation script from HotpotQA. To evaluate the hallucination problem of explanations, we introduce novel new metrics including "completeness" and "logical consistency" (Huang et al., 2023).

**Completeness** refers to the step-wise accuracy of the matches between the golden explanations and the predicted explanations. We report precision, recall, and F1-score. Predicted explanations can

   & Precision & Recall & F1 \\ 
**ChatGPT (GPT-3.5-turbo-1106)** & & & \\ HotpotQA & 0.745 & 0.779 & 0.733 \\
2WikiMultihop & 0.501 & 0.724 & 0.532 \\ MEQA & 0.190 & 0.536 & 0.238 \\ 
**ChatGPT CoT-QA (+ Entity)** & & & \\ HotpotQA & 0.777 & 0.813 & 0.763 \\
2WikiMultihop & 0.534 & 0.757 & 0.565 \\ MEQA & 0.364 & 0.394 & 0.350 \\ 
**ChatGPT CoT-QA (+ Event Triggers)** & & & \\ MEQA & 0.321 & 0.377 & 0.312 \\ 
**Human** & & & \\ MEQA & 0.783 & 0.836 & 0.811 \\  

Table 2: Performance on different methods over HotpotQA, 2WikiMultihopQA, and MEQA.

    & Paragraph A: The 2015 Diamond Head Classic was a college basketball tournament... \\  & **Buddy Hield** _was named the tournament’s MVP_. \\  & Paragraph B: **Chavano Rainier ”Buddy” Hield** is a Bahamian professional basketball \\  & player for the _Sacramento Kings_ of the NBA... \\  Question & Which team does the player named 2015 Diamond Head Classic’s MVP play for? \\  Answer & Sacramento Kings \\    & [...] nation’s Defense Ministry confirmed that a _major general_ was **killed** in Syria by an \\  & improvised explosive device, _Al-Monitor_ online reported. [...] In 2017, a _lieutenant_ \\  & _general_ was **killed** in the same province, [...] \\  Question & Who **died** before _Al-Monitor_ reported online? \\  Answer & major general, lieutenant general \\  

Table 3: HotpotQA entity-centric example (top) and MEQA event-centric example (bottom).

have multiple formats, such as QA-format or freeform format. We can always split them into smaller sub-steps, such as one question-answer pair or sentence.

For each question, golden reasoning explanations consist of lists of QA pairs obtained via the method in Section 4.1. We design an algorithm to count the number of matches between predicted and golden explanations. The core idea is that we compare explanations step-wise and follow the order of the golden explanation steps. If one golden step is unmatched, it won't be matched later. Predicted explanations are single-hop questions in which sub-step may contain one or two golden steps. The pseudo-code and prompt for calculating the metric is illustrated in Appendix H.

**Logical Consistency** is a reference-free metric. It measures whether each sub-step is consistent with the previous steps or the original question (Golovneva et al., 2023; Huang et al., 2023). If there is no logical contradiction for a sub-step, we count it as logically consistent. Otherwise, we count it as inconsistent. More specially, we input the history and the current step to LLM to determine whether they have any logical contradiction.

### Experimental Settings

We benchmark various methods based on prompting and fine-tuning. In all experiments, we utilize the few-shot method to include demonstrations as input. In the following experiments, we denote context as **C**, question as **Q**, QA-format explanation as **E**, freeform explanation as **FE**, and answer as **A**. We use \(\) to connect the instruction/input contents and output formats. All experiments using LLM are performed using ChatGPT (GPT-3.5-turbo-1106). Detailed prompts are in Appendix G.

We evaluate the performance of a variety of methods on our MEQA dataset. For prompting-based method, we include the following variations: (1) base fewshot (C+Q\(\)A); (2) CoT-QA (C+Q\(\)E+A); and (3) CoT-Freeform (C+Q\(\)FE+A). The "CoT-Freeform" refers to the traditional CoT method in Wei et al. (2022) in which freeform sentences compose the reasoning chains. We also design fine-tuning based methods using T5 (Raffel et al., 2023) as the base model.

In addition, we explore the impact of incorporating _golden_ structured information (such as event graphs) on this event-centric multi-hop question answering task. This investigation is motivated by two key factors: (1) golden structured information represents the upper bound of additional information, and (2) the study conducted by Li et al. (2023) has demonstrated its effectiveness. More specifically, The "Entity" structure exclusively consists of lists of entities. In contrast, the "Entity KG" structure encompasses extracted entity-based knowledge graphs presented in a triple format derived from contexts. The "Event Triggers" structure solely encompasses triggers from corresponding events. Meanwhile, the "Event Triggers + Arguments" structure encompasses all trigger-argument pairs, along with trigger-trigger pairs if temporal or causality relations exist between two events. Finally, the "Full Event KG" structure represents a comprehensive iteration of the aforementioned structure. It incorporates roles between trigger-argument pairs and relations between events, thus providing a more exhaustive representation. We add the variations of the four baselines as different groups.

### Experimental Results

The main results are in Table 4. More experiments using Claude and Llama3 are in Appendix I. As discussed in Li and Du (2023), recall is a better metric for evaluating LLMs' performance. The "CoT-Freeform" method has the highest recall value among baselines of each group, but it does not significantly differ from the "Fewshot" method. The performance of "CoT-QA" is the worst because of the challenge of generating the sequence of QA pair-based explanations which lead to the final answer: (1) QA-based explanations contain fruitful intermediate information and are mandatory outputs; (2) they are harder to generate: all intermediate questions should have logical connections and be faithful to the context. We can also observe that the precision of the "CoT-QA" method is the best. One main reason is that the format of QA-based explanation successfully guided ChatGPT to output short answers. Moreover, the discussion of potential leakages of the dataset is in Appendix J.

Table 5 presents the performance of different question types in the Full Event KG setting using GPT-3.5. "Entity Bridging" achieves the highest overall performance, likely because it is the simplest question type and similar questions are commonly encountered by LLMs. In contrast, "Event Relation" has the lowest recall, as it involves more complex reasoning process.

**Explanations from CoT-QA are more faithful than CoT-Freeform.** Comparing completeness metrics, "CoT-QA" shows higher recall but lower precision, suggesting it produces more explanations matching golden explanations but also generates redundant QA pairs, potentially leading to hallucination issues. In contrast, "CoT-Freeform" explanations, while shorter, are less related to golden explanations, indicating fidelity issues and potential hallucination. Notably, "CoT-QA w/ Full Event KG" outperforms all models in recall and F1 score, implying better alignment with golden explanations. Incorporating comprehensive structured data notably enhances both completeness and faithfulness.

Logical consistency results of the "CoT-QA" and "CoT-Freeform" are not comparable. The reason that "CoT-Freeform" has such a high consistency score is its length. Since ChatGPT generated a shorter reasoning chain without multi-hop explanation, our evaluation script always counts explanations as consistent if the results are correct. Also, even if the result was incorrect, generated explanations always repeated partial original questions. These explanations were always counted as consistent.

**Rich structured information improves performance.** We set several levels of structured information as additional information added in inputs. According to the richness of structured information, we rank additional structures as following: Entity \(\) Event Triggers < Entity KG < Event Triggers + Arguments < Full Event KG.

The performance trend in Table 4 correlates with the richness of additional event-related structured information. Models tend to perform better when they are provided with richer information. Across all groups (grey rows), the performance with "Full Event KG" is always better than others, not only because of its graph structure information but also its explicit relations between all elements. An example is shown in Figure 4, in which the explanations are not applicable unless the complete event KG is supplied. The context and further analysis of additional information is in Appendix K.

**Human Correlation Studies.** We sample 100 questions from "CoT-QA" (Full Event KG) and ask annotators to follow the definition of completeness and logical consistency to judge the quality of generated explanations. Annotators report the number of matched explanation steps, and then calculate the correlations between automatically generated scores and human evaluations.

The correlation of the "completeness" and "logical consistency" metrics are 0.693 and 0.601, respectively, Both correlations exceed 0.35, a threshold deemed moderate according to Taylor (1990). They satisfy the requirement of the most recent LLM reasoning evaluation work by Huang et al. (2023). Thus, our metrics are usable to evaluate explanations for this specific aspect.

    &  &  &  \\  & Precision & Recall & F1 & Precision & Recall & F1 & Consistency \\ 
**T5** (**C4Q\(\)A**) & 0.3012* & 0.2761 & 0.2831 & - & - & - & - \\ _w/ Entity KG_ & 0.3187 & 0.2813 & 0.2942 & - & - & - & - \\  Fewshot (**C+Q\(\)A**) & 0.1902 & 0.5360 & 0.2377 & - & - & - & - \\ _w/ Full Event KG_ & 0.4541 & 0.6355 & 0.4581 & - & - & - & - \\  CoT-QA (**C+Q\(\)E+A**) & 0.2832 & 0.3903 & 0.2940* & 0.1963 & 0.2141* & 0.2001 & 0.6442 \\ _w/ Entity KG_ & 0.3636 & 0.3943 & 0.3500 & 0.2052 & 0.2321 & 0.2145 & 0.6161 \\ _w/ Entity KG_ & 0.3522 & 0.3913 & 0.3344 & 0.1935 & 0.2118 & 0.1978 & 0.6318 \\ _w/ Event Triggers_ & 0.3210 & 0.3773 & 0.3120 & 0.2792 & 0.2946 & 0.2835 & 0.6693 \\ _w/ Event Triggers + Arguments_ & 0.4910 & 0.4878 & 0.4471 & 0.3431 & 0.3698 & 0.3481 & 0.6553 \\ _w/ Full Event KG_ & **0.5299** & 0.5298 & **0.4940** & 0.3989 & **0.4653** & **0.4208** & 0.7327 \\  CoT-Freeform (**C+Q\(\)FE+A**) & 0.1044 & 0.5392* & 0.1494 & 0.3368* & 0.1678 & 0.2161* & **0.9132*** \\ _w/ Full Event KG_ & 0.3680 & **0.6575** & 0.3823 & **0.4566** & 0.2506 & 0.3145 & 0.8889 \\   

Table 4: Performance on all experiments. Four baselines and their further experiments are grouped in the table. In each group, the first line is the performance of the baseline. All the following lines in a group indicate additional contents that are appended after context **C**. **Bold numbers** shows the best results in each column. Numbers with (*) indicate they are the best among all baselines.

  
**GPT-3.5-turbo-1106** &  \\ CoT (Full Event KG) & Precision & Recall & F1 \\  Event Relation & 0.4740 & 0.4492 & 0.4265 \\ Entity Bridging & 0.5539 & 0.5404 & 0.5094 \\ Event Listing and Counting & 0.3895 & 0.5024 & 0.4049 \\ Event Comparison & 0.3682 & 0.4622 & 0.3963 \\   

Table 5: Performance of question types on GPT-3.5.

We also provide a comprehensive error analysis over randomly selected 50 samples. There are two major types of errors: _Incorrect Start Event Identification_ and _Incorrect Event Relation Identification_. Details are in Appendix L.

### Comparing CoT-QA and CoT-Freeform

In our experiment, we encounter scenarios where both CoT-QA and CoT-Freeform methods exhibit effective performance in the process of answer generation. As shown in Table 6, CoT-QA excels in providing detailed, specific answers by breaking down queries into smaller, structured components; however, it may not always align closely with the main focus of the question. CoT-Freeform offers a broader context and tends to address queries more holistically, but it can sometimes lack the specific details in explanations required by the question. These insights suggest that while both approaches have their unique strengths, a tailored application based on the specific needs of the query might yield the best results.

## 7 Conclusion, Limitation, and Societial Impacts

**Conclusion.** We introduce MEQA, the first benchmark containing multi-hop questions requiring reasoning about both entities and events, as well as explanations. It is annotated based on our new QA generation strategy -- utilizing document-level event structures to bootstrap natural questions in an efficient way. We demonstrate the potential and quality of this new dataset through a detailed analysis of its contents. We conduct experiments and show that the benchmark is challenging for a variety of state-of-the-art models, especially that they are prone to incompleteness and inconsistency issues when generating the reasoning explanations.

**Limitation.** Until now, MEQA only covers English documents which limits the advancement of event-centric multi-hop QA in multilingual scenarios. In the future, we plan to extend MEQA to evaluate the performance of language models in more languages.

**Societial Impacts.** Our benchmark could facilitate the development of more intelligent multi-hop event-centric QA models, which has substantial impacts in a variety of applications, e.g., news understanding, emergency response, etc.

  
**Question:_Which object was mentioned in a discussion before given to Dzhokhar?_** \\
**Answer:**_Pistol_ \\ 
**CoT-QA Explanation:** \\
1. What event contains Dzhokhar as the recipient of an object? borrowing \\
2. What event before \#1 involves the object? discuss \\
3. What was the object in \#2? Pistol \\
**Answer:**_Pistol_ \\ 
**CoT-Freeform Explanation:** \\ Before giving the object to Dzhokhar, the object was discussed among [...] \\
**Answer:**_Pistol_ \\   

Table 6: An example of comparing two CoT methods. Context is in Table 13.

Figure 4: An example of the significance of additional structured information.