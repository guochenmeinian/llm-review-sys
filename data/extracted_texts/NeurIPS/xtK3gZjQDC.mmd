# Towards Human-AI Complementarity

with Prediction Sets

 Giovanni De Toni

Fondazione Bruno Kessler & University of Trento

Trento, Italy

giovanni.detoni@unitn.it

&Nastaran Okati

Max Planck Institute for Software Systems

Kaiserslautern, Germany

nastaran@mpi-sws.org

&Suhas Thejaswi

Max Planck Institute for Software Systems

Kaiserslautern, Germany

thejaswi@mpi-sws.org

&Eleni Straitouri

Max Planck Institute for Software Systems

Kaiserslautern, Germany

estraitouri@mpi-sws.org

&Manuel Gomez-Rodriguez

Max Planck Institute for Software Systems

Kaiserslautern, Germany

manuelgr@mpi-sws.org

The author contributed to this paper during an internship at the Max Planck Institute for Software Systems.

###### Abstract

Decision support systems based on prediction sets have proven to be effective at helping human experts solve classification tasks. Rather than providing single-label predictions, these systems provide sets of label predictions constructed using conformal prediction, namely prediction sets, and ask human experts to predict label values from these sets. In this paper, we first show that the prediction sets constructed using conformal prediction are, in general, suboptimal in terms of average accuracy. Then, we show that the problem of finding the optimal prediction sets under which the human experts achieve the highest average accuracy is \(\)-hard. More strongly, unless \(=\), we show that the problem is hard to approximate to any factor less than the size of the label set. However, we introduce a simple and efficient greedy algorithm that, for a large class of expert models and non-conformity scores, is guaranteed to find prediction sets that provably offer equal or greater performance than those constructed using conformal prediction. Further, using a simulation study with both synthetic and real expert predictions, we demonstrate that, in practice, our greedy algorithm finds near-optimal prediction sets offering greater performance than conformal prediction.

## 1 Introduction

In recent years, there has been increasing excitement about the potential of decision support systems based on machine learning to help human experts make more accurate predictions in a variety of application domains, including medicine, education and science . In this context, the ultimate goal is human-AI complementarity--the predictions made by the human expert who uses a decision support system are more accurate than the predictions made by the expert on their own and by the classifier used by the decision support system .

The conventional wisdom is that to achieve human-AI complementarity, decision support systems should help humans understand when and how to use their predictions to update their own. As a result,a flurry of empirical studies has analyzed how factors such as confidence, explanations, or calibration influence when and how humans use the predictions provided by a decision support system . Unfortunately, these studies have been so far inconclusive and it is yet unclear how to design decision support systems that achieve human-AI complementarity .

In this context, Straitouri et al.  have recently argued, both theoretically and empirically, that an alternative type of decision support systems may achieve human-AI complementarity, by design. Rather than providing a single label prediction and letting a human expert decide when and how to use the predicted label to update their own prediction, these systems provide a set of label predictions, namely a prediction set, and ask the expert to predict a label value from the set.2 To construct each prediction set, these systems rely on a conformal predictor . The conformal predictor first computes a non-conformity score for each potential label value using the output provided by a classifier (_e.g._, the softmax scores), and then adds a label value to the prediction set if its non-conformity score is below a data-driven threshold computed using a calibration set. Further, to optimize the performance of these systems, Straitouri et al. have introduced several methods to efficiently find the optimal value of the threshold used by the conformal predictor.3 However, it is unclear whether the optimal prediction sets maximizing the average accuracy achieved by an expert who uses such systems can always be constructed using a deterministic threshold rule as the one used by a conformal predictor. Motivated by this observation, in this work, our goal is to understand how to construct optimal prediction sets under which human experts achieve the highest average accuracy.

**Our contributions.** We first demonstrate that there exist (many) data distributions for which the optimal prediction sets under which the human experts achieve the highest average accuracy cannot be constructed using a conformal predictor. Then, we show that the problem of finding the optimal prediction sets is \(\)-hard by using a reduction from the \(k\)-clique problem . More strongly, unless \(P=\), we show that the problem is hard to approximate to any factor less than the size of the label set. However, we introduce a simple and computationally efficient greedy algorithm that, for a large class of non-conformity scores and expert models parameterized by a mixture of multinomial logit models (MNLs), is guaranteed to find prediction sets that provably offer equal or greater performance than those constructed using conformal prediction. Moreover, using a simulation study with both synthetic and real expert predictions, we demonstrate that, in practice, our greedy algorithm finds near-optimal prediction sets offering greater performance than conformal prediction. We have released an open-source implementation of our greedy algorithm as well as the code and data used in our experiments at https://github.com/Networks-Learning/towards-human-ai-complementarity-predictions-sets.

**Further related work.** Our work builds upon further related work on set-valued predictors, assortment optimization, and learning under algorithmic triage.

The literature on set-valued predictors aims to develop predictors that, for each sample, output a set of label values, namely a prediction set . Set-valued predictors have not been designed nor evaluated by their ability to help human experts make more accurate predictions , except for a few notable exceptions . These exceptions provide empirical evidence that conformal predictors, a specific type of set-valued predictors, may help human experts make more accurate predictions. Among these exceptions, the work by Straitouri et al. , which we have already discussed previously, is most related to ours. In this context, it is also worth noting that a recent theoretical study has argued that prediction sets may also help experts create more accurate rankings .

The literature on assortment optimization aims to develop methods to help a seller select a subset of products from a universe of substitutable products, namely an assortment, with maximum expected revenue . Within this literature, the work most closely related to ours tackles the assortment optimization problem under customization , where there are different types of customers and each type of customer chooses products following a different multinomial logit model. More specifically, by mapping products to label values, types of customers to ground truth label values, and revenue to accuracy, one could think of our problem as an assortment optimization problem under customization. However, in the assortment optimization problem under customization, the type of each customer is known and thus may be offered different subsets of products whereas, in our problem, the ground truth label is unknown. As a result, (the complexity of) our problem and our technical contributions are fundamentally different.

The literature on learning under algorithmic triage aims to develop classifiers that make predictions for a given fraction of the samples and leave the remaining ones to human experts, as instructed by a triage policy [37; 38; 39; 40; 41; 42]. In contrast, in our work, for each sample, a classifier is used to construct a prediction set and a human expert needs to predict a label value from the set. In this context, it is also worth noting that learning under algorithmic triage has been extended to reinforcement learning settings [43; 44; 45; 46].

## 2 Decision Support Systems Based on Prediction Sets

Given a multiclass classification task where, for each task instance, a human expert needs to predict the value of a ground truth label \(y=\{1,,L\}\), we focus on the design of a decision support system that, given a set of features \(x\), helps the expert by narrowing down the set of potential label values to a subset of them \((x)\). Here, similarly as in Straitouri et al. [18; 19], we assume that, for any instance with features \(x\), the system asks the expert's prediction \(\) to belong to the prediction set \((x)\), _i.e._, \((x)\). The key rationale for restricting the expert's agency is that, if we would allow the expert to predict label values from outside the prediction set, a good performance would depend on the expert developing a good understanding of when to predict a label from the prediction set. In this context, it is worth highlighting that Straitouri et al.  run a large-scale human subject study to compare the above setting against an alternative setting where experts are allowed to predict label values from outside the prediction sets. They found that, in the alternative setting, the number of predictions in which the prediction sets do not contain the true label and the experts succeed is consistently smaller than the number of predictions in which the prediction sets contain the true label and the experts fail. As a consequence, in the alternative setting, experts perform worse. Refer to Figure 1 for an illustration of the decision support system.

Then, for any \(x\), our goal is to find the optimal prediction set \(^{*}(x)\) that maximizes the average accuracy of the expert's prediction,4_i.e._,

\[^{*}(x)=*{argmax}_{}g (\,|\,x) g(\,|\,x)=_{Y  P(Y\,|\,X), P_{}(\,|\,X,Y)}[ \{=Y\}\,|\,X=x],\] (1)

where \(P(Y\,|\,X)\) denotes the conditional distribution of the ground-truth label \(Y\) and \(P_{}(\,|\,X,Y)\) denotes the conditional distribution of the expert's predictions \(\) under the prediction set \(\).5

Figure 1: Our automated decision support system. Given an instance with a feature vector \(x\), the system \(\) helps the expert by automatically narrowing down the set of potential label values to a prediction set \((x)\). The system asks the expert to predict a label value \(\) from \((x)\).

## 3 On the Suboptimality of Conformal Prediction

Given a user-specified parameter \(\), a conformal predictor uses a choice of non-conformity score \(s\,:\,\) and a calibration set \(_{}=\{(x_{i},y_{i})\}_{i=1}^{m}\), where \((x_{i},y_{i}) P(X)P(Y X)\), to construct the prediction sets \((X)=_{}(X)\) as follows:

\[_{}(X)=\{y\,|\,s(X,y)_{}\},\] (2)

where \(_{}\) is the \((m+1)(1-)/m\) empirical quantile of the non-conformity scores of the samples in the calibration set \(_{}\). By using the above construction, the conformal predictor guarantees that the probability that the true label \(Y\) belongs to the subset \(_{}(X)\) is almost exactly \(1-\), _i.e._, \(1- P(Y_{}(X)) 1-+1/(m+1)\), as shown elsewhere [20; 21].

Under common choices of non-conformity scores [48; 49], there are many data distributions for which the optimal prediction set under which the human expert achieves the highest accuracy cannot be constructed using a conformal predictor. Consider the following example where \(=\{1,2,3\}\) and,

\[P(Y=y\,|\,X=x)=0.4&y=1\\ 0.35&y=2\,\,P_{}(=\,|\,X=x,Y=y)= y}}{_{y^{}}C_{y^{}y}},\\ 0.25&y=3\]

where \(C_{1,1}=C_{2,1}=C_{3,1}=0.33\), \(C_{1,2}=C_{1,3}=0.4\), \(C_{2,2}=C_{3,3}=0.6\), and \(C_{3,2}=C_{2,3}=0\). A brute force search reveals that the optimal prediction set is \(\{2,3\}\) and, under this set, the expert achieves accuracy \(0.6\). Now, assume we have access to a perfectly calibrated classifier \(f(x)^{L}\), _i.e._, for all \(x\) and \(y\), it holds that \(f_{y}(x)=P(Y=y\,|\,X=x)\). Then, for any choice of \(\), as long as the non-conformity scores rank the label set in decreasing order of \(f_{y}(x)\), the prediction set provided by conformal prediction can only be among the sets \(\{,\{1\}\,,\,\{1,2\}\,,\,\{1,2,3\}\}\). Among these sets, the set under which the expert achieves the highest accuracy is \(\{1,2,3\}\) and, under this set, the expert achieves accuracy \(0.49<0.6\).

Motivated by the above example, one may think of closing the above performance gap by incorporating information about the distribution of experts' predictions in the definition of the non-conformity score. However, we cannot expect to fully close the performance gap since, as we will show next, the problem of finding the optimal prediction sets is \(\)-hard to solve and approximate to any factor less than the size of the label set \(\).

## 4 On the Hardness of Finding the Optimal Prediction Sets

In this section, we first show that, given \(x\), we cannot expect to find the optimal prediction set \(^{*}(x)\) that maximizes the accuracy of the expert's prediction in polynomial time:6

**Theorem 1**: _The problem of finding the optimal prediction set, as defined in Eq. 1, is \(\)-hard._

In the proof of the above theorem, we first reduce the \(k\)-clique problem,7 which is known to be \(\)-complete , to an instance of the problem of deciding whether there exists a prediction set \(\) such that \(g(\,|\,x) B\) given a constant \(B>0\). More specifically, given a \(k\)-clique problem defined over a graph \(=(,)\) with \(k||\), we reduce it to an instance of the above decision problem in which \(=\), \(B=|}\) and, for all \(y\), we have that \(P(Y=y\,|\,X=x)=|}\) and

\[P_{}(=\,|\,X=x,Y=y)=y}}{_{y^{ }}C_{y^{}y}}\,\,C_{y^{}y}=0&(y^{ },y)\\ 1/_{}(y)&,\] (3)

and \(_{}(y)\) denotes the number of vertices that are not adjacent to \(y\). Then, since the above decision problem can be trivially reduced to the problem of finding the optimal prediction set (in polynomial time), we conclude that the problem is \(\)-hard.

Motivated by the above result, we may think in looking for desirable properties for the objective function \(g(\,|\,x)\) such as monotonicity and submodularity,8 which would allow for the design ofapproximation algorithms with non-trivial approximation guarantees . Unfortunately, there are many data distributions for which the objective function is neither monotone nor submodular. For example, assume \(=\{1,2,3\}\),

\[P(Y=y\,|\,X=x)=0.4&y=1\\ 0.35&y=2\\ 0.25&y=3P_{}(=\,|\,X=x,Y=y)= y}}{_{y^{}}C_{y^{}y}},\]

where \(C_{1,1}=0.2\), \(C_{1,2}=C_{2,1}=C_{1,3}=C_{3,1}=0.4\), \(C_{2,2}=C_{3,3}=0.6\) and \(C_{2,3}=C_{3,2}=0\). For \(=\{1\}=\{1,2\}\), it holds that \(g(\,|\,x)=0.4>g(\,|\,x)=0.34\) and \(g(\,|\,x)=0.34<g(\,|\,x)=0.44\), and thus we can conclude it is not monotone. Moreover, it also holds that \(g(\{3\}\,|\,x)-g(\,|\,x)=-0.116<g(\{ 3\}\,\,|\,x)-g(\,|\,x)=0.096\), and thus we can conclude it is not submodular.

In fact, the following theorem shows that we cannot expect to find a polynomial-time algorithm to find a non-trivial approximation to our problem:

**Theorem 2**: _The problem of finding the optimal prediction set, as defined in Eq. 1, is \(\)-hard to approximate to any factor less than the size \(L\) of the label set \(\)._

In the proof of the above theorem, we first show that, given a polynomial-time \(\)-approximation algorithm for the problem of finding the optimal prediction set, we can obtain a polynomial-time \(\)-approximation algorithm for the problem of finding the maximum clique in a graph \(=(,)\).9 Then, since it is known that, for any \(>0\), the latter problem is \(\)-hard to approximate to a factor \(||^{1-}\), we can conclude that the problem of finding the optimal prediction set is \(\)-hard to approximate to a factor \(||^{1-}\).

While the above hardness results may be discouraging, in what follows, we will introduce a simple greedy algorithm that provably offers equal or greater performance than conformal prediction for a large class of non-conformity scores and expert models, and in practice, often succeeds at finding (near-)optimal prediction sets.

**A simple greedy algorithm.** Given a sample with features \(x\) and a prediction set \(\), our greedy algorithm estimates the accuracy of the expert's prediction, as defined in Eq. 1, using the following estimator:

\[(\,|\,x)=_{y}(x)}_{(a)} }{_{y^{}}C_{y^{}y}}}_{(b)}\] (4)where (a) approximates \(P(Y=y\,|\,X=x)\) using a well-calibrated classifier \(f(x)^{L}\) and, similarly as in Straitouri et al. , (b) approximates \(P_{}(=y\,|\,X=x,Y=y)\) using a mixture of multinomial logit models (MNLs) parameterized by the confusion matrix of the predictions made by the expert on their own, _i.e._, \(C_{y^{}y}=P_{}(=y^{}\,|\,Y=y)\).

The greedy algorithm first ranks each label value \(y\) using the output \(f_{y}(x)\) of the classifier. Let \(y_{(1)},,y_{(L)}\) be the label values ordered according to such a ranking, where \(_{(i)}\) denotes the \(i\)-th label value in the ranking and \(f_{y_{(i)}}(x) f_{y_{(i)}}(x)\) for all \(i<j\). Then, it runs \(L\) rounds and, at each \(k\)-th round, it starts from the prediction set \(_{k}=\) and iteratively adds to \(_{k}\) the label value \(y\{y_{(1)},,y_{(k)}\}_{k}\) that provides the maximum marginal gain \((_{k}\{y\}\,|\,x)-(_{k}\,|\,x)\) until it exhausts the set \(\{y_{(1)},,y_{(k)}\}\). Moreover, at each iteration and round, it keeps track of the set with the highest objective value. At each of the \(L\) runs of the greedy algorithm, at most \(L\) elements are added to the set \(\), and adding each element needs at most \(L\) times computing the marginal gain \((\{y\}\,|\,x)-(\,|\,x)\), which takes \(O(L)\) to compute. Hence, our algorithm has an overall complexity of \(O(L^{4})\). See Appendix B for a detailed running time analysis. Algorithm 1 provides a pseudocode implementation of the procedure.

Importantly, the prediction sets provided by the greedy algorithm are guaranteed to achieve higher objective value \(\) than those provided by any conformal predictor using a non-conformity score \(s(x,y)\) that is nonincreasing with respect to \(f_{y}(x)\), as formalized by the following proposition:10

**Proposition 1**: _For any \(x\), let \(\) be the prediction set provided by Algorithm 1 and \(_{cp}\) be the prediction set provided by any conformal prediction with a non-conformity score \(s(x,y)\) that is nonincreasing with respect to \(f_{y}(x)\), then, it holds that \((\,|\,x)(_{cp}\,|\,x)\)._

## 5 Experiments with Synthetic Data

In this section, we compare the average accuracy achieved by different simulated human experts using prediction sets constructed with our greedy algorithm (Algorithm 1), brute force search, and conformal prediction on several synthetic multiclass classification tasks where the experts and the classifier used by the greedy algorithm, brute force search, and conformal prediction achieve different accuracies on their own.

**Experimental setup**. We create several synthetic multiclass classification tasks, each with \(n=20\) features per sample and varying difficulty. Out of 20 features per sample, only \(d=4\) of these features are _informative11_ while the rest are drawn at random. Refer to Appendix C for more details about the classification tasks. For each classification task, we generate \(19{,}000\) samples, which we split into a training set (\(16{,}000\) samples), a calibration set (\(1000\) samples), a validation set (\(1000\) samples) and a test set (\(1000\) samples).

We use the first half of the samples in the training set to train a multinomial logistic regression model \(f(x)\). This model is used by the greedy algorithm, brute force search and conformal prediction. It achieves a different average test accuracy \(P(Y^{}=Y)\), depending on the difficulty of the classification task. We use the second half of the samples in the training set to train another multinomial logistic regression model \((x)\). However, during the training of this model, we modify the value \(a\) of one of the (informative) features of each training sample to \((1-)a+\), where \((0,1)\) and \(\) controls the average accuracy of the resulting model. Then, we use the (estimated) confusion matrix \(()\) of the predictions made by \((x)\) to model (the predictions made by) the simulated expert by means of a mixture of MNLs, _i.e._, \(P_{}(=y\,|\,X=x,Y=y)=()}{_{y^{} }C^{}y^{}()}\).

Further, we use the calibration set to calibrate the (softmax) outputs of the logistic regression model \(f\) using top-\(k\)-label calibration  with \(k=5\). We also use it to estimate the confusion matrices \(()\) that parameterize the mixture of MNLs used to model (the predictions made by) the simulated human expert, and calculate the quantile \(_{}\) used by conformal prediction. Finally, we use the test set to evaluate the average accuracy achieved by the simulated expert using prediction sets constructed with our greedy algorithm, brute force search and conformal prediction. Here, note that our greedy algorithm and brute force search have access to the true mixtures of MNLs used to model the simulated human expert. In our experiments, we implement conformal prediction using several non-conformity scores:

\[s(x,y) =1-f_{y}(x)]})}, s (x,y)=_{y^{}:f_{y^{}}(x) f_{y}(x)}f_{y^{}}(x)]})},\] \[s(x,y) =f_{y}(x)+_{y^{}:f_{y^{}}(x) f_{y}(x)}f_{y^{ }}(x)+_{raps}(o(x,y)-k_{reg})^{+}]})},\] \[s(x,y) =_{y^{}}f_{y^{}}(x)&o(x,y)=1\\ _{y^{}}f_{y^{}}(x)+_{saps}(o(x,y)-2)&o(x,y)>1 ]})},\]

where \(o(x,y)=|\{y^{}:f_{y^{}}(x) f_{y}(x)\}|\) denotes the ranking of label \(y\) according to \(f_{y}(x)\) and we decided to omit the randomization for Aps, Raps and Saps as it is only required to achieve exact \(1-\) coverage and it did not have an influence on the empirical average accuracy achieved by the simulated human experts in our experiments. For Raps and Saps, we run the procedure outlined in Appendix E in Angelopoulous et al.  to optimize the additional hyperparameters, \(k_{regs}\) and \(_{raps}\), for Raps, and \(_{saps}\) for Saps, using the validation set. Further, for each non-conformity score and classification task, we report the results for the \(\) value under which the expert achieves the highest average test accuracy and, to avoid empty sets, we always include the label value with the lowest non-conformity score in the prediction sets. In this context, note that, in practice, one would need to select \(\) using a held-out dataset, however, our evaluation aims to show how our greedy algorithm

   \(\) & **Method** & \(P(Y^{}=Y)=0.3\) & \(P(Y^{}=Y)=0.5\) & \(P(Y^{}=Y)=0.7\) & \(P(Y^{}=Y)=0.9\) \\   & Naive & \(0.340\,{}0.014\) & \(0.588\,{}0.015\) & \(0.799\,{}0.013\) & \(0.944\,{}0.006\) \\  & Aps & \(0.341\,{}0.013\) & \(0.587\,{}0.013\) & \(0.804\,{}0.015\) & \(0.941\,{}0.006\) \\  & Raps & \(0.341\,{}0.013\) & \(0.587\,{}0.013\) & \(0.804\,{}0.014\) & \(0.941\,{}0.006\) \\  & Saps & \(0.340\,{}0.015\) & \(0.585\,{}0.012\) & \(0.804\,{}0.015\) & \(0.940\,{}0.008\) \\  & Greedy & \(0.015}\) & \(0.014}\) & \(0.012}\) & \(0.005}\) \\  & None & \(0.281\,{}0.018\) & \(0.485\,{}0.019\) & \(0.693\,{}0.018\) & \(0.883\,{}0.008\) \\   & Naive & \(0.328\,{}0.014\) & \(0.564\,{}0.012\) & \(0.774\,{}0.014\) & \(0.932\,{}0.007\) \\  & Aps & \(0.329\,{}0.012\) & \(0.565\,{}0.010\) & \(0.787\,{}0.013\) & \(0.932\,{}0.008\) \\  & Raps & \(0.330\,{}0.012\) & \(0.566\,{}0.010\) & \(0.787\,{}0.013\) & \(0.932\,{}0.008\) \\  & Saps & \(0.329\,{}0.013\) & \(0.563\,{}0.008\) & \(0.787\,{}0.014\) & \(0.932\,{}0.009\) \\  & Greedy & \(0.015}\) & \(0.010}\) & \(0.012}\) & \(0.004}\) \\  & None & \(0.261\,{}0.016\) & \(0.446\,{}0.013\) & \(0.644\,{}0.019\) & \(0.843\,{}0.011\) \\   & Naive & \(0.319\,{}0.012\) & \(0.534\,{}0.013\) & \(0.737\,{}0.013\) & \(0.908\,{}0.006\) \\  & Aps & \(0.320\,{}0.008\) & \(0.542\,{}0.012\) & \(0.759\,{}0.015\) & \(0.913\,{}0.008\) \\  & Raps & \(0.320\,{}0.008\) & \(0.542\,{}0.012\) & \(0.760\,{}0.015\) & \(0.914\,{}0.008\) \\  & Saps & \(0.319\,{}0.009\) & \(0.534\,{}0.012\) & \(0.760\,{}0.014\) & \(0.915\,{}0.009\) \\  & Greedy & \(0.011}\) & \(0.010}\) & \(0.013}\) & \(0.006}\) \\  & None & \(0.238\,{}0.011\) & \(0.380\,{}0.015\) & \(0.540\,{}0.018\) & \(0.716\,{}0.013\) \\   & Naive & \(0.314\,{}0.015\) & \(0.517\,{}0.011\) & \(0.714\,{}0.015\) & \(0.894\,{}0.012\) \\  & Aps & \(0.316\,{}0.013\) & \(0.525\,{}0.009\) & \(0.733\,{}0.014\) & \(0.895\,{}0.010\) \\   & Raps & \(0.316\,{}0.013\) & \(0.525\,{}0.009\) & \(0.734\,{}0.015\) & \(0.896\,{}0.010\) \\   & Saps & \(0.315\,{}0.014\) & \(0.517\,{}0.011\) & \(0.734\,{}0.015\) & \(0.896\,{}0.009\) \\   & Greedy & \(0.013}\) & \(0.011}\) & \(0.015}\) & \(0.007}\) \\   & None & \(0.214\,{}0.017\) & \(0.303\,{}0.014\) & \(0.382\,{}0.021\) & \(0.452\,{}0.021\) \\   

Table 1: Empirical average test accuracy achieved by four different (simulated) human experts, each with a different noise value \(\), on their own (None) and using prediction sets constructed with conformal prediction (Naive, Aps, Raps and Saps) and with the greedy algorithm (Greedy) on four synthetic classification tasks. In each classification task, the classifier \(f\) used by conformal prediction and the greedy algorithm achieves a different average accuracy \(P(Y^{}=Yimproves over conformal prediction for _any_ value of \(\). Finally, we repeat each experiment ten times and, each time, we sample different training, calibration, validation and test sets.

**Results.** We first estimate the average test accuracy achieved by four different (simulated) human experts, each with a different \(\) value, on four classification tasks where the classifier \(f\) achieves a different average accuracy \(P(Y^{}=Y)\). We report their average test accuracy on their own (None) and when using prediction sets constructed with conformal prediction (Naive, Aps, Raps and Saps), our greedy algorithm (Greedy) and brute force search (Brute Force Search). Table 1 summarizes the results, where we have not included brute force search because it achieves the same performance as our greedy algorithm. The results show that, using the greedy algorithm to construct prediction sets, the experts consistently achieve the highest average accuracy across classification tasks. Moreover, the results also show that, under the prediction sets constructed using the greedy algorithm, the average accuracy achieved by the expert degrades gracefully as \(\) increases whereas, under the prediction sets constructed using conformal prediction, the average accuracy degrades significantly. Refer to Appendix D for additional results for \(L\{25,50\}\) showing that the relative gain in average accuracy offered by the greedy algorithm increases with the number of labels and noise \(\). Refer to Appendix E for additional results showing that the empirical average coverage achieved by the prediction sets constructed using conformal prediction and our greedy algorithm may be a bad proxy for estimating the average accuracy achieved by human experts using prediction sets.

To better understand why the prediction sets constructed by the greedy algorithm help human experts achieve higher average accuracy than those constructed by conformal prediction, we now look closer into the structure of the prediction sets. Given a ground truth-label \(Y=y\), let \(=*{argmax}_{y^{} y}C_{y^{}y}\) be the label that is most frequently mistaken with \(y\). Then, we estimate the empirical conditional probability that a prediction set includes \(\{y,\}\) given \(Y=y\) with the greedy algorithm and conformal prediction. Figure 2 summarizes the results for \(=0.7\) and \(P(Y^{}=Y)=0.7\) and \(y\{0,2,6,8\}\). Appendix F includes additional results for other configurations. The results show that, with the greedy algorithm, the empirical probability that a prediction set includes \(\{y,\}\) given \(Y=y\) is much lower (_i.e._, \(2\)-\(3\)x lower) than with conformal prediction despite it creates overall larger prediction sets.

## 6 Experiments with Real Data

In this section, we compare the average accuracy achieved by a simulated human expert using prediction sets constructed with our greedy algorithm, brute force search and conformal prediction on a real multiclass classification task over noisy natural images. The simulated human expert follows

Figure 2: (Left) Confusion matrix \(\) for the predictions made by a (simulated) human expert on their own. The label \(=*{argmax}_{y^{} y}C_{y^{}y}\) that is most frequently mistaken with the ground truth-label \(y\) is highlighted in red for \(y\{0,2,6,8\}\). (Right) Empirical conditional probability that a prediction set includes \(\{y,\}\) given \(Y=y\) with conformal prediction (Naive, Aps, Raps and Saps) and our greedy algorithm (Greedy). In both panels, \(=0.7\) and \((Y^{}=Y)=0.7\).

the mixture of MNLs introduced in Eq. 4, which is parameterized by the (estimated) confusion matrix of the predictions made by real human experts on their own.12

**Experimental setup.** We experiment with the ImageNet16H dataset , which was created using \(1,\!200\) natural images from the ImageNet Large Scale Visual Recognition Challenge (ILSRVR) 2012 dataset . More specifically, in the ImageNet16H dataset, each of the above images was used to create four noisy images with different levels of phase noise distortion \(\{80,95,110,125\}\) and the same ground-truth label \(y\) from a label set \(\) of size \(n=16\). In addition, for each noisy image, the dataset contains (approximately) six label predictions made by human experts on their own. In our experiments, we run and evaluate each method separately by grouping the above noisy images (and expert predictions) according to their level of noise. For each group of images and method, we use the deep neural network classifier VGG-19  after \(10\) epochs of fine-tuning as provided by Steyvers et al. . Further, we randomly split the images (and expert predictions) in each group into two disjoint subsets, a calibration set (\(800\) images), and a test set (\(400\) images). The accuracy of the (pretrained) VGG-19 classifier on the test set is \(0.900 0.014\) (\(=80\)), \(0.895 0.009\) (\(=95\)), \(0.857 0.016\) (\(=110\)) and \(0.792 0.016\) (\(=125\)). We use the calibration set to (i) calibrate the (softmax) outputs of the VGG-19 scores using top-k-label calibration with \(k=5\), (ii) estimate the confusion matrix \(\) that parameterizes the mixture of MNLs used to model the simulated human expert, and (iii) calculate the quantile \(_{}\) used by conformal prediction13.We use the test set to evaluate the average accuracy the simulated expert achieves using prediction sets constructed with our greedy algorithm, brute force search and conformal prediction. Here, note that, similarly to in the experiments in synthetic data, our greedy algorithm and brute force search have access to the true mixture of MNLs used to model the simulated human expert. We implement conformal prediction using the same non-conformity scores used in the experiments with synthetic data and, for each non-conformity score and group of images, we report the results for the \(\) value under which the expert achieves the highest average test accuracy. In Appendix H, we report results for all \(\) values. To obtain error bars, we repeat each experiment \(10\) times, sampling different calibration and test sets.

**Results.** Table 2 and Figure 3 show the average test accuracy and complementary cumulative distribution (cCDF) of the test per-image accuracy achieved by a simulated human expert using the prediction sets constructed with conformal prediction (Naive, APS, Raps and Saps), our greedy algorithm (Greedy) and brute force search (Brute Force Search) for different values of noise \(\). The results show that, similarly as in our experiments with synthetic data, the greedy algorithm achieves the same performance as brute force search. Moreover, they also show that, using greedy algorithm to construct prediction sets, the expert achieves the highest average accuracy in all groups of images except the group with \(=80\), where both the greedy algorithm and one of the conformal predictors offer comparable performance. Similarly as in the synthetic experiments, refer to Appendix E for additional results regarding the empirical average coverage achieved by the prediction sets constructed using conformal prediction and our greedy algorithm.

   Method & \(=80\) & \(=95\) & \(=110\) & \(=125\) \\  Naive & \(\) & \(0.946 0.008\) & \(0.919 0.006\) & \(0.860 0.008\) \\ Aps & \(0.944 0.004\) & \(0.932 0.005\) & \(0.902 0.008\) & \(0.852 0.010\) \\ Raps & \(0.950 0.009\) & \(0.943 0.010\) & \(0.914 0.010\) & \(0.849 0.011\) \\ Saps & \(0.953 0.010\) & \(0.942 0.009\) & \(0.918 0.009\) & \(0.855 0.007\) \\ Greedy & \(\) & \(\) & \(\) & \(\) \\ None & \(0.900 0.002\) & \(0.859 0.003\) & \(0.771 0.005\) & \(0.603 0.007\) \\   

Table 2: Average test accuracy achieved by a (simulated) human expert on their own (None), and using the prediction sets constructed with conformal prediction (Naive, Aps, Raps and Saps), and our greedy algorithm (Greedy) on the ImageNet16H dataset. We do not include brute force search because it achieves the same performance as the greedy algorithm. Each cell shows the average and standard deviation over \(10\) runs. We denote the best results in bold.

## 7 Discussion and Limitations

In this section, we discuss several assumptions and limitations of our work, which open up interesting avenues for future work.

**Hardness analysis.** In our hardness analysis, our reduction utilizes an instance of our problem in which, for every prediction set, the predictions made by experts follow a mixture of MNLs. As an immediate consequence, this implies that, in general, the problem of finding the prediction set \(\) that maximizes \((\,|\,x)\) is \(\)-hard to approximate. However, in our experiments, the greedy algorithm is almost always able to find such a set \(\). As a result, we hypothesize that there may be certain conditions on the parameters of the mixture of MNLs under which the problem can be efficiently approximated to a factor less than the size of the label set.

**Methodology.** Our greedy algorithm assumes that, for every prediction set, the predictions made by the human expert follow a parameterized expert model--the above mentioned mixture of MNLs. It would be worthy to develop model-free algorithms since, in the context of prediction sets constructed using conformal prediction, they have been shown to be superior to their model-based counterparts . To this end, a good starting point may be the literature on (contextual) combinatorial multi-armed bandits [57; 58], where one can map each arm to a label value and each subset of arms, namely a super arm, to a prediction set.

**Evaluation.** The results of our experiments suggest that the prediction sets constructed using our greedy algorithm may help human experts make more accurate predictions than the prediction sets constructed using conformal prediction. However, one may argue that the difference in performance is partly due to the fact that the non-conformity scores used in conformal prediction do not incorporate information about the distribution of experts' predictions. Motivated by this observation, it would be important to investigate how to incorporate such information in the definition of non-conformity scores. Moreover, in our experiments, the true distribution of experts' predictions matches the mixture of MNLs used by the greedy algorithm and brute force search. However, in practice, there may be a mismatch between the true distribution of experts' predictions and the mixture of MNLs, and this may decrease performance. Finally, in our experiments with real data, the ground truth labels are estimated by aggregating (multiple) predictions by human annotators using majority voting, however, this may introduce additional sources of errors that may influence our results .

**Broader impact.** We have focused on maximizing the average accuracy of the predictions made by an expert using a decision support system based on prediction sets. However, in high-stakes application domains, it would be important to extend our methodology to account for fairness considerations.

## 8 Conclusions

We have looked at the problem of finding the optimal prediction sets under which human experts achieve the highest accuracy in a given multiclass classification task. We have shown that this problem is \(\)-hard to solve and to approximate to any factor less than the size of the label set. However, we have empirically shown that, for a large parameterized class of expert models, a simple greedy algorithm consistently outperforms conformal prediction.

Figure 3: Complementary cumulative distribution (cCDF) of the per-image test accuracy achieved by a simulated human expert using the prediction sets constructed with conformal prediction (Naive, Aps, Raps and Saps) and our greedy algorithm (Greedy) on the ImageNet16H dataset.