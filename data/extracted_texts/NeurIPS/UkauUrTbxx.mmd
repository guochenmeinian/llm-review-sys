# ProTransformer: Robustify Transformers via

Plug-and-Play Paradigm

 Zhichao Hou\({}^{1}\)  Weizhi Gao\({}^{1}\)  Yuchen Shen\({}^{2}\)  Feiyi Wang\({}^{3}\)  Xiaorui Liu\({}^{1}\)

Corresponding author.

\({}^{1}\)North Carolina State University, \({}^{2}\)Carnegie Mellon University, \({}^{3}\)Oak Ridge National Laboratory

{zhou4,wgao23,xliu96}@ncsu.edu yuchens3@cs.cmu.edu fwang2@ornl.gov

###### Abstract

Transformer-based architectures have dominated various areas of machine learning in recent years. In this paper, we introduce a novel robust attention mechanism designed to enhance the resilience of transformer-based architectures. Crucially, this technique can be integrated into existing transformers as a plug-and-play layer, improving their robustness without the need for additional training or fine-tuning. Through comprehensive experiments and ablation studies, we demonstrate that our ProTransformer significantly enhances the robustness of transformer models across a variety of prediction tasks, attack mechanisms, backbone architectures, and data domains. Notably, without further fine-tuning, the ProTransformer consistently improves the performance of vanilla transformers by 19.5%, 28.3%, 16.1%, and 11.4% for BERT, ALBERT, DistilBERT, and RoBERTa, respectively, under the classical TextFooler attack. Furthermore, ProTransformer shows promising resilience in large language models (LLMs) against prompting-based attacks, improving the performance of T5 and LLaMA by 24.8% and 17.8%, respectively, and enhancing Vicuna by an average of 10.4% against the Jailbreaking attack. Beyond the language domain, ProTransformer also demonstrates outstanding robustness in both vision and graph domains. Our code is available at https://github.com/chris-hzc/ProTransformer.

## 1 Introduction

In recent years, attention mechanisms and transformer-based architectures have drawn significant attention across many domains in machine learning, such as natural language processing (NLP) [1; 2], computer vision [3; 4], and graph learning [5; 6]. In particular, transformers have demonstrated superior capabilities to learn and model complex relations in data through powerful and universal attention mechanisms, and they have dominated many popular NLP tasks such as topic classification, sentiment analysis, textual entailment, machine translation, dialogue generation, etc . Despite their success in NLP and beyond, many recent studies have demonstrated that transformers are highly vulnerable to adversarial attacks such that even small modifications to the input can easily fool the model [7; 8; 9]. However, most research on transformer architectures focuses on accuracy and efficiency, largely ignoring their security and robustness [10; 11].

With the increasing popularity of Large Language Models (LLMs) [12; 13], the robustness and security concerns of transformer architectures become particularly of interest. It has been shown that malicious attackers can invade the language models through various approaches as shown in Figure 1. The attacker can modify the input content in text attacks  or the prompt template inprompt attacks to mislead the model predictions . Moreover, by adding adversarial suffixes, the jailbreaking attack  can prompt a LLM to generate toxic and illegal content which could lead to catastrophic legal and ethical impacts such as malicious speech or privacy leaks. Given the broad applications of transformers and their vulnerabilities under attacks, it is imperative to design a universal and effective strategy to enhance the robustness of transformers.

Existing research attempting to improve the robustness of transformers can be roughly divided into empirical defenses [17; 18; 19; 20; 21] and certifiable defenses [22; 23; 24; 25]. Nevertheless, these defenses require excessive computation costs for training, inference, or both. In addition to these architecture-agnostic defenses, there are also several works proposing to enhance the robustness of transformers architecture [26; 27; 28; 29]. However, these approaches either require substantial computations or rely on specific domain knowledge, which hinders their extensions to larger models or broader application domains.

In this paper, given the limitations of existing works and the enormous training cost of transformers, we aim to robustify transformer architectures via a plug-and-play paradigm without additional training or fine-tuning. Our proposed _ProAttention_ (Algorithm 1) can be readily plugged into the given transformers to convert them to _ProTransformer_ (as shown in Figure 2) with significantly stronger robustness. Specifically, our contributions can be summarized as follows:

* We establish a novel connection between the attention mechanism in transformers and the weighted least square estimator. We provide interpretation and numerical simulation to reveal its vulnerability against potential adversarial attacks.
* From our new perspective, we propose robust token estimators to improve the resilience of token aggregation against adversarial attacks. We also propose an efficient Newton-IRLS algorithm to approximate the non-convex and non-smooth robust token estimator with convergence guarantees. The derived algorithm can be plugged into the given transformer as a plug-and-play layer to enhance its robustness against attacks even without additional training or fine-tuning.

Figure 1: Various attack mechanisms on language models. _Classic text attacks_ modify the input content using typos or synonyms; _Prompt attacks_ perturb the prompt template within the input; and _Jailbreaks_ append adversarial, non-semantic suffixes to manipulate the model into producing malicious outputs.

Figure 2: Overview of ProTransformer. ProAttention can be plugged into pretrained transformers without additional training. The ProTransformer is versatile and can be applied across various domains, including language, image, and graph.

* Our comprehensive experiments and ablation studies demonstrate that the proposed ProTransformer is effective, efficient, and generalizable. It significantly improves the robustness of transformers across various machine learning tasks, attack mechanisms, backbone architectures, and data domains such as language, vision, and graphs.

## 2 Related Work

In this section, we mainly summarize related works on the attacks and defenses of transformers focusing on language domains since this is the focus of this paper.

**Attacks.** Compared to the attack mechanisms in vision domain [30; 31], the text attacks in the language domain are highly complicated due to the natural irregularity of data structure. According to the perturbation units, text attacks can be classified into character-level [7; 32], word-level [33; 34; 35; 36; 9; 14; 37], sentence-level , and multi-level [39; 40; 8]. These classic text attacks typically generate adversarial examples through misspellings, synonym replacement, etc. In the era of LLMs, several new types of attacks have emerged, such as jailbreak attacks [16; 41; 42; 43] and prompt injection [44; 45; 46]. These prompting-based attacks aim to trick models into generating unsafe outputs using adversarially crafted prompts.

**Defenses.** There have been some works proposed to defend against adversarial text attacks from various perspectives. Empirical defenses, such as data augmentation  and adversarial training [47; 18; 19; 20; 21], attempt to robustify models by exposing them to a wider range of adversaries during training. On the other hand, several certifiable defenses [24; 25; 22; 23] have been proposed to guarantee the model robustness regardless of the attacks. However, these defenses require excessive computation costs for training, inference, or both, which limits their application in large-scale problems such as LLMs. Besides, all these methods are typically architecture-agnostic, which are orthogonal to and can be combined with our proposed defenses on the transformer architecture to further enhance the robustness.

To safeguard the transformers, several endeavors have been made from the transformer architecture perspective. Li et al.  modify the attention mechanism and position embedding to robustify text-to-speech transformers. In the crisis detection and recognition task, Liu et al.  propose an end-to-end attention-based classifier to enhance robustness. For tabular data, TableFormer  adopts structural-aware table-text encodings that are more robust to row and column order perturbations. However, these architectures are tailored for specific tasks, which require specific domain knowledge and can not be generalized across tasks. Han et al.  propose a general framework for self-attention modules via robust kernel density estimation (RKDE). However, this method introduces excess computation cost and shows relatively limited robustness improvement. Generally speaking, existing approaches either require substantial computations or rely on specific domain knowledge, which hinders their extensions to larger models or broader application domains.

## 3 ProTransformer

The main goal of this paper is to design robust self-attention mechanisms that are more resilient to adversarial attacks so they can be applied to robustify Transformer architectures. In this section, we first provide a new interpretation of the self-attention mechanism in Transformer architecture as the weighted least-square token estimator in Section 3.1. Then we propose robust token estimators that are more resilient to the dominating impact of input tokens in Section 3.2. An efficient Newton-IRLS algorithm is derived with a convergence guarantee to approximate the robust token estimator in Section 3.3. Finally, we describe how the proposed algorithm can be unrolled as robust attention layers to enhance the robustness of transformer architectures in Section 3.4.

### Attention Mechanism as WLS Token Estimator

First, we provide a new perspective to formulate the vanilla attention mechanism as the weighted least squares (WLS) token estimator. In the self-attention layer, each output token \(\) aggregates the values of input tokens \(\{_{j}\}\) as their weighted sum according to the attention weights: \(=_{j=1}^{N}a_{j}_{j}\), where \(\{a_{j}\}_{j[N]}\) are the attention weights and \(\{_{j}\}_{j[N]}\) are value vectors for all \(N\) input tokens. This weighted sum can be interpreted as the optimal solution of the following weighted least squares(WLS) error minimization problem:

\[*{arg\,min}_{}()=_{j=1}^{N}a_{j} \|_{j}-\|^{2},\] (1)

whose first-order optimality condition (\(()=_{j=1}^{N}a_{j} 2(- _{j})=\)) yields \(=_{j=1}^{N}a_{j}_{j}\).

**Vulnerability analysis of vanilla attention.** When adversaries perturb the input tokens, these tokens will dominate the impact on output tokens since the quadratic penalty on the residual \(\|_{j}-\|^{2}\) will dominate the WLS estimator. Therefore, the output token \(\) will be shifted to those dominating input tokens. As a result, the adversarial input tokens will significantly impact the representation of output tokens. We also provide an empirical study to verify that adversarial attacks will significantly increase the residual \(\|_{j}-\|^{2}\) in Appendix F.3. Moreover, we simulate a mean estimation problem under outlier data points using synthetic data to better illustrate the sensitivity of the WLS estimator. The detailed setting and visualization results of the numerical simulation are provided in Appendix F.

### Robust WLS Token Estimators

The analysis above provides a valid explanation of why various attention-based transformer architectures are easily compromised by introducing adversarial perturbations in the input data. Also, our interpretation of the attention mechanism in transformers as WLS estimator provides a rigorous perspective to design robust alternatives. To dampen the effect of outlier data, multiple robust regression algorithms have been proposed in robust statistics using least absolute deviations , Huber regression , and Minimax Concave Penalty (MCP) . Motivated by these advancements with rigorous robustness guarantees, we propose the robust weighted least squares token estimators to enhance the resilience against potential adversarial attacks as follows:

\[*{arg\,min}_{}()=_{j=1}^{N}a_ {j}(\|_{j}-\|)\] (2)

where \(\) can be flexibly replaced with the specific robust penalties in Figure 3.

**Special cases of \(\).** (1) The quadratic \(_{2}\) loss recovers vanilla WLS estimator; \(_{1}\) loss exerts linear effect on the residuals; (2) Huber loss performs as \(_{2}\) loss within the range \((0,)\), and becomes similar to \(_{1}\) when \(z>\); (3) MCP loss behaves like \(_{1}\) loss near zero and becomes constant when \(z\) is large than \(\). (4) We also propose Huber-MCP to combine the advantage of Huber and MCP loss. The detailed formulations are available in Appendix B.4 due to the space limit.

### Newton-IRLS algorithm

The proposed robust token estimator in Eq. (2) is non-convex and non-smooth, posing a challenge for efficient algorithm design. Moreover, the exploding model size of evolving transformers further necessitates the design of efficient neural network layers. To this end, we propose an efficient Newton iterative reweighted least square (Newton-IRLS) algorithm to tackle this challenging problem. We first design a localized upper bound for the original objective and then optimize the upper bound with a second-order Newton method. We also provide a rigorous theoretical loss descent guarantee. The precise statements are presented as follows and the detailed proof are provided in Appendix B.

**Localized upper bound.** Instead of directly optimizing the original loss function \(()\) in Eq. (2), we optimize a convex localized upper bound at the current iteration \(^{(k)}\) as follows:

**Lemma 3.1** (Localized Upper Bound).: _Suppose the loss objective is defined as in Eq. (2), where \(()\) is any non-convex function. For any fixed point \(^{(k)}\), there exists a convex localized upper

Figure 3: Different \((z)\).

bound as:_

\[}()=_{j=1}^{N}a_{j} w_{j}^{(k)}\|_{j}-\|^{2}+C(^{(k)}),\] (3)

_where \(w_{j}^{(k)}=(\|_{j}-^{(k)}\|)}{2\| _{j}-^{(k)}\|}\) and \(^{}\) is the first derivative of \(\). Particularly, the constant \(C(^{(k)})\) guarantees the equality of \(}\) and \(\) at \(^{(k)}\), i.e., \(}(^{(k)})=(^{(k)})\)._

Proof.: Please refer to Appendix B.1. 

As \(C(^{(k)})\) is treated as a constant during the optimization at the current step, the upper bound in Eq. (3) becomes convex and can be efficiently optimized.

**Newton-IRLS iteration.** After obtaining the convex upper bound \(}\) in Eq. (3), we can derive a concise closed-form iteration using the second-order Newton method as follows:

\[^{(k+1)}=^{(k)}-[^{2}}( ^{(k)})]^{-1}}(^{(k)})= {_{j}a_{j} w_{j}^{(k)}_{j}}{_{j}a_{j} w_{j}^ {(k)}}.\] (4)

Eq. (4) can be interpreted as a reweighted sum, in which the derived \(w_{j}^{(k)}\) modifies the original attention score \(a_{j}\) on the value vector \(_{j}\). We leave detailed derivations of Newton-IRLS algorithm in Appendix B.2. Its convergence and rigorous loss descent are guaranteed by the following Theorem 3.2.

**Theorem 3.2** (Convergence guarantee).: _Suppose the loss objective \(()\) is defined as in Eq. (2) and its corresponding convex localized upper bound is in Eq. (3). Then, through the iteration in Eq. (4), the following inequality holds:_

\[(^{(k+1)})}(^{(k+1)}) }(^{(k)})=(^{(k)}),\] (5)

_that is, optimizing upper bound \(}\) can guarantee the rigorous descent of \(\)._

Proof.: Please refer to Appendix B.3. 

Although the loss \(()\) is not necessarily convex and does not possess a global optimum, Theorem 3.2 guarantees that the Newton-IRLS iteration, which optimizes \(}()\), can rigorously reduce the original loss \(()\). The algorithm analyses in Appendix F, along with the main experiments in Section 4 and Section 5, validate that the local optimal solution achieved by our algorithm performs well in terms of both convergence and empirical robustness.

**Robust token estimator by reweighting the tokens**. The robust estimator in Eq. (2) provides a general framework that covers several special cases. By choosing different penalty functions \(\) on the residuals \(\|_{j}-^{(k)}\|\), we obtain various reweighting schemes in Eq. (4). Take the MCP function as the instance, the weight is derived as \(w_{j}^{(k)}=^{}(\|_{j}-^{(k)}\|)}{2\| _{j}-^{(k)}\|}=[_{j}- ^{(k)}\|}-,0]\). Obviously, the weight \(w_{j}^{(k)}\) becomes smaller as \(\|_{j}-^{(k)}\|\) increases, thereby down-weighting the large residuals. The residuals will be completely removed when it exceeds the threshold \(\), since the weight then becomes \(0\). The complete discussions for all cases are provided in Appendix B.4.

### ProAttention: Robust Attention Layers

In the previous subsection, we formulate the token-wise Newton-IRLS approach for notation simplicity. Here, we will present the corresponding matrix version for the entire attention layer.

**Matrix Form.** Denote \(=\{_{j}\}_{j[N]}\) and \(=\{a_{ij}\}_{i,j[N]}\) are value matrix and the attention matrix, respectively. \(^{(k)}=\{_{i}^{(k)}\}_{i[N]}\) is the estimator for token \(i\) at the \(k\)-th iteration. Subsequently, the pairwise distance \(^{(k)}=\{\|_{j}-_{i}^{(k)}\|\}_{i,j[N]}\) between \(^{(k)}\) and \(\) can be efficiently computed using the torch.cdist function in PyTorch. Following this, the weight \(^{(k)}=\{w_{ij}^{(k)}\}_{i,j[N]}\) can be calculated element-wise based on \(^{(k)}\). Then the next step \(^{(k+1)}\) is updated as a reweighted matrix multiplication \((^{(k)})\).

**Plug-and-Play Robust Attention.** The proposed algorithm can be packaged as a robust attention module, which can be readily plugged into the transformers as a **Plug**-and-Play **Robust** Attention (**ProAttention**) layer without additional training or fine-tuning as shown in Figure 2. The implementation of ProAttention using MCP penalty in PyTorch is shown in Algorithm 1. The complete pseudocode for other penalties is presented in in Appendix A.

**Complexity analysis.** Let \(N\), \(D\), and \(K\) represent the length of tokens, the dimension of vectors, and the steps of the iterations, respectively. The vanilla attention requires \(2 N N D\) basic operations while our ProAttention needs \((1+2K) N N D\). However, our ProAttention remains efficient, as the Newton-IRLS method can effectively approximate the solution within only \(3\) steps (\(K 3\)) (Figure 4 (a)) and ProTransformers do not introduce additional computation for training or fine-tuning. We provide the detailed complexity analysis of various attentions in Appendix L.

**Advantages.** Our proposed ProAttention enjoys the following advantages: (1) _Simplicity_: it is simple and easy to implement with only 4 core lines of code in Algorithm 1; (2) _Efficiency_: it is a plug-and-play layer that can be integrated into any trained transformer without additional training or fine-tuning; (3) _Universality_: it is a universal framework that advances the vanilla attention mechanism into a series of robust derivatives with different penalties. Moreover, it can be applied to any attention-based model across various modalities and tasks.

In the following sections, we will present comprehensive experiments and studies to validate the effectiveness of the proposed ProAttention on language modeling in Section 4 as well as computer vision and graph learning in Section 5.

## 4 Experiment on Language Modeling

In this section, we evaluate the effectiveness of the proposed ProAttention and ProTransformer under classic text attacks on pre-trained language models, and two prompting-based attacks (prompt attack and jailbreak attack) in the context of LLMs with comprehensive ablation studies.

### Experiment Setting

**Tasks and Datasets.** For topic classification, we use AG's News Corpus (AGNEWS) . For sentiment analysis, we utilize two widely-used datasets: Internet Movie Database (IMDB)  and Stanford Sentiment Treebank (SST-2) . For textual entailment, we make use of Recognizing Textual Entailment (RTE) in the General Language Understanding Evaluation benchmark . For jailbreak attack, we select a new dataset Behaviors introduced in . For the detailed information on these datasets, please refer to Appendix C.

**Backbone Architectures.** For classical pre-trained language models, we choose BERT  and its variants including RoBERTa , ALBERT  and DistilBERT . For large language models (LLMs), we choose T5 , LLMA  and Vicuna . For the detailed information on backbone architectures, please refer to Appendix D.2.

**Attacks.** We not only evaluate several classic text attacks but also include popular prompt attacks and jailbreak attacks on the LLMs. The three attack mechanisms and their differences are illustrated in Figure 1. For classic text attacks, we evaluate the attacks at various levels, including the character-level DeepWordBug , word-level PWWS , TextFooler , and multi-level TextBugger . For prompt attacks, we modify the prompt template according to the aforementioned text attacks following the evaluation setting in PromptBench . For jailbreak, we evaluate the suffix attack using Greedy Coordinate Gradient (GCG) method  and we test both attacks transferred from surrogate model Vicuna (transfer attack) and attacks directly targeting the victim models (adaptive attack). Please refer to Appendix E for details on attacks.

**Defense Baselines**. We include the following defense baselines in our experiments: MixADA , PGD-Adv , FreeLB , TA-VAT  and SmoothLLM . Additionally, we also include the adversarial training (AT), wherein the augmented perturbations are generated by the attack to be assessed. Details of these defense methods are provided in Appendix D.1.

**Evaluation metrics.** Following , we use 3 metrics to evaluate the model performance. Clean accuracy (**Clean%**) is the model accuracy on the clean testing data. Accuracy under attack (**AUA%**) is the accuracy on the perturbed data under specific attack. Attack success rate (**ASR%**) is the ratio of the number of successfully perturbed cases divided by the number of attempted texts.

**Hyperparameters.** For text attack setting, we follow the setting in the TextAttack framework . For prompt attack, we follow the setting in PromptBench . For GCG-based jailbreak attack, we follow the setting in . The detailed attack settings can be found in Appendix E. For defense baselines, we follow the settings in their original papers. For our ProTransformer, we set the default number of ProAttention layers as \(K=3\) since it can quickly converge to a reasonable precision within 3 layers. Finally we tune \(\) (default 1) or \(\) (default 4) in the penalties (Huber and MCP loss) to obtain the optimal parameters.

### Classic Text Attacks on Language Models

To demonstrate the effectiveness of the proposed ProTransformer, we compare the robustness of our methods with several popular defenses in three classical tasks: topic classification, sentiment analysis, and textual entailment.

#### 4.2.1 Adversarial Robustness

**Performance analysis.** The experimental results of topic classification (AGNEWS) are presented in Table 1, and we provide the results of sentiment analysis (IMDB) and textual entailment (RTE) in Appendix G.1 and G.2 due to the space limit. From the experiment results, we can make the following observations:

* The proposed ProAttention is a highly effective plug-in module that significantly and consistently enhances the robustness of various transformer backbones across various adversarial attacks. Taking AGNEWS as the instance, when combined with ProAttention (MCP), under the attacks {Textfooler, TextBugger, DeepWordBug, PWWS}: (1) ALBERT is improved by {28.3%, 15.7%, 20.6%, 27.2%} (2) DistilBERT is improved by {16.1%, 15.1%, 4.3%, 14.0%} (3) RoBERTa is improved by {11.4%, 1.8%, 4.3%, 5.4%} (4) BERT is improved by {19.5%, 16.6%, 14.3%, 13.1%}.
* Our method, Pro-BERT (MCP) + AT, exhibits best robustness among all the baselines. By simply plugging in ProAttention (MCP) module without fine-tuning, our Pro-BERT can achieve comparable robustness to most adversarial training-based methods which require substantial computational time and resources. Furthermore, our framework is orthogonal to most existing defenses, allowing for combined use with them to further enhance robustness. For instance, when combined with AT technique, our Pro-BERT (MCP) + AT can further improve BERT + AT by {14.7%, 4.6%, 18.6%, 6.2%} under {TextFooler, TextBugger, DeepWordBug, PWWS}.

    & &  &  &  &  \\ Model & Clean\% & \(\) & Aus\% \(\) & ASR\% \(\) & AUA\% \(\) & ASR\% \(\) & AUA\% \(\) & ASR\% \(\) & AUA\% \(\) & ASR\% \(\) \\  ALBERT & 93.0 & 20.6 & 77.9 & 26.1 & 71.9 & 38.9 & 58.2 & 35.9 & 61.4 \\ Pro-ALBERT (MCP) (Ours) & 93.8 & 48.9 & 47.3 & 41.8 & 55.3 & 59.5 & 35.9 & 63.1 & 32.0 \\  DistilBERT & 93.5 & 13.2 & 85.9 & 33.6 & 63.4 & 30.0 & 67.9 & 36.5 & 61.0 \\ Pro-DistilBERT (MCP) (Ours) & 93.9 & 29.3 & 68.5 & 48.7 & 47.9 & 34.3 & 63.1 & 50.5 & 45.6 \\  RoBERTa & 93.4 & 13.0 & 86.1 & 32.5 & 64.5 & 41.2 & 55.9 & 34.0 & 63.6 \\ Pro-RoBERTa (MCP) (Ours) & 93.7 & 24.4 & 73.7 & 34.3 & 62.8 & 45.5 & 51.5 & 39.4 & 57.5 \\  BERT & 94.2 & 19.7 & 78.9 & 31.7 & 67.5 & 37.5 & 59.8 & 43.1 & 53.8 \\ + FreeLB & 94.2 & 38.0 & 59.5 & 42.8 & 55.5 & 56.1 & 40.9 & 57.0 & 39.9 \\ + PGD & 94.1 & 36.8 & 61.7 & 40.5 & 57.1 & 47.6 & 49.7 & 48.7 & 48.6 \\ + MixADA & 94.3 & 35.6 & 62.4 & 35.4 & 62.9 & 38.2 & 50.5 & 46.8 & 50.4 \\ + TA-VAT & **94.4** & 36.2 & 61.8 & 39.2 & 58.2 & 49.5 & 48.1 & 47.0 & 50.7 \\ + AT & 94.1 & 42.1 & 54.8 & 56.1 & 39.4 & 42.4 & 54.1 & 62.6 & 32.5 \\  Pro-BERT (\(_{1}\)) (Ours) & 94.2 & 23.8 & 74.5 & 43.8 & 53.0 & 48.7 & 47.8 & 46.5 & 50.1 \\ Pro-BERT (Huber) (Ours) & 94.2 & 24.2 & 74.0 & 43.7 & 52.9 & 46.0 & 50.5 & 48.4 & 47.9 \\ Pro-BERT (MCP) (Ours) & 93.2 & 39.2 & 57.7 & 48.3 & 48.5 & 51.8 & 43.8 & 56.2 & 39.2 \\ Pro-BERT (MCP) + AT (Ours) & 94.0 & **56.8** & **38.9** & **60.7** & **35.1** & **61.0** & **34.1** & **68.8** & **25.7** \\   

Table 1: The results of topic classification on AGNEWS.

[MISSING_PAGE_FAIL:8]

#### 4.3.1 Prompt Attack

As shown in Figure 1, the most significant distinction between prompt attacks and classical text attacks is that prompt attacks aim to mislead the models by altering the prompt template rather than the input content. We display the results of T5 in Figure 5 and leave the comprehensive study in Appendix H.1. We also present the results on LLaMA in Appendix H.2. From the results, we can make the following observations: (1) For T5, the choice of the penalty would affect the robustness of defenses. Specifically, Pro-T5 (MCP) exhibits a significant advantage over other methods, and this advantage becomes even more evident as the number of perturbed words increases. Pro-T5 (\(_{1}\)) and Pro-T5 (Huber) show a slight improvement over the backbone model T5. (2) For LLaMA, Huber-MCP and Huber-based methods exhibit better robustness than other methods while preserving good clean performance. The detailed experiments and discussions can be found in Appendix H.2.

#### 4.3.2 Jailbreak Attack

In recent years, prompts have played a pivotal role in guiding models to generate desired outputs. Nevertheless, there exist malicious "jailbreak prompts", which are intentionally designed to bypass the built-in safeguards in LLMs, causing the model to produce harmful content that violates the legal policies. As illustrated in Figure 1, the suffix-injection jailbreaks attempt to append a non-semantic suffix to the user's prompt to fool the models. We select GCG method to evaluate the resilience of models comprehensively.

In Figure 6, we compare the Attack Success Rates (ASRs) of Vicuna and its corresponding Pro-Vicuna (Huber) with various \(\) values on Behaviors. In each column, we also include SmoothLLM  with different smoothing extent \(q(\%)\) to further reinforce the resilience of every single model. The last row of matrix (\(q=0\)) stands for the performance without random smoothing. The additional results of random smoothing with swap, insert and patch, as well as the results under adaptive jailbreaking attack are presented in Appendix I.

From the results, we can observe that: (1) Our Pro-Vicuna can significantly improve the robustness of Vicuna. As shown in the last row of Figure 6, with \(=0.1\), we successfully reduce the ASR to 1.8%, which is comparable to the random smoothing defense that requires multiple random perturbations, inferences and aggregations. (2) Our ProAttention is orthogonal to randomized smoothing defense and can be combined with it to further improve the robustness.

## 5 Experiment beyond Language Modeling

In the previous section, we have provided comprehensive experiments to validate the effectiveness of our ProTransformer in the (large) language models. In fact, as shown in Figure 2, our ProAttention is a fundamental module which can reinforce any attention-based models across various domains or modalities. In this section, we will integrate ProAttention into vision models and graph learning models to further validate the effectiveness and generality of our approach.

Figure 5: Prompt attack results. Figure 6: Attack success rates (ASRs) under transfer jailbreak.

[MISSING_PAGE_FAIL:10]