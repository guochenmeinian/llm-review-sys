# Finite Population Regression Adjustment and Non-asymptotic Guarantees for Treatment Effect Estimation

Finite Population Regression Adjustment and Non-asymptotic Guarantees for Treatment Effect Estimation

 Mehrdad Ghadiri

MIT

mehrdadg@mit.edu

&David Arbour

Adobe Research

arbour@adobe.com

&Tung Mai

Adobe Research

tumai@adobe.com

&Cameron Musco

UMass Amherst

cmusco@cs.umass.edu

&Anup B. Rao

Adobe Research

anuprao@adobe.com

###### Abstract

The design and analysis of randomized experiments is fundamental to many areas, from the physical and social sciences to industrial settings. _Regression adjustment_ is a popular technique to reduce the variance of estimates obtained from experiments, by utilizing information contained in auxiliary covariates. While there is a large literature within the statistics community studying various approaches to regression adjustment and their asymptotic properties, little focus has been given to approaches in the finite population setting with non-asymptotic accuracy bounds. Further, prior work typically assumes that an entire population is exposed to an experiment, whereas practitioners often seek to minimize the number of subjects exposed to an experiment, for ethical and pragmatic reasons. In this work, we study the problems of estimating the sample mean, individual treatment effects, and average treatment effect with regression adjustment. We propose approaches that use techniques from randomized numerical linear algebra to sample a subset of the population on which to perform an experiment. We give non-asymptotic accuracy bounds for our methods and demonstrate that they compare favorably with prior approaches.

## 1 Introduction

Randomized experiments play a crucial role in estimating counterfactual outcomes. The strength of randomization lies in its ability to ensure independence between the treatment group and pre-treatment covariates [14; 21]. This independence, in turn, results in unbiased estimation of treatment effects through the comparison of sample averages. However, in finite sample settings, the use of simple randomization techniques result in high variance of the estimated quantities. There have been two broad approaches to address this issue by reducing the variance of the estimated effects when subject level covariates are available: (1) _design-based_ approaches, which focus on the mechanism for assigning individuals to treatment and control groups in order to minimize imbalance (c.f. [21; 18; 27; 34; 5; 15]), and (2) _regression adjustment_ based approaches, which correct for imbalances post hoc by incorporating a regression of the covariates on the outcome (c.f. [9; 24; 7; 13; 29]).

Much of the prior work examining variance reduction for experimentation has focused on asymptotic properties of approaches [23; 24; 13; 21; 23]. Recently, there has been work focusing on design-based variance reduction, which ties the treatment assignment problem to discrepancy minimization and gives finite sample error bounds for estimating causal estimands such as the average treatment effect (ATE) [18; 1; 5] and individual treatment effect (ITE) . In particular, Harshaw et al. propose a design based on the _Gram-Schmidt random walk (GSW)_ and give finite sample guarantees on the mean squared error for average treatment effect estimation when using this design with the classic Horvitz-Thompson estimator. Addanki, et al.  use GSW and leverage score sampling to provide algorithms and finite sample guarantees for both average and individual treatment effect estimation in the _partial observation_ setting where only a subset of subjects are included in the experiment.

However, little work has examined the finite sample properties for experimental designs based on regression adjustment. Mou, et al.  provide non-asymptotic guarantees, but the analysis is in infinite population settings. The main contribution of this work is to analyze the finite population behavior of regression adjustment for variance reduction in treatment effect estimation, for ATE and ITE estimation in both the full and partial (sub-population) settings.

### Problem Statement: Finite Population Treatment Effect Estimation

In the finite population treatment effect estimation problem, we have \(n\) units (or individuals) associated with covariates \(_{1},,_{n}^{d}\) which are the rows of a covariate matrix \(^{n d}\). Each unit \(i\) is associated with two real numbers \(y_{i}^{(1)},y_{i}^{(0)}\) called the potential outcomes under the _treatment_ and _control_, respectively. We denote the corresponding potential outcome vectors with \(^{(1)},^{(0)}^{n}\). The _Individual Treatment Effect (ITE)_ vector \(^{n}\) is defined as \(:=^{(1)}-^{(0)}\) and the _Average Treatment Effect (ATE)_\(\) is defined as the mean of this vector, i.e., \(:=_{i=1}^{n}t_{i}\).

In treatment effect estimation, we seek to estimate either the ITE or ATE, under the constraint that at most one of the potential outcomes, either \(y_{i}^{(1)}\) or \(y_{i}^{(0)}\), can be observed for each unit \(i\). E.g., that unit may be assigned to a treatment group in a controlled trial, and thus \(y_{i}^{(1)}\) is observed while \(y_{i}^{(0)}\) is not.

In _full observation_ treatment effect estimation, we observe one potential outcome for each unit in the population [9; 13; 24; 18]. In _partial observation_ treatment effect estimation, we further restrict ourselves to observing outcomes for just a small subsample of the population. This setting is important when the goal is to minimize experimental costs by e.g., limiting the size of a controlled trial [30; 20; 32; 1].

To reduce variance, many approaches to ATE and ITE estimation, including our own, attempt to leverage the population covariates \(\) to make inferences about the treatment outcomes. In particular, throughout this work, we will target error bounds that depend on the best linear fit of the treatment outcomes to our covariates. Such linear effects models are common in the literature on treatment effect estimation [33; 18; 1] and provide a useful baseline for establishing theoretical bounds.

### Our Contributions

We give new approaches to both ITE and ATE estimation in the full and partial observation settings that combine the classic statistical technique of regression adjustment with techniques from randomized numerical linear algebra. Our algorithms give natural, non-asymptotic error bounds depending on the best linear fit for either the ITE vector \(:=^{(1)}-^{(0)}\) or the sum of the potential outcome vectors \(:=^{(1)}+^{(0)}\) to our covariates. \(\) is a natural quantity that arises e.g., in variance bounds for classic approaches to ATE estimation, such as the Hovitz-Thompson estimator .

#### 1.2.1 Individual Treatment Effect Estimation

Our first result is for individual treatment effect estimation in the full and partial observation settings.

**Theorem 1** (ITE Estimation).: _For any \(,,(0,1]\), there exists a randomized algorithm (Algorithm 3) that observes a potential outcome for \(O(d(d/)/^{2}+ n)\) units and outputs a vector \(}\). Moreover there is an event \(\) such that \(() 1-\) (over the randomness of the algorithm) and_

\[[\|}-\|_{2}^{2}| ](1+)_{^{d}}\|-\|_{2}^{2}+(1+) \|^{(1)}+^{(0)}\|_{}^{2}.\]

Theorem 1 shows that with high probability (at least \(1-\)), our algorithm achieves an expected error bound that nearly matches the best linear fit of the ITE vector to our covariates (i.e., \(_{^{d}}\|-\|_ {2}^{2}\)), up to an additive term depending on the maximum magnitude of the sum of potential outcomes for any individual. In the full observation setting, when \(=1\), this additive term is \(O(d)\), assuming that the maximum magnitude does not scale with the population size \(n\). In contrast, we expect the best fit error (\(_{^{d}}\|-\|_ {2}^{2}\)) to grow as \((n)\), and thus the additive term is lower order. In the partial observation setting, as long as we set \(=(1/n)\), we still expect this additive term to be lower order. Thus, the case when \(d n\), we are able to nearly match the best fit error while only observing a very small subset of the full population.

We sketch the proof of Theorem 1 in Section 2. Our algorithm is based on _pseudo-outcome_ regression, which is studied in the asymptotic setting by Kennedy et al. for conditional ATE estimation . Roughly, in the full observation setting, we construct a vector \(\) where \(v_{i}=2y_{i}^{(1)}\) with probability \(1/2\) and \(v_{i}=-2y_{i}^{(0)}\) with probability \(1/2\). We can see that \([]=\) (i.e., \(\) is equal to the ITE vector in expectation), and importantly, that constructing \(\) only requires observing one potential outcome for each individual. By regressing \(\) onto our covariates, we obtain our estimate \(}\) for \(\).

In the partial observation setting, we further subsample individuals according to the _leverage scores_ of the covariate matrix \(\). This is a standard technique in randomized linear algebra , which allows us to approximately solve the above regression problem while only observing a subset of \((d/^{2})\) entries of \(\) (and thus only observing outcomes for a subset of the population). Addanki et al.  similarly propose an algorithm for the partial observation setting based on leverage score sampling. However, instead of pseudo-outcome regression, they learn separate approximations to both outcome vectors \(^{(1)}\) and \(^{(0)}\). Thus, their final error depends on the best linear fit error for both these vectors, rather than the more natural best linear fit error for the ITE vector \(\).

#### 1.2.2 Average Treatment Effect Estimation

We can extend our general approach to give bounds for average treatment effect estimation. In the full observation setting we show:

**Theorem 2** (ATE Estimation - Full Observation).: _For any \(,(0,1]\), there exists a randomized algorithm (Algorithm 1) that computes an unbiased estimate \(\) of the ATE \(\), and for which there is an event \(\) with \(() 1-\) (over the randomness of the algorithm) such that_

\[[(-)^{2}|]}_{^{d}}(\| -\|_{2}^{2}+100(n/)^{2} \|\|_{2}^{2})+}\| ^{(1)}-^{(0)}\|_{}^{2},\]

_where \(:=_{i[n]}\|_{i}\|_{2}\) and \(:=^{(0)}+^{(1)}\) is the total outcome vector._

As with our result in ITE estimate, the error bound of Theorem 2 matches the error of the best linear fit of the total outcome vector \(\) to our covariates up to a constant factor plus additive terms depending on 1) the maximum magnitude of the sum of potential outcomes for any individual, and 2) the norm of the coefficient vector used to approximately reconstruct \(\) from the covariates. Again, we expect these additive terms to scale as \(O(1/n^{2})\) while we expect the term depending on the best fit error to scale as \(O(1/n)\). Thus, we generally expect the additive error to be lower order.

Harshaw et al.  proposed an algorithm based on the _Gram-Schmidt walk (GSW) design_ (see Algorithm 5 in the appendix) for balancing assignments to treatment and control groups that achieves an unbiased estimator for ATE with variance of \(}_{}[\|- \|_{2}^{2}+}{1-}\|\|_{2}^{2}]\), where \((0,1)\) is chosen by the experiment designer. This guarantee is comparable to but stronger than ours, e.g., when one sets \(=1/2\). If we ignore the additive error terms and focus just on the best linear fit error \(\|-\|_{2}^{2}\), then GSW is better than our guarantees by about a factor of \(8\). But the GSW design is computationally more expensive and, since is based on balancing the covariates, requires availability of all covariates before running the experiment and hence cannot be applied in an online setting . In contrast, our approach can be applied directly in the online setting since we place each unit independently in the treatment or control groups with equal probability. Moreover, as we show in our experiments, the empirical performance of our algorithm is much closer to GSW than the theoretical bounds we are able to show.

We sketch the ideas behind Theorem 2 in Section 2. Our algorithm is based on the classic Horvitz-Thompson estimator . Roughly, this estimator randomly assigns individuals to control and treatment groups, and estimates ATE as the difference in the average outcome between these groups, appropriately weighted by the assignment probabilities. It is well known that the variance of this estimator is bounded by \(}\|\|_{2}^{2}\). To reduce this variance, we introduce _full-observation regression_adjusted Horvitz-Thompson (RAHT) estimator_, which estimates ATE as the following. We partition the units into two groups \(S\) and \(\) (this is a different partitioning than the partitioning into control and treatment), and we regress \(_{S}\) and \(_{}\) onto their corresponding covariates. As in the ITE case, since we cannot directly form \(\), we instead perform the above regressions using a random vector \(\) with \([]=\), and where \(\) can be formed by only observing one potential outcome per individual. We then use the solution vector of group \(S\) to adjust the outcomes of the group \(\) and vice versa, and we apply the HT estimator to the adjusted outcomes. This gives the bound in Theorem 2.

Again, following a similar approach as in the ITE case, we can apply leverage score based subsampling to tackle ATE estimation in the partial observation setting as well, obtaining:

**Theorem 3** (ATE Estimation - Partial Observation).: _For any \(,(0,1]\), \((0,1)\), and \(m[n]\), there exists a randomized algorithm (Algorithm 4) that observes a potential outcome for \(O(d(d/)/^{2}+m)\) units and outputs an unbiased estimate \(\) of the ATE \(\), for which there is an event \(\) with \(() 1-\) (over the randomness of the algorithm), such that_

\[[(-)^{2}|] \|^{*}- \|_{2}^{2}+}}{(1-)} \|^{*}\|_{2}^{2}+ ^{2}}\|\|_{}^{2}\] \[+(1+)(\| \|_{}^{2}+\|(-}) }-(-})\|_{2}^{2}+\|}\|_{2}^{2}),\]

_where \(:=_{i[n]}\|_{i}\|_{2}\), \(:=^{(0)}+^{(1)}\), \(=}^{2}\), \(}^{n}\) is a vector where all the entries are equal to \(=_{i=1}^{n}t_{i}\), and_

\[^{*}=_{b^{d}}[\| -\|_{2}^{2}+^{2}\|\| _{2}^{2}],}=*{arg\,min}_{ ^{d}}[\|(-})-(-})\|_{2}^{2}+\|\|_{2}^{2}].\]

The error bound of Theorem 3 depends on 1) the error of best linear fits of the covariates onto \(\) and \(-}\), 2) the norm of the coefficient vector used to approximately reconstruct \(\) and \(-}\) from the covariates, and 3) the largest component of the total outcome vector \(\). We expect both the best linear fit and \(\|\|_{}\) terms to scale as \(O()\). All other terms are lower order. Namely, The terms depending on the norms of the coefficient vectors scale as \(O()\) and \(O(})\). For the case where \(md n\), by increasing the number of samples to \(O(d(d/)/^{2}+md)\), we can make the \(\|\|_{}\) term to scale as \(O(1/m^{2})\) as well.

We deploy a different regression adjustment technique for achieving Theorem 3, which we call _partial-observation regression adjusted Horvitz-Thompson (RAHT) estimator_. In the partial observation setting, a simple approach is just to apply the full observation algorithm to a uniformly selected subset of \(m\) units and then report the ATE estimate for these \(m\) units. However, this leads to a variance bound depending on \(\|-}\|_{2}^{2}\). To reduce this variance, the partial observation RAHT adjusts the estimate obtained from the \(m\) units using regression adjustment techniques. We give the details behind the full algorithm in Section 2.3 and provide the full analysis in the appendix.

#### 1.2.3 Experimental Evaluation

We compliment the above theoretical bounds with experiments using both synthetic and real data, focusing on the full observation ATE and ITE estimation problems. For ATE, we compare our method with classical regression adjustment , the GSW method , and the algorithm of . We observe that our estimator (which is unbiased) performs as well or better than all other estimators except the classic regression adjustment, which is a biased estimator. For ITE, we compare our approach with the method of , and observe superior performance. A discussion about the running time of our algorithm and comparison to GSW design is included in the appendix.

### Roadmap

We present the high-level ideas and techniques for ITE and ATE estimation for the full and partial observation settings in Section 2. We then present notation and preliminaries required for a more in-depth discussion of our result in Section 3. In Section 4, we give a detailed non-asymptotic analysis of the random vector (i.e., pseudo-outcome) regression technique, which forms the basis for all of our algorithms. We present our algorithm for the full observation ATE estimation and sketch the proof of its accuracy (Theorem 2). Finally, we present our experimental results in Section 5.

We relegate the details of other algorithms and proofs to the appendix, along with additional details and results for the experimental results. In the appendix, we also include a warm-up result for the mean estimation problem in the partial observation setting (where there is only one outcome \(\) and we seek to estimate \(_{i=1}^{n}y_{i}\) from a small number of samples). This result illustrates the key ideas behind leverage score sampling, which is used for our results on both ITE and ATE estimation. In addition we discuss how to remove the \(1-\) probability from our results in Section C.1.

## 2 Technical Overview

In this section, we sketch the key ideas behind our main results on ITE estimation (Theorem 1) and ATE estimation (Theorems 2 and 3).

### ITE Estimation

We first consider ITE estimation in the full observation setting. The key challenge here is that we must infer the full treatment effect vector \(=^{(1)}-^{(0)}\), while only observing one outcome, \(y_{i}^{(1)}\) or \(y_{i}^{(0)}\) for any unit \(i\). To do so, we use the idea of _pseudo-outcome_ regression in . We construct a random vector \(^{n}\) that is equal to \(\) in expectation, i.e., \([]=\) but only requires observing one outcome per unit. Specifically, we independently set \(v_{i}=2y_{i}^{(1)}\) with probability \(0.5\) and \(v_{i}=-2y_{i}^{(0)}\) with probability \(0.5\). We then regress \(\) onto our covariates, and use the result to estimate \(\). In particular, we set \(}:=_{^{d}}\| -\|_{2}^{2}\) and construct our ITE estimate as \(}=}\).

To give a non-asymptotic error bound for this approach, we observe that we can write \(=+\), where \(\{1,-1\}^{n}\) is a random sign vector, \(=^{(1)}+^{(0)}\) is the sum of potential outcome vectors, and \(\) denotes the Hadamard (entrywise) product. Thus, letting \(_{}^{n n}\) be the projection matrix onto the column span of \(\), we can write \(}=_{}=_{}+ _{}()\). In turn, we have

\[[\|}-\|_{2}^{2}] =[\|}-\|_{2} ^{2}]=[\|_{}- \|_{2}^{2}+\|_{}() \|_{2}^{2}+2^{T}^{T}( )].\]

The first term on the righthand side is simply the best linear fit error for \(\), \(\|_{}-\|_{2}^{2}=_{^{d}}\|-\|_{2}^{2}\). The third term is \(0\) in expectation since \([]=0\). Finally, since the entries of \(\) are independent and mean \(0\), the expectation of the second term can be bounded by \((_{})\|\|_{ }^{2}=d\|\|_{}^{2}\). Putting these all together, we have

\[[\|}-\|_{2}^{2}] _{^{d}}\|- \|_{2}^{2}+d\|\|_{}^{2},\] (1)

which gives the bound of Theorem 1 in the full observation setting. We note that this expected error bound can also be turned into a high probability bound using concentration inequalities, i.e., Hanson-wright inequality .

**ITE Estimation with Partial Observation.** Our next step is to extend the above to the partial observation setting. To do so, we apply leverage score sampling, which is a standard technique for approximate regression via subsampling . It is well known that if we sample \(O(d(d/)/^{2})\) rows of \(\) and the corresponding entries of \(\) according to the leverage scores of \(\), then we can solve a reweighted regression problem on just these rows to find \(}\) satisfying \(\|}-\|_{2}^{2}(1+ )_{^{d}}\|- \|_{2}^{2}\). Observe that to compute \(}\), we only need to observe potential outcomes for the units (i.e., the entries of \(\)) that are sampled. Thus, this almost allows us to recover a similar bound to (1), up to a multiplicative \((1+)\) factor, in the partial observation setting. However, the leverage score of any one row may be very small, and if sampled, that row may be scaled up by a large factor in our reweighted regression problem (to preserve expectation). It thus becomes difficult to control to error introduced by approximating \(\) by \(\).

To handle this issue, we mix leverage score sampling, with uniform sampling at rate \(\). This increases our sample complexity to \(O(d(d/)/^{2}+ n)\), but allows us to bound the error due to approximating \(\) by \(\) by \(\|\|_{2}^{2}\). This yields the final sample complexity and error bound of Theorem 1. For a full proof, see Appendix B.

### ATE with Full Observation

We build on our techniques for ITE estimation to tackle the ATE estimation problem. We start with the classic Horvitz-Thompson (HT) estimator for ATE, defined below.

**Definition 4** (Horvitz-Thompson estimator).: _Given two outcome vectors \(^{(1)},^{(0)}^{n}\), let \(Z^{+}\) and \(Z^{-}=[n] Z^{+}\) be a random partitioning of the units to two groups under a distribution \(\). Then the Horvitz-Thompson (HT) estimator is \(:=(_{i Z^{+}}^{(1)}}{_{ }[i Z^{+}]}-_{i Z^{-}}^{(0)}}{_{ }[i Z^{-}]}),\) where \(_{}[i Z^{+}]\) denotes the probability of placing \(i\) in \(Z^{+}\) when the partitioning is performed according to distribution \(\)._

Note that if units are assigned independently with equal probability to treatment and control, the HT estimator is equivalent to outputting the average of the random vector \(\) used in our regression-based estimate for ITE. In this case, the HT estimator is unbiased and well known to have variance bounded by \(}\|\|_{2}^{2}\), where \(:=^{(1)}+^{(0)}\). Our goal is to improve this variance to instead depend on the error of the best linear fit to \(\), \(_{^{d}}\|-\|_ {2}^{2}\), as in Theorem 2. To do so, we will employ the classic technique of regression adjustment.

**Horvitz-Thompson on Regression Adjusted Outcomes.** Suppose we were given a vector \(^{d}\) determined independently of the randomness in the HT estimator. Then applying the HT estimator directly on the regression adjusted vectors \(}^{(i)}:=^{(i)}-\) (which have the same ATE as the original vectors), would yield an unbiased estimate for the ATE with variance at most \(}\|-\|_{2}^{2}\). Naturally, we would like to find such a \(\) making this variance bound as small as possible.

To do so, we will apply the same approach as in ITE estimation. We cannot directly regress \(\) onto our covariates to find an optimal \(\), as we cannot observe any entry of \(\) since it is the sum of the two potential outcomes. Instead, we can construct a vector \(\) with \([]=\), and use it as a surrogate in our regression problem. In particular, we let \(u_{i}=2y_{i}^{(1)}\) with probability \(1/2\) and \(u_{i}=2y_{i}^{(0)}\) with probability \(1/2\). As in the ITE setting, we can bound the error incurred by approximating \(\) by \(\) in our regression problem, and thus bound the variance of our regression adjusted HT estimator. We note that to the best of our knowledge, this idea has not been leveraged in ATE estimation.

Unfortunately, a difficulty arises in this approach. We need to assign each unit to a treatment or control group both when computing \(\) to solve our regression problem, and later when applying the HT estimator. If we use the same assignment for both steps (as is required since we can only observe one of \(y_{i}^{(1)}\) or \(y_{i}^{(0)}\) for each unit), we introduce bias in our estimator.

**Avoiding Estimator Bias.** To alleviate this issue, we use _ridge leverage score sampling_ to first partition the units into two groups. Ridge leverage score sampling is a technique similar to leverage score sampling that gives similar approximation guarantees to linear regression but for ridge regression problems (i.e., linear regression problems with an \(_{2}\) regularization term). We pick a ridge (regularization) parameter that guarantees that each unit only needs to be in the first or second group with a probability of at most \(0.5\) while guaranteeing that the solution obtained from this sampling is a \((1+)\) approximation of the optimal ridge regression solution. Then we solve ridge regression problems on the groups separately to obtain two vectors \(}^{(1)}\) and \(}^{(2)}\), for the first and the second group, respectively. We then use \(}^{(1)}\) to adjust the outcomes of the second group and use \(}^{(2)}\) to adjust the outcomes of the first group. Since the vector of each group is not used to adjust the outcomes of itself, this gives an unbiased estimator.

### ATE with Partial Observation

We now consider ATE estimation where we desire to only observe \(y_{i}^{(1)}\) or \(y_{i}^{(0)}\) for a small subset of the population. If we draw a subset of size \(m\) uniformly at random and observe the outcomes of the samples in the subset independently and with equal probability and apply the Horvitz-Thompson estimator, the variance is \([(-)^{2}]\|_ {2}^{2}}{mn}+-}\|_{2}^{2}}{m}.\) We would like to replace terms \(\|\|_{2}^{2}\) and \(\|-}\|_{2}^{2}\) with \(_{}\|-\|^{2}\) and \(_{}\|}-}-(- })\|^{2}\), respectively. To deal with the \(\|\|_{2}^{2}\) term, we use the HT estimator applied to the GSW design  on the uniformly selected subset of size \(m\) of the data. Then for \((0,1)\) that the experiment designer selects, The variance reduces to \(}_{b^{d}}[\| -\|_{2}^{2}+}{1-}\| \|_{2}^{2}]+-}\|_{2}^{2} }{mn}.\) To replace the \(-}\|_{2}^{2}}{mn}\) term with the errorof the best linear fit, we use the partial observation RAHT estimator that given a solution vector \(}\) adjusts the estimate obtained by the GSW design on the sample of size \(m\). Similar to our approach for partial observation ITE estimation, we compute \(}\) by sampling about \(d\) units according to leverage scores of the matrix and performing the random vector regression (pseudo-outcome regression) on a vector that is equal to \(/2\) in expectation. Let \(S\) be the set of units selected by leverage score sampling and \(=[n] S\). We denote the estimate obtained by sampling \(m\) units from \(\) (we denote this set with \(T\)) and applying GSW design with \(_{}\). Then, the RAHT estimator estimates the ATE as \(_{}-_{i T}(_{i}- }^{})}\). This estimate is biased because some units have a higher probability of being included in \(S\) and, therefore, would contribute to the estimate with a lower probability. To resolve this issue, we also estimate the ATE on the set \(S\) with the same random vector of outcomes that is used in regression to learn \(}\). We use HT estimator for this estimate, and we denote it by \(_{S}\). Then, our final estimate is a convex combination of the two estimates as the following.

\[|}{n}(_{}- _{i T}(_{i}-}^{})})+_{}.\]

This estimator is unbiased and achieves the variance bounds stated in Theorem 3.

## 3 Preliminaries

**Notation.** Vectors and matrices are denoted with bold lowercase and capital letters, respectively. the Hadamard (entrywise) product of \(,\) is denoted by \(\). We denote the vectors of all ones and all zeros in \(^{n}\) by \(_{n}\) and \(_{n}\), respectively. We drop the subscript if the dimension is clear. We denote the identity matrix of dimension \(n\) by \(_{n}\). For \(^{n d}\) we denote the projection matrix of \(\) by \(_{}:=(}^{})^{+} \), where \(+\) denotes the pseudoinverse. For \(^{n}\) and \(i n\), \(x_{i}\) denotes its \(i^{th}\) entry and \(_{1:i}\) denotes a vector in \(^{i}\) that is equal to its first \(i\) entries. We use \(_{i,:}\) and \(_{i}\) interchangeably to denote row \(i\) of \(\). Similarly for \(S[n]\), \(_{S,:}^{|S| d}\) denotes the submatrix with row indices in \(S\) and for \(^{n}\), \(_{S}^{|S|}\) denotes the vector restricted to indices in \(S\).

**Definition 5** (Ridge leverage scores ).: _Let \(^{n d}\), \( 0\), and \(_{i}^{d}\) be the \(i^{th}\) row of \(\). Then the \(\)-ridge leverage score of row \(i\) of \(\) is defined as \(_{i}^{()}:=_{i}^{}(^{}+ )^{-1}_{i}\). We denote the leverage scores (i.e., \(=0\)) with \(_{i}\). If the corresponding matrix \(\) is not clear from the context, we denote the \(\)-ridge leverage scores with \(_{i}^{()}()\)._

Note that \(_{i}\) is the \(i\)'th diagonal entry of the projection matrix \(_{}=(^{})^{-1}^{}\). Thus, since \((_{})=()\), \(_{i=1}^{n}_{i}=()\). Since in our approach, we need to sample different sets of population for different purposes, we require a bound on the ridge leverage scores that can be achieved by taking the regularization factor \(\) to be large enough.

**Theorem 6**.: _Let \(:=_{i[n]}\|_{i}\|_{2}\). Then for \( c^{2}\), for \(c 1\), \(_{i}^{()}()\) for all \(i[n]\)._

The following theorem provides a sampling procedure based on \(\)-ridge leverage scores to solve a linear ridge regression problem approximately.

**Theorem 7** (6, 12).: _Let \(,(0,1]\), and \(^{n}\) be a vector that upper bounds the \(\)-ridge leverage scores of a matrix \(^{n d}\), i.e., \(_{i}^{()} u_{i}\). Let \(=^{n n}\) be a random diagonal matrix in which \(_{ii}=1/}\) with probability \(p_{i}=\{1,3 u_{i}(d/)/\}\), and \(_{ii}=0\), otherwise. Then for any \(^{n}\), letting \(}=_{^{d}}[\| -\|_{2}^{2}+ \|\|_{2}^{2}],\) with probably at least \(1-\),_

\[\|}-\|_{2}^{2}+\| }\|_{2}^{2}(1+)_{^{d}} [\|-\|_{2}^{2}+\| \|_{2}^{2}].\]

## 4 Average Treatment Effect Estimation

In this section, we first analyze the random vector regression (i.e., pseudo-outcome regression) approach which is used both for our ITE and ATE estimation. We then discuss our full observation ATE estimation using this approach and provide a sketch of the proof of Theorem 2.

### Random Vector Regression

For both ATE and ITE estimation, regression adjustment allows us to reduce the variance of estimation to the error of the best linear fit on vectors \(\) and \(\), respectively. Since we do not have access to the entries of these vectors, we cannot compute a solution vector directly. Therefore, for ATE (or ITE), our approach is to instead compute a solution vector \(\) by performing regression on a random vector that, in expectation, is equal to \(\) (or \(\)). The following theorem characterizes the error of the solution vector obtained in this way.

The following is a more general result that combines this random assignment technique with leverage score sampling to allow us to observe only a small subset of the population. Algorithm 3 and the proof of the following theorem are presented in the appendix.

**Theorem 8** (Random vector regression).: _Let \(^{(0)},^{(1)}^{n}\) and \(^{n}\) be a random vector such that for each \(i[n]\), \(y_{i}\) is independently and with equal probability is either equal to \(y_{i}^{(0)}\) or \(y_{i}^{(1)}\). Moreover, let \(^{*}=_{}\|-\|_{2}^ {2}\). Let \(:=^{(1)}+^{(0)}\). Then_

\[[\|2^{*}-\|_{2}^{2} ] d\|^{(1)}-^{(0)}\|_{}^{2}+_{}\|-\|_{2}^{2}.\]

Proof.: Let \(\{-1,+1\}^{n}\), where \(z_{i}=+1\) if \(y_{i}=y_{i}^{(1)}\), and \(z_{i}=-1\), otherwise. First, note that \(2=+\), where \(=^{(1)}+^{(0)}\), and \(=^{(1)}-^{(0)}\). Therefore

\[-2^{*}=-2_{}=-_{}(+)=( -_{})-_{}( ).\]

Moreover since \(-_{}\) and \(_{}\) are orthogonal to each other,

\[\|-2^{*}\|_{2}^{2}=\|(-_{})\|_{2}^{2}+\|_{ }()\|_{2}^{2}.\]

Now note that \(\|(-_{})\|_{2}^{2}= _{}\|-\|_{2}^{2}\). So we only need to bound \([\|_{}()\|_{2 }^{2}]\). We have \(\|_{}()\|_{2}^{2}=()^{}_{}()= ^{}_{}\), where \(\) is the diagonal matrix associated with the vector \(\). Since \(z_{i}\) and \(_{j}\) are independent for \(i j\), we have

\[[^{}_{}]=_{i=1}^{n}t_{i}^{2}(_{})_{ii}\| \|_{}^{2}(_{})_{ii}.\]

Then the result follows by noting that \((_{})_{ii}\) is equal to the leverage score of row \(i\), and the sum of leverage scores is less than or equal to \(d\). 

We can use the same theorem to characterize the regression error for \(\) by changing the sign of \(^{(0)}\).

### Full Observation ATE

We now sketch a proof of our result for ATE estimation with full observation.

Proof sketch of Theorem 2.: Here, we only sketch our proof for the variance bound. For \(S[n]\) and \(\{-1,+1\}^{n}\) (where \(Z^{+}=\{i:z_{i}=+1\}\) and \(Z^{-}=\{i:z_{i}=-1\}\)), we define

\[_{S}:=_{i S}(y_{i}^{(1)}-y_{i}^{(0)}),\,_{S}:=(_{i Z^{+} S}_{i} ^{(2,1)}-_{i Z^{-} S}_{i}^{(2,0)}).\]

Similarly, define \(_{S}\) and \(_{S}\). Then for \(\) as defined in Algorithm 1, we have,

\[_{}[_{S}[(- )^{2}]]=_{}[_{S}[ (_{S}+|_{S}}{n}-+|_{S}|_{S}}{n})^{2}]]\] \[=_{S}[|^{2}}{n^{2}}_{}[(_{S}-_{S})^{2}]+}{n^{2}} _{}[(_{S}-_{S})^{2}] ]+_{S}[2|}{n^{2}} _{}[(_{S}-_{S})(_ {S}-_{S})]].\]

By Cauchy-Schwarz inequality, \(_{}[(_{S}-_{S})(_ {S}-_{S})]_{}[(_{S}- _{S})^{2}]_{}[(_{S}-_ {S})^{2}]}\). Therefore we only need to bound \(_{}[(_{S}-_{S})^{2}]\) and \(_{}[(_{S}-_{S})^{2}]\). We can bound \(_{}[(_{S}-_{S})^{2}]\) by Theorem 7 (ridge leverage score sampling) and using the fact that \(_{S}\) is obtained by \(}^{(2,1)}\) and \(}^{(2,0)}\) which are adjusted by \(}^{(2)}\), a vector that is learned from the units in the set \(\). Since \(S\) and \(\) are disjoint, \(}^{(2)}\) is independent of \(_{S}\) and this allows us to bound \(_{}[(_{S}-_{S})^{2}]\). The bound on \(_{}[(_{S}-_{S})^{2}]\) follows from a similar argument. 

## 5 Experiments

In this section, we compare our method for estimating ATE and ITE with full observation with prior art and baselines on synthetic and real-world datasets. For ATE estimation, our experiments demonstrate that our approach either outperforms other unbiased estimation approaches or has comparable performance in terms of the achieved variance.

**ATE methods.** Our ATE estimation method is compared against five other approaches. (i) **HT Uniform:** This employs the Hurwitz Thompson estimator, using a uniform assignment of units to treatment and control groups. (ii) **GSW:** This design follows the methodology in . (iii) **Classic Regression Adjustment:** As described in , this estimator is inherently biased. (iv) **Leverage Score-Based:** In this method, we learn two distinct vectors for \(^{(1)}\) and \(^{(0)}\) through leverage score sampling. The difference between these vectors predicts the treatment effect. Though this method has its advantages, it is biased and was previously employed by Addanki et al. . (v) **4 Vectors:** Obtained by specializing Mou et al.  to linear regressors, this approach adopts the same cross-adjustment mechanism to mitigate bias as in this paper. However, instead of random vector regression, it determines separate vectors for \(^{(1)}\) and \(^{(0)}\) for each group, leading to four vectors obtained by linear regressions.

**ITE methods.** We discuss two distinct approaches for ITE methods. (i) **Baseline:** The ideal scenario or "oracle" baseline involves utilizing a linear function, trained on vector \(\). Naturally, this is not feasible in practical applications. (ii) **leverage score based:** This is based on learning two different vectors by ridge leverage score sampling and using the difference of the two linear functions for estimating \(\). This is an adaptation of the algorithm in  to the full observation setting -- we make this adaptation because the exact algorithm of  cannot be applied to the full-observation setting due to sampling with replacement. We do not compare to  because their guarantees are only for an infinite population.

**Synthetic Datasets.** For synthetic datasets, we will construct \(^{(0)},^{(1)}\) such that \(\) (for the ATE case) or \(\) (for the ITE case) is a linear function of the covariate matrix.

1. **ATE Dataset.** There are 50 covariates (i.e., features). Each entry of \(^{n d}\) is a uniform random number in \([0,0.01]\). Each entry of \(^{50}\) is a uniform random number in \(\). The individual treatment effect vector \(\) is \(+\), where each entry of \(^{n}\) is randomly picked from a mean zero Gaussian distribution with standard deviation of 0.2. Each entry of \(^{(0)}\) is picked uniformly at random from \(\). Then \(^{(1)}=-^{(0)}+\) and \(=^{(1)}-^{(0)}\). The results for this dataset with different number of samples is shown in the right plot of Figure 1. Our experiments illustrate that on the synthetic dataset, our approach outperforms all other approaches except the GSW design. However, we note that our approach is computationally more efficient than GSW design (see Table 6) and gives a much simpler design that can be used in different settings.
2. **ITE Dataset.** The covariate matrix \(\) and the vectors \(,,^{(0)}\) are picked similar to the ATE dataset. Then individual treatment effect vector \(\) is \(+\) and \(^{(1)}=^{(0)}+\). The results for this dataset with different number of samples is shown in the left plot of Figure 1. ITE error for our method is consistently smaller than leverage score based method.

**Real-World Datasets** We analyze the following three distinct real-world datasets. A comprehensive breakdown, including variance and other measurements for each dataset, is available in Table 5 in the appendix.

1. **Boston Dataset :** This is a dataset of housing prices in the Boston area, consisting of 506 samples and 13 features. Since it has only one label, we set \(^{(1)}=^{(0)}\), i.e., ATE is zero.
2. **IHDP Dataset :** Derived from the characteristics of children and their mothers, this dataset comprises 747 samples and 25 features. ATE for is -4.016.
3. **Twins Dataset :** This dataset is constructed based on the characteristics and mortality rates of twin births in the US. We specifically selected samples that have complete feature values, resulting in a dataset of 32,120 samples with 50 features. The ATE of this dataset is 0.0064.

## 6 Conclusion

In this paper, we considered the problems of mean estimation, ATE and ITE estimations in the presence of covariates. We considered the finite population setting and provided non-asymptotic variance bounds for several novel variants of the classical regression adjustment method-based estimators. Our guarantees are model-free, even if the covariates are arbitrary (are not informative of the estimand and may be chosen adversarially), the variance bounds are still meaningful and almost match the variance bounds of widely used estimators not based on covariates.

Our algorithms are simple and efficient, and we believe they readily extend to many related settings like arbitrary assignment probabilities and online treatment assignments. We also believe the results can be stated for the kernel setting. These extensions will be part of future work.

  Dataset & Uniform HT & GSW & Classic Reg Adj & Lev Score & 4 vecs & Ours \\   & \(1.736\) & \(0.663\) & \(0.333\) & \(0.658\) & \(1.677\) & \(0.628\) \\  & \( 1.339\) & \( 0.510\) & \( 0.255\) & \( 0.504\) & \( 1.256\) & \( 0.459\) \\   & \(0.272\) & \(0.042\) & \(0.012\) & \(0.536\) & \(0.264\) & \(0.040\) \\  & \( 0.206\) & \( 0.031\) & \( 0.009\) & \( 0.050\) & \( 0.203\) & \( 0.030\) \\   & \(1.351{ e}{-3}\) & \(1.231{ e}{-3}\) & \(1.201{ e}{-3}\) & \(1.226{ e}{-3}\) & \(1.369{ e}{-3}\) & \(1.218{ e}{-3}\) \\  & \( 0.025{ e}{-3}\) & \( 0.937{ e}{-3}\) & \( 0.899{ e}{-3}\) & \( 0.911{ e}{-3}\) & \( 0.015{ e}{-3}\) & \( 0.936{ e}{-3}\) \\  

Table 1: Results of ATE estimation. For each result, the first number is the average of \(|-|\) over 1000 trials and the second number is the standard deviation of this quantity.

  Dataset & Baseline & Leverage Score & Ours & Leverage Score Relative & Ours Relative \\  Boston & \(0.0\) & \(0.827 0.469\) & \(7.654 1.678\) & - & - \\  IHDP & \(0.548\) & \(2.449 0.071\) & \(1.765 0.223\) & \(0.021 0.637{ e}{-3}\) & \(0.015 1.9{ e}{-3}\) \\  Twins & \(0.155\) & \(0.156 2.66{ e}{-5}\) & \(0.156 2.21{ e}{-4}\) & \(5.571{ e}{-3} 9.5{ e}{-7}\) & \(5.571{ e}{-3} 7.9{ e}{-7}\) \\  

Table 2: Results of ITE estimation. For each result, the first number is the average of \(}\|-}\|_{2}\) over 1000 trials and the second number is the standard deviation of this quantity. The relative error columns report \(}\|-}\|_{2}/\| \|_{2}\).

Figure 1: Results for synthetic ITE and ATE datasets are shown on the left and right, respectively. For different population sizes (i.e., \(n\)), estimation is performed for 1000 trials. Then the average of \(-}\|_{2}}{\| \|_{2}}\) over these trials is shown with solid lines for ITE and the shades around these lines denote the standard deviation. For ATE, the solid lines represent the average of \(|-|/||\) over 1000 trials.