# Resolving Discrepancies in Compute-Optimal Scaling of Language Models

Tomer Porian

Tel Aviv University; correspondence to tomerpor@gmail.com and ycarmon@tauex.tau.ac.il.

Mitchell Wortsman

University of Washington.

Jenia Jitsev

Jcarton@yahoo.com

Ludwig Schmidt

Julich Supercomputing Centre (JSC) and LAION.

Yair Carmon

Tel Aviv University; correspondence to tomerpor@gmail.com and ycarmon@tauex.tau.ac.il.

###### Abstract

Kaplan et al.  and Hoffmann et al.  developed influential scaling laws for the optimal model size as a function of the compute budget, but these laws yield substantially different predictions. We explain the discrepancy by reproducing the Kaplan et al. scaling law on two datasets (OpenWebText2 and RefinedWeb) and identifying three factors causing the difference: last layer computational cost, warmup duration, and scale-dependent optimizer tuning. With these factors corrected, we obtain excellent agreement with the Hoffmann et al. (i.e., "Chinchilla") scaling law. Counter to a hypothesis implied in Hoffmann et al. , we find that careful learning rate decay is not essential for the validity of their scaling law. As a secondary result, we derive scaling laws for the optimal learning rate and batch size, finding that tuning the AdamW \(_{2}\) parameter is essential at lower batch sizes.

## 1 Introduction

We consider the problem of compute-optimal language model training: given a compute budget \(C\), we wish to predict how to best allocate it across model size (in parameters) and dataset size(in tokens). With pretraining budgets ever-increasing, compute-optimal scaling is a question of paramount importance. In their seminal work, Kaplan et al.  proposed a scaling law predicting that the optimal ratio of tokens to parameters decays as a power of \(C\). This scaling law was influential in determining the size of GPT-3 and several subsequent models . However, Hoffmann et al.  challenged its validity, arguing instead that the optimal token-to-parameter ratio should be approximately independent of \(C\), and that contemporary models had too many parameters relative to their number of training tokens. Based on this prediction, they trained a 67B parameters model called Chinchilla and which outperformed larger models with a similar compute budget.

While Hoffmann et al.  and subsequent work  established that following the Hoffmann et al. scaling law leads to better performance than Kaplan et al. scaling, it is still important to understand _why_ the two works arrived at different conclusions. Is the difference due to architecture, training setup, pretraining data, results analysis, or perhaps something else entirely? The answer could teach us important lessons on how to correctly predict and perform model scaling.

Hoffmann et al.  hypothesize that the scaling law discrepancy is due to Kaplan et al.  not tailoring the learning rate schedule for each token budget separately. While they demonstrate that mismatched learning rate decay results in a higher loss, they do not show it leads to a different compute-optimal scaling law. We further discuss Hoffmann et al. 's hypothesis in Appendix A. To the best of our knowledge, this hypothesis is the only explanation offered in the literature so far.

Our contribution.In this work, we uncover three factors contributing to the discrepancy, and disprove Hoffman et al.'s hypothesis about the role of learning rate decay; Figure 1 illustrates our mainresults. We begin by reproducing the Kaplan et al. scaling law in a Llama-derived pretraining setup using the OpenLM library  and the RefinedWeb dataset  (Figure 1a). Our first observation is that accounting for the computational cost of the decoding layer (as done in Hoffmann et al.  but not in Kaplan et al. ) shifts compute-optimal scaling toward a more constant token-to-parameter ratio (Figure 1b). Second, we note that the constant-length warmup period of Kaplan et al.  is too long for smaller models, inflating the optimal number of tokens at lower compute budgets; scaling the warmup period with the model size further shifts the scaling in the Hoffmann et al. direction (Figure 1c). Next, we match the learning rate decay to the token budget of each configuration we test (as Hoffmann et al.  conjecture to be essential) but observe little effect on the compute-optimal scaling law (Figure 1d). Finally, we set the learning rate, batch size, and the AdamW \(_{2}\) parameters individually for each model size, leading to compute-optimal scaling that agrees closely with Hoffmann et al. (Figure 1e). Notably, the latter configuration uses a constant learning rate schedule, showing that learning rate decay is not essential for the Hoffmann et al. scaling law to emerge. We repeat our experiment on the OpenWebText2 dataset , observing similar results despite performing hyperparameter tuning only on RefinedWeb.

We complement our main results with the following analyses:

1. In the last phase of our experiments (Figure 1e) we choose different hyperparameters for each model size. To do so, we conduct a hyperparameter sweep for small-scale models and use the results to fit power laws for the optimal batch size and learning rate as a function of model parameters. This approach is inspired by DeepSeek , and our hyperparameter scaling laws roughly agree. However, we observe that setting the AdamW \(_{2}\) parameter to be \(0.95\) is suboptimal at smaller batch sizes (128 and below), and increasing it allows establishing clear trends from our small-scale hyperparameter sweep.
2. We study the scaling of the optimal loss as a function of the compute budget. We show that the steps we take to settle the Kaplan et al./Hoffmann et al. discrepancy (namely shortening warmup and scaling learning rate and batch size) significantly decrease this loss at smaller scales, but only marginally improve it at larger scales. In contrast, introducing a cosine learning rate decay

Figure 1: By analyzing over 900 training runs, we uncover the factors leading to the discrepency between the scaling laws of Kaplan et al. (panel a) and Hoffmann et al. (panel e). Each panel shows observations of the optimal model size \(N^{}\) as a function of the compute budget \(C\), as well as power law fits of the form \(N^{}(C) C^{a}\). Labels show point estimates and 95% confidence intervals for \(a\) and for the optimal model at \(C_{C}=5.88e23\), the compute budget used for training Chinchilla.

schedule substantially decreases the loss, with benefits persisting at larger scales. Similar to Hoffmann et al. , we observe some curvature on the optimal loss curve. Nevertheless, the optimal loss with tuned hyperparameters is fairly consistent with a saturating power law.
3. We calculate the computational cost of each of our experiments and plot how prediction quality improves as we consider larger training runs. We observe that the cost of our hyperparameter sweep is comparable to that of a scaling law fit experiment, but the compute saved by using a constant instead of a cosine learning rate schedule roughly makes up for that cost.

Code and data release.To facilitate future research, we share the data and the code necessary to reproduce our analyses and figures at https://github.com/formll/resolving-scaling-law-discrepancies. In addition, checkpoints of the models we train are available at https://huggingface.co/formll/resolving-scaling-law-discrepancies.

## 2 Preliminaries and experiment design

### Notation and problem setting

We train language models of _size_\(N\) on \(D\) tokens of data (essentially without repetition). The precise definition of \(N\) plays an important role in this paper: Unless mentioned otherwise, \(N\) denotes the number of parameters in all the linear layers of the model. That is, \(N\) excludes embedding layers, but includes the model's _head_: the final linear layer producing the predicted token logits. (In the models we train there is no tying of the embeddings and the head).

Let \((N,D)\) be the ammount of floating point operations (FLOPs) required to train a model of size \(N\) on \(D\) tokens. Throughout, we employ the approximation

\[(N,D) 6ND\] (1)

In Sections 3.1 and 3.2 we compare our definition of \(N\) to the one used in Kaplan et al. . In Appendix B we also discuss the effect of taking attention FLOPs into account and FLOP estimation approaches in other works.

Let \(L(N,D)\) be the log loss (in expectation over the training data distribution and any randomness of the training procedure) obtained by a model of size \(N\) trained for \(D\) tokens.4 Assuming a fixed compute budget \(C\), we aim to predict

\[N^{}(C)*{argmin}_{N>0}LN, *{argmin}_{N>0}_{D:(N,D)=C}L(N,D),\] (2)

i.e., the model size yielding the smallest loss when trained with compute budget \(C\) under the approximation (1).

We also let

\[D^{}(C)}\;\;\;\;^{}(C) (C)}{N^{}(C)}=(C)]^{2}}\] (3)

denote the optimal number of tokens and the optimal token-to-parameter ratio. To predict these quantities, we use power laws of the form:

\[N^{}(C) N_{0}^{} C^{a}\;\;,\;D^{}(C) D_{0} ^{} C^{b}\;\;\;\;^{}(C)_{0}^{}  C^{r},\] (4)

and fit the _exponents_\(a,b,r\) and _coefficients_ where \(N_{0}^{},D_{0}^{},_{0}^{}\) from data as described below.

### Training setup

We train decoder-only Transformer language models using OpenLM , which integrates many of the architecture and training advances in Llama [55; 56] and subsequent works. We largely base our initial training configuration on the hyperparameter search in Gadre et al. . Our setup does not replicate Kaplan et al. , but we match or closely approximate several key hyperparameters as discussed in Section 3. See Appendix C for a detailed description of our setup and chosen hyperparameters.

Model set.We search for compute-optimal models over a set consisting of 16 models with sizes ranging from \(5\)M to \(901\)M. We pick model layer numbers \(l\) and widths \(d\) such that \(N\) increases by multiples of roughly \(\) while the aspect ratio \(d/l\) stays between \(32\) and \(64\) as suggested in Kaplan et al. . The number of attention heads in each configuration is \(4\), as preliminary experiments showed this is optimal for smaller models, and increasing it did not noticeably improve larger models. Table 2 in the appendix specifies all the models in our grid.

Data.We perform our experiments on OpenWebText2  which contains roughly \(30\)B tokens of data from Reddit and resembles the WebText2 dataset used in Kaplan et al. , as well a RefinedWeb  dataset which contains roughly \(600\)B tokens from CommonCrawl  and resembles the MassiveWeb dataset that formed roughly half of the data mix in Hoffmann et al. .

Evaluation and FLOP grid.We evaluate models on \(160\)M tokens held out from the training data. We perform the evaluation whenever the product of \(6N\) and the number \(D\) of training tokens seen so far crosses an element of a _FLOP grid_ of the form \(\{1.25\)e\(16 2^{i}\}_{i=0}^{11}\). This grid plays a central role in our data analysis. We also record the average training loss every 20 steps.

### Data analysis

Our technique for estimating the compute-optimal power law is akin to the second (IsoFLOP-based) approach of Hoffmann et al. , but differs in several details. The approach consists of two steps: directly estimating \(N^{}(C_{i})\) for all \(C_{i}\) in our FLOPs grid, and fitting a power law to these estimates. We briefly outline each step below and provide full details in Appendix D.

Estimating \(N^{}(C_{i})\).For each value of \(C_{i}\), we train several models from our set (Table 2) for \(C_{i}\) FLOPs and extract an _IsoFLOP curve_ of loss vs. model size (see Figure 10). For FLOP values where validation loss is not available (specifically Section 3.1 and Appendix B) we use the smoothed training loss instead. We estimate \(N^{}(C_{i})\) and its uncertainty using a noise-and-interpolate procedure based on Gaussian noise with empirically-calibrated magnitude and Akima interpoaltion . For every \(C_{i}\), this yields a "bootstrap sample" population optimal size estimates; we take their median as the point estimate for \(N^{}(C_{i})\). The procedure also yields an estimate of the log-scale standard deviation of \(N^{}(C_{i})\) (shown as error bars in Figure 1).

Fitting a power law.We fit power laws of the form (4) by performing weighted linear regression in log space, with the weights inversely proportional to the squared log-space standard deviations computed above (i.e., log-space Gaussian maximum likelihood estimation). To obtain a point estimate for the power law parameters we fit the point estimates for each \(N^{}(C_{i})\) value. To quantify uncertainty, we fit power laws to bootstrap samples, obtaining a population of \(N^{}_{0}\), \(a\), and \(N^{}()\) samples. We construct confidence intervals from their quantiles.

## 3 Main results: settling the scaling law discrepancy

In this section, we describe in detail our main results, visualized in Figure 1, tabulated in Table 1 and plotted in detail in Appendix E. The following subsections address each panel of Figure 1 in order.

### Reproducing the Kaplan et al. scaling law

To reproduce the Kaplan et al. scaling law, we match the setup of  in terms of the batch size (\(2^{19}\) tokens) and in terms of the learning rate schedule (warmup for \(3000 2^{19} 1.57\)B tokens followed by cosine decay to zero at \(2.5\)e\(5 2^{19} 131\)B tokens). Other configurations do not match exactly, but the suite of models we train covers a range of sizes and compute similar to Kaplan et al. . For this reproduction only, we also take the "model size" \(N\) to be the number of parameters in all linear layers except the head (last decoding layer). That is, for a model of width \(d\) and vocabulary size \(v\), we subtract \(d v\) from our usual definition of \(N\) (see Table 2, last column).

As Figure 0(a) shows, with this setting we obtain a compute-optimal exponent \(a\) and power law fits close to the power law \(1.669(C/8.64e19)^{0.88}\) obtained by Kaplan et al. [30, Figure 14, left].

### Counting last layer FLOPs

Kaplan et al.  chose to define model size without counting embedding parameters since they found this makes scaling laws in the infinite-compute regime more consistent across network depths (30, Figure 6). Perhaps because their model head and embeddings had tied weights, this led them to also discount the contribution of the model head to the model's FLOPs per token (30, Table 1, last row). However, as Table 2 reveals, not accounting for the model head leads to under-approximation that grows smoothly as model size decreases, from roughly \(10\%\) at larger models to roughly \(90\%\) at smaller models. Thus, counting the head FLOPs (i.e., using our definition of \(N\)) results in a significantly more accurate approximation. As shown in Figure 0(b), switching to our model size count also reduces the exponent \(a\) by more than \(0.1\), closer to Hoffmann et al. but not all the way there.

### Correcting learning rate warmup

Next, we address the duration of the learning rate warmup period, which Kaplan et al.  set proportionally to their full training duration, designed to reach complete convergence. Figure 2 (left) shows this warmup period is too long: for smaller-scale models, the optimal number of tokens as a function of compute is less than or close to the number of warmup tokens, and therefore these models are suboptimally trained. The same issue is evident in Figure 14 (right) of Kaplan et al.  which shows that for many compute budgets the optimal number of steps is below or close to the number of warmup steps (fixed at 3000). Figure 2 (left) also provides an intuitive explanation for the increased value of \(a\): at smaller compute scales, models are 'forced' to use more training tokens than would otherwise be optimal in order to 'escape' the long warmup period. Having escaped, the warmup Once this warmup period is escaped, the optimal number of tokens grows only slowly, leading to a fast rate of increase in the optimal model size and hence the large exponent.

With the problem identified, we propose a simple heuristic for more appropriately choosing the warmup duration: for each model, we set the number of warmup tokens to be identical to the model size \(N\). We validate our warmup heuristic in Appendix F. The bottom row of Figure 1(b) illustrates the validity of our new choice of warmup, showing that the optimal number of tokens is always at least 5 times greater than the (interpolated) duration of the warmup period corresponding to the model of the appropriate size. As is evident from this figure and from Figure 0(c), shortening the warmup shifts the scaling law in the direction of Hoffmann et al. further, yielding an exponent \(a\) of roughly 0.6.

### Learning rate decay has limited impact on compute-optimal allocation

With learning rate warmup corrected, we turn to study learning rate decay, which Hoffmann et al.  conjecture to be a main cause of the difference between their result and Kaplan et al. . We

   Experiment & Dataset & \(a\) estimate & \(R^{2}\) of \(a\) fit & \(^{}\) range \\  Hoffmann et al.  & MassiveText & \(0.5\) & & \\ Kaplan et al.  & WebText2 & \(0.88\) & & \\ Adjusted Kaplan et al.  & WebText2 & \(0.73\) & & \\  Reproducing Kaplan et al. (§3.1) & OpenWebText2 & \(0.864\) (\(0.82\), \(0.90\)) & \(0.998\) & \((5,2617)\) \\  & RefinedWeb & \(0.835\) (\(0.82\), \(0.85\)) & \(0.999\) & \((8,1536)\) \\ Counting last layer FLOPs (§3.2) & OpenWebText2 & \(0.699\) (\(0.66\), \(0.72\)) & \(0.998\) & \((8,262)\) \\  & RefinedWeb & \(0.706\) (\(0.69\), \(0.72\)) & \(0.998\) & \((9,232)\) \\ Correcting warmup (§3.3) & OpenWebText2 & \(0.603\) (\(0.57\), \(0.63\)) & \(0.994\) & \((7,55)\) \\  & RefinedWeb & \(0.602\) (\(0.59\), \(0.62\)) & \(0.993\) & \((7,50)\) \\ Cosine decay (§3.4) & OpenWebText2 & \(0.574\) (\(0.54\), \(0.61\)) & \(0.999\) & \((7,42)\) \\  & RefinedWeb & \(0.571\) (\(0.56\), \(0.59\)) & \(0.998\) & \((10,39)\) \\ Optimizer tuning (no decay) (§3.5) & OpenWebText2 & \(0.518\) (\(0.49\), \(0.54\)) & \(0.998\) & \((11,22)\) \\  & RefinedWeb & \(0.497\) (\(0.49\), \(0.50\)) & \(0.997\) & \((14,16)\) \\ Reprod. adjusted Kaplan et al. (§H) & RefinedWeb & \(0.717\) (\(0.71\), \(0.72\)) & \(0.992\) & \((12,345)\) \\   

Table 1: Summary of our main results (described in Section 3).

observe that the long \(131\)B tokens decay period in Kaplan et al. , which is aimed toward training to full convergence, means that their compute-constrained experiments see virtually no learning rate decay: Figure 2 shows that, at our compute scales, it is never optimal to train for more than \(10\)B, which corresponds to less than \(1.5\%\) decay with a cosine schedule.

To correct this, we follow the second approach of Hoffmann et al.  and choose the learning rate schedule for every model and FLOP budget individually. For each FLOP value in our grid, we pick the \(7\) models from Table 2 which yield token-to-parameter ratios in the range \(1\) to \(100\), and train them with a cosine learning rate schedule that decays to \(1\%\) of the maximum learning rate when reaching the target FLOP value.5 This is roughly twice as expensive as previous experiments, which required only a single training run for each model size (see additional discussion in Section 4.2). As Figure 1d shows, adding cosine decay results in a slightly cleaner linear trend (\(R^{2}\) improves from \(0.993\) to \(0.998\)) and an exponent slightly closer to the Hoffmann et al. scaling law (\(0.57\) instead of \(0.6\)), but most of the gap remains. Therefore, even with the FLOP count and warmup issues corrected, adding learning rate decay is not sufficient to reproduce the Hoffmann et al. scaling law.

### Correcting batch size, learning rate and \(_{2}\)

A final factor contributing to the Kaplan et al./Hoffmann et al. discrepancy is the choice of optimization hyperparameters, particularly the batch size: with a fixed batch size of \(2^{19}\) tokens, compute-optimal models at smaller scales train for only a few hundred steps, which is likely too little. Kaplan et al.  notice this issue, and attempt to correct for it using post-processing based on an empirical model of large-batch size training ; we return to their result at the end of this section.

Here, we take the more direct approach of predicting near-optimal hyperparameters for each model size.6 Since changing the batch size often also requires re-tuning the learning rate [19; 36; 49; 61], we sweep over both parameters for models of sizes \(5\)M to \(108\)M, with an additional validation sweep over models of size \(220\)M. Initially, we kept \(_{2}\) at its previous value of \(0.95\). However, this led to poor results at smaller batch sizes: as the batch size gets smaller, the squared gradients become noisier, and AdamW requires more smoothing to obtain a correct denominator. Therefore, we added \(0.99\) and \(0.999\) to the sweep, obtaining improved performance on small batch sizes. This empirical observation matches the theoretical work by Zhang et al. , that showed that when the batch size is small, higher values of \(_{2}\) are crucial for the convergence of Adam. In Appendix G we describe the parameter sweep in full, validate the optimality of our prescribed hyperparameters on the largest model we train, and provide additional discussion about the role of \(_{2}\).

Figure 3 plots our estimates for the optimal values of batch size and learning rate for each model size. It shows clear trends, to which we fit power laws in the number of parameters \(N\). Observing good

Figure 2: The optimal number of tokens \(D^{}\) as a function of the compute budget \(C\). **Left:** Using the warmup period of Kaplan et al. , smaller models reach compute-optimality during warmup. **Right:** Setting the number of warmup tokens to be identical to the model size (visualized using the power law fit) ensures models reach compute-optimality well after the warmup and yields a scaling law closer to Hoffmann et al.. We replicate these plots for all of our experiments in Appendix E.

extrapolation to nearby values of \(N\), we apply these power laws (with slight rounding) to select the batch size and learning rate for all model sizes and tabulate the results in Table 4. In Appendix G.5 we validate our learning rate and batch size scaling law for a model with \(901\)M parameters.

Our parameter tuning approach is inspired by DeepSeek , who predict the optimal batch size and learning rate as a function of compute. Translating compute to model size using the Hoffmann et al. scaling law, we find remarkable agreement in the batch size predictions (a difference of less than \(0.05\) in exponent and less than \(60\%\) in predictions over our models), and somewhat different learning rate predictions (a difference of \(0.11\) exponent and a factor of \(2\)-\(3\) in predictions), potentially due to using different weight decay. Both our results appear to contradict the conventional wisdom about the existence of a critical batch size [49; 61; 36] below which every batch size is good, finding instead an optimal batch size below which performance degrades. This suggests further tuning of \(_{2}\) or other hyperparameters may be warranted. We discuss DeepSeek  further in Section 5.1.

With the new hyperparameters, we obtain a close reproduction of the Hoffmann et al. scaling law (Figure 0(e)) with the scaling exponent matching \(0.5\) to within \(0.6\)% and the predicted model size at Chinchilla compute within \(15\)% of Chinchilla's size. Notably, here we use a _constant learning rate schedule_, demonstrating that careful learning rate decay is not necessary for this scaling law to hold.

Finally, we reproduce the adjusted scaling law \(N^{}(C)=1.39(C/8.6419)^{0.73}\) which Kaplan et al.  obtain by estimating the compute required to reach the same results at a sufficiently low batch size. To do so, we use our tuned hyperparameters as a proxy for suitable batch size and revert our previous corrections (head FLOP count and warmup duration). We obtain an exponent of \(0.717\) and good agreement with their adjusted scaling law; see Figure 18 in Appendix H.

## 4 Additional Analysis

### Trends in compute-optimal loss

Figure 4 shows the minimum loss achievable for each compute budget \(C\) in the experiments shown in Figure 1. We estimate the minimum loss using the same interpolation procedure we use to extract the optimal parameter number \(N^{}\) and token count \(D^{}\). The figure shows that, at low compute scales, shortening the warmup duration and tuning hyperparameters leads to substantial loss improvements (each by up to 0.5 nat per token). However, at larger scales these interventions do not significantly improve the loss. In contrast, learning rate decay becomes increasingly beneficial as compute grows, and appears to also improve the rate of decrease in the loss. Perhaps coincidentally, the effects of overestimating the optimal loss (due to long warmup and large batch size) seem to closely offset the effect of underestimating computational cost (by discounting the contribution from the model's head): the first and last curves in Figure 4 closely overlap.

Similarly to Hoffmann et al.  we observe a curvature in the optimal loss, while Kaplan et al.  report a near-perfect power law behavior. This difference is due to a combination of the difference in FLOP counts discussed in Section 3.2 and the fact that the experiments of Hoffmann et al. 

Figure 3: Fitting scaling laws for the optimal batch size and learning rate as a function of the model size \(N\). Markers indicating grid points are shaded by their excess loss compared to all configurations for this parameter, reaching maximum transparency for loss that is suboptimal by \(0.03\) or more. We also plot interpolation-based estimates of the optimal parameter values and fit them with power laws.

extend to higher compute budgets where the loss is closer to its irreducible level. Indeed, for the tuned optimizer experiment (Section 3.5) we find that a saturating power law fits the optimal loss and extrapolates well, while extrapolating poorly for other experiments (see Figure 19 in the appendix). This suggests that a predictable trend in \(L(N^{}(C),D^{}(C))\) is an indicator of locally-optimal hyperparameters. The exponent of our saturating power fit is approximately \(-0.1\), twice as large as the exponent found in Kaplan et al. .

Finally, we validate our compute-optimal loss scaling law by training and evaluating a model using a larger compute budget. Specifically, we train a \(901\)M parameter model with a compute budget of \(C_{+} 819\) FLOPs, which our scaling law predicts to be compute-optimal, using the batch size and learning prescribed by our hyperparameter scaling laws (see Table 4), reaching a loss value of \(L_{+}=2.943\). In Figure 6, we add the point \((C_{+},L_{+})\) to the red curve in Figure 4 and find that it falls within the predicted trend. Notably, \(L_{+}\) is obtained with a single training run using our predicted optimal configuration, while at lower compute values we estimate the optimal loss by interpolating an IsoFLOP curve.

### Scaling law accuracy as a function of compute

We now estimate the computational cost of our scaling law experiments, quantifying the effect of the learning rate schedule, and plot how our predictions improve and become more confident with increased computation. We find that the training cost of each experiment that utilized a fixed learning rate schedule was \(1.5420\) FLOPs, while the experiments that used a varying-length cosine learning rate schedule required \(2.9920\) FLOPs; essentially double the compute; see Appendix J for more details. We also find that the cost of the hyperparameter sweep described in Section 3.5 was \(2.0420\) FLOPs--slightly less than the combined cost of two scaling experiments that leveraged it (one on each dataset). Moreover, in hindsight, we could have arrived at similar hyperparameters using only models of size at most \(57\)M and a simple heuristic for choosing \(_{2}\) based on batch size, which would have cost only \(1.4419\) FLOPs.

Figure 4: The minimum loss achievable by models with compute budget \(C\). For the Kaplan et al. scaling law reproduction, we estimate \(C\) as in Section 3.1. See expanded version in Figure 19.

Figure 5: Compute optimal exponent prediction, confidence, and root-mean-square relative error as a function of the total scaling experiment budget for the tuned optimizer experiment described in Section 3.5.

Figure 6: The compute-optimal loss curve of Figure 4 extended to compute budget \(C_{+} 819\) by training a single model with size, learning rate and batch size determined using our scaling laws. Each subplot uses a different number of lower-compute loss measurement to fit the loss trend.

Figure 5 shows the evolution of the predicted compute-optimal model size exponent \(a\), its confidence interval, and a measure of the prediction accuracy as we modulate the experiment's compute budget by truncating our FLOP grid. The figure shows that the prediction becomes steadily more accurate and confident as compute increases. We present these results, as well as the results in Section 4.1, on the OpenWebText2 dataset as well (see Appendix I).

## 5 Discussion

### Related work

While neural scaling laws precede the advent of large language models [24; 45], breakthroughs in model  and data [42; 44] scaling allowed Kaplan et al.  to demonstrate the dramatic possibility of unbounded improvement with scale, triggering an explosion in the literature on the topic. Here we focus on the relatively fewer works that tackle optimal resource allocation under a _compute_ constraint.

For language modeling, Hu et al.  and DeepSeek  repeat subsets of the analyses in Hoffmann et al.  and derive compute optimal scaling laws. Employing Approach 3 of Hoffmann et al.  (see also ), Hu et al.  find that, for their models, optimal scaling favors larger token-to-parameter ratios than in Hoffmann et al.  and in our results. They attribute this difference to modeling improvements since  and argue the same holds for Llama 2 . However, our setup incorporates most of the advances in Llama 2 and still produces power laws very close to Hoffmann et al. . Like us, DeepSeek  perform hyperparameter tuning and use isoFLOP analysis to determine compute-optimal model sizes on multiple datasets. While they arrive at an exponent on the order of Hoffmann et al.  for the main dataset they study, they report a higher exponent \(a=0.578\) for OpenWebText2 (i.e., predicting lower token-parameter-ratio at scale), which they attribute to the superior quality of the dataset. We also study this dataset but arrive much closer to the Hoffmann et al. scaling law. We conjecture the larger exponent might be due to repeating training data, which likely occurred in their experiment given the dataset's limited size and their compute budget. Settling these discrepancies could be a source of further valuable lessons on optimal model scaling.

Recent work also studies compute-bounded scaling laws beyond the compute-optimal regime. Informed by the increasingly common practice of training medium-scale models beyond compute optimality [e.g., 55; 56; 29], Sardana and Frankle  account for the expected inference cost of the model, showing that it naturally skews optimal settings toward smaller models. Gadre et al.  directly predict the loss and downstream performance for models trained past the point of compute optimality, and Muennighoff et al.  model joint compute-data bottlenecks. All three works rely on the Hoffmann et al. law as a reference point, with [17; 37] baking it to their parametric forms.

Compute-optimal scaling is studied beyond the language domain, particularly in vision. Henighan et al.  study autoregressive modeling for a variety of tasks and find scaling laws roughly consistent with the Kaplan et al.  adjusted scaling law (with exponent \(a=0.73\)). That work shares the methodological issues described in the top row of Figure 1 (FLOP count and long warmup), but performs hyperparameter tuning for smaller scale models; in Appendix H we reach similar results when doing the same. Zhai et al.  characterize the compute-efficient frontier of Vision Transformers (ViTs), while Cherti et al.  studies compute constrained scaling of CLIP models. However, they do not offer a power law for scaling model size with compute. Alabdulmohsin et al.  tackle model design under an _inference compute_ constraint by fitting multi-term parametric forms to obtain predictions for the optimal ViT shape. Goyal et al.  point out an intricate interplay between data filtering and compute constraints. Finally, Bachmann et al.  study compute-optimal scaling of MLP's and obtain exponent \(a=0.35\), suggesting that MLP require much more rapid data growth than more sophisticated architecture. Overall, whether and to what extent does Hoffmann et al. scaling hold in the vision domain remains a compelling open problem.

Recent theoretical work study compute-optimal scaling laws in simplified, analytically tractable settings [11; 33; 38; 28]. In particular, Paquette et al.  obtain a power law with exponent \(a=0.5\) (as in Hoffmann et al. ) for a random-feature linear regression setting [35; 5], conjecturing that this is a part of a broader, universal phenomenon. Jeon and Van Roy  also establish an exponent of \(0.5\) for data generated by infinite-width two-layer ReLU networks, using information-theoretic arguments.

We also remark on two themes of our paper that draw from prior work. The first is the importance of hyperparameter tuning: several works [30; 27; 17; 15] make the case that smooth, predictable scaling laws emerge when models on all scales are properly tuned. Our work (and particularly Section 4.1) provides another example of this principle and agrees with previous observations that tuning is particularly important at smaller scales. Second, previous studies [59; 8; 15; 26] as well as the concurrent work , propose alternative learning rate schedules that address a key shortcoming of cosine decay: the need to commit to a step budget in advance. We consider a constant learning rate that requires no commitment at all. We show this simple choice suffices to reproduce the Hoffmann et al. law and quantify the computational savings compared to a cosine schedule. However, Section 4.1 (and also , among others) show that in terms of loss, the constant schedule clearly underperforms the cosine schedule.

Concurrent and independent work.Pearce and Song  also study the discrepency between the Kaplan et al. and Hoffmann et al. scaling laws. By re-analyzing data extracted from the Hoffmann et al.  experiments by Besiroglu et al. , they identify the last layer FLOP count as a cause for the discrepancy. Moreover, they report on a small-scale experimental study (with model sizes up to \(5\)M and training tokens number up to \(530\)M) in which they observe that a non-decaying learning rate schedule is sufficient for reproducing the Hoffmann et al. exponent and that learning rate tuning is necessary. These results independently corroborate part of our observations in Sections 3.2, 3.4 and 3.5. Pearce and Song  do not identify the warmup duration issue we describe in Section 3.3. As a consequence, when reproducing the Kaplan et al. exponent they reach a value close to 0.73 rather than the 'raw' value 0.88 reported in Kaplan et al.  (see discussion in Section 3.1, Section 3.5, and Appendix H). In addition, our experiments roughly match the Kaplan et al.  compute budget, which is about 3 orders of magnitudes larger than budget in Pearce and Song , and we perform careful tuning of both the learning rate and the batch size.

### Limitations

Computational scale is a notable limitation, as well as a defining feature, of our results: our experiments are roughly on the scale of those in Kaplan et al.  but are substantially smaller than those of Hoffmann et al. . Scaling may effectively mitigate each of the issues we identify: with scale, the contribution of the model head becomes negligible, any (fixed) warmup period eventually becomes reasonably long, and hyperparameter sensitivity decreases, as shown in Figure 4 and Figure 15. Nevertheless, we believe that experimental protocols that induce correct scaling behavior at low computational budgets are crucial for developing the empirical science of machine learning, particularly in academic settings.

Due to limited compute budgets, our hyperparameter sweep only targeted the smaller models in our grid, and furthermore trained each model for only \(20N\) steps, i.e., the optimal point according to the Hoffmann et al. scaling law. This raises the concern that the hyperparameters we chose unfairly favor models trained for that particular token-to-parameter ratio, and rerunning our experiment with perfect tuning for each model size _and_ each token-to-parameter ratio would have yielded different results. We believe this is unlikely: at small scales (where hyperparameter tuning is crucial) our original set of hyperparameters favored higher token-to-parameter ratios because they still had a sufficient number of steps to train for, and therefore choosing hyperparameters specifically for them is not likely to result in significant gains. In Appendix G.4 we analyze our existing tuning results to estimate the potential gains from perfect tuning, and find that they are likely to have small impact on our conclusions. Moreover, transferring our hyperparameters to another dataset yields similar results.

Finally, a broader limitation of compute-optimal scaling as defined by Kaplan et al. , Hoffmann et al.  and our work, is that it only concerns the pretraining loss rather than more direct measures of a model's capabilities. Here again, scale is an issue: most zero-shot and in-context capabilities do not emerge at the scales we consider here, and predicting them from small-scale proxies is an important open problem [48; 17]. Instead, it is possible to study downstream performance via fine-tuning, though this may cause the clean scaling patterns seen in pretraining to break down , potentially because the fine-tuning procedure is sensitive to the choice of hyperparameters .