# Guarantees for Self-Play in Multiplayer Games via Polymatrix Decomposability

Revan MacQueen

Department of Computing Science

University of Alberta / Amii

revan@ualberta.ca

&James R. Wright

Department of Computing Science

University of Alberta / Amii

james.wright@ualberta.ca

###### Abstract

Self-play is a technique for machine learning in multi-agent systems where a learning algorithm learns by interacting with copies of itself. Self-play is useful for generating large quantities of data for learning, but has the drawback that the agents the learner will face post-training may have dramatically different behavior than the learner came to expect by interacting with itself. For the special case of two-player constant-sum games, self-play that reaches Nash equilibrium is guaranteed to produce strategies that perform well against any post-training opponent; however, no such guarantee exists for multiplayer games. We show that in games that approximately decompose into a set of two-player constant-sum games (called constant-sum polymatrix games) where global \(\)-Nash equilibria are boundedly far from Nash equilibria in each subgame (called subgame stability), any no-external-regret algorithm that learns by self-play will produce a strategy with bounded vulnerability. For the first time, our results identify a structural property of multiplayer games that enable performance guarantees for the strategies produced by a broad class of self-play algorithms. We demonstrate our findings through experiments on Leduc poker.

## 1 Introduction

Self-play is one of the most commonly used approaches for machine learning in multi-agent systems. In self-play, a learner interacts with copies of itself to produce data that will be used for training. Some of the most noteworthy successes of AI in the past decade have been based on self-play; by employing the procedure, algorithms have been able to achieve super-human abilities in various games, including Poker (Moravcik et al., 2017; Brown and Sandholm, 2018, 2019), Go and Chess (Silver et al., 2016, 2018), Starcraft (Vinyals et al., 2019), Diplomacy (Paquette et al., 2019), and Stratego (Perolat et al., 2022).

Self-play has the desirable property that unbounded quantities of training data can be generated (assuming access to a simulator). But using self-play necessarily involves a choice of agents for the learner to train with: copies of itself. Strategies that perform well during training may perform poorly against new agents, whose behavior may differ dramatically from that of the agents that the learner trained against.

The problem of learning strategies during training that perform well against new agents is a central challenge in algorithmic game theory and multi-agent reinforcement learning (MARL) (Matignon et al., 2012; Lanctot et al., 2017). In particular, a self-play trained agent interacting with agents from an independent self-play instance--differing only by random seed--can lead to dramatically worse performance (Lanctot et al., 2017). The self-play-based DORA (Bakhtin et al., 2021) performs well aginst copies of itself in the game of no-press Diplomacy, but poorly against human-like agents(Bakhtin et al., 2022). Self-play has also been known to perform poorly against new agents in games with highly specialized conventions (Hu et al., 2020), such as Hanabi (Bard et al., 2020). In training, one instance of a self-play algorithm may learn conventions that are incompatible with the conventions of another instance.

There are special classes of games where the strategies learned through self-play generalize well to new agents. In two-player, constant-sum games there exist strong theoretical results guaranteeing the performance of a strategy learned through self-play: Nash equilibrium strategies are maxmin strategies, which will perform equally well against any optimal opponent and can guarantee the value of the game against any opponent.

We lose these guarantees outside of two-player constant-sum games. For example, consider the simple two-player coordination game of Figure 1. If both players choose the same action, both receive a utility of 1, otherwise they receive 0. Suppose the row player learned in self-play to choose \(a\) (which performs well against another \(a\)-player). Similarly, column learned to play \(b\). If these two players played against each other, both agents would regret their actions. Upon the introduction of a new agent who did not train with a learner, despite \(a\) and \(b\) being optimal strategies during training, they fail to generalize to new agents. As this example demonstrates, equilibrium strategies in general are _vulnerable_: agents are not guaranteed the equilibrium's value against new agents-even if the new agent's also play an equilibrium strategy.

In multiplayer, general-sum games, no-regret self-play is no longer guaranteed to produce Nash equilibra--instead, algorithms converge to a _mediated equilibrium_, where a mediator recommends actions to each player (Von Stengel and Forges, 2008; Farina et al., 2019, 2020; Morrill et al., 2021b). The mediator can represent an external entity that makes explicit recommendations, such as traffic lights mediating traffic flows. More commonly in machine learning, correlation can arise through the shared history of actions of learning agents interacting with each other (Hart and Mas-Colell, 2000). If a strategy learned in self-play were played against new agents, these new agents may not have access to the actions taken by other agents during training, so agents would no longer be able to correlate their actions. In fact, even if all agents play a decorrelated strategy from _the same_ mediated equilibrium, the result may not be an equilibrium.1

Despite the problems of vulnerability and loss of correlation, self-play has shown promising results outside of two-player constant-sum games. For example, algorithms based on self-play have outperformed professional poker players in multiplayer Texas hold 'em, despite the lack of theoretical guarantees (Brown and Sandholm, 2019).

We seek to understand what structure in multiplayer games will allow self-play to compute a good strategy. We show that any multiplayer game can be projected into the set of two-player constant-sum games between each pair of players, called _constant-sum polymatrix games_. The closer a game is to this space, the less the problem of correlation affects the removal of a mediator. We identify a second important property, called _subgame stability_, where global \(\)-Nash equilibria are boundedly far from Nash equilibria in each two-player subgame. We show that if a multiplayer, general-sum game is close to a subgame stable constant-sum polymatrix game, this is sufficient for strategies learned via self-play to generalize to new agents, as they do in two-player constant-sum games.

Throughout this work, we take an algorithm-agnostic approach by assuming only that self-play is performed by a _regret minimizing algorithm_. This is accomplished by analyzing directly the equilibria that no-regret algorithms converge to--namely coarse correlated equilibria. As a result, our analysis applies to a broad class of game-theoretically-inspired learning algorithms but also to MARL algorithms that converge to coarse correlated equilibria (Marris et al., 2021; Liu et al., 2021; Jin et al., 2021), since any policy can be transformed into a mixed strategy with Kuhn's Theorem (Kuhn, 1953). For the remainder of this work, when we say "self-play" we are referring to self-play using a no-regret algorithm.

Decomposition-based approaches have been used in prior work to show convergence of fictitious play to Nash equilibria in two-player games (Chen et al., 2022) and evolutionary dynamics (Tuyls et al., 2018). Cheung and Tao (2020) decompose games into zero-sum and cooperative parts to analyse

Figure 1: A simple coordination game

the chaotic nature of Multiplicative Weights and Follow-the-Regularized-Leader. We focus less on the convergence of algorithms per se, and focus instead on the generalization of learned strategies to new agents. We are, to the best of our knowledge, the first to do so.

After defining our structural properties and proving our main results, we conclude with experiments on Leduc poker to elucidate why self-play performs well in multiplayer poker. Our results suggest that regret-minimization techniques converge to a subset of the game's strategy space that is well-approximated by a subgame stable constant-sum polymatrix game.

## 2 Background

Normal Form GamesA normal form game is a 3 tuple \(G=(N,,u)\) where \(N\) is a set of players, \(=_{i N}_{i}\) is a joint pure strategy space and \(_{i}\) is a set of _pure strategies_ for player \(i\). Let \(n=|N|\). Pure strategies are deterministic choices of actions in the game. We call \(\) a _pure strategy profile_. \(u=(u_{i})_{i N}\) is a set of _utility functions_ where \(u_{i}:\). A player \(i\) can randomize by playing a _mixed strategy_, a probability distribution \(s_{i}\) over \(i\)'s pure strategies. Let \(S_{i}=(_{i})\) be the set of player \(i\)'s mixed strategies (where \((X)\) denotes the set of probability distributions over a domain \(X\)), and let \(S=_{i N}S_{i}\) be the set of mixed strategy profiles. We overload the definition of utility function to accept mixed strategies as follows: \(u_{i}(s)=_{}(_{i N}s_{i}(_{i}))u_{ i}()\). We use \(_{-i}\) and \(s_{-i}\) to denote a joint assignment of pure (resp. mixed) strategies to all players except for \(i\), thus \(s=(s_{i},s_{-i})\). We use \(_{-i}\) and \(S_{-i}\) to denote the sets of all such assignments.

Hindsight RationalityThe hindsight rationality framework (Morrill et al., 2021b) conceptualizes the goal of an agent as finding a strategy that minimizes regret with respect to a set of deviations \(\). A deviation \(\) is a mapping \(:S_{i} S_{i}\) that transforms a learner's strategy into some other strategy. Regret measures the amount the learner would prefer to deviate to \((s_{i})\): \(u_{i}((s_{i}),s_{-i})-u_{i}(s_{i},s_{-i})\). An agent is hindsight rational with respect to a set of deviations \(\) if the agent does not have positive regret with respect to any deviation in \(\), i.e. \(,u_{i}((s_{i}),s_{-i})-u_{i}(s_{i},s_{-i}) 0\). Let \(()\) be a distribution over pure strategy profiles and \((_{i})_{i N}\) be a choice of deviation sets for each player.

**Definition 2.1** (\(\)-Mediated Equilibrium (Morrill et al., 2021b)).: We say \(m=(,(_{i})_{i N})\) is an \(\)_-mediated equilibrium_ if \( i N,_{i}\) we have \(_{}[u_{i}((_{i}),_{-i})-u_{i}() ]\). A _mediated equilibrium_ is a 0-mediated equilibrium.

Learning takes place in an online learning environment. At each iteration \(t\), a learning agent \(i\) chooses a strategy \(s^{t}_{i}\) while all other agents choose a strategy profile \(s^{t}_{-i}\). No-\(\)-regret learning algorithms ensure that the maximum average positive regret tends to 0:

\[_{T}(_{}_{t=1}^{T}u_{i}(( s^{t}_{i}),s^{t}_{-i})-u_{i}(s^{t}_{i},s^{t}_{-i})) 0.\]

If all agents use a no-regret learning algorithms w.r.t. a set of deviations \(_{i}\), the _empirical distribution of play_ converges to a mediated equilibrium. Formally, let \(()\) be the empirical distribution of play, where the weight on \(\) is \(()_{t=1}^{T}(_{i N}s^{t}_{i}(_{i}))\). As \(T\), \(\) converges to \(\) of a mediated equilibrium \((,(_{i})_{i N})\).

Different sets of deviations determine the strength of a mediated equilibrium. For normal-form games, the set of _swap_ deviations, \(_{SW}\), are all possible mappings \(:_{i}_{i}\). We may apply a swap deviation \(\) to a mixed strategy \(s_{i}\) by taking its pushforward measure: \([(s_{i})](_{i})=_{^{}_{i}^{-1}(_{i})}s_{i}( ^{}_{i})\), where \(^{-1}(_{i})=\{^{}_{i}_{i}(^{} _{i})=_{i}\}\). The set of _internal_ deviations \(_{I}\), which replace a single pure strategy with another, offer the same strategic power as swap deviations (Roster & Vohra, 1999). Formally, \(_{I}=\{_{SW}_{i},^{}_{i}:[_{i}( _{i})=^{}_{i}][^{}_{i}_{i}, (^{}_{i})=^{}_{i}]\}\). The set of _external_ deviations, \(_{EX}\), is a very nonrestrict: any \(_{EX}\) maps all (mixed) strategies to some particular pure strategy; i.e. \(_{EX}=\{_{SW}_{i}:^{}_{i},( ^{}_{i})=_{i}\}\). The choice of \((_{i})_{i N}\) determines the nature of the mediated equilibrium--provided the learning algorithm for player \(i\) is no-\(_{i}\)-regret (Greenwald et al., 2011). For example, if all players are hindsight rational w.r.t. \(_{EX}\), then \(\) converges to the set of coarse correlated equilibria (CCE) (Moulin & Vial, 1978) and if all players are hindsight rational w.r.t. \(_{I}\) then \(\) converges to the set of correlated equilibria (Aumann, 1974).

A special case of mediated equilibria are _Nash equilibria_. If some mediated equilibrium \(m=(,(_{i})_{i N})\) is a product distribution (i.e. \(()=_{i N}s_{i}()\ \) ) and \(_{i}_{EX}\  i N\) then \(\) is a Nash equilibrium. Similarly, an \(\)-mediated equilibrium is an \(\)-Nash equilibrium if \(\) is a product distribution and \(_{i}_{EX}\  i N\).

In sequential decision making scenarios (often modelled as extensive form games), the set of deviations is even more rich (Morrill et al., 2021). All of these deviation classes--with the exception of action deviations (Selten, 1988) (which are so weak they do not even imply Nash equilibria in two-player constant-sum games, see appendix)--are stronger than external deviations. This means that the equilibria of any algorithm that minimizes regret w.r.t. a stronger class of deviations than external deviations still inherit all the properties of CCE (for example Hart and Mas-Colell (2000); Zinkevich et al. (2008); Celli et al. (2020); Steinberger et al. (2020); Morrill et al. (2021)). Thus, we focus on CCE since the analysis generalizes broadly. Moreover, CCE can be computed efficiently, either analytically (Jiang and Leyton-Brown, 2011) or by a learning algorithm. When we refer to CCE, we use the distribution \(\) to refer to the CCE, since \(\) is implicit.

## 3 Vulnerability

The choice of other agents during learning affects the strategy that is learned. Choosing which agents make good "opponents" during training is an open research question (Lanctot et al., 2017; Marris et al., 2021). One common approach, _self-play_, is to have a learning algorithm train with copies of itself as the other agents. If the algorithm is a no-\(\)-regret algorithm, the learned behavior will converge to a mediated equilibrium; this gives a nice characterization of the convergence behavior of the algorithm.

However, in general the strategies in a mediated equilibrium are correlated with each other. This means that in order to deploy a strategy learned in self-play, an agent must first extract it by marginalizing out other agent's strategies. This new _marginal strategy_ can then be played against new agents with whom the agent did not train (and thus correlate).

**Definition 3.1** (Marginal strategy).: Given some mediated equilibrium \((,(_{i})_{i=1}^{N})\), let \(s_{i}^{}\) be the _marginal strategy_ for \(i\), where \(s_{i}^{}(_{i})_{_{-i}_{-i}}(_{i}, _{-i})\). Let \(s^{}\) be a _marginal strategy profile_, where each \( i N\) plays \(s_{i}^{}\).

Once a strategy has been extracted via marginalization, learning can either continue with the new agents (and potentially re-correlate), or the strategy can remain fixed.2 We focus on the case where the strategy remains fixed. In doing so we can guarantee the performance of this strategy if learning stops, but also the show guarantees about the initial performance of a strategy that continues to learn; this is especially important in safety-critical domains.

Given a marginal strategy \(s_{i}^{}\), we can bound its underperformance against new agents that behave differently from the (decorrelated) training opponents by a quantity which we call _vulnerability_.

**Definition 3.2** (Vulnerability).: The vulnerability of a strategy profile \(s\) for player \(i\) with respect to \(S_{-i}^{} S_{-i}\) is

\[_{i}(s,S_{-i}^{}) u_{i}(s)-_{s_{-i}^{ } S_{-i}^{}}u_{i}(s_{i},s_{-i}^{}).\]

Vulnerability gives a measure of how much worse \(s\) will perform with new agents compared to its training performance under pessimistic assumptions--that \(-i\) play the strategy profile in \(S_{-i}^{}\) that is worst for \(i\). We assume that \(-i\) are not able to correlate their strategies.

Thus, if a marginal strategy profile \(s^{}\) is learned through self-play and \(_{i}(s^{},S_{-i}^{})\) is small, then \(s_{i}^{}\) performs roughly as well against new agents \(-i\) playing some strategy profile in \(S_{-i}^{}\). \(S_{-i}^{}\) is used to encode assumptions about the strategies of opponents. \(S_{-i}^{}=S_{-i}\) means opponents could play _any_ strategy, but we could also set \(S_{-i}^{}\) to be the set of strategies learnable through self-play if we believe that opponents would also be using self-play as a training procedure.

Some games have properties that make the vulnerability low. For example, in two-player constant-sum games the marginal strategies learned in self-play generalize well to new opponents since any Nash equilibrium strategy is also a maxmin strategy (von Neumann, 1928).

## 4 Guarantees via Polymatrix Decomposability

Multiplayer games are fundamentally more complex than two-player constant-sum games (Daskalakis Papadimitriou, 2005; Daskalakis et al., 2009). However, certain multiplayer games can be decomposed into a graph of two-player games, where a player's payoffs depend only on their actions and the actions of players who are neighbors in the graph (Bergman and Fokin, 1998). In these _polymatrix_ games (a subset of graphical games (Kearns et al., 2013)) Nash equilibria can be computed efficiently if player's utilities sum to a constant (Cai and Daskalakis, 2011; Cai et al., 2016).

**Definition 4.1** (Polymatrix game).: A _polymatrix game_\(G=(N,E,,u)\) consists of a set \(N\) of players, a set of edges \(E\) between players, a set of pure strategy profiles \(\), and a set of utility functions \(u=\{u_{ij},u_{ji}(i,j) E\}\) where \(u_{ij},u_{ji}:_{i}_{j}\) are utility functions associated with the edge \((i,j)\) for players \(i\) and \(j\), respectively.

We refer to the normal-form _subgame_ between \((i,j)\) as \(G_{ij}=(\{i,j\},_{i}_{j},(u_{ij},u_{ji}))\). We use \(u_{i}\) to denote the _global utility function_\(u_{i}:\) where \(u_{i}()=_{(i,j) E}u_{ij}(_{i},_{j})\) for each player. We use \(E_{i} E\) to denote the set of edges where \(i\) is a player and \(|E_{i}|\) to denote the number of such edges.

**Definition 4.2** (Constant-sum polymatrix).: We say a polymatrix game \(G\) is _constant-sum_ if for some constant \(c\) we have that \(\), \(_{i N}u_{i}()=c\).

Constant-sum polymatrix (CSP) games have the desirable property that all CCE factor into a product distribution; i.e., are Nash equilibria (Cai et al., 2016). We give a relaxed version:

**Proposition 4.3**.: _If \(\) is an \(\)-CCE of a CSP game \(G\), \(s^{}\) is an \(n\)-Nash of \(G\)._

This means no-external-regret learning algorithms will converge to Nash equilibria, and thus do not require a mediator to enable the equilibrium. However, they do not necessarily have the property of two-player constant-sum games that all (marginal) equilibrium strategies are maxmin strategies (Cai et al., 2016). Thus Nash equilibrium strategies in CSP games have no vulnerability guarantees. Cai et al. (2016) show that CSP games that are constant sum in each subgame are no more or less general than CSP games that are constant sum globally, since there exists a payoff preserving transformation between the two sets. For this reason we focus on CSP games that are constant sum in each subgame without loss of generality. Note that the constant need not be the same in each subgame.

### Vulnerability on a Simple Constant-Sum Polymatrix Game

We next demonstrate why CSP games do not have bounded vulnerability on their own without additional properties. Consider the simple 3-player CSP game called Offense-Defense (Figure 1(a)). There are 3 players: \(0\), \(1\) and \(2\). Players \(1\) and \(2\) have the option to either attack \(0\) (\(a_{0}\)) or attack the other (e.g. \(a_{1}\)); player \(0\), on the other hand, may either relax (\(r\)) or defend (\(d\)). If either \(1\) or \(2\) attacks the other while the other is attacking \(0\), the attacker gets \(\) and the other gets \(-\) in that subgame. If both \(1\) and \(2\) attack \(0\), \(1\) and \(2\) get 0 in their subgame and if they attack each other, their attacks cancel out and they both get \(0\). If \(0\) plays \(d\), they defend and will always get \(0\). If they relax, they get \(-\) if they are attacked and \(0\) otherwise. Offense-Defense is a CSP game, so any CCE is a Nash equilibrium.

Note that \(=(r,a_{2},a_{1})\) is a Nash equilibrium. Each \(i\{1,2\}\) are attacking the other \(j\{1,2\}\{i\}\), so has expected utility of \(0\). Deviating to attacking \(0\) would leave them open against the other, so \(a_{0}\) is not a profitable deviation, as it would also give utility \(0\). Additionally, \(0\) has no incentive to deviate to \(d\), since this would also give them a utility of \(0\).

However, \(\) is not a Nash equilibrium of the subgames--all \(i\{1,2\}\) have a profitable deviation in their subgame against \(0\), which leaves \(0\) vulnerable in that subgame. If \(1\) and \(2\) were to both deviate to \(a_{0}\), and \(0\) continues to play their Nash equilibrium strategy of \(r\), \(0\) would lose \(2\) utility from their equilibrium value; in other words, the vulnerability of player \(0\) is \(2\).

### Subgame Stability

However, some constant-sum polymatrix games _do_ have have bounded vulnerability; we call these _subgame stable games_. In subgame stable games, global equilibria imply equilibria at each pairwise subgame.

**Definition 4.4** (Subgame stable profile).: Let \(G\) be a polymatrix game with global utility functions \((u_{i})_{i N}\). We say a strategy profile \(s\) is \(\)_-subgame stable_ if \((i,j) E\), we have \((s_{i},s_{j})\) is a \(\)-Nash of \(G_{ij}\); that is \(u_{ij}(_{i},s_{j})-u_{ij}(s_{i},s_{j})_{i} _{i}\) and \(u_{ji}(_{j},s_{i})-u_{ji}(s_{j},s_{i})_{j} _{j}\)

For example, in Offense-Defense, \((r,a_{2},a_{1})\) is \(\)-subgame stable; it is a Nash equilibrium but is a \(\)-Nash of the subgame between \(0\) and \(1\) and the subgame between \(0\) and \(2\).

**Definition 4.5** (Subgame stable game).: Let \(G\) be a polymatrix game. We say \(G\) is _\((,)\)-subgame stable_ if for _any_\(\)-Nash equilibrium \(s\) of \(G\), \(s\) is \(\)-subgame stable.

Subgame stability connects the global behavior of play (\(\)-Nash equilibrium in \(G\)) to local behavior in a subgame (\(\)-Nash in \(G_{ij}\)). If a polymatrix game is both constant-sum and is \((0,)\)-subgame stable then we can bound the vulnerability of any marginal strategy.

**Theorem 4.6**.: _Let \(G\) be a CSP game. If \(G\) is \((0,)\)-subgame stable, then for any player \(i N\) and CCE \(\) of \(G\), we have \(}_{i}(s^{},S_{-i})|E_{i}|\)._

Theorem 4.6 tells us that using self-play to compute a marginal strategy \(s^{}\) on constant-sum polymatrix games will have low vulnerability against worst-case opponents if \(\) is low. Thus, these are a set of multiplayer games where self-play is an effective training procedure.

Proof idea.Since \(G\) is a CSP game, \(s^{}\) is a Nash equilbrium. Since \(G\) is \((0,)\)-subgame stable, \((s^{}_{i},s^{}_{j})\) is a \(\)-Nash equilibrium in each subgame, which bounds the vulnerability in each subgame. This is because

\[_{s_{-i} S_{-i}}u_{i}(s^{}_{i},s_{-i})=_{(i,j) E_{i}}_{s _{j} S_{j}}u_{ij}(s^{}_{i},s_{j})\]

since players \(j i\) can minimize \(i\)'s utility without coordinating, as \(G\) is a polymatrix game.

We give an algorithm for finding the minimum value of \(\) such that a CSP game is \((0,)\)-subgame stable in Appendix D.1.

Figure 2: (a) Offense-Defense, a simple CSP game. We only show payoffs for the row player, column player payoffs are zero minus the row playerâ€™s payoffs. (b) Bad Card: a game that is not overall CSP, but the subset of strategies learnable by self-play are. At the terminals, we show the dealers utility first, followed by players \(0,1\) and \(2\), respectively.

### Approximate Constant-Sum Polymatrix Games

Most games are not factorizable into CSP games. However, we can take any game \(G\) and project it into the space of CSP games.

**Definition 4.7** (\(\)-constant sum polymatrix).: A game \(G\) is \(\)_-constant sum polymatrix_ (\(\)-CSP) if there exists a CSP game \(\) with global utility function \(\) such that \( i N,\), \(|u_{i}()-_{i}()|\). Given \(G\), we denote the set of such CSP games as \(_{}(G)\).

**Proposition 4.8**.: _In a \(\)-CSP game \(G\) the following hold._

1. _Any CCE of_ \(G\) _is a_ \(2\)_-CCE of any_ \(_{}(G)\)_._
2. _The marginal strategy profile of any CCE of_ \(G\) _is a_ \(2n\)_-Nash equilibrium of any_ \(_{}(G)\)_._
3. _The marginal strategy profile of any CCE of_ \(G\) _is a_ \(2(n+1)\)_-Nash equilibrium of_ \(G\)_._

From (3) we have that the removal of the mediator impacts players utilities by a bounded amount in \(\)-CSP games. We give a linear program in Appendix D.2 that will find the minimum \(\) such that \(G\) is \(\)-CSP and returns a CSP game \(_{}(G)\).

Combining \(\)-CSP with \((,)\)-subgame stability lets us bound vulnerability in _any_ game.

**Theorem 4.9**.: _If \(G\) is \(\)-CSP and \(_{}(G)\) that is \((2n,)\)-subgame stable and \(\) is a CCE of \(G\), then_

\[_{i}(s^{},S_{-i})|E_{i}|+2(n- 1)+2.\]

Theorem 4.9 shows that games which are close to the space of subgame stable CSP (SS-CSP) games are cases where the marginal strategies learned through self-play have bounded worst-case performance. This makes them suitable for any no-external-regret learning algorithm.

## 5 Vulnerability Against Other Self-Taught Agents

Theorem 4.9 bounds vulnerability in worst-case scenarios, where \(-i\) play any strategy profile to minimize \(i\)'s utility. In reality, however, each player \(j-i\) has their own interests and would only play a strategy that is reasonable under these own interests. In particular, what if each agent were also determining their own strategy via self-play in a separate training instance. How much utility can \(i\) guarantee themselves in this setup?

While no-external-regret learning algorithms converge to the set of CCE, other assumptions can be made with additional information about the type of regret being minimized. For example, no-external-regret learning algorithms will play strictly dominated strategies with vanishing probability and CFR will play dominated actions with vanishing probability (Gibson, 2014). These refinements can tighten our bounds, since the part of the game that no-regret learning algorithms converge to might be closer to a CSP game than the game overall.

Consider the game shown in Figure 1(b), called "Bad Card". The game starts with each player except the dealer putting \(/2\) into the pot. A dealer player \(d\)--who always receives utility 0 regardless of the strategies of the other players--then selects a player from \(\{0,1,2\}\) to receive a "bad card", while the other two players receive a "good card". The player who receives the bad card has an option to fold, after which the game ends and all players receive their ante back. Otherwise if this player calls, the other two players can either fold or call. The pot of \(\) is divided among the players with good cards who call. If one player with a good card calls, they win the pot of \(\). If both good card players call then they split the pot. If both players with good cards fold, then the player with the bad card wins the pot.

As we shall soon show, Bad Card does not have a CSP decomposition--in fact it does not have _any_ polymatrix decomposition. Since Bad Card is an extensive-form game without chance, each pure strategy profile leads to a single terminal history. Let \((z)\) be the set of pure strategy profiles that play to a terminal \(z\). In order for Bad Card to be polymatrix, we would need to find subgame utility functions such that \(\), \(u_{0}()=u_{0,d}(_{0},_{d})+u_{0,1}(_{0},_{1})+u_{0,2}(_{ 0},_{2})\). Equivalently, wecould write \( z Z,(z),u_{0}(z)=u_{0,d}(_{0},_{d})+u_{0,1}( _{0},_{1})+u_{0,2}(_{0},_{2})\) where \(Z\) is the set of terminals. A subset of these constraints results in an infeasible system of equations.

Consider the terminals in the subtree shown in Figure 1(b): \(z^{1}=(0,c,c,c)\), \(z^{2}=(0,c,c,f)\), \(z^{3}=(0,c,f,c)\) and \(z^{4}=(0,c,f,f)\). Let \(_{i}^{c}\) be any pure strategy that plays \(c\) in this subtree and \(_{i}^{f}\) be any strategy that plays \(f\) in this subtree for player \(i\). In order for Bad Card to decompose into a polymatrix game we would need to solve the following infeasible system of linear equations:

\[u_{0}(z^{1}) =u_{0,d}(_{0}^{c},0)+u_{0,1}(_{0}^{c},_{1}^{c})+u_{0,2 }(_{0}^{c},_{2}^{c})=-\] \[u_{0}(z^{2}) =u_{0,d}(_{0}^{c},0)+u_{0,1}(_{0}^{c},_{1}^{c})+u_{0,2 }(_{0}^{c},_{1}^{f})=-\] \[u_{0}(z^{3}) =u_{0,d}(_{0}^{c},0)+u_{0,1}(_{0}^{c},_{1}^{f})+u_{0,2 }(_{0}^{c},_{2}^{c})=-\] \[u_{0}(z^{4}) =u_{0,d}(_{0}^{c},0)+u_{0,1}(_{0}^{c},_{1}^{f})+u_{0,2 }(_{0}^{c},_{2}^{f})=\]

Thus, Bad Card is not a CSP game, although it is a \(\)-CSP game. However, if we prune out dominated actions (namely, those in which a player folds after receiving a good card), the resulting game is indeed a \(0\)-CSP game.

Let \(()\) be the set of mediated equilibria than an algorithm \(\) converges to in self-play. For example, if \(\) is a no-external-regret algorithm, \(()\) is the set of CCE without strictly dominated strategies in their support. Let \(S()\{s^{}\;(,(_{i})_{i N})( )\}\) be the set of marginal strategy profiles of \(()\), and let \(S_{i}()\{s_{i} s S()\}\) be the set of \(i\)'s marginal strategies from \(S()\).

Now, consider if each player \(i\) learns with their own self-play algorithm \(_{i}\). Let \(_{N}(_{1},..._{n})\) be the profile of learning algorithms, then let \(S^{}(_{N})_{i N}S_{i}(_{i})\) and \(S^{}_{-i}(_{N})_{j-i}S_{j}(_{j})\). Summarizing, if each player learns with a no-\(_{i}\)-regret learning algorithm \(_{i}\), they will converge to the set of \((_{i})\) equilibria. The set of marginal strategies from this set of equilibria is \(S_{i}(_{i})\) and the set of marginal strategy profiles is \(S(_{i})\). If each player plays a (potentially) different learning algorithm, \(S^{}(_{N})\) is the set of possible joint match-ups if each player plays a marginal strategy from their own algorithm's set of equilibria and \(S^{}_{-i}(_{N})\) is the set of profiles for \(-i\).

**Definition 5.1**.: We say a game \(G\) is \(\)_-CSP in the neighborhood of \(S^{} S\)_ if there exists a CSP game \(\) such that \( s S^{}\) we have \(|u_{i}(s)-_{i}(s)|\). We denote the set of such CSP games as \(_{}(G,S^{})\).

**Definition 5.2**.: We say a polymatrix game \(G\) is \(\)_-subgame stable in the neighborhood of \(S^{}\)_ if \( s S^{},(i,j) E\) we have that \((s_{i},s_{j})\) is a \(\)-Nash of \(G_{ij}\).

These definitions allow us to prove the following generalization of Theorem 4.9.

**Theorem 5.3**.: _For any \(i N\), if \(G\) is \(\)-CSP in the neighborhood of \(S^{}(_{N})\) and \(_{}(G,S^{}(_{N}))\) that is \(\)-subgame stable in the the neighborhood of \(S(_{i})\), then for any \(s S(_{i})\)_

\[_{i}(s,S^{}_{-i}(_{N}))|E_{i}| +2(n-1)+2.\]

An implication of Theorem 5.3 is that if agents use self-play to compute a marginal strategy from some mediated equilibrium and there is an SS-CSP game that is close to the original game for these strategies, then this is sufficient to bound vulnerability against strategies learned in self-play.

## 6 Computing an SS-CSP Decomposition in a Neighborhood

How might one determine if a game is well-approximated by an SS-CSP game? In addition to the algorithms presented in Appendix D, we give an algorithm, _SGDecompose_, that finds an SS-CSP decomposition for a game in a given neighborhood of strategy profiles. Pseudocode is given in Algorithm 1. SGDecompose could be used to test whether a game is well-approximated by an SS-CSP game before potentially analytically showing this property holds. We will use this algorithm in the following section to decompose Leduc poker.

As input, SGDecompose receives a neighborhood of strategies \(S^{}\) and the set of match-ups between strategies in \(S^{}\), given by \(S^{}_{i N}S^{}_{i}\). The idea is to compute a CSP game \(\) that minimizesa loss function with two components: how close \(\) is to \(G\) in the neighborhood of \(S^{}\) and how subgame stable \(\) is for the neighborhood of \(S^{}\). First, \(^{}\) is the error between the utility functions of \(G\) and \(\) (\(u\) and \(\), respectively); it is a proxy for \(\) in \(\)-CSP. The loss for a single strategy profile \(s\) is

\[^{}(s;,u)_{i N}| {u}_{i}(s)-u_{i}(s)|.\]

The other component of the overall loss function, \(^{}\), measures the subgame stability. First, we define \(^{}_{ij}\), which only applies to a single subgame. Let \(s_{ij}=(s_{i},s_{j})\) be a profile for a subgame and \(s^{*}_{ij}=(s^{*}_{i},s^{*}_{j})\) is a profile of deviations for that subgame. The \(^{}_{ij}\) loss for this subgame is

\[^{}_{ij}(s_{ij},s^{*}_{ij};)( {u}_{ij}(s^{*}_{i},s_{j})-_{ij}(s_{ij}),0)+( _{ji}(s_{i},s^{*}_{j})-_{ji}(s_{ij}),0).\]

Then, given a strategy profile \(s\) and deviation profile \(s^{*}\) for _all_ players \(N\), we have

\[^{}(s,s^{*};)_{(i,j) E} ^{}_{ij}(s_{ij},s^{*}_{ij};).\]

SGDecompose repeats over a number of epoches \(T\). At the start of an epoch, we compute a best-response (for example, via sequence-form linear programming in extensive-form games) to each strategy \(s^{}_{i}\) in \(S^{}\) in each subgame; the full process is shown in Algorithm 4 in the appendix. After computing these best-responses for the current utility function of \(\), SGDecompose fits \(\) to be nearly CSP in the neighborhood of \(S^{}\) and subgame stable in the neighborhood of \(S^{}\). Since \(S^{}\) is exponentially larger than \(S^{}\), we partition it into batches, then use batch gradient descent.3

We use the following batch loss function, which computes the average values of \(^{}\) and \(^{}\) over the batch then weights the losses with \(\). Let \(S^{b}\) denote a batch of strategy profiles from \(S^{}\) with size \(B\),

\[(S^{b},S^{},S^{*};,u)_{ s S^{b}}^{}(;,u)+|}_{s S^{}}_{s^{*} S^{*}}^{}(s,s^ {*};).\]

We use this loss function to update \(\), which is guaranteed to a be a valid utility function for a CSP game via its representation, see Appendix F for details. In Appendix F, we give the procedure in terms of a more efficient representation of polymatrix decompositions for extensive-form games, which we call _poly-EFGs_; which we describe in Appendix E.

``` Input:\(G\), \(S^{}\), hyperparameters \(\), \(T\), \(\), \(B\)  Initialize \(\) to all \(0\) \(S^{}_{i N}_{i}\) for\(t 1...T\)do \(S^{*}\) getBRs(\(\), \(S^{}\)) \(\) partition of \(S^{}\) into batches of size \(B\) for\(S^{b}\)do \(g_{}(S^{b},S^{},S^{*};,u)\) \(-_{2}\) {update \(\) using normalized gradient; this helps with stability } endfor endfor {Lastly, output \(\) and \(\)} \(_{s S^{}}|u_{i}(s)-_{i}(s)|\) \(_{s S^{}}_{i j N N}( {u}_{ij}(BR_{ij}(s_{j}),s_{j})-_{ij}(s_{i},s_{j}))\) return\(,,\) ```

**Algorithm 1** SGDecompose

## 7 Experiments

Approaches using regret-minimization in self-play have been shown to outperform expert human players in some multiplayer games, the most notable example being multiplayer no-limit Texas hold 'em (Brown and Sandholm, 2019), despite no formal guarantees.

**Conjecture 7.1**.: Self-play with regret minimization performs well in multiplayer Texas hold 'em because "good" players (whether professional players or strategies learned by self-play) play in a part of the games' strategy space that is close to an SS-CSP game (i.e. low values of \(,\)).

While multiplayer no-limit Texas hold 'em is too large to directly check the properties developed in this work, we use a smaller poker game, called Leduc poker (Southey et al., 2012), to suggest why regret-minimization "works" in multiplayer Texas hold 'em. Leduc poker was originally developed for two players but was extended to a 3-player variant by Abou Risk & Szafron (2010); we use the 3-player variant here. The game has 8 cards, two rounds of betting, one private and one public card.

We first use a self-play algorithm to learn strategies, then use SGDecompose to see if this part of Leduc Poker is close to an SS-CSP game. We give a summary of results here, please refer to Appendix G for full details. We use CFR+ (Tammelin, 2014; Tammelin et al., 2015) as a self-play algorithm to compute a set of approximate marginal strategies.4 CFR+ was chosen because of its improved efficiency over CFR. CFR+ is a deterministic algorithm, so we use different random initializations of CFR+'s initial strategy in order to generate a set of CCE. We will use 30 runs of CFR+ as input to SGDecompose; and have 30 runs of SGDecompose (i.e. we trained CFR+ 900 times in total in self-play). This will give us a value of \(\) and \(\) for each run.

We found that Leduc poker was well-approximated by an SS-CSP game in the neighborhood of strategies learned by CFR+. In particular, across runs, Leduc poker was on average (with standard errors) \(=0.009 0.00046\)-CSP and \(=0.004 0.00016\)-subgame stable in the neighborhood of CFR+learned strategies. How well do these values bound vulnerability with respect to other \(\)-learned strategies? For each of the runs, we computed the vulnerability with respect to the strategies of that run, by evaluating each strategy against each other and taking the maximum vulnerability. We compare these values to the upper bounds implies by the values of \(\) and \(\) for each run and Theorem 5.3. We found the computed values of \(\) and \(\) do a good job of upper bounding the vulnerability. Across the runs, the bounds are at minimum \(1.89\) times the vulnerability, at maximum \(3.05\) times the vulnerability and on average \(2.51\) times as large, with a standard error of \(0.049\).

We repeated these experiments with a toy hanabi game--where strategies learned in self-play are highly vulnerable--which we found to have much higher values of \(\) and \(\); details are in Appendix H.

It was previously believed that CFR does not compute an \(\)-Nash equilibrium on 3-player Leduc for any reasonable value of \(\). Abou Risk & Szafron (2010) found CFR computed a \(0.130\)-Nash equilibrium. We found that CFR+ always computed an approximate Nash equilbrium with \( 0.013\). Appendix G.2 shows that CFR also computes an approximate Nash equilbrium with \( 0.017\).

## 8 Conclusion

Self-play has been incredibly successful in producing strategies that perform well against new opponents in two-player constant-sum games. Despite a lack of theoretical guarantees, self-play seems to also produce good strategies in some multiplayer games (Brown & Sandholm, 2019). We identify a structural property of multiplayer, general-sum game that allow us to establish guarantees on the performance of strategies learned via self-play against new opponents. We show that any game can be projected into the space of constant-sum polymatrix games, and if there exists a game with this set with high subgame stability (low \(\)), strategies learned through self-play have bounded loss of performance against new opponents.

We conjecture that Texas hold 'em is one such game. We investigate this claim on Leduc poker, and find that CFR+ plays strategies from a part of the strategy space in Leduc poker that is well-approximated by a subgame stable constant-sum polymatrix game. This work lays the groundwork for guarantees for self-play in multiplayer games. However, there is room for algorithmic improvement and efficiency gains for checking these properties in very large extensive-form games.