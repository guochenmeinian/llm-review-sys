# BeanCounter: A low-toxicity, large-scale, and open dataset of business-oriented text

Siyan Wang

University of Chicago

Chicago, IL 60637

&Bradford Levy

University of Chicago

Chicago, IL 60637

Corresponding author: bradford.levy@chicagobooth.edu

Code: [https://github.com/bradfordlynch/beancounter](https://github.com/bradfordlynch/beancounter)

Data: [https://huggingface.co/datasets/bradfordleyy/BeanCounter](https://huggingface.co/datasets/bradfordleyy/BeanCounter)

###### Abstract

Many of the recent breakthroughs in language modeling have resulted from scaling effectively the same model architecture to larger datasets. In this vein, recent work has highlighted performance gains from increasing training dataset size and quality, suggesting a need for novel sources of large-scale datasets. In this work, we introduce BeanCounter, a public dataset consisting of more than 159B tokens extracted from businesses' disclosures. We show that this data is indeed novel: less than 0.1% of BeanCounter appears in Common Crawl-based datasets and the data is an order of magnitude larger than datasets relying on similar sources. Given the data's provenance, we hypothesize that BeanCounter is comparatively more factual and less toxic than web-based datasets. Exploring this hypothesis, we find that many demographic identities occur with similar prevalence in BeanCounter but with _significantly less toxic_ context relative to other datasets. To demonstrate the utility of BeanCounter, we evaluate and compare two LLMs continually pre-trained on BeanCounter with their base models. We find an 18-33% reduction in toxic generation and improved performance within the finance domain for the continually pretrained models. Collectively, our work suggests that BeanCounter is a novel source of low-toxicity and high-quality domain-specific data with sufficient scale to train multi-billion parameter LLMs.

## 1 Introduction

A key ingredient to the recent breakthroughs in language modeling has been the availability of large-scale datasets. Along these lines, recent work has shown that training incrementally larger language models demands a similar increase in training data [29; 27], and that data quality is a significant determinant of a model's ultimate performance [38; 23]. However, the creation and scaling of text datasets sourced from the public domain raises a variety of concerns such as inclusion of personally identifiable information , degrading quality , and biases or false information .

In this work, we contribute to overcoming the challenges of scaling text datasets. Specifically, we introduce BeanCounter, a dataset comprised of more than 159B tokens extracted from public domain business-oriented disclosures. The introduction of BeanCounter makes four primary contributions:

1. Novel large-scale dataset of domain specific content.BeanCounter consists of content produced by businesses to communicate with a variety of stakeholders such as investors and regulators. This content is not easily accessible via web scraping and we find that little of BeanCounter is included in other commonly used datasets. For example, less than 0.1% of BeanCounter is in C4 . Inconsidering the scale of BeanCounter, we define "large-scale" as sufficient size to pre-train a multi-billion parameter model , and of similar order of magnitude as other web-based datasets (i.e., 100B+ tokens) . Along these lines, BeanCounter consists of more than 159B tokens of cleaned text and more than 111B tokens of deduplicated text (see Table 1). Thus, to the best of our knowledge, BeanCounter is the largest and most comprehensive public dataset of business-oriented text.

2. Timely, factual, and high-quality content.Considering timeliness, Dhingra et al.  show that incorporating the concept of time into LLMs can enhance their ability to recall time-dependent facts, e.g., the current president of the United States. In contrast to web-based datasets which usually do not have a precise timestamp of when the web page was created or updated, every observation in BeanCounter has a timestamp of when the content became available-accurate to the second. Considering factuality and quality, there are at least two reasons BeanCounter is comparatively more factual and of higher-quality than web-based datasets. First, the principal executive and financial officers, e.g., the CEO and CFO, must certify the disclosures from which BeanCounter is sourced . Second, these disclosures are the primary channel used by businesses to communicate with stakeholders. Thus, the businesses are incentivized to produce effective communications. In this sense, BeanCounter supports future research on the role of time in LLMs, factuality, and quality.

3. Toxicity and demographic identity analysis.As access to LLMs has proliferated, a common concern is the extent to which LLMs produce toxic or otherwise harmful content. Along these lines, a large literature has examined the toxicity of various web-based datasets. In this vein, we examine the prevalence of a variety of demographic identity descriptors and the toxicity of text in which they are mentioned . We contrast our findings to text from C4  and find that many descriptors are similarly prevalent in BeanCounter but the context in which they are mentioned is _significantly less toxic_. For instance, the descriptor "Asian" is mentioned in 4.67% and 9.26% of documents in C4 and filings in BeanCounter, respectively, yet the text surrounding "Asian" in BeanCounter is 77.0% less toxic, on average. Similarly, "LGBTQ" is mentioned 0.37% and 0.17% in C4 and BeanCounter, respectively, yet the toxicity of its surrounding content is 85.7% lower in BeanCounter.1 We find this general trend across nearly all descriptors.

4. Model Evaluation.Given the influence of training data on model generation, our analyses suggest that models trained on BeanCounter may exhibit less toxic generation. We explore this possibility by continuing the pre-training of models on BeanCounter and evaluate their performance on domain-specific tasks and toxic generation. We find that models trained on BeanCounter exhibit better performance within financial applications while exhibiting 18-33% less toxic generation.

In summary, this paper introduces a novel business-oriented text dataset comprised of more than 159B tokens which are comparatively more factual, of higher-quality, and less toxic than commonly used web-based datasets. We have open-sourced BeanCounter and made it available via Hugging Face Hub. We hope that BeanCounter will enable the creation of new foundation models which are less likely to generate toxic content and are better suited to business-oriented tasks.

## 2 Related Work

Foundation models and large-scale text datasets.The current scale of Transformer-based language models has grown more than 1000-fold since early examples of such models, e.g., from GPT-1

   Dataset & \# tokens & size \\  BeanCounter.clean & 159.4B & 865.2 GB \\ BeanCounter.fraud & 0.3B & 1.7 GB \\ BeanCounter.final & 110.5B & 598.8 GB \\    
    \\ 
10-K & 1.0B & 8.78\% \\
10-Q & 1.1B & 8.80\% \\
8-K & 3.9B & 23.3\% \\   

Table 1: Statistics for the BeanCounter duplication rate  to OPT-175B . While the scale of these models has grown dramatically, the size of the data has grown comparatively more slowly, e.g., even though OPT-175B is 100 times larger than GPT-2  it was trained on only 10 times as much data (180B versus roughly 20B tokens). In this vein, recent work has explored how to best allocate a fixed _training_ compute budget and found results which suggest earlier models were significantly under-trained, i.e., a 10-fold increase in model size should correspond to a 10-fold increase in training data size . Alternatively, one can view this resource allocation problem as desiring the best possible _inference_ performance for a fixed budget . In which case, models should be trained on even more data than that suggested by the scaling laws of Hoffmann et al. . The common theme throughout this literature is that an increase in model size requires at least a similar increase in training data as well. This begs the question of where to source the additional data.

Early models such as BERT  and GPT-1  sourced training data from books  and/or Wikipedia. As models scaled, researchers began collecting larger datasets by scraping text from the web, e.g., WebText consists of upvoted links from Reddit , and this approach is now the dominant method for assembling large-scale text datasets . While this is now the dominant approach, it is not without concerns. These concerns range from socially benign, e.g., low-quality content that leads to gibberish generations  or large amounts of duplicate content , to more socially harmful outcomes such as generation which is: biased and/or toxic , revealing of personally identifiable information , or factually inaccurate . In this paper, we present a novel dataset sourced from business-oriented content which is more factually accurate, less toxic, and paired with the time the content became public.

Business-oriented text datasets.Early business-oriented corpuses for NLP research were generally of small scale, i.e., tens of millions of tokens , and therefore more suitable for fine-tuning or downstream evaluation than pre-training. Recently, datasets on the scale of billions of tokens have been developed using proprietary data, e.g., Bloomberg's catalog of content , and publicly available data, e.g., content from the Securities and Exchange Commission's (SEC) Electronic Data Gathering and Retrieval (EDGAR) system. Notably, prior work using EDGAR data generally considers only a subset of the disclosures posted to EDGAR. For example, Loukas et al.  extract text solely from annual reports to create a dataset of 6.5B tokens. Wu et al.  extract text from annual _and_ quarterly reports to create a _non_-public dataset of 14.5B tokens. Other datasets relying on EDGAR are of significantly smaller size, i.e., less than 1B tokens . We extend this literature by considering all disclosures on EDGAR and find that 83% of BeanCounter comes from disclosure types not included in prior work. Additionally, we consider not only the main content of EDGAR disclosures but attachments as well and find that 29% of tokens are sourced from attachments.2

Figure 1: **Overview of dataset construction: All EDGAR filings are downloaded, text is extracted from filings using content-type specific extractors, extracted text is then cleaned and deduplicated.**

[MISSING_PAGE_FAIL:4]

Compared to prior literature, we find that our approach appears to extract nearly twice as much content: 10.9B vs. 6.5B tokens from annual reports  and 22.6B vs. 14.5B tokens from both annual and quarterly reports . Based on manual inspection of Loukas et al. , we attribute this difference to our processing of not only the main filing but also any attachments.

Industry and Firm.To better understand group representation in BeanCounter, we explore which industries and firms contribute the most content. We adopt the commonly used industry classification of Fama and French  to assign each firm to one of 48 high-level industries. Figure 3 shows that Banks and Financial Services are the two most represented industries followed by Business Services, Pharmaceuticals, and Chip Fabrication.

Next, we consider which specific firms are most prevalent in BeanCounter. After matching on firm identification from the S&P Capital IQ dataset, there are 16.6K firms in the BeanCounter dataset. Given that the financial services industries are very highly represented, we present representation separately for the top 20 firms and the top 20 _non-financial_ firms, i.e. all firms not in the Finance, Insurance, Banks nor Real Estate industries. Figure 3(a) shows that many well-known financial firms, e.g., Goldman Sachs, contribute large volumes of content to BeanCounter. Figure 3(b) shows that the most represented non-financial firms tend to be older, well-established firms such as AT&T and United Airlines.

### Exploration of Gender and Pronouns

Numerous studies have shown that NLP systems tend to propagate gender bias found in their training corpora . Along these lines, we explore gender biases in BeanCounter by computing the percentage of filings which contain at least one instance of a specific type of pronoun. Table 2 presents results. Similar to prior work, we observe that _He_ pronouns are over-represented relative to _She_ pronouns . This could mean that models trained on BeanCounter may generate _He_ pronouns at a higher rate than _She_ pronouns. We also observe that 2nd person pronouns are used roughly 20% less frequently-on an absolute basis-than in the dataset of Touvron et al. .

Figure 4: Firms that contribute the most to textual volume.

Figure 3: Top 10 industries with the highest token volume.

### Exploration of Demographic Identities

Similar to Touvron et al. , we measure demographic representation in BeanCounter by aggregating observations of descriptors from the HolisticBias dataset . Specifically, we compute the number of filings that contain at least one specific demographic descriptor across five axes: Gender and Sex, Sexual Orientation, Nationality, Race and Ethnicity, and Religion. We remove several demographic terms such as "straight", "white", "Black", "bi", "pan", "ace" and "poly" because these terms can have multiple meanings and after manual inspection we found these are generally used in ways other than as demographic descriptors. We also include expanded versions of abbreviated descriptors (e.g., "AFAB" and "Assigned-Female-At-Birth") as well as terms that are often hyphenated or capitalized (e.g. "African-American" and "African American"; "Male-to-Female" and "male-to-female") to capture as many mentions of these descriptors as possible. We note that while most of the terms are used in the intended context, some terms may be used in contexts outside of demographic identity, e.g., "Christian" is also a common English name, and we do not exclude these from the analysis.

In comparison to the pretraining corpuses of Touvron et al.  and Dodge et al. , Table 3 shows that BeanCounter has significantly more representation of content along the _Nationality_ and _Race and Ethnicity_ axes, whereas it has a significantly lower representation under the _Sexual Orientation_ axis.3 Similar to the two aforementioned datasets, we observe that BeanCounter is also skewed towards over-representation of Western identities such as "American", "European" and "Christian."

### Toxicity Analysis of Content Associated with Demographic Identities

To our knowledge, toxicity has not been explored in business-oriented corpuses. We extend this literature by exploring toxic language surrounding the demographic descriptors identified in the previous prevalence analysis (Section 3.3). Prior work on toxicity analysis typically focuses on assigning a toxicity score to randomly sampled spans of 100 tokens  or computing document-level average toxicity by scoring each line of a document separately . Our approach improves upon previous approaches in several ways.

First, prior literature has identified that toxic language is relatively rare, e.g., 0.2% of documents in the corpus of Touvron et al.  are labeled as toxic. Thus, randomly sampling text spans could omit highly toxic samples and hence underestimate the toxicity of the dataset. Further, even though highly toxic text may be rare, it does not mean such text is harmless or has limited downstream impact. Rather than randomly sampling parts of BeanCounter to measure toxicity, _we focus on instances where toxicity is likely to occur_: around demographic descriptors. As a result, one can view our analysis as providing evidence on the question "conditional on the existence of a demographic descriptor, how likely is the surrounding content to be toxic?"

Second, most SOTA toxicity classifiers are trained on comments from various web-based sources  or machine generated sentences about minority groups . As a result, they are best equipped to evaluate toxicity on texts of sentence lengths and are likely to make erroneous predictions for larger text spans such as those from breaking a document by new-line characters. We address this issue by identifying sentences containing these descriptors and passing them into the Perspective API . Manual inspection of classification results suggests that this leads to more accurate results relative to attempting to classify longer snippets or using other models such as ToxiGen-HateBERT .

  
**Gender Pronouns** & **68.23\%** & **Grammatical Pronouns** & **94.49\%** \\ 
**She** (she, her, hers, herself) & 29.81\% & **1st Person** (I, me, my, mine, myself, we...) & 69.22\% \\
**He** (he, him, his, himself) & 48.51\% & **2nd Person** (you, your,...) & 43.78\% \\
**Unknown** (they, them, their...) & 93.76\% & **3rd Person** (it, its, itself, she...) & 95.57\% \\   

Table 2: Percentage of filings in BeanCounter that contain gender and grammatical pronouns. 68.2% of all filings contain gender pronouns, and 29.8% of this subset contains _She_ pronouns.

Specifically, for every sentence in BeanCounter, we use a regex statement to determine whether the sentence contains any demographic descriptors; if there are multiple descriptors in a sentence, the sentence is assigned to the longest descriptor, i.e., if a sentence contains the descriptor "Latin American", it would be assigned to "Latin American" instead of "Latin" or "American". This is a generally reliable heuristic since most of the extracted sentences contain one descriptor; however, when sentences contain multiple distinct descriptors, it becomes unclear which descriptor is the primary "target." We follow the same procedure to extract all sentences with descriptors from the C4-en dataset. Since C4-en has a different frequency of descriptors than BeanCounter, we generate a balanced sample from C4-en which matches the frequency in BeanCounter. In particular, for each descriptor \(d_{i}\), we count the number of sentences \(n_{i}\) in BeanCounter which contain \(d_{i}\). We then randomly sample \(n_{i}\) sentences from C4-en which contain \(d_{i}\). We find that the average sentence length is 54 tokens, which is of comparable length to the training data of toxicity classifiers .

Next, we assign each sentence a toxicity score between 0 and 1 using the Perspective API . Table 2(b) presents the difference in average toxicity between C4-en and BeanCounter on the most prevalent descriptors in each demographic axis. We find that the average toxicity of sentences containing a given descriptor in BeanCounter is at least 59% lower than those from C4-en. In addition to quantitatively exploring differences in toxicity, we also read and compare examples of toxic content

Table 3: Demographic representation and toxicity from C4-en and BeanCounter. We find that toxic content in BeanCounter tends to come from the discussion of business models which involve breeding animals, medical ailments which affect certain portions of the population at different rates, firms providing examples of actions that would violate their ethics policies, and entities raising money to produce certain artistic works, e.g., movies, which contain content labeled as toxic. In contrast, the toxic content in C4-en tends to be directed at specific groups. Appendix E presents a sample of the most toxic examples from both datasets.

Considering other datasets, Rae et al.  also uses Perspective API and reports mean and median toxicity scores of 0.10 and 0.07, respectively, for their random sample of text spans. Even when explicitly sampling text likely to contain toxic language, toxicity of BeanCounter is an order of magnitude lower: the mean and median toxicity score of BeanCounter's descriptor sentences are 0.009 and 0.006, respectively. This suggests that BeanCounter can serve as a large-scale corpus of very low toxicity text.

## 4 Evaluation

Next, we explore the potential utility of BeanCounter along two dimensions: (i) domain-specific applications within finance and (ii) generation toxicity. To facilitate this analysis, we continue pre-training Pythia-1.4B  and Phi-1.5  for 1B tokens sampled from BeanCounter stratified by year. We choose these models because Pythia-1.4B is a well-studied model which is known to generate toxic content and Phi-1.5, while newer, has demonstrated excellent performance across a number of domains and exhibits comparatively low toxicity. We use the same optimization parameters used for initial pre-training of these models except that we reduce the maximum learning rate by 50% and cosine decay to 10% of the maximum without any warm-up phase . Details of financial, toxicity and general LLM evaluation benchmarks can be found in Appendix B.

### Financial Domain

To understand whether BeanCounter can improve a model's finance domain knowledge, we evaluated the models on two widely used benchmarks: Financial Phrasebank (FPB)  and Fin NER. FPB is a sentiment classification task on 4840 sentences sampled from financial news. The sentences are assigned "positive", "neutral" and "negative" labels according to how it will impact the mentioned company's stock prices. Fin NER is a named entity recognition task consisting of 1467 labeled sentences extracted from financial agreements filed on the SEC .

As seen in Table 4, we find larger improvements in the Phi-1.5 model after continued pretraining on BeanCounter; there is a 4.3% improvement on the Fin NER task and 2.5% improvement on FPB. The improvements for Pythia-1.4B are 1.4% and 1.1% for Fin NER and FPB respectively.

### Generation Toxicity

To quantify the models' propensity for toxic generation, we evaluate them on RealToxicityPrompts  and SafeNLP . RealToxicityPrompts is a set of 100K prompts extracted from sentences in the OpenWebText corpus intended to illicit toxic generation . The generation is assigned a toxicity score of 0 if the Perspective API  score is! 0.5 and 1 otherwise. SafeNLP uses a subset of the ToxiGen dataset  to compute safety scores across 13 marginalized demographics for a pre-trained language model .

    & **Fin NER** & **FPB** &  \\  & F-1 score & F-1 score & Toxicity score & Perspective score \\  Pythia-1.4B & 0.72 & 0.92 & 0.0579 & 0.1297 \\ Pythia-1.4B-BeanCounter & **0.73** & **0.93** & **0.0385** & **0.1118** \\  Phi-1.5 & 0.71 & 0.93 & 0.0224 & 0.0901 \\ Phi-1.5-BeanCounter & **0.74** & **0.95** & **0.0184** & **0.0887** \\   

Table 4: Model performance on financial domain and toxicity tasksTable 4 presents results. We find a reduction of 18-33% in Toxicity score after continued pretraining on BeanCounter. In terms of safety scores in Figure 5, these increase by 6-21% across demographics for Pythia-1.4B and by 3-7% for Phi-1.5 except for "jewish," "lating," "mexican," and "physical disability." It is important to note that although higher safety scores indicate less propensity to generate toxic content, it does not mean that the models are immune to harmful generations.

We hypothesize that the reduction in toxic generation results from (i) multi-turn conversations between firms and shareholders discussing topics associated with toxicity and/or (ii) firms presenting and explaining examples of violations of their ethics policies. For example, Section E.1 presents a discussion between the CEO of PepsiCo and a shareholder. While the discussion is some of the more toxic content in BeanCounter, it is still comparatively civil and each side explains their point of view. If LLMs can learn from such multi-turn conversations then this may be one reason we observe an improvement in safety scores.

## 5 Limitations

AblationWe evaluated the impact of continued pre-training with BeanCounter on two small LLM models (1.3B and 1.4B parameters for Phi-1.5 and Pythia-1.4B respectively). It is unclear if BeanCounter would have the same, lesser, or greater impact on model performance with different baseline model architectures, larger sizes, or training data mixes, e.g., pre-training solely on BeanCounter. Future work could explore additional models and incorporating BeanCounter into the training data mix at various percentages.

Deceptive ContentThe content in BeanCounter is produced by businesses with economic incentives to communicate a specific image of their business within the confines of the regulatory environment. As a result, the content may comply with regulations while being subtly misleading. Training on such data could lead to models which are misaligned with users' preferences, e.g., models which intentionally deceive the user in ways that are difficult to detect. In more extreme cases, the content may be entirely false, e.g., in the case of frauds such as ENRON and Wirecord. While this type of deceptive content is likely present in web-based datasets, researchers should be aware that the more regulated environment in which the contents of BeanCounter was produced does not guarantee that the data is free of deceptive content. The "BeanCounter.fraud" split of the data containing content from known frauds could enable future research to detect deceptive content.

GeneralizabilityWhile this work suggests that BeanCounter is a novel source of low-toxicity data, the data is sourced from an environment that differs substantially in content and style from the web. Along these lines, we find that some descriptors have different meanings within the business domain, and the prevalence of certain demographics differs from prior datasets. As a result, models trained solely on BeanCounter may generate text which lacks imagination, is not desirable for a broad

Figure 5: Changes in safety scores across 13 demographic target groups for Pythia-1.4B and Phi-1.5 after continued pre-training on BeanCounter. The safety score ranges from 0 to 1 where a higher score indicates a lower likelihood for the model to produce toxic generation relative to benign generation.

audience, fails to capture the meaning of certain words, and/or exhibits biases. Related to biases, there are limitations to our exploration of toxicity and demographic identities. Specifically, we only explored a subset of demographic identity descriptors from five axes. Future works could explore the additional axes, e.g., Ability, Socioeconomic Status and Age, to understand how these identities are represented in NLP datasets.

Out-of-Domain PerformanceWhile our work suggests that continued pre-training on BeanCounter can improve performance on financial related tasks, this may come at the expense of reduced performance on tasks unrelated to BeanCounter's content. Results for the two small LLMs we consider are mixed. Table 5 illustrates the models' performance on a suite of common general LLM comprehension and reasoning tasks. Pythia-1.4B models seem to perform similarly or slightly better on these benchmarks after continued pre-training, whereas Phi-1.5 models seem to experience decreases in performance in some tasks. Hence, the impact of continued pre-training on BeanCounter may be model-specific, or there may be interactions between the datasets used to train the models (e.g. Phi-1.5 is trained on a novel mix of data ).

## 6 Conclusion

In this work, we present BeanCounter, a 159B token dataset of business-oriented text extracted from publicly available financial disclosures. To our knowledge, BeanCounter is the largest public corpus in the business domain. Relative to datasets derived from the web, the content in BeanCounter is more likely to be truthful and of high quality because the entities producing the content face civil and criminal penalties if it is not. Additionally, each piece of content in BeanCounter is associated with a timestamp representing when the content was made available thereby facilitating future research on the role of time in language models. Lastly, we explore biases and toxicity in the data using a novel evaluation scheme which focuses on locations where biased and/or toxic content is most likely to be present. We find that BeanCounter is significantly less toxic compared to another widely used corpus, and our preliminary exploration of using BeanCounter for continued pre-training shows improvements in finance domain knowledge and reduced toxic generation.