# ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport

Nazar Buzun

AIRI, MIPT

buzun@airi.net

&Maksim Bobrin

Skoltech, AIRI

m.bobrin@skoltech.ru

Equal contribution.

Dmitry V. Dylov

Skoltech, AIRI

d.dylov@skoltech.ru

###### Abstract

We present a new approach for Neural Optimal Transport (NOT) training procedure, capable of accurately and efficiently estimating optimal transportation plan via specific regularization on dual Kantorovich potentials. The main bottleneck of existing NOT solvers is associated with the procedure of finding a near-exact approximation of the conjugate operator (_i.e._, the \(c\)-transform), which is done either by optimizing over non-convex max-min objectives or by the computationally intensive fine-tuning of the initial approximated prediction. We resolve both issues by proposing a new theoretically justified loss in the form of expectile regularization which enforces binding conditions on the learning process of the dual potentials. Such a regularization provides the upper bound estimation over the distribution of possible conjugate potentials and makes the learning stable, completely eliminating the need for additional extensive fine-tuning. Proposed method, called Expectile-Regularized Neural Optimal Transport (ENOT), outperforms previous state-of-the-art approaches in the established Wasserstein-2 benchmark tasks by a large margin (up to a 3-fold improvement in quality and up to a 10-fold improvement in runtime). Moreover, we showcase performance of ENOT for various cost functions in different tasks, such as image generation, demonstrating generalizability and robustness of the proposed algorithm.

_Project page with code_

https://skylooop.github.io/enot/

## 1 Introduction

Computational optimal transport (OT) has enriched machine learning (ML) by offering a new view-angle on the conventional ML tasks through the lens of comparison of probability measures (Villani et al. (2009); Ambrosio et al. (2003); Peyre et al. (2019); Santambrogio (2015)). In different works, OT is primarily employed either 1) as a differentiable proxy, with the OT distance playing the role of a similarity metric between the measures, or 2) as a generative model, defined by the plan of optimal transportation. One notable advantage of using OT in the latter setting is that, compared to other generative approaches, such as GANs, Normalizing Flows, or Diffusion Models, there is no assumption for one of the measures to be defined in a closed form (_e.g._, Gaussian or uniform) or to be pairwise-aligned, admitting various applications of the OT theory. Both loss objective and generative formulations of OT proved successful in a vast range of modern ML areas, includinggenerative modelling (Arjovsky et al. (2017); Gulrajani et al. (2017); Korotin et al. (2021); Liu et al. (2019); Leygonie et al. (2019)), reinforcement learning (Fickinger et al. (2022); Haldar et al. (2023); Papagiannis and Li (2022); Luo et al. (2023)), domain adaptation (Xie et al. (2019); Shen et al. (2018)), change point detection (Shvetsov et al. (2020)), barycenter estimation (Kroshnin et al. (2021); Buzun (2023); Bespalov et al. (2022a, b)), biology and genomics (Bunne et al. (2022)). Low dimensional discrete OT problems are solved via Sinkhorn algorithm (Cuturi (2013)), which employs _entropic regularization_. This technique makes the entire optimization problem differentiable and efficient computationally, but may require numerous iterations to converge to an optimal solution, whereas the OT problem for the tasks supported on high-dimensional measure spaces are usually intractable, oftentimes solvable only for the distributions which admit a closed-form density formulations. As a result, the need for computationally-efficient OT solvers has become both evident (Peyre et al. (2019)) and pressing (Montesuma et al. (2023); Khamis et al. (2024)).

In this work, we will be concerned with the complexity, the quality, and the runtime speed of the computational estimation of a _deterministic_ OT plan \(T\) between two probability measures \(\) and \(\) supported on measurable spaces \(,^{d}\) with Borel sigma-algebra. The OT problem in _Monge's formulation_ (MP) for a cost function \(c:\) is stated as:

\[(,)=_{T:T_{\#}=}_{ }c(x,T(x))(x),\] (1)

where \(\{T:T_{\#}=\}\) is the set of measure-preserving maps, defined by a push forward operator \(T_{\#}(B)=(T^{-1}(B))=(B)\) for any Borel subset \(B\). The minimizer of the cost above exists if \(\) is compact, \(\) is _atomless_ (i.e. \( x:(\{x\})=0\)) and the cost function is continuous (ref. Santambrogio (2015) Theorem 1.22 and Theorem 1.33).

However, MP formulation of the OT problem is intractable, since it requires finding the maps \(T\) under the coupling constraints (which is non-convex optimization problem) and is not general enough to provide a way for some mass-splitting solutions. By relaxing constraints in equation (1), the OT problem becomes convex and this form is known as _Kantorovich problem_ (KP) (ref. Villani et al. (2009)):

\[(,)=_{[, ]}_{}c(x,y)d(x,y)=_{ [,]}_{}[c(x,y)],\] (2)

where \([,]=\{( ):_{}d(x,y)=d(x),_{}d(x,y)=d(y)\}\) is a set of admissible couplings with respective marginals \(,\). MP and KP problems are equivalent in case \(=\) are compact, the cost function \(c(x,y)\) is continuous and \(\) is atomless. Since KP (2) is convex, it admits _dual formulation_ (DP), which is constrained concave maximization problem and is derived via Lagrange multipliers (Kantorovich potentials) \(f\) and \(g\):

\[(,)=_{(f,g) L_{1}() L_{ 1}()}_{}[f(x)]+_{} [g(y)]+_{,>0}\,_{}c (x,y)-f(x)-g(y),\] (3)

where \(L_{1}\) is a set of absolutely integrable functions with respect to underlying measures \(,\). The exchange between infimum and supremum is possible by strong duality (_Slater's_ condition). If one decomposes the outer expectation \(_{}\) in the last equation as \(_{(x)}_{(y|x)}\), we can notice that the supremum by \(f(x)\) should satisfy to the condition:

\[f(x) g^{c}(x)=_{}_{(y|x)}[c(x,y)-g(y)]=_{T :\,}c(x,T(x))-g(T(x)),\] (4)

otherwise, the infimum by \(\) would yield the \(-\) value. Operator \(g^{c}\) is called _c-conjugate_ transformation. If MP\(=\)KP, the solution \((y|x)\) is deterministic and one may set \((y|x)=T(x)\). Finally, DP (3) may be reduced to a single potential optimization task (using inequality (4), ref. Villani et al. (2009) Theorem 5.10):

\[(,) =_{g L_{1}()}_{}[g^ {c}(x)]+_{}[g(y)]\] (5) \[=_{g L_{1}()}_{T:\, }_{}[c(x,T(x))]+_{} [g(y)]-_{}[g(T(x))].\] (6)

In practice, during optimization process the infimum by \(T\) in c-conjugate transformation is approximated by a parametric model \(T_{}\), such that

\[g^{c}(x) g^{T}(x)=c(x,T_{}(x))-g(T_{}(x)).\] (7)A number of approaches were proposed to model \(T_{}\) in equation (6) _e.g._, with the Input Convex Neural Networks (ICNN) (Amos et al. (2017); Makkuva et al. (2020); Taghvaei and Jalali (2019)) or with arbitrary non-convex neural networks (Rout et al. (2021); Korotin et al. (2023)). Most of these approaches make an assumption that the cost is squared Euclidean and utilize Brenier theorem (Brenier (1991)), from which optimal map recovers as gradient of a convex function. The main bottleneck of these parametric solvers is their _instability in finding the optimal \(c\)-conjugate potential_\(g^{T}\) from equation (7) and the rough estimation of \(T\) often results in a situation where the sum of potentials \(g\) and \(g^{T}\) diverges. These instability problems were thoroughly discussed in works Amos (2023); Korotin et al. (2021). Recently, Amos (2023) showed that it is possible to find a near exact conjugate approximation by performing fine-tuning on top of initial guess (\(T_{}\)) in order to achieve closest lower bound in the inequality (7). Despite being an exact approximation to the true conjugate, such procedure requires extensive hyperparameter tuning and will definitely introduce an additional computational overhead.2

In this work, we propose to mitigate above issues by constraining the solution class of conjugate potentials through a novel form of _expectile regression_ regularization \(_{g}\). In order to make joint optimization of \(g\) and \(T_{}\) stable and more balanced (optimize \(g\) and \(T_{}\) in the OT problem (6) synchronously and with the same frequency), we argue that it is possible to measure proximity of potential \(g\) to \((g^{T})^{c}\) without an explicit estimation of the infimum in \(c\)-conjugate transform (4) and instead optimize the following objective:

\[_{}[g^{T}(x)]+_{}[g(y)]-_{ ,}[_{g}(x,y)].\] (8)

The regularizer will have to constrain the differences \(g(y)-(g^{T})^{c}(y)\) and \(g^{T}(x)-g^{c}(x)\) that should match at the end of training. We show that such a natural regularization outperforms the state-of-the-art NOT approaches in all of the tasks of the established benchmark for the computational OT problems (the Wasserstein-2 benchmark, presented in Korotin et al. (2021)), with a remarkable 5 to 10-fold acceleration of training compared to previous works and achieving faster convergence on synthetic datasets with desirable properties posed on OT map. Moreover, we show that proposed method obtains state-of-the-art results on generative image-to-image tasks in terms of FID and MSE.

## 2 Related Work

In the essence, the main challenge of finding the optimal Kantorovich potentials in equation (6) lies in alternating computation of the exact \(c\)-conjugate operators (4). Recent approaches consider the dual OT problem from the perspective of optimization over the parametrized family of potentials. Namely, parametrizing potential \(g_{}\) either as a non-convex Multi-Layer Perceptron (MLP) (Dam et al. (2019)) or as an Input-Convex Neural Network (ICNN) (Amos et al. (2017)). Different strategies for finding the solution to the conjugate operator can be investigated under a more general formulation of the following optimization (Makkuva et al. (2020); Amos (2023)):

\[_{}-_{}[g_{}((x))]+ _{}[g_{}(y)],_{}_{ }[_{}(T_{}(x),(x)) ],\] (9)

with \((x)\) being the fine-tuned argmin of \(c\)-conjugate transform (4) with initial value \(T_{}(x)\). Loss objective \(_{}\) can be one of three types of amortization losses which makes \(T_{}(x)\) converge to \((x)\). This max-min problem is similar to adversarial learning, where \(g_{}\) acts as a discriminator and \(T_{}\) finds a deterministic mapping from the measure \(\) to \(\). The first objective in equation (9) is well-defined under certain assumptions and the optimal parameters can be found by differentiating w.r.t. \(\), according to the Danskin's envelope theorem (ref. Danskin (1966)). We briefly overview main design choices of the amortized models \(T_{}(x)\) in the form of continuous dual solvers and the corresponding amortization objective options for \(_{}\) in the Appendix C.

Another method considers the solution to the optimal map in (1) from a different perspective by introducing a regularization term named Monge Gap (Uscidda and Cuturi (2023)) and learns optimal \(T\) map from Monge formulation directly without any dependence on conjugate potentials. More explicitly, by finding the reference measure \(\) with Support(\(\)) \(\) Support(\(\)), the following regularizer quantifies deviation of \(T\) from being optimal transport map:

\[_{}^{c}=_{}[c(x,T(x))]-^{c}( {},T_{\#})\] (10)with \(^{}\) being entropy-regularized Kantorovich problem (2). However, despite its elegance, we still need some method to compute \(^{}\), and the underlying measure \(\) should be chosen thoughtfully, considering that its choice impacts the resulting optimal transport map and the case when \(=\) does not always provide expected outcomes.

## 3 Background

Bidirectional transport mapping.We employ notations with hats (\(}\), \(\), \(\), \(=()^{c}\)) to indicate the correspondence to the solution (argmins) of the OT problem (6). Optimality in equation (6) is obtained whenever complementary slackness is satisfied, namely: \((x,y)(}):^{c}(x)+ (y)=c(x,y)\). Consider a specific setting when the optimal transport plan \(}(x,y)\) is deterministic and MP=KP. Let the domains of \(,\) be equal and compact, i.e. \(=\), for some strictly convex function \(h\) the cost \(c(x,y)=h(x-y)\). Denote by \(h^{*}\) the convex conjugate of \(h\), implying that \(( h)^{-1}= h^{*}\). If \(\) is absolutely continuous, then \(}(x,y)\) is unique and concentrated on graph \((x,(x))\). Moreover, one may link it with Kantorovich potential \(\) as follows (Santambrogio (2015) Theorem 1.17): \((x)_{xC}(x,(x))\) and particularly for \(c(x,y)=h(x-y)\)

\[(x)=x- h^{*}((x)).\] (11)

If the same conditions are met for measure \(\) we can express the inverse mapping \(^{-1}(y)\) through the potential \(\):

\[^{-1}(y)=y- h^{*}((y)).\] (12)

Max-min optimization in problem (6) by means of parametric models \(f_{}\) and \(g_{}\) is unstable due to non-convex nature of the problem. One way to improve robustness is to simultaneously train bidirectional mappings \((x)\) and \(^{-1}(y)\) expressed by formulas (11) and (12), thus yielding self-improving iterative procedure. During the training, we also may use the equations (11) and (12) with non-optimal functions \(f_{}\) and \(g_{}\), because there are no restrictions on \(T\) in problem (6) and we can use any representation for the transport mapping function. Under weaker constraints (for example \(==^{d}\)), the c-concavity of the potentials \(f\) and \(g\) may be required. In this case, we can rely on a local c-concavity in the data concentration region or, if the conditions for equations (11) and (12) are not satisfied, we can use an arbitrary function for \(T_{}\) and do not express it through the potential \(f_{}\). Under Brenier's theorem conditions (Brenier (1991)) in domain \(^{n}\) for the squared Euclidean cost, it holds that \((x)=x-(x)\), where \(\) is some \(l_{2}\)-concave function. It follows that the optimal potentials \(\) and \(\) are \(l_{2}\)-concave, even if one uses not \(l_{2}\)-concave potentials \(f\), \(g\) in the training process.

Expectile regression.The idea behind the proposed regularization approach is to minimize the least asymmetrically weighted squares. It is a popular option for estimating conditional maximum of a distribution through neural networks. Recently, expectile regression was used in some offline Reinforcement Learning algorithms and representation learning approaches (Ghosh et al. (2023)). Let \(f_{}:^{d}\) be some parametric model from \(L_{2}(^{d})\) space and \(x,y\) be dependent random variables in \(^{d}\), where \(y\) has finite second moment. By definition (Newey and Powell (1987)), the expectile regression problem is:

\[_{}_{}(y-f_{}(x))= _{}-[y f_{}(x)]\,(y-f_{ }(x))^{2}, 0.5.\] (13)

The expectation is taken over the \(\{x,y\}\) pairs. The asymmetric loss \(_{}\) reduces the contribution of those values of \(y\) that are smaller than \(f_{}(x)\), while the larger values are weighted more heavily (ref. Figure 5). The expectile model \(f_{}(x)\) is strictly monotonic in parameter \(\). Particularly, the important property for us is when \( 1\), it approximates the conditional (on \(x\)) maximum operator over the corresponding values of \(y\)(Bellini et al. (2014)). Below we compute c-conjugate transformation by means of expectile.

## 4 Proposed Method

The main motivation behind our method is to regularize optimization objective in DP (6) with non-exact approximation of \(c\)-conjugate potential \(g^{T}(x)\), defined in (7). The regularisation term \(_{g}(x,y)\) should "pull" \(g(y)\) towards \((g^{T})^{c}(y)\) and \(g^{T}(x)\) towards \(g^{c}(x)\). Instead of finding explicit \(c\)-conjugate transform, we compute \(\)-expectile of random variables \(g^{T}(x)-c(x,y)\), treating \(y\) as a condition. From the properties of expectile regression described above and equation (4) follows that when \( 1\), the expectile converges to

\[_{x}g^{T}(x)-c(x,y)=-(g^{T})^{c}(y).\] (14)

Let the parametric models of Kantorovich potentials be represented as \(f_{}(x)\) and \(g_{}(y)\). The transport mapping \(T_{}(x)\) has the same parameters as \(f_{}(x)\) if it can be expressed through \(f_{}\) (ref. 11), or otherwise, when \(f_{}\) is not used (one-directional training), it is its own parameters. Let approximate the maximum of eq. (14) by \(\)-expectile of \(g_{}^{T}(x)-c(x,y)\) conditioning on \(y\). So the target (term \(y\) in eq. 13) of the expectile regression here is \(g_{}^{T}(x)-c(x,y)\). The model in this case is \(-g_{}(y)\). It has a negative sign because we approximate c-transform of \(g_{}^{T}(x)\), which equals to \(_{x}c(x,y)-g_{}^{T}(x)\). The corresponding regression loss is \(_{}g_{}^{T}(x)-c(x,y)+g_{}(y)\). Accounting the definition of \(g^{T}\) (7) we obtain the regularization loss for potential \(g_{}\):

\[_{g}(,x,y)=_{}c(x,T_{}(x))-g_{} (T_{}(x))-c(x,y)+g_{}(y).\] (15)

The proposed expectile regularisation is incorporated into alternating step of learning the Kantorovich potentials by implicitly estimating \(c\)-conjugate transformation, additionally encouraging model \(g\) to satisfy the \(c\)_-concavity criterion_(Villani et al. (2009) Proposition 5.8). We minimize \(R_{g}()=_{,}_{g }(,x,y)\) by \(\) and simultaneously do training of the dual OT problem (6), splitting it into two losses

\[L_{g}()=-_{}[g_{}(y)]+ _{}[g_{}(T_{}(x))],\] (16) \[L_{f}()=-_{}[g_{}(T_{ }(x))]+_{}[c(x,T_{}(x))].\] (17)

**Input**: samples from unknown distributions \(x\) and \(y\); cost function \(c(x,y)\);

**Parameters**: parametric potential model \(\) or vector field \(=\), parametric potential model \(\), optimizers opt_f and opt_g, batch size \(n\), train steps \(N\), expectile \(\), expectile loss weight \(\), bidirectional training flag is_bidirectional;

**function** train_step(\(\), \(\), \(\{x_{1},,x_{n}\}\), \(\{y_{1},,y_{n}\}\))

```
1:{Assign OT mapping \((x)\)}
2:ifis_bidirectional is true\((x)=x- h^{*}((x))\)else\((x)=(x)\)
3:{Compute dual OT losses and expectile regularisation \(R_{q}\)}
4:\(L_{q}=_{i=1}^{n}((x_{i}))-_{i =1}^{n}(y_{i})\)
5:\(L_{}=_{i=1}^{n}c(x_{i},(x_{i}))- ((x_{i}))\)
6:\(R_{q}=_{i=1}^{n}_{}}c(x_{i}, (x_{i}))-c(x_{i},y_{i})+(y_{i})-((x_{i} ))\)
7:{Apply gradient updates for parameters of models \(\) and \(\)}
8:opt_f.minimize(\(\), loss \(=L_{}\))
9:opt_g.minimize(\(\), loss \(=L_{q}+ R_{}\))
10:endfunction
11:{Main train loop}
12:for\(t 1,,N\)do
13: sample \(x_{1},,x_{n}\), \(y_{1},,y_{n}\)
14:ifis_bidirectional is false or \(t 2=0\)then
15: train_step(\(\), \(\), \(\{x_{1},,x_{n}\}\), \(\{y_{1},,y_{n}\}\))
16:else
17:{Update inverse mapping \(\) by swapping f and g}
18: train_step(\(\), \(\), \(\{y_{1},,y_{n}\}\), \(\{x_{1},,x_{n}\}\))
19:endif
20:endfor
21: sample \(x_{1},,x_{n}\), \(y_{1},,y_{n}\)
22:{Approximate OT distance by sum of conjugate potentials, get \(T\) from step 2}
23:dist\(=_{i=1}^{n}(y_{i})+c(x_{i},(x_{i}))- ((x_{i}))\); \((x)=c(x,(x))-((x))\)
24:returnf,g,dist ```

**Algorithm 1** ENOT Training

Algorithm 1 describes a complete training loop with full objective expression \(L_{f}()+L_{g}()+ R_{g}()\) and hyperparameters \(\) (expectile) and \(\). It includes two training options: one-directional with models \(g_{}\) and \(T_{}\); and bidirectional for strictly convex cost functions in form of \(h(x-y)\) with models \(f_{},g_{}\) and \(T_{},T_{}^{-1}\) (the latter are represented in terms of \(f_{},g_{}\) by formulas (11), (12)). The bidirectional training procedure updates \(g_{},T_{}\) in one optimization step and then switches to \(f_{},T_{}^{-1}\) update in the next step. This option includes analogical regularisation term for the potential \(f_{}\):

\[_{f}(,x,y)=_{}c(T_{}^{-1}(y),y)-f_ {}(T_{}^{-1}(y))-c(x,y)+f_{}(x).\] (18)

In the end of training we approximate the correspondent Wasserstein distance by expression

\[W_{c}(,)=_{}[g_{}(y)]- _{}[g_{}(T_{}(x))]+ _{}[c(x,T_{}(x))]\] (19)

with optimized parameters \(,\). We include a formal convergence analysis for \(\) regularized functions being a tight bound on the exact solution to the c-conjugate transform in Appendix D.

## 5 Experiments

In this section, we provide a thorough validation of ENOT on a popular W2 benchmark to test the quality of recovered OT maps. We compare ENOT with the state-of-the-art approaches and also showcase its performance in generative tasks. Additional results and visualizations on 2D _synthetic_ tasks are provided in Appendix F.

### Results on Wasserstein-2 Benchmark

While evaluating ENOT, we also measured the wall-clock runtime on all Wasserstein-2 benchmark data (Korotin et al. (2021)). The tasks in the benchmark consist of finding the optimal map under the squared Euclidean norm \(c(x,y)=\|x-y\|^{2}\) between either: (**1**) high-dimensional (HD) pairs \((,)\) of Gaussian mixtures, where the target measure is constructed as an average of gradients of learned ICNN models via \(W_{2}\)(Korotin et al. (2019)) or (**2**) samples from pretrained generative model W2GN (Korotin et al. (2021)) on CelebA dataset (Liu et al. (2015)). The quality of the map \(\) from \(\) to \(\) is evaluated against the ground truth optimal transport plan \(T^{*}\) via _unexplained variance percentage_ (\(_{2}^{}\)) (Korotin et al. (2019, 2021); Makkuva et al. (2020)), which quantifies deviation from the optimal alignment \(T^{*}\), normalized by the variance of \(\):

\[_{2}^{}(,,)=100 _{}\|(x)-T^{*}(x)\|^{2}}{_{ {}}[y]}.\] (20)

The results of the experiments are provided in Table 1 for CelebA64 (\(64 64\) image size) and in Table 2 for the mixture of Gaussian distributions with a varying number of dimensions \(D\). Overall, ENOT manages to approximate optimal plan \(T^{*}\) accurately and without any computational overhead compared to the baseline methods which require an inner conjugate optimization loop solution. To be consistent with the baseline approaches, we averaged our results across \(3\)-\(5\) different seeds. All the hyperparameters are listed in Appendix E.2 (Table 6).

Despite the fact that we compute at each train step a _non-exact_ c-transform, the expectile regularization enables the method to outperform all _exact_ methods in all our extensive tests. In actuality, the regularization does not introduce an additional bias, neither in theory, nor in practice. At the end of training (or upon convergence), we obtain the exact estimate of the c-conjugate transformation. Other methods demand near-exact estimation at each optimization step, requiring additional inner optimization and introducing significant overhead. We assume that introduces an imbalance in the simultaneous optimization by \(g\) and \(T\) in equation (6), underestimating the OT distance as a result.

### Different Cost Functionals

We further investigate how ENOT performs for different cost functions and compare Monge gap regularization (Uscidda and Cuturi (2023)) and ENOT between the measures defined on 2D synthetic datasets. In Figure 1, we observe that despite recovering Monge-like transport maps \(T_{}\), ENOT achieves convergence up to \(2\) faster and produces more desirable OT-like optimal maps. To test other specific use cases, we conducted experiments on 2D spheres data (Figure 2), where we parametrizethe map \(T_{}\) as a MLP and test the algorithms with the geodesic cost \(c(x,y)=(x^{T}y)\) with \(n=1000\) iterations. We set here flag is_bidirectional=False (meaning the training mode is one-directional in this example). Remarkably, the time required for convergence is minimal for ENOT, while the Monge gap takes up to three times longer. Moreover, in our experiments, Monge gap solver diverged for \(n>1300\) iterations. ENOT consistently estimates accurate and continuous OT maps.

  
**Method** & Conjugate & Early Generator & Mid Generator & Late Generator \\  W2-Cycle & None & 1.7 & 0.5 & 0.25 \\ MM-Objective & None & 2.2 & 0.9 & 0.53 \\ MM-R-Objective & None & 1.4 & 0.4 & 0.22 \\  W2OT-Cycle & None & \(>100\) & 26.50\(\) 60.14 & 0.29 \(\) 0.59 \\ W2OT-Objective & None & \(>100\) & 0.29\(\) 0.15 & 0.69 \(\) 0.9 \\  W2OT-Cycle & L-BFGS & 0.62 \(\) 0.01 & 0.20 \(\) 0.00 & 0.09 \(\) 0.00 \\ W2OT-Objective & L-BFGS & 0.61 \(\) 0.01 & 0.20 \(\) 0.00 & 0.09 \(\) 0.00 \\ W2OT-Regression & L-BFGS & 0.62 \(\) 0.01 & 0.20 \(\) 0.00 & 0.09 \(\) 0.00 \\ W2OT-Cycle & Adam & 0.65 \(\) 0.02 & 0.21 \(\) 0.00 & 0.11 \(\) 0.05 \\ W2OT-Objective & Adam & 0.65 \(\) 0.02 & 0.21 \(\) 0.00 & 0.11 \(\) 0.05 \\ W2OT-Regression & Adam & 0.66 \(\) 0.01 & 0.21 \(\) 0.00 & 0.12 \(\) 0.00 \\   

Table 1: \(_{2}^{}\) comparison of ENOT on CelebA64 tasks from the Wasserstein-2 benchmark. The attributes after the method names (‘Cycle’, ‘Objective’, ‘Regression’) correspond to the type of amortisation loss. Column ‘Conjugate’ indicates the selected optimizer for the internal fine-tuning of \(c\)-conjugate transform. The results of our method include the mean and the standard deviation across \(3\) different seeds. The best scores are highlighted.

Figure 1: Fitting of three different transport maps \(T_{}\) between source and target measures in \(^{2}\) with Euclidean cost function \(c(x,y)=\|x-y\|\). We use the same number of iterations and MLP architecture for each method. **Left**: Sinkhorn divergence; **Middle**: Monge gap; **Right**: ENOT.

Figure 2: Recovered OT maps \(T_{}\) between synthetic measures on 2-sphere with geodesic cost \(c(x,y)=(x^{T}y)\). All models are MLPs with outputs normalized to be on a unit sphere. Blue dots are the empirical source measure, red crosses are the empirical target measure and the orange crosses are the result of the found transport map. **Left**: Sinkhorn; **Middle**: Monge; **Right**: ENOT.

### Unpaired Image-to-Image Translation

To showcase the power of expectile regularization beyond the \(W_{2}\) benchmarks, we apply our method to an unpaired image-to-image translation task. The corresponding image datasets are: female subset of Celebrity faces (CelebA(f)) (Liu et al. (2015)), Anime Faces (Anime)3, Flickr-Faces-HQ (FFHQ) (Karras et al. (2019)), comic faces v2 (Comics)4, Handbags and Shoes5. The datasets are pre-processed in the conventional way as described in (Gazdieva et al. (2023)). The trained transport maps include: Handbags to Shoes, FFHQ to Comics, CelebA(f) to Anime. We employ squared Euclidean cost function divided by the image size (\(64\) or \(128\)), basic U-Net architecture (Ronneberger et al. (2015)) for the transport map \(T_{}(x)\), and ResNet from WGAN-QC (Liu et al. (2019)) as a potential \(g_{}(y)\). ENOT trains in one-directional mode with total steps count \(N=120k\). Appendix Table 7 contains a complete list of hyperparameters (conventionally, we have used FID (Heusel et al. (2017)) metric for the hyperparameters tuning). We report the learned transport maps in Figure 3 as well as the widely used FID and MSE metrics in Table 3. Appendix 10 includes additional evaluation on test images.

  
**Method** & Conjugate & \(D=2\) & \(D=4\) & \(D=8\) & \(D=16\) & \(D=32\) \\  W2-Cycle & None & 0.1 & 0.7 & 2.6 & 3.3 & **6.0** \\ MM-Objective & None & 0.2 & 1.0 & 1.8 & 1.4 & **6.9** \\ MM-R-Objective & None & 0.1 & 0.68 & 2.2 & 3.1 & **5.3** \\  Monge Gap & None & \(0.1 0.0\) & \(0.57 0.0\) & \(2.05 0.06\) & \(4.22 0.1\) & \(7.24 0.17\) \\  W2OT-Cycle & None & \(0.05 0.0\) & \(0.35 0.01\) & \(>100\) & \(>100\) & \(>100\) \\ W2OT-Objective & None & \(>100\) & \(>100\) & \(>100\) & \(>100\) \\  W2OT-Cycle & L-BFGS & \(>100\) & \(>100\) & \(>100\) & \(>100\) \\ W2OT-Objective & L-BFGS & \(0.03 0.0\) & \(0.22 0.01\) & \(0.6 0.03\) & \(0.8 0.11\) & \(2.09 0.31\) \\ W2OT-Regression & L-BFGS & \(0.03 0.0\) & \(0.22 0.01\) & \(0.61 0.04\) & \(0.77 0.1\) & \(1.97 0.38\) \\ W2OT-Cycle & Adam & \(0.18 0.03\) & \(0.69 0.56\) & \(1.62 2.82\) & \(>100\) & \(>100\) \\ W2OT-Objective & Adam & \(0.06 0.01\) & \(0.26 0.02\) & \(0.63 0.07\) & \(0.81 0.10\) & \(1.99 0.32\) \\ W2OT-Regression & Adam & \(0.22 0.01\) & \(0.28 0.02\) & \(0.61 0.07\) & \(0.8 0.10\) & \(2.07 0.38\) \\ 
**ENOT (Ours)** & None & \(0.02 0.0\) & \(0.03 0.001\) & \(0.14 0.01\) & \(0.24 0.03\) & \(0.67 0.02\) \\   

Table 2: \(_{2}^{}\) comparison of ENOT with baseline methods on the high-dimensional (HD) tasks from Wasserstein-2 benchmark. The suffixes (‘Cycle’, ‘Objective’, ‘Regression’) correspond to the type of amortisation loss. Column ‘Conjugate’ indicates the selected optimizer for the internal fine-tuning of \(c\)-conjugate transform. \(D\) is the dimension of the measures domain. The mean and the standard deviations of our method are computed across 5 different seeds. The best scores are highlighted.

Image-to-image translation baselines include popular GAN-based approaches: CycleGAN (Zhu et al., 2017) and StarGAN-v2 (Choi et al., 2020), and two other recent neural OT methods: Extremal OT (Gazdieva et al., 2023) and Kernel OT (Korotin et al., 2023). _ENOT outperforms the baselines in all tasks in therms of FID score and, as in all other experiments, significantly speeds up the computation._ It takes about 5 hours to train the transport model on one GPU RTX 3090 Ti with image size \(64 64\) and about 16 hours when the image size is \(128 128\), while an approximate training time of the other OT algorithms and GANs is about 3 days on the same GPU.

### Ablation Study: Varying hyperparameters expectile and regularization weight

Figure 4 presents the study of the impact of the proposed expectile regularization on the \(_{2}^{}\) metric. This is done by varying the values of the expectile hyperparameter \(\) and the scaling of the expectile loss coefficient \(\) in Algorithm 1. Colored contour plots show the areas of the lowest and the highest values of \(_{2}^{}\). The grey areas depict the cases when the OT solver diverged. For example, in high-dimensions, D \( 64\), it is the case for \(=0\), pointing out that the expectile regularization with \(\) is necessary to prevent the instability during training.

The ablation study shows that, even when the parameter choices of \(\) and \(\) are not optimal, _ENOT still outperforms the other baseline solvers_ in Table 2, making ENOT approach robust to extensive hyperparameter tuning, compared to amortized optimization approach (Amos, 2023) sensitive to the hyperparameters of the conjugate solver. All ablation study experiments were conducted using the network structure and the learning rates in Appendix E.2 (Table 4) (they coincide with those in Table 2).

  
**Task and image size** & CycleGAN & StarGAN & Extr. OT & Ker. OT & ENOT \\  Handbags & Shoes 128 & 23.4 & 22.36 & 27.10 & 26.7 & 19.19 \\  FFHQ & Comics 128 & - & - & 20.95 & 20.81 & 17.11 \\  CelebA(f) & Anime 64 & 20.8 & 22.40 & 14.65 & 18.28 & 13.12 \\  CelebA(f) & Anime 128 & - & - & 19.44 & 21.96 & 18.85 \\    
  
**Task and image size** & CycleGAN & StarGAN & Extr. OT & Ker. OT & ENOT \\  Handbags & Shoes 128 & 0.43 & 0.24 & 0.37 & 0.37 & 0.34 \\  FFHQ & Comics 128 & - & - & 0.22 & 0.21 & 0.20 \\  CelebA(f) & Anime 64 & 0.32 & 0.21 & 0.30 & 0.34 & 0.26 \\  CelebA(f) & Anime 128 & - & - & 0.31 & 0.36 & 0.28 \\   

Table 3: Comparison of ENOT to baseline methods for image-to-image translation. We evaluate generation task between two different datasets: Source \(\) Target. And compare resulting images based on Frechet Inception Distance (FID) and Mean Squared Error (MSE). Empty cells indicate that original authors of particular method did not include results for those tasks.

Figure 3: **Left**: Handbags to Shoes; **Middle**: FFHQ to Comics; **Right**: CelebA(f) to Anime; all images sizes are 128x128, the 1st row contains the source images, the 2nd row contains predicted generative mapping by ENOT; **Cost function**: \(L^{2}\) divided by the image size.

## 6 Conclusion, Limitations and Future Work

Our paper introduces a new method, ENOT, for efficient computation of the conjugate potentials in neural optimal transport with the help of expectile regularisation. We show that a solution to such a regularization objective is indeed a close approximation to the true \(c\)-conjugate potential. Remarkably, ENOT surpasses the current state-of-the-art approaches, yielding an up to a 10-fold improvement in terms of the computation speed both on synthetic 2D tasks and on well-recognized Wasserstein-2 benchmark.

The proposed regularized objective on the conjugate potentials relies on two additional hyperparameters, namely: the expectile coefficient \(\) and the expectile loss trade-off scaler \(\), thus requiring a re-evaluation for new data. However, given the outcome of our ablation studies, the optimal parameters found on the Wasserstein-2 benchmark are _optimal enough_ or at least provide a good starting point.

We believe that ENOT will become a new baseline to compare against for the future NOT solvers and will accelerate research in the applications of optimal transport in high-dimensional tasks, such as generative modelling. As for future directions, ENOT can be tested with the other types of cost functions, such as Lagrangian costs, defined on non-Euclidean spaces and in the dynamical optimal transport settings, such as flow matching.