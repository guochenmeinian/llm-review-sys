# BiVLC: Extending Vision-Language Compositionality Evaluation with Text-to-Image Retrieval

Imanol Miranda &Ander Salaberria &Eneko Agirre &Gorka Zakune

HiTZ Center - Ixa, University of the Basque Country (UPV/EHU)

{imanol.miranda, ander.salaberria, e.agirre, gorka.azcune}@ehu.eus

###### Abstract

Existing Vision-Language Compositionality (VLC) benchmarks like SugarCrepe are formulated as image-to-text retrieval problems, where, given an image, the models need to select between the correct textual description and a synthetic hard negative text. In this work, we present the Bidirectional Vision-Language Compositionality (BiVLC) dataset. The novelty of BiVLC is to add a synthetic hard negative image generated from the synthetic text, resulting in two image-to-text retrieval examples (one for each image) and, more importantly, two text-to-image retrieval examples (one for each text). Human annotators filter out ill-formed examples ensuring the validity of the benchmark. The experiments on BiVLC uncover a weakness of current multimodal models, as they perform poorly in the text-to-image direction. In fact, when considering both retrieval directions, the conclusions obtained in previous works change significantly. In addition to the benchmark, we show that a contrastive model trained using synthetic images and texts significantly improves over the base model in SugarCrepe and in BiVLC for both retrieval directions. The gap to human performance in BiVLC confirms that Vision-Language Compositionality is still a challenging problem. BiVLC and code are available at https://imirandam.github.io/BiVLC_project_page.

## 1 Introduction

Vision-Language Compositionality (VLC) refers to the ability to discern whether the composition of a given set of elements is the same for an image and a text. For example, assuming an image depicts a black dog and a white cat, but the text states that there is a white dog and a black cat, both compositions do not match. VLC performance is usually evaluated on image-to-text retrieval datasets: given an image and some textual descriptions, a model has to retrieve the description which matches the image. The candidates include hard negative distractors which differ from the correct description only in compositional details. However, there is no theoretical reason to favour image-to-text retrieval over text-to-image retrieval. The dominant use of the former lies probably on practical reasons, i.e. it is much easier to collect hard negative texts than hard negative images.

In this paper, we propose a semi-automatic method to generate an evaluation dataset for VLC that includes image-to-text and text-to-image retrieval (referred as I2T and T2I respectively). We refer to this problem _bidirectional_ Vision-Language Compositionality, since it combines both retrieval directions. For that purpose, we extend the SugarCrepe dataset , the reference benchmark for image-to-text retrieval-based VLC. We leverage the hard negative textual descriptions provided in the dataset and, for every hard negative text, we generate four candidate images using off-the-shelf text-to-image generators. We ask human annotators to select the image that better describes the text and to discard those instances where no image is correct. In a second annotation step, we ask annotators to remove instances where the generated image matches both captions,resulting in an ambiguous instance. As a result, we build a high-quality evaluation dataset called BiVLC (**B**idirectional **V**ision-**L**anguage **C**ompositionality), with almost three thousand instances composed by two images and two captions, which accounts to more than eleven thousand retrieval instances (Figure 1). In addition to the human filtering, we performed experiments which show that distinguishing between synthetic and natural images-texts is not enough to perform well in BiVLC, confirming the validity of the synthetic images.

BiVLC allows us to evaluate models on both text-to-image and image-to-text retrieval, offering a more complete picture of VLC capabilities. The main contribution of this work is the dataset itself, as well as our findings based on the experiments and analysis on BiVLC. We summarize those findings as follows: (i) Humans perform comparably for both retrieval directions, but multimodal models are significantly worse for text-to-image retrieval; (ii) Bidirectional VLC is more difficult than image-to-text retrieval; (iii) The strongest models for image-to-text retrieval are not necessarily the strongest for bidirectional VLC; (iv) Training with hard negative images can boost the performance of multimodal contrastive models, obtaining competitive results for BiVLC, but still far from humans.

## 2 Related Work

To measure Vision-Language Compositionality, image-to-text retrieval is the dominant paradigm in the literature. Different datasets and benchmarks have been proposed following this approach: for example,  introduced CREPE, a large dataset containing hard negative texts generated by means of heuristic rules. The dataset was designed to measure systematicity and productivity, two important aspects of compositionality. A similar benchmark is ARO , which stands for Attribution, Relation, and Order, and also uses heuristic rules to produce hard negative captions given a matching image and text. ARO also stands out for its size, with more than 50,000 test cases.

Due to the use of heuristic rules for hard negative text generation,  found that very high accuracies can be obtained for those two datasets, even without using the images at all. Taking advantage of the biases introduced by the rule-based hard negative texts, such as nonsensical phrases or grammatically incorrect texts, purely textual models outperform the best multimodal models. Similar language biases have also been studied for concurrent benchmarks by . In consequence,  proposed a new methodology to produce hard negative texts, leveraging modern Large Language Models (LLM), adversarial refinement and human annotation. As a result, the SugarCrepe dataset was proposed, which showed that some of the conclusions obtained when using previous datasets, do not hold anymore. In this work, we extend SugarCrepe with hard negative images, i.e. with images that match the hard negative textual captions and differ from the original caption and image in compositional details. This way, we can measure bidirectional compositionality, i.e. we can measure the performance of multimodal models for image-to-text retrieval and also for text-to-image retrieval.

Figure 1: Given an image and two captions from SugarCrepe, BiVLC constructs an instance adding a negative image (Img-) generated from the negative caption (Caption-). The instance produces four retrieval examples: two for image-to-text retrieval and two for text-to-image retrieval.

In a similar fashion, Winoground (Thrush et al., 2022) also includes both retrieval directions to measure model performance. However, as shown by (Diwan et al., 2022), Winoground has several problems: (i) it is very small (it contains 400 instances), which makes the comparison among models difficult, (ii) it contains very few instances which actually measure Vision-Language Compositionality (only 171), and (iii) some other challenges like commonsense reasoning or locating small, out-of-focus objects in low resolution images are very important to perform well on the task. In this work, mimicking Winoground, we also build every instance of the dataset with two images and two captions, but we only focus on compositionality and we provide almost 3 thousand instances, increasing the size of Winoground considerably.

Recently, two other bidirectional datasets have been published: EqBen (Wang et al., 2023) and Cola (Ray et al., 2024). EqBen has been derived from video-text datasets and also offers a set of synthetic images generated with a graphic engine. However, the test set contains low-quality images (e.g. very dark or blurry pictures) (Lin et al., 2024) and authors have released a higher quality subset of 280 pairs of image-text pairs, which is even smaller than Winoground. Cola, on the other hand, has no natural texts, as the captions are produced using templates, and covers only a small subset of compositionality phenomena, as it focuses on object-attributes and spatial relations.

## 3 BiVLC: Bidirectional Vision-Language Compositionality Dataset

We introduce BiVLC, a new benchmark to measure bidirectional vision-language compositionality. Each instance of our dataset is comprised by two images and two captions, with the first pair labeled as positive (image+ and caption+) and the second pair labeled as negative (image- and caption-). For each of the images, one of the captions is the correct one and the other is a hard negative distractor. The same happens for each of the captions. Thus, we can measure image-to-text and text-to-image retrieval with hard negative pairs. Some examples are depicted in Figure 2 (See Appendix G for more examples).

### Dataset construction methodology

To generate BiVLC, we take SugarCrepe as the basis, since it already contains around 7 thousand images with their corresponding positive captions and hard negative captions, carefully created and filtered to avoid any undesirable biases. We apply the following steps (see Figure 3):

Step 1 - Uniformly format positive and hard negative captions:Since the formatting of the captions provided in COCO (which are used also for SugarCrepe) can vary, starting in lower case, all capitals, missing the final punctuation and so on, we have taken two steps to unify the formatting with the generated captions: 1) process all captions so that they start with capitals and continue in lower case, preserving capitals for named entities, and 2) add the final punctuation mark in case it is missing. This way, we avoid any form-based bias between natural and synthetic captions.

Figure 2: Three instances of BiVLC. Bottom row with negative captions and the corresponding images created by us. From left to right, negative captions created by Replace, Swap and Add.

Step 2 - Generate hard negative images:For every hard negative caption in the dataset, we generate 4 corresponding images. For that purpose we use SDXL-DPO (Wallace et al., 2023), a state-of-the-art text-to-image generator, which fine-tunes a Stable Diffusion XL model using the Direct Preference Optimization algorithm (Rafailov et al., 2024) on the Pick-a-Pic dataset (Kirstain et al., 2024). SDXL-DPO has shown to follow better human preferences than previous models. We basically prompt SDXL-DPO with the hard negative captions and make it generate 4 images, which we store.

Step 3 - Ask human annotators to choose the best generated image:Given a hard negative caption from SugarCrepe and the 4 images generated in Step 2, we hired human annotators to choose the best of the images. SDXL-DPO does not always generate a suitable image for a given textual description, so annotators were allowed to discard all 4 images. In such cases, we also discard the hard negative caption and its associated positive caption and image (more details about the annotation process in Appendix H.1)). After this annotation step, we have 5,382 instances of two images and two captions. In this step, we used 10 annotators divided into pairs to score a total of 500 annotations and obtain inter-tagger agreement. We calculated Cohen's Kappa score for each pair (Appendix H.3). Since there can be more than one valid images for a caption, we focused on the agreement between selecting any of the 4 images or none. We obtained a mean score of 0.49 between the 5 groups of annotators, which is interpreted as moderate agreement.

Step 4 - Filter ambiguous instances:During Step 3, annotators only see the hard negative caption and the four generated images. As they do not see the original image and its caption, there can be cases where the annotators marked an image as suitable for a given caption, but that same image could also match the original caption, creating an ambiguous instance. This is specially problematic for the Add category, for which by definition, the generated image also follows the original caption. To minimize those ambiguous instances, we run a filtering step, hiring again human annotators. More concretely, for each instance, we give annotators the original caption and both images. We ask them to choose the image that best describes the caption. If the annotator chooses the generated image, we consider that ambiguous and remove the instance from the dataset. If both images are equally suitable, annotators can mark those instances as ambiguous and we also filter them out.

As a result of this process, B1VLC has 2,933 instances of two images and two captions, or equivalently 11,732 retrieval instances, 50% of which are image-to-text retrieval and the other 50% text-to-image retrieval tests. As we extended SugarCrepe, we also keep its categories and variants. During both annotation steps (steps 3 and 4) we removed 61% of the instances, which accounts to the noise

Figure 3: Diagram of dataset construction: Starting from SugarCrepe instances, uniformly format positive and hard negative captions (Step 1), generate hard negative images (Step 2), ask human annotators to choose the best generated image (Step 3), and filter out ambiguous instances (Step 4). As a result. we get B1VLC instances, consisting of 2 captions and 2 images.

introduced by the text-to-image generator. The statistics of our dataset can be found in Table 1, where we also add SugarCrepe and Winoground for reference.

### Evaluation metrics

The main task defined in BiVLC consists on, given two images and two captions, selecting which image is paired with which caption. To measure the performance on the task, we use the same metrics as Winoground (Thrush et al., 2022), namely, the text, image and group scores. As shown by (Lin et al., 2014), those evaluation metrics discourage blind solutions and are more robust to language biases. For clarity and coherence, we rename the text score as image-to-text accuracy (I2T for short) and the image score as text-to-image accuracy (T2I). Basically, **I2T** measures the performance for image-to-text retrieval, thus it is equivalent to SugarCrepe. However, in contrast with SugarCrepe, for each instance in our dataset, we actually have two image-to-text retrieval examples. To obtain a perfect 12T score, the correct captions for both images have to be selected. The **T2I** is similarly defined for text-to-image retrieval. Finally, the **group score** is the main metric, since it combines the performance for image-to-text and text-to-image retrieval. To obtain a perfect group score for a given instance, both images have to be matched with the suitable captions and both captions with the suitable images (formal definitions can be found in Appendix D).

## 4 Experiments and Findings in BiVLC

In this section, we show the experiments performed in BiVLC and our findings using state-of-the-art open multimodal models for VLC. More concretely, we select the following contrastive models: (i) the original CLIP model (Radford et al., 2021), trained on a private dataset of 400M image-caption pairs; (ii) CLIPCOCO, a CLIP model we fine-tuned on the COCO dataset (Lin et al., 2014) using in-batch negatives (training details in Appendix E); (iii) NegCLIP, another CLIP model fine-tuned on the COCO dataset augmented with rule-based hard negative captions (Yuksekgonul et al., 2022); (iv) Generative Negative Mining (GNM) (Sahin et al., 2024), a CLIP model fine-tuned on a dataset which contains hard negative texts and images derived from COCO using image editing techniques. For all of them, we use ViT-B/32 as the backbone visual model, to ensure fair comparisons (cf Appendix E). We also evaluate the following generative models: (i) Open CapPa1, a recent open-source implementation of CapPa (Tschannen et al., 2024) which has shown similar performance as the original model, based on an encoder-decoder architecture with ViT-L-16 with registers and a decoder of 12 layers; (ii) VQAScore (Lin et al., 2024), based on an architecture which combines a CLIP vision encoder (ViT-L-336) with a Flan-T5 LLM (Chung et al., 2024) of varying sizes (XL refers to 3B and XXL to 11B). Table 2 shows the number of parameters of the different models, with VQAScore being significantly larger.

We also estimate human performance on BiVLC. For that purpose, we hired again human annotators. We took a random sample of 500 instances, and randomly divided them into groups of 25 for each of the 20 annotators. Then, we generated the 4 possible combinations, i.e. image-to-text and text-to-image retrieval, obtaining 2,000 queries. Each query consists of a base text or image and its positive and negative pair of the other modality. The goal is to select the image or text that best represents the base. In consequence, human annotators had to solve independent image-to-text and text-to-image

    &  &  &  &  &  &  \\  & & Obj & & & Att & & Rel & Obj & Att & Rel & Obj & Att \\  Winoground & ✓ & ✓ & & & & & 668 & & & 1,036 & & & 1,600\(\) \\ SugarCrepe & ✓ & & & 1,652 & 788 & 1,406 & 246 & 666 & & 2,062 & 692 & 7,512 \\ BiVLC (ours) & ✓ & ✓ & 4,800 & 1,748 & 1,848 & 324 & 1,112 & & 1,596 & 304 & 11,732 \\   

Table 1: Number of retrieval examples in VLC datasets, divided into different categories (see text) and total. \(\) Winoground subset with vision-language compositionality is lower, only 684. Winoground has 104 retrieval examples categorized both as Swap-Obj and Swap-Rel.

retrieval tasks, similarly to how we evaluate multimodal models. Combining the different annotations derived from an instance, we obtain human performance on our three metrics.

Table 2 shows the results of multimodal models both for B1VLC (depicting the three metrics) and for SugarCrepe. We also show human performance and the scores of a random system. Note that random performance for B1VLC is lower than for SugarCrepe, which means the task is inherently more difficult.

Finding 1: Contrary to humans, current models underperform on text-to-image retrieval.While humans show a balanced performance between text-to-image and image-to-text retrieval scores, the models behave very differently, i.e. the T2I score is in all cases significantly lower than the I2T score, being Open CapPa the only exception but with very low performance for both metrics. Those results show that current multimodal models do not have human-comparable VLC skills at all, and highlight the importance of measuring text-to-image retrieval.

Finding 2: The gap to humans is bigger in B1VLC than in SugarCrepe.The gap between the best model and humans in B1VLC is 10 points (based on group score, since it is the metric that measures bidirectional performance); for SugarCrepe, 5 points. This means that the bidirectional task is comparatively more difficult than image-to-text retrieval for models than for humans and that there is more room for improvement.

Finding 3: SugarCrepe and BiVLC performance are not correlated.The rank of the multimodal models is different in each of the datasets. For contrastive models, while NegCLIP is the best model for SugarCrepe, the best model in B1VLC is CLIP\({}_{}\), which uses COCO training examples as in-batch negatives. This is surprising: while NegCLIP uses hard negative texts for training and GNM also adds hard negative images (both derived from COCO), CLIP\({}_{}\) does not use any technique to mine any kind of hard negatives. On the other hand, the generative Open CapPa model is clearly ahead of all contrastive models for SugarCrepe, but it is the worst model in B1VLC. This can be explained by the language bias of its LLM decoder, which can be beneficial in SugarCrepe, but is strongly penalized in our bidirectional setting, as already shown in [Lin et al.].

## 5 Exploring training strategies for vision-language compositionality

We will focus on contrastive models for this exploration, since they are the _de-facto_ standard for retrieval problems and are more manageable for training (state-of-the-art generative models are significantly larger). There are two main strategies in the literature to improve the VLC skills of a contrastive model: (i) using hard negative texts for training (exemplified by NegCLIP [Yuksekgoul et al., 2022]), and (ii) using both, hard negative texts and images (the case of GNM [Sahin et al., 2024]). None of them has been successful in B1VLC. In Sections 5.1 and 5.2 we analyse the

    & **Model** & **Params** & **SugarCrepe** &  \\  & & & **I2T** & **T2I** & **Group** \\   & Human & N/A & 98.93 & 90.40 & 93.00 & 86.80 \\  & Random & N/A & 50.00 & 25.00 & 25.00 & 16.67 \\   & CLIP & & 76.56 & 75.83 & 52.40 & 49.06 \\  & CLIP\({}_{}\) & & 84.66 & **82.75** & **63.89** & **60.96** \\  & NegCLIP & & **85.64** & 80.74 & 61.95 & 58.75 \\  & GNM & & 81.83 & 81.32 & 60.86 & 57.96 \\   & Open CapPa & 676M & 90.59 & 57.72 & 56.19 & 41.97 \\  & VQAScore-XL & 3B & 90.85 & 81.96 & 76.61 & 70.20 \\  & VQAScore-XXL & 11B & **93.72** & **86.16** & **81.93** & **76.47** \\   

Table 2: Results for existing models (see text for details) on SugarCrepe and BiVLC. For the later, we provide the three metrics I2T, T2I and Group score. Human and random performance are also depicted. Bold for the best of each model family (contrastive and generative).

limitations of current implementations and propose two new models based on those strategies. We then evaluate them in SugarCrepe and BiVLC (Section 5.3).

### TROHN-Text: Training on Hard Negative Texts

NegCLIP uses manually defined rules to generate hard negative texts.  showed that texts generated that way show clear biases which prevent models from learning VLC skills properly. To overcome those limitations, we build TROHN-Text (**TR**aining **On H**ard **N**egative **Texts**), an automatic large-scale dataset of images and captions. Instead of relying on heuristic rules to mine those hard negative captions as previous works , we leverage modern LLMs. Basically, we take image-caption pairs from the COCO train split  and for every caption, we generate as many hard negative captions as category variants defined in SugarCrepe.

We used the open-source LLM OpenChat-3.5-0106  and the templates provided in SugarCrepe (more details in Appendix F). We generated 591,753 negative captions for each category, for a total of 4,142,271. We then defined a heuristic to filter out the generations that do not follow the templates. After this filtering, we have a total of 3,652,846 negative captions. Finally, we divided the instances consisting of two captions and one image randomly into 80% for training and 20% for validation (more details can be found in Appendix C). We fine-tune a pretrained CLIP model on TROHN-Text and call it CLIP\({}_{}\) (training details in Appendix E).

### TROHN-Img: Training on Hard Negative Images

GNM  used automatically generated images as hard negative examples, but to generate them, GNM uses image editing techniques and is limited to the Replace category. SyViC  also explores training with synthetic images, but their images are generated with a graphic engine and require techniques such as style transfer to make them more natural.

To overcome those limitations, we design a new dataset named TROHN-Img (**TR**aining **On H**ard **N**egative **I**mages). This dataset is based on the negative captions previously generated for TROHN-Text. Due to the resources and time needed for image generation, TROHN-Text is too big (>3M hard negative captions), so we decided to filter the instances to keep the best ones and obtain a training set equivalent to COCO in size . This way, we can also measure better the contribution of the hard negative images, since the size of the training set is similar to the one used for our CLIP\({}_{}\) model, the best contrastive model for BiVLC so far. Thus, we apply the following steps to generate TROHN-Img:

Step 1: Select the best instances based on plausibility and linguistic acceptability:Inspired by SugarCrepe, we used the Vera  and CoLA  models to score the negative captions and then select instances in the top 35 percentile, according to the combined score of both models. This step also contributes to generate more natural images, since the most implausible captions are discarded.

Step 2: Data deduplication and cleaning:We remove duplicate negative captions, captions in the form of questions, and generations that do not end with a final punctuation mark.

Step 3: Debiasing the dataset:As proposed in SugarCrepe, we applied adversarial refinement to the remaining captions based on the previous plausibility and linguistic acceptability scores of the positive and negative captions.

Step 4: Generating images based in negative captions:Using the captions obtained in step 3 as prompts, we generate one image per caption with the SDXL-DPO model.

After all these steps, we end up with 296,070 instances formed by two images and two captions, which we divided randomly into 80% for training and 20% for validation. Once again, we fine-tune a pretrained CLIP model, resulting on CLIP\({}_{}\) (training details in Appendix E).

### Results and analysis

Table 3 shows the results of our two proposed models, i.e. \(_{}\) and \(_{}\) compared to comparable contrastive models of the same size, both in SugarCrepe and BiVLC main metrics (leftmost four columns). As it can be observed, \(_{}\) outperforms all the other contrastive models by large margins for the main three metrics of BiVLC. This shows the effectiveness of our training process. Interestingly, the improvement comes mainly from text-to-image retrieval, which greatly benefits from training with hard negative images. However, \(_{}\) is still much better for image-to-text retrieval and its group score is well below human performance, which shows that further research is needed for achieving human-comparable performance for bidirectional VLC. As mentioned in Section 4, we were able to run VQAScore and the open version of CapPa on BiVLC (see Table 2). \(_{}\) compares favorably to Open CapPa by a large margin, and it is comparable to VQAScore-XL, even though being orders of magnitude smaller (3B vs 151M parameters), but is clearly outperformed by the largest VQAScore. Regarding the two directions, \(_{}\) is best on I2T but lags behind both VQAScore models in T2I. Given the large difference in size, it is not clear whether the model architecture of VQAScore is key for the high performance.

On the other hand, \(_{}\) is the best contrastive model for SugarCrepe, outperforming NegCLIP by 8 points and closing the gap with humans to 5 points. However, this strong performance does not reflect on BiVLC, where \(_{}\) is even worse than \(_{}\), NegCLIP and GNM. This highlights our previous finding about the lack of correlation between SugarCrepe and BiVLC results (Section 4).

To put it into the context of the state-of-the-art, the strongest models for SugarCrepe are VQAScore-XXL [Lin et al., 2024] (93.72), CapPa [Tschannen et al., 2024] (92.88) and GPT-4V [Achiam et al., 2023] (92.19)2. \(_{}\) is on par with VQAScore-XXL and outperforms CapPa and GPT-4V, showing it is very strong for image-to-text retrieval, despite its significantly smaller size.

The lower performance of \(_{}\) on SugarCrepe can be attributed to TROHN-Img containing 10 times less hard negative captions than TROHN-Text, and could improve with more instances.

Why does training with hard negative images help?We will now refer to the four finer metrics reported in the four rightmost columns in Table 3. The two multimodal systems trained with hard negative images, \(_{}\) and GNM, obtain a difference of around 9 points for the \(_{}\) metric, i.e. the text-to-image retrieval accuracy for the positive caption. The contrastive training with hard negative images promotes learning features that maximize the distance between the positive caption and the negative image. This is not guaranteed when only hard negative texts are used for training. We show a conceptual diagram to illustrate this difference in Figure 4, which explains the benefits of training with hard negative images.

    &  &  &  \\  & & **I2T** & **T2I** & **Group** & **I\({}_{}\)2T** & **I\({}_{}\)2T** & **T\({}_{}\)2I** & **T\({}_{}\)2I** \\  Random & 50.00 & 25.00 & 25.00 & 16.67 & 50.00 & 50.00 & 50.00 & 50.00 \\  CLIP & 76.56 & 75.83 & 52.40 & 49.06 & 84.32 & 89.50 & 69.21 & 82.82 \\ \(_{}\) & 84.66 & 82.75 & 63.89 & 60.96 & 89.06 & 91.75 & 72.79 & 90.86 \\ NegCLIP & 85.64 & 80.74 & 61.95 & 58.75 & 89.53 & 89.53 & 70.10 & **91.34** \\ GNM & 81.83 & 81.32 & 60.86 & 57.96 & 88.00 & 91.24 & 80.33 & 80.33 \\  \(_{}\) & **93.40** & 78.18 & 62.19 & 57.48 & **93.25** & 83.87 & 71.05 & 90.59 \\ \(_{}\) & 89.40 & **88.54** & **71.84** & **69.25** & 22.12 & **95.33** & **81.45** & 90.15 \\   

Table 3: Results for contrastive models of the same size on SugarCrepe and BiVLC, including our models (bottom rows). Bold for best, underline for second best. For BiVLC, we show the three main scores plus individual retrieval task scores: \(_{}\) refers to image-to-text retrieval with positive image, \(_{}\) for image-to-text retrieval with negative image, etc.

Which category is the most difficult?Table 4 shows results of all multimodal models in BiVLC according to caption generation categories. We find that, similar to SugarCrepe, the Swap category is the hardest one for all models also for BiVLC. The group scores for all models are more than 20 points below the scores for the Replace category. But contrary to SugarCrepe, we observe that the Add category is also challenging for models in BiVLC. It is surprising to see the low performance of Open CapPa for that category, since its performance in SugarCrepe is close to perfection. This highlights again the importance of language biases and evaluation protocols that mitigate the effects of those biases. In that sense, it is also interesting to see that CLIP\({}_{}\) is the best model for the Add category, outperforming VQAScore-XXL by almost 10 points, despite its significantly smaller size.

Why is CLIP\({}_{}\) still far from humans?The steps followed to create BiVLC showed that current text-to-image systems such as SDXL-DPO do not always generate images faithfully. In fact, the manual filtering process used to produce the test dataset (see Section 3) allows to estimate the noise rate in TROHN-Img to be around 61%, combining image generation failures and ambiguous instances. We think that this high noise rate in the training data prevents CLIP\({}_{}\) from learning better. This is reflected again in the T\({}_{}\)2I metric, which is the weakest retrieval case (the other three retrieval scores are over 90). We can conclude that the generation of hard negative images is a promising direction, where further advances would be possible, like an automatic and scalable procedure to remove incorrect instances from TROHN-Img, or, alternatively, new methods to produce cleaner training sets.

Are our models just distinguishing between synthetic and natural?Since the TROHN-Img dataset contains synthetic and natural image-caption pairs, the good performance of CLIP\({}_{}\) on BiVLC could be attributed to its capacity to distinguish between synthetic and natural images and

    &  &  &  \\
**Model** & **I2T** & **T2I** & **Group** & **I2T** & **T2I** & **Group** & **I2T** & **T2I** & **Group** \\  Random & 25.00 & 25.00 & 16.67 & 25.00 & 25.00 & 16.67 & 25.00 & 25.00 & 16.67 \\  CLIP & 82.09 & 60.36 & 57.27 & 46.52 & 16.16 & 13.65 & 70.32 & 44.63 & 39.58 \\ CLIP\({}_{}\) & 87.99 & 72.42 & 69.94 & 51.53 & 25.07 & 20.89 & 83.16 & 55.58 & 51.58 \\ NegCLIP & 86.37 & 70.80 & 68.03 & 47.08 & 24.23 & 18.66 & 81.26 & 51.37 & 48.00 \\ GNM & 86.14 & 68.70 & 65.75 & 50.70 & 17.83 & 15.88 & 83.16 & 58.74 & 55.37 \\  CLIP\({}_{}\) & 83.61 & 70.03 & 65.84 & 54.32 & 25.35 & 20.61 & 72.21 & 55.37 & 48.42 \\ CLIP\({}_{}\) & **91.62** & 79.18 & 76.61 & 62.40 & 32.03 & 27.86 & **94.74** & **69.47** & **68.00** \\  Open CapPa & 70.32 & 62.79 & 53.03 & 40.95 & 30.64 & 20.61 & 14.74 & 46.32 & 9.263 \\ VQAScore-XL & 85.95 & 84.18 & 78.28 & 65.18 & 58.77 & 48.75 & 77.05 & 56.63 & 50.74 \\ VQAScore-XXL & 89.85 & **88.33** & **83.85** & **72.14** & **66.30** & **56.82** & 80.42 & 65.47 & 58.74 \\   

Table 4: Results of multimodal models for BiVLC, divided into Replace, Swap and Add categories. Bold for best, underline for second best.

Figure 4: When we train only with hard negative texts, the distance of the positive caption (Caption+) and the negative image (Image-) may be even smaller than the distance of the positive caption to the positive image (Image+) (left). When we add hard negative images, we force to increase the distance between the positive caption and the negative image, while minimizing the distance between the positive caption and image (right).

captions, rather than to VLC skills. To analyse this, we develop two new systems which are trained to detect synthetic and natural images and captions. CLIP\({}_{}\) is composed by the original pretrained CLIP visual and text encoders, where a binary classification head is added to each encoder. Both binary classifiers are trained separately for synthetic image or caption detection reusing TROHN-Img training images and texts (see Appendix E for details). CLIP\({}_{}\), is built similarly, but using the visual and text encoders of our CLIP\({}_{}\) model.

The two detection columns in Table 5 show the performance of our methods when detecting synthetic text and images at BiVLC instances, with a perfect performance for synthetic image detection, i.e. both systems are able to detect which images in BiVLC are synthetic. The accuracy for text detection is lower, with CLIP\({}_{}\) obtaining the highest accuracy with 61.34.

We can also evaluate the detectors on the BiVLC retrieval tasks: (i) for image-to-text retrieval, we first use the visual encoder to predict the type of image (natural or synthetic); if the detection is correct, the output of the text detector is used to see which of the captions has the highest probability to be of the same type as the image, and we select it as the matching caption; (ii) for text-to-image retrieval, we follow the same procedure, but using first the text detector over the given caption, and afterwards the visual detector for the two candidate images. The rightmost columns in Figure 5 show that BiVLC group score is very low for both detectors, underperforming clearly the multimodal models (cf. Table 3). This means that our models are actually learning much more than just distinguishing synthetic and natural images-captions. We already had another evidence of that in Table 3, where all models do equally well on I\({}_{}\)2T and I\({}_{}\)2T, i.e. they work equally well for both natural and synthetic images.

However, we also observe that the I2T score is significantly higher than random, scoring 75.04 in the best case. This result suggests that image-to-text retrieval is more sensitive to natural/synthetic bias, so we also checked the results of our detectors for SugarCrepe. Surprisingly, we obtained an accuracy of 74.59 for CLIP\({}_{}\) and 77.65 for CLIP\({}_{}\). Those results are actually in par with the original CLIP using similarity between the image and both captions (see Table 3). This is yet another reason to consider bidirectional datasets when evaluating compositionality.

## 6 Conclusions

We have presented BiVLC, a dataset for bidirectional vision-language compositionality. Our dataset offers the possibility of evaluating the compositional representation of multimodal models in both retrieval directions: image-to-text and text-to-image. We have shown that including text-to-image retrieval leads to novel findings, such as the unbalanced performance of multimodal models in the two retrieval directions, in contrast to humans. We also uncovered the low performance for BiVLC of the models which have been specifically trained for image-to-text retrieval. Furthermore, we have proposed a new way to train models with hard negative images, resulting on the best contrastive model for BiVLC so far. The gap with human performance shows that BiVLC can be used to measure further advances in modelling VLC.

As for future work, we would like to better analyse why text-to-image retrieval is harder than the other direction. We also plan to explore novel strategies to automatically generate and filter hard negative images for training, with the objective of improving the performance of multimodal systems in modelling compositionality in vision-language scenarios.

  
**Model** & **Text detection acc** & **Img detection acc** & **I2T** & **T2I** & **Group** \\  Random & 50.00 & 50.00 & 25.00 & 25.00 & 16.67 \\  CLIP\({}_{}\) & 57.00 & 100.00 & 66.69 & 19.64 & 19.64 \\ CLIP\({}_{}\) & 61.34 & 100.00 & 75.04 & 26.42 & 26.42 \\   

Table 5: Results on BiVLC for synthetic vs. natural image and text detection-based systems. We show text and image detection accuracies, as well as the scores on the three main evaluation metrics.