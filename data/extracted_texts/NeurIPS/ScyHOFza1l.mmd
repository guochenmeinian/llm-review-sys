# Simulating Message Passing via Spiking Neural Networks Using Logical Gates

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

It is hypothesized that the brain functions as a Bayesian inference engine, continuously updating its beliefs based on sensory input and prior knowledge. Message passing is an effective method for performing Bayesian inference within graphical models. In this paper, we propose that the XOR and the Equality factor nodes, which are important components in binary message passing, can be realized through a series of logical operations within a spiking neural network framework. Spiking neural networks simulate the behavior of neurons in a more biologically plausible manner. By constructing these factor nodes with a series of logical operations, we achieve the desired results using a minimal number of neurons and synaptic connections, potentially advancing the development of biological neuron-based computation. We validate our approach with two experiments, demonstrating the alignment between our proposed network and the sum-product message-passing algorithm.

## 1 Introduction

The Bayesian brain hypothesis proposes that the brain functions as a Bayesian inference machine, continuously updating its beliefs about the world by integrating sensory input with prior knowledge (1). This concept aligns with the Free Energy Principle, which asserts that biological systems must minimize uncertainty by using internal models to predict and adapt to environmental changes (3). Experimental evidence supporting these principles can be observed in studies involving in-vitro biological neurons cultured on Micro-Electrode Arrays (MEA). For instance, research by (8) highlights the computational capabilities of cultured neurons in a simulated game of Pong, while (7) investigates their application in blind source separation.

A widely used technique for probabilistic inference under uncertainty is the sum-product message passing on a factor graph, also known as belief propagation. This method offers a structured approach for efficiently performing Bayesian inference in graphical models by exchanging information between factor nodes to update beliefs (12).

Spiking Neural Networks (SNNs) are a class of artificial neural networks that emulate the behavior of biological neurons by using discrete spikes or action potentials for communication. SNNs encode information in the precise timing of these spikes (13). The biologically inspired nature of SNNs provides a more accurate simulation of neural processes, making them significant for advancements in computational neuroscience. In (10), the utility of SNNs is demonstrated by showing how Spike-Timing-Dependent Plasticity (STDP) can be leveraged to enable a self-learning spiking network to control a mobile robot. This study highlights the potential of SNNs in practical applications, where their ability to learn and adapt can enhance robotic control and autonomous decision-makingIn this paper, we propose SNN-based factor nodes for simulating Bernoulli message passing using logical nodes. We demonstrate that the proposed network, constructed with a minimal number of neurons and synaptic connections, yields results closely aligned with numerical sum-product rules. This approach represents a potential step toward exploring the use of biological neurons as computational functions.

## 2 Background

### Sum-Product Message Passing on Forney-style Factor Graphs

The belief propagation or sum-product message passing algorithm, applied to Forney-style Factor Graphs (FFGs), is a powerful method for performing Bayesian inference in probabilistic models (12). In an FFG, edges represent random variables, and nodes represent factors in a probabilistic model. An edge connects to a node if the variable on that edge is an argument of the node's function. The sum-product algorithm works by iteratively passing "messages" along the edges of the graph. We denote the forward and backward messages using the notations \(()\) and \(()\), respectively. In general, for any node \(f(y,x_{1},,x_{n})\), the sum-product rule for an outgoing message over edge \(y\) is given by

\[(y)}_{}=_{x_{1}, ,x_{n}}(x_{1})...(x _{n})}_{},...,x_{n})}_{}\,.\] (1)

This algorithm is particularly scalable because it exploits the graph's structure to compute locally, significantly reducing the complexity compared to naive approaches that require summing over all possible configurations globally. The local and distributed nature of the sum-product algorithm allows it to handle large-scale problems efficiently, making it widely applicable. For example, in multi-agent trajectory planning (2) and active inference in complex environments (9).

### Leaky Integrate-and-Fire Neurons

SNNs encompass several biophysical models that describe how neurons generate spikes to a varying degree of realism. Among these, the Leaky Integrate-and-Fire (LIF) model is one of the most widely used, and it is the one we employ here. In the LIF model, a neuron's membrane potential accumulates with incoming synaptic inputs over time, which can be either excitatory or inhibitory. The membrane potential also 'leaks' over time, gradually returning to the resting state in the absence of input, reflecting the neuron's inherent electrical properties. When the membrane potential reaches a certain threshold, the neuron 'fires' an action potential or spike, which propagates down the axon to stimulate other neurons.

## 3 Simulations

In this section, we start by implementing logical node diagrams using a small number of LIF neurons. We then utilize these diagrams to carry out message passing according to the sum-product rule with Bernoulli messages. Finally, we apply this method to a specific example and compare the results.

To simulate neural activities we utilized the Brian toolbox (4). We set the threshold for neuron firing at \(1.0\), the resting state at \(0.0\), and we assumed \(dv/dt=-v/\) for \(=1.0\) ms for all neurons.

### Logical Gates

We design the neurons' synaptic connections so as to achieve the output described in the truth table 1 for each logical gate. This process is more straightforward for the AND, OR, and NOT gates, as we can determine appropriate synaptic weights to achieve the desired output. The proposed SNN gates are illustrated in figure 1. We restricted the input spikes to every 10 ms. A sample set of inputs and their corresponding outputs are illustrated in the figure 1. \(x_{1}\) spikes at times \(\{10,20,40,50,70\}\), and \(x_{2}\) spikes at times \(\{0,20,40,60\}\). The outputs of each gate match the expected results.

The XOR gate is more complex to generate, meaning it cannot be implemented solely by adjusting synaptic weights, as was done for the gates in Fig 1. However, it can be constructed by combining the implemented gates, as outlined in Fig 2.

### Message Passing

We consider \((x_{1})=er(x_{1}|p_{1})\) and \((x_{2})=er(x_{2}|p_{2})\) as the input messages, we sample from these distributions every 10 ms and use them as input spikes. For the XOR gate, we define the output message \((y)=er(y|)\), where \(spike-count\) is the total number of the output spikes according to the 2, and the \(spike-time\) is the total times we sample from the input messages. For example, if we run the simulation for \(10000\) ms and sample each 10 ms from the inputs, the \(spike-time\) is \(1000\) times.

For the Equality node, it is more complicated. As we can see in the truth table, the output spikes of this node are similar to the AND gate, but the combinations of unequal inputs are not defined for this operation. So we can use the AND output for the \(spike-count\) and consider just the number of equal inputs for \(spike-time\). This can be achieved by connecting the inputs to an XOR gate (which gives us the number of unequal inputs) followed by a NOT gate.

As calculated in the Appendix, the sum-product message according to Equation (1) follows

\[(y) =er(y\,|\,p_{1}-2p_{1}p_{2}+p_{2})\] (2) \[(y) =er(y\,|\,p_{2}}{1-p_{1}+2p_{1}p_{2}-p_{2}})\] (3)

for the XOR and Equality nodes respectively. As shown in Fig. 3, the proposed SNNs yield results that closely match the numerical solution according to (2), and (3).

### An example

We can now apply our proposed SNN-based message passing to solve an example. We used the problem introduced in (11). Consider the FFG depicted in Figure 4, which represents the binary code \(C=\{(0,0,0,0),(0,1,1,1),(1,0,1,1),(1,1,0,0)\}\). In this example, the messages \(((X_{1}),(X_{2}), (X_{3}),(X_{4}))\), indicated by orange arrows on the graph, serve as the inputs. Table 1 compares the other messages passed along the edges with the results obtained from our SNN-based approach, demonstrating that our method closely matches the expected outcomes.

### Conclusion

In this paper we implemented SNN-based message passing with Bernoulli messages on XOR and Equality factor nodes using logical gates. This result also aligns with the model proposed in (15), where continuous sum-product message passing was implemented using a Liquid State Machine (14). While their approach involved hundreds of synaptic weights, our goal in this paper is to implement the process as simply as possible. This simplification is crucial for our next step, which focuses on using self-learning STDP algorithms like those described in (5) and (6), to set synaptic weights in a way that is more biologically plausible. The ultimate aim is to implement the entire system on an MEA chip, paving the way for the development of a hybrid bio-computational chip.

  Message & Ground truth (11) & Our result \\  \((Y)\) & \(er(0.18)\) & \(er(0.174)\) \\ \((Y)\) & \(er(0.5)\) & \(er(0.488)\) \\ \((X_{1})\) & \(er(0.5)\) & \(er(0.526)\) \\ \((X_{2})\) & \(er(0.5)\) & \(er(0.526)\) \\ \((X_{3})\) & \(er(0.024)\) & \(er(0.022)\) \\ \((X_{4})\) & \(er(0.664)\) & \(er(0.657)\) \\  

Table 1: Comparison of the ground truth messages in the example with the results obtained from our SNN-based factor nodes.

Figure 4: The FFG corresponding to the discussed example. Input messages are highlighted with orange arrows.

Figure 3: Comparison of results from the proposed SNN nodes for passing Bernoulli messages. \(X_{1}\) and \(X_{2}\) represent randomly selected parameters for the input Bernoulli messages. Each pair of inputs was sampled every 10 ms over a period of 100 seconds. The firing rate of the SNN gates was calculated as described in the text and is compared with those obtained using the sum-product formula.

## Appendices

We can derive the forward sum-product message \((y)\) according to the XOR factor node, given Bernoulli input messages \(x_{1}\) and \(x_{2}\), as

\[(y) =_{x_{1}}_{x_{2}}(x_{1})\ (x_{2})\ f(y,x_{1},x_{2})\] \[=_{x_{1}}_{x_{2}}(x_{1}|p_{1})\,(x_{2}|p_{2})\ f(y,x_{1},x_{2})\] \[=(0|p_{1})\,(0|p_{2})f(y,0,0)+(1|p_{1})\,(0|p_{2})f(y,1,0)\] \[+(0|p_{1})\,(1|p_{2})f(y,0,1)+ (1|p_{1})\,(1|p_{2})f(y,1,1)\] \[=(1-p_{1})(1-p_{2})f(y,0,0)+p_{1}(1-p_{2})f(y,1,0)\] \[+(1-p_{1})p_{2}f(y,0,1)+p_{1}p_{2}f(y,1,1)\,.\]

Using the truth table, we can substitute \(y\) and evaluate the terms, such that

\[(y) =p_{1}(1-p_{2})+(1-p_{1})p_{2}&y=1\\ (1-p_{1})(1-p_{2})+p_{1}p_{2}&y=0=p_{1}-2p_{1}p_{2}+p_ {2}&y=1\\ 1-p_{1}+2p_{1}p_{2}-p_{2}&y=0\] \[=(y|p_{1}-2p_{1}p_{2}+p_{2})\,.\]

Since the XOR factor is symmetric according to their truth table forward and backward messages are equal. For instance, if we want the backward message \((x_{1})\) given \((x_{2})\) and \((y)\), it is:

\[(x_{1})=(x_{1}|p_{2}-2p_{y}p_{2}+p_{y})\,.\]

So we don't need any other gate for computing the backward messages and we can use the proposed gate but with the backward messages as the inputs.

Similarly, we have the following computations for the equality factor node:

\[(y) =p_{1}p_{2}&y=1\\ (1-p_{1})(1-p_{2})&y=0\] \[=(y|p_{2}}{1-p_{1}+2p_{1}p_{2}-p_{2}})\,.\]