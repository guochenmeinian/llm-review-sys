# Practical Bayesian Algorithm Execution via

Posterior Sampling

 Chu Xin Cheng

California Institute of Technology

ccheng2@caltech.edu

&Raul Astudillo

California Institute of Technology

rastudil@caltech.edu

&Thomas Desautels

Lawrence Livermore National Laboratory

desautels2@llnl.gov

&Yisong Yue

California Institute of Technology

yyue@caltech.edu

Equal contribution.

###### Abstract

We consider Bayesian algorithm execution (BAX), a framework for efficiently selecting evaluation points of an expensive function to infer a property of interest encoded as the output of a base algorithm. Since the base algorithm typically requires more evaluations than are feasible, it cannot be directly applied. Instead, BAX methods sequentially select evaluation points using a probabilistic numerical approach. Current BAX methods use expected information gain to guide this selection. However, this approach is computationally intensive. Observing that, in many tasks, the property of interest corresponds to a target set of points defined by the function, we introduce _PS-BAX_, a simple, effective, and scalable BAX method based on posterior sampling. PS-BAX is applicable to a wide range of problems, including many optimization variants and level set estimation. Experiments across diverse tasks demonstrate that PS-BAX performs competitively with existing baselines while being significantly faster, simpler to implement, and easily parallelizable, setting a strong baseline for future research. Additionally, we establish conditions under which PS-BAX is asymptotically convergent, offering new insights into posterior sampling as an algorithm design paradigm.

## 1 Introduction

Many real-world problems can be cast as estimating a property of a black-box function with expensive evaluations. Bayesian optimization (BO)  has focused on the case where the property of interest is the function's global optimum. Similarly, level set estimation  deals with the problem of estimating the subset of points above (or below) a user-specified threshold.

In many cases, an algorithm to compute the property of interest is available, which we refer to as the _base algorithm_. However, this algorithm typically requires more evaluations than are feasible in practice and cannot be used directly. Instead, evaluation points must be carefully selected through other means. Similar to BO and level set estimation, the Bayesian algorithm execution (BAX) framework selects evaluation points using two key components: (1) a Bayesian probabilistic model of the function and (2) a sequential sampling criterion that leverages this model to choose new points for evaluation .

Existing approaches to BAX rely on expected information gain (EIG) as the criterion for selecting which points to evaluate . However, maximizing the EIG presents a significant computationalchallenge, particularly in high-dimensional problems or when the property of interest is complex. As a result, heuristic approximations are frequently employed, which can lead to suboptimal performance and limit the applicability of BAX in real-world scenarios.

To address this challenge, we propose _PS-BAX_, a simple yet effective and scalable approach based on posterior sampling. Our approach is built upon the key observation that many real-world BAX tasks aim to find a _target set_. For instance, in BO, the goal is to locate the function's global optimum, while in level set estimation, the objective is to find the points whose function value is above a specified threshold. PS-BAX only requires a single base algorithm execution at each iteration, making it much faster than EIG-based approaches, which require executing the base algorithm multiple times and optimizing over the input space. Despite its simple computation, we show that PS-BAX is competitive with existing baselines while being significantly faster. Additionally, we show that it enjoys appealing theoretical guarantees. Specifically, we prove that PS-BAX is asymptotically convergent mild regularity conditions.

Our contributions are summarized as follows:

* We derive PS-BAX, a posterior sampling-based BAX method applicable to a broad class of BAX problems, unlocking new applications and offering a fresh perspective on the scope of posterior sampling algorithms.
* We show that PS-BAX is orders of magnitude faster to compute than the EIG-based approach INFO-BAX  while remaining competitive with this and other specialized algorithms.
* We prove that PS-BAX is asymptotically convergent under mild regularity conditions.

## 2 Related Work

Our work falls within the broader field of probabilistic numerics , which frames numerical problems, such as optimization or integration, as probabilistic inference tasks. This probabilistic perspective enables uncertainty quantification, which is particularly important in settings with limited computational budgets, where budget allocation must be carefully planned, often adaptively. While much of the recent work in probabilistic numerics has focused on (Bayesian) optimization [1; 5], there have also been efforts in other areas, including integration (Bayesian quadrature) [6; 7; 8], level set estimation [2; 9], and solving differential equations [10; 11].

Recently,  proposed INFO-BAX, an approach to estimate an arbitrary property of interest that could be computed by a known base algorithm. Since the base algorithm requires a potentially large number of function evaluations, it cannot be applied directly. Instead, following the probabilistic numerics paradigm, a Bayesian probabilistic model of the function is used to iteratively select new points to evaluate. At each iteration, the next evaluation point is chosen by maximizing the expected information gain (EIG) between the function's value at the point and the property of interest. Similar EIG-based approaches have been employed in statistical design of experiments [12; 13; 14] and BO [15; 16; 17], often yielding excellent performance. However, these methods are computationally demanding due to the look-ahead nature of the EIG computation. Moreover, in most cases, the EIG cannot be computed in closed form and must be approximated via Monte Carlo sampling. As a result, EIG-based approaches are mainly useful in low-dimensional settings or when function evaluations are highly expensive, limiting their applicability in real-world problems.

In response to the limitations of EIG-based approaches, we explore an alternative family of strategies known as posterior sampling or Thompson sampling [18; 19]. Posterior sampling algorithms have been widely used in BO [20; 21; 22], multi-armed bandits [23; 24; 25], and reinforcement learning [26; 27; 28]. In such settings, these approaches select a point at each iteration according to the posterior probability of being the optimum. To our knowledge, our work represents the first extension of posterior sampling beyond optimization settings, offering fresh insights into this algorithmic family. While the range of problems our approach can address is narrower than those that EIG-based methods can conceptually tackle, it still encompasses a substantial class. Notably, this includes the problems explored empirically by  and follow-up work , among others.

Our work aligns with recent efforts to broaden the applicability of BO to complex real-world problems. Many such problems deviate from classical optimization formulations, exhibiting diverse structures such as combinatorial , robust [30; 31], or multi-level optimization . Traditional BO algorithms often fail to naturally accommodate these structures, limiting their practical utility. Weintroduce a straightforward algorithm applicable to these diverse settings, providing a robust baseline for future exploration. Furthermore, our approach benefits from recent advances in probabilistic modeling tools [33; 34; 35], paving the way for applying these tools to a broader range of problems.

## 3 Bayesian Algorithm Execution via Posterior Sampling

Problem SettingOur work takes place within the Bayesian algorithm execution (BAX) framework introduced by . The goal is to estimate \(_{}(f)\), the output of a _base algorithm_\(\) applied to a function \(f:\). We assume that \(f\) is expensive to evaluate, which means that employing \(\) directly on \(f\) is infeasible (as it would require evaluating \(f\) too many times). Instead, we select the points at which \(f\) is evaluated sequentially, aided by a probabilistic model described below. We specifically focus on problems where the property of interest can be encoded by a set \(_{}(f)\), which we term the _target set_. As we shall see later, our framework encompasses a wide range of problems, including BO2, level-set estimation, shortest-path finding on graphs, and top-\(k\) estimation, with applications to topographic estimation and drug discovery.

Probabilistic ModelSimilar to many probabilistic numerical methods, our algorithm relies on a probabilistic model encoded by a prior distribution over \(f\), which we denote by \(p\). Although our framework is more general and can be used with other priors, we assume for concreteness that \(f\) follows a Gaussian process (GP) prior . Let \(_{n-1}=\{(x_{k},y_{k})\}_{k=1}^{n-1}\) denote the data collected after \(n-1\) evaluations of \(f\). We assume these evaluations are corrupted with i.i.d. Gaussian noise, i.e., \(y_{k}=f(x_{k})+_{k}\), where \(_{1},,_{n-1}\) are i.i.d. with common distribution \((0,^{2})\), and \(^{2}\) is a non-negative scalar. Under these assumptions, the posterior distribution over \(f\) given \(_{n-1}\), denoted by \(p(f_{n-1})\), is again a GP whose mean and covariance functions can be computed in closed form using the classical GP regression equations.

INFO-BAX and its ShortcomingsBefore introducing our algorithm, we briefly comment on prior work based on the expected information gain (EIG) . Succinctly, the INFO-BAX approach proposed by  selects at each iteration the point that maximizes the expected entropy reduction between the function's value at the evaluated point and \(_{}(f)\). Evaluating an expectation is generally difficult, and one often resorts to Monte Carlo sampling. Moreover, computing the EIG specifically requires expensive calculations of conditional posterior distributions and entropy. These computational issues are also present in similar information-theoretic acquisition functions proposed in the classic BO setting. However, for BAX tasks, the computation burden of EIG can be much more pronounced if \(|_{}(f)|\) is large. This occurs, for example, in the level set estimation setting, where \(_{}(f)\) can be comprised of a large number of points. We defer a more detailed discussion of the computation of the EIG to Appendices A and B.

Ps-BaxTo overcome the computational limitations of EIG-based approaches, we introduce a simple strategy based on posterior sampling, which we term PS-BAX. For ease of exposition, we only describe the fully sequential version of our algorithm and defer the batched (parallelized) version to Appendix C. Our algorithm is summarized in Algorithm 1. PS-BAX is comprised of two steps, detailed below.

1. We sample a target set \(X_{n}\) according to the posterior probability that \(X_{n}=_{}(f)\) (lines 2-3 in Algorithm 1). This can be achieved by drawing a sample from the posterior over \(f\), denoted by \(_{n}\) (line 2), and then setting \(X_{n}=_{}(_{n})\) (line 3).
2. We select the point in the sampled target set \(X_{n}\) with maximal uncertainty or entropy: \(x_{n}_{x X_{n}}[f(x)|_{n}]\) (line 4 in Algorithm 1). For a Gaussian posterior, \(x_{n}\) can be equivalently selected using the maximal posterior standard deviation: \(x_{n}_{x X_{n}}_{n}(x)\), where \(_{n}(x)\) is the posterior standard deviation of \(f(x)\).

Note that the second step is unnecessary in standard BO, since \(X_{n}\) is typically a singleton.

Depiction for Level Set EstimationFigure 1 depicts an iteration of PS-BAX for the level-set estimation problem, where \(_{}(f):=\{x f(x)>\}\), for a user-specified value of \(\). Line 3 of Algorithm 1 returns a target set \(X_{n}\) based on where the sampled function \(_{n}\) (green line in Figure 1) is above the threshold \(\) (green region in Figure 1). Line 4 of Algorithm 1 then chooses the point \(x_{n} X_{n}\) that has maximal uncertainty (red line in Figure 1). In the standard BO setting where \(\) is computing the maximizer of \(f\), the target region \(X_{n}\) is simply a singleton point where the sampled \(\) has highest value (and Line 4 in Algorithm 1 is not necessary).

DiscussionWe now provide an intuitive explanation of why one might expect PS-BAX to perform well. In the standard BO setting, posterior sampling is known to deliver excellent performance  and enjoys strong theoretical guarantees . Like in the BO setting, the intrinsic goal of posterior sampling in our setting is to balance exploration and exploitation. In our case, this means selecting points for which, according to our probabilistic model, membership in the target set is still highly uncertain among the likely candidates. To achieve this, the first step of PS-BAX selects a random set \(X_{n}\), according to the probability of this set being the target set, in the same vein as traditional posterior sampling in the BO setting (the set of likely candidates). Unlike in BO, however, the target set is, in principle, comprised of several points, and thus, we must come up with a criterion to choose one. To overcome this, the second step simply selects the point with the highest _uncertainty_ among points in \(X_{n}\), which is a standard strategy in the active learning literature .

Computational EfficiencyPS-BAX requires running \(\) only once on a single sample of \(f\), contributing to its practicality and scalability. Furthermore, similar to posterior sampling in the standard

Figure 1: Depiction of PS-BAX (Algorithm 1) for the level-set estimation problem. We plot the objective function \(f\) (black line), the current available data \(_{n-1}\) (black points), the threshold (grey dashed line), the posterior distribution \(p(f_{n-1})\) (blue line and light blue region), a sample from the posterior \(_{n} p(f_{n-1})\) (green line), the corresponding sampled target set \(X_{n}=_{}(_{n})\) (green region) (this is the set of inputs where the green line is above the threshold), the variance of \(p(f_{n-1})\) (green line, bottom row), and the next point to evaluate selected by PS-BAX \(x_{n} X_{n}\) (input marked by the vertical red line). The key step is computing the target set \(X_{n}\) using the sampled function \(_{n}\), which generalizes posterior sampling for standard BO.

BO setting, PS-BAX avoids the need to maximize an acquisition function over \(\), a process that is computationally expensive because it involves calculating the expected value of quantities like information gain. As demonstrated in our experiments, this makes PS-BAX significantly faster than INFO-BAX , particularly in problems where either \(_{}(f)\) or \(\) are large.

Convergence of PS-BAXA natural question is under which conditions is PS-BAX able to _find_ the target set given enough evaluations. We address this question below. Before stating our results, we introduce a definition related to the characterization of problems where PS-BAX converges.

**Definition 1**.: _A target set estimated by an algorithm \(\) is said to be complement-independent if, for any pair of functions \(f,f^{}:\), it holds that \(_{}(f)=_{}(f^{})\) whenever there exists a set \(S\) such that \(_{}(f)_{}(f^{}) S\) and \(f(x)=f^{}(x)\) for all \(x S\)._

Many target sets of interest, such as a function's optimum or level set, are complement-independent. Indeed, the value of \(f\) at points that are not the optimum or that do not lie in the level of interest do not influence these properties. Theorem 1 below shows that PS-BAX enjoys Bayesian posterior concentration, provided the target set of interest is complement-independent. Intuitively, this result means that if \(f\) is drawn from the prior used by our algorithm (i.e., the prior is well-specified), then, with probability one, the posterior will concentrate around the true target set. Corollary 1 gives an asymptotically consistent estimator of the target set. Finally, we also show there are problems where the target set is not complement-independent and PS-BAX is not asymptotically consistent in Theorem 2. The proofs of these results can be found in Appendix D.

**Theorem 1**.: _Suppose that \(\) is finite and that the target set estimated by \(\) is complement-independent. If the sequence of points \(\{x_{n}\}_{n=1}^{}\) is chosen according to the PS-BAX algorithm, then, for each \(X\), \(_{n}_{n}(_{}(f)=X)=\{ _{}(f)=X\}\) almost surely for \(f\) drawn from the prior._

**Corollary 1**.: _Suppose the assumptions of Theorem 1 hold, and let \(T_{n}_{X}_{n}(_{ }(f)=X)\). Then, for \(f\) drawn from the prior, we have \(T_{n}=_{}(f)\) for all sufficiently large \(n\) almost surely._

**Theorem 2**.: _There exists a problem instance (i.e., \(\), a Bayesian prior over \(f\), and \(\)) such that if the sequence of points \(\{x_{n}\}_{n=1}^{}\) is chosen according to the PS-BAX algorithm, then there is a set \(X\) such that \(_{n}_{n}(_{}(f)=X)=1/2\) almost surely for \(f\) drawn from the prior._

## 4 Numerical Experiments

We evaluate the performance of PS-BAX on eight problems across four problem classes. For each problem class, we specify the base algorithm used. We compare the performance of PS-BAX against INFO-BAX  and uniform random sampling over \(\) (Random). When available, we also include an algorithm from the literature specifically designed for the problem class. Additional implementation details of the algorithms are described in Appendix E. In all experiments, an initial dataset is generated by sampling \(2(d+1)\) inputs uniformly at random from \(\), where \(d\) denotes the dimensionality of \(\). Following this initialization, each algorithm sequentially selects additional batches of points. Unless stated otherwise, the batch size is set to \(q=1\). The performance of each algorithm is determined by applying \(\) on \(_{n}\), the posterior mean of \(f\) given \(_{n}\) and subsequently computing a suitable performance metric on \(_{}(_{n})\). Each experiment was replicated 30 times, with plots showing mean performance plus and minus 1.96 standard errors. Code to reproduce our experiments is available at https://github.com/RaulAstudillo06/PSBAX.

Summary of FindingsOverall, we find that PS-BAX is always competitive with and sometimes significantly outperforms INFO-BAX across all of our experiments. Additionally, as shown in Table 1, PS-BAX can be orders of magnitude faster in wall-clock runtime. PS-BAX outperforms INFO-BAX on five out of eight problems, offering particularly large improvements in the Local Optimization and DiscoBAX problem classes. Moreover, in the Local Optimization and Level Set Estimation problems, PS-BAX also outperforms algorithms from the literature specifically designed for such problem classes. On the simpler problems, such as those in the Top-\(k\) problem class, PS-BAX is competitive with INFO-BAX while still being significantly faster.

### Local Optimization

We explore the performance of our algorithm in the local optimization setting, where \(\) is a classic optimization algorithm, i.e., an algorithm designed for optimization problems where \(f\) (and potentially its gradients) can be evaluated at a large number of points. Examples of such algorithms include evolutionary algorithms , trust-region methods , and many gradient-based optimization algorithms [41; 42]. This setting reduces to the classical BO setting if \(\) can recover the global optimum of \(f\). In such case, the INFO-BAX reduces to the classical predictive entropy search acquisition function  when computed exactly and to the joint entropy search acquisition function  under the approximation proposed by  that we use in our experiments. PS-BAX, in turn, reduces to the classical posterior sampling strategy used in BO . However, due to its practical relevance and the lack of an empirical comparison between joint entropy search and posterior sampling, we still include this setting in our experiments. We also use this setting to illustrate nuances that arise when choosing a base algorithm.

In our experiments, we use a gradient-based optimization method as a base algorithm instead of an evolutionary algorithm as pursued by . Gradient-based methods typically exhibit faster convergence than their gradient-free counterparts. However, they are often infeasible if gradients cannot be obtained analytically and instead are obtained, e.g., via finite differences. Since in most applications, analytic gradients of \(f\) are unavailable, directly applying such methods on \(f\) is infeasible. However, PS-BAX and INFO-BAX can make use of gradient-based methods thanks to the availability of gradients of most probabilistic models used in practice, including GPs.

We consider the Hartmann and Ackley functions, with input dimensions of 6 and 10, respectively, as test functions. Both functions have many local minima and are standard test functions in the BO literature. For Ackley, we set the batch size to \(q=2\). As a performance metric, we report the log10 inference regret, given by \(_{10}(f^{*}-f(_{n}^{*}))\), where \(_{n}^{*}\) is obtained by applying \(\) on \(_{n}\). The results of these experiments are depicted in Figure 2. As a baseline, we also include the

   Problem & PS-BAX Runtime (s) & INFO-BAX Runtime (s) \\  Local Optimization: Hartmann (6D) & 0.37 & 7.64 \\ Local Optimization: Ackley (10D) & 3.36 & 29.31 \\ Level Set Estimation: Himmelblau & 0.57 & 14.97 \\ Level Set Estimation: Volcano & 0.49 & 289.91 \\ Top-\(k\): Rosenbrock (\(k=6\)) & 0.92 & 18.31 \\ Top-\(k\): GB1 (\(k=10\)) & 145.23 & 865.85 \\ DiscoBAX: Tau Protein Assay & 3.78 & 113.20 \\ DiscoBAX: Interferon-Gamma Assay & 3.95 & 97.03 \\   

Table 1: Average runtimes per iteration of PS-BAX and INFO-BAX across our test problems. In all of them, PS-BAX is between one and three orders of magnitude faster than INFO-BAX. We also note that the runtimes for both algorithms are significantly longer on the Top-10 GB1 problem due to the use of a deep kernel GP model.

Figure 2: Results for Local Optimization, showing the log10 inference regret achieved by the compared algorithms (lower values indicate better performance). The left and right panels present results for the Hartmann-6D and Ackley-10D functions, respectively. On Hartmann-6D, PS-BAX and EI perform comparably, both outperforming INFO-BAX. On Ackley-10D, PS-BAX achieves significantly better results than the rest of the algorithms.

expected improvement (EI), arguably the most popular BO acquisition function. On Hartmann-6D, PS-BAX performs on pair with EI, and both algorithms outperform INFO-BAX. Notably, PS-BAX outperforms both INFO-BAX and EI on Ackley-10D.

### Level Set Estimation

Level set estimation involves finding all points in \(\) with \(f(x)>\), for a user-specified threshold value \(\). This task arises in applications such as environmental monitoring, where a mobile sensing device detects regions with dangerous pollution levels , and topographic mapping, where the goal is to infer the portion of a geographic area above a specified altitude using limited measurements . For both problems considered in our work, \(\) is finite; therefore, the base algorithm \(\) simply ranks all objective values and returns the points where the function value exceeds the threshold.

We evaluate the algorithms on a synthetic problem (the 2-dimensional Himmelblau function) and a real-world topographic dataset, consisting of 87 x 61 height measurements from a large geographic area around Auckland's Maunga Whau volcano . The threshold \(\) is set to the 0.55 quantile of all function values in the domain for both problems. An illustration of single runs on the topographic problem over 100 iterations for both INFO-BAX and PS-BAX is shown in Figure 4.

The performance metric used is the F1 score, defined by

\[F1=,\] (1)

where \(TP\), \(FP\), and \(FN\) represent true positives, false positives, and false negatives, respectively. The results of this experiment are shown in Figure 3. As an additional baseline specifically designed for level set estimation, we include the popular LSE algorithm proposed by . PS-BAX demonstrates strong performance, outperforming all benchmarks in the topographic mapping problem.

### Top-\(k\) Estimation

We consider the top-\(k\) estimation setting, where \(\) is a finite (but potentially large) set, and the goal is to identify the \(k\) points with the largest values of \(f(x)\). In this scenario, the base algorithm evaluates \(f\) at all points in \(\) and returns the \(k\) best points. Following , we use as performance metric the Jaccard distance between the estimated output \(S_{n}=_{}(_{n})\) and the ground truth optimal set \(S^{*}\), defined by

\[d(S_{n},S^{*})=1- S^{*}|}{|S_{n} S^{*}|}.\] (2)

We consider two test problems. The first problem uses 3-dimensional Rosenbrock function, a standard benchmark in the optimization literature. The input space is obtained by taking a uniform grid of 1,000 points over \([-2,2]^{3}\). For this problem we set \(k=4\).

The second problem is a real-world top-\(k\) (\(k=10\)) selection task in protein design, where the goal is to maximize stability fitness predictions for the Guanine nucleotide-binding protein GB1,

Figure 3: Results for Level Set Estimation, showing the F1 score (where higher is better). The left and right panels present results for the Himmelblau test function and the topographic mapping problem, respectively. In the former problem, all algorithms perform similarly, while in the latter, PS-BAX outperforms all baselines.

given different sequence mutations in a target region of 4 residues . GB1 is well-studied by biologists, and its domain is known to be highly rugged, dominated by "dead" variants with very low fitness scores . There are \(20^{4}\) possible combinations, with 20 amino acids and 4 positions, and we represent the input space \(\) as one-hot vectors in an 80-dimensional space. To avoid excessive runtimes, we randomly sample 10,000 points from the original dataset. Due to the high dimensionality, vast input space, and sparse fitness landscape, this dataset poses significant challenges for standard GP models. Therefore, we use a deep kernel GP  as our probabilistic model. Given the dataset's size, we perform batched evaluations with batch size of \(q=4\) for both PS-BAX and INFO-BAX.

The results of these experiments are shown in Figure 5. In both problems, PS-BAX performs comparably to INFO-BAX, with both algorithms significantly outperforming Random.

### DiscoBAX: Drug Discovery Application

As a final application, we consider the DiscoBAX problem setting from  in the context of drug discovery, where the task is to identify a set of optimal genomic interventions to determine suitable drug targets. Formally, let \(\) represent a pool of genetic interventions, and for each \(x\), let \(f(x)\)

Figure 4: Depiction of the INFO-BAX (left) and PS-BAX (right) algorithms on the topographic level set estimation problem described in Section 4.2. Each figure shows the ground truth super-level set (small black dots), the points evaluated after 100 iterations (green and blue dots for INFO-BAX and PS-BAX, respectively), and the estimated level set from the final posterior mean (red dots). PS-BAX provides an accurate estimate of the level set, whereas INFO-BAX misses a significant portion.

Figure 5: Results for Top-\(k\) Estimation, showing the Jaccard distance (where lower is better). The left panel presents results for the 3-dimensional Rosenbrock test function with \(k=4\), while the right panel shows results for the real-world protein design GB1 dataset with \(k=10\). In both problems, PS-BAX performs similarly to INFO-BAX.

denote an in vitro phenotype measurement correlated with the effectiveness of genetic intervention \(x\). The effectiveness of the intervention is assumed to be \(f(x)+(x)\), where \((x)\) captures noise and other exogenous factors not reflected in the in vitro measurement. Following the setup in , we simulate \(\) using a GP with mean 0 and an RBF covariance function. The goal is to identify a small set of genomic interventions in \(\) that maximize an objective function balancing two characteristics: high expected change in the target phenotype and high diversity to maximize success in subsequent stages of drug development. This is formalized in  as the following optimization problem:

\[_{S:|S|=k}_{}_{x S}f(x)+ (x),\] (3)

where \(k\) is the desired size of the intervention set. This problem aims to find a set of interventions \(S\) such that the best-performing intervention in \(S\) has the highest expected effectiveness (over \(\)). Solving Equation 3 exactly is challenging due to its combinatorial nature, even if could evaluate \(f\) many times, but a computationally efficient approximation is possible by leveraging the submodularity of the objective function. For more details on the base algorithm, we refer the reader to .

Following , we use the tau protein assay  and interferon-gamma assay  datasets from the Achilles project . Originally, the gene embeddings in this dataset are represented as 808-dimensional vectors, and a Bayesian MLP is used as the probabilistic model instead of a GP. To reduce dimensionality, we preprocess the dataset using Principal Component Analysis (PCA) and then fit a GP to the lower-dimensional representation. Additionally, we truncate the dataset to the 5000 genes with the highest intervention values to ensure computational feasibility in our experiments. The results of these experiments are shown in Figure 6. PS-BAX significantly outperforms INFO-BAX, whose performance is only marginally better than that of Random.

## 5 Conclusion

Many real-world problems involve estimating the output of a base algorithm applied to a black-box function with costly evaluations. While the INFO-BAX algorithm proposed by  offers a solution, it faces practical limitations. In response, we introduced PS-BAX, a novel posterior sampling strategy built upon the observation that, in many cases, the algorithm's output can be characterized as a target set of input points. Our experiments demonstrate that PS-BAX is not only competitive with previous approaches but also significantly faster to compute. Moreover, we established conditions under which PS-BAX is asymptotically convergent.

Looking ahead, our approach provides a pathway to extend the success of Bayesian optimization to a broader range of problems, potentially unlocking new and impactful applications. Additionally, PS-BAX serves as a robust baseline for future research aimed at developing tailored strategies for specific domains. Furthermore, our findings offer new perspectives on posterior sampling algorithms and their application scope, suggesting several promising avenues for future exploration in this area.

Figure 6: Results for DiscoBAX , showing the regret between the solution found by applying a greedy submodular optimization algorithm to the objective in Equation 3 and the solution obtained from applying the same algorithm over the posterior mean instead of the true function. Both problems are based on the Achilles dataset , with the left panel presenting results for the tau protein assay  and the right panel showing results for the interferon-gamma assay. In both cases, PS-BAX significantly outperforms INFO-BAX, which performs only marginally better than Random.