# SmoothHess: ReLU Network Feature Interactions

via Stein's Lemma

 Max Torop,\({}^{1*}\) Aria Masoomi,\({}^{1*}\) Davin Hill,\({}^{1}\) Kivanc Kose,\({}^{2}\) Stratis Ioannidis,\({}^{1}\) Jennifer Dy\({}^{1}\)

\({}^{1}\) Northeastern University, \({}^{2}\) Memorial Sloan Kettering Cancer Center

{torop.m, masoomi.a}@northeastern.edu,

{dhill, ioannidis, jdy}@ece.neu.edu,

{kosek}@mskcc.org

Equal contribution

###### Abstract

Several recent methods for interpretability model feature interactions by looking at the Hessian of a neural network. This poses a challenge for ReLU networks, which are piecewise-linear and thus have a zero Hessian almost everywhere. We propose _SmoothHess_, a method of estimating second-order interactions through Stein's Lemma. In particular, we estimate the Hessian of the network convolved with a Gaussian through an efficient sampling algorithm, requiring only network gradient calls. SmoothHess is applied post-hoc, requires no modifications to the ReLU network architecture, and the extent of smoothing can be controlled explicitly. We provide a non-asymptotic bound on the sample complexity of our estimation procedure. We validate the superior ability of SmoothHess to capture interactions on benchmark datasets and a real-world medical spirometry dataset.

## 1 Introduction

As machine learning models are increasingly relied upon in a variety of high-stakes applications such as credit lending  medicine , or law , it is important that users are able to interpret model predictions. To this end, many methods have been developed to assess the importance of individual input features in effecting model output . However, one may achieve a deeper understanding of model behavior by quantifying how features _interact_ to affect predictions. While diverse notions of feature interaction have been proposed , in this work, we focus on the intuitive partial-derivative definition of feature interaction .

Given a function and point, the Interaction Effect  between a given set of features on the output is the partial derivative of the function output with respect to the features; intuitively, it represents the infinitesimal change in the function engendered by a joint infinitesimal change in each chosen feature. The Interaction Effect derives in part from Friedman and Popescu , who define the _global_ interaction between a set of features over the data distribution as the expected square of the partial-derivative with respect to those features. As in prior works , we eschew the expectation to focus on _local_ interactions occurring around a given point \(x\) and avoid squaring partial-derivatives to maintain the directionality of the interaction. We focus on _pair-wise feature interactions_, which, in the context of the Interaction Effect, are the elements of the Hessian.

ReLU networks are a popular class of neural networks that use ReLU activation functions . The use of ReLU has desirable properties, such as the mitigation of the vanishing gradient problem , and it is the sole activation function used in popular neural network families such as ResNet  and VGG . However, ReLU networks are piece-wise linear  and thus have a zero Hessian almost everywhere (a.e.), posing a problem for quantifying interactions (see Figure 1(a)).

A common approach for estimating the Hessian in ReLU networks is to take the Hessian of a smooth surrogate network which approximates the original: each ReLU is replaced with SoftPlus, a smooth approximation to ReLU [28; 22], before differentiating [39; 20]. However, this approach affords one only coarse control over the smoothing as _each internal neuron_ is smoothed, leading to unwieldy asymmetric effects, as can be seen in Figure 2.

In this work, we propose _SmoothHess_: the Hessian of the ReLU network convolved with a Gaussian. Such a function is a more flexible smooth surrogate than SoftPlus as _smoothing is done on the network output_, and the covariance of the Gaussian allows one to mediate the contributions of points based on their Mahalanobis distance. Unfortunately, obtaining the Hessian of the convolved ReLU network is impossible using naive Monte-Carlo (MC) averaging. However, by proving an extension of Stein's Lemma [78; 49], we show that such a quantity can be efficiently estimated using _only gradient calls_ on the original network. For an illustration of ReLU network smoothing, see Figure 1(b).

Our **main contributions** are as follows:

* We propose _SmoothHess_, the Hessian of the Gaussian smoothing of a neural network, as a model of the second-order feature interactions.
* We derive a variant of Stein's Lemma, which allows one to estimate SmoothHess for ReLU networks using only gradient calls.
* We prove non-asymptotic sample complexity bounds for our SmoothHess estimator.
* We empirically validate the superior flexibility of SmoothHess to capture interactions on MNIST, FMNIST, and CIFAR10. We utilize SmoothHess to derive insights into a network trained on a real-world medical spirometry dataset. Our code is publicly available.*

Footnote *: https://github.com/MaxTorop/SmoothHess

The remainder of this paper is organized as follows: In Sec. 2, we summarize the gradient-based methods for feature importance and interactions that are most related to our work. In Sec. 3, we provide a technical preliminary covering the definitions and techniques underlying both our method and competing works. Next, in Sec. 4, we introduce our method, SmoothHess, explain how to estimate it, and provide sample complexity bounds for our estimation procedure. In Sec. 5, we experimentally demonstrate the ability of SmoothHess to model interactions. Finally, in Sec. 6, we summarize our method and results, followed by a discussion of limitations and possible solutions.

## 2 Related Work

**Feature Importance and First-Order Methods:** Methods that quantify feature importance fall into two categories: (i) perturbation-based methods (e.g., [53; 66; 17]), which evaluate the change in model outputs with respect to perturbed inputs, and (ii) gradient-based methods (e.g., [72; 76; 81]), which leverage the natural interpretation of the gradient as infinitesimally local importance for a given sample. Most relevant to our work are gradient-based approaches. The saliency map, as defined in , is simply the gradient of model output with respect to the input. Several variants are developed to address the shortcomings of the saliency maps. SmoothGrad  was developed

Figure 1: **(a)** An exemplar illustration of a simple \(5\)-hidden-layer ReLU network \(f:^{2}\). Note that \(f\) is piece-wise linear and thus has \(_{x}^{2}f(x)=0\) a.e. **(b)** The ReLU network convolved with \(q_{0.3I}:^{2}\), the density function of \((0,0.3I)\). This function is no longer piece-wise linear and admits non-zero higher order derivatives.

to address noise by averaging saliency maps (see also Sec. 3), and comes with sample complexity guarantees . Sundararajan et al.  introduce Integrated Gradients, the path integral between an input and an uninformative baseline. This is extended to the Shapley framework by Erion et al. . The Grad-CAM line of work [93; 70; 61] is similar in nature to the methods above, with the key distinction that importance is modeled over internal (hidden) layers.

**Feature Interactions:** A variety of methods have been proposed to estimate higher-order feature interactions, which can again be separated into perturbation-based [54; 82; 86; 57; 92] and gradient-based approaches. Among the latter, Tsang et al.  propose Gradient-NID, which estimates feature interaction strength as the corresponding Hessian element squared. Janizek et al.  propose Integrated Hessian, which extends Integrated Gradients to use a path-integrated Hessian. Lerman et al.  propose Taylor-CAM, a higher-order generalization of Grad-Cam . Cui et al.  quantify global interactions for Bayesian neural networks in terms of the expected value of the Hessian over the data distribution. For classification ReLU networks, the CASO and CAFO explanation vectors exploit feature interactions in the loss function using Hessian estimates .

Unfortunately, due to their piecewise linearity, existing Hessian-based interaction methods cannot be readily applied to ReLU networks. Janizek et al.  replace each ReLU activation with SoftPlus post-hoc, before applying Integrated Hessians. Similarly, Tsang et al.  apply their method to networks with the SoftPlus activation instead of ReLU. For regression tasks, Lerman et al.  replace ReLU with the smooth activation function GELU  before training. Although SoftMax outputs and the cross-entropy loss admit higher order derivatives, pre- or post-hoc smoothing, as above, is necessary for finding interactions affecting logits, internal neurons, or regression outputs. Indeed, while Singla et al.  estimate interactions on the original ReLU network, they are only with respect to the loss function.

In contrast, we propose a method for quantifying feature interactions that works with any ReLU network post-hoc without requiring retraining or modifications to the network architecture. It also can be directly estimated with respect to model _as well as_ intermediate layer outputs. Furthermore, our experiments in Sec. 5 show the superior ability of our method to model logit outputs, internal neurons, and SoftMax probabilities as compared to a SoftPlus smoothed network.

## 3 Technical Preliminary

ReLU Network Background:We denote a ReLU network by \(F:^{d}^{c}\), where \(c=1\) in the case of regression. We denote the function which we wish to explain as \(f:^{d}\), an arbitrary neuron \(f_{i}^{(l)}\) (the \(i^{th}\) neuron in the \(l^{th}\) layer) in our ReLU network, or a SoftMax Probability

Figure 2: Estimated Hessian element between features \(1\) and \(2\) at \(x_{0}=(0,0)^{T}\) for a \(6\)-layer ReLU Network \(f:^{2}\) trained to memorize the Four Quadrant toy dataset. **(a)** SmoothHess (SH) is estimated with isotropic covariance \(=^{2}I\) using granularly sampled \(^{2}\{1e3,,1\}\). Aside from at minute \(^{2}<1e2.5\), where hyper-local noisy behavior is captured, we have \(^{2}(f*q_{})(x_{0})_{1,2}(5+3+12-10)/4=2.5\), the average of the memorized “ground truth” off-diagonal Hessian element over the four quadrants. This indicates a symmetry in the weighting of the contributions from points around \(x_{0}\) at _every level of smoothing_. **(b)** The Hessian of the SoftPlus smoothed function \(f_{}\) (SP Hess) is computed using granularly sampled \(\{1e1,,1e4\}\). The average value of \(2.5\) is not achieved at any value of \(\), aside from briefly between \(_{10}=2\) and \(_{10}=3.5\) indicating that SoftPlus fails to incorporate the information around \(x_{0}\) in a symmetric manner at _every level of smoothing_.

[MISSING_PAGE_FAIL:4]

**Lemma 2**.: _(First-Order Oracle Stein's Lemma ) Given \(x_{0}^{d}\), covariance matrix \(^{d d}\), multivariate normal random vector \(^{d}\) distributed from \((0,)\) and continuously differentiable function \(g(z):^{d}\) with locally Lipschitz+ gradients \( g:^{d}^{d}\), then_

Footnote †: margin: _\(\)_

\[_{}[^{-1}[_{x}g(x_{0}+)]^{T}]=_{}[_{x}^{2}g(x_{0}+)].\] (4)

Complexity bounds have been derived for similar identities, which express the Hessian using zero-th order information . However, Lemma 2_fails_ for ReLU networks. This is precisely because ReLU networks are piecewise linear and, therefore, _are not continuously differentiable_. In the next section, we directly address this through our method for estimating a smoothed Hessian for ReLU networks.

## 4 SmoothHess

Our main contributions are: (1) We propose _SmoothHess_, the Hessian of the network convolved with a Gaussian, for modeling feature interactions. (2) We use Stein's Lemma to prove that SmoothHess may be estimated for ReLU networks using only gradient oracle calls. (3) We prove non-asymptotic sample complexity bounds for our SmoothHess estimator.

**Gaussian Convolution as a Smooth Surrogate:** An alternative smooth-surrogate to \(f_{}\) is \(h_{f,}:^{d}\), the convolution of \(f\) with a Gaussian:

\[h_{f,}(x_{0})=(f*q_{})(x_{0})=_{z^{d}}f(z)q_{ }(z-x_{0})dz,\] (5)

where \(^{d d}\) is a covariance matrix, \(q_{}(z-x_{0})=(2)^{-}||^{-}(- {2}d(x_{0},z)_{})\) is the density function of the Gaussian distribution \((0,)\), \(||\) is the determinant and \(d(x_{0},z)_{}=(x_{0}-z)^{T}^{-1}(x_{0}-z)\) is the Mahalanobis distance between \(x_{0}\) and \(z\).

The Gaussian-smoothed function \(h_{f,}\) is infinitely differentiable and does not suffer from the limitations of surrogates obtained from internal smoothing. Here, smoothing is done on the _output image_ of \(f\) and thus the relationship between \(h_{f,}(x_{0})\), \(f(z)\), \(z\) and \(x_{0}\) is made explicit by Eq. (5): the relative contribution of \(f(z)\) to \(h_{f,}(x_{0})\) is proportional to the exponentiated negative half Mahalanobis distance \(-d(x_{0},z)_{}\). The ability to adjust \(\) gives a user fine-grained and localized control over smoothing; the eigenvectors and eigenvalues of \(\) respectively encode directions of input space and a corresponding locality for their contribution to \(h_{f,}\).

We define SmoothHess as the Hessian of \(h_{f,}\):

**Definition 1**.: _(SmoothHess) Given ReLU network \(f:^{d}\), point to explain \(x_{0}^{d}\), covariance matrix \(^{d d}\) and \(q_{}:^{d}\), the density function of Gaussian distribution \((0,)\), then SmoothHess is defined to be the Hessian of \(f\) convolved with \(q_{}\) evaluated at \(x_{0}\):_

\[_{x}^{2}h_{f,}(x_{0})=_{x}^{2}(f*q_{})(x_{0})=_{z ^{d}}f(z)_{x}^{2}q_{}(z-x_{0})dz.\] (6)

Well-known properties of the Gaussian distribution may be used to encode desiderata into the convolved function and, accordingly, to SmoothHess through the choice of the covariance. For instance, it is known that as \(d\) the isotropic Gaussian distribution \((0,^{2}I_{d d})\) converges to \(U(S_{}^{d-1})\), a uniform distribution over the radius \(\) sphere . Thus, given large enough \(d\), as is commonly encountered in deep learning datasets, one may choose \(=(r/)I\) to approximately ensure that \(h_{f,}(x_{0})\) incorporates information from the radius \(r\) sphere around \(x_{0}\). We exploit and validate this intuition in our experiments (see Table 1).

Finally, we must highlight the strong connection between SmoothHess and SmoothGrad , which Wang et al.  prove is equivalent to the gradient of the same smooth surrogate: \(_{x}h_{f,}(x_{0})\). Thus,SmoothGrad and SmoothHess together define a second-order Taylor expansion of \(h_{f,}\) at \(x_{0}\), which can be used as a second-order model of \(f\) around \(x_{0}\).

**SmoothHess Computation via Stein's Lemma:** We relate our method for estimating SmoothHess for ReLU networks. As stated above, Lemma 2_does not hold for ReLU networks._ However, we extend the arguments of Wang et al.  and Lin et al.  to show that the LHS from Lemma 2_is equivalent to_ SmoothHess for all Lipschitz continuous functions1:

**Proposition 1**.: _Given \(x_{0}^{d}\), \(L\)-Lipschitz continuous function \(g:^{d}\), covariance matrix \(^{d d}\) and random vector \(^{d}\) distributed from \((0,)\) with density function \(q_{}:^{d}\), then_

\[_{}[^{-1}[_{x}g(x_{0}+)]^{T}]=_ {x}^{2}[(g*q_{})(x_{0})]=_{x}^{2}h_{g,}(x_{0}),\] (7)

_where \(*\) denotes convolution._

In other words, even though Lemma 2 does not hold for ReLU networks, Stein's Lemma is indeed evaluating a Hessian: namely, SmoothHess given by Eq. (6). The proof consists of moving the Hessian operator on the RHS of Eq. (7) into the integral defined by \((g*q_{})(x_{0})\). The resulting expression is simplified into a form for which Lin et al.  prove is equivalent to the LHS of Eq. (7). The proof is provided in App. B.

Proposition 1 opens up the possibility of an MC-estimate of SmoothHess _that only require access to a first-order oracle \( f\)._ This is computed by sampling a set of \(n\) perturbations \(\{_{i}\}_{i=1}^{n},_{i}(0,)\), and for each \(_{i}\) querying \(_{x}f(x_{0}+_{i})\) before taking the outer product \(_{i}[_{x}f(x_{0}+_{i})]^{T}^{d d}\), Monte-Carlo averaging and finally symmetrizing:

\[_{n}^{}(x_{0},f,) =_{i=1}^{n}^{-1}_{i}[_{x}f(x_{0 }+_{i})]^{T},\] (8a) \[_{n}(x_{0},f,) =(_{n}^{}(x_{0},f,)+_{n}^{ ^{T}}(x_{0},f,)).\] (8b)

Each first-order oracle call has the same time complexity as one forward pass and may be computed efficiently in batches using standard deep learning frameworks . As \(n\) is finite, \(_{n}^{}(x_{0},f,)\) is not guaranteed to be symmetric in practice. Thus, we symmetrize our estimator in Eq. (8b). A straightforward consequence of Proposition 1 is that \(_{n}_{n}(x_{0},f,)=_{x}^{2}h_{f,}(x_{0})\), which we formally show in the Proof of Theorem 1 in App. C.

Estimation of SmoothGrad may be amortized with SmoothHess, obtaining \(_{x}h_{f,}(x_{0})\)_at a significantly reduced cost_. The main computational expense when estimating SmoothGrad is the querying of the first-order oracle. As this querying is part of SmoothHess estimation, the gradients which we compute may be averaged at a minimal additional cost of \((nd)\) to obtain a SmoothGrad estimate. Likewise, SmoothHess may be obtained at a reduced cost of \((nd^{2})\), the cost of the outer products, during SmoothGrad estimation. For details of our algorithm, see App. D.

Last, we prove non-asymptotic bounds for our SmoothHess estimator:

**Theorem 1**.: _Let \(f:^{d}\) be a piece-wise linear function over a finite partition of \(^{d}\). Let \(x_{0}^{d}\), and denote \(\{_{i}\}_{i=1}^{n}\), a set of \(n\) i.i.d random vectors in \(^{d}\) distributed from \(_{i}(0,)\). Given \(_{n}(x_{0},f,)\) as in Eq. (8), for any fixed \(,(0,1]\), given \(n}[((C^{+}+} })^{2},(C^{-}+}})^{2})]\) then_

\[_{n}-H_{2}>,\] (9)

_where \(H=_{x}^{2}[(f*q_{})(x_{0})]\), \(C^{+},C^{-}c^{+},c^{-}>0\) are constants depending on the function \(f\) and covariance \(\) and \(q_{}:^{d}\) is the density function of \((0,)\)._

We elect to use the non-asymptotic bounds presented in Theorem 3.39 of Vershynin , which hold for sums of outer products of a sub-gaussian random vector with itself. This poses a challenge as \(_{n}\) is the sum of outer products of two sub-gaussian random vectors which are _not necessarily equal_. To deal with this issue, we separately prove non-asymptotic bounds for the outer productsof the positive and negative eigenvectors of the summands. This is accomplished using an identity expressing the eigenvalues of the summands in terms of \(^{-1}\) and \(_{x}f(x_{0}+)\), which we state and prove in App. C. The proof is completed by applying the triangle inequality and union bound to combine the two bounds into Eq. (9). Our proof is included in App. C.

## 5 Experiments

### Experimental Setup

**Datasets and Models**: Experiments were conducted on a real-world spirometry regression dataset, three image datasets (MNIST , FMNIST  and CIFAR10 ), and one synthetic dataset (Four Quadrant). The Four Quadrant dataset consists of points \(x^{}^{2}\) sampled uniformly from the grid \([-2,2]^{2}\), with a spacing of \(0.008\). The label of a point \(y(x^{})=Kx_{1}^{}x_{2}^{}\) is chosen based upon its quadrant, with \(K=5,3,12,-10\) for Quadrants \(1,2,3\) and \(4\) respectively. We train a 5-layer network on MNIST and FMNIST a ResNet-18  on CIFAR10 and a 6-layer network on Four Quadrant. A 10-layer 1-D convolutional neural network was trained on the spirometry regression dataset. Additional model, hyperparameter and dataset details are outlined in App. E.

**Hardware:** Experiments were performed on an internal cluster using NVIDIA A100 GPUs and AMD EPYC223 7302 16-Core processors.

**Metrics**: Gradient-based explainers are evaluated using a Taylor expansion-like function as a proxy. We estimate a gradient Hessian pair \(_{(x_{0},g)}^{d},_{(x_{0},g)}^{d  d}\) for some function \(g:^{d}\) around point \(x_{0}\). Here \(g\) is either a smooth-surrogate of \(f\) or \(f\) itself. We define the following Taylor expansion-like function:

\[_{x_{0},_{g},_{g}}()=f(x_{0})+_{g}^ {T}(-x_{0})+(-x_{0})^{T}_{g}(-x_{0}).\] (10)

For readability, we remove the dependence on \(x_{0}\) from \(_{(x_{0},g)}\) and \(_{(x_{0},g)}\). Eq. (10) is almost equivalent to the Taylor-expansion of \(g\) at \(x_{0}\), with the key difference that the zero-th order term is \(f(x_{0})\) as opposed to \(g(x_{0})\). This is done to isolate the impact of the explainers \(_{g}\) and \(_{g}\). Setting \(=0\) yields \(_{x_{0},_{g},0}()\), a first-order Taylor expansion-like function. For brevity, we refer to \(_{x_{0},_{g},_{g}}\) simply as a Taylor expansion below. We use the following two metrics to quantify the efficacy of SmoothHess:

**Perturbation MSE**: We introduce the Perturbation Mean-Squared-Error (\(_{MSE}\)) to assess the ability of \(\) and/or \(\) to capture the behaviour of \(f\) over a given neighborhood. Given point \(x_{0}\) and radius \(>0\), the \(_{MSE}\) directly measures the fidelity of \(_{x_{0},_{g},_{g}}\) to \(f\) when restricted to the ball \(B_{}(x_{0})\):

\[_{MSE}(x_{0},f,_{x_{0},_{g},_{g}}, )=(B_{}(x_{0}))}_{x^{} B _{}(x_{0})}(_{x_{0},_{g},_{g}}(x^{ })-f(x^{}))^{2}dx^{}\] (11)

A low \(_{MSE}\) value indicates that \(_{x_{0},_{g},_{g}}\) is a good fit to \(f\) when restricted to \(B_{}(x_{0})\), and thus that \(_{g}\) and \(_{g}\) capture the behaviour of \(f\) over \(B_{}(x_{0})\). \(_{MSE}\) may be estimated for a point

  _Dataset_ &  &  &  \\  _Function_ &  &  &  &  &  &  &  \\  \(e\) & 0.25 & 0.50 & 1.00 & 0.25 & 0.50 & 1.00 & 0.25 & 0.50 & 1.00 & 0.25 & 0.50 & 1.00 & 0.25 & 0.50 & 1.00 \\  SH+SG (Us) & **9.6e-7** & **7.8** & **6.7e-5** & **4.9e-8** & **4.0e-7** & **3.3e-6** & **6.5e-7** & **4.0e-6** & **4.3e-5** & **2.0e-8** & **1.8e-7** & **1.6e-6** & **9.3e-4** & **2.2e-2** & **1.2e-1** & **8.1e-7** & **1.4e-5** & **1.6e-4** \\ SG  & 4.5e-6 & 4.1e-5 & 3.9e-4 & 2.1e-7 & 1.7e-6 & 1.5e-5 & 3.0e-6 & 2.7e-5 & 2.6e-4 & 1.0e-7 & 9.0e-7 & 7.0e-6 & 1.3e-2 & 8.6e-2 & 4.9e-1 & 1.3e-5 & 1.1e-4 & 8.3e-4 \\ SP (H + G) & 1.2e-6 & 9.6e-6 & 8.1e-5 & 5.5e-8 & 4.4e-7 & 3.7e-6 & 9.6e-7 & 7.5e-6 & 6.5e-5 & 3.0e-8 & 2.1e-7 & 1.8e-6 & 2.1e-3 & 3.3e-2 & 2.5e-1 & 1.1e-5 & 1.0e-4 & 7.0e-4 \\ SP G & 4.6e-6 & 4.1e-5 & 3.9e-4 & 2.1e-7 & 1.7e-6 & 1.5e-5 & 3.2e-6 & 2.8e-5 & 2.6e-4 & 1.0e-7 & 8.5e-7 & 7.2e-6 & 1.3e-2 & 9.0e-5 & 2.5e-1 & 5.1e-5 & 2.9e-4 & 1.6e-3 \\ G  & 4.2e-3 & 1.7e-6 & 2.7e-2 & 2.0e-3 & 7.0e-3 & 2.9e-2 & 3.8e-1 & 1.5e-2 & 6.0e-2 & 1.0e-4 & 4.0e-4 & 1.8e-3 & 3.0e-1 & 1.2e-0 & 5.0e-9 & 0.0e-4 & 3.5e-3 & 1.4e-2 \\  

Table 1: Average \(_{MSE}\) at three radii \(\). Results are calculated for SmoothHess + SmoothGrad (SH+SG,Us) SmoothGrad (SG) SoftPlus Gradient (SP G) SoftPlus Hessian + SoftPlus Gradient (SP (H + G)) and the vanilla Gradient (G). Results are provided for the predicted class logit, and the penultimate neuron maximally activated by the “three”, “dress and cat classes for MNIST, FMNIST and CIFAR10 respectively. While SP (H + G) and SP G results are reported using the best \(\)_chosen from over \(100\) values_ based on validation set performance, _only \(3\) values were checked_ for SH + SG and SG based upon \(=\). SH + SG outperforms the competing methods for all 18 permutations of dataset, function and \(\).

\(x_{0}\) by sampling a set of \(n\) points \(\{x_{i}^{}\}_{i=1}^{n}\) uniformly from \(B_{}(x_{0})\), computing the errors and MC-averaging: \(_{i=1}^{n}(_{x_{0},_{g},_{g}}(x_{i}^{ })-f(x_{i}^{}))^{2}_{MSE}(x_{0},f,_{x_{0 },_{g},_{g}},)\).

**Adversarial Attacks**: Given \(\) and \(\), one can use \(_{x_{0},,}\) to construct adversarial attacks of any desired magnitude \(>0\). We denote \(^{*}^{d}\) as the perturbed input, and corresponding attack vectors by \(^{*}-x_{0}\). Given only \(_{g}\) (i.e. \(_{g}=0\)), the first-order Taylor expansion attack yields: \(^{*}-x_{0}=-_{g}\|_{g}\|_{2}\). Otherwise, the attack may be framed as the solution to a quadratic optimization minimizing the second-order Taylor expansion:

\[_{^{d}}_{x_{0},_{g},_ {g}}(), s.t.\|-x_{0}\|_{2}.\] (12)

Although this optimization is non-convex (as \(_{g}\) is not guaranteed to be positive-semi-definite), it can be solved exactly . For implementation details, and a discussion of the similarities and differences with Singla et al. , see App. D. We set \(g\) to be the smoothed version of the predicted class SoftMax probability function. Given a set of test points \(\{x_{i}\}_{i=1}^{n},n\), we validate the efficacy of our attacks using the average post-hoc accuracy metric \(_{i=1}^{n}[_{x_{i}}F(x_{i})=_{x_{i}}F(_{i}^{*})]\), where \(\) denotes the indicator function, and \(_{i}^{*}\) the output after \(x_{i}\) is attacked.

**Setup**: We provide the details for our experiments below:

**Four Quadrant:** We train the network to memorize the Four Quadrant dataset. We measure the interactions at \(x_{0}=(0,0)^{T}\) using both SmoothHess and SoftPlus Hessian, estimated with granularly sampled \(^{2}\{1e{}3,,1\}\) and \(\{1e{}1,,4\}\).

\(_{MSE}\) **:** We set \(g\) to be the smoothed version of a predicted class logit or a penultimate neuron. The penultimate neuron is chosen to be the maximally activated neuron on average over the train data by the "three," dress and cat classes for MNIST, FMNIST and CIFAR10, respectively. We compute \(_{MSE}\) for three neighborhood sizes \(\{0.25,0.50,1.0\}\). While _over one-hundred values_ of \(\) are checked on a validation set before selection, _only three values_ of \(\) are checked for SmoothHess and SmoothGrad, based on the common-sense criterion \(=/:\ \{/2,3/4 ,/\}\) outlined in Sec 4. The standard deviation of \(_{MSE}\) results is reported in App. F.

**Adversarial Attacks:** Adversarial attacks are performed after selecting the best \(\) and \(\) from a held-out validation set. We refrain from using the criterion \(=/\) for adversarial attacks, as they rely upon the extremal, as opposed to average, behavior of \(f\). We check between \( 10\) and \( 30\) values of \(\) and \(\) on the held-out validation set before selecting the values resulting in the most effective attacks. Attacks are performed using magnitudes \(\{0.25,0.50,0.75,1.25,1.75\}\) for MNIST and FMNIST and \(\{0.1,0.2,0.3,0.4,1.0\}\) for CIFAR10, which is easier to successfully attack at lower magnitudes due to it's complexity. For more details see App. D.

**Competing Methods**: We compare SmoothHess with other gradient-based methods that model \(f\) locally; i.e. those which can be associated with the Taylor expansion around a smooth surrogate of \(f\) or \(f\) itself. Specifically, we compare with the Gradient and Hessian of SoftPlus smoothed network \(f_{}\), SmoothGrad  and the vanilla (unsmoothed) Gradient. We also compare adversarial attacks with random vectors scaled to the attack magnitude \(\) as a baseline.

   &  &  &  \\  _Attack Magnitude_ & \(\) & \(0.25\) & \(0.50\) & \(0.75\) & \(1.25\) & \(1.75\) & \(0.25\) & \(0.50\) & \(0.75\) & \(1.25\) & \(1.75\) & \(0.1\) & \(0.2\) & \(0.3\) & \(0.4\) & \(1.0\) \\  SH+SG (Us) & **93.0** & **80.3** & **48.0** & **10.5** & **2.0** & **79.5** & **46.8** & **25.0** & **3.5** & **0.5** & **0.62** & **38.5** & **26.5** & **15.0** & 4.5 \\ SG  & 93.3 & 81.8 & 48.8 & 11.3 & 2.8 & **79.5** & 49.3 & 26.3 & 4.0 & **0.0** & 65.0 & 42.0 & 27.5 & 17.0 & **0.0** \\ SP (H + G) & **93.0** & 81.8 & 51.5 & 15.8 & 7.5 & 79.8 & 51.0 & 27.5 & 5.3 & 0.8 & 64.5 & 42.0 & 31.0 & 23.5 & 7.5 \\ SP G & 93.3 & 82.3 & 53.8 & 16.3 & 5.0 & 79.8 & 51.5 & 29.5 & 7.8 & 1.0 & 66.5 & 47.5 & 36.0 & 29.5 & 8.5 \\ G  & 93.3 & 82.8 & 56.0 & 18.5 & 8.8 & 80.3 & 52.3 & 31.8 & 11.0 & 2.5 & 69.0 & 51.5 & 41.0 & 34.0 & 21.5 \\ Random & 99.8 & 99.5 & 99.0 & 99.0 & 98.8 & 99.3 & 98.0 & 97.3 & 95.5 & 93.8 & 100.0 & 99.5 & 99.0 & 98.5 & 96.5 \\  

Table 2: Post-hoc accuracy of adversarial attacks performed on the predicted SoftMax probability, at five attack magnitudes \(\). Lower is better. Results for SmoothHess + SmoothGrad (SH + SG, Ours), SmoothGrad (SG), SoftPlus Gradient (SP G) and SoftPlus Hessian + SoftPlus Gradient (SP (H + G)) are reported using parameters \(=^{2}I^{d d}\) and \(>0\) chosen based upon performance on a held-out validation set. We additionally compare with the vanilla (unsmoothed) Gradient (G). First order attack vectors are constructed by scaling the normalized gradient by \(\) and subtracting from the input. Second order attack vectors are found by minimizing the corresponding second-order Taylor expansions.

Additional experiments are provided in App. F. We compare the efficacy of SmoothHess with the unsmoothed Hessian (for adversarial attacks on SoftMax) and Swish  smoothed networks, both of which our method outperforms. We also run the \(_{MSE}\) experiment using a ResNet101 model, validating the superior ability of SmoothHess to capture interactions in larger networks.

### Results

**Symmetric Smoothing - Four Quadrant Dataset:** We investigate the ability of both SmoothHess and the SoftPlus Hessian to capture local feature interactions symmetrically. Results are shown in Figure 2. At each value of \(^{2}\), aside from extremely small \(^{2}<1e\)-\(2.5\), the off-diagonal element of SmoothHess is approximately equal to \(2.5\), the average interaction over the quadrants. The off-diagonal element of the SoftPlus Hessian is essentially never equal to \(2.5\). The results for SmoothHess follow from the fact that an isotropic covariance was used, the rotation invariance of which weights points that are equidistant from \(x\) equally. The SmoothHess off-diagonal element is not \( 2.5\) at small \(\) because, despite the network being trained to memorize the data, such a small neighborhood will inherently reflect noise. The inability of SoftPlus to capture interactions symmetrically follows from the fact that smoothing is done internally on each neuron, which does not guarantee symmetry.

**Perturbation Mean-Squared-Error:** We show SmoothHess can be used to capture interactions occurring in a variety of neighborhoods around \(x\). It is shown in Table 1 that SmoothHess + SmoothGrad (SH + SG) achieves the lowest \(_{MSE}\) for all \(18\) combinations of dataset, function and neighborhood size \(\). We emphasize that this is despite the fact that _only three values_ of \(\) were validated for SG + SH and SG on a held-out set compared to _over one-hundred values_ of \(\) for SP (H + G) and SP G. This indicates the superior ability of SmoothHess to model \(f\) over a diverse set of neighborhood sizes \(\). Further, it can be seen that second-order methods SH + SG and SP (H + G) achieve significantly lower \(_{MSE}\) than their first-order counterparts, SG and SP G, respectively, sometimes by an order of magnitude. This confirms the intuition that higher order Taylor expansions of the smooth surrogate provide more accurate models of \(f\). Interestingly, while SH + SG is clearly superior to SP (H + G), we see that SG and SP G are tied in many cases. This could indicate that the symmetric properties of Gaussian smoothing are comparatively more important when higher-order derivatives are considered. However, more investigation is needed.

**Adversarial Attacks:** We use the interactions found by SmoothHess to generate adversarial attacks on the classifier. We see from Table 2 that the Taylor expansion associated with SmoothHess and SmoothGrad generates the most powerful adversarial attacks for all datasets and attack magnitudes \(\), aside from CIFAR10 at \(=1.0\) where both methods successfully change most class predictions. The superiority of the SmoothHess attacks indicates that the Gaussian smoothing is capturing the extremal behavior of the predicted SoftMax probability more effectively than the other methods. One hypothesis for SmoothGrad outperforming SmoothHess in the largest neighborhood for CIFAR10 is that the network behavior is highly complex over a large area, and adding higher order terms decreases performance.

Figure 3: Evaluation of network predictions for two spirometry samples (Left and Right), using SmoothGrad and SmoothHess. Spirometry curves are plotted in blue. SmoothGrad (green) and SmoothHess (red) attributions are grouped into 0.5s intervals and plotted as bars for each interval. The plotted SmoothHess values are interactions with respect to the first 0.5s time interval. The red arrows point to the features that exhibit the largest interactions (endpoint) with the initial 0.5s (source). Sample 1 exhibits coughing within the first 4 seconds, as indicated by plateauing spirometry curve. We observe that the strongest interactions for Sample 1 occur at this initial cough-induced plateau. In contrast, Sample 2 exhibits no coughing within the first 4 seconds; the strongest interactions occur later near small fluctuations in the spirometry curve.

**Qualitative Analysis of FEV\({}_{1}\) Prediction using Rejected Spirometry:**

A spirometry test is a common procedure used to evaluate pulmonary function, and is the main diagnostic tool for lung diseases such as Chronic Obstructive Pulmonary Disease . During an exam, the patient blows into a spirometer, which records the exhalation as a volume-time curve. Recent works have investigated the use of deep learning on spirograms for tasks such as subtyping , genetics [16; 91], and mortality prediction . Additional background on spirometry is outlined in App. E.

Traditionally, exhalations interrupted by coughing are discarded for quality control reasons. We train a CNN on raw spirograms from the UK Biobank  to predict the patient's "Forced Expiratory Volume in 1 Second" (FEV\({}_{1}\)), a metric frequently used to evaluate lung health, using efforts that were rejected due to coughing. In Fig. 3, we apply SmoothHess and SmoothGrad on two spirometry samples to understand their FEV\({}_{1}\) predictions. In Sample 1, coughing occurs within the first 4 seconds of the curve, as evidenced by the early plateauing of the curve which indicates a pause in exhalation. In contrast, Sample 2 exhibits no detectable coughing within the first 4 seconds. To improve the interpretability of the results, we group the features into 0.5 second time intervals. FEV\({}_{1}\) is traditionally measured in the initial 2 seconds of the non-rejected samples (see App. E), therefore we calculate SmoothHess interactions with respect to the first 0.5 second time interval.

FEV\({}_{1}\) is known to be strongly affected by the presence of coughing . The time intervals where coughing occurs, indicated by plateaus in the spirometry curves in Figure 3, should be an important signal for any model trained to predict FEV\({}_{1}\). Indeed, we observe that for Sample 1, the SmoothGrad attribution for the first 0.5s interval is relatively low, with strong interactions occurring at the coughing-induced plateaus. In contrast, the first 0.5s interval for Sample 2 shows high importance, with lower magnitude interactions that may be indicative of small fluctuations in the volume-time curve.

In Figure 4 we present the SmoothHess matrix for Sample 1 in Fig. 3, applied on two CNN models with different convolution kernel width. Interestingly, the smaller kernel width constrains the interactions to features that are spatially close together. In contrast, the features in the large kernel model have long-ranging interactions. These results agree with the intuition that smaller kernel widths, which constrain the receptive field of neurons up until the final layers, may correspondingly limit feature interactions.

## 6 Conclusion and Future Work

We introduce SmoothHess, the Hessian of the network convolved with a Gaussian, as a method for quantifying second-order feature interactions. SmoothHess estimation, which relies only on gradient oracle calls, and cannot be performed with naive MC-averaging, is made possible by an extension of Stein's Lemma that we derive. We provide non-asymptotic complexity bounds for our estimation procedure. In contrast to previous works, our method can be run post-hoc, does not require architecture changes, affords the user localized flexibility over the weighting of input space, and can be run on any network output. We experimentally validate the superior ability of SmoothHess to capture interactions over a variety of neighborhoods compared to competing methods. Last, we use SmoothHess to glean insight into a network trained on real-world spirometry data.

**Limitations:** The outer product computations in SmoothHess estimation have space and time complexity \((d^{2})\). When \(d 0\) this becomes expensive: for instance ImageNet  typically has \(d^{2} 10^{10}\). This is a common problem for all interaction effect methods. We leave as future work to explore computationally efficient alternatives or approximations to these outer products; a potential remedy is to devise an appropriate power method to estimate the top eigenvectors of SmoothHess, rather than its entirety, which fits well our estimation via sampling low-rank matrices.

Figure 4: Heatmap of the SmoothHess interactions for the spirometry sample in Fig. 3, calculated on CNNs with varying convolution kernel width.