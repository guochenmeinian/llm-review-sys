# DiffuLT: Diffusion for Long-tail Recognition Without External Knowledge

Jie Shao  Ke Zhu  Hanxiao Zhang  Jianxin Wu

National Key Laboratory for Novel Software Technology, Nanjing University, China

School of Artificial Intelligence, Nanjing University, China

{shaoj, zhuk, zhanghx}@lamda.nju.edu.cn, wujx2001@nju.edu.cn

J. Wu is the corresponding author.

###### Abstract

This paper introduces a novel pipeline for long-tail (LT) recognition that diverges from conventional strategies. Instead, it leverages the long-tailed dataset itself to generate a balanced proxy dataset without utilizing external data or model. We deploy a diffusion model trained from scratch on only the long-tailed dataset to create this proxy and verify the effectiveness of the data produced. Our analysis identifies approximately-in-distribution (AID) samples, which slightly deviate from the real data distribution and incorporate a blend of class information, as the crucial samples for enhancing the generative model's performance in long-tail classification. We promote the generation of AID samples during the training of a generative model by utilizing a feature extractor to guide the process and filter out detrimental samples during generation. Our approach, termed Diffusion model for Long-Tail recognition (DiffuLT), represents a pioneer application of generative models in long-tail recognition. DiffuLT achieves state-of-the-art results on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT, surpassing leading competitors by significant margins. Comprehensive ablations enhance the interpretability of our pipeline. Notably, the entire generative process is conducted without relying on external data or pre-trained model weights, which leads to its generalizability to real-world long-tailed scenarios.

## 1 Introduction

Deep learning has exhibited remarkable success across a spectrum of computer vision tasks, especially in image classification, e.g., as exhibited by He et al. (2016), Dosovitskiy et al. (2021), Liu et al. (2021). These models, however, encounter obstacles when faced with real-world long-tailed (LT) data, where the majority classes have abundant samples but the minority ones are sparsely represented. The intrinsic bias of deep learning architectures towards more populous classes exacerbates this issue, leading to sub-optimal recognition of minority classes despite their critical importance in practical applications.

Conventional long-tailed learning strategies such as re-weighting (Lin et al. (2017), Cao et al. (2019)), re-sampling (Zhou et al. (2020), Zhang et al. (2021)), and structural adjustments (Wang et al. (2020), Cui et al. (2022)), share a commonality: they acknowledge the data's imbalance and focus on the training of models. They demand meticulous design and are challenging to generalize. Recently, a shift towards studying the dataset itself and involving more training samples through external knowledge to mitigate long-tailed challenges has emerged (Zhang et al. (2021, 2024)). Yet, in many real-world scenarios, access to specialized data is limited and closely guarded, such as in military or medical contexts. This predicament prompts a critical inquiry: _Is it feasible to balance long-tailed datasets without depending on external resources or models?_Our answer is _yes_. Recent advances in diffusion models have demonstrated their significant potential in generating high-quality images (Ho et al. (2020); Song et al. (2020); Rombach et al. (2022)). Assuming that diffusion models are proficient at learning distributions, we develop _a diffusion model trained from scratch_ on _only_ the long-tail distributed dataset. This model creates new samples for underrepresented classes, which are then used to train a classifier on the re-balanced dataset, leading to improved accuracy. We are the first to demonstrate the effectiveness of using generated samples, without relying on external data or models, in improving long-tail classification. We observe a notable pattern: enhancing the performance of the generative model with a loss modification called CBDM (Qin et al. (2023)) also enhances the classifier's accuracy, as illustrated in fig. 1. This phenomenon implies that a generative model with better performance tends to generate samples that are more beneficial for the classification task. This observation raises an important question: What are _the most valuable generated samples_ for classification, and how are they generated? This question is critical, as it determines whether a diffusion model is going to be beneficial or detrimental for LT recognition.

We answer this question by analyzing features of generated samples, as visualized in fig. 1 using t-SNE (Van der Maaten and Hinton (2008)). We categorize the generated samples into three groups: in-distribution (ID), approximately in-distribution (AID), and out-of-distribution (OOD). Our research indicates that AID samples are pivotal in enhancing classifier performance. Through experiments, we conclude that a diffusion model can assimilate patterns from the head classes and integrates them into the tail ones to produce AID samples. These samples significantly enhance the quantity and diversity of the tail classes, thereby substantially improving their performance. Then the important question to solve is: How can we generate AID samples efficiently?

To encourage the model to predominantly generate AID samples, we introduce a novel type of loss. This loss employs a feature extractor to penalize the generation of ID and OOD samples. Such a strategy not only elevates the performance of the generative model on long-tail datasets but also renders it more effective and efficient in enhancing classifier performance.

In general, we introduce a new pipeline, DiffuLT (_Diffusion_ model for _L_ong-_T_ail recognition), for long-tail datasets. It has three steps: initial training, sample generation, and retraining. Initially, we train a feature extractor and a diffusion model incorporating a supervision term to encourage the generation of AID samples. Subsequently, this generative model is employed to augment the dataset towards balance. The final step involves training a new classifier on the enriched dataset, with a minor adjustment to reduce the impact of synthetic samples. It is crucial to underscore the importance of training the diffusion model _without external data or knowledge, to maintain fairness in comparison_.

Our contributions are summarized as follows:

* We pioneer in addressing long-tail recognition by synthesizing images using diffusion models without relying on external data.

Figure 1: The samples generated by diffusion models improve long-tail classification on CIFAR100-LT, showing a correlation between FID and accuracy and a stronger correlation between the proportion of AID samples and accuracy. Our method significantly boosts classifier accuracy compared with others (left). Feature space visualization reveals that different diffusion models generate samples with varying distributions, and our model biases the generative process toward AID samples (right).

* Our research delves into the mechanisms underlying our approach, highlighting the significance of the generated AID samples. These samples emerge from a fusion of information from both head and tail classes, playing a crucial role in enhancing classifier performance.
* We introduce a novel loss function that enhances the performance of diffusion models on long-tailed datasets and biases them towards generating AID samples, thereby making the generation process more effective and efficient for classification.

Extensive experimental validation across CIFAR10-LT, CIFAR100-LT, and ImageNet-LT datasets demonstrates the superior performance of our method over existing approaches.

## 2 Related Work

Long-tailed recognitionLong-tailed recognition is a challenging and practical task (Cui et al. (2019); Zhou et al. (2020); Cao et al. (2019); Zhang et al. (2023); Zhu et al. (2024)), since natural data often constitute a squeezed and imbalanced distribution. The majority of traditional long-tailed learning methods can be viewed as (or special cases) of re-weighting (Cao et al. (2019); Kang et al. (2020); Zhong et al. (2021); Wang et al. (2024)) and re-sampling (Cui et al. (2019)), with more emphasis on the deferred tail class to seek an optimization trade-off. There are variants of them that adopt self-supervised learning (Zhu et al. (2023); Li et al. (2021)), theoretical analysis (Li et al. (2022); Menon et al. (2021); Yang et al. (2024)) and decoupling pipeline (Kang et al. (2020); Zhou et al. (2020)) to tackle long-tailed learning from various aspects, and they all achieve seemingly decent performance in downstream tasks.

One of the core difficulties in long-tailed learning is the _insufficiency of tail samples_. And recently, quite some works start to focus on this aspect by _involving more training samples through external knowledge_(Zhang et al. (2021); Ramanathan et al. (2020); Dong et al. (2022); Shi et al. (2023)). Nevertheless, the most distinct drawback of these works is that they either rely on _external data source_ or _strong model weights_. This condition can seldomly hold true in practical scenarios where only a handful of _specialized_ data are available and are secretly kept (consider some important military or medical data). We thus raise a natural question about long-tailed learning: _can we utilize the advantage of generating tail samples without resorting to any external data or model?_ That is, the whole process is done in an in-domain (also called held-in) manner. In this paper, we propose to adopt the off-the-shelf diffusion model to learn and generate samples from the data at hand.

Diffusion models and synthetic dataDiffusion models have been highly competitive in recent years (Ho et al. (2020); Song et al. (2020)), producing promising image quality in both unconditional and conditional settings (Dhariwal and Nichol (2021); Rombach et al. (2022); Ramesh et al. (2021)). Despite the predominant use in creating digital art, the application of diffusion models in scenarios of limited data remains under-explored. This paper affirms the utility of diffusion models in enhancing representation learning, particularly within the long-tailed learning framework, offering a novel insight into their application beyond conventional generative tasks.

The integration of synthetic data into deep learning, generated through methods like GANs Goodfellow et al. (2014); Isola et al. (2017) and diffusion models (Dhariwal and Nichol (2021); Rombach et al. (2022)), has been explored to enhance performance in image classification (Kong et al. (2019); Azizi et al. (2023); Zhang et al. (2024); Trabucco et al. (2023)), object detection (Zhang et al. (2023)), and semantic segmentation (Zhang et al. (2022, 2023)). These approaches often depend on substantial volumes of training data or leverage pre-trained models, such as Stable Diffusion, for high-quality data generation. Yet, the efficacy of generative models and synthetic data under the constraint of limited available data and in addressing imbalanced data distributions remains an unresolved inquiry. This paper specifically addresses this question, evaluating the viability of generative models and synthetic data in scenarios where data is scarce and imbalanced.

## 3 Method

### Preliminaries

For image classification, we have a long-tail dataset \(=\{(x_{i},y_{i})\}_{i=1}^{N},y_{i}\) with each \(x_{i}\) representing an input image and \(y_{i}\) representing its corresponding label from the set of all classes \(\). In the long-tail setting, a few classes dominate with many samples, while most classes have very few images, leading to a significant class imbalance. The classes in \(\) are ordered by sample count with \(|c_{1}||c_{2}|...|c_{M}|\), where \(|c_{j}|\) denotes the number of training samples in class \(c_{j}\) and \(|c_{1}||c_{M}|\). The ratio \(r=|}{|c_{M}|}\) is defined as the long-tail ratio. The goal of long-tail classification is to learn a classifier \(f_{}:\) capable of effectively handling the tail classes.

The naive idea is to train a generative model \(\) on the long-tail dataset \(\) and use the trained model to generate new samples and supplement the tail classes. Inspired by its superior performance, we select diffusion models as the generative model in our pipeline. In our approach, we follow the Denoising Diffusion Probabilistic Model (DDPM by Ho et al. (2020)) framework. Given a dataset \(=\{x_{i},y_{i}\}_{i=1}^{N}\), we train a diffusion model to maximize the likelihood of the dataset. At every training step, we sample a mini-batch of images \(_{0}\) from the dataset and add noise to obtain \(_{t}\),

\[q(_{t}_{0})=(_{t}}_{0},(1- _{t}))\,,\] (1)

where \(_{t}=_{i=1}^{t}(1-_{i})\) is calculated through pre-defined variance schedule \(\{_{t}(0,1)\}_{t=1}^{T}\). After training a diffusion model \(\) to get \(p_{}(_{t-1}_{t},t)\), we reverse the above process step by step to recover the original image \(_{0}\) from pure noise \(_{T}(,)\). The training objective is to reduce the gap between the added noise in forward process and the estimated noise in reverse process:

\[L_{}=_{t[1,T],_{0},_{t}}[\|_{t}-_{}(_{t}}_{0}+_{t}}_{t},t)\|^{2}]\,,\] (2)

where \(_{t}(,)\) is the noise added to original images and \(_{}\) is the noise estimated by the trainable model with parameters \(\). DDPM can be conditional by transforming \(y\) into a trainable class embedding and incorporating the label \(y\) directly as am input, similar to time \(t\). To improve the performance of DDPM on long-tailed dataset, several works (Qin et al. (2023), Zhang et al. (2024)) have been proposed to adjust the distribution of generated samples. CBDM adds a distribution adjustment regularizer at the loss term. This term is designed to promote the generation of samples for tail classes, which is defined as (where \(\) means stop gradient):

\[L_{}=|}_{y^{}} (\|_{}(_{t},t,y)-(_{}( _{t},t,y^{}))\|^{2}+\|\,(_{} (_{t},t,y))-_{}(_{t},t,y^{})\|^{2})\,.\] (3)

### DiffuLT: Diffusion model for Long-Tail recognition

**Diffusion model helps long-tail classification.** In this phase, a _randomly initialized_ diffusion model \(\) is trained to enrich the dataset. Preliminary experiments involve training a DDPM on a long-tailed dataset and using it to generate additional data. A threshold \(N_{t}\) is set, and for classes \(c_{j}\) with fewer than \(N_{t}\) samples, we generate the images to meet this threshold. This augmentation results in a collection of synthetic samples, \(_{}=\{(x_{i},y_{i})\}_{i=1}^{N_{}}\), where \(N_{}=_{c_{j}}(0,N_{t}-|c_{j}|)\) represents the total number of generated samples. These generated samples are then integrated with the original dataset, forming an augmented dataset \(_{}\), on which a classifier is trained to enhance classification performance.

We conducted experiments on CIFAR100-LT with an imbalance ratio of 100 and set \(N_{t}=500\) to supplement the data. The results, detailed in the second line of table 1, show a 5.5% accuracy increase for the classifier trained on \(_{}\) compared to the baseline. This improvement underscores the effectiveness of our straightforward method in boosting overall performance. Considering the generated samples (especially for tail classes) may be of lower quality due to limited data availability,

   Model & FID & Acc. (\%) \\  Baseline & - & 38.3 \\ DDPM & 7.76 & 43.8 \\ CBDM (\(=3\)) & 7.42 & 44.8 \\ CBDM (\(=2\)) & 6.82 & 46.0 \\ CBDM (\(=1\)) & 5.86 & 46.6 \\   

Table 1: FID of different generation models and their corresponding classifiersâ€™ accuracy.

   Model & \(p_{ID}\) & \(p_{AID}\) & \(p_{OOD}\) \\  DDPM & 39.1 & 21.2 & 39.7 \\ CBDM (\(=3\)) & 38.6 & 29.1 & 32.3 \\ CBDM (\(=2\)) & 40.2 & 33.5 & 26.3 \\ CBDM (\(=1\)) & 44.8 & 36.3 & 18.9 \\   

Table 2: Percentage of different types of generated samples for each model.

Class-Balancing Diffusion Models (CBDM) is employed to improve generation quality in long-tailed settings. By integrating \(L_{}\) and \(L_{}\) in training the model \(\) on \(\), the dataset is enhanced, and a classifier is trained as described previously. Subsequent testing on CIFAR100-LT reveals that the classifier achieves an accuracy of 46.6%, marking an 8.3% increase over the baseline, as noted in the final line of table 1.

**What samples are helpful? AID samples!** We adjusted the hyper-parameter \(\) in \(L_{}\) and evaluate models with varying FID scores. Results presented in table 1 show that accuracy improves as FID decreases. Lower FID scores indicate that generated samples more closely resemble the real data distribution. Notably, some generated samples clearly fail, while others correctly resemble their intended class. This observation motivates further investigation into the efficacy of samples.

Class 90 (truck) is selected randomly as a representative example in CIFAR100-LT. A baseline classifier (\(_{0}\)), trained exclusively on the original dataset \(\), is used to analyze the generated data. This classifier extracts features which are then visualized using t-SNE, as shown in fig. 2. The visualization reveals that samples generated via CBDM tend to be more centralized. For deeper analysis, we define the center \(f_{o}\) of a class's features as the average of the real data in feature space, and set the maximum Euclidean distance between two real samples' features as a threshold \(d_{f}\). We then define 3 types of the generated samples based on their distance to \(f_{o}\):

\[d_{i}=\|f_{i}-f_{o}\|_{2}:\{d_{i} d_{f},& \\ d_{f}<d_{i} 2d_{f},&\\ d_{i}>2d_{f},&.\] (4)

where ID denotes in-distribution samples, which closely match the patterns of the original data. We define and name approximately in-distribution (AID) samples, which exhibit slight deviations. OOD stands for out-of-distribution ones, which are significantly differing from the center. We summarize the composition of samples generated by each model in table 2. Notably, the CBDM model generates a lower proportion of OOD samples, consistent with its FID score. For evaluating the impact of each type, we train classifiers using only the ID, AID, and OOD samples generated by CBDM with \(=1\) respectively as \(_{}\), combined with \(\), and present the results in table 3. Surprisingly, classifiers trained with AID samples achieve the highest accuracy and show the greatest average improvement per sample. Based on this finding, our hypothesis is that _AID samples are the most beneficial in enhancing classifier performance_.

**Mechanisms behind the AID samples.** We conducted experiments to explore how AID samples enhance classifier performance and where their new and useful information originates. A diffusion model (CBDM with \(=1\)) is trained using images from tail classes (fewer than 100 samples), supplemented by a variable proportion \(p_{h}\) of head class images. This model generates samplesspecifically for tail classes with the proportion of AID samples \(p_{AID}\), and gets the performance of the corresponding classifier denoted as \(_{}\). The results, presented in table 4, show that at \(p_{h}=0\%\), relying solely on tail class images, \(p_{AID}\) is 25.8%, and \(_{}\) improves marginally to 26.0%, only 1% above the baseline. As \(p_{h}\) increases, both \(p_{AID}\) and \(_{}\) rise, peaking at \(p_{h}=100\%\). This trend illustrates the diffusion model's ability to transfer information from populous to underrepresented classes, effectively blending data across different classes into AID samples. Examples of the sample groups are displayed in fig. 3, where ID samples closely resemble real images, AID samples blend patterns from multiple classes, and OOD samples typically exhibit anomalies.

**Generation of AID samples**. How can we efficiently generate AID samples? While a filtering strategy can be used to collect AID samples, it is not the most efficient method. A more effective approach involves encouraging the generation model to specifically produce AID samples. Given that AID samples are defined by their distance from the center of real images in feature space, we can utilize the baseline classifier \(_{0}\) as a feature extractor to guide the generation of AID samples. Our goal is to encourage a controlled deviation within feature space. After \(T\) denoising steps, the deviation should ideally be within the range of \(d_{f}\) to \(2d_{f}\). Assuming that the deviation in each step is proportional to the noise strength, we introduce an additional term in the loss function to encourage small, stepwise deviations. We define the deviation at each step in feature space as

\[d_{t}=_{T}}}{_{t}}}\|_{0 }(_{0})-_{0}(_{0}+_{t}-_{}( _{t},t,y))\|_{2},\] (5)

where \(_{0}(_{0}+_{t}-_{}(_{t},t,y ))\) represents the de-noised images' feature. The new AID loss is then

\[L_{}=_{t[1,T],_{0},_{t}}\|d_{t }-d_{f}\|^{2}\,.\] (6)

where \(\) is a hyper-parameter and defaulted to 0.1. We incorporate this term into both \(L_{}\) and \(L_{}\) to train the generation model. After training, we use this model to generate data. During the generation process, we employ \(_{0}\) to filter out harmful OOD samples, resulting in \(_{}\). We then train the classifier using the combined dataset \(_{}\). Recognizing that generated data are less crucial than real images, we introduce a weighting term to the cross-entropy loss to adjust the influence of the generated samples:

\[L_{}=-_{(x,y,y_{g})_{ }}( y_{g}+(1-y_{g}))(x))}{_ {i=1}^{M}(f_{,c_{i}}(x))}\,,\] (7)

where \(\) controls the weight of generated samples and is set to 0.3 by default. \(y_{g}\) is an additional label assigned to each image \(x\), which distinguishes between generated and original samples. Specifically, \(y_{g}=1\) is used for generated samples, while \(y_{g}=0\) marks the original ones.

### Overall Pipeline and Discussion

Now we are ready propose a new pipeline called DiffuLT to address long-tail recognition. The pipeline is shown in fig. 4 with four steps:

* **Training:** Initially, we train a feature extractor \(_{0}\) and a conditional, AID-biased diffusion model \(\) using the original long-tailed dataset \(\) alone.

Figure 3: Examples of three groups of generated samples.

* **Generating:** We establish a threshold \(N_{t}\) and employ the trained diffusion model \(\) to generate and supplement samples. Using \(_{0}\), we filter out OOD samples, resulting in a refined dataset \(_{}\).
* **Training:** We then train a new classifier \(f_{}\) on the augmented dataset \(_{}\) using weighted cross-entropy, forming our final model.

Compared to traditional methods that focus primarily on training, ours not only enhances performance but is also reusable for model updates. Our method requires more training time, typically four times longer, to train the generation model and produce samples. However, our methods prove valuable when performance improvement is critical. Unlike typical data expansion methods, our approach offers both practical and theoretical benefits because it don't rely on any external dataset or model. For detailed analysis, please refer to the appendix B.

## 4 Experiment

### Experimental setup

**Datasets.** Our research evaluate three long-tailed datasets: CIFAR10-LT (Cao et al. (2019)), CIFAR100-LT (Cao et al. (2019)), and ImageNet-LT (Liu et al. (2019)). Following the methodology described in (Cao et al. (2019)), we construct long-tailed versions of the first two datasets by adjusting the long-tail ratio \(r\) to 100, 50, and 10 to test our method against various levels of imbalance.

**Baselines.** In our comparative analysis, we benchmark against a broad spectrum of classical and contemporary long-tailed learning strategies. The methods compared can be classified into multiple genres like re-weighting and re-sampling techniques, head-to-tail knowledge transfer approaches, data-augmentation, and so on. Some methods have issues such as unfair comparisons or implementation problems. We document both their results and our implementation outcomes in the appendix A.

**Implementation.** We set \(=0.1\), and \(=0.3\). The generation thresholds \(N_{t}\) for CIFAR10-LT and CIFAR100-LT were fixed at 5000 and 500, respectively. We employ ResNet-32 as the classifier backbone. For ImageNet-LT experiments, we set a generation threshold of \(N_{t}=300\). The classifiers were based on ResNet-10 and ResNet-50 architectures with \(=0.5\).

More details about the experimental setup are available in the appendix A.

### Experimental Results

**Generative Results.** We assess the efficacy of our specially designed loss function, detailed in table 5. This function improves the FID by reducing OOD samples, while also increasing the number of AID samples and the classifier's accuracy. Further experiments in table 6 highlight the necessity of our training loss. For benchmarking, we use a basic filtering strategy for CBDM, with all models generating samples to meet the threshold \(N_{t}\) for each class. The terms "Kept" and "G-Num" denote

Figure 4: The overall pipeline of our method DiffuLT.

the types of samples retained and the total number of samples generated before filtering, respectively. Our methods enhance the generation process's efficiency and achieve the highest accuracy.

**CIFAR100-LT & CIFAR10-LT.** We benchmark our approach against a range of methods on the CIFAR100-LT and CIFAR10-LT datasets, with results detailed in table 7. The results not shown in the original papers are indicated as "-" in the table. On CIFAR100-LT, our method surpasses competing models, achieving accuracy improvements of 13.2%, 12.4%, and 8.1% compared with the baseline for \(r=100\), 50, and 10, respectively. On CIFAR10-LT, our model also demonstrates strong competitiveness, enhancing accuracy by 14.3%, 12.1%, and 4.3% across the long-tail ratios, further validating the effectiveness of our method. Since our methods solely modify the training data, they can be easily integrated with other methods to achieve better results.

For CIFAR100-LT with an imbalanced ratio of 100, performance is also assessed across three categories: many (classes with over 100 samples), medium (classes with 20 to 100 samples), and few (classes with fewer than 20 samples). While our approach does not lead in the "Many" category, it excels in "Med." and "Few", significantly outperforming others in the "Few" group with a 29.7% accuracy -- 8.3% above the nearest competitor and 20.6% beyond the baseline.

**ImageNet-LT.** On the ImageNet-LT dataset, our methodology is evaluated against existing approaches, with results summarized in table 8. Utilizing a ResNet-10 backbone, our method registers a 50.4% accuracy, outperforming the nearest competitor by 4.5%. With ResNet-50, the accuracy further escalates to 56.4%, marking a substantial 14.8% enhancement over the baseline. Despite a slight decline in the "Many" category relative to the baseline, our approach excels in "Med." and "Few", with the latter witnessing a remarkable 33.6% improvement over the baseline. Our method can be combined with others to achieve enhanced results.

   Method & Kept & G-Num & Acc. (\%) \\  CBDM & All & 39,153 & 46.6 \\ CBDM & AID & 108,684 & 48.1 \\ CBDM & ID \& AID & 48,414 & 47.1 \\ Ours & All & 39,153 & 49.7 \\   

Table 6: Methods and types of retained samples, pre-filtering counts, and classification accuracy.

    &  &  &  \\  & 100 & 50 & 10 & 100 & 50 & 10 & Many & Med. & Few \\  CE & 38.3 & 43.9 & 55.7 & 70.4 & 74.8 & 86.4 & 65.2 & 37.1 & 9.1 \\ Focal Loss Lin et al. (2017) & 38.4 & 44.3 & 55.8 & 70.4 & 76.7 & 65.3 & 38.4 & 8.1 \\ LDAM-DRW Cao et al. (2019) & 42.0 & 46.6 & 58.7 & 77.0 & 81.0 & 88.2 & 61.5 & 41.7 & 20.2 \\ cRT Kang et al. (2019) & 42.3 & 46.8 & 58.1 & 75.7 & 80.4 & 88.3 & 64.0 & 44.8 & 18.1 \\ BBN Zhou et al. (2020) & 42.6 & 47.0 & 59.1 & 79.8 & 82.2 & 88.3 & - & - & - \\ RIDE (3 experts) Wang et al. (2020) & 48.0 & - & - & - & - & - & 68.1 & 49.2 & 23.9 \\ CAM-BS Zhang et al. (2021) & 41.7 & 46.0 & - & 75.4 & 81.4 & - & - & - & - \\ MisLAS Zhong et al. (2021) & 47.0 & 52.3 & 63.2 & 82.1 & 85.7 & 90.0 & - & - & - \\ DiVE He et al. (2021) & 45.4 & 51.1 & 62.0 & - & - & - & - & - & - \\ CMO Park et al. (2022) & 47.2 & 51.7 & 58.4 & - & - & **70.4** & 42.5 & 14.4 \\ SAM Rangwani et al. (2022) & 45.4 & - & - & 81.9 & - & - & 64.4 & 46.2 & 20.8 \\ CUDA Ahn et al. (2023) & 47.6 & 51.1 & 58.4 & - & - & - & 67.3 & 50.4 & 21.4 \\ CSA Shi et al. (2023) & 46.6 & 51.9 & 62.6 & 82.5 & 86.0 & 90.8 & 64.3 & 49.7 & 18.2 \\ ADRW Wang et al. (2024) & 46.4 & - & 61.9 & 83.6 & - & 90.3 & - & - & - \\ H2T Li et al. (2023) & 48.9 & 53.8 & - & - & - & - & - & - \\  DifftLT & 51.5 & 56.3 & 63.8 & 84.7 & 86.9 & 90.7 & 69.0 & 51.6 & 29.7 \\ DifftLT + BBN & 51.9 & 56.7 & 64.0 & 85.0 & 87.2 & **90.9** & 69.5 & 51.9 & 30.2 \\ DifftLT + RIDE (3 experts) & **52.4** & **56.9** & **64.2** & **85.3** & **87.3** & 90.9 & 70.3 & **52.1** & **30.7** \\   

Table 7: Results on CIFAR100-LT and CIFAR10-LT datasets. The imbalance ratio \(r\) is set to 100, 50 and 10. The highest-performing results are in bold, with the second-best in underline. Additionally, we present the results for different groups (many, medium, and few) in CIFAR100-LT with \(r=100\).

   Method & FID & \(p_{ID}\) & \(p_{AID}\) & \(p_{OOD}\) & Acc. (\%) \\  DDPM & 7.76 & 39.1 & 21.2 & 39.7 & 43.8 \\ CBDM & 5.86 & 44.8 & 36.3 & 18.9 & 46.6 \\ Ours & 5.37 & 40.7 & 50.1 & 9.2 & 49.7 \\   

Table 5: FID of diffusion model, proportion of samples, and corresponding classifier accuracy 

### Ablation Study

**Different modules in our pipeline.** Our methodology comprises several critical components: generated samples (using CBDM), AID-biased loss, filtering, and weighted cross-entropy. We conduct ablation experiments on CIFAR100-LT with \(r=100\). The results, presented in table 9, highlight the crucial role each component plays in enhancing the overall performance. Notably, the generated samples and AID-biased loss are the most influential factors.

**Hyper-parameters.** We adjust the parameters \(\) in the weighted cross-entropy and \(\) in \(L_{}\) on CIFAR100-LT with \(r=100\) and evaluate the classification results. These results are summarized in table 10. Through iterative adjustments, we find that the optimal performance, a 51.5% classification accuracy, is achieved when \(=0.3\). Similarly, the best setting for \(\) is determined to be 0.1. Consequently, we establish \(=0.3\) and \(=0.1\) as the default settings for our method.

## 5 Conclusion

In this research, we proposed a novel, data-centric approach designed to address the challenges of long-tail classification. We defined and identified AID (approximately in-distribution) samples as the important ones. We then revised a diffusion model trained with an AID-biased loss term on only the original dataset for the purpose of generating more AID samples, thereby significantly enriching the dataset. Following sample generation, we trained a classifier on this enhanced dataset and employed a weighted cross-entropy loss. Our method has shown to deliver competitive performance, highlighting its efficacy in real-world applications. The experiments conducted as part of this study notably emphasize the critical role played by AID samples and their significant impact.

We propose that this approach introduces a new paradigm for tackling long-tail classification challenges, offering a substantial complement to existing methodologies. It provides a robust frameworkthat can be adapted to various scenarios where performance is a critical factor. Despite its advantages, the training of the diffusion model and the generation of samples are time-consuming. The need for optimization in training and generation speeds represents a limitation of our current method. We will leave this point as future work to further improve the effectiveness and efficiency of our method.