# Paralinguistics-Aware Speech-Empowered Large Language Models for Natural Conversation

Heeseung Kim\({}^{1}\)  Soonshin Seo\({}^{2}\)  Kyeongseok Jeong\({}^{2}\)  Ohsung Kwon\({}^{2}\)  Soyoon Kim\({}^{2}\)  Jungwhan Kim\({}^{2}\)  Jaehong Lee\({}^{2}\)  Eunwoo Song\({}^{2,4}\)  Myungwoo Oh\({}^{2}\)  Jung-Woo Ha\({}^{2,3}\)  Sungroh Yoon\({}^{1,4,5}\)  Kang Min Yoo\({}^{2,3,4}\)

\({}^{1}\)Data Science and AI Lab, Department of ECE, Seoul National University

\({}^{2}\)NAVER Cloud \({}^{3}\)NAVER AI Lab

\({}^{4}\)Artificial Intelligence Institute, Seoul National University

\({}^{5}\)ASRI, INMC, ISRC, and Interdisciplinary Program in AI, Seoul National University

###### Abstract

Recent work shows promising results in expanding the capabilities of large language models (LLM) to directly understand and synthesize speech. However, an LLM-based strategy for modeling spoken dialogs remains elusive, calling for further investigation. This paper introduces an extensive speech-text LLM framework, the Unified Spoken Dialog Model (USDM), designed to generate coherent spoken responses with naturally occurring prosodic features relevant to the given input speech without relying on explicit automatic speech recognition (ASR) or text-to-speech (TTS) systems. We have verified the inclusion of prosody in speech tokens that predominantly contain semantic information and have used this foundation to construct a prosody-infused speech-text model. Additionally, we propose a generalized speech-text pretraining scheme that enhances the capture of cross-modal semantics. To construct USDM, we fine-tune our speech-text model on spoken dialog data using a multi-step spoken dialog template that stimulates the chain-of-reasoning capabilities exhibited by the underlying LLM. Automatic and human evaluations on the DailyTalk dataset demonstrate that our approach effectively generates natural-sounding spoken responses, surpassing previous and cascaded baselines. Our code and checkpoints are available at https://github.com/naverai/usdm.

## 1 Introduction

Large language models (LLMs) have gained significant traction thanks to emergent capabilities , such as few-shot in-context learning, complex reasoning , and instruction-following . These remarkable discoveries led to chat-enabled LLMs and generative personal assistants . However, text-based agents are limited in usability due to their medium of interaction. Ideally, speech-enabled LLMs would recognize the user's emotional state or subtle nuance and generate spoken responses with prosody most appropriate to the user's context. Although automatic speech recognition (ASR) and text-to-speech (TTS) systems can be easily employed, the linguistic discrepancy between speech and text causes dialog inefficiencies and result in sub-optimal user experience . As such, systematically integrating the speech modality into LLMs can unlock speech interactivity while retaining LLMs' powerful capabilities.

Recent advances have also spurred the idea of large foundational models (LFM) for other modalities (e.g., vision, speech, etc.) , unifying LLMs with the sensory spaces. In the vision domain, numerous works have explored interfacing pretrained language models with the visual modality . More recently, Yu et al.  proposed a vision language model (VLM) based on a pretrained LLM that directly generates discrete vision tokens, which can be decoded into high-fidelity images. Such work shows that autoregressive language models can model the vision modality.

In the speech domain, while earlier work focused on text-less speech modeling , recent work has either taken inspiration from the LLM architecture to achieve speech synthesis  or incorporate pretrained language models into speech-understanding tasks , where the model is limited to text outputs. More recent work explores the possibility of empowering pretrained LLMs to autoregressively generate discrete speech tokens for speech translation  and speech-instruction following tasks . Despite these successes, more work is needed to understand whether LLMs are capable of generating speech, understanding, and incorporating paralinguistics that are appropriate and natural for the social context, especially in spoken dialog settings.

We introduce the Unified Spoken Dialog Model (USDM), a novel LLM-based approach for modeling spoken dialogs in an end-to-end fashion. We propose a novel speech-text pretraining scheme that promotes learning cross-modal distributional semantics, which is vital for imbuing LLMs with the ability to generate coherent speeches in spoken dialog modeling. In particular, based on the observation that any subsample of either speech or text in corresponding speech-text pairs form two types of relationships with the other modality (right side of Figure 1), we formulate a large number of combinations of training objectives that theoretically benefits all speech-text tasks, including spoken dialog modeling. We then fine-tune our pretrained speech-text model with spoken dialog data, breaking the speech-to-speech modeling problem into intermediary steps that are easier for the underlying pretrained LLM to handle (Figure 1). To enhance the effects of our speech-text pretraining and spoken dialog modeling, in addition, we adopt the prosody-infused speech tokenization scheme based on the discovery that the speech token, previously used to convey semantic information, also contains prosody information.

We demonstrate that USDM outperforms baselines in spoken dialog modeling for the DailyTalk dataset. We further validate the effectiveness of our pretraining and fine-tuning schemes through comprehensive ablation studies. Along with various analyses, we highlight the capabilities of USDM with diverse samples on our demo page.2 Our contributions are as follows.

* We propose a unified pretraining strategy for modeling the comprehensive relationship between the speech and text modalities that is especially effective for downstream speech-to-speech spoken dialog generation.
* We present an extensive spoken dialog modeling framework detailing the discrete speech tokenization scheme utilizing a pair of a prosody-infusing encoder and a decoder. Additionally, we propose an LLM-based modeling strategy for generating natural-sounding and semantically coherent dialog responses in an end-to-end fashion.
* Our work establishes the foundation for speech-enabled chat-based LLMs, showcasing a prototype that not only maintains the LLM's ability to generate dialog responses but also enhances LLM with speech-interaction capabilities.

Figure 1: Overview of our spoken dialog modeling approach (Left). All possible self-supervised learning objectives from our speech-text pretraining scheme. (Right)

Related Work

Discrete Speech Representations.To construct spoken language models (SLM), various discrete speech representations have been utilized in previous works [17; 20; 28; 29; 30]. These representations are primarily categorized into two types: tokens based on speech self-supervised representations [17; 30] and neural audio codecs .

A discrete token based on speech self-supervised representation [17; 31] is obtained by \(k\)-means clustering of the intermediate representation from a speech self-supervised model. These tokens, often called acoustic units, are typically encoded with a frequency range of 25Hz to 50Hz. The amount of speech information within the compressed discrete speech token is determined by the number of clusters, denoted as \(k\). With a relatively small value for \(k\), many works have preserved the semantic information in the tokens and utilized these to construct SLMs [32; 33].

Neural audio codecs, another type of discrete token, capture both semantic and paralinguistic information of speech [34; 35; 36; 37; 38]. A speech encoder and decoder are trained using an autoencoder architecture with residual vector quantizer for the encoder output . This representation includes most of the perceptual information of audio and is widely used for audio synthesis [20; 40; 41; 42].

Spoken Language and Dialog Models.Many studies have recently explored spoken language modeling to address a variety of tasks involving speech and text [23; 43; 44; 45]. Various works tackle tasks such as automatic speech recognition [27; 46; 47; 48; 49], spoken question answering [50; 51; 52], and speech-to-text translation [27; 48; 53], which process speech as input and output text. Conversely, there are also emerging works focused on tasks like speech synthesis [20; 26; 28; 54; 55], where text is used as input to generate speech output. Early SLMs that process speech as input and output are trained solely based on speech data without language models [17; 19]. With the advancement of LLMs, several studies aim to construct SLMs that extend language models to handle both speech input and output. These studies are primarily proposed for speech modality pretraining [30; 56; 57], or introduced in specific tasks such as speech-to-speech translation [24; 58; 59; 60] and spoken conversation modeling [25; 61].

Recently, several works have been proposed for spoken dialog modeling with speech input and output [19; 62; 25]. Nguyen et al.  develop a decoder-only transformer model trained from scratch, designed for modeling conversations between two speakers. In contrast, Lin et al.  adopt a cascaded approach for spoken dialog modeling that consists of separate ASR, an LLM-based emotion-aware text dialog model, and emotional TTS components. Zhang et al.  build SLMs on top of a pretrained LLM with objective functions designed for ASR and TTS tasks.

Among the previous works, end-to-end pipelines [19; 25] that focus solely on speech-only training or leverage simple cross-modal objectives for speech-text pretraining fail to fully utilize the capabilities of pretrained language models. Additionally, cascaded models , which use separate ASR and TTS for spoken dialog, need explicit labels to incorporate paralinguistic features. This label dependency makes data collection challenging and limits the models to representing label-definable non-verbal cues. Furthermore, the error propagation inherent in the cascaded pipeline  increases their susceptibility to compounded errors.

## 3 Our Approach

In this section, we describe the components that enable coherent and prosodic spoken dialog modeling, distinguishing our research from previous works. We first explain the discrete speech representation used for spoken dialog modeling in Section 3.1, demonstrating its suitability for prosody modeling. We then propose a unified speech-text pretraining scheme that extends the capabilities of the pretrained LLM into the domain of spoken language modeling in Section 3.2. Finally, in Section 3.3 and 3.4, we introduce USDM, a spoken dialog model fine-tuned with a multi-step spoken dialog template, and the speech decoder that restores the output speech token to a raw waveform.

### Speech-to-Unit Encoder

To model natural speech conversations, the speech representation must contain not only the content of the speech but also paralinguistic features such as emotions, which are crucial for conversation. Weadopt acoustic units as speech tokens that are derived from \(k\)-means clustering of a self-supervised model's intermediate speech representation, which is known to predominantly capture content and pronunciation . The information captured by an acoustic unit token varies depending on the number of clusters; the greater the number of clusters, the more encoded information. Hence, among publicly available schemes, we consider the acoustic unit tokenization scheme with the largest vocabulary size, \(k=10,000\). We then analyze whether the tokens derived from this scheme contain non-verbal information.

The acoustic unit extractor used in SeamlessM4T , first resamples the speech to a sampling rate of 16kHz and then feeds it to the XLS-R , obtaining 50Hz intermediate continuous representations. Unit sequences are extracted by clustering these representations into 10,000 clusters, determining the vocabulary size of the speech tokens. Using this unit extractor, we conduct two experiments to investigate features captured in the unit sequence besides the semantic content. First, we perform unit emotion recognition tasks with speech emotion recognition data to ascertain whether the unit sequences contain paralinguistic information. Next, we train a separate unit-to-speech reconstruction model and use this model to compare the original and reconstructed speech, investigating the information encoded in the unit sequences.

For the unit emotion recognition task, we train 3-layer transformer-based emotion classifiers using acoustic units on CREMA-D , which is a speech emotion recognition dataset with six emotion categories. If the units lack paralinguistic information, the classification accuracy would approximate the probability of random guessing, which is \(16.6\%\). However, we observe that the classification accuracy is \(60.8\%\), indicating that the acoustic units contain emotional cues.

To further investigate the paralinguistic information contained in the units, we train a separate unit-to-speech reconstruction model using 54,000 hours of speech data. The reconstruction model is trained using the architecture of Voicebox, one of the zero-shot stochastic TTS models . Unlike Voicebox, which takes text and reference speech as inputs for adaptation, our model is trained to generate speech solely from a unit sequence without any reference speech. The comparison between the original audio and the speech reconstructed from the extracted units shows that while the timbre and absolute pitch of the reconstructed speech differ, the pitch variation has a similar trend, as shown in Figure 2. This implies a crucial role in conveying non-verbal characteristics such as emotions, which closely match the original audio. Additionally, we have uploaded samples of several ground truth audios and corresponding reconstructed audios on our demo page.

Through these two experiments, we confirm that the acoustic units, commonly known to primarily encode semantic information, also contain a significant amount of paralinguistic information, such as emotions and pitch variations. We adopt this speech tokenization scheme for our speech-text pretraining and spoken dialog fine-tuning to help capture non-verbal cues in spoken conversations. More detailed descriptions of these experiments are provided in Appendix A.3.1 and A.3.3.

### Unified Speech-Text Pretraining

In this section, we introduce a unified speech-text pretraining scheme that extends pretrained LLM to speech-text cross-modality. Our overall speech-text pretraining scheme is in Figure 3.

Figure 2: Pitch contour of the original audio and the audio reconstructed from extracted acoustic units. Due to the stochastic nature of the reconstruction model, we attempt reconstruction twice, demonstrating that the pitch variation closely mirrors the ground truth.

For pretraining the speech-text model, we utilize Mistral-7B  as a pretrained LLM. To its existing vocabulary, we add 10,000 unit tokens and 2 special tokens, which will be described later, reinitializing only the embedding weights of these new tokens. We pretrain the speech-text model with approximately 87,000 hours of English ASR data. Each \(<\)speech, text\(>\) pair is used to create an interleaved speech-text sample \(_{j}=i_{1,j},i_{2,j},...,i_{||_{j}||,j}\) to model various cross-modal relationships. Here, \(i_{k,j}\) can be either an acoustic unit token, a text token, or a special token, and we construct the sequence \(_{j}\) with a proposed per-sample speech-text interleaving method in the following paragraph. Given the dataset \(=\{_{1},_{2},...,_{|| ||}\}\), the objective of our pretraining scheme is as follows:

\[()=-_{j=1}^{||||}_{k=1}^{||_{j}||} p(i_{k,j}|i_{<k,j};),\] (1)

where \(\) refers to the parameters of LLM and the embedding weights of the newly added tokens.

When constructing a spoken language model by extending pretrained LLM, relying on a specific task, such as ASR , TTS , uni-modal  and cross-modal continuation task , despite its large amount of dataset, may limit the model's capabilities to only those predefined relationships. To build a comprehensive speech-text model capable of both receiving and generating speech, we reinterpret the cross-modal relationship in terms of continuation or correspondence as shown in Figure 1. Our proposed method, which focuses on this redefined relationship, is capable of generating diverse speech-text interleaved sequences, ensuring the model can handle complex speech-text interactions.

**Speech-Text Alignment Extraction** We first extract word-level alignments of the speech and its transcript using the Montreal Forced Aligner . These alignments yield speech time intervals for each word, which we then convert into index intervals of unit sequences at a resolution of 50Hz.

**Pair-wise Segmentation and Segment-wise Main Modality Random Selection** Using the intervals, we divide each unit and text pair into \(N\) segments. Subsequently, from each of these segments, we randomly sample data of only one modality, either unit or text. A large value of \(N\) may lead to each segment containing short acoustic units and text sequences, which poses challenges in modeling unimodal text and unimodal unit sequences. To mitigate this issue, we dynamically set the value of \(N\) based on the speech duration, \(N= S/10+1\), where \(S\) is the speech length measured in seconds.

**Sub Modality Random Insertion and Special Token Insertion** This segmentation and selection process allows us to generate a unified cross-modal interleaved sequence with continuation relationships.

Figure 3: The overall speech-text pretraining scheme.

For correspondence relationship modeling, data from the non-selected modality in each segment is inserted with a \(50\%\) probability after the pre-selected modality data. Additionally, to indicate the relationship between speech and text tokens, we introduce two special tokens: <|correspond|> and <|continue|>. The former indicates that a token of the corresponding remaining modality will follow, while the latter indicates that a token of the subsequent position will follow. These tokens are added to the sequence only where the modality of the data changes.

Through this procedure, we can obtain interleaved speech-text sequences \(\{_{j}\}_{j=1,,||||}\). These sequences enable our speech-text model to perform not only unimodal modeling but also comprehensive cross-modal modeling. These interleaved sequences are utilized in Eq. 1 for the pretraining.

### Unified Spoken Dialog Model

We construct the Unified Spoken Dialog Model (USDM) by fine-tuning our speech-text model with spoken dialog data, with an overview presented as shown in Figure 1. The basic template for spoken dialog fine-tuning involves directly modeling the response speech tokens from the input speech tokens. However, we adopt a template designed to fully exploit the capabilities of the pretrained LLM.

Inspired by the step-by-step reasoning mechanism employed by LLMs , existing works [24; 25; 57] use text to bridge the speech. Similarly, instead of directly modeling output speech from input speech, our model transcribes the speech, generates the response text, and produces the corresponding speech in an end-to-end pipeline. The insertion of the text-related tasks between speech inputs and outputs allows the model to benefit from the pretraining LLM and chained reasoning over the intermediary modality . Since each stage in the pipeline attends to all input and output tokens generated in prior stages, our approach is more robust against transcription errors and better at generating contextually relevant spoken responses than if it were carried out in independent modules (i.e., the cascaded approach), which we will discuss further in Section 4.3.

The supervised fine-tuning template we use is shown in Figure 5 in the Appendix. We calculate the loss using Eq. 1 only for the input transcript, answer text, and answer unit part, as highlighted in Figure 5.

### Unit-to-Speech Decoder

We train the unit-to-speech model using the Voicebox architecture  to reconstruct speech from units. Voicebox is a zero-shot TTS model that takes text and reference speech as inputs to perform personalized TTS. Unlike the reconstruction model used in Section 3.1, we leverage not only unit sequences but also reference speech to adapt and perform zero-shot unit-to-speech reconstruction. Our model utilizes the reference speech and the paralinguistic features contained in the units to generate prosodic spoken responses of the target speaker. For clarity, we refer to this unit-to-speech model as unit-Voicebox. More details of our decoder are in Appendix A.3.3.

## 4 Experiments and Results

### Model Comparisons

#### 4.1.1 Training Details and Baselines

We compare USDM to 3 baselines, From Scratch, Cascaded, and SpeechGPT , on DailyTalk . DailyTalk comprises 20 hours of spoken dialog data with a sampling rate of 22,050 Hz, involving one male and one female, and we describe further details in Appendix A.4.1. We also present the models used for each component of USDM and the baselines in Table 4 in the Appendix.

**USDM.** For speech-to-unit module, we adopt the official checkpoint of XLS-R  and a quantizer with \(k=10,000\), trained on 436K hours of multilingual speech data. As a speech decoder, we follow the architecture and hyperparameters of Le et al.  and train the unit-Voicebox on the English subset of Multilingual LibriSpeech  and GigaSpeech  for 10 epochs using 64 NVIDIA A100-40GB GPUs, with a batch size of 256. We use the Adam optimizer  with a learning rate of \(10^{-4}\). We utilize the official checkpoint of BigVGAN  as our vocoder.

Our proposed unified speech-text pretraining is conducted using 512 NVIDIA A100-40GB GPUs, with a global batch size of 1,024 for 8,000 iterations. For pretraining, we utilize approximately 87,000 hours of English ASR data; the English subset of Multilingual LibriSpeech , People's Speech , GigaSpeech , Common Voice 15.0 , and the English subset of Voxpopuli . The data used for pretraining is packed to a maximum sequence length of 8,192. For spoken dialog modeling, we fine-tune a speech-text model with a global batch size of 64 for 5 epochs. We use linear learning rate scheduling with a peak learning rate of \(2 10^{-5}\) for both pretraining and fine-tuning.

**From Scratch.** The From Scratch model is nearly identical to the USDM but excludes speech-text pretraining. Specifically, we fine-tune the pretrained Mistral-7B model directly on spoken dialog data, with the hyperparameters identical to those of the USDM.

**Cascaded.** We include a Cascaded model, which employs separate ASR and TTS models, as a baseline for comparison. For the ASR model, we use the official checkpoint of _whisper-large-v3_, which is trained on 5M hours of speech data. For the speech synthesis model, we train Voicebox with text input using the same hyperparameters and datasets as unit-Voicebox. As the LLM, we utilize the transcript of the spoken dialog dataset to create text dialog data and fine-tune the Mistral-7B on this data using the same hyperparameters as the USDM.

**SpeechGPT.** For SpeechGPT , we use the official implementations and checkpoints for the speech-to-unit module, spoken language model, and speech decoder module. Specifically, we fine-tune _SpeechGPT-7B-cm_, a pretrained speech-text model, with DailyTalk for a fair comparison.

#### 4.1.2 Evaluation and Comparison Results

We conduct various evaluations on the spoken responses generated for the given spoken dialogs. When generating samples for evaluation, we adopt a sampling scheme with top_k \(=40\), top_p \(=0.7\), and temperature \(=0.3\), except for SpeechGPT, where we use their own strategy. For Voicebox and unit-Voicebox, we utilize the speech from the previous turn as the reference speech. While SpeechGPT generates audio at 16kHz, other models synthesize speech at 22,050Hz. For a fair comparison, all audio samples are resampled to 16kHz and volume normalized to -27dB for evaluation.

To compare the overall preference of our model and the baselines, we conduct a human preference test via Amazon Mechanical Turk. Given a randomly selected 50 spoken dialogs from the test split of a dataset, we instruct the evaluators to compare the spoken response of our model and the baseline, considering the comprehensive aspects such as naturalness, prosody, and semantic coherence. To evaluate the prosody and the naturalness, we additionally measure the 5-scale prosody mean opinion score (P-MOS) and the 5-scale mean opinion score (MOS) through Amazon Mechanical Turk. As explained in Section 3.3, our model first generates the text to be spoken before generating a spoken response, which allows us to fix the content of the generated speech by predetermining the text. Unlike the aforementioned human preference test, to focus solely on the prosody and naturalness, respectively, we provide the ground truth response text to the model to ensure consistency in the content of the output speech, thereby preventing difficulties in evaluations that may arise from variations in content. Instructions and detailed descriptions of our evaluations are in Appendix A.4.2.

Furthermore, to evaluate the content appropriateness of spoken responses, we first generate the spoken responses of all models given the spoken dialogs for the entire test set. We then pass these samples through the ASR model, _whisper-large-v3_, to calculate METEOR and ROUGE-L scores, which are widely used in various NLP tasks such as text summarization and are commonly employed to measure performance in dialog modeling . We also conduct a GPT-4-based  preference test  between the transcribed texts of our model and all baselines.

We also measure the Word Error Rate (WER) for the speech-to-text part and text-to-speech part of each model. For the speech-to-text part (STT WER), we calculate the WER across the entire test set. We use the outputs from the _whisper-large-v3_ model for the Cascaded baseline, while the remaining end-to-end pipelines are assessed using the intermediate transcribed text of each model. For the text-to-speech part (TTS WER), similar to our MOS and P-MOS evaluations, we calculate the WER using generated samples given the randomly selected 50 spoken dialogs and the corresponding ground truth written-form response. We generate each spoken response 5 times and report the average WER. For measuring TTS WER, we utilize the _whisper-large-v3_ model as the ASR model.

The results are presented in Table 1 and 2. In human preference tests that consider comprehensive factors, our model is preferred similarly to the Ground Truth and demonstrates superior preferencescompared to the baselines (\(p\)-value \(<0.05\) from the Wilcoxon signed-rank test). For the semantic aspect, our USDM outperforms the baselines in both quantitative evaluations and the GPT-4-based preference test (\(p\)-value \(<0.05\)). We also observe that our model surpasses the baselines in the P-MOS evaluations (\(p\)-value \(<0.05\)), which measure the prosody naturalness of the speech given spoken dialog. Notably, the USDM shows superior prosody compared to the Cascaded model. These results demonstrate that our model effectively incorporates prosody information in the spoken language model and is capable of generating spoken responses with content well-aligned to input speech.

We also confirm that cross-modal pretraining is essential to leverage the capabilities of LLM. We observe that the From Scratch model, which directly models spoken dialog without pretraining, tends to overlook the bridging text and generates a spoken response that does not correspond to the pre-generated written response, thus negatively impacting its performance. This results in worse TTS and STT WERs and adversely affects the P-MOS and MOS, which are based on the prosody and naturalness of the spoken response given the transcript. This result indicates the difficulty of transferring the capabilities of text models to spoken dialog modeling without cross-modal pretraining.

### Ablation Studies

#### 4.2.1 Pretraining Schemes

In this section, we compare the effects of correspondence and continuation modeling, which are crucial to our pretraining method. We consider three additional pretraining schemes. **Setup 1** uses an interleaved unit-text sequence without a correspondence relationship, relying solely on continuation, similar to previous works . **Setup 2** utilizes data that maintains only a correspondence relationship, as seen in Zhang et al. . **Setup 3** is similar to our fine-tuning approach, interleaving speech with its transcript and subsequent text and speech, as proposed in Nachmani et al. . All setups are trained in the same way as our speech-text pretraining, with details in Section A.5.2.

We evaluate these pretrained models on sequence modeling and spoken dialog modeling tasks. Performance is first assessed by measuring perplexity (PPL) of various speech-text sequences from the test-clean subset of the LibriSpeech dataset . We construct six types of interleaved sequences: unimodal sequences for both unit (1) and text (2), sequences with unit followed by their corresponding text (3) and vice versa (4) to evaluate correspondence relationships, and sequences generated by dividing the unit and text in half, combining the first half's unit with the remaining half's text (5), and the first half's text with the remaining unit (6) for assessing continuation. We then calculate the average PPL for all combinations by taking the logarithm of each subsequent modality's PPL within each sequence type, averaging these logarithmic values, and then applying

    &  &  \\   & _win_ & _tie_ & _lose_ & **MOS** & **P-MOS** \\  Ground Truth & \(45.9\%\) & \(8.0\%\) & \(46.1\%\) & \(4.51 0.05\) & \(4.35 0.05\) \\ USDM & \(-\) & \(-\) & \(-\) & \(4.31 0.07\) & \(4.31 0.06\) \\ Cascaded & \(55.3\%\) & \(4.9\%\) & \(39.8\%\) & \(4.26 0.07\) & \(4.22 0.07\) \\ From Scratch & \(53.3\%\) & \(7.6\%\) & \(39.1\%\) & \(3.71 0.11\) & \(3.65 0.10\) \\ SpeechGPT  & \(53.8\%\) & \(6.9\%\) & \(39.3\%\) & \(4.08 0.09\) & \(4.04 0.08\) \\   

Table 1: Human evaluation results of our model and the baselines. We report the MOS and P-MOS scores with a 95% confidence interval.

    &  &  \\   & _win_ & _tie_ & _lose_ & **METEOR** & **ROUGE-L** & **STT** & **TTS** \\  Ground Truth & \(32.7\%\) & \(19.6\%\) & \(47.7\%\) & \(-\) & \(-\) & \(-\) & \(2.2\%\) \\ USDM & \(-\) & \(-\) & \(-\) & \(13.1\) & \(15.7\) & \(7.4\%\) & \(2.0\%\) \\ Cascaded & \(42.7\%\) & \(24.6\%\) & \(32.7\%\) & \(12.5\) & \(15.0\) & \(3.8\%\) & \(1.3\%\) \\ From Scratch & \(79.7\%\) & \(10.1\%\) & \(10.2\%\) & \(8.6\) & \(10.6\) & \(58.1\%\) & \(64.0\%\) \\ SpeechGPT  & \(61.0\%\) & \(13.1\%\) & \(25.9\%\) & \(9.9\) & \(11.8\) & \(12.4\%\) & \(23.2\%\) \\   

Table 2: GPT-4 evaluation and quantitative results of our model and the baselines.

the exponential function. For spoken dialog modeling, we fine-tune each model with DailyTalk and measure STT WER, TTS WER, METEOR, and ROUGE-L, as described in Section 4.1.2.

We present the average PPL of each modality in Table 3 and the PPL for each combination in Table 9 in the Appendix. Our model demonstrates superior average PPL across both modalities. Focusing solely on either correspondence or continuation relationships, or on specific templates tends to make the model specialize in certain objectives but hinders its ability to model diverse relationships effectively. Our proposed unified speech-text pretraining scheme performs uniformly well without being overly focused on specific relationships. We also show in Table 3 that our speech-text model is beneficial to spoken dialog modeling, as evidenced by the WER, METEOR, and ROUGE-L scores. Particularly, Setup 1, which lacks correspondence relationship pretraining, exhibits significantly higher STT and TTS WERs, resulting in deteriorated semantic performance in spoken responses.

#### 4.2.2 Fine-tuning Schemes

As explained in Section 3.3, USDM first models the input and output text as a bridge when given a speech input before generating the spoken response. To demonstrate this approach, we train a spoken dialog model that models the speech output directly from the speech input (S1 \(\) S2). We evaluate the generated response speech through METEOR and ROUGE-L scores with the same samples described in Section 4.1.2, and the results are shown in Table 3. We find that intermediate text modeling in spoken dialog generation helps generate appropriate spoken responses. This confirms that the process of generating text before speech leverages the capabilities of the pretrained model effectively.

### Analysis on Input Modality

As seen in Table 2, the Cascaded model exhibits a lower ASR WER compared to USDM. This is due to the separate ASR model used in the Cascaded model, _whisper-large-v3_, which has been trained on approximately 5 million hours of speech data. However, in terms of the semantics of the spoken response, our model outperforms the Cascaded model.

Similar to previous works that show the advantages of end-to-end pipelines over cascaded approaches in several tasks , USDM leverages input speech to generate more semantically coherent answers. To empirically verify that our generated text responses utilize information from both the preceding transcript and the input speech, we plot the attention maps for each layer, as illustrated in Figure 4. We calculate the probability that each token in the generated response attends to each token in the input unit sequence and corresponding transcript by averaging the probabilities across all heads of the attention modules for each layer. Subsequently, we aggregate these probabilities for input unit tokens to compute a cumulative probability for the speech input, and similarly for text tokens relative to the transcript. As shown in Figure 4, our generated tokens attend not only to the transcribed text but also to the speech input, indicating that our model benefits from the speech input.

In Figure 4, we also observe that the generated responses notably attend to the transcribed text. To analyze the impact of more accurate transcription on model performance, we substitute the model-generated input transcript with the ground truth transcript in the middle of the inference of USDM and measure the METEOR and ROUGE-L scores for the generated spoken responses. The measured scores are 13.6 and 16.2, respectively, surpassing our model's previous results of 13.1 and 15.7 in Table 2. This confirms that enhancing the accuracy of unit-to-text conversion in USDM also improves the semantic coherence of the spoken responses.

    &  &  \\   & **Text PPL** & **Unit PPL** & **STT WER** & **TTS WER** & **METEOR** & **ROUGE-L** \\  Ours & \(6.886\) & \(4.813\) & \(7.4\%\) & \(2.0\%\) & \(13.1\) & \(15.7\) \\ Setup 1 & \(14.485\) & \(5.261\) & \(57.8\%\) & \(82.1\%\) & \(8.9\) & \(10.6\) \\ Setup 2 & \(31.679\) & \(5.619\) & \(11.2\%\) & \(2.5\%\) & \(12.5\) & \(15.1\) \\ Setup 3 & \(21.392\) & \(5.146\) & \(7.3\%\) & \(2.0\%\) & \(12.7\) & \(15.4\) \\  S1 \(\) S2 & \(-\) & \(-\) & \(-\) & \(-\) & \(6.5\) & \(7.7\) \\   

Table 3: Results of the ablation studies on the pretraining and fine-tuning schemes. For PPL, we report the average PPL for each modality across the six combinations described in the text.

## 5 Conclusion

In this work, we presented USDM, a model synthesizing spoken dialog responses enriched with natural prosody. We proposed a novel speech-text pretraining scheme that models the comprehensive relationship between speech and text, which proves beneficial for spoken dialog modeling. Our approach is complemented by leveraging an acoustic unit tokenization scheme that preserves prosodic information, coupled with a supporting pair of an encoder and a decoder. We showed that USDM outperforms the baselines regarding content, prosody, and naturalness as a spoken response for the DailyTalk dataset. Additionally, we demonstrated that our pretraining and fine-tuning scheme benefits the USDM in modeling spoken dialog through ablation studies. Various samples for diverse scenarios in our demo page also showcased the capabilities of USDM. We believe that USDM has laid the groundwork for extending the conversational capabilities of LLMs to the voice domains.

Despite these advantages, our model has several limitations and areas for improvement. Firstly, the exploration of datasets and models used for pretraining is limited. Further investigation is necessary to determine which data are crucial for our pretraining scheme and to explore whether our pretraining scheme could be effective with other LLMs beyond Mistral-7B. Secondly, building a spoken dialog model capable of directly generating spoken responses from input spoken dialog without the need for cross-modal chaining can be a promising direction. Next, the current pretraining scheme is based on tens of thousands of hours of English data, and it has limitations when applied to other languages with relatively smaller amounts of speech data compared to English. We plan to expand our model to a variety of languages in addition to English. Lastly, we also plan to investigate whether our pretraining approach is beneficial for other speech-text tasks beyond spoken dialog modeling.