# Reusable Slotwise Mechanisms

Trang Nguyen

Mila - Quebec AI Institute

FPT Software AI Center

Montreal, Canada

trang.nguyen@mila.quebec

&Amin Mansouri

Mila - Quebec AI Institute

Universite de Montreal

Montreal, Canada

amin.mansouri@mila.quebec

Kanika Madan

Mila - Quebec AI Institute

Universite de Montreal

Montreal, Canada

madankan@mila.quebec

&Khuong Nguyen

FPT Software AI Center

Ho Chi Minh City, Vietnam

khuongnd6@fpt.com

&Kartik Ahuja

Mila - Quebec AI Institute

Montreal, Canada

kartik.ahuja@mila.quebec

&Dianbo Liu

Mila - Quebec AI Institute

National University of Singapore

Montreal, Canada

dianbo.liu@mila.quebec

&Yoshua Bengio

Mila - Quebec AI Institute

Universite de Montreal, CIFAR AI Chair

Montreal, Canada

yoshua.bengio@mila.quebec

Work done under an internship at Mila - Quebec AI Institute and FPT Software AI Residency Program. Corresponding author.

###### Abstract

Agents with the ability to comprehend and reason about the dynamics of objects would be expected to exhibit improved robustness and generalization in novel scenarios. However, achieving this capability necessitates not only an effective scene representation but also an understanding of the mechanisms governing interactions among object subsets. Recent studies have made significant progress in representing scenes using object slots. In this work, we introduce Reusable Slotwise Mechanisms, or RSM, a framework that models object dynamics by leveraging communication among slots along with a modular architecture capable of dynamically selecting reusable mechanisms for predicting the future states of each object slot. Crucially, RSM leverages the _Central Contextual Information (CCI)_, enabling selected mechanisms to access the remaining slots through a bottleneck, effectively allowing for modeling of higher order and complex interactions that might require a sparse subset of objects. Experimental results demonstrate the superior performance of RSM compared to state-of-the-art methods across various future prediction and related downstream tasks, including Visual Question Answering and action planning. Furthermore, we showcase RSM's Out-of-Distribution generalization ability to handle scenes in intricate scenarios.

## 1 Introduction

Accurate prediction of future frames and reasoning over objects is crucial in various computer vision tasks. These capabilities are essential for constructing comprehensive world models in applications like autonomous driving and reinforcement learning for robots. Traditional deep learning-based representation learning methods compress entire scenes into monolithic representations, lackingcompositionality and object-centric understanding. As a result, these representations struggle with systematic generalization, interpretability, and capturing interactions between objects. This limitation leads to poor generalization performance as causal variables become entangled in non-trivial ways.

There has been growing interest in slot-based and modular representations that decompose scenes into individual entities, deviating from fixed-size monolithic feature vector representations [19; 38; 16; 17; 15; 18; 32; 21; 28; 37; 39; 48; 30]. These novel approaches offer significantly more flexibility when dealing with environments that comprise multiple objects. By employing an encoder that segments a scene into its independent constituent entities instead of compressing information into a fixed-size representation, these methods allow for greater flexibility and parameter sharing when learning object-centric representations, and their compositional nature enables better generalization. Compositional and object-centric representations can be effectively utilized alongside complex world models that accurately capture the interactions and dynamics of different entities in a scene.

These world models, when presented with proper representations, in principle, can model the transition functions that relate latent causal factors across consecutive time steps of a rollout. While monolithic blocks are still used occasionally with object-centric methods , recent attempts have incorporated similar inductive biases related to the object-centricity of images in modeling interactions. Structured world models and representations seem truly promising for systematically generalizing to novel scenes. Structured world models would ideally decompose the description of the evolution of a scene into causal and independent sub-modules, making it easy to recombine and repurpose those mechanisms in novel ways to solve challenges in unseen scenarios. Such separation of dynamics modeling makes structured world models more adaptable to distribution shifts, as only the parameters of a few mechanisms that have changed in a new environment would have to be retrained, and not all of the parameters in the case of a monolithic model .

A major class of such structured world models aims at baking in some inductive bias about the nature of object interactions. On one extreme, there have been studies that employ Graph Neural Networks (GNNs) to capture object dependencies through dense connections, while on the other hand, there has been contrasting work aiming at modeling the dynamics through only pairwise interactions. We believe, however, that ideally, an agent should be able to learn, select, and reuse a set of prediction rules based on contextual information and the characteristics of each object.

In this work, we argue that the assumptions made in previous attempts at learning the dynamics among slots may be insufficient in more realistic domains. To address these limitations, we propose Reusable Slotwise Mechanisms (RSM), a novel modular architecture incorporating a set of deep neural networks representing reusable mechanisms on top of slotwise representations [31; 10]. Inspired by the Global Workspace Theory (GWT) in the cognitive neuroscience of working memory [2; 3], we introduce the concept of Central Contextual Information (CCI), which allows each reusable mechanism, _i.e._, a possible explanation of state evolution, to access information from all other slots through a bottleneck, enabling accurate predictions of the next state for a specific slot. The CCI's bottleneck amounts to a relaxed inductive bias compared to the extreme cases of pairwise or fully dense interactions among slots. Finally, comprehensive experiments demonstrate that RSM outperforms the state-of-the-art in various next-step prediction tasks, including independent and identically distributed (iid) and Out-of-Distribution (OOD) scenarios.

In summary, the presented work makes the following contributions:

1. RSM: A modular dynamics model comprising a set of reusable mechanisms that take as input slot representations through an attention bottleneck and sequential slot updates.
2. RSM strengthens communication among slots in dynamics modeling by relaxing inductive biases in dynamics modeling to be not too dense or sparse but depend on a specific context.
3. RSM outperforms baselines in multiple tasks, including next frames prediction, Visual Question Answering, and action planning in both iid and OOD cases.

## 2 Proposed Method: RSM - Reusable Slotwise Mechanisms

### RSM Overview

We introduce RSM, a modular architecture consisting of a set of \(M\) Multilayer Perceptrons (MLPs) that act as reusable mechanisms, operating on slotwise representations to predict changes in the slots.

What sets RSM apart from other architectures is incorporating the CCI buffer, which enhances its adaptability when dealing with environments characterized by varying levels of interaction complexity among objects by allowing the propagation of information about all other slots. Unlike previous approaches [18; 16; 17; 23], we enable a sparse subset of slots to transmit contextual information through a bottleneck for updating each slot. This inductive bias becomes helpful in environments where higher-order complex interactions need to be captured by reusable mechanisms as the CCI effectively modulates the complexity of the mechanisms and accommodates those that require one or two slots, as well as those that rely on a larger subset of slots.

The training pipeline of RSM, which is visualized in Figure 1(a), is designed to predict \(K\) rollout steps of \(N\) object slots, based on \(T\) burn-in frames with a temporal window of maximum \(\) history steps for each prediction step. Prediction of the rollout begins by processing the given \(T\) burn-in frames to obtain the \(N\) slot representations for each of the \(T\) burn-in steps using an object-centric model. For any rollout step after the burn-in frames, the slots in a window of \( T\) previous steps will be fed to the model as additional context to predict the state (slots) at \(t+1\). The model takes this input and updates the slots sequentially to output the slots at \(t+1\). The sequential updating of slots means that updates from (according to some ordering) slots can influence the prediction of later slots within a prediction step, as illustrated in Figure 1(b). It is worth highlighting that the sequential way of updating slots breaks the symmetries in mechanism selection, and also allows for a more expressive transition function, similar to how autoregressive models enable encoding of rich distributions. The prediction process is repeated until the slots for \(K\) rollout steps are predicted.

Figure 1: The future prediction pipeline (Figure 1(a)), RSM Intuition (Figure 1(b)), and Computational Flow (Figure 1(c)). Colored circles represent slots, with the dashed border denoting changes in a slot. The Central Contextual Information (CCI) is derived from all object slots as context, assists in selecting a mechanism for slots, and acts as an input of mechanisms. Slots are sequentially updated in four steps: **(1)** Compute CCI by unrolling slots (updated and non-updated) over the past \(\) steps, **(2)** Select a mechanism based on CCI and the slot of interest, **(3)** Predict the next state by the selected mechanism’s dynamics, and **(4)** Update the predicted slot and prepare for the next object’s turn.

### Computational Flow in RSM

This section describes the computational flow of RSM in more detail along with a 4-step process that will be repeated for all slots within a time-step \(t\), in a sequential manner, as illustrated in Figure 1(c) and Algorithm 1 in the Appendix. The following are the main components of the architecture, where \(d_{s}\) and \(d_{cci}\) denote the dimension of slots and the CCI, respectively:

1. \(():^{((+1) N) d_{s}} ^{d_{s}}\) followed by a **projection**\(():^{d_{s}}^{d_{oi}}\) that computes the CCI, denoted as \(^{d_{oi}}\), from all of the \(N\) slots in the past \(T\) steps concatenated. Keys, queries, and values all come from slots, so the CCI is not affected by the order of slot representations.
2. The set of \(M\)**reusable mechanisms**\(\{g_{1},,g_{M}\}\) where \(g_{i}():^{d_{oi}+d_{s}}^{d_{s}}\) are represented by independently parametrized MLPs implicitly trained to specialize in explaining different transitions. Each such \(g_{i}()\) takes as input one slot concatenated with the CCI.
3. \(():^{d_{oi}+d_{s}}^{M}\) that takes as input the CCI and the slot of interest \(s^{i}_{t}\). It computes a categorical distribution over the possible choices of mechanisms for \(s^{i}_{t}\), and outputs a sample of that distribution to be used for updating \(s^{i}_{t}\).

Considering the \(N\) slots per each of the \(\) steps in the temporal window before \(t\), \(s^{1:N}_{^{*}:t}=\{s^{1}_{t-+1},s^{2}_{t-+1},,s^{N}_{t-+1 },,s^{1}_{t},s^{2}_{t},,s^{N}_{t}\}\) with \(^{*}=t-+1\), RSM predicts the next state of slots, denoted as \(s^{ 1:N}_{t}\), using the following 4-step process, which is sequentially applied to each of the slots. Suppose an ordering has been fixed over the slots for a rollout, and according to that ordering, for some \(0<n N\), we have that \(n-1\) slots have been updated to their predicted values at \(t+1\) and are denoted by \(s^{ 1}_{t+1},,s^{ n-1}_{t+1}\). Below, we explain the process of computing the next state for \(s^{n}_{t}\) (e.g. the blue slot in Figure 1(c)).

_Step 1._ **Compute the CCI**: We first append \(s^{ 1:N}_{t}\) to the current temporal window to achieve \(s^{1:N}_{^{*}:t+1}\). The duplicated \(s^{ 1:N}_{t}\) serves as a placeholder, which will be overwritten with the predicted values in subsequent steps. Subsequently, as presented in Equation 1, a \(()\) takes \(s^{1:N}_{^{*}:t+1}\) as input before passing through \(()\) to produce the central contextual information cci.

\[=((s^{1:N}_{^{*}:t+1})) \]

_Step 2._ **Select one mechanism for \(s^{n}_{t}\)**: \(()\) takes two arguments as inputs, cci from Step 1 and \(s^{n}_{t}\), to output the logits of a categorical distribution \(_{1:M}\) over \(M\) possible choices of mechanisms. We employ a hard \(softmax}\) layer with Straight-through Trick  on top of \(\)'s outputs, as described in Equation 2, to select the mechanism.

\[_{1:M}=softmax}((,s^{n}_{t})) \]

_Step 3._ **Predict the changes of \(s^{n}_{t}\)**: Let \( s^{n}_{t}\) denote the change of \(s^{n}_{t}\) from \(t\) to \(t+1\). \( s^{n}_{t}\) is presented in Equation 3, where \(k=*{argmax}(_{1:M})\).

\[ s^{n}_{t}=g_{k}(,s^{n}_{t}) \]

_Step 4._ **Update and sync \(s^{n}_{t+1}\)**: \(s^{ n}_{t+1}\) is then computed by adding the predicted transformation from the previous step and replaces the value of \(s^{n}_{t+1}\) in the slots buffer, as described in Equation 4.

\[s^{ n}_{t+1}=s^{n}_{t}+ s^{n}_{t} \]

The process above is repeated for all slots at time \(t\), to obtain the next state (slots) prediction \(s^{ 1:N}_{t+1}\).

## 3 Experiments Setup

This study evaluates RSM's dynamics modeling and generalization capabilities through video prediction, VQA, and action planning tasks. We aim to provide empirical evidence supporting the underlying hypotheses that guided the architectural design of RSM.

* \(_{1}\): Slots communication through the CCI and reusable mechanisms reduces information loss during prediction, resulting in accurate prediction of future rollout frames (**Section 4.1**).

* \(_{2}\): RSM effectively handles novel scenarios in the downstream tasks (**Section 4.2**), especially enhancing OOD generalization (**Section 4.3**).
* \(_{3}\): The synergy between the CCI and the disentanglement of objects dynamics into reusable mechanisms is essential to RSM (analyses in **Section 4.4** and ablations in **Section 4.5**).

In the following subsections, we describe the experiments focusing on the transition of slots over rollout steps with pre-trained object-centric models. Additionally, Appendix E provides experiments and analyses with an end-to-end training pipeline.

### Environments

**OBJ3D** contains dynamic scenes of a sphere colliding with static objects. Following Lin et al. , Wu et al. , we use 3 to 5 static objects and one launched sphere for interaction.

**CLEVRER** shares similarities with OBJ3D, but additionally has multiple moving objects in various directions throughout the scene. For the VQA downstream task, CLEVRER offers four question types: descriptive, explanatory, predictive, and counterfactual, among which, in the spirit of improving video prediction, we focus on boosting the performance on answering predictive questions which require an understanding of future object interactions.

**PHYRE** is a 2D physics puzzle platform where the goal is strategically placing red objects such that the green object touches the blue or purple object. Bakhtin et al.  design _templates_ that describe such tasks with varying initial states. Subsequently, they define (1) _within-template_ protocol where the test set contains the same _templates_ as in training, and (2) _cross-template_ protocol that tests on different _templates_ than those in training. We report results both on _within-template_ as iid and on _cross-template_ to obtain the OOD evaluation.

**Physion** is a VQA dataset that assesses a model's capability in predicting objects' movement and interaction in realistic simulated 3D environments in eight physical phenomena.

For further details and data visualization, we refer the readers to Appendix B.

### Baselines

We compare RSM against three main baselines: **SlotFormer**, Switch Transformer  denoted as **SwitchFormer**, and **NPS**. We show the efficacy of relaxing the inductive bias on communication density among slots in RSM by contrasting with dense communication methods (SlotFormer and SwitchFormer) and a pair-wise communication method (NPS). Likewise, comparing RSM to SlotFormer highlights the role of disentangling objects' dynamics into mechanisms, while comparing to SwitchFormer and NPS emphasizes the vital role of communication among slots via the CCI. Additionally, we compare to **SAVi-Dyn**, which is the SOTA on CLEVRER. In other experiments, we compare to **SlotFormer** (current SOTA).

In the tables, we present our reproduced SlotFormer (marked by "\(\)") and our re-implemented SwitchFormer and NPS (marked by "\(\)"), alongside SAVi-Dyn reported by Wu et al. . 2

### Implementation Details

Following Wu et al. , we focus on the transition of slots and take advantage of the pre-trained object-centric _encoder-decoder_ pair that convert input frames into slots and vice versa. We use the pre-trained weights of SAVi and STEVE provided by Wu et al. , including **SAVi** for OBJ3D, CLEVRER, and PHYRE; and **STEVE** for Physion.

We present the best validation set configuration of RSM for each dataset, along with fine-tuning results and model size scaling in Appendix D. In summary, (1) OBJ3D and CLEVRER include 7 mechanisms, while PHYRE and Physion use 5, and (2) the number of parameters in RSM is slightly lower than that of SlotFormer in corresponding experiments. Additionally, we re-implemented SwitchFormer and NPS with a similar number of parameters as in RSM and SlotFormer.

## 4 Experimental Results

We report mean and standard deviation across 5 different runs. Video visualizations of our experiments are provided in our repository 3. See also Appendix A on the reproducibility of our results.

### Video Prediction Quality

To demonstrate \(_{1}\), we provide the video prediction quality on OBJ3D and CLEVRER in Table 1 and Figure 2. In general, RSM demonstrates its effectiveness in handling object dynamics in the long-term future prediction over baselines.

**Evaluation Metrics** We evaluate the quality and similarity of the predicted rollout frames using **SSIM** and **LPIPS** metrics. Since the range of **LPIPS** metric is small, we report the actual values times 100, denoted as **LPIPS\({}_{ 100}\)**, to facilitate comparisons among the methods. Additionally, we also assess the performance using **ARI**, **FG-ARI**, and **FG-mIoU** metrics, which measure clustering similarity and foreground segmentation accuracy of object bounding boxes and segmentation masks. We evaluate the model's performance averaged over unrolling 44 steps on OBJ3D and 42 steps on CLEVRER with six burn-in frames in both datasets.

Table 1 exhibits that RSM outperforms other approaches and achieves the highest scores across evaluation metrics for both datasets. Notably, compared to SlotFormer, RSM improves LPIPS\({}_{ 100}\) by \(0.46\) points in OBJ3D, \(1.12\) points in CLEVRER, and increases FG-mIoU by \(3.05\) points in CLEVRER. Following RSM, NPS consistently ranks second in performance among the baselines.

These results are supported by Figure 2, which illustrates the rollout frames in OBJ3D. RSM's outputs' accurate rollout predictions with high visual fidelity demonstrate the efficacy of slot communication by having less error accumulated along time steps than any of the baselines. It is worth emphasizing that RSM excels in handling a significant series of complex movements in steps 20-30, particularly

    &  &  \\   & SSIM\(\) & LPIPS\({}_{ 100}\) & SSIM\(\) & LPIPS\({}_{ 100}\) & ARI\(\) & FG-ARI\(\) & FG-mIoU\(\) \\  SAVi-Dyn & 0.91 & 12.00 & 0.89 & 19.00 & 8.64 & 64.32 & 18.25 \\ NPS\({}^{*}\) & 0.90\({}^{ 0.2}\) & 8.24\({}^{ 0.2}\) & 0.89\({}^{ 0.2}\) & 12.51\({}^{ 0.0}\) & 62.84\({}^{ 0.2}\) & 64.62\({}^{ 0.3}\) & 30.39\({}^{ 0.2}\) \\ SwitchFormer\({}^{*}\) & 0.91\({}^{ 0.2}\) & 8.09\({}^{ 0.3}\) & 0.88\({}^{ 0.3}\) & 14.28\({}^{ 0.1}\) & 60.61\({}^{ 0.4}\) & 59.32\({}^{ 0.3}\) & 28.94\({}^{ 0.2}\) \\ SlotFormer\({}^{}\) & 0.90\({}^{ 0.2}\) & 8.32\({}^{ 0.2}\) & 0.88\({}^{ 0.2}\) & 13.09\({}^{ 0.1}\) & 63.38\({}^{ 0.3}\) & 62.91\({}^{ 0.2}\) & 29.68\({}^{ 0.3}\) \\
**RSM (Ours)** & **0.92\({}^{ 0.1}\)** & **7.88\({}^{ 0.1}\)** & **0.91\({}^{ 0.1}\)** & **11.96\({}^{ 0.1}\)** & **67.72\({}^{ 0.2}\)** & **66.15\({}^{ 0.2}\)** & **32.73\({}^{ 0.2}\)** \\   

Table 1: **Future frame prediction quality on OBJ3D and CLEVRER. Bold scores indicate the best performance, with the RSM consistently outperforming baselines by a remarkable margin.**

Figure 2: **Comparison of rollout frames in OBJ3D. RSM renders frames with precise dynamics and holds visual quality, even complex actions in steps 20 to 30. In contrast, the baselines produce ones with artifacts (yellow boxes) and inaccurate dynamics (red boxes). Best viewed in video format.**during a sequence of rapid collision of object pairs. In contrast, we find that the baselines struggle with complex object movements during this period, leading to inaccuracies in predicting the dynamics towards the end. RSM effectively maintains visual quality by predicting the _changes_ of slots instead of predicting the next state of a slot. This approach allows RSM to handle action-free scenarios well and significantly reduce error accumulation by facilitating null transitions that preserve slot integrity.

Furthermore, RSM demonstrates flexible slot communication with relaxed inductive biases on interaction density, enabling it to adapt to environments comprising mechanisms with varying levels of complexity. In contrast, we find that NPS with sparse interactions faces difficulties with close-by objects and rapid collisions (seen in OBJ3D), while SwitchFormer with dense communication struggles with distant objects (as in CLEVERER).

### Downstream Tasks: Visual Question Answering and Action Planning

#### 4.2.1 Visual Question Answering task

To demonstrate \(_{2}\), we validate the performance of future frames generated by the models on the VQA task in CLEVERER and Physion, and the action planning task in PHYRE in the next section. The general pipeline is to solve the VQA task with the predicted rollout frames from the given input frames. Specifically, we employ **Aloe** as the base reasoning model on top of the unrolled frames in CLEVERER. Likewise, in Physion, we adhere to the official protocol by training a linear model on the predicted future slots to detect objects' contact. In Physion, we also include the results obtained from human participants  for reference. Likewise, we collect the results from observed frames (_Obs._), which are obtained by training the VQA model on top of pre-trained burn-in slots and compare them to the performance of rollout slots (_Dyn._).

In Table 2, RSM consistently outperforms all three baselines in VQA for CLEVERER and Physion. On CLEVERER, RSM achieves the highest scores of \(96.8\%\) (per option) and \(94.3\%\) (per question), surpassing SlotFormer and NPS. In Physion, RSM shows notable improvement, with a \(3.1\%\) increase from \(65.0\%\) in _Obs._ to \(68.1\%\) in _Dyn._, outperforming all other baselines, indicating the benefit of enhancing the dynamics modeling to improve the downstream tasks. However, RSM is still far from human performance in Physion, showing room for further research into this class of algorithms.

#### 4.2.2 Action Planning task

We adopt the approach from prior works [36; 44] and construct a task success classifier as an action scoring function. This function is designed as a simple linear classifier, which considers the predicted object slots, to rank a pre-defined set of 10,000 actions from , executed accordingly. We utilize AUCCESS, which quantifies the success rate over the number of attempts curve's Area Under Curve

    &  &  \\   & per opt. \(\) & per ques. \(\) & Obs. \(\) & Dyn. \(\) & Gap \(\) \\  Human & - & - & **74.7** & - & - \\ NPS\({}^{*}\) & 95.3\({}^{ 0.3}\) & 93.8\({}^{ 0.2}\) & & 65.6\({}^{ 0.3}\) & +0.6 \\ SwitchFormer\({}^{*}\) & 92.8\({}^{ 0.3}\) & 90.4\({}^{ 0.2}\) & & 66.2\({}^{ 0.1}\) & +1.2 \\ SlotFormer\({}^{}\) & 96.1\({}^{ 0.2}\) & 93.3\({}^{ 0.1}\) & & 66.9\({}^{ 0.2}\) & +1.9 \\
**RSM (Ours)** & **96.8\({}^{ 0.1}\)** & **94.3\({}^{ 0.0}\)** & & **68.1\({}^{ 0.0}\)** & **+3.1** \\   

Table 2: **VQA performance on CLEVERER and Physion. Despite not surpassing human performance in Physion, RSM outperforms baselines in both datasets. All scores are in percentage.**

   PHYRE-B & NPS\({}^{*}\) & SwitchFormer\({}^{*}\) & SlotFormer\({}^{}\) & **RSM** \\ 
**iid** (_within-template_) & 80.52\({}^{ 1.0}\) & 78.27\({}^{ 1.9}\) & 76.4\({}^{ 1.1}\) & **82.89\({}^{ 0.6}\)** \\
**OOD** (_cross-template_) & 42.63\({}^{ 1.3}\) & 48.36\({}^{ 1.4}\) & 42.46\({}^{ 1.7}\) & **57.37\({}^{ 1.4}\)** \\   

Table 3: **Action planning task in PHYRE. RSM outperforms all baselines in both iid and OOD.**(AUC) for the first 100 actions. We report the average score over ten folds, with the best performance among five different runs on each fold.

The first line in Table 3 indicates the action planning results of the proposed RSM and baselines in the iid setting (_within-template_ protocol). RSM achieves the highest performance compared to baselines, indicating the critical role of efficient slots communication in complex tasks like action planning. In addition, Figure 3 shows a successful case of RSM solving the planning task by strategically placing the red object at step 0, causing a collision between the green and blue objects at the end.

### Out-of-Distribution (OOD) Generalization

To provide more evidence for \(_{2}\), we resume analyzing the performance of the action planning task in PHYRE in two OOD scenarios:

OOD in TemplatesPerformance on _cross-template_ protocol is indicated in the last line in Table 3. Overall, RSM demonstrates strong generalization and has the smallest gap between iid and OOD performance compared to the baselines. The _cross-template_ is a natural method to evaluate the OOD generalization in PHYRE since scenes in the train and test sets are in distinct _templates_ and contain dissimilar object sizes and objects in the background . We refer the reader to Appendix B for further details and visualizations.

OOD in DynamicsWe introduce modifications to PHYRE dataset generation that introduce distribution shifts to the dynamics of the blue objects. Particularly, the blue objects are static floors during training. However, in the test set, the object can be assigned as a blue ball, inheriting all dynamics as a red or green ball. As a result, the blue ball appearing only in the test set is considered a strong OOD case, as the model does not have a chance to construct the blue ball and its dynamics during training. Presented in Figure 5, although revealing some distortion in shape, RSM demonstrates the ability to generalize the movements of the ball-shaped object in intensive OOD cases.

### The Ability to Disentangle Object Dynamics into Mechanisms

To demonstrate \(_{3}\), we conduct qualitative analysis on the underlying mechanisms assignment within the four steps of Figure 3 and visualize results in Figure 4. While there is no explicit assignment

Figure 4: **The underlying mechanism assignment in PHYRE. Mechanisms are assigned to each slot at \(t\) to obtain the updates at \(t+1\). RSM disentangles objects’ dynamics into reusable mechanisms, which can be expressed as Collision (2), Moving left or right (3), Idle (4), and Falling (5).**

Figure 3: **Action planning task in PHYRE. RSM strategically positions a red ball at step 0 to prevent the green ball from falling onto the glass by causing a collision that alters the original trajectory of the green ball and causes it to make contact with the blue floor (indicated by the arrow).**

of roles to mechanisms during training, we can infer their functionality at convergence based on slot visualizations and patterns of activation as follows: Mechanism **2** handles collisions, which can be observed in the collision between the green and red balls in step 1 and that of the red ball with the right-side wall in step 5 (blue boxes). Mechanism **3** controls the horizontal movements, observed in the green ball from steps 5 to 10 (green boxes). Mechanism **4** acts as an idleness mechanism. Mechanism **5** handles downward free-fall motion, observed from steps 2 to 3 of the two balls. Mechanism **1** does not seem to be doing anything meaningful, and this could be due to the model using more capacity than it requires to model the dynamics in this environment.

The inferred functions provide the following insights into understanding the efficacy of RSM. Firstly, RSM successfully disentangles the dynamics into reusable mechanisms, as described in Section 2.1. Secondly, RSM properly assigns such mechanisms to slots throughout the rollout steps, which not only helps preserve the accuracy of prediction but also emphasizes the effectiveness of communication among slots in deciding mechanisms for one another. The automatic emergence of a null mechanism (mechanism 4) is also worth highlighting, which significantly helps reduce the error accumulation in action-free settings, such as idle objects in the background.

### Ablation Studies

To provide more evidence for \(_{3}\), we conduct ablations to understand the individual effects of (1) the CCI, (2) mechanisms and their disentanglement, and (3) the sequential update of slots in RSM, and visualize the results in Figure 6. Overall, the ablations confirm the superiority of the original RSM design compared to all variations, highlighting a significant disparity in the absence of CCI.

**RSM:**2 masks out the CCI in step 2 at inference. We observe that the lack of CCI leads to a degenerate selection of mechanisms for slots, with 4 out of 5 object slots being controlled by mechanism 3.

**RSM:**3 masks out the CCI in step 3 during inference. We have found that CCI not only captures comprehensive spatial-temporal information but also provides guidance regarding specific movement

Figure 5: **Ablation on OOD objects’ dynamics. The blue objects are static during training (blue floor) and can move during testing time (blue balls). RSM responds correctly to the blue ball’s dynamics. Pred* are predictions by a model trained with the standard training set, Pred are predictions by a model trained with the modified training data, and Mechanism are mechanisms assigned for the blue object’s slot when obtaining Pred. h.Move denotes the horizontal movement.**

Figure 6: **Ablation studies in OBJ3D. The original design of RSM always demonstrates the superior performance and the accurately predicted future frames compared to the modified versions, including breaking the usages of the CCI in step 2 (RSM:**2) and step 3 (RSM:**3), randomly selecting mechanisms (RSM:**k), and parallel update of slots (RSM:**p). Best viewed in video format (See our repository).**

details, including directions. In particular, even when the correct mechanism is assigned (_e.g._, moving backwards), the slots become confused about the exact direction of movement. Additionally, slots encountered a color-related issue in the subsequent steps.

\(}}\) randomly assigns mechanisms to slots by a randomized mechanism index, \(k\), to replace the distribution \(\) in Equation 2. We observe that the launched ball moves in the wrong direction and exits the scene early, while other objects shake in their positions, underscoring the importance of correctly assigning mechanisms to slots.

\(}\) is the parallel version of RSM that modifies the model \(()\) in Equation 2 to assign mechanisms to \(N\) slots simultaneously, in both training and inference time. We find that \(}\) stands out as a promising model, as it demonstrates a notable **LPIPS** performance; however, it is essential to note the partially inaccurate dynamics caused by the weaker communication among slots.

## 5 Related Work

Modular dynamics models.In the domain of modular neural networks, RIMs  pioneered the exploration of modularity for learning dynamics and long-term dependencies in non-stationary settings. However, RIMs suffer from conflating object and mechanism concepts, limiting their effectiveness. SCOFF  introduced object files (OF) and schematic to address this limitation, but it struggles with generalizing out-of-distribution (OOD) scenarios. Key distinctions between RSM and SCOFF are as follows: SCOFF schemas can handle only one OF at a time, while RSM allows multiple slots to be input to reusable mechanisms using an attention bottleneck. SCOFF and RIMs use sparse communication among OFs for rare events involving multiple objects, whereas RSM leverages the CCI to activate suitable reusable mechanisms. NPS  incorporates sparse interactions directly into its modular architecture, eliminating the need for sparse communication among slots or OFs. Their "production rules" handle rare events involving multiple objects by taking one primary slot and a contextual slot as input. A recent benchmark  evaluates causal discovery models and introduces a modular architecture with dense object interactions, similar to GNN-based methods, but assigns separate mechanisms to each object. Among the class of algorithms with less modularity in their dynamics model, R-NEM  was a pioneering unsupervised method for modeling dynamics using a learned object-centric latent space through iterative inference (see also ) which along similar approaches (, ) used Graph Neural Networks (GNNs) to model pairwise interactions and differ from our work in two key aspects. Firstly, we do not rely on GNNs to model interactions, as dense interactions are not always realistic in many environments. Secondly, we focus on learning a set of simple and reusable mechanisms that can be applied flexibly to different scenarios, rather than compressing information through shared node and edge updates in a GNN.

Unsupervised learning of object-centric representations.To decompose a scene into meaningful sub-parts, there have been lots of recent works on unsupervised learning of object-centric representations from static images [48; 20; 31; 47], videos [35; 27; 25; 12; 44], and theoretical results on the identifiability of such representations [34; 1; 9]. Although lots of these methods work very well in practice, we decided to proceed with slot attention  to be consistent with the baselines.

## 6 Conclusion

In this study, we developed RSM, a novel framework that leverages an efficient communication protocol among slots to model object dynamics. RSM comprises a set of reusable mechanisms that take as input slot representations passed through a bottleneck, the Central Contextual Information (CCI), which are then processed sequentially to obtain slot updates. Through comprehensive empirical evaluations and analysis, we show RSM's advantages over the baselines in various tasks, including video prediction, visual question answering, and action planning tasks, especially in OOD settings. Our results suggest the importance of CCI, which integrates and coordinates knowledge from different slots for both mechanism assignment and predicting slot updates. We believe there is a promise for future research endeavors in exploring more sophisticated stochastic attention mechanisms for information integration, aligning with the principles of higher-level cognition, coping with a large number of object slots, and enabling uncertainty quantification in the predictions. In Appendix F, we discuss the limitations of this work and future directions.