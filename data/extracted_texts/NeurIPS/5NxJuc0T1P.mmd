# Debias Coarsely, Sample Conditionally:

Statistical Downscaling through Optimal Transport

and Probabilistic Diffusion Models

 Zhong Yi Wan

Google Research

Mountain View, CA 94043, USA

wanzy@google.com

&Ricardo Baptista

California Institute of Technology

Pasadena, CA 91106, USA

rsb@caltech.edu

&Yi-fan Chen

Google Research

Mountain View, CA 94043, USA

yifanchen@google.com

&John Roberts Anderson

Google Research

Mountain View, CA 94043, USA

janders@google.com

&Anudhyan Boral

Google Research

Mountain View, CA 94043, USA

anudhyan@google.com

&Fei Sha

Google Research

Mountain View, CA 94043, USA

fsha@google.com

&Leonardo Zepeda-Nunez

Google Research

Mountain View, CA 94043, USA

lzepedanunez@google.com

Equal contribution

###### Abstract

We introduce a two-stage probabilistic framework for statistical downscaling _using unpaired data_. Statistical downscaling seeks a probabilistic map to transform low-resolution data from a _biased_ coarse-grained numerical scheme to high-resolution data that is consistent with a high-fidelity scheme. Our framework tackles the problem by composing two transformations: (i) a debiasing step via an optimal transport map, and (ii) an upsampling step achieved by a probabilistic diffusion model with _a posteriori_ conditional sampling. This approach characterizes a conditional distribution _without needing paired data_, and faithfully recovers relevant physical statistics from biased samples. We demonstrate the utility of the proposed approach on one- and two-dimensional fluid flow problems, which are representative of the core difficulties present in numerical simulations of weather and climate. Our method produces realistic high-resolution outputs from low-resolution inputs, by upsampling resolutions of \(8\) and \(16\). Moreover, our procedure correctly matches the statistics of physical quantities, even when the low-frequency content of the inputs and outputs do not match, a crucial but difficult-to-satisfy assumption needed by current state-of-the-art alternatives. Code for this work is available at: [https://github.com/google-research/swirl-dynamics/tree/main/swirl_dynamics/projects/probabilistic_diffusion](https://github.com/google-research/swirl-dynamics/tree/main/swirl_dynamics/projects/probabilistic_diffusion).

Introduction

Statistical downscaling is crucial to understanding and correlating simulations of complex dynamical systems at multiple resolutions. For example, in climate modeling, the computational complexity of general circulation models (GCMs)  grows rapidly with resolution. This severely limits the resolution of long-running climate simulations. Consequently, accurate predictions (as in forecasting localized, regional and short-term weather conditions) need to be _downscaled_ from coarser lower-resolution models' outputs. This is a challenging task: coarser models do not resolve small-scale dynamics, thus creating bias [16; 69; 84]. They also lack the necessary physical details (for instance, regional weather depends heavily on local topography) to be of practical use for regional or local climate impact studies [33; 36], such as the prediction or risk assessment of extreme flooding [35; 44], heat waves , or wildfires .

At the most abstract level, _statistical downscaling_[81; 82] learns a map from low- to high-resolution data. However, it has several unique challenges. First, unlike supervised machine learning (ML), there is _no natural pairing of samples_ from the low-resolution model (such as climate models ) with samples from higher-resolution ones (such as weather models that assimilate observations ). Even in simplified cases of idealized fluids problems, one cannot naively align the simulations in time, due to the chaotic behavior of the models: two simulations with very close initial conditions will diverge rapidly. Several recent studies in climate sciences have relied on synthetically generated paired datasets. The synthesis process, however, requires accessing both low- and high-resolution models and either (re)running costly high-resolution models while respecting the physical quantities in the low-resolution simulations [25; 43] or (re)running low-resolution models with additional terms nudging the outputs towards high-resolution trajectories . In short, requiring data in correspondence for training severely limits the potential applicability of supervised ML methodologies in practice, despite their promising results [37; 39; 60; 66; 38].

Second, unlike the setting of (image) super-resolution , in which an ML model learns the (pseudo) inverse of a downsampling operator [13; 78], downscaling additionally needs to correct the bias. This difference is depicted in Fig. 1(a). Super-resolution can be recast as frequency extrapolation , in which the model reconstructs high-frequency contents, while matching the low-frequency contents of a low-resolution input. However, the restriction of the target high-resolution data may not match the distribution of the low-resolution data in Fourier space . Therefore, debiasing is necessary to correct the Fourier spectrum of the low-resolution input to render it admissible for the target distribution (moving solid red to solid blue lines with the dashed blue extrapolation in Fig. 1). Debiasing allows us to address the crucial yet challenging prerequisite of aligning the low-frequency statistics between the low- and high-resolution datasets.

Given these two difficulties, statistical downscaling should be more naturally framed as matching two probability distributions linked by an unknown map; such a map emerges from both distributions representing the same underlying physical system, albeit with different characterizations of the system's statistics at multiple spatial and temporal resolutions. The core challenge is then: _how do we structure the downscaling map so that the (probabilistic) matching can effectively remediate the bias introduced by the coarser, i.e., the low-resolution, data distribution?_

Thus, the main idea behind our work is to introduce a debiasing step so that the debiased (yet, still coarser) distribution is closer to the target distribution of the high-resolution data. This step results in an intermediate representation for the data that preserves the correct statistics needed in the follow-up step of upsampling to yield the high-resolution distribution. In contrast to recent works on distribution matching for unpaired image-to-image translation  and climate modeling , the additional structure our work imposes on learning the mapping prevents the bias in the low-resolution data from polluting the upsampling step. We review those approaches in SS2 and compare to them in SS4.

Concretely, we propose a new probabilistic formulation for the downscaling problem that handles _unpaired data_ directly, based on a factorization of the unknown map linking both low- and high-resolution distributions. This factorization is depicted in Fig. 1(b). By appropriately restricting the maps in the factorization, we rewrite the downscaling map as the composition of two procedures: a debiasing step performed using an optimal transport map , which _couples the data distributions_ and corrects the biases of the low-resolution snapshots; followed by an upsampling step performed using conditional probabilistic diffusion models, which have produced state-of-the-art results for image synthesis and flow construction [5; 52; 71; 73].

We showcase the performance of our framework on idealized fluids problems that exhibit the same core difficulty present in atmospheric flows. We show that our framework is able to generate realistic snapshots that are faithful to the physical statistics, while outperforming several baselines.

## 2 Related work

The most direct approach to upsampling low-resolution data is to learn a low- to high-resolution mapping via paired data when it is possible to collect such data. For complex dynamical systems, several methods carefully manipulate high- and low-resolution models, either by nudging or by enforcing boundary conditions, to produce paired data without introducing spectral biases [12; 25]. Alternatively, if one has strong prior knowledge about the process of downsampling, optimization methods can solve an inverse problem to directly estimate the high-resolution data, leveraging prior assumptions such as sparsity in compressive sensing [9; 10] or translation invariance .

In our setting, there is no straightforward way to obtain paired data due to the nature of the problem (i.e., turbulent flows, with characteristically different statistics across a large span of spatio-temporal scales). In the weather and climate literature (see  for an extensive overview), prior knowledge can be exploited to downscale specific variables . One of the most predominant methods of this type is bias-correction spatial disaggregation (BCSD), which combines traditional spline interpolation with a quantile matching bias correction , and linear models . Recently, several studies have used ML to downscale physical quantities such as precipitation , but without quantifying the prediction uncertainty. Yet, a generally applicable method to downscale arbitrary variables is lacking.

Another difficulty is to remove the bias in the low resolution data. This is an instance of domain adaptation, a topic popularly studied in computer vision. Recent work has used generative models such as GANs and diffusion models to bridge the gap between two domains [5; 7; 14; 58; 60; 62; 68; 74; 83; 85]. A popular domain alignment method that was used in  for downscaling weather data is AlignFlow . This approach learns normalizing flows for source and target data of the same dimension, and uses their common latent space to move across domains. The advantage of those methods is that they do not require training data from two domains in correspondence. Many of those approaches are related to optimal transport (OT), a rigorous mathematical framework for learning maps between two domains without paired data . Recent computational advances in OT for discrete (i.e., empirical) measures [21; 64] have resulted in a wide set of methods for domain adaptation [20; 31]. Despite their empirical success with careful choices of regularization, their use alone for high-dimensional images has remained limited .

Our work uses diffusion models to perform upsampling after a debiasing step implemented with OT. We avoid common issues from GANs  and flow-based methods , which include over-smoothing, mode collapse and large model footprints [24; 52]. Also, due to the debiasing map, which matches the low-frequency content in distribution (see Fig. 1(a)), we do not need to explicitly impose that the low-frequency power spectra of the two datasets match like some competing methods do . Compared to formulations that perform upsampling and debiasing simultaneously [5; 78], our

Figure 1: (a) Upsampling (super-resolution) as frequency extrapolation in the Fourier domain. The model extrapolates low-frequency content to higher-frequencies (dashed blue). The debiasing map corrects the biased low-frequency content (solid red). (b) Illustration of the proposed framework where \(\) is the space of high-resolution data, \(\) is the space of low-resolution data, \(C\) is an _unknown nonlinear_ map linking \(\) and \(\), \(C^{}\) is a _known linear_ downsampling map, \(^{}\) is an intermediate (low-resolution) space induced by the image of \(C^{}\), and \(T\) is an invertible debiasing map such that \(C\) can be factorized as \(T^{-1} C^{}\). The conditional probabilities \(p(x|C^{}x=y^{})\) are used for the probabilistic upsampling procedure.

framework performs these two tasks separately, by only training (and independently validating) a single probabilistic diffusion model for the high-resolution data once. This allows us to quickly assess different modeling choices, such as the linear downsampling map, by combining the diffusion model with different debiasing maps. Lastly, in comparison to other two-stage approaches [5; 32], debiasing is conducted at low-resolutions, which is less expensive as it is performed on a much smaller space, and more efficient as it is not hampered from spurious biases introduced by interpolation techniques.

## 3 Methodology

SetupWe consider two spaces: the high-fidelity, high-resolution space \(=^{d}\) and the low-fidelity, low-resolution space \(=^{d^{}}\), where we suppose that \(d>d^{}\). We model the elements \(X\) and \(Y\) as random variables with marginal distributions, \(_{X}\) and \(_{Y}\), respectively. In addition, we suppose there is a statistical model relating the \(X\) and \(Y\) variables via \(C\), an unknown and possibly nonlinear, downsampling map. See Fig. 1(b) for a diagram.

Given an observed realization \(\), which we refer to as a _snapshot_, we formulate downscaling as the problem of sampling from the conditional probability distribution \(p(x|E_{})\) for the event \(E_{}:=\{x\,|\,C(x)=\}\), which we denote by \(p(x|C(x)=)\). Our objective is to sample this distribution given only access to marginal samples of \(X\) and \(Y\).

Main ideaIn general, downscaling is an ill-posed problem given that the joint distribution of \(X\) and \(Y\) is not prescribed by a known statistical model. Therefore, we seek an approximation to \(C\) so the statistical properties of \(X\) are preserved given samples of \(_{Y}\). In particular, such a map should satisfy \(C_{}_{X}=_{Y}\), where \(C_{}_{X}\) denotes the push-forward measure of \(_{X}\) through \(C\).

In this work, we impose a structured ansatz to approximate \(C\). Specifically, we _factorize_ the map \(C\) as the composition of a known and linear _downsampling map_\(C^{}\), and an invertible _debiasing map_\(T\):

\[C=T^{-1} C^{},(T^{-1} C^{})_{ }_{X}=_{Y}, \]

or alternatively, \(C^{}_{}_{X}=T_{}_{Y}\). This factorization decouples and explicitly addresses two entangled goals in downscaling: debiasing and upsampling. We discuss the advantage of such factorization, after sketching how \(C^{}\) and \(T\) are implemented.

The range of the downsampling map \(C^{}^{}\) defines an _intermediate_ space \(^{}=^{d^{}}\) of high-fidelity low-resolution samples with measure \(_{Y^{}}\). Moreover, the joint space \(^{}\) is built by projecting samples of \(X\) into \(^{}\), i.e., \((x,y^{})=(x,C^{}x)^{}\); see Fig. 1(b). Using these spaces, we decompose the domain adaptation problem into the following three sub-problems:

1. _High-resolution prior_: Estimate the marginal density \(p(x)\);
2. _Conditional modeling_: For the joint variables \(X Y^{}\), approximate \(p(x|C^{}x=y^{})\);
3. _Debiasing_: Compute a transport map such that \(T_{}_{Y}=C^{}_{}_{X}\).

For the first sub-problem, we train an _unconditional_ model to approximate \(_{X}\), or \(p(x)\), as explained in SS3.1. For the second sub-problem, we leverage the prior model and \(y^{}^{}\) to build a model for _a posteriori_ conditional sampling of \(p(x|C^{}x=y^{})\), which allows us to upsample snapshots from \(^{}\) to \(\), as explained in SS3.2. For the third sub-problem, we use domain adaptation to shift the resulting model from the source domain \(^{}\) to the target domain \(\), for which there is no labeled data. For such a task, we build a transport map \(T:^{}\) satisfying the condition that \(T_{}_{Y}=_{Y^{}}=C^{}_{}_{X}\). This map is found by solving an optimal transport problem, which we explain in SS3.3.

Lastly, we merge the solutions to the sub-problems to arrive at our core downscaling methodology, which is summarized in Alg. 1. In particular, given a low-fidelity and low-resolution sample \(\), we use the optimal transport map \(T\) to project the sample to the high-fidelity space \(^{}=T()\) and use the conditional model to sample \(p(x|C^{}x=^{})\). The resulting samples are contained in the high-fidelity and high-resolution space.

The factorization in Eq. (1) has several advantages. We do not require a cycle-consistency type of loss [34; 86]: the consistency condition is automatically enforced by Eq. (1) and the conditional sampling. By using a linear downsampling map \(C^{}\), it is trivial to create the intermediate space \(^{}\), while rendering the conditional sampling tractable: conditional sampling with a nonlinear map is often more expensive and it requires more involved tuning [17; 18]. The factorization also allows us to compute the debiasing map in a considerably lower dimensional space, which conveniently requires less data to cover the full distribution, and fewer iterations to find the optimal map .

### High-resolution prior

To approximate the prior of the high-resolution snapshots we use a probabilistic diffusion model, which is known to avoid several drawbacks of other generative models used for super-resolution , while providing greater flexibility for _a posteriori_ conditioning [17; 30; 46; 47].

Intuitively, diffusion-based generative models involves iteratively transforming samples from an initial noise distribution \(p_{T}\) into ones from the target data distribution \(p_{0}=p_{}\). Noise is removed sequentially such that samples follow a family of marginal distributions \(p_{t}(x_{t};_{t})\) for decreasing diffusion times \(t\) and noise levels \(_{t}\). Conveniently, such distributions are given by a forward noising process that is described by the stochastic differential equation (SDE) [45; 73]

\[dx_{t}=f(x_{t},t)dt+g(x_{t},t)dW_{t}, \]

with drift \(f\), diffusion coefficient \(g\), and the standard Wiener process \(W_{t}\). Following , we set

\[f(x_{t},t)=f(t)x_{t}:=_{t}}{s_{t}}x_{t}, g( x_{t},t)=g(t):=s_{t}_{t}_{t}}. \]

Solving the SDE in Eq. (2) forward in time with an initial condition \(x_{0}\) leads to the Gaussian perturbation kernel \(p(x_{t}|x_{0})=(x_{t};s_{t}x_{0},s_{t}^{2}_{t}^{2})\). Integrating the kernel over the data distribution \(p_{0}(x_{0})=p_{}\), we obtain the marginal distribution \(p_{t}(x_{t})\) at any \(t\). As such, one may prescribe the profiles of \(s_{t}\) and \(_{t}\) so that \(p_{0}=p_{}\) (with \(s_{0}=1,_{0}=0\)), and more importantly

\[p_{T}(x_{T})(x_{T};0,s_{T}^{2}_{T}^{2}), \]

i.e., the distribution at the terminal time \(T\) becomes indistinguishable from an isotropic, zero-mean Gaussian. To sample from \(p_{}\), we utilize the fact that the reverse-time SDE

\[dx_{t}=f(t)x_{t}-g(t)^{2}_{x_{t}} p_{t}(x_{t})dt+g(t) dW_{t}, \]

has the same marginals as Eq. (2). Thus, by solving Eq. (5) backwards using Eq. (4) as the final condition at time \(T\), we obtain samples from \(p_{}\) at \(t=0\).

Therefore, the problem is reduced to estimating the _score function_\(_{x_{t}} p_{t}(x_{t})\) resulting from \(p_{}\) and the prescribed diffusion schedule \((s_{t},_{t})\). We adopt the denoising formulation in  and learn a neural network \(D_{}(x_{0}+_{t},_{t})\), where \(\) denotes the network parameters. The learning seeks to minimize the \(L_{2}\)-error in predicting the true sample \(x_{0}\) given a noise level \(_{t}\) and the sample noised with \(_{t}=_{t}\) where \(\) is drawn from a standard Gaussian. The score can then be readily obtained from the denoiser \(D_{}\) via the asymptotic relation (i.e., Tweedie's formula )

\[_{x_{t}} p_{t}(x_{t})(_{t},_{t })-_{t}}{s_{t}_{t}^{2}},_{t}=x_{t}/s_{t}. \]

### _A posteriori_ conditioning via post-processed denoiser

We seek to super-resolve a low-resolution snapshot \(^{}^{}\) to a high-resolution one by leveraging the high-resolution prior modeled by the diffusion model introduced above. Abstractly, our goal is to sample from \(p(x_{0}|E^{}_{^{}})\), where \(E^{}_{^{}}=\{x_{0}:C^{}x_{0}\!=\!^{}\}\). Following , this may be approximated by modifying the learned denoiser \(D_{}\) at _inference time_ (see Appendix A for more details):

\[_{}(_{t},_{t})=(C^{})^{}^{ }+(I-VV^{T})[D_{}(_{t},_{t})-_{_{t}}\|C^{}D_{}(_{t},_{t})-^{}\|^{2} ], \]where \((C^{})^{}=V^{-1}U^{T}\) is the pseudo-inverse of \(C^{}\) based on its singular value decomposition (SVD) \(C^{}=U V^{T}\), and \(\) is a hyperparameter that is empirically tuned. The \(_{}\) defined in Eq. (7) directly replaces \(D_{}\) in Eq. (6) to construct a conditional score function \(_{x_{t}} p_{t}(x_{t}|E^{}_{^{}})\) that facilitates the sampling of \(p(x_{0}|E^{}_{^{}})\) using the reverse-time SDE in Eq. (5).

### Debiasing via optimal transport

In order to upsample a biased low-resolution data \(\), we first seek to find a mapping \(T\) such that \(^{}=T()^{}\) is a representative sample from the distribution of unbiased low-resolution data. Among the infinitely many maps that satisfy this condition, the framework of optimal transport (OT) selects a map by minimizing an integrated transportation distance based on the cost function \(c^{}^{+}\). The function \(c(y,y^{})\) defines the cost of moving one unit of probability mass from \(y^{}\) to \(y\). By treating \(Y,Y^{}\) as random variables on \(,^{}\) with measures \(_{Y},_{Y^{}}\), respectively, the OT map is given by the solution to the Monge problem

\[_{T}\{ c(y,T(y))d_{Y}(y):T_{}_{Y}=_{Y^{}} \}. \]

In practice, directly solving the Monge problem is hard and may not even admit a solution . One common relaxation of Eq. (8) is to seek a joint distribution, known as a coupling or transport plan, which relates the underlying random variables . A valid plan is a probability measure \(\) on \(^{}\) with marginals \(_{Y}\) and \(_{Y^{}}\). To efficiently estimate the plan when the \(c\) is the quadratic cost (i.e., \(c(y,y^{})=\|y-y^{}\|^{2}\)), we solve the entropy regularized problem

\[_{(_{Y^{}},_{Y^{}})}\|y-y^{ }\|^{2}d(y,y^{})+ D_{}(||_{Y^{ }}_{Y}), \]

where \(D_{}\) denotes the KL divergence, and \(>0\) is a small regularization parameter, using the Sinkhorn's algorithm , which leverages the structure of the optimal plan to solve Eq. (9) with small runtime complexity . The solution to Eq. (9) is the transport plan \(_{}(,)\) given by

\[_{}(y,y^{})=((f_{}(y)+g_{}(y^{ })-\|y-y^{}\|^{2})/)d_{Y}(y)d_{Y^{ }}(y^{}), \]

in terms of potential functions \(f_{},g_{}\) that are chosen to satisfy the marginal constraints. After finding these potentials, we can approximate the transport map using the barycentric projection \(T_{}(y)=_{}[Y^{}|Y=y]\), for a plan \((_{Y},_{Y^{}})\). For the plan in Eq. (10), the map is given by

\[T_{_{}}(y)=e^{(g_{}(y^{})- \|y-y^{}\|^{2})/}d_{Y}(y^{})}{ e^{(g_{ }(y^{})-\|y-y^{}\|^{2})/}d_{Y}(y^{ })}. \]

In this work, we estimate the potential functions \(f_{},g_{}\) from samples, i.e., empirical approximations of the measures \(_{Y},_{Y^{}}\). Plugging in the estimated potentials in Eq. (11) defines an approximate transport map to push forward samples of \(_{Y}\) to \(_{Y^{}}\). More details on the estimation of the OT map are provided in Appendix H.

A core advantage of this methodology is that it provides us with the flexibility of changing the cost function \(c\) in Eq. (8), and embed it with structural biases that one wishes to preserve in the push-forward distribution. Such direction is left for future work.

## 4 Numerical experiments

### Data and setup

We showcase the efficacy and performance of the proposed approach on one- and two-dimensional fluid flow problems that are representative of the core difficulties present in numerical simulations of weather and climate. We consider the one-dimensional Kuramoto-Sivashinski (KS) equation and the two-dimensional Navier-Stokes (NS) equation under Kolmogorov forcing (details in Appendix F) in periodic domains. The low-fidelity (LF), low-resolution (LR) data (\(\) in Fig. 1(b)) is generated using a finite volume discretization in space  and a fractional discretization in time, while the high-fidelity(HF), high-resolution (HR) data (\(\) in Fig. 1(b)) is simulated using a spectral discretization in space with an implicit-explicit scheme in time. Both schemes are implemented with jax-cfd and its finite-volume and spectral toolboxes  respectively. After generating the HF data in HR, we run the LF solver using a spatial discretization that is \(8\) coarser (in each dimension) with permissible time steps. For NS, we additionally create a \(16\) coarser LFLR dataset by further downsampling by a factor of two the \(8\) LFLR data. See Appendix F for further details.

For both systems, the datasets consist of long trajectories generated with random initial conditions2, which are sufficiently downsampled in time to ensure that consecutive samples are decorrelated. We stress once more that even when the grids and time stamps of both methods are aligned, there is _no pointwise correspondence_ between elements of \(\) and \(\). This arises from the different modeling biases inherent to the LF and HF solvers, which inevitably disrupt any short-term correspondence over the long time horizon in a strongly nonlinear dynamical setting.

Finally, we create the intermediate space \(^{}\) in Fig. 1(b) by downsampling the HFHR data with a simple selection mask3 (i.e., the map \(C^{}\)). This creates the new HFLR dataset \(^{}\) with the same resolution as \(\), but with the low-frequency bias structure of \(\) induced by the push-forward of \(C^{}\).

**Baselines and definitions.** We define the following ablating variants of our proposed method

* Unconditional diffusion sampling (_UncondDfn_).
* Diffusion sampling conditioned on LFLR data without OT correction (_Raw cDfn_).
* [_Main_] Diffusion sampling conditioned on OT-corrected (HFLR) data (_OT+cDfn_).

We additionally consider the following baselines to benchmark our method:

* Cubic interpolation approximating HR target using local third-order splines (_Cubic_).
* Vision transformer (_ViT_)  based deterministic super-resolution model.
* Bias correction and statistical disaggregation (_BCSD_), involving upsampling with cubic interpolation, followed by a quantile-matching debiasing step.
* CycleGAN, which is adapted from  to enable learning transformations between spaces of different dimensions (_cycGAN_).
* ClimAlign (adapted from ), in which the input is upsampled using cubic interpolation, and the debiasing step is performed using AlignFlow  (_ClimAlign_).

The first two baselines require paired data and, therefore, learn the upsampling map \(^{}\) (i.e., HFLR to HFHR) and are composed with OT debiasing as factorized baselines. BCSD is a common approach used in the climate literature. The last two baselines present end-to-end alternatives and are trained directly on unpaired LFLR and HFHR samples. Further information about the implemented baselines can be found in Appendix D.

**OT training.** To learn the transport map in Eq. (11), we solve the entropic OT problem in Eq. (9) with \(=0.001\) using a Sinkhorn  iteration with Anderson acceleration and parallel updates. We use \(90,000\) i.i.d. samples of \(Y\) and \(Y^{}^{}\), and perform \(5000\) iterations. Implementations are based on the ott-jax library .

**Denoiser training and conditional sampling.** The denoiser \(D_{}\) is parametrized with a standard U-Net architecture similar to the one used in . We additionally incorporate the preconditioning technique proposed in . For \(s_{t}\) and \(_{t}\) schedules, we employ the variance-preserving (VP) scheme originally introduced in . Furthermore, we adopt a data augmentation procedure to increase the effective training data size by taking advantage of the translation symmetries in the studied systems.

Samples are generated by solving the SDE based on the post-processed denoiser \(_{}\) using the Euler-Maruyama scheme with exponential time steps, i.e., \(\{t_{i}\}\) is set such that \((t_{i})=_{}(_{}/_{})^{i/N}\) for \(i=\{0,...,N\}\). The number of steps used, \(N\), vary between systems and downscaling factors. More details regarding denoiser training and sampling are included in Appendix B.

**Metrics.** To quantitatively assess the quality of the resulting snapshots we compare a number of physical and statistical properties of the snapshots: (i) the energy spectrum, which measures the energy in each Fourier mode and thereby providing insights into the similarity between the generated and reference samples, (ii) a spatial covariance metric, which characterizes the spatial correlations within the snapshots, (iii) the KL-divergence (KLD) of the kernel density estimation for each point, which serves as a measure for the local structures (iv) the maximum mean discrepancy (MMD), and (v) the empirical Wasserstein-1 metric (Wass1). We present (i) below and leave the rest described in Appendix C as they are commonly used in the context of probabilistic modeling.

The energy spectrum is defined4 as

\[E(k)=_{||=k}|()|^{2}=_{| |=k}|_{i}u(x_{i})(-j2 x_{i}/L)|^{2} \]

where \(u\) is a snapshot system state, and \(k\) is the magnitude of the wave-number (wave-vector in 2D) \(\). To assess the overall consistency of the spectrum between the generated and reference samples using a single scalar measure, we consider the mean energy log ratio (MELR):

\[=_{k}w_{k}|(E_{}(k)/E_{}(k) )|, \]

where \(w_{k}\) represents the weight assigned to each \(k\). We further define \(w_{k}^{}=1/(k)\) and \(w_{k}^{}=E_{}(k)/_{k}E_{}(k)\). The latter skews more towards high-energy/low-frequency modes.

### Main results

**Effective debiasing via optimal transport.** Table 1 shows that the OT map effectively corrects the statistical biases in the LF snapshots for all three experiments considered. Significant improvements are observed across all metrics, demonstrating that the OT map approximately achieves \(C_{}^{}_{X} T_{}_{Y}\) as elaborated in SS3 (extra comparisons are included in Appendix H).

Indeed, the OT correction proves crucial for the success of our subsequent conditional sampling procedure: the unconditional diffusion samples may not have the correct energy spectrum (see

   &  &  &  \\ 
**Metric** & LFLR & OT-corrected & LFLR & OT-corrected & LFLR & OT-corrected \\  covRMSE \(\) & \(0.343\) & \(\) & \(0.458\) & \(\) & \(0.477\) & \(\) \\ MELRu \(\) & \(0.201\) & \(\) & \(1.254\) & \(\) & \(0.600\) & \(\) \\ MELRw \(\) & \(0.144\) & \(\) & \(0.196\) & \(\) & \(0.200\) & \(\) \\ KLD \(\) & \(1.464\) & \(\) & \(29.30\) & \(\) & \(12.26\) & \(\) \\  

Table 1: Metrics of the LFLR source and OT-corrected samples for KS and NS. The precise metric definitions are provided in Appendix C.

Figure 2: (a) KS samples generated with diffusion model conditioned on LR information with and without OT correction applied, (b) empirical probability density function for relevant LR and HR samples in KS and (c) mode-wise log energy ratios with respect to the true samples (Eq. (13) without weighted sum) at \(8\) downscaling for NS.

_UncondDfn_ in Fig. 2(c), i.e. suffering from _color shifts_ - a known problem for score-based diffusion models . The conditioning on OT corrected data serves as a sparse anchor which draws the diffusion trajectories to the correct statistics at sampling time. In fact, when conditioned on uncorrected data, the bias effectively pollutes the statistics of the samples (_Raw cDfn_ in Table 2). Fig. 2(b) shows that the same pollution is present for the KS case, despite the unconditional sampler being unbiased.

In Appendix E, we present additional ablation studies that demonstrate the importance of OT correction in the factorized benchmarks.

**Comparison vs. factorized alternatives.** Fig. 3 displays NS samples generated by all benchmarked methods. Qualitatively, our method is able to provide highly realistic small-scale features. In comparison, we observe that _Cubic_ expectedly yields the lowest quality results; the deterministic _ViT_ produces samples with color shift and excessive smoothing, especially at \(16\) downscaling factor.

Quantitatively, our method outperforms all competitors in terms of MELR and KLD metrics in the NS tasks, while demonstrating consistently good performance in both \(8\) and \(16\) downscaling, despite the lack of recognizable features in the uncorrected LR data (Fig. 3(a) bottom) in the latter case. Other baselines, on the other hand, experience a significant performance drop. This showcases the value of having an unconditional prior to rely on when the conditioning provides limited information.

**Comparison vs. end-to-end downscaling.** Although the _cycGAN_ baseline is capable of generating high-quality samples at \(8\) downscaling (albeit with some smoothing) reflecting competitive metrics, we encountered persistent stability issues during training, particularly in the \(16\) downscaling case.

**Diffusion samples exhibit ample variability.** Due to the probabilistic nature of our approach, we can observe from Table 2 that the OT-conditioned diffusion model provides some variability in the downscaling task, which increases when the downscaling factor increases. This variability provides a measure of uncertainty quantification in the generated snapshots as a result of the consistent formulation of our approach on probability spaces.

## 5 Conclusion

We introduced a two-stage probabilistic framework for the statistical downscaling problem. The framework performs a debiasing step to correct the low-frequency statistics, followed by an upsampling step using a conditional diffusion model. We demonstrate that when applied to idealized physical fluids, our method provides high-resolution samples whose statistics are physically correct, even when there is a mismatch in the low-frequency energy spectra between the low- and high

Figure 3: Example showing the vorticity field of samples debiased and super-resolved using different techniques at \(8\) (top row) and \(16\) (bottom row) downscaling factors. From left to right: **(a)** LR snapshots produced by the **low-fidelity solver** (input \(\) of Alg. 1), **(b) OT-corrected** snapshots (\(^{}\) in line \(1\) of Alg. 1), **(c) BCSD** applied to LR snapshots, **(d)** snapshots downscaled with **cycle-GAN** directly from LR snapshots, **(e) ClimAlign** applied to LR snapshots, **(f) cubic interpolation** of the OT-corrected snapshots, **(g)** deterministic upsample of the OT-corrected snapshots with **ViT**, **(h) diffusion sample conditioned on the OT-corrected snapshots** (output \(\) in Alg. 1, ours), and **(i)** two **true HR samples** in the training data with the closest Euclidean distance to the OT-corrected generated sample. The \(16\) source is the same as the \(8\) source but further downsampled by a factor of two. OT maps are computed independently between resolutions.

resolution data distributions. We have shown that our method is competitive and outperforms several commonly used alternative methods.

Future work will consider fine-tuning transport maps by adapting the map to the goal of conditional sampling, and introducing physically-motivated cost functions in the debiasing map. Moreover, we will address current limitations of the methodology, such as the high-computational complexity of learning OT-maps that scales quadratically with the size of the training set, and investigate the model's robustness to added noise in the collected samples as is found in weather and climate datasets. We will also further develop this methodology to cover other downscaling setups such as perfect prognosis  and spatio-temporal downscaling.

## Broader impact

Statistical downscaling is important to weather and climate modeling. In this work, we propose a new method for improving the accuracy of high-resolution forecasts (on which risk assessment would be made) from low resolution climate modeling. Weather and climate research and other scientific communities in computational fluid dynamics will benefit from this work for its potential to reduce computational costs. We do not believe this research will disadvantage anyone.