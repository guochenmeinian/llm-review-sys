# GRANOLA: Adaptive Normalization for Graph Neural Networks

Moshe Eliasof

University of Cambridge

me532@cam.ac.uk

&Beatrice Bevilacqua

Purdue University

bbevilac@purdue.edu

Carola-Bibiane Schonlieb

University of Cambridge

cbs31@cam.ac.uk

&Haggai Maron

Technion & NVIDIA Research

hmaron@nvidia.com

Equal Contribution

###### Abstract

Despite the widespread adoption of Graph Neural Networks (GNNs), these models often incorporate off-the-shelf normalization layers like BatchNorm or InstanceNorm, which were not originally designed for GNNs. Consequently, these normalization layers may not effectively capture the unique characteristics of graph-structured data, potentially even weakening the expressive power of the overall architecture. While existing graph-specific normalization layers have been proposed, they often struggle to offer substantial and consistent benefits. In this paper, we propose Granola, a novel graph-adaptive normalization layer. Unlike existing normalization layers, Granola normalizes node features by adapting to the specific characteristics of the graph, particularly by generating expressive representations of its nodes, obtained by leveraging the propagation of Random Node Features (RNF) in the graph. We provide theoretical results that support our design choices as well as an extensive empirical evaluation demonstrating the superior performance of Granola over existing normalization techniques. Furthermore, Granola emerges as the top-performing method among all baselines in the same time complexity class of Message Passing Neural Networks (MPNNs).

## 1 Introduction

Graph Neural Networks (GNNs) have achieved remarkable success in several application domains [35; 61], showcasing their ability to leverage the rich structural information within graph data. Recently, a plethora of different layer designs has been proposed, each tailored to address specific challenges in the context of GNNs, such as limited expressive power [63; 41; 40] and oversmoothing . Notably, analogously to architectures in other domains [30; 18], these GNN layers are often interleaved with normalization layers, as the integration of normalization methods has empirically proven beneficial in optimizing neural networks, facilitating convergence and enhancing generalization [34; 7; 55].

In practice, most existing GNN architectures employ standard normalization techniques, such as BatchNorm , LayerNorm , or InstanceNorm . However, these widely adopted normalization techniques were not originally designed with graphs and GNNs in mind. Consequently, they may not effectively capture the unique characteristics of graph-structured data, and can also hinder the expressive power of the overall architecture . These observations highlight the need for graph-specific normalization layers.

Recent works have taken initial steps in this direction, mainly targeting oversmoothing [72; 64; 74] or the expressive power of the overall architecture [11; 14]. Despite the promise shown by these methods, a consensus on a single normalization technique best suited for diverse tasks remains elusive, with no single normalization technique proving clearly superior across all benchmarks and scenarios.

**Our approach.** In this paper, we identify adaptivity to the input graph structure as a desirable property for an effective normalization layer in graph learning. Intuitively, this property ensures that the normalization is tailored to the specific input graph, capturing attributes such as the graph size, node degrees, and connectivity. Importantly, we claim and demonstrate that, given the limitations of practical GNNs, achieving full adaptivity requires expressive architectures that can detect and disambiguate graph substructures, thereby better adapting to input graphs.

Guided by this desirable property, which is absent in existing normalization methods, we introduce our proposed approach - Granola (Graph Adaptive Normalization Layer). Granola aims at dynamically adjusting node features at each layer by leveraging learnable characteristics of the node neighborhood structure derived through the utilization of Random Node Features (RNF) [44; 1; 51; 56; 17]. More precisely, Granola samples RNF and uses them in an additional _normalization_ GNN to obtain expressive intermediate node representations. The intermediate representations are then used to scale and shift the node representations obtained by the preceding GNN layer.

We present theoretical results that justify the primary design choices behind our method. Specifically, we demonstrate that Granola is fully adaptive to the input graph, which, in other words, means that Granola can predict different normalization values for non-isomorphic nodes. This property arises from the maximal expressive power of the normalization GNN we employ (MPNN augmented with RNF [1; 51]). In addition, we show that our method inherits this expressive power. Lastly, we show that using standard MPNN layers without RNF within Granola cannot result in a fully adaptive method or in any additional expressive power.

Empirically, we show that Granola significantly and consistently outperforms all existing standard as well as GNN-specific normalization schemes on a variety of different graph benchmarks and architectures. Furthermore, Granola proves to be the best-performing method among all baselines that have the same time complexity as the most widely adopted GNNs, namely the family of MPNNs.

**Contributions.** Our contributions are as follows: (1) We provide an overview of different normalization schemes in graph learning, outlining adaptivity as a desirable property of normalization layers that existing methods are missing, (2) We introduce Granola, a novel normalization technique adjusting node features based on learnable characteristics of their neighborhood structure, (3) We present an intuitive theoretical analysis of our method, giving insights into the design choices we have made, and (4) We conduct an extensive empirical study, providing a thorough benchmarking of existing normalization methods and showcasing the consistently superior performance of Granola.

## 2 Normalization layers for GNNs

### Basic setup and definitions

Let \(G=(,)\) denote graph with \(N\) nodes, where \(^{N N}\) is the adjacency matrix and \(^{N C}\) is the node feature matrix, with \(C\) the feature dimension. Consider a batch of \(B\) graphs encoded by the adjacency matrices \(\{_{b}\}_{b=0}^{B-1}\), and, for simplicity, assume that all graphs in the batch have the same number of nodes \(N\). We consider a model composed of \(L\) GNN layers, with \(L\). Each GNN layer is followed by a normalization layer Norm and an activation function \(\). At any layer \([L]\), the output of the GNN layer for a batch of graphs consists of (intermediate) node representations, which can be gathered in a matrix \(}^{()}^{B N C}\) (since all graphs have the same number of nodes per our assumption2). These undergo a normalization and an activation layer, resulting in node representations denoted by \(^{()}^{B N C}\), which serve as input of the next GNN layer, with \(^{(0)}\).3 Throughout this paper, for any three-dimensional tensor, we use subscripts to denote access to a corresponding dimension. For example, we denote the intermediate node representations of graph \(b[B]\) by \(}_{b}^{()}^{N C}\), and by \(_{b,n,c}^{()}\) the value of feature \(c[C]\) in node \(n[N]\) of graph \(b[B]\).

Formally, the intermediate, _pre-normalized_ node features for the \(b\)-th graph in the batch are defined as

\[}_{b}^{()}=_{}^{(-1)}( _{b},_{b}^{(-1)}).\] (1)

Then, the overall update rule for the batch of \(B\) graphs can be written as

\[^{()}=((}^{()}; )),\] (2)

for \([L]\). Equation (2) serves as a general blueprint, and in what follows we will show different ways to customize it in order to implement different existing normalization techniques.

We consider normalization layers based on the standardization  of their inputs, as this represents the most common choice in widely used normalizations. Generally, a standardization-based normalization layer first shifts each element \(_{b,n,c}^{()}\) by some mean \(_{b,n,c}\), and then scales it by the corresponding standard deviation \(_{b,n,c}\), i.e.,

\[(_{b,n,c}^{()};}^{()},)= _{c}^{()}_{b,n,c}^{()}-_{b,n,c}}{_{b,n, c}}+_{c}^{()},\] (3)

where \(_{c}^{()}\), \(_{c}^{()}\) are learnable _affine_ parameters, that do not depend on \(b\) nor \(n\).

### Current normalization layers for GNNs

The difference among different normalization schemes lies in the set of values used to compute the mean and standard deviation statistics for each element, or, more precisely, across which dimensions of \(}^{()}\) they are computed. We present them below, and visualize them in Figure 1.

**BatchNorm.** BatchNorm  computes the statistics across all nodes and all graphs in the batch, for each feature separately. Therefore, we have

\[_{b,n,c}=_{b=1}^{B}_{n=1}^{N}_{b,n,c}^{() },_{b,n,c}^{2}=_{b=1}^{B}_{n=1}^{N}(_{b,n, c}^{()}-_{b,n,c})^{2},\] (4)

which implies that \(_{b,n,c}=_{b,n^{},c}\) for any \(b^{}[B]\) and any \(n^{}[N]\) (and similarly for \(_{b,n,c}^{2}\)). Despite the widespread use of BatchNorm in graph learning , it is important to recognize scenarios where alternative normalization schemes might be more suitable. A concrete example illustrating this need is presented in Figure 2, focusing on the task of predicting the degree of each node.4 In this example, BatchNorm, by subtracting the mean computed across the batch, results in negative values for nodes with outputs below the mean. The subsequent ReLU application, as standard practice , zeros out these negative values, leading to predictions of 0 for such nodes, irrespective of their actual degree. Additional insights are further discussed in Example C.1 in Appendix C, where it is demonstrated that relying on the affine parameter \(_{c}^{()}\) to shift the negative output does not provide a definitive solution, since \(_{c}^{()}\) is the same for all graphs.

Figure 1: Illustration of normalization layers. We denote by \(B,N\) and \(C\) the number of graphs (batch size), nodes, and channels (node features), respectively. For simplicity of presentation, we use the same number of nodes for all graphs. We color in blue the elements used to compute the statistics employed inside the normalization layer.

**InstanceNorm.** InstanceNorm  is similar to BatchNorm, but considers each graph separately, that is

\[_{b,n,c}=_{n=1}^{N}_{b,n,c}^{()},\] (5) \[_{b,n,c}^{2}=_{n=1}^{N}(_{b,n,c}^{( )}-_{b,n,c})^{2},\]

which implies that \(_{b,n,c}=_{b,n^{},c}\) for any \(n^{}[N]\) (and similarly for \(_{b,n,c}^{2}\)). Notably, the example in Figure 2 can be extended to InstanceNorm by considering all graphs in the batch as disconnected components in a single graph, as we show in Example C.4 in Appendix C.

**LayerNorm.** LayerNorm  can be defined in two ways in the context of graphs . The first, which we call _LayerNorm-node_, behaves similarly to LayerNorm in Transformer architectures , and computes statistics across the features for each node separately, that is

\[_{b,n,c}=_{c=1}^{C}_{b,n,c}^{()},\] (6) \[_{b,n,c}^{2}=_{c=1}^{C}(_{b,n,c}^{ ()}-_{b,n,c})^{2},\]

and therefore \(_{b,n,c}=_{b,n,c^{}}\) for any \(c^{}[C]\) (and similarly for \(_{b,n,c}^{2}\)). The second variant, which we call _LayerNorm-graph_, is similar to LayerNorm in Computer Vision , and computes statistics across the features and across all the nodes in each graph, that is

\[_{b,n,c}=_{n=1}^{N}_{c=1}^{C}_{b,n,c}^{() },_{b,n,c}^{2}=_{n=1}^{N}_{c=1}^{C}(_{b,n,c}^{()}-_{b,n,c})^{2},\] (7)

and therefore \(_{b,n,c}=_{b,n^{},c^{}}\) for any \(n^{}[N]\) and any \(c^{}[C]\) (and similarly for \(_{b,n,c}^{2}\)). Example C.5 in Appendix C presents a motivating example similar to Figure 2 for LayerNorm.

**Graph-Specific normalizations.** Several normalization methods tailored to graphs have been recently proposed. We categorize them based on which dimensions (the batch dimension \(B\), the node dimension \(N\) within each graph, and the channel dimension \(C\)) are used to compute the statistics employed within the normalization layer. We illustrate this categorization in Figure 1. DiffGroupNorm  and GraphSizeNorm  normalize features considering nodes across different graphs, akin to BatchNorm. Similarly to InstanceNorm, PairNorm  and MeanSubtractionNorm  shift the input by the mean computed per channel across all the nodes in the graph, with differences in the scaling strategies. GraphNorm  extends InstanceNorm by incorporating a multiplicative factor to the mean. NodeNorm  behaves similarly to LayerNorm-node but only scales the input, without shifting it. We provide additional details in Appendices A and B, and include methods that normalize the adjacency matrix before the GNN layers, which, however, fall outside the scope of this work.

## 3 Method

The following section describes the proposed Granola framework. We start with identifying adaptivity as a desired property in a graph normalization layer.

Figure 2: A batch of two graphs, where subtracting the mean of the node features computed across the batch, as in BatchNorm and related methods, results in the loss of capacity to compute node degrees.

Motivation.In the previous section, we observed that current normalization schemes used within GNNs are mostly borrowed or adapted from other domains and that they possess two main limitations: (i) Using BatchNorm and InstanceNorm (as well as many methods derived from them, including graph-specific methods) can limit the expressive power of GNNs that use them, preventing them from being able to represent even very simple functions such as computing node degrees; (ii) As shown in previous works, as well as in our experiments in Section 5, these methods do not provide a consistent benefit in downstream performance on different tasks and datasets.

One possible reason for these limitations is that the discussed methods use the same affine parameters in the normalization process, irrespective of the input graph. Crucially, unlike other more structured data types, such as images and time series, graphs do not differ merely in the values associated with each element or time step. Instead, graphs fundamentally vary in the actual connectivity between the nodes. Therefore, it might make sense to employ different (adaptive) normalization schemes based on the features and the connectivity of the input graph. Next, we explore this concept and show that carefully accounting for the graph structure in the normalization process may enhance the expressive power of the GNN, overcoming the previously mentioned failure cases, and providing consistent experimental behavior and overall improved performance.

### Design considerations and overview

In an adaptive normalization, instead of using the same affine parameters \(_{c}^{()}\) and \(_{c}^{()}\) (Equation (3)) for all the nodes in all the graphs, the normalization method utilizes specific parameters conditioned on the input graph. Importantly, in other domains, adaptivity in normalization techniques has proven to be a valuable property [19; 33; 48; 76; 47; 49]. In Granola, we achieve this property by generating affine parameters using an additional _normalization_ GNN that takes the graph as input, similarly to Hypernetworks  that generate weights for another network. Importantly, for full graph adaptivity, where the network can assign different normalization parameters for every pair of non-isomorphic nodes, the normalization GNN should have maximal expressive power. Due to the limited expressive power of MPNNs [41; 63], when designing Granola, we advocate for using a normalization GNN with high expressiveness. Notably, most expressive GNNs come at the expense of significantly increased computational complexity [16; 71; 4; 69; 40; 43]. For this reason, we parameterize the normalization GNN using an MPNN augmented with Random Node Features (RNF), which provide strong function approximation guarantees [1; 51] while retaining the efficiency of standard MPNNs.

### Granola

We are now ready to describe our Granola approach, which is based on the utilization of an additional GNN to compute the affine parameters and the integration of RNF . These affine parameters are then used to normalize node representations obtained by the GNN layer preceding the normalization. An overview of Granola is visualized in Figure 3 and described in Algorithm 1.

Granola layer.At any given layer \([L]\), for each graph \(b[B]\), we sample RNF \(_{b}^{()}^{N K}\), \(K\), from some joint probability distribution, e.g., Normal, \(_{b}^{()}\). Then, we concatenate \(_{b}^{()}\) with the intermediate node features \(}_{b}^{()}\) obtained from \(_{}^{(-1)}\) (Equation (1)), and pass the resulting feature matrix through an additional GNN, i.e.,

\[_{b}^{()}=_{}^{()}(_{b},}_{b}^{()}_{b}^{()}),\] (8)

where \(\) denotes feature-wise concatenation, and \(_{}^{()}\) is a shallow GNN architecture, with \(L_{}^{()}\) layers. The affine parameters are then computed from \(_{b}^{()}^{N C}\), that is

\[_{b,n}^{()}=f_{1}^{()}(z_{b,n}^{()}),_{b,n}^{( )}=f_{2}^{()}(z_{b,n}^{()}).\] (9)

where \(f_{1}^{()},f_{2}^{()}\) are learnable functions (e.g., MLPs), and \(_{b,n}^{()},_{b,n}^{()}^{C}\). Then, a Granola layer is defined as

\[(_{b,n,c}^{()};}^{()},)= _{b,n,c}^{()}_{b,n,c}^{()}-_{b,n,c}}{_{ b,n,c}}+_{b,n,c}^{()},\] (10)

where \(_{b,n,c}\) and \(_{b,n,c}\) are the mean and std of \(}^{()}\), computed per node across the feature dimension, exactly as in LayerNorm-node (Equation (6)). We highlight here a noticeable difference compared to standard normalization formulations, as presented in Equation (3): in Granola, \(_{b,n,c}^{()},_{b,n,c}^{()}\) not only depend on \(c\), but they also have a dependency on \(b\) and \(n\) and indeed vary for different graphs and nodes. Notably, this is possible because our normalization is adaptive to the input graph. Methods that disregard this information are compelled to use the same learnable normalization values for all graphs, as they operate without knowledge of the specific input graph being considered.

Granola-no-rnf.We consider a variant of Granola that does not sample RNF and instead uses only \(}_{b}^{()}\) to obtain \(_{b}^{()}\) in Equation (8), which therefore becomes

\[_{b}^{()}=_{}^{()}(, }_{b}^{()}).\] (11)

We refer to this variant as Granola-no-rnf, as it allows us to directly quantify the contribution of the expressiveness offered by augmenting it with RNF as in Equation (8).

Complexity.We conclude by remarking that both Granola and its variant, Granola-no-rnf, do not impact the asymptotic time and space complexity of standard MPNNs, which remains linear in the number of nodes and edges, as we show in Appendix G.

## 4 Theoretical Analysis

In this section, we explain the main design choices taken in the development of Granola. Specifically, we elaborate on the advantages obtained by utilizing RNF as part of the normalization scheme as opposed to relying solely on the node features. We assume all GNN layers, including those within Granola (Equation (8)) are message-passing layers, as the ones in Xu et al. , Morris et al. .

**The advantages of using RNF-augmented MPNNs for normalization.** We start by observing that the integration of our Granola in an MPNN allows to easily default to an MPNN augmented with RNF , as we formalize in Proposition E.2 in Appendix E. The idea of the proof lies in the ability of the first normalization layer to default to outputting its input RNF, enabling the rest of the architecture to function as an MPNN augmented with these RNF. The significance of this result lies in the fact that MPNN + Granola inherits the \((,)\)-universal approximation properties previously proved for MPNNs augmented with RNF . This naturally solves the limitations of existing normalizations identified in Section 2.2, as an MPNN + Granola can leverage universality to approximate such functions. Importantly, the universality of MPNNs augmented with RNF further implies that Granola is _fully adaptive_, as it can approximate any equivariant function on the input graph , and therefore can approximate functions returning different normalization values for non-isomorphic nodes.

**Why RNF are necessary?** While Granola employs RNF to compute the normalization affine parameters \(_{b,n,c}^{()}\) and \(_{b,n,c}^{()}\), the same procedure can be applied without the use of RNF, a variant we denoted as Granola-no-rnf in Section 3 (Equation (11)). However, we theoretically demonstrate next that an MPNN + Granola-no-rnf not only loses the universal approximation properties, but is also not more expressive than standard MPNNs.

**Proposition 4.1** (RNF are necessary in Granola for increased expressive power).: _Assume our input domain consists of graphs of a specific size. For every MPNN with Granola-no-rnf (Equation (11)) there exists a standard MPNN with the same expressive power._

Proposition 4.1 is proven by showing that an MPNN with Granola-no-rnf can be implemented by a standard MPNN, and, therefore, its expressive power is bounded by the expressive power of a standard MPNN. The proof can be found in Appendix E. This limitation underscores the significance of RNF within Granola. Furthermore, our experiments in Section 5 show that omitting the RNF within the normalization results in degraded performance, as Granola always outperforms Granola-no-rnf. Finally, we remark that this theoretical result emphasizes the necessity of RNF for increased expressiveness and, consequently, for ensuring full adaptivity to the input graph. That is, any normalization generated by Granola-no-rnf will be limited by the expressive power of standard MPNNs while Granola does not have this limitation.

**Relation to expressive GNNs.** The results in this section highlight the connection between Granola and expressive GNNs, as our method inherits enhanced expressiveness of MPNNs augmented with RNF. Notably, while MPNNs with RNF have demonstrated theoretical improvements in expressiveness, their practical performance on real-world datasets has not consistently reflected these benefits . Our experimental results indicate that Granola serves as a valuable bridge between theory and practice. Specifically, our findings address the gap between the theoretical expressiveness of MPNNs that use RNF, and their limited practical utility. This is achieved by effectively incorporating RNF within the normalization process rather than simply treating it as an additional input to the MPNN. Importantly, Granola gives rise to a method that is efficient, provably expressive, and performs well in practice, something that, to our knowledge, has never been accomplished by any previous architecture. This provides an additional perspective to our contribution: beyond designing an effective and efficient normalization layer, which is the main scope of our work, we offer a practical approach to realizing the theoretical benefits of RNF. Finally, we conclude this section by remarking that \(_{}^{()}\) in Equation (8) can be modified to be any other expressive architecture, and our design choice was mainly guided by the computational practicality of RNF, that allows Granola to offer increased expressive power while retaining the linear complexity of MPNNs, as discussed in Appendix G. We refer the reader to Morris et al.  for a survey on expressive methods.

## 5 Experimental Results

In this section, we evaluate the performance of Granola. In particular, we seek to address the following questions: (1) _How does Granola compare to other normalization methods?_ (2) _Does Granola achieve better performance than its natural baselines that also leverage RNF, that is, how important is the graph-adaptivity within Granola?_ (3) _How does Granola compare to its variant Granola-no-rnf, which does not leverage RNF within the normalization, that is, how important are the RNF within Granola?_ In what follows, we present our main results and refer to Appendices F to H for details and additional experiments, including timings and ablation studies. Our code is available at https://github.com/MosheEliasof/GRANOLA.

[MISSING_PAGE_FAIL:8]

ter) 0.960, compared to the second best graph normalization layer, GraphNorm, that achieves 1.044. It is also noteworthy to mention the performance gap between Granola and its natural baselines, such as RNF-PE, which emphasizes the practical benefit of graph-adaptivity within Granola.

**TUDatasets.** We experimented with popular datasets from the TUD  repository. Our results are reported in Table 14 in Appendix H.8, suggesting that Granola consistently achieves higher accuracy compared to its natural baselines, as well as standard and graph-designated normalization techniques. For example, on the NCI109 dataset, Granola achieves an accuracy of \(83.7\%\), compared to the second-best normalization technique, GraphNorm, with an accuracy \(82.4\%\).

**Training convergence of Granola.** In addition to improved downstream task performance being one of the main benefits of a normalization layer, accelerated training convergence is also an important desired property [34; 11]. Figure 4 shows that Granola offers faster convergence and lower MAE compared to other methods.

**Combining Granola with expressive methods.** Since our goal is to understand the impact of normalization layers, our experiments focus on studying the benefit of augmenting standard and well-understood MPNNs with Granola. However, it is also interesting to understand if expressive, domain-expert approaches such as GSN  can also benefit from Granola. To this end, we augment GSN with Granola, and report the results in Table 3. These results further underscore the versatility of Granola, which can be coupled with any GNN layer and improve its performance.

**Discussion.** Our experimental results cover standard normalization layers, as well as graph normalization methods, evaluated on 11 datasets from diverse sources, and applied to various tasks. Throughout all experiments, a common theme is the performance _consistency_ of Granola. Specifically, Granola always improves over its natural baselines and other normalization techniques across all datasets. In contrast, other existing methods exhibit less clear trends in their performance. While some methods achieve competitive results on certain datasets, they may struggle on others. Notable examples are GraphNorm and PairNorm, which, despite offering improved performance compared to BatchNorm on most of the OGB datasets, show worse results on ZINC-12k. Furthermore, standard normalization layers also lack consistency. As an example, consider InstanceNorm, which is beneficial in some OGB datasets, yet, it does not offer favorable results on ZINC-12k.

## 6 Conclusions

In this paper, we discuss the existing landscape of feature normalization techniques in Graph Neural Networks (GNNs). Despite recent advances in designing graph normalization techniques, the optimal choice remains unclear, with methods not offering consistent performance improvements across tasks. To address this challenge, we identify a desirable property of graph normalization layers, namely

Figure 4: Training convergence of Granola compared with existing normalization techniques show that Granola achieves faster convergence and overall lower (better) MAE.

    & ZINC & molesol & moutox21 & moblace & mobliv \\  & MAE \(\) & RMSE \(\) & ROC-AUC \(\) & ROC-AUC \(\) & ROC-AUC \(\) \\  GSN  & 0.1010\(\)0.010 & 1.003\(\)0.037 & 76.08\(\)0.79 & 77.40\(\)2.92 & 80.39\(\)0.90 \\ GSN + Granola & 0.0766\(\)0.008 & 0.941\(\)0.024 & 77.84\(\)0.63 & 80.41\(\)2.07 & 81.12\(\)0.79 \\   

Table 3: Empirical results of GSN with Granola show that Granola can also improve the performance of expressive methods.

adaptivity to the input graph, and argue that it can be obtained only with expressive architectures. To incorporate this property, we present Granola, a normalization layer that adjusts node features based on the specific input graph, leveraging Random Node Features (RNF). Our theoretical analyses support the design choices of Granola, demonstrating its increased expressiveness. Empirical evaluations across diverse benchmarks consistently highlight the superior performance of Granola over existing normalization methods, as well as other baselines with the same time complexity.

**Limitations and impact.** Although Granola exhibits promising results, there are areas for potential improvement in future research. For instance, investigating alternative designs for \(^{()}_{}}\) could further enhance the performance, as, in certain datasets, there is still a gap between Granola and expressive GNNs. Additionally, exploring ways to reduce memory and time complexity (which are still linear in the number of nodes) represents an important avenue for future research. Furthermore, by improving the performance of GNN through Granola we envision a positive impact in domains such as drug discovery.