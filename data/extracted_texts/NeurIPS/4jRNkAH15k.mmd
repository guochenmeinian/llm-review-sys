# DETAIL: Task DEmonsTration Attribution for Interpretable In-context Learning

Zijian Zhou\({}^{13}\) Xiaoqiang Lin\({}^{1}\) Xinyi Xu\({}^{12}\) Alok Prakash\({}^{3}\)

Daniela Rus\({}^{34}\) Bryan Kian Hsiang Low\({}^{1}\)

\({}^{1}\)Department of Computer Science, National University of Singapore, Singapore

\({}^{2}\)Institute for Infocomm Research, A*STAR, Singapore

\({}^{3}\)Singapore-MIT Alliance for Research and Technology Centre, Singapore

\({}^{4}\)CSAIL, MIT, USA

{zhzijian,xiaoqiang.lin,xuxinyi,lowkh}@comp.nus.edu.sg

{zijian.zhou,alok.prakash}@smart.mit.edu

rus@csail.mit.edu

###### Abstract

In-context learning (ICL) allows transformer-based language models that are pre-trained on general text to quickly learn a specific task with a few "task demonstrations" without updating their parameters, significantly boosting their flexibility and generality. ICL possesses many distinct characteristics from conventional machine learning, thereby requiring new approaches to interpret this learning paradigm. Taking the viewpoint of recent works showing that transformers learn in context by formulating an internal optimizer, we propose an influence function-based attribution technique, DETAIL, that addresses the specific characteristics of ICL. We empirically verify the effectiveness of our approach for demonstration attribution while being computationally efficient. Leveraging the results, we then show how DETAIL can help improve model performance in real-world scenarios through demonstration reordering and curation. Finally, we experimentally prove the wide applicability of DETAIL by showing our attribution scores obtained on white-box models are transferable to black-box models in improving model performance.

## 1 Introduction

The rapid development of transformer-based language models [12; 14; 17; 71] has inspired a new in-context learning (ICL) paradigm , which allows a language model sufficiently pre-trained on general text to quickly adapt to specific tasks. This lightweight approach of customizing a general model for specific tasks is in contrast to fine-tuning [31; 68] that necessitates both access to the model parameters and resource-intensive step of tuning these parameters for model adaptation. In ICL, a few task demonstrations are included in the input text (i.e., prompt) together with a query to help the language model better understand how to answer the query. It has been shown that including task demonstrations in the prompt can enhance the capability of language models to apply common sense and logical reasoning [65; 72] and learn patterns from the supplied demonstrations , significantly enhancing the flexibility and generality of language models. In the ICL paradigm, each demonstration can be viewed as a "training data point" for ICL. Analogous to how the performance of a conventional supervised machine learning (ML) model depends on the quality of training data, the performance of ICL depends on the quality of task demonstrations . A research question naturally arises: How to attribute and interpret ICL demonstrations that are helpful or harmful for model prediction?

Though there are many prior works on interpreting and attributing model prediction for conventional ML models [25; 34; 52], these methods are not readily applicable to ICL due to its unique characteristics. Firstly, many existing attribution techniques require either computing the gradients  or multiple queries to the model , both of which are slow and computationally expensive. In contrast, ICL is often applied in _real-time_ to a large foundation model  that necessitates the attribution approaches for ICL to be fast and efficient. Secondly, ICL is known to be sensitive to ordering: The same set of demonstrations can result in significantly different model performance under different permutations [42; 44]. However, conventional methods do not explicitly consider the ordering of training examples. Thirdly, ICL demonstration is usually supplied as a sentence comprising a sequence of tokens, rendering conventional token-level attribution methods ineffective, as they do not capture contextual information of each ICL demonstration [6; 58]. Lastly, ICL does not update model parameters, rendering conventional techniques that analyze model parameter change  not applicable. Moreover, the absence of the need to update model parameters also allows a good attribution result for ICL to be transferable across different language models.

To address these challenges, we propose DETAIL, a novel technique that takes advantage of a classical attribution approach while tackling the unique characteristics of ICL. We adopt the perspective that transformers formulate an internal optimizer [3; 16; 61; 69] for ICL. Based on this internal optimizer, we design a method to understand the impact of each demonstration on the transformer's prediction. Notably, this approach allows us to leverage powerful existing analysis tools for transformer-based models in ICL, where otherwise the characteristics of ICL make applying these tools difficult.

Specifically, we describe an intuitive (re-)formulation of the influence function , a popular attribution method for conventional ML, on the internal optimizer and show that DETAIL addresses the challenges of computational cost, sensitivity to order, and attribution quality. Then, we empirically verify that our formulation can identify demonstrations helpful for model prediction and outlier demonstrations. Additionally, we apply our method to tasks with real-world implications including prompt reordering, noisy demonstration detection, and demonstration curation to show its effectiveness. We demonstrate that DETAIL achieves improved performance when applied to typical white-box large language models (LLMs). Furthermore, as many powerful LLMs are currently closed-source (thus black-box), we show that our DETAIL score obtained on a white-box LLM (e.g., Vicuna-7b ) exhibits transferable characteristics to the performance on a popular black-box model (ChatGPT).

## 2 Related Work

Understanding in-context learning.Prior works have attempted to understand the ICL capability of transformer-based language models [2; 3; 7; 10; 14; 20; 24; 37; 49; 51; 69; 61; 69; 74].  empirically demonstrated that language models can act as few-shot learners for unseen NLP tasks.  explained ICL by viewing attention heads as implementing simple algorithms for token sequence completion. [51; 69] studied the ICL capability of transformers by casting it as an implicit Bayesian inference. [3; 10; 24] further showed that transformers can learn (regularized) linear and discrete functions in context.  showed that transformers can adaptively implement appropriate algorithms for ICL.  provided a statistical generalization bound for ICL. [20; 61; 62; 74] mathematically showed that transformers with specific parameters can perform gradient descent on parameters of an internal optimizer given the demonstrations. [2; 74] further proved that the parameters (of the internal optimizer) can converge during forward passing. Inspired by the theoretical grounding, which is the focus of these works, we design our novel attribution technique for task demonstrations by adopting a similar view (i.e. transformers learn in context by implementing an optimization algorithm internally).

Data attribution.Past works have focused on explaining and attributing model performance to training data of conventional ML [8; 19; 25; 26; 34; 40; 52; 56; 70; 79]. The rise of LLMs has inspired research efforts on attribution w.r.t. prompts with a focus on task demonstrations [11; 43; 45; 54; 73], which is distinct from training data attribution since demonstrations are provided in context. Specifically,  used human annotators to evaluate the verifiability of attributing model answers to a prompt. [11; 73] relied on LLMs to evaluate attribution errors. These prior works are either computationally heavy (requiring additional queries of LLMs) or time-consuming (requiring human annotators).  proposed an interpretability toolkit for sequence generation models using gradient- and perturbation-based methods. , a contemporary work, proposed to use a decoder module on the token embeddings for per-token attribution but requires costly training to learn the decoder weights. Moreover, these methods do not specifically target demonstration attribution. Some prior techniques [19; 25] can be adapted for attributing ICL in LLMs but may be costly or ineffective. We empirically compare our method with those and the attribution methods consolidated in .

Attribution in LLMs.Past works have attempted to apply various methods including influence in attributing language models [26; 32; 35; 38; 39; 48; 63; 53; 67; 68; 75].  considered a simplification of the influence function for task demonstration curation. [26; 35; 68] applied influence to pre-training and fine-tuning data of LLMs.  used influence to select demonstration inputs for annotation.  builds a classifier on the embeddings of demonstrations using a small LLM and computes influence w.r.t. the classifier for demonstration selection. In contrast, we demonstrate various use cases of our method including on-the-fly demonstration curation, reordering, and noisy demonstration detection. A contemporary work that shares technical similarity  focuses on demonstration selection whereas we focus on attribution and  is shown to be less effective than our method in Sec. 5.3. Additionally, compared to prior works leveraging influence to address specific problems, we apply influence function to provide a _general attribution_ for demonstrations, with many applications that we empirically show.

## 3 Preliminaries

In-context learning (ICL).ICL is a learning paradigm that provides a few task demonstrations with formatted input and output for a pre-trained transformer (e.g., an LLM) to learn the mapping function from inputs to outputs in context (i.e., via the forward pass) . Formally, a transformer model \(f\) takes in a prompt \(p(,x_{})\) comprising a formatted sequence of demonstrations \(=(z_{1},z_{2},,z_{n})\) where \(z_{i}=\{x_{i},y_{i}\}\) from a specific downstream task along with a query input \(x_{}\) (i.e., prompt) to predict its label as \(_{}=f(p(,x_{}))\). An visual for an example prompt is provided in App. C. We wish to attribute the model prediction \(_{}\) to each \(z_{i}()\).

ICL as implementing an internal optimizer.With the growing interest in the internal mechanism of transformers, previous works [3; 61; 62; 74] have theoretically shown that ICL can be treated as the transformer implementing an internal optimizer on the ICL demonstrations. Specifically, [61; 62; 74] formulated the objective of ICL optimizer with (regularized) mean-squared error on a linear weight applied to the token itself in linear self-attentions (LSAs)  or a transformation of tokens (i.e., kernelized mean-squared error) if an extra multi-layered perception is attached before the LSA [61; Proposition 2] in a recurrent transformer architecture. The transformer layers then function as performing gradient descent on the weight to minimize the objective [61; 74].

Influence function.Influence function  approximates the change of the loss of a test data point \(z_{}\) when up-weighting a training data point \(z_{i}\). Formally, the influence of \(z_{i}\) on predicting \(z_{}\) is1

\[(z_{i},z_{})_{}L(z_{}, )^{}_{}(z_{i})=_{}L(z_{ {test}},)^{}H_{}^{-1}_{}L(z_{i}, {})\] (1)

where \(L(z_{},)\) refers to the loss (function) on a test point \(z_{}\) of the model parameterized by \(\) and \(H_{} 1/n_{i=1}^{n}_{}^{2}L(z_{i},)\) is the Hessian. However, this definition cannot be directly applied to ICL since there is no model parameter change during ICL, unlike the learning settings in [8; 34]. We show how to adapt the formulation of Eq. (1) to ICL in our proposed method DETAIL next.

## 4 Influence Function on Internal Kernel Regression

Following the idea that transformers learn in context by implementing an internal kernelized least-square objective, we present our formulation of DETAIL by computing the influence function on a kernelized linear regression . Specifically, we build the regression w.r.t. the following kernel

\[k(x,x^{}) m(x)^{}m(x^{})\] (2)

where \(m(x)^{1 d}\) refers to (the mapping of an ICL demonstration2 to) an internal representation of \(x\) (e.g., hidden state of a transformer layer) with output dimension \(d\). Let \(X(x_{1},x_{2},,x_{n})\)and \(Y(y_{1},y_{2},,y_{n})\) be the vectors of inputs and outputs in \(\) respectively. The equivalent kernel regression can be written as \( m(X)\) where \(^{d 1}\) is the weight vector over the kernelized feature space. In practice, the dimension \(d\) of \(m\) is usually much larger than the number of demonstrations, causing severe over-parameterization. Such over-parameterization renders the influence values fragile . As such, we follow  and adopt an \(_{2}\) regularization on \(\) controlled by a hyper-parameter \(\), which forms a _kernelized ridge regression_ with loss:

\[L(x,y)=[m(x)-y]^{2}+^{}\,.\] (3)

Taking the \(2\)nd derivative of Eq. (3), we obtain the hessian \(H_{}\) as

\[H_{}(1/n)_{i=1}^{n}_{}^{2}L(x_{i},y_{i})=(2/n) _{i=1}^{n}(m(x_{i})^{}m(x_{i})+ I)\.\] (4)

Adopting a matrix multiplication form for the summation in Eq. (4), we write the influence of training data on the model parameters \(\) as follows,

\[_{}(z) H_{}^{-1}_{}L(x,y)=n(K+  I)^{-1}[m(x)^{}(m(x)-y)+]\] (5)

where \(K m(X)^{}m(X)^{d d}\) is the Gram matrix and \(_{}^{d}\) refers to the influence of a particular demonstration \((x,y)\) w.r.t. the kernel regression weights \(\). Then, combining Eqs. (1), (3) and (5), we can express the DETAIL score as the influence of a demonstration \(z\) on a query \(z_{}\):

\[&(z_{},z)_{ }L(x_{},y_{})^{}_{}(z)\\ &=n[m(x_{})^{}(m(x_{})-y_{})+](K+ I)^{-1}[m(x)^{}(m(x)-y)+]\] (6)

where \(\) has a closed-form expression (shown in Alg. 1 in App. B). While inverting matrices in Eq. (6) requires \((d^{3})\) time, \(d\) is usually in the thousands: A typical LLM like Llama-2-7b  has an embedding size \(d=4096\), allowing reasonable computation time of \(\) (e.g., a few seconds). In practice, this computation is accelerated by the techniques already implemented in existing scientific computation libraries admitting sub-cubic complexity for matrix inversion.

Computing self-influence.One important application of the influence function in ML is identifying outliers via self-influence [8; 34]. The conventional definition of \(\) trivially admits computing self-influence simply by replacing \(z_{}\) in Eq. (1) with \(z_{i}\). While the same approach applies to DETAIL, there are two shortcomings: (i) As the embedding is sensitive to the position, placing the same demonstration at the end of the prompt (as a query) or in the middle (as a demonstration) results in different embeddings, leading to unreasonable influence score. (ii) For each demonstration, it needs one forward pass of the model to compute the self-influence, which can be costly when the ICL dataset size is large. Instead, we implement self-influence for DETAIL by _reusing_ the demonstration's embedding. This way, we keep the two sides of Eq. (1) consistent and only require one forward pass of the model to compute the self-influence for all demonstrations.

Further speedup via random matrix projection.While the current formulation in Eq. (6) is already computationally cheap, a relatively large embedding size (e.g. \(d=4096\) for Llama-2-7b) can become a bottleneck as inverting the matrix can be relatively slow. We apply an insight that for ICL, much of the information in the embedding \(m\) is redundant (we do not need a \(4096\)-dimensional \(\) to fit \(20\) demonstrations). Hence, we project \(m\) to a much lower dimensional space via a random matrix projection while preserving the necessary information, following the Johnson-Lindenstrauss lemma [21; 33], precisely represented as a projection matrix \(P^{d d^{}}\) with each entry i.i.d. from \((0,1/d^{})\). We provide a more detailed discussion in App. C. Empirically, we show that we can compress \(m\) to a much smaller dimension \(d^{} 1000\), resulting in a \(10\) computation speedup on a typical 7B LLM on an NVIDIA L40 GPU (see Sec. 5.2).

A visualization of our proposed method is in Fig. 1 and a pseudo-code implementation is in App. B.

## 5 Empirical Evaluation

We evaluate the effectiveness of DETAIL (i.e., Eq. (6)) on two metrics: computational time (via logged system time required) and effectiveness in attribution (via performance metrics for tasks). We start by visualizing how the DETAIL scores, particularly \((z_{},z_{i})\) (test influence, abbreviated as \(_{}\)) and \((z_{i},z_{i})\) (self influence, abbreviated as \(_{}\)) following [34; Sections 5.1 & 5.4], attribute demonstrations to a query first on a custom transformer and then on LLMs. Note that the hyper-parameter \(\) varies under different scenarios and we discuss some heuristics for setting \(\) in App. C.

Enforcing ICL behavior.We consider tasks where transformers learn from the demonstrations and form an internal optimizer. To evaluate the effectiveness of our method, we enforce the ICL behavior by mapping the labels of the demonstrations to a token that carries no semantic meaning. This way, pre-trained transformers cannot leverage memorized knowledge to produce answers but have to learn the correct answer from the supplied demonstrations. Specifically, we map all labels to one of \(\{A,B,C,D,E\}\) depending on the number of classes. More details are in each section.

### Evaluation on a Custom Transformer

We use the MNIST dataset  to visualize how \(_{}\) can be applied to attribute model prediction to each demonstration. We design a task where the transformer needs to learn a mapping of the digit image to a label letter in context. Specifically, for each image of \(28 28\) pixels, we flatten the pixels and concatenate them with a mapped label to form a \(785\)-dimensional token vector. For simplicity of illustration, we only use images of digits in \(\{0,1\}\). For each ICL dataset, we assign to each distinct digit a letter in \(\{A,B,C,D,E\}\) randomly. We build a recurrent transformer based on the design in  with \(10\) recurrent layers each consisting of \(15\) attention heads. We pre-train the transformer with random label mappings from randomly selected digits so that the transformer cannot memorize the mapping but has to infer from the demonstrations. We use the hidden state after the \(1\)st layer as \(m\) to compute \(_{}\). A qualitative visualization of \(_{}\)'s attribution and a quantitative plot showing how the test prediction varies by removing demonstrations with the highest (lowest) \(_{}\) are in Fig. 2. Left shows that removing tokens (represented by the image pixels and the corresponding label) with the largest \(_{}\) makes the model make wrong predictions, whereas removing tokens with the lowest \(_{}\) can retain the correct model predictions. Right shows that removing tokens with the lowest \(_{}\) results in a slower decrease in prediction accuracy than removing the highest \(_{}\), demonstrating that tokens with the highest \(_{}\)'s are more helpful and _vice versa_.

### Evaluation on Large Language Models

With the insight obtained in Sec. 5.1, we now apply DETAIL to full-fledged LLMs. We start with demonstration perturbation and noisy demonstration detection to demonstrate that DETAIL can be used to interpret the quality of a demonstration. Then, leveraging these results, we further show how we can apply the DETAIL scores to tasks more closely related to real-world scenarios.

A distinction between LLMs and the custom transformer used above is that demonstrations in LLMs are usually _a sequence of tokens_, whereas demonstrations in the custom transformer are single tokens representing the actual numerical values of the problems (see Fig. 2). This distinction makes it difficult to find an appropriate internal representation for each demonstration (i.e., \(m\)). To overcome this challenge, we draw inspiration from prior works [64; 69] which suggest that information flows from input words to output labels in ICL. As an implementation detail, we take the embedding of the _last token before the label token_ (hereafter referred to as the target position) in the _middle layer_ where most of the information has flown to the target token positions. We include ablation studies on using different layers' embeddings in App. D.6 and using different target positions in App. D.7.

Figure 1: Illustration of computing DETAIL score for transformer-based ICL. Note that we use the same notation \(m_{p[]}\) before and after the random projection since the projection is optional.

Setup.We consider (for white-box models) mainly a Vicuna-7b v1.3  and also a Llama-2-13b  on some tasks using greedy decoding to show that DETAIL works on models with different training data and of varying sizes. While our theoretical backing stands for transformer-based models, we experiment DETAIL on Mamba-2.8b , a state-space model architecture that has received increased attention recently in App. D.8. We primarily evaluate our method on AG News (\(4\) classes) , SST-2 (\(2\) classes) , Rotten Tomatoes (\(2\) classes) , and Subj (\(2\) classes)  datasets which all admit classification tasks. Due to space limits, some results are deferred to App. D.

Demonstration perturbation.We show that DETAIL can explain LLM predictions by showing how perturbation (i.e., corrupting the labels of some demonstrations to an incorrect class or removing some demonstrations) with the high/low \(_{}\) affects the model's predictive power. We randomly pick \(20\) ICL datasets each comprising \(20\) demonstrations and \(1\) query from AG News and find the average and standard error of the accuracy of predicting the query after perturbation using Vicuna-7b and Llama-2-13b, shown in Fig. 3 (results for other datasets deferred to App. D.1). It can be observed that perturbing demonstrations with low \(_{}\) results in a slower drop (or even improvement) in accuracy and _vice versa_, similar to the trend observed in Fig. 2, showcasing the applicability of DETAIL to LLMs. We additionally include the results using Falcon-7b  and Llama-2-7b  in App. D.1 where we perturb \(10\) demonstrations and observe a similar accuracy gap.

Noisy demonstration detection.We utilize the DETAIL score to detect noisy demonstrations with corrupted labels. The experiment setup largely follows [34, Section 5.4]. We randomly draw \(100\) ICL datasets each consisting of \(20\) demonstrations and \(1\) query. For each ICL dataset, we randomly corrupt the labels of \(4\) demonstrations (i.e., flipping the label to an incorrect class). The demonstrations are

Figure 3: (\(1\)st and \(2\)nd) Corrupting labels of demonstrations and (\(3\)rd and \(4\)th) removing demonstrations with high/low DETAIL scores (\(_{}\)) on AG News. Perturbing demonstrations randomly result in an accuracy in the middle as expected. All experiments are repeated \(10\) trials. \(=1.0\). Lines and shades represent the mean and standard error respectively.

Figure 2: (Left) Visualization of learning label mapping of MNIST digits in context. The left \(9\) images in each row are demonstrations while the _right-most_ one is a query image. Below each image shows its mapped label (“A” to “E”). Above each ICL image is its \(_{}\) w.r.t. the query image with high values highlighted in green and low values highlighted in red. Above the query image is the prediction (pred) made by the pre-trained transformer which is in green if consistent with the ground truth (GT) and red otherwise. Top row shows that using all \(9\) demonstrations allows the transformer to learn the mapping in context as GT=pred=“E”. Middle shows removing \(5\) demonstrations with the highest \(_{}\) results in most digit \(0\)’s removed, leading to a wrong prediction. Bottom shows removing \(5\) demonstrations with the lowest \(_{}\) results in \(3\) digit \(0\)’s remaining for the transformer to learn in context, leading to correct prediction. (Right) Average accuracy on \(1013\) ICL datasets repeated over \(10\) trials; \(=0.01\); Lines and shades represent mean and standard error over \(10\) independent trials.

then ranked in descending order of their \(_{}\). The fraction of noisy demonstrations detected is plotted in the first \(3\) figures of Fig. 4 (result for other datasets deferred to App. D.2). We compare our method with the leave-one-out (LOO) score  where the difference in cross-entropy loss of the model output is used as the utility. It can be observed that LOO performs close to random selection, whereas our method has a much higher identification rate w.r.t. the number of demonstrations checked. We also note that our method _not only_ outperforms LOO in effectiveness but is also around \(10\) faster than LOO since LOO requires multiple LLM inferences for each demonstration in the ICL dataset.

Dimension reduction via random projection.We analyze the impact of random projection on the effectiveness of DETAIL. Intuitively, dimension reduction trades off the effectiveness of DETAIL with computational efficiency, specifically the \((d^{3})\) cost for inverting \(K_{}\). To understand the trade-off, we follow the same setup as the noisy demonstration detection experiment and compare the change in AUC ROC of detection and system wall time as the dimension \(d^{}\) of the projection matrix \(P\) decreases. The result for Subj is in the last figure of Fig. 4 (results for other datasets deferred to App. D.3), showing that wall time stays minimal (\( 0.3\)s) for project dimensions up to \(1000\) generally before it exponentially increases. Effectiveness measured in terms of AUC reaches optimal with \(d^{} 1000\). The results suggest a "sweet spot" - \(d^{} 1000\) - for a low running time and high performance.

### Applications of Detail

With the two experiments above verifying the effectiveness of DETAIL (\(_{}\) and \(_{}\)) and the experiment on random projection which ensures computational efficiency, we demonstrate next how DETAIL, with \(_{}\) for noisy demonstration detection and \(_{}\) for demonstration perturbation, can be applied to real-world scenarios, achieving superior performance and speed.

ICL order optimization.One distinctive trait of ICL compared to conventional ML is that the order of demonstrations affects the model's predictive performance [42; 44]. We show that \(_{}\) helps reorder the demonstrations with improved model predictive performance. We first show, using a Vicuna-7b model, that moving demonstrations with lower quality to the front (or back) of the prompt tends to improve the test accuracy of the model. To see this, we corrupt the label of a random demonstration and allocate this corrupted demonstration to different positions of the ICL dataset (each with \(20\) demonstrations with permutations drawn from a Sobol sequence  to capture the average performance better). A general trend with decreasing-then-increasing accuracy can be observed in Fig. 5: Allocating noisy demonstrations to the front (or the back) results in much higher test accuracy. Leveraging this insight, we utilize \(_{}\) to reorder a random permutation of ICL demonstrations and show the reordered prompt improves the test accuracy. For each randomly ordered prompt, \(_{}\) for each demonstration is computed (note that this computation only requires 1 pass of the LLM). Then, based on the trend observed in Fig. 5, for Subj and Rotten Tomatoes datasets, the demonstrations are reordered by placing the two demonstrations with the largest \(_{}\) in front followed by the rest in ascending order. For SST-2, the demonstrations are reordered in descending order of \(_{}\). To simulate situations where demonstrations have varying quality, we additionally consider randomly perturbing \(3\) demonstrations (and \(6\) demonstrations in App. D.4) in each ICL dataset. We note a clear improvement in test accuracy of \(1.4\% 3.0\%\) via reordering demonstrations _only_, as shown in Table 1. The improvement demonstrates that \(_{}\) can identify demonstrations that are low-quality or inconsistent with other demonstrations in the ICL dataset.

Figure 4: (1st and 2nd) Fraction of noisy labels identified vs. number of demonstrations ranked by DETAIL (with \(d^{}=1000\)) and LOO checked on Subj using Vicuna-7b and Llama-2-13b respectively. (3rd) Wall time comparison between DETAIL and LOO on all datasets. (4th) wall time in seconds (left \(y\)-axis) and AUCROC (right \(y\)-axis) vs. projection dimension on Subj using Vicuna-7b. All experiments are repeated \(10\) trials. \(=10^{-9}\). Lines and shades represent the mean and std. error.

ICL demonstration curation.In the demonstration perturbation experiment, we have verified that our \(_{}\) can correctly attribute helpful demonstrations w.r.t. a query. A direct application is demonstration curation where a subset of most helpful demonstrations are selected to prompt the LLM while maintaining accuracy _on a test dataset_.3 This application is useful, especially for saving the cost of querying LLMs.4 For proprietary LLMs, reducing the prompt length can also significantly save inference time which scales quadratically in the prompt length. As a setup, we fix a randomly selected set of \(120\) demonstrations as the test set. In each trial, we randomly pick \(20\) demonstrations to form an ICL dataset and another \(20\) demonstrations as the validation set. The individual \(_{}\)'s on each validation demonstration are summed as the final score. Then, demonstrations with the lowest scores are removed (in position). We randomly corrupt \(5\) demonstrations in each ICL dataset to simulate prompts with varying qualities. The results are shown in Fig. 6 (results on other datasets deferred to App. D.5). A clear gap between the test accuracy after removing demonstrations with high/low \(_{}\) can be observed for both Vicuna-7b and Llama-2-13 on both binary (Rotten Tomatoes) and \(4\)-way classification (AG News). Removing demonstrations with lower \(_{}\)'s maintains (or even improves) the test accuracy. Moreover, the gap for the 13B model is wider and more certain (shorter error bars), signaling better curation. We attribute this phenomenon to the better capability of the larger model to formulate an "internal optimizer", which enhances the attributive power of \(_{}\).

### Comparison with Other Attribution Methods

We compare our DETAIL score with other metrics proposed for demonstration attribution/selection or can be directly extended to attributing demonstrations. We analyze both the attributability via a demonstration curation experiment and the computational cost via recording the system wall time for performing the attribution. We select representative conventional approaches from different paradigms, including integrated gradients (IG)  and LIME  (Attention  in App. D.5). As these methods are originally designed for token-level attribution, we use the sum of the scores of all tokens in each demonstration as the attribution score. We also compare recent efforts on demonstration selection [15; 48; 53]. In natural language processing, a popular choice for attribution has been using BERT-based scores [23; 76] to match the similarity between texts. These methods enjoy the benefit

    & Subj & SST-2 & Rotten Tomatoes \\ 
**No corrupted demo** & & & \\ Baseline (random) & 0.722 (7.22e-03) & 0.665 (5.24e-03) & 0.660 (1.08e-02) \\ Reorder (DETAIL) & 0.743 (7.10e-03) & 0.679 (5.42e-03) & 0.684 (1.15e-02) \\ Difference \(\) & **0.0206 (7.40e-03)** & **0.0139 (6.08e-03)** & **0.0244 (1.11e-02)** \\ 
**Corrupt \(3\) demos** & & & \\ Baseline (random) & 0.655 (8.54e-03) & 0.607 (7.61e-03) & 0.553 (1.10e-02) \\ Reorder (DETAIL) & 0.685 (9.39e-03) & 0.630 (7.04e-03) & 0.582 (1.42e-02) \\ Difference \(\) & **0.0300 (9.10e-03)** & **0.0230 (7.22e-03)** & **0.0291 (1.06e-02)** \\   

Table 1: Predictive accuracy of demonstrations permuted randomly and based on \(_{}\), respectively. The mean and standard error (in bracket) with \(80\) repeated trials is shown.

Figure 5: Test accuracy (mean and std. error) vs. position of the demonstration with the corrupted label over \(50\) trials.

Figure 6: Test accuracy vs. number of demonstrations removed using \(_{}\) on (\(1\)st and \(2\)nd) AG news and (\(3\)rd and \(4\)th) Rotten Tomatoes using Vicuna-7b and Llama-2-13b. All experiments are repeated with \(80\) trials. Lines and bars represent the mean and standard error.

[MISSING_PAGE_FAIL:9]

Conclusion, Limitation, and Future Work

We tackle the problem of attributing demonstrations in ICL for transformers. Based on the well-known influence function commonly used for attributing conventional ML, we propose DETAIL, an innovative adoption of the influence function to ICL through the lens of treating the transformer as implementing an internal kernelized ridge regression. Combined with a dimension reduction technique using random projection, DETAIL can be computed in real-time with an impressive performance on various real-world related tasks such as demonstration order optimization and demonstration curation. One limitation of our approach is the need to access the internal state of the transformer, which we mitigate by additionally showing that DETAIL scores are transferable to black-box models. As a first step toward attributing demonstrations w.r.t. a transformer's internal optimizer, we hope this work serves as a building block for future research to develop attribution techniques for more generalized prompting settings such as chain-of-thought .