# DaTaSeg: Taming a Universal Multi-Dataset

Multi-Task Segmentation Model

 Xiuye Gu  Yin Cui1 &Jonathan Huang1 &Abdullah Rashwan1 &Xuan Yang1 Xingyi Zhou1 &Golnaz Ghiasi &Weicheng Kuo &Huizhong Chen &Liang-Chieh Chen2 &David Ross

Google Research

Equal contribution. Correspondence to: Xiuye Gu \(\)xiuvegu@google.com\(\).Work done while at Google. Now at NVIDIA.Work done while at Google. Now at ByteDance.

###### Abstract

Observing the close relationship among panoptic, semantic and instance segmentation tasks, we propose to train a universal multi-**d**ataset multi-**t**ask **seg**mentation model: DaTaSeg. We use a shared representation (mask proposals with class predictions) for all tasks. To tackle task discrepancy, we adopt different merge operations and post-processing for different tasks. We also leverage weak-supervision, allowing our segmentation model to benefit from cheaper bounding box annotations. To share knowledge across datasets, we use text embeddings from the same semantic embedding space as classifiers and share all network parameters among datasets. We train DaTaSeg on ADE semantic, COCO panoptic, and Objects365 detection datasets. DaTaSeg improves performance on _all_ datasets, especially small-scale datasets, achieving 54.0 mIoU on ADE semantic and 53.5 PQ on COCO panoptic. DaTaSeg also enables weakly-supervised knowledge transfer on ADE panoptic and Objects365 instance segmentation. Experiments show DaTaSeg scales with the number of training datasets and enables open-vocabulary segmentation through direct transfer. In addition, we annotate an Objects365 instance segmentation set of 1,000 images and release it as a public evaluation benchmark on https://laoreja.github.io/dataseteg.

## 1 Introduction

Image segmentation is a core computer vision task with wide applications in photo editing, medical imaging, autonomous driving, and beyond. To suit different needs, various forms of segmentation tasks have arisen, the most popular ones being panoptic , semantic , and instance  segmentation. Prior works generally use specific model architectures tailored to each individual task . However, these segmentation tasks are closely related, as they can all be regarded as grouping pixels and assigning a semantic label to each group. In this paper, we address the following question: _Can we leverage a diverse collection of segmentation datasets to co-train a single model for all segmentation tasks?_ A successful solution to this problem would leverage knowledge sharing among datasets, boosting model performance across the board, especially on smaller datasets.

Existing works on unified segmentation models either focus on a single architecture to handle multiple tasks , but with separate weights for different datasets; or a single set of weights formultiple datasets [27; 33; 77], but on the same task. By contrast, in this work, we aim to train a _single_ model on _multiple_ datasets for _multiple_ tasks. Our key idea of unifying these segmentation tasks is to use a universal intermediate mask representation: a set of mask proposals (_i.e._, grouped pixels) with class labels [63; 10]. Different segmentation tasks can be realized by applying different merge and post-processing operations on this unified representation. This allows us to train our network on the same output space for different tasks. Furthermore, using this representation, we can exploit weak bounding box supervision for segmentation, which is far cheaper to collect than mask annotations.

To encourage knowledge sharing and transfer among the multiple segmentation sources, our network architecture shares the same set of model weights across all datasets and tasks. In addition, we utilize text embeddings as the category classifier, which maps class labels from different datasets into a _shared_ semantic embedding space. This design further enhances knowledge sharing among categories with similar meanings in different datasets, _e.g._, 'windowpane' and 'window-other'. Our approach can be contrasted with an alternative of using dataset-specific model components -- we show in our experiments that our simpler, unified approach leads to improved performance and enables open-vocabulary segmentation via simply switching the text embedding classifier.

Putting these techniques together, we propose **DaTaSeg** as shown in Fig. 1, a universal segmentation model, together with a cotraining recipe for the multi-task and multi-dataset setting. We train DaTaSeg on the ADE20k semantic , COCO panoptic , and Objects365 detection  datasets. We show that DaTaSeg improves performance on all datasets comparing with training separately, and significantly benefits relatively small-scale datasets (ADE20k semantic), outperforming a model trained only on ADE20k semantic by **+6.1** and **+5.1** mIoU with ResNet50  and ViTDet-B  backbones. The multi-dataset multi-task setting also allows us to _seamlessly_ perform weakly-supervised segmentation by transferring knowledge from other fully supervised source datasets, which we demonstrate on ADE20k panoptic and Objects365 instance segmentation. DaTaSeg also directly transfers to other datasets not seen during training. It outperforms open-vocabulary panoptic segmentation methods on the Cityscapes dataset  and performs comparably with open-vocabulary semantic segmentation works on the Pascal Context dataset .

To summarize our contributions, we present DaTaSeg, a single universal segmentation model on multiple segmentation tasks and datasets. DaTaSeg leverages knowledge from multiple sources to boost performance on all datasets. It seamlessly enables weakly-supervised segmentation, directly transfers to other datasets and is capable of open-vocabulary segmentation. As an additional contribution, we have annotated a subset of the Objects365 validation set with groundtruth instance masks and release it as an evaluation benchmark for instance segmentation.

## 2 Related Work

**Muti-dataset training:** Training on multiple datasets has become popular for developing robust computer vision models [64; 78; 33; 47; 61; 67]. For object detection, Wang _et al_.  train an object detector on 11 datasets in different domains and show improved robustness. UniDet  trains a unified detector on 4 large-scale detection datasets with an automatic label merging algorithm. DetectionHub  employs text embeddings to accommodate different vocabularies from multiple datasets, and features dataset-specific designs. For segmentation, MSeg  manually merges the vocabularies of 7 semantic segmentation datasets, and trains a unified model on all of them; however, the manual efforts are expensive and hard to scale. LMSeg  dynamically aligns segment queries with category embeddings. UniSeg  explores the label relation and conflicts between multiple

Figure 1: Considering the similarities across various segmentation tasks, and recognizing the potential for enhancing segmentation performance by harnessing data from multiple sources, **we propose to train a universal segmentation model**, **DaTaSeg**, on multiple datasets to perform multiple segmentation tasks.

datasets in a learned way. Most of these works merge datasets using a unified vocabulary on a single task, while our work focuses on a more challenging problem: we merge datasets with different vocabularies _and_ different tasks.

**Unified segmentation model:** Panoptic segmentation  unifies semantic and instance segmentation. Prior works [28; 66; 69; 7; 52] exploit separate modules for semantic segmentation [45; 5] and instance segmentation [19; 21], followed by another fusion module. Recently, the mask transformer framework proposed by MaX-DeepLab  directly predicts masks with class predictions, allowing end-to-end panoptic segmentation. MaskFormer  and K-Net  adopt a single transformer-based model for different segmentation tasks. Mask2Former  improves upon MaskFormer by proposing masked attention, while kMaX-DeepLab  develops k-means cross-attention. OneFormer  extends Mask2Former with a multi-task train-once design. All these works still train separate weights on _different_ datasets. By contrast, we aim at a single unified model that can perform well across multiple segmentation tasks and datasets.

**Weakly-supervised segmentation:** Facing the issue of expensive segmentation mask annotations, many works have proposed to learn segmentation masks from cheaper forms of supervision [50; 12; 42]. In particular, box-supervised approaches are most related to our work, including BoxInst , Box2Mask , Cut-and-Paste  and MAL  for instance segmentation, Box2Seg  and BCM  for semantic segmentation, and DiscoBox  and SimpleDoesIt  for both semantic and instance segmentation. Despite the popularity of box-driven semantic and instance segmentation, fewer attempts have been made at weakly-supervised panoptic segmentation. Li _et al_.  employ a pretrained model to generate pseudo-ground-truth in advance, in addition to weak supervision from bounding boxes and image level labels. Shen _et al_.  add a handcrafted branch on top of semantic and instance segmentation models to generate panoptic proposals. Unlike these methods that require customized components, our approach can realize knowledge sharing among datasets with different forms of annotations in a more systematic and data-centric fashion, and seamlessly enable weakly-supervised segmentation.

## 3 Method

### A universal segmentation representation

Segmentation aims to group pixels of the same concept together. We propose to use an intermediate and universal representation, namely mask proposals, for all segmentation tasks. A mask proposal is a binary foreground mask with a class prediction. We use this representation for its versatility: one mask proposal can represent a single instance, any number of instances, a region, or even a part of an object. They can overlap and can be combined to form higher-level segmentation outputs -- an instance ("thing") or a region of the same semantic class ("stuff"). Thus they are well suited for the different flavors of segmentation considered in this paper. We note that a similar concept has been used in existing works [63; 10; 8; 71], for specific segmentation tasks on a single dataset. We show this representation is especially beneficial in our multi-dataset setting, as different datasets define "thing" and "stuff" categories differently. For example, 'table' is a thing category in ADE20k panoptic dataset, but there is a 'table-merged' stuff category in COCO panoptic. In our framework, both "thing" and "stuff" categories are represented using this same representation, and we treat them differently in the next step.

### Merging predictions for specific tasks

**Notations:** We introduce the notations we use in the equations. \(\) denotes predictions. Let \((_{j},_{j})\) denote the \(j\)-th mask proposal \(_{j}\) with its class prediction \(_{j}\), where \(_{j}^{H W}\) is the (pre-sigmoid) logits for the mask proposal and \(_{j,c_{k}}\) is the logit for class \(c_{k}\). We assume a fixed set of \(N\) mask proposals. \(_{d,thing}\) and \(_{d,stuff}\) are the set of thing and stuff categories in dataset \(d\).

**Merge operation (Merge):** Given the varying formats of groundtruth annotations for different segmentation tasks, we propose a Merge operation to merge the mask proposals predicting the same category \(c_{k}\). We adopt a simple element-wise max operation to merge the mask proposal logits. In particular, the merged mask proposal logits at the \((p,q)\) location for class \(c_{k}\) is computed as:

\[(c_{k})_{p,q}=_{j}[( _{j})=c_{k}]_{j,p,q}.\] (1)We choose element-wise max so that applying MERGE on raw mask logits is equivalent to applying MERGE on the soft mask predictions post-sigmoid. We also choose to merge at the level of raw logits for numerical stability. If no proposal predicts class \(c_{k}\), then the corresponding merged mask \((c_{k})_{p,q}\) is set to -100 to ensure it is close to 0 after sigmoid.

In addition to merging masks, we merge their corresponding class predictions by simply averaging the class prediction logits of the proposals predicting class \(c_{k}\):

\[(c_{k})=([(_{j })=c_{k}]_{j})}{_{j}([( _{j})=c_{k}])+}\,,\] (2)

where \(\) is a small number to prevent division by zero.

How we apply the above merge operations depends on the given task, as we now describe:

**Panoptic segmentation:** To make the predicted mask proposals have the same format as the groundtruth, we apply MERGE to all "stuff" categories \(c_{k}_{d,staff}\).

**Semantic segmentation:** In contrast with panoptic segmentation, the semantic segmentation groundtruth for each "thing" category is also a single mask (_i.e._, "thing" categories are treated equivalently as "stuff" categories). We thus apply MERGE to all predicted mask proposals to cover all thing and stuff categories \(_{d,stuff}_{d,thing}\).

**Instance segmentation:** MERGE is not needed in this task, since the desired outputs are separate masks for separate instances. There are no "stuff" categories in the vocabulary (\(_{d,stuff}=\)), and therefore no stuff proposals.

**Training:** During training, we apply the corresponding MERGE operations based on the segmentation task. We then use one-to-one Hungarian matching  to match the merged outputs with the groundtruth, and calculate the training losses. Detailed training losses are provided in Sec. 3.5.

**Inference:** At inference time, we apply the same MERGE operations to predicted mask proposals based on the given task. We post-process the mask proposals to obtain non-overlapping outputs for panoptic and semantic segmentation. For instance segmentation, we simply take the top mask proposals as final outputs, since overlaps are allowed. Please refer to supplementary for more details.

We illustrate how the MERGE operation works for different segmentation tasks in Fig. 2.

### Weakly-supervised instance segmentation

Bounding boxes are much cheaper to annotate than instance masks, and thus detection datasets can have a much larger scale than instance segmentation datasets. In order to train on larger detection datasets with bounding box annotations only, as well as to demonstrate the versatility of our framework to handle various tasks, we propose to perform weak-bounding-box-supervised instance segmentation.

Figure 2: **The universal representation, MERGE operations, and losses for different segmentation tasks. We use a universal representation for all tasks: a set of mask proposals with class predictions. Then we adopt distinct MERGE operations based on the segmentation task. For panoptic segmentation, we merge proposals predicting the same “stuff” category. For semantic segmentation, both “thing” and “stuff” categories undergo merging. In instance segmentation, we do not perform MERGE and there are no “stuff” categories. During training, we use Hungarian matching, and apply different losses based on the supervision types.**

We adopt a simple projection loss \(_{proj}\) from , which measures consistency of vertical and horizontal projections of the predicted masks \(_{j}\) against groundtruth boxes \(b_{j}\):

\[_{proj}(_{j},b_{j})=_{dice}(_{x}(S( _{j})),_{x}(b_{j}))+_{dice}(_{y}(S( _{j})),_{y}(b_{j})),\] (3)

where \(_{dice}\) is the dice loss ; \(b_{j}\) is the groundtruth bounding box matched to \(j\)-th mask proposal \(_{j}\); \(_{x/y}\) denotes the projection operation along the \(x\) or \(y\) axis, which can be implemented by a max operation along the axis; and \(S()\) denotes the sigmoid function.

By itself, a box consistency loss such as Eqn. 3 is insufficient as a supervision signal for segmentation (e.g., Eqn. 3 is equally satisfied by predicting the bounding box of an object instead of its mask). Thus other works have resorted to additional, often more complex, loss terms (such as the pairwise affinity loss from ). However, by training on multiple datasets and multiple segmentation tasks, these handcrafted losses are not necessary, as our model can transfer knowledge gained from other fully-supervised segmentation tasks on other datasets.

### Network architecture with knowledge sharing

**Network architecture:** We now describe the network architecture (similar to ) that predicts mask proposal and class prediction pairs \((_{j},_{j})\). The input image first goes through a _backbone_, and we use a _pixel decoder_ to 1) output multi-scale image features \(_{multi}\), and 2) output a high-resolution feature map \(\) that fuses information from the multi-scale image features. \(N\) mask proposal pairs are then generated from \(N\) learnable _segment queries_\(_{s}\): The segment queries are fed into a _transformer decoder_, which cross attends to the multi-scale image features \(_{multi}\) from the pixel decoder. The \(N\) decoder outputs are then passed to class embedding and mask embedding heads (both MLPs) to obtain \(N\) (mask embedding, class embedding) pairs. To extract final mask proposals and class predictions, we apply a dot product between the high-resolution feature map \(\) and the mask embeddings to obtain \(N\) mask proposal logits \(_{j}\). And to obtain class predictions, we compute dot products of the class embeddings against a per-category classifier \(w_{k}\) for each category \(c_{k}\). Fig. 3 shows an overview.

**Knowledge sharing:** We bake knowledge sharing into our model in two ways. First, _all network parameters are shared_ across all datasets, allowing knowledge sharing among different datasets and knowledge transfer among different tasks. Additionally, to share knowledge gathered from training samples of similar categories across datasets, _e.g._, 'windowpane' in ADE20k and 'window-other' in COCO, we propose to _map all category names into a shared semantic embedding space_. We feed the category names into a frozen pre-trained text encoder to set the per-category classifier \(w_{k}\). _i.e._, we let the fixed text embeddings serve as classifiers. This is similar to open-vocabulary

Figure 3: **Overview of our universal multi-dataset multi-task segmentation model (DaTaSeg). We feed \(N\) learnable segment queries \(_{s}\) into a transformer decoder, which cross-attends to multi-scale image features \(_{multi}\) from the pixel decoder. The outputs serve as the universal representation for all tasks: \(N\) mask proposals \(_{j}\) accompanied by \(N\) class predictions \(_{j}\). To promote knowledge sharing, we share all network parameters across different datasets and tasks. The only dataset-specific design is a classifier consisting of frozen text embeddings of categories specific to each dataset. Additionally, we employ a shared learnable background classifier.**

segmentation [16; 70; 72], but with a different purpose: our focus here is on knowledge sharing among different datasets. This automatic approach scales better than manually unifying label spaces in different datasets .

### Co-training strategy

We adopt a simple co-training strategy: at each iteration, we randomly sample _one_ dataset, then sample the entire batch for that iteration from the selected dataset. This can be contrasted with sampling from multiple datasets in one iteration. The main advantage for our strategy is that it is simple to implement, and allows more freedom to use different settings for different datasets, including applying distinct losses to various tasks. To account for various dataset scales, we use per-dataset sampling ratios, which randomly samples each dataset at a dataset-specific ratio during training, similar to .

For fully-supervised tasks, we employ direct _mask supervision_, which can be obtained from panoptic/semantic/instance segmentation groundtruth. To calculate the Hungarian matching cost and the training loss, we use a combination of focal binary cross-entropy loss \(_{focal}\), and dice loss \(_{dice}\), following . Regarding weak _bounding box supervision_, we adopt the projection loss \(_{proj}\) introduced in Sec. 3.3 for the matching cost and training loss. In both cases of supervision, we use the negative of the class prediction probability \(p\) as the classification matching cost , and use cross-entropy loss \(_{ce}\) for classification training. The total training loss is:

\[_{d}=_{ce,d}_{ce}+_{focal,d}_{ focal}+_{dice,d}_{dice}+_{proj,d}_{proj}.\] (4)

The Hungarian matching cost is defined similarly:

\[_{d}=-_{ce,d} p+_{focal,d}_{focal}+_{ dice,d}_{dice}+_{proj,d}_{proj}.\] (5)

Here, \(_{*,d}\) and \(_{*,d}\) are the weights for dataset \(d\).

## 4 Experiments

**Datasets and metrics:** We train and evaluate DaTaSeg on COCO panoptic  and ADE20k semantic  using mask supervision, as well as Objects365-v2  detection datasets using bounding box weak supervision. **COCO panoptic** is the most popular panoptic segmentation benchmark with 118,287 training images and 5,000 validation images. COCO has 80 thing categories and 53 stuff categories. **ADE20k semantic** is one of the most widely used semantic segmentation benchmarks with 150 categories, 20,210 training images, and 2,000 validation images.

For evaluation, besides the training datasets, we also evaluate on **ADE20k panoptic**, which uses the same validation images as ADE20k semantic but with panoptic annotations. The original 150 categories are divided into 100 thing categories and 50 stuff categories. Finally, we train on the **Objects365-v2 detection** (bounding boxes only) dataset, which has 365 categories and 1,662,292 training images. To evaluate the weakly-supervised instance segmentation results, we manually label a subset of 1,000 images from the Objects365 validation set. Our annotation pipeline involves 20 well-trained human annotators, following the protocol in , without using any automatic assistant tools. Please refer to Appendix B for more details. We release this **Objects365 instance segmentation evaluation set** as a public benchmark.

To compare DaTaSeg with the state-of-the-art methods, we in addition evaluate on popular open-vocabulary segmentation benchmarks. Following OpenSeg , we evaluate on **PASCAL Context** datasets  with 5k val images. We use its two versions: 59 classes (**PC-59**) and 459 classes (**PC-459**). We also evaluate on the 500-image val set of **Cityscapes** panoptic dataset , which focuses on urban street scenes with 11 stuff categories and 8 thing categories.

We report results in standard evaluation metrics: panoptic quality (**PQ**), mean intersection-over-union (**mIoU**), and mean average precision (**AP**) for panoptic, semantic, and instance segmentation, respectively.

**Implementation details:** We experiment with ResNet  and ViTDet  backbones. For ResNet, we use an ImageNet-1K  pretrained checkpoint, with a backbone learning rate multiplier of 0.1 ; the pixel decoder consists of a transformer encoder  and a modified FPN  following ,except that we use Layer Norm  and GeLU  activation for training stability. For ViTDet, we use the ImageNet-1K MAE pretrained checkpoint  with the layerwise lr decay ; the pixel decoder is the simple feature pyramid inside ViTDet, whose outputs are upsampled and added together to get the high resolution feature map \(\). The mask embedding head is a 3-layer MLP. The class embedding head contains a linear layer followed by ReLU. We use CLIP-L/14  as the pretrained text encoder. We use 100 segment queries \(_{s}\), unless otherwise stated. See Appendix H for more details.

### DaTaSeg improves over dataset-specific models

As an apples-to-apples comparison, we separately train a model on each dataset. We ensure that the models see the same number of images in cotraining and separately-training on each dataset. Table 1 (left) shows the results. DaTaSeg outperforms separately trained models on all datasets. Since we use exactly the same settings, we attribute the performance boost to our multi-dataset multi-task training recipe, which harnesses knowledge from multiple sources. Especially, DaTaSeg leads to a significant performance boost on ADE20k semantic: **+6.1 mIoU** and **+5.1 mIoU** with ResNet50 and ViTDet-B backbones, respectively. This proves our argument that cotraining on more data helps overcome the data limitation issue in smaller-scale segmentation datasets.

### DaTaSeg enables weakly-supervised transfer

In Table 1 (right), we evaluate DaTaSeg on ADE20k panoptic and Objects365 instance segmentation tasks. Note we only have weak supervision (ADE20k semantic and Objects365 bbox) during training. Again, we compare to models separately trained on one dataset as our baselines. Specifically, we use the ADE semantic for ADE panoptic evaluation, and an Objects365 model trained only using weak box supervision for Objects365 instance evaluation, respectively.

By cotraining DaTaSeg in a multi-dataset multi-task fashion, we improve the ADE20k panoptic performance by **+2.9 PQ** and **+5.4 PQ** with ResNet50 and ViTDet-B backbones. Our design allows the panoptic segmentation knowledge to transfer from COCO to ADE20k. For reference, the fully-supervised performance is 37.7 PQ on ResNet50 and 42.3 PQ on ViTDet-B.

    & &  &  \\   &  & ADE & COCO & ADE &  ADE \\ semantic \(\) panoptic \\ PQ \\  } \\  & & mIoU & PQ & & PQ \\   & Separate & 42.0 & 48.2 & 26.9 & 12.3 \\  & DaTaSeg & 48.1 (+6.1) & 49.0 (+0.8) & 29.8 (+2.9) & 14.3 (+2.0) \\  ViTDet-B &  Separate \\ DaTaSeg \\  } & 46.3 & 51.9 & 27.5 & 14.7 \\  & DaTaSeg & 51.4 (+5.1) & 52.8 (+0.9) & 32.9 (+5.4) & 16.1 (+1.4) \\  ViTDet-L & DaTaSeg & 54.0 & 53.5 & 33.4 & 16.4 \\   

Table 1: **Comparing DaTaSeg with separate dataset-specific models. We show results on both the training tasks (fully-supervised) and new tasks (weakly-supervised transfer). Our single DaTaSeg outperforms separately trained models on _all_ datasets. They are trained under the same settings, so the performance gains come from knowledge in other datasets through our multi-dataset multi-task cotraining recipe. We observe: 1) DaTaSeg _significantly_ improves tasks with limited data (ADE20k semantic); 2) DaTaSeg enables weakly-supervised knowledge transfer (ADE20k panoptic and O365 instance); 3) DaTaSeg scales well with backbones.**

    &  &  \\  ADE & COCO & O365 & ADE & COCO & ADE & O365 \\ semantic & panoptic & bbox & semantic & panoptic & semantic \(\) panoptic & box \(\) instance \\  & & & mIoU & PQ & PQ & max \\  ✓ & & & 42.0 & 8.4 & 26.7 & 1.0 \\  & ✓ & & 15.3 & 48.2 & 11.6 & 5.8 \\  & & ✓ & 11.0 & 12.4 & 5.8 & 12.3 \\  ✓ & & ✓ & 46.3 (+4.3) & 14.0 & 26.4 (-0.3) & 15.0 (+2.7) \\  & ✓ & ✓ & 18.3 & 48.9 (+0.7) & 12.3 & 15.2 (+2.9) \\ ✓ & ✓ & & 47.3 (+5.3) & 49.0 (+0.8) & 30.5 (+3.8) & 5.6 \\  ✓ & ✓ & ✓ & 48.1 (+6.1) & 49.0 (+0.8) & 29.8 (+2.9) & 14.3 (+2.0) \\   

Table 2: **Importance of training on multiple datasets. We train DaTaSeg on all combinations the three datasets. Experiments are done on ResNet50 under same settings. DaTaSeg scales with the number of training datasets.**On Objects365, we only train with a weak box consistency loss \(_{proj}\). The multi-dataset multi-task training recipe improves the mask AP by **+2.0 AP** and **+1.4 AP** for the two backbones. The improvement is most likely from the instance segmentation knowledge in COCO panoptic.

### DaTaSeg scales with size of backbones

Our DaTaSeg design is orthogonal to detailed network architectures, as long as it is able to output the universal representation (mask proposals with class predictions). This is supported by the consistent performance gains among ResNet50, ViTDet-B, and ViTDet-L backbones as shown in Table 1. It also scales well as we increase the sizes of the backbones. With ViTDet-L, DaTaSeg reaches **54.0 mIoU** on ADE20k semantic and **53.5 PQ** on COCO panoptic in a single model, without bells and whistles.

### DaTaSeg scales with number of datasets

We study how DaTaSeg scales with the number of training datasets. We train DaTaSeg on all combinations of one, two, and three datasets among the ADE20k semantic, COCO panoptic, and Objects365 detection datasets, in order to conduct a comprehensive study, with a ResNet50 backbone. Table 2 presents the results. Looking at each column, we see that performance on each dataset generally improves as the number of training datasets increases, especially for ADE semantic.

Since DaTaSeg shares all parameters among all datasets and tasks, we can evaluate the cross-dataset transfer performance. We notice the model transfers to datasets that are not trained. _e.g._, A model trained on COCO panoptic and Objects365 detection achieves **18.3 mIoU** on ADE semantic, which is comparable to open-vocabulary segmentation performance (LSeg+ [16; 36] achieves 18.0 mIoU on a ResNet101 backbone).

### DaTaSeg enables open-vocabulary segmentation

A further advantage of our fully-shared architecture is that we have the ability to directly transfer to other segmentation datasets. We simply switch the text embedding classifier with the vocabularies

   \# queries & 50 & 100 & 150 \\  ADE semantic & mIoU & 47.4 & 48.1 & 48.7 \\ COCO panoptic & PQ & 48.7 & 49.0 & 48.9 \\  ADE panoptic\({}^{}\) & PQ & 30.7 & 29.8 & 29.1 \\ O365 instance\({}^{}\) & AP & 13.4 & 14.3 & 15.8 \\   

Table 4: **In general, the performance of DaTaSeg increases as we increase the number of segment queries.** Besides, using only 50 queries already achieves good performance. Experiments are conducted using a ResNet50 backbone. \({}^{}\): weakly-supervised tasks.

   Method & Backbone & Training data &  PC-59 \\ mIoU \\  &  PC-459 \\ mIoU \\  &  COCO \\ mIoU \\  & 
 Cityscapes \\ **PQ** \\  \\  ODISE  & UNet+M2F & LAION+CLIP+COCO & 55.3 & 13.8 & 52.4 & **23.9** \\  MaskCLIP  & R-50 & COCO pan+CLIP & 45.9 & 10.0 & — & – \\  OVSeg  & R-101c & COCO stuff+cap & 53.3 & 11.0 & – & – \\  & Swin-B & COCO stuff+cap & 55.7 & 12.4 & – & – \\  OpenSeg  & R-101 & COCO pan+cap & 42.1 & 9.0 & 36.1 & – \\  & Eff-b7 & COCO+Loc. Narr. & 44.8 & 11.5 & 38.0 & – \\   & R-50 & COCO panoptic & 50.9 & 11.1 & 57.7 & 30.0 \\  & ViTDet-B & +ADE semantic & 51.1 & 11.6 & 62.7 & **28.0** \\   & ViTDet-L & + \(\)365 bbox & 51.4 & 11.1 & 62.9 & **29.8** \\   

Table 3: **Compare DaTaSeg to state-of-the-art open-vocabulary segmentation models.** We apply DaTaSeg directly to other semantic or panoptic segmentation datasets without finetuning. We compare to ODISE , MaskCLIP , OVSeg , and OpenSeg  on their corresponding benchmarks. We only conduct system-level comparison due to differences in training data and backbones. DaTaSeg performs comparably on all semantic segmentation benchmarks and outperforms the recent work ODISE on Cityscapes. PC: Pascal Context. COCO: COCO is included in the training set for all models, and is not completely “open-vocabulary”.

    &  \\  ADE semantic & mIoU & 48.1 & 48.1 (-0.0) \\ COCO panoptic & PQ & 49.0 & 46.0 (-3.0) \\  ADE panoptic\({}^{}\) & PQ & 29.8 & 26.9 (-2.9) \\ O365 instance\({}^{}\) & AP & 14.3 & 10.9 (-3.4) \\   

Table 5: **Adding dataset-specific (D-S) modules hurts performance on almost all datasets.** This shows the importance of sharing all parameters for better knowledge sharing, which particularly benefits the weakly-supervised tasks. Experiments are conducted on a ResNet50 backbone.

in a target dataset not used in training. Table 3 shows the results comparing DaTaSeg to several open-vocabulary works: ODISE  and MaskCLIP  are open-vocabulary panoptic segmentation methods and OVSeg  and OpenSeg  are open-vocabulary semantic segmentation approaches. Unlike these works, our method does not train on large-scale image-text data or use pretrained image-text models (except the pretrained text encoder), which puts our method at a disadvantage. Despite this disadvantage, DaTaSeg outperforms ODISE on the Cityscapes panoptic dataset. Note that there is a domain gap between our training sets and Cityscapes, which focuses on urban street scenes. DaTaSeg achieves comparable performance on PC-59 and PC-459. All methods in comparison train on COCO panoptic and ours has the best performance on COCO semantic. These results show that DaTaSeg enables open-vocabulary segmentation via our multi-dataset multi-task training approach.

### DaTaSeg scales with number of queries

We perform ablation study on the number of segment queries \(_{s}\) and show the results in Table 4. The performance improves as we increase the number of queries from 50 to 150, on almost all datasets. Besides, DaTaSeg achieves reasonably good performance with only 50 queries, which may benefit memory-limited application scenarios. We perform more ablation studies in Appendix D.

### Adding dataset-specific modules hurts performance

In multi-dataset learning, prior works have introduced dataset-specific modules , in order to address inconsistencies across datasets [33; 61]. However, such a design may weaken knowledge sharing across different datasets and tasks, especially for weakly-supervised tasks that may rely more on other datasets and tasks to compensate for the weak supervision. To study this issue thoroughly, we carefully design several dataset-specific modules, including dataset-specific queries, dataset-specific heads, and dataset-specific prompts. These modules are lightweight by design so as to still encourage knowledge sharing through other shared parameters.

In Table 5, we evaluate the effectiveness of these dataset-specific modules by adding them to DaTaSeg, and keeping all other settings the same. Comparing with DaTaSeg that shares all parameters, results reveal that using dataset-specific modules hurts performance on all datasets except ADE20k semantic. Moreover, we see that the performance drops more on weakly-supervised tasks (ADE panoptic and O365 instance). This verifies that removing dataset-specific modules yields better knowledge sharing across datasets and tasks, and thus greatly improves performance. See Appendix E for more details.

Figure 4: **Qualitative results of DaTaSeg on all datasets & tasks. For every pair of images, the left is DaTaSeg prediction and the right is groundtruth. DaTaSeg succeeds on hard cases (_e.g._, transparent wine glasses on the top left) as well as weakly-supervised datasets. For object instances in panoptic and instance segmentation, the colors are not matched to the groundtruths. Black denotes ignored regions.**

### Qualitative analysis

We show qualitative results (with ViTDet-L) in Fig. 4. DaTaSeg performs well in both fully and weakly supervised settings. We observe that localization quality is good in general, while classification is more error-prone. Nevertheless, DaTaSeg succeeds in challenging cases including transparent objects, occlusion, and crowd scenes.

## 5 Conclusion

The goal of our work is to train a single universal segmentation model on multiple datasets and multiple tasks (_i.e._, semantic, instance and panoptic segmentation). We present DaTaSeg, which leverages a universal segmentation representation, shared network parameters, and shared semantic embedding space for classification. DaTaSeg surpasses separately trained models, leading to significant gains on smaller datasets such as ADE20k semantic. It unlocks a new weakly-supervised transfer capability on datasets that were not explicitly trained for a particular task (_e.g._, ADE20k panoptic and Objects365 instance). DaTaSeg also directly transfer to more segmentation datasets and enables open-vocabulary segmentation. We believe that in the future, this multi-task multi-dataset setting will be the norm rather than the outlier as it currently is, and we see our paper as taking a step in this direction.