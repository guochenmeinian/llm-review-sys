# Inner Product-based Neural Network Similarity

Wei Chen\({}^{}\), Zichen Miao\({}^{}\), Qiang Qiu

Department of ECE

Purdue University

{chen2732, miaoz, qqiu}@purdue.edu

Equal contribution.

###### Abstract

Analyzing representational similarity among neural networks (NNs) is essential for interpreting or transferring deep models. In application scenarios where numerous NN models are learned, it becomes crucial to assess model similarities in computationally efficient ways. In this paper, we propose a new paradigm for reducing NN representational similarity to filter subspace distance. Specifically, when convolutional filters are decomposed as a linear combination of a set of filter subspace elements, denoted as _filter atoms_, and have those decomposed atom coefficients shared across networks, NN representational similarity can be significantly simplified as calculating the cosine distance among respective filter atoms, to achieve _millions of times_ computation reduction over popular probing-based methods. We provide both theoretical and empirical evidence that such simplified filter subspace-based similarity preserves a strong linear correlation with other popular probing-based metrics, while being significantly more efficient to obtain and robust to probing data. We further validate the effectiveness of the proposed method in various application scenarios where numerous models exist, such as federated and continual learning as well as analyzing training dynamics. We hope our findings can help further explorations of real-time large-scale representational similarity analysis in neural networks.

## 1 Introduction

Deep neural networks (NNs) have shown unprecedented performance in a large variety of tasks . In many scenarios, numerous models are learned and their relations can be beneficial to exploit. For example, as illustrated in Figure 1(a), to aggregate knowledge across space, federated learning (FL) trains models over a large number of clients while keeping data localized. To preserve knowledge across time while learning new ones, continual learning (CL) can be addressed by training a large group of models, one for each timestep. Finding the relations among models is the cornerstone to boosting performance in these scenarios, such as improving personalization for FL  or providing knowledge retrieval for CL . Considering the large number of NNs potentially allowed in those scenarios, e.g., to model growing spatial/temporal coverage, it becomes crucial to have a highly computationally efficient way to assess NN model similarity.

We are inspired by one recent state-of-the-art CL framework in , where each convolutional filter is represented as a linear combination of a set of filter subspace elements, denoted as _filter atoms_. It is easy to notice that each convolutional layer now becomes two convolutional layers, a filter atom layer followed by an atom coefficient layer with \(1 1\) filters. Then, motivated by the literature on task subspace modeling  that tasks can be modeled as a set of latent basis tasks and their linear combinations, a group of tasks are sequentially modeled using NNs by learning for each task a different set of filter atoms, while sharing common atom coefficients acrosstasks.  has in detail analyzed and validated this framework in the CL context. This learning framework with individually modeled filter subspaces but shared rules of linear combinations is generally applicable to many multi-model scenarios, especially CL and FL settings, where numerous NNs with the same architecture are learned. For example, FL learns a single global model to fit the data on all clients, and the majority of aggregating methods in FL require NNs to maintain the same network structure [28; 29; 34].

In the above setting, it is easy to observe that the representation variations across different NNs now become dominated by respective filter atoms. Thus,  adopts in experiments filter subspace distance to assess task relevancy, however, without formal justification. In this paper, we formally explore NN representational similarity using filter subspace distance, with detailed theoretical and empirical justifications. We first simplify the filter subspace distance to the cosine distance of two sets of filter atoms, to eliminate the computation of singular value decomposition in calculating principal angles. Then, we show both theoretically and empirically that the obtained filter subspace similarity preserves a strong linear correlation with other popular probing-based similarity measures such as CCA , which require external probing data as input stimuli. Our representational similarity is also immune to inappropriate choices of probing data, while probing-based metrics can be perturbed drastically.

Previous works [37; 43] measure representational similarity directly relying on deep representations revealed by input data. These approaches introduce heavy computation from both the forward pass of numerous probing data and the calculation of high-dimensional covariance matrices. As these similarity metrics are probing-dependent, their quality can potentially deteriorate when probing data are inappropriately chosen, scarce or unavailable. Such properties make the popular probing-based approaches less appropriate for our target scenarios where a large number of NN models are present.

The proposed filter subspace similarity shows extreme efficiency in both memory and computation. Since our similarity computation does not involve network forward pass, no GPU memory access is required, whereas other probing-based measures consume the same amount of GPU memory as regular inference. On the other hand, the proposed method involves only inner product calculations on filter atoms, which takes negligible time for similarity evaluation. The evaluation time of probing-based measures includes the time of both the forward pass of probing data and the calculation of high-dimensional covariance matrices. We report later the dramatically improved evaluation time of the proposed method against other popular probing-based methods, e.g., CKA . These unique

Figure 1: (a) The illustration of scenarios where numerous models exist, such as federated learning (FL), continual learning (CL), and model training process. The relations among models are usually critical, and the computational cost to assess the model relation can be a major bottleneck. (b) Comparison between our method and probing-based methods. (left) Feature space similarity metrics, e.g., CCA, rely on probing data, and calculate the correlation between large groups of features generated by the forward pass of probing data through NNs. (right) In comparison, our filter subspace-based method decomposes convolutional filters \(\) as _filter atoms_\(\) (filter subspace elements) and _atom coefficients_\(\), \(=\), and only calculates the filter subspace similarity between a small portion of parameters, _i.e._, filter atoms, which is independent from probing data and computation efficient. The proposed filter subspace-based method can achieve _millions of times_ computation reduction than popular probing-based methods.

properties make our method highly desirable for exploring NN similarity under scenarios with a large number of NN models.

We further validate our filter subspace similarity for knowledge transfer with various CL and FL tasks, as sample examples to exploit NN model relations. In both settings, we fix the atom coefficients, learn the filter atoms for each task, and finally conduct knowledge transfer among tasks by recalling the most similar models for the ensemble. Compared with probing-based similarity metrics, the proposed measure achieves competitive performance with _millions of times_ reduction in the computational cost.

We summarize our contributions as follows,

* We formally explore NN representational similarity measure using filter subspace distance.
* We show both theoretically and empirically that the proposed filter subspace-based measure preserves a strong linear correlation with other popular probing-based measures, while being significantly more robust and efficient in both memory and computation.
* We demonstrate the effectiveness of the proposed similarity measure using several simple examples, such as federated and continual learning as well as analyzing training dynamics.

## 2 Methodology

In this section, we first review probing-based representational similarities and show their limitations. Then, we provide a filter subspace formulation for NNs, and propose a NN similarity metric based on a simplified filter subspace distance. We further demonstrate that under certain assumptions, the proposed measure shows a strong linear relationship with popular probing-based measures, while exhibiting dramatic improvement in computational efficiency and data robustness. These unique characteristics of the proposed measure can potentially enable real-time large-scale NN similarity assessment, e.g., helping fast knowledge retrieval across a large number of NN models.

### Revisiting Representational Similarity in Feature Space

Intuitively, the NN representational similarity can be directly assessed via features generated from different neural networks. As shown in Figure 1(b), it usually includes three steps to evaluate probing-based representational similarity between two NNs \(_{u}\) and \(_{v}\): (1) Collect an appropriate and sufficient amount of external probing data \(_{p}^{n c^{} h^{} w^{ }}\) that can represent the whole data distribution. (2) Generate the feature \(_{u}\) and \(_{v}\) (\(_{u},_{v}^{n c h w}\)) by the forward pass of probing data through different neural networks, \(_{u}=_{u}(_{p},_{u})\) and \(_{v}=_{v}(_{p},_{v})\), where \(_{u},_{v}\) denote parameters of two NNs. (3) Choose a probing-based metric to assess the model similarity. Several popular probing-based methods can be adopted in step (3), and we will give a brief introduction below.

Cca. proposes to analyze the NN representational similarity by conducting canonical correlation analysis on \(_{u},_{v}\), which is a recursive process of finding projection directions for two matrices that their correlation is maximized. Specifically, let \(Q_{u},Q_{v}\) denote the orthonormal bases of \(_{u},_{v}\), the CCA can be denoted as,

\[_{CCA}(_{u},_{v})=_{l=1 }^{c}_{l}^{2}},\] (1)

where \(_{l}\) denotes the \(l\)-th eigenvalue of \(_{u,v}=Q_{u}^{}Q_{v}\).

Cka. proposes another way to assess the NN similarity based on Centered Kernel Alignment (CKA). Let \(K_{u}=_{u}_{u}^{}\), \(K_{v}=}_{v}_{v}^{}\) denote the Gram matrices of two feature space, the CKA is computed by,

\[_{CKA}(_{u},_{v})=(K_{u},K_ {v})}{(K_{u},K_{v})(K_{u},K_{v})}},\] (2)

where HSIC is the Hilbert-Schmidt Independence Criterion .

However, in addition to the forward pass, all the aforementioned approaches further introduce significant computational costs while performing evaluation in the representation space. Nevertheless,their qualities rely heavily on the mindful choice of probing data \(_{p}\), which undermines their robustness.

### Representational Similarity in Filter Subspace

Filter subspace.As in , the convolutional filter \(^{c^{} c k k}\) (\(c^{}\) and \(c\) are the number of input and output channels, \(k\) is the kernel size) can be decomposed over \(m\)_filter atoms_ (filter subspace elements) \([i]^{k k}(i=1,...,m)\), linearly combined by _atom coefficients_\(^{m c^{} c}\) as \(=\). Note that each convolutional layer now becomes two convolutional layers, a filter atom layer followed by an atom coefficient layer with \(1 1\) filters. The filter subspace is then expressed as \(=\{,...,[m]\}\). With this formulation, we consider a paradigm where filter subspaces are model-specific, and subspace linear combination rules, _i.e._, atom coefficients, are shared across different networks. The intuition and detailed validation of this learning paradigm can be found in , where state-of-the-art performance in the continual learning context is reported.

In this setting, we dive deep into the relationship between filter subspaces and representations. For simplicity, let \(c=c^{}=1\), and the argument extends. Given an input image \((b)\) (\(b,^{2}\)), define the local input norm \(||||_{F,N_{b}}(_{b^{} N_{b}}(b-b^{ })^{2})^{1/2}\) and the convolution \(,w_{N_{b}}_{b^{} N_{b}}(b-b^{})w(b^{})\), where \(N_{b}\) is a local Euclidean grid centered at \(b\). Then the decomposed convolution can be written as \((b)=_{i=1}^{m}_{i},_{i}_{N_{b}}\), where \([i]\) denotes the \(i\)-th atom, \(_{i}\) is the corresponded \(i\)-th coefficient.

**Proposition 2.1**.: _Suppose \(_{u}\) and \(_{v}\) are two different sets of filter atoms for a convolutional layer with the common atom coefficients \(\), we can upper bound the changes in the corresponding features \(_{u},_{v}\) with atom changes,_

\[||_{u}-_{v}||_{F}(||||_{F}) |}||_{u}-_{v}||_{F},=_{b}||||_{F,N_{b}}.\] (3)

The proof is provided in Appendix A.1. We further empirically validate this relationship in Section A.3.

Filter subspace similarityThe above theorem suggests the possibility to measure the representational similarity of two NNs by simply measuring the distance of their filter subspaces. As proposed in , the representational similarity of two NNs with different filter subspaces \(_{u},_{v}\) can be assessed by the similarity based on Grassmann distance between \(_{u},_{v}\) as,

\[_{Gras}(_{u},_{v})=d(_{u}, _{v})=_{i}_{i},\] (4)

where \(_{i}\) is the \(i\)-th principal angle between \(_{u}\) and \(_{v}\).

However, the above metric requires costly singular value decomposition. Note that filter atoms in different NNs are intrinsically aligned under shared atom coefficients, which allows us to approximate the filter subspace similarity using the cosine similarity of the corresponding filter atoms. To this end, as shown in Figure 1(b), we propose a significantly simplified representational similarity measure with filter atom similarity.

Figure 2: (a) Correlation between Grassmann similarity and filter subspace similarity; (b) Correlation between CCA and filter subspace similarity. (Table) Correlation between filter subspace similarity and other approaches.

**Definition 2.2**.: Suppose two convolution neural networks \(_{u},_{v}\) share atom coefficients layer-wise, and their model-specific filter atoms are \(_{u},_{v}\), then the filter subspace representational similarity is simplified as,

\[_{Atom}(_{u},_{v})=(_{u },_{v})=_{u}),vec(_{v})>}{||vec( _{u})||||vec(_{v})||}.\] (5)

The above definition is a layer-wise similarity, allowing us to compare the similarity of different networks per layer, and we simply average layer-wise similarities for the network-wise similarity.

_Remark 2.3_.: The filter subspace similarity measure becomes a proper metric after taking the arccosine, _i.e._, \((_{Atom}(_{u},_{v}))\) is a proper metric.

We further show that \(_{Atom}\) and \(_{Gras}\) are equivalent under certain assumption.

**Proposition 2.4**.: _Assume \(_{u},_{v}^{k^{2} m}\) are orthogonal matrices, then \(_{Gras}=_{Atom}\)._

The proof is provided in Appendix A.1. We empirically show in Figure 2(a) that the above simplified filter subspace similarity has still a strong linear correlation with the Grassmann subspace similarity even without imposing the above orthogonality over atoms.

Note that our filter subspace similarity measure only involves linear operations of vectorized atoms of around hundreds of dimensions, which requires negligible computation. Additionally, the proposed method depends solely on models themselves and eliminates the reliance on external probing data, equipping our similarity with robustness to inappropriate choice of probing data.

### Algorithm Complexity Analysis

Here, we provide a detailed comparison of computation complexity between the proposed filter subspace similarity and probing-based similarities. Consider one convolutional layer with filter \(^{c^{} c k k}\) (\(=\), \(^{m k k}\)) which transforms the input \(_{p}^{n c^{} h^{} w^{ }}\) to output \(^{n c h w}\). The complexity of our method is dominated by inner product of two tiny filter atoms, \((m k^{2})\), _e.g._, \(m=9,k=3\) in a typical setting.

In contrast, probing-based similarity measure first forward feeds \(n\) probing samples with a complexity of \((n h^{}w^{} k^{2} cc^{})\), then calculates covariance matrix with the complexity of \((n^{2} hw c)\). In total, the time complexity of CCA is \((n h^{}w^{} k^{2} cc^{}+n^{2}  hw c)\). Our method is at least \(w^{} k^{2} cc^{}+n^{2} hw  c}{m k^{2}}\) times more efficient than probing-based similarity measures. As \(h k\), \(cc^{} m\), the computational cost of our method is negligible. For example, with 10k probing datapoints, the CCA calculation requires \(1.14 10^{7}\) times more FLOPs than the proposed method.

### Relationship with Probing-based Similarities

The proposed filter subspace similarity not only shows extreme efficiency but also exhibits a strong linear relationship with other popular probing-based similarities. Here, we analyze the proposed filter subspace similarity \(_{Atom}\) with CCA, \(_{CCA}\). Suppose forward passes of decomposed convolutional layer for \(_{u}\) and \(_{v}\) are \(_{u}=_{p}_{u}\), \(_{v}=_{p}_{v}\), respectively. 2 To start with, we show that the \(_{CCA}\) is upper bounded by the proposed \(_{Atom}\).

**Theorem 2.5**.: _Let \(=(_{p}^{}^{ }_{p}),=_{min}(_{p}^{}^{} _{p})\). Assume \((_{u}^{}_{u}),(_{v} ^{}_{v})\). Then \(_{CCA}(_{u},_{v})\) is upper bounded by \(_{Atom}(_{u},_{v})\),_

\[}{^{}}_{ CCA}(_{u},_{v})_{Atom}(_{u},_{v}),\] (6)

where \(()\) denotes trace of a matrix, \(_{min}\) indicates the minimum eigenvalue, \((A)\) denotes the condition number of matrix \(A\). We provide the proof in Appendix A.1.

Since \(_{CCA}\) is probing-dependent, the calculated value varies depending on the choice of probing data, and the value range shows bounded by our filter subspace similarity, as in the theorem above.

With additional assumptions imposed, we can further show a near-linear relationship between CCA and our filter subspace similarity.

**Assumption 2.6**.: Suppose the diagonal elements of \(_{u}^{}_{u}\), \(_{u}^{}_{v}\) and \(_{v}^{}_{v}\) are larger than non-diagonal element, _i.e._, \((_{u}^{}_{u})_{ii}(_{u}^{} _{u})_{ij}\).

The Assumption 2.6 suggests different channels of feature \(\) have a low correlation. Reducing channel-wise dependencies has been studied in  and has been shown to benefit model stability. We provide the empirical verification of the assumption in Appendix A.3.

**Theorem 2.7**.: _If Assumption 2.6 holds, \(_{CCA}(_{u},_{v})\) is approximately linear to filter subspace similarity,_

\[}{_{1}_{2}_{3}}_{CCA}( _{u},_{v})=_{Atom}(_{u},_{v}),\] (7)

where \(_{1}\), \(_{2}\) and \(_{3}\) contain higher order of features, which can be found in detail with the proof in Appendix A.1. Specifically, we have \(_{2}=^{2}_{3}^{2}}(_{u},_{v})}}\), and since \(\) are small, with Taylor expansion, \(_{2} 1-^{2}_{2}^{2}} (_{u},_{v})}\). The term \((_{u},_{v})}\) causes non-linearity in the relation between CCA and filter subspace similarity.

As in Figure 2, we empirically observe the linear correlation between CCA and filter subspace similarity, which agrees with our theoretical findings. In addition, we find that the proposed similarity also shows a strong correlation with CKA.

## 3 Experiments

In this section, we first validate our theorems with several validation experiments and then demonstrate simple example applications of the proposed filter subspace similarity in efficiently analyzing training dynamics as well as in federated and continual learning scenarios.

### Validation Experiments

We conduct empirical validation to confirm the near-linear relationship between filter subspace similarity and probing-based similarity and explored the limitations of probing-based similarities.

Correlation of CCA and filter subspace similarity.The empirical verification of the correlation between CCA and filter subspace similarity is presented in Figure 2. In this experiment, 10 tasks are

Figure 3: (a) The ratio of the computational cost savings of our filter subspace similarity over probing-based similarities. (b) The performance of probing-based similarities can be compromised by poorly selected probing data. For models trained on CIFAR-100, they have high CCA and CKA similarities with probing from CIFAR-100 but low similarities with probing from other datasets. In contrast, our filter subspace similarity does not rely on probing data and shows a high similarity between the networks, aligning with our expectations.

generated from CIFAR-100 dataset , each consisting of 10 classes. We employ the ResNet18 model , training only the filter atoms while keeping the atom coefficients fixed on each task. The CCA and filter subspace similarity are calculated among 45 pairs of models. The correlation between CCA and filter subspace similarity is _0.9327_, as depicted in Figure 2(b). Furthermore, the correlation between Centered Kernel Alignment (CKA) and filter subspace similarity is also reported in Table 2. These findings clearly indicate that the proposed filter subspace similarity exhibits a strong linear relationship with well-established probing-based similarities, supporting the claims made in Theorem 2.5 and Theorem 2.7.

Limitations of probing-based similarities.The consistency of probing-based similarities can vary depending on the probing data. Ideally, we anticipate a high similarity value when comparing models trained on the same dataset. To investigate this, we conduct an experiment where models are trained on the CIFAR-100 dataset. Figure 3(b) displays the distribution of model similarity with different probing data, where the y-axis represents the similarity and the x-axis represents the corresponding density of models. And with CIFAR-100 probing data, the CCA similarity between models yields a value over 0.8. However, when the probing data are derived from the other datasets including CIFAR-10, SVHN , CelebA , and etc., the CCA similarity drops to 0.59. A similar inconsistency in values is observed with the CKA similarity using different probing data. In contrast, the average of our proposed filter subspace similarity between models is 0.91, which aligns well with our expectation of high similarity. This finding demonstrates the effectiveness of our approach in capturing the inherent similarities between models trained on the same dataset, irrespective of the specific choice of probing data.

### Learning Dynamics

The filter subspace similarity has various applications in analyzing NNs. It is capable of reflecting the data similarity and measuring the evolution of model similarity during the training time. We examine the training dynamics based on the heat map of filter subspace similarities. In this experiment, AlexNet  is trained on CIFAR-100  for 150 epochs and VGG11  is fine-tuned on ImageNet  for 20 epochs. For both models, we train and store atoms at each epoch. Figure 4 shows heat maps of similarities of the model among different training epochs.

Figure 4(a-c) are heat maps of the 1st, 3rd and 5th convolutional layers of Alexnet. We mark the epoch when the parameters of each layer reaches 0.99 similarity with the

Figure 4: Layer-wise similarity matrices that show relations of model parameters of different training time points. (a)(b)(c) are the 1st, 3rd and 5th convolutional layer of AlexNet trained on CIFAR-100. (d)(e)(f) are the 1st, 4th and 8th convolutional layer of VGG11 trained on ImageNet. We mark the epoch when the parameter reaches 0.99/ 0.999 similarity to its final state with white lines. For both models, we observe bottom-up learning dynamics where layers closer to the input solidify into their final states faster than very top layers, which is in accord with previous studies [37; 43].

epoch. The first layer reaches 0.99 similarity at epoch 36 which is earlier than final layers. In Figure 4(d-f), VGG11 shows a similar behavior. Several previous works have also indicated this bottom-up learning dynamics where layers closer to the input solidify into their final states faster than very top layers [37; 43]. Our filter subspace similarity provides a highly efficient way to examine the training dynamics while showing results in accord with previous studies. Moreover, we can apply our method to calculate the similarity of a model trained on different tasks, so we can track the process of the same model interacting with different datasets. The details are shown in Appendix A.2.

### Federated Learning

Federated learning (FL) aims at learning models collaboratively by leveraging the local computational power and data of all users with the concern of privacy . Personalized Federated Learning (PFL) emerges to address some challenges in FL, such as poor convergence on heterogeneous data and lack of solution personalization .

In this setting, our framework achieves personalization by enforcing FL models with the shared atom coefficients for all users and specific filter atoms for each user. As illustrated in Figure 6, the shared coefficients preserve common knowledge, while user-specific atoms hold personalized information about each user. Then, we can assess model relationships with our filter subspace similarity without any probing data, which meets the privacy requirement of the FL scenario.

The shared atom coefficients can be achieved in different ways. With our framework, the coefficient can be obtained from a model pre-trained on a public dataset or from a global model trained by other FL approaches. We can also get the coefficients by training the model locally and evolving the coefficients at each communication round.

Measuring user similarity.With the shared atom coefficients and user-specific filter atoms, we can simply get relations of users by calculating filter subspace similarity. To be specific, we expect that users with similar data have a higher similarity. In this experiment, we distribute data of CIFAR-100  and SVHN  to 120 clients, containing 20 SVHN clients and 100 CIFAR clients. Specifically, the SVHN dataset is randomly distributed in 20 SVHN clients. And the CIFAR-100 dataset is split into 20 subtasks with 5 classes in each subtask, and each subtask is shared by 5 CIFAR clients. The model is AlexNet  with 3 convolutional layers. The models share the same random initialization and filter atoms are trained independently without communication with other clients. All models are trained for \(T=100\) communication rounds on datasets. At each round, the client executes 1 epoch of SGD with momentum to train the local model, the learning rate is 0.01 and the momentum is 0.9. The experimental details are described in Appendix A.2.

Figure 5 shows the filter subspace similarity among the last 40 clients of the CIFAR-100 task and 20 clients of the SVHN task. Specifically, a distinct cluster of the 20 SVHN clients is observed, indicating a higher similarity among these clients and dissimilarity with the CIFAR clients. Additionally, every group of 5 CIFAR clients, who share the same task, also exhibit a high similarity among themselves.

Figure 5: Similarity matrices that show relations among 60 users in FL with our filter subspace similarity through the training process. The labels of x-axis represent the ID’s of CIFAR tasks. We can clearly see user clusters in all three figures. Specifically, the last 20 clients with SVHN data show higher similarities with themselves than the first 40 clients with CIFAR data, while every five of the first 40 clients sharing the same CIFAR task also show high similarities within themselves.

This clustering capability holds great potential for facilitating efficient cluster identification in federated learning scenarios . Refer to Appendix Figure 7 for the results of all 120 clients.

The computational cost of three different approaches is shown in Figure 3(a). Notably, calculating the filter subspace similarity is significantly faster (_million_ times), requiring \(0\) GPU memory usage than probing-based methods. Note that the advantages in computational efficiency of filter subspace similarity become more prominent as the number of models increases.

Improving personalized model with ensemble of similar users.Once we get the relationships of users, we can further improve the accuracy of the current model by the ensemble of similar models, which is effective to mitigate the data heterogeneity problem in FL. The experiment is described in detail in Appendix A.2. The final results are shown in Table 1. With ensemble, the accuracies of all FL methods can be improved. Note that the results of model ensemble selected by our filter subspace similarity are comparable with probing-based methods while consuming much fewer resources.

### Continual Learning

Continual learning is an open problem in machine learning in which data from multiple tasks arrive sequentially and the model is learned to adapt to new tasks while not forgetting the knowledge from the past . Some of the tasks in continual learning are related, so models trained with these tasks can be benefited from aggregating knowledge from each other. We adopt the setting in , and apply filter subspace similarity to find related models. Specifically, we _10-Split_ CIFAR-100 dataset, where the 100 classes is broken down into 10 tasks with 10 classes per task. We train AlexNet including atoms and atom coefficients on the first task, and train only the atoms on the following tasks. Then, we calculate the task similarity with filter subspace similarity, and report the model ensemble result with most similar members. The accuracy and the similarity computation costs are shown in Table 2. Our method provides higher results and has faster speed compared with probing-based methods.

    &  &  \\   & & MFLOPs & Time (s) & GPU Memory (MB) \\  AtomCL (base) & 78.11 \(\) 0.13 & - & - & - \\  +CCA  & 79.83 \(\) 0.04 & 35.2 & 0.26 & 1996 \\ +CKA  & 80.01 \(\) 0.06 & 111 & 0.3 & 1637 \\ **+Ours** & **80.19 \(\) 0.09** & **0.007** & **0.0008** & **0** \\   

Table 2: Continual Learning Results. The model ensemble using our filter subspace similarity is significantly faster and consumes much fewer resources than probing-based methods, while maintaining comparable classification accuracy.

   FL Results & Base & **+Ours** & +CCA  & +CKA  \\  FedAvg  & 83.78\(\) 0.08 & **85.82 \(\) 0.35** & 85.65 \(\) 0.21 & 85.29 \(\) 0.18 \\ Ditto  & 82.98 \(\) 0.13 & 85.49 \(\) 0.21 & **85.54 \(\) 0.19** & 85.37 \(\) 0.2 \\ FedRep  & 76.44 \(\) 0.06 & **78.35 \(\) 0.24** & 78.18 \(\) 0.18 & 77.73 \(\) 0.19 \\ FedProx  & 80.6 \(\) 0.1 & **82.95 \(\) 0.16** & 82.55 \(\) 0.19 & 82.86 \(\) 0.16 \\ FedPer  & 83.57 \(\) 0.07 & **85.21 \(\) 0.2** & 84.91 \(\) 0.18 & 84.9 \(\) 0.14 \\ Pretrain & 81.77 \(\) 0.08 & 85.41 \(\) 0.19 & 85.24 \(\) 0.13 & **86.33 \(\) 0.14** \\   _Similarity Computation Cost_ & & & & \\ GFLOPs & & **0.019** & 258,610 & 2,225 \\ Time (s) & & **0.016** & 1930.4 & 92.6 \\ GPU Memory (MB) & & **0** & 4915 & 3965 \\   

Table 1: Classification accuracy of model ensemble using different FL methods and model selection strategies: Models are selected with different similarity measures in each setting. The model ensemble using our filter subspace-based method is millions of times faster and consumes much fewer resources than probing-based methods while producing comparable performance.

Related Work

Model similarity.Representational similarity analysis (RSA)  demonstrates the method of understanding brain activities by computing similarities between brain responses in different regions. Measuring the similarity of models is beneficial for understanding neural network (NN) architectures and learning dynamics [9; 21; 37; 43]. Model similarity can be used to understand or incorporate various machine learning paradigms across different areas, including contrastive learning [13; 15], knowledge distillation , meta-learning , and transfer learning [5; 39; 44].

Multiple approaches are proposed to estimate the representational similarity of NNs. Some early works show that individual neurons can capture meaningful information [3; 4; 61; 65]. Later, gradient-based methods emerge to provide a visual explanation of deep neural networks . Current popular representational similarity methods rely on features of NN.  proposes SVCCA to measure similarity by calculating the covariance matrix of the features of each layer after channel alignments.  discusses the invariance properties of similarity indices and proposes CKA with consistent correspondences between layers. probing-based similarities are data-dependent and computationally expensive. But our method measures the representational similarity only via atoms, a portion of model parameters, which is data-agnostic and much more efficient.

Learning paradigm with numerous models.Some machine learning tasks involve numerous models. For example, in Federated learning , thousands of models are trained across clients. In Continual learning, there are multiple models generated across time . Federated learning (FL) aims to improve the performance of the system by continuously training and aggregating models from users without collecting data [20; 34; 51]. FL requires communication efficiency while thousands or even millions of clients may be involved . It also required to achieve personalization [14; 53] considering data heterogeneity of different users [6; 17; 28]. Estimating user similarity can effectively address these challenges in FL. Continual learning (CL) aims at providing long-term knowledge accumulation, and the main challenge is to avoid catastrophic forgetting by learning new tasks while remembering the old ones [1; 18; 19; 26; 62]. One promising way is to store neural networks for each task [16; 30; 32; 48; 60]. As the number of tasks increases, a large number of models are generated and stored. It is important to find a way to access their relations to reuse models.

Filter atom decomposition.The research in task subspace modeling treats tasks as compositions of latent basis tasks and their linear combinations [10; 25; 33; 45; 64]. In the context of convolutional filter decomposition, DCFNet  introduces the filter subspace as an expansion of convolutional filters using a predetermined set of filter atoms. With the filter subspace, a group of tasks are separately modeled using neural networks, with sets of filter atoms learned for individual tasks, while a common set of atom coefficients is shared among tasks. The applications of filter subspace span among various domains, including domain adaptation [54; 57], continual learning , adaptive convolution [56; 59], image generation [55; 58], video comprehension , and graph convolution .

## 5 Conclusion

In this paper, we proposed a new paradigm for reducing representational similarity analysis in CNNs to filter subspace distance assessment, which is targeted for application scenarios where numerous models are learned. The proposed approach is targeted for application scenarios where numerous models are learned, and a computationally efficient method to assess model similarities is critical in these scenarios. We provided both theoretical and empirical evidence that the proposed filter subspace-based similarity exhibits a strong linear correlation with popular probing-based metrics while being significantly more efficient and robust in probing data. It was evaluated on both federated learning and continual learning tasks and achieves competitive performance with millions of times reduction in computational cost.

The majority of approaches in FL or CL are applied to the models with the same architecture, the proposed similarity measure with shared atom coefficient is more advantageous to be incorporated in these tasks. Our method currently assumes respective layers among compared CNNs to have coefficients with the same dimension. For our future work, we will explore the way to share atom coefficients among layers to achieve filter subspace similarity with different dimensions.