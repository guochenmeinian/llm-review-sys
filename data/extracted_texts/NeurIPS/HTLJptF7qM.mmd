# Noisy Label Learning with Instance-Dependent Outliers: Identifiability via Crowd Wisdom

Tri Nguyen

School of EECS

Oregon State University

Corvallis, Oregon, USA

nguyetr9@oregonstate.edu

&Shahana Ibrahim

Department of ECE

University of Central Florida

Orlando, Florida, USA

shahana.ibrahim@ucf.edu

Equal contribution.

&Xiao Fu

School of EECS

Oregon State University

Corvallis, Oregon, USA

xiao.fu@oregonstate.edu

###### Abstract

The generation of label noise is often modeled as a process involving a probability transition matrix (also interpreted as the _annotator confusion matrix_) imposed onto the label distribution. Under this model, learning the "ground-truth classifier"--i.e., the classifier that can be learned if no noise was present--and the confusion matrix boils down to a model identification problem. Prior works along this line demonstrated appealing empirical performance, yet identifiability of the model was mostly established by assuming an instance-invariant confusion matrix. Having an (occasionally) instance-dependent confusion matrix across data samples is apparently more realistic, but inevitably introduces outliers to the model. Our interest lies in confusion matrix-based noisy label learning with such outliers taken into consideration. We begin with pointing out that under the model of interest, using labels produced by only one annotator is fundamentally insufficient to detect the outliers or identify the ground-truth classifier. Then, we prove that by employing a crowdsourcing strategy involving multiple annotators, a carefully designed loss function can establish the desired model identifiability under reasonable conditions. Our development builds upon a link between the noisy label model and a column-corrupted matrix factorization mode--based on which we show that crowdsourced annotations distinguish nominal data and instance-dependent outliers using a low-dimensional subspace. Experiments show that our learning scheme substantially improves outlier detection and the classifier's testing accuracy.

## 1 Introduction

Deep neural networks can easily overfit to noisy labels due to its excessive expressiveness . Many strategies have been proposed to avoid negative impacts of label noise when training neural classifiers: see, e.g., noisy label filtering , robust loss design , and noise generation modeling (or loss correction) . In the last genre, a noisy class label is modeled as a realization of a categorical distribution, which is the ground-truth label distribution being distorted by a _probability transition matrix_. The transition matrix is interpreted as the "confusion matrix"  that can effectivelymodel the annotators' expertise level and the difficulty of annotating each class/sample, and thus is considered intuitive. Under this model, learning the "label noise-free" target neural classifier boils down to identifying the confusion matrix.

The confusion matrix-based models have proven quite useful in practice--algorithms developed in this line of work often exhibits appealing empirical performance; see, e.g., . In addition, these models admit interesting statistical and algebraic structures, leading to plausible results on identifiability of the confusion matrix and/or the "ground-truth classifier"--i.e., the classifier that can be learned if no noisy annotations were present. However, most of the aforementioned early works considered an instance-invariant confusion matrix--i.e., a confusion matrix is not affected by sample features, but only classes--for analytical and computational simplicity. Considering _instance-dependent_ confusion models is more realistic, as the sample characteristics, e.g., lightening and resolution of an image, affect the annotation accuracy . The existence of such (at least occasionally occurred) instance-dependent noisy labels inevitably introduces outliers to the instance-invariant confusion models, leading to performance degradation. In general, learning under instance-dependent confusion matrices is heavily ill-posed. Hence, various problem-specific structures were exploited to add regularization terms and constraints; see, e.g., . Nonetheless, unlike the instance-invariant confusion matrix case, identifiability guarantees of the target classifier have been largely under-studied. The lack of theoretical understanding also affects algorithm design--many approaches in this domain had to resort to somewhat ad-hoc treatments with multi-stage training procedures, often involving nontrivial pre- and post-processing; see .

**Contributions.** To advance understanding, we consider a model where instance-dependent confusion matrices occur occasionally across the samples, and the rest of data share a common _nominal_ confusion matrix. This way, the instance-dependent noisy labels can be regarded as outliers. The model is motivated by the fact that only a proportion of all instances may have a labeling difficulty that significantly deviates from the general population . Our contributions are as follows:

_(i) Identifiability via Crowdsourcing._ We first show that using the sparsity prior on outliers is insufficient to identify the model of interest, if only one annotator is present. To circumvent this challenge, we propose to employ a _crowd_ of annotators--which is the common practice in data labeling . We show that, by incorporating a carefully designed column sparsity constraint to a _coupled cross-entropy_ (CCE) loss from crowdsourcing  to integrate the annotators' outputs, the outliers can be provably identified. Consequently, the ground-truth classifier can be learned with generalization guarantees.

_(ii) End-to-end One-Stage Implementation._ Our approach features a one-stage continuous optimization-based implementation. To be specific, our proposed learning loss allows to approximate the column-sparsity constraint using a smoothed nonconvex \(_{p}\) (where \(0<p 1\)) function based regularization (see ). This way, the overall loss is differentiable and can be readily tackled by off-the-shelf optimizers. This is in contrast to many existing methods that involve multiple stages (e.g., )--and is arguably easier to implement.

We evaluate the proposed method over a number real datasets that are annotated by machine and human annotators under various conditions and observed nontrivial improvements of testing accuracy.

**Notation.** The notations are summarized in the supplementary material in Sec. A.

## 2 Problem Statement

**The Confusion Model and Learning Goal.** Consider \(N\) data items \(\{_{n}\}_{n=1}^{N}\) from \(K\) classes. Here, \(_{n}^{D}\) represents the feature vector of the \(n\)th data item. Let \(\{y_{n}\}_{n=1}^{N}\) be the set of ground-truth labels, where \(y_{n}[K]\). Assume that \(y_{n}\)'s are unobserved. Instead, we observe the "noisy" version \(\{_{n}\}_{n=1}^{N}\). The label \(_{n}[K]\) is noisy due to various reasons, e.g., the lack of expertise of the annotator. In this setting, our goal is to learn a performance-guaranteed classifier using the data items \(\{_{n}\}_{n=1}^{N}\), and noisy labels \(\{_{n}\}_{n=1}^{N}\). We consider the following expression of \((_{n}=k|_{n})\):

\[(_{n}=k|_{n})=_{k^{}=1}^{K}( _{n}=k|y_{n}=k^{},_{n})(y_{n}=k^{}| {x}_{n}).\] (1)

Note that \((y_{n}=k|_{n})\) is the _ground-truth label distribution_ given the sample \(_{n}\). This is also the distribution that the _target classifier_ wishes to learn from. We represent the _ground-truth classifier_using a function \(^{}:^{D}^{K}\) such that \([^{}(_{n})]_{k}(y_{n}=k|_{n})\). We call \(^{}\) the ground-truth classifier as it is the function that we aim to learn--and it can be learned under ideal conditions (e.g., when \(N=\) and the learner is a universal approximator), given that no noise is present. We define \(^{}:^{D}^{K K}\) such that \([^{}(_{n})]_{k,k^{}}(_{n}=k|y_{n}=k^{},_{n}),_{n}\). That is, \(^{}(_{n})\) is the _label transition matrix_ or _confusion matrix_ of sample \(_{n}\). Let \([_{n}^{}]_{k}(_{n}=k|_{n})\). Accordingly, the noisy label generation process for \(_{n}\)\(}}{{}}\)\(_{}\) is modeled as follows:

\[_{n}^{} =^{}(_{n})^{}(_{n}),\] (2a) \[_{n} (_{n}^{}),\] (2b)

where \(()\) denotes the \(K\)-dimensional categorical distribution. Per the physical meaning of \(^{}(_{n})\) and \(^{}(_{n})\), we have \(^{}^{}(_{n})=^{},^{}( _{n})\) and \(^{}^{}(_{n})=1\), \(^{}(_{n})\), for all \(n\). Under this model, the main goal is to learn \(^{}\) from \(\{_{n}\}_{n=1}^{N}\) and \(\{_{n}\}_{n=1}^{N}\).

**Identifability under Instance-Invariant \(^{}()\).** From (2a), it becomes apparent that learning \(^{}\) is not a straightforward task. Even if \(_{n}^{}\) is observed (which is not), it is hard to identify \(^{}(_{n})\) or \(^{}(_{n})\) from the product \(_{n}^{}=^{}(_{n})^{}(_{n})\). To tackle the identifiability challenge, a popular approach is to simplify (2) by assuming that all instances have the same confusion matrix, i.e., \(^{}(_{n})=^{}\) with a certain \(^{}^{K K}\) for all \(n\). Under this assumption, many used the "loss correction" based formulation, e.g., . The loss correction idea modifies the training loss of the classifier by taking the confusion matrix into consideration, e.g.,

\[*{minimize}_{,}\;-\;_{n=1}^{N}_{k=1}^{K}[_{n}]_{k}[(_{n})]_{k},\] (3)

where the objective is a modified _cross-entropy_ (CE) loss. The "noise correction" term \(\) and \(\) are used to learn \(^{}\) and \(^{}\), respectively, \(}_{n}\{0,1\}^{K}\) denotes the one-hot encoding of the noisy label \(_{n}\), and \(\) and \(\) denote appropriate constraint sets--i.e., \(=\{^{K K}^{}=^{ },\;\}\) and \(\) is a certain deep neural network function class whose outputs reside in the probability simplex. Note that the objective is often used together with regularization and additional constraints of \(\) for various purposes; see, e.g., . When \(N\), the CE term enforces the learned \(\) and \(\) to satisfy \(_{n}^{}=(_{n})\) for all \(n\). Note that \(}=[_{1}^{},,_{N}^{}]\) can be expressed as \(}=^{}[^{}(_{1}), ,^{}(_{N})]=^{}^{},\) where \(^{}=[^{}(_{1}),,^{}( _{N})]\). Consequently, Eq. (3) can be understood as learning \(\) and \(\) such that \(}\). As both \(^{}\) and \(^{}\) are nonnegative matrices (due to their physical meaning), the identifiability of \(^{}\) can be connected to uniqueness of the _nonnegative matrix factorization_ (NMF) model \(}=^{}^{}\); see .

These are interesting developments, yet the key assumption \(^{}(_{n})=^{}\) for all \(n\) appears to be overly stringent. As mentioned, it makes sense to believe that at least a proportion of samples have instance-dependent \(^{}(_{n})\)'s . Not considering such samples may cause performance degradation.

**Instance-Dependent Confusion-Induced Outliers.** When the instance-dependent confusion happens sparingly instead of overwhelmingly, we can re-express \(^{}(_{n})\) using the following decomposition:

\[^{}(_{n})=^{}+^{}(_{n}),\] (4)

where \(^{}^{K K}\) represents an instance-independent (class-dependent) confusion matrix--which is the nominal confusion matrix that reflects the general annotation difficulty of the dataset. The term \(^{}(_{n})\) represents the instance-dependent "perturbation". For many \(n\)'s, \(^{}(_{n})=\). When \(^{}(_{n})\), we have \((^{}+^{}(_{n}))^{}(_{n})= ^{}^{}(_{n})+_{n}^{}\), where \(_{n}^{}=^{}(_{n})^{}(_{n})\). Using (4), the model in (2a) can be expressed as follows:

\[_{n}^{}=^{}^{}(_{n })+_{n}^{},& n\\ ^{}^{}(_{n}),& n^{c}, \] (5)

where \(\) and \(^{c}\) represent the instance index set where \(^{}(_{n})\) and its complement, respectively. In other words, \(\) is the outlier index set.

A remark is that from this point on we will ignore the structure \(_{n}^{}=^{}(_{n})^{}(_{n})\) of the outliers. Disregarding the structure comes with some losses. For example, this way, our method is not able to learn \(^{}(_{n})\) that could be of interest. Nonetheless, considering \(^{}(_{n})\) renders extra modeling and computational burdens. Our treatment simplifies the subsequent analytical and computational developments, particularly, algorithm design. In addition, other types of outliers and anomalies can also be handled by the proposed approach due to the unstructured treatment.

**Challenge of Outlier Detection.** Under the model in (5), the natural idea is to first identify \(\) and remove the impacts of outliers. Our first attempt is to explicitly model \(_{n}^{}\) and modify (3) as follows:

\[*{minimize}_{,\{_{n} \},} -_{n=1}^{N}_{k=1}^{K}[}_{n}]_{k} [(_{n})+_{n}]_{k},\] \[*{subject\;to} _{n=1}^{N}\{\|_{n}\|_{2}>0\} C,\]

where \(=\{^{K}^{}=0\}\) is the constraint set for \(_{n}\)'s (due to the probability simplex constraints on the model parameters in (5)) and \(C\) is a scalar that is an estimation of \(||\). The idea is to use the prior knowledge that \(_{n}\) does not occur overwhelmingly as constraint. The hope is that the solution of (6) can detect \(\), thereby enabling accurate estimation of \(^{}\) and \(^{}\). However, the following fact reveals a conflicting insight:

**Fact 2.1**.: _Assume that all \(\) are universal function approximators, that \(*{rank}(^{})=K\), and that \(N\). Suppose that \(\). Optimal solutions of Problem (6) can attain the minimal value of the objective function and satisfy the sparsity constraint without detecting any outliers. One such trivial solution \((^{},^{},_{n}^{})\) is \(^{}=,^{}()=^{}^{ }(_{n})+_{n}^{},\) and \(_{n}^{}=\) for all \(n\)._

This fact is easy to show (see Appendix F), yet it highlights a somewhat unexpected issue in confusion matrix-based noisy label learning: If only one nominal confusion matrix \(^{}\) is present (i.e., only one annotator is employed), it does not suffice to recover the ground-truth \(^{}\) when there exists instance-dependent outliers (or any other types of outliers) under the model in (5)--under the condition that \(\) is a universal function approximator. The practical implication is undesirable: as deep neural networks are powerful function approximators and are usually very expressive, Fact 2.1 means that the learned neural networks easily overfit to outliers, even if the sparsity prior on outliers is explicitly used in the loss function.

## 3 Proposed Approach

**Intuition: Exploiting Crowd-induced Subspace.** To get around the issues illustrated in Fact 2.1, our idea is to "create" a low-dimensional subspace where the nominal data reside while the outliers are likely far away from. To this end, we propose a _crowdsourcing_ approach. Consider the scenario where \(M\) annotators label the dataset \(\{_{n}\}_{n=1}^{N}\). The noisy label provided by an annotator \(m\) for the data item \(_{n}\) is denoted as \(_{n}^{(m)}[K]\) and follows the generation model as follows:

\[_{n}^{(m)}(_{n}^{(m)}),\ _{n}^{(m)}=_{m}^{}^{}(_{n})+ _{n}^{(m)}, m,n,\] (6)

where \(_{m}^{}\) is the annotator \(m\)'s confusion matrix and \(_{n}^{(m)}\) is the outlier term induced by annotator \(m\)'s instance-dependent confusion.

Putting together, we have the following expression:

\[^{}=_{1}^{(1)}&&_{N}^{ (1)}\\ &&\\ _{1}^{(M)}&&_{N}^{(M)}= _{1}^{}^{}(_{1})+_{1}^{(1)}& &_{1}^{}^{}(_{N})+_{N}^{ (1)}\\ &&\\ _{M}^{}^{}(_{1})+_{1}^{(M)}& &_{M}^{}^{}(_{N})+_{N}^{ (M)}\] \[^{}=^{}^{ }+^{},\] (7)

where \(^{}\) is defined as before, and

\[^{}=[(_{1}^{})^{},,(_{M}^{ })^{}]^{},_{n}^{}=[(_{n}^{(1)})^{ },,(_{n}^{(M)})^{}]^{}.\] (8)

Denote \(^{(m)}\) as the index set where \(_{n}^{(m)}\). Then \(=^{(1)}^{(M)}\) stands for the nonzero column support of \(^{}\).

From (7), one can see that now the "data columns", i.e., the columns of the matrix \(^{}\), live in a high-dimensional space, i.e., \(^{MK}\). However, the nominal data part \(^{}^{}\) resides in a \(K\)-dimensional subspace \((^{})\) where \(K MK\), if multiple annotators are employed. More importantly, \(^{}_{j}\) for \(j\) is an \(MK\)-dimensional vector and is unlikely to be inside \((^{})\). With this geometry, it is much more likely that one can separate the outliers from the nominal data.

Next, we will use the above intuition to build a learning loss. We will take into consideration of practical aspects, e.g., missing annotations and finite dataset size \(N\).

**Proposed Identification Criterion & Analysis.** In this section, we connect our proposed idea to a practically convenient, end-to-end identification criterion and provide identifiability guarantees for the desired model parameters. We consider the following empirical risk minimization under the crowdsourcing model in (6):

\[*{minimize}_{\{_{m}\},\{^{ (m)}_{n}\},}_{} -_{(m,n)}_{k=1}^{K}[}^{(m)}_{n}]_{k}[_{m}(_{n})+^{(m)}_{n} ]_{k},\] (9a) \[\;_{n=1}^{N}\{ _{m=1}^{M}\|^{(m)}_{n}\|_{2}>0\} C,\] (9b)

where \([M][N]\) denotes the set of observed noisy labels indexed by \((m,n)\) with \(S=||\) (note that all data items may not be labeled by an annotator), \(}^{(m)}_{n}\) is the one-hot encoding of the annotator-provided label \(^{(m)}_{n}\), and \(C\) is an estimate of \(||\). Here, the constraint sets \(\), \(\), and \(\) are as defined before. If \(^{(m)}_{n}\) is not considered, the objective function (9a) is sometimes referred to as _coupled cross-entropy minimization_ (CCEM) in the end-to-end crowdsourcing literature . CCEM has not been used together with the outlier detection constraint (9b)--and the theoretical characterizations of the constrained formulation is unknown.

We will use the following notations in our analysis. We use \([N]\) to denote index set of the outliers, i.e., \(=\{n^{}_{n}\}\). We also consider that the function class in our learning problem, i.e., \(\), has a complexity measure \(_{}\). In particular, we adopt the so-called spectral-complexity upper bound of the function class \(\); see Lemma C.4 in the Appendix.

We first establish that the criterion (9) recovers the ground-truth \(^{(m)}_{n}\)'s with a reasonable accuracy. Specifically, we hope to characterize the average estimation accuracy of \(}^{(m)}_{n}\)'s where \(}^{(m)}_{n}=}_{m}}(_{n})+ }^{(m)}_{n}\) and \(}_{m}\)'s, \(}^{(m)}_{n}\)'s and \(}\) are estimated via solving (9):

**Lemma 3.1**.: _Assume that the observed index pairs \((m,n)\) are sampled uniformly at random with replacement. Also assume \(^{}\), \(|| C N/2\), and that each \([_{m}(_{n})+^{(m)}_{n}]_{k},\;_{m} ,^{(m)}_{n},\) are lower bounded by \((1/)\) for a certain \(>0\). Then, with probability greater than \(1-\), the following holds:_

\[_{n=1}^{N}_{m=1}^{M}\|^{(m)}_{n}-}^{(m)}_{n}\|_{2}^{2}_{}(S),_{}(S) =(_{S} S/}}{{(1/ )}}/{}),\]

_where \(_{}^{2}=K^{2}(K)+\| ^{2}_{}_{}}}{{S}}+CMK\) and \(=[_{n_{1}},,_{n_{S}}]\) are the annotated samples, in which \((m_{s},n_{s})\) for \(s=1,,S\)._

The proof is in Appendix C, which uses a similar idea as [31, Proposition 1] but takes into consideration of the outliers. Lemma 3.1 reveals that the criterion (9) essentially recovers the complete matrix \(^{}\) [cf. Eq, (7)] from the noisy and incomplete observations of its entries, when \(}{{C}}\) is sufficiently large. To proceed, we will need a suite of assumptions and definitions:

**Assumption 3.2**.: The outliers satisfy \(^{}_{i}(^{}), i \), where \((^{})=K\).

Assumption 3.2 presents the key condition to disassociate instance-dependent and instance-independent confusions. As discussed after Eq. (8), having a larger \(M\) would decrease the possibility that any \(^{}_{n}\) belongs to \((^{})\). We also characterize the "quantity" that \(^{}_{n}\) is perturbed from \((^{})\) in the following definition:

**Definition 3.3**.: The _outlier impact score_\((_{i}^{})\) of data \(i\) is defined as

\[(_{i}^{})_{ ;,_{i},\\ ^{},||=N-2|| }(\|^{}^{}(:,)-\|_{}^{2}+\|^{}_{i}^{}+_{i}^{ }-\|^{2})\] (10)

where \(==[_{1}^{},,_{M}^{}]^{} _{m}}\).

A larger score \((_{i}^{})\) implies it is easier for our criterion to distinguish the outliers from the nominal data. Fig. 1 illustrates the geometry that we rely on to detect outliers. One can notice that when \(M=1\), we always have \((_{i}^{})=0\), as the outlier satisfies \(_{i}^{}(^{})\).

In addition, we consider the following assumptions to assist identifying \(^{}\) and \(^{}\):

**Assumption 3.4** (**Class Specialists and Anchor Points)**.: Assume that the following conditions hold:

1. There exists a near-class specialist for each class \(k\), i.e., \( k[K], m_{k}[M]\) such that \(\|_{m_{k}}^{}(k,:)-}_{k}^{}\|_{1}\), where \(}_{k}\) is a unit vector.
2. There exists a near-anchor point sample for each class \(k\) in the dataset, i.e., \( k[K], n_{k}^{c}\) such that \(\|^{}(_{n_{k}})-}_{k}^{}\|_{2}\), where \(}_{k}\) is a unit vector.

Assumption 3.4.a. is sometimes used in the crowdsourcing literature (e.g., ) to characterize the expertise of annotators. Assumption 3.4.b. is often seen in loss correction based noisy label learning; see, e.g., . Under Assumptions 3.2 and 3.4, we have the following result:

**Theorem 3.5** (**Identifiability and Generalization)**.: _Let \(\{(}_{m})\), \(\{}_{n}^{(m)}\}\), \(}\}\) be any optimal solution of (9) with \(}=n[N]}_{n}=[(}_{n}^{(m)})^{},,(}_{n}^{(M)})^{}]^{}}\). Suppose that the conditions in Lemma 3.1 holds, that we set \(C=||\), and that assumption 3.4 holds with \(_{1},_{2} 1/K\). Denote \(\) as the upper bound \(_{}(_{m}^{}),\; m\). In addition, assume that \(^{},||=N-2\,| ^{}|\), \((^{}(:,))=K\). Then, for some \(>0\), and \(S>S_{0}\) where \(S_{0}\) as the smallest integer such that \((_{i}^{})_{}(S_{0}),\; i \), the following result holds with probability greater than \(1-2/S-K/T^{}\):_

\[}_{_{}} [_{}\|}()-^{}^{ }()\|_{2}^{2}]& K(+_{1}+_{2}),\\ _{}\|}_{m}-_{m}^{}\|_{}^{2}&=K^{2}(+_{1}+_{2}),\;  m,\]

_where \(^{2}=(}}{{}}( +(\|\|_{})^{0.25}))\), \(\) a permutation matrix, \(\) is defined as before, and \(T=N-||\). In addition, we have exact outlier detection, i.e., \(}=\)._

The proof is in Appendix D. We should mention that we set \(C=||\) for notation simplicity. With a notation-wise slightly more complicated definition of \(\), the same proof holds under \(C||\), which leads to \(T=N-C\) and \(}\). That is, over-estimated \(C\) still enables identifying \(\) with the price of discarding some nominal samples. While the criterion (9) offers the desired identifiability, it requires the presence of class-specialist annotators and the anchor data points (c.f. Assumption 3.4), which are considered relatively restrictive . In , the CE loss was combined with a simplex volume minimization-based regularization to establish identifiability of \(^{}\) under more relaxed conditions, namely, the _sufficiently scattered condition_ (SSC) from the NMF literature . We show that this is also viable in the presence of outliers:

**Theorem 3.6** (Enhanced Identifiability).: _Let \((\{}_{m}\},\{}_{n}^{(m)}\},\,})\) be any optimal solution of (9) with \(}[}_{1}^{},,}_{ M}^{}]^{}\) admitting the minimum volume of \(\{}\}\) (i.e., the simplex spanned by its columns) over all possible optimal solutions. Assume that we set \(C=||\) and that \(^{}(:,^{c})\) satisfies the SSC. Under Assumptions 3.2 and the same conditions used in Lemma 3.1, the following result holds with probability greater than \(1-\), when \(S\) grows to infinity:_

\[*{}_{_{}}\|}()-^{}^{}()\|_{2}^{2}= (|^{c}|^{-5/8}(2\|(:,^{c})\|_{ }_{})^{}+^{c}|}).\]

_In addition, we have \(\|}_{m}-_{m}^{}\|_{}=0, m\) and \(}=\)._

The proof is relegated to Appendix E, which also holds for \(C||\) with slight modifications as discussed before. Theorem 3.6 shows that the target classifier can be accurately estimated in the presence of instance-dependent outliers, without needing anchor samples or class specialists.

**Implementation.** Our learning losses allows relatively easy implementation and optimization. We consider the following regularized criterion:

\[*{minimize}_{_{m},_{m}^{(m)},}}+_{1}}+_{2} }.\]

Note that we use \(}=_{n=1}^{N}(_{m=1}^{M}\|_{n}^{(m)}\|_{2}^{2 }+)^{}\) where \(>0\) and \(0<p 1\) to approximate the column-sparsity constraint. The \(_{2}/_{p}\) nonconvex mixed quasi-norm has proven to be a very effective approximator for column sparsity . In addition, as the function is differentiable, it offers an optimization-friendly surrogate.

For the minimum-volume loss, we use \(}=-(^{})\), where \(=[(_{1}),,(_{N})]\). Note that the term encourages maximizing the volume of \(\{\}\), which in turn minimizes the volume of \(\{\}\) under the model \(}=\). Therefore, the volume of \(\{\}\) can be minimized via using either \((^{})\) or \(-(^{})\) as the regularizer. The reason why we choose the latter is because minimizing \((^{})\) is ill-defined--the term encourages a rank-deficient \(\) instead of a small-volume full-rank \(\). The remedy in the literature is to use \((^{}+)\) with \(>0\) or adding structural constraints (e.g., diagonal dominance) to \(\), but these require more parameter tuning and more prior knowledge.

The constraints on \(_{m}\)'s can be handled by adding softmax to the columns. The function class \(\) can be a designated deep neural network model (e.g., ResNet), where the outputs are also constrained by softmax. The constraint on \(_{n}^{(m)}\) can be handled by parameterizing each \(_{n}^{m}=}_{n}^{(m)}-(^{}}_{n}^{ (m)})/K\), where \(}_{n}^{(m)}\) is trainable weight vector. More details of the implementation are provided in Appendix G.1. We name our approach as _Crowdsourcing-based Outlier-robust criterion and INstance-dependence-aware deep neural Network learning_, abbreviated as COINNet.

## 4 Related Works

Our development is related to works in transition matrix-based [14; 15; 16; 17; 18; 19; 21; 23; 27; 28; 29; 33; 47; 55; 56] and sample selection-based [33; 35; 37; 39; 28] noisy label learning, confusion identification in crowdsourcing [14; 18; 57], end-to-end crowdsourcing [30; 20; 31; 49] (with some paying particular attention to incomplete annotations [58; 59]), and identifiability of NMF [53; 60]--see detailed discussions of the related work in Appendix B.

A particular connection to highlight is the usage of multiple annotators as a key enabler of our identifiability guarantees. This result (implicitly) aligns with some recently proposed approaches that advocate for the use of multiple labels in learning under label noise [45; 49]. Specifically, the work by  established the identifiability of instance-independent label noise using the consensus of at least three similar data items with the same true label. Similarly, the work by  leveraged Kruskal's identifiability-based arguments to recommend using at least three noisy labels to establish the identifiability of the instance-dependent noise transition matrix. These works, particularly , were more concerned with the theoretical limitations other than realizable methods. They did not connect the existence of repeated noisy labels to crowdsourcing, but used clusterability properties of datasets to support their arguments. In this work, we advocate _diversity in crowd wisdom_--labels from multiple annotators--based on an intuitive geometric characteristic of the model, which leads to clearly realizable implementations.

Experiments

**Baselines.** The proposed method is compared with a number of existing baselines in noisy label learning. We choose several end-to-end crowdsourcing methods, namely, GeoCrowdNet(F)&(W) , TraceReg, CrowdLayer, and Max-MIG. We also employ four different instance-dependent noisy learning approach, namely, MEIDTM, PTD, and BLTM--which all use a single annotator. In addition, we present the result of VolMinNet that uses the instance-independent confusion model and the volume minimization regularization. We also use two noise-robust loss function-based approaches, namely GCE and Reweight. For baselines that were developed for single annotator cases, we train them using the labels after majority voting. Note that all the confusion matrix-based noise correction methods (including ours) inherently have a permutation ambiguity (cf. the \(\) term in Theorems 3.5 and 3.6). Hence, we report the highest classification accuracy attained among all possible permutation matrices for every method. In practice, \(\) can be removed by additional annotator inspection on several anchor samples, but we skipped these steps to keep the evaluation simple.

### Experiments with Machine Annotations

**Dataset.** We consider the CIFAR-10  and the STL-10 datasets --see Appendix G for details.

**Experiment Setup.** To generate multiple noisy labels, we use \(M=3\) machine annotators for each dataset. In order to simulate a wide range of annotation behaviors, we employ different classification and clustering methods such as _\(k\)-nearest neighbors_ (kNN), logistic regression, and convolutional neural networks. Each machine annotator is trained by randomly choosing a subset of the training data. We control the labeling accuracies of these machine annotators by varying the size of the dataset and the number of epochs during their training phase. This results in annotators with different labeling qualities, with their average individual noise rates around 20% (_good_), 40% (_medium_), and 70% (_low_). This way, we set up the following three cases: _(i)_ High Noise: Three machine annotators are with low, medium, and good quality, respectively. _(i)_ Medium Noise: Two machine annotators are with medium quality and one machine annotator with good quality. _(i)_ Low Noise: One machine annotators is with medium quality and two machine annotators with good quality. More details are in Appendix G.2.

**Neural Network and Optimizer Settings.** We use ResNet-34 and ResNet-9 architectures  as the backbone to run all methods on CIFAR-10 and STL-10 datasets, respectively. For our proposed approach COINNet, we fix \(=10^{-10},p=0.4\), and \(_{1}=_{2}=0.01\). Adam  is used as the optimizer with weight decay of \(10^{-4}\), learning rate of 0.01, and batch size of 512.

More details and ablation studies with the hyperparameters are provided in Appendix G.

**Results.** Table 1 presents the average testing accuracy of the methods. One can see that our approach COINNet consistently outperforms all baselines in different scenarios under test. Notably, the performance gap between COINNet and the second-best method is more significant in the high noise regime. For example, in the CIFAR-10 high noise scenario, COINNet shows around 4% improvement over the second-best performing baseline GeoCrowdNet(F). Another key observation is that the instance-dependent modeling based baselines such as MEIDTM, PTD, and BLTM, exhibit much degraded performance in high-noise scenarios, possibly due to their multi-stage procedures that accumulate errors easily under such circumstances. In addition, our approach outperforms the instance-independent confusion-based baselines, such as VolMinNet, Reweight and GeoCrowdNet(W), by nontrivial margins, showing that modeling instance-dependent outliers is indeed beneficial.

### Experiments Using Real Annotations

In this section, we use three datasets to evaluate the algorithms.

**CIFAR-10N.** The first dataset that we use is the CIFAR-10N dataset . The data has \(N=60,000\) samples from \(K=10\) classes and the samples were labeled by \(M=3\) anonymous real annotators. The error rates of the 3 annotators are 17.23%, 18.12%, and 17.64%, respectively.

**LabelMe.** We also test the algorithms over the LabelMe dataset . The dataset has \(N=2,688\) samples from \(K=8\) classes, and \(M=59\) anonymous real annotators were involved in labeling the data. The average error rate is 25.95%.

**ImageNet-15N.** In addition to existing datasets, we also acquire noisy annotations by asking AMT workers to annotate some images from ImageNet. We select \(K=15\) classes and submit randomly chosen images to AMT for labeling. Eventually we collect annotations for \(N=2,514\) images from \(M=100\) anonymous real annotators, which serve as our training set. The average error rate of the annotators is 42.68%. The validation and testing sets have 1,462 and 13,157 images, respectively. We release the code and our acquired noisy annotations at https://github.com/ductri/COINNet.

**Settings.** For CIFAR-10N, we employ the ResNet-34 architecture to serve as \(\). For the LabelMe and ImageNet-15N datasets, we employ similar settings as in . Specifically, as the training sets are small, the pretrained VGG-16  and CLIP  models are used to first extract image embeddings for the experiments on LabelMe and ImageNet-15N, respectively. The embeddings are then fed to \(\), which is a fully connected neural network with one hidden layer and \(128\) hidden ReLU units. The same encoders and architecture choices are employed for the all methods under test. For our approach COINNet, we set \(_{1}=_{2}=0.1\) for LabelMe, and \(_{1}=_{2}=0.01\) for CIFAR-10N and ImageNet-15N.

**Results.** Table 2 presents the average testing accuracy of different methods on the three datasets. The proposed approach, COINNet, shows promising results in all cases, clearly outperforming the baselines by a noticeable edge. This is consistent with the machine annotator experiments.

Fig. 2 demonstrates the outlier identification results using the CIFAR-10N dataset. Here, we define the outlier indicator as \(s_{n}=_{m=1}^{M}\|}_{n}^{(m)}\|_{2}, n\), where \(}_{n}^{(m)}\) are the learned instance-dependent perturbations in the model (5). The histogram plot of these scores in Fig. 2 clearly shows that our method does output two types of samples, i.e., nominal samples and outliers, based on these perturbations. In the figure, the images on the middle and the right are examples from the low and high outlier indicator value regimes, respectively. The images with high \(s_{n}\) values show more instance-dependent confusion characteristics (such as background noise and blurring) compared to

 
**Method** &  &  \\   & High Noise & Medium Noise & Low Noise & High Noise & Medium Noise & Low Noise \\  PTD & 61.98 \(\) 0.35 & 65.71 \(\) 0.69 & 79.21 \(\) 0.41 & 22.94 \(\) 2.15 & 30.92 \(\) 5.45 & 23.98 \(\) 11.52 \\ BLTM & 52.62 \(\) 1.12 & 61.19 \(\) 0.45 & 71.36 \(\) 0.48 & 31.28 \(\) 2.78 & 30.57 \(\) 4.34 & 32.73 \(\) 3.45 \\ VolMinNet & 49.83 \(\) 2.66 & 50.93 \(\) 3.34 & 70.44 \(\) 7.32 & 42.38 \(\) 3.64 & 60.79 \(\) 4.67 \(\) 2.93 \\ Reweight & 59.73 \(\) 1.25 & 67.57 \(\) 0.62 & 77.40 \(\) 0.66 & 48.57 \(\) 8.52 & 54.23 \(\) 3.86 & 54.67 \(\) 6.93 \\ GCE & 52.14 \(\) 0.67 & 63.90 \(\) 0.12 & 73.26 \(\) 0.38 & 58.35 \(\) 0.74 & 60.65 \(\) 0.69 & 61.70 \(\) 2.54 \\ MEIDTM & 51.38 \(\) 0.62 & 56.20 \(\) 0.65 & 71.14 \(\) 0.50 & 55.50 \(\) 0.52 & 60.20 \(\) 0.22 & 59.84 \(\) 3.57 \\ CrowLayer & 26.76 \(\) 3.12 & 67.24 \(\) 2.26 & 80.00 \(\) 3.02 & 47.83 \(\) 5.51 & 61.46 \(\) 4.58 & 66.64 \(\) 2.48 \\ TraceReg & 64.90 \(\) 0.26 & 71.21 \(\) 0.32 & 81.81 \(\) 0.42 & 50.14 \(\) 5.47 & 64.50 \(\) 0.04 & 65.67 \(\) 3.86 \\ MaxMG & 51.92 \(\) 1.10 & 68.45 \(\) 0.21 & 83.16 \(\) 0.53 & 67.92 \(\) 0.49 & 71.75 \(\) 0.20 & 74.32 \(\) 0.76 \\ GeoCrowdlet(F) & 67.22 \(\) 0.27 & 71.91 \(\) 0.50 & 82.18 \(\) 0.39 & 64.70 \(\) 0.82 & 66.15 \(\) 0.49 & 68.40 \(\) 0.31 \\ GeoCrowdlet(W) & 64.08 \(\) 0.83 & 70.01 \(\) 0.08 & 81.53 \(\) 0.27 & 49.30 \(\) 4.62 & 61.54 \(\) 4.90 & 68.37 \(\) 0.41 \\  COINNet (Ours) & **71.22 \(\) 0.72** & **73.31 \(\) 0.99** & **84.14 \(\) 0.38** & **70.12 \(\) 0.48** & **73.11 \(\) 0.37** & **76.39 \(\) 0.58** \\  

Table 1: Average classification accuracy using machine annotators in CIFAR-10 and STL-10 datasets under different labeling scenarios. **Bold black** represents the best and blue represents the second best.

 
**Method/Dataset** & CIFAR-10N & LabelMe & ImageNet-15N \\   PTD & 89.52 \(\) 0.24 & 84.18 \(\) 1.36 & 65.53 \(\) 0.18 \\ BLTM & 75.68 \(\) 0.47 & 82.10 \(\) 0.56 & 66.57 \(\) 0.76 \\ VolMinNet & 86.58 \(\) 0.21 & 79.97 \(\) 0.16 & 63.11 \(\) 1.08 \\ Reweight & 89.56 \(\) 0.30 & 84.51 \(\) 0.50 & 65.85 \(\) 2.93 \\ GCE & 78.01 \(\) 7.23 & 83.41 \(\) 0.59 & 64.71 \(\) 1.38 \\ MEIDTM & 68.69 \(\) 0.31 & 83.53 \(\) 0.21 & 72.66 \(\) 0.58 \\ CrowdLayer & 87.38 \(\) 0.43 & 82.80 \(\) 0.90 & 61.36 \(\) 2.73 \\ TraceReg & 86.57 \(\) 0.24 & 82.83 \(\) 0.23 & 68.43 \(\) 0.12 \\ MaxMIG & 90.11 \(\) 0.09 & 83.73 \(\) 0.84 & 81.13 \(\) 1.42 \\ GeoCrowdNet(F) & 87.19 \(\) 0.37 & 87.21 \(\) 0.39 & 80.45 \(\) 1.77 \\ GeoCrowdNet(W) & 86.43 \(\) 0.44 & 82.83 \(\) 0.75 & 68.79 \(\) 0.27 \\  COINNet (Ours) & **92.09 \(\) 0.47** & **87.60 \(\) 0.54** & **93.71 \(\) 3.26** \\  

Table 2: Average classification accuracy on CIFAR-10N, LabelMe, and ImageNet-15N datasets, labeled by human annotators. **Bold black** represents the best and blue represents the second best.

those in the middle. Overall, these results indicate the effectiveness of our outlier-based model in real-world settings.

Fig. 3 shows some examples selected from high-\(s_{n}\) and low-\(s_{n}\) regimes from the results output by COINNet in the ImageNet-15N experiment. Similar as before, the images in the first row that have lower \(s_{n}\) scores are visually much easier to recognize. The images in the second row that have about 5 times higher \(s_{n}\) scores are apparently more visually confusing.

More experiments can be found in Appendix H.

## 6 Conclusion

In this work, we considered the noisy label learning problem under a confusion matrix-based model. We developed theory and algorithms in the presence of instance-dependent outliers. Our study revealed that relying solely on single-annotator labels is insufficient for effective outlier detection under the model of interest. We further demonstrated that a crowdsourcing approach, leveraging multiple annotators and a sparsity-constrained loss function, can successfully detect outliers and identify the desired, label noise-free learning system under reasonable conditions. Our analyses and design feature a one-stage differentiable training loss that is optimization-friendly. Empirical results underscore the plausibility of our model and the effectiveness of our proposed method, showing noticeable performance improvements over existing baselines.

**Limitations.** Our work explores a noise transition matrix-based model and treats instance-dependent noisy labels as outliers. The key assumption is that the outliers do not occur overwhelmingly. The assumption is useful, but also could be debatable. In principle, all labels could be generated in an instance-dependent manner. More sophisticated models are needed to deal with this more general case. In addition, our treatment is outlier structure-agnostic. The upshot is that this treatment can also help exclude the negative impacts of other types of outliers. However, the downside is that the structural prior knowledge of the outliers is not fully exploited. If the outlier-generating function could be learned, it can help detect "difficult samples" _before_ sending for annotation, which would potentially save resources and reduce annotation errors. We leave this direction for future work.

Figure 3: Some examples from ImageNet-15N with low (top) and high (bottom) \(s_{n}\)’s

Figure 2: Histogram of the learned outlier indicator values \(s_{n}=_{m=1}^{M}\|}_{n}^{(m)}\|_{2}^{2}\) over all training images in the CIFAR-10N dataset (left), and examples with low (middle) and high (right) \(s_{n}\)’s —see more examples in Appendix H.