# LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images

Viraj Prabhu  Sriram Yenamandra  Prithvijit Chattopadhyay  Judy Hoffman

Georgia Institute of Technology

{virajp,sriramy,prithvijit3,judy}@gatech.edu

###### Abstract

We propose an automated algorithm to stress-test a trained visual model by generating language-guided counterfactual test images (LANCE). Our method leverages recent progress in large language modeling and text-based image editing to augment an IID test set with a suite of diverse, realistic, and challenging test images without altering model weights. We benchmark the performance of a diverse set of pretrained models on our generated data and observe significant and consistent performance drops. We further analyze model sensitivity across different types of edits, and demonstrate its applicability at surfacing previously unknown class-level model biases in ImageNet. Code: https://github.com/virajprabhu/lance.

## 1 Introduction

As deep visual models become ubiquitous in high-stakes applications, robust stress-testing assumes paramount importance. However, the traditional paradigm of evaluating performance on large-scale  IID test sets does not adequately vet models for deployment in the wild. First, such models are typically evaluated via aggregate metrics such as accuracy, IoU, or average precision , which treat all test samples equivalently and do not distinguish between error types. Further, such test sets do not adequately capture the "long-tail"  of the data distribution: rare concepts, unseen concepts, and the (combinatorial explosion) of their compositions .

To address this, a considerable number of recent efforts have sought to develop realistic benchmarks for _out-of-distribution_ (OOD) evaluation . However, while useful, such OOD benchmarks are typically _static_ across models and time, rather than being curated to probe a specific instance of a trained model, which diminishes their utility.

Figure 1: The predominant paradigm in computer vision is to benchmark trained models on IID test sets using aggregate metrics such as accuracy, which does not adequately vet models for deployment, _e.g._ for the test image above, while a trained ResNet-50 model  accurately predicts the ground truth label (“dog sled”), it is in truth _highly_ sensitive to the dog _breed_. We propose LANCE, an automated method to surface model vulnerabilities across a diverse range of interventions by generating such counterfactual images using language guidance.

In this work, we eschew the traditional paradigm of static test sets and instead generate _dynamic test suites_. Specifically, we propose a technique that automatically generates a test suite of challenging but realistic _counterfactual_ examples to stress-test a given visual model. Our main insight is that while the scope of possible visual variation is vast, _language_ can serve as a concise intermediate scaffold that captures salient visual features while abstracting away irrelevant detail. Further, while it is extremely challenging to run counterfactual queries on images directly, intervening on a discrete text representation is considerably easier. We call counterfactuals generated by our method Language-guided Counterfactual Images (LANCE).

Our method leverages pretrained foundation models for text-to-image synthesis [15; 16; 17] and large language modeling (LLM) , in combination with recent progress in text-based image editing [19; 20], to generate realistic counterfactual examples. It operates as follows: Given a test image and its ground truth label, we generate a caption conditioned on its label using an image captioning model . We then use an LLM that has been fine-tuned for text editing  to generate realistic perturbations to a single concept at a time - caption subject, object, background, adjectives, or image domain (while leaving words corresponding to the ground truth category unchanged). Finally, we use a guided-diffusion model  with null-text inversion  to generate an edited counterfactual image conditioned on the original image and the perturbed prompt. We obtain a prediction from the trained model on both the original image and the generated counterfactual and compute the average drop in accuracy as a measure of the model's sensitivity to the given attribute.

We benchmark the performance of diverse models pretrained on ImageNet on our generated images, and find that performance drops more consistently and significantly than baseline approaches. Next, we demonstrate applications of our approach: in comparing the relative sensitivity of models to perturbations of different types, and in deriving actionable insights by isolating consistent class-level interventions that change model predictions.

## 2 Related Work

**Diagnosing deep visual models.** Several recent works have focused on the discovery of systematic failure modes in trained neural networks. Early works propose human-in-the-loop approaches based on annotating spurious features learned by sparse linear layers , adversarially robust models , and discovering a hyperplane corresponding to potentially biased attribute . Some prior work also proposes fully automated techniques: by learning decision trees over misclassified instance features from an adversarially robust model  or targeted data collection . Several recent approaches to this problem leverage multi-modal CLIP  embeddings, via error-aware mixture modeling , identifying failure directions in latent space  or learning the task directly on top of CLIP embeddings . However, these works _rationalize_ failures but don't close the loop by evaluating the predicted rationale. Further, shortcut learning  of spurious correlations can also lead to success but for the _wrong reasons_, but prior work only studies failure cases. In contrast, we propose a method that generates visual counterfactuals that can surface a diverse range of model biases that bidirectionally influence performance.

**Image Editing with generative models.** Using generative models to edit images has seen considerable work. Early efforts focused on specialized editing such as style transfer  or translating from one domain to another . More recent work has performed editing in latent space of a model like StyleGAN [35; 36; 37; 38; 39]. Recently, pretrained text-to-image diffusion models have become the tool of choice for image editing [19; 20; 17]. While superior to GAN's at image synthesis , _targeted_ editing using such models that modifies only a specific visual attribute while keeping the rest unchanged is non-trivial. Recent work has presented techniques based on prompt-to-prompt tuning , by targeted insertion of attention maps from cross-attention layers during the diffusion process. Follow-up work generalizes this approach to real images, with additional null-text inversion  or delta denoising scoring functions , which enables instruction-based editing of images  and 3D scenes . We leverage this technique of prompt-to-prompt tuning with null-text inversion but rather than generic image editing focus on generating challenging counterfactual examples.

**Probing discriminative networks with generated counterfactuals.** Several works have attempted to use generative models to obtain explanations from visual models. A popular line of work generates minimal image perturbations that flip a model's prediction [44; 45; 46; 47] (_e.g._changing lip curvature to alter the prediction of a "smiling" classifier). Our work shares a similar goal but with a crucial distinction:we seek to discover minimal perturbations to parts of the image _not_ directly relevant to the task at hand (_e.g._does changing hair color or perceived gender alter the smiling classifier's prediction?).

Closest to our work is Luo _et al._, which determines model sensitivity to a set of user-defined text attributes by tuning a weighted combination of edit vectors in StyleGAN  latent space so as to flip the model prediction while maintaining global structure and semantics via additional losses. However, this method requires the user to enumerate all possible attributes of interest, may not generalize to more complex datasets, and requires optimizing several losses in conjunction. Our work also shares similar motivations as Li _et al._, which uses diffusion models to generate ImageNet-E(diting), a robustness benchmark with varying backgrounds, object sizes, position, and directions. However, their benchmark is static across models, requires object masks per image to generate, and measures robustness to a constrained set of attribute changes. Our work is also related to Wiles _et al._ which generates cluster-based error rationales, Vendrow _et al._, which diagnoses failures by generating near-counterfactuals from human-generated text variations, and Dunlap _et al._, which performs diffusion-based data augmentation. In contrast, we study bias _discovery_ rather than mitigation, deriving causal insights into failures using a targeted editing algorithm, and support stress-testing across an unconstrained, automatically discovered set of attributes.

## 3 Language-guided Counterfactual Image Generation (LANCE)

**Overview.** We introduce LANCE, our algorithm for generating language-guided counterfactual images to stress-test a given visual model. Our key insight is to use language as a structured discrete scaffold to perform interventions on the model. LANCE does this by perturbing a text representation (caption) of an image to conditionally generate a counterfactual image (see Fig 2).

**Perturbed Captions.** We first use a pre-trained captioner (BLIP-2 ), to produce a text description for a given test image. We use beam search decoding with a minimum caption length of 20 words and a high repetition penalty to encourage descriptive captions with low redundancy (see Fig. 5 for examples). Our algorithm then edits the generated caption using a language model to generate variations that only change a _single_ aspect at a time.

**Edited Images.** Next, we use a text-to-image latent diffusion model to generate a new image that reflects the text edit while remaining faithful to the original image in every other respect. We repeat this process for multiple perturbations to generate a challenging test set.

**Sensitivity Analysis.** Finally, we ascertain model sensitivity to different factors of variation by reporting the change in the model's accuracy over the corresponding counterfactual test set.

Next, we detail each step, beginning with describing the factors of visual variation that we stress-test against and the strategy we use to train a structured caption perturbed. Then, we describe our image

Figure 2: **Overview. We propose a method to generate challenging counterfactual examples to stress-test a given visual model. Given a trained model and test set, LANCE generates a textual description (from a captioning model) and perturbed caption (using a large language model (LLM)), which is fed along with the original image to a text-to-image denoising diffusion probabilistic model (DDPM) to perform counterfactual editing. The process is repeated for multiple perturbations to generate a challenging test set. Finally, we ascertain model sensitivity to different factors of variation by reporting the change in the model accuracy over the corresponding counterfactual test set.**

editing methodology and the various checks and balances we insert to ensure that our end-to-end approach yields challenging yet realistic test examples.

### Training a Structured Caption Perturber

To perform meaningful interventions, we train a structured caption perturber that generates targeted caption edits. Motivated by prior work in finetuning instruction-following large language models (LLMs), we seek to train a structured perturber LLM that can generate diverse and realistic caption edits that capture the long-tail of visual variation while still representing realistic scenarios.

**Selecting factors of variation.** To train our perturber model, we select five factors of visual variation against which we will stress-test. While the space of _potential_ factors of visual variation is vast, image captions can constrain this search space by capturing salient visual concepts while abstracting away irrelevant detail. Concretely, we measure resilience to the following factors of variation:

* **Subject.** Modifications to the subject of an image caption (e.g., _a {man, dog, cat,..., horse,)_} can stress-test a model's ability to recognize the ground truth category when it co-occurs with other subjects, including subjects that co-occur rarely (or never) with the ground truth in the training data.
* **Object.** Similarly, modifications to the object of an image caption (e.g., _a {table, chair,..., bed}_) can stress-test a model's resilience to novel or unseen co-occurring concepts.
* **Background.** Modifications to the context of an image caption (e.g., _a {kitchen, bedroom,..., living room}_) can stress-test a model's ability to generalize to different scenes, including diverse backgrounds and weather conditions.
* **Adjective.** Modifications to the adjective of an image caption (e.g., _a {red, blue,..., green}_) can stress-test a model's ability to generalize to visual variations captured by object attributes.
* **Data domain.** Modifications to the data domain of an image caption (e.g., _a {painting, sketch,..., sculpture}_) can stress-test a model's ability to generalize to different data distributions.

While by no means exhaustive, these perturbation types capture a large and representative set of visual variations. Further, we note that our approach is agnostic to this choice and can easily be extended to accommodate additional factors (e.g., camera angle, lighting, etc.). Having defined our perturbation set, we proceed to collect a dataset to train our structured perturbation model.

**Dataset collection.** We use GPT-3.5 turbo  to programmatically collect a small dataset of 0.6k caption perturbations spanning the aforementioned types. For instance, to edit an "adjective", we prompt the model with: _Generate all possible variations of the provided sentence by only adding or altering a single adjective or attribute._ (all prompts in appendix). We find that even with zero-shot prompting, we are able to acquire caption perturbations that modify only the desired factor of variation. We follow this procedure to generate perturbations for randomly selected captions from the MSCOCO dataset . Table. 1 (_top_) highlights example inputs and perturbations. As seen, the dataset contains both simple edits (_e.g., a bicycle \(\)_[_red, blue,..., green_] _bicycle_) as well as more complex ones (_e.g., a bicycle \(\)_bicycle parked outside a coffee shop_), without requiring any human supervision whatsoever.

    &  &  &  \\   **GPT-3.5 turbo** \\ (0.6k samples) \\  } &  Subject (0.1k) \\  } &  bicycle=(score, unicycle,..., shopping cart) \\  } \\    & & & Object (0.1k) \\    & & Object (0.1k) \\    & & & Object (0.1k) \\    & & & Object (0.1k) \\    & & & Object (0.2k) \\    **Finetuned LLAMA-7B** \\ (35k samples) \\  } &  a hockey puck \\ sitting on top \\ of a wooden table \\  } &  Subject (6.2k) \\  } &  hocery puck \\  } \\    & & Object (4.8k) \\    & & & & \\    & & & & \\    & & & & \\    & & & & \\   

Table 1: **Perturbation dataset statistics.** We first programmatically collect a small dataset (0.6k samples) of 5 types of targeted caption perturbations for random MSCOCO  captions from GPT-3.5 turbo  (_top_). We then finetune LLAMA-7B  on this dataset to perform targeted perturbation, and use it to perturb image captions generated for ImageNet  (_bottom_).

**Model training.** Next, we finetune an LLM on the dataset described above to perform targeted caption editing: specifically, we use a LLAMA-7B  LLM and perform LoRA (Low-Rank Adaptation) finetuning : the original LLAMA-7B model is kept frozen, while the _change_ in weight matrices \(W^{d k}\) of the self-attention modules post-adaptation is restricted to have a low-rank decomposition \(W_{ft}=W_{pt}+ W=W_{pt}+AB\), where \(A^{d r}\) and \(B^{r k}\). Here the rank \(r\) is kept low, resulting in very few trainable parameters in \(A\) and \(B\).

We use this finetuned model to perturb generated captions from ImageNet . Table. 1 (_top_) shows examples of perturbations generated by this model. As seen, it is able to generate sensible and diverse edits corresponding to the edit type without changing the remainder of the caption.

**Caption editing: Checks and balances.** An undesirable type of edit would modify a word corresponding to the ground truth category label of the image. For instance, if the ground truth label of an image is 'dog', we do not want the model to generate a perturbation that changes the word 'dog' to 'cat'. To address this, we simply filter our caption edits wherein the edit is semantically similar (measured using a sentence BERT model ) to the ground truth category. Similarly, we also filter our captions where the edit is semantically similar to the original word or phrase being modified.

### Counterfactual Image Generation

Having generated image captions and their perturbations, we proceed to generate counterfactual images conditioned on the original image and edited caption using a text-to-image latent diffusion model, specifically Stable Diffusion . However, despite its remarkable ability at generating high-quality text-conditioned images, targeted editing using such models has historically been challenging, as changing even a single word in the text prompt could dramatically change the generated output image. Naturally, this is undesirable for our task, as we want to generate counterfactual images that are as similar as possible to the original image, while still reflecting the caption edit.

To address this, we leverage the recently proposed prompt-to-prompt 1 image editing technique, which performs targeted injection of cross-attention maps that correspond to the caption edit for a subset of the denoising diffusion process. However, prompt-to-prompt is designed for generated images, and applying it to real images requires accurate image inversion to latent space.

We employ the recent null-text inversion technique proposed by Mokady _et al._. Concretely, for input image \(\) with caption \(c\), we perform DDIM inversion by setting the initial latent vector \(z_{0}\) to the encoding of the image to be inverted \(\), and run the diffusion process in the reverse direction  for \(K\) timesteps \(z_{0} z_{K}\). Further, latent diffusion models employ classifier-free  guidance for text-conditioned generation, wherein the diffusion process is run twice, with text conditioning and unconditionally using a null-text token. To encourage accurate reconstruction of the original image, we follow Mokady _et al._ to use the initial noisy diffusion trajectory (generated with a small guidance scale) as a pivot and update \(_{k}\), the null-text embedding at timestep \(k\), to minimize a reconstruction mean square error between the predicted latent code \(_{k}\) and the pivot \(z_{k}\), using the default large guidance scale recommended for classifier-free guidance. This helps bring the backward diffusion trajectory close to the original image encoding \(z_{0}\), and thus achieve faithful reconstruction. Let \(S_{k-1}(_{k},_{k},c)\) denote one step of deterministic DDIM sampling . We optimize:

\[_{_{k}}\|z_{k-1}-S_{k-1}(_{k},_{k},c) \|_{2}^{2}\] (1)

**Image editing: Checks and balances.** We find that image editing using prompt-to-prompt with null-text inversion is highly sensitive to a specific hyperparameter which controls the fraction of diffusion steps for which self-attention maps for the original image are injected. Let \(f\) denote this fraction. The optimal value of \(f\) varies according to the degree of change, with larger changes (say editing the background or weather) requiring a small value. In service of making our algorithm fully automated, we follow prior work  to automatically tune this hyperparameter: we sweep over a range of values and threshold based on a CLIP  directional similarity metric , which measures the consistency in the change across images and captions in embedding space. Let \(E_{I}\) and \(E_{T}\) denote the CLIP image and text encoders. The CLIP directional similarity criterion \((.)\) is given by:

\[(,^{},c,c^{})=1-()- E_{I}(^{}))(E_{T}(c)-E_{T}(c^{}))}{|E_{I}( )-E_{I}(^{})||E_{T}(c)-E_{T}(c^{})|}\] (2)Finally, we ensure that the generated image is more similar to the edited rather than the original caption in CLIP's  embedding space. Algo. 1 details our full approach.

## 4 Experiments

In Section 4.1, we overview our experimental setup, describing the data, metrics, baselines, and implementation details used. Next, we present our results (Section 4.2), comparing the performance of a diverse set of pretrained models on the subset of the ImageNet test set, and on our generated counterfactual test sets. Additionally, we analyze the sensitivity of models to different types of edits and demonstrate the applicability of our method in deriving class-level insights into model bias.

### Experimental Setup

**Dataset.** We evaluate LANCE on a subset of the ImageNet  validation set. Specifically, we study the 15 classes included in the Hard ImageNet benchmark . Models trained on ImageNet have been shown to rely heavily on spurious features (_e.g._ context) to make predictions for these classes, which makes it an ideal testbed for our approach. We consider the original ImageNet validation sets for these 15 classes, with 50 images/class, as our base set. Further, testing our approach on ImageNet has a few other advantages: i) The dataset contains naturally occurring spurious correlations rather than manually generated ones (_e.g._ by introducing a dataset imbalance). ii) Testing our method on ImageNet images rather than constrained settings used in prior work _e.g._ celebrity faces in CelebA allows us to validate its effectiveness in more practical settings. iii) Finally, using ImageNet allows us to stress-test a wide range of pretrained models.

**Metrics.** We report the model's accuracy@k (\(k\{1,5\}\)) over the original test set \(\) and generated counterfactual test set \(^{}\). For model \(\), we are interested in understanding the _drop_ in accuracy@k over the counterfactual test set, compared to the original test set. We define this metric as follows:

\[=^{}|}_{(^{ },y)^{}}((^{ }),y)-|}_{(,y)} ((),y)\] (3)

However, it is possible for an intervention to alter model confidence without leading to a change in its final prediction. As a more fine-grained measure, we also report the absolute difference in predicted model confidence for the ground truth class over the original and counterfactual images. For a single instance, we define this as: \( p(y_{GT}|)=|p(y_{GT}|)-p(y_{GT}|^{ })|\)

Finally, to evaluate realism we also report the FID  score of our generated image test sets and perplexity of the generated and perturbed captions under LLAMA-7B .

[MISSING_PAGE_FAIL:7]

a photo to a sculpture) significant predictive changes result - from correctly identifying a howler monkey to incorrectly predicting a chimpanzee. As another example, the subject edit (changing the dogs from husky to schnauzer) prohibits the model from accurately predicting dogsfed, conceivably because few schnauzers appeared pulling a sled in the training data.

**Perturbation sensitivity varies by perturbation type and model.** Now that we have confirmed that our generated test set provides challenging examples to assess our models, we may further analyze the impact of different types of perturbations. Figure 3 reports the relative change in predictive performance in response to different perturbation types. We find that changes to the background have the highest impact, followed by domain. Further, some models are more sensitive to LANCE images, with ResNet-50  having the most sensitivity.

**LANCE can be used to derive class-level insights into model bias.** To do so, we compute the L1 distance between CLIP features for the original and edited words, and run K-Means clustering. We then visualize the clusters with the highest \( p(y_{GT}|)\), corresponding to high sensitivity to a given change. Figure 6 illustrates examples. As seen, our method can also be used to surface class-level failure modes to inform downstream mitigation strategies. Importantly, we stress that while some failure modes are corroborated by other diagnostic datasets , (_e.g._using color-based context-cues to predict "howler monkey"), others go beyond the capabilities of conventional IID test set-based diagnosis (_e.g._models relying on dog breed to predict "dog sled"; see Figs. 1, 11).

**Are generated images and perturbed captions realistic?** As we are _generating_ a new test set, it is important to verify that any performance changes are not due to an impact on the realism of our images. In Table 4 we report the FID scores of our generated images and the perplexity scores of our generated captions and observe both to be within a reasonable range. Further, we reiterate that using our pipeline to reconstruct the original image results in a test set where each model measures near identical performance (Table 2).

Figure 5: **Visualizing counterfactual images**. We visualize the counterfactual images generated by LANCE for images from the HardImageNet dataset. Each row corresponds to a specific edit type. Above each image, we display its generated caption, highlighting the original and edited word(s). Below each image, we display the top-5 classes predicted by a ResNet-50 model and the associated model confidence, coloring the ground truth class in green.

**LANCE's checks & balances are highly effective.** Next, we evaluate the accuracy of the caption perturber at making the type of edit specified by the prompt (_e.g._correctly changing only the domain for a domain edit). To do so, we manually validate a random subset of 500 captions and their perturbations (100 per type) generated by the LANCE. We observe an overall accuracy of 89%, with the object (84%) and adjective (82%) types achieving the lowest accuracies: we find that most failures for these types result from incorrect identification of the object or adjective in the sentence. We note that even in failure cases (_e.g._changing an irrelevant word), the edits made are still reasonable.

Next, we evaluate the efficacy of our caption editing checks and balances. We first label the same subset of 500 examples by hand with the correct action (filter / no filter) and compare against LANCE. Across perturbation types, LANCE achieves both high precision and recall, with a low overall false positive (2.8%) and false negative (1.6%) rate. Of these, a majority of false negatives (edits that alter the ground truth that we fail to catch) are for the subject and object types: we find these typically

 
**Metric** & **Subject (\%)** & **Object (\%)** & **Adjective (\%)** & **Domain (\%)** & **Background (\%)** & **Overall (\%)** \\ 
**Edit Success (\%)** & 92 & 84 & 82 & 98 & 92 & 89 \\
**Filter prec. (\%) / recall (\%)** & 99.0/97.0 & 100/96.0 & 92.9/98.9 & 100/100 & 94.1/100 & 97.2/98.4 \\  

Table 3: **Evaluating caption perturber.** LANCE performs successful edits without mutating labels.

Figure 6: **Deriving class-level insights.** LANCE can be used to derive model bias at a per-class level by clustering text edits and visualizing clusters with the highest average \( p(y_{GT}|)\). Above we highlight insights for three classes with a ResNet-50 model: i) “howler monkey”: high sensitivity to the background color ii) “sunglasses”: high sensitivity to the data domain, and iii) seat belt: high sensitivity to the (barely perceptible) vehicle type.

change the ground truth _inadvertently_ _e.g._ for a "keyboard space bar" image of a typewriter, the edit typewriter\(\) painting erroneously passes our filter (paintings do not usually have space bars).

**Human evaluators validate the realism and efficacy of LANCE-generated edits.** We conduct a human evaluation along 5 axes: i) Image Realism (1-5, 5=best): How easy is it to tell that the counterfactual image was generated by AI? ii) Edit success (1-5, 5=best): Does the generated image correctly incorporate the edit? iii) Image fidelity (1-5, 5=best): Are all the changes made relevant to accomplishing the edit? iv) Label consistency (Yes/No): Is the original image label still plausible for the generated image?, and v) Ethical issues (Text input): Is the generated image objectionable, or raise ethical concerns around consent, privacy, stereotypes, demographics, etc.?

We collect responses from 15 external respondents for a random subset of 50 <image, ground truth, generated caption, perturbed caption, counterfactual image> tuples (10 per-perturbation type), and report mean and variance. We include a screenshot of our study interface in Fig. 13, and report results in Table 4. As seen, images generated by LANCE are rated to be high on realism (4.2/5 on average) and fidelity (4.5/5 on average), and slightly lower on edit quality (3.7/5 on average). Of these, we find that background edits score lowest on edit success, due to sometimes altering the wrong region of the image. Further, generated images have high label consistency (95%), verifying the efficacy of LANCE's checks and balances. Finally, \(\)2% of images are found to raise ethical concerns, which we further analyze below.

**Ethical Issues.** On inspection of the images marked as objectionable in our human study, we find that most arise from bias encoded in the generative model: _e.g._for the edit woman\(\)lifeguard, the generative model also alters the perceived gender and makes the individual muscular. Further, for people\(\)athletes, the model also alters the person's perceived race. The model also sometimes imbues stereotypical definitions of subjective characteristics such as "stylish" (_e.g._by adding makeup). To address this, we manually inspect the entire dataset of 781 images for similar issues and exclude 8 additional images. Going forward, we hope to ameliorate this by excluding subjective text edits and using text-to-image models with improved fairness  and steerability .

**Failure modes and limitations.** In Fig 12, we visualize LANCE's 5 most frequent failure modes: a) Label-incompatible text edit: _e.g._black howler monkey\(\)purple howler monkey (howler monkeys are almost always black or red). b) Poor quality image edit: Images with low realism, which are largely eliminated via our checks and balances. c) Image edit that inadvertently changes the ground truth: _e.g._typewriter\(\)stapler for the class "space bar" (staplers do not contain space bars). d) Inherited bias: _e.g._changing perceived gender based on stereotypes. e) Meaningless caption edit: _e.g._boy in back-seat driver in the back seat. Such failures can confound an observed performance drop but this is largely mitigated by their low overall incidence.

LANCE also has a few intrinsic limitations. Firstly, it leverages several large-scale pretrained models, each of which may possess its own inherent biases and failure modes, which is highly challenging to control for. Secondly, intervening on language may exclude more abstract perturbations that are not easily expressible in words. However, we envision that LANCE will directly benefit from the rapid ongoing progress in the generative capabilities  and steerability  of image and text foundation models, paving the road towards robust stress-testing in high-stakes AI applications.

**Acknowledgements.** This work was supported in part by funding from NSF #2144194, Cisco, Google, and DARPA LwLL. We thank Simar Kareer for help with figures, members of the Hoffman Lab for project feedback, and the volunteers who participated in our human study.

  
**Metric** & **Subject** & **Object** & **Adjective** & **Domain** & **Background** & **Overall** \\ 
**Image Realism (1-5)** & 4.2\(\)0.5 & 4.4\(\)0.5 & 4.1\(\)0.4 & 3.9\(\)0.8 & 4.2\(\)0.6 & 4.2\(\)0.5 \\
**Edit Success (1-5)** & 3.8\(\)0.4 & 3.6\(\)0.5 & 4.1\(\)0.5 & 3.7\(\)0.5 & 3.3\(\)0.4 & 3.7\(\)0.4 \\
**Image Fidelity (1-5)** & 4.7\(\)0.4 & 4.5\(\)0.5 & 4.6\(\)0.5 & 4.7\(\)0.2 & 4.4\(\)0.5 & 4.5\(\)0.4 \\ 
**Label Consistency (\%)** & 97.0\(\)4.0 & 94.0\(\)6.0 & 99.0\(\)0.0 & 93.0\(\)9.0 & 93.0\(\)9.0 & 95.0\(\)4.0 \\ 
**Ethical issues (\%)** & 3.0\(\)7.0 & 1.0\(\)3.0 & 5.0\(\)8.0 & 0\(\)0 & 0\(\)0 & 2.0\(\)2.6 \\   

Table 4: **Human study.** LANCE-generated images score high on realism, edit success, and fidelity.