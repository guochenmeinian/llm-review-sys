# Benchmarking LLMs via Uncertainty Quantification

Fanghua Ye\({}^{1,2}\) Mingming Yang\({}^{1}\) Jianhui Pang\({}^{1,3}\) Longyue Wang\({}^{1,}\)

**Derek F. Wong\({}^{3}\) Emine Yilmaz\({}^{2}\) Shuming Shi\({}^{1}\) Zhaopeng Tu\({}^{1}\) \({}^{1}\)**Tencent AI Lab \({}^{2}\)University College London \({}^{3}\)University of Macau

fanghua.ye.19@ucl.ac.uk, nlp2ct.pangjh3@gmail.com

derekfw@um.edu.mo, emine.yilmaz@ucl.ac.uk

{shanemmyang, vinnylywang, shumingshi, zptu}@tencent.com

Corresponding author.

###### Abstract

The proliferation of open-source Large Language Models (LLMs) from various institutions has highlighted the urgent need for comprehensive evaluation methods. However, current evaluation platforms, such as the widely recognized HuggingFace open LLM leaderboard, neglect a crucial aspect - **uncertainty**, which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce a new benchmarking approach for LLMs that integrates uncertainty quantification. Our examination involves nine LLMs (LLM series) spanning five representative natural language processing tasks. Our findings reveal that: I) _LLMs with higher accuracy may exhibit lower certainty_; II) _Larger-scale LLMs may display greater uncertainty compared to their smaller counterparts_; and III) _Instruction-finetuning tends to increase the uncertainty of LLMs_. These results underscore the significance of incorporating uncertainty into the evaluation of LLMs. Our implementation is available at https://github.com/smartyfh/LLM-Uncertainty-Bench.

## 1 Introduction

Large Language Models (LLMs) have gained significant traction within both academia and industry, with numerous organizations and companies open-sourcing their versions of LLMs [9; 74; 36; 68]. LLMs are highly versatile, demonstrating capabilities in various tasks such as question answering, document summarization, dialogue systems, and machine translation [70; 52]. Given the growing interest and advancements in LLMs, it is crucial to establish appropriate methods for evaluating their performance [42; 71; 9]. However, conducting a comprehensive evaluation of LLMs remains a challenging endeavor [28; 75].

To address this challenge, several open leaderboards such as the popular HuggingFace open LLM leaderboard,2 OpenCompass , Chatbot Arena , and FlagEval  have emerged, providing a comparative analysis of LLM performance. Despite their usefulness, these leaderboards possess a significant limitation: _They do not take into account the uncertainty of LLMs_. For example, the HuggingFace open LLM leaderboard only utilizes accuracy as the evaluation metric. However, as demonstrated in Figure 1, two LLMs may achieve identical accuracy scores but exhibit different levels of uncertainty regarding the question. This is analogous to students taking exams of multiple-choice questions, where two students may select the same answer but actually possess distinct degrees of uncertainty or comprehension about the question. Consequently, it is necessary to incorporate uncertainty into the evaluation process to achieve a more comprehensive assessment of LLMs.

In this paper, we propose the utilization of conformal prediction [64; 5] as the method to quantify uncertainty in LLMs. Compared to alternative methods such as Bayesian variational inference ,conformal prediction offers multiple advantages including ease of implementation, high efficiency, distribution-free and model-agnostic, and a statistically **rigorous** estimation of uncertainty rather than a heuristic approximation . Hence, conformal prediction can serve as a practical and principled means for assessing the uncertainty of LLMs.

Specifically, we benchmark nine open-source LLMs (LLM series) across five typical Natural Language Processing (NLP) tasks, namely question answering, reading comprehension, commonsense inference, dialogue response selection, and document summarization. Given that most existing open leaderboards and benchmarking datasets  focus on multiple-choice tasks, we also adopt the multiple-choice question setting for all tasks. Although some of these tasks (e.g., document summarization) are inherently generative, it is challenging to develop a deterministic and reproducible method for quantifying the uncertainty within the generated text due to randomness in the generation process. Instead, we convert all tasks into multiple-choice questions, with the objective of each task being to select the correct option from the provided choices. Our empirical results reveal the following observations: I) _LLMs demonstrating higher accuracy may exhibit lower certainty_; II) _LLMs with larger scales may display greater uncertainty than their smaller counterparts_; and III) _LLMs after instruction-finetuning tend to possess higher uncertainty_.

## 2 Related work

Uncertainty QuantificationUncertainty quantification [22; 2; 25] has been an active area of research in both machine learning and NLP due to its importance in real-world applications such as decision making, risk assessment, human-AI collaboration, and so on. Typical uncertainty quantification methods include confidence-based methods , Bayesian methods , and ensemble methods . Confidence-based methods such as entropy can be sensitive to poor calibration and may not fully capture models' underlying uncertainties . Bayesian methods and ensemble methods usually suffer from high computational complexity , making them not suitable for assessing the uncertainty of LLMs.

Conformal PredictionRecently, there has been a growing interest in applying conformal prediction for uncertainty quantification [5; 38; 53; 46]. For example, conformal prediction has been applied to part-of-speech prediction , paraphrase detection , and fact verification . Similar to the process of elimination used by students during exams, conformal prediction identifies a subset of potential labels in classification tasks by excluding improbable labels, which is statistically guaranteed to contain the true label, and quantifies uncertainty as the size of this subset . The coverage guarantee property makes conformal prediction a highly robust uncertainty quantification method. In addition, conformal prediction is non-parametric, distribution-free (i.e. _not dependent on any specific distributional assumptions about the data_), model-agnostic, and computationally efficient . Therefore, it is a favorable choice in the context of LLMs.

LLM EvaluationEvaluating the performance of LLMs is a crucial aspect of their development and deployment . Current studies assess LLMs from different angles using specific datasets, such as MMLU  for knowledge, HellaSwag  for reasoning, HaluEval  for hallucination, GSM8K  for math, and BOLD  for fairness. Besides, evaluation platforms like HuggingFace open LLM leaderboard and Chatbot Arena  have also been developed to facilitate comparisons among LLMs. Despite these efforts, the critical aspect of uncertainty in LLMs remains underexplored. More recently, some research has begun to consider uncertainty in LLMs [67; 69; 43; 10; 65; 20]. However, these approaches such as the sampling-based semantic entropy  are heuristic and lack

Figure 1: An illustration of two LLMs accurately predicting the true answer (with option A possessing the highest probability), but showing different levels of uncertainty. Note that when both LLMs predict a wrong answer, they may also display different levels of uncertainty.

a standardized methodology for benchmarking purposes. In contrast, our utilization of conformal prediction can provide a robust and systematic evaluation of uncertainty.

## 3 Background of conformal prediction

Conformal prediction serves as a **distribution-free** and **model-agnostic** approach to uncertainty quantification [64; 8; 5; 23]. It can transform any heuristic notion of uncertainty from any model into a statistically **rigorous** one. As aforementioned, for multi-class classification tasks, conformal prediction outputs a prediction set of possible labels (answers) that encompasses the correct label with a user-specified error rate and expresses uncertainty as the set size. Intuitively, a larger set size indicates higher uncertainty and vice versa.

Formally, let \(f\) be a model that classifies an input \(X\) into \(K\) pre-defined classes, represented by \(=\{1,,K\}\). To measure the uncertainty of \(f\), for any given test instance \(X_{t}\) and its corresponding true label \(Y_{t}\), conformal prediction produces a prediction set of labels \((X_{t})\) such that

\[p(Y_{t}(X_{t})) 1-,\] (1)

where \((0,1)\) is a user-specified error rate.

Equation (1) requires that the generated prediction set should contain the true label \(Y_{t}\) with a probability no smaller than \(1-\). This coverage guarantee requirement can be achieved with the aid of a small amount of held-out _calibration data_\(_{cal}=\{(X_{c}^{(1)},Y_{c}^{(1)}),,(X_{c}^{(n)},Y_{c}^{(n)})\}\), where \(n\) denotes the number of data points in the calibration set. More specifically, conformal prediction works in the following process  to create the prediction set:

1. Identify a heuristic notion of uncertainty based on the model \(f\);
2. Define a conformal score function \(s(X,Y)\) with larger scores encoding worse agreement between \(X\) and \(Y\);
3. Compute conformal scores on the calibration set \(s_{1}=s(X_{c}^{(1)},Y_{c}^{(1)}),,s_{n}=(X_{c}^{(n)},Y_{c}^{(n)})\) and calculate a threshold \(\) as the \(\) quantile of the calibration scores, \[=\{s_{1},,s_{n}\},,\] (2) where \(\) is the ceiling function;
4. Construct the prediction set for each test instance \(X_{t}\) as \[(X_{t})=\{Y^{}:s(X_{t},Y^{})\}.\] (3)

For classification tasks, it is a common choice to adopt the softmax score (i.e. estimated probability of each class by the model) as the _heuristic_ notion of uncertainty. However, this score usually does not reflect the true class distribution due to over-confident or under-confident model predictions. In this work, we consider two conformal score functions to convert the softmax score to a _statistically rigorous_ notion of uncertainty (which is calibrated in the sense that the prediction sets satisfy the coverage guarantee requirement).

Least Ambiguous set-valued Classifiers (LAC)LAC  defines the conformal score function as

\[s(X,Y)=1-f(X)_{Y},\] (4)

where \(f(X)_{Y}\) is the softmax score corresponding to the true label. It has been proven that LAC can lead to prediction sets with the smallest average size . However, it may undercover hard instances and overcover easy ones.

Adaptive Prediction Sets (APS)APS  defines the conformal score function as

\[s(X,Y)=_{\{Y^{}:f(x)_{Y^{}} f(x)_{Y}\}}f(X) _{Y^{}},\] (5)

where \(f(X)_{Y^{}}\) represents the softmax score corresponding to the label \(Y^{}\). Equation (5) is equivalent to summing the ranked scores of each label, from the higher to the lower, until reaching the true label. Compared to LAC, APS leverages the softmax scores of all labels, not just the true label. It addresses the limitation of LAC but suffers from, on average, larger prediction sets.

The overall process of employing conformal prediction for uncertainty quantification in LLMs is illustrated in Figure 2. In the following sections, we first elucidate on the evaluation tasks and their associated datasets, then provide details about the evaluation prompts used to extract softmax scores (i.e. predicted probabilities) from LLMs, and finally, introduce the adopted evaluation metrics.

## 4 Evaluation tasks and datasets

LLMs have demonstrated remarkable capabilities across various aspects [28; 50]. It is essential to develop multiple tasks to evaluate their performance comprehensively. For this purpose, we consider five typical NLP tasks, including question answering, reading comprehension, commonsense inference, dialogue response selection, and document summarization. For each task, we prepare a dataset with 10,000 instances. In addition, we formulate each task as a Multiple-Choice Question Answering (MCQA) task and the objective is to select the _only_ correct answer out of six possible options (i.e. A, B, C, D, E, and F). It is worth emphasizing that the prevailing benchmarking open leaderboards and datasets also focus on MCQA tasks [28; 45].

Question Answering (QA)QA is applied to evaluate an LLM's proficiency in utilizing its extensive world knowledge to provide answers to a diverse range of questions. For this task, we adopt **MMLU** as the evaluation dataset. MMLU encompasses a total of 57 subjects, spanning various disciplines such as elementary mathematics, US history, computer science, and law. These subjects are further classified into four broad categories, namely humanities, social sciences, STEM, and others (business, health, misc.). For each category, we sample 2500 instances, leading to 10,000 instances in total.

Reading Comprehension (RC)RC is used for testing an LLM's ability to understand and analyze a given context, comprehend the meaning of words and sentences, and answer questions based

Figure 2: The overall process of applying conformal prediction for uncertainty quantification in LLMs. (a) Five distinct tasks are considered, and a dataset comprising 10,000 instances is prepared for each task. (b) Each data instance is transformed into a multiple-choice question, and nine LLMs (LLM series) are prompted to generate predicted probabilities for the given options. (c) Each dataset is divided into a calibration set and a test set, followed by the application of conformal prediction to generate prediction sets for test set instances. For illustrative purposes, demonstrations in the prompt input are excluded, and solely the process of constructing prediction sets utilizing the LAC conformal score function is demonstrated. In addition, only four options of the question are presented.

on the information presented in the context. It also tests the ability of LLMs to make inferences and draw conclusions from the given context. We take **CosmosQA** as the evaluation dataset. CosmosQA focuses on _reading between the lines_ over a diverse collection of people's everyday narratives that require reasoning beyond the exact text spans in the context. Due to the unavailability of ground truth labels for the test set in CosmosQA, we sample 10,000 instances from the training and development sets.

Commonsense Inference (CI)CI is leveraged to evaluate the ability of LLMs to understand and reason about the relationships between concepts and events based on commonsense and background knowledge. This type of testing helps to assess the generalization and reasoning abilities of LLMs beyond simple pattern recognition tasks. We employ **HellaSwag** as the evaluation dataset. HellaSwag focuses on _commonsense natural language inference_ whose target is to select the most likely followup to a given event description. Same as CosmosQA, we sample 10,000 instances from the training and development sets of HellaSwag for the purpose of evaluation.

Dialogue Response Selection (DRS)DRS is adopted for assessing the ability of LLMs to comprehend the meaning of a given dialogue and select an appropriate response from a set of possible responses. This includes the ability to comprehend the meaning of the user's input, select appropriate responses that are relevant to the conversational context, and maintain coherence and consistency in the dialogue. We utilize the dialogue data from the HaluEval  benchmark as the evaluation dataset (denoted as **HaluDial**). This dataset consists of exactly 10,000 instances and is built upon OpenDialKG , a knowledge-grounded dialogue dataset.

Document Summarization (DS)DS is taken to evaluate the proficiency of LLMs in comprehending the substance and context of a given document, and in producing a succinct and cohesive summary that effectively conveys the crucial information and main ideas of the document. This requires the LLM to have a good understanding of the language, the topic, and the structure of the document. Similar to DRS, we adopt the summarization data from the HaluEval  benchmark as the evaluation dataset (denoted as **HaluSum**). This dataset also comprises precisely 10,000 instances and is derived from CNN/Daily Mail , a summarization dataset pertaining to news articles.

Out of the aforementioned five datasets, MMLU, CosmosQA, and HellaSwag originally consisted of questions with four options each, while HaluDial and HaluSum had only two options per question. To standardize the number of options, two additional choices were added to each question in HaluDial and HaluSum by randomly selecting from the choices of other questions in HaluDial and HaluSum, respectively. Furthermore, all datasets were modified to include two more options, _"I don't know"_ and _"None of the above"_, resulting in a total of six possible options for each question.

## 5 Evaluation prompts and metrics

As delineated in SS 3, one crucial step of leveraging conformal prediction for uncertainty quantification is to acquire the softmax score corresponding to each option. Next, we explicate the method used to elicit these scores from LLMs, followed by a description of the evaluation metrics utilized.

Prompting StrategiesFollowing previous works [29; 24; 73; 12; 75], we rely on prompt engineering rather than supervised finetuning as the testing approach to evaluating the performance of LLMs on each task. However, our preliminary results show that LLMs are sensitive to prompts. In this regard, we consider three prompting strategies, including _Base Prompt_, _Shared Instruction Prompt_, and _Task-specific Instruction Prompt_, to reduce the influence of LLMs' sensitivity to different prompts, thereby ensuring a fairer comparison.

* **Base Prompt:** This strategy directly combines the question and all of its options as the input and prompts the LLM to output the correct option with a prefix "Answer:".
* **Shared Instruction Prompt:** This strategy adds a general description of the task before the question, informing the LLM that the task is to solve a multiple-choice question and there is only one correct answer out of six options.
* **Task-specific Instruction Prompt:** Instead of using a shared instruction, this strategy provides a task-specific instruction that briefly describes the task and the expected type of option.

These prompting strategies facilitate a systematic and standardized evaluation of the performance of LLMs. The prompt templates linked with each strategy are elaborated in Appendix D. The softmax score for each prompt is derived by subjecting the logits corresponding to each option letter (i.e. A, B, C, D, E, and F) to the softmax function. The said logits are generated by the language modeling head in contemporary causal LLMs. It is worth noting that only the logits associated with the last token of the prompt input are utilized.

Evaluation MetricsWe evaluate LLMs from two perspectives, namely prediction accuracy and prediction uncertainty. For prediction accuracy, we adopt the commonly used metric - Accuracy (**Acc**). To evaluate prediction uncertainty, we use Set Size (**SS**), which is a primary metric for conformal prediction . Let \(Y_{p}\) be the prediction for the test instance \((X_{t},Y_{t})_{test}\). These two metrics can be calculated as follows:

\[Acc=_{test}|}_{(X_{t},Y_{t})_{test}} (Y_{p}=Y_{t}), SS=_{test}|}_{(X_{t},Y _{t})_{test}}|(X_{t})|,\] (6)

where \(()\) is the indicator function.

In addition to Acc and SS, we report the Coverage Rate (**CR**) to verify if the coverage guarantee requirement shown in Eq. (1) has been satisfied. The CR metric is calculated as

\[CR=_{test}|}_{(X_{t},Y_{t})_{test}} (Y_{t}(X_{t})).\] (7)

## 6 Evaluation results

### Setup

In our experiments, we set the error rate \(\) to 0.1, implying that the prediction set should include the true label with a probability of at least 0.9. In order to better excite the ability of LLMs, we incorporate examples or demonstrations in the prompt, adhering to the in-context learning paradigm . Specifically, we provide five demonstrations for QA, RC, and CI tasks. For the DRS task, we utilize three demonstrations, while we use only one demonstration for the DS task due to constraints on input length and inference cost. The maximum input length for all tasks is set to 2048 tokens. In addition, for each task, we allocate 50% of the data as the calibration set and the remaining 50% as the test set. We report results on the test set. These results represent the average value obtained from the two conformal score functions, namely, LAC and APS, as well as the three prompting strategies. It is noteworthy that while both LAC and APS fulfill the coverage guarantee requirement, they may produce prediction sets of varying sizes. By taking the average value of these two score functions, we aim to mitigate the influence of different score functions on the evaluation of uncertainty, thereby ensuring a more rigorous and reliable assessment.

### Evaluated models

We select a diverse set of nine representative models (or model series) from the vast array of open-source LLMs available. These models encompass various architectures and training methodologies, thereby allowing for a comprehensive benchmarking analysis. Specifically, the chosen models include the Llama-2 series , Mistral-7B , Falcon series4, MPT-7B , Gemma-7B , Owen series , Yi series , DeepSeek series , and InternLM-7B . For all models, we utilize their checkpoints from the HuggingFace platform: https://huggingface.co/models.

### Main findings

In our primary experiments, we focus on LLMs with sizes ranging from 6B to 14B parameters. The outcomes of CR, Acc and SS are presented in Table 1. As previously mentioned, the reported results are the mean value derived from the two conformal score functions, LAC and APS. For a detailed analysis of the results pertaining to each function, please refer to Appendix C.1.

From Table 1, it is evident that in the majority of cases, the coverage rate is at least 90%, indicating that the coverage guarantee requirement has been met. Although there are cases where the coverage rate falls below 90%,5 the values are still in close proximity to the 90% threshold. The lowest coverage rate is attained by Qwen-7B on the DS task, with a value of 89.56%. Moreover, all models achieve an average coverage rate exceeding 90% across the five tasks. These findings suggest that the generated prediction sets are meaningful, as they can cover the true label with a high probability. Therefore, the size of the prediction set can serve as a reliable indicator of uncertainty.

In principle, an LLM having higher accuracy is expected to demonstrate lower uncertainty. However, as shown in Table 1, the results regarding the SS metric reveal that in practice, higher accuracy does not necessarily correlate with lower uncertainty. Concretely, for each task, we observe that the ranking of LLMs based on accuracy differs from that based on uncertainty, suggesting that some LLMs possessing higher accuracy actually display higher uncertainty. Notably, _two LLMs with a significant difference in accuracy may even display inverse uncertainty_. For example, on the DRS task, InterLM-7B demonstrates higher performance than MPT-7B in accuracy by 19.34 absolute points, yet it shows higher uncertainty. This pattern is also observed on the QA task with Qwen-7B and Llama-2-7B (9.61), the CI task with Qwen-14B and Yi-6B (14.50), and the DS task with InterLM-7B and Falcon-7B (9.69). In each case, the LLM with much higher accuracy exhibits greater uncertainty compared to its counterpart. These observations underscore the importance of considering uncertainty in addition to accuracy when evaluating LLMs.

### Effects of model scale

LLMs with larger sizes are usually pretrained on more data and tend to exhibit superior capabilities across various tasks. In this study, we aim to investigate how an LLM's performance changes when scaling its model size. We present the results of the Qwen model series in Figure 3. It is evident that in the majority of cases, the coverage guarantee requirement has been satisfied. In terms of Acc, with the exception of Qwen-72B and Qwen-14B on the CI task, an increase in model size

  
**LLMs** & **QA** & **RC** & **CI** & **DRS** & **DS** & **Avg.** \\   \\  Qwen-14B & 92.58 & 95.20 & 95.29 & 91.92 & 89.71 & 92.94 \\ Yi-6B & 91.30 & 94.06 & 92.35 & 91.36 & 91.38 & 92.09 \\ Gemma-7B & 93.57 & 94.16 & 92.13 & 91.59 & 90.70 & 92.43 \\ Mistral-7B & 93.00 & 92.91 & 89.94 & 91.02 & 92.05 & 91.78 \\ Lima-2-13B & 92.59 & 93.15 & 90.50 & 90.92 & 90.82 & 91.60 \\ Qwen-7B & 92.97 & 94.02 & 91.53 & 92.43 & 89.56 & 92.07 \\ IntemLM-7B & 90.68 & 93.28 & 90.10 & 90.40 & 90.34 & 90.96 \\ Llama-2-7B & 91.37 & 90.69 & 90.97 & 89.60 & 90.04 & 90.53 \\ DeepSec-7B & 91.18 & 89.95 & 90.16 & 90.89 & 90.21 & 90.48 \\ MPT-7B & 89.79 & 90.54 & 90.12 & 90.80 & 89.71 & 90.19 \\ Falcon-7B & 90.04 & 89.95 & 89.82 & 90.46 & 90.71 & 90.19 \\   \\  Qwen-14B & 64.28\% & 91.52\% & 91.00\% & 73.90\% & 49.33\% & 74.00\% \\ Yi-6B & 57.70\% & 85.90\% & 76.50\% & 58.72\% & **66.06\%** & 68.97\% \\ Gemma-7B & 62.24\% & 85.29\% & 73.58\% & 66.79\% & 40.80\% & 65.74\% \\ Mistral-7B & 60.44\% & 81.94\% & 62.93\% & 53.21\% & 62.10\% & 64.14\% \\ Lima-2-13B & 52.52\% & 77.23\% & 59.65\% & 52.65\% & 60.05\% & 60.42\% \\ Qwen-7B & 55.21\% & 83.89\% & 63.70\% & 64.04\% & 32.53\% & 59.87\% \\ IntemLM-7B & 48.37\% & 73.86\% & 46.21\% & 43.72\% & 34.38\% & 49.31\% \\ Lima-2-7B & 45.60\% & 65.79\% & 43.05\% & 32.61\% & 45.60\% & 46.53\% \\ DeepSec-7B & 45.60\% & 65.59\% & 42.66\% & 33.50\% & 42.15\% & 45.87\% \\ MPT-7B & 29.49\% & 31.69\% & 25.50\% & 42.38\% & 24.86\% & 27.18\% \\ Falcon-7B & 23.75\% & 24.98\% & 24.91\% & 25.86\% & 24.69\% & 24.84\% \\   \\  Qwen-14B & 2.80\% & 1.74\% & 2.02\% & 19.40\% & 2.37\% & 2.10\% \\ Yi-6B & 3.20\% & 1.92\% & 1.88\% & 2.85\% & 1.96\% & 2.36\% \\ Gemma-7B & 2.72\% & 1.88\% & 2.04\% & 2.14\% & 3.11\% & 2.38\% \\ Mistral-7B & 2.80\% & 1.75\% & 2.48\% & 2.71\% & 2.40\% & 2.43\% \\ Lima-2-13B & 3.06\% & 2.24\% & 2.72\% & 2.55\% & 2.24\% & 2.56\% \\ Qwen-7B & 3.26\% & 2.15\% & 2.28\%consistently leads to improved performance. Concerning uncertainty (i.e. SS), there is a general trend of decreasing uncertainty when scaling the model size from 1.8B to 14B. However, Qwen-7B displays higher uncertainty compared to Qwen-1.8B on the QA task. When further scaling the model size from 14B to 72B, the enhancements in uncertainty become less pronounced, and more variations are observed. Notably, on both the RC and DRS tasks, Qwen-72B demonstrates higher uncertainty than Qwen-14B, although Qwen-72B achieves higher accuracy.

We provide the results of the Llama-2 series, Yi series, DeepSeek series, and Falcon series in Appendix C.2, which reveal similar findings.

### Effects of instruction finetuning

In this part, we further delve into the comparative analysis of the performance between the base pretrained model and the instruction-finetuned variant of LLMs. The average results across five tasks of the Llama-2 model series are illustrated in Figure 4. For the instruction-finetuned version, two methods are adopted to prepare the prompt input. The first method aligns with the format of the instruction data6 (denoted as **Chat-V1**). This method aims to evaluate the model's proficiency in adhering to instructions to accomplish tasks. The second method employs the same prompt format as the base version (denoted as **Chat-V2**). This method aims to assess the extent of the base model's capabilities retained after instruction-finetuning.

Figure 4 shows that Chat-V1 consistently results in lower accuracy and higher uncertainty across all model sizes. Conversely, Chat-V2 enhances accuracy for Llama-2-7B and Llama-2-13B. However, for Llama-2-70B, Chat-V2 also leads to a decline in accuracy. Regarding uncertainty, Chat-V2 consistently results in increased uncertainty compared to the base model, although the extent of degradation is less severe than Chat-V1. These findings suggest that instruction-finetuning tends to impair model performance, particularly in terms of uncertainty.

We provide the results of the Yi series, DeepSeek series, and Falcon series in Appendix C.3.

Figure 4: Mean performance outcomes of the **Llama-2** series’ base pretrained model and the instruction-finetuned chat model across five tasks. Chat-V1 converts inputs into a chat format. Chat-V2 shares the same format as Base.

Figure 3: Performance comparison of different versions of the Qwen series (1.8B to 72B).

### Comparison to other uncertainty quantification methods

Given the predicted probabilities, another widely used measure of uncertainty is entropy . There have also been some entropy-based uncertainty quantification methods for language models . Here, we compare conformal prediction to entropy. Since conformal prediction uses the prediction set size (SS) to measure uncertainty, a direct comparison with entropy is not straightforward. To address this issue, we convert entropy to perplexity , which is defined as \(PPL=2^{H}\), where \(H\) denotes the entropy. Perplexity takes values in the range of \([1,||]\), allowing it to be interpreted as prediction set size. For instance, when the predicted probability for each class (option) is \(|}\), \(PPL=||\).

The results regarding InternLM-7B are presented in Table 2, from which, we observe that the coverage rate of perplexity varies significantly across different tasks. On the QA task, the coverage rate is only \(83.44\%\). In contrast, the coverage rate of conformal prediction consistently exceeds \(90\%\). This is because when measuring uncertainty, entropy doesn't take accuracy into account. Entropy remains the same when predicted probabilities are permuted, even though prediction accuracy may differ.

To further demonstrate the superiority of conformal prediction, we conduct additional experiments comparing it with entropy and maximal predicted probability in terms of the Expected Calibration Error (ECE) metric . The results corresponding to InternLM-7B are presented in Table 3. The observation that conformal prediction yields the lowest average ECE score suggests that it offers more reliable uncertainty quantification.

Overall, these results demonstrate the advantages of adopting conformal prediction for uncertainty quantification. We provide more analyses in Appendix C.7.

### Expanding benchmarking to closed-source LLMs

In this part, we extend our benchmarking from open-source LLMs to closed-source LLMs. While obtaining the exact output logits of closed-source LLMs is challenging, we can sample multiple answers and then estimate the probability of each choice. We perform an experiment on the MMLU (the QA task) dataset with GPT-3.5 and GPT-4 as the closed-source LLMs. To save cost, we only consider the base prompting strategy. Specifically, we first sample 50 answers for each question and calculate the frequency of each option. Then, we apply the softmax function with temperature scaling to prevent zero probabilities. To demonstrate the quality of this approximation, we also report the results of the open-source model Qwen-72B when getting its predictions via sampling and via logits, respectively. The results are shown in Table 4.

It is observed that GPT-4 demonstrates the highest accuracy and the lowest uncertainty. In addition, the average prediction set size (SS) of Qwen-72B (sampling) is relatively close to that of Qwen-72B (logits). For each question, we further calculate the Jensen-Shannon divergence (JSD) between the predictions of Qwen-72B (sampling) and Qwen-72B (logits). The average JSD is 0.05, indicating that

  
**LLMs** & **CR (\%)** & **Acc (\%)** & **SS** \\  GPT-4 & 90.41 & 81.75 & 1.65 \\ GPT-3.5 & 89.98 & 62.99 & 3.05 \\   Qwen-72B (sampling) & 90.54 & 70.29 & 2.43 \\ Qwen-72B (logits) & 93.34 & 73.55 & 2.33 \\   

Table 4: The evaluation results of closed-source LLMs.

    &  &  \\   & **CP** & **PPL** & **CP** & **PPL** \\  QA & 90.68 & 83.44 & 3.49 & 2.89 \\ RC & 93.28 & 95.48 & 2.19 & 2.39 \\ CI & 90.10 & 96.25 & 3.28 & 3.97 \\ DRS & 90.40 & 86.80 & 3.63 & 3.42 \\ DS & 90.34 & 87.13 & 4.47 & 4.33 \\  Avg. & 90.96 & 89.82 & 3.41 & 3.40 \\   

Table 2: Comparison between conformal prediction (**CP**) and perplexity (**PPL**) using InternLM-7B.

  
**Tasks** & **CP** & **Entropy** & \(}\) \\  QA & 15.83 & 15.83 & 15.83 \\ RC & 1.33 & 1.32 & 1.41 \\ CI & 3.16 & 3.45 & 3.75 \\ DRS & 12.11 & 12.40 & 12.45 \\ DS & 9.30 & 9.30 & 9.62 \\  Avg. & **8.35** & 8.46 & 8.61 \\   

Table 3: Comparison among conformal prediction (**CP**), entropy (**Entropy**), and maximal predicted probability (\(}\)) using InternLM-7B in terms of **ECE (\%)**.

the two predictions (estimated probability distributions) are highly similar. Therefore, we conclude that the approximation is of high quality.

### Expanding benchmarking to free-form text generation

Here, we further extend our benchmarking from multiple-choice question answering to free-form text generation. However, applying conformal prediction to text generation is a complex task due to the extensive range of potential responses. It is not feasible to compute the probability for each possible response and then use conformal prediction to select a subset. Nevertheless, many potential responses have a low probability of being generated, which allows us to reduce the selection space by sampling multiple generations.

Specifically, we adopt the TriviaQA dataset  (sampling 10,000 dev instances) for free-form text generation. We first generate 20 answers for each question. Then, we employ the perplexity  of each generation as the conformal score function and utilize exact match to verify the accuracy of the generated answer. The results are displayed in Table 5. Note that the value of SS falls into \(\).

From Table 5, we observe that the prediction set size (SS) varies among LLMs, which could provide some insights into the uncertainty of these models. However, we must note that in this sampling setting, the coverage rate cannot be guaranteed any more because there might not be a correct answer within the 20 sampled responses. In other words, even if the prediction set size is 20, indicating high model uncertainty, the coverage rate for that instance could still be zero if there are no correct answers present. Nonetheless, it is observed that when the LLM is stronger, the coverage guarantee requirement is more likely to be satisfied.

### In-depth analysis of the set size metric in relation to prediction accuracy

While our main focus is on the high probability of the prediction set covering the ground truth, it is also insightful to explore the relationship between stratified set size and prediction accuracy. In our experiments, we employ InternLM-7B on the QA task, grouping instances by their predicted set size and reporting the accuracy within each group in Table 6. The results reveal that instances with smaller set sizes are generally associated with higher prediction accuracy, indicating that set size serves as a useful indicator of prediction uncertainty. However, it is important to note that even when the set size reaches its maximum value, there are still instances where the prediction is accurate. Consequently, a comprehensive analysis of both prediction accuracy and prediction uncertainty is essential for a thorough assessment of the performance of LLMs.

## 7 Conclusion

In this work, we have provided an extensive examination of the performance of LLMs by focusing on prediction uncertainty. To achieve this, we have employed conformal prediction for uncertainty quantification. Our comprehensive investigation, which involves nine open-source LLMs (or LLM series) and spans five typical NLP tasks, demonstrates that relying solely on accuracy for benchmarking LLMs is insufficient. Instead, it is imperative to take uncertainty into account when assessing their overall performance. Last but not least, we have verified the superiority of conformal prediction compared to several other uncertainty quantification methods. We have also extended our analyses to closed-source LLMs and free-form text generation.

  
**SS** & **LAC** & **APS** & **Avg.** \\ 
1 & 80.39 & 92.69 & 86.54 \\
2 & 59.77 & 82.21 & 70.99 \\
3 & 40.55 & 63.70 & 52.12 \\
4 & 40.07 & 41.71 & 40.89 \\
5 & 31.50 & 34.92 & 33.21 \\
6 & 13.43 & None & 13.42 \\   

Table 6: The relationship between stratified prediction set size (SS) and prediction accuracy (Acc).

  
**LLMs** & **CR (\%)** & **Acc (\%)** & **SS** \\  Qwen-72B & 88.92 & 76.45 & 2.63 \\ Llama-2-13B & 83.89 & 71.83 & 2.40 \\ Qwen-14B & 82.79 & 66.57 & 3.83 \\ Llama-2-7B & 78.83 & 64.91 & 3.06 \\ Qwen-7B & 77.89 & 59.44 & 5.02 \\ DeepSeek-7B & 78.41 & 57.52 & 6.12 \\ Falcon-7B & 76.51 & 55.74 & 6.27 \\   

Table 5: The evaluation results of free-form text generation on TriviaQA.