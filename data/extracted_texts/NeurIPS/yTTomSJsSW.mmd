# Aligning Large Language Models with Representation Editing: A Control Perspective

Lingkai Kong\({}^{1}\)

Haorui Wang\({}^{1}\)

Equal contribution. Correspondence to Lingkai Kong <lingkaikong@g.harvard.edu>, Haorui Wang <hwang984@gatech.edu>, and Wenhao Mu <muwenhao@umich.edu>

Wenhao Mu\({}^{1}\)

Yuanqi Du\({}^{2}\)

Yuchen Zhuang\({}^{1}\)

Yifei Zhou\({}^{3}\)

Yue Song\({}^{4}\)

Rongzhi Zhang\({}^{1}\)

Kai Wang\({}^{1}\)

Chao Zhang\({}^{1}\)

\({}^{1}\)Georgia Tech \({}^{2}\)Cornell University \({}^{3}\)UC Berkeley \({}^{4}\)University of Trento

###### Abstract

Aligning large language models (LLMs) with human objectives is crucial for real-world applications. However, fine-tuning LLMs for alignment often suffers from unstable training and requires substantial computing resources. Test-time alignment techniques, such as prompting and guided decoding, do not modify the underlying model, and their performance remains dependent on the original model's capabilities. To address these challenges, we propose aligning LLMs through representation editing. The core of our method is to view a pre-trained autoregressive LLM as a discrete-time stochastic dynamical system. To achieve alignment for specific objectives, we introduce external control signals into the state space of this language dynamical system. We train a value function directly on the hidden states according to the Bellman equation, enabling gradient-based optimization to obtain the optimal control signals at test time. Our experiments demonstrate that our method outperforms existing test-time alignment techniques while requiring significantly fewer resources compared to fine-tuning methods. Our code is available at https://github.com/Lingkai-Kong/RE-Control.

## 1 Introduction

Autoregressive large language models (LLMs) such as ChatGPT , PaLM , and LLama , which are trained on extensive datasets, have demonstrated impressive abilities across a diverse array of tasks. However, the heterogeneous nature of their training data may lead these models to inadvertently generate misinformation and harmful content . This issue highlights the critical challenge of aligning language models with human objectives and safety considerations, a concern extensively discussed in recent research .

Existing approaches to aligning LLMs generally fall into two categories: fine-tuning and test-time alignment. Among fine-tuning methods, Reinforcement Learning from Human Feedback (RLHF; ) is particularly powerful. RLHF involves training a Reward Model (RM) based on human preferences and then using this model to fine-tune LLMs through reinforcement learning techniques . However, RL training can be difficult and unstable. Recent works  propose simpler alternatives to RLHF, but these methods still demand substantial computational resources. Additionally, the necessity of fine-tuning to adapt alignment objectives complicates the ability to swiftly customize models in response to evolving datasets and emerging needs.

On the other front, several test-time alignment techniques have been developed to tailor LLMs to specific objectives without altering their weights, such as prompt engineering and guided decoding . However, since these methods do not modify the underlying LLM, their alignment capability remains questionable, and performance may heavily depend on the original LLM.

In this paper, we take an alternative approach to aligning LLMs using representation editing. Instead of updating model weights, representation engineering perturbs a small fraction of model representations to steer behaviors, demonstrating great potential in improving LLMs' truthfulness  and reducing hallucinations . However, previous works typically rely on adding a fixed perturbation to the representation space during the generation process and do not take into account the autoregressive generation nature of LLMs. To address this, we propose a dynamic representation editing method from a control perspective.

The foundation of our model design is the connection between discrete-time stochastic dynamical systems and autoregressive language models. Inspired by techniques from control theory, we introduce control signals to the state space of the language dynamical system to achieve specific alignment objectives. According to Bellman equation, we directly train a value function in the representation space of LLMs. At test time, we perform gradient-based optimization to determine the control signals. Since the value function is simply a two- or three-layer neural network, the intervention is very fast and efficient. To align with the objective while preserving the generation quality of the original LLMs, we regularize the control signal to be as small as possible. This regularization is equivalent to control the step size or the number of steps during interventions at test time.

The main contributions of our work are: (1) We propose a new representation editing method to align LLMs from a control perspective. Our model, named Re-Control, does not require extensive computing resources compared to fine-tuning methods. Unlike existing test-time alignment methods such as prompt engineering and guided decoding, our approach perturbs the representation space of LLMs, offering greater flexibility. (2) We propose training a value function and computing the control signal at test time using gradient-based optimization. (3) We empirically show that Re-Control outperforms various existing test-time alignment methods and exhibits strong generalization ability.

## 2 Related Works

### Large Language Model Alignment

Alignment through Fine-tuning.RLHF has been a popular method in LLM alignment [51; 74; 59]. While effective, RLHF entails a complex process that involves training multiple models and continuously sampling from the LM policy during the learning loop. DPO  simplifies the RLHF framework by using a direct optimization objective derived from Proximal Policy Optimization (PPO; ), reducing the process to supervised training of the policy model alone. However, DPO is memory-intensive and resource-demanded as it requires managing two policies simultaneously. Contrastive Preference Optimization (CPO; ) mitigates these challenges by utilizing a uniform reference model, which not only reduces memory requirements but also enhances training efficiency. Alternative methods such as [71; 50] simplify model management and parameter tuning in the RLHF framework by adopting a supervised fine-tuning (SFT) approach. Additionally, RSO  and RAFT  employ rejection sampling to refine the alignment process. RSO focuses on estimating the optimal policy more accurately, while RAFT uses high-quality samples for iterative fine-tuning of the policy model.

Despite these advancements, a notable limitation of aligning LLMs through fine-tuning methods is their inflexibility in adapting quickly to emerging data and standards without extensive retraining, which poses challenges in dynamic environments where rapid adaptability is crucial.

Test time alignment.The other branch of methods to align LLMs involves adjustments at inference time. The simplest way is through prompt engineering. Existing works [4; 72; 36] have proposed the use of prompts that blend instructions with in-context examples to enhance the honesty and harmlessness of responses from LLMs. For instruction-tuned models, it has been shown that simply employing prompt engineering--without the addition of in-context examples--can enhance the safety of the models, as reported in .

In addition to prompting methods, guided decoding techniques have also been explored. ARGS , incorporate the score of a pre-trained reward model into the token probabilities. Other works [43; 25] learn a prefix scorer for the reward that is used to steer the generation from a partially decoded path. Moreover, DeAL  approaches the decoding process as an A* search agent, optimizing the selection of tokens

### Representation Engineering

Representation engineering  introduces steering vectors to the representation space of LLMs to enable controlled generation without resource-intensive fine-tuning. This concept of activation perturbation has its origins in plug-and-play controllable text generation methods , which utilizes a separate classifier for each attribute to perturb the model's activations, thereby producing text that aligns more closely with the classifier's target attributes. Prior research have demonstrated that both trained and manually selected steering vectors can facilitate style transfer in language models [53; 60]. Li et al.  have shown that steering the outputs of attention heads can enhance the truthfulness of LLMs. Liu et al.  suggest that standard in-context learning can be seen as a process of "shifting" the latent states of a transformer. More recently, representation fine-tuning [68; 67] has been introduced as a direct substitute for existing parameter-efficient fine-tuning methods. Remarkably, Wu et al. show that the representation editing can even surpass fine-tuning based methods by intervening on hidden representations within the linear subspace defined by a low-rank projection matrix. The effectiveness of these approaches confirms that the representations of pretrained LMs are semantically rich. Liu et al.  also explore representation engineering for aligning LLMs. However, their approach is notably more complex, necessitating an initial fine-tuning phase to capture the representation pattern, followed by a subsequent fine-tuning of the final model based on these patterns.

### Control Theory and Large Language Models

Understanding LLMs from a dynamical system perspective is a burgeoning field. Current research leverages control theory to enhance prompt design, demonstrating that LLMs can be effectively directed by carefully chosen inputs ("prompts") given sufficient time and memory resources. The seminal work by Soatto et al.  investigates the controllability of LLMs, focusing on'meaningful sentences' defined as the sigma-algebra generated by text fragments on the Internet. Subsequent research  broadens this analysis to encompass arbitrary sentences. Additionally, Luo et al.  expand the scope to include multi-round interactions with LLMs and multi-agent collaboration, offering new insights into the dynamical capabilities of these models. To the best of our knowledge, our study is the first to investigate optimal control for representation editing in LLMs.

## 3 Background: Stochastic Dynamical System and Optimal Control

Optimal control theory [56; 6], when applied to discrete-time dynamical systems , seeks to determine a control strategy that maximizes a cumulative reward over a sequence of time steps. This framework is particularly relevant to fields such as robotics [57; 58; 34; 28], automated trading systems [41; 65; 16; 41], autonomous vehicle navigation [30; 62; 29; 33], where decisions must be made sequentially to achieve a long-term goal.

Formally, a discrete-time stochastic dynamical system can be defined as follows:

\[s_{t+1}=f(s_{t},u_{t},_{t}),\]

where \(s_{t}\) denotes the system's state at time \(t\), and \(u_{t}\) represents the control input at the same time step. The stochastic term \(_{t}\) is typically modeled as a random noise drawn from a known probability distribution (e.g. Brownian motion), which introduces uncertainty into the state transition process. The function \(f\) specifies the state transition dynamics influenced by the current state, control input, and the stochastic nature of the environment.

The process begins from an initial state \(s_{0}\), which serves as the starting point for all subsequent decisions and state transitions. The aim of optimal control is to determine a control policy \(:\), mapping states to optimal control actions, that maximizes the expected cumulative reward:

\[_{}[R]=_{}[_{t=0}^{T}r(s_{t})],\]

where \(R\) is the cumulative reward and \(r(s_{t})\) is the intermediate reward received at each time step.

Methods such as policy iteration [7; 37] can be used to determine the optimal control policy. Each iteration involves two steps. First, we evaluate the current policy \(\) by solving the Bellman equation:

\[V^{}(s_{t})=_{_{t}}[r(s_{t})+V^{}(f(s_{t},u_{t },_{t}))],\]where \(V^{}(s_{t})\) represents the expected return over \(_{t}\) when the system starts in state \(s_{t}\) and follows policy \(\).

Next, we improve the policy:

\[(s_{t})*{arg\,max}_{u}(r(s_{t})+ _{_{t}}[V^{}(f(s_{t},u_{t},_{t}))]).\]

These evaluation and improvement steps are repeated until convergence.

## 4 Aligning Large Language Models from a Control Perspective

In this section, we present our method, Re-Control. First, we explain how autoregressive language models can be viewed as discrete-time stochastic dynamical systems. Next, we describe how to introduce control through representation editing. Finally, we detail the process of training the value function and performing test-time alignment.

### Autoregressive LLMs are Discrete-Time Stochastic Dynamical Systems

A pre-trained autoregressive language model processes a sequence of input tokens and predicts subsequent tokens by recursively processing the sequence. we focus on the transformer-based architecture  prevalent in modern language models [10; 55; 1].

**Definition 4.1** (Language dynamical system): _The behavior of a language dynamical system is governed by a function \(f_{}\), which acts as the state transition function, defined as:_

\[y_{t}(Wo_{t}), h_{t+1},o_{t+1}=f_{}(h_{t},y_ {t}).\]

_Here, \(y_{t}\) is the newly generated token at each time step. \(h_{t}\) comprises key-value pairs accumulated from previous time steps, represented as \(h_{t}=[\{(K_{0}^{(l)},V_{0}^{(l)})\}_{l=1}^{L},,\{(K_{t}^{(l)},V_{t}^{( l)})\}_{l=1}^{L}]\). Each pair \((K_{t}^{(i)},V_{t}^{(i)})\) corresponds to the key-value pairs generated from the \(i\)-th layer at time \(t\). \(W\) is a linear transformation that maps the logits \(o_{t+1}\) to a probability distribution over the vocabulary space \(\). The system's evolution continues until \(y_{t}=\), where \(\) represents a special stopping token that signifies the end of the system._

In this system, the hidden state \(h_{t}\) along with the logits \(o_{t}\) corresponds to the state \(s_{t}\) in a traditional stochastic dynamical system. The newly sampled token \(y_{t}\) at each time step plays a role similar to the random variable \(_{t}\), introducing stochasticity into the system. The initial state, \(s_{0}=\{h_{0},o_{0}\}\), is set by a given prompt \(\), marking the starting point of the dynamical process.

Figure 1: Overview of Re-Control: A value function is trained on the hidden space of an LLM to predict the expected reward. At test time, we optimize the hidden state of the LLM to maximize the value score. Re-Control effectively steers LLMs toward specific alignment objectives while avoiding the expensive fine-tuning process.

However, unlike typical dynamical systems, this model lacks a direct control signal, functioning as an uncontrolled system. Next, we will explore how optimal control techniques can be applied to align the behavior of pre-trained language models with specific objectives.

### Adding Control Signals to Large Language Models with Representation Editing

We introduce control signals \(u_{t}=\{u_{t}^{h},u_{t}^{o}\}\) into the state of the language dynamical system \(s_{t}=\{h_{t},o_{t}\}\) at each time step to achieve specific alignment objectives. Thus, the controlled language dynamical system is described as follows:

\[y_{t}(W(o_{t}+u_{t}^{o})), h_{t+1},o_{t+1}= f_{}(h_{t}+u_{t}^{h},y_{t}).\]

As we can see, adding control to such a language dynamical system is similar to representation editing. However, unlike existing representation editing methods , which add a fixed vector during the generation process, we dynamically perturb the representation space from a control perspective, offering greater flexibility. In practice, it is not necessary to add controls to the entire state space; perturbing only a subset is sufficient. For example, we can perturb only the state of the last layer.

For an alignment task, the reward function is defined as:

\[R([,_{t}]):=0&y_{t} \\ r([,_{t}])&y_{t}=,\]

where \([,_{t}]\) denotes the concatenation of the prompt and the model's response generated up to time \(t\). A reward is given only upon completion of decoding, with no reward assigned to a partial decoding path. The reward on the final response \(r\) can come from a pre-trained reward model  based on human preference data or specified by heuristics, such as a concise summary in fewer than 10 words, with a reward of 1 if achieved and 0 if it fails.

Our objective is to determine the control signals at each time step that maximize the expected reward while not deviating too much from the original state:

\[*{arg\,max}_{\{u_{t}\}_{t=1}^{T}}[R]-_{t=1} ^{T}||u_{t}||_{2}^{2},\] (1)

where \(\) is a hyper-parameter for regularization. The regularization term is designed to prevent reward overoptimization and maintain the generation quality of the perturbed LLMs.

### Training of Value Function

Traditional policy iteration involves multiple iterations of policy evaluation and policy improvement. However, in our case, to avoid significant deviation from the pre-trained model's original state, we perform only one-step policy iteration. The initial policy is to not add any control signal to LLMs, i.e., \(u_{t}=0\). Therefore, we only need to estimate the value function of the original language model.

The value function of the initial zero policy satisfies the Bellman equation :

\[V(s_{t})=_{s_{t+1}}[V(s_{t+1})],&y_{t} \\ r([,_{t}]),&y_{t}=\,.\]

Figure 2: At test time, we perform gradient-based optimization to determine the control signals added to the language dynamical system for alignment. The color represents the value score on the state space, with darker colors indicating higher scores. Our goal is not to update the state to the global optimum but to control the state to achieve a better value score while remaining close to the original state.

To construct the training dataset for the value function, for a prompt \(^{i}\) in the given training dataset, we sample \(M\) responses \(\{^{i,m}\}_{m=1}^{M}\). We score each response using the reward function and extract the states along the trajectories \(_{V}=\{\{^{i,m},^{i,m},r^{i,m}\}_{m=1}^{M}\}_{i=1}^ {N}\). Our training objective is:

\[=_{i}_{m}_{t}(V_{}(s_{t}^{i,m})-grad}(v_{t}^{i,m}))^{2}.\]

Here, \(s_{t}^{i,m}\) and \(v_{t}^{i,m}\) represent the state and the generated token of the LLM at generation time step \(t\). \(grad}()\) indicates that the gradient is not propagated through \(v_{t}^{i,m}\). The target value \(v_{t}^{i,m}\) is computed as follows:

\[v_{t}^{i,m}=V_{}(s_{t+1}^{i,m})&y_{t}^{i,m} \\ r^{i,m},&y_{t}^{i,m}=\,.\]

### Test-time Intervention

At inference time, we can directly perform gradient ascent on the model states to maximize the expected value score, as we train the value function on the state space. Our goal is not to find the global optimum in the state space but to improve the current state while staying close to the original state. Specifically, we initialize \(u_{t}=0\) and update \(u_{t}\) through gradient ascent as:

\[u_{t}=u_{t}+_{s_{t}}V_{}(s_{t}+u_{t}),\]

where \(\) is the step size. This update step can be repeated \(n\) times.

**Implicit Regularization.** Note that this update already incorporates the regularization effect. The regularization is achieved by using a small step size \(\) and a limited number of updates \(n\), ensuring that the control signal remains small. After adding the final control signals to the hidden states, we perform a forward pass in the language model to generate a new token.

**Parameterization of the Value Function.** Rigorously, policy iteration requires the input to the value function to be the full state, but it does not require the control signals to be added to the full state. This means we can train the value function on the full state and backpropagate through it with respect to partial inputs at test time. The simplest approach is to add control signals only to the logit \(o_{t}\). In this case, we find that training a two- or three-layer neural network using \(o_{t}\) as the input is already sufficient for achieving good empirical performance. If we want to further incorporate the attention key-value pairs \(h_{t}\) in the input, we need to address the input's varying size. To achieve this, we can initialize a vector and compute an attention weight by taking the dot product with the keys to aggregate all value embeddings. Then, we concatenate the aggregated value embedding with \(o_{t}\) and input it into a neural network.

## 5 Experiment

In this section, we conduct experiments to examine the effectiveness of our method. Our focus is on aligning large language models (LLMs) for helpfulness and minimizing harmfulness, which are essential qualities for an AI assistant.

### Experimental Setup

We evaluate our method on the HH-RLHF  and Stanford SHP (SHP)  datasets, which are popular for LLM alignment. These two datasets are used to improve the AI assistant's helpfulness and harmlessness. Each sample in the datasets contains a prompt and two responses with one preferred over another. For the base model, we adopt Vicuna-7B , Falcon-7B  and L1ama3-8B  as the instructed fine-tuned AI assistant. We evaluate these models by generating text responses based on test prompts of HH-RLHF and SHP. For reproducibility, we use publicly available reward models2. We train the value network on the last layer of the hidden states \(o_{t}\), and at test time, we add control signals only to this layer. For future studies, we can also explore adding controls to the attention key-value pairs \(h_{t}\) which should further improve the performance.

Following , we leverage Diversity, Coherence, Average Reward, and Win Rate as our evaluation metrics. **Diversity** measures the frequency of repeated n-grams in generated text. The diversity score for a given response \(\) is represented as \(_{n=2}^{4}()}{( )}\). A higher diversity score suggests a broader vocabulary range in text generation. **Coherence** calculates the cosine similarity between the embeddings of the prompt and its continuation. We use the pre-trained SimCSE sentence embedding model, following the approach outlined in , to obtain these embeddings. **Average Reward** is the mean of the rewards evaluated by the reward model across all responses corresponding to the test prompts. **Win Rate** is the rate at which the model's response is rated better than the preferred response in the dataset. Following [31; 12], we use GPT-4 as the judge, having it review and score two responses to the same prompt on a scale from 1 to 10. We provide explicit instructions to assess the responses based on criteria such as helpfulness, harmlessness, relevance, accuracy, and insightfulness. The detailed prompt is provided in Appendix D. We randomly sample 300 prompts from the test set of HH-RLHF for the GPT-4 evaluation. To mitigate position bias, we randomize the order in which we present the generated responses to GPT-4, as in . Additionally, we also present the **Inference Time** in hour under batch size of 32.

We randomly sample 1000 data points from the training set as a separate validation set to select the hyperparameters--the step size \(\) and the number of updates \(n\)--based on the sum of coherence, diversity, and average reward. Additional experimental details are provided in Appendix C.

### Baselines

We compare our method with several existing test-time alignment methods.

**Prompt Engineering:** In this method, we instruct the model within the prompt to provide responses that are more helpful and harmless . **Controlled Decoding (CD):** During the decoding process of LLMs, this method combines token probabilities with reward scores. We consider two versions. The first version  directly uses a reward model trained on human preference data, requiring the tokenization strategies of both the reward model and the base model to be the same. The second version  trains a prefix scorer to predict the expected reward from partially generated responses. We refer to it as CD prefix. **Static Representation Editing (RE):** Following , we first train a linear regression layer on the hidden state of an LLM, after feeding it the prompt, to predict the

   Dataset & Backbone & Model & Diversity \(\) & Coherence \(\) & Average Reward \(\) & Win Rate (\%) \(\) & Inference time (hour) \\   &  & Base & 0.816 & 0.568 & 5.894 & 57.6 & 0.60 \\  & & Static RE & 0.818 & 0.568 & 5.907 & 64.3 & 0.65 \\  & & CD & 0.806 & **0.608** & 5.458 & 72.3 & 47.43 \\  & & CD prefix & 0.805 & 0.576 & 6.105 & 74.6 & 32.13 \\  & & Ours. & 0.824 & 0.579 & 6.214 & **75.6** & 0.85 \\   &  & 0.817 & 0.570 & 5.913 & 66.0 & 0.69 \\  & & CD prefix + Prompting & 0.812 & 0.593 & 6.120 & 74.3 & 47.16 \\  & & Ours+Prompting & **0.830** & 0.577 & **6.267** & **80.3** & 0.93 \\  HH-RLHF &  & Base & 0.705 & 0.613 & 3.439 & 42.3 & 0.67 \\  & & Static RE & 0.698 & 0.610 & 3.449 & 52.6 & 0.56 \\  & & CD & N/A & N/A & N/A & N/A & N/A \\  & & CD prefix & 0.648 & 0.575 & **4.397** & 49.6 & 48.13 \\  & & Ours & 0.699 & 0.615 & 3.512 & 58.0 & 1.93 \\  & & Proempting & **0.746** & 0.620 & 4.010 & 52.3 & 0.59 \\  & & CD prefix + Prompting & 0.571 & **0.638** & 3.619 & 51.6 & 47.87 \\  & & Ours+Prompting & 0.741 & 0.619 & 4.083 & **62.6** & 2.00 \\   &  & Base & 0.845 & 0.657 & -5.68 & 40.3 & 0.13 \\  & & Static RE & 0.848 & 0.652 & -5.65 & 49.3 & 0.15 \\  & & CD & 0.845 & 0.655 & -5.65 & 55.6 & 22.16 \\  & & CD prefix & 0.838 & **0.660** & -5.62 & 41.0 & 14.15 \\  & & Ours & 0.849 & 0.652 & -5.38 & 58.0 & 0.21 \\  & & Prompting & 0.847 & 0.570 & -4.83 & 56.6 & 0.13 \\  & & CD prefix + Prompting & 0.842 & 0.574 & -4.88 & 56.3 & 14.32 \\  & & Ours+Prompting & **0.854** & 0.571 & **-46.63** & **63.6** & 0.23 \\   &  & Base & 0.878 & 0.672 & -4.64 & 56.3 & 0.13 \\  & & Static RE & 0.875 & 0.674 & -4.49 & 57.0 & 0.15 \\   & & CD & N/A & N/A & N/A & N/A & N/A \\   & & CD prefix & 0.862 & **0.685** & -4.41 & 64.0 & 12.16 \\   & & Ours & 0.883 & 0.669 & -4.39 & 71.0 & 0.21 \\   & & Prompting & 0.891 & 0.605 & -4.45 & 59.6 & 0.13 \\   & & CD prefix + Prompting & 0.872 & 0.603 & -4.25 & 68.0 & 12.74 \\   & & Ours+Prompting & **0.893** & 0.605 & **-4.14** & **77.0** & 0.24 \\   

Table 1: Performance comparison between Re-Control and other test-time alignment approaches. The win rate is evaluated by GPT-4 as the rate at which the model’s response is rated better than the preferred response in the dataset. Note that CD  requires the base model to have the same tokenization strategy as the reward model.

expected reward. At test time, we shift the activation space along the direction of the weights of the linear layer. Unlike our method, this approach adds a fixed vector to the representation space along the generation trajectory.

We provide more implementation details of the baselines in Appendix C.

### Experimental Results

Table 1 shows the performance of all the methods. Our findings can summarized as follows: (1) Re-Control achieves the highest alignment score in terms of the win rate evaluated by GPT-4. Moreover, it maintains generation quality, as measured by diversity and coherence. (2) The strongest baseline is controlled decoding. However, controlled decoding is 20 times slower than Re-Control. This is because controlled decoding needs to evaluate multiple candidate tokens and perform forward passes through the entire reward model repeatedly, while Re-Control only requires optimization through a value function that is a two- or three-layer neural network, making it much faster. Additionally, CD  lacks support for batch generation, making it even slower. (3) Combining prompt engineering with Re-Control can further improve alignment performance in terms of both average reward and GPT-4 evaluation. Specifically, it outperforms the strongest baseline by {7.6%, 19.0%, 12.4%, 13.2%} in terms of the GPT-4 win rate. In contrast, controlled decoding with prompting shows only marginal improvements. This might be because Re-Control perturbs the activation space of the LLM, which is more flexible than merely changing the final token probability. (4) Re-Control significantly outperforms static representation editing by {17.6%, 10.2% 17.6%, 24.6%}. This is because Re-Control dynamically adjusts the representation during the autoregressive generation process, offering more control. In contrast, static representation editing applies a fixed shift, which is more rigid.

In Table 2, we present a qualitative example demonstrating how Re-Control can steer the base model to output more helpful and harmless responses. In this example, the user asks for suggestions on lying to an organization. The base model provides various tactics, while Re-Control refuses to give such suggestions and emphasizes that lying can damage relationships and trust within an organization.

## 6 Further Analysis

### Comparison with Training-time Alignment

In the previous section, we compared Re-Control with test-time alignment methods that do not require extensive computing resources. This feature is crucial when we need the model to quickly adapt to different requirements, as it only involves training a simple value network with just two or three layers. In this subsection, we further compare Re-Control with fine-tuning based approaches with LoRA . Figure 3 shows the comparison between Re-Control, Proximal Policy Optimization (PPO), and Direct Preference Optimization (DPO) .

 Example \\ 
**Prompt** \\ USER: What’s the easiest way to lie your way to the top of an organization? ASSISTANT: \\
**Base** \\ There is no easy way to lie your way to the top of an organization, as it ultimately depends on the specific company and industry. However, some common tactics that people may use to advance their careers through deception include: 1. Exaggering skills or accomplishments: This can be done by inflating the scope or impact of past projects, or by claiming skills or certifications that you do not have. 2. Pretending to be someone you’re not: This can involve adopting a different personality or presenting yourself as more confident or experienced than you actually are. 3. \\
**Re-Control** \\ There is no easy way to lie your way to the top of an organization, as ethical and honest behavior is typically expected of leaders. Additionally, lying and deceit can damage relationships and trust within an organization, which can ultimately harm one’s career prospects. It’s important to focus on building credibility and trust through honest and transparent communication, as well as demonstrating competence and dedication in one’s work. \\  

Table 2: Comparative examples of base model Vicuna-7B and Re-Control.

All the models use Vicuna-7B as the base model and we test them on HH-RLHF. The training details for LoRa-based PPO and DPO are provided in C. Overall, the results indicate that our approach is a competitive alternative to LoRa-based fine-tuning methods. Similar findings have also been reported in the controlled decoding literature . Overall, for users prioritizing real-time inference, amortizing computation during the training process remains preferable. However, for those without resources for fine-tuning, test-time alignment is a more practical choice, as it easily adapts to different alignment objectives, albeit with increased inference time.

### Generalization to a new input distribution

An important question is how our method can generalize to a new input distribution different from the value function is trained on. To investigate this question, we further test on a out-of-distribution (OOD) dataset HarmfulQA  with the value function trained on HH-RLHF. The test split of HarmfulQA contains harmful questions to evaluate language model performance against red-teaming attempts. We focus on the GPT-4 evaluation since the reward model will not be accurate for the OOD data. We compare Re-Control + Prompting with other test-time alignment methods + Prompting. Figure 4 presents the results. As illustrated, Re-Control + Prompting achieves the highest performance in terms of the GPT-4 win rate on both Vicuna-7B and Falcon-7B. This is an important ability especially when we want to deploy the LLM in the open world.

### Hyperparameter Study

To better understand the characteristics of Re-Control, we vary two hyperparameters--the step size \(\) and the number of updates \(n\) for the test-time intervention--and measure key performance statistics. Figure 5 shows the diversity, coherence, and average reward of the generated responses in relation to these two parameters on 1000 randomly sampled prompts from HH-RLHF.

As we can see, increasing the step size \(\) initially improves the reward, but beyond a certain point, larger step sizes fail to compute the control signal accurately, causing the reward to decrease. The influence of the number of updates \(n\) shows a more complex pattern: the reward first improves, then decreases, and improves again, indicating a transition from escaping a local minimum to moving towards another minimum. The coherence and diversity metrics drop to nearly zero, which is evidence of reward overoptimization. Thus, regularization to prevent significant deviation from the original states is essential. In practice, we select these two hyperparameters based on the sum of all three metrics on the validation set.

Figure 4: Testing on out-of-distribution data HarmfulQA. The win rate is measured by GPT-4 as the rate at which responses are better than those of the base model, since the test set of HarmfulQA does not provide reference responses.

Figure 3: Comparison with LoRa-based fine-tuning methods using Vicuna-7B as the base model on HH-RLHF.

### Inference Time Analysis

We provide additional analysis of the inference time. Figure 6 presents the inference time across different batch sizes. As shown, increasing the batch size reduces the discrepancy between Re-Control and the base model, becoming negligible at a batch size of 32. Figure 7 illustrates the compute-performance tradeoff between Re-Control and CD. For Re-Control, we vary the number of iterations when optimizing through the value function at test time, while for CD, we adjust the number of candidate tokens. As shown, the performance of RE-Control initially increases with more computing time but eventually decreases. This decline occurs because a large number of iterations at test time can lead to reward hacking, reducing the quality of the generated sentences. As discussed in Section 6.3, this hyperparameter can be selected based on the validation set. Since CD does not support batch generation, its inference speed is significantly slower than Re-Control. Even when RE-Control does not use batch generation, it outperforms CD when using the same computing resources. For example, when the inference time is around \(155\) minutes, the win rate of Re-Control is \(75\%\), while CD is only \(62\%\).

## 7 Conclusion, Limitations and Future Work

In this paper, we propose Re-Control to align large language models (LLMs) at test-time using representation editing. We view autoregressive language models as discrete-time stochastic dynamical systems and introduce control signals to their representation space. Throughout the generation process, the representation space is dynamically perturbed to achieve higher value scores. Our method does not require fine-tuning the LLMs and offers more flexibility than existing test-time alignment methods such as prompting and guided decoding. We empirically show that Re-Control outperforms existing test-time alignment methods and exhibits strong generalization ability. Due to the space limit, we discuss limitations and future work in Appendix A.