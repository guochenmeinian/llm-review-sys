# _When LLMs Meet Cuning Texts_:

A Fallacy Understanding Benchmark for

Large Language Models

Yinghui Li\({}^{1}\)1, Qingyu Zhou\({}^{2,}\)2,\({}^{*}\)3, Yuanzhen Luo\({}^{*}\), Shirong Ma\({}^{1}\),

**Yangning Li\({}^{1}\), Hai-Tao Zheng\({}^{1,}\)4, Xuming Hu\({}^{3,}\)5, Philip S. Yu\({}^{4}\)**

\({}^{1}\)Tsinghua University, \({}^{2}\) Bytedance Inc.

\({}^{3}\)The Hong Kong University of Science and Technology (Guangzhou)

\({}^{4}\)University of Illinois Chicago

liyinghu20@mails.tsinghua.edu.cn

indicates equal contribution.Corresponding authors

###### Abstract

Recently, Large Language Models (LLMs) make remarkable evolutions in language understanding and generation. Following this, various benchmarks for measuring all kinds of capabilities of LLMs have sprung up. In this paper, we challenge the reasoning and understanding abilities of LLMs by proposing a **FaL**lacy **U**nderstanding **B**enchmark (**FLUB**) containing cunning texts that are easy for humans to understand but difficult for models to grasp. Specifically, the cunning texts that **FLUB** focuses on mainly consist of the tricky, humorous, and misleading texts collected from the real internet environment. And we design three tasks with increasing difficulty in the **FLUB** benchmark to evaluate the fallacy understanding ability of LLMs. Based on **FLUB**, we investigate the performance of multiple representative and advanced LLMs, reflecting our **FLUB** is challenging and worthy of more future study. Interesting discoveries and valuable insights are achieved in our extensive experiments and detailed analyses. We hope that our benchmark can encourage the community to improve LLMs' ability to understand fallacies. Our data and codes are available at https://github.com/THUKElab/FLUB.

## 1 Introduction

Large Language Models (LLMs) have shown great abilities to understand human languages, including information extraction , text correction , humor understanding , etc. Researchers have constructed numerous benchmarks to evaluate LLMs in various aspects . By using constructed benchmarks to interact with LLMs, researchers can analyze the behavior of LLMs to compare the performance of different LLMs and study how to further improve LLMs in a targeted manner.

Although many LLM benchmarks have sprung up, we believe that existing benchmarks are not challenging enough to truly measure the human-like intelligence of LLMs. In particular, we are still wondering whether LLMs can understand cunning texts that may contain misleading, wrong premise, intentional ambiguity, and so forth, considering that almost all LLMs are trained on "cleaned" and "correct" corpora. Therefore, we build a **FaL**lacy **U**nderstanding **B**enchmark (**FLUB**) to challenge LLMs for solving these problems.

Figure 0(a) shows the running examples from **FLUB**. From these cases, we directly feel the different behaviors of LLMs and humans when facing cunning texts. In the first example, LLMs ignore thecommon sense that the lotus root itself has many holes in its structure and fall into the trap of the running text, wrongly judging that the holes in the lotus root are caused by insect infestation. In the second example, LLMs fail to see the logic that depositing money into random ATMs does not create problems and therefore give an answer that seems reasonable but is absurdly laughable. In fact, these running texts for LLMs are very easy to handle for human intelligence. **Therefore, it is very urgent and meaningful to construct a benchmark composed of cunning texts to evaluate and thereby promote the improvement of LLMs' fallacy understanding capabilities.**

Inspired by the above motivation, we collect real cunning texts as our raw data from a famous Chinese online forum, the "Ruozhiba" (retard forum) 3. This forum is popular for its cunning and unreasonable posts, which are generally easy for humans to understand but challenging for LLMs. The characteristics of the posts contained in this forum are consistent with our research motivation, so choosing it as the data source well supports FLUB's evaluation of LLMs' fallacy understanding ability. After data cleaning and annotating of cunning types, FLUB has 8 fine-grained types of cunning texts and most of the texts in FLUB fall into two types of fallacy, namely, faulty reasoning and word game. Moreover, we also manually annotated one correct answer (i.e., the explanation of the cunning text) and three confusing wrong answers for each input text in FLUB, as shown in Figure 0(b).

Based on our constructed FLUB and its annotation information, we design three tasks with increasing difficulty to test whether the LLMs can understand the fallacy and solve the "cunning" texts. Specifically, (1) **Answer Selection**: The model is asked to select the correct one from the four answers provided by FLUB for each input text. (2) **Cunning Type Classification**: Given a cunning text as input, the model is expected to directly identify its fallacy type defined in our scheme. (3) **Fallacy Explanation**: We hope the model sees a cunning text and intelligently generates a correct explanation for the fallacy contained in the text, just like humans, without falling into its trap.

In our experiments, we select representative and advanced LLMs to be evaluated on FLUB. Our empirical study reveals: (1) LLMs are very poor in their ability to perceive fallacy types in cunning texts. (2) For a specific task, LLMs with larger parameter sizes do not always perform better. (3) There is a close relationship between the Answer Selection task and the Fallacy Explanation task, and the interaction between them is critical to promoting the understanding of fallacies in LLMs. (4) On FLUB, the widely used Chain-of-Thought and In-context Learning techniques deserve further improvement and research. We believe that our proposed FLUB and all our findings are crucial for LLMs to comprehend the fallacy and handle cunning texts in the real world.

Figure 1: The running examples and annotation examples of FLUB.

## 2 The FLUB Benchmark

### Benchmark Construction

Data CollectionWe collect raw text data from "Ruozhiba" in Baidu Tiepa 4. "Ruozhiba" is one of the most famous online forums in the Chinese internet community, and people often post interesting or "silly" texts on it just for fun. In addition, the recent study  also shows that the Ruozhiba data is very useful for improving the ability of Chinese LLMs. We find that many of the posts on this forum are tricky texts or brain-teaser-like texts, which is exactly in line with our purpose of using cunning texts to challenge LLMs, so we utilize this forum as our data source. As a result of automatic crawling, we initially collect 9,927 candidate posts. Notably, according to the Baidu Bar agreement 5, the data on Baidu Tiepa can be used for academic research free of charge and without liability.

Data CleaningWe employ annotators to manually filter out irrelevant posts that do not present cunning texts. Since the collected original posts contain irrelevant content such as links and images, we also require annotators to extract the fallacious and illogical contents from the raw post and rewrite them into a complete sentence. Besides, it is worth noting that we carefully ensure that the texts in FLUB are ethical texts. This process includes user information anonymization, sensitive information removal, and filtering of impolite posts. In total, we obtain 834 data samples to form FLUB.

Data AnnotationTo ensure the annotation quality, our criteria for selecting annotators is that the person must be a native Chinese speaker and have a bachelor's degree. In addition, because FLUB comes from the online forum, we also require annotators to have more than five years of experience as netizens. The detailed annotation workflows are as follows:

1. **Cunning Type Annotation**: We first define 8 cunning types within the collected texts along with their corresponding examples, as shown in Figure 2. Specifically, our core authors make a comprehensive summary based on careful observation of the 9,927 initial candidate posts, thus defining 8 types. Subsequently, each data sample is processed by three junior annotators, who are required to select an appropriate cunning type for the sample. We achieve the initial annotation results based on the voting results among three annotators. The initial annotation results become the final annotation information after being reviewed by the senior annotator (and modified if necessary). Particularly, there are still a small number

Figure 2: The definitions and examples of the cunning types in FLUB.

of samples that fall into multiple types. For these samples, senior annotators and our core authors will discuss carefully and select the main type (i.e., the most obvious type among multiple types) as the annotation result.
2. **Correct Explanation Annotation**: We assign two junior annotators to write the explanation or answer for each sample independently. We ask them to try to explain the given text in a detailed, objective, and unambiguous way. The senior annotator then selects (and modifies if necessary) the more suitable text written by the two junior annotators.
3. **Wrong Candidates Annotation**: This part annotation is to obtain the wrong candidate answer that may be likely to be answered incorrectly for each input text. We assign three junior annotators for each sample and require each of them to write three different incorrect answers based on their understanding of the text. Particularly, we emphasize to each junior annotator that the three different wrong answers they write should ensure diversity and resemble as much as possible the answers that LLMs can easily produce. For each sample's nine initial incorrect answers, the senior annotator selects the three most challenging sentences as the final wrong candidates.

Since the annotation difficulty of different information is different, the salary we pay to the annotators we employ is also different. Specifically, we pay each person who annotates the cunning type $0.5 per sample, each person who writes the correct explanation $1 per sample, and each person who writes the wrong candidates $2 per sample. In addition to the junior annotators providing the initial annotation results, we also set three senior annotators with a salary of $2 per sample, who are responsible for carefully checking the correctness of the annotation results provided by the junior annotators.

It is worth mentioning that we have prepared sufficient and representative samples for annotators to learn and pre-annotate to ensure that they fully understand the information we want to annotate before they officially start annotation. Specifically, we select senior annotators based on their performance in the pre-annotation process. If an annotator's success rate is above 95%, he or she will be appointed as a senior annotator. In addition, it is worth mentioning that, all of our formal annotators have a success rate of over 80% in the pre-annotation process. At the same time, to avoid bias caused by the subjectivity of annotators as much as possible, our core authors also carefully checked the final annotation results of each data sample. Our entire annotation process lasted 2 weeks.

### Dataset Analysis

Data SizeFLUB comprises 834 samples that span 8 cunning types. It is worth emphasizing that the data size is not directly related to the evaluation effectiveness of a LLM benchmark. For example, TruthfulQA  and FreshQA , these benchmarks that have been widely used and had deep impacts, only have 817 and 500 test samples respectively. The main reasons limiting the size of FLUB are that it is derived entirely from real-world online forum posts and our rigorous high-quality data cleaning process, which retained 834 final samples from 9,927 candidate posts.

Data DistributionAs for the cunning type distribution of FLUB, most data in FLUB belong to the types of reasoning errors (53.4%) and word games (28.7%). This is because these two types of posts appear widely in "Ruozhi Bar" forum whose purpose is to challenge human intelligence. A large number of cunning texts involving reasoning errors and word games ensure that FLUB is challenging enough. Besides, we observe that some types of texts are relatively rare, such as phonetic errors (0.6%). In fact, this is because our data come entirely from the real world and are all carefully constructed by netizens. Cases of cunning texts caused by phonetic errors are indeed rare in the real world. To eliminate the impact of type imbalance when FLUB evaluating LLMs, we choose the F-1 score as the evaluation metric which comprehensively considers the type coverage.

Annotation QualitySince cunning type annotation is essentially a classification process performed by multiple annotators, we analyze the annotation quality of this information. Specifically, we calculate Fleiss' Kappa  to reflect the three junior annotator's Inter-Annotator Agreement (IAA). Our final obtained Fleiss Kappa result is greater than 0.767, which shows that our annotation results have excellent consistency and quality . On the other hand, we further ensure annotation quality by checking the annotation and modification results of the senior annotators. According to our statistics, senior annotators modified a total of 159 initial annotation results of data samples, that is, the modification rate of senior annotators was 19.06%. This reflects the excellent workload of our senior annotators and also reflects the high quality of our dataset. Moreover, after further checking of the modification results of the senior annotators by our core authors, we found that the main reason for the modifications was the disagreement between the senior annotators and the junior annotators on the cunning types (most of the cases were the ones we mentioned before that may fall into multiple types of samples). For these cases, our core authors made the most reasonable choices and personally modified the annotation results to maximize the quality of the annotation. After all, no one knows the full picture of our work better than our core authors.

### Benchmark Task Setups

To evaluate the fallacy understanding ability of LLMs, we design three benchmark tasks on FLUB: Answer Selection, Cunning Type Classification, and Fallacy Explanation. For each task, we design prompts to guide LLMs on the expected output. We also explore the prompting strategies of Chain-of-Thought and In-context Learning to conduct in-depth exploration on FLUB. The details of our designed prompts are shown in Appendix A. Below we introduce the details of our three tasks:

Task 1: Answer SelectionIn Task 1, LLMs are required to select the correct answer from four given candidate explanations for each input text. The annotation of candidate explanations is illustrated in Figure 0(b). In general, each sample in this task is a tuple \(\{p,q,O_{A},O_{B},O_{C},O_{D},l\}\), where \(p\) is our given prompt as shown in Appendix A, \(q\) is the input text, \(O_{A}\), \(O_{B}\), \(O_{C}\), and \(O_{D}\) are four candidate explanations, and \(l\{A,B,C,D\}\) is the golden label indicating \(O_{l}\) is the correct explanation. The design motivation of this task is to test whether LLMs can distinguish right from wrong when seeing the correct and wrong answers in the context of a given cunning text.

Task 2: Cunning Type ClassificationIf LLMs are directly tasked with determining the corresponding cunning type, it will help us in conducting an initial automated assessment of the LLM's understanding ability. The cunning type classification task is specifically designed to evaluate whether LLMs can classify the cunning text into categories aligned with human intuition based on the hidden irrational aspects within the current text. The annotated problem types are shown in Figure 2. During task evaluation, all the problem types will be combined with the prompt to allow LLMs to directly pick the correct type of cunning text.

Task 3: Fallacy ExplanationTo further test whether LLMs truly understand the given cunning text, we design the explanation task. In this task, the designed prompt and input texts are directly input into LLMs, enabling them to "read" input texts and generate corresponding explanations. Note that since some texts are not expressed in the form of inquiries, we also set a prompt to guide LLMs in identifying the question (See Appendix A). The generated explanations will be compared with the correct explanation for evaluation. If LLMs can generate reasonable explanations, we believe that they have at least developed the ability to identify and avoid the traps of cunning texts.

Automatic Evaluation MetricsFor Task 1, we calculate **Accuracy** directly based on the LLMs' selection results. For Task 2, considering that there are a few cunning types in FLUB with small sample size, we choose the **F-1 Score** to measure the performance of LLMs because it focuses on both the accuracy of model prediction and the coverage for positive class samples, thereby effectively avoiding bias caused by type imbalance and ensuring the rationality and reliability of evaluation. To evaluate the quality of LLMs' generated explanations in Task 3, inspired by MT-Bench , we construct prompts that incorporate the task instruction, input texts, LLM's explanations, and reference answers. These prompts are fed into GPT-4, which is tasked with assigning a **GPT-4 Score** ranging from 1 to 10. The prompt for the automated evaluation is illustrated in Appendix B.

Human Evaluation SettingsFor Task 1 and Task 2, **we conduct human evaluations to explore how well human-level intelligence could perform these two tasks.** To ensure the fairness of the comparison between humans and LLMs, we hire 3 new persons who do not participate in the construction process of FLUB. After briefly introducing them to the objectives of Task 1 and Task 2 (without introducing additional knowledge and information), let them directly carry out selection and classification. For the human evaluation of Task 3, **we mainly want to verify the effectiveness of the automatic GPT-4 score we use,** therefore, we hire 3 evaluation annotators to rate LLMs' explanations, with scores ranging from \(\{1,2,3,4,5\}\). To ensure an accurate evaluation of the explanations of LLMs, we developed a set of scoring guidelines for annotators, including the definitions and relevant examples for each score. The scoring guidelines of human evaluation are presented in Appendix C.

When designing the GPT-4 scoring range and the human scoring range, we have different motivations. We hope that GPT-4's scoring range can be as unbiased and detailed as possible, so we set its scoring range to 1-10. But this scoring range is too fine-grained and difficult for humans, so we set the human scoring range to 1-5. Therefore, for comparability of GPT-4 scores and human scores in Table 2, we multiply human scores by 2 to match the range of GPT-4 scores.

## 3 Experiments

### Experimental Settings

To better reflect the evaluation of FLUB's fallacy understanding ability of LLMs, we select some advanced LLMs that are widely used in the Chinese community: (1) **ERNIE-Bot** is a series of closed-sourced commercial LLMs released by Baidu. We evaluate the three latest chat models, including ERNIE-Bot-3.5, ERNIE-Bot-3.5-Turbo, and ERNIE-Bot-4.0. (2) **ChatGPT** ChatGPT is undoubtedly the hottest model developed by OpenAI. We evaluate GPT-3.5-Turbo and GPT-4-Turbo. (3) **ChatGLM3** is the latest open-sourced model of the ChatGLM which is a series of bilingual LLMs. We evaluate the only open-sourced parameter size of ChatGLM3-6B. (4) **Qwen** is the open-sourced LLMs developed by the Alibaba Group. We select three chat Qwen models, including Owen-7B-Chat, Qwen-14B-Chat, and Owen-72B-Chat. (5) **Yi** series models are open-sourced LLMs trained from scratch by 01-AI. In our experiments, we select Yi-6B-Chat and Yi-34B-Chat to be evaluated on FLUB. (6) **Baichuan2** has achieved the competitive performance of its size on many Chinese benchmarks. We select Baichuan2-7B-Chat and Baichuan2-13B-Chat.

When running LLMs inference, for closed-sourced LLMs, we access corresponding models via the official APIs. Meanwhile, open-sourced models are deployed on 1 to 4 NVIDIA A100 GPUs depending on their parameter size.

### Automatic Evaluation Results

The main results are presented in Table 1 and we have the following insights:

1. **For the difficulty of different tasks**, the Answer Selection task is the simplest, which shows that LLMs should have a certain ability to distinguish right from wrong when seeing correct and wrong answers. However, we also see that the performance of all models on

    &  &  &  &  &  \\  & &  &  &  &  \\  & **Source** & **w/o CoT** & **CoT** & **w/o CoT** & **CoT** & **w/o CoT** & **CoT** & **w/o CoT** & **CoT** \\  ERNIE-Bot-3.5-Turbo  & ✗ & 32.97 & 34.65\({}^{}\) & 1.99 & 6.09\({}^{}\) & 5.78 & 5.83\({}^{}\) & 7.24 & 10.72\({}^{}\) \\ ERNIE-Bot-3.5  & ✗ & 52.76 & 38.37\({}^{}\) & 10.33 & 11.15\({}^{}\) & 6.35 & 6.22\({}^{}\) & 15.13 & 13.86\({}^{}\) \\ ERNIE-Bot-4.0  & ✗ & 75.66 & 71.34\({}^{}\) & 11.84 & **14.42\({}^{}\)** & 7.38 & 8.11\({}^{}\) & 19.06 & 20.28\({}^{}\) \\ GPT-3.5-Turbo  & ✗ & 50.48 & 48.08\({}^{}\) & 3.09 & 6.15\({}^{}\) & 6.23 & 7.00\({}^{}\) & 9.91 & 12.74\({}^{}\) \\ GPT-4-Turbo  & ✗ & **79.38** & **82.73\({}^{}\)** & **12.31** & 13.97\({}^{}\) & **8.95** & **9.21\({}^{}\)** & **20.60** & **22.00\({}^{}\)** \\  ChatGLM3-6B  & ✓ & 35.85 & 35.01\({}^{}\) & 7.48 & 9.34\({}^{}\) & 4.98 & 4.82\({}^{}\) & 11.01 & 11.64\({}^{}\) \\ Qwen-7B-Chat  & ✓ & 38.49 & 33.69\({}^{}\) & 8.00 & 10.97\({}^{}\) & 5.39 & 5.65\({}^{}\) & 11.84 & 11.98\({}^{}\) \\ Qwen-14B-Chat  & ✓ & 42.57 & 43.05\({}^{}\) & **10.34** & 10.44\({}^{}\) & 5.24 & 6.24 & 13.21 & 14.10\({}^{}\) \\ Qwen-72B-Chat  & ✓ & **58.63** & **61.51\({}^{}\)** & 9.32 & **12.26\({}^{}\)** & **7.34** & **7.90\({}^{}\)** & **15.89** & **18.13\({}^{}\)** \\ Yi-6B-Chat  & ✓ & 32.37 & 29.26\({}^{}\) & 8.87 & 9.84\({}^{}\) & 5.73 & 5.39\({}^{}\) & 18.11 & 15.8\({}^{}\) \\ Yi-34B-Chat  & ✓ & 47.96 & 48.80\({}^{}\) & 4.74 & 11.70\({}^{}\) & 6.97 & 7.52\({}^{}\) & 11.66 & 16.17\({}^{}\) \\ Baichuan2-7B-Chat  & ✓ & 43.17 & 37.17\({}^{}\) & 1.02 & 4.45\({}^{}\) & 5.48 & 4.85\({}^{}\) & 6.23 & 9.29\({}^{}\) \\ Baichuan2-13B-Chat  & ✓ & 37.05 & 38.01\({}^{}\) & 3.52 & 4.58\({}^{}\) & 5.79 & 5.84\({}^{}\) & 9.11 & 10.06\({}^{}\) \\  Random & - & 25.00 & & 7.90 & & - & - \\  Human & - & 93.35 & & 63.69 & & - & - \\   

Table 1: We **bold** the optimal and underline the suboptimal of closed/open-source models. We report the overall performance by calculating the **geometric mean** of the three tasks. We color the result that Chain-of-Thought (CoT) brings positive / negative gain as green\({}^{}\)/ red\({}^{}\).

the Cunningham Type Classification task is unsatisfactory, with F-1 scores below 15.0, and some models even perform below random performance. This deficiency may stem from the models' limited capability to comprehend the semantics of various running types.
2. **For the connection between different tasks**, the comparative outcomes among different models across the three tasks are not consistent. Nevertheless, models that exhibit superior performance in the Answer Selection task tend to generate more plausible explanations. This phenomenon reminds us that there is a close relationship between the Answer Selection task and the Fallacy Explanation task. The interaction between these two tasks is very critical for improving the fallacy understanding ability of LLMs.
3. **For the model performance of different scale parameters**, overall, models of larger scale are better equipped to understand cunning texts, which aligns with intuitive expectations. Of course, there are exceptions. We find that for the Qwen and Yi models, as the parameter size increases, the performance of the Cunning Type Classification task decreases. This is because this task requires a deep understanding of the Chinese language, especially the popular Internet language, and we observe that as the Qwen and Yi models become larger, their ability to understand special Internet language becomes poorer. Besides, the another reason for the poor cunning type classification performance of the models is that they cannot accurately understand the defined types. Therefore, how to improve the perception ability of LLMs for the cunning types will be the key challenge to improving the performance of LLMs on the cunning classification task.
4. **For the impact of Chain-of-Thought**, to our surprise, Chain-of-Thought (CoT) does not bring stable improvements to LLMs' fallacy understanding ability. Especially for the Answer Selection and Fallacy Explanation tasks, CoT even has negative impacts on some models. We think there are two main reasons for this phenomenon: (1) We notice that when the model size exceeds 10B, CoT still has positive effects on these two tasks. This reflects the challenge of our tasks, which makes CoT unable to stimulate the small models to have sufficient capabilities to cope with them. (2) For traditional QA tasks (such as commonsense reasoning, mathematical reasoning, etc.), CoT can improve performance because these tasks themselves are relatively logical, and the process of solving their questions can be modeled as the logical reasoning process. Unlike these tasks, our tasks are not very logical problems but require more intuition about the language. Hence, adding intermediate steps by the CoT has no significant effect on our tasks. In summary, our proposed tasks deserve further research to improve the fallacy understanding ability of LLMs.
5. **For the overall performance**, considering that the performance values of the three sub-tasks are very different, we use the geometric mean to balance the impact of each sub-task and avoid the excessive impact of a single extreme value on the overall performance. We see that the overall performance of each model is basically consistent with common sense, that is, the larger the model, the better the performance, and CoT also brings positive effects. This shows that \(\) is of high quality and suitable to measure the fallacy understanding ability of LLMs from an overall perspective.
6. **For the human performance**, we see that humans perform well on the Answer Selection and Cunning Type Classification tasks, which reflects the considerable gap in fallacy understanding between human intelligence and LLMs. It also shows that our proposed new benchmark and tasks are conducive to further promoting the progress of LLMs. Note that the reason why the Fallacy Explanation task is not suitable for evaluating human performance is that its automatic evaluation indicator is the GPT-4 Score. We think that using GPT-4 to evaluate explanations written by humans is unreasonable and unnecessary.

### The Impact of In-context Learning

We select 5 high-performing LLMs to study the impact of in-context learning on LLMs' fallacy understanding ability. Demonstrations used for in-context learning are randomly selected. As shown in Figure 3, unlike Chain-of-Thought which has no stable positive effect, the LLMs' performance with in-context learning is basically on the rise as demonstrations increase. This indicates that letting LLMs see more examples can improve their fallacy understanding ability, but the number of examples must be large enough because we have also seen that when only one shot example is added, the performance of LLMs sometimes declines compared to the zero-shot cases.

### Human Evaluation of Explanation

To verify the effectiveness of our designed automatic GPT-4 score for Task 3, we randomly select 50 data samples from FLUB, along with outputs from 5 high-performing LLMs for human evaluation by our contracted annotators. From the human evaluation results in Table 2, we observe that:

1. The overall correlation coefficient between the automatic and human evaluation is 0.69, indicating a high consistency between GPT-4 scores and human preferences. Besides, the correlation results also verify the effectiveness of our designed GPT-4 score for Task 3.
2. Both automatic and human evaluations exhibit a broadly consistent ranking across the selected five models. The GPT-4-Turbo achieves superior performance over all other models. In contrast, human annotators perceive marginal performance disparities among ERNIE-Bot-4.0, Qwen-72B-Chat, and Yi-34B-Chat models.
3. From the human evaluation results, except for GPT-4-Turbo, which can exceed the passing score of 6, the performance of other LLMs is still not ideal, which shows that the community still needs to further study how to improve the fallacy understanding ability of LLMs.

### Case Study

To analyze FLUB's challenges, we conduct case studies on the two advanced models with better performance in the fallacy explanation task in Table 3. From the first case, we see that GPT-4-Turbo gives a relatively perfect explanation, while ERNIE-Bot-4.0's answer does not explain the causal relationship clearly although its final conclusion is correct. According to ERNIE-Bot-4.0's explanation, if the egg is added with water, it can be restored. This is obviously wrong. In the second case which is more difficult, both ERNIE-Bot-4.0 and GPT-4-Turbo easily fail when facing these cunning texts. Specifically, ERNIE-Bot-4.0 follows the trap of the input text, not clearly stating that "putting heads on the shore" is an impossible operation, but giving a dumbfounding explanation. In comparison, GPT-4-Turbo's performance is slightly better, but it does not perceive the trap in the input text at all, resulting in an answer that is not what is questioned. It can be seen from these two cases that LLMs' ability to handle cunning texts is still insufficient.

  
**Models** & **Human** & **GPT-4** & **Correlation** \\  GPT-4-Turbo & 7.12 & 8.60 & 0.57 \\ ERNIE-Bot-4.0 & 5.82 & 7.20 & 0.71 \\ Qwen-72B-Chat & 5.74 & 7.82 & 0.42 \\ Yi-34B-Chat & 5.42 & 6.44 & 0.74 \\ Baichuan2-13B-Chat & 4.42 & 5.84 & 0.63 \\ Overall & - & - & 0.69 \\   

Table 2: Human evaluation on the explanation task. Note that we multiply the human results by 2 to normalize their range to be the same as the GPT-4 results’ range. The reported correlations are Spearman’s rank correlation coefficients. All correlations are extremely significant with \(p<0.01\).

Figure 3: The results of in-context learning with 0/1/2/5-shots demonstrations.

[MISSING_PAGE_FAIL:9]

Ethics Statement

In this paper, we present a new benchmark, FLUB. We have described the details of the collection, preprocessing, and annotation of FLUB. And we ensure that no infringement or unethical behavior occurred during the dataset construction. In terms of the data itself, to ensure that the dataset we need to release in the future meets ethical requirements, we spend lots of energy on data anonymization, data desensitization, improper data cleaning, etc. Besides, the cunning texts we are concerned about come from daily life and are very common. Therefore, the new research direction and tasks we propose will not cause harm to human society.

## 7 Conclusion

In this work, we construct FLUB, a high-quality benchmark consisting of cunning texts designed to evaluate the fallacy understanding ability of LLMs. Furthermore, we evaluate advanced LLMs on FLUB. Detailed analyses indicate FLUB is very challenging and of great research value. To date, most existing LLMs still can not understand the fallacy well, which results in them being far from dealing with complex problems in the real world as easily as humans. We believe that the benchmark and the research direction we provide are valuable for the LLMs community.