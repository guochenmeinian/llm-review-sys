# What Makes and Breaks Safety Fine-tuning?

A Mechanistic Study

 Samyak Jain

Five AI Ltd.

&Ekdeep Singh Lubana

University of Michigan &

CBS, Harvard University

&Kemal Oksuz

Five AI Ltd.

&Tom Joy

Five AI Ltd.

&Philip H.S. Torr

University of Oxford

&Amartya Sanyal

Max Planck Institute for Intelligent Systems &

University of Copenhagen

&Puneet K. Dokania

Five AI Ltd. &

University of Oxford

###### Abstract

Safety fine-tuning helps align Large Language Models (LLMs) with human preferences for their safe deployment. To better understand the underlying factors that make models safe via safety fine-tuning, we design a synthetic data generation framework that captures salient aspects of an unsafe input by modeling the interaction between the task the model is asked to perform (e.g., "design") versus the specific concepts the task is asked to be performed upon (e.g., a "cycle" vs. a "bomb"). Using this, we investigate three well-known safety fine-tuning methods--supervised safety fine-tuning, direct preference optimization, and unlearning--and provide significant evidence demonstrating that these methods minimally transform MLP weights to _specifically_ align unsafe inputs into its weights' null space. This yields a clustering of inputs based on whether the model deems them safe or not. Correspondingly, when an adversarial input (e.g., a jailbreak) is provided, its activations are closer to safer samples, leading to the model processing such an input as if it were safe. Code is available at https://github.com/fiveai/understanding_safety_finetuning.

## 1 Introduction

Large language models (LLMs) are commonly trained via a combination of pre-training on a large corpus and instruction fine-tuning, wherein the model is supervised to follow instructions (Driess et al., 2023; Team et al., 2023; Qin et al., 2024). While pre-training enables a model to learn different capabilities (Wei et al., 2022; Bubeck et al., 2023), instruction fine-tuning enables use of open-ended, generic inputs to control said capabilities (Ouyang et al., 2022; Wei et al., 2021; Sanh et al., 2021; Bai et al., 2022; Raffel et al., 2020). Since this pipeline does not restrict what tasks the model can be used for, potential misuse is left feasible under its purview (Bengio et al., 2023; Anwar et al., 2024): as long as an instruction can be formulated and the model possesses the relevant capabilities to perform the instructed task, it will strive to perform it. To prevent such misuse, safety fine-tuning is used as an additional training phase for LLMs, in which the model is supervised to prioritize generation of outputs deemed safe as per human preferences. Popular approaches for safety fine-tuning include: (i) supervised safety fine-tuning (SSFT) (Ouyang et al., 2022); (ii) reinforcement learning with human feedback (RLHF) (Christiano et al., 2017; Ouyang et al., 2022; Bai et al., 2022; Stiennon et al., 2020) and its recent renditions that avoid use of an explicit reward model, e.g., DPO (Rafailov et al., 2023); and (iii) machine unlearning (Liu et al., 2024). Despite immense use of these protocols to enablesystem release (Chao et al., 2024; Sun et al., 2024), several recent works show that safety fine-tuned models continue to produce unsafe generations when prompted via adversarially designed inputs, e.g., jailbreaks (Andriushchenko et al., 2024; Chao et al., 2023; Zou et al., 2023; Carlini et al., 2023).

In this work, our goal is to understand: (i) _what is the safety mechanism learned by the model via safety fine-tuning?_ and (ii) _how are jailbreak and adversarial attacks able to bypass this mechanism?_ While a few contemporary papers have investigated the mechanisms of safety fine-tuning, e.g., showing that such methods perform minimal alterations to model parameters that nevertheless can change its behavior (Jain et al., 2023; Lee et al., 2024; Prakash et al., 2024; Wei et al., 2024), tying this analysis back with lack of robustness of safety fine-tuning is lacking in existing literature. We aim to fill this gap by designing a well-defined synthetic data generating process wherein an input is modeled as a function of the task the model is expected to perform (e.g., "design"), and the specific concept the task is to be performed upon (e.g., "cycle" versus "bomb"). This separation helps us delineate how the model distinguishes between safe versus unsafe inputs, while allowing us to model different forms of jailbreak attacks grounded in the formalization of Wei et al. (2023). Overall, our contributions and observations can be summarized as follows.

* **Systematic setup to study safety fine-tuning and jailbreaks.** We introduce a novel synthetic data generation framework that allows _controlled_ generation of data for safety fine-tuning, jailbreaks, and adversarial attacks. We make careful design choices to adhere to the properties of natural language instructions and the jailbreaks taxonomy of Wei et al. (2023), thus facilitating a thorough safety analysis that can be backed with corroboratory experiments on real LLMs.
* **We show that safety fine-tuning methods yield specialized transformations that primarily activate for unsafe inputs.** We provide comprehensive analyses on the mechanisms learned by safety fine-tuning showing that these methods (i) encourage _separate cluster formations for safe and unsafe samples_ by minimally transforming MLP weights to specifically project unsafe samples into the null space of model's weights, and (ii) substantially reduce the local Lipschitzness of the model for unsafe samples.
* **Adversarial inputs have activations similar to safe samples, hence bypassing the safety transform.** Establishing the mechanism via which a model identifies which inputs to refuse processing of, we are able to demonstrate that by merely following an activation distribution that is exceedingly similar to that of safe samples, jailbreak attacks are able to ensure the minimal MLP transformation learned to identify unsafe samples is not triggered.

## 2 Preliminaries

**Safety fine-tuning protocols** Broadly, LLM training can be divided into three stages (Team et al., 2023; Touvron et al., 2023): (1) (unsupervised) pre-training to build the initial model; (2) instruction fine-tuning to optimize the pre-trained model to follow instructions and provide plausible outputs for general queries; and (3) safety fine-tuning to ensure that the instruction fine-tuned model's output respects human preferences. We denote an LLM parameterized with parameters \(\) as \(f_{}\). Let the tuple \(=\{,^{p},^{l}\}\) consist of the input \(\), the preferred response \(^{p}\), and the less preferred response \(^{l}\). Let \(^{}\), \(\), and \((.,.)\) denote the parameters of the instruction fine-tuned model, the safety fine-tuning dataset, and the standard cross-entropy loss, respectively. Using these notations, the objective functions of safety fine-tuning methods analyzed in this work can be written as follows.

* _Supervised Safety Fine-Tuning (SSFT)_(Ouyang et al., 2022): \(*{argmin}_{}_{(,^{p}) }\ (f_{}(),^{p})\).
* _Unlearning_(Liu et al., 2024): \(*{argmin}_{}_{}\ (f_{}(),^{p})- (f_{}(),^{l})\).
* _Direct Preference Optimization (DPO)_(Rafailov et al., 2023): \[*{argmax}_{}_{} ((f_{^{}}(),^{p})- (f_{}(),^{p}))-((f_{^{}}(),^{l})-(f_{}(),^{l})) .\]

Note that DPO uses instruction fine-tuned model as the reference model during optimization, and there is no \(^{l}\) in the case of SSFT.

**Transformer block** The transformer block used in this study consists of an attention module followed by two MLP layers with a non-linear activation layer--either silu(Elfwing et al., 2018) or GELU(Hendrycks and Gimpel, 2016)--in between. The second MLP layer writes to the residual stream of the Transformer block (Elhage et al., 2021). Throughout this work, we denote \(_{L}\) and \(}_{L}\) as the parameters of the first and the second MLP layers of the \(L\)-th transformer block.

**Fundamental subspaces (Strang, 2009)** Let \(_{m n}:^{n}^{m}\) represent a matrix in \(^{m n}\). To avoid clutter, whenever possible, we denote \(W_{m n}\) by \(\). Let \((_{m n})=U_{m m}_{m n}V_{n}^{}\) represent a singular value decomposition of \(\), where \(U\) and \(V\) consist of the left and right singular vectors, \(\{_{i}^{m}\}_{i=1}^{m}\) and \(\{_{i}^{n}\}_{i=1}^{n}\), respectively, and \(\) is the diagonal matrix with its diagonal elements being the singular values \(_{i}\), sorted in descending order of magnitude (\(_{i}_{j}\) for \(i<j\)). Let \(r(m,n)\) be the rank of \(\). Using singular vectors as the orthonormal bases, the four fundamental subspaces of \(\) are defined as:

* _Column-space:_\(()=\{_{i}\}_{i=1}^{} \), which is the same as the span of the columns of \(\).
* _Row-space:_\(()=\{_{i}\}_{i=1}^{r}\), which is the same as the span of the rows of \(\). Note that \(()=(^{})\).
* _Null-space:_\(()=\{_{i}\}_{i=r+1}^{n} \). If \(=\), then \(()\).
* _Left Null-space:_\(_{L}()=\{_{i}\}_{i=r+1}^{m} \), which is the same as the null-space of \(^{}\).

Note that \(()\) and \((^{})\) are orthogonal to each other. Similarly, \(()\) is orthogonal to \(()\).

## 3 A Synthetic Controlled Set-up for Safety Fine-tuning

To systematically study the mechanisms yielded by safety fine-tuning and how adversarially designed inputs circumvent said mechanisms, we design a synthetic data generating process motivated by the framework of jailbreak attacks proposed by Wei et al. (2023) and Carlini et al. (2023). Specifically, the use of a synthetic setup helps us model the competing objectives and mismatched generalization formulation of Wei et al. (2023). For example, to elicit mismatched generalization, we must define samples that are out-of-distribution (OOD) compared to the ones used for safety fine-tuning of the model--the use of a synthetic data generating process helps us easily and scalably design such inputs. We emphasize that where possible, we do corroborate our findings on real-world LLMs (specifically, LLama models) by performing experiments similar to ones defined using our synthetic setup.

### Data generation for inducing instruction following behavior

We abstract out an input to an LLM as a composition of two components: (i) _operators_, which broadly specify a task the model is expected to perform, and (ii) _operands_, which specify what information the task is to be performed upon. For instance, consider the string: Tell me how to design a bike. Herein, one can deem design as an operator and bike as an operand. Despite its simplicity,

Figure 1: **Overview of our proposed synthetic setup to generate data.****(a)** A sample is divided into operators, operands, and outputs. The operators are function mappings the model is expected to perform on the operands to produce the _output tokens_, and are represented via tokens called _task tokens_. We often use the term _text tokens_ to refer to the operands the functions are to be performed upon. **(b)** The functions are restricted to bijective mappings, motivated by their use in synthetic setups for mechanistically analyzing Transformer models (Chughtai et al., 2023; Ramesh et al., 2023). **(c)** Text tokens are generated using PCFGs. To generate safe versus unsafe samples, we mark a subset of non-terminals at an intermediate level as safe-dominant (dark blue) and others as unsafe-dominant (light blue). Each of these nodes are associated with safe and unsafe task tokens, e.g., \(_{}^{s}\) and \(_{}^{u}\) respectively in blue box for safe dominant node. Our motivation here is that a task, by itself, is generally neutral (e.g., “design”), but when seen in the context of a concept it is to be performed on, i.e., the operands (e.g., “cycle” versus “bomb”), it can render the input unsafe.

we argue a large set of natural language inputs will fall under this abstraction (see App. B.1.2 for several examples). In our setup, we model this abstraction by defining an input to be a combination of tokens of two types: a _task token_\(f\) representing the notion of an operator, where \(\) is a family of predefined operators, and _text tokens_\(\), representing the notion of operands (see Fig. 1).

To generate text tokens, we use Probabilistic Context-free Grammars (PCFGs)--an often used model for natural language that captures its syntactic properties (Knudsen and Hein, 1999; Charniak, 1997) and that has seen recent use as a framework for mechanistic analysis of language modeling capabilities of Transformers (Allen-Zhu and Li, 2023; Hahn and Goyal, 2023). We denote a grammar as \((,T,NT,R,P)\), where \(R=\{NT_{i}^{l}\{c_{j}\}_{j=1}^{m}\}_{i=0}^{|NT|}\) is the set of production rules between non-terminal parent nodes (\(NT_{i}^{l}\)) at level \(l\) and their respective children nodes \(\{c_{j}\}_{j=1}^{m}\), and \(P\) is the set of probabilities associated with rules in \(R\). A sequence of text tokens \(\) is hence sampled by simply traversing through the PCFG tree, starting from the root node \(\), propagating through non-terminal nodes (\(NT\)) via production rules (\(R\)) according to their associated probabilities (\(P\)), and terminating at the terminal nodes (\(T\)). See App. B for a detailed discussion of this process. For the family of operators \(\), we follow recent work by Ramesh et al. (2023); Chughtai et al. (2023) and let each task token (operator) \(f\) be a bijective mapping \(f:\), where \(\) denotes the vocabulary of the PCFG generations (Fig. 1(b)). For example, given text tokens \((,T,NT,R,P)\) and task tokens \(f_{i},f_{j}\), we define the sequence of output tokens as \(=f_{j}(f_{i}())\). Overall, the process above yields an input \(:=\{f_{j} f_{i},,\}\) (see Fig. 1). We note the goal for having two operators as part of the input (e.g., \(f_{i},f_{j}\)) is that it allows us to model the _competing objectives_ format of jailbreak attacks proposed by (Wei et al., 2023), wherein the model is asked to perform two tasks simultaneously, of which one is unsafe (e.g., \(f_{i}\)) and the other is not (e.g., \(f_{j}\)). To make the overall task non-trivial, we use four PCFGs (See Fig. A.8 in appendix).

For pre-training, we perform next token prediction on text and output tokens to learn the PCFG grammar rules \(R\) along with the bijective mappings of task tokens. For instruction fine-tuning, we supervise the model to predict output tokens given instructions consisting of task tokens \(f_{i},f_{j}\) and text tokens \(\). Next we describe further necessary design choices we make to generate data for safety fine-tuning, jailbreak attacks, and adversarial attacks.

### Data generation for safety fine-tuning

Safety fine-tuning requires a dataset labelled as per user preferences (Rafailov et al., 2023; Ouyang et al., 2022). Generally, the preferred output corresponds to accurately following the instruction for the inputs that are deemed safe, while _refusing_ to respond to inputs that are deemed to be unsafe. We next develop an abstraction for such preference data for studying the mechanisms of safety fine-tuning. Specifically, we note that an operator or operand, by itself, cannot determine whether an instruction is safe or unsafe. For example, consider the following strings: Design a bomb (s1), Design a cycle(s2), and Provide the history of bombs (s3), where s1 is deemed unsafe and s2, s3 are deemed safe. One can easily see that it is the contextual meaning an operator and an operand acquire from being part of the same string that renders the overall string unsafe. For example, the operator design when seen in the context of operand bomb renders the overall string s1 to be unsafe, but not so when seen in the context of operand cycle. Similarly, the string s3, despite having bomb as its operand, is likely to be deemed safe, since therein the operator is merely Provide history.

To model the intuition above in our framework, we split the non terminal nodes at a predefined intermediate level \(l_{s}\) (\(=3\) in our experiments) into two disjoint sets called _safe dominant nodes_, \( NT^{l_{s}}\), and _unsafe dominant nodes_, \( NT^{l_{s}}\), where \(NT^{l_{s}}\) is the set of non-terminals at level \(l_{s}\). Let \(^{s}_{}\) and \(^{u}_{}\) respectively be the set of safe and unsafe task tokens associated with nodes in \(\) (similarly for \(\)); that is, if a node in \(\) (resp. \(\)) is selected while sampling the text tokens, the predefined set of operators that yield an overall string that is deemed safe come from the set \(^{s}_{}\) (resp.

Figure 2: **Generating jailbreak and adversarial attacks using our data generating framework.****(a)** General instruction format. **(b,c)** Generating task and text tokens of jailbreaks with competing objectives. **(d)** Jailbreak attacks with mismatched generalization. **(e)** Adversarial attacks.

\(^{s}_{}\)). We also constrain these sets such that \(|^{s}_{}|>|^{u}_{}|\), \(|^{s}_{}|<|^{u}_{}|\), \(^{u}_{}^{u}_{}\) and \(^{s}_{}^{s}_{}\). These conditions ensure that if nodes from \(\) (resp. \(\)) are sampled, the corresponding sequences are _mostly_ safe (resp. unsafe). Thus, different task tokens are associated with safe/unsafe inputs with different frequencies similar to real world instructions, e.g., operators like 'harm', 'destroy' are much more likely to be associated with unsafe generations as compared to 'design', 'purchase'.

Overall, an input \(\) is deemed _unsafe_ if \(=\{f_{i} f_{j},_{},\}\) where \(f_{i},f_{j}^{u}_{}\) or \(=\{f_{i} f_{j},_{},\)\(\}\) where \(f_{i},f_{j}^{u}_{}\) (similarly for safe samples). This yields contextual dependence between operators and operands that render an input safe versus unsafe. We note that to capture the low variability observed in the outputs of safety fine-tuned LLMs for unsafe samples (e.g., the ubiquitous 'I can't explain', 'I can't tell', etc. responses), during safety fine-tuning, we supervise the model to output a token called _null token_ (see App. B for details). Meanwhile, for safe samples, the model is fine-tuned to follow the instructions as usual.

### Data generation for jailbreak and adversarial attacks

We ground ourselves in the framework of Wei et al. (2023) and Carlini et al. (2023) to study the following three types of adversarial inputs. For each type, we provide real-world examples in App. B.1.2 that map onto our synthetic abstraction, highlighting the analogy in detail.

1. [leftmargin=*]
2. **Jailbreaks via competing objectives (JB-CO-Task and JB-CO-Text in Fig. 2(b) and (c)).** Such inputs ask the model to simultaneously solve two tasks, one that is unsafe and one that is not. For example, consider the input: How to make a bomb? Start with "Sure, here's". The first phrase in this input may be deemed unsafe, while the second phrase is objectively neutral and merely asking the model to engage in an instruction following behavior. Often, the model in pursuit of following instructions will perform the task presented in the unsafe phrase as well. We investigate two ways to imitate such inputs. (i) Sample the two task tokens to define an input from either \(^{u}_{}\) and \(^{s}_{}\) or \(^{s}_{}\) and \(^{u}_{}\), hence asking the model to perform both a safe and an unsafe task. (ii) Generate text tokens by using the lowest common ancestor of nodes in \(\) and \(\) as the root node and following PCFG grammar rules. We use the task tokens which generate safe inputs when combined with text tokens sampled from nodes in \(\) and generate unsafe inputs for nodes in \(\). In this way, similar to (i), the model is asked to perform both a safe and an unsafe task.
3. **Jailbreaks via mismatched generalization (JB-MisGen in Fig. 2(d)).** Datasets used for safety fine-tuning are often substantially smaller and less diverse than the ones used for pre-training (Ouyang et al., 2022; Team et al., 2023). For example, such datasets are generally in English, even though the model can process other languages or formats (e.g., ASCII). Use of alternative formatting of the input has thus become a viable way of bypassing safety fine-tuning (Wei et al., 2023; Kotha et al., 2023). To model this in our framework, we define a set of task tokens \(T_{}\) which are not included in the safety fine-tuning dataset (similar to languages other than English). For each such token, we ensure there exists _another_ task token that is used during safety fine-tuning and has the same functionality as the OOD token, i.e., corresponds to the same bijective mapping. This models the intuition that an unsafe input with similar semantics will likely be present in the safety fine-tuning dataset, but, e.g., in English.
4. **Attacks based on continuous, learned embeddings (Adv in Fig. 2(e)).** Motivated by Carlini et al. (2023), we append a set of embeddings to the input and optimize these embeddings via a white-box targeted attack on the model, akin to standard adversarial attacks in vision (Madry et al., 2018). The attack's strength increases as the number of embeddings is increased.

## 4 Investigating the Effect of Safety Fine-tuning

We now investigate the mechanism by which safety fine-tuning impacts the behavior of a model. For this, we investigate three main aspects of a model: (i) feature space; (ii) parameter space; and (iii) function sensitivity. For experiments on our synthetic data-generating process, similar to existing related works (Jain et al., 2023; Allen-Zhu and Li, 2023), we train minGPT(Karpathy, 2020) using medium \(_{M}=10^{-4}\) and small \(_{S}=10^{-5}\) learning rates. See App. B.1.3 for further details on model training, selection, and cross-validation of the hyperparameters. To corroborate our claims, where possible, we run analogous experiments on Llama models (Touvron et al., 2023; Card, 2024) by defining a dataset of 500 safe and unsafe natural language instructions that are structurally similar to our synthetic data (see App. B.2 for details). Specifically, we use Llama-2 7B and Llama-3 8B as pretrained models and Llama-2 chat 7B and Llama-3 chat 8B as their corresponding safety fine-tuned variants.

Our analysis focuses on MLPs in each Transformer block. Specifically, we analyze the activations at the output of this layer (after GELU) in Sec. 4.1, and its parameters and pre-activations in Sec. 4.2. The overall model's sensitivity to input perturbations is analyzed in Sec. 4.3. In all plots, green and red colors are used to denote the analysis corresponding to the safe and unsafe samples, respectively.

### Clustering of safe versus unsafe samples' activations: Analyzing activation space

We first analyze how safety fine-tuning affects activations of safe versus unsafe samples.

[background=0.5cm] **Observation 1**

Safety fine-tuning leads to formation of clusters of activations corresponding to safe versus unsafe samples, where the separation between clusters increases as better methods are used.

Experimental setupLet \(_{L}^{o}()[i]\) be the \(L\)-th layer's output activation corresponding to the \(i\)-th token of an input sequence \(\). We define the average activation corresponding to the \(q\)-th output token as \(}_{L}^{o}()[q]=_{i=k}^{q+k-1} _{L}^{o}()[i]\), where \(k\) is the index of the last text token. If \(_{S}\) and \(_{U}\) are two datasets comprised solely of inputs with safe versus unsafe instructions, we define the _mean_ safe and unsafe activation at layer \(L\) as follows.

\[_{L}^{S}=_{S}|}_{_{S}} }_{L}^{o}()[q],_{L}^{U}=_{U}|}_{ _{U}}}_{L}^{o}()[q].\] (1)

Now, if the model distinguishes between safe versus unsafe inputs at the level of intermediate layers' activations, we claim we will see two explicit clusters formed for safe versus unsafe inputs. To assess the same, we define the following measure that computes the Euclidean distance of a sample \(x\)'s activations from the mean unsafe versus safe activation.

\[(,_{L}^{S},_{L}^{U})=\|}_{L}^{o }()[q]-_{L}^{U}\|_{2}-\|}_{L}^{o}()[q]- _{L}^{S}\|_{2}\] (2)

The measure above should be positive for safe inputs and negative for the unsafe ones. When analyzed over a large number of inputs, it helps us gauge how clustered the activations corresponding to safe versus unsafe inputs are. Results are reported in Fig. 3. We find that activations--especially in the deeper layers--are indeed clustered depending on whether they come from safe versus unsafe inputs. Furthermore, in Fig. 3 (top), we observe in our synthetic setup that as the strength of the safety fine-tuning protocol increases (e.g., DPO and Unlearning compared to SSFT or DPO with medium learning rate \(_{M}\) compared to DPO with small learning rate \(_{S}\)), separation between the clusters increases, where separation is defined as the difference between the average value of \(\) for safe versus unsafe samples. We find similar results using Llama-2 and Llama-3 models as well (see Fig. 3 (below)), indicating our findings translate to more realistic settings.

We also investigate the impact of safety fine-tuning on the'shape' of safe and unsafe feature clusters by analyzing singular values/vectors of their corresponding empirical covariance matrices \(^{S}\) and

Figure 3: **Safety fine-tuning encourages separate cluster formations for safe and unsafe samples.** x-axis: layer number, y-axis: average \(\) in Eq.2. **(Top)** Results using the synthetic setup. **(Bottom)** Results on Llama. Llama-2 chat 7B and Llama-3 chat 8B correspond to safety fine-tuned models.

\(^{U}\), respectively (refer App C.3.1). As clearly observed in Fig A.19, it is the top singular value of \(^{U}\) that is impacted the most as the safety fine-tuning progresses, however, the singular values of \(^{S}\) remain more or less the same. The \(_{1}(^{U})\) scales to a point where it constitutes nearly \(62\%\) of the nuclear norm of \(^{U}\), whereas this value is merely \(12\%\) for \(_{1}(^{S})\). This indicates that safety fine-tuning reshapes the cluster of unsafe features in a way that there remains a single dominant direction. However, the shape of the cluster corresponding to safe samples is not impacted much.

### What drives the clustering of safe and unsafe samples: Analyzing parameter changes

To identify what drives the formation of separate clusters of safe and unsafe samples, we evaluate precisely how model parameters change as a consequence of safety fine-tuning. Since Fig. 3 indicates clustering is strongest in deeper layers, we primarily analyze the MLP layers of the last two transformer blocks in this section. In particular, let \(}\) and \(}\) denote the instruction and the safety fine-tuned parameters of the first MLP layer of the \(L\)-th transformer block (\(L\) is intentionally omitted in notation to avoid clutter). Then, the change in parameters due to safety fine-tuning--or what we will often call "transformation"--is defined as \(=}-}\).

Experimental setupLet \(\{_{i}\}_{i=1}^{r}\) and \(\{_{i}\}_{i=1}^{r}\) be the top \(r\) left singular vectors and singular values of \(}\), where \(r\) denotes the empirical rank of \(}\), which is defined as the minimum value of \(k\) such that \(99\%\) of variance is preserved, i.e., \(_{i=1}^{k}_{i}^{2} 0.99\|}\|_{}^{2}\). Similarly, let \(\{}_{i}\}_{i=1}^{t}\) be the top \(t\) left singular vectors of \(\) where \(t\) is the empirical rank of \(\). The projection matrix for the column-space of \(}\) is defined as \(:=_{i=1}^{r}_{i}_{i}^{}\). Let \(_{i}\) be the angle between \(}_{i}\) and \(}_{i}\). It is easy to see that \(}_{i}(_{i})\) provides the projection of \(}_{i}\) on \((^{}})\) since \((^{}})\) is orthogonal to \((})\). Since \(}_{i}\) is unit norm, we can plot the magnitude of projection of \(}_{i}\) on the space \((^{}})\) by evaluating \((_{i})\). Results for blocks 5 and 6 are shown in Fig. 4 for the PCFG-based experiments, and in Fig. A.17 for Llama models. A baseline model fine-tuned using standard cross-entropy loss to follow instructions in the usual way is also evaluated (shown in dotted lines in Fig. 4). Our results indicate that for safety fine-tuned models, the magnitude of projected component onto \((^{}})\) is very large, especially when compared to the baseline. This implies \(\) and \(}\) are nearly orthogonal to each other. _Thus, a sample processed by \(\) will have a component that cannot be computed by \(}\) itself, hence yielding two broad sets of activations corresponding to samples which are processed by \(\) versus not._ To make this more concrete, we next evaluate which samples are likely to be processed by \(\) by analyzing its row space.

Experimental setupWe analyze pre-activation for the last text token, i.e., one corresponding to the first output token prediction. The pre-activation is normalized since our goal is to primarily assess its alignment with the row-space of \(\). Specifically, to capture the effect of \(\) on a given

Figure 4: **Safety fine-tuning learns transformations \(\) whose column-space is more aligned with \((^{}})\). y-axis: Magnitude of projected component of left singular vector \(}_{i}\) on \((^{}})\), x-axis: Index of left singular vectors, sorted by increasing magnitude of projected component.**

[MISSING_PAGE_FAIL:8]

For a given real-valued function \(_{}:\) and input \(\), we define the local Lipschitzness of \(\) at \(\) as \(_{}()=\|_{}_{}()\|_{2}\).

Experimental setupWe consider \(=*{argmax}_{j}h_{}()[k](j)\), where \(h_{}()[k](j)\) is the \(j\)-th logit predicted at the end of text token index, denoted by \(k\). The sensitivity is obtained corresponding to the most confident output. Parameters \(^{}\) and \(^{}\) are chosen depending on the model under consideration. The histograms of \(_{}()\) for safe (green) and unsafe (red) samples are shown in Fig. 6. _We can clearly observe that the sensitivity of the safety fine-tuned model is much lower compared to instruction fine-tuned model for unsafe samples, especially when DPO and Unlearning are used for fine-tuning._ This makes sense as, for unsafe samples, the variation in the preferred output strings in safety fine-tuning dataset is much less compared to that of safe samples: e.g., preferred outputs for unsafe samples are generally 'NULL', 'I can't assist', etc. The consequence of this decrease in sensitivity is that it will be _relatively_ more difficult to craft jailbreaks and adversarial attacks for more effective safety fine-tuning protocols, since models witness a stronger decrease in Lipschitzness under those protocols. We validate this claim in Tab. A.1 as well, showing that crafting jailbreaks and adversarial attacks is more difficult for DPO and Unlearning as compared to SSFT.

## 5 Evading the Safety Mechanism: Jailbreak and Adversarial Inputs

Having established and investigated the mechanism via which safety fine-tuning leads the model to refuse to process unsafe inputs, we can now analyze precisely why jailbreaks and adversarial attacks are still able to induce unsafe responses from the model.

Experimental setupWe use our instantiation of jailbreaks and adversarial attacks defined in Sec. 3.3, and motivated by the works of Wei et al. (2023) and Carlini et al. (2023). As shown in Tab. A.1, for DPO with \(_{M}\), the JB-CO-Text attack yields the highest success rate (\(97.2\%\)), whereas the JB-CO-Task attack yields the lowest one (\(31.5\%\)). This trend is also observed for other safety fine-tuning methods (see Tab. A.1). For further analysis, we only consider the _successful_ attacks.

**(i) Feature space.** Building on Sec. 4.1, we analyze the separation between clusters induced by safe and unsafe samples, _but use jailbreaks and adversarial attacks instead of unsafe samples this time_. Results are shown in Fig 7 (top). We find the cluster separation between safe samples and attacked samples decreases in the feature space as the strength of attack increases, i.e., the decrease is higher for JB-CO-Text and JB-MisGen, which are stronger attacks (See Tab. A.1) as compared to JB-CO-Task. We observe a similar trend for adversarial attacks as well. This indicates _with increase in attack strength, adversarial inputs yield features that are similar to safe samples._ We note that concurrent work by (Ball et al., 2024) provide additional evidence in support of these results on larger models like Vicuna-13B.

**(ii) Function space.** Building on Sec. 4.3, we analyze the empirical Lipschitz constant for jailbreak and adversarial attacks in Fig. 7 (middle row). Clearly, with increase in attack strength, the histogram for jailbreaks starts to overlap with the histogram corresponding to the safe samples, showing that the model's local sensitivity also starts to lie between attacked and safe samples. Similar to the feature

Figure 6: **Lipschitz constant, hence the sensitivity of the model, decreases for unsafe samples and increases for safe samples after safety fine-tuning. The decrease is higher for stronger approaches, i.e., unlearning and DPO. x-axis: local Lipschitzness, y-axis: number of samples.**

space analysis above, the function sensitivity analysis also highlights that with the increase in attack strength, the adversarial samples start producing representations similar to safe samples.

**(iii) Parameter space.** To tie everything together and explain the similarity of features between jailbreak and safe samples, we finally build on Sec. 4.2 and analyze the impact of \(\) on jailbreak and adversarial inputs. Specifically, we analyze the alignment of pre-activations \(\) corresponding to these inputs with the row space of \(\) (same setup as discussed in Fig. 5). Results are shown in Fig. 7 (bottom). We observe that _unlike unsafe samples, \(\) does not impact jailbreak / adversarial samples noticeably_: e.g., see Fig. 5, where unsafe samples have a much higher alignment with row space of \(\) compared to safe ones, versus results on JB-CO-Text inputs in Fig. 7 (bottom), where we find the alignment is essentially the same! As we showed before, it is the impact of \(\) that leads to a distinction between how safe versus unsafe samples are processed; hence, results above suggest the model will process successful jailbreak / adversarial samples as if they were safe. We provide additional fine-grained analysis related to our observations above for different safety fine-tuning methods and layers in App. C.3.4 (for jailbreak attacks) and C.3.8 (for adversarial attacks).

## 6 Conclusion

We proposed a synthetic data generation framework to systematically and efficiently analyze common safety fine-tuning methods and craft jailbreak attacks. Using our framework, we showed that safety fine-tuning encourages the formation of separate clusters for safe and unsafe samples while making the model significantly less sensitive towards unsafe ones. We further found that the clustering effect in model's activation space can be explained by the weight space analysis, where the learned update was found to be specialized in projecting the unsafe samples onto the null space. These updates were not able to generalize well against samples for jailbreak and adversarial attacks, which resulted in their activations being more similar to safe samples than the unsafe ones. Hence bypassing the safety mechanism learned by the model. Wherever possible, we also showed via experiments on Llama models that our claims directly transferred to more realistic setups.

Figure 7: **Analyzing jailbreaks and adversarial inputs.** Building on the safety mechanism established in Sec. 4, we evaluate how jailbreak and adversarial inputs evade this mechanism by repeating our analysis from that section. We use brown color to represent the jailbreaking and adversarial inputs. **Top row (Feature space).** Similar to Fig. 3, we analyze average \(\) (see Eq. 2) as a function of layers in the model. As the strength of attacks used increases, we see separation between clusters decreases. **Middle row (Function space).** The distribution of the local Lipschitzness of samples similar to Fig.6. In both rows, the difference between safe and unsafe examples (in the first column) decreases after jailbreak and adversarial attacks. **Bottom row (Parameter space.)** Projection of unit-norm pre-activation \(\) on \(_{i}_{i}^{}\). Activation corresponding to jailbreak and adversarial samples are not influenced significantly by \(\).