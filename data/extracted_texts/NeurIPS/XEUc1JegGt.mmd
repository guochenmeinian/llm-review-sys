# An Inductive Bias for Tabular Deep Learning

Ege Beyazit

Amazon

beyazit@amazon.com

&Jonathan Kozaczuk

Amazon

jonkozac@amazon.com

&Bo Li

Amazon

booli@amazon.com

&Vanessa Wallace

Amazon

vwall@amazon.com

&Bilal Fadallah

Amazon

bhf@amazon.com

These authors contributed equally.

###### Abstract

Deep learning methods have achieved state-of-the-art performance in most modeling tasks involving images, text and audio, however, they typically underperform tree-based methods on tabular data. In this paper, we hypothesize that a significant contributor to this performance gap is the interaction between irregular target functions resulting from the heterogeneous nature of tabular feature spaces, and the well-known tendency of neural networks to learn smooth functions. Utilizing tools from spectral analysis, we show that functions described by tabular datasets often have high irregularity, and that they can be smoothed by transformations such as scaling and ranking in order to improve performance. However, because these transformations tend to lose information or negatively impact the loss landscape during optimization, they need to be rigorously fine-tuned for each feature to achieve performance gains. To address these problems, we propose introducing _frequency reduction_ as an inductive bias. We realize this bias as a neural network layer that promotes learning _low-frequency representations_ of the input features, allowing the network to operate in a space where the target function is more regular. Our proposed method introduces less computational complexity than a fully connected layer, while significantly improving neural network performance, and speeding up its convergence on 14 tabular datasets.

## 1 Introduction

Despite being one of the simplest and most natural ways to describe data, tabular data with heterogeneous columns poses a significant challenge for deep learning models. Recently,  benchmarked various deep learning approaches against tree-based models and showed that tree-based models tend to perform the best on tabular data. Nevertheless, novel neural network architectures provide several advantages over classical machine learning methods. Specifically, neural networks operate on differentiable loss functions, are able to learn meaningful and high-dimensional data representations, and can scale to large datasets. In addition, the rapid advances of neural networks in generative image and language modeling clearly indicate their capability to encode complex information . In order to fully utilize these benefits on tasks that include tabular datasets, identifying and addressing the reasons behind neural networks' lack of performance on this form of data is essential.

Several influential research efforts on synthetic and image data point out that, despite being universal approximators, neural networks have a spectral bias, i.e., they tend to learn the low frequency components of a function much more easily than higher frequency components . In addition, recent empirical studies on tabular deep learning provide insights on why neural networks typicallyunderperform on tabular data, including the latter's tendency to feature irregular target functions [13; 5]. Notably, the feature spaces of the tabular datasets considered in these studies are _heterogeneous_. Unlike images in tabular form where columns correspond to the same signal on different sensors, columns of tabular deep learning tasks typically represent features of different nature. These features are engineered to be informative as individuals (or subsets) and tend to have different statistical properties than each other.

In this work, we connect these lines of inquiry and claim that the spectral bias of neural networks may prevent them from learning the information present in the high frequency components of heterogeneous tabular datasets. We provide evidence to support this claim by analyzing the Fourier components of image and tabular datasets, as well as neural networks. To best of our knowledge, we present the first analysis on the relationship between spectral bias and tabular deep learning. We suggest that transforming tabular features into _low-frequency representations_ may alleviate the negative impact of spectral bias. Notably, such transformations need to be performed in a way that balances the benefits of reduced frequency against potential information loss and additional negative effects on optimization (e.g., creating large gradients along certain directions) that can arise when altering the representation of the data. Figure 1 illustrates an example of the analyses we conduct. From the left panel, it can be observed that many tabular datasets tend to have higher power in their high frequency components, compared to some of the popular image datasets. We extend this comparison and present additional observations in Appendix C. The right panel depicts the impact of a frequency-reducing transformation on the functions learned by a multilayer perceptron (MLP) and Extreme Gradient Boosting (XGB)  on synthetic data. From this panel, it can be observed that the MLP is able to fit the target function better after the frequency-reducing transformation. Details of this analysis are provided in Appendix B.

Driven by these observations, we propose using _frequency reduction_ as an inductive bias during neural network training. We apply this bias by introducing a trainable layer that promotes learning lower frequency feature mappings. We show that several commonly-used data processing methods impact the frequency spectrum of the target function as well as the resulting neural network learned from the data, however, their impact on performance varies with the dataset. Our proposal leads to the best performance, converges faster across all datasets considered, and introduces minimal computational complexity.

## 2 Related Work

**Spectral Analysis of Neural Networks.** Spectral properties of neural networks have been extensively studied. [22; 27] rely on Fourier analysis to investigate the learning behavior of neural networks,

Figure 1: **(Left)** Due to their heterogeneous nature, tabular datasets tend to describe higher frequency target functions compared to images. The spectra corresponding to image datasets (curves in color) tend to feature lower Fourier amplitudes at higher frequencies than heterogeneous tabular datasets (cyan region). **(Top Right)** Data with labels varying over small distances are difficult for neural networks to learn, but easy for tree-based methods. **(Bottom Right)** Ranking is a transformation that redistributes the data and increases the scales over which the feature labels vary. This transformation effectively lowers the frequency of the target function, making it easier for a neural network to learn.

observing that neural networks learn lower frequency components of target functions first. This phenomenon is referred to as _spectral bias_ or the _frequency principle_.  study spectral bias on data with non-uniform density by examining the NTK for deep fully-connected networks, observing that both data density and frequency impact the gradient descent behavior.  show that during neural network training, error terms from different frequencies are controlled by the eigenvalues of the NTK.  decompose the reconstruction error of neural networks into low and high frequency components with respect to a cutoff frequency, and use these components to define a metric that quantifies spectral bias.  study the spectral bias of coordinate-based MLPs for modeling functions in low dimensions. Using NTKs, they show that mapping input coordinates to Fourier features  enables learning higher frequency information. Notably, they focus on the frequency described by pixels of a given image, rather than the frequency of the target function described by the whole dataset. The existing studies mainly focus on advancing the understanding of spectral bias on synthetic and image datasets, however, real-world tabular datasets are not considered.

**Tabular Deep Learning.** Due to neural networks' inferior performance on tabular data compared to tree-based models, various approaches have been proposed to close this performance gap.  take raw tabular data as input and use sequential attention to choose the set of features to use for every step of inference. This instance-wise feature selection is shown to improve performance and enable interpretability. [28; 2] extend self and semi-supervised learning to tabular data by introducing pretext tasks that aim to optimize a model to recover input samples from their corrupted variants.  leverage row- and column-wise attention combined with contrastive pre-training.  embed numerical features using linear transformations and categorical features using lookup tables, and feed these embeddings to a transformer.  show that using hyperparameter tuning, one can find a strong combination of 13 regularizers to be used with a simple MLP to outperform other tabular deep learning methods, as well as tree-based methods such as XGB.  propose individually embedding numerical features onto high-dimensional spaces before jointly using them in a model. The authors explore approaches such as piecewise linear embeddings that use bins selected by decision trees, and periodic activations to transform numerical features into high-dimensional representations.

**Discussion.** In most learning tasks that include images, audio or text, deep neural networks provide state-of-the-art performance. Breakthroughs in these domains are strongly connected to the design choices that exploit the salient characteristics of the corresponding data modality (e.g., convolutional layers in computer vision). On the other hand, existing studies on tabular deep learning mainly focus on applying approaches that are successful in other domains to improve performance [1; 12; 2; 25]. These studies do not rigorously investigate the fundamental reasons behind the performance gap between neural networks and tree-based methods. As a result, in order to gain performance, these approaches tend to sacrifice simplicity by complicating training and hyper-parameter tuning significantly. Such complexity causes these models to perform inconsistently across different studies, as utilizing the full potential of these models can be challenging. For example,  provide an extensive set of benchmarks, showing that tree-based approaches significantly outperform various tabular deep learning methods that originally claimed to outperform tree-based models. Similarly,  show that simple MLPs perform better than TabNet, an architecture that is tailored for tabular deep learning. As a result, instead of focusing on designing complex models to improve performance, we aim to identify and address the fundamental weaknesses of neural networks when learning from tabular data. We show that our approach improves performance of neural networks significantly without introducing additional complexity for fine-tuning, and helps them to converge faster.

## 3 Methodology

### Background

Consider classification tasks defined on a tabular dataset \(D=\{(_{i},y_{i})\}_{i=1}^{N}\) where \(_{i}^{n}\) and \(y_{i}\{0,1\}^{m}\) is the corresponding label. The task is to learn a classifier \(f:^{n}\{0,1\}^{m}\). For simplicity, set \(m=1\) (i.e., binary classification), and let \(y=1\) correspond to the minority class in all cases. In Section 4, we provide experimental results and analyses for multi-class datasets as well, by extending this approach via one-vs-rest.

We are interested in the frequency spectra of various functions, including target functions implicitly defined by the data, for which \(f(_{i})=y_{i}\). Since we only have the labels defined at the datapoints supplied in each dataset, we rely on spectral analysis methods that take an arbitrary set of points as input and output a measure of the strength of fluctuations at particular scales. One such method is a generalized Fourier transform known as the non-uniform discrete Fourier transform (NUDFT) . For a function \(f()\) and a set of points \(X=\{_{i}\}_{i=1}^{N}\), the NUDFT defines a function of frequency vector \(^{n}\) as

\[_{X}()=_{i=1}^{N}f(_{i})e^{-2  i_{i}}.\] (1)

Equation (1) can be interpreted as an approximation of the multidimensional continuous Fourier transform (FT) of \(f()\) weighted by the empirical distribution represented by the datapoints \(X\), further discussed in Appendix A.  demonstrate the spectral bias of neural networks in terms of the NUDFT.

The Fourier transform approximated in Equation 1 evaluated at a given \(\), corresponds to the coefficient of the \(e^{2 i}\) term in a projection onto complex exponentials. Therefore, a natural definition of one function being _higher frequency_ than another corresponds to the former having larger-magnitude Fourier amplitudes away from a specified low-frequency regime than the latter. We can make this comparison concrete by evaluating amplitudes along specific directions in \(k\)-space. Defining \(=k}\), with \(}\) a unit vector in \(^{n}\), we can compare the size of the squared NUDFT amplitudes, \(P_{f}()=|_{X}()|^{2}\), for functions along a given \(\) direction by computing the sum

\[S_{}}[f]=^{2}}_{n=0}^{(k_{*}-k_{0})/  k}P_{f}((k_{0}+n k)}) k,\] (2)

where \(f\) is a function of interest. Here, \(\) is the mean of \(f()\) (i.e., \(y\)) evaluated on the the points in \(X\), so that the scales at low frequencies are comparable for any given dataset (see Appendix A.2). Equation (2) can then be used to compare spectra: if \(S_{}}[f_{1}]>S_{}}[f_{2}]\) for two functions \(f_{1}\) and \(f_{2}\) defined on \(^{n}\), we say that \(f_{2}\) is a _higher frequency function_ than \(f_{1}\) along the direction \(}\), in the sense that it has more normalized signal energy (variance) in the region of interest, and refer to _frequency reduction_ as reducing \(S_{}}()\). In the above expression, \(k_{0}\) defines the boundary between the low- and high-frequency regions, and \(k_{*}\) is a cutoff frequency, discussed in Appendix A.4. Notably, Equation (2) only considers the magnitude of Fourier amplitudes along one dimension of \(^{n}\) at a time. In Appendices A.3 and A.6, we discuss an alternative approach that works in the full feature space \(^{n}\) and show that using (2) to evaluate along principal components (PCs) yields similar results to working in \(^{n}\), while being more computationally efficient. Therefore, we rely on evaluations along PCs.

### Spectral Analysis of a Class of Target Functions

The NUDFT, when evaluated using the labels \(f(_{i})=y_{i}\), can be understood in terms of the continuous Fourier transform of a class of target functions fitting the data. Given a binary classification task described by a dataset \(D=\{(_{i},y_{i})\}_{i=1}^{N}\), a typical way to train a neural network \(f\) is minimizing the empirical risk \(1/N_{i=1}^{N}(y_{i},f(_{i}))\), where \(\) denotes the cross-entropy loss. Assuming that \(D\) maps each unique \(_{i}\) to a single \(y_{i}\), the cross-entropy loss without explicit regularization terms is exactly minimized for any function \(f()\) such that \(f(_{i})=y_{i} i\{1,,N\}\). This defines a set of equalities satisfied by any ideal target function perfectly fitting the training set2, but does not further specify the form of the function away from the training instances. Indeed, there is no unique definition of the target function given only a finite set of points. However, we can parameterize a class of target functions minimizing the loss by convolving the target values at known data points with a given envelope function, \(G(,^{})\), that falls off sufficiently rapidly away from the points \(\{_{i}\}\):

\[[,G]_{i=1}^{N}_{^{n}}y_{i}G(,^{};_{i})^{n}(^{}-_{i})d^{n} ^{},\] (3)

where \(^{n}()\) is the Dirac delta function in \(^{n}\) and \(_{i}\) denotes the parameters defining the envelope for the point \(_{i}\). A typical loss function (neglecting regularization terms) will be exactly minimizedprovided \(G(_{i},_{i};_{i})=1\) and \(G(_{i},_{j};_{i})=0\,\,_{i},_{j} D\). Perhaps the simplest such envelope is a multi-dimensional generalization of the boxcar function

\[B(^{},;_{i})=_{j=1}^{n} ^{}-x_{j}}{_{i}},\] (4)

where \((-a}{})=1\) if \(|a^{}-a|</2,=0.5\) if \(|a^{}-a|=/2\), and vanishes otherwise. Notably, \(\) corresponds to an upper bound on the \(L_{}\) distance between \(a\) and \(a^{}\), in order for them to be assigned to the same value of \(y\). The target function defined by inserting Equation (4) into Equation (3) is then

\[f()=_{i=1}^{N}y_{i}_{j=1}^{n}_{^{n}}(-x_{j}^{}}{_{i}})(x_{j}^{}-( _{i})_{j})d^{}.\] (5)

Provided that \(_{i}\) satisfies

\[|_{i}-_{j}|_{}_{i}\,_{j}  D,\] (6)

for each \(_{i}\), this function minimizes the cross-entropy loss on the training set, since its values at each point are simply the corresponding label. This corresponds to _tiling_ the feature space with \(n\)-dimensional hypercubes of edge lengths \(_{i} i\), centered on each datapoint \(_{i}\) and taking the target function to be constant within that region with value \(y_{i}\). This is the simplest possible choice for the envelope function in that it does not assume any spectral features present in the target functions on distances smaller than \(_{i}\).

We can now use the continuous Fourier transform to analyze the frequency spectrum of the target function defined in Equation (5). Denote the Fourier transform of a function \(g()\) as \(_{}[g()]()\). By the convolution theorem, we have

\[_{}[f()]()= _{i=1}^{N}y_{i}e^{-2 i_{i}} _{i}^{n}(_{j=1}^{n}(k_{j}_{i})),\] (7)

where \((x)=( x)/( x)\). Different choices for the \(_{i}\) correspond to different assumptions about the target function away from the datapoints. One simple class of target functions is obtained by taking a common \(_{i}==\{|_{i}-_{j}|\}_{i=1,j=i +1}^{N-1,N}\). This choice satisfies Eq. (6) and in the limit \(||_{} 1/()\), yields

\[_{}[f()]()^{n}_{i=1 }^{N}y_{i}e^{-2 i_{i}}=N\,^{n}\,_{X}()\] (8)

where the approximation holds up to terms \((||^{2}^{2}^{2})\) (here \(||\) denotes Euclidean distance). The RHS above is the NUDFT up to a constant dimensionful scaling factor, which will drop out when using a procedure like Equation (2) that normalizes the amplitudes to the value at \(k=0\). Therefore, the NUDFT reproduces the spectral properties of a simple class of target functions defined by Equation (3) for frequencies below the scale set by the inverse minimum nearest-neighbor separation of points in the dataset. We implicitly enforce the latter restriction by cutting off our analysis at \(k_{*}\), as discussed in Appendix A.4. Note that other choices for the \(_{i}\) result in different target functions with spectra that can be analyzed by appropriately modifying the NUDFT.

In order to compare against the same quantity in all analyses, we also utilize Equation (1) for analyzing the frequency spectra of classifiers. A similar argument holds, and the corresponding results can be interpreted as analyzing the spectra of a classifier coarse-grained by convolving the neural network predictions at the datapoints with a narrow envelope as in Equation (5). In all cases, the classifier spectra defined in this way converge to that of the target function as training progresses, up to deviations caused by spectral bias (see also  for similar results). This suggests that high-frequency target functions, defined as above, are best fit by high-frequency classifiers, and that transformations affecting the target function spectrum will impact the frequency spectrum of the corresponding classifier.

### Case Study: Impact of Transformations on Target Function Spectra

Most neural networks need their input to be transformed in order to converge faster and generalize better.  show that these transformations serve to regulate layer input/output gradient statistics and/or the conditioning of input/gradient covariance matrices. A typical example of such transformations is scaling the features so that they vary over the unit interval \(\), i.e., _min-max_ scaling. Different choices of transformations can dramatically impact the target function frequency spectrum, and therefore the spectrum of the network that best fits the data with a given loss function. Due to the spectral bias of neural networks, we expect that using transformations that _reliably_ reduce the high frequency components of the target functions result in better performance (see Appendix D for further discussion and illustration of this point).

Let us consider two types of such transformations, \(\) and \(\), that are applied to each feature independently. Consider a vector \(^{}^{N}\) with elements \((^{})_{r}\{(_{1})_{},,(_{N})_ {}\}\) such that \((^{})_{r}(^{})_{s}\: r s\). Then

\[((_{i})_{}) =\{r|(^{}) _{r}=(_{i})_{}\},\] (9) \[((_{i})_{}) =a(_{i})_{},\] (10)

for \(_{i} X\) and \(a\). Equation (9) simply maps a feature to the range \(\), proportional to its position when ranked by magnitude along with the other values in the dataset. For any \(()_{}\) not in the original dataset, \(\) returns a value interpolated between \((_{i})_{}\) and \((_{j})_{}\) for the \((_{i})_{}\), \((_{j})_{} X\) nearest to \(()_{}\) such that \((_{i})_{}<()_{}<(_{j})_{}\). Both \(\) and \(\) may alter the frequency spectrum of the target function described by a dataset by either stretching/squeezing fluctuations while preserving relative distances between points or neglecting distance information altogether and forcing the points to be uniformly distributed along a direction, respectively.

Using Equation (2), we present the high-frequency Fourier amplitudes (i.e., \(||>0.5\)) of the target functions of 8 tabular binary classification datasets with different transformations and their impact on performance across 10 random seeds in Table 1. Here, we implement scale as standardization, such that \(a\) in Equation (10) is the inverse standard deviation of the feature in the training set population. From the top panel of Table 1, it can be observed that \(\) and \(\) consistently reduce the frequency of the target functions of all datasets, although by different relative amounts.

The bottom panel of Table 1 shows that, overall, lowering target function frequency is correlated with better NN performance: converting the averaged \(S_{}\) and NN performance for unit length, \(\), and \(\) to their ranks within each dataset and comparing yields a Spearman's rank correlation coefficient of \(-0.7\) with a \(p-\)value of \( 3 10^{-4}\). However, this correlation is not exact: large reduction in high-frequency Fourier amplitudes may not always result in significant performance improvements. This is because the amount of high frequency information encoded by each feature is different across datasets. Also, scaling up the feature values by a significant amount reduces the high-frequency energy, but can also negatively impact optimization by generating large gradients

   & house & electricity & phoneme & Magic-Telescope & bankMarketing & MiniBooNE & abbert & california \\  unit length & 1.31 & 1.46 & 1.66 & 0.13 & 2.01 & 16.24 & 1.74 & 1.5 \\ rank & 0.11 & 0.18 & 0.44 & 0.07 & 0.74 & 0.34 & 0.17 & 0.3 \\ scale & 0.01 & 0.18 & 0.06 & 0.01 & 0.17 & 0.35 & 0.04 & 0.03 \\ selrank & 0.11 & 0.22 & 0.58 & 0.07 & 1.28 & 0.34 & 0.29 & 0.32 \\    & house & electricity & phoneme & Magic-Telescope & bankMarketing & MiniBooNE & abbert & california \\  unit length & 88.94 \(\) 0.17 & 85.28 \(\) 0.12 & 87.78 \(\) 0.28 & 87.84 \(\) 0.18 & **90.56 \(\) 0.11** & 90.78 \(\) 0.04 & 66.35 \(\) 0.04 & 88.51 \(\) 0.15 \\ rank & 89.61 \(\) 0.14 & 86.10 \(\) 0.19 & 88.0 \(\) 0.46 & 87.9 \(\) 0.24 & 90.41 \(\) 0.08 & **95.01 \(\) 0.06** & 66.21 \(\) 0.08 & 88.51 \(\) 0.17 \\ scale & 89.65 \(\) 0.16 & 85.19 \(\) 0.13 & **88.64 \(\) 0.21** & 87.88 \(\) 0.23 & **90.56 \(\) 0.13** & 94.04 \(\) 0.04 & **66.48 \(\) 0.05** & **85.53 \(\) 0.22** \\ selrank & **89.91 \(\) 0.16** & **87.04 \(\) 0.17** & 88.15 \(\) 0.32 & **88.14 \(\) 0.27** & **90.56 \(\) 0.13** & 94.94 \(\) 0.05 & 66.28 \(\) 0.06 & 88.48 \(\) 0.2 \\  

Table 1: (**Top**) Normalized sum of high-frequency Fourier amplitudes, \(S_{}\), for various binary classification datasets and transformations, averaged over the 1st principal component direction after each transform is applied. Ranking and scaling individual features tends to significantly reduce the high-frequency energy of tabular datasets relative to features normalized to unit scale. For an extended discussion, see Appendix A.6. (**Bottom**) Accuracy measurements corresponding to different transformations. Overall, lower frequencies are correlated with better performance. However, the benefits of frequency reduction is transformation- and dataset-dependent.

during training. Finally, \(\) discards relative distance information between points in favor of frequency reduction, and also impacts optimization as it produces non-centered features. Therefore, it is important to simultaneously balance the effect of a given frequency-reducing transformation with other potentially negative impacts on the loss landscape or due to information loss.

These observations indicate that there is not a simple one-size-fits-all transformation that _reliably_ and consistently reduces the high-frequency spectral energy that results in improved network performance. However, with this trade-off between frequency reduction and potential loss of information or effects on the loss landscape in mind, we can take a first step in improving over the \(\) and \(\) transformations by doing them _selectively_. For example, to balance the aforementioned trade-off we consider a selective version of \(\) defined as

\[(()_{})=\{( ()_{}), S_{}_{}}[\{(( _{i})_{}),y_{i}\}_{i=1}^{N_{}}] S_{}_{}}[\{(_{i})_{},y_{i}\}_{i=1}^{N_{}}] \\ ()_{},.\] (11)

where the datapoints in the conditions are those in the training set and \(\). \(\) applies \(\) only if it reduces the high frequency energy of the target function defined for the starting data representation by a certain relative amount, \(\). Otherwise, \(\) leaves a given feature as-is. The threshold \(\) parameterizes how significant the frequency reduction must be in order to outweigh the potentially detrimental loss of information or impact on the loss landscape. From Table 1, it can be observed that \(\) performs comparably or better than its counterpart \(\) across the datasets considered, motivating the use of frequency-informed transformations.

### Frequency Reduction as Inductive Bias

Reducing the frequency of the target function may improve neural network performance by reducing the impact of spectral bias. On the other hand, the way we reduce frequency may impact other factors that play a significant role in neural network training. Since the interactions between transformations that reduce frequency and other learning dynamics are unique to how a feature is originally distributed, we can loosely formulate finding a transformation \(g\) parameterized by \(_{g}\) that reliably and beneficially reduces the frequency as a constrained optimization problem:

\[*{argmin}_{_{f},_{g}}_{i=1}^{N} (y_{i},f(g(_{i};_{g}));_{f}) _{K}P_{f g}()d<_{K}P_{f^{*}}()d ,\] (12)

where \(_{f}\) denotes the trainable parameters of neural network \(f\), \(P\) can be evaluated on arbitrary (including unlabeled) points, \(K\) denotes a high frequency region of \(k\)-space, and \(f^{*}\) denotes a neural network that minimizes the empirical loss \(1/N_{i=1}^{N}(y_{i},f^{*}(_{i};_{f^{*}}))\). Equation (12) suggests that a beneficial frequency-reducing transformation can be found by limiting the space of acceptable solutions represented by \(f\) to yield lower spectral energy in its high frequency components, compared to its analogous model \(f^{*}\) trained on the raw data. One way to attempt solving this optimization problem is to simplify the frequency reduction constraint and directly use \(P_{f}()\) to regularize the training loss towards finding a low frequency solution. However, this approach is computationally expensive, would in general require transformations on multiple features simultaneously, and would require careful pre-determination of the region \(K\) to represent the spectral energy in high-frequency components of the learned function.

To alleviate these challenges we make the following simplifications. First, to avoid the computational complexity of working in the full feature space, we assume that all input features are conditionally independent given the target \(y\), hence, they do not have interactions that significantly impact the frequency of the high dimensional decision boundary described by the data. Notably, we only use this assumption to limit our consideration to transformations that reduce the high frequency spectral energy by acting on individual features one at a time instead of jointly. After the transformation is conducted, consideration of feature interactions for the learning task is dictated by the underlying network architecture. Second, instead of relying on \(S_{}}\) as a proxy for \(_{K}P_{f}()d\) to impose a frequency reduction constraint on \(f g\) during training, we constrain \(g\) to be a composition of the two frequency-reducing transformations \(\) (\(\)) and \(\) (\(\)) (understood to be acting on the individual components of the feature vector \(\)):

\[g;_{g}:\{_{g}^{s},_{g}^{r},_{g}^{c}\} =_{g}^{c}(;_{g}^{ r});_{g}^{s}+(1-_{g}^{c})\,(; _{g}^{s})_{g}^{c},\] (13)

where \(_{g}^{r}\) represents the set of parameters specifying the mapping from original feature values to their ranked counterparts (e.g., how the interpolation is done), and \(_{g}^{s}\) denotes the set of coefficientsused to scale the input features. Note that \(^{r}_{g}\) is not a variable of this optimization problem as it only depends on the order of feature values in training set. Equation (13) describes a linear trajectory between scaled features and their ranked counterparts, while the degree of scaling applied to original and ranked features can be different. Notably, although we describe a linear relationship, one can encode more complex relationships to enable exploration of a larger space of frequency reducing transformations. With \(g\) applying frequency reduction as a soft constraint, we drop the constraint from Equation (12), and set \(\{_{f},_{g}\}\) to the minimizers of the empirical loss. We implement Equation (13) as the input layer of a neural network and let \(_{g}\) get updated jointly with the rest of the network parameters during backpropagation. We satisfy the constraint \(^{c}_{g}\) by clamping these weights after each update. Note that, computational complexity introduced by Equation (13) is linear in the number of input features (see Appendix F).

It is important to note that \(\) and \(\) are used to individually reduce the high frequency spectral energy of the empirical distribution function \(()\), even though our objective is to reduce the spectral energy for \(f(;_{f}) P(y|)\) instead. This is motivated by the observation that (13) is designed as an inductive bias that exploits the neural network \(f\)'s spectral bias, and \(g\)'s parameters are selected towards minimizing the empirical loss. Given that neural networks have strong spectral bias towards learning low frequency functions, when jointly optimized during training, \(f\) prefers mapping the training instances to an informative low-frequency form using \(g\) and fitting to this low-frequency alternative in order to minimize the loss further. In Section 4, we show empirical evidence that neural networks trained with \(g\) indeed have reduced spectral energy in their high-frequency components compared to the networks trained on the same input data.

## 4 Experiments and Results

Our evaluation focuses on 3 key metrics: performance, rate of convergence and the irregularity of functions learned. Performance is evaluated using accuracy and the area under the receiver operating characteristic curve (AUROC). Rate of convergence is evaluated using the mean number of training epochs required to minimize validation loss. Finally, irregularity of functions learned by the neural network models are measured using total high-frequency power in Equation 2 along top principal components (PCs). We evaluate our proposed approach using 14 benchmark classification datasets listed in Table 2. These datasets are used by  to demonstrate the performance gap between tree-based models and neural networks. However, unlike , we use these datasets without truncating any samples, and we drop _id_ and _date_ type features as the approaches we compare are not designed to utilize the information conveyed by them. Because we consider a wide variety of datasets, the evaluation metrics we collect for the baselines and the proposed method highly fluctuate. This makes it challenging to draw conclusions for the overall behavior of these methods. Based on these observations, similar to , we normalize our measurements and aggregate them across datasets. In Appendix E, we provide the raw measurements and additional details on data preparation and training. Implementation details to reproduce our results are provided in Appendix H.

**Frequency Reduction with Neural Networks.** We present normalized and aggregated statistics across 14 datasets to highlight the general behavior of different transformations. Figure 2 depicts these statistics, where boxes represent quartiles, notches represent \(95\%\) confidence intervals and whiskers represent the full range except outliers. From the figure, it can be observed that our proposed

  Name & \#Samples & \#Features & Source \\  electricity  & 43512 & 9 & https://openml.org/4/151 \\ house\_16H & 22784 & 17 & https://openml.org/4/821 \\ pol & 15000 & 49 & https://openml.org/4/722 \\ kdd\_pums\_1, g7\_small & 7019 & 61 & https://openml.org/4/993 \\ MagicTelescope  & 19020 & 11 & https://openml.org/4/1120 \\ bank-marketing  & 45211 & 17 & https://openml.org/4/1461 \\ phonome & 5404 & 6 & https://openml.org/4/1489 \\ MiniBONE  & 130064 & 51 & https://openml.org/4/1150 \\ eye\_movements  & 10936 & 28 & https://openml.org/1/044 \\ jannis & 83733 & 55 & https://openml.org/4/4168 \\ california  & 20640 & 8 & https://www.dc.fc.wp.pl/trology/Regression/cal_housing.html \\ albert & 425240 & 79 & https://openml.org/4/4147 \\ credit card clients  & 30000 & 24 & https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients \\ Diabetes  & 768 & 9 & https://www.openml.org/searchtype=dataSort+runskid=37 \\  

Table 2: 14 Tabular datasets used in the experimentsmethod provides significantly higher normalized accuracy and AUROC, while taking significantly less epochs to converge. All of the transformations considered result in reduction of frequency of the function learned, compared to the neural network trained on unit scaled data. However, as discussed in Section 3.3, existing methods do not benefit from frequency reduction as much as our proposed methods, as they may have side effects such as losing relevant information (i.e., \(\)), or negatively impacting optimization (i.e., \(\), \(\) and \(\)). On the other hand, our proposed method is able to balance the tradeoff between performance gain from frequency reduction and loss due to the side effects of the transformations used, providing an effective inductive bias for tabular deep learning.

**Investigating the Learned Weights.** Figure 3 depicts the change of the weights of our proposed approach across 10 random seeds on _electricity_ dataset. Appendix E.1 extends this plot to all datasets. Across different seeds, initialization of our proposed layer's weights stay the same (i.e., \(_{g}^{s}:=\) and \(_{g}^{c}:=\)), however, the network weights \(_{f}\) and data splits change. The first two columns of the figure correspond to the \(\) weights \(_{g}^{s}\) for the raw features and \(\) features, and the third column corresponds to the weights \(_{g}^{c}\) that combine these features from Equation (13). Shaded regions in the figure represent \(95\%\) confidence intervals. From the figure, it can be observed that the confidence intervals of the learned weights are small, relative to their change over epochs. This implies that the values of these weights stay consistent across random seeds. Consequently, the network consistently learns similar representations for the same feature and the representations do not overfit to the underlying network architecture. The figure also shows that the amount of scaling and ranking that can be applied to gain performance is data-dependent. Therefore, it is computationally expensive to exhaustively find the best transformations for each feature.

**Frequency Reduction with Tree-based Models.** XGB typically outperforms all other tabular learning approaches and is known to not be impacted by irregularity of target functions . Therefore, in order to isolate the performance improvements observed by our approach to spectral bias of neural networks, it is important to confirm that our approach does not improve XGB performance. In Appendix E, we provide empirical evidence that our approach's impact to XGB is insignificant. Additionally, even though we do not intend this study to be a benchmarking of state-of-the-art methods, we see that using our approach, a simple MLP with no regularization and limited hyperparameter

Figure 3: Weights learned by the proposed approach on _electricity_ dataset. Lines represent values taken by the weights \(_{g}\) throughout 250 epochs, and shaded regions represent the 95% confidence intervals. The weights consistently converge to similar values across 10 random seeds that vary data splits and neural network initializations. Due to heterogeneous nature of tabular datasets, scaling and ranking weights that minimize the loss for different features are different.

Figure 2: Normalized measurements of performance, convergence and frequency across datasets and seeds. The simple frequency-informed approach, \(\), with \(\) implemented as quantile transform improves upon \(\) alone. Additionally, our proposed methods significantly outperform all baselines, while converging faster.

tuning can outperform XGB in 4 of the datasets that were identified to be tree-friendly by previous studies , while providing competitive performance in 2 datasets.

**Frequency Reduction with other Network Architectures.** Frequency reduction as an inductive bias can improve the performance of more sophisticated deep learning architectures as well. For example, our proposed layer's mappings can be used as an input to TabNet to significantly reduce the amount of hyperparameter tuning required. As discussed in Section 2, it is challenging to utilize TabNet's full potential due to its hyperparameter sensitivity, causing inconsistent results across different studies. However, this model also offers additional benefits beyond performance improvements such as interpretability. Therefore, we believe TabNet is a good candidate to evaluate our inductive bias on beyond the vanilla MLP. Similar to our other experiments, we limit the number of hyperparameter configurations swept for tuning to 100. For each dataset, we train an MLP with our proposed layer until convergence and use the layer to generate low-frequency versions of the corresponding dataset. We compare the performance of our approach to TabNet trained by following the author's suggestions . We find that our approach improves TabNet's AUROC by \(4\% 1.6\%\) (see Appendix E.2).

**Performance over Varying Target Function Frequency.** Due to the diverse nature of the datasets considered in our experiments, it may appear challenging to reliably draw conclusions about the effect of the proposed method. Different dataset characteristics naturally lead to variation in the performance gain observed from our methods. To address these concerns, we provide additional results using synthetic data. Specifically, we generate synthetic datasets of varying target function frequency by applying a common scale factor to \(k_{i}\) in Equation (24) while keeping all other parameters fixed. We train the same 2-hidden-layer MLP for 200 epochs on the unit-scaled, ranked, and scaled (via standardization) features. Figure 4 depicts the AUROCs of these MLPs over different target function frequencies. From the figure we observe that for low target function frequencies, all methods perform comparably. Also, performance decreases with increasing target function frequency due to NN's spectral bias. Our proposed method consistently outperforms the others and is the most robust to high frequency target functions.

## 5 Conclusion

In this paper, we studied the impact of spectral bias of neural networks on tabular datasets. We showed that tabular datasets tend to describe irregular target functions and this irregularity negatively impacts neural network performance. We proposed incorporating frequency reduction as an inductive bias when training neural networks to reduce the irregularity of these functions. We showed that our proposed method significantly improves neural network performance while reducing the number of training epochs needed to converge. Although we propose the new direction of using frequency reduction as an inductive bias for tabular deep learning, we merely scratched the surface when it comes to realizing said bias. For example, our analyses can be extended to other tabular deep learning approaches to determine if the performance improvements they offer can be explained from the lens of spectral analysis. We discuss other future directions as well as the limitations of our work in Appendix G.

Figure 4: AUROCs of MLPs using the baseline transforms and the proposed method over multiple scale factors. As the scale factor increases, the target function frequency of the dataset also increases. Shaded regions correspond to \(95\%\) confidence intervals across 10 random seeds.