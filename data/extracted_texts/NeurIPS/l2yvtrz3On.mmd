# Improved Sample Complexity for Multiclass PAC Learning

Steve Hanneke

Purdue University

steve.hanneke@gmail.com

&Shay Moran

Technion and Google Research

smoran@technion.ac.il

&Qian Zhang

Purdue University

zhan3761@purdue.edu

###### Abstract

We aim to understand the optimal PAC sample complexity in multiclass learning. While finiteness of the Daniely-Shaley-Shwartz (DS) dimension has been shown to characterize the PAC learnability of a concept class , there exist polylog factor gaps in the leading term of the sample complexity. In this paper, we reduce the gap in terms of the dependence on the error parameter to a single log factor and also propose two possible routes towards completely resolving the optimal sample complexity, each based on a key open question we formulate: one concerning list learning with bounded list size, the other concerning a new type of shifting for multiclass concept classes. We prove that a positive answer to either of the two questions would completely resolve the optimal sample complexity up to log factors of the DS dimension.

## 1 Introduction

Multiclass learning refers to the problem of classifying an input feature from a set (feature space) \(\) to a label in a set (label space) \(\) with \(||>2\) (\(\) can be infinite) . When \(||=2\), the problem is known as binary classification. Multiclass learning has wide applications to various tasks in machine learning including image classification , natural language processing , tissue classification , etc. For theoretical analysis of multiclass learning, a probabilistic setting is typically assumed, where all the feature-label pairs in the training sequence are assumed to be independent and identically distributed (iid) samples from some distribution \(P\) over \(\). Then, the objective of the learner is to minimize the error rate of the output classifier under the distribution \(P\). A basic framework in the probabilistic setting is Probably Approximately Correct (PAC) learning . Though the characterization of PAC learnability of a binary concept class with the finiteness of its Vapnik-Chervonenkis (VC) dimension has been proved by Vapnik and Chervonenkis (1968), the characterization of the multiclass PAC learnability remained open until Brukhim et al. (2022) showed the equivalence between the PAC learnability of a concept class and the finiteness of its Daniely-Shaley-Shwartz (DS) dimension (dim, see Definition 1.4)  instead of Natarajan dimension or graph dimension.

However, the problem of establishing the optimal sample complexity or error rate (see Section 1.1 for formal definitions) for multiclass learning remains unsolved. For binary concept classes, Hanneke (2016) showed that the sample complexity is in \(((d+(1/))/)\) where \(d\) is the VC dimension of the concept class. Since DS dimension and VC dimension coincide for binary concept classes, it is natural to ask if the sample complexity of a concept class \(^{}\) for \(||>2\) is also in\(((d+(1/))/)\) where \(d=()\) is the DS dimension of \(\). In terms of the upper bound, it asks if there exists a multiclass learner whose worst case error rate is in \(O((d+(1/))/n)\) with probability at least \(1-\), where \(n\) denotes the size of the training sequence. However, on the one hand, an explicit proof of the \(((d+(1/))/)\) lower bound on the sample complexity is still missing in the literature. On the other hand, the current best upper bound on worst case error rate to our knowledge is \(O(d)+d((n)))^{2}(n)+(1/)}{n} \)(Brukhim et al., 2022), which differs from the conjectured rate by the factor \((((n))+(d))^{2}(n)\).

In this paper, we step forward towards improved sample complexity and error rate in multiclass learning. As the concept class is fixed and the sample size increases during an online learning process, we mainly focus on improving the error rate in terms of the sample size \(n\). Specifically, for a concept class \(^{}\) with \(()=d\), we prove an \(((d+(1/))/)\) lower bound on its sample complexity and construct a multiclass learner whose worst case error rate is \(O((d^{3/2}(d)(n)+(1/))/n)\) with probability at least \(1-\), which implies that the sample complexity of \(\) is \(O(d)(d/)+(1/)}{} \). Our results greatly narrow the gap between the upper and lower bounds of the sample complexity and the error rate. The dependence of the upper bound of the error rate on the sample size has also been improved from \(O(((n))^{2}(n)/n)\) to \(O((n)/n)\) (treating \(d\) as a constant). The multiclass learner we construct builds upon a list learner which predicts a list of labels for the test point (see Section 2.1 for a detailed introduction to list learning). Actually, we prove a reduction from multiclass learning to list learning and upper bound the error rate of the constructed multiclass learner with some function of the list size and the expected error rate of the list learner (the probability of excluding the true label in the predicted list). Moreover, the upper bound indicates that a list learner with size independent of \(n\) and expected error rate scaling linearly in \(1/n\) in terms of the sample size \(n\) would imply an \(O(1/n)\) error rate (treating \(d\) as a constant). We leave the construction of such list learners an open question.

Furthermore, we also explore an alternative combinatorial approach towards improved sample complexity in multiclass learning. For a concept class, we can define a hypergraph called the one-inclusion graph (Haussler et al., 1994) on its projection to a finite sequence of features (see Section 1.1 for definitions). Then, informally speaking, the "density" (defined through the average degree of the one-inclusion graph) of a concept class can be used to upper bound the error rate of multiclass learning (Daniely and Shalev-Shwartz, 2014; Aden-Ali et al., 2023). Specifically, if we can upper bound the density of any concept class \(\) by a multiple of its DS dimension, then the sample complexity is in \(O((()+(1/))/)\), which matches the lower bound we prove. Thus, a proof of the above upper bound directly leads to a \(((()+(1/))/)\) sample complexity for multiclass learning. When \(()=1\), we successfully prove the \(((1/)/)\) sample complexity in Theorem 3.2. For general concept classes, we develop a technique named "pivot shifting" similar to the shifting operator (Haussler, 1995) on concept classes. We show that if a pivot shifting does not increase the DS dimension of a concept class, then its density is upper bounded by twice the DS dimension. We leave the impact of pivot shifting on DS dimension as another open question.

Throughout the paper, we use \(\) to denote the set of positive integers. For any \(n\), we define \([n]:=\{1,,n\}\). For any sets \(\), \(\), sequence \(=(x_{1},,x_{n})^{n}\), and function \(f^{}\), we define the subsequence \(_{-i}:=(x_{1},,x_{i-1},x_{i+1},,x_{n})\) for \(i[n]\) and \(f|_{}:=(f(x_{1}),,f(x_{n}))\). The projection of a set \(F^{}\) to **x** is defined as \(F|_{}:=\{f|_{}:f F\}^{n}\).

OutlineIn Section 1.1, we introduce the problem of multiclass learning and review some existing results. In Section 1.2, we summarize the key points of our theoretical results. In Section 2, we introduce list learning, present the reduction from multiclass learning to list learning, and improve the sample complexity upper bound of multiclass learning via this reduction together with a boosting technique for list learners. In Section 3, we prove the optimal sample complexity for classes of DS dimension 1, introduce the intuition and the definition of "pivot shifting", and demonstrate its potential application to the proof of the optimal sample complexity of multiclass learning.

### Multiclass learning

In this section, we formally introduce the problem of multiclass learning (Valiant, 1984). For any distribution \(P\) over \(\), the error rate of a classifier \(h^{}\) under \(P\) is defined as

\[_{P}(h):=P(\{(x,y):y h(x)\}).\]In this paper, we focus on **realizable** distributions: for a concept class \(^{}\), a distribution \(P\) over \(\) is \(\)-realizable if \(_{h}_{P}(h)=0\). Let \(()\) denote the set of \(\)-realizable distributions. Besides, \(((x_{i},y_{i}))_{i=1}^{n}()^{n}\) is \(\)-realizable if \( h\) such that \(y_{i}=h(x_{i})\), \( i[n]\).

**Definition 1.1** (Multiclass learner).: _A **multiclass learner** (or a learner) \(\) is an algorithm which given a sequence \(_{n=0}^{}()^{n}\) and a concept class \(^{}\), outputs a classifier \((,)^{}\)._

Then, we can define multiclass PAC learning as follows.

**Definition 1.2** (Multiclass PAC learning).: _For any concept class \(^{}\), the **(PAC) sample complexity**\(_{,}:(0,1)^{2}\) of a multiclass learner \(\) is a mapping from \((,)(0,1)^{2}\) to the smallest positive integer such that for any \(m_{,}(,)\) and any distribution \(P()\), \(_{S P^{n}}(_{P}((S,))> )\), and we define \(_{,}(,)=\) if no such integer exists. We say \(\) is **PAC learnable** by \(\) if \(_{,}(,)<\) for all \((,)(0,1)^{2}\). The **(PAC) sample complexity** of \(\) is defined as \(_{}(,):=_{}_{ ,}(,)\) for any \((,)(0,1)^{2}\)._

Sometimes it is easier to analyze the expected error rate

\[_{,,P}:,\ \ n_{S P^{n}} _{P}((S,))=_{(S,(X,Y)) P ^{n+1}}(Y(S,)(X))\]

for a learner \(\) and distribution \(P\) over \(\), or transductive error rate

\[_{,,}:,\ \ n_{=((x_{1},h(x_{1})),,(x_{n},h(x_{n})))( )^{n}:h}_{i=1}^{n} _{h(x_{i})(_{-i},)(x_{i})}.\]

We further define \(_{,}:=_{P()} _{,,P}\), \(_{}:=_{}_{,}\), and \(_{,}:=_{}_{ ,,}\). By a leave-one-out argument (Brukhim et al., 2022, Fact 14), we observe that \(_{,}_{,, }\). Aden-Ali et al. (2023, Theorem 2.1) upper bounded the high probability error rate using the transductive error rate, which leads to a guarantee on PAC sample complexity. Based on their result, we prove the same upper bound up to a multiplicative constant on the high probability error rate using the expected error rate in Theorem 2.6.

Next, we define pseudo-cubes and DS dimensions of concept classes. Here, we also present their extensions to the setting of \(k\)-list learning for future reference in Section 2.1.

**Definition 1.3** (Pseudo-cube and \(k\)-pseudo-cube).: _For any \(d,k\), a class \(H^{d}\) is called a \(k\)-**pseudo-cube** of dimension \(d\) if it is non-empty, finite, and for every \(h H\) and \(i[d]\), there exist at least \(k\)-neighbors of \(h\) in \(H\), where \(g\) is an \(i\)-neighbor of \(h\) if \(g(i) h(i)\) and \(g(j)=h(j)\) for all \(j[d]\{i\}\). A **pseudo-cube** of dimension \(d\) is a \(1\)-pseudo-cube of dimension \(d\)._

**Definition 1.4** (DS dimension and \(k\)-DS dimension, Charikar and Pabbaraju 2023).: _For any \(d,k\), we say \(^{d}\) is \(k\)-**DS shattered** by \(^{}\) if \(|_{}\) contains a \(d\)-dimensional \(k\)-pseudo-cube. The \(k\)-**DS dimension**\(_{k}()\) of \(\) is the maximum size of a \(k\)-DS shattered sequence. We say \(\) is **DS shattered** by \(\) if it is 1-DS shattered by \(\). The **DS dimension**\(()\) of \(\) is defined as \(_{1}()\)._

Now, we introduce some existing results in multiclass learning. Brukhim et al. (2022) proved that a class \(^{}\) is PAC learnable if and only if \(d:=()<\), and there exists a multiclass learner \(\) which for any \(P()\), \((0,1)\), \(n\), and \(S P^{n}\), satisfies that with probability at least \(1-\),

\[_{P}((S,))=O((d)+d( (n)))^{2}(n)+(1/)}{n}),\] (1)

which is also the best upper bound before this paper to our knowledge. In terms of lower bound, it follows from Charikar and Pabbaraju (2023, Theorem 6) that \(_{}(n)=(d/n)\). Thus, the current upper and lower bounds of the expected error rate does not match. Moreover, a potentially sharp lower bound on the sample complexity \(_{}\) is still missing.

The learner \(\) in Brukhim et al. (2022) relies on orienting the one-inclusion graphs defined below as a building block.

**Definition 1.5** (One-inclusion graph, Haussler et al. 1994).: _The **one-inclusion graph** (OIG) of \(H^{n}\) for \(n\) is a hypergraph \((H)=(H,E)\) where \(H\) is the vertex-set and \(E\) denotes the edge-set defined as follows. For any \(i[n]\) and \(f:[n]\{i\}\), we define the set \(e_{i,f}:=\{h H:h(j)=f(j),\  j[n]\{i\}\}\). Then, the edge-set is defined as_

\[E:=\{(e_{i,f},i):i[n],f:[n]\{i\},e_{i,f}\}.\]

_For any \((e_{i,f},i) E\) and \(h H\), we say \(h(e_{i,f},i)\) if \(h e_{i,f}\) and the size of the edge is \(|(e_{i,f},i)|:=|e_{i,f}|\)._Typically, we consider the one-inclusion graph of the projection of a concept class \(^{}\) to a sequence \(^{n}\) with \(n\), i.e., \((|_{})\). The "density" of \(\) discussed in Section 3 is defined via the "maximal average degree" (defined below) of the hypergraph \((|_{})\).

**Definition 1.6** (Degree and average degree).: _For any hypergraph \(G=(V,E)\) and \(v V\), we define the **degree** of \(v\) in \(G\) to be \((v;G):=|\{e E:v e,|e| 2\}|\). When the underlying graph is clear in the context, we simply write \((v)\) in abbreviation. If \(|V|<\), we can define the **average degree** and **average out-degree** of \(G\) to be_

\[(G):=_{v V}(v;G)=_{e E:|e| 2}|e|\;\;\;\;(G):= _{e E}(|e|-1).\]

_For general \(V\), we can define the **maximal average degree** of \(G\) to be \((G):=_{U V:|U|<}(G[U])\), where \(G[U]=(U,E[U])\) denotes the induced hypergraph of \(G\) on \(U V\) with \(E[U]:=\{e U:e E,e U\}\)._

Note that for finite graphs, the average out-degree does not depend on the choice of orientation on \(G\). Moreover, since \(|e| 2\) for all \(e E_{n}\), we have

\[(G)=_{e E:|e| 2}|e| _{e E}2(|e|-1)=2(G).\] (2)

Now, we can define the density of a concept class as follows.

**Definition 1.7**.: _The **density** of \(^{}\) is defined as \(_{}(m):=_{^{m}}((|_{})),\; m\)._

### Main results

In this section, we summarize the key points of our theoretical results. The full versions of some results are stated in Section 2 and 3. We first need the following definition to rule out trivial concept classes for which one training point suffices to achieve zero error rate under any realizable distribution.

**Definition 1.8** (Nondegenerate concept class, Hanneke et al., 2023).: _A concept class \(^{}\) is called **nondegenerate** if there exist \(h_{1},h_{2}\) and \(x_{0},x_{1}\) such that \(h_{1}(x_{0})=h_{2}(x_{0})\) and \(h_{1}(x_{1}) h_{2}(x_{1})\). \(\) is called **degenerate** if it is not nondegenerate._

Our main result on the multiclass PAC sample complexity is as follows.

**Theorem 1.9** (Partial summary of Theorem 2.5 and 2.11).: _For any nondegenerate concept class \(^{}\) with \(()=d\) and any \((,)(0,1)^{2}\), we have_

\[((d+(1/))/)_{}( ,) O((d^{3/2}(d)(d/)+(1/))/ ).\] (3)

Our upper bound follows from a reduction to list learning and an improved sample complexity for list learning summarized below.

**Theorem 1.10** (Informal summary of Theorem 2.7 and 2.10).: _Assume that there exists a list learner which given a concept class \(\) with \(()=d\) and training sequence of size \(n\) outputs a menu of size \(p(,n)\) with expected error rate upper bounded by \((,n)/n\) for some functions \(p\) and \(\) nondecreasing in \(n\). Then, there exists a multiclass learner whose error rate is_

\[O(((,n)+d(p(,n))+(1/))/n)1-.\]

_Moreover, there exists a list learner satisfying \(p(,n)=O(e)^{}(n)\) and \((,n)=O(d^{3/2}(d)(n))\)._

We refer readers to Section 2.1 for detailed definitions regarding list learning. Note that if \(p(,n)\) and \((,n)\) of some list learner is independent of \(n\), there exists a multiclass learner with error rate linear in \(1/n\). We leave the establishment of such list learners as Open Question 1.

In addition to the above approach, we propose an alternative route toward obtaining the conjectured \(((d+(1/))/)\) sample complexity, by directly bounding the average degrees of one-inclusion graphs. In particular, we show in Theorem 3.2 that any \(\) with \(()=1\) has \(_{}(,)=((1/)/)\), which was not previously known. Moreover, we approach the general case via a new technique we call "pivot shifting". Specifically, we obtain the following result, which relies on an assumption on such pivot shifting. The verification of this assumption is left as Open Question 2.

**Proposition 1.11** (Informal summary of the results in Section 3).: _Assume that for any finite concept class, there exists a pivot shifting such that the DS dimension of the concept class does not increase after the pivot shifting, then we have \(_{}(,)=((( )+(1/))/)\) for any \(^{}\)._Multiclass learning via list learning

In this section, we prove a reduction from multiclass learning to list learning in Section 2.2. We improve the existing list learners using boosting in Section 2.3. Then, using a boosted list learner and the reduction, we improve the multiclass learning sample complexity upper bound in Section 2.4. We first present some definitions and results of list learning in Section 2.1.

### List learning

In list learning, the menus defined below serve as classifiers in multiclass learning.

**Definition 2.1** (\(k\)-menu, Brukhim et al. 2022).: _A **menu** of size \(k\) is a function \(:\{:|| k\}\). A \(1\)-menu can be viewed as a classifier in \(^{}\), and vice versa._

For any distribution \(P\) over \(\), the error rate of a \(k\)-menu \(\) under \(P\) is defined as \(_{P}():=P(\{(x,y):y(x)\})\) which agrees with the definition of the error rate of classifiers when the size of the menu is 1.

**Definition 2.2** (\(k\)-list learner).: _A **list learner**\(\) of size \(k\) is an algorithm which given a sequence \(_{n=0}^{}()^{n}\) and a concept class \(^{}\), outputs a \(k\)-menu \((,)\). A \(1\)-list learner can be viewed as a multiclass learner, and vice versa._

Similar to multiclass learners, the expected error rate of a list learner \(\) is defined as

\[_{,,P}:,\ \ n _{S P^{n}}[_{P}((,)) ]=_{(S,(X,Y)) P^{n+1}}(Y(S,)(X))\]

for any concept class \(^{}\) and distribution \(P\) over \(\). Restricting to realizable distributions, we can define

\[_{,}:=_{P()} _{,,P}_{ }^{k}:=_{k,}_{ ,}.\]

Next, we define list PAC learning.

**Definition 2.3** (List PAC learning, Charikar and Pabbaraju 2023).: _For any concept class \(^{}\) and \(k\), the **(PAC) sample complexity**\(_{,}:(0,1)^{2}\) of a \(k\)-list learner \(\) is a mapping from \((,)(0,1)^{2}\) to the smallest positive integer such that for every \(m_{,}(,)\) and every distribution \(P()\), \(_{S P^{n}}(_{P}((,)) >)\), and we define \(_{,}(,)=\) if no such integer exists. We say \(\) is \(k\)-list PAC learnable by \(\) if \(_{,}(,)<\) for all \((,)(0,1)^{2}\). The \(k\)-list **(PAC) sample complexity** of \(\) is defined as \(_{}^{k}(,):=_{k, }_{,}(,)\) for any \((,)(0,1)^{2}\)._

Note that \(\) is PAC learnable by a learner \(\) if and only if \(\) is \(1\)-list learnable by \(\), and the PAC sample complexity of \(\) is \(_{}=_{}^{1}\). For list PAC learning, it was proved by Charikar and Pabbaraju (2023) that a concept class \(\) is \(k\)-list learnable if and only if \(d_{k}:=_{k}()<\), and there exists a \(k\)-list learner \(^{k}\) which for any \(P()\), \((0,1)\), \(n\), and \(S P^{n}\), satisfies that with probability at least \(1-\),

\[_{P}(^{k}(S,))=O(d_{k}(}(d_{k})+(k(n)))^{2}(n)+(1/)}{n}).\] (4)

For the expected error rate, the lower bound \(_{}^{k}(n)=(d_{k}/(kn))\) has been proved in Charikar and Pabbaraju (2023, Theorem 6). However, a lower bound of the same order on the \(k\)-list PAC sample complexity is still missing in the literature. To establish a lower bound, we also need to rule out trivial classes in list learning. In analogy to Definition 1.8, we define \(k\)-nondegeneracy as follows.

**Definition 2.4** (\(k\)-nondegenerate concept class).: _A concept class \(^{}\) is called \(k\)-**nondegenerate** for \(k\) if there exist \(h_{1},,h_{k+1}\) and \(x_{0},x_{1}\) such that \(|\{h_{j}(x_{0}):j[k+1]\}|=1\) and \(|\{h_{j}(x_{1}):j[k+1]\}|=k+1\). \(\) is called \(k\)-**degenerate** if it is not \(k\)-nondegenerate._

We claim that only one training point is sufficient for the \(k\)-list learning of a \(k\)-degenerate concept class \(\). Indeed, \(\) is \(k\)-list learnable if \(|| k\). Now, suppose that \(|| k+1\) and is \(k\)-degenerate. Upon observing any point \((x^{},y^{})\) realizable by \(\), if \(|\{h:h(x^{})=y^{}\}| k\), then \(x\{h(x):h,h(x^{})=y^{}\}\) is a \(k\)-menu which always contains the correct label. If \(|\{h:h(x^{})=y^{}\}| k+1\), then, for any \(x\{x^{}\}\), we must have \(|\{h(x):h,h(x^{})=y^{}\}| k\)because otherwise \(\) is \(k\)-nondegenerate. Then, \(x\{h(x):h,h(x^{})=y^{}\}\) is a \(k\)-list which always contains the correct label.

Now, we are ready to present the following lower bound on the \(k\)-list PAC sample complexity.

**Theorem 2.5**.: _For any \(k\), \(k\)-nondegenerate concept class \(^{}\) with \(_{k}()=d_{k}\), \((0,)\), and \((0,)\), we have \(_{}^{k}(,)-1)(2)+4 (1/)}{16(k+1)}\). In particular, when \(k=1\), for any \((0,1/16)\) and \((0,1/8)\), we have_

\[_{}(,)( )-1)(2)+4(1/)}{32}.\] (5)

The proof of Theorem 2.5 is presented in Appendix A where we construct hard distributions based on properties of \(k\)-pseudo-cubes.

### Reduction from multiclass learning to list learning

We first introduce the theorem that provides a guarantee on PAC sample complexity based on expected error rate, which will be used frequently in our analysis.

**Theorem 2.6**.: _Fix a concept class \(^{}\) and consider a learner \(\) which satisfies \(_{,,P}(n) M_{n}/n\) for any \(n\) and \(P()\) with \(M_{n}\) nondecreasing in the sample size \(n\). Then, there exists a learner \(^{}\) such that for any \(P()\), \((0,1)\), \(n 4\), and the training sequence \(S P^{n}\), with probability at least \(1-\), we have_

\[_{P}(^{}(S,)) 4.82(8.34M_{[n/2]} +(2/))/n.\]

The proof of Theorem 2.6 is provided in Appendix B. Now, we consider general list learners whose sizes may depend on the sample size. The theorem below states our reduction to list learning.

**Theorem 2.7**.: _Assume that there exists a list learner \(_{}\) which for any \(^{}\), \(()\), \(n\), and \(S^{n}\), outputs a menu \(_{}(S,)\) of size \(p(,n)\) satisfying \(_{_{},,}( ,n)/n\) for some function \(:2^{^{}}[0,)\). Without loss of generality, we assume that \(p(,n)\) and \((,n)\) are nondecreasing in \(n\). Then, there exist multiclass learners \(_{}\) (see Algorithm 1) and \(_{}^{}\) which for any concept class \(\) of DS dimension \(d\), \(()\), \((0,1)\), and \(n 4\), satisfy_

\[_{_{},,}(n)=O(( (,n_{1})+d p(,n_{1}))/n)\]

_where \(n_{1}:=n-2[n/3]\), and for \(S^{n}\), with probability at least \(1-\),_

\[_{}(_{}^{}(S,) )=O(((,n_{1})+d p(,n_{1})+(1/))/ n).\] (6)

The proof of Theorem 2.7 is presented in Appendix C. Note that the order of the error rate upper bound of the constructed multiclass learner is not smaller than that of the original list learner in the above theorem. Thus, the list learner \(^{k}\) of size \(k\) developed in Charikar and Pabbaraju (2023) cannot lead to an improved error rate of multiclass learning using our current result. The construction of \(_{}\) from \(_{}\) is shown in Algorithm 1.

``` Input: List learner \(_{}\), concept class \(^{}\), training sequence \(S=((x_{1},y_{1}),,(x_{n},y_{n}))()^{n}\) for \(n 3\), test feature \(x_{n+1}\). Output: A label \(y\) for the feature \(x_{n+1}\).
1\(n_{1} n-2[n/3],\ n_{2}[n/3]\);
2\(S^{1}((x_{i},y_{i}))_{i[n_{1}]}^{i},\ S^{2}((x_{i},y_{i}) )_{i=n_{1}+1}^{i},\ ^{}(x_{n_{1}+1},,x_{n},x_{n+1})\);
3\(_{}(S^{1},),\ N _{(x,y) S^{2}}_{y(x)}\);
4\(_{}\{h|_{}:h,|\{i[ n+1]\}[n_{1}]:h(x_{i})(x_{i})\}| N+1\}\);
5 Sample \((I_{1},,I_{n_{2}})([2n_{2}])^{n_{2}}\);
6\( A_{G}(T,_{})\) where \(T((I_{j},y_{I_{j}+n_{1}}))_{j[n_{2}]}\);
7return the label \((2n_{2}+1)\). ```

**Algorithm 1**Multiclass learner \(_{}\) using a list learner \(_{}\)

In step 6 of Algorithm 1, \(A_{G}\) is a multiclass learner defined in Proposition H.5 in the appendix. Moreover, we prove in Proposition H.5 that for any \(()\), \(n\), \((0,1)\), and \(S^{n}\), with probability at least \(1-\), we have

\[_{}(A_{G}(S,))=O((_{G}( )+(1/))/n)\]where \(_{G}()\) is the graph dimension (Natarajan and Tadepalli, 1988) of \(\) (see Definition H.1 in the appendix). The above bound for classes of finite graph dimensions is also novel in the literature.

We briefly comment on the analysis of Algorithm 1. We first apply the list learner to the first third of the training samples to obtain the menu \(\). Then, we count the number of errors (\(N\)) made by \(\) in the last two thirds samples. Then, we consider \(_{^{}}\) which is a subset of \(|_{^{}}\) such that the number of errors on \(^{}\) is bounded by \(N+1\). We show in Lemma H.9 that \(_{G}(_{^{}})\) is well controlled. However, as we do not observe the label of the test point, we can only consider resampling from elements in \(S^{2}\) as the new training sequence fed to \(A_{G}\) together with the concept class \(_{^{}}\). Thus, there still exist great challenges of upper bounding the error probability for the test point that will never be sampled. We need to emphasize that the standard leave-one-out argument (Brukhim et al., 2022, Fact 14) cannot be directly applied as the definition of \(N\) that determines \(_{^{}}\) only depends on \(S^{2}\) but not the test point \((X_{n+1},Y_{n+1})\). We tackle this challenge by proving that some permutation of the error event together with the constraint on correctness of \(\) on the last two points in \(^{}\) when leaving the last element (i.e., the test point) out is a subset of the error event when leaving the previous element (i.e., the point in \(S^{2}\)) out. The details of the proof is presented in Appendix C.

### Sampled boosting of list learners

We now build a list learner whose invocation to Theorem 2.7 yields the upper bound in Theorem 1.9. Brukhim et al. (2022, Lemma 39) proposed a list sample compression scheme of size \(r=O(d^{3/2}(n))\) for concept classes of DS dimension \(d\) and sample size \(n\). One can show that its error rate is \(O((r(n/r)+(1/))/n)\) using standard techniques for sample compression schemes (David et al., 2016), which however brings the extra log factor \((n/r)\). Recently, da Cunha et al. (2024) proposed stable randomized sample compression schemes for binary classification whose generalization does not induce the extra log factor in \(n\) and used this framework to analyze a subsampling-based boosting algorithm for weak learners. Motivated by its success, we extend their boosting algorithm (da Cunha et al., 2024, Algorithm 1) for multiclass list learners in Algorithm 2. Before presenting the algorithm, we first need to define the majority vote of menus. For \(K\) menus \(_{1},,_{K}\) each of size \(p\), we define their majority vote to be \(=(_{1},,_{k})\) with

\[(x)=(_{1},,_{k})(x):=\{y:|\{k[K]:y _{k}(x)\}|>K/2\}\,,\; x.\]

Note that \(\) has size \(2p-1\). For \(p=1\), the above definition recovers the majority vote of classifiers.

``` Input: List learner \(_{}\), concept class \(^{}\), training sequence \(S=\{(x_{1},y_{1}),,(x_{n},y_{n})\}()^{n}\), \((0,1/2)\), \((0,/18]\), \((0,1)\). Output: Menu \(\).
1for\(i=1,,n\)do
2\(_{1}(\{(x_{i},y_{i})\}) 1/n\);
3\(((1+)/(1-))\), \(m_{_{},}(1/2-,)\), \(K[4(n/)/]\);
4for\(k=1,,K\)do
5 Draw \(m\) samples \(S^{k}_{k}^{m}\);
6\(_{k}_{}(S^{k},)\);
7for\(i=1,,n\)do
8\(_{k+1}(\{(x_{i},y_{i})\})_{k}(\{(x_{i},y_{i}) \})(-(2_{y_{i}_{k}(x_{i})}-1))\);
9\(_{k+1}_{k+1}/(_{i=1}^{n}_{k} (\{(x_{i},y_{i})\})(-(2_{y_{i}_{k}(x_{i})}- 1)))\);
10return\(((_{k})_{k[K]})\). ```

**Algorithm 2**Sampled boosting \(_{}\) of a list learner \(_{}\)

Here, \(\) and \(\) are fixed constants, enabling us to invoke weak list learners (of constant error and confidence levels) to Algorithm 2. Next, we upper bound the error rate of the boosted list learner.

**Theorem 2.8**.: _Assume that \(_{}\) is a list learner with \(_{_{},}(1/2-,)<\) for some \((0,1/2)\) and \((0,/18]\). Then, for any \(()\), \(n\), and \(>0\), sampling \(S^{n}\), with probability at least \(1-\), the menu \(\) produced by \(_{}\) using \(_{}\) in Algorithm 2 satisfies that_

\[_{}()=O(_{_{},}(1/2-,)(n/)}{ n}).\]The proof of Theorem 2.8 is a generalization of the proof of da Cunha et al. (2024, Theorem 1.1) and is presented in Appendix D together with the proofs of other results in this section. Since multiclass learners are list learners of size 1, we can also boost multiclass learners using Algorithm 2. For instance, invoking the multiclass learner in Brukhim et al. (2022, Theorem 1) to Algorithm 2 and applying Theorem 2.6, we achieve the following sample complexity in multiclass learning.

**Corollary 2.9**.: _There exists a multiclass learner \(\) with \(_{,}(n)=O^{2}(d)( n)}{n}\) and \(_{,}(,)=O ^{2}(d)(d/)+(1/)}{}\) for any \(n,\,(,)(0,1)^{2}\), and \(^{}\) with \(()=d\)._

There is an extra \((d)\) factor in the above upper bound compared to that in Theorem 1.9, which explains the reason of routing through list learning with our reduction in Theorem 2.7: the list sample compression scheme in Brukhim et al. (2022) saves a \((d)\) factor compared to their sample compression scheme. Therefore, we invoke their list sample compression scheme as \(_{}\) to Algorithm 2 and build a list learner whose error rate depends on only one log factor in both \(n\) and \(d\).

**Theorem 2.10**.: _There exists a list learner \(_{L}\) which for any \(^{}\) with \(()=d\) and sample size \(n\) outputs a menu of size \(O(e)^{}(n)\) with \(_{_{L},}(n)=O(d)( n)}{n}\)._

### Improved upper bounds on sample complexity

Applying the list learner \(A_{L}\) in Theorem 2.10 to our reduction, Theorem 2.7 immediately implies the following result.

**Theorem 2.11**.: _There exists a multiclass learner \(_{}\) such that for any \(^{}\) of DS dimension \(d\), \(()\), \((0,1)\), \(n d+1\), and \(S^{n}\), with probability at least \(1-\), we have_

\[_{}(_{}(S,))=O( (d)(n)+(1/)}{n}),\] (7)

_which implies that_

\[_{_{},}(,)=O ((d)(d/)+(1/)}{} ),\ ,(0,1).\] (8)

_Furthermore, if there exists a list learner \(_{}\) of size \(f_{1}(d)\) and expected error rate \(_{_{},}(n) f_{2}(d)/n\) for some functions \(f_{1}:\) and \(f_{2}:[0,)\), then, there exists a multiclass learner \(_{}\) such that_

\[_{_{},}(,)=O ((d(f_{1}(d))+f_{2}(d)+(1/))/),\  ,(0,1).\] (9)

The proof of Theorem 2.11 follows directly from Theorem 2.7 and Theorem 2.10 and is provided in Appendix E. Moreover, observing that \(_{k}()_{k^{}}()\) for \(k<k^{}\), our requirement on \(_{}\) does not violate the lower bound in Charikar and Pabbaraju (2023, Theorem 6).

Compared to the upper bound (1) by Brukhim et al. (2022), (7) improves the dependence of the error rate on the sample size \(n\) from \(O(((n))^{2}(n)/n)\) to \(O((n)/n)\), which steps further towards the goal of \(O(1/n)\) expected error rate (treating the DS dimension as a constant). Combining (5) and (8), we arrive at (3) where the gap has been improved to the factor \((d)(d/)\). However, we are not aware of any existing list learner satisfying the requirements of \(_{}\) in Theorem 2.11. Thus, we leave the construction of \(_{}\) as an open question.

**Open Question 1**.: _Does there exist a list learner such that given a concept class \(^{}\), its size is \(f_{1}(())\) and its expected error rate is \(_{_{},}(n)=f_{2}(( ))/n\) for some functions \(f_{1}:\) and \(f_{2}:[0,)\)?_

Ideally, we would expect a list learner with size \(O(())\) and expected error rate \(O(()/n)\) as it immediately implies an upper bound \(_{}(,)=O((() (())+(1/))/)\) which matches the lower bound in (5) up to the factor \((())\).

## 3 Density, DS dimension, and pivot shifting

We now introduce an alternative route toward proving the optimal sample complexity of multiclass PAC learning: bounding the density \(_{}:[0,)\) (Definition 1.7) of concept classes \(\). The following proposition summarizes existing results that illustrate the role of density in multiclass learning.

**Proposition 3.1** (Daniely and Shalev-Shwartz 2014, Charikar and Pabbaraju 2023, Aden-Ali et al. 2023).: _For any \(^{}\) and \(n\), we have_

\[_{}(n)/(2en)_{}_{ ,}_{}(n)/n.\] (10)

_Assume that \(_{}(n) f(())\) for some function \(f:[0,)\) and all \(n\). Then, there exists a learner \(\) based on orienting the one-clusion graph (Definition 1.5) of the projected concept class (see Aden-Ali et al. (2023, Appendix A) for the formal definition of the algorithm) with sample complexity \(_{,}(,)=O())+(1/)}{}\) for all \(,(0,1)\).1_

In Proposition 3.1, the first inequality of (10) follows from Charikar and Pabbaraju (2023, Theorem 6), the last inequality of (10) follows from Daniely and Shalev-Shwartz (2014, Theorem 2), and the last paragraph follows from Aden-Ali et al. (2023, Theorem 2.2). Thus, for sharper multiclass sample complexity, it suffices to bound the density of a concept class with some functions of its DS dimension. Furthermore, by Definition 1.7 and (2), it suffices to bound the average out-degree (Definition 1.6) of finite one-inclusion graphs. In fact, it has been conjectured that \(_{}(n) c()\) for some constant \(c>0\)Daniely and Shalev-Shwartz (2014) and the question remained open since then. A positive resolution of this conjecture would immediately imply that the \(_{}(,)=((( )+(1/))/n)\) by Proposition 3.1 and Theorem 2.5. It is worth mentioning that for \(\{0,1\}^{}\), Haussler et al. (1994) proved that \(_{} 2()\) (for binary classes, the DS dimension is the VC dimension), which also motivates the above conjecture. In this paper, we confirm the above conjecture for concept classes of DS dimension 1.

**Theorem 3.2**.: _For any \(^{}\) with \(()=1\), we have \(_{}(n) 2,\  n\). Thus, \(_{}(,)=((1/)/ )\) for any positive \(, O(1)\) and any \(\) with \(()=1\)._

The above theorem follows from the following fact we prove for one-inclusion graphs of DS dimension 1 concept classes. The proofs of Theorem 3.2 and Proposition 3.3 are presented in Appendix F.

**Proposition 3.3**.: _For any \(n\) and \(V_{n}^{n}\) with \(|V_{n}|<\) and \((V_{n})=1\), there exists no cycle (see Definition 3.4) in the one-inclusion graph \((V_{n})\) (see Definition 1.5)._

**Definition 3.4** (Cycle in finite hypergraph).: _A **cycle** of length \(m\{1\}\) in a finite hypergraph \(G=(V,E)\) consists of pairwise different vertices \(v^{0},,v^{m-1} V\) and pairwise different edges \(e^{0},,e^{m-1} E\) such that \(v^{j},v^{(j+1) m} e^{j}\) for all \(0 j m-1\)._

We prove Proposition 3.3 by contradiction and analyzing different cases of the cycle. However, it is hard to extend such result to classes of higher DS dimensions. For general concept classes, motivated by the proof for binary classes (Haussler et al., 1994, Lemma 2.4), we also consider proving by induction on the size of the sequence the class projects to. Though the analysis for binary classes does not apply to general concept classes, we discover that the analysis in the induction step proceeds seamlessly for some special concept classes where a common label which we call a "pivot" exists for each edge in the last dimension of size greater than 1 in its one-inclusion graph. Before summarizing this result in Lemma 3.6 below, we first introduce the definition of a "pivot" formally.

**Definition 3.5** (Pivot of finite concept class).: _For any \(n\{1\}\) and \(V_{n}^{n}\), we define_

\[(V_{n}):=_{y}_{y^{} \{y\}}\{(y_{1},,y_{n-1})^{n-1}:(y_{1}, ,y_{n-1},y),(y_{1},,y_{n-1},y^{}) V_{n}\}.\]

_Then, \(a\) is said to be a **pivot** of \(V_{n}\) if \((y_{1},,y_{n-1},a) V_{n}\) for any \((y_{1},,y_{n-1})(V_{n})\). We emphasize that when \((V_{n})=\), every \(a\) is a pivot of \(V_{n}\)._

Then, we can present Lemma 3.6 whose proof is provided in Appendix G.

**Lemma 3.6**.: _Assume that for some \(n\{1\}\), any \(d\), any \(m[n-1]\), and any \(H^{m}\) with \((H) d\) and \(|H|<\), we have \(((H)) d\). Consider an arbitrary set \(V_{n}^{n}\) such that \(|V_{n}|<\) and \((V_{n}) d\). If \(V_{n}\) has a pivot, then we have \(((V_{n})) d\)._Though it only works for special classes, Lemma 3.6 can serve as a building block in the induction step for the proof of \((())()\) for finite \(_{n}^{n}\). Moreover, the base case \(n=d+1\) has been verified in Brukhim et al. (2022, Lemma 13). Consequently, it suffices to extend the induction step for concept classes without a pivot. With Lemma 3.6, it is natural to consider modifying the concept class to create a pivot for it while at the same time preserving the DS dimension of the modified class nonincreasing. The technique used here is similar to shifting (Haussler, 1995, Brukhim et al., 2022), though we do not shift the whole edge "downwards" but only shift the last label in some vertex of the edge to a candidate pivot. The difference is necessary, as it has already been shown that the DS dimension of a concept class can increase after the standard shifting (Brukhim et al., 2022, Example 19). Thus, we name the technique used here "pivot shifting".

**Definition 3.7** (Pivot shifting).: _For any \(n\{1\}\), \(a\), and \(V_{n}^{n}\) with \(|V_{n}|<\), we define_

\[_{a}(V_{n}):=_{y}\{(y_{1},,y_{n-1}) ^{n-1}:(y_{1},,y_{n-1},y) V_{n},(y_{1},,y_{n-1},a ) V_{n}\}.\]

_For any \(=(y_{1},,y_{n-1})_{a}(V_{n})\) and the edge \((e_{n,},n)\) in \((V_{n})\), we define the set_

\[L_{}:=\{y:(y_{1},,y_{n-1},y)(e_{n, },n)\}.\]

_A mapping \(:_{a}(V_{n})\) is called a **pivot shifting** on \(V_{n}\) to a if \(() L_{}\) for all \(_{a}(V_{n})\). Let \(_{a,V_{n}}\) denote the set of all pivot shifting on \(V_{n}\) to \(a\). For any \(_{a,V_{n}}\), we define_

\[V_{n}^{}:=(V_{n}\{(,()): _{a}(V_{n})\})\{(,a):_{ a}(V_{n})\};\]

_i.e., \(V_{n,}\) is obtained by replacing the label \(()\) in \((,())\) with \(a\) for all \(_{a}(V_{n})\)._

We prove that the average out-degree does not decrease after pivot shiftings in the following lemma.

**Lemma 3.8**.: _For any \(a\), \(V_{n=2}^{}^{n}\) with \(|V|<\), and \(_{a,V}\), we have_

\[((V^{}))((V)).\]

The proof is presented in Appendix G. A key observation for the proof is that by definition, only edges of sizes greater than one contribute to the average out-degree. However, we are not able to show that the DS dimension does not increase after some pivot shifting, which we leave as an open question. Thus, whether pivot shifting is applicable to upper bounding density with DS dimension remains open.

**Open Question 2**.: _For any \(d\) and any \(V_{n=d+2}^{}^{n}\) with \(|V|<\) and \((V)=d\), are there some \(a\) and \(_{a,V}\) such that \((V^{}) d\)?_

Nevertheless, we have taken a further and specific step toward the verification of the conjecture that \(_{} 2()\): a positive resolution of the above question would lead to the conclusion that \(_{} 2()\) by Lemma 3.6, Lemma 3.8, and Brukhim et al. (2022, Lemma 13).