# Topological Obstructions and How to Avoid Them

Babak Esmaeili

Generative AI Group

Eindhoven University of Technology

b.esmaeili@tue.nl

&Robin Walters

Khoury College of Computer Sciences

Northeastern University

r.walters@northeastern.edu

Heiko Zimmermann

Amsterdam Machine Learning Lab

University of Amsterdam

h.zimmermann@uva.nl

Equal contribution

Jan-Willem van de Meent

Amsterdam Machine Learning Lab

University of Amsterdam

j.w.vandemement@uva.nl

###### Abstract

Incorporating geometric inductive biases into models can aid interpretability and generalization, but encoding to a specific geometric structure can be challenging due to the imposed topological constraints. In this paper, we theoretically and empirically characterize obstructions to training encoders with geometric latent spaces. We show that local optima can arise due to singularities (e.g. self-intersection) or due to an incorrect degree or winding number. We then discuss how normalizing flows can potentially circumvent these obstructions by defining multimodal variational distributions. Inspired by this observation, we propose a new flow-based model that maps data points to multimodal distributions over geometric spaces and empirically evaluate our model on 2 domains. We observe improved stability during training and a higher chance of converging to a homeomorphic encoder.

## 1 Introduction

A well-established idea in machine learning research is that geometric inductive biases can help us learn representations that reflect the underlying structure of a dataset (Bronstein et al., 2021; Higgins et al., 2022). A key intuition behind this line of research is that such representations make it easier to reason about the similarities of different instances in the dataset, for example by relating inputs using a rotation or translation, which in turn aids interpretability and data-efficiency. Geometric inductive biases have been explored in a wide variety of forms, including models that are defined on hyperbolic or spherical Riemannian manifolds (Lezcano-Casado and Martinez-Rubio, 2019; Ganea et al., 2018), models that are equivariant or invariant with respect to particular symmetry groups (Kondor and Trivedi, 2018; Cohen and Welling, 2016), and work that leverages symmetries to learn disentangled representations that factorize into distinct axes of variation (Higgins et al., 2018).

In this paper, we consider symmetry-based approaches to learning representations in an unsupervised manner by imposing geometric inductive biases on the representation space. In this context, our notion of a representation that "reflects the underlying structure of the data" is a representation that is _homeomorphic_ to the true generative factors. We are specifically interested in the optimization challenges that arise when encoding to geometric spaces such as Lie groups. While a common intuition is that an inductive bias that matches the underlying topology of the data will guide a model towards a homeomorphic representation, there are also indications that certain inductive biases can make a model more difficult to train in practice (Park et al., 2022; Falorsi et al., 2018; Batson et al., 2021), which limits their practical utility.

To understand why encoding to geometric structures can give rise to optimization challenges, we formalize topological defects that can occur in a randomly initialized encoder, such as discrepancies in the winding number or crossing number relative to those in a homeomorphic encoder. We show that these topological defects will be preserved under continuous optimization, which suggests that escaping these local optima relies on the discrete jumps that are employed during optimization.

To circumvent these obstacles to optimization, we propose Group-Flow Variational Autoencoders (GF-VAEs), which leverage normalizing flows to model complex multimodal distributions on Riemannian manifolds (Figure 1). We show that if we define the mode of the variational distribution to be the representation, normalizing flows can circumvent some of the challenges associated with local optima due to their multimodal nature. Experiments demonstrate that GF-VAEs can escape local optima during the early stages of training, resulting in more reliable convergence to a homeomorphic mapping and a greater degree of continuity after training.

We summarize the main contributions of this paper as follows:

* We characterize topological defects that can arise in encoders that map onto spaces with geometric structure. We show that some obstructions that arise from topological defects cannot be resolved using continuous optimization.
* We define evaluation criteria based on the winding number, crossing number, and continuity to measure topological defects and homeomorphism violations in the encoder.
* We propose GF-VAEs, a new VAE-based model that that employs normalizing flows to define complex distribution on Lie groups. We empirically show that GF-VAEs are able to aid in circumventing identified optimization obstructions.

## 2 Problem Statement

Homeomorphic Encoders.Our goal is to learn representations in domains where we have prior knowledge of the geometric structure, specifically structure in the form of a symmetry group that can be associated with the input data. We assume that data lies on a low-dimensional manifold \(\) that is embedded in a higher-dimensional space \(:=^{n}\) via a mapping \(g_{}:\). This is commonly known as the _manifold hypothesis_(Bengio et al., 2013). We denote the image of the mapping by \(_{x}:=g_{}()\). Then \(g_{}\) is a homeomorphism, or topological isomorphism, onto its image \(g:_{x}\). That is, \(g\) is continuous, bijective, and has a continuous inverse.

We wish to learn an encoder \(f_{}\) such that its restriction \(f_{}|_{_{x}}_{x}\) is a homeomorphism. Following Falorsi et al. (2018), we will define this mapping in terms of a network \(h_{}\)

Figure 1: A GF-VAE consists of an encoder network \(h_{}\) that maps data \(x\) to an intermediate parameter space \(\). The encoded vector \(y\) is split into \(K\) parts, where each sub-vector \(y_{k}\) corresponds to the parameters of a normalizing flow at layer \(k\). We can sample from the variational distribution \(q_{}\) by first sampling from a uniform prior defined on the Lie group (\((2)\) in this case), followed by a sequence of \(K\) bijective transformations \(r(;y_{k})\). The output of the normalizing flows, denoted as \(z_{K}\), is then passed to a decoder \(f^{*}_{}\) that maps from the Lie group \(\) to the data space \(\).

that maps to an intermediate space \(:=^{d}\), followed by a known projection \(:\),

\[f_{}}}.\] (1)

Lie Groups.Our work focuses the specific case where the manifolds \(\) and \(\) are Lie groups. Any set of symmetries may be formally described by a _group_\(G\), which is a set of invertible transformations which may be composed using a binary operation \(:G G G\). A _Lie group_ is a group that is also a differentiable manifold. Lie groups describe continuous symmetries such as rotations and translations, and are therefore a natural mathematical setting for any system with spatio-temporal symmetries.

Reasoning about Lie groups is difficult due to their non-flat structure. _Lie algebras_ provide a way to study Lie groups by considering the tangent space \(\) of the manifold at the identity element. The exponential map \(: G\) maps points the Lie algebra to points on the group manifold as such. A vector \(v\) defines a vector field on \(G\) using the group action to transport \(v\) around \(G\). Then \((v)\) is defined to be the point reached by flowing along this vector field for unit time. For more details on Lie groups, we refer the readers to [Hall and Hall, 2013].

Variational Autoencoders on Lie Groups.In this paper, we primarily focus on variational autoencoder (VAE) [Kingma and Welling, 2014, Rezende et al., 2014] as a means to learn a homeomorphic embedding. In this setting, we define a generative model by composing a uniform prior \(p(z)=(z)\) on \(\) with a likelihood model \(p_{}(x|z)=(x;f_{}^{*}(z),_{x}^{2}I_{n})\) that is defined in terms of a decoder network \(f_{}^{*}:\). We use the encoder \(f_{}\) to define a variational distribution \(q_{}(z|x)\), whose design we discuss below and train the encoder and decoder jointly by optimizing the variational lower bound,

\[^{}_{,}(x)=_{q_{}(x|z)}[ p _{}(x|z)]-D_{}[q_{}(z|x)\|p(z)] p _{}(x).\] (2)

Defining a variational distribution on a manifold is generally not straightforward as it requires finding an expression of the density on the manifold or keeping track of the change of volume when projecting to the manifold from the tangent space. Falorsi et al.  define a reparameterized construction for sampling from a Gaussian distribution on \(\) by first sampling \((0,I_{p})\) from the \(p\)-dimensional Lie algebra associated with \(\), then rescaling \(\) by way of element-wise multiplication \(_{}(x)\) using a network \(_{}:^{p}\), and computing the corresponding element \(z_{}\) on the group manifold using the exponential map. By left multiplying this randomly sampled \(z_{}\) by the group element \(f_{}(x)\) to _move_ the mode of the final distribution to its intended location, we obtain the construction

\[(;0,I_{p}), z_{}=(_{}( x)), z=f_{}(x) z_{}, p(z_{})=p( )||^{-1}.\] (3)

We account for the possible change in volume using the change of variable formula. Note that composing group elements \(f_{}(x) z_{}\) does not change the density of the resulting point on the manifold because the _Haar_ measure, a standard choice for Lie groups, is left invariant.

Running Example.As a concrete running example, which we will use throughout this paper, we will consider data on the circle \(=(2)\) that is embedded into a space of images \(\). The subset \(_{x}\) of images generated by a function \(g(2)_{x}\) corresponds to images of an object that is subject to a one-dimensional rotation. We will consider the case of in-plane rotations of images, as well images of a 3-dimensional object that is rotated around a single axis.

The special orthogonal Lie group \((2)\) is defined as

\[(2):=\{z\:|\:z(2),z^{T}z=I,(z)=1\}=A(y): =y_{1}&-y_{2}\\ y_{2}&y_{1}\:|\:y^{2},\|y\|=1},\]

where \((2)\) is the general linear group, which is the group of invertible \(2 2\) matrices under matrix multiplication. The set of images \(_{x}\) lives on a manifold that is homeomorphic to the rotation group \((2)\), embedded into the space of images by \(g\). Because we assume we know the underlying manifold \(\), we can design \(\) to have the same structure. When \(=(2)\), we can use \(:=^{2}\). The function \(h_{}\) in Equation 1 is then an ordinary neural network, and the projection to \(\) is simply the projection onto the unit circle \(:^{2}(2):=y A(y/\|y\|)\).

## 3 Optimization Obstructions

In practice, training homeomorphic encoders can give rise to optimization challenges. To develop intuition for these challenges, we will consider the running example \(=(2)\) with data in the form of in-plane rotated images on \(=^{32 32}\) (Figure 2). We train a VAE in which the network \(h_{}\) is a standard convolutional network which is paired with a deconvolutional decoder \(f^{*}:(2)^{32 32}\) (see Appendix A). Figure 2 shows encodings in the intermediate space \(\) after training with ELBO with 3 random seeds. For the first two seeds, we see that \(h_{}(_{x})\) crosses over itself, resulting in an figure "8" shape. When projected onto \(:=(2)\), this results in discontinuities at the crossover points. The second seed in addition also exhibits a sparse region in the intermediate space, leading to a gap in the \((2)\) projection. Only the third initialization has converged to the correct topology. These local optima are not unique to this example; obstructions have also been encountered in Falorsi et al. (2018) when trying to learn 3D orientations of a rotating multi-color cube from 2D images using a homeomorphic VAE. Park et al. (2022) show that the homeomorphic VAE cannot generalize well to other shapes and tends to learn a degenerate embedding to a small part of \((3)\).

The main observation that we make in this paper is that imposing a geometric structure on the latent space can introduce topological obstructions _during optimization_. This insight is distinct from the homological obstructions identified by de Haan and Falorsi (2018), who describe the obstructions that emerge from the choice of parameterization on the Lie group. We will refer to the topological defects that we identify in this paper as "optimization obstructions". The problem that we identify here is that randomly-initialized layers have a high probability of exhibiting topological defects (degree, crossing, etc.) that cannot be resolved under continuous optimization using gradient flow. Removal of these topological defects is thus only possible by relying on the jumps coming from performing SGD with a large enough learning rate. This implies that while escaping such local minima is possible, it is difficult and may require many epochs to do so, dramatically slowing training and undercutting the advantages of learning a homeomorphic representation.

We now discuss several specific optimization obstructions. We focus on the case where \(\) is the Lie group \((2)\), with the same example setting described in Section 2. All the optimization obstructions we consider occur in this case and in the case of higher-dimensional manifolds \(\) as well, but are simpler to describe for \(S^{1}\). See Appendix B for proofs.

### Figure Eight Local Minima

To more precisely describe obstructions that might arise during optimization, we consider continuous-time training along a gradient flow. We denote the weights of the initialized encoder as \((0)\) and the trained weights as \((1)\). We consider the idealized setting in which \((t)\) is a continuous function of \(t\).

Empirically, either at initialization or after some training, we often observe a "figure 8" pattern in \(\) of roughly the form \((h^{} g)()=(,)^{}\). In such a case, the embedding into \(\) contains two disjoint pieces \([-3/4,-/4][/4,3/4]\). One half of the circle is mapped to one piece at

Figure 2: Example of topological defects in learned encoders for a VAE with data on \(=^{32x32}\) in the form of rotations L-shaped tetrominoes and \(=(2)\) (the unit circle). Our goal is to learn an encoder \(f_{}\) that defines a homeomorphism (a continuous bijection with continous inverse) between the manifold of images of L-shaped tetrominoes \(_{x}\) and that of the latent space \(=(2)\). The encoder \(f_{}= h_{}\) is defined by composing a network \(h_{}:\) with a projection \(:\). On the right, we show the intermediate space \(_{x}=h_{}(_{x})\) and its projection \(_{x}=(_{x})\) for 3 random seeds after convergence. Colours indicate the angle associated with each data point on the manifold \(\). Optimization obstructions can arise when the network \(h_{}\) maps data onto a trajectory \(_{x}\) that exhibits topological defects, such as the crossing in a “figure 8” shape, which gives rise to discontinuities in the projection \(_{x}\) onto the latent space.

the top of the circle and the other half of the circle is mapped to the other disjoint piece at the bottom of circle. Such a mapping takes advantage of the singularity at \((0,0)^{2}\) into order to embed \(S^{1}\) in two continuous pieces. The resulting mapping is _nearly_ bijective, failing to reconstruct on only a small region near the two discontinuities. It is also mostly continuous, having only two discontinuities.

Once this local minimum is obtained, it is very difficult to move out of it using gradient descent. It is unlikely for the two pieces to join together and become a homeomorpic embedding since this would require passing one disjoint segment through the other and reversing its orientation which would violate bijectivity and increase the reconstruction loss.

We make this observation precise by noting that continuous optimization preserves the ordering of points on the circle. Let \((z_{1},z_{2},z_{3},z_{4})\) denote the four end points of the two disjoint intervals of \(f_{(0)}\). The ordering of these points is only defined up to cyclic permutations \(\ C_{4}\). Proposition 3.1 states that continuous optimization must preserve ordering \(\ C_{4}\). Figure 3 illustrates the proof.

**Proposition 3.1**: _Assume that \(f_{(t)}\) undergoes continuous optimization. Assume that \(f_{(t)} g\) is injective for all \(t\). The cyclic ordering induced on \(k\) points by \(f_{(0)}\) is equal to \(f_{(1)}\). Thus a figure 8 embedding, which corresponds to cyclic order \((z_{1},z_{2},z_{4},z_{3})\ \ C_{4}\), cannot be transformed to a homeomorphic embedding, which has cyclic order \((z_{1},z_{2},z_{3},z_{4})\ \ C_{4}\) or \((z_{4},z_{3},z_{2},z_{1})\ \ C_{4}\)._

In other words, transition from a figure 8 embedding to a homeomorphic embedding is impossible without violating continuity during optimization. This indicates that escaping a figure 8 local optimum during training would need to rely on discrete jumps and likely the stochasticity of the gradient estimate.

### Degree Obstructions

A second class of topological defects that can arise are encodings with a discrepancy in the winding number. We can compute the degree, or winding number, of a map \(:S^{1} S^{1}\) around the origin by summing up the differentials along its path on the sphere,

\[()=_{S^{1}}d().\]

This concept can be extended to arbitrary connected oriented manifolds, where it is usually referred to as the degree of a mapping. Intuitively, it describes the number of times that the _domain manifold_ wraps around the _co-domain manifold_.

If the embedded image \(h_{(t)}(_{x})\) does not contain the origin, then the mapping factors through \(^{2}\{(0,0)\}\) and is consequently continuous. Therefore, a continuous path in \(\) yields a continuous path in \(\). If \(f_{}|_{_{x}}\) is continuous, then the function \(_{}:= h_{} g:S^{1} S^{1}\) has a well-defined degree, also known as winding number \(w(_{})\). In order for \(f_{}|_{_{x}}\) to be a homeomorphism, the winding number must be \(w(_{})\{-1,1\}\). Under random initialization, however, the initial network may have winding number equal to any integer. Assuming continuous optimization and continuous embeddings, then \(h_{(t)}(x)\) is a continuous function of both \(x,t\). We assume that \(h_{(t)}(x)(0,0)\) for any \(t,x\) and thus winding number is defined for any time. The following proposition thus holds.

**Proposition 3.2**: _Winding numbers of the initialized and final model are equal \(w(_{(0)})=w(_{(1)})\)._

In practice, neither the continuous optimization assumption nor the avoidance of the origin holds. Rather \(h_{}\) is updated by SGD in discrete jumps and \(h_{} g\) may map to the origin. Thus, empirically, we do see that the winding number may change during training. However, if the initialization avoids

Figure 3: The figure 8 pattern in \(\) (grey) maps to two disconnected components in \(\). The cyclic order of these 4 endpoints is preserved by homotopy. Following the parameterization of the data manifold the cyclic order is \((z_{1},z_{2},z_{4},z_{3})\), which is distinct from the cyclic order of a homeomorphic embedding, either \((z_{1},z_{2},z_{3},z_{4})\) or \((z_{4},z_{3},z_{2},z_{1})\).

the origin, then due to the tendency of the magnitude of the unnormalized embeddings \(h_{}(x)\) to grow during optimization (see Section 3.3), the winding number changing becomes more unlikely. This means that defects in the winding number pose significant obstruction to learning homeomorphic embeddings. The winding number is also the primary optimization obstruction which makes it impractical to remove the hard projection \(\). If instead we decode directly from \(y\) but push embeddings to the unit circle using the loss \(\|y\|-1\|\), then it is far more likely we converge to discontinuous embeddings with the incorrect winding number (Appendix D).

### Magnitude Growth in \(\)

Empirically, we observe the values of the embeddings in \(:=^{2}\) continually grow during training. This phenomenon makes it more difficult for the embedded data manifold \(h_{}(_{x})\) to cross the origin and for the winding number to change. We give a theoretical explanation for this behavior.

Consider what would happen if the embedding \(y\) were updated directly based on the gradient of the loss \(_{y}\) with respect to \(y\). We assume the loss depends only on \(z=y/\|y\|\), and so has level sets which are unions of radial rays from the origin. The gradient \(_{y}\) must then be tangent to a circle about the origin. That is, for \(y=(a,b)\), the gradient \(_{y}=( b, a)\). Under gradient flow, the evolution of \(y\) in time \(y_{t}\) would thus flow along circles of fixed radius and so \(\|y_{0}\|=\|y_{t}\|\). Under gradient descent, however, due to the convexity of the flow lines, which are circular, the embeddings \(y\) will tend to grow in magnitude. For \(_{>0}\), we compute

\[\|y-_{y}\|^{2}=(a b)^{2}+(b a)^{2}=(a^{2}+b ^{2})(1+^{2})>\|y\|^{2}.\]

In practice, however, we do not update \(y\) based on \(_{y}\) but rather based on the gradient with respect to model parameters \(\). Let \(F\) be the map from model parameters \(\) to \(y\) given fixed input data \(x\). Then the actual update to \(y\) is \(_{y}=dF^{T}_{}\) where \(dF\) is the total derivative or Jacobian of the map \(F\). Since \(_{}=(dF)_{y}\) we have \(_{y}=dF^{T}dF_{y}\). The angle between \(_{y}\) and \(_{y}\) is bounded by some \(\) a quantity depending on the eigenvalues of the operator \(dF\). Given that \(_{y}\) is tangential to the circle, assuming for simplicity \(_{y}\) has constant length \(L\) and uniform distribution \([-,]\) in angle to \(_{y}\), the norm of \(y\) still grows _in expectation_.

**Proposition 3.3**: _Assume a circle of radius \(R\). Let \(v\) be a random vector at \(y\) on the circle of length \(L\) with angle to the tangent uniform in \([-,]\). Then_

\[[\|y+v\|^{2}]=L^{2}+R^{2}>R^{2}=\|y\|^{2}.\]

## 4 GroupFlow-VAE

The topological obstructions to optimization that we identified in Section 3 cannot be resolved under continuous optimization. Moreover, even if we allow for a degree of discontinuity, resolving obstructions like a figure 8 will require violating bijectivity, which implies that the reconstruction loss

Figure 4: The initialized encoder (t=0) may contain defects as described in Section 3, such as a figure 8 pattern. With a standard VAE, where the learned representation (red point) is the mean of a conditional gaussian, these defects are unlikely to be resolved during optimization, as it would require passing through intermediate parameters (and hence representations) with higher reconstruction loss. Using a multimodal variational distribution, the parameters and corresponding representations are less tightly coupled, in the sense that a continuous change in the parameter space can result in a discontinues change in representation (i.e. the mode of the distribution) without passing through _high-loss_ areas of the parameters space.

must increase to escape this defect. This suggests that a VAE with a reparameterized construction as described in Equation 3 will be susceptible to local optima, which aligns with the empirical observation that homeomorphic VAEs can be difficult to train.

This raises the question of whether we can make VAEs less susceptible to topological obstructions by employing a different parameterization from that of Equation 3. More concretely, we will consider a parameterization that admits multiple modes in the variational distribution \(q_{}(z|x)\), rather than the unimodal construction in Equation 3, and define \(f_{}(x)\) in terms of the mode,

\[f_{}(x):=*{arg\,max}_{z}q_{}(z|x).\] (4)

The intuition behind this approach is illustrated in Figure 4. If the variational distribution contains multiple modes, rather than a single peak centered at \((h_{}(x))\), then it may becomes possible for small changes to result in non-local changes, such as the reordering of points that is needed to untwist a figure 8, by switching between modes in the variational distribution. Moreover, increasing the number of parameters of \(q_{}(z|x)\) is likely beneficial for escaping some of the topological obstructions as some defects such as self-intersection are less likely to occur in high-dimensional spaces.

Motivated by this intuition, we consider a parameterization of \(q_{}(z|x)\) that employs normalizing flows (Rezende and Mohamed, 2015). In a normalizing flow, the sample \(z\) is defined as a push forward of sequence of smooth bijective transformations, which makes it possible to reshape a simple unimodal distribution into a more complex multimodal distribution. The probability of a sample from the final density can be computed by repeatedly applying the rule for change of variables. Concretely, given a base distribution \(p(z_{0})\) and a sequences of bijective transformations \(r_{k}:\), we obtain a sample \(z=z_{K}\) and probability by first sampling \(z_{0} p(z_{0})\) and defining a sequence of transformations

\[z_{k}=r_{k}(z_{k-1}), p(z_{k})= p(z_{k-1})-|(z_{k-1})}{\;z_{k-1}}|k=1  K.\] (5)

Normalizing flows have been used to define distributions on geometric structures such as Lie groups by either defining a flow on the Lie algebra and computing the push-forward density of the exponential map (Falorsi et al., 2019), or designing structure-specific transformations (Rezende et al., 2020).

In the GF-VAE, we will define a construction in which the encoder network returns the parameters of the flow. We show an overview of the architecture in Figure 1. We define a network \(h_{}::=^{K l}\) where \(K\) and \(l\) are the number of flow layers and parameters respectively, a sequence of bijective transformations \(\{r(;y_{k})\}_{k=1}^{K}\) parameterized by \(\{y_{k}\}_{k=1}^{K}\), and a base distribution distribution \(p(z_{0})\) which we define as a distribution on the group. The sequence \(\{r(;y_{k})\}_{k=1}^{K}\) is then used to define a new distribution on \(\) given an \(x\) by transforming the base distribution. This defines a conditional flow that can be trained using a stand lower bound (Eq. 2),

\[z_{0} p(z_{0}), z_{k}=r(z_{k-1};y_{k}), q(z_{k}|x)= q(z _{k-1}|x)-|;y_{k})}{\;z_{k-1}} |.\]

The choice of the flow \(r\) is very important here as normalizing flows are typically defined on flat spaces. This means that, for a specific manifold \(\), additional care must be taken when designing them to ensure that they are a diffeomorphism from \(\) to itself (Rezende et al., 2020; Durkan et al., 2019; Mathieu and Nickel, 2020). In this paper, we employ a similar method as Falorsi et al. where we define an affine layer followed by a single layer of spline flow, and a Tanh layer multiplied by \(\) as the last layer to push all the probability density between \(-\) and \(\).

## 5 Experiments

We perform a series of experiments to evaluate the difficulty of learning a homeomorphic encoder. Concretely, we investigate how often we fall into one of the failure cases described in Section 3 in standard VAEs with geometric latent spaces in practice. Subsequently, we examine how well GF-VAE can circumvent these topological obstructions during training.

Baselines.Throughout our experiments, we compare against (1) a standard VAE, and (2) a supervised VAE where in addition to maximizing the ELBO, the encoder is trained to predict the ground truth representations. These two scenarios serve as extremes on the spectrum of guiding the model to the right representation. We also compare against a deterministic autoencoder (AE) which does not regularize the embedding space. We also tried regularizing the y-space to be close \(S_{1}\) (by penalizing \((\|y\|_{2}-1)^{2}\)) in order to mitigate the optimization problem discussed in Subsection 3.3, which we refer to as "reg-\(y\)" loss. Finally, we evaluate the \(\)-VAE objective (Higgins et al., 2017) which increases the regularization on the latent space by upweighting the KL term in Eq 2. All models employ a 4-layer CNN architecture for the encoder and decoder with LeakyReLU activations. For the decoder, we also experiment with _action-decoder_ proposed by Falorsi et al., which we found helpful for learning a homeomorphic mapping. The action-decoder uses a special first layer where the group action is applied to a set of learned Fourier coefficients rather than directly being passed as input to the architecture. For further details regarding our experiments, please refer to Appendix A.

Evaluation.We evaluate all models based on two criteria: (1) Has the encoder learned a homeomorphic mapping? and (2) Has the decoder learned a good model of the data? Assessing whether a learned mapping is homeomorphic is challenging. To verify homeomorphism, we follow the evaluation proposed in Falorsi et al. (2018) by examining whether the encoder yields a continuous path when interpolating in the data manifold from \(-\) to \(\). Details on evaluating continuity are provided in Appendix C. To determine how often the models encounter the topological obstructions described in Section 3, we also report crossing and winding numbers in Appendix E. A crossing number grater than 0 implies a "figure 8" obstruction, and a winding number that is not equal to \(1\) or \(-1\) implies winding number obstruction. Lastly, we measure the log-likelihood to assess how well each model approximates the data manifold. If the model has diverged during training due to posterior collapse, we report it as a non-homeomorphic mapping and ignore its continuity score in the average.

### Images: SO(2)

In our first experiment, we train on images of an L-shaped tetromino (Bozkurt et al., 2021), a teapot, and an airplane (Shilane et al., 2004). The \((2)\) manifold corresponding to each object is made by rotating the image of the object around the center. We report our findings in Tables 1 and 3.

  &  &  &  \\  & \# H. & Continuity & \# H. & Continuity & \# H. & Continuity \\  AE & 2/15 & \(137.28 54.15\) & 0/15 & \(173.37 21.99\) & 0/15 & \(132.95 40.61\) \\ VAE (\(=1\)) & 0/15 & \(117.02 23.17\) & 6/15 & \(14.21 7.30\) & 0/15 & \(86.72 65.12\) \\ VAE (\(=4\)) & 2/15 & \(132.45 58.79\) & **15/15** & \(\) & 5/15 & \(67.88 59.20\) \\ VAE + y-reg & 1/15 & \(152.71 74.20\) & 1/15 & \(14.51 81\) & 3/15 & \(124.10 93.53\) \\ GF-VAE (\(=1\)) & 5/15 & \(50.51 58.18\) & 7/15 & \(21.07 24.42\) & 0/15 & \(63.94 34.43\) \\ GF-VAE (\(=4\)) & 9/15 & \(24.04 29.14\) & 13/15 & \(8.16 17.89\) & 7/15 & \(\) \\ Action-GF-VAE (\(=4\)) & **10/15** & \(\) & 13/15 & \(3.14 1.74\) & **9/15** & \(33.26 40.03\) \\  Sup-VAE & **15/15** & **5.74\(\)0.49** & 13/15 & 5.67\(\) 0.34 & 0/15 & \(96.78 38\). \\ 

Table 1: Number of learned homeomorphic encoders and their continuity score for different objectives trained with 15 different random seeds.

Figure 5: _Top_: Latent traversals in the decoder for a VAE and GF-VAE after training. _Bottom:_ The representations in latent space \(\) for VAEs and GF-VAEs initialized with 15 different random seeds. For VAEs, the gray line inside the circle show the (scaled-down) \(y\)-traversals. For GF-VAEs, we it is difficult to visualize \(\) space as it is high-dimensional space. Therefore, to inspect for obstructions, we show the traversal of the \(z\) vectors instead.

We observe that even though the types of obstructions vary across images, both VAE and AE in general fail to learn a homeomorphic encoder. The GF-VAE objective improves performance noticeably across both metrics. We also did not find \(y\) regularization to be very helpful as it mainly stabilized training towards whatever mapping that was achieved at the early stages of training. We observed the main failure case for most of the non-homeomorphic encoders was due to discontinuity emerging from figure-eight obstruction. Moreover, looking at the learning curves in Figure 9, we observe that the winding number is susceptible to change during training which suggests that the continuity assumption is of importance in proposition 3.2. GF-VAE resolves both issues, which allows us to interpolate nicely in the \(\)-space (Figure 5). What is very surprising is that in the case of airplanes, we see that even supervised objective fails to overcome these optimization obstructions, while a GF-VAE is able to achieve this at a much better rate.

We also observe that increasing the \(\) value for the KL term generally helps. This is perhaps unsurprising given that a high \(\) value encourages the latent space to cover the prior and therefore discourages the winding numbers from being 0 and improves the performance on the continuity metric. In the case of the teapots, we in fact observe that increasing \(\) is sufficient to learn a homeomorphic encoder. However, GF-VAE still generally scores better in terms of continuity and winding numbers.

### Images: Torus

In this experiment, we consider a ring torus as the latent space, which is homeomorphic to the Lie group \((2)(2)\). We create a dataset homeomorphic to this group by independently rotating the L-shaped tetromino in both orientation and color. All models can be extended to Tori by simply duplicating the latent space and learning multiple encodings to \((2)\) in parallel. This is done by defining a factorized density \(q_{}(z^{(1)}|x^{(2)}|x)=q_{}(z^{(1)}|x)q_{}(z^{(2)}|x)\), where each distribution \(q_{}(z^{(i)}|x)\) defines a distribution on a \(S^{1}\). Ideally, we want one subgroup to correspond to colour and the other to orientation. In this setting, we evaluate homeomorphism by picking 10 different values in either colour or orientation and measuring the continuity of the encoded path when interpolating in the data manifold on the other attribute. We identify the encoder as homeomorphic if the average continuity score of all 10 paths is below a certain threshold (see Appendix C). Unsurprisingly, we found that learning a homeomorphic mapping in tori is more challenging compared to circles. Out of 15 runs, the VAE models learn a homeomorphic mapping 1 time, while a GF-VAE manages to learn a homeomorphic mapping 7 times. To be able to align each Lie group with the corresponding attribute, we also used the weakly supervised proposed by Locatello et al., where the model receives a sequence of images in which only the angle or the colour is changing. We show the latent traversal in the decoder model for one of the successful runs of GF-VAE in Figure 6. As we can see, the model has successfully managed to disentangle colour and orientation in the latent space.

## 6 Related Work

Learning Geometric Representations.There has been a large amount of work concerned with learning representations from data with geometric structure in the unsupervised or weakly supervised setting. One line of work has focused on the use of geometric spaces such as lie groups in latent space (Davidson et al., 2018; Falorsi et al., 2018; Perez Rey et al., 2020; Vadgama et al., 2022). In (Miao et al., 2018), it was argued that that geometric inductive biases in VAEs should be Incorporated via a deterministic mapping rather than the prior which is consistent with our work. As we show in this

Figure 6: Latent traversals in the decoder of a GF-VAE trained on Tetrominoes with a torus latent spaces. In each traversal, we keep the value for one Lie group fixed and do a geodesic interpolation across the other group. The traversals in the latent space highlighted are by red and orange.

work, naively incorporating a geometric bias leads to topological obstructions. Another line of work has focused on inferring the latent structure by exploring the local connectivity information (Moor et al., 2020; Chen et al., 2021; Lee et al., 2021; Pfau et al., 2020).

Learning Disentangled Representations.Topological group structure can be used to define a notion of disentanglement that is based on equivariant properties under group transformations (Higgins et al., 2018). There exists a body of work that aims to learn both disentangled and equivariant group representations. One set of methods relies on agent actions to predict the group element (Caselles-Dupre et al., 2019; Quessard et al., 2020). An adjacent line of work is to regularize encoders to be equivariant with respect to the group action, using triplets of the form \((x_{t},m_{t},x_{t+1})\) or longer sequences (Guo et al., 2019; Dupont et al., 2020; Tonnaer et al., 2022). Some approaches focus on learning symmetry-based disentangled representations in fully unsupervised settings or by enforcing commutativity in the latent Lie group (Yang et al., 2022; Zhu et al., 2021). Another related area disentangles _class_ and _pose_ in the latent space (Marchetti et al., 2022; Winter et al., 2022). Our work does not focus on disentanglement, but the topological obstructions that we describe are relevant to this domain.

Topological Obstructions in Learning.The topological obstructions that we consider in this work are distinct from the homological obstructions that have been characterized in prior work (de Haan and Falorsi, 2018; Batson et al., 2021; Falorsi et al., 2018; Perez Rey et al., 2020). Theorem 1 by de Haan and Falorsi (2018) defines homological obstructions as follows: For any latent space \(\) with non-trivial topology, it is possible to learn an encoder \(f_{}\) that is continuous when restricted to \(_{x}\), but this encoder must be discontinuous on the full space \(\). For this reason, Falorsi et al. (2018) and others (Xu and Durrett, 2018; Meng et al., 2019) use the two-part encoder from equation 1, inserting discontinuous layers \(\) when mapping to circles, spheres, \((n)\), or other manifolds. This explicit discontinuity circumvents the homological obstruction without forcing the linear layers of the network to approximate discontinuities using large weights, which we and others find leads to instability during training and inferior reconstructions (Section 5).

Normalizing Flows on Manifolds.In recent years, there has been a surge of interest in extending normalizing flows, originally formulated for Euclidean spaces, to Riemannian manifolds (Rezende et al., 2020; Mathieu and Nickel, 2020; Kohler et al., 2021; Durkan et al., 2019). One approach involves leveraging the Lie group structure of the manifold to define a parametrization of the flow (Rezende et al., 2020), which is also the strategy we adopt in this work. These recent advancements have paved the way for applying normalizing flows to the group \((3)\) in order to learn pose estimation in molecular structures (Kohler et al., 2023) and images (Liu et al., 2023; Murphy et al., 2021). Our work differs from these efforts in that the primary objective of our method is not to target flexible distributions on Riemannian manifolds, but rather to demonstrate that utilizing a flow as a variational distribution aids _optimization_ in learning a homeomorphic embedding.

## 7 Conclusion

In this paper, we investigate obstructions to optimization that can arise when learning encoders for topological spaces. We classify different types of obstructions, provide evidence these are encountered in practice, and give mathematical explanations for how they occur under certain assumptions. We propose GF-VAE, a novel model that employs normalizing flows as variational distributions to help circumvent these issues and show its effectiveness across several datasets when encoding to circles and tori. This work contains several limitations. Firstly, our theoretical analysis is limited by the idealized assumptions necessary to analyze the method using topological tools which do not exactly match those encountered in practice. Secondly, the metrics we define such as winding and crossing numbers are harder to define and compute for higher dimensional manifolds. Future work includes expanding our analysis and techniques to a wider array of Lie groups and non-group manifolds.