# Debiasing Conditional Stochastic Optimization

Lie He

EPFL

lie.he@epfl.ch

Shiva Prasad Kasiviswanathan

Amazon

kasivisw@gmail.com

Work initiated during internship at Amazon.

###### Abstract

In this paper, we study the conditional stochastic optimization (CSO) problem which covers a variety of applications including portfolio selection, reinforcement learning, robust learning, causal inference, etc. The sample-averaged gradient of the CSO objective is biased due to its nested structure, and therefore requires a high sample complexity for convergence. We introduce a general _stochastic extrapolation_ technique that effectively reduces the bias. We show that for non-convex smooth objectives, combining this extrapolation with variance reduction techniques can achieve a significantly better sample complexity than the existing bounds. Additionally, we develop new algorithms for the finite-sum variant of the CSO problem that also significantly improve upon existing results. Finally, we believe that our debiasing technique has the potential to be a useful tool for addressing similar challenges in other stochastic optimization problems.

## 1 Introduction

In this paper, we investigate the _conditional stochastic optimization_ (CSO) problem as presented by Hu et al. , which is formulated as follows:

\[_{^{d}}F()=_{}[f_{}(_{ |}[g_{}(;)])],\] (CSO)

where \(\) and \(\) represent two random variables, with \(\) conditioned on \(\). The \(f_{}:^{p}\) and \(g_{}:^{d}^{p}\) denote a stochastic function and a mapping respectively. The inner expectation is calculated with respect to the conditional distribution of \(|\). In line with the established CSO framework [16; 15], throughout this paper, we assume access to samples from the distribution \(()\) and the conditional distribution \((|)\).

Many machine learning tasks can be formulated as a CSO problem, such as policy evaluation and control in reinforcement learning [6; 24], and linearly-solvable Markov decision process . Other examples of the CSO problem include instrumental variable regression  and invariant learning . Moreover, the widely-used Model-Agnostic Meta-Learning (MAML) framework, which seeks to determine a meta-initialization parameter using metadata for related learning tasks that are trained through gradient-based algorithms, is another example of a CSO problem. In this context, tasks \(\) are drawn randomly, followed by the drawing of samples \(|\) from the specified task . It is noteworthy that the standard stochastic optimization problem \(_{}_{}[f_{}()]\) represents a degenerate case of the CSO problem, achieved by setting \(g_{}\) as an identity function.

In numerous prevalent CSO problems, such as first-order MAML (FO-MAML) , the outer random variable \(\) only takes value in a finite set (say in \(\{1,,n\}\)). These problems can be reformulated to have a finite-sum structure in the outer loop and referred to as _Finite-sum Coupled Compositional Optimization_ (_FCCO_) problem in [33; 19]. In this paper, we also study this problem, formulated as:

\[_{^{d}}F_{n}()=_{i=1}^{n}f_{i}( _{|i}[g_{}(;i)]).\] (FCCO)

The FCCO problem also has broad applications in machine learning for optimizing average precision, listwise ranking losses, neighborhood component analysis, deep survival analysis, deep latent variable models [33; 19].

Although the CSO and FCCO problems are widespread, they present challenges for optimization algorithms. Based on the special composition structure of CSO, using chain rule, under mild conditions, the full gradient of CSO is given by

\[ F()=_{}(_{|}[ g_{ }(;)])^{} f_{}(_{|}[g_{}( ;)]).\]

Constructing an unbiased stochastic estimator for the gradient is generally computationally expensive (and even impossible). A straightforward estimation of \( F()\) is to estimate \(_{}\) with 1 sample of \(\), estimate \(_{|}[g_{}()]\) with a set \(H_{}\) of \(m\) independent and identically distributed (i.i.d.) samples drawn from the conditional distribution \((|)\), and \(_{|}[ g_{}()]\) with a different set \(_{}\) of \(m\) i.i.d. samples drawn from the same conditional distribution, i.e.,

\[_{m}():=_{ H_{}}  g_{}(;)^{} f_{}( _{ H_{}}g_{}(;)).\] (1)

Note that \(_{m}()\) consists of two terms. The first term, \((1/m)_{_{}} g_{}(;)\), is an unbiased estimate of \(_{|}[ g_{}(;)]\). However, the second term is generally biased, i.e.,

\[_{|}[ f_{}(_{ H_{}}g_{ }(;))] f_{}(_{|}[g_{}(; )]).\]

Consequently, \(_{m}()\) is a biased estimator of \( F()\). To reach the \(\)-stationary point of \(F()\) (Definition 1), the bias has to be sufficiently small.

Optimization with biased gradients converges only to a neighborhood of the stationary point. While the bias diminishes with increasing batch size, it also introduces additional sample complexity. For nonconvex objectives, Biased Stochastic Gradient Descent (BSGD) requires a total sample complexity of \((^{-6})\) to reach an \(\)-stationary point . This contrasts with standard stochastic optimization, where sample-averaged gradients are unbiased with a sample complexity of \((^{-4})\). This discrepancy has spurred a multitude of proposals aimed at reducing the sample complexities of both CSO and FCCO problems. Hu et al.  introduced Biased SpiderBoost (BSpiderBoost), which, based on the variance reduction technique SpiderBoost from Wang et al. , reduces the variance of \(\) to achieve a sample complexity of \((^{-5})\) for the CSO problem. Hu et al.  proposed multi-level Monte Carlo (MLMC) gradient methods V-MLMC and RT-MLMC to further enhance the sample complexity to \((^{-4})\). The SOX  and MSVR-V2  algorithms concentrated on the FCCO problem and improved the sample complexity to \((n^{-4})\) and \((n^{-3})\), respectively.

**Our Contributions.** In this paper, we improve the sample complexities for both the CSO and FCCO problems (see Table 1). To facilitate a clear and concise presentation, we will suppress the dependence on specific problem parameters throughout the ensuing discussion.

1. [leftmargin=*]
2. Our main technical tool in this paper is an extrapolation-based scheme that mitigates. bias in gradient estimations. Considering a suitably differentiable function \(q()\) and a random variable \(\), we show that we can approximate the value of \(q([])\) via extrapolation from a limited number of evaluations of \(q()\), while maintaining a minimal bias. In the context of CSO and FCCO problems, this scheme is used in gradient estimation, where the function \(q\) corresponds to \( f_{}\) and the random variable \(\) corresponds to \(g_{}\).
3. For the CSO problem, we present novel algorithms that integrate the above extrapolation-based scheme with BSGD and BSpiderBoost algorithms of Hu et al. . Our algorithms, referred to as E-BSGD and E-BSpiderBoost, achieve a sample complexity of \((^{-4.5})\) and \((^{-3.5})\) respectively, in order to attain an \(\)-stationary point for nonconvex smooth objectives. Notably, the sample complexity of E-BSpiderBoost improves the best-known sample complexity of \((n^{-4})\) for the CSO problem from Hu et al. .
4. For the FCCO problem2 we propose a new algorithm that again combines the extrapolation-based scheme with a multi-level variance reduction applied to both inner and outer parts of the problem. Our algorithm, referred to as E-NestedVR, achieves a sample complexity of \((n^{-3})\) if \(n^{-2/3}\) and \((\{^{-2.5},^{-4}/\})\) if \(n>^{-2/3}\) for nonconvex smooth objectives and second-order extrapolation scheme. Our bound is never worse than the \((n^{-3})\) bound of MSVR-V2 algorithm of Jiang et al.  and is in fact better if \(n=(^{-2/3})\). As an illustration, when \(n=(^{-1.5})\), our bound of \((^{-3.25})\) is significantly better than the MSVR-V2 bound of \((^{-4.5})\).

In terms of proof techniques, our approach diverges from conventional analyses for the CSO and FCCO problems in that we focus on explicitly bounding the bias and variance terms of the gradient estimator to establish the convergence guarantee. Compared to previous results, our improvements do require an additional mild regularity assumption on \(f_{}\) and \(g_{}\) mainly that \( f_{}\) is \(4\)th order differentiable. Firstly, as we discuss in Remark 2 most common instantiations of CSO/FCCO framework such as: 1) invariant logistic regression , 2) instrumental variable regression , 3) first-order MAML for sine-wave few shot regression  and other problems, 4) deep average precision maximization [26; 34], tend to satisfy this assumption. Secondly, we highlight that the bounds derived from previous studies do not improve when incorporating this additional regularity assumption. Thirdly, \((^{-3})\) remains the lower bound for stochastic optimization even under the arbitrary smoothness constraint , demonstrating that our improvement is non-trivial. Our results show that, this regularity assumption, which seems to practically valid, can be exploited through a novel extrapolation-based bias reduction technique to provide substantial improvements in sample complexity.3

We defer some additional related work to Appendix B and conclude with some preliminaries.

**Notation.** Vectors are denoted by boldface letters. For a vector \(\), \(\|\|_{2}\) denotes its \(_{2}\)-norm. A function with \(k\) continuous derivatives is called a \(^{k}\) function. We use \(a b\) to denote that \(a Cb\) for some constant \(C>0\). We consider expectation over various randomness: \(_{}[]\) denotes expectation over the random variable \(\), \(_{|}[]\) denotes expectation over the conditional distribution of \(|\). Unless otherwise specified, for a random variable \(X\), \([X]\) denotes expectation over the randomness in \(X\). We focus on nonconvex objectives in this paper and use the following standard convergence criterion for nonconvex optimization .

**Definition 1** (\(\)-stationary point): _For a differentiable function \(F()\), we say that \(\) is a first-order \(\)-stationary point if \(\| F()\|^{2}^{2}\)._

For notational convenience, in the rest of this paper, we omit the dependence on \(\) (or \(i\) in the FCCO context) in the function \(g\) and use \(g_{}()\) to represent \(g_{}(;)\).

## 2 Stochastic Extrapolation as a Tool for Bias Correction

In this section, we present an approach for tackling the bias problem as appears in optimization procedures such as BSGD, BSpiderBoost, etc. Importantly, our approach addresses a general problem appearing in optimization settings and could be of independent interest. All missing details from this section are presented in Appendix C.

For ease of presentation, we start by considering the 1-dimensional case and assume a function \(q:\), a constant \(s\). Let \(\) be a random variable drawn from an arbitrary distribution \(\) over \(\). In Sections 3 and 4, we apply these ideas to the CSO and FCCO problems where the random variable \(\) is played by \(g_{}()\) and function \(q\) is played by \( f_{}\). Informally stated, our goal in this section will be to

\[q(s+[])\{q(s+)\}_{}.}\]

   Problem &  &  \\   & Algorithm & Bound & Algorithm & Bound \\  CSO & BSGD  & \((^{-6})\) & E-BSGD & \((^{-4.5})\) \\ CSO & BSpiderBoost  & \((^{-5})\) & E-BSpiderBoost & \((^{-3.5})\) \\ CSO & RT-MLMC  & \((^{-4})\) & & \\ FCCO & MSVR-V2  & \((n^{-3})\) & E-NestedVR & \((n^{-3})&n^{-2/3}\\ (\{}{^{2/3}}, ^{4}}\}),&n>^{-2/3}\) \\   

Table 1: Sample complexities needed to reach \(\)-stationary point for FCCO and CSO problems with nonconvex smooth objectives. Assumptions are comparable, but our results require an additional mild regularity on \(f_{}\) and \(g_{}\). For FCCO also see Footnote 2. Note that \((^{-3})\) is a sample complexity lower bound for standard stochastic nonconvex optimization , and hence, also for the problems considered in this paper.

An interesting case is when \(s=0\), where we are approximating \(q([])\) with evaluations of \(\{q()\}_{}\). Now, if \(q\) is an affine function, then \(q(s+[])=[q(s+)]\). However, the equality does not hold true for general \(q\), and there exists a bias, i.e., \(|q(s+[])-[q(s+)]|>0\). In this section, we introduce a stochastic _extrapolation_-based method, where we use an affine combination of biased stochastic estimates, to achieve better approximation.

Suppose \(q^{2k}\) is a continuous differentiable up to \(2k\)-th derivative and let \(h=[]\). We expand \(q(s+)\), the most straightforward approximation of \(q(s+[])\), using Taylor series at \(s+h\), and take expectation,

\[[q(s+)]=& q(s+h)+q^{ }(s+h)\,[-h]+(s+h)}{2}\, [(-h)^{2}]+(s+h)}{6}\,[(-h)^{3}]\\ +&+(s+h)}{(2k-1)!}\, [(-h)^{(2k-1)}]+\,[q^{(2k)}(_{})( -h)^{2k}],\] (2)

where \(_{}\) between \(s+\) and \(s+h\). While \([q(s+)]\) matches \(q(s+h)\) in the first 2 terms, the third term is no longer zero. The approximation error (bias) is

\[|\,[q(s+)]-q(s+h)|=|(s+h)}{2}\,[(-h)^{2}]++\,[q^{(2k)}(_{})( -h)^{2k}]|.\]

In order to analyze the upper bound, we make the following assumption on \(\) and \(q\).

**Assumption 1** (Bounded moments): _For all \(\) has bounded higher-order moments: \(_{l}:=|\,[(-[])^{l}]|<\) for \(l=2,3, 2k\)._

**Assumption 2** (Bounded derivatives): _The \(q^{2k}\) and has bounded derivatives, i.e., \(a_{l}:=_{s dom(q)}|q^{(l)}(s)|<\) for \(l=1,2,,2k\)._

In addition, we consider a sample averaged distribution \(_{m}\) derived from \(\) as follows.

**Definition 2**: _Given a distribution \(\) satisfying Assumption 1 and \(m^{+}\), we define the distribution \(_{m}\) that outputs \(\) where \(=_{i=1}^{m}_{i}\) with \(_{i}}{{}}\)._

The moments of such distribution \(_{m}\) decrease with batch size \(m\) as \(k 2\), \(|\,[(-[])^{k}]|=(m^{- k/2 })\) (see Lemma 1). Our desiderata would be to construct a scheme that uses some samples from the distribution \(_{m}\) to construct an approximation of \(q(s+[])\) that satisfies the following requirement.

**Definition 3** (\(k\)**th-order Extrapolation Operator**): _Given a function \(q:\) satisfying Assumption 2 and distribution \(_{m}\) satisfying Assumption 1, we define a \(k\)th-order extrapolation operator \(_{_{m}}^{(k)}\) as an operator from \(^{2k}^{2k}\) that given \(N=N(k)\) i.i.d. samples \(_{1},,_{N}\) from \(_{m}\) satisfies \( s|\,[_{m}^{(k)}q(s)]-q(s+ [])|=(m^{-k})\)._

We now propose a sequence of operators \(_{_{m}}^{(1)},_{_{m}}^{(2)}, _{_{m}}^{(3)},\) that satisfy the above definition. The \(_{_{m}}^{(k)}q(s)\) is designed to ensure its Taylor expansion at \(s+h\) has a form of \(q(s+h)+([(-h)^{2k}])\). The remainder \(([(-h)^{2k}])\) is bounded by \((m^{-k})\) due to Lemma 1.

**A First-order Extrapolation Operator.** We define the simplest operator

\[_{_{m}}^{(1)}q:s[q(s+)]}{{}}_{m}.\]

In Proposition 2 (Appendix C), we show that \(_{_{m}}^{(1)}\) is a first-order extrapolation operator.4

**A Second-order Extrapolation Operator.** We define the following linear operator \(_{_{m}}^{(2)}\) which transforms \(q^{4}\) into \(_{_{m}}^{(2)}q\) which has lesser bias (but similar variance, as shown later).

**Definition 4** (\(_{_{m}}^{(2)}\) Operator): _Given \(_{m}\) and \(q\), define the following operator,_

\[_{_{m}}^{(2)}q:s[2 q(s+ +_{2}}{2})-)+q(s+_{2})}{2}]_{1},_{2}}{{}}_{m}.\]Note that \(+_{2}}{2}\) is same as sampling from \(_{2m}\). The absolute difference in the Taylor expansion of \(_{_{m}}^{(2)}q\) at \(s+h\) differs from \(q(s+h)\) as,

\[(|[2(+_{2}}{2}-h)^{ 3}-((_{1}-h)^{3}+(_{2}-h)^{3})]|)= (|([(-h)^{3}]|)}{{}} _{m}.\] (3)

The bias error of this scheme can be bounded through the following proposition.

**Proposition 1** (**Second-order Guarantee**): _Assume that distribution \(_{m}\) and \(q()\) satisfies Assumption 1 and 2 respectively with \(k=2\). Then, for all \(s\), \(|[_{_{m}}^{(2)}q(s)]-q(s+ [])|_{3}+9a_{4}_{2}^{2}}{48 m^{2}}+}{96}-3_{2}^{2}}{m^{3}}\)._

**Remark 1**: _While extrapolation is motivated by Taylor expansion which requires smoothness, higher order derivatives are not explicitly computed. Appendix F.3 empirically shows that applying extrapolation to non-smooth functions achieves similar bias correction. Relaxing the smoothness conditions is a direction for future work._

The above proposition shows that \(_{_{m}}^{(2)}\) is in fact a second-order extrapolation operator with \(k=2\) under Definition 3. We will use this operator when we consider the CSO and FCCO problems later. Now, focusing on variance, we can relate the variance of \(_{_{m}}^{(2)}q(s)\) in terms of the variance of \(q(s+)\). In particular, a consequence of Lemma 2 is that

\[[(_{_{m}}^{(2)}q(s)-[ _{_{m}}^{(2)}q(s)])^{2}]=( [(q(s+)-[q(s+)])^{2}]).\]

**Extension of \(_{_{m}}^{(2)}\) to Higher-dimensional Case.** If \(q:^{p}^{}\) is a vector-valued function, then there is a straightforward extension of Definition 4. Now, for distribution \(\) over \(^{p}\) and corresponding sampled averaged distribution \(_{m}\), and \(^{p}\)

\[_{_{m}}^{(2)}q:[2 q(+ _{1}+_{2}}{2})-+_{1})+ q(+_{2})}{2}]_{1},_{2}}}{{}}_{m}.\] (4)

**Higher-order Extrapolation Operators.** The idea behind the construction of \(_{_{m}}^{(2)}\) can be generalized to higher \(k\)'s. For example, in Proposition 3, we construct a third-order extrapolation operator \(_{_{m}}^{(3)}\) through higher degree Taylor series approximation

\[_{_{m}}^{(3)}q:s(-_{ _{m}}^{(2)}+_{_{2m}}^{(2)}- _{_{3m}}^{(2)}-_{ _{4m}}^{(2)}+3_{_{6m}}^{(2)})q(s).\]

While this idea of expressing the \(k\)-th order operator as an affine combination of lower-order operators works for every \(k\), explicit constructions soon become tedious.

In Fig. 1, we empirically demonstrate the effectiveness of extrapolation in stochastic estimation. 5 In Fig. 0(a), we choose \(q(s)=s^{2}/2\), \((10,100)\). For both \(_{_{6m}}^{(2)}q(s)\) and \(_{_{m}}^{(3)}q(s)\), their estimation errors converge to 0 with increasing number of estimates. This coincides with Proposition 1 as \(a_{3}=0\)and \(a_{4}=0\) for quadratic \(q\). In contrast, biased first order method only converges to a neighborhood. In Fig. 0(b), we consider \(q(s)=s^{4}\) and \(p()=/2\) where \(\). All three methods are biased and their biases decrease with \(m\), i.e. \((m^{-k})\) for \(k\)th order method. Depending on the constants (e.g. \(a_{i}\), \(_{i}\)), a higher-order extrapolation method may need decently large \(m\) (burn-in phase) to outperform lower-order methods.

## 3 Applying Stochastic Extrapolation in the CSO Problem

In this section, we apply the extrapolation-based scheme from the previous section to reduce the bias in the CSO problem. We focus on variants of BSGD and their accelerated version BSpiderBoost based on our second-order approximation operator (Definition 4). Let \(H_{}\), \(_{}\), and \(H_{}^{}\) indicate different sets, each of which contains \(m\) i.i.d. random variables/samples drawn from the conditional distribution \((|)\). Remember that, as mentioned earlier, we use \(g_{}()\) to represent \(g_{}(;)\).

**Extrapolated BSGD.** At time \(t\), BSGD constructs a biased estimator of \( F(^{t})\) using one sample \(\) and \(2m\) i.i.d. samples from the conditional distribution as in (1)

\[G_{}^{t+1}=_{ _{}} g_{}(^{t})^{} f_{ }_{ H_{}}g_{}(^{t}).\] (5)

To reduce this bias, we apply the second-order extrapolation operator from (4). At time \(t\), we define \(_{,}^{t+1}\) to be the distribution of the random variable \(_{ H_{}}g_{}(^{t})\). Then we apply \(_{_{,}^{t+1}}^{(2)}\) by setting \(q\) to \( f_{}\) and \(=0\), i.e.

\[_{_{,}^{t+1}}^{(2)} f_{} (0):=2 f_{}(_{ H_{}}g_{ }(^{t})+_{^{} H_{}^{}} g_{^{}}(^{t}))\] \[-( f_{}( _{ H_{}}g_{}(^{t}))+ f_{ }(_{^{} H_{}^{}}g_{^{ }}(^{t}))),\] (6)

where \(_{ H_{}}g_{}(^{t})\) and \(_{^{} H_{}^{}}g_{^{}}( ^{t})\) are i.i.d. drawn from \(_{,}^{t+1}\). In Algorithm 2 (Appendix A), we present our extrapolated BSGD (E-BSGD) scheme, where we replace \( f_{}(_{ H_{}}g_{}(^{t}))\) in (5) by \(_{_{,}^{t+1}}^{(2)} f_{}(0)\) resulting in this following gradient estimate:

\[G_{}^{t+1}=(_{ _{}} g_{}(^{t}))^{}_{ _{,}^{t+1}}^{(2)} f_{}(0).\] (7)

**Extrapolated BSpiderBoost.** BSpiderBoost, proposed by Hu et al. , uses the variance reduction methods for nonconvex smooth stochastic optimization developed by Fang et al. , Wang et al. . BSpiderBoost builds upon BSGD and has two kinds of updates: a large batch and a small batch update. In each step, it decides which update to apply based on a random coin. With probability \(p_{}\), it selects a large batch update with \(B_{1}\) outer samples of \(\). With remaining probability \(1-p_{}\), it selects a small batch update where the gradient estimated will be updated with gradient information in the current iteration generated with \(B_{2}\) outer samples of \(\) and the information from the last iteration. Formally, it constructs a gradient estimate as follows,

\[G_{}^{t+1}=G_{}^{t}+} _{_{2},|_{2}|=B_{2}}(G_{}^{t+1} -G_{}^{t})&1-p_{}\\ }_{_{1},|_{1}|=B_{1}}G_{ }^{t+1}&p_{}.\] (8)

We propose our extrapolated BSpiderBoost scheme (formally defined in Algorithm 3, Appendix A) by replacing the BSGD gradient estimates in (8) with E-BSGD.

\[G_{}^{t+1}=G_{}^{t}+} _{_{2},|_{2}|=B_{2}}(G_{}^{t+ 1}-G_{}^{t})&1-p_{}\\ }_{_{1},|_{1}|=B_{1}}G_{ }^{t+1}&p_{}.\] (9)

**Sample Complexity Analyses of E-BSGD and E-BSpiderBoost.** We adopt the standard assumptions used in the literature [27; 35; 33; 41]. All proofs are deferred to Appendix D.

**Assumption 3** (Lower bound): \(F\) _is lower bounded by \(F^{}\)._

**Assumption 4** (Bounded variance): _Assume that \(g_{}\) and \( g_{}\) have bounded variances, i.e., for all \(\) in the support of \(()\) and \(^{p}\), \(_{g}^{2}:=_{[}g_{}(;)-_{ |}[g_{}(;)]_{2}^{2}]<\) and \(_{g}^{2}:=_{|}[ g_{}(;)- _{|}[ g_{}(;)]_{2}^{2}]<\)._

**Assumption 5** (Lipschitz continuity/smoothness of \(f_{}\) and \(g_{}\)): _For all \(\) in the support of \(()\), \(f_{}()\) is \(C_{f}\)-Lipschitz continuous (i.e., \(\|f_{}()-f_{}(^{})\|_{2} C_{f}\|-^ {}\|_{2}\,,^{}^{p}\)) and \(L_{f}\)-Lipschitz smooth (i.e., \(\| f_{}()- f_{}(^{})\|_{2} L_{f}\| -^{}\|_{2}\), \(,^{}^{p}\)) for any \(\). Similarly, for all \(\) in the support of \(()\) and \(\) in the support of \((|)\), \(g_{}(;)\) is \(C_{g}\)-Lipschitz continuous and \(L_{g}\)-Lipschitz smooth._

The smoothness of \(f_{}\) and \(g_{}\) naturally implies the smoothness of \(F\). Zhang and Xiao [41, Lemma 4.2] show that Assumption 5 ensures \(F\) is: 1) \(C_{F}\)-Lipschitz continuous with \(C_{F}=C_{f}C_{g}\); and 2) \(L_{F}\)-Lipschitz smooth with \(L_{F}=L_{g}C_{f}+C_{g}^{2}L_{f}\). We denote \(_{F}=_{g}C_{f}+_{g}C_{g}L_{f}\). Moreover, Assumption 5 also guarantees that \(f_{}\) and \(g_{}\) have bounded gradients. In addition, \(f_{}\) and \(g_{}\) are assumed to satisfy the following regularity condition in order to apply our extrapolation-based scheme from Section 2.

**Assumption 6** (Regularity): _For all \(\) in the support of \(()\), \( f_{}\) is 4th-order differentiable with bounded derivatives (i.e., \(a_{l}:=_{^{p}}^{(l)}f_{}() _{2}<\) for \(l=1,2,3,4\), \(^{p}\)) and \(g_{}\) has bounded moments upto 4th-order (i.e., \(_{k}=_{^{d}}_{}_{|} [_{i=1}^{p}g_{}()-_{|}[g_{}( )]_{i}^{k}]<,k=1,2,3,4\))._

**Remark 2**: _The core piece of Assumption 6 is the 4th order differentiability of \( f_{}\) as other parts can be easily satisfied through appropriate boundedness assumptions. This condition though is satisfied by common instantiations of CSO/FCCO. We discuss some examples including invariant logistic regression, instrumental variable regression, first-order MAML for sine-wave few-shot regression task, deep average precision maximization in Section 5. Therefore, our improvements in sample complexity apply to all these problems._

Consider some time \(t>0\). Let \(G^{t+1}\) be a stochastic estimate of \( F(^{t})\) where \(^{t}\) is the current iterate. The next iterate \(^{t+1}:=^{t}- G^{t}\). Let \([]\) denote the conditional expectation, where we condition on all the randomness until time \(t\). We consider the bias and variance terms coming from our gradient estimate. Formally, we define the following two quantities

\[_{}^{t+1}= F(^{t})-[G^{t+ 1}]_{2}^{2},_{}^{t+1}=[G^ {t+1}-[G^{t+1}]_{2}^{2}].\]

Our idea of getting to an \(\)-stationary point (Definition 1) will be to ensure that \(_{}^{t+1}\) and \(_{}^{t+1}\) are bounded. The main technical component of our analyses is in fact analyzing these bias and variance terms for the various gradient estimates considered. For this purpose, we first analyze the bias and variance terms for the (original) BSGD (Lemma 5) and BSpiderBoost (Lemma 7) algorithms, which are then used to get the corresponding bounds for our E-BSGD (Lemma 6) and E-BSpiderBoost (Lemma 8) algorithms. Through these bias and variance bounds, we establish the following main results of this section.

**Theorem 3**: _[E-BSGD Convergence] Consider the (CSO) problem. Suppose Assumptions 3, 4, 5, 6 hold true and \(L_{F},C_{F},_{F},C_{g},F^{}\) are constants and \(C_{e}(f;g):=_{3}+18a_{9}_{2}^{2}+5a_{9}_{4}}{96}\) defined in Corollary 1 are associated with second order extrapolation in the CSO problem. Let step size \( 1/(2L_{F})\). Then the output \(^{s}\) of E-BSGD (Algorithm 2) satisfies: \([\| F(^{s})\|_{2}^{2}]^{2}\), for nonconvex \(F\), if the inner batch size \(m=(C_{e}C_{g}^{-1/2})\), and the number of iterations_

\[T=(L_{F}(F(^{0})-F^{})(_{F}^{2}/m+C_{F}^{2}) ^{-4}).\]

The E-BSGD takes \((^{-4})\) iterations to converge and compute \((^{-0.5})\) gradients per iteration. Therefore, its resulting sample complexity is \((^{-4.5})\) which is more efficient than \((^{-6})\) of BSGD. Similar improvements can be observed for E-BSpiderBoost in Theorem 4.

**Theorem 4**: _[E-BSpiderBoost Convergence] Consider the (CSO) problem under the same assumptions as Theorem 3. Let step size \( 1/(13L_{F})\). Then the output \(^{s}\) of E-BSpiderBoost (Algorithm 3)_satisfies: \([|| F(^{s})||_{2}^{2}]^{2}\), for nonconvex \(F\), if the inner batch size \(m=(C_{e}C_{g}^{-0.5})\), the hyperparameters of the outer loop of E-BSpiderBoost \(B_{1}=(_{F}^{2}/m+C_{F}^{2})^{-2}, B_{2}=}, p_{}=1/B_{2}\), and the number of iterations_

\[T=(L_{F}(F(^{0})-F^{})^{-2}).\]

The resulting sample complexity of E-BSpiderBoost is \((^{-3.5})\), which improves \((^{-5})\) bound of BSpiderBoost  and \((^{-4})\) bound of V-MLMC/RT-MLMC .

## 4 Applying Stochastic Extrapolation in the FCCO Problem

In this section, we apply the extrapolation-based scheme from Section 2 to the FCCO problem. We focus on case where \(n=O(^{-2})\). For larger \(n\), we can treat the FCCO problem as a CSO problem and get an \((^{-3.5})\) bound from Theorem 4. All missing details are presented in Appendix E.

Now, a straightforward algorithm for FCCO is to use the finite-sum variant of SpiderBoost (or SPIDER) [10; 38] in Algorithm 3. In this case, if we choose the outer batch sizes to be \(B_{1}=n\), \(B_{2}=\) and the inner batch size to be \(m=\{^{-2}/n,^{-1/2}\}\). The resulting sample complexity of E-BSpiderBoost now becomes, \((\{/^{2.5},1/^{4}\})\), which recovers \((^{-3.5})\) bound as in Theorem 4 for \(n=(^{-2})\). However, when \(n\) is small, such as \(n=(1)\), the sample complexity degenerates to \((^{-4})\) which is worse than the \((^{-3})\) lower bound of stochastic optimization . We leave the details to Theorem 8. We still use Assumptions 3, 4, 5, 6 for the analysis of FCCO problem, replacing the role of \(\) with \(i\).

```
1:Input:\(^{0}^{d}\), step-size \(\), batch sizes \(S_{1}\), \(S_{2}\), \(B_{1}\), \(B_{2}\), Probability \(p_{},p_{}\)
2:for\(t=0,1,,T-1\)do
3:if (\(t=0\)) or (with prob. \(p_{}\)) then\(\) Large outer batch
4:for\(i_{1}[n]\) with \(|_{1}|=B_{1}\)do
5: draw \(_{i}^{t+1}\) from distribution \(_{,i}^{t+1}\) defined in (10)
6: compute \(_{i}^{t+1}\) using (11) and define \(_{i}^{t}=^{t}\)
7:endfor
8:\(G_{}^{t+1}=}_{i_{2}}(_{i}^{t +1})^{}_{_{,i}^{t+1}}^{(2)} f_{i}(0)\)
9:else
10:for\(i_{2}\) with \(|_{2}|=B_{2}\)do
11: draw \(_{i}^{t+1}\) and \(_{i}^{t}\) from distribution \(_{,i}^{t+1}\) and \(_{,i}^{t}\) defined in (10)
12: compute \(_{i}^{t+1}\) using (11) and define \(_{i}^{t}=^{t}\)
13:endfor
14:\(G_{}^{t+1}=G_{}^{t}+}_{i_{2}}(_{i}^{t+1})^{}(_{_{,i}^{(2)}}^{(2)}  f_{i}(0)-_{_{,i}^{(2)}}^{(2)} f_{i}(0))\)
15:endif
16:\(^{t+1}=^{t}- G_{}^{t+1}\)
17:endfor
18:Output:\(^{s}\) picked uniformly at random from \(\{^{t}\}_{t=0}^{T-1}\) ```

**Extrapolated NestedVR.** We now introduce a nested variance reduction algorithm E-NestedVR which reaches low sample complexity for all choices of \(n\). Missing proofs from this section are presented in Appendix E. For the stochasticities in the FCCO problem, our idea is to use two nested SpiderBoost variance reduction components: one for the outer random variable \(i\) and the other for the inner random variable \(|i\). In each outer (resp. inner) SpiderBoost step, we choose large batch \(B_{1}\) (resp. \(S_{1}\)) with probability \(p_{}\) (resp. \(p_{}\)); otherwise we choose small batch. Let \(H_{i}\) denote a set of \(m\) i.i.d. samples drawn from the conditional distribution \((|i)\). Similarly, let \(_{i}\) denote another set of \(m\) i.i.d. samples drawn from the same conditional distribution. For each given \(i\), we approximate \(_{|i}[g_{}(^{t})]\) with \(_{i}^{t+1}\) from distribution \(_{,i}^{t+1}\) where,

\[_{i}^{t+1}=}_{ H_{i}}g_{}(^ {t})&}$ or $t=0$}\\ _{i}^{t}+}_{ H_{i}}(g_{}(^{t})-g_{}( _{i}^{t}))&}$.}\] (10)

Similarly, we approximate \(_{|i}[ g_{}(^{t})]\) with \(_{i}^{t+1}\) defined as follows

\[_{i}^{t+1}=}_{_{i}} g _{}(^{t})&}$ or $t=0$}\\ _{i}^{t}+}_{_{i}}( g_{}(^{t})-  g_{}(_{i}^{t}))&}$,}\] (11)where \(_{i}^{t}\) is the last time \(i\) is visited before time \(t\). If \(i\) is not selected at time \(t\), then \(_{i}^{t+1}=_{i}^{t}\) and \(_{i}^{t+1}=_{i}^{t}\). Note that we use independent samples for \(_{i}^{t+1}\) and \(_{i}^{t+1}\).

Finally, we present E-NestedVR in Algorithm 1 where second-order extrapolation operator \(^{(2)}\) is applied to each occurrence of \( f_{i}\). We now analyze its convergence guarantee. Our analysis works by first looking at the effect of multi-level variance reduction without the extrapolation (that we refer to as NestedVR, Theorem 10, Appendix E.2), and then showing how extrapolation could further help to drive down the sample complexity.

**Theorem 5**: _[E-NestedVR Convergence] Consider the (FCCO) problem. Under the same assumptions as Theorem 3._

* _If_ \(n=(^{-2/3})\)_, then we choose the hyperparameters of E-NestedVR (Algorithm_ 1_) as_ \(B_{1}=B_{2}=n,p_{out}=1,S_{1}=_{P}^{2}^{-2},S_{2}=_{F}^{-1},p_{in}=_{F}^{-1},=( }).\)__
* _If_ \(n=(^{-2/3})\)_, then we choose the hyperparameters of E-NestedVR as_ \(B_{1}=n,B_{2}=,p_{out}=1/,S_{1}=S_{2}=\{C_{e}C_{g} ^{-1/2},_{F}^{2}/(n^{2})\},p_{in}=1, =(}).\)__

_Then the output \(^{s}\) of E-NestedVR satisfies: \([\| F(^{s})\|_{2}^{2}]^{2}\), for nonconvex \(F\) with iterations_

\[T=(L_{F}(F(^{0})-F^{})^{-2}).\]

From Theorem 5, E-NestedVR has a sample complexity of \((n^{-3})\) in the small \(n\) regime (\(n=(^{-2/3})\)) and \((\{/^{2.5},1/^{4}\})\) in the large \(n\) regime (\(n=(^{-2/3})\)). Therefore, in the large \(n\) regime, this improves the \((n^{-3})\) sample complexity of MSVR-V2 .

## 5 Applications

In this section, we demonstrate the numerical performance of our proposed algorithms. We focus on the application of invariant logistic regression here. In Appendix F, we discuss performance of our proposed algorithms on other common CSO/FCCO applications.

### Application of Invariant Risk Minimization

Invariant learning has wide applications in machine learning and related areas [22; 1]. Invariant logistic regression  is formulated as follows:

\[_{}_{=(,b)}[(1+(-b_{|}[ ]^{})],\]

where \(\) and \(b\) represent a sample and its corresponding label, and \(\) is a noisy observation of the sample \(\). This first part can be considered as a CSO objective, with \(f_{}(y):=(1+(-by))\) and \(g_{}(;):=^{}\). As the loss \(f_{}^{}\) is smooth, our results from Sections 3 and 4 are applicable.

An \(_{2}\)-regularizer is added to ensure the existence of an unique minimizer. Since the gradient of the penalization term is unbiased, we only have to consider the bias of the data-dependent term. We generate a synthetic dataset with \(d=10\) dimensions. The minimizer is drawn from Gaussian distribution \(^{}(0,1)^{d}\). We draw invariant samples \(\{(_{i},b_{i})\}_{i}\) where \(_{i}(0,1)^{d}\) and compute \(b_{i}=(_{i}^{}^{})\) and its perturbed observation \((_{i},100)^{d}\).

We consider drawing \(\) from a large set (\(n=50000\)) and a small set (\(n=50\)) as CSO and FCCO problems respectively. As baselines, we implemented the BSGD and BSpiderBoost methods

Figure 2: Performances of algorithms and their extrapolated versions on the invariant logistic regression task. Algorithms in each subplot use the same amount of inner batch size in each iteration. The shaded region represents the 95%-confidence interval computed over 10 runs.

from , V-MLMC approach from , and NestedVR approach from Appendix E.2 which achieves the same complexity as MSVR-V2  for the FCCO problem. RT-MLMC shares the same sample complexity as V-MLMC, and is thus omitted from the experiment . The results are shown in Fig. 2. In the CSO setting, we compare biased gradient methods with their extrapolated variants (BSGD vs. E-BSGD, BSpiderBoost vs. E-BSpiderBoost, and NestedVR vs. E-NestedVR). The extrapolated versions of BSGD, BSpiderBoost, and NestedVR consistently reach lower error than their non-extrapolated counterparts, as is evident in Figure 1(a). In this case, the performance of BSpiderBoost is similar to BSGD as also noted by the authors of these techniques , and a drawback of BSpiderBoost seems to be that it is much harder to tune in practice. However, it is clear that E-BSGD outperforms BSGD, and E-BSpiderBoost outperforms BSpiderBoost, respectively. In the FCCO setting, we compare extrapolation based methods and MLMC based methods. Figure 1(a), shows that E-NestedVR outperforms all other extrapolated algorithms, including the V-MLMC approach of , matching our theoretical findings.

### Application of Instrumental Variable Regression

Instrumental variable regression is a popular technique in econometrics that aims to estimate the causal effect of input \(X\) on a confounded outcome \(Y\) with an instrument variable \(Z\), which, for a given \(X\), is conditionally independent of \(Y\). As noted by , the instrumental variable regression is a special case of the CSO problem. The instrumental variable regression problem is phrased as:

\[_{w}_{=(Y,Z)}[(Y,_{X|Z}[g_{X}(w)])]\]

where \(=(Y,Z),=X\). This can be viewed in the CSO framework with \(f_{}()=(Y,)\). We choose \((y,)=(y-)\) as regression loss function and \(g_{X}(w)=w^{}X\) to be a linear regressor. In this case, \(f_{}^{}\) with \( f_{}()=(-Y)\), and our results from Sections 3 and 4 apply. We generate the data similar to 

\[Z([-3,3]^{2}), e(0,1), (0,0.1),(10)\] \[X=z_{1}+e+, Y=X+e+\]

where \(z_{1}\) is the first dimension of \(Z\), \(e\) is the confounding variable and \(\), \(\) are noises. In this experiment, we solve the instrumental variable regression using BSGD, BSpiderBoost, NestedVR and their extrapolated variants described in Sections 3 and 4. In each pair of experiments, the samples used per iteration are fixed same, i.e.: 1) BSGD uses \(m=2\) and E-BSGD uses \(m=1\); 2) For BSpiderBoost and E-BSpiderBoost, we use cycle length of 10, small batch and large batch in Spider to be 10 and 100 respectively, and we choose inner batch sizes \(m=2\) for BSpiderBoost and \(m=1\) for E-BSpiderBoost; 3) For NestedVR and E-NestedVR, we fix the outer batch size to 10 and 5 respectively, and choose fix the inner Spider Cycle to be 10 with large batch 100 and small batch 10. The results are presented in Figure 3. As is quite evident, the extrapolation variants achieve faster convergence in all 3 cases, confirming our theoretical findings.

## 6 Concluding Remarks

In this paper, we consider the conditional stochastic optimization CSO problem and its finite-sum variant FCCO. Due to the interplay between nested structure and stochasticity, most of the existing gradient estimates suffer from large biases and have large sample complexity of \((^{-5})\). We propose stochastic extrapolation-based algorithms that tackle this bias problem and improve the sample complexities for both these problems. While we focus on nonconvex objectives, our proposed algorithms can also be beneficial when used with strongly convex, convex objectives. We also believe that similar ideas could also prove helpful for multi-level stochastic optimization problems  with nested dependency.

Figure 3: Performances of algorithms and their extrapolated versions on the instrumental variable regression task. The shaded region represents the 95%-confidence interval.