# Universality laws for Gaussian mixtures

in generalized linear models

 Yatin Dandi, Ludovic Stephan, Florent Krzakala

Idephics, EPFL, Switzerland

 Bruno Loureiro

DI, Ecole Normale Superieure, Paris, France

&Lenka Zdeborova

SPOC, EPFL, Switzerland

###### Abstract

A recent line of work in high-dimensional statistics working under the Gaussian mixture hypothesis has led to a number of results in the context of empirical risk minimization, Bayesian uncertainty quantification, separation of kernel methods and neural networks, ensembling and fluctuation of random features. We provide rigorous proofs for the applicability of these results to a general class of datasets \((_{i},y_{i})_{i=1,,n}\) containing independent samples from mixture distribution \(_{c C}_{c}P_{c}^{}\). Specifically, we consider the hypothesis class of generalized linear models \(=F(^{})\) and investigate the asymptotic joint statistics of a family of generalized linear estimators \((^{(1)},,^{(M)})\), obtained either from (a) minimizing an empirical risk \(_{m}^{(m)}(^{(m)};,)\) or (b) sampling from the associated Gibbs measure \((- n_{n}^{(m)}(^{(m)};,))\). Our main contribution is to characterize under which conditions the asymptotic joint statistics of this family depend (on a weak sense) only on the means and covariances of the class conditional feature distribution \(P_{c}^{}\). This allows us to prove the universality of different quantities of interest, including training and generalization errors, as well as the geometrical properties and correlations of the estimators.

A recurrent topic in high-dimensional statistics is the investigation of the typical properties of signal processing and machine learning methods on synthetic, _i.i.d._ Gaussian data, a scenario often known under the umbrella of _Gaussian design_. A less restrictive assumption arises when considering that many machine learning tasks deal with data partitioned into a fixed number of classes. In these cases, the data distribution is naturally described by a _mixture model_, where each sample is generated _conditionally_ on the class. In other words: data is generated by first sampling the class assignment and _then_ generating the input conditioned on the class. Arguably the simplest example of such distributions is that of a _Gaussian mixture_, which shall be our focus in this work.

Gaussian mixtures are a popular model in high-dimensional statistics since, besides being an universal approximator, they often lead to mathematically tractable problems. Indeed, a recent line of work has analyzed the asymptotic performance of a large class of machine learning problems in the proportional high-dimensional limit under the Gaussian mixture data assumption, see e.g. . The goal of the present work is to show that this assumption, and the conclusions derived therein, are far more general than previously anticipated.

A recent line of work , initiated by the work of  for Kernel matrices, posits that for generalized linear estimation on non-linear feature maps satisfying certain regularity conditions and a "one-dimensional CLT", the data distribution can be replaced by equivalent Gaussian data without affecting the training and generalization errors. This was recently proven by  for empiricalrisk minimization under the setup of strongly convex objectives, and extended to a larger class of objectives by .

However, there is strong empirical evidence that Gaussian universality holds in a more general sense . First, existing results rely on the assumption of a target function depending on linear projections in the latent or feature space. This excludes the rich class of classification on mixture distributions, where the target function is given by the label. For such distributions, a more appropriate equivalent distribution is given by a mixture of Gaussians. Such a "Gaussian mixture equivalence" has been conjectured and used in existing works, such as  and was found to closely approximately classification on real datasets.

Furthermore, equivalence with mixtures of Gaussians has been observed to hold not only for training, generalization errors but other quantities of the estimators such as overlaps, variance, etc. For instance,  empirically observed that the equivalence holds even while considering the joint distribution of multiple uncertainty estimators or ensembles of multiple random feature mappings. This suggests the equivalence of the distributions of the minimizers themselves.

Our results fill these gaps and provide rigorous justification for the universality in all the aforementioned works. Namely, we show that the joint statistics of multiple generalized estimators obtained either from ERM or sampling on a mixture model asymptotically agrees (in a weak sense) with the statistics of estimators from the same class trained on a Gaussian mixture model with matching first and second order moments. Our **main contributions** are as follows:

* Through a generalization of recent developments in the Gaussian equivalence principle , we prove the universality of empirical risk minimization and sampling for a generic mixture distribution and an equivalent mixture of Gaussians. In particular, we show that a Gaussian mixture observed through a random feature map is also a Gaussian mixture in the high-dimensional limit, a fact used for instance (without rigorous justification) in .
* A consequence of our results is that, with conditions on the matrix weights, data generated by conditional Generative Adversarial Networks (cGAN) behave as a Gaussian mixture when observed through the prism of generalized linear models (kernels, feature maps, etc...), as illustrated in Figs 1 and 2. This further generalizes the work of  that only considered the universality of Gram matrices for GAN generated data through the prism of random matrix theory.
* We construct a unified framework involving multiple sets of parameters arising from simultaneous minimization of different objectives as well as sampling from Gibbs distributions defined by the empirical risk. Through the design of suitable reductions and a convexity-based argument, we establish conditions for the asymptotic universality of arbitrary functions of the set of minimizers or samples from different Gibbs distributions (Theorem 4). For instance, it includes ensembling ), and the uncertainty quantification and Bayesian setting assumed (without proof) in .
* Finally, we show that for multi-class classification, the conditions leading to universality hold for a large class of functions of the minimizers, such as overlaps and sparsity measures, leading to the equivalence between their distributions of themselves, and provide a theorem for their weak convergence (Theorem 5).
* As a technical contribution of independent interest, our proof of Theorem 5 demonstrates a principled approach for leveraging existing results on the exact asymptotics for simple data distributions (such as for Gaussian mixture models in ) to prove the weak convergence and universality of the joint-empirical measure of the estimators and parameters (means, covariances) of the data-distribution.

**Related works --** Universality is an important topic in applied mathematics, as it motivates the scope of tractable mathematical models. It has been extensively studied in the context of random matrix theory , signal processing problems  and kernel methods . Closer to us is the recent stream of works that investigated the Gaussian universality of the asymptotic error of generalized linear models trained on non-linear features, starting from single-layer random feature maps  and later extended to single-layer NTK  and deep random features . These results, together with numerical observations that Gaussian universality holds for more general classes of features, led to the formulation of different _Gaussian equivalence_ conjectures . Our results build on the line of works on the proofs of these conjectures, through the use of the one-dimensional CLT (Central Limit Theorem), stated formally in  who proved it for random features of Gaussian data. We generalize this principle to a Gaussian equivalence conditioned on the cluster assignment in a mixture-model with a corresponding conditional 1d-CLT (Assumption 10).

A complementary line of research has investigated cases in which the distribution of the features is multi-modal, suggesting a Gaussian mixture universality class instead . A bridge between these two lines of work has been recently investigated with random labels and teachers in . Our results provide rigorous extensions of Gaussian universality to the setups of mixture models as well as uncertainty quantificatio and ensembling.

## 1 Setting and models

Consider a supervised learning problem where the training data \((_{i},y_{i})^{p}\), \(i[n]\{1,,n\}\) is drawn _i.i.d._ from a mixture distribution:

\[_{i} _{c}_{c}P_{c}^{}, (c_{i}=c)=_{c},\] (1)

with \(c_{i}\) a categorical random variable denoting the cluster assignment for the \(i_{th}\) example \(_{i}\). Let \(_{c}\), \(_{c}\) denote the mean and covariance of \(P_{c}^{}\), and \(k=||\). Further, assume that the labels \(y_{i}\) are generated from the following target function:

\[y_{i}()=(_{}^{}_{i},_{i},c_{i}),\] (2)

where \(:^{3}\) is a general label generating function, \(_{}^{k p}\) and \(_{i}\) is an i.i.d source of randomness. It is important to stress that the class labels (2) are themselves not constrained to arise from a simple function of the inputs \(_{i}\). For instance, the functional form in (2) includes the case where the labels are exclusively given by a function of the mixture index \(y_{i}=(c_{i})\). This will allow us to handle complex targets, such as data generated using conditional Generative Adversarial Networks (cGANs).

In this manuscript, we will be interested in hypothesis classes defined by parametric predictors of the form \(y_{}()=F(^{})\), where \(^{k p}\) are the parameters and \(F:^{k}\) a possibly non-linear function. For a given loss function \(:^{k}_{+}\) and regularization term \(r:^{k p}_{+}\), define the (regularized) empirical risk over the training data:

\[}_{n}(;,):=_{i=1}^ {n}(^{}_{i},y_{i})+r(),\] (3)

where we have defined the feature matrix \(^{p n}\) by stacking the features \(_{i}\) column-wise and the labels \(y_{i}\) in a vector \(^{n}\). In what follows, we will be interested in the following two tasks:

1. Minimization: in a minimization task, the statistician's goal is to find a good predictor by minimizing the empirical risk (3), possibly over a constraint set \(_{p}\): \[}_{}(,)*{arg\,min}_{ _{p}}}_{n}(;, ),\] (4)

This encompasses diverse settings such as generalized linear models with noise, two-layer networks with a constant number of neurons and fixed second layer, mixture classification, but also the random label setting (with \((_{}^{}_{i},_{i},c_{i})=_{i}\)). In the following, we denote \(}_{n}^{}(,)_{} }_{n}(;,)\)
2. Sampling: here, instead of minimizing the empirical risk (3), the statistician's goal is to sample from a Gibbs distribution that weights different hypothesis according to their empirical error: \[_{}(,) P_{}() (- n}_{n}(;,) )()\] (5)

where \(\) is reference prior measure and \(>0\) is a parameter known as the _inverse temperature_. Note that minimization can be seen as a particular example of sampling when \(\), since in this limit the above measure peaks on the global minima of (4).

Applications of interest--So far, the setting defined above is quite generic, and the motivation to study this problem might not appear evident to the reader. Therefore, we briefly discuss a few scenarios of interest which are covered by this model.

1. _Conditional GANs (cGANs):_ These were introduced by  as a generative model to learn mixture distributions. Once trained in samples from the target distribution, they define a function \(\) that maps Gaussian mixtures (defining the latent space) to samples from the target mixture that preserve the label structure. In other words, conditioned on the label: \[ c,(_{c},_{ c})_{c}=(,c) P_{c}^{}\] (6)

The connection to model (1) is immediate. This scenario was extensively studied by , and is illustrated in Fig. 1. In Fig. 2 we report on a concrete experiment with a cGAN trained on the fashion-MNIST dataset.
2. _Multiple objectives:_ Our framework also allows to characterize the joint statistics of estimators \((_{1},,_{M})\) obtained from empirical risk minimization and/or sampling from different objective functions \(_{n}^{m}\) defined on the same training data \((,)\). This can be of interest in different scenarios. For instance,  has characterized the correlation in the calibration of different uncertainty measures of interest, e.g. last-layer scores and Bayesian training of last-layer weights. This crucially depends on the correlation matrix \(}_{}_{}^{}^{ k k}\) which fits our framework.
3. _Ensemble of features:_ Another example covered by the multi-objective framework above is that of ensembling. Let \((_{i},y_{i})^{d}\) denote some training data from a mixture model akin to (1). A popular ensembling scheme often employed in the context of deep learning  is to take a family of \(M\) feature maps \(_{i}_{i}^{(m)}\!=\!_{m}(_{i})\) (e.g. neural network features trained from different random initialization) and train \(M\) independent learners: \[}_{}^{(m)}*{arg\,min}_{ _{p}}_{i=1}^{n}(^{}_{i}^ {(m)},y_{i})+r()\] (7)

Prediction on a new sample \(\) is then made by ensembling the independent learners, e.g. by taking their average \(}=}{{M}}_{m=1}^{M}}_{} ^{(m)}_{m}()\). A closely related model was studied in .

Note that in all the applications above, having the labels depending on the features \(\) would not be natural, since they are either generated from a latent space, as in \((i)\), or chosen by the statistician, as in \((ii)\), \((iii)\). Indeed, in these cases the most natural label model is given by the mixture index \(y=c\) itself, which is a particular case of (2). This highlights the flexibility of our target model with respect to prior work . Instead,  assumes that the target is a function of a _latent variable_, which would correspond to a mismatched setting. The discussion here can be generalized also to this case, but require an additional assumption discussed in Appendix B.

Universality --Given these tasks, the goal of the statistician is to characterize different statistical properties of these predictors. These can be, for instance, point performance metrics such as the empirical and population risks, or uncertainty metrics such as the calibration of the predictor or moments of the posterior distribution (5). These examples, as well as many different other quantities of interest, are functions of the joint statistics of the pre-activations \((_{}^{},^{})\), for \(\) either a test or

Figure 1: Illustration of Corollary 2: high-dimensional data generated by generative neural networks starting from a mixture of Gaussian in latent space (\(^{H}\)) are (with conditions on the weights matrices) equivalent, in high-dimension and for generalized linear models, to data sampled from a Gaussian mixture. A concrete example is shown in Fig. 2.

training sample from (1). For instance, in a Gaussian mixture model, where \(_{c}_{c}\,(_{c},_{c})\), the sufficient statistics are simply given by the first two moments of these pre-activations. However, for a general mixture model (1), the sufficient statistics will generically depend on all moments of these pre-activations. Surprisingly, our key result in this work is to show that in the high-dimensional limit this is not the case. In other words, under some conditions which are made precise in Section 2, we show that expectations with respect to (1) can be exchanged by expectations over a Gaussian mixture with matching moments. This can be formalized as follows. Define an _equivalent Gaussian data set_\((_{i},y_{i})_{i=1}^{n}^{p}\) with samples drawn _i.i.d._ from the _equivalent Gaussian mixture model_:

\[_{i}_{c}_{c}\,(_{c},_{c}), y_{i}()=(_{+}^{}_{i},_{i},c _{i}).\] (8)

We recall that \(_{c}\), \(_{c}\) denotes the mean and covariance of \(P_{c}^{}\) from (1). Consider a family of estimators \((_{1},,_{M})\) defined by minimization (3) and/or sampling (5) over the training data \((,)\) from the mixture model (1). Let \(h\) be a statistical metric of interest. Then, in the proportional high-dimensional limit where \(n,p\!\!\) at a fixed \(\!=\!}{{d}}\!>0\), and where \(\) denote the expectation with respect to the Gibbs distribution (5), we define universality as:

\[_{}[ h(_{1},,_{M}) _{}]_{}[  h(_{1},,_{M})_{}]\] (9)

The goal of the next section is to make this statement precise.

## 2 Main results

We now present the main theoretical contributions of the present work and discuss its consequences. Our proofs for Theorems 4 and 6 build upon existing results on the universality of empirical risk minimization for uni-model distributions [16; 17] and therefore rely on similar technical regularity and concentration assumptions. Concretely, our work relies on the following assumptions:

**Assumption 1** (Loss and regularization).: _The loss function \(:^{k+1}\) is nonnegative and Lipschitz, and the regularization function \(r:^{p k}\) is locally Lipschitz, with constants independent from \(p\)._

Figure 2: Illustration of the universality scenario described in Fig.1. Logistic (left) & ridge (right) regression test (up) and training (bottom) errors are shown versus the sample complexity \(=}{{d}}\) for an odd vs. even binary classification task on two data models: Blue dots data generated from a conditional GAN  trained on the fashion-MNIST dataset  and pre-processed with a random features map \(()\) with Gaussian weights \(W\!\!^{1176 784}\); Red dots are the \(10\)- clusters Gaussian mixture model with means and covariances matching each fashion-MNIST cluster conditioned on labels (\(_{2}\) regularization is \(=10^{-4}\)). Details on the simulations are discussed in Appendix D.

**Assumption 2** (Boundedness and concentration).: _The constraint set \(_{p}\) is a compact subset of \(^{k p}\). Further, there exists a constant \(M>0\) such that for any \(c 0\),_

\[_{_{p},\|\|_{2} 1}\|^{ }\|_{_{2}} M,_{_{p},\|\|_{2} 1}\|_{c}^{1/2}\|_{2} M,\|_{c}\|_{2} M\] (10)

_where \(\|\|_{_{2}}\) is the sub-gaussian norm, and \(_{p}^{p}\) is such that \(_{p}_{p}^{k}\)._

**Assumption 3** (Labels).: _The labeling function \(\) is Lipschitz, the teacher vector \(\) belongs to \(_{p}\), and the noise variables \(_{i}\) are i.i.d sub-gaussian with \(\|_{i}\|_{_{2}} M\) for some constant \(M>0\)._

Those three assumptions are fairly technical, and it is possible that the universality properties proven in this article hold irrespective of these conditions. The crucial assumption in our theorems is that of a _conditional one-dimensional CLT_:

**Assumption 4**.: _For any Lipschitz function \(:\),_

\[_{n,p}_{_{p}}|[ (^{})\,\,c_{}=c]- [(^{})\,\,c_{}=c]|=0,  c\] (11)

where \(\) and \(\) denote samples from the given mixture distribution and the equivalent gaussian mixture distribution in equations (1) and (8) respectively.

The above assumption is a generalization of the "one-dimensional CLT" underlying the line of work based on the Gaussian equivalence (GE) Principle [13; 14; 17; 16]. The above assumption splits the proof of universality for a general mixture distribution into two parts. First, one shows that asymptotic universality of an observable \(h\) can be reduced to the proof of a one-dimensional CLT. Second, one proves this CLT holds for the particular class of features of interest. This proof scheme streamlines universality proofs. Our work provides a general proof of the first step in Theorem 4, conditioned on the second, later showing that Assumption 4 holds for some natural feature maps of interest, i.e. random features applied to a Gaussian mixture (Theorem 6). However, Assumption 4 is expected to hold for a large class of features, as supported by our empirical observations in Figure 2 and arguments in Appendix C.

### Universality of Mixture Models

We start by proving the universality of the free energy for a Gibbs distribution defined through the objective \(}_{n}(;,)\) for the data distribution (1) and its equivalent Gaussian mixture (8).

**Theorem 1** (Universality of Free Energy).: _Let \(_{p}()\) be a sequence of Borel probability measures with compact supports \(_{p}\). Define the following free energy function:_

\[f_{,n}()=-(- n}_{n}(;,()))d_{p}()\] (12)

_Under Assumptions 1-4 on \(\) and \(_{p}\), for any bounded differentiable function \(\) with bounded Lipschitz derivative, we have:_

\[_{n,p}|[(f_{,n}()) ]-[(f_{,n}())]|=0.\]

When \(_{p}\) corresponds to discrete measures supported on an \(\)-net in \(_{p}\), using the reduction from Lemma 1 to Theorem 1 in , we obtain the following corollary:

**Corollary 2** (Universality of Training Error).: _For any bounded Lipschitz function \(:\):_

\[_{n,p}|[(}_{n}^{ }(,()))]-[( {}_{n}^{}(,()))]|=0\]

_In particular, for any \(\), and denoting \(}}{{}}\) the convergence in probability:_

\[}_{n}^{}(,())}}{{}}}_{n}^{}(,())}}{{}},\] (13)

The full theorem, as well as its proof, is presented in Appendix A, along with additional remarks and an intuitive sketch. The proof combines the conditional 1d-CLT in Assumption 4 with theinterpolation of the free-energy in . For strongly-convex losses, one may alternatively use the Lindeberg's method as in .

In a nutshell, this theorem shows that the multi-modal data generated by any generative neural network is equivalent to a _finite_ mixture of Gaussian in high-dimensions: in other words, a _finite_ mixture of Gaussians leads to the same loss as for data generated by (for instance) a cGAN. Since the function \(:^{k+1}\) need not be convex, we can take

\[(_{},y)=^{}((_{}),y),\]

where \(\) is an already pretrained neural network. In particular, if \(\) is the output of all but the last layer of a neural net, we can view \(\) as the averaging procedure for a small committee machine.

Note that Corollary 2 depends crucially on Assumption 4 (the one-dimensional CLT), which is by no means evident. We discuss the conditions on the weights matrix for which it can be proven in Section 2.4. However, one can observe empirically that the validity of Corollary 2 goes well beyond what can be currently proven. A number of numerical illustrations of this property can be found in the work of [23; 39; 40], who already derived similar (albeit more limited) results using random matrix theory. Additionally, we observed that even with trained GANs, when we observed data through a random feature map , the Gaussian mixture universality is well obeyed. This scenario is illustrated in Fig. 1, with a concrete example in Fig. 2. Even though we did not prove the one-dimensional CLT for arbitrary learned matrices, and worked with finite moderate sizes, the realistic data generated by our cGAN behaves extremely closely to those generated by the corresponding Gaussian mixture.

A second remark is that the interest of Corollary 2 lies in the fact that it requires only a _finite_ mixture to approximate the loss. Indeed, while we could use the standard approximation results (e.g. the Stone-Weierstrass theorem) to approximate the data density to arbitrary precision by Gaussian mixtures, this would require a diverging number of Gaussian in the mixture. The fact that loss is captured with finite \(\) is key to our approach.

### Convergence of expectations for Joint Minimization and Sampling

Our next result establishes a general relationship between the differentiability of the limit of expected training errors or free energies for empirical risk minimization or free energies for sampling and the universality of expectations of a given function of a set of parameters arising from multiple objectives. As a motivating example, consider the uncertainty quantification in Section 1 that uses both Bayesian and ERM estimators [19; 24]. The parameters \(}_{}\) and \(_{}\) are obtained through empirical risk minimization and posterior sampling respectively on the same sequence of training data. In general, the inputs used in different objectives could be different but have some correlation structure. In the setup of ensembling (Equation 7), they are correlated through the feature mapping \(_{i}_{i}^{(m)}=_{m}(_{i})\). In light of these considerations, we present the following general setup: Consider a sequence of \(M\) risks:

\[}_{n}^{(m)}(;^{(m)},^{(m)}):= {1}{n}_{i=1}^{n}_{m}(^{}_{i}^{(m)},y_{i}^{(m)})+ r_{m}(), m[M]\] (14)

with possibly different losses, regularizers and datasets. For simplicity, we assume that the objectives are defined on parameters having the same dimension \(^{p k}\). We aim to minimize \(M_{1}\) of them:

\[}^{(m)}()*{arg\,min}_{ _{p}^{(m)}}\ }_{n}^{(m)}(;^{(m)},^{(m)}), m [M_{1}]\] (15)

and the \(M_{2}\) remaining parameters are independently sampled from a family of Gibbs distributions:

\[^{(m)} P_{m}()-_{m} {}_{n}^{(m)};^{(m)},^{(m)} d_{m}(), m[M_{1}+1,M],\] (16)

where \(M=M_{1}+M_{2}\). The joint distribution of the \(_{i}=(_{i}^{(1)},,_{i}^{(M)})\) is assumed to be a mixture of the form (1). However, we assume that the labels \(_{i}^{(m)}\) only depend on the vectors \(_{i}^{(m)}\):

\[y_{i}^{(m)}(^{(m)})=(_{}^{(m)}_{i}^{(m)}, _{i}^{(m)},c_{i}).\] (17)

The equivalent Gaussian inputs \(_{i}=(_{i}^{(1)},,_{i}^{(M)})\) and their labels \(()\) are defined as in (8).

Statistical metric and free energy --Our goal is to study statistical metrics for some function \(h:^{M k p}\) of the form \(h(^{(1)},,^{(M)})\). For instance, the metric \(h\) could be the population risk (a.k.a. generalization error), or some overlap between \(\) and \(_{}\). We define the following coupling free energy function:

\[f_{n,s}([1:M_{1}],,)=-\!e^{-sn\;h(^{(1)},,^{(M)})}dP^{(M_{1}+1):M},\] (18)

where \(P^{(M_{1}+1):M}\) denotes the product measure of the \(P_{m}\) defined in (16). This gives rise to the following joint objective:

\[}_{n,s}([1:M_{1}],,)= _{m=1}^{M_{1}}}_{n}^{(m)}(^{(m)};^{( m)},^{(m)})+f_{n,s}([1:M_{1}],,).\] (19)

In particular, when \(s=0\) we have \(f_{n,0}=0\) and the problem reduces to the joint minimization problem in (15). Our first result concerns the universality of the minimum of the above problem:

**Proposition 3** (Universality for joint minimization and sampling).: _Under Assumptions 1-4, for any \(s>0\) and bounded Lipschitz function \(:\), and denoting \(}_{n,s}^{}(,):=} _{n,s}(;,)\):_

\[_{n,p}|[(}_{n,s}^{}(,()))]-[ (}_{n,s}^{}(,())) ]|=0\]

The proof uses a reduction to Corollary 2, and can be found in App. A.5. The next result concerns the value of \(h\) at the minimizers point \((}^{(1)},,}^{(M)})\). We make the following additional assumptions:

**Assumption 5** (Differentiable Limit).: _There exists a neighborhood of \(0\) such that the function \(q_{n}(s)=[}_{n,s}^{}(,( ))]\) converges pointwise to a function \(q(s)\) that is differentiable at \(0\)._

The above assumption stems from the convexity based argument used to prove Theorem 4.

For a fixed realization of the dataset \(\), we denote by \( h(^{(1)},,^{(M)})_{}\) the expected value of \(h\) when \((}^{(1)},,}^{(M_{1})})\) are obtained through the minimization of (15) and \((^{(M_{1}+1)},,^{(M)})\) are sampled according to the Boltzmann distributions (16).

**Assumption 6**.: _With high probability on \(,\), the value \( h(^{(1)},,^{(M)})_{}\) (resp. the same for \(\)) is independent from the chosen minimizers in (15)._

The above assumption is motivated by the fact that commonly non-convex problems contain minima exhibiting a specific symmetry. For example, all the global minima for a two-layer neural network are permutation invariant. Assumption 6 reflects that the quantity \(h\) respects these symmetries by taking the same value at each global minimum. This can be replaced by the stronger condition of a unique minimizer. Then the following holds:

**Theorem 4**.: _Under Assumptions 1-6, we have:_

\[_{n,p}|[ h(^{(1)},,^{(M)})_{}]- [ h(^{(1)},,^{(M) })_{}]|=0,\] (20)

Proof Sketch:Our proof relies on the observation that \(q_{n}(s)\) is a concave function of \(s\). Further:

\[q_{n}^{}(0)=[ h(^{(1)}, ,^{(M)})_{}].\] (21)

This allows us to leverage a result of convex analysis relating the convergence of a sequence of convex or concave functions to the convergence of the corresponding derivatives, bypassing the more involved probabilistic arguments in . Our approach also generalizes in a straightforward manner to the setup of multiple objectives.

The above result shows that the expected value of \(h(^{(1)},,^{(M)})\) for a multi-modal data satisfying the 1d CLT is equivalent to that of a mixture of Gaussians. The full theorem is presented and proven in Appendix A.

### Universal Weak Convergence

Theorem 4 provides a general framework for proving the equivalence of arbitrary functions of parameters obtained by minimization/sampling on a given mixture dataset and the equivalent gaussian mixture distribution. However, it relies on the assumption of a differentiable limit of the free energy (Assumption 5). If the assumption holds for a sequence of functions belonging to dense subsets of particular classes of functions, it allows us to prove convergence of minimizers themselves, in a weak sense. We illustrate this through a simple setup considered in , which precisely characterized the asymptotic distribution of the minimizers of empirical risk with GMM data in the strictly convex case. Consider the following setup:

\[(}^{},}^{})=\ *{arg \,min}_{,}\ _{i=1}^{n}(_{i}}{}+,_{i} )+ r(),\] (22)

where \(^{|| d}\), \(^{||}\) and \(_{i}^{||}\) is the one-hot encoding of the class index \(c_{i}\). We make the following assumptions:

**Assumption 7**.: _All of the covariance matrices \(_{c}\) are diagonal, with strictly positive eigenvalues \((_{c,i})_{i[d]}\), and there exists a constant \(M>0\) such that for any \(c\) we have \(_{c,i} M\|_{c}\|_{2} M.\)_

Secondly, since we aim at obtaining a result on the weak convergence of the estimators, we assume the same weak convergence for the means and covariances, and that the regularization only depends on the empirical measure of \(\).

**Assumption 8**.: _The empirical distribution of the \(_{c}\) and \(_{c}\) converges weakly as follows:_

\[_{i=1}^{d}_{c}(_{c}-_{c,i })(_{c}-_{c,i})[d]{}  p(,)\] (23)

**Assumption 9**.: _The regularizer \(r()\) is a pseudo-Lipshitz function of finite-order having the following form: \(r()=_{i=1}^{d}_{r}(_{i}),\) for some convex, differentiable function \(_{r}:\). This includes, in particular the squared regularization \(r()=_{i=1}^{d}_{i}^{2}\)._

We briefly comment on the choice of the above assumptions. The boundedness of the \(_{c}\) and \(\) in Assumption 7 guarantees that we are in a case covered both by  and by the assumptions of Theorem 4. The diagonal property of the \(_{c}\) in 7, as well as the joint convergence in Assumption 8, ensure that we can view the minimization problem 22 ensures that \(W^{},b^{}\) converge towards a well-defined limit. Finally, the separability assumption on \(r\) in assumption 9 responds to the fact that we aim for a result on the empirical coordinate distribution of \(W^{},b^{}\)

Under these conditions, the joint empirical measure of the minimizers and of the data moments converges weakly to a fixed limit, indepdendent of the data-distribution:

**Theorem 5**.: _Assume that conditions 1-9 hold, and further that the function \((,y)+r()\) is convex, coercive and differentiable. Then, for any bounded-Lipschitz function: \(:^{3||}\), we have:_

\[[_{i=1}^{d}(\{(}^{})_{c,i }\}_{c},\{_{c,i}\}_{c},\{_{c,i}\}_{c })][n/d=>0]{n,d+}_{ }[(,,)],\] (24)

_where \(\) is a measure on \(^{3||}\), that is determined by the so-called replica equations._

Proof SketchThe proof starts with the observation that the nonlinear system of (replica) equations in  describes the joint-empirical measure of the parameters, means and covariances of the mixtures in a self-consistent manner. Furthermore, for \(h()\) having bounded second derivatives, the perturbation term \(sh()\) can be absorbed into the regularizer. We then utilize topological and analytical arguments to relate the weak convergence to the differentiability Assumption 5 for functions that can be expressed as expectations w.r.t the joint empirical measure in 24. More details can be found in Appendix A.8.

In particular, the above result implies the universality of the overlaps of the minimizers with means, covariances, as well as their geometrical properties such as \(L^{p}\) norms.

### One-dimensional CLT for Random Features

We finally show a conditional one-dimensional CLT for a random features map applied to a mixture of gaussians, in the vein of those shown in . Concretely, we consider the following setup:

\[_{i}=(_{i}),_{i}_{c} (_{c}^{},_{c}^{}),\] (25)

where the feature matrix \(^{p d}\) has i.i.d \((0,}{{d}})\) entries. This setup is much more permissive than the ones in , that restrict the samples \(\) to standard normal vectors. However, we do require some technical assumptions:

**Assumption 10**.: _The activation function \(\) is thrice differentiable, with \(\|^{(i)}\| M\) for some \(M>0\), and we have \(_{g(0,1)}[(g)]=0\). Additionally, the cluster means and covariances of \(\) satisfy for all \(c\)\(\|_{c}^{}\| M,\|_{c}^{}\|_{} M\) for some constant \(M>0\)._

We also place ourselves in the proportional regime, i.e. a regime where \(p/d[^{-1},]\) for some \(>0\). For simplicity, we will consider the case \(k=1\); and the constraint set \(_{p}\) as follows:

\[_{p}=\{^{d}\;\;\|\|_ {2} R,\|\|_{} Cp^{-}\}\] (26)

for a given \(>0\). We show in the appendix the following theorem:

**Theorem 6**.: _Under Assumption 10, and with high probability on the feature matrix \(\), the data \(\) satisfy the concentration assumption 2, as well as the one-dimensional CLT of Assumption 4. Consequently, the results of Theorems 1 and 4 apply to \(\) and their Gaussian equivalent \(\)._

Proof SketchOur proof proceeds by defining the following neuron-wise activation functions:

\[_{i,c}(u)=(u+_{i}^{}_{c}).\] (27)

We subsequently control the effects of the means, covariances and the dimensions of the inputs to prove a result analogous to the one-dimensional CLT for random features in . While we prove the above result for random weights, we note, however that the non-asymptotic results in  also hold for deterministic matrices satisfying approximate orthogonality conditions. Therefore, we expect the one-dimensional CLT to approximately hold for a much larger class of feature maps. Finally, we also note that the above extension of the one-dimensional CLT to mixture of gaussians also provides a proof for the asymptotic error for random features in .

**Conclusions --** We demonstrate the universality of the Gaussian mixture assumption in high-dimension for various machine learning tasks such as empirical risk minimization, sampling and ensembling, in a variety of settings including random features or GAN generated data. We also show that universality holds for a large class of functions, and provide a weak convergence theorem. These results, we believe, vindicate the classical theoretical line of works on the Gaussian mixture design. We hope that our results will stimulate further research in this area. We also believe it crucial to understand the limitations of our extended universality framework, for instance in the cases of data with low-dimensional structure or sparsity.