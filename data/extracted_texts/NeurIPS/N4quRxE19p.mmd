# Avatar: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning

Shirley Wu\({}^{@sectionsign}\), Shiyu Zhao\({}^{@sectionsign}\), Qian Huang\({}^{@sectionsign}\), Kexin Huang\({}^{@sectionsign}\), Michihiro Yasunaga\({}^{@sectionsign}\), Kaidi Cao\({}^{@sectionsign}\)

**Vassilis N. Ioannidis\({}^{}\), Karthik Subbian\({}^{}\), Jure Leskovec\({}^{*@sectionsign}\), James Zou\({}^{*@sectionsign}\)**

\({}^{*}\)Equal senior authorship.

\({}^{@sectionsign}\)Department of Computer Science, Stanford University \({}^{}\)Amazon

###### Abstract

Large language model (LLM) agents have demonstrated impressive capabilities in utilizing external tools and knowledge to boost accuracy and reduce hallucinations. However, developing prompting techniques that enable LLM agents to effectively use these tools and knowledge remains a heuristic and labor-intensive task. Here, we introduce AvaTaR, a novel and automated framework that optimizes an LLM agent to effectively leverage provided tools, improving performance on a given task. During optimization, we design a comparator module to iteratively deliver insightful and comprehensive prompts to the LLM agent by contrastively reasoning between positive and negative examples sampled from training data. We demonstrate AvaTaR on four complex multimodal retrieval datasets featuring textual, visual, and relational information, and three general question-answering (QA) datasets. We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the QA datasets. Code and dataset are available at https://github.com/zou-group/avatar.

## 1 Introduction

Autonomous agents powered by large language models (LLMs) offer substantial promise for complex problem-solving . These agents demonstrate remarkable capabilities in reasoning  and planning . Additionally, their functionality is extended through the use of external tools that provide access to external or private data and specialized operations, such as APIs for interacting with knowledge bases and search engines. These tools enable agents to perform complex tasks like multi-step problem-solving and retrieving diverse information, which is essential for complex retrieval and question-answering (QA) .

Despite the promising capabilities of LLM agents, it remains challenging to engineer effective prompts that guide these agents through a multi-stage process for real-world problem-solving. This process involves (1) decomposing a complex question into an actionable plan with simpler steps, (2) strategically using provided tools to gather relevant information, and, finally, (3) synthesizing intermediate results to produce a coherent and accurate response. Each step requires extensive manual effort and numerous iterations of trial and error to refine the prompts.

Current approaches have primarily focused on directly deploying agents using complex human-designed "mega-prompts" , which require lots of manual trial and error. Nevertheless, such hand-engineered mega-prompts may also result in brittle implementations with suboptimal accuracy (see Figure 2 (a)), where the ReAct agent  easily produces trivial and misleading answers tocustomers' queries about specific products. Furthermore, existing research  on employing LLMs as optimizers often fails to adequately refine the complex strategies for enhancing tool integration and usage. This lack of strategic optimization can lead to less effective, non-generalizable agent applications in complex real-world scenarios.

**Present work:** AvaTaR. To address these challenges, we introduce AvaTaR, an automated framework that optimizes agents for effective tool utilization and excellent task performance. Specifically, we leverage key insights from contrastive reasoning and build a comparator module ("trainer") to generate holistic instructions and prompts (_i.e.,_, computing a robust "gradient") to optimize an actor LLM. We demonstrate our framework on challenging tasks of knowledge base retrieval, which involve complex multi-stage procedures and extensive tool usage, and general QA tasks. Specifically, AvaTaR includes two phases:

* **Optimization phase**. The core of our optimization framework (Figure 1) is a comparator LLM that automatically generates holistic prompts to teach a actor LLM to differentiate between effective and ineffective tool usage. The comparator takes positive and negative data samples, where the current agent performs well and poorly, respectively, to identify overall gaps and systematic errors exhibited by the agent. Unlike per-sample instructions, which can easily lead to overfitting on individual data points, by constructing multiple samples as a "batch," the comparator can extract a more robust "gradient" to "backpropagate" to the actor. In other words, the comparator can provide more effective and adaptive prompts through batch-wise contrastive reasoning, helping the agent identify flaws in solving challenging multi-stage problems. Following previous methods , we also maintain a memory bank with selected past instructions to prevent the actor LLM from repeating previous mistakes.
* **Deployment phase**. After the optimization phase, the actor with best-performing prompts can be selected for the testing instances. Moreover, in complex retrieval tasks, the iterative optimization through our AvaTaR framework updates the actor for more effective and generalizable action sequences, enabling direct generalization to novel user inquiries at deployment. In Figure 2 (b), the optimized actor creates three novel strategies: 1) precise decomposition of problems by extracting multifaceted attributes, 2) effective tool usage through a sophisticated and robust scoring system, and 3) the strategic combination of different scores, determined by learned coefficients, ensuring accurate and comprehensive retrieval.

**Experimental evaluation**. We conduct extensive experiments on four retrieval datasets and three QA datasets. The retrieval tasks are highly complex, involving multimodal data, including textual, visual, and relational information. AvaTaR consistently outperforms state-of-the-art methods, showing a

Figure 1: **Overview of AvaTaR.** AvaTaR consists of a actor LLM and a comparator LLM. (a) During optimization, the actor generates actions to answer queries by leveraging the provided tools. Then, the comparator contrasts a set of well-performing (positive) and poorly-performing (negative) queries, automatically generating holistic prompts to teach the actor more effective retrieval strategies and tool usage (_cf._ Section 4). (b) At deployment, the actor with optimized prompts or actions can be effectively used to answer new queries.

substantial 14% improvement in the Hit@1 metric. Impressively, with only 25 iterations, AvaTAR boosts the Hit@1 metric from an initial 5.1% to 28.6% on Flickr30K-Entities  and the Recall@20 metric from 30.3% to 39.3% on STARK-Prime . For general QA datasets, AvaTAR outperforms state-of-the-art methods by 13% on average. These improvements, achieved through iterative updates to the prompts, underscore AvaTAR's ability to optimize agents for complex tasks and effective tool usage. Our key contributions are:

* We introduce AvaTAR, a novel framework that optimizes an actor for effective tool utilization through a comparator module that automatically generates holistic prompts.
* We demonstrate AvaTAR on four complex retrieval tasks and three QA tasks, where it significantly outperforms existing agent methods in terms of task performance and generalization ability.
* We provide a comprehensive analysis of the actor's evolution during optimization, highlighting how comparator automatically provides targeted instructions that improve and generalize the actor.

## 2 Related Work

**LLM Agents**. Recent research has leveraged the remarkable language understanding and reasoning abilities of LLMs [1; 47; 54; 55; 41] to complete downstream tasks. For complex tasks that require enhanced capabilities, previous works have positioned LLMs as agents that can interact with environments [4; 6; 13; 18; 21; 26; 27; 40; 48; 55], leverage external tools [6; 28; 31; 33; 36; 38; 39; 66; 68], and gather experiences [7; 61]. For example, ReAct  conducts reasoning and action in an interleaved way, retrieving information from Wikipedia to support reasoning.

**LLM Agents for Retrieval**. Previous research has applied LLM agents to Information Retrieval (IR) systems through pretraining [2; 9; 16; 57], reranking [12; 42], and prompting techniques [11; 18]. In IR systems, the retriever module directly influences the performance of downstream tasks, such as retrieval-augmented generation [20; 29; 30] and knowledge-intensive question answering [34; 52]. For example, EHRAgent  is designed for EHR question-answering, capable of retrieving relevant clinical knowledge through a structured tool-use planning process and an interactive coding mechanism. However, these LLM agents usually employ heuristic (zero-shot) prompts or rely on few-shot examples [18; 25; 40; 55] for downstream tasks, which lack more informed guidance on generating effective retrieval strategies and tool-assisted actions.

**Agent Optimization**. In the field of optimizing LLM agents, previous works have modified the parameters of LLM backbones through fine-tuning or instruction tuning to enhance agent capability [3; 15; 19; 23; 32; 33; 37; 43; 51; 58; 59] or generated better prompts through iterative prompt tuning [11; 18; 45; 50; 56]. Recently, Zhang et al.  conducted agent training by iteratively updating the agents' functions according to the execution history. However, these methods do not explicitly consider targeted optimization for tool usage or the impact on complex multi-stage tasks. Additionally, enhancing agents' generalization abilities [10; 31; 44], essential for real-world applications, has received less attention. In our work, we focus on automatically generating holistic instructions via a novel contrastive reasoning mechanism, targeting effective tool usage and agents' generalization ability. Compared to fine-tuning approaches, AvaTAR only require a small subset of training data and tool descriptions, making it more adaptable and less computationally intensive.

## 3 Problem Formulation

**Definition 1: Tools**. We define tools or APIs as a set of implemented functions with specified input and output variables. We denote the abstract tool space as \(=\{f_{k}:_{f_{k}}_{f_{k}} k=1, 2,\}\), where \(f_{k}\) maps the input \(_{f_{k}}\) to the output \(_{f_{k}}\). For example, the tools can be APIs used for accessing external knowledge via a search index, an encoder model that generates vector representations from text or image data, or a task-specific classifier that outputs probabilities over a list of classes.

**Definition 2: Agents**. An LLM agent, defined as \(:\), is controlled by verbal prompts to generate a flow of actions needed to complete a task. Here \(\) denotes the action sequence \([_{1},,_{L}]\), where each action is defined by a tuple \((f,i_{f},o_{f})\), consisting of a tool function, specified input(s), and a designated variable that receives the output(s). Each action in the sequence can leverage the outputs generated by previous actions, with the final action \(_{L}\) rendering the results for the task.

**Multi-step problem-solving**. Real-world problems are inherently complex and cannot be effectively addressed through straightforward solutions or simple tool usage alone. Solving real-world problems with LLM agents can be structured into a multi-stage procedure:

* **Decomposition of the problem**: The procedure begins by breaking down a complex question into an actionable plan characterized by simpler steps. This decomposition is crucial for setting clear objectives and facilitating focused problem-solving.
* **Tool-assisted subproblem solving**: In the subsequent phase, agents strategically utilize tools from the established tool space \(\) to gather solutions for each step. This stage is essential for acquiring

Figure 2: **Comparison between AVATaR and ReAct**. (a) The ReAct agent exhibits incomplete task decomposition and employs suboptimal tool combinations, such as lengthy string matching, leading to poor task performance. (b) AVATaR decomposes the task into multiple steps, such as type filtering and flexible token matching. Moreover, it implements robust tool usage and precise synthesis with learned parameters from the optimization phase to achieve excellent performance on new queries.

the necessary information required to effectively address each subproblem of the decomposed problem.
* **Synthesis and response formulation**: The final stage involves synthesizing the intermediate results to construct a precise response. This synthesis not only combines the data but may also refine the response through trials and adjustments, ensuring the solution's accuracy and relevance.

For example, retrieval tasks are inherently complex and demanding. Given a user query \(q\), retrieval tasks aim to identify or generate a ranked list of relevant entities \(E\) from the entity space of a knowledge base. Each query is associated with a set of ground truth answers, denoted as \(Y\), which are used to compute the quality of the prediction. Specifically, the LLM agent is required to 1) comprehend a user's request, 2) utilize the provided tools to identify and analyze relevant information in the large knowledge space, which may contain multimodal data sources, and finally, 3) integrate all gathered information to reason and generate an accurate response.

## 4 Our Method: Optimizing Agents for Tool-Assisted Multi-Step Tasks

Each step in the multi-stage problem-solving process (described in Section 3) requires effective prompts to identify key flaws and improve task performance. However, refining the agents' prompts demands extensive manual effort and numerous iterations of trial and error.

To address this, we introduce an automated and novel optimization framework, AvaTaR, which generates prompts to improve agents' tool usage and task performance. In Table 1, we highlight four critical aspects of our approach compared with prevailing agent frameworks [27; 41; 55]. Here, we introduce the two main LLM components in AvaTaR: a actor LLM (Section 4.1) and a comparator LLM (Section 4.2).

### Actor Construction and Challenges

**Actor**. The actor agent, as defined in Section 3, is responsible for generating initial actions based on the initial instructions/prompts and adjusting actions according to updated instructions. Specifically, the initial instructions provide details about the task and available tools, where tools can be introduced in programming languages such as Python. During optimization, the prompts further incorporate the previous action sequence and updated instructions to adjust these actions. The actor then generates revised actions, which could include a combination of tool usage through programming language (code generation) along with natural language explanations of how the tools are employed.

**Challenges in multi-step complex tasks**. A common approach to updating instructions utilizes execution results or performance data from a specific instance, often through techniques like self-explanation [4; 27] or self-reflection [41; 56]. However, this approach may not be suitable for complex tasks involving tool usage. Complex multi-step tasks include multiple interacting factors that influence overall performance, such as problem decomposition and tool selection. Consequently, instructions generated for a failed/negative query instance tend to be narrow in scope and may fail to identify flaws across all components of a complex solution. Additionally, while certain tool combinations may be effective for one type of input, their effectiveness can vary across different scenarios, potentially leading to decreased performance when applied to varied cases.

    & Self-Improvement & Memory & Generalization & 
 Holistic Prompt Generation \\ (on Tool Usage) \\  \\  ReAct  & ✗ & ✗ & ✗ & ✗ \\ Self-refine  & ✓ & ✗ & ✗ & ✗ \\ Reflexion  & ✓ & ✓ & ✗ & ✗ \\ AvaTaR (Ours) & ✓ & ✓ & ✓ & ✓ \\   

Table 1: **Key differences between AvaTaR and prevailing agent methods.** AvaTAR demonstrates the ability to: 1) self-improve on specific tasks, 2) retain memory throughout the optimization process, 3) enhance the agent’s generalization capability, and 4) autonomously generate holistic, high-quality prompts for better tool usage. Please refer to Section 4 for details.

### Automate Holistic Instruction Generation with Comparator

To address these challenges, we construct a comparator LLM to update the instructions for the actor. Instead of optimizing on a sampled instance, comparator aims to identify systematic flaws throughout the structured actions/solutions.

**Step 1: Constructing positive and negative queries**. To achieve this goal, as shown in Figure 1, the comparator samples a set of data (question-answer pairs), evaluates the current action sequence on the queries, and categorizes them into well-performing (positive) and poorly-performing (negative) groups based on their performance. Specifically, we define two thresholds, \(\) and \(h\) (where \(0<h<1\)), which serve as the upper and lower bounds for constructing positive and negative queries, respectively. Queries with an evaluation metric (e.g., Recall) value above \(\) are classified as positive, while those below \(h\) are classified as negative. Based on the training dynamics, one could consider adapting the lower bound to ensure a sufficient number of negative samples for selection. After classification, we use random sampling to create a mini-batch of \(b\) queries, with an equal split of positive and negative queries (\(b/2\) each) for contrastive reasoning.

**Step 2: Generating instructions through contrastive reasoning**. After this, the comparator is tasked with contrasting the two groups of queries based on their key characteristics, attributing the performance gap to specific tool usage within the complex solution, and finally suggesting general modifications that can improve overall task performance. The instructions generated by the comparator are then appended to the initial prompts to update the actor.

**Insights/Justification for the comparator**. To illustrate the insights, we draw an analogy from deep neural network training, where extremely small batch sizes can introduce significant noise in gradient estimates and high variance in model updates. By adopting a batched training strategy and sampling positive and negative queries as two "mini-batches," comparator can extract a robust "gradient" to update the actor. This approach encourages comparator to generate more general and comprehensive instructions on the complex action sequence, including problem decomposition, solutions to subproblems, and the final synthesis. Moreover, as contrastive reasoning directly targets disentangling the performance gap related to input patterns and how they are handled differently by the tools, it is particularly effective in helping comparator differentiate and select tools for use. Finally, by identifying systemic flaws across a wide array of negative queries, comparator generates modifications that are not only tailored to individual samples but also to diverse data samples, enhancing generalization to novel cases.

Figure 3: **Demonstration example during optimization. Best viewed in color. The task of the comparator is to automatically generate instructions based on sampled positive and negative queries. Then comparator provides holistic instructions that guide the actor to improve query decomposition, utilize better tools, and incorporate more comprehensive information.**

**Demonstration example**. Figure 3 illustrates an example where comparator contrasts the patterns of positive and negative queries, identifying discrepancies in tool usage within the action sequence. It reveals that, compared to positive queries, negative queries feature more complex product descriptions, more subtle brand mentions, and additional relevant product mentions. These observations suggest: 1) an incomplete problem decomposition involving query attributes like detailed product features, 2) a potentially imprecise brand match using embedding similarity, and 3) a lack of consideration for related products in the results. Informed by these insights, actor updates its action sequence to address these subproblems and use the tools more effectively for the task, such as replacing the embedding tool with an LLM verification tool.

### Logistic Instructions and Memory Construction

**Logistic instructions**. While instructions from the comparator are designed to improve task performance, we incorporate two types of orthogonal instructions to ensure the actions are valid and can be executed efficiently.

* **Validity check**: This instruction is triggered internally during the execution of each action. It ensures the validity of the actor's actions, such as verifying the correct use of function calls.
* **Timeout error**: To prevent inefficient action sequences that may stall the actor, we implement a timeout mechanism that triggers an error if processing exceeds a specified threshold. This error prompts the actor to adopt more efficient strategies, such as eliminating redundant operations.

**Memory Bank**. During optimization, we utilize a memory bank inspired by human decision-making processes, following Shinn et al. , where humans typically address current problems by analyzing the current situation and referencing past experiences. The memory bank stores tuples of action sequences, instructions from comparator, and the performance of these action sequences on a small training set (sampled from positive and negative queries). To manage the context size input to actor, we retain only the top-\(5\) action sequences with the best performance. This memory bank enables actor to learn from both immediate instructions and historical results.

**Deployment**. At deployment, we can apply the optimized instructions or, as shown in Figure 1, the optimized actor /action sequence, which includes effective tool utilization and problem-solving strategies, to answer queries or retrieve entities. In the experiments, we demonstrate AvaTAR's flexibility by applying different deployment strategies.

## 5 Experiments

**Tasks and Evaluation**. We conduct experiments on the following datasets:

* [leftmargin=*]
* **Four challenging retrieval datasets from STARK  and Flickr30K-Entities ** to demonstrate AvaTAR in handling complex real-world tasks (_cf._ details in Appendix A). For each query in the retrieval datasets, the task is to retrieve relevant entities, such as nodes in a knowledge graph or images in knowledge bases. During deployment, we directly apply the optimized action sequence to the test queries. We assess task performance by comparing the consistency of the results with the ground truth answers in the datasets, using Hit@1, Hit@5, Recall@20, and Mean Reciprocal Rank (MRR) as the metrics.
* **Three question-answering (QA) benchmarks: HotpotQA , ArxivQA , ToolQA **, where the task is to provide natural language answers to the questions. We sample 100, 100, and 40 training queries, and 100, 100, and 60 testing queries for the three benchmarks, respectively. During deployment, the actor LLM uses optimized instructions to generate the action sequence for obtaining the answer. We use exact match (EM) score on HotpotQA, following previous methods. For ArxivQA and ToolQA, we use the LLM judge score for more reliable evaluation.

**Baselines**. For the knowledge retrieval tasks, we employ several embedding-based retriever models for our evaluation, following Wu et al. : Dense Passage Retriever (DPR) Karpukhin et al. ; Vector Similarity Search methods ada-002 and multi-ada-002 using text-embedding-ada-002 from OpenAI; and a relation-aware model, QAGNN , for the STARK benchmark. Additionally, we include four prevailing agent frameworks to further enrich our evaluation:

* **ReAct ** conducts reasoning and action in an in-context and interleaved manner to enable LLMs to interactively analyze observed information and perform actions.

* **Reflexion** uses self-reflection on the current task completion and stores these reflections in an episodic memory buffer to enhance decision-making in subsequent trials.
* **ExpeL** extracts insights from successful and failed action sequences, retrieving and including them in the context during inference. We apply ExpeL on the QA datasets and, due to its high cost on large-scale retrieval tasks, compare it with AvaTAR on a sampled STARK-MAG test set.
* **Retroformer** reinforces LLM agents and automatically tunes their prompts by learning a retrospective model through policy gradient. We compare the performance of AvaTAR with the reported result by Retroformer on HotpotQA due to the additional training involved.

We include an ablation model, AvaTAR-C, which removes the comparator from our optimization pipeline. This comparison aims to validate the effectiveness of the comparator. The LLM version information is provided in Appendix B.

**Function library**. For the knowledge retrieval tasks, our function library consists of twenty-eight functions that facilitate access to, operation on, and reasoning over the knowledge information by LLM agents. For the QA tasks, we provide web search tools such as Google and Arxiv search APIs. See Appendix E for details. We used the same function library across all agent methods.

**General pipeline**. For AvaTAR, we optimize the agent for a fixed number of epochs and select the action sequence or instruction with the highest performance. We use the same initial prompt structure, the metric Recall@20 or Accuracy for constructing positive and negative queries, and hyperparameters (\(=h=0.5\), \(b=20\)) for all datasets.

### Textual and Relational Retrieval Tasks

We employ the Amazon, MAG, and Prime datasets from the STARK benchmark , a large-scale semi-structured retrieval benchmark that integrates textual and relational knowledge (_cf._ detailed description in Appendix A). Here, the entities to be retrieved are defined as nodes in a graph structure, with knowledge associated with each entity including both textual descriptions and relational data. We use the official splits from the STARK benchmark.

**Takeaway 1:**AvaTAR outperforms state-of-the-art models**. Table 3 shows that AvaTAR substantially outperforms leading models such as Reflexion across all metrics on the STARK benchmark. Notably, the average improvement of AvaTAR is 15.6% on Hit@1 and 9.5% on MRR. ReAct agents, however, cannot optimize based on instructions for improved tool usage and tend to select tools based on the LLM's prior knowledge, which may not be optimal for the given task. We observe that ReAct agents apply similar tools across various queries and struggle to explore alternative tool usage even with extensive in-context reasoning. Results for agent methods using GPT-4 Turbo are provided in Appendix B, showing similar conclusions. For comparison with ExpeL, the results in Table 6 show that it performs similarly to ReAct, underperforming AvaTARby a large margin.

**Takeaway 2:**Comparator greatly impacts the actor's performance**. The comparison of AvaTAR with its ablation variant, AvaTAR-C, highlights the significant advantages of the comparator module. Although AvaTAR-C conducts validity and timeout checks, integrating Comparator into AvaTAR adds a comprehensive instruction mechanism crucial for identifying clear directions to improve the agents, underlining comparator's key role in optimizing actor.

    &  &  &  \\   & **Hit@1** & **Hit@5** & **R@20** & **MRR** & **Hit@1** & **Hit@5** & **R@20** & **MRR** & **Hit@1** & **Hit@5** & **R@20** & **MRR** \\  DPR & 15.29 & 47.93 & 44.49 & 30.20 & 10.51 & 35.23 & 42.11 & 21.34 & 4.46 & 21.85 & 30.13 & 12.38 \\ QAGNN & 26.56 & 50.01 & 52.05 & 37.75 & 12.88 & 39.01 & 46.97 & 29.12 & 8.85 & 21.35 & 29.63 & 14.73 \\ ada-002 & 39.16 & 62.73 & 53.29 & 50.35 & 29.08 & 49.61 & 48.36 & 38.62 & 12.63 & 31.49 & 36.00 & 21.41 \\ multi-ada-002 & 40.07 & 64.98 & 55.12 & 55.52 & 29.50 & 54.03 & **50.80** & 36.94 & 15.10 & 33.56 & 38.05 & 23.49 \\ ReAct & 42.14 & 64.56 & 50.81 & 52.30 & 31.07 & 49.49 & 47.03 & 39.25 & 15.28 & 31.95 & 38.63 & 22.76 \\ Reflection & 42.79 & 65.05 & 54.70 & 52.91 & 40.71 & 54.44 & 49.55 & 47.06 & 14.28 & 34.99 & 38.52 & 24.82 \\ AvaTAR-C & 40.92 & 63.63 & 53.68 & 51.73 & 33.25 & 52.17 & 47.88 & 41.34 & 8.82 & 23.82 & 30.32 & 16.20 \\ AvaTAR & **49.87** & **69.16** & **60.57** & **58.70** & **44.36** & **59.66** & 50.63 & **51.15** & **18.44** & **36.73** & **39.31** & **26.73** \\  Relative Improvement & 16.6\% & 6.3\% & 9.9\% & 12.2\% & 9.6\% & 2.1\% & -0.3\% & 8.7\% & 20.7\% & 5.0\% & 2.1\% & 7.7\% \\   

Table 2: Retrieval performance (%) on STARK benchmark. Last row shows the relative improvements over the best metric value in each column.

**Takeaway 3:** AvaTaR effectively improves agents during optimization**. Figure 4 illustrates the agents' performance on the validation set during optimization. Impressively, AvaTaR agents show significant performance improvements, e.g., from 35% to 75% on Amazon and from 20% to 78% on MAG. This evidence strongly supports the effectiveness of the instructions generated by our comparator. Additionally, our memory bank, which stores past best-performing actions, encourages AvaTaR agents to gradually converge by the end of the optimization process.

**Takeaway 4:** AvaTaR can generalize to real-world tasks**. Comparator generates instructions tailored to groups of retrieval queries, promoting generalizable modifications for novel queries. We validate this capability by applying optimized actions to human-generated leave-out queries from the STARK benchmark, which differ notably from the training data used to optimize our agents. Results in Table 5 (Appendix B) show that AvaTaR significantly outperforms other models, achieving an average improvement of 20.9% on Hit@1. Further, in another study of Appendix B, we assess AvaTaR's robustness to hyperparameters \(h\) and \(\), showing that it maintains stable performance and generalization across different parameter values.

### Image Retrieval Task

We further experiment on Flickr30K Entities, an image retrieval dataset of 30k images with annotated bounding boxes and descriptive phrases (Appendix A). In Table 2, AvaTaR again shows significant improvements. In contrast, Reflexion agents struggle with "overfitting," where they are easily misled by specific image data, leading to inappropriate actions (e.g., trying to "extract the color of a hat" from images without hats). AvaTaR effectively avoids such pitfalls through batch-wise contrastive reasoning, which provides a broader perspective.

**Takeaway 5:** AvaTaR generates impressive and generalizable actions**. In Figure 5 (left), the final actions of the AvaTaR agent detailed in Figure 8 (Appendix B), achieve advanced performance. Notably, AvaTaR skillfully manages input queries and leverages Inverse Document Frequency (IDF) scores to refine phrase matching, ultimately synthesizing accurate answers. Beyond using existing tools, AvaTaR agents can develop high-level tools, such as IDF-based reweighting, suggesting a promising direction for dynamic tool libraries and enhanced tool generation.

**Takeaway 6: Emerging behaviors during optimization**. In Figure 6, we present concrete cases illustrating key interactions between actor and comparator. In each instance, comparator identifies critical flaws, including information omission, ineffective tool usage, and suboptimal synthesis of varying scores. The instructions subsequently prompt actor to enhance retrieval strategies, tool selection, and precise score combinations. Furthermore, frequent references to tool usage underscore comparator's focused examination of tool utilization during optimization.

Figure 4: **Optimization dynamics of AvaTaR agents on STARK. The figures show validation performance (solid line) and its moving average (dashed line) during the optimization of AvaTaR.**

Figure 5: Performance (left) and AvaTaR’s optimization dynamics (right) on Flickr30K-Entities.

### Question Answering Tasks

Finally, we applied AvaTaR to three widely used QA benchmarks. For ToolQA, we tested AvaTaR and the baselines on two different domains: SciREX, which focuses on extracting information from full-length machine learning papers, and Agenda, which involves personal agenda-related questions. Both datasets have easy and hard versions.

**Takeaway 7:**AvaTaR outperforms on QA tasks by offering better context understanding. Table 3 shows that AvaTaR consistently outperforms state-of-the-art methods across all three QA datasets, with especially strong results on ToolQA. In SciREX-hard, which focuses on extracting complex information from long scientific papers, AvaTaR shows a 33.1% improvement, while in Agenda-hard, it achieves a 25.0% relative gain. These improvements are attributed to AvaTaR's ability to generate optimized prompts that help the agent better understand the broader patterns and contexts of the questions, leading to more accurate answers and improved generalization across question types, from simple to complex.

## 6 Conclusion and Future Work

In this study, we introduce AvaTaR, a novel framework that automates the optimization of LLM agents for enhanced tool utilization in multi-step problems, focusing on complex retrieval and QA tasks. AvaTaR demonstrates remarkable improvements across seven diverse datasets. This success can largely be attributed to the comparator module, which effectively refines agent performance through the iterative generation of holistic and strategic prompts. A key innovation of comparator is its use of contrastive reasoning with batch-wise sampling, enabling it to identify systemic flaws and extract robust "gradients" for comprehensive agent improvement across diverse scenarios. While we observe substantial progress from AvaTaR, we discuss its limitations in Appendix D regarding its scalability _etc_.Future work can explore extending this methodology to other challenging agent tasks, visual reasoning tasks, and more dynamic environments, or designing better memory banks for dynamically storing knowledge and experience from past training.

    & HotpotQA & ArxivQA &  \\   & &  &  & Agenda-easy & Agenda-hard \\  CoT & 28.0\% & 58.0\% & 1.7\% & 0.0\% & 0.0\% & 0.0\% \\ ReAct & 40.0\% & 72.0\% & 31.7\% & 17.5\% & 38.3\% & 3.33\% \\ Reflexion & 46.0\% & 77.0\% & 28.3\% & 13.3\% & 30.0\% & 3.33\% \\ Expel. & 39.0\% & 73.0\% & 36.7\% & 14.5\% & 56.6\% & 1.67\% \\ Retroformer (\#retty=1) & 51.0\% & - & - & - & - & - \\ AvaTAR-C & 41.0\% & 73.0\% & 31.7\% & 13.3\% & 31.7\% & 1.67\% \\ AvaTAR & **53.0\%** & **84.0\%** & **37.5\%** & **23.3\%** & **60.0\%** & **4.17\%** \\  Relative Improvement & 3.92\% & 9.09\% & 2.18\% & 33.1\% & 5.82\% & 25.0\% \\   

Table 3: Performance (%) on three QA benchmarks. Last row shows the relative improvements over the best metric value in each column.

Figure 6: **Representative instruction types from the comparator. We provide three cases where the comparator guides the actor towards (1) better divide-and-conquer strategies for multi-step problem-solving, (2) more sensible differentiation between good and bad tool usage/combinations, and (3) adjustments in the weights to generate the final answers. We record the number of occurrences \(X\) under each instruction type over 25 iterations on Flickr30K-Entities, indicated by (\(X\)/25).**