# Stability Guarantees for Feature Attributions with Multiplicative Smoothing

Anton Xue Rajeev Alur Eric Wong

Department of Computer and Information Science

University of Pennsylvania

Philadelphia, PA 19104

{antonxue,alur,exwong}@seas.upenn.edu

###### Abstract

Explanation methods for machine learning models tend not to provide any formal guarantees and may not reflect the underlying decision-making process. In this work, we analyze stability as a property for reliable feature attribution methods. We prove that relaxed variants of stability are guaranteed if the model is sufficiently Lipschitz with respect to the masking of features. We develop a smoothing method called Multiplicative Smoothing (MuS) to achieve such a model. We show that MuS overcomes the theoretical limitations of standard smoothing techniques and can be integrated with any classifier and feature attribution method. We evaluate MuS on vision and language models with various feature attribution methods, such as LIME and SHAP, and demonstrate that MuS endows feature attributions with non-trivial stability guarantees.

## 1 Introduction

Modern machine learning models are incredibly powerful at challenging prediction tasks but notoriously black-box in their decision-making. One can therefore achieve impressive performance without fully understanding _why_. In settings like medical diagnosis  and legal analysis , where accurate and well-justified decisions are important, such power without proof is insufficient. In order to fully yield the power of such models while ensuring reliability and trust, a user needs accurate and insightful _explanations_ of model behavior.

One popular family of explanation methods is _feature attributions_. Given a model and input, a feature attribution method generates a score for each input feature that denotes its importance to the overall prediction. For instance, consider Figure 1, in which the Vision Transformer  classifier predicts the full image (left) as "Goldfish". We then use a feature attribution method like SHAP  to score each feature and select the top-25%, for which the masked image (middle) is consistently predicted as "Goldfish". However, adding a single patch of features (right) alters the prediction confidence so much that it now yields "Axolotl". This suggests that the explanation is brittle , as small changes easily cause it to induce some other class. In this paper, we study how to overcome such behavior by analyzing the _stability_ of an explanation: we consider an explanation to be stable if, once the explanatory features are included, the addition of more features does not change the prediction.

Stability implies that the selected features are enough to explain the prediction  and that this selection maintains strong explanatory power even in the presence of additional information . Similar properties are studied in literature and identified as useful for interpretability , and we emphasize that our main focus is on analyzing and achieving provable guarantees. Stability guarantees, in particular, are useful as they allow one to predict how model behavior varies with the explanation. Given a stable explanation, one can include more features, e.g., adding context, whilemaintaining confidence in the consistency of the underlying explanatory power. Crucially, we observe that such guarantees only make sense when jointly considering the model and explanation method: the explanation method necessarily depends on the model to yield an explanation, and stability is then evaluated with respect to the model.

Thus far, existing works on feature attributions with formal guarantees face computational tractability and explanatory utility challenges. While some methods take an axiomatic approach [8; 17], others use metrics that appear reasonable but may not reliably reflect useful model behavior, a common and known limitation . Such explanations have been criticized as a plausible guess at best, and completely misleading  at worst.

In this paper, we study how to construct explainable models with provable stability guarantees. We jointly consider the classification model and explanation method and present a formalization for studying such properties that we call _explainable models_. We focus on _binary feature attributions_ wherein each feature is either marked as explanatory (1) or not explanatory (0). We present a method to solve this problem, inspired by techniques from adversarial robustness, particularly randomized smoothing [21; 22]. Our method can take _any_ off-the-shelf classifier and feature attribution method to efficiently yield an explainable model that satisfies provable stability guarantees. In summary, our contributions are as follows:

* We formalize stability as a key property for binary feature attributions and study this in the framework of explainable models. We prove that relaxed variants of stability are guaranteed if the model is sufficiently Lipschitz with respect to the masking of features.
* To achieve the sufficient Lipschitz condition, we develop a smoothing method called Multiplicative Smoothing (MuS). We show that MuS achieves strong smoothness conditions, overcomes key theoretical and practical limitations of standard smoothing techniques, and can be integrated with any classifier and feature attribution method.
* We evaluate MuS on vision and language models along with different feature attribution methods. We demonstrate that MuS-smoothed explainable models achieve strong stability guarantees at a small cost to accuracy.

Figure 1: (Top) Classification by Vision Transformer . (Bottom) Pneumonia detection from an X-ray image by DenseNet-Res224 from TorchXRayVision . Both are \(224 224\) pixel images whose attributions are derived from SHAP  with top-25% selection. A single \(28 28\) pixel patch of difference between the two attributions (marked **green**) significantly affects prediction confidence.

Overview

We observe that formal guarantees for explanations must take into account both the model and explanation method, and for this, we present in Section 2.1 a pairing that we call _explainable models_. This formulation allows us to describe the desired stability properties in Section 2.2. We show in Section 2.3 that a classifier with sufficient Lipschitz smoothness with respect to feature masking allows us to yield provable guarantees of stability. Finally, in Section 2.4, we show how to adapt existing feature attribution methods into our explainable model framework.

### Explainable Models

We first present explainable models as a formalism for rigorously studying explanations. Let \(=^{n}\) be the space of inputs, a classifier \(f:^{m}\) maps inputs \(x\) to \(m\) class probabilities that sum to \(1\), where the class of \(f(x)^{m}\) is taken to be the largest coordinate. Similarly, an explanation method \(:\{0,1\}^{n}\) maps an input \(x\) to an explanation \((x)\{0,1\}^{n}\) that indicates which features are considered explanatory for the prediction \(f(x)\). In particular, we may pick and adapt \(\) from among a selection of existing feature attribution methods like LIME , SHAP , and many others [5; 8; 23; 24; 25], wherein \(\) may be thought of as a top-\(k\) feature selector. Note that the selection of input features necessarily depends on the explanation method executing or analyzing the model, and so it makes sense to jointly study the model and explanation method: given a classifier \(f\) and explanation method \(\), we call the pairing \( f,\) an _explainable model_. Given some \(x\), the explainable model \( f,\) maps \(x\) to both a prediction and explanation. We show this in Figure 2, where \( f,(x)^{m}\{0,1\}^{n}\) pairs the class probabilities and the feature attribution.

For an input \(x\), we will evaluate the quality of the binary feature attribution \((x)\) through its masking on \(x\). That is, we will study the behavior of \(f\) on the masked input \(x(x)\), where \(\) is the element-wise vector product. To do this, we define a notion of _prediction equivalence_: for two \(x,x^{}\), we write \(f(x) f(x^{})\) to mean that \(f(x)\) and \(f(x^{})\) yield the same class. This allows us to formalize the intuition that an explanation \((x)\) should recover the prediction of \(x\) under \(f\).

**Definition 2.1**.: _The explainable model \( f,\) is consistent at \(x\) if \(f(x) f(x(x))\)._

Evaluating \(f\) on \(x(x)\) this way lets us apply the model as-is and therefore avoids the challenge of constructing a surrogate model that is accurate to the original . Moreover, this approach is reasonable, especially in domains like vision -- where one intuitively expects that a masked image retaining only the important features should induce the intended prediction. Indeed, architectures like Vision Transformer  can maintain high accuracy with only a fraction of the image present .

Particularly, we would like for \( f,\) to generate explanations that are stable and concise (i.e. sparse). The former is our central guarantee and is ensured through smoothing. The latter implies that \((x)\) has few ones entries, and is a desirable property since a good explanation should not contain too much redundant information. However, sparsity is a more difficult property to enforce, as this is contingent on the model having high accuracy with respect to heavily masked inputs. For sparsity, we present a simple heuristic in Section 2.4 and evaluate its effectiveness in Section 4.

### Stability Properties of Explainable Models

Given an explainable model \( f,\) and some \(x\), stability means that the prediction does not change even if one adds more explanatory features to \((x)\). For instance, the model-explanation pair in Figure 1 is _not_ stable, as the inclusion of a single feature group (patch) changes the prediction. To formalize this notion of stability, we first introduce a partial ordering: for \(,^{}\{0,1\}^{n}\), we write \(^{}\) iff \(_{i}_{i}^{}\) for all \(i=1,,n\). That is, \(^{}\) iff \(\) includes all the features selected by \(^{}\)

Figure 2: An explainable model \( f,\) outputs both a classification and a feature attribution. The feature attribution is a binary-valued mask (white 1, black 0) that can be applied over the original input. Here \(f\) is Vision Transformer  and \(\) is SHAP  with top-25% feature selection.

**Definition 2.2**.: _The explainable model \( f,\) is stable at \(x\) if \(f(x) f(x(x))\) for all \((x)\)._

Note that the constant explanation \((x)=\), the vector of ones, makes \( f,\) trivially stable at every \(x\), though this is not a concise explanation. Additionally, stability at \(x\) implies consistency at \(x\).

Unfortunately, stability is a difficult property to enforce in general, as it requires that \(f\) satisfy a monotone-like behavior with respect to feature inclusion -- which is especially challenging for complex models like neural networks. Checking stability without additional assumptions on \(f\) is also hard: if \(k=\|(x)\|_{1}\) is the number of ones in \((x)\), then there are \(2^{n-k}\) possible \((x)\) to check. This large space of possible \((x)\) motivates us to examine instead _relaxations_ of stability. We introduce lower and upper relaxations of stability below.

**Definition 2.3**.: _The explainable model \( f,\) is incrementally stable at \(x\) with radius \(r\) if \(f(x) f(x(x))\) for all \((x)\) where \(\|-(x)\|_{1} r\)._

Incremental stability is the lower relaxation since it considers the case where the mask \(\) has only a few features more than \((x)\). For instance, if one can probably add up to \(r\) features to a masked \(x(x)\) without altering the prediction, then \( f,\) would be incrementally stable at \(x\) with radius \(r\). We next introduce the upper relaxation that we call decremental stability.

**Definition 2.4**.: _The explainable model \( f,\) is decrementally stable at \(x\) with radius \(r\) if \(f(x) f(x(x))\) for all \((x)\) where \(\|-\|_{1} r\)._

Decmental stability is a subtractive form of stability, in contrast to the additive nature of incremental stability. Particularly, decremental stability considers the case where \(\) has much more features than \((x)\). If one can provably remove up to \(r\) non-explanatory features from the full \(x\) without altering the prediction, then \( f,\) is decrementally stable at \(x\) with radius \(r\). Note also that decremental stability necessarily entails consistency of \( f,\), but for simplicity of definitions, we do not enforce this for incremental stability. Furthermore, observe that for a sufficiently large radius of \(r=(n-\|(x)\|_{1})/2\), incremental and decremental stability together imply stability.

**Remark 2.5**.: _Similar notions to the above have been proposed in the literature, and we refer to  for an extensive survey. In particular, for , consistency is akin to preservation, and stability is similar to continuity, except we are concerned with adding features. In this regard, incremental stability is most similar to incremental addition and decremental stability to incremental deletion._

### Lipschitz Smoothness Entails Stability Guarantees

If \(f:^{m}\) is Lipschitz with respect to the masking of features, then we can guarantee relaxed stability properties for the explainable model \( f,\). In particular, we require for all \(x\) that \(f(x)\) is Lipschitz with respect to the mask \(\{0,1\}^{n}\). This then allows us to establish our main result (Theorem 3.3), which we preview below in Remark 2.6.

**Remark 2.6** (Sketch of main result).: _Consider an explainable model \( f,\) where for all \(x\) the function \(g(x,)=f(x)\) is \(\)-Lipschitz in \(\{0,1\}^{n}\) with respect to the \(^{1}\) norm. Then at any \(x\), the radius of incremental stability \(r_{}\) and radius of decremental stability \(r_{}\) are respectively:_

\[r_{}=(x,(x))-g_{B}(x,(x))}{2},  r_{}=(x,)-g_{B}(x,)}{2},\]

_where \(g_{A}-g_{B}\) is called the confidence gap, with \(g_{A},g_{B}\) the top-two class probabilities:_

\[k^{}=*{argmax}_{1 k m}g_{k}(x,), g_{A} (x,)=g_{k^{}}(x,), g_{B}(x,)=_{i k^{ }}g_{i}(x,).\] (1)

Observe that Lipschitz smoothness is, in fact, a stronger assumption than necessary, as besides \((x)\), it also imposes guarantees on \((x)\). Nevertheless, Lipschitz smoothness is one of the few classes of properties that can be guaranteed and analyzed at scale on arbitrary models [22; 28].

### Adapting Existing Feature Attribution Methods

Most existing feature attribution methods assign a real-valued score to feature importance, rather than a binary value. We therefore need to convert this to a binary-valued method for use with astable explainable model. Let \(:^{n}\) be such a continuous-valued method like LIME  or SHAP , and fix some desired incremental stability radius \(r_{}\) and decremental stability radius \(r_{}\). Given some \(x\) a simple construction for binary \((x)\{0,1\}^{n}\) is described next.

**Remark 2.7** (Iterative construction of \((x)\)).: _Consider any \(x\) and let \(\) be an index ordering on \((x)\) from high-to-low (i.e. most probable class first). Initialize \(=\), and for each \(i\): assign \(_{i} 1\) then check whether \( f,:x\) is now consistent, incrementally stable with radius \(r_{}\), and decrementally stable with radius \(r_{}\). If so, terminate with \((x)=\), and continue otherwise._

Note that the above method of constructing \((x)\) does not impose sparsity guarantees in the way that we may guarantee stability through Lipschitz smoothness. Instead, the ordering from a continuous-valued \((x)\) serves as a greedy heuristic for constructing \((x)\). We show in Section 4 that some feature attributions (e.g., SHAP ) tend to yield sparser selections on average compared to others (e.g., Vanilla Gradient Saliency ).

## 3 Multiplicative Smoothing for Lipschitz Constants

In this section, we present our main technical contribution of Multiplicative Smoothing (MuS). The goal is to transform an arbitrary base classifier \(h:^{m}\) into a smoothed classifier \(f:^{m}\) that is Lipschitz with respect to the masking of features. This then allows one to couple \(f\) with an explanation method \(\) in order to form an explainable model \( f,\) with provable stability guarantees.

We give an overview of our MuS in Section 3.1, where we illustrate that a principal motivation for its development is because standard smoothing techniques may violate a property that we call _masking equivalence_. We present the Lipschitz constant of the smoothed classifier \(f\) in Section 3.2 and show how this is used to certify stability. Finally, we give an efficient computation of MuS in Section 3.3, allowing us to exactly evaluate \(f\) at a low sample complexity.

### Technical Overview of MuS

Our key insight is that randomly dropping (i.e., zeroing) features will attain the desired smoothness. In particular, we uniformly drop features with probability \(1-\) by sampling binary masks \(s\{0,1\}^{n}\) from some distribution \(\) where each coordinate is distributed as \([s_{i}=1]=\). Then define \(f\) as:

\[f(x)=}_{s}h(x s),s_{i}()i=1,,n\] (2)

where \(()\) is the Bernoulli distribution with parameter \(\). We give an overview of evaluating \(f(x)\) in Figure 3. Importantly, our main results on smoothness (Theorem 3.2) and stability (Theorem 3.3) hold provided each coordinate of \(\) is marginally Bernoulli with parameter \(\), and so we avoid fixing a particular choice for now. However, it will be easy to intuit the exposition with \(=^{n}()\), the coordinate-wise i.i.d. Bernoulli distribution with parameter \(\).

We can equivalently parametrize \(f\) using the mapping \(g(x,)=f(x)\), where it follows that:

\[g(x,)=}_{s}h(x), = s.\] (3)

Figure 3: Evaluating \(f(x)\) is done in three stages. **(Stage 1)** Generate \(N\) samples of binary masks \(s^{(1)},,s^{(N)}\{0,1\}^{n}\), where each coordinate is Bernoulli with parameter \(\) (here \(=1/4\)). **(Stage 2)** Apply each mask on the input to yield \(x s^{(i)}\) for \(i=1,,N\). **(Stage 3)** Average over \(h(x s^{(i)})\) to compute \(f(x)\), and note that the predicted class is given by a weighted average.

[MISSING_PAGE_FAIL:6]

\(\) with structured dependence in Section 3.3, such that we may exactly and efficiently evaluate \(g(x,)\) at a sample complexity of \(N 2^{n}\). A low sample complexity is important for making MuS practically usable, as otherwise, one must settle for the expected value subject to probabilistic guarantees. For instance, simpler distributions like \(^{n}()\) do satisfy the requirements of Theorem 3.2 -- but costs \(2^{n}\) samples because of coordinate-wise independence. Whatever choice of \(\), one can guarantee stability so long as \(g\) is Lipschitz in its second argument.

**Theorem 3.3** (Stability).: _Consider any \(h:^{m}\) with coordinates \(h_{1},,h_{m}\). Fix \(\) and let \(g_{1},,g_{m}\) be the respectively smoothed coordinates as in Theorem 3.2, using which we analogously define \(g:\{0,1\}^{n}^{m}\). Also define \(f(x)=g(x,)\). Then for any explanation method \(\) and input \(x\), the explainable model \( f,\) is incrementally stable with radius \(r_{}\) and decremently stable with radius \(r_{}\):_

\[r_{}=(x,(x))-g_{B}(x,(x))}{2},  r_{}=(x,)-g_{B}(x,)}{2 },\]

_where \(g_{A}-g_{B}\) is the confidence gap as in (1)._

Note that it is only in the case where the radius \( 1\) that non-trivial stability guarantees exist. Because each \(g_{k}\) has range in \(\), this means that a Lipschitz constant of \( 1/2\) is necessary to attain at least one radius of stability. We present in Appendix A.2 some extensions to MuS that allow one to achieve higher coverage of features.

### Exploiting Structured Dependency

We now present \(_{qv}()\), a distribution on \(\{0,1\}^{n}\) that allows for efficient and exact evaluation of a MuS-smoothed classifier. Our construction is an adaption of  from uniform to Bernoulli noise, where the primary insight is that one can parametrize \(n\)-dimensional noise using a single dimension via structured coordinate-wise dependence. In particular, we use a _seed vector_\(v\), where with an integer _quantization parameter_\(q>1\) there will only exist \(q\) distinct choices of \(s_{qv}()\). All the while, we still enforce that any such \(s\) is coordinate-wise Bernoulli with \(s_{i}()\). Thus, for a sufficiently small quantization parameter (i.e., \(q 2^{n}\)), we may tractably enumerate through all \(q\) possible choices of \(s\) and thereby evaluate a MuS-smoothed model with only \(q\) samples.

**Proposition 3.4**.: _Fix integer \(q>1\) and consider any vector \(v\{0,1/q,,(q-1)/q\}^{n}\) and scalar \(\{1/q,,q/q\}\). Define \(s_{qv}()\) to be a random vector in \(\{0,1\}^{n}\) with coordinates given by_

\[s_{i}=[t_{i}], t_{i}=v_{i}+s_{} 1, s_{}(\{1/q,,q/q\})-1/(2q).\]

_Then there are \(q\) distinct values of \(s\) and each coordinate is distributed as \(s_{i}()\)._

Proof.: First, observe that each of the \(q\) distinct values of \(s_{}\) defines a unique value of \(s\) since we have assumed \(v\) and \(\) to be fixed. Next, observe that each \(t_{i}\) has \(q\) unique values uniformly distributed as \(t_{i}(1/q,,q/q))-1/(2q)\). Because \(\{1/q,,q/q\}\) we therefore have \([t_{i}]=\), which implies that \(s_{i}()\). 

The seed vector \(v\) is the source of our structured coordinate-wise dependence, and the one-dimensional source of randomness \(s_{}\) is used to generate the \(n\)-dimensional \(s\). Such \(s_{qv}()\) then satisfies the conditions for use in MuS (Theorem 3.2), and this noise allows for an exact evaluation of the smoothed classifier in \(q\) samples. We have found \(q=64\) to be sufficient in practice and that values as low as \(q=16\) also yield good performance. We remark that one drawback is that one may get an unlucky seed \(v\), but we have not yet observed this in our experiments.

## 4 Empirical Evaluations

We evaluate the quality of MuS on different classification models and explanation methods as they relate to stability guarantees. To that end, we perform the following experiments.

**(E1) How good are the stability guarantees?** A natural measure of quality for stability guarantees over a dataset exists: what radii are achieved, and at what frequency. We investigate how different combinations of models, explanation methods, and \(\) affect this measure.

**(E2) What is the cost of smoothing?** To increase the radius of a provable stability guarantee, we must decrease the Lipschitz constant \(\). However, as \(\) decreases, more features are dropped during the smoothing process. This experiment investigates this stability-accuracy trade-off.

**(E3) Which explanation method is best?** We evaluate which existing feature attribution methods are amenable to strong stability guarantees. We examine LIME , SHAP , Vanilla Gradient Saliency (VGrad) , and Integrated Gradients (IGrad) , with focus on the size of the explanation.

**(Experimental Setup)** We use on two vision models (Vision Transformer  and ResNet50 ) and one language model (RoBERTa ). We use ImageNet1K  as our vision dataset and TweetEval  sentiment analysis as our language dataset. We use _feature grouping_ from Appendix A.2.1 on ImageNet1K to reduce the \(3 224 224\) dimensional input into \(n=64\) superpixel patches. We report stability radii \(r\) in terms of the fraction of features, i.e., \(r/n\). In all our experiments, we use the quantized noise as in Section 3.3 with \(q=64\) unless specified otherwise. We refer to Appendix B for training details and the comprehensive experiments.

### (E1) Quality of Stability Guarantees

We study how much radius of consistent and incremental (resp. decremental) stability is achieved, and how often. We take an explainable model \( f,\) where \(f\) is Vision Transformer and \(\) is SHAP with top-25% feature selection. We plot the rate at which a property holds (e.g., consistent and incrementally stable with radius \(r\)) as a function of radius (expressed as a fraction of features \(r/n\)).

We show our results in Figure 4, where on the left we have the certified guarantees for \(N_{}=2000\) samples from ImageNet1K; on the right we have the empirical radii for \(N_{}=250\) samples obtained by applying a standard box attack  strategy with \(q=16\). We observe from the certified results that the decremental stability radii are larger than those of incremental stability. This is reasonable since the base classifier sees much more of the input when analyzing decremental stability and is thus more confident on average, i.e., achieves a larger confidence gap. Moreover, our empirical radii often cover up to half of the input, which suggests that our certified analysis is quite conservative.

### (E2) Stability-Accuracy Trade-Offs

We next investigate how smoothing impacts the classifier accuracy. As \(\) decreases due to more smoothing, the base classifier sees increasingly zeroed-out features -- which should hurt accuracy. We took \(N=2000\) samples for each classifier on their respective datasets and plotted the certified accuracy vs. radius of decremental stability.

Figure 4: Rate of consistency and incremental (decremental) stability up to radius \(r\) vs. fraction of feature coverage \(r/n\). Left: certified \(N_{}=2000\); Right: empirical \(N_{}=250\) with \(q=16\).

We show the results in Figure 5, where the clean accuracy (in parentheses) decreases with \(\) as expected. This accuracy drop is especially pronounced for ResNet50, and we suspect that the transformer architecture of Vision Transformer and RoBERTa makes them more resilient to the randomized masking of features. Nevertheless, this experiment demonstrates that large models, especially transformers, can tolerate non-trivial noise from MuS while maintaining high accuracy.

### (E3) Which Explanation Method to Pick?

Finally, we explore which feature attribution method is best suited to stability guarantees of explainable model \( f,\). All four methods \(\{,,,\}\) are continuous-valued, for which we samples \(N=250\) inputs from each model's respective dataset. For each input \(x\), we use the feature importance ranking generated by \((x)\) to iteratively build \((x)\) in a greedy manner like in Section 2.4. For some \(x\), let \(k_{x}=(x)/n\) be the number fraction of features needed for \( f,\) to be consistent, incrementally stable with radius \(1\), and decrementally stable with radius \(1\). We then plot the average \(k_{x}\) for different methods at \(\{1/8,,4/8\}\) in Figure 6, where note that SHAP tends to require fewer features to achieve the desired properties, while VGrad tends to require more. However, we do not believe these to be decisive results, as many curves are relatively close, especially for Vision Transformer and ResNet50.

## 5 Related Works

For extensive surveys on explainability methods see [16; 20; 34; 35; 36; 37; 38]. Notable feature attribution methods include Vanilla Gradient Saliency , SmoothGrad , Integrated Gradients , Grad-CAM , Occlusion , LIME , SHAP , and their variants. Of these, Shapley value-based  methods [7; 24; 25] are rooted in axiomatic principles, as are Integrated Gradients [8; 41]. The work of  finds confidence intervals over attribution scores. A study of common feature attribution methods is done in . Similar to our approach is , which studies binary-valued classifiers and presents an algorithm with succinctness and probabilistic precision guarantees. Different metrics for evaluating feature attributions are studied in [16; 18; 45; 46; 47; 48; 49; 50; 51]. Whether an attribution correctly identifies relevant features is a well-known issue [52; 53]. Many methods are also susceptible to adversarial attacks [54; 55]. As a negative result,  shows that

Figure 5: Certified accuracy vs. decremental stability radius. \(N=2000\).

Figure 6: Average \(k/n\) vs. \(\), where \(k=\|(x)\|_{1}\) is the number of features for \( f,\) to be consistent, incrementally stable with radius \(1\), and decrementally stable with radius \(1\). \(N=250\).

feature attributions have provably poor performance on sufficiently rich model classes. Related to feature attributions are _data attributions_, which assigns values to training data points. Also related to formal guarantees are formal methods-based approaches towards explainability .

## 6 Conclusion

We study provable stability guarantees for binary feature attribution methods through the framework of explainable models. A selection of features is stable if the additional inclusion of other features does not alter its explanatory power. We show that if the classifier is Lipschitz with respect to the masking of features, then one can guarantee relaxed variants of stability. To achieve this Lipschitz condition, we develop a smoothing method called Multiplicative Smoothing (MuS). This method is parametric to the choice of noise distribution, allowing us to construct and exploit distributions with structured dependence for exact and efficient evaluation. We evaluate MuS on vision and language models and demonstrate that MuS yields strong stability guarantees at only a small cost to accuracy.