# Boosting Learning for LDPC Codes

to Improve the Error-Floor Performance

 Hee-Youl Kwak

University of Ulsan

ghy1228@gmail.com &Dae-Young Yun

Seoul National University

bigbow1204@snu.ac.kr &Yongjune Kim

POSTECH

yongjune@postech.ac.kr &Sang-Hyo Kim

Sungkyunkwan University

iamshkim@skku.edu &Jong-Seon No

Seoul National University

jsno@snu.ac.kr

Corresponding authors.

###### Abstract

Low-density parity-check (LDPC) codes have been successfully commercialized in communication systems due to their strong error correction capabilities and simple decoding process. However, the error-floor phenomenon of LDPC codes, in which the error rate stops decreasing rapidly at a certain level, presents challenges for achieving extremely low error rates and deploying LDPC codes in scenarios demanding ultra-high reliability. In this work, we propose training methods for neural min-sum (NMS) decoders to eliminate the error-floor effect. First, by leveraging _the boosting learning technique_ of ensemble networks, we divide the decoding network into two neural decoders and train the post decoder to be specialized for uncorrected words that the first decoder fails to correct. Secondly, to address the vanishing gradient issue in training, we introduce a _block-wise training schedule_ that locally trains a block of weights while retraining the preceding block. Lastly, we show that assigning different weights to unsatisfied check nodes effectively lowers the error-floor with a minimal number of weights. By applying these training methods to standard LDPC codes, we achieve the best error-floor performance compared to other decoding methods. The proposed NMS decoder, optimized solely through novel training methods without additional modules, can be integrated into existing LDPC decoders without incurring extra hardware costs. The source code is available at https://github.com/ghy1228/LDPC_Error_Floor.

## 1 Introduction

The field of learning-based decoding for error-correcting codes began with research on training neural networks to produce the information vector when given a distorted codeword [1; 2; 3; 4]. These works assume an arbitrary neural network with no prior knowledge of decoding algorithms, and accordingly, face the challenge of learning a decoding algorithm. In contrast, model-based neural decoders are designed by mapping a well-known graph-based iterative decoding algorithm, such as belief propagation (BP) and min-sum (MS) decoding algorithms, to a neural network and then training its weights . Compared to the arbitrary network approaches or the error correction transformer , model-based neural decoders offer the advantages of guaranteeing the performance of existing iterative algorithms and using hardware architectures  that are already well optimized for iterative decoding algorithms.

LDPC codes have been incorporated into WiMAX and 5G communication systems [8; 9], owing to their strong error-correcting capabilities and low decoding complexity [10; 11]. However, more advanced LDPC coding technology needs to be developed for diverse communication environments lying in the scope of future 6G systems. In particular, for environments that require extremely low frame error rate (FER) such as the next generation ultra-reliable and low-latency communications (xURLLC) , it is crucial to mitigate the error-floor in the decoding of LDPC codes. The error-floor phenomenon refers to an abnormal phenomenon where the FER does not decrease as rapidly as in the waterfall region [11; 13]. The error-floor phenomenon also should be addressed for systems demanding very high reliability, such as solid-state drive (SSD) storage , DNA storage , and cryptosystems . However, enhancing other features of LDPC codes often inadvertently reinforces the error-floor phenomenon as a side effect. For instance, the error-floor tends to be intensified when optimizing LDPC codes for superior waterfall performance or decoding by low complexity decoders such as quantized MS decoders . Therefore, research focused on alleviating the error-floor, especially when decoding LDPC codes optimized for performance with low decoding complexity, has become significant. Such advancements will broaden the applications of LDPC codes.

### Main contributions

With this need in mind, we focus on how to train a low-complexity neural MS (NMS) decoder to prevent the error-floor in well designed LDPC codes. The main contributions of the paper are threefold as follows.

_Boosting learning using uncorrected words:_ We first leverage the boosting learning technique [18; 19] that employs a sequential training approach for multiple classifiers, wherein subsequent classifiers concentrate on the data samples that preceding classifiers incorrectly classify. Inspired by this method, we divide the neural decoder into two cascaded neural decoders and train the first decoder to be focused on the waterfall performance, while training the second decoder to be specialized in handling the uncorrected words that are not corrected by the first decoder due to the error-floor phenomenon. Uncorrected words in the error-floor region mostly contain small-error patterns related to trapping sets or absorbing sets , which can be effectively corrected by weighting decoding messages. As a result, a significant performance improvement in the error-floor region is achieved by boosting learning.

_Block-wise training schedule with retraining:_ To mitigate the error-floor, iterative decoding typically requires a large number of decoding iterations, often exceeding \(50\)[17; 20; 21; 22]. However, NMS decoders encompassing many iterations can undergo the vanishing gradient problem in training . To address this problem, we propose a new training schedule inspired by block-wise training methods [24; 25]. The proposed block-wise training schedule divides the entire decoding iterations into sub-blocks and trains these sub-blocks in a sequential manner. Additionally, rather than fixing the weights trained from previous blocks, we retrain them to escape from local minima. As a result, the proposed schedule enables to train numerous weights for all \(50\) iterations successfully while outperforming both the multi-loss method  and the iter-by-iter schedule .

_Weight sharing technique with dynamic weight allocation:_ The weight sharing technique is a way to reduce the number of trainable weights by grouping specific weights to share a common value. The waterfall performance does not severely degrade even if we bundle all weights for each iteration [27; 26]. However, our observations indicate that this does not hold true in the error-floor region, implying that a higher degree of weight diversity is necessary to correct error patterns causing the error-floor. To obtain sufficient diversity with a minimal number of weights, we dynamically assign different weights to unsatisfied check nodes (UCNs) and satisfied check nodes (SCNs) in the decoding process. By utilizing only two weight values for SCNs and UCNs each iteration, we achieve the performance of the NMS decoder using different weights for every edge. This method reduces the number of weights to be trained to only 2.6% of the original number of weights.

We employ these training methods on a range of representative LDPC codes adopted for standards such as WiMAX , IEEE 802.11n , and 5G new radio . The FER point at the onset of the error-floor diminishes by over two orders of magnitude for all codes compared to conventional weighted MS (WMS) decoding . Compared to existing NMS decoding approaches [27; 26], our proposed scheme exhibits a notably enhanced capability to suppress the error-floor. This scheme also achieves a similar performance as the state-of-the-art post-processing method in , with only a third of the iterations.

### Related works

We compare the proposed training scheme with the existing schemes for NMS decoders in Table 1. First, all works except [31; 32] aim to improve the waterfall performance. Although the scope of the works in [31; 32] includes the error-floor performance, they assumed specific conditions of binary symmetric channels (BSC) and FAID, while we deal with the more general situation of additive white Gaussian noise (AWGN) channels and MS decoding. Regarding the training sample selection, the training samples can be received words randomly taken from the AWGN channel [5; 27; 31; 33], or codewords with erroneous trapping sets (or absorbing sets) [30; 32]. However, to use the method in [30; 32], trapping sets should be enumerated, which is only applicable to short LDPC codes and not feasible for medium to large length standard LDPC codes. In contrast, the proposed boosting method, which generates training samples through decoding with linear complexity, can be applied even to codes of several thousand lengths.

For scheduling of training, it is common to train all weights at once (One-shot training)  and some works sequentially train the weights corresponding to a single iteration locally [27; 33], while we train a block of iterations with retraining. In terms of the weight sharing techniques, we confirm that the proposed sharing technique using UCN weights is superior to the spatial or temporal sharing technique used in [5; 27; 30; 31; 32]. Meanwhile, a method of assigning different weights to UCNs has been introduced in , but they applied the same UCN weight to all CNs belonging to a proto CN when at least one CN is unsatisfied, whereas we pinpoint specific UCNs and apply weights individually.

There have been studies adding hypernetworks to the Vanilla NMS decoder [34; 35; 36] or using a transformer architecture  to improve the waterfall performance at the expense of increased training and decoding costs. While the proposed training techniques are broadly applicable to these augmented neural decoders, this work primarily aims to improve the error-floor performance of the Vanilla NMS decoder under practical conditions.

## 2 Preliminaries

### LDPC codes

In this paper, we consider quasi-cyclic (QC) LDPC codes, which have been adopted in various applications due to their implementation advantages [13; 37]. The Tanner graph of a QC-LDPC code, consisting of \(n=Nz\) VNs and \(m=Mz\) CNs, can be obtained by lifting the protograph, composed of \(M\) proto CNs and \(N\) proto VNs, with a lifting factor \(z\)[37; 38]. Let \(E\) be the total number of edges in the protograph. As a running example, we use the WiMAX QC-LDPC code of length \(n=576\) and code-rate \(3/4\) with \(N=24,M=6,E=88\), and \(z=24\).

### Neural min-sum decoding

For iteration \(\), let \(m_{c v}^{()}\) represent the message from CN \(c\) to VN \(v\) and let \(m_{v c}^{()}\) represent the message from VN \(v\) to CN \(c\). The neighboring nodes of \(x\) are represented by \((x)\). The initial conditions are \(m_{v c}^{(0)}=m_{v}^{}\), \(m_{c v}^{(0)}=0\) for the channel LLR \(m_{v}^{}\) of VN \(v\). For \(=1,,\), the

   Reference & Codes & Target region & Decoders & Training sample & Training schedule & Weight sharing \\  This work & Standard LDPC & Waterfall, & MS & Uncorrected words &  Block-wise \\ with retraining \\  & 
 Spatial with \\ UCN weights \\  \\   & BCH & Waterfall & BP, MS & Received words & One-shot & Temporal \\   & Standard LDPC & Waterfall & BP, MS & Received words & Iter-by-Iter & Spatial \\   & Short LDPC & Waterfall & BP & Absorbing set & One-shot & Temporal \\   & Regular LDPC & Waterfall, & FAID & Received words & One-shot & Temporal \\   & Short LDPC & Waterfall, & FAID & Trapping set & One-shot & Temporal \\   & Standard LDPC & Waterfall & Layered & Received words & Iter-by-Iter & UCN weights \\   

Table 1: Comparison between model-based neural decodersNMS decoding algorithm  updates the messages as follows

\[m_{v c}^{()} =_{v}^{()}m_{v}^{}+_{c^{} (v) c}m_{c^{} v}^{(-1)}\] (1) \[m_{c v}^{()} =w_{c v}^{()}(_{v^{}(c)  v}(m_{v^{} c}^{()}) )_{v^{}(c) v}|m_{v^{} c}^{( )}|,\] (2)

where \(_{v}^{()}\) and \(w_{c v}^{()}\) are called the VN weight and CN weight, respectively. At the last iteration \(\), output LLRs \(m_{v}^{}\) are computed as \(m_{v}^{}=m_{v}^{}+_{c^{}(v)}m_{c^{ } v}^{()}\).

By quantizing \(m_{v c}^{()}\), \(m_{c v}^{()}\), and \(m_{v}^{}\), the quantized NMS decoding algorithm is obtained. The quantized decoders are widely used in practical applications due to its low complexity and commonly employed in existing error-floor researches [17; 20; 21; 22]. Therefore, we use it to ensure a fair comparison. Specifically, we use 5-bit uniform quantization with a maximum magnitude of \(7.5\) and a step size of \(0.5\) for the quantized NMS decoder as in [20; 21; 22].

### Training weights for the NMS decoder

If all the weights in (1) and (2) are set to \(1\), NMS decoding is equivalent to MS decoding , or if VN weights \(_{v}^{()}\) are \(1\) and CN weights \(w_{c v}^{()}\) have the same value, the decoder operates as the WMS decoder . The NMS decoder gains performance improvement over the WMS or MS decoder by greatly increasing the diversity of weights. However, the full diversity weights increase the training complexity and require a large amount of memory to store the weights. Therefore, previous studies used weight sharing techniques by assigning the same value to weights with the same attributes. First, since this paper deals with QC-LDPC codes, we use the protograph weight sharing technique  by default, assigning the same weight value to the VNs (or CNs) belonging to a proto VN (or CN). Then, the weights to be trained are represented by \(\{_{v_{p}}^{()},w_{c_{p} v_{p}}^{()}\}\) for a proto VN \(v_{p}\) and a proto CN \(c_{p}\). The total number of weights is then \((N+E)\). If we employ spatial weight sharing in , only one VN weight and one CN weight remain for each iteration, and the weights are \(\{^{()},w^{()}\}_{=1}^{}\), with a total number of \(2\). On the other hand, by using temporal weight sharing  to eliminate differences between iterations, the weights are \(\{_{v_{p}},w_{c_{p} v_{p}}\}\), and the number is \((N+E)\).

The neural network in Fig. 1(b) corresponds to NMS decoding of the Tanner graph in Fig. 1(a) with \(=2\). The input to this neural network is the channel LLR vector \((m_{1}^{},,m_{n}^{})\), and the output is the output LLR vector \((m_{1}^{},,m_{n}^{})\). For each iteration, two hidden layers are arranged, and each hidden layer has a number of nodes equal to the number of edges in the Tanner graph. In the odd hidden layers, the VN to CN message operation in (1) is performed, while in the even hidden layers, the CN to VN message operation in (2) is performed. The input layer is also connected to the odd hidden layers, which corresponds to the addition of the channel LLR in (1). The messages from the \(2\)-th hidden layer to the \((2+1)\)-th hidden layer are weighted by \(w_{c v}^{()}\), and the messages from the

Figure 1: (a) The Tanner graph of an LDPC code and (b) the neural network corresponding to NMS decoding with a maximum iteration of \(=2\).

input nodes to the \((2+1)\)-th hidden layer are weighted by \(_{v}^{()}\). As our goal is to reduce the FER in the error-floor region, we use the FER loss, \(1-_{1 v N}m_{v}^{ }\).

## 3 Proposed training method

In this section, we introduce the proposed training methods through three subsections. The organized training algorithm is shown in Fig. 2.

### Boosting learning using uncorrected words

For the boosting learning approach, we first divide the entire decoding process into two stages: the base decoding stage \(\{1,,_{1}\}\) and the post decoding stage \(\{_{1}+1,,_{1}+_{2}=\}\). Training of base decoder follows the conventional training method: the received words sampled from \(}/}\) region \(_{1}\) are used as training samples. Specifically, we set \(_{1}=20\) and \(_{1}=\{2.0,2.5,3.0,3.5,4.0\}\), which corresponds the waterfall region of WMS decoding. We use the spatial sharing technique (i.e., \(_{B}=\{^{()},w^{()}\}_{=1}^{_{1}}\)) since this achieves comparable performance to the full diversity weights in the waterfall region.

In Fig. 3(a), the base NMS decoder is compared with the MS decoder and the WMS decoder with a single weight of \(0.75\) for \(=20\). The WMS decoding performance has a severe error-floor even though its waterfall performance is better than the MS decoding performance. Compared to the MS and WMS decoders, the NMS decoder for \(=20\) performs better over the training range \(_{1}\). On the other hand, the NMS decoder performs worse than the MS decoder in the error-floor region (e.g., \(4.5\) dB), which is outside \(_{1}\). To improve the performance in the error-floor region, a straightforward approach is extending the training range \(_{1}\) to include the error-floor region. However, the FER of the base decoder for received words from the error-floor region is very low, resulting in an almost negligible FER loss. Consequently, integrating the error-floor region into the training range does not impact the weight update process.

Before training the post decoder, we first collect uncorrected words that the trained base decoder fails to correct among the received words sampled from region \(_{2}\). Then, the uncorrected words serve as training samples for the post decoder, which is distinct from the conventional training methods. The post decoder trains the weights \(\{_{v_{p}}^{()},w_{c_{p} v_{p}}^{()}\}_{= _{1}+1}^{}\) with the aim of correcting the uncorrected words. After completing the training, the trained weights are used for the NMS decoding algorithm in (1)-(2). From the perspective of the NMS decoder, it performs continuous decoding up to iteration \(\) using the trained weights, but for the sake of discussion, we assume as if there are two cascaded decoders (base and post) in the perspective of training. Note that we employ the full diversity weights for the post decoder to confirm the best performance but we will introduce the shared weights \(_{P}\) (used in Fig. 2) in the next subsection. We also set \(l_{2}=10\), \(=30\) for this experiment, and subsequently extend the maximum number of iterations in the following subsection.

To analyze the effectiveness of the proposed boosting learning, we compare the following three cases.

Figure 2: The proposed training method represented by (a) an algorithm and (b) a block diagram.

Case 1: Uncorrected words sampled at \(4.5\) dB in the error-floor region (i.e., \(_{2}=4.5\)).

Case 2: Uncorrected words sampled at \(3.5\) dB in the waterfall region (i.e., \(_{2}=3.5\)).

Case 3: Received words sampled at \(4.5\) dB without filtering.

Regarding Case 1 and Case 2, we collect a total of \(60{,}000\) uncorrected words, allocating \(50{,}000\) for training, \(5{,}000\) for validation, and remaining \(5{,}000\) for test. Training is conducted for \(100\) epochs. Fig. 3(b) shows the distribution of the number of errors after base decoding and post decoding for the test samples used in Case 1 and Case 2. For Case 1, the uncorrected words collected in the error-floor region mainly have a small number of errors since most of decoding errors are trapped in small trapping sets, so the distribution is concentrated on small numbers (see Case 1 after base decoding). For ease of use, we refer to codewords with fewer than \(11\) remaining errors as small-error words. The post decoder, which is mainly trained on these small-error words, corrects a significant number of small-error words (see Case 1 after post decoding). Out of the total \(5{,}000\) test samples, \(68.5\%\) of samples are corrected by the post decoder, resulting that the test FER for the test samples is \(0.315\). This means that, when decoding for received words of the AWGN channel, the resulting FER at \(/N_{0}}=4.5\) dB after post decoding is \(0.315\) times of the FER after base decoding as shown in Fig 3(a). In other words, the post decoder is successfully trained to correct small-error words inducing the error-floor.

On the other hand, Case 2, where the samples are collected from the waterfall region, has a distribution that is widespread across all areas (see Case 2 after base decoding). In addition, the distribution remains almost the same after post decoding (see Case 2 after post decoding), which means that the post decoder fails to reduce the FER sufficiently. For the test samples, the test FER at \(/N_{0}}=3.5\) dB after post decoding is \(0.77\) times of the FER after base decoding whose difference is not noticeable as shown in Fig 3(a). Comparing the results of the two cases, we conclude that composing mainly with small-error words facilitates the post decoder to learn to correct small-error words more effectively. As a result, Fig. 3(a) shows that Case 1 mitigates the error-floor more than Case 2. Meanwhile, for Case 3, where all received words are used as training samples without filtering, almost all of them are corrected during base decoding. Since the post stage training is mainly performed on codewords without errors, the loss function becomes almost \(0\). Then, the weights of the post decoder are unchanged from the initial value \(1\), and accordingly, the performance approaches the MS decoding performance, as shown in Fig. 3(a).

### Block-wise training schedule

In the previous subsection, the number of iterations for the post decoder is set to \(_{2}=10\). To lower the error-floor further, a large number of iterations is required, so we set \(_{2}=30,=50\). However, deep neural decoders with a large iteration number are prone to suffer from the vanishing gradient problem. In order to tackle this issue, we propose a block-wise training schedule which is shown in Fig. 4(a). The proposed training schedule locally trains the weights corresponding to a block of \(_{1}\) iterations at each training stage. In the first stage, the weights belonging to the first block are trained,

Figure 3: (a) Decoding performances of the MS, WMS, NMS decoders and (b) Error distributions after base and post decoding for Case 1 and Case 2.

and in the next stage, the weights of the subsequent \(_{1}\) iterations are trained. At this point, the weight values of previous \(_{2}\) iterations, which are already trained in the first stage, are further trained by taking the result of the first stage training as its initial state. This retraining, which is not used in the tier-by-iter training schedule , assists in preventing the learning process from falling into a local minimum. Note that the method of one-shot training  corresponds to the case of \(_{1}=_{2},_{2}=0\), and the iter-by-iter training schedule  is equivalent to the case of \(_{1}=1,_{2}=0\).

We employ a greedy approach to determine the optimal values for \(_{1}\) and \(_{2}\), resulting that \(_{1}=5,_{2}=10\) offers superior performance in terms of the test FER. Fig. 4(b) shows the evolution of test FER as a function of the iteration number. In the case of one-shot training , the vanishing gradient problem hinders training the weights of earlier iterations, so the test FER stays roughly the same until iteration \(40\) and starts to fall thereafter. The same behavior is observed for \(=40\). Thus, the test FERs of \(=40\) and \(=50\) are almost the same. Next, since the iter-by-iter training schedule  finds the optimal weight for each individual iteration, the test FER falls even at the first iteration of the post decoder (i.e., \(=21\)) without encountering the vanishing gradient problem. However, this local optimization leads to a degraded local minimum, and consequently, the test FER gradually and slowly decreases with each iteration. Likewise, the multi-loss method  shows a similar result. In contrast, the block-wise training schedule with \(_{1}=5,_{2}=0\) shows a superior test FER at iteration \(25\) compared to the other training schedules because it results in a better solution by training the weights of multiple iterations simultaneously. Moreover, the schedule with retraining (i.e., \(_{1}=5,_{2}=10\)) outperforms the schedule without retraining (i.e., \(_{1}=5,_{2}=0\)) at iteration \(30\) though it shows a worse result at iteration \(25\). This implies that through retraining, the weights of intermediate iterations have been adjusted to preprocess error patterns thereby leading to stronger correction capabilities in the final iteration. As a result, at the maximum of \(50\) iterations, the proposed training schedule with \(_{1}=5,_{2}=10\) provides the better test FER value compared to the other training schedules as shown in Fig. 4(b): \(0.11\) for the block-wise, \(0.16\) for the multi-loss, \(0.18\) for the one-shot, \(0.37\) for the iter-by-iter,.

### Weight sharing technique using UCN weights

Assuming the techniques proposed thus far (in detail, uncorrected words at \(/N_{0}}=4.5\) dB and training schedule with \(_{1}=5,_{2}=10\)) are used, we compare the weight sharing techniques

Figure 4: (a): Block-wise training schedule and (b): Evolution of the test FER across iterations.

Figure 5: Illustration of the proposed weight sharing technique and comparison with other sharing techniques.

in Fig. 5. Compared to the full diversity weights, the spatial and temporal sharing techniques significantly reduce the number of distinct weights, but cause performance degradation. In contrast, the proposed sharing technique that introduces a new weight type called UCN weight shows almost identical performance while using only about 2.6% of the weights compared to the full diversity weights. The proposed sharing technique assigns different weights to UCNs and SCNs as shown in Fig. 5. This is feasible because the decoder knows whether a CN satisfies the check equation or not. Using the spatial sharing technique and distinguishing between SCN weight \(w^{()}\) and UCN weight \(^{()}\), the proposed sharing technique can be represented as \(\{^{()},w^{()},^{()}\}_{}\) for iteration \(\) and the total number of distinct weights becomes \(3_{2}\).

Techniques using different weights for UCNs and SCNs have also been proposed in . However, the work  uses only one suppression factor \(\) to represent the UCN weight (i.e., \(^{()}=(1+)w^{()}\)), making the UCN weight dependent on the CN weight. As a result, due to the limited degree of freedom for the UCN weight, it is difficult to obtain the decoding diversity for effectively removing various types of error patterns. Moreover, in , if at least one of the CNs belonging to a single proto CN is unsatisfied, all \(z\) CNs from the proto CN are weighted by the UCN weight. This approach, which applies the same weight to a large number of CNs tied together at the proto level, is not suitable for correcting words with a small number of UCNs, because it does not separately handle individual CNs like the proposed method.

## 4 Performance evaluation

In this section, we compare the proposed and other conventional decoding schemes in terms of the decoding performance. All simulations are performed by NVIDIA GeForce RTX 3090 GPU and AMD Ryzen 9 5950X 16-Core Processor CPU. For training, the weights are trained by the Adam optimizer  with a learning rate of \(0.001\). We evaluate the decoding performance using Monte Carlo methods with at least \(500\) uncorrected words for each FER point. Fig. 6(a) shows the FER performances of the proposed scheme for the WiMAX LDPC code. The proposed scheme incorporates the i) boosting learning with uncorrected words, ii) block-wise training schedule with retraining, and iii) spatial weight sharing with UCN weights. The performance is compared with MS decoding, WMS decoding, and existing NMS decoding schemes in . Among the neural decoder studies listed in Table 1, we exclude comparison with the studies that use FAID and layered decoding, or that require enumerating trapping sets and absorbing sets. In addition, we choose not to compare with augmented neural networks  since our approach does not increase model complexity to deal with the low-error-rate region of long codes. A comparative analysis for short codes in the waterfall region can be found in the appendix.

For the NMS decoding schemes in , the base decoder is used for iterations from \(1\) to \(20\) like the proposed scheme, and the training methods introduced in  are employed for the post stage. The full diversity weights are used for the schemes in  to observe their best performance. For the scheme in , received words in the waterfall region (\(_{}/_{0}\)\(2\)-\(4\) dB) are used as training samples, and the weights for the post stage are trained all at once without a training schedule. For the scheme

Figure 6: FER performances of (a): WiMAX LDPC (length \(576\), rate \(3/4\)), (b): IEEE802.11n LDPC (length \(648\), rate \(5/6\)), (c): 5G LDPC (length \(552\), rate \(1/2\)) codes.

in , received words from the \(/N_{0}}\) points where the MS decoder achieves bit error rate of \(10^{-3}\) are used as training samples, and the iter-by-iter training schedule is employed. The remaining hyper-parameters are set in the same way as in the proposed scheme. As shown in Fig. 6(a), the conventional NMS decoders in [5; 26] show good performance in the waterfall region (\(/N_{0}}\)\(2\)-\(4\) dB), but the error-floor occurs from \(4\) dB. This is because the training samples are composed of received words without filtering. In contrast, the proposed scheme shows excellent performance in both the waterfall and error-floor regions, and the error-floor phenomenon is barely noticeable down to FER of \(10^{-7}\). In particular, comparing the results of \(=20\) and \(=50\), it is confirmed that the post decoder successfully removes the error-floor.

In addition, we compare the proposed scheme with the state-of-the-art post processing scheme in . We directly reference the simulation results from . As shown in Fig. 6(a), the scheme in  shows a similar or worse performance to the proposed scheme, but it has disadvantages of having very high decoding complexity and latency since it consumes a large number of iterations \(=150\). Table 2 compares the schemes in terms of the decoding complexity. The NMS decoder has more multiplications than the MS decoder by \((E+N)z\) due to the weighting operation. The number of other operations is the same as in the MS decoder. Total complexity is evaluated with assumption that the comparison \(C\) is as twice as complex than the addition \(A\) and multiplication \(M\). The additional memory for storing the weights of the proposed scheme is \(3\) which is much lower than those of [5; 26] which exploit full weight diversity. Since the scheme in  does not use weighting, the complexity per iteration is lower than the proposed NMS scheme, but the total complexity is more than twice as high as the proposed NMS scheme due to the higher number of iterations. Moreover, additional complexity is required for the error path detector . In Fig. 6(b), (c), similar results are observed for the IEEE802.11n LDPC and 5G LDPC codes, where the proposed scheme outperforms the other schemes and achieves an FER of \(10^{-7}\) without a severe error-floor.

## 5 Conclusions

This paper proposed training methods for the NMS decoder of LDPC codes to enhance the error-floor performance. Using uncorrected words from the base decoder, we trained the post decoder to be specialized for error patterns causing the error-floor, promoting decoding diversity in the cascaded base and post decoders. We also proposed a training schedule to circumvent the vanishing gradient and local minimum problems, and a weight sharing technique that significantly reduces the number of distinct weights without sacrificing performance. The proposed NMS decoder using the trained weights showed the excellent waterfall and error-floor performances for several standard LDPC codes. Along with the performance improvement, the proposed training scheme has the advantage of being flexibly applicable regardless of the types of channel, code, and decoding algorithm. This scheme can also be implemented directly on hardware architectures without additional costs, and can be directly utilized with no prior analysis of the target code and decoding algorithm.

## 6 Acknowledgments

This work was supported by Samsung Electronics Co., Ltd (IO230411-05859-01), by Electronics and Telecommunications Research Institute (ETRI) grant funded by the Korean government [2021-0-00746, Development of Tbps wireless communication technology], by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS-2023-00212103),and by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS-2023-00247197).