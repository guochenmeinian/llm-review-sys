# How many samples are needed to leverage smoothness?

Vivien Cabannes

Meta AI &Stefano Vigogna

University of Rome Tor Vergata

###### Abstract

A core principle in statistical learning is that smoothness of target functions allows to break the curse of dimensionality. However, learning a smooth function seems to require enough samples close to one another to get meaningful estimate of high-order derivatives, which would be hard in machine learning problems where the ratio between number of data and input dimension is relatively small. By deriving new lower bounds on the generalization error, this paper formalizes such an intuition, before investigating the role of constants and transitory regimes which are usually not depicted beyond classical learning theory statements while they play a dominant role in practice.

## 1 Introduction

The current practice of machine learning consists in feeding a machine with many samples for it to infer a useful rule. In supervised learning, the samples are input/output (I/O) pairs, and the rule is a relationship to predict outputs from inputs. Once learned, the I/O mapping can be deployed in the wild to infer useful information from new inputs. Because it was not engineered by hand, one can question the existence of unwanted behaviors. Classical guarantees regarding the mapping's correctness are offered by statistics: assuming that the training samples are independent and identically distributed according to the future use cases, it is possible to derive theorems akin to the central limit theorem.

While many statistical learning principles offer practical insights, theoretical results often appear somewhat obscure, and forming intuition about them is often challenging, which limits their impact. In this paper, we focus on one simple principle: "smoothness allows us to break the curse of dimensionality". The curse of dimensionality is a generic term referring to a set of high-dimensional phenomena with significant practical consequences. In supervised learning, it manifests as follows: without a good prior on the I/O mapping to be learned, one can only get good estimates of the mapping close to the observed examples; as a consequence, to obtain a good global estimate, one needs to collect enough data points to finely cover the input space, which implies that the number of data points should scale exponentially with the dimension of the input space. Yet, when provided with the information that the mapping has some structure, one might need significantly less examples to learn from. This is notably the case when the mapping is assumed to be smooth. The goal of this paper is to better understand how and when we can expect to get much better convergence guarantees when the target function is known to be smooth.

Related work.Nonparametric local estimators were introduced as soon as the field of learning began to form in the 50's , and their consistency was studied extensively in the second half of the twentieth century (see Stone , Devroye et al.  and references therein). Introduced for scatter plots , local polynomials were the first estimators to leverage smoothness to improve regression . They were later replaced by kernel methods, which are a powerful way to adapt to smoothness without much fine-tuning, and were widely regarded as state-of-the-art before the deep learning era . Convergence results for kernel methods can be understood through the size of their associated functional spaces , how those sizes relate to generalization guarantees , and how those spaces adhere to \(L^{2}\). For least-squares regression, relations between the size of those spacesand generalization guarantees are usually derived through operator concentration [27; 6]. More recently, transitory regime behaviors were described in Mei et al. , Mei and Montanari , and high-dimensional phenomena that lift the need to have more data than dimensions were studied in [23; 14], with Bach  relating the latter analyses with the former ones.

Contributions.This work focuses on the role of smoothness in breaking the curse of dimensionality for typical supervised learning problems. At a high-level, our main contribution is to showcase the importance of _transitory regimes_ where one does not have enough samples to leverage high-order smoothness. In such transitory regimes, the behavior of the excess risk can be quite different than its asymptotic "stationary" behavior. Usually not well described by theory, they might be the dominating regimes in applied machine learning, where the number of samples is often relatively small compared to the input dimension (see Figure 1 for an illustration). This arguably explains the poor performance of kernel methods without strong kernel engineering in the deep learning era: they try to leverage smoothness, but usually do not access enough samples to meaningfully estimate high-order derivatives. More precisely, our contributions are twofold:

* We provide two generic "minimax" lower bounds that illustrate how the curse of dimensionality can not be fully beaten under smoothness assumptions alone.
* We delve more specifically into guarantees offered by algorithms that are built to leverage smoothness assumptions, and get a fine-grained picture of some of the transitory regimes where learning takes place in practice.

All results are illustrated by numerical experiments, some of them to be found in Appendix C.

## 2 The Significance of Constants

This section reviews classical results in learning theory, before providing generic lower bounds when relying solely on smoothness assumptions.

### Established Upper Bounds

Supervised learning is concerned with learning a function from an input space \(\) to an output space \(\) from a dataset \(_{n}=(X_{i},Y_{i})_{i[n]}\) of \(n\) examples.1 For simplicity, we will assume \(=\) and \(=^{d}\), or \(=^{d}\) being the torus. A learning rule, or learning algorithm, is a mapping \(:_{n} f_{n}\) that builds a function \(f_{n}:\) based on the dataset \(_{n}\), with the goal of capturing

Figure 1: Log-log-log-log plots of excess risk (in color) with respect to the number of samples (\(z\)-axis), the regularizer \(\) (\(x\)-axis) and the bandwidth \(\) (\(y\)-axis) when \(f^{*}(x)=( x,e_{1})\) and \(_{}\) is uniform on \([-1,1]^{2}\). Asymptotically, there exists some \((_{n},_{n})\) to ensure that the excess risk decreases in \(O(n^{-})\) for a \(\) predicted by theory. However, for finite values of \(n\), the excess risk can present different power law decays. The dark blue line on each plot indicates the transition between low- and high-sample regime: it corresponds to the graph \(\{(,,n)\,|\,n=_{1}(,)\}\) (see (10) for the definition of \(_{1}\)). The figure illustrates a double descent phenomenon where excess risk peaks are reached when \(n=_{1}\) before a second descent takes place. Those peaks can be avoided in practice by computing the effective dimension \(\) and tuning hyperparameters to ensure it to be smaller than \(n\).

the underlying I/O relation. To discuss generalization to unseen examples, it is standard to model both the already collected and the future examples as independent realizations of a random pair \((X,Y)\).

**Assumption 1**.: _There exists a distribution \(_{}\) that has generated \(n\) independent training samples \(_{n}=(X_{i},Y_{i})_{i[n]}^{ n}\), and will generate future test samples independently._

Under Assumption 1, the quality of a mapping \(f:\) is measured through the excess risk

\[(f)=(f)-(f^{*})=\|f-f^{*}\|_{L^{2 }(_{})}^{2}, \]

where \(_{}\) is the marginal distribution of \(\) on \(\) and, assuming that \((Y\,|\,X=x)\) has a second order moment for every \(x\),

\[f^{*}(x)=[Y\,|\,X=x], (f)=[|f(X)-Y|^{2}]. \]

The risk \((f)\) represents the average error of guessing \(f(X)\) in place of \(Y\) when the error is measured through the least-squares loss. From a statistical viewpoint, it is useful to model \(f_{n}=(_{n})\) as a random function (inheriting its randomness from the samples), so to study the expectation or the upper tail of the excess risk \((f_{n})\). Provided that \(f^{*}\) is measurable, it is possible to find methods such that \((f_{n})\) converges to zero in probability. Without additional assumptions on \(f^{*}\), it is not possible to give any guarantee on the speed of this convergence [12, Theorem 3.1]. However, when \(f^{*}\) is assumed to be smooth, the picture improves consequently.

**Theorem 1** (Breaking the curse of dimensionality ).: _Under Assumption 1, when \(f^{*}\) is \(\)-smooth for \(>0\) in the sense that it admits \(\) derivatives that are regular, more precisely if \(f^{*} C^{}\) (i.e. \(f\) is \(\)-Holder regular), or \(f^{*} H^{}=W^{,2}\) (i.e. \(f\) is \(\)-Sobolev regular), there exists a learning rule \(f_{n}=(_{n})\) that guarantees_

\[_{_{n}}[(f_{n})] cn^{-2/(2+d)}, \]

_where \(c\) is a constant independent of \(n\). Moreover, the bound (3) is minimax optimal, in the sense that for any rule \(f_{n}=(_{n})\) there exists a distribution \(\) such that \(f^{*}^{}\), or \(f^{*} H^{}\), and the upper bound (3) holds as a lower bound with a different constant \(c\)._

Why do constants matter?At first glance, when two algorithms \(_{1}\) and \(_{2}\) guarantee two different upper bounds \(O(n^{-_{1}})\) and \(O(n^{-_{2}})\) on the expected excess risk, \(_{1}\) will be deemed superior to \(_{2}\) if \(_{1}_{2}\), since after a certain number of samples, we will have that \(_{_{n}}[_{1}(_{n})]_ {_{n}}[_{2}(_{n})]\). However, the constants hidden in the front of the big \(O\)s might lead to a different picture when given a small number of samples: \(_{1}\) might actually be a so-called "galactic algorithm", similarly to Strassen's algorithm for matrix multiplication, that might not be worth using without an indecently large number of samples.

### Minimax Lower Bounds

The following lower bounds show how classical algorithms that reach the minimax optimal convergence rates based on smoothness assumptions (3) necessarily present constants that are growing fast with respect to the input dimension. Our analysis holds in noisy settings.2

**Assumption 2** (Homoscedasticity).: _The noise in the label \((Y\,|\,X=x)\) is assumed to be independent of \(x\) with \((Y\,|\,X=x)=^{2}\)._

**Theorem 2**.: _Under Assumptions 2, for any algorithm \(\), there exists a target function \(f^{*}\) such that \(f^{*(+1)}=0\), and_

\[_{_{n}}[((_{n}))] }{n}. \]

**Theorem 3**.: _Under Assumption 2, for any algorithm \(\), there exists a target function \(f^{*}\) such that its Fourier transform is compactly supported on the \(^{}\)-disk of radius \(\), i.e., \(^{*}(^{})=0\) for all \(\|^{}\|_{}>\), and_

\[_{_{n}}[((_{n}))] (2+1)^{d}}{n}. \]Proof Sketch (detailed in Appendix).: The proofs of those _two new theorems_ consist in retaking the standard lower bound in \(^{2}D/n\) when performing linear regression with \(D\) orthogonal features. In Theorem 2, \(D\) corresponds to the number of polynomials of degree less than \(\) with \(d\) variables; in Theorem 3, \(D\) is the number of integer vector \(m^{d}\) whose norm is smaller than \(\). 

Theorems 2 and 3 illustrate how the usage of strong smoothness assumptions on \(f^{*}\) can not guarantee an excess risk lower than \(^{2}\) without accessing an indecently large number of samples with respect to the input dimension (e.g., \(n(1+/d)^{d}\) or \(n 2^{d}^{d}\) respectively). Empirical validations are offered by Figure 2. In their inner-workings, those theorems capture how the rates derived through Theorem 1 are deceptive when one does not have enough samples compared to the size of the hypothesis space that \(f^{*}\) is assumed to belong to; and that the size of smooth functions spaces grows quite fast with respect to the dimension of the input space. Similar lower bound theorems can be proven with rates in \(n^{-2/(2+d)}\) under more detailed assumptions, as illustrated with Theorem 7 in Appendix. Efficient learning beyond the limits imposed by those theorems can only take place when leveraging other priors: for example, sparsity priors would reduce the minimax rates from \(^{2}D/n\) to \(^{2}s(D)/n\) where \(s\) is the sparsity index of \(f^{*}\).

## 3 Crisp Picture in RKHS Settings

To provide a fine-grained analysis of the phenomena at stake, this section presents stylized settings where constants and transitory regimes can be studied precisely, allowing to get a better picture of convergence rates in practical machine learning setups.

### Backbone Analysis

In the following, we shall consider a feature map \(:\), with \(\) a Hilbert space and \( L^{2}(_{})\). The map \(\) linearly parameterizes the space of functions

\[=\{f_{}:x(x),_{ }|\,\} L^{2}(_{}). \]

For example, \(\) could be \(^{k}\), \(\) seen as defining \(k\) features \(_{i}(x)\) on inputs \(x\). This model can be used to estimate \(f^{*}\) through the empirical risk minimizer

\[f_{n,0}*{arg\,min}_{f}_{i[n]}|f(X_{i })-Y_{i}|^{2}. \]

In order to ensure that \(\) can learn any function \(f^{*}\), the features can be enriched by concatenating an infinite countable number of features together. In this setting, it is more convenient to describe the geometry induced by \(\) through the (reproducing) kernel \(k:\) defined as

Figure 2: (Left) Convergence rates as a function of the input dimension \(d\) and the number of samples \(n\) when the target function is \(f^{*}(x)=x_{1}^{n}\) and \(k(x,y)(1+x^{}y)^{5}\). We observe that convergence rates depend heavily on the dimension, which is mainly due to “changing constants”. (Middle) Theoretical lower bound. As the number of samples increases and we enter the high-sample regime, the lower bound resembles the real convergence rates. (Right) Illustration of \(()=\) for \(d=100\), which corresponds to the dimension of the space of polynomials with \(d\) variables of degree at most \(\). Given a number of samples, Taylor expansion can only be estimated meaningfully up to the order \(\) such that \(() n\). In particular, it shows that in dimension one-hundred, one millions samples (\(n=10^{6}\)) only allow to leverage no more than fourth-order smoothness \((=4)\). See Appendix C.2 for details.

\((x),(x^{})\). When \(\) can fit too many functions, the estimate (7) needs to be refined to avoid overfitting. This paper will focus on Tikhonov (also known as ridge) regularization3

\[f_{n}*{arg\,min}_{f}_{i[n]}|f(X_{i})- Y_{i}|^{2}+n\|f\|_{}^{2}, \]

where the norm is defined from (6) as \(\|f\|_{}=\{\|\||f=f_{ }.\}\), but can also be expressed with the sole usage of \(k\) through the integral operator \(K:L^{2}(_{}) L^{2}(_{})\),

\[Kf(x)=_{}k(x,x^{})f(x^{})_{}(x^{})=_{X}[k(x,X)f(X)], \]

as \(\|f\|_{}=\|K^{-1/2}f\|_{L^{2}(_{})}\), with the convention \(K^{-1}( K)=\{+\}\).

The statistical quality of \(f_{n}\) (8) depends on two central quantities, defined as

\[_{a}(K)=(K^{a}(K+1)^{-a})(K)=\|(K+1)^{-1}f^{*}\|_{L^{2}(_{})}^{2}, \]

where \(a=2\). The first term, known as the effective dimension, quantifies the size of the space \(\) in which \(f^{*}\) is searched for. It relates to the variance of the estimator as a function of the dataset \(_{n}\). It will capture the estimation error, the error due to the finite number of accessed samples, related to the risk of overfitting. The second term quantifies the adherence of \(f^{*}\) to \(\). It can be understood as the proximal distance between \(f^{*}\) on \(\) since \((K+1)^{-1}=I-K(K+1)^{-1}\) is a proximal projector. It will capture the approximation error, the error due to the fact that our model does not exactly fit the target function, related to the risk of underfitting.

**Theorem 4** (High-sample regime learning behavior).: _Under Assumptions 1 and 2, as well as two mild technical Assumptions 3 and 4, when \(f^{*}\) is in the closure of \(\) in \(L^{2}(_{})\), there exists a constant \(c\) such that the estimate (8) verifies_

\[|_{_{n}}[(f_{n})]-_{2}(K)}{n}-(K)| c_ {1}(K)(a_{n}_{1}(K)}{n}+a_{n}^{1/2} (K)) \]

_where \(a_{n}=_{+}(K)/n\), and \(_{+}(K)=*{ess\,sup}_{x_{}} (K_{x}(K+1)^{-1})\) with \(K_{x}\) the rank-one operator on \(L^{2}(_{})\) that maps \(f\) to the constant function equal to \([f]k(x,x)\). The different notions of search space size are always related by \(_{2}_{1}_{+}\).4 Moreover, under the interpolation property \(K^{p}(L^{2}(_{})) L^{}(_{})\), i.e. \(\|K^{pf}\|_{}\|f\|_{L^{2}(_{})}\), it holds that \(_{+}(^{-1}K)=O(^{-2p})\); while under the source condition \(f^{*} K^{r}(L^{2}(_{}))\), it holds that \((^{-1}K)=O(^{2r})\)._

While the excess risk upper bound deriving from Theorem 4 is somewhat standard, the _lower bound is new_. This theorem states that the generalization error \(_{_{n}}[(f_{n})]\) behaves as \(A(n,K):=^{2}_{2}(K)/n+(K)\) up to higher order terms specified in the right-hand side. Theorem 4 also holds for ridge-less regression (7) with \(_{1}(K,0)=_{2}(K,0)=\), \(_{+}(K,0)=\|K^{-1}\|^{-1}\|\|_{}\), and \((K,0)=\|f^{*}-_{}f^{*}\|^{2}\), where \(_{}\) is the \(L^{2}(_{})\)-orthogonal projection onto \(\). In this setting, \(_{_{n}}[(f_{n,0})]=A(n,K,0)(1+O(n^{-1}))\). More in general, we conjecture the right-hand side of Theorem 4 to be improvable with the removal of \(_{1}(K)\) in front of the rates (which is due to our usage of concentration inequalities on operators rather than on scalar values), the change of the second \(_{1}\) into \(_{2}\), and the substitution of \(a_{n}^{1/2}\) by \(a_{n}\). This would show that \(_{_{n}}[(f_{n})]\) behaves as \(A(n,K)(1+O(a_{n}))\). In the following, we will call very high-sample regimes situations where \(a_{n} 1\), and _high-sample regimes_ situations where \(_{2}(K) n\).

### Approach Generality

Despite their apparent simplicity, reproducing kernels \(k\) describe rich spaces of functions \(\), known as reproducing kernel Hilbert space (RKHS), namely any Hilbert space of functions with continuous pointwise evaluations . Classical examples are provided by subspaces of analytical functions \(C^{}\)through the Gaussian kernels \(k(x,x^{})=(-\|x-x^{}\|^{2}/^{2})\), and by the Sobolev space \(H^{(d+1)/2}\) through the exponential kernel \(k(x,x^{})=(-\|x-x^{}\|/)\). Reproducing kernels encompass several approaches and algorithms that have been suggested to leverage smoothness of the target function \(f^{*}\).

The first approach consists in estimating local Taylor expansion through local polynomials, defined through

\[(x)=^{-1/2}(_{x A}x^{i})_{i,A }, \]

for \(\) a partition on \(\), \(\) a degree, \(>0\) a regularization parameter. Intuitively in high-dimension problems, where \(d=()\) is big, leveraging local properties may not be very reasonable, since the covering of \(\) with local neighborhoods grows exponentially with the dimension \(d\) (when \(\) has unit volume, and neighborhoods have a fixed radius), meaning that if one wants to have enough samples per neighborhood, \(n\) should scale exponentially with \(d\).

Rather than local properties, the second approach consists in leveraging global smoothness properties, through the estimation of Fourier coefficients. Such estimators can be built implicitly from translation-invariant kernels, defined as

\[k(x,x^{})=^{-1}q((x-x^{})/), \]

for \(q:^{d}\) a basic function, \(\) a bandwidth parameter, and \(\) a regularization parameter. However, the number of frequencies smaller than a cut-off frequency, i.e., the number of trigonometric monomials \(x e^{im^{}x}\) with \(m^{d}\) such that \(\|m\|\), grows exponentially with the dimension, and this approach will not escape from the curse of dimensionality.

Other approaches, such as windowed Fourier estimation, or wavelets expansion estimation (aiming to reconstruct both fine local details together with coarse large-scale behaviors), could be thought of and described through the lens of RKHS. Yet, whatsoever the definition of smoothness considered (i.e., Holder, Sobolev, Besov), all those methods will hit an inherent limit: the number of "smooth" functions increases really fast as the dimension increases. Indeed, since their proofs simply consist in finding \(D\) linearly independent function, lower bound theorems akin to Theorems 2 and 3 could be derived without difficulties for other notions of smoothness.

All the previously described methods are usually endowed with a few hyperparameters that modify the integral operator \(K\) and the norm \(\|\|_{}\) defining the estimator (8). Geometrically, a change of the front regularization parameter \(\) leads to an isotropic rescaling of the ball \(\|\|_{}^{-1}\{1\}\) inside \(L^{2}(_{})\), while a change of other hyperparameters could favor certain directions, or even remove some functions in \(\). In practice, fitting hyperparameters through cross-validation can be understood as implicitly searching to balance and minimize \((K)\) and \((K)\). This fitting allows all those methods to reach the performance of Theorem 1 in \(O(n^{-2/(2+d)})\) as we explain in Appendix.

Figure 3: Illustration of transitory regimes. In essence, Theorem 4 states that \(_{n}:=_{_{n}}[(f_{n})]=A(n,K)(1+h(n,K))\) for \(h=O(_{+}(K)/n)\). We illustrate our upper-lower bound when \(A(n,K)=n^{-1/2}\) and \(nh(n,K)\) is known to be in \([-10^{2},10^{2}]\). The upper-lower bound forces \(_{n}\) to behave in \(n^{-1/2}\) when \(n\) goes to infinity, yet when \(n\) is small, it can showcase quite different “transitory” behaviors.

### Exploration of Transitory Regimes

Given \(n\) samples, Theorem 4 suggests to tune \(K\) so as to minimize \(^{2}(K)/n+(K)\). Interestingly, while theory tends to focus on deriving convergence rates in \(O(n^{-2/(2+d)})\) that maximize the coefficient \(\),5 Theorem 4 can also be leveraged to characterize tightly the expected decay of the generalization error when accessing a small number of samples. To ground the discussion, we will focus on a stylized setting where \(\) and \(\) can be studied in detail as a function of hyperparameters.

**Proposition 1** (Capacity and bias bounds).: _When \(_{}\) is uniform on the torus \(^{d}=^{d}/^{d}\), and \(k\) is a translation-invariant kernel \(k(x,y)=^{-1}q((x-y)/)\), the capacity of the space defined through \(k\) with regularization \(\) and bandwidth \(\) verifies, for \(a\{1,2\}\),_

\[_{a}(,)=_{^{d}}(( )}{()+^{-d}})^{a}\#( ), \]

_where \(\#\) is the counting measure on \(^{d}^{d}\), and \(\) is the (discrete) Fourier transform of \(q\). Similarly, the biases quantifying the adherence of \(f^{*}\) in \(\) verify_

\[(,)=_{^{d}}^{*}( )|^{2}}{(^{d}()+)^{2}}\#( ). \]

_Moreover on \(=^{d}\), if \(_{}\) has a density bounded above by \(_{}<+\), then (14) and (15) become upper bounds for \(\) the Lebesgue measure and \(\) the continuous Fourier transform, at the cost of extra constants in front of their right-hand sides (respectively \(_{}\) and \((_{},1)\) for \(_{a}\) and \(\))._

Proof Sketch.: This relatively standard fact follows from the assumption on \(_{}\) which implies that \(K\) is diagonal in the Fourier domain. 

Proposition 1 unlocks a precise sense of the effective dimension for the Gaussian kernel, defined with \(q(x)=(-\|x\|^{2})\), the exponential kernel, with \(q(x)=(-\|x\|)\), and the Sobolev kernel, with \(()=(1+\|\|^{2})^{-}\), as well as the bias term \(\) when approximating a function \(f H^{}\) with those kernels. This is reported in Table 1 and proved in Appendix B.

High-sample regimes in harmonics settings.Proposition 1 is useful to describe formally different convergence rates profiles that one may expect in practice. In particular, the linearity of the bias characterization (15) is theoretically useful to decorrelate the estimation of different power laws appearing in the Fourier transform of \(f\). More precisely, if

\[|^{*}()|^{2}=_{}^{}c_{}(1+ \|\|^{2})^{-}(),\]

with \(c_{}\) being the inverse of the constant in front of the characterization of \((,;H^{})\) in Table 1, and \(\) some measure with a profile that ensures the good definition of \(f^{*} H^{}\), we have, taking for example \(k\) as the Gaussian kernel,

\[(,;f^{*},k)_{}^{}(^{2} (^{-1}^{d})^{-1})^{}().\]

  Kernel & \(\) & \((,)\) & \((,;H^{})\) \\  Gaussian & \( C^{}\) & \(^{-d}(^{-1}^{d})^{d/2}\) & \(^{2}(^{-1}^{d})^{-}\) \\ Matérn & \(H^{}\) & \(^{-d(2-d)/2}^{-d/2}\) & \(^{(2-d)/}^{/}\) \\ Exponential & \(H^{(d+1)/2}\) & \(()^{-d/(d+1)}\) & \(()^{2/(d+1)}\) \\  

Table 1: Example of translation-invariant kernels, their associated function classes, upper bounds (up to multiplicative constants) on their sizes as a function of the bandwidth \(\) and regularization parameter \(\), as well as on the bias when approximating a function in the Sobolev space \(H^{}\). Here \(C^{}\) stands for the set of analytical functions. Proofs and details are to be found in Appendix B.

In particular, with \(^{2}(^{-1}^{d})^{-1}=n^{-r}\), we get the following convergence rate profile, with \(c_{}\) the constant in front of the characterization of \((,)\) in Table 1,

\[_{_{n}}[(f_{n})]_{r} (^{2}c_{}n^{-1+rd/2}+_{}^{}n^{-  r}())(1+O(a_{n})). \]

This characterization enables us to easily create target functions exhibiting different convergence profiles, as long as we stay in the high-sample regimes where our bounds are meaningful (i.e. when the factor in \(1+O(a_{n})\) is relatively constant).

* _Fast then slow profile._ The first type of profile is built from \(\) that charges most of its mass on fast decays, but also puts some small mass on slow decays. It corresponds to target functions that are roughly well approximated by highly smooth functions, but whose exact reconstruction needs to incorporate less regular functions. The smooth part of the function will be learned quickly, yet the non-smooth part will be learned slowly. Typical examples of such a profile are provided by non-smooth functions that can be turned into infinitely differentiable ones after introducing infinitesimal perturbations, such as \(f^{*}(x)=(-(|x|^{2},M))\), which is only \(C^{0}\), but where one can expect to learn fast before stalling to estimate the \(C^{1}\)-singularity. Another example is given by a function made of a sum of one low-frequency cosine with large amplitude easy to learn together with one high-frequency cosine with small amplitude much harder to learn. We illustrate these profiles on Figure 4.
* _Slow then fast profile._ The second type of profile that can be created is for functions that are supported on a few eigenfunctions of \(K\) associated with small eigenvalues. A typical example of this profile in one dimension would be \(f^{*}(x)=( x)\) for a high frequency \(\). For this target function, no meaningful learning can be done when the search space is too small, because small search spaces do not contain high-frequency functions. On the other hand, when the search space is big enough, the bias quickly goes to zero, allowing for fast learning as long as one controls the estimation error. When provided with few samples, one would prefer a small search space to avoid blowing up of the estimation error, and learning will stall until enough samples are collected to explore bigger search spaces, where \(f^{*}\) could be learned quickly. We illustrate this profile on Figure 4.

Those examples illustrate how, given a target function and a range on the number of available samples, convergence behaviors might fall in regimes that do not correspond to the steady convergence rate in \(O(n^{-2/2+d})\) predicted by Theorem 1. The intuition beyond those examples is not specific to translation-invariant kernels in harmonics settings, but holds more generically for abstract RKHS.

Empirical study of low-sample regimes.In this work, we have focused on "under-parameterized" situations where the parameters were set to have more samples than the effective dimension of the

Figure 4: Composite convergence rates profile. The \(x\)-axis corresponds to the number of samples, while the \(y\)-axis corresponds to the excess risk. From the analysis in Section 3.3, one can build different convergence rates profiles. For example, regular functions with relatively high-frequencies are going to be hard to learn with few samples but really easy after a certain number of samples, roughly equals to the number of harmonics with lower-frequencies); while regular low-frequency functions with singularity are going to show convergence rates where the coarse details of the functions are learned with few samples, but the reconstructions of fine-grained details will required much more samples. The former profile is illustrated with the left figure, and the latter on the right figure. For any sample sizes, excess risk is reported for the best hyperparameters, found with cross-validation. More details are provided in Appendix C.3.

resulting functional space \(_{t(n)}\). In a deep learning world, where many phenomena are understood as taking place in the "over-parameterized" regime, it is of interest to compare our perspective with the double descent phenomenon. Figure 1 shows the excess risk as a function of two of the three parameters \((n,,)\), as well as the graph defined by \(\{(n,,)\,|\,_{1}(,)=n\}\). It illustrates a double descent phenomena with "phase" transition governed by the passage from the low-sample to the high-sample regime. To delve into this, we investigate into regression weight learned by kernel ridge regression. When given access to the knowledge of the full distribution \(\), the estimator in (8) can be rewritten as

\[f_{,}=[Y_{X}],_{X}:x(K+ I)^ {-1}k(X,x). \]

As such, kernel ridge regression can be seen as learning in an unsupervised fashion the weights \(: L^{2}()\), which then indicate how to fold the input space to use information provided by the labels. At a high level, one can think of a scheme, given some input points, to perform finite differences and leverage the result to build an estimate of the target function from Taylor expansions, whatsoever would be the label observations. Figure 5 shows how, when \(\) is not too big, the reconstruction \(f_{,}(x_{0})\) (\(x_{0}\) being the same point at the bluest center on the different pictures on this Figure) depends on observations made far away from \(x_{0}\) according to some periodic pattern, implicitly assuming that the target function should be regular when looked at in the Fourier domain. Similarly, one can look at the weights \(_{X}\) satisfying \(_{_{n}}[f_{n}(x)]=_{(X,Y)}[_{X}(x)Y]\), and whose closed form is given in Appendix C.4. Those weights are shown on Figures 6. They present weird behaviors when the number of data \(n\) is closed to the search space size \(_{2}(K)\). Note that this double descent phenomenon actually takes place in the regularized setup, and not in the interpolation regime, contrarily to prior works on the matter (e.g. 28; 21).

## 4 Conclusion

In this paper, we have shown how subtle is the saying that smoothness allows to break the curse of dimensionality. In essence, without implicit bias and in presence of noise, one needs to be in the high-sample regime where the size of the search space \(\) is smaller than the number of samples to avoid overfitting. As the input dimension grows, many more smooth functions can be defined. This constrains the diversity of functions within \(\), which will typically be devoid of fine-grained details (linked with high-order, eventually trigonometric, polynomials), hence unable to harness high-order smoothness without accessing a large number of samples \(n\).

Future work.Since we have shown that smoothness alone is not a strong enough prior to build efficient learning algorithms in high-dimensions, other priors could be investigated. As such, sparsity assumptions, multi-index models, feature learning or multi-scale behaviors might offer more realistic models to break the curse of dimensionality. How deep learning models exploit such priors has been an active line of research, although linking theoretical results with "interpretable" observations in neural networks remains challenging, and theory has not yet provided that many meaningful insights for practitioners.

Figure 5: Level lines of the weights \(x_{x}(x_{0})\) (17) for a given \(x_{0}\), when \(\) is the torus \(^{2}/^{2}\) and the kernel is taken as the Gaussian kernel with the Riemannian metric on the torus (think of an unrolled donut). Parameters are taken as \(=1\) together with \(=10^{6}\) (left), \(=10^{2}\) (middle) or \(=1\) (right). From this picture, one can build examples of non-smooth functions where the kernel inductive bias will have adversarial effects.

Furthermore, going beyond the sole selection of a few hyperparameters, it would be interesting to understand more aggressive model selection. In particular, given some observations \((X_{i},Y_{i})\) and some hypothesis classes \((_{t})_{t}\), it seems natural to trade a term that fits the data as per (7), together with a regularization term \(_{t}\|f\|_{_{t}}\) that selects \(_{t}\) so that \(f_{n}\) has a small \(_{t}\) norm. We understand this as a _lex parsimoniae_, where each \(_{t}\) encodes different notions of simplicity (e.g. different priors) while \(f_{n}\) only needs to satisfy one of them.

Finally, while this work heavily relies on the least-square loss, practitioners tend to favor other losses such as the cross-entropy. How losses deform and modify the size of the search space \(\) and its adherence properties to some target functions \(f^{*}\) is an open-question -not to mention its adherence properties when the final predictor is built as a decoding \(y(x)=_{y}f(y\,|\,x)\) in order to learn a discrete \(y\) from a score \(f(y\,|\,x)\) that relates to \(_{(X,Y)}(Y=x\,|\,X=x)\).

Experiments reproduction.All the code to run figures is available at [https://github.com/facebookresearch/rates](https://github.com/facebookresearch/rates).

Acknowledgements.VC would like to thank Alberto Bietti, Jaoud Mourtada and Francis Bach for useful discussions. SV was partially supported by the MUR Excellence Department Project MatMod@TOV awarded to the Department of Mathematics, University of Rome Tor Vergata.