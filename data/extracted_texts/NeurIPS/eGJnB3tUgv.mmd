# The four axioms of the NBS:

[MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_FAIL:2]

Finally, we present a thoroughgoing set of empirical studies that evaluate our method using synthetic and real-world datasets, encompassing six key fairness datasets and two image classification tasks. As we'll show, our framework uniformly enhances the performance of one-stage meta-learning methods, yielding up to 10% overall performance improvement and a 67% drop in disparity (Figure 1).

The remainder of this paper is structured as follows: Section 2 describes the problem. Section 3 details our method, including the bargaining game formulation, solution, and theoretical analyses (SS3.1-3.3), and our two-stage meta-learning framework, its theoretical foundations, and dynamical issues (SS3.4-3.6). Section 4 presents empirical results and analysis. Related work is deferred to Appendix A.1.

## 2 Problem Statement

Let \(\) denote a vector of model parameters, let \(D^{(train)}\) denote the training data and let \(D^{(val)}\) a validation set, with \(K\) sensitive groups \(D^{(val)}_{i}\), for \(i[K]\). Let \(L\) be a vector-valued function where each entry corresponds to the per-example conventional loss (e.g., cross-entropy). Define _group-wise validation loss_\(L^{(val)}\) as a vector of size \(K\) where \(L^{(val)}_{i}=_{i}|}L(D^{(val)}_{i}|^{*}(w))^{} \) (the averaged loss over samples in group \(i\)). Define \(L_{}=^{}L^{(val)}\) as the fairness loss with vector \(\) of size \(K\) encoding the fairness objectives under consideration. Following established work , we target group-level fairness objectives via meta-learning as follows:

\[w^{*} =_{w 0}L_{}(D^{(val)}|^{*}(w)),\] (1) \[^{*}(w) =_{}w L(D^{(train)}|).\] (2)

The vector \(w\) is a set of hyperparameters that reweigh each training example in the current minibatch when updating \(\), optimized on \(D^{(val)}\) to improve group-level fairness.

Existing fairness-aware meta-learning work can be characterized as different protocols for \(\). **LtR** computes the average of all group-losses with demographic and label balanced \(D^{(val)}\) (i.e., \(_{}=\)). **FORML** calculates the difference between the maximum and minimum group-loss (i.e., \(_{}\) has 1 for max, -1 for min, and 0 otherwise), emphasizing parity. Meanwhile, group-level Max-Min fairness inspired from  focuses solely on the maximum group-loss , yielding a procedure referred to as **Meta-gDRO**, with \(_{}\) set equal to one for the max and zero otherwise. Throughout the training process, the hypergradient of these methods, \(_{w}L_{}(D^{(val)}|^{*}(w))\), is derived by applying the aggregation protocol \(\) to the _group-wise hyper-gradients_, \(_{w}L^{(val)}_{i}(D^{(val)}|^{*}(w))\). Their training process, wherein the aggregated hypergradient is iteratively utilized to update \(w\), is referred to as a _one-stage method_. They differ from our _two-stage method_, which employs distinct hypergradient aggregation rules at two separate stages.

**Unreliability of one-stage meta-learning for fairness.** The traditional approach of plugging a fairness objective into \(L_{}\) seems natural. However, we find that the effect of this approach on performance and fairness can be unstable. We evaluate the above three one-stage fairness-aware

Figure 2: The unreliable performance of conventional one-stage fairness-aware meta-learning.

methods based on targeted fairness metrics (detailed settings in SS4.2): Overall Area Under the Curve (**Overall AUC**) for LtR which also measures prediction performance, maximum group AUC disparity (**Max-gAUCD**) for FORML, and worst group AUC (**Worst-gAUC**) for Meta-gDRO (Figure 1(a)).

**Delving into the cause.** After training on real data for several epochs, we find that most subsequent epochs have _fewer than 3%_ of the aggregated hypergradients aligned to the optimization objectives of each subgroup. That is, we see _hypergradient conflicts_:

\[ i,g_{i}^{} L_{}<0,\] (3)

with the group-wise gradient \(g_{i}=_{w}L_{i}^{(val)}(D^{(val)}|(w))\) and aggregated direction \( L_{}=_{w}L_{}(D^{(val)}|(w))\). The prevalence of intrinsic hypergradient conflict in one-stage methods is unsurprising, because their aggregation methods are unable to prevent overlooking or incorrectly de-prioritizing certain groups. We study this phenomenon in synthetic settings to isolate structural issues from randomness in stochastic optimization (Figure 1(b), settings in Appendix A.6): We define the performance goal as convergence to the Pareto front, while the fairness goal corresponds to the line \(x=y\). As observed, alignment issues in synthetic experiments are prominent (Figure 1(b)): (**I**) Averaging (LLR) may induce oscillatory dominance among groups. (**II**) Parity-based method (FORML) produces conflicting hypergradients as it subtracts the loss of one group, necessitating performance trade-offs for fairness. (**III**) Minimizing the worst-group loss (Meta-gDRO) often exhibits toggling dominance as it solely prioritizes the current least-performing group, which may create conflicts and cannot land on the Pareto front.

**Hypergradient conflict resolution.** Given the observation of hypergradient conflicts and convergence issues in one-stage meta-learning, we turn to cooperative bargaining and propose a two-stage method that seeks to attain more reliable improvements by resolving conflicts at the early stage of training. Our methodology draws inspiration from the _Nash Bargaining Solution_ (NBS), a cornerstone of axiomatic bargaining in game theory, known for its general applicability and robustness . Nash Bargaining is chosen for its desirable axiomatic properties, which prohibit unconsented unilateral gains by Pareto Optimality, and its principled approach of effectively balancing interests, making it appealing for practical deployment. We provide additional discussion of the game-theoretic perspective and additional empirical comparisons in Appendix A.2.

While the NBS has been studied in multi-task learning , it has yet to be explored in fairness-aware meta-learning. Unlike the settings in , applying the NBS in our context challenges the assumption of linear independence among tasks, which is generally untenable for group-wise utility towards the same goal of performance gain (i.e., the settings of fairness). This drives our exploration into novel proofs and applications of the NBS in hypergradient aggregation, aiming to circumvent the need for linear independence and optimize shared outcomes through strategic negotiation and nested optimization.

## 3 Methodology

### Nash Bargaining framework

We start with some preliminaries. Consider \(K\) players faced with a set \(A\) of alternatives. If all players reach a consensus on a specific alternative, denoted as \(a\) in set \(A\), then \(a\) will be the designated outcome. In the event of a failure to agree on an outcome, a predetermined disagreement result, denoted as \(d\), will be the final outcome. The individual utility payoff functions are denoted \(u_{i}:A\{D\}\), which represent the players' preferences over \(A\). Denote the set of feasible utility payoffs as \(S=(u_{1}(a),...,u_{K}(a)):a A^{K}\) and the disagreement point as \(d=(u_{1}(D),...,u_{K}(D))\). Nash proposed to study solutions to the bargaining problem through functions \(f:(S,d)\). The unique Nash bargaining solution (NBS), originally proposed for two players  and latter extended to multiple players , maximizes the function \(f(S,d)=_{j}(x_{i}-d_{i})\), where \(x_{i}\) is the bargained payoff and \(d_{i}\) is the disagreement payoff for player \(i\). The NBS fulfills four axioms: Pareto Optimality, Symmetry, Independence of Irrelevant Alternatives, and Invariant to Affine Transformations. See Appendix A.3 for detailed definitions, and A.4 for additional assumptions.

In our problem, we want to find \(\) in Algorithm 1, an intermediate vector for the optimal \(w^{*}\) to reweigh each training sample. Let \(L_{i}^{(val)}\) be the validation loss and \(g_{i}=_{}L_{i}^{(val)}\) be the hypergradient of group \(i\). Let \(G\) be the matrix with columns \(g_{i}\). The central question is to find the protocol \(\) as the weights applied to individual group \(i\)'s loss. It associates an update step \( L_{}\) to \(\) that improves the aggregated validation loss among all groups.

We frame this problem as a cooperative bargaining game between the \(K\) groups. Define the utility function for group \(i\) as

\[u_{i}( L_{})=g_{i}^{} L_{}.\] (4)

The intuition is that the utility tells us how much of proposed update is applied in the direction of hypergradient of group \(i\) (as it can be written as \(\|g_{i}\|\| L_{}\|\) with angle \(\) between \(g_{i}\) and \( L_{}\)). This gives the projection of \( L_{}\) along \(g_{i}\), or the "in effect" update for group \(i\). If \(g_{i}\) and \( L_{}\) aligns well, the utility of group \(i\) is large (or _vice versa_). Denote \(B_{}\) the ball of radius \(\) centered at 0. We are interested in the update \( L_{}\) in the agreement alternative set \(A=\{ L: L B_{}, L_{}^{}g_{i}-D^{}g _{i}>0, i[K]\}\). Assume \(A\) is feasible and the disagreement point is \(D=0\) (i.e. the update \( L_{}=0\), staying at \(\)). The goal is to find a \( L_{}\) that maximizes the product of the deviations of each group's payoff from their disagreement point. Since \(u_{i}\) forms a (shifted) linear approximation at \(\), we are essentially maximizing the utility of \(L_{}\) locally. We provide further discussions on the problem setup and assumptions in Appendix A.4.

### Solving the problem

In this section, we will show the NBS is (up to scaling) achieved at \( L_{}=_{i[K]}_{i}g_{i}\), where \(_{+}^{K}\) solves \(G^{}G=\) by the following two theorems, with full proofs available in Appendix A.5:

**Theorem 3.1**.: _Under \(D=0\), \(_{ L_{} A}_{i[K]}(u_{i}( L_{})-d_{i})\) is achieved at_

\[_{i[K]} g_{i}}g_{i}= L_{ },.\] (5)

**Proof Sketch.** We employ the same techniques as in Claim 3.1 of .

**Theorem 3.2**.: _The solution to Equation 5 is (up to scaling) \( L_{}=_{i K}_{i}g_{i}\) where_

\[G^{}G=\] (6)

_with the element-wise reciprocal \(\)._

**Proof sketch.** Let \(x= L_{}\). In line with  we observe that \(x=_{i K}(x^{}g_{i})^{-1}g_{i}\). However, whereas  relied on the linear independence of the \(g_{i}\)'s to uniquely determine each coefficient \((x^{}g_{i})^{-1}\), our technique makes no such assumption. Instead, we multiply both sides of Equation 5 by \(g_{j}\) and obtain \(_{i[K]}(x^{}g_{i})^{-1}(g_{i}^{}g_{j})= x^{}g_{i},j [K]\). Set \(_{i}=(x^{}g_{i})^{-1}\). This is equivalent to \(g_{i}^{}_{i[K]}g_{i}_{i}=_{j}^{-1}\), which is the desired solution when written in the matrix form.

While our solution aligns with that of multi-task learning , our proof of Theorem 3.2 circumvents the necessity for linear independence among \(g_{i}\), one of the core initial assumptions in the previous work. Linear independence does not hold in general as the goals for individual groups (or tasks) might overlap (such as sharing common underlying features) or contradict each other (when there is negative multiplicity). Our proof removes this assumption and sheds light on the effectiveness of updates based on the NBS in general cases. See Appendix A.5 for extended discussions on linear independence.

Furthermore, we derive two useful properties of the NBS in additional to its four axioms, with full proofs available in Appendix A.5:

**Corollary 3.3**.: _(Norm of bargained update) The solution in Theorem 3.2 has \(^{2}\)-norm \(\)._

**Corollary 3.4**.: _If \(g_{j}\) is \(\)-bounded for \(j[K]\), \(\|_{j}^{-1}\|\) is \(()\)-bounded for \(j[K]\)._

Informally, the NBS has implicit \(^{2}\) regularization (Corollary 3.3) which substantiates our empirical observation that a separate \(^{2}\)-normalization on \( L_{}\) for meta-learning rate adjustment yields better performance than \(^{1}\)-normalization (the conventional setting in ). Note that we do not impose any assumptions on the boundedness of the hypergradient \(g_{i}\), ensuring the stability even when certain hypergradients are extreme. Furthermore, when \(g_{i}\) is bounded, Corollary 3.4 implies that \(\|_{j}\|\) is bounded below and no groups are left behind.

### Game-theoretic underpinnings

The NBS provides incentives for each player to participate in the bargaining game by assuming the existence of at least one feasible solution that all players prefer over disagreement (\( x S\) s.t. \(x d\)). This aligns players' interests in reaching an agreement. The constraint \(g_{i}^{} L_{}>0\) resolves hypergradient conflict upon agreement.

Second, note that Equation 6 shows relationship between the individual and interactive components:

\[\|_{i}g_{i}\|_{2}^{2}+_{j i}(_{i}g_{i})^{}(_{j}g _{j})=1,\] (7)

for \(i[K]\). + The relative weights \(_{i}\) emerge from a player's own impact (\(\|_{i}g_{i}\|_{2}^{2}\)) and interactions with others (\((_{i}g_{i})^{}(_{j}g_{j})\)). This trade-off embodies individual versus collective rationality. Positive interactions (i.e., \(g_{i}^{}g_{j}>0\)) incentivize collective improvements by downweighting \(_{i}\). Negative interactions (i.e., \(g_{i}^{}g_{j}<0\)) increase \(_{i}\) to prioritize individual objectives. Furthermore, each player accounts for a nontrivial contribution to the chosen alternative \( L_{}\) under mild assumptions (Corollary 3.4). The negotiated solution balances individual and collective rationality through participation incentives and conflict resolution. This equilibrium encapsulates game-theoretic bargaining.

Footnote †: To give game-theoretic justifications, we build on a similar expression (Equation 2, ).

### Two-stage Nash-Meta-Learning

We now present our two-stage method (Algorithm 1) that incorporates Nash bargaining into meta-learning training. Previous one-stage algorithms fix a predetermined \(L_{}\); our two-stage method assigns \(=\) in the NBS in Stage 1 and sets it back to the original \(_{0}\) in Stage 2. Define \(^{(t)}\) as themodel parameter at step \(t\), \(T\) as the number of total steps, and \(T_{(bar)} T\) as the number of bargaining steps in Stage 1. Let \(_{}(L)\) be the backward automatic differentiation of computational graph \(L\) w.r.t. \(\), and \(()\) be \(^{2}\)-normalization function. Each step \(t\) is constituted by four parts: **The first part** is unrolling inner optimization, a common technique to approximate the solution to a nested optimization problem . We compute a temporary (unweighted) update \(^{(t)}\) on training data, which will be withdrawn after obtaining the udpated \(w\). \(\) is initialized to zero and included into the computation graph. **The second part** calculates the hypergradient \(g_{k}^{(t)}\) which ascertains the descent direction of each training data locally on the validation loss surface. **The third part** is to aggregate the hypergradients as the update direction for \(\) by \(\) from the NBS. In Stage 1, successful bargaining grant the update of \(\) by the NBS. If the bargaining game is infeasible or if we are in Stage 2 (not in the bargaining steps), we calculate \(\) based on the fairness loss \(L_{}\) of our choice. **The last part** is the update of parameter \(^{(t+1)}\) using the clipped and normalized weights \(w\) for each training data in the minibatch.

### Theoretical Properties

**Theorem 3.5**.: _(Update rule of \(\)) Denote \(L_{i}^{(train)}=L(D_{i}^{(t)}|^{(t)})\) for the \(i\)-th sample in training minibatch \(D^{(t)}\) at step \(t\). \(\) is updated as \(^{(t+1)}=^{(t)}-}{|D^{(t)}|}_{i=1}^{|D^{(t)}|} ^{i}\) with \(^{i}=((^{(t)})^{}L^{(val)}))^{ }_{}L_{i}^{(train)},0)_{}L_{i}^{(train)}}\)._

**Theorem 3.6**.: _(Pareto improvement of \(\)) Use \(^{(t)}\) for the update. Assume \(L_{i}^{(val)}\) is Lipschitz-smooth with constant \(C\) and \(g_{i}^{(t)}\) is \(\)-bounded at step \(t\). If the meta learning rate for \(\) satisfies \(^{(t)}^{(t)}}\) for \(j[K]\), then \(L_{i}^{(val)}(^{(t+1)}) L_{i}^{(val)}(^{(t)})\) for any group \(i[K]\)._

**Theorem 3.7**.: _Assume \(L^{(val)}\) is Lipschitz-smooth with constant \(C\) and \(_{}L_{i}^{(train)}\) is \(\)-bounded. If the learning rate for \(\) satisfies \(^{(t)}|}{C\|^{(t)}\|^{2}}\), then \(L_{^{(t)}}(^{(t+1)}) L_{^{(t)}}(^{(t)})\) for any fixed vector \(^{(t)}\) with finite \(\|^{(t)}\|\) used to update \(^{(t)}\)._

Informally, the closed-form update rule of \(\) indicates that the weight of a training sample is determined by its local similarity between the \(\)-reweighed validation loss surface and the training loss surface (Theorem 3.5). Under mild conditions, \(\) yields Pareto improvement for all groups for the outer optimization using the NBS (Theorem 3.6). For the inner optimization, under mild conditions, the fairness loss \(L_{^{(t)}}\) monotonically decreases w.r.t. \(^{(t)}\) regardless of the choice of protocol (Theorem 3.7). This generalizes  from \(=\) to any \(\) with finite norm and provides a uniform property for the fairness-aware meta-learning methods. It entails the flexibility of our two-stage design that switches \(\) between phases. Setting \(=\) gives the desired property for the NBS. The validation loss surface reweighed by the NBS has the maximum joint utility, which empirically boosts the overall performance when used to update \(\). Full proofs are given in Appendix A.5.

### Dynamics of Two-stage Nash-Meta-Learning

Our method captures the interplay and synergy among different groups. Specifically, previous methods like linear scalarization (i.e., assigning a fix weight to each group) are limited to identifying points on the convex envelope of the Pareto front . Our method offers a more adaptable navigation mechanism by dynamically adjusting the weight with the NBS, which accounts for the intricate interactions and negotiations among groups. Moreover, the optimal \( L_{}\) maximizes the utilization of information on validation loss landscape and leads to empirical faster and more robust convergence to the Pareto front even with distant initial points. Although first-order methods typically avoid saddle points [16; 39], if one is encountered, switching to the fairness goal upon unsuccessful bargaining offers a fresh starting point for subsequent bargaining iterations and helps to escape (Figure 6d, Appendix A.7). Our synthetic experiments show that Stage 2 training focused solely on the fairness goal does not deviate the model from the Pareto front (Figure 3). Specifically, the worst group utility \(g_{i} L_{}\) tends to concentrate around zero, ensuring the model stays in the neighborhood of the Pareto front and implying the robustness of our approach. The theoretical understanding of this interesting phenomenon is an open problem for future study.

Evaluation

We evaluate our method in three key areas: synthetic simulation (SS4.1) for Pareto optimality vis-a-vis fairness objectives, real-world fairness datasets (SS4.2), and two imbalanced image classification scenarios (Appendix A.7.3).

### Simulation

Under the synthetic settings (SS2 and Appendix A.6), we observe the convergence enhancements compared to Figure 2b and the effect of continuous bargaining throughout the entire training (another case of one-stage). Figure 3 demonstrates that Nash bargaining effectively resolves gradient conflicts and facilitates convergence to the Pareto front in comparison to Figure 2b. This is evident from the reduced number of non-Pareto-converged nodes in both continuous bargaining (Figure 3a) and early-stage bargaining (our final solution, Figure 3b). Notice that one-stage NBS doesn't always enhance fairness, evidenced by the observation that nodes at the Pareto front tend to stagnate. The NBS does not leverage any information about specific fairness objectives. Our two-stage approach built on the bargaining steps can further push the model to the ideal endpoints.

### Standard fairness benchmarks

We test our method on six standard fairness datasets across various sectors of fairness tasks: financial services (Adult Income , Credit Default ), marketing (Bank Telemarketing ), criminal justice (Communities and Crime ), education (Student Performance ), and disaster response (Titanic Survival ). Test sets comprise 3% of each dataset (10% for the student performance dataset with 649 samples) by randomly selecting a demographically and label-balanced subset. See Table 2 in Appendix A.6 for data distribution specifics.

**General settings and metrics.** We compare our two-stage Nash-Meta-Learning with conventional one-stage fairness-aware meta-learning (i.e., LtR, FORML, Meta-gDRO), baseline training, and Distributional Robust Optimization (DRO) . All methods share the same model architecture and training hyperparameters on each dataset. Our approach features a 15-epoch bargaining phase within the total 50 epochs. See Appendix A.6 for training details. Unlike synthetic experiments, the Pareto Front of real world datasets could be computationally intractable, so we cannot directly evaluate regarding this. Three metrics from SS2 are used: **Overall AUC(\(\))**, **Max-gAUCD** (\(\)), and **Worst-gAUC** (\(\)). corresponding to the goal of LtR, FORML, and Meta-gDRO, respectively.

**Results and analysis.** Our NBS-enhanced two-stage meta-learning improves the overall performance, fairness objectives (in color), and stability, as in Table 1, and with 95% CI in Table 4, Appendix A.7. While the results without bargaining majorly agree with the prior work, bargaining increases FORML's Overall AUC by 10.34% (from 0.706 to 0.779) with a tighter 95% CI (from 0.202 to 0.054), and decreases Max-gAUCD by 26% (from 0.039 to 0.029) with a tighter CI (from 0.046 to 0.018). However, our method faced challenges in two datasets: Credit Default, where performance and fairness occasionally declined, and Communities and Crime, where minimal improvement was observed (in particular, the Meta-gDRO). We diagnose that these two dataset's validation set contains low feature-label mutual information, leading to noisy outcomes (Appendix A.7) and affecting

Figure 3: Synthetic illustration of the bargaining effects. “\(\)”: final point not close to the fairness goal (**x=y**). “\(\)”: final point not at the Pareto front. **(a)** Bargaining across all 1000 steps; **(b)** Bargaining only included in the first 100 steps (two-stage method).

most tested methods (e.g., LtR, FORML). Additionally, for Communities and Crime, our method is influenced by the low bargaining success/feasible rate, possibly due to the lack of favorable (positive) training samples for the minority groups (Table 2, Appendix A.6). Conversely, our method still yields the anticipated bargaining results on the adult income dataset with only one positive Amer-Indian sample. These insights emphasize the importance of validation set quality and representative samples in the training.

**Effects of bargaining on hypergradient conflicts.** Bargaining enhances hypergradient alignment by varying degrees among different one-stage algorithms (Figure 4, 6). For instance, LtR's alignment rate improves from 60% to 80%, and FORML jumps from 0 to 76.9% accompanied with more substantial performance and fairness gains on Bank Telemarketing (Figure 4). FORML consistently benefits more from bargaining compared to the other two, likely due to its optimization goal that could intensify hypergradient conflicts. Moreover, our approach uniformly promotes hypergradient alignment during Stage 1 (Figure 6). We show that early-stage bargaining, accompanied by its hypergradient conflict resolution, is crucial for enhancing model performance and fairness. Further illustrations and analyses are in Appendix A.7.

**Discussions on scalability.** To scale up models and datasets, one natural question is whether the bargaining game can still be feasible. For larger number of groups, although the likelihood of getting unresolvable conflict may get higher due to the fact that more players are getting involved, we

    &  &  &  &  & }}\)**} \\   & & & one-stage & two-stage (ours) & one-stage & two-stage (ours) & one-stage & two-stage (ours) \\  
**I. Adult Income**, (sensitive attribute: **sex**) & & & & & & & & \\ 
**Overall ALC** (\(\)) & 0.778 & 0.761 & 0.830 & 0.830 (+0.000) & 0.801 (+0.000) & 0.810 (+0.000) & 0.837 (+ 0.027) \\
**Max-gLUC** (\(\)) & **0.916** & 0.029 & 0.050 & 0.047 (+0.001) & 0.075 (+0.001) & 0.052 (+0.004) & 0.052 (+0.006) \\
**Worst-gAUC** (\(\)) & 0.770 & 0.747 & 0.805 & 0.807 (+0.002) & 0.763 & 0.795 (+0.032) & 0.809 & **0.814 (+0.006)** \\ 
**II. Adult Income**, (sensitive attribute: **race: **) & * one group in training data contains only one positive (favorable) label. & & & & & \\ 
**Overall ALC** (\(\)) & 0.668 & 0.652 & 0.803 & **0.805 (+0.002)** & 0.710 & 0.775 (+0.065) & 0.715 (+0.018) \\
**Max-gLUC** (\(\)) & 0.225 & 0.236 & **0.090** & **0.090 (+0.000)** & 0.290 & 0.134 (+5.056) & 0.163 (+0.005) \\
**Worst-gAUC** (\(\)) & 0.544 & 0.538 & 0.755 & **0.760 (+0.005)** & 0.540 & 0.688 (+1.43) & **0.694** & 0.703 (+0.009) \\ 
**III. Bank Telemarketing**, (sensitive attribute: **age: **age**) & & & & & & & \\ 
**Overall ALC** (\(\)) & 0.697 & 0.686 & 0.724 & 0.728 (+0.004) & 0.706 & **0.779 (+0.073)** & 0.698 & 0.722 (+0.024) \\
**Max-gLUC** (\(\)) & **0.913** & 0.025 & 0.099 & 0.083 (+0.016) & 0.039 & 0.029 (+0.001) & 0.098 & 0.079 (+0.007) \\
**Worst-gAUC** (\(\)) & 0.691 & 0.691 & 0.675 & 0.686 (+0.011) & 0.686 & **0.764 (+0.078)** & 0.649 & 0.683 (+0.054) \\ 
**IV. Credit Default**, (sensitive attribute: **sex: **w**) & * one group in training data contains only one positive (favorable) label. & & & & & \\ 
**Overall ALC** (\(\)) & 0.634 & 0.624 & 0.630 & 0.616 (+0.014) & 0.554 & 0.611 (+0.057) & **0.631** & 0.661 (+0.021) \\
**Max-gLUC** (\(\)) & 0.024 & 0.022 & 0.037 & 0.025 (+0.12) & 0.017 & 0.033 (+0.016) & **0.016** & 0.022 (+0.005) \\
**Worst-gAUC** (\(\)) & 0.522 & 0.613 & 0.612 & 0.603 (+0.009) & 0.545 & 0.595 (+0.00) & **0.674** & 0.650 (+0.024) \\ 
**V. Communities and Crime**, (sensitive attribute: **backggot: **w**) & * val data is noisy, meanwhile one group in training data are all positive labels.) & & & & & \\ 
**Overall ALC** (\(\)) & 0.525 & 0.568 & 0.679 & **0.700 (+0.021)** & 0.554 & 0.568 (+0.014) & 0.666 (+0.003) \\
**Max-gLUC** (\(\)) & **0.450** & 0.136 & 0.071 & 0.128 (+0.058) & 0.107 & 0.136 (+0.029) & 0.114 (+0.000) \\
**Worst-gAUC** (\(\)) & 0.500 & 0.500 & **0.643** & 0.636 (+0.007) & 0.500 & 0.500 (+0.000) & 0.629 (+0.000) \\ 
**VI. Titanate Survival**, (sensitive attribute: **sex**) & & & & & & & \\ 
**Overall ALC** (\(\)) & 0.972 & **0.983** & 0.967 & **0.978 (+0.011)** & 0.950 & 0.972 (+0.022) & 0.961 & 0.972 (+0.011) \\
**Max-gLUC** (\(\)) & 0.956 & 0.033 & 0.044 & 0.044 (+0.00) & 0.0033 & **0.011** (+0.022) & 0.033 & 0.033 (+0.009) \\
**Worst-gAUC** (\(\)) & 0.944 & **0.967** & 0.944 & 0.956 (+0.12) & 0.933 & **0.967** (+0.034) & 0.944 & 0.956 (+0.012) \\ 
**VII. Student Performance**, (sensitive attribute: **sex**) & & & & & & & \\ 
**Overall ALC** (\(\)) & 0.784 & 0.816 & 0.900 & 0.900 (+0.000) & 0.828 & 0.822 ( +0.006) & 0.909 & **0.912 (+0.013)** \\
**Max-gLUC** (\(\)) & 0.119 & 0.106 & **0.013** & 0.037 (+0.024) & 0.056 & 0.031 (+0.025) & 0.031 & 0.025 (+0.006) \\
**Worst-gAUC** (\(\)) & 0.725 & 0.762 & 0.854 & 0.831 (+0.013) & 0.800 & 0.805 (+0.006) & 0.894 & **0.900 (+0conjecture that the feasibility of \(A\) depends more on the group structure rather than the number of groups. For example, the interdependencies and shared factors between groups may cause dependency in hypergradients to enable the feasibility of \(A\). When goals of groups rely on common resources or have shared objectives at a higher level, it is still likely to have nonempty \(A\).

## 5 Discussion & Conclusion

In conclusion, our study offers several key insights: We identified _hypergradient conflict_ as a pivotal issue undermining stable performance in one-stage fairness-aware meta-learning algorithms. To mitigate this, we proposed a two-stage framework that initially employs the NBS to resolve these conflicts, and then optimizes for fairness. Our assumption-free proof of the NBS extended its applicability to a broader range of gradient aggregation problems. Empirical results demonstrated our method's superior performance across synthetic scenarios, seven real-world fairness settings on six key fairness datasets, and two image classification tasks.

**Future directions.** First, addressing the absence of a specific label in the training subgroup and the low quality of the validation set that affected our method's effectiveness may be mitigated by fairness-aware synthetic data  or data-sifting methods . Pairing our method with them and exploring more resilient solutions adapted to these extreme cases can be a promising direction. Second, Theorem 3.7 establishes the validity to switch the choice of \(\) during training. Future work can focus on designing and flexibly choosing outer optimization goals to delicately improve performance, fairness, or other metrics in interest. Third, we derive the NBS under \(D=0\) for conflict resolution. Future study could investigate general \(D 0\), which might not be useful for conflict resolution (the scope of our paper), but could be used in other cases as a gradient aggregation method that gains advantages from axiomatic properties, as discussed in Appendix A.4.