# Tuning Multi-mode Token-level Prompt Alignment across Modalities

Dongsheng Wang, Miaoge Li, Xinyang Liu, MingSheng Xu, Bo Chen

School of Electronic Engineering, Xidian University, Xi'an, China

{wds,limiaoge,xinyangatk,msxu}@stu.xidian.edu.cn

bchen@mail.xidian.edu.cn

&Hanwang Zhang

School of Computer Science and Engineering, Nanyang Technological University, Singapore

hanwangzhang@ntu.edu.sg

Corresponding author

###### Abstract

Advancements in prompt tuning of vision-language models have underscored their potential in enhancing open-world visual concept comprehension. However, prior works only primarily focus on single-mode (only one prompt for each modality) and holistic level (image or sentence) semantic alignment, which fails to capture the sample diversity, leading to sub-optimal prompt discovery. To address the limitation, we propose a multi-mode token-level tuning framework that leverages the optimal transportation to learn and align a set of prompt tokens across modalities. Specifically, we rely on two essential factors: 1) multi-mode prompts discovery, which guarantees diverse semantic representations, and 2) token-level alignment, which helps explore fine-grained similarity. Consequently, the similarity can be calculated as a hierarchical transportation problem between the modality-specific sets. Extensive experiments on popular image recognition benchmarks show the superior generalization and few-shot abilities of our approach. The qualitative analysis demonstrates that the learned prompt tokens have the ability to capture diverse visual concepts. The code is available at https://github.com/wds2014/ALIGN.

## 1 Introduction

Recently, prompt tuning has experienced significant advancements in adapting large pre-trained vision language models (PVLs) such as CLIP  and BLIP  to downstream tasks [3; 4; 5; 6]. A typical PVL model consists of two branch networks: the text and image encoders. These networks are used to extract the corresponding modality features. PVLs are often contrastively pre-trained on Web-scale image-text pairs, which encourage the alignment of visual concepts with natural language in the shared semantic space. One of the core ideas behind prompt tuning is to formulate the downstream tasks into the original pre-training pipeline. For example, CLIP designs category descriptions with a manual prompt template "_a photo of a \(\{class\}\)_", which works well in generic image recognition. Unlike fine-tuning, where the entire model is tuned using task-specific objectives, demands prohibitive computing cost, and poses a risk of knowledge shift issues [7; 8; 9], prompt tuning fixes the model parameters instead and optimizes prompt vectors, which act as demonstrations to help extract task-related features. This significantly benefits the representations via PVLs, even in performing zero-shot inference without training samples.

However, identifying optimal prompts for PVLs is not a trivial task, which usually needs to solve the intricate semantic alignments between the textual and visual modalities. Inspired by the successof prompt learning in neural language models (NLP) [10; 7; 11], approaches called textual prompt tuning (TPT) are proposed to learn continuous prompt embeddings for CLIP's text encoder, _e.g._, "\(X\)\(X\)\(XX\)\(\{class\}\)", where "X" denotes the learnable vectors [3; 4]. Optimized with a task-specific loss, the learned prompt embeddings distill the pre-trained knowledge encoded in the fixed parameters, achieving better flexibility and efficiency than hand-crafted methods . To improve the generalization of TPT on unseen classes, many studies attempt to give the solutions from gradient flow [12; 13], prototype and composition prompt learning [14; 15; 16]. Moving beyond learning a single-mode prompt, which often fails to capture diverse concepts, various methods prefer to explore multiple prompts based on ensemble learning, optimal transport  and Bayesian inference [18; 19; 20], showing robust alignments and better performance.

In parallel with TPT, visual prompt tuning (VPT) focuses on the patch embedding space of the CLIP's image encoder . VPT views images as a patch sequence and introduces visual prompts to enhance the image representations, _e.g._, "\(X\)\(XX\)\(\{image\}\)", where "image" denotes the image patch sequence. VPT provides a simple and efficient idea to extract task-relevant visual features, which has been widely applied to many visual tasks, for example, video understanding , domain adaptation , transfer learning  and image segmentation [24; 25; 26]. More recently, there has been a research trend to combine TPT and VPT to learn multi-modal prompts together [27; 28]. However, they currently concentrate on single-mode prompt discovery, _i.e._, only one prompt for one modality, which may be insufficient to represent a class . This issue is even more acute in multi-modal prompt learning, where both visual and textual concepts and their alignments need to be inferred. Additionally, it is less sound to represent the image and label only with the global features [29; 30], which could lose local region features of the target object, resulting in sub-optimal classification.

To this end, this work develops a comprehensive prompt tuning framework, where multi-modal multi-mode prompts are learned by building the prompt and token-level optimal transport (OT). Formally, after feeding multiple prompt inputs into the modality-specific encoder, our prompt-level OT views each image as a discrete distribution \(P\) over the visual prompt space and views each label as a discrete distribution \(Q\) over the textual prompt space. With such formulation, the classification task becomes to measure the distance between \(P\) and \(Q\). Moreover, along with the global prompt-level features, the patch (or token) embeddings capture the local region features of the target object (or category description). This motivates the token-level OT, where each prompt output is modeled as a discrete distribution over the token embedding space. The cost matrix is then calculated between the visual patches and textual tokens, enabling the token-level alignments. Crucially, the cost matrix in prompt-level OT that measures the transport cost between prompts from two domains is now converted to integrate the global features and the output of the token-level OT. This hierarchical connection makes it possible to predict the label with the detailed token and patch features, resulting in higher accuracy.

In summary, our method provides a novel prompt tuning framework that incorporates multiple modalities and token-level alignments via the hierarchical OT. The prompt-level OT learns the diverse semantics of a class from both image and language domains, and the token-level OT explores fine-grained alignments between token embeddings. Notably, with different hyperparameter settings, the variants of the proposed model cover many previous works, offering flexibility for easy adaptation across diverse applications. The main contributions of the paper are as follows:

* We propose a multi-mode token-level alignment framework for multi-modal prompts tuning, where multiple prompts are learned to improve the representation for both visual and textual modalities. With special settings, many previous works can be margined into our framework.

Figure 1: The alignment comparison in recent prompt tuning methods. The proposed ALIGN learns multi-modal multi-mode prompts at the same time, resulting in comprehensive alignments.

* We formulate the prompt tuning task as the distribution matching problem, and develop the prompt and token-level OT to tackle the task with a principle and elegant solution.
* We apply our method to few-shot classification, dataset transfer learning and domain generalization. Experiential results on widely used datasets show the superiority of the proposed model.

## 2 Background

### Multi-modal Prompt Tuning

Multi-modal prompt tuning (MPT) [28; 27] is a newly developed task that enables the joint learning of textual and visual prompts for PVLs. Instead of optimizing the unimodal prompts separately, the joint tuning paradigm not only leverages the two branch networks of PVLs, but also allows interactions between two modalities during training, resulting in dynamic alignments. Without loss of generality, we use a vision transformer (ViT) based CLIP for example, which consists of a ViT as an image encoder \(f\) and a transformer as a language encoder \(g\). Given an input image \( R^{H W 3}\) and \(K\) label names \(\{class_{k}\}_{k=1}^{K}\). MPT first incorporates \(b\) learnable tokens as visual prompts \(\{_{i} R^{d_{v}}\}_{i=1}^{b}\), and another set of \(b\) learnable tokens as textual prompts \(\{_{i} R^{d_{l}}\}_{i=1}^{b}\). After concatenating them alongside the image patches and class names, one can obtain the output of CLIP as:

\[[,}_{1},...,}_{O},}_{1},...,}_{b}] =f(<cls>,_{1},...,_{O},_{1},...,_{b}),\] \[[-,}_{1},...,}_{b},}_{k,1},...,}_{k,k_{l}},_{k}] =g(<cls>,_{1},...,_{b},_{k,1},...,_{k,k_{ l}},<eos>),\]

where \(<cls>,<eos>\) are virtual tokens, \([_{1},...,_{O}]\) are \(O\) image patch embeddings, and \([_{k,1},...,_{k,k_{l}}]\) are token embeddings with length \(k_{l}\) of \(k\)-th class. After the stacked self-attention layers of \(f\) and \(g\), CLIP outputs the token embeddings and views \(\) and \(_{k}\) as the prompt-level features of the image and label, respectively. Empirical findings suggest that it is more effective to obtain the vision prompt \(\) by projecting the language prompt \(\) through a vision-to-language mapping function, such as \(=F()\), rather than learning them independently [28; 6]. Finally, MPT estimates the label of \(\) according to the cosine similarity score:

\[p(y=k|)=((,_{k})/)}{_{k^{ }=1}^{K}((,_{k^{}})/)},\] (1)

where \(\) is the fixed temperature parameter. MPT unifies the ideas of TPT and VPT by directly tuning the visual prompts \(\) and textual prompt \(\) at the same time. Eq. 1 indicates that the text encoder \(g\) takes the category prompts as input and outputs \(\), which serves as the corresponding classifier weights. Thanks to the pre-trained knowledge in CLIP, MPT retains the ability to perform open-set classification. Note that both the encoders \(f\) and \(g\) in CLIP are frozen and only the prompt sequences \(\) and \(\) are optimized during downstream training. This process can be seen as a bootstrapping step that helps guide the encoders to extract task-relevant features.

### Optimal Transport Distance

Optimal transport (OT) is an efficient tool to measure the distance between two distributions, which is widely used in recent machine learning studies, such as text analysis [31; 32; 33], computer vision [34; 35; 36; 37; 38; 39] and generative model [40; 41]. Here we review the discrete OT matching and refer readers to  for details. Given two sets of data points \(=\{x_{i}\}_{i=1}^{m}\) and \(=\{y_{j}\}_{j=1}^{n}\), of which discrete distributions are formulated as \(p=_{i=1}^{m}a_{i}_{x_{i}}\) and \(q=_{j=1}^{n}b_{j}_{y_{j}}\), respectively. \(^{m}\) and \(^{n}\), where \(^{m}\) denotes the probability simple of \(R^{m}\). We define the cost matrix between \(\) and \(\) as \(=(C_{ij}) R_{ 0}^{m n}\), where \(C_{ij}=c(x_{i},y_{j})\) is the transport cost from \(x_{i}\) to \(y_{j}\), with \(c\) is the cost function. The goal of OT is to optimally transport \(p\) to \(q\) at the smallest cost:

\[d_{}(p,q;):=(p,q)}{min}<,>,\] (2)

where \(<,>\) denotes the Frobenius dot-product and \(_{>0}^{m n}\) denotes the transport plan to be learned. OT distance is then minimized over all the joint probabilities of \(m n\) space with two marginal constraints \((p,q):=\{:_{n}=,^{T}_{m}=\}\), where \(_{m}\) denotes m-dimensionalall-one vector. As directly learning the optimal plan \(\) in Eq. 2 can be time-consuming for large-scale problems, Sinkhorn distance from [42; 43] introduces the entropic constraint on the transport plan \(h()=_{m,n}-T_{mn}(T_{mn})\) and thus the resulting algorithm estimates \(\) within a few iterations, showing better flexibility and scalability.

## 3 The Proposed Model

### Overall Method

In this section, we introduce the technical details of our proposed model, named ALIGN, a holistic framework for multi-modal prompt tuning with optimal transport (shown in Fig. 2). Benefiting from the carefully designed multi-mode token-level alignment module, most existing works can be merged into our ALIGN with special settings. Intuitively, humans learn one class with various concepts, which provides sufficient semantic features, such as color, layout, and shape, to distinguish it from others . Inspired by this, one of the goals of this work is to learn \(M\) visual prompt and \(N\) textual prompt simultaneously. Specifically, we first introduce our prompt-level OT, where each image and label are modeled as the discrete distributions \(P\) and \(Q\) over \(M\)-dimensional visual space and \(N\)-dimensional textual space. Moreover, instead of representing the prompt outputs as a single point, _e.g._, the global features \(\) and \(\), we distill the token-level knowledge implied in CLIP. Recalling that, the \(n\)-th textual prompt output of the \(k\)-th class contains \(b+k_{l}\) token embeddings and the \(m\)-th visual prompt output of an image contains \(b+O\) patch embeddings, which capture the local-region features of corresponding modalities. This motivated us to develop the token-level OT that makes token-level comparisons for fine-grained alignments. As a result, \(m\)-th and \(n\)-th points in \(P\) and \(Q\) themselves are further modeled as discrete distributions over the shared token embedding space. Due to the compelling two-level OT connections, where the cost matrix in prompt-level OT is obtained by the output of token-level OT, the learned transport plan captures both the prompt and token-level features, which provides a principled and elegant way to estimate the distance between label and image sets.

### Multi-mode Token-level Prompt Alignment

Moving beyond MPT which learns a single-mode prompt to describe the class and estimates the similarity based on prompt-level features, we aim to explore multi-mode representations in the textual and visual domains and make fine-grained alignment to improve the prediction accuracy. Now we have \(M\) groups of visual prompts \(\{^{m}\}_{m=1}^{M}\) and \(N\) groups of textual prompts \(\{^{n}\}_{n=1}^{N}\), where each \(^{m} R^{d_{v} b}\) and \(^{n} R^{d_{l} b}\) are learnable prompt sequences with length \(b\). Mathematically, we employ two empirical distributions \(P\) and \(Q\) to model the sets of two modalities:

\[P=_{m=1}^{M}_{_{m}}, Q=_{n=1}^{N}_{_{n}},\] (3)

where \(_{m}\) and \(_{n}\) denote the \(m\)-th visual output and \(n\)-th textual output in the \(d\)-dimensional latent space. They are further modeled as discrete distributions over token-level embeddings, which will be introduced later. Eq. 3 views each prompt equally and adopts the uniform distribution to model the weights. With those two semantic sets \(P\) and \(Q\), the distance between images and labels is no longer calculated by first representing each image and label as a single point and then using the cosine similarity. ALIGN prefers to mine multi-mode features to describe various class concepts, resulting in better representations. The distance thus can be formulated as an entropy-regularized prompt-level OT problem :

\[d_{}^{}(P,Q;):=d_{}(P,Q;)-  h()\] (4)

where \(>0\) is the weight of regularization, and \( R^{M N}\) is the cost matrix between visual set \(\) and textual set \(\). \( R^{M N}\) is the to-be-learned transport plan with the marginal constraint, _e.g._,\(_{N}=1/M,^{T}_{M}=1/N\). Note that, \(T_{mn}\) measures the transported probability from \(m\)-th visual prompt to \(n\)-th textual prompt, and a large value means the high semantic connection between two prompts across modalities. Therefore, Eq. 4 estimates the expected transport cost between \(P\) and \(Q\), which provides a principle solution to calculate the similarity between the images and labels.

Noticeably, the cost matrix \(\) in Eq. 4 plays a critical role in the learning of \(\), and intuitively, the larger the transport costs between two points are, the lower the transport probabilities will be. A natural choice is to specify \(\) with the global features \(C_{mn}=1-(^{m},^{n})\), where \(^{m}\) and \(^{n}\) denote the prompt-level features of \(m\)-th visual prompt and \(n\)-th textual prompt. However, the above definition primarily emphasizes prompt-level representation and might have a limited capacity to capture the detailed token-level features, _e.g._, different patches within an image may capture various local region features. Thus, the obtained transport plan may fail to reflect the true relations between \(P\) and \(Q\). To this end, we further introduce the token-level OT that considers token-level alignments between two prompts. Specifically, we specify the visual output \(\) and textual output \(\) as two empirical distributions over token embeddings (here we omit the subscript \(m\) and \(n\) for clarity):

\[=_{j=1}^{J}_{_{j}},=_{l=1}^{L }_{_{l}},\]

where \(=[}_{1},...,}_{O},}_ {1},...,}_{b}]\) is the output visual patches with length \(J=b+O\), and \(=[_{1},...,_{b},}_{k,1},..., }_{k,k_{l}}]\) is the output textual tokens with length \(b+k_{l}\). Unlike \(\) and \(\) that agent the prompt-level features, \(\) and \(\) collect the token-level features in the shared embedding space of CLIP. Naturally, the cost matrix \(} R^{J L}\) in the token-level OT is defined as \(}=1-(_{j},_{l})\), which measures the transport cost between the visual patches and textual tokens. As a result, the distance between \(\) and \(\) is the total transport cost of the token-level OT:

\[d^{}_{}(,;})=d^{}_{ }(,;})- h(}),\] (5)

where the transport plan \(} R^{J L}\) denotes how likely is that the \(j\)-th visual patch transports to the \(l\)-th token feature, providing a principle solution to align token-level features. This motivated us to develop a combined cost matrix that considers prompt and token-level features together:

\[C_{mn}=1-(^{m},^{n})+ d^{}_{}( {x}_{m},_{n};}^{mn}),\] (6)

where \(\) is a trade-off parameter that controls the weight of token-level cost. The first two terms are the cosine distance between prompt-level features, and the last term is the OT distance between the token-level sets. In this way, Eq. 6 combines the pre-trained knowledge from two levels: the prompt-level features and the token-level embeddings. This enables the learned transport plan \(\) in prompt-level OT to make fine-grained matching between \(M\) visual and \(N\) textual features, resulting in detailed alignments and better representations.

Once Eq. 4 is computed, we follow previous work  and predict the label of image \(_{j}\) as:

\[p(y=k|_{j})=((1-d^{}_{}(P_{j},Q_{k} ;^{jk}))/)}{_{k^{}=1}^{K}((1-d^{}_{ }(P_{j},Q_{k^{}};^{jk^{}}))/)},\] (7)

where \(^{j,k}\) denote the cost matrix of \(j\)-th image and \(k\)-th label. Note that the weight of the classifier \(Q_{k}\) in our model can be viewed as a discrete uniform distribution over \(N\) textual prompts of label \(k\), which contains multiple class-related semantics, improving the classification results. Thanks to the

Figure 2: (a) The framework of the proposed ALIGN. ALIGN learns multiple prompts for PVLs by aligning modality-specific distributions with hierarchical OT. (b) The t-SNE visualization of image embeddings of ALIGN.

differentiable Sinkhorn algorithm, all parameters of the proposed model can be optimized end-to-end by minimizing the following cross-entropy loss:

\[L=-|}_{}_{k=1}^{K}y_{ ,c}p(y=k|).\] (8)

where \(y_{}\) is the one-hot label vector of image \(\) Due to the OT formulation, our proposed ALIGN aims to learn \(M\) visual prompt sequences and \(N\) textual prompt sequences without introducing any neural networks. We describe our proposed model in the Appendix Algoritm. 1.

## 4 Related Work

Single-modal prompt tuning:There are two storylines of single-modal prompt tuning, TPT and VPT. The former focuses on the language branch of a PLV and is interested in prompt learning in continuous embedding space. As one of the representative works, CoOp  models a prompt's context using a set of learnable vectors and shows great improvement over intensively-tuned manual prompts. To solve the weak generalizability on unseen category, CoCoOp  extends CoOp by explicitly conditioning prompts on image instances, which shifts the concentrations away from a specific set of classes to each input instance, enabling a stronger generalization performance. Instead of single-mode prompt learning, PLOT  learns multiple textual prompts by adopting the OT distance between prompts and image patches, achieving diverse prompt tuning. ProDA  first maturely designs multiple prompts and then models the uncertainty of prompts by employing the Gaussian distribution to model prompt embeddings. Correspondingly, VPTs refer to prepending visual patches to the image input space, which also shows impressive results in adapting PVLs into downstream tasks. For example, Jia et al.  introduces trainable visual prompt vectors into the image patch sequence of each Transformer layer and learns them along with a linear head. Despite the promising performance on various visual tasks, those models are designed to learn single-modal prompts, which fails to make use of the pre-trained multi-modal knowledge.

Multi-modal prompt tuning:Moving beyond single-modal prompt tuning, MPT is a recently introduced task that learns textual prompts and visual prompts at the same time. This jointly tuning strategy not only distills the multi-modal knowledge but enables the dynamic alignments between prompts across modalities, showing better generalization. Zang et al.  propose a unified prompt tuning framework (UPT)  that shares an initial prompt across different modalities and designs a tiny network to generate the modality-specific prompts together. Almost parallel to UPT, Khattak et al.  proposed multi-modal prompt tuning (MaPLe) and adopted a projection matrix to condition vision prompts on their language counterparts explicitly allowing mutual propagation of gradients to promote synergy. In comparison, this work aims to learn multi-modal multi-mode prompts to better meet the requirement of diverse comprehensive representations. Besides, unlike measuring the similarity between images and labels by the global prompt-level features, we model each prompt as an empirical distribution over the token-level embedding space, and the similarity score is calculated by combining the prompt and token-level features under a hierarchical OT framework, which provides a novel and elegant tool to adapt PVLs into downstream tasks.

## 5 Experiments

### Experimental Setup

DatasetsTo make a comprehensive evaluation, we performed extensive experiments on 4 task settings, such as few-shot image recognition, base-to-new generalization, cross-dataset transfer learning, and domain generalization. Those experiments are conducted on 15 widely used image datasets, varying in scale and domains, including ImageNet , Caltech101 , OxfordPets , StanfordCars , Flowers102 , Food101 , FGVCAircraft , EuroSAT , UCF101 , DTD , SUN397 , ImageNetV2 , ImageNet-Sketch , ImageNet-A , and ImageNet-R . The details of each dataset are provided in the Appendix Table. B. 1.

BaselinesWe compare ALIGN with the state-of-the-art methods, including: CLIP , which provides the base results without prompt tuning; the single-modal prompt tuning methods,_e.g._,TPTs: CoOP , CoCoOp  and PLOT , and VPTs: VPT , and multi-modal prompt tuning methods: UPT  and MaPLe . Note that we modified the official code of PLOT and changed the backbone to ViT-B/16 for a fair comparison.

Implementation DetailsFollowing previous MaPLe , we load the pre-trained Vit-B/16 CLIP model as our backbone, where \(d_{l}=512\), \(d_{v}=768\) and \(d=512\). We set the number of textual and visual prompts \(M=N=4\), the length of prompt tokens \(b=2\), the hyperparameter \(=0.1\), and \(=1\). The maximum iteration number in the Sinkhorn algorithm is set as 100. For all tasks, we train our model with a batch-size of 4, a learning rate of 0.0035, and an optimizer as SGD. For each task, we optimize the number of epochs. Following MaPLe we run 2 epochs to train ImageNet as a source model with a learning rate of 0.0026. The reported results are the averaged value over 3 seeds. Please refer to the Appendix Sec. B for more details. For all baselines, we set the length of prompts as 4 and collect their results according to the original papers or previous works. Thus, some experimental results may be missing.

### Evaluation with the Standard Setting

Few-Shot Learning.We first evaluate our model on few-shot classification, where models are trained on 1,2,4,8, and 16 shots and then applied to the test sets. We report the accuracy scores of all models across 11 datasets at Fig. 3. Overall, our proposed ALIGN outperforms others in most cases and achieves consistent improvement over CoOp on all datasets. Among the multi-modal prompt tuning methods, our method shows superior performance compared to UPV and MaPLe in general, except for the EuroSAT datasets. This demonstrates that ALIGN has the ability to distill the across-modalities knowledge and efficiently adapt PVLs into downstream tasks. Although there is a small margin between our model and those two models on some datasets, the competing models usually cannot achieve high performance over all datasets, _e.g._, ALIGN exhibits 9.06/3.56/2.56/2.61/0.78(%) accuracy improvements compared with UPT on DTD datasets and achieves 6.56/8.75/7.63/6.64/4.47(%) improvements compared with MaPLe. In addition, we also find that our ALIGN performs better on 1/2/4 shots settings, showing the efficiency of our fine-grained alignments, which provide the token-level comparison during prediction. This capability contributes to more accurate classification, even with limited training samples.

Base-to-New Generalization.To assess the generalizability of our model, we follow CoCoOp  to equally split the classes into base and new sets, and models are only trained on the base classes while tested on the new sets. Table. 1 reports the results, and we have the following observations: First, our proposed ALIGN surpasses previous baselines by achieving the highest average scores,

Figure 3: The few-shot learning results on 7 datasets (more detailed results of other datasets can be found in the Appendix Table. D. 1.). The red solid line denotes our ALIGN method, and the dotted lines represent various baselines. All results are reported as the mean value over three seeds.

thereby illustrating the superiority of the proposed framework. Second, ALIGN outperforms others in terms of H score across all datasets, except for the DTD dataset which indicates our method offers a more favorable trade-off between the base and new sets. We attribute this success to the token-level multi-mode prompt tuning strategy, where the multi-mode prompts enhance the ability to identify diverse visual concepts, which plays an essential role in unseen category prediction. Furthermore, for datasets that have small intra-class variances, such as Stanford Cars and FGVC Aircraft, ALIGN achieves a noticeable improvement over MaPLe. The token-level alignment in ALIGN might account for this improvement, as it makes it more effective for fine-grained image classification.

Transfer Learning and Domain Generalization.To investigate the generalizability across-datasets or across-domains, we first train our model on ImageNet, utilizing all 1,000 classes, and subsequently apply it to 1) other 10 datasets and 2) other 4 domain shift datasets. We report those results at Table. 2 and 3, respectively. Based on those results, we find that our approach outperforms the baseline methods on 8/10 datasets with the best average accuracy score on dataset transfer learning task and 3/4 datasets on domain shift setting. These overall improvements highlight that ALIGN is less susceptible to the distribution shift between the source and target domains, thus revealing the robust generalizability of our model. Despite the marginal performance gain of ALIGN in contrast to MaPLe and UPT, our method outperforms them in most cases in terms of all four tasks and provides a novel multi-mode token-level alignment alternative for prompt tuning.

    &  &  &  &  \\  & Base & New & H & Base & New & H & Base & New & H & Base & New & H \\  CLIP & 69.34 & 74.22 & 71.69 & 72.34 & 68.14 & 70.21 & 96.84 & 94.00 & 95.39 & 91.17 & 97.26 & 94.11 \\ CoOp & 82.66 & 63.22 & 71.65 & 76.14 & 67.88 & 71.77 & 98.00 & 89.81 & 93.72 & 93.67 & 95.29 & 94.47 \\ CoCoOp & 80.47 & 71.69 & 75.83 & 75.98 & 70.43 & 73.10 & 97.96 & 93.81 & 95.84 & 95.20 & 97.69 & 96.43 \\ PLOT & 77.20 & 60.38 & 67.76 & 75.97 & 69.23 & 72.44 & 96.53 & 82.86 & 89.17 & 93.45 & 97.96 & 86.06 \\ MaPLe & 82.28 & 75.14 & 78.85 & 76.66 & 70.54 & 73.47 & 97.74 & 94.36 & 96.02 & 95.43 & 97.76 & 96.58 \\ ALIGN & **83.38** & **75.51** & **79.25** & **76.89** & **72.15** & **74.45** & **98.37** & **94.70** & **96.50** & **95.67** & **97.93** & **96.79** \\   &  &  &  &  \\  & Base & New & H & Base & New & H & Base & New & H & Base & New & H \\ CLIP & 63.37 & 74.89 & 68.65 & 72.08 & **77.80** & 74.83 & 90.10 & 91.22 & 90.65 & 27.19 & 36.29 & 31.08 \\ CoOp & **78.12** & 60.40 & 68.12 & 97.60 & 59.67 & 74.06 & 88.33 & 82.26 & 85.18 & **40.44** & 22.30 & 28.74 \\ CoCoOp & 70.49 & 73.59 & 72.10 & 94.87 & 71.75 & 81.71 & 90.70 & 91.29 & 90.99 & 33.41 & 23.71 & 27.74 \\ PLOT & 61.41 & 42.69 & 50.37 & 95.26 & 56.03 & 70.56 & 88.45 & 85.28 & 86.84 & 29.63 & 16.17 & 20.92 \\ MaPLe & 72.94 & 74.00 & 73.47 & 95.92 & 72.46 & 82.56 & 90.71 & 92.05 & 91.38 & 37.44 & 35.61 & 36.50 \\ ALIGN & 77.24 & **76.38** & **76.80** & **97.70** & 73.3 & **83.75** & **90.77** & **92.07** & **91.42** & 37.56 & **36.97** & **37.26** \\   &  &  &  &  \\  & Base & New & H & Base & New & H & Base & New & H & Base & New & H \\ CLIP & 69.36 & 75.35 & 72.23 & 53.24 & 59.90 & 56.37 & 56.48 & 64.05 & 60.02 & 70.53 & 77.50 & 73.85 \\ CoOp & 80.60 & 65.89 & 72.50 & 79.44 & 41.18 & 54.24 & 92.19 & 54.74 & 68.69 & **84.69** & 56.05 & 67.45 \\ CoCoOp & 79.74 & 76.86 & 78.27 & 77.01 & 56.00 & 64.85 & 87.49 & 60.04 & 71.21 & 82.33 & 73.45 & 77.64 \\ PLOT & 78.56 & 72.34 & 75.32 & 69.87 & 53.63 & 60.68 & 87.39 & 67.63 & 74.30 & 72.71 & 41.51 & 52.84 \\ MaPLe & 80.82 & 78.70 & 79.75 & 80.36 & **59.18** & **68.16** & **94.07** & 73.23 & 82.35 & 83.00 & 78.66 & 80.77 \\ ALIGN & **82.47** & **79.68** & **81.05** & **82.13** & 54.17 & 65.28 & 94.03 & **74.9** & **83.38** & 84.43 & **78.33** & **81.27** \\   

Table 1: Base-to-New on 11 datasets. The prompts are learned from the 16-shots base set. We report the classification accuracy on base set (Base), new set (New), and their harmonic mean (H), where \(=(2)/(+)\). The best results are **highlighted**.

    &  &  \\   &  &  &  &  &  &  \\ 
**Method** & **Model** & **Model** & **Model** & **Model** & **Model** & **Model** & **Model** & **Model** & **Model** & **Model** & **Model** \\  CoOp & 71.51 & 93.70 & 89.14 & 65.41 & 68.71 & 85.30 & 18.47 & 64.15 & 41.92 & 46.39 & 66.55 & 63.88 \\ CoCoOp & 71.02 & **94.43** & 90.14 & 65.32 & 71.88 & 86.06 & 22.94 & 67.36 & 45.73 & 45.37 & 68.21 & 65.74 \\ MaPLe & 70.72 & 93.53 & 90.49 & 65.57 & 72.23 & 86.20 & 24.74 & 67.01 & 46.49 & **48.06** & 68.69 & 66.30 \\ ALIGN & **72.03** & 93.91 & **90.55** & **65.84** & **73.75** & **86.40** & **24.95** & **67.59** & **46.75** & 47.25 & **69.60** & **67.15** \\   

Table 2: Cross-dataset transfer learning accuracy results. Here we use the key letters to denote the datasets. The best results are **highlighted**.

Qualitative AnalysisBesides the extensive quantitative results, we are also interested in the learned visual concepts of the proposed token-level alignments. Fortunately, the transport plan learned in our token-level OT provides us with access to a convenient tool to visualize the most related visual patches. We report the qualitative analysis at Fig. 2(b) and Fig. 4. From Fig. 2(b), we find that our model prefers to learn separable representations in both base and new classes. Recalling that \(^{n}\) denotes the global feature of \(n\)-th prompts, to visualize the learned prompt and obtain the attention map over the patch space, we calculate the cosine similarity between \(^{n}\) and all patch embeddings \(} R^{d O}\). We then view the normalized cosine similarity as the attention map and visualize the learned \(N=4\) prompts at the top row of Fig. 4. We observe that different prompts tend to align different patch regions, each of which contributes to the final prediction. This finding also meets with the motivation of the multi-mode prompt tuning, where each prompt aims to learn specific visual semantics.

Moving beyond the prompt-level visualization, we also visualize the token-level concepts. Specifically, for \(l\)-th column of the learned transport plan \(}_{l} R^{J}\) in token-level OT, it measures how likely the \(l\)-th token is transported to \(J=b+O\) patches. Here we focus on the \(O\) image patches and visualize the transport plan of an image sampled from the base set and new set at the second and third row in Fig. 4, respectively. We find that 1) different tokens within a prompt can capture various patches with similar visual concepts. For example, both p_1\(t\)2 and p_1\(t\)3 attend to the head of the cat; 2) Learning from the base set, the prompt tokens prefer to align the similar patches in the new set, which reveals that our token-level alignment module has the ability to transfer from the base set to the new set, rather than over-fitting to the base categories.

Complexity AnalysisAs discussed above, one of the key ideas of the proposed ALIGN is to learn multiple prompts for vision and language inputs and explore the token-level alignments under the hierarchical OT framework. To demonstrate the computation cost, we report the complexity analysis at Table. 4, where we focus on the number of trainable parameters (#Paras) and inference speed (fps). We find that 1) Overall, the multimodel prompts tuning methods (last three) require more trainable parameters and inference time than single-modal methods. 2) The proposed ALIGN requires

Figure 4: Visualization of the learned prompts and tokens. p_m denotes the \(m\)-th prompt and p_m_t_l denotes the \(l\)-th token of \(m\)-th prompt.

    &  &  \\ 
**Method** & **Learnable** & **ImageNet** & **ImageNetV2** & **ImageNet-Sketch** & **ImageNet-A** & **ImageNet-R** \\  CLIP & ✗ & 66.73 & 60.83 & 46.15 & 47.77 & 73.96 \\ CoOp & ✓ & 71.51 & 64.20 & 47.99 & 49.71 & 75.21 \\ CoCoOp & ✓ & 71.02 & 64.07 & 48.75 & 50.63 & 76.18 \\ VPT & ✓ & 70.57 & 63.67 & 47.66 & 43.85 & 74.42 \\ UPT & ✓ & **72.63** & 64.35 & 48.66 & 50.66 & 76.24 \\ MaPLe & ✓ & 70.72 & 64.07 & 49.15 & 50.90 & **76.98** \\ ALIGN & ✓ & 72.03 & **64.64** & **49.96** & **50.94** & 76.16 \\   

Table 3: Cross-domain generalization accuracy results. The best results are **highlighted**.

slightly more training parameters than UPT and MAPLE because of the multiple prompts. And it also requires more inference time than MAPLE, due to the hierarchical OT operations. 3) Thanks to the independent OT operations, which can be calculated parallelly with the GPU, ALIGN has a faster testing time than CoCoOp, and achieves 62 fps at the test stage.

## 6 Conclusion

This paper introduces a novel multi-mode token-level alignment framework for multi-modal prompt tuning under optimal transport. We first employ the prompt-level OT to model the multi-mode prompts across modalities, and then introduce the token-level OT by viewing each prompt itself as a set over token embedding space. By coupling those two-level OT via the cost matrix, the final prediction is obtained by combining the prompt-level features and the token-level embeddings, enabling fine-grained alignments. Extensive experiments have been conducted, showing that our proposed model achieves competing performance on four settings. In terms of the **limitations**, the users may still need large GPU memory to load the pre-trained weights of PVLs to apply the proposed model to the test process. One potential solution is to combine prompt tuning with knowledge distillation. We leave it as a future study. Thanks to the open-world visual concept understanding of PVLs, our model shows promising zero-shot/few-shot ability for image recognition, which has the potential to encourage researchers to derive new and better methods for prompt tuning. Our work may indirectly lead to a negative **impacts** if there is a sufficiently malicious or ill-informed choice of a few-shot classification task.