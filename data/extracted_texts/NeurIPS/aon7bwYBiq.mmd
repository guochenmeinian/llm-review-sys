# Differentially Private Graph Diffusion with Applications in Personalized PageRanks

Rongzhe Wei

Georgia Institute of Technology

rongzhe.wei@gatech.edu

&Eli Chien

Georgia Institute of Technology

ichien6@gatech.edu

&Pan Li

Georgia Institute of Technology

panli@gatech.edu

###### Abstract

Graph diffusion, which iteratively propagates real-valued substances among the graph, is used in numerous graph/network-involved applications. However, releasing diffusion vectors may reveal sensitive linking information in the data such as transaction information in financial network data. Protecting the privacy of graph data is challenging due to its interconnected nature. This work proposes a novel graph diffusion framework with edge-level differential privacy guarantees by using _noisy diffusion iterates_. The algorithm injects Laplace noise per diffusion iteration and adopts a degree-based thresholding function to mitigate the high sensitivity induced by low-degree nodes. Our privacy loss analysis is based on Privacy Amplification by Iteration (PABI), which to our best knowledge, is the first effort that analyzes PABI with Laplace noise and provides relevant applications. We also introduce a novel \(\)-Wasserstein distance tracking method, which tightens the analysis of privacy leakage and makes PABI practically applicable. We evaluate this framework by applying it to Personalized Pagerank computation for ranking tasks. Experiments on real-world network data demonstrate the superiority of our method under stringent privacy conditions.

## 1 Introduction

Graph diffusion, characterized by propagating signals across networks, is used in a variety of real-world applications. Variants of graph diffusion such as PageRank  and heat kernel diffusion  has revolutionized the domains such as web searching , community detection , network analysis  and advancements in graph neural networks . Despite their widespread applications, directly releasing diffusion vectors can inadvertently leak sensitive graph information and raise privacy concerns. Hoskins et al.  demonstrate that the access to a small subset of random walk-based similarities (e.g., commute times, personalized PageRank scores) could disclose significant portions of network's edges, a phenomenon known as _link disclosure_. Such attacks, for instance, may enable advertisers to deploy invasive advertising tactics  or reveal sensitive transaction information within financial networks . Consequently, it becomes critically important to design graph diffusion algorithms with privacy safeguards.

Differential privacy (DP) is recognized as a gold standard used for characterizing the privacy risk of data processing algorithms . However, the inherently interconnected nature of graph-structured data renders the adaptation of DP to graphs non-trivial . Previous studies often conduct the analysis of output sensitivity and adopt output perturbation to keep graph data private, which include the study on differentially private personalized PageRanks (PPRs) , and other relevant graphalgorithms such as max flow-min cut , graph sparsification , spectral analysis [23; 24]. However, output perturbation-based approaches often provide a less-than-ideal utility-privacy tradeoff. Numerous studies suggest that incorporating noise during the process, rather than at the output, can potentially enhance utility-privacy tradeoffs .

In this work, we introduce a graph diffusion framework that ensures edge-level DP guarantees based on noisy diffusion iterates. Our framework is the _first_ to incorporate privacy amplification by iterations (PABI) technique  into graph diffusions. As graph diffusion can be viewed as iterating contraction maps in \(_{1}\) space, we adopt per-iterate Laplace noise due to its better performance than the Gaussian mechanism commonly adopted in previous PABI frameworks and provide new analysis dedicated to Laplace noise. We also propose a novel \(\)-Wasserstein distance tracking analysis that can tighten the state-of-the-art PABI bound  that relies on the space diameter, which makes the bound valid for practical usage. Noticing diffusion from low-degree nodes may introduce high sensitivity, our framework also also proposes a theory-informed _degree-based thresholding function_ at each step diffusion to improve the utility-privacy tradeoff. Lastly, we specialize our framework in the computation of PPR for node ranking tasks. Extensive experiments reveal the advantages of our framework over baselines, especially under stringent privacy requirements.

### More Related Works

Extensive research has been dedicated to privacy protection within graphs, specifically in the release of graph statistics and structures under DP guarantees [28; 29]. The primary techniques to safeguard graph structures involve the Laplace and exponential mechanisms . Early contributions by Nissim et al.  calibrated noise based on the smooth sensitivity of graph queries, expanding beyond output perturbation. Karwa et al.  improved the efficiency of privacy-preserving subgraph-counting queries by calibrating Laplace noise according to smooth sensitivity. Different from these methods, Zhang et al.  employed the exponential mechanism  to enhance privacy protections. Concurrently, Hay et al.  developed a constraint-based inference algorithm as a post-processing step to improve the quality of degree sequences derived from output perturbation mechanisms. Further, Kasiviswanathan et al.  used a top-down degree projection technique to limit the maximum degree in graphs, thus controlling the sensitivity of degree sequence queries. Additional efforts in privately releasing graph statistics include outlinks , cluster coefficients , graph eigenvectors , and edge weights .

In the realm of graph diffusion, the most related work to ours is by Epasto et al. , which focuses on releasing the PPR vector using _forward push_ and Laplace output perturbation. Some studies have shown that injecting noise during the process may offer better privacy-utility trade-offs compared to output perturbation methods  and our study compared with  provides another use case. Traditionally, the composition theorem  is used to track privacy guarantees for iterative algorithms, but it results in a bound that may diverge with the number of iterations. Recently, the technique of PABI [26; 27] was introduced to strengthen the privacy analysis of adding noise during the process, which demonstrates a non-divergent privacy bound for releasing final results if the iterations adopt contraction maps . This substantially tightens the divergent bound given by the naive application of the DP composition theorem [43; 44]. Our framework also benefits from this advantage and we further adapt Altschuler et al.'s analysis  to incorporate the Laplace mechanism and provide a tightened space diameter tracking, which makes the bound practically applicable in the graph diffusion applications. Besides, works that share a similar spirit in leveraging PABI have been conducted for other scenarios including machine unlearning  and improving hidden state DP .

## 2 Preliminaries

Let \(=(,)\) represent an undirected graph, where \(\) is the set of nodes and \(\) is the set of edges, equipped with an adjacency matrix \(\{0,1\}^{n n}\), where \(n\) denotes the total number of nodes, i.e., \(n=||\). By establishing an order for the nodes within the graph, we denote \(=[d_{1},d_{2},...,d_{n}]^{T}\) as the degree vector. Additionally, let \(=()\) and \(_{i}\) signifies the \(i\)-th standard basis. We denote \((,)\) and \((,^{2})\) as the zero mean Laplace and Gaussian distributions, respectively. We define the set \([n]=\{1,2,,n\}\), and \(X_{i:j},i,j_{+},i j\) as joint couple of \((X_{i},X_{i+1},,X_{j})\).

**Graph Diffusion.** First, we introduce the concept of **Graph Diffusion**\(\), which is commonly characterized by a series of diffusion map \(_{k}\) defined by the random walk matrix \(=^{-1}\)[6; 12;47]. Formally, We define the graph diffusion \(()\) with the initial seed \(\) as

\[()=_{K}_{K}=_{K}_{K }_{1}(),_{k}()=(_{1,k}+_{2,k}) +_{3,k}.\] (1)

where \(^{||}\) is a stochastic vector on the graph, and \(_{1,k}+_{2,k}+_{3,k}=1\). Let \(_{}=_{k}|_{1,k}|+|_{2,k}|\) denote the Lipschitz constant of the graph diffusion mapping, and \(_{}^{(1)}=_{k}|_{1,k}|\) denote the maximum diffusion coefficient. \(_{K}\) is the diffusion vector at time \(K\). The essence of a diffusion process is to model how an initial vector \(\) propagates through the graph over time. Coefficient \(_{i,k}\)'s control how different resources contribute to the \(k\)th step diffusion. When taking \(_{1,k}=1-_{3,k}=\) and \(_{2,k}=0\), Eq. (1) is recognized as the PageRank Diffusion  with teleport probability \(1-\). The Exponential kernel diffusion, which includes the specific case of the Heat Kernel diffusion , can also be characterized with the composition of diffusion mappings via the infinitely divisible property .

**Personalization.** Personalized graph diffusions, tailored to individual nodes or localized neighborhoods, play a crucial role in many real-world applications. These include recommendation systems , where personalized diffusions improve suggestion relevance, community detection for identifying subgroups within larger networks , targeted marketing strategies for enhancing campaign effectiveness . These diffusions are defined by setting the graph diffusion vector \(\) as \(_{i}\) for an individual node or \(=_{i S}_{i}/|S|\) for a neighborhood set \(S\). In this paper, we primarily discuss the single-node case while our analysis can be generalized to a set of seed nodes.

**Privacy Definition**. Differential Privacy (DP)  is widely recognized as the standard framework for providing formal privacy guarantees for algorithms that process sensitive data. This framework has further been extended under Renyi divergence . Its principles have been applied to safeguard sensitive structures within graph algorithms, an metric noted as Edge-level Renyi Differential Privacy (RDP). Details on the conversion from RDP to DP are elaborated in App. E.

**Definition 1** (Edge-level RDP ).: _A randomized graph algorithm \(\) is \((,)\)-edge-level RDP if for any adjacent graphs \(,^{}\) that differs in a single edge, we have \(_{}(()\|(^{ }))\), where the Renyi Divergence \(_{}(X\|Y)=_{x}( )^{}\) with \(X,Y\)._

More practical cases find that the seed node (user) of personalized graph diffusion algorithms is already aware of their direct connections within the network, such as one's friend list in social networks, and one's transaction record in financial networks, and therefore protecting the edges directly attached to the seed node becomes unnecessary. Instead, the focus of privacy protection shifts towards obscuring the connections between the remaining nodes. To address this specific need, we follow the previous study  and introduce Personalized Edge-level RDP.

**Definition 2** (Personalized Edge-level RDP ).: _Consider a graph \(\) and a personalized graph algorithm \(\). The algorithm \(\) satisfies personalized \((,)\)-edge-level RDP if for any node \(v\) as the seed node in \(\), and for any graph \(^{}\) adjacent to \(\) differing by one edge not incident to \(v\), we have \(_{}((,v)\|(^{ },v))\)._

## 3 Methodology

This study centers on a category of \(_{}<1\) Lipschitz continuous graph diffusions, encompassing prevalent techniques such as PageRank  and PPR . It is noted that each diffusion map \(_{k}\) within graph diffusion maintains the total mass of the diffusion vector \(_{k}\), owing to the condition \(_{i}_{i,k}=1\) and the property of the random walk matrix \(\) being a left stochastic matrix. This observation entails that the diffusion map \(_{k}\) in Eq. (1) constitutes a strictly contraction map in the metric space \((^{||},\|\|_{1})\). Consequently, graph diffusion can be construed as a composite of contraction maps. The PABI technique has been devised to privatize contractive iterations by injecting random noise per iteration. Empirical studies suggest that distributing noise throughout the diffusion steps can provide improved utility-privacy trade-offs compared to output perturbation alone . This insight serves as a key motivation for employing PABI to establish privacy-preserving graph diffusion.

### Preliminaries: Privacy Amplification by Iteration

The technique of Privacy Amplification by Iteration (PABI), originally introduced by Feldman et al.  for convex risk minimization problems via noisy gradient descent, bounds the privacy loss of an iterative algorithm without releasing the full sequence of iterates. This approach applies to processes generated by Contractive Noisy Iteration (CNI) defined as follows.

**Definition 3** (Contractive Noisy Iteration (CNI) ).: _Consider a Banach space \((,\|\|)\) with an initial random state \(X_{0}\), a series of contractions (i.e., \(c\)-Lipschitz functions, \(c 1\)) \(_{k}:\), and a sequence of noise random variables \(\{_{k}\}\). Defining \(\) as a convex bounded set, the Contractive Noisy Iteration \((X_{0},\{_{k}\},\{_{k}\},)\) is governed by the update rule:_

\[X_{k+1}=_{}[_{t}(X_{k})+_{k+1}]\] (2)

_where \(_{}\) is the projection operator onto \(\), respecting the norm \(\|\|\)._

In the PABI analysis by Feldman et al. , gradient descent is conceptualized as a contractive mapping \(_{t}\) in the \(_{2}\) space. Leveraging an additive Gaussian noise mechanism after each iteration, i.e., \(_{k}(,^{2})\), leads to the observation that the Renyi divergence of identical CNIs with differing initial conditions \(X_{0}\) and \(X_{0}^{}\) decays inversely with respect to the total number of iterations \(K\). Specifically, it is observed that \(_{}(X_{K}\|X_{K}^{})_{0}- _{0}^{}\|_{2}}{2K^{2}}\). Altschuler et al.  further extended this framework with improved bound as follows:

**Proposition 1**.: _Let \(X_{K}\) and \(X_{K}^{}\) denote the outputs from \((X_{0},\{_{k}\},\{_{k}\},)\) and \((X_{0},\{_{k}^{}\},\{_{k}^{}\},)\), respectively, where \(_{k},_{k}^{}(0,^{2}I_{d})\). Define distortion \(:=_{k,x}\|_{k}(x)-_{k}^{}(x)\|\) and let \(\) have diameter \(D\). If \(\{_{k}\}\) and \(\{_{k}^{}\}\) are contractions with coefficient \(c<1\), then for any \(\{0,,K-1\}\),_

\[_{}(X_{K}\|X_{K}^{})} }_{}+D^{2}}_{}.\] (3)

The bound in Eq. (3) demonstrates that the Renyi divergence between two CNIs can be quantified by the cumulative Renyi divergence over Gaussian noise with a distortion factor \(\) (the Distortion Absorption term), complemented by a PABI term. The latter indicates that identical contractive transformations applied to bounded processes reduce privacy leakage in an exponential manner. Note that the bound in Eq. (3) is convex with respect to \(\), optimized selection of \(\) leads to non-divergent upper bound \(((D^{2}/)+1)\) where \(=^{2}/2(1/c)\).

**Note on Parameter Set Diameter \(D\).** The privacy bound in Eq. (3) relies on the assumption of bounded diameter \(D\) of the parameter set \(\) to upper bound \(\)-Wasserstein distance (definition in App. A) between the coupled CNI processes \(X_{}\) and \(X_{}^{}\). Although in theory, the upper bound of Eq. (3) only depends on \( D\) by optimizing \(\), we notice that the value \(D\) is important to get a _practically meaningful_ privacy bound. To tighten this term, we will introduce a novel \(\)-Wasserstein distance tracking method that circumvents the need for the diameter parameter in Lemma 3 (detailed later): A high-level idea is to track the \(\)-Wasserstein distance between noisy iterates via constructed couplings instead of using the default set diameter as an upper bound.

**Note on Noise Random Variables \(_{k}\).** The traditional PABI analysis primarily examines gradient descent within the \(_{2}\) space employing the Gaussian mechanism. In contrast, in the context of graph diffusions, modifications to Proposition 1 are necessary to accommodate the \(_{1}\) norm and the application of Laplace noise. This adaptation to the Laplace mechanism, crucial for the graph diffusion applications, has not been previously addressed in the literature to our knowledge.

### Private Graph Diffusions

We now introduce our noisy graph diffusion framework, designed to ensure edge-level RDP and its personalized variant. Our approach consists of injecting Laplace noise into the contractive diffusion process and integrating a graph-dependent thresholding function to mitigate the high sensitivity associated with perturbations of low-degree nodes.

Given a graph diffusion process \(=\{_{k}\}_{k=1}^{}\), we introduce a _noisy graph diffusion_\(_{K,}\) where \(K\) denotes the diffusion steps and \(\) is the standard deviation of the added noise, constructed by a series of composing _noisy graph diffusion mappings_\(_{k,}\):

\[_{K,}=_{K,}_{K-1,} _{1,},_{k,}(_{k-1})=_{k}(f(_{k-1}))+_{k}^{(1)}+ _{k}^{(2)}.\] (4)where \(f\) is a _graph-dependent degree-based function_ set as \(f()=((,-),)\) with a threshold parameter \(\) to balance privacy-utility trade-off. Specifically, \(f\) clips the values of the diffusion vector according to node degrees. Notably, the thresholding function \(f\) allows for negative signals, capturing scenarios where the diffusion coefficient \(_{1,k}\) can be negative. Noise variables \(_{k}^{(1)}\) and \(_{k}^{(2)}\) are independently sampled from a Laplace distribution \((,)\). It is noteworthy that our framework can also be extended to accommodate Gaussian distributions. However, Gaussian noise has been shown to be suboptimal for graph diffusion in \(_{1}\) space, with empirical evidence provided in App. D.4.

**Design of Thresholding Function \(f\).** In the noisy graph diffusion process, the role of the graph-dependent thresholding function \(f\) is twofold. Firstly, \(f\) ensures a bounded distance between the coupled diffusions over two adjacent graphs, analogous to the role of the general projection operator \(_{B}\) in the standard CNI as defined in Eq. (2).

Such a bounding effect is also crucial for the later analysis of \(\)-Wasserstein distance tracking in Lemma 3. Secondly, and more critically, our theoretical analysis reveals that edge perturbation affecting low-degree nodes results in increased distortion at each diffusion step (illustrated in Fig. 1). Uniform thresholding coupled with randomness injection for all nodes typically yields suboptimal performance in such cases. Our degree-dependent design naturally controls the distortion per iteration caused by low-degree nodes which helps with reducing the added noise. More detailed distortion analysis on \(f\) is shown later in Lemma 2. The threshold parameter \(\) is commonly employed to optimize the privacy-utility trade-off in practical applications . The empirical benefits of \(f\) are explored in experiments detailed in Sec. 4.2.

**Discussion on Dual Noise Injection.** Our framework employs a noise-splitting technique, injecting dual Laplace noise at each diffusion step to construct non-divergent privacy bounds, as outlined in Eq. (3). Theoretical justifications for this design is provided in the proof sketch.

Following this, we present our main result on the privacy guarantee of noisy graph diffusion:

**Theorem 1** (Privacy Guarantees of Noisy Graph Diffusions).: _Given a graph \(\), an associate graph diffusion \(=\{_{k}\}_{k=1}^{}\), then noisy graph diffusion mechanism \(_{K,}\) ensures edge-level \((,)\)-RDP with \(\) satisfies:_

\[_{\{0,1,,K-1\}}[(K-) g_{}( ,_{})+g_{}(,}(1-_{}^{})}{1-_{}}_{ }^{K-})]\] (5)

_where \(g_{}(,)=(( )+(-))\) denotes the Renyi divergence induced by the Laplace mechanism , and \(_{}=(4_{}^{(1)},2_{})\) represents the maximum single-step distortion incurred by diffusion on adjacent graphs that involves Lipschitz continuity coefficient \(_{}\), and maximum diffusion coefficient \(_{}^{(1)}\)._

_By selecting \(= K-((}}+} })}})/(}})\), privacy budget \(\) remains bounded by_

\[}}{(}})}[((}}+}})}})+1 ].\] (6)

The privacy bound in Eq. (5) consists of two components: the distortion absorption term (the first term on the RHS) and the PABI term (the second term in RHS). Distortion absorption quantifies the cumulative Renyi divergence over Laplace noise with single-step distortion \(_{}\), while the PABI term quantifies the exponential decay rate, echoing the result in Eq. (3). However, a key difference lies in our approach; instead of leveraging the projected set diameter \(D\) to control the distance between coupled CNIs, our proposed \(\)-Wasserstein tracking method yields a more practical term,

Figure 1: Illustration of Distortion from Edge Perturbations over Adjacent Graphs for Nodes with Low and High Degrees.

\(}(1-_{}^{*})}{1-_{}}\). Further details and utility evaluations of this tool are presented in the proof sketch and Sec. 4.2, respectively.

The function \(g_{}(,)\), which measures Renyi divergence for the Laplace mechanism, increases with distortion \(\) and decreases with noise scale \(\). This behavior implies that reducing distortion and increasing the noise scale enhances privacy. To achieve better calibrated noise within a given privacy budget \(\), we calculate the two terms in Eq. (5) for each \(\). Leveraging the monotonicity of \(g_{}(,)\), we employ a binary search to identify the appropriate noise scale \(\). The optimal noise scale is then determined by selecting the minimum value across various \(\) values, achieving this efficiently with linear complexity relative to \(\).

It is important to note that the maximum single-step distortion \(_{}\) in Eq. (5) is tight and conveys several messages. First, as defined in Eq. (1), when the diffusion process is relatively slow (i.e., \(_{1,k}<_{2,k}\)), the distortion remains tight, governed by the Lipschitz constant \(_{}\) of the diffusion mapping. In contrast, when the diffusion is relatively fast (i.e., \(_{1,k}_{2,k}\)), the distortion bound becomes asymptotically tight, depending on graph structures, with worst-case scenarios detailed in App. B.1.

In Eq. (6), we demonstrate the convergence of the privacy budget with respect to diffusion steps \(K\). This approach differs from the adaptive composition theorem , which analyzes how privacy guarantees degrade when composed mechanisms are applied. Although this method has commonly been employed to protect privacy in graph learning models , it leads to a linear increase in the privacy budget with the number of iterations \(K\) under Renyi divergence , potentially resulting in unbounded losses as \(K\) grows to infinity. More importantly, even for a small number of diffusion steps, our framework achieves a significantly better privacy budget under practical PPR diffusion settings, as illustrated in Fig. 2. Further empirical evaluations are detailed in App. D.4.

### Proof Sketch of Theorem 1

**Proof Idea.** Similar to Eq. (3), the privacy loss of adjacent graph diffusion processes can be bounded as the sum of distortion absorption term incurred by Laplace noise and a PABI term at intermediate step \(\) (Step 1 & 2). Subsequently, we explore degree-based thresholding to manage distortion, achieving a superior utility-privacy tradeoff (Step 3), and introduce \(\)-Wasserstein distance tracking to further tighten the divergence at \(\) (Step 4).

**Step 1: Interpretation of Iterates as Conditional CNI Sequences.** Consider the coupled graph diffusions \(=\{_{k}\}_{k=1}^{}\) and \(^{}=\{^{}_{k}\}_{k=1}^{}\), and the thresholding functions \(f\) and \(f^{}\), operating over adjacent graphs \(\) and \(^{}\), respectively. In each diffusion step, the first noise component constructs noisy iterates, while the second noise component is used to absorb distortion incurred between the adjacent graphs. We encapsulate the discussion as follows:

\[_{k}=(f(_{k-1}))+_{k}^{(1)}+_{k }^{(2)}}_{},\ ^{}_{k}=^{}_{k}(f^{}(^{}_{k-1} ))+^{(1)}_{k}+^{(2)}_{k}}{{=}} (f(^{}_{k-1}))+^{(1)}_{k}}_{}+^{(2)}_{k}.\]

where \(_{k}^{(1)},_{k}^{(2)},_{k}^{(1)},_{k}^{(2)}(,)\), and \(_{k}^{(2)}(^{}_{k}(f^{}( ^{}_{k-1}))-_{k}(f(^{}_{k-1})),)\), and \(}{{=}}\) denotes equality in distribution.

When the distortion \(^{}_{k}(f^{}(^{}_{k-1}))-_{k}(f( ^{}_{k-1}))\) is absorbed by the conditional event of noise variables, i.e., \(_{k}^{(2)}=_{k}^{(2)}\), the coupled diffusion vectors evolve with identical CNIs through the contractive mapping \(_{k} f\). Note that the Laplace distribution is essential for fully exploiting \(_{1}\) distortion in our analysis.

**Step 2: Bounding Privacy Loss through Distortion Absorption and PABI.** The privacy loss of coupled iterates \(_{}(_{K}\|^{}_{K})\) can be bounded by the distortion from graph diffusion, and PABI:

\[_{}(_{K}\|^{}_{K}) _{}(_{+1:K}^{(2)}\|_{+1:K}^{(2)})}_{}+ _{}(_{K}|_{+1:K}^{(2)}=\|^{ }_{K}|_{+1:K}^{(2)}=)}_{}\] (7)This inequality arises from leveraging the post-processing and strong composition rules of Renyi divergence. Here, \(\) represents a joint noise realization, and the parameter \(\) is introduced to balance the privacy leakage from the two terms -- the divergence between the shifted noise variables accumulated from step \(+1\) to step \(K\) (Distortion Absorption), and the divergence across conditional CNIs employing identical transformations \(_{k} f\) (PABI).

**Step 3: Bounding Distortion Absorption**Lemma 2** (Absorption of Distortion in Laplace Distribution).: _For any \(\{0,1,...,K-1\}\), we have_

\[_{}(_{+1:K}^{(2)}\|_{+1:K}^{(2)} )(K-)g_{}(,)\] (8)

_Here, \(\) quantifies the maximum distortion introduced by a single-step diffusion and is determined by the thresholding function \(f\) normalized by node degrees, i.e., \(_{k})|^{}_{i}}{d_{i}}\)._

The observation on \(\) highlights the importance of a degree-based design for the thresholding function \(f\). Uniform thresholding across all nodes results in distortion proportional to \(}}\), introducing unnecessarily large noise induced by low-degree nodes and degrading overall performance. This in principle inspires the choice of \(f\) relying on node degrees. Consequently, \(\) is tightly bounded by \(_{}=(4_{}^{(1)},2_{})\).

**Step 4: Upper Bounding PABI with \(\)-Wasserstein distance tracking.** To perform tight privacy analysis for the second term in Eq. (7), we develop a novel \(\)-Wasserstein distance tracking method for coupled CNIs, where we denote the \(\)-Wasserstein distance at step \(\) by \(w_{}\). This method discards the original boundedness condition in PABI (Eq. (3) Second Term), which relies on the diameter \(D\).

**Lemma 3** (PABI with \(\)-Wasserstein Distance Tracking).: _Given two coupled graph diffusions mentioned above, for any \(\{0,1,...,K-1\}\), any noise realization \(\), we have_

\[_{}(_{K}|_{+1:K}^{(2)}=\|_{ K}^{}|_{+1:K}^{(2)}=) g_{}(, _{}^{K-}w_{})\] (9)

_where the tracked \(\)-Wasserstein distance over coupled CNIs is given by \(w_{}=}(1-_{}^{})}{1- _{}}\) and is naturally upper bounded by \(}}{1-_{}}:=w\)._

We argue that using \(w_{}\) (or the upper bound \(w\)) instead of the default diameter \(D\) is crucial to make the algorithm practically useful. There is no numerical evaluation in the previous study . Numerical comparison between \(w\) and the diameter of thresholding function \(D=_{i=1}^{||}d_{i}\), using the real-world _BlogCatalog_ dataset (detailed in Sec. 4), is illustrated in Fig.3. \(w\) achieves orders-of-magnitude improvement, which is still significant even if \(D\) impacts privacy loss via a logarithmic term. Further empirical validations demonstrating significant utility improvements are detailed in Sec. 4.2.

By substituting the bounds from Eq. (8) and Eq. (9) into Eq. (7), we establish Theorem 1.

### Personalized Graph Diffusion Algorithms with Application in PPR Diffusion

In practice, graph diffusions often originate from a single node \(_{i}\), personalizing the algorithm to this seed node (user). Since the output is provided only to the seed node, protecting its edge connections (one-hop neighbors) becomes unnecessary, ensuring no privacy leakage in the first diffusion step under personalized privacy guarantees. Consequently, the thresholding function is tailored as follows: \(f()=((,-}), })\), where \([}]_{j}=[}]_{j}\) for \(j i\) and \([}]_{i}\) can be set to any positive threshold, i.e., no control is needed for the diffusion over seed node. We employ personalized edge-level RDP (Definition 2), caring two adjacent graphs with a difference only in a single edge not linked directly to the seed node. This approach is encapsulated in the following theorem:

**Theorem 4** (Privacy Guarantees for Personalized Noisy Graph Diffusions).: _Given a graph \(\), an associate graph diffusion \(=\{_{k}\}_{k=1}^{}\), then personalized noisy graph diffusion mechanism \(_{K,}\) with corresponding \(f()=((,-}), })\) ensures personalized edge-level \((,)\)-RDP_with \(\) satisfies:_

\[_{\{0,1,,K-1\}}[(K-) g_{}( ,_{} 1_{ 0})+g_{}(,}(1-_{}^{})}{1-_{}} _{}^{K-})]\] (10)

_where \(\) denote indicator function._

Note that a key difference from Theorem 1 is that in personalized privacy settings, there is no privacy leakage in the first diffusion step (\(K=1\)).

**PPR Application.** Among various graph diffusions, PPR stands out as a prevalent node proximity metric extensively used in graph mining and network analysis. We may apply our noisy diffusion framework to PPR diffusion. We consider PPR with lazy random walk as follows:

\[()=(1-)_{k=0}^{}^{k} =_{K}_{K}_{1}(),_{k}()=+(1-) .\] (11)

where lazy random walk matrix \(=(+)\) and \((1-)\) represents teleport probability with \((0,1]\).

Our framework incorporates noise into the diffusion process of each step of PPR. The privacy guarantees for this noisy PPR are derived from Theorem 4 with \(_{}=2\) and \(_{}=\). Note that, since \(_{j,k}>0\) for all \(j\{1,2,3\}\) in PPR scenarios, all signals propagating among nodes should be non-negative. Consequently, the degree-based thresholding function \(f\) can be further modified as \(f()=((,),})\).

## 4 Experiments

In this section, we present empirical evaluations to support our theoretical findings. Specifically, we apply the widely-used PPR algorithm (Sec. 3.4) to real-world graphs. In practice, we also include a projection step onto the unit \(_{1}\) ball after injecting Laplace noise at each diffusion step. This adjustment has been observed to slightly improve the utility of the resulting PPR without impacting our theoretical analysis (see App. B.1 for details). We focus on the accuracy of noisy PPR in ranking tasks under personalized edge-level DP due to its practicality as noted in .

**Benchmark Datasets.** We conduct experiments on the following datasets: _BlogCatalog_, a social network of bloggers with 10,312 nodes and 333,983 edges; _Flickr_, a photo-sharing social network with 80,513 nodes and 5,899,882 edges; and _TheMarker_, an online social network with 69,400 nodes and 1,600,000 edges.

**Baselines.** Our experimental study includes two baselines. DP-PUSHFLOWCAP is the only private PPR method using Laplace output perturbation, adapting the approximate PPR algorithm with push operations . Edge-Flipping is the other baseline, which uses a randomized response mechanism  on the adjacency matrix, excluding seed node-connected edges in personalized scenarios. Entries are replaced with values in \(\{0,1\}\) uniformly at random with probability \(p\) (detailed in App. E), or retained otherwise. This method requires \((||^{2})\) time to generate a private adjacency matrix and increases its edge density, which limits its practicality. Both our approach and DP-PUSHFLOWCAP offer better scalability. A comparison of running times between different approaches is provided in App. D.2. In all experiments, we only report results if a single trial can be completed within 12 hours on an AMD EPYC 7763 64-Core Processor, and thus Edge-Flipping cannot be run on _Flickr_.

**Metrics.** For utility, we employ two ranking-based metrics: normalized discounted cumulative gain at R (NDCG@R) and Recall@R , where R denotes the cutoff point for the top-ranked items in the list. In our experiments, R is set to 100. For privacy assessments, we utilize the personalized edge-level \((,)\)-DP, with \(\) set to \(}\) following . To ensure a fair comparison, both DP-PUSHFLOWCAP and Edge-Flipping are analyzed using RDP. The privacy budgets for all methods are subsequently converted to DP from RDP results, as elaborated in App. E. All results are reported as averages over 100 independent trials, with 95% confidence intervals.

### Evaluating Privacy-Utility Tradeoffs on Ranking Tasks

In this series of experiments, we aim to assess the ranking performance of our noisy graph diffusion (as delineated in Sec. 3.4) compared with baselines on real-world graphs. We specifically examine privacy budget \(\) ranging from \(10^{-2}\) to \(1\), PPR with parameter \(=0.8\). Considering that both our approach and DP-PUSHFLOWCAP employ a thresholding parameter \(\) to balance the privacy-utility trade-off, we select \(\) from a set of seven values spanning orders of magnitude from \(10^{-10}\) to \(10^{-4}\), a range empirically determined to be optimal across various datasets for both methods (\(=10^{-6}\) is chosen for DP-PUSHFLOWCAP in ). For each experiment, we randomly choose an initial seed for diffusion and execute PPR for \(100\) iterations following . We report the average NDCG@100 score and Recall@100 compared to the standard noise-free PPR diffusion (Eq. (11)) over 100 independent trials in Fig. 4 and Fig. 5 respectively.

**Results for NDCG@100.** As illustrated in Fig. 4, our noisy graph diffusion surpasses both baselines across all three datasets, where values below 0.7 are ignored. In a strong privacy regime (\( 0.5\)), our approach demonstrates significant improvement over DP-PUSHFLOWCAP, which relies on output perturbation. This validates our claim that a noisy process achieves a superior privacy-utility trade-off in stringent privacy settings.

**Results for Recall@100.** Fig. 5 illustrates the overlap of the top-100 predictions of privacy-preserving PPR variants with standard PPR. Across all datasets, our method outperforms two baselines for \(\) values ranging from \(10^{-2}\) to \(1\), further substantiating the advantages of our framework.

Additional experiments on the sensitivity of ranking performance over variations in \(\) are deferred to App. D.3, which demonstrate that our approach is significantly more robust on the choice of the hyperparameter \(\).

### Ablation Study

In this section, we conduct an ablation study to verify the effectiveness of our theory-guided designs, including the degree-based thresholding function \(f\), the \(\)-Wasserstein distance tracking tool. Experiments were conducted on BlogCatalog, utilizing PPR diffusion with parameter \(=0.8\), and a total of \(K=100\) diffusion steps. Additional ablation studies focusing on variations in noise type and comparative analyses of noise scales across different methods are detailed in App. D.4.

**Degree-based Thresholding Function & \(\)-Wasserstein Tracking.** We evaluate the effectiveness of our graph-dependent thresholding function \(f\) and \(\)-Wasserstein distance tracking. We vary \(\) uniformly across seven discrete values in from \(10^{-4}\) to \(10^{-10}\) and report the optimal performance for all methods in Fig. 6. Firstly, we compare the degree-based thresholding function \(f()=((,0),)\) (Red \(\)) against its graph-independent variant \(()=((,0),)\) (Red \(\)), which uniformly clips over nodes regardless of their degree. As \(\) varies from \(0.1\) to \(3\), the degree-based thresholding function consistently outperforms its graph-independent counterpart, with a margin of \(0.15\) to \(0.2\) in the NDCG score.

Figure 4: Trade-off between NDCG and Personalized Edge-level Privacy.

Figure 5: Trade-off between Recall and Personalized Edge-level Privacy.

This verifies the effectiveness of graph-dependent thresholding function \(f\), which can balance the privacy-utility trade-off more effectively by focusing less on sensitive low-degree nodes. Note that the graph-dependent (in)dependent thresholding function \(f\) (\(\)) naturally induces a diameter \(||\) (\(2||\)), respectively, which can be utilized in the modification of Theorem 1 for privacy accounting. However, the graph dependent thresholding function \(f\) results in a significantly larger diameter, which substantially degrades utility. This underscores the importance of leveraging \(\)-Wasserstein tracking.

We showcase the benefits of our \(\)-Wasserstein distance tracking analysis (Red Lines) on the privacy-utility tradeoff against diameter induced bounds derived from \(_{1}\) projection (Green Lines) or thresholding function-induced diameter (Blue Lines). The privacy bound of the \(_{1}\) projection is established based on a Laplace modification of Eq. (3) with a diameter of 1. As shown in Fig. 6, methods employing \(\)-Wasserstein distance tracking analysis consistently outperform those based on \(_{1}\) projection and thresholding-induced diameter. We conclude that our \(\)-Wasserstein tracking analysis and the design of graph-dependent thresholding function \(f\) markedly enhance the privacy-utility tradeoff.

## 5 Conclusion

In summary, we introduce a noisy graph diffusion framework for edge-level privacy protection, utilizing a novel application of PABI in \(_{1}\) space. By incorporating a theory-guided design for a graph-dependent thresholding function and employing a new \(\)-Wasserstein distance tracking tool, we outperform SOTA methods in ranking performance on benchmark graph datasets.

**Societal Impact and Limitations.** Our work advances the development of DP graph algorithms, offering strong privacy protection when properly implemented. For its limitations, we refer readers to standard textbooks on the subject . The effectiveness of our noisy graph diffusion framework has been demonstrated primarily in the context of PPR diffusion. Extending these results to other types of graph diffusions, such as heat kernel diffusion, is a promising direction for future research. This paper focuses on edge-level privacy protections; exploring extensions to node-level DP protections could further broaden the scope of our research.

## Acknowledge

This work is supported by NSF awards CCF-2402816, IIS-2239565, and JPMC faculty award 2023. The authors would like to thank Haoyu Wang, Haoteng Yin for the valuable discussion. The authors also would like to thank Alessandro Epasto for the discussion about the implementation of the output-perturbation-based differentially private Personalized Pagerank.