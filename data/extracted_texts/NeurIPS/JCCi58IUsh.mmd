# Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents

Wenlong Huang\({}^{1}\), Fei Xia\({}^{2}\), Dhruv Shah\({}^{3}\), Danny Driess\({}^{2}\), Andy Zeng\({}^{2}\),

**Yao Lu\({}^{2}\), Pete Florence\({}^{2}\), Igor Mordatch\({}^{2}\), Sergey Levine\({}^{2,3}\), Karol Hausman\({}^{2}\), Brian Ichter\({}^{2}\)**

\({}^{1}\)Stanford University, \({}^{2}\)Google Deepmind, \({}^{3}\)UC Berkeley

grounded-decoding.github.io

Work done as an intern at Google.

###### Abstract

Recent progress in large language models (LLMs) has demonstrated the ability to learn and leverage Internet-scale knowledge through pre-training with autoregressive models. Unfortunately, applying such models to settings with embodied agents, such as robots, is challenging due to their lack of experience with the physical world, inability to parse non-language observations, and ignorance of rewards or safety constraints that robots may require. On the other hand, language-conditioned robotic policies that learn from interaction data can provide the necessary grounding that allows the agent to be correctly situated in the real world, but such policies are limited by the lack of high-level semantic understanding due to the limited breadth of the interaction data available for training them. Thus, if we want to make use of the semantic knowledge in a language model while still situating it in an embodied setting, we must construct an action sequence that is _both_ likely according to the language model and also realizable according to grounded models of the environment. We frame this as a problem similar to probabilistic filtering: decode a sequence that both has high probability under the language model and high probability under a set of grounded model objectives. We demonstrate how such grounded models can be obtained across three simulation and real-world domains, and that the proposed decoding strategy is able to solve complex, long-horizon embodiment tasks in a robotic setting by leveraging the knowledge of both models.

## 1 Introduction

Recent works have demonstrated robots that are increasingly proficient at understanding and acting upon natural language, whether through planning or conditioned policies. Complementing such progress, the field of natural language processing has recently seen large language models (LLMs) become ubiquitously used as pre-trained or few-shot prompted models, due to their impressive few-shot performance and vast knowledge-base. These LLMs have efficiently learned from web-scale data through autoregressively modeling the probability distribution over text tokens and thus generate text. However, the nature of this process is such that applying such models to embodied settings remains a challenge. They have not interacted with their environment, lack observability of non-language observation modalities (e.g., images), and may not know what is safe or possible for a particular embodiment.

Determining how to execute long-horizon behaviors based on high-level verbal commands is one particular area of robotics where the rich semantic knowledge in large language models can be especially useful. This problem combines elements of semantic reasoning and planning: the robotmust understand the instruction, determine the steps needed to fulfill it, and also determine how to sequence those steps appropriately given its capabilities and the current state of the environment. However, this is not a problem that can be solved purely with semantics, as it requires sufficient grounding to understand how the task should be performed _in context_ - for example, in the example in Figure 1, the language model alone has no way of knowing which block to pick up because this requires knowledge of which blocks are present, and also what manipulations the robot is capable of performing on them. Thus, although a language model can assign probabilities for how likely various steps are to correspond to the desired task _semantically_, the constraints of the planning problem must also enter into the process. These constraints could themselves be represented as probabilities that mirror the token probabilities generated by a language model, reflecting their applicability to the current environment rather than their semantic likelihood. We can frame this as a problem similar to probabilistic filtering: decode a sequence (i.e., a task description) that both has a high probability under the language model and a high probability under a grounded model that predicts how applicable this sequence is to the current scene.

Herein, we present **Grounded Decoding (GD)**, a scalable, general approach to planning with LLMs embodied domains. Grounded Decoding jointly decodes the token probability of an LLM and token probabilities from token-conditioned, robotic functions, such as affordance functions capturing the abilities of a robot given its embodiment, safety functions, or more. By guiding the LLM directly at its output, Grounded Decoding enables a general and flexible family of planning algorithms that combines LLM's strength of _long-horizon_ and _semantic_ reasoning and grounded models' strength of _local_ and _embodiment_ grounding.

Our contributions are as followed: 1) we present a robot-centric formulation for decoding language models to perform long-horizon robotic tasks with token-conditioned grounded models, 2) we demonstrate techniques for learning such grounded models, serving different purposes such as affordances and safety requirements, and 3) we show empirical evidence, across three simulation and real-world domains, that the proposed method performs strongly on a wide range of tasks while also significantly outperforming prior methods in efficiency.

## 2 Related Work

**Guided Decoding for Language Models.** Decoding strategies for large language models is an active area of research within natural language processing [77; 87; 25; 85; 38]. A number of recent works have focused on developing decoding heuristics for natural text generation [49; 35; 48; 18; 25; 6; 36]. Another line of works use external classifiers for maximizing certain language-space utilities when decoding language models [71; 92; 26; 39; 37; 21; 38; 4; 12; 23]. Most closely related to our work are classifier-guided decoding methods developed for offline domains, such as image captioning [78; 74] and task-oriented dialog [72; 83]. However, extensions to embodied domains, which we investigate exclusively in this work, remain non-trivial because grounding in embodied domains is bounded by the abilities of the agent and by environment state transition as the agent actively interacts with the environment.

Figure 1: Grounded Decoding solves robotic tasks by taking an instruction as input and selecting tokens that have high probability under a Large Language Model (LLM) and a set of Grounded Models (GM). Thus, it leverages the open-vocabulary and semantic knowledge of LLMs while being grounded in the environment and in the robotâ€™s capabilities. Furthermore, the whole process does not require expensive fine-tuning of the LLM.

**Embodied and Multimodal Language Models.** Training language models to understand embodiment is an active area of research. Training multimodal models can enable some degree of embodied reasoning, such as understanding images and videos [9; 40; 76; 3]. Directly fine-tuning language models to output actions has also been investigated [75; 55; 66]. Lastly, training downstream models on language model embeddings shows promise [46; 52; 24; 94; 60; 41]. In this work, we investigate leveraging large frozen language models for embodied applications [29; 2; 96; 8; 64; 30; 42; 70; 47; 27; 58; 73; 44; 43; 14; 16; 82; 93; 89; 45; 56], with grounded models to provide domain-specific grounding during decoding process.

**Comparison to SayCan.** The most closely related work to our work is SayCan . SayCan uses a large language model and a value function to select robotic skills among a constrained set of primitives. This constrained set of primitives enables SayCan to use the so-called "scoring-mode" of the LLM to get the probability of a skill being useful to a high-level instruction. This requirement to consider only a fixed and enumerated set of primitives limits the applicability of SayCan in scenarios with many possible skills, such as open vocabulary or combinatorial tasks. Grounded Decoding on the other hand jointly decodes the LLM and the grounded model at the token level, allowing for expressive decoding with an open vocabulary. Furthermore, SayCan considers only grounding functions derived from RL-trained value functions for affordance grounding functions, while Grounded Decoding explores many types of grounding functions to propose a broad family of algorithms.

**Task and Motion Planning.** Task and motion planning  seeks to solve high-level instructions via sequencing tasks in dynamically feasible manner. Research within this area generally focuses on symbolic planning  or optimization-based  approaches. Machine learning has increasingly been used to accelerate planning and enable new domains [91; 61; 69; 20; 17; 28; 90; 31; 1; 65; 51; 86; 88; 15]. However, planning constraints are often explicitly specified for TAMP methods. In contrast, we specify constraints as (learned) probabilities, which are baked into the decoding process and provided by domain-specific grounded models.

## 3 Grounded Decoding

### LLMs and Grounding Models

**Large Language Models.** LLMs are trained to predict the probability \(p(W)\) of a text sequence \(W\), represented as a sequence of tokens \(W=w_{1:N}=(w_{1},,w_{N})\). The tokens are elements of a fixed vocabulary \(\). Typical neural architectures factorize the joint probability into \(p(W)=_{n=1}^{N}p_{}(w_{n}|w_{1:n-1})\), where \(p_{}\) is predicted by a transformer network . Given \(p_{}\), generating a text consisting of \(N\)-many tokens, the so-called decoding process, can be seen as the optimization problem \(_{w_{1:N}}_{n=1}^{N}p_{}(w_{n}|w_{1:n-1})\), which in practice is solved, e.g., using greedy search, beam search, or sampling strategies. To further ensure the LLM is solving a desired task, one typically starts with a given text, the so-called _prefix_ or _prompt_, that describes the task, and then the LLM completes this task in its decoding process.

Figure 2: Overview of **Grounded Decoding**. Given a _free-form_ language instruction, a language model and grounded models jointly decide the next candidate token to be decoded by combining their respective likelihood. Language model proposes likely tokens that produce goal-directed and coherent long-horizon behaviors, while grounded models connect them to the physical scene, through a flexible composition of multiple objective functions from multiple modalities, such as affordance, preferences, and safety.

**Grounding Functions.** We use the concept of grounding functions, \(p_{}(w_{1:n}|s)\), which seek to model a probability of tokens \(w_{1:n}\) given (potentially non-textual) state \(s\). This state is intended to capture the embodiment of the robot and the environment, which may be an image, proprioception of the robot, or the environment state. Thus the grounding function models probabilities relevant to the robot embodiment and environment, such as whether the tokens are possible for the robot to execute given the state (affordances), or other values like safety, cost, or user preferences.

### Problem formulation.

Given an instruction in language \(\), we look at the problem of using an LLM to decode a language plan \(w_{1:N}\), which is typically done by finding the most likely tokens under the probability distribution predicted by the LLM, \(p_{}(w_{1:N}|)\), with \(\) being the prefix. However, based on the instruction \(\) as the prefix alone, the LLM can easily generate text that is not grounded in the physical state of the environment, rendering such plans useless in the real world. In order to _ground_ the language model in an actual physical embodiment, we propose _Grounded Decoding_ (GD): The main idea of GD is to guide the generation of token sequences with _grounding function(s)_ that are conditioned on the embodiment of the system.

Formally, let \(s\) denote a representation of the state of the world. Then, GD attempts to generate text that is consistent with both the instruction \(\)_and_ the physical state \(s\):

\[w_{1:N}^{*}=_{w_{1:N},w_{n}}p_{}(w_{1:N}|s,)\] (1)

To leverage the Internet-scale knowledge of LLMs, we factorize \(p_{}(w_{1:N}|s,)\) as follows 2:

\[p_{}(w_{1:N}|s,) =)\;p(w_{1:N})}{p(s,)}\] (2) \[=)p(|w_{1:N})p(w_{1:N})}{p(s,)}\] (3) \[=|)p()p(w_{1:N}|s)p(s)p(w_{1:N})}{p(s, )p(w_{1:N})p(w_{1:N})}\] (4) \[|)}{p(w_{1:N})}p(w_{1:N}|s)\] (5) \[ p(w_{1:N}|)p(w_{1:N}|s).\] (6)

To decode autoregressively with the formulation, we factorize above into token decoding:

\[p_{}(w_{1:N}|s,)_{n=1}^{N}p_{}(w_{n}|w_{1: n-1},)\;p_{}(w_{1:n}|s).\] (7)

The first term, \(p_{}(w_{n}|w_{1:n-1},)\), can be modeled as the probability of the LLM predicting the token for the given instruction \(\) appended previously decoded tokens \(w_{1:n-1}\) without the state \(s\) as input. The second term, \(p_{}(w_{1:n}|s)\), is the grounding function that is only conditioned on the state \(s\) and judges whether the generated text \(w_{1:n}\) is consistent with the physical state. The core idea behind this factorization is that LLMs exhibit long-term planning capabilities, while the grounding function guides the planning of the LLM to be possible in the concrete embodied physical world without needing to be informed or capable of reasoning over the long-horizon instruction.

### Grounded Decoding

This work investigates grounded decoding exclusively in the context of task planning for embodied agents. Figure 2 visualizes a single step of the simplest greedy search form of GD, and accompanying pseudo-code can be found in Algorithm 1. Given a high-level language instruction and history of executed actions, GD proceeds through a process similar to probabilistic filtering by selecting tokens iteratively that have high probability under the language model and the grounded model. Aftereach token is selected, it is appended to the prefix. The process continues until a token in the terminal set \(_{}\) is selected, which could be a period sign "." indicating the end of a single-step skill (e.g., pick-and-place). Then the command \(w_{1:i}\) is sent to a language-conditioned policy \((a|s,w_{1:i})\) that executes the action \(a\) conditioned on the environment state \(s\). Crucially, this grounding function must accept partial commands to enable grounding during decoding.3 Additionally, we note that GD, in its essence, provides a grounded scoring function; thus, it can be easily extended to any search methods such as beam search, top-k sampling, etc.

```
1:Given: state \(s\), instruction \(\), terminal set \(_{}\)
2:Initialize:\(w=\{\}\), \(n=0\)
3:while\(w_{n}\ _{}\)do
4:\(n=n+1\)
5:\(w_{n}=*{arg\,max}_{w_{n}}p_{}(w_{n}|w_{1: n-1},)\ p_{}(w_{1:n}|s)\)
6:endwhile
7:Return:\(w\) ```

**Algorithm 1** Grounded Decoding (GD) w/ Greedy Search

### Techniques for Obtaining Grounded Models

Unlike language tasks, where a single model is capable of performing general semantic reasoning, a singular grounded model remains an open problem. Indeed, each domain may impose varied environmental and embodiment constraints. Despite these challenges, we present several techniques for obtaining grounded models that can be leveraged in GD's formulation, and validate them in three domains in Section 4.

**Token-Conditioned Value Functions.** Assuming a robot acts with action \(a\) according to policy \((a|s,w_{1:n})\), that aims to maximize certain a utility and that the utility captures a task objective, a natural choice that can provide "grounding score" is the action-value function \(Q(s,a|w_{1:n})\) as it necessarily captures the embodiment of the robot. Additional objectives, such as task constraints, can also be encapsulated in \(Q(s,a|w_{1:n})\) to ensure grounding. Note that unlike the formulation proposed in , \(w_{1:n}\) cannot be restricted to a fixed repertoire of token sequences. In practice, to obtain a \(Q(s,a|w_{1:n})\) that satisfies the requirements, one can train multi-task language-conditioned agents, either through reinforcement learning (Section 4.2) or supervised learning (Section 4.1).

**Multimodal Foundation Models.** A general choice to ground LLMs is through using multimodal foundation models, such as CLIP  or open-vocabulary object detectors [22; 34; 50]. Although these models can connect language to other grounded modalities (e.g., vision), they often lack the capability for complex or long-horizon reasoning, and they do not consider embodiment constraints. As a result, to leverage them in the decoding process, they need to constrained to where they are the most applicable rather than always decoding jointly. To this end, we use a prompt-based technique that allows LLMs to choose when to jointly decode (Section 4.3), which we find to be effective in most cases.4.

**Rule-based Methods.** Another source of grounding may come from features \(x=(w_{1:n})\) designed with expert domain knowledge, which can then be used to map \(w_{1:n}\) to a "grounding score" using parametric or non-parametric functions \(f(x)\). Such techniques may be most applicable when interpretability and enforcing hard constraints are required, such as safety-critical settings, or when data are scarce, such as cases involving preferences of individual users (as shown in Section 4.1).

### Comparisons to Prompt-based Methods

One alternative approach for grounding is including scene information as part of the prompt (e.g., object detection results ), which complements the grounding method proposed in this work.

However, we note that prompting is often insufficient for grounding, as information about the scene and about the capabilities of the robot may not always be succinctly described in the prompt. Such examples include 1) in a block stacking task, a block that has been stacked on cannot be picked, 2) in a navigation task, to open a door, one must have a key and that door must be reachable, and 3) in a mobile manipulation domain, an object may be visible but is out of reach of the manipulator. Therefore, Grounded Decoding is a more general and flexible grounding method that injects _continuous probabilities_ during decoding, which may even come from grounding functions from _other modalities_ (e.g., vision).

## 4 Experiments

### Long-Horizon Tabletop Manipulation

Herein we experiment with a simulated tabletop manipulation environment based on RAVENS . We create a custom set of 20 tasks, with 10 seen tasks and 10 unseen tasks. Seen tasks are used for training (for supervised baseline) or for few-shot prompting. They are grouped by following categories. Detailed breakdown can be found in Appendix A.2.

i. **Letters**: Rearranging alphabetical letters ("sort the letters in alphabetical order"). ii. **Blocks & Bowls**: Rearranging or combining blocks and bowls ("put blocks in matching bowls"). iii. **Box Packing**: Sorting food items and utensils into boxes in accordance with safety constraints and user preferences ("Can you pack the picnic box for me?").

Given only high-level language instructions and top-down visual observation of the environment, Grounded Decoding decodes a sequence of text tokens representing the step command to be executed. Note that because GD generated grounded _free-form_ actions, it does not require each step to strictly map to a repertoire of skill as in [29; 2]. After a complete command is generated, it is executed via a pre-trained multi-task language-conditioned CLIPort . An example rollout is shown in Figure 4.To demonstrate the techniques proposed in Section 3.4 to obtain grounding functions, we study the composition of following grounding functions (overall grounding score is calculated as \(p_{}=_{i=1}^{n}p_{i}\)) depending on the task categories. Refer to the Appendix A.2 for details.

Figure 3: Example rollouts and likelihood of representative tokens under Grounded Decoding objective in three distinct domains: simulated tabletop rearrangement **(top)**, Minigrid 2D Maze (**middle**), and real-world kitchen mobile manipulation (**bottom**). Each domain uses different prompts, grounded models, and low-level primitives. The GD formulation is shared across the domains, decoding a pre-trained langauge model with respect to domain-specific grounded models to decompose a _open-ended_ instruction into actionable steps.

[MISSING_PAGE_FAIL:7]

Affordance Grounding Function.Following the recipe from Section 3.3, we train token-conditioned affordance function to be used in GD. The difference is that the grounding function here is the value function from the goal-conditioned policy that is trained with PPO  instead of from demonstrations as in CLIPort . The policy performs short-horizon skills such as "Go to red key" or "Open the door" and are conditioned on CLIP embeddings of the skill and an image of the scene. Accordingly, the goal-conditioned value function evaluates the feasibility given the current observation and the (partially) decoded skill.

**Baselines.** We compare the two variants of GD - with greedy and beam search - with 1) a solitary PPO policy , 2) a hierarchical RL algorithm which plans over the low-level skills, and 3) a hierarchical method that uses an ungrounded language model for planning .

**Analysis.** Table 2 reports the success rate, averaged across 100 episodes of randomly initialized environments. The "flat" RL agent performs poorly in all but the simplest environments, owing to difficulties with understanding the high-level instruction and reasoning over long horizons (often over 100 steps). Planning over low-level skills using hierarchical RL  improves this performance, since the high-level decision-making problem is greatly simplified. However, the high-level RL agent still needs to reason over low-level (textual) skills by understanding their underlying capabilities and stitching them together. Using the planning capabilities of large language models to reason over textual skills significantly boosts this performance , since the language model can inherit the strong reasoning capabilities from its training data. This tends to be insufficient in challenging environments, however, since the number of _potentially viable_ skills may be very large and the LLM has no information about the robot's observations. GD can leverage the learned affordance function (in this case, the goal-conditioned value function) to inform the language model's plans, enabling successful long-horizon reasoning. We further find that beam search improves performance modestly, particularly in long-horizon tasks.

### Mobile Manipulation in a Physical Kitchen

Our last environment is a kitchen robot in the real world, and we follow the same implementations of the mobile manipulation platform and skills in SayCan . We perform instruction following tasks, as in . An example task is "Bring an apple", for which the robot needs to plan and execute a sequence of "1. Find an apple, 2. Pick up the apple, 3. Bring it to you. 4. Put it down, 5. Done". We split the tasks into two categories. _Unambiguous_ means the instruction explicitly contains the object of interest, and _Ambiguous_ means the instruction does not contain the object name. For example, when human asks "bring me the fruit", the robot needs to first determine available fruits. We assume all necessary objects are in the field of view. More details can be found in Appendix A.4.

**Grounded Decoding with Chain-of-thought.** We demonstrate using multimodal foundation models for Grounded Decoding, as proposed in Section 3.4. In particular, we use open-vocabulary object detector owl-vit . Note that because these off-the-shelf models are not trained on robot domain data, we find that it works best by constraining their influence on decoding. We achieve this by making a slight modification to the SayCan algorithm : before generating action plans, we prompt the LLM to generate _visually-grounded_ chain-of-thought  by giving LLM the option of when to enable grounded decoding and disable grounded decoding, as visualized in Fig. 5. Specifically,

Figure 4: Greedy decoding rollout with GD, where key decoded tokens are shown (_yellow_, _purple_, _red_, _yellow_). Combined scores are normalized to the maximum for visual clarity; others are normalized to their sum.

LMs can be prompted to generate a left bracket to start decoding jointly with grounded models and a right bracket to revert to ungrounded decoding. After chain-of-thought, we use SayCan scoring mode for decoding the action plans.

**Analysis.** Table 3 shows that GD recovers similar performance on _Unambiguous_ tasks, and gain 25% in planning performance on _Ambiguous_ tasks. This shows that GD with multimodal foundation models can effectively use _visually-grounded_ chain-of-thought to disambiguate abstract tasks.

## 5 Analysis

### Comparison to SayCan

In this section, we directly compare GD to SayCan , which is related to our method in that both combine language model knowledge and grounded model knowledge (discussed in more detail in Section 2). However, SayCan uses the language model to score all pre-specified options, rendering it inefficient at dealing with large or combinatorial action spaces. In contrast, GD computation considers all possible language token in the autoregressive decoding process, which is _independent_ of the size of the action space. Results shown in Table 4 demonstrate that GD is two orders of magnitude more efficient on our tasks, with comparable performance. Furthermore, by decoding at the most basic functioning unit of language, GD's formulation allows open-vocabulary grounding beyond just affordances, e.g. safety, preferences, and multimodal embeddings such as CLIP.

### Breakdown of Failure Reasons

Because all hierarchical approaches share an imperfect low-level policy for step execution, the results reported in Table 1 are compounded with both planning failures and low-level policy failure. In Figure 6, we provide failure breakdown analysis for Grounded Decoding and associated baselines. Note that the CLIPort baselines are solitary methods that do not use a planner, so the failures are solely composed of policy failures. As shown in Figure 6, while all planning-based methods use the same underlying low-level policy, Grounded Decoding significantly reduces planning failure by being able to incorporate grounded scene information into the decoding process. Moreover, we observe that despite the shared affordance function across beam and greedy search, the beam search variant performs stronger by being aware of the full-length single-step instructions during decoding.

### Grounded Action Manifold

A central goal of this work is to investigate the integration of grounded information into language model decoding to output instructions actionable by a policy. To investigate this, we use a t-SNE  plot to illustrate the extent to which grounded models help narrow down the search space for language models. Specifically, we first enumerate all meaningful instructions in the tabletop domain, such as "pick x and place it on y," which are represented as dots in the figure. We then compute the affordance values with respect to four different scenes, where each color represents one scene. Finally, we group the dots using t-SNE and BERT embeddings . Figure 7 shows that grounded

    & **GD (Greedy)** & **GD (Beam)** & **SayCan** \\ 
**Success Rate** & 50\% & 60\% & **64\%** \\
**Token Count** & **1x** & 4.3x & 113.7x \\   

Table 4: By avoiding full enumeration of skills, GD is more efficient than SayCan while staying performant.

Figure 5: Example prompt and rollout in real-world kitchen environment.

    &  &  \\ 
**Tasks** & **Planning** & **Execution** & **Planning** & **Execution** \\ 
**Unambiguous** & 85\% & 57\% & 85\% & 57\% \\
**Ambiguous** & 58\% & 44\% & 33\% & 25\% \\   

Table 3: Success rates in kitchen environment.

models can effectively identify achievable skills to produce an actionable manifold within the language space and that this grounding is required, as language alone does not perfectly group actionable skills. It is worth noting that while we provide manual enumeration of all possible skills for practical analysis, the full language space is much larger. This highlights the even more pronounced narrowing of the search in the language space.

## 6 Conclusions, Limitations, & Future Works

We presented Grounded Decoding (GD), an approach for leveraging the knowledge and capabilities of large language models in embodied settings through grounding functions, which model the probabilities of tokens given an embodiment. GD resembles probabilistic filtering, by decoding tokens that have high probabilities under the language model _and_ under grounded model(s). By guiding the LLM's decoding directly at its output, GD is a general, flexible, and expressive approach to embodied tasks. This is demonstrated on three embodied domains, showing GD is capable of solving complex, long-horizon tasks.

Though quite general and flexible, GD has a few limitations. First, although we present several techniques for obtaining grounding functions in different domains, it remains a question whether a capable and general grounding function can be obtained. We hope that recent progress in large-scale robotics models (e.g.  and ) can remove this bottleneck, and note that the flexibility of GD allows such progress to be straightforwardly leveraged. Second, prompt engineering is often required to steer LLMs to the desired action space (e.g., likely action verbs, likely present objects). Finally, while not requiring additional training, the joint decoding may be limiting compared to a single model capable of both grounding and language reasoning [3; 9; 16].

This work presented a family of algorithms for grounding LLMs in embodiment, for which there are many avenues for future work. The flexibility of the approach enables many other grounding functions and ways to integrate grounding. Furthermore, the development and integration of a foundation model for grounding would improve performance significantly. Finally, though GD's probabilistic filtering-based approach is quite general, fusing grounding information to the language model _after_ each token decoding may be limiting and future works can investigate how such grounding can be elegantly integrated _during_ decoding.

#### Acknowledgments

The authors would like to acknowledge Pierre Sermanet, Carolina Parada, Jie Tan, Yevgen Chebotar, Vincent Vanhoucke, and Dorsa Sadigh for their feedback and contributions. This work is supported in part by OpenAI academic access program, granted to Wenlong Huang.

Figure 6: Failure breakdown on tabletop domain. GD achieves lowest planning failure among planning-based methods, among which beam search variant performs the best.

Figure 7: Visualization of actions colored by affordance values in different scenes. Every dot represents a possible action in the tabletop domain, where the majority of the actions are infeasible. We show how grounded models can identify the feasible actions for specific scenes. Notably, these actions are not always clustered in language space, requiring the grounding function to determine what action to perform.