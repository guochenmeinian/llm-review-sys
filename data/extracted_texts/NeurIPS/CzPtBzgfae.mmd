# Don't Compress Gradients in Random Reshuffling:

Compress Gradient Differences

Abdurakhmon Sadiev\({}^{1,2}\), Grigory Malinovsky\({}^{1}\), Eduard Gorbunov\({}^{1,2,3,4}\),

**Igor Sokolov\({}^{1}\)**, Ahmed Khaled\({}^{5}\), Konstantin Burlachenko\({}^{1}\), Peter Richtarik\({}^{1}\)

\({}^{1}\)King Abdullah University of Science and Technology, Saudi Arabia

\({}^{2}\)Moscow Institute of Physics and Technology, Russian Federation

\({}^{3}\)Mohamed bin Zayed University of Artificial Intelligence, UAE

\({}^{4}\)Mila, Universite de Montreal, Canada

\({}^{5}\)Princeton University, USA

Part of the work was done when E. Gorbunov was a researcher at MIPT and Mila & UdeM and also a visiting researcher at KAUST, in the Optimization and Machine Learning Lab of P. Richtarik.Corresponding author, eduard.gorbunov@mbzuai.ac.ae

###### Abstract

Gradient compression is a popular technique for improving communication complexity of stochastic first-order methods in distributed training of machine learning models. However, the existing works consider only with-replacement sampling of stochastic gradients. In contrast, it is well-known in practice and recently confirmed in theory that stochastic methods based on without-replacement sampling, e.g., Random Reshuffling (RR) method, perform better than ones that sample the gradients with-replacement. In this work, we close this gap in the literature and provide the first analysis of methods with gradient compression and without-replacement sampling. We first develop a distributed variant of random reshuffling with gradient compression (Q-RR), and show how to reduce the variance coming from gradient quantization through the use of control iterates. Next, to have a better fit to Federated Learning applications, we incorporate local computation and propose a variant of Q-RR called Q-NASTYA. Q-NASTYA uses local gradient steps and different local and global stepsizes. Next, we show how to reduce compression variance in this setting as well. Finally, we prove the convergence results for the proposed methods and outline several settings in which they improve upon existing algorithms.

## 1 Introduction

Distributed learning plays a crucial role in the training of modern Deep Learning (DL) models since distributed approaches are able to significantly reduce training time . Moreover, distributed methods are mandatory for such applications as Federated learning (FL) , where multiple nodes connected over a network collaborate on a learning task. Each node possesses its own dataset and cannot share this data with other nodes or a central server. As a result, algorithms for federated learning often rely on local computation and lack access to the entire dataset of training examples. Federated learning finds applications in diverse fields, including language modeling for mobile keyboards ,healthcare (Antunes et al., 2022), and wireless communications (Yang et al., 2022). Its applications extend to various other domains (Kairouz et al., 2019).

Distributed learning tasks are often solved through _empirical-risk minimization_ (ERM), where the \(m\)-th device contributes an empirical loss function \(f_{m}(x)\) representing the average loss of model \(x\) on its local dataset, and our goal is to then minimize the average loss over all the nodes:

\[_{x^{d}}[f(x)}}{{=}} _{m=1}^{M}f_{m}(x)], \]

where the function \(f\) represents the average loss. Every \(f_{m}\) is an average of sample loss functions \(f_{m}^{i}\) each representing the loss of model \(x\) on the \(i\)-th datapoint on the \(m\)-th clients' dataset: that is for each \(m\{1,2,,M\}\) we have

\[f_{m}(x)}}{{=}}}_{i=1}^{ n_{m}}f_{m}^{i}(x).\]

For simplicity we shall assume that the datasets on all clients are of equal size: \(n_{1}=n_{2}==n_{M}\), though this assumption is only for convenience and our results easily extend to the case when clients have datasets of unequal sizes. Thus our optimization problem is

\[_{x^{d}}[f(x)=_{m=1}^{M}_{i=1}^{n}f_ {m}^{i}(x)]. \]

Because \(d\) is often very large in practice, the dominant paradigm for solving (2) relies on first-order (gradient) information. Federated learning algorithms have access to two key primitives: (a) local computation, where for a given model \(x^{d}\)we can compute stochastic gradients \( f_{m}^{i}(x)\) locally on client \(m\), and (b) communication, where the different clients can exchange their gradients or models with a central server.

### Communication compression

In practice, communication is more expensive than local computation (Kairouz et al., 2019), and as such one of the chief concerns of algorithms for distributed learning is communication efficiency. Algorithms for distributed/federated learning have thus focused on achieving communication efficiency, with one common ingredient being the use of _gradient compression_, where each client sends a compressed or quantized version of their update instead of the full update vector, potentially saving communication bandwidth by sending fewer bits over the network. There are many operators that can be used for compressing the update vectors: stochastic quantization (Alistarh et al., 2017), random sparsification (Wangni et al., 2018; Stich et al., 2018), and others (Tang et al., 2020). In this work we consider compression operators satisfying the following assumption:

**Assumption 1**.: _A compression operator is an operator \(:^{d}^{d}\) such that for some \(>0\), the relations_

\[[(x)]=x[ \|(x)-x\|^{2}]\|x\|^{2}x^{d}.\]

Unbiased compressors can reduce the number of bits clients communicate per round, but also increases the variance of the stochastic gradients used slowing down overall convergence, see e.g. (Khirirat et al., 2018, Theorem 5.2) and (Stich, 2020, Theorem 1). By using control iterates, Mishchenko et al. (2019) developed DIANA--an algorithm that can reduce the variance due to gradient compression with unbiased compression operators, and thus ensure fast convergence. DIANA has been extended and analyzed in many settings (Horvath et al., 2019; Stich, 2020; Safaryan et al., 2021) and forms an important tool in our arsenal for using gradient compression.

### Random Reshuffling

Despite the importance of addressing the communication bottleneck, local computations also significantly affect the training. For simplicity, consider the \(1\)-node scenario. In this case, the update rule of the standard work-horse method in stochastic optimization - stochastic gradient descent (SGD)[Robbins and Monro, 1951] - can be written as follows: \(x_{t+1}=x_{t}- f^{j}(x_{t})\), where \(j\) is sampled from \(\{1,,n\}\) uniformly at random. This procedure thus uses _with-replacement sampling_ in order to select the stochastic gradient used at each step from the dataset. However, in the training of DL models, _without-replacement sampling_ is used much more often: that is, at the beginning of each _epoch_ we choose a permutation \(_{1},_{2},,_{n}\) of \(\{1,2,,n\}\) and do the \(i\)-th update using the \(_{i}\)-ith gradient: \(x_{t}^{i+1}=x_{t}^{i}- f^{_{i}}(x_{t}^{i})\). Without-replacement sampling SGD, also known as Random Reshuffling (RR) [Bottou, 2009], typically achieves better asymptotic convergence rates compared to with-replacement SGD and can improve upon it in many settings as shown by recent theoretical progress [Mishchenko et al., 2020, Ahn et al., 2020, Rajput et al., 2020, Safran and Shamir, 2021]. While with-replacement SGD achieves an error proportional to \(()\) after \(T\) steps [Stich, 2019], Random Reshuffling achieves an error of \((})\) after \(T\) steps, faster than SGD when the number of steps \(T\) is large.

### Can Communication Compression and Random Reshuffling be Friends?

As we described earlier, Random Reshuffling and communication compression are two important tools for training modern DL models, and both techniques are relatively well understood. However, there are no papers that study Random Reshuffling and communication compression in combination. This leads us to the natural question: _how these techniques should be combined to improve the convergence speed of existing distributed methods?_

### Contributions

In this paper, we aim to develop methods for Distributed and Federated Learning that combine gradient compression and random reshuffling. While each of these techniques can aid in reducing the communication complexity of distributed optimization, their combination is under-explored. Thus our goal is to design methods that improve upon existing algorithms in convergence rates and in practice. We summarize our contributions as follows.

   Method & Complexity & RR? & C? & H? & CVX? \\  SGD & \(+^{2}}{^{2}}\) & & ✗ & ✗ & ✗\({}^{(1)}\) & ✓ \\ RR & \(+}{}n}{ }}\) & ✓ & ✗ & ✗\({}^{(1)}\) & ✗ \\ RR & \(n+}{}n}{}}\) & ✓ & ✗ & ✗\({}^{(1)}\) & ✓ \\  QSGD & \((1+)+^{2} +_{*}^{2}}{^{2}^{2}}+^{2}}{M^{2} }^{(3)}\) & ✗ & ✓ & ✗ & ✓ \\  Q-RR & \((1+)+^{2}+_{*}^{2}}{^{2}}+}{}n}{}}\) & ✓ & ✓ & ✗ & ✗ \\ Corollary 1 & \((n+)+^{2} +_{*}^{2}}{^{2}}+}{}n}{}}\) & ✓ & ✓ & ✗ & ✓ \\ Q-RR & \((n+)+^{2} +_{*}^{2}}{^{2}}+}{}n}{}}\) & ✓ & ✓ & ✗ & ✓ \\ Corollary 6 & \((n+)+^{2} +_{*}^{2}}{^{2}}+}{}n}{}}\) & ✓ & ✓ & ✗ & ✓ \\ DL/ANA & \((1+)+^{2} +_{*}^{2}}{M^{2}}+^{2}}{M^{2}}\) & ✗ & ✓ & ✓ & ✓ \\  DIANAR-RR & \(n(1+)+(1+)+ }{}n}{}}\) & ✓ & ✓ & ✗ \\ Corollary 2 & \(.n(1+)+(n+)+}{}n}{}}.\) & ✓ & ✓ & ✓ & ✗ \\ DIANAR-RR & \(n(1+)+(n+)+}{} n}{}}\) & ✓ & ✓ & ✓ & ✓ \\ Corollary 8 & \(n(1+)+(n+)+}{} n}{}}\) & ✓ & ✓ & ✓ & ✓ \\   

* \({}^{(i)}\) In the case of SGD, RR we use \(\) in “H?” to show that the complexity of these methods is provided in the non-distributed setup.
* \({}^{(2)}\) The following inequality is useful for the comparison of complexities: \(_{*,n}^{2}_{*}^{2}\).
* \({}^{(3)}\) We denote \(_{*}^{2}=(_{*}^{2}+_{*}^{2})+_{*,n}^{2}\).

Table 1: Summary of known and new complexity results for solving distributed finite-sum optimization (2). Column “Complexity” indicates the number of communication rounds to find a solution with accuracy \(>0\). Column “RR?” shows whether an algorithm uses _Random Reshuffling_, “C?” indicates whether a method applies the compression of gradients or difference between the gradients and also whether methods for communication, “H?” means independence from the constant of data heterogeneity in the complexity, “CVX?” indicates whether each loss on the \(i\)-th datapoint on the \(m\)-th client is convex, but not strongly convex. Notation: \(=L_{/n}/\) and \(=/}{}\) are conditional number of problem (2), where \(L_{}=\) Lipschitz constant, \(\) and \(\) are the strong convexity constants of \(f\) and \(f_{n}^{}\) respectively; variances at the solution point \(x_{*}\): \(_{*}^{2}=_{m=1}^{M}_{i=1}^{M}\| f _{m}^{i}(x_{*})- f_{m}(x_{*})\|^{2}\) and \(_{*,n}^{2}=_{i=1}^{n}\| f^{i}(x_{*})\|^{2}\); heterogeneity constant \(_{*}^{2}=_{m=1}^{M}\| f_{m}(x_{*})\|^{2}\). The results of this paper are highlighted in blue.

* **The issue: naive combination has no improvements.** As a natural step towards our goal, we propose and study a new algorithm, Q-RR (Algorithm 1), that combines random reshuffling with gradient compression at every communication round. However, for Q-RR our theoretical results do not show any improvement upon QSGD when the compression level is reasonable (see Table 1). Moreover, we observe similar performance of Q-RR and QSGD in various numerical experiments. Therefore, we conclude that this phenomenon is not an artifact of our analysis but rather an issue of Q-RR: communication compression adds an additional noise that dominates the one coming from the stochastic gradients sampling.
* **The remedy: reduction of compression variance.** To remove the additional variance added by the compression and unleash the potential of Random Reshuffling in distributed learning with compression, we propose DIANA-RR (Algorithm 2), a combination of Q-RR and the DIANA algorithm. We derive the convergence rates of the new method and show that it improves upon the convergence rates of Q-RR, QSGD, and DIANA (see Table 1). We point out that to achieve such results we use \(n\) shift-vectors per worker in DIANA-RR unlike DIANA that uses only \(1\) shift-vector.
* **Extensions to the local steps.** Inspired by the NASTYA algorithm of Malinovsky et al. (2022), we propose a variant of NASTYA, Q-NASTYA (Algorithm 3), that naively mixes quantization, local steps with random reshuffling, and uses different local and server stepsizes. Although it improves in per-round communication cost over NASTYA but, similar to Q-RR, we show that Q-NASTYA suffers from added variance due to gradient quantization. To overcome this issue, we propose another algorithm, DIANA-NASTYA (Algorithm 4), that adds DIANA-style variance reduction to Q-NASTYA and removes the additional variance.

Finally, to illustrate our theoretical findings we conduct experiments on federated logistic regression tasks and on distributed training of neural networks.

## 2 Algorithms and convergence theory

We will primarily consider the setting of strongly-convex and smooth optimization. We assume that the average function \(f\) is strongly convex:

**Assumption 2**.: _Function \(f:^{d}\) is \(\)-strongly convex, i.e., for all \(x,y^{d}\),_

\[f(x)-f(y)- f(y),x-y\|x-y\|^{2}, \]

_and functions \(f^{i}_{1},f^{i}_{2},,f^{i}_{M}:^{d}\) are convex for all \(i=1,,n\)._

Examples of objectives satisfying Assumption 2 include \(_{2}\)-regularized linear and logistic regression. Throughout the paper, we assume that \(f\) has the unique minimizer \(x_{}^{d}\). We also use the assumption that each individual loss \(f^{i}_{m}\) is smooth, i.e. has Lipschitz-continuous first-order derivatives:

**Assumption 3**.: _Function \(f^{i}_{m}:^{d}\) is \(L_{i,m}\)-smooth for every \(i[n]\) and \(m[M]\), i.e., for all \(x,y^{d}\) and for all \(m[M]\) and \(i[n]\),_

\[\| f^{i}_{m}(x)- f^{i}_{m}(y)\| L_{i,m}\|x-y\|. \]

_We denote the maximal smoothness constant as \(L_{}}}{{=}}_{i,m}L_{i,m}\)._

For some methods, we shall additionally impose the assumption that _each_ function is strongly convex:

**Assumption 4**.: _Each function \(f^{i}_{m}:^{d}\) is \(\)-strongly convex._

The _Bregman divergence_ associated with a convex function \(h\) is defined for all \(x,y^{d}\) as

\[D_{h}(x,y)}}{{=}}h(x)-h(y)-  h(y),x-y.\]

Note that the inequality (3) defining strong convexity can be written as \(D_{f}(x,y)\|x-y\|^{2}\).

### Algorithm Q-RR

The first method we introduce is Q-RR (Algorithm 1). Q-RR is a straightforward combination of distributed random reshuffling and gradient quantization. This method can be seen as the stochastic without-replacement analogue of the distributed quantized gradient method of Khirirat et al. (2018).

We shall use the notion of _shuffling radius_ defined by Mishchenko et al. (2021) for the analysis of distributed methods with random reshuffling:

**Definition 2.1**.: _Define the iterate sequence \(x_{}^{i+1}=x_{}^{i}-_{m=1}^{M} f_{m}^{_{ }^{i}}(x_{})\). Then the shuffling radius is the quantity_

\[_{}^{2}}}{{=}}_{i} \{M}_{m=1}^{M}D_{f_{m}^{_{}^{i}} }(x_{}^{i},x_{})\}.\]

We provide clarifications regarding this term in Appendix C.1. To compare our subsequent results with known ones, we introduce bounds on the shuffling radius. The following lemma demonstrates that these bounds are independent of the stepsize \(\), even though \(\) is used in Definition 2.1.

**Lemma 2.1** ((Mishchenko et al., 2020)).: _Let Assumptions 3, 4 hold. Then the shuffling radius \(_{}^{2}\) satisfies the following inequality_

\[n}{8}_{,n}^{2}_{}^{2} n}{4}_{,n}^{2},\]

_where \(_{,n}^{2}}}{{=}} _{i=1}^{n}\| f^{i}(x_{})\|^{2}\), and \(f^{i}=_{m=1}^{M}f^{i}_{m}\)._

We now state the main convergence theorem for Algorithm 1:

**Theorem 2.1**.: _Let Assumptions 1, 3, 4 hold and let the stepsize satisfy \(0<}{M})L_{}}\). Then, for all \(T 0\) the iterates produced by \(\)-\(\) (Algorithm 1) satisfy_

\[\|x_{T}-x_{}\|^{2}(1-)^{nT} \|x_{0}-x_{}\|^{2}+_{}^{2}}{}+M}(_{}^{2}+_{}^ {2}), \]

_where \(_{}^{2}}}{{=}} _{m=1}^{M}\| f_{m}(x_{})\|^{2}\), and \(_{}^{2}}}{{=}} _{m=1}^{M}_{i=1}^{n}\| f^{i}_{m}(x_{})- f_{ m}(x_{})\|^{2}\)._

All proofs are relegated to the appendix. By choosing the stepsize \(\) properly, we can obtain the communication complexity (number of communication rounds) needed to find an \(\)-approximate solution as follows:

**Corollary 1**.: _Under the same conditions as Theorem 2.1 and for Algorithm 1, there exists a stepsize \(>0\) such that the number of communication rounds \(nT\) to find a solution with accuracy \(>0\) (i.e. \(\|x_{T}-x_{}\|^{2}\)) is equal to \(}((1+)}{ }+^{2}+_{}^{2})}{M ^{2}}+}}{^{2}}}),\) where \(}()\) hides constants and logarithmic factors._

The complexity of Quantized SGD (QSGD) is (Gorbunov et al., 2020): \(}((1+)}{ }+^{2}+(1+)_{}^{2})} {M^{2}}).\) For simplicity, let us neglect the differences between \(\) and \(\). First, when \(=0\) we recover the complexity of FedRR (Mishchenko et al., 2021) which is known to be better than the one of SGD as long as \(\) is sufficiently small as we have \(n_{,n}^{2}/8_{}^{2} nL_{,n}^{2}/4\) from Lemma 2.1. Next, when \(M=1\) and \(=0\) (single node, no compression) our results recovers the rate of RR (Mishchenko et al., 2020).

However, it is more interesting to compare \(\)-RR and \(\) when \(M>1\) and \(>1\), which is typically the case. In these settings, \(\)-RR and \(\) have _the same complexity_ since the \((}{{}})\) term dominates the \((}{{}})\) one if \(\) is sufficiently small. That is, the derived result for \(\)-RR has no advantages over the known one for \(\) unless \(\) is very small, which means that there is almost no compression at all. We also observe this phenomenon in the experiments.

The main reason for that is the variance appearing due to compression. Indeed, even if the current point is the solution of the problem (\(x_{t}^{i}=x_{}\)), the update direction \(-_{m=1}^{M}( f_{m}^{_{m}^{i}}( x_{t}^{i}))\) has the compression variance

\[_{}_{m=1}^{M}( ( f_{m}^{_{m}^{i}}(x_{}))- f_{m}^{_{m}^{i}}( x_{}))^{2}}{M^{2}}_{m=1}^{M} \| f_{m}^{_{m}^{i}}(x_{})\|^{2}.\]

This upper bound is tight and non-zero in general. Moreover, it is proportional to \(^{2}\) that creates the term proportional to \(\) in (5) like in the convergence results for \(/\), while the RR-variance is proportional to \(^{2}\) in the same bound. Therefore, during the later stages of the convergence \(\)-RR behaves similarly to \(\) when we decrease the stepsize.

### Algorithm \(\)-Rr

To reduce the additional variance caused by compression, we apply \(\)-style shift sequences (Mishchenko et al., 2019, Horvath et al., 2019). Thus we obtain \(\)-RR (Algorithm 2), which applies compression to the differences between the gradients and learnable shifts. Since the shifts are updated using the past gradients information, one can see \(\)-RR as a method with compression of gradient differences. We notice that unlike \(\), \(\)-RR has \(n\) shift-vectors on each node.

**Theorem 2.2**.: _Let Assumptions 1, 3, 4 hold and suppose that the stepsizes satisfy \(\{},)L_{}}\},\) and \(\). Define the following Lyapunov function for every \(t 0\)_

\[_{t+1}}}{{=}}\|x_{t+1}-x_{}\|^{2}+ }{ M^{2}}_{m=1}^{M}_{j=0}^{n-1}(1- )^{j}\|_{t+1,m}^{j}\|^{2}, \]_where \(_{t+1,m}^{j}=h_{t+1,m}^{_{j}^{j}}- f_{m}^{_{j}^{j}}(x_{})\) Then, for all \(T 0\) the iterates produced by_ DIANA-RR _(Algorithm 2) satisfy_

\[[_{T}](1-)^{nT} _{0}+_{}^{2}}{}\]

**Corollary 2**.: _Under the same conditions as Theorem 2.2 and for Algorithm 2, there exists stepsizes \(,>0\) such that the number of communication rounds \(nT\) to find a solution with accuracy \(>0\) is \(}(n(1+)+(1+) {L_{}}{}+}}{ ^{3}}).\)_

Unlike Q-RR/QSGD/DIANA, DIANA-RR does not have a \(}(}{{}})\)-term, which makes it superior to Q-RR/QSGD/DIANA for small enough \(\). However, the complexity of DIANA-RR has an additive \(}(n(1+))\) term arising due to learning the shifts \(\{h_{t,m}^{i}\}_{m[M],i[n]}\). Nevertheless, this additional term is not the dominating one when \(\) is small enough. Next, we elaborate a bit more on the comparison between DIANA and DIANA-RR. That is, DIANA has \(}((1+)}{ }+^{2}}{M^{2}})\) complexity [Gorbunov et al., 2020]. Neglecting the differences between \(\) and \(\), we observe a similar relation between DIANA-RR and DIANA as between RR and SGD: instead of the term \((^{2}}}{{(M^{2})}})\) appearing in the complexity of DIANA, DIANA-RR has \((}}}{{^{3}}})\) term much better depending on \(\). To the best of our knowledge, our result is the only known one establishing the theoretical superiority of RR to regular SGD in the context of distributed learning with gradient compression. Moreover, when \(=0\) (no compression) we recover the rate of FedRR and when additionally \(M=1\) (single worker) we recover the rate of RR.

### Algorithms with Local Steps

In this subsection, we study a new variant of NASTYA, Q-NASTYA (Algorithm 3), that unifies quantization, local steps with random reshuffling, and uses different local and server stepsizes. Although it improves in per-round communication cost over NASTYA but, similar to Q-RR, we show that Q-NASTYA suffers from added variance due to gradient quantization. To overcome this issue, we propose another algorithm, DIANA-NASTYA (Algorithm 4), that adds DIANA-style variance reduction to Q-NASTYA and removes the additional variance.

**Theorem 2.3**.: _Let Assumptions 1, 2, 3 hold. Let the stepsizes \(\), \(\) satisfy \(0<(1+)},\)\(0<}.\) Then, for all \(T 0\) the iterates produced by_ Q-NASTYA _(Algorithm 3) satisfy_

\[[\|x_{T}-x_{}\|^{2}](1- )^{T}\|x_{0}-x_{}\|^{2}+8_{}^{2} +nL_{}}{}((n+1)_{}^{2}+ _{}^{2}).\]

**Corollary 3**.: _Under the same conditions as Theorem E.1 and for Algorithm 3, there exist stepsizes \(=}{{n}}\) and \(>0\) such that the number of communication rounds \(T\) to find a solution with accuracy 

[MISSING_PAGE_FAIL:8]

In contrast to Q-NASTYA, DIANA-NASTYA does not suffer from the \(}(}{{}})\) term in the complexity bound. This shows the superiority of DIANA-NASTYA to Q-NASTYA. Next, FedCRR-VR (Malinovsky and Richtarik, 2022) has the rate \(}()^{n}}{(1-(1-)^{n} )^{2}}+(_{*}+_{*})}{} ),\) which depends on \(}(}{{}})\). However, the first term is close to \(}((+1)^{2})\) for a large condition number. FedCRR-VR-2 utilizes variance reduction technique from Malinovsky et al. (2021) and it allows to get rid of permutation variance. This method has \(}(})^{}}{(1-(1-})^{})^{2}}+ _{*}}{})\) complexity, but it requires additional assumption on number of functions \(n\) and thus not directly comparable with our result. Note that if we have no compression \((=0)\), DIANA-NASTYA recovers rate of NASTYA.

In Appendix J, we provide versions of Q-NASTYA and DIANA-NASTYA with partial participation of clients, which is another important aspect of FL, and derive the convergence results for them.

## 3 Experiments

We evaluated our methods for solving logistic regression problems and training neural networks in three parts: (i) Comparison of the proposed non-local methods with existing baselines; (ii) Comparison of the proposed local methods with existing baselines; (iii) Comparison of the proposed non-local methods in training ResNet-18 on CIFAR10.

Logistic Regression.To confirm our theoretical results we conducted several numerical experiments on binary classification problem with L2 regularized logistic regression of the form

\[_{x^{d}}[f(x)}}{{=}} _{m=1}^{M}}_{i=1}^{n_{m}}f_{m,i}], \]

where \(f_{m,i}}}{{=}}(1+(-y_{mi}a_{mi} ^{}x))+\|x\|_{2}^{2}(a_{mi},y_{mi})^{ d}\{-1,1\},i=1,,n_{m}\) are the training data samples stored on machines \(m=1,,M\), and \(>0\) is a regularization parameter. In all experiments, for each method, we used the largest stepsize allowed by its theory

Figure 3: The comparison of the proposed methods (Q-NASTYA, DIANA-NASTYA, Q-RR, DIANA-RR), DIANA-RR-1S (a modification of DIANA-RR), and existing baselines (QSGD, DIANA, FedCOM, FedPAQ). All methods use tuned stepsizes and the Rand-\(k\) compressor.

Figure 2: Local methods

multiplied by some individually tuned constant multiplier. For better parallelism, each worker \(m\) uses mini-batches of size \( 0.1n_{m}\). In all algorithms, as a compression operator \(\), we use Rand-\(k\)(Beznosikov et al., 2020) with fixed compression ratio \(}{{d}} 0.02\), where \(d\) is the number of features in the dataset.

In our first experiment (see Figure 1), we compare Q-RR, DIANA-RR, and DIANA-RR-1S with classical baselines (QSGD(Alistarh et al., 2017), DIANA(Mishchenko et al., 2019)) that use a with-replacement mini-batch SGD estimator. DIANA-RR-1S is a memory-friendly version of DIANA-RR that stores and uses a single shift \(h_{t,m}\) on the worker side rather than \(n\) individual shifts \(h_{t,m}^{*}\). Figure 1 illustrates that Q-RR exhibits similar behavior to QSGD, with both methods being slower than DIANA methods across all considered datasets. DIANA-RR-1S and DIANA show comparable convergence rates, indicating that random reshuffling alone, without introducing additional shifts, does not make a significant difference. Finally, DIANA-RR achieves the best rate among all considered non-local methods, efficiently reducing the variance and reaching the lowest functional sub-optimality tolerance. These experimental results align perfectly with our theoretical analysis.

The second experiment shows that DIANA-based method can significantly outperform in practice when one applies it to local methods as well. In particular, whereas Q-NASTYA shows comparative behavior as existing methods FedCOM(Haddadpour et al., 2021), FedPAQ(Reisizadeh et al., 2020) in all considered datasets, DIANA-NASTYA noticeably outperforms other methods.

Training Deep Neural Network model: ResNet-18 on CIFAR-10.Since random reshuffling is a very popular technique in training neural networks, it is natural to test the proposed methods on such problems. Therefore, in the second set of experiments, we consider training ResNet-18(He et al., 2016) model on the CIFAR10 dataset Krizhevsky and Hinton (2009). To conduct these experiments we use FL_PyTorch simulator (Burlachenko et al., 2021).

The main goal of this experiment is to verify the phenomenon observed in Experiment 1 on the training of a deep neural network. That is, we tested Q-RR, QSGD, DIANA, and DIANA-RR in the distributed training of ResNet-18 on CIFAR10, see Figure 4. As in the logistic regression experiments, we observe that (i) Q-RR and QSGD behave similarly and (ii) DIANA-RR outperforms DIANA. For further experimental results and details, we refer to Appendix B.

## 4 Conclusion

In this work, we provide the first study of distributed random reshuffling with communication compression. Our theoretical and empirical findings illustrate the inefficiency of naive combination of random reshuffling and communication compression. We also show how this issue can be resolved via the usage of shifts for communication compression. Finally, we develop and analyze methods with random reshuffling, communication compression, and local steps. It is worth mentioning that although our theoretical results are obtained for strongly convex problems, the considered methods perform well in the experiments on non-convex tasks like training neural networks.

Figure 4: The comparison of Q-RR, QSGD, DIANA, and DIANA-RR on the task of training ResNet-18 on CIFAR-10 with \(n=10\) workers. Top-1 accuracy on test set is reported. Stepsizes were tuned and workers used Rand-\(k\) compressor with \(}{{d}} 0.05\).