# Higher-Rank Irreducible Cartesian Tensors for

Equivariant Message Passing

 Viktor Zaverkin\({}^{1,}\)1  Francesco Alesiani\({}^{1,}\)2  Takashi Maruyama\({}^{1,}\)3  Federico Errica\({}^{2}\)

Henrik Christiansen\({}^{1}\)  Makoto Takamoto\({}^{1}\)  Nicolas Weber\({}^{1}\)  Matthias Niepert\({}^{1,3}\)

\({}^{1}\)NEC Laboratories Europe \({}^{2}\)NEC Italy \({}^{3}\)University of Stuttgart

###### Abstract

The ability to perform fast and accurate atomistic simulations is crucial for advancing the chemical sciences. By learning from high-quality data, machine-learned interatomic potentials achieve accuracy on par with ab initio and first-principles methods at a fraction of their computational cost. The success of machine-learned interatomic potentials arises from integrating inductive biases such as equivariance to group actions on an atomic system, e.g., equivariance to rotations and reflections. In particular, the field has notably advanced with the emergence of equivariant message passing. Most of these models represent an atomic system using spherical tensors, tensor products of which require complicated numerical coefficients and can be computationally demanding. Cartesian tensors offer a promising alternative, though state-of-the-art methods lack flexibility in message-passing mechanisms, restricting their architectures and expressive power. This work explores higher-rank irreducible Cartesian tensors to address these limitations. We integrate irreducible Cartesian tensor products into message-passing neural networks and prove the equivariance and traceless property of the resulting layers. Through empirical evaluations on various benchmark data sets, we consistently observe on-par or better performance than that of state-of-the-art spherical and Cartesian models.

## 1 Introduction

The ability to perform sufficiently fast and accurate atomistic simulations for large molecular and material systems holds the potential to revolutionize molecular and materials science . Conventionally, computational chemistry and materials science rely on ab initio or first-principles approaches--e.g., coupled cluster  or density functional theory (DFT) , respectively--that are accurate but computationally demanding, thus limiting the accessible simulation time and system sizes. However, the ability to generate high-quality, first-principles-based data sets has prompted the development of machine-learned interatomic potentials (MLIPs). These potentials enable atomistic simulations with accuracy that is on par with the reference first-principles method but at a fraction of the computational cost. Message-passing neural networks (MPNNs)  have been employed in chemical and materials sciences, including the development of MLIPs, due to their efficient processing of the graph representation of the atomic system . Achieving the desired MLIPs' performance, however, requires the inclusion of inductive biases, like the invariance of total energy to translations, reflections, and rotations in the three-dimensional space, and an effective encoding of the atomic system into a learnable representation . Designing equivariant MPNNs , which preserve the directional information of the local atomic environment, has been one of the most active research directions of the last years. Previous work often achieves equivariance by representing an atomicsystem as a graph, with node features expressed in spherical harmonics basis or using lower-rank Cartesian tensors and designing ad-hoc message-passing layers [25; 28; 29; 30; 31; 32; 33; 34; 35; 36; 37].

MLIPs based on spherical harmonics [25; 33; 34], which are the basis for irreducible representations of the three-dimensional rotation group, often demonstrate a better performance compared to those that use lower-rank Cartesian representations (scalars and vectors) [28; 37]. Spherical tensors, however, require the definition of a particular rotational axis, often chosen as the \(z\)-axis, resulting in inherent bias . Furthermore, coupling spherical tensors via tensor products, involved in designing equivariant convolutions and constructing many-body features [33; 39; 40], requires the definition of complicated numerical coefficients, e.g., Wigner \(3\)-\(j\) symbols defined in terms of the Clebsch-Gordan coefficients , and can be computationally demanding. In contrast, irreducible Cartesian tensors have no preferential directions; their tensor products are simpler and have a better computational complexity--up to a certain tensor rank--than the tensor products of spherical tensors [42; 43; 44; 45; 38]. Recent work improved results of Cartesian MPNNs by using rank-two tensors and decomposing them into irreducible representations of the three-dimensional rotation group , i.e., into representations of dimension 1 (trace), 3 (anti-symmetric part), and 5 (traceless symmetric part) . Higher-rank reducible Cartesian tensors have also been integrated into many-body MPNNs . These state-of-the-art Cartesian approaches, however, lack the flexibility of their message-passing mechanisms. They rely exclusively on convolutions with invariant filters and restrict the construction of many-body features, limiting the range of possible architectures and their expressive power.

**Contributions.** In this work, we address the limitations of state-of-the-art Cartesian models and demonstrate that operating with irreducible Cartesian tensors leads to on-par or, sometimes, even better performance than that of spherical counterparts. Particularly, this work goes beyond scalars, vectors, and rank-two tensors and explores the integration of higher-rank irreducible Cartesian tensors and their products into equivariant MPNNs: i) We demonstrate how irreducible Cartesian tensors that are symmetric and traceless can be constructed from a unit vector and a product of two irreducible Cartesian tensors, essential for equivariant convolutions and constructing many-body features; ii) We prove that the resulting tensors are traceless and equivariant under the action of the three-dimensional rotation and reflection group; iii) We demonstrate that higher-rank irreducible Cartesian tensors can be used to design cost-efficient--up to a certain tensor rank--and accurate equivariant models of many-body interactions; iv) We conduct different experiments to assess the effectiveness of equivariant message passing based on irreducible Cartesian tensors, and achieve state-of-the-art performance on benchmark data sets such as rMD17 , MD22 , 3BPA , acetylacetone , and Ta-V-Cr-W . We hope our contributions will offer new insights into the use of Cartesian tensors for constructing accurate and cost-efficient MLIPs and beyond.

## 2 Background

**Group representations and equivariance.** A group \((G,)\) is defined by a set of elements \(G\) and a group product \(\). A representation \(D\) of a group is a function from \(G\) to square matrices such that \(D[g]D[g^{}]=D[g g^{}],\,\,g,g^{} G\). This representation defines an action of \(G\) to any vector space \(\) (of the same dimension as the dimension of square matrices) through the matrix-vector multiplication, i.e., \((g,) D_{}[g]\) for \(\,g G\) and \(\,\). For vector spaces \(\) and \(\), a function \(f:\) is called equivariant to the action of a group \(G\) to \(\) and \(\) iff

\[f(D_{}[g])=D_{}[g]f()\,,\,g  G.\]

Invariance can be seen as a special type of equivariance with \(D_{}[g]\) being the identity for all \(g\). An important class of equivariant models focuses on equivariance to the action of the Euclidean group \((3)\), comprising translations and the orthogonal group \((3)\), i.e., rotation group \((3)\) and reflections, in \(^{3}\). MLIPs based on equivariant models usually focus on equivariance to the action of the orthogonal group \((3)\) and are invariant to translations.

**Cartesian tensors.** A vector \(^{3}\) transforms under the action of the rotation group \((3)\) as \(^{}=R\), i.e., each component of it transforms as \(()^{}_{i}=_{j}R_{ij}()_{j}\). Here, \(R=D_{}[g]^{3 3}\) denotes the rotation matrix representation of \(g(3)\). Cartesian tensors of rank \(n\) are described by \(3^{n}\) numbers and generalize the concept of vectors. A rank-\(n\) Cartesian tensor \(\) can be viewed as a multidimensional array with \(n\) indices, i.e., \(()_{i_{1}i_{2} i_{n}}\) with \(i_{k}\{1,2,3\}\) for \(\,k\{1,,n\}\). Furthermore, each index of \(()_{i_{1}i_{2} i_{n}}\) transforms independently as a vector under rotation. For example, a rank-two tensor transforms under rotation as \(^{}=RR^{}\), i.e., each component of it transforms as \(()_{i_{1}i_{2}}^{l}=_{j_{1},j_{2}}R_{i_{1}j_{1}}R_{i_{2}j _{2}}()_{j_{2}j_{2}}\). For Cartesian tensors, one defines an \(r\)-fold tensor contraction and an outer product, i.e., \(()_{j_{1} j_{s}}=_{i_{1},,i_{r}}( )_{i_{1} i_{r}j_{1} j_{s}}()_{ i_{1} i_{r}}\) and \(()_{i_{1} i_{r}j_{1} j_{s}}=( )_{i_{1} i_{r}}()_{j_{1} j_{s}}\), respectively. Cartesian tensors are generally reducible and can, thus, be decomposed into smaller representations that transform independently within their linear subspaces under rotation. For example, a rank-two reducible Cartesian tensor contains representations of dimension 1 (trace), 3 (anti-symmetric part), and 5 (traceless symmetric part): \(_{i_{1}}()_{i_{1}i_{1}}\), \(()_{i_{1}i_{2}}-()_{i_{2}i_{1}}\), and \(()_{i_{1}i_{2}}+()_{i_{2}i_{1}}-2/ 3_{i_{1}}()_{i_{1}i_{1}}\), respectively. Under rotation, the traceless symmetric part remains within its irreducible subspace, with a similar behavior for other irreducible components of \(()_{i_{1}i_{2}}\). Furthermore, for a reducible Cartesian tensor of rank \(n\), an action of the rotation group \((3)\) can be represented with a \(3^{n} 3^{n}\)-dimensional rotation matrix. This rotation matrix is also reducible, meaning that an appropriate change of basis can block diagonalize it into smaller subsets, the irreducible representations of the rotation group \((3)\).

**Spherical tensors.** Spherical harmonics (or spherical tensors) \(Y_{m}^{l}\) are functions from the points on a sphere to complex or real numbers, with degree \(l 0\) and components \(-l m l\). Collecting all components for a given \(l\) we obtain a \((2l+1)\)-dimensional object \(^{l}=(Y_{-l}^{l},,Y_{l-1}^{l},Y_{l}^{l})\). Spherical tensors transform under rotation as \(Y_{m}^{l}(R})=_{m^{}}()_{mm^{}}^{l}Y_ {m^{}}^{l}(})\), where \(()_{mm^{}}^{l}\) is the irreducible representation of \((3)\)--the Wigner \(D\)-matrix--and \(R\) denotes the rotation matrix. Spherical tensors are irreducible and can be combined using the Clebsch-Gordan tensor product

\[(Y_{m_{1}}^{l_{1}}_{}Y_{m_{2}}^{l_{2}})_{m_{3}}^{ l_{3}}=_{m_{1}=-l_{1}}^{l_{1}}_{m_{2}=-l_{2}}^{l_{2}}C_{l_{1}m_{1},l_{2}m_{ 2}}^{l_{3}m_{3}}Y_{m_{1}}^{l_{1}}Y_{m_{2}}^{l_{2}},\]

with Clebsch-Gordan coefficients \(C_{l_{1}m_{1},l_{2}m_{2}}^{l_{3}m_{3}}\) and \(l_{3}\{|l_{1}-l_{2}|,,l_{1}+l_{2}\}\). Furthermore, any reducible Cartesian tensor of rank \(n\) can be written in spherical harmonics as \((})_{i_{1}}(})_{i_{2}} (})_{i_{n}}=_{l=0}^{n}_{m=-l}^{l}X_{i_ {1}i_{2} i_{n}}^{lm}Y_{m}^{l}\), with coefficients \(X_{i_{1}i_{2} i_{n}}^{lm}\) defined elsewhere .

## 3 Related work

**Equivariant message-passing potentials.** Equivariant MPNNs [28; 29; 30; 31; 32; 33; 34; 35; 52; 53; 54] often outperform more traditional invariant models [55; 56; 57; 58; 59; 60]. While many invariant and equivariant MPNNs rely on two-body features within a single message-passing layer, there is a growing interest in constructing higher-body-order features, such as angles and dihedrals, to model many-body interactions in atomic systems [57; 59; 60]. Note that equivariant models with two-body features in a single message-passing layer build many-body ones through repeated message-passing iterations, increasing the receptive field and, thus, computational cost. Recent work recognized the importance of higher-body-order features but faced challenges due to explicit summation over triplets or quadruplets [57; 59]. In contrast, MACE advances the current state-of-the-art by combining ACE and equivariant message passing, introducing cost-efficient many-body message-passing potentials . With just two message-passing layers, MACE yields accurate potentials for interacting many-body systems .

**Beyond Clebsch-Gordan tensor product.** Despite the success of equivariant MPNNs based on spherical tensors, the high computational cost of the Clebsch-Gordan tensor product limits their computational efficiency [28; 30; 33; 34; 53; 54; 62; 63; 64]. Thus, current research focuses on alternatives to the Clebsch-Gordan tensor product, which has a \((L^{5})\) complexity for tensors up to degree \(L\). Recently, the relation of Clebsch-Gordan coefficients to the integral of products of three spherical harmonics, known as the Gaunt coefficients , has been exploited to reduce the computational cost of the tensor product of spherical tensors to \((L^{3})\), compared to \((L^{6})\) of the full Clebsch-Gordan tensor product including all \((l_{1},l_{2}) l_{3}\). However, this approach excludes odd tensor products and, thus, restricts the expressive power and the range of possible architectures.

Cartesian tensors and their products offer another promising alternative to the Clebsch-Gordan tensor product, enabling the efficient construction of message-passing layers with two- and many-body features. Recent work has explored decomposing rank-two tensors into their irreducible representations and using higher-rank reducible tensors [30; 31]. These approaches, however, are limited to convolutions with invariant filters, restricting the range of possible architectures and, thus, the expressive power of resulting Cartesian models. Furthermore, they provide limited mechanisms for constructing higher-body-order features, restricted to three-body features obtained through matrix-matrix multiplication and invariant many-body features obtained through full tensor contractions, similar to moment tensor potentials and Gaussian moments [65; 66; 67]. Finally, using reducible Cartesian tensors during message-passing leads to mixing different irreducible representations, which can hinder the performance of resulting models compared to state-of-the-art spherical models .

## 4 Methods

We define an atomic configuration \(=\{_{u},Z_{u}\}_{u=1}^{N_{}}\), where \(_{u}^{3}\) denotes Cartesian coordinates and \(Z_{u}\) represents the atomic number of atom \(u\), with a total of \(N_{}\) atoms. Our focus lies on message-passing MLIPs, parameterized by \(\), that learn a mapping from a configuration \(\) to the total energy \(E\), i.e., \(f_{}: E\). Thus, we represent molecular and material systems as graphs in a three-dimensional Euclidean space. An edge \(\{u,v\}\) exists if atoms \(u\) and \(v\) are within a cutoff distance \(r_{c}\), i.e., \(\|_{u}-_{v}\|_{2} r_{c}\). For more details on MPNNs, see Appendix A. The total energy of an atomic configuration is defined by the sum of individual atomic energy contributions , i.e., \(E=_{u=1}^{N_{}}E_{u}\). Atomic forces are computed as negative gradients of the total energy with respect to atomic coordinates, i.e., \(_{u}=-_{_{u}}E\).

### Irreducible Cartesian tensor product

In the following, we explore irreducible Cartesian tensors and their products based on the three-dimensional vector space and the three-dimensional orthogonal group \((3)\), which comprises rotations and reflections. The respective tensors and tensor products are schematically illustrated in Fig. 1 (a) and (b), respectively, and are further employed in constructing MPNNs for atomic systems equivariant under actions of the orthogonal group. An irreducible Cartesian tensor \(_{n}(^{3})^{ n}\) of rank \(n\) and weight \(l n\) (related to the degree \(l\) of spherical tensors) can be represented by a tensor with \(3^{n}\) components in a three-dimensional vector space. These \(3^{n}\) components form a basis for a \((2l+1)\)-dimensional irreducible representation of the three-dimensional rotation group \((3)\)[69; 70]. Thus, only \(2l+1\) of the \(3^{n}\) components are independent.

The rotation in the space of all tensors of rank \(n\) is induced through the \(n\)-fold outer product of a rotation matrix \(R^{3 3}\), i.e., \(R^{ n}=R R\). The obtained representation of the

Figure 1: **Schematic illustration of (a) the construction of an irreducible Cartesian tensor for a local atomic environment and (b) the tensor product of two irreducible Cartesian tensors of rank \(l_{1}\) and \(l_{2}\).** The construction of an irreducible Cartesian tensor from a unit vector \(}\) is defined in Eq. (1). In this work, we use tensors with the same rank \(n\) and weight \(l\), i.e., \(n=l\), avoiding the need for embedding tensors with \(l<n\) in a higher-dimensional tensor space. Therefore, we use \(l\) to identify the rank and the weight of an irreducible Cartesian tensor. The tensor product is defined in Eqs. (2) and (3), resulting in a new tensor \(_{l_{3}}=(_{l_{1}}_{}_{l_{2 }})_{l_{3}}\) of rank \(l_{3}=\{|l_{1}-l_{2}|,,l_{1}+l_{2}\}\). Transparent boxes denote the linearly dependent elements of symmetric and traceless tensors. The tensor product can be even or odd, defined by \(l_{1}+l_{2}-l_{3}\).

rotation group \((3)\) is reducible for all \(n\) except \(n=\{0,1\}\). Reducing a tensor of rank \(n\) yields a unique irreducible tensor with the same weight and rank (\(n=l\)), which is characterized by being symmetric, i.e., \((_{n})_{ i j}=(_{n})_{ j i }\), \(\;i j\{i_{1},i_{n}\}\), and traceless, i.e., \(_{i}(_{n})_{ i i}=0\;,\,i\{i_{1}, ,i_{n}\}\). An irreducible tensor of rank \(n\) and weight \(l\) with \(l<n\) can be viewed as a \(l\)-rank tensor embedded in the \(n\)-rank tensor space, e.g., by computing an outer product with the identity matrix. However, the embedding is often not unique. Thus, we construct tensors with the same weight and rank (\(n=l\)) in the following.

**Irreducible Cartesian tensors from unit vectors.** Hereafter, we denote the rank and the weight of irreducible Cartesian tensors by \(l\) to distinguish them from reducible counterparts. An irreducible Cartesian tensor of an arbitrary rank \(l\) can be constructed from a unit vector \(}\) in the form 

\[_{l}(})=C_{m=0}^{ l/2}(-1 )^{m}}^{(l-2m)} }^{ m}},\] (1)

resulting in a symmetric and traceless tensor of rank \(l\). Here, \(\) denotes the \(3 3\) identity matrix, \(}^{(l-2m)}=}}\) and \(^{ m}=\) are the corresponding \((l-2m)\)- and \(m\)-fold outer products. The curly brackets indicate the summation over all permutations of the \(l\) unsymmetrized indices , i.e., \(\{_{l}\}_{i_{1} i_{l}}=_{ S_{l}}(_{l})_{i _{(1)} i_{(l)}}\), with \(S_{l}\) being the corresponding set of permutations. The expression in Eq. (1) involves three distinct outer products \((}})_{i_{1}i_{2}}=_{i_{1}} _{i_{2}}\), \(()_{i_{1}i_{2}i_{3}i_{4}}=_{i_{1}i_{2}} _{i_{3}i_{4}}\), and \((})_{i_{1}i_{2}i_{3}}=_{i_{1}}_{ i_{2}i_{3}}\), where \(_{i_{2}i_{3}}\) denotes the Kronecker delta. The normalization constant \(C=(2l-1)!!/l!\) is chosen such that an \(l\)-fold contraction of \(_{l}\) with the unit vector \(}\) yields unity. An example of an irreducible Cartesian tensor with rank \(l=3\) is \((_{l=3})_{i_{1}i_{2}i_{3}}=_{i_{1}} _{i_{2}}_{i_{3}}-(_{i_{1}}_{i_{2}i_{3}}+ _{i_{2}}_{i_{3}i_{1}}+_{i_{3}}_{i_{1}i_{2}})\).

**Irreducible Cartesian tensor product.** The following defines the product of two irreducible Cartesian tensors \(_{l_{1}}(^{3})^{ l_{1}}\) and \(_{l_{2}}(^{3})^{ l_{2}}\), yielding an irreducible Cartesian tensor of rank \(l_{3}\), i.e., \(_{l_{3}}=(_{l_{1}}_{}_{l_{2} })_{l_{3}}(^{3})^{ l_{3}}\,\,l_{3}\{|l_{1}-l_{2} |,,l_{1}+l_{2}\}\), that is symmetric and traceless. The irreducible Cartesian tensor product is crucial for designing equivariant MPNNs and is used for equivariant convolutions and constructing many-body features in Section 4.2. For an even \(l_{1}+l_{2}-l_{3}=2k\), the general form of an irreducible Cartesian tensor of rank \(l_{3}\) reads 

\[&(_{l_{1}}_{}_{l _{2}})_{l_{3}}\\ =& C_{l_{1}l_{2}l_{3}}_{m=0}^{(l_{1},l_{2})-k}(-1)^{m}2^{m}-2m-1)!!}{(2l_{3}-1)!!}\,(_{l_{1}}(k+m)_{l_{2}})^{ m} },\] (2)

where \((_{l_{1}}(k+m)_{l_{2}})=_{i_{1},,i_{k +m}}(_{l_{1}})_{i_{1} i_{k+m}}(_{l_{2}})_{i_{1}  i_{k+m}}\) denotes an \((k+m)\)-fold tensor contraction, which results in a tensor of rank \(l_{1}+l_{2}-2(k+m)\). For simplicity, we skip the uncontracted indices in the above definition. For example, for \(l_{1}=4\) and \(l_{2}=3\) and a three-fold tensor contraction we obtain \((_{l_{1}} 3_{l_{2}})_{i_{4}}=_{i_{1},i_{2},i_{3}}( _{l_{1}})_{i_{1}i_{2}i_{3}i_{4}}(_{l_{2}})_{i_{1}i_{2}i_{3}}\), i.e., the corresponding tensors are contracted along \(i_{1}\), \(i_{2}\), and \(i_{3}\). Note that the final result is independent of the index permutation, as the contracted tensors are symmetric. For an odd \(l_{1}+l_{2}-l_{3}=2k+1\), we define 

\[(_{l_{1}}_{}_{l _{2}})_{l_{3}}\\ =& D_{l_{1}l_{2}l_{3}}_{m=0}^{(l_{1},l_{2})-k-1}(-1)^{m}2^{m}-2m-1)!!}{(2l_{3}-1)!!}\,( :_{l_{1}}(k+m)_{l_{2}}) ^{ m}},\] (3)

with \(\) denoting the Levi-Civita symbol (\(_{i_{1}i_{2}i_{3}}=-_{i_{3}i_{2}i_{4}}\) and \(_{i_{1}i_{1}i_{1}}=0\)). The double contraction with the Levi-Civita symbol reads \((:_{l_{1}}(k+m)_{l_{2}})_{ i_{1}}=_{i_{2},i_{3}}_{i_{1}i_{2}i_{3}}\,(_{l_{1}} (k+m)_{l_{2}})_{i_{2}i_{3}}\), and yields a tensor of rank \(l_{1}+l_{2}-2(k+m)-1\). Details on the normalization constants \(C_{l_{1}l_{2}l_{3}}\) and \(D_{l_{1}l_{2}l_{3}}\) are provided in Appendix B.1.

### Equivariant message-passing based on irreducible Cartesian tensors

The following section introduces the basic operations for constructing equivariant MPNNs based on irreducible Cartesian tensors. Using their irreducible tensor products, we demonstrate how to build equivariant two- and many-body features, crucial for modeling many-body interactions in molecular and materials systems. Particularly, we focus on MLIPs based on equivariant MPNNs and extend the state-of-the-art MACE architecture  to the Cartesian basis. Following the MACE architecture, we use only even tensor products. We split vectors \(_{uv}=_{u}-_{v}^{3}\) from atom \(u\) to atom \(v\), schematically shown in Fig. 1 (a), into their radial and angular components (unit vectors), i.e.,\(r_{uv}=\|_{uv}\|_{2}\) and \(}_{uv}=_{uv}/r_{uv}^{3}\), respectively. In the \(t\)-th message-passing layer, edges \(\{u,v\}\) are embedded using a fully connected neural network \(R_{kl_{1}l_{2}l_{3}}^{(t)}:\) with \(k\) output feature channels. The radial function \(R_{kl_{1}l_{2}l_{3}}^{(t)}\) takes as an input radial distances \(r_{uv}\), which are embedded through Bessel functions and multiplied by a smooth polynomial cutoff function . Finally, we use irreducible Cartesian tensors \(_{1}(})\), similar to spherical tensors \(^{l}(})\), to embed unit vectors into the tensor space of maximal rank \(l_{}\).

**Equivariant convolutions and two-body features.** Rotation equivariance in MPNNs is typically achieved by constraining convolution filters to be the products between learnable radial functions and spherical tensors, i.e., \(R_{kl_{1}l_{2}l_{3}}^{(t)}(r_{uv})Y_{m_{1}}^{l_{1}}(}_{uv})\). The two-body features \(A_{ukl_{3}m_{3}}^{(t)}\) are further obtained through the tensor product--the point-wise convolution --between the respective filters and neighbors' equivariant features \(h_{ukl_{2}m_{2}}^{(t)}\). The permutational invariance is enforced by pooling over the neighbors \(v(u)\). Here, we use irreducible Cartesian tensors, with the rotation-equivariant filters given by \(R_{kl_{1}l_{2}l_{3}}^{(t)}(r_{uv})_{l_{1}}(}_{ uv})_{i_{1}i_{2} i_{l_{3}}}\). Thus, two-body features \(_{ukl_{3}}^{(t)}_{i_{1}i_{2} i_{l_{3}}}\) are obtained using the irreducible Cartesian tensor product and are represented by rank-\(l_{3}\) irreducible Cartesian tensors. The Cartesian two-body features are defined by

\[_{ukl_{3}}^{(t)}_{i_{1}i_{2} i_{l_{3}}}= _{v(u)}R_{kl_{1}l_{2}l_{3}}^{(t)}(r_{uv}) _{l_{1}}(}_{uv})_{}}}_{k^{}}W_{kk^{}l_{2}}^{(t)}_{ vk^{}l_{2}}^{(t)}_{i_{1}i_{2} i_{l_{3}}},\] (4)

where \(d_{t}\) represents the number of feature channels in the node embeddings \(_{vk^{}l_{2}}^{(t)}\) of the \(t\)-th message-passing layer. In the first message-passing layer, node embeddings are initialized as learnable weights \(W_{kZ_{v}}\) that are invariant to actions of the orthogonal group, i.e., are scalars or tensors of rank \(l_{2}=0\), and embed the atom type \(Z_{v}\). Thus, constructing equivariant two-body features simplifies to

\[_{ukl_{1}}^{(1)}_{i_{1}i_{2} i_{l_{1}}}= _{v(u)}R_{kl_{1}}^{(1)}(r_{uv})_{l_{1} }(}_{uv})_{i_{1}i_{2} i_{l_{1}}}W_{kZ_{v}}.\] (5)

**Equivariant many-body features.** The importance of many-body terms arises from the fact that the interaction between atoms changes when additional atoms are present; see Appendix A. Furthermore, many-body terms are often required to ensure the generalization of interatomic potentials, i.e., their ability to accurately predict energies and forces for temperatures and stoichiometries on which they were not trained . Here, we construct \((+1)\)-body equivariant features from \(_{ukl_{}}^{(t)}_{i_{1}i_{2} i_{l_{}}}\) obtained using Eqs. (4) or (5). The \(\)-fold Cartesian tensor product, which yields \((+1)\)-body features represented by an irreducible Cartesian tensor of rank \(L\), reads

\[_{u_{},kL}^{(t)}_{i_{1}i_{2} i_{L}}= }_{ukl_{1}}^{(t)}_{} _{}}_{ukl_{}}^{(t)}}_{} _{i_{1}i_{2} i_{L}},\] (6)

where \(_{}\) counts all possible \(\)-fold products of \(\{l_{1},,l_{}\}\)-rank tensors, yielding rank-\(L\) irreducible Cartesian tensors, and \(}_{ukl_{}}^{(t)}=}}_{k^{}}W_{ kk^{}l_{}}^{(t)}_{uk^{}l_{}}^{(t)}\) with \(d_{t}\) feature channels.

The irreducible Cartesian tensor product does not allow pre-computing the coefficients of the \(\)-fold tensor product, differing from spherical tensors that use the generalized Clebsch-Gordan coefficients, contracted with weights from Eq. (A1) and summed over the possible paths \((_{})\), for this purpose . Thus, we obtain the result of Eq. (6) by iteratively applying the irreducible Cartesian tensor product \((-1)\) times and refer to the respective models as irreducible Cartesian tensor potentials (ICTPs) with the full product basis or ICTPfull. However, two-fold tensor products in Eq. (6) are symmetric to permutations of involved tensors. Thus, the number of the \(\)-fold tensor products, \((_{})\), can be significantly reduced; we refer to the corresponding models as ICTPsym. Furthermore, we can reduce the computational cost of the Cartesian product basis by performing the calculations in a latent feature space. We use learnable weights \(W_{kk^{}l_{}}\) to reduce the number of feature channels for the product basis calculation and then increase it again for subsequent steps; we refer to the corresponding models as ICTPlt. For more details on the model architecture, such as the construction of updated many-body node embeddings, readout functions, and different options for the Cartesian product basis, see Appendix B.2.

**Runtime considerations.** When choosing an architecture to implement MLIPs, the runtime per energy and force evaluation is crucial. The computational complexity as a function of rank \(L\) is \((9^{L}L!/(2^{L/2}(L/2)!))\) for the irreducible Cartesian tensor product and \((L^{5})\) for the Clebsch-Gordan one; see Appendix B.3 for more details. Thus, equivariant convolutions based on spherical tensors are more computationally efficient for \(L\) than those based on irreducible Cartesian tensors. However, state-of-the-art models and physical properties typically require \(L 4\)[33; 72], making sub-leading terms and implementation-dependent amplitudes crucial. Based on our analysis, for \(L 4\), we can expect equivariant convolutions based on irreducible Cartesian tensors to be more computationally and memory efficient than their spherical counterparts. For \(L 4\), the \(\)-fold tensor product in the Cartesian basis can also offer computational advantages. Its cost, as a function of rank \(L\) and correlation order \(\), is \((9^{L}L!/(2^{L/2}(L/2)!))^{-1}\) and \(L^{(+3)}\) for ICTP and MACE, respectively. The pre-factor \(\), which counts all possible \(\)-fold tensor products, is removed in MACE through generalized Clebsch-Gordan coefficients, though these coefficients increase the memory spherical models use. Therefore, it is essential to consider the inference times for a fair comparison, which we provide in Section 5.

**Equivariance and tracels property of message-passing layers.** We conclude this section by giving a theoretical result that ensures the equivariance of message-passing layers based on irreducible Cartesian tensors and their irreducible tensor products to actions of the orthogonal group. The proof is provided in Appendix C. We also prove in Appendix D that these message-passing layers preserve the traceless property of irreducible Cartesian tensors.

**Proposition 4.1**.: _The message-passing layers based on irreducible Cartesian tensors and their irreducible tensor products are equivariant to actions of the orthogonal group._

**Proposition 4.2**.: _The message-passing layers based on irreducible Cartesian tensors and their irreducible tensor products preserve the traceless property of irreducible Cartesian tensors._

## 5 Experiments and results

This section presents the results for the five benchmark data sets: rMD17, MD22, 3BPA, acetylacetone, and Ta-V-Cr-W. We describe data sets and training details in Appendices E.1 and E.2, respectively.

**Scaling and computational cost**. The expressive power and computational efficiency of equivariant many-body message-passing potentials depend on the tensor ranks employed in equivariant message passing and embedding local atomic environments, i.e., \(L\) and \(l_{}\), respectively, as well as the correlation order \(\). Recent work has shown that for identifying environments with \(L\)-fold symmetries, at least rank-\(L\) tensors are required . These symmetries are typically lifted in atomistic simulations, motivating the use of \(L 4\). Higher body-order correlations \(\), in turn, are required if atomic environments are degenerate to a lower body-order correlation \(-1\)[73; 74]. Figure 2 demonstrates inference times and memory consumption of models based on irreducible Cartesian and spherical tensors, i.e., ICTP and MACE, respectively, as a function of the tensor rank and the correlation order. Table A1 presents the corresponding numerical results. We find that irreducible Cartesian tensors

Figure 2: **Inference times and memory consumption as a function of the tensor rank \(L\) (a)–(b) and the correlation order \(\) (c)–(d).** All results are obtained for the 3BPA data set and \(l_{}=L\). We used eight feature channels to allow experiments with larger \(\) values. MACE models use intermediate tensors with \(l>l_{}\) for their product basis, which we fixed to \(l=l_{}\). Otherwise, pre-computing generalized Clebsch–Gordan coefficients for \(>4\) would require more than 2 TB of RAM. For ICTP, we used the full product basis to compute the same number of \(\)-fold tensor products as in MACE.

outperform spherical ones for most parameter values. In particular, irreducible Cartesian tensors allow spanning the \(\)-space more efficiently, in line with our theoretical results in Section 4.2.

**Molecular dynamics trajectories.** We assess the performance of ICTP models using the revised MD17 (rMD17) data set, which includes structures, total energies, and atomic forces for ten small organic molecules obtained from ab initio molecular dynamics simulations . Table 1 shows that ICTP\({}_{}\) achieves accuracy on par with state-of-the-art spherical and Cartesian models. Notably, several methods exhibit similar accuracy when trained with 950 configurations. However, the achieved accuracy is much lower than the desired accuracy of \(43.37\) meV \( 1\) kcal/mol, making a model comparison less meaningful. Therefore, we also compare ICTP\({}_{}\) with MACE and NequIP, trained using 50 configurations, making learning accurate MLIPs more challenging. From Table 1, we see that ICTP\({}_{}\) outperforms MACE and NequIP for most molecules in this scenario.

We further evaluate the performance of ICTP using the MD22 data set, which contains seven molecular systems with 42-370 atoms . This data set spans four major classes of biomolecules and supramolecules and was designed to challenge short-range models. Table 2 shows that ICTP achieves an accuracy on par with or better than state-of-the-art models, including long-range ones.

**Extrapolation to out-of-domain data.** We continue to assess the performance of ICTP models using the 3BPA data set . The training data set comprises 500 configurations, total energies, and atomic forces acquired from molecular dynamics at 300 K. The test data set is obtained from simulations at 300 K, 600 K, and 1200 K. We also test our models using energies and forces along dihedral rotations of the molecule. Table 2 shows that ICTP models trained using 450 configurations perform on par with state-of-the-art spherical models, similar to the results for rMD17. However, we were not able to reproduce the original results using the current MACE source code and the described training setup . Therefore, for a fair comparison, we unified the training setup for ICTP and MACE (see Appendix E.2) and Table 2 reports the corresponding results for 450 training configurations. In Table 3, we present the results obtained for ICTP and MACE trained using 50 configurations.

From Table 2, we observe that ICTP\({}_{}\) slightly outperforms MACE in total energy and atomic force RMSEs but is \( 1.4\) times less computationally efficient. This difference arises from MACE using the generalized Clebsch-Gordan coefficients and pre-computing their product with the weights in the linear expansion in Eq. (A1) , which reduces the effective number of evaluated tensor products. Thus, we may attribute the lower computational efficiency of ICTP\({}_{}\) to the use of the

    & & & \(N_{}=950\) & & & & \(N_{}=50\) & \\  & & ICTP\({}_{}\) & TensorNet  & MACE  & AllEGro  & NequIP  & NequIP  & MACE  & ICTP\({}_{}\) \\  Aspirin & E & \(\) & 2.4 & **2.2** & 2.3 & 2.3 & 19.5 & 17.0 & \(\) \\  & F & \(\) & \(8.9 0.1\) & **6.6** & 7.3 & 8.2 & 52.0 & 43.9 & \(\) \\  Axobenzene & E & \(1.20 0.01\) & **0.7** & 1.2 & 1.2 & **0.7** & 6.0 & **5.4** & \(\) \\  & F & \(2.92 0.03\) & 3.1 & 3.0 & **2.6** & 2.9 & 20.0 & **17.7** & \(\) \\  Benzene & E & \(0.26 0.00\) & **0.02** & 0.4 & 0.3 & 0.04 & 0.6 & 0.7 & \(\) \\  & F & \(0.34 0.02\) & 0.3 & 0.3 & **0.2** & 0.3 & 2.9 & 2.7 & \(\) \\  Ethanol & E & \(\) & 0.5 & **0.4** & **0.4** & **0.4** & 8.7 & 6.7 & \(\) \\  & F & \(2.63 0.10\) & 3.5 & **2.1** & **2.1** & 2.8 & 40.2 & 32.6 & \(\) \\  Malonaldehyde & E & \(0.82 0.03\) & 0.8 & 0.8 & **0.6** & 0.8 & 12.7 & **10.0** & \(\) \\  & F & \(4.96 0.21\) & 5.4 & 4.1 & **3.6** & 5.1 & 52.5 & **43.3** & \(\) \\  Naphthalene & E & \(0.56 0.00\) & **0.2** & 0.5 & **0.2** & 0.9 & **2.1** & **2.1** & \(\) \\  & F & \(1.45 0.05\) & 1.6 & 1.6 & **0.9** & 1.3 & 10.0 & **9.2** & \(\) \\  Panacetamol & E & \(1.44 0.03\) & **1.3** & 1.5 & 1.4 & 14.3 & 9.7 & \(\) \\  & F & \(\) & \(5.9 0.1\) & **4.8** & 4.9 & 5.9 & 39.7 & \(\) & \(\) \\  Salicylic acid & E & \(0.97 0.01\) & 0.8 & 0.9 & 0.9 & **0.7** & 8.0 & 6.5 & \(\) \\  & F & \(3.66 0.06\) & \(4.6 0.1\) & 3.1 & **2.9** & 4.0 & 35.9 & \(\) & \(\) \\  Toluene & E & \(0.46 0.00\) & **0.3** & 0.5 & 0.4 & **0.3** & 3.3 & 3.1 & \(\) \\  & F & \(1.61 0.02\) & 1.7 & **1.5** & 1.8 & 1.6 & 15.1 & 12.1 & \(\) \\  Uracil & E & \(0.57 0.01\) & **0.4** & 0.5 & 0.6 & **0.4** & 7.3 & **4.4** & \(4.66 0.16\) \\  & F & \(2.64 0.08\) & 3.1 & 2.1 & **1.8** & 3.1 & 40.1 & **25.9** & \(\) \\   

Table 1: **Energy (E) and force (F) mean absolute errors (MAEs) for the rMD17 data set. E- and F-MAE are given in meV and meV/Å, respectively. Results are shown for models trained using \(N_{}=\{950,50\}\) configurations randomly drawn from the data set, with further 50 used for early stopping. All values are obtained by averaging over five independent runs, with the standard deviation provided if available. Best performances, considering the standard deviation, are highlighted in bold.**MACE architecture, which results in a larger pre-factor \(\) for Cartesian models but facilitates a fair comparison between irreducible Cartesian and spherical tensors. Using the symmetric Cartesian product basis and that in the latent space, for example, we further improve the runtime of our models while maintaining accuracy on par with MACE.

Table A4 presents additional results obtained for \(=1\), i.e., focusing on models that rely exclusively on two-body interactions. We observe that Cartesian models exhibit shorter inference times than spherical ones, with MACE and ICTP\({}_{}\) achieving \(2.96 0.06\) ms and \(2.63 0.02\) ms, respectively. Regarding memory consumption, MACE and ICTP perform similarly despite the larger number of tensor products required for the Cartesian product basis in Eq. (6). This observation can be attributed to, for example, the Clebsch-Gordan tensor product requiring the computation of intermediate tensors with \((2L+1)^{2}\) elements, whereas irreducible Cartesian tensors contain \(3^{L}\) elements.

Figure 3 compares potential energy profiles obtained with ICTP and MACE trained using 50 configurations. For the potential energy cut at \(=120^{}\) (left panel), ICTP and MACE perform similarly, except for the energy barrier at \( 143^{}\), which ICTP tends to underestimate stronger than MACE.

    & & ICTP\({}_{}\) & ICTP\({}_{}\) & ICTP\({}_{+1}\) & MACE\({}^{}\) & CACE  & MACE  & NequIP  \\   & E & **2.70 \(\) 0.22** & **2.70 \(\) 0.08** & **2.98 \(\) 0.34** & **2.81 \(\) 0.18** & 6.3 & **3.0 \(\) 0.2** & 3.28 \(\) 0.10 \\  & F & **9.45 \(\) 0.29** & **9.39 \(\) 0.31** & **9.57 \(\) 0.20** & **9.47 \(\) 0.42** & 21.4 & **8.8 \(\) 0.3** & 10.77 \(\) 0.19 \\   & E & **10.74 \(\) 0.31** & **10.38 \(\) 0.80** & **10.29 \(\) 0.90** & **11.11 \(\) 1.41** & 18.0 & **9.7 \(\) 0.5** & 11.16 \(\) 0.14 \\  & F & **22.99 \(\) 0.64** & **22.87 \(\) 0.91** & **23.03 \(\) 0.76** & **23.27 \(\) 1.45** & 45.2 & **21.8 \(\) 0.6** & 26.37 \(\) 0.09 \\   & E & **29.80 \(\) 0.92** & **30.84 \(\) 1.87** & **31.32 \(\) 1.80** & **31.15 \(\) 1.58** & 58.0 & **29.8 \(\) 1.0** & 38.52 \(\) 1.63 \\  & F & **62.82 \(\) 1.23** & **64.54 \(\) 3.88** & **65.36 \(\) 3.47** & **62.32 \(\) 3.52** & 113.3 & **62.0 \(\) 0.7** & 76.18 \(\) 1.11 \\   & E & **9.82 \(\) 0.79** & 10.64 \(\) 1.07 & 13.03 \(\) 3.44 & **8.56 \(\) 1.53** & – & **7.8 \(\) 0.6** & 23.23  \\  & F & **17.52 \(\) 0.54** & **17.18 \(\) 0.81** & 19.31 \(\) 0.83 & **17.69 \(\) 1.29** & – & **16.5 \(\) 1.7** & 23.1  \\   & & 6.45 \(\) 0.50 & 5.31 \(\) 0.02 & **3.51 \(\) 0.22** & 4.66 \(\) 0.05 & – & **24.3\({}^{}\)** & 103.5\({}^{}\) \\   & Memory consumption & 49.66 \(\) 0.00 & 42.01 \(\) 0.11 & 39.08 \(\) 0.00 & **36.26 \(\) 0.00** & – & – & – \\    \({}^{a}\) During inference time measurements with the MACE source code, we were not able to reproduce the original results . Thus, we re-run MACE experiments using a training setup similar to that of ICTP; see Appendix E.2. \({}^{b}\) The original publication did not report the batch size used to measure inference time . Therefore, the values provided are used solely to demonstrate the relative computational cost of MACE and NequIP.

Table 2: **Energy (E) and force (F) root-mean-square errors (RMSEs) for the 3BPA data set.** E- and F-RMSE are given in meV and meV/Å, respectively. Results are shown for models trained using 450 configurations randomly drawn from the training data set collected at 300 K, with further 50 used for early stopping. All ICTP results are obtained by averaging over five independent runs. For MACE and NequIP, the results are reported for three runs. The standard deviation is provided if it is available. Best performances, considering the standard deviation, are highlighted in bold. Inference time and memory consumption are measured for a batch size of 100. Inference time is reported per structure in ms, while memory consumption is provided for the entire batch in GB.

Figure 3: **Potential energy profiles for three cuts through the 3BPA molecule’s potential energy surface.** All models are trained using 50 configurations, and additional 50 are used for early stopping. The 3BPA molecule, including the three dihedral angles (\(\), \(\), and \(\)), provided in degrees \({}^{}\), is shown as an inset. The color code of the inset molecule is C grey, O red, N blue, and H white. The reference potential energy profile (DFT) is shown in black. Each profile is shifted such that each model’s lowest energy is zero. Shaded areas denote standard deviations across five independent runs.

For \(=150^{}\) (middle panel), however, ICTP\({}_{}\) and ICTP\({}_{}\) outperform MACE across nearly the entire range of the dihedral angle \(\). For \(=180^{}\) (right panel), all models perform similarly. Figure A1 shows the corresponding potential energy profiles for models trained with 450 configurations. All models perform similarly in this scenario, with energy profiles close to the reference (DFT).

**Flexibility and reactivity.** We further use the acetylacetone data set to assess the ICTP models' extrapolation capabilities to higher temperatures (similar to 3BPA), bond breaking, and bond torsions . Table 3 shows that ICTP models achieve state-of-the-art results while employing fewer parameters than spherical counterparts. Appendix E.3 includes additional results for the acetylacetone data set, such as total energy and atomic force RMSEs for models trained with 50 configurations and details on the potential energy profiles for hydrogen transfer and C-C bond rotation. Overall, ICTP and MACE perform similarly, demonstrating excellent generalization capability. However, when trained using 50 configurations, ICTP\({}_{}\) is the only MLIP consistently producing the potential energy profile for hydrogen transfer close to the reference (DFT).

**Multicomponent alloys.** We finally evaluate the ICTP and MACE models using the Ta-V-Cr-W data set, designed to assess the performance of state-of-the-art MLIPs in modeling chemically complex multicomponent systems. In this evaluation, we attempt to predict energies and forces for Ta-V-Cr-W subsystems under two scenarios: The 0 K energies and forces in binary, ternary, and quaternary systems and near-melting temperature energies and forces in 4-component disordered alloys. Table A6 shows that ICTP\({}_{}\) outperforms MACE in nearly all subsystems, particularly in energy prediction. ICTP achieves an overall accuracy of 1.38 \(\) 0.09 meV/atom for energies and 0.028 \(\) 0.001 eV/A for forces, compared to 2.19 \(\) 0.31 meV/atom and 0.029 \(\) 0.001 eV/A by MACE. However, the \(\) pre-factor from the \(\)-fold tensor product results in longer inference times for ICTP than MACE, in line with the discussion for 3BPA.

## 6 Conclusions and limitations

This work introduces many-body equivariant MPNNs based on higher-rank irreducible Cartesian tensors, offering an alternative to spherical models and addressing the limitations of state-of-the-art Cartesian models. We assess the performance of resulting MPNNs using five benchmark data sets, such as rMD17, MD22, 3BPA, acetylacetone, and Ta-V-Cr-W. In these experiments, MPNNs based on irreducible Cartesian tensors show a lower computational cost of individual operations compared to spherical counterparts. Furthermore, we demonstrate that these Cartesian models achieve accuracy and generalization capability on par with or better than state-of-the-art spherical models while memory consumption is comparable. Our results hold across the typical range of tensor ranks used in modeling many-body interactions and relevant physical properties, i.e., \(L 4\).

**Limitations.** We emphasize our focus on introducing MPNNs based on irreducible Cartesian tensors and prove their equivariance and traceless property. We adapted the MACE architecture, which uses only even tensor products, to enable a fair comparison with state-of-the-art spherical models. Further modifications to the architecture are possible and necessary, e.g., to reduce the pre-factor arising from the Cartesian product basis, before we can fully exploit the potential of irreducible Cartesian tensors.

    & & ICTP\({}_{}\) & ICTP\({}_{}\) & ICTP\({}_{+}\) & MACE\({}^{}\) & MACE  & NequIP  \\   & E & **0.75 \(\) 0.04** & **0.76 \(\) 0.03** & **0.77 \(\) 0.04** & **0.75 \(\) 0.05** & 0.9 \(\) 0.03 & **0.81 \(\) 0.05** \\  & F & **5.08 \(\) 0.11** & **5.17 \(\) 0.10** & **5.18 \(\) 0.16** & **5.00 \(\) 0.17** & **5.1 \(\) 0.1** & 5.90 \(\) 0.46 \\   & E & **5.39 \(\) 1.22** & **4.43 \(\) 0.34** & **5.12 \(\) 0.29** & **4.96 \(\) 0.64** & **4.6 \(\) 0.3** & 6.04 \(\) 1.54 \\  & F & **23.21 \(\) 1.96** & **22.90 \(\) 1.62** & **24.05 \(\) 1.71** & **23.25 \(\) 1.82** & **22.4 \(\) 0.9** & 27.80 \(\) 4.03 \\   &  &  &  &  &  &  \\   

* Similar to Table 2, we re-run MACE experiments using the similar training setup as for ICTP; see Appendix E.2.

Table 3: **Energy (E) and force (F) root-mean-square errors (RMSEs) for the acetylacetone data set. E- and F-RMSE are given in meV and meV/A, respectively. Results are shown for models trained using 450 configurations randomly drawn from the training data set collected at 300 K, with further 50 used for early stopping. All ICTP results are obtained by averaging over five independent runs. For MACE and NequIP, the results are reported for three runs. The standard deviation is provided if it is available. Best performances, considering the standard deviation, are highlighted in bold.**

## Data availability

All data sets used in this study are publicly available: rMD17 (https://doi.org/10.6084/m9.figshare.12672038.v3), MD22 (http://www.sgdml.org), 3BPA (https://github.com/davkovacs/BOTNet-datasets), acetylacetone (https://github.com/davkovacs/BOTNet-datasets), and Ta-V-Cr-W (https://doi.org/10.18419/darus-3516).

## Code availability

The source code is available on GitHub and can be accessed via this link: https://github.com/nec-research/ictp.