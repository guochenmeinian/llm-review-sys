# On the Powerfulness of Textual Outlier Exposure

for Visual OoD Detection

Sangha Park\({}^{1}\), Jisoo Mok\({}^{1}\), Dahuin Jung\({}^{1}\), Saehyung Lee\({}^{1}\), Sungroh Yoon\({}^{1,2,*}\)

\({}^{1}\)Department of Electrical and Computer Engineering, Seoul National University

\({}^{2}\)Interdisciplinary Program in Artificial Intelligence, Seoul National University

{wiarae, magicshop1118, annajung0625, halo8218, sryoon}@snu.ac.kr

###### Abstract

Successful detection of Out-of-Distribution (OoD) data is becoming increasingly important to ensure safe deployment of neural networks. One of the main challenges in OoD detection is that neural networks output overconfident predictions on OoD data, make it difficult to determine OoD-ness of data solely based on their predictions. Outlier exposure addresses this issue by introducing an additional loss that encourages low-confidence predictions on OoD data during training. While outlier exposure has shown promising potential in improving OoD detection performance, all previous studies on outlier exposure have been limited to utilizing visual outliers. Drawing inspiration from the recent advancements in vision-language pre-training, this paper venture out to the uncharted territory of textual outlier exposure. First, we uncover the benefits of using textual outliers by replacing real or virtual outliers in the image-domain with textual equivalents. Then, we propose various ways of generating preferable textual outliers. Our extensive experiments demonstrate that generated textual outliers achieve competitive performance on large-scale OoD and hard OoD benchmarks. Furthermore, we conduct empirical analyses of textual outliers to provide primary criteria for designing advantageous textual outliers: near-distribution, descriptiveness, and inclusion of visual semantics. Code is available at https://github.com/wiarae/TOE

## 1 Introduction

The standard assumption when deploying neural networks is that both the train and test data will fall under the same distribution, in which case, test data are considered to be in-distribution (ID). However, in the real world, they often encounter out-of-distribution (OoD) data, which refers to data that are distant from the training data distribution. Because the generalization capacity of neural networks does not extend outside the distribution of observed train data, OoD data can cause them to perform poorly or even fail . Therefore, it is important for neural networks to not only achieve high accuracy on ID data but also successfully detect OoD data to preclude them from the inference process. Unfortunately, neural networks yield highly over-confident predictions on OoD data, making OoD detection a non-trivial research problem [39; 43].

One promising approach to OoD detection is "outlier exposure," which modifies the training procedure of neural networks [23; 18], such that they can accurately classify ID data while also reliably detecting OoD data. To simultaneously accomplish the two training objectives, outlier exposure utilizes proxy data that can emulate actual OoD data that the neural network may confront at test time. Outlier exposure then defines a regularization term that encourages low-confidence predictions on those proxy data. Finally, the neural network is optimized to minimize the regularization term in additionto the supervised loss. The major challenge in outlier exposure is providing proxy data for training without explicit knowledge of OoD data.

Previously when neural networks could only process single-modal data, outlier exposure could only rely on visual outliers from the image domain. The recent emergence of multi-modal neural networks [37; 21] has opened new research opportunities. In this work, we study the powerfulness of textual outlier exposure by utilizing multi-modal neural networks. We first conduct preliminary studies of textual outlier exposure by replacing visual outliers with their textual counterparts in two widely-studied outlier exposure methods: real and virtual outliers. Our empirical results reveal that visual outliers suffer from high performance variation according to the choice of an auxiliary dataset and non-negligible time consumption. Textual outliers, on the contrary, do not experience the above problems, demonstrating that they can serve as attractive proxy data for outlier exposure.

Textual outliers can be designed in various forms, from single words to detailed descriptions. Initially, it may be unclear which type of textual outlier is most effective for facilitating textual outlier exposure. To address this ambiguity, we explore textual outliers at three verbosity levels: word, description, and caption. These textual outliers are generated using large-language models (LLMs) such as GPT-3, BERT, or vision-language models (VLMs) like BLIP-2. Among generated outputs, only those that can be considered OoD are utilized for outlier exposure. Our textual outlier generation methods rely solely on images and class labels of ID data. As a result, utilizing generated textual outliers eliminates the need to collect and curate proxy data to perform outlier exposure.

Our comprehensive experimental results present compelling evidence that the proposed textual outlier approach outperforms existing methods for OoD detection. Specifically, compared to the method in Hendrycks et al.  that employs real auxiliary datasets of outliers, our caption-level textual outlier reduces the FPR95 from 73.80% to 58.21%, yielding a direct improvement of 15.59%p on the challenging ImageNet-1K benchmark . Additionally, our textual outlier approach surpasses advanced baselines in visual outlier exposure, demonstrating the advantages of the outlier synthesis approach in the textual space. Furthermore, through comparative analyses of visual versus textual outliers and different forms of textual outliers, we establish three key criteria for advantageous textual outliers: near-distribution, descriptiveness, and inclusion of visual semantics.

Our contributions are summarized as follows:

* This is the first work to investigate the potential of textual outlier exposure with multi-modal neural networks. We reveal that textual outlier exposure can outperform its visual counterparts and overcome inherent limitations associated with them.
* We propose to utilize LLMs or VLMs to generate diverse drafts of textual outliers at different verbosity levels: word, description, and caption. Generative models facilitate the convenient and cost-effective collection of potential texts suitable for formatting into textual outliers. The generated outputs are then refined into more effective forms for textual outlier exposure.
* Proposed textual outliers are validated in various OoD detection settings through comparison with visual outlier exposure baselines. In particular, description-level textual outlier outperforms a competitive baseline from visual outlier exposure by 38.60%p (AUROC) on a challenging detection scenario with semantically similar classes.

## 2 Problem Setup

**Background and Notations.** Let \(\) represent the input space, and \(=\{1,...,C\}\) denote the label space. In our framework, we consider two main datasets: the ID dataset, \(D_{}\), and the OoD dataset, \(D_{}\). The \(D_{}=\{(_{i},y_{i})\}_{i=1}^{n}\) is a set of every sample from ID and is defined over \(\). On the other hand, \(D_{}\) is defined solely over the input space \(\) and does not have any associated labels. During training, the neural network never observes the OoD dataset. In the inference process, the neural network should be able to reliably filter out OoD samples.

**Out-of-distribution detection** can be formulated as a binary classification problem. Given a classifier \(f:^{C}\) trained on samples in a subset of \(D_{}\), the objective is to design a binary function estimator,

\[g()=&S(,f)\\ &S(,f)<\] (1)that classifies whether a sample \(\) belongs to \(D_{}\) or not. This is determined by the score function \(S(,f)\), which is defined based on the classifier's output. The threshold parameter \(\) is typically chosen to ensure a high percentage (e.g., 95%) of correctly classified in-distribution samples.

**Evaluation Metrics.** The OoD detection performance can be assessed by using the following two evaluation metrics: (1) the false positive rate of OoD samples at a fixed true positive rate of 95% (FPR95) for ID samples, with lower values indicating better performance, and (2) the area under the receiver operating characteristic curve (AUROC), with higher values indicating better performance. Additionally, we report the accuracy of the ID classification.

**Framework Overview.** CLIP  is a large-scale vision-language model that has been trained on a vast dataset consisting of 400 million image-text pairs, collected from the internet. Its remarkable performance has been demonstrated across various tasks, leading to numerous studies exploring its transferable feature representation [59; 13]. Several previous works have shown improvements in diverse tasks by operating in the CLIP space with the frozen CLIP encoder [42; 14; 15; 9; 35; 1]. Likewise, we leverage the strength of the CLIP encoder to build a reliable classifier by training a linear classifier on top of it. The CLIP embedding space that jointly models images and texts enables usage of textual cues for visual OoD detection. Therefore, we employ textual, instead of visual, outliers to regularize the classifier; train data for the supervised loss and test data, both ID and OoD, remain defined in the image domain. The general framework for textual outlier exposure is illustrated in Figure 1.

## 3 Understanding the Potential of Textual Outlier

Prior to exploring various forms of textual outliers, we first show that even naive approaches to textual outlier exposure can overcome the inherent limitations of its visual counterpart. We reveal the advantages of using textual outliers over images by replacing images with texts in two representative types of outlier exposure: (1) the use of real auxiliary datasets and (2) virtual outlier synthesis in the feature space. The empirical results demonstrate that textual outliers can achieve superior OoD detection performance, as well as significantly reducing the variance of performance and time consumption in (1) and (2). In this section, all of the experiments are conducted following the procedure in Figure 1.

### Auxiliary datasets

**Experimental setting.** Hendrycks _et al._ proposed to utilize real auxiliary datasets as proxy data for outlier exposure to improve the detection performance. We leverage four auxiliary datasets to evaluate visual and textual outliers (Table 1). To use texts instead of images, we adopt class labels from an auxiliary dataset as outliers, _e.g._, "a photo of _bamboo forest_" when SUN  is used as an

Figure 1: Framework overview. (a) During the training phase, we utilize textual outliers embeddings as proxies for test OoD data. For the ID image data, we train the model using cross-entropy loss, while for the textual outlier data, we employ outlier exposure loss, which enforce the model to output the low-confidence predictions on them. To reduce the modality gap between the two domains, we inject noise to textual outlier. (b) During the testing phase, the trained classifier filters out OoO data by thresholding output score.

auxiliary dataset. To mitigate the imbalance in size between image and text datasets, we, following the experimental setting in Zhang _et al._, train the model for 300 epochs with textual outliers and 25 epochs with visual outliers. We use ImageNet20 as ID data and ImageNet10 as OoD data for evaluation as proposed in Ming _et al._.

**Results.** Table 2 presents a comparison of the detection performance and classification accuracy between visual and textual outliers. Notably, textual outliers demonstrate superior performance across all evaluation metrics. Furthermore, the detection performance of models trained with visual outliers is highly contingent on the choice of an auxiliary dataset, leading to significant performance variation. This poses a considerable challenge in determining the optimal dataset for effective outlier exposure. In contrast, the use of textual outliers noticeably reduces performance variation. Additional results on other benchmark datasets can be found in the Appendix C.1.

### Synthesis in feature space

**Experimental setting.** As an alternative to using auxiliary datasets, virtual outlier synthesis in the feature space, which does not rely on real datasets, has been proposed [8; 46]. The feature space of a classifier can be represented as a Gaussian mixture model with the class mean and variance. It is reasonable to consider instances sampled from the low-likelihood regions of this distribution as outliers. To obtain means and variances per class using text instead of images, we adopt class labels, synonyms of class labels, and descriptions of classes obtained from GPT-3 . Examples of labels and associated synonyms and descriptions are provided in the Appendix C.2. We choose VOS  from a family of methods in outlier synthesis as the testbed to compare virtual outliers. For both textual and visual outliers, we utilize the CLIP embedding space to obtain class means and variances. We train the linear classifier with ImageNet10 and use ImageNet20 as OoD dataset for evaluation . The number of training epochs is set to 300 for both images and texts.

**Results.** Our findings, as shown in Table 3, indicate that utilizing class means and variances derived from texts for outlier synthesis outperforms those derived from images, while also demonstrating better sample efficiency. The majority of computational overhead in virtual outlier synthesis arises from computing the updated class mean and variance after each epoch to generate a new set of virtual outliers. Notably, when using text data, the training time is significantly reduced by a factor of 13 (18 minutes _vs._ approximately 4 hours), while still achieving comparable performance. When we reduce the amount of image data to match the synthesis and training time for textual outliers, the OoD detection performance experiences a considerable decline from 97.77 to 91.65 in terms of AUROC.

### Analysis

The superior performance of textual outliers can be attributed to their proximity to ID data. To analyze the relationship between ID data and different outliers, we utilize UMAP , a popular technique for dimension reduction. In Figure 2 (a), we visualize the latent space that con

    &  & Places & SUN & iNaturalist &  \\  Image & 5640 & 10000 & 10000 & 10000 & 10000 \\ Text & 47 & 50 & 50 & 110 \\   

Table 1: Number of images / texts (class labels) in auxiliary datasets.

    &  \\   & 18m & 30m & 3h 54m \\  Image & 91.65 & 96.59 & 97.77 \\
**Text** & **97.85** & - & - \\   

Table 3: AUROC achieved after varying amounts of time consumption for virtual outlier synthesis.

tains ID data and two types of real auxiliary datasets, Texture  and iNaturalist . They are the two auxiliary datasets whose textual outliers led to greatest performance improvements. Similarly, Figure 2 (b) shows the latent space representing three types of data: ID data, class-wise means of ID data in both image and text domains, and outliers sampled from each domain's distribution. Textual outliers, whether real or virtual, lie closer to ID data compared to their visual counterparts. This observation implies that textual outliers, which reside in the vicinity of ID data distribution, can offer informative signals about boundary regions [8; 33].

As a result, the usage of textual outliers contributes to improving OoD detection performance by assisting the neural network in modeling the ID-OoD boundary.

Based on these empirical findings, we conclude that exposing the neural network to textual outliers offers significant advantages, including robustness to performance variation and enhanced sample efficiency. In the subsequent sections, we delve into potential designs for textual outliers (Section 4) and analyze the factors that contribute to their effectiveness in outlier exposure (Section 5).

## 4 Method

Unlike visual outliers, textual outliers can take various forms, ranging from a single word to more verbose descriptions. We investigate three different designs for textual outliers: 1) words retrieved from ID images, 2) descriptions of ID class-labels, and 3) captions of ID images generated by an image captioning model. Figure 3 visualizes the generation process for each type of textual outlier. We utilize LLMs and a VLM to generate textual outliers and selectively employ the generated outputs that are OoD but still lie adjacent to the ID data. We highlight that no additional data except for ID data is used to generate textual outliers.

### Generating textual outliers in three types

#### 4.1.1 Word-level textual outliers

**(i) Motivation.** The most straightforward way of designing textual outliers is in the form of "a photo of {word}," which we coin word-level textual outliers. As discussed in Section 3.1, word-level textual outliers can generated by simply leveraging class labels or comparable terms from auxiliary datasets, but they may not always lie in the proximity of ID data. In Section 3.3, we have observed that textual outliers that are closer to the ID data tend to provide more informative signals. To obtain word-level textual outliers that are near the ID data, we utilize image-based text retrieval. Utilizing texts that are close to images enhances the classifier's ability to capture the ID-OoD boundary, thereby improving its performance in distinguishing between ID and OoD data.

**(ii) Generation.** We exploit the framework proposed by Esmaeilpour _et al._ to perform text retrieval from ID images. The retrieval process consists of the following steps. First, we use the CLIP image encoder to extract image embeddings of ID data. These embeddings are then treated as sequences of tokens and fed into the multi-head cross-attention head of BERT, which functions as a text decoder. Finally, the resulting text embeddings are linearly projected onto the text space. Figure 3 (a) provides a high-level overview of the retrieval process. Images from the validation set of the ID dataset, which does not overlap with the test set, are used as inputs.

**(iii) Filtering.** Initial versions of generated text outputs are not always guaranteed to be OoD. Therefore, we select a subset of retrieved words based on their cosine similarity to the ID image

  “A photo of a {}” \\ “A blurry photo of a {}” \\ “A photo of many {}” \\ “A photo of the large {}” \\ “A photo of the small {}” \\  

Table 4: Templates for prompt ensembling with textual outliers.

Figure 2: Feature space analysis of image and text outliers in (a) auxiliary datasets and (b) synthesis settings.

embeddings. Only those with top-\(k\) to \(k+\) similarity values are selected, and afterwards, class labels are explicitly removed. Through this filtering process, we can effectively keep texts that are OoD but near ID data. To prompt CLIP with selected word-level textual outliers, we employ prompt ensemble using the templates outlined in Table 4.

#### 4.1.2 Description-level textual outliers

**(i) Motivation.** We now consider generating textual outliers from class labels of ID data, instead of images. With LLMs, class labels can easily be expanded into illustrative descriptions that include linguistic semantics; we name these description-level textual outliers.

**(ii) Generation.** We utilize GPT-3, a powerful and widely-adopted LLM, to obtain class descriptions. GPT-3 is prompted with the phrase "describe what the {class name} looks like" .

**(iii) Filtering.** These class descriptions can be utilized as textual outliers without further refinement because they already are OoD from CLIP's viewpoint. For instance, when we prompt CLIP with a description of a Siamese cat (_e.g.,_ "a photo of _+ light-colored fur with dark points on the face, ears, legs, and tail_"), it retrieves images that noticeably deviate from the actual visual characteristics of a Siamese cat. An illustrative example demonstrating this phenomenon can be found in the Appendix E.1. Therefore, we employ the class descriptions obtained from GPT-3, after excluding the corresponding class labels, as textual outliers and prepend templates in Table 4 to each description.

#### 4.1.3 Caption-level textual outliers

**(i) Motivation.** To generate descriptive textual outliers that also contain visual semantics, we propose to reformulate captions into textual outliers. The so-called caption-level textual outliers, generated with visual cues, can capture the complex visual semantics in images and also contain rich linguistic semantics, encompassing the benefits of word- and description-level textual outliers.

**(ii) Generation.** To acquire a set of captions, denoted as \(=\{_{1},_{2},...,_{N}\}\), we utilize BLIP-2 , known for its exceptional descriptive capabilities . Images of the validation set of the ID data are used as inputs to BLIP-2. However, some of the captions that resemble the ID samples too closely are ineffective for outlier exposure.

**(iii) Filtering.** To filter out such ineffective captions, we utilize the Mahalanobis distance \(()\) as a measure of closeness to ID data. The Mahalanobis distance is computed as follows:

\[()=_{c}(-(h_{}()-_{c})^{}}^{-1}(h_{}()-_{c})),\] (2)

Figure 3: Illustration of (a) word-level, (b) description-level, and (c) caption-level textual outlier generation processes. A different generative model is used for each type of textual outlier. In (a), CLIP and BERT are used to perform image-based text retrieval to create word-level textual outliers. In (b) and (c), description-level and caption-level textual outliers are generated from GPT-3 and BLIP-2, respectively. For word- and caption-level textual outliers, the generated outliers are refined for the purpose of outlier exposure. See Section 4 for details.

where \(\) and \(}\) denote class-wise mean and variance of the training set. \(\) and \(}\) are derived using the following equations:

\[_{c}=}_{i:y_{i}=c}h_{}(_{i}), \ }=_{c}_{i:y_{i}=c}(h_{}( _{i})-_{c})(h_{}(_{i})-_{c })^{},\] (3)

where \(h_{\{\}}()\) refers to the CLIP image or text encoder. We only consider the top-\(p\)% of the data with the largest \(()\).

### Optimizing ID embeddings with textual outlier

We train our classifier using the loss as defined below:

\[(f)=_{(,y)_{ID}}[_{CE}(f(h_{}()),y)]+ _{_{out}^{OF}}[_{OE}(f(h_{ }())],\] (4)

where the left term is cross-entropy loss and the right term is Kullback-Leibler divergence to the uniform distribution, which can be expressed as \(-_{c}_{c}f()\). \(_{out}^{OF}\) refers to the generated textual outliers. Our loss aims to increase the classification performance on train data and reduce the confidence on \(_{out}^{OF}\). We add Gaussian noise to the textual outliers to reduce the modality gap .

### Test time OoD detection

When testing, we use Energy score  for OoD detection \(S(,f)=-E(;f)\), where

\[E(;f)=-T_{i:y_{i}=c}^{C}e^{f_{i}(x)/T}.\] (5)

Energy score is known to be less susceptible to the overconfident issue. It is worth mentioning that the sign of the energy function, denoted as \(E()\), is reversed to align with the convention where samples with higher scores are classified as ID, while samples with lower scores are classified as OoD. Our method can be used with any OoD score that utilizes output probabilities.

## 5 Experiments

In this section, we conduct extensive experiments to showcase the effectiveness of our novel technique for textual outlier exposure. Through comprehensive experiments, we evaluate and investigate our approach from three main perspectives: (1) comparison of our method with existing methods in outlier exposure, (2) comparison of performance among proposed textual outliers, and (3) assessment of the proposed textual outlier exposure in challenging OoD detection scenarios.

**Datasets.** We use the large-scale ImageNet-1K  OoD detection benchmark proposed by Huang _et al._. We conduct experiments on four OoD test datasets: subsets of iNaturalist , SUN , Places , Texture . The categories of OoD datasets are disjoint from those of the ID dataset.

**Experimental setting.** The CLIP model used in this paper is adapted from OpenAI's public repository, with ViT-B/16 serving as the default vision and language backbone. For word-level textual outliers, the BERT  large model with 24 layers and a hidden size of 1024 from huggingface  is used. The model is trained on the MS-COCO  dataset using the Adam optimizer  with a fixed learning rate of \(10^{-5}\) for 100 epochs. A simple teacher-forcing algorithm is employed to retrieve text during the training process. To generate description-level textual outliers, the GPT-3-text-davinci-002 model is used. For caption-level textual outliers, BLIP-2 opt 2.7b from huggingface is used. The Adam optimizer is used to train our linear classifier for OoD detection, and batch size, learning rate, and other hyperparameters are tuned on the validation set. We use 0.5 for \(\) in our training loss, and batch size for both ID and train time outlier is 32. The temperature value \(T\) for Energy score is set to 1, as per the original paper. Model checkpoints with the highest validation accuracy are evaluated on the test set. We employ a value of 30 for \(k\), which is used in word-level outliers filtering, and a value of 25 for \(\). Furthermore, a filtering ratio of 15 is utilized as the \(p\) for caption-level outlier analysis. The ablation study examining these parameters can be found in the Appendix B.

[MISSING_PAGE_FAIL:8]

### Textual outliers in hard OoD situation

**Description-level textual outliers outperforms in hard OoD.** Furthermore, we explore the effectiveness of textual outliers in handling challenging OoD inputs. In these scenarios, the OoD samples closely resemble the ID samples in terms of their semantic content. To evaluate the performance of OoD detection in such a semantically hard scenario, we first utilize the ImageNet10 _vs._ ImageNet20 benchmark proposed by Ming _et al._. This task involves distinguishing between high-resolution images with semantically similar categories, such as dog _vs._ wolf. Our experimental results, shown in Table 6 (first and second columns), demonstrate the superiority of our textual outliers over OE, achieving an 89.6%p improvement in FPR95 for ImageNet10 (ID) _vs._ ImageNet20 (OoD), and a 82.6%p improvement when ID and OoD data are switched.

Additionally, we conduct experiments on the CUB dataset  benchmark, which Vaze _et al._ proposed (third column in Table 6). This benchmark is designed to discriminate between ID and OoD samples that possess numerous shared attributes, thereby introducing difficulties in accurately identifying OoD instances. As shown in Table 6, our description-based textual outliers outperform the auxiliary dataset and synthesis methods. While descriptions are known to improve classification performance, they can also be effective outliers in situations where class names are not considered. The class labels for the hard OoD benchmarks are provided in the Appendix F.2.

## 6 Related work

**OoD detection.** A considerable body of research has been dedicated to designing scoring functions for detecting samples that lie outside the training categories. These methods include Maximum Softmax Probability , ODIN score , Mahalanobis-based score , Energy score , gradient-based score , and non-parametric KNN score . While there have been studies proposing OoD scores using CLIP's textual information [32; 12; 10], none of them have incorporated textual information into outlier exposure during training. An alternative approach to address the out-of-distribution (OoD) detection problem is through training-time regularization, as discussed in previous research [23; 18]. These methods encourage models to provide predictions with lower confidence or higher energies, aiming to improve OoD detection. However, most of these approaches rely on the availability of an auxiliary outlier dataset. To overcome this limitation, recent studies have explored the synthesis of virtual outliers in the feature space, as seen in works such as [8; 46]. This approach aims to generate synthetic outliers without the need for an additional dataset. Our proposed textual outliers differ from previous methods in that they are defined in the input space and do not rely on an auxiliary dataset.

**Multi-modal models.** The adoption of pre-trained VLMs in multi-modal tasks has attracted considerable interest and yielded impressive results. Prominent among these approaches are dual-stream models such as CLIP , ALIGN , and FILIP . These models utilize distinct encoders for text and image data and optimize them through contrastive objectives to align semantically similar features across heterogeneous modalities. The success of CLIP-like models inspired numerous subsequent investigations [26; 56; 59], aiming to enhance data efficiency and adaptability for various downstream tasks. Recent progress in this field, exemplified by innovations such as DALL-E 2 , ClipCap , and related studies [5; 13], has pushed the boundaries of exploration in multi-modal contrastive representation spaces, uncovering their untapped potential. To the best of our knowledge, this is the first time that joint embedding has been utilized for outlier exposure.

    & ID &  ImageNet10 \\ ImageNet20 \\  } &  &  \\   & & &  FPR \\  &  AUROC \\  &  FPR \\  &  AUROC \\  &  FPR \\  & 
 AUROC \\  \\   & OE & 93.20 & 54.51 & 89.80 & 57.63 & 87.20 & 60.70 \\  & VOS & 91.00 & 60.42 & 93.50 & 44.38 & 94.79 & 49.25 \\   & Word & 4.60 & 98.92 & 9.00 & 98.39 & 80.34 & 71.62 \\  & **Desc.** & **3.60** & **99.02** & **7.20** & **98.69** & **82.56** & **72.33** \\   & Caption & 5.30 & 98.85 & 8.60 & 98.43 & 85.92 & 64.79 \\   

Table 6: Effectiveness of textual outliers exposure in hard OoD situation, a more challenging OoD detection scenario. Proposed textual outliers are compared against strong baseline approaches on three different ID/OOD dataset combinations. The best result in each column is in bold.

Conclusion

In this paper, we propose a novel outlier framework, transitioning from the traditional single-modal paradigm to a multi-modal regime. Unlike existing methods that rely on image data, we leverage textual outliers during the training process. We analyze the advantages of textual outliers compared to visual outliers and propose three levels of textual outliers. Our proposed textual outliers significantly enhance the neural network's capability to distinguish between ID and OoD data, leading to superior OoD detection performance while maintaining the classification performance on ID data. Furthermore, our analysis of each level of outliers provides valuable insights into the characteristics of effective textual outliers. We anticipate that our work will serve as a source of inspiration for future research on OoD detection methods that leverage textual outlier exposure. **Limitations.** Our proposed textual outlier method involves a heuristic selection process to refine outputs of generative models. In future work, it is necessary to reduce the dependence on heuristic methods and explore alternative approaches.