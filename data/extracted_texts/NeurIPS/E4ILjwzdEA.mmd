# Length Optimization in Conformal Prediction

Shayan Kiyani, George Pappas, Hamed Hassani

Department of Electrical and Systems Engineering

University of Pennsylvania

{shayank, pappasg, hassani}@seas.upenn.edu

###### Abstract

Conditional validity and length efficiency are two crucial aspects of conformal prediction (CP). Conditional validity ensures accurate uncertainty quantification for data subpopulations, while proper length efficiency ensures that the prediction sets remain informative. Despite significant efforts to address each of these issues individually, a principled framework that reconciles these two objectives has been missing in the CP literature. In this paper, we develop Conformal Prediction with Length-Optimization (CPL) - a novel and practical framework that constructs prediction sets with (near-) optimal length while ensuring conditional validity under various classes of covariate shifts, including the key cases of marginal and group-conditional coverage. In the infinite sample regime, we provide strong duality results which indicate that CPL achieves conditional validity and length optimality. In the finite sample regime, we show that CPL constructs conditionally valid prediction sets. Our extensive empirical evaluations demonstrate the superior prediction set size performance of CPL compared to state-of-the-art methods across diverse real-world and synthetic datasets in classification, regression, and large language model-based multiple choice question answering. An Implementation of our algorithm can be accessed at the following link: https://github.com/shayankiyani98/CP.

## 1 Introduction

Consider a distribution \(\) over the domain \(\), where \(\) is the covariate space and \(\) is the label space. Using a set of calibration samples \((X_{1},Y_{1}),,(X_{n},Y_{n})\) drawn i.i.d. from \(\), the objective of conformal prediction (CP) is to create a prediction set \(C(x)\), for each input \(x\), that is likely to include the true label \(y\). This is formalized through specific coverage guarantees on the prediction sets. For example, the simplest, and yet the most commonly-used guarantee is the _marginal coverage_: The prediction sets \(C(x)\) achieve marginal coverage if, for a test sample \((X_{n+1},Y_{n+1})\), we have \((Y_{n+1} C(X_{n+1}))=1-\). Here, \(\) is the given miscoverage rate, and the probability is over the randomness in the calibration and test points.

Conformal Prediction faces two major challenges in practice: conditional validity and length inefficiency. Marginal coverage is often a weak guarantee; in many practical scenarios we need coverage guarantees that hold across different sub-populations of the data. E.g., in healthcare applications, obtaining valid prediction sets for different patient demographics is crucial; we often need to construct accurate prediction sets for certain age groups or medical conditions. This is known as conditional validity - which ideally seeks a property called _full conditional coverage_: For every \(x\) we require

\[(Y_{n+1} C(X_{n+1}) X_{n+1}=x)=1-.\] (1)

Alas, achieving full conditional coverage with finite calibration data is known to be impossible . Consequently, in recent years several algorithmic frameworks have been developed that guarantee relaxations of (1). However, the prediction sets constructed by these methods are often observed to be (unnecessarily) large in size ; i.e., it is possible to find other conditionally-valid prediction sets with smaller length. Even in the case of marginal coverage, theprediction sets constructed by algorithms such as split-conformal can be far from optimal in size. This brings us to the second major challenge of CP, namely length efficiency.

Length efficiency is about constructing prediction sets whose size (or length) is as small as possible while maintaining conditional validity. Here, "length" refers to the average length of the prediction sets \(C(X)\) over the distribution of \(X\). Length efficiency is crucial for the prediction sets to be informative . In fact, the performance of CP methods are closely tied to the length efficiency of the prediction sets in practical applications in decision making, robotics, communication systems , and large language models . This raises the question of how length and coverage fundamentally interact. Along this line, we ask: _How can we construct prediction sets that are (near-) optimal in length while maintaining valid (conditional) coverage?_

In this paper, we answer this question by proposing a unified framework for length optimization under coverage constraints. Before providing the details, we find it necessary to explain (i) where in the CP pipeline our framework is operating, and (ii) the crucial role of the covariate \(X\).

First, we note that the pipeline of CP consists of three stages (see Figure 1) which are often treated separately : Training a model using training data, designing a conformity score, and constructing the prediction sets. Our framework operates at the third stage, i.e., we aim at designing the prediction sets assuming a given predictive model as well as a conformity score. In recent years, various powerful frameworks have been developed for designing better conformity scores  and obtaining conditional guarantees using a given score . The missing piece in this picture is length optimization which is the subject of this paper.

Second, we emphasize that length optimization can be highly dependent on the structural properties of the covariate \(X\). It has been known in the CP community that the structure of \(X\) can play a role in terms of length efficiency and coverage validity (see e.g. ). However, to our knowledge, a principled study of length optimization using the covariate \(X\) is missing. To showcase the principles and challenges of using the covariate \(X\) in the design of prediction sets, we provide a toy example.

**Example.** Let \(=[-1,+1]\). Assume \(X\) is uniformly distributed over \(\), and \(Y\) is distributed as:

-if \(x<0\), then \(Y x+(0,2)\), and -if \(x 0\), then \(Y x+(0,1)\),

see Figure 2-(a). For simplicity, we assume in this example that we have infinitely many data points available from this distribution. As the model, we consider \(f(x)=[Y X=x]=x\). As a result, considering the conformity score \(S(x,y)=|y-f(x)|\), the distribution of \(S\) follows the folded Gaussian distribution: i.e. for \(x<0\) we have \(_{S|x}=|N(0,2)|\), and for \(x 0\) we have \(_{S|x}=|N(0,1)|\) - see Figure 2-(b). We aim for marginal coverage of \(0.9\), and consider prediction sets \(C(x)\) in the following form:

-if \(x<0:C(x)=\{y\,\ S(x,y) q_{-}\},\ \ \ \ x 0:C(x)=\{y\,\ S(x,y) q_{+}\}\).

For every value of \(q_{+}\) in the range \([1.4,+]\) there exists a unique \(q_{-}\) that ensures 0.9-marginal coverage. This provides a continuous _family of prediction sets_, parameterized by \(q_{+}\), all of which are marginally valid, but have different average lengths. Length optimization over this family amounts to minimizing the average length (which is equal to \(q_{-}+q_{+}\)) over the choice of \(q_{+}\). Figure 2-(c) plots the average length versus \(q_{+}\). Four lessons from this example are as follows: (i) First, as we see from Figure 2-(c), the optimal-length solution is different from the sets constructed by the Split-Conformal method (for which \(q_{-}\) and \(q_{+}\) are both equal to the 0.9-quantile of the marginal distribution of \(S\)). Hence, the structure of the optimal prediction sets _depends on the structure of the covariates_ (in this case the sign of the covariate). It can be shown that the optimal-solution found is the unique globally optimal-length solution among all the possible marginally-valid prediction

Figure 1: The CP pipeline.

sets. Hence, the feature \(h(x)=(x)\) is crucial in determining the optimal-length sets. (ii) Second, for \((q_{-}^{*},q_{+}^{*})\) corresponding to the optimal-length sets, the conditional PDFs take the same value; i.e. \(p(S=q_{+}^{*}|x 0)=p(S=q_{-}^{*}|x 0)-\) see Figure 2-(b). This is not a coincidence, and as we will show, it is the main deriving principle to find the optimal sets - see Example 3.2 below.

**Contributions.** We develop a principled and practical framework to design length-optimized prediction sets that are conditionally valid. We formalize conditional validity using a class of covariate shifts (as in ). As for the class, we consider a linear span of finitely-many basis functions of the covariates. This is a broad class; E.g., by adjusting the basis functions we can recover marginal coverage or group-conditional coverage as special cases.

We develop a foundational minimax notion where the min part ensures conditional validity and the max part optimizes length. In the infinite-sample setting, the solution of the minimax problem is proved to be the optimal-length prediction sets, through a strong duality result. To design our practical algorithm for the finite-sample setting (termed as CPL), our key insight is to relax minimax problem using a given conformity score and a hypothesis class that aims to learn from data features that are important for length optimization and uncertainty quantification. In the finite sample setting, we guarantee that our algorithm remains conditionally valid up to a finite-sample error.

We extensively evaluate the performance of CPL on a variety of real world and synthetic datasets in classification, regression, and text-based settings against numerous state-of-the-art algorithms ranging from Split-Conformal, Jacknife, Local-CP, and CQR, to BatchGCP and Conditional-Calibration. A detailed discussion of our experiments in section 5 showcase the overall superior performance of CPL in achieving significantly better length efficiency in different scenarios where we demand marginal validity, group conditional validity, or the more complex case of conditional validity with respect to a class of covariate shifts.

## 2 Problem Formulation

### Preliminaries on conditional validity and length of prediction sets

Recall that \((X,Y)\) is generated from a distribution \(\) supported on \(\) (distributions \(_{X}\) and \(_{Y|X}\) are then defined naturally). The prediction sets are of the form \(C(x): 2^{}\) for \(x\).

**Conditional Coverage Validity.** Ultimately, in conformal prediction, the goal is to design prediction sets that satisfy the _full conditional coverage_ introduced in (1). Despite the importance, full conditional coverage is impossible to achieve in the finite-sample setting . Therefore, several weaker notions of coverage such as marginal , group conditional , and coverage with respect to a class of covariate shifts  have been considered in the literature. Here, we focus on the notion of conditional validity with respect to a class of covariate shifts, which unifies all these notions, and contains each of them as a special case by adjusting the class. Fix any non-negative function \(f\) and let \(_{f}\) denote the setting in which \(\{(X_{i},Y_{i})\}_{i=1}^{n}\) is sampled i.i.d. from \(\), while \((X_{n+1},Y_{n+1})\) is sampled independently from the distribution in which \(_{X}\) is _shifted_ by \(f\), i.e.,

\[X_{n+1}_{}[f(X)]} d_{X}( x), Y_{n+1} X_{n+1}_{Y|X}.\]

We define the exact coverage validity under non-negative _covariate shift_\(f\) as, \(_{X_{n+1}_{f}}(Y_{n+1} C(X_{n+1}))=1-\), which can be equivalently written as,

\[_{X_{n+1}}[f(X_{n+1})[Y_{n+1 } C(X_{n+1})]-(1-)}]=0.\] (2)

Figure 2: (a) Distribution of the labels conditioned on the covariate \(x\) (b) The conditional PDFs. (c) Avg length vs \(q_{+}\). The red dots correspond to three different marginally-valid prediction sets.

This last equality enables us to define the notion of generalized covariate shift. For prediction sets \(C\), we say that coverage with respect to a function \(f\) (which can take negative values) is guaranteed if (2) holds. We often drop the term "generalized" when we refer to generalized covariate shifts.

Let \(\) be a class of functions from \(\) to \(\). We say the prediction sets \(C\) satisfy valid conditional coverage with respect to the class of covariate shifts \(\), if (2) holds for every \(f\).

It is easy to see that if \(\) is the class of all (measurable) functions on \(\), then conditional validity w.r.t. \(\) is equivalent to full conditional converge (1). Accordingly, using less complex choices for \(\) would result in relaxed notions of conditional coverage. We now review three important instances of \(\):

1) **Marginal Coverage:** Setting \(=\{x c c\}\), i.e. constant functions, recovers the marginal validity, i.e., \(_{X_{n+1}}(Y_{n+1} C(X_{n+1}))=1-\).

2) **Group-Conditional Coverage:** Let \(G_{1},G_{2},,G_{m}\) be a collection of sub-groups in the covariate space. Define \(f_{i}(x)=[x G_{i}]\) for every \(i[1,m]\). By using the class of covariate shifts \(=\{_{i=1}^{m}_{i}f_{i}(x)_{i}i[1,,m]\}_{i=1}^{m}\) we can obtain the group-conditional coverage validity; i.e. \(_{X_{n+1}}(Y_{n+1} C(X_{n+1})|X G_{i})=1-\), for every \(i[1,m]\).

3) **Finite-dimensional Affine Class of Covariate Shifts:** Let \(:^{d}\) be a _predefined_ function (a.k.a. the finite-dimensional basis). The class \(\) is defined as \(=\{,(x)|^{d}\}\). Here, \(\) is an affine class of covariate shifts as for any \(f,f^{}\) and \(\) we have \(f+ f^{}\). We use the term "bounded" class of covariate shifts when we assume, \(_{1 i d}_{x}\,_{i}(x)<,\) where \((x)=[_{1}(x),,_{d}(x)]\).

The finite-dimensional affine class is fairly general and covers a broad range of scenarios in theory and practice (see Section 5.3, Remark D.1 in appendix D, as well as ). As special cases, it includes both of the marginal and group-conditional settings. To see this, one can pick \((x)=1\) for the marginal case and \(=[f_{1}(x),f_{2}(x),,f_{m}(x)]^{T}\) for the group-conditional scenario. Consequently, from now on we will focus on the scenario where \(\) is a \(d\)-dimensional affine class of covariate shifts.

**Length.** We use the term \((C(x))\) to denote the length of a prediction set at point \(x\). Length can have different interpretations in different contexts, yet it always depicts the same intuitive meaning of size. In the case of regression when \(=\) we have, \((C(x))=_{}[y C(x)]dy\). Here, \(dy\) can be interpreted as Lebesgue integral (i.e. the usual way to measure length on \(\)). Similarly, in the classification setting, when \(=\{y_{1},y_{2},,y_{L}\}\), we let \((C(x))=_{i=1}^{L}[y C(x)]\).

### Problem Statement

In the previous section, we defined conditional validity and length as two main notions for prediction sets. The canonical problem that we study in this paper is:

**Primary Problem:**

\[} _{X}[(C(X))]\] subject to \[_{X,Y}[f(X)[Y C(X)]-(1- )}]=0, f\]

I.e., we want to construct predictions sets \(C(x)\), \(x\) with optimal (i.e. minimal) average length while being conditionally valid with respect to a (finite-dimensional) class of covariate shifts \(\).

In the special case of marginal validity the constraint becomes \([c\,\{[Y C(X)]-(1-)\}]=0\) for every \(c\), or equivalently \((Y C(X))=1-\). Similarly, in the special case of group-conditional the constraint boils down to \((Y C(X)|X G_{i})=1-\) for every \(i[1,,m]\).

## 3 Minimax Formulations

In this section we analyze the Primary Problem 2.2 in the infinite-data regime. Our goal is to derive the principles of an algorithmic framework for Primary Problem 2.2. We will do so by deriving an equivalent minimax formulation whose solutions have a rich interpretation in terms of level sets of the data distribution. Having the practical finite-sample setting in mind, we then further relax the minimax formulation using a given conformity score and a hypothesis class, where we also analyze the impact of this relaxation on conditional coverage validity and length optimality.

### The Equivalent Minimax Formulation: A Duality Perspective

A natural way to think about the Primary Problem2.2 is to look at the dual formulation. Let us define,

\[g_{}(f,C)=_{X,Y}[f(X)[Y C(X)]-(1- )}]-_{X}[(C(X))],\] (3)

Note that the first term in \(g_{}(f,C)\) corresponds to coverage w.r.t. to \(f\) and the second term corresponds to length. One can easily check that \(g_{}(f,C)\) is equivalent1to the Lagrangian for the Primary Problem (given that \(\) is an affine finite-dimensional class). The dual problem can be written as:

 
**Minimax Problem:** \\  \(}{}\) & \(g_{}(f,C)\) \\  

**Proposition 3.1** (Strong Duality): _Assuming \(_{Y|X}\) is continuous, the Primary Problem and the Minimax Problem are equivalent. Let \((f^{*},C^{*}(x))\) be an optimal solution of the Minimax Problem. Then, \(C^{*}\) is also the optimal solution of the Primary Problem. Furthermore, \(C^{*}\) has the following form:_

\[C^{*}(x)=\{y f^{*}(x)p(y|x) 1\}\] (4)

**Example 3.2** (Marginal case): _For the marginal case, where \(=\{x cc\}\), we have \(C^{*}(x)=\{y c^{*}p(y|x) 1\}\). In other words, the minimal length marginally valid prediction set is of the form of a specific level set of \(p(y|x)\). Note that the value of \(c^{*}\) can be found using the fact that the marginal coverage should be \(1-\)._

Simply put, Proposition 3.1 states that the answer to the Primary Problem corresponds to specific level sets of the distributions \(p(y|x)\). Further, this optimal level set has an equivalent minimax formulation. We will see in section 4 how a relaxation of this minimax can be used to derive a finite sample algorithm. Next, we will discuss the roles of the inner maximization and the outer minimization.

**The Inner Maximization.** For a function \(f:\) we define the level sets corresponding to \(f\) as

\[C_{f}(x)=[f(x)p(y|x) 1].\] (5)

From Proposition 3.1, the optimal prediction sets have the form \(C_{f^{*}}\) for some \(f^{*}\). The following proposition, shows that for a fixed \(f\) the outcome of the inner maximization _is exactly the level set \(C_{f}\)_.

**Proposition 3.3** (Variational representation): _For any \(f\), and \(\) we have,_

\[C_{f}=}\ g_{}(f,C),\] (6)

**The Outer Minimization.** So far, we know that for every \(f\), the inner maximization chooses the level set \(C_{f}\) given in (5). The next question is which of these level sets will be chosen by the outer minimization? The answer is simple: The one that is conditionally valid with respect to the class \(\). I.e., the outer minimization chooses a \(f^{*}\) such that \(C_{f^{*}}\) has correct conditional coverage w.r.t. to \(\).

**Lemma 3.4**: _Let \(f\). Recall that \(\) is an affine class of functions. This means for every \(\) and \(\), \(f+\). We have for every \(\),_

\[g_{}(f+,C_{f+ {f}})_{=0}=[(X)[ Y C_{f}(X)]-(1-)}].\]

At the optimum solution \(f^{*}\) of the outer minimization, all the directional derivatives are zero (since \(f^{*}\) is a stationary point). Hence, using the lemma, \(C_{f^{*}}\) satisfies coverage validity on the class \(\).

Let us summarize: The outer minimization ensures that we navigate the space of conditionally valid prediction sets while the inner maximization finds the most length efficient among them.

### Relaxed Minimax Formulation using Structured Prediction Sets

Taking a closer look at the Minimax formulation 3.1, the inner maximization is over the space of all the possible prediction sets, which is an overly complex set. We need to relax this maximization due to two reasons: (1) Eventually, we would like to solve the finite-sample version of this problem

[MISSING_PAGE_FAIL:6]

Problem has an optimal solution of the form \((f^{*},h^{*})\). Next, we consider a bounded-complexity class \(\). We show that under a realizability assumption, i.e., \(h^{*}\), strong duality still holds.

We denote the optimal value of the Relaxed Minimax Problem by \(\) and the optimal value of the Relaxed Primary Problem by \(L^{*}\). One can interpret \(L^{*}\) as the smallest possible length over all the structured prediction sets that are conditionally valid with respect to \(\). In this context, strong duality means \(L^{*}=-\), as our definition of \(g_{}(f,h)\) differs from the conventional definition of Lagrangian by a negative sign, as we wanted to stay consistent with the literature on level set estimation (e.g. see ). Let us now state a technical assumption needed to develop our own.

**Assumption 1**: _Recall that the class \(\) is defined as \(=\{,(x)| ^{d}\}\), where \(:^{d}\) is a predefined function (a.k.a. the finite-dimensional basis). Let \((x)=[_{1}(x),,_{d}(x)]\). We assume each \(_{i}(x)\) takes countably many values._

Assumption 1 is mainly a technical assumption that helps to obtain a more simplified result. Assumption 1 holds in both marginal and group-conditional settings (as in these cases \(_{i}\)'s take their value in \(\{0,1\}\)). We will provide a more general but weaker result without assumption 1 in Appendix F.

**Theorem 3.5**: _Let us assume \(_{S|X}\) and \(_{X}\) are continuous. Let \(\) be a bounded finite-dimensional affine class of covariate shifts and \(\) be the class of all (measurable) functions from \(\) to \(\). Under assumption 1, the strong duality holds between the Relaxed Primary Problem and the Relaxed Minimax Problem. In other words,_

\[}{}g_{}(f,h)= }{}g_{}(f,h),\]

_and the optimal values of the two problems coincide (\(L^{*}=-\))._

_If the Relaxed Primary Problem (i.e., the right hand side of the above equation) attains it's optimal value at \(h^{*}\), then there exists \(f^{*}\) such that \((f^{*},h^{*})\) is an optimal solution to the Relaxed Minimax Problem. Otherwise, let \(f^{*}\) denote the optimal solution to the outer minimization of the Relaxed Minimax Problem. Then for every \(>0\), there exists \(h^{*}\) such that,_

_(i)_ \(|g_{}(f^{*},h^{*})-|\)_._

_(ii)_ \(h^{*}\) _is conditionally valid; i.e., for every_ \(f\) _we have,_

\[[f(X)[Y C^{S}_{h^{*}}(X)]-(1-) }]=0.\]

_(iii)_ \(_{X}(C^{S}_{h^{*}}(X)) L^{*}+\)_._

Put it simply, Theorem 3.5 says for any \(>0\), there is an \(\)-close optimal solution of the Relaxed Minimax Problem (statement (i)) such that it is a feasible solution of the Relaxed Primary Problem with exact conditional coverage (statement (ii)), and it achieves only an \(\)-larger prediction set length than the smallest possible, which is the solution of Relaxed Minimax Problem (statement (iii)).

**Bounded complexity class \(\):** The following realizability condition paves the way to generalize the strong duality results to the case where \(\) is a class of functions with bounded complexity.

**Definition 3.6** (Realizability): _Let \(\) be a class of real valued functions defined on \(\). We say \(\) is Realizable if there exists \(h^{*}\) such that \(h^{*}\) is an optimal solution of the Relaxed Primary Problem and the Relaxed Minimax Problem._

**Proposition 3.7**: _Let \(=\{h_{}\}\), be a realizable class of functions where \(\) is a compact set. Then the Relaxed Minimax Problem and the Relaxed Primary Problem are equivalent. In other words, strong duality holds and the min and max are interchangeable in the Relaxed Minimax Problem._

## 4 Finite Sample Setting: The Main Algorithm

In this section, we present and analyze our main algorithm. Assume we have access to \(n\) independent and identically distributed (i.i.d.) calibration data \(\{(x_{i},y_{i})\}_{i=1}^{n}\) from the distribution \(\). Let \(S(x,y):\) represents a given conformity score. We consider a bounded capacity class of functions \(\)-e.g. a neural network parameterized by its weights. Additionally, we consider a given finite-dimensional affine class of functions \(\) that represents the level of conditional validity to be achieved.

**Unbiased Estimation.** From (3), the objective \(g_{}(f,h)\) of the Relaxed Minimax Problem 3.2 admits a straightforward unbiased estimator using \(n\) i.i.d. samples \(\{(x_{i},y_{i})\}_{i=1}^{n}\):

\[g_{,n}(f,h)=_{i=1}^{n}f(x_{i}) [S(x_{i},y_{i}) h(x_{i})]-(1-)}-_{i=1} ^{n}_{}[S(x_{i},y) h(x_{i})]dy.\]

**Smoothing.** The objective \(g_{,n}(f,h)\) is non-smooth as it involves indicator functions. To make it differentiable, we will need to smooth the objective. A common way to smooth the indicator function \([a<b]\) is via Gaussian smoothing. Consider the Gaussian error function, \((x)=}_{0}^{x}e^{-t^{2}}\,dt\), and define the smoothed indicator function as: \(}(a,b)=(1+(}))\), where \(\) controls the smoothness of the transition between \(0\) to \(1\). The value of \(\) is often chosen to be small, and when it approaches zero we retrieve the indicator function. The smoothed version of our objective is

\[_{,n}(f,h)=_{i=1}^{n}[f(x_{i}) }(S(x_{i},y_{i}),h(x_{i}))-(1-)}]-_{i=1}^{n}_{}}(S(x_{i},y),h(x_{i}))dy.\]

Given this smoothed objective, our main algorithm is presented in Algorithm 1.

```
1:Input: Miscoverage level \(\), conditional coverage requirements \(\), conformity score \(S\), class \(\).
2:Objective: \[}{}_{,n}(f,h).\]
3:while not converged do
4: Perform a few iterations (e.g. SGD steps) on \(h\) to maximize \(_{,n}(f,h)\)
5: Perform a few iterations (e.g. SGD steps) on \(f\) to minimize \(_{,n}(f,h)\)
6:endwhile
7:\(f^{*}_{} f, h^{*}_{} h\)
8: Let \(C^{*}_{}(x)=\{y S(x,y) h^{*}_{}( x)\}\). ```

**Algorithm 1** Conformal Prediction with Length-Optimization (CPL)

**Remark 4.1**: _Oftentimes in practice, the machine learning models, such as Neural Networks, are parametric (\(h_{}\), where \(\) is the set of parameters). In that case, performing the iteration on \(h\) can be implemented by updating \(\)._

**Remark 4.2**: _Recalling the definition of \(=\{,(x)| ^{d}\}\), each \(f\) can be represented by \(f(x)=,(x) f_{}(x)\). Hence, performing the iteration on \(f\) can be implements by updating \(^{d}\)._

### Finite Sample Guarantees

In this section we provide finite sample guarantees for the conditional coverage of the prediction sets constructed by CPL using the covering number of class \(\) (denoted by \(\)). We analyse the properties of stationary points of CPL. This is not a very restrictive choice as it is well known in the optimization literature that the converging points of gradient descent methods (like CPL) are the stationary points of the minimax problem (e.g. look at ). We now state the required technical assumption.

**Definition 4.3**: _A distribution \(\), is called \(L\)-lipschitz if we have for every real valued numbers \(q q^{}\),_

\[_{X}\,(X q^{})-_{X}\,(X q)  L\,(q^{}-q)\,.\]

**Assumption 2**: _The conditional distribution \(_{S X}\) is \(L\)-Lipschitz, almost surely with respect to \(_{X}\)._

Assumption 2 is often needed for concentration guarantees in CP literature . Consider CPL with smoothed function \(_{,n}\) with smoothing parameter \(=}\). Let \((f^{*}_{},h^{*}_{})\) denote a stationary point reached by Algorithm 1. Also, the corresponding prediction sets are given as \(C^{*}_{}(x)=\{y S(x,y) h^{*}_{}( x)\}\).

**Theorem 4.4** (Conditional coverage validity): _Under assumption 2, let \((f^{*}_{},h^{*}_{})\) denote a stationary point reached by Algorithm 1 (if exists) and \(C^{*}_{}(x)\) its corresponding prediction sets. Then with probability \(1-\) we have,_

\[\,[_{}(X)[Y C ^{*}_{}(X)]-(1-)}\, (,d_{},)}{ })}+c_{2}}{}, f_{} ,\]

_where \(c_{1}=B||||_{1}\), \(c_{2}=||||_{1}BL}+||||_{ 1}}\), \(B=_{i}_{x}_{i}(x)\), and \(\) is the basis for \(\) (see Section 2.1)._

PAC-style guarantees of the order \(O(})\) are generally optimal and common in CP literature . Look at Appendix C for further discussions and interpretations.

Experiments

In this section, we empirically examine the length performance of CPL compared to state-of-the-art (SOTA) baselines under varying levels of conditional validity requirements. This section is organized into three parts, each dedicated to setups that demand a specific level of conditional validity. Throughout this section, we demonstrate across different data modalities and experimental settings that CPL provides significantly tighter prediction sets (i.e. smaller length) that are conditionally valid. Additionally, we have provided a Python notebook with a step-by-step efficient implementation of our algorithm, which can be accessed at the following link: https://github.com/shayankiyani98/CP.

### Part I: Marginal Coverage for Multiple Choice Question Answering

We use multiple-choice question answering datasets, including TruthfulQA , MMLU , OpenBookQA , PIQA, and BigBench . The task is to quantify the uncertainty of Llama 2  and create prediction sets using this model. We follow a procedure similar to the one proposed by  described below.

For each dataset, we tackle the task of multiple-choice question answering. We pass the question to Llama 2 using a simple and fixed prompt engineering approach throughout the experiment:

This is a 4-choice question that you should answer: {question} The correct answer to this question is:

We then look at the logits of the first output token for the options A, B, C, and D. By applying a softmax function, we obtain a probability vector. The conformity score is defined as \((1-\) probability of the correct option), which is similar to \((1-f(x)_{y})\) for classification.

Our method is implemented using \(\) as a linear head on top of a pre-trained GPT-2 . GPT-2 has 768-dimensional hidden layers, so the optimization for the inner maximization involves a 768-dimensional linear map from GPT-2's last hidden layer representations to a real-valued scalar. We also implemented the method of , which directly applies split conformal method on the scores.

Figures 2(a) shows the performance of our method (CPL) compared to the baseline. CPL achieves significantly smaller set sizes while ensuring proper 90% coverage.

### Part II: Group-Conditional Coverage for Synthetic Regression

We use a synthetic regression task as in  to compare CPL with BatchGCP . The covariate \(X=(X_{1},,X_{100})\) is a vector in \(^{100}\). The first ten coordinates are independent uniform binary variables, and the remaining 90 coordinates are i.i.d. Gaussian variables. The label \(y\) is generated as \(y=,X+_{X}\) where \(_{X}\) is a zero-mean Gaussian with variance \(_{X}^{2}=1+_{i=1}^{10}iX_{i}+(40[_{i=11}^{10 }X_{i} 0]-20).\) We generate 150K training samples, 50K calibration data points, and 50K test data points. We evaluate all algorithms over 100 trials and report average performance.

We define 20 overlapping groups based on ten binary components of \(X\). For each \(i\) in 1 to 10, Group \(2i-1\) corresponds to \(X_{i}=0\) and Group \(2i\) to \(X_{i}=1\). BatchGCP and CPL are implemented to provide group-conditional coverage for these groups. We use a 2-hidden-layer NN with layers of

Figure 3: Left-hand-side plot shows coverage and right-hand-side shows mean prediction set size.

20 and 10 neurons for the inner maximization. We also include the Split Conformal method and an optimal oracle baseline, calculated numerically using the optimal formulation of Proposition 3.1.

As seen in Figure 4, the Split Conformal under covers in high-variance groups and over covers in low-variance groups. CPL and BatchGCP provide near-perfect coverage in all groups, matching the Optimal Oracle. The mean interval plot shows that BatchGCP performs similarly to Split Conformal, while CPL significantly reduces the average length, nearing the Optimal Oracle solution.

### Part III: Coverage w.r.t. a General Class of Covariate Shifts for Image Classification

We conduct experiments on the RxRx1 dataset from the WILDS repository, which consists of cell images for predicting one of 1,339 genetic treatments across 51 experiments, each exhibiting covariate shifts. Our objective is to construct prediction sets that retain valid coverage across these shifts. We compare two methods: CPL and the Conditional Calibration algorithm introduced by , which employs quantile regression to achieve valid conditional coverage under covariate shifts.

To implement these methods, we follow the approach in , where covariate shifts are defined by partitioning the calibration data. A multinomial linear regression model with \(_{2}\) regularization is then trained on the first half of the calibration data to predict which experiment each image belongs to, and the features learned by this regression serve as the basis of the covariate shift class.

For the genetic treatment prediction task, we use a pre-trained ResNet50 model (trained by ), \(f(x)\), trained on data from 37 experiments. The remaining 14 experiments are split into separate calibration and test sets. To handle the inner optimization of our method, we train a linear classifier on the final-layer representations of the pre-trained ResNet50. We then compute conformity scores as follows: for each image \(x\), let \(f^{i}(x)_{i=1}^{1339}\) represent the output of the ResNet50 for each treatment class. We apply temperature scaling and a softmax function to convert these outputs into probability estimates, \(^{i}(x):=(Tf_{i}(x))/(_{j}(Tf_{j}(x)))\), where \(T\) is the temperature parameter. The conformity score \(S(x,y)\) is calculated as the sum of \(_{i}(x)\) over treatments for which \(_{i}(x)>_{y}\).

Figure 5 compares CPL, Conditional Calibration, and Split Conformal methods. While both CPL and Conditional Calibration maintain valid coverage, CPL achieves significantly smaller average prediction set sizes by focusing on feature learning without sacrificing conditional validity.

Figure 4: Left-hand-side plot shows coverage and right-hand-side shows mean interval length.

Figure 5: Left-hand-side plot shows coverage and right-hand-side shows mean prediction set size. The reported values are averaged over 20 different splits of calibration data.

[MISSING_PAGE_FAIL:11]

*  Tiffany Ding, Anastasios Angelopoulos, Stephen Bates, Michael Jordan, and Ryan J Tibshirani. Class-conditional conformal prediction with many classes. _Advances in Neural Information Processing Systems_, 36, 2024.
*  Wenwen Si, Sangdon Park, Insup Lee, Edgar Dobriban, and Osbert Bastani. Pac prediction sets under label shift. _arXiv preprint arXiv:2310.12964_, 2023.
*  Shayan Kiyani, George Pappas, and Hamed Hassani. Conformal prediction with learned features. _arXiv preprint arXiv:2404.17487_, 2024.
*  Lars Lindemann, Matthew Cleaveland, Gihyun Shim, and George J Pappas. Safe planning in dynamic environments using conformal prediction. _IEEE Robotics and Automation Letters_, 2023.
*  Matthew Cleaveland, Insup Lee, George J Pappas, and Lars Lindemann. Conformal prediction regions for time series using linear complementarity programming. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 20984-20992, 2024.
* 1111, 2016.
*  Mauricio Sadinle, Jing Lei, and Larry Wasserman. Least ambiguous set-valued classifiers with bounded error levels. _Journal of the American Statistical Association_, 114(525):223-234, 2019.
*  Yu Bai, Song Mei, Huan Wang, Yingbo Zhou, and Caiming Xiong. Efficient and differentiable conformal prediction with general function classes. _arXiv preprint arXiv:2202.11091_, 2022.
*  Matteo Zecchin, Sangwoo Park, Osvaldo Simeone, and Fredrik Hellstrom. Generalization and informativeness of conformal prediction. _arXiv preprint arXiv:2401.11810_, 2024.
*  Jesse C Cresswell, Yi Sui, Bhargava Kumar, and Noel Voutisis. Conformal prediction sets improve human decision making. _arXiv preprint arXiv:2401.13744_, 2024.
*  Eleni Straitouri, Lequn Wang, Nastaran Okati, and Manuel Gomez Rodriguez. Improving expert predictions with conformal prediction. In _International Conference on Machine Learning_, pages 32633-32653. PMLR, 2023.
*  Varun Babbar, Umang Bhatt, and Adrian Weller. On the utility of prediction sets in human-ai teams. _arXiv preprint arXiv:2205.01411_, 2022.
*  Eleni Straitouri and Manuel Gomez Rodriguez. Designing decision support systems using counterfactual prediction sets. _arXiv preprint arXiv:2306.03928_, 2023.
*  Kfir M Cohen, Sangwoo Park, Osvaldo Simeone, and Shlomo Shamai. Calibrating ai models for wireless communications via conformal prediction. _IEEE Transactions on Machine Learning in Communications and Networking_, 2023.
*  Meiyi Zhu, Matteo Zecchin, Sangwoo Park, Caili Guo, Chunyan Feng, and Osvaldo Simeone. Federated inference with reliable uncertainty quantification over wireless channels via conformal prediction. _IEEE Transactions on Signal Processing_, 2024.
*  Christopher Mohri and Tatsunori Hashimoto. Language models with conformal factuality guarantees. _arXiv preprint arXiv:2402.10978_, 2024.
*  Bhawesh Kumar, Charlie Lu, Gauri Gupta, Anil Palepu, David Bellamy, Ramesh Raskar, and Andrew Beam. Conformal prediction with large language models for multi-choice question answering. _arXiv preprint arXiv:2305.18404_, 2023.
*  John J Cherian, Isaac Gibbs, and Emmanuel J Candes. Large language model validity via enhanced conformal prediction methods. _arXiv preprint arXiv:2406.09714_, 2024.
*  Yaniv Romano, Evan Patterson, and Emmanuel Candes. Conformalized quantile regression. _Advances in neural information processing systems_, 32, 2019.

*  Victor Chernozhukov, Kaspar Wuthrich, and Yinchu Zhu. Distributional conformal prediction. _Proceedings of the National Academy of Sciences_, 118(48):e2107794118, 2021.
*  Nicolas Deutschmann, Mattia Rigotti, and Maria Rodriguez Martinez. Adaptive conformal regression with jackknife+ rescaled scores. _arXiv preprint arXiv:2305.19901_, 2023.
*  Shai Feldman, Stephen Bates, and Yaniv Romano. Improving conditional coverage via orthogonal quantile regression. _Advances in neural information processing systems_, 34:2060-2071, 2021.
*  Yaniv Romano, Matteo Sesia, and Emmanuel Candes. Classification with valid and adaptive coverage. _Advances in Neural Information Processing Systems_, 33:3581-3591, 2020.
*  Yachong Yang and Arun Kumar Kuchibhotla. Selection and aggregation of conformal prediction sets. _Journal of the American Statistical Association_, (just-accepted):1-25, 2024.
*  Harris Papadopoulos, Vladimir Vovk, and Alexander Gammerman. Regression conformal prediction with nearest neighbours. _Journal of Artificial Intelligence Research_, 40:815-840, 2011.
*  Georgy Noarov, Ramya Ramalingam, Aaron Roth, and Stephan Xie. High-dimensional prediction for sequential decision making. _Manuscript; Oral presentation at 15th International OPT Workshop on Optimization for ML_, 2023.
*  Matteo Sesia, Stefano Favaro, and Edgar Dobriban. Conformal frequency estimation using discrete sketched data with coverage for distinct queries. _Journal of Machine Learning Research_, 24(348):1-80, 2023.
*  Harris Papadopoulos, Kostas Proedrou, Vladimir Vovk, and Alexander Gammerman. Inductive confidence machines for regression. In _European Conference on Machine Learning_, 2002.
*  Rina Foygel Barber, Emmanuel J Candes, Aaditya Ramdas, and Ryan J Tibshirani. Predictive inference with the jackknife+. 2021.
*  Rebecca M Willett and Robert D Nowak. Minimax optimal level-set estimation. _IEEE Transactions on Image Processing_, 16(12):2965-2979, 2007.
*  Chi Jin, Praneeth Netrapalli, and Michael Jordan. What is local optimality in nonconvex-nonconcave minimax optimization? In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 4880-4889. PMLR, 13-18 Jul 2020.
*  Matteo Sesia and Yaniv Romano. Conformal prediction using conditional histograms. In _Neural Information Processing Systems_, 2021.
*  Jing Lei and Larry A. Wasserman. Distribution free prediction bands. _ArXiv_, abs/1203.5422, 2012.
*  Sangdon Park, Osbert Bastani, Nikolai Matni, and Insup Lee. Pac confidence sets for deep neural networks via calibrated prediction. In _International Conference on Learning Representations_, 2020.
*  Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. _arXiv preprint arXiv:2109.07958_, 2021.
*  Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. _arXiv preprint arXiv:2009.03300_, 2020.
*  Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In _EMNLP_, 2018.
*  Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 7432-7439, 2020.

*  BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _Transactions on Machine Learning Research_, 2023.
*  Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
*  Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
*  Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton Earnshaw, Imran Haque, Sara M Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. Wilds: A benchmark of in-the-wild distribution shifts. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 5637-5664. PMLR, 18-24 Jul 2021.
*  Ran Xie, Rina Foygel Barber, and Emmanuel J Candes. Boosted conformal prediction intervals. _arXiv preprint arXiv:2406.07449_, 2024.
*  Valery Manokhin. Awesome conformal prediction. _If you use Awesome Conformal Prediction. please cite it as below_, 2022.
*  Alessandro Rinaldo and Larry Wasserman. Generalized density clustering1. _The Annals of Statistics_, 38(5):2678-2722, 2010.
*  PHILIPPE RIGOLLET and REGIS VERT. Optimal rates for plug-in estimators of density level sets. _Bernoulli_, 15(4):1154-1178, 2009.
*  Wolfgang Polonik. Measuring mass concentrations and estimating density contour clusters-an excess mass approach. _The annals of Statistics_, pages 855-881, 1995.
*  Jing Lei, James Robins, and Larry Wasserman. Efficient nonparametric conformal prediction regions. _arXiv preprint arXiv:1111.1418_, 2011.
*  David Stutz, Krishnamurthy Dj Dvijotham, Ali Taylan Cemgil, and Arnaud Doucet. Learning optimal conformal classifiers. In _International Conference on Learning Representations_, 2022.
*  Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
*  Ramon Van Handel. Probability in high dimension. _Lecture Notes (Princeton University)_, 2014.
*  David G Luenberger. _Optimization by vector space methods_. John Wiley & Sons, 1969.
*  Jerzy Neyman and Egon Sharpe Pearson. Ix. on the problem of the most efficient tests of statistical hypotheses. _Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character_, 231(694-706):289-337, 1933.
*  Harris Papadopoulos, Alex Gammerman, and Volodya Vovk. Normalized nonconformity measures for regression conformal prediction. In _Proceedings of the IASTED International Conference on Artificial Intelligence and Applications (AIA 2008)_, pages 64-69, 2008.
*  Samuel H Zavekas and David Kashihara. The impacts of the covid-19 pandemic on the medical expenditure panel survey. _American journal of public health_, 111(12):2157-2166, 2021.
*  Svetlana Pashchenko and Pompoje Porapakkarm. Medical spending in the us: facts from the medical expenditure panel survey data set. _Fiscal Studies_, 37(3-4):689-716, 2016.
*  Krisztian Buza. Feedback prediction for blogs. In _Data analysis, machine learning and knowledge discovery_, pages 145-152. Springer, 2013.
*  Prashant S Rana. Physicochemical properties of protein tertiary structure data set. _UCI Machine Learning Repository_, 2013.

*  Hadi Fanaee-T. Bike Sharing. UCI Machine Learning Repository, 2013. DOI: https://doi.org/10.24432/C5W894.
*  Michael Redmond. Communities and Crime. UCI Machine Learning Repository, 2009. DOI: https://doi.org/10.24432/C53W3X.
*  Helen Pate-Bain, Jayne Boyd-Zaharias, Van A Cain, Elizabeth Word, and M Edward Binkley. Star follow-up studies, 1996-1997: The student/teacher achievement ratio (star) project. 1997.
*  I-Cheng Yeh. Concrete Compressive Strength. UCI Machine Learning Repository, 2007. DOI: https://doi.org/10.24432/C5PK67.
*  Kamaljot Singh. Facebook Comment Volume. UCI Machine Learning Repository, 2016. DOI: https://doi.org/10.24432/C5Q886.
*  Kamaljot Singh, Ranjeet Kaur Sandhu, and Dinesh Kumar. Comment volume prediction using neural networks and decision trees. In _IEEE UKSim-AMSS 17th International Conference on Computer Modelling and Simulation, UKSim2015 (UKSim2015)_, 2015.

Outline
* Section B provides more discussions on the existing related works.
* Appendix C is dedicated on some interpretations of the finite sample guarantees of Section 4.
* Appendix D lists the all the Remarks that we moved to Appendix from the main body due to space limit.
* Appendix E provides the Lemmas, including their proofs, that are necessary for the development of out theoretical framework.
* Section F provides the proofs of all the statements proposed in Section 3 along with an extra Theorem.
* Section G provides the proofs of all the statements proposed in Section 4.
* Section H provides an extensive experimental setting to showcase the ability of CPL to improve length efficiency in real-world regression scenarios.
* Section I provides an extra experiment setup that showcases the possibility of applying CPL on top of a CIFAR-10 classifier that is trained by conformal training.
* Section J provides references to all the 11 regression datasets used in experiment setup of Section H.

## Appendix B Further Related Works

We now review some of the remaining prior works that are relevant.

**Designing better conformity scores:** There is a growing body of work on designing better conformity scores to improve conditional validity and/or length efficiency  (see  for more references). As shown in Fig. 1, our framework applies post selection of the score, and hence can use any of the conformity scores designed in the literature (see Section 5 for experiments with different scores).

**Level set estimation.** As we will see, a key ingredient to our framework is to study the problem of CP through the lens of level set estimation. Level set estimation has been extensively studied in the literature of statistics , and it was introduced to CP community by  and further studied by . Our framework expands and builds upon the connection of level set estimation and CP. In particular, comparing to , we take three significant steps. (i) We propose to utilize the covariate, \(X\), to directly _optimize length via an adaptive threshold_. This results in major improvements in length efficiency in practice. (ii) We go beyond marginal coverage to different levels of conditional validity. (iii) Finally, unlike , we do not need to directly estimate any marginalconditional density function. Level sets and their approximations appear naturally in our optimization framework.

**Conformal training:** Several works have focused on directly optimizing the score function to enhance the length efficiency of conformal prediction pipelines . In particular,  introduced conformal training, where a (marginal) calibration procedure is fixed, and derivatives of the prediction set size with respect to the model parameters are taken. This enables the model to be optimized not for predictive accuracy, as is typical, but for producing smaller prediction sets. However, a key distinction separates our approach from the broader idea of conformal training. To illustrate this difference, consider the simpler case where we aim only to improve length efficiency under marginal coverage. Let \(S_{}(x,y):\) be a parameterized conformity score (e.g., where \(\) represents the parameters of a neural network). Conformal training optimizes \(\) to improve the length efficiency of prediction sets of the form \(S_{}(x,y) q\), where \(q\) is a fixed threshold. In contrast, we keep \(\) fixed and optimize an adaptive threshold \(h(x):\) to improve the length efficiency of prediction sets of the form \(S_{}(x,y) h(x)\). That is to say, our method can also be applied on top of a score trained by conformal training. In appendix I, we show that it is possible to optimize length even further by applying our method on top of the conformal training introduced by . It's important to note that our use of covariate-adaptive thresholds is specifically for length optimization, distinct from the use in addressing conditional coverage in prior literature. Beyond the theoretical framework, our perspective allows for length optimization in various real-world applications. In practice, access to a model's internal parameters may be restricted due to privacy, security, or policy concerns. In section 5.1, we demonstrate how to construct efficient prediction sets for large language models (LLMs) without optimizing their internal parameters. Additionally, in uncertainty quantification, conformal prediction sets often serve to assess the behavior of a given black-box model. In these cases, the statistician's role is to analyze the model's behavior rather than further optimizing it, which could degrade performance on nominal or out-of-distribution tasks. Additionally, in a concurrent work to ours,  extended conformal training idea to a more general case, allowing the score function to be optimized while targeting conditional coverage in the calibration procedure. They develop a novel technique to differentiate through the conditional conformal algorithm introduced by . However, while both approaches utilize similar relaxations for targeting conditional coverage, our work diverges in the underlying objective. Whereas  focuses on optimizing the score function, we emphasize optimizing the threshold leaving the scores untouched. This leads to two distinct theoretical and algorithmic developments.

## Appendix C Further Discussions on Finite Sample Result

**Theorem C.1** (Conditional coverage validity): _Under assumption 2, let \((f^{*}_{},h^{*}_{})\) denote a stationary point reached by Algorithm 1 (if exists) and \(C^{*}_{}(x)\) its corresponding prediction sets. Then with probability \(1-\) we have,_

\[\,[f_{}(X)[Y C^{*}_{ }(X)]-(1-)}](,d_{},)}{} )}+c_{2}}{}, f_{},\]

_where \(c_{1}=B||||_{1}\), \(c_{2}=||||_{1}BL}+||||_{1}}\), \(B=_{i}_{x}_{i}(x)\), and \(\) is the basis for \(\) (see Sec. 2.1)._

We now provide three Corollaries to this Theorem.

**The case of marginal validity:** For this case we have to pick \(=\{x c c\}\), i.e. constant functions. In this case one can see \(B=1\), so we have the following result.

**Corollary C.2** (Finite sample marginal validity): _Under assumption 2, let \((f^{*}_{},h^{*}_{})\) denote a stationary point reached by Algorithm 1 (if exists) and \(C^{*}_{}(x)\) its corresponding prediction sets. Then with probability \(1-\) we have,_

\[(Y C^{*}_{}(X))-(1-) (,d_{},)}{ })}+c_{2}}{},\]

_where \(c_{1}=\), \(c_{2}=L}+}\)._

**The case of group-conditional validity:** Let \(G_{1},,G_{m}\) be a collection of groups: each group is a subset of covariate space, i.e. \(G_{i}\). These groups can be fully arbitrary and highly overlapping. Define \(f_{i}(x)=[x G_{i}]\) for every \(i[1,m]\). We can then run \(\) using the class of covariate shifts \(=\{_{i=1}^{m}_{i}f_{i}(x)_{i}i[1,,m]\}_{i=1}^{m}\) and obtain tight prediction sets with group-conditional coverage validity. In this case one can see \(B=1\), so we have the following result.

**Corollary C.3** (Finite sample group-conditional validity): _Under assumption 2, let \((f^{*}_{},h^{*}_{})\) denote a stationary point reached by Algorithm 1 (if exists) and \(C^{*}_{}(x)\) its corresponding prediction sets. Then with probability \(1-\) we have,_

\[(Y C^{*}_{}(X) X G_{i})-(1-) (,d_{}, )}{})}+c_{2}}{}, i[1,,m],\]

_where \(c_{1}=\), \(c_{2}=L}+}\)._Further Remarks

**Remark D.1**: _Another important special case of conditional validity with respect to an affine finite dimensional class of covariate shifts is the framework introduced by  which provides CP methods that guarantee a valid coverage when there is a known covariate shift between calibration data and test data. This exactly falls to our framework as the special case if we consider the class \(=\{x cf(x)\ |\ \ c\}\), where \(f\) is the known covariate shift (i.e. likelihood ratio between test and calibration data)._

**Remark D.2**: _We will need to assume that the members of the class \(\) are bounded functions; i.e. for every \(h\) and \(x\) we have \(h(x)[0,]\). Two points are in order. (i) This assumption is purely for the sake of theory development. In practice, one can run our algorithm with any off-the-shelf machine learning class of models. In fact, in Section 5, we run our algorithm with a variety of models including deep neural networks (Resnet 50) and show case its performance. (ii) This assumption is not very far from practice. In fact, one can satisfy this assumption by using a Sigmoid activation function (or a scaled version of it) on the output layer to satisfy this assumption. Similar assumptions has been posed in the literature ._

## Appendix E Technical Lemmas

**Lemma E.1**: _Let \(f(a,x)=(1+(}))\) be the smoothed indicator function, where \((x)\) is the error function, defined as \((x)=}_{0}^{x}e^{-t^{2}}\ dt\), and \(\) is the variance of the Gaussian kernel used for smoothing. Then \(f(a,x)\) is Lipschitz continuous with respect to \(x\) with a Lipschitz constant \(}\)._

**Proof:** To prove that \(f(a,x)\) is Lipschitz continuous with respect to \(x\), we compute the derivative of \(f(a,x)\) with respect to \(x\):

\[f(a,x)=(1+( }))\] \[f(a,x)=(1+(}))= }}e^{-( })^{2}}\]

Simplifying this, we get:

\[f(a,x)=-}e^{-}{2^{2}}}\]

To show that this derivative is bounded, observe that:

\[|f(a,x)|=|-}e^{-}{2^{2}}}|}\]

Since \(}\) is a constant, we conclude that \(f(a,x)\) is Lipschitz continuous with respect to \(x\) with Lipschitz constant \(}\).

**Lemma E.2**: _Let \(}[a<b]\) be the smoothed indicator function defined by_

\[}[a<b]=(1+( }))\]

_where \((x)\) is the error function, defined as \((x)=}_{0}^{x}e^{-t^{2}}\ dt\), and \(\) is the variance of the Gaussian kernel used for smoothing. The indicator function \([a<b]\) is also defined as_

\[[a<b]=1&a<b\\ 0&\]_The error between the smoothed indicator function \(}[a<b]\) and the actual indicator function \([a<b]\), integrated over all \(a\), is given by_

\[E=_{-}^{}|}[a<b]-[a<b]|\,da\]

_This integral evaluates to_

\[E=}\]

**Proof:** To compute the integral, we consider the two cases separately: \(a<b\) and \(a b\).

For \(a<b\), the indicator function \([a<b]=1\), so the absolute difference is

\[|}[a<b]-1|=|(1+(}))-1|=|( })-1|\]

For \(a b\), the indicator function \([a<b]=0\), so the absolute difference is

\[|}[a<b]-0|=(1+(}))\]

Combining these, the integral can be written as

\[E=_{-}^{b}|( })-1|\,da+_{b}^{}(1+ (}))\,da\]

We know that

\[(x)=2(x)-1\]

where \((x)\) is the CDF of the standard normal distribution.

Using symmetry properties of the error function and Gaussian integrals, we can simplify the calculations as follows:

\[_{-}^{b}((})-1 )\,da=-}\]

\[_{b}^{}(1+(}) )\,da=}\]

Thus, the total error integral is:

\[E=(}+} )=(2})= {2}}\]

**Lemma E.3**: _Let \(f\) be a fixed function such that \(_{x}f(x) B\) and let us define,_

\[Z_{h}=|_{i=1}^{n} f(x_{i})}(S(x_{i},y_{i}),h(x_ {i}))-(1-)}\] \[-[f(x)}(S(x,y),h(x))-( 1-)}\,\]

_The following uniform convergence holds: Fixing any \(>0\), we have with probability \(1-\),_

\[|Z_{h}|}+B(,d_{},)}{})} }{}$.}\]

**Proof.** Let \(h_{1},h_{2}\) be two arbitrary functions. We have,

\[|Z_{h_{1}}-Z_{h_{2}}| }{{}}|_{i=1}^{n }[f(x_{i})}(S(x_{i},y_{i}),h_{1}(x_{i}))- }(S(x_{i},y_{i}),h_{2}(x_{i}))}].\] \[-[f(x)}(S(x,y),h_{ 1}(x))-}(S(x,y),h_{2}(x))}]\] \[}{{}}|_{i=1}^ {n}[f(x_{i})}(S(x_{i},y_{i}),h_{1}(x_{i}))- }(S(x_{i},y_{i}),h_{2}(x_{i}))}]|\] \[+|[f(x)}(S(x, y),h_{1}(x))-}(S(x,y),h_{2}(x))}].\] \[}{{}}B_{i= 1}^{n}[}(S(x_{i},y_{i}),h_{1}(x_{i}))-}(S(x_{i},y_{i}),h_{2}(x_{i}))}]|\] \[+B[\{}(S(x,y),h _{1}(x))-}(S(x,y),h_{2}(x))\}]\] \[}{{}}} _{i=1}^{n}[h_{1}(x_{i})-h_{2}(x_{i}) ]\] \[+}[h _{1}(x)-h_{2}(x)]\] \[}{{}}} _{x}|h_{1}(x)-h_{2}(x)|\]

where (a) is a triangle inequality, (b) is another triangle inequality, (c) comes from the definition of \(B\), (d) follows from the Lipschitness Lemma E.1, and finally (e) is from the definition of sup.

Now let us define \(d_{}(h_{1},h_{2})=_{x}|h_{1}(x)-h_{2}(x)|\). Therefore, \(|Z_{h_{1}}-Zh_{2}| 2Bd_{}(h_{1},h_{2})\). By Hoeffding's inequality for general bounded random variables (look at chapter 2 of ), fixing \(h\), we have with probability \(1-\),

\[|Z_{h}|B}}{}.\] (9)

Now as a result of Lemma 5.7 of  (a standard covering number argument) we conclude,

\[|Z_{h}|}+B(,d_{},)}{}} }{}h.\]

**Lemma E.4**: _The following equivalence between the three problems hold,_

_Structured Minimax Problem_

\[\]

_Convex Structured Minimax Problem_

\[\]

_Convex Structured Primary Problem_

where by -**Convex Structured Primary Problem** we mean the optimal value of **Convex Structured Primary Problem** is equal to the negative of the optimal values of the other two problems.

_Proof._ We start by the following claim which will be proven shortly.

**Claim 1**: _Convex Structured Minimax Problem is equivalent to the Structured Minimax Problem._Proof.: Let us remind the objective,

\[g_{}(f,C)=[f(X)C(X,Y)-(1-)}]- _{}C(X,y)dy,\]

which is linear in terms of \(C\). Now fixing \(f\), since \(_{^{}}_{^{}}^{ }\) we have,

\[_{^{}}^{}}{}g_{}(f,C)_{^{}}}{}g_{}(f,C).\]

We now proceed by showing the other direction. Let \(\{C_{i}\}_{i=1}^{}\), where \(C_{i}_{^{}}^{}\) and \(_{i}g_{}(f,C_{i})=_{^{ }}^{}}{}g_{}(f,C)\) (pay attention that it is not trivial that this maximum is achievable by a single member of its domain). Now by the definition of \(_{^{}}^{}\), we know each \(C_{i}\) is a convex combination of finitely many members of \(_{^{}}\). That is to say, for each \(C_{i}\), there exists \(\{C_{ij}\}_{j=1}^{T}\) where \(C_{ij}_{^{}}\) and,

\[C_{i}=_{j=1}^{T}a_{j}C_{ij}(x,y),_{j=1}^{T}a_{j}=1,a_{j} 0(  1 i T).\]

Hence,

\[g_{}(f,C_{i})=g_{}(f,_{j=1}^{T}a_{j}C_{ij}(x,y))=_{j=1}^{T }a_{j}g_{}(f,C_{ij})_{j=1}^{T}g_{}(f,C_{ij})\]

Let us assume that final maximum is achieved by the index \(j_{i}\). Therefore, for each \(C_{i}_{^{}}^{}\) there exists a \(C_{ij_{i}}_{^{}}\) such that \(g_{}(f,C_{i}) g_{}(f,C_{ij_{i}})\). Hence we have,

\[_{^{}}^{}}{}g_{}(f,C)=_{i}g_{}(f,C_{i})_{i}g_ {}(f,C_{ij_{i}})_{^{}}}{ }g_{}(f,C)\]

Putting everything together, for every \(f\) we have,

\[_{^{}}^{}}{}g_{}(f,C)=_{^{}}}{}g_{}(f,C),\]

which concludes the claim. 

Now Let us define the Convex Structured Primary Problem.

\[_{^{}}^{}}{}[(C(X))]\] \[[f(X)C(X,Y)-(1-) }]=0, f\]

As the Convex Structured Primary Problem is a linear minimization over a convex set, \(_{^{}}^{}\), with finitely many linear constraints, strong duality holds for this problem (See Theorem 1, Section 8.3 of ; Also see Problem 7 in Chapter 8 of the same reference), which means Convex Structured Primary Problem is equivalent to Convex Structured Minimax Problem (Here we are using the fact that \(_{S|X}\) is continuous hence the Convex Structured Primary Problem is feasible). Now putting everything together we have,

\[}{}\] \[\] \[-\]

The negative sign appeared as our definition of \(_{}(f,h)\) has a minus sign with respect to the conventional definition of Lagrangian.

**Lemma E.5**: _Given a random variable \(Z\) taking values in \(\) and \([Z]>0\), we have,_

\[(Z).\]

_Proof._ Let us define \(=(Z)\). Now we have,

\[[Z] 1+(1-).\]

Hence,

\[(1-) 2.\]

Therefore,

\[}{1-}.\] (10)

\(\)

**Lemma E.6**: _Consider \(>0,>>0\) and \(N\) numbers \(z_{1},,z_{N}\) such that, \(_{i=1}^{N}z_{i}=\) and \(z_{i}[0,]\) for every \(1 i N\). Then there exists a subset \(S[N]\) such that,_

\[_{i S}z_{i}[}^{}}{2},2^{}^{}].\] (11)

_Proof._ Let us define for every \(1 i N\),

\[y_{i}=z_{i}&\;^{} ^{},\\ 0&\;1-^{}^{},\] (12)

and assume that \(y_{i}\)s are independently generated. By Hoeffding inequality we have,

\[(|_{i=1}^{N}y_{i}-_{i=1}^{N}[y_{i}]| ) 2}{_{i=1}^{N}z_{i}^{2}} }.\]

This results in,

\[(|_{i=1}^{N}y_{i}-^{}^{ }|) 2}{ }}.\]

Now setting \(=^{}^{}\), we get,

\[(|_{i=1}^{N}y_{i}-^{}^{ {1}{4}}|^{}^{}) 2 }}}<1.\]

This means there exists a deterministic realization that satisfies 11. \(\)

## Appendix F Proofs of Section 3

Here we provide a version of Theorem 3.5, which does not need the assumption 1. Before that, let us remind that each element \(f\) can be represented by a \(^{d}\), where we use the notation \(f_{}(x)=,(x)\) (look at section 2.1 for more details).

**Theorem F.1**: _Let us assume \(_{S|}\) and \(_{}\) are continuous. Let \(\) be a bounded finite-dimensional affine class of covariate shifts and \(\) be the class of all measurable functions. Let \(f^{*}\) denote the optimal solution to the outer minimization and \(\) denotes the optimal value of the Relaxed Minimax Problem 3.2. Finally, let \(L^{*}\) denotes the optimal value for the Relaxed Primary Problem 3.3. For every \(>0\), there exists \(h^{*}\) such that,__(i) \(|g_{}(f^{*},h^{*})-|\)._

_(ii) For every_ \(f_{}\) _we have,_

\[\,[f_{}(X_{n+1})[Y_{n+1} C _{h^{*}}^{S}(X_{n+1})]-(1-)}]||||_{1}.\]

_(iii) \((C_{h^{*}}^{S}(X)) L^{*}+\)._

Put it simply, Theorem F.1 says fixing any \(>0\), there is an \(\)-close optimal solution of the Relaxed Minimax Problem (statement (i)) such that it is \(\)-close to the feasible solutions of the Relaxed Primary Problem which have perfect conditional coverage (statement (ii)), and it achieves an at most \(\)-larger prediction set length compare to the smallest possible, which is the solution of Relaxed Minimax Problem (statement (iii)).

**Proof of Theorem F.1:**Let us define \(^{}\) as the class of all measurable functions from \(\) to \(\). Now let us restate the Structured Minimax Problem for \(^{}\).

**Structured Minimax Problem:**

\[}{}g_{}(f,h).\]

We can now rewrite this problem in the equivalent form of the following, where we change the domain of the maximization from \(_{}\) to corresponding set functions. Let \(_{^{}}\) be the set of all function from \(C(x,y):\) such that there exists \(h^{}\) such that \(C(x,y)=1[S(x,y) h(x)]\).

**Structured Minimax Problem:**

\[}{}g_{}(f,C).\]

We now proceed by defining a convexified version of this problem. Let us define

\[_{^{}}^{}=\{_{i=1}^{T}a_{i}C_{i}(x, y)\ |\ _{i=1}^{T}a_{i}=1,a_{i} 0( 1 i T),C_{i}_{ ^{}}( 1 i T),T\}.\]

Now we can define the following problem.

**Convex Structured Minimax Problem:**

\[}{}g_{}(f,C).\]

Now applying lemma E.4 we have,

**Structured Minimax Problem**

\[\]

**Convex Structured Minimax Problem**

\[\]

\[-\]

The negative sign appears because our definition of \(_{}(f,h)\) has a minus sign with respect to the conventional definition of Lagrangian. Let us call the optimal value for all these three problems OPT (here pay attention that based off of our definition OPT is always a negative number, hence when we are addressing the optimal length the term \(-\)OPT shows up). With some abuse of notation, Let us assume \(\{C_{i}\}_{i=1}^{}\), where \(C_{i}_{^{}}^{}\) is a feasible sequence of Convex Structured Primary Problem which achieves the optimal value in the limit, i.e, \(_{i}g_{}(f,C_{i})=-\). This means, for any \( 0\), there is an index \(i_{}\) such that \(|\,(C_{i_{}})+|\). Let us fix the value of \(\) for now, we will determine its value later. By definition, there should be \(\{}\}_{i=1}^{t}\) and corresponding \(\{h_{i}\}_{i=1}^{t}\) such that \(}_{^{}}\), \(h_{i}^{}\), \(}=[S(x,y) h_{i}(x)]\), and we have \(C_{i_{}}=_{i=1}^{t}a_{i}}(x,y)\) and \(_{i=1}^{t}a_{i}=1,a_{i} 0\ ( 1 i t)\).

Now Let us define \(L_{i}=}(_{i})\) for every \(1 i t\) (all of these expectations are finite cause \(|\,}(C_{i_{}})+|\)). Now if we look at the truncations of these expectations, by Dominated Convergence Theorem we have, \([(_{i})[(_{i})>k]] 0\).

This means, for any \( 0\), there exists a real valued number \(_{1}\) such that we can define the set \(_{_{1}}\) so that (i) for every \(1 i t\) and for every \(x_{_{1}}\), \((_{i}(x))_{1}\), and (ii) \([(_{i_{}})[x_{ _{1}}]]\).

Now remind that \(=^{p}\). Since we know \(_{X}\) is continuous then by Dominated Convergence Theorem there exists large enough real value \(_{2}\) such that \((X(0,_{2}))\) where \((0,_{2})\) is the ball with radius \(_{2}\) and \(\) is a positive real number that we will determine later.

Similarly we can again look at \(_{ik}=L_{i}[||X||_{2} k]\). By Dominated Convergence Theorem there is a \(_{3}\) such that for every \(1 i t\) we have \([L_{i}[||X||_{2}_{3}]]\).

Let \(R=\{_{1},_{2},_{3},1\}\) and \(A=(0,R) L_{R}\).

Now the rest of the proof proceed as follows. We will construct \(h^{*}^{}\), and as a result the corresponding set \(C^{*}(x,y)=[S(x,y) h^{*}(x)]\), in a way that coverage properties and the length of \(C^{*}\) is close to the ones for \(C_{i_{}}\). Let us proceed with the construction of \(h^{*}\).

**case 1:**\(x A\). For this case we define \(h^{*}(x)=0\).

**case 2:**\(x A\). This case is more involved. Let us fix \(_{1} 0\). Let \(\{(b_{i},_{1})\}_{i=1}^{N}\) be a minimal \(_{1}\) covering for \(A\). We can then further prune this covering balls to \(\{W_{i}\}_{i=1}^{N}\), where (i) \(W_{i} W_{j}=\) for every \(i j\), (ii) \(_{i=1}^{N}W_{i}=A\), and (iii) \(W_{i}(b_{i},_{1})\) for every \(i\). Now we use the following randomized method to prove a desirable construction for \(h^{*}\) exists. Let \(\{Z_{i}\}_{i=1}^{}\) be a collection of uniform iid discrete random variables where they take values between \(1\) and \(t\). Then we define the random function \(h_{rand}(x)\) inside \(A\) in the following way. By construction, for each \(x A\) there is a unique \(W_{i}\) that includes \(x\). We set \(h_{rand}(x)=h_{Z_{i}}(x)\). We denote the corresponding set function to \(h_{rand}\) by \(C_{rand}\). Now Let us remind that \(F\) is a finite dimensional affine class of functions. In particular, let \((x)=[_{1}(x),,_{d}(x)]\) be a _predefined_ function (a.k.a. the finite-dimensional basis). The class \(\) is defined as \(=\{,(x)| ^{d}\}\). Now for each \(k[1,,d]\) we can do the following calculations.

\[|}{}| _{i=1}^{N}_{k}(X)(C_{rand}(X,Y)-(1-))[X  W_{i}]|\] \[=|}{}|_{k}(X)(C_{ rand}(X,Y)-(1-))[X A]|\] \[=|}|_{k}(X)(_{ i_{}}(x,y)-(1-))[X A]|\] \[=|0-}|_{k}(X)(C_{i_{ }}(X,Y)-(1-))[X A]|\] \[}[|_{k}(X)| |(C_{i_{}}(X,Y)-(1-))||[X A] |]\] \[ B\,\,[X A]\] \[ B(\,(0,R)+\,L_{R})\] \[ 2B,\]

where \(B\) is the upper bound for \(_{k}\). We now define the random variable

\[E_{i}=}[_{k}(X)(C_{rand}(X,Y)-(1-)) [X W_{i}]]\]for every \(1 i N\). The computation above indicates a bound on the expectation of the sum of these variables. By the construction of \(C_{rand}\) we know that they are independent. Furthermore, each random variable \(E_{i}\) is bounded by the quantity \(2BM((0,_{1}))\), where \(B\) is the upper bound for \(_{k}\), \(M=_{x(0,R)}p(x)\) (which is finite as \((0,R)\) is a compact set and we assumed \(_{}\) is continuous), and \((0,_{1})\) appears as a result of \(W_{i}(b_{i},_{1})\). Hence, we can apply Hoeffding inequality for general bounded random variables and derive for every \( 0\),

\[(|_{i=1}^{N}E_{i}-[_{i=1}^{N}E_{i}]|< ) 1-2}{4NB^{2}M^{2}((0,_{1}))^{2}}}.\]

This results in,

\[(|}_{k}(X)(C_{rand}( X,Y)- (1-))[X A]|<+2B)\] \[ 1-2}{4NB^{2}M^{2} ((0,_{1}))^{2}}}.\]

Furthermore,

\[|}_{k}(X)(C_{rand}(X,Y)- (1-))[X A]|\] \[=}|_{k}(X)| (C_{rand}(X,Y)-(1-))[X A]\] \[ B\,\,[X A]\] \[ B(\,(0,R)+\,L_{R})\] \[ 2B.\]

Therefore,

\[(|}_{k}(X)(C_{rand}(X, Y)- (1-))|<+4B)\] \[ 1-2}{4NB^{2}M^{2}((0,_{1}))^{2}}}.\]

Now since we have this argument for each \(1 k d\), by a union bound we have,

\[( k:1 k d:\ |} _{k}(X)(C_{rand}(X,Y)- (1-))|<+4B)\] (13) \[ 1-2d}{4NB^{2}M^{2} ((0,_{1}))^{2}}}.\]

Now one can argue a similar inequality should hold for length too. This time we can define \(Q_{i}=}[_{}C_{rand}(X,y)dy[X W_{i}]]\). Now these variables are also independent and as a result of the construction of the covering set they are bounded by \(RM((0,_{1}))\). We also have,

\[|}{}_{i=1}^{N}Q_{i} [X W_{i}]+| =|}{}_{}C_{rand} (X,y)dy[X A]+|\] \[=|}_{}C_{i_{ }}(X,y)dy[X A]-|\] \[=|}_{}C_{i_{ }}(X,y)dy+|\] \[-}_{}C_{i_{ }}(X,y)dy[X A]|\] \[+}_{}C_{i_ {}}(X,y)dy[X A]\] \[+}_{}C_{i_ {}}(X,y)dy[X(0,R)]\] \[+}_{}C_{i_{ }}(X,y)dy[X L_{R})]\] \[ 3.\]

Now again by applying Hoeffding inequality for general bounded random variables we have for every \( 0\),

\[(|_{i=1}^{N}Q_{i}-[_{i=1}^{N}Q_{i}]|< ) 1-2}{NR^{2}M^{2}((0,_{1}))^{2}}}.\]

This results in,

\[(|}_{}C_{rand}(X,y)dy [X A]+|<4) 1-2}{NR^{2}M^{2}((0,_{1}))^{2}}}.\]

Furthermore, by construction of \(C_{rand}\),

\[|}_{}C_{rand}(X,y)dy[X  A]|=0.\]

Therefore,

\[(|}_{}C_{rand}(X,y)dy+|<4) 1-2}{NR^{2}M^{2} ((0,_{1}))^{2}}}.\] (14)

Now a union bound between (13) and (14) leads to,

\[ k:1 k d:\ |}[_{k}(X)(C_{rand}(X,Y)-(1-))]|<+ 4B\] \[|}_{}C_{ rand}(X,y)dy+|<4\] \[ 1-2d}{4NB^{2}M^{2}((0,_{1}))^{2}}}-2}{NR^{2}M^{2} ((0,_{1}))^{2}}}.\]

Recalling, \(N=(A,||.||_{2},_{1})\), where \(\) denotes the covering number, we have the following inequality (look at chapter 4 of ),

\[N(})^{p}(A)}{((0,1))},((0,_{1}))=_{1}^{p}((0,1)).\]This results in,

\[( k:1 k d:\ |} .|_{k}(X)(C_{rand}(X,Y)-(1-))<+4B \] \[|}_{}C_{rand }(X,y)dy+|<4\] \[ 1-2d}{3^{p}(A)B^ {2}M^{2}_{1}^{p}((0,1))}}-2 }{3^{p}(A)R^{2}M^{2}_{1}^{p}((0,1))}}.\]

Since we have this inequality for every \(_{1}>0\), we can pick a small enough \(_{1}\) such that,

\[( k:1 k d:\ |} .|_{k}(X)(C_{rand}(X,Y)-(1-))< +4B\] \[|}_{}C_{rand }(X,y)dy+|<4.\]

This means there is a realization of \(h_{rand}\) and accordingly \(C_{rand}\), which we denote them by \(h^{*}\) and \(C^{*}\) such that,

\[ k:1 k d:\ |} .[_{k}(X)(C^{*}(X,Y)-(1-))]<+4B \] \[|}_{ }C^{*}(X,y)dy+|<4.\]

Now since we proved this for any \(>0\), we can put \(=^{{}^{}}\{,\}\). Therefore the following statement holds. For every \(^{{}^{}}>0\) there exists \(h^{*}^{}\) and its corresponding set function \(C^{*}(x,y)=[S(x,y) h^{*}(x)]\) such that,

\[ k:1 k d:\ |} .[_{k}(X)(C^{*}(X,Y)-(1-))]<^{{ }^{}}\] \[|}_{ }C^{*}(X,y)dy+|<^{{}^{}}.\]

This immediately proves the statement (ii) of the Theorem 3.5. The statement (iii) also follows by the fact that by weak duality between the Relaxed Minimax Problem and the Relaxed Primary Problem we have \(- L^{*}\). Finally, let \(f^{*}\) denotes an optimal solution to the outer minimization of the Relaxed Minimax Problem. There exists a \(^{*}\) such that \(f^{*}(x)=^{*},(x)\). Now we have,

\[|g_{}(f^{*},h^{*})-|=|\, [^{*},(X)C^{*}(X,Y)-(1- )}]-_{}C^{*}(X,y)dy|\] \[|\,[ {}^{*},(X)C^{*}(X,Y)-(1-)}] +|\,_{}C^{*}(X,y)dy+|\] \[^{{}^{}}| {}^{*}.|_{1}+^{{}^{}}.\]

Hence \(^{{}^{}}^{*} \|_{1}}\) prove the statement (i) of the Theorem 3.5.

**Proof of Theorem 3.5:** Let us define \(^{}\) as the class of all measurable functions from \(\) to \(\). Now let us restate the Structured Minimax Problem for \(^{}\).

**Structured Minimax Problem:**

\[}{}\ ^{}}{ }\ g_{}(f,h).\]

We can now rewrite this problem in the equivalent form of the following, where we change the domain of the maximization from \(_{}\) to corresponding set functions. Let \(_{^{}}\) be the set of all function from \(C(x,y):\) such that there exists \(h^{}\) such that \(C(x,y)=1[S(x,y) h(x)]\).

**Structured Minimax Problem:**

\[}{}g_{}(f,C).\]

We now proceed by defining a convexified version of this problem. Let us define

\[_{^{}}^{}=\{_{i=1}^{T}a_{i}C_{i}(x,y )\ |\ _{i=1}^{T}a_{i}=1,a_{i} 0( 1 i T),C_{i}_{ ^{}}( 1 i T),T\}.\]

Now we can define the following problem.

**Convex Structured Minimax Problem:**

\[}{}g_{}(f,C).\]

Naturally we can also define the Convex Structured Primary Problem.

**Convex Structured Primary Problem:**

\[_{^{}}^{}}{} [(C(X))]\] subject to \[[f(X)C(X,Y)-(1-)}]=0,  f\]

Now applying lemma E.4 we have,

**Structured Minimax Problem**

\[\]

**Convex Structured Minimax Problem**

\[\]

\[-\]

Let us call the optimal value for all these three problems OPT (here pay attention that based off of our definition OPT is always a negative number, hence when we are addressing the optimal length the term \(-\) shows up). With some abuse of notation, Let us assume \(\{C_{i}\}_{i=1}^{}\), where \(C_{i}_{^{}}^{}\) is a feasible sequence of Convex Structured Primary Problem which achieves the optimal value in the limit, i.e, \(_{i}g_{}(f,C_{i})=-\). This means, for any \( 0\), there is an index \(i_{}\) such that \(|}(C_{i_{}})+|\). Let us fix the value of \(\) for now, we will determine its value later. By definition, there should be \(\{_{i}\}_{i=1}^{t}\) and corresponding \(\{h_{i}\}_{i=1}^{t}\) such that \(_{i}_{^{}}\), \(h_{i}^{}\), \(_{i}=[S(x,y) h_{i}(x)]\), and we have \(C_{i_{}}=_{i=1}^{t}a_{i}_{i}(x,y)\) and \(_{i=1}^{t}a_{i}=1,a_{i} 0\ ( 1 i t)\).

Let us recall that the class \(\) is defined as \(=\{,(x)|^{d}\}\), where \(:^{d}\) is a _predefined_ function (a.k.a. the finite-dimensional basis). Now let \(\{_{i}\}_{i=1}^{}\) be all the possible countable values that it takes. In other words, each \(_{i}\) is a vector in \(^{d}\) and there exists \(x\) such that \((x)=_{i}\). Then we have,

\[_{X}_{i=1}^{t}a_{i}(x)(_{}[S(x,y)  h_{i}(x)]p(y|x)dy)p(x)dx=(1-)1\\ 1\\ \\ 1.\] (15)

Here we implicitly assumed that \((x)\) is properly normalized so that the Right hand side don't need any extra normalization. Let us define,

\[_{i}(x)=_{}[S(x,y) h_{i}(x)]p(y|x)dy,\]\[l_{i}(x)=_{}[S(x,y) h_{i}(x)]dy.\]

from (15) we have,

\[_{}_{i=1}^{t}a_{i}(x)_{i}(x)p(x)dx=(1-) 1\\ 1\\ \\ 1.\]

We can then write,

\[_{j=1}^{}_{j}_{_{j}}(_{i=1}^{t}a_{i} _{i}(x))p(x)dx=(1-)1\\ 1\\ \\ 1,\] (16)

where \(_{j}=\{x(x)=_{j}\}\). Now two points are in order. (i) Without loss of generality, one can assume \(p(_{j})>0\) for every \(j\) as otherwise we can just omit that \(_{j}\) since it does not contribute to any integral due to continuity assumptions. (ii) \(\{_{j}\}_{j=1}^{}\) partitions \(\), i.e., their union is \(\) and they are pairwise disjoint. Now the idea behind the rest of the proof is we show for each region \(_{j}\), there is a single function \(h\) such that,

\[(i)_{_{j}}(_{i=1}^{t}a_{i}_{i} (x))p(x)dx=_{_{i}}_{h}(x)p(x)dx,\] \[_{h}(x)=_{}[S(x,y)  h(x)]p(y|x)dy.\] \[(ii)_{_{j}}l_{h}(x)p(x)dx_{_{j }}(_{i=1}^{t}a_{i}l_{i}(x))p(x)dx+ p(_{j }),\] \[l_{h}(x)=_{}[S(x,y) h(x)]dy.\]

Hence, from now on we fix an arbitrary \(_{j}\) and prove the existence of such \(h\). For the ease of notation let us define \(}=_{j}\) and \((x)=})}\). This way \(_{}}(x)=1\) and \((x)\) is still continuous.

Now we can rewrite our goals with this new notation. With a simple normalization we have,

\[(i)_{}}(_{i=1}^{t}a_{i} _{i}(x))(x)dx=_{}}_{h}(x) (x)dx,\] \[_{h}(x)=_{}[S(x,y)  h(x)]p(y|x)dy.\] \[(ii)_{}}l_{h}(x)(x)dx_{ }}(_{i=1}^{t}a_{i}l_{i}(x))(x)dx+ ,\] \[l_{h}(x)=_{}[S(x,y)  h(x)]dy.\]

Now, as a result of Dominated Convergence Theorem, there is a large enough real value \(R\) such that for the set of \(}_{1}=\{x}||x||_{2} R,_{i [1,,t]}l_{i}(x) R\}\) we have,

\[_{i=1}^{t}_{}}l_{i}(x)[x}_{1}](x)dx_{1},\] (17) \[_{i=1}^{t}_{}}_{i}(x) [x}_{1}](x)dx_{1},\] (18)where the value of \(_{1}>0\) will be chosen later in a way that it would be small enough for the proof to work. Among the functions \(_{1},,_{t}\) there should be at least two of them, without loss of generality \(_{1}\) and \(_{2}\) so that \(_{}}_{1}(x)(x)dx_{}}_{2}(x)(x)dx+\) where \(>0\). Otherwise we have already achieved our goal by picking the \(h_{i}\) with the smallest average length. Now assume \(_{1}<\) (we will make sure to pick \(_{1}\) in a way that it satisfies this condition). Hence we have,

\[_{}_{1}}_{1}(x)(x)dx_{}_{1}}_{2}(x)(x)dx+.\] (19)

Now applying Lemma E.5 there exists \(}_{2}}_{1}\) such that \((}_{2})\) and for every \(x}_{2}\) we have \(_{1}(x)_{2}(x)+\). (Here one have to use Lemma E.5 using \(Z=(_{1}(x)-_{2}(x))\,[_{1}(x) _{2}(x)][x}_{1}]\))

Let us consider the following three sets that partition the space \(}\),

\[A =}_{2}\] (20) \[B =}_{1}}_{2}\] \[C =}}_{1}\]

Note that these three sets are pair-wise disjoint and cover the space \(}\). LEt us first consider the set \(C\). we would like to consider a function \(h_{C}\) such that,

\[_{C}(_{i=1}^{t}a_{i}_{i}(x))(x)dx=_{C}_{h_{C}}(x)(x)dx,\] \[_{h_{C}}(x)=_{}[S(x,y)  h_{C}(x)]p(y|x)dy.\]

To do so, without loss of generality, we assume,

\[_{C}_{1}(x)(x)dx_{C}_{2}(x)(x)dx_{C}_{t}(x)(x)dx.\]

Hence,

\[_{C}_{1}(x)(x)dx_{C}(_{i=1}^{t}a_{i} _{i}(x))(x)dx_{C}_{t}(x)(x)dx.\]

For \(r[0,)\) let,

\[f(r)=_{C(0,r)}_{t}(x)(x)dx+_{C (0,r)}_{1}(x)(x)dx.\]

Note that \(f(0)=_{C}_{1}(x)(x)dx\) and \(f()=_{C}_{t}(x)(x)dx.\) Also, as a result of continuity assumptions, \(f(r)\) is a continuous function, therefore we can apply Intermediate Value Theorem. As a result, there should be \(r_{0}<\) such that,

\[f(r_{0})=_{C}(_{i=1}^{t}a_{i}_{i}(x))(x)dx.\]

We then naturally pick \(h_{c}\) to be,

\[h_{C}(x)=h_{t}(x)&x C(0,r),\\ h_{1}(x)&x C(0,r).\] (21)

Let us now consider the sets \(A\) and \(B\). Note that by construction they both are a subset of \((0,R)\). Now let us consider a \(\)-net for \(A\) and a separate \(\)-net for \(B\). Similar to our arguments in the proof of Theorem F.1, we can construct these \(\)-nets in a way that they partition \(A\) and \(B\). That is to say we have, \(A=_{j=1}^{N_{A}}A_{j}\) and \(B=_{j=1}^{N_{B}}B_{j}\) such that withing each \(\)-net the pair wise intersections are empty. Now, for each \(A_{j}\) (and \(B_{j}\)) we choose independently one of \(\{h_{i}\}_{i=1}^{t}\) with probabilities \(\{a_{i}\}_{i=1}^{t}\) at random.

Let \(J(A,j)\) (similarly \(J(B,j)\)) be the index of the function \(h_{i}\) assigned to \(A_{j}\) (similarly \(B_{j}\)). Then we have,

**Event 1:**

\[|_{j=1}^{N_{A}}_{A_{j}}_{h_{J(A,j)}}(x)dx+_{j=1}^{N_{B}}_{B_{j}}_{h_{J(B,j)}}(x)dx-_ {A B}_{i=1}^{t}a_{i}_{i}(x)(x)dx|\] \[.\]

**Event 2:**

\[_{j:J(A,j)=1}(A_{j})}{2}(A).\]

**Event 3:**

\[_{j:J(A,j)=2}(A_{j})}{2}(A).\]

Now we can repeat the probabilistic argument of proof of Theorem F.1 here. The key insight is all of the three above-mentioned events are high probability events, in a way that by letting \(\) (the precision of the covering net) to be sufficiently small then the probability of these events happening approaches to 1. Hence, there is a deterministic realization that make all these events happen. Now on, we fix that deterministic realization. Particularly, this means we have a realization that satisfies all the events for arbitrary small \(\). Now the idea is to make a small change in the configuration that comes from this realization that make the approximate coverage of event 1 to be an exact coverage. With a little abuse of notation, let \(J(A,j)\) (similarly \(J(B,j)\)) be the assignments of that deterministic realization. Without loss of generality, we can assume,

\[|_{j=1}^{N_{A}}_{A_{j}}_{h_{J(A,j)}}(x)dx+ _{j=1}^{N_{B}}_{B_{j}}_{h_{J(B,j)}}(x)dx|= _{A B}_{i=1}^{t}a_{i}_{i}(x)(x)dx-_{ 2},\] (22)

where \(_{2} 0\) and \(_{2}\) (the case of \(_{2} 0\) can be similarly handled). That is to say, our deterministic assignment \(J(A,j)\) and \(J(B,j)\) led to a small under-coverage of \(_{2}\). The idea now is to "engineer" the assignments of some of the \(A_{j}\)s in a way that we make the coverage exact while not changing the average length of the current assignment significantly. Remind the definition the event 3 we defined above. Recall that \(a_{2}\) and \((A)\) are strictly positive numbers. Hence, as a result of (19) and (20) we have, \((A_{2})\). Now we can argue through Lemma E.6. Consider the set \(Q=\{j[N_{a}] J(A,j)=2\}\). We know from event 3 that \((Q)}{2}(A)}{24}\). Now let for every \(j Q:Z_{j}=(A_{j})\). We know that \(Z_{j}\) and \(^{{}^{}}_{j Q}Z_{j}}{24}\). Applying Lemma E.6 there exists a \(Q^{{}^{}} Q\) such that \(_{j Q^{{}^{}}}(A_{j})[^{}} ^{}}{2},2^{{}^{}}^{ }]\). Recall that since \(Q^{{}^{}} Q\), then for any \(j Q^{{}^{}}\) we have \(J(A,j)=2\). now if we reassign all the \(A_{j}\)'s, \(j Q^{{}^{}}\), to \(h_{1}\) (i,e changing \(J(A,j)\) to \(1\) for every \(j Q^{{}^{}}\)) then the amount of added coverage would be, \(_{j S^{{}^{}}}(A_{j})\), where \(\) appears following (19) and the fact that \(A_{i}}_{2}\). Hence, the amount of added coverage would be bounded by,

\[_{j S^{{}^{}}}(A_{j})^{}}^{}}{2}}{24}}^{}}^{}}{2}.\] (23)

Recall that we can pick \(\) as small as we want. Hence, we can pick \(\) small enough that we have,

\[}{24}}^{}}^{}}{2} _{2},\] (24)

where the last inequality follows from the definition of \(_{2}\) in (22). This can be done by letting \((}{24}}^{}}}{2} )^{4}\). Now by noting (23) and (24) it should be clear that by reassigning all any \(A_{j}\)\(j Q^{{}^{}}\), the total coverage added will be larger than \(_{2}\) (hence in total we will over-cover). Now we can apply Intermediate Value Theorem once again. Let us define for \(r[0,]\),

\[f(r)=_{j Q^{{}^{}}}_{A_{j}(0,r)}_{1}(x )(x)dx+_{A_{j}(0,r)}_{2}(x)( x)dx.\]

Here note that again as a consequence of continuity assumptions \(f\) is a continuous function. Also, \(f(R)-f(0)_{2}\). Hence there exists \(r_{0}[0,R]\) such that, \(f(r_{0})-f(0)=_{2}\). That is to say we can make the coverage exactly valid. Now let us analyze the deviation in length. Not that for \(x A B\) we have length is bounded by \(R\). This means by reassigning the elements in the set \(A_{j}\), \(j Q^{{}^{}}\), the change in length is at most \(R_{j Q^{{}^{}}}(A_{j}) 2R^{{}^{} }^{}\), where the last inequaity comes from the fact that \(_{j Q^{{}^{}}}(A_{j})[^{} }^{}}{2},2^{{}^{}}^ {}]\). Here again by choosing \(\) small enough, particularly \((^{}}})^ {4}\), we can ensure the change in length is smaller than \(\), hence we achieved our goal.

**Proof of Proposition 3.7:** Let \(h^{*}\) be an optimal solution of the Relaxed Minimax Problem and the Relaxed Primary Problem, when the optimization is over all the measurable functions from \(\) to \(\). Let us denote the corresponding optimal solution to the outer minimization of the Relaxed Minimax Problem by \(f^{*}\). Recall that \(\) is a finite dimensional affine class of functions. That is to say there exists a vector \(^{*}\) such that \(f^{*}(x)=^{*},(x) f_{^{*}}(x)\). Since \(h^{*}\) is a solution of the Relaxed Primary Problem so it should be conditionally valid with respect to class \(\). Therefore, as a result of Lemma 3.4, we should have,

\[_{}.g_{}(f_{},h^{*}) |_{^{*}}=.\] (25)

Now define \(()=_{}\,g_{}(f_{ },h_{})\). Since \(\) is a compact set and \(g_{}(f_{},h_{})\) is convex (in fact linear) in \(\) so the Danskin's theorem applies to \(()\). Therefore, as a result of (25),

\[_{}.() |_{^{*}}.\] (26)

Now again as a result of Danskin's theorem, \(()\) is convex in \(\). Therefore, as a consequence of (26), \(^{*}\) is a minimizer of \(()\). That is to say, \((f^{*},h^{*})\) is an optimal solution to the Relaxed Minimax Problem. On the other hand, \(h^{*}\) is also a solution to the Relaxed Primary Problem (as it is a solution to the Relaxed Primary Problem when solved over all the measurable functions). Putting everything together, \(h^{*}\) is a joint optimal solution to both the Relaxed Minimax Problem and the Relaxed Primary Problem, hence we have strong duality.

**Proof of Proposition 3.3:** Let us start by recalling the definitions,

\[g_{}(f,C)=[f(X)[Y C(X)]-(1- )}]-_{}[y C(X)]dy\]

where

\[C_{f}(x)=\{y f(x)p(y|x) 1\}.\]

Now, fixing \(f\), all we have to show is that \(g_{}(f,C_{f}(x))-g_{}(f,C(x)) 0\) for every \(C(x): 2^{}\).

**Claim 2**: \(g_{}(f,C_{f}(x))-g_{}(f,C(x)) 0\) _for every \(C(x): 2^{}\)_

_proof:_ We have,

\[g_{}(f,C_{f}(x))-g_{}(f,C(x))}{{=}}_{X}_{}(f(X)p(y|X)-1\{( [y C_{f}(X)]-[y C(X)])\,}dy\] \[}{{=}}_{X}_{}(f(X)p(y|X)-1)[y C_{f}(X) C(X)]- [y C(X) C_{f}(X)]}dy\] \[}{{=}}_{X}_{}(f(X)p(y|X)-1)[y C_{f}(X)  C(X)]}dy\] \[ 0,\]where, (a) follows from the definitions, (b) follows from the definition of set difference operation (\(\)) where, \(A B=\{x x Ax B\}\), and (c) comes from the definition of \(C_{f}(x)\). The proof is complete now. One can define,

\[_{X}[(C_{f},C)]_{X}_{} (f(X)p(y|X)-1)[y C_{f}(X) C(X)] }dy,\]

and rewrite the above calculation as,

\[g_{}(f,C(x))=g_{}(f,C_{f}(x))-_{X}[(C_{f},C)].\]

This reformulation is very intuitive. at a high level, it says maximizing \(g_{}(f,C(x))\) over \(C\), is equivalent to minimizing a distance between \(C\) and \(C_{f}\).

**Proof of Proposition 3.1:** Let us start by restating the proposition.

**Proposition F.2**: _The Primary Problem and the Minimax Problem are equivalent. Let \((f^{*},C^{*}(x))\) be the optimal solution of Minimax Problem. Then, \(C^{*}\) is also the optimal solution of the PP. Furthermore, \(C^{*}\) has the following form:_

\[C^{*}(x)=\{y f^{*}(x)p(y|x) 1\}\] (27)

Let us also recall the definition of Primary Problem,

 
**Primary Problem (PP):** & \\ Minimz\({}_{C(x)}\) & \([(C(X))]\) \\ subject to & \([f(X)[Y C(X)]-(1-)}]= 0, f\) \\  

Recall that in this paper we assume that \(=\{,(x)|^{d}\}\) is a finite dimensional affine class of functions (see section 2.1).

Assuming \(=[_{1},_{2},,_{d}]\). The Primary Problem can be equivalently written in terms of \(d\) linear constraints on the prediction sets \(C(x)\).

 
**Primary Problem- finite constraints:** & \\ Minimz\({}_{C(x)}\) & \([(C(X))]\) \\ subject to & \([_{i}(X)[Y C(X)]-(1-)} ]=0, i[1,d]\). \\  

We now take a closer look at the main optimization variable, i.e. the prediction sets \(C(x)\), and put it in the proper format. The prediction sets \(C(x)\) can be equivalently represented by a function \(C:\{0,1\}\) such that \(C(x,y)=[y C(x)]\). Furthermore, we can expand the optimization domain from \(C(x,y):\{0,1\}\) to \(C(x,y):\). One should pay attention that this change does not affect the optimal solutions as the solutions will be integer values. Putting everything together we can write the following problem,

 
**Linear PP** & \\ Minimz\({}_{C(x,y):}\) & \(_{}C(x,y)p(x)(y)dxdy\) \\ subject to & \([_{i}(X)C(X,Y)-(1-)}]=0,  i[1,d]\). \\  

Here \(\) is the lebesgue measure; recall from Section 2.1 that we defined length through the lebesgue measure on \(=\), which is equivalent to cardinality of a set in the case where \(\) is a discrete and finite set. This problem is a Linear program in terms of \(C\) with finitely many constraints. Also, the Linear PP, includes the Primary Problem in the sense that any solution to the Primary Problem is a feasible solution for Linear PP. That is to say, to prove Proposition 3.1, we just have to prove it for Linear PP.

For \(d=1\), Linear PP can be seen through the lens of the Neyman-Pearson Lemma in Hypothesis Testing . As a result, our analysis of the Linear PP (for general \(d\)) can be considered as an extension of this lemma which will be done using the KKT conditions.

Now, let us rewrite the Linear PP:

\[ _{}C(x,y)p(x)(y)dxdy\] \[ _{}_{i}(x)C(x,y)p(x,y)dxdy-(1- )=0, i[1,d]\] \[C(x,y) x,y\]

The above is a standard linear program on \(C(x,y)\). In what follows, we will find a closed-form solution using the dual of this program. Here, the "optimization variable" \(C(x,y)\) belongs to an infinite-dimensional space. Hence, in order to be fully rigorous, we will need to use the duality theory developed for general linear spaces that are not necessarily finite-dimensional. For a reader who is less familiar with infinite-dimensional spaces, what appears below is a direct extension of the duality theory (i.e. writing the Lagrangian) for the usual linear programs in finite-dimensional spaces.

Let \(\) be the set of all measurable function defined on \(\). Note that \(\) is a linear space. Let \(\) be the set of all the measurable functions on \(\) which are bounded between \(0\) and \(1\); I.e.

\[=\{CC:\}\] (28)

Note that \(\) is a convex set. We can then rewrite our linear program as follows:

\[ _{}C(x,y)p(x)(y)dxdy\] \[ _{}_{i}(x)C(x,y)p(x,y)dxdy-(1- )=0, i[1,d]\] \[C\]

Moreover, let us define the functional \(F:\) as

\[F(C)=_{}C(x,y)p(x)(y)dxdy,\] (29)

and also define, for \(i[1,d]\), the functional \(G_{i}:\) as

\[G_{i}(C)=_{}_{i}(x)C(x,y)p(x,y)dxdy-(1- ).\] (30)

Finally, we define the mapping \(:^{d}\) as

\[(C)=[G_{1}(C),G_{2}(C),,G_{d}(C)].\]

Note that \(G\) is a linear (and hence convex) mapping from \(\) to the Euclidean space \(^{d}\).

Using the above-defined notation, our linear program becomes:

\[ F(C)\] \[ (C)=\] \[C\]

where \(^{d}\) is the all-zero vector.

Note that the feasibility set of the above program is non-empty, as \(C(x,y)=1-\), for all \((x,y)\), is a feasible point. We can now use the duality theory of convex programs in vector spaces (See Theorem 1, Section 8.3 of ; Also see Problem 7 in Chapter 8 of the same reference). Specifically, let OPT be the optimal value achievable in the above linear program. Then, there exists a vector \(^{d}\) such that the following holds:

\[=_{C}\{F(C)-,(C) \},\] (31)where \(,(C)\) denotes the Euclidean inner-product of the two vectors \(,(C)^{d}\). Here, note that the vector \(\) is the usual Lagrange multiplier.

By denoting \((x)=(_{1}(x),_{2}(x),,_{d}(x))\), and using (30), we can write

\[,(C)=_{} ,(x) C(x,y)p(x,y)dxdy-(1-),,\]

where \(^{d}\) is the all-ones vector. As a result, by using (29), in order to solve the optimization in (31) we need to solve the following optimization:

\[_{C}\{_{}C(x,y)(p(x) (y)-,(x) p(x,y))dxdy\}+(1- ),.\]

And by noting the fact the \(p(x,y)=p(x)p(y x)\), and removing the term \((1-),\) which is independent of \(C\), our dual optimization becomes:

\[_{C}\{_{}C(x,y)( y)-,(x) p(y|x)p(x)dxdy\}\] (32)

Now, it is easy to see that as \(C(x,y)\) (due to the constraint \(C\)), the above optimization problem has a closed-form solution \(C^{*}\):

\[C^{*}(x,y)=1&,(x) p(y|x )>(y),\\ 0&,(x) p(y|x)<(y),\\ \{0,1\}&.\] (33)

Finally, note that in this paper the measure \(\) is considered to be the Lebesgue measure - see Section 2.1 - hence, up to a constant normalization factor that can be absorbed into \(\), we have \((y)=1\) for all \(y\).

The structure given in (33) is also sufficient. I.e. for any pair \((,C^{*})\) such that (i) \(C^{*}\) has the form given in (33); and (2) \(C^{*}\) satisfies the coverage-constraints \((C^{*})=0\), then \(C^{*}\) is an optimal solution of the Primary Problem(F). This sufficiency result follows again from the duality theory of linear programs in linear spaces; E.g. see Theorem 1 in Section 8.4 of . This necessary and sufficient condition is known as Strong Duality, which then results in the equivalence of Minimax Problem and Primary Problem, i.e we can change the order of min and max.

## Appendix G Proofs of Section 4

**Proof of Theorem 4.4:** Let us recall the algorithm.

 
**CPL:** & 
 Minimize \(f\) \\ where, \(C^{S}_{h}(x)=\{y S(x,y) h(x)\}\). \\  \\  \\ 

For the ease of notation let us call \((f,h)=_{,n}(f,h)\). let us also call the stationary solution to CPL by \(h^{*}_{}\) and \(f^{*}_{}\). Now since \(\) is a smooth function with respect to both of its arguments we have the following optimality condition,

\[(f^{*}_{}+ f,h^{*}_{ })_{=0}=0,f.\] (34)

Recall that \(\) is a \(d\)-dimensional affine class of functions over the basis \(=[_{1},,_{d}]\). We can then rewrite 34 as what follows.

\[(f^{*}_{}+_{j},h^{* }_{})_{=0}=0,j[1,,d].\] (35)

Now looking at the definition of \(g(f,h)=\)

\[_{i=1}^{n}f(x_{i})}[S(x_{i},y_{i} ),h(x_{i})]-(1-)}-_{i=1}^{n}_{}}[S(x_{i},y) h(x_{i})]-(1-)dy.\]We can take the derivative with respect to \(f\). Hence we can rewrite 35 as what follows:

\[_{i=1}^{n}[_{j}(x_{i})}[S(x_{i}, y_{i}),h_{}^{*}(x_{i})]-(1-)}]=0,j[1,,d].\] (36)

One can think of the mathematical term above as the smoothed version of coverage under covariate shift \(_{j}\). Now we can apply Lemma E.3. Therefore, fixing \(j[1,,d]\), with probability \(1-\) we have,

\[|_{i=1}^{n}[_{j}(x_{i}) }[S(x_{i},y_{i}),h^{*}(x_{i})]-(1-)}]- [_{j}(x)}[S(x,y),h_{}^ {*}(x)]-(1-)}].\] \[}+B(,d_{},)}{} )}}{}>0.\]

Combining with (36) we get,

\[|\,[_{j}(x)}[S(x,y),h_{ }^{*}(x)]-(1-)}]|}+B( ,d_{},)}{})}}{} >0.\] (37)

Union bounding over (36) we have with probability \(1-\) for any \(j[1,,d]\),

\[|\,[_{j}(x)}[S(x,y),h_{ }^{*}(x)]-(1-)}]|}+B( ,d_{},)}{})}}{} >0.\] (38)

In other words, we proved that the expected smoothed version of coverage is bounded. The last step that we have to take is then to prove that the expected smoothed coverage is actually close to the expected actual coverage. The following claim makes this precise.

**Claim 3**: _The following inequality holds for every \(j[1,,d]\)._

\[|\,[_{j}(x)}[S( x,y),h_{}^{*}(x)]-(1-)}]-[_{j}(x) [S(x,y) h_{}^{*}(x)]-(1-)} ]|\] \[ BL}\]

**Proof.**

\[|\,[_{j}(x)}[S( x,y),h_{}^{*}(x)]-(1-)}]-[_{j}(x) [S(x,y) h_{}^{*}(x)]-(1-)} ]|\] \[=|\,[_{j}(x)}[S(x,y),h_{ }^{*}(x)]]-[_{j}(x)[S(x,y) h _{}^{*}(x)]]|\] \[}{{}}[|_ {j}(x)|(}(S(x,y),h_{}^{*}(x))- [S(x,y) h_{}^{*}(x)])]\] \[}{{}}B\,[| (}(S(x,y),h_{}^{*}(x))-[S(x,y)  h_{}^{*}(x)])|]\] \[=B\,_{X}\,_{S|X}[|(}(S(x,y),h_{}^{*}(x))-[S(x,y) h_{}^{*}(x)])|]\] \[}{{}}B\,_{X}\,L }\] \[=BL},\]where (a) comes from triangle inequality, (b) is derived by the definition of \(B=_{i[1,,d]}_{x}_{i}(x)\), and (c) is followed by assumption 2 and Lemma E.2. Now combining Claim 3 and (37) we have with probability \(1-\),

\[|}[_{j}(x)[S(x,y) h_{}^{*}(x)]-(1-)}]| \] \[BL}+ }+B(,d_{ },)}{})}}{}>0.\]

Putting \(=}\) and \(=\) we have for every \(j[1,,d]\),

\[|}[_{j}(x)[S(x,y) h_{}^{*}(x)]-(1-)}]| B(,d_{}, )}{})}+BL}+}}{ }\]

Let us also remind that each element \(f\) can be represented by a \(^{d}\), where we use the notation \(f(x)=,(x) f_{}(x)\) (look at section 2.1 for more details). By linearity of the class \(\) we can conclude with probability \(1-\) for every \(f_{}\),

\[|}[f_{}(x) [S(x,y) h_{}^{*}(x)]-(1-)}] |||_{1}B(,d_{},)}{})}+|| ||_{1}BL}+||||_{1} }}{}.\]

## Appendix H Marginal Coverage Regression Experiment

In this section we aim at showcasing the ability of CPL in improving length efficiency in designing prediction sets with marginal coverage validity. We compare the performance of our method, CPL, in terms of marginal coverage and length, to various split conformal prediction (SCP) methodologies. Specifically, we compare with: (i) Split Conformal (SC)  and Jackknife , as the main methods in standard conformal prediction to achieve marginal validity; (ii) Local Split Conformal (Local-SC)  and LocalCP , as locally adaptive variants of Split Conformal; (iii) Conformalized Quantile Regression (CQR) , as a state-of-the-art method for achieving marginal coverage with small set size.

Following the setup from , we evaluate performance on 11 real-world regression datasets (see Appendix J) and report the average performance over all of them. Each dataset is standardized, and the response is rescaled by dividing it by its mean absolute value. We split each dataset into training (40%), calibration (40%), and test (20%) sets.

**First Part:** We focus on methods applicable to black-box predictors and conformity score. We compare SC, Jackknife, Local-SC, LocalCP, and CPL using the conformity score \(S(x,y)=|y-f(x)|\), where \(f\) is a NN with two hidden layers of 64 ReLU neurons, trained on the training set.

**Second Part:** We evaluate CQR, which requires quantile regressors trained on the training set. We also examine our method combined with CQR (i.e. we use the score obtained by CQR), referred to as CQR+CPL. Tables 1 and 2 summarize our results, reporting average performance over 100 random splits of the datasets. All reported numbers have standard deviations below \(1\) percent.

In Table 1, our method is shown to improve the interval length using a generic conformity score. In Table 2, our method shows strength with sophisticated scores, highlighting that our minimax

  Method & Length & Coverage \\  Split Conformal & 2.16 & 89.92 \\ Jacknife & 1.95 & 89.81 \\ Local-SC & 1.81 & 89.95 \\ LocalCP & 1.73 & 90.09 \\ CPL & **1.68** & **90.11** \\   
  Method & Length & Coverage \\  CQR+Local-SC & 1.59 & 90.15 \\ CQR+LocalCP & 1.48 & 90.01 \\ CQR+SC & 1.40 & 90.05 \\ CQR+Jackknife & 1.32 & 89.78 \\ CQR+CPL & **1.16** & **90.06** \\  

Table 1: First part procedure significantly enhances length efficiency across various tasks and scoring methods. Our framework's advantages over CQR are: (i) It can use CQR scores to improve length efficiency, and (ii) it can use other scores, including residual scores with pre-trained predictors. This is advantageous when training quantile regressions from scratch is costly, while pre-trained models are accessible.

## Appendix I CIFAR-10 Experiment

We conducted our experiments on the CIFAR-10 dataset, using two distinct training procedures: empirical risk minimization (ERM) with cross-entropy loss, and conformal training (ConfTr by ). The neural network architecture employed in all setups was ResNet-32. To evaluate the predictive performance of these models, we aimed to generate prediction sets that achieve a marginal coverage level of 95

For the conformal training procedure, we utilized a batch size of 500 and employed split conformal prediction to compute the prediction set sizes, which is a crucial component of the conformal training approach. We further fine-tuned the hyperparameters using grid search to optimize the setup.

We considered four experimental scenarios: training the model with either ERM or ConfTr, followed by calibration using either split conformal prediction or CPL. The primary goal was to compare the effectiveness of these approaches in terms of both coverage and prediction set efficiency.

In setups involving ConfTr, we further optimized the data split ratios to improve the length efficiency of the prediction sets, which explains the variation in train/calibration/test splits across the different ConfTr setups. For the ERM-based setups, no optimization was performed on the split ratios, as the focus was to emphasize the black-box nature of calibration approaches like CPL and split conformal prediction, which can act as wrappers around the trained models without altering the training procedure.

As shown in the results (see Table 3), CPL combined with ConfTr produces more efficient prediction sets in terms of average length compared to split conformal prediction. This highlights the effectiveness of CPL in improving length efficiency while maintaining the desired coverage level, particularly in conjunction with conformal training.

## Appendix J References for 11 datasets for Section H

Here we list all the datasets.

* MEPS-19 
* MEPS-20 
* MEPS-21 
* blog feedback (blog-data)
* physicochemical properties of protein tertiary structure (bio) 
* bike sharing (bike) 
* community and crimes (community) 
* Tennessee's student teacher achievement ratio (STAR) 
* concrete compressive strength (concrete) 
* Facebook comment volume variants one (facebook-1) 
* Facebook comment volume variants two (facebook-2) 

 
**Training** & **Calibration** & **Coverage** & **Avg Length** & **Samples (Train/Calib/Test)** & **Base Accuracy** \\  ERM & Split Conformal & 0.951 & 2.36 & 40k/10k/10k & 82.6\% \\  ERM & CPL & 0.948 & 2.06 & 40k/10k/10k & 82.6\% \\  ConfTr & Split Conformal & 0.954 & 2.11 & 45k/5k/10k & 82.3\% \\  ConfTr & CPL & 0.947 & 1.94 & 35k/15k/10k & 82.3\% \\  

Table 3: Comparison of different training and calibration methods

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: All the contributions and claims are properly explained in the introduction and abstract. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have a separate section called Limitation and Future work that we discuss the limitations of the current work and the possibilities for further research. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: All the assumptions are noted in the main body of paper in proper forms. We have moved all the proofs to Appendix section due to space limit. In the appendix, we have derived all the proofs carefully and either proved or gave proper references for all the technical details. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Our experimental setups are very easy to understand and well explained. We will also publish our codes for the camera ready version in case of acceptance. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We will publish our codes for the camera ready version in case of acceptance. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have explained all the details necessary for a self-contained experiment section. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Yes, we do report all the necessary statistics to have a fair comparison. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The running time for the experiments of this paper can also be done on CPUs (in the order of half a day). The point of view of our experiment section is regardless of compute time. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This paper has no foreseeable ethical issues. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: In this paper, we focus on developing a new algorithmic framework for Conformal Prediction which has immediate applications in areas such as healthcare. We do not anticipate any negative societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We don't see any foreseeable need for safeguards. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: We have properly cited all the datasets and prior works. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: There is no new asset as an output. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We do not perform such experiments. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: We do not do such studies. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.

* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.