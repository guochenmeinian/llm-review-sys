# FedFed: Feature Distillation against Data Heterogeneity in Federated Learning

Zhiqin Yang\({}^{1,2}\) Yonggang Zhang\({}^{2}\) Yu Zheng\({}^{3}\) Xinmei Tian\({}^{5}\)

**Hao Peng\({}^{1,6}\) Tongliang Liu\({}^{4}\) Bo Han\({}^{2}\)**

\({}^{1}\)Beihang University \({}^{2}\)Hong Kong Baptist University \({}^{3}\)Chinese University of Hong Kong

\({}^{4}\)Sydney AI Centre, The University of Sydney \({}^{5}\)University of Science and Technology of China

\({}^{6}\) Kunming University of Science and Technology

Equal contributions.Corresponding author (penghao@act.buaa.edu.cn)

###### Abstract

Federated learning (FL) typically faces data heterogeneity, i.e., distribution shifting among clients. Sharing clients' information has shown great potentiality in mitigating data heterogeneity, yet incurs a dilemma in preserving privacy and promoting model performance. To alleviate the dilemma, we raise a fundamental question: _Is it possible to share partial features in the data to tackle data heterogeneity?_ In this work, we give an affirmative answer to this question by proposing a novel approach called **Fed**erated **F**eature **d**istillation (FedFed). Specifically, FedFed partitions data into performance-sensitive features (i.e., greatly contributing to model performance) and performance-robust features (i.e., limitedly contributing to model performance). The performance-sensitive features are globally shared to mitigate data heterogeneity, while the performance-robust features are kept locally. FedFed enables clients to train models over local and shared data. Comprehensive experiments demonstrate the efficacy of FedFed in promoting model performance. The code is publicly available at: https://github.com/tmlr-group/FedFed

## 1 Introduction

Federated learning (FL), beneficial for training over multiple distributed data sources, has recently received increasing attention . In FL, many clients collaboratively train a global model by aggregating gradients (or model parameters) without the need to share local data. However, the major concern is heterogeneity issues  caused by Non-IID distribution of distributed data and diverse computing capability across clients. The heterogeneity issues can cause unstable model convergence and degraded prediction accuracy, hindering further FL deployments in practice .

To address the heterogeneity challenge, the seminal work, federated averaging (FedAvg), introduces the model aggregation of locally trained models . It addresses the diversity of computing and communication but still faces the issue of client drift induced by data heterogeneity . Therefore, a branch of works for defending data heterogeneity has been explored by devising new learning objectives , aggregation strategies , and constructing shareable information across clients . Among explorations as aforementioned, sharing clients' information has been considered to be a straightforward and promising approach to mitigate data heterogeneity .

However, the dilemma of preserving data privacy and promoting model performance hinders the practical effectiveness of the information-sharing strategy. Specifically, it shows that a limited amount of shared data could significantly improve model performance . Unfortunately, no matter for sharing raw data, synthesized data, logits, or statistical information  canincur privacy concerns . Injecting random noise to data provides provable security for protecting privacy . Yet, the primary concern for applying noise to data lies in performance degradation  as the injected noise negatively contributes to model performance. Consequently, effectively fulfilling the role of shared information necessitates addressing the dilemma of data privacy and model performance.

We revisit the purpose of sharing information to alleviate the dilemma in information sharing and performance improvements and ask:

_Q.1 Is it possible to share partial features in the data to mitigate heterogeneity in FL?_ This question is fundamental to reaching a new phase for mitigating heterogeneity with the information-sharing strategy. Inspired by the partition strategy of spurious feature and robust feature , the privacy and performance dilemma could be solved if the data features were separated into performance-robust and performance-sensitive parts without overlapping, where the performance-robust features contain almost all the information in the data. Namely, the performance-robust features that limitedly contribute to model performance are kept locally. Meanwhile, the performance-sensitive features that could contribute to model generalization are selected to be shared across clients. Accordingly, the server can construct a global dataset using the shared performance-sensitive features, which enables clients to train their models over the local and shared data.

_Q.2 How to divide data into performance-robust features and performance-sensitive features?_ The question is inherently related to the spirit of the information bottleneck (IB) method . In IB, ideal features should discard all information in data features except for the minimal sufficient information for generalization . Concerning the information-sharing and performance-improvement dilemma, the features discarded in IB may contain performance-robust features, i.e., private information, thus, they are unnecessary to be shared for heterogeneity mitigation. Meanwhile, the performance-sensitive features contain information for generalization, which are the minimum sufficient information and should be shared across clients for heterogeneity mitigation. Therefore, IB provides an information-theoretic perspective of dividing data features for heterogeneity mitigation in FL.

_Q.3 What if performance-sensitive features contain private information?_ This question lies in the fact that a non-trivial information-sharing strategy should contain necessary data information to mitigate the issue of data heterogeneity. That is, the information-sharing strategy unavoidably causes privacy risks. Fortunately, we can follow the conventional style in applying random noise to protect performance-sensitive features because the noise injection approach can provide a de facto standard way for provable security . Notably, applying random noise to performance-sensitive features differs from applying random noise for all data features. More specifically, sharing partial features in the data is more accessible to preserve privacy than sharing complete data features, which is fortunately consistent with our theoretical analysis, see Theorem 3.3 in Sec. 3.3.

**Our Solution.** Built upon the above analysis, we propose a novel framework, named **Fed**erated **F**eature **d**istillation (FedFed), to tackle data heterogeneity by generating and sharing performance-sensitive features. According to the question _Q.1_, FedFed introduces a competitive mechanism by decomposing data features \(^{d}\) with dimension \(d\) into performance-robust features \(_{r}^{d}\) and performance-sensitive features \(_{s}^{d}\), i.e., \(=_{r}+_{s}\). Then following the question _Q.2_, FedFed generates performance-robust features \(_{r}\) in an IB manner for data \(\). In line with _Q.3_, FedFed enables clients to securely share their protected features \(_{p}\) by applying random noise \(\) to performance sensitive features \(_{s}\), i.e., \(_{p}=_{s}+\), where \(\) is drawn from a Gaussian distribution \((,_{s}^{2})\) with variance \(_{s}^{2}\). To this end, the server can construct a global dataset to tackle data heterogeneity using the protected features \(_{p}\), enabling clients to train models over the local private and globally shared data.

We deploy FedFed on four popular FL algorithms, including FedAvg , FedProx , SCAF-FOLD , and FedNova . Atop them, we conduct comprehensive experiments on various scenarios regarding different amounts of clients, varying degrees of heterogeneity, and four datasets. Extensive results show that the FedFed achieves considerable performance gains in all settings.

Our contributions are summarized as follows:

1. We pose a foundation question to challenge the necessity of sharing all data features for mitigating heterogeneity in FL with information-sharing strategies. The question sheds light on solving the privacy and performance dilemma, in a way of sharing partial features that contribute to data heterogeneity mitigation (Sec 3.1)2. To solve the dilemma in information sharing and performance improvements, we propose a new framework FedFed. In FedFed, each client performs feature distillation--partitioning local data into performance-robust and performance-sensitive features (Sec 3.2) --and shares the latter with random noise globally (Sec 3.3). Consequently, FedFed can mitigate data heterogeneity by enabling clients to train their models over the local and shared data.
3. We conduct comprehensive experiments to show that FedFed consistently and significantly enhances the convergence rate and generalization performance of FL models across different scenarios under various datasets (Sec 4.2).

## 2 Preliminary

**Federated Learning.** Federated learning allows multiple clients to collaboratively train a global model parameterized by \(\) without exposing clients' data [4; 1]. In general, the global model aims to minimize a global objective function \(()\) over all clients' data distributions:

\[_{}()=_{k=1}^{K}_{k}_{k}(_{k }),\] (1)

where \(K\) represents the total number of clients, \(_{k}\) is the weight of the \(k\)-th client. The local objective function \(_{k}(_{k})\) of client \(k\) is defined on the distribution \(P(X_{k},Y_{k})\) with random variables \(X_{k},Y_{k}\):

\[_{k}(_{k})_{(,) P( X_{k},Y_{k})}(_{k};,),\] (2)

where \(\) is input data with its label \(\) and \(()\) stands for the loss function, e.g., cross-entropy loss. Due to the distributed property of clients' data, the global objective function \(()\) is optimized round-by-round. Specifically, within the \(r\)-th communication round, a set of clients \(C\) are selected to perform local training on their private data, resulting in \(|C|\) optimized models \(\{_{k}^{r}\}_{k=1}^{|C|}\). These optimized models are then sent to a central server to derive a global model for the \((r+1)\)-th communication round by an aggregation mechanism \(AGG()\), which may vary from different FL algorithms:

\[^{r+1}=AGG(\{_{k}^{r}\}_{k=1}^{|C|}).\] (3)

**Differential Privacy.** Differential privacy  is a framework to quantify to what extent individual privacy in a dataset is preserved while releasing the data.

**Definition 2.1**.: _(Differential Privacy). A randomized mechanism \(\) provides \((,)\)-differential privacy (DP) if for any two neighboring datasets \(D\) and \(D^{}\) that differ in a single entry, \( S Range()\),_

\[((D) S) e^{}((D^{}) S )+.\]

_where \(\) is the privacy budget and \(\) is the failure probability._

The definition of \((,)\)-DP shows the difference of two neighboring datasets in the probability that the output of \(\) falls within an arbitrary set \(S\) is related to \(\) and an error term \(\). Similar outputs of \(\) on \(D,D^{}\) (i.e., smaller \(\)) represent a stronger privacy guarantee. In sum, DP as a de facto standard of quantitative privacy, provides provable security for protecting privacy.

**Information Bottleneck.** Traditionally, in the context of the Information Bottleneck (IB) framework, the goal is to effectively capture the information relevant to the output label \(Y\), denoted as \(Z\), while simultaneously achieving maximum compression of the input \(X\). \(Z\) represents the latent embedding, which serves as a compressed and informative representation of \(X\), preserving the essential information while minimizing redundancy. This objective can be formulated as:

\[_{}=I(X;Y|Z), s.t.\ I(X;Z) I_{}\] (4)

where \(I()\) denote the mutual information and \(I_{}\) is a constant. This function is defined as a rate-distortion problem, indicating that IB is to extract the most efficient and informative features.

## 3 Methodology

We detail the Federated Feature distillation (FedFed) proposed to mitigate data heterogeneity in FL. FedFed adopts the information-sharing strategy with the spirits of information bottleneck (IB). Roughly, it shares the minimal sufficient features, while keeping other features at clients1.

### Motivation

Data heterogeneity inherently comes from the difference in data distribution among clients. Aggregating models without accessing data would inevitably bring performance degradation. Sharing clients' data benefits model performance greatly, but intrinsically violates privacy. Applying DP to protect shared data looks feasible; however, it is not a free lunch with paying the accuracy loss on the model.

Draw inspiration from the content and style partition of data causes , we investigate the dilemma in information sharing and performance improvements through a feature partition perspective. By introducing an appropriate partition, we can share performance-sensitive features while keeping performance-robust features locally. Namely, _performance-sensitive features in the data are all we need for mitigating data heterogeneity!_

We will elucidate the definitions of performance-sensitive features and performance-robust features. Firstly, we provide a precise definition of a valid partition (Definition 3.1), which captures the desirable attributes when partitioning features. Subsequently, adhering to the rules of a valid partition, we formalize the two types of features (Definition 3.2).

**Definition 3.1**.: _(Valid Partition). A partition strategy is to partition a variable \(X\) into two parts in the same measure space such that \(X=X_{1}+X_{2}\). We say the partition strategy is valid if it holds: (i) \(H(X_{1},X_{2}|X)=0\); (ii) \(H(X|X_{1},X_{2})=0\); (iii) \(I(X_{1};X_{2})=0\); where \(H()\) denotes the information entropy and \(I()\) is the mutual information._

**Definition 3.2**.: _(Performance-sensitive and Performance-robust Features). Let \(X=X_{s}+X_{r}\) be a valid partition strategy. We say \(X_{s}\) are performance-sensitive features such that \(I(X;Y|X_{s})=0\), where \(Y\) is the label of \(X\). Accordingly, \(X_{r}\) are the performance-robust features._

Intuitively, performance-sensitive features contain all label information, while performance-robust features contain all information about the data except for the label information. More discussions about Definition 3.1 and Definition 3.2 can be found in Appendix C.1. Now, the challenge turns out to be: Can we derive performance-sensitive features to mitigate data heterogeneity? FedFed provides an affirmative answer from an IB perspective, boosting a simple yet effective data-sharing strategy.

### Feature Distillation

Given the motivation above, we propose to distil features such that data features can be partitioned into performance-robust features depicting mostly data and performance-sensitive features favourable to model performance. We design feature distillation and answer Q.2 below.

We draw inspiration from the information bottleneck method . Specifically, only minimal sufficient information is preserved in learned representations while the other features are dismissed . This can be formulated as 2:

\[_{Z}I(X;Y|Z),I(X;Z) I_{},\] (5)

where \(Z\) represents the desired representation extracted from input \(X\) with its label \(Y\), \(I()\) is the mutual information, and \(I_{}\) stands for a constant. Namely, given the learned representation \(Z\), the mutual information between \(X\) and \(Y\) is minimized. Meanwhile, \(Z\) dismisses most information about the input \(X\) so that the mutual information \(I(X;Z)\) is less than a constant \(I_{}\).

Similarly, in FedFed, only minimal sufficient information is necessary to be shared across clients to mitigate data heterogeneity, while the other features are dismissed before sharing data, i.e., kept locally on the client. The differences between IB and FedFed have two folds: 1) FedFed aims to make

Figure 1: Have a Guessing Game! Question: Which one is the first image from? A or B or C?the dismissed features close to the original features (i.e., depict most private data information), while IB focuses on the dissimilarity between the preserved and original features (i.e., contain minimal sufficient information about data); 2) FedFed dismisses information in the data space while IB does that in the representation space. More specifically, the objective of feature distillation is:

\[_{Z}I(X;Y|Z), }I(X;X-Z|Z) I_{},\] (6)

where \(Z\) represents the performance-sensitive features in \(X\), \(Y\) is the label of \(X\), \((X-Z)\) stands for performance-robust features that are unnecessary to be shared, and \(I_{}\) is a constant. In Eq. (6), the \((X-Z)\) should represent data mostly conditioned on performance-sensitive features \(Z\), while the \(Z\) should contain necessary information about the label \(Y\). Consequently, learning with the objective can divide data features into performance-sensitive features and performance-robust features, achieving the goal of feature distillation.

To make the feature distillation tractable, we derive an objective equal to the original objective in Eq. (6) (see Appendix C.2 for more details) for client \(k\) as follows:

\[_{}-_{(,) P(X_{k},Y_{k})} p( |(;)), }\|(;)\|_{2}^{2},\] (7)

where \(\) is the parameter to be optimized for generating performance-sensitive features \((;)\), \((,)\) represents the input pair drawn from the joint distribution \(P(X_{k},Y_{k})\) of client \(k\), \(p(|)\) is the probability of predicting \(Y=\), and \(>0\) stands for a constant. The underlying insight of the objective function in Eq. (7) is intuitive. Specifically, the learned performance-sensitive features \((;)\) are capable of predicting the label \(\) while having the minimal \(_{2}\)-norm.

Unfortunately, the original formulation in Eq. (7) can hardly be used for dividing data features. This is because the feature distillation in Eq. 7 cannot make the preserved features \(-(;)\) similar to the raw features \(\). Namely, the original Eq. (7) cannot guarantee the desired property of the preserved features. To solve the problem, we propose an explicit competition mechanism in the data space. Specifically, we model preserved features, i.e., performance-robust features, with \(q(;)\) explicitly and model the performance-sensitive features \((;)-q(;)\) implicitly:

\[_{}-_{(,) P(X_{k},Y_{k})} p( |-q(;)), }\|-q(;)\|_{2}^{2}.\] (8)

Thus, by Eq. 8, performance-sensitive features can predict labels and performance-robust features are almost the same as raw features.

The realization of Eq. (8) is straightforward. To be specific, we can employ a generative model parameterized with \(\) to generate performance-robust features, i.e., \(q(;)\) in Eq. (8). Meanwhile, we train a local classifier \(f(;_{k})\) parameterized with \(_{k}\) for client-\(k\) to model the process of predicting labels, i.e., \(- p(|)\) in Eq. (8). Accordingly, the realization of feature distillation is formulated as follows with \((;)-q(;)\):

\[_{,_{k}}-_{(,) P(X_{k},X_{k})}(f((;);_{k}),), }\|(;)\|_{2}^{2},\] (9)

where \(()\) is the cross-entropy loss, \(q(;)\) stands for a generative model, and \(\) represents a tunable hyper-parameter. Built upon Eq. (9), we can perform feature distillation, namely, the outputs of a generator \(q(;)\) serve as performance-robust features and \(-q(;)\) are used as performance-sensitive features. We merely share \(-q(;)\) to tackle data heterogeneity by training models over both local and shared data. Algorithm 1 summarizes the procedure of feature distillation.

### Protection for Performance-Sensitive Features

Until now, we have intentionally overlooked the overlap between performance-sensitive features and performance-robust features. Since we prioritize data heterogeneity, overlapping is almost unavoidable in practice, i.e., performance-sensitive features containing certain data privacy. Accordingly, merely sharing performance-sensitive features can risk privacy. Thus, we answer Q. 3 below.

**Why employ DP?** The constructed performance-sensitive features may contain individual privacy, thus, the goal of introducing a protection approach is for individual privacy. In addition, the employed protection approach is expected to be robust against privacy attacks [24; 25]. According to the above analysis, differential privacy (DP) is naturally suitable in our scenario of feature distillation. Thus, we employ DP to protect performance-sensitive features before sending them to the server.

**Algorithm 1** Feature Distillation

**Algorithm 2** Feature Distillation

**How to apply DP?** Applying noise (e.g., Gaussian or Laplacian) to performance-sensitive features before sharing can protect features with DP guarantee , i.e., \(_{p}=_{s}+\). Consequently, the server can collect protected features \(_{p}\) from clients to construct a global dataset and send the dataset back to clients. To give a vivid illustration, we provide a guessing game3 in Figure 1, showing \(4\) images. The first image represents the DP-protected performance-sensitive features of one image (A, B, or C). It is hard to identify which one of the raw images (A, B, C) the first image belongs to.

**Training with DP.** The protected performance-sensitive features are shared, thus, the server can construct a globally shared dataset. Using the global dataset, clients can train local classifier \(f(;_{k})\) parameterized by \(_{k}\) with the private and shared data:

\[_{_{k}}_{(,) P(X_{k},Y_{k})}(f( ;_{k}),)+_{(_{p},) P ((X_{k}),Y_{k})}(f(_{p};_{k}),),\] (10)

where \(f(;_{k})\) is trained for model aggregation, \((_{p},)\) are protected performance-sensitive features that are collected from all clients, and \((X_{k}):=(X_{k})+n\) with noise \(n\). Pseudo-code of how to apply FedFed are listed in Appendix B.

**DP guarantee.** Following DP-SGD , we employ the idea of the \(_{2}\)-norm clipping relating to the selection of the noise level \(\). Specifically, we realize the constraint in Eq. (9) as follows: \(\|(;)\|_{2}\|\|\), with \(0<<1\) (i.e., \(\|_{s}\|\|\|\)). For aligning analyses with conventional DP guarantee, we let the random mechanism on performance-robust features \(_{r}\) be \(_{r}+n,n(0,_{r}^{2})\). Similar for performance-sensitive features, we have \(_{s}+n,n(0,_{s}^{2})\). Since \(_{r}\) is kept locally, an adversary views nothing (or random data without any entropy). This keeps equal to adding a sufficiently large noise, i.e., \(_{r}=\) on \(_{r}\) to make it random enough. Consequently, FedFed can provide a strong privacy guarantee. Moreover, for each client, we show that FedFed requires a relatively small noise level \(\) for achieving identical privacy, which is given in Theorem 3.3 (detailed analysis is left in Appendix D).

**Theorem 3.3**.: _Let \(\) be the noise scale for FedFed and \(^{}\) be the noise scale for sharing raw \(\). Given identical \(,\), we attain \(<^{}\) such that \(\|_{s}\|^{2}\)._

Besides each client, we give the privacy analysis for the FL system paired with the proposed FedFed.

**Theorem 3.4** (Composition of FedFed).: _For \(k\) clients with \((,)\)-differential privacy, FedFed satisfies \((_{},1-(1-)_{i}(1-_{i}))\)-differential privacy,

    &  \\   & ACC\(\) & Gain\(\) & Round \(\) & Speedup\(\) & ACC\(\) & Gain\(\) & Round \(\) & Speedup\(\) \\  FedAvg & **92.34**(86.73) & 5.61\(\) & **14**(121) \(\)**8.6**(\(\)1.0) & **90.69**(78.34) & 12.35\(\) & **16**(420) \(\)**26.3**(\(\)1.0) \\ FedProx & **92.09**(87.73) & 4.36\(\) & **32**(129) \(\)**2.1**(\(\)0.9) & **89.68**(82.03) & 7.65\(\) & **16**(44) \(\)**26.3**(9.5) \\ SCAFFOLD & **91.62**(86.31) & 3.89\(\) & **29**(147) \(\)**4.2**(\(\)0.8) & **80.48**(76.63) & 3.85\(\) & **139**(None) \(\)**6.2**(None) \\ FedNova & **92.39**(87.03) & 5.36\(\) & **18**(88) \(\)**6.7**(\(\)1.4) & **89.72**(79.98) & 9.74\(\) & **16**(531) \(\)**26.3**(\(\) 0.8) \\    &  &  \\   &  &  \\   &  &  \\   &  &  \\   &  &  \\   &  &  \\   &  &  \\   &  &  \\   &  &  \\   &  &  \\   &  &  \\   &  &  \\   &  &  \\   &  &  \\   &  &  \\   &  &  \\   &  &  \\   &  &  \\   &  &  \\   &  &  \\   &  &  \\   &  &  \\   &  &  \\   &  &  \\   &  &  \\   & 
    &  \\   & ACC\(\) & Gain\(\) & Round \(\) & Speedup\(\) & ACC\(\) & Gain\(\) & Round \(\) & Speedup\(\) \\   & \(=0.1,E=1,K=10\) & (Target ACC =88\%) & \(=0.05,E=1,K=10\) & **137**(503) & \(\)**3.7(\( 1.0\))** \\ FedProx & **90.02**(65.34) & 4.68\(\) & **233**(None) & \(\)**2.1(None)** & **69.03**(61.29) & 7.74\(\) & **141**(485) & \(\)**3.6(\(1.0\))** \\ SCAFFOLD & **70.14**(67.23) & 2.91\(\) & **198**(769) & \(\)**2.5(\( 0.6\))** & **69.32**(58.78) & 10.54\(\) & **81**(None) & \(\)**6.2**(None) \\ FedNova & **70.48**(67.98) & 2.57 & **147**(432) & \(\)**3.4(\( 1.1\))** & **68.92**(60.53) & 8.39\(\) & **87**(None) & \(\)**5.8**(None) \\    & \(=0.1,E=5,K=10\) & (Target ACC =69\%) & \(=0.1,E=1,K=100\) & (Target ACC =88\%) \\  FedAvg & **70.96**(69.34) & 1.62\(\) & **79**(276) & \(\)**3.5(\( 1.0\))** & **60.58**(48.21) & 12.37\(\) & **448**(967) & \(\)**2.2**(\( 1.0\)) \\ FedProx & **69.66**(62.32) & 7.34\(\) & **285**(None) & \(\)**1.0(None)** & **67.69**(48.78) & 18.91\(\) & **200**(932) & \(\)**4.8**(\( 1.0\)) \\ SCAFFOLD & **70.76**(70.23) & 0.53\(\) & **108**(174) & \(\)**2.6**(\( 1.6\)) & **66.67**(51.03) & 15.64\(\) & **181**(832) & \(\)**5.3**(\( 1.2\)) \\ FedNova & **69.98**(69.78) & 0.2\(\) & **89**(290) & \(\)**3.1**(\( 1.0\)) & **67.62**(48.03) & 19.59\(\) & **198**(976) & \(\)**4.9**(\( 1.0\)) \\   

Table 3: Top-1 accuracy with(without) FedFed under different heterogeneity degree, local epochs, and clients number on CIFAR-100.

    &  \\   & ACC\(\) & Gain\(\) & Round \(\) & Speedup\(\) & ACC\(\) & Gain\(\) & Round \(\) & Speedup\(\) \\   & \(=0.1,E=1,K=10\) & (Target ACC =67\%) & \(=0.05,E=1,K=10\) & (Target ACC =61\%) \\  FedAvg & **69.64**(67.84) & 1.8\(\) & **283**(495) & \(\)**1.7**(\( 1.0\)) & **68.49**(62.01) & 6.48\(\) & **137**(503) & \(\)**3.7**(\( 1.0\)) \\ FedProx & **70.02**(65.34) & 4.68\(\) & **233**(None) & \(\)**2.1**(None) & **69.03**(61.29) & 7.74\(\) & **141**(485) & \(\)**3.6**(1.0) \\ SCAFFOLD & **70.14**(67.23) & 2.91\(\) & **198**(769) & \(\)**2.5**(\( 0.6\)) & **69.32**(58.78) & 10.54\(\) & **81**(None) & \(\)**6.2**(None) \\ FedNova & **70.48**(67.98) & 2.5\(\) & **147**(432) & \(\)**3.4**(\( 1.1\)) & **68.92**(60.53) & 8.39\(\) & **87**(None) & \(\)**5.8**(None) \\    & \(=0.1,E=5,K=10\) & (Target ACC =69\%) & \(=0.1,E=1,K=100\) & (Target ACC =88\%) \\  FedAvg & **70.96**(69.34) & 1.62\(\) & **79**(276) & \(\)**3.5**(\( 1.0\)) & **60.58**(48.21) & 12.37\(\) & **448**(967) & \(\)**2.2**(\( 1.0\)) \\ FedProx & **69.66**(62.32) & 7.34\(\) & **285**(None) & \(\)**1.0**(None) & **67.69**(48.78) & 18.91\(\) & **200**(932) & \(\)**4.8**(\( 1.0\)) \\ SCAFFOLD & **70.76**(70.23) & 0.53\(\) & **108**(174) & \(\)**2.6**(\( 1.6\)) & **66.67**(51.03) & 15.64\(\) & **181**(832) & \(\)**5.3**(\( 1.2\)) \\ FedNova & **69.98**(69.78) & 0.2\(\) & **89**(290) & \(\)**3.1**(\( 1.0\)) & **67.62**(48.03) & 19.59\(\) & **198**(976) & \(\)**4.9**(\( 1.0\)) \\   

Table 4: Top-1 accuracy with(without) FedFed under different heterogeneity degree, local epochs, and clients number on SVHN.

is that existing methods can achieve performance comparable to centralized training. Moreover, we conducted an additional experiment involving sharing the full data with DP protection. The results, presented in Appendix F.3, indicate that sharing the full data with DP protection leads to a degradation in the performance of the FL system. This degradation occurs because protecting the full data necessitates a relatively large noise to achieve the corresponding protection strength required to safeguard performance-sensitive features.

**Surprising Observations.** We find that training among \(100\) clients in CIFAR-10 and CIFAR100 reaches a significant improvement (e.g., at most \(40.67\%\)!). A possible reason is that the missed data knowledge can be well replenished by FedFed. Moreover, all methods paired with FedFed under various settings can achieve similar prediction accuracies, demonstrating that FedFed endows FL models robustness against data heterogeneity. Table 5 shows that two kinds of heterogeneity partition cause more performance decline than LDA (\(=0.1\)). Yet, FedFed attains noteworthy improvement, indicating the robustness against Non-IID partition.

### Ablation Study

**DP Noise.** To explore the relationship between privacy level \(\) and prediction accuracy, we conduct experiments with different noise levels \(_{s}^{2}\). As shown in Figure 2(c), the prediction accuracy decreases with increasing noise level (more results in Appendix F.5). To verify whether the FedFed is robust against the selection of noise, we also consider Laplacian noise in applying DP to privacy protection. The results of Laplacian noise are reported in Table 6, demonstrating the robustness of FedFed.

**Hyper-parameters.** We further evaluate the robustness of FedFed against hyper-parameters. During the feature distillation process, as the constraint parameter \(\) in Eq. 9 decreases, the DP strength to protect performance-sensitive features decreases, and the performance of the global model decreases, owing to the less information contained in performance-sensitive features.

### Privacy Verification

Besides the theoretical analysis, we provide empirical analysis to support the privacy guarantee of FedFed. We wonder whether the globally shared data can be inferred by some attacking methods. Thus, we resort to model inversion attack , widely used in the literature to reconstruct data. The results5 in Figure 3 (a) and (b) indicate that FedFed could protect globally shared data. We also conduct another model inversion attack  and the results can be found in Appendix E.1.

    &  \\   & FedAvg & FedProx & SCAFFOLD & FedNova \\  Gaussian Noise & 92.34 & 92.12 & 89.66 & 92.23 \\ Laplacian Noise & 92.30 & 91.36 & 91.24 & 91.73 \\   

Table 6: Experiment results with different noise adding on CIFAR-10.

    &  \\   & FedAvg & FedProx & SCAFFOLD & FedNova \\  \(a=0.1\) & **92.34**(79.35) & **92.12**(83.06) & **89.66**(83.67) & **92.23**(80.95) \\ \(\#C=2\) & **89.23**(42.54) & **88.17**(758.45) & **84.43**(36.82) & **89.54**(45.42) \\ Subset & **90.29**(95.93) & **89.11**(32.87) & **89.92**(52.96) & **90.00**(38.52) \\   

Table 5: Experiment results of different Non-IID partition methods on CIFAR-10 with 10 clients.

Figure 2: More facts of FedFed. (a) Convergence rate of different generative models (i.e., \(\)-VAE and ResNet generator) compared with vanilla FedAvg. (b) Test accuracy and convergence rate on different federated learning algorithms with or without FedFed under \(=0.1,E=1,K=100\). (c) Test accuracy on FMNIST with different noise level \(_{s}^{2}\) in Theorem 3.3, obtaining various privacy \(\) (lower \(\) is preferred). As the noise increased, the level of protection gradually increased.

Additionally, we perform membership inference attack  to illustrate the difference between FedFed and sharing all data features with DP protection. The results illustrated in Figure 3 (c) show that noise level \(^{2}=0.3\) over raw data \(\) can achieve similar protection as noise \(_{s}^{2}=0.15\) over globally shared data \(_{p}\), which aligns with Theorem 3.3. More details can be found in Appendix E.2

## 5 Related Work

Federated Learning (FL) models typically perform poorly when meeting severe Non-IID data [2; 7]. To tackle data heterogeneity, advanced works introduce various learning objectives to calibrate the updated direction of local training from being far away from the global model, e.g., FedProx, FedIR , SCAFFOLD , and MOON . Designing model aggregation schemes like FedAvgM , FedNova , FedMA , and FedBN  shows the efficacy on heterogeneity mitigation. Another promising direction is the information-sharing strategy, which mainly focuses on synthesizing and sharing clients' information to mitigate heterogeneity [9; 46; 47]. To avoid exposing privacy caused by the shared data, some methods utilize the statistics of data , representations of data , logits [49; 10], embedding . However, advanced attacks pose potential threats to methods with data-sharing strategies .

FedFed is inspired by , where pure noise is shared across clients to tackle data heterogeneity. In this work, we relax the privacy concern by sharing partial features in the data with DP protection. In addition, our work is technically similar to an adversarial learning approach . Our method distinguishes by defining various types of features and delving into the exploration of data heterogeneity within FL. More discussion can be found in Appendix G.

## 6 Conclusion

In this work, we propose a novel framework called FedFed to tackle data heterogeneity in FL by employing the promising information-sharing approach. Our work extends the research line of constructing shareable information of clients by proposing to share partial features in data. This shares a new route of improving training performance and maintaining privacy. Furthermore, FedFed has served as a source of inspiration for a new direction focused on improving performance in open-set scenarios . Another avenue of exploration involves deploying FedFed in other real-time FL application scenarios, such as recommendation systems  and healthcare system , to uncover its potential benefits.

**Limitation.** In reality, limited local hardware resources may limit the power of FedFed, since FedFed introduces some extra overheads like communication and storage overheads. We leave it as future works to explore a hardware-friendly version or real-world application [56; 57]. Additionally, FedFed raises potential privacy concerns that we leave as a future research exploration, such as integrating cryptography [58; 59]. We anticipate that our work will inspire further investigations to comprehensively evaluate the privacy risks associated with information-sharing strategies aimed at mitigating data heterogeneity.

Figure 3: Attack results on FedFed. (a) shows the protected data \(_{p}\). (b) reports the model inversion attacked data. (c) shows results of membership inference attacks: the green star represents the recall for FedFed, while the blue stars show the searching process varying with \(_{s}^{2}\) for sharing raw data.