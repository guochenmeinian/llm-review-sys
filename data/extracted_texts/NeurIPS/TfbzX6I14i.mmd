# Sparse Modular Activation for

Efficient Sequence Modeling

Liliang Ren\({}^{1}\) Yang Liu\({}^{2}\) Shuohang Wang\({}^{2}\) Yichong Xu\({}^{}\)

**Chenguang Zhu\({}^{2}\) Chengxiang Zhai\({}^{1}\)**

\({}^{1}\)University of Illinois at Urbana-Champaign \({}^{2}\)Microsoft

{liliang3,czhai}@illinois.edu

{yaliu10,shuowa,chezhu}@microsoft.com

xuyc11@gmail.com

Work partially done during internship at Microsoft.Work done at Microsoft.Work done at Microsoft.

###### Abstract

Recent hybrid models combining Linear State Space Models (SSMs) with self-attention mechanisms have demonstrated impressive results across a range of sequence modeling tasks. However, current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. To address this limitation, we introduce _Sparse Modular Activation_ (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption of neural networks at both training and inference stages. To validate the effectiveness of SMA on sequence modeling, we design a novel neural architecture, _SeqBoat_, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including long sequence modeling, speech classification and language modeling, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity, and reveals the amount of attention needed for each task through the learned sparse activation patterns. Our code is publicly available at https://github.com/renll/SeqBoat.

## 1 Introduction

Recent advance on efficient sequence modeling with State Space Models (SSMs)  has shown impressive performance for a wide range of tasks across modalities, such as text classification, image recognition and speech recognition. SSMs, as first-order linear models, defined by a set of input, output, and state variables connected by first-order differential equations, can efficiently capture the recurrent structure in sequential data with carefully designed state matrices and the application of convolutional parallelism . However, they still significantly underperform the self-attention  based model in both language modeling and machine translation  tasks. A recent work  reveals that this is due to its deficiency of modeling the second-order pairwise comparisons between the input tokens, and shows that the augmentation of an additional shifted SSM layer can improve SSM'sassociative recalling ability. Furthermore, better quality-efficiency trade-off can be achieved by directly introducing extra self-attention modules to form a hybrid model (e.g. MEGA  and Hybrid H3 ) that utilizes both the first and the second order inductive biases, _i.e._, SSM and self-attention. However, the current hybrid models apply the attention modules statically and uniformly to each of the input token regardless the property of the task itself. This can lead to sub-optimal quality-efficiency trade-offs since not all input tokens require second-order modeling and this computation need can vary substantially depending on both its context and the task difficulty.

In this paper, we aim to answer the following research questions for efficiently combining attention with SSMs:

* **RQ1:** Can neural networks learn to activate their attention modules on demand to achieve better quality-efficiency trade-off?
* **RQ2:** How much extra attention is needed for the SSMs on a task-by-task basis?

To answer these questions, we develop a new general mechanism, _Sparse Modular Activation (SMA)_, that allows a neural network to sparsely and dynamically activate its sub-modules for each of the input token in a fully differentiable manner. Specifically, we assume a neural model can be composed of multiple heterogeneous sub-modules. For the input sequence, a latent configurator sparsely maps tokens to multiple compressed sequences corresponding to sub-modules. Each sub-module is then only applied on its mapped shorter sequence. Compared with activating all sub-modules on the whole input, Sparse Modular Activation can reduce computation and memory consumption for both the training and inference stages. Notably, SMA is proved to have a full coverage of the combinatorial search space of module activation, which is further explained in Section 3.2.

Efficient learning of dynamic sparsity is notoriously difficult under the constraint of the current parallel hardware . To enable the practical efficiency gains from our module-level sparsity, we provide a simple yet efficient parallel implementation of SMA without any custom fused GPU kernels. Specifically, when compressing a batch of sequences in SMA, our implementation conducts both token selection and the sequence re-padding simultaneously using a single _scatter_ operation that is widely optimized and present in modern deep learning frameworks.

To address **RQ1**, we apply SMA to construct a novel neural architecture, _SeqBoat_, that sparsely activate a Gated Attention Unit (GAU)  based on the state representation learned from an SSM. Both the GAU and the SSM representations are then aggregated through simple addition and activation to form a layer-level representation. Multiple same-sized SeqBoat layers are stacked sequentially to form a full neural model. Inspired by the working memory mechanism  used in human cognition, we further restrict the GAU to only apply local attention on the compressed sequence, which allows our model to have linear sequence inference complexity but theoretically infinite attention span.

We conduct comprehensive experiments to show that SeqBoat has significantly better quality-efficiency trade-off than state-of-the-art hybrid models on a wide range of tasks, including Long Range Arena (LRA) , speech classification  and language modeling . On the competitive LRA benchmark, SeqBoat achieves 1.96 higher average accuracy than MEGA-chunk , the previous best hybrid model, with a 10.4\(\) training speed up and a 95% memory reduction compared to the Transformer  on the Text task with 4,096 input length. Thanks to the intrinsic modular sparsity brought by SMA, SeqBoat directly reveals the amount of attention needed for each data sample of each task through its sparse activation patterns of GAU, addressing **RQ2**. We demonstrate that our working memory mechanism provides substantially better computation-accuracy trade-off than chunking based models, and analyze the relationship between the working memory size and the effective attention span on various long sequence modeling tasks.

## 2 Background

To motivate and clarify our proposed techniques, we first present a mathematical formulation of our Sparse Modular Activation mechanism and show how it encompasses and generalizes previous attempts that aimed for module-level dynamic sparsity. A dedicated section for detailed comparisons between our approach with the related works is also included in Appendix F. We begin by reviewing how the standard sequence modeling is formalized to establish the common ground for our discussion.

### Time-Invariant Sequence Modeling

Given a discrete sequence, \(=\{x_{1},...,x_{n}\}^{n}\), consisting of \(n\) tokens, a time-invariant sequence model \(P_{}\) is optimized to maximize the likelihood of the observed sequences by factorizing them as follows:

\[_{}P_{}()=_{t=1}^{n}P(x_{t}|_{<t}, ),\]

where \(_{<t}=\{x_{1},...,x_{t-1}\}\) is the sequence history at time step \(t\), and the parameter \(\) is independent of the time step \(t\). This formulation implies that the full model parameters \(\) and the full history \(_{<t}\) are both essential for the conditional prediction of each token \(x_{t}\). However, one potential issue is as the prediction difficulty of each token may differ depending on the context and the position, this static model \(P_{}\) can lead to sub-optimal accuracy-efficiency trade-off by wasting computation on either unimportant context  or easy-to-predict tokens .

## 3 Learning Sparse Modular Activation

To cover a larger search space that may contain more efficient sequence models, we propose to formulate sequence modeling as a problem of finding an optimal time-variant model that can dynamically activate a subset of modules from a pre-defined function space for each time step.

### Time-Variant Sequence Modeling

Formally, a time-variant sequence model is defined on a compact function space \(:_{t}^{c}^{n V}\), where \(V\) is the size of the vocabulary and \(_{t}^{c}=\{_{t}^{c}:_{t}^{c}_{<t}^{n}\}\), contains all possible sub-sequences of the sequence history \(_{<t}\). Then for each of the token prediction at the time step \(t\), the model learns to apply a function \(f_{t}\) with the parameters \(_{t}\) that maximizes the sequence probability, _i.e._,

\[_{f_{t},_{t},_{t}^{c}}P_{}()=_{ t=1}^{n}P_{f_{t}}(x_{t}|_{t}^{c},_{t}) _{t}^{c}_{<t}\] (1)

This formulation generalizes the previous works in pursuing a dynamic and sparse model for sequence modeling, where the connections are further explained in Appendix F. In this work, we assume the function space \(\) is chain-structured, _i.e._, \(=_{N}_{1} \), where \(:^{n d_{m}}^{n V}\) is the classification function, \(:^{n}^{n d_{m}}\) is the embedding function, \(N\) is the number of intermediate layers, \(d_{m}\) is the model size and \(:^{n d_{m}}^{n d_{m}}\) is the function space of the intermediate mappings. We further assume that \(\) is the spanning set of a finite number of the function \(f_{i}^{l}\) with its parameters \(_{i}^{l}\), _i.e._, \(=\{f_{1}^{l},...,f_{M}^{l}\}\), where \(M\) is the number of pre-defined functions. These assumptions justify the design of our Sparse Modular Activation mechanism, which is further explained in the following section.

### Sparse Modular Activation

Sparse Modular Activation (SMA) introduces a latent configurator at each time step \(t\) and each layer of a neural sequence model. The configurator produces a binary decision vector \(_{t}\{0,1\}^{M}\) to only activate those functions \(f_{i}^{l}\) that have the decision value \(a_{t}^{i}=1\). Only the activated function will be applied to process the input representations and return the outputs. These outputs are then aggregated with a linear combination to generate the final vector, \(_{t}\), of the layer. To enable end-to-end differentiability, we also require the latent configurator to output the confidence probability of its decisions, \(_{t}^{M}\), and respectively scale the output from the activated function with its confidence values \(c_{t}^{i}\). The function space of a layer equipped with SMA can be described as follow,

\[_{}(_{t},_{t})=\{_{i I} _{i}c_{t}^{i}f_{i}^{l}_{i},I=\{i a_{t}^ {i}=1,1 i M\}\}\]

where \(I\) is the index set of activated functions. Notably, SMA has the following important property,

**Theorem 1** (Function Space Coverage of SMA).: _For any \(^{}=\{f_{1}^{l},...,f_{M}^{l}\}\), there exists a pair of \((_{t}^{},_{t}^{})\) that \(_{}(_{t}^{},_{t}^{})= ^{}\). In other words, SMA has a **full** coverage of the function space \(\)._The proof is provided in Appendix B. Intuitively, we can see that the original function space \(\) is recovered, _i.e._, \(_{}=\), if all the functions are activated, and the model can adaptively select any subspace of \(\) through controlling the binary decision vector \(_{t}\). During sequential inference stage, for each function that requires the contextual information, we keep a record of its activated input representations across the time steps to form a memory \(_{t}^{c}_{<t}^{n d_{m}}\) (where \(_{<t}\) is the input sequence history), so that each function can recall its contextual memory for each of the decoding step. During parallel training stage, the activated input representations for each function (or module) are sparsely selected to compose the compressed sequence \(^{c}\) for the module to conduct parallel processing, and this process is implemented as a _compress_ operator. The returned representations are then mapped to its original input position in a new sequence whose inactivated positions are filled with zero vectors, which is denoted as an _extract_ operator. We illustrate this parallel implementation in Figure 2 for better understanding. The Pytorch-like  code snippets of the _compress_ and _extract_ operators are provided in the Appendix A.1 with an efficient support of batched sequences using the _scatter_ operation.

In this work, we explore applying SMA to address our **RQs**, in which the following two types of functions in the function space \(\) are considered: (1) a linear State Space Model (SSM)  that efficiently captures the recurrent sequential structure, and (2) a Gated Attention Unit (GAU)  that performs second-order pairwise comparisons between the sequence elements. We also keep the SSM as an always-activated module and only apply SMA to the GAU, and in this case the effective number of modules \(M=1\). This is because for long sequence modeling, the main computation and memory bottleneck comes from the second-order self-attention and the gating operations inside the GAU module. We modularize the combination of SSM, GAU and SMA as a neural layer, which we call a SeqBoat layer. Its details are further explained in the next section.

### Model Architecture

Figure 1 illustrates the proposed architecture for our SeqBoat layer. Given an input sequence representation \(^{l}^{n d_{m}}\) for the \(l\)-th layer, where \(n\) is the length of the sequence and \(d_{m}\) is the hidden size, we first apply a linear time-invariant State Space Model (SSM)  over the sequence

Figure 1: The proposed SeqBoat layer. The black lines indicate that the gradients can be back-propagated and the red lines stand for gradients blocking. \(\) means the element-wise multiplication, and \(\) is the point-wise addition. The _max_, _argmax_ and _softmax_ operators are all applied to the projected dimension after the linear layer in the latent configurator block. The sparse activation operators are respectively instantiated as _compress_ and _extract_ operators for parallel processing.

Figure 2: The proposed parallel implementation of the Sparse Modular Activation (SMA) mechanism. In this example, we have two modules \(f_{1}^{l},f_{2}^{l}\) and an input sequence \(=\{_{1},,_{4}\}\). We assume only \(a_{2}^{1},a_{4}^{1},a_{4}^{2}\) have values equal to one. The white block means a zero vector. The compressed sequences for modules \(f_{1}^{l}\) and \(f_{2}^{l}\) are \(_{1}^{c}=\{_{2},_{4}\}\) and \(_{2}^{c}=\{_{4}\}\) respectively. The final outputs are aggregated as \(_{4}=c_{4}^{1}_{4}^{1}+c_{4}^{2}_{4}^{2}\), and \(_{2}=c_{2}^{1}_{2}^{1}\).

followed by a SiLU activation  to obtain the state representations \(^{n d_{m}}\),

\[(^{l}) =*^{l}+^{l}\] (2) \[ =((^{l}))\]

where \(*\) is a convolution operator which is efficiently implemented using FFTs , \(\) indicates the element-wise multiplication, \(^{1 d_{m}}\) is the learnable parameters, and \(^{n d_{m}}\) is the SSM convolution kernel. In this work, we use the Multi-dimensional Damped Exponential Moving Average (MD-EMA) proposed by MEGA  for the kernel parameterization, whose details are included in Appendix A.3. Note that our work is orthogonal to the SSM kernel design as long as it can provide an efficient computation of the recurrent state representation. It is possible that using a more advanced SSM such as Liquid-S4  or S5  can lead to better performance, but we keep using MD-EMA for a fair comparison with MEGA. Given the state representation \(^{n d_{m}}\), a latent configurator with one linear layer is applied to obtain the decision vector \(\{0,1\}^{n}\) and the confidence probability vector \(^{n}\) with the Tempered Softmax, _i.e._,

\[_{i} =_{i}+b_{i})/}}{e^{\;( _{0}+b_{0})/}+e^{\;(_{1}+b_{1})/ }}$}\] \[ =_{i}},\;\;\;=}\,_{i}\]

where \(_{0},_{1}^{d_{m}},b_{0},b_{1}\) are learnable parameters, \(_{>0}\) is a learnable temperature parameter which is initialized as \(}\) (where \(\) is a hyperparameter denoted as the initial temperature scale). Previous works  often seek applying auxiliary losses to regularize the exploration and the exploitation of the latent decisions. However, in this work, we don't apply any auxiliary losses and empirically observe that solely using Tempered Softmax works well for SMA. To provide an explanation of this phenomenon, we show that the implicit regularization  of gradient decent will conditionally encourage the exploration of the latent decisions if the Softmax function with learnable temperature (i.e. Tempered Softmax) is used for decision confidence calculation. The details of the theorem and the proof are provided in Appendix C.

Based on the decision values \(\), a Gated Attention Unit (GAU) is activated and applied on the compressed sequence \(^{c}\) resulted from the compress operator as previously described in Section 3.2,

\[^{c} =(,) ^{r d_{m}}\] \[^{c} =(^{c}) ^{r d_{m}}\] \[ =(^{c},) ^{n d_{m}}\]

where \(r=_{t=1}^{n}a_{t} n\) is the total number of the time steps when the GAU module is activated. We use Squared ReLU  for the attention function on the image tasks and Softmax for the text related tasks. The detailed implementation of GAU is shown in Appendix A.2. With the extracted output \(^{n d_{m}}\), the final layer output is computed with a linear aggregation followed by a non-linear activation, _i.e._,

\[^{l+1}=(++ +^{l})\]

where \(^{d_{m} d_{m}},^{d_{m}}\) are the learnable parameters. We also add a normalization layer either before the Linear SSM (_i.e._, Pre-Norm) in the residual branch or after the final layer output \(^{l+1}\) (_i.e._, Post-Norm) to stabilize the training process. The SeqBoat model is then constructed as stacking \(N\) number of SeqBoat layers of the same hidden size \(d_{m}\) with the task specific embedding and classification layers. Notably, our model does **not** have any Feed-Forward Networks (FFN) anywhere in the architecture.

Working Memory MechanismThe computation complexity of our model is dynamically dependent on the activation probability of the GAU module \(p=r/n=_{t=1}^{n}a_{t}/n\) for each of the SeqBoat layer. This means that the computation complexity is \(O(r^{2}+n(n))\) for parallel training and \(O(r^{2}+n)\) for auto-regressive decoding. While it is possible that the number of the GAU-activated time steps is far smaller than the length of the full sequence for a trained model, a random initialized SeqBoat model will still activate the GAU module for \(n/2\) number of times on average, leading to a non-ideal \(O(n^{2})\) training complexity at the early stage of training. To solve this problem, we restrict the GAU to only conduct local attention  with a sliding window of size \(w n\) on the compressed sequence \(^{c}\) during the parallel training stage. Each token will attend to the nearest \(w\) tokens in the compressed sequence, _i.e._, \(w/2\) past tokens and \(w/2\) future tokens for bidirectional sequence modeling, and \(w\) number of past tokens for auto-regressive sequence modeling. This approach keeps a First-In-First-Out (FIFO) memory with size \(w\) of the compressed sequence history during auto-regressive decoding, which can be understand as a working memory  for the GAU module. With this mechanism, SeqBoot reduces the training complexity to \(O(rw+n(n))\) and the inference complexity to \(O(rw+n)\), while maintaining the ability of attending to the key tokens whose distances from the query are longer than the working memory size \(w\). This is because the elapsed time steps between two GAU activations is generally not bounded by the size of the working memory.

## 4 Experiments and Results

We evaluate the SeqBoot model on three representative long-range sequence modeling benchmarks across image, text and speech modalities to have a comprehensive understanding of its capacity. The implementation details of our model for each of the tasks are presented in Appendix A.

Besides model performance on task metrics, we also report training speed and memory consumption as the efficiency of each model. Since the dynamic sparsity of SeqBoot will affect both the speed and the memory consumption throughout the training process, we measure its training speed up as the average per step wall time across the full training stage, and the memory consumption as the average per step GPU memory allocation. More details of the efficiency measurement are included in Appendix A.6.

### Baseline Models

We compare our proposed SeqBoot model with multiple strong baseline models besides the Transformer  and its variant Transformer-XL .

**S4** is a linear State Space Model (SSM) that efficiently captures the recurrent structure in sequential data. It employs a new parameterization for the state space model, enabling efficient and principled modeling of long-range dependencies. S4D-LegS  is a simpler variant of S4 that uses a diagonal state matrix.

**MEGA** is a hybrid model that combines a gated self-attention mechanism with a Multi-dimensional Damped Exponential Moving Average (MD-EMA) parameterized SSM. It aims to achieve a better quality-efficiency trade-off by utilizing both the first and second-order inductive biases from SSM and self-attention. MD-EMA can be considered as a simpler variant of S4 that is similar to S4D. MEGA also has a linear complexity variant, MEGA-chunk, that applies attention to each local chunk of fixed length.

**H3** introduces a hybrid SSM layer that stacks a diagonal SSM and a shifted SSM to model the comparisons between the elements in a sequence. It also provides a hybrid model that includes extra self-attention layers for better language modeling performance.

**S5** is the state-of-the-art SSM model that improves over S4 with multi-input multi-output and time-domain processing. It leverages the parallel scan operator in JAX  for an efficient parallel implementation of recurrence unrolling.

**Liquid-S4** extends over S4 through allowing the state transition to be input-dependent. The proposed approach can be cast back to the convolutional framework and re-use the Cauchy kernel for efficient computation.

**Reformer** is a memory-efficient variant of the Transformer that uses locality-sensitive hashing (LSH) to perform approximate nearest neighbor search for attention computation.

**Performance and Linformer** are two efficient variants of the Transformer model. The Performer  uses the Fast Attention via positive Orthogonal Random features algorithm to approximate the self-attention mechanism, reducing the computational complexity. The Linformer  employs low-rank approximations to project the self-attention mechanism into a lower-dimensional space, resulting in a more efficient system.

**Luna** is a linear unified nested attention mechanism that approximates Softmax attention with two nested linear attention functions, resulting in linear time and space complexity.

### Long Sequence Modeling

We first evaluate SeqBoat and SeqBoat-full, a variant of SeqBoat conducting the full attention over the compressed sequence without the working memory mechanism, on the Long Range Arena (LRA) benchmark . LRA is designed to test the ability of neural models to capture long-range dependencies in various modalities with the following six tasks: ListOps , text classification (Text; ), document retrieval (Retrieval; ), image classification (Image; ), Pathfinder  and its longer-range variant Path-X.

From Table 1, we can see that compared with other hybrid models with linear inference complexity, SeqBoat achieves the best average accuracy across all tasks, with a 10.4\(\) speed up and a 95% memory reduction compared to the Transformer. When compared with a pure SSM-based model, S4, which has more general modeling power than the MD-EMA used in our model, SeqBoat achieves a substantially higher accuracy on average with a 2.17\(\) speed up and a 64% memory reduction. Our model even achieves a significantly better average accuracy than state-of-the-art SSM model, S5 , with a 0.16 absolute improvement, resulting in a new high score for models with linear inference complexity. Surprisingly, SeqBoat outperforms SeqBoat-full on Pathfinder and the Path-X tasks. This demonstrates the effectiveness of our proposed working memory mechanism to efficiently capture long-range dependencies, which is further analysed in Section 5. Since the learned module-level sparsity of our model is dynamic and task-specific, we also include the measurements for training and inference speedup and training memory allocation for all the tasks of LRA in Appendix D to provide a holistic view of the efficiency of our model.

### Speech Classification

We also evaluate SeqBoat on the long-range speech classification task using the Speech Commands dataset . We follow S4  to use the SC10 subset of the dataset. It is a 10-class

  
**Models** & **ListOps** & **Text** & **Retr.** & **Image** & **Path.** & **Path-X** & **Avg.** & **Speed** & **Mem.** \\    \\  Transformer & 37.11 & 65.21 & 79.14 & 42.94 & 71.83 & ✗ & 59.24 & 1\(\) & 1\(\) \\ MEGA* & 63.14 & 90.43 & 91.25 & 90.44 & 96.01 & 97.98 & 88.21 & 2.9\(\) & 0.31\(\) \\   \\    & 37.27 & 56.10 & 53.40 & 38.07 & 68.50 & ✗ & 50.67 & 0.8\(\) & 0.24\(\) \\ BigBird & 36.05 & 64.02 & 59.29 & 40.83 & 74.87 & ✗ & 55.01 & 1.1\(\) & 0.30\(\) \\
**SeqBoat-full*** & 61.65 & 89.60 & 91.67 & 89.96 & 95.87 & 95.28 & 87.33 & 6.2\(\) & 0.07\(\) \\   \\    & 18.01 & 65.40 & 53.82 & 42.77 & 77.05 & ✗ & 51.41 & 5.7\(\) & 0.11\(\) \\ Linformer & 35.70 & 53.94 & 52.27 & 38.56 & 76.34 & ✗ & 51.36 & 5.5\(\) & 0.10\(\) \\ Luna-256 & 37.98 & 65.78 & 79.56 & 47.86 & 78.55 & ✗ & 61.95 & 4.9\(\) & 0.16\(\) \\ S4 & 59.10 & 86.53 & 90.94 & 88.48 & 94.01 & 96.07 & 85.86 & 4.8\(\) & 0.14\(\) \\ S4D-LegS & 60.47 & 86.18 & 89.46 & 88.19 & 93.06 & 91.95 & 84.89 & 6.1\(\) & 0.14\(\) \\ S5 & 62.15 & 89.31 & **91.40** & 88.00 & 95.33 & **98.58** & 87.46 & 6.1\(\) & 0.14\(\) \\ Liquid-S4 & **62.75** & 89.02 & 91.20 & 89.50 & 94.8 & 96.66 & 87.32 & 1.2\(\) & 0.17\(\) \\ H3* & 57.50 & 88.20 & 91.00 & 87.30 & 93.00 & 91.80 & 84.80 & 6.0\(\) & 0.24\(\) \\ MEGA-chunk* & 58.76 & **90.19** & 90.97 & 85.80 & 94.41 & 93.81 & 85.66 & 7.0\(\) & 0.09\(\) \\
**SeqBoat*** & 61.70 & 89.60 & 91.28 & **90.10** & **96.35** & 96.68 & **87.62** & 10.4\(\) & 0.05\(\) \\   

Table 1: Test accuracy on the LRA benchmark. We report the relative training speed up and memory reductions on the Text task with 4,096 sequence length. * indicates a hybrid model. “Retr.” and “Path.” are the abbreviations of Retrieval and the Pathfinder tasks respectively. Best results of linear inference complexity models are in bold, second best are underlined.

classification task of raw speech which contains sequences of length 16k. As shown in Table 2, SeqBoat achieves competitive performance compared to the state-of-the-art S4 model and MEGA-chunk, with a 1.32\(\) speed up and a 56% memory reduction compared to MEGA-chunk.

### Language Modeling

We evaluate our model on enwik8 , a popular language modeling benchmark. It consists of the first 100 million bytes of the English Wikipedia and is commonly used to evaluate the performance of sequence models on long-range dependencies. We follow previous studies to use it as a character-level language modeling task with a vocabulary size of around 200. During inference, we evaluate our model on the test set of enwik8 with a sequence length of 8,192 tokens and each token is equipped with a minimum of 7,000 tokens context window. We report the bits per character (BPC) metric for evaluation. As shown in Table 3, we compare SeqBoat with Transformer-XL , Adaptive Span , and MEGA-chunk. SeqBoat achieves the same performance as the previous best hybrid model, MEGA-chunk, but with a significant 1.16\(\) training speed up. This demonstrates the effectiveness of our proposed model in auto-regressive language modeling of long texts.

## 5 Analysis

In this section, we analyze experimental results of SMA from several research aspects. We also include comprehensive ablation studies of our model in Appendix E.

How much attention is needed for different sequence modeling tasks?As shown in Figure 3, we draw the activation time of the GAU module at different layers of our SeqBoat-full models for each of the task in the LRA benchmark. We measure the mean and the standard deviation (plotted as error bars) of the activation time on 100 sequences randomly sampled from the validation set of each task. The lines seem to be truncated due to the fact that different models have different number of layers. Generally, we can see that Image (in Blue and Circle) and Pathfinder (in Green and Triangle) need significantly more attentions than the text-based tasks (_i.e._, Text, Retrieval and ListOps). We can also see that our model trained on ListOps (in Orange and Square) has a much larger variance of GAU activation time than other tasks, which

  
**Model** & **\#Param.** & **Acc. (\(\))** & **Speed (\(\))** & **Mem. (\(\))** \\  S4 & 300K & **97.50** & - & - \\ MEGA-chunk & 300K & 96.92 & 1.00\(\) & 1.00\(\) \\
**SeqBoat** & 293K & 97.35 & 1.32\(\) & 0.44\(\) \\   

Table 2: Test accuracy on the Speech Commands 10-way classification task of raw audio (Input length 16,000). We also report the training speed up and the memory consumption reduction compared with MEGA-chunk.

Figure 3: The activation time (with error bars) of the GAU module at different layers of the SeqBoat-full model for different tasks in the LRA benchmark.

  
**Model** & **\#Param.** & **bpc (\(\))** & **Speed (\(\))** & **Mem. (\(\))** \\  Transformer-XL & 41M & 1.06 & - & - \\ Adaptive Span & 39M & 1.02 & - & - \\ MEGA-chunk & 39M & 1.02 & 1.00\(\) & 1.00\(\) \\
**SeqBoat** & 39M & 1.02 & 1.16\(\) & 1.07\(\) \\   

Table 3: Model performance on the enwik8 character-level language modeling task with a 8192 training sequence length. Training speed up and memory consumption reduction compared with MEGA-chunk are also reported.

may reflect that the task difficulty per data sample of ListOps has a larger variance than other tasks. Another trend we can see is that the bottom layers near the inputs need much less attention (and sometimes even drop the attention module entirely) than the high level layers. We conjecture that this is because the SSMs with first-order complexity is already capable to capture the low-level features while the attention modules are more demanded at higher levels for conducting more complex computations of second-order pairwise comparisons.

How is the learned sparse modular activation distributed over the input sequence?As shown in Figure 4, we draw the confidence probabilities of the sparse modular activation for both the SeqBoat-full and the SeqBoat models on the Pathfinder task from the LRA benchmark. Generally, we can see that different input sequences has different modular activation patterns, which indicates that our modular activation is dynamically sparse. We can also see that the SeqBoat activation pattern is learned to be more structured than the SeqBoat-full, which may explain the superior performance of SeqBoat over SeqBoat-full on the Pathfinder task. Another trend we can see is that different layers have their own patterns: _e.g._, the Layer 5 of the SeqBoat model seems trying to find the curly paths in the latent representation space, while the Layer 6 seems aggregating the results from the contiguous time steps.

How does the working memory size affect the speed-quality trade-off?We compare the trade-off ability of our SeqBoat with MEGA-chunk on Image and Pathfinder tasks of the LRA benchmark,

Figure 4: The confidence probabilities of the GAU modular activation at each time step for the last two layers of the SeqBoat-full and the SeqBoat model. The results are measured on three input sequences randomly sampled from the validation set of the Pathfinder task. The sequences are reshaped back to \(32 32\) squares for better visualization. Darker the blue color, higher the confidence. The white blocks indicate the time steps when GAUs are not activated.

Figure 5: Training Speed v.s. Validation Accuracy trade-off on Image and Pathfinder of the LRA benchmark for different models with varying memory/chunk sizes. SeqBoat keeps a working memory of the compressed sequence, while MEGA-chunk splits the input sequence into non-overlapping sub-sequences. The memory/chunk sizes are marked along the lines. The GPU-hours for Image are measured on NVIDIA RTX A5000 GPUs, and Pathfinder on V100 GPUs with 32GB memory.

as shown in Figure 5. We draw the Pareto frontiers by respectively varying the working memory size for SeqBoat and the chunk size for MEGA-chunk from the set \(\{16,32,64,128,256,512\}\). The results are averaged with three random seeds. We can see that our models have significantly better trade-off than MEGA-chunk models for both the Image and the Pathfinder tasks. Notably, the Mega-chunk models generally fail (achieving around 50% accuracy as random guessing) on Pathfinder tasks with less than 128 chunk size, while our SeqBoat can still achieve around 90% accuracy with only a memory size of 16. This indicates the effectiveness of our working memory mechanism in capturing long term interactions.

**How does the size of working memory affect the effective attention length of the GAU module?** We draw Figure 6 to investigate the relationship between the working memory size and the average effective attention length on both Pathfinder and the enwik8 datasets. We first measure the average attention distance from the query token to its key tokens for each of the time step, and the results are further averaged for all the time steps in a sequence. The average attention span on Pathfinder is measured on 100 sequences randomly sampled from the validation set. The average attention span on enwik8 is calculated based on a sequence of length 8,192 sampled from the validation set, and each token is equipped with a minimum of 7,000 tokens context window. From the figures, we can see a general trend that the models with a small working memory size (Size 16, 32 and 64 for Pathfinder, and Size 512 for enwik8) can have a far longer average attention span than its memory size. Notably, the layer 3 of the SeqBoat model with only 16 working memory slots surprisingly achieves an average attention span of 245 for the Pathfinder task. This partially explains the superior performance of our model over MEGA-chunk with small memory sizes and directly proves that our working memory mechanism can efficiently enable long-term interactions between the sequence elements.

## 6 Conclusion

In this paper, we present Sparse Modular Activation (SMA), a general mechanism for sparsely and dynamically activating sub-modules in neural networks. SMA not only has an efficient parallel implementation but also provides a theoretical guarantee for a full coverage of the function space of multiple modules. To address the challenges of combining attention with Linear State Space Models (SSMs), we apply SMA to develop a novel neural architecture, SeqBoat, that sparsely activates a Gated Attention Unit (GAU) based on the state representations from an SSM. The proposed model demonstrates a significantly better quality-efficiency trade-off compared to existing hybrid models across various sequence modeling tasks. To the best of our knowledge, SMA is the first mechanism that allows a neural network to obtain practical efficiency and complexity gains from sparsely activating a self-attention-like module. The strong empirical performance show that even with a preliminary application of SMA, we could already see substantial efficiency and interpretability benefits. For future work, we mainly want to explore the scaling behavior of SMA and also how to incorporate SMA with the pre-trained large language models.

Figure 6: Average Attention Span v.s. Layer Index on both the Pathfinder and the enwik8 tasks for different SeqBoat models with different working memory sizes. A smaller layer index indicates the layer is closer to the input. For reference, the average attention span of a sliding window based local attention is the half of the window size.