# Asymmetric Certified Robustness via

Feature-Convex Neural Networks

 Samuel Pfrommer

Equal contribution.

Brendon G. Anderson1

Julien Piet

Somayeh Sojoudi

University of California, Berkeley

{sam.pfrommer,bganderson,piet,sojoudi}@berkeley.edu

###### Abstract

Real-world adversarial attacks on machine learning models often feature an asymmetric structure wherein adversaries only attempt to induce false negatives (e.g., classify a spam email as not spam). We formalize the asymmetric robustness certification problem and correspondingly present the _feature-convex neural network_ architecture, which composes an input-convex neural network (ICNN) with a Lipschitz continuous feature map in order to achieve asymmetric adversarial robustness. We consider the aforementioned binary setting with one "sensitive" class, and for this class we prove deterministic, closed-form, and easily-computable certified robust radii for arbitrary \(_{p}\)-norms. We theoretically justify the use of these models by extending the universal approximation theorem for ICNN regression to the classification setting, and proving a lower bound on the probability that such models perfectly fit even unstructured uniformly distributed data in sufficiently high dimensions. Experiments on Malimg malware classification and subsets of the MNIST, Fashion-MNIST, and CIFAR-10 datasets show that feature-convex classifiers attain substantial certified \(_{1}\), \(_{2}\), and \(_{}\)-radii while being far more computationally efficient than competitive baselines. 1

## 1 Introduction

Although neural networks achieve state-of-the-art performance across a range of machine learning tasks, researchers have shown that they can be highly sensitive to adversarial inputs that are maliciously designed to fool the model . For example, the works Eykholt et al.  and Liu et al.  show that small physical and digital alterations of vehicle traffic signs can cause image classifiers to fail. In safety-critical applications of neural networks, such as autonomous driving  and medical diagnostics , this sensitivity to adversarial inputs is clearly unacceptable.

A line of heuristic defenses against adversarial inputs has been proposed, only to be defeated by stronger attack methods . This has led researchers to develop certifiably robust methods that provide a provable guarantee of safe performance. The strength of such certificates can be highly dependent on network architecture; general off-the-shelf models tend to have large Lipschitz constants, leading to loose Lipschitz-based robustness guarantees . Consequently, lines of work that impose certificate-amenable structures onto networks have been popularized, e.g., specialized model layers , randomized smoothing-based networks , and \(\) networks that are certified using convex optimization and mixed-integer programming . The first category only directly certifies against one specific choice of norm, producing poorly scaled radii for other norms in high dimensions. The latter two approaches incur serious computational challenges: randomized smoothing typically requires the classification of thousandsof randomly perturbed samples per input, while optimization-based solutions scale poorly to large networks.

Despite the moderate success of these certifiable classifiers, conventional assumptions in the literature are unnecessarily restrictive for many practical adversarial settings. Specifically, most works consider a multiclass setting where certificates are desired for inputs of any class. By contrast, many real-world adversarial attacks involve a binary setting with only one _sensitive class_ that must be robust to adversarial perturbations. Consider the representative problem of spam classification; a malicious adversary crafting a spam email will only attempt to fool the classifier toward the "not-spam" class--never conversely . Similar logic applies for a range of applications such as malware detection , malicious network traffic filtering , fake news and social media bot detection , hate speech removal , insurance claims filtering , and financial fraud detection .

The important asymmetric nature of these classification problems has long been recognized in various subfields, and some domain-specific attempts at robustification have been proposed with this in mind. This commonly involves robustifying against adversaries appending features to the classifier input. In spam classification, such an attack is known as the "good word" attack . In malware detection, numerous approaches have been proposed to provably counter such additive-only adversaries using special classifier structures such as non-negative networks  and monotonic classifiers . We note these works strictly focus on _additive_ adversaries and cannot handle general adversarial perturbations of the input that are capable of perturbing existing features. We propose adding this important asymmetric structure to the study of norm ball-certifiably robust classifiers. This narrowing of the problem to the asymmetric setting provides prospects for novel certifiable architectures, and we present feature-convex neural networks as one such possibility.

### Problem Statement and Contributions

This section formalizes the _asymmetric robustness certification problem_ for general norm-bounded adversaries. Specifically, we assume a binary classification setting wherein one class is "sensitive"--meaning we seek to certify that, if some input is classified into this sensitive class, then adversarial perturbations of sufficiently small magnitude cannot change the prediction.

Formally, consider a binary classifier \(f_{}^{d}\{1,2\}\), where class \(1\) is the sensitive class for which we desire certificates. We take \(f_{}\) to be a standard thresholded version of a soft classifier \(g^{d}\), expressible as \(f_{}(x)=T_{}(g(x))\), where \(T_{}\{1,2\}\) is the thresholding function defined by

\[T_{}(y)=1&y+>0,\\ 2&y+ 0,\] (1)

with \(\) being a user-specified parameter that shifts the classification threshold. A classifier \(f_{}\) is considered certifiably robust at a class \(1\) input \(x^{d}\) with a radius \(r(x)_{+}\) if \(f_{}(x+)=f_{}(x)=1\) for all \(^{d}\) with \(\|\|<r(x)\) for some norm \(\|\|\). Thus, \(\) induces a tradeoff between the clean accuracy on class \(2\) and certification performance on class \(1\). As \(\), \(f_{}\) approaches a constant classifier which achieves infinite class \(1\) certified radii but has zero class \(2\) accuracy.

For a particular choice of \(\), the performance of \(f_{}\) can be analyzed similarly to a typical certified classifier. Namely, it exhibits a class \(2\) clean accuracy \(_{2}()\) as well as a class \(1\) certified accuracy surface \(\) with values \((r,)\) that capture the fraction of the class \(1\) samples that can be certifiably classified by \(f_{}\) at radius \(r_{+}\). The class \(1\) clean accuracy \(_{1}()=(0,)\) is inferable from \(\) as the certified accuracy at \(r=0\).

The full asymmetric certification performance of the family of classifiers \(f_{}\) can be captured by plotting the surface \((r,)\), as will be shown in Figure 0(a). Instead of plotting against \(\) directly, we plot against the more informative difference in clean accuracies \(_{1}()-_{2}()\). This surface can be viewed as an asymmetric robustness analogue to the classic receiver operating characteristic curve.

Note that while computing the asymmetric robustness surface is possible for our feature-convex architecture (to be defined shortly), it is computationally prohibitive for conventional certification methods. We therefore standardize our comparisons throughout this work to the certified accuracy cross section \((r,^{*})\) for a \(^{*}\) such that clean accuracies are balanced in the sense that \(_{2}(^{*})=_{1}(^{*})\), noting that \(_{1}\) monotonically increases in \(\) and \(_{2}\) monotonically decreases in \(\). We discuss finding such a \(^{*}\) in Appendix E.4. This choice allows for a direct comparison of the resulting certified accuracy curves without considering the non-sensitive class clean accuracy.

With the above formalization in place, the goal at hand is two-fold: 1) develop a classification architecture tailored for the asymmetric setting with high robustness, as characterized by the surface \(\), and 2) provide efficient methods for computing the certified robust radii \(r(x)\) used to generate \(\).

**Contributions.** We tackle the above two goals by proposing _feature-convex neural networks_ and achieve the following contributions:

1. We exploit the feature-convex structure of the proposed classifier to provide asymmetrically tailored closed-form class \(1\) certified robust radii for arbitrary \(_{p}\)-norms, solving the second goal above and yielding efficient computation of \(\).
2. We characterize the decision region geometry of convex classifiers, extend the universal approximation theorem for input-convex \(\) neural networks to the classification setting, and show that convex classifiers are sufficiently expressive for high-dimensional data.
3. We evaluate against several baselines on MNIST 3-8 , Malimg malware classification , Fashion-MNIST shirts , and CIFAR-10 cats-dogs , and show that our classifiers yield certified robust radii competitive with the state-of-the-art, empirically addressing the first goal listed above.

All proofs and appendices can be found in the Supplemental Material.

### Related Works

**Certified adversarial robustness.** Three of the most popular approaches for generating robustness certificates are Lipschitz-based bounds, randomized smoothing, and optimization-based methods. Successfully bounding the Lipschitz constant of a neural network can give rise to an efficient certified radius of robustness, e.g., via the methods proposed in Hein and Andriushchenko . However, in practice such Lipschitz constants are too large to yield meaningful certificates, or it is computationally burdensome to compute or bound the Lipschitz constants in the first place [65; 23; 73]. To overcome these computational limitations, certain methods impose special structures on their model layers to provide immediate Lipschitz guarantees. Specifically, Trockman and Kolter  uses the Cayley transform to derive convolutional layers with immediate \(_{2}\)-Lipschitz constants, and Zhang et al.  introduces a \(_{}\)-distance neuron that provides similar Lipschitz guarantees with respect to the \(_{}\)-norm. We compare with both these approaches in our experiments.

Randomized smoothing, popularized by Lecuyer et al. , Li et al. , Cohen et al. , uses the expected prediction of a model when subjected to Gaussian input noise. These works derive \(_{2}\)-norm balls around inputs on which the smoothed classifier remains constant, but suffer from nondeterminism and high computational burden. Follow-up works generalize randomized smoothing to certify input regions defined by different metrics, e.g., Wasserstein, \(_{1}\)-, and \(_{}\)-norms [39; 62; 72]. Other works focus on enlarging the certified regions by optimizing the smoothing distribution [76; 21; 5], incorporating adversarial training into the base classifier [57; 78], and employing dimensionality reduction at the input .

Optimization-based certificates typically seek to derive a tractable over-approximation of the set of possible outputs when the input is subject to adversarial perturbations, and show that this over-approximation is safe. Various over-approximations have been proposed, e.g., based on linear programming and bounding [68; 67], semidefinite programming , and branch-and-bound [4; 45; 66]. The \(,\)-CROWN method  uses an efficient bound propagation to linearly bound the neural network output in conjunction with a per-neuron branching heuristic to achieve state-of-the-art certified radii, winning both the 2021 and the 2022 VNN certification competitions [8; 48]. In contrast to optimization-based methods, our approach directly exploits the convex structure of input-convex neural networks to derive closed-form robustness certificates, altogether avoiding any efficiency-tightness tradeoffs.

**Input-convex neural networks.** Input-convex neural networks, popularized by Amos et al. , are a class of parameterized models whose input-output mapping is convex. The authors develop tractable methods to learn input-convex neural networks, and show that such models yield state-of-the-art results in a variety of domains where convexity may be exploited, e.g., optimization-based inference. Subsequent works propose novel applications of input-convex neural networks in areassuch as optimal control and reinforcement learning [16; 75], optimal transport , and optimal power flow [17; 79]. Other works have generalized input-convex networks to input-invex networks [58; 51] and global optimization networks  so as to maintain the benign optimization properties of input-convexity. The authors of Siahkamari et al.  present algorithms for efficiently learning convex functions, while Chen et al. , Kim and Kim  derive universal approximation theorems for input-convex neural networks in the convex regression setting. The work Sivaprasad et al.  shows that input-convex neural networks do not suffer from overfitting, and generalize better than multilayer perceptrons on common benchmark datasets. In this work, we incorporate input-convex neural networks as a part of our feature-convex architecture and leverage convexity properties to derive novel robustness guarantees.

### Notations

The sets of natural numbers, real numbers, and nonnegative real numbers are denoted by \(\), \(\), and \(_{+}\) respectively. The \(d d\) identity matrix is written as \(I_{d}^{d d}\), and the identity map on \(^{d}\) is denoted by \( x x\). For \(A^{n d}\), we define \(|A|^{n d}\) by \(|A|_{ij}=|A_{ij}|\) for all \(i,j\), and we write \(A 0\) if and only if \(A_{ij} 0\) for all \(i,j\). The \(_{p}\)-norm on \(^{d}\) is given by \(\|\|_{p} x(|x_{1}|^{p}++|x_{d}|^{p})^{1/p}\) for \(p[1,)\) and by \(\|\|_{p} x\{|x_{1}|,,|x_{d}|\}\) for \(p=\). The dual norm of \(\|\|_{p}\) is denoted by \(\|\|_{p,*}\). The convex hull of a set \(X^{d}\) is denoted by \((X)\). The subdifferential of a convex function \(g^{d}\) at \(x^{d}\) is denoted by \( g(x)\). If \(^{d}\) is a random variable on a probability space \((,,)\) and \(P\) is a predicate defined on \(^{d}\), then we write \((P())\) to mean \((\{:P(())\})\). Lebesgue measure on \(^{d}\) is denoted by \(m\). We define \(\) as \((x)=\{0,x\}\), and if \(x^{d}\), \((x)\) denotes \(((x_{1}),,(x_{d}))\). We recall the threshold function \(T_{}\{1,2\}\) defined by (1), and we define \(T=T_{0}\). For a function \(^{d}^{q}\) and \(p[1,]\), we define \(_{p}()=\{K 0:\|(x)-(x^{})\|_{p}  K\|x-x^{}\|_{p}\) for all \(x,x^{}^{d}\}\), and if \(_{p}()<\) we say that \(\) is Lipschitz continuous with constant \(_{p}()\) (with respect to the \(_{p}\)-norm).

## 2 Feature-Convex Classifiers

Let \(d,q\) and \(p[1,]\) be fixed, and consider the task of classifying inputs from a subset of \(^{d}\) into a fixed set of classes \(\). In what follows, we restrict to the binary setting where \(=\{1,2\}\) and class \(1\) is the sensitive class for which we desire robustness certificates (Section 1). In Appendix A, we briefly discuss avenues to generalize our framework to multiclass settings using one-versus-all and sequential classification methodologies and provide a proof-of-concept example for the Malimg dataset.

We now formally define the classifiers considered in this work. Note that the classification threshold \(\) discussed in Section 1.1 is omitted for simplicity.

**Definition 2.1**.: Let \(f^{d}\{1,2\}\) be defined by \(f(x)=T(g((x)))\) for some \(^{d}^{q}\) and some \(g^{q}\). Then \(f\) is said to be a _feature-convex classifier_ if the _feature map_\(\) is Lipschitz continuous with constant \(_{p}()<\) and \(g\) is a convex function.

We denote the class of all feature-convex classifiers by \(\). Furthermore, for \(q=d\), the subclass of all feature-convex classifiers with \(=\) is denoted by \(_{}\).

As we will see in Section 3.1, defining our classifiers using the composition of a convex classifier with a Lipschitz feature map enables the fast computation of certified regions in the input space. This naturally arises from the global underestimation of convex functions by first-order Taylor approximations. Since sublevel sets of such \(g\) are restricted to be convex, the feature map \(\) is included to increase the representation power of our architecture (see Appendix B for a motivating example). In practice, we find that it suffices to choose \(\) to be a simple map with a small closed-form Lipschitz constant. For example, in our experiments that follow with \(q=2d\), we choose \((x)=(x-,|x-|)\) with a constant channel-wise dataset mean \(\), yielding \(_{1}() 2\), \(_{2}()\), and \(_{}() 1\). Although this particular choice of \(\) is convex, the function \(g\) need not be monotone, and therefore the composition \(g\) is nonconvex in general. The prediction and certification of feature-convex classifiers are illustrated in Figure 0(b).

In practice, we implement feature-convex classifiers using parameterizations of \(g\), which we now make explicit. Following Amos et al. , we instantiate \(g\) as a neural network with nonnegative weight matrices and nondecreasing convex nonlinearities. Specifically, we consider \(\) nonlinearities, which is not restrictive, as our universal approximation result in Theorem 3.6 proves.

**Definition 2.2**.: A _feature-convex \(\) neural network_ is a function \(^{d}\{1,2\}\) defined by \((x)=T(((x)))\) with \(^{d}^{q}\) Lipschitz continuous with constant \(_{p}()<\) and \(^{q}\) defined by

\[(x^{(0)})=A^{(L)}x^{(L-1)}+b^{(L)}+C^{(L)}x^{(0)}, x^{(l)}=(A^{(l)}x^{(l-1)}+b^{(l)}+C^{(l)}x^{(0)}),\]

for all \(l\{1,2,,L-1\}\) for some \(L\), \(L>1\), and for some consistently sized matrices \(A^{(l)},C^{(l)}\) and vectors \(b^{(l)}\) satisfying \(A^{(l)} 0\) for all \(l\{2,3,,L\}\).

Going forward, we denote the class of all feature-convex \(\) neural networks by \(}\). Furthermore, if \(q=d\), the subclass of all feature-convex \(\) neural networks with \(=\) is denoted by \(}_{}\), which corresponds to the input-convex \(\) neural networks proposed in Amos et al. .

For every \(}\), it holds that \(\) is convex due to the rules for composition and nonnegatively weighted sums of convex functions [13, Section 3.2], and therefore \(}\) and \(}_{}_{}\). The "passthrough" weights \(C^{(l)}\) were originally included by Amos et al.  to improve the practical performance of the architecture. In some of our more challenging experiments that follow, we remove these passthrough operations and instead add residual identity mappings between hidden layers, which also preserves convexity. We note that the transformations defined by \(A^{(l)}\) and \(C^{(l)}\) can be taken to be convolutions, which are nonnegatively weighted linear operations and thus preserve convexity .

## 3 Certification and Analysis of Feature-Convex Classifiers

We present our main theoretical results in this section. First, we derive asymmetric robustness certificates (Theorem 3.1) for our feature-convex classifiers in Section 3.1. Then, in Section 3.2, we introduce the notion of convexly separable sets in order to theoretically characterize the representation power of our classifiers. Our primary representation results give a universal function approximation

Figure 1: (a) The asymmetric certified accuracy surface \((r,)\) for MNIST \(3\)-\(8\), as described in Section 1.1. The “clean accuracy difference” axis plots \(_{1}()-_{2}()\), and the black line highlights the certified robustness curve for when clean accuracy is equal across the two classes. (b) Illustration of feature-convex classifiers and their certification. Since \(g\) is convex, it is globally underapproximated by its tangent plane at \((x)\), yielding certified sets for norm balls in the higher-dimensional feature space. Lipschitzness of \(\) then yields appropriately scaled certificates in the original input space.

theorem for our classifiers with \(=\) and \(\) activation functions (Theorem 3.6) and show that such classifiers can perfectly fit convexly separable datasets (Theorem 3.7), including the CIFAR-10 cats-dogs training data (Fact 3.8). We also show that this strong learning capacity generalizes by proving that feature-convex classifiers can perfectly fit high-dimensional uniformly distributed data with high probability (Theorem 3.10).

### Certified Robustness Guarantees

In this section, we address the asymmetric certified robustness problem by providing class \(1\) robustness certificates for feature-convex classifiers \(f\). Such robustness corresponds to proving the absence of false negatives in the case that class \(1\) represents positives and class \(2\) represents negatives. For example, if in a malware detection setting class \(1\) represents malware and class \(2\) represents non-malware, the following certificate gives a lower bound on the magnitude of the malware file alteration needed in order to misclassify the file as non-malware.

**Theorem 3.1**.: _Let \(f\) be as in Definition 2.1 and let \(x f^{-1}(\{1\})=\{x^{}^{d}:f(x^{})=1\}\). If \( g((x))^{q}\) is a nonzero subgradient of the convex function \(g\) at \((x)\), then \(f(x+)=1\) for all \(^{d}\) such that_

\[\|\|_{p}<r(x)_{p}()\|  g((x))\|_{p,*}}.\]

_Remark 3.2_.: For \(f\) and \(x f^{-1}(\{1\})\), a subgradient \( g((x))^{q}\) of \(g\) always exists at \((x)\), since the subdifferential \( g((x))\) is a nonempty closed bounded convex set, as \(g\) is a finite convex function on all of \(^{q}\)--see Theorem 23.4 in Rockafellar  and the discussion thereafter. Furthermore, if \(f\) is not a constant classifier, such a subgradient \( g((x))\) must necessarily be nonzero, since, if it were zero, then \(g(y) g((x))+ g((x))^{}(y-(x))=g((x))>0\) for all \(y^{q}\), implying that \(f\) identically predicts class \(1\), which is a contradiction. Thus, the certified radius given in Theorem 3.1 is always well-defined in practical settings.

Theorem 3.1 is derived from the fact that a convex function is globally underapproximated by any tangent plane. The nonconstant terms in Theorem 3.1 afford an intuitive interpretation: the radius scales proportionally to the confidence \(g((x))\) and inversely with the input sensitivity \(\| g((x))\|_{p,*}\). In practice, \(_{p}()\) can be made quite small as mentioned in Section 2, and furthermore the subgradient \( g((x))\) is easily evaluated as the Jacobian of \(g\) at \((x)\) using standard automatic differentiation packages. This provides fast, deterministic class \(1\) certificates for any \(_{p}\)-norm without modification of the feature-convex network's training procedure or architecture. We emphasize that our robustness certificates of Theorem 3.1 are independent of the architecture of \(f\).

### Representation Power Characterization

We now restrict our analysis to the class \(_{}\) of feature-convex classifiers with an identity feature map. This can be equivalently considered as the class of classifiers for which the input-to-logit map is convex. We therefore refer to models in \(_{}\) as _input-convex classifiers_. While the feature map \(\) is useful in boosting the practical performance of our classifiers, the theoretical results in this section suggest that there is significant potential in using input-convex classifiers as a standalone solution.

**Classifying convexly separable sets.** We begin by introducing the notion of convexly separable sets, which are intimately related to decision regions representable by the class \(_{}\).

**Definition 3.3**.: Let \(X_{1},X_{2}^{d}\). The ordered pair \((X_{1},X_{2})\) is said to be _convexly separable_ if there exists a nonempty closed convex set \(X^{d}\) such that \(X_{2} X\) and \(X_{1}^{d} X\).

Notice that it may be the case that a pair \((X_{1},X_{2})\) is convexly separable yet the pair \((X_{2},X_{1})\) is not. Although low-dimensional intuition may raise concerns regarding the convex separability of binary-labeled data, we will soon see in Fact 3.8 and Theorem 3.10 that convex separability typically holds in high dimensions. We now show that convexly separable datasets possess the property that they may always be perfectly fit by input-convex classifiers.

**Proposition 3.4**.: _For any nonempty closed convex set \(X^{d}\), there exists \(f_{}\) such that \(X=f^{-1}(\{2\})=\{x^{d}:f(x)=2\}\). In particular, this shows that if \((X_{1},X_{2})\) is a convexly separable pair of subsets of \(^{d}\), then there exists \(f_{}\) such that \(f(x)=1\) for all \(x X_{1}\) and \(f(x)=2\) for all \(x X_{2}\)._

We also show that the converse of Proposition 3.4 holds: the geometry of the decision regions of classifiers in \(_{}\) consists of a convex set and its complement.

**Proposition 3.5**.: _Let \(f_{}\). The decision region under \(f\) associated to class \(2\), namely \(X f^{-1}(\{2\})=\{x^{d}:f(x)=2\}\), is a closed convex set._

Note that this is not necessarily true for our more general feature-convex architectures with \(\). We continue our theoretical analysis of input-convex classifiers by extending the universal approximation theorem for regressing upon real-valued convex functions (given in Chen et al. ) to the classification setting. In particular, Theorem 3.6 below shows that any input-convex classifier \(f_{}\) can be approximated arbitrarily well on any compact set by \(\) neural networks with nonnegative weights. Here, "arbitrarily well" means that the set of inputs where the neural network prediction differs from that of \(f\) can be made to have arbitrarily small Lebesgue measure.

**Theorem 3.6**.: _For any \(f_{}\), any compact convex subset \(X\) of \(^{d}\), and any \(>0\), there exists \(}_{}\) such that \(m(\{x X:(x) f(x)\})<\)._

An extension of the proof of Theorem 3.6 combined with Proposition 3.4 yields that input-convex \(\) neural networks can perfectly fit convexly separable sampled datasets.

**Theorem 3.7**.: _If \((X_{1},X_{2})\) is a convexly separable pair of finite subsets of \(^{d}\), then there exists \(}_{}\) such that \((x)=1\) for all \(x X_{1}\) and \((x)=2\) for all \(x X_{2}\)._

Theorems 3.6 and 3.7, being specialized to models with \(\) activation functions, theoretically justify the particular parameterization in Definition 2.2 for learning feature-convex classifiers to fit convexly separable data.

Empirical convex separability.Interestingly, we find empirically that high-dimensional image training data is convexly separable. We illustrate this in Appendix D by attempting to reconstruct a CIFAR-10 cat image from a convex combination of the dogs and vice versa; the error is significantly positive for _every_ sample in the training dataset, and image reconstruction is visually poor. This fact, combined with Theorem 3.7, immediately yields the following result.

**Fact 3.8**.: _There exists \(}_{}\) such that \(\) achieves perfect training accuracy for the unaugmented CIFAR-10 cats-versus-dogs dataset._

The gap between this theoretical guarantee and our practical performance is large; without the feature map, our CIFAR-10 cats-dogs classifier achieves just \(73.4\%\) training accuracy (Table 4). While high training accuracy does not necessarily imply strong test set performance, Fact 3.8 demonstrates that the typical deep learning paradigm of overfitting to the training dataset is theoretically attainable . We thus posit that there is substantial room for improvement in the design and optimization of input-convex classifiers. We leave the challenge of overfitting to the CIFAR-10 cats-dogs training data with an input-convex classifier as an open research problem for the field.

**Open Problem 3.9**.: _Learn an input-convex \(\) neural network that achieves \(100\%\) training accuracy on the unaugmented CIFAR-10 cats-versus-dogs dataset._

Convex separability in high dimensions.We conclude by investigating _why_ the convex separability property that allows for Fact 3.8 may hold for natural image datasets. We argue that dimensionality facilitates this phenomenon by showing that data is easily separated by some \(f}_{}\) when \(d\) is sufficiently large. In particular, although it may seem restrictive to rely on models in \(}_{}\) with convex class \(2\) decision regions, we show in Theorem 3.10 below that even uninformative data distributions that are seemingly difficult to classify may be fit by such models with high probability as the dimensionality of the data increases.

**Theorem 3.10**.: _Consider \(M,N\). Let \(X_{1}=\{x^{(1)},,x^{(M)}\}^{d}\) and \(X_{2}=\{y^{(1)},,y^{(N)}\}^{d}\) be samples with all elements \(x_{k}^{(i)},y_{l}^{(j)}\) drawn independently and identically from the uniform probability distribution on \([-1,1]\). Then, it holds that_

\[((X_{1},X_{2})) 1-(1-)^{d}&d,\\ 1&d M+N.\] (2)

_In particular, \(}_{}\) contains an input-convex \(\) neural network that classifies all \(x^{(i)}\) into class \(1\) and all \(y^{(j)}\) into class \(2\) almost surely for sufficiently large dimensions \(d\)._

Although the uniformly distributed data in Theorem 3.10 is unrealistic in practice, the result demonstrates that the class \(}_{}\) of input-convex \(\) neural networks has sufficient complexity to fit even the most unstructured data in high dimensions. Despite this ability, researchers have found that current input-convex neural networks tend to not overfit in practice, yielding small generalization gaps relative to conventional neural networks . Achieving the modern deep learning paradigm of overfitting to the training dataset with input-convex networks is an exciting open challenge .

## 4 Experiments

This section compares our feature-convex classifiers against a variety of state-of-the-art baselines in the asymmetric setting. Before discussing the results, we briefly describe the datasets, baselines, and architectures used. For a more in-depth description and hyperparameter details, see Appendix E.

**Datasets.** We use four datasets. First, we consider distinguishing between \(28 28\) greyscale MNIST digits \(3\) and \(8\), which are generally more visually similar and challenging to distinguish than other digit pairs. Next, we consider identifying malware from the "Allaple.A" class in the Malimg dataset of \(512 512\) bytewise encodings of malware . Next, we consider distinguishing between shirts and T-shirts in the Fashion-MNIST dataset of \(28 28\) greyscale images , which tend to be the hardest classes to distinguish . Finally, we consider the \(32 32\) RGB CIFAR-10 cat and dog images since they are relatively difficult to distinguish [26; 43; 30]. The latter two datasets can be considered as our more challenging settings. All pixel values are normalized into the interval \(\).

**Baseline Methods.** We consider several state-of-the-art randomized and deterministic baselines. For all datasets, we evaluate the randomized smoothing certificates of Yang et al.  for the Gaussian, Laplacian, and uniform distributions trained with noise augmentation (denoted RS Gaussian, RS Laplacian, and RS Uniform, respectively), as well as the deterministic bound propagation framework \(,\)-CROWN , which is scatter plotted since certification is only reported as a binary answer at a given radius. We also evaluate, when applicable, deterministic certified methods for each norm ball. These include the splitting-noise \(_{1}\)-certificates from Levine and Feizi  (denoted Splitting), the orthogonality-based \(_{2}\)-certificates from Trockman and Kolter  (denoted Cayley), and the \(_{}\)-distance-based \(_{}\)-certificates from Zhang et al.  (denoted \(_{}\)-Net). The last two deterministic methods are not evaluated on the large-scale Malimg dataset due to their prohibitive runtime. Furthermore, the \(_{}\)-Net was unable to significantly outperform a random classifier on the CIFAR-10 cats-dogs dataset, and is therefore only included in the MNIST 3-8 and Fashion-MNIST shirts experiments. Notice that the three randomized smoothing baselines have fundamentally different predictions and certificates than the deterministic methods (including ours), namely, the predictions are random and the certificates hold only with high probability.

**Feature-Convex Architecture.** Our simple experiments (MNIST 3-8 and Malimg) require no feature map to achieve high accuracy (\(=\)). The Fashion-MNIST shirts dataset also benefited minimally from the feature map inclusion. For the CIFAR-10 cats-dogs task, we let our feature map be the concatenation \((x)=(x-,|x-|)\), as motivated by Appendix B, where \(\) is the channel-wise dataset mean (e.g., size \(3\) for an RGB image) broadcasted to the appropriate dimensions. Our MNIST 3-8 and Malimg architecture then consists of a simple two-hidden-layer input-convex multilayer perceptron with \((n_{1},n_{2})=(200,50)\) hidden features, \(\) nonlinearities, and passt through weights. For the Fashion-MNIST shirts (CIFAR-10 cats-dogs, resp.) dataset, we use a convex ConvNet architecture consisting of \(3\) (\(5\), resp.) convolutional, BatchNorm, and \(\) layers. All models are trained using SGD on the standard binary cross entropy loss with Jacobian regularization, and clean accuracies are balanced as described in Section 1.1 and Appendix E.4 to ensure a fair comparison of different robustness certificates.

Results and Discussion.Experimental results for \(_{1}\)-norm certification are reported in Figure 2, where our feature-convex classifier radii, denoted by Convex*, are similar or better than all other baselines across all datasets. Also reported is each method's clean test accuracy without any attacks, denoted by "clean." Due to space constraints, we defer the corresponding plots for \(_{2}\)- and \(_{}\)-norm balls to Appendix F, where our certified radii are not dominant but still comparable to methods tailored specifically for a particular norm. We accomplish this while maintaining completely deterministic, closed-form certificates with orders-of-magnitude faster computation time than competitive baselines.

For the MNIST 3-8 and Malimg datasets (Figures 1(a) and 1(b)), all methods achieve high clean test accuracy. Our \(_{1}\)-radii scale exceptionally well with the dimensionality of the input, with two orders of magnitude improvement over smoothing baselines for the Malimg dataset. The Malimg certificates in particular have an interesting concrete interpretation. As each pixel corresponds to one byte in the original malware file, an \(_{1}\)-certificate of radius \(r\) provides a robustness certificate for up to \(r\) bytes in the file. Namely, even if a malware designer were to arbitrarily change \(r\) malware bytes, they would be unable to fool our classifier into returning a false negative. We note that this is primarily illustrative and is unlikely to have an immediate practical impact as small semantic changes (e.g., reordering unrelated instructions) can induce large \(_{p}\)-norm shifts.

While our method produces competitive robustness certificates for \(_{2}\)- and \(_{}\)-norms (Appendix F), it offers the largest improvement for \(_{1}\)-certificates in the high-dimensional image spaces considered. This is likely due to the characteristics of the subgradient dual norm factor in the denominator of

Figure 2: Class \(1\) certified radii curves for the \(_{1}\)-norm. Note the \(\)-scale on the Malimg plot.

Theorem 3.1. The dual of the \(_{1}\)-norm is the \(_{}\)-norm, which selects the largest magnitude element in the gradient of the output logit with respect to the input pixels. As the input image scales, it is natural for the classifier to become less dependent on any one specific pixel, shrinking the denominator in Theorem 3.1. Conversely, when certifying for the \(_{}\)-norm, one must evaluate the \(_{1}\)-norm of the gradient, which scales proportionally to the input size. Nevertheless, we find in Appendix F that our \(_{2}\)- and \(_{}\)-radii are generally comparable those of the baselines while maintaining speed and determinism.

Our feature-convex neural network certificates are almost immediate, requiring just one forward pass and one backward pass through the network. This certification procedure requires a few milliseconds per sample on our hardware and scales well with network size. This is substantially faster than the runtime for randomized smoothing, which scales from several seconds per CIFAR-10 image to minutes for an ImageNet image . The only method that rivaled our \(_{1}\)-norm certificates was \(\), \(\)-CROWN; however, such bound propagation frameworks suffer from exponential computational complexity in network size, and even for small CIFAR-10 ConvNets typically take on the order of minutes to certify nontrivial radii. For computational tractability, we therefore used a smaller network in our experiments (Appendix E). Certification time for all methods is reported in Table 1.

Unlike the randomized smoothing baselines, our method is completely deterministic in both prediction and certification. Randomized prediction poses a particular problem for randomized smoothing certificates: even for a perturbation of a "certified" magnitude, repeated evaluations at the perturbed point will eventually yield misclassification for any nontrivial classifier. While the splitting-based certificates of Levine and Feizi  are deterministic, they only certify quantized (not continuous) \(_{1}\)-perturbations, which scale poorly to \(_{2}\)- and \(_{}\)-certificates (Appendix F). Furthermore, the certification runtime grows linearly in the smoothing noise \(\); evaluating the certified radii at \(\) used for the Malimg experiment takes several minutes per sample.

Ablation tests examining the impact of Jacobian regularization, the feature map \(\), and data augmentation are included in Appendix G. We illustrate the certification performance of our method across all combinations of MNIST classes in Appendix H.

## 5 Conclusion

This work introduces the problem of asymmetric certified robustness, which we show naturally applies to a number of practical adversarial settings. We define feature-convex classifiers in this context and theoretically characterize their representation power from geometric, approximation theoretic, and statistical lenses. Closed-form sensitive-class certified robust radii for the feature-convex architecture are provided for arbitrary \(_{p}\)-norms. We find that our \(_{1}\)-robustness certificates in particular match or outperform those of the current state-of-the-art methods, with our \(_{2}\)- and \(_{}\)-radii also competitive to methods tailored for a particular norm. Unlike smoothing and bound propagation baselines, we accomplish this with a completely deterministic and near-immediate computation scheme. We also show theoretically that significant performance improvements should be realizable for natural image datasets such as CIFAR-10 cats-versus-dogs. Possible directions for future research include bridging the gap between the theoretical power of feature-convex models and their practical implementation, as well as exploring more sophisticated choices of the feature map \(\).

    & MNIST 3-8 & Malimg & Fashion-MNIST shirts & CIFAR-10 cats-dogs \\  Convex\({}^{*}\) & 0.00159 & 0.00295 & 0.00180 & 0.00180 \\ RS Gaussian & 2.16 & 111.9 & 2.41 & 5.78 \\ RS Laplacian & 2.23 & 114.8 & 2.51 & 5.81 \\ RS Uniform & 2.18 & 112.4 & 2.44 & 5.80 \\ Splitting & 0.597 & 994.5 & 0.185 & 0.774 \\ \(,\)-CROWN\({}^{}\) & 6.088 & 6.138 & 6.425 & 9.133 \\ Cayley & 0.000505 & — & 0.0451 & 0.0441 \\ \(_{}\)-Net\({}^{}\) & 0.138 & — & 0.115 & — \\   

Table 1: Average runtimes (seconds) per input for computing the \(_{1}\), \(_{2}\), and \(_{}\)-robust radii. \({}^{*}\) = our method. \({}^{}\) = per-property verification time. \({}^{}\) = certified radius computed via binary search.