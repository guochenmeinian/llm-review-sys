# User-item fairness tradeoffs in recommendations

Sophie Greenwood

Computer Science

Cornell Tech

&Sudalakshmee Chiniah

Operations Research and Information Engineering

Cornell Tech

&Nikhil Garg

Operations Research and Information Engineering

Cornell Tech

###### Abstract

In the basic recommendation paradigm, the most (predicted) relevant item is recommended to each user. This may result in some items receiving lower exposure than they "should"; to counter this, several algorithmic approaches have been developed to ensure _item fairness_. These approaches necessarily degrade recommendations for some users to improve outcomes for items, leading to _user fairness_ concerns. In turn, a recent line of work has focused on developing algorithms for multi-sided fairness, to jointly optimize user fairness, item fairness, and overall recommendation quality. This induces the question: _what is the tradeoff between these objectives, and what are the characteristics of (multi-objective) optimal solutions?_ Theoretically, we develop a model of recommendations with user and item fairness objectives and characterize the solutions of fairness-constrained optimization. We identify two phenomena: (a) when user preferences are diverse, there is "free" item and user fairness; and (b) users whose preferences are misestimated can be _especially_ disadvantaged by item fairness constraints. Empirically, we prototype a recommendation system for preprints on arXiv and implement our framework, measuring the phenomena in practice and showing how these phenomena inform the _design_ of markets with recommendation systems-intermediated matching.

## 1 Introduction

Recommendation systems are employed throughout modern online platforms to suggest _items_ (media, songs, books, products, or jobs) to _users_ (viewers, listeners, readers, consumers, or job seekers). The platform learns user preferences and shows each user personalized recommendations. One recommendation paradigm is to simply show the user the items they most prefer. However, this approach may result in disparately poor outcomes for some items, which may not be most preferred by any user . For example, in our empirical application in prototyping a recommender system for arXiv prepints, we find that on average more than 47% of papers have less than a \(0.0001\%\) probability of being recommended to any user, even when the number of users and items are the same. Thus, many algorithmic techniques have been proposed to improve item fairness in recommendation . However, by not solely optimizing for user engagement, these techniques impose a cost both to overall recommendation quality  and especially for some individual users more than others.

Accordingly, algorithms have recently been introduced to address the problem of _two-sided fairness_ (or _multi-sided fairness_), in which the platform aims to balance user fairness, item fairness, and overall recommendation quality . These algorithms formalize the desired balance in terms of an optimization problem - for example, maximizing the difference between overall recommendation quality and unfairness penalties , or the overall recommendation quality subject to fairnessconstraints . The relative importance of user and item fairness is described by the relative strength of the respective unfairness penalties or slack in the fairness constraints.

However, an open question is, _what are the implications of such algorithms on the recommendations, i.e., what do multi-sided constraints do, and what is the price of (multi-sided) fairness?_ More specifically, _are there settings in which we can simultaneously maximize all objectives, "for free?," as opposed to there being large tradeoffs as commonly emphasized? Are some users or items - for example, those new to the platform - more affected than others? How do the answers to the above questions depend on the context and, in real-world settings, do "fairness" constraints substantially affect recommendation characteristics?_ Such real-world considerations are essential for platform designers to understand. In fact, a recent survey and critique of the fair ranking literature advocated for such grounded analyses to understand algorithmic implications and tradeoffs , as opposed to black-box deployment of fairness algorithms.

Answering such questions is challenging. Theoretically, multi-sided fairness is cast as an optimization problem, and conceptually characterizing optimization solutions is often intractable. For example, given an arbitrary utility matrix and a constraint on the exposure provided to each item, it is not tractable to calculate recommendations in closed form: the solution depends on global structure, users may not receive their most preferred items, and items may not be recommended to the users who most prefer them. Then, once phenomena are theoretically identified, empirically verifying phenomena requires specifying a recommendation setting and measuring user-item utilities as a function of their characteristics. Given these challenges, our contributions are as follows.

Theoretical framework to characterize solutions of multi-sided fair recommendations.We formulate a concave optimization problem in which user fairness is formalized as an objective on the minimum normalized utility provided to each user - and item fairness constraints determine the problem's feasible region, through the solutions of another concave optimization. This formulation qualitatively captures standard multi-objective approaches for user-item fairness [5; 12]. We show that when item fairness constraints are maximal, the solutions to this optimization problem (recommendation probabilities for each user) have a sparse structure that can be characterized as a function of the problem inputs (e.g., estimated utilities for each user-item pair, or slack given in the item fairness constraint).

Conceptual insights on the price of fairness.We use our theoretical framework to characterize user-item fairness tradeoffs. We identify two phenomena: (a) "free fairness" as a function of user preference diversity: if users have sufficiently diverse preferences, imposing item fairness constraints can have large benefits to individual items with little cost to users, i.e., there is a small _price of fairness_. (b) "Reinforced disparate effects" due to preference uncertainty. Of course, users for whom the platform has poor preference estimation (e.g., "cold start" users on whom the platform has no data) typically receive more inaccurate recommendations; we show that this effect may be _worsened_ with item fairness constraints, in a worst case sense: when a user's preferences are uncertain, item-fair recommendation algorithms will recommend them the globally least preferred items - _even when attempting to maximize the minimum user utility_.

Empirical measurement.Finally, we use real data to prototype a recommendation engine for new arXiv preprints and use this system to measure the above phenomena in practice. For example, we find that more homogeneous groups of users have steeper user-item fairness tradeoffs - as theoretically predicted, diverse user preferences decrease the price of item fairness. Furthermore, we find that the "price of misestimation" is high (users for whom less training data is available receive poor recommendations), but on average item fairness constraints do not increase this cost.

Putting things together, we show that the real-world effects of user-item recommendation fairness constraints heavily depend on the empirical context. In some cases, "item fairness" comes for free, with little cost to users. In others, deploying such an algorithm may lead to especially poor recommendations for some users, in ways that cannot be mechanically addressed by adding user fairness terms. We urge designers of fair recommendation systems in practice to develop such evaluations to measure such individual-level effects, and for researchers to further characterize the potential implications of such algorithms. Our code is available at the following repository: https://github.com/vschiniah/ArXiv_Recommendation_Research.

## 2 Formal Model

Our setup is characterized by user and item (estimated) utilities, recommendation optimization with user and item fairness desiderata, and evaluation of the effects of such desiderata. (As discussed below, some of these modeling choices are made for concreteness, and our results extend beyond these specific choices).

Users, items, and utilities.There is a finite population of \(m\) users and \(n\) items. Let \(w_{ij}>0\) be the utility of recommending item \(j\) to user \(i\); this could represent a click-through rate or purchase probability. We suppose that user-item utilities are symmetric: each of the user \(i\) and item \(j\) receives utility \(w_{ij}\) from being recommended item \(j\).

Let \(_{n-1}\) denote the simplex in \(^{n}\). The platform's task is to choose a recommendation policy \(_{n-1}^{m}\). For each user \(i\), the platform will recommend one item to user \(i\) selected randomly according to the distribution \(_{i}\). Given a recommendation policy \(\), user \(i\)'s expected utility from using the platform is \(_{j}_{ij}w_{ij}\); item \(j\)'s expected utility is \(_{i}_{ij}w_{ij}\), where \(_{j}_{ij}=1\) for each user \(i\).

Fairness desiderata.We suppose that the platform uses as a benchmark, for each user or item, the _best_ thing that it could do for that agent if it ignored the utilities of others. Thus, given a recommendation policy \(\), let \(U_{i}(,w)\) be user \(i\)'s utility from \(\), normalized by the utility \(_{j}w_{ij}\) they would receive from being recommended their best match. Let \(I_{j}(,w)\) be item \(j\)'s utility from \(\), normalized by the utility \(j\) receives if it is recommended to _every_ user, \(_{i}w_{ij}\). The normalized utilities are:

\[U_{i}(,w)=_{ij}w_{ij}}{_{j}w_{ij}}, I_{j}(,w)=_{ij}w_{ij}}{_{i}w_{ij}}.\]

The normalizations capture that recommendations should not be affected by scaling utilities, or be distorted by a user who is not satisfied with any item or an item that is generally undesirable to users.

Given a recommendation policy \(\), user fairness is quantified as the _minimum_ normalized user utility \(U_{}(,w)=_{i}U_{i}(,w)\), and item fairness analogously as \(I_{}(,w)=_{j}I_{j}(,w)\).

Multi-objective recommendation optimization.We suppose that the platform seeks to satisfy its fairness desiderata as follows. At the extremes, the platform could choose a maximally fair solution for one side, ignoring the other. Denote the optimal user fair utility as \(U_{}^{*}(w):=_{}U_{}(,w)\) (achieved by giving each user their favorite item deterministically), and the optimal item fair utility as \(I_{}^{*}(w):=_{}I_{}(,w)\).

Finally, we cast the two-sided fair optimization - for the optimal \(\)-constrained user fair solution - as

\[U_{}^{*}(,w)=_{} U_{}(,w)\] (1) subject to \[I_{}(,w) I_{}^{*}(w),\]

i.e., we maximize the minimum normalized user utility, subject to the minimum normalized item utility being at least a fraction \(\) of the optimal item fair solution.

Research questions: price of fairness and misestimation.We can now define the price of item fairness on user fairness \(_{U|I}^{F}\) ("price of fairness") as the decrease in user fairness with maximal item fairness constraints:

\[_{U|I}^{F}(w):=^{*}(w)-U_{}^{*}(=1,w)}{U_{}^{*}(w)}.\]

We ask how \(_{U|I}^{F}\) changes with the utility matrix \(w\). We note that while this question (in terms of solutions to Problem (1)) can be simply stated, as we detail in Section 3, finding a closed form expression for \(U_{}^{*}(1,w)\) - the optimal minimum normalized user utility given item fairness constraints - in terms of \(w\) is theoretically challenging.

Similarly, we investigate the price of misestimation. Let \(_{ij}\) denote the platform's _estimate_ of the utility of recommending \(i\) to \(j\); let \(()\) be a policy that solves the optimization problem above withmisestimated utilities, that is, \(()\) attains \(U_{}^{*}(,)\).1 Then we define the price of misestimation on user utility ("price of misestimation") \(_{U}^{M}\) as

\[_{U}^{M}(,w,):=^{*}(,w )-U_{}((),w)}{U_{}^{*}(,w )},\]

that is, the decrease in true user fairness due to optimizing with estimated utilities. In particular, in Section 5 we will examine whether item fairness exacerbates the price of misestimation by comparing \(_{U}^{M}(0,w,)\) and \(_{U}^{M}(1,w,)\), particularly in the case of cold start users.

Discussion.We note several modeling choices. First, we assume that utility \(w_{ij}\) is shared - both the item \(j\) and user \(i\) benefit equally from a successful recommendation. This choice captures, e.g., purchase or click-through rates. It also helps us isolate effects due to fairness constraints and effects across items and users, as opposed to misaligned utilities. We expect the price of fairness to increase - and potentially be arbitrarily high - with such misaligned utilities. We further justify and relax the shared utility assumption in Appendix A. Second, we quantify fairness through the minimum normalized utility. Normalization is standard in related algorithmic work, such as , and avoids solutions in which an item that provides utility \(\) to every user needs to be recommended to every user to equalize utilities. We use _individual egalitarian_ fairness (minimum utility over individuals) instead of group fairness to capture settings in which group identity is not available, and systems in which individual-level disparities may be widespread; egalitarian fairness is widespread in algorithmic fairness [2; 17; 43; 45]. In Appendix A we show that our empirical findings extend to other measures of individual fairness. Finally, in the user-item fairness tradeoff problem (Problem 1) we used an item fairness constraint rather than adding an item fairness term to the objective; these two approaches can be thought of conceptually as dual to one another, and so we expect similar properties to hold in either formulation.

## 3 Theoretical framework

To determine how the price of fairness \(_{U|I}^{F}\) depends on utility matrix \(w\), we need to compute \(U_{}^{*}(1,w)\) and \(U_{}^{*}(w)\). It turns out that \(U_{}^{*}(w)\) is easy to describe: without item fairness constraints, the optimal recommendation policy deterministically recommends each user their most preferred item. Each user attains the maximum possible normalized utility of 1, and \(U_{}^{*}(w)=1\).

However, with an item fairness constraint, we can no longer select each user's recommendation policies independently. Thus characterizing \(U_{}^{*}(1,w)\) is much more complicated. Recall that \(U_{}^{*}(1,w)\) is the minimum normalized user utility of the optimal user-fair recommendation subject to maximal item fairness constraints. Plugging into Problem (1) and expanding the definitions, we have

\[U_{}^{*}(1,w)=_{_{n-1}^{ n}} _{i}w_{ij}_{ij}}{_{j}w_{ij}}\] (2) subject to \[_{_{n-1}^{*}}_{j}w _{ij}_{ij}}{_{i}w_{ij}}.\]

Thus we need to find closed-form solutions in terms of the utilities \(w\) to a non-linear concave program, in which both the objective and the constraint depend on \(w\), and indeed even determining the constraint requires solving another non-linear concave program.

In Proposition 1, we develop a framework to solve this problem, which we later apply to show our main results Theorems 3 and 4.

**Proposition 1**.: _Suppose that for a set of recommendation policies \(_{n-1}^{m}\),_

1. \(\) _can be described by a finite set of linear constraints_
2. _There exists an optimal solution_ \(^{*}\) _to Problem (_2_) such that_ \(^{*}\)__
3. \(^{*}\) _is the unique feasible solution to Problem (_2_) in_ \(\)___Then, finding an optimal solution \(^{*}\) to Problem (2) can be reduced to solving a linear program \(\)._

With Proposition 1, the key technical challenges become: (a) given utility matrix \(w\), finding \(\) satisfying the above conditions; (b) given \(\) and \(w\), finding a closed-form expression for \(^{*}\); (c) given a closed form for \(^{*}\) in terms of \(w\), reasoning about the properties of item fair solution \(U^{*}_{}(1,w)\).

To do this, for our main results, we construct \(\) as a set of recommendations with a particular _symmetric_ structure. Suppose users come in \(K\) types, where a user of type \(k\) shares a utility vector. Then, consider \(_{}=\{:_{i}=_{i}w_{i}=w_{i^{}}\} ^{m}_{n-1}\), the set of policies where users of the same type are given the same recommendation probabilities. Note that, in the extreme case, each user is their own type. Furthermore, let \(_{k}\) denote the recommendation policy for type \(k\), for \(_{}\).

**Proposition 2**.: \(_{}\) _satisfies conditions (i) and (ii) in Proposition 1. Furthermore, solutions \(_{}\) to Problem (2) have a sparse structure:_

* _If_ \(\) _is a basic feasible solution to the linear program_ \(\) _in Proposition_ 1_, then there are at most_ \(n+K-1\) _type-item pairs_ \((k,j)\) _such that_ \(_{kj}>0\) _(out of_ \(nK\) _possible pairs)._
* _If_ \(\) _is also optimal, then there are at most_ \(K-1\) _items that are ever recommended to more than one type of user, i.e., where_ \(_{kj},_{k^{}j}>0\) _for_ \(k k^{}\)_._

For our main results, we will leverage the sparsity structure in Proposition 2 to show, with further restrictions on utility matrix \(w\), that \(_{}\) also satisfies \((iii)\). We will then derive closed-form expressions for that solution, and show how it changes as a function of \(w\).

The sparsity structure in Proposition 2 is also interesting in its own right. Without item fairness constraints, solutions will be highly sparse: each user will be recommended their most preferred item deterministically; each other item will have a recommendation probability of zero. Once we add item fairness constraints, solutions do not necessarily remain sparse. However, Proposition 2 shows that sparse solutions still arise under item fairness constraints in settings with symmetric solutions.

## 4 User preference diversity and fairness tradeoffs

We now use the theoretical framework developed in the above section to understand the effect of the structure of user utilities on the price of fairness. In particular, we identify "free fairness," i.e., the price of fairness is low, when preferences are sufficiently diverse.

**Example 1**.: For intuition as to why user diversity affects the price of fairness, consider the following example. Suppose we have \(n=2\) items and \(m\) users; half the users have utility \(\) for the first item and \(1-\) for the second item, and the other half has utility \(1-\) for the first item and \(\) for the second. Then, the recommender can simply give each user their favorite item (the user optimal solution), and this solution simultaneously maximizes user and item fairness as well as total user and item utility.

On the other hand, suppose all users have the same preferences: each user has utility \(1-\) for the first item and utility \(\) for the second item, for \(>0\) that is small. Then, _any_ recommendation probability given to the second item comes at a cost to users who receive that item instead of the first item; however, (normalized) item fairness would require that the second item receives \(\) as much utility as the first item. Below, we show that this results in a tradeoff even between linear _normalized_ user and item utilities, where we account for the fact that the second item is on average less preferred by users.

Since all users have the same preferences for items, using Proposition 2 it is sufficient to consider them receiving the same recommendation probabilities \(_{1}\) for the first item and \(_{2}=1-_{1}\) for the second. Bounding minimum item utility \(I_{}\) by the utility of the second item, we have

\[I_{}\, I_{2}()=_{2}}{m}=_{2}  1-_{1}.\]

For a given recommendation probability \(_{1}\), the minimum normalized user utility is

\[U_{}\,=(1-)_{1}+_{2}=+(1-2)_{1 }_{1}+.\]

Rearranging, we get that

\[U_{}\,-_{1} 1-I_{} U_{}\,+I_{} \, 1+.\]

Thus the minimum normalized user utility and the minimum normalized item utility essentially follow a negative _linear_ relationship - guaranteeing the second item even \(\) as much utility as the first item results in a linear cost to users. \(\)We now formalize and generalize this example, when the level of heterogeneity in the population can be captured by a single parameter. Consider the following utility matrix structure. Let \(v_{1}>v_{2}>...>v_{n}>0\). Suppose that there are two user types: a user \(i\) either has utility \(w_{ij}=v_{j}\) or \(w_{ij}=v_{n-j+1}\), and say that user \(i\) is of type 1 or 2 respectively. In words, the two types have opposite preferences, but the preferences can otherwise be generic and be for any number of items. Now, the direct solution by symmetry for the example no longer directly holds - the items in the middle, not necessarily preferred by either user type, may be binding in terms of item fairness constraints.

Let \(\) be the proportion of type 1 users in the population, out of a fixed population of \(m\) users. Parameter \(\) thus controls the population heterogeneity; if \(\) is near 0 or 1, the population is highly homogeneous, dominated by users of the same type. If \(=\), the population is split evenly between the two types and is highly heterogeneous. Since we parametrize \(w\) by \(\), we may write the price of fairness as,

\[_{U|I}^{F}():=^{*}()-U_{}^{*} (=1,)}{U_{}^{*}()}.\]

Given this structure, Theorem 3 states that the price of fairness \(_{U|I}^{F}()\) increases in the homogeneity of the users - heterogeneous user populations are less affected by incorporating item fairness constraints.

**Theorem 3**.: \(_{U|I}^{F}()\) _is decreasing in \(\) for \(0< 1/2\), and increasing in \(\) for \(1/2<1\)._

Proof sketch for Theorem 3.: We show that when in a population with two opposing types as described above, the sparsity condition in Proposition 2 yields a unique solution, for which we can find a closed form and express in terms of \(\). We then evaluate \(U_{}(,)\) at this solution to find \(U_{}^{*}(1,)\), and show that this is indeed increasing. The full proof is in Appendix D. 

## 5 Uncertainty and fairness tradeoffs

A basic fact - often ignored in fair recommendation - is that recommendations are made with (mis)estimated utilities. Platforms do not have full knowledge of user preferences, especially those new to the platform. Of course, recommendations under misestimated utilities may be poor; here, we show that adding item fairness constraints may _worsen_ the cost of this misestimation even further.

Intuitively - for a new user for whom the platform has no data - the platform would estimate the user's preferences as the average of preferences of existing users (e.g., in a Bayesian fashion). Thus, without item fairness considerations, it would show the user generally popular items. However, the new user's preferences are generally estimated as "weaker" than the preferences of others for any given item (since it averages preferences of users who may either like or dislike any given item). Thus, with fairness constraints, the optimization is incentivized to show the user otherwise unpopular items, since all the user's estimated preferences are weaker. Liu and Burke  for example develop an algorithm for item fairness where users with weaker preferences are explicitly leveraged in this way.

For a given item fairness level \(\), true utility matrix \(w\), and estimated utility matrix \(\), let \(()\) be a recommendation policy that solves the recommendation problem (Problem 1) with respect to the misestimated utilities, that is, \(\) solves \(U_{}^{*}(,)\). Recall that we define the price of misestimation

\[_{U}^{M}(,w,)=^{*}(,w)-U_{ }((),w)}{U_{}^{*}(,w)},\]

which represents the relative decrease in minimum normalized user utility as a result of misestimation. Item fairness _worsens_ the price of misestimation if \(_{U}^{M}(=1,w,)>_{U}^{M}(=0,w,)\).

We now formalize the above argument, building on the analysis in the previous section. As in Section 4, suppose that there are 2 types of users, with opposing preferences (i.e., with values \(v_{1},v_{2}...,v_{n}\) and \(v_{n},v_{n-1},...,v_{1}\), respectively) - and the platform has correctly estimated these preferences. However, now, these two types only make up a proportion \(\) of the population each.

Now, we suppose that there is a fraction \(1-\) of the user population who are "new" users. We assume that these users are drawn from the same distribution as the remaining users, but the platform does not know their preferences. It thus constructs a prior by averaging over the known users' preferences - (mis)estimating the users' utility for each item \(j\) as \(+v_{n-j+1}}{2}\).

**Theorem 4**.: _If \(>\) and \(w\) and \(\) are as described above, then fairness constraints can arbitrarily worsen the price of misestimation._

* _The price of misestimation without fairness constraints is low: for all_ \(\{v_{j}\}\)_, there is a recommendation policy_ \(\) _that solves the misestimated problem_ \(U^{*}_{}(0,)\) _so that_ \[^{M}_{U}(0,w,).\]
* _The price of misestimation with fairness constraints can be arbitrarily large:_ \(\)_, there exists_ \(\{v_{j}\}\) _and a recommendation policy_ \(\) _that solves the problem_ \(U^{*}_{}(1,)\) _such that_ \[^{M}_{U}(1,w,)>1-.\]

Proof sketch.: The main task is to find the price of misestimation with fairness constraints, which requires computing \(U^{*}_{}(1,w)\) when users' values are correctly estimated, and computing \(U^{*}_{}(1,)\) when users' values are incorrectly estimated. To find \(U^{*}_{}(1,w)\), note that \(w\) is a population with two opposing types of users, so we may leverage the insights of Theorem 3. To find \(U^{*}_{}(1,)\) we again use the framework in Section 3, showing that in the setting of this theorem, we can find an optimal policy \(^{*}\) for \(U^{*}_{}(1,)\) in a set \(^{}_{}\) of policies with an additional _column_-symmetry property. We use an analogue of the sparsity result in Proposition 2 to show that there is a unique feasible solution \(^{}\) and obtain a closed form expression for \(^{*}\), and find an upper bound for \(U_{}(,)=U^{*}_{}(1,)\). In fact, we show that as long as \(>\), under \(\) the mis-estimated users will _never_ be recommended their most preferred items. The full proof is in Appendix E. 

The idea follows the above intuition: without item fairness constraints and assuming cold start users follow the same distribution as existing users, the platform's price of misestimation is low because the platform treats cold start users as the average of the existing population. Fairness constraints, however, can make this cost arbitrarily high, as _in expectation_ cold start users _relatively_ enjoy items other users do not. Such effects suggest that a more careful treatment of uncertainty and fairness together is necessary for recommendation algorithms.

## 6 Empirical findings: arXiv recommendation engine

We prototype a recommender for preprints on arXiv, to illustrate our conceptual findings. We consider the cold start setting for items (papers), when they are newly uploaded to arXiv and so only have metadata and paper text but no associated interaction or citation data. For users (readers), we use as data the papers that _they_ have shared on arXiv to estimate their preferences.

Empirical setup.We use data from arXiv and Semantic Scholar . As training for user preferences, we consider 139,308 CS papers by 178,260 distinct authors before 2020; as the items to be recommended, we consider the 14,307 papers uploaded to arXiv in 2020. We apply two natural language processing-based models - TF-IDF  and the sentence transformer model SPECTER  - to textual features such as the paper's abstract (for both items and the user's historical papers) to generate embeddings for all papers in the training set. We use these embeddings to compute similarity scores (utility matrices) for users and items. To compute the similarity score (utility) between a user (an author of at least one paper before 2020) and an item (a paper uploaded in 2020), we compute the cosine similarity between the embedding of each of the user's pre-2020 papers and the item's embedding. We then use either the _mean_ or the _max_ similarity amongst the pre-2020 papers and the item; the _max_ similarity score may more effectively capture a user's diverse interests . We then generate recommendations for each user, at various levels of user and item fairness constraints.

To validate our recommendation approach, we use citation data from Semantic Scholar  to determine for each user and each paper published in 2020 whether the user cites that paper in their post-2020 work. We then examine how well the user-item similarity score generated by our recommendation engine predicts the presence of a citation. The recommendations effectively predict whether a user cites a paper with a high score after 2020. In Table 1 we show the results of a logistic regression between each similarity score and the presence of a citation, where the coefficient on the score is large and statistically significant for each model; the predictive power of our models isreasonable for a sparse, high variance event such as citations. We generally find that the _max_ score models are more predictive of future citations. Appendix B includes details and evaluations; our computational experiments in the main text use the max score, TF-IDF model.

### Empirical results.

We examine the tradeoffs between item and user fairness and the effect of misestimation on this tradeoff empirically. We use the similarity scores generated by the recommendation engine described above - in particular, the max score, TF-IDF model - as the utility values \(w_{ij}\). We consider a pool of 14,307 papers in the computer science category posted to arXiv in 2020, and a pool of 20,512 authors who posted papers in the computer science category to arXiv both in 2020 and prior to 2020 (we use the papers prior to 2020 to compute the similarity scores as described above). We then subsample recommendation settings from this pool; we give further details about the sampling process below. For a given value of \(\), to compute \(U_{}^{*}()\) we use the cvxpy implementation of the convex optimization algorithm SCS .

User-item tradeoffs as function of user diversity.Figure 2(a) shows the tradeoff between user fairness and item fairness in a random population and in a population of homogeneous users. To generate a tradeoff curve for the heterogeneous population, we sampled 200 random papers and 500 random authors to form \(w\), and computed \(U_{}^{*}(,w)\) for 50 values of \(\) between 0 and 1. To generate tradeoff curves from homogeneous user populations, we clustered all 20,512 users into 10 clusters using the \(k\)-means algorithm. For a single curve, to form \(w\) we sampled 200 random papers and 500 random authors from one random cluster. We run 10 experiments and plot the mean of \(U_{}^{*}(,w)\) at each value of \(\) across the 10 curves as well as two std. error bars for the mean.

**Model** & **Coefficient** & **Std. Err** & **z-value** & **Adjusted \(R^{2}\)** \\  Max score, TF-IDF & 12.4100 & 0.058 & 212.178 & 0.08915 \\ Mean score, TF-IDF & 20.2122 & 0.131 & 154.835 & 0.04616 \\ Max score, Sentence transformer & 18.4557 & 0.250 & 73.695 & 0.1347 \\ Mean score, Sentence transformer & 16.2482 & 0.246 & 66.148 & 0.09085 \\ 

Table 1: Logistic regression results for predicting whether user \(i\) cites paper \(j\) from the similarity score \(w_{ij}\) for each model.

Figure 1: Empirical (using our arXiv recommender) tradeoff between the minimum user (Y axis) and item (X axis) utility. Recall \(\) is the fraction of the best possible minimum normalized item utility \(I_{}^{*}\) guaranteed. (a) Illustrating Theorem 3 empirically â€“ homogeneous populations have a higher price of fairness. Empirically, however, the price of fairness is small except with strict item fairness constraints \( 1\). (b) For a set of users, holding other users fixed, the cost to the worst-off user of misestimating preferences, at varying \(\). Empirically, the cost of misestimation is already so high that it is not worsened with item fairness constraints, as in the worst case analysis of Theorem 4.

Figure (a)a demonstrates that in real data, for moderate item fairness guarantees (\(0 0.9\)), on average there is a fairly low cost to user fairness, but as we approach optimal item fairness (\( 1\)), the tradeoff becomes steep. Furthermore, Figure (a)a shows that item fairness tends to impose a higher cost to user fairness in more homogeneous populations, as in Theorem 3.

Price of misestimation.Figure (b)b shows how user fairness is affected by item fairness guarantees in the presence of misestimation. For sets of 200 random papers and 500 random authors, we select a set \(10\%\) of these users at random and treat them as if we did not have any data for them, estimating these users' utility for item \(j\) as the average utility for item \(j\) for the other \(90\%\) of the users (\(\) in the notation of Theorem 4). For 50 values of \(\) between 0 and 1, we compute \(U^{*}_{}(1,)\) for the misestimated utility matrix. We also compute, counterfactually, this quantity if the utility matrix had been correctly estimated, \(U^{*}_{}(1,w)\). We plot the true minimum normalized user utility under the recommendation policies that attain \(U^{*}_{}(1,w)\) and \(U^{*}_{}(1,)\). We run 10 experiments and plot the mean value of the minimum normalized user utility and two std. error bars.

In Figure (b)b, near \(=0\), the price of misestimation - the gap between the two curves - is higher than the cost of misestimation near \(=1\). This results because the item fairness constraint barely changes the (already low) utility of these users when their preferences are misestimated, but substantially changes their utility when their preferences are correctly estimated. While theoretically, item fairness constraints can arbitrarily increase the cost of misestimation, in practice, on average they do not affect this cost - the cost of misestimation without item fairness is already high.

## 7 Related work

There is a large literature on (item) fair recommendation and ranking [3; 35; 48; 50; 51] and, more recently, on multi-sided user-item fair recommendation [5; 10; 11; 12; 39; 47]. This literature is primarily _algorithmic_: for a given formulation of user and item utility (and other desiderata), how do we devise an efficient algorithm for multiple objectives or constraints? In contrast, our goal is primarily _conceptual_, to aid algorithm designers in choosing when to use such algorithms: for example, by explaining when we might expect the tradeoff to be especially sharp, and to understand the cost to cold start users in particular. For example, we theoretically analyze recommendations from a constrained optimization-based approach akin to that in Basu et al. , in terms of the implications of such an approach on recommendations.

Several papers observe related phenomena to the ones we study, especially empirically. Wang and Joachims  develop an optimization algorithm to be fair to users (in terms of group fairness to demographic groups) and items (similar to our normalized utility metric). Theoretically, they show that there is a tradeoff between user and item utility metrics, and further empirically show how fairness interacts with the diversity of items shown to each user. While their focus is also primarily algorithmic, they do show that there is a fundamental tension between item and user fairness. In concurrent work, Kleinberg and Meister  also theoretically demonstrate and characterize this tension. They focus on the cost to individual users caused by imposing maximal item fairness constraints - similar to how we define price of fairness - and determine the relationship between a cost level and the proportion of users in the worst-case recommendation setting who experience that cost. In constrast, we examine the cost to the single worst-off user as a function of properties of the recommendation setting such as user diversity and recommender mis-estimation. Rahmani et al.  demonstrate a similar tension between item and user fairness in empirical data. Liu and Burke  do not examine user fairness, but observe that one can mitigate the cost of item fairness in multi-sided recommendations by recommending to users with weaker preferences for items that may otherwise be less preferred. Subsequently, Farastu et al.  examine which users bear the cost of item fairness, pointing to Liu and Burke  to argue that the cost to users of item fairness constraints disproportionately falls on users with flexible preferences, creating an incentive for users to misrepresent their preferences as more rigid than reality. We build on these arguments by theoretically analyzing the cost of item fairness on (individual) users, especially the users most affected, as a function of the estimated user preference matrix.

In focusing on conceptual phenomena, our work is related to work analyzing the _price of fairness_ and efficiency-equity tradeoffs in various settings beyond recommendations. Bertsimas et al.  first defined the _price of fairness_ as the normalized decrease in the utility of an algorithmic outcome after adding fairness considerations, and develop general bounds on the price of fairness in an array of optimization scenarios. This concept has been subsequently applied in a variety of domains, including auction theory , fair division and resource allocation [6; 32; 46], and computer networking . Barre et al.  apply a similar concept in assortment optimization, examining the cost of item visibility constraints on the revenue of a platform. Most similarly, Chen et al.  define the price of fairness in the setting of multi-sided fairness as the cost on platform revenue of including both fairness constraints; in contrast we define the price of fairness as the cost to user fairness of imposing item fairness constraints in order to capture the interplay between user and item fairness. The authors then show that this price of fairness on the revenue depends on _objective misalignment_ - the difference in fairness between the item/user utility required by the constraints, and the item/user utility in a revenue-optimal solution. We study how the price of item fairness on user fairness depends on _user preference diversity_ - the agreement between users' utilities. These concepts are related: user preference diversity may cause objective alignment. Moreover, they also consider the problem of unknown preferences: they examine how to algorithmically impose fairness constraints when preferences are unknown, while we address the question of whether fairness constraints disproportionately harm users with unknown preferences. Finally, our result that diverse population preferences can mitigate the price of fairness resembles the result of Bastani et al.  that greedy contextual bandits can perform well without exploration if there is sufficient contextual diversity.

Finally, there is a large literature on other tradeoffs in recommendations, rankings, and ratings: engagement versus value, diversity, strategic behavior, uncertainty, and over-time dynamics [9; 14; 15; 18; 19; 20; 21; 22; 23; 27; 29; 30; 33; 34; 36]. In the context of set recommendations when users consume their favorite item out of the multiple recommended, e.g., Peng et al.  show that there is a minimal utility-diversity tradeoff, and Besbes et al.  show that there is a minimal exploration-exploitation tradeoff.

## 8 Discussion

We investigate the relationship between user fairness and item fairness in recommendation settings. We develop (a) a theoretical framework to enable us to solve for the price of fairness for many population settings, and (b) a recommendation engine using real data to allow us to investigate user-item fairness tradeoffs in practice. Our work informs the design of fair recommendation systems: (1) it emphasizes the benefits of a diverse user population, and suggests that item fairness constraints should not be imposed on _sub-markets_ (sub-groups), but instead on the entire population together. (2) It cautions designers to be especially mindful of effects on individual users (especially cold start users), who may receive disproportionately poor recommendations with item fairness constraints, even with a user fairness objective. Our empirical analysis supports our theoretical analysis--the userbase diversity affects the severity of user-side effects of imposing item fairness; however, the price of misestimating user utility is already high without item fairness constraints, and so imposing such constraints does not have additional effects. Such results speak to the importance of instance-specific analyses, cf. : one cannot make general statements about the specific effects of item-fairness constraints on users (or vice versa) outside of a specific context, though we identify two relevant phenomena (user diversity and misestimation) that modulate these effects.

Limitations.Our theoretical analysis explores these fairness tradeoffs in a fairly restricted setting. First, we assume that users are only recommended a single item; future work should investigate how the price of fairness changes as the number of recommended items increases. We do not expect our theoretical framework to easily extend to other definitions of fairness; however, we extend our computational arXiv experiments to other definitions of fairness in Appendix A. These extended experiments also show that diverse user preferences reduce fairness tradeoffs; an interesting direction for future work is to theoretically characterize user-item fairness tradeoffs under other definitions of fairness. Furthermore, in practice platforms are unwilling to maximize the worst-off user's item or user fairness at the expense of the entire platform's utility; the problem is really one of balancing user and item fairness with overall platform performance. Algorithms to optimize these multi-sided problems are explored in other work , but it would be interesting to develop qualitative observations about user-item fairness tradeoffs in the presence of a total utility constraint. Finally, we show our Theorems 3 and 4 in a limited context with only two or three types of users. However, the theoretical framework developed in Section 3 is significantly more general. It is would be interesting to apply this framework to other population structures such as when users do not have perfectly opposite preferences and where there are more than three groups of users.