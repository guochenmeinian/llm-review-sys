# Navigating the Effect of Parametrization

for Dimensionality Reduction

 Haiyang Huang Yingfan Wang Cynthia Rudin

Duke University

{hyhuang, yw416, cynthia}@cs.duke.edu

Equal contribution.

###### Abstract

Parametric dimensionality reduction methods have gained prominence for their ability to generalize to unseen datasets, an advantage that traditional approaches typically lack. Despite their growing popularity, there remains a prevalent misconception among practitioners about the equivalence in performance between parametric and non-parametric methods. Here, we show that these methods are not equivalent - parametric methods retain global structure but lose significant local details. To explain this, we provide evidence that parameterized approaches lack the ability to repulse negative pairs, and the choice of loss function also has an impact. Addressing these issues, we developed a new parametric method, ParamRepulsor, that incorporates Hard Negative Mining and a loss function that applies a strong repulsive force. This new method achieves state-of-the-art performance on local structure preservation for parametric methods without sacrificing the fidelity of global structural representation. Our code is available at https://github.com/hyhuang00/ParamRepulsor.

## 1 Introduction

Dimension reduction (DR) methods are incredibly useful for data analysis. They provide a bird's eye view of a dataset that shows clusters and their relationships. These algorithms have been used for examining and processing images , text documents [2; 3], and biological datasets [4; 5; 6; 7; 8]. The successes of modern DR methods can mostly be attributed to neighborhood embedding (NE), which is the basis for modern DR methods  including \(t\)-SNE, LargeVis, UMAP, and PaCMAP [10; 11; 12; 13]. These algorithms aim to optimize the low-dimensional layout of the data, such that the high dimensional local structure (i.e., neighborhoods) are preserved.

A major weakness of existing NE algorithms is that they struggle with adaptability to large, incrementally updated datasets. These algorithms depend on a \(K\)-Nearest Neighbor graph, encompassing the entire dataset, to generate the embedding. Consequently, the introduction of new data necessitates a complete re-computation of the embedding, leading to significant time and computational resource demands for large datasets. Although recent adaptations have been developed to optimize only the additional data [14; 13], these modifications potentially alter the original algorithm's objective function, thereby compromising the embedding's quality.

Addressing these challenges, recent developments in combining neural networks with NE algorithms have shown promise. These algorithms maintain the same objectives as traditional NE methods but leverage neural networks to optimize the projection of high-dimensional data into lower-dimensional spaces [16; 17; 18]. The integration of neural networks allows these NE algorithms to be effectively trained on large datasets and generalize to unseen data. Throughout this paper, we refer to this class of algorithms as parametric algorithms. However, as shown in Fig. 1, despite the similarity in lossfunctions between the non-parametric and parametric versions, their outcomes are often completely different, and such difference has been largely overlooked by machine learning practitioners. This paper aims to illuminate and explain these differences, highlighting that parametrization often leads to worse local structure and visualization. Our investigation reveals that parameterized approaches lack the ability to identify cluster boundaries and separate negatives compared to nonparametric approaches. We further show that DR algorithms using Negative Sampling (NEG)-style loss functions exhibit greater adaptability to parametrization than others using Noise Contrastive Estimation (NCE) or InfoNCE loss. This observation is noteworthy as such discrepancies are not observed in nonparametric approaches.

Building on these insights, we propose a novel parametric DR method that effectively mines hard negatives without relying on labels. Our approach incorporates additional repulsive forces, placing even greater emphasis on pairs we identify as hard negatives. This enhancement ensures better separation and structure preservation, significantly improving the performance of parametric DR. We select a loss function tailored for optimizing the parametric case, addressing local structure preservation. Our new DR algorithm, ParamRepulsor, approaches the performance of leading non-parametric methods while surpassing existing parametric approaches in preserving both local and global structure. It offers a functional mapping from high- to low-dimensional space, ensuring superior scalability, adaptability, and generalization to unseen data.

To summarize, our contributions in this study are:

* We conduct a comprehensive analysis of the impact of parametrization on the performance of DR methods, demonstrating that it may compromise local structure. Our findings attribute this issue to insufficient repulsive forces on negative pairs in the parametric setting. Notably, algorithms employing NEG-style loss functions (e.g., UMAP, PaCMAP) exhibit greater adaptability to parametrization than those using NCE-style loss functions (e.g., InfoNc-t-SNE, NCVis).
* Inspired by contrastive learning, we propose ParamRepulsor, a new method that uses hard negative sampling to improve the handling of negative pairs, combined with a contrastive loss tailored for the parametric setting. ParamRepulsor is a novel, fast algorithm that achieves excellent local structure preservation while maintaining global structure.

## 2 Fundamentals of Neighborhood Embedding Algorithms and Contrastive Learning

We provide essential background on Neighborhood Embedding (NE) methods and notation. We notate the high dimensional data as \(X=_{1}_{n}^{D}\), where \(n\) is the number of data points, and \(D\) is the dimension. NE algorithms aim to preserve predefined high-dimensional similarities within a low-dimensional embedding to reveal the local and global structure of \(X\). Specifically,

Figure 1: Dimensionality reduction results on the MNIST digit dataset . Parametric methods (bottom row) fail to preserve the local structure of the dataset compared to their non-parametric counterparts (top row). Our method, ParamRepulsor, effectively resolves this problem via Hard Negative Mining.

NE methods identify a mapping function \(f_{}\) that constructs the corresponding low dimensional embedding \(Y=_{1}_{n}^{d}\), where \(_{i}=f_{}(_{i})\). We will use \(_{i}\) and \(f_{}(_{i})\) interchangably. For nonparametric DR methods, the function \(f_{}\) is not defined outside of \(_{1},,_{n}\), though it is possible to interpolate. For visualization purposes, \(d\) is usually set to 2 or 3. Since the introduction of t-SNE , these algorithms have become widely used due to their ability to identify clusters and manifolds within high-dimensional data. They typically have two stages:

Similarity Construction Phase.For all pairs of points \((i,j)\), their high-dimensional similarity, \(s_{ij}\), is captured by a similarity function \((_{i},_{j})\) related to their distance. Due to the curse of dimensionality, the Euclidean distance metric fails to accurately represent distances along the data manifold in high-dimensional spaces . A common solution to this issue is to only consider similarities between \(K\) nearest neighbors: \(s_{ij}\) is set to be non-zero iff \(_{i}\) or \(_{j}\) are within the \(K\) nearest neighbors of each other, where \(K\) is a hyperparameter, usually 15-30.

Embedding Optimization Phase.After constructing the graph, NE algorithms try to optimize a function \(f_{}\). The objective is encoded by a loss function \(()\):

\[()=_{ij NN}_{NN}(\|f_{}( _{i})-f_{}(_{j})\|_{2})+_{ik NN} _{FP}(\|f_{}(_{i})-f_{}(_{k})\|_{2 }),\] (1)

where \(_{NN}\) denotes the loss for \(i,j\) that are similar (among the \(K\)-nearest neighbors), and \(_{FP}\) denotes the loss for pairs that are not nearest neighbors in the high-dimensional space. Typically, \(_{NN}\) decreases when \(\|_{i}-_{j}\|_{2}\) decreases, and \(_{FP}\) decreases when \(\|_{i}-_{k}\|_{2}\) increases. Their gradients therefore act like forces that _attract_ or _repulse_ the \(NN\) or \(FP\) pairs, respectively.

Relationship to Contrastive Learning.The similarity of the decomposition to self-supervised contrastive learning has recently been noted by [20; 18]. Specifically, loss functions of major NE algorithms can be considered as cases of Noise Contrastive Estimation (NCE) , Info-Noise Contrastive Estimation (InfoNCE)  or Negative Sampling (NEG) .

Using the framework above, we dive into details of **t-SNE**, **NCVis**, **UMAP** and **PaCMAP**, which are four major recent NE algorithms.

### NCE/InfoNCE-based: t-SNE and NCVis

Both NCE- and InfoNCE-based approaches assume the high-dimensional data similarities (the \(s_{ij}\)'s) follow an underlying data similarity pattern, represented by an unknown distribution \(p\). These methods learn a function \(f_{}\) that generates a similar low-dimensional similarity pattern, described by a distribution \(q\), aiming to match \(p\). \(q\) decreases as the pairwise distances in the low-dimensional space increase, though their exact relationship can vary. Since \(q\) represents a probability distribution, it must be normalized to ensure all possibilities sum up to 1. The only difference is that NCE uses a logistic loss, whereas InfoNCE uses a cross-entropy loss for the data distribution match.

Approximating \(p\) as a Bernoulli distribution  with value 1 for NN pairs and 0 for FP pairs. Assuming that for each step in the optimization, we optimize a batch that contains one NN pair and \(m\) FP pairs, \(q\) should minimize:

\[^{NCE}=-_{ij NN,ik_{c=1 m} NN}( }{q_{ij}+_{c=1 m}q_{ik_{c}}}-m(1-}{ q_{ij}+_{c=1 m}q_{ik_{c}}}))\] (2)

\[^{InfoNCE}=-_{ij NN,ik_{1 m} NN}(  q_{ij}-(q_{ij}+_{c=1 m}q_{ik_{c}})).\] (3)

t-SNE, the most popular NE algorithm, utilizes a loss defined over the full data set. The raw t-SNE loss is usually written as the KL-divergence between high-dimensional and low-dimensional conditional probability distributions \(p\) and \(q\). Here, we separate the loss following [25; 13].  notes that the exact values of the \(p_{ij}\)'s have limited impact and can be treated as binary weights without impacting outcomes. To simplify the calculation and allow for mini-batch stochastic gradient descent,  rewrote this loss as an InfoNCE loss . Denoting \(d_{2}(i,j)=\|f_{}(_{i})-f_{}(_{j})\|_{2}^{2}+1 =\|_{i}-_{j}\|_{2}^{2}+1\), the t-SNE loss function can be rewritten as an InfoNCE loss:

\[^{t-SNE}()=-_{ij NN,ik_{1 m} NN} ((i,j)}-((i,j)}+_{c=1 m }(i,k_{c})})).\] (4)Following , we call the mini-batch variant Info-NC-t-SNE. We will use it from now on since the vanilla t-SNE loss requires computing pairwise distances between all points in a dataset, and it is challenging to incorporate that into the mini-batch parametric DR framework.

NCVis  uses an NCE  loss. We denote the number of negative pairs in a batch as \(m\), and set \(q_{ij}=(i,j)}\). The NCVis loss is:

\[^{NCVis}()=-_{ij NN,ik_{1 m} NN}( (i,j)}{d_{2}(i,k_{c})}}-m( 1-(i,j)}{d_{2}(i,k_{c})}})).\] (5)

 provides a modern implementation for both algorithms. It also provides parametric versions that adopt multilayer perceptrons (MLP) with [100; 100; 100] hidden neurons and ReLU activation.

### NEG-based: UMAP

Negative Sampling (NEG)  simplifies the modeling process. Define \(q_{}\) to be a similarity function in the low dimensional space:

\[^{NEG}()=-_{ij NN}(}{1+q_{ ij}})-m_{ij}(}).\] (6)

UMAP  is a DR algorithm that utilizes the NEG loss . Its loss function is

\[^{UMAP}()=-_{ij NN}(^ {UMAP}(i,j)}{1+q_{}^{UMAP}(i,j)})-m_{ij NN} (^{UMAP}(i,j)}).\] (7)

which is NEG with the similarity kernel \(q_{}^{UMAP}(i,j)=(i,j)-1}\).

### PaCMAP

PaCMAP  is another recent DR algorithm that achieves high-quality data visualization. Compared to other NE algorithms, PaCMAP's loss function is designed to follow several mathematical design principles, but does not have a probabilistic explanation. The loss function (omitting the mid-near pairs term, as it is only relevant during the initial epochs, see Appendix D) is defined as follows:

\[_{ij NN}=W_{NN}(i,j)}{d_{2}(i,j)+C_{1}},\;_{ik FP}=W_{FP}(i,j)+C_{2}}\]

in which the \(W\) weights change based on the epoch, and \(C_{1}\) and \(C_{2}\) are set to 10 and 1, respectively.

To study the effect of parametrization on DR algorithms, we extend the PaCMAP framework to incorporate an MLP to map the high-dimensional input to the low-dimensional embedding. We refer to the resulting parametric algorithm as _ParamPaCMAP_. Implementation details are in Appendix F..

## 3 Effect of Parametrization on DR Results

Machine learning practitioners have long believed that parametric NE algorithms behave similarly to their non-parametric counterparts [17; 18]. In this section, we investigate the performance of the aforementioned parametric and non-parametric versions of these algorithms. To understand the effect of parametrization more thoroughly, for each parametric DR method, we additionally implemented three new versions, using a neural network with 0 hidden layers (i.e., a linear model), 1 hidden layer, or 2 hidden layers as a projector. We fix the number of neurons for each layer to 100.

**Observation 1: Parametric NE algorithms typically lead to worse visual effects as well as worse local structure preservation.** Our results indicate that parametric NE algorithms often fail to produce embeddings of the same quality as their non-parametric counterparts, even on simple datasets such as MNIST . The non-parametric methods in the rightmost column of Fig. 2 are able to separate the clusters fairly well, but from the first four columns of Fig. 2, we see that all four parametric algorithms generate clusters that are densely packed with indistinct boundaries, despite the fact that clusters in MNIST are actually separated. These blurred boundaries result in poorer preservation of local structure, with the possible exception of Parametric PaCMAP.

The challenge of accurately preserving local structure is exacerbated in scenarios where ground truth labels are unknown, especially in large-scale biological and chemical data, where dimensionality reduction is widely used for data exploration. In these scenarios, users may struggle to identify potential clusters within the large, indistinct conglomerates produced by the NE algorithms.

Figure 2 quantitatively evaluates the quality of the embedding via the SVM accuracy, which measures local structure preservation (described in Section 5.1.). Fig. 3 further quantifies the observation. Here, we sample three kinds of pairs from the points with labels "3" and "8" from the embedding, and calculate the pairwise distance for each kind of pair. NN denotes the pairs of points that are \(10\)-nearest neighbors, and FP denotes pairs of points that are uniformly sampled from the population. MN denotes "mid-near" pairs that are further than NNs but still relatively close (detailed in Sec. 4.) For each embedding, we scale the distance with respect to the scale of the embedding, and calculate the ratio of the mean FP distance to NN distance, as well as the mean MN distance to NN distance. Our analysis reveals that, in comparison to the non-parametric methods, all the parametric counterparts (App. H.1) have a smaller FP distance ratio, meaning further pairs are positioned closer together, which explains the blurred boundaries between clusters.

**Observation 2: NE algorithms with NEG loss perform better when parameterized.** A widely accepted explanation for the failure of small neural network projectors, such as those used here, is that they lack the capacity to capture the complexity of the data. While adding more layers to the projector is believed to effectively mitigate the loss in local structure preservation, our experimental results in App. A shows that adding additional layers beyond three yields diminishing returns. As we discuss in App. B, adjusting hyperparameters for parametric DR algorithms--such as the number of nearest neighbors used in NN-graph construction--also had minimal impact on the resulting embeddings. In all cases, the visual quality of the embedding remained suboptimal compared to nonparametric DR methods.

Figure 3: The low-dimensional scaled distance distribution between various types of point pairs with labels “3” and “8” in the embedding of the MNIST digit dataset , generated by PaCMAP, ParamPaCMAP, and ParamRepulsor (other methods in App. H.1). See definitions in Sec. 2 & 4.

Figure 2: Embeddings of the MNIST  dataset generated by various DR methods with different numbers of hidden layers: 0 (Linear), 1, 2, or 3, or non-parametric variant. See Section 5.1 for details of SVM Acc. It is helpful to envision these images in black and white (without labels) to see when clusters would be difficult to visually separate. More datasets/methods can be found in App. C.

While all four algorithms achieve comparable performance on the MNIST dataset in the non-parametric setting, their ability to adapt to the parametric setting varies significantly. Specifically, NE methods that optimize the NEG loss (UMAP and PaCMAP) perform substantially better than those that optimize the InfoNCE/NCE loss (Info-NC-\(t\)-SNE and NCVis). As illustrated in the first four columns of Figure 2, Info-NC-\(t\)-SNE and NCVis continue to struggle with local structure preservation in the embeddings when the number of hidden layers is one or two, whereas UMAP and PaCMAP are already capable of grouping similar samples together effectively. Why is this the case? We hypothesize that it is because UMAP and PaCMAP use NEG losses rather than InfoNCE/NCE losses.

We found that PaCMAP's loss is a generalized NEG loss, with a separate similarity function \(q_{}\) defined for \(ij NN\) and \(ij FP\). We now state this formally.

**Theorem 3.1**.: _The loss of PaCMAP is generalized NEG with low-dimensional similarity functions \(q_{}^{NN}\) and \(q_{}^{FP}\):_

\[^{PaCMAP}()=-_{ij NN}(^{NN}(_{i},_{j})}{1+q_{}^{NN}(_{i}, _{j})})-m_{ij NN}(^{FP}(_{i},_{j})}),\] (8)

_in which the functions \(q_{}^{NN}\) and \(q_{}^{FP}\) are_

\[q_{}^{NN}(_{i},_{j})=}{d_{2 }(i,j)+C_{1}})}{1-(}{d_{2}(i,j)+C_{1}})},\;q_{}^{FP}( _{i},_{j})=(i,j)}{d_{2}(i,j)+C_{2 }})}{((i,j)}{d_{2}(i,j)+C_{2}})}.\] (9)

Proof: see Appendix D.

To better understand the difference in performance between the NCE, InfoNCE and NEG losses, we compare their terms (Eq 2, 3, 6). The term that attracts the nearest neighbors in these algorithms is consistently in the form of \( q_{ij}\). This similarity is also evident in the fact that UMAP and t-SNE share the same loss function for nearest neighbors. The key distinction _lies in the treatment of negative pairs_ or points that are not nearest neighbors. For NEG-based UMAP, the FP loss for each negative pair \((i,j)\) is \(-(1-(i,j)})\), and for PaCMAP it is \((i,j)+C_{2}}\). This term _solely_ depends on a negative pair, ensuring that the gradient is large when the negative pair becomes close.

This is in contrast to both NCE and InfoNCE losses, where _each_ negative pair term is based on _all_ pairs. Theoretical studies [26; 27] in standard contrastive learning have found that such design of loss functions may lead to a reduced gradient and worse performance under a multi-layer perceptron (MLP) model. On the other hand, the NEG loss effectively penalizes the proximity of negative pairs, enhancing the separation between dissimilar points.

## 4 ParamRepulsor

While our ParamPaCMAP algorithm preserves better local structure, the embedding space remains suboptimal, with some clusters that should be distinct still merged together. To solve the problem from its root cause, we propose ParamRepulsor, a novel parametric algorithm built upon our ParamPaCMAP. Pseudocode for ParamRepulsor is found in Alg. 1 and detailed in Alg. 2 in App. F.

There are several major differences of ParamRepulsor from other methods, the major one being the use of **Hard Negative Mining** in the repulsive terms. Our goal is to learn from Hard Negative (HN) Samples - pairs whose DR projections are close but should be far apart. Efficiently sampling HNs could be challenging. Existing approaches either rely on ground truth labels that are not applicable in the unsupervised DR setting [28; 29], or rely on the InfoNCE loss  which is less useful for NEG losses. We select _mid-near (MN) pairs_ for HN sampling (for the opposite purpose they are used in PaCMAP, where they exert attractive forces). A MN point for point \(i\) is identified through the following process: 1) sample \(h\{1,n\}\) points from the high-dimensional data, and 2) select the second closest point from the sampled set. Here, we use \(h=6\). We justify the use of MN pairs as HN samples based on two key observations.

**Observation 3: Using MN for HN sampling reduces the probability for false negatives.** Existing DR algorithms sample the negative pairs from an (approximately) uniform distribution over all possible pairs. While this approach enhances computational efficiency, it often results in false negatives, which is known to be problematic for contrastive learning [31; 30]. We show that MN pairs are ideal candidates for negatives as they rarely become false negatives.

**Theorem 4.1**.: _The probability that a sampled MN point is a false negative in a dataset of size \(n\) converges to 0 at a rate of \(O(})\)._

**Corollary 4.2**.: _MN points are less likely to be false negatives than uniformly sampled points in datasets with \(n 10^{3}\). See Appendix E for empirical results and Fig. 21 in Appendix E for projection._

Proof: see Appendix E. Theorem 4.1 and Corollary 4.2 state that the likelihood for an MN to be a false negative is low. Furthermore, the simplicity of MN sampling ensures efficiency: the sampling cost is still constant for each mid-near point.

**Observation 4: MN pairs are challenging negatives that provide better gradients for local structure preservation.** The shallow parametrization used in NE DR methods ensures that distances in the high-dimensional space remain correlated with those in the low-dimensional embedding. As shown in Fig. 3, in the blurred boundaries of clusters "3" and "8," MN pairs tend to be closer than normal FP pairs in the embeddings of all methods (see Fig. 22 in App. H.1 for other methods). This proximity makes MN pairs challenging negatives for the algorithm, resulting in large gradients during the loss calculation. Fig. 4 illustrates the representations learned by repulsing MN hard negatives. Our approach improves the boundaries between clusters, while maintaining the proximity between close clusters. It not only achieves state-of-the-art cluster separation in parametric methods but also outperforms several non-parametric methods.

Besides adopting Hard Negative Mining, we made other technical improvements to further enhance repulsive forces. More details can be found in Appendix F.

Figure 4: Effect of Hard Negative Mining on MNIST. We progressively increase the coefficient of the _repulsive_ force applied to MN hard negatives. Close clusters are circled. Results indicate that Hard Negative Mining alone effectively preserves local structure while maintaining relative proximities.

Experiments

Here, we evaluate the performance of our ParamPaCMAP and ParamRespulsor algorithms empirically. To contextualize our findings, we juxtapose our results against those obtained from other contemporary parametric DR algorithms. Visualization for the embeddings generated by all algorithms can be found in App. C.

**Datasets.** We use a wide-ranging collection of datasets across various disciplines. For image analysis, we analyzed the MNIST  and Fashion-MNIST (F-MNIST)  datasets, along with COIL-20  and COIL-100 . In the domain of computational biology, our assessment leveraged single-cell RNA-sequencing (scRNA-seq) datasets from studies by , , , . Further diversifying our dataset selection, the 20 Newsgroups (20NG)  text dataset was included for textual data analysis. The preprocessing of scRNA-seq datasets adhered to the methodology outlined by . Additionally, simulated datasets featuring predefined known structures - such as Circle, Mammoth , , Gaussian Lineage, and Gaussian Hierarchical  - were integrated into our analysis. See Section G.1 for more details. This multifaceted dataset compilation enables a thorough examination of the DR algorithms' performance across a spectrum of datasets.

**Algorithms.** Besides the two algorithms we proposed in this work, ParamPaCMAP (P-PaCMAP) and ParamRepulsor (P-Rep), we also perform experiments on other recent Parametric NE algorithms: Parametric UMAP (P-UMAP) , Parametric Info-NC-t-SNE (P-ItsNE) , Parametric Neg-t-SNE (P-NiSNE), and Parametric NCVis (P-NCVis) [24; 18]. Besides NE algorithms, we also compare against Geometric Autoencoder (GeoAE) , an autoencoder-based DR algorithm. While we note that there are many other parametric DR algorithms, they either aim to serve as an intermediate representation for downstream tasks (i.e., not visualization) [45; 46], or focus only on image dataset only . We refer readers to Section 6 for more details. We compare these algorithms on local and global structure preservation. For each algorithm, we use the hyperparameter settings and the network structure suggested in their implementation. Coincidentally, all the parametric algorithms in our experiment (except for GeoAE) are equipped with a 3-layer 100-neuron fully-connected neural network as their parametric projector \(f_{}\).

**Other setup.** For each experiment, we ran each DR algorithm 10 times using different random seeds to obtain 10 embeddings. We report the average metric measured across these 10 embeddings, highlighting the highest value in **bold**. An independent t-test with a significance level of \(p=0.05\) was conducted to assess significant differences between methods. Metrics not significantly different from the highest value are in _italics_.

### Local Structure Evaluation

We first look into the local structure of the embedding, which examines DR algorithms' ability to discover the cluster structure. Following previous works [47; 48; 13; 43], we evaluate local structure using three approaches, with results below. All visualizations can be found in App. C. We achieve state-of-the-art performance in local structure preservation.

**Local Structure 1: \(k\)-NN Accuracy.** Here, DR is performed and the labels are revealed afterwards. A \(k\)-NN model then classifies points in the DR projection, with its accuracy as the metric of interest. We perform leave-one-out cross validation, and utilize a \(k\)-NN classifier to predict the label of the point. For embedding data with good local structure, points that belong to the same class should be close to each other, which would yield a higher \(k\)-NN accuracy. In this study, we use \(k=10\). Table 1 presents the 10-NN accuracy of each DR algorithm. ParamRepulsor achieves the highest accuracy on 10 out of 14 datasets and comes close to the highest accuracy on the remaining datasets, demonstrating its strong performance in preserving local structure.

**Local Structure 2: SVM Accuracy.** Table 2 in App. H.3 illustrates the SVM accuracy, estimated using 5-fold cross-validation with an SVM classifier. ParamRepulsor achieves the highest accuracy on 9 out of the 14 datasets and achieves near-highest accuracy on the remaining datasets. These results demonstrate that ParamRepulsor attains state-of-the-art performance in preserving local structure.

**Local Structure 3: Nearest Neighbor Kept.** We further evaluate the ability of DR methods to maintain high-dimensional \(k\)-NN in the low-dimensional space. We use \(k=30\) to provide a more robust estimate of neighborhood preservation ability. Using a larger \(k=30\) value ensures that even if the first nearest neighbor in the high-dimensional space is placed as the tenth nearest neighbor in the embedding, it is still considered preserved. This approach mitigates the effects of the reduced dimensionality of the embedding, where small shifts can otherwise result in the loss of neighborhood relationships. Table 3 in App. H.3 demonstrates that ParamRepulsor achieves the highest accuracy on 10 out of 14 datasets and nearly the highest on 3 others, showcasing its strong performance in preserving local structure. Additionally, our implementation of ParamPaCMAP performs comparably to the best methods on all but 2 datasets.

### Global Structure Evaluation

We evaluate global structure by evaluating the preservation of cluster-level triplet relationships. Cluster-level triplet relationship preservation is important, particularly for computational biologists performing lineage analysis. Following , the metric for this is a Spearman (rank correlation). To compute it, we take one cluster centroid \(c\) and rank all other centroids based on low-dimensional distance to \(c\). We also repeat this for all \(C\) cluster centroids and place these rankings in a single vector. We repeat the process for the high-dimensional space and place these rankings in another vector. The Spearman correlation between these two vectors is the result, shown in Table 4 in App. H.3. Out of the 14 datasets, ParamPaCMAP achieves the highest correlation on 5 of them, whereas ParamRepulsor achieves the highest on 4. These results suggest our ideas are powerful for global structure preservation.

## 6 Related Work

The evolution of DR algorithms can be broadly categorized into two distinct phases. In the initial phase, the focus was on the development of methods that preserved only _global structure_. Key techniques in this category include Principal Components Analysis (PCA) , Multidimensional Scaling , and Non-negative Matrix Factorization . While these methods effectively maintain the global layout of the data, their primary limitation is that they often fail to retain the inherent neighborhoods and clusters of the data.

Subsequent DR methods were developed to address the shortcomings by emphasizing the preservation of _local structure_, specifically focusing on preserving \(k\) nearest neighbor relationships in the original dataset. These local methods, such as Isomap , Local Linear Embedding (LLE) , Laplacian Eigenmap , and more recent Neighborhood Embedding (NE) algorithms like t-SNE  and UMAP , are particularly adept at maintaining cluster structure. However, they may not adequately preserve the overall spatial layout of clusters. NE methods are more frequently used because they show clusters and manifolds in the high-dimensional space that are difficult to see any other way.

NE methods are typically non-parametric, creating a low-dimensional embedding that maps each data point to a location in 2D, but there does not exist a function that maps from the original (high-dimensional) space to the embedding space. To map new points to the low dimensional space, one

   method & P-UMAP & P-1tSNE & P-NtSNE & P-NCVis & GeoAE & **P-PacCMAP** & **P-Rep** \\  MNIST & _0.965_ & 0.830 & 0.862 & 0.829 & 0.791 & _0.968_ & **0.969** \\ F-MNIST & _0.733_ & 0.714 & 0.714 & 0.626 & _0.718_ & _0.744_ & **0.778** \\ USPS & _0.957_ & _0.939_ & _0.940_ & _0.938_ & 0.846 & _0.960_ & **0.957** \\ COIL-20 & _0.843_ & – & – & – & 0.724 & _0.853_ & **0.887** \\ COIL-100 & 0.145 & – & – & – & 0.611 & 0.896 & **0.928** \\ 
20NG & **0.505** & 0.340 & 0.401 & _0.442_ & 0.061 & 0.437 & _0.460_ \\  Kang & _0.954_ & _0.956_ & _0.956_ & _0.955_ & 0.468 & _0.960_ & **0.961** \\ Kazer & _0.939_ & _0.937_ & _0.937_ & _0.937_ & 0.700 & **0.940** & _0.939_ \\ Muraro & _0.960_ & _0.961_ & _0.961_ & _0.961_ & 0.565 & _0.961_ & **0.962** \\ Stuart & _0.851_ & _0.854_ & _0.853_ & _0.854_ & 0.394 & _0.855_ & **0.856** \\  Circle & _0.901_ & _0.900_ & _0.904_ & **0.911** & _0.898_ & _0.904_ & _0.895_ \\ Mammoth & _0.934_ & 0.916 & 0.914 & 0.915 & **0.962** & 0.915 & _0.938_ \\ Lineage & **1.000** & **1.000** & **1.000** & **1.000** & **1.000** & **1.000** & **1.000** \\ Hierarchy & **1.000** & **1.000** & **1.000** & **1.000** & _0.976_ & **1.000** & **1.000** \\   

Table 1: 10-NN Accuracy of DR methods measured on various datasets. The absence of values indicate the method failed to produce a valid embedding.

typically creates a nonparametric map from high to low dimensions that places new points near their high-dimensional neighbors (assuming one does not want to rerun the algorithm when adding new points). This approach creates crowding problems, where many high-dimensional points map to the same location in low dimensions.

To address the challenges posed by non-parametric NE algorithms, parametric NE algorithms have emerged as an effective solution. These algorithms focus on learning a function that maps data from a high-dimensional space into a low-dimensional embedding, typically using a neural network. Examples of this approach include the Multi-layer Perceptron based Parametric t-SNE , DEC , kernel t-SNE  and Parametric UMAP . Furthermore, recent advancements have integrated concepts from Contrastive Learning and Representation Learning, with significant contributions from TopoAE , GeoAE , t-SimCNE , and Parametric InfoNC-t-SNE .

Recently, ,  and  discussed the effect of the loss function forces in NE algorithms. Our work differs from them; in our work, we discuss the effect of parametrization, which is not discussed in previous works.

Learning from Hard Negatives has proven effective in supervised learning , metric learning , as well as contrastive learning . To the best of our knowledge, our work is the first that explores the effect of Hard Negative Mining in dimensionality reduction.

## 7 Discussion and Limitations

Parameterization of DR methods has major practical advantages. It allows for new data to be mapped directly from the high-dimensional space to the low-dimensional space by a function. We introduced a new method called ParamRepulsor, which demonstrates enhanced preservation of local structure without compromising global structure metrics, making it applicable across a broad spectrum of scientific inquiry.

We note that our method also exhibit limitations. Although ParamRepulsor outperforms Parametric UMAP in terms of speed, it requires more computational time than Parametric Info-NC-t-SNE. Other open questions that are not resolved by this work include the design of evaluation metrics that better reflect performance, and choosing the optimal architecture for both preservation and generalization.

## Code and data availability

Implementations of ParamRepulsor/ParamPaCMAP discussed in this paper, along with the code for the experiments, are available at https://github.com/hyhuang00/ParamRepulsor. The datasets used in our study are publicly accessible from their original publications.