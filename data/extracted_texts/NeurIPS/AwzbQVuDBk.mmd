# ProteinNPT: Improving Protein Property Prediction and Design with Non-Parametric Transformers

Pascal Notin\({}^{}\)

Computer Science,

University of Oxford

&Ruben Weitzman

Computer Science,

University of Oxford

Debora S. Marks

Harvard Medical School

Broad Institute

&Yarin Gal

Computer Science,

University of Oxford

Correspondence: pascal.notin@cs.ox.ac.uk, ruben.weitzman@cs.ox.ac.uk; \(\) Equal contribution

###### Abstract

Protein design holds immense potential for optimizing naturally occurring proteins, with broad applications in drug discovery, material design, and sustainability. However, computational methods for protein engineering are confronted with significant challenges, such as an expansive design space, sparse functional regions, and a scarcity of available labels. These issues are further exacerbated in practice by the fact most real-life design scenarios necessitate the simultaneous optimization of multiple properties. In this work, we introduce ProteinNPT, a non-parametric transformer variant tailored to protein sequences and particularly suited to label-scarce and multi-task learning settings. We first focus on the supervised fitness prediction setting and develop several cross-validation schemes which support robust performance assessment. We subsequently reimplement prior top-performing baselines, introduce several extensions of these baselines by integrating diverse branches of the protein engineering literature, and demonstrate that ProteinNPT consistently outperforms all of them across a diverse set of protein property prediction tasks. Finally, we demonstrate the value of our approach for iterative protein design across extensive in silico Bayesian optimization and conditional sampling experiments.

## 1 Introduction

Proteins, vital to most biological processes, have evolved over billions of years, shaping their structure through countless evolutionary experiments. The vastness of the protein sequence space offers immense opportunities for enhancing the properties of existing proteins, paving the way for advancements in healthcare, sustainability, and the development of novel materials (Arnold, 2018; Huang et al., 2016). Recent progress in sequencing technologies and machine learning methods bear the promise to explore this space more efficiently and design _functional_ proteins with the desired properties. A key step toward that goal is the ability to predict the _fitness_ of novel protein sequences - i.e., learning a mapping between structure and fitness of proteins, commonly referred to as a 'fitness landscape' (Romero et al., 2013). However, the protein space is sparsely functional and experimentally-collected labels about properties of interest are scarce relative to the size of protein space (Biswas et al., 2021). Additionally, real-life protein design scenario often seek to optimize multiple properties of the protein simultaneously, which further aggravates the label scarcity issues. Historically, learning fitness landscapes has been tackled by two different lines of research in the computational biology literature. Firstly, supervised methods are trained to approximate thefitness landscape by learning from experimental labels measuring a phenotype that is hypothesized to correlate with the function of interest. These methods have for instance been shown to lead to more efficient directed evolution pipelines (Dougherty and Arnold, 2009; Yang et al., 2019; Wittmann et al., 2020). Given the scarcity of labels, they are typically constrained in terms of parameter count and expressivity of the underlying model (e.g., ridge regression, shallow neural network) (Heinzinger et al., 2019; Dallago et al., 2021, 2022; Stark et al., 2021) to avoid overfitting. Secondly, unsupervised fitness predictors have reached increasingly higher fitness prediction performance without being subject to the limitations and potential biases stemming from training on sparse labels (Hopf et al., 2017; Riesselman et al., 2018; Laine et al., 2019; Frazer et al., 2021; Meier et al., 2022; Notin et al., 2022; Marquet et al., 2022; Montanari et al., 2022). Recently, Hsu et al. (2022) proposed to combine these two lines of research into a unified architecture in which a ridge regression is fed both one-hot encodings of input protein sequences and unsupervised fitness scores, and then trained on experimental labels to reach increased performance across several fitness prediction benchmarks. Yet, one-hot encoding features offer a relatively contrived and brittle sequence representation. For instance, they do not allow to make predictions at unseen positions resulting in low performance when attempting to extrapolate across positions (SS 4.3). Embeddings from protein language models offer a promising alternative but, since the resulting dimension of the full sequence embedding is typically too high, prior methods have resorted to dimensionality reduction techniques such as mean-pooling embeddings across the full sequence (Alley et al., 2019; Biswas et al., 2021). While this works reasonably well in practice, critical information may be lost in the pooling operation and, since not all residues may be relevant to a given task, we may want to be selective about which ones to consider. In this work, we introduce ProteinNPT (SS 3), a non-parametric transformer (Kossen et al., 2022) variant which is ideally suited to label-scarce settings through an additional regularizing denoising objective, straightforwardly extends to multi-task optimization settings and addresses all aforementioned issues. In order to quantify the ability of different models to extrapolate to unseen sequence positions, we devise several cross-validation schemes (SS 4.1) which we apply to all Deep Mutational Scanning (DMS) assays in the ProteinGym benchmarks (Notin et al., 2022). We then show that ProteinNPT achieves state-of-the-art performance across a wide range of protein property prediction and iterative redesign tasks. We summarize our contributions are as follows:

* We introduce ProteinNPT, a semi-supervised conditional pseudo-generative model for protein property prediction and design tasks (SS 3);
* We explore novel aspects of non-parametric transformers such as the use of auxiliary labels (SS 3.2), the multiple property prediction setting (SS 4.5), conditional sampling (SS 3.3) and uncertainty quantification (Appendix G.1);
* We devise several cross-validation schemes to assess the performance of fitness predictors in the supervised setting and their ability to extrapolate across positions (SS 4.1);
* We implement several of the current top-performing baselines, illustrate certain of their limitations, then improve these baselines accordingly by integrating ideas from diverse branches of the protein fitness prediction and engineering literature (SS 4.2);
* We subsequently show that ProteinNPT outperforms all of these baselines across a wide spectrum of property prediction tasks (SS 4.3-4.5);
* We demonstrate the potential of ProteinNPT for protein design through a diverse set of in silico Bayesian optimization (SS 5.1) and conditional sampling (SS 5.2) experiments.

## 2 Related work

Protein fitness predictionLearning a fitness landscape is typically cast as a discriminative supervised learning task (Yang et al., 2018; Gelman et al., 2020; Freschlin et al., 2022). Given the usually scarce number of labels relative to the dimensionality of the input space, the majority of approaches to date have relied on lightweight parametric models (Stark et al., 2021; Heinzinger et al., 2019) of the primary protein structure and, occasionally, non-parametric models based on the tertiary protein structure (Romero et al., 2013). Model features have revolved primarily around hand-crafted physico-chemical properties (Wittmann et al., 2020), one-hot-encodings of the amino acid sequence (Dallago et al., 2022) and, more recently, embeddings from deep generative models of evolutionary sequences (Alley et al., 2019; Brandes et al., 2021; Hesslow et al., 2022; Elnaggar et al., 2023). Deep generative models have also been successfully leveraged for zero-shot fitness prediction (Hopf et al., 2017a; Riesselman et al., 2018; Meier et al., 2022; Notin et al., 2022a). Their use is particularly compelling when experimental labels are not available or difficult to obtain for the property of interest. For instance, they enable accurate prediction of the effects of genetic mutations on human disease risks (Frazer et al., 2021; Brandes et al., 2022; Jagota et al., 2023), or can identify viral mutants with high escape potential at the beginning of a pandemic (Hie et al., 2020; Thadani et al., 2023).

Protein redesignThe goal of protein redesign is to iteratively mutate a starting protein, typically a naturally occurring sequence, in order to enhance one or several of its properties, such as its stability, substrate specificity, or catalytic efficiency. Directed evolution (Arnold, 2018) is a widely-used experimental method in this field, emulating the way natural evolution works, but within a controlled lab environment. In each cycle, the starting protein sequences undergo random mutations, and the desired traits are then measured experimentally. The most promising variants are selected as the basis for the next round, and the process is repeated through several cycles until the desired design goals are achieved. Machine learning (Yang et al., 2019; Wu et al., 2019) has been shown to significantly enhance this process by exploring the combinatorial space of mutations more efficiently, for instance by reaching the same final performance with fewer experimental rounds (Biswas et al., 2021b).

Self-attention over multi-dimensional objectsPopularized by the transformer architecture (Vaswani et al., 2017), self-attention enables to learn rich representations of complex structured objects, such as natural language sequences. One of its key limitations is the quadratic complexity of the attention mechanism with respect to the length of the input. Axial attention (Ho et al., 2019)

Figure 1: **ProteinNPT architecture. (Left) The model takes as input the primary structure of a batch of proteins of length \(L_{seq}\) along with the corresponding \(L_{t}\) labels and, optionally, \(L_{a}\) auxiliary labels (for simplicity we consider \(L_{t}\)=\(L_{a}\)=1 here). Each input is embedded separately, then all resulting embeddings are concatenated into a single tensor. Several ProteinNPT layers are subsequently applied to learn a representation of the entire batch, which is ultimately used to predict both masked tokens and targets (depicted by question marks). (Right) A ProteinNPT layer alternates between tied row and column attention to learn rich embeddings of the labeled batch.**

addresses that issue for multi-dimensional objects such as images by decomposing the traditional attention mechanism along each axis: separate self-attention mechanisms are successively applied on each axis, thereby drastically reducing computational complexity. The MSA Transformer (Rao et al., 2021) leverages axial attention to learn representations of a large and diverse set of Multiple Sequence Alignments (MSA) - two-dimensional objects comprised of homologous protein sequences aligned in the same coordinate system. As MSAs encapsulate rich evolutionary and structural information about the corresponding proteins, the MSA Transformer has lead to strong performance for several tasks, such as zero-shot contact prediction, fitness prediction (Meier et al., 2022) and protein engineering (Sgarbossa et al., 2023). Viewing tabular data as two-dimensional objects, Non-Parametric Transformers (Kossen et al., 2022) applies axial attention to learn a representation of an entire labeled training dataset in a supervised setting. Self-attention is thus used to model relationships both between features and labels (across columns), and between labeled objects (across rows).

## 3 Methods

### ProteinNPT model

Model architectureProteinNPT (Fig. 1) is a semi-supervised conditional pseudo-generative model that learns a joint representation of protein sequences and associated property labels. The model takes as input _both_ the primary structure representation of the proteins along with the corresponding labels for the property of interest. Let \((X^{},Y^{})\) be the full training dataset where \(X^{}\{1,20\}^{N.L_{s}}\) are protein sequences (with N the total number of labeled protein sequences and L the sequence length), and \(Y^{}^{N.T}\) the corresponding property labels (where \(T\) is the number of distinct such labels, including \(L_{t}\) true targets and \(L_{a}\) auxiliary labels, as discussed in SS 3.2). Depending on whether we are at training or inference time, we sample a batch of \(B\) points and mask different parts of this combined input as per the procedure described later in this section. We separately embed protein sequences and labels, concatenate the resulting sequence and label embeddings (each of dimension \(d\)) into a single tensor \(Z^{(B.(L_{s}+T).d)}\), which we then feed into several ProteinNPT layers. A ProteinNPT layer (Fig. 1 - right) learns joint representation of protein sequences and labels by applying successively self-attention between residues and labels for a given sequence (row-attention), self-attention across sequences in the input batch at a given position (column-attention), and a feedforward layer. Each of these transforms is preceded by a LayerNorm operator \(LN(.)\) and we add residual connections to the output of each step. For the multi-head row-attention sub-layer, we linearly project embeddings for each labeled sequence \(n\{1,B\}\) for each attention head \(i\{1,H\}\) via the linear embeddings \(Wr_{i}^{K}\), \(Wr_{i}^{Q}\) and \(Wr_{i}^{V}\) respectively. Mathematically, we have:

\[(Z)=Z+(LN(Z))=Z+(O_{1},O_{2},...,O_{H}).W^{O}\] (1)

where the concatenation is performed row-wise, \(W_{O}\) mixes outputs \(O_{i}\) from different heads, and we use tied row-attention as defined in Rao et al. (2021) as the attention maps ought to be similar across labeled instances from the same protein family:

\[((Q_{n},K_{n},V_{n})_{n\{1,B\}})=(_{n =1}^{B}.K_{n}^{T}}{}).V_{n}\] (2)

We then apply column-attention as follows:

\[(Z)=Z+(LN(Z))=Z+(P_{1},P_{2},...,P_{H}).W^{P}\] (3)

where the concatenation is performed column-wise, \(W_{P}\) mixes outputs \(P_{i}\) from different heads, and the standard self-attention operator \((Q,K,V)=(Q.K^{T}/).V\). Lastly, the feed-forward sub-layer applies a row-wise feed-forward network:

\[(Z)=Z+(LN(Z))\] (4)

In the final stage, the learned embeddings from the last layer are used to predict both the masked tokens and targets: the embeddings of masked targets are input into a L2-penalized linear projection to predict masked target values, and the embeddings of masked tokens are linearly projected then input into a softmax activation to predict the corresponding original tokens.

Model trainingOur overall architecture is trained with a two-stage semi-supervised procedure. First, due to the scarcity of labeled instances relative to the size of the underlying protein sequence space, we leverage embeddings from protein language models that have been pre-trained on large quantities of _unlabeled_ natural sequences. Throughout training, the parameters of the model used to obtain these protein embeddings are frozen to prevent overfitting. This also helps alleviating several practical challenges pertaining to GPU memory bottlenecks at train time (Appendix C.3). Second, similarly to what was proposed in Non-Parametric Transformers (Kossen et al., 2022), we train our architecture on a composite task combining input denoising and target prediction. In our case, the former is equivalent to a masked language modeling objective on amino acid tokens (Devlin et al., 2019). We randomly mask a fraction of amino acids and of input labels across all sequences in the mini batch (15% for both), and seek to predict these masked items. Our training loss is thus comprised of two components: 1) a reconstruction loss over masked amino acids 2) a target prediction loss over masked targets:

\[^{}=_{t}.^{}+(1-_{t}).^{}\] (5)

where \(^{}\) is target prediction loss (e.g., Mean Squared Error between predictions for the _masked_ labels and corresponding true labels for continuous targets), \(^{}\) the cross-entropy loss for the masked input tokens, and \(_{t}\) balances out the two objectives. As suggested by Kossen et al. (2022), \(_{t}\) is progressively annealed to increasingly focus on target prediction during training. The amino acid denoising objective entices the network to learn a representation of the full dataset, which in turn acts as a regularization mechanism which is beneficial in label scarce settings.

InferenceAt inference, we form input batches by concatenating row-wise a fraction of test sequences (with unknown, masked labels) with training instances (with known, unmasked labels). Predictions are therefore dependent on both the learned relationships between tokens and targets, as well as homology across labeled instances. When the size of the training set is large enough, we cannot use the entire training set at inference due to memory bottlenecks, and have to resort to sampling training instances. We explore the impact of the number of training sequences sampled at inference in SS 6.

Advantages of tri-axial attentionTo obtain the initial protein sequence embeddings, we experimented with several best-in-class protein language models and obtained better results with the MSA Transformer (Appendix C.2). As a result, our overall architecture operates _tri-axial_ self-attention to output predictions: across residues and labels, across homologous sequences from the retrieved MSA (in MSA Transformer layers only) and across labeled examples (in ProteinNPT layers only). There are several advantages conferred by this architecture. First, it seamlessly adapts to multiple target predictions, by concatenating as many label columns in its input as required. Second, it naturally captures correlations between these targets and automatically handles - at training and at inference - observations with partially available labels. Third, to avoid overfitting to sparse label sets, prior semi-supervised approaches typically apply a pooling operator (e.g., average pooling across the full sequence length) to reduce the dimensionality of input features. This pooling operation potentially destroys valuable information for the downstream task. In contrast, no such pooling operation is applied in the ProteinNPT architecture, and the network leverages self-attention to learn dependencies between labels and the embeddings of specific residues in the sequence.

### Auxiliary labels

The effectiveness of our architecture is significantly enhanced by the incorporation of _auxiliary labels_ at training and inference. We define auxiliary labels as additional inputs which are both 1) easy to obtain for all or for a subset of train and test instances 2) known to be correlated with the target of interest. They are concatenated column-wise to the input batch and are handled by ProteinNPT layers in the forward pass just like any other target. However, they are excluded from the loss computation and are only meant to increase performance on the target prediction task by instilling complementary information into the model. In the fitness prediction tasks discussed in SS 4, we use as auxiliary labels the _unsupervised fitness predictions_ obtained with the underlying protein language model used to extract the input sequence embeddings (i.e., MSA Transformer). As shown in ablations (Appendix C.2), and consistent with the findings from Hsu et al. (2022), this helps significantly increase our prediction performance, in particular when performing inference at positions where no mutation was observed during training.

### Conditional sampling

Diverging from prior works focusing on supervised protein property predictions (SS 2 and SS 4.2), ProteinNPT learns a joint representation of both protein sequences and labels. Since it is trained with a masked-language model objective, its output at a given masked position is a distribution over the amino acid vocabulary at that position conditioned on non-masked tokens, including labels. As such, it is a conditional _pseudo-generative_ model which can be used to sample new sequences iteratively. Prior works have investigated sampling with protein masked-language models in the unsupervised setting (Sgarbossa et al., 2023). However, a distinctive feature of our architecture is its ability to condition on values of the property of interest. This approach allows to steer the generation of new sequences to align with specific protein design goals, as detailed in SS 5.2.

## 4 Protein property prediction

### ProteinGym benchmark

The fitness landscape of naturally occurring proteins is the result of an intricate set of overlapping constraints that these proteins are subjected to in an organism. It is thus challenging to identify a single molecular property that is both easy to measure experimentally and that reflects that complexity. Experimental measurements are imperfect ground truth measurements of the underlying fitness. Furthermore, there is no such thing as fitness in the absolute but rather fitness at a specific temperature, pH, or in a given cell type. To make the conclusions that we seek to draw in the subsequent sections more meaningful, it is critical to consider a large and diverse set of experimental assays. To that end, we conducted our experiments on ProteinGym (Notin et al., 2022), which contains an extensive set of Deep Mutational Scanning (DMS) assays covering a wide range of functional properties (e.g., thermostability, ligand binding, viral replication, drug resistance). We include an additional 13 DMS assays to ProteinGym, bringing the total to 100 DMS assays (Appendix A).

We develop 3 distinct cross-validation schemes to assess the ability of each model to extrapolate to positions not encountered during training. In the _Random_ scheme, commonly-used in other supervised fitness prediction benchmarks (Rao et al., 2019; Dallago et al., 2022), each mutation is randomly allocated to one of five distinct folds. In the _Contiguous_ scheme, the sequence is split into five contiguous segments along its length, with mutations assigned to each segment based on the position they occur in the sequence. Lastly, the _Modulo_ scheme uses the modulo operator to assign mutated positions to each fold. For example, position 1 is assigned to fold 1, position 2 to fold 2, and so on, looping back to fold 1 at position 6. This pattern continues throughout the sequence. We note that there is no inherent issue with using a Random cross-validation scheme to estimate the performance of predictive models. However, the conclusions drawn and the generalizability claims based on it require careful consideration.

### Baselines

We start from the top-performing model variant from (Hsu et al., 2022), namely a ridge regression on one-hot encodings of protein sequences, augmented with unsupervised fitness predictions from DeepSequence (Riesselman et al., 2018). Building on recent progress from the protein property prediction and engineering literature, we then improve this baseline in several ways. First, we show that augmenting the model from Hsu et al. (2022) with the latest state-of-the-art unsupervised fitness predictors helps further boost the predictive performance, with the performance boost being consistent with the zero-shot performance of the corresponding unsupervised models (Appendix D). Then, considering the performance across the various cross-validation schemes described in SS 4.1, we show the limits of One-Hot-Encoded (OHE) features to generalize across positions and propose to use instead embeddings from large protein language models. There is a rich literature investigating this idea, and we test out a wide range of model architectures that have been proposed to learn a mapping between embeddings and labels: L2-penalized regression, shallow Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), ConvBERT (Elmaggar et al., 2023) and Light Attention (Stark et al., 2021). Combining these different ideas together, we obtain the main baselines we compare against in subsequent sections: models that both leverage embeddings from pretrained language models, and are augmented with unsupervised fitness predictions. We carry out thorough ablations and hyperparameter search (Appendix C.2) to identify the best performing combinations.

### Single mutant property prediction

**Experimental setup** We first focus on the task of predicting fitness for single substitution mutants. For each assay in ProteinGym and cross-validation scheme introduced in SS 4.1, we perform a 5-fold cross-validation, selecting the first 4 folds for training, and using the remaining one as test set. Intuitively, since the Random cross-validation scheme is such that mutations in the test set may occur at the same positions as certain mutations observed in the training data, model performance will typically be higher in that regime. Similarly, the Contiguous scheme is intuitively the most difficult regime in which we assess the ability of the different models to extrapolate on positions relatively 'far' in the linear sequence from any mutation being part of the training data.

**Results** We find that ProteinNPT markedly outperforms all baselines, across all cross-validation schemes, and both in terms of Spearman's rank correlation and Mean Squared Error (MSE) (Table 1). Our results also confirm that a model that would learn solely based on one-hot-encoded inputs would be unable to make any valuable prediction at positions that were not observed at training time (Spearman's rank correlation near zero on the Contiguous and Modulo schemes). Augmenting these models with predictions from unsupervised predictors helps to generally address that issue, although the performance on the Contiguous and Modulo schemes is similar to the zero-shot performance of unsupervised models (the average Spearman's rank correlation across all assays for the MSA Transformer in the zero-shot setting is 0.41). The baseline combining a transform of embeddings from the MSA Transformer, augmented with unsupervised fitness predictions ('Embeddings - Augmented (MSAT) in Table 1) performs better across the board than the one-hot encoding equivalent, yet its performance is far from that of ProteinNPT, in particular in the Random cross-validation scheme. We interpret the latter by the fact the column-wise attention model in the ProteinNPT layer (Fig. 1) is more beneficial to performance when a mutation at that position has been observed at train time.

### Multiple mutants property prediction

**Experimental setup** We next consider all assays in the extended ProteinGym benchmark that include multiple mutants. Given the challenges to assign multiple mutants to non-overlapping position sets, we only consider the Random cross-validation scheme in this analysis. We report the Spearman's rank correlation in Fig. 2 and MSE in Appendix E (conclusions are consistent for the two metrics).

**Results** The performance lift of ProteinNPT observed in the single mutants analyses translates to multiple mutants: ProteinNPT has superior aggregate performance and outperforms all other baselines in 14 out of the 17 assays with multiple mutants (Fig. 2, left). The performance lift is also robust to the mutational depth considered (Fig. 2, right).

### Multiple property prediction

ProteinGym contains a subset of protein families for which several experimental assays have been carried out, offering the possibility to test out the ability of various models to predict multiple properties of interest simultaneously. We report the performance on the Random cross-validation scheme in Fig. 9, and other schemes are provided in Appendix E. ProteinNPT outperforms all other baselines in that regime as well, consistently across all settings analyzed.

    &  &  \\
**Model name** & **Contig.** & **Mod.** & **Rand.** & **Avg.** & **Contig.** & **Mod.** & **Rand.** & **Avg.** \\  OHE & 0.08 & 0.02 & 0.54 & 0.21 & 1.17 & 1.11 & 0.92 & 1.06 \\ OHE - Aug. (DS) & 0.40 & 0.39 & 0.48 & 0.42 & 0.98 & 0.93 & 0.78 & 0.90 \\ OHE - Aug. (MSAT) & 0.41 & 0.40 & 0.50 & 0.44 & 0.97 & 0.92 & 0.77 & 0.89 \\ Embed. - Aug. (MSAT) & 0.47 & 0.49 & 0.57 & 0.51 & **0.93** & 0.85 & 0.67 & 0.82 \\ ProteinNPT & **0.48** & **0.51** & **0.65** & **0.54** & **0.93** & **0.83** & **0.53** & **0.77** \\   

Table 1: **Fitness prediction performance**. Spearman’s rank correlation & MSE between model predictions & experimental measurements, averaged across all assays in ProteinGym. DS and MSAT are shorthand for DeepSequence and MSA Transformer respectively. “Aug.” indicates models augmented with unsupervised predictions. The Spearman for MSAT in the zero-shot setting is 0.41 (independent of CV split). Assay-level performance & standard errors are reported in Appendix D.

## 5 Protein redesign

### In silico iterative protein redesign

Experimental setupOur goal is to emulate _in silico_ a real-world protein engineering scenario where we iteratively redesign a known protein sequence across several experimental rounds to increase its fitness. Performing multiple rounds of experiments sequentially allows us to harness the information acquired in prior rounds, as is commonly done in directed evolution. In a standard wet lab setting, there is no constraint on the subset of mutated sequences that can be experimentally tested. However, in our in silico setup, we are constrained to select mutants that belong to a predefined pool of sequences which have been experimentally measured in the DMS assays we consider, such as the ones from ProteinGym. We provide a detailed algorithm for our procedure in Fig. 3 (left), which we cast as a batch Bayesian optimization task. At the beginning of each in silico experiment, this pool is entirely unlabeled. In each subsequent round, we score all possible variants in the unlabeled pool and select from it a subset \(B\) of items which are deemed optimal by the Upper Confidence Bound (UCB) acquisition function:

\[a(x;)=(x)+.(x)\] (6)

where \(\) is the predicted fitness value, \(\) quantifies uncertainty in the prediction, and \(\) controls the exploration-exploitation trade-off. We develop and test various uncertainty quantification methods for non-parametric transformer architectures, based on Monte Carlo dropout  and resampling inference batches with replacement (see details in Appendix G.1). We find that a _hybrid_ scheme in which we both sample model parameters via Monte Carlo dropout and resample inference batches works best. After each acquisition cycle, all models are retrained from scratch for 2k training steps. On Fig. 3, we plot the proportion of acquired points that are above a predefined threshold (top 3 deciles of all measurements in the DMS assays) as a function of the number of acquisition rounds performed, averaged across all assays. The baselines considered are the same as for the property prediction experiments. Individual plots for each assay are provided in Appendix G.3.

Figure 2: **Multiples mutants performance.** (Left) Spearman’s rank correlation between model predictions and experimental measurements, for each assay in ProteinGym with multiple mutants (see Appendix A.1). (Right) Average Spearman’s rank correlation, overall and by mutational depth.

ResultsWe find that ProteinNPT vastly outperforms all other baselines in aggregate across all assays, and achieves superior performance on the large majority of individual assays. Furthermore, we observe that the performance lift is more pronounced for assays for which we start with a larger pool of unlabeled sequences. This suggests that in a real-world redesign scenario, in which choices are not limited to a predefined pool as in our in silico setting, ProteinNPT might exhibit even greater performance gains relative to the baseline models.

### Conditional sampling

As a supervised pseudo-generative model of labeled protein sequences, ProteinNPT has the ability to generate novel sequences conditioned on particular values of these properties. This is in contrast to prior generative models that have been used to sample natural-like proteins with no bias towards properties of interest (Repecka et al., 2021; Sgarbossa et al., 2023), or conditioned on broad taxonomic labels characterizing full protein families (Madani et al., 2023) as opposed to the sequence-specific properties we consider here. In this experiment, we illustrate the ability of ProteinNPT to sample novel protein sequences with desired property from the Green Fluorescent Protein (GFP), but the approach can be applied to any other protein. We first identify the sequence with the highest measured property in the GFP DMS assay (Sarkisyan et al., 2016). We then form an input batch by randomly selecting other labeled sequences, mask a fixed number of tokens (5 in our experiment) in the fittest sequence, obtain the resulting log softmax over the masked positions with a forward pass and sample from these to obtain new sequences. Critically, rather than selecting the masked positions at random, we sample them from the positions with the highest row-wise attention coefficient with the target (averaged across heads) in the last layer of ProteinNPT. This helps ensure we sample new mutations at the positions with the highest impact on the target of interest. We generate 1k new sequences with that process and measure the corresponding fitness with an independent zero-shot fitness predictor (ESM-1v). Finally, we compare the resulting fitness distribution across all samples with distributions obtained with two baselines: the first in which mutations are randomly selected, and the second in which mutations are selected to further minimize the fitness of the less fit protein. We report the corresponding results in Figure 4, and observe that proteins sampled with ProteinNPT have substantially higher fitness values relative to the two baseline sampling strategies and relative to sequences in the original DMS assay.

## 6 Discussion

Row-wise attention captures dependencies between residues and function.Since we learn a joint representation of sequence and labels, the attention maps of ProteinNPT capture meaningful correlations between residues and the property labels we train on. We represent the row-wise attention coefficients for a ProteinNPT model trained on a DHFR protein assay (Thompson et al., 2020) in Fig. 5 (last layer, first attention head). The analysis recapitulates certain known facts about important residues for the assayed property (e.g., the highest attention score in red corresponds to a core substrate binding site), and may help identify new residues involved the protein function of interest.

Figure 3: **In silico protein redesign.** (Left) Iterative redesign algorithm (Right) Recall rate of high fitness points (top 3 deciles) vs acquisition cycle, averaged across all DMS assays in ProteinGym.

Column-wise attention is critical to reach top performance.We test out the impact of column-attention on model performance. All analyses are conducted on the same subset of 8 DMS assays used in other ablations (Appendix B.2). We first re-train ProteinNPT models without the column-wise attention layers, and observe that the downstream performance drops significantly (Table 2, left). At inference, we find that predicting with 100 randomly-selected labeled sequences captures most of the performance lift, and observe no benefit beyond 1k batch points (Table 2, right).

## 7 Conclusion

This work introduced ProteinNPT, a novel conditional pseudo-generative architecture which reaches state-of-the-art performance across a wide range of fitness prediction and iterative redesign settings, covering a diverse set of protein families and assayed properties (e.g, thermostability, binding, enzymatic activity). The architecture straightforwardly extends to the multiple property prediction setting, can leverage auxiliary labels, supports uncertainty quantification, and allows to sample new sequences conditioned on desired property values. While sequence embeddings from the MSA Transformer lead to the best performance in our ablations, the architecture of ProteinNPT is agnostic to the model used to obtain these embeddings. As protein language models continue to improve, ProteinNPT will also benefit from the resulting improved embeddings. Future work will investigate additional strategies for conditional sampling, extension of the architecture to handle insertions and deletions, and generalization across protein families.