# SimMTM: A Simple Pre-Training Framework for

Masked Time-Series Modeling

 Jiaxiang Dong, Haixu Wu, Haoran Zhang, Li Zhang, Jianmin Wang, Mingsheng Long

School of Software, BNRist, Tsinghua University, China

{djx20,z-hr20}@mails.tsinghua.edu.cn, vuhaixu98@gmail.com

{lizhang,jimwang,mingsheng}@tsinghua.edu.cn

Equal Contribution

###### Abstract

Time series analysis is widely used in extensive areas. Recently, to reduce labeling expenses and benefit various tasks, self-supervised pre-training has attracted immense interest. One mainstream paradigm is masked modeling, which successfully pre-trains deep models by learning to reconstruct the masked content based on the unmasked part. However, since the semantic information of time series is mainly contained in temporal variations, the standard way of randomly masking a portion of time points will seriously ruin vital temporal variations of time series, making the reconstruction task too difficult to guide representation learning. We thus present SimMTM, a Simple pre-training framework for Masked Time-series Modeling. By relating masked modeling to manifold learning, SimMTM proposes to recover masked time points by the weighted aggregation of multiple neighbors outside the manifold, which eases the reconstruction task by assembling ruined but complementary temporal variations from multiple masked series. SimMTM further learns to uncover the local structure of the manifold, which is helpful for masked modeling. Experimentally, SimMTM achieves state-of-the-art fine-tuning performance compared to the most advanced time series pre-training methods in two canonical time series analysis tasks: forecasting and classification, covering both in- and cross-domain settings. Code is available at https://github.com/thuml/SimMTM.

## 1 Introduction

Time series analysis has attached immense importance in extensive real-world applications, such as financial analysis, energy planning and etc . Vast amounts of time series are incrementally collected from IoT and wearable devices. However, the semantic information of time series is mainly buried in human-indiscernible temporal variations, making it difficult to annotate. Recently, self-supervised pre-training has been widely explored , which benefits deep models from pretext knowledge learned over large-scale unlabeled data and further promotes the performance of various downstream tasks. Mainly, as a well-recognized pre-training paradigm, masked modeling has achieved great success in many areas, such as masked language modeling (MLM)  and masked image modeling (MIM) . This paper extends pre-training methods to time series, especially masked time-series modeling (MTM).

The canonical technique of masked modeling is to optimize the model by learning to reconstruct the masked content based on the unmasked part . However, unlike images and natural languages whose patches or words contain much even redundant semantic information, we observe that the valuable semantic information of time series is mainly included in the temporal variations, such as the trend, periodicity, and peak valley, which can correspond to unique weather processes, abnormal faults, etc. in the real world. Therefore, directly masking a portion of time points will seriously ruinthe temporal variations of the original time series, which makes the reconstruction task too difficult to guide representation learning of time series. (Figure 1).

According to the analysis in stacked denoising autoencoders , as shown in Figure 1, we can view the randomly masked series as the "neighbor" of the original time series outside the manifold and the reconstruction process is to project the masked series back to the manifold of original series. However, as we analyzed above, direct reconstruction may fail since the essential temporal variations are ruined by random masking. Inspired by the manifold perspective, we go beyond the straightforward reconstruction convention of masked modeling and propose a natural idea of reconstructing the original data from its _multiple_ "neighbors", i.e. multiple masked series. Although the temporal variations of the original time series have been partially dropped in each randomly masked series, the multiple randomly masked series will complement each other, making the reconstruction process much more accessible than directly reconstructing the original series from a single masked series. This process will also pre-train the model to uncover the local structure of the time series manifold implicitly, thereby benefiting masked modeling and representation learning [35; 41].

Based on the above insights, we propose the SimMTM as a simple but effective pre-training framework for time series in this paper. Instead of directly reconstructing the masked time points from unmasked parts, SimMTM recovers the original time series from multiple randomly masked time series. Technically, SimMTM presents a neighborhood aggregation design for reconstruction, which is to aggregate the point-wise representations of time series based on similarities learned in the series-wise representation space. In addition to the reconstruction loss, a constraint loss is presented to guide the series-wise representation learning based on the neighborhood assumption of the time series manifold. Empowering by above designs, SimMTM achieves consistent state-of-the-art in various time series analysis tasks when fine-tuning the pre-trained model into downstream tasks, covering both the low-level forecasting and high-level classification tasks, even if there is a clear domain shift between pre-training and fine-tuning datasets. Overall, our contributions are summarized as follows:

* Inspired by the manifold perspective of masking, we propose a new task for masked time-series modeling, which is to reconstruct the original time series on the manifold based on multiple masked series outside the manifold.
* Technically, we present SimMTM as a simple but effective pre-training framework, which aggregates point-wise representations for reconstruction based on the similarities learned in series-wise representation space.
* SimMTM consistently achieves the state-of-the-art fine-tuning performance in typical time series analysis tasks, including low-level forecasting and high-level classification, covering both in- and cross-domain settings.

## 2 Related Work

### Self-supervised Pre-training

Self-supervised pre-training is an important research topic for learning generalizable and shared knowledge from large-scale data and further benefiting downstream tasks . Originally, this topic has been widely explored in computer vision and natural language processing. Elaborative manually-designed self-supervised tasks are presented, which can be roughly categorized into contrastive learning [12; 5; 4] and masked modeling [7; 11]. Recently, following the well-established contrastive learning and masked modeling paradigms, some self-supervised pre-training methods for time series have been proposed [9; 27; 34; 33; 36; 54; 52; 6].

Figure 1: Comparison between (a) canonical masked modeling and (b) SimMTM in both manifold perspective and reconstruction performance. The showcase is to recover 50% masked time series.

Contrastive learning.The critical insight of contrastive learning is to optimize the representation space based on the manually designed positive and negative pairs. where representations of positive pairs are optimized to be close to each other. In contrast, negative ones tend to be far apart [48; 14]. The canonical design presented in SimCLR  views different augmentations of the same sample as positive pairs and augmentations among different samples as negative pairs.

Recently, in time series pre-training, many designs of positive and negative pairs have been proposed by utilizing the invariant properties of time series. Concretely, to make the representation learning seamlessly related to temporal variations, TimCLR  adopts the DTW  to generate phase-shift and amplitude-change augmentations, which is more suitable for time series context. TS2Vec  splits multiple time series into several patches and further defines the contrastive loss in both instance-wise and patch-wise aspects. TS-TCC  presents a new temporal contrastive learning task as making the augmentations predict each other's future. Mixing-up  exploits a data augmentation scheme in which new samples are generated by mixing two data samples. LaST  aims to disentangle the seasonal-trend representations in the latent space based on variational inference. Afterward, CoST  employs contrastive losses in both time and frequency domain to learn discriminative seasonal and trend representations. Besides, TF-C  proposes a novel time-frequency consistency architecture and optimizes time-based and frequency-based representations of the same example to be close to each other. Note that contrastive learning mainly focuses on the high-level information , and the series-wise or patch-wise representations inherently mismatch the low-level tasks, such as time series forecasting. Thus, in this paper, we focus on the masked modeling paradigm.

Masked modeling.The masked modeling paradigm optimizes the model by learning to reconstruct the masked content from the unmasked part. This paradigm has been widely explored in computer vision and natural language processing, which is to predict the masked words of a sentence  and masked patches of an image [2; 11; 50] respectively. As for the time series analysis, TST  follows the canonical masked modeling paradigm and learns to predict removed time points based on the remaining time points. Next, PatchTST  proposes to predict masked subseries-level patches to capture the local semantic information and reduce memory usage. Ti-MAE  uses mask modeling as an auxiliary task to boost the forecasting and classification performances of advanced Transformer-based methods. However, as we stated before, directly masking time series will ruin the essential temporal variations, making the reconstruction too difficult to guide the representation learning. Unlike the direct reconstruction in previous works, SimMTM presents a new masked modeling task, which is reconstructing the original time series from multiple randomly masked series.

### Understanding Masked Modeling

Masked modeling has been explored in stacked denoising autoencoders , where masking is viewed as adding noise to the original data and the masked modeling is to project masked data from the neighborhood back to the original manifold, namely denoising. It has recently been widely used in pre-training, which can learn valuable low-level information from data unsupervisedly . Inspired by the manifold perspective, we go beyond the classical denoising process and project the masked data back to the manifold by aggregating multiple masked time series within the neighborhood.

## 3 SimMTM

As aforementioned, to tackle the problem that randomly masking time series will ruin the essential temporal variation information, SimMTM proposes to reconstruct the original time series from multiple masked time series. To implement this, SimMTM first learns similarities among multiple time series in the series-wise representation space and then aggregates the point-wise representations of these time series based on pre-learned series-wise similarities. Next, we will detail the techniques in both model architecture and pre-training protocol aspects.

### Overall Architecture

The reconstruction process of SimMTM involves the following four modules: masking, representation learning, series-wise similarity learning and point-wise reconstruction.

Masking.Given \(\{_{i}\}_{i=1}^{N}\) as a mini-batch of \(N\) time series samples, where \(_{i}^{L C}\) contains \(L\) time points and \(C\) observed variates, we can easily generate a set of masked series for each sample \(_{i}\) by randomly masking a portion of time points along the temporal dimension, formalizing by:

\[\{}_{i}^{j}\}_{j=1}^{M}=_{r}(_{i}),\] (1)

where \(r\) denotes the masked portion. \(M\) is a hyperparameter for the number of masked time series. \(}_{i}^{j}^{L C}\) represents the \(j\)-th masked time series of \(_{i}\), where the values of masked time points are replaced by zeros. With above process, we can obtain a batch of augmented time series. For clarity, we present all the \((N(M+1))\) input series in a set as \(=_{i=1}^{N}(\{_{i}\}\{}_{i}^{j}\}_{j=1}^{M})\).

Representation learning.After the encoder and projector layer, we can obtain the point-wise representations \(\) and series-wise representations \(\), which can be formalized as:

\[=_{i=1}^{N}(\{_{i}\}\{}_{i}^{j}\}_{j=1}^{M})=(),\ \ =_{i=1}^{N}(\{_{i}\}\{}_{i}^{j}\}_{j=1}^{M})=(),\] (2)

where \(_{i},}_{i}^{j}^{L d_{}}\) and \(_{i},}_{i}^{j}^{1 d_{}}\). \(()\) denotes the model encoder, which can project the input data into deep representations and will be transferred to downstream tasks during the fine-tuning process. In this paper, we implement the encoder as a well-acknowledged Transformer  and ResNet . As for the \(()\), we employ a simple MLP layer along the temporal dimension to obtain series-wise representations. More details can be found in Section 4. Technically, the encoder is applied to input series separately, namely \(_{i=1}^{N}((_{i})\{( }_{i}^{j})\}_{j=1}^{M})\), and so does for projector. Here we adopt the set-style formalization for conciseness.

Series-wise similarity learning.Note that directly averaging multiple masked time series will result in the over-smoothing problem , impeding the representation learning. Thus, to precisely reconstruct the original time series, we attempt to utilize the similarities among series-wise representations \(\) for weighted aggregation, namely exploiting the local structure of the time series manifold. For simplification, we formalize the calculation of series-wise similarities as follows:

\[=()^{D D},D=N(M+1),\ \ _{,}=^{}}{ \|\|\|\|},,,\] (3)

where \(\) is the matrix of pair-wise similarities for \((N(M+1))\) input samples in series-wise representation space, which are measured by the cosine distance. \(_{,}\) is the calculated similarity between series-wise representations \(,\).

Point-wise aggregation.As shown in Figure 2, based on the learned series-wise similarities, the aggregation process for the \(i\)-th original time series is:

\[}_{i}=_{^{}\{ _{i}\}}_{_{i},^{}}/ )}{_{^{}\{_{i} \}}(_{_{i},^{}}/)}^ {},\] (4)

where \(^{}\) represents the corresponding point-wise representation of \(^{}\), i.e. \(^{}=(^{})\). \(}_{i}^{L d_{}}\) is the reconstructed point-wise representation. \(\) denotes the temperature hyperparameter

Figure 2: Architecture of SimMTM, which reconstructs the original time series by adaptive aggregating multiple masked time series based on series-wise similarities learned contrastively from data.

of softmax normalization for series-wise similarities. It is notable that as described in Eq. (4), for each time series \(_{i}\), the reconstruction is not only based its own masked series \(\{}_{i}^{j}\}_{j=1}^{M}\). We also introduce other series representations \(\{_{i}\}\) into aggregation, which requires the model to suppress the interference of less-related noise series and precisely learn similar representations for both the masked and the original series, namely guiding the model to learn the manifold structure better. After the decoder, we can obtain the reconstructed original time series, namely

\[\{}_{i}\}_{i=1}^{N}=(\{}_{i}\}_{i=1}^{N}),\] (5)

where \(}_{i}^{L C}\) is the reconstruction to \(_{i}\). \(()\) is instantiated as a simple MLP layer along the channel dimension following .

### Self-supervised Pre-training

Following the masked modeling paradigm, SimMTM is supervised by a reconstruction loss:

\[_{}=_{i=1}^{N}\|_{i}-}_{i}\|_{2}^{2}.\] (6)

Note that the reconstruction process is directly based on the series-wise similarities, while it is hard to guarantee the model captures the precise similarities without explicit constraints in the series-wise representation space. Thus, to avoid trivial aggregation, we also utilize the neighborhood assumption of the time series manifold to calibrate the structure of series-wise representation space \(\). For clarity, we formalize the neighborhood assumption by defining positive and negative pairs as follows:

\[~{}(_{i},_{i}^{+}),~{}_{i}^{+}\{}_{i}^{j}\}_{j=1}^{M }\\ ~{}(_{i},_{i}^{-}),~{} _{i}^{-}\{_{k}\}\{}_{k}^{j}\}_ {j=1}^{M},i k\] (7)

where \(_{i}^{+}\) and \(_{i}^{-}\) mean the elements that are assumed as close to and far away from \(_{i}\) respectively. Eq. (7) indicates that the original time series and its masked series will present close representations and be far away from the representations from other series in \(\). For each series-wise representation \(\), we define the set of its assumed close series as \(^{+}\). Note that to avoid the dominating representation, we assume that \(^{+}\). With the above formalization, we can define manifold constraint to series-wise representation space as:

\[_{}=-_{}(_{ ^{}^{+}}_{, ^{}}/)}{_{^{} \{\}}(_{,^{} }/)}),\] (8)

which can optimize the learned series-wise representations to satisfy the neighborhood assumption in Eq. (7) better. Finally, the overall optimization process of SimMTM can be represented as follows:

\[_{}_{}+_{},\] (9)

where \(\) denotes the set of all parameters of the deep architecture. To trade off the two parts in Eq. (9), we adopt the tuning strategy presented by , which can adjust the hyperparameters \(\) adaptively according to the homoscedastic uncertainty of each loss.

## 4 Experiments

To fully evaluate SimMTM, we conduct experiments on two typical time series analysis tasks: forecasting and classification, covering low-level and high-level representation learning. Further, we present the fine-tuning performance for each task under in- and cross-domain settings.

Benchmarks.We summarize the experiment benchmarks in Table 1, comprising twelve real-world datasets that cover two mainstream time series analysis tasks: time series forecasting and classification. Concretely, we have followed the standard experimental setups in Autoformer  for the forecasting tasks and the experiment settings proposed by TF-C  for classification.

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

### Classification

In-domain.We investigate the in-domain pre-training effect on the classification tasks in Table 4. Note that different from forecasting, the classification task requires the model to learn high-level time series representations. From Table 4, we can find that the contrastive pre-training baselines achieve competitive performances. In contrast, the masking-based model Ti-MAE  and TST  perform poorly, and TST even exhibits a negative transfer phenomenon compared to the random initialization, indicating that contrastive learning is generally more suitable for classification tasks. Surprisingly, although SimMTM follows the masked modeling paradigm, with our specially-designed reconstruction task, it can still achieve the best performance in the classification task. This is benefited from the neighborhood aggregation from _multiple_ masked series, which enables the model to exploit the local structure of the time series manifold.

Cross-domain.We experiment with four cross-domain transfer scenarios in Table 4: SleepEEG \(\) {Epilepsy, FD-B, Gesture, EMG}, where the target datasets are distinct from the pre-training dataset. Due to the large gap between pre-training and fine-tuning datasets, the baselines perform poorly in most cases, while SimMTM still surpasses other baselines and the random initialization significantly. Especially for SleepEEG \(\) EMG, SimMTM remarkably surpasses previous state-of-the-art TF-C (_Accuracy_: 97.56% vs. 81.74%). These results demonstrate that SimMTM can precisely capture valuable knowledge from pre-training datasets and uniformly benefit extensive downstream datasets.

### Model Analysis

Ablations.As shown in Figure 4, we provide ablations to two parts of the training loss in SimMTM. It is observed that both \(_{}\) and \(_{}\) are essential to the final performance. Especially, for the SleepEEG \(\) EMG experiment, SimMTM surpasses the random initialization remarkably, where reconstruction and constraint losses provide 9.7% and 16.0% absolute improvement respectively. Besides, we can also find that in comparison to \(_{}\), \(_{}\) provides more contributions to the final results. This comes from our design that the constraint loss uncovers a proper time series

   Models & **SimMTM** Random init. Ti-MAE  & TST  & LaST  & TF-C  & CoST  & TS2Vec  \\  Epilepsy \(\) Epilepsy & **94.75** & 89.83 & 80.34 & 80.89 & 92.11 & 93.96 & 92.35 & 92.33 \\  SleepEEG \(\) Epilepsy & **95.49** & 89.83 & 73.45 & 82.89 & 86.46 & 94.95 & 93.66 & 94.46 \\  SleepEEG \(\) FD-B & **69.40** & 47.36 & 67.98 & 65.57 & 46.67 & 69.38 & 54.82 & 60.74 \\  SleepEEG \(\) Gesture & **80.00** & 42.19 & 75.54 & 75.12 & 64.17 & 76.42 & 73.33 & 73.33 \\  SleepEEG \(\) EMG & **97.56** & 77.80 & 63.52 & 75.89 & 66.34 & 81.74 & 73.17 & 80.92 \\  Avg & **87.44** & 69.40 & 72.17 & 76.07 & 71.15 & 83.29 & 77.47 & 80.36 \\   

Table 4: In- and cross-domain settings of classification. For the in-domain setting, we pre-train and fine-tune the model on the same dataset Epilepsy. For the cross-domain setting, we pre-train a model on SleepEEG and fine-tune it to multiple target datasets: Epilepsy, FD-B, Gesture, EMG. Accuracy (%) score are recorded. Full results are included in Appendix E.

Figure 4: Ablations of SimMTM on the reconstruction loss (\(_{}\)) and constraint loss (\(_{}\)) in time series forecasting (left part) and classification (right part) tasks under both in-domain and cross-domain settings. More ablations are included in Appendix E.

manifold helpful for reconstruction from multiple masked series, without which the neighborhood aggregation will degenerate to the trivial average.

Representation analysis.To illustrate the advantages of SimMTM intuitively, we provide a representation analysis in Table 5, where we can find the following observations. Firstly, we can find that the CKA value of SimMTM in the classification task is clearly smaller than the values in the forecasting task, where the former is a high-level task and the latter requires low-level representations. These results demonstrate that SimMTM can learn adaptive representations for different tasks, which can be benefited from our design in the pre-training loss. Concretely, the temporal variations of classification pre-training datasets are much more diverse than the forecasting datasets. Thus, the \(_{}\) will be easier for optimization in classification, deriving a smaller CKA value. Secondly, from \(|_{}|\), it is observed that the models pre-trained from SimMTM present a smaller gap with respect to the final fine-tuned model in representation learning properties, which is why SimMTM can consistently improve downstream tasks.

Model generality.From Table 6, we can find that as a general time series pre-training framework, SimMTM can consistently improve the forecasting performance of diverse advanced base models, even for the state-of-the-art time series forecasting model PatchTST . This generality also

   } &  &  &  &  \\   & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\  Transformer  & 1.088 & 0.836 & 4.103 & 1.612 & 0.901 & 0.704 & 1.624 & 0.901 \\
**+ SimMTM** & **0.927** & **0.761** & **3.498** & **1.487** & **0.809** & **0.663** & **1.322** & **0.808** \\  Autoformer  & 0.573 & 0.573 & 0.550 & 0.559 & 0.615 & 0.528 & 0.324 & 0.368 \\
**+ SimMTM** & **0.561** & **0.568** & **0.543** & **0.555** & **0.553** & **0.505** & **0.315** & **0.360** \\  NS Transformer  & 0.570 & 0.537 & 0.526 & 0.516 & 0.481 & 0.456 & 0.306 & 0.347 \\
**+ SimMTM** & **0.543** & **0.527** & **0.493** & **0.514** & **0.431** & **0.455** & **0.301** & **0.345** \\  PatchTST  & 0.417 & 0.431 & 0.331 & 0.379 & 0.352 & 0.382 & 0.258 & 0.317 \\
**+ Sub-series Masking** & 0.430\(\) & 0.445\(\) & 0.355\(\) & 0.394\(\) & **0.341** & 0.379 & 0.258 & 0.318\(\) \\
**+ SimMTM** & **0.409** & **0.428** & **0.329** & 0.379 & 0.348 & **0.378** & **0.254** & **0.313** \\   

Table 6: Performance by applying SimMTM to four advanced time series forecasting models under the in-domain setting. “+ Sub-series Masking” refers to the sub-series masked modeling pre-training method proposed by PatchTST  itself. MSE and MAE are averaged from all prediction lengths in {96,192,336,720}. The detailed results for each forecasting horizon are in Appendix E.

    & Ti-MAE  & TST  & LaST  & TF-C  & CoST  & TS2vec  & **SimMTM** \\   & Pre-training & 84.12 & 54.98 & 82.01 & 85.78 & 60.74 & 70.01 & 33.87 \\  & Fine-tuning & 87.26 & 55.80 & 79.56 & 86.30 & 62.24 & 69.79 & 32.84 \\   & \(|}}|\) & 3.14 & 0.82 & 2.45 & 1.53 & 1.50 & **0.22** & 1.04 \\   & Pre-training & 83.60 & 99.76 & 75.20 & 59.35 & 87.09 & 70.20 & 97.79 \\  & Fine-tuning & 91.24 & 94.92 & 79.25 & 60.60 & 77.38 & 83.73 & 97.89 \\    & \(|}}|\) & 7.64 & 4.84 & 4.05 & 1.25 & 9.70 & 13.53 & **0.11** \\  Sum\(|}}|\) & 10.78 & 5.66 & 6.50 & 2.77 & 11.20 & 13.75 & **1.15** \\   

Table 5: Representation analysis for different methods in classification and forecasting tasks. For each model, we calculate the Centered Kernel Alignment (CKA) similarity (%)  between representations from the first and the last encoder layers to measure the representation-learning property of deep models. Since the bottom layer representations usually contain low-level or detailed information, a smaller CKA similarity means the top layer includes different information from the bottom layer and indicates the model tends to learn high-level representations or more abstract information. For comparison, we also calculate the \(|_{}|\) between pre-trained and fine-tuned models, where a smaller value indicates a smaller representation gap between pre-training and fine-tuning, and the representations have stronger universality and portability.

indicates that we can further improve the model's performance by employing advanced base models as encoders. It is also notable that different from the negative transfer phenomenon caused by the canonical sub-series masked modeling used in the PatchTST paper , the consistency improvement of SimMTM further verifies the effectiveness of our design.

Fine-tuning to limited data scenarios.One essential application of pre-training models is to provide prior knowledge for downstream tasks, especially for limited data scenarios, which is critical to the fast-adaption of deep models. Thus, to verify the effectiveness of SimMTM and other pre-training methods in data-limited scenarios, we pre-train a model on ETTh2 and fine-tune it to ETTh1 with different choices for the remaining proportions of training data. All results are presented in Figure 5. We can find that SimMTM achieves significant performance gains in different data proportions compared to other time series pre-training methods. Specifically, for the 10% data fine-tuning setting, SimMTM significantly outperforms the advanced masking-based method Ti-MAE  (_MSE_: 0.591 vs. 0.660). Compared with the contrastive-based method TF-C , SimMTM also achieves 26.0% MSE reduction. These results further verify that SimMTM can effectively capture valuable knowledge from datasets and boost the final performance, even in limited data scenarios.

Masking strategy.Note that the difficulty of reconstructing the original time series increases along with the increase of the masked ratio, but decreases when the number of neighbor masked series increases. We explore the potential relationship between the masked ratio and the number of masked series used for reconstruction, namely \(r\) and \(M\) in Eq. (1) respectively. The experimental results in Figure 5 show that we need to set \(M r\) to obtain better results. Experimentally, we choose the masking ratio as 50% and adopt three masked series for reconstruction throughout this paper. In addtion, we can find that only using one masked series (\(M=1\)) for pre-training generally performs worse than the settings with larger \(M\), where the latter enables the model to discover the relations between input series and its neighbors. See Figure 1 for an intuitive understanding. These results further highlight the advantage of our design in manifold learning.

## 5 Conclusion

This paper presents SimMTM, a simple pre-training framework for masked time-series modeling. Going beyond the previous convention in reconstructing the original time series from unmasked time points, SimMTM proposes a new masked modeling task as reconstructing the original series from its multiple neighbor masked series. Concretely, SimMTM aggregates the point-wise representations based on the series-wise similarities, which are carefully constrained by the neighborhood assumption on the time series manifold. Experimentally, SimMTM can furthest bridge the gap between pre-trained and fine-tuned models, thereby achieving consistent state-of-the-art in distinct forecasting and classification tasks compared to the most advanced time series pre-training methods, covering both in-domain and cross-domain settings. In the future, we will further extend SimMTM to large-scale and diverse pre-training datasets in pursuing the foundation model for time series analysis.

Figure 5: Model analysis. Left part is for fine-tuning ETTh2 pre-trained model to ETTh1 with limited data, where a smaller MSE indicates better performance. Right part presents the MSE performance of SimMTM in the ETTh1 “input-336-predict-96” in-domain setting with different masked ratio \(r\) and numbers of masked series \(M\), where a darker red means better performance.