# On the Robustness of Removal-Based Feature Attributions

Chris Lin

University of Washington

clin25@cs.washington.edu

&Ian Covert

Stanford University

icovert@stanford.edu

Equal contribution. \(\)Work done while at the University of Washington.

Su-In Lee

University of Washington

suinlee@cs.washington.edu

###### Abstract

To explain predictions made by complex machine learning models, many feature attribution methods have been developed that assign importance scores to input features. Some recent work challenges the robustness of these methods by showing that they are sensitive to input and model perturbations, while other work addresses this issue by proposing robust attribution methods. However, previous work on attribution robustness has focused primarily on gradient-based feature attributions, whereas the robustness of removal-based attribution methods is not currently well understood. To bridge this gap, we theoretically characterize the robustness properties of removal-based feature attributions. Specifically, we provide a unified analysis of such methods and derive upper bounds for the difference between intact and perturbed attributions, under settings of both input and model perturbations. Our empirical results on synthetic and real-world data validate our theoretical results and demonstrate their practical implications, including the ability to increase attribution robustness by improving the model's Lipschitz regularity.

## 1 Introduction

In recent years, machine learning has shown great promise in a variety of real-world applications. An obstacle to its widespread deployment is the lack of transparency, an issue that has prompted a wave of research on interpretable or explainable machine learning [45; 51; 53; 56; 60; 67]. One popular way of explaining a machine learning model is feature attribution, which assigns importance scores to input features of the model [45; 51; 53; 56; 60]. Many feature attribution methods can be categorized as either _gradient-based_ methods that compute gradients of model predictions with respect to input features , or _removal-based_ methods that remove features to quantify each feature's influence . While substantial progress has been made and current tools are widely used for model debugging and scientific discovery [10; 18; 37; 50], some important concerns remain. Among them is _unclear robustness properties_: feature attributions are vulnerable to adversarial attacks, and even without an adversary appear unstable under small changes to the input or model.

As a motivating example, consider the work of Ghorbani et al. , which shows that minor perturbations to an image can lead to substantially different feature attributions while preserving the original prediction. Similar to adversarial examples designed to alter model predictions [29; 43; 61], this phenomenon violates the intuition that explanations should be invariant to imperceptible changes. A natural question, and the focus of this work, is therefore: _When can we guarantee that feature_attributions are robust to small changes in the input or small changes in the model?_ In answering this question, we find that previous theoretical work on this topic has primarily focused on gradient-based methods. Hence, in this work we provide a unified analysis to characterize the robustness of removal-based feature attributions, under both input and model perturbations, and considering a range of existing methods in combination with different approaches to implement feature removal.

**Related work.** Recent work has demonstrated that gradient-based attributions for neural networks are susceptible to small input perturbations that can lead to markedly different attributions [21; 28; 41]. Theoretical analyses have established that this issue relates to model smoothness, and approaches like stochastic smoothing of gradients, weight decay regularization, smoothing activation functions, and Hessian minimization have been shown to generate more robust gradient-based attributions [4; 21; 22; 65]. As for the robustness of removal-based attributions under input perturbations, Alvarez-Melis and Jaakkola  empirically assess the robustness of LIME, SHAP, and Occlusion with the notion of Lipschitz continuity in local neighborhoods. Khan et al.  provide theoretical guarantees on the robustness of SHAP, RISE, and leave-one-out attributions using Lipschitz continuity, in the specific setting where held-out features are replaced with zeros. Agarwal et al.  study the robustness of discrete attributions for graph neural networks and establish Lipschitz upper bounds for the robustness of GraphLIME and GraphMASK. Our analysis of input perturbations is most similar to Khan et al. , but we generalize this work by considering alternative techniques for both feature removal and importance summarization (e.g., LIME's weighted least squares approach ).

Other works have considered model perturbations and manipulated neural networks to produce targeted, arbitrary gradient-based attributions [7; 34]. Anders et al.  explain this phenomenon through the insight that neural network gradients are underdetermined with many degrees of freedom to exploit, and they demonstrate better robustness when gradient-based attributions are projected onto the tangent space of the data manifold. As for removal-based attributions, it has been demonstrated that models can be modified to hide their reliance on sensitive features in LIME and SHAP [20; 58]; however, Frye et al.  show that such hidden features can be discovered if features are removed using their conditional distribution, suggesting a key role for the feature removal approach in determining sensitivity to such attacks. Our work formalizes these results through the lens of robustness under model perturbations, and provides new theoretical guarantees.

**Contribution.** The main contribution of this work is to develop a comprehensive characterization of the robustness properties of removal-based feature attributions, focusing on two notions of robustness that have received attention in the literature: stability under input changes and stability under model changes. Our specific contributions are the following: (1) We provide theoretical guarantees for the robustness of model predictions with the removal of arbitrary feature subsets, under both input and model perturbations. (2) We analyze the robustness of techniques for summarizing each feature's influence (e.g., Shapley values) and derive best- and worst-case robustness within several classes of such techniques. (3) We combine the analyses above to prove unified attribution robustness properties to both input and model perturbations, where changes in removal-based attributions are controlled by the scale of perturbations in either input space or function space. (4) We validate our theoretical results and demonstrate practical implications using synthetic and real-world datasets.

## 2 Background

Here, we introduce the notation used in the paper and review removal-based feature attributions.

### Notation

Let \(f:^{d}\) be a model whose predictions we seek to explain. We consider that the input variable \(\) consists of \(d\) separate features, or \(=(_{1},,_{d})\). Given an index set \(S[d]\{1,,d\}\), we define the corresponding feature subset as \(_{S}\{_{i}:i S\}\). The power set of \([d]\) is denoted by \((d)\), and we let \([] S\) denote the set complement. We consider the model's output space to be one-dimensional, which is not restrictive because attributions are typically calculated for scalar predictions (e.g., the probability for a given class). We use the bold symbols \(,_{S}\) to denote random variables, the symbols \(x,x_{S}\) to denote specific values, and \(p()\) to represent the data distribution with support on \(^{d}\). We assume that all explanations are generated for inputs such that \(x\)

### Removal-based feature attributions

Our work focuses on a class of methods known as _removal-based explanations_. Intuitively, these are algorithms that remove subsets of inputs and summarize how each feature affects the model. This framework describes a large number of methods that are distinguished by two main implementation choices: (1) how feature information is removed from the model, and (2) how the algorithm summarizes each feature's influence.2 This perspective shows a close connection between methods like leave-one-out , LIME  and Shapley values , and enables a unified analysis of their robustness properties. Below, we describe the two main implementation choices in detail.

**Feature removal.** Most machine learning models require all feature values to generate predictions, so we must specify a convention for depriving the model of feature information. We denote the prediction given partial inputs by \(f(x_{S})\). Many implementations have been discussed in the literature , but we focus here on three common choices. Given a set of observed values \(x_{S}\), these techniques can all be viewed as averaging the prediction over a distribution \(q(_{})\) for the held-out feature values:

\[f(x_{S}):=_{q(_{})}[f(x_{S},_{}) ]= f(x_{S},x_{})q(x_{})dx_{}.\] (1)

Specifically, the three choices we consider are:

* (Baseline values) Given a baseline input \(b^{d}\), we can set the held-out features to their corresponding values \(b_{}\). This is equivalent to letting \(q(_{})\) be a Dirac delta centered at \(b_{}\).
* (Marginal distribution) Rather than using a single replacement value, we can average across values sampled from the input's marginal distribution, or let \(q(_{})=p(_{})\).
* (Conditional distribution) Finally, we can average across replacement values while conditioning on the available features, which is equivalent to setting \(q(_{})=p(_{S} x_{S})\).

The first two options are commonly implemented in practice because they are simple to estimate, but the third choice is viewed by some work as more informative to users . The conditional distribution approach requires more complex and error-prone estimation procedures (see  for a discussion), but we assume here that all three versions of \(f(x_{S})\) can be calculated exactly.

**Summary technique.** Given a feature removal technique that allows us to query the model with arbitrary feature sets, we must define a convention for summarizing each feature's influence. This is challenging due to the exponential number of feature sets, or because \(|(d)|=2^{d}\). Again, many techniques have been discussed in the literature , with the simplest option comparing the prediction with all features included and with a single feature missing . We take a broad perspective here, considering a range of approaches that define attributions as a linear combination of predictions with different feature sets. These methods include leave-one-out , RISE , LIME , Shapley values , and Banzhaf values , among other possible options.

All of these methods yield per-feature attribution scores \(_{i}(f,x)\) for \(i[d]\). For example, the Shapley value calculates feature attribution scores as follows :

\[_{i}(f,x)=_{S[d]\{i\}}^ {-1}(f(x_{S\{i\}})-f(x_{S})).\] (2)

The specific linear combinations for the other approaches we consider are shown in Table 3. The remainder of the paper uses the notation \(f(x_{S})\) to refer to predictions with partial information, and \((f,x)=[_{1}(f,x),,_{d}(f,x)]^{d}\) to refer to feature attributions. We do not introduce separate notation to distinguish between implementation choices, opting instead to make the relevant choices clear in each result.

### Problem formulation

The problem formulation in this work is straightforward: our goal is to understand the stability of feature attributions under input perturbations and model perturbations. Formally, we aim to study1. whether \(\|(f,x)-(f,x^{})\|\) is controlled by \(\|x-x^{}\|\) (**input perturbation**), and
2. whether \(\|(f,x)-(f^{},x)\|\) is controlled by \(\|f-f^{}\|\) (**model perturbation**).

The following sections show how this can be guaranteed using certain distance metrics, and under certain assumptions about the model and/or the data distribution.

## 3 Preliminary results

As an intermediate step towards understanding explanation robustness, we first provide results for sub-components of the algorithms. We begin by addressing the robustness of predictions with partial information to input and model perturbations, and we then discuss robustness properties of the summary technique. Proofs for all results are in the Appendix.

Before proceeding, we introduce two assumptions that are necessary for our analysis.

**Assumption 1**.: _We assume that the model \(f\) is globally \(L\)-Lipschitz continuous, or that we have \(|f(x)-f(x^{})| L\|x-x^{}\|_{2}\) for all \(x,x^{}^{d}\)._

**Assumption 2**.: _We assume that the model \(f\) has bounded predictions, or that there exists a constant \(B\) such that \(|f(x)| B\) for all \(x^{d}\)._

The first assumption holds for most deep learning architectures, because they typically compose a series of Lipschitz continuous layers . The second assumption holds with \(B=1\) for classification models. These are therefore mild assumptions, but they are discussed further in Section 6.

We also define the notion of a functional norm, which is useful for several of our results.

**Definition 1**.: _The \(L^{p}\) norm for a function \(g:^{d}\) is defined as \(\|g\|_{p}(|g(x)|^{p}dx)^{1/p}\), where the integral is taken over \(^{d}\). \(\|g\|_{p,^{}}\) denotes the same integral taken over the domain \(^{}^{d}\)._

Our results adopt this to define a notion of distance between functions \(\|g-g^{}\|_{p}\), focusing on the cases where \(p=1\) for the \(L^{1}\) distance and \(p=\) for the Chebyshev distance.

### Prediction robustness to input perturbations

Our goal here is to understand how the prediction function \(f(x_{S})\) for a feature set \(x_{S}\) behaves under small input perturbations. We first consider the case where features are removed using the baseline or marginal approach, and we find that Lipschitz continuity is inherited from the original model.

**Lemma 1**.: _When removing features using either the **baseline** or **marginal** approaches, the prediction function \(f(x_{S})\) for any feature set \(x_{S}\) is \(L\)-Lipschitz continuous:_

\[|f(x_{S})-f(x^{}_{S})| L\|x_{S}-x^{}_{S}\|_{2} \;x_{S},x^{}_{S}^{|S|}.\]

Next, we consider the case where features are removed using the conditional distribution approach. We show that the continuity of \(f(x_{S})\) depends not only on the original model, but also on the similarity of the conditional distribution for the held-out features.

**Lemma 2**.: _When removing features using the **conditional** approach, the prediction function \(f(x_{S})\) for a feature set \(x_{S}\) satisfies_

\[|f(x_{S})-f(x^{}_{S})| L\|x_{S}-x^{}_{S}\|_{2}+2B d _{TV}p(_{S} x_{S}),p(_{S} x^{}_{S}),\]

_where the total variation distance is defined via the \(L^{1}\) functional distance as_

\[d_{TV}p(_{} x_{S}),p(_{} x^{}_{ S})p(_{} x_{S})-p(_{ } x^{}_{S})_{1}.\]

Lemma 2 does not immediately imply Lipschitz continuity for \(f(x_{S})\), because we have not bounded the total variation distance in terms of \(\|x_{S}-x^{}_{S}\|_{2}\). To address this, we require the following Lipschitz-like continuity property in the total variation distance.

**Assumption 3**.: _We assume that there exists a constant \(M\) such that for all \(S[d]\), we have_

\[d_{TV}p(_{S} x_{S}),p(_{S} x^{}_{S})  M\|x_{S}-x^{}_{S}\|_{2} x_{S},x^{}_{S} ^{|S|}.\]Intuitively, this says that the conditional distribution \(p(_{S} x_{S})\) cannot change too quickly as a function of the observed features. This property does not hold for all data distributions \(p()\), and it may hold in some cases only for large \(M\); in such scenarios, the function \(f(x_{S})\) is not guaranteed to change slowly. However, there are cases where it holds. For example, we have \(M=0\) if the features are independent, and the following example shows a case where it holds with dependent features.

**Example 1**.: _For a Gaussian random variable \((,)\) with mean \(^{d}\) and covariance \(^{d d}\), Assumption 3 holds with \(M=(^{-1})-_{}(^{-1})}\). If \(\) is assumed to be standardized, this captures the case where independent features yield \(M=0\). Intuitively, it also means that if one dimension is roughly a linear combination of the others, or if \(_{}(^{-1})\) is large, the conditional distribution can change quickly as a function of the conditioning variables._

Now, assuming that this property holds for \(p()\), we show that we can improve upon Lemma 2.

**Lemma 3**.: _Under Assumption 3, the prediction function \(f(x_{S})\) defined using the **conditional** approach is Lipschitz continuous with constant \(L+2BM\) for any feature set \(x_{S}\)._

Between Lemmas 1 and 3, we have established that the function \(f(x_{S})\) remains Lipschitz continuous for any feature set \(x_{S}\), although in some cases with a larger constant that depends on the data distribution. These results are summarized in Table 1. In Appendix B we show that our analysis can be extended to account for sampling in eq. (1) when the expectation is not calculated exactly.

### Prediction robustness to model perturbations

Our next goal is to understand how the function \(f(x_{S})\) for a feature set \(x_{S}\) behaves under small changes to the model. The intuition is that if two models make very similar predictions with all features, they should continue to make similar predictions with a subset of features. We first derive a general result involving the proximity between two models \(f\) and \(f^{}\) within a subdomain.

**Lemma 4**.: _For two models \(f,f^{}:^{d}\) and a subdomain \(^{}^{d}\), the prediction functions \(f(x_{S}),f^{}(x_{S})\) for any feature set \(x_{S}\) satisfy_

\[|f(x_{S})-f^{}(x_{S})|\|f-f^{}\|_{,^{} } Q_{x_{S}}(^{})+2B1-Q_{x_{S}}(^ {}),\]

_where \(Q_{x_{S}}(^{})\) is the probability of imputed samples lying in \(^{}\) based on the distribution \(q(_{})\):_

\[Q_{x_{S}}(^{})_{q(_{})}[ \{(x_{S},_{})^{}\} ].\]

The difference in predictions therefore depends on the distance between the models only within the subdomain \(^{}\), as well as the likelihood of imputed sampled lying in this subdomain. This result illustrates a point shown by Slack et al. : that two models which are equivalent on a small subdomain, or even on the entire data manifold \(\), can lead to different attributions if we use a removal technique that yields a low value for \(Q_{x_{S}}(^{})\). In fact, when \(Q_{x_{S}}(^{}) 0\), Lemma 4 reduces to a trivial bound \(|f(x_{S})-f^{}(x_{S})| 2B\) that follows directly from Assumption 2.

The general result in Lemma 4 allows us to show two simpler ones. In both cases, we choose the subdomain \(^{}\) and removal technique \(q(_{})\) so that \(Q_{x_{S}}(^{})=1\).

**Lemma 5**.: _When removing features using the **conditional** approach, the prediction functions \(f(x_{S}),f^{}(x_{S})\) for two models \(f,f^{}\) and any feature set \(x_{S}\) satisfy_

\[|f(x_{S})-f^{}(x_{S})|\|f-f^{}\|_{,}.\]

The next result is similar but involves a potentially larger upper bound \(\|f-f^{}\|_{}\|f-f^{}\|_{,}\).

**Lemma 6**.: _When removing features using the **baseline** or **marginal** approach, the prediction functions \(f(x_{S}),f^{}(x_{S})\) for two models \(f,f^{}\) and any feature set \(x_{S}\) satisfy_

\[|f(x_{S})-f^{}(x_{S})|\|f-f^{}\|_{}.\]

   Baseline & Marginal & Conditional \\  \(L\) & \(L\) & \(L+2BM\) \\   

Table 1: Lipschitz constants induced by each feature removal technique.

   Baseline & Marginal & Conditional \\  \(\|f-f^{}\|_{}\) & \(\|f-f^{}\|_{}\) & \(\|f-f^{}\|_{,}\) \\   

Table 2: Relevant functional distances for each feature removal technique.

The relevant functional distances for Lemma 5 and Lemma 6 are summarized in Table 2.

**Remark 1**.: Lemma 6 implies that if two models \(f,f^{}\) are functionally equivalent, or \(f(x)=f(x^{})\) for all \(x^{d}\), they remain equivalent with any feature subset \(x_{S}\). In Section 4, we will see that this implies equal attributions for the two models--a natural property that, perhaps surprisingly, is not satisfied by all feature attribution methods [47; 55], and that has been described in the literature as _implementation invariance_. This property holds automatically for all removal-based methods.

**Remark 2**.: In contrast, Lemma 5 implies the same for models that are equivalent _only on the data manifold_\(^{d}\). This is a less stringent requirement to guarantee equal attributions, and it is perhaps reasonable that models with equal predictions for all realistic inputs receive equal attributions. To emphasize this, we propose distinguishing between a notion of _weak implementation invariance_ and _strong implementation invariance_, depending on whether equal attributions are guaranteed when we have \(f(x)=f^{}(x)\) everywhere (\(x^{d}\)) or almost everywhere (\(x\)).

### Summary technique robustness

We previously focused on the effects of the feature removal choice, so our goal is now to understand the role of the summary technique. Given a removal approach that lets us query the model with any feature set, the summary generates attribution scores \((f,x)^{d}\), often using a linear combination of the outputs \(f(x_{S})\) for each \(S[d]\). We formalize this in the following proposition.

**Proposition 1**.: _The attributions for each method can be calculated by applying a linear operator \(A^{d 2^{d}}\) to a vector \(v^{2^{d}}\) representing the predictions with each feature set, or_

\[(f,x)=Av,\]

_where the linear operator \(A\) for each method is listed in Table 3, and \(v\) is defined as \(v_{S}=f(x_{S})\) for each \(S[d]\) based on the chosen feature removal technique._

In this notation, the entries of \(v\) are indexed as \(v_{S}\) for \(S[d]\), and the entries of \(A\) are indexed as \(A_{iS}\) for \(i[d]\) and \(S[d]\). The operation \(Av\) can be understood as summing across all subsets, or \((Av)_{i}=_{S[d]}A_{iS} v_{S}\). Representing the attributions this way is convenient for our next results.

The entries for the linear operator in Table 3 are straightforward for the first four methods, but LIME depends on several implementation choices: these include the choice of weighting kernel, regularization, and intercept term. The first three methods are in fact special cases of LIME . The class of weighting kernel used in practice is difficult to characterize analytically, but its default parameters for tabular, image and text data approach limiting cases where LIME reduces to other methods (Appendix H). For simplicity, the remainder of this section focuses on the other methods.

Perturbing either the input or model induces a change in \(v^{2^{d}}\), and we must consider how the attributions differ between the original vector \(v\) and a perturbed version \(v^{}\). Proposition 1 suggests that we can bound the attribuion difference via the change in model outputs \(\|v-v^{}\|\) and properties of the matrix \(A\). We can even do this under different distance metrics, as we show in the next result.

**Lemma 7**.: _The difference in attributions given the same summary technique \(A\) and different model outputs \(v,v^{}\) can be bounded as_

\[\|Av-Av^{}\|_{2}\|A\|_{2}\|v-v^{}\|_{2} \|Av-Av^{}\|_{2}\|A\|_{1,}\|v-v^{}\|_{ },\]

_where \(\|A\|_{2}\) is the spectral norm, and the operator norm \(\|A\|_{1,}\) is the square root of the sum of squared row 1-norms, with values for each \(A\) given in Table 3._

   Summary & Method & \(A_{iS}\) (\(i S\)) & \(A_{iS}\) (\(i S\)) & \(\|A\|_{1,}\) & \(\|A\|_{2}\) \\  Leave-one-out & Occlusion  & \(\{|S|=d\}\) & \(-\{|S|=d-1\}\) & \(2\) & \(\) \\ Shapley value & SHAP  & \(}{d^{2}}\) & \(-}{d^{2}}\) & \(2\) & \(\) \\ Banzhaf value & Banzhaf  & \(1/2^{d-1}\) & \(-1/2^{d-1}\) & \(2\) & \(1/2^{d/2-1}\) \\ Mean when included & RISE  & \(1/2^{d-1}\) & 0 & \(\) & \(}\) \\ Weighted least squares & LIME  &  \\   

Table 3: Summary technique linear operators used by various removal-based explanations.

For both inequalities, smaller norms \(\|A\|\) represent stronger robustness to perturbations. Neither bound is necessarily tighter, and the choice of which to use depends on how \(v-v^{}\) is bounded. The first bound using the Euclidean distance \(\|v-v^{}\|_{2}\) is perhaps more intuitive, but the second bound is more useful here because the results in Sections 3.1 and 3.2 effectively relate to \(\|v-v^{}\|_{}\).

This view of the summary technique's robustness raises a natural question, which is whether the approaches used in practice have relatively good or bad robustness (Table 3). The answer is not immediately clear, because within the class of linear operators we can control the robustness arbitrarily: we can achieve \(\|A\|_{2}=\|A\|_{1,}=0\) by setting \(A=0\) (strong robustness), and we can likewise get \(\|A\|_{2},\|A\|_{1,}\) by setting entries of \(A\) to a large value (weak robustness). These results are not useful, however, because neither limiting case results in meaningful attributions.

Rather than considering robustness within the class of _all_ linear operators \(A^{d 2^{d}}\), we therefore restrict our attention to attributions that are in some sense meaningful. There are multiple ways to define this, but we consider solutions that satisfy one or more of the following properties, which are motivated by axioms from the literature on game-theoretic credit allocation .

**Definition 2**.: _We define the following properties for linear operators \(A^{d 2^{d}}\) that are used for removal-based explanations:_

* _(Boundedness) For all_ \(v^{2^{d}}\)_, the attributions are bounded by each feature's smallest and largest contributions, or_ \(_{S^{d}}(v_{S\{i\}}-v_{S})(Av)_{i}_{S^{d}}(v_{S\{i\}} -v_{S})\) _for all_ \(i[d]\)_._
* _(Symmetry) For all_ \(v^{2^{d}}\)_, two features that make equal marginal contributions to all feature sets must have equal attributions, or_ \((Av)_{i}=(Av)_{j}\) _if_ \(v_{S\{i\}}=v_{S\{j\}}\) _for all_ \(S[d]\)_._
* _(Efficiency) For all_ \(v^{2^{d}}\)_, the attributions sum to the difference in predictions for the complete and empty sets, or_ \(^{}Av=v_{[d]}-v_{\{\}}\)_._

Among these properties, we prioritize boundedness because this leads to a class of solutions that are well-known in game theory, and which are called _probabilistic values_. We now show that constraining the summary technique to satisfy different combinations of these properties yields clear bounds for the robustness. We first address the operator norm \(\|A\|_{1,}\), which is the simpler case.

**Lemma 8**.: _When the linear operator \(A\) satisfies the **boundedness** property, we have \(\|A\|_{1,}=2\)._

The above result applies to several summary techniques shown in Table 3, including leave-one-out , Shapley values  and Banzhaf values . We now address the case of the spectral norm \(\|A\|_{2}\), beginning with the case where boundedness and symmetry are satisfied.

**Lemma 9**.: _When the linear operator \(A\) satisfies the **boundedness** and **symmetry** properties, the spectral norm is bounded as follows,_

\[}\|A\|_{2},\]

_with \(1/2^{d/2-1}\) achieved by the Banzhaf value and \(\) achieved by leave-one-out._

This shows that the Banzhaf value is the most robust summary within a class of linear operators, which are known as _semivalues_. Its robustness is even better than the Shapley value, a result that was recently discussed by Wang and Jia . However, the Banzhaf value is criticized for failing to satisfy the efficiency property , so we now address the case where this property holds.

**Lemma 10**.: _When the linear operator \(A\) satisfies the **boundedness** and **efficiency** properties, the spectral norm is bounded as follows,_

\[}\|A\|_{2} )},\]

_with \(\) achieved by the Shapley value._

We therefore observe that the Shapley value is the most robust choice within a different class of linear operators, which are known as _random-order values_. In comparison, the worst-case robustness is achieved by a method that has not been discussed in the literature, but which is a special case of a previously proposed method . The upper bound in Lemma 10 is approximately equal to \(2\), whereas the upper bound in Lemma 9 grows with the input dimensionality, suggesting that the efficiency property imposes a minimum level of robustness but limits robustness in the best case.

Finally, we can trivially say that \(\|A\|_{2}=\) when boundedness, symmetry and efficiency all hold, because the Shapley value is the only linear operator to satisfy all three properties [46; 54].

**Lemma 11**.: _When the linear operator \(A\) satisfies the **boundedness**, **symmetry** and **efficiency** properties, the spectral norm is \(\|A\|_{2}=\)._

## 4 Feature attribution robustness: main results

We now shift our attention to the robustness properties of entire feature attribution algorithms. These results build on those presented in Section 3, combining them to provide general results that apply to existing methods, and to new methods that combine their implementation choices arbitrarily.

Our first main result relates to the robustness to input perturbations.

**Theorem 1**.: _The robustness of removal-based explanations to input perturbations is given by the following meta-formula,_

\[\|(f,x)-(f,x^{})\|_{2} g(}) h( })\|x-x^{}\|_{2},\]

_where the factors for each method are defined as follows:_

\[g(}) =L&}=\\ L+2BM&}=,\] \[h(}) =2&}=,,\\ &}=.\]

We therefore observe that the explanation functions themselves are Lipschitz continuous, but with a constant that combines properties of the original model with implementation choices of the attribution method. The model's inherent robustness interacts with the removal choice to yield the \(g(})\) factor, and the summary technique is accounted for by \(h(})\).

Our second result takes a similar form, but relates to the robustness to model perturbations.

**Theorem 2**.: _The robustness of removal-based explanations to model perturbations is given by the following meta-formula,_

\[\|(f,x)-(f^{},x)\|_{2} h(})\|f-f^{ }\|,\]

_where the functional distance and factor associated with the summary technique are specified as follows:_

\[\|f-f^{}\| =\|f-f^{}\|_{}&}=\\ \|f-f^{}\|_{,}&}=,\] \[h(}) =2&}=,,\\ &}=.\]

Similarly, we see here that the attribution difference is determined by the strength of the model perturbation, as measured by a functional distance metric that depends on the feature removal technique. This represents a form of Lipschitz continuity with respect to the model \(f\). These results address the case with either an input perturbation or model perturbation, but we can also account for simultaneous input and model perturbations, as shown in Corollary 1 in Appendix E. The remainder of the paper empirically verifies these results and discusses their practical implications.

## 5 Summary of experiments

Due to space constraints, we defer our experiments to Appendix A but briefly describe the results here. First, we empirically validate our results using synthetic data and a logistic regression classifier, where the model's Lipschitz constant and the upper bound for the total variation constant \(M\) can be readily computed. This allows us to verify the robustness bound under input perturbations (Theorem 1). Using the same setup, we construct logistic regression classifiers where the functional distance on the data manifold is small, and we show that feature removal with the conditional distribution is indeed more robust to model perturbations than baseline or marginal removal, as implied by Theorem 2.

Next, we demonstrate two practical implications of our findings with the UCI wine quality , MNIST , CIFAR-10  and Imagenette datasets . (1) Removal-based attributions are more robust under input perturbations for networks trained with weight decay, which encourages a smaller Lipschitz constant. Formally, this is because a network's Lipschitz constant is upper bounded by the product of its layers' spectral norms , which are respectively upper bounded by each layer's Frobenius norm. These results are shown for Shapley value attributions in Figure 1 for the wine quality and MNIST datasets, where we see a consistent effect of weight decay across feature removal approaches. We speculate that a similar effect may be observed with other techniques that encourage

Figure 1: Shapley attribution difference for networks trained with increasing weight decay, under input perturbations with varying perturbation norms. The results include (a) the wine quality dataset with FCNs and baseline, marginal, and conditional feature removal; and (b) MNIST with CNNs and baseline feature removal with either training set means or zeros. Error bars show the mean and \(95\%\) confidence intervals across explicand-perturbation pairs.

Figure 2: Sanity checks for attributions using cascading randomization for the FCN trained on the wine quality dataset. Attribution similarity is measured by Pearson correlation and Spearman rank correlation. We show the mean and \(95\%\) confidence intervals across \(10\) random seeds.

Lipschitz regularity [8; 30; 63]. (2) Viewing the sanity check with parameter randomization proposed by Adebayo et al.  as a form of model perturbation, we show that removal-based attributions empirically pass the sanity check, in contrast to certain gradient-based methods [55; 60]. Figure 2 shows results for the wine quality dataset, where the similarity for removal-based methods decays more rapidly than for IntGrad and Grad \(\) Input. Results for the remaining datasets are in Appendix A.

## 6 Discussion

Our analysis focused on the robustness properties of removal-based feature attributions to perturbations in both the model \(f\) and input \(\). For input perturbations, Theorem 1 shows that the attribution change is proportional to the input perturbation strength. The Lipchitz constant is determined by the attribution's removal and summary technique, as well as the model's inherent robustness. We find that both implementation choices play an important role, but our analysis in Section 3 reveals that the associated factors in our main result can only be so small for meaningful attributions, implying that attribution robustness in some sense reduces to model robustness. Fortunately, there are many existing works on improving a neural network's Lipschitz regularity [8; 30; 63].

For model perturbations, Theorem 2 shows that the attribution difference is proportional to functional distance between the perturbed and original models, as measured by the Chebyshev distance (or infinity norm). Depending on the removal approach, the distance may depend on only a subdomain of the input space. One practical implication is that attributions should remain similar under minimal model changes, e.g., accurate model distillation or quantization. Another implication is that bringing the model closer to its correct form (e.g., the Bayes classifier) results in attributions closer to those of the correct model; several recent works have demonstrated this by exploring ensembled attributions [23; 31; 37; 48], which are in some cases mathematically equivalent to attributions of an ensembled model.

Our robustness results provide a new lens to compare different removal-based attribution methods. In terms of the summary technique, our main proof technique relies on the associated operator norm, which does not distinguish between several approaches (Shapley, leave-one-out, Banzhaf); however, our results for the spectral norm suggest that the Banzhaf value is in some sense most robust. This result echoes recent work  and has implications for other uses of game-theoretic credit allocation, but we also find that the Shapley value is most robust within a specific class of summary techniques. Our findings regarding the removal approach are more complex: the conditional distribution approach has been praised by some works for being more informative than baseline or marginal [17; 27], but this comes at a cost of _worse_ robustness to input perturbations. On the other hand, it leads to _improved_ robustness to model perturbations, which has implications for fooling attributions with imperceptible off-manifold model changes .

Next, we make connections between current understanding of gradient-based methods and our findings for removal-based methods. Overall, the notion of Lipschitz continuity is important for robustness guarantees under input perturbations. Our results for removal-based attributions rely on Lipschitz continuity of the model itself, whereas gradient-based attributions rely on Lipschitz continuity of the model's _gradient_ (i.e., Lipschitz smoothness) . Practically, both are computationally intensive to determine for realistic networks [30; 63]. Empirically, weight decay is an effective approach for generating robust attributions, for both removal-based methods (our results) and gradient-based methods (see Dombrowski et al. ), as the Frobenius norms of a neural network's weights upper bound both the network's Lipschitz constant and Hessian norm. Under model perturbations, removal-based attributions are more robust when features are removed such that \(_{S}\) stay on the data manifold, whereas gradient-based attributions are more robust when gradients are projected on the tangent space of the data manifold .

Finally, we consider limitations of our analysis. Future work may consider bounding attribution differences via different distance measures (e.g., \(_{p}\) norms for \(p 2\)). The main limitation to our results is that they are conservative. By relying on global properties of the model, i.e., the Lipschitz constant and global prediction bounds, we arrive at worst-case bounds that are in many cases overly conservative, as reflected in our experiments. An alternative approach can focus on localized versions of our bounds. Analogous to recent work on certified robustness [38; 39; 52; 57; 62], tighter bounds may be achievable with specific perturbation strengths, unlike our approach that provides bounds simultaneously for any perturbation strength via global continuity properties.