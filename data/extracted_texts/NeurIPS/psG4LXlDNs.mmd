# Achieving \(\tilde{O}(1/\varepsilon)\) Sample Complexity for Constrained Markov Decision Process

# Achieving \((1/)\) Sample Complexity for Constrained Markov Decision Process

Jiashuo Jiang

Department of Industrial Engineering & Decision Analytics

Hong Kong University of Science and Technology

Hong Kong, China

jsjiang@ust.hk

&Yinyu Ye

Department of Management Science & Engineering

Institue of Computational Mathematics and Engineering

Stanford University

California, US

yyye@stanford.edu

###### Abstract

We consider the reinforcement learning problem for the constrained Markov decision process (CMDP), which plays a central role in satisfying safety or resource constraints in sequential learning and decision-making. In this problem, we are given finite resources and a MDP with unknown transition probabilities. At each stage, we take an action, collecting a reward and consuming some resources, all assumed to be unknown and need to be learned over time. In this work, we take the first step towards deriving optimal problem-dependent guarantees for the CMDP problems. We derive a logarithmic regret bound, which translates into a \(O(^{2}(1/))\) sample complexity bound, with \(\) being a problem-dependent parameter, yet independent of \(\). Our sample complexity bound improves upon the state-of-art \(O(1/^{2})\) sample complexity for CMDP problems established in the previous literature, in terms of the dependency on \(\). To achieve this advance, we develop a new framework for analyzing CMDP problems. To be specific, our algorithm operates in the primal space and we resolve the primal LP for the CMDP problem at each period in an online manner, with _adaptive_ remaining resource capacities. The key elements of our algorithm are: i) a characterization of the instance hardness via LP basis, ii) an eliminating procedure that identifies one optimal basis of the primal LP, and; iii) a resolving procedure that is adaptive to the remaining resources and sticks to the characterized optimal basis.

## 1 Introduction

Reinforcement learning (RL) is pivotal in the realm of dynamic decision-making under uncertainty, where the objective is to maximize total reward through ongoing interaction with and learning from an enigmatic environment. Markov Decision Processes (MDPs) are a prevalent framework for encapsulating environmental dynamics. MDPs have been instrumental in various domains, such as video gaming , robotics , recommender systems , inventory control , and beyond. Yet, they fall short in accommodating additional constraints that may influence the formulation of the optimal policy and the decision-maker's engagement with the uncertain environment. Often, in MDP applications, there are stringent constraints on utilities or costs, emanating from areas like safeautonomous driving , robotics , revenue management  and financial management . These constraints might also symbolize limitations on resources within resource allocation applications. Constrained MDPs (CMDPs), as introduced in , enhance MDPs to factor in constraints affecting long-term policy results. In CMDPs, the decision-maker aims to optimize cumulative rewards while adhering to these constraints. Our paper focuses on CMDPs, and we aim to develop efficient algorithmic solutions.

The significance of RL in CMDP contexts has garnered substantial attention in recent years. A variety of methods for tackling CMDPs have been developed, including the primal-dual technique [21; 49; 12; 69; 18; 45; 25; 26], which leverages the Lagrangian dual of CMDPs and implements an online learning strategy for the iterative update of dual variables. Other methods encompass constrained optimization [1; 61], the Lyapunov technique , among others. Previous research has established _minimax_ bounds for CMDPs, representing the optimal regret that can be achieved for the most challenging problem within a specific class of problems. Nonetheless, these minimax regret bounds and worst-case scenarios can be overly conservative, leading to a disconnect between theoretical guarantees and practical performance for specific problem instances. A more tailored approach is warranted--one that ensures great performance on every single problem instance and furnishes problem-dependent guarantees. Our research takes the first step towards deriving optimal problem-dependent guarantees for CMDP problems.

### Preliminaries

We consider a CMDP problem with a finite set of states \(=\{1,2,,||\}\) and a finite set of actions \(=\{1,2,,||\}\). We denote by \((0,1)\) a discount factor. We also denote by \(P:()\) the probability transition kernel of the CMDP, where \(()\) denotes a probability measure over the state space \(\). Then, \(P(s^{}|s,a)\) denotes the probability of transiting from state \(s\) to state \(s^{}\) when the action \(a\) is executed. The initial distribution over the states of the CMDP is denoted by \(_{1}\).

There is a _stochaastic_ reward function \(r:\) and \(K\)_stochastic_ cost functions \(c_{k}:\) for each \(k[K]\). We also denote by \((s,a)=[r(s,a)]\) for each \((s,a)\) and \((s,a)=[c_{k}(s,a)]\) for each \((s,a)\). For any Markovian policy \(\), where the action of \(\) depends only on the current state and the action of \(\) is allowed to be randomized, we denote by \(V_{r}(,_{1})\) the infinite horizon discounted reward of the policy \(\), with the formulation of \(V_{r}(,_{1})\) given below:

\[V_{r}(,_{0})=[_{t=0}^{}^{t} r(s_{t}, a_{t})_{1}],\] (1)

where \((s_{t},a_{t})\) is generated according to the policy \(\) and the transition kernel \(P\) with the initial state distribution \(_{1}\). For each \(k[K]\), the infinite horizon discounted cost of the policy \(\) is denoted by \(V_{k}(,_{1})\), and the following constraint needs to be satisfied by the policy \(\),

\[V_{k}(,_{1})=[_{t=0}^{}^{t} c_{k}(s_ {t},a_{t})_{1}]_{k},\ \  k[K].\] (2)

To solve the CMDP problem, we aim to find an optimal Markovian policy, denoted by \(^{*}\), that maximizes the reward in (1) while satisfying the cost constraint (2) for each \(k[K]\), with \(_{k}[0,]\) being a pre-specified value for each \(k[K]\). Importantly, we assume that the reward function \(r\), the cost functions \(\{c_{k}\}_{k=1}^{K}\), and the transition kernel \(P\), are all **unknown** to the decision maker. Our goal is to obtain a policy \(\) that approximates the optimal policy \(^{*}\) with as few samples as possible. We now describe the sampling procedure and present the performance measure of our policy. We assume the existence of a stylized _generative model_\(\), as studied in [39; 38; 44]. The model \(\) satisfies the following condition.

**Assumption 1.1**: _For each state and action pair \((s,a)\), we can query the model \(\) to obtain an observation of \(r(s,a)\), \(c_{k}(s,a)\) for each \(k[K]\), and the new state \(s^{}\), where the transition from \(s\) to \(s^{}\) follows the probability kernel \(P(s^{}|s,a)\) independently._

Note that in reinforcement learning for CMDP problems, querying the generative model \(\) can be costly. Therefore, it is desirable to query the model \(\) as less as possible, while guaranteeing the near optimality of the approximate policy. To this end, we use _sample complexity_ as the measure of the performance of our policy. For any \(\), we aim to find an \(\)-accurate policy \(\) such that

\[V_{r}(^{*},_{1})-V_{r}(,_{1})\;\;\;\;V_{k}( ,_{1})-_{k},\; k[K],\] (3)

with as few samples as possible.

### Our Main Results and Contributions

The main result of our research is the introduction of a novel algorithm that promises a \(O(^{2}(1/))\) sample complexity bound, where \(\) is a positive constant that characterizes the gap between the optimal policy and the sub-optimal ones. Note that the state-of-the-art sample complexity bound under the worst-case scenarios is \(O(1/^{2})\), which has been established in a series of work . Though the \(O(1/)\) or better iteration complexity has been achieved in , it comes with a sample complexity bound no better than \(O(1/^{2})\). Our algorithm enjoys a sample complexity bound that has a better dependency in terms of \(\). To achieve this improved result, we develop several new elements listed below.

**Contribution 1**: we develop new characterizations of the problem instance hardness for CMDP problems. Note that a key component for achieving instance-dependent bounds is to characterize the "hardness" of the underlying problem instance. That is, we need to identify a positive gap to separate the optimal policy from the sub-optimal policies for a particular problem instance. The importance of identifying such a gap has been demonstrated in instance-optimal learning for multi-arm-bandit problems (e.g. ) and reinforcement learning problems (e.g. ), among others. For CMDP, identifying such a gap is non-trivial because the optimal policies for CMDP are randomized policies . Then, the policies can be represented by distributions over the action set and the sub-optimal policies can be arbitrarily close to the optimal policy as long as the corresponding sub-optimal distributions converge to the optimal one. To tackle this problem, we show that the feasible region for the policies can be represented as a polytope and we only need to focus on the corner points of this polytope to find an optimal policy. Therefore, the hardness can simply be characterized as the distance between the optimal corner point and the sub-optimal corner point, as illustrated in detail in Section 2.1. This is the first characterization of problem instance hardness for CMDP problems.

**Contribution 2**: we devise a new algorithmic framework to analyze CMDP problems, inspired by the online packing/linear programming (LP) literature . Specifically, we utilize a linear programming reformulation of the CMDP problem, where policies are delineated via _occupancy measures_. The optimal policy emerges from the LP's solution; however, the indeterminate model parameters mean the LP cannot be solved directly but must be approached online as we obtain more samples from the generative model. Each generative model query leads to solving an empirical LP with accrued samples, and our final policy is derived from averaging these solutions--an approach akin to the methodology in online LP. A critical feature of our algorithm is the adaptiveness of the LP constraints' right-hand side to the input samples, a technique demonstrated to achieve logarithmic regret in online LP literature, which we now apply to CMDP problems.

**Contribution 3**: we extend our contributions to the online LP literature. Note that after adopting the LP reformulation, the corner points of the feasible region for policies can be represented by the basis of the LP. Separating the optimal policy from the others is equivalent to identifying one optimal basis of the LP. We utilize an approach that lexicographically restricts one variable to zero and tests whether the LP value has changed. We show that this approach systematically pinpoints a particular optimal LP basis with a high probability. Then, we develop a resolving procedure that capitalizes on the structure of the identified optimal basis, which involves only the non-zero basic variables and the active constraints. This is a new approach of deriving problem-dependent bound for online LP.

**Other related literature.** Problem-dependent guarantees have been considered extensively in the RL literature, where a series of work  establishes the \((N)\) regret bound or the \(O(^{-1})\) sample complexity, with \(\) being a problem-dependent constant. Our approach can also be directly applied to the RL problems. Moreover, our approach can handle long-term constraints and can deal with multi-objective (safe) RL problems.

our work presents a new algorithm for safe RL problems. We adopt an occupancy measure representation of the optimal and obtain an LP to work with, which is similar to the previous work. However, our algorithm resolves an LP and operates in the primal space, which is fundamentally different from the previous work that adopts a primal-dual update (e.g. ). There is also work developing primal-based algorithms, for example (, , , , ). Our algorithm is completely different from the previous work and we obtain new results. The result is that we are able to obtain an instance-dependent \((1/)\) sample complexity the first time in the literature, which improves upon the \(O(1/^{2})\) worst-case sample complexity established in the previous work. Though the constrained optimization approach and the Lyapunov approach have also been developed for CMDP problems, they do not enjoy a theoretical guarantee. In comparison to the literature, we develop a new primal-based algorithm and achieve the first instance-dependent sample complexity for CMDP problems.

Our new primal-based algorithm is motivated from the literature of online linear programming [3; 40; 47] and bandits with knapsack problems (e.g. , ). In these problems, the optimal policy can be written as an LP and we need to develop an online policy to solve this LP sequentially. Note that a prevalent strategy is to resolve the LP adaptive to the remaining resources, which has been developed in a long line of research on various applications, for example , , , , ,  and . We make the innovation of resolving the LP while sticking to the identified optimal basis, which distinguishes our algorithm from the previous ones in online LP. Note that the online LP techniques has also been extended to handle non-stationarity, for example in , . It is an interesting future topic to explore whether our algorithm can be extended to non-stationary environment.

## 2 LP Reformulation

The infinite horizon discounted setting described in Section 1.1 admits a linear programming reformulation. To be specific, due to the existence of the constraints, the optimal policy of a CMDP can be randomized policies (e.g. ), where it is optimal to take a stochastic action given the current state. Therefore, it is convenient to represent a policy through the _occupation measure_, which gives us the desired linear programming reformulations of the CMDP problems.

For the infinite horizon discounted problem, the occupancy measure is defined as \(q_{}(s,a)\) for any state \(s\), action \(a\), and policy \(\). Note that \(q_{}(s,a)\) represents the total expected discounted time spent on the state-action pair \((s,a)\), under policy \(\), multiplied by \(1-\). Then, following , the optimal policy (and the optimal occupancy measure) can be obtained from the following linear programming.

\[V^{}= \ _{s}_{a}(s,a) q (s,a)\] (4a) \[\ _{s}_{a}_{k}(s,a)  q(s,a)_{k}  k[K]\] (4b) \[_{s^{}}_{a}q(s^{ },a)(_{s,s^{}}- P(s|s^{},a))=(1-) _{1}(s)  s\] (4c) \[q(s,a) 0  s,a,\] (4d)

where \(_{s,s^{}}=_{s=s^{}}\), and \(_{1}(s)\) denotes the probability for the first state to be realized as \(s\) following the initial distribution \(_{1}\). Note that for an optimal solution \(\{q^{*}(s,a)\}_{ s, a}\) to (4), the corresponding optimal policy \(^{*}\) will be

\[P(^{*}(s)=a)=(s,a)}{_{a^{}} q^{*}(s,a^{})},&_{a^{}}q^{*}(s,a^{})>0\\ 1/||,&_{a^{}}q^{*}(s,a^{})=0, \] (5)

where \(^{*}(s)\) denotes the probability for the policy \(^{*}\) to take the action \(a\) given the state \(s\). In fact, when \(_{a^{}}q^{*}(s,a^{})=0\), we can take an arbitrary action. In what follows, we rely on the linear programming formulation (4) to derive our results.

### Characterization of Instance Hardness

We now rewrite the LP (4) into the following standard formulation to proceed with our illustration.

\[V=&}^{}\\ &C\\ &B=\\ & 0.\] (6)

Note that one crucial step for achieving problem-dependent bounds is to characterize the hardness of the underlying problem instance and define a gap that separates the optimal policies from the others. For multi-arm-bandit problem, the characterization of the hardness can be the gap between the optimal arm and the best sub-optimal arm (e.g. ). For reinforcement learning problem, the characterization of the hardness can also be the gap between the optimal policy and the best sub-optimal policy (e.g. ). As long as this separation gap is a positive constant, denoted by \(\), separating the optimal policy from the others with a probability at least \(1-\) would require samples at most \((1/)\), which finally implies the instance-optimal sample complexity bound.

For CMDP problem, characterizing the hardness of the problem instance can be hard. Based on LP (6), we know that a feasible policy corresponds to a feasible solution and the sub-optimal solution can be arbitrarily close to the optimal solution since the feasible set is "continuous". Therefore, there is no direct way to identify a positive gap between the optimal policies and the sub-optimal ones. However, from standard LP theory, we know that one **corner point** of the feasible region must be one optimal solution. Therefore, we can simply focus on the corner points when solving LP (6) and we define the gap as _the distance between the optimal corner point and the sub-optimal corner point_, as illustrated in Figure 1. As we will show later, the LP reformulation (6) and such a characterization of hardness via corner points will inspire our entire approach.

Since the problem hardness is characterized via corner points, it is essential to provide further characterization of the corner points. Note that in LP theory, the corner point is called _basic solution_ and can be represented by _LP basis_, which involves the set of basic variables that are allowed to be non-zero, and the set of active constraints that are binding under the corresponding basic solution. Our next lemma follows from standard LP theory, where the proof is provided in the appendix for completeness.

**Lemma 2.1**: _Denote by \(b\) the number of rows in the matrix \(B\). Then, there exists a subset \(^{*}[K]\) with \(K^{}=|^{*}|\) and an optimal solution \(^{*}\) to LP (6) such that there are \(b+K^{}\) variables in \(^{*}\) are non-zero. Moreover, we denote by \(^{*}\) the index set of the non-zero element in \(^{*}\). Then, the optimal solution \(^{*}\) can be uniquely determined as the solution to the linear system_

\[C(^{*},^{*})_{^{*}}=_{^{*}},\] (7a) \[B(:,^{*})_{^{*}}=,\] (7b) \[_{^{*c}}=0.\] (7c)

_with \(^{*c}\) being the complementary set of the index set \(^{*}\)._

**Remark**. Note that the minimax lower bound \((1/^{2})\) has been established in previous work . However, this does not contradict with our \((1/)\) sample complexity after we introduce the instance

Figure 1: A graph illustration of the hardness characterization via LP basis, where the shaded area denotes the feasible region for the policies.

hardness measure \(\). To be specific, for a problem instance \(I\), we can denote by \(S(I,)\) the number of samples needed to construct an \(\)-optimal policy. Then the worst-case lower bound implies that \(_{I}S(I,)=(1/^{2})\). However, if we do not consider the worst-case guarantee, i.e., if we do not maximize over the problem instance \(I\), then we can characterize an instance-dependent constant \((I)\) (independent of \(\)) such that \(S(I,)=(I)/(1/)\). When the problem instance is favorable such that the constant \((I)\) is smaller than \(1/\), our bound strictly improves upon the worst-case bound.

**Overview of our approach**: in the first part, we aim to identify the optimal basis \(\) and \(\). In this way, we identify the optimal corner point to look at. The detailed procedure is described in Section 3. In the second part, we learn the optimal solution given the optimal basis we have identified, which finally gives us a near-optimal policy with the desired sample complexity bound. The detailed procedure is described in Section 4.

## 3 Construct Estimates and Identify Optimal Basis

We describe how to construct estimates for LP (6). To this end, for a round \(N_{0}\), we denote by \(_{N_{0}}\) the filtration of all the information collected up to round \(N_{0}\). Then, we denote by \(_{N_{0}}\) (resp. \(_{N_{0}}\)) an estimate of the matrix \(C\) (resp. \(B\)), constructed using the information in the set \(_{N_{0}}\). We also denote by \(}_{N_{0}}\) and estimate of \(}\) constructed from the information in the set \(_{N_{0}}\). To be specific, we define

\[_{N_{0}}(s,a)=^{N_{0}}r^{n}(s,a)}{N_{0}},_{k,N_{ 0}}(s,a)=^{N_{0}}c_{k}^{n}(s,a)}{N_{0}},_{N_{0}}(s^{}|s,a)=^{N_{0}}_{s^{n}(s,a)=s^{}}}{N_{0}},\] (8)

where \(r^{n}(s,a)\) denotes the \(n\)-th observation of the reward, and \(c_{k}^{n}(s,a)\) denotes the \(n\)-th observation of the \(k\)-th cost, and \(s^{n}(s,a)\) denotes the \(n\)-th observation of the state transition for the state-action pair \((s,a)\), for \(n[N_{0}]\). Then, similar to , we can use the following LP to obtain an estimate of \(V\) (6).

\[_{N_{0}}=  (}_{N_{0}})^{}\] s.t. \[_{N_{0}}+_{N_{0}}\] (9) \[_{N_{0}}+_{N_{0}}\] \[_{N_{0}}-_{N_{0}}\] \[ 0,\]

with \(_{N_{0}}\) being a parameter that we specify later. To bound the estimation gap between \(_{N_{0}}\) and \(V\), it is useful to bound the optimal dual solution to (6). To this end, we adopt the approach in [29; 55] that utilizes Slater's condition, which is imposed as an assumption below.

**Assumption 3.1**: _There exists a policy \(\) such that all the resource constraints are satisfied strictly. In other words, there exists an occupancy measure \(}\) such that \(B}=\) and \(C}<\). In fact, for each state \(s\), there exists a null action that consumes no resource._

The Slater point \(}\) can be set as the policy that takes the null action given each state. The estimation error will be related to the gap between the Slater point \(}\) and the optimal point \(^{*}\). We then define the lower gap as

\[_{1}(N_{0},) V-_{N_{0}}\] (10)

and the upper gap as

\[_{2}(N_{0},)_{N_{0}}-V\] (11)

with both inequalities (10) and (11) hold with probability at least \(1-\).

### Bound the Estimation Gap

We denote by \((N_{0},)=}}\). Following the standard Hoeffding's inequality, we know that \(|_{N_{0}}(s,a)-(s,a)|\), \(|_{k,N_{0}}(s,a)-_{k}(s,a)|\), and \(|_{N_{0}}(s^{}|s,a)-P(s^{}|s,a)|\) are all upper bounded by \((N_{0},)\) with probability at least \(1-\). We can simply set

\[_{1}(N_{0},)=(N_{0},)\] (12)\[_{2}(N_{0},)=(N_{0}, )}{_{k[K]}\{_{k}\}}(1+|}{1- })+^{2}(N_{0},)}{_{k[K]}\{ _{k}\}}(||+|^{2}}{1-} ).\] (13)

We have the following result, where the proof is relegated to appendix.

**Lemma 3.2**: _As long as \(_{N_{0}}=(N_{0},)\), the following inequality_

\[V_{N_{0}}+_{1}(N_{0},) V+ _{1}(N_{0},)+_{2}(N_{0},).\] (14)

_holds with probability at least \(1-(K||||-||^{2}||)\), where \(_{1}(N_{0},)\) is defined in (12) and \(_{2}(N_{0},)\) is defined in (13)._

### Characterize One Optimal Basis

We now describe how to identify one optimal basis of the LP (6) as required in Lemma 2.1, by sequentially discarding the sub-optimal actions and the redundant constraints. The formal algorithm to identify such non-zero elements and the constraints is given in Algorithm 1.

```
1:Input: the historical sample set \(_{N_{0}}\) that contains \(N_{0}\) samples for each \((s,a)\).
2:Compute the value of \(_{N_{0}}\) as in (9).
3:Initialize \(\) to be the whole index set that contains every column index of matrix \(B\) in (6) and \(=[K]\).
4:for\(i\)do
5: Let \(^{}=\{i\}\).
6: Compute the value of \(_{^{},N_{0}}\) as in (15).
7: If \(|_{^{},N_{0}}-_{N_{0}}| 2_{1}(N_{0}, )+2_{2}(N_{0},)\), then we set \(=^{}\).
8:endfor
9:for\(k=1,,K\)do
10: Let \(^{}=\{q\}\).
11: Compute the value of \(_{^{},,N_{0}}\) as in (18).
12: If \(|_{N_{0}}-_{^{},,N_{0}}| 2 _{1}(N_{0},)+2_{2}(N_{0},)\), then we set \(=^{}\).
13:endfor
14:Output: the set of indexes \(\) and \(\). ```

**Algorithm 1** Algorithm for identifying one optimal basis

We now explain the intuition why Algorithm 1 works. Denote by \(^{*}\) an optimal solution to LP (6). We describe how to identify the non-zero elements in \(^{*}\) and how to identify the constraints such that the values of the non-zero elements of \(^{*}\) can be uniquely determined by the corresponding linear equation. For each \(i\)-th element of \(^{*}\), we compare the value of \(V\) (6) against \(V\) with an additional constraint that \(q_{i}=0\). If the two values are different, we identify a non-zero element. To this end, for an index set \(\), we define an LP, as well as its estimate, as follows.

\[V_{}=&&}^{} &_{,N_{0}}=&&(_{N_{0}})^{} \\ &&C&&_{N_{0} }+_{N_{0}}\\ &&B=&&|_{N_{0}}- |_{N_{0}}\\ &&_{^{c}}=0&&_{^{c}}=0\\ && 0,& 0.\] (15)

where \(^{c}\) denotes the complementary set of \(\). Note that if \(V-V_{}>0\), we know that \(^{c}\) contains a non-zero basic variable. The steps 4-8 in Algorithm 1 reflect this point. Starting from \(\) denoting the whole index set, we sequentially delete one element \(i\) (denoting \((s,a)\) in the infinite horizon discounted problem) from the set \(\). Once we detected that \(V-V_{\{i\}}>0\), we know that \(i\) is a non-zero basic variable and we add \(i\) back into the set \(\). In this way, we can classify all the basic variables into the set \(\). However, since we do not know the exact value of \(V\) and \(V_{}\), we use the estimates and compare the value of \(_{N_{0}}\) and \(_{,N_{0}}\). For this comparison to be valid, the estimation error has to be smaller than the intrinsic gap between \(V\) and \(V_{}\). We define a constant

\[_{1}=_{}\{V-V_{}:V-V_{} >0\}\] (16)and we need \(N_{0}\) to be large enough such that the estimation gap is smaller than \(_{1}/2\).

To find the corresponding active constraints, we consider the dual program of \(V_{}\), where \(\) is determined in steps 4-8 in Algorithm 1, and similarly, we test which dual variable can be set to \(0\) without influencing the dual objective value. For a dual variable index subset \([K]\), we consider the dual program as follows.

\[_{,}= ^{}+^{ }\] (17a) s.t. \[(C(:,))^{}+(B(:,))^{ }}_{}\] (17b) \[_{^{c}}=0\] (17c) \[ 0,-,\] (17d)

where \(^{c}=[K]\). The estimate of \(_{,}\), can be obtained from the estimate of its dual, which is given below.

\[_{,,N_{0}}=_{,,N_{0}}= (}_{N_{0}})^{}\] (18a) s.t. \[_{N_{0}}(,:,:)+_{N_{0}}\] (18b) \[|_{N_{0}}-|_{N_{0}}\] (18c) \[_{}=0\] (18d) \[ 0,\] (18e)

Similarly, we compare the value of \(_{}\), where \(_{}=_{^{},}\) with \(^{}=[K]\), and \(_{,}\). However, we can only compare the value of their estimates \(_{,N_{0}}\) and \(_{,,N_{0}}\). To this end, we define a constant

\[_{2}=_{[K]}\{_{,}-_{}:_{,}-_{ }>0\}.\] (19)

For the comparison to be valid, we need \(N_{0}\) to be large enough such that the estimation error is smaller than \(_{2}/2\). In this way, we identify the linearly independent binding constraints corresponding to \(^{*}\), as described in steps 9-13 of Algorithm 1.

## 4 Our Final Algorithm

We now describe our formal algorithm. From the output of Algorithm 1, we characterize one optimal solution. If the sample size \(n\) is used in Algorithm 1, we denote by \(_{n}\) and \(_{n}\) the output of Algorithm 1. To be specific, we have \(_{_{n}}^{*}=0\) and the non-zero elements \(_{_{n}}^{*}\) can be given as the solution to

\[C(_{n},_{n})\\ B(:,_{n})_{_{n}}^{*}= _{_{n}}\\ (1-).\] (20)

However, in practice, both the matrices \(C(_{n},_{n})\) and \(B(:,_{n})\) are unknown. We aim to use the samples to learn the matrices \(C(_{n},_{n})\) and \(B(:,_{n})\) such that the \(_{_{n}}^{*}\) can also be determined. Our formal algorithm is given in Algorithm 2. The steps 3-8 in Algorithm 2 is to use Algorithm 1 as a subroutine to identify the set \(\) and \(\) that satisfy the conditions in Theorem 2.1. We can show that as long as \(n N_{0}^{}\), where \(N_{0}^{}\) is a threshold that depends on the problem parameters, Algorithm 1 correctly obtains the set \(\) and \(\) satisfying the conditions in Theorem 2.1. Therefore, we exponentially increase the value of \(n\) as input to Algorithm 1 to reach \(N_{0}^{}\).

A crucial element in Algorithm 2 (step 10) is that we adaptively update the value of \(_{_{n-1}}^{n}\) and \(^{n}(s^{})\) as in (22) and (23). We then use the updated \(_{_{n-1}}^{n}\) and \(^{n}(s^{})\) to obtain the value of \(}_{_{n}}^{n}\) as in (21). Such an algorithmic design follows the resolving idea from online LP to achieve a problem-dependent bound. In step 11, we further project \(}^{n+1}\) to the set \(\{ 0:\|\|_{1} 2\}\) to obtain \(^{n+1}\). This prevents \(}^{n}\) from behaving ill when \(n\) is not large and the estimates of \(C(_{n},_{n})\) and \(B(:,_{n})\) are not accurate enough. We can show that when \(n\) is large enough, \(}^{n+1}\) automatically belongs to the set \(\{ 0:\|\|_{1} 2\}\).

## 5 Theoretical Analysis

In this section, we conduct our theoretical analysis. The analysis can be divided into two parts. In the first part, we show that Algorithm 1 can successfully help us identify one optimal basis to work with. In the second part, we show how to learn the optimal distribution over the optimal basis we have identified.

We now present the theorem showing that Algorithm 1 indeed helps us identify one optimal basis with a high probability. In practice, the value of \(\) will be set to \(1/N\) in the following theorem.

**Theorem 5.1**: _For any \(>0\), as long as \(N_{0}\) satisfies the condition_

\[2_{1}(N_{0},)+2_{2}(N_{0},) \{_{1},_{2}\}\] (25)

_the outputs \(_{N_{0}}\) and \(_{N_{0}}\) of Algorithm 1 satisfy the conditions described in Lemma 2.1 with probability at least \(1-(K||||-||^{2}||)\). Moreover, the sets \(_{n}\) and \(_{n}\) will be common for any \(n N_{0}\) satisfying (25), which we denote by \(^{*}\) and \(^{*}\)._

An important problem parameter related to \(^{*}\) and \(^{*}\) can be described as follows. Define

\[A^{*}=C(^{*},^{*})\\ B(:,^{*}).\] (26)We then denote by \(\{_{1}(A^{*}),,_{||+K^{}}(A^{*})\}\) the eigenvalues of the matrix \(A^{*}\). We define \(\) as

\[=\{|_{1}(A^{*})|,,|_{||+K^{}} (A^{*})|\}.\] (27)

From the non-singularity of the matrix \(A^{*}\), we know that \(>0\). We then have the following bound.

**Theorem 5.2**: _With a sample complexity bound of_

\[O(|+K)^{3}||||}{ ^{2}(1-)\{^{2},(1-)^{2} \}}(1/)}{}),\]

_where \(=\{_{1}^{2},_{2}^{2}\}\), \(=_{(s,a)^{*}}\{q^{*}(s,a)\}\) and \(q^{*}\) denotes the optimal solution to LP (6) corresponding to the optimal basis \(^{*}\) and \(^{*}\), we obtain a policy \(^{N}\) from Algorithm 2 (defined in (24)) such that_

\[V_{r}(^{*},_{1})-V_{r}(^{N},_{1})\;\;\;\;V_{k}(^{N},_{1})-_{k},\; k[K].\]

Our Algorithm 2 can be directly applied to solving MDP problems without resource constraints and our bounds in Theorem E.2 and Theorem 5.2 still hold. Note that in the MDP problems, the parameter \(\) can be lower bounded by \(1-\), which follows from the fact that the matrix \(A^{*}\) can simply be represented by the probability transition matrix. Then, we have the following sample complexity bound for our Algorithm 2.

**Proposition 5.3**: _For the MDP problems without resource constraints, i.e., \(K=0\), with a sample complexity bound of_

\[O(|^{4}||}{(1-)^{4} }(1/)}{}),\] (28)

_we obtain a policy \(^{N}\) from Algorithm 2 (defined in (24)) such that_

\[V_{r}(^{*},_{1})-V_{r}(^{N},_{1})\;\;\;\;V_{k}(^{N},_{1})-_{k},\; k[K].\]

In terms of the dependency of our sample complexity bound on other problem parameters such as \(||\), \(||\), and \(1-\), we compare to the series of work [58; 67; 68; 2; 27], that subsequently achieves a sample complexity bound of \(O(|||}{(1-)^{3}^ {2}})\), where the dependency over \(||\), \(||\), and \(1-\) is optimal [24; 43]. Our sample complexity bound in (28) has a worse dependency in terms of \(||\) and \(1-\). This is because we construct an empirical LP to estimate the value of LP (6) and resolve the linear equation as in (21), where the size of the LP (which is \(||\)) and the eigenvalues of the matrix \(A^{*}\) (which is bounded by \(1-\)) will play a part. However, our bound (28) enjoys a better dependency in terms of \(\).

## 6 Conclusions

In this paper, we develop the first instance-dependent \((1/)\) sample complexity for constrained MDP problems. We characterize the instance hardness via corner points of the LP formulation and we develop a resolving algorithm to learn the optimal solution while sticking to the identified optimal basis. The work presented by this paper advances the field of Machine Learning and the algorithmic ideas developed in this paper have a broader impact to inspire new algorithms. Our results are developed for the tabular settings, which pose some limitations to the real-world applications of our methods. We leave the extensions to more involved settings for future work.