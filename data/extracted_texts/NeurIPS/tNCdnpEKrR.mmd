# QGym: Scalable Simulation and Benchmarking of Queuing Network Controllers

Haozhe Chen\({}^{1,}\)1  Ang Li\({}^{1,}\)1  Ethan Che\({}^{2,}\)1

Tianyi Peng\({}^{2}\)  Jing Dong\({}^{2}\)  Hongseok Namkoong\({}^{2}\)

Equal contribution.

###### Abstract

Queuing network control determines the allocation of scarce resources to manage congestion, a fundamental problem in manufacturing, communications, and healthcare. Compared to standard RL problems, queueing problems are distinguished by unique challenges: i) a system operating in continuous time, ii) high stochasticity, and iii) long horizons over which the system can become unstable (exploding delays). To spur methodological progress tackling these challenges, we present an open-sourced queueing simulation framework, QGym, that benchmark queueing policies across realistic problem instances. Our modular framework allows the researchers to build on our initial instances, which provide a wide range of environments including parallel servers, criss-cross, tandem, and re-entrant networks, as well as a realistically calibrated hospital queuing system. QGym makes it easy to compare multiple policies, including both model-free RL methods and classical queuing policies. Our testbed complements the traditional focus on evaluating algorithms based on mathematical guarantees in idealized settings, and significantly expands the scope of empirical benchmarking in prior work. QGym code is open-sourced at https://github.com/namkoong-lab/QGym.

## 1 Introduction

Queuing network control is a fundamental control problem in managing congestion in job-processing systems, such as semiconductor manufacturing fabrication plants, communications networks, cloud computing facilities, call centers, healthcare delivery systems, ride-sharing platforms, and limit-order books . In a typical queuing system, jobs arrive at random intervals, wait in queues until an available server can service them, and then either leave the system or move on to another queue for further processing. The stochastic workload is the defining challenge in queueing: the variability inherent in real-world systems makes the time it takes to process jobs random. To manage performance objectives such as minimizing processing delays, balancing workloads, and improving overall service quality and efficiency, a good controller must dynamically allocate resources accounting for future stochasticity. The ability to plan is especially crucial for systems that experience varying levels of congestion over time.

Routing/scheduling control (i.e., matching jobs with servers) in industrial-scale systems is challenging due to several factors. First, queueing networks can be large and complex, with many different job classes and server types (see, e.g., ). The processing speeds can depend on the server and jobtypes, which must be taken into account to make effective scheduling decisions. Second, in systems such as semiconductor fabrication that involve multiple stages of _sequential processing_, congestion can occur at various points in the queuing network, creating bottlenecks that slow down the entire system. Third, workloads are highly non-stationary, featuring predictable and unpredictable demand spikes over time.

There is a large body of work devoted to developing good routing/scheduling policies for queueing networks. The traditional focus of methodological development has been on simple policies with good theoretical performance guarantees in specific network structures. These include (1) load-balancing rules such as joining the shortest queue, and variations such as the power-of-\(d\) choices where one randomly samples \(d\) queues and sends the job to the shortest one among them ; (2) scheduling rules to minimize delay cost such as shortest processing time first, and variations of it such as the \(c\)-rule  and the generalized \(c\)-rule  when we do not know the exact job size and different job classes are associated with different delay costs; and (3) policies that achieve the maximum stability region such as MaxWeight  and maximum pressure policies .

While easy to implement and interpret, these policies are restrictive to specific queuing architectures or objectives, not data-driven, and difficult to adapt to network-specific features and nonstationarity. As a result, there is a growing interest in using black-box reinforcement learning (RL) techniques to learn queuing controllers in a more data-driven manner, which can better handle realistic settings featuring complex networks with non-stationary workloads.

However, queuing network control poses unique challenges, necessitating new methodological innovation over existing RL algorithms. Compared to typical robotics or game-playing environments, queueing problems have high stochasticity and longer horizons over which the system can become unstable (exploding delays). As a result, model-free RL algorithms have been observed to suffer from instability and substantial stochastic noise in these environments .

Another unique challenge is that the queueing system naturally evolves in continuous time, in contrast to the standard discrete time formulation of RL problems. Existing studies assume inter-arrival times and job processing times are exponentially distributed . This so-called Markovian assumption is invoked to represent the queuing network as a discrete-time Markov Decision Process (MDP) with queue lengths as state variables . However, this assumption frequently does not hold in practice, as realistic event times typically exhibit higher variances .

Figure 1: Highlights of QGym framework for developing and benchmarking queuing algorithms. QGym provides an event-driven simulator and benchmarks a wide range of queuing policies and systems. QGym interface also allows users to easily specify new queuing policies and systems.

Prior works  demonstrate the performance of RL algorithms on a small number of problem instances and there is a lack of a common simulation environment or benchmark suite that provides comprehensive evaluations of baselines, including RL algorithms and theory-driven queuing policies. To address this need, we develop a flexible queuing simulation framework (open-sourced and public), QGym, suitable for benchmarking queueing policies across a wide range of problem instances. Our framework can simulate systems with general, non-exponential, and non-stationary event time distributions, requiring only samples from these distributions, which may be obtained from datasets.

This is achieved through the _discrete event dynamical system_ representation of queuing networks, a dominant paradigm for queuing simulation (see, e.g., AnyLogic , SimPy ). Although our queueing setting is markedly different, our framework is broadly inspired by OpenAI Gym . Our modeling framework helps bridge the gap between existing applications of RL, which deal with idealized environments, and industrial simulation paradigms for performance analysis in real-world systems.

We instantiate our abstract and flexible queuing simulation framework with a comprehensive list of queueing environments. Specifically, we consider parallel server systems motivated by skill-based routing problems in service system applications, where the processing speeds depends on both the server type and job type (match of skills) . We also implement the criss-cross network that is widely studied in the queuing control literature . Finally, we consider networks with tandem and reentrant structures that arise in both manufacturing and service systems and is known to suffer from bottleneck resources .

The initial set of environments we provide include systems calibrated from real-world applications. For instance, we have a parallel server system with 8 customer classes and 11 different types of servers (server pools) modeling patient flow in the hospital inpatient ward network .

Finally, we provide a comprehensive list of baseline policies that span multiple literature. From the classical queueing literature, we implement the \(c\)-rule, the MaxWeight and maximum pressure policy, and the fluid-based policies . For model-free RL algorithms, we implement several variations of PPO algorithms tailored for queuing network controls. These resources aim to facilitate a thorough and standardized evaluation of RL methods in diverse queuing scenarios.

Figure 2: QGym provides a unified and comprehensive benchmarking system for queueing policies, across a range of realistic environments.

Taken together, the \(\) framework provides the first comprehensive and flexible framework for benchmarking queuing algorithms across a range of different environments. Our initial empirical benchmarking highlight the following considerations that impact the practical performance of RL policies (Sec. 4).

* **Policy architecture is important.** Without any modifications, RL algorithms such as Proximal Policy Optimization (PPO) fail to achieve stability. But equipped with a simple modification inspired by queuing theory, it is able to outperform baseline queuing methods in 77% of instances.
* **Performance gains of RL are larger in noisy, non-exponential environments.** Our modified PPO is able to tailor the policy to the higher noise environment, achieving larger relative gains.
* **Larger networks are still hard to control.** PPO mostly outperforms queuing baselines in small networks, but struggles for larger, more realistic ones

Related Work.Our work is related to three bodies of work. First, it connects to the research developing RL algorithms for queuing network control problems (see, [36; 43; 41; 16; 29; 50] for some recent development). Most of these studies focus on stationary and Markovian systems, and the algorithm performance is empirically tested on a limited set of problem instances. Our work complements this research by creating a flexible queuing simulation framework that can handle more complex systems. \(\) offers a diverse set of problems for empirically validating the performance of different RL algorithms and comparing them to standard queueing policies.

Second, our work is related to research on discrete-event simulation [22; 5; 39] and simulation software such as Simio and AnyLogic. Our work builds on these foundations and extends them to facilitate the benchmarking of RL methods in an _open-sourced, public forum_ so that the research community can build on our framework (e.g., crowdsourcing more environments).

Lastly, our work aligns with the growing efforts to build RL library and benchmarking suite for sequential decision-making problems in Operations Research [27; 3; 21]. Our contribution complements these initiatives by focusing specifically on queuing network control problems. This specialization enables us to develop a tailored queuing simulation environment that is highly flexible and capable of addressing a diverse set of queuing control problems. See Table 3 for a comparison between our work and related lines of work.

## 2 An Event-Driven Queuing Simulation Framework

We implement the following key features in order to design a flexible framework for training and evaluating queueing policies across across diverse environments.

1. [leftmargin=*]
2. Event-Driven Architecture: To address the continuous time nature of the problem, \(\) employs an event-driven approach, where system states are updated when new events occur. This enhances the scalability of our framework and allows supporting _arbitrary_ arrival patterns, in contrast to traditional discrete time-step-driven models.
3. Extensive Customizability: \(\) allows extensive customization in the queuing network topology, job processing pipelines, and stochastic inputs, enabling a broad set of queuing systems that meet the needs of both academic and industrial applications (see Sec. 4).
4. OpenAI Gym Integration: Built on the OpenAI Gym interface, \(\) facilitates easy testing and deployment of diverse queuing policies, both RL-based and traditional. Its modular design promotes easy integration of new functionalities, supporting continuous evolution.

Event-Driven MDP FormulationWe begin by describing how to convert a classical multi-class queuing scheduling problem into an _event-driven_ MDP problem. Consider a queuing system with

Figure 3: Comparison of our work with related methods

[MISSING_PAGE_FAIL:5]

Network Topology.We allow customized network topologies. Given a binary matrix \(B^{M N}\), server \(j\) is permitted to serve jobs from queue \(i\) only when \(B_{ij}=1\).

Job Transition.Completed jobs from queue \(i[M]\) can transition to another queue \(i^{}[M]\) instead of leaving the system. This facilitates complex job processing pipelines, such as re-entrant and tandem queues.

Arbitrary Arrivals.Users can define arbitrary arrival patterns for queues via a Python function that inputs the current time and outputs the time until the next arrival. This feature allows for the simulation of time-varying and non-Poisson arrivals. Arrivals "generated" from real data are also supported.

Service Time Distribution.Service times are drawn from arbitrary distributions specified by the user, with service rates as parameters. Although time-varying service rates are not yet supported, they can be implemented in a manner similar to arrival patterns.

Server Pool.Users can define each class of server as a pool. When many servers share the same characteristics, users can specify the number of servers in each class (server pool) instead of creating numerous separate servers and inflating the size of the network matrix. This mechanism enables the simulation of large-scale systems without compromising performance.

Job-Level Tracking.The simulator tracks states at the job level, monitoring the service time for each job in a queue. The fine-grained job-level tracking enables simulation of parallel-server systems, where multiple servers can serve a single queue. To illustrate why a less fine-grained choice of tracking at queue level could fail, we consider the cases where multiple servers serve a single queue and the jobs are preempted. Only tracking at queue level does not allow recording remaining service time for each job and resuming service them later. Tracking states at job level enables flexible simulations of parallel server systems.

Reward.Users can define rewards using arbitrary functions on states and actions. In this paper, we focus on minimizing holding costs as a representative example.

OpenAI Gym Design.The simulator environment is structured as an OpenAI Gym environment, adhering to its design principles. This allows users to train and test a variety of reinforcement learning algorithms seamlessly. Each simulation trajectory consists of a sequence of steps (defined in the OpenAI Gym step format) and supports batch-based GPU execution to accelerate computation. We provide a range of environments (e.g., N-model, reentrant, re-reentrant, criss-cross, etc.) and policies, including both RL methods and traditional ones (see Sec. 3), to facilitate easy testing. Users can conveniently configure environment and policy parameters using.yaml files, and new environments and policies can be added by following the OpenAI Gym convention. See Figure 4 for code snippets of our user-friendly interface for defining and runnning experiments. More details can be found in the Appendix C.

## 3 Benchmark Policies

In this section, we introduce the queuing policies benchmarked in our testbed. Formally, each policy \((|o)\) maps an observation \(o^{N}\) (the current queue-lengths) to an action \(a\{0,1\}^{N M}\). These include both traditional control-based policies and RL-based policies tailored for queuing.

Figure 4: QGym provides an user-friendly interface to define and run experiments for evaluating routing policies on queuing networks.

### Traditional Policies

All traditional policies considered fall into the class of policies using the linear assignment rule: given a policy \(\), a priority matrix \(^{M N}\) is outputted from \(\) at each step, the action (i.e., job-server assignment) is then decided by

\[_{a}_{i,j}_{ij}a_{ij}\]

where \(^{M N}\) captures the feasibility (e.g., compatibility and resource capacity) constraints.

\(c_{}\)**-rule.** A classic policy for scheduling multiple classes of jobs is the \(c_{}\)-rule, which has been shown to minimize the linear waiting cost in multi-class single-server queues . In this case, \(_{ij}=c_{i}_{ij}\). Server \(j\) prioritizes the queue with a larger \(c_{i}_{ij}\)-index, where \(c_{i}\) denotes the holding cost per job per unit time for queue \(i\), and \(_{ij}\) is the service rate when server \(j\) processes a job from queue \(i\).

**MaxWeight.** Another important class of policies is known as MaxWeight policies, which has been shown to be maximally stable for single-hop networks  and are also known for their favorable asymptotic properties under a resource pooling condition . We consider a specific form of MaxWeight policy where \(_{ij}=c_{i}Q_{i}_{ij}\) with \(Q_{i}\) being the queue length of queue \(i\). Here, by taking the queue lengths into account, we are able to better balance the workload in the system.

**Maximum pressure.** The maximum pressure policies, which are also known as the back pressure policies, are similar to the MaxWeight policies but account for workload externality within the network. This additional consideration allows for better workload balancing in the multihop setting, especially in networks with tandem or reentrant structures. These policies have been shown to be maximally stable in multi-hop networks [48; 17]. We consider a specific form of the maximum pressure policy under which \(_{ij}=(c_{i}Q_{i}_{ij}-_{k=1}^{M}c_{k}Q_{k}_{ij}p_{ik})\), where \(p_{ik}\) is the probability that after a class \(i\) job is processed by server \(j\), it will join queue \(k\) next. Note that when \(p_{ik}=0\) for all \(k\), the maximum pressure policy simplifies to the MaxWeight policy. However, when \(p_{ik}>0\) for some \(k\), the maximum pressure policy accounts for the fact that processing a class \(i\) job will generate a class \(k\) job, thus considering the impact on "downstream" queues.

**Fluid Policy.** One can derive a 'fluid model' of the queuing network as a system of ordinary differential equations (ODEs) driven by the service and arrival rates of the network . By discretizing the ODEs on a finite grid, one can minimize the linear holding costs by solving a linear program (LP). We then use the computed priorities \(_{ij}\) in the original queuing network. To maintain fidelity with the original dynamics, we periodically re-solve the LP. We solve the LP via CVX , and resolve after every 1000 steps.

### Deep RL based methods

Proximal Policy Optimization (PPO) has been a popular choice for applying RL to queuing [29; 16]. Following the convention, we implemented a few variants of PPO in our testbed. We apply existing and develop new modifications to improve the stability and scalability of PPO in queuing systems.

**PPO.** The action space in our problem is \(a^{M N}\). Thus, directly applying vanilla PPO  will suffer from the explosion of dimensionality. To address this issue, we require \(\{a_{ij}\}_{i=1}^{M}\) to be a probability distribution for all \(j[N]\). We sample \(a^{}_{j}\{a_{ij}\}_{i=1}^{M}\) for each \(j\) independently to decide which queue server \(j\) serves. The feasibility constraint is then verified by the environment. Compared to the existing queuing RL method that discretizes the action space , this parameterization is much more scalable when \(M\) and \(N\) both grow. The classic normalization tricks have been implemented to make PPO more stable (e.g., advantage function normalization, reward normalization, etc.). In addition, to reduce the variance for the Generalized Advantage Estimation (GAE) in PPO

\[_{t}^{(,)}:=_{l=0}^{T}()^{l} _{t+l}^{V}_{t}^{V}=r_{t}+ V(s_{t+1} )-V(s_{t}),\]

we truncate \(T\) to \(T_{0}\) where \(T_{0}\) is the first time that all queues are empty (i.e., regenerative point).

**PPO with Behavior Cloning (PPO-BC).** Implementing the PPO described above with a random initial policy still suffers from poor performance due to instability. Similar to , we address this by using behavior cloning. We first train \(\) to imitate a Max-Weight style policy that assigns serversto classes with probability proportional to \(e^{Q_{i}}\). Using this procedure as a warm-up significantly enhances the stability of training and achieves much better results compared to PPO alone.

**Work-Conserving PPO (PPO-WC).** We have a simple observation: the policy should never assign server capacity to an empty queue (so-called 'work conservation' rule ). We impose this 'inductive bias' to the policy design directly. To do so, we mask the probabilities \(a_{ij}\) obtained from PPO while preserving differentiability:

\[a_{ij}=\{Q_{i}>0\}}{_{i=1}^{M}a_{ij}\{Q_ {i}>0\}}\]

where \(Q_{i}\) is the length of queue \(i\) when taking actions. In case the queues are all empty, we avoid division-by-zero errors by clipping the denominator for some small \(\). As we observe in the experiments in Section 4, this small change greatly improves the performance of PPO. With randomly initialized policy parameters, training algorithms utilizing WC policy parameterization consistently outperform those using vanilla parameterization. Notably, the PPO-WC training algorithm demonstrates a high training stability, such that action clipping is almost never required, which was the core advantage of PPO. To further validate the effectiveness and advantages of WC parameterization, we implemented **A2C** (a vanilla actor-critic algorithm without clipping and KL regularization) with the same WC parameterization and observed comparable performance to that of PPO-WC. These results underscore the robustness and generalizability of WC parameterization.

## 4 Experiments

Using our environment, we benchmark the performance of PPO and traditional queuing baselines across a diverse suite of queuing networks. We curate a set of queuing network instances, drawing upon networks studied in the queuing literature as well as novel instances, with coverage of network architectures relevant to manufacturing, hospital patient flow, and wireless network applications. Overall, we observe that while PPO alone performs quite poorly, _PPO-WC outperforms the traditional policies in 77% of all instances_, highlighting the importance of incorporating queuing structure in RL policy design.

### Setup

**Network structure.** In total, we consider 20 unique problem instances across the following networks. See Fig. 5 for the corresponding network topologies.

1. **Hospital**: Patients arrive to \(M=8\) specialties (Cardiology, Surgery, Orthopedics, Respiratory disease, Gastroenterology and endoscopy, Renal disease, General Medicine, Neurology) split across \(11\) inpatient wards. Each ward consists of multiple beds (servers). In total, this is modeled by \(N=497\) servers across the \(11\) wards. The hospital employs a focused-care model where each ward is primarily designated to serve patients from one specialty or two specialties. The network topology, arrival rates, and service rates are calibrated to a real hospital setting.
2. **Input-Queue Switch**[17; 33]: Packets in a crossbar switch arrive to \(M=6\) queues, and are processed by \(N=3\) servers.
3. **Reentrant (\(L\))**[8; 16]: Manufacturing lines process goods in several sequential steps \(L\). We consider a family of instances with \(L\{2,...,10\}\) with \(M=3L\) queues and \(N=L\) servers. High variance in the service times can lead to bottlenecks in the network, and so we also consider instances with hyper-exponential service times, which are mixtures of exponential distributions.
4. **Five-by-Five Network**: Call-centers route customers from \(M=5\) classes to \(N=5\) servers. Call center demand changes throughout the day, which we model through time-varying inter-arrival times \(^{A}((t))\).
5. **Criss-Cross**[25; 6; 31]: A standard reentrant network considered in the literature, consisting of \(M=3\) queues and \(N=2\) servers.
6. **N-model**: A standard parallel-server system considered in the literature, consisting of \(M=2\) queues and \(N=2\) servers.

**Objective.** The core performance metric we consider is the long-run-average total queue-length, which is approximated by averaging over a long horizon of \(n\) events.

\[[}_{k=1}^{n}_{i=1}^{N}Q_{i}(t_{k})(t_{k+1}-t _{k})]=[}_{0}^{t_{n}}_{i=1}^{N}Q_{i} (t)dt]\] (4)

For each policy, we estimate the expected time-average total queue length by evaluating the policy over \(100\) trajectories. We also report the corresponding standard errors.

**Training Procedure** All PPO variants were trained under the same conditions. Each policy was trained over \(100\) episodes, each consisting of \(50,000\) environment steps parallelized over \(50\) actors. Following existing works , we used a discount factor of \(=0.998\), a GAE parameter of \(=0.99\), and set the KL divergence penalty of \(=0.03\). For the value network, we used a batch size of \(2500\), while for the policy network, we used the entire rollout buffer (batch size of \(50,000\)) to take one gradient step. We performed \(3\) PPO gradient updates on the same rollout data. For all the experiments, we used Adam optimizer with a cosine decaying warming-up learning rate scheduler. The learning rates were set to \(3 10^{-4}\) for the value network and \(9 10^{-4}\) for the policy network. We used 3% of the training horizon to warm up to the maximum learning rate and then cosine decayed to \(1 10^{-5}\) for both networks.

### Results

Tables 1-5 document the time-averaged total queue length for the policies we consider. Our systematic benchmarking illustrates the differences in practical performance of reinforcement learning and traditional queuing policies. Our findings can be summarized into three folds.

**PPO learns an effective controller, but only under the right policy architecture.** Without any modifications, in every setting PPO fails to stabilize the queuing network, systematically confirming an observation made in previous works . Behavior cloning a stabilizing policy drastically improves the training process, yet the policy still fails to achieve parity with the traditional queuing methods. It is only when we endow PPO with a work-conserving policy, that we are able to completely stabilize the training process and surpass the performance of traditional queuing methods in most

Figure 5: Queuing systems QGym benchmarks. (a) Real-world example of hospital routing. (b) Real-world example of data input-switch routing. (c) and (f) Two variations of reentrant networks. (d) Five-by-Five netowrk for modeling call centers. (e) Criss-cross network. See details in section 4

[MISSING_PAGE_FAIL:10]