# Locally Invariant Explanations: Towards Stable and Unidirectional Explanations through Local Invariant Learning

Locally Invariant Explanations: Towards Stable and Unidirectional Explanations through Local Invariant Learning

 Amit Dhurandhar

IBM Research

Yorktown Heights, USA

adhuran@us.ibm.com

&Karthikeyan Natesan Ramamurthy

IBM Research

Yorktown Heights, USA

knatesa@us.ibm.com

Equal contribution

&Kartik Ahuja

Mila

Montreal, Canada

kartik.ahuja@mila.quebec

&Vijay Arya

IBM Research

Bangalore, India

vijay.arya@in.ibm.com

###### Abstract

Locally interpretable model agnostic explanations (LIME) method is one of the most popular methods used to explain black-box models at a per example level. Although many variants have been proposed, few provide a simple way to produce high fidelity explanations that are also stable and intuitive. In this work, we provide a novel perspective by proposing a model agnostic local explanation method inspired by the invariant risk minimization (IRM) principle - originally proposed for (global) out-of-distribution generalization - to provide such high fidelity explanations that are also stable and unidirectional across nearby examples. Our method is based on a game theoretic formulation where we theoretically show that our approach has a strong tendency to eliminate features where the gradient of the black-box function abruptly changes sign in the locality of the example we want to explain, while in other cases it is more careful and will choose a more conservative (feature) attribution, a behavior which can be highly desirable for recourse. Empirically, we show on tabular, image and text data that the quality of our explanations with neighborhoods formed using random perturbations are much better than LIME and in some cases even comparable to other methods that use realistic neighbors sampled from the data manifold. This is desirable given that learning a manifold to either create realistic neighbors or to project explanations is typically expensive or may even be impossible. Moreover, our algorithm is simple and efficient to train, and can ascertain stable input features for local decisions of a black-box without access to side information such as a (partial) causal graph as has been seen in some recent works.

## 1 Introduction

Deployment and usage of neural black-box models has significantly grown in industry over the last few years creating the need for new tools to help users understand and trust models . Even well-studied application domains such as image recognition require some form of prediction understanding in order for the user to incorporate the model into important decisions [46; 33]. An example of this could be a doctor who is advised by a model of a positive cancer diagnosis based on an image scan. Since the doctor holds responsibility for the final diagnosis, the model must provide sufficient reason for its prediction. Even new text categorization tasks  are becoming important with the growing need for social media companies to provide better monitoring of public content. Twitter was monitoring tweets related to COVID-19 in order to label tweets containing misleading information, disputed claims, or unverified claims . Laws will likely emerge requiring explanations for why red flags were or were not raised in many examples. In fact, the General Data Protection and Regulation (GDPR)  act passed in Europe already requires automated systems that make decisions affecting humans to be able to explain them. Given this acute need, a number of methods have been proposed to explain local decisions (i.e. example specific decisions) of classifiers [41; 35; 46; 33; 13]. Locally interpretable model-agnostic explanations (LIME) is arguably the most well-known local explanation method that requires only query (or black-box) access to the model. Although LIME is a popular method, it is known to be sensitive to certain design choices such as i) (random) sampling to create the _(perturbation) neighborhood_2, ii) the size of this neighborhood (number of samples) and iii) (local) fitting procedure to learn the explanation model [37; 54]. The first, most serious issue could lead to nearby examples having drastically different explanations making effective recourse a challenge. One possible mitigation is to increase the neighborhood size but one cannot arbitrarily do so as it not only leads to higher computational cost, but also in today's cloud computing-driven world it could have direct monetary implications where every query to a black-box model has an associated cost . There have been variants suggested to overcome these limitations [8; 45; 39; 15] primarily through mechanisms that create realistic neighborhoods or through adversarial training , however, their efficacy is restricted to certain settings and modalities based on their assumptions and training strategies.

In this paper we introduce a new method called Locally INvariant EXplanations (LINEX) inspired by the invariant risk minimization (IRM) principle , that produces explanations in the form of feature attributions that are robust to neighborhood sampling and can recover faithful (i.e. mimic black-box behavior), stable (i.e. similar for closeby examples) and unidirectional (i.e. same sign attributions a.k.a. feature importances) for closeby examples, see section 4.1) explanations across tabular, image, and text modalities. In particular, we show that our method performs better than the competitors for random as well as realistic neighborhood generation, where in some cases even with the prior strategy our explanation quality is close to methods that employ the latter. Qualitatively, our method highlights (local) features as important that in the particular locality i) have consistently high gradient with respect to (w.r.t.) the black-box function and ii) where the gradient does not change significantly, especially in sign. Such stable behavior for LINEX is visualized in Figure 1, where we get similar explanations for nearby examples in the IRIS dataset. The (in)fidelity of LINEX is still similar to LIME (see Table 2), but of course our explanations are much more stable.

## 2 Related Work

Posthoc explanations can typically be partitioned into two broad categories global and local. Global explainability avers to trying to understand a black-box model at a holistic level where the typical tact is knowledge transfer [28; 17; 16] where (soft/hard) labels of the black-box model are used to train an interpretable model such as a decision tree or rule list . Local explanations on the other hand avers to understanding individual decisions. These explanations are typically in two

Figure 1: Above we visualize for the IRIS dataset the Coefficient Inconsistency (CI) (see Section 5 for exact definition and setup details) between the explanation (top two features) for an example and its nearest neighbor in the dataset. Each circle denotes an example and a _rainbow_ colormap depicts the degree of inconsistency w.r.t. its nearest neighbor where red implies least inconsistency, while violet implies the most. As can be seen LINEX explanations are much more consistent than LIMEâ€™s.

forms, either exemplar based or feature based. For exemplar based as the name suggests similar but diverse examples [30; 25] are provided as explanations for the input in question. While for feature based [41; 35; 13; 33; 55], which is the focus of this work, important features are returned as being important for the decision made for the input. There are some methods that do both . Moreover, there are methods which provide explanations that are local, global as well as at a group level . All of these methods though may not still provide stable and robust local feature based explanations which can be desirable in practice .

Given this there have been more recent works that try to learn either robust or even causal explanations. In  the authors try to learn robust and stable local explanations relative to distribution shifts and adversarial attacks. However, the distribution shifts they consider are linear shifts and adversarial training is performed which can be slow and sometimes unstable . Moreover, the method seems to be applicable primarily to tabular data. There are also works [18; 19] which try to robustify gradient based explanations assuming white box access to the model. Works on causal explanations [22; 27] mainly modify SHAP and assume access to a partial causal graph. Some others  assume white-box access. In this work we do not assume availability of such additional information. There are also works which show that creating realistic neighborhoods by learning the data manifold for LIME [8; 45] can lead to better quality explanations, where in a particular work  it is suggested that projecting explanations themselves on to the manifold can also make them more robust. The need for stability in a exemplar neighborhood for LIME like methods has been highlighted in , with the general desire for stable explanations being also expressed in [52; 49]. Furthermore, it was recently surmised through expert and crowd worker user studies that stability is a key factor when it comes to assessing capability of a model or when learning a new domain .

Given that our approach is inspired from IRM we now describe, how it is novel w.r.t. to it. It is important to realize that IRM approaches such as [3; 4] are designed for the out-of-distribution (OOD) generalization, which learn global models directly from the data. The main similarity of these works to ours is only that they also are game theory based approaches, but with the details being quite different. For one, they assume accessibility to environments which (ideally) correspond to different interventional distributions and with assumptions on the structural causal model derive results on how the true causal factors could be divulged. In our case, we propose ways to _generate environments_ as they are not given, and _have \(l_{1}\) and \(l_{}\) constraints on the entire and environment specific parts_ of the model respectively, which is not the case with these prior works. As such those algorithms do not produce _sparse unidirectional models_ that are also consumable. Moreover, the _perspective we provide is novel_ in the context of local posthoc explanations where a priori it is not obvious that approaches from OOD generalization could be extended and adapted. Additionally, we propose a novel metric _Unidirectionality_ which is not part of any of these works, but as we have argued it is a desirable property for explanations.

## 3 Preliminaries

**Invariant Risk Minimization:** Given a collection of training datasets \(D=\{D_{e}\}_{e_{tr}}\) gathered from a set of environments \(_{tr}\), where \(D_{e}=\{_{e}^{i},y_{e}^{i}\}_{i=1}^{n_{e}}\) is the dataset gathered from environment \(e_{tr}\) and \(n_{e}\) is the number of points in environment \(e\). The feature value for data point \(i\) is \(_{e}^{i}\) and the corresponding label is \(y_{e}^{i}\), where \(^{d}\) and \(\). Each point \((_{e}^{i},y_{e}^{i})\) in environment \(e\) is drawn i.i.d from a distribution \(_{e}\). Define a predictor \(f:\).

The goal of IRM is to use these collection of datasets \(D\) to construct a predictor \(f\) that performs well across many unseen environments \(_{all}\), where \(_{all}_{tr}\). Define the risk achieved by \(f\) in environment \(e\) as \(R_{e}(f)=_{e}(f(_{e}),Y_{e})\), where \(\) is the square loss when \(f(_{e})\) is the predicted value and \(Y_{e}\) is the corresponding label, \((_{e},Y_{e})_{e}\) and the expectation \(_{e}\) is defined w.r.t. the distribution of points in environment \(e\).

An invariant predictor is composed of two parts a representation \(^{d n}\) and a predictor (with the constant term) \(^{d 1}\). We say that a data representation \(\) elicits an invariant predictor \(^{}\) across the set of environments \(_{tr}\) if there is a predictor \(\) that achieves the minimum risk for all the environments \(*{argmin}_{}^{d 1}}R_{e}(}^{}),\  e_{tr}\). IRM may be phrased as the following constrained optimization problem :

\[_{^{d n},^{d 1}} _{e_{tr}}R_{e}(^{})\ \ \  *{argmin}_{}^{d 1}}R_{e}(}^{ }),\  e_{tr}\] (1)If \(^{}\) solves the above, then it is an invariant predictor across the training environments \(_{tr}\).

**Nash Equilibrium (NE):** To understand how certain key aspects of our method function let us revisit the notion of Nash Equilibrium . A standard normal form game is written as a tuple \(=(,\{u_{i}\}_{i},\{_{i}\}_{i })\), where \(\) is a finite set of players. Player \(i\) takes actions from a strategy set \(_{i}\). The utility of player \(i\) is \(u_{i}:\), where we write the joint set of actions of all the players as \(=_{i}_{i}\). The joint strategy of all the players is given as \(\), the strategy of player \(i\) is \(_{i}\) and the strategy of the rest of players is \(_{-i}=(_{{}_{i^{}}})_{i^{} i}\).

**Definition 1**.: _A strategy \(^{}\) is said to be a pure strategy Nash equilibrium (NE) if it satisfies, \(u_{i}(^{}_{i},^{}_{-i}) u_{i}(k,^{ }_{-i}), k_{i}, i\), where \(u_{i}(^{}_{i},^{}_{-i})=u_{i}(^{}_{1}, ^{}_{2},...,^{}_{})=u_{i}(^{ })\)._

NE thus identifies a state where each player is using the best possible strategy in response to the rest of the players leaving no incentive for any player to alter their strategy. In seminal work by  it was shown that for a special class of games called concave games such a pure NE always exists. This is relevant because the game implied by Algorithm 1 falls in this category.

## 4 Methodology

We first define desirable properties for our explanation methods. The first three have been seen in previous works, while the last _Unidirectionality_ is new. We then describe our method where the goal is to explain a black-box model \(f:\) for individual inputs \(\) based on predictors \(\) by looking at their corresponding components, also termed as feature attributions.

We take inspiration from IRM since, our goal here too is to extract robust features that are ideally stable and unidirectional. The main difference is that we do not learn a new (possibly invariant) representation since, we desire interpretability and this new representation may not be interpretable. We hence, are restricted to the provided input or some other interpretable representation. Thus, given that \(\) where \(n=1\) (since local explanations) in our setup, our goal is to find the best predictor \(\) (viz. high fidelity) for an input that will eliminate or at least mitigate the effect of unstable features. In other words, we want to identify features in the input space that will (roughly) have the same importance (i.e. are invariant) in the neighborhood of the example we want to explain. Our approach as we will see is similar in spirit to IRM games , where we adopt a game theoretic strategy to obtain such explanations. The differences with IRM games are mentioned in the last paragraph of Section 2.

### Desirable Properties

We now discuss certain properties we would like our explainability method to have in order to provide robust explanations that could potentially be used for recourse. Let \(D_{t}\) denote a (test) dataset with examples \((x,y)\) where \(y_{b}(x)\) is the black-box models prediction on \(x\) and \(y^{x^{}}_{e}(x)\) is the prediction on \(x\) (\(\)) using the explanation model at \(x^{}\). The feature attributions (or coefficients) for the explanation model at \(x\) are denoted by \(c^{x}_{e},_{x}\) denotes the exemplar neighborhood of \(x\) with \(|.|_{}\) denoting cardinality and \(|.|\) denoting absolute value.

**Fidelity:** This is the most standard property which all proxy model based explanation methods are evaluated against [41; 35; 32] as it measures how well the proxy model simulates the behavior of the black-box (i.e. faithfulness to the black box) it is attempting to explain. Here we define inverse of it, that is _Infidelity (INFD)_, as the MAE between the black-box and explanation model predictions across all the test points:

\[=|_{}}_{(x,y) D_{t}}|y_{b}(x)-y^{x }_{e}(x)|.\] (2)

We also define another metric here called _Generalized Infidelity (GI)_, which also been used in previous works  to measure the generalizability of local explanations to neighboring test points. It is defined as:

\[=|_{}}_{(x,y) D_{t}}_{x}|_{}}_{x^{}_{x}}|y_{b}(x)-y^ {x^{}}_{e}(x)|.\] (3)

**Stability:** This is also a popular notion [26; 40; 52] to evaluate robustness of explanations. Largely, stability can be measured at three levels. One is prediction stability, which measures how much the predictions of an explanation model change for the same example subject to different randomizations within the method or across close by examples. The second is the variance in the feature attributions again for the same or close by examples. It is good for a method to showcase stability w.r.t. both even though in many cases the latter might imply the former. An interesting third notion of stability is the correlation between the feature attributions of an explanation model and average feature values of examples belonging to a particular class. This measures how much does the explanation method pick features that are important for the class, rather than spurious ones that seem important for just the example. We thus define two stability metrics.

_Coefficient Inconsistency (CI):_ This notion has been used before  to measure an explanation methods robustness. It can be defined as the MAE between the attributions of the test points and their respective neighbors:

\[=|_{}}_{(x,y) D_{t}}_{x}|_{}}_{x^{}_{x}}|c^{x}_{e} -c^{x^{}}_{e}|_{1}.\] (4)

_Class-Attribution Consistency (CAC):_ For local explanations of classification black-boxes, we expect certain important features to be highlighted across most of the explanations of a class. This is codified by this metric which is defined as follows:

\[=|_{}}_{y}r(^{ y}_{e},_{y}),\] (5)

where \(\) denotes the set of class labels in the dataset, \(_{y}\) the mean (vector) of all inputs in class \(y\), \(^{y}_{e}\) the mean explanation for class \(y\) and \(r\) the Pearson's correlation coefficient. This metric quantifies the consistency between the important features for a class and attributions provided by the explanations.

**Black-box Invariance:** This is the same as implementation invariance defined in . Essentially, if two models have exactly the same behavior on all inputs then their explanations should also be the same. Since, our method is model agnostic with only query access to the model it is easy to see that it satisfies this property if the same environments are created.

**Unidirectionality:** This is a new property, but as we argue that this is a natural one to have. Loosely speaking, unidirectionality would measure how consistently the sign of the predictor for a feature is maintained for the same or close by examples by an explanation method. This is a natural metric , which from an algorithmic recourse  perspective is also highly desirable. For instance, recommending a person to increase their salary to get a loan and then recommending to another person with a very similar profile to decrease their salary for the same outcome makes little sense.

We define the unidirectionality \(\) as a measure of how consistent the sign of the attribution for a particular feature in a local explanation is when varying neighborhoods for the same example or when considering different close by examples. As such, given \(m\) attributions for each of \(d\) features denoted by \(w^{(1)}_{1},...,w^{(d)}_{m}\) the metric for an example is:

\[=_{i=1}^{d}|_{j=1}^{m}(w^{( i)}_{j})|\] (6)

where \(|.|\) stands for absolute value. Clearly, the more consistent the signs for the attribution of a particular feature across \(m\) attributions the higher the value, where the maximum value can be one. If equal number of attributions have different signs for all features then \(\) will be zero, the lowest possible value. This property thus measures how intuitively consistent (ignoring magnitude) the explanations are. Given its sole focus on the sign of the attributions it compliments the above metrics along with attributional robustness metrics [9; 44].

### Method

#### 4.2.1 Description

In Algorithm 1, we show the steps of our method LINEX. The input is the example we want to explain \(\), the black-box predictor, a few thresholds that we describe next and \(k\) (local) environmentswhose creation is described in Section 4.2.2. In the algorithm we iteratively learn a constrained least squares predictor for each environment, where the final (local) linear predictor is the sum of these individual predictors. In each iteration when computing the contribution of environment \(e_{i}\) to the final summed predictor, the most recent contributions of the other predictors are summed and the residual is optimized subject to the constraints. The first constraint is a standard lasso type constraint which tries to keep the final predictor sparse as in LIME.

**Why \(l_{}\) constraint?** The second constraint is more unique and is a \(l_{}\) constraint on the predictor of just the current environment. This constraint as we prove in Section 4.3 is essential for obtaining robust predictors. To intuitively understand why this is the case consider we have two environments. In this case if the optimal predictors for a feature in each environment have opposite signs, then the Nash equilibrium (NE) is when each predictor takes \(+\) or \(-\) values as they try to force the sum to have the same sign as them. _In other words, features that have a disagreement in even the direction of their impact are eliminated by our method._ LIME type methods on the other hand would simply choose some form of average value of the predictors which may be a risky choice especially for actionability/recourse given that the directions change so abruptly. On the other hand, if the optimal predictors for a feature in the two environments have the same sign, the lower absolute valued predictor would be chosen (assuming \(\) is greater) making it a careful choice. The reasoning for this and a discussion involving more than two environments is given in Section 4.3.

The overall algorithm resembles a (simultaneous) game where each environment is a player trying to find the best predictor for its environment given all other predictors and constraints. Formally, for \(i\{1,...,k\}\) the players are \(=\{_{i}\}\), their strategy space is \(_{i}=[-,]^{d}\) and their utility \(u_{i}(}_{i},}_{-i}^{+})=-_{}_{i}()}(f(})-}_{-i}^{+^{ }}}-}_{i}^{}})^{2}\). The optimization problem solved by each player is convex as norms are convex.

#### 4.2.2 Creating Local Environments

In standard IRM, environments are assumed to be given. In our case of local explainability we have to decide how to produce them. We offer a few options for the environment creation functions \(_{i}\)\( i\{1,...,k\}\) in Algorithm 1.

**Random Perturbation:** This simple approach is similar to what LIME employs. We could perturb the input example by adding zero mean gaussian noise to create the base environment (used by LIME) and then perform bootstrap sampling to create the \(k\) different environments. This will efficiently create neighbors in each environment, although they may be unrealistic in the sense that they could correspond to low probability points w.r.t. the underlying distribution.

**Realistic Generation/Selection:** One could also create neighbors using data generators such as done in MeLIME  or select neighboring examples from the training set as done in MAPLE  to create the base environment following which bootstrap sampling could be done to form the \(k\) different environments. This could provide more realistic neighbors than the previous one, but may be much more computationally expensive. Other than bootstrapping one could also oversample and try to find the optimal hard/soft partition through various clustering type objectives [2; 10].

### Theoretical Results

In this section, we analyze the output of Algorithm 1 with two environments. The extension to multiple environments is discussed following this result, where the general intuition is still maintained but some special cases arise depending on whether there are an even or odd number of environments. To prove our main result we make two assumptions.

**Assumption 1**_The features of the samples in the local environments are independent._

This assumption is satisfied by the most standard way of creating neighborhoods/environments, where gaussian noise is used to create them as described in Section 4.2.2.

**Assumption 2**\(t d\)_, where \(d\) is the dimensionality of the feature vector._

Here \(t\) is the parameter in the \(_{1}\) penalty and \(\) in the \(_{}\) as noted in Algorithm 1. Making this assumption ensures that we closely analyze the role of the \(_{}\) penalty, which is one of our main novelties.

**Definition 2** Let the explanation that each environment \(_{i}\) arrives at for an example \(\) based on unconstrained least squares minimization be \(_{i}^{*}\) where,

\[_{i}^{*}*{argmin}_{^{d}}_{}_{i}(x)}[(f(})-^{} })^{2}]\] (7)

The expectation is taken w.r.t the environment generation distribution.

**Theorem 1**.: _The output of Algorithm 1 under Assumptions 1, 2 and equation 7 is given by:_

\[=_{1}^{*}_{|_{2}^{*}||_{1}^ {*}|}+_{2}^{*}_{|_{1}^{*}|>|_{2}^{*}|} _{_{1}^{*}_{2}^{*}}\] (8)

_where \(\) is element wise product and \(\) is the indicator function._

Proof Sketch.: The above expression describes the NE of the game played between the two local environments each trying to move \(\) towards their least squares optimal solution. Given assumptions 1 and 2, we witness the following behavior of our method. Let the \(i^{th}\) feature of the predictors \(_{1}\) and \(_{2}\) from Algorithm 1 be \(_{1i}\) and \(_{2i}\) respectively. Let the corresponding least squares optimal predictors for the \(i^{th}\) feature have the following relation: \(w_{1i}^{*}>w_{2i}^{*}\) and \(|w_{1i}^{*}|>|w_{2i}^{*}|\). Then the two environments will push the ensemble predictor, \(_{1i}+_{2i}\), in opposite directions during their turns, with the first environment increasing its weight, \(_{1i}\), and the second environment decreasing its weight, \(_{2i}\). Eventually, the environment with a higher absolute value (\(_{1}=1\) since \(|w_{1i}^{*}|>|w_{2i}^{*}|\)) reaches the boundary (\(_{1i}=\)) and cannot move any further due to the \(l_{}\) constant. The other environment \(_{2}\) best responds, where it either hits the other end of the boundary (\(_{2i}=-\)), in which case the weight of the ensemble for component \(i\) is zero, a case which occurs if \(w_{1i}^{*}\) and \(w_{2i}^{*}\) have opposite signs; or gets close to the other boundary while staying in the interior (\(_{2i}=w_{2i}^{*}-\)), in which case the weight of the ensemble for feature \(i\) is \(w_{2i}^{*}\), a situation which occurs if \(w_{1i}^{*}\) and \(w_{2i}^{*}\) have the same sign. 

**Implications of the Theorem 1:** The following are the main takeaways from Theorem 1: (1) If the signs of the explanations for unconstrained least squares for the two environments differ for some feature, then the algorithm outputs a zero for that feature attribution. (2) If the signs of the explanations for the two environments are the same, then the algorithm outputs the lesser magnitude of the two. These two properties are highly desirable from an algorithmic recourse or actionability perspective, where the first biases us to not rely on features where the black-box function changes direction rapidly (unidirectionality). The second, provides a reserved estimate so that we do not incorrectly over rely on the particular feature (stability). Based on similar logic presented in the proof sketch the behavior for more than two environments for LINEX is discussed in Suppl. C.

## 5 Experiments

We test our method on five real world datasets covering all three modalities: IRIS (Tabular) , Medical Expenditure Panel Survey (Tabular) , Fashion MNIST (Image) , CIFAR10 (Image)  and Rotten Tomatoes reviews (Text)  with LIME-like random (rand) and MeLIME-like realistic neighborhood generation (real) or MAPLE-like realistic neighborhood selection (mpl). The summary of black-box classifier accuracies, and type of realistic perturbation used for the datasets are provided in Table 3 in the Supplement. In other cases except FMNIST and CIFAR10 which come with their own test partition we randomly split the datasets into 80/20% train/test partition and average results for the local explanations over this test partition. For LINEX we produce two environments where the two environments are formed by performing bootstrap sampling on the base environment which is created either by rand, real or mpl type neighborhood generation. Thus in all cases the union of the environments is the same as a single neighborhood used to produce explanations for the competitors making it a fair comparison. Behavior with more environments is in Suppl. E.

Given the neighborhood generation schemes we compare LINEX with LIME, Smoothed LIME (S-LIME), MeLIME and MAPLE, where for S-LIME we average the explanations of LIME across the LINEX environments. SHAP's results are in Suppl. H, since it is not a natural fit here. Nor are methods such as saliency maps, gradcam, integrated gradients as they are _white-box_ methods requiring access to a differentiable model.

**Metrics:** We evaluate using five simple metrics: Infidelity (INFD), Generalized Infidelity (GI), Coefficient Inconsistency (CI), Class Attribution Consistency (CAC) and Unidirectionality (\(\)), which are defined in section 4.1. The first two evaluate _faithfulness_, the next two _stability_ and the last _goodness for recourse_.

We report the above metrics in Table 2. Each result in Table 2 is mean \(\) standard error of the mean over five kernel sizes \(\) generally, where \(=\{0.05,0.1,0.25,0.5,0.75\}\). Test neighborhoods do not make sense for random perturbations with FMNIST, CIFAR10 and Rotten Tomatoes because the features (viz. superpixels) used by neighboring test examples are different.

  Example 1 & Example 2 & Example 3 \\   & moving tale of love and destruction in unexpected places, unexamined lives & spare yet audacious.. \\  

Table 1: Below are three example positive sentiment sentences from the Rotten Tomatoes dataset. Green and red indicate the most important word highlighted by MeLIME and LINEX respectively. As seen LINEX highlights stronger positive sentiment words. More examples in Suppl. F.

Figure 3: Sample results using CIFAR10 dataset for dog and bird class. As can be seen LINEX focuses more on salient features such as head and legs for the dog, and wings for the bird (rather than also the background). More examples are shown in Suppl. G.

Figure 2: Sample results using FMNIST dataset for two classes. (a-c): Class _Dress_, (d-f): Class _Sandal_. (a, d): MeLIME explanations. (b, d): LINEX explanations. (c, f): Original images. We observe that LINEX explanations capture important artifacts and thus exhibit significantly higher correlation with the original images for the same level of sparsity, where in aggregate too the correlations are high w.r.t. images belonging to a particular class, thus showcasing higher stability (i.e. high CAC) as is seen in Table 2. More examples are shown in Suppl. G.

Also, we do not use realistic perturbations with MEPS since KDE and VAE generators do not work well with categorical data. In addition, since MEPS data uses regression black-box, CAC cannot be computed. Also for CIFAR10 images in a class are not aligned so CAC is inapplicable. All these justify the missing entries in Table 2. The results were generated on Linux machines with \(56\) cores and \(242\) GB RAM. More details regarding the exact perturbation schemes for LIME/MeLIME/MAPLE, the perturbation neighborhood sizes and the time taken by the methods are in Suppl. A and Suppl. D.

**Observations:**_Quantitatively_, we see that in terms of CAC, LINEX is better than baselines in all cases which indicates that on average the LINEX explanations highlight the important features characterizing the entire class, making them more stable. This is also verified by looking at \(\) and CI metrics where LINEX is similar or better than others. For GI and INFD, the results are more evenly spread which implies that LINEX's key advantage is obtaining stable and unidirectional explanations that are faithful to a similar degree. Ablation studies showing superiority of LINEX over MeLIME on the FMNIST dataset where we have significantly higher INFD than MeLIME are given in Suppl. J.

An interesting observation is that when it comes to the stability metrics (CI and CAC) and unidirectionality LINEX with even random perturbation model is better than MeLIME in some cases. This is very promising as it means LINEX could be potentially be trusted without the need to generate realistic perturbations which may be computationally expensive or not even possible.

_Qualitatively_, we see in Figures 2 and 3, that LINEX explanations are more coherent and highlight more salient features compared to MeLIME. Even on the text data we see more reasonable attributions in Table 1, where "masterpiece", "moving" and "audacious" are highlighted as the most important words indicative of positive sentiment in the three examples. We also performed qualitative error

  _Dataset_ & _Method_ & INFD \(\) & GI \(\) & CI \(\) & \(\) & CAC \(\) \\    & LIME & \(0.015 0.011\) & \(0.132 0.042\) & \(0.319 0.132\) & \(0.646 0.040\) & \(0.667 0.167\) \\  & S-LIME & \(0.015 0.010\) & \(0.077 0.011\) & \(0.143 0.045\) & \(0.704 0.037\) & \(0.878 0.034\) \\  & LINEX/rand & \(0.013 0.009\) & \(\) & \(\) & \(\) & \(\) \\   & MeLIME & \(0.008 0.003\) & \(0.049 0.018\) & \(0.219 0.018\) & \(0.629 0.013\) & \(0.464 0.100\) \\  & LINEX/real & \(0.009 0.003\) & \(\) & \(\) & \(\) & \(\) \\   & MAPLE & \(0.009 0.001\) & \(0.038 0.004\) & \(0.261 0.033\) & \(0.458 0.032\) & \(0.586 0.035\) \\  & LINEX/mpl & \(0.013 0.000\) & \(\) & \(\) & \(\) & \(\) \\   & LIME & \(0.158 0.066\) & \(0.214 0.041\) & \(0.005 0.001\) & \(0.981 0.006\) &  \\  & S-LIME & \(0.158 0.066\) & \(0.214 0.042\) & \(0.005 0.001\) & \(0.974 0.008\) & \\  & LINEX/rand & \(\) & \(\) & \(0.003 0.001\) & \(0.979 0.006\) & \\   & MAPLE & \(\) & \(\) & \(0.007 0.000\) & \(0.957 0.000\) & \\   & LINE/mpl & \(0.098 0.001\) & \(0.094 0.001\) & \(0.007 0.000\) & \(0.950 0.000\) & \\    & LIME & \(0.162 0.003\) & & & & \\  & S-LIME & \(0.142 0.003\) & NA & NA & NA & NA \\   & LINE/rand & \(0.149 0.002\) & & & & \\   & MeLIME & \(\) & \(\) & \(0.007 0.000\) & \(0.769 0.000\) & \(0.327 0.000\) \\  & LINEX/real & \(0.100 0.002\) & \(0.304 0.001\) & \(0.002 0.000\) & \(\) & \(\) \\    & LIME & \(0.191 0.005\) & & & & & \\  & S-LIME & \(0.185 0.002\) & NA & NA & NA & NA \\   & LINE/rand & \(0.186 0.002\) & & & & & \\   & MELME & \(0.100 0.003\) & \(0.412 0.007\) & \(0.014 0.000\) & \(0.546 0.003\) & \\   & LINE/real & \(0.090 0.005\) & \(\) & \(\) & \(\) & \\   & LIME & \(0.079 0.036\) & & & & & \\   & S-LIME & \(0.075 0.035\) & NA & NA & NA & NA \\   & LINE/rand & \(0.069 0.032\) & & & & & \\   & MeLIME & \(\) & \(0.391 0.000\) & \(0.000 0.000\) & \(0.999 0.000\) & \(0.909 0.000\) \\   & LINE/real & \(0.053 0.000\) & \(\) & \(0.000 0.000\) & \(1.000 0.000\) & \(\) \\  

Table 2: Comparison of the different methods based on infidelity (INFD), generalized infidelity (GI), coefficient inconsistency (CI), class attribution consistency (CAC) and unidirectionality (\(\)). \(\) indicates higher value for the metric is better, and \(\) indicates lower is better. Statistically significant results based on paired t-test are bolded. LINEX is better than baselines in 21 out of 40 cases, and worse only in 5 cases. Plots showing behavior with varying neighborhood size, number of environments and kernel width are in Suppl. E.

analysis on FMNIST where our INFD is much worse than MeLIME and is described in Suppl. I. We see that even where LINEX has high infidelity it invariably still focuses on salient features ignoring superfluous features which may result in lower fidelity but may not be critical for correct identification. The goodness of these features identified by LINEX can be further verified by looking at other metrics such as GI, CAC, CI and \(\) in Table 2 where it is either comparable or better than MeLIME.

## 6 Discussion

In this paper we have provided a method based on a game theoretic formulation and inspired by the invariant risk minimization principle to provide faithful, stable and unidirectional explanations. We have defined the latter property and argued that it is somewhat of a necessity (may not be sufficient) for recourse. We have theoretically shown that our method has a strong tendency to be stable and unidirectional as we will mostly eliminate features where the black-box models gradient changes abruptly and in other cases choose a conservative value. Empirically, we have verified this where we outperform competitors in majority of the cases on these metrics. Interestingly, in some cases our method provides more stable and unidirectional explanations with just a random perturbation model relative to more expensive methods that use realistic neighbors.

We now discuss a real world use case we tested our method on. We worked with a large financial institution to explain the fraud detection model they had built. The Association of Certified Fraud Examiners (ACFE) claims that roughly 5% of a companies revenue is lost to fraud every year. Thus, catching fraud or even non-compliance is extremely important for any organization. Their model (\(=1\) else \(0\)) had \(\) 91% accuracy. The inputs to the model were (transactional) invoices and details corresponding to those invoices such as vendor name, invoice amount, purchase order (PO) or not, vendor address, commodity code, country perception indices (CPI), etc. Since, one of the focuses is to reduce false positives accurate explanations are important. We applied LINEX to this setting to explain why certain invoices were classified as fraudulent. The experts found that in majority of the cases (913 out of 1000) the attributions of LINEX especially in terms of sign made sense. For instance, low CPI implies high risk and so LINEX gave a negative coefficient for this feature for most examples, while LIME gave a positive coefficient for many instances. Going forward their plan is to incorporate such capabilities into their workflow to further improve fraud detection precision.

In the future, it would be worth experimenting with more varied strategies to form environments and if possible find the optimal ones , which may lead to picking even more relevant features that are "causal" to the local decision.

## 7 Summary of the Supplement

Information about black-box classifier accuracies and realistic perturbation methods used for the datasets are provided in Table 3. Suppl. A has run time comparisons. Suppl. B has proof of Theorem 1. Suppl. C discusses theoretical behavior of LINEX for more than two environments. Suppl. D has dataset details and hyperparameter specifications. Suppl. E has experiments with different hyperparameter combinations (including more than 2 environments). Suppl. F has additional examples of text data attributions. Suppl. G has example feature attributions with image data. Suppl. H has SHAP results. Suppl. I, J and K has error analysis and ablation studies. Suppl. L has additional synthetic experiments. Suppl. M discusses sensitivity to \(\). Suppl. N demonstrates convergence of LINEX. Suppl. O discusses limitations of LINEX. Figure 30 depicts SLIME variants using median and median of means which turn out to be worse than using the (typical) mean.